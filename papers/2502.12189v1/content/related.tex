\subsection{Preference Alignment and Ranking}
Learning from human preferences \cite{christiano2017deep} aims to better align language models with human intentions and values, making their generated content more helpful, factual, and ethical \cite{ouyang2022training}.
RLHF  \cite{ouyang2022training, stiennon2020learning}  can achieve this alignment through PPO\cite{schulman2017proximal} based on human feedback data.
To circumvent the complexities of the RLHF, DPO \cite{rafailov2024direct} directly learns the distinction between human-labeled preferences and non-preferences by minimizing the difference in their log probabilities.
SLiC \cite{zhao2023slic} and RRHF \cite{yuan2024rrhf} use pair-wise hinge loss to align policy responses.
Curry-DPO \cite{pattnaik2024curry} simulates curriculum learning by sequentially ranking during training, using multiple preference pairs.
Therefore, most frameworks \cite{azar2024general, liu2023statistical} are limited to pairwise preferences and heavily rely on human annotations.
Although DPO proposes list-wise alignment based on the Plackett-Luce assumption \cite{luce1959individual}, no experimental results are provided.

At this stage, PRO \cite{song2024preference} introduces list maximum likelihood estimation (MLE) loss to focus on preference ranking, marking a pioneering effort in list-wise alignment.
However, it lacks attention to other intrinsic attribute values of the responses beyond the semantic content.
LiPO \cite{liu2024lipo}, which is most similar to ours, directly optimizes list-based preferences and considers response labels but has not yet addressed the combination of multiple labels.

% \subsection{Alignment of LLMs.}
% The language modeling objective of LLMs (e.g., predicting the next word) differs from the ultimate goals in LLM applications, such as following instructions and being helpful, factual, and harmless\cite{qi2023fine,bhardwaj2024language,yi2024vulnerability}.
% The behavior of pre-trained LLMs may not necessarily align with the principles of their intended use cases. 
% Therefore, alignment of LLMs \cite{zhu2024lire,wang2024arithmetic} aims to adjust the outputs of general pre-trained language models to better align with human preferences, significantly improving the performance of LLMs in various downstream applications, such as Summarization\cite{hu2024moments}, dialogue agents \cite{niu2024enhancing}, and question-answering \cite{panda2024holmes}.
% Currently, the two most common alignment techniques are instruction tuning \cite{ren2024learning} and reinforcement learning from human feedback (RLHF) \cite{bai2022constitutional,ouyang2022training}. 
% Additionally, emerging alignment techniques such as Constitutional AI \cite{bai2022constitutional} and self-alignment \cite{ren2024learning} are also gaining attention.
% These primarily focus on embedding alignment rules into pre-trained models to constrain harmful behavior during inference. However, they have not explored how to align objectives with multiple attributes implying preferences. 
% Our study demonstrates that the objectives of preference alignment are influenced by multiple factors.

% \subsection{Preference Alignment and Ranking}
% Learning from human preferences \cite{christiano2017deep} aims to better align generative language models with intentions and values, making generations more helpful, factual, and ethical, among other desiderata \cite{ouyang2022training}.
% An important approach for it is reinforcement learning from human feedback (RLHF) \cite{ouyang2022training,stiennon2020learning}, which involves training a reward model as a classifier for preferred and non-preferred actions.
% To circumvent the complexities of the RLHF, based-on Bradley-Terry \cite{bradley1952rank} Direct Preference Optimization (DPO) \cite{rafailov2024direct} replaces rewards with the relative log probabilities of responses.
% SLiC \cite{zhao2023slic} and RRHF \cite{yuan2024rrhf} use pair-wise hinge loss to align policy responses.
% Curry-DPO \cite{pattnaik2024curry} simulates curriculum learning by sequentially ranking during training, using multiple preference pairs.
% Overall, almost all existing preference optimization frameworks \cite{azar2024general, liu2023statistical} are limited to pairwise preferences and lack consideration of list-wise alignment scenarios.

% Although DPO proposes based-on the list-wise Plackett-Luce alignment learning \cite{luce1959individual}, no experimental results are provided.
% At this stage, PRO \cite{song2024preference} introduces list maximum likelihood estimation (MLE) loss and begins to focus on preference ranking, marking a pioneering effort in list-wise preference alignment.
% However, it lacks attention to other intrinsic attribute values of the responses beyond the semantic content.
% LiPO \cite{liu2024lipo}, which is most similar to our approach, directly optimizes list-based ranking preferences and starts to focus on response labels but has not yet addressed the combination of multiple labels.




% This method first collects a large amount of data, with each data point consisting of a context, a pair of context continuations (also known as generated content), and a pair of human preferences indicating which generated content is better. Then, a policy is learned from the collected data to generate high-quality content \cite{azar2024general}.


% Learning from human preferences \cite{christiano2017deep} aims to better align generative language models with human needs. This method first collects a large amount of data, where each data point consists of a context, a pair of context continuations (also known as generated content), and a pair of human preferences indicating which generated content is better. Then, a policy for generating high-quality content is learned from the collected data \cite{azar2024general}.





% Aligning large language models (LLMs) with human preferences can be directly achieved through reinforcement learning (RL), where the model requires only sparse supervisory signals from reward models that act as human proxies, called RLHF. 
% To circumvent the complexities of the RLHF, Direct Preference Optimization (DPO) \cite{rafailov2024direct}, under the pair-wise Bradley-Terry assumption \cite{bradley1952rank} or list-wise Plackett-Luce \cite{luce1959individual}, replaces rewards with the relative log probabilities of responses but does not provide list-wise experimental results. Methods like SLiC \cite{zhao2023slic} and RRHF \cite{yuan2024rrhf} use pair-wise hinge loss to align policy responses.
% In practice, nearly all existing preference optimization frameworks \cite{azar2024general,liu2023statistical} are limited to pair-wise preferences. Meanwhile, PRO \cite{song2024preference} introduces list maximum likelihood estimation (MLE) loss and begins to focus on preference ranking, marking a pioneering effort in list-wise preference alignment.
% LiPO \cite{liu2024lipo}, which is most similar to our approach, directly optimizes list-based ranking preferences and starts to focus on response labels but has not yet addressed the combination of multiple labels. However, these supervised learning methods rely on human or AI annotations of preference pairs or lists to specify the best responses that need alignment, which is not cost-effective. Curry-DPO simulates curriculum learning by sequentially ranking during training, using multiple preference pairs, but primarily focuses on pair-wise preference optimization.