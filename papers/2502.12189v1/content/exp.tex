
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[t]
\setlength{\tabcolsep}{3pt}
\centering
\renewcommand{\arraystretch}{1.1}
\tabcolsep=0.2cm
\begin{adjustbox}{max width=\textwidth}  % Set the maximum width to text width
\begin{tabular}{c| cccc ||  c| cc cc}
\toprule
General & \multicolumn{3}{c}{Preference} & Accuracy & Supervised & \multicolumn{3}{c}{Preference} & Accuracy \\ 
LLMs & PrefHit & PrefRecall & Reward & BLEU & Alignment & PrefHit & PrefRecall & Reward & BLEU \\ 
\midrule
GPT-J & 0.2572 & 0.6268 & 0.2410 & 0.0923 & Llama2-7B & 0.2029 & 0.803 & 0.0933 & 0.0947 \\
Pythia-2.8B & 0.3370 & 0.6449 & 0.1716 & 0.1355 & SFT & 0.2428 & 0.8125 & 0.1738 & 0.1364 \\
Qwen2-7B & 0.2790 & 0.8179 & 0.1593 & 0.2530 & Slic & 0.2464 & 0.6171 & 0.1700 & 0.1400 \\
Qwen2-57B & 0.3086 & 0.6481 & 0.6854 & 0.2568 & RRHF & 0.3297 & 0.8234 & 0.2263 & 0.1504 \\
Qwen2-72B & 0.3212 & 0.5555 & 0.6901 & 0.2286 & DPO-BT & 0.2500 & 0.8125 & 0.1728 & 0.1363 \\ 
StarCoder2-15B & 0.2464 & 0.6292 & 0.2962 & 0.1159 & DPO-PT & 0.2572 & 0.8067 & 0.1700 & 0.1348 \\
ChatGLM4-9B & 0.2246 & 0.6099 & 0.1686 & 0.1529 & PRO & 0.3025 & 0.6605 & 0.1802 & 0.1197 \\ 
Llama3-8B & 0.2826 & 0.6425 & 0.2458 & 0.1723 & \textbf{\shortname}* & \textbf{0.3659} & \textbf{0.8279} & \textbf{0.2301} & \textbf{0.1412} \\ 
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Main results on the StaCoCoQA. The left shows the performance of general LLMs, while the right presents the performance of the fine-tuned LLaMA2-7B across various strong benchmarks for preference alignment. Our method SeAdpra is highlighted in \textbf{bold}.}
\label{main}
\vspace{-0.2cm}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.02}
% \tabcolsep=0.1cm
\begin{adjustbox}{width=0.48\textwidth} % Adjust table width
\begin{tabularx}{0.495\textwidth}{p{1.2cm} p{0.7cm} p{0.95cm}p{0.95cm}p{0.7cm}p{0.7cm}}
     \toprule
    \multirow{2}{*}{\small \textbf{Dataset}} & \multirow{2}{*}{\small Model} & \multicolumn{2}{c}{\small Preference} & \multicolumn{2}{c}{\small Acc } \\ 
    & & \small \textit{PrefHit} & \small \textit{PrefRec} & \small \textit{Reward} & \small \textit{Rouge} \\ 
    \midrule
    \multirow{2}{*}{\small \textbf{Academia}}   & \small PRO & 33.78 & 59.56 & 69.94 & 9.84 \\ 
                                & \small \textbf{Ours} & 36.44 & 60.89 & 70.17 & 10.69 \\ 
    \midrule
    \multirow{2}{*}{\small \textbf{Chemistry}}  & \small PRO & 36.31 & 63.39 & 69.15 & 11.16 \\ 
                                & \small \textbf{Ours} & 38.69 & 64.68 & 69.31 & 12.27 \\ 
    \midrule
    \multirow{2}{*}{\small \textbf{Cooking}}    & \small PRO & 35.29 & 58.32 & 69.87 & 12.13 \\ 
                                & \small \textbf{Ours} & 38.50 & 60.01 & 69.93 & 13.73 \\ 
    \midrule
    \multirow{2}{*}{\small \textbf{Math}}       & \small PRO & 30.00 & 56.50 & 69.06 & 13.50 \\ 
                                & \small \textbf{Ours} & 32.00 & 58.54 & 69.21 & 14.45 \\ 
    \midrule
    \multirow{2}{*}{\small \textbf{Music}}      & \small PRO & 34.33 & 60.22 & 70.29 & 13.05 \\ 
                                & \small \textbf{Ours} & 37.00 & 60.61 & 70.84 & 13.82 \\ 
    \midrule
    \multirow{2}{*}{\small \textbf{Politics}}   & \small PRO & 41.77 & 66.10 & 69.52 & 9.31 \\ 
                                & \small \textbf{Ours} & 42.19 & 66.03 & 69.74 & 9.38 \\ 
    \midrule
    \multirow{2}{*}{\small \textbf{Code}} & \small PRO & 26.00 & 51.13 & 69.17 & 12.44 \\ 
                                & \small \textbf{Ours} & 27.00 & 51.77 & 69.46 & 13.33 \\ 
    \midrule
    \multirow{2}{*}{\small \textbf{Security}}   & \small PRO & 23.62 & 49.23 & 70.13 & 10.63 \\ 
                                & \small \textbf{Ours} & 25.20 & 49.24 & 70.92 & 10.98 \\ 
    \midrule
    \multirow{2}{*}{\small \textbf{Mean}}       & \small PRO & 32.64 & 58.05 & 69.64 & 11.51 \\ 
                                & \small \textbf{Ours} & \textbf{34.25} & \textbf{58.98} & \textbf{69.88} & \textbf{12.33} \\ 
    \bottomrule
\end{tabularx}
\end{adjustbox}
\caption{Main results (\%) on eight publicly available and popular CoQA datasets, comparing the strong list-wise benchmark PRO and \textbf{ours with bold}.}
\label{public}
\end{table}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.02}
\begin{tabularx}{0.48\textwidth}{p{1.45cm} p{0.56cm} p{0.6cm} p{0.6cm} p{0.50cm} p{0.45cm} X}
\toprule
\multirow{2}{*}{Method} & \multicolumn{3}{c}{Preference \((\uparrow)\)} & \multicolumn{3}{c}{Accuracy \((\uparrow)\)} \\ \cmidrule{2-4} \cmidrule{5-7}
& \small PrefHit & \small PrefRec & \small Reward & \small CoSim & \small BLEU & \small Rouge \\ \midrule
\small{SeAdpra} & \textbf{34.8} & \textbf{82.5} & \textbf{22.3} & \textbf{69.1} & \textbf{17.4} & \textbf{21.8} \\ 
\small{-w/o PerAl} & \underline{30.4} & 83.0 & 18.7 & 68.8 & \underline{12.6} & 21.0 \\
\small{-w/o PerCo} & 32.6 & 82.3 & \underline{24.2} & 69.3 & 16.4 & 21.0 \\
\small{-w/o \(\Delta_{Se}\)} & 31.2 & 82.8 & 18.6 & 68.3 & \underline{12.4} & 20.9 \\
\small{-w/o \(\Delta_{Po}\)} & \underline{29.4} & 82.2 & 22.1 & 69.0 & 16.6 & 21.4 \\
\small{\(PerCo_{Se}\)} & 30.9 & 83.5 & 15.6 & 67.6 & \underline{9.9} & 19.6 \\
\small{\(PerCo_{Po}\)} & \underline{30.3} & 82.7 & 20.5 & 68.9 & 14.4 & 20.1 \\ 
\bottomrule
\end{tabularx}
\caption{Ablation Results (\%). \(PerCo_{Se}\) or \(PerCo_{Po}\) only employs Single-APDF in Perceptual Comparison, replacing \(\Delta_{M}\) with \(\Delta_{Se}\) or \(\Delta_{Po}\). The bold represents the overall effect. The underlining highlights the most significant metric for each component's impact.}
\label{ablation}
% \vspace{-0.2cm}
\end{table}

\subsection{Dataset}

% These CoQA datasets contain questions and answers from the Stack Overflow data dump\footnote{https://archive.org/details/stackexchange}, intended for training preference models. 

Due to the additional challenges that programming QA presents for LLMs and the lack of high-quality, authentic multi-answer code preference datasets, we turned to StackExchange \footnote{https://archive.org/details/stackexchange}, a platform with forums that are accompanied by rich question-answering metadata. Based on this, we constructed a large-scale programming QA dataset in real-time (as of May 2024), called StaCoCoQA. It contains over 60,738 programming directories, as shown in Table~\ref{tab:stacocoqa_tags}, and 9,978,474 entries, with partial data statistics displayed in Figure~\ref{fig:dataset}. The data format of StaCoCoQA is presented in Table~\ref{fig::stacocoqa}.

The initial dataset \(D_I\) contains 24,101,803 entries, and is processed by the following steps:
(1) Select entries with "Questioner-picked answer" pairs to represent the preferences of the questioners, resulting in 12,260,106 entries in the \(D_Q\).
(2) Select data where the question includes at least one code block to focus on specific-domain programming QA, resulting in 9,978,474 entries in the dataset \(D_C\).
(3) All HTML tags were cleaned using BeautifulSoup \footnote{https://beautiful-soup-4.readthedocs.io/en/latest/} to ensure that the model is not affected by overly complex and meaningless content.
(4) Control the quality of the dataset by considering factors such as the time the question was posted, the size of the response pool, the difference between the highest and lowest votes within a pool, the votes for each response, the token-level length of the question and the answers, which yields varying sizes: 3K, 8K, 18K, 29K, and 64K. 
The controlled creation time variable and the data details after each processing step are shown in Table~\ref{tab:statistics}.

To further validate the effectiveness of SeAdpra, we also select eight popular topic CoQA datasets\footnote{https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences}, which have been filtered to meet specific criteria for preference models \cite{askell2021general}. Their detailed data information is provided in Table~\ref{domain}.
% Examples of some control variables are shown in Table~\ref{tab:statistics}.
% \noindent\textbf{Baselines}. 
% Following the DPO \cite{rafailov2024direct}, we evaluated several existing approaches aligned with human preference, including GPT-J \cite{gpt-j} and Pythia-2.8B \cite{biderman2023pythia}.  
% Next, we assessed StarCoder2 \cite{lozhkov2024starcoder}, which has demonstrated strong performance in code generation, alongside several general-purpose LLMs: Qwen2 \cite{qwen2}, ChatGLM4 \cite{wang2023cogvlm, glm2024chatglm} and LLaMA serials \cite{touvron2023llama,llama3modelcard}.
% Finally, we fine-tuned LLaMA2-7B on the StaCoCoQA and compared its performance with other strong baselines for supervised learning in preference alignment, including SFT, RRHF \cite{yuan2024rrhf}, Silc \cite{zhao2023slic}, DPO, and PRO \cite{song2024preference}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% For preference evaluation, traditional win-rate assessments are costly and not scalable. For instance, when an existing model \(M_A\) is evaluated against comparison methods \((M_B, M_C, M_D)\) in terms of win rates, upgrading model \(M_A\) would necessitate a reevaluation of its win rates against other models. Furthermore, if a new comparison method \(M_E\) is introduced, the win rates of model \(M_A\) against \(M_E\) would also need to be reassessed. Whether AI or humans are employed as evaluation mediators, binary preference between preferred and non-preferred choices or to score the inference results of the modified model, the costs of this process are substantial. 
% Therefore, from an economic perspective, we propose a novel list preference evaluation method. We utilize manually ranking results as the gold standard for assessing human preferences, to calculate the Hit and Recall, referred to as PrefHit and PrefRecall, respectively. Regardless of whether improving model \(M_A\) or expanding comparison method \(M_E\), only the calculation of PrefHit and PrefRecall for the modified model is required, eliminating the need for human evaluation. 
% We also employ a professional reward model\footnote{https://huggingface.co/OpenAssistant/reward-model-deberta-v3-large}
% for evaluation, denoted as the Reward metric.

% \subsection{Baseline} 
% Following the DPO \cite{rafailov2024direct}, we evaluated several existing approaches aligned with human preference, including GPT-J \cite{gpt-j} and Pythia-2.8B \cite{biderman2023pythia}.  
% Next, we assessed StarCoder2 \cite{lozhkov2024starcoder}, which has demonstrated strong performance in code generation, alongside several general-purpose LLMs: Qwen2 \cite{qwen2}, ChatGLM4 \cite{wang2023cogvlm, glm2024chatglm} and LLaMA serials \cite{touvron2023llama,llama3modelcard}.
% Finally, we fine-tuned LLaMA2-7B on the StaCoCoQA and compared its performance with other strong baselines for supervised learning in preference alignment, including SFT, RRHF \cite{yuan2024rrhf}, Silc \cite{zhao2023slic}, DPO, and PRO \cite{song2024preference}.
\subsection{Evaluation Metrics}
\label{sec: metric}
For preference evaluation, we design PrefHit and PrefRecall, adhering to the "CSTC" criterion outlined in Appendix \ref{sec::cstc}, which overcome the limitations of existing evaluation methods, as detailed in Appendix \ref{metric::mot}.
In addition, we demonstrate the effectiveness of thees new evaluation from two main aspects: 1) consistency with traditional metrics, and 2) applicability in different application scenarios in Appendix \ref{metric::ana}.
Following the previous \cite{song2024preference}, we also employ a professional reward.
% Following the previous \cite{song2024preference}, we also employ a professional reward model\footnote{https://huggingface.co/OpenAssistant/reward-model-deberta-v3-large} \cite{song2024preference}, denoted as the Reward.

For accuracy evaluation, we alternately employ BLEU \cite{papineni2002bleu}, RougeL \cite{lin2004rouge}, and CoSim. Similar to codebertscore \cite{zhou2023codebertscore}, CoSim not only focuses on the semantics of the code but also considers structural matching.
Additionally, the implementation details of SeAdpra are described in detail in the Appendix \ref{sec::imp}.
\subsection{Main Results}
We compared the performance of \shortname with general LLMs and strong preference alignment benchmarks on the StaCoCoQA dataset, as shown in Table~\ref{main}. Additionally, we compared SeAdpra with the strongly supervised alignment model PRO \cite{song2024preference} on eight publicly available CoQA datasets, as presented in Table~\ref{public} and Figure~\ref{fig::public}.

\textbf{Larger Model Parameters, Higher Preference.}
Firstly, the Qwen2 series has adopted DPO \cite{rafailov2024direct} in post-training, resulting in a significant enhancement in Reward.
In a horizontal comparison, the performance of Qwen2-7B and LLaMA2-7B in terms of PrefHit is comparable.
Gradually increasing the parameter size of Qwen2 \cite{qwen2} and LLaMA leads to higher PrefHit and Reward.
Additionally, general LLMs continue to demonstrate strong capabilities of programming understanding and generation preference datasets, contributing to high BLEU scores.
These findings indicate that increasing parameter size can significantly improve alignment.

\textbf{List-wise Ranking Outperforms Pair-wise Comparison.}
Intuitively, list-wise DPO-PT surpasses pair-wise DPO-{BT} on PrefHit. Other list-wise methods, such as RRHF, PRO, and our \shortname, also undoubtedly surpass the pair-wise Slic.

\textbf{Both Parameter Size and Alignment Strategies are Effective.}
Compared to other models, Pythia-2.8B achieved impressive results with significantly fewer parameters .
Effective alignment strategies can balance the performance differences brought by parameter size. For example, LLaMA2-7B with PRO achieves results close to Qwen2-57B in PrefHit. Moreover, LLaMA2-7B combined with our method SeAdpra has already far exceeded the PrefHit of Qwen2-57B.

\textbf{Rather not Higher Reward, Higher PrefHit.}
It is evident that Reward and PrefHit are not always positively correlated, indicating that models do not always accurately learn human preferences and cannot fully replace real human evaluation. Therefore, relying solely on a single public reward model is not sufficiently comprehensive when assessing preference alignment.

% In conclusion, during ensuring precise alignment, SeAdpra will focuse on PrefHit@1, even though the trade-off between PrefHit and PrefRecall is a common issue and increasing recall may sometimes lead to a decrease in hit rate. The positive correlation between Reward and BLEU, indicates that improving the quality of the generated text typically enhances the Reward. 
% Most importantly, evaluating preferences solely based on reward is clearly insufficient, as a high reward does not necessarily correspond to a high PrefHit or PrefRecall.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%
\begin{figure}
  \centering
  \begin{subfigure}{0.49\linewidth}
    \includegraphics[width=\linewidth]{latex/pic/hit.png}
    \caption{The PrefHit}
    \label{scale:hit}
  \end{subfigure}
  \begin{subfigure}{0.49\linewidth}
    \includegraphics[width=\linewidth]{latex/pic/Recall.png}
    \caption{The PrefRecall}
    \label{scale:recall}
  \end{subfigure}
  \medskip
  \begin{subfigure}{0.48\linewidth}
    \includegraphics[width=\linewidth]{latex/pic/reward.png}
    \caption{The Reward}
    \label{scale:reward}
  \end{subfigure}
  \begin{subfigure}{0.48\linewidth}
    \includegraphics[width=\linewidth]{latex/pic/bleu.png}
    \caption{The BLEU}
    \label{scale:bleu}
  \end{subfigure}
  \caption{The performance with Confidence Interval (CI) of our SeAdpra and PRO at different data scales.}
  \label{fig:scale}
  % \vspace{-0.2cm}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Ablation Study}

In this section, we discuss the effectiveness of each component of SeAdpra and its impact on various metrics. The results are presented in Table \ref{ablation}.

\textbf{Perceptual Comparison} aims to prevent the model from relying solely on linguistic probability ordering while neglecting the significance of APDF. Removing this Reward will significantly increase the margin, but PrefHit will decrease, which may hinder the model's ability to compare and learn the preference differences between responses.

\textbf{Perceptual Alignment} seeks to align with the optimal responses; removing it will lead to a significant decrease in PrefHit, while the Reward and accuracy metrics like CoSim will significantly increase, as it tends to favor preference over accuracy.

\textbf{Semantic Perceptual Distance} plays a crucial role in maintaining semantic accuracy in alignment learning. Removing it leads to a significant decrease in BLEU and Rouge. Since sacrificing accuracy recalls more possibilities, PrefHit decreases while PrefRecall increases. Moreover, eliminating both Semantic Perceptual Distance and Perceptual Alignment in \(PerCo_{Po}\) further increases PrefRecall, while the other metrics decline again, consistent with previous observations.


\textbf{Popularity Perceptual Distance} is most closely associated with PrefHit. Eliminating it causes PrefHit to drop to its lowest value, indicating that the popularity attribute is an extremely important factor in code communities.

% In summary, each module has a varying impact on preference and accuracy, but all outperform their respective foundation models and other baselines, as shown in Table \ref{main}, proving their effectiveness.


\subsection{Analysis and Discussion}

\textbf{SeAdpra adept at high-quality data rather than large-scale data.}
In StaCoCoQA, we tested PRO and SeAdpra across different data scales, and the results are shown in Figure~\ref{fig:scale}.
Since we rely on the popularity and clarity of questions and answers to filter data, a larger data scale often results in more pronounced deterioration in data quality. In Figure~\ref{scale:hit}, SeAdpra is highly sensitive to data quality in PrefHit, whereas PRO demonstrates improved performance with larger-scale data. Their performance on Prefrecall is consistent. In the native reward model of PRO, as depicted in Figure~\ref{scale:reward}, the reward fluctuations are minimal, while SeAdpra shows remarkable improvement.

\textbf{SeAdpra is relatively insensitive to ranking length.} 
We assessed SeAdpra's performance on different ranking lengths, as shown in Figure 6a. Unlike PRO, which varied with increasing ranking length, SeAdpra shows no significant differences across different lengths. There is a slight increase in performance on PrefHit and PrefRecall. Additionally, SeAdpra performs better at odd lengths compared to even lengths, which is an interesting phenomenon warranting further investigation.


\textbf{Balance Preference and Accuracy.} 
We analyzed the effect of control weights for Perceptual Comparisons in the optimization objective on preference and accuracy, with the findings presented in Figure~\ref{para:weight}.
When \( \alpha \) is greater than 0.05, the trends in PrefHit and BLEU are consistent, indicating that preference and accuracy can be optimized in tandem. However, when \( \alpha \) is 0.01, PrefHit is highest, but BLEU drops sharply.
Additionally, as \( \alpha \) changes, the variations in PrefHit and Reward, which are related to preference, are consistent with each other, reflecting their unified relationship in the optimization. Similarly, the variations in Recall and BLEU, which are related to accuracy, are also consistent, indicating a strong correlation between generation quality and comprehensiveness. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
  \centering
  \begin{subfigure}{0.475\linewidth}
    \includegraphics[width=\linewidth]{latex/pic/Rank1.png}
    \caption{Ranking length}
    \label{para:rank}
  \end{subfigure}
  \begin{subfigure}{0.475\linewidth}
    \includegraphics[width=\linewidth]{latex/pic/weights1.png}
    \caption{The \(\alpha\) in \(Loss\)}
    \label{para:weight}
  \end{subfigure}
  \caption{Parameters Analysis. Results of experiments on different ranking lengths and the weight \(\alpha\) in \(Loss\).}
  \label{fig:para}
  % \vspace{-0.2cm}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}
  \centering
  \begin{subfigure}{0.305\linewidth}
    \includegraphics[width=\linewidth]{latex/pic/se2.pdf}
    \caption{The \(\Delta_{Se}\)}
    \label{visual:se}
  \end{subfigure}
  \begin{subfigure}{0.305\linewidth}
    \includegraphics[width=\linewidth]{latex/pic/po2.pdf}
    \caption{The \(\Delta_{Po}\)}
    \label{visual:po}
  \end{subfigure}
  \begin{subfigure}{0.305\linewidth}
    \includegraphics[width=\linewidth]{latex/pic/sv2.pdf}
    \caption{The \(\Delta_{M}\)}
    \label{visual:sv}
  \end{subfigure}
  \caption{The Visualization of Attribute-Perceptual Distance Factors (APDF) matrix of five responses. The blue represents the response with the highest APDF, and SeAdpra aligns with the fifth response corresponding to the maximum Multi-APDF in (c). The green represents the second response that is next best to the red one.}
  \label{visual}
  % \vspace{-0.2cm}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textbf{Single-APDF Matrix Cannot Predict the Optimal Response.} We randomly selected a pair with a golden label and visualized its specific iteration in Figure~\ref{visual}.
It can be observed that the optimal response in a Single-APDF matrix is not necessarily the same as that in the Multi-APDF matrix.
Specifically, the optimal response in the Semantic Perceptual Factor matrix \(\Delta_{Se}\) is the fifth response in Figure~\ref{visual:se}, while in the Popularity Perceptual Factor matrix \(\Delta_{Po}\) (Figure~\ref{visual:po}), it is the third response. Ultimately, in the Multiple Perceptual Distance Factor matrix \(\Delta_{M}\), the third response is slightly inferior to the fifth response (0.037 vs. 0.038) in Figure~\ref{visual:sv}, and this result aligns with the golden label.
More key findings regarding the ADPF are described in Figure \ref{fig::hot1} and Figure \ref{fig::hot2}.