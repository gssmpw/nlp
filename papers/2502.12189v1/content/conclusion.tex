% In this paper, we propose \shortname, aiming to better align LLMs with community question answering. 
% By introducing Attribute-Perceptual Distance Factors (APDF), SeAdpra precisely quantifies preference differences between multiple responses, enabling label-free self-supervised dynamic ranking. 
% Based on the ranking results, \shortname \ performs multiple rounds of preference comparison to achieve end-to-end preference difference learning.
% To validate the effectiveness of \shortname, we also construct a challenging programming-domain CoQA preference dataset, StaCoCoQA, and conduct extensive experiments on both public datasets and StaCoCoQA. The experimental results demonstrate that SeAdpra outperforms general LLMs and supervised alignment baselines while maintaining safety. Furthermore, we explore the impact of various factors on SeAdpra's performance. Overall, we provide a new perspective for aligning LLMs with multi-factor human preferences.


In this paper, we propose SeAdpra by introducing the Attribute-Aware Preference Distance Factor (APDF), SeAdpra precisely quantifies preference differences among multiple responses, enabling label-free self-supervised dynamic ranking. Based on the ranking results, SeAdpra performs multiple rounds of preference comparison to achieve better alignment between LLMs and community question answering.
To validate the effectiveness of SeAdpra, we introduce cost-effective, scalable, transferable, and consistent evaluation metrics, PrefHit and PrefRecall. Additionally, we construct a challenging programming-oriented CoQA preference dataset, StaCoCoQA. Extensive experimental results on public datasets and StaCoCoQA demonstrate that SeAdpra outperforms general LLMs and supervised alignment baselines while maintaining safety.
Furthermore, we explore the impact of various factors on SeAdpraâ€™s performance. Overall, our work offers a novel perspective on aligning LLMs with multifactorial human preferences.