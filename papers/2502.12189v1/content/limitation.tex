The domain adaptability of SeAdpra relies to some extent on predefined attributes, requiring manual adaptation of the attribute system, which bears similarities to the domain transfer bottlenecks observed in rule-based reward models.
In fine-grained preference alignment, the model may face a "preference-generalization" trade-off, where over-optimizing for specific preferences could weaken its general generation ability, a common issue in post-training stages like instruction fine-tuning and reward modeling.
At this stage, we focus on preference and accuracy, without evaluating the coherence and factual correctness of responses. In the future, we will work towards addressing these issues.