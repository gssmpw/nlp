% To explore the impact of enhanced preference alignment on the original level of safety, we conducted additional preference alignment experiments on the safety alignment dataset PKU-SafeRLHF \cite{ji2024beavertails,ji2024pku}, as shown in Figure~\ref{fig::pkusafe}.
% Furthermore, to avoid biases introduced by inconsistencies between the preference alignment and safety alignment objectives, as well as malicious data, we selected a benign dataset where the preference alignment and safety alignment objectives are consistent for training. 
% These data are considered absolutely safe. 
% To eliminate the interference of reasoning length on the evaluation, we conducted experiments using Llama2-7B with varying reasoning lengths. The results are presented in Table~\ref{tab:safe64} and Table~\ref{tab:safe32} and other details are in Appendix \ref{sec::safety}. We have two key findings:
To explore the impact of enhanced preference on the original safety, we conducted additional preference alignment experiments on the absolutely benign data from the safety alignment dataset PKU-SafeRLHF \cite{ji2024beavertails,ji2024pku}, as shown in Figure~\ref{fig::pkusafe}.
The results are presented in Table~\ref{tab:safe64} and Table~\ref{tab:safe32} and other details are described in Appendix \ref{sec::safety}. 

\textbf{PrefHit and PrefRecall can be transferred to other attribute alignments, such as safety alignment.}
As long as there is a preference order on a certain attribute, such as the \(safer\_response\_id\) in Figure \ref{fig::pkusafe}, PrefHit and PrefRecall can be transferred to evaluate the alignment of the corresponding attribute, such as SaferHit and SaferRecall. 
Since the safety alignment dataset PKU-SafeRLHF only has two candidate responses, SaferHit is equal to SaferRecall, so we only present SaferHit in the Table \ref{tab:safe64} and Table \ref{tab:safe32}.

\textbf{Safety is positively correlated with preference.} No matter the preference alignment strategy, the toxicity decreases significantly as PrefHit increases, ultimately stabilizing at a negligible level of 0.006. SaferHit represents a preference for safer responses, evaluating both safety and preference. It is positively correlated with PrefHit and negatively correlated with toxicity.

% \noindent\textbf{SeAdpra excels with high-quality data rather than large-scale data.}
% We examined the performance of PRO and SeAdpra across various data scales in StaCoCoQA, as illustrated in Figure~\ref{fig:scale}.
% Since we rely on the popularity and clarity of questions and answers to filter data, a larger data scale often results in more pronounced deterioration in data quality. As shown in Figure~\ref{scale:hit}, SeAdpra is highly sensitive to data quality in PrefHit, whereas PRO demonstrates improved performance with larger-scale data. Their performance on recall is consistent. In the native reward model of PRO, as depicted in Figure~\ref{scale:reward}, the reward fluctuations are minimal, while SeAdpra shows remarkable improvement.

% \noindent\textbf{SeAdpra is relatively insensitive to ranking length.} 
% We assessed \shortname's performance across different ranking lengths, as depicted in Figure~\ref{para:rank}.
% Unlike the performance of PRO, which varies with increasing ranking length, SeAdpra shows no significant differences across different lengths. There is a slight increase in performance on PrefHit and PrefRecall. Additionally, SeAdpra performs better at odd lengths compared to even lengths, which is an interesting phenomenon warranting further investigation.

% \noindent\textbf{Balance Preference and Accuracy.} 
% We analyzed the effect of control weights for Perceptual Comparisons in the optimization objective \( \mathcal{L}_{Muapdf} \) on preference and accuracy, with the findings presented in Figure~\ref{para:weight}.
% When \( \alpha \) is greater than 0.05, the trends in PrefHit and BLEU are consistent, indicating that preference and accuracy can be optimized in tandem. However, when \( \alpha \) is 0.01, PrefHit is highest, but this leads to a drastic drop in BLEU.
% Additionally, as \( \alpha \) changes, the variations in PrefHit and Reward, which are related to preference, are consistent with each other, reflecting their direct relationship in the optimization objective. Similarly, the variations in Recall and BLEU, which are related to accuracy, are consistent, reflecting the intrinsic connection between generation quality and comprehensiveness. 

% \noindent\textbf{Single-APDF Matrix Cannot Predict the Optimal Response.} We randomly selected a pair with a golden label and visualized its specific iteration in Figure~\ref{visual}.
% It can be observed that the optimal response in a single-attribute perceptual factor matrix is not necessarily the same as that in the Multiple Perceptual Factor matrix.
% Specifically, the optimal response in the Semantic Perceptual Factor matrix \(\Delta_{Se}\) is the fifth response in Figure~\ref{visual:se}, while in the Popularity Perceptual Factor matrix \(\Delta_{Po}\) (Figure~\ref{visual:po}), it is the third response. Ultimately, in the Multiple Perceptual Distance Factor matrix \(\Delta_{MuAPDF}\), the third response is slightly inferior to the fifth response (0.037 vs. 0.038) in Figure~\ref{visual:sv}, and this result aligns with the golden label.

% \noindent\textbf{SeAdpra does not compromise the original safety.}
% To evaluate changes in model safety while improving the degree of preference alignment, we conducted safety assessment experiments on a safety alignment dataset using the base model LLaMA, the state-of-the-art preference ranking model PRO, and our SeAdpra. Detailed results are provided in Appendix \ref{}. From Table in Appendix \ref{}

