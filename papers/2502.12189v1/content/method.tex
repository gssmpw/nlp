% \begin{figure*}
%     \centering
%     \includegraphics[width=\linewidth]{latex/pic/framework1.pdf}
%     \caption{}
%     \label{intro}
% \end{figure*}
\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{latex/pic/single_framework1_new.pdf}
    \caption{The overall framework of \shortname \
    , which includes: (Part1.) Multi-attribute Perception for quantifying preference, containing the Construction of Multi-APDF Matrix and Self-supervised dynamic ranking; (Part2.) Perceptual Alignment for aligning the optimal ranks objective; (Part3.) Perceptual Comparison on all candidates for learning on-chain preference difference.}
    \label{overall}
    \vspace{-0.5cm}
\end{figure*}
\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{latex/pic/PC.pdf}
    \caption{Implementation Workflow of Perceptual Comparison. In each round, the reward of the current positive is maximized, and the penalty for the remaining negative is minimized sequentially.}
    \vspace{-0.2cm}
\end{figure}
\subsection{Problem Definition}
Our goal is to align an LLM with user preferences in CoQA using our Unsupervised Attribute-aware Dynamic Preference Ranking strategy.
The training dataset is denoted as \(\mathcal{D} = \{Q^{i}, R^{i}\}_{i=1}^{N}\). For a given question \(Q\), it corresponds to a series of responses \(R = \{R_1, \ldots, R_M\}\), where each response \(R_i = (C, A)\), with \(C\) representing the content and \(A\) representing the scalable attributes. The size \(L\) of the scalable attribute \(A = \{A_1, \ldots, A_L\}\) is determined by community characteristics. For example, in the code community, \(L=3\) and \(A = \{S, P, T\}\).
Here, \(S\) represents the semantic similarity between \(C\) and \(Q\); \(P\) represents the popularity of \(R\), and \(T\) represents the creation time of each response.
% It is worth noting that for any \(i > j\), it is not necessarily true that \(R_i \succ R_j\), where \(\mathcal{\succ}\) denotes the degree of preference.
\subsection{Multi-attribute Perception}
% In this part, we will sequentially introduce the (1) Attribute-Perceptual Distance Factor (APDF), (2) the Construction of the Multi-APDF Matrix, and (3) the Self-supervision Dynamic Ranking.
% In this part, we sequentially introduce (1) Multi-attribute Perception, including the introduction of Attribute Perceptual Distance Factors for quantifying preference levels and the unsupervised dynamic ranking algorithm described in Algorithm~\ref{algo1}, (2) Perceptual Comparison for learning on-chain preference differences, and (3) Perceptual Alignment for aligning the best responses.

\subsubsection{Attribute-Perceptual Distance Factor}
The existing alignment optimization objectives \cite{rafailov2024direct, song2024preference} do not take into account the attributes of the candidates, which can differentiate their preferences. 
Therefore, there is a need to explore optimization methods that can effectively incorporate these attributes.
In this context, LambdARank \cite{burges2005learning, donmez2009local, wang2018lambdaloss, jagerman2022optimizing} introduces Lambda weights \(\lambda_{ij}\), which scale the gradient of each pair of scores based on the labels of the pairs to optimize a metric-driven loss function and effectively incorporating label information into the optimization process.

Inspired by the \(\lambda_{ij}\), the Attribute-Perceptual Distance Factor \(\delta_{i,j}\) is designed to quantify the preference difference between two candidates \(i\) and \(j\) in the optimization objective. It not only considers the positional relationship of candidates in preference ranks but also incorporates their label values through the gain function, and expressed as:
\begin{equation}
    \delta_{i,j} = (G(i) - G(j)) \cdot \left( T(i) - T(j) \right) 
\end{equation}
\begin{equation}
    T(i) = 1/log(l_i+1)
\end{equation}
where \(l_i\) and \(l_j\) are the ranking positions of response \(i\) and \(j\), respectively. 
The gain function \(G(\cdot)\) varies with different intrinsic attributes.
% Since the existing alignment optimization objectives\cite{rafailov2024direct,song2024preference} do not consider the intrinsic attributes of the candidates, and these intrinsic attributes can differentiate the preference levels of candidates, we have explored a variety of optimization objectives and metrics in ranking studies. Among them, LambdARank \cite{burges2005learning, donmez2009local, wang2018lambdaloss, jagerman2022optimizing} introduces Lambda weights \(\lambda_{ij}\), which scale the gradient of each pair of scores based on the label values of the pairs to optimize a metric-driven loss function. In fact, \(\lambda_{ij}\) benefits from Normalized Discounted Cumulative Gain (NDCG) \cite{yue2007using}, which evaluates ranking performance by assessing the positional relationship between two candidates \(i\) and \(j\), and by considering their label values through the gain function. Therefore, we use them as the basis for the Attribute-Perceptual Distance Factor \(\delta_{i,j}\) in the optimization objective, which is represented as follows:
% Considering the intrinsic attributes of responses to differentiate their degree of preference, we have explored the rich optimization objectives and metrics in ranking studies. Among them, Normalized Discounted Cumulative Gain (NDCG) \cite{yue2007using} evaluates ranking performance not only by assessing the positional relationship between two candidates \(i\) and \(j\), but also by considering their label values through the gain function. 
% Therefore, we use it as the basis for the Attribute-Perceptual Distance Factor \(\delta_{i,j}\).

% Additionally, point-wise or pair-wise ranking methods cannot directly optimize ranking metrics such as NDCG because their loss functions are typically smooth convex functions that are easy to optimize but do not effectively reflect ranking quality. To address this, LambdARank \cite{burges2005learning, donmez2009local, wang2018lambdaloss, jagerman2022optimizing} introduced Lambda weights \(\lambda_{ij}\), which scale the gradient of each pair of scores based on the labels of pairs to optimize a metric-driven loss function. Inspired by this, the final Attribute-Perceptual Distance Factor \(\delta_{i,j}\) is represented as follows:

% \begin{equation}
%     \delta_{i,j} = (G(i) - G(j)) \cdot \left( \frac{1}{\log(1 + l_i)} - \frac{1}{\log(1 + l_j)} \right)
% \end{equation}


\subsubsection{Construction of the Multi-APDF Matrix}
Given the response \(R = \{R_1, \ldots, R_M\}\) to question \(Q\), the construction of the Multi-APDF matrix is a dot-product fusion of \(L\) Single-APDF matrix.
Based on the characteristics of the code community shown in Figure \ref{intro}, the main attributes that influence user preferences are semantics (text content), popularity, and creation time. 
% Additionally, the following phenomena occur: popularity accumulates over time, and there may be outdated information in responses due to updates and iterations of programming libraries.

\textbf{Semantic-APDF matrix} \(\Delta_{Se} = \{\delta_{Se_{ij}} | i,j \in M\}\), we define \(G^{Se}(i) = 2^{\varphi(i) - 1}\), where \(\varphi(i) = \cos(E_{Q}, E_{C_i})\). Here, \(E_{Q} \in \mathbb{R}^{q \times d}\) and \(E_{C_i} \in \mathbb{R}^{r \times d}\) represent the semantic vectors of the question \(Q\) and the text content \(C_i\) of response \(R_i\), encoded by prompt-based LLMs \cite{behnamghader2024llm2vec}. Here, \(q\) is the length of the question, \(r\) is the length of the text content, and \(d\) is the dimension of the embedding space.

\textbf{Popularity-APDF matrix} \(\Delta_{Po} = \{\delta_{Po_{ij}} | i,j \in M\}\)
To mitigate the bias caused by the accumulation of popularity over time, we apply time decay to \(P\) based on \(T\), denoted as \(\tilde{P}\). 
To avoid bias caused by extreme values and excessive numerical differences, we set \(G^{Po}(i) = \lg(\tilde{P_i} + 1)\).

\textbf{Multi-APDF matrix} on the scalable attribute \( A = \{ A_1, \ldots, A_L \} \) is represented generally as:
\begin{equation}
  \label{muapdf}
    \Delta_{M} =  {\textstyle \prod_{k=1}^{L}}  \Delta_{A_k}
\end{equation}
where \( \Delta_{A_k} \) is the APDF matrix corresponding to attribute \( A_k \). Similarly, The code Multi-APDF matrix \(\Delta_{M}^{code} \in \mathbb{R}^{M \times M}\) is represented as follows:
\begin{equation}
    \Delta_{M}^{code} = \Delta_{Se} \cdot \Delta_{Po}
\end{equation}
% we define the Semantic Perceptual Distance Factor matrix \(\Delta_{Se} = \{\delta_{Se_{ij}} | i,j \in M\}\) and the Popularity Perceptual Distance Factor matrix \(\Delta_{Po} = \{\delta_{Po_{ij}} | i,j \in M\}\).







% \begin{equation}
%   \label{muapdf}
%     \Delta_{MuAPDF} =  \prod_{k=1}^{L} \Delta_{A_k}
% \end{equation}
% To determine the optimal response order in Perceptual Comparison, in each iteration, we sequentially identify the response corresponding to the largest \( \delta_{MuAPDF}\).

\subsubsection{Self-supervision Dynamic Ranking}
To avoid relying on manually labeled alignment targets, we propose the Self-supervised Dynamic Ranking based on the Multi-APDF Matrix.  
It iteratively selects the most significant pair-wise distance (Multi-APDF \(\delta_{M}\)) and ranks the candidates according to the semantic ranks, which ensures that the ranking not only reflects pair-wise perceptual differences but also adheres to semantic priorities.
Its implementation details are provided in the Algorithm~\ref{algo1}.
The \(D^R\) represents the set of candidates' positions after dynamic ranking:
\begin{equation}
    D^R = \{i_1, i_2, \ldots, i_M\}
\end{equation}
\subsection{Perceptual Alignment}
Since the most effective learning for domain knowledge method is SFT \cite{stiennon2020learning}, and the most direct one in alignment is also to perform SFT on a high-quality preference dataset \cite{rafailov2024direct}, 
we align the optimal response by treating the first response in dynamic ranking as the target for SFT for the question \( Q \).
The first optimization objective is represented as follows:
\begin{equation}
   L_{Pa} = - \frac{1}{|R_b|} \sum_{j=1}^{|R_b|} \log P(R_b(j) | Q, R_b(<j))
\end{equation}
where \( R_{D^R(0)}\) denotes as \( R_b \). The \( D^R(i) \) is the \( i \)-th element, and \( R_b(j) \) is the \( j \)-th token.

\subsection{Perceptual Comparison}
In terms of many list-wise loss functions, the softmax cross-entropy loss in ListNet \cite{cao2007learning} uses double summation to emphasize comparisons between different samples, making it suitable for ranking loss. Therefore, we adopt it as the basis for the second optimization objective and conduct a total of \(M-1\) iterative comparisons.
To deepen the impact of preference differences, for each iteration, we maximize the reward for positive and minimize the penalty for remains negative sequentially.

\textbf{Maximizing the reward} is achieved by finding all maximum value in the mapped row of the alignment target in all Single-APDF matrix, and then multiplying the values together. For the \(m\)-th comparison, it is represented as follows:
\begin{equation}
  W_{m}^{r}= {\textstyle \prod_{k=1}^{L}} max(\Delta_{A_k}(D^R(m),\cdot))
\end{equation}
where \(\Delta_{A_k}(i, j)\) refers to the element at the \(i\)-th row and \(j\)-th column of \(\Delta_{A_k}\), and \(\cdot\) represents all elements in the row or column.

\textbf{Minimizing the penalty} involves differentiating the penalty strengths based on preference levels, where a slight penalty is applied to \( R_{D_R(i)} \) and a stronger penalty is applied to \( R_{D_R(i+1)} \). This approach contrasts with the existing method, which applies the same penalty to all negative examples, and ensures that the penalty for responses ranked higher in the self-supervised ranking \( D_R \) is minimized.
For the negative \( R_i \), its penalty is represented as follows:
\begin{equation}
\begin{aligned}
     W_{i}^{p}  =  sort(\Delta_{M}(D^R(m),\cdot ))(i)
\end{aligned}
\end{equation}
where \(sort(\cdot)\) is the function that sorts in an ascending order. 
\(\Delta_{M}(i, j)\) is the \(i\)-th row and the \(j\)-th APDF in the Multi-APDF matrix.

To achieve on-chain ranking and fine-grained distinction among all responses, unlike traditional optimization methods that sequentially remove the optimal response, all responses participate in each iteration. Moreover, the corresponding penalties or rewards for the responses change throughout the iterations.
The second optimization objective is represented as:
\begin{equation}
   L_{Pc} = -\sum_{m=1}^{M-1} \log \left( \frac{ \tau_r(b)}{\sum_{i \ne b}^{M} \tau_p(i) + \tau_r(b)} \right)
\end{equation}
\begin{equation}
    \tau_r (b)= \exp(\pi_s(Q, R_{b})) * W_{m}^{r}
\end{equation}
\begin{equation}
    \tau_p (i)= \exp(\pi_s(Q, R_i)) * W_{i}^{p}
\end{equation}
\begin{equation}
    \pi_s(Q, R_i) = \frac{1}{t} {\textstyle \sum_{k=1}^{t}\log P(r_k|Q, r_{<k}) }
\end{equation}
Here, \(D^R(m)\) denotes as \(b\). the \(\pi_s(\cdot)\) represents a policy network that replaces the reward in RLHF with language modeling logits. The labeled response \(R\), composed of \(t\) tokens, is denoted as \(R_i = \{r_1,\ldots,r_t\}\).
Finally, \shortname\ enables LLMs to be trained by the following objective:
\begin{equation}
    Loss= L_{Pc} + \alpha \cdot L_{Pa}
    \label{eq::final}
\end{equation}
To avoid overfitting the initial best response, \(\alpha\) will control the balance between it and the remaining preferences, thereby ensuring text quality.

%%%%%%
% % The previous preference ranking methods have the following main drawbacks:
% % (1) They depend on human or AI annotations of preference pairs or lists to specify the optimal responses that need to be aligned.
% % (2) Most methods rely on pair-wise comparisons\cite{hullermeier2008label,zhao2023slic,rafailov2024direct,liu2023statistical}, while only a few list-based methods \cite{yuan2024rrhf} conduct pair-wise comparisons within the list itself, rather than performing a true comparison of the entire list.
% % (3) They focus solely on the preference ranking of responses, neglecting the attribute values of the responses themselves, such as language probability distributions, semantics, popularity, response time, etc.
% % Specifically, these drawbacks can be divided into three points:
% % First, these methods only focus on the optimal response while ignoring the relative preference magnitudes of other responses. For instance, for the response set \(\{y_1, y_2, y_3, \cdots, y_M\}\), the language probability distributions 1 and 2 are \([0.98, 0.55, 0.79, \cdots, 0.81]\) and \([0.98, 0.79, 0.13, \cdots, 0.81]\), respectively. In both cases, the result is equivalent, ultimately identifying \(y_1\) as the optimal response. However, it is evident that in distribution 1, \(y_2 \succ y_3\), whereas in distribution 2, the opposite is true.
% % Second, these methods lack attention to attribute values, such as logical probability. For example, after confirming \(y_1\) as the optimal response in the first round, it no longer participates in subsequent iterative optimization. However, significant differences in language probabilities represent the distance of preferences. Specifically, the preference gaps between \(y_2\) and \(y_1\) in distributions 1 and 2 are \((0.98 - 0.55)\) and \((0.98 - 0.79)\), respectively, indicating varying degrees of preference.
% % Third, these methods ignore the distinction between good responses and poor responses. The distance between the best response and the worst response \((0.98 - 0.55)\) is evidently smaller than \((0.98 - 0.13)\). If we only consider the language probability of \(y_1\) as its reward value, this difference will be overlooked.
% % (4) The criterion for evaluating these different objectives is to measure when the results of the generative PROre superior to those of another model, referred to as the win rate in PPO. This approach is neither sustainable nor economical for improving the proposed PROnd extending the methods to be compared, which will be discussed in detail in \ref{subsec::metric}.


% \subsection{Problem Definition}
% In the list-wise formulation, the training dataset is \(\mathcal{D} = \{Q^{i}, R^{i}, A^{i}\}_{i=1}^{N}\). Given a question \(Q\), it corresponds to a series of responses \(R = \{R_1, \ldots, R_M\}\).
% The scalable attribute \(A = \{A_1, \ldots, A_L\}\) has a size \(L\) that varies across different communities. For example, in the code community, \(L=3\) and \(A = \{S, P, T\}\).
% Here, \(S\) represents the semantic similarity of the response to the question; \(P\) represents popularity, and \(T\) represents the creation time of each response. These attributes are of the same size as the responses \(R\), specifically, for a \(R_m^{i}\), it has an attribute set \((S_m^{i}, P_m^{i}, T_m^{i})\)
% It is worth noting that for any \(i > j\), it is not necessarily true that \(R_i \succ R_j\), where \(\mathcal{\succ}\) denotes the degree of preference. 

% \subsection{Multi-attribute Perception}
% In this section, we will first introduce the Attribute-Perceptual Distance Factor, and use the code community Q&A as an example, elaborating on its corresponding Semantic Perceptual Distance Factor and Popularity Perceptual Distance Factor. Then, we will describe how to comprehensively consider multiple APDF.
% Finally, we will elaborate on determining preference order based on Unsupervised Dynamic Ranking in Algorithm~\ref{algo1}.
% \subsubsection{Attribute-Perceptual Distance Factor}
% To focus on the intrinsic attribute values of responses and quantify the level of preference differences between different responses, we delved into the rich optimization objectives in ranking studies. 
% Since NDCG \cite{yue2007using} (Normalized Discounted Cumulative Gain) measures ranking performance, it not only evaluates the relationship between the positions of two candidates \(i\) and \(j\) but also introduces the gain function to consider their label values, which we take as the basis for the Attribute-Perceptual Distance Factor \(\delta_{i,j}\).
% % The NDCG and the weight \(\delta_{ij}\) are represented as follows:
% % \[
% % NDCG_i  \equiv \frac{1}{N_i} \sum_{j=1}^{T} \frac{2^{l_i(j)} - 1}{\log(1 + j)}
% % \]
% % $l(i)$ represents the label values of documents $i$. Note that the symbol $l_i(j)$ depends on the relationship between $l(i)$ and $l(j)$.

% Additionally, some studies have shown that neither point-wise nor pair-wise ranking methods can directly optimize ranking metrics such as NDCG. This is because their loss functions are typically smooth convex functions that are easy to optimize but do not effectively reflect ranking quality. Therefore, inspired from the Lambda weights \(\lambda_{ij}\) in LambdaRank \cite{burges2005learning,donmez2009local,wang2018lambdaloss,jagerman2022optimizing}, which scale the gradient of each pair of scores \((s_i, s_j)\) based on the labels of pairs for optimizing a metric-driven loss function, the final Attribute-Perceptual Distance Factor \(\delta_{ij}\) is represented as follows:
% \begin{equation}
%     \delta_{i,j} = (G(i)- G(j)) \cdot \left( \frac{1}{\log(1 + r_i)} - \frac{1}{\log(1 + r_j)} \right)
% \end{equation}
% $r_i$ and $r_j$ are the ranking positions of documents $i$ and $j$, respectively. The gain function of the response \( R \) is represented as \( G(\cdot) \).
% \subsubsection{Multiple Attribute-Perceptual Distance}
% In this section, we describe the Multiple Perceptual Distance Factor using the example of a code community question-answering platform. In this community, the main response attributes that reflect user preferences include semantics, popularity, and creation time. Additionally, to address the issue of accumulated popularity over time and the presence of outdated information in responses, the popularity attribute \(P\) is subject to time decay based on \(T\), denoted as \(\tilde{P}\). Therefore, given the response \(R = \{R_1, \ldots, R_M\}\) of question \(Q\), we define the Semantic Perceptual Distance Factor \(\delta_{Se}\) in \(\Delta_{Se}=\{\(\delta_{Se_{ij}}|i,j \in M\}\) and the Popularity Perceptual Distance Factor \(\delta_{Po}\) in \(\Delta_{Po}=\{\(\delta_{Po_{ij}}|i,j \in M\}\). 

% For the \(\delta_{Se}\), we define \(G(i) = 2^{\varphi(i) - 1}\), where \(\varphi(i) = \cos(E_{Q}, E_{R})\). Here, \(E_{Q}\) and \(E_{R}\) represent the semantic vectors of question \(Q\) and response \(R\) under prompt \(X\), using the encoder LLM2Vec \cite{behnamghader2024llm2vec}. For the \(\delta_{Po}\), we define \(G(i) = \log_{10}(\tilde{P} + 1)\). The final two-dimensional Multiple Perceptual Distance factor matrix size of \([M, M]\) is represented as follows:
% \begin{equation}
%     \Delta_{MuAPDF} =\Delta_{Se} \cdot \Delta_{Po}
% \end{equation}
%  \begin{algorithm}[H]
% \caption{Unsupervised Dynamic Rankings}
% \label{algo1}
% \KwIn{ \\
% $\Delta_{MuAPDF}$: Multiple Attribute-Perceptual Distance, \\
% $SeRank$: the order of \(\varphi(R)\) of size \(M\),\\
% $M$: is the size of response \(R\) of a given  question \(Q\) \\} \\
% \KwOut{$AilgnRank$} \\
% $AilgnRank \gets [ ] $  \\
%     \For{$i \gets 0$ \KwTo $M - 1$}{
%         $\delta_{max} \gets \max(\Delta_{MuAPDF})$ \; \\
%         $index \gets \text{where}(\Delta_{MuAPDF} == \delta_{max})$ \; \\
%         $row \gets index[0][0]$ \ ; 
%         $col \gets index[1][0]$ \; \\
%         \If{$SeRank[row] < SeRank[col]$}{ \\
%             $AilgnRank.append(row)$ \; \\
%             $\Delta_{MuAPDF}[:, row] \gets 0$ \ ; 
%             $\Delta_{MuAPDF}[row, :] \gets 0$ \; 
%         } 
%         \Else{ \\
%             $AilgnRank.append(col)$ \; \\
%             $\Delta_{MuAPDF}[:, col] \gets 0$ \ ; 
%             $\Delta_{MuAPDF}[col, :] \gets 0$ \ ; \\
%         } \\
%     } \\
%      $AilgnRank.append(SeRank.notin(AilgnRank))$ \ ; \\
% \Return $AilgnRank$ \; 
% \end{algorithm}

% \subsection{Perceptual Comparison}
% The list-wise loss functions can be roughly divided into three types: the first can fit all pairs using pairwise-logistic or pair-wise hinge losses \cite{zhao2023slic,yuan2024rrhf}. 
% The second is to directly fit a Maximum Likelihood Estimation (MLE) on the list-wise ranked data \cite{xia2008listwise}, denoted as \(\mathcal{L}_{ListMLE}\), which are used in DPO\_{PL} \cite{rafailov2024direct} and PRO \cite{song2024preference}.
% \begin{equation}
%    \mathcal{L}_{ListMLE} = -\log \prod_{m=1}^{M}  \left( \frac{ \exp(r_s(Q, R_m)}{\sum_{i=m}^{M} \exp(r_s(Q, R_i)} \right)
% \end{equation}
% where τ(i) is the document ranked at the \(i\)-th position in the list-wise permutation determined by the label.

% Third is softmax cross entropy loss as in ListNet\cite{cao2007learning}, denoted as \( \mathcal{L}_{softmax}\)
% \begin{equation}
% \label{loss_soft}
%    \mathcal{L}_{softmax} =  -\sum_{m=1}^{M}\log \left( \frac{  \exp(r_s(Q, R_m)}{\sum_{i=m}^{M} \exp(r_s(Q, R_i)} \right) \\
% \end{equation}
% where the LLM is regarded as both a reward model that replaces reward with language logits and a policy network, denoted as \(r_s\).
% Due to the \(\mathcal{L}_{softmax}\) is more relevant to ranking loss and utilizes double summation to emphasize comparisons between different samples, we adopt the equation Eq.~(\ref{loss_soft}) as the basis for our optimization objective. For the attribute \( A = \{ A_1, \ldots, A_L \} \), their APDF matrix is represented as \( \delta_{A_i} \). 
% To learn the preference gap between the best and worst responses of different language probability distributions, in each iteration of every round  \(m \in AlignRank \), we seek to maximize a multi-attribute distance-aware factor as the reward for the best response, denoted as \(\mathcal{R}_{m}\).
% \begin{equation}
%     \mathcal{R}_{m}=\prod_{k=1}^{L} \Delta_{A_k}(m,AlignRank[-1])
% \end{equation}
% where the AlignRank is the dynamic ranking result of the Algorithm.~(\ref{algo1}), and \(\Delta_{A_k}(x, y)\) is the \((x, y)\)-th value in the Attribute-Perceptual Distance Factor matrix \(A_k\).
% In order to distinguish the remaining responses and quantify their preference levels, we introduce different penalty factors for each response, denoted as \(\mathcal{P}_{i>m}\)
% \begin{equation}
%     \mathcal{P}_{i>m}^{i}=\prod_{k=1}^{L} \Delta_{A_k}(m,AlignRank[i])
% \end{equation}
% Here, as the difference between \(y\) and \(x\) decreases, \(\Delta_{A_k}(x, y)\) also decreases, indicating a smaller penalty being incurred.

% The optimization objective of the final Perceptual Comparison is represented as:
% \begin{equation}
%     \tau (i)= \exp(r_s(Q, R_i))
% \end{equation}
% \begin{equation}
%    \mathcal{L}_{PerCo} =  -\sum_{m=1}^{M}\log \left( \frac{ \tau (m)*\mathcal{R}_{m}}{\sum_{i\ne m}^{M} \tau (i)* \mathcal{P}^{i}+\tau (m)*\mathcal{R}_{m}} \right) 
% \end{equation}
% \begin{table}[h]
% \centering
% \caption{Statistics of the public training dataset for Community Q&A. We align LLMs to Q&A in different domain \(Category\), each with varying response size \(Step\) and data volume \(Volume\).}
%   \label{domain}
%   \renewcommand{\arraystretch}{0.8}
%   \tabcolsep=0.1cm
%   % { \fontnotesize{7}
%   \begin{tabular}{ ccc | ccc}
%     \toprule
%       Category & Volume & Step & Category & Volume & Step \\ \midrule
      
%         Academia & 16,783 &4 & Chemistry & 11,058 & 3 \\ 
%         Cooking & 15,036 &5 & Electronics & 20,384 &5 \\ 
%       History &6,600 & 3 & Math & 25,860&6 \\ 
%        Music & 16200 & 4 & Politics & 8,014 & 3 \\
%        Security &  31,327 & 6&  Stackoverflow & 23,926 &7  \\
%        % StaCoCoQA (Ours) & 9,978,474 & 5\\
%   \bottomrule
% \end{tabular}
% \end{table}

% % \Xcline{3-5}{0.4pt} \Xcline{6-7}{0.4pt} 
% % \belowrulesep=0.2pt
% % \aboverulesep=0.2pt
% \begin{table*}[h]
% \setlength{\tabcolsep}{3pt}
% \centering

%  \caption{Main results}
% \label{zero-shot}
% \renewcommand{\arraystretch}{1.25}
%    \tabcolsep=0.2cm
% \begin{adjustbox}{max width=\textwidth}  % 设置最大宽度为文本宽度
% \begin{tabular}{ c |ccc c||c cc cc}
% \toprule
% General& \multicolumn{3}{c}{Preference \((\uparrow)\)} & \multicolumn{1}{c}{Accuracy \((\uparrow)\)} & Alignment  & \multicolumn{3}{c}{Preference \((\uparrow)\)} & \multicolumn{}{1}{Accuracy \((\uparrow)\)} \\ 
%  LLMs& PrefHit & PrefRecall & Reward   & BLEU   & Method & PrefHit & PrefRecall & Reward   & BLEU   \\ \midrule
%  GPT-J & 0.2572 & 0.4130 & 0.6854 & 0.1354   & Llama2-7B & 0.2029 & 0.3986 & 0.0933 & 0.0947 \\
%  Pythia-2.8B & 0.2572 & 0.4130 & 0.6854 & 0.1354 & SFT & 0.2428 & 0.4185 & 0.4738 & 0.1364 \\
%  Qwen2-7B & 0.2789 & 0.4511 & 0.6593 & 0.2530   & Slic & 0.2464 & 0.2464 & 0.4700 & 0.1400 \\
%  Qwen2-52B & 0.2572 & 0.4130 & 0.6854 & 0.1354 & RRHF & 0.3297 & 0.4801 & 0.2263 & 0.1504\\
% ChatGLM4-9B & 0.2572 & 0.4130 & 0.6854 & 0.1354  & DPO_{BT} & 0.2500 & 0.4167 & 0.4728 & 0.1363  \\ 
% StarCoder2-15B & 0.2572 & 0.4130 & 0.6854 & 0.1354 & DPO_{PT} & 0.2572 & 0.4130 & 0.4700 & 0.1348\\
%  Llama3-8B &  0.2826 & 0.2826 & 0.2458 & 0.1723 & PRO & 0.2609 & 0.4565 & 0.1899 & 0.1379  \\ \hline 
%  \textbf{\shortname} & 0.3478 & 0.5127 & 0.2233 & 0.1741 & \textbf{SeAdpra}_{\textbf{large}} & 0.3659 & 0.5036 & 0.2301  & 0.2079  \\ 
% \bottomrule
% \end{tabular}
% \end{adjustbox}
% \end{table*}

% \begin{table}[h]
% \centering
% \caption{Statistics of the public training dataset for Community Q&A. We align LLMs to Q&A in different domain \(Category\), each with varying response size \(Step\) and data volume \(Volume\).}
% \label{domain}
% \renewcommand{\arraystretch}{1.1}
% \tabcolsep=0.08cm
% \begin{adjustbox}{width=0.47\textwidth} % 调整表格宽度
% \begin{tabular}{l lcccc}
%     \toprule
%     \multirow{2}{*}{\small \textit{Dataset}} & \multirow{2}{*}{\small \textit{Model}} & \multicolumn{3}{c}{\small \textit{Preference \((\uparrow)\)}} & \multicolumn{1}{c}{\small \textit{Accuracy \((\uparrow)\)}} \\
%     && \small \textit{PrefHit} & \small \textit{PrefRecall} & \small \textit{Reward} & \small \textit{BLEU} \\ \midrule
%     \multirow{2}{*}{\small \textit{Academia}}   & \small \textit{PRO} & 33.78 & \underline{59.56} & \underline{69.94} & 9.84 \\ 
%                                 & \small \textit{\shortname} & 36.44 & \underline{60.89} & \underline{70.17} & 10.69 \\ \midrule
%     \multirow{2}{*}{\small \textit{Chemistry}}    & \small \textit{PRO} & 36.31 & 63.39 & 69.15 & 11.16 \\ 
%                                 & \small \textit{\shortname} & 38.69 & 64.68 & 69.31 & 11.27 \\ \midrule
%     \multirow{2}{*}{\small \textit{Cooking}}    & \small \textit{PRO} & \underline{35.29} & 58.32 & 69.87 & 12.13 \\ 
%                                 & \small \textit{\shortname} & \underline{38.50} & 60.01 & 69.93 & 11.73 \\ \midrule
%     \multirow{2}{*}{\small \textit{Math}}      & \small \textit{PRO} & 30.00 & 56.50 & 69.06 & \underline{13.50} \\ 
%                                 & \small \textit{\shortname} & 32.00 & 58.54 & 69.21 & \underline{14.45} \\ \midrule
%     \multirow{2}{*}{\small \textit{Music}}      & \small \textit{PRO} & 34.33 & 60.22 & 70.29 & 13.05 \\ 
%                                 & \small \textit{\shortname} & 37.00 & 60.61 & 69.84 & 9.82 \\ \midrule
%     \multirow{2}{*}{\small \textit{Politics}}      & \small \textit{PRO} & 41.77 & 66.10 & 69.52 & 9.31 \\ 
%                                 & \small \textit{\shortname} & 42.19 & 66.03 & 69.44 & 9.38 \\ \midrule
%     \multirow{2}{*}{\small \textit{Stackoverflow}}      & \small \textit{PRO} & 26.00 & 51.13 & 69.17 & 12.44 \\ 
%                                 & \small \textit{\shortname} & 27.00 & 51.77 & 69.16 & 11.33 \\ \midrule
%     \multirow{2}{*}{\small \textit{Security}}      & \small \textit{PRO} & 23.62 & 49.23 & 70.13 & 10.63 \\ 
%                                 & \small \textit{\shortname} & 25.20 & 49.24 & 69.92 & 8.98 \\ \midrule
%     \multirow{2}{*}{\small \textit{Mean}}       & \small \textit{PRO} & 34.33 & 60.22 & 70.29 & 13.05 \\ 
%                                 & \small \textit{\shortname} & \textbf{37.10 \text{\(\uparrow\)}} & \textbf{60.66 \text{\(\uparrow\)}} & 69.88 & 11.47 \\ 
% \bottomrule
% \end{tabular}

 

% \end{adjustbox}
% \end{table}



% \subsection{Perceptual Alignment}
% Due to the effective method of learning domain-specific knowledge being supervised fine-tuning, and the most direct approach in preference learning also being to perform supervised fine-tuning on a high-quality preference dataset \cite{rafailov2024direct}, we align the best response \(R_{AlignRank[0]} \) from the \(R\) as the target for supervised fine-tuning to the question \(Q\), denoted as \(R_b\).
% \begin{equation}
%     \mathcal{L}_{PerAl} = - \frac{1}{|R_b|} \sum_{j=1}^{|R_b|} \log \mathcal{O}(R_b(j) | Q, R_b(i<j))
% \end{equation}
% Where, \(|R_b|\) is the sequence length of \(R_b\). \(R_b{(j)}\) is the \(j\)-th token of \(R_b\), and \(\mathcal{O}\) denotes the token probability predicted by a language model \(LM\). 
% Finally, \shortname\  enables the agent LLM \(LM\) to be directly trained by the following objective:
% \begin{equation}
%     \mathcal{L}_{\shortname}= \mathcal{L}_{PerCo}+\alpha*\mathcal{L}_{PerAl}
% \end{equation}
% \(\alpha\) is the hyper-parameter to maintain the balance between text quality and human preference. 


