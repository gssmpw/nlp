%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[h]
  \centering

  % 第一行：hit1_cons_XX
  \begin{subfigure}{0.3\linewidth}
    \includegraphics[width=\linewidth]{latex/pic/hit1_cons_w.png}
    \caption{The PrefHit@1 (W)}
    \label{fig::hit1_cons_w}
  \end{subfigure}
  \begin{subfigure}{0.3\linewidth}
    \includegraphics[width=\linewidth]{latex/pic/hit1_cons_bs.png}
    \caption{The PrefHit@1 (BS)}
    \label{fig::hit1_cons_bs}
  \end{subfigure}
  \begin{subfigure}{0.3\linewidth}
    \includegraphics[width=\linewidth]{latex/pic/hit1_cons_lr.png}
    \caption{The PrefHit@1 (LR)}
    \label{fig::hit1_cons_lr}
  \end{subfigure}

  \medskip

  % 第二行：hit3_cons_XX
  \begin{subfigure}{0.3\linewidth}
    \includegraphics[width=\linewidth]{latex/pic/hit3_cons_w.png}
    \caption{The PrefHit@3 (W)}
    \label{fig::hit3_cons_w}
  \end{subfigure}
  \begin{subfigure}{0.3\linewidth}
    \includegraphics[width=\linewidth]{latex/pic/hit3_cons_bs.png}
    \caption{The PrefHit@3 (BS)}
    \label{fig::hit3_cons_bs}
  \end{subfigure}
  \begin{subfigure}{0.3\linewidth}
    \includegraphics[width=\linewidth]{latex/pic/hit3_cons_lr.png}
    \caption{The PrefHit@3 (LR)}
    \label{fig::hit3_cons_lr}
  \end{subfigure}

  \medskip

  % 第三行：recall2_cons_XX
  \begin{subfigure}{0.3\linewidth}
    \includegraphics[width=\linewidth]{latex/pic/recall2_cons_w.png}
    \caption{The PrefRecall@1 (W)}
    \label{fig::recall2_cons_w}
  \end{subfigure}
  \begin{subfigure}{0.3\linewidth}
    \includegraphics[width=\linewidth]{latex/pic/recall2_cons_bs.png}
    \caption{The PrefRecall@1 (BS)}
    \label{fig::recall2_cons_bs}
  \end{subfigure}
  \begin{subfigure}{0.3\linewidth}
    \includegraphics[width=\linewidth]{latex/pic/recall2_cons_lr.png}
    \caption{The PrefRecall@1 (LR)}
    \label{fig::recall2_cons_lr}
  \end{subfigure}

  \medskip

  % 第四行：recall4_cons_XX
  \begin{subfigure}{0.3\linewidth}
    \includegraphics[width=\linewidth]{latex/pic/recall4_cons_w.png}
    \caption{The PrefRecall@3 (W)}
    \label{fig::recall4_cons_w}
  \end{subfigure}
  \begin{subfigure}{0.3\linewidth}
    \includegraphics[width=\linewidth]{latex/pic/recall4_cons_bs.png}
    \caption{The PrefRecall@3 (BS)}
    \label{fig::recall4_cons_bs}
  \end{subfigure}
  \begin{subfigure}{0.3\linewidth}
    \includegraphics[width=\linewidth]{latex/pic/recall4_cons_lr.png}
    \caption{The PrefRecall@3 (LR)}
    \label{fig::recall4_cons_lr}
  \end{subfigure}
  \caption{The consistency relationship between the new metrics (PrefHit and PrefRecall) and classic metrics (closer to 1 indicates stronger positive correlation, while closer to -1 indicates stronger negative correlation). Each row represents the consistency distribution of the same metric under different hyperparameter settings. Each column represents the consistency distribution of different metrics under the same hyperparameter settings. The W represents \(\alpha\) in Eq.(\ref{eq::final}) with results shown in Table \ref{table::w}. The BS represent the batch size, with results shown in Table \ref{table::bs}. The LR represents the learning rate, and its results are shown in Table \ref{table::lr}.}
  \label{fig:consistency}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Reinforcement Learning from Human Feedback}
Given a preference dataset \( D = \{(x, y_w, y_l)\} \), where \( x \) is an input, \( y_w \) and \( y_l \) are the preferred and dispreferred outputs (i.e., \( y_w \succ y_l \) for \( x \)), and \( r^* \) is the “true” reward function underlying the preferences. 
Specifically, it is first assumed that the probability that \( y_w \) is preferred to \( y_l \) can be captured with a specific function class, typically a Bradley-Terry model \cite{bradley1952rank}. Where \( \sigma \) is the logistic function:
\begin{equation}
    p^*(y_w \succ y_l | x) = \sigma(r^*(x, y_w) - r^*(x, y_l)) 
\end{equation}

Since getting the true reward from a human would be intractably expensive \cite{ethayarajh2024kto}, a reward model \( r_\phi \) learns to serve as a proxy, done by minimizing the negative log-likelihood of the human preference data:
\begin{equation}
    L(r_\phi) = \mathbb{E}_{x, y_w, y_l \sim D}[- \log \sigma(r_\phi(x, y_w) - r_\phi(x, y_l))]
\end{equation}


But solely maximizing the reward might come at the expense of desiderata such as generating grammatical text. To avoid this, a KL divergence penalty is introduced to restrict how far the language model can drift from \( \pi_{ref} \). Where \( \pi_\theta \) is the model we are optimizing, the optimal model \( \pi^* \) is the one that maximizes:
\begin{equation}
    \mathbb{E}_{x \in D, y \in \pi_\theta} [r_\phi(x, y)] - \beta D_{KL}(\pi_\theta(y|x) \parallel \pi_{ref}(y|x)) 
\end{equation}
where \( \beta > 0 \) is a hyperparameter. Since this objective is not differentiable, we need to use an RL algorithm like PPO \cite{schulman2017proximal}.



\subsection{Direct Preference Optimization}
However, the RLHF faces the challenge of extensive hyperparameter search due to the instability of PPO \cite{rafailov2024direct} and the sensitivity of the reward model \cite{gao2023scaling}.
Therefore, recent research has focused on designing stable closed-form loss functions that maximize the margin between preferred and dispreferred generations. 
In particular, Bradley-Terry-based Direct Preference Optimization (DPO) \cite{rafailov2024direct} has emerged as a popular alternative, as it allows the recovery of the same optimal policy as in RLHF under certain conditions:
\begin{equation}
\begin{aligned}
    & L_{DPO}(\pi_\theta, \pi_{ref}) = \mathbb{E}_{x, y_w, y_l \sim D} \\
    & \Big[  - \log \sigma \left( \beta \log \frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)}  \right) \Big]
    \label{eq::bradly}
\end{aligned}
\end{equation}





\subsubsection{the Plackett-Luce Model}
\label{sec::PL}
The Plackett-Luce model \cite{luce1959individual} is a generalization of the Bradley-Terry\cite{bradley1952rank} model in Eq.(\ref{eq::bradly}) to rankings (rather than just pairwise comparisons). Similar to the Bradley-Terry model, it stipulates that when faced with a set of possible choices, individuals prefer a choice with a probability proportional to the value of some latent reward function for that choice.
In our context, given a question \( Q \) and a set of candidate responses \( \{R_1,\ldots, R_M\}\), a user outputs a permutation \( \tau: [M] \to [M] \) that represents their ranking of the answers. The Plackett-Luce model specifies as follows:
\begin{equation}
\begin{aligned}
    \label{eq::pl1}
   & p^*(\tau \mid R_1, \ldots, R_M, Q) = \\
   & \frac{\exp(r^*(Q, R_{\tau(m)}))}{\sum_{j=m}^M \exp(r^*(Q, R_{\tau(j)}))}
\end{aligned}
\end{equation}
% \begin{equation}
%     \label{eq::pl1}
%     p^*(\tau \mid R_1, \ldots, R_M, Q) = \frac{\exp(r^*(Q, R_{\tau(m)}))}{\sum_{j=m}^M \exp(r^*(Q, R_{\tau(j)}))}
% \end{equation}
Please note that when \( K = 2 \), Eq.(~\ref{eq::pl1}) simplifies to the Bradley-Terry model.
However, for the general Plackett-Luce model, we can still utilize the logistic probability to replace the reward function similar with the DPO.
\begin{equation}
    \label{eq::pl2}
    r(Q, R) = \beta \log \frac{\pi_{\text{ref}}(R \mid Q)}{\pi_r(R \mid Q)} + \beta \log Z(Q)
\end{equation}
This Eq.(\ref{eq::pl2}) represents the reward function in terms of its corresponding optimal policy \( \pi^* \), reference policy \( \pi_{\text{ref}} \), and the unknown partition function \( Z(\cdot) \).
When the normalization constant \( Z(x) \) cancels out and we’re left with:
\begin{equation}
\begin{aligned}
   & p^*(\tau \mid R_1, \ldots, R_M, Q) = \\
    & \frac{\exp \left( \beta \log \frac{\pi^*(R_{\tau(k)} \mid Q)}{\pi_{\text{ref}}(R_{\tau(k)} \mid Q)} \right)}{\sum_{j=m}^M \exp \left( \beta \log \frac{\pi^*(R_{\tau(j)} \mid Q)}{\pi_{\text{ref}}(R_{\tau(j)} \mid Q)} \right)}
\end{aligned}
\end{equation}

% \begin{equation}
%        p^*(\tau \mid R_1, \ldots, R_M, Q) = \frac{\exp \left( \beta \log \frac{\pi^*(R_{\tau(k)} \mid Q)}{\pi_{\text{ref}}(R_{\tau(k)} \mid Q)} \right)}{\sum_{j=m}^M \exp \left( \beta \log \frac{\pi^*(R_{\tau(j)} \mid Q)}{\pi_{\text{ref}}(R_{\tau(j)} \mid Q)} \right)}
% \end{equation}
For the CoQA dataset \(\mathcal{D} = \{Q^{i}, R^{i}\}_{i=1}^{N}\), which contains prompts and user-specified rankings, we can use a parameterized model and optimize this objective using maximum likelihood:
\begin{equation}
\begin{aligned}
   & L(\pi_\theta, \pi_{\text{ref}}) = \\
    & -\mathbb{E}  \log \frac{\exp \left( \beta \log \frac{\pi_\theta(R_{\tau(k)} \mid Q)}{\pi_{\text{ref}}(R_{\tau(k)} \mid Q)} \right)}{\sum_{j=k}^K \exp \left( \beta \log \frac{\pi_\theta(R_{\tau(j)} \mid Q)}{\pi_{\text{ref}}(R_{\tau(j)} \mid Q)} \right)} 
\end{aligned}
\end{equation}
% \begin{equation}
%     L(\pi_\theta, \pi_{\text{ref}}) = -\mathbb{E}  \log \frac{\exp \left( \beta \log \frac{\pi_\theta(R_{\tau(k)} \mid Q)}{\pi_{\text{ref}}(R_{\tau(k)} \mid Q)} \right)}{\sum_{j=k}^K \exp \left( \beta \log \frac{\pi_\theta(R_{\tau(j)} \mid Q)}{\pi_{\text{ref}}(R_{\tau(j)} \mid Q)} \right)} 
% \end{equation}
