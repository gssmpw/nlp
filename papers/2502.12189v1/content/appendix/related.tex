% \subsection{Preference Alignment and Ranking}
% Learning from human preferences \cite{christiano2017deep} aims to better align generative language models with human needs. This method first collects a large amount of data, where each data point consists of a context, a pair of context continuations (also known as generated content), and a pair of human preferences indicating which generated content is better. Then, a policy for generating high-quality content is learned from the collected data \cite{azar2024general}.



% Aligning large language models (LLMs) with human preferences can be directly achieved through reinforcement learning (RL), where the model requires only sparse supervisory signals from reward models that act as human proxies, called RLHF. 
% To circumvent the complexities of the RLHF, Direct Preference Optimization (DPO) \cite{rafailov2024direct}, under the pair-wise Bradley-Terry assumption \cite{bradley1952rank} or list-wise Plackett-Luce \cite{luce1959individual}, replaces rewards with the relative log probabilities of responses but does not provide list-wise experimental results. Methods like SLiC \cite{zhao2023slic} and RRHF \cite{yuan2024rrhf} use pair-wise hinge loss to align policy responses.
% In practice, nearly all existing preference optimization frameworks \cite{azar2024general,liu2023statistical} are limited to pair-wise preferences. Meanwhile, PRO \cite{song2024preference} introduces list maximum likelihood estimation (MLE) loss and begins to focus on preference ranking, marking a pioneering effort in list-wise preference alignment.
% LiPO \cite{liu2024lipo}, which is most similar to our approach, directly optimizes list-based ranking preferences and starts to focus on response labels but has not yet addressed the combination of multiple labels. However, these supervised learning methods rely on human or AI annotations of preference pairs or lists to specify the best responses that need alignment, which is not cost-effective. Curry-DPO simulates curriculum learning by sequentially ranking during training, using multiple preference pairs, but primarily focuses on pair-wise preference optimization.

\subsection{Alignment of LLMs.}
The language modeling objective of Large Language Models (e.g., predicting the next word) differs from the ultimate goals in LLM applications, such as following instructions and being helpful, factual, and harmless\cite{qi2023fine,bhardwaj2024language,yi2024vulnerability}.
The behavior of pre-trained LLMs may not necessarily align with the principles of their intended use cases. 
Therefore, alignment of LLMs \cite{zhu2024lire,wang2024arithmetic} aims to adjust the outputs of general pre-trained language models to better align with human preferences, significantly improving the performance of LLMs in various downstream applications, such as Summarization\cite{hu2024moments}, dialogue agents \cite{niu2024enhancing}, and question-answering \cite{panda2024holmes}.
Currently, the two most common alignment techniques are instruction tuning \cite{ren2024learning} and reinforcement learning from human feedback (RLHF) \cite{bai2022constitutional,ouyang2022training}. 
Additionally, emerging alignment techniques such as Constitutional AI \cite{bai2022constitutional} and self-alignment \cite{ren2024learning} are also gaining attention.
These primarily focus on embedding alignment rules into pre-trained models to constrain harmful behavior during inference. However, they have not explored how to align objectives with multiple attributes. 
Our study demonstrates that the objectives of preference alignment are influenced by multiple factors.

\subsection{Supervised Alignment}
Large Language Models (LLMs) alignment typically involves two steps. 
The first is supervised fine-tuned (SFT) on high-quality demonstration data to adapt to a specific scenario \cite{stiennon2020learning}.
The second is to learn a strategy for generating high-quality content on preference data to align with human expectations \cite{azar2024general}.
Each preference data item consists of a context, a pair of generated contents, and a pair of human preferences indicating which generated content is better. Additionally, annotating preference data requires some level of expert knowledge.

Learning to align LLMs with human preferences can be achieved through reinforcement learning (RL).
SFT is crucial for ensuring the stable update of the active policy relative to the old policy in preference alignment methods within reinforcement learning \cite{schulman2017proximal}.
In addition, empirical research shows that even in non-RL alignment methods, the SFT is also key to achieve convergence to the desired outcomes \cite{rafailov2024direct, tunstall2023zephyr}.
Therefore, PRO \cite{song2024preference} incorporates the softmax values of the reference response set into the negative log-likelihood loss to merge supervised fine-tuning and preference alignment.
Both SFT and most alignment methods \cite{rafailov2024direct,christiano2017deep,song2024preference,zhao2023slic} rely on annotated data; however, preference data is relatively scarce and expensive to collect in practice \cite{casper2023open}. Therefore, there is an urgent need for an unsupervised method that dynamically annotates preferences during learning to achieve cost-effective preference learning.

% \subsection{Preference Alignment and Ranking}
% Learning from human preferences \cite{christiano2017deep} aims to better align generative language models with intentions and values, making generations more helpful, factual, and ethical, among other desiderata \cite{ouyang2022training}.
% An important approach to learning from human preferences is reinforcement learning from human feedback (RLHF) \cite{ouyang2022training,stiennon2020learning}, which involves training a reward model as a classifier for preferred and non-preferred actions.
% To circumvent the complexities of the RLHF, based-on Bradley-Terry \cite{bradley1952rank} Direct Preference Optimization (DPO) \cite{rafailov2024direct} replaces rewards with the relative log probabilities of responses.
% SLiC \cite{zhao2023slic} and RRHF \cite{yuan2024rrhf} use pair-wise hinge loss to align policy responses.
% Curry-DPO \cite{pattnaik2024curry} simulates curriculum learning by sequentially ranking during training, using multiple preference pairs.
% Therefore, almost all existing preference optimization frameworks \cite{azar2024general, liu2023statistical} are limited to pairwise preferences and lack consideration of list-wise alignment scenarios.
% Although DPO proposes alignment learning based on the list-wise Plackett-Luce assumption \cite{luce1959individual}, no experimental results are provided.
% At this stage, PRO \cite{song2024preference} introduces list maximum likelihood estimation (MLE) loss and begins to focus on preference ranking, marking a pioneering effort in list-wise preference alignment.
% However, it lacks attention to other intrinsic attribute values of the responses beyond the semantic content.
% LiPO \cite{liu2024lipo}, which is most similar to our approach, directly optimizes list-based ranking preferences and starts to focus on response labels but has not yet addressed the combination of multiple labels.
