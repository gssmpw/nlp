\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{latex/pic/public.pdf}
    \caption{Visualization of main results (\%) on eight publicly available and popular CoQA datasets, comparing the strong list-wise supervised preference ranking benchmark PRO and Ours SeAdpra.}
    \label{fig::public}
\end{figure*}

\subsection{Motivation}
\label{metric::mot}
The existing alignment evaluation methods are mainly divided into two categories.

The first relies on reward models \cite{song2024preference,liu2024lipo}, useing ranking models to measure the degree of human preference. To avoid unfairness, two different ranking models are typically selected for training and evaluation. This metric enables the automated evaluation of numerous models. However, we hope for more automated preference ranking metrics to emerge, allowing for a comprehensive assessment of the degree of list-wise preference alignment.

The second is human or GPT-4 evaluations. 
Human evaluation is the gold standard for measuring human preferences \cite{zhou2024lima}.
These methods require human or AI evaluators to assign an Absolute Quality Score (AQS) to each response generated by different LLMs.
The win rate\cite{ouyang2022training,rafailov2024direct} is defined as the percentage of cases where the AQS of a model's response is higher than that of another model's corresponding response.
However, this win-rate assessments is costly when method upgrades and the addition of baselines occur.
For instance, when an existing model \(M_A\) is evaluated against comparison methods \((M_B, M_C, M_D)\) in terms of win rates, upgrading model \(M_A\) would necessitate a reevaluation of its win rates against other models. Furthermore, if a new comparison method \(M_E\) is introduced, the win rates of model \(M_A\) against \(M_E\) would also need to be reassessed. 
Moreover, this win-rate evaluation involves a binary judgment between preferred and non-preferred choices and has not yet been extended to list-wise preference ranking evaluation.
% Whether AI or humans are employed as evaluation mediators, binary preference between preferred and non-preferred choices or to score the inference results of the modified model, the costs of this process are substantial. 

\subsection{The "CSTC" Criterion}
\label{sec::cstc}

\noindent\textbf{Cost-effectiveness}
Whether upgrading the original method \( M_A \) to \( M_{A1} \) or expanding the comparison method \( M_E \), only one evaluation of \( M_{A1} \) or \( M_E \) is required, instead of pairwise comparisons between \( M_A1 \) and \( (M_B, M_C, M_D) \), or \( M_E \) and \( M_A \). Importantly, we have discovered new metrics achieves a consistency of 0.98 with human annotations.

\noindent\textbf{Scalability} is reflected in three aspects: 1)The upgrade of the original method; 2)The expansion of the comparison method; 3) The transformation of candidate responses from binary to multiple.

\noindent\textbf{Transferability}
This evaluation has broad applicability across various domains. Specifically, it not only assesses preference alignment but can also be transferred to other alignment areas, such as SaferHit in safety alignment, as shown in Eq.(\ref{eq::saferhit}).

\noindent\textbf{Consistency}
To validate the effectiveness of new metrics, we conducted consistency checks between them and commonly used reward model-based preference alignment evaluation methods, as well as metrics for evaluating model general reasoning abilities, namely BLEU and ROUGE.
The results show that PrefHit and PrefRecall are strongly consistent with hese classic metrics.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%
% \begin{figure*}[t]
%   \centering
%   \begin{subfigure}{0.245\linewidth}
%     \includegraphics[width=\linewidth]{latex/pic/hit1_cons_w.png}
%     \caption{The PrefHit@1}
%     \label{fig::hit1_cons_w}
%   \end{subfigure}
%   \begin{subfigure}{0.245\linewidth}
%     \includegraphics[width=\linewidth]{latex/pic/hit3_cons_w.png}
%     \caption{The PrefHit@3}
%     \label{fig::hit3_cons_w}
%   \end{subfigure}
%   \begin{subfigure}{0.245\linewidth}
%     \includegraphics[width=\linewidth]{latex/pic/recall2_cons_w.png}
%     \caption{The PrefRecall@1}
%     \label{fig::recall2_cons_w}
%   \end{subfigure}
%   \begin{subfigure}{0.245\linewidth}
%     \includegraphics[width=\linewidth]{latex/pic/recall4_cons_w.png}
%     \caption{The PrefRecall@3}
%     \label{fig::recall4_cons_w}
%   \end{subfigure}
  
%   \medskip
%   \begin{subfigure}{0.245\linewidth}
%     \includegraphics[width=\linewidth]{latex/pic/hit1_cons_bs.png}
%     \caption{The Reward}
%     \label{fig::hit1_cons_bs}
%   \end{subfigure}
%   \begin{subfigure}{0.245\linewidth}
%     \includegraphics[width=\linewidth]{latex/pic/hit3_cons_bs.png}
%     \caption{The BLEU}
%     \label{fig::hit_cons_bs}
%   \end{subfigure}
%   \begin{subfigure}{0.245\linewidth}
%     \includegraphics[width=\linewidth]{latex/pic/recall2_cons_bs.png}
%     \caption{The BLEU}
%     \label{fig::recall2_cons_bs}
%   \end{subfigure}
%    \begin{subfigure}{0.245\linewidth}
%     \includegraphics[width=\linewidth]{latex/pic/recall4_cons_bs.png}
%     \caption{The BLEU}
%     \label{fig::recall4_cons_bs}
%   \end{subfigure}
  
%   \medskip
%     \begin{subfigure}{0.245\linewidth}
%     \includegraphics[width=\linewidth]{latex/pic/hit1_cons_lr.png}
%     \caption{The PrefHit@1}
%     \label{fig::hit1_cons_lr}
%   \end{subfigure}
%   \begin{subfigure}{0.245\linewidth}
%     \includegraphics[width=\linewidth]{latex/pic/hit3_cons_lr.png}
%     \caption{The PrefHit@3}
%     \label{fig::hit3_cons_lr}
%   \end{subfigure}
%   \begin{subfigure}{0.245\linewidth}
%     \includegraphics[width=\linewidth]{latex/pic/recall2_cons_lr.png}
%     \caption{The PrefRecall@1}
%     \label{fig::recall2_cons_lr}
%   \end{subfigure}
%    \begin{subfigure}{0.245\linewidth}
%     \includegraphics[width=\linewidth]{latex/pic/recall4_cons_lr.png}
%     \caption{The PrefRecall@3}
%     \label{fig::recall4_cons_lr}
%   \end{subfigure}
%   \caption{The performance with Confidence Interval (CI) of our SeAdpra and PRO at different data scales.}
%   \label{fig:consistency}
% \end{figure*}
% % %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%
% \begin{figure*}[t]
%   \centering
%   \begin{subfigure}{0.32\linewidth}
%     \includegraphics[width=\linewidth]{latex/pic/hit1_cons_w.png}
%     \caption{The PrefHit@1}
%     \label{fig::hit1_cons_w}
%   \end{subfigure}
%   \begin{subfigure}{0.32\linewidth}
%     \includegraphics[width=\linewidth]{latex/pic/hit1_cons_bs.png}
%     \caption{The Reward}
%     \label{fig::hit1_cons_bs}
%   \end{subfigure}
%    \begin{subfigure}{0.32\linewidth}
%     \includegraphics[width=\linewidth]{latex/pic/hit1_cons_lr.png}
%     \caption{The Reward}
%     \label{fig::hit1_cons_lr}
%   \end{subfigure}
%    \caption{The performance with Confidence Interval (CI) of our SeAdpra and PRO at different data scales.}
%   \label{fig:consistency_hit}
% \end{figure*}
%  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
% \begin{figure*}[t]
%   \centering
%   \begin{subfigure}{0.32\linewidth}
%     \includegraphics[width=\linewidth]{latex/pic/hit3_cons_w.png}
%     \caption{The PrefRecall}
%     \label{fig::hit3_cons_w}
%   \end{subfigure}
%   \begin{subfigure}{0.32\linewidth}
%     \includegraphics[width=\linewidth]{latex/pic/hit3_cons_bs.png}
%     \caption{The BLEU}
%     \label{fig::hit3_cons_bs}
%   \end{subfigure}
%   \begin{subfigure}{0.32\linewidth}
%     \includegraphics[width=\linewidth]{latex/pic/hit3_cons_lr.png}
%     \caption{The BLEU}
%     \label{fig::hit3_cons_lr}
%   \end{subfigure}
%   \caption{The performance with Confidence Interval (CI) of our SeAdpra and PRO at different data scales. (Hit@3)}
%   \label{fig:hit3}
% \end{figure*}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{figure*}[t]
%   \centering
%   \begin{subfigure}{0.32\linewidth}
%     \includegraphics[width=\linewidth]{latex/pic/recall2_cons_w.png}
%     \caption{The PrefRecall}
%     \label{fig::recall2_cons_w}
%   \end{subfigure}
%   \begin{subfigure}{0.32\linewidth}
%     \includegraphics[width=\linewidth]{latex/pic/recall2_cons_bs.png}
%     \caption{The BLEU}
%     \label{fig::recall2_cons_bs}
%   \end{subfigure}
%   \begin{subfigure}{0.32\linewidth}
%     \includegraphics[width=\linewidth]{latex/pic/recall2_cons_lr.png}
%     \caption{The BLEU}
%     \label{fig::recall2_cons_lr}
%   \end{subfigure}
%   \caption{The performance with Confidence Interval (CI) of our SeAdpra and PRO at different data scales. (Recall@2)}
%   \label{fig:recall2}
% \end{figure*}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
% \begin{figure*}[t]
%   \centering
%   \begin{subfigure}{0.32\linewidth}
%     \includegraphics[width=\linewidth]{latex/pic/recall4_cons_w.png}
%     \caption{The PrefRecall}
%     \label{fig::recall4_cons_w}
%   \end{subfigure}
%   \begin{subfigure}{0.32\linewidth}
%     \includegraphics[width=\linewidth]{latex/pic/recall4_cons_bs.png}
%     \caption{The BLEU}
%     \label{fig::recall4_cons_bs}
%   \end{subfigure}
%   \begin{subfigure}{0.32\linewidth}
%     \includegraphics[width=\linewidth]{latex/pic/recall4_cons_lr.png}
%     \caption{The BLEU}
%     \label{fig::recall4_cons_lr}
%   \end{subfigure}
%   \caption{The performance with Confidence Interval (CI) of our SeAdpra and PRO at different data scales. (Recall@4)}
%   \label{fig:recall4}
% \end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{PrefHit and PrefRecall}
To adapt to the list-wise CoQA and adhere to the CSTC guidelines proposed in Appendix \ref{sec::cstc}, enspired by the Hit and Recall, the specific calculation methods are as follows:
\begin{equation}
\text{PrefHit@k} = \frac{1}{N} \sum_{i=1}^{N} \mathbb{I}(\Phi (x,R^i)\in G_i(k))
\end{equation}
Here, \( \Phi(x, R^i) \) denotes the similarity between \( x \), which represents a response generated by the LLM to be evaluated, and \( k \) instances of \( R^i = \{R_1^i, \ldots, R_k^i\} \), a set of candidate responses for a given question \( Q \), and returns the index corresponding to the maximum similarity.
\( G_i(k) \) denotes the indices of the top \( k \) items in the list-wise golden label of the \(R^i\).

\begin{equation}
   \Phi (x,R) =\arg \max_{i} \, \text{Sim}(x, R_i)
   \label{eq::sim}
\end{equation}
Similarly,
\begin{equation}
    \text{PrefRecall@k} = \frac{1}{N} \sum_{i=1}^{N} \frac{\left| \Psi (x,R^i,k) \cap G_i(k) \right|}{2}
\end{equation}
Here, \(\Psi(x, R^i, k)\) represents the indices of the top \(k\) most similar \(R_i\) to \(x\) based on the similarity.
\begin{equation}
   \Psi(x, R^i, k) = argsort_{i<k} \left( \text{Sim}(x, R_i) \right)
\end{equation}

It is worth noting that \(\text{Sim}(x, R_i)\) has traditionally been evaluated by human annotators, which is expensive and time-consuming. We propose an alternative using llm2vec\footnote{https://github.com/McGill-NLP/llm2vec} \cite{behnamghader2024llm2vec}, as Large Language Models are powerful text encoders. We chose this replacement because its scores on 276-item test set are highly consistent with human labels, with a correlation of 0.98.


\subsection{Effectiveness Analysis}
\label{metric::ana}
The SeAdpra we proposed performs quite well on both domain-specific and public CoQA regarding the new metrics, as shown in Table \ref{main} and Table \ref{public}. 
In addition, we present the visual comparison of the performance between the state-of-the-art supervised preference ranking methods PRO and ours SeAdpra in Figure \ref{fig::public}.
To further explore the effectiveness of the new metrics PrefHit and PrefRecall, we will analyze them from two main aspects: 1) consistency with traditional metrics, and 2) applicability in different application scenarios.

\subsubsection{Consistency and Robustness}
To gauge the consistency between PrefHit and PrefRecall with classic preference alignment metrics (Reward) and semantic-related metrics (BLEU and Rouge), we employ two key statistical correlation coefficients under different hyperparameters: Pearson R (\(r_p\)) \cite{bravais1844analyse} and Spearman R (\(r_s\)) \cite{pranklin1974introduction}. 
Furthermore, to ensure fairness as much as possible, we evaluated their consistency with two different reward models: reward1 \footnote{https://huggingface.co/OpenAssistant/oasst-rm-2-pythia-6.9b-epoch-1} and reward2 \footnote{https://huggingface.co/OpenAssistant/oasst-rm-2.1-pythia-1.4b-epoch-2.5}.These results are presented in Figure \ref{fig:consistency}.
The outcomes are depicted in Figure \ref{fig:consistency}.

% Kendall-Tau ($\tau$) \cite{kendall1938new} is a statistic used to measure the ordinal association between two measured data:
% \begin{equation}
%     \tau =\frac{Concordant-Discordant}{Concordant + Discordant} 
% \end{equation}
% where Concordant indicates the number of occurrences that two evaluation data $M^1$ and $M^2$ exist either both \(M_i^1\) > \(M_j^1\) and \(M_i^2\) > \(M_j^2\) or both  \(M_i^1\) < \(M_j^1\) and \(M_i^2\) < \(M_j^2\), and Discordant indicates the number of occurrences opposite to $Concordant$.
% Pearson R (\(r_p\)) is a measure of linear correlation:
% \begin{equation}
%     r_s=\frac{cov(M^1,M^2)}{\sigma _{M^1}\sigma _{M^2}}  
% \end{equation} 

% Spearman R (\(r_s\)) is a nonparametric measure of rank correlation (statistical dependence between the rankings of two data):
% \begin{equation}
%     r_s=\frac{cov(R(M^1),R(M^2))}{\sigma _{R(M^1)}\sigma _{R(M^2)}}  
% \end{equation}
% where $M^1$ and $M^2$ represents two evaluation datas. \(R(M^1)\) and \(R(M^2)\) represent the rankings of \(M^1\) and \(M^2\), cov(·, ·) means the covariance function, and \(\sigma_M\) means the standard deviation of $M$.

\textbf{PrefHit and PrefRecall are strongly consistent with classic metrics.}
Although there are slight differences in the consistency distribution under different hyperparameter settings, a clear strong positive correlation is observed. Most of the \(Pearson\) correlations are above 0.8, and even reach 1. Most of the \(Spearman\) correlations are above 0.6, and also reach 1. The results are shown in Figure \ref{fig::hit3_cons_bs}, Figure \ref{fig::recall2_cons_bs}, and Figure \ref{fig::recall4_cons_bs}.

\textbf{The consistency is independent of hyperparameter across different reward models.}
As can be seen from each column in Figure \ref{fig:consistency}, the consistency scores of \(Reward1\) and \(Reward2\) are almost identical. Although there are some differences in the third column as shown in Figure \ref{fig:consistency}(c,f and i), the distribution of these differences is nearly the same, indicating that the new metrics are not only unaffected by the type of reward model, but also that their performance across different reward models is independent of hyperparameters.

\textbf{The consistency of semantic metrics is similar to that of preference metrics.}  
The consistency between the new metrics, BLEU, and Rouge is almost identical to their consistency with Reward, indicating that as preference alignment increases, SeAdpra improves in semantic accuracy. This demonstrates SeAdpra's robustness across various metrics.

\subsubsection{Transferability and Adaptability}

\textbf{PrefHit and PrefRecall are applicable to the general CoQA.}
PrefHit and PrefRecall are not specifically tailored for the code dataset we contributed.
They are applicable for evaluating CoQA on any topic, such as chemistry, mathematics, and cooking. 
As shown in the visual results in Figure \ref{fig::public}(a,b and d), the performance distributions of PrefHit, PrefRecall, and Reward are quite similar across different domains. 
Additionally, our SeAdpra consistently outperforms the strong list-wise supervised preference ranking benchmark PRO on all metrics.

\textbf{PrefHit and PrefRecall can be transferred to other attribute alignments, such as safety alignment.}
As long as there is a preference order on a certain attribute of the response, such as the \(safer\_response\_id\) in Figure \ref{fig::pkusafe}, PrefHit and PrefRecall can be transferred to evaluate the alignment of the corresponding attribute, such as SaferHit and SaferRecall. 
Since the safety alignment dataset PKU-SafeRLHF only has two candidate responses, SaferHit is equal to SaferRecall, so we only present SaferHit in the Table \ref{tab:safe64} and Table \ref{tab:safe32}.