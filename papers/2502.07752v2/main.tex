%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for 
\usepackage{hyperref}
% \usepackage{tocappendix}
\PassOptionsToPackage{sort}{natbib}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[final]{neurips_2022}

% % For theorems and such
% \usepackage{amsmath}
% \usepackage{amssymb}
% \usepackage{mathtools}
% \usepackage{amsthm}
\input{math_commands}

\usepackage{ulem}
\usepackage{pifont}

% if you use cleveref..


\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage{amsmath}


\usepackage[textsize=tiny]{todonotes}



% \usepackage{bm}

\input{glossary}

\usepackage[capitalize,noabbrev]{cleveref}
% \newcommand{\theHalgorithm}{\arabic{algorithm}}

\renewcommand{\algorithmiccomment}[1]{\hfill\(\triangleright\){\textcolor{blue}{\textit{#1}}}}

\newcommand{\TopComment}[1]{%
    \Statex \textcolor{blue}{\textit{#1}}
}

% \crefname{proposition}{proposition}{propositions}
% \Crefname{proposition}{Proposition}{Propositions}




\title{Towards Efficient Optimizer Design for LLM via Structured Fisher Approximation with a Low-Rank Extension}

\author{%
Wenbo Gong$^*$ \\
Microsoft Research \\
\texttt{wenbogong@microsoft.com} \\
\And
Meyer Scetbon$^*$ \\
Microsoft Research \\
\texttt{t-mscetbon@microsoft.com} \\
\And
Chao Ma$^*$ \\
Microsoft Research \\
\texttt{chao.ma@microsoft.com} \\
\And
Edward Meeds \\
Microsoft Research \\
\texttt{edward.meeds@microsoft.com} \\
}


\crefname{equation}{Eq.}{Eqs.}
\Crefname{equation}{Eq.}{Eqs.}
\crefname{appendix}{App.}{Apps.}   % For lowercase \cref
\Crefname{appendix}{App.}{Apps.}   % For capitalized \Cref

\crefname{figure}{Fig.}{Figs.}     % For lowercase \cref
\Crefname{figure}{Fig.}{Figs.}     % For capitalized \Cref
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Sec.}{Secs.}









\begin{document}

\maketitle


\def\thefootnote{*}\footnotetext{These authors contributed equally to this work.}
% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.



\begin{abstract}
Designing efficient optimizers for large language models (LLMs) with low-memory requirements and fast convergence is an important and challenging problem. 
This paper makes a step towards the systematic design of such optimizers through the lens of structured \gls{fim} approximation. We show that many state-of-the-art efficient optimizers can be viewed as solutions to \gls{fim} approximation (under the Frobenius norm) with specific structural assumptions. Building on these insights, we propose two design recommendations of practical efficient optimizers for LLMs, involving the careful selection of structural assumptions to balance generality and efficiency, and enhancing memory efficiency of optimizers with general structures through a novel low-rank extension framework. We demonstrate how to use each design approach by deriving new memory-efficient optimizers: \gls{ssgd} and \gls{alice}. Experiments on LLaMA pre-training (up to 1B parameters) validate the effectiveness, showing faster and better convergence than existing memory-efficient baselines and Adam with little memory overhead. Notably, \gls{alice} achieves better than $2\times$ faster convergence over Adam, while \gls{ssgd} delivers strong performance on the 1B model with SGD-like memory.
\end{abstract}

\section{Introduction}
\input{introduction}
\section{Preliminaries}
\label{sec: preliminary}
\input{Preliminary/notations}
\input{Preliminary/fisher_info}
\section{Structural Approximation to \gls{fim}}
\input{Methodology/UFE}
\section{\gls{alice}: memory-efficient optimizer from low-rank extension framework}
\input{Methodology/memory_efficient_opt}
\input{related}

\section{Experiments}
\input{Experiments/experiments}
\input{conclusion}




% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\clearpage
\newpage
% \input{sections/impact_statement}
\bibliography{example_paper}
\bibliographystyle{plainnat}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
% \appendix
% \onecolumn
% \input{sections/appendix}

\appendix
\onecolumn

\input{Appendix/glossary}
\input{Appendix/Background}
\input{Appendix/Derivation_update_rule}
\input{Appendix/UFE_proof}
\input{Appendix/Algorithms}
\input{Appendix/discussion}
\input{Experiments/experiment_details}



\end{document}

