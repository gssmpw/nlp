\subsection{Basic notations and setup}
\label{subsec: notations and setups}
Throughout the paper we consider  $2$D matrix parameters $\mW$ (i.e. layer weights) of size $m\times n$ and $m\leq n$; the operation $\vect(\cdot)$ vectorizes the input matrix by stacking its columns; $\devect(\cdot)$ is the inverse of $\vect(\cdot)$ and reshapes the vector back into a matrix. We use $\mathcal{L}_\theta$ as the loss function where $\theta = \vect(\mW)$. 
$\mG=\nabla_{\mW}\mathcal{L}$ is the matrix gradient and $\vecg$ is the vectorized gradient, i.e.~$\vecg = \vect(\mG)$. $\vg_i$ denotes the $i^{\text{th}}$ column of $\mG$. In the paper, we assume $\mW$ are parameters of one layer. $\otimes$ indicates the Kronecker product. By default, we use $\mM\elesquare$ and $\sqrt{\mM}$ to denote the element-wise square and square-root of matrix, and $\mM^2$ and $\mM^{\frac{1}{2}}$ indicate the matrix square and square-root by default. For vectors $\vv$, both $\vv^{2}$ and $\vv^{\frac{1}{2}}$ represent element-wise operations. 
$\eig(\mM,r)$ performs the \gls{evd} and keeps the top $r$ eigenvectors ordered by the descending eigenvalues. If $r$ is omitted, we keep all eigenvectors. $\qr(\mM)$ with orthonormal $\mM\in\Rmr$ obtains the remaining $m-r$ orthogonal basis via QR decomposition.

We also introduce the following diagonal operations:
$\diag(\mM)$ will extract the diagonals of $\mM$ to a vector. $\diagb(\mM_1,\ldots,\mM_n)$ will stack the input sequence of matrices to form a larger block diagonal matrix. $\diagv(\vv)$ will expand the input vector $\vv$ to a pure diagonal matrix. $\diagm(\mM)$ will expand the input $\mM$ to a larger pure diagonal matrix by stacking the element of $\mM$ in a column-wise order. We demonstrate the above operations with examples in \cref{app: example diagonal operations}.
Note that results in this paper may involve the expectation $\E[\cdot]$ of some quantities, we assume the use of an \gls{ema} as its practical estimation. 