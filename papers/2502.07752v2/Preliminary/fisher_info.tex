\subsection{Fisher information and natural gradient descent}
\label{subsec: fisher info}
Fisher information is a fundamental concept in statistics that measures the amount of information a random variable carries about a parameter of interest. In this paper, we ignore dependence between layers and treat each layer independently. 
Under the context of LLMs, with the vectorized mini-batched gradient of one layer $\vecg$, we define the empirical \gls{fim} for that layer as $\mF = \E[\vecg\vecg^T]$.
\Gls{ngd} leverages the inverse of \gls{fim} to smooth the local geometry to improve convergence and stabilize training \citep{martens2020new}. 
In practice, the square-root inverse of \gls{fim} may be preferred due to its properties and improved performance on certain optimization problems \citep{yang2008principal, lin2024can, loshchilov2016sgdr, bergstra2012random, choi2019empirical}.  
The corresponding update of $\mW$ is:
\begin{align}
    \mW\leftarrow \mW-\lambda \devect(\mF^{-\frac{1}{2}}\nabla_{\theta}\mathcal{L}).
    \label{eq: square root ngd}
\end{align}
Due to the large size of $\mF\in\R^{mn\times mn}$, computing its inverse is a significant impediment to applying this update rule in practice. One way to address this is to enforce certain structures to approximate \gls{fim}. In this paper, we consider two main classes of structures: block diagonal and Kronecker product matrices. They possess favorable properties that can significantly reduce computational complexity. \Cref{subapp: kronecker product and block diagonals} provides a brief introduction and summary of their key properties. We also include a comprehensive background in \cref{app: background} to be more self-contained.