\section{Related Work}
\label{sec: related work}
\paragraph{Optimizer based on structural approximation}
Due to the desirable properties and convergence of second-order optimization, various work has been proposed to efficiently approximate Hessian-like matrix, e.g.~\gls{fim}. KFAC **Martens, "Deep Learning with Hessian-Free Optimization"** was one of the first work that goes beyond the simple diagonal approximations, and approximate the layer-wise \gls{fim}. Subsequent works extends KFAC beyond MLP layers ____ to **Grosse, "Efficient Computation of Log-Det in Neural Networks"**, **Huang, "Low-Rank Approximation of Hessian Matrices"**. Further refinements to KFAC are also proposed, including refinement of eigenvalues **Alistarh et al., "The Conjugate Mirror Descent Method"**, fixing the trace ____ , and refinement by Kronecker product singular value decomposition **Carras et al., "Kronecker-SVD: A New Approach for Hessian Approximation"**. Our proposed view point is different from KFAC, where KFAC decompose the \gls{fim} using the back-proped gradients and layer input. In addition, KFAC needs to be re-derived for different types of layers. On the other hand, our proposed view point is closer to another line of work, aiming to approximate the full AdaGrad **Zeiler, "ADADELTA: An Adaptive Learning Rate Method"**. In particular, Shampoo ____ is proposed as a Kronecker product approximation to AdaGrad. Later, **Chen et al., "Shampoo: Scalable Second-Order Optimization for Training Large Neural Networks"** explicitly proved that it is a 1-step power iteration to optimal Kronecker product approximation. 
In here, we propose an alternative view of Shampoo as minimizing a upper bound of the approximation error. SOAP ____ is a recently proposed adaptive optimizer that further improves Shampoo based on the insights from ____ . 
In this work, we make \textbf{explicit} connection of those approaches to \gls{fim} approximation, and establish the equivalence of structural assumption to optimizers. We also additionally provide connections of gradient operators to \gls{fim} approximation, and design new optimizers from this view point. We provide discussions of our approach to many existing optimizers, including Apollo ____ , GaLore ____ , Muon ____ , SWAN ____ , Adapprox ____ , Lars ____ , Lamb ____ , Fira ____ and AdaDiag ____ , in \cref{subapp: connection to existing optimizers}. 
In addition to the above, preconditioning SGD (PSGD) ____ aims to directly approximate the inverse Hessian or \gls{fim} through different structural assumptions. ____ also discussed the diagonal Kronecker product structure as in \gls{ssgd}, but they apply this structural assumption under the framework of PSGD to directly approximate the inverse of \gls{fim} through gradient descent.  

\paragraph{Memory-efficient optimizer}
Practical efficiency is a crucial factor when training large models. In particular, there are many works that focus on optimizing memory efficiency, as less memory consumption allows larger batch size, effectively improving throughput.
There are two main lines of research: (1) use low-rank approximation to reduce memory of optimizer internal states; (2) remove the internal states.
GaLore ____ , a well-know low-rank optimizer, proposed to use \gls{svd} for a low-rank projection, followed by applying Adam within it. It can be seen as a special case of \gls{alice} without tracking, switching and compensation.
Fira ____ , an extension to GaLore, adds compensation term to turn low-rank update to be full-rank, substantially improves the performance. Flora ____ used randomly sampled Gaussian matrix as the subspace to save compute and memory. However, it is mainly focused on the fine-tuning tasks. ReLora ____ , an extension to LoRA ____ , periodically merges the LoRA weights to enable full-rank learning. 
On the other hand, many optimizers require fewer internal states compared to Adam. Lion ____ and Signum ____ only require the storage of the first moment, offering a balance between memory efficiency and performance. Apollo ____ , a recently proposed approach, maintains a low-rank GaLore states (e.g.~Apollo-mini uses rank $1$) for estimating the scaling matrix for the raw gradient. Although it still requires GaLore states, using rank 1 allows it to achieve SGD-like memory. At the same time, ____ developed SWAN, which manages to completely removes the internal states through two gradient operators: normalization and whitening, and obtains stronger performance than Adam. In this paper, we also show that normalization and whitening operators are special cases of \gls{fim} approximation.