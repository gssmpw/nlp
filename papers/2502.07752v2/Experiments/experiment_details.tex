\section{Experiment details}
\label{app: experiment details}
In this section, we will include the detailed setup, hyperparameters and additional experiment results. 

\subsection{Implementation details of baselines}
\label{subapp: baseline implementation}
\paragraph{GaLore} We leveraged the official GaLore package released by the author (\url{https://github.com/jiaweizzhao/GaLore}). 
\paragraph{Fira} Since the main difference compared to GaLore is the additional compensation term, we follow the implementation of the official Fira repo (\url{https://github.com/xichen-fy/Fira}) and add the compensation to GaLore. 
% \paragraph{Apollo} We reproduce Apollo's pre-training performance using their official repo (\url{https://github.com/zhuhanqing/APOLLO}).

\subsection{Experiment setup for pretraining LLaMA}
\label{subapp: pretrain experiment setup}
For all model parameters, optimizer states, we use BF16 format.
We use context length of $256$ and batch size of $128$ with $4$ gradient accumulations, apart from $60$M and $1.3$B (batch size $256$ with $2$ gradient accumulations). \gls{ssgd} and \gls{alice} are applied to all linear modules of attention and MLPs, others are optimized with Adam.
We use the first $10\%$ of the total training steps as the warm-up, followed by a cosine learning rate decay to $10\%$ of the original learning rate. For hyperparameters, we perform a grid search on our proposed \gls{ssgd}, \gls{alice} as well as GaLore, Fira and full-rank Adam. For other methods, we directly cite their results in \citep{zhu2024apollo}. For Adam, we use $0.9$ and $0.999$ for $\beta_1$ and $\beta_2$, and enable the bias correction without weight decay. \Cref{tab:galore fira hyperparameter} summarizes the hyperparameters used for both GaLore and Fira. \Cref{tab:hyperparameters for adam} summarizes the hyperparameters for Adam optimizer. \Cref{tab:hyperparameters for ssgd} and \cref{tab: alice hyperparameters} summarizes the hyperparameters used for \gls{ssgd} and \gls{alice}, respectively. In addition, for all methods with Fira limiter, we use threshold $\gamma=1.01$ as suggested in \cite{chen2024fira}. For \gls{ssgd}, we use Adam to train the last layer of LLaMA, following the same setup as Apollo for fair comparison. In summary, \gls{alice}, GaLore and Fira do not use Adam to train the last layer to fully test the capabilities of low-rank methods, whereas Apollo, \gls{ssgd} and Adam utilize Adam for last layer. 
The total training steps for $60$M, $130$M, $350$M and $1.3$B are $10$K, $20$K, $60$K and $100$K, respectively. These correspond to $1.1$B, $2.6$B, $7.8$B and $13.1$B training tokens, summarized in \cref{tab: model architecture}. All experiments are conduced on NVIDIA A100 GPUs.

\begin{table}[ht]
\centering
\begin{minipage}{0.45\textwidth}
    \centering
    \caption{The hyperparameters for GaLore and Fira}
    \label{tab:galore fira hyperparameter}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|llll}
    \hline
         & learning rate & update scale & rank & update interval \\ \hline
    60M  & 0.02          & 0.3          & 128  & 200          \\
    130M & 0.02          & 0.3          & 256  & 200          \\
    350M & 0.02          & 0.3          & 256  & 200          \\
    1.3B & 0.01          & 0.25         & 512  & 200          \\ \hline
    \end{tabular}}
\end{minipage}
\hfill
\begin{minipage}{0.45\textwidth}
    \centering
    \caption{The hyperparameters used for Adam optimizer.}
    \label{tab:hyperparameters for adam}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|llll}
    \hline
         & learning rate & $\beta_1$ & $\beta_2$ & correct bias \\ \hline
    60M  & 0.001         & 0.9       & 0.999     & True         \\
    130M & 0.001         & 0.9       & 0.999     & True         \\
    350M & 0.001         & 0.9       & 0.999     & True         \\
    1.3B & $7\times 10^{-4}$ & 0.9   & 0.999     & True         \\ \hline
    \end{tabular}}
\end{minipage}
\end{table}

\begin{table}[ht]
\begin{minipage}{0.3\textwidth}
    \centering
    \caption{The hyperparameters for \gls{ssgd}.}
    \label{tab:hyperparameters for ssgd}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|lll}
    \hline
         & learning rate & $\beta$ & scale $\alpha$  \\ \hline
    60M  & 0.02         & 0.9       & 0.05             \\
    130M & 0.02       & 0.9       & 0.05              \\
    350M & 0.02         & 0.9       & 0.05             \\
    1.3B & 0.02 & 0.9   & 0.02           \\ \hline
    \end{tabular}}
\end{minipage}\hfill
\begin{minipage}{0.6\textwidth}
    \centering
\caption{Model architectures and training steps}
\label{tab: model architecture}
\resizebox{\textwidth}{!}{
\begin{tabular}{l|llllll}
\hline
     & Hidden & Intermediate & Heads & Layers & Steps & Data amount \\ \hline
60M  & 512    & 1376         & 8     & 8      & 10K   & 1.3B        \\
130M & 768    & 2048         & 12    & 12     & 20K   & 2.6B        \\
350M & 1024   & 2736         & 16    & 24     & 60K   & 7.8B        \\
1.3B & 4096   & 5461         & 24    & 32     & 100K  & 13.1B       \\ \hline
\end{tabular}}
\end{minipage}
\end{table}

\begin{table}[]
\centering
\caption{The hyperparmeters for \gls{alice} optimizer}
\label{tab: alice hyperparameters}
\resizebox{\textwidth}{!}{
\begin{tabular}{l|lllllllll}
\hline
     & learning rate & scale $\alpha$ & compensation scale $\alpha_c$ & $\beta_1$ & $\beta_2$ & $\beta_3$ & update interval $K$ & rank $r$ & leading basis number $l$ \\ \hline
60M  & 0.02          & 0.3            & 0.4                           & 0.9      & 0.9      & 0.999    & 200                 & 128      & 40                       \\
130M & 0.02          & 0.3            & 0.4                           & 0.9      & 0.9      & 0.999    & 200                 & 256      & 40                       \\
350M & 0.02          & 0.3            & 0.4                           & 0.9      & 0.9      & 0.999    & 200                 & 256      & 40                       \\
1.3B & 0.02          & 0.25           & 0.2                          & 0.9      & 0.9      & 0.999    & 200                 & 512      & 160                      \\ \hline
\end{tabular}}
\end{table}

\subsection{Setup of 1B v.s. 7B}
\label{subapp: setup 1B vs 7B}
We mainly follow the setup as \citet{zhu2024apollo}. For 1B model, we use 16 per-device batch size with 8xA100s and 4 gradient accumulations, which is 512 batch size in total, same as 7B model. For \gls{alice}, we use learning rate $0.02$, scale $\alpha=0.3$ and compensation scale $\alpha_c=0.2$ with rank $r=512$, leading basis number $l=160$. We also use full-rank Adam to train the last layer for better performance. For \gls{ssgd}, we follow the same setup of 1.3B as reported in \cref{tab:hyperparameters for ssgd}.
% For \gls{ssgd}, we follow the same setup as \cref{tab:hyperparameters for ssgd}. 
For memory estimation, we assume the use of BF16 format. 
For 8-bit optimizers, we assume weights are stored in BF16, but optimizer states use FP8. GaLore uses $r=1024$ for 7B model. 

\subsection{Memory estimation}
Following the setup of \cite{zhao2024galore}, we provide the estimated GPU memory for each optimizers due to the difficulty of directly measuring their practical memory consumption without considering the activations, internal storage for gradient, etc. We assume BF16 format, which consuming 2 Bytes per element. For example, 1273M parameters are optimized by \gls{alice} and 65M parameters are optimized by Adam for 1B model with rank $512$. Therefore, \gls{alice} part will consume $3.86$GB and Adam will consume $0.37$GB, summing to $4.42$GB for \gls{alice} optimizer.

We also report the actual memory footprint with BF16 format. We use token batch size of $25$, following the same setup as \cite{zhao2024galore}. "-layerwise" represents layer-wise training where we only store the gradient of the current back-propagated layer. 

\subsection{Throughput estimation}
\label{subapp: exp details throughput}
We report both the actual throughput and effective throughput. The effective throughput of a target optimizer compared to a reference optimizer is defined as the ratio of training tokens consumed from the target optimizer w.r.t total time consumption of reference optimizer when reaching the same evaluation loss of the reference optimizer at the end of training. Compared to the standard throughput, this considers the speed-up effect. 

\subsection{Additional pretrain results}
\label{subapp: additional pretrain results}
\Cref{fig: additional pretrain curve} presents additional training curves with 60M, 130M and 350M models sizes. For all cases, the proposed \gls{alice} and \gls{ssgd} outperforms the baselines with noticable margin, and achieves clear speed-ups compared to full-rank Adam.
\begin{figure}
\subfigure[]{
    \centering
    \includegraphics[scale=0.24]{fig/llama_60m_icml_pretrain.pdf}
    \label{fig:60M llama pretrain curve}
}\hfill
\subfigure[]{
    \centering
    \includegraphics[scale=0.24]{fig/llama_130m_icml_pretrain.pdf}
    \label{fig:130M llama pretrain curve}
}\\
\centering
\subfigure[]{
    \centering
    \includegraphics[scale=0.24]{fig/llama_350m_icml_pretrain.pdf}
    \label{fig:350M llama pretrain curve}
}
\caption{Additional LLaMA C4 pretrain performance curve. (a), (b) and (c) represents the 60M, 130M and 350M, respectively. "+lm head" represents that the last layer of LLaMA is trained by full-rank Adam.}
    \label{fig: additional pretrain curve}
\end{figure}

\begin{figure}
\centering
\subfigure[Absolute throughput]{
    \centering
    \includegraphics[scale=0.5]{fig/llama_actual_throughput.pdf}
    \label{fig:actual throughput}
}\\
\subfigure[Effective throughput]{
    \centering
    \includegraphics[scale=0.5]{fig/llama_effective_throughput.pdf}
    \label{fig:effective throughput}}
\caption{Throughput of various methods. (a) this reports the absolute throughput, representing the number of training token processed per second. (b) the effective throughput using Adam as the reference optimizer. This represents the absolute throughput adjusted by the speed-up factor. The effective throughput of GaLore and Fira is $0$ for some model sizes since they under-perform the Adam. }
    \label{fig: throughput plot}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[scale=0.5]{fig/llama_memory_footprint.pdf}
    \caption{The memory footprint of various optimizers. We use token batch size of $256$ following the same setup as \cite{zhao2024galore} under BF16 format. The suffix "layerwise" represents the memory consumption when enabling layerwise training so that only the gradient of the current layer is stored. }
    \label{fig:actual memory footprint}
\end{figure}

\subsection{Results for ablation studies}
\label{subapp: ablation results}
For all ablations, we consider 130M LLaMA model. Apart from the ablation on the last layer, we assume the last layer is trained by the candidate optimizer, rather than the full-rank Adam for thorough evaluation. 

\paragraph{Setup: ablation of tracking} We disable the compensation step, and verify the effect of low-rank tracking under our proposed switch strategy or purely replying on \gls{evd} (i.e.~no switching). The other hyperparameters follow the pre-training setup as \cref{subapp: pretrain experiment setup}. 

\paragraph{Setup: switch strategy}
For this setup, we enable low-rank tracking. Apart from Gaussian, we use $40$ as the leading basis number. The Gaussian distribution is a standard isotropic Gaussian with zero mean and unit variance. To make sure the norm of sampled Gaussian vectors are consistent with the real basis, we normalize those sampled vectors to have a unit $l_2$ norm. The same operation is also applied to Gaussian-mix. 

\paragraph{Setup: compensation strategy}
We enable the tracking and switching, but with different compensation terms. For Fira, we directly leverage the compensation proposed in \cite{chen2024fira}. Fira+ modifies original Fira by the following two steps: (1) rescale the Fira compensation to have the same $l_2$ norm as the \gls{alice} low-rank update (i.e.~first term in \cref{eq: alice update decomposition}); (2) multiply this compensation by a scale parameter like $\alpha_c$ in \cref{alg: alice optimizer}. We found this empirical trick boosts the performance of Fira. 


\paragraph{Effects of last layer}
One crucial setup difference during evaluation for low-rank methods is whether the last layer is trained by full-rank Adam or not. Most previous work train the last layer, arguably one of the most important layer \citep{zhao2024deconstructing}, using full-rank Adam. This effectively reduces the performance gap compared to full-rank method, and does not reveal their true capabilities. We investigate the effect of the last layer to \gls{alice} compared to GaLore and Fira. From the \cref{tab: pretrain performance}, we can see that for all model sizes, GaLore and Fira are greatly affected by this effect. Training last layer with full-rank Adam will boost their performance significantly. On the other hand, \gls{alice} is less impacted with marginally worse performance. For example, \cref{fig: effect of last layer} shows the training curve comparison with 130M model. When the rank is sufficiently large (e.g.~rank 128 is sufficient large for 60M model), \cref{fig:60M llama pretrain curve} shows that using Adam to train the last layer even decreases the performance. 
These serve as evidences that \gls{alice} is a better optimizer than GaLore and Fira, and has the potential to surpass full-rank Adam with large enough rank.  

\paragraph{Effect of \gls{ema} in \gls{ssgd}}
The only internal states inside \gls{ssgd} are the \gls{ema} tracking states of two vectors $\vs$ and $\vq$. We investigate the importance of \gls{ema} scheme. From \Cref{fig: effect of ema}, \gls{ssgd} without \gls{ema} performs much worse than \gls{ssgd}, suggesting the \gls{ema} is necessary for satisfactory performance of \gls{ssgd}. 


\begin{figure}
\subfigure[Effect of tracking]{
    \centering
    \includegraphics[scale=0.24]{fig/llama_130m_icml_ablation_tracking.pdf}
    \label{fig: effect of tracking}}\hfill
\subfigure[Effect of switching]{
    \centering
    \includegraphics[scale=0.24]{fig/llama_130m_icml_ablation_switch.pdf}
    \label{fig: effect of switch strategy}}\\
\subfigure[Effect of compensation]{
    \centering
    \includegraphics[scale=0.24]{fig/llama_130m_icml_ablation_compensation.pdf}
    \label{fig: effect of compensation}}\hfill
\subfigure[Effect of last layer. "+lm head" represents the last layer is trained by full-rank Adam.]{
    \centering
    \includegraphics[scale=0.24]{fig/llama_130m_icml_ablation_lm_head.pdf}
    \label{fig: effect of last layer}}\\
\centering
\subfigure[Effect of \gls{ema} in \gls{ssgd}.]{
    \centering
    \includegraphics[scale=0.24]{fig/llama_130m_icml_ablation_ssgd_no_ema.pdf}
    \label{fig: effect of ema}}
\caption{The pre-training curve to verify the effectiveness of the design choice. We consider 130M model size.}
\end{figure}

\begin{figure}
\subfigure[Index 1]{
    \centering
    \includegraphics[scale=0.45]{fig/cossim_eigen_space_1.pdf}
    \label{fig:cossim index 1}}\hfill
\subfigure[Index 2]{
\centering
    \includegraphics[scale=0.45]{fig/cossim_eigen_space_2.pdf}
    \label{fig:cossim index 2}
}\\
\subfigure[Index 4]{
\centering
    \includegraphics[scale=0.45]{fig/cossim_eigen_space_4.pdf}
    \label{fig:cossim index 4}
}\hfill
\subfigure[Index 8]{
\centering
    \includegraphics[scale=0.45]{fig/cossim_eigen_space_8.pdf}
    \label{fig:cossim index 8}
}
    \caption{The cosine similarity between eigenvectors per 200 steps. Since the update interval of subspace is $200$ steps, this essentially compare the similarity before and after updating the projection $\mU$ with a certain index. We always arrange the eigenvectors in a descending order based on the eigenvalues. }
    \label{fig: eigenspace cosine similiary}
\end{figure}



