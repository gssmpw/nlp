\label{sec: experiments}
We include all setup details along with additional experiment results in \cref{app: experiment details}.

\subsection{Pretraining LLaMA with C4 dataset}
\label{sbusec: experiment pretrain C4} 
\paragraph{Setup}
We evaluate the proposed \gls{ssgd}, \gls{alice} and its variant \gls{alicez} on pre-training LLaMA \citep{touvron2023llama} with the C4 dataset \citep{raffel2020C4}. We train the following model sizes: $60$M, $130$M, $350$M and $1.3$B using a similar setup as \citet{zhao2024galore, zhu2024apollo}. For baselines, we consider GaLore, Fira, Apollo-mini, Apollo-svd and Adam.
An important consideration in our experiments is that \textbf{all previous low-rank methods rely on full-rank Adam to train the last layer, which is arguably one of the most important layers} \citep{zhao2024deconstructing}. To thoroughly assess their effectiveness, we report performance for both cases when evaluating low-rank methods—training the last layer with and without Adam—but prioritize the latter as the main evaluation criterion. For full-rank methods (i.e.~\gls{ssgd}, Apollo-mini, Apolli-svd and Adam), we assume the last layer is trained by Adam. 

\begin{table}[]
\caption{LLaMA pretraining performance. Ppl.~is the evaluation perplexity when the last layer is not trained by Adam; and Ppl.* is when the last layer is trained by Adam. For Adam, we report both our reproduced performance and perplexity cited from \citet{zhu2024apollo}. We also cite Ppl.* of other baselines from \citet{zhu2024apollo}. 
The speed-up is measured against Adam in terms of training steps. The TP is the number of training tokens processed per second, and effective TP is total token used by Adam divided by time used by the candidate optimizer to reach the same final eval ppl.~of Adam.}
\label{tab: pretrain performance}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{c|cccccccc}
\hline
Methods                         & \multicolumn{2}{c}{60M}          & \multicolumn{2}{c}{130M}         & \multicolumn{2}{c}{350M}         & \multicolumn{2}{c}{1.3B}         \\ \hline
                                & Ppl.            & Ppl.*          & Ppl.             & Ppl.*         & Ppl.             & Ppl.*         & Ppl.             & Ppl.*         \\
Adam                            & NA              & 33.94         & NA               & 25.03         & NA               & 19.24         & NA               & 16.44         \\ 
Adam (cited) & NA & 34.06 & NA & 25.08 & NA&18.80&NA&15.56\\
\hline
GaLore                          & 38.91           & 34.88          & 27.18            & 25.36         & 21.11            & 18.95         & 16.60            & 15.64         \\
Fira                            & 33.77           & 31.06          & 25.21            & 22.73         & 18.93            & 17.03         & 15.14            & 14.31         \\
Apollo-mini                     & NA              & 31.93          & NA               & 23.84         & NA               & 17.18         & NA               & 14.17         \\
Apollo-svd                      & NA              & 31.26          & NA               & 22.84         & NA               & 16.67         & NA               & 14.10         \\ \hline
RACS                            & NA              & 30.25          & NA               & 22.67         & NA               & \textbf{16.51}         & NA               & \textbf{13.52}         \\
Alice-0                         & \textbf{28.83}           & 29.74          & \textbf{21.99}            & 22.43         & \textbf{16.66}            & \textbf{16.43}         & \textbf{13.97}            & \textbf{13.47}         \\
Alice                           & \textbf{28.69}           & 29.33          & \textbf{21.95}            & \textbf{21.79}         & \textbf{16.61}            & \textbf{16.37}         & \textbf{13.85}            & \textbf{13.52}         \\ \hline
Speed-up in steps (Alice)       & \multicolumn{2}{c}{2.22x}        & \multicolumn{2}{c}{2.00x}        & \multicolumn{2}{c}{2.45x}        & \multicolumn{2}{c}{2.82x}        \\
Throughput(TP)/Effect TP (Adam) & \multicolumn{2}{c}{97748/97748}  & \multicolumn{2}{c}{82247/82247}  & \multicolumn{2}{c}{63139/63139}  & \multicolumn{2}{l}{53588/53588}  \\
Throughput(TP)/Effect TP(Alice) & \multicolumn{2}{c}{92589/\textbf{202058}} & \multicolumn{2}{c}{71583/\textbf{141148}} & \multicolumn{2}{l}{58847/\textbf{143088}} & \multicolumn{2}{c}{45523/123048} \\
Throughput(TP)/Effect TP (RACS) & \multicolumn{2}{c}{98238/162423} & \multicolumn{2}{c}{73233/123116} & \multicolumn{2}{c}{55970/131372} & \multicolumn{2}{c}{47488/\textbf{129817}} \\ \hline
\end{tabular}}
\end{table}


\begin{table}[]
\caption{Memory consumption estimation. The reported memory is the combined consumption of: weight parameters; Adam optimizer states for non-matrix parameters; and candidate optimizer states for matrix parameters. Mem.* represent the consumption when the last layer is trained by Adam. }
\label{tab: memory consumption}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{c|cccccccc}
\hline
Methods     & \multicolumn{2}{c}{60M} & \multicolumn{2}{c}{130M} & \multicolumn{2}{c}{350M} & \multicolumn{2}{c}{1.3B} \\ \hline
            & Mem.       & Mem.*      & Mem.        & Mem.*      & Mem.        & Mem.*      & Mem.        & Mem.*      \\
Adam        & NA         & 0.32G      & NA          & 0.75G      & NA          & 2.05G      & NA          & 7.48G      \\ \hline
GaLore      & 0.21G      & 0.26G      & 0.51G       & 0.57G      & 1.2G        & 1.29G      & 4.25G       & 4.43G      \\
Fira        & 0.21G      & 0.26G      & 0.51G       & 0.57G      & 1.2G        & 1.29G      & 4.25G       & 4.43G      \\
Apollo-mini & NA         & 0.23G      & NA          & 0.43G      & NA          & 0.93G      & NA          & 2.98G      \\
Apollo-svd  & NA      & 0.26G      & NA       & 0.57G      &NA       & 1.29G      & NA       & 4.43G      \\ \hline
RACS        & NA         & 0.23G      & NA          & 0.43G      & NA          & 0.93G      & NA          & 2.98G      \\
Alice-0     & 0.21G      & 0.26G      & 0.51G       & 0.57G      & 1.2G        & 1.29G      & 4.25G       & 4.44G      \\
Alice       & 0.22G      & 0.26G      & 0.53G       & 0.59G      & 1.24G       & 1.33G      & 4.42G       & 4.6G       \\ \hline
\end{tabular}}
\end{table}

\begin{table}[]
\centering
\caption{Eval ppl.~of 1B v.s. 7B LLaMA at different training steps. For Apollo, Apollo-mini, 8-bit Adam and Galore, we cite the number from \citet{zhu2024apollo}.}
\label{tab: 7B preformance}
\begin{tabular}{l|l|llll}
\hline
Method           & Mem.   & 40K & 80K & 120K & 150K \\ \hline
8-bit Adam (7B)&26G &18.09 & 15.47 & 14.83 & 14.61  \\
8-bit Galore (7B) &18G & 17.94 & 15.39 &14.95 &14.65\\
Apollo (7B)      & 15.03G &  17.55   &  14.39   &  13.23    &   13.02   \\
Apollo-mini (7B) & 13.53G &  18.03   &  14.60   &  13.32    &  13.09    \\\hline
\gls{ssgd} (1B) & 2.98G & 16.43 &14.26& \textbf{13.20}& \textbf{13.01}\\
Alice (1B)       & 4.6G   &  \textbf{15.93}   &  \textbf{14.08}   & \textbf{13.15}     & \textbf{12.97}     \\
% RACS (1B)        & 2.98G  &  16.39   & 14.30    &      &      \\
\hline
\end{tabular}
\end{table}




\paragraph{Main results} \Cref{tab: pretrain performance} reports the pretraining performance in terms of evaluation perplexity, and \cref{tab: memory consumption} summarizes the corresponding estimated memory consumption. Our proposed \gls{ssgd} and \gls{alice} outperforms the other memory-efficient baselines and full-rank Adam consistently across various model sizes. \gls{alice} and \gls{alicez} perform on par with each other, suggesting \gls{alicez} may be preferred for better memory efficiency. One major advantage of \gls{alice} is its fast convergence compared to Adam, and achieves more than $2\times$ speed-ups across model sizes, while maintaining similar memory consumption as GaLore. \Cref{fig:1b pretrain curve} demonstrate this fast convergence of the evaluation perplexity during training for the 1B model. 
Despite the simplicity of \gls{ssgd}, it performs well for 350M and 1.3B model, and surpasses the other scaling-based baseline, Apollo-mini and Apollo-svd, consistently. 
From the throughput (TP), we can observe \gls{alice} and \gls{ssgd} are not significantly slower than Adam with $15\%$ and $11\%$ drop for 1B model, respectively. 
Considering the speedup effect, we report the effective TP to represent how quickly the optimizers reach a target loss in wall-clock time. \gls{alice} and \gls{ssgd} achieve $123048$ and $129817$ with 1B model, respectively, compared to $53588$ for Adam, resulting in more than 2× faster convergence in wall-clock time to reach Adam’s final evaluation perplexity.

For Adam, since we cannot reproduce the exact number reported in \citet{zhu2024apollo}. For fair comparison, we also cite the perplexities of Adam from \citet{zhu2024apollo}, and compute the corresponding speed-ups in terms of steps. \gls{alice} achieves $2.22$x, $2.11$x, $2.18$x, and $2.15$x for 60M, 130M, 350M and 1B models, respectively.  
For the actual memory footprints and additional training curves, see \cref{subapp: additional pretrain results}.
\begin{figure}
    \centering
    \includegraphics[scale=0.3]{fig/llama_1b_icml_pretrain.pdf}
    \caption{1B LLaMA C4 pretraining evaluation ppl.~curve. "+lm head" represents the last layer is trained by full-rank Adam.}
    \label{fig:1b pretrain curve}
\end{figure}
\paragraph{1B v.s.~7B LLaMA}
To further demonstrate the effectiveness, we follow the setup of \citet{zhu2024apollo}, but train a 1B LLaMA with \gls{alice} and \gls{ssgd} to compare with 7B LLaMA trained by Apollo, Apollo-mini, 8-bit Adam and 8-bit Galore. \Cref{tab: 7B preformance} shows that 1B model trained with our proposed optimizers consistently outperforms 7B model at different training checkpoints with less memory cost\footnote{The total memory cost consists of (1) model parameters; (2) Adam optimizer cost for non-matrix parameters; (3) candidate optimizer cost for matrix parameters. Here, we report the combined cost.}. To complete 150K steps with 8xA100 GPUs, Apollo requires \textbf{15} days while \gls{alice} and \gls{ssgd} require around \textbf{3.8} days.

\subsection{Ablation: Effectiveness of the design choice}
\label{subsec: exp ablation study}

\paragraph{Effect of low-rank tracking}
First, we verify whether low-rank tracking (\cref{eq: tracking step}) is beneficial and whether our conjecture about the stability of the leading basis holds. As shown in \cref{tab: pretrain performance}, \gls{alicez} performs on par with \gls{alice}, suggesting that tracking does not provide a significant boost. However, \cref{fig: effect of tracking} indicates that tracking is helpful when compensation is disabled and must be used alongside switching. Without switching, tracking leads to inferior performance due to the stability of the eigenspace. \Cref{fig: eigenspace cosine similiary} supports this conjecture by showing the cosine similarity before and after updating 
$\mU$ every $200$ steps, confirming that tracking contributes to the stability of the leading basis.

\paragraph{Switching strategy}
We evaluate the effectiveness of our theory-inspired switching strategy compared to other heuristics. The considered alternatives are: (1) \textbf{Gaussian}: $\mU$ is sampled from a Gaussian distribution; (2) \textbf{Gaussian mix}: the leading basis is mixed with vectors sampled from a Gaussian matrix; (3) \textbf{full basis}: instead of sampling the $r-l$ basis in \cref{alg: subspace switching} solely from $m-r$ complement basis, they are sampled jointly from the entire basis excluding the top $l$ leading eigenvectors, i.e.~$[\mU,\mU_c]\backslash {\mU_{:,:l}}$. As shown in \Cref{fig: effect of switch strategy}, our strategy outperforms these alternatives, particularly the Gaussian-based approaches. One possible reason is that the orthogonality of the complement basis ensures a more effective switch, whereas randomly sampled Gaussian vectors may introduce overlaps between switches.

\paragraph{Compensation strategy}
The closest work to our compensation step is Fira \citep{chen2024fira}, which introduces a heuristic-based compensation term. To evaluate the effectiveness of our optimal compensation, we compare it against Fira and a no-compensation baselines by integrating these alternatives into \gls{alice}. Additionally, we introduce Fira+, our proposed modification that further enhances Fira’s performance. 
As shown in \Cref{fig: effect of compensation}, our optimal compensation achieves better performance and convergence than Fira-based compensations, with only a small additional sublinear memory cost (i.e., $n$). Compared to the no-compensation, all strategies yield noticeable performance improvements. \Cref{tab: effectiveness of each component in alice} summarizes the contributions of each component.

\begin{table}[]
\centering
\caption{Effectiveness of each components in \gls{alice} with 130M LLaMA.}
\label{tab: effectiveness of each component in alice}
\begin{tabular}{l|l}
\hline
      Components                       & Evaluation ppl. \\ \hline
No tracking, switch, compen. & 26.96           \\
Tracking                     & 27.35           \\
Tracking+Switch              & 25.11           \\
Tracking+Switch+Compen.      & 21.95           \\ \hline
\end{tabular}
\end{table}

\paragraph{Other ablations}
We also performed additional ablations, examining (1) the effect of the last layer and (2) the effect of \gls{ema} in \gls{ssgd}. For more details, see \cref{subapp: ablation results}.


