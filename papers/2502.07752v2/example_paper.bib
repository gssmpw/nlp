@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2021}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}
@article{song2022fast,
  title={Fast differentiable matrix square root and inverse square root},
  author={Song, Yue and Sebe, Nicu and Wang, Wei},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={45},
  number={6},
  pages={7367--7380},
  year={2022},
  publisher={IEEE}
}
@article{martens2020new,
  title={New insights and perspectives on the natural gradient method},
  author={Martens, James},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={146},
  pages={1--76},
  year={2020}
}
@article{tieleman2012rmsprop,
  title={Rmsprop: Divide the gradient by a running average of its recent magnitude. coursera: Neural networks for machine learning},
  author={Tieleman, Tijmen and Hinton, Geoffrey},
  journal={COURSERA Neural Networks Mach. Learn},
  volume={17},
  year={2012}
}
@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}
@article{lin2024can,
  title={Can we remove the square-root in adaptive gradient methods? a second-order perspective},
  author={Lin, Wu and Dangel, Felix and Eschenhagen, Runa and Bae, Juhan and Turner, Richard E and Makhzani, Alireza},
  journal={arXiv preprint arXiv:2402.03496},
  year={2024}
}
@article{loshchilov2016sgdr,
  title={Sgdr: Stochastic gradient descent with warm restarts},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1608.03983},
  year={2016}
}
@article{bergstra2012random,
  title={Random search for hyper-parameter optimization.},
  author={Bergstra, James and Bengio, Yoshua},
  journal={Journal of machine learning research},
  volume={13},
  number={2},
  year={2012}
}
@article{choi2019empirical,
  title={On empirical comparisons of optimizers for deep learning},
  author={Choi, D},
  journal={arXiv preprint arXiv:1910.05446},
  year={2019}
}
@inproceedings{gupta2018shampoo,
  title={Shampoo: Preconditioned stochastic tensor optimization},
  author={Gupta, Vineet and Koren, Tomer and Singer, Yoram},
  booktitle={International Conference on Machine Learning},
  pages={1842--1850},
  year={2018},
  organization={PMLR}
}
@article{shi2023distributed,
  title={A distributed data-parallel pytorch implementation of the distributed shampoo optimizer for training neural networks at-scale},
  author={Shi, Hao-Jun Michael and Lee, Tsung-Hsien and Iwasaki, Shintaro and Gallego-Posada, Jose and Li, Zhijing and Rangadurai, Kaushik and Mudigere, Dheevatsa and Rabbat, Michael},
  journal={arXiv preprint arXiv:2309.06497},
  year={2023}
}
@article{anil2020scalable,
  title={Scalable second order optimization for deep learning},
  author={Anil, Rohan and Gupta, Vineet and Koren, Tomer and Regan, Kevin and Singer, Yoram},
  journal={arXiv preprint arXiv:2002.09018},
  year={2020}
}
@article{vyas2024soap,
  title={Soap: Improving and stabilizing shampoo using adam},
  author={Vyas, Nikhil and Morwani, Depen and Zhao, Rosie and Shapira, Itai and Brandfonbrener, David and Janson, Lucas and Kakade, Sham},
  journal={arXiv preprint arXiv:2409.11321},
  year={2024}
}
@article{george2018fast,
  title={Fast approximate natural gradient descent in a kronecker factored eigenbasis},
  author={George, Thomas and Laurent, C{\'e}sar and Bouthillier, Xavier and Ballas, Nicolas and Vincent, Pascal},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}
@inproceedings{grosse2016kronecker,
  title={A kronecker-factored approximate fisher matrix for convolution layers},
  author={Grosse, Roger and Martens, James},
  booktitle={International Conference on Machine Learning},
  pages={573--582},
  year={2016},
  organization={PMLR}
}
@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}
@article{si2024flora,
  title={FLoRA: Low-Rank Core Space for N-dimension},
  author={Si, Chongjie and Wang, Xuehui and Yang, Xue and Xu, Zhengqin and Li, Qingyun and Dai, Jifeng and Qiao, Yu and Yang, Xiaokang and Shen, Wei},
  journal={arXiv preprint arXiv:2405.14739},
  year={2024}
}
@inproceedings{lialin2023relora,
  title={Relora: High-rank training through low-rank updates},
  author={Lialin, Vladislav and Muckatira, Sherin and Shivagunde, Namrata and Rumshisky, Anna},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}
@article{zhao2024galore,
  title={Galore: Memory-efficient llm training by gradient low-rank projection},
  author={Zhao, Jiawei and Zhang, Zhenyu and Chen, Beidi and Wang, Zhangyang and Anandkumar, Anima and Tian, Yuandong},
  journal={arXiv preprint arXiv:2403.03507},
  year={2024}
}
@article{chen2024fira,
  title={Fira: Can We Achieve Full-rank Training of LLMs Under Low-rank Constraint?},
  author={Chen, Xi and Feng, Kaituo and Li, Changsheng and Lai, Xunhao and Yue, Xiangyu and Yuan, Ye and Wang, Guoren},
  journal={arXiv preprint arXiv:2410.01623},
  year={2024}
}
@article{das2024natural,
  title={Natural GaLore: Accelerating GaLore for memory-efficient LLM Training and Fine-tuning},
  author={Das, Arijit},
  journal={arXiv preprint arXiv:2410.16029},
  year={2024}
}
@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}
@article{raffel2020C4,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}
@article{wang2018glue,
  title={Glue: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}
@article{morwani2024new,
  title={A New Perspective on Shampoo's Preconditioner},
  author={Morwani, Depen and Shapira, Itai and Vyas, Nikhil and Malach, Eran and Kakade, Sham and Janson, Lucas},
  journal={arXiv preprint arXiv:2406.17748},
  year={2024}
}
@article{tian2020understanding,
  title={Understanding self-supervised learning with dual deep networks},
  author={Tian, Yuandong and Yu, Lantao and Chen, Xinlei and Ganguli, Surya},
  journal={arXiv preprint arXiv:2010.00578},
  year={2020}
}
@inproceedings{li2018towards,
  title={Towards faster training of global covariance pooling networks by iterative matrix square root normalization},
  author={Li, Peihua and Xie, Jiangtao and Wang, Qilong and Gao, Zilin},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={947--955},
  year={2018}
}
@inproceedings{huang2019iterative,
  title={Iterative normalization: Beyond standardization towards efficient whitening},
  author={Huang, Lei and Zhou, Yi and Zhu, Fan and Liu, Li and Shao, Ling},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={4874--4883},
  year={2019}
}
@article{li2017universal,
  title={Universal style transfer via feature transforms},
  author={Li, Yijun and Fang, Chen and Yang, Jimei and Wang, Zhaowen and Lu, Xin and Yang, Ming-Hsuan},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@inproceedings{cho2019image,
  title={Image-to-image translation via group-wise deep whitening-and-coloring transformation},
  author={Cho, Wonwoong and Choi, Sungha and Park, David Keetae and Shin, Inkyu and Choo, Jaegul},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10639--10647},
  year={2019}
}
@inproceedings{choi2021robustnet,
  title={Robustnet: Improving domain generalization in urban-scene segmentation via instance selective whitening},
  author={Choi, Sungha and Jung, Sanghun and Yun, Huiwon and Kim, Joanne T and Kim, Seungryong and Choo, Jaegul},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={11580--11590},
  year={2021}
}

@article{koroko2022efficient,
  title={Efficient approximations of the fisher matrix in neural networks using kronecker product singular value decomposition},
  author={Koroko, Abdoulaye and Anciaux-Sedrakian, Ani and Gharbia, Ibtihel Ben and Gar{\`e}s, Val{\'e}rie and Haddou, Mounir and Tran, Quang Huy},
  journal={arXiv preprint arXiv:2201.10285},
  year={2022}
}
@article{liao2018approximate,
  title={Approximate fisher information matrix to characterize the training of deep neural networks},
  author={Liao, Zhibin and Drummond, Tom and Reid, Ian and Carneiro, Gustavo},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={42},
  number={1},
  pages={15--26},
  year={2018},
  publisher={IEEE}
}
@article{zhang2023eva,
  title={Eva: A General Vectorized Approximation Framework for Second-order Optimization},
  author={Zhang, Lin and Shi, Shaohuai and Li, Bo},
  journal={arXiv preprint arXiv:2308.02123},
  year={2023}
}
@inproceedings{benzing2022gradient,
  title={Gradient descent on neurons and its link to approximate second-order optimization},
  author={Benzing, Frederik},
  booktitle={International Conference on Machine Learning},
  pages={1817--1853},
  year={2022},
  organization={PMLR}
}
@article{bernstein2024old,
  title={Old optimizer, new norm: An anthology},
  author={Bernstein, Jeremy and Newhouse, Laker},
  journal={arXiv preprint arXiv:2409.20325},
  year={2024}
}
@inproceedings{bernstein2018signsgd,
  title={signSGD: Compressed optimisation for non-convex problems},
  author={Bernstein, Jeremy and Wang, Yu-Xiang and Azizzadenesheli, Kamyar and Anandkumar, Animashree},
  booktitle={International Conference on Machine Learning},
  pages={560--569},
  year={2018},
  organization={PMLR}
}
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}
@article{llama3,
  author       = {Abhimanyu Dubey and
                  Abhinav Jauhri and
                  Abhinav Pandey and
                  Abhishek Kadian and
                  Ahmad Al{-}Dahle and
                  Aiesha Letman and
                  Akhil Mathur and
                  Alan Schelten and
                  Amy Yang and
                  Angela Fan and
                  Anirudh Goyal and
                  Anthony Hartshorn and
                  Aobo Yang and
                  Archi Mitra and
                  Archie Sravankumar and
                  Artem Korenev and
                  Arthur Hinsvark and
                  Arun Rao and
                  Aston Zhang and
                  Aur{\'{e}}lien Rodriguez and
                  Austen Gregerson and
                  Ava Spataru and
                  Baptiste Rozi{\`{e}}re and
                  Bethany Biron and
                  Binh Tang and
                  Bobbie Chern and
                  Charlotte Caucheteux and
                  Chaya Nayak and
                  Chloe Bi and
                  Chris Marra and
                  Chris McConnell and
                  Christian Keller and
                  Christophe Touret and
                  Chunyang Wu and
                  Corinne Wong and
                  Cristian Canton Ferrer and
                  Cyrus Nikolaidis and
                  Damien Allonsius and
                  Daniel Song and
                  Danielle Pintz and
                  Danny Livshits and
                  David Esiobu and
                  Dhruv Choudhary and
                  Dhruv Mahajan and
                  Diego Garcia{-}Olano and
                  Diego Perino and
                  Dieuwke Hupkes and
                  Egor Lakomkin and
                  Ehab AlBadawy and
                  Elina Lobanova and
                  Emily Dinan and
                  Eric Michael Smith and
                  Filip Radenovic and
                  Frank Zhang and
                  Gabriel Synnaeve and
                  Gabrielle Lee and
                  Georgia Lewis Anderson and
                  Graeme Nail and
                  Gr{\'{e}}goire Mialon and
                  Guan Pang and
                  Guillem Cucurell and
                  Hailey Nguyen and
                  Hannah Korevaar and
                  Hu Xu and
                  Hugo Touvron and
                  Iliyan Zarov and
                  Imanol Arrieta Ibarra and
                  Isabel M. Kloumann and
                  Ishan Misra and
                  Ivan Evtimov and
                  Jade Copet and
                  Jaewon Lee and
                  Jan Geffert and
                  Jana Vranes and
                  Jason Park and
                  Jay Mahadeokar and
                  Jeet Shah and
                  Jelmer van der Linde and
                  Jennifer Billock and
                  Jenny Hong and
                  Jenya Lee and
                  Jeremy Fu and
                  Jianfeng Chi and
                  Jianyu Huang and
                  Jiawen Liu and
                  Jie Wang and
                  Jiecao Yu and
                  Joanna Bitton and
                  Joe Spisak and
                  Jongsoo Park and
                  Joseph Rocca and
                  Joshua Johnstun and
                  Joshua Saxe and
                  Junteng Jia and
                  Kalyan Vasuden Alwala and
                  Kartikeya Upasani and
                  Kate Plawiak and
                  Ke Li and
                  Kenneth Heafield and
                  Kevin Stone},
  title        = {The Llama 3 Herd of Models},
  journal      = {CoRR},
  volume       = {abs/2407.21783},
  year         = {2024}
}
@article{korthikanti2023reducing,
  title={Reducing activation recomputation in large transformer models},
  author={Korthikanti, Vijay Anand and Casper, Jared and Lym, Sangkug and McAfee, Lawrence and Andersch, Michael and Shoeybi, Mohammad and Catanzaro, Bryan},
  journal={Proceedings of Machine Learning and Systems},
  volume={5},
  pages={341--353},
  year={2023}
}
@article{yang2008principal,
  title={Principal whitened gradient for information geometry},
  author={Yang, Zhirong and Laaksonen, Jorma},
  journal={Neural Networks},
  volume={21},
  number={2-3},
  pages={232--240},
  year={2008},
  publisher={Elsevier}
}
@article{gao2023eigenvalue,
  title={Eigenvalue-corrected natural gradient based on a new approximation},
  author={Gao, Kaixin and Huang, Zheng-Hai and Liu, Xiaolei and Wang, Min and Wang, Shuangling and Wang, Zidong and Xu, Dachuan and Yu, Fan},
  journal={Asia-Pacific Journal of Operational Research},
  volume={40},
  number={01},
  pages={2340005},
  year={2023},
  publisher={World Scientific}
}
@inproceedings{shazeer2018adafactor,
  title={Adafactor: Adaptive learning rates with sublinear memory cost},
  author={Shazeer, Noam and Stern, Mitchell},
  booktitle={International Conference on Machine Learning},
  pages={4596--4604},
  year={2018},
  organization={PMLR}
}
@article{lee1999learning,
  title={Learning the parts of objects by non-negative matrix factorization},
  author={Lee, Daniel D and Seung, H Sebastian},
  journal={nature},
  volume={401},
  number={6755},
  pages={788--791},
  year={1999},
  publisher={Nature Publishing Group UK London}
}
@inproceedings{martens2015optimizing,
  title={Optimizing neural networks with kronecker-factored approximate curvature},
  author={Martens, James and Grosse, Roger},
  booktitle={International conference on machine learning},
  pages={2408--2417},
  year={2015},
  organization={PMLR}
}
@inproceedings{martens2018kronecker,
  title={Kronecker-factored curvature approximations for recurrent neural networks},
  author={Martens, James and Ba, Jimmy and Johnson, Matt},
  booktitle={International Conference on Learning Representations},
  year={2018}
}
@inproceedings{gao2021trace,
  title={A trace-restricted kronecker-factored approximation to natural gradient},
  author={Gao, Kaixin and Liu, Xiaolei and Huang, Zhenghai and Wang, Min and Wang, Zidong and Xu, Dachuan and Yu, Fan},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={9},
  pages={7519--7527},
  year={2021}
}
@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization.},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of machine learning research},
  volume={12},
  number={7},
  year={2011}
}
@article{chen2024symbolic,
  title={Symbolic discovery of optimization algorithms},
  author={Chen, Xiangning and Liang, Chen and Huang, Da and Real, Esteban and Wang, Kaiyuan and Pham, Hieu and Dong, Xuanyi and Luong, Thang and Hsieh, Cho-Jui and Lu, Yifeng and others},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}
@article{liu2023sophia,
  title={Sophia: A scalable stochastic second-order optimizer for language model pre-training},
  author={Liu, Hong and Li, Zhiyuan and Hall, David and Liang, Percy and Ma, Tengyu},
  journal={arXiv preprint arXiv:2305.14342},
  year={2023}
}
@article{zhang2019root,
  title={Root mean square layer normalization},
  author={Zhang, Biao and Sennrich, Rico},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@article{shazeer2020glu,
  title={Glu variants improve transformer},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:2002.05202},
  year={2020}
}
@inproceedings{
anonymous2024improving,
title={Improving Adaptive Moment Optimization via Preconditioner Diagonalization},
author={Anonymous},
booktitle={Submitted to The Thirteenth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=NdNuKMEv9y},
note={under review}
}
@article{ma2024swan,
  title={SWAN: Preprocessing SGD Enables Adam-Level Performance On LLM Training With Significant Memory Reduction},
  author={Ma, Chao and Gong, Wenbo and Scetbon, Meyer and Meeds, Edward},
  journal={arXiv preprint arXiv:2412.13148},
  year={2024}
}
@misc{jordan2024muon,
  author       = {Keller Jordan and Yuchen Jin and Vlado Boza and Jiacheng You and
                  Franz Cecista and Laker Newhouse and Jeremy Bernstein},
  title        = {Muon: An optimizer for hidden layers in neural networks},
  year         = {2024},
  url          = {https://kellerjordan.github.io/posts/muon/}
}
@article{zhu2024apollo,
  title={APOLLO: SGD-like Memory, AdamW-level Performance},
  author={Zhu, Hanqing and Zhang, Zhenyu and Cong, Wenyan and Liu, Xi and Park, Sem and Chandra, Vikas and Long, Bo and Pan, David Z and Wang, Zhangyang and Lee, Jinwon},
  journal={arXiv preprint arXiv:2412.05270},
  year={2024}
}
@article{you2019lamb,
  title={Large batch optimization for deep learning: Training bert in 76 minutes},
  author={You, Yang and Li, Jing and Reddi, Sashank and Hseu, Jonathan and Kumar, Sanjiv and Bhojanapalli, Srinadh and Song, Xiaodan and Demmel, James and Keutzer, Kurt and Hsieh, Cho-Jui},
  journal={arXiv preprint arXiv:1904.00962},
  year={2019}
}
@article{you2017lars,
  title={Large batch training of convolutional networks},
  author={You, Yang and Gitman, Igor and Ginsburg, Boris},
  journal={arXiv preprint arXiv:1708.03888},
  year={2017}
}
@article{team2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Team, Gemini and Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}
@inproceedings{cutkosky2020momentum,
  title={Momentum improves normalized sgd},
  author={Cutkosky, Ashok and Mehta, Harsh},
  booktitle={International conference on machine learning},
  pages={2260--2268},
  year={2020},
  organization={PMLR}
}
@article{hazan2015beyond,
  title={Beyond convexity: Stochastic quasi-convex optimization},
  author={Hazan, Elad and Levy, Kfir and Shalev-Shwartz, Shai},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}
@article{hwang2024fadam,
  title={FAdam: Adam is a natural gradient optimizer using diagonal empirical Fisher information},
  author={Hwang, Dongseong},
  journal={arXiv preprint arXiv:2405.12807},
  year={2024}
}
@inproceedings{huang2018decorrelated,
  title={Decorrelated batch normalization},
  author={Huang, Lei and Yang, Dawei and Lang, Bo and Deng, Jia},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={791--800},
  year={2018}
}
@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}
@article{jelassi2021adamnobetter,
  title={Adam is no better than normalized SGD: Dissecting how adaptivity improves GAN performance},
  author={Jelassi, Samy and Mensch, Arthur and Gidel, Gauthier and Li, Yuanzhi},
  year={2021}
}
@article{zhao2024adapprox,
  title={Adapprox: Adaptive Approximation in Adam Optimization via Randomized Low-Rank Matrices},
  author={Zhao, Pengxiang and Li, Ping and Gu, Yingjie and Zheng, Yi and K{\"o}lker, Stephan Ludger and Wang, Zhefeng and Yuan, Xiaoming},
  journal={arXiv preprint arXiv:2403.14958},
  year={2024}
}
@article{zhang2024adam,
  title={Adam-mini: Use fewer learning rates to gain more},
  author={Zhang, Yushun and Chen, Congliang and Li, Ziniu and Ding, Tian and Wu, Chenwei and Ye, Yinyu and Luo, Zhi-Quan and Sun, Ruoyu},
  journal={arXiv preprint arXiv:2406.16793},
  year={2024}
}
@misc{xu2024adamlearningratescaling,
      title={No More Adam: Learning Rate Scaling at Initialization is All You Need}, 
      author={Minghao Xu and Lichuan Xiang and Xu Cai and Hongkai Wen},
      year={2024},
      eprint={2412.11768},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2412.11768}, 
}
@book{golub2013matrix,
  title={Matrix computations},
  author={Golub, Gene H and Van Loan, Charles F},
  year={2013},
  publisher={JHU press}
}
@article{zhao2024deconstructing,
  title={Deconstructing what makes a good optimizer for language models},
  author={Zhao, Rosie and Morwani, Depen and Brandfonbrener, David and Vyas, Nikhil and Kakade, Sham},
  journal={arXiv preprint arXiv:2407.07972},
  year={2024}
}
@article{li2017preconditioned,
  title={Preconditioned stochastic gradient descent},
  author={Li, Xi-Lin},
  journal={IEEE transactions on neural networks and learning systems},
  volume={29},
  number={5},
  pages={1454--1466},
  year={2017},
  publisher={IEEE}
}
@article{pooladzandi2024curvature,
  title={Curvature-Informed SGD via General Purpose Lie-Group Preconditioners},
  author={Pooladzandi, Omead and Li, Xi-Lin},
  journal={arXiv preprint arXiv:2402.04553},
  year={2024}
}
@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan},
  journal={arXiv preprint arXiv:1907.11692},
  volume={364},
  year={2019}
}
@article{li2018preconditioner,
  title={Preconditioner on matrix lie group for sgd},
  author={Li, Xi-Lin},
  journal={arXiv preprint arXiv:1809.10232},
  year={2018}
}
@inproceedings{sun2021connection,
  title={Connection of Diagonal Hessian Estimates to Natural Gradients in Stochastic Optimization},
  author={Sun, Shiqing and Spall, James C},
  booktitle={2021 55th Annual Conference on Information Sciences and Systems (CISS)},
  pages={1--6},
  year={2021},
  organization={IEEE}
}


@inproceedings{duvvuricombining,
  title={Combining Axes Preconditioners through Kronecker Approximation for Deep Learning},
  author={Duvvuri, Sai Surya and Devvrit, Fnu and Anil, Rohan and Hsieh, Cho-Jui and Dhillon, Inderjit S},
  booktitle={The Twelfth International Conference on Learning Representations}
}