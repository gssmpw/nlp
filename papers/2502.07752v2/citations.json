[
  {
    "index": 0,
    "papers": [
      {
        "key": "martens2015optimizing",
        "author": "Martens, James and Grosse, Roger",
        "title": "Optimizing neural networks with kronecker-factored approximate curvature"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "grosse2016kronecker",
        "author": "Grosse, Roger and Martens, James",
        "title": "A kronecker-factored approximate fisher matrix for convolution layers"
      },
      {
        "key": "martens2018kronecker",
        "author": "Martens, James and Ba, Jimmy and Johnson, Matt",
        "title": "Kronecker-factored curvature approximations for recurrent neural networks"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "george2018fast",
        "author": "George, Thomas and Laurent, C{\\'e}sar and Bouthillier, Xavier and Ballas, Nicolas and Vincent, Pascal",
        "title": "Fast approximate natural gradient descent in a kronecker factored eigenbasis"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "gao2021trace",
        "author": "Gao, Kaixin and Liu, Xiaolei and Huang, Zhenghai and Wang, Min and Wang, Zidong and Xu, Dachuan and Yu, Fan",
        "title": "A trace-restricted kronecker-factored approximation to natural gradient"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "koroko2022efficient",
        "author": "Koroko, Abdoulaye and Anciaux-Sedrakian, Ani and Gharbia, Ibtihel Ben and Gar{\\`e}s, Val{\\'e}rie and Haddou, Mounir and Tran, Quang Huy",
        "title": "Efficient approximations of the fisher matrix in neural networks using kronecker product singular value decomposition"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "duchi2011adaptive",
        "author": "Duchi, John and Hazan, Elad and Singer, Yoram",
        "title": "Adaptive subgradient methods for online learning and stochastic optimization."
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "gupta2018shampoo",
        "author": "Gupta, Vineet and Koren, Tomer and Singer, Yoram",
        "title": "Shampoo: Preconditioned stochastic tensor optimization"
      },
      {
        "key": "anil2020scalable",
        "author": "Anil, Rohan and Gupta, Vineet and Koren, Tomer and Regan, Kevin and Singer, Yoram",
        "title": "Scalable second order optimization for deep learning"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "morwani2024new",
        "author": "Morwani, Depen and Shapira, Itai and Vyas, Nikhil and Malach, Eran and Kakade, Sham and Janson, Lucas",
        "title": "A New Perspective on Shampoo's Preconditioner"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "vyas2024soap",
        "author": "Vyas, Nikhil and Morwani, Depen and Zhao, Rosie and Shapira, Itai and Brandfonbrener, David and Janson, Lucas and Kakade, Sham",
        "title": "Soap: Improving and stabilizing shampoo using adam"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "george2018fast",
        "author": "George, Thomas and Laurent, C{\\'e}sar and Bouthillier, Xavier and Ballas, Nicolas and Vincent, Pascal",
        "title": "Fast approximate natural gradient descent in a kronecker factored eigenbasis"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "zhu2024apollo",
        "author": "Zhu, Hanqing and Zhang, Zhenyu and Cong, Wenyan and Liu, Xi and Park, Sem and Chandra, Vikas and Long, Bo and Pan, David Z and Wang, Zhangyang and Lee, Jinwon",
        "title": "APOLLO: SGD-like Memory, AdamW-level Performance"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "zhao2024galore",
        "author": "Zhao, Jiawei and Zhang, Zhenyu and Chen, Beidi and Wang, Zhangyang and Anandkumar, Anima and Tian, Yuandong",
        "title": "Galore: Memory-efficient llm training by gradient low-rank projection"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "jordan2024muon",
        "author": "Keller Jordan and Yuchen Jin and Vlado Boza and Jiacheng You and\nFranz Cecista and Laker Newhouse and Jeremy Bernstein",
        "title": "Muon: An optimizer for hidden layers in neural networks"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "ma2024swan",
        "author": "Ma, Chao and Gong, Wenbo and Scetbon, Meyer and Meeds, Edward",
        "title": "SWAN: Preprocessing SGD Enables Adam-Level Performance On LLM Training With Significant Memory Reduction"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "zhao2024adapprox",
        "author": "Zhao, Pengxiang and Li, Ping and Gu, Yingjie and Zheng, Yi and K{\\\"o}lker, Stephan Ludger and Wang, Zhefeng and Yuan, Xiaoming",
        "title": "Adapprox: Adaptive Approximation in Adam Optimization via Randomized Low-Rank Matrices"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "you2017lars",
        "author": "You, Yang and Gitman, Igor and Ginsburg, Boris",
        "title": "Large batch training of convolutional networks"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "you2019lamb",
        "author": "You, Yang and Li, Jing and Reddi, Sashank and Hseu, Jonathan and Kumar, Sanjiv and Bhojanapalli, Srinadh and Song, Xiaodan and Demmel, James and Keutzer, Kurt and Hsieh, Cho-Jui",
        "title": "Large batch optimization for deep learning: Training bert in 76 minutes"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "chen2024fira",
        "author": "Chen, Xi and Feng, Kaituo and Li, Changsheng and Lai, Xunhao and Yue, Xiangyu and Yuan, Ye and Wang, Guoren",
        "title": "Fira: Can We Achieve Full-rank Training of LLMs Under Low-rank Constraint?"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "anonymous2024improving",
        "author": "Anonymous",
        "title": "Improving Adaptive Moment Optimization via Preconditioner Diagonalization"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "li2017preconditioned",
        "author": "Li, Xi-Lin",
        "title": "Preconditioned stochastic gradient descent"
      },
      {
        "key": "pooladzandi2024curvature",
        "author": "Pooladzandi, Omead and Li, Xi-Lin",
        "title": "Curvature-Informed SGD via General Purpose Lie-Group Preconditioners"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "li2018preconditioner",
        "author": "Li, Xi-Lin",
        "title": "Preconditioner on matrix lie group for sgd"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "zhao2024galore",
        "author": "Zhao, Jiawei and Zhang, Zhenyu and Chen, Beidi and Wang, Zhangyang and Anandkumar, Anima and Tian, Yuandong",
        "title": "Galore: Memory-efficient llm training by gradient low-rank projection"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "chen2024fira",
        "author": "Chen, Xi and Feng, Kaituo and Li, Changsheng and Lai, Xunhao and Yue, Xiangyu and Yuan, Ye and Wang, Guoren",
        "title": "Fira: Can We Achieve Full-rank Training of LLMs Under Low-rank Constraint?"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "si2024flora",
        "author": "Si, Chongjie and Wang, Xuehui and Yang, Xue and Xu, Zhengqin and Li, Qingyun and Dai, Jifeng and Qiao, Yu and Yang, Xiaokang and Shen, Wei",
        "title": "FLoRA: Low-Rank Core Space for N-dimension"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "lialin2023relora",
        "author": "Lialin, Vladislav and Muckatira, Sherin and Shivagunde, Namrata and Rumshisky, Anna",
        "title": "Relora: High-rank training through low-rank updates"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "hu2021lora",
        "author": "Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu",
        "title": "Lora: Low-rank adaptation of large language models"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "chen2024symbolic",
        "author": "Chen, Xiangning and Liang, Chen and Huang, Da and Real, Esteban and Wang, Kaiyuan and Pham, Hieu and Dong, Xuanyi and Luong, Thang and Hsieh, Cho-Jui and Lu, Yifeng and others",
        "title": "Symbolic discovery of optimization algorithms"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "bernstein2018signsgd",
        "author": "Bernstein, Jeremy and Wang, Yu-Xiang and Azizzadenesheli, Kamyar and Anandkumar, Animashree",
        "title": "signSGD: Compressed optimisation for non-convex problems"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "zhu2024apollo",
        "author": "Zhu, Hanqing and Zhang, Zhenyu and Cong, Wenyan and Liu, Xi and Park, Sem and Chandra, Vikas and Long, Bo and Pan, David Z and Wang, Zhangyang and Lee, Jinwon",
        "title": "APOLLO: SGD-like Memory, AdamW-level Performance"
      }
    ]
  },
  {
    "index": 29,
    "papers": [
      {
        "key": "ma2024swan",
        "author": "Ma, Chao and Gong, Wenbo and Scetbon, Meyer and Meeds, Edward",
        "title": "SWAN: Preprocessing SGD Enables Adam-Level Performance On LLM Training With Significant Memory Reduction"
      }
    ]
  }
]