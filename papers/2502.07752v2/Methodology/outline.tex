\label{sec: structure approx fim}
\begin{table*}[]
\centering
\caption{The summary of underlying structure assumptions of existing and newly proposed optimizers, along with practical efficiency. ``Generalizes" refers to the optimizer whose structure is generalized, e.g.~\gls{alicec}'s structure generalizes Adam (see \cref{subapp: connections between structural assumptions} for further details). Optimizers in \textcolor{blue}{blue} color are new. The ``Computation" is the cost of per-step of the optimizer update. We assume $n\geq m\gg r$. }
\label{tab: summary table}
\resizebox{\textwidth}{!}{
\begin{tabular}{l|lllllll}
\hline
            & Adam          & Shampoo                                          & Eigen-Adam\slash AdaDiag                                                                                                        &  SOAP/AdaDiag++                                                  & GaLore          & \textcolor{blue}{RACS}            & \textcolor{blue}{Alice}             \\ \hline
Stucture    & $\diagv(\vv)$ & $\mR_n^{\frac{1}{2}}\otimes \mL_m^{\frac{1}{2}}$ &$\diagb(\{\mUf\mD_i\mUf^T\}_i)$ &  $(\mU_R\otimes\mU_L)\tilde{\mD}(\mU_R\otimes\mU_L)^T$ & Approx. Alice   & $\mS\otimes\mQ$ & Low-rank to Eigen-Adam \\
Generalizes & N/A           & N/A                                              &Adam                                                                                                         &  Eigen-Adam + Shampoo                                       & N/A             & Normalization   & GaLore            \\
Computation & $O(mn)$       &$O(m^3+n^3)$                                     & $O(m^3)$                                          & $O(m^3+n^3)$                                                                                                     &  $O(mnr+m^2n/K)$ & $O(mn)$         & $O(mnr+m^2r/K)$   \\
Memory      & $3mn$         & $mn+m^2+n^2$                                     &$3mn+2m^2$                                                                                                   &  $3mn+2m^2+2n^2$                                       & $mn+2nr+mr$     & $mn+m+n+1$      & $mn+2nr+mr+n+r^2$ \\ 
Full-rank update& \checkmark & \checkmark & \checkmark & \checkmark & \ding{55} & \checkmark & \checkmark\\
Section & \cref{subsec: existing optimizer} &\cref{subsec: shampoo} & \cref{subsec: alicec} &  \cref{subsec: new opt combination} & \cref{subapp: Low rank optimizers} & \cref{sec: ssgd} & \cref{subsec: alice optimizer}\\
\hline
\end{tabular}}
\end{table*}
The structured \gls{fim} approximation framework consists of three steps: first, choose a structure to approximate \gls{fim} $\Ft$; second, find a solution for the approximation by minimizing the following objective:
\begin{equation}
    \min_{\Ft\in\family} \Fnorm{\Ft-\bm{F}},
    \label{eq: UFE equation}
\end{equation}
where $\bm{F}$ is the empirical \gls{fim}, $\family$ is the matrix family corresponding to the structural assumption; third, derive the square-root \gls{ngd} update \Cref{eq: square root ngd} with approximated $\Ft$. In this section, we show that many existing optimizers and gradient operators can be reformulated within this framework by varying structural assumptions, thereby establishing connections between the choice of structure and optimizers.