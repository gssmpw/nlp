\section{\gls{ssgd}: memory-efficient optimizer from a carefully selected structure}
\label{sec: ssgd}
The structured \gls{fim} approximation reveals two important insights: there exists a correspondence between structural assumption and optimizers, and structural generality often comes at the cost of practical efficiency. For example, while the structures of \gls{alicec} and SOAP offer more accurate \gls{fim} approximations than a simple structure like gradient normalization, they require expensive computation and memory consumption (\cref{tab: summary table}), making them impractical for training LLMs. Building on this, our first design recommendation is to \textbf{select structures that balance generality and practical efficiency.} 

To demonstrate this, we select a structure that generalizes gradient normalization, which scales both the rows and columns simultaneously:
\begin{align}
    \family = \{\mS \otimes \mQ\}
    \label{eq: ssgd structure}
\end{align}
where $\mS\in\Rnn$, $\mQ\in\Rmm$ are positive diagonal matrices. The idea of diagonal approximation has also been leveraged in previous work under different setups \citep{zhao2024adapprox, shazeer2018adafactor, li2018preconditioner}. 
The optimal solution of \cref{eq: UFE equation} can be solved by a fixed-point iterative procedure:

\begin{proposition}[Two-sided scaling]
Assuming the structure of \cref{eq: ssgd structure}, and $\E[\mG\elesquare]$ contains only positive values, solving \cref{eq: UFE equation} admits an iterative fixed point procedure:
\begin{align}
    \vs = \frac{\diag\left(\E[\mG^T\mQ\mG]\right)}{\Vert\mQ\Vert_F^2},\;\;\;
    \vq =\frac{\diag\left(\E[\mG\mS\mG^T]\right)}{\Vert\mS\Vert_F^2}.
    \label{eq: iterative procedure double scaling}
\end{align}
where $\vs=\diag(\mS)$, $\vq=\diag(\mQ)$.
Additionally, the fixed point solution $\vs$, $\vq$ converges to the right and left principal singular vector of $\E[\mG\elesquare]$ up to a scaling factor with unique $\mS^*\otimes \mQ^*$.
\label{prop: two sided scaling}
\end{proposition}
In practice, we find initializing $\vq=\bm{1}$ and use 1-sample estimate of $\E[\cdot]$ gives good performance. 
Interestingly, \citet{morwani2024new} also connects Shampoo to 1-step power iteration. Here, the \cref{eq: iterative procedure double scaling} can also be viewed as a power iteration algorithm. The main difference is that \citet{morwani2024new} assumes full \gls{spd} matrix $\mS$ and $\mQ$, but our structural constraint is positive diagonal matrix. Consequently, our procedure is computationally efficient and allows for multiple iterations.

The corresponding square-root \gls{ngd} update scales both rows and columns through $\mS$ and $\mQ$ (i.e.~$\devect(\Ft^{-\frac{1}{2}}\vecg) = \mQ^{-\frac{1}{2}}\mG\mS^{-\frac{1}{2}}$).
We name this optimizer, \glsreset{ssgd}\gls{ssgd} (\cref{alg: ssgd optimizer}). Although analytic solutions of $\vs$, $\vq$ exist, we perform $5$ steps of \cref{eq: iterative procedure double scaling} as an efficient approximation. To further stabilize training, we also incorporate the norm-growth limiter used in \citet{chen2024fira}. \gls{ssgd} is highly memory efficient since it only needs the storage of two diagonal matrices $\mS$ and $\mQ$ and a scalar for the limiter, consuming $m+n+1$ memory. In \cref{subapp: connection to existing optimizers} we discuss  connections to Adapprox, Apollo and Adafactor \citep{zhao2024adapprox, zhu2024apollo, shazeer2018adafactor}.
\begin{algorithm}
    \caption{\gls{ssgd}}
    \label{alg: ssgd optimizer}
    \begin{algorithmic}[1]
        \STATE {\bfseries Input:} learning rate $\lambda$, $\beta$, scale $\alpha$, limiter threshold $\gamma$, optimization steps $T$.
        \STATE $\vs_0 = 0$; $\vq_0=0$; $\phi_0=0$
        \FOR{$t=1,\ldots,T$}
            \STATE $\mG_t=\nabla_{\mW_t}\mathcal{L}$
            \STATE Obtain $\mS_t$ and $\mQ_t$ by \cref{eq: iterative procedure double scaling}
            \STATE $\vs_t=\beta\vs_{t-1}+(1-\beta)\diag(\mS_t)$; 
            \STATE $\vq_t=\beta\vq_{t-1}+(1-\beta)\diag(\mQ_t)$
            \STATE$\tilde{\mG}_t= \diagv(\vq_t)^{-\frac{1}{2}}\mG\diagv(\vs_t)^{-\frac{1}{2}}$
            % \STATE \textcolor{blue}{\%\textit{Norm-growth limiter}}
            \STATE $\eta =\gamma/\max\{\frac{\Vert\tilde{\mG}_t\Vert}{\phi_{t-1}}, \gamma\}$ if $t>1$ else $1$ 
            \STATE $\phi_t = \eta\Vert\tilde{\mG}_t\Vert$
            \STATE $\mW_{t+1}=\mW_t -\lambda\eta \alpha\tilde{\mG}_t$
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

This design recommendation has its limitations. Finding such a structure with a balanced trade-off may not always be easy, and the resulting structure tends to be simple, offering less accurate approximation to \gls{fim} compared to the general ones. Since the main bottleneck of more general optimizers is their practical efficiency, our second design recommendation is to: \textbf{improve their efficiency by converting full-rank optimizers into low-rank counterparts using a novel low-rank extension framework}.