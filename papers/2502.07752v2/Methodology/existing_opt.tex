\subsection{Adam: purely diagonal structure}
\label{subsec: existing optimizer}
There have been many work arguing that Adam's second moment aims to approximate the diagonal of \gls{fim} \citep{kingma2014adam, hwang2024fadam, sun2021connection}. Although it is easy to prove that this is, in fact, optimal approximation under \cref{eq: UFE equation}, we include the following proposition for completeness. 
% When Adam was originally proposed, it was argued that the purpose of the second moment was to approximate the diagonal elements of \gls{fim} \citep{kingma2014adam, hwang2024fadam}. With a purely diagonal structural assumption, this is, in fact, the optimal solution under \cref{eq: UFE equation}:
\begin{proposition}[diagonal approximation]
    Assuming $\family =\{\diagv(\vv); v_i>0\}$, then \cref{eq: UFE equation} has analytic solution 
    \begin{equation}
        \Ft^* = \diagv(\E[\vecg^2])
    \end{equation}
where $\vecg^2$ indicates the element-wise square of $\vecg=\vect(\mG)$. 
\label{prop: adam solution}
\end{proposition}
It is trivial to show the resulting square-root \gls{ngd} recovers Adam's second moment when using the \gls{ema} to estimate $\E$. Together with the first moment, one can recover Adam.

\subsection{Shampoo: Kronecker product structure}
\label{subsec: shampoo}
Although previous work \citep{morwani2024new} has demonstrated the connection of Shampoo \citep{gupta2018shampoo} to the Kronecker product approximation to \gls{fim} through power iteration algorithm, 
we will make its connection to \cref{eq: UFE equation} more explicit and provide an alternative view: Shampoo's pre-conditioner can be derived by minimizing an upper bound of \cref{eq: UFE equation} with this structural assumption: 
\begin{align*}
    \family=\{\Rnr \otimes \Lmr; \mR_n\in\Rnn, \mL_m\in\Rmm\}
\end{align*}
where $\mR_n$ and $\mL_m$ are \gls{spd} matrices.
\begin{theorem}[Shampoo pre-conditioner]
    Assume the above structural assumption, then we have an upper bound of \cref{eq: UFE equation}
    \begin{align}
        \Fnorm{\Ft-\mF} &\leq 3(mn\Vert\mA^2-\mC^2\Vert_F\Vert\mB^2-\mC^2\Vert_F\nonumber\\
        &+ \sqrt{mn}\Fnorm{\mC}(\Vert\mA^2-\mC^2\Vert_F+\Vert\mB^2-\mC^2\Vert_F))
        \label{eq: shampoo upper bound}
    \end{align}
    where $\mA = \Rnr\otimes \mI_m$, $\mB = \mI_n \otimes \Lmr$, $\mC=\E[\vecg\vecg^T]^{\frac{1}{2}}$. Minimizing the upper-bound admits
    \begin{align*}
        \mR_n^* = \frac{1}{m}\E[\mG^T\mG],\;\;\; \mL_m^* = \frac{1}{n}\E[\mG\mG^T]
    \end{align*}
    \label{thm: optimal shampoo}
\end{theorem}
\cref{subapp: Shampoo update formula} shows that the corresponding square-root \gls{ngd} leads to the Shampoo's update formula. Therefore, the structure behind Shampoo is the Kronecker product of two square-root \gls{spd} matrices.

\subsection{Normalization and Whitening: Simple block diagonal structures}
\label{subsec: sve}
For an input $\mG$, the whitening and normalization operator are defined as 
\begin{align*}
    \whiten(\mG) =& ~(\mG\mG^T)^{-\frac{1}{2}}\mG\\
    \normalize(\mG) =& ~\mG\mS^{-\frac{1}{2}}
    \label{eq: whiten and normal operator}
\end{align*}
where $(\cdot)^{-\frac{1}{2}}$ denotes square root inverse, and $\diag(\mS)$ contains the squared column $l_2$ norm of $\mG$. Next we provide an new interpretation of these operators and show that they are the square-root \gls{ngd} updates under the following structural assumptions:
\begin{align}
    \family =& \{\mI_n\otimes \mM\} \;\;\; (\text{Whitening}) \text{\footnotemark}  \\
    \family =& \{\mS\otimes \mI_m; S_{ii}>0,\; \forall i\} \;\;\; (\text{Normalization}) 
\end{align}
\footnotetext{Note that this structure has also been proposed and discussed  \cite{duvvuricombining} under a slightly different setting.}
where $\mM\in\Rmm$ is \gls{spd} and $\mS\in\Rnn$ is a positive diagonal matrix. Given those structural assumptions, one can show:


\begin{proposition}[Normalization and whitening]
    Assuming $\family = \{\mI_n\otimes \mM\}$, minimizing \cref{eq: UFE equation} admits
    \begin{align}
        \mM^* = \frac{1}{n}\E[\mG\mG^T]
        \label{eq: generalization whitening}
    \end{align}
    If one assumes $\family = \{\mS\otimes \mI_m; S_{ii}>0,\; \forall i\}$, then the corresponding solution is 
    \begin{align}
        \mS^* = \frac{1}{m}\E[\diagv(\vg_1^T\vg_1,\ldots,\vg_n^T\vg_n)]
    \label{eq: generalization to normalization}
    \end{align}
    \label{coro: generalization to whitening and normalization}
\end{proposition}
The proof can be found in \cref{subapp: proof normalization}, where we prove a much more general solution (\cref{thm: generalization to normal and whiten}), and \cref{coro: generalization to whitening and normalization} is a special case. The corresponding square-root \gls{ngd} update with \cref{eq: generalization whitening} is $\devect(\Ft^{-\frac{1}{2}}\vecg)= \sqrt{n}\E[\mG\mG^T]^{-\frac{1}{2}}\mG$ (refer to \cref{subapp: update of generalization of whitening}). Therefore, we can view $\whiten(\cdot)$ as a special case with one-sample estimate for $\E$. Similarly, normalization is the square-root \gls{ngd} update ($\devect(\Ft^{-\frac{1}{2}}\vecg)= \sqrt{m}\mG\mS^{*-\frac{1}{2}}$) with \cref{eq: generalization to normalization} and one-sample estimate of $\E$.

Many recently proposed optimizers, such as Muon, SWAN, LARS and LAMB \citep{jordan2024muon, ma2024swan, you2017lars, you2019lamb}, rely on normalization and/or whitening. These gradient operators improve convergence \citep{you2017lars, jordan2024muon} and can replace Adamâ€™s internal states \citep{ma2024swan}. See \cref{subapp: connection to existing optimizers} for a detailed discussion.
