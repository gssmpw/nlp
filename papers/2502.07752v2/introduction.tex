\label{sec: introduction}
Adaptive optimizers are critical in training large language models (LLMs). Yet, as models and datasets continue to grow, one important issue associated with scaling is the memory overhead of many optimizers, especially in a distributed training setup \citep{llama3, korthikanti2023reducing}. This has several implications for training, including increased GPU requirements or reduced per-device batch size, which lowers overall training throughput. Adam, for instance, triples memory requirements due to the storage of two internal \gls{ema} states, while other optimizers \citep{gupta2018shampoo, vyas2024soap} with faster convergence (in terms of training steps) can further inflate the total memory.  Meanwhile, some memory efficient optimizers, like \gls{sgd}, fail to train the LLMs effectively. Thus, designing efficient optimizers  has become increasingly important.\footnote{We define efficiency as the amount of memory and wall-clock time used to achieve a target evaluation loss}

There are several promising lines of research that have advanced one or more of these aspects of efficiency. One aims to design new optimizers that reduce, or remove entirely, internal optimizer states without expensive per-step computation \citep{ma2024swan, jordan2024muon, zhang2024adam, xu2024adamlearningratescaling, zhu2024apollo}. Alternatively, low-rank approximations of gradients have also been used to reduce the memory of states with a slight degradation in performance \citep{hu2021lora, lialin2023relora, zhao2024galore, chen2024fira, si2024flora}. 
Despite these advances, developing new optimizers remains challenging. To address this, this paper explores structured \gls{fim} approximation as a practical framework for optimizer design.

To demonstrate the effectiveness of this framework, we begin by showing that many existing optimizers and gradient operators, including Adam, Shampoo, gradient normalization and whitening \citep{zhang2024adam, gupta2018shampoo, vyas2024soap, you2019lamb, ma2024swan, jordan2024muon}, can be recast under it, with different structural assumptions. We then go on to show how to derive two generalizations of Adam\footnote{Generality of structural assumption defines how relaxed this assumption is. We say structure $A$ is more general than $B$ iff.~$B$ can be recovered by applying further constraints on $A$. The general structures tend to give better approximations to \gls{fim} than the less general one. } with structures based on block diagonal matrices and Kronecker products, named \gls{alicec} and SOAP/AdaDiag++ \citep{vyas2024soap, anonymous2024improving}. 
Although this framework provides a clear link between structures and optimizers, working with more general structures that improve the \gls{fim} approximation can come at the cost of efficiency. For example, SOAP can require $7$ times more memory than SGD.  

Building on these insights, our first design recommendation proposes to choose structural assumptions that balance generality with practical efficiency. We demonstrate this by choosing a structure that generalizes gradient normalization, leading to a new efficient optimizer, \glsreset{ssgd}\gls{ssgd}, with \gls{sgd}-like memory requirements.

For optimizers with more general structures it may not be always possible to achieve such a balance. Instead of rejecting the potential of these generalizations, our second design recommendation proposes to apply a novel low-rank extension framework to improve their efficiency. This framework consists of three steps that can convert full-rank optimizers with more general structures into their low-rank approximations with reduced memory and computational costs. 
We demonstrate this by deriving a low-rank extension of \gls{alicec}, called \glsreset{alice}\gls{alice}. 

On experiments of pre-training LLaMA \citep{touvron2023llama} with C4 dataset \citep{raffel2020C4}, we demonstrate that \gls{ssgd} and \gls{alice} consistently outperform Adam and several memory-efficient baselines. \gls{alice} achieves better than $2\times$ speed-ups compared to Adam, and \gls{ssgd} performs strongly on pre-training the 1B LLaMA. Additionally, our preliminary results indicate that the 1B model trained by \gls{alice} reaches evaluation perplexity on-par or better than the 7B model trained with other memory-efficient baselines. 

To summarize, our contributions are:
\begin{itemize}
    \item We propose structured \gls{fim} approximation as a practical framework for optimizer design and show that several existing optimizers can be viewed as special cases within this framework.
    \item We propose to design optimizers by choosing structures that balance generality and efficiency and demonstrate it with a new optimizer, \gls{ssgd}.
    \item We present a low-rank extension framework to convert full-rank optimizers to corresponding low-rank approximations and demonstrate it with a new optimizer \gls{alice}.
    \item We demonstrate the effectiveness of \gls{ssgd} and \gls{alice} on LLaMa pre-training tasks when compared to Adam and other baselines. 
\end{itemize}