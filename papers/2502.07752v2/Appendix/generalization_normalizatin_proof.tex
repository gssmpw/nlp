\subsection{Proof of \cref{coro: generalization to whitening and normalization} and \cref{prop: two sided scaling}}
\label{subapp: proof normalization}
Instead of proving the \cref{coro: generalization to whitening and normalization}, we propose a generalization to those gradient operations, where \cref{coro: generalization to whitening and normalization} is a special case.

\paragraph{Structure assumption} We consider $\family=\{\mS\otimes \mM\}$ with identical \gls{spd} $\mM\in\Rmm$ and positive diagonal $\mS\in\Rnn$. 
The following theorem proves that the optimal solution can be solved by a fixed-point iteration. 
\begin{theorem}
    Assuming $\family=\{\mS\otimes \mM\}$ with positive diagonal $\mS\in\Rnn$ and \gls{spd} $\mM\in\Rmm$, and $\E_{GG'}[(\mG^T\mG')\elesquare]$ contains positive values, solving \cref{eq: UFE equation} admits a fixed point procedure:
    \begin{align}
        \diag(\mS) = \frac{\diag(\E[\mG^T\mM\mG])}{\Fnorm{\mM}},\;\;\;
        \mM=\frac{\E[\mG\mS\mG^T]}{\Fnorm{\mS}}.
        \label{eq: optimal S and M}
    \end{align}
    The solution $\diag(\mS^*)$ converges to the principal eigenvector of $\E[(\mG^T\mG')\elesquare]$ up to a scaling with unique $\mS^*\otimes \mM^*$. 
    \label{thm: generalization to normal and whiten}
\end{theorem}

To prove \cref{thm: generalization to normal and whiten}, we first introduce some classic results. 
\begin{theorem}[Perron-Frobenius theorem]
For a matrix $\mA\in\Rnn$ with positive entries, the principal eigenvalue $r$ is positive, called Perron-Frobenius eigenvalue. The corresponding eigenvector $\vv$ of $\mA$ is called Perron vector and only contains positive components: $\mA\vv=r\vv$ with $v_i>0$. In addition, there are no other positive eigenvectors of $\mA$. 
\label{thm: Perron-Frobenius theorem}
\end{theorem}

\begin{definition}[Hilbert projective metric]
    For any given vectors $\vv$, $\vw$ in $C\slash \{0\}$ where $C$ is a closed convex pointed non-negative cone $C$, i.e.~$C\cap (-C)=\{0\}$, the Hilbert projective metric is defined as 
    \begin{align*}
        d_H(\vv,\vw) = \log \left(\max_i \frac{v_i}{w_i}\right) - \log \left(\min_i \frac{v_i}{w_i}\right)
    \end{align*}
\end{definition}
This is a pseudo metric since it has a scaling invariance property: $d_H(\vv,\alpha\vm)=d_H(\vv,\vm)$ for $\alpha>0$. This means $d_H(\vv,\vm)=0$ does not mean $\vv=\vm$ but $\vv=\alpha\vm$ with some positive scaling $\alpha$. However, this is a metric on the space of rays inside the cone. 

\begin{theorem}[Birkhoff-Hopf theorem]
Let $\mP\in\Rnn$ be a positive matrix and let
\begin{align*}
    \kappa(\mP) = \inf\left\{\alpha\geq 0: d_H(\mP\vx,\mP\vy)\leq \alpha d_H(\vx,\vy), \forall \vx,\vy \in C_+, \vx \sim \vy \right\}
\end{align*}
where $C_+$ is the cone that each element is non-negative and $\sim$ is the induced equivalence relation. Namely, if $\vx\sim\vy$, there exists $\alpha,\beta>0$ such that $\alpha\vx<\vy<\beta\vx$, and $\vx<\vy$ means $y-x\in C_+$. Then, it holds
\begin{align*}
    \kappa(\mP) = \tanh{\frac{1}{4}\Delta(\mP)}\;\;\;\text{with}\;\Delta(\mP) = \max_{i,j,k,l}\frac{P_{ij}P_{kl}}{P_{il}P_{kj}}
\end{align*}
\label{thm: Birkhoff-Hopf}
\end{theorem}
This theorem suggests that when $\mP$ is a positive matrix, the corresponding linear mapping is contractive since $\tanh(\cdot)\leq 1$ under Hilbert projective metric. 

Now, let's prove the \cref{thm: generalization to normal and whiten}.
\begin{proof}
    First, we can simplify the \cref{eq: UFE equation} using \cref{lemma: block diagonal simplification}:
    \begin{align*}
        &\Fnorm{\mS\otimes\mM - \mF}\\
        =& \sum_{i=1}^n S_i^2\Fnorm{\mF} - 2\tr(S_i\mM\E[\vg_i\vg_i^T]) +C
    \end{align*}
    Then, we simply take its derivative w.r.t. $s_i$, and obtain
    \begin{align*}
        &2S_i\Fnorm{\mM} = 2\tr(\mM\E[\vg_i\vg_i^T])\\
        \Longrightarrow& S_i = \frac{\tr(\mM\E[\vg_i\vg_i^T])}{\Fnorm{\mM}}\\
        \Longrightarrow& \diag(\mS) = \frac{\diag\left(\E[\mG^T\mM\mG]\right)}{\Fnorm{\mM}}
    \end{align*}
    Similarly, we have
    \begin{align*}
        \mM =& \frac{\sum_{i=1}^nS_i\E[\vg_i\vg_i^T]}{\Fnorm{\mS}}\\
        =& \frac{\E[\mG\mS\mG^T]}{\Fnorm{\mS}}
    \end{align*}
These define an iterative procedure. Next, we will show it converges.
Let's substitute $\mM$ into $\mS$, and obtain
\begin{align*}
    \mS =& \diag\left(\E_{G}\left[\mG^T\E_{G'}[\mG'\mS\mG^{'T}]\mG\right]\right)\alpha(\mS)\\
    =& \diag\left(\E_{GG'}\left[\underbrace{\mG\mG^{'T}}_{\mH}\mS\mG^{'T}\mG\right]\right)
\end{align*}
where $\alpha(\mS)$ is the scaling term. In the following, we use $\E$ as $\E_{GG'}$.
Since we can show
\begin{align*}
    S_i = \E\left[\sum_{j}^nS_j\right],
\end{align*}
we can write $\mS$ in its vector format:
\begin{align*}
    \vs = \underbrace{\E\left[\mH\elesquare\right]}_{\mP}\vs.
\end{align*}
From the assumption, we know $\mP$ contains only positive values, let's define a quotient space for positive vectors $\vs$ and $\vq$ under the equivalence relation $\vs\sim \vs'$ if $\vs =\alpha \vs'$ for some positive scaling $\alpha$. Namely, we define a space of rays inside the positive cone. Therefore, the Hilbert projective metric becomes a real metric inside the quotient space. 

From the \cref{thm: Birkhoff-Hopf}, we know the linear mapping associated with $\mP$ is contractive. Therefore, we can follow the proof of Banach fixed point theorem on the previously defined quotient space with Hilbert projective metric to show the convergence of this fixed point iteration on $\vs$.  

Now, we show the solution $\vs^*$ is always positive. Since it is converging, therefor, the solution satisfies 
\begin{align*}
    \vs^* = \alpha(\vs^*) \mP\vs^*
\end{align*}
This is equivalent to finding the eigenvectors of $\mP$. By leveraging Perron-Frobenius theorem (\cref{thm: Perron-Frobenius theorem}), we know $\vs^*$ is the principal eigenvector of $\mP$, and only contain positive values. It is also easy to verify that this fixed point converges upto a positive scaling factor (this is expected since the contractive mapping holds true for the quotient space with Hilbert metric, that is invariant to scaling.)

Although $\vs^*$ is not unique, but $\mS\otimes \mM$ is, since for arbitrary positive scaling $\beta$
\begin{align*}
    &\vs^{'*}=\beta\vs^* \Longrightarrow \mM^{'*} = \frac{1}{\beta}\frac{\E[\mG\mS^*\mG^T]}{\Vert\vs\Vert_2^2}\\
    \Longrightarrow& \mS^{'*}\otimes\mM^{'*} = \mS^*\otimes \mM^*
\end{align*}
\end{proof}

Therefore, \cref{coro: generalization to whitening and normalization} is a direct consequence by substituting $\Ft = \mI_n\otimes\mM$ and $\Ft = \mS\otimes \mI_m$ into \cref{eq: optimal D for compensation}.


Next, we prove \cref{prop: two sided scaling}.
\begin{proof}
    From the \cref{thm: generalization to normal and whiten}, the iterative procedure for $\mQ$ can be simply obtained by taking the diagonals of $\mM$:
    \begin{align*}
        \mQ = \frac{\diag\left(\E\left[\mG\mS\mG^T\right]\right)}{\Fnorm{\mS}}.
    \end{align*}
    Following the same proof strategy of \cref{thm: generalization to normal and whiten}, we substitute $\mQ$ into the update of $\mS$ and re-write it into the vector format. First, let's rewrite the update of $\mS$
    \begin{align*}
        &S_i \propto \E[\sum_{j=1}G_{ji}^2 Q_j]\\
        \Longrightarrow& \vs = \frac{\mP^T \vq}{\Vert\vq\Vert_2^2}
    \end{align*}
    where $\mP=\E[\mG\elesquare]$. Similarly, $\vq = \frac{\mP\vs}{\Vert\vs\Vert_2^2}$.
    Thus,
    \begin{align*}
        \vs = \alpha(\vs) \mP^T\mP \vs
    \end{align*}
    From the assumption $\mP$ contains only positive values, we can follow the exact same argument made in \cref{thm: generalization to normal and whiten} to show the convergence of this fixed point update and the positivity of the final solution $\vs^*$. Precisely, $\vs^*$ and $\vq^*$ are the right and left principal singular vectors of $\mP$, respectively, and $\mS^*\otimes \mQ^*$ are unique. 
\end{proof}