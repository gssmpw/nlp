\subsection{Proofs of \gls{alicec}}
\label{subapp: proofs of alicec}
% proof of general UDU under kernel PCA
% proof of the 1-step refinement
\subsubsection{Proof of \cref{thm: alicec 1 step refinement}}

\begin{proof}
    For simplicity, we omit the subscript $f$ in $\mU_{f}$. If we assume all $\mD_i$ are equal and only contain positive values, then each block $\mU\mD_i\mU^T$ are the same for all $i$, and it is \gls{spd} matrix. Then, to minimize the the loss \cref{eq: UFE equation}, we can directly leverage the whitening results in \cref{coro: generalization to whitening and normalization}, and obtain $\mM^*=\E[\mG\mG^T]$. Due to the structure of $\mU\mD\mU^T$, the optimal $\mU^*$ is exactly the eigen-matrix of $\mM^*$. 

    Next, we prove for any fixed $\mU$, we can find the corresponding optimal $\mD_i$. 
    From the block diagonal structure and \cref{lemma: block diagonal simplification}, we have
    \begin{align*}
        &\Fnorm{\Ft-\mF}\\
        =&\sum_{i=1}^n \Fnorm{\mU\mD_i\mU^T} - 2\tr\left(\mU^T\E[\vg_i\vg_i^T]\mU \mD_i\right) + C\\
        =& \sum_{i=1}^n \Fnorm{\mD_i} - 2\tr\left(\mU^T\underbrace{\E[\vg_i\vg_i^T]}_{\mH_i}\mU\mD_i\right) + C\\
        =&\sum_{i=1}^n\sum_{j=1}^m D_{i,jj}^2 - 2\sum_{i=1}^n\sum_{j=1}^m D_{i,jj}\vu_j^T\mH_i\vu_j + C
    \end{align*}
Taking the derivative w.r.t. $D_{i,jj}$, we can find the optimal $D_{i,jj}$ is 
\begin{align*}
    D^*_{i,jj} =& \vu_j^T\mH_i\vu_j\\
    =& \E[(\vu_j^T\vg_i)^2]
\end{align*}
Now, by simple algebra manipulation, we have
\begin{align*}
    \mD^*_i = \diagv(\E[(\mU^T\vg_i)^2])
\end{align*}
where $\diag_M$ is to expand the vector to a diagonal matrix. 
Finally, for $\tilde{\mD}$, we have
\begin{equation}
    \tilde{\mD} = \diagm(\E[(\mU^T\mG)\elesquare])
\end{equation}

The optimality of $\widetilde{\mD}$ can also be obtained by leveraging the Lemma 1 in \citep{george2018fast}, and set the eigenbasis as $\mI_n\otimes \mU$. 
\end{proof}