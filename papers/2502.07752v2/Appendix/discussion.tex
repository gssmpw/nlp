\section{Further discussion}
\label{app: further discussion}
\subsection{Connections between different structural assumptions}
\label{subapp: connections between structural assumptions}
Here, we will make explicit connections between different structures in terms of their generality.

\paragraph{\gls{alicec} generalizes Adam} Since the structure behind \gls{alicec} is $\diagb(\mU\mD_1\mU^T,\ldots,\mU\mD_n\mU^T)$, when constraining $\mU=\mI_m$, the resulting structural is pure diagonal matrix $\diagb(\mD_1,\ldots,\mD_n)$. This coincide with the pure diagonal structure behind Adam.

\paragraph{\gls{alicec} generalizes $\mS\otimes \mM$} \gls{alicec} not only extends Adam, but also generalizes the structure considered in \cref{thm: generalization to normal and whiten}. Consider setting $\mD_i = S_i\mD$, then we have
\begin{align*}
    \mU\mD_i\mU^T = S_i\underbrace{\mU\mD\mU^T}_{\mM}
\end{align*}
Therefore, $\diagb(\mU\mD_1\mU^T,\ldots, \mU\mD_n\mU^T) = \diagb(S_1\mM,\ldots,S_n\mM)$. Since the structures of normalization and whitening are special cases of \cref{thm: generalization to normal and whiten}, \gls{alicec} generalizes these two gradient operators as a consequence.  

\paragraph{SOAP generalizes \gls{alicec}} We can re-write the structural assumption of \gls{alicec} in the Kronecker product format:
\begin{align*}
    \diagb(\mU\mD_1\mU^T,\ldots, \mU\mD_n\mU^T) = (\mI\otimes \mU)\underbrace{\diagb(\mD_1,\ldots,\mD_n)}_{\tilde{\mD}}(\mI\otimes \mU^T).
\end{align*}
It is clear that this is a special case of SOAP structure by containing $\mU_R=\mI$. 

\paragraph{SOAP generalizes Shampoo} The structure of Shampoo consists of two \gls{spd} matrices $\mR_n$ and $\mL_m$. If we eigen-decompose those, and let $\mR_n=\mU_R\mD_R\mU_R^T$ and $\mL_m=\mU_L\mD_L\mU_L^T$, then the structure of Shampoo can be re-write as 
\begin{align*}
    &\mR_n^{\frac{1}{2}}\otimes \mL_m^{\frac{1}{2}}\\
    =&(\mU_R\sqrt{\mD_R}\mU_R^T)\otimes(\mU_L\sqrt{\mD_L}\mU_L^T)\\
    =& (\mU_R\otimes \mU_L)\underbrace{(\sqrt{\mD_R\otimes \mD_L})}_{\tilde{\mD}} (\mU_R^T\otimes \mU_L^T).
\end{align*}
This coincides with SOAP's structure when the positive diagonal $\tilde{\mD}$ can be decomposed based on Kronecker product. 

\subsection{Memory consumption comparison}
\label{subapp: more thorough memory consumption}
\begin{table}[h]
\centering
Apart from the \cref{tab: summary table} provided in the main paper, we also include the memory consumption of more optimizers. 
\resizebox{\textwidth}{!}{
\begin{tabular}{l|l|llll}
\hline
              & Adam & GaLore & Fira & \gls{alice} & \gls{alicez}\\ \hline
Total         &  $3mn$    &  $mn+2nr+mr$ & $mn+2nr+mr$  &  $mn+2nr+mr+r^2+n$       &   $mn+2nr+mr+n$        \\ \hline
Weight        &   $mn$   &  $mn$   &  $mn$       &  $mn$       &  $mn$      \\
First moment  &  $mn$    &  $nr$   &   $nr$      &     $nr$    &    $nr$     \\
Second moment &   $mn$   &  $nr$  &    $nr$     &   $nr$      &      $nr$    \\
$\mU$ & N/A     &  $mr$   &    $mr$     &   $mr$      &   $mr$      \\
$\mC_t$  &  N/A    &  N/A   &    N/A     &    $n$     &    $n$     \\
$\widetilde{\mQ}$  &  N/A    &  N/A   &   N/A      &    $r^2$    &    N/A     \\
\hline
\end{tabular}}
\caption{The memory consumption of low-rank optimizers. Here, we assume the weight has a shape $m\times n$ with $m<n$ and $r\ll m$. Note that memory consumption $n$ and $r^2$ is typically very small. For example, let's take the largest possible $n=30K$ at the output layer. For 1B LLaMA, we select $r=512$, leading to $r^2\approx 262K$, which is 8x larger than $n$. However, both are marginal compared to $mr = 5120\times512$, which is 10x larger than $r^2$, and 80x larger than $n$.}
\label{tab: memory consumption for low-rank}
\end{table}



\subsection{Comparison to previous \gls{fim} approximation}
\label{subapp: comparison to previous work}
The idea of efficiently approximating the \gls{fim} is not novel, and has been extensively studied in the previous work. KFAC \citep{martens2015optimizing} is a well-known second order optimization algorithm for neural networks based on structural approximation to \gls{fim}. In particular, they explicitly express the \gls{fim} in terms of the gradient w.r.t this layer's output and the input to this layer. Namely, they directly decompose the gradient $\mG$ used in our paper, whereas in our paper, we treat $\mG$ as a whole quantity. In addition, the original KFAC also makes one crucial approximation: $\E[\mA\otimes \mB]\approx \E[\mA]\otimes \E[\mB]$. They do not show theoretically whether this is a good approximation and under what metric, but argue in practice this gives good accuracy. Last but not least, the original KFAC considers the \gls{fim} for entire layers, and each block (i.e.~corresponding to each layer) under their setup is actually the entire \gls{fim} we aim to approximate. To be precise, the principle is to structurally approximate the diagonal block of KFAC without decomposing the gradient $\mG$ using KFAC. 

Another more related work is AdaGrad \citep{duchi2011adaptive}. If we only considers layer-wise gradient, the full AdaGrad matrix is the $\sum_{t=1}^T\vecg_t\vecg_t^T$. Although our principle is to approximate \gls{fim}, in practice, we use \gls{ema} as the approximation of $\E$. Therefore, our principle can also be viwed as a structural approximation to full \gls{ema} AdaGrad matrix. Under this view point, there are some previous work on structural approximations. Shampoo \citep{gupta2018shampoo}, was originally proposed as an approximation to full AdaGrad matrix under the online learning setup. However, they do not explicit show under what metric this approximation is based on, and whether it is optimal or not. Later, there is a follow-up work \citep{morwani2024new}, showing that Shampoo is a 1-step power iteration algorithm of optimal Kronecker product approximation to \gls{fim} under Frobenius norm \citep{koroko2022efficient}. In this paper, we further extend the idea of approximating \gls{fim}/AdaGrad matrix to more efficient structures, under Frobenius norm. 

\subsection{Solutions to general block diagonal structures}
\label{subapp: solution to general block diagonal}
Here, we present the solution of the most general block diagonal approximation to \gls{fim}, and discuss why this is not practical. 

\begin{proposition}[General block diagonal approximation]
    Assume $\Ft=\diag(\mM_1,\ldots, \mM_n)$ with \gls{spd} matrix $\mM_i\in\Rmm$, then minimizing \cref{eq: UFE equation} admits analytic solutions:
    \begin{align}
        \mM_i^* = \E[\vg_i\vg_i^T]
        \label{eq: optimal general block diagonal}
    \end{align}
where $\vg_i$ is the $i^{\text{th}}$ column of $\mG$.
\end{proposition}
\begin{proof}
    This is straightforward by taking the derivative w.r.t. $\mM_i$. By leveraging \cref{lemma: block diagonal simplification}, we have
    \begin{align*}
        &\Fnorm{\Ft-\mF}\\
        =&\sum_{i=1}^n \Fnorm{\mM_i} - 2\tr(\mM_i^T\E[\vg_i\vg_i^T])
    \end{align*}
    Then, taking derivative w.r.t. $\mM_i$, we get 
    \begin{align*}
        \mM_i^* = \E[\vg_i\vg_i^T]
    \end{align*}
\end{proof}
From the above, although it admits analytic solutions, its corresponding practical procedure requires the \gls{ema} to estimate $\E[\vg_i\vg_i^T]\in\Rmm$ for all $i=1,\ldots, n$, leading to expensive memory cost $nm^2$. Another issue is that it does not allow efficient computation of the inverse. From the property of block diagonal matrix, to compute $\Ft^{-\frac{1}{2}}\vecg$, one needs to invert each $\mM_i$, incurring $O(m^3)$ computational cost. In total, the computational cost of inverting matrix $\Ft$ is $O(nm^3)$. Due to both memory and computational constraints, this structural assumption does not lead to practical algorithms. 


\subsection{Connections to existing optimizers}
\label{subapp: connection to existing optimizers}
\paragraph{Lars and Lamb}
Lars and Lamb \citep{you2017lars, you2019lamb} relies on the normalization operation of the gradients. The main difference is that Lars proposed to normalize the raw gradient, whereas Lamb normalizes the processed gradient from Adam optimizer. They also involves a scaling parameter that is a function of the weight. Compared to the normalization discussed in \cref{subsec: sve}, the main difference is that we use channel-wise normalization with unit column or row norm, whereas Lars/Lamb uses matrix-wise normalization where the norm of the matrix is regularized to be 1. However, if one vectorizes the matrix weight into a vector, and stacks those vectors into a larger matrix. Then, the normalization step of Lamb and Lars can be viewed as a 1-sample approximation to \gls{fim}\footnote{This \gls{fim} is now the full Fisher across layers, compared to the 1-layer \gls{fim} considered in the paper} under the structure considered in \cref{subsec: sve}. 

\paragraph{Muon}
Muon \citep{jordan2024muon} performs the whitening operation on the momentum. \Cref{subapp: Muon} gives a brief introduction. In fact, Muon can be viewed as a special case of \cref{eq: generalization whitening} in \cref{coro: generalization to whitening and normalization} by considering the following approximation:
\begin{equation}
    \E[\mG\mG^T] \approx \E[\mG]\E[\mG^T].
\end{equation}
Thus, the resulting operation becomes the whitening of the $\E[\mG]$, which is estimated by the momentum in practice. There exists one difference: Muon omits the $\frac{1}{n}$ scalar, which serves as a layer-wise effective learning rate. 

\paragraph{SWAN}
SWAN composes the gradient normalization and whitening operations as the replacement of Adam's first and second moments. Each individual operations can be viewed as a special case of \cref{coro: generalization to whitening and normalization}. However, \cref{subsec: sve} does not provide an explanation for composing these two operators. Namely, the whitening operator is estimated using normalized gradients, rather than the raw gradient. We will leave the investigation of operator composition for future work. 

\paragraph{Adapprox}
Adapprox \citep{zhao2024adapprox} uses low-rank approximation for Adam's second moments through randomized \gls{svd} to boost the memory efficiency. However, its performance will be similar to Adam. In fact, \Cref{prop: two sided scaling} of \gls{ssgd} proves that the converged $\vs$ and $\vq$ represents the right and left singular vectors, coinciding with the rank-1 reconstruction. However, the proposed \gls{ssgd} is different from rank-1 Adapprox in three main aspects: (1) Adapprox proposes to use low-rank \gls{ema} tracking on $\mG\elesquare$, whereas we use \gls{ema} directly on scaling vectors $\vs$, $\vq$. The resulting vectors are no longer singular vectors; (2) we do not have separate scaling apart from norm-growth limiter and user-defined scale, whereas Adapprox uses the scaling from Adafactor; (3) \gls{ssgd} do not use any first moment. Specifically, (1) is an important difference, since for any low-rank reconstruction with rank $r>1$, it is not guaranteed to be positive, causing numerical issue during square-root inverse scaling. Adapprox adds a manually defined offset to ensure positivity. On the other hand, \gls{ssgd} is guaranteed to have positive $\vs$, $\vq$ at each step from Perron-Frobenius theorem. Therefore, \gls{ssgd} is numerically stable. 

\paragraph{Adafactor}
Adafactor \citep{shazeer2018adafactor} is another optimizer that uses rank-1 approximation to Adam's second moment. However, their scaling is derived by minimizing a different norm, compared to Frobenius norm in this paper. Adapprox \cite{zhao2024adapprox} has demonstrated the advantages of using Frobenius norm compared to Adafactor with a slightly better performance on LLM training. 

\paragraph{AdaDiag and one-sided SOAP}
We acknowledge that there exists two concurrent work: AdaDiag \citep{anonymous2024improving} and one-sided SOAP \citep{vyas2024soap}, that are mathematically equivalent to \gls{alicec}. They are derived based on distinct view points. AdaDiag is based on the intuition to transform the gradient covariance matrix so that it is diagonalizable, where this diagonal matrix is estimated using Adam's second moments. One-sided SOAP, a memory-efficient version of SOAP, is proposed to based on intuitions that only one-sided eigenspace is enough for LLM training. On the other hand, \gls{alicec} is derived on the basis of the structured \gls{fim} view point. providing a deeper connections to optimizers considered in this paper. 



\paragraph{Apollo}
There is one concurrent work, Apollo \citep{zhu2024apollo}, that is also based on scaling the raw stochastic gradients for memory-efficient LLM training. It proposed to scale columns \textbf{or} rows of the $\mG$ through a scaling matrix estimated by similar procedure as Fira \cite{chen2024fira}. The main idea is that column or row norm after scaling matches the column or row norm of the gradient from GaLore update. Thus, they require a GaLore procedure to compute this update at each step. Our proposed \gls{ssgd} scales both columns and rows at the same time. The main differences are: (1) the scaling estimation in \gls{ssgd} is different from Apollo; (2) the scaling scheme of \gls{ssgd} is inspired by the generalization of normalization, providing theoretical support. They enjoy a similar memory consumption (i.e.~$mn+2n+2$ of Apollo compared to $mn+m+n+1$ of \gls{ssgd}). 

\paragraph{Fira}
Fira \cite{chen2024fira} was proposed as an improvement towards GaLore, which modifies the low-rank update to full-rank one by adding a compensation term. Their idea is similar to the compensation used in \gls{alice}. The main differences is that our proposed compensation is inspired by approximating \gls{fim} and has the theoretical foundation on its optimality. Also, the compensation strategy is different to Fira. We have conduced the ablation study in \cref{sec: experiments} to show the advantage of our approach. 

\paragraph{GaLore}
Comparing the \gls{alice} procedure to GaLore (\cref{alg: GaLore optimizer}), we can clearly see that GaLore is a special case of \gls{alice} by disabling tracking, switching and compensation. From the connection of \gls{alice} to \gls{alicec}, GaLore is a simple low-rank extension of \gls{alicec}, a more general optimizer than Adam. Therefore, the \gls{fim} view point reveals that GaLore is inherently a different optimizer compared to Adam, despite its similarity to Adam's update. This also provides an explanation on why GaLore can sometimes outperforms Adam under certain scenarios. 



\subsection{Discussion of low-rank extension framework}
\label{subapp: discussion low-rank}
First, we derive how to decompose the full-rank update of \gls{alicec} into low-rank update and its residuals in the main text. We assume $\tilde{\mU}=[\mU,\mU_c]$, where $\mU$, $\mU_c$ are defined in \cref{subsec: compensation}.
\begin{align*}
    \devect(\Ft^{-\frac{1}{2}}\vecg) =& \tilde{\mU}\frac{\tilde{\mU}^T\mG}{\sqrt{\E[(\tilde{\mU}^T\mG)\elesquare]}}\\
    =& [\mU,\mU_c] \frac{\left[\begin{array}{c}
         \mU^T  \\
          \mU_c^T
    \end{array}\right]\mG}{\sqrt{\E\left[\left(\left[\begin{array}{c}
         \mU^T  \\
          \mU_c^T
    \end{array}\right]\mG\right)\elesquare\right]}}\\
    =& [\mU,\mU_c]\frac{\left[\begin{array}{c}
         \mU^T\mG  \\
         \mU_c^T\mG 
    \end{array}\right]}{\sqrt{\E\left[\left(\left[\begin{array}{c}
         \mU^T\mG  \\
         \mU_c^T\mG
    \end{array}\right]\right)\elesquare\right]}}\\
    =&[\mU,\mU_c]\left[\begin{array}{c}
           \frac{\mU^T\mG}{\sqrt{\E[(\mU^T\mG)\elesquare]}}\\
         \frac{\mU_c^T\mG}{\sqrt{\E[(\mU_c^T\mG)\elesquare]}}
    \end{array}\right]\\
    =& \mU\frac{\mU^T\mG}{\sqrt{\E[(\mU^T\mG)\elesquare]}} + \mU_c\frac{\mU_c^T\mG}{\sqrt{\E[(\mU_c^T\mG)\elesquare]}}
\end{align*}

\paragraph{Moore-Penrose inverse} In \cref{eq: alice update decomposition}, we define $\Ft^{-\frac{1}{2}}$ using the pseudo-inverse since each block in $\Ft_c$ is low-rank and not invertible. To be precise, for any block $\mU_c\mD_{ci}\mU_c^T$, its pseudo-inverse can be easily verified as $\mU_c\mD_{ci}^{-1}\mU_c^T$ by checking the definition. Similar to typical inverse, for any block diagonal $\Ft_c$, the pseudo-inverse is equivalent to applying pseudo-inverse of each blocks. The square-root pseudo inverse can be defined through a similar way. 

Similarly, for the proposed compensation, we can easily verify its vectorized format (ignoring the subscript $t$ for simplicity)
\begin{align*}
    \vect(\mU_c\mU_c^T\mG\mS) =& (\mS\otimes \mU_c\mU_c^T)\vecg\\
    =& (\mS^{-2}\otimes \mU_c\mU_c^T)^{-\frac{1}{2}}\vecg
\end{align*}
where the $^{-\frac{1}{2}}$ is the pseudo-inverse as the above. The second inequality can be easily verified by the following facts:
(1) the square root pseudo inverse of $\mU_c\mU_c^T$ is itself; (2) the square root pseudo-inverse of block-diagonal matrix is the pseudo-inverse of each individual block. 