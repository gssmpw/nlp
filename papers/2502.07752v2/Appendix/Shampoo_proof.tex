\subsection{Proof of \cref{thm: optimal shampoo}}
\label{subapp: proof of shampoo optimiality}

To prove this theorem, we need to leverage the \cref{coro: generalization to whitening and normalization} for generalized whitening (\cref{eq: generalization whitening}) in \cref{subsec: sve}. This is proved in \cref{subapp: proof normalization}. But in the following, we will provide an alternative proof for completeness.

\begin{proof}
    From \cref{lemma: block diagonal simplification}, we have
    \begin{align*}
        &\Fnorm{\Ft-\mF} \\
        =& \sum_{i=1}^n \Fnorm{\mM} - 2\tr(\mM^T\E[\vg_i\vg_i^T]) + C\\
        =& n\Fnorm{\mM} - 2\tr(\mM^T\E[\sum_{i=1}^n \vg_i\vg_i^T]) + C\\
        =& n\Fnorm{\mM} - 2\tr(\mM^T\E[\mG\mG^T]) + C \\
    \end{align*}
    To minimize this, we take the derivative w.r.t. $\mM$, we have
    \begin{align*}
        2n\mM - 2\E[\mG\mG^T] = 0 \Rightarrow \mM = \frac{1}{n} \E[\mG\mG^T]
    \end{align*}
\end{proof}
Next, we prove another proposition that is "symmetric" to the whitening results in \cref{coro: generalization to whitening and normalization}.
\begin{proposition}
    Assume $\family = \{\mR_n \otimes \mI_m\}$, where $\mR_n\in \Rnn$ is \gls{spd} matrix, then \cref{eq: UFE equation} can be analytically solved with the optimal solution as 
    \begin{equation}
        \mR_n^* = \frac{1}{m} \E[\mG^T\mG]
        \label{eq: optimal shampoo right}
    \end{equation}
    \label{prop: optimal shampoo right}
\end{proposition}
\begin{proof}
    Since $\mR_n\otimes \mI_m$ does not have a nice block diagonal structure like the previous proposition, we need to analyze it a bit more. First, we have
    \begin{align*}
        &\Fnorm{\mR_n \otimes \mI_m - \mF}\\
        =& \Fnorm{\mR_n\otimes \mI_m} - 2\tr\left(\underbrace{(\mR_n\otimes \mI_m)^T\E[\vecg\vecg^T]}_{\mZ}\right) + C\\
    \end{align*}
Since we only care about the diagonal of $\mZ$, therefore, we only inspect the block diagonal of $\mZ$ with each block $\mZ_i$ of size $\Rmm$, and $i=1,\ldots, n$. By basic algebra, we have
\begin{align*}
    \mZ_i = \sum_{k=1}^n R_{ik} \vg_k\vg_i^T
\end{align*}
where $\vg_k$ is the $k^{\text{th}}$ column of $\mG$. Therefore, we can simplify the trace of $\mZ$ as 
\begin{align*}
    \tr(\mZ) &= \sum_{i=1}^n\tr(\mZ_i)\\
    =& \tr(\sum_{i=1}^n\sum_{k=1}^n R_{ij}\vg_k\vg_i^T)\\
    =& \sum_{i=1}^n\sum_{k=1}^n\sum_{j=1}^m R_{ik}[\mG]_{ji}[\mG^T]_{kj}
\end{align*}
where $[\mG]_{ji}$ is the element of $\mG$ at $j^{\text{th}}$ row and $i^{\text{th}}$ column. 

Now, let's perform the same analysis of the following quantity
\begin{align*}
    \tr\left((\mI_m \otimes \mR_n)\E[\vecgt\vecgt^T]\right)
\end{align*}
where $\vecgt$ is the vectorized transposed gradient $\mG^T$. Namely, it now stacks the rows of $\mG$ instead of columns of $\mG$ like $\vecg$. This object is simple to treat due to its block diagonal structure, by algebric manipulation, we have
\begin{align*}
    \tr\left((\mI_m\otimes \mR_n)\E[\vecgt\vecgt^T]\right) &= \underbrace{\sum_{k=1}^m}_{\text{over blocks}}\tr(R_{ij}\underbrace{[\mG^T]_k}_{\text{kth column of }\mG^T}[\mG^T]_k^T)\\
    =&\sum_{k=1}^m \sum_{i=1}^n \sum_{j=1}^n R_{ij} [\mG^T]_{jk}[\mG]_{ki}
\end{align*}
Now, let's change the variable $i=i$, $j=k$ and $k=j$, the above becomes
\begin{align}
    &\tr\left((\mI_m\otimes \mR_n)\E[\vecgt\vecgt^T]\right) \nonumber\\
    =& \sum_{j=1}^m \sum_{i=1}^n \sum_{k=1}^n R_{ik} [\mG^T]_{kj}[\mG]_{ji} \nonumber\\
    =& \tr(\mZ) \label{eq: proof NI=IN}
\end{align}
We should also note that
\begin{align*}
    &\Fnorm{\mR_n\otimes \mI_m}\\
    =& \tr\left((\mR_n\otimes \mI_m)^T(\mR_n\otimes \mI_m)\right)\\
    =& \tr\left((\mR_n^T\mR_n)\otimes \mI_m\right)\\
    =& \tr(\mR_n^T\mR_n)\tr(\mI_m)\\
    =&\tr\left((\mI_m\otimes \mR_n)^T(\mI_m\otimes \mR_n)\right)\\
    =& \Fnorm{(\mI_m\otimes \mR_n)}
\end{align*}
Therefore, by using the above equation and \cref{eq: proof NI=IN}, the original minimization problem is translated to 
\begin{align*}
    \argmin_{\mR_n} \Fnorm{\mR_n\otimes \mI_m -\mF} = \argmin_{\mR_n}\Fnorm{\mI_m \otimes \mR_n -\E[\vecgt\vecgt^T]}
\end{align*}
Thus, we can leverage \cref{coro: generalization to whitening and normalization} to obtain the optimal solution
\begin{align*}
    \mR_n^* = \frac{1}{m} \E[\mG^T\mG]
\end{align*}
\end{proof}

With the above two propositions, we can start to prove \cref{thm: optimal shampoo}.
\begin{proof}
    First, we note that
    \begin{align*}
        &\Fnorm{\Rnr\otimes \Lmr - \mF}\\
        =& \Fnorm{\underbrace{(\mR_n\otimes \mI_m)^{\frac{1}{2}}}_{\mA}\underbrace{(\mI_n\otimes \mL_m)^{\frac{1}{2}}}_{\mB}-\underbrace{\E[\vecg\vecg^T]^{\frac{1}{2}}}_{\mC}\E[\vecg\vecg^T]^{\frac{1}{2}}}\\
        =& \Fnorm{\mA\mB - \mC\mC}
    \end{align*}
    Next, we will upper bound this quantity.
    First, we have
    \begin{align*}
        \mA\mB-\mC\mC = \mA(\mB-\mC) + (\mA-\mC)\mC
    \end{align*}
    By triangular inequality, we have
    \begin{align*}
        &\Vert\mA\mB-\mC\mC\Vert_F\\
        &\leq \Vert\mA(\mB-\mC)\Vert_F+\Vert(\mA-\mC)\mC\Vert_F \\
        &\leq \Vert\mA\Vert_F\Vert\mB-\mC\Vert_F+ \Vert\mA-\mC\Vert_F\Vert\mC\Vert_F\\
        &\leq (\Vert\mA-\mC\Vert_F+\Vert\mC\Vert_F)\Vert\mB-\mC\Vert_F+ \Vert\mA-\mC\Vert_F\Vert\mC\Vert_F\\
        &= \Vert\mA-\mC\Vert_F\Vert\mB-\mC\Vert_F+\Vert\mC\Vert_F(\Vert\mB-\mC\Vert_F+ \Vert\mA-\mC\Vert_F)
    \end{align*}
    Now, the squared norm can be upper bounded by 
    \begin{align}
        \Fnorm{\mA\mB-\mC\mC} \leq& 3\left(\Fnorm{\mA-\mC}\Fnorm{\mB-\mC}+\Fnorm{\mC}\Fnorm{\mA-\mC}+\Fnorm{\mC}\Fnorm{\mB-\mC}\right)\nonumber\\
        \leq&3\left(mn\Vert\mA^2-\mC^2\Vert_F\Vert\mB^2-\mC^2\Vert_F+\sqrt{mn}\Fnorm{\mC}\Vert\mA^2-\mC^2\Vert_F+\sqrt{mn}\Fnorm{\mC}\Vert\mB^2-\mC^2\Vert_F\right)
        \label{eq: proof upper bound shampoo}
    \end{align}
The first inequality is obtained by the fact that for any three matrix $\mP$, $\mQ$ and $\mH$, we have
\begin{align*}
    \Fnorm{\mP+\mQ+\mH}\leq& \left(\Vert\mP\Vert_F+\Vert\mQ\Vert_F+\Vert\mH\Vert_F\right)^2\\
    =& \Fnorm{\mP}+\Fnorm{\mQ}+\Fnorm{\mH} + 2\Vert\mP\Vert_F\Vert\mQ\Vert_F + 2\Vert\mP\Vert_F\Vert\mH\Vert_F+2\Vert\mQ\Vert_F\Vert\mH\Vert_F\\
    \leq& 3\left(\Fnorm{\mP}+\Fnorm{\mQ}+\Fnorm{\mH}\right)
\end{align*}
The second inequality is obtained by directly applying Powers-Stormer's inequality and Holder's inequality. For completeness, we will show how to upper-bound $\Fnorm{\mA-\mC}$, the rest can be bounded in the same way. 
From \cref{lemma: powers stormer inequality} and both $\mA$, $\mC$ are \gls{spd} matrix, we have
\begin{align*}
    \Fnorm{\mA-\mC}\leq \Vert\mA^2-\mC^2\Vert_1
\end{align*}
Then, we can select $p=q=2$ for Holder's inequaity and obtain
\begin{align*}
    \Vert\mA^2-\mC^2\Vert_1\leq \sqrt{mn}\Vert\mA^2-\mC^2\Vert_F
\end{align*}
where $\sqrt{mn}$ comes from the $\Vert\mI_{mn}\Vert_F$ in Holder's inequality. By substitute it back, we obtain the upper bound.

We can see that minimizing the upper bound \cref{eq: proof upper bound shampoo} is equivalent to minimize each $\Vert\mA^2-\mC^2\Vert_F$, $\Vert\mB^2-\mC^2\Vert_F$ individually, and 
\begin{align*}
    \Vert\mA^2-\mC^2\Vert_F &= \Vert\mR_n\otimes \mI_m - \mF\Vert_F\\
    \Vert\mB^2-\mC^2\Vert_F &= \Vert\mI_n\otimes \mL_m-\mF\Vert_F
\end{align*}
Thus, from \cref{coro: generalization to whitening and normalization} and \cref{prop: optimal shampoo right}, we prove the theorem. 
\end{proof}
