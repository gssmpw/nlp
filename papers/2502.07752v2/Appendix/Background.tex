\section{Examples of diagonal operations}
\label{app: example diagonal operations}
In this section, we will give some detailed examples on each of the diagonal operations we introduced in \cref{sec: preliminary}. 
\paragraph{Example of $\diag(\cdot)$}
This will extract the diagonals of a input matrix into a vector:
\begin{align*}
    \mM = \left[\begin{array}{ccc}
       a_{11}  & a_{12}&a_{13}  \\
        a_{21} & a_{22} & a_{23}\\
        a_{31}&a_{32} & a_{33}
    \end{array}\right],\;\;\; \diag(\mM) = [a_{11}, a_{22}, a_{33}]^T
\end{align*}

\paragraph{Example of $\diagb(\cdot)$}
This simply stack the input matrices sequence into a larger block diagonal matrix:
\begin{align*}
    \diagb(\mM_1,\mM_2,\mM_3) = \left[\begin{array}{ccc}
        \mM_1 & \bm{0} & \bm{0}  \\
        \bm{0} & \mM_2 & \bm{0}\\
        \bm{0} & \bm{0} & \mM_3
    \end{array}\right]
\end{align*}

\paragraph{Example of $\diagv(\cdot)$}
This will stack the vector element into a pure diagonal matrix:
\begin{align*}
    \diagv([a_{11},a_{22}, a_{33}]^T) = \left[\begin{array}{ccc}
        a_{11} & 0 & 0  \\
         0&a_{22}&0\\
         0&0&a_{33}
    \end{array}\right]
\end{align*}

\paragraph{Example of $\diagm(\cdot)$}
This will stack the elements in input matrix to form a larger pure diagonal matrix in a column-wise manner.
\begin{align*}
    \diagm\left(\left[\begin{array}{cc}
        a_{11} & a_{12}  \\
        a_{21} &  a_{22}
    \end{array}\right]\right) = \left[\begin{array}{cccc}
        a_{11} & 0 & 0 &0  \\
        0 & a_{21} & 0 & 0\\
        0&0&a_{12} & 0\\
        0&0&0&a_{22}
    \end{array}\right]
\end{align*}

\section{Background}
\label{app: background}
In this section, we will provide a more comprehensive backgrounds.

\subsection{Fisher information}
\label{subapp: background fisher information}
Fisher information can also be viewed as the "sharpness" of the likelihood around the true parameters from the maximum likelihood view point. 
Formally, under the context of LLMs $f_\theta(\cdot)$, we consider a dataset $\{\vx_i\}_{i=1}^N$, where $N$ is total batched sentences, and $\vx_i$ is the token sequence of $i^{\text{th}}$ sentence. One can define sentence-level auto-regressive loss function $\mathcal{L} = \sum_{j=1}c(x_{i,j+1}, f_{\theta}(\vx_{i,:j})$, where $j$ is the token index and $c$ is the user-defined loss metric. The corresponding likelihood can be defined as $p_\theta(\vx_i)\propto \exp(-\sum_{j=1}c(x_{i,j+1},f_{\theta}(\vx_{i,:j})))$. 
The standard empirical \gls{fim} is defined as 
\begin{align*}
    \mF = \sum_{i=1}^N\nabla_\theta \log p_\theta(\vx_i)\nabla_\theta^T\log p_\theta(\vx_i)
\end{align*}
In practice, we often use the mini-batched gradient $\vecg = \frac{1}{N_B}\sum_{i=1}^{N_B} \log p_\theta(\vx_i)$ with batch size $N_B$ for empirical \gls{fim} during training. More discussion can be found in \cite{lin2024can}.
Throughout this paper, we adopt the notation $\E[\vecg\vecg^T]$ as $\mF$. 

One standard application of \Gls{fim} is efficient optimization. \Gls{ngd} leverage the inverse of \gls{fim} to smooth the local information geometry, leading to the steepest descent in the probability space with KL divergence metric. This typically leads to faster convergences compared to its parameter-space counterpart; and more stable optimization \citep{martens2020new}. The update of \gls{ngd} with step size $\lambda$ is
\begin{align*}
    \theta\leftarrow \theta-\lambda \mF^{-1}\nabla_{\theta}\mathcal{L}.
\end{align*}
However, square-root inverse is sometimes more favorable than inverse, and has been shown that it provides a better approximation to the geodesic flow, compared with the default natural gradient update \cite{yang2008principal}. Empirically, it demonstrates stronger performance and desired properties when using non-constant learning rate \citep{lin2024can, loshchilov2016sgdr, bergstra2012random,choi2019empirical}. 
The corresponding update is to simply replace $\mF^{-1}$ with $\mF^{-\frac{1}{2}}$:
\begin{align}
    \theta\leftarrow \theta-\lambda \mF^{-\frac{1}{2}}\nabla_{\theta}\mathcal{L}.
\end{align}
In this paper, we will use the square-root inverse view point, but our analysis is agnostic to it and can be easily extended to the direct inverse. However, matrix multiplication involving \gls{fim} and its inverse are computationally expensive for large models since $\mF\in\R^{mn\times mn}$ for vectorized parameters $\theta\in\R^{mn}$. Next, we will briefly introduce Kronecker product and block diagonals, which are two main classes of structural assumptions used in this paper. Their nice properties can significantly reduce the assocaited computation burden. 


\subsection{Kronecker product and block diagonals}
\label{subapp: kronecker product and block diagonals}
Kronecker product, denoted by $\otimes$, is to combine two arbitrary matrices into a larger matrix with block-wise patterns. It has emerged as a powerful tool for approximating higher-order information like Hessian \citep{grosse2016kronecker} or \gls{fim}. 
The main advantages are their nice properties regarding matrix operations, leading to efficient practical procedures when dealing with large matrix. We will mainly use two properties:
\begin{align}
    (\mA\otimes \mB)^{-\frac{1}{2}} &= \mA^{-\frac{1}{2}}\otimes \mB^{-\frac{1}{2}} \label{eq: Kronecker product root inverse}\\
    (\mA\otimes \mB)\vect(\mC) &= \vect(\mB\mC\mA^T)\label{eq: Kronecker product vector matrix conversion}
\end{align}
where the first one holds when $\mA$,$\mB$ are square-root invertible. The second one is particular useful to reduce the computation associated with matrix-vector multiplication in \cref{eq: square root ngd}. For $\mA\otimes\mB \in \R^{mn\times mn}$, it reduces the computation from $O(m^2n^2)$ to $O(mn^2+m^2n)$ with $\mA\in\Rnn$, $\mB\in\Rmm$, and $\mC\in\Rmn$. 

For block diagonal matrix, one can also easily compute its square-root inverse:
\begin{align}
    \diagb(\mM_1,\ldots,\mM_n)^{-\frac{1}{2}} = \diagb(\mM_1^{-\frac{1}{2}},\ldots,\mM_n^{-\frac{1}{2}})
    \label{eq: block diagonal square root inverse}
\end{align}
where each $\mM_i$ is square-root invertible. When each $\mM_i$ is also a diagonal matrix with positive values, we have the following:
\begin{equation}
    \diagb(\mM_1,\ldots,\mM_n)^{-\frac{1}{2}}\vect(\mC) = \vect\left(\frac{\mC}{\sqrt{\devect(\vv)}}\right)
    \label{eq: block diagonal elementwise division}
\end{equation}
where $\vv$ is the vector containing the diagonals of $\diagb(\mM_1,\ldots,\mM_n)$, transforming matrix vector product into element-wise operation.  

\subsection{Operators for gradient: normalization and whitening}
\label{subapp: normalization and whitening}
Recently, there are some optimizers \citep{ma2024swan, jordan2024muon, you2017lars, you2019lamb} that apply operators to pre-process the gradient and use it in standard SGD. Empirical evidence has verified their effectiveness in training LLMs. In particular, there are two well-known operators: normalization and whitening, where the above optimizers relies on one or both of them. In particular,
\begin{align}
    \normalize(\mG) =& \frac{\mG}{\bm{1}\sqrt{\vs^T}}= \mG\mS^{-\frac{1}{2}}\\
    \whiten(\mG) =& (\mG\mG^T)^{-\frac{1}{2}}\mG.
\end{align}
where $\vs\in\R^n$, $\vs_i= \sum_{j=1}^m \mG^2_{ij}$ with $\mG\in\Rmn$, and $\mS=\diagv(\vs)$. Namely, vector $\vs$ contains the squared column norm of $\mG$, and $\mS$ is a scaling matrix to normalize the columns. Normalizing the rows can also be written in a similar format. 
$\whiten$ operator essentially orthogonalizes $\mG$, that compute the closest orthogonal matrix to $\mG$ under Frobenius norm. In practice, the whitening operator can be iteratively solved using Newton-Schulz algorithm without explicitly computing the square-root inverse. 

\subsection{Shampoo Optimizer}
\label{subapp: Shampoo optimizer}
Shampoo \citep{gupta2018shampoo} was originally proposed as the second-order optimization technique over the tensor space. Under the context of transformers, typically matrix parameters are considered. Its core design principle also aims to approximate the \gls{fim} with structural assumptions $\Ft_t=\mR_{n,t}^{\frac{1}{2}}\otimes \mL_{m,t}^{\frac{1}{2}}$, with $\mR_{n,t} = \mR_{n,t-1}+ \mG_t^T\mG_t$ and $\mL_{m,t} = \mL_{m,t-1}+ \mG_t\mG_t^T$. The above update rule can be seen as the moving average estimate of $\E[\mG_t^T\mG_t]$ and $\E[\mG_t\mG_t^T]$.
However, the Shampoo paper \citep{gupta2018shampoo} does not explicitly show why these design choices of $\mR_n$, $\mL_m$ approximate the \gls{fim}. In fact, they only show $\mR_n^{\frac{1}{2}}\otimes \mL_m^{\frac{1}{2}}$ forms an upper bound of \gls{fim}\footnote{Lemma 8 in \citep{gupta2018shampoo}. Note that our paper assumes $\vect(\cdot)$ is stacking columns of matrix whereas \cite{gupta2018shampoo} assumes stacking the rows, explaining the reverse order of presentation}. This is not helpful in understanding whether this approximates the \gls{fim} or not. Another very recent follow-up work \citep{morwani2024new} provides an explanation of the shampoo preconditioner in terms of approximating \gls{fim}. They show the square of Shampoo's preconditioner is equivalent to a single step of power iteration of computing optimal Kronecker product approximation to \gls{fim}. It indirectly establishes the connections to \gls{fim} approximation since the approximation is expressed as an iterative algorithm. To the best of our knowledge, our result is the first to directly establish the connection of Shampoo to \gls{fim} approximation as minimizing a upper bound of the loss with the Frobenius norm (\cref{thm: optimal shampoo}). 

Nevertheless, the original Shampoo algorithm is summarized in \cref{alg: Shampoo optimizer}. 

\begin{algorithm}
    \caption{Shampoo Optimizer}
    \label{alg: Shampoo optimizer}
    \begin{algorithmic}
        \STATE {\bfseries Input:} $\mL_m=\epsilon\mI_m$, $\mR_n=\epsilon\mI_n$, learning rate $\lambda$, optimization step $T$, loss function $\mathcal{L}$.
        \FOR{$t=1,\ldots, T$}
            \STATE $\mG_t=\nabla_{\mW_t}\mathcal{L}$
            \STATE $\mL_{m,t}=\mL_{m,t-1}+\mG_t\mG_t^T$
            \STATE $\mR_{n,t}=\mR_{n,t-1}+\mG_t^T\mG_t$
            \STATE $\mW_{t}=\mW_{t-1}+\lambda \mL_{m,t}^{-\frac{1}{4}}\mG_t\mR_{n,t}^{-\frac{1}{4}}$
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

\subsection{SOAP/AdaDiag++}
\label{subapp: SOAP}
SOAP/AdaDiag++ \citep{vyas2024soap, anonymous2024improving} is a recently proposed adaptive optimizer aiming to improve the practical convergence and stability of Shampoo. Their main intuition behind (from the view point of \cite{vyas2024soap}) is that they show Shampoo is equivalent to performing Adafactor \citep{shazeer2018adafactor} under the Shampoo's eigen-space. Namely, the eigen-matrix of $\mL_{m,t}$ and $\mR_{n,t}$ in \cref{alg: Shampoo optimizer}. Since Adafactor is an approximation to Adam, they propose to use Adam instead of Adafactor in Shampoo's eigen-space, to further improve the performance. They propose the following update rule: 
\begin{align}
    &\vm_{t} = \beta_1\vm_{t-1}+(1-\beta_1)\mG_t\;\;\; (\text{first moment})\nonumber\\
    &\mL_{m,t} = \beta_3\mL_{m,t-1}+(1-\beta_3)\mG_t\mG_t^T\;\;\;(\text{Shampoo's }\mL_{m,t})\nonumber\\
    & \mR_{n,t} = \beta_3\mR_{n,t-1} + (1-\beta_3) \mG_t^T\mG_t\;\;\;(\text{Shampoo's }\mR_{n,t})\nonumber\\
    & \mU_{L,t} = \eig(\mL_{m,t})\nonumber \;\;\;(\text{Shampoo's left eigen-space})\\
    & \mU_{R,t} = \eig(\mR_{n,t})\nonumber \;\;\; (\text{Shampoo's right eigen-space})\\
    & \vv_t = \beta_2\vv_{t-1}+(1-\beta_2)(\mU_{L,t}^T\mG_t\mU_{R,t})\elesquare\;\;\;(\text{second moment})\nonumber\\
    &\Delta = \mU_{L,t}\frac{\mU_{L,t}^T\vm_{t}\mU_{R,t}}{\sqrt{\vv_t}}\mU_{R,t}^T
\label{eq: practical soap updates}    
\end{align}
These update rules exactly describes the procedure to applying Adam updates in the "rotated" space defined by $\mU_{L,t}$ and $\mU_{R,t}$. Due to the computational burden associated with $\eig$, SOAP proposed to only update $\mU_{L,t}$, $\mU_{R,t}$ at certain intervals. This leads to the following algorithm:
\begin{algorithm}
    \caption{SOAP optimizer}
    \label{alg: SOAP optimizer}
    \begin{algorithmic}
        \STATE {\bfseries Input:} learning rate $\lambda$, update interval $K$, $\beta_1$, $\beta_2$, $\beta_3$, optimization step $T$.
        \STATE $\vm_0=0$; $\vv_0=0$ \COMMENT{Initialize two moments}
        \STATE $\mL_{m,0}=0$; $\mR_{n,0}=0$ \COMMENT{Initialize two \gls{ema} states for $\mG\mG^T$, $\mG^T\mG$}
        \FOR{$t=1,\ldots, T$}
            \STATE $\mG_t=\nabla_{\mW_t}\mathcal{L}$
            \STATE $\vm_t = \beta_1\vm_{t-1}+(1-\beta_1)\mG_t$

            \STATE $\mL_{m,t}=\beta_3\mL_{m,t-1}+(1-\beta_3)\mG_t\mG_t^T$ \COMMENT{Accumulation for $\mG\mG^T$}
            \STATE $\mR_{n,t}=\beta_3\mR_{n,t-1}+(1-\beta_3)\mG_t^T\mG_t$ \COMMENT{Accumulation for $\mG^T\mG$}
            \IF{$t==1$ or $(t\mod K)==0$}
                \STATE $\mU_{R,t}=\eig(\mR_{n,t})$ \COMMENT{Get right eigen-space $\mU_R$}
                \STATE $\mU_{L,t}=\eig(\mL_{m,t})$ \COMMENT{Get left eigen-space $\mU_L$}
            \ELSE
                \STATE $\mU_{R,t}=\mU_{R,t-1}$
                \STATE $\mU_{L,t}= \mU_{L,t-1}$
            \ENDIF
            \STATE $\widetilde{\vm}_t = \mU_{L,t}^T\vm_t\mU_{R,t}$ \COMMENT{Get rotated 1st moment}
            \STATE $\vv_t = \beta_2\vv_{t-1}+(1-\beta_2)(\mU_{L,t}^T\mG_t\mU_{R,t})\elesquare$ \COMMENT{Compute second moments}
            \STATE $\mW_{t+1}=\mW_{t} -\lambda \mU_{L,t}\frac{\widetilde{\vm}_t}{\sqrt{\vv_t}}\mU_{R,t}^T$
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

\subsection{AdaDiag and one-side SOAP}
\label{subapp: AdaDiag}
AdaDiag++ \citep{anonymous2024improving}, a concurrent work to SOAP, independently develops the equivalent update rules as SOAP. The only difference is that they disable the \gls{ema} tracking for $\mL_{m,t}$ and $\mR_{n,t}$. The resulting optimizer is both computational and memory expensive due to the storage of $\mU_{R}$, $\mU_L$ and two eigenvalue decompositions. To address this issue, they both propose a one-side version called AdaDiag and one-side SOAP by only considering either left or right eigen-space. The resulting update rule is exactly the same as our proposed \gls{alicec} (i.e.~\cref{eq: practical alicec equations}). 

However, they propose this design choice purely based on intuition to reduce computation and memory consumption, and do not explicitly reveal the connections to their two-sided version. Thus, it lacks the understanding on why two-sided version obtains empirically better performance. 
Based on \cref{subsec: alicec} and \cref{subsec: new opt combination},  we show that although one-sided version has similar updates as the two-sided twins, they are different optimizers with distinct underlying structural assumptions. In fact, the structures of SOAP/AdaDiag++ strictly generalizes their one-sided version, explaining the better empirical performance. 
The resulting algorithm is the following:
\begin{algorithm}
    \caption{\gls{alicec}/AdaDiag/one-side SOAP optimizer}
    \label{alg: alicec optimizer}
    \begin{algorithmic}
        \STATE {\bfseries Input:} learning rate $\lambda$, update interval $K$, $\beta_1$, $\beta_2$, $\beta_3$, optimization step $T$.
        \STATE $\vm_0 = 0$; $\vv_0=0$ \COMMENT{Initialize two moments}
        \STATE $\mQ_0=0$ \COMMENT{Initialize the \gls{ema} state for $\mG\mG^T$}
        \FOR{$t=1,\ldots,T$}
            \STATE $\mG_t=\nabla_{\mW_t}\mathcal{L}$
            \STATE $\mQ_t = \beta_3 \mQ_{t-1} + (1-\beta_3)\mG_t\mG_t^T$ \COMMENT{Accumulate the $\mG\mG^T$}
            \STATE $\vm_t = \beta_1\vm_{t-1}+(1-\beta_1)\mG_t$ \COMMENT{Accumulate the first moment}
            \IF{$t==1$ or $(t \mod K)==0$}
                \STATE $\mU_{f,t} = \eig(\mQ_t)$ \COMMENT{Obtain the $\mU$}
            \ELSE
                \STATE $\mU_{f,t} = \mU_{f,t-1}$
            \ENDIF
            \STATE $\tilde{\vm}_t = \mU_{f,t}^T\vm_t$ \COMMENT{Rotate the first moment}
            \STATE $\vv_t = \beta_2\vv_{t-1}+(1-\beta_2)(\mU_{f,t}^T\mG_t)\elesquare$ \COMMENT{Accumulate the second moments}
            \STATE $\mW_{t+1}=\mW_t -\lambda \mU_{f,t}\frac{\tilde{\vm}_t}{\sqrt{\vv_t}}$\COMMENT{Update in the original space}
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

\subsection{SWAN}
\label{subapp: SWAN optimizer}
Recently, there is a newly proposed adaptive optimizer that completely removes the needs of storing internal states, called SWAN. It relies on two processing operators applied to raw current gradient: \emph{GradNorm} and \emph{GradWhitening}, as a replacement of first and second moments. For a current gradient $\mG$,
\begin{align}
    &\mathtt{GradNorm}(\mG) = \frac{ \mG - \bar{\vecg} \mathbf{1}_{n}^\top }{ \vs \mathbf{1}_{n}^\top  } \\
    & \mathtt{GradWhitening}(\mG) = (\mG\mG^T)^{-\frac{1}{2}}\mG
    \label{eq: gradnorm and whitening}
\end{align}
where $ \bar{\vecg} = \frac{1}{n} \sum_{i=1}^{n} \mG_{:, i} $ is the mean across rows; $ \vs = \sqrt{ \frac{1}{n} \sum_{i=1}^{n} (\mG_{:, i} - \bar{\vecg})^2} $ is the standard deviation across rows; $\bar{\vecg}$ and $\vs$ are ${m}$-dimensional column vectors; and $ \mathbf{1}_{n} $ is a ${n}$-dimensional column vector of ones. 
Then, SWAN performs the following to generate the update:
\begin{align}
    \tilde{\mG} = \mathtt{GradNorm}(\mG)\nonumber\\
    \Delta = \mathtt{GradWhitening}(\tilde{\mG})
    \label{eq: SWAN update}
\end{align}
We can see that the proposed $\mathtt{GradNorm}$ is equivalent to normalization up  to a scaling $\sqrt{n}$ when the mean $ \bar{\vecg}$ is $0$.
SWAN derives these two steps from investigating the LLM dynamics. In practice, SWAN proposes to compute the $(\mG\mG^T)^{-\frac{1}{2}}$ using Newton-Schulz iterations.

\subsection{Newton Schulz iteration}
\label{subapp: Newton schulz iteration}
In many machine learning applications, like successive whitening and coloring transform \citep{li2017universal,cho2019image,choi2021robustnet}, one often encountered the computation of square root inverse of some \gls{spd} matrix. One standard approach is to compute the \gls{evd} and take the square root inverse of the eigenvalue matrix. However, \gls{evd} is computationally expensive. Another alternative approach is use Newton-Schulz iteration (NS), an iterative updates of two matrix. Specifically, 
\begin{align*}
    \mY_0 &= \frac{\mA}{\Vert\mA\Vert_F}\;\;\;\ \mZ_0=\mI\\
    \mY_{t+1} &= \frac{1}{2}\mY_t(3\mI-\mZ_t\mY_t)\\
    \mZ_{t+1} &= \frac{1}{2}(3\mI-\mZ_t\mY_t)\mZ_t
\end{align*}
with convergence $\mY_t \rightarrow \frac{\mA^{\frac{1}{2}}}{\sqrt{\Vert\mA\Vert_F}}$ and $\mZ_t\rightarrow \mA^{-\frac{1}{2}}\sqrt{\Vert\mA\Vert_F}$. Typically, NS converges very fast with only 5 steps \citep{li2018towards,huang2019iterative}. 

\subsection{Muon}
\label{subapp: Muon}
Muon \citep{jordan2024muon}, is recently proposed to speed-up the training of LLMs, that relies on the whitening operator similar to SWAN. The core of the Muon is to orthogonalize the the momentum. The proposed update rule is 
\begin{align}
    \vm_t =& \beta_1 \vm_{t-1}+(1-\beta_1)\mG_t\nonumber\\
    \Delta =& \mathtt{GradWhitening}(\vm_t).
    \label{eq: Muon update}
\end{align}
Similarly, the $\mathtt{GradWhitening}$ step is computed using Newton-Schulz iteration. The main difference between Muon and SWAN is that Muon still requires the storage of first moments as state, whereas SWAN relies on the $\mathtt{GradNorm}$ operator applied to the raw gradient.  

\subsection{Lars}
\label{subapp: Lars}
SWAN and Muon both involve the whitening operator with/without normalization, respectively. On the other hand, Lars \citep{you2017lars} is an operator that only relies on layer-wise normalization. For each layer, it simply compute the first moments, followed by a normalization operation. The update rule for each layer is 
\begin{align}
    \vm_t = \beta_1\vm_{t-1} + (1-\beta_1)\mG_t\nonumber\\
    \Delta = \phi(\Vert\theta\Vert)\frac{\vm_t}{\Vert\vm_t\Vert}
    \label{eq: Lars update}
\end{align}
where $\phi$ is a scaling function with input of the parameter $\theta$ norm. One major difference of this layer-wise normalization to SWAN is that it is applied on the layer-wise level, whereas SWAN applies row/column-wise normalization. 


\subsection{Low rank optimizers}
\label{subapp: Low rank optimizers}
The primary goal of low-rank optimizer is to reduce the memory consumed by the states of adaptive optimizers. The popularity of low-rank based method for large models starts from the well-known LoRA \citep{hu2021lora}, where each weight is inserted with an low-rank adapter $\mW+\mA\mB$ with $\mA \in \Rmr$ and $\mB\in \Rrn$ during the finetuning stage. This formulation directly modifies the model architecture. \cite{si2024flora} explicitly show that LoRA is secretly a gradient compressor, which translates the modification to model architecture into a low-rank optimizer with randomly sampled matrix. At the same time, GaLore \citep{zhao2024galore} popularizes the use of low-rank optimizers, which demonstrates on-par performance compared to full-rank Adam training. GaLore is proposed based on the analysis of reversible networks \citep{tian2020understanding}. However, in practice, transformer may not satisfy the reversibility condition. Thus, GaLore does not provide a clear understanding on why it works on LLMs. 

\Cref{alg: GaLore optimizer} summarizes the procedures. In practice, GaLore can be viewed as the composition of subspace search algorithm (i.e.~SVD) with standard Adam optimizer. The original GaLore does not provide an explanation on the choice of Adam. On the other hand, our analysis reveals that the GaLore is an approximate low-rank extension to a different optimizer, \gls{alicec}/AdaDiag/one-side SOAP, that is generalizes Adam (see \cref{subsec: alicec}). 

\begin{algorithm}
    \caption{GaLore Optimizer}
    \label{alg: GaLore optimizer}
    \begin{algorithmic}
        \STATE \bfseries{Input:} learning rate $\lambda$, decay rates $\beta_1$, $\beta_2$, rank $r$, update interval $k$, scale $\alpha$
        \FOR{$t=1,\ldots, T$}
        \STATE $\mG_t=\nabla_{\mW_t}\mathcal{L}$
        \IF{$t==1$ or $t\mod k==0$}
            \STATE $\mU_t=\svd(\mG_t, r)$
        \ELSE
            \STATE $\mU_t=\mU_{t-1}$
        \ENDIF
        \STATE $\bm{\sigma}_t=\mU_t^T\mG_t$
        \STATE $\Delta = Adam(\bm{\sigma}_t, \beta_1,\beta_2)$
        \STATE $\mW_t = \mW_{t-1} + \lambda\alpha\Delta$
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

Since the states of Adam optimizer are based on the projected gradient $\bm{\sigma}_t$, the overall memory consumption of GaLore is $mn+2nr+mr$. 

\subsection{Apollo}
\label{subapp: Apollo}
Concurrent to this work, there is a recently proposed optimizer, called Apollo \citep{zhu2024apollo}, that only scales the raw gradient and obtains SGD-like memory with Adam-level performance. They propose to scale the columns or rows similar to normalization in SWAN, but the scaling factor is estimated following the procedure proposed by Fira \citep{chen2024fira}. The core idea of Apollo is to obtain $\Delta$ from GaLore algorithm (\cref{alg: GaLore optimizer}), followed by computing the column/row-wise norm of $\Delta$. This norm will be used as the scaling factor for the raw gradient $\mG$. Apollo has many variants. In particular, we choose Apollo-mini and Apollo-svd, where the former uses rank-1 random projection for scaling estimation, and the latter relies on the use of top $r$ singular vectors as the projection, same as GaLore.
Apollo-mini only maintains the rank-1 states, leading to significant memory savings. The memory consumption is $mn+2n+2$ for parameter $\mW\in\Rmn$. And Apollo-svd consumes the same memory as GaLore. \cref{alg: Apollo optimizer} summarizes the procedures.

\begin{algorithm}
    \caption{Apollo Optimizer}
    \label{alg: Apollo optimizer}
    \begin{algorithmic}
        \STATE \bfseries{Input:} learning rate $\lambda$, decay rates $\beta_1$, $\beta_2$, rank $r$, update interval $k$, scale $\alpha$
        \FOR{$t=1,\ldots, T$}
        \STATE $\mG_t=\nabla_{\mW_t}\mathcal{L}$
        \IF{$t==1$ or $t\mod k==0$}
            \STATE $\mU_t\sim \mathcal{N}(0,\frac{1}{r})$
            \STATE seed $\leftarrow$ an independent new random seed
        \ELSE
            \STATE $\mU_t=\mU_{t-1}$
        \ENDIF
        \STATE $\bm{\sigma}_t=\mU_t^T\mG_t$
        \STATE $\Delta_t = Adam(\bm{\sigma}_t, \beta_1,\beta_2)$
        \STATE $\mS_t \leftarrow \diagv(s_0, \ldots, s_m)\;\{s_i=\frac{\Vert\Delta_{t,:,i}\Vert_2}{\Vert\bm{\sigma_{t,:,i}}\Vert_2}\}$ 
        \STATE $\mW_t = \mW_{t-1} + \lambda\alpha\mG_t\mS_t$
        \ENDFOR
    \end{algorithmic}
\end{algorithm}
Note that when rank $r$ is set to $1$, they propose to use global scaling $\frac{\Vert\Delta_t\Vert_2}{\Vert\bm{\sigma_t}\Vert_2}$ instead of row/column-wise scaling. 


\subsection{Subspace iteration}
\label{subapp: background subspace iteration}
The subspace iteration method—also known as the block power method is a classical iterative technique for computing the dominant eigenvalues and corresponding eigenvectors of a matrix. It generalizes the basic power method from operating on a single vector to operating on a subspace, typically spanned by a initial matrix. When the initial matrix is closed to the targeting eigen-matrix, the convergence is fast. Empirically, we found that only 1 step of iteration is enough to give a satisfactory performance. \Cref{alg: subspace iteration} summarizes the subspace iteration algorithm to for finding the top $r$ eigenvectors of a matrix $\mA$. We can see that the computation is bottlenecked by the matrix multiplication $\mA\mU_{t-1}$ which is $O(m^2r)$ if only performing 1 step. 

\begin{algorithm}
    \caption{Subspace iteration}
    \label{alg: subspace iteration}
    \begin{algorithmic}
        \STATE \bfseries{Input:} symmetric matrix $\mA\in\Rmm$, iteration step $T$, initial matrix $\mM\in\Rmr$
        \STATE $\mU_0=\mM$
        \FOR{$t=1,\ldots, T$}
        \STATE $\mH_t = \mA\mU_{t-1}$
        \STATE $\mU_t=\text{QR decomposition}(\mH_t)$
        \ENDFOR
        \STATE $\mV = \mU_T^T\mA\mU_T$
        \STATE $\mU = \eig(\mV)$
        \STATE {\bfseries Return} $\mU$
    \end{algorithmic}
\end{algorithm}

% \section{SOAP: A further generalization to \gls{alicec}}
% \label{subsec: new opt combination}
% All previous structures, apart from the one behind Shampoo, are under the class of block diagonal. However, this only allows one to model the off-diagonal elements near the diagonal. Structure under Kronecker product, like the one behind Shampoo, can go beyond this. 
% Therefore, we can consider combining the structure of \gls{alicec} with Shampoo, to obtain the most general structural assumption in this paper. We show this exactly recovers SOAP \citep{vyas2024soap}.

% Specifically, we consider the following structural assumption: $\family = \{(\mU_R\otimes \mU_L)\tilde{\mD}(\mU_R\otimes \mU_L)^T\}$, where $\mU_R\in\Rnn$, $\mU_L\in\Rmm$ are orthonormal matrix, and $\tilde{\mD}\in\mathbb{R}^{mn\otimes mn}$ is a diagonal matrix with positive values. We can easily show that structure behind \gls{alicec} is a special case by constraining $\mU_R=\mI_n$; and Shampoo is also a special case by constraining $\tilde{\mD}$ to be decomposed by Kronecker product (refer to \cref{subapp: connections between structural assumptions}). 

% Similar to \gls{alicec}, it is hard to directly minimizing \cref{eq: UFE equation} with this assumption. We can approximate the solution by 1-step refinement procedure as \gls{alicec}. 

% \begin{theorem}[1-step refinement of SOAP]
%     Assuming the above structural assumptions.
%     Consider the following 1-step refinement: (1) assume $\tilde{\mD}$ can be decomposed as Kronecker product of two diagonal matrix, find corresponding $\mU_R$, $\mU_L$; (2) fix $\mU_R$, $\mU_L$, find corresponding $\tilde{\mD}$.
%     Step (1) admits analytic for minimizing the upper bound of \cref{eq: UFE equation} (i.e.~\cref{eq: shampoo upper bound}):
%     \begin{align*}
%         \mU_R = \eig(\E[\mG^T\mG]),\;\;\; \mU_L = \eig(\E[\mG\mG^T]).
%     \end{align*} Step (2) admits an analytic solution for minimizing \cref{eq: UFE equation}:
%     \begin{align*}
%         \tilde{\mD} = \diagm(\E[(\mU_L^T\mG\mU_R)\elesquare])
%     \end{align*}. 
%     \label{thm: optimal asham}
% \end{theorem}
% The proof is a straightforward combination of 
% \cref{thm: optimal shampoo} and \cref{thm: alicec 1 step refinement}, and can be found in \cref{subapp: proof asham}. One can show that the corresponding optimizer associated with the above result is equivalent to SOAP (refer to \cref{subapp: update formula for SOAP} for details). 
