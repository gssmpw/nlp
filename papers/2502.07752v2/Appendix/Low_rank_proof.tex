\subsection{Proof of \cref{prop: subspace switching}}
\label{subapp: proof subspace switching}
\begin{proof}
    Within the time block $i+1$ with low-rank mapping $\mU$, the gradient at each step can be decomposed as 
    \begin{align*}
        \mG_t = \underbrace{\mU\mU^T\mG_t}_{\widetilde{\mG_t}} + \underbrace{(\mG_t - \mU\mU^T\mG_t)}_{\mR_t}
    \end{align*}
    Therefore, the true state $\mQ^*_{(i+1)k}$ can be simplified as 
    \begin{align*}
        & \mQ_{(i+1)k} = \sum_{t=ik+1}^{(i+1)k}\mG_t\mG_t^T\\
        =& \sum_{t=ik+1}^{(i+1)k} (\widetilde{\mG}_t+\mR_t)(\widetilde{\mG}_t+\mR_t)^T\\
        =& \sum_{t=ik+1}^{(i+1)k} \widetilde{\mG}_t\widetilde{\mG}_t^T+ \underbrace{\widetilde{\mG}_t\mR_t^T}_{0}+ \underbrace{\mR_t\widetilde{\mG}_t^T}_{0} + \mR_t\mR_t^T
    \end{align*}
The third equality is obtained because we assume $\mG_t\mG_t^T$ shares the same eigen-basis as $\mQ^*_{ik}$. Namely, 
\begin{align*}
\mG_t\mG_t^T=&[\mU,\mU_c]\left[\begin{array}{cc}
    \mA_{t} & 0 \\
   0  & \bm{\Sigma}_t
\end{array}\right]\left[\begin{array}{c}
     \mU^T  \\
     \mU_c^T
\end{array}\right]\\
=& \mU\mA_t\mU^T + \mU_c\bm{\Sigma}_t\mU_c^T
\end{align*}
where $\mA_t$ and $\bm{\Sigma}_t$ are diagonal matrix. Then, we have
\begin{align*}
    &\widetilde{\mG}_t\mR_t^T\\
    =& \mU\mU^T\mG_t(\mU_c\mU_c^T\mG_t)^T\\
    =& \mU\mU^T(\mU\mA_t\mU^T+\mU_c\bm{\Sigma}_t\mU_c^T)\mU_c\mU_c^T\\
    =& \mU\mA_t\underbrace{\mU^T\mU_c}_{0}\mU_c^T + \mU\underbrace{\mU^T\mU_c}_{0}\bm{\Sigma}_t\mU_c^T\mU_c\mU_c^T\\
    =&\bm{0}
\end{align*}
In addition, we can also simplify
\begin{align*}
    &\mR_t\mR_t^T\\
    =& \mU_c\mU_c^T\mG_t\mG_t^T\mU_c\mU_c^T\\
    =& \mU_c\mU_c^T(\mU\mA_t\mU^T+\mU_c\bm{\Sigma}_t\mU_c^T)\mU_c\mU_c^T\\
    =&\mU_c\bm{\Sigma}_t\mU_c^T
\end{align*}
Therefore, 
\begin{align*}
    \mQ^*_{(i+1)k} = \sum_{t=ik+1}^{(i+1)k} \widetilde{\mG}_t\widetilde{\mG}_t^T + \mU_c\bm{\Sigma}_t\mU_c^T
\end{align*}
\end{proof}


\subsection{Proof of \cref{thm: optimal compensation}}
\label{subapp: proof optimal compensation}

\begin{proof}
For simplicity, we ignore the subscript $t$ for the following proof.
% First, we note that $(\mI-\mU\mU^T) = \mU_c\mU_c^T$.
First, we let $\mO=\mS^{-2}$
    Then, the loss function can be written as 
    \begin{align*}
        &\Fnorm{\mO\otimes \mU_c\mU_c^T-\Ft_{c}}\\
        =& \sum_{i=1}^n \Fnorm{O_{ii} \mU_c\mU_c^T} - 2\tr((O_{ii}\mU_c\mU_c^T)^T(\mU_c\mM_i\mU_c^T))\\
        =&\sum_{i=1}^n \Fnorm{O_{ii} \mU_c\mU_c^T} - 2\tr(O_{ii}\mM_i)\\
        =& \sum_{i=1}^n\sum_{k=1}^{m-r}\sum_{j=1}^m O_{ii}^2U_{c,jk}^2 - 2\tr(O_{ii}\mM_i)
    \end{align*}
    where $\mM_i=\diag(\E[(\mU_c^T\vg_i)^2])$, $\vg_i$ is the $i^{\text{th}}$ column of $\mG$, and $U_{c, jk}$ is the element in $j^{\text{th}}$ row, $k^{\text{th}}$ column of $\mU_c$. 
    Then, we take the derivative w.r.t. $O_{ii}$, and we have
    \begin{align*}
        &2O_{ii} \sum_{k=1}^{m-r}\sum_{j=1}^m U_{c,jk}^2 = 2\sum_{k=1}^{m-r} \E[(\mU_c^T\vg_i)^2_{k}]\\
        \Longrightarrow& O_{ii} = \frac{\E[\sum_{k=1}^{m-r}(\mU_c^T\vg_i)_k^2]}{m-r}
    \end{align*}
    This form still requires the access to $\mU_c$. Next, let's simplify it. 
    First, let $\tilde{\mU} = [\mU, \mU_c]$ to be the complete basis, we can show
    \begin{align*}
        &\sum_{k=1}^m (\tilde{\mU}^T\vg_i)^2_k \\
        =&\tr((\tilde{\mU}^T\vg_i)^T(\tilde{\mU}^T\vg_i))\\
        =&\tr(\vg_i^T\underbrace{\tilde{\mU}\tilde{\mU}^T}_{\mI}\vg_i)\\
        =&\vg_i^T\vg_i\\
    \end{align*}
    Now, let's re-write the above in a different format:
    \begin{align*}
    &\sum_{k=1}^m (\tilde{\mU}^T\vg_i)^2_k \\
        =&\tr(\vg_i^T\tilde{\mU}\tilde{\mU}^T\vg_i)\\
        =&\tr(\tilde{\mU}^T\vg_i\vg_i^T\tilde{\mU})\\
        =&\tr\left(\left[\begin{array}{c}
             \mU^T  \\
             \mU_c^T
        \end{array}\right]\vg_i\vg_i^T[\mU,\mU_c]\right)\\
        =& \tr(\mU\mU^T(\vg_i\vg_i^T)+ \mU_c\mU_c^T(\vg_i\vg_i^T))\\
        =& \tr((\mU^T\vg_i)^T(\mU^T\vg_i)) + \tr((\mU_c^T\vg_i)^T(\mU_c^T\vg_i))\\
        =& \sum_{k=1}^r(\mU^T\vg_i)_k^2 + \sum_{k=1}^{m-r} (\mU_c^T\vg_i)^2_k
    \end{align*}
    Therefore, we have
    \begin{align*}
        \E[\sum_{k=1}^{m-r}(\mU_c^T\vg_i)_k^2] = \E[\vg_i^T\vg_i-\sum_{k=1}^r (\mU^T\vg_i)_k^2]
    \end{align*}
    So, we have 
    \begin{align*}
        \diag(\mO) = \frac{\E[\bm{1}_m^T\mG\elesquare - \bm{1}_r^T(\mU^T\mG)\elesquare]}{m-r}
    \end{align*}
    and 
    \begin{align*}
        \diag(\mD) = \frac{\sqrt{m-r}}{\sqrt{\E[\bm{1}_m^T\mG\elesquare - \bm{1}_r^T(\mU^T\mG)\elesquare]}} 
    \end{align*}
    
    
\end{proof}