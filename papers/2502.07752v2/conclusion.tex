\section{Conclusion}
\label{sec: conclusion}
In this paper, we take a step toward the systematic design of efficient optimizers for LLMs through structured \gls{fim} approximation. We first establish the connection between structural assumptions and optimizers by solving the structured \gls{fim} approximation. Building on this insight, we propose two design approaches:
(1) Selecting a structure that balances generality and practical efficiency, then solving the \gls{fim} approximation problem accordingly;  
(2) Using a general structure for \gls{fim} approximation, followed by our proposed low-rank framework to improve efficiency.  
Following these principles, we develop two memory-efficient optimizers, \gls{ssgd} and \gls{alice}, each corresponds to one of these design approaches. Experimental validation on LLaMA pre-training demonstrates their effectiveness.  

Our work lays the foundation for a more systematic approach to efficient optimizer design, opening up several promising directions for future research, including: developing low-rank counterparts for SOAP; exploring other possible classes of structures; and investigating approximation problems beyond \gls{fim}.  
By providing a structured perspective on optimizer design, we hope to inspire further advancements in scalable and efficient training methods for LLMs.