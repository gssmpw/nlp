\paragraph{Notation and Setup:}
Let $M$ be a pretrained LLM with hidden dimension $d$. When $M$ processes an input sequence of tokens of length $n$, it produces hidden representations $\{\mathbf{h}_1, \mathbf{h}_2, \ldots, \mathbf{h}_n\}$ for each layer, where each $\mathbf{h}_t \in \mathbb{R}^d$. We consider three versions of \texttt{Gemma 2} models \cite{gemmateam2024gemma2improvingopen} in this work, the \textbf{2B},  \textbf{9B} and instruction-tuned variant, \textbf{9B-IT}.

\paragraph{SAE-Based Activation Extraction:}
We use \textbf{pretrained} SAEs provided by \texttt{Gemma Scope} \cite{lieberum2024gemma}, choosing the SAE with $L_0$ loss closest to 100. We extract each tokenâ€™s residual stream activations from layers that have been instrumented with the \texttt{SAELens} \cite{bloom2024saetrainingcodebase} tool. Specifically for the 2B model, we extract SAE features from layers 5, 12, 19 (early, middle, late) where 9B \& 9B-IT models with layers 9, 20, and 31 from the residual stream.

Each SAE has a designated \emph{width} (i.e., number of feature directions). We evaluate \textbf{16K} and \textbf{65K} widths for the 2B model, and \textbf{16K} and \textbf{131K} for 9B and 9B-IT \footnote{we choose 131k for 9B and 65k for 2B models due to their same expansion ratio to original model hidden states}, following the pretrained SAEs made available in \texttt{Gemma Scope} \cite{lieberum2024gemma}. \textbf{Note}: we do \textit{not} train any SAEs ourselves; our workflow involves only extracting the hidden states and the corresponding \emph{pretrained} SAE activations.

\paragraph{Pooling and Binarization}
Since SAEs generate token-level feature activations, an essential step in classification is aggregating these activations into a fixed-size sequence representation. Without pooling, the model lacks a structured way to combine token-level representations. Previous NLP works have explored various pooling strategies for feature aggregation in neural representations \cite{shen2018baseline}. However, it remains unclear which pooling method is most effective for LLMs' SAE features. We systematically evaluate different pooling approaches (displayed in \ref{fig:workflow}, considering (1) \emph{Top-$N$ feature selection per token} \footnote{Token-level top-$N$ where n=0 indicates the absence of max pooling.} and (2) \emph{summation-based aggregation}\footnote{this approach is also adopted by parallel research \cite{brinkmann2025large}.} which collapses token-level activations into a single sequence vector:

\begin{equation}
    \mathbf{F} = \sum_{t=1}^n \mathbf{f}_t,
\end{equation}

where $\mathbf{f}_t \in \mathbb{R}^m$ is the SAE feature vector of dimension $m$ for token $t$. The summation method aggregates all token activations, while top-n selects the strongest activations per token.

Beyond pooling, we investigate \emph{binarization} to enhance interpretability and efficiency. This transformation converts $\mathbf{F}$ into a binary vector $\mathbf{F}_{\text{bin}}$, activating only the dimensions that exceed a threshold:

\begin{equation}
    \mathbf{F}_{\text{bin}}[i] = 
    \begin{cases}
      1, & \text{if } \mathbf{F}[i] > 1, \\
      0, & \text{otherwise}.
    \end{cases}
\end{equation}

Binarization provides multiple advantages: (1) it produces compact, memory-efficient representations, (2) it acts as a non-linear activation akin to ReLU \cite{agarap2019deeplearningusingrectified}, and (3) it serves as an implicit feature selection mechanism, highlighting only the most salient SAE activations. By thresholding weaker activations, this approach enhances the robustness and interpretability of extracted features in downstream classification tasks.


\paragraph{Classification with Logistic Regression:}
To measure how informative these SAE-derived features are for various tasks, we train a \emph{logistic regression} (LR) classifier. In all experiments, LR models are evaluated using \textbf{5-fold cross-validation}. This is the only learned component of our pipeline;
\paragraph{Baselines:} We compare against:
\begin{itemize}[itemsep=-1.7pt,topsep=1.5pt]
    \item \textbf{TF-IDF}: A classic variation of bag-of-words without neural representations \cite{sparck_jones_1972}.
    \item \textbf{Hidden State}: Like prior studies \cite{features_as_classifiers}, we did compare to \emph{last-token} hidden state probing as well. 
\end{itemize}


\paragraph{Code and Reproducibility:}
All code for data loading, activation extraction, pooling, detailed hyper-parameters and classification results is provided in a public repository. A simple YAML configuration file controls model scale, layer indices, SAE width, and \texttt{huggingface} dataset paths, enabling reproducible workflows with Apache 2 license. All our experiments are conducted on three Nvidia A6000 GPUs with CUDA version 12.4. 
