\subsection{Interpretable Feature Extraction}

MI has evolved from neuron-level analysis to sophisticated feature extraction frameworks \cite{olah2020zoom, rajamanoharan2024improving}. Early approaches targeting individual neurons encountered fundamental limitations due to polysemanticity, where activation patterns span multiple, often unrelated concepts \cite{bolukbasi2021interpretability, elhage2022toy}. While techniques like activation patching \cite{meng2022locating} and attribution patching \cite{syed2023attribution} offered insights into component-level contributions, they highlighted the need for more comprehensive representational frameworks.

SAEs address these limitations by providing more interpretable feature sets \cite{bricken2023towards, cunningham2023scaling}. Recent scaling efforts have demonstrated SAE viability across LLMs from Claude 3 Sonnet \cite{templeton2024scaling} to GPT-4 \cite{gao2024scaling} with extensions to multimodal architectures like CLIP \cite{bhalla2024interpreting}. Although these studies have revealed interpretable feature dimensions and computational circuits \cite{marks2024sparse, zhao2024steeringknowledgeselectionbehaviours}, they focus mainly on descriptive feature discovery rather than systematic evaluation of their downstream applications. Our work bridges this gap by providing standardized evaluation frameworks for SAE-based classification and cross-modal transfer, establishing quantitative metrics and methods for feature utility across diverse tasks.

\subsection{SAE-Based Classification and its Limitations}
Reports have demonstrated that SAE-derived features can outperform traditional hidden-state probing for classification, particularly in scenarios with noisy or limited data with closed datasets \cite{anthropic2024features} or simplified tasks \cite{sae_probing}. However, more recent studies, such as \citet{wu2025axbenchsteeringllmssimple}, suggest that SAEs may not be superior, particularly for model steering (instead of classification). These seemingly conflicting results highlight a critical gap in the current understanding of SAE-based classification: a lack of systematic exploration of how hyperparameters, feature aggregation strategies, and other methodological choices impact performance. 

Existing evaluations often focus on narrow settings, making it unclear whether discrepancies arise from task differences, dataset choices, or specific configurations. This work addresses this gap by systematically evaluating SAE-based classification. We examine key hyperparameters and methodological choices like feature pooling, layer selection, and SAE width across diverse datasets and tasks, ensuring a fair comparison with established baselines.



