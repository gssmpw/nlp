In this work, we systematically evaluated the utility of \emph{pretrained} Sparse Autoencoders (SAEs) across a range of NLP tasks spanning single-label classification, multilingual toxicity detection, and behavioral (action) prediction. Our findings provide new evidence that summation-then-binarization of SAE-based features outperforms hidden-state probes and TF-IDF baselines in most settings, with comparatively modest hyperparameter tuning overhead. Below, we discuss key insights and practical takeaways from this study.

\paragraph{Interpretability and Layer-Wise Insights:}
A central observation is that \emph{middle-layer} SAE features often produce the highest accuracy across tasks. This trend echoes prior work suggesting that intermediate layers encode richer, more compositional representations than either early or late layers. Crucially, we find that binarizing the full set of SAE features offers a robust one-size-fits-all approach, whereas selecting a top-$N$ subset can yield slightly higher performance but requires additional computational steps. From an interpretability perspective, the binarization strategy also grants a straightforward notion of “feature activation”: whether or not a feature dimension was triggered above zero. This can facilitate feature-level analyses and potential explanations for model decisions.

\paragraph{Multilingual Performance and Transferability:}
Our results in \S\ref{sec:multilingual_results} affirm that SAE-based representations retain cross-lingual promise, although \emph{native} training remains superior to transferring a model trained on English alone. Interestingly, instruction tuning (\texttt{gemma-2-9b-it}) yields modest improvements in transferring to other languages, suggesting that the distributional shifts introduced during instruction tuning may make the learned representations more adaptable. Nevertheless, even an English-trained SAE can achieve respectable performance when applied to related languages, highlighting the potential for broad coverage in multilingual applications.

\paragraph{Behavioral Prediction and Scalable Oversight:}
In \S\ref{sec:behavioral_results}, we tested whether a smaller LM (2B) can effectively predict the correctness of a larger instruction-tuned model’s (9B-IT) outputs on question-answering tasks. Our findings indicate that SAE-based features from a smaller model can indeed forecast the larger model’s behavior at a level comparable to the larger model’s own internal features. This result has important implications for \emph{scalable oversight}: smaller, more resource-efficient models might provide viable early warnings or audits for more capable systems, especially in high-stakes domains where verifying correctness or safety is critical.

\paragraph{Limitations:}
Our study relies on \textbf{pretrained} SAEs from \emph{Gemma Scope} and does not explore how variations in SAE training regimes or architectures may affect performance. Furthermore, although we included a range of binary and multi-class tasks, some specialized domains (e.g., generation, retrieval, or low-resource settings) remain underexamined. Finally, while the summation+binarization approach proved generally robust, certain tasks showed marginal gains with top-$N$ feature selection, indicating that the optimal strategy may be domain- and dataset- dependent.

\paragraph{Future Work:}
Two especially promising directions for further research are: (i) Extending SAE-based analyses to \emph{fully multimodal} tasks, verifying whether the advantages observed in partial text-based interactions with \texttt{PaliGemma} models hold in end-to-end image-text scenarios; and (ii) Systematically studying the \emph{overlap} and \emph{feature auto-interp explanations} in feature sets across languages, to determine if a specific subset of SAE features consistently transfers among typologically diverse languages. Such investigations could lead to more unified, language-agnostic interpretability frameworks. Additionally, exploring whether small oversight models can reliably predict the behavior of larger instruction-tuned systems would clarify the scaling laws for model auditing.
