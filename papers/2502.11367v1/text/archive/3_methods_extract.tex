\subsection{Preliminaries}

\paragraph{Notation and Setup:}
Let $M$ be a pretrained language model (LM) of hidden dimensionality $d$. When $M$ processes an input sequence of tokens of length $n$, it produces hidden representations $\{\mathbf{h}_1, \mathbf{h}_2, \ldots, \mathbf{h}_n\}$ for each layer, where each $\mathbf{h}_t \in \mathbb{R}^d$. We consider three versions of \texttt{Gemma 2} models \cite{gemmateam2024gemma2improvingopen} in this work, the \textbf{2B},  \textbf{9B} and instruction-tuned variant, \textbf{9B-IT}.

\paragraph{SAE-Based Activation Extraction:}
We use \textbf{pretrained} SAEs provided by \texttt{Gemma Scope} \cite{lieberum2024gemma}. We extract each token’s residual stream activation coefficients from layers that have been instrumented with the \texttt{SAELens} \cite{bloom2024saetrainingcodebase} tool  (automatically loading the SAE with $L_0$ loss closest to 100). Specifically:
\begin{itemize}
    \item \textbf{2B model:} We extract SAE features from layers 5, 12, and 19 (early, middle, late).
    \item \textbf{9B \& 9B-IT models:} We extract SAE features from layers 9, 20, and 31 (early, middle, late).
\end{itemize}
Each SAE has a designated \emph{width} (i.e., number of feature directions). We evaluate \textbf{16K} and \textbf{65K} widths for the 2B model, and \textbf{16K} and \textbf{131K} for 9B and 9B-IT \footnote{we choose 131k for 9B and 65k for 2B models due to their same expansion ratio to original model hidden states}, following the pretrained models made available in \texttt{Gemma Scope} \cite{lieberum2024gemma}. \textbf{Note}: we do \textit{not} train any SAEs ourselves; our workflow involves only extracting the hidden states and the corresponding \emph{pretrained} SAE activations.

\paragraph{Pooling and Binarization:}
Because each input sequence is split into multiple tokens (each with its own SAE feature coefficients), we investigate different pooling methods \tom{any way to say pooling is like a key decision point to motivate this so that it doesn't appear randomly? Could be motivated more by saying SAEs require pooling and pooling's really important, or less by saying we consider 3 pooling strategies: 1), 2), 3). Or "recent works use X different pooling strategies. However, it remains unclear when each is appropriate. We therefore consider [pooling method 1], [pooling method 2], ...}. We either pool the \emph{top-n features} that is activated at \textbf{each token level} (token level top-n activations, Figure 1). Or, taking the simple \emph{summation} to aggregate all token-level features into a single vector(token level top-n where $n == 0$):
\[
    \mathbf{F} = \sum_{t=1}^n \mathbf{f}_t,
\]
where $\mathbf{f}_t \in \mathbb{R}^m$ is the SAE feature vector (of dimension $m$) for token $t$ (sum across tokens, Figure 1.  Or taking the top-n values tokenwise across the dimension. 

% Variants for reference
% top n mean difference selection
% token level top_n activations
% hidden states
% full sae features


We also explore the impact of \emph{binarizing} each entry above a threshold akin to a binary activation:
\[
    \mathbf{F}_{\text{bin}}[i] = 
    \begin{cases}
      1 & \text{if } \mathbf{F}[i] > 1, \\
      0 & \text{otherwise}.
    \end{cases}
\]
This yields a sparse binary vector per sequence (marking only the important/activated SAE feature across tokens level), which we found both interpretable and reproducible in downstream classifiers.

\paragraph{Classification with Logistic Regression:}
To measure how informative these SAE-derived features are for various tasks, we train a \emph{logistic regression} (LR) classifier. In all experiments, LR models are evaluated using \textbf{5-fold cross-validation}, ensuring that we obtain robust hyperparameter choices without overfitting. This is the only learned component of our pipeline; we do not fine-tune model $M$ or retrain SAEs.

\paragraph{Baselines:}
We compare against:
\begin{itemize}
    \item \textbf{TF-IDF}: A classic bag-of-words technique without neural representations \cite{sparck_jones_1972}.
    \item \textbf{Top-$N$ Mean-Difference Selection}: For binary tasks like multilingual toxicity detection, we consider a simpler SAE selection baseline where we pick the top-$n$ SAE features (e.g., $n=10,20,50$) by largest mean-activation difference between positive and negative classes, then apply logistic regression \cite{sae_probing}.
    \item \textbf{Hidden State}: Like prior studies \cite{features_as_classifiers}, we did compare to \emph{last-token} hidden state probing as well. 
\end{itemize}

\subsection{Classification Tasks and Multimodal Transfer \tom{it's atypical in NLP to introduce all the experiments and their results separately. It should usually be one unified section where experiments are introduced, conducted, and their results reported all together}}
\label{sec:part1}

\paragraph{Goal and Motivation:}
The first part of our study aims to identify best practices in combining \emph{GemmaScope} SAE features (scale, width, layer choice) for classification tasks. We also extend this analysis to test whether such text-trained SAE features transfer to a \emph{PaliGemma 2} vision-language model, examining cross-modal applicability.

\paragraph{Datasets:}
We evaluate a comprehensive set of classification benchmarks spanning binary safety tasks and multi-class scenarios. For binary classification, we utilize three safety-focused datasets: jailbreak detection, election misinformation, and harmful prompt detection. The multi-class evaluation leverages four diverse datasets targeting user intent classification, scenario understanding, abortion intent detection from tweets, and banking queries classifications. This collection enables systematic assessment across varying classification complexities and semantic domains, with detailed dataset characteristics provided in Appendix~\ref{app:model_info}.

\paragraph{Experiment Design:}
We systematically vary:
\begin{itemize}
    \item Model scale/type (\textbf{2B}, \textbf{9B}, \textbf{9B-IT}),
    \item SAE width (\textbf{16K}, \textbf{65K} for 2B; \textbf{16K}, \textbf{131K} for 9B/9B-IT),
    \item Layer depth (early, middle, late),
    \item Pooling strategy (top 0\footnote{no token level pooling}, top 20, top 50)
    \item Binarizing strategy on SAE features level after aggregation across tokens
\end{itemize}
extracting token-level SAE features and pooling them as described. We compare LR classifiers trained on the resulting binarized features to TF-IDF. 

For our cross-modal applicability experiments, we feed activations from a Gemma-based LLaVa model \cite{liu2023improvedllava} (PaliGemma) into a Gemma based SAE of equivalent size and perform classification on CIFAR 100 \cite{krizhevsky2009learning}, Indian food \cite{rajistics_indian_food_images} and Oxford flower \cite{Nilsback08} datasets. 

\subsection{Multilingual Classification and Transferability}
\label{sec:part2}

\paragraph{Goal and Motivation:}
Next, we evaluate cross-lingual robustness. We use \textbf{9B} and \textbf{9B-IT} to investigate if features from multilingual datasets are consistent with those found in monolingual contexts and whether there is a correlation with models’ cross-lingual prediction performance.
 
\paragraph{Dataset:}
We adopt the multilingual toxicity detection dataset \cite{dementieva2024overview}, which includes five languages: English (EN), Chinese (ZH), French (FR), Spanish (ES), and Russian (RU). Each instance is labeled for toxicity (\emph{binary}).

\paragraph{Experiment Design:}
Following the best configurations discovered in Part 1, we extract SAE features from 9B and 9B-IT (with widths \textbf{16K} or \textbf{131K}). We train the same linear classifiers on one language’s data and test on another (cross-lingual) or the same language (within-lingual). We compare performance against a previous\&simpler SAE feature selection approach (the \emph{top-n mean-difference} baseline) to see if the entire feature set is necessary.

\subsection{Behavioral Analysis and Output Prediction}
\label{sec:part3}

\paragraph{Goal and Motivation:}
We investigate whether smaller models (e.g., 2B) can predict their own behavior or that of larger models (9B, 9B-IT) on knowledge QA tasks when provided correct or incorrect factual information. This speaks to a \emph{scalable oversight} scenario, where a smaller model could monitor the behavior of a more capable system when they shared same pertaining corpus and architecture.

\paragraph{Datasets:}
We repurposed the entity-based knowledge conflicts in question answering dataset each containing binary labels indicating correctness of a model’s response \cite{longpre2022entitybasedknowledgeconflictsquestion}. Open-ended generation is performed using a \emph{vllm} package \cite{kwon2023efficient}, then scored using the \emph{inspect ai} package \cite{UKGovernmentBEIS_inspect_ai} using GPT-4o-mini as a grader to produce ground-truth correctness labels.

\paragraph{Experiment Design:}
For a given model $M$ (2B, 9B, 9B-IT):
\begin{enumerate}
    \item Generate open-ended answers to prompts.
    \item Use \texttt{GPT-4o-mini-0718} to label each answer as correct or incorrect.
    \item Extract pretrained SAE activations from the input question with/without contexts.
    \item Train a logistic regression model to predict correctness (binary) from those features.
\end{enumerate}
We also do cross-model prediction: e.g., \emph{Gemma 2B} tries to predict the correctness of \emph{9B}’s answers similar to \cite{binder2024lookinginwardlanguagemodels}. We fix an SAE width of 16K for simplicity.

\paragraph{Comparisons:}
We evaluate the quality of these “self-knowledge” predictions and inter-model predictions, contrasting them with the top-$n$ mean difference feature baseline. We also analyze the auto interpret descriptions of features behind these predictions to see if the SAE is picking up useful features for this task. 

\paragraph{Code and Reproducibility:}
All code for data loading, activation extraction, pooling, and classification is provided in a public repository. A simple YAML configuration file controls model scale, layer indices, SAE width, and \texttt{huggingface} dataset paths, enabling reproducible workflows.
