We now present our empirical findings in three parts, mirroring the structure of our experiments: \emph{(1) Classification Tasks and Hyperparameter Analysis}, \emph{(2) Multilingual Transfer}, and \emph{(3) Behavioral (Action) Prediction}. Throughout, we compare SAE-based features against baselines including hidden-state probes and classical bag-of-words approaches (TF-IDF). We also reference illustrative figures to highlight key trends.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/final_violin_plots.png}
    \caption{\textbf{(a)} Layer-wise classification performance for each model scale. The dotted black line indicates a TF-IDF baseline, while the red dashed line indicates a last token hidden-state probe baseline. SAE-based methods (colored violin plots) often surpass these baselines, with middle-layer SAE features typically achieving the highest scores. \textbf{(b)} Token level top-$N$ vs.\ full binarized features. Binarization proves robust across tasks, though token level top-$N$ can excel in some multi-class settings.}
    \label{fig:pooling} 
\end{figure}
\tom{Figure 2: Text too small, especially legends. I suggest using subfigures with captions instead of plot titles, which are easily missed (and tiny)}

\subsection{Classification Tasks and Hyperparameter Analysis}
\label{sec:classification_results}

\paragraph{Overall Performance by Layer and Model Scale.}
Figure~\ref{fig:pooling}(a) depicts layer-wise performance for the three model scales (\texttt{gemma-2-2b}, \texttt{gemma-2-9b}, \texttt{gemma-2-9b-it}) across our text-based classification tasks. 
We observe in Figure~\ref{fig:pooling}(a) that:
\begin{itemize}
    \item \textbf{Layer Influence:} Middle-layer activations often yield slightly higher F1 scores than early- or late-layer features, although the difference is slight.
    \item \textbf{Model Scale:} Larger models (9B, 9B-IT) achieve consistently higher mean performance (above 0.85 F1) compared to the 2B model.
\end{itemize}

\paragraph{Token level top-$N$ vs.\ Token summ:}
Figure~\ref{fig:pooling}(b) compares the impact of two feature selection strategies: (1) token level summation+binarization of \emph{all} SAE features, and (2) selecting the top-$N$ token level activations, here 20 and 50.

Our results suggest:
\begin{itemize}
    \item \textbf{SAE Features vs.\ Baselines:} Summation-pooled, binarized SAE features generally outperform both hidden-state probes and bag-of-words (dotted lines in Figure~\ref{fig:pooling}(b)). 
    \item \textbf{Token level top-$N$ Selection:} Can outperform the summed binarized approach in certain settings, especially when $N$ increases, and not binarized. However, the margin is typically small, and top-$N$ selection demands additional computation to identify discriminative features.
\end{itemize}
These observations validate our decision to adopt binarized summation as a default due to the reduced computational overhead whilst maintaining performance, while acknowledging that token-level top-$N$ might excel for certain tasks.

\paragraph{Multimodal Extension:}
We also tested whether these text-derived SAE features could transfer to a \emph{PaliGemma2} setup \cite{steiner2024paligemma2familyversatile}. Results, shown in Appendix \ref{app:multimodal}) indicate that summation+binarization from \texttt{gemma-2-2b} remains effective for partial textual inputs within a vision-language environment, though a dedicated study is warranted to confirm broader multimodal benefits.
