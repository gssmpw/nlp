\subsection{Behavioral (Action) Prediction}
\label{sec:behavioral_results}

\paragraph{Predicting 9B-IT’s Behavior.}
Finally, we consider whether smaller models (2B, 9B) can predict the output correctness or “action” of a larger, instruction-tuned model (9B-IT) in knowledge-intensive QA tasks. Figure~\ref{fig:action} summarizes the macro F1 scores across several \emph{NQ-Swap} and \emph{inspect\_evals} conditions (Original context, Question Swap, Subject Swap, etc.).

Key findings include:
\begin{itemize}
    \item \textbf{Context Fidelity}: The “Original” setting (i.e., correct context provided) leads to the highest F1 scores (above 80\%), whereas removing or swapping context causes a 20\% drop.
    \item \textbf{Inter-Model Prediction}: Surprisingly, 2B-based SAE features can predict 9B-IT’s correctness nearly as well as (and sometimes better than) 9B-IT’s own features. 
    \item \textbf{Comparison to Hidden States}: Hidden-state baselines (black lines) generally trail the performance of binarized SAE feature sets, reinforcing the utility of an SAE-based approach for “behavior prediction” tasks.
\end{itemize}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/action_prediction.png}
    \caption{Action prediction performance for 9B-IT across different context manipulations (Original, Question Swap, Subject Swap). Each bar represents a different LLM acting as the “classifier” (2B, 9B, 9B-IT) using SAE features; the black horizontal lines indicate the hidden-states baseline. We observe high predictive power when the correct context is available, with a significant performance drop under context manipulations. Notably, 2B-based features can be competitive in predicting 9B-IT’s behavior.} 
    \label{fig:action}
\end{figure}
\tom{Figure 6: all labels and ticks are way too small. Avoid rotated ticklabels. Add an x axis so the bars don't float.}
 
\paragraph{Implications for Scalable Oversight:}
These behavioral findings underscore the promise of using smaller SAEs to interpret or predict the actions of more powerful LMs. While context consistency remains crucial, the capability to forecast a larger model’s decisions has important implications for AI safety and governance, particularly in risk-sensitive domains.

\bigskip
\noindent
In summary, our results demonstrate that:
\begin{enumerate}
    \item SAE-based features consistently outperform hidden-state and TF-IDF baselines across classification tasks, especially when using summation + binarization.
    \item For multilingual toxicity detection, native training outperforms cross-lingual transfer, though instruction-tuned models (e.g., 9B-IT) may exhibit modestly better transfer.
    \item Smaller LMs can leverage SAE features to accurately predict the behavior of larger instruction-tuned models, suggesting a scalable mechanism for oversight and auditing.
\end{enumerate}

