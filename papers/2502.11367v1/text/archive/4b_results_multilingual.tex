
\subsection{Multilingual Classification and Transfer}
\label{sec:multilingual_results}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/multilingual.png}
    \caption{Multilingual toxicity-detection results (middle-layer features). \textbf{Native SAE Training} (pink) generally yields the best F1 scores, while transferring from English (green) or using translated inputs (blue) shows moderate declines. 9B-IT models exhibit a similar pattern but with slightly better cross-lingual generalization in certain pairs.}
    \label{fig:multilingual}
\end{figure}
\tom{Figure 3: x and y ticks fonts too small. I suggest removing the numbers from inside the bar plot---the important part is the comparison, not the exact values so they just add complexity. If you're dying to keep them, at least use 2 decimal places and remove the preceding 0s: "0.994" -> ".99". Center the EN bar over "EN." I suggest also adding a vertical line separating EN from the others and adding a "Train" label somehow to make it clearly separate.}

\paragraph{Native vs.\ Transfer Performance:}
Figure~\ref{fig:multilingual} shows the F1 scores for toxicity detection in English (EN), Spanish (ES), Russian (RU), German (DE), and Chinese (ZH). Bars in pink correspond to \emph{native SAE training} (i.e., training and testing on the same language), while green and blue indicate cross-lingual or translated transfer settings.
Our main observations are:
\begin{itemize}
    \item \textbf{Native Training} yields the highest scores (e.g., EN $\to$ EN can reach 0.99+ F1). 
    \item \textbf{English SAE Transfer} (green bars) still performs decently on ES, RU, and DE, though with a drop of 15--20\% relative to native.
    \item \textbf{Translation vs.\ Direct Transfer:} Translating foreign language inputs into English for classification does not outperform direct training on that languageâ€™s data, confirming the transferability of native language signals.
\end{itemize}

\paragraph{Full SAE vs.\ Hidden States vs.\ Mean-diff Top-$N$ (Mean-Diff):}
Figure~\ref{fig:layerwise} breaks down average F1 scores across layers for the multilingual task for \texttt{gemma-2-9b} and \texttt{gemma-2-9b-it}. We show performance across using: all SAE features, hidden states alone, and the top-$N$ mean difference approaches.
We see that:
\begin{itemize}
    \item \textbf{Full SAE Features(SAE)} our method that aggregates across tokens and binarizing  the full SAE features outperform final-hidden-state probes in most early and middle layers. 
    \item \textbf{Mean-Diff} top-$N$ SAE feature selections exhibit greater variation compared to using the full feature set and generally lag behind the full binarized approach.
\end{itemize}

\tom{Figure 4: tick labels are too small. I suggest avoiding using the rotated labels because they add needless whitespace. You can abbreviate things like "Features" to "Feat." or give them each 2 rows}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/layerwise.png}
    \caption{Comparison of average F1 scores by different feature selection methods on the Multilingual Classification and Transfer task. The boxes represent the mean $\pm$ standard deviation, and the whiskers indicate the interquartile range (IQR).}
    \label{fig:layerwise}
\end{figure}



\paragraph{Impact of Training Data Sampling:} Figure~\ref{fig:sampling-main} demonstrates the robust performance characteristics of our SAE-based feature extraction across varying training set sizes. We evaluate two training regimes: native language training and English transfer, comparing three feature types: full SAE features with binarization, hidden states, and mean diff top-$N$ feature selection. Native language training consistently outperforms English transfer across all sampling rates (0.1-1.0), with our full binarized SAE features achieving superior F1 scores (0.85-0.90) compared to both hidden states (0.80-0.85) and top-n selection (0.75-0.80). Notably, the performance gap between native and transfer settings remains stable even with limited training data, suggesting our feature extraction framework maintains robustness under data-constrained scenarios. 


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/lineplot_averaged_models_with_error.png}
    \caption{Average performance across sampling rates for all SAE features, mean difference selection, and hidden states, for native languages, and English trained transfer.}
    \label{fig:sampling-main}
\end{figure}

\tom{Figure 5: legend text way too small. Lots of unused space, could zoom the y axis in? This figure also doesn't show that much information... really looks like the point is that sampling rate doesn't change much? Might be possible to do as a barplot or something to better highlight the difference between Native and English}