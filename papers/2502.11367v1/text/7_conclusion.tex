We present a comprehensive study of SAE features across multiple model scales, tasks, languages, and modalities, highlighting both their practical strengths and interpretive advantages. Specifically, summation-then-binarization of SAE features surpassed hidden-state probes and bag-of-words baselines in most tasks, while demonstrating cross-lingual transferability. Moreover, we showed that smaller LLMs equipped with SAE features can effectively predict the actions of larger models, pointing to a potential mechanism for \emph{scalable oversight} and auditing. Taken together, these results reinforce the idea that learning (or adopting) a sparse, disentangled representation of internal activations can yield significant performance benefits and support interpretability objectives.

We hope this work will serve as a foundation for future studies that exploit SAEs in broader multimodal, diverse languages, and complex real-world workflows where trust and accountability are paramount. By marrying strong classification performance with clearer feature-level insights, SAE-based methods represent a promising path toward safer and more transparent LLM applications.

\section{Limitations}

While our study demonstrates the effectiveness of SAE features for classification and transferability, several limitations remain.

\paragraph{Dependence on Gemma 2 Pretrained-SAEs}  
Our analysis is restricted to SAEs trained with Jump ReLU activation on Gemma 2 models. This limits generalizability to other model architectures and training paradigms. Future work should explore diverse SAE training strategies and model sources.

\paragraph{Limited Multimodal and Cross-Lingual Evaluation}  
Our cross-modal experiments are preliminary, and further research is needed to validate SAE generalization across different modalities and low-resource languages.

\paragraph{Sensitivity to Task and Data Distribution}  
SAE performance varies across datasets, and its robustness under adversarial conditions or domain shifts needs further study.

\paragraph{Interpretability Challenges}  
Despite improved feature transparency, the semantic alignment of SAE features with human-interpretable concepts remains an open question.

% These limitations pave the way for exciting advancements to improve the robustness, efficiency, and interpretability of SAE-based methods. Looking forward to what the future holds!

\paragraph{Potential Risks}
The toxicity or other safety-related open-sourced data we use contained offensive language, which we have not shown in the manuscript. And the auto-interp features are fully AI generated by neuronpedia.org.

\section*{Acknowledgments}
The authors acknowledge financial support from the Google PhD Fellowship (SC), the Woods Foundation (DB, SC, JG), the NIH (NIH R01CA294033 (SC, JG, DB), NIH U54CA274516-01A1 (SC, DB) and the American Cancer Society and American Society for Radiation Oncology, ASTRO-CSDG-24-1244514-01-CTPS Grant DOI: ACS.ASTRO-CSDG-24-1244514-01-CTPS.pc.gr.222210 (DB).

The authors extend their gratitude to John Osborne from UAB for his support and to Zidi Xiong from Harvard for proofreading the preprint. Author SC also appreciates the advice on this project from Fred Zhang and Asma Ghandeharioun from Google through the mentorships program.

