This section evaluates the cross-lingual robustness of SAE features. We investigate whether features extracted from multilingual datasets are consistent with those found in monolingual contexts and explore the correlation between SAE feature transferability and cross-lingual prediction performance.  We conduct three primary experiments: (1) comparing native and cross-lingual transfer, (2) evaluating different feature selection methods, and (3) assessing the impact of training data sampling.

\paragraph{Dataset:}
We use the multilingual toxicity detection dataset \cite{dementieva2024overview}, which contains text in five languages labeled with a binary toxicity label: English (EN), Chinese (ZH), French (FR), Spanish (ES), and Russian (RU).

\subsection{Native vs. Cross-Lingual Transfer}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/multilingual_enhanced.pdf}
    \vspace{-0.2cm} % Reduce space before caption
    \caption{Multilingual toxicity detection results (middle-layer features): \textbf{Native SAE Training} (pink) consistently achieves the best F1 scores. Transferring from English (gold) or using translated inputs (green) leads to moderate performance declines. 9B-IT models show a similar trend, with slightly improved cross-lingual generalization in some language pairs.}
    \label{fig:multilingual}
\end{figure}

We first investigate the performance of SAE features when training and testing on the same language (native) versus training on one language and testing on another (cross-lingual).

\paragraph{Experimental Setup:}
Following the best configurations from previous Section, we extract SAE features from \texttt{gemma-2-9b} and \texttt{9b-it} (widths of 16K or 131K). We train linear classifiers on one language's data and test on the same or a different language. We also compare against a simpler SAE feature selection approach, the \emph{top-n mean-difference} baseline (Mean-Diff), to determine if the entire feature set is necessary.

\paragraph{Results and Discussion:}
Figure~\ref{fig:multilingual} presents the F1 scores. Pink bars show \emph{native SAE training}, gold bars show English-trained models tested on other languages, and green bars show English-translated models tested on translated inputs Key findings:

\begin{itemize}[itemsep=-1.7pt,topsep=1.5pt]
    \item \textbf{Native Training Superiority:} Native training consistently yields the highest F1 scores (e.g., EN $\to$ EN can reach over 0.99 F1).
    \item \textbf{English Transfer Effectiveness:} Transferring SAE features trained on English (gold bars) achieves reasonable performance on ES, RU, and DE, but with a 15-20\% F1 score decrease compared to native training. This indicates some cross-lingual features generalization internally inside of the models.
    \item \textbf{Direct Transfer Outperforms Translation:} Translating foreign language inputs into English before classification \textbf{does not} outperform direct training on the original language data. Native language signals can be effectively transferred into a shared SAE feature space, proving valuable even without explicit translation.
\end{itemize}

These results suggest that SAE-based representations have cross-lingual potential, but \emph{native} training remains superior. Instruction tuning (\texttt{9B-IT}) yields modest gains, implying distributional shifts from instruction tuning may improve adaptability. Notably, an English-trained SAE performs well in related languages, even better than translations.

\subsection{Feature Selection Methods: Full SAE vs. Hidden States vs. Mean-Diff}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/boxplot_final.pdf}
    \caption{Comparison of average F1 scores by different feature selection methods on the Multilingual Classification and Transfer task. The boxes represent the mean $\pm$ standard deviation, and the whiskers indicate the interquartile range (IQR).}
    \label{fig:boxplot}
\end{figure}

% Next, we compare different feature selection methods within the multilingual toxicity detection task.

\paragraph{Experimental Setup:}
We compare feature selection methods on \texttt{gemma-2-9b} and \texttt{9b-it}, analyzing performance across different layers using: all SAE features (with binarization), last token hidden-state probing (baseline), and the top-$N$ mean-difference (Mean-Diff) approach.

\paragraph{Results and Discussion:}
Figure~\ref{fig:boxplot} shows the average F1 scores across layers.\footnote{Large variance of the box plot here are caused by transfer across 5 languages and 3 layer settings within 2 models.} \textbf{SAE features achieve the highest macro F1 scores} but exhibit \textbf{greater variance}, particularly due to DE → ZH transfer. Despite this, they remain the \textbf{most preferable choice} due to their superior peak performance. \textbf{Hidden-state probing} performs competitively with \textbf{lower variance} but does not reach the highest scores, making it a more stable alternative. Meanwhile, \textbf{Mean-Diff top-$N$ selection} (Top-10, Top-20, Top-50) consistently lags behind SAE features and hidden states, offering \textbf{similar variance but lower effectiveness}, reinforcing the benefit of using the full SAE feature set.\footnote{These different methods also utilize different important features to do classification which results in performance differences as shown in Appendix \ref{app:cross_lingual_features}.}


We then examine the robustness of SAE feature extraction with varying amounts of training data.

\paragraph{Experimental Setup:}
We assess performance across training set sampling rates (0.1–1.0), comparing native language training and English transfer. For each, we evaluate SAE binarized features, hidden states, and Mean-Diff top-$N$ selection.

\paragraph{Results and Discussion:}
Figure~\ref{fig:sampling-main} shows the performance across sampling rates. Key findings:

\begin{itemize}\itemsep=-1.7pt
    \item \textbf{Native Outperforms Transfer:} Native language training consistently outperforms English transfer across \textbf{all sampling rates}.
    \item \textbf{SAE Features are Superior:} Our full binarized SAE features achieve superior F1 scores (0.85-0.90) compared to both hidden states (0.80-0.85) and top-$N$ selection (0.75-0.80).
    \item \textbf{Stable Performance Gap:} The performance difference between native and transfer settings remains relatively stable even with limited data. This shows that our feature extraction method is robust even when data is scarce.
\end{itemize}

