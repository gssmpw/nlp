\section{Appendix}

% \subsection{Mechanistic Diagram of pooling and binarising variations}
% \label{app:diagram}

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=\linewidth]{figures/diagram.png}
%     \caption{Diagram explaining our approaches to token-level pooling and aggregatation of SAE features.}
%     \label{fig:gemma} 
% \end{figure}

% This diagram shows a visual mechanism for understanding the pooling strategies compared in this study.


\subsection{Models and Dataset Information}
\label{app:model_info}

Table~\ref{tab:model_config} describes the configurations of the Gemma 2 models under study, including which layers are analyzed, the width of our SAE, and whether the model is base or instruction-tuned. 
These particular layers were selected based on availability of SAE widths across model sizes, and to reflect progression throughout the model.

\begin{table*}[ht]
    \centering
    \caption{Model Configurations and SAE Specifications. We analyze select intermediate layers (see \emph{Layers Analyzed}) to extract representations for the Stacked Autoencoder, whose width is indicated.}
    \label{tab:model_config}
    \begin{tabular}{llll}
        \toprule
        \textbf{Model} & \textbf{Layers Analyzed} & \textbf{SAE Width} & \textbf{Model Type} \\
        \midrule
        Gemma 2 2B & 5, 12, 19 & $2^{14}$, $2^{16}$ & Base \\
        Gemma 2 9B & 9, 20, 31 & $2^{14}$, $2^{17}$ & Base \\
        Gemma 2 9B-IT & 9, 20, 31 & $2^{14}$, $2^{17}$ & Instruction-tuned \\
        \bottomrule
    \end{tabular}
\end{table*}

Table~\ref{tab:dataset_specs} outlines each dataset used, specifying the type of task, a brief description, and the corresponding number of classes. 
These datasets focus on safety based tasks such as toxicity detection, and the multimodal datasets use the vision task such as CIFAR-100. 
Our goal was to test each model’s robustness across both domain (language vs.\ vision) and complexity (binary vs.\ multi-class classification), thereby evaluating classifiers applicability.

\begin{table}[ht]
    \centering
    \caption{Dataset Specifications, Task Descriptions, and Class Information. Each dataset is evaluated based on its primary task and class distribution. V) noted for vision tasks otherwise are pure text classification tasks}
    \label{tab:dataset_specs}
    \begin{tabular}{lp{5cm}c}
        \toprule
        \textbf{Dataset} & \textbf{Description} & \textbf{Classes} \\
        \midrule
        Multilingual Toxicity \cite{dementieva2024overview} 
            & Cross-lingual toxicity detection & 2 \\
        Election Questions \cite{anthropic_election_questions} 
            & Classify election-related claims  & 2 \\
        Reject Prompts \cite{arditi2024refusal} 
            & Detect unsafe instructions & 2 \\
        Jailbreak Classification \cite{jackhhao_jailbreak_classification} 
            & Detect model jailbreak attempts & 2 \\
        MASSIVE Intent \cite{fitzgerald2023massive} 
            & Massive intent classification & 60 \\
        MASSIVE Scenario \cite{fitzgerald2023massive} 
            & Massive scenario classification & 18 \\
        Banking77 \cite{Casanueva2020} 
            & Banking-related queries intent classification & 77 \\
        TweetEval Stance Abortion \cite{setfit_tweet_eval_stance_abortion} 
            & Stances on abortion: favor, against, neutral  & 3 \\
        NQ-Swap-original \cite{longpre2022entitybasedknowledgeconflictsquestion} 
            & Robustness testing with correct or incorrect factual information swapped QA & 2 \\
        V) CIFAR-100 \cite{krizhevsky2009learning} 
            & General image classification & 100 \\
        V) Oxford Flowers \cite{nelorth_oxford_flowers} 
            & Classification of 102 flower types & 102 \\
        V) Indian Food Images \cite{rajistics_indian_food_images} 
            & Classification of Indian dishes & 20 \\
        \bottomrule
    \end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\FloatBarrier
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\FloatBarrier
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsection{Performance variation on the width}
\label{app:width}
\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{figures/sae_width.pdf}
    \caption{Performance evaluation of SAE feature transfer across different model widths for Gemma-2 models. Results are presented under different binarization and pooling settings, demonstrating a decline in mean score as width increases. The observed trends indicate that larger widths may reduce feature discriminability, particularly in non-binarized settings.}
    \label{fig:width-performance}
\end{figure}

We conduct an analysis of the effect of width scaling on full SAE features among our safety text classification tasks. The evaluation compares models with and without max pooling, as well as binarized and non-binarized activations, to determine their impact on classification performance. Consistently increasing the width results in decline in the mean score across all configurations, with the steepest drop observed in non-binarized cases, which is surprisingly different from \citet{sae_probing} demonstrate the opposite using mean-diff feature SAE selection.


\newpage
\subsection{Multimodal performance}
\label{app:multimodal}

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{figures/multimodal.pdf}
    \caption{Performance of SAE features from gemmascope being utilised on activations derived from Peligemma 2 models. Token-n = 0 and binarization yielded overall best performance. These results also demonstrate the promise on direct SAE transfer in multimodal settings.}
    \label{fig:enter-label}
\end{figure}

We also implemented an unsupervised approach and analyzed the retrieved features to evaluate whether meaningful features could be identified through this transfer method among other models and pretrained SAEs. Initially, features were cleaned to remove those overrepresented across instances, which could add noise or reduce interpretability. 

Considering the CIFAR-100 dataset again, which comprises 100 labels with 100 instances per label, the expected maximum occurrence of any feature under uniform distribution is approximately 100. To address potential anomalies, a higher threshold of 1000 occurrences was selected as the cutoff for identifying and excluding overrepresented features. This conservative threshold ensured that dominant, potentially less informative features were removed while retaining those likely to contribute meaningfully to the analysis.

In this study, we also tried the Intel Gemma-2B LLaVA 1.5-based model (Intel/llava-gemma-2b) \cite{hinck2024llavagemma} as the foundation for our experiments. For feature extraction, we incorporate pre-trained SAEs from jbloom/Gemma-2b-Residual-Stream-SAEs, trained on the Gemma-1-2B model. These SAEs include 16,384 features (an expansion factor of 8 × 2048) and are designed to capture sparse and interpretable representations.

After cleaning, we examined the retrieved features across different model layers (0–12 of 19 layers). We found that deeper layers exhibited increasingly useful/relevant features.

Below, we provide some examples of retrieved features from both high-performing and underperforming classes, demonstrating the range of interpretability outcomes.

\subsection{Top retrieved features}
\label{app:multimodal_retrieve}

\begin{longtable}{|c|c|p{7cm}|}
\hline
\textbf{Category} & \textbf{Layer} & \textbf{Top 2 Features (Occurrences)} \\
\hline
\endfirsthead

\multicolumn{3}{c}%
{{\bfseries -- Continued from previous page --}} \\
\hline
\textbf{Category} & \textbf{Layer} & \textbf{Top 2 Features (Occurrences)} \\
\hline
\endhead

\hline \multicolumn{3}{|r|}{{Continued on next page}} \\ \hline
\endfoot

\hline
\endlastfoot

Dolphin & Layer 0 & Technical information related to cooking recipes and server deployment (30/100) \\
 &  & References to international topics or content (26/100) \\
\hline
Dolphin & Layer 6 & Phrases related to a specific book title: \textit{The Blue Zones} (25/100) \\
 &  & Mentions of water-related activities and resources in a community context (17/100) \\
\hline
Dolphin & Layer 10 & Terms related to underwater animals and marine research (88/100) \\
 &  & Actions involving immersion, dipping, or submerging in water (61/100) \\
\hline
Dolphin & Layer 12 & Terms related to oceanic fauna and their habitats (77/100) \\
 &  & References to the ocean (53/100) \\
\hline
Dolphin & Layer 12-it & Mentions of the ocean (60/100) \\
 &  & Terms related to maritime activities, such as ships, sea, and naval battles (40/100) \\
\hline
Skyscraper & Layer 0 & Information related to real estate listings and office spaces (11/100) \\
 &  & References to sports teams and community organizations (7/100) \\
\hline
Skyscraper & Layer 6 & Details related to magnification and inspection, especially for physical objects and images (32/100) \\
 &  & Especially for physical objects and images (28/100) \\
\hline
Skyscraper & Layer 10 & References to physical structures or buildings (68/100) \\
 &  & Character names and references to narrative elements in storytelling (62/100) \\
\hline
Skyscraper & Layer 12 & References to buildings and structures (87/100) \\
 &  & Locations and facilities related to sports and recreation (61/100) \\
\hline
Skyscraper & Layer 12-it & Terms related to architecture and specific buildings (78/100) \\
 &  & References to the sun (57/100) \\
\hline
Boy & Layer 0 & References to sports teams and community organizations (17/100) \\
 &  & Words related to communication and sharing of information (10/100) \\
\hline
Boy  & Layer 6 & Phrases related to interior design elements, specifically focusing on color and furnishings (52/100) \\
 &  & Hair styling instructions and descriptions (25/100) \\
\hline
Boy  & Layer 10 & Descriptions of attire related to cultural or traditional clothing (87/100) \\
 &  & References to familial relationships, particularly focusing on children and parenting (83/100) \\
\hline
Boy  & Layer 12 & Words associated with clothing and apparel products (89/100) \\
 &  & Phrases related to parental guidance and involvement (60/100) \\
\hline
Boy  & Layer 12-it & Patterns related to monitoring and parental care (88/100) \\
 &  & Descriptions related to political issues and personal beliefs (67/100) \\
\hline
Cloud & Layer 0 & Possessive pronouns referring to one's own or someone else's belongings or relationships (4/100) \\
 &  & References to sports teams and community organizations (3/100) \\
\hline
Cloud & Layer 6 & Descriptive words related to weather conditions (24/100) \\
 &  & Mentions of astronomical events and celestial bodies (21/100) \\
\hline
Cloud & Layer 10 & Terms related to aerial activities and operations (62/100) \\
 &  & References and descriptions of skin aging or skin conditions (59/100) \\
\hline
Cloud & Layer 12 & Themes related to divine creation and celestial glory (92/100) \\
 &  & Terms related to cloud computing and infrastructure (89/100) \\
\hline
Cloud & Layer 12-it & The word "cloud" in various contexts (80/100) \\
 &  & References to the sun (47/100) \\
\hline

\end{longtable}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\FloatBarrier
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\subsection{Performance Tables}
\label{app:performance_table}

Below we present the full results for evaluating our multilingual toxicity classification experiments, focusing on different feature extraction methods, top-$n$ feature selection, and the overall experimental design. 

%------------------------------------------------------------
% 1) FIRST TABLE (SAE Features vs Hidden States)
%------------------------------------------------------------
\begin{table}[ht]
\centering
\caption{Multilingual Toxicity Classification Performance Comparison}
\label{tab:results}
\begin{tabular}{l l c c c c c c}
\toprule
Model & Transfer & \multicolumn{3}{c}{SAE Features} & \multicolumn{3}{c}{Hidden States} \\
% \cmidrule(lr){3-5} \cmidrule(lr){6-8}
 & & Layer 9 & 20 & 31 & Layer 9 & 20 & 31 \\
\midrule
Gemma2 - 9B & Original   & 0.759 & \textbf{0.794} & 0.766 & 0.772 & 0.792 & 0.765 \\
 & Translated              & 0.763 & \textbf{0.798} & 0.771 & 0.771 & 0.794 & 0.766 \\
Gemma2 - 9B IT & Original & 0.754 & \textbf{0.784} & 0.751 & 0.755 & 0.770 & 0.755 \\
 & Translated              & 0.761 & \textbf{0.778} & 0.753 & 0.761 & 0.776 & 0.747 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{SAE Features vs.\ Hidden States.}
Table~\ref{tab:results} reports macro F1 scores for two Gemma2 9B model variants (base and instruction-tuned), comparing:
\begin{enumerate}
    \item \textit{SAE Features:} Representations learned by a Sparse Autoencoder at specific layers.
    \item \textit{Hidden States:} Direct residual stream outputs/hidden states from the same transformer layers.
\end{enumerate}
We evaluate both \emph{Original} (multilingual) and \emph{Translated} (All translated to English) test sets. 
Across most settings, SAE-based features at layer 20 or 31 produce competitive (often superior) results, suggesting that deeper layers encode richer semantic information for toxicity detection. 
The instruction-tuned model (Gemma2 - 9B IT) also benefits from SAE features, although its absolute scores are slightly lower than the base model’s best results, surprisingly, on both using full SAE features and hidden states.

%------------------------------------------------------------
% 2) SECOND TABLE (top-$N$ Feature Selection)
%------------------------------------------------------------
\begin{table}[ht]
\centering
\caption{Comparison of F1 scores across different layers and top-$N$ token selections. 
\textbf{top-$N$} indicates evaluation on the top 10, 20, or 50 \textbf{mean top-diff SAE features}.
\textbf{Original} refers to the original input language, while \textbf{Translated} corresponds to translated input to English. 
Bold values highlight the highest scores for each row.}
\label{tab:layer_eval}
\vspace{1em}
\begin{tabular}{l l ccc ccc ccc}
\toprule
 & Transfer & \multicolumn{3}{c}{Top 10} & \multicolumn{3}{c}{Top 20} & \multicolumn{3}{c}{Top 50} \\
\cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11}
\textbf{Model} & \textbf{Setting} & L9 & L20 & L31 & L9 & L20 & L31 & L9 & L20 & L31 \\
\midrule
\multirow{2}{*}{Gemma2 - 9B}    & Original     & 0.724 & 0.764 & \textbf{0.785}  & 0.724 & 0.764 & \textbf{0.785}  & 0.724 & 0.764 & \textbf{0.785} \\
                                & Translated   & 0.723 & 0.763 & \textbf{0.783}  & 0.723 & 0.763 & \textbf{0.783}  & 0.723 & 0.763 & \textbf{0.783} \\
\midrule
\multirow{2}{*}{Gemma2 - 9B IT} & Original     & 0.722 & 0.735 & \textbf{0.767}  & 0.722 & 0.735 & \textbf{0.767}  & 0.722 & 0.735 & \textbf{0.767} \\
                                & Translated   & 0.721 & 0.734 & \textbf{0.764}  & 0.721 & 0.734 & \textbf{0.764}  & 0.721 & 0.734 & \textbf{0.764} \\
\bottomrule
\end{tabular}
\end{table}

In the table above, we investigate selecting only the top 10, 20, or 50 most salient SAE features. Interestingly, reduced features can maintain or sometimes even slightly improve macro F1 performance. 


% %------------------------------------------------------------
% % 3) THIRD TABLE (Experimental Design Matrix)
% %------------------------------------------------------------
% \begin{table}[ht]
%     \centering
%     \caption{Experimental Design Matrix}
%     \label{tab:exp_design}
%     \begin{tabular}{lll}
%         \toprule
%         \textbf{Study Focus} & \textbf{Variables} & \textbf{Configurations} \\
%         \midrule
%         Model Scale & Model Size & 2B, 9B \\
%         vs. Accuracy & Layer Depth & Early, Middle, Late \\
%         \midrule
%         SAE Width & Model Size & 2B, 9B \\
%         Impact    & Width Range & $2^{14}$, $2^{16}$ \\
%         \midrule
%         Base vs.  & Model & Gemma 2 9B \\
%         Instruction & Configuration & Base vs. IT \\
%         Tuning    & Width Range & $2^{14}$, $2^{17}$ \\
%         \bottomrule
%     \end{tabular}
% \end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\FloatBarrier
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Cross Lingual Transfer of Feature Activations}
\label{app:cross_lingual_features}
A more detailed set of visualizations are provided below showing how feature extraction methods perform when transferring across different languages. We first show a high-level summary of cross-lingual transfer via a heatmap (Figure~\ref{fig:multilingual_heatmap}), then we provide a series of line plots (Figures~\ref{fig:lineplot_9b_it_original}--\ref{fig:lineplot_9b_translated}) illustrating performance versus sampling rate for five target languages. These plots compare \emph{Native SAE Training} with \emph{English SAE Transfer} under three feature extraction strategies: \emph{full SAE features}, \emph{hidden states}, and \textbf{mean difference} \emph{top-$n$ SAE features}. 


\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/multilingual_heatmap.png}
    \caption{Average F1 scores for each training language (y-axis) versus test language (x-axis). 
    We compare hidden states, SAE features, and top-$n$ feature selection. 
    The top row shows models trained on native-language datasets; 
    the bottom row uses English-translated data for training. 
    Darker cells indicate higher F1 performance.}
    \label{fig:multilingual_heatmap}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/lineplot_model_9b_it_original_swap.pdf}
    \caption{Performance vs.\ sampling rate for the 9B \emph{instruction-tuned} model on \emph{original-language} data. 
    The x-axis is the sampling rate (from 0.25 to 1.0), and the y-axis is F1 score. 
    Each subplot corresponds to a different language (ES, DE, EN, RU, ZH), while line colors distinguish \emph{Native SAE Training} from \emph{English SAE Transfer}. 
    Markers reflect the feature extraction approach (\emph{features}, \emph{hidden\_states}, or \textbf{mean difference} \emph{top\_n\_features}).}
    \label{fig:lineplot_9b_it_original}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/lineplot_model_9b_it_translated_swap.pdf}
    \caption{Performance vs.\ sampling rate for the 9B \emph{instruction-tuned} model on different \emph{translated-language} data. 
    As in Figure~\ref{fig:lineplot_9b_it_original}, the x-axis shows sampling rate, the y-axis is F1, and subplots detail performance across ES, DE, EN, RU, and ZH. 
    Lines and markers compare \emph{English SAE Transfer} to \emph{Native SAE Training} under different feature types.}
    \label{fig:lineplot_9b_it_translated}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth] {figures/lineplot_model_9b_original_swap.pdf}
    \caption{Performance vs.\ sampling rate for the 9B \emph{base} model using \emph{original-language} data. 
    Subplots again separate ES, DE, EN, RU, and ZH. 
    The curves illustrate how training type (Native vs.\ English transfer) and feature extraction (full features, hidden states, \textbf{mean difference} top $n$ features) affect F1 across varying sampling rates.}
    \label{fig:lineplot_9b_original}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/lineplot_model_9b_translated_swap.pdf}
    \caption{Performance vs.\ sampling rate for the 9B \emph{base} model using \emph{translated} datasets. 
    The x-axis is sampling rate, the y-axis is F1, and each subplot is a distinct target language. 
    Color and marker styles reflect training type and feature extraction, as in prior figures.}
    \label{fig:lineplot_9b_translated}
\end{figure}

\paragraph{Analysis of Feature Overlap.}
Table~\ref{tab:scores_overlap} compares F1 scores for different training approaches (\emph{English Transfer}, \emph{Native}, and \emph{Translated SAE}) across five languages (DE, EN, ES, RU, ZH). The \emph{Overlap} columns indicate how many of the top 20 SAE features are shared with each respective training scheme. As expected, each model has a complete overlap (1.000) with its own native features. In contrast, cross-lingual overlaps (e.g., \emph{Overlap English} for Spanish or Chinese) are comparatively low (often around 0.06--0.26). Top 20 features were stored for each model trained on a language. Overlaps were calculated as standard jaccard similarities measures between train and test language sites, where we compare the features from the training set of one language to that of the top 20 features derived during training on the test language. For example, English-Spanish overlap is calculated using the top 20 SAE features derived from logistic regression training on the English dataset, and the top 20 features derived from logistic regression training on the Spanish Dataset. We then compute the similarity metric between the two.


\begin{table*}[ht]
    \centering
    \caption{F1 Scores and Overlap for Models and Test Languages. 
    F1 scores are reported for three evaluation strategies: 
    \textbf{F1 (EN-T)}: Trained on English SAE features and tested on other languages (Transfer), 
    \textbf{F1 (N)}: Trained and tested natively, 
    \textbf{F1 (Tr-SAE)}: Trained on translated inputs with extracted SAE features. 
    Overlap measures indicate representation similarity: 
    \textbf{Ovlp (EN)}: Overlap with English Transfer, 
    \textbf{Ovlp (Tr)}: Overlap with Translated SAE.}
    \label{tab:scores_overlap}
    \begin{tabular}{llccc cc}
        \toprule
        \textbf{Model} & \textbf{Lang} & \textbf{F1 (EN-T)} & \textbf{F1 (N)} & \textbf{F1 (Tr-SAE)} & \textbf{Ovlp (EN)} & \textbf{Ovlp (Tr)} \\
        \midrule
        9b    & DE  & 0.710 & 0.945 & 0.708 & 0.098 & 0.099 \\
        9b    & EN  & --    & 0.969 & --    & --    & --    \\
        9b    & ES  & 0.768 & 0.926 & 0.771 & 0.212 & 0.200 \\
        9b    & RU  & 0.888 & 0.973 & 0.886 & 0.237 & 0.221 \\
        9b    & ZH  & 0.592 & 0.856 & 0.593 & 0.061 & 0.064 \\
        \cmidrule(lr){1-7}
        9b it & DE  & 0.722 & 0.941 & 0.723 & 0.093 & 0.089 \\
        9b it & EN  & --    & 0.969 & --    & --    & --    \\
        9b it & ES  & 0.792 & 0.928 & 0.790 & 0.207 & 0.209 \\
        9b it & RU  & 0.903 & 0.973 & 0.903 & 0.263 & 0.253 \\
        9b it & ZH  & 0.599 & 0.858 & 0.602 & 0.086 & 0.071 \\
        \bottomrule
    \end{tabular}
\end{table*}



Despite relatively small overlaps in top features, the \emph{English Transfer} and \emph{Translated SAE} configurations can still yield competitive F1 scores (e.g., RU with English Transfer at 0.888 or 0.903 for instruction-tuned). This suggests that, although the top features in one language are not strictly identical to those in another, a significant subset of high-impact features appears useful across languages. At the same time, the strongest performance generally occurs under \emph{Native} training.

\subsubsection{Full SAE learns classifiers find different features than Mean-diff top-N features}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/cross_lingual_feature_activations.pdf}
    \caption{Feature overlap count between Full SAE Top-20 and Mean-Difference Top-20 feature selection across sampling rates among native-language trained SAEs (from 9B-IT, layer 31). Higher overlap suggests greater consistency in feature selection between the two methods.}
    \label{fig:feature_overlap}
\end{figure}

As we have seen in Figure 10-13, our Full SAE learns features outperform the Mean-diff Top-20 features. This makes sense because our features are learned through supervision, while the other method is done by naive clustering. You can also see that the top-20 "useful features" found by two different method from 9B-IT model is different in Figure \ref{fig:feature_overlap}. As we use more data, the overlap fully got washed out.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\FloatBarrier
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Action prediction}
\label{app:action_paired_plot}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/action_paired.pdf}
    \caption{Paired bar plot for hidden state compared to SAE feature performance for behavior prediction across datasets.}
    \label{fig:action_paired}
\end{figure}

below we show the disaggregated performance of SAE features vs.\ hidden states ability to predict a model's actions or behaviors across multiple task scenarios. Specifically, we focus on the 9B instruction-tuned model (\emph{9b it}) under three dataset conditions:
\begin{enumerate}
    \item \textbf{Original questions without context:} Queries posed directly with no additional background.
    \item \textbf{Questions with correct context:} Queries augmented by relevant information aligned with the true scenario.
    \item \textbf{Questions with incorrect context:} Queries intentionally combined with misleading or contradictory statements.
\end{enumerate}

Figure~\ref{fig:action_paired} presents a paired bar plot that compares \emph{hidden states} (gold bars) and \emph{SAE features} (pink bars) for predicting whether the model will respond with a particular action or behavior. Each subplot corresponds to a different dataset, illustrating how these features perform under various context conditions. Notably, the SAE-based classifier often achieves performance levels on par with or superior to the raw hidden-state baseline, suggesting that SAE features may help isolate key aspects of the model’s decision-making process. This pattern holds across original questions (no context) as well as questions provided with correct or incorrect context, indicating that SAE features can enhance interpretability and robustness in action prediction tasks.


\subsection{Action Features}
\label{app:action_autointerp}

To further investigate how these learned representations drive action prediction, we highlight in the tables below the top classifier features for the original and no context scenario in the middle layer setting, reflecting the core layers from which features are extracted. 

The goal would be to identify if similar concepts are activated across model sizes e.g. are features from the 2b similar to the concepts on the 9b-it that is trying to predict its own behaviour? These tables help reveal whether similar conceptual features emerge across different context conditions (e.g., \textit{No Context} vs.\ \textit{original}) or whether the model learns context-specific indicators tied to the question setup. 

\input{tables/latex_tables/No_Context_layer_middle}
\input{tables/latex_tables/Original_layer_middle}

\paragraph{High-Level Consistencies Across Models.}
Across the tables comparing 2B, 9B, and 9B-IT, we see frequent mentions of programming-related features (e.g., code syntax, function definitions, data structures). Such technical elements dominate many of the top features identified by our \textit{autointerpretable} definitions. However, we also observe several non-programming references (e.g., legal terminology, societal or economic concepts) shared across models—particularly at middle or late layers.

An example we observe is the presence of \emph{Economic and Socio-Political Commentary} across models. The 2B model identifies phrases relating to “economic inequality and socio-political commentary” (Feature 4214), whereas 9B-IT surfaces “legal terminology and concepts related to administrative and tax liability” (Feature 8568). Both target broader sociopolitical or legal contexts.

It is important to note that our similarity claims are constrained by the level of granularity in \textit{autointerpretable} annotations. Different feature IDs may describe related or overlapping real-world concepts, even if they are not labeled identically. At a high level, these tables suggest that all three Gemma-2 variants (2B, 9B, and 9B-IT) learn to capture similar domains, with broad thematic parallels (legal frameworks, social dynamics, etc.) emerging beyond mere code-based patterns. Thus, even though the precise feature names differ, it appears plausible that many of these salient features reflect similar underlying concepts. 

