Here, we investigate best practices for using \emph{GemmaScope} SAE features in classification tasks across model scale, SAE width, layer depth, pooling strategies, and binarization. We also briefly touch upon the cross-modal applicability of text-trained SAE features to a \emph{PaliGemma 2} vision-language model.

\paragraph{Datasets:}
We evaluate performance on a suite of open-source classification benchmarks, including binary safety tasks (jailbreak detection, election misinformation, harmful prompt detection) and multi-class scenarios (user intent classification, scenario understanding, abortion intent detection from tweets, banking queries classification).  Detailed dataset characteristics are in Appendix~\ref{app:model_info}.

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/pooling_layers.pdf}
        \caption{Layer-wise classification performance for each model scale. The dotted black line indicates a TF-IDF baseline, while the red dashed line indicates a last token hidden-state probe baseline. SAE-based methods (colored violin plots) often surpass these baselines, with middle-layer SAE features typically achieving the highest scores.}
        \label{fig:pooling_a}
    \end{subfigure}
    
    \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/pooling_binary.pdf}
        \caption{Token level top-$N$ vs.\ full binarized features. Token level top-$N$ improves with larger values of $N$, and binarization can worsen this performance. However, binarization of all tokenwise activations reached the best performance of Token level top-$N$ whilst removing the need to compute top-$N$ values, which would be important as $N$ scales, offering a more efficient alternative.}
        \label{fig:pooling_b}
    \end{subfigure}
    \caption{Analysis of model performance across different layers and pooling strategies. A strong baseline is established by averaging the optimal performance per task across the hidden states across three models.}
    \label{fig:pooling}
\end{figure}

\subsection{Impact of Layer Depth and Model Scale}

We evaluate \texttt{gemma-2-2b}, \texttt{9b}, and \texttt{9b-it}, using their early, middle, and late layers, with SAE widths of 16K/65K for \texttt{gemma-2-2b} and 16K/131K for \texttt{gemma-2-9b} and \texttt{9b-it}, using different pooling strategies.


We extract token-level SAE features and train LR classifiers, comparing the results to TF-IDF and final-layer hidden-state baselines \footnote{we did not benchmark against mean-diff here because that required task to be binary classification}.

Figure~\ref{fig:pooling}(a) depicts the layer-wise performance for the three model scales across our text-based classification tasks. We observe:

\begin{itemize}[itemsep=-2pt,topsep=1.5pt]
    \item \textbf{Layer Influence:} Middle-layer activations typically produce slightly higher F1 scores than early- or late-layer features, indicating that mid-level representations strike a useful balance between semantic and syntactic information for classification tasks.
    \item \textbf{Model Scale:} Larger models (9B, 9B-IT) achieve consistently higher mean performance (above 0.85 F1) compared to the 2B model. This aligns with the general larger hidden dimension in these models facilitating richer representations.
    \item \textbf{SAE Outperforms Baselines:} SAE based features often exceed the performance of the TF-IDF baseline (dotted black line) and final-hidden-state probe (red dashed line)
\end{itemize}


\subsection{Pooling Strategies and Binarization}

We next examine pooling and binarization strategies. Token level max activation pooling methods included no max pooling (top-0), top-20, and top-50 features per token. Binarization is applied after token aggregation. 

Figure~\ref{fig:pooling}(b) compares two feature selection strategies: (1) no max pooling with summation of \emph{all} SAE features, and (2) selecting the top-$N$ token level activations (here, 20 and 50), with and without binarization. LR classifiers are trained on the resulting features.
\begin{itemize}[itemsep=-1.7pt,topsep=1.5pt]
    \item \textbf{Binarization:}  Binarized and no max pooling of SAE features outperform both hidden-state probes and bag-of-words (dotted lines in Figure~\ref{fig:pooling}(b)).  This indicates the effectiveness of SAE features, particularly when combined with binarization, for capturing relevant information.
    \item \textbf{Token level top-$N$ Selection:} Can outperform the binarized and no max pooling approach in certain settings, especially when $N$ increases, and not binarized. However, the margin is typically small, and top-$N$ selection demands additional computation to identify discriminative features.
\end{itemize}

These observations motivate our decision to adopt binarized and no max pooling as a default due to the reduced computational overhead whilst maintaining performance, while acknowledging that token-level top-$N$ might excel for certain tasks.
\vspace{-3pt}

\paragraph{Interpretability and Layer-Wise Insights:}
We find that \emph{middle-layer} SAE features often produce the highest accuracy across tasks. This trend echoes prior work suggesting that intermediate layers encode richer, more compositional representations than either early or late layers. Crucially, we find that binarizing the full set of SAE features offers a robust one-size-fits-all approach, whereas selecting a top-$N$ subset can yield slightly higher performance but requires additional computational steps. From an interpretability perspective, the binarization strategy also grants a straightforward notion of “feature activation”: whether or not a feature dimension was triggered above zero. Such a thresholding approach can facilitate more useful and usable feature-level analyses and potential explanations for model decisions.

\subsection{Cross-Modal Transfer of Text-Trained SAE Features}

Finally, we conduct a preliminary investigation into the cross-modal applicability of SAE features trained on text. Specifically, we tested whether features useful for text classification could also be beneficial in a vision-language setting.

\paragraph{Experimental Setup:}
Instead of using text-based Gemma models directly, we use a Gemma-based LLaVa model (\emph{PaliGemma 2}) \cite{liu2023improvedllava}, which processes both image and text inputs. Activations from image-text pairs were fed into a Gemma-based SAE of equivalent size to assess whether a text-trained SAE could extract meaningful features from multimodal representations. We then classified images from CIFAR-100 \cite{krizhevsky2009learning}, Indian food \cite{rajistics_indian_food_images}, and Oxford Flowers \cite{Nilsback08} using SAE-derived features.

\paragraph{SAE Features Transfer Modalities Effectively:}
The results of these cross-modal experiments are detailed in Appendix \ref{app:multimodal}. We found that the binarization and no max pooling strategy, effective for text-only tasks, remained effective with SAE features derived from \emph{PaliGemma 2} processing partial textual inputs in a vision-language environment. While these initial findings are promising, a more comprehensive study tailored for multimodal analysis is needed to fully explore the benefits and limitations of transferring text-trained SAE features to vision-language tasks.