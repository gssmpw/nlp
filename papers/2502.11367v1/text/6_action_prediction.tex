This section examines whether smaller models can predict the output correctness ("action") of larger, instruction-tuned models in knowledge-intensive QA tasks. This relates to \emph{scalable oversight}, where a smaller, interpretable model monitors a more capable system. We focus on predicting the \texttt{9B-IT} model's behavior using features from smaller models and assess the impact of context fidelity.

\paragraph{Goal and Motivation:}
We aim to determine whether smaller and/or base models (\texttt{Gemma 2-2B, 9B}) can predict their own behavior or that of a larger and/or fine-tuned model (\texttt{9B-IT}) on knowledge-based QA tasks, based on correct or incorrect factual information. This aligns with a \emph{scalable oversight} scenario, where a smaller model monitors a more capable system when they share the same corpus and architecture.
\vspace{-0.2em}

\paragraph{Datasets:}
We use the entity-based knowledge conflicts in question answering dataset \cite{longpre2022entitybasedknowledgeconflictsquestion}, which provides binary correctness labels for model responses. Open-ended generation is performed with \emph{vllm} \cite{kwon2023efficient}, and answers are scored using \emph{inspect ai} \cite{UKGovernmentBEIS_inspect_ai} with GPT-4o-mini as the grader.
\vspace{-0.2em}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/action.pdf}
    \caption{Action prediction performance for \texttt{9B-IT} across different context manipulations (Original, Question Swap, Subject Swap). Each bar represents a different LLM extracted features trained into classifiers (\texttt{2B, 9B, 9B-IT}) using SAE features; the black horizontal lines indicate the hidden-states baseline. High predictive power is observed with the correct context, dropping significantly with context manipulations. 2B-based features are competitive in predicting 9B-ITâ€™s behaviors.}
    \label{fig:action}
\end{figure}
\vspace{-0.2em}

\paragraph{Experimental Design and Results:}
We focus on predicting \texttt{9B-IT}'s output correctness. For a given model $M$ (\texttt{2B, 9B, 9B-IT}):
\begin{enumerate}[itemsep=-1.7pt,topsep=1.5pt]
    \item We generate open-ended answers to prompts using the model.
    \item We use \texttt{GPT-4o-mini-0718} to label each answer as correct or incorrect.
    \item We extract pretrained SAE activations from the input question, with and without provided contexts.
    \item We train a logistic regression model to predict the binary correctness label from these extracted features.
\end{enumerate}


We also perform cross-model prediction (e.g., \texttt{2B} predicting \texttt{9B}'s correctness), similar to \cite{binder2024lookinginwardlanguagemodels}. We fix the SAE width to 16K and compare the quality of predictions using full SAE binarized approach to those using the Top-$N$ mean difference feature method, and analyze auto-interpretable descriptions of features to understand if similar explanations are shared in the top features across models.

Figure~\ref{fig:action} summarizes the macro F1 scores across several conditions from the \emph{NQ-Swap} and \emph{inspect\_evals} datasets: Original context, Question Swap, and Subject Swap. Key findings:

\begin{itemize}[itemsep=-1.7pt,topsep=1.5pt]
    \item \textbf{Context Fidelity is Crucial:} Providing the correct context ("Original" setting) yields the highest F1 scores (above 80\%). Removing or swapping the context causes a significant drop (20\%), underscoring the importance of reliable response prediction across contexts.
    \item \textbf{Inter-Model Prediction is Effective:} Surprisingly, 2B-based SAE features can predict \texttt{9B-IT}'s correctness nearly as well as, and sometimes \emph{better than}, \texttt{9B-IT}'s \emph{own} features. This is a key result for scalable oversight.
    \item \textbf{SAE Features Outperform Hidden States:} Hidden-state baselines (black lines) generally perform worse than the binarized SAE feature sets, reinforcing the utility of the SAE-based approach for this "behavior prediction" task.
\end{itemize}

\paragraph{Implications for Scalable Oversight:}
These findings highlight the promise of using smaller SAEs to interpret or predict the actions of more powerful LMs. Although context consistency is critical, the ability to forecast a larger model's decisions has significant implications for AI safety and governance, especially in risk-sensitive domains.
In summary, our results demonstrate that:
\begin{enumerate}[itemsep=-1.7pt,topsep=1.5pt]
    \item SAE-based features consistently outperform hidden-state and TF-IDF baselines across classification tasks, especially when using summation + binarization.
    \item For multilingual toxicity detection, native training outperforms cross-lingual transfer, though instruction-tuned models (e.g., \texttt{9B-IT}) may exhibit modestly better transfer as you can see in Appendix \ref{app:action_paired_plot} and \ref{app:action_autointerp}.
    \item Smaller LMs can leverage SAE features to accurately predict the behavior of larger instruction-tuned models, suggesting a scalable mechanism for oversight and auditing.
\end{enumerate}
