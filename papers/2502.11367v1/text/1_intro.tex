\begin{figure}[!htp]
    \centering
    \includegraphics[width=\linewidth]{figures/performance_comparison.pdf}
    \caption{Multilingual performance comparison across three feature selection methods under varying training data sampling rates. Solid bars represent models trained on native language data, while hatched bars show performance with English transfer learning. Binarized SAE features demonstrate robustness across different training data constraints.}
    \label{fig:sampling-main}
\end{figure}

% Paragraph 1 (What is the problem?)
Large language models (LLMs) have transformed natural language processing (NLP), demonstrating impressive performance on diverse tasks and languages, even in knowledge-intensive and safety-sensitive scenarios \cite{hendrycks2023overview, ngo2022alignment, cammarata2021curve}.  However, the internal decision-making processes of LLMs remain largely opaque \cite{cammarata2021curve}, raising concerns about trustworthiness and oversight, especially given the potential for deceptive or unintended behaviors. Mechanistic interpretability (MI), the study of the internal processes and representations that drive a model's outputs, offers a promising approach to address this challenge \cite{elhage2022toy, wang2022interpretability}. However, despite its potential, applying MI to real-world tasks presents significant challenges.


\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/sae_workflow.pdf}
    \caption{Diagram explaining our approaches to evaluating token-level pooling and aggregation of SAE features.}
    \label{fig:workflow}
\end{figure*}
\raggedbottom


% Paragraph 2 (Why is it interesting and important?)
Sparse Autoencoders (SAEs) have recently emerged as a promising technique within MI for understanding LLMs. SAEs generally work by learning a compressed, sparse representation of the LLM's internal activations.  This is achieved by up-projecting the dense hidden state of the LLM to a sparser, ideally monosemantic, representation \cite{bricken2023towards, cunningham2023scaling}. Identifying semantically meaningful features within LLMs using SAEs allows for deploying these features into explainable classification pipelines. This has the potential to boost performance and detect harmful biases or spurious correlations before they manifest in downstream tasks \cite{anthropic2024features}.  The ability to employ SAE features for classification across diverse settings, ranging from toxicity detection to user intent, offers a scalable form of "model insight" \cite{bowman2022measuringprogressscalableoversight}, which is crucial for building trust, safety, and accountability in high-stakes domains like medicine and law \cite{abdulaal2024xrayworth15features}.

% Paragraph 3 (Why is it hard?)
Despite the promise of SAEs for MI, surprisingly few systematic studies have provided practical guidance on their use for classification. While promising results have been reported across various tasks \cite{features_as_classifiers, sae_probing, sae_features_llava}, inconsistent experimental protocols, a lack of standardized benchmarks, and limited exploration of key architectural decisions hinder comparability and the development of best practices. Although tools like Transformer Lens \cite{nanda2022transformerlens} and SAE Lens \cite{bloom2024saetrainingcodebase} have improved standardization in sampling activations, critical questions about optimal configurations for diverse tasks, particularly in multilingual and multimodal settings, remain unanswered. This makes it challenging to establish the robustness and generalizability of SAE-based classification approaches.


This work directly addresses these limitations by providing a comprehensive and systematic investigation of SAE-based classification for LLMs. We introduce a reproducible pipeline for large-scale activation extraction and classification, enabling robust and generalizable conclusions. Specifically, we explore critical methodological choices, evaluate performance across diverse datasets and tasks, and investigate the potential for SAEs to facilitate model introspection and oversight (Figure \ref{fig:workflow}).


% Paragraph 5 (Our approach, results, and limitations)
\paragraph{Summary of Contributions}
\begin{enumerate}[itemsep=-1.7pt,topsep=1.5pt]
    \item \textit{Systematic Classification Benchmarks (Section~\ref{sec:pooling}, Part~1 ):} We introduce a robust methodology to evaluate and select SAE-based features in safety-critical classification tasks and show superior performance overall. 
    
    \item \textit{Multilingual Transfer Analysis (Section~\ref{sec:multilingual}, Part~2):} We analyze the cross-lingual transferability of SAE features in multilingual toxicity detection and show SAE features outperform everything in-domain and demonstrate potential on cross-lingual feature generalization.
    
    \item \textit{Behavioral Analysis and Model Oversight (Section~\ref{sec:action}, Part~3):} We extend SAE-based features to model introspection tasks, investigating whether LLMs can predict their own correctness and that of larger models, showing the potential of scalable model oversight.
    
    % \item \textit{Unified Framework and Open-Source Pipeline:} We provide an end-to-end pipeline to extract residual stream features from SAE representations, with configurable layers, widths, pooling, and aggregation, standardizing SAE-based research.

\end{enumerate}

