\subsection{\mname{} Training Pipeline}
We provide an overview of the full training process in Algorithm~\ref{alg:full}. The algorithm has two stages. In the first stage, we first collect manipulation trajectories with multiple RL policies. In the second stage, we distill the experience into our controller. For the dataset filtering step, we apply a very simple heuristic rule. If a rollout ends with dropping the object, then we directly discard the last $2$ second transitions. 

\begin{algorithm}[htbp]
\caption{Training Procedure of \mname{} Controller $p_\theta$}\label{alg:full}
\begin{algorithmic}[1]
\REQUIRE Manipulation tasks $\{T_i\}$ in simulation (e.g. Anygrasp to Anygrasp).
\STATE Train RL policy $\pi_i$ on each $\{T_i\}$ to convergence.
\STATE Collect training dataset $\mathcal{D}=\cup_i \small {\rm Rollout(\pi_i)}$. 
\STATE Preprocess dataset $\mathcal{D}$ by filtering failure transitions.
\STATE Train \mname{} controller $p_\theta$ on $\mathcal{D}$.
\RETURN $p_\theta$
\end{algorithmic}
\end{algorithm}

\subsection{Implementation of Anygrasp-to-Anygrasp}
The core dexterous manipulation task used by Algorithm~\ref{alg:full} is Anygrasp-to-Anygrasp. We describe its implementation as follows. 

\textbf{Grasp Generation} To define this task, we first need to generate the grasp set for each object with the Grasp Generation Algorithm~\ref{alg:graspgen}. The algorithm first generates a base grasp set using heuristic sampling, and we further expand this grasp set via RRT search to ensure that it can cover as many configurations as possible. Note that there exist many approaches for synthesizing grasps. Here, we just provide one option that works well empirically.

\begin{algorithm}[htbp]
\caption{Grasp Generation}\label{alg:graspgen}
\begin{algorithmic}[1]
\REQUIRE Object mesh $\mathcal{M}$. Initial Grasp Set Size $N$. RRT Step $N_{RRT}$.
\STATE Grasp Set $S\leftarrow {\rm HeuristicSample}(\mathcal{M}, N)$.
\STATE $S\leftarrow {\rm GraspRRTExpand} (S, \mathcal{M}, N_{RRT})$.
\RETURN $S$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[htbp]
\caption{HeuristicSample}\label{alg:hsample}
\begin{algorithmic}[1]
\REQUIRE Object mesh $\mathcal{M}$, Num Samples $N$.
\STATE $S\leftarrow [\quad]$.
\WHILE{${\rm len}(S) < N$}
\STATE $N_{pts}=\rm random([2,3,4])$.   \COMMENT{Num grasp point.}
\STATE Point $P$, Normal $n$ $\leftarrow {\rm SampleSurface}(\mathcal{M}, N_{pts})$.
\IF{${\rm GraspAnalysis}(P, n)$}
    \STATE Object Pose $p\leftarrow {\rm RandomPose()}$.
    \STATE Finger Configuration $q\leftarrow {\rm Assign}(\mathcal{M}, P, n, q)$.
    \IF{${\rm NoCollision}(q, p, \mathcal{M})$}
    \STATE $S\leftarrow S\cup \{(q, p)\}$
    \ENDIF 
\ENDIF
\ENDWHILE
\RETURN $S$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[htbp]
\caption{GraspAnalysis}\label{alg:analysis}
\begin{algorithmic}[1]
\REQUIRE Contact Points $P$, Contact Normals $n$.
\STATE $F_{min}\leftarrow$ Min solution to Net Force Opt. $(n)$.
\IF{$F_{min} < F_{thresh}$}
\STATE \textbf{return}  TRUE
\ENDIF 
\RETURN FALSE
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[t]
\caption{GraspRRTExpand}\label{alg:analysis}
\begin{algorithmic}[1]
\REQUIRE Grasp set $S$, Object Mesh $\mathcal{M}$, RRT Step $N_{RRT}$.
\FOR{$i=1,2,..., N_{RRT}$}
\STATE $(q, p)\leftarrow {\rm RandomSample()}$. \COMMENT{$q$ finger configuration. $p$ object pose.}
\STATE $(q^*, p^*)\leftarrow {\rm NearestNeighbor}((q,p), S)$.
\STATE $(q, p)\leftarrow{\rm Interpolate}((q,p), (q^*, p^*))$.
\STATE $(q, p)\leftarrow {\rm FixContactAndCollision}(q, p, \mathcal{M})$.
\STATE $S=S\cup \{(q, p)\}$.
\ENDFOR
\RETURN $S$
\end{algorithmic}
\end{algorithm}

For Algorithm~\ref{alg:analysis}, we originally followed the implementation proposed by~\cite{khandate2023sampling}. However, we find that minimizing the wrench can be too strict and it is not efficient for large-scale generation. Therefore, we introduce a simplified optimization problem for grasp analysis as follows, which we find effective in practice.
\begin{tcolorbox}[colback=black!5!white, colframe=black!75!black, 
    boxsep=5pt, % Controls padding inside the box
    left=5pt, % Extra margin on the left
    right=5pt, % Extra margin on the right
    top=3pt, % Extra margin on the top
    bottom=3pt, % Extra margin on the bottom
    title=Net Force Optimization $(\{n_i\})$, % Title text
    fonttitle=\bfseries, % Title font style
    coltitle=white, % Title color 
]
\[
\begin{aligned}
    &\underset{f_i}{\text{Minimize: }}  && \left\Vert\sum f_in_i\right\Vert^2 \\
    &\text{s.t. } && \forall i, f_i\geq 0, \\
    &                   && \exists i,  f_i = 1.
\end{aligned}
\]
\end{tcolorbox}
Intuitively, we apply force $f_i$ at each contact point along contact normal $n_i$ and we optimize for a nontrivial force combination~($\exists f_i=1$) that can generate a near-zero net force. If the minimizer of this problem is below a threshold, we consider this grasp as stable. Note that the second existence constraint is hard to directly parameterize as a differentiable loss function. In our implementation, we decompose this problem into several subproblems by enforcing $f_1=1, f_2=1, ..., f_n=1$ in each subproblem.

\textbf{Reward Design} The reward function for the Anygrasp-to-Anygrasp task is as follows. It is composed of three different terms, goal-related reward $r_{goal}$, style-related reward $r_{style}$, and regularization terms $r_{reg}$.
\begin{equation}
    r = w_{goal} r_{goal} + w_{style}r_{style} + w_{reg}r_{reg}.
\end{equation}
The goal-related reward term $r_{goal}$ involves target object pose and finger joint positions:
\begin{align}
    r_{goal} &= \exp (-\alpha_{pos} \Vert p_{obj}-p_{obj}^{target}\Vert^2 -\alpha_{orn} d(R_{obj}, R_{obj}^{target})) \\ & - \alpha_{hand} \Vert q - q^{target}\Vert^2 \\
    &+ \alpha_{bonus}\mathbf{1}(\text{goal achieved}). 
\end{align}
The regularization term includes the penalty on the action scale, applied torque, and work:
\begin{align}
    r_{reg} &= - \alpha_{work} |\dot{q}^T||\tau| - \alpha_{action} \Vert a\Vert^2 - \alpha_{tau}\Vert\tau\Vert^2. 
\end{align}
For the style reward, it is a penalty term on the fingertip velocity. This can elicit different manipulation styles (fast movement or slow movement). This reward term is mainly used to boost data diversity in temporal dimensions, see discussion below. 
\begin{align}
    r_{style}=\sum_i\alpha_i \Vert \dot{x}_{tip}^{i}\Vert.
\end{align}
\textbf{Goal Dynamics}
A crucial design in the Anygrasp-to-Anygrasp task is the goal dynamics. We find that when we set a goal very far away, the RL policy can usually fail to reach that goal and as a result, the RL learning process can plateau very early. Therefore, throughout the RL process, we set goals within a moderate distance to ensure effective RL learning. Specifically, when the current goal is achieved, we search for a grasp in our grasp cache whose object distance is within a certain range as our next goal. We achieve this through a Nearest Neighbor search. Since NN search is computationally expensive for a large grasp set, we first perform a random down-sampling at each update step before the next goal computation.

\subsection{Boosting Dataset Diversity with Diverse Rewards}
To boost the diversity of the training dataset, we use multiple reward setups to train policies of different styles and use all of these policies for data collection. In this paper, we train RL policies with different $w_{style}$ and $w_{reg}$ coefficients and this yields policies of both fast and slow object manipulation behavior. This ensures that real-world states, whether they are from a good policy or a suboptimal one, are effectively managed by our controller.

\subsection{RL Training Setups}
We implement all the training tasks and data collection using the IsaacGym simulator~\cite{makoviychuk2021isaac}. We use Proximal Policy Optimization~(PPO)~\cite{schulman2017proximal} as our RL algorithm. We use asymmetric actor-critic during training, where the actor only observes proprioception information and desired goal~(represented by a relative transformation from current state to goal state), while the critic network observes all the state information such as object position and velocity etc. We use MLP to parameterize both actor and critic networks, whose hidden dimensions are both $[1024, 512, 512, 256, 256]$. We use a learning rate 0.0005, batch size 8192, PPO clip value 0.2, with $\gamma=0.99$ and GAE $\tau=0.95$. We use 8192 environments in parallel. 

\subsection{Domain Randomization}
We apply extensive domain randomizations in both training and data collection. We list the randomized components in the Table~\ref{table:dr}. 

\begin{table}[!t]
\renewcommand\arraystretch{1.05}
\caption{Domain Randomization Setup}
\centering
\begin{tabular*}{0.87\linewidth}{l@{\extracolsep{\fill}}c}
\toprule
Object: Mass~(kg)             & [0.03, 0.25]    \\
Object: Friction              & [0.5, 1.2]     \\
Object: Shape                 & $\times\mathcal{U}(0.95, 1.05)$     \\
Hand: Initial Joint Noise     & [-0.05, 0.05] \\
Hand: Friction                & [0.5, 1.2]    \\
\midrule
PD Controller: P Gain         &  $\times\mathcal{U}(0.8, 1.1)$      \\
PD Controller: D Gain         &  $\times\mathcal{U}(0.7, 1.2)$     \\
\midrule
Random Force: Scale           & 1.0/2.0       \\
Random Force: Probability     & 0.2    \\
Random Force: Decay Coeff. and Interval & 0.99 every 0.1s     \\ 
\midrule
Joint Observation Noise (white noise) &  $+\mathcal{N}(0, 0.025)$      \\
Joint Observation Noise (episode noise)      & $+\mathcal{N}(0, 0.005)$  \\
Action Noise                 & $+\mathcal{N}(0, 0.05)$   \\
\bottomrule
\end{tabular*}
\label{table:dr}
\end{table}


\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{src/figure/rss_diffusion_arch.pdf}
    \caption{Diffusion Model in \mname{} Controller. We use a standard U-Net based diffusion model with FiLM conditioning. }
    \label{fig:dm}
\end{figure*}
\subsection{Diffusion Model Architecture}
We illustrate our diffusion model architecture in Figure~\ref{fig:dm}. We first use a state encoder and a mode encoder to produce a compact representation for the conditional inputs. Then, we use a UNet to predict the noise added to the current sample, with FiLM-conditioning~\cite{perez2018film} in the middle layers. We use 3 blocks for both UNet encoder and decoder. For the UNet, we use a hidden dimension of 768 and replace 1D convolution layers with fully connected layers. We implement the state encoder as a 6-layer MLP with hidden dimension 1024. We also use GroupNorm after each MLP layer with group size 8. We experimented with 8 and 12 DDIM steps during the diffusion model inference. We find there is a tradeoff between sample fidelity~(action accuracy) and latency, and they affect user experience in different ways.

In this paper, we use $T=2$ as the future motion prediction horizon, which corresponds to 0.2s future. We use $K=8$ finger keypoints~(PIP of each finger and the fingertips). In early experiments, we used $K=4$ finger keypoints (fingertips only), but this representation behaves suboptimally in the experiments and has a large inverse dynamics training loss. The simplified representation does not encode the full action information, as $K=4$ 3D keypoints only span a 12-dimensional space but the hand is 16DOF. We stack 4 history steps of robot proprioception including fingertip position, joint position, target joint position, and control error as input to the diffusion model.

