\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{src/figure/rss_method.pdf}
    \caption{\textbf{Model:} Architecture of the \mname{} controller. The whole system takes robot state, external motion conditioning, and mode conditioning as input. A diffusion model first generates the motion as the intermediate action representation. The motion conditioning is not fed into the diffusion model directly but as the gradient guidance during the diffusion sampling. Then, another inverse dynamics model will translate the generated motion to executable robot action. We implement our diffusion model as a UNet in this paper. The inverse dynamics model is a residual multilayer perceptron.  }
    \label{fig:arch}
\end{figure*}
\section{The \mname{} Controller}

We propose to pretrain a generative behavior model $p_{\theta}(a|o)$ on the simulation dataset to model prior action distribution so that it can generate stable and effective actions $a$ conditioned on the robot state $o$. During inference, we can sample actions from this distribution and further aligned with external motion commands using gradient guidance. We detail the dataset used for training the model in section~\ref{ref:data}, the model architecture in section~\ref{method:arch}, and the inference procedure in section~\ref{method:inference}.

\subsection{Preliminaries}
\label{method:prelim}
\paragraph{Diffusion Models} Diffusion Model~\cite{ho2020denoising} is a powerful generative model capable of capturing highly complex probabilistic distributions, which we use as our base model. The classical form of the diffusion model is the Denoising Diffusion Probabilistic Model~(DDPM)~\cite{ho2020denoising}. DDPM defines a forward process that gradually adds noise to the data sample $x_0\sim p_{data}(x)$:
\begin{equation}
x_t = \sqrt{\alpha_t} x_{t-1} + \sqrt{1 - \alpha_t} \epsilon_t,
\end{equation}
where $\alpha_t$ is some noising schedule. We have $x_t \sim \mathcal{N}(\sqrt{\bar{\alpha}_t} x_0, (1 - \bar\alpha_t) I)$ where $\bar\alpha_t = \prod_{s=1}^t \alpha_s$ goes to 0 as $t\to+\infty$. DDPM trains a model $\mu_\theta(x_t, t)$ to predict denoised sample $x_0$ given the noised sample $x_t$ with its timestep $t$. During sampling, DDPM generates the sample by removing the noise through a reverse diffusion process:
\begin{equation}
p(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \sigma_t^2 I)
\end{equation}
DDPM can generate high-fidelity samples in both vision and robotics applications. In addition to the power to generate data samples faithfully, diffusion models also support guided sampling~\cite{janner2022planning}, which turns out to be very useful in our set-up. 

Specifically,  when $p_{data}(x)$ is modeled by a diffusion model, we can sample from a product probability distribution $p(x) \propto p_{data}(x) h(x)$ where $h(x)$ is given by a differentiable energy function $h(x)=\exp J(x)$. To do this, we only need to introduce a small modification to the reverse diffusion step. Given the current sample $\mu$, we add a correction term $\alpha\Sigma\nabla J{(\mu)}$ to $\mu$. Here, $\alpha$ is a step size hyperparameter and $\Sigma$ is the variance in each diffusion step. This will guide the sample towards high-energy regions in the sample space. We can set $h(x)$ to control the style of generated samples.

\paragraph{Robot System and Notations} In this paper, we assume the robot hand is driven by a widely used PD controller. At each control timestep, we command a joint target position $\tilde{q}_t$ and the controller will use torque $\tau = K_p (\tilde{q}_t - q_t) - K_d \dot{q}_t$ to drive the joints.  Here, $q_t$ is the current joint position, and $\dot{q}_t$ is the joint velocity. $K_p$ and $K_d$ are two constant scalar gains. We use $x_t$ to denote the key point positions of finger links at time $t$ in the wrist frame. Note that our algorithm does not rely on a specific system implementation and can be extended to other robot systems. We can also specify keypoints and actions for other robots to implement our proposed algorithm.

\subsection{Large-Scale Behavior Dataset Generation}
\label{ref:data}
Since human teleoperation or external policies will control the robot hand to interact with the object in diverse ways, our model should be capable of providing refinement for all these potential scenarios~(states). To achieve this, we require a large-scale behavior dataset to pretrain our \mname{} model, ensuring comprehensive coverage of the state space. We accomplish this by collecting object manipulation trajectories in simulation through reinforcement learning. 

\textbf{Anygrasp-to-Anygrasp} To ensure our dataset can cover a broad range of potential states, we introduce Anygrasp-to-Anygrasp as our central pretraining task. This task captures the essential part of in-hand manipulation, which is to move the object to arbitrary configurations. For each object, we define our training task as follows. We first generate a set of object grasps using Grasp Analysis and Rapidly-exploring Random Tree (RRT)~\cite{lavalle2001rapidly}, similar to the Manipulation RRT procedure~\cite{khandate2023sampling}. Each generated grasp is defined as a tuple (hand joint position, object pose). In each RL rollout, we initialize the object in the hand with a random grasp. We set the goal to be a randomly selected nearby grasp using the k Nearest-Neighbor search. After reaching the current grasp goal, we update the goal in the same way. We find it crucial to select a nearby reachable goal during the training process, as learning to reach a distant grasp directly can be difficult. After training, we use this anygrasp-to-anygrasp policy to rollout grasp transition sequences to cover all the possible hand-object interaction modes. We sample over 100K grasp for most objects during grasp generation to ensure coverage. This training procedure yields a rich repertoire of useful skills, including object translation and reorientation, which the high-level policy can leverage for solving downstream tasks~(Figure~\ref{fig:space}). In addition to the Anygrasp-to-Anygrasp task, we also introduce other tasks such as free finger moving and fine-grained manipulation~(e.g. fine rotation) to handle tasks that have special precision requirements. 

During RL training, we use a diverse set of random objects and wrist poses. For each task, we include random geometrical objects with different physical properties. To enhance the robustness of our policy, we randomly adjust the wrist to different poses throughout the process, in addition to employing commonly used domain randomizations, so the policy will learn to counteract the gravity effects and exhibit prehensile manipulation behavior~(Figure~\ref{fig:dataset}). By combining all these data, the robot hand can manipulate different kinds of objects in different wrist configurations against gravity rather than being limited to manipulating a single object at a certain pose. More details of the RL training can be found in Appendix.

We collect a total of $1\times 10^{10}$ transitions as our simulation dataset, equivalent to 31.7 years of real world experience. Generating this dataset~(by rolling out trained RL policies) requires 300 GPU hours. Although the dataset is large, we hypothesize it can still be far from sufficient as the human dexterity emerges from millions of years of evolution. Nevertheless, this simulated dataset still enables reliable dexterous behavior that have not been showed before.



\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{src/figure/rss_method_space_new.pdf}
    \caption{Our large-scale, multi-task pretraining dataset covers diverse grasp to grasp transitions~(arrows). DexGen controller learns the dataset action distribution~(purple shaded area) at each state, and we can use sequential motion prompting~(purple triangle) to perform a useful long-horizon skill, connecting two distance states.}
    \label{fig:space}
    \vspace{-0.3cm}
\end{figure}

\subsection{\mname{} Model Architecture}
\label{method:arch}
We illustrate our \mname{} model architecture in Figure~\ref{fig:arch}. The \mname{} model has two modules. The first module is a diffusion model that characterizes the distribution of robot finger keypoint motions given current observations. Here we use 3D keypoint motions $\Delta x \in \mathbb{R}^{T\times K\times3}$ in the robot hand frame as an intermediate action representation, This representation is particularly advantageous for integrating guidance from human teleoperation. In this context, $T$ is the future horizon, $K$ is the number of finger keypoints. The second module in \mname{} is an inverse dynamics model, which converts the keypoint motions to executable robot actions~(i.e. target joint position) $a_t=\tilde{q}_t$.

We use a UNet-based~\cite{ronneberger2015u} diffusion model to fit the complex keypoint motion distribution of our multitask dataset. Our model learns to generate several future finger keypoint offsets $\Delta{x}_{i} = x_{t+i} - x_{t}$ conditioned on the robot state at timestep $t$ and a mode conditioning variable. The state is a stack of historical proprioception information. The mode conditioning variable is a one-hot vector to explicitly indicate the intention of the task. For instance, when placing an object we do not want the model to produce actions that will make the robot hold the object firmly. Without introducing a ``release object'' indicator, it is hard to prompt the hand to release the object if most of the actions in the dataset will keep the object in the palm. In our dataset, the majority of transitions are labeled with a ``default'' (unconditional) label, and only a small portion of them corresponding to specialized scenarios has a special mode label. We only use a specialized precision rotation mode label for screwdriver in our experiments. For releasing object, we find that disabling DexGen controller is sufficient in practice.

The inverse dynamics model is a simple residual multilayer perceptron that outputs a normal distribution to model the actions conditioned on the current robot state and motion command. We train both the diffusion model and inverse dynamics model with our generated simulation dataset using the standard diffusion model loss function and the MSE loss for regression respectively. We train these models with the AdamW optimizer~\cite{loshchilov2017decoupled, kingma2014adam} for 15 epochs using 96 GPUs, which takes approximately 3 days. The detailed network setup can be found in the appendix. 

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth, height=0.65\linewidth]{src/figure/rss2025-setup.pdf}
    \caption{Real world experimental setup based on Allegro Hand with a Franka Panda Arm (Left). We use human teleoperation (Right) as a proxy for high-level policy.}
    \label{fig:enter-label}
\end{figure}

\subsection{Inference: Motion Conditioning with Guided Sampling}
\label{method:inference}
Our goal is to sample a keypoint motion that is both safe (i.e. from our learned distribution $p_\theta(\Delta x|o)$) and can maximally preserve the input reference motion. Formally, this can be written as $\Delta{x}\sim p_\theta(\Delta x|o) \exp (-{\rm Dist}(\Delta x, \Delta x_{input}))$. Here, $\Delta x_{input}\in\mathbb{R}^{K\times 3}$ is the input commanded fingertip offset, and ${\rm Dist}$ is a distance function that quantifies the distance between the predicted sequence and the input reference. There can be many ways to instantiate this distance function. In this paper, we find the following simple distance function works well empirically: 
\begin{equation}
    {\rm Dist}(\Delta{x}, \Delta x_{input}) = \sum_{i=1}^T\Vert \Delta{x}_{i} - \Delta x_{input}  \Vert^2.
\end{equation}
The above function encourages the generated future fingertip position to closely match the commanded fingertip position. Since the action of the robot hand has a high degree of freedom~(16 for the Allegro hand used in this paper), naive sampling strategies become computationally intractable. To address this, we propose using gradient guidance in the diffusion sampling process to incorporate motion conditioning. In each diffusion step, we adjust the denoised sample $\Delta{x}$ by subtracting $\alpha\Sigma\nabla_{\Delta{x}} {\rm Dist}(\Delta{x}, \Delta x_{input})$ as a guide. Here $\alpha$ is a parameter of the strength of the guidance to be tuned, which we will study in experiments. The generated finger keypoint movement is then converted to action by the inverse dynamics model. We use DDIM sampler~\cite{song2020denoising} during inference for 10Hz control. The total sampling time is around 27ms~(37Hz) on a Lambda workstation equipped with an NVIDIA RTX 4090 GPU. 
