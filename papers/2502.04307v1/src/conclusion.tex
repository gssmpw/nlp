\section{Conclusion}
In this paper, we have presented \mnamefull{} as an initial attempt towards building a foundational low-level controller for dexterous manipulation. We have demonstrated that generative pretraining on diverse multi-task simulated trajectories yield a powerful generative controller that can translate coarse motion prompts to effective low-level actions. Combined with external high-level policy, our controller exhibits unprecedented dexterity. We believe that our work opens up new possibilities in various dimensions of dexterous manipulation. 

\textbf{Limitations and Future Work} Our exploration still has some limitations to be addressed in future works, which we discuss as follows.
\begin{enumerate}
    \item \textbf{Touch Sensing} In this work, we rely on joint angle proprioception for implicit touch sensing~(i.e. inferring force by reading control errors), which can be insufficient and nonrobust for fine-grained problems. In many cases, it is impossible to recover contact geometry based on joint angle error. In the future, we will add touch to pretraining, which has been shown possible for sim-to-real transfer. We hope that this can further improve the robustness of our system. 
    \item \textbf{Vision: Hand-Eye Coordination} Our low-level controller does not involve vision. Nevertheless, we observe that vision feedback is necessary for producing accurate tool motions for many tasks such as using a screwdriver. It is unclear whether this vision processing should be in the high-level policy or part of the proposed foundation low-level controller, and we leave this to future research.
     \item \textbf{Real-world Finetuning} In this work, we deploy our controller in a zero-shot manner. However, due to the sim-to-real gap, it can still be necessary to fine-tune our controller with some real world experience. 
     

\end{enumerate}



