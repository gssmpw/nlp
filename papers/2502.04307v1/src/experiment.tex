\section{Experiments}
In the experiments, we first validate the effectiveness of \mname{} through simulated experiments, demonstrating its ability to enhance the robustness and success rate of extremely suboptimal policies. Then, we test our system in the real world with a focus on its application in shared autonomy.  Our results show that \mname{} can assist a human operator in executing unprecedented dexterous manipulation skills with remarkable generalizability.

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth,height=0.65\linewidth]{src/figure/test_objects.png}
    \caption{Part of our real world testing objects, which are not present in our pretraining dataset. We include objects of different sizes, masses, and aspect ratios.}
    \label{fig:enter-label}
\end{figure}
\subsection{System Setup}
In this paper, we use Allegro Hand as our manipulator and we attach the Allegro Hand to a Franka-panda robot arm. In the teleoperation experiments in real world, we use a retargeting-based system to control the robot with human hand gestures. The human hand pose is captured by Manus Glove and retargeted to the Allegro hand through a confidential fast retargeting method that runs at 300Hz, which we will release in a future report. We obtain the 6D human wrist pose via the Vive tracking system and use it to control the robot arm separately. Although we use this single robot setup in our experiments, we believe our method is general and can be applied to other hand setups. 
\begin{figure*}
    \centering
    \includegraphics[width=1.0\linewidth]{src/figure/sim_result.pdf}
    \caption{Results of simulation evaluation. We use \mname{} to correct several noise-corrupted expert policies. Note that each dimension of action space is bounded by [-1, 1] and these noises ruin the expert action most of the time. We measure the average duration (in seconds) and number of achieved goals per trial over a 20-minute simulated experiment. As shown in the figure, \mname{} can successfully improve the performance of these policies. Across the experiments, \mname{} can boost the duration by 10-100x and even help an extremely perturbed policy to achieve success where the baseline fails. }
    \label{fig:simulation}
\end{figure*}
\subsection{Simulated Experiments}

\subsubsection{Experimental Setup} 
We first test the capability of \mname{} in assisting suboptimal policies in solving the Anygrasp-to-Anygrasp task in simulation. We simulate 2 kinds of suboptimal policies with an expert RL policy $\pi_{exp}$. The first one is $\pi_{noisy}(a|s) = \pi_{exp}(a|s) + \mathcal{U}(-\alpha, \alpha)$, which simulates an expert that can perform dangerous suboptimal actions through additive uniform noise. The second is $\pi_{slow}(a|s) = \mathcal{U}(0, \alpha) \pi_{exp}(a|s)$, which is a slowdown version of expert. We compare these suboptimal experts $\pi$ to their assisted counterparts ${\rm \mname{}}\circ \pi$. We record the average number of critical failures~(drop the object) and the number of goal achievements within a certain time of different policies. 

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{src/figure/rss_qualitative.pdf}
    \caption{\mname{} can maximally preserve input action while correcting dangerous actions. \mname{} can reject users' behavior (open up the palm) and keep holding the object.}
    \label{fig:qual}
\end{figure}

\subsubsection{Main Results}  \quad We plot the result of different policies in Figure~\ref{fig:simulation}. We find that without our assistance, the noisy expert has much more frequent failures. As a result, it can only hardly achieve any goals in the evaluation. In contrast, with the assistance of \mname, we can partially recover the performance of this noisy expert. We also find that for different policies, the optimal guidance value is also different. Fortunately, there is a common region working well for all these policies. Moreover, when the guidance is relatively small, although we can maintain the object in hand, we can not achieve the desired goal as well because \mname{} does not know what the goal is. When the guidance becomes too large, the potentially suboptimal external motion command may take over \mname{} guidance and lead to a lower duration in some cases.  
\input{src/table/real}
\subsection{Real World Experiments:}
We have demonstrated that our system can provide effective assistance through simulated validation. Then, we further design several tasks for benchmarking in the real world. In the first set of experiments, we ask a human teleoperator to act as an external high-level policy and we evaluate whether our system can assist humans to solve diverse dexterous manipulation tasks. We introduce a set of atomic skills that covers common in-hand dexterous manipulation behavior.
\begin{itemize}
    \item \textbf{In-hand Object Reorientation} The user is required to control the hand to rotate a given object to a specific pose. In the beginning, we initialize the object in the air over the palm, and the user needs to first teleoperate the hand to grasp the object. 
    
    \item \textbf{Functional Grasping} Regrasping is a necessary step in tool manipulation. The user is asked to perform a power grasp on the tool handle placed either horizontally~(normal) or vertically in the air~(horizontal functional grasp). In the beginning, the user can only perform a pinch grasp or precision grasp.

    \item \textbf{In-hand Regrasping} We define this task as a harder version of object reorientation. In this task, the user is asked to achieve a specific grasp configuration~(object pose + finger pose). In the beginning, the object is initialized with a precision grasp on the fingertip.
\end{itemize}
Besides these tasks, we demonstrate some more realistic, long-horizon tasks as well. These tasks require the user to combine the skills above. In the main text, we only study the following two tasks. We leave more examples in the demo video in the appendix.
\begin{itemize}
    \item \textbf{Screwdriver} In this task, the user needs to pick up a screwdriver lying on the table and use it to tighten a bolt. 
    \item \textbf{Syringe} In this task, the user needs to pick up a syringe and inject some liquid into a target region.
\end{itemize}
\paragraph{Evaluation Protocol} We evaluate the performance of a teleoperation system by measuring the success rate a human user can achieve when using it to solve certain tasks. Before evaluation, we let users familiarize themselves with each evaluated teleoperation system in 30 minutes. Our experiments involve 2 users in this section.

\subsection{Real World Results} The performance of different approaches is shown in Table~\ref{table:real}. We observe that humans can hardly use the baseline teleoperation system to solve the tasks above. The user can drop the object easily during the contact-rich manipulation process. Compared to the baseline, our system can successfully help the user to solve many tasks in various challenging setups. During these experiments, we also observe the following intriguing properties of our system:

\paragraph{Protective ``Magnetic Effect''} We find that the fingertips show some ``magnetic effect'' when they are in contact with the object. When the user mistakenly moves a supporting finger which may drop the object, our model can override that behavior and maintain the contact as if the fingertips are sticking to the object~(Figure \ref{fig:qual} second row). This explains why the user can achieve a much higher success rate in these dexterous tasks. 

\paragraph{Intention Following} Although our model overrides dangerous user action, we find that in most cases our model can follow the user's intention~(action) well and move along the user-commanded moving direction. During the manipulation procedure, the user can still have a sense of agency over the robot hand and complete a complex task. This finding echoes our simulated result with noisy policies: DexGen can realize the intention in the noisy suboptimal actions. 

We also present a breakdown analysis of the long-horizon tasks in Table~\ref{tab:longhorizon}. For the first time, we enable such long-horizon dexterous manipulation behavior in the real world through teleoperation. Achieving tool use remains challenging as it involves several stages of complex dexterous manipulation: we can achieve a reasonable stage-wise success rate, but chaining these skills together is difficult. However, we believe that improving stage-wise policy in the future can eventually close the gap~(see the conclusion part). 
