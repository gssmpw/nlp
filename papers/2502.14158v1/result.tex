\section{Result}
\begin{table*}[ht]
\small
\centering
\caption{Results (\%) of different models using fewer tasks on datasets under the 5-way 5-shot \textit{in-domain} setting. Bold: best (based on the pairwise t-test with 95\% confidence). Underline: runner-up.} %Rel. Imp.: relative improvement over the second-best model.}
\label{res_one}
\resizebox{0.9\textwidth}{!}{%
\begin{tabular}{@{}c|cccccccc|cccccccc@{}}
\toprule
\multirow{3}{*}{Model} & \multicolumn{8}{c|}{Amazon-Clothing}                                                                                                  & \multicolumn{8}{c}{CoraFull}                                                                                                         \\ \cmidrule(l){2-17} 
                       & \multicolumn{2}{c}{5 tasks}     & \multicolumn{2}{c}{10 tasks}    & \multicolumn{2}{c}{15 tasks}    & \multicolumn{2}{c|}{20 tasks}   & \multicolumn{2}{c}{5 tasks}     & \multicolumn{2}{c}{10 tasks}    & \multicolumn{2}{c}{15 tasks}    & \multicolumn{2}{c}{20 tasks}    \\ \cmidrule(l){2-17} 
                       & Acc            & F1             & Acc            & F1             & Acc            & F1             & Acc            & F1             & Acc            & F1             & Acc            & F1             & Acc            & F1             & Acc            & F1             \\ \midrule
Protonet               & 49.17          & 48.36          & 53.51          & 52.55          & 55.82          & 54.99          & 57.99          & 57.14          & 37.20          & 35.98          & 40.14          & 38.89          & 43.90          & 42.96          & 45.58          & 44.34          \\
MAML                   & 44.90          & 43.66          & 45.67          & 44.44          & 46.29          & 44.97          & 46.90          & 45.60          & 38.15          & 36.83          & 42.26          & 41.28          & 44.21          & 43.95          & 46.37          & 45.43          \\ \midrule
MetaMix                   & 78.32          & 78.22          & 78.66          & 78.52           & 80.16          & 79.15          & 81.09          & 80.52          & 62.95          & 62.25          & 64.20          & 63.95          & 65.72          & 64.19          & 67.59          & 66.26          \\
MLTI                   & 79.19          & 78.59          & 79.91          & 78.92           & 80.22          & 79.39          & 81.27          & 80.86          & 63.19          & 63.06          & 65.72          & 65.69          & 66.25          & 64.92          & 67.15          & 66.10          \\
Meta-Inter                   & \underline{79.92}          & \underline{79.22}          & \underline{80.12}          & \underline{79.56}           & \underline{80.55}          & \underline{79.90}          & \underline{81.26}          & \underline{81.05}          & \underline{63.82}          & \underline{63.36}          & 66.59          & 65.92          & 67.19          & 65.50          & 68.22          & 67.59          \\ \midrule
Meta-GNN               & 55.29          & 50.44          & 57.19          & 53.65          & 62.29          & 59.55          & 70.19          & 67.22          & 42.96          & 40.83          & 45.09          & 42.87          & 47.15          & 45.38          & 49.88          & 48.12          \\
GPN                    & 68.23          & 67.16          & 70.06          & 69.57          & 72.40          & 71.95          & 72.81          & 71.56          & 43.35          & 42.08          & 46.19          & 44.81          & 51.56          & 50.24          & 55.83          & 54.76          \\
G-Meta                 & 60.43          & 60.11          & 64.51          & 63.74          & 68.99          & 67.96          & 71.98          & 72.75          & 45.84          & 44.27          & 49.22          & 48.91          & 51.15          & 50.53          & 59.12          & 58.56          \\
Meta-GPS               & 62.02          & 59.76          & 69.21          & 69.04          & 73.01          & 71.92          & 75.74          & 74.85          & 50.33          & 48.22          & 57.85          & 54.86          & 61.28          & 60.11          & 63.76          & 62.28          \\
%TENT                   & 73.15    & 70.99    & 75.77    & 72.19    & 79.90    & 78.73    & 81.90    & 79.36    &52.62    &50.15    &59.69    &56.69    & 66.88    & 64.44    & 68.90    & 66.46    \\ 
X-FNC                   & 69.12    & 68.29    & 72.12    & 71.11    & 75.19    & 74.63    & 79.26    & 78.02    & 55.06          & 53.10          & 61.53          & 60.29          & 65.22    & 64.10    & 66.09    & 65.12    \\ 
COSMIC                   & 75.66    & 74.92    & 76.39    & 75.72    & 77.92    & 76.59    & 78.36    & 77.39    & 62.29          & 60.39          & 65.39          & 64.80          & 66.72    & 65.72    & 68.29    & 67.20
\\
TLP                   & 71.39    & 70.39    & 73.39    & 72.52    & 74.72    & 73.36    & 75.60    & 74.29    & 51.79          & 49.72          & 56.72          & 55.79          & 57.72    & 56.73    & 57.99    & 57.30
\\ 
TEG                   & 78.55    & 77.92    & 80.26    & 79.30    & 80.82    & 79.99    & 81.19    & 80.16    & 62.89          & 61.26          & \underline{68.29}          & \underline{67.39}          & \underline{68.59}    & \underline{67.55}    & \underline{70.06}    & \underline{69.29}
\\ \midrule
%GraphPrompt                   & 70.92    & 69.72    & 72.39    & 71.99    & 73.92    & 73.26    & 74.28    & 73.55    & 50.12          & 49.72          & 53.72          & 52.19          & 56.72    & 55.18    & 57.72    & 56.19 \\
%GPF                    & 71.96    & 70.39    & 72.23    & 71.56    & 74.72    & 73.92    & 75.38    & 74.39    & 51.39          & 50.09          & 54.19          & 53.72          & 56.30    & 55.92    & 57.92    & 56.92 \\ \midrule
SMILE                   & \textbf{82.80} & \textbf{82.49} & \textbf{83.46} & \textbf{82.88} & \textbf{83.92} & \textbf{83.33} & \textbf{84.66} & \textbf{84.52} & \textbf{66.34} & \textbf{65.70} & \textbf{71.72} & \textbf{71.15} & \textbf{70.78} & \textbf{70.19} & \textbf{72.60} & \textbf{72.10} \\ \midrule
%Rel. Imp.              & 13.19          & 16.20          & 10.15           & 14.81          & 5.03           & 5.84           & 3.37           & 4.60           & 20.49          & 23.73          & 16.56          & 18.01          & 5.83           & 8.92           & 5.37           & 8.49           \\ \midrule
\multirow{3}{*}{Model} & \multicolumn{8}{c|}{Amazon-Electronics}                                                                                               & \multicolumn{8}{c}{DBLP}                                                                                                              \\ \cmidrule(l){2-17} 
                       & \multicolumn{2}{c}{5 tasks}     & \multicolumn{2}{c}{10 tasks}    & \multicolumn{2}{c}{15 tasks}    & \multicolumn{2}{c|}{20 tasks}   & \multicolumn{2}{c}{5 tasks}     & \multicolumn{2}{c}{10 tasks}    & \multicolumn{2}{c}{15 tasks}    & \multicolumn{2}{c}{20 tasks}    \\ \cmidrule(l){2-17} 
                       & Acc            & F1             & Acc            & F1             & Acc            & F1             & Acc            & F1             & Acc            & F1             & Acc            & F1             & Acc            & F1             & Acc            & F1             \\ \midrule
Protonet               & 46.20          & 45.09          & 49.56          & 48.57          & 51.98          & 51.05          & 54.03          & 53.20          & 46.57          & 45.47          & 50.90          & 49.81          & 51.02          & 49.74          & 52.09          & 51.05          \\
MAML                   & 34.34          & 33.42          & 34.76          & 33.76          & 35.42          & 34.41          & 35.91          & 34.95          & 39.71          & 38.86          & 40.34          & 39.58          & 40.70           & 39.85          & 41.31          & 40.58          \\ \midrule
MetaMix                   & 61.96          & 61.82          & 63.72          & 63.66          & 65.19          & 64.92          & 66.15          & 65.72          & 72.12          & 71.15          & 73.19          & 72.12          & 75.16          & 73.95          & 76.22          & 74.79          \\
MLTI                   & 62.25          & 62.02          & 65.26          & 65.09          & 66.72          & 65.59          & 67.19          & 66.22          & 72.36          & 71.96          & 72.92          & 72.55          & 73.22          & 73.10          & 75.10          & 74.95          \\
Meta-Inter                   & 62.79          & 62.56          & 65.76          & 65.52          & 67.19          & 66.15          & 68.99          & 67.29          & 72.52          & 72.11          & 73.19          & 72.99          & 74.28          & 73.25          & 75.29          & 75.10          \\ \midrule
Meta-GNN               & 40.52          & 39.74          & 46.16          & 45.87          & 48.92          & 47.93          & 50.86          & 50.07          & 50.68          & 49.04          & 53.86          & 49.67          & 59.72          & 59.36          & 65.49          & 62.12          \\
GPN                    & 49.08          & 47.91          & 51.12          & 49.98          & 54.24          & 53.23          & 56.69          & 55.62          & 70.26    & 69.13    & \underline{74.42}    & \underline{73.48}    & \underline{76.02}          & \underline{75.03}          & \underline{76.61}          & \underline{75.60}          \\
G-Meta                 & 43.29          & 42.20          & 49.57          & 52.90          & 56.96          & 55.38          & 60.41          & 59.91          & 53.08          & 48.13          & 55.92          & 53.64          & 57.82          & 56.76          & 63.17          & 62.85          \\
Meta-GPS               & 46.11          & 43.62          & 57.90          & 56.20          & 67.73          & 66.69          & 70.13          & 69.15          & 56.59          & 54.12          & 65.20          & 63.20          & 73.00          & 72.35          & 75.16          & 73.19          \\
%TENT                   & \underline{63.36}    & \underline{63.59}    & \underline{69.70}    & \underline{67.12}    & \underline{70.09}    & \underline{68.60}    & \underline{72.09}    & \underline{71.60}    & 70.15          & 69.02          & 74.31          & 72.22          & \underline{76.89}    & \underline{76.72}    & \underline{77.87}    & \underline{77.70}    \\ 
X-FNC                   & 59.26    & 56.39    & 63.72    & 62.10    & \underline{69.82}    & 67.63    & 71.36    & 70.02    & 69.06          & 68.10          & 72.53          & 71.29          & 74.29    & 73.22    & 76.19    & 75.20    \\ 
COSMIC                   & 64.06    & 63.02    & \underline{67.36}    & \underline{66.32}    & 68.22    & 67.09    & 70.16    & 69.30    & 71.29          & 70.19          & 72.09          & 70.80          & 73.02    & 71.20    & 75.16    & 72.22    \\
TLP                   & 63.09    & 62.19    & 64.30    & 63.59    & 65.72    & 64.32    & 67.18    & 66.72    & 71.26          & 70.75          & 72.87          & 72.09          & 73.39    & 73.06    & 75.16    & 74.69    \\
TEG                   & \underline{65.90}    & \underline{64.62}    & 67.29    & 66.22    & 69.80    & \underline{68.29}    & \underline{72.12}    & \underline{71.16}    & \underline{72.59}          & \underline{72.26}          & 73.79          & 72.19          & 75.52    & 74.50    & 76.26    & 75.12    \\
\midrule
%GraphPrompt                   & 59.90    & 57.92    & 60.19    & 59.29    & 62.02    & 61.25    & 64.18    & 63.25    & 66.92          & 65.72          & 67.70          & 66.12          & 69.72    & 68.28    & 70.27    & 69.59 \\
%GPF                    & 59.96    & 58.99    & 61.93    & 60.96    & 62.12    & 61.96    & 63.82    & 63.29    & 66.52          & 65.59          & 70.12          & 69.92          & 70.29    & 70.29    & 71.12    & 70.92 \\ \midrule
SMILE                   & \textbf{67.30} & \textbf{66.30} & \textbf{70.76} & \textbf{70.05} & \textbf{73.48} & \textbf{72.66} & \textbf{75.42} & \textbf{75.42} & \textbf{75.88} & \textbf{75.05} & \textbf{76.64} & \textbf{75.77} & \textbf{79.56} & \textbf{78.77} & \textbf{80.50} & \textbf{79.61} \\ \bottomrule
%Rel. Imp.              & 6.22           & 4.62           & 1.52           & 4.37           & 4.84           & 5.92           & 4.62           & 5.34           & 8.00           & 8.56           & 2.98           & 3.12           & 3.47           & 2.67           & 3.38           & 2.46           \\ \bottomrule
\end{tabular} %
}
\end{table*}


\begin{table*}[ht]
\centering
\small
\caption{Results (\%) of different models using fewer tasks on datasets under the 5-way 5-shot \textit{cross-domain} setting. A$\rightarrow$B denotes the model is trained on A and evaluated on B.}
\label{res_cross}
\resizebox{0.9\textwidth}{!}{ %
\begin{tabular}{@{}c|cccccccc|cccccccc@{}}
\toprule
\multirow{3}{*}{Dataset} & \multicolumn{8}{c|}{Amazon-Clothing$\rightarrow$CoraFull}                                                                           & \multicolumn{8}{c}{CoraFull$\rightarrow$Amazon-Clothing}                                                                            \\ \cmidrule(l){2-17} 
                         & \multicolumn{2}{c}{5 tasks}     & \multicolumn{2}{c}{10 tasks}    & \multicolumn{2}{c}{15 tasks}    & \multicolumn{2}{c|}{20 tasks}   & \multicolumn{2}{c}{5 tasks}     & \multicolumn{2}{c}{10 tasks}    & \multicolumn{2}{c}{15 tasks}    & \multicolumn{2}{c}{20 tasks}    \\ \cmidrule(l){2-17} 
                         & Acc            & F1             & Acc            & F1             & Acc            & F1             & Acc            & F1             & Acc            & F1             & Acc            & F1             & Acc            & F1             & Acc            & F1             \\ \midrule
Protonet                 & 20.72          & 7.90           & 22.84          & 10.89          & 29.70          & 15.91          & 32.96          & 18.19          & 24.84          & 13.18          & 29.96          & 22.49          & 32.84          & 26.01          & 34.58          & 29.90          \\
MAML                     & 20.40          & 13.51          & 20.74          & 13.19          & 26.68          & 12.22          & 30.19          & 15.92          & 23.56          & 12.10          & 27.35          & 20.16          & 30.19          & 23.95          & 32.96          & 27.96          \\ \midrule
MetaMix                   & 31.96          & 28.76          & 33.12          & 31.22          & 35.22          & 33.19          & 37.15          & 35.55          & 34.76          & 31.66          & 36.25          & 33.69          & 39.72          & 37.16          & 41.26          & 39.25 \\
MLTI                   & 33.29          & 30.12          & 35.16          & 32.29          & 38.25          & 35.52          & 40.22          & 38.29          & 35.12          & 33.49          & 37.22          & 35.29          & 42.19          & 41.52          & 45.66          & 43.95          \\
Meta-Inter                   & 34.72          & 32.19         & 35.76          & 34.26          & 40.16          & 37.59          & 42.29          & 40.32          & 41.76          & 39.59          & 43.22          & 41.57          & 44.11          & 42.25          & 47.29          & 45.55          \\ \midrule
Meta-GNN                 & 26.36          & 20.99          & 30.50          & 26.72          & 33.22          & 30.15          & 35.99          & 32.16          & 32.16          & 22.39          & 35.22          & 26.62          & 38.16          & 29.35          & 39.66          & 32.90          \\
GPN                      & 35.86    & 34.81    & 39.38    & 38.03    & 41.10    & 39.82    & 41.96    & 41.15    & 40.08          & 38.73          & 41.78          & 40.67          & 43.90          & 42.87          & 45.04          & 44.30          \\
G-Meta                   & 30.36          & 26.95          & 33.19          & 29.62          & 35.29          & 33.16          & 36.21          & 35.20          & 35.22          & 30.16          & 37.22          & 30.29          & 40.19          & 32.29          & 41.19          & 36.96          \\
Meta-GPS                 & 32.02          & 27.07          & 34.15          & 30.19          & 35.66          & 34.15          & 39.26          & 37.55          & 45.59          & 43.29          & 47.62          & 45.10          & 50.19          & 47.12          & 52.19          & 49.32          \\
%TENT                     & 34.37          & 32.75          & 36.17          & 33.71          & 38.68          & 36.08          & 40.56          & 37.91          & \underline{49.29}    & \underline{46.93}    & \underline{52.80}    & \underline{49.72}    & \underline{54.91}    & \underline{52.38}    & \underline{55.69}    & \underline{54.62}    \\
X-FNC                    & 33.59          & 31.10          & 35.15          & 32.19          & 37.25          & 34.12          & 39.72          & 36.29          & 47.26          & 45.16          & 49.30          & 46.22          & 52.20          & 49.29          & 53.72          & 50.22          \\ 
COSMIC                    & \underline{38.02}          & 36.22          & 40.09          & 37.05          & \underline{42.20}          & 39.09          & \underline{42.46}          & 40.30          & 49.20          & 47.19          & 52.02          & 51.29          & 53.09          & 52.16          & \underline{55.39}          & \underline{53.90}          \\
TLP                    & 37.99          & \underline{37.29}          & \underline{41.23}          & \underline{39.59}          & 41.99          & \underline{40.92}          & 42.26          & \underline{41.25}          & \underline{51.12}          & \underline{50.15}          & \underline{53.90}          & \underline{52.29}          & \underline{54.26}          & \underline{52.66}          & 55.20          & 53.30          \\
TEG                    & 33.05          & 31.29          & 35.26          & 34.32          & 35.80          & 34.69          & 36.35          & 35.36          & 41.09          & 40.20          & 42.12          & 41.39          & 43.72          & 42.60          & 46.56          & 43.87          \\
\midrule
%GraphPrompt                    & 32.10          & 31.92          & 35.29          & 34.11          & 36.22          & 35.55          & 37.28          & 36.22          & 42.12          & 40.12          & 44.36          & 42.29          & 47.39          & 45.32          & 49.17          & 48.29          \\
%GPF                    & 32.26          & 31.72          & 34.23          & 33.06          & 35.19          & 35.06          & 37.26          & 36.99          & 42.92          & 41.50          & 43.17          & 42.72          & 45.12          & 44.29          & 46.22          & 45.90          \\ \midrule
SMILE                    & \textbf{42.64} & \textbf{41.27} & \textbf{45.14} & \textbf{43.69} & \textbf{45.88} & \textbf{44.10} & \textbf{46.72} & \textbf{45.65} & \textbf{56.36} & \textbf{55.25} & \textbf{58.84} & \textbf{57.53} & \textbf{59.08} & \textbf{57.96} & \textbf{59.38} & \textbf{58.25} \\ \midrule
%Rel. Imp.                 & 18.91          & 18.56          & 14.62          & 14.88          & 11.63          & 10.75          & 11.34          & 10.93          & 14.34          & 17.73          & 11.44          & 15.71          & 7.59           & 6.83           & 6.63           & 6.65           \\ \midrule
\multirow{3}{*}{Dataset} & \multicolumn{8}{c|}{Amazon-Electronics$\rightarrow$DBLP}                                                                            & \multicolumn{8}{c}{DBLP$\rightarrow$Amazon-Electronics}                                                                             \\ \cmidrule(l){2-17} 
                         & \multicolumn{2}{c}{5 tasks}     & \multicolumn{2}{c}{10 tasks}    & \multicolumn{2}{c}{15 tasks}    & \multicolumn{2}{c|}{20 tasks}   & \multicolumn{2}{c}{5 tasks}     & \multicolumn{2}{c}{10 tasks}    & \multicolumn{2}{c}{15 tasks}    & \multicolumn{2}{c}{20 tasks}    \\ \cmidrule(l){2-17} 
                         & Acc            & F1             & Acc            & F1             & Acc            & F1             & Acc            & F1             & Acc            & F1             & Acc            & F1             & Acc            & F1             & Acc            & F1             \\ \midrule
Protonet                 & 31.86          & 22.56          & 32.58          & 23.73          & 35.90          & 32.76          & 39.88          & 35.71          & 28.84          & 18.62          & 30.54          & 20.08          & 33.10          & 22.37          & 35.46          & 25.20          \\
MAML                     & 29.17          & 19.13          & 30.10          & 22.15          & 32.97          & 25.98          & 35.25          & 29.11          & 26.59          & 17.99          & 28.36          & 19.29          & 30.02          & 20.15          & 32.16          & 22.16          \\ \midrule
MetaMix                   & 40.16          & 35.68           & 43.25          & 41.69          & 45.19          & 43.12          & 49.12          & 43.59          & 37.70          & 35.22          & 40.20          & 39.09          & 42.25         & 40.16          & 44.19          & 42.20          \\
MLTI                   & 42.12          & 37.19          & 46.39          & 45.06          & 49.19          & 47.25          & 51.35          & 50.39          & 38.22          & 36.96          & 41.39          & 40.25          & 43.39          & 42.05          & 46.12          & 45.09          \\
Meta-Inter                   & 46.19          & 45.12          & 48.15          & 46.79          & 51.29          & 49.76          & 53.18          & 51.02          & 41.50          & 40.15          & 43.19          & 41.10          & 45.20          & 43.35          & 47.15          & 45.05          \\ \midrule
Meta-GNN                 & 39.19          & 34.72          & 42.26          & 39.16          & 43.96          & 39.55          & 45.66          & 42.19          & 35.72          & 33.20          & 39.59          & 38.62          & 40.39          & 39.29          & 41.26          & 40.22          \\
GPN                      & \underline{60.08}    & \underline{58.75}    & \underline{61.92}    & \underline{61.58}    & \underline{63.19}    & \underline{62.60}    & \underline{63.99}    & \underline{63.10}    & 42.99          & 41.46          & \underline{46.36}    & \underline{44.73}    & \underline{47.09}    & \underline{45.42}    & \underline{47.52}          & \underline{45.76}          \\
G-Meta                   & 45.72          & 43.32          & 47.22          & 45.09          & 47.96          & 45.99          & 49.56          & 48.39          & 37.22          & 35.93          & 40.19          & 39.52          & 42.35          & 40.19          & 43.62          & 42.19          \\
Meta-GPS                 & 47.59          & 46.70          & 49.20          & 47.16          & 50.26          & 49.96          & 52.39          & 51.22          & \underline{43.06}          & \underline{42.05}          & 45.12          & 43.16          & 46.02          & 44.95          & 46.79          & 45.02          \\
%TENT                     & 50.79          & 49.41          & 51.01          & 49.54          & 53.45          & 51.73          & 54.67          & 53.19          & \underline{43.37}    & \underline{42.94}    & 44.75          & 43.38          & 46.71          & 45.37          & \underline{47.99}    & \underline{46.79}    \\
X-FNC                    & 49.19          & 48.36          & 49.55          & 48.02          & 51.35          & 50.26          & 52.90          & 51.39          & 41.59          & 40.02          & 42.36          & 42.19          & 44.16          & 43.16          & 46.39          & 45.25          \\ 
COSMIC                    & 57.22          & 55.30          & 58.29          & 57.35          & 60.20          & 61.19          & 61.36          & 62.30          & 39.20          & 37.11          & 41.02          & 40.22          & 43.03          & 42.22          & 44.32          & 43.26          \\
TLP                    & 58.25          & 57.19          & 59.33          & 59.01          & 61.29          & 60.02          & 62.16          & 61.25          & 41.11          & 40.16          & 43.20          & 42.25          & 44.16          & 42.32          & 45.20          & 43.90          \\
TEG                    & 38.05          & 36.29          & 40.21          & 39.32          & 42.80          & 41.69          & 43.35          & 42.36          & 33.19          & 31.20          & 34.22          & 33.52          & 35.70          & 34.62          & 36.55          & 35.37          \\ \midrule
%GraphPrompt                    & 55.30          & 53.90          & 56.19          & 55.39          & 57.62          & 56.15          & 59.28          & 56.22          & 37.22          & 36.29          & 38.56          & 37.56          & 40.19          & 39.32          & 41.17          & 40.29          \\
%GPF                    & 54.29          & 53.79          & 56.13          & 55.26          & 57.29          & 55.36          & 59.16          & 56.39          & 38.92          & 37.52          & 41.27          & 40.17          & 41.92          & 40.29          & 42.19          & 40.69          \\ \midrule
SMILE                    & \textbf{62.44} & \textbf{61.66} & \textbf{64.54} & \textbf{64.16} & \textbf{65.04} & \textbf{64.43} & \textbf{65.78} & \textbf{65.42} & \textbf{46.24} & \textbf{44.54} & \textbf{48.82} & \textbf{47.26} & \textbf{49.26} & \textbf{47.70} & \textbf{49.52} & \textbf{47.88} \\ \midrule
%Rel. Imp.                 & 3.93           & 4.95           & 4.23           & 4.19           & 2.93           & 2.92           & 2.80           & 3.68           & 6.62           & 3.73           & 5.31           & 5.66           & 4.61           & 4.79           & 3.19           & 2.33           \\ \bottomrule
\end{tabular} %
}
\end{table*}

%\paragraph{Model Performance.}
\noindent \textbf{Model Performance.} 
%\subsection{Model Performance}
We present the results of our proposed SMILE and other models under both \textit{in-domain} and \textit{cross-domain} settings with different number of tasks across several datasets in Tables \ref{res_one} and \ref{res_cross}. %For readability, we have placed the results with \textit{standard deviations} in \textbf{Appendix} \ref{full_results}.
According to above results, we can obtain the following in-depth analysis. %We present the results of our proposed SMILE and other baselines under 5-way 5-shot experimental setting with fewer tasks across several datasets in Table \ref{res_one}. Also, we show the results of these models under 10-way 5-shot and sufficient meta-training tasks experimental settings in the Appendix \ref{more_experiment}. According to Table \ref{res_one}, we can obtain the following in-depth analysis.
We can find that our approach achieves the best performance across varying numbers of meta-training tasks in both in-domain and cross-domain settings for all datasets, demonstrating its superiority in dealing with graph few-shot learning with fewer tasks. %This phenomenon is in accordance with our conjecture. 
One plausible reason is that we explicitly introduce degree-based prior in the node representation stage, resulting in more discriminative features beneficial for subsequent tasks. Furthermore, we employ a dual-level mixup strategy, enriching the diversity of both within-task and across-task data, effectively alleviating the negative impact of data and task scarcity. These strategies facilitate the model to extract more transferable meta-knowledge, thereby greatly enhancing its generalization capability.

%We find that as the number of available meta-training tasks decreases, the gains of SMILE become increasingly considerable, while the remaining models exhibit an intolerable performance decline. This aligns with our expectations. Taking the 5 tasks setting on the Cora-Full dataset as an example, as shown in Table \ref{res_one}, SMILE outperforms the second-best model X-FNC by a large margin, reaching 20\%. With the scenarios with fewer tasks, our method can densify the task distribution by explicitly increasing the number of meta-training tasks. However, if there are already adequate tasks, it is not difficult for these models to extract transferable knowledge during meta-training to adapt to meta-testing tasks. Thus, the improvements brought by our method decreases. %However, as the number of original tasks increases, the resulting improvement diminishes. Generally, graph meta-learning methods consistently surpass traditional meta-learning methods because the former simultaneously consider both the feature and structure information of the graph.

We find that graph meta-learning models represented by COSMIC and TEG perform well in scenarios with more tasks across multiple datasets, which aligns with our expectations. These models are specifically designed for graph few-shot learning and utilize unique few-shot algorithms that enable them to achieve discriminative node representations with limited labeled data. However, they struggle in scenarios with fewer tasks, performing significantly worse than our model. This is because they can only extract sufficient transferable meta-knowledge when there are ample meta-training tasks. 
Moreover, the meta-learning with fewer tasks models equipped with SGC, such as MLTI and Meta-Inter, also demonstrate impressive performance in the both in-domain and cross-domain experimental settings. We attribute this phenomenon to the specific strategies employed by these models to mitigate the negative effects caused by limited tasks. However, this type of models still significantly lag behind our model, as they do not incorporate degree-based prior knowledge and fail to address scarcity issues from both data and task perspectives simultaneously. Additionally, traditional meta-learning methods consistently underperform compared to other methods because they completely overlook the important structural information in the graph.

%Also, we show additional results of these models under different experimental settings in the \textbf{Appendix} \ref{more_experiment}, including model performance with sufficient tasks and across various graph encoders and so on, due to the space constraints.

%\paragraph{Ablation Study.} 
\noindent \textbf{Ablation Study.} 
%\subsection{Ablation Study}
To demonstrate the effectiveness of our adopted strategies, we design several model variants. (I) \textit{vanilla mixup}: Without the dual-level mixup, we first compute the prototypes using the support set, and then perform vanilla mixup to query set, which involves mixing the features and generates soft labels. (II) \textit{internal mixup}: In the absence of dual-level mixup, we directly perform mixup on different classes within each task and generate corresponding hard labels, treating them as novel classes. %and the generated labels are treated as novel classes. 
(III) \textit{w/o within and across}: We simultaneously discard both within-task and across-task mixup operations. (IV) \textit{w/o within}: We delete the within-task operation and leave the across-task one. (V) \textit{w/o across}: We remove the across-task strategy and retain the within-task one. (VI) \textit{w/o degree}: We exclude the utilization of degree information and solely employ the vanilla SGC for node representation learning.

%Through the analysis of the results presented in 
According to Table \ref{ablation}, it is evident that the employed strategies have a favorable impact on the model performance. The introduction of dual-level mixup enriches the samples within each task and provides diverse tasks, making significant contributions to enhancing the model. The adopted degree-based prior information also improves the model by learning expressive node embeddings, especially in cross-domain setting. When removing the degree information, the performance drastically declines. One plausible reason is that this module explicitly utilizes structural prior knowledge from the target graph domain, benefiting downstream tasks. Additionally, the results of model variants I and II demonstrate that performing label interpolation within each task, whether generating soft or hard labels, degrades the model performance. %The possible reason is that the introduced mixing labels can confuse the model.

\begin{table*}[ht]
\caption{Results of different model variants with respect to 5 tasks under the 5-way 5-shot setting.}
\label{ablation}
\centering
\normalsize
\resizebox{0.9\textwidth}{!}{%
\begin{tabular}{@{}c|cccccccc|cccccccc@{}}
\toprule
\multirow{2}{*}{Dataset} & \multicolumn{2}{c}{Clothing} & \multicolumn{2}{c}{Electronics} & \multicolumn{2}{c}{DBLP}        & \multicolumn{2}{c|}{CoraFull}   & \multicolumn{2}{c}{Clothing$\rightarrow$CoraFull} & \multicolumn{2}{c}{CoraFull$\rightarrow$Clothing} & \multicolumn{2}{c}{Electronics$\rightarrow$DBLP} & \multicolumn{2}{c}{DBLP$\rightarrow$Electronics} \\ \cmidrule(l){2-17} 
                         & Acc              & F1               & Acc                & F1                & Acc            & F1             & Acc            & F1             & Acc                      & F1                       & Acc                      & F1                       & Acc                      & F1                      & Acc                      & F1                      \\ \midrule
vanilla mixup            & 78.82            & 78.45            & 60.98              & 60.70             & 74.06          & 73.03          & 61.90          & 61.07          & 38.90                    & 38.26                    & 52.93                    & 52.29                    & 57.92                    & 57.30                   & 40.52                    & 39.26                   \\
internal mixup           & 78.96            & 78.72            & 63.59              & 62.21             & 75.34          & 74.45          & 63.64          & 63.00          & 39.12                    & 38.59                    & 53.22                    & 53.19                    & 59.02                    & 57.96                   & 41.36                    & 40.56                   \\
w/o within and across    & 79.10            & 78.97            & 62.36              & 61.08             & 73.14          & 72.36          & 62.44          & 61.85          & 39.72                    & 39.19                    & 53.58                    & 52.42                    & 59.34                    & 58.94                   & 42.12                    & 41.41                   \\
w/o within               & 80.72            & 80.93            & 65.96              & 64.99             & 75.44          & 74.59          & 64.06          & 63.40          & 40.28                    & 39.63                    & 54.52                    & 53.50                    & 60.86                    & 59.16                   & 42.28                    & 41.79                   \\
w/o across               & 81.18            & 80.19            & 64.40              & 63.57             & 74.67          & 73.68          & 63.08          & 62.56          & 41.88                    & 41.38                    & 54.76                    & 53.92                    & 61.52                    & 61.26                   & 43.84                    & 42.29                   \\
w/o degree               & 79.14            & 80.16            & 63.49              & 62.12             & 74.36          & 73.28          & 63.68          & 62.96          & \color{gray}{30.89}                   & \color{gray}{21.05}                    & \color{gray}{32.82}                    & \color{gray}{23.07}                    & \color{gray}{31.12}                    & \color{gray}{21.61}                   & \color{gray}{30.44}                    & \color{gray}{21.08}                   \\
ours                     & \textbf{82.80}   & \textbf{82.49}   & \textbf{67.30}     & \textbf{66.30}    & \textbf{75.88} & \textbf{75.05} & \textbf{66.34} & \textbf{65.70} & \textbf{42.64}           & \textbf{41.27}           & \textbf{56.36}           & \textbf{55.25}           & \textbf{62.44}           & \textbf{61.66}          & \textbf{46.24}           & \textbf{44.54}          \\ \bottomrule
\end{tabular}%
}
\end{table*}

\begin{figure}
    \centering
    \includegraphics[width=0.23\textwidth]{picture/intra_task_number.pdf}
    \includegraphics[width=0.23\textwidth]{picture/inter_task_number.pdf}
    \caption{Results vary %across different datasets 
    with hyperparameters.}%Hyperparameter sensitivity on different datasets.
    \label{hyper}
\end{figure}

%%\paragraph{Hyperparameter Sensitivity.} 
\noindent \textbf{Hyperparameter Sensitivity.}
%\subsection{Hyperparameter Sensitivity}
In the 5-way 5-shot in-domain setting, we investigate the impact of two primary hyperparameters on the model performance: the ratio of generated nodes to the original nodes per task (\textit{i.e.}, $\frac{n_{s}^\prime+n_{q}^\prime}{n_s+n_q}$), and the number of generated tasks $\mathrm{T}_{aug}$. Notably, when the studied hyperparameter changes, we set others to their default values. The results are presented in Fig. \ref{hyper}. We can observe that both parameters demonstrate similar trends, with the model performance showing an initial increase followed by a decrease. We attribute this behavior to the substantial enrichment of data diversity by increasing the number of nodes within each task or the number of tasks. However, beyond a certain threshold, the introduced additional data fails to further densify the data distribution, resulting in limited information gain. %Moreover, we also provide the visualization study in \textbf{Appendix} \ref{visualization_study}. %and the time complexity analysis in Appendix \ref{complexity}.
 