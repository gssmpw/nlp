\section{Experiment}
%\paragraph{Datasets.} 
\noindent \textbf{Datasets.}
%\subsection{Dataset}
To demonstrate the effectiveness of our approach, we conduct few-shot node classification with fewer tasks on four selected prevalent datasets widely used by previous researches, including \textbf{Amazon-Clothing} \cite{mcauley2015inferring}, \textbf{CoraFull} \cite{bojchevski2017deep}, \textbf{Amazon-Electronics} \cite{mcauley2015inferring}, and \textbf{DBLP} \cite{tang2008arnetminer}. Table \ref{dataset} shows the statistics of these datasets. Concisely, \# Nodes and \# Edges represent the number of nodes and edges in the dataset, respectively. \# Features denotes the dimension of the initialized node features, and \# Labels is the number of classes. Class Splits represents the number of classes used for meta-training/meta-validation/meta-testing. 
The detailed descriptions of these evaluated datasets can be found in \textbf{Appendix} \ref{dataset_description}.

\begin{table}[ht]
\centering
\caption{Statistics of the datasets.}
\label{dataset}
\resizebox{0.45\textwidth}{!}{%
\begin{tabular}{@{}c|ccccc@{}}
\toprule
Dataset            & \# Nodes & \# Edges & \# Features & \# Labels & Class Splits \\ \midrule
Amazon-Clothing    & 24,919   & 91,680   & 9,034       & 77        & 40/17/20     \\
Cora-Full          & 19,793   & 65,311   & 8,710       & 70        & 25/20/25     \\
Amazon-Electronics & 42,318   & 43,556   & 8,669       & 167       & 90/37/40     \\
DBLP               & 40,672   & 288,270  & 7,202       & 137       & 80/27/30     \\ \bottomrule
\end{tabular}%
}
\end{table}

%\paragraph{Baselines.} 
\noindent \textbf{Baselines.} 
%\subsection{Baselines}
We mainly select three types of baselines for comparison to verify the superiority of the proposed SMILE. \textit{Traditional meta-learning} methods consist of \textbf{Protonet} \cite{snell2017prototypical} and \textbf{MAML} \cite{finn2017model}. \textit{Meta-learning with fewer tasks} methods comprise \textbf{MetaMix} \cite{yao2021improving}, \textbf{MLTI} \cite{yao2021meta}, and \textbf{Meta-Inter} \cite{lee2022set}. \textit{Graph meta-learning} methods include \textbf{Meta-GNN} \cite{zhou2019meta}, \textbf{GPN} \cite{ding2020graph}, \textbf{G-Meta} \cite{huang2020graph}, \textbf{Meta-GPS} \cite{liu2022few}, \textbf{X-FNC} \cite{wang2023few}, \textbf{COSMIC} \cite{wang2023contrastive}, \textbf{TLP} \cite{tan2022transductive}, and \textbf{TEG} \cite{kim2023task}. %\textit{Graph prompting} methods contain \textbf{GraphPrompt} \cite{liu2023graphprompt}, and \textbf{GPF} \cite{fang2024universal}. 
The descriptions and implementations of these baselines are provided in \textbf{Appendix} \ref{baseline}. 

%\paragraph{Implementation Details.} 
\noindent \textbf{Implementation Details.} In the \textit{node representation learning} stage, we adopt 2-layer SGC with 16 hidden units. In the \textit{dual-level mixup} stage, we uniformly set the two parameters involved in the beta distribution to 0.5, \textit{i.e.}, $\eta\!=\!\gamma\!=\!0.5$. Moreover, in within-task mixup, we generate the same number of nodes as the original support and query set in each meta-training task by default, that is, $n_{s^\prime}\!=\!n_s$, $n_{q^\prime}=n_q$. In across-task mixup, we generate as many interpolated tasks as original tasks, that is $\mathrm{T}_{aug}\!=\!\mathrm{T}_{org}$. %For the graph meta-learning baselines, we use the hyperparameters recommended in the original papers. For traditional meta-learning models, we conduct careful hyperparameter search and report their optimal performance. Particularly, 
In the cross-domain setting, we meta-train the model on one source domain and then meta-test it on another target domain. To ensure fair comparison, we perform meta-training on the same sampled tasks for all models. %For those meta-learning with fewer tasks baselines, we uniformly use SGC as the graph encoder. 
Moreover, we evaluate the performance of our model using the average accuracy (Acc) and macro-F1 (F1) score across 50 randomly selected meta-testing tasks.