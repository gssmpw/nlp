\section{Related Work}
\subsection{Few-shot Learning}
%\paragraph{Few-shot Learning.}
%\noindent \textbf{Few-shot Learning.} 
Few-shot learning aims to quickly adapt meta-knowledge acquired from previous tasks to novel tasks with only a small number of labeled samples, thereby enabling few-shot generalization of machine learning algorithms \cite{finn2017model, finn2019online, hospedales2021meta}. %Initially, it was widely applied in the fields of computer vision and natural language processing to address real-world problems. 
Typically, there are three main strategies to solve few-shot learning. Some methods \cite{vinyals2016matching, snell2017prototypical, mishra2017simple} utilize prior knowledge to constrain the hypothesis space at the \textit{model level}, learning a reliable model within the resulting smaller hypothesis space. A series of methods \cite{finn2018probabilistic, flennerhag2019meta, grant2018recasting, rusu2018meta, oh2020boil} improve the search strategy at the \textit{algorithm level} by providing good initialization or guiding the search steps. Another line of works \cite{hariharan2017low, xu2021augnlg, yao2021improving} augment tasks at the \textit{data level} to obtain precise empirical risk minimizers. For the few-shot learning with limited tasks, there are various explorations in Euclidean data, such as images and texts. For example, TAML \cite{jamal2019task} and MR-MAML \cite{yin2019meta} directly apply regularization on the few-shot learner to reduce their reliance on the number of tasks. Meta-aug \cite{rajendran2020meta} and MetaMix \cite{yao2021improving} perform data augmentation on individual tasks to enrich the data distribution. MLTI \cite{yao2021meta} and Meta-Inter \cite{lee2022set} directly generate source tasks to densify the task distribution. 
%MetaMix linearly combines features and labels from samples in the support and query sets to augment meta-training tasks with more data. MLTI proposes generating new tasks through interpolation between existing meta-training tasks to alleviate the problem of insufficient tasks. Modulation uses neural networks to adjust batch normalization parameters during meta-training to increase task density, while modifying parameters at various network levels to enhance task diversity.

%Our approach differs from the aforementioned methods in that we simultaneously perform within-task and across-task interpolation, with each strategy playing a crucial role. Meanwhile, we integrated task prototypes into the mixup process and explicitly adopt those previously overlooked original tasks, resulting in superior performance, which can be supported by the results in \textbf{Appendices} \ref{alternative_mixup} and \ref{ori_task}. Moreover, previous methods are not applicable to graph-structured data, whereas SMILE introduces a strategy that leverages the prior information provided by the graph. %Additionally, while those methods are not applicable to graph-structured data, we directly leverage the prior information in the graph by considering node degrees.

\subsection{Graph Few-shot Learning}
%\paragraph{Graph Few-shot Learning.}
%\noindent \textbf{Graph Few-shot Learning.}
Inspired by the success of few-shot learning in computer vision \cite{lifchitz2019dense, bateni2020improved, tian2020rethinking} and natural language processing \cite{mukherjee2020uncertainty, bao2019few, wang2021grad2task}, few-shot learning on graphs has recently seen significant development \cite{liu2022few, wang2022task, wang2023few, tan2023virtual, liu2024meta, liu2024simple, liu2025enhancing}. %Few-shot node classification is a fundamental and widely popular task aimed at training a graph model to rapidly identify novel classes given a limited number of labeled nodes. %The task of few-shot node classification is the most popular, aiming to train a graph model to rapidly identify new classes given a limited number of labeled nodes.
The core concept of current mainstream methods is to develop complicated algorithms to address the problem of few-shot learning on graphs. For instance, Meta-GNN \cite{zhou2019meta}, G-Meta \cite{huang2020graph}, and Meta-GPS \cite{liu2022few} are all subjected to specific modifications based on the MAML \cite{finn2017model} algorithm, employing a bi-level optimization strategy to learn better parameter initialization. %Despite fruitful success, the high complexity of these models severely hinder their further improvements. 
While the above models yield fruitful results, their reliance on substantial and diverse of meta-training tasks, coupled with their high complexity, has impeded their further advancement. %Additionally, these models have not altered the data distribution, and there
Recently, TLP \cite{tan2022transductive} and TEG \cite{kim2023task} attempt to alleviate the limited diversity in meta-training datasets by using graph contrastive learning and equivariant neural networks, respectively. %Moreover, some proposed graph prompting methods \cite{liu2023graphprompt, fang2024universal}, by harnessing the power of prompting techniques, have emerged as effective methods under the scarcity of labeled nodes. 
With the aid of sophisticated network designs, these methods have yielded promising results in graph few-shot scenarios. %Benefiting from sophisticated network designs, the above methods have achieved promising results in graph few-shot scenarios. 
However, there is little effort to address the graph few-shot learning problem from the perspective of data augmentation.