\section{Theoretical Analysis}
\label{theorem_section}
In this section, we theoretically analyze why our proposed SMILE, equipped with intra-task and inter-task mixup, can alleviate overfitting and exhibit better generalization capabilities. We first present the obtained key points: \textit{SMILE can regularize the weight parameters in a distribution-dependent manner and reduce the upper bound of the generalization gap by controlling the Rademacher complexity.} Next, we elaborate on the proposed theorems to support the aforementioned points. %For a better analysis of Smile's behavior and for
For simplicity, we conduct a detailed theoretical analysis of SMILE in the binary classification scenario, assuming the use of preprocessed centralized dataset that satisfies the condition $\mathbb E_{\mathrm{X}}[\mathrm{X}]\!=\!0$. Moreover, the proposed SMILE can be modeled as $f_\theta(\mathrm{Z})\!=\!\theta^\top g_\zeta(\mathrm{Z})\!=\!\theta^\top \mathrm{X}$, where $g_\zeta(\cdot)$ denotes the graph encoder parameterized by $\zeta$. We consider using the loss from Eq.\ref{proto} for tasks in $\mathcal{D}_{all}$.
Particularly, the empirical loss function based on enriched training samples for binary classification can be simplified as: 
\begin{equation}
\label{d_loss}
\begin{aligned}
    &\mathcal{L}(\mathcal{D}_{all};\theta)\!=\!\sum_{t=1}^\mathrm{T}\sum_{i=1}^m(1\!+\!\exp(\langle\mathrm{X}_{t,i}^q\!-\!(\mathrm{C}_1\!+\!\mathrm{C}_2)/2,\theta\rangle))^{-1}, \\
    &\mathrm{C}_k\!=\!\frac{1}{|\mathcal{S}_{t,k}|}\sum_{(\mathrm{X}_{t,i}^s,\mathrm{Y}_{t,i}^s)\in\mathcal{S}_{t}}\mathbb{I}_{\mathrm{Y}_{t,i}=k}\mathrm{X}_{t,i}^s,
\end{aligned}
\end{equation}
where $\langle\cdot,\cdot\rangle$ denote the dot product operation. The approximation of the loss function $\mathcal{L}(\mathcal{D}_{all};\theta)$ in Eq.\ref{d_loss} is formalized as:
\begin{equation}
    \label{loss_appro}\mathcal{L}(\mathcal{D}_{all};\theta)\approx
    \mathcal{L}(\mathcal{D}_{org};\theta)\!+\!\mathcal{L}(\bar{\lambda}\mathcal{D}_{org};\theta)\!+\!\mathcal{M}(\theta),
\end{equation}
where $\bar{\lambda}\!=\!\mathbb E_{\rho_\lambda}[\lambda]$ and $\mathcal{M}(\theta)$ is a quadratic regularization term with respect to $\theta$, defined as: 
\begin{equation}
\label{m_theta}
\begin{aligned}
    \mathcal{M}(\theta)\!=\!\mathbb E_{\mathcal{T}_t\sim p(\mathcal{T})}\mathbb E_{(\mathrm{X}_t,\mathrm{Y}_t)\sim q(\mathcal{T}_t)} \frac{\phi(\mathrm{P}_t)(\phi(\mathrm{P}_t)\!-\!0.5)}{2(1\!+\!\exp{(\mathrm{P}_t)})}(\theta^\top\Sigma_\mathrm{X}\theta)
\end{aligned}
\end{equation}
in which $\mathrm
P_t\!=\!\langle\mathrm{X}_t^q \! - \! (\mathrm{C}_1 \! + \! \mathrm{C}_2)/2,\theta\rangle$, $\phi(\mathrm{P}_t)\!=\!\exp(\mathrm{P}_t)/(1\!+\!\exp(\mathrm{P}_t))$, and $\Sigma_\mathrm{X}\!=\!\mathbb E[\mathrm{X}\mathrm{X}^\top]\!=\!\frac{1}{m}\sum\nolimits_{i=1}^m\mathrm{X}_i\mathrm{X}_i^\top$. %The detailed proofs for Eqs.\ref{loss_appro} and \ref{m_theta} can be found in Lemma \ref{lemma_first} of Appendix \ref{proof}.

Eq.\ref{loss_appro} shows that SMILE imposes an additional regularization term on the trainable weights to constrain the solution space, thereby reducing the likelihood of overfitting.

% $\mathcal{L}(\mathcal{D}_{all};\theta)\!=\!\sum_{t=1}^\mathrm{T}\sum_{i=1}^m(1\!+\!\exp(\langle\mathrm{X}_{t,i}^q\!-\!(\mathrm{C}_1\!+\!\mathrm{C}_2)/2,\theta\rangle))^{-1}, \mathrm{C}_k\!=\!\frac{1}{|\mathcal{S}_{t,k}|}\sum_{(\mathrm{X}_{t,i}^s,\mathrm{Y}_{t,i}^s)\in\mathcal{S}_{t}}\mathbb{I}_{\mathrm{Y}_{t,i}=k}\mathrm{X}_{t,i}^s$, where $\langle\cdot,\cdot\rangle$ denote the dot product operation.  
% We provide the Lemma \ref{lemma_regularization} below, which gives the second-order approximation of the loss function $\mathcal{L}(\mathcal{D}_{all};\theta)$.

% \begin{lemma}
% \label{lemma_regularization}
% Suppose the designed model with mixup distribution $\lambda \sim \text{Beta}(\eta,\gamma)$. Let $\rho_\lambda \sim \frac{\eta}{\eta\!+\!\gamma}\text{Beta}(\eta\!+\!1,\gamma)\!+\!\frac{\gamma}{\eta\!+\!\gamma}(\gamma\!+\!1,\eta)$. The second-order approximation of the loss function $\mathcal{L}(\mathcal{D}_{all};\theta)$ is given by,
% \begin{equation}
%     \label{loss_appro}\mathcal{L}(\mathcal{D}_{all};\theta)\approx %\mathcal{L}_{\text{org}}+\mathcal{L}_{\text{aug}}=
%     \mathcal{L}(\mathcal{D}_{org};\theta)\!+\!\mathcal{L}(\bar{\lambda}\mathcal{D}_{org};\theta)\!+\!\mathcal{M}(\theta),%\theta\Sigma_\mathrm{X}\theta^\top
% \end{equation}
% where $\bar{\lambda}\!=\!\mathbb E_{\rho_\lambda}[\lambda]$ and $\mathcal{M}(\theta)$ is a quadratic regularization term with respect to $\theta$, defined as 
% \begin{equation}
% \label{m_theta}
% \begin{aligned}
%     \mathcal{M}(\theta)\!=\!\mathbb E_{\mathcal{T}_t\sim p(\mathcal{T})}\mathbb E_{(\mathrm{X}_t,\mathrm{Y}_t)\sim q(\mathcal{T}_t)} \frac{\phi(\mathrm{P}_t)(\phi(\mathrm{P}_t)\!-\!0.5)}{2(1\!+\!\exp{(\mathrm{P}_t)})}(\theta^\top\Sigma_\mathrm{X}\theta)
% \end{aligned}
% \end{equation}
% %$\mathcal{M}(\theta)=\mathbb E_{\mathcal{T}_t \sim p(\mathcal{T})} \mathbb E_{(\mathrm{X}_t, \mathrm{Y}_t) \sim q(\mathcal{T}_t)} \left[\phi(\langle \mathrm{X}^q_{t}-(\mathrm{C}_1+\mathrm{C}_2)/2,\theta \rangle)\theta^\top\Sigma_\mathrm{X}\theta\right]$. 
% in which $\mathrm
% P_t\!=\!\langle\mathrm{X}_t^q \! - \! (\mathrm{C}_1 \! + \! \mathrm{C}_2)/2,\theta\rangle$, $\phi(\mathrm{P}_t)\!=\!\exp(\mathrm{P}_t)/(1\!+\!\exp(\mathrm{P}_t))$,  and $\Sigma_\mathrm{X}\!=\!\mathbb E[\mathrm{X}\mathrm{X}^\top]\!=\!\frac{1}{m}\sum\nolimits_{i=1}^m\mathrm{X}_i\mathrm{X}_i^\top$.%, and $c$ is a constant.
% \end{lemma}
% Lemma \ref{lemma_regularization} shows that SMILE imposes an additional regularization term on the trainable weights to constrain the solution space, thereby reducing the likelihood of overfitting. 

To define the generalization gap problem formally, we introduce a function class of the dual form related to the regularization term in Eq.\ref{loss_appro}, as shown in Eq.\ref{dual}.
\begin{equation}
\label{dual}
\mathcal{F}_\nu\!=\!\{\mathrm{X}\rightarrow\theta^\top\mathrm{X}:\theta^\top\Sigma_\mathrm{X}\theta\leq\nu\}.
\end{equation}
Moreover, we represent the expected risk $\mathsf R$ and empirical risk $\hat{\mathsf R}$ as follows:
\begin{equation}
    \begin{aligned}
        \mathsf R\!=\!\mathbb E_{\mathcal{T}_i \sim p(\mathcal{T})}\mathbb E_{(\mathrm{X}_j,\mathrm{Y}_j)\sim q(\mathcal{T}_i)}\mathcal{L}(f_\theta(\mathrm{X}_j),\mathrm{Y}_j), \\
        \hat{\mathsf R}\!=\!\mathbb E_{\mathcal{T}_i\sim\hat{p}(\mathcal{T})}\mathbb E_{(\mathrm{X}_j,\mathrm{Y}_j)\sim\hat{q}(\mathcal{T}_i)}\mathcal{L}(f_\theta(\mathrm{X}_j),\mathrm{Y}_j).
    \end{aligned}
\end{equation}
Then, we present the following theorem for improved generalization gap brought by SMILE.
\begin{theorem}
\label{theorem_1}
  Assume that $\mathrm{X}, \mathrm{Y}$ and $\theta$ are bounded. For all $f\!\in\! \mathcal{F}_\nu$, where $\theta$ satisfies $\theta^\top\Sigma_\mathrm{X}\theta\!\leq\! \nu$, we have the following generalization gap bound, with probability at least $(1-\epsilon)$ over the training samples,
  %\begin{equation}
  % \begin{align*}
  %   &|\hat{\mathsf{R}}-\mathsf{R}|\leq 2\left(\sqrt{\frac{\nu\cdot\textrm{rank}(\sum_\mathrm{X})}{m}} + \sqrt{\frac{\nu}{\mathrm{T}}}\left(\left\Vert\Sigma_\mathrm{X}^{\dagger/2}\mu_\mathrm{X}\right\Vert + \\
  %   &\textrm{rank}(\Sigma_\mathrm{X}) \right) \right) + 3\left(\sqrt{\frac{(b-a)^2\log(2/\epsilon)}{2m}}+\sqrt{\frac{(c-d)^2\log(2/\epsilon)}{2\mathrm{T}}}\right)
  % \end{align*}
  %\end{equation}
  \begin{equation}
  \begin{split}
    &|\hat{\mathsf{R}}-\mathsf{R}|\leq 2\Bigg(\sqrt{\frac{\nu\cdot\textrm{rank}(\sum_\mathrm{X})}{m}} \!+\! \sqrt{\frac{\nu}{\mathrm{T}}}\bigg( \textrm{rank}(\Sigma_\mathrm{X}) \bigg) \Bigg) \\ 
    &\!+\!  3\left(\sqrt{\frac{\log(2/\epsilon)}{2m}}\!+\!\sqrt{\frac{\log(2/\epsilon)}{2\mathrm{T}}}\right),
  \end{split}
  \end{equation}
  where $m$ and $\mathrm{T}$ denote the number of nodes in the query set and the number of meta-training tasks.
\end{theorem}
%On the one hand, according to Lemma \ref{lemma_first}, the weight space induced by our method are regularized, leading to a smaller Î¼. On the other hand, the task interpolation and task transfer interpolation increase m and T. These two aspects work together to help reduce the generalization error of the model and alleviate the overfitting problem.

Based on Theorem \ref{theorem_1}, we can obtain several in-depth findings. On the one hand, SMILE induces a regularized weight space for $\theta$, leading to a smaller $\nu$. On the other hand, the introduced intra-task and inter-task interpolations increase $m$ and $\mathrm{T}$ simultaneously. These two aspects work together to reduce the upper bound of the generalization gap of SMILE and alleviate overfitting.

According to the above theorem, we can naturally confirm the following corollary.

\begin{corollary}
\label{corollary_1}
    Let $|\hat{\mathsf R}\!-\!\mathsf R|$ and $|\hat{\mathsf R}_\mathrm{ori}\!-\!\mathsf R_\mathrm{ori}|$ denote the model generalization bounds trained under our proposed task augmentation strategy and standard training strategy, respectively. %obtained from training using our proposed task augmentation strategy and standard training, respectively. 
    We have the following inequality holding,
    \begin{equation}
    \mathsf U (|\hat{\mathsf R}\!-\!\mathsf R|) \leq \mathsf U (|\hat{\mathsf R}_{\mathrm{ori}}\!-\!\mathsf R_{\mathrm{ori}}|),
    \end{equation}
    where $\mathsf U(\cdot)$ denotes the operation of taking the upper bound.
\end{corollary}

Corollary \ref{corollary_1} suggests that SMILE achieves tight generalization bound than other models trained in a standard way. %the upper bound of generalization gap for models trained with our strategy is often smaller than that for models trained with standard training methods.

Suppose the empirical distribution of source tasks in the meta-training be $\mathbb{\hat{P}}$ %, the empirical distribution of the support set for target tasks in the meta-testing be $\mathbb{\hat{Q}}$, 
and the expected distribution of the query set of target tasks be $\mathbb Q$, then during the adaptation process, we expect to reduce the data-dependent upper bound, defined as $\underset{f\in\mathcal{F}}{\text{sup}}|\mathbb E_{\hat{\mathbb P}} \!-\! \mathbb E_{\mathbb Q}|$.
Empirically, when source tasks and target tasks are more similar, the model is more likely to extract generalizable meta-knowledge from source tasks to quickly adapt to target tasks. Theoretically, we present the Theorem \ref{theorem_2}, which demonstrates the reduction of the upper bound between $\mathbb{\hat{P}}$ and $\mathbb{Q}$ induced by our proposed strategy. %We have the following theorem demonstrating the improvement in  induced by our strategy.
\begin{theorem}
\label{theorem_2}
Assume the source tasks and target tasks are drawn from distribution $\mathbb{\hat{P}}$ and $\mathbb{Q}$, and they are independent. For $\epsilon\!>\!0$, with probability at least $(1\!-\!\epsilon)$ over the draws of samples, we have the following upper bound between data distributions,
    \begin{equation}
    \begin{split}
      \underset{f\in\mathcal{F}}{\mathrm{sup}}|\mathbb{E}_{\hat{\mathbb P} }\!-\!\mathbb E_{\mathbb Q}| \leq \Bigg(2\sqrt{\nu\cdot\text{rank}(\Sigma_{\mathrm{X}})} + \sqrt{\frac{\log(1/\epsilon)}{2}} \Bigg) \left( \sqrt{\frac{1}{m}} \!+\! \sqrt{\frac{1}{n_q}} \right),  
    \end{split}
    \end{equation}
    where $n_q$ denotes the number of nodes in the query set of the meta-testing task.
\end{theorem}
We can draw the conclusion that the introduction of intra-task interpolation leads to an increase in the value of $m$. Additionally, according to Eq.\ref{loss_appro}, the regularization effect can result in a decrease of $\nu$. Consequently, Theorem \ref{theorem_2} suggests that our method has the capability to diminish the disparity between the distributions of the source task and target task, facilitating the extraction of pertinent knowledge and, in turn, enhancing the model's generalization. %All detailed proofs can be found in \textbf{Appendix} \ref{proof}.