\section{Preliminary}
Given a graph $\mathcal{G}\!=\!\{\mathcal{V},\mathcal{E},\mathrm{Z},\mathrm{A}\}$, $\mathcal{V}$ and $\mathcal{E}$ represent the sets of nodes and edges, respectively. $\mathrm{Z}\!\in\!\mathbb R^{n\times d}$ is the feature matrix of nodes and $\mathrm{A}\!\in\!\mathbb R^{n\times n}$ is the corresponding adjacency matrix. Our model adheres to the prevalent meta-learning training paradigm, which involves training on sampled tasks. %a per-task basis. %Our model is trained using a task-based approach, which is also the most widely adopted method for training meta-models. %The meta-training tasks $\mathcal{D}_{org}\!=\!\{\mathcal{T}_{t}\}_{t=1}^{\mathrm{T}_{org}}$ are sampled from a task distribution $p(\mathcal{T})$, where each task contains a support set $\mathcal{S}_t\!=\!\{(\mathrm{Z}_{t,i}^s,\mathrm{Y}_{t,i}^s)\}_{i=1}^{n_s}$ and a query set $\mathcal{Q}_t\!=\!\{(\mathrm{Z}_{t,i}^q,\mathrm{Y}_{t,i}^q)\}_{i=1}^{n_q}$, denoted as $\mathcal{T}_t\!=\!\{\mathcal{S}_t,\mathcal{Q}_t\}$. Here, $\mathrm{Z}_{t,i}^\ast$ and $\mathrm{Y}_{t,i}^\ast\!\in\!\mathcal{Y}_{tra}$ denote a labeled node and its corresponding label, where $\mathcal{Y}_{tra}$ denotes the set of base classes. For the meta-testing task $\mathcal{T}_{tes}\!=\!\{\mathcal{S}_{tes},\mathcal{Q}_{tes}\}\!=\!\{\{(\mathrm{Z}_{tes,i}^s,\mathrm{Y}_{tes,i}^s)\}_{i=1}^{n_s}, \{(\mathrm{Z}_{tes,i}^q,\mathrm{Y}_{tes,i}^q)\}_{i=1}^{n_q}\}$, it is composed in the same way as the meta-training task $\mathcal{T}_t$, with the only difference being that the node label belong to the novel class set $\mathcal{Y}_{tes}$, which is disjoint from $\mathcal{Y}_{tra}$, \textit{i.e.}, $\mathcal{Y}_{tra}\!\cap\!\mathcal{Y}_{tes}\!=\!\emptyset$. When the support set $\mathcal{S}_{tes}$ consists of $N$ sampled classes, each with $K$ nodes, we refer to it as an $N$-way $K$-shot problem. Typically, the model is first trained on the meta-training tasks $\mathcal{D}_{org}$. During the meta-testing stage, the model is fine-tuned on $\mathcal{S}_{tes}$ and then is evaluated the performance on $\mathcal{Q}_{tes}$. 
In this work, we mainly focus on few-shot node classification, which is the most prevailing and representative task in graph few-shot learning. Moreover, we highlight that in our focused scenarios, the number of available meta-training tasks sampled from an unknown task distribution is extremely small compared to traditional experimental settings, referred to as \textit{few-shot node classification with fewer tasks}. Our goal is to enable the model to effectively extract meta-knowledge even from such limited tasks, which can generalize to novel tasks in the meta-testing phase. %For better understanding, we summarize the main symbols of this work in \textbf{Appendix} \ref{symbol}.


% $\mathcal{D}_{org}=\{\mathcal{T}_{t}\}_{t=1}^{\mathrm{T}_{org}}, \mathcal{T}_t=\{\mathcal{S}_t,\mathcal{Q}_t\}$, 

% $\mathcal{S}_t=\{(\mathrm{X}_{t,i}^s,\mathrm{Y}_{t,i}^s)\}_{i=1}^{n_s}$, 
% $\mathcal{Q}_t=\{(\mathrm{X}_{t,i}^q,\mathrm{Y}_{t,i}^q)\}_{i=1}^{n_q}$, 

% $\mathcal{S}_t^\prime=\{(\mathrm{X}_{t,i}^{\prime s},\mathrm{Y}_{t,i}^{\prime s})\}_{i=1}^{n_{s^\prime}}$, 
% $\mathcal{Q}_t^\prime=\{(\mathrm{X}_{t,i}^{\prime q},\mathrm{Y}_{t,i}^{\prime q})\}_{i=1}^{n_{q^\prime}}$,

% $\mathcal{T}_t=\{\mathcal{S}_t\cup\mathcal{S}_t^\prime,\mathcal{Q}_t\cup\mathcal{Q}_t^\prime\}$

% $\mathcal{D}_{all}=\mathcal{D}_{org}\cup\mathcal{D}_{aug}$, $\mathcal{D}_{aug}=\{\mathcal{T}_t^{aug}\}_{t=1}^{\mathrm{T}_{aug}}, \mathrm{T}=\mathrm{T}_{org}+\mathrm{T}_{aug}$

%\paragraph{Problem Statement.}