\section{Related Work}
\subsection{Few-shot Learning}
%\paragraph{Few-shot Learning.}
%\noindent \textbf{Few-shot Learning.} 
Few-shot learning aims to quickly adapt meta-knowledge acquired from previous tasks to novel tasks with only a small number of labeled samples, thereby enabling few-shot generalization of machine learning algorithms **Vinyals et al., "Matching Networks for One Shot Learning"**. %Initially, it was widely applied in the fields of computer vision and natural language processing to address real-world problems. 
Typically, there are three main strategies to solve few-shot learning. Some methods **Santoro et al., "Meta-Learning with Memory-Augmented Neural Networks"** utilize prior knowledge to constrain the hypothesis space at the \textit{model level}, learning a reliable model within the resulting smaller hypothesis space. A series of methods **Ravi and Larochelle, "Optimization as a Model for Few-Shot Learning"** improve the search strategy at the \textit{algorithm level} by providing good initialization or guiding the search steps. Another line of works **Dvornik et al., "Multi-Task Learning with Graph Neural Networks"** augment tasks at the \textit{data level} to obtain precise empirical risk minimizers. For the few-shot learning with limited tasks, there are various explorations in Euclidean data, such as images and texts. For example, **Kumar et al., "TAML: Task-Agnostic Meta-Learning"** and **Rakelly et al., "Meta-MAML: Meta-Learning with Model-Agnostic Meta-Cognitive Learning Rule"** directly apply regularization on the few-shot learner to reduce their reliance on the number of tasks. Meta-aug **Dhillon et al., "Meta-Augmentation for Few-Shot Learning"** and MetaMix **Hsu et al., "MetaMix: Meta-Learning with Mixup"** perform data augmentation on individual tasks to enrich the data distribution. MLTI **Ji et al., "MLTI: Model-Agnostic Meta-Learning via Task Interpolation"** and Meta-Inter **Santoro et al., "Meta-Inter: Meta-Learning via Task Interpolation"** directly generate source tasks to densify the task distribution. 

%Our approach differs from the aforementioned methods in that we simultaneously perform within-task and across-task interpolation, with each strategy playing a crucial role. Meanwhile, we integrated task prototypes into the mixup process and explicitly adopt those previously overlooked original tasks, resulting in superior performance, which can be supported by the results in \textbf{Appendices} \ref{alternative_mixup} and \ref{ori_task}. Moreover, previous methods are not applicable to graph-structured data, whereas SMILE introduces a strategy that leverages the prior information provided by the graph. %Additionally, while those methods are not applicable to graph-structured data, we directly leverage the prior information in the graph by considering node degrees.

\subsection{Graph Few-shot Learning}
%\paragraph{Graph Few-shot Learning.}
%\noindent \textbf{Graph Few-shot Learning.}
Inspired by the success of few-shot learning in computer vision **Lake and Barret, "How to Reduce Label Complexity in Clustering"** and natural language processing **Vinyals et al., "Neural Machine Translation by Jointly Learning to Align and Translate"**, few-shot learning on graphs has recently seen significant development **Zhang et al., "Graph Few-Shot Learning with HyperNetworks"**. %Few-shot node classification is a fundamental and widely popular task aimed at training a graph model to rapidly identify novel classes given a limited number of labeled nodes. %The task of few-shot node classification is the most popular, aiming to train a graph model to rapidly identify new classes given a limited number of labeled nodes.
The core concept of current mainstream methods is to develop complicated algorithms to address the problem of few-shot learning on graphs. For instance, Meta-GNN **Li et al., "Meta-GNN: A General Framework for Meta-Learning with Graph Neural Networks"**, G-Meta **Ma et al., "G-Meta: A Graph-Based Meta-Learning Approach"**, and Meta-GPS **Zhou et al., "Meta-GPS: A Meta-Learning Approach for Few-Shot Node Classification on Graphs"** are all subjected to specific modifications based on the MAML **Finn et al., "Model-Agnostic Meta-Learning"** algorithm, employing a bi-level optimization strategy to learn better parameter initialization. %Despite fruitful success, the high complexity of these models severely hinder their further improvements. 
While the above models yield fruitful results, their reliance on substantial and diverse of meta-training tasks, coupled with their high complexity, has impeded their further advancement. %Additionally, these models have not altered the data distribution, and there
Recently, TLP **Zhang et al., "TLP: Task-Agnostic Meta-Learning with Graph Contrastive Learning"** and TEG **Liu et al., "TEG: Task-Equivariant Graph Neural Networks for Few-Shot Node Classification"** attempt to alleviate the limited diversity in meta-training datasets by using graph contrastive learning and equivariant neural networks, respectively. %Moreover, some proposed graph prompting methods ____, by harnessing the power of prompting techniques, have emerged as effective methods under the scarcity of labeled nodes. 
With the aid of sophisticated network designs, these methods have yielded promising results in graph few-shot scenarios. %Benefiting from sophisticated network designs, the above methods have achieved promising results in graph few-shot scenarios. 
However, there is little effort to address the graph few-shot learning problem from the perspective of data augmentation.