%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Shape derivative of a functional of a region supporting the boundary conditions of the conductivity equation} \label{sec.optbcconduc}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\noindent This section takes place in the physical setting of electrostatics, governed by the conductivity equation, where the salient points of our investigations can be presented with a minimum amount of technicality.
After introducing the physical problem at stake in \cref{sec.setconduc} and summarizing a few facts about geodesic signed distance functions in \cref{sec.distmanifold}, we focus in \cref{sec.hepsconduc} on the case of a functional depending on the region $G \subset \partial \Omega$ supporting a homogeneous Dirichlet boundary condition. We notably explain
why ``exact'' shape derivatives with respect to a transition zone between homogeneous Dirichlet and Neumann boundary conditions are difficult to handle, and we propose a suitable approximate version of these derivatives. In \cref{sec.approxSD}, we provide practical, implementation friendly versions of these formulas. 
A few extensions of this material are discussed in \cref{sec.extsdconduc}, including the simpler treatment of a functional of the region of $\partial \Omega$ bearing inhomogeneous Neumann boundary conditions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{A brief presentation of the conductivity equation}\label{sec.setconduc}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\noindent Let $\Omega \subset \R^d$ be a smooth bounded domain, whose boundary $\partial \Omega$ is made of three disjoint parts: 
$$ \partial \Omega = \overline{\Gamma_D} \cup \overline{\Gamma_N} \cup \overline{\Gamma} \text{ such that } \overline{\Gamma_D} \cap \overline{\Gamma_N} = \emptyset.$$
We assume that:
\begin{itemize}
\item The voltage potential $u$ is set to $0$ on $\Gamma_D$; 
\item A smooth electric current $g:\R^d \to \R$ is applied on $\Gamma_N$; 
\item The region $\Gamma$ is insulated from the outside.
\end{itemize}
The domain $\Omega$ is filled by a material with smooth conductivity $\gamma \in \calC^\infty(\overline\Omega)$, which is uniformly bounded away from $0$ and $\infty$, i.e. 
\begin{equation}\label{eq.bdgamma}
 \exists \: 0 < \gamma_- \leq \gamma_+ < \infty \: \text{ s.t. } \:\forall  x \in \Omega, \quad \gamma_- \leq \gamma(x) \leq \gamma_+.
 \end{equation}
In this situation, the voltage potential $u$ induced by a smooth source $f : \R^d \to \R$ belongs to the space 
$$ H^1_{\Gamma_D}(\Omega) := \left\{ u \in H^1(\Omega), \:\: u =0 \text{ on } \Gamma_D \right\}, $$
and it is the unique solution in the latter to the following boundary value problem:
\begin{equation}\label{eq.conduc}
\left\{
\begin{array}{cl}
-\dv(\gamma \nabla u) = f & \text{in } \Omega, \\
u = 0 & \text{on } \Gamma_D, \\
\gamma \frac{\partial u}{\partial n} = g & \text{on }\Gamma_N, \\[0.2em]
\gamma \frac{\partial u}{\partial n} = 0 & \text{on } \Gamma.
\end{array}
\right.
\end{equation} 

The performance of the configuration is evaluated in terms of the quantity 
\begin{equation}\label{eq.defJ}
 \int_\Omega j(u) \:\d x,
\end{equation}
where $j \in \calC^2 (\mathbb{R}^d)$ is a given function, satisfying the following growth conditions:
\begin{equation} \label{eq.jgrowth}
\exists \: C >0 \text{ s.t. }    \forall t \in \mathbb{R}^d, \: \quad \lvert  j(t) \lvert  \: \leq \: C (1 + \lvert t \lvert ^2), \: \lvert  j^\prime(t) \lvert  \: \leq \: C (1 + \lvert t \lvert), \text{ and } \lvert j^{\prime\prime}(t)\lvert \: \leq \: C .
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Preliminaries about the geodesic signed distance function in $\partial \Omega$} \label{sec.distmanifold}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\noindent In this section we briefly recall some material related to the signed distance function to a 
subset $G$ of the boundary of the smooth ambient domain $\Omega$. We refer to \cite{dapogny2020optimization} and the contributions mentioned therein for more  details.\par\medskip 

We first recall the notion of geodesic distance between points on $\partial \Omega$, see e.g. \cite{do2016differential}.

\begin{definition}
Let $x , y$ be two points in $\partial \Omega$; the geodesic distance $d^{\partial \Omega}(x,y)$ between $x$ and $y$ is the shortest length of a curve on $\partial \Omega$ with endpoints $x$, $y$: 
$$
d^{\partial \Omega}(x,y) = \inf \left\{ \int_0^1 |\gamma^\prime(t)| \:\d t \: : \: \gamma \in \calC^1([0,1],\partial\Omega), \: \gamma(0) = x, \: \gamma(1) = y\right\}.
$$
\end{definition}

\begin{definition} \label{def.SignedDistance}
Let $G$ be a Lipschitz open subset of $\partial \Omega$ with boundary $\Sigma = \partial G$. 
    The geodesic signed distance function $d^{\partial \Omega}_G$ to $G$ is:
    \begin{equation*}
        \forall x \in \partial \Omega, \quad d^{\partial\Omega}_G(x) =
        \left\{
        \begin{aligned}
            -d^{\partial \Omega}(x, \Sigma) \quad & \text{if } x \in G,\\
            0 \quad & \text{if } x \in \Sigma,\\
            d^{\partial \Omega}(x, \Sigma) \quad & \text{if } x \in \partial \Omega \backslash \overline{G},
        \end{aligned}
        \right.
    \end{equation*}
where $d^{\partial \Omega}(x,\Sigma)$ is the geodesic distance from $x$ to the compact subset $\Sigma \subset \partial \Omega$: 
\begin{equation}\label{eq.dSigma}
d^{\partial \Omega}(x,\Sigma) = \min\limits_{y \in \Sigma} d^{\partial \Omega}(x,y). 
\end{equation}
\end{definition}

\begin{definition} \label{def.SignedDistance.Projection}
Let $y$ be a point on $\partial \Omega$; 
\begin{enumerate}
\item The set $\Pi_\Sigma^{\partial \Omega}(y)$ of projections of $y$ onto $\Sigma$ is the set of minimizers in \cref{eq.dSigma}. 
\item When $\Pi_\Sigma^{\partial \Omega}(y)$ is a singleton, its unique element $\pi_\Sigma(y)$ is called the projection of $y$ onto $\Sigma$.
\end{enumerate}
\end{definition}

Let us next recall the definitions of the exponential and logarithm mappings on the hypersurface $\partial\Omega$.
\begin{propdef}
Let $x$ be a point on $\partial \Omega$. The exponential at $x$ is the mapping $\exp_x$, defined from a neighborhood $U$ of $0$ in $T_x \partial \Omega$ into a neighborhood $V$ of $x$ in $\partial \Omega$ by: 
$$\forall v \in U, \quad \exp_x(v) = \gamma_v(1),$$
where $t \mapsto \gamma_v(t)$ is the unique geodesic curve passing through $x$ at $t=0$ with velocity $\gamma_v^\prime(0) = v$.

Up to shrinking $U$ and $V$, $\exp_x$ is a smooth diffeomorphism from $U$ onto $V$, whose inverse mapping is denoted by $\log_x : V \to U$: 
$$ \forall y \in V, \:\: \log_x(y) \text{ is the unique vector } v \in U \subset T_x \partial \Omega \text{ s.t. } \exp_x(v) = y.$$
\end{propdef}

The following result states that when the region $G$ is sufficiently smooth, there exists a tubular neighborhood of the boundary $\Sigma$ with small width $2\delta$ which is diffeomorphic to the product set $\Sigma \times (-\delta, \delta)$. It follows from a simple adaptation of the proof of Th. 3.1 in \cite{ambrosio1994level}, which is based on the Implicit Function Theorem, see \cref{fig.illusdist} for an illustration.

\begin{theorem} \label{theorem.SignedDistance.ExpMapDiffeo}
Let $G$ be a smooth open subset of $\partial \Omega$, with boundary $\Sigma = \partial G$. 
Then, there exists a number $\delta > 0$ such that the mapping $F : \Sigma \times (-\delta, \delta) \to \partial \Omega$ defined by:
$$
\forall x \in \Sigma, \: t \in (-\delta, \delta), \quad F(x,t) = \exp_x (t n_\Sigma(x))
$$
is a smooth diffeomorphism onto its image $U_\delta:= F(\Sigma \times (-\delta,\delta)) \subset \partial \Omega$. Its inverse reads:
$$
\forall y \in U_\delta, \quad F^{-1}(y) = (\pi_\Sigma(y), d_G^{\partial\Omega}(y)).
$$
\end{theorem}

Let us now provide a handful Fubini-like formula for rewriting integrals posed on the tubular neighborhood $U_\delta$ introduced in \cref{theorem.SignedDistance.ExpMapDiffeo} as nested integrals over $\Sigma$ and the interval $(-\delta,\delta) \subset \R$. This result follows from the change of variables formula on a manifold, see e.g. Chap. XVI in \cite{lang2012fundamentals}: 

\begin{proposition}\label{prop.changevar}
Let $G$ be a smooth open subset of $\partial \Omega$, let $\delta >0$ be small enough so that the conclusion of \cref{theorem.SignedDistance.ExpMapDiffeo} holds true, and let $U_\delta$ be the tubular neighborhood of $\Sigma$ introduced in there. Then, a function $\varphi : U_\delta \to \R$ is integrable if and only if the composite function $\varphi \circ F$ is integrable on $\Sigma \times (-\delta,\delta)$, and:
$$ \int_{U_\delta} \varphi \:\d s = \int_\Sigma \int_{-\delta}^{\delta} \lvert \det \nabla F(x,t)\lvert \:\varphi \circ F(x,t) \:\d t \:\d \ell(x).$$
Here, the definition of the $(d-1) \times (d-1)$ matrix  $\nabla F(x,t)$ involves an arbitrary local orthonormal basis $(\tau_1,\ldots,\tau_{d-2})$ 
of tangent vectors to the codimension $1$ submanifold $\Sigma$ of $\partial \Omega$. This matrix gathers the components of the tangent vectors
$$ \frac{\partial F}{\partial \tau_1}(x,t) , \ldots,  \frac{\partial F}{\partial \tau_{d-2}}(x,t) , \frac{\partial F}{\partial t}(x,t)   \ \in T_{F(x,t)} \partial \Omega,  $$
in an arbitrary orthonormal basis of $T_{F(x,t)} \partial \Omega$.
\end{proposition}

\begin{remark}\label{rem.Fx0}
For further reference, we note the following expression of the determinant $\det\nabla F(x,t)$ when $t= 0$: 
$$ \forall x \in \Sigma, \quad \det\nabla F(x,0) = 1. $$
\end{remark}

The next result deals with the derivative of the signed distance function.

\begin{proposition} \label{prop.SignedDistance.Gradient}
    Let $G \subset \partial \Omega$ be a smooth region with boundary $\Sigma$ and let $\delta >0$ and $U_\delta$ be as in the statement of \cref{theorem.SignedDistance.ExpMapDiffeo}.
    Then the signed distance function $d_G^{\partial\Omega}$ is differentiable at any point $x \in U_\delta$, and its tangential gradient reads:
    \begin{equation}
        \nabla_{\partial \Omega} \: d^{\partial \Omega}_G(x) = -\dfrac{1}{d^{\partial \Omega}_G(x)} \log_x (\pi_\Sigma(x)) .
    \end{equation}
\end{proposition}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/illusdist}
    \caption{\it Illustration of some of the definitions related to the geodesic signed distance function introduced in \cref{sec.distmanifold}.}
    \label{fig.illusdist}
\end{figure}


Eventually, we shall require the expressions of the derivative of the signed distance function $d_G^{\partial \Omega}$ with respect to variations of the region $G$. The following result deals with the ``Lagrangian'' version of this derivative, that is, the derivative of the mapping 
\begin{equation}\label{eq.transmap}
\theta \mapsto d_{G_\theta}^{\partial\Omega_\theta}(y+\theta(y)), \quad y \in \partial\Omega,
\end{equation}
evaluating the signed distance function to the perturbed region $G_\theta$ at the perturbed location $y+\theta(y)$ of a given point $y \in \partial\Omega$.


\begin{proposition} \label{prop.SignedDistance.ShapeDerivative}
Let $G$ be a smooth open subset of $\partial\Omega$ with boundary $\Sigma$, and let $U_\delta$ be a tubular neighborhood of $\Sigma$ supplied by \cref{theorem.SignedDistance.ExpMapDiffeo}. Let $y$ be a fixed point in $ U_\delta \backslash \overline{\Sigma}$ and let $x := \pi_\Sigma(y) \in \Sigma$. For any smooth vector field $\theta$, we define:
\begin{equation*}
D(\theta) := d^{\partial \Omega_\theta}_{G_\theta} (y + \theta(y)).
\end{equation*}
Then $D(\theta)$ is Fr\'echet differentiable at $\theta = 0$ and its derivative reads, for a smooth tangential deformation $\theta$:
\begin{equation*}
    D^\prime(0)(\theta) = -\theta(y) \cdot \dfrac{\log_y (x)}{d^{\partial \Omega}_G(y)} - \theta(x) \cdot n_\Sigma(x) .
\end{equation*}
\end{proposition}

\begin{remark}\label{rem.EulDersdf}
The combination of \cref{prop.SignedDistance.ShapeDerivative} with \cref{prop.SignedDistance.Gradient} allows to compute the ``Eulerian'' version of the shape derivative of $G \mapsto d_G^{\partial\Omega}$, i.e. the derivative of the mapping $\theta \mapsto d_{G_\theta}^{\partial\Omega_\theta}(y)$ for a fixed point $y \in \partial\Omega$. Note that this mapping is not well-defined since the point $y$ does not a priori belong to the perturbed boundary $\partial\Omega_\theta$, even when $\theta$ is ``small''. However, it is customary to define this ``Eulerian'' derivative by the following formula, for any tangential deformation $\theta$:
 $$\begin{array}{>{\displaystyle}cc>{\displaystyle}l}
 \left(d^{\partial\Omega}_{G} \right)^\prime(\theta)(y) &:=& D^\prime(0)(\theta) - \nabla_{\partial \Omega} d_G^{\partial \Omega}(y) \cdot \theta(y)\\[0.5em]
 &=& - \theta(x) \cdot n_\Sigma(x) ,
 \end{array}$$
suggested by the (formal) application of the chain rule to \cref{eq.transmap}.
We refer to \cite{allaire2020survey,allaire2007conception,azegami2020shape} about the concepts of ``Lagrangian'' and ``Eulerian'' derivatives for functions defined on the domain.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{A smoothed approximation method for the optimization of the Dirichlet--Neumann transition}\label{sec.hepsconduc}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
\noindent In this section, we focus on the situation where the performance criterion \cref{eq.defJ} is optimized with respect to the shape of the region $\Gamma_D$ supporting the homogeneous Dirichlet conditions in the boundary value problem \cref{eq.conduc}. In the language of \cref{sec.Preliminaries}, this amounts to setting $G = \Gamma_D$, $\Sigma=\Sigma_D := \partial \Gamma_D$ and to considering the objective functional 
\begin{equation}\label{eq.JGsdcalc}
J(\Gamma_D) = \int_\Omega j(u_{\Gamma_D}) \:\d x,
\end{equation}
where $j$ satisfies \cref{eq.jgrowth} and we have denoted by $u_{\Gamma_D}$ the solution to \cref{eq.conduc} in this situation. 
Since in the present context the region $\Gamma_N \subset \partial \Omega$ is not subject to optimization, the set $\Theta_{\Gamma_D}$ of deformations considered in the practice of the method of Hadamard is: 
\begin{equation}\label{eq.ThetaG}
\Theta_{\Gamma_D} := \Big\{ \theta : \R^d \to \R^d \text{ is smooth, with } \theta \cdot n = 0 \text{ on } \partial \Omega \text{ and } \theta = 0 \ \text{on} \ \Gamma_N \Big\} ,
\end{equation}
see \cref{rem.TG}. 

Computing the shape derivative of the functional $J(\Gamma_D)$ in \cref{eq.JGsdcalc} raises multiple difficulties, which are essentially due to the weakly singular behavior of $u_{\Gamma_D}$ at the points $x \in \Sigma_D$ 
marking the transition between homogeneous Dirichlet and homogeneous Neumann boundary conditions in \cref{eq.conduc}. 
Without entering into details, the variational Lax-Milgram theory guarantees that $u_{\Gamma_D}$ belongs to $H^1(\Omega)$. Under our regularity assumptions on $\Omega$ and $\gamma$, this function is actually smooth, except in the vicinity of $\Sigma_D$, where it does not enjoy $H^2$ regularity. 
Loosely speaking, this phenomenon is caused by the existence of non trivial ``weakly singular'' solutions to the homogeneous version of \cref{eq.conduc}, that belong to $H^1(\Omega)$, but not to $H^2(\Omega)$, see e.g. \cite{dauge2006elliptic,grisvard2011elliptic,kozlov1997elliptic} for a comprehensive treatment of these questions.

From the theoretical viewpoint, this lack of regularity of $u_{\Gamma_D}$ implies that the calculation of the shape derivative $J^\prime(\Gamma_D)(\theta)$ under the convenient form \cref{eq.ShapeDerivatives.Structure} cannot be conducted by standard techniques, as those exposed in \cite{allaire2020survey,allaire2007conception}. In particular, C\'ea's formal method for the fast computation of shape derivatives (see  \cite{cea1986conception}) leads to an erroneous result as it tacitly rests on the assumption that $u_{\Gamma_D}$ is ``smooth'', see  \cite{dapogny2020optimization,fremiot2001shape} about this point. In addition to being mathematically difficult to calculate, the expression of $J^\prime(\Gamma_D)(\theta)$ is awkward from the numerical viewpoint, as it brings into play the coefficients of a decomposition of $u_{\Gamma_D}$ along the above mentioned weakly singular solutions to the homogeneous version of \cref{eq.conduc}, which cannot be computed in closed form in general.

In order to overcome both issues, we follow the idea presented in \cite{dapogny2020optimization} (see also \cite{lalainamaster}): we trade the weakly singular solution $u_{\Gamma_D}$ to the ``exact'' problem \cref{eq.conduc}, featuring a sharp transition between Dirichlet and Neumann boundary conditions, for the regularized version $ u_{\Gamma_D, \e} $ characterized as the $H^1(\Omega)$ solution to the following boundary value problem:
\begin{equation} \label{eq.ShapeDerivatives.StateApprox}
\left\{
    \begin{array}{cl}
    -\dv(\gamma\nabla u_{\Gamma_D, \e}) = f & \text{in } \Omega,\\
    \gamma  \frac{\partial u_{\Gamma_D, \e} }{\partial n} + h_{\Gamma_D, \e} u_{\Gamma_D, \e} = 0 & \text{on } \Gamma \cup \Gamma_D,\\
   \gamma \frac{\partial u_{\Gamma_D, \e} }{\partial n} = g & \text{on } \Gamma_N.
    \end{array}
\right.
\end{equation}
Here, $h_{\Gamma_D, \e} : \partial \Omega \rightarrow \mathbb{R}$ is the function defined by:
\begin{equation}
    h_{\Gamma_D, \e} (x) = \dfrac{1}{\e} h\left(\dfrac{d^{\partial\Omega}_{\Gamma_D}(x)}{\e}\right),\\
\end{equation}
where $d^{\partial \Omega}_{\Gamma_D}$ is the geodesic signed distance function to $\Gamma_D$ on $\partial \Omega$ (see \cref{sec.distmanifold}), and $h \in C^\infty(\mathbb{R})$ satisfies:
\begin{equation} \label{eq.ShapeDerivatives.BumpFunction}
    0 \leq h \leq 1, \ h \equiv 1 \text{ on } (-\infty, -1), \ h(0) > 0, \text{ and } \ h \equiv 0 \text{ on } [1, \infty).
\end{equation}
Intuitively, as $\e$ gets ``small'', $h_{\Gamma_D,\e}$ takes very large values on $\overline{\Gamma_D}$, where the Robin boundary condition in \cref{eq.ShapeDerivatives.StateApprox} then mimicks a homogeneous Dirichlet boundary condition; on the contrary, $h_{\Gamma_D,\e}$ vanishes ``well inside'' $\Gamma$, and so this condition boils down to a homogeneous Neumann boundary condition, see \cref{fig.heps}. 
The key feature of this approximation procedure is that, since $\Omega$ and $h_{\Gamma_D,\e}$ are smooth, the classical elliptic regularity theory implies that the solution $u_{\Gamma_D, \e}$ to \cref{eq.ShapeDerivatives.StateApprox} is smooth, contrary to $u_{\Gamma_D}$, see e.g. Chap. 9 in \cite{brezis2010functional} or \cite{gilbarg2015elliptic}. 

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/heps}
    \caption{\it Approximation of a ``sharp'' transition between homogeneous Dirichlet and homogeneous Neumann conditions by a smooth Robin boundary condition in \cref{sec.hepsconduc}.}
    \label{fig.heps}
\end{figure}

%Additionally, the weak formulation for the solution $u_{G, \e} \in H^1(\Omega)$ to \cref{eq.ShapeDerivatives.StateApprox} is:
%\begin{equation}
%    \forall v \in H^1(\Omega), \quad \int_{\Omega} \gamma \nabla u_{G, \e} \cdot \nabla v \: \d x +
%    \int_{\Gamma \cup G} h_{G, \e} \: u_{G, \e} v  \: \d s = \int_{\Omega} fv \: \d s + \int_{S} g v \: \d s .
%\end{equation}

We then replace the exact shape functional $J(\Gamma_D)$ in \cref{eq.JGsdcalc} by the following approximate counterpart:
\begin{equation}\label{eq.defJe}
J_\e(\Gamma_D) = \int_\Omega j(u_{\Gamma_D, \e}) \:\d x.
\end{equation}
The shape derivative of this regularized functional can now be calculated by standard means.

\begin{proposition}\label{prop.SDDirtoNeu}
The functional $J_\e(\Gamma_D)$ in \cref{eq.defJe} is shape differentiable at $\Gamma_D$ when deformations $\theta$ are taken in the set $\Theta_{\Gamma_D}$ in \cref{eq.ThetaG}, and its shape derivative reads:
\begin{equation} \label{eq.ShapeDerivatives.SurfaceExpression}
\forall \theta \in \Theta_{\Gamma_D}, \quad    J^\prime_\e(\Gamma_D)(\theta) = -\dfrac{1}{\e^2} \int_{\Gamma \cup \Gamma_D} h'\left( \frac{d^{\partial \Omega}_{\Gamma_D}(x)}{\e} \right) \theta(\pi_{\Sigma_D}(x)) \cdot n_{\Sigma_D}(\pi_{\Sigma_D}(x)) \: u_{\Gamma_D, \e}(x) \: p_{\Gamma_D, \e}(x) \:\d s(x) ,
\end{equation}
where the adjoint state $p_{\Gamma_D,\e}$ is the unique solution in $H^1(\Omega)$ to the following boundary value problem:
\begin{equation} \label{eq.ShapeDerivatives.Adj}
\left\{
    \begin{array}{cl}
    -\dv(\gamma\nabla p_{\Gamma_D, \e}) = -j^\prime(u_{\Gamma_D,\e}) & \text{in } \Omega,\\
    \gamma  \frac{\partial p_{\Gamma_D, \e} }{\partial n} + h_{\Gamma_D, \e} p_{\Gamma_D, \e} = 0 & \text{on } \Gamma \cup \Gamma_D,\\
   \gamma \frac{\partial p_{\Gamma_D, \e} }{\partial n} = 0 & \text{on } \Gamma_N.
    \end{array}
\right.
\end{equation}
\end{proposition}
\begin{proof}[Sketch of the proof]
This result is proved rigorously in \cite{dapogny2020optimization}. Here, we outline a formal and relatively simple calculation which can be readily adapted to various situations. The latter is based on the formal method of C\'ea, whose practice is legitimate in the treatment of the approximate functional $J_\e(\Gamma_D)$, since all the objects featured in its expression (the region $\Gamma_D$, the state function $u_{\Gamma_D,\e}$, etc.) are smooth, see again \cite{allaire2007conception,allaire2020survey,cea1986conception}.

For a given region $\Gamma_D\subset \partial\Omega$, we introduce the Lagrangian $\calL(\Gamma_D,\cdot,\cdot):H^1(\R^d) \times H^1(\R^d) \to \R$ defined by:
$$ \calL(\Gamma_D,u,p) = \int_\Omega j(u) \:\d x + \int_\Omega \gamma \nabla u \cdot \nabla p \:\d x + \int_{\partial \Omega} h_{\Gamma_D,\e} up\:\d s - \int_\Omega f p \:\d x - \int_{\Gamma_N} g p \:\d s.$$
Intuitively, the latter is obtained by replacing the state function $u_{\Gamma_D,\e}$ by a dummy function $u$ in the definition \cref{eq.defJe} of $J_\e(\Gamma_D)$, and then enforcing the constraint that $u=u_{\Gamma_D,\e}$ with a Lagrange multiplier $p$.
By construction, it holds:
\begin{equation}\label{eq.JeGcalL}
\forall p \in H^1(\R^d), \quad J_\e(\Gamma_D) = \calL(\Gamma_D,u_{\Gamma_D,\e},p).
\end{equation}
For a fixed region $\Gamma_D$, we now search for the saddle points $(u,p) \in H^1(\R^d)\times H^1(\R^d)$ of the function $\calL(\Gamma_D,\cdot,\cdot)$, and to this end, we calculate its partial derivatives:
\begin{itemize}
\item The partial derivative $\frac{\partial\calL}{\partial p}(\Gamma_D,u,p)$ equals: 
$$\forall \widehat p \in H^1(\R^d), \quad \frac{\partial\calL}{\partial p}(\Gamma_D,u,p)(\widehat p) = \int_\Omega \gamma \nabla u \cdot \nabla \widehat p \:\d x + \int_{\partial \Omega} h_{\Gamma_D,\e} u \widehat p \:\d s - \int_\Omega f \widehat p \:\d x - \int_{\Gamma_N} g \widehat p \:\d s .$$
\item The partial derivative $\frac{\partial\calL}{\partial u}(\Gamma_D,u,p)$ reads: 
$$\forall \widehat u \in H^1(\R^d),\quad\frac{\partial\calL}{\partial u}(\Gamma_D,u,p)(\widehat u) =  \int_\Omega j^\prime(u) \widehat u \:\d x + \int_\Omega \gamma \nabla \widehat u \cdot \nabla p \:\d x + \int_{\partial \Omega} h_{\Gamma_D,\e} \widehat u p \:\d s.$$
\end{itemize}
The condition that the derivative $\frac{\partial\calL}{\partial p}(\Gamma_D,u,p)(\widehat p)$ vanish for arbitrary variations $\widehat p \in H^1(\R^d)$ implies that $u = u_{\Gamma_D,\e}$, the solution to \cref{eq.ShapeDerivatives.StateApprox}.
Likewise, since $\frac{\partial\calL}{\partial u}(\Gamma_D,u,p)(\widehat u)$ should vanish for arbitrary variations $\widehat u \in H^1(\R^d)$, we recognize that the function $p$ is the unique solution $p_{\Gamma_D,\e}$ to the adjoint problem \cref{eq.ShapeDerivatives.Adj}.

Now returning to \cref{eq.JeGcalL} and taking derivatives with respect to $\Gamma_D$ in a direction $\theta \in \Theta_{\Gamma_D}$, we obtain from the chain rule that, for an arbitrary function $p \in H^1(\R^d)$:
\begin{equation}\label{eq.JpGammaDtheta}
J^\prime_\e(\Gamma_D)(\theta) = \frac{\partial \calL}{\partial \Gamma_D}(\Gamma_D,u_{\Gamma_D,\e},p)(\theta) + \frac{\partial \calL}{\partial u}(\Gamma_D,u_{\Gamma_D,\e},p) \left( u_{\Gamma_D,\e}^\prime(\theta)\right),
\end{equation}
where $u_{\Gamma_D,\e}^\prime(\theta)$ is the ``Eulerian'' derivative of $\Gamma_D \mapsto u_{\Gamma_D,\e}$, i.e. for each $x \in \Omega$, $u_{\Gamma_D,\e}^\prime(\theta)(x)$ is the derivative of the mapping $\theta \mapsto u_{(\Gamma_D)_\theta,\e}(x)$, defined from a neighborhood of $0$ in $\Theta_{\Gamma_D}$ into $\R$, see \cref{rem.EulDersdf}.
In order to eliminate the second term from the above formula, involving this ``difficult'' quantity, we make the particular choice $p=p_{\Gamma_D,\e}$ in \cref{eq.JpGammaDtheta}. This yields:
$$ J^\prime_\e(\Gamma_D)(\theta) = \frac{\partial \calL}{\partial \Gamma_D}(\Gamma_D,u_{\Gamma_D,\e},p_{\Gamma_D,\e})(\theta),$$
featuring the partial derivative of $\calL$ with respect to its explicit dependence on the region $\Gamma_D$. The latter is now easily calculated thanks to \cref{prop.simplesd,prop.SignedDistance.ShapeDerivative} and \cref{rem.EulDersdf}, thus leading to the desired result. 
\end{proof}

The above replacement procedure of the quantities $J(\Gamma_D)$, $u_{\Gamma_D}$, attached to a sharp transition between homogeneous Dirichlet and Neumann boundary conditions by the smoothed counterparts $J_\e(\Gamma_D)$, $u_{\Gamma_D,\e}$ has been proved to be consistent in our previous work \cite{dapogny2020optimization}. Loosely speaking, under particular assumptions about the geometry of the region $\Sigma_D$, the approximate state $u_{\Gamma_D,\e}$, the approximate objective function $J_\e(\Gamma_D)$ and its shape derivative $J^\prime_\e(\Gamma_D)(\theta)$ converge to their exact counterparts $u_{\Gamma_D}$, $J(\Gamma_D)$ and $J^\prime(\Gamma_D)(\theta)$ as the regularization parameter $\e$ vanishes.

%%%%%%%%%%%%%%%%%%%%%%
\subsection{A practical approximate formula for the regularized shape derivative}\label{sec.approxSD}
%%%%%%%%%%%%%%%%%%%%%%

\noindent However tractable, the numerical evaluation of the approximate shape derivative $J^\prime_\e(\Gamma_D)(\theta)$ in \cref{eq.ShapeDerivatives.SurfaceExpression} still proves difficult. 
Firstly, it requires the calculation of the projection $\pi_{\Sigma_D}(y)$ onto the boundary $\Sigma_D$ of $\Gamma_D$ for each point $y$ in a tubular neighborhood of $\Sigma_D$ -- an issue which is already quite difficult to address with satisfying accuracy in the classical setting of a domain in the Euclidean space, see \cite{feppon2019variational}. Secondly, the formula \cref{eq.ShapeDerivatives.SurfaceExpression} does not conform to the structure \cref{eq.ShapeDerivatives.Structure}, and thus it does not directly provide a descent direction for $J_\e(\Gamma_D)$. Both issues can be alleviated thanks to a subsequent approximation of \cref{eq.ShapeDerivatives.SurfaceExpression} inspired by our previous work \cite{allaire2014multi}, that we now describe.\par\medskip

Since the derivative $h^\prime$ of the smoothing profile $h$ has compact support inside $[-1,1]$, 
the integrand of the formula \cref{eq.ShapeDerivatives.SurfaceExpression} has compact support inside the tubular neighborhood $U_\e = \left\{ x \in \partial \Omega \text{ s.t. }d^{\partial \Omega}(x,\Sigma) < \e \right\}$, and \cref{prop.changevar} yields:
\begin{align*}
    J^\prime_\e(\Gamma_D)(\theta) &= -\dfrac{1}{\e^2} \int_{U_\e}  h'\left(\frac{d^{\partial \Omega}_{\Gamma_D}(y)}{\e}\right) \: \theta(\pi_{\Sigma_D}(y)) \cdot n_{\Sigma_D}(\pi_{\Sigma_D}(y)) \: u_{\Gamma_D, \e} (y) \: p_{\Gamma_D, \e} (y) \: \d s(y)\\
    &= -\dfrac{1}{\e^2} \int_{-\e}^\e \int_{\Sigma_D} h'\left(\frac{t}{\e}\right) \: \lvert \det \nabla F(x,t) \lvert \: \theta(x) \cdot n_{\Sigma_D}(x) \: u_{\Gamma_D, \e} (F(x,t)) \: p_{\Gamma_D, \e} (F(x,t)) \: \d \ell(x) \: \d t.
\end{align*}
Yet another change of variables in the outermost integral yields:
$$
J^\prime_\e(\Gamma_D)(\theta) = -\dfrac{1}{\e} \int_{-1}^1 \int_{\Sigma_D} h^\prime(t) \: \lvert \det \nabla F(x,\e t) \lvert \:  \theta(x) \cdot n_{\Sigma_D}(x) \: u_{\Gamma_D, \e} (F(x,\e t)) \: p_{\Gamma_D, \e} (F(x,\e t)) \: \d \ell(x) \: \d t.
$$
Since  \(u_{\Gamma_D, \e}\) and \(p_{\Gamma_D, \e}\) are smooth, it holds, for $\e > 0$ small enough:
$$\forall x \in \Sigma_D, \:\: t \in (-1,1), \quad u_{\Gamma_D,\e}(F(x,\e t)) \approx u_{\Gamma_D,\e}(F(x,0)) = u_{\Gamma_D,\e}(x),$$
and likewise for $p_{\Gamma_D,\e}$. Hence, we obtain:

\begin{equation*} \label{eq.ShapeDerivatives.ApproxFormula}
\begin{array}{>{\displaystyle}cc>{\displaystyle}l}
    J^\prime_\e(\Gamma_D)(\theta) &\approx&  -\frac{1}{\e} \int_{-1}^1 \int_{\Sigma_D} h^\prime(t) \lvert \det \nabla F(x,0) \lvert  \theta(x) \cdot n_{\Sigma_D}(x) \: u_{\Gamma_D, \e} (x) \: p_{\Gamma_D, \e} (x) \: \d \ell(x) \: \d t \\[1em]
    &=& -\frac{1}{\e} \int_{-1}^1 \int_{\Sigma_D} h^\prime(t) \: \theta(x) \cdot n_{\Sigma_D}(x) \: u_{\Gamma_D, \e} (x) \: p_{\Gamma_D, \e} (x) \: \d \ell(x) \: \d t, \\
\end{array}
\end{equation*}
where we have used \cref{rem.Fx0} to pass from the first line to the second.
Exploiting once more the properties \cref{eq.ShapeDerivatives.BumpFunction} of the function $h$, we arrive at:
$$
J^\prime_\e(\Gamma_D)(\theta) \approx  -\frac{1}{\e}  \int_{\Sigma_D}  \theta(x) \cdot n_{\Sigma_D}(x) \: u_{\Gamma_D, \e} (x) \: p_{\Gamma_D, \e} (x) \: \d \ell(x),
$$
an approximate formula which is simple to evaluate in practice and fulfills the desirable structure \cref{eq.ShapeDerivatives.Structure}.


%%%%%%%%%%%%%
\subsection{Shape derivative of a function depending on the inhomogeneous Neumann region}\label{sec.extsdconduc}
%%%%%%%%%%%%

\noindent In this section, we briefly discuss the situation where the region $G \subset \partial \Omega$ to be optimized is that $\Gamma_N$ supporting the inhomogeneous Neumann boundary condition in the problem \cref{eq.conduc}. Accordingly, we set $\Sigma_N = \partial \Gamma_N$, and we now denote by $u_{\Gamma_N}$ the solution to \cref{eq.conduc}; we then show how to calculate the shape derivative of the model objective function
$$ J(\Gamma_N) = \int_\Omega j(u_{\Gamma_N}) \:\d x,$$
where $j : \R \to \R$ again satisfies \cref{eq.jgrowth}. 
In this context, we consider the following subset of $\Winfty$ for deformations $\theta$ in the practice \cref{eq.Omtheta,eq.Gtheta} of the method of Hadamard:
\begin{equation}\label{eq.TGNeu}
\Theta_{\Gamma_N} := \left\{ \theta: \R^d \to \R^d \text{ is smooth}, \:\: \theta\cdot n =0 \text{ on } \partial \Omega \text{ and } \theta = 0 \text{ on }\Gamma_D \right\}.
\end{equation}
This situation turns out to be much easier to handle than that considered in \cref{sec.hepsconduc,sec.approxSD}. The potential $u$ in \cref{eq.conduc} is indeed less singular near the optimized boundary $\Sigma_N$, now marking the transition between homogeneous and inhomogeneous Neumann boundary conditions, than near the boundary $\Sigma_D$ between homogeneous Dirichlet and Neumann conditions, that was optimized in the previous developments. In particular, the calculation of the shape derivative $J^\prime(\Gamma_N)(\theta)$ does not require an approximation procedure in the spirit of that developed in \cref{sec.hepsconduc,sec.approxSD}, and its ``exact'' shape derivative can be used as is in practice.

The main result in the present situation is the following; we refer to our previous work \cite{dapogny2020optimization} for a rigorous proof. Interestingly, the correct expression for the derivative can be obtained by an application of the formal C\'ea's method exemplified in the proof of \cref{prop.SDDirtoNeu}. 

\begin{theorem}
The shape functional $J(\Gamma_N)$ is shape differentiable when deformations are considered in the set $\Theta_{\Gamma_N}$ in \cref{eq.TGNeu}, and its shape derivative reads:
$$ \forall \theta \in \Theta_{\Gamma_N}, \quad J^\prime(\Gamma_N)(\theta) = -\int_{\Sigma_N} g p_{\Gamma_N} \theta\cdot n_{\Sigma_N}\:\d \ell,$$
where the adjoint state $p_{\Gamma_N}$ is the unique solution in $H^1(\Omega)$ to the following boundary value problem:
$$
\left\{
\begin{array}{cl}
-\dv(\gamma \nabla p_{\Gamma_N}) = -j^\prime(u_{\Gamma_N}) & \text{in } \Omega, \\
p_{\Gamma_N} = 0 & \text{on } \Gamma_D, \\
\gamma \frac{\partial p_{\Gamma_N}}{\partial n} = 0 & \text{on }\Gamma \cup \Gamma_N.
\end{array}
\right.
$$
\end{theorem}



\begin{remark}
Multiple variations of the above configurations can be treated with the same techniques. For instance, one could consider the case where inhomogeneous Dirichlet boundary conditions are imposed on the region $G = \Gamma_D$ to be optimized, of the form $u = u_{\text{\rm in}}$, where $u_{\text{\rm in}}$ is a given, smooth function, see \cref{sec.CathodeAnode} for a related numerical example. On a different note, one could deal with a function $J(G)$ depending on a region $G$ bearing Robin boundary conditions, see \cref{sec.Helmholtz} for a related study in the physical context of acoustics, and \cref{sec.Acoustics} for a numerical example.
\end{remark}