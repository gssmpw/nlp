% In this paper, we studied and proposed a training technique for model robustness and a dynamic image enhancement pipeline for supporting the SDG medical color image segmentation tasks. 
Due to the large domain shifts between different throat datasets, the $S_{\mathrm{base}}$ model and the $S_{\mathrm{aug}}$ model (even with data augmentations), struggled to generalize on the unseen images, resulting in substantial performance drops on both the $\mathrm{LQ}_{seg}$ and $\mathrm{SP}_{seg}$ datasets. 
Visually, the results in Fig. \ref{fig:fig_3} illustrate that the baseline $S_{\mathrm{base}}$ model's predictions without DCIN are completely unusable. 
As discussed earlier, the large difference in image quality and image color between domains is the main reason for the vast decline in the performance of deep learning models. 

% We address these issues at both train time (using the CQG loss) and test time (using the DCIN module) and achieve greatly improved Dice scores for all segmentation models. 
We address these issues during both training (using the CQG loss) and testing (using the DCIN module). 
The CQG loss demonstrated its effectiveness as the $S_{\mathrm{CQG}}$ model outperformed other models in all experiments. 
By employing the contrastive constraint on different color and quality conditions from the same input, the CQG loss has increased the robustness of the model on other data domains. 
In addition, applying the DCIN module also significantly improved the segmentation performance on both datasets (Table \ref{tab:table_1} and \ref{tab:table_2}). 
The $S_{\mathrm{CQG}}$ with the full DCIN (GRIS + LRIS) produces results that closely resemble the ground truth. 
Notably, its visual results on the $\mathrm{SP}_{seg}$ dataset (Fig. \ref{fig:fig_3}, bottom part) appear very clean and accurate. 
In this context, the DCIN module effectively aligns the color distribution with that of the source domain, further boosting the segmentation performances. 

To confirm the impact of the two image reference selection strategies (GRIS and LRIS) in the DCIN module versus expert-selected reference image (ExRI), we also reported the results under different configurations: DCIN using ExRI, GRIS, and LRIS. 
In most experiments, the LRIS and GRIS strategies outperformed the DCIN with ExRI. 
This suggests that the subjective expert-selected reference image is not optimal while our objective GRIS and LRIS strategies often provided more suitable reference images. 

Although achieving promising results, there remain several limitations in our proposals. 
First, our current DCIN performed $l\alpha \beta$ color transfers on the CPU, which is somewhat inefficient as the image is later processed on the GPU. 
Adapting this operation to run directly on the GPU could improve processing speed. 
Second, the segmentation model faces challenges when target domains contain artifacts absent in the source domain. 
For instance, many images in the $\mathrm{SP}_{seg}$ dataset displayed teeth, which often confused the model and degraded prediction quality. 
Introducing additional post-processing techniques could improve the modelâ€™s ability to generalize when domain shifts extend beyond color and quality differences. 
There is room for improvement and we plan to mitigate these limitations in future works. 