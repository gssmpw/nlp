\subsection{Data Collection}
In this work, we collected 16,000 high-quality (HQ) clean throat images of around 900 patients from several hospitals in Japan and refer to it as the $\mathrm{HQ}_{all}$ dataset. 
They were obtained by a special camera type designed for taking pharyngeal images. 
Among the $\mathrm{HQ}_{all}$ dataset, there are 2,000 images that were labeled by experts containing the semantic pixel-level annotations of four areas inside the throat: uvula, tongue, tonsil, and pharyngeal wall. 
We randomly split 1,600 images for training (refer as $\mathrm{HQ}_{seg/train}$ set) and 400 images for validation (refer as $\mathrm{HQ}_{seg/val}$ set). 
In addition, the $\mathrm{HQ}_{all}$ dataset is used for color reference image selection as in the DCIN module. 

To evaluate the performance of the medical image segmentation models, we additionally collected data from two non-source domains. 
First, we collected 255 LQ images (e.g., blurry, hazy, compression artifacts, poor lighting, etc.) from over 150 patients. 
These images were taken by different camera devices, and we refer this as the $\mathrm{LQ}_{seg}$ dataset. 
Second, we collected 125 smartphone (SP) images. 
These images were taken primarily with iPhone SE (1st gen) and Sony Xperia XZ1 (SO-01K) cameras, and we refer to this as the $\mathrm{SP}_{seg}$ dataset. 
Images from all datasets are resized to the size of $768 \times 512$ pixels. 
Please refer to Fig. \ref{fig:fig_1} for some samples of these datasets. 

\subsection{Reference Image Selection}
For each test image, we utilize two images that correspond to the GRIS and LRIS strategies in our DCIN module. 
The global reference image $x_g$ is selected by applying the color histogram reference image selection pipeline on the $\mathrm{HQ}_{seg/train}$ dataset. 
We created the histograms in RGB space and used eight bins per channel based on our preliminary experiments. 
Note again that the image is the same across all test images and is selected before the test time. 
The local reference image $x_l$ is selected at inference time for each test image. 
Specifically, for all images in the $\mathrm{HQ}_{all}$ dataset, we pre-computed embedding vectors using a Swin-V2-Large model \cite{liu2021swin} which was pre-trained on the ImageNet dataset \cite{deng09imagenet}. 

To further validate the effectiveness of our GRIS and LRIS strategies, we incorporated a reference image selected by physicians for color transfer before testing. 
The image was chosen based on specific criteria, including cleanliness and color balance, ensuring its suitability for diagnostic purposes. 
We refer to this image as the expert-selected reference image (ExRI). 
% Fig. 3
\input{figures/tex_files/Fig_3}

\subsection{Training Throat Segmentation Models}
To evaluate the effectiveness of our proposals for supporting SDG medical image segmentation tasks, in this experiment, we trained three different throat image segmentation models on the $\mathrm{HQ}_{seg/train}$ dataset. 
All models are built based on the U-Net \cite{ronneberger15unet} model with the pre-trained EfficientNet-B2 \cite{tan2019efficientnet} on the ImageNet dataset as the backbone. 
The three segmentation models are:
\begin{itemize}
    \item Baseline ($S_{\mathrm{base}}$): The baseline model is trained with minimal preprocessing, consisting only of resizing and normalization. 
    A simple loss function comprising the sum of Dice and cross-entropy losses is applied without any data augmentations. 
    
    \item Baseline + augmentations ($S_{\mathrm{aug}}$): The baseline model is additionally trained with the augmentations as in the CQG loss. 
    The data augmentation only creates one output from an input image and the CQG loss is not applied. 
    
    \item Baseline + CQG ($S_{\mathrm{CQG}}$): The baseline model is trained with the color-quality generalization (CQG) loss function. 
    We chose to use $\lambda_{1}=0.3, \lambda_{2}=0.7, \lambda_{3}=1.0$ based on our preliminary experiments. 
\end{itemize}

All three throat segmentation models were optimized using the Adam optimizer \cite{kingma2015adam} with a learning rate of $1 \times 10^{-3}$. 
The batch size was set to 2 and training was completed after 15 epochs. 
For the evaluation metric, we employed the commonly used Dice score to measure the overlap between the prediction and the segmentation ground-truth masks. 

\subsection{Results of Throat Segmentation Models}
After training, the three segmentation models $S_{\mathrm{base}}$, $S_{\mathrm{aug}}$, and $S_{\mathrm{CQG}}$ achieved Dice scores of 88.9, 87.8, and 88.6 on the $\mathrm{HQ}_{seg/val}$ dataset, respectively. 
All models demonstrated high accuracy on the source domain, indicating the capability of accurately segmenting HQ images. 

Table \ref{tab:table_1} and \ref{tab:table_2} summarize the Dice scores for the three models on the $\mathrm{LQ}_{seg}$ and $\mathrm{SP}_{seg}$ datasets (the best performance of each model across all DCIN configurations is in \textbf{bold} text). 
The visual comparison of segmentation results is provided in Fig. \ref{fig:fig_3}. 
Note that the results with DCIN depicted in Fig. \ref{fig:fig_3} are from the full DCIN (i.e., GRIS + LIRS). 
All models experienced a severe performance drop when moving from the HQ dataset to other non-source datasets. 
For instance, without DCIN module, the $S_{\mathrm{base}}$ model's Dice score largely dropped from 88.9 on $\mathrm{HQ}_{seg/val}$ to 36.9 on the $\mathrm{LQ}_{seg}$ dataset, and down to 55.3 on the $\mathrm{SP}_{seg}$ dataset. 

Both the DCIN module and CQG loss consistently boosted the segmentation performances in all models. 
The most effective configuration was achieved by combining CQG training with the full DCIN (i.e., GRIS + LRIS), resulting in increments in Dice score over the baseline models, from 36.9 to 69.2 on the $\mathrm{LQ}_{seg}$, and 55.3 to 68.6 on the $\mathrm{SP}_{seg}$ datasets. 
% Table I
\input{tables/Table_I}
% Table II
\input{tables/Table_II}