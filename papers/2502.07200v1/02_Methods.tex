\subsection{Dynamic Color Image Normalization}
Fig. \ref{fig:fig_2} (blue dashed box) shows the data flow of our dynamic color image normalization (DCIN) method. 
For a given input test image from a non-source domain, the reference image selection module strategically identifies suitable reference images from the training source domain. 
The color transfer in the perception-based color space $l\alpha \beta$ \cite{reinhard2001color} is then applied to align the color distribution of the reference images with that of the test image. 
The reference image selection module incorporates two strategies: \say{global} reference selection and \say{local} reference selection. 
The global strategy assigns a single reference image to all test images, whereas the local strategy selects a unique reference image for each individual test image. 
Detailed descriptions of these strategies are described below. 

\subsubsection{Global Reference Image Selection}
We propose the global reference image selection (GRIS) strategy that utilizes color histograms such that each image in the source domain is converted into a $b$-bin normalized color histogram vector. 
The global reference image $x_g$ is selected as the image whose color histogram minimizes the average pairwise distance between histograms of all other images in the source domain. 
The average pairwise distance $\mathcal{D}_{\mathrm{pairwise}}(x_i) = \frac1N\overset N{\underset{j=1}{\sum d_{i,j}}}$, where $d_{i,j}=\sqrt{\sum_{k=1}^b\left(H_k\left(x_i\right)-H_k\left(x_j\right)\right)^2}$ is the Euclidean distance between two normalized histogram vectors. 
In this case, $H_k(x)$ is the $k$-th bin value of the image $x$, and $N$ is the number of images in the source domain. 
The selected image $x_g$ will be used as the color normalization reference of \textit{all} test images before making predictions. 

\subsubsection{Local Reference Image Selection}
As stated previously, segmentation results vary on different reference images. 
Thus, we believe selecting a semantically similar image to the test image from the training data can benefit the segmentation performance. 
We propose a local reference image selection (LRIS) strategy that utilizes a pre-trained CNN model to select a more tailored reference image for each test image. 
First, the pre-trained CNN was used to extract feature vectors from all source images, which were then normalized into unit vectors. 
For a given image $x_{test}$ in a test domain, a local reference image $x_l$ is selected as the one whose feature vector has the highest cosine similarity with $x_{test}$. 
The selected image $x_l$ will be used as the color normalization reference of the test image $x_{test}$ before making predictions. 

\subsubsection{Ensembling Both Selected Reference Images}
To further improve generalization capabilities, the results of the above two strategies can be ensembles. 
From the selected global and local reference images, two color-normalized input images were produced, which were then used to generate two corresponding output masks. 
Finally, the final prediction is formed by taking a pixel-wise mean of the two predicted masks. 
Here, we refer to the \say{full DCIN} as the complete module with this ensemble method (i.e., GRIS + LRIS). 

\subsection{The Color-Quality Generalization Loss}
The color-quality generalization loss (CQG) is a training objective function inspired by contrastive loss. 
The idea is that given the same input presented in varying colors and qualities, the model should produce identical segmentation masks. 
This approach encourages the model to adapt effectively to images with diverse variations. 
Fig. \ref{fig:fig_2} (purple dashed box) illustrates the data flow of the CQG loss. 
For each training image $x$, we apply transformations randomly to generate two inputs for the segmentation model. 
The first input $x_1$ is obtained by applying geometric transformations, and the second input $x_2$ is from both geometric and photometric transformations. 
Geometric transformations are applied to change the input $x$ geometrically while photometric transformations change its color and quality. 
Specifically, geometric transformations include random horizontal flip, shear, shift, scale, rotation, and elastic transform. 
Photometric transformations include random blur, sharpening, Gaussian noise, brightness contrast, and RGB shifts. 
Both $x_1$ and $x_2$ have the same ground-truth mask $y$.

Given a segmentation model $S$ and a ground-truth mask $y$, we have $y_1 = S(x_1)$, and $y_2 = S(x_2)$. 
Our CQG loss is defined as:
\begin{equation}
    \mathcal{L} = \lambda_{1}\mathrm{DC}(y,y_1) + \lambda_{2}\mathrm{DC}(y,y_2) + \lambda_{3}\mathrm{MSE}(y_1,y_2),
\end{equation}
where $\mathrm{DC}(y,y\prime)$ is the sum of the Dice loss and cross-entropy loss between the ground truth $y$ and predicted mask $y\prime$. 
$\mathrm{MSE}(y_1,y_2)$ is the mean squared error loss between the predicted masks $y_1$ and $y_2$. 
Here, $\lambda_{1}, \lambda_{2}, \lambda_{3}$ are the hyper-parameters controlling the weight of each loss term. 

The CQG loss can be viewed as a form of image augmentation. By leveraging this loss, the model is encouraged to produce identical predictions for both original and augmented images, regardless of color and quality shifts. 