%Version 3 October 2023
% See section 11 of the User Manual for version history
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%======================================================%%
%% to compile with pdflatex/xelatex use pdflatex option %%
%%======================================================%%

%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style


%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove Numbered in the optional parenthesis. 
%%The option is available for: sn-basic.bst, sn-vancouver.bst, sn-chicago.bst%  
 
%%\documentclass[sn-nature]{sn-jnl}% Style for submissions to Nature Portfolio journals
%%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
%\documentclass[sn-mathphys-num]{sn-jnl}% Math and Physical Sciences Numbered Reference Style 
\documentclass[sn-mathphys-ay]{sn-jnl}% Math and Physical Sciences Author Year Reference Style
%%\documentclass[sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[sn-vancouver,Numbered]{sn-jnl}% Vancouver Reference Style
%%\documentclass[sn-apa]{sn-jnl}% APA Reference Style 
%%\documentclass[sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style

%%%% Standard Packages
%%<additional latex packages if required can be included here>

\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%
%%%%

\usepackage{enumitem}%
\usepackage{braket}%
\usepackage{subcaption}%
\usepackage{comment}%


%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

%% as per the requirement new theorem styles can be included as shown below
\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}

\title[QAE classification]{Quantum autoencoders for image classification}

\author*[1]{\fnm{Hinako} \sur{Asaoka}}\email{asaoka.hinako@is.ocha.ac.jp}

\author[1,2]{\fnm{Kazue} \sur{Kudo}}
\equalcont{These authors contributed equally to this work.}

\affil*[1]{\orgdiv{Department of Computer Science}, \orgname{Ochanomizu University}, \orgaddress{\street{2-1-1 Otsuka}, \city{Bunkyo-ku}, \postcode{112-8610}, \state{Tokyo}, \country{Japan}}}

\affil[2]{\orgdiv{Graduate School of Information Sciences}, \orgname{Tohoku University}, \orgaddress{\street{6-3-09 Aoba, Aramaki-aza Aoba-ku}, \city{Sendai}, \postcode{980-8579}, \state{Miyagi}, \country{Japan}}}


% 語数制限250
\abstract{Classical machine learning often struggles with complex, high-dimensional data. Quantum machine learning offers a potential solution, promising more efficient processing. While the quantum convolutional neural network (QCNN), a hybrid quantum-classical algorithm, is suitable for current noisy intermediate-scale quantum-era hardware, its learning process relies heavily on classical computation. Future large-scale, gate-based quantum computers could unlock the full potential of quantum effects in machine learning. In contrast to QCNNs, quantum autoencoders (QAEs) leverage classical optimization solely for parameter tuning. Data compression and reconstruction are handled entirely within quantum circuits, enabling purely quantum-based feature extraction. This study introduces a novel image-classification approach using QAEs, achieving classification without requiring additional qubits compared with conventional QAE implementations. The quantum circuit structure significantly impacts classification accuracy. Unlike hybrid methods such as QCNN, QAE-based classification emphasizes quantum computation. Our experiments demonstrate high accuracy in a four-class classification task, evaluating various quantum-gate configurations to understand the impact of different parameterized quantum circuit (ansatz) structures on classification performance. Our results reveal that specific ansatz structures achieve superior accuracy, and we provide an analysis of their effectiveness. Moreover, the proposed approach achieves performance comparable to that of conventional machine-learning methods while significantly reducing the number of parameters requiring optimization. These findings indicate that QAEs can serve as efficient classification models with fewer parameters and highlight the potential of utilizing quantum circuits for complete end-to-end learning, a departure from hybrid approaches such as QCNN.}

\keywords{Quantum autoencoders, Image classification, Quantum machine Learning, Quantum circuit learning}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle

\section{Introduction}\label{sec1}

Quantum machine learning (QML) is an emerging field exploring the application of quantum computers to machine-learning tasks. QML leverages the intrinsic parallelism of quantum computation, offering the potential for more efficient processing of complex data compared to classical machine-learning methods\citep{Shor_1997, Biamonte_2017, Adhikary_2020, Chakraborty2020, Zhang_2020, Tycola_2023, Thakkar_2024, Song_2024, Xiong_2025}. Although current noisy intermediate-scale quantum (NISQ) devices are limited by qubit count and high noise, various hybrid quantum-classical approaches, such as quantum circuit learning, have been proposed to enable practical QML applications. A prominent example is the quantum convolutional neural network (QCNN), which integrates gate-based quantum computing with conventional machine-learning models\citep{Cong_2019, Henderson_2019}. Classical convolutional neural networks, widely used in image classification for their ability to extract abstract features through convolution, achieve robust accuracy by learning generalized image representations\citep{Khan_2020}. QCNN aims to enhance processing speed and accuracy by replacing classical convolution operations with quantum circuits. However, because only a portion of the convolutional process occurs on a quantum circuit, with the remainder handled by a classical computer, QCNN remains a hybrid algorithm well-suited to the limitations of NISQ-era hardware. This hybrid approach attempts to capitalize on the benefits of quantum computing while relying on classical processing for robustness\citep{Wei_2022, Matic_2022, Hur_2022, Hassan_2024}. Critically, the dependence on classical algorithms remains high, as only a fraction of the learning process is performed quantum mechanically.

While hybrid methods such as QCNN are pragmatic for current technology, developing approaches where most of the learning process occurs within quantum circuits is also essential. Although such algorithms may not be immediately practical in the NISQ era, future advancements in large-scale, gate-based quantum computing promise to unlock the full potential of quantum effects such as superposition and entanglement in machine learning. This could lead to significantly faster learning and greater model expressiveness than classical approaches. A promising avenue for quantum feature learning is the quantum autoencoder (QAE)\citep{Romero_2017}. QAEs are generative models that compress input data into fewer qubits and then reconstruct the original information by reversing the trained quantum circuit. While classical optimization is used for parameter tuning in QAE training, data compression and reconstruction are performed entirely within the quantum circuit. This demonstrates the capability of quantum circuits alone to effectively extract essential features. Prior research has primarily employed QAEs for data compression and reconstruction, including applications in data preprocessing, feature transformation, and anomaly detection, all leveraging their compression-reconstruction capabilities\citep{Srikumar_2021, Mangini_2022, Ngair_2022, Sakhnenko_2022, Zhu_2023, Wang_2024}. However, to the best of our knowledge, no prior work has demonstrated the application of QAEs to other tasks.

Therefore, this study proposes a novel approach applying QAEs to image classification, demonstrating that QAEs, as generative models, can perform tasks beyond data compression and reconstruction. In a previous study, we successfully applied nonnegative binary matrix factorization (NBMF) to image classification, leveraging binary optimization for feature compression\citep{O_Malley_2018, Asaoka_2023}. This resulted in faster convergence and higher accuracy compared to other machine-learning methods. Similarly, we anticipate that QAEs will offer several advantages in image classification. In our proposed QAE-based image classification, the label information is incorporated during training. Traditional QAE training optimizes quantum circuit parameters to minimize reconstruction error between input images and their reconstructed outputs. Our approach encodes both image and label information into the quantum circuit and optimizes parameters to minimize the error between the reconstructed output and label information. This allows the trained circuit to predict class labels for unseen test images. Importantly, this method enables image classification without increasing the number of qubits compared to conventional QAE implementations. While parameter optimization uses classical algorithms, the quantum circuit transforms the quantum representation of images and labels. Consequently, the quantum circuit structure significantly influences classification accuracy. Unlike hybrid models such as QCNN, which perform only a portion of the computation quantum mechanically, QAE-based classification emphasizes quantum computation in machine learning.

This study experimented with multiple qubit control configurations to investigate the effect of different ansatz structures within the parameterized quantum circuit of the QAE. We demonstrate that specific ansatz structures achieve high classification accuracy and analyze the underlying reasons for their effectiveness. Additionally, we compare the performance of QAE-based classification with that of classical generative models and other classical machine-learning models under identical data conditions. We identify scenarios where QAEs outperform classical approaches and those wherein their learning efficiency is lower, providing insights into the potential advantages of QML.
The key contributions of this paper are as follows:
\begin{enumerate}
\item It proposes the application of QAE to image-classification tasks.
\item It identifies effective ansatz structures for QAE-based image classification.
\item It demonstrates that QAE-based classification outperforms certain classical machine-learning methods under specific conditions. 
\end{enumerate}


\section{Related work}\label{sec2}

\subsection{Autoencoders}
An autoencoder is a classical generative model designed for dimensionality reduction and reconstruction of input data\citep{Hinton_2006}. It achieves this through an unsupervised neural network. Because the primary goal is input data reconstruction, the output maintains the same dimensionality as the input. This study considers a three-layer autoencoder structure: an input encoder, a latent layer, and an output decoder. The centrally located latent layer has fewer dimensions than the input layer, facilitating data compression. Network parameters are optimized during training to minimize the discrepancy between input and output data, ensuring accurate reconstruction. This training process enables the extraction of essential features within the latent layer, which are crucial for data reconstruction. Autoencoders are used in various applications, including generating new data by manipulating latent layer variables in a trained network, detecting anomalies by training on standard data, and identifying deviations in reconstruction when presented with abnormal input data.

\subsection{Quantum circuit learning (QCL)}
QCL is a hybrid classical-quantum machine-learning framework\citep{Mitarai_2018}. It involves encoding input data into a quantum circuit and employing classical optimization algorithms to iteratively adjust circuit parameters to achieve the desired output. QCL aims to leverage the exponentially large state space of quantum systems for machine learning, potentially solving tasks that are intractable for classical algorithms and demonstrating quantum advantage. Training data with QCL begins by encoding the data into a quantum state, $\ket{\psi_{\text{in}}}$, through a unitary transformation. Applying a parameterized quantum circuit ansatz, $U(\theta)$, to the input state yields the output state $U(\theta)\ket{\psi_{\text{in}}}$. In supervised machine-learning scenarios, a cost function is defined in terms of the measurement results of the output state and the training data. The desired circuit is trained by iteratively optimizing the parameter $\theta$ to minimize this cost function.

\subsection{Quantum autoencoders}
A QAE is an algorithm that employs QCL to construct an autoencoder network\citep{Romero_2017}. Similar to conventional autoencoders, the goal is to compress and reconstruct the dimensionality of input data. This is achieved by optimizing an ansatz, $U(\theta)$, to compress the quantum state of the input and then restore it to a state closely approximating the original input. Figure~\ref{fig:QAE_before_swap} illustrates the structure of the target quantum circuit.
\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{qae-paper-figure/Fig1.jpg}
\caption{Structure of a conventional quantum autoencoder}\label{fig:QAE_before_swap}
\end{figure}
In the input AB system (Figure~\ref{fig:QAE_before_swap}), A is the latent state, where input information is compressed, and B is the trash state. Additionally, B' is the reference state, containing the same number of qubits as B. Successful compression and reconstruction are achieved if the input AB system matches the output AB' system after applying $U^\dagger(\theta)$. This means maximizing the fidelity between the input and output states. The input state of each qubit in the AB system is $\ket{\psi_i}$, and the quantum state of the B' system is $\ket{a}$. The density operator of the AB' system, which includes the output state, is obtained by tracing out state B from the entire system. This can be expressed by the following equation:
\begin{equation}
\rho_{AB'} = U_{AB'}^\dagger(\vec{\theta}) \mathrm{Tr}_B \big[ U_{AB}(\vec{\theta}) \big[ \ket{\psi_i} \bra{\psi_i}_{AB} \otimes \ket{a} \bra{a}_{B'} \big] U_{AB}^\dagger(\vec{\theta}) \big] U_{AB'}(\vec{\theta}). \label{eq:rho_AB'}
\end{equation}
The objective function $C(\vec{\theta})$, which must be maximized, can be defined as follows:
\begin{equation}
C(\vec{\theta}) = \sum_i p_i F \big(\ket{\psi_i}, \rho_{AB'} \big), \label{eq:C1}
\end{equation}
where $p_i$ is the probability that $\ket{\psi_i}$ is included in the output state.
However, determining the match between the quantum states of the AB and the AB' systems would typically require numerous qubit measurements. A swap test is performed on B and B' to address this challenge. The fidelity of the input and output states of the AB system after swapping the quantum states of B and B' can be expressed as follows:
\begin{align}
F \big(\ket{\psi_i}, \rho_{AB} \big) &= F \big( \ket{\psi_i}, U_{AB}^\dagger \mathrm{Tr}_{B'} \big[ U_{AB'} \big[ \ket{\psi_i} \bra{\psi_i}_{AB'} \otimes \ket{a} \bra{a}_{B} \big] U_{AB'}^\dagger \big] U_{AB} \big) \nonumber \\
&= \bra{\psi_i} U_{AB}^\dagger \mathrm{Tr}_{B'} \big[ U_{AB'} \big[ \ket{\psi_i} \bra{\psi_i}_{AB'} \otimes \ket{a} \bra{a}_{B} \big] U_{AB'}^\dagger \big] U_{AB}  \ket{\psi_i}. \label{eq:F_AB}
\end{align}
The fidelity of the input and output states of the AB' system before the swap can be expressed as follows:
\begin{equation}
F \big( U_{AB'} \ket{\psi_i}, \mathrm{Tr}_{B} \big[ U_{AB} \ket{\psi_i} \bra{\psi_i} U_{AB}^\dagger \otimes \ket{a} \bra{a}_{B'} \big] \big). \label{eq:F_AB'}
\end{equation}
After swapping B and B', the fidelity can be rewritten as follows, demonstrating equivalence with the fidelity of the AB system.
\begin{align}
F &\big( U_{AB} \ket{\psi_i}, \mathrm{Tr}_{B'} \big[ U_{AB'} \ket{\psi_i} \bra{\psi_i} U_{AB'}^\dagger \otimes \ket{a} \bra{a}_{B} \big] \big) \nonumber \\
&= \bra{\psi_i} U_{AB}^\dagger \mathrm{Tr}_{B'} \big[ U_{AB'} \big[ \ket{\psi_i} \bra{\psi_i}_{AB'} \otimes \ket{a} \bra{a}_{B} \big] U_{AB'}^\dagger \big] U_{AB}  \ket{\psi_i} \nonumber \\
&= F \big(\ket{\psi_i}, \rho_{AB} \big) \label{eq:F_AB'_swap}
\end{align}
Therefore, swapping B and B' is equivalent to reconstructing the input. The swap test leverages the agreement between quantum states, which can be determined by measuring a single ancillary qubit, significantly reducing the measurement overhead compared to the pre-swap scenario. Figure~\ref{fig:QAE_with_swap} depicts the quantum circuit configured for learning image data with a QAE.
\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{qae-paper-figure/Fig2.jpg}
\caption{Quantum autoencoder structure after conducting the swap test}\label{fig:QAE_with_swap}
\end{figure}
Image data are encoded into the quantum state by setting them as parameters in the circuit $V(x)$. The parameter $\theta$ of the ansatz $U(\theta)$ is optimized using a classical algorithm to maximize the agreement between the B and B' states. The swap test is introduced to facilitate this. As mentioned previously, the swap test is an algorithm for determining the agreement of two quantum states. An auxiliary qubit is prepared for measurement; if the two quantum states match, the measurement result of the auxiliary qubit will be $\ket{0}$. Consequently, by defining an objective function that minimizes the probability of measuring $\ket {1}$, the parameter $\theta$ is optimized to align the states of B and B'.


\section{Quantum autoencoders for image classification}

This study proposes a novel quantum circuit for image-classification training using QAEs. Figure~\ref{fig:QAE_classification_train} provides an overview of the circuit.
\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{qae-paper-figure/Fig3.jpg}
\caption{Quantum autoencoder circuit for image-classification training. Class information is encoded in $V_L(y)$}\label{fig:QAE_classification_train}
\end{figure}
The circuit $V(x)$ for encoding input image data, ansatz circuit $U(\theta)$, and swap test are based on the original QAE architecture. To adapt the QAE for image classification, the reference state B' is set using the circuit $V_L(y)$. Figure~\ref{fig:reference_state_for_classification} illustrates the structure of $V_L(y)$.
\begin{figure}[t]
\centering
\includegraphics[width=0.3\textwidth]{qae-paper-figure/Fig4.jpg}
\caption{Circuit diagram for $V_L(y)$, used for encoding class information into the reference state B'}\label{fig:reference_state_for_classification}
\end{figure}
In the $V_L(y)$ circuit, an RX gate is assigned to each qubit. The parameter for the RX gate corresponds to the rotation angle of the quantum state around the x-axis, thus changing the state of the qubit based on the parameter value. When a qubit is initialized to the state $\ket{0}$, and the RX gate parameter is set to $\pi$, the state flips to $\ket{1}$. This phenomenon is used to represent class information. If B' comprises $l$ qubits, there are $2^l$ possible quantum states, allowing for assigning up to $2^l$ different classes. For example, consider the case where $l = 1$ and the state $\ket{0}$ represents class $y = 0$, whereas $\ket{1}$ represents class $y = 1$. If the image data input into $V(x)$ belongs to class 0, the state of $V_L(y=0)$ should remain $\ket{0}$. To achieve this, the RX gate parameter is set to zero, leaving the initial state unchanged. Conversely, if it belongs to class 1, the $V_L(y=1)$ state should be $\ket{1}$. This is achieved by setting the RX gate parameter to $\pi$, flipping the initial state. Under these conditions, parameter optimization learning proceeds similarly to that in previous QAE studies. The parameters of $U(\theta)$ are trained such that B encodes class information based on the swap test results between B and B'. Figure~\ref{fig:QAE_classification_test} shows the circuit used for classifying images using the trained $\theta$.
\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{qae-paper-figure/Fig5.jpg}
\caption{Quantum autoencoder circuit for image-classification prediction}\label{fig:QAE_classification_test}
\end{figure}
The parameters of $V(x)$ are set using the information from the test image to be classified and the trained parameters are applied to $U(\theta)$. The circuit is then measured. Because $\theta$ has been optimized using the swap test results between B and B' as the objective function, the class information of the image is output to B. The class of the test image can be predicted by comparing the qubit measurement results of the quantum state in B with the correspondence between quantum states and class information defined in B' during training. The predicted result is the class corresponding to the quantum state with the highest probability among the measurement results. In conventional QAE methods, B is in a trash state and unused for tasks such as information recovery. However, the proposed method encodes label information into B, allowing the qubits in B to be effectively used for image classification.

\section{Experiments}

\subsection{Experimental settings}
To validate the proposed image-classification approach, experiments were conducted using the MNIST dataset of handwritten digit images. This experiment focused on a subset of the MNIST dataset, selecting images from four classes (digits 0–3). The training set comprised 500 images, whereas the test set comprised another 500. The dataset was constructed to contain a nearly equal number of samples for each class, mitigating the risk of class imbalance.
Amplitude encoding was employed for the image encoder $V(x)$. Amplitude encoding maps classical data $x$ of dimension $2^n$ to an n-qubits quantum state. Because the dimension of $x$ must be an integer when calculating $log_{2}2^n$, the image size was set to $16 \times 16 = 2^8$ for this experiment. Consequently, the AB system comprised eight qubits.

Given the four-class dataset, four distinct quantum states were necessary for classification; these states required a minimum of two qubits. The trash state B, which encodes class information, was set to $l = 3$ qubits. This setting allowed for a comparative experiment with an eight-class classification task using a QAE classification circuit of the same structure. Eight-class classification necessitates eight quantum states, representable with a minimum of three qubits. Maintaining the same number of qubits for the trash and reference states in the AB system facilitated this comparison. The remaining $k = 5$ qubits in AB were used for the latent state A. The reference state B' had the same number of qubits as B and was, therefore, also set to three qubits. An additional qubit was required for the swap test. Thus, 12 qubits were used in this QAE classification experiment.

The classical optimization method constrained optimization by linear approximation (COBYLA) was employed to optimize the parameter $\theta$ of the ansatz $U(\theta)$. A key characteristic of COBYLA is that it does not require gradient information. Because current quantum hardware is highly susceptible to noise, gradient-based optimization using numerical differentiation can be unstable. COBYLA, being gradient-free, is expected to yield more stable results in QCL\citep{Bonet_Monroig_2023, Pellow_Jarman_2024}. However, this experiment utilized a quantum circuit simulator, which is noise-free compared to actual quantum hardware. The proposed method was designed with future implementation on real hardware in mind; therefore, results using COBYLA are presented herein. In this experiment, parameter optimization with COBYLA was performed for 5000 epochs.

For qubit measurement during training and testing, the quantum circuit simulator provided by Qiskit was used\citep{Qiskit}. Specifically, we employed the the statevector simulator, which emulates the theoretical state of a quantum computer, computing the state vector after quantum circuit execution. Qiskit also offers the Qasm simulator, which probabilistically samples measurement results and access to gate-based quantum computers via the cloud. However, given the current state of quantum error correction and hardware, quantum measurements using these methods are highly susceptible to noise-induced errors. This experiment excludes the influence of device noise on computational accuracy, presenting results obtained under ideal conditions as an indicator of the potential of the proposed method. A seed value was set for all processes that would otherwise produce stochastic results, ensuring reproducibility.

\subsection{Ansatz}
The construction of the ansatz $U(\theta)$ in the QAE classification circuit primarily employs several types of ansatze frequently used for training conventional QAEs. Each ansatz includes a section that is repeated multiple times, as indicated in the ansatz schematic. The number of repetitions is denoted as $M$.

\begin{figure}[t]
\centering
% 1行目（3つの画像）
\begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[width=\linewidth]{qae-paper-figure/Fig6a.jpg}
    \caption{Circuit 1}
    \label{fig:circ1}
\end{subfigure}
\begin{subfigure}{0.29\textwidth}
    \centering
    \includegraphics[width=\linewidth]{qae-paper-figure/Fig6b.jpg}
    \caption{Circuit 2}
    \label{fig:circ2}
\end{subfigure}
\begin{subfigure}{0.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{qae-paper-figure/Fig6c.jpg}
    \caption{Circuit 3}
    \label{fig:circ3}
\end{subfigure}
\vspace{10pt}
% 2行目（1つの画像）
\begin{subfigure}{1.0\textwidth}
    \centering
    \includegraphics[width=\linewidth]{qae-paper-figure/Fig6d.jpg}
    \caption{Circuit 4}
    \label{fig:circ4}
\end{subfigure}
\vspace{10pt}
% 3行目（1つの画像）
\begin{subfigure}{1.0\textwidth}
    \centering
    \includegraphics[width=\linewidth, height=1.5cm]{qae-paper-figure/Fig6e.jpg}
    \caption{Circuit 5}
    \label{fig:circ5}
\end{subfigure}
\caption{Quantum circuit structures for different parameterized ansatze (Circuits 1–5). Circuits 1–3 have real-amplitude circuit variations, whereas Circuits 4 and 5 are based on the original QAE. $M$ represents the number of repetitions in each ansatz}
\label{fig:circ_1_to_5}
\end{figure}

The first ansatz considered is the real-amplitude circuit, wherein classical information is initially encoded through rotations using RY gates\citep{Alessandro_2022}. Subsequently, adjacent qubits are entangled via CNOT gates, followed by additional rotations using RY gates to adjust the quantum state. This entanglement sequence with CNOT gates and rotation with RY gates is repeated $M$ times. Several patterns exist for applying entanglement via CNOT gates.
One such pattern is the reverse linear pattern, which begins by applying a CNOT gate with the control and target qubits being the second-to-last and last qubits, respectively. The control qubit is then shifted sequentially, one by one, up to the first qubit, with CNOT gates applied at each step. This configuration is presented as Circuit 1 in Figure~\ref{fig:circ1}. Conversely, the linear pattern applies CNOT gates sequentially, with the control qubits starting from the first and moving down to the lower qubits. This configuration is presented as Circuit 2 in Figure~\ref{fig:circ2}. A variation similar to the linear pattern, the circular pattern, adds a CNOT gate to control the first qubit from the last qubit before establishing entanglement among adjacent qubits. This creates a circular connection among qubits, enhancing entanglement, and is shown as Circuit 3 in Figure~\ref{fig:circ3}. The real-amplitude circuit represents a simple circuit structure; its effectiveness in image-classification tasks, along with variations owing to different entanglement patterns, was examined in the experiments.

Circuits 4 and 5 adopt the ansatz structures proposed in the original QAE. Circuit 4 entangles all possible qubit pairings by applying Controlled-RY gates. The application of Controlled-RY gates to all qubit pairs constitutes one iteration, repeated $M$ times. Figure~\ref{fig:circ4} presents the structure of Circuit 4. Circuit 5 is designed to incorporate all possible controlled rotations for each qubit. Initially, each qubit undergoes a rotation using an RX gate. Subsequently, the first qubit controls all other qubits using Controlled-RY gates. This process is performed sequentially for each qubit in the ansatz, moving from the first to the last. Once the last qubit has completed its control operations, each qubit undergoes another RX rotation, similar to that in the initial step. This entire sequence constitutes one iteration and is repeated $M$ times. The structure of Circuit 5 is presented in Figure~\ref{fig:circ5}. Circuits 4 and 5 have been used in previous research to compress the ground states of the Hubbard model and molecular Hamiltonians. This study investigated their applicability for training in image-classification tasks.


\subsection{Other machine learning methods for comparison}

\subsubsection{Fully connected neural network}
In a fully connected neural network (FCNN), neurons are connected between consecutive layers. The network learns features by propagating information from the input to the output layer. Image data are provided as input and transformed through the hidden layer, and the predicted class is output. The network is trained using the multiclass cross-entropy loss between the output layer and the true class labels as the objective function. Parameters are updated to minimize this objective function, thereby minimizing the difference between the FCNN's output and the actual class labels. A single-hidden-layer FCNN can be considered, where the hidden layer compresses the input, serving a role analogous to the latent state in QAE classification. Therefore, the number of nodes in the hidden layer of an FCNN is considered equivalent to the information content of the latent state in the classical space of QAE classification. This study used a three-layer FCNN (input, hidden, and output) with $2^k = 32$ nodes in the hidden layer for comparative experiments under consistent dataset conditions. 

\subsubsection{Nonnegative/binary matrix factorization}
NBMF was originally proposed as a generative model trained using a quantum annealing machine\citep{O_Malley_2018}. It is an algorithm that optimizes features necessary for reconstructing the original data by factorizing a matrix into nonnegative real-valued basis and binary coefficient matrices. Optimizing the elements of the binary matrix can be viewed as a combinatorial optimization problem suitable for annealing methods. Previous studies have proposed NBMF as a multiclass image-classification model that takes a matrix comprising image data and corresponding class information as the input\citep{Asaoka_2023}. NBMF decomposition of this input yields a basis matrix that extracts features associating the image data with the class information. This matrix enables class prediction for test data.
The training process in NBMF for image classification leverages nonnegativity constraints and a binary combinatorial optimization procedure. This allows for effective feature extraction in the early stages of learning, even with small datasets and limited features, resulting in high accuracy. Leveraging quantum optimization to retain only necessary information is expected to further improve accuracy, even with less data.

Because NBMF performs feature extraction for classification through a single matrix factorization step, it can be structurally mapped to a three-layer FCNN. The column vectors of the coefficient matrix correspond to the nodes in the hidden layer of the FCNN, whereas the components of the basis matrix correspond to the edge weights. The feature dimension in NBMF, equivalent to the coefficient matrix's dimension and the number of columns in the basis matrix, corresponds to the number of nodes in the hidden layer of the FCNN. This feature dimension can also be considered equivalent to the information content in the classical space of the latent state in QAE classification. A comparative experiment was conducted with the feature dimension set to $2^k = 32$, decomposing the dataset under conditions identical to those in the QAE classification evaluation.

\subsubsection{Singular value decomposition}
Singular Value Decomposition (SVD) is a technique that factorizes a matrix $A$ into the product of three matrices:
\begin{equation}
A = UDV^T.
\end{equation}
The central matrix $D$ is a diagonal matrix with diagonal elements, known as singular values, arranged in descending order. The magnitude of each singular value corresponds to its importance within the data. By retaining only the most significant singular values and computing the matrix product, the input matrix can be approximated using only its most significant features. 
The three matrices resulting from SVD can be rearranged. By taking the square root of the diagonal matrix ($\sqrt{D}$) and multiplying it with the preceding and succeeding matrices, a two-matrix factorization, $U\sqrt{D}$ and $\sqrt{D}V^T$, can be considered. Leveraging this, image classification is performed analogously to NBMF. The input, a matrix of image data and corresponding class information, is factorized into two matrices using SVD. To approximate the input matrix, the number of dimensions retained in the diagonal matrix is set to be the same as the dimension of the coefficient matrix and the number of columns in the basis matrix in NBMF, achieving equivalent dimensionality reduction. The resulting basis state is then used for image classification in the same way as in NBMF. Key differences from NBMF include the absence of binary optimization of matrix elements and the lack of a nonnegativity constraint on the basis matrix. As SVD is a classical matrix factorization method, it does not benefit from the quantum optimization effects of NBMF. The number of dimensions retained in the diagonal matrix corresponds to the information content in the classical space of the latent state in QAE classification. Therefore, a comparative experiment was conducted with the dimension of the diagonal matrix set to $2^k = 32$, using the same dataset as that in the QAE classification evaluation as input.


\section{Results}

Image-classification training using the QAE classification quantum circuit was conducted using multiple ansatz configurations. Table~\ref{tab:ansatz_accuracy} presents the ansatz structure for each pattern, along with the classification accuracy when performing class prediction on test images using the optimized parameters obtained during training.
\begin{table*}[t]
\caption{Performance comparison of different quantum circuit ansatze (Circuits 1–5) used in the QAE classification model. Metrics include the number of parameters and test accuracy for different repetition counts ($M$)}\label{tab:ansatz_accuracy}%
\centering
\begin{tabular}{@{}llll@{}}
\toprule
Ansatz & $M$ & Parameters & Test accuracy \\
\midrule
Circuit 1 & 20 & 168 & 80.6\% \\
Circuit 2 & 20 & 168 & 87.6\% \\
Circuit 3 & 20 & 168 & 90.4\% \\
Circuit 4 & 6 & 168 & 82.0\% \\
Circuit 5 & 3 & 216 & 73.6\% \\
\botrule
\end{tabular}
\end{table*}
Circuit 3 achieves the highest accuracy. Although this circuit employs the same real-amplitude circuit as Circuits 1 and 2, it differs in that the CNOT gate entanglement is circular. Furthermore, in Circuit 3, all qubit interactions are established exclusively through CNOT gates, whereas Circuits 4 and 5 use Controlled-RY gates for entanglement, allowing parameter-controlled rotation angle adjustments. Despite this difference, the ansatz structure of Circuit 3 results in the highest accuracy.

Table~\ref{tab:ansatz_8class} presents the results of applying Circuit 3 to an eight-class image-classification task.
\begin{table}[t]
\caption{Test accuracies of QAE classification using Circuit 3 under various experimental conditions: number of classes, repetitions ($M$), parameters, training data size, image resolution, and latent qubit count}\label{tab:ansatz_8class}%
\begin{tabular}{@{}lllllll@{}}
\toprule
Classes & $M$ & Parameters & Training data & Image size & Latent qubits & Test accuracy \\
\midrule
4 & 20 & 168 & 500 & $16\times16$ & 5 & 90.4\% \\
8 & 20 & 168 & 500 & $16\times16$ & 5 & 49.4\% \\
8 & 30 & 248 & 500 & $16\times16$ & 5 & 55.2\% \\
8 & 20 & 168 & 800 & $16\times16$ & 5 & 63.6\% \\
8 & 20 & 210 & 500 & $32\times32$ & 7 & 56.4\% \\
\botrule
\end{tabular}
\end{table}
The initial conditions for the QAE classification circuit, including the training and test set sizes and configurations, remain unchanged, except that new data from classes four to seven are introduced into the dataset. As expected, the test accuracy decreases significantly. To improve accuracy, three approaches were implemented:
\begin{enumerate}
\item Increasing the number of ansatz repetitions from 20 to 30, thereby increasing the number of optimized parameters.
\item Expanding the training dataset from 500 to 800 images.
\item Expanding the input image size from $16 \times 16$ to $32 \times 32$.
\end{enumerate}
All modifications improved the test accuracy compared to the initial eight-class results; however, the accuracy did not reach the level achieved in the four-class classification task. In the third modification, the input image resolution was increased, thereby increasing the number of qubits in the latent state. The previous input image size of $16 \times 16$ was expanded to $32 \times 32$. Because $32 \times 32 = 2^{10}$, the number of qubits in the AB quantum system increased to ten. With the trash state remaining at $l = 3$ qubits, the latent state could be expanded to $k = 7$ qubits. This expansion was expected to capture more class-specific features owing to the increased dimensionality of the latent space. However, there was no significant improvement in test accuracy away from expectations.

A comparative analysis of four-class classification was conducted between QAE classification and conventional machine-learning methods, including FCNN, NBMF, and SVD. The same training and test datasets used in the QAE classification experiments were utilized. For FCNN and NBMF, parameter optimization was performed for 5000 epochs, similar to that in QAE classification. In contrast, SVD performed a single matrix factorization. Table~\ref{tab:other_methods} presents the number of parameters optimized per image for each method and the test accuracy obtained from the training results.
\begin{table*}[t]
\caption{Comparison of different classification methods regarding the number of parameters and test accuracy. The methods include QAE classification, Fully Connected Neural Network (FCNN), Nonnegative Binary Matrix Factorization (NBMF), and Singular Value Decomposition (SVD)}\label{tab:other_methods}%
\centering
\begin{tabular}{@{}lll@{}}
\toprule
Method & Parameters & Test accuracy \\
\midrule
QAE classification & 168 & 90.4\% \\
FCNN & 8356 & 95.0\% \\
NBMF & 8480 & 91.0\% \\
SVD & 9504 &  83.2\%\\
\botrule
\end{tabular}
\end{table*}
The test accuracy of the QAE classification surpassed that of SVD. Additionally, FCNN achieved the highest test accuracy, consistent with the well-established effectiveness of neural networks in image classification. A significant difference between the proposed method and conventional approaches is the number of optimized parameters. The parameter reduction rates achieved by QAE classification are as follows:
\begin{itemize}
\item 98.00\% compared to FCNN
\item 98.02\% compared to NBMF
\item 98.23\% compared to SVD
\end{itemize}
Despite this significant parameter reduction, QAE classification achieves higher accuracy than SVD while maintaining only 0.6 and 4.6\% accuracy reductions compared to NBMF and FCNN, respectively.


\section{Discussion}
In the QAE classification circuit, image-classification experiments were conducted using five different ansatz structures, wherein Circuit 3 achieved the highest test accuracy. To understand this result, we first examine the relationship between the classification results for each ansatz and the characteristics of the test data. Principal component analysis (PCA) was applied to reduce the dimensionality of the test image data to a two-dimensional feature vector representation. Figure~\ref{fig:pca_circ_1_to_5} plots the data points for each ansatz structure, class information, and classification results.
\begin{figure}[t]
\centering
% 1行目（3つの画像）
\begin{subfigure}{0.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{qae-paper-figure/Fig7a.jpg}
    \caption{Circuit 1}
    \label{fig:pca_circ1}
\end{subfigure}
\begin{subfigure}{0.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{qae-paper-figure/Fig7b.jpg}
    \caption{Circuit 2}
    \label{fig:pca_circ2}
\end{subfigure}
\begin{subfigure}{0.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{qae-paper-figure/Fig7c.jpg}
    \caption{Circuit 3}
    \label{fig:pca_circ3}
\end{subfigure}
\vspace{10pt}
% 2行目（2つの画像）
\begin{subfigure}{0.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{qae-paper-figure/Fig7d.jpg}
    \caption{Circuit 4}
    \label{fig:pca_circ4}
\end{subfigure}
\begin{subfigure}{0.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{qae-paper-figure/Fig7e.jpg}
    \caption{Circuit 5}
    \label{fig:pca_circ5}
\end{subfigure}
\caption{Principal component analysis (PCA) distributions for each ansatz (Circuits 1–5). Colors represent classes: Class 0 (blue), Class 1 (orange), Class 2 (green), and Class 3 (red). Circles indicate correct classification, whereas crosses indicate misclassification}
\label{fig:pca_circ_1_to_5}
\end{figure}
The color of each data point represents its class, as shown in the legend. Circles indicate correct classification, whereas crosses signify misclassification. For Circuit 3, most misclassified data points lie near the class boundaries in the feature space. This suggests that these data points contain ambiguous features, making them difficult to distinguish between classes. The misclassification likely stems not from the QAE classification mechanism or the structure of Circuit 3 itself but rather from the inherent complexity of the data, which would also be challenging for conventional machine-learning methods. In contrast, for other ansatz structures, misclassifications occur not only at class boundaries but also for data points with seemingly distinct class-specific features. Adopting the structure of Circuit 3 improves classification accuracy for data that other structures fail to handle.

In this experiment, class prediction involved inputting quantum states created from test data into the trained ansatz and measuring the qubits of the trash state. The quantum state of the trash state with the highest prediction probability was adopted as the class prediction. The closer this highest probability was to 1, the higher the confidence in the class prediction. Using Circuit 3 as the ansatz structure is expected to yield predictions with higher confidence than other structures. Logarithmic loss is used to quantitatively evaluate this. Logarithmic loss, $L$, serves as a metric for assessing how well predicted probabilities align with the correct class and is defined by the following equation:
\begin{equation}
L = -\frac{1}{N} \sum_{i=1}^N \sum_{k=1}^C y_{ik} \log(p_{ik}),
\label{eq:log_loss}
\end{equation}
where $N$ represents the number of test data points and $C$ denotes the number of classes. The variable $y_{ik}$ is set to 1 if test data $i$ belong to class $k$, and zero otherwise. $p_{ik}$ represents the predicted probability that test data $i$ belong to class $k$. If the test data belong to class $k$ but are predicted with low confidence, $\log p_{ik}$ takes a large negative value. Frequent occurrences of such cases lead to an increased $L$. Conversely, correct predictions with high confidence result in a small $L$. Table~\ref{tab:log_loss} presents the logarithmic loss results obtained from the QAE classification of the test data using each ansatz.
\begin{table*}[t]
\caption{Logarithmic loss for different quantum circuit ansatze (Circuits 1–5). Lower values indicate better predictive performance}\label{tab:log_loss}%
\centering
\begin{tabular}{@{}ll@{}}
\toprule
Ansatz & Logarithmic loss \\
\midrule
Circuit 1 & 0.8513 \\
Circuit 2 & 0.8397 \\
Circuit 3 & 0.8334 \\
Circuit 4 & 0.8499 \\
Circuit 5 & 0.9381 \\
\botrule
\end{tabular}
\end{table*}
The logarithmic loss for Circuit 3 is the smallest among all structures, indicating that Circuit 3 provides more confident and accurate predictions than the other ansatz structures. Despite using the same dataset and a nearly identical number of optimized parameters, modifying the combination of quantum gates within the ansatz enhances prediction performance.

To investigate the impact of different quantum-gate combinations, we analyzed the expressibility of each ansatz—a numerical metric quantifying the range of quantum states a circuit can represent\citep{Sim_2019, Hubregtsen_2021}. In principle, a highly expressive quantum circuit can produce any quantum state with nearly equal probability. This ideal distribution is approximated by the ensemble of Haar-random states, a uniform distribution over all possible quantum states. Therefore, expressibility is evaluated by calculating the Kullback–Leibler (KL) divergence between the distribution of output states generated by the ansatz and that of pure states under the Haar measure. Lower KL divergence values, approaching zero, indicate higher expressibility, meaning the circuit can access a broader region of the Hilbert space. Because prediction confidence varies with the quantum-gate configuration, we hypothesized that the diversity of representable quantum states also differs between ansatze. Consequently, we measured the expressibility of each ansatz. For this measurement, 1024 patterns of randomly generated, normalized real values were used as input. This input data size matched the image dimensions used in the experiment ($16 \times 16$), and these values were converted to quantum states via amplitude encoding. These encoded states were then input into each ansatz, using the parameters previously optimized over 5000 training epochs. The expressibility was then calculated from the resulting output states. This process was repeated five times with different sets of 1024 random input values, and the mean expressibility was calculated for each ansatz. Table~\ref{tab:exprs} presents the results, comparing expressibility across the different ansatz structures.
\begin{table*}[h]
\caption{Expressibility of different quantum circuit ansatze (Circuits 1–5). Expressibility measures how well a quantum circuit can explore the Hilbert space, with lower values indicating a broader range of representable quantum states}\label{tab:exprs}%
\centering
\begin{tabular}{@{}ll@{}}
\toprule
Ansatz & Expressibility \\
\midrule
Circuit 1 & 0.749451 \\
Circuit 2 & 0.749445 \\
Circuit 3 & 0.749403 \\
Circuit 4 & 0.749420 \\
Circuit 5 & 0.749612 \\
\botrule
\end{tabular}
\end{table*}
Among the tested ansatze, Circuit 3 exhibited the lowest expressibility value, suggesting it possesses the broadest capacity torepresent diverse quantum states. The key structural distinction between Circuits 3 and 2 lies in a single CNOT gate: Circuit 3 includes a CNOT gate that controls the first qubit from the last qubit, creating a circular connection. This seemingly minor modification results in a substantial difference in expressibility. Furthermore, Circuit 3, which relies solely on CNOT gates for qubit control, demonstrates higher expressibility than Circuits 4 and 5, which utilize parameterized rotation gates. This implies that a wider range of quantum states can be effectively represented using CNOT gates alone, without the need for fine-grained angle adjustments via continuous parameters.

Extending the QAE classification model from the four-class task (where it achieved 90.4\% test accuracy) to an eight-class task resulted in the test accuracy significantly dropping to 49.4\%. Initial attempts to improve performance mirrored common strategies in traditional machine learning: increasing the number of training samples, optimized parameters (by increasing ansatz repetitions), and the number of qubits in the latent space. While these modifications led to a modest improvement in test accuracy, they were insufficient to recover the performance levels observed in the four-class scenario. This suggests that, similar to classical machine learning, expanding the diversity of the feature space is beneficial but not a complete solution. 
It points to the need for more sophisticated improvements, such as refining the data encoding method or fundamentally restructuring the ansatz. Figure~\ref{fig:prediction_heatmap} depicts the confusion matrix for the eight-class classification task after increasing the number of training samples (resulting in a test accuracy of 63.6\%).
\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{qae-paper-figure/Fig8.jpg}
\caption{Confusion matrix for the eight-class classification task, showing predicted versus true labels. The matrix provides a detailed breakdown of correct and misclassified instances, wherein darker blue boxes indicate more samples in that cell. Diagonal elements represent correct classifications, whereas off-diagonal elements indicate misclassifications}\label{fig:prediction_heatmap}
\end{figure}
The vertical axis represents the true class of the test data, whereas the horizontal axis indicates the predicted class output by the QAE. Each cell's value represents the number of samples belonging to the true class (vertical axis) that were classified as the predicted class (horizontal axis).
Analysis of the confusion matrix (Figure~\ref{fig:prediction_heatmap}) reveals that certain digit classes exhibit significantly lower classification accuracy, suggesting a weakness in recognizing specific numerical shapes. This issue likely arises because the QAE classification architecture, as implemented in this experiment, fails to adequately capture the relevant features of these digit shapes. One contributing factor could be the data encoding process. Amplitude encoding, used in this experiment, represents pixel intensities but disregards the spatial relationships between pixels. An alternative approach, flexible representation of quantum images (FRQI), incorporates positional information alongside pixel values during quantum encoding. However, FRQI necessitates prelearning circuit parameters for each image, drastically increasing encoding time compared to amplitude encoding. Thus, even if FRQI-based encoding were to improve test accuracy, its practical applicability is limited by this substantial computational overhead. Another potential avenue for improving digit shape recognition involves leveraging classical preprocessing techniques before QAE classification. For instance, PCA could be used to transform the raw image data into principal components, capturing the dominant features. Encoding these PCA-extracted features, rather than the raw pixel values, into quantum states might allow the QAE to begin with a more structured and informative representation of the digit shapes, potentially boosting classification accuracy. However, this approach introduces a classical preprocessing step, compromising the end-to-end quantum nature of the QAE classification and making it a less desirable solution from the perspective of exploring purely quantum methods. Additionally, the relative simplicity of the ansatz structure in Circuit 3, while beneficial for expressibility, might limit its ability to capture the intricate features of certain digit shapes. Modifying or enhancing the qubit interactions within the ansatz—perhaps by exploring different connectivity patterns or incorporating more complex gate sequences—could improve feature extraction for these challenging classes. However, excessively increasing the number of parameters or introducing deeply hierarchical ansatz structures could negatively impact learning efficiency. Therefore, a careful balance between enhancing representational power and maintaining computational tractability is crucial when considering such modifications.

As confirmed in the previous section, comparing QAE classification with classical machine-learning methods highlights a key advantage: QAE classification achieves comparable test accuracy while substantially reducing the number of optimized parameters. The initial 256-dimensional classical image data ($16 \times 16$ pixels) is compressed into an eight-qubit quantum state in the QAE (and further down to a five-qubit latent space). In contrast, the conventional methods employed 32-dimensional feature vectors. Despite this significant reduction in both parameters and the dimensionality of the feature representation, QAE classification maintains high accuracy. This strongly suggests that QCL, even with a relatively simple ansatz, can efficiently extract essential features for classification. By transferring the nonlocal observable to a single-qubit observable using entangling gate such as the controlled-NOT gate, QCL efficiently extracts high-order polynomial terms into a measurable form\citep{Mitarai_2018}. The tensor product structure of quantum systems readily calculates the product of input states, which may enable learning of complex functions with few parameters. While NBMF, another comparative method, utilizes annealing-based (potentially quantum-accelerated) computation, the results of this study demonstrate the effectiveness of gate-based QCL, indicating the broader potential of quantum algorithms in the machine-learning pipeline. However, it is important to acknowledge that the classical FCNN achieves superior test accuracy. Moreover, from a practical standpoint, FCNNs operating on classical hardware are far more accessible and resource-efficient than QAE classification, which relies on quantum simulators or, ideally, future fault-tolerant quantum computers. Additionally, training convergence is significantly faster with classical methods. Among the classical techniques, SVD achieved test accuracy comparable to QAE classification using only a single matrix factorization, underscoring its efficiency. Nevertheless, the current computational disadvantage of QCL may diminish with future advancements in gate-based quantum-computing hardware.

\section{Conclusion}
This study introduced a novel approach to image classification using QAEs, extending their applicability beyond conventional data compression and reconstruction tasks. The results demonstrated that QAE-based classification achieved high accuracy in a four-class classification task, comparable to conventional machine-learning methods, while substantially reducing the number of parameters requiring optimization. Furthermore, it highlighted the crucial role of the quantum-gate structure within the ansatz, emphasizing the importance of circuit design in QML. These findings contribute to the advancement of QML by demonstrating that QAEs can serve as efficient and compact classification models. Unlike hybrid quantum-classical approaches such as QCNNs, this study provides insights into leveraging purely quantum circuits for end-to-end learning, showcasing the potential advantages of a fully quantum approach.

However, the study also revealed certain limitations. The current QAE classification architecture struggled to maintain high accuracy when scaled beyond a four-class problem. Moreover, owing to current hardware constraints in gate-based quantum computing, training times are significantly longer than those of conventional machine-learning models. These factors currently limit the practical applicability of the proposed method. Future research will focus on developing new ansatz structures and data encoding techniques that enable high-accuracy, multiclass classification while preserving the parameter efficiency of QAEs. Balancing the trade-off between accuracy and computational efficiency will be paramount. As quantum hardware progresses, it will be crucial to reassess the performance of QAE-based classification against conventional approaches. The key objective is to demonstrate superior efficiency and scalability in real-world applications. Addressing these challenges will lay the groundwork for developing practical, scalable, and ultimately advantageous quantum machine-learning models, thereby realizing the full potential of quantum computing in classification tasks.


\backmatter

\bmhead{Acknowledgements}
This study was partially supported by JSPS KAKENHI Grant Number JP23H04499.
This study was partially supported by the Ochanomizu University Graduate Student Research Grant for FY2024.
The authors thank Editage (www.editage.jp) for English language editing.

\bmhead{Author contributions}
H.A. conceived and conducted the experiments and analyzed the results.
K.K. supervised the project.
All the authors wrote and reviewed the manuscript.

\section*{Declarations}

\bmhead{Conflict of interest}
The authors declare no competing interests.


%\bibliographystyle{plainnat}
\bibliography{sn-bibliography}% common bib file
%% if required, the content of .bbl file can be included here once bbl is generated
%%\input sn-article.bbl


\end{document}
