\section{Related Work}
\label{sec:related}
\noindent\textbf{Simulation-to-Reality Transfer.} Two main approaches have been proposed for overcoming the dynamics gap between simulation and reality:  1) adapting simulation parameters to real-world data ~\citep{chebotar19close, ramos19bayessim, memmel24asid} and 2) learning adaptive or robust policies to account for uncertain real-world dynamics ~\citep{haozhirma, rma, yu17prep}. However, these approaches still display failure modes in regimes where simulation and reality diverge substantially \cite{smith2022walk} -- namely, where no set of simulation parameters closely match the real-world dynamics. 


\noindent\textbf{Adapting Policies with Real-World Data.} 
Many RL approaches have focused on the general fine-tuning problem~\citep{Rajeswaran-RSS-18, nair2020awac, kostrikov2021offline, hu2023imitation, nakamoto2024cal}. These works initialize policies, Q-functions, and replay buffers from offline data and continue training them with standard RL methods. A number of works have specifically considered mixing simulated and real data during policy optimization -- either through co-training~\citep{torne24rialto}, simply initializing the replay-buffer with simulation data \citep{smith2022walk, ball2023efficient}, or adaptively sampling the simulated dataset and up-weighting transitions that approximately match the true dynamics~\citep{eysenbach2020off, liu2022dara, xu2023cross, niu2022trust}. However, recent work \cite{zhou2024efficient} has demonstrated that there are minimal benefits to sampling off-domain samples when adapting online in new environments, as this can bias learned policies towards sub-optimal solutions. In contrast to these prior approaches -- which primarily use simulated experience to initialize real-world learning -- we focus on distilling effective exploration strategies from simulation, using $V_{sim}$ to guide real-world learning. We demonstrate theoretically that this approach to transfer leads to low bias in the policies learned by \Method. 
\begin{comment}
. While going back from the real world to simulation can help target the simulation parameters more accurately~\citep{chebotar19close, ramos19bayessim, memmel24asid}, it cannot overcome inherent model misspecification, as we show in our experimental evaluation. Learning adaptive policies to account for changing real-world dynamics~\citep{haozhirma, rma, yu17prep} can help to some extent but is unable to guide exploration and adapt beyond the training dynamics range.\end{comment} Prior work has additionally considered mixing simulated and real data during policy optimization -- either through co-training~\citep{torne24rialto}, simply initializing the replay-buffer with simulation data \citep{smith2022walk, ball2023efficient}, or adaptively sampling the simulated dataset and up-weighting transitions that approximately match the true dynamics~\citep{eysenbach2020off, liu2022dara, xu2023cross, niu2022trust}. However, recent work \cite{zhou2024efficient} has demonstrated that there are minimal benefits to sampling off-domain samples when adapting online in new environments, as it introduces bias into the learned policy.  In contrast, our approach focuses on distilling effective \emph{guidance for exploration} from simulation, and which we demonstrate theoretically leads to low-bias in the policies learned by \Method . 

\noindent\textbf{Adapting Policies with Real-World Data.} 
Many RL approaches have focused on the general fine-tuning problem~\citep{Rajeswaran-RSS-18, nair2020awac, kostrikov2021offline, hu2023imitation, nakamoto2024cal}. These works initialize policies, Q-functions, and replay buffers from offline data and continue training them with standard RL methods, but do not use the pre-training data beyond initialization and populating the replay buffer. We utilize the pretraining in simulation not just for initialization but also to provide guidance \emph{throughout} the real-world policy improvement process. 

% Moreover, the learned value functions and models in our proposed methodology have coverage over state-space as opposed to limited coverage pre-training methods with offline RL and imitation learning. 

\noindent\textbf{Reward Design in Reinforcement Learning.} 
A significant component of our methodology is learning dense shaped reward in simulation to guide real-world fine-tuning. Prior techniques have tried to infer rewards from expert demos~\citep{ziebart2008maxentirl, ho2016generative}, success examples~\citep{fu2018variational,li2021mural}, LLMs~\citep{ma2023eureka, yu2023language}, and heuristics~\citep{margolisrapid, dota2}. We rely on simulation to provide reward supervision using the PBRS formalism~\citep{ng1999policy}. This effectively encodes information both about the dynamics of the simulator and optimal behaviors in the simulated environment. This enables use to shorten the learning horizon \citep{cheng2021heuristic,westenbroek2022lyapunov}, which reduces the sample complexity of obtaining an effective policy in the real-world. 
\begin{comment}
Another line of work complementary to our comes from \cite{eysenbach2020off, liu2022dara, xu2023cross, niu2022trust}, which relabel rewards from off-task (simulated) data, effectively up-weighting transitions that approximately match the dynamics observed in the target domain. These works focus on the \emph{retrieval} of useful samples from prior datasets with shifted dynamics. In contrast, our approach uses prior data to guide the discovery of novel sequences of states and actions in the target domain. In principle, these techniques could be used in conjunction; we leave this to future work. 
\end{comment}
% \twnote{maybe mention we can use these together?} 



\noindent\textbf{Model-Based Reinforcement Learning.} A significant body of work on model-based RL learns a generative dynamics models to accelerate policy optimization~\citep{sutton1991dyna, wang2019exploring, janner2019trust, yu2020mopo, kidambi2020morel, ebert2018visual,zhang2019solar}. In principle, a model enables the agent to make predictions about states and actions not contained in the training, enabling rapid adaptation to new situations. However, the central challenge for model-based methods is that small inaccuracies in predictive models can quickly compound over time, leading to large \emph{model-bias} and a drop in controller performance. An effective critic can be used to shorten search horizons~\citep{hansen2024tdmpc, bhardwaj2020blending, hafner2019learning, jadbabaie2001unconstrained, grune2008infinite} yielding easier decision-making problems, but learning such a critic from scratch can still require large amounts of on-task data. We demonstrate that, for many real-world continuous control problems, critics learned entirely in simulation can be robustly transferred to the real-world and substantially accelerate model-based learning.  

\noindent\textbf{Reward Design in Reinforcement Learning.} 
A significant component of our methodology is learning dense shaped reward in simulation to guide real-world fine-tuning. Prior techniques have tried to infer rewards from expert demos~\citep{ziebart2008maxentirl, ho2016generative}, success examples~\citep{fu2018variational,li2021mural}, LLMs~\citep{ma2023eureka, yu2023language}, and heuristics~\citep{margolisrapid, dota2}. We rely on simulation to provide reward supervision \cite{westenbroek2022lyapunov} using the PBRS formalism~\citep{ng1999policy}. This effectively encodes information about the dynamics and optimal behaviors in the simulator, enabling us to shorten the learning horizon and improve sample efficiency \cite{cheng2021heuristic, westenbroek2022lyapunov}.