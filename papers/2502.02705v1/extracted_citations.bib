@INPROCEEDINGS{Rajeswaran-RSS-18,
    AUTHOR    = {Aravind Rajeswaran AND Vikash Kumar AND Abhishek Gupta AND
                 Giulia Vezzani AND John Schulman AND Emanuel Todorov AND Sergey Levine},
    TITLE     = "{Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations}",
    BOOKTITLE = {Proceedings of Robotics: Science and Systems (RSS)},
    YEAR      = {2018},
}

@inproceedings{ball2023efficient,
  title={Efficient online reinforcement learning with offline data},
  author={Ball, Philip J and Smith, Laura and Kostrikov, Ilya and Levine, Sergey},
  booktitle={International Conference on Machine Learning},
  pages={1577--1594},
  year={2023},
  organization={PMLR}
}

@article{bhardwaj2020blending,
  title={Blending mpc \& value function approximation for efficient reinforcement learning},
  author={Bhardwaj, Mohak and Choudhury, Sanjiban and Boots, Byron},
  journal={arXiv preprint arXiv:2012.05909},
  year={2020}
}

@inproceedings{chebotar19close,
  author       = {Yevgen Chebotar and
                  Ankur Handa and
                  Viktor Makoviychuk and
                  Miles Macklin and
                  Jan Issac and
                  Nathan D. Ratliff and
                  Dieter Fox},
  title        = {Closing the Sim-to-Real Loop: Adapting Simulation Randomization with
                  Real World Experience},
  booktitle    = {ICRA},
  year         = {2019},
}

@article{cheng2021heuristic,
  title={Heuristic-guided reinforcement learning},
  author={Cheng, Ching-An and Kolobov, Andrey and Swaminathan, Adith},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={13550--13563},
  year={2021}
}

@article{dota2,
  author       = {Christopher Berner and
                  Greg Brockman and
                  Brooke Chan and
                  Vicki Cheung and
                  Przemyslaw Debiak and
                  Christy Dennison and
                  David Farhi and
                  Quirin Fischer and
                  Shariq Hashme and
                  Christopher Hesse and
                  Rafal J{\'{o}}zefowicz and
                  Scott Gray and
                  Catherine Olsson and
                  Jakub Pachocki and
                  Michael Petrov and
                  Henrique Pond{\'{e}} de Oliveira Pinto and
                  Jonathan Raiman and
                  Tim Salimans and
                  Jeremy Schlatter and
                  Jonas Schneider and
                  Szymon Sidor and
                  Ilya Sutskever and
                  Jie Tang and
                  Filip Wolski and
                  Susan Zhang},
  title        = {Dota 2 with Large Scale Deep Reinforcement Learning},
  journal      = {CoRR},
  volume       = {abs/1912.06680},
  year         = {2019},
  url          = {http://arxiv.org/abs/1912.06680},
  eprinttype    = {arXiv},
  eprint       = {1912.06680},
  timestamp    = {Wed, 03 Jun 2020 10:56:28 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1912-06680.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{ebert2018visual,
  title={Visual foresight: Model-based deep reinforcement learning for vision-based robotic control},
  author={Ebert, Frederik and Finn, Chelsea and Dasari, Sudeep and Xie, Annie and Lee, Alex and Levine, Sergey},
  journal={arXiv preprint arXiv:1812.00568},
  year={2018}
}

@article{eysenbach2020off,
  title={Off-dynamics reinforcement learning: Training for transfer with domain classifiers},
  author={Eysenbach, Benjamin and Asawa, Swapnil and Chaudhari, Shreyas and Levine, Sergey and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:2006.13916},
  year={2020}
}

@article{fu2018variational,
  title={Variational inverse control with events: A general framework for data-driven reward definition},
  author={Fu, Justin and Singh, Avi and Ghosh, Dibya and Yang, Larry and Levine, Sergey},
  journal={NeurIPS},
  year={2018}
}

@article{grune2008infinite,
  title={On the infinite horizon performance of receding horizon controllers},
  author={Grune, Lars and Rantzer, Anders},
  journal={IEEE Transactions on Automatic Control},
  volume={53},
  number={9},
  pages={2100--2111},
  year={2008},
  publisher={IEEE}
}

@inproceedings{hafner2019learning,
  title={Learning latent dynamics for planning from pixels},
  author={Hafner, Danijar and Lillicrap, Timothy and Fischer, Ian and Villegas, Ruben and Ha, David and Lee, Honglak and Davidson, James},
  booktitle={International conference on machine learning},
  pages={2555--2565},
  year={2019},
  organization={PMLR}
}

@inproceedings{haozhirma,
  author       = {Haozhi Qi and
                  Ashish Kumar and
                  Roberto Calandra and
                  Yi Ma and
                  Jitendra Malik},
  title        = {In-Hand Object Rotation via Rapid Motor Adaptation},
  booktitle    = {CoRL},
  year         = {2022},
  url          = {https://proceedings.mlr.press/v205/qi23a.html}
}

@article{ho2016generative,
  title={Generative adversarial imitation learning},
  author={Ho, Jonathan and Ermon, Stefano},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{hu2023imitation,
  title={Imitation bootstrapped reinforcement learning},
  author={Hu, Hengyuan and Mirchandani, Suvir and Sadigh, Dorsa},
  journal={arXiv preprint arXiv:2311.02198},
  year={2023}
}

@article{jadbabaie2001unconstrained,
  title={Unconstrained receding-horizon control of nonlinear systems},
  journal={IEEE Transactions on Automatic Control},
  Author = {Ali Jadbabaie and  Jie Yu and and John Hauser},
  volume={46},
  number={5},
  pages={776--783},
  year={2001},
  publisher={IEEE}
}

@article{janner2019trust,
  title={When to trust your model: Model-based policy optimization},
  author={Janner, Michael and Fu, Justin and Zhang, Marvin and Levine, Sergey},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{kidambi2020morel,
  title={Morel: Model-based offline reinforcement learning},
  author={Kidambi, Rahul and Rajeswaran, Aravind and Netrapalli, Praneeth and Joachims, Thorsten},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={21810--21823},
  year={2020}
}

@inproceedings{kostrikov2021offline,
  title={Offline Reinforcement Learning with Implicit Q-Learning},
  author={Kostrikov, Ilya and Nair, Ashvin and Levine, Sergey},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{li2021mural,
  title={Mural: Meta-learning uncertainty-aware rewards for outcome-driven reinforcement learning},
  author={Li, Kevin and Gupta, Abhishek and Reddy, Ashwin and Pong, Vitchyr H and Zhou, Aurick and Yu, Justin and Levine, Sergey},
  booktitle={ICML},
  year={2021}
}

@article{liu2022dara,
  title={Dara: Dynamics-aware reward augmentation in offline reinforcement learning},
  author={Liu, Jinxin and Zhang, Hongyin and Wang, Donglin},
  journal={arXiv preprint arXiv:2203.06662},
  year={2022}
}

@article{ma2023eureka,
  title={Eureka: Human-level reward design via coding large language models},
  author={Ma, Yecheng Jason and Liang, William and Wang, Guanzhi and Huang, De-An and Bastani, Osbert and Jayaraman, Dinesh and Zhu, Yuke and Fan, Linxi and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2310.12931},
  year={2023}
}

@article{margolisrapid,
  author       = {Gabriel B. Margolis and
                  Ge Yang and
                  Kartik Paigwar and
                  Tao Chen and
                  Pulkit Agrawal},
  title        = {Rapid locomotion via reinforcement learning},
  journal      = {Int. J. Robotics Res.},
  volume       = {43},
  number       = {4},
  pages        = {572--587},
  year         = {2024},
  url          = {https://doi.org/10.1177/02783649231224053},
  doi          = {10.1177/02783649231224053},
  timestamp    = {Sun, 02 Jun 2024 13:06:46 +0200},
  biburl       = {https://dblp.org/rec/journals/ijrr/MargolisYPCA24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{memmel24asid,
  author       = {Marius Memmel and
                  Andrew Wagenmaker and
                  Chuning Zhu and
                  Patrick Yin and
                  Dieter Fox and
                  Abhishek Gupta},
  title        = {{ASID:} Active Exploration for System Identification in Robotic Manipulation},
  journal      = {CoRR},
  volume       = {abs/2404.12308},
  year         = {2024},
}

@article{nair2020awac,
  title={Awac: Accelerating online reinforcement learning with offline datasets},
  author={Nair, Ashvin and Gupta, Abhishek and Dalal, Murtaza and Levine, Sergey},
  journal={arXiv preprint arXiv:2006.09359},
  year={2020}
}

@article{nakamoto2024cal,
  title={Cal-ql: Calibrated offline rl pre-training for efficient online fine-tuning},
  author={Nakamoto, Mitsuhiko and Zhai, Simon and Singh, Anikait and Sobol Mark, Max and Ma, Yi and Finn, Chelsea and Kumar, Aviral and Levine, Sergey},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{ng1999policy,
  title={Policy invariance under reward transformations: Theory and application to reward shaping},
  author={Ng, Andrew Y and Harada, Daishi and Russell, Stuart},
  booktitle={Icml},
  volume={99},
  pages={278--287},
  year={1999}
}

@article{niu2022trust,
  title={When to trust your simulator: Dynamics-aware hybrid offline-and-online reinforcement learning},
  author={Niu, Haoyi and Qiu, Yiwen and Li, Ming and Zhou, Guyue and Hu, Jianming and Zhan, Xianyuan and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={36599--36612},
  year={2022}
}

@inproceedings{ramos19bayessim,
  author       = {Fabio Ramos and
                  Rafael Possas and
                  Dieter Fox},
  title        = {BayesSim: Adaptive Domain Randomization Via Probabilistic Inference
                  for Robotics Simulators},
  booktitle    = {RSS},
  year         = {2019}
}

@inproceedings{rma,
  author       = {Ashish Kumar and
                  Zipeng Fu and
                  Deepak Pathak and
                  Jitendra Malik},
  title        = {{RMA:} Rapid Motor Adaptation for Legged Robots},
  booktitle    = {RSS},
  year         = {2021}
}

@article{smith2022walk,
  title={A walk in the park: Learning to walk in 20 minutes with model-free reinforcement learning},
  author={Smith, Laura and Kostrikov, Ilya and Levine, Sergey},
  journal={arXiv preprint arXiv:2208.07860},
  year={2022}
}

@article{sutton1991dyna,
author = {Sutton, Richard S.},
title = {Dyna, an integrated architecture for learning, planning, and reacting},
year = {1991},
issue_date = {Aug. 1991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {0163-5719},
url = {https://doi.org/10.1145/122344.122377},
doi = {10.1145/122344.122377},
abstract = {Dyna is an AI architecture that integrates learning, planning, and reactive execution. Learning methods are used in Dyna both for compiling planning results and for updating a model of the effects of the agent's actions on the world. Planning is incremental and can use the probabilistic and ofttimes incorrect world models generated by learning processes. Execution is fully reactive in the sense that no planning intervenes between perception and action. Dyna relies on machine learning methods for learning from examples---these are among the basic building blocks making up the architecture---yet is not tied to any particular method. This paper briefly introduces Dyna and discusses its strengths and weaknesses with respect to other architectures.},
journal = {SIGART Bull.},
month = {jul},
pages = {160–163},
numpages = {4}
}

@article{torne24rialto,
  author       = {Marcel Torne and
                  Anthony Simeonov and
                  Zechu Li and
                  April Chan and
                  Tao Chen and
                  Abhishek Gupta and
                  Pulkit Agrawal},
  title        = {Reconciling Reality through Simulation: {A} Real-to-Sim-to-Real Approach
                  for Robust Manipulation},
  journal      = {CoRR},
  volume       = {abs/2403.03949},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2403.03949},
  doi          = {10.48550/ARXIV.2403.03949},
  eprinttype    = {arXiv},
  eprint       = {2403.03949},
  timestamp    = {Mon, 29 Jul 2024 16:18:15 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2403-03949.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{wang2019exploring,
  title={Exploring model-based planning with policy networks},
  author={Wang, Tingwu and Ba, Jimmy},
  journal={arXiv preprint arXiv:1906.08649},
  year={2019}
}

@article{westenbroek2022lyapunov,
  title={Lyapunov design for robust and efficient robotic reinforcement learning},
  author={Westenbroek, Tyler and Castaneda, Fernando and Agrawal, Ayush and Sastry, Shankar and Sreenath, Koushil},
  journal={arXiv preprint arXiv:2208.06721},
  year={2022}
}

@article{xu2023cross,
  title={Cross-domain policy adaptation via value-guided data filtering},
  author={Xu, Kang and Bai, Chenjia and Ma, Xiaoteng and Wang, Dong and Zhao, Bin and Wang, Zhen and Li, Xuelong and Li, Wei},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={73395--73421},
  year={2023}
}

@inproceedings{yu17prep,
  author       = {Wenhao Yu and
                  Jie Tan and
                  C. Karen Liu and
                  Greg Turk},
  title        = {Preparing for the Unknown: Learning a Universal Policy with Online
                  System Identification},
  booktitle    = {RSS},
  year         = {2017},
  url          = {http://www.roboticsproceedings.org/rss13/p48.html}
}

@article{yu2020mopo,
  title={Mopo: Model-based offline policy optimization},
  author={Yu, Tianhe and Thomas, Garrett and Yu, Lantao and Ermon, Stefano and Zou, James Y and Levine, Sergey and Finn, Chelsea and Ma, Tengyu},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={14129--14142},
  year={2020}
}

@article{yu2023language,
  title={Language to rewards for robotic skill synthesis},
  author={Yu, Wenhao and Gileadi, Nimrod and Fu, Chuyuan and Kirmani, Sean and Lee, Kuang-Huei and Arenas, Montse Gonzalez and Chiang, Hao-Tien Lewis and Erez, Tom and Hasenclever, Leonard and Humplik, Jan and others},
  journal={arXiv preprint arXiv:2306.08647},
  year={2023}
}

@inproceedings{zhang2019solar,
  title={Solar: Deep structured representations for model-based reinforcement learning},
  author={Zhang, Marvin and Vikram, Sharad and Smith, Laura and Abbeel, Pieter and Johnson, Matthew and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={7444--7453},
  year={2019},
  organization={PMLR}
}

@article{zhou2024efficient,
  title={Efficient Online Reinforcement Learning Fine-Tuning Need Not Retain Offline Data},
  author={Zhou, Zhiyuan and Peng, Andy and Li, Qiyang and Levine, Sergey and Kumar, Aviral},
  journal={arXiv preprint arXiv:2412.07762},
  year={2024}
}

@inproceedings{ziebart2008maxentirl,
author = {Ziebart, Brian D. and Maas, Andrew and Bagnell, J. Andrew and Dey, Anind K.},
title = {Maximum entropy inverse reinforcement learning},
year = {2008},
isbn = {9781577353683},
publisher = {AAAI Press},
abstract = {Recent research has shown the benefit of framing problems of imitation learning as solutions to Markov Decision Problems. This approach reduces learning to the problem of recovering a utility function that makes the behavior induced by a near-optimal policy closely mimic demonstrated behavior. In this work, we develop a probabilistic approach based on the principle of maximum entropy. Our approach provides a well-defined, globally normalized distribution over decision sequences, while providing the same performance guarantees as existing methods.We develop our technique in the context of modeling real-world navigation and driving behaviors where collected data is inherently noisy and imperfect. Our probabilistic approach enables modeling of route preferences as well as a powerful new approach to inferring destinations and routes based on partial trajectories.},
booktitle = {Proceedings of the 23rd National Conference on Artificial Intelligence - Volume 3},
pages = {1433–1438},
numpages = {6},
location = {Chicago, Illinois},
series = {AAAI'08}
}

