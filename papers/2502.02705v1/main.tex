
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}
\usepackage{wrapfig}
\iclrfinalcopy
\usepackage{algorithmicx}
\usepackage[ruled]{algorithm}
\usepackage{algpseudocode}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs} 
\usepackage{cleveref}
\crefalias{prop}{proposition}



\title{\centering{Rapidly Adapting Policies to the Real World via Simulation-Guided Fine-Tuning}}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Patrick Yin$^{*,1}$, Tyler Westenbroek\thanks{ Equal contribution 1. University of Washington 2. Microsoft Research}  $^{,1}$,  Simran Bagaria$^1$, Kevin Huang$^1$, \\ \centering{\textbf{Ching-an Cheng}$^2$, \textbf{Andrey Kobolov}$^2$ \& \textbf{Abhishek Gupta}$^{1}$}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\chingan}[1]{\textcolor{blue}{Ching-An: {#1}}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{figure}[h]
    \captionsetup{width=.85\linewidth}
\begin{center}
    \includegraphics[width=0.8\linewidth]{figs/cover_figure.png}
    \vspace{-3mm}
    \caption{\footnotesize{Five dynamic, contact-rich manipulation tasks -- hammering (\textbf{top left}), insertion (\textbf{right}), and three pushing (\textbf{bottom left}) tasks -- solved in the real world using \Method.}}
    \label{fig:cover_fig}
    \vspace{-2mm}
\end{center}
\end{figure}

\begin{abstract}
Robot learning requires a considerable amount of high-quality data to realize the promise of generalization. However, large data sets are costly to collect in the real world. Physics simulators can cheaply generate vast data sets with broad coverage over states, actions, and environments. However, physics engines are fundamentally misspecified approximations to reality. This makes direct zero-shot transfer from simulation to reality challenging, especially in tasks where precise and force-sensitive manipulation is necessary. Thus, fine-tuning these policies with small real-world data sets is an appealing pathway for scaling robot learning. However, current reinforcement learning fine-tuning frameworks leverage general, unstructured exploration strategies which are too inefficient to make real-world adaptation practical. This paper introduces the \emph{Simulation-Guided Fine-tuning} (SGFT) framework, which demonstrates how to extract structural priors from physics simulators to substantially accelerate real-world adaptation. Specifically, our approach uses a value function learned in simulation to guide real-world exploration. We demonstrate this approach across five real-world dexterous manipulation tasks where zero-shot sim-to-real transfer fails. We further demonstrate our framework substantially outperforms baseline fine-tuning methods, requiring up to an order of magnitude fewer real-world samples and succeeding at difficult tasks where prior approaches fail entirely. Last but not least, we provide theoretical justification for this new paradigm which underpins how SGFT can rapidly learn high-performance policies in the face of large sim-to-real dynamics gaps. Project webpage: \href{https://weirdlabuw.github.io/sgft/}{weirdlabuw.github.io/sgft}

% Model-based reinforcement learning holds the promise of rapid adaptation in new environments, but breaks down in situations where the model is inaccurate. It is conceptually useful two distinguish to two broad paradigms which leverage models for robot reinforcement learning. Sim-to-real methods use approximate physics simulators to generate vast synthetic datasets covering a wide range of states and dynamics parameters. This ability to obtain \emph{data coverage} enables these approaches to synthesize globally coherent policies which are robust to some dynamics shifts, but can fail completely in scenarios where the simulator is fundamentally \emph{misspecified} with respect to reality. The alternative proposal is to train black-box neural-network models from scratch with real-world data, to be used either for data-augmentation or online planning. While these models can accurately model the dynamics near the training set, to be useful for controller synthesis they must have data coverage over regions visited by successful policies -- a difficult exploration problem which is impractical to solve for many real-world problems. This paper proposes \textbf{Simulation-Guided Fine-Tuning} (\Method), a general framework which leverages the strengths of both paradigms and be can built on top of any base MBRL algorithm. \Method\ uses a value function trained in simulation....
\end{abstract}

% \begin{wrapfigure}{r}{0.5\linewidth}
    % \begin{center}
    %     \includegraphics[width=0.8\linewidth]{figs/cover_figure.png}
    %     \caption{\footnotesize{Three dynamic, contact-rich manipulation tasks -- hammering (\textbf{top}), pushing (\textbf{bottom left}), and inserting (\textbf{bottom right}) -- solved in the real world using our method \Method.}}
        
    %     \vspace{-2mm}
    % \end{center}
% \end{wrapfigure}

\section{Introduction}

Robot learning offers a pathway to building robust, general-purpose robotic agents that can rapidly adapt their behavior to new environments and tasks. This shifts the burden from designing task-specific controllers by hand to the problem of collecting large behavioral datasets with sufficient coverage. This raises a fundamental question: \emph{how do we cheaply obtain and leverage such datasets at scale?} Real-world data collection via teleoperation~\citep{walke2023bridgedata, khazatsky2024droid} can generate high-quality trajectories but scales linearly with human effort. Even with community-driven teleoperation~\citep{mandlekar2018roboturk,embodimentcollaboration2024open}, current robotics datasets are orders of magnitude smaller than those powering vision and language applications. 
% \twnote{maybe talk about needing subopt data}


% Simulation as a solution to data coverage

Massively parallelized physics simulation \citep{mujoco,makoviychuk2021isaac} can cheaply generate vast synthetic robotics data sets. Indeed, generating data sets with extensive \emph{coverage} over environments, states, and actions can be largely automated using techniques such as automatic scene generation~\citep{chen2024urdformer, procthor}, dynamics randomization~\citep{peng2018sim, andrychowicz2020learning}, and search algorithms such as reinforcement learning. 

Unfortunately, simulation-generated data is not a silver bullet, as it provides cheap but ultimately \emph{off-domain data}. Namely, simulators are fundamentally \emph{misspecified} approximations to reality. This is highlighted in tasks like hammering in a nail, where the modeling of high-impact, deformable contact remains an open problem~\citep{acosta2022validating, levy2024learning}. In these regimes, \emph{no choice of parameters for the physics simulator accurately capture the real-world dynamics}. This gap persists despite efforts towards improving existing physics simulators with system identification~\citep{memmel24asid, huang23compass, levy2024learning}. Thus, despite impressive performance for many tasks, methods that transfer policies from simulation to reality zero-shot~\citep{rma, lee2020learning, peng2018sim, andrychowicz2020learning} run into failure modes when they encounter novel dynamics not covered by the simulator~\citep{smith22legged}. 


The question becomes: \textit{can inaccurate simulation models be useful in the face of fundamental misspecifications?} A natural technique is to fine-tune policies pre-trained in a simulator using real-world experience~\citep{smith22legged, cherrypick}. This approach can overcome misspecification by training directly on data from the target domain. However, existing approaches typically employ the unstructured exploration strategies used by standard, general reinforcement learning algorithms \cite{haarnojasac}. As a result, current RL fine-tuning frameworks remain too sample-inefficient for real-world deployment. We argue the following: despite getting the finer details wrong, physics simulators capture the rough structure of real-world dynamics well enough to \emph{transfer targeted exploration and adaptation strategies from simulation to reality}. 

This motivates the \emph{Simulation-Guided Fine-Tuning} (SGFT) framework, which uses a value function $V_{sim}$ learned in the simulator to transfer behavioral priors from simulation to reality. Our key conceptual insight is that the ordering defined by $V_{sim}$  captures successful behaviors -- such as reaching towards and object, picking it, and moving it to a desired position -- which are invariant across simulation and reality, even if the low-level dynamics diverge substantially in the two domains. In more detail, \Method\ departs from standard finetuning strategies by $1)$ using $V_{sim}$ to synthesize dense rewards to guide targeted real-world exploration and $2)$ shortening the learning horizon when adapting in the real-world. Prior approaches \cite{smith22legged, cherrypick} optimize the same infinite-horizon objective in both simulation and reality, and thus \emph{only use the simulator to initialize real-world learning}. These approaches often suffer from \emph{catastrophic forgetting} \cite{wolczyk2024fine}, where there is a substantial drop in policy performance early in the fine-tuning phase. Incorporating $V_{sim}$ into the reward enables the agent to retain structural information about the optimal behaviors learned in simulation throughout the fine-tuning processes, while shorter learning horizons are well-known to yield more trackable policy optimization problems \cite{westenbroek2022lyapunov, cheng2019predictor}. Altogether, \Method\ provides a stronger learning signal which enables base policy optimization algorithms to consistently and rapidly improve real-world performance. 


 Although \Method\ is a very general framework for sim-to-real fine-tuning, we place a special emphasis on implementations which use sample efficient model-based reinforcement learning algorithms \cite{janner2019trust, hansen2024tdmpc}. Model-based approaches typically struggle with long-horizon tasks due to compounding errors in model predictions \cite{janner2019trust}, but our horizon-shortening strategy conveniently side-steps this challenge, fully unlocking the performance benefits promised by generative world models. We outline our contributions as follows: 
 


 
 
 
\begin{figure}
\begin{center}
    \includegraphics[width=\textwidth]{figs/intro_figure.png}
    \vspace{-3mm}
    \caption{\footnotesize{Depiction of a model-based instantiation of \Method. While prior approaches optimize the same challenging infinite-horizon objective during simulation pretraining and real-world fine-tuning,   \Method\ modifies the fine-tuning objective by $a)$ using the value function learned in simulation $V_{sim}$ to reshape rewards and guide efficient real-world exploration $b)$ shortening the search horizon to make real-world learning more tractable. For the model-based instantiation depicted (and outlined in Section \ref{sec:models}), this amounts to branching short, exploratory model-based rollouts from real-world trajectories to search for sequences of actions which will improve policy performance. Intuitively, \Method\  uses the simulator to approximately bootstrap long-horizon rewards (via $V_{sim}$), while small amounts of real-world data are used to search for short sequences of optimal actions under the real-world dynamics. }}
    \label{fig:action_space}
    \vspace{-0.8cm}
\end{center}
\end{figure}


\begin{enumerate}
    \item We introduce the \Method\ framework, which requires modifying only a few lines of code in existing sim-to-real RL finetuning frameworks \cite{smith22legged,zhang2019solar} and can be built on top of \emph{any base policy optimization algorithm}.
    \item We implement two model-based instantiations of \Method\ and demonstrate through five contact-rich, sim-to-real robot manipulation experiments that \Method\  provides substantial sample complexity gains over existing fine-tuning methods.
    \item We demonstrate theoretically how $a)$ \Method\ can learn highly performant policies, despite the bias introduced by the \Method\ objective and $b)$ how this enables \Method\ to overcome compounding errors which plague standard MBRL approaches. 
\end{enumerate}
Together, these insights underscore how \Method\ effectively leverages plentiful off-domain data from a simulator alongside small real-world data sets to substantially accelerate real-world fine-tuning.


\begin{comment}
\Method\ accomplishes this by optimizing an auxiliary $H$-step look ahead objective that uses the value function from simulation $V_{sim}$ to approximately bootstrap long-horizon returns. By reducing the effective horizon, the real-world policy search problem becomes significantly more sample-efficient and thereby practical~\citep{laidlaw2023bridging, li21horizon}. Intuitively, $V_{sim}$ roughly captures the behaviors of $\pi_{sim}$ in a form that is \emph{robust to dynamics shifts}. Indeed, by optimizing the aforementioned objective, \Method\ discovers sequences of actions that increase $V_{sim}$ over $H$-step interval \emph{under the real dynamics}. This enables the fine-tuned policy to approximately match the observed behavior of $\pi_{sim}$ in simulation, but when operating under real-world dynamics. Crucially, to preserve the strong learning signal from $V_{sim}$, we keep $V_{sim}$ frozen throughout the fine-tuning process. We observe that this prevents \emph{catastrophic forgetting}, wherein policies completely unlearn useful behaviors early in the fine-tuning process. Capitalizing on insights from the MPC literature, we demonstrate theoretically that $\mathbf{(1)}$ \Method\ can lead to highly effective policies even when there is a large dynamics gap and $\mathbf{(2)}$ \Method\ can be paired with a base model-based RL method to learn effectively in low-data regimes. Finally, we instantiate \Method\ on a variety of dynamic, contact-rich manipulation tasks on a real robot and show significant improvements in efficiency and performance that it affords. % using \Method\ to enhance fine-tuning. 
% \twnote{sentence about our beautiful results}


Prior work has additionally considered mixing simulated and real data during policy optimization -- either through co-training~\citep{torne24rialto}, simply initializing the replay-buffer with simulation data \citep{smith2022walk, ball2023efficient}, or adaptively sampling the simulated dataset and up-weighting transitions that approximately match the true dynamics~\citep{eysenbach2020off, liu2022dara, xu2023cross, niu2022trust}.

\end{comment}
\begin{comment}
Robot learning offers a path toward building robust, general-purpose decision making AI agents in the physical world, shifting the burden of controller design from designing accurate models and optimizers to collecting task-relevant training data. This raises a fundamental question: \emph{How do we cheaply obtain large quantities and coverage of behavior data to enable robot learning at scale?} Real-world data collection directly on robots via teleoperation~\citep{walke2023bridgedata, khazatsky2024droid} is tedious, scaling linearly with human effort. Community-driven teleoperation~\citep{mandlekar2018roboturk,embodimentcollaboration2024open} has taken this idea further, but the resulting datasets are still orders of magnitude smaller than those powering vision and language applications. Moreover, the actual diversity of the data collected is still relatively narrow since users often demonstrate repetitive, near-optimal behavior. The next generation of general-purpose robotic agents need more scalable ways of obtaining diverse and abundant data coverage without incurring an impractical degree of human cost. 

% Simulation as a solution to data coverage
Physics-based simulation \citep{mujoco,makoviychuk2021isaac} is a solution that can provide data coverage without requiring excessive human effort. With the availability of privileged environment access such as Lagrangian state and arbitrary resets, simulation environments enable quick and cheap policy learning via techniques such as reinforcement learning (RL) or trajectory optimization, yielding near-optimal behavior with a high-degree of state coverage. Notably, using randomization of positions, configurations, and dynamics in the simulation, this learning can be done essentially for free up to the limit of compute. In particular, simulation avoids the expense of manual real-world teleoperation or the infrastructural challenges of real-world RL, while gaining the advantages of broad data coverage. 

Unfortunately, simulation-generated data is not a silver bullet. Producing data in simulation has two primary challenges: \emph{1)} content creation, \emph{2)} inherent model misspecification. Building simulation environments to bootstrap real-world robot learning involves designing high-fidelity simulated scenes, assets, and physics. This is challenging, especially with limited modeling time or expertise. Moreover, even with considerable effort is invested in constructing a simulation, there is often an inherent, irreducible modeling gap between the physics of the simulation and of the real world. Imperfections in simulation assets and inaccuracies in simulated physics have essentially the same effect -- environment misspecification. While efforts in system identification ~\citep{memmel24asid, huang23compass} can resolve some of this modeling error, there remains a degree of irreducible error between simulation and the real world. This suggests that there exists \emph{no} setting of simulation parameters that is sufficiently explanatory of real world behavior. For instance, modeling contact rich behavior of interactions such as hammering in a nail can be extremely challenging due to the effects of dynamic and deformable interactions, leading to an inherent level of \emph{misspecification}. This prevents success on zero-shot transfer between simulation and the real world on tasks with a considerable degree of precision and dynamism. \footnote{Modeling can always be improved on a certain problem with additional time, effort and data. But we keep our problem setting bounded to a reasonable limit of time and data for tractability.} The question becomes: \textit{can inaccurate simulation models be useful despite fundamental model misspecifications?} 

% Real-world RL



% Simulation guided fine-tuning
A natural technique to leverage these inaccurate simulation models has been to train a near-optimal policy in simulation, and use it as an initialization for RL fine-tuning in the real-world ~\citep{smith22legged, cherrypick}. Performing reinforcement learning in the real world can overcome fundamental model misspecification, since data is collected directly in the test environment. As opposed to techniques that simply perform zero-shot transfer, techniques for fine-tuning with reinforcement learning should be asymptotically optimal. Prior work has attempted to use both simulation and real world data, either through co-training [cite Coline] or simulation initialized fine-tuning [cite ra, RLPD]. While this can be a useful step, this only provides simulation-driven guidance at policy initialization and does not utilize information gathered in simulation thereafter, with the benefit of simulation subsiding over the course of real-world policy improvement. Simply treating real-world fine-tuning as a continuation of the simulation pre-training process leads to a degradation of targeted exploration behavior, often performing no better than learning from scratch. These challenges have made direct applications of simulation initialized fine-tuning procedures too inefficient for practical usage. 


In this work, we argue that despite being misspecified, information from simulators can be effective beyond providing a policy initialization for fine-tuning, guiding the learning process throughout the entirety of policy improvement. In particular, we show that simulation can provide guidance for real-world fine-tuning through two additional channels - 1) providing data augmentation beyond collected experience by transferring dynamics models from simulation and using them for short-horizon hallucination, and 2) ensuring that short-horizon hallucinated rollouts can provide policy improvement signal by transferring a dense potential-based reward shaping from simulation. By transferring approximate dynamics models and reward shaping, simulation can serve not just to provide a policy initialization but also to provide directions for policy improvement in the low data regime by reducing the variance and effective horizon~\citep{cheng2021heuristic} throughout the process of real-world fine-tuning. The core assumption behind this insight is that while policies may not transfer directly, coarse value functions that capture the ``structure" of the task do remain similar between simulation and reality. The proposed paradigm, simulation guided fine-tuning (\Method), is able to leverage simulation-learned value functions to shorten the effective horizon of policy search, while using models to provide local guidance on short-horizon policy improvement. In doing so, we propose a general paradigm that can be applied in conjunction with essentially any policy improvement procedure that is pretrained in simulation and fine-tuned in the real world. We note that this work is less about proposing a specific reinforcement learning \emph{algorithm}, and more about how entities such as models and values transferred from simulation can make the learning problem easier for any base RL improvement algorithm. We validate this through real-world fine-tuning experiments across several high-precision, dynamic sim-to-real robotic manipulation domains, showing more efficient, natural and performant learning in the real world than typical techniques for RL fine-tuning. In doing so, we are proposing that simulation can serve as a rich source of information that can very efficiently enable the acquisition of complex, real-world robotic behaviors. 

\end{comment}
\section{Related Work}\label{sec:related}
\noindent\textbf{Simulation-to-Reality Transfer.} Two main approaches have been proposed for overcoming the dynamics gap between simulation and reality:  1) adapting simulation parameters to real-world data ~\citep{chebotar19close, ramos19bayessim, memmel24asid} and 2) learning adaptive or robust policies to account for uncertain real-world dynamics ~\citep{haozhirma, rma, yu17prep}. However, these approaches still display failure modes in regimes where simulation and reality diverge substantially \cite{smith2022walk} -- namely, where no set of simulation parameters closely match the real-world dynamics. 


\noindent\textbf{Adapting Policies with Real-World Data.} 
Many RL approaches have focused on the general fine-tuning problem~\citep{Rajeswaran-RSS-18, nair2020awac, kostrikov2021offline, hu2023imitation, nakamoto2024cal}. These works initialize policies, Q-functions, and replay buffers from offline data and continue training them with standard RL methods. A number of works have specifically considered mixing simulated and real data during policy optimization -- either through co-training~\citep{torne24rialto}, simply initializing the replay-buffer with simulation data \citep{smith2022walk, ball2023efficient}, or adaptively sampling the simulated dataset and up-weighting transitions that approximately match the true dynamics~\citep{eysenbach2020off, liu2022dara, xu2023cross, niu2022trust}. However, recent work \cite{zhou2024efficient} has demonstrated that there are minimal benefits to sampling off-domain samples when adapting online in new environments, as this can bias learned policies towards sub-optimal solutions. In contrast to these prior approaches -- which primarily use simulated experience to initialize real-world learning -- we focus on distilling effective exploration strategies from simulation, using $V_{sim}$ to guide real-world learning. We demonstrate theoretically that this approach to transfer leads to low bias in the policies learned by \Method. 
\begin{comment}
. While going back from the real world to simulation can help target the simulation parameters more accurately~\citep{chebotar19close, ramos19bayessim, memmel24asid}, it cannot overcome inherent model misspecification, as we show in our experimental evaluation. Learning adaptive policies to account for changing real-world dynamics~\citep{haozhirma, rma, yu17prep} can help to some extent but is unable to guide exploration and adapt beyond the training dynamics range.\end{comment} Prior work has additionally considered mixing simulated and real data during policy optimization -- either through co-training~\citep{torne24rialto}, simply initializing the replay-buffer with simulation data \citep{smith2022walk, ball2023efficient}, or adaptively sampling the simulated dataset and up-weighting transitions that approximately match the true dynamics~\citep{eysenbach2020off, liu2022dara, xu2023cross, niu2022trust}. However, recent work \cite{zhou2024efficient} has demonstrated that there are minimal benefits to sampling off-domain samples when adapting online in new environments, as it introduces bias into the learned policy.  In contrast, our approach focuses on distilling effective \emph{guidance for exploration} from simulation, and which we demonstrate theoretically leads to low-bias in the policies learned by \Method . 

\noindent\textbf{Adapting Policies with Real-World Data.} 
Many RL approaches have focused on the general fine-tuning problem~\citep{Rajeswaran-RSS-18, nair2020awac, kostrikov2021offline, hu2023imitation, nakamoto2024cal}. These works initialize policies, Q-functions, and replay buffers from offline data and continue training them with standard RL methods, but do not use the pre-training data beyond initialization and populating the replay buffer. We utilize the pretraining in simulation not just for initialization but also to provide guidance \emph{throughout} the real-world policy improvement process. 

% Moreover, the learned value functions and models in our proposed methodology have coverage over state-space as opposed to limited coverage pre-training methods with offline RL and imitation learning. 

\noindent\textbf{Reward Design in Reinforcement Learning.} 
A significant component of our methodology is learning dense shaped reward in simulation to guide real-world fine-tuning. Prior techniques have tried to infer rewards from expert demos~\citep{ziebart2008maxentirl, ho2016generative}, success examples~\citep{fu2018variational,li2021mural}, LLMs~\citep{ma2023eureka, yu2023language}, and heuristics~\citep{margolisrapid, dota2}. We rely on simulation to provide reward supervision using the PBRS formalism~\citep{ng1999policy}. This effectively encodes information both about the dynamics of the simulator and optimal behaviors in the simulated environment. This enables use to shorten the learning horizon \citep{cheng2021heuristic,westenbroek2022lyapunov}, which reduces the sample complexity of obtaining an effective policy in the real-world. 
\begin{comment}
Another line of work complementary to our comes from \cite{eysenbach2020off, liu2022dara, xu2023cross, niu2022trust}, which relabel rewards from off-task (simulated) data, effectively up-weighting transitions that approximately match the dynamics observed in the target domain. These works focus on the \emph{retrieval} of useful samples from prior datasets with shifted dynamics. In contrast, our approach uses prior data to guide the discovery of novel sequences of states and actions in the target domain. In principle, these techniques could be used in conjunction; we leave this to future work. 
\end{comment}
% \twnote{maybe mention we can use these together?} 



\noindent\textbf{Model-Based Reinforcement Learning.} A significant body of work on model-based RL learns a generative dynamics models to accelerate policy optimization~\citep{sutton1991dyna, wang2019exploring, janner2019trust, yu2020mopo, kidambi2020morel, ebert2018visual,zhang2019solar}. In principle, a model enables the agent to make predictions about states and actions not contained in the training, enabling rapid adaptation to new situations. However, the central challenge for model-based methods is that small inaccuracies in predictive models can quickly compound over time, leading to large \emph{model-bias} and a drop in controller performance. An effective critic can be used to shorten search horizons~\citep{hansen2024tdmpc, bhardwaj2020blending, hafner2019learning, jadbabaie2001unconstrained, grune2008infinite} yielding easier decision-making problems, but learning such a critic from scratch can still require large amounts of on-task data. We demonstrate that, for many real-world continuous control problems, critics learned entirely in simulation can be robustly transferred to the real-world and substantially accelerate model-based learning.  

\noindent\textbf{Reward Design in Reinforcement Learning.} 
A significant component of our methodology is learning dense shaped reward in simulation to guide real-world fine-tuning. Prior techniques have tried to infer rewards from expert demos~\citep{ziebart2008maxentirl, ho2016generative}, success examples~\citep{fu2018variational,li2021mural}, LLMs~\citep{ma2023eureka, yu2023language}, and heuristics~\citep{margolisrapid, dota2}. We rely on simulation to provide reward supervision \cite{westenbroek2022lyapunov} using the PBRS formalism~\citep{ng1999policy}. This effectively encodes information about the dynamics and optimal behaviors in the simulator, enabling us to shorten the learning horizon and improve sample efficiency \cite{cheng2021heuristic, westenbroek2022lyapunov}.




\section{Preliminaries}
\label{sec:prelims}
%\textbf{Notation and Problem Setting.} 
Let $\S$ and $\A$ be state and action spaces.
Our goal is to control a real-world system defined by an unknown Markovian dynamics $s' \sim p_{real}(\cdot| s, a)$, where $s,s' \in \S$ are states and  $a \in \A$ is an action. The usual formalism for solving tasks with RL is to define a Markov Decision Process of the form $\M_{r} = (\S, \A, p_{real}, \rho_{real}^0, r, \gamma)$ with initial real-world state distribution $\rho_{real}^0$, reward function $r$, and discount factor $\gamma \in [0,1)$. Given a policy $\pi$, we let $d_{real}^\pi(s)$ denote the distribution over trajectories $\tau = (s_0, a_0, s_1, a_1, \dots)$ generated by applying $\pi$ starting at initial state $s_0$. Defining the value function under $\pi$ as $V_{real}^\pi(s) = \mathbb{E}_{s_t \sim d_{real}^\pi(s)}[\sum_{t}\gamma^t r_t(s_t)]$, our objective is to find $
\pi_{real}^* \leftarrow \sup_{\pi}\mathbb{E}_{s \sim \rho_{real}^0}[V_{real}^\pi(s)]$. We define the optimal value function as $V_{real}^*(s):=\sup_\pi V_{real}^\pi(s)$.

Unfortunately, obtaining a good approximation to $\pi_{real}^*$ using only real-world data is often impractical.  Thus, many approaches leverage an approximate simulation environment $s' \sim p_{sim}(s, a)$ and solve an approximate MDP of the form $\mathcal{M}_{sim} := (\S,\A,p_{sim}, \rho_{sim}^0, r,\gamma)$ to train a policy $\pi_{sim}$  meant to approximate $\pi_{real}^*$ . We let $V_{sim}$ denote $\pi_{sim}$'s value function with respect to $\mathcal{M}_{sim}$. Here, $\rho_{sim}^0$ is the distribution over initial conditions in the simulator.





\section{Simulation-Guided Fine-Tuning}

We build our framework around the following intuition: even when the finer details of $p_{real}$ and $p_{sim}$ differ substantially, we can often assume that $\pi_{sim}$ captures the rough motions needed to complete a task in the real world (such as swinging a hammer towards a nail). Specifically, we hypothesize that the \emph{ordering} defined by $V_{sim}$ captures these behaviors in a form that can be robustly transferred from simulation to reality and used to guide efficient real-world adaptation.  

\begin{comment}
\subsection{Reward Shaping and Horizon Shortening}
 We implement our horizon-shortening approach using the Potential-Based Reward Shaping (PRBS) formalism \citep{ng1999policy}, which replaces the reward $r(s)$ with $\bar{r}(s,s') = r(s) + \gamma \Phi(s') - \Phi(s)$ for some function $\Phi$ and is usually applied to infinite horizon MDPs $\M$ to form a new MDP $\overline{\mathcal{M}}$. Intuitively, the addition of the potential term encourages policy search algorithms to follow $\Phi$ by increasing its value during each transition. A well-designed $\Phi$ can guide exploration and decrease the variance of policy search by effectively acting as a \emph{baseline}, but the reshaped reward does not change the optimal policy. Indeed, by telescoping out the rewards one may observe that $\sum_{t =0}^{\infty}\bar{r}_t = \sum_{t=0}^\infty r_t  -\Phi(s_0)$. Because $\Phi(s_0)$ does not depend on the choice of the policy, the reshaped reward does not affect the ordering over policies. However, even with an informative reshaped reward, solving the infinite horizon reshaped MDP is still costly. This motivates us to shorten the horizon of the returns that we optimize in the real-world, and thus we will investigate optimizing $H$-step returns of the form $\sum_{t=0}^{H-1}\bar{r}_t = \sum_{t=0}^{H-1} r_t + \gamma^H \Phi(s_H) - \Phi(s_0)$. Here, the $\gamma^H \Phi(s_H)$ term can be interpreted as a fixed approximation to the true long-horizon returns $\gamma^{H}V_{real}^{*}(s_H)$, while $- \Phi_{sim}(s_0)$ again acts as a baseline to reduce variance. While this introduces bias into the returns, optimizing the $H$-step return from a given initial condition presents a significantly more tractable problem. 
\end{comment}


\subsection{Simulation-Guided Fine-Tuning}
When fine-tuning $\pi_{sim}$ to the real world we propose $a)$ reshaping the original reward function according to $r(s) \rightarrow \bar{r}(s,s') = r(s) + \gamma V_{sim}(s') - V_{sim}(s)$ and $b)$ shortening the search horizon to a more tractable $H$-step objective $\sum_{t=0}^{H-1} \bar{r}(s_t,s_{t+1})$. The reshaped objective is an instance of the Potential-Based Reward Shaping (PRBS) formalism \citep{ng1999policy}, where $V_{sim}$ is used as the \emph{potential function}. This reshaping approach is typically applied to infinite-horizon objectives, where it can be shown that the optimal policy under $\bar{r}(s,s')$ is the same as the optimal policy under the original reward $r(s)$ \citep{ng1999policy}. In contrast, the finite horizon search problem biases the objective towards behaviors which were successful in the simulator.  Indeed, by telescoping out terms we can rewrite our policy optimization objective as:
\begin{equation}\label{eq:short_horizon}
V_H^{\pi_H}(s) = \mathbb{E}\left[\gamma ^H V_{sim}(s_H) +  \sum_{t=0}^{H-1} \gamma^t r(s_t) - V_{sim}(s_0) \bigg | s_0 = s, a_t \sim \pi_{H,t}(\cdot |s_t) \right],
\end{equation}
\begin{equation*}
V_{H}^{*}(s):= \sup_{\pi_H}V_{H}^{\pi_H}(s), \ \ \ \  Q_{H}^*(s,\pi) := \mathbb{E}_{a\sim \pi(\cdot|s)} \big[\gamma V_H^*(s') + r(s)\big]. 
\end{equation*}
Here $\pi_H = \set{\pi_{H,0}, \pi_{H,1}, \dots, \pi_{H,H-1}}$ denotes $H$-step time-dependent policies, where $\pi_{H,t}$ is the policy applied at time $t$. We emphasize that these $H$-step returns are optimized over the \emph{real-world dynamics}. We propose learning a policy which optimizes these $H$-step returns from each state:
\begin{equation*}
 \pi_{H}^*(\cdot|s) \leftarrow \sup_{\pi}Q_H^*(s,\pi).
\end{equation*}


\paragraph{What behavior does this objective elicit?} For each $H$ , the $H$-step Bellman equation dictates that the optimal policy $\pi_{real}^*$ for the original MDP $\M_{real}$ can be found by solving: 
\begin{equation*}
\pi_{real}^*(\cdot|s) \leftarrow \sup_{\pi} \mathbb{E}\left[\gamma ^H V_{real}^{*}(s_H) +  \sum_{t=0}^{H-1} \gamma^t r(s_t) - V_{real}^*(s_0) \bigg | s_0 = s, a_t \sim \pi(\cdot |s_t) \right].
\end{equation*}
Thus, \eqref{eq:short_horizon} effectively uses $V_{sim}$ as a surrogate for $V_{real}^*$. Namely, \eqref{eq:short_horizon} uses the simulator to bootstrap long-horizon returns, while real-world interaction is optimized only over short trajectory segments. In the extreme case where $H=1$, $\pi_H^*$ will greedily attempt to increase $V_{sim}$ at each timestep; namely, the policy search problem will be reduced to a contextual bandit problem. As we take $H\to \infty$, $\pi_H^*$ will optimize purely real long-horizon returns and thus recover the behavior of $\pi_{real}^*$.

We are particularly interested in optimizing this objective with small values of $H$, as this provides an ideal separation between what is learned using cheap, plentiful interactions with $p_{sim}$ and what is learned with more costly interactions with $p_{real}$. In simulation, we can easily generate enough data to explore many paths through the state space and discover which motions lead to higher returns (e.g. swinging a hammer towards a nail). This information is distilled into $V_{sim}$ during the learning process, which defines an ordering over which states are more desirable to reach $H$ steps in the future. When learning in the real world, by optimizing \Cref{eq:short_horizon}  for small values of $H$, we only need to learn short sequences of actions which move the system to states where $V_{sim}$ is higher. Intuitively, \eqref{eq:short_horizon} 
 learns \emph{where to go} with large amounts of simulated data and \emph{how to get there} with small amounts of real-world data, efficiently adapting $\pi_{sim}$ to the real-world dynamcis.


\paragraph{Connections to the MPC Literature:} Note that the $-V_{sim}(s_0)$ term in \eqref{eq:short_horizon} does not depend on the choice of policy, and thus does not affect the choice of optimal policy. Thus, \eqref{eq:short_horizon} is equivalent to the planning objective used by model predictive control (MPC) methods \citep{jadbabaie2001unconstrained, hansen2024tdmpc, sun2018truncated, bhardwaj2020blending} with $H$-step look-ahead and a terminal reward of $V_{sim}$ (assuming oracle access to the real-world dynamics), and $\pi_{H}^*$ is the resulting optimal MPC controller. Of course, we cannot calculate $\pi_H^*$ directly because we do not know the real-world dynamics, and thus seek to approximate its behavior by learning from real-world interactions. 



\begin{wrapfigure}{R}{0.6\textwidth}
\begin{minipage}{0.6\textwidth}
\begin{algorithm}[H]
\caption{Simulation-Guided Fine-tuning (~\Method)}
\begin{algorithmic}[1]
\Require Pretrained policy $\pi_{sim}$ and value function $V_{sim}$
\State $\pi \leftarrow \pi_{sim}$, $\mathcal{D} \leftarrow \emptyset$
\For{each iteration k}
    \For {$\text{time step } t=1, ..., T$}
        \State $a_t \sim \pi(\cdot| s_t)$
        \State Observe the state $s_{t+1}$ and the reward $r_t$.
        \State $\bar{r}_t \leftarrow r_t + \gamma V_{sim}(s_{t+1}) - V_{sim}(s_t)$
        \State $\mathcal{D} \leftarrow \mathcal{D} \cup \{(s_t, a_t, \bar{r}_t, s_{t+1})\}$
    \EndFor
\State Approx. optimize $\pi \leftarrow \max_{\pi} Q_H^*(s,\pi(s_j))$ \newline
\hspace*{1.3em} using transitions in $\mathcal{D}$, at all observed states $s_j$.
\EndFor
\end{algorithmic}
\label{alg:SGFT}
\end{algorithm}
    \end{minipage}
\end{wrapfigure}
\paragraph{The Simulation-Guided Fine-Tuning Training Loop.}  We propose the general \emph{Simulation-Guided Fine-Tuning (SGFT)}  framework in the pseudo-code in \Cref{alg:SGFT}. \Method\ fine-tunes $\pi_{sim}$ to succeed under the real-world dynamics by iteratively $1)$ unrolling the current policy to collect transitions from $p_{real}$ and $2)$ using the current dataset $\mathcal{D}$ of transitions to approximately optimize $\pi \leftarrow \max_{\pi} Q_H^*(s,\pi(s_j))$ at each state $s_j$ the agent has visited. By optimizing \eqref{eq:short_horizon}, \Method\ optimizes policies towards the actions taken by $\pi_H^*$. Note that this framework can be built on top of any base policy optimization method; however, over the next two sections we will argue that this framework is particularly beneficial when paired with model-based search strategies.

 


\begin{comment}
Even though we do not have direct access to $\pi_H^*$, we can implicitly learn its actions by optimizing policies to maximize the $H$-step return (\Cref{eq:short_horizon}) starting from every initial condition $s \in \mathcal{S}$. Specifically, note that the expert actions are given by $\pi_H^* \leftarrow \max_{\pi}Q_H^*(s,\pi)}$, and that many standard (model-based and model-free) RL methods can be leveraged to solve this short-horizon policy optimization problem. , and we discuss how this step can be performed with model-based methods below. By approximately optimizing $\pi \leftarrow \max_{\pi} Q_H^*(s,\pi(s_j))$, \Method\ is implicitly attempting to learn and approximate the actions taken $\pi_H^*$ at every state the agent has visited (which is reminiscent of the learning loop used by DAgger \citep{ross2011reduction}, with the obvious caveat that in our case the expert is not directly available). 
\end{comment}
\begin{comment}
Indeed, \Method\ implicitly attempts to learn and match the actions taken by $\pi_H^*$ on the aggregate dataset of states visited by the previous policies. Thus, at this level of abstraction, the learning loop for \Method\ is reminiscent of DAgger \citep{ross2011reduction}, a seminal approach to imitation learning which aims to mimic the expert behavior on states the policy is likely to visit. However, in our setting we do not assume direct access to an expert which can succeed on $\M_{real}$, but only an approximate expert $\pi_{sim}$ whose behavior must be adapted. \twnote{patrick find this confusing}
\end{comment}



\begin{comment}
\subsection{ Implicit MPC Expert Policies}

The short horizon objective \Cref{eq:short_horizon} has a strong connection to the model-predictive control literature \twnote{citations}. Specifically, \Cref{eq:short_horizon} is the objective optimized by these methods when they use a prediction horizon $H$, the terminal reward $V_{sim}$, and are given oracle access to the real world dynamics $p_{real}$ (instead of using an approximate model $\hat{p}_{real}$, as is done in practice). Thus, by optimizing \Cref{eq:short_horizon} we are implicitly attempting to generate the actions take by an expert MPC controller of the form: 

\begin{equation}
V_H^\pi(s) = \mathbb{E}_{d^\pi}[]  
\end{equation}

\begin{equation}
\pi_H^*(s) \leftarrow \max_{\pi}\mathbb{E}\left[\gamma ^H V_{sim}(s_H) +\sum_{t=0}^{H-1} \gamma^t r(s_t) \ \bigg| \pi ,s_0 = s\right].
\end{equation}

The clear caveat here is that we do not know the actions taken by the expert policy \emph{a priori}, as the real world dynamics are unknown. In practice, by optimizing \Cref{eq:short_horizon} to approximate optimality over the aggregated dataset $\mathcal{D}$, at each iteration the \Method\ framework will synthesize a policy which approximately matches the actions of $\pi_H^*$ on the current aggregated dataset. Thus, \Method\ can be understood as a noisy approximation to the seminal DAgger \twnote{citation} framework for imitation learning. However, the setting we consider here is substantially more general, as we do not assume direct access to an expert policy for the target domain, but rather only an approximate expert ($\pi_{sim}$) whose behavior must be modified to yield a successful policy.

\end{comment}
\subsection{Leveraging Short Model Roll-outs}\label{sec:models}

Learning a generative model $\hat{p}$ for $p_{real}$ with the real-world data set $\mathcal{D}$ enables an agent to generate synthetic rollouts and reason about trajectories not contained in the data set. In principle, this should substantially accelerate the learning of effective policies. The central challenge for model-based reinforcement learning is that small errors in $\hat{p}$ can quickly compound over multiple steps, degrading the quality of the predictions \cite{janner2019trust}. As a consequence, learning a model accurate enough to solve long-horizon problems can often take as much data as solving the task with modern model-free methods \citep{chen2021randomized, hiraoka2021dropout}. By bootstrapping $V_{sim}$ in simulation, where data is plentiful, the \Method\ framework enables agents to act effectively over long horizons using only short, local $H$-step predictions about the real-world dynamics. This side-steps the core challenge for model-based methods, and enables extremely efficient real-world learning. We now outline how the SGFT can be built on top of two predominant classes of MBRL algorithms. 


% $V_{\textbf{sim}}(s)$
% $V_{\textbf{s}}(s)$
% $V_{\texttt{s}}(s)$
% $V_{sim}(s)$.


\begin{comment}
In what follows, we denote the following returns under the model for $\tilde{\pi}_H = \set{\pi_{H,t}}_{t=0}^{H-1}$:
\begin{equation}\label{eq:short_horizon}
\hat{V}_H^{\tilde{\pi}_H}(s) = \mathbb{E}\left[\gamma ^H V_{sim}(s_H) +  \sum_{t=0}^{H-1} \gamma^t r(s_t) -V_{sim}(s_0)  \bigg| s_0 = s, a_t \sim \pi_{H,t}(\cdot |s_t), s_{t+1} \sim \hat{p}(s_t,a_t) \right].
\end{equation}
\begin{equation}
\hat{V}_H^{*}(s) := \sup_{\tilde{\pi}_H}\hat{V}_H^{\tilde{\pi}_H}(s) \ \ \ \ \  \ \ \ \ \hat{Q}_H^*(s,a) = \mathbb{E}_{s'\sim \hat{p}(s,a)}\left[\gamma \hat{V}_{H-1}^*(s') + \bar{r}(s,s') \right]
\end{equation}
The corresponding MPC that uses $\hat{p}$ is given by: 
\begin{equation}\label{eq:approx_q}
\hat{\pi}_{H}^*(\cdot| s) \leftarrow \argmax_{\pi}\hat{Q}_H^*(s,\pi).
\end{equation}

Next, we discuss two broad approaches that use $\hat{p}$ to approximately learn $\hat{\pi}_H^*$ in each iteration.
\end{comment}
\begin{wrapfigure}{R}{0.6\textwidth}
\vspace{-0.3in}
\begin{minipage}{0.6\textwidth}
\begin{algorithm}[H]
\caption{Dyna-\Method}
\begin{algorithmic}[1]
\Require Pretrained policy $\pi_{sim}$ and value function $V_{sim}$
\State $\pi \leftarrow \pi_{sim}$
\For{each iteration k}
        \State Generate rollout $\set{(s_t, a_t, r_t, s_{t+1})}_{t=0}^T$ under $\pi$.
        \State $\bar{r}_t \leftarrow r_t + \gamma V_{sim}(s_{t+1}) -V_{sim}(s_t)$
\State $\mathcal{D} \leftarrow \mathcal{D} \cup (s_t, a_t, \bar{r}_t, s_{t+1})$
\State Fit generative model $\hat{p}$ with $\mathcal{D}$.
\For{G policy updates}

\State Generate synthetic branched rollouts $\hat{\mathcal{D}}$ under $\pi$. 
\State Approx. optimize $\pi \leftarrow \max_{\pi} Q_H^*(s,\pi(s_j))$ \newline
\hspace*{2.8em} $\forall s_j \in \mathcal{D}$ using augmented dataset $\hat{\mathcal{D}} \cup \mathcal{D}$
\EndFor
\EndFor
\end{algorithmic}
\label{alg:dyna}
\end{algorithm}
\end{minipage}
%
\begin{minipage}{0.6\textwidth}
\begin{algorithm}[H]
\caption{MPC-\Method}
\begin{algorithmic}[1]
\Require Pretrained value $V_{sim}$ and initialized model $\hat{p}$.
\For{each iteration k}
        \State Generate rollout $\set{(s_t, a_t, r_t, s_{t+1})}_{t=0}^T$ using $\hat{p}$ \newline \hspace*{1.3em} and trajectory optimization to calculate $\hat{\pi}_H^*$
        \State $\bar{r}_t \leftarrow r_t + \gamma V_{sim}(s_{t+1}) -V_{sim}(s_t)$
\State $\mathcal{D} \leftarrow \mathcal{D} \cup \{ (s_t, a_t, \bar{r}_t, s_{t+1})\}$.
\State Fit generative model $\hat{p}$ with $\mathcal{D}$.
\EndFor
\end{algorithmic}
\label{algo:MPC}
\end{algorithm}
\end{minipage}
\vspace{-0.3in}
\end{wrapfigure}

\paragraph{Notation.} In what follows, we will use `hat' notation to denote $H$-step returns of the form \eqref{eq:short_horizon} under the transitions generated by the model $\hat{p}$ rather than the real dynamics $p_{real}$. Namely, $\hat{V}_H^{\pi_H}$ is the $H$-step value of policy $\pi_H$ under $\hat{p}$; $\hat{V}_H^*$ and $\hat{Q}_H^*$ are the optimal values. Similarly, $\hat{\pi}_H^*$ denotes the optimal $H$-step policy under the model dynamics. Note that this is simply the MPC policy generated by using $\hat{p}$ and optimizing \eqref{eq:short_horizon} with $\hat{p}$ substituted in for $p_{real}$. 

\paragraph{Improved Sample Efficiency with Data Augmentation (\Cref{alg:dyna}).} The generative model $\hat{p}$ can be used for \emph{data augmentation} by generating a dataset of synthetic rollouts $\hat{\mathcal{D}}$ to supplement the real-world dataset $\mathcal{D}$~\citep{janner2019trust, sutton1990integrated, gu2016continuous}. The combined dataset can then be fed to any policy optimization strategy, such as generic model-free algorithms. We consider state-of-the-art Dyna-style algorithms~\citep{janner2019trust}, which, in our context, branch $H$-step rollouts from states the agent has visited previously in the real-world. As \Cref{alg:dyna} shows, after each data-collection phase, this approach updates $\hat{p}$ and then repeatedly $a)$ generates a dataset $\hat{\mathcal{D}}$ of synthetic $H$-step rollouts under the current policy $\pi$ starting from states in $\mathcal{D}$, and $b)$ approximately solves $\pi \leftarrow \max_{\bar{\pi}}Q_H^*(s,\bar{\pi}(s))$ at the observed real-world states using the augmented dataset $\hat{\mathcal{D}}\cup \mathcal{D}$ and a base model-free method such as SAC~\citep{haarnojasac}.




\begin{comment}
\begin{wrapfigure}{R}{0.6\textwidth}
\vspace{-0.2in}
\begin{minipage}{0.6\textwidth}
\begin{algorithm}[H]
\caption{MPC-\Method}
\begin{algorithmic}[1]
\Require Pretrained value $V_{sim}$ and initialized model $\hat{p}$.
\For{each iteration k}
        \State Generate rollout $\set{(s_t, a_t, r_t, s_{t+1})}_{t=0}^T$ under $\hat{\pi}_H^*$.
        \State $\bar{r}_t \leftarrow r_t + \gamma V_{sim}(s_{t+1}) -V_{sim}(s_t)$
\State $\mathcal{D} \leftarrow \mathcal{D} \cup \{ (s_t, a_t, \bar{r}_t, s_{t+1})\}$.
\State Fit generative model $\hat{p}$ with $\mathcal{D}$.
\EndFor
\end{algorithmic}
\label{algo:MPC}
\end{algorithm}
    \end{minipage}
\end{wrapfigure}
\end{comment}

\paragraph{Online Planning (\Cref{algo:MPC}).}

The most straightforward way to approximate the behavior of $\pi_H^*$ is simply to apply the MPC controller $\hat{\pi}_H^*$ generated using the current best guess for the dynamics $\hat{p}$. \Cref{algo:MPC} provides general pseudocode for this approach, which iteratively $1)$ rolls out $\hat{\pi}_H^*$ (which is calculated using online optimization and $\hat{p}$~\citep{williams2017model}) then $2)$ updates the model on the current dataset of transitions $\mathcal{D}$.  This high-level approach encompasses a wide array of methods proposed in the literature, e.g., \cite{ebert2018visual} and \cite{zhang2019solar}. For the experiments in Section \ref{sec:exp}, we implemented this approach using the TDMPC-2~\citep{hansen2024tdmpc} algorithm, which additionally learns a policy prior to accelerate the trajectory optimization.










\section{Theoretical Analysis}\label{sec:theory}

The preceding discussions have covered how the dense rewards and horizon shortening strategy employed by \Method\ can lead to efficient real-world adaptation. However this leaves the outstanding question: \emph{how does the bias introduced by the reward shaping and horizon shortening affect policy performance?} Longer prediction horizons decrease the bias of the objective by relying more heavily on real returns, at the cost of increase sample efficiency. Given these competing challenges, can we expect \Method\ to simultaneously learn high-performing policies and adapt rapidly in the real-world? 


This section introduces a novel geometric analysis which demonstrates that \Method\ achieve both goals, even when there is a large gap between $p_{sim}$ and $p_{real}$. Specifically, we introduce mild technical conditions on the structure of $p_{sim}$, $V_{sim}$ and $p_{real}$ which ensure that $\pi_{H}^*$ is nearly optimal for $\M_{real}$, even when short prediction horizons $H$ are used. Our analysis builds on the following insight: even when there is a large gap in the magnitude $|p_{sim} - p_{real}|_{\infty}$ between simulation and reality, it is reasonable to assume that $V_{sim}$ still defines a reasonable \emph{ordering} over desirable states under the real-world dynamics $p_{real}$. Namely, we argue that $V_{sim}$ can capture the structure of motions that complete the desired task (such as reaching towards and object, picking it up, and moving it to a desired location), even if the low-level sequences of actions needed to realize these motions differs substantially between simulation and reality  (due e.g. to difficult-to-model contact dynamics). We use the following formal definition to capture this intuition, which is from \cite{cheng2019predictor} and shares strong connections to Lyapunov \cite{westenbroek2022lyapunov, grune2008infinite} and Dissipation  \cite{brogliato2007dissipative} theories from dynamical systems and control: 

\begin{definition}\label{def:improve}
We say that $V_{sim}$ is \emph{improvable} with respect to $\mathcal{M}_{real}$ if for each $s \in \mathcal{S}$ we have: 
\begin{equation}
\max_a \mathbb{E}_{s' \sim p_{real}(\cdot|s,a)}[\gamma V_{sim}(s')] - V_{sim}(s) \geq - r(s).
\end{equation}
\end{definition}
To unpack why this definition is useful, note that $V_{sim}$ is by definition improvable with respect to $\M_{sim}$. Indeed, by the temporal difference equation we have $\mathbb{E}_{s' \sim p_{sim}(\cdot|s,a), \  a \sim \pi_{sim}(\cdot|s)}[\gamma V_{sim}(s')] -V_{sim}(s) = -r(s)$. Namely, $V_{sim}$ is constructed so that polices can greedily increase $V_{sim}$ under $p_{sim}$ at each step and be guaranteed to reach maxima of $V_{sim}$. These maxima correspond to states (such as points where an object has been moved to a desired location) which correspond to task success. Definition \ref{def:improve} ensures that this condition then holds \emph{under the real-world dynamics}; this requirement ensures that the \emph{ordering} defined by $V_{sim}$ encodes feasible motions that solve the task in the real-world. This enables policy optimization algorithms to greedily follow $V_{sim}$ at each step while ensuring that the resulting behavior succesfully completes the task \cite{westenbroek2022lyapunov, cheng2021heuristic}. We use the following pedagogical example to investigate why this is a reasonable property to assume for continuous control problems: 

\paragraph{Pedagogical Example.} Consider the following case where the real and simulated dynamics are both deterministic, namely, $s' = p_{real}(s,a)$ and $s' = p_{sim}(s,a)$. Specifically, consider the case where $s = (s_1, s_2) \in \mathcal{S} \subset \R^2$, $a \in \mathcal{A} = \R$, and the dynamics are given by:
\begin{equation*}
p_{sim}(s,a) =\begin{bmatrix}
s_1' \\
s_2'
\end{bmatrix} = \begin{bmatrix}
s_1 \\
s_2
\end{bmatrix} + \Delta t \begin{bmatrix}
s_2 \\
\frac{g}{l}\sin(s_1) + a
\end{bmatrix}
\end{equation*}
\begin{equation*}
p_{real}(s,a) =\begin{bmatrix}
s_1' \\
s_2'
\end{bmatrix} = \begin{bmatrix}
s_1 \\
s_2
\end{bmatrix} + \Delta t \begin{bmatrix}
s_2 \\
\frac{g}{l}\sin(s_1) + a + e(s_1, s_2)
\end{bmatrix}.
\end{equation*}
These are the equations of motion for a simple pendulum \citep{wang2022dynamic} under an Euler discretization with time step $\Delta t$, where $s_1$ is the angle of the arm, $s_2$ is the angular velocity, $a$ is the torque applied by the motor, $g$ is the gravitational constant, and $l$ is the length of the arm. The real-world dynamics contains unmodeled terms $e(s_1,s_2)$, which might correspond to complex frictional or damping effects. Consider the policy for the real world given by $\pi_{real}(s) = \pi_{sim}(s) - e(s_1,s_2)$ and observe that $p_{sim}(s,\pi_{sim}(s)) = p_{real}(s,\pi_{real}(s))$. Even though the difference in transition dynamics $e(s_1,s_2)$ might be quite large, the set of feasible next states is the same for the two environments. That is, the space of feasible motions in the two MDPs are identical, even though it takes \emph{substantially different policies to realize these motions}. 

\paragraph{Geometric Insight.} More broadly, if for each $s$ there exists some $a$ such that $\mathbb{E}[p_{real}(\cdot| s,a)] = \mathbb{E}[p_{sim}(\cdot|s, \pi_{sim}(s))]$, then $V_{sim}$ is improvable with respect to $\M_{real}$. This follows from the fact that $V_{sim}$ is improvable under $p_{sim}$ by definition (see above discussion) and the fact that there exists actions which exactly match the state transitions in simulation and reality. More generally, it is reasonable to expect that $p_{sim}$ approximately captures the geometry of what motions are feasible under $p_{real}$, even if the actions required to realize the motions differ substantially in the two MDPs. Thus it is reasonable to assume $V_{sim}$ is improvable. This intuition is highlighted by our real-world examples in cases where we use \Method\ with a prediction horizon of $H=1$. In these cases the learned policy is able to greedily follow $V_{sim}$ at each state and reach the goal, even in the face of large dynamics gaps. Relatedly, work on Lyapunov theory \cite{westenbroek2022lyapunov} has demonstrated theoretically that value functions are naturally robust to dynamics shifts under mild conditions. 

\paragraph{Main Theoretical Result.} Before presenting our main theoretical result, we provide a useful point of comparison from the literature \cite{bhardwaj2020blending}. When translated to our setting, \footnote{We note that \cite{bhardwaj2020blending} focuses on general MPC problems where some approximation $\hat{V} \approx V_{real}^*$ is used as the terminal cost for the planning objective. However, despite the difference in settings, the structure of the underlying $H$-step objective we consider is identical to much of the MPC literature.} \cite[Theorem 3.1]{bhardwaj2020blending} assumes that $\forall (s,a)$ we have $a)$ the value gap between simulation and reality is bounded by $\|V_{sim}(s) - V_{real}^* (s)\| < \epsilon$ and $b)$ that a generative model $\hat{p}$ is used for planning wherein $\|\hat{p}(\cdot|s,a)-p_{real}(\cdot|s,a)\|_1 < \alpha$. Recalling that $\hat{\pi}_{H}^*$ is the controller synthesized using the model, \cite[Theorem 3.1]{bhardwaj2020blending} bounds the suboptimality for the model-based controller as $V_{real}^*(s) - V_{real}^{\hat{\pi}_H^*}(s) \leq O\left(\frac{\gamma}{1-\gamma} \alpha H + \frac{\gamma^H}{1-\gamma^H}\epsilon \right)$. To understand the bound, first set $\alpha =0$ so that there is no modeling error (as in the case of model-free instantiations of \Method). In this regime we are incentivized to increase $H$, as this will decrease the $\frac{\gamma^H}{1-\gamma^H}\epsilon$ term and improve performance. However this term scales very poorly for long-horizon problems where $\gamma \approx 1$, and we may need large values of $H$ to obtain near optimal policies. When $\alpha> 0$, the situation becomes even more challenging, as increasing $H$ will increase the $\frac{\gamma}{1-\gamma} \alpha H$ term, since longer prediction horizons lead to propagation of errors in the learned dynamics model. Thus, this result cannot justify that \Method\ can learn high-performance controllers when small values of $H$ are used. The following result demonstrates this is possible when $V_{sim}$ is improvable with respect to $\M_{real}$:


\begin{comment}
\begin{theorem}
Suppose that $V_{sim}$ is \twnote{todo} and further suppose that $\max_{s \in \S}|V_{sim}(s) -V_{real}^*(s)|< \epsilon$. Then any policy $\pi$ which satisfies $A_H^*(s,\pi)=Q_H^*(s,\pi)-V_H^*(s) \geq -\delta$ will satisfy:
\begin{equation}
V_{real}^{*}(s) - V_{real}^{\pi}(s) \leq \gamma^{H}\epsilon + \frac{\delta}{1-\gamma}.
\end{equation}
\end{theorem}
\end{comment}


\begin{theorem}\label{thm:subopt}
Assumes that $\forall (s,a)$ we have $a)$ the value gap between simulation and reality is bounded by $\|V_{sim}(s) - V_{real}^* (s)\|< \epsilon$ and $b)$ if a generative model $\hat{p}$ is used by \Method\ then  $\|\hat{p}(\cdot|s,a)-p_{real}(\cdot|s,a)\|_1 < \alpha$.  Further suppose that $V_{sim}$ is \emph{improvable} with respect to $\M_{real}$. Then for $H$ sufficiently small and each $s \in \S$ we have: 
\begin{equation}
V_{real}^*(s) - V_{real}^{\hat{\pi}_H^*}(s) \leq O\left(\frac{\gamma}{1-\gamma} \alpha H + \gamma^H\epsilon \right),
\end{equation}
where $\hat{\pi}_{H}^*$ is the policy learned by \Method.
\end{theorem} 

See \Cref{sec:proofs} for proof. This result demonstrates that when $V_{sim}$ is improvable with respect to $\M_{real}$ \Method\ can obtain nearly optimal policies, even when $H$ is small. 
When compared to \cite[Theorem 3.1]{bhardwaj2020blending}, which assumes uniform worst-case bounds on $\|V_{sim}-V_{real}\|$, when $V_{sim}$ is improvable we gain a substantial factor of $\frac{1}{1-\gamma^H}$ when bounding the effects of errors in $V_{sim}$. In short, this result demonstrates that even mild structural consistency between $p_{sim}$ and $p_{real}$ is enough to ensure that \Method\ provides effective guidance towards performant policies when $H$ is small. In the case of MBRL ($\alpha >0$), this is especially important as keeping $H$ small combats compounding errors in the dynamics model. Our proof technique adapts ideas from the theoretical control literature \cite{grune2008infinite, westenbroek2022lyapunov} to the more general setting we consider, and can be seen as a model-based analogy to the results from \cite{cheng2021heuristic}. 
\begin{comment}



In this section we analyze the effectiveness of the $H$-step simulation-guided expert $\pi_H^*$. Specifically, we seek suboptimality bounds for this agent and a characterization of situations when the behavior of this idealized policy will be robust to the errors made by MBRL techniques, which leverage an approximate model $\hat{p}$ for $p_{real}$ that has been learned from real-world data. We are particularly interested in understanding how \Method\ with short prediction horizons $H$ can mitigate errors in $\hat{p}$ with a small number of samples. %, as this will be the case we face when fine-tuning in the real-world with a small number of samples.  
To set the stage for this analysis, we first recall several relevant theoretical results from prior RL work. %relate the results from several prior theoretical analyses from the RL literature. 
\aknote{Before doing this, it would really help to summarize the overall characterization that you will derive in this section. Without such a "preview", this section is tricky to follow.}

\begin{prop}\label{prop:basic_{sim}ubopt} \aknote{Consider moving this proposition to after the Main Insight, or possibly even just mentioning it as a side note at the very end of this section where you discuss the similarity between Theorem 1 and Proposition 1, or even dropping Proposition 1 entirely. It is not critical to anything in this section, and starting the section with it is a little confusing.}
Suppose that $\max_{s \in \mathcal{S}}|V_{sim}(s) - V_{real}^*(s)| \leq \epsilon$ and that $\|\hat{p}(s,a) - p_{real}(s,a)\| \leq \alpha$. Then for $H$ sufficiently small and for each $s \in \mathcal{S}$ we have: 
\begin{equation}
V_{real}^*(s) - V_{real}^{\hat{\pi}_H^*}(s) \leq O\left(\frac{\gamma}{1-\gamma} \alpha H + \frac{\gamma^H}{1-\gamma^H}\epsilon \right),
\end{equation}
where $\hat{\pi}_H^*$ is the MPC policy under the approximate mode as in \Cref{eq:approx_q}
\end{prop}
This result is a direct translation of \cite[Theorem 3.1]{bhardwaj2020blending} to our setting, where the big-$O$ notation suppresses problem-dependent constants and lower-order terms for small values of $H$ which are not important for our discussion.  Intuitively, this follows from the fact that $V_{sim}(s_H)$ can be viewed as an approximation to $V_{real}^*(s_H)$ in the $H$-step look ahead objective \Cref{eq:short_horizon}. Because this approximation is scaled by $\gamma^H$, the effect of errors in $V_{sim}$ is diminished for larger values of $H$. Note, however, that this result scales poorly for small values of $H$. This is especially true for long-horizon problems where $\gamma \approx 1$. On the other hand, the term involving $\alpha$ captures how errors in the model accumulate for different horizons $H$, leading to mistakes in decision making. This term incentivizes us to make $H$ as small as possible. In particular, we are interested in understanding how we can mitigate large values of $\alpha$, which will arise in low-data regimes. 





Since results like \Cref{prop:basic_{sim}ubopt} simply assume uniform worst-cases bounds on the difference between between the magnitudes of $V_{sim}$ and $V_{real}^*$, they do not capture the fact that $V_{sim}$ may still preserve an ordering over states that is useful for guiding real-world decision making, i.e., when the \emph{geometry} of $V_{sim}$ enables \Method\ to guide policy search algorithms towards effective policies under the real dynamics. We use the following definition from \cite{cheng2021heuristic}, which is similar to properties from other works \citep{westenbroek2022lyapunov, grune2008infinite}:

\end{comment}
\section{Experiments}\label{sec:exp}

We answer the following: \textbf{(1)} Can \Method\  facilitate rapid online fine-tuning for dynamic, contact-rich manipulation tasks? \textbf{(2)} Does \Method\  improve the sample efficiency of fine-tuning compared to baselines? \textbf{(3)} Can \Method\ learn successful policies where prior methods fail entirely? 

\subsection{Methods Evaluated}

\paragraph{\Method\ Instantiations.} We implement concrete instantiations of the general Dyna-\Method\ and MPC-\Method\ frameworks sketched in Algorithms \ref{alg:dyna} and \ref{algo:MPC}. \textbf{\Method-SAC} fits a model to real world transitions to perform data augmentation and uses SAC as a base model-free policy optimization algorithm. We use $H=1$ in all our experiments. Crucially, we set the `done' flag to true at the end of each rollout -- this ensures SAC does not bootstrap its own critic from the real-world data and only uses $V_{sim}$ to bootstrap long-horizon returns. \textbf{\Method-TDMPC-2} uses TDMPC-2 \cite{hansen2024tdmpc} as a backbone. The base method learns a critic, a policy, and an approximate dynamics model through interactions with the environment. When acting in the world, the MPC controller solves online planning problems using the approximate model, the critic as a terminal reward, and uses the policy prior to seed an MPPI planner \cite{williams2017model}. To integrate this method with SGFT, when transferring to the real world we simply freeze the critic learned in simulation and use the reshaped objective in \Cref{eq:short_horizon} as the online planning objective. For our experiments, we use $H=4$ and the default hyperparameters reported in \cite{hansen2024tdmpc}. %\vspace{0.1cm}

%\noindent
\paragraph{Baseline Fine-tuning Methods.} We finetune simulation pre-trained polices using the original infinite-horizon objective for $\mathcal{M}_{real}$, including \textbf{SAC} \cite{haarnojasac}, \textbf{TDMPC-2} \cite{hansen2024tdmpc}, \textbf{IQL} \cite{kostrikov2021offline}. \textbf{PBRS} fine-tunes the policy under a reshaped infinite-horizon MDP using the reshaped reward $\bar{r}$ and uses SAC~\citep{haarnojasac}-- it does not use horizon shortening.  \textbf{RLPD} fine-tunes a fresh policy to solve the original MDP $\mathcal{M}_{real}$ using RLPD~\citep{ball2023efficient}. 
 \emph{These algorithm cover state-of-the art model-based, model-free, and offline-to-online adaptation algorithms.}
 %\vspace{0.1cm}

%\noindent
\paragraph{Baseline Sim-to-Real Methods.} Our \textbf{Domain Randomization} baseline refers to policies trained with extensive domain randomization in simulation and transferred directly to the real world. These policies rely only on the previous observation. \textbf{Recurrent Policy + Domain Randomization} uses policies conditioned on histories of observations, similar to methods such as \cite{rma}. \textbf{Asymmetric Actor-Critic}~\citep{pinto2018asymmetric} uses policies trained in simulation with a critic conditioned on privileged information. \textbf{DOREAMON}~\citep{tiboni2023domain} is a recently proposed transfer method which automatically generates curricula to enable robust transfer. \textbf{ASID}~\citep{memmel24asid} and \textbf{DROPO}~\citep{tiboni2023dropo} identify simulation parameters with small real-world data sets. 
 \emph{These methods serve as a state-of-the-art baselines for sim-to-real transfer methods.}


% \begin{figure}
% \begin{minipage}{.3\textwidth}
%   \centering
%   \includegraphics[width=1.0\linewidth]{ICLR 2025 Template/figs/hammering_sim_real.png}
%   \caption{\footnotesize{\textbf{Sim2Real Hammering:} Simulation setup for pretraining (left) and execution of real-world fine-tuning of real-world hammering (right).}}
%   \label{fig:hammering}
% \end{minipage}
% \hspace{1mm}
% \begin{minipage}{.3\textwidth}
%   \centering
%   \includegraphics[width=1.0\linewidth]{ICLR 2025 Template/figs/inserting_sim_real.png}
%   \caption{\footnotesize{\textbf{Sim2Real Inserting:} Simulation setup for pretraining (left) and execution of real-world fine-tuning of autonomous real-world inserting (right).}}
%   \label{fig:inserting}
% \end{minipage}
% \hspace{1mm}
% \begin{minipage}{.3\textwidth}
%   \centering
%   \includegraphics[width=1.0\linewidth]{ICLR 2025 Template/figs/pushing_sim_real.png}
%   \caption{\footnotesize{\textbf{Sim2Real Pushing:} Simulation setup for pretraining (left) and execution of real-world fine-tuning of autonomous real-world pushing (right).}}
%   \label{fig:pushing}
% \end{minipage}
% \end{figure}

% \begin{figure}[htbp]
%     \centering
%     \begin{minipage}{0.45\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{figs/sim_real.png}
%         \caption{\footnotesize
%         \textbf{Sim-to-Real Setup} Simulation setup for pretraining (\textbf{top}) and execution of real-world fine-tuning (\textbf{bottom}) of real-world hammering (\textbf{left}), insertion (\textbf{middle}), and pushing (\textbf{right}).}
%         \label{fig:sim_real_vis}
%     \end{minipage}%
%     \hfill
%     \begin{minipage}{0.5\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{figs/real_plots.pdf}
%         \caption{\footnotesize{\textbf{Real-world success rates during the course of online fine-tuning.} We plot task success rates over number of fine-tuning rollouts for the tasks described in Sec.~\ref{sec:exp}. We see that \Method\ yields significant improvements in success and efficiency.}}
%         \label{fig:real_plots}
%     \end{minipage}
%     \caption{Two figures side by side}
%     \label{fig:sidebyside}
% \end{figure}

\begin{wrapfigure}{r}{0.5\textwidth}
% \begin{figure}
\vspace{-12mm}
\begin{center}
    \includegraphics[width=\linewidth]{figs/sim_real.png}
    \vspace{-3mm}
    \caption{\footnotesize
    \textbf{Sim-to-Real Setup} Simulation setup for pretraining (\textbf{top}) and execution of real-world fine-tuning (\textbf{bottom}) of real-world hammering (\textbf{left}), insertion (\textbf{middle}), and pushing (\textbf{right}).}
    \label{fig:sim_real_vis}
    \vspace{-0.8cm}
\end{center}
% \end{figure}
\end{wrapfigure}

\begin{figure}
\begin{center}
    \includegraphics[width=\linewidth]{figs/real_plots.png}
    \vspace{-3mm}
    \caption{\footnotesize{\textbf{Real-world success rates during the course of online fine-tuning.} We plot task success rates over number of fine-tuning rollouts for the tasks described in Sec.~\ref{sec:exp}. We see that \Method\ yields significant improvements in success and efficiency.}}
    \label{fig:real_plots}
    \vspace{-0.8cm}
\end{center}
\end{figure}

% \begin{figure}[h]
%     \begin{center}
%         \includegraphics[width=0.8\linewidth]{figs/sim_real.png}
%         \vspace{-3mm}
%         \caption{
%         \footnotesize
%         \textbf{Sim-to-Real Setup} Simulation setup for pretraining (\textbf{top}) and execution of real-world fine-tuning (\textbf{bottom}) of real-world hammering (\textbf{left}), insertion (\textbf{middle}), and pushing (\textbf{right}).
%         }
%         \label{fig:sim_real_vis}
%         \vspace{-2mm}
%     \end{center}
% \end{figure}

% \begin{figure*}[t!]
%     \centering
%     \includegraphics[width=1.0\textwidth]{figs/real_plots.png}
%     \vspace{-3mm}
%     \caption{\textbf{Real-world success rates during the course of online fine-tuning.} We plot task success rates over number of fine-tuning rollouts for the tasks described in Sec.~\ref{sec:exps}. We see that \Method\ yields significant improvements in success rate and asymptotic performance. 
%     }
%     \vspace{-5mm}
%     \label{fig:real_plots}
% \end{figure*}



\subsection{Sim-to-Real Evaluations}
We test each method on five real-world manipulation tasks illustrated in \Cref{fig:sim_real_vis} and two additional real-world deformable object pushing tasks illustrated in \Cref{fig:cover_fig}, demonstrating that both the \textbf{\Method-SAC} and \textbf{\Method-TDMPC-2} instantiations of \Method\ excel at learning policies with minimal real-world data. 

%\vspace{0.1in}
\noindent
\textbf{Hammering} is a highly dynamic task involving force and contact dynamics that are impractical to precisely model in simulation. In our setting, the robot is tasked with hammering a nail in a board. The nail has high, variable dry friction along its shaft. In order to hammer the nail into the board, the robot must hit the nail with high force repeatedly. The dynamics are inherently misspecified between simulation and reality here due to the infeasibility of accurately modeling the properties of the nail and its contact interaction with the hammer and board.

%\vspace{0.1in}
\noindent
\textbf{Insertion}~\citep{heo2023furniturebenchreproduciblerealworldbenchmark} involves the robot grasping a table leg and accurately inserting it into a table hole. The contact dynamics between the leg and the table differ between simulation and real-world conditions. In the simulation, the robot successfully completes the task by wiggling the leg into the hole, but in the real world this precise motion becomes challenging due to inherent noise in the real-world observations as well as contact discrepancies between the leg and the table hole.

% \vspace{0.1in}
\noindent
\textbf{Puck Pushing} requires pushing a puck of unknown mass and friction forward to the edge of the table without it falling off the edge. Here, the underlying feedback controller of the real world robot inherently behaves differently from simulation. Additionally, retrieving and processing sensor information from cameras incurs variable amounts of latency. As a result, the controller executes each commanded action for variable amounts of time. These factors all contribute to the sim-to-real dynamics shift, requiring real-world fine-tuning to reconcile. 

\noindent
\textbf{Deformable Object Pushing} is similar to puck pushing but involves pushing deformable objects which are challenging to model in simulation. As a result, we choose to model the object as a puck in simulation, making the simulation dynamics fundamentally misspecified compared to the real world. This example exemplifies how value functions trained under highly different dynamics in simulation can still provide useful behavioral priors for real-world exploration. We include two deformable pushing tasks -- a towel and a squishy toy ball.

Each task is evaluated on a physical setup using a Franka FR3 operating with either Cartesian position control or joint position control at 5Hz. We compute object positions by color-thresholding pointclouds or by Aruco marker tracking, although this approach could easily be upgraded. Further details on reward functions, robot setups and environments can be found in Appendix \ref{sec:envs}. 

\paragraph{Analysis.}
The results of real-world fine-tuning on these five tasks are presented in~\Cref{fig:real_plots}. For all five tasks, zero-shot performance is quite poor due to the dynamics sim-to-real gap. The poor performance of direct sim-to-real transfer methods such as Domain Randomization, Recurrent Policy + Domain Randomization~\citep{rma}, Asymmetric Actor-Critic~\citep{pinto2018asymmetric}, ASID~\citep{memmel24asid}, DROPO~\citep{tiboni2023dropo}, and DOREAMON~\citep{tiboni2023domain} highlight that these gaps are due to more than parameter misidentification or poor policy training in simulation, but rather stem from fundamental simulator misspecification. 

The second class of comparison methods includes offline pretraining with online fine-tuning techniques like IQL~\citep{kostrikov2021offline}, SAC~\citep{haarnojasac}, and RLPD~\citep{ball2023efficient}.  Whether model-free or model-based, the SGFT finetuning methods (ours) substantially outperform these techniques in terms of efficiency and asymptotic performance. Moreover, they prevent \emph{catastrophic forgetting}, wherein finetuning leads to periods of sharp degradation in the policies effectiveness. This suggests that simulation can offer more guidance during real-world policy search than just network weight initialization and/or replay buffer data initialization for subsequent fine-tuning. Our full system consistently leads to significant improvement from fine-tuning, achieving 100\% success for hammering and pushing within an hour of fine-tuning and 70\% success for inserting within two hours of fine-tuning. The fact that SGFT outperforms both TD-MPC2~\citep{hansen2024tdmpc} and PBRS-SAC, suggests that efficient fine-tuning requires a \emph{combination} of both short model rollouts and value-driven reward shaping. 


Last but not least, note that SGFT offers improvements on top of both SAC and TDMPC2, showing the generality of the proposed paradigm. We next perform simulated benchmarks and ablations to further test our design decisions. 



\subsection{Sim-to-Sim Experiments}
\label{app:sim2sim}
Here we additionally test each of the proposed methods on the sim-to-sim set-up from \cite{du2021auto}, which is meant to mock sim-to-real gaps but for familiar RL benchmark tasks. The results are depicted in Figure \ref{fig:sim_plots} for the Walker Walk, Cheetah Run, and Rope Peg-in-Hole environments. For all tasks, we use the precise settings from \cite{du2021auto}. Note that the general trend of these results matches our real world experiments -- \Method\ substantially accelerates learning and overcomes the dynamics gap between the `simulation' and `real' environments. 

Finally, we additionally use the Walker and Peg-in-Hole environments to ablate the effects of the hyper parameter $H$. Intuitively, the peg-in-hole environment requires much more precise actions, and is thus should be more sensitive to errors in the pretraining environment. Thus, we should expect that SGFT will benefit from larger values of $H$ for this environment, as this will correspond to relying more heavily on returns from the target environment. We see that this trend holds in Figure \ref{fig:h-ablation}, where the Walker environments is barely affected by the choice of $H$ but this hyper parameter has a large impact on the performance in the Peg-in-Hole Environment.

\begin{figure*}[!h]
    \centering
    \includegraphics[width=1.0\textwidth]{figs/H-ablation.pdf}
    \vspace{-3mm}
    \caption{
    \footnotesize{\textbf{Normalized Rewards for Sim-to-Sim Transfer}. Ablating the effects of the horizon $H$ across two sim-to-sim expirments. The choice of $H$ has a much larger effect on the peg-in-hole task, which requires much more precise actions to achieve succes. 
    }}
    \vspace{-5mm}
    \label{fig:h-ablation}
\end{figure*}

\begin{figure*}[!h]
    \centering
    \includegraphics[width=1.0\textwidth]{figs/sim2sim.png}
    \vspace{-3mm}
    \caption{
    \footnotesize{\textbf{Normalized Rewards for Sim-to-Sim Transfer}. We plot the normalized rewards for two sim-to-sim transfer tasks, where the rewards are normalized by the maximum reward achieved by any method. 
    }}
    \vspace{-5mm}
    \label{fig:sim_plots}
\end{figure*}


% \subsection{Overview of Sim-to-sim Experiments}



\section{Limitations and Future Work}
We present \Method, a general framework for efficient sim-to-real fine-tuning with existing RL algorithms. The key idea is to leverage value functions learned in simulation to provide guidance for real-world exploration. In the future it will be essential to scale methods to work directly from perceptual inputs. Calculating dense rewards from raw visual inputs is challenging, and represents an important limitation of the current instantiation of \Method. Future work will investigate which visual modalities lead to robust sim-to-real transfer. Moreover, despite the sample complexity gains \Method\ affords existing RL algorithms, we believe new base adaptation algorithms designed specifically for low-data regimes will be needed to make truly autonomous real world adaptation practical. For all methods tested, we found that tuning for real-world performance was time-consuming. Thus, future work will investigate using large simulated data sets to distill novel policy optimization algorithms that can be transferred to the real-world with less effort. 


\section{Acknowledgments}
We would like to thank Marius Memmel, Vidyaaranya Macha, Yunchu Zhang, Octi Zhang, and Arhan Jain for their invaluable engineering help. We would like to thank Lars Lien Ankile and Marcel Torne for their fruitful engineering discussions and constructive feedbacks. Patrick Yin is supported by sponsored research with Hitachi and the Amazon Science Hub. 

\begin{comment}

\subsection{Does \Method\ \ enable efficient real-world fine-tuning for dynamic tasks?}
\label{sec:real_world_experiments}
We evaluate \Method\  on three real-world tabletop manipulation tasks: hammering, pushing, and inserting. For all three tasks, we use a Franka FR3 robot operating with Cartesian position control at 5Hz for fine-tuning and evaluating task performance in the real world. 

For hammering and pushing, we compute the object's position by color-thresholding the pointclouds from two calibrated depth cameras - this approach could easily be replaced by a more sophisticated pose estimation system if desired. Sparse reward is computed from measured proprioception of the robot and estimated pose of the object. For inserting, we compute object positions by using Realsense cameras to detect and estimate poses of April Tags that are attached to each object. Then, we use these object poses to compute dense reward which is used for fine-tuning. 

The results for real-world evaluation during fine-tuning on these three tasks are presented in Fig.~\ref{fig:real_plots}. For all three tasks, zero-shot performance seen at the start of the plot is quite poor due to the dynamics gap between sim and real. Our full system consistently leads to significant improvement from fine-tuning, achieving 100\% success for hammering and pushing within an hour of fine-tuning, \textcolor{red}{and ??? success for inserting within two hours of fine-tuning}. Running our method without hallucinations greatly reduces the sample efficiency of our method due to the lack of real-world data to train on early in fine-tuning. Standard off-policy reinforcement learning without hallucinations and PBRS achieves achieves much lower success for puck pushing and collapses for hammering due to the lack of gradient signal from collected sparse, environment reward. \textcolor{red}{TODO FOR INSERTING}
\end{comment}
\begin{comment}
\begin{figure}[h]
    \begin{center}
        \includegraphics[width=0.5\linewidth]{figs/puck_pushing_vis.png}
        \vspace{-3mm}
        \caption{
        \footnotesize
        \textbf{Visualization of real rollout, hallucinated states, and value function.} The red dots indicate states along a real rollout in simulation. The blue dots indicate hallucinated states branching off real states generated by the learned dynamics model. The green heatmap indicates the value function estimates at different states. A corresponding image of the state is shown for two states. Since it is hard to directly visualize states and values due to the high-dimensionality of the state space, we only show a part of the trajectory where the puck does not move. This allows us to visualize states and values along changes in only end effector xy.
        }
        \label{fig:puck_pushing_vis}
        \vspace{-2mm}
    \end{center}
\end{figure}
\end{comment}




\begin{comment}
\begin{lemma} 
\twnote{dont actually need this here?}(\citep{chebotar2017combining)} Assume that for each $s \in \S$ and $a \in \A$ we have $\|p_{real}(s,a)-\hat{p}(s,a) \|_1 \leq \delta$ for $\delta >0$. Then for each $s \in \mathcal{S}$ and $a \in \A$ we have:
\begin{equation}
\tilde{V}_H^{\hat{\pi}}(s) - V_H^{*}(s)  \leq \left[\gamma\left(\frac{1-\gamma^{H-1}}{1-\gamma}\right) \left(r_{max} -r_{min}\right) + \gamma^H \left(V_{max} - V_{min}\right) \right] \delta H
\end{equation}
\end{lemma}

\end{comment}




% \begin{comment}



% It is crucial for us to understand both $a)$ when small choices of $H$ yields a successful simulation-guided expert policy $\pi_H^*$ and $b)$ when the behavior of $\pi_H^*$ is easy to imitate over long horizons. Together, these two features will guarantee that we can efficiently learn policies which complete the task by optimizing \Cref{eq:short_horizon} within the \Method\ framework introduced above. \twnote{revisit this sentence} Our main theoretical result is to demonstrate that this will be the case when $\pi_{sim}$ has error-correcting behaviors that are encoded into $V_{sim}$. A similar requirement is essential for efficient learning even for methods such as DAgger, which assume expert actions can be queried directly \twnote{citations}. We formalize this notion as follows:

% \begin{equation}
% V_{H,T}^*(s) = \mathbb{E}_{\rho^{\pi_H^*}(s)} \left[V_{sim}(S_T) + \sum_{t=0}^{T-1}r(s_t)\right]
% \end{equation}

% \begin{equation}
% Q_{H,T}^*(s,a) = \mathbb{E}_{s' \sim p_{real}(s,a)}\left[ V_{H,T}^*(s') + r(s,a)\right]  
% \end{equation}

% \begin{lemma}
% Let a policy $\pi$ be such that $\mathbb{E}_{s \sim \rho_{real}^\pi}\left[ Q_{H,t}^*(s,\pi) - Q(s,\pi_H^*)\right]$

% \twnote{need to resolve notation for this so that it depends on the initial distribution}. 
% \end{lemma}


% \begin{equation}
% \tilde{Q}_H(s_0,a)= \mathbb{E}\left[
% }V_{sim}(s_{h}) + \sum_{t=0}^{H-1}r(s_t,a_t)\right]
% \end{equation}

% \begin{equation}
% \tilde{\pi}_H^*(s) \leftarrow \max_{a}\tilde{Q}_H(s,a)
% \end{equation}

% \begin{definiion}
% adsfasdf
% \end{definition}

% \begin{enumerate}
% \item Define what we mean by following the value function over $H$-steps
% \item Define Error-Correcting Policies
% \item 
% \end{enumerate}


% \subsection{Justifying and Leveraging Short-Horizon Model Rollouts}

% \subsection{A Reduction to Online Learning}

% \end{comment}

\bibliography{references}
\bibliographystyle{iclr2025_conference}

% \bibliography{iclr2025_conference}
% \bibliographystyle{references}\newpage
\newpage
\appendix
\section{Proofs}\label{sec:proofs}
\paragraph{Notation Recap.} We remind the reviewer of notation we have built up throughout the paper. We use the `hat' notation to denote a generative dynamics model $\hat{p}$, as well that the optimal values $\hat{V}_H^*$, $\hat{Q}_H^*$ obtained by optimizing the $H$-step objective under these dynamics. $\hat{\pi}_H^*$ is then the policy obtained by optimizing the $H$-step objective under the model dynamics: $\hat{\pi}_H(|s) \leftarrow \max_{\pi}\hat{Q}_H^*(a,\pi)$.

We first present several Lemma's used in the proof of Theorem \ref{thm:subopt}. The first result bounds the difference between the true $H$-step returns for a policy $\pi_H$ and the $H$-step returns predicted under the dynamics model $\hat{p}$. 
\begin{lemma}\cite[Lemma A.1.]{bhardwaj2020blending}\label{lem:mow} Suppose that $\| \hat{p}(s,a) -p_{real}(s,a)\|_1 \leq \alpha$. Further suppose $\Delta r = \max_{s} r(s) - \min_{s}r(s)$ and $\Delta V =\max_{s} V_{s}(s) - \min_{s}V_{s}(s)$ are finite. Then, for each policy $\pi$ we may bound the $H$-step returns under the model and true dynamics by:
\begin{equation}
\|\hat{V}_H^{\pi_H}(s) - V_H^{\pi_H}\|_\infty \leq \gamma \left(\frac{1-\gamma^{H-1}}{1-\gamma}\frac{\Delta r}{2} + \gamma^H\frac{\Delta V}{2}\right) \cdot \alpha H.
\end{equation}
\end{lemma}
\begin{proof}
This result follows imediatly from the proof of \cite[Lemma A.1.]{bhardwaj2020blending}, with changes to notation and noting that we assume access to the true reward. In particular, the full result of \cite[Lemma A.1.]{bhardwaj2020blending} includes an extra $\frac{1-\gamma^H}{1-\gamma}\alpha$ term which comes from the usage of a model $\hat{r}$ which estimates the true reward. We do not consider such effects, and thus suppress this dependence.  
\end{proof}

The next result uses this \ref{lem:mow} to bound the difference between the optimal $H$-step returns and the $H$-step returns generated by the policy $\hat{\pi}_H^*$ which is optimal under the dynamics model $\hat{p}$: 
\begin{lemma}\label{lem:dyn}
Suppose that $\| \hat{p}(s,a) -p_{real}(s,a)\|_1 \leq \alpha$. Further suppose $\Delta r = \max_{s} r(s) - \min_{s}r(s)$ and $\Delta V =\max_{s} V_{s}(s) - \min_{s}V_{s}(s)$ are finite. Then for each state $s \in \mathcal{S}$ we have:
\begin{equation}
 V_H^*(s)  - V_H^{\hat{\pi}_H^*}(s)\leq  \left(\frac{1-\gamma^{H-1}}{1-\gamma}\Delta r + \gamma^H\Delta V\right)  
\end{equation}
where $\hat{\pi}_H^* \leftarrow \max_{\pi_H} \hat{V}^{\tilde{\pi}_H}(s)$.
\end{lemma}
\begin{proof}
Let $\pi_H^* \leftarrow \max_{\pi_H}V_H^{\pi_H}(s)$ be the optimal $H$-step policy under the true dynamics. By Lemma \ref{lem:mow} we have both that
\begin{equation}
 V_H^*(s) \leq \hat{V}^{\pi_H^*}(s) +  \gamma \left(\frac{1-\gamma^{H-1}}{1-\gamma}\frac{\Delta r}{2} + \gamma^H\frac{\Delta V}{2}\right) \cdot \alpha H.
\end{equation}
\begin{equation}
\hat{V}_H^{\hat{\pi}_H^*}(s) \leq V_H^{\hat{\pi}_H^*}(s)+ \gamma \left(\frac{1-\gamma^{H-1}}{1-\gamma}\frac{\Delta r}{2} + \gamma^H\frac{\Delta V}{2}\right) \cdot \alpha H.
\end{equation}
Combining these two bounds with the fact that $\hat{V}^{\pi_H^*}(s) \leq \hat{V}_H^{\hat{\pi}_H^*}(s)$ yields the desired result. 
\end{proof}

The following result establishes an important monotonicity property on the optimal $H$-step value functions which is important for the main result.: 
\begin{lemma}\label{lemma:1_{sim}tep}
Suppose that $\sup_{a}\mathbb{E}_{s\sim p_{real}(s,a)}[\gamma V_{sim}(s')] - V_{sim}(s) > -r(s)$. Then we have $V_{H}^*(s) \geq  V_{H-1}^*(s)$ for each $s \in \mathcal{S}$.
Then for each $s \in \mathcal{S}$ we have:
\begin{equation}
V_{H}^*(s) \geq V_{H-1}^*(s)
\end{equation}

\end{lemma}
\begin{proof}
 Fix an initial condition $s_0 \in \mathcal{S}$. Let $\pi$ be arbitrary, and fix the shorthand $\pi^* = \set{\pi_0^*, \dots, \pi_{H-1}^*}$ for the time-varying policy $\pi^* \leftarrow \max_{\hat{\pi}}V_{H-2}^{\hat{\pi}}(s_0)$. Then, concatenate these policies to define: $\bar{\pi} = \set{\pi_1^*, \dots, \pi_{H-2}^*, \pi}$, which is simply the result of applying the optimal policy for the $(H-1)$-step look ahead objective \Cref{eq:short_horizon} starting from $s_0$, followed by applying $\pi$ for a single step. Letting the following distributions over trajectories by generated by $\pi^*$, by the definition of $V_H^*$: 
\begin{align*}
&V_H^*(s_0) \\
&\geq \mathbb{E}\left[\gamma^{H}V_{sim}(s_{H}) +\sum_{t=1}^{H-1} \gamma^{t}r(s_t) - V_{sim}(s_0)\right]\\
&=\mathbb{E}\left[\gamma^{H}V_{sim}(s_{H}) -\gamma^{H-1}V_{sim}(s_{H-1})+ \gamma^H r(s_{H-1})\right] \\
& \qquad \qquad + \mathbb{E}\left[\gamma^{H-1} V_{sim}(s_{H-1}) + \sum_{t=1}^{H-2} \gamma^{t}r(s_t)   - V_{sim}(s_0)\right]\\ 
& = \mathbb{E} \bigg[\gamma^{H}V_{sim}(s_{H}) -\gamma^{H-1}V_{sim}(s_{H-1})+ \gamma^H r(s_{H-1}) + V_{H-1}^*(s_0) \bigg]
\end{align*}
The first inequality follows from the fact that the return of $\bar{\pi}$ cannot be greater than that of $\pi^*$, the first equality follows from rearanging terms to isolate $V_{H-1}^*$, and the second equality follows from the definition of $V_{H-1}^*$. 
Now, since our choice of $\pi$ used to define $\bar{\pi}$ was arbitrary, we choose $\pi$ to be deterministic and such that $\mathbb{E}_{s' \sim p_{real}(s,a)}[\gamma V_{sim}(s')] - V_{sim}(s) > -r(s)$ at each state $s \in \mathcal{S}$,  as guaranteed by the assumption made for the result. This choice of policy grantees that:
\begin{equation}
\mathbb{E} \bigg[\gamma^{H}V_{sim}(s_{H}) -\gamma^{H-1}V_{sim}(s_{H-1})+ \gamma^H r(s_{H-1})\bigg] \geq 0.
\end{equation}

The desired result follows immediately by combining the two preceding bounds, and noting that our choice of initial condition was arbitrary, meaning the preceding analysis holds for all initial conditions. 
\end{proof}


%\textbf{Proof of Theorem \ref{thm:subopt}}
Our final lemma bounds the sub-optimality of \Method\ policies $\pi$ in terms of $a)$ errors in the sim value function and $b)$ additional suboptimalities cause by $\pi$ being sub-optimal for the $H$-step objective: 
\begin{lemma}\label{lem:subopt}
Suppose that $V_{sim}$ is improvable and further suppose that $\max_{s \in \S}|V_{sim}(s) -V_{real}^*(s)|< \epsilon$. Then any policy $\pi$ which satisfies $A_H^*(s,\pi)=Q_H^*(s,\pi)-V_H^*(s) \geq -\delta$ will satisfy:
\begin{equation}
V_{real}^{*}(s) - V_{real}^{\pi}(s) \leq \gamma^{H}\epsilon + \frac{\delta}{1-\gamma}.
\end{equation}
\end{lemma}
\begin{proof}
Our goal is first to bound how $Q_H^*(s,\pi)$ changes on expectation when applying the given policy for a single step. We have that:
\begin{equation}
Q_{H}^*(s,\pi) + \delta \geq V_H^*(s)
\end{equation}
\begin{equation}
V_{H}^*(s) \geq V_{H-1}^{*}(s)
\end{equation}
\begin{equation}
Q_{H}^*(s,\pi) = \mathbb{E}[\gamma V_{H-1}^*(s') + \bar{r}(s,s')]
\end{equation}
where the first inequality follows from the ssumption of the theorem, the second inequality follows from Lemma \ref{lemma:1_{sim}tep} and, the third inequality is simply the definition of $Q_H^*$. Letting $s' \sim p_{real}(s,a)$ with $a \sim \pi(\cdot | s)$, we can take expectations can combine the previous relations to obtain: 
\begin{equation}
\mathbb{E}\left[\gamma Q_{H}^*(s',\pi)+ \bar{r}(s,s')\right] +   \gamma \delta \geq \mathbb{E}\left[\gamma V_H^*(s') + \bar{r}(s,s')\right] +\geq \mathbb{E}\left[\gamma V_{H-1}^*(s') + \bar{r}(s,s')\right]   = Q_H^*(s,\pi).
\end{equation}
That is:
\begin{equation}
\gamma\mathbb{E}\left[Q_{H}^*(s',\pi)\right] + \bar{r}(s) + \gamma \delta \geq Q_H^*(s,\pi).
\end{equation}
Alternatively:
\begin{equation}\label{eq:r_bound}
\bar{r}(s) \geq Q_{H}^*(s,\pi) - \gamma \mathbb{E}[Q_H^*(s',\pi)] - \gamma \delta.
\end{equation}
Next, we use this bound to provide a lower bound for $V_{real}^\pi(s)$. Because the previous analysis holds at all states when we apply $\pi$, the following holds over the distribution of trajectories generated by applying $\pi$ starting from the initial condition $s_0:$
\begin{align*}\mathbb{E}_{\rho_{real}^\pi(s)}\left[\sum_{t=0}^\infty \gamma^t \bar{r}(s_t)\right] &= V_{real}^\pi(s) - V_{s}(s_0) \\
&\geq \mathbb{E}_{\rho_{real}^\pi(s)}\left[\sum_{t=0}^{\infty}\gamma^t \bigg(Q_H^*(s_t,\pi) - \gamma Q_H^*(s_{t+1},\pi) \bigg)\right]  - \gamma \delta \sum_{t=0}^\infty \gamma^t \\
& = Q_H^*(s_0,\pi) - \frac{\gamma \delta }{1-\gamma},
\end{align*}
where we have repeatedly telescoped out sums to cancel out terms in the first equality, used \eqref{eq:r_bound} in the second equality, and canceled out terms to generate the final equality. 


Thus, we have the lower-bound:
\begin{align}\label{eq:performance_bound1}
V_{real}^\pi(s) \geq Q_H^*(s,\pi) + V_{sim}(s_0) - \frac{\gamma \delta}{1-\gamma}\\
\nonumber \geq V_{H}^*(s) + V_{sim}(s_0) - \frac{\gamma \delta}{1-\gamma}
\end{align}
Next, we may bound:
\begin{align}\label{eq:diff_bound}
V_{H}^*(s_0) + V_{sim}(s_0) &\geq \mathbb{E}_{\rho^{\pi_{real}^*}(s_0)} \left[ \gamma^H V_{sim}(s_H) + \sum_{t =0}^{H-1} \gamma^t r(s_t)\right] \\ \nonumber
& =\mathbb{E}_{\rho^{\pi_{real}^*}(s_0)}  \left[\gamma^H V_{sim}(s_H) -\gamma^H V_{real}^*(s_H) + \gamma^H V_{real}^*(s_H) + \sum_{t =0}^{H-1} \gamma^t r(s_t)\right] \\ \nonumber
&=\mathbb{E}_{\rho^{\pi_{real}^*}(s_0)}\left[ \gamma^H V_{sim}(s_H) -\gamma^H V_{real}^*(s_H)\right] + V_{real}^*(s_0).
\end{align}
Invoking the assumption that $\max_{s}|V_{sim}(s) - V_{real}^*(s)| < \epsilon$, we can combined this with the preceding bound to yield: 
\begin{equation}
V_{H}^*(s_0) + V_{sim}(s_0) \geq V_{real}^*(s) -\gamma^H \epsilon.
\end{equation}
Finally, once more invoking the fact that $Q_H^*(s,\pi) + \delta \geq V_H^*(s)$ for each $s \in S$ and combining this with \Cref{eq:performance_bound1} and \Cref{eq:diff_bound}, we obtain that:
\begin{align*}
V_{real}^\pi(s) &\geq Q_H^*(s,\pi) + V_{sim}(s_0) - \frac{\gamma \delta}{1-\gamma} \\
&\geq V_{H}^*(s_0) + V_{sim}(s_0)- \frac{\gamma \delta}{1-\gamma} -\delta\\
&\geq V_{real}^*(s)  - \frac{\gamma \delta}{1-\gamma} -\delta - \gamma^H \epsilon\\
& =V_{real}^*(s) -  \frac{\delta }{1-\gamma}-\gamma^H \epsilon
\end{align*}
from which the state result follows immediately. 
\end{proof}

\noindent
\textbf{Proof of Theorem \ref{thm:subopt}:}
\begin{proof}
The result follows directly from a combination of Lemma \ref{lem:subopt} and Lemma \ref{lem:dyn} by suppressing problem-dependent constants and lower order terms in the discount factor $\gamma$. 
\end{proof}

\section{Environment Details}\label{sec:envs}
\textbf{Sim2Real Environment.}
We use a 7-DoF Franka FR3 robot with a 1-DoF parallel-jaw gripper. Two calibrated Intel Realsense D455 cameras are mounted across from the robot to capture position of the object by color-thresholding pointcloud readings or retrieving pose estimation from aruco tags. Commands are sent to the controller at 5Hz. We restrict the end-effector workspace of the robot in a rectangle for safety so the robot arm doesn't collide dangerously with the table and objects outside the workspace. We conduct extensive domain randomization and randomize the initial gripper pose during simulation training. The reward is computed from measured proprioception of the robot and estimated pose of the object. Details for each task are listed below.

\textbf{Hammering.}
For hammering, the action is 3-dimensional and sets delta joint targets for 3 joints of the robot using joint position control. The observation space is 12-dimensional and includes end-effector cartesian xyz, joint angles of the 3 movable joints, joint velocites of the 3 movable joints, the z position of the nail, and the xz position of the goal. Each trajectory is 50 timesteps. In simulation, we randomize over the position, damping, height, radius, mass, and thickness of the nail. Details are listed in Tab.~\ref{tab:domain_rand_hammer}. 

The reward function is parameterized as $r(t) = -10 \cdot r_{\text{nail}-\text{goal}}(t)$ where $r_{\text{nail}-\text{goal}}=(\textbf{r}_{\text{nail}})_z - (\textbf{r}_{\text{goal}})_z$ represents the distance in the z dimension of the nail head to the goal, which we set to be the height of the board the nail is on.

\textbf{Pushing.}
For pushing, the action is 2-dimensional and sets delta cartesian xy position targets using end-effector position control. The observation space is 4-dimensional and includes end-effector cartesian xy and the xy position of the puck object. Each trajectory is 40 timesteps. In simulation, we randomize over the position of the puck. Details are listed listed in Tab.~\ref{tab:domain_rand_puck}. 

Let $\textbf{r}_{\text{ee}}$ be the cartesian position of the end effector and $\textbf{r}_{\text{obj}}$ be the cartesian position of the object. The reward function is parameterized as $r(t) = - r_{\text{ee}-\text{goal}}(t) - r_{\text{obj}-\text{goal}}(t) + r_{\text{threshold}}(t) - r_{\text{table}}(t)$ where $r_{\text{ee}-\text{goal}}(t)=\| \textbf{r}_{\text{ee}}(t) - \textbf{r}_{\text{obj}}(t) + [3.5\text{cm}, 0.0\text{cm}, 0.0\text{cm}] \|$ represents the distance of the end effector to the back of the puck, $r_{\text{obj}-\text{goal}}(t)=\| (\textbf{r}_{\text{obj}}(t))_x - 55\text{cm} \|$ represents the distance of the puck to the goal (which is the edge of the table along the x dimension), $r_{\text{threshold}}(t)=\mathbb{I}[r_{\text{obj}-\text{goal}}(t) \geq 2.5\text{cm}]$ represents a goal reaching binary signal, and $r_{\text{table}}(t)=\mathbb{I}[(r_{\text{obj}}(t))_z \leq 0.0]$ represents a binary signal for when the object falls of the table.

\textbf{Inserting.}
For inserting, the action is 3-dimensional and sets delta cartesian xyz position targets using end effector position control. The observation space is 9-dimensional and includes end-effector cartesian xyz, the xyz of the leg, and the xyz of the table hole. Each trajectory is 40 timesteps.  We enlarge the table by a scale of 1.08 compared in the original table in FurnitureBench in order to make the table leg insertable without twisting. In simulation, we randomize the initial gripper position, position of the table, and friction of both the table and the leg.

Let $\mathbf{r}_{\text{pos1}}(t)$ and $\mathbf{r}_{\text{pos2}}(t)$ represent the Cartesian positions of the leg and table hole. Let: 

\[
x_{\text{distance}}(t) = \text{clip}\left( | \mathbf{r}_{\text{pos1},x}(t) - \mathbf{r}_{\text{pos2},x}(t) |, 0.0, 0.1 \right)
\]
\[
y_{\text{distance}}(t) = \text{clip}\left( | \mathbf{r}_{\text{pos1},z}(t) - \mathbf{r}_{\text{pos2},z}(t) |, 0.0, 0.1 \right)
\]
\[
z_{\text{distance}}(t) = \text{clip}\left( | \mathbf{r}_{\text{pos1},y}(t) - \mathbf{r}_{\text{pos2},y}(t) |, 0.0, 0.1 \right)
\]

Let the success condition be defined as:
\[
r_{\text{success}}(t) = \mathbb{I}\left[ x_{\text{distance}}(t) < 0.01 \, \text{and} \, y_{\text{distance}}(t) < 0.01 \, \text{and} \, z_{\text{distance}}(t) < 0.01 \right]
\]

The reward function is now:

\[
r(t) = r_{\text{success}}(t) - 100 *\left( x_{\text{distance}}(t)^2 + y_{\text{distance}}(t)^2 + z_{\text{distance}}(t)^2 \right)
\]

% \begin{wrapfigure}{r}{0.5\textwidth}
% \begin{figure}
% \vspace{-8mm}
% \begin{center}
%     \includegraphics[width=\linewidth]{figs/deformable_pushing.png}
%     \vspace{-3mm}
%     \caption{\footnotesize
%     \textbf{Sim-to-Real Setup} Two additional deformable object manipulation tasks -- pushing a squishy toy (\textbf{left}) and a towel (\textbf{right}).}
%     \label{fig:deformable_pushing}
%     \vspace{-0.8cm}
% \end{center}
% \end{figure}
% \end{wrapfigure}

\textbf{Sim2Sim Environment.}
We additionally attempt to model a sim2real dynamics gap in simulation by taking the hammering environment and create a proxy for the real environment by fixing the domain randomization parameters, fixing the initial gripper pose, and rescaling the action magnitudes before rolling out in the environment.

% \textbf{Deformable Pushing Tasks} 
% We show in \Cref{fig:deformable_pushing} the two additional deformable pushing tasks we run -- cloth pushing and ball pushing.

\section{Implementation Details} 
\textbf{Algorithm Details.}
We use SAC as our base off-policy RL algorithm for training in simulation and fine-tuning in the real world. For our method, we additionally add in two networks: a dynamics model that predicts next state given current state and action, and a state-conditioned value network which regresses towards the Q-value estimates for actions taken by the current policy. These networks are training jointly with the actor and critic during SAC training in simulation.

\textbf{Network Architectures.}
The Q-network, value network, and dynamics model are all parameterized by a two-layer MLP of size 512. The dynamics model is implemented as a delta dynamics model where model predictions are added to the input state to generate next states. The policy network produces the mean $\mu_a$ and a state-dependent log standard deviation $\log{\sigma_a}$ which is jointly learned from the action distribution. The policy network is parameterized by a two-layer MLP of size 512, with a mean head and log standard deviation head on top parameterized by a FC layer. 

\textbf{Pretraining in Simulation.} 
For hammering and puck pushing, we collect 25,000,000 transitions of random actions and pre-compute the mean and standard deviation of each observation across this dataset. We train SAC in simulation on the desired task by sampling 50-50 from the random action dataset and the replay buffer. We normalize our observations by the pre-computed mean and standard deviation before passing them into the networks. We additionally add Gaussian noise centered at 0 with standard deviation 0.004 to our observations with 30\% probability during training. For inserting, we train SAC in simulation with no normalization. We train SAC with autotuned temperature set initially to 1 and a UTD of 1. We use Adam optimizer with a learning rate of $3 \times 10^{-4}$, batch size of 256, and discount factor $\gamma=.99$.

\textbf{Fine-tuning in Real World.}
We pre-collect 20 real-world trajectories with the policy learned in simulation to fill the empty replay buffer. We then reset the critic with random weights and continue training SAC with a fixed temperature of $\alpha =0.01$ and with a UTD of $2d$ with the pretrained actor and dynamics model. We freeze the value network learned from simulation and use it to relabel PBRS rewards during fine-tuning. During fine-tuning, for each state sampled from the replay buffer, we additionally hallucinate 5 branches off and add it to the training batch. As a result, our batch size effectively becomes 1536. The policy, Q-network, and dynamics model are all trained jointly on the real data during SAC fine-tuning. We don't train on any simulation data during real-world fine-tuning because we empirically found it didn't help fine-tuning performance in our settings.

\begin{figure}
\centering
\begin{minipage}[t]{.45\textwidth}
    \centering

    \captionof{table}{Domain randomization of hammering task in simulation}
    \centering
    \begin{tabular}{lcccc}
    \toprule
    Name & Range \\
    \midrule
    Nail x position (m) & [0.3, 0.4] \\
    Nail z position (m) & [0.55, 0.65] \\
    Nail damping & [250.0, 2500.0] \\
    Nail half height (m) & [0.02, 0.06] \\
    Nail radius (m) & [0.005, 0.015] \\
    Nail head radius (m) & [0.03, 0.04] \\
    Nail head thickness (m) & [0.001, 0.01] \\
    Hammer mass (kg) & [0.015, 0.15] \\
    \bottomrule
    \end{tabular}
    \label{tab:domain_rand_hammer}
\end{minipage}
% \vspace{-1.5em}
\hspace{1mm}
\begin{minipage}[t]{.45\textwidth}

    \captionof{table}{Domain randomization of pushing task in simulation}
    \centering
    \begin{tabular}{lcccc}
    \toprule
    Name & Range \\
    \midrule
    Object x position (m) & [0.0, 0.3] \\
    Object y position (m) & [-0.25, 0.25] \\
    \bottomrule
    \end{tabular}
    \label{tab:domain_rand_puck}

    \captionof{table}{Domain randomization of inserting task in simulation} 
    \centering
    \begin{tabular}{lcccc}
    \\
    \toprule
    Name & Range \\
    \midrule
    Parts x/y position (m) & [-0.05, 0.05] \\
    Parts rotation (degrees) & [0, 15] \\
    Parts friction & [-0.01, 0.01] \\
    \bottomrule
    \end{tabular}
    \label{tab:domain_rand_puck}

\end{minipage}
\end{figure}


\newpage

\section{Qualitative Results}
\label{app:vfviz}
% \begin{wrapfigure}{r}{0.6\linewidth}
\begin{figure}
    \begin{center}
        \includegraphics[width=\linewidth]{figs/puck_pushing_vis.png}
        \vspace{-10mm}
        \caption{
        \footnotesize
        \textbf{Visualization of real rollout, hallucinated states, and value function.} The red dots indicate states along a real rollout in simulation. The blue dots indicate hallucinated states branching off real states generated by the learned dynamics model. The green heatmap indicates the value function estimates at different states. A corresponding image of the state is shown for two states. Since it is hard to directly visualize states and values due to the high-dimensionality of the state space, we only show a part of the trajectory where the puck does not move. This allows us to visualize states and values along changes in only end effector xy.
        }
        \label{fig:puck_pushing_vis}
        \vspace{-5mm}
    \end{center}
\end{figure}
% \end{wrapfigure}

We analyze the characteristics of hallucinated states and value functions in Fig.~\ref{fig:puck_pushing_vis}. We visualize a trajectory of executing puck pushing in simulation using the learned policy in this plot. The red dots indicate states along a real rollout in simulation. The blue dots indicate hallucinated states branching off real states generated by the learned dynamics model. The green heatmap indicates the value function estimates at different states. A corresponding image of the state is shown for two states. The trajectory shown in the figure shows the learned policy moving closer to the puck before pushing it. The value function heatmap shows higher values when the end effector is closer to the puck and lower values when further. Hallucinated states branching off each state show generated states for fine-tuning the learned policy. 

Note that it is hard to directly visualize states and values due to the high-dimensionality of the state space. To get around this for puck pushing, we only show a part of the trajectory where the puck does not move. This allows us to visualize states and values along changes in only end effector xy.

\begin{comment}
\newpage
\section{Sim-to-Sim Experiments}
\label{app:sim2sim}
Here we additionally test each of the proposed methods on the sim-to-sim set-up from \cite{du2021auto}, which is meant to mock sim-to-real gaps but for familiar RL benchmark tasks. The results are depicted in Figure \ref{fig:sim_plots} for the Walker Walk, Cheetah Run, and Rope Peg-in-Hole environments. For all tasks, we use the precise settings from \cite{du2021auto}. Note that the general trend of these results matches our real world experiments -- \Method\ substantially accelerates learning and overcomes the dynamics gap between the `simulation' and `real' environments. 

Finally, we additionally use the Walker and Peg-in-Hole environments to ablate the effects of the hyper parameter $H$. Intuitively, the peg-in-hole environment requires much more precise actions, and is thus should be more sensitive to errors in the pretraining environment. Thus, we should expect that SGFT will benefit from larger values of $H$ for this environment, as this will correspond to relying more heavily on returns from the target environment. We see that this trend holds in Figure \ref{fig:h-ablation}, where the Walker environments is barely affected by the choice of $H$ but this hyper parameter has a large impact on the performance in the Peg-in-Hole Environment.

\begin{figure*}[!h]
    \centering
    \includegraphics[width=1.0\textwidth]{ICLR 2025 Template/figs/H-ablation.pdf}
    \vspace{-3mm}
    \caption{
    \footnotesize{\textbf{Normalized Rewards for Sim-to-Sim Transfer}. Ablating the effects of the horizon $H$ across two sim-to-sim expirments. The choice of $H$ has a much larger effect on the peg-in-hole task, which requires much more precise actions to achieve succes. 
    }}
    \vspace{-5mm}
    \label{fig:h-ablation}
\end{figure*}

\begin{figure*}[!h]
    \centering
    \includegraphics[width=1.0\textwidth]{figs/sim2sim.png}
    \vspace{-3mm}
    \caption{
    \footnotesize{\textbf{Normalized Rewards for Sim-to-Sim Transfer}. We plot the normalized rewards for two sim-to-sim transfer tasks, where the rewards are normalized by the maximum reward achieved by any method. 
    }}
    \vspace{-5mm}
    \label{fig:sim_plots}
\end{figure*}







\end{comment}








\end{document}
