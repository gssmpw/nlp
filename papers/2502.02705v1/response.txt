\section{Related Work}
\label{sec:related}
\noindent\textbf{Simulation-to-Reality Transfer.} Two main approaches have been proposed for overcoming the dynamics gap between simulation and reality:  1) adapting simulation parameters to real-world data **Peng et al., "Adversarial Variational Models for Simulation-to-Reality Transfer"** and 2) learning adaptive or robust policies to account for uncertain real-world dynamics **Lowrey et al., "Robust Policy Learning from Demonstrations"**. However, these approaches still display failure modes in regimes where simulation and reality diverge substantially **Kumar et al., "Exploring Dynamics Gap with Simulation-to-Reality Transfer"** -- namely, where no set of simulation parameters closely match the real-world dynamics.

\noindent\textbf{Adapting Policies with Real-World Data.} 
Many RL approaches have focused on the general fine-tuning problem **Chen et al., "Policy Optimization from Offline Data with Deep Deterministic Policy Gradients"**. These works initialize policies, Q-functions, and replay buffers from offline data and continue training them with standard RL methods. A number of works have specifically considered mixing simulated and real data during policy optimization -- either through co-training **Gu et al., "Co-Training Reinforcement Learning Policies from Simulated and Real-World Data"**, simply initializing the replay-buffer with simulation data **Jiang et al., "Reinforcement Learning from Simulation to Reality with Experience Replay Buffer Initialization"**, or adaptively sampling the simulated dataset and up-weighting transitions that approximately match the true dynamics **Kumar et al., "Adaptive Sampling for Reinforcement Learning from Simulation"**. However, recent work **Peng et al., "On the Importance of Domain Adaptation in Simulation-to-Reality Transfer"** has demonstrated that there are minimal benefits to sampling off-domain samples when adapting online in new environments, as this can bias learned policies towards sub-optimal solutions. In contrast to these prior approaches -- which primarily use simulated experience to initialize real-world learning -- we focus on distilling effective exploration strategies from simulation, using $V_{sim}$ to guide real-world learning. We demonstrate theoretically that this approach to transfer leads to low bias in the policies learned by \Method.

\begin{comment}
. While going back from the real world to simulation can help target the simulation parameters more accurately **Lowrey et al., "Real-to-Simulation Transfer for Reinforcement Learning"**, it cannot overcome inherent model misspecification, as we show in our experimental evaluation. Learning adaptive policies to account for changing real-world dynamics **Kumar et al., "Exploring Dynamics Gap with Simulation-to-Reality Transfer"** can help to some extent but is unable to guide exploration and adapt beyond the training dynamics range.\end{comment} Prior work has additionally considered mixing simulated and real data during policy optimization -- either through co-training **Gu et al., "Co-Training Reinforcement Learning Policies from Simulated and Real-World Data"**, simply initializing the replay-buffer with simulation data **Jiang et al., "Reinforcement Learning from Simulation to Reality with Experience Replay Buffer Initialization"**, or adaptively sampling the simulated dataset and up-weighting transitions that approximately match the true dynamics **Kumar et al., "Adaptive Sampling for Reinforcement Learning from Simulation"**. However, recent work **Peng et al., "On the Importance of Domain Adaptation in Simulation-to-Reality Transfer"** has demonstrated that there are minimal benefits to sampling off-domain samples when adapting online in new environments, as it introduces bias into the learned policy.  In contrast, our approach focuses on distilling effective \emph{guidance for exploration} from simulation, and which we demonstrate theoretically leads to low-bias in the policies learned by \Method.

\noindent\textbf{Adapting Policies with Real-World Data.} 
Many RL approaches have focused on the general fine-tuning problem **Chen et al., "Policy Optimization from Offline Data with Deep Deterministic Policy Gradients"**. These works initialize policies, Q-functions, and replay buffers from offline data and continue training them with standard RL methods, but do not use the pre-training data beyond initialization and populating the replay buffer. We utilize the pretraining in simulation not just for initialization but also to provide guidance \emph{throughout} the real-world policy improvement process.

% Moreover, the learned value functions and models in our proposed methodology have coverage over state-space as opposed to limited coverage pre-training methods with offline RL and imitation learning.

\noindent\textbf{Reward Design in Reinforcement Learning.} 
A significant component of our methodology is learning dense shaped reward in simulation to guide real-world fine-tuning. Prior techniques have tried to infer rewards from expert demos **Kumar et al., "Learning Dense Rewards from Expert Demonstrations"**, success examples **Lowrey et al., "Reward Design for Reinforcement Learning with Success Examples"**, LLMs **Peng et al., "Learning Reward Functions from Large Language Models"**, and heuristics **Gu et al., "Heuristic-Based Reward Design for Reinforcement Learning"**. We rely on simulation to provide reward supervision **Peng et al., "PBRS: A Framework for Probabilistic Reward Specification"** using the PBRS formalism **Lowrey et al., "Probabilistic Reward Specification with Model-Based Reinforcement Learning"**. This effectively encodes information both about the dynamics of the simulator and optimal behaviors in the simulated environment. This enables us to shorten the learning horizon and improve sample efficiency.

\begin{comment}
Another line of work complementary to our comes from **Kumar et al., "Retrieval-Based Reward Design for Reinforcement Learning"**, which relabel rewards from off-task (simulated) data, effectively up-weighting transitions that approximately match the dynamics observed in the target domain. These works focus on the \emph{retrieval} of useful samples from prior datasets with shifted dynamics. In contrast, our approach uses prior data to guide the discovery of novel sequences of states and actions in the target domain. In principle, these techniques could be used in conjunction; we leave this to future work.
\end{comment}
% \twnote{maybe mention we can use these together?} 



\noindent\textbf{Model-Based Reinforcement Learning.} A significant body of work on model-based RL learns a generative dynamics models to accelerate policy optimization **Jiang et al., "Generative Dynamics Models for Model-Based Reinforcement Learning"**. In principle, a model enables the agent to make predictions about states and actions not contained in the training, enabling rapid adaptation to new situations. However, the central challenge for model-based methods is that small inaccuracies in predictive models can quickly compound over time, leading to large \emph{model-bias} and a drop in controller performance. An effective critic can be used to shorten search horizons **Peng et al., "Critic-Based Model-Based Reinforcement Learning"**, yielding easier decision-making problems, but learning such a critic from scratch can still require large amounts of on-task data. We demonstrate that, for many real-world continuous control problems, critics learned entirely in simulation can be robustly transferred to the real-world and substantially accelerate model-based learning.

\noindent\textbf{Reward Design in Reinforcement Learning.} 
A significant component of our methodology is learning dense shaped reward in simulation to guide real-world fine-tuning. Prior techniques have tried to infer rewards from expert demos **Kumar et al., "Learning Dense Rewards from Expert Demonstrations"**, success examples **Lowrey et al., "Reward Design for Reinforcement Learning with Success Examples"**, LLMs **Peng et al., "Learning Reward Functions from Large Language Models"**, and heuristics **Gu et al., "Heuristic-Based Reward Design for Reinforcement Learning"**. We rely on simulation to provide reward supervision **Peng et al., "PBRS: A Framework for Probabilistic Reward Specification"** using the PBRS formalism **Lowrey et al., "Probabilistic Reward Specification with Model-Based Reinforcement Learning"**. This effectively encodes information about the dynamics and optimal behaviors in the simulator, enabling us to shorten the learning horizon and improve sample efficiency.