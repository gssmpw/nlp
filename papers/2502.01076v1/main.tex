
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}


\usepackage{hyperref}
\usepackage{url}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors


\usepackage{algorithm}
\usepackage{algorithmic}
% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tabularx}
\usepackage{graphicx}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem*{remark*}{Remark}
\usepackage{booktabs}
\usepackage{multirow}


\usepackage{enumitem} % 引入枚举条目宏包


\title{qNBO: quasi-Newton Meets Bilevel Optimization}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{}
\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}


\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\vspace{-5em}
\maketitle
\vspace{-6em}
\textbf{Sheng Fang}\textsuperscript{1}\textsuperscript{2}, 
\textbf{Yong-Jin Liu}\textsuperscript{2}\textsuperscript{3}, 
\textbf{Wei Yao}\textsuperscript{4}\textsuperscript{5}, \textbf{Chengming Yu}\textsuperscript{4}\textsuperscript{6}, 
\textbf{Jin Zhang}\textsuperscript{4}\textsuperscript{5}\textsuperscript{7}\footnote{Correspondence to Jin Zhang (zhangj9@sustech.edu.cn) } \\
\textsuperscript{1}{Public Health School, Fujian Medical University},
\textsuperscript{2}{School of Mathematics and Statistics, Fuzhou University},
\textsuperscript{3}{Center for Applied Mathematics of Fujian Province},
\textsuperscript{4}{National Center for Applied Mathematics Shenzhen, SUSTech},
\textsuperscript{5}{Department of Mathematics, SUSTech},
\textsuperscript{6}{School of Science, BUPT},
\textsuperscript{7}{CETC Key Laboratory of Smart City Modeling Simulation and Intelligent Technology, The Smart City Research Institute of CETC} \\ 
    \texttt{shengfang1996@163.com}, \texttt{yjliu@fzu.edu.cn} \\
\texttt{yaow@sustech.edu.cn}, \texttt{2024010618@bupt.cn},
\texttt{zhangj9@sustech.edu.cn}\\

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
%\begin{document}


\maketitle

\begin{abstract}
Bilevel optimization, addressing challenges in hierarchical learning tasks, has gained significant interest in machine learning. The practical implementation of the gradient descent method to bilevel optimization encounters computational hurdles, notably the computation of the exact lower-level solution and the inverse Hessian of the lower-level objective. Although these two aspects are inherently connected, existing methods typically handle them separately by solving the lower-level problem and a linear system for the inverse Hessian-vector product. In this paper, we introduce a general framework to address these computational challenges in a coordinated manner. Specifically, we leverage quasi-Newton algorithms to accelerate the resolution of the lower-level problem while efficiently approximating the inverse Hessian-vector product. Furthermore, by exploiting the superlinear convergence properties of BFGS, we establish the non-asymptotic convergence analysis of the BFGS adaptation within our framework.
Numerical experiments demonstrate the comparable or superior performance of the proposed algorithms in real-world learning tasks, including hyperparameter optimization, data hyper-cleaning, and few-shot meta-learning.
\end{abstract}

\section{Introductions}
\label{intro}

Bilevel optimization (BLO), which addresses  challenges in hierarchical decision process, has gained significant interest in many real-world applications. Typical applications in machine learning include meta-learning \citep{franceschi2018,rajes2019meta}, hyperparameter optimization \citep{pedregosa2016,franceschi2017,okuno2021}, adversarial learning \citep{goodfellow2014,pfau2016}, neural architecture search \citep{nas19,nas20}, and reinforcement learning \citep{rl19,tthong}. In this study, we revisit the following BLO problem:
\begin{equation}\label{blp}
		\min_{x\in {\mathbb R}^m}\ \  \Phi(x):=F(x, y^*(x))\quad
		{\rm s.t.} \ \  y^*(x) = \underset{y\in\mathbb{R}^{n}} {\mathrm{arg\,min}} \ f(x, y),
\end{equation}
in which the upper-level (UL) objective function $F:{\mathbb R}^m\times {\mathbb R}^n\rightarrow {\mathbb R}$ is generally nonconvex, while the lower-level (LL) objective function $f:{\mathbb R}^m\times {\mathbb R}^n\rightarrow {\mathbb R}$ is strongly convex with respect to (\textit{w.r.t.}) the LL variable $y\in {\mathbb R}^n$.

The gradient of $\Phi(x)$, known as hypergradient, is crucial not only for applying gradient descent but also for developing accelerated gradient-based methods for BLO problems. 
Therefore, a fundamental question in solving  BLO problems is how to efficiently estimate the hypergradient. Assuming that $f$ is continuously twice differentiable, and by  applying the chain rule and utilizing the first-order optimality condition $\nabla_y f(x, y^*(x))=0$ of the LL optimization problem \citep{ghadimi2018approximation}, the hypergradient is given  by 
\begin{equation}\label{eq:hg}
	\begin{split}
		\nabla\Phi(x)&=\nabla_x F(x, y^*(x))-[\nabla^2_{x y}f(x, y^*(x))]^T [\nabla^2_{y y} f(x, y^*(x))]^{-1} \nabla_y F(x, y^*(x)).
	\end{split}
\end{equation}

Two main challenges in estimating the hypergradient are: ({\bf C1}) evaluating the LL solution $y^*(x)$; ({\bf C2}) estimating the Jacobian-inverse Hessian-vector product $[\nabla^2_{x y}f(x, y)]^T[\nabla^2_{y y} f(x, y)]^{-1} \nabla_y F(x, y)$, once a good proxy $y$ for the LL solution $y^*(x)$ is obtained. 

For (C1), the common approach is to perform a few additional gradient descent steps for the LL problem on the current estimate $y_k$, using it as a proxy for the LL solution. 
For (C2), two main approaches have been proposed in the literature. The first is to estimate the inverse Hessian using the (truncated) Neumann series \citep{ghadimi2018approximation,ji2021bilevel}. 
The second approach is to compute the inverse Hessian-vector product $[\nabla^2_{y y} f(x, y)]^{-1} \nabla_y F(x, y)$ by solving the linear system $[\nabla^2_{y y} f(x, y)] z = \nabla_y F(x, y)$ for $z$, and then calculating $[\nabla^2_{x y}f(x, y)]^T z$ \citep{pedregosa2016,arbel2022amortized,2022framework}. 
Clearly, the hypergradient approximation error depends on the errors in both (C1) and (C2). Most existing methods handle (C1) and (C2) separately, using different techniques.

A notable exception is the recent breakthrough by \citep{ramzi21shine}, which introduces a novel approach (named SHINE), specifically designed for deep equilibrium models (DEQs) \citep{bai2019deep,bai2020multiscale} and BLO problems where the UL objective function does not explicitly depend on the UL variable, \textit{i.e.}, $F(x,y)=\mathcal{L}(y)$. 
The novelty of SHINE lies in its approach to addressing (C1) and (C2) closely. 
The main idea is to use quasi-Newton (qN) matrices from the LL solution process to efficiently approximate the inverse Hessian in the direction needed for the hypergradient computation. 
SHINE provides three methods for approximating the hypergradient by incorporating a technique OPA with Broyden's method and BFGS. 
Note that in the OPA method from \cite{ramzi21shine}, the qN matrices derived from the LL resolution are influenced by additional updates. This can potentially lead to incorrect inversion, as noted in \cite{ramzi21shine}. To mitigate this issue, they employ a fallback strategy. 
In theory, SHINE demonstrates asymptotic convergence to the hypergradient under various conditions but does not guarantee convergence of the algorithmic iterates.


Therefore, inspired by SHINE, our focus is on improving hypergradient approximation and reducing barriers to solving BLO problems. 
In particular, our main research question is: 
\textit{Can we develop a method to enhance  hypergradient approximation for solving the bilevel optimization problem in (\ref{blp}) with a guaranteed convergence rate?}

\subsection{Main contributions}\label{sec:contri}

Our response to this question is affirmative, and our main contributions are summarized below.

\begin{itemize}
\item qNBO, a new algorithmic framework utilizing quasi-Newton techniques, is proposed for solving the BLO problem (\ref{blp}). 
Unlike SHINE, qNBO includes a subroutine  that applies quasi-Newton recursion schemes specifically tailored for the direction $\nabla_y F(x,y)$, avoiding incorrect inversion. 
%Notably, aside from the computation of $\nabla_{xy}^2 f(x,y)$, all other operations in qNBO involve sequences of inner products and vector summations. As a result, when  $\nabla_{xy}^2 f(x,y)$ is easy to compute, qNBO can significantly reduce the time required to solve BLO problems. 

\item We validate the effectiveness and efficiency of qNBO with two practical algorithms: qNBO (BFGS) and qNBO (SR1), corresponding to two prominent quasi-Newton  methods. 
The numerical results demonstrate qNBO’s comparable or superior performance compared to its closest competitors, SHINE \citep{ramzi21shine} and PZOBO \citep{sow2022convergence}, as well as other BLO algorithms, including two popular fully first-order methods: BOME \citep{bome} and ${\rm F^2 SA}$ \citep{kwon23fully}.

\item By leveraging the superlinear convergence rates of BFGS, we analyze the non-asymptotic convergence of BFGS adaptation within our framework, qNBO.
\end{itemize}



\subsection{Additional related work}
\label{relatedwork1}

\paragraph{Quasi-Newton methods.} 
Because of the low computation cost per iteration and fast convergence rate, quasi-Newton (qN) methods has been extensively studied  \citep{nocedal2006numerical}. 
The most common qN methods are the Broyden-Fletcher-Goldfarb-Shanno (BFGS) \citep{bfgs1,bfgs2,bfgs3,bfgs4,bfgs5}, its low-memory extension L-BFGS \citep{liu1989limited}, and the symmetric rank one method (SR1) \citep{sr1,sr11}. 
For BLO problems, \cite{pedregosa2016} first uses L-BFGS to solve the LL problem to a certain tolerance. Then a conjugate-gradient method is applied to solve the linear system $[\nabla^2_{y y} f(x, y)] z = \nabla_y F(x, y)$ through matrix-vector products. Finally, $[\nabla^2_{x y}f(x, y)]^T z$ is calculated. 

\paragraph{Hypergradient approximation methods.} 
Various bilevel methods have been proposed recently to approximate the inverse Hessian or omit some second-order derivative computations in the hypergradient. For example, FOMAML \citep{maml,fomaml2} skips calculating all second-order derivatives.
DARTS \citep{liu2018darts} solves the LL problem with just one gradient descent step. The Jacobian-Free method (JFB) \citep{fung2021fixed} approximates the inverse Hessian with the identity. 
\cite{giovannelli2021inexact} proposes practical low-rank bilevel methods (BSG1 and BSG-N-FD) that use first-order approximations for second-order derivatives through a finite-difference scheme or rank-1 approximations. 
%However, these methods do not provide general convergence guarantees. 
Recently, several zeroth-order methods have been proposed to approximate the full hypergradient, such as ES-MAML \citep{esmaml} and HOZOG \citep{hozog}. Another zeroth-order method, PZOBO \citep{sow2022convergence}, approximates only part of the hypergradient by comparing two optimization paths.

%Ramzi et al. \cite{ramzi21shine} designed the SHINE method for solving bilevel problems, using the BFGS matrix to approximate the Hessian in the hypergradient. SHINE is a double-loop method, and it lacks non-asymptotic convergence analysis.

There is another line of research for BLO problems, which does not explicitly use the hypergradient in (\ref{eq:hg}), see, \textit{e.g.}, \cite{liu23value,sow2022constrained,shen23penalty,bome,kwon23fully,kwon2024on}. 
These algorithms first use the value function of the lower-level problem to transform the bilevel problem into a single-level problem. Then, they apply the penalty function method or other techniques to solve the reformulated problem.
For instance, BOME \citep{bome} is a novel and fast gradient-based method that uses a modified dynamic barrier gradient descent on the value-function reformulation. ${\rm F^2 SA}$ \citep{kwon23fully} is a fully first-order method developed from a value function-based penalty formulation. It can be implemented in a single-loop manner. Additionally, it is shown in \cite{kwon23fully} that the update direction of ${\rm F^2 SA}$ has a global $\mathcal{O}(1/\lambda)$-approximability of the exact hypergradient, where $\lambda$ is the penalty parameter.

\section{Proposed framework}\label{pf2}

In this section, we introduce a general framework, qNBO, to enhance hypergradient approximation. It addresses the computational challenges (C1) and (C2) using quasi-Newton techniques. We begin by rewriting the hypergradient as:
\begin{equation*}
	\begin{split}
		\nabla\Phi(x)&=\nabla_x F(x, y^*(x))-[\nabla^2_{x y}f(x, y^*(x))]^T u^*(x, y^*(x)),
	\end{split}
\end{equation*}
where $u^*(x,y):=[\nabla^2_{y y} f(x, y)]^{-1} \nabla_y F(x, y)$. 
As in \cite{arbel2022amortized} and \cite{2022framework}, the proposed algorithms introduce an additional variable $u_k$ alongside $x_k$ and $y_k$. Naturally, qNBO consists of three components. The details of qNBO are presented in Algorithm \ref{alg:foa}.

	\begin{algorithm}[htb]
	\caption{ qNBO : quasi-Newton Meets Bilevel Optimization } 
	\label{alg:foa}
\raggedright {\bf Input:} $x_0, y_0$; initial matrix $H_0$; stepsize $\alpha>0$; iterates numbers $K,\{Q_k\}_{k=0}^{K-1}$

{\bf for} $k=0,1,\ldots, K-1$ {\bf do}
	\begin{algorithmic}
			\STATE  1. ${y_{k+1}} \leftarrow {\cal A} (x_k,y_k)$
   {\color{blue}\# update $y_{k+1}$ by a subroutine ${\cal A}$}
   %Compute ${y_{k+1}}$, $\{s_{t},g_t\}_{t=0}^{T-1}$ from ${\cal A}_k (x_k,y_k) $.
	\STATE 2.\ \ if $Q_k=1$:\\
         \ \ \ \ \ \ \ \ \ \ ${u_{k+1}}\leftarrow{\cal C}_{qN}\big(\nabla_y F(x_k,y_{k+1}), H_0, \{s_{t},g_t\}_{t=0}^{T-1}\big)$ 
         {\color{blue} \# share $\{s_{t},g_t\}_{t=0}^{T-1}$ with ${\cal A} (x_k,y_k)$ }
        % or ${\cal C}_s\big(\nabla_y F(x_k,y_{k+1}), H_0, \{s_{t},g_t\}_{t=0}^{T-1}\big)$;
        \\
          \ \ \ \ \ else:\\
         \ \ \ \ \ \ \ \ \ \ ${u_{k+1}}\leftarrow {\cal B} (x_k,y_{k+1},H_0,\nabla_y F(x_k,y_{k+1}),Q_k) $ 
         {\color{blue} \# improve $u_{k+1}$ by a subroutine ${\cal B}$ }
	\STATE 3. $x_{k+1} \leftarrow x_k-\alpha \left(\nabla_x F(x_k, y_{k+1})-[\nabla^2_{x y}f(x_k, y_{k+1})]^Tu_{k+1}\right)$
	\end{algorithmic}
	{\bf end for}
\end{algorithm}


{\bf Part 1:} qNBO updates $y_k$ towards $y^*(x_k)$ using a qN-based subroutine ${\cal A} (x_k,y_k)$ in Algorithm~\ref{alg:yk}, starting from $y_k$. The key is a quasi-Newton recursion scheme ${\cal C}_{qN}$, which computes the matrix-vector product $H d$ by performing a sequence of inner products and vector summations involving $d$ and the pairs $\{s_i , g_i \}_{i=0}^{t-1}$. Here $H$ represents the inverse Hessian approximation of the LL objective, $d=\nabla_y f(x, y_t)$, $s_i=y_{i+1}-y_i$, and $g_i=\nabla_y f(x, y_{i+1})-\nabla_y f(x, y_i)$ in this subroutine. 
Two prominent quasi-Newton recursion schemes are provided in Appendix \ref{sec:atwoloop}.

{\bf Part 2:} To update $u_{k+1}$, we provide two options: $Q_k=1$ or $Q_k>1$. In the case of $Q_k=1$, qNBO updates $u_{k+1}$ similarly to SHINE. The pairs $\{s_i , g_i \}_{i=0}^{T-1}$ from ${\cal A} (x_k,y_k)$ are shared to approximate the inverse Hessian in the direction $\nabla_y F(x_k,y_{k+1})$. 
Unfortunately, incorrect inversion may occur because the pairs $\{s_i , g_i \}_{i=0}^{T-1}$ in ${\cal A} (x_k,y_k)$ are designed to satisfy the secant equation $H_{t+1} g_t = s_t$. 
To address this issue, qNBO adds a subroutine ${\cal B}$ in Algorithm~\ref{alg:uk}, which uses quasi-Newton recursion schemes for the direction $\nabla_y F(x_k,y_{k+1})$ when $Q_k>1$. The price to pay is that the pairs $\{s_i , g_i \}$ in ${\cal A}$ cannot be shared with  ${\cal B}$, increasing the computational cost. Thus, choosing between $Q_k=1$ and  $Q_k>1$ involves a trade-off between performance and computational cost.

{\bf Part 3:} After computing $y_{k+1}$ and $u_{k+1}$, qNBO updates $x_{k+1}$ by  
\begin{equation*}
    x_{k+1}=x_k-\alpha_k \left(\nabla_x F(x_k, y_{k+1})-[\nabla^2_{x y}f(x_k, y_{k+1})]^Tu_{k+1}\right),
\end{equation*}
where $\alpha_k$ is a stepsize, and ${\tilde{\nabla} \Phi(x_k)}:=\nabla_x F(x_k, y_{k+1})-[\nabla^2_{x y}f(x_k, y_{k+1})]^Tu_{k+1}$ is a hypergradient approximation. This update rule for $x_k$ is commonly used in gradient-based algorithms, , such as those in \cite{arbel2022amortized} and \cite{2022framework}. 



\begin{algorithm}[htb]
	\caption{ ${\cal A} (x,y^0)$: gradient descent steps + qN steps for the LL problem} 
	\label{alg:yk}
\raggedright {\bf Input:} $x, y^0$; initial matrix $H_0$; stepsizes $\beta, \gamma>0$; iterates numbers $P, T$\\

	\begin{algorithmic}
			\STATE  1. {\bf for} \ \ $j=0,1,2,\ldots,P-1$\\
	\ \ \ \ \ \ \ \ \ \ $y^{j+1} \leftarrow y^{j}-\beta \nabla_y f(x,y^{j})$\\
	\ \ \ \  {\bf end for}\\
	\STATE  2. $y_{0} \leftarrow y^P$\\
	\ \ \ \ \ {\bf for} \ \ $t=0,\ldots,T-1$\\
	\ \ \ \ \ \ \ \  $y_{t+1} \leftarrow y_{t}-\gamma d_{t}$, \\ \ \ \ \ \ \ \ \  where $d_{t}\leftarrow{\cal C}_{qN}\big(\nabla_y f(x,y_{t}), H_0, \{s_{i},g_i\}_{i=0}^{t-1}\big)$ 
($t\geq 1$),  $d_0 \leftarrow H_0\nabla_y f(x,y_{0})$\\
	\ \ \ \ \ \ \ \ % Compute and save: 
 $s_t \leftarrow y_{t+1}-y_{t}, $ 
 $g_t \leftarrow \nabla_y f(x, y_{t+1})-\nabla_y f(x, y_{t})$\\
	\ \ \ \ \ {\bf end for}\\
 \STATE Return $y_{T}$, $\{s_{t},g_t\}_{t=0}^{T-1}$.
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}[htb]
\raggedright {\bf Input:} $x, y$; initial matrix $H_0$; vector $d$; stepsize $\xi_i>0$; iterates number $Q$\\
	\caption{ ${\cal B} (x,y,H_0,d,Q)$} 
	\label{alg:uk}
%\raggedright {\bf Parameters:} iterate $Q\in {\mathbb N}$.\\

	\begin{algorithmic}
\STATE  1. 
 ${u_{0}} \leftarrow H_0 d$, $\tilde{s}_0 \leftarrow \xi_0 u_{0}$ and $ \tilde{g}_0 \leftarrow \nabla_y f(x, y+\tilde{s}_0)-\nabla_y f(x, y)$ \\
			\STATE  2. \ \  {\bf for} \ \ $i=1,2,\ldots,Q-1$\\
\ \  \ \ \ \ \ \ \ \ ${u_{i}}\leftarrow{\cal C}_{qN}\big(d, H_0, \{\tilde{s}_{j}, \tilde{g}_j\}_{j=0}^{i-1}\big)$ 
%or ${\cal C}_s\big(d, H_0, \{s_{j},g_j\}_{j=0}^{i-1}\big)$.
\\
	\ \ \ \ \ \ \ \  \  
 %{\rm Compute}{\rm\  and\ save\ }
 $\tilde{s}_i \leftarrow \xi_i u_{i}, \tilde{g}_i \leftarrow \nabla_y f(x, y+\tilde{s}_i)-\nabla_y f(x, y)$
 \\
	\ \ \ \ \ \ \ {\bf end for}\\
 \STATE Return $u_{Q-1}$.
\end{algorithmic}
\end{algorithm}


\paragraph{Two practical qNBO algorithms: qNBO~(BFGS) and qNBO~(SR1).}\label{sec:foa}

We describe two prominent quasi-Newton recursion schemes, ${\cal C}_{qN}$, for computing the inverse Hessian approximation-vector product $H_t d$. 
These schemes involve a sequence of inner products and vector summations with $d$ and pairs $\{s_i , g_i \}_{i=0}^{t-1}$. 
One is the BFGS two-loop recursion scheme, outlined in Algorithm \ref{alg:twobfgs}, corresponding to the BFGS updating formula \citep{bfgs1,bfgs2,bfgs3,bfgs4,bfgs5}: 
 \begin{equation}\label{eq:bfgs}
 H_{t+1} = \left(I - \rho_t s_t g_t^T\right) H_t \left(I - \rho_t g_t s_t^T\right) + \rho_t s_t s_t^T, \quad \rho_t = \frac{1}{g_t^T s_t}.
 \end{equation}
 The second algorithm is presented in Algorithm \ref{alg:sr1rec}, which corresponds to the symmetric-rank-one (SR1) updating formula \citep{sr1,sr11}: 
	\begin{equation}\label{eq:sr1}
     H_{t+1}=H_t+\frac{(s_t-H_t g_t)( s_t-H_t g_t)^T}{( s_t-H_t g_t)^T g_t}.
	\end{equation} 



\paragraph{Implementation and improvement.}

Several details and enhancements are needed for an efficient implementation of qNBO.

First, the purpose of including a few gradient descent steps in subroutine \({\cal A}\) is to bring the iterators closer to a neighborhood of the LL solution, enabling superlinear convergence in subsequent quasi-Newton steps. A warm-start for $y^0$ is effective because $y^*(x_{k+1})$ remains close to $y^*(x_{k})$ when $x_{k+1}$ is near $x_{k}$. This is guaranteed by the Lipschitz continuity of $y^*(x)$. In practice, we conjecture that a few initial gradient descent steps are sufficient, although they are necessary for the theoretical analysis.

Second, because \(f(x,y)\) exhibits strong convexity \textit{w.r.t.} \(y\) in our context, the curvature condition \(s_t^T g_t > 0\), required for BFGS, is consistently satisfied. This allows the use of a fixed step size, eliminating the need for time-consuming line searches. 
Furthermore, as the solution approaches a region conducive to superlinear convergence, employing a few quasi-Newton steps is sufficient to achieve a satisfactory solution. 


Third, in experiments, the initial matrix \(H_0\) is chosen as a scalar multiple of the identity matrix. This simplification ensures that the recursion algorithms, Algorithms \ref{alg:twobfgs} and \ref{alg:sr1rec},  involve only vector inner products, significantly reducing computational costs. In Algorithm~\ref{alg:uk}, the parameter $\xi_i$ is typically set to either $1$ or $\|u_{i}\|$ in most cases.

Fourth, qNBO is a flexible framework that can integrate other quasi-Newton methods, such as  limited memory BFGS (L-BFGS) \citep{liu1989limited}. It also supports a ``non-loop" implementation of L-BFGS by representing quasi-Newton matrices in outer-product form \citep{byrd1994repre}. 

Finally, qNBO consists of three parts, with the first two utilizing quasi-Newton recursion schemes. A stochastic adaptation involves replacing these schemes with stochastic methods (e.g., K-BFGS \citep{goldfarb2020practical}, Stochastic Block BFGS\citep{gower2016stochastic}) and using stochastic gradients in Part 3, aligning with \cite{2022framework}. Key challenges in implementing the stochastic adaptation include constructing effective unbiased or biased estimators in Part 3 using techniques like variance reduction and momentum, and analyzing the convergence rate and complexity of the proposed stochastic algorithms in a bilevel setting that leverages noisy second-order information. Addressing these theoretical issues may require breakthroughs beyond the scope of this work.




\section{Theoretical analysis}
In this section, we analyze the non-asymptotic convergence of the qNBO algorithm, as outlined in Algorithm \ref{alg:foa}, under standard assumptions commonly used in BLO literature \citep{ghadimi2018approximation,ji2021bilevel,chen2022single,2022framework,ji2022will}.

\subsection{Assumptions}\label{sec:ass}


\begin{assumption}\label{ass:F}
 Assume that the UL objective function $F$ satisfies the following properties:
\begin{itemize}[leftmargin=*, label={}]
    \item[](i) For all $x$, the gradients $\nabla_x F(x, y)$ and $\nabla_y F(x, y)$ are Lipschitz continuous \textit{w.r.t.} $y$, with Lipschitz constants $L_{F_x}$ and $L_{F_y}$, respectively.
    \item[](ii) For all $y$, $\nabla_y F(x, y)$ is Lipschitz continuous \textit{w.r.t.} $x$, with a Lipschitz constant $\bar{L}_{F_y}$.
    \item[](iii) There exists a constant $C_{F_y}$ such that $\|\nabla_y F(x, y)\| \leq C_{F_y}$ for all $x, y$.
\end{itemize}
\end{assumption}

\begin{assumption}\label{ass:f}
	Assume that the LL objective function $f$ has the following properties:
	\begin{itemize}[leftmargin=*, label={}]
		\item[](i) For all $x$ and $y$,  $f$ is continuously twice differentiable in $(x, y)$.
		\item[](ii) For all $x$,  $f(x, y)$ is strongly convex \textit{w.r.t.} $y$ with parameter $\mu>0$. Moreover,  $\nabla_y f(x, y)$ and $\nabla_{yy}^2 f(x, y)$ are Lipschitz continuous \textit{w.r.t.} $y$  with parameter $L$ and $L_{f_{yy}}$, respectively.
		 \item[](iii) For all $x$, $\nabla^2_{x y}f(x,y)$ is Lipschitz continuous \textit{w.r.t.} $y$ with constant $L_{f_{x y}}$.
		\item[](iv) For all $x, y$, we have $\|\nabla^2_{x y}f(x, y)\|\leq M_{f_{xy}}$ for some constant $M_{f_{xy}}$.
\item[](v) For all $y$, $\nabla^2_{x y}f(x,y)$ and $\nabla_{yy}^2 f(x, y)$ are Lipschitz continuous \textit{w.r.t.} $x$  with constants $\bar{L}_{f_{xy}}$ and $\bar{L}_{f_{yy}}$, respectively.
\end{itemize}
\end{assumption}

{\begin{assumption}\label{ass:phi}
There exists $\iota\in \mathbb{R}$ such that $\inf_x \Phi(x)\geq \iota$.
\end{assumption}}

\subsection{Convergence results}\label{sec:conv}

The evolving nature of inverse Hessian approximations through updating formulas in qNBO significantly complicates the analysis of non-asymptotic convergence. Additionally, various update formulas present different challenges, similar to studying the convergence rates of quasi-Newton methods. In this section, we focus on the non-asymptotic convergence of qNBO (BFGS), leveraging the superlinear convergence rates of BFGS. Some results can also be extended to qNBO (SR1). Comprehensive proofs of these results are provided in Appendix \ref{sec:proof}.


%Since the existing convergence rate results of qN methods depend on the objective functions, the convergence rate results of the qNBO algorithm varies with the form of the LL function. 

%Specifically, Theorem \ref{qfblprate} analyzes the convergence rate when the LL function is quadratic, and Theorem \ref{gblprate} when it is general. 


Let $L_{\Phi}$ be the Lipschitz constant of  $\nabla\Phi$ given in Lemma \ref{prelemma}. We first present the convergence results of qNBO (BFGS) for solving the bilevel problem (\ref{blp}), where the LL objective function is quadratic.

\begin{theorem}[quadratic case] \label{qfblprate}
 Suppose that $f$ in (\ref{blp}) takes the following quadratic form:
 \begin{equation}\label{qf1}
	f(x,y)=\frac{1}{2}y^T A y-y^T  x,
\end{equation}
where $\mu I\preceq A \preceq LI$. Assume that Assumption \ref{ass:F} and \ref{ass:phi} hold. 
Set $Q_k=k+1$ and \textcolor{black}{$H_0=LI$}. Let $\kappa:=L/\mu$, $t_b:=4n{\rm ln}\kappa$, $c_t:=2t_b^{\frac{T}{2}}$, and $\omega:=c_1(1+\frac{1}{\varepsilon})c_t^2 \kappa^3(\frac{1}{T})^{T}$, where $c_1$ is a positive constant given in Theorem \ref{agblprate1}. We can choose positive parameters $\alpha$, $\varepsilon$ and $T$ such that $\tau:=c_t^2 \kappa^3(\frac{1}{T})^{T}\big((1+\varepsilon)+(1+\frac{1}{\varepsilon})\alpha^2c_1\big)<1$ and $\alpha L_{\Phi}+\omega \alpha^2 \left( \frac{1}{2} + \alpha L_{\Phi} \right)\frac{1}{1-\tau}\leq \frac{1}{4}$. 
Then the iterates $x_k$ generated by  qNBO~(BFGS) satisfy:
 \begin{equation}
     \frac{1}{K}\sum_{k=0}^{K-1}\|\nabla{\Phi}(x_k)\|^2\leq  \frac{4(\Phi(x_0)-{\color{black}\inf_x \Phi(x)})}{\alpha K}+\frac{3\delta_0}{K(1-\tau)}+{\color{black}\frac{18nL C_{F_y}^2{\rm ln}K}{\mu^3 K}},
 \end{equation}
 with the initial error $\delta_0=3c_t^2 \kappa^3(\frac{1}{T})^{T}c_2\|y_0^*-y_{0}\|^2$, where $c_2$ is a constant.
\end{theorem}

\begin{remark}
    For the quadratic case, quasi-Newton methods achieve global superlinear convergence \citep{ye2023sr1, Nesterov2021rates, newqn}, allowing  $P = 0$  in Algorithm \ref{alg:yk}.
\end{remark}

\begin{remark}
   The convergence rate of qNBO (SR1) for the quadratic case is similar to that in Theorem \ref{qfblprate}. However, for the general case, the qNBO (SR1) algorithm lacks convergence guarantees without specific corrections used to achieve numerical stability, as noted in \cite{ye2023sr1}.
\end{remark}

Next, we explore the case where the LL objective function in (\ref{blp}) takes a general form. While the convergence rate of qNBO (BFGS) resembles that of the previous quadratic case, it is much more challenging. The specific convergence rate for this general case is detailed in the following theorem.

\begin{theorem}[general case]
\label{gblprate}
Suppose that Assumptions \ref{ass:F}, \ref{ass:f} and \ref{ass:phi}  hold. Set $Q_k=k+1$. 
Choose the parameters $\beta$ and  $P$ such that $(1-\beta\mu)^P\|y_{k}-y_{k}^*\|\leq \frac{1}{300\sqrt{\mu}}$, 
and assume $H_0$ satisfies: $\|\nabla^2_{yy}f(x_k,y^*(x_k))^{-1/2}\big(H_0^{-1}-\nabla^2_{yy}f(x_k,y^*(x_k))\big)\nabla^2_{yy}f(x_k,y^*(x_k))^{-1/2}\|_F\leq\frac{1}{7}$. 
Define $\tau:=\kappa (\frac{1}{T})^{T}(1-\beta\mu)^P\big((1+\varepsilon)+(1+\frac{1}{\varepsilon})\alpha^2c_3\big)$ and $\omega:=c_3(1+\frac{1}{\varepsilon})\kappa (\frac{1}{T})^{T}(1-\beta\mu)^P$, with a constant $c_3$ given in Theorem \ref{thm36g}. 
We can choose positive parameters $\alpha$, $\varepsilon$ and $T$ such that $\tau<1$ and $\alpha L_{\Phi}+\omega \alpha^2 \left( \frac{1}{2} + \alpha L_{\Phi} \right)\frac{1}{1-\tau}\leq \frac{1}{4}$.  
Then the iterates $x_k$ generated by qNBO~(BFGS) satisfy:
 \begin{equation}
     \frac{1}{K}\sum_{k=0}^{K-1}\|\nabla{\Phi}(x_k)\|^2\leq  \frac{4(\Phi(x_0)-{\color{black}\inf_x \Phi(x)})}{\alpha K}+\frac{3\delta_0}{K(1-\tau)}+\frac{18nLM_{f_{xy}}^2 C_{F_y}^2{\rm ln}K}{\mu^3{\tilde{\xi}}  K},
 \end{equation}
 where $\delta_0=3\kappa (\frac{1}{T})^{T}(1-\beta\mu)^Pc_4\|y_0^*-y_{0}\|^2$ is the initial error with constant $c_4$. The constant $\tilde{\xi}$ is related to the property of $f$, as given in (\ref{tildexi}).
\end{theorem}

The proof sketch of Theorem \ref{gblprate} can be found in Appendix \ref{sec:proofske}. The complete version of the parameter specifications and the proofs of Theorems \ref{qfblprate} and \ref{gblprate} are provided in Appendices \ref{sec:proofqfrate} and \ref{sec:proofgrate}, respectively.

\textbf{Discussion on convergence rate and complexity.}
   Choose \(T = \Theta(\ln \kappa)\) and the step size \(\alpha = \Theta(\kappa^{-3})\) such that \(\tau < 1\) and \(\alpha L_{\Phi} + \omega \alpha^2 \left( \frac{1}{2} + \alpha L_{\Phi} \right) \frac{1}{1 - \tau} \leq \frac{1}{4}\). 
   Under the same setting as Theorem \ref{gblprate}, we have \(\frac{1}{K} \sum_{k=0}^{K-1} \| \nabla \Phi(x_k) \|^2 = \mathcal{O} \left( \frac{\kappa^3}{K} + \frac{\kappa^3 \ln K}{K} \right)\). To achieve an \(\epsilon\)-stationary point, it is required that \(K = \tilde{\mathcal{O}}(\kappa^3 \epsilon^{-1})\), resulting in a gradient complexity of \(Gc(f, \epsilon) = \tilde{\mathcal{O}}(\kappa^6 \epsilon^{-2})\) and \(Gc(F, \epsilon) = \tilde{\mathcal{O}}(\kappa^3 \epsilon^{-1})\), as well as a Jacobian-vector product complexity of \(JV(\epsilon) = \tilde{\mathcal{O}}(\kappa^3 \epsilon^{-1})\). 
   
   The details for ensuring \(\tau < 1\) and \(\alpha L_{\Phi} + \omega \alpha^2 \left( \frac{1}{2} + \alpha L_{\Phi} \right) \frac{1}{1 - \tau} \leq \frac{1}{4}\), as well as the specific complexity analysis, can be found in Appendix \ref{sec:complexity}.


\textbf{Theoretical comparisons.}
 Our analysis provides a non-asymptotic convergence rate, superior to that of SHINE \citep{ramzi21shine}. It is established  in \cite{ji2022will} that the fastest deterministic convergence rate, $\frac{1}{K}\sum_{k=0}^{K-1}\Vert\nabla \Phi(x_k)\Vert^2 = {\cal O}(\frac{1}{K})$, is achievable. In comparison, BOME \citep{bome} reaches a convergence rate of ${\cal O}(K^{-1/4}) $, F$^2$SA \citep{kwon23fully} attains ${\cal O}(\frac{\ln K}{K^{2/3}})$, and SABA \citep {2022framework} achieves ${\cal O}(\frac{1}{K})$. To achieve an $\epsilon$-stationary point, the matrix-vector complexity for qNBO is $\tilde{\mathcal{O}}(\kappa^3 \epsilon^{-1})$, primarily due to the $[\nabla_{xy}^2 f(x, y)]^T u$ calculations. {\color{black} It is worth noting that the number of Jacobian-vector products and Hessian-vector products for AID-BIO \citep{ji2021bilevel} are of the orders $\mathcal{O}(\kappa^{3}\epsilon^{-1})$ and $\mathcal{O}(\kappa^{3.5}\epsilon^{-1})$, respectively}. Although the gradient complexity $Gc(f,\epsilon)$ for qNBO is higher than that in AID-BIO \citep{ji2021bilevel}, the computation of gradients is generally less complex than performing matrix-vector operations.

\section{Numerical experiment}\label{sec:exp}

In this section, we conduct numerical experiments to evaluate the performance of the qNBO algorithms in solving bilevel optimization problems. 
We first validate the theoretical convergence through experiments on a toy example, followed by an assessment of efficiency by comparing qNBO with its closest competitor, SHINE \citep{ramzi21shine}, as well as other bilevel optimization (BLO) algorithms such as 
{\color{black} AID-BIO (\cite{ji2021bilevel} with CG method), AID-TN (\cite{ji2021bilevel} with Truncated Neumann method), AMIGO (\cite{arbel2022amortized} with CG method), }
SABA \citep{2022framework} and BSG1 \citep{giovannelli2021inexact}. Additionally, we compare qNBO with two widely used fully first-order algorithms, BOME \citep{bome} and ${\rm F^2 SA}$ \citep{kwon23fully}, in real-world applications including hyperparameter optimization and data hyper-cleaning. Finally, we explore qNBO’s applicability to complex machine learning tasks by exclusively comparing it with PZOBO \citep{sow2022convergence} in a meta-learning experiment, where PZOBO is regarded as the leading algorithm for few-shot meta-learning. 
Details of all experimental specifications are provided in Appendix \ref{aexp}. Additionally, we perform an ablation study in Appendix \ref{abl}.

\begin{figure}[h]
  % 左边的图例及三张图
   \vspace{-1mm}
    \begin{minipage}[t]{1\textwidth}
    \centering
    \caption*{}
    \includegraphics[width=12cm]{legendtoy.jpg}
    \end{minipage}
    \vspace{-3mm}
    \begin{minipage}[t]{0.4\columnwidth}
    \caption*{(a)}
    \vspace{-4mm}
\centering    \includegraphics[width=\textwidth]{dx1.jpg}
    \end{minipage}
    \hspace{0.4in}
    \begin{minipage}[t]{0.4\columnwidth}
    \caption*{(b)}
    \vspace{-3mm}
\centering    \includegraphics[width=\textwidth]{dy1.jpg}
    \end{minipage}

    \vspace{2mm}
    \begin{minipage}[t]{0.4\columnwidth}
    \caption*{(c)}
    \vspace{-4mm}
    \centering    \includegraphics[width=\textwidth]{dsit1.jpg}
    \end{minipage}
    \hspace{.3in}
    \begin{minipage}[t]{0.41\columnwidth}
    \caption*{(d)}
    \vspace{-4mm}
    \centering    \includegraphics[width=\textwidth]{K.jpg}
    \end{minipage}
    \vspace{2mm}
  \caption{\textcolor{black}{Numerical results on toy example. (d) Testing results on the impact of the parameter $\{Q_k\}_{k=0}^{K-1}$ in qNBO~(BFGS).}}
  \label{fig:toy}
\end{figure}

\subsection{Toy example}
In this section, we consider a quadratic bilevel problem where both the UL  and LL objective functions are quadratic. Given $z_0 \in {\mathbb R}^n$ and the symmetric positive definite matrix $A \in {\mathbb R}^{n\times n}$, the problem is formulated as follows:
\begin{equation}\label{toyblp}
%\begin{split}
    \min_{x\in {\mathbb R}^n}\  \frac{1}{2}\|x-z_0\|^2+\frac{1}{2}y^*(x)^T A y^*(x)\ \ 
		{\rm s.t.} \ \ 
  y^*(x)=\underset{y\in\mathbb{R}^{n}}{\mathrm{arg\,min}}\ \frac{1}{2}y^T A  y-x^Ty.
%\end{split}
\end{equation}
In the experiment, the vector $z_0$ and the matrix $A$ are randomly generated, with $n=1000$. The hypergradient is given by $\nabla\Phi(x)=(A^{-1}+I)x-z_0$, which yields the unique solution $(x^*, y^*)=\big((A^{-1}+I)^{-1}z_0, A^{-1}(A^{-1}+I)^{-1}z_0\big)$. To evaluate the performance of various methods in solving this problem, we analyze the hypergradient estimation errors and the distances between the iterates $(x_k, y_k)$ and the optimal solution $(x^*, y^*)$. The results, shown in Figure \ref{fig:toy}(a), indicate that  
{\color{black} AID-BIO, AMIGO-CG and qNBO~(SR1) achieve} smaller hypergradient errors and produce iterates closer to the optimal solutions compared to other {\color{black} methods}. Furthermore, we analyze the impact of parameter $\{Q_k\}_{k=0}^{K-1}$ in Algorithm \ref{alg:foa} on the performance of qNBO~(BFGS). As depicted in Figure \ref{fig:toy}(d) shows that the hypergradient $\nabla\Phi(x_k)$ generally decreases as $Q_k$ increases, with $\{Q_k\}_{k=0}^{K-1}=\{k+1\}_{k=0}^{K-1}$ performing the best, thereby supporting the claims of Theorem \ref{qfblprate}.

\begin{figure}[h]
  % Images side by side
   \vspace{-10mm}
  \begin{minipage}[t]{1\textwidth}
    \centering
    \caption*{}
    \includegraphics[width=10cm]{legend-log-appendix.jpg}
    \end{minipage}\hfill
    \vspace{0mm} % 图例和下面图像之间的空间
  \begin{minipage}[t]{0.4\columnwidth}
    \centering
\includegraphics[width=\textwidth]{20news-appendix.jpg}
    \caption*{}
  \end{minipage}%
  \hspace{.5in}
  \begin{minipage}[t]{0.395\columnwidth}
    \centering    \includegraphics[width=\textwidth]{real-sim-appendix.jpg}
    \caption*{}
  \end{minipage}
\vspace{-5mm}
 \caption{
{\color{black} Hyperparameter optimization experiments for \(l_2\)-regularized logistic regression on two datasets }(Left: \textbf{20News}; Right: \textbf{Real-sim}). %For each method, the median performance over 10 runs is plotted. 
 }
  \label{fig:logreg}
\end{figure}


\subsection{Hyperparameter optimization in logistic regression}
We perform hyperparameter optimization for $l_2$-regularized logistic regression on the 20News \citep{lang1995newsweeder} and Real-sim \citep{libsvm} datasets, formulated as a bilevel problem:
\begin{equation}\label{hr}
		\min_{x\in\mathbb{R}}\  \sum_{i=1}^{n^{\prime}}\ell(a_i^{\prime}, b_i^{\prime},y^{*}(x))\ \ 
		{\rm s.t.} \  \ y^{*}(x)=\underset{y\in\mathbb{R}^{n}}{\mathrm{arg\,min}}
\sum_{i=1}^{n}\ell(a_i, b_i, y)+\frac{{\rm exp}(x)}{2}\|y\|^2,	
\end{equation}
where $(a_i, b_i)\in {\cal D}_{train}$ and $(a_i^{\prime}, b_i^{\prime})\in {\cal D}_{val}$ are the training data and validation data respectively, and $\ell(a_i, b_i, y):={\rm log}\big(1+{\rm exp}\big(-b_{i}{a_{i}}^{T} y\big)\big)$. The LL variable $y$ is the model's parameter, while the UL variable $x$ refers to the regularization hyperparameter.

The performance of different methods on the unseen dataset 
${\cal D}_{test}$ is shown in Figure \ref{fig:logreg}, where the results over 10 runs are plotted for each method. 
{\color{black} Here AMIGO refers to the algorithm AMIGO in \cite{arbel2022amortized}  that employs gradient descent to solve the auxiliary quadratic problem.}
The results show that qNBO (BFGS) reaches its lowest loss faster than the other methods. Notably, the performance of qNBO (SR1) on the 20News dataset was omitted due to its ineffectiveness in this experiment, which resulted in oscillations. This issue arises from the numerical instability of SR1 when solving general functions without a correction strategy \citep{ye2023sr1}. 
%Furthermore, we observe that BSG1 encounters significant oscillations on the 20news dataset, while it performs well on the real-sim dataset.


\begin{figure}[h]
\vspace{-5mm}
\begin{minipage}[t]{1.0\textwidth}
    \centering
    \includegraphics[width=10cm]{legend-data-appendxi.jpg}
\end{minipage}%
\vspace{-5mm}
\vskip 0.3in
% 第一行
 \begin{minipage}[t]{0.4\columnwidth}
    \centering
\includegraphics[width=\textwidth]{mnist-appendix.jpg}
 \end{minipage}%
\hspace{.5in}
 \begin{minipage}[t]{0.4\columnwidth}
    \centering
\includegraphics[width=\textwidth]{fashion-appendix.jpg}
 \end{minipage}%

\caption{{\color{black}Data hyper-cleaning results on two datasets.} (Left: \textbf{MNIST}; Right: \textbf{FashionMNIST}). 
}
\label{fig:dc}
\vskip -0.2in
\end{figure}

\subsection{Data hyper-cleaning}
This subsection focuses on data hyper-cleaning for the MNIST \citep{mnist} and FashionMNIST \citep{fashionmnist} to enhance model accuracy, using a noisy training set  ${\cal D}_{\rm train}:=\{a_i, b_i\}_{i=1}^m$ and a clean validation set ${\cal D}_{\rm val}$. The objective is to adjust the training data weights to improve performance on ${\cal D}_{\rm val}$. This task can be formalized as the bilevel problem:
	\begin{equation}\label{dc}
	    \min_{x} \ \  \ell^{\rm val}(y^{*}(x))\ \ 
	{\rm s.t.} \ y^{*}(x)=\underset{y}{\mathrm{arg\,min}}\{\ell^{\rm train}(x,y)+c\|y\|^2\},
	\end{equation} 	
where $\ell^{\rm val}$ is the validation loss on ${\cal D}_{\rm val}$ and $\ell^{\rm train}=\sum_{i=1}^{m}\sigma(x_i)\ell(a_i, b_i, y)$ is a weighted training loss with $\sigma(x)={\rm Clip}(x, [0,1])$ and $x\in {\mathbb R}^m$. In the experiment, both $\ell(a_i, b_i, y)$ and $\ell^{\rm val}$ are the cross entropy loss, with $c=0.001$. 

\begin{small}
    \begin{table}[h]
  \caption{
  Comparison of results for hyper-cleaning. We compare the time and F1 score of various algorithms in achieving specific test accuracies (91.50\% for MNIST and 83.00\% for FashionMNIST). Bold font indicates the \textbf{fastest time} to reach the target accuracy. If an algorithm fails to reach the required test accuracy, the time is recorded up to the highest accuracy it achieves.
  }
  \label{table:dc}
  \centering
\begin{tabular}{lcccccc}
\hline
 & \multicolumn{3}{c}{MNIST} & \multicolumn{3}{c}{FashionMNIST} \\
Method & Time (s) & Acc. (\%) & F1 score & Time (s) & Acc. (\%) & F1 score \\
\hline
qNBO~(BFGS) &  \textbf{ 0.42} &{91.54} & 95.34 &  \textbf{0.83} &{83.04} & 93.56 \\
qNBO~(SR1)&  {3.68} &{91.51} & 94.59 &  {1.53} &{83.02} & 94.09 \\
BOME & 6.31 &{91.50} & 94.94 & 3.59 & {83.00} & 93.48 \\
SABA & 3.35 &{91.44} & 94.79 & 44.29 &{82.79} & 88.81 \\
F$^2$SA & 8.06 & {91.46} & 93.31 & 8.72 &{82.98} & 86.55 \\
SHINE-OPA & 20.25 & {91.51} & 95.44 & 9.96 &{83.07} & 93.87 \\
PZOBO & 1.05 & {91.46} & 95.46 & 2.96 &{83.05} & 93.77 \\
\hline
\end{tabular}
\end{table}
\end{small}

As shown in Figure \ref{fig:dc}, qNBO~(BFGS) significantly outperforms other methods, achieving lower test loss and higher test accuracy more quickly. All results are averaged over 10 random trials. 
Table \ref{table:dc} illustrates that while qNBO, BOME, and SHINE-OPA are able to achieve the required accuracy, qNBO~(BFGS) does so in the shortest time. Notably, both F$^2$SA and SABA fail to reach the target accuracy on either dataset. For example, on the MNIST dataset, qNBO~(BFGS) requires less than one-tenth of the time taken by BOME, the second-fastest method. The exclusion of BSG1's performance from this experiment is due to its ineffectiveness in addressing these data hyper-cleaning problems.


\subsection{Meta-Learning}
%with $m$ tasks $\{{\cal T}_i, i=1, \cdots, m\}$ sampled over a distribution ${\cal P}_{\cal T}$. Each task ${\cal T}_i$ has a loss function ${\cal L}(x,y_i;\xi)$ with data sample $\xi$, the task-specific parameters $y_i$ and the parameters $x$ of an embedding model shared by all tasks. 

%The model parameters $(x,y)$ are trained via a bilevel procedure. In the lower-level stage, fixing $x$, each base learner of task ${\cal T}_i$ finds  the minimizer  $y_i^*$  of its loss over the training set ${\cal S}_i$. In the upper-level stage, the meta-learner evaluates the validation loss with ($x, y_i^*$) over the validation set ${\cal D}_i$, and optimizes $x$ of the embedding model by minimizing the  corresponding loss over all tasks. The problem 

%Hence, the lower-level function is strongly-convex w.r.t. $y$ and the upper-level function ${\cal L}_{{\cal D}_i}(x, y_{i}^{*}(x))$ is nonconvex w.r.t to $x$. 
In this subsection, we consider the few-shot meta-learning, which can be described as:
\begin{equation*}\label{ml} 
	   \min_{x} \ \ \frac{1}{m}\sum_{i=1}^{m}{\cal L}_{{\cal D}_i}(x, y_{i}^{*}(x))\ \ 
		{\rm s.t.} \ y^{*}(x)=\underset{y}{\mathrm{arg\,min}}\frac{1}{m}\sum_{i=1}^{m}{\cal L}_{{\cal S}_i}(x, y_i).
\end{equation*}


Due to the superior performance demonstrated by PZOBO compared to the baseline methods (MAML \citep{maml} and ANIL \citep{anil}), the comparison of qNBO~(BFGS) is exclusively conducted against PZOBO, excluding the aforementioned baseline methods. 



\begin{figure}[h]
\vspace{-10mm}
  % 两个图的minipage，各占页面宽度的40%
 \begin{minipage}[t]{0.39\columnwidth}
    \centering
    \caption*{}
\includegraphics[width=\textwidth]{meta1.jpg}
\end{minipage}%
\hspace{0.5in}
\begin{minipage}[t]{0.4\columnwidth}
    \centering
    \caption*{}
    \vspace{1.5mm}
\includegraphics[width=\textwidth]{meta2.jpg}
\end{minipage}%
  \caption{5-way 5-shot experiments on two datasets. (Left: \textbf{miniImageNet}; Right: \textbf{Omniglot}.) 
}
  \label{fig:metalearn}
\end{figure}
 
	As shown in Figure \ref{fig:metalearn} and Table \ref{table:m},  qNBO~(BFGS) achieves superior accuracy compared to PZOBO on the miniImageNet \citep{miniI} and Omniglot \citep{omni} datasets. 
 Results are averaged over 5 runs, with all algorithms starting from the same initial point with a test accuracy of 20\%. For clarity, the graphs begin at the second data point, omitting the initial one. It is reported that qNBO~(BFGS) reaches peak test accuracies on the Omniglot dataset within 800 seconds, after which performance declines, likely due to overfitting or other factors. 
 Notably, on the miniImageNet dataset, qNBO~(BFGS) attains a test accuracy exceeding 60\%, while PZOBO fails to achieve comparable results. Furthermore, the results in Table \ref{table:m} show that qNBO~(BFGS) reaches higher test accuracy in significantly less time compared to PZOBO as the number of ways increases. We do not plot the curves for other methods like SABA, SHINE-OPA, and BOME, as they are difficult to converge under various hyperparameter configurations using their source codes.

\begin{table}[h]
\caption{Few-shot meta-learning on the Omniglot dataset: highest test accuracy and time required by each algorithm.}
\label{table:m}
\vskip -15pt
\begin{center}
\begin{small}
%\begin{sc}
\begin{tabular}{lcccr}
\toprule
\multirow{2.5}{*}{5-shot}  & \multicolumn{2}{c}{PZOBO}  & \multicolumn{2}{c}{qNBO~(BFGS)}  \\  \cmidrule(l){2-3} \cmidrule(l){4-5} 
&Acc. (\%) &  Time (s) &Acc. (\%) &  Time (s) \\ \midrule
5-way   & \textbf{99.31} & 11124 & 99.14& \textbf{772} \\ 
20-way &97.94 & 17869 & \textbf{99.14}& \textbf{1648} \\
30-way  & 97.15 & 21043 & \textbf{99.05} & \textbf{2978}\\
\bottomrule
\end{tabular}
%\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}




\section{Conclusion}\label{sec:conlim}

This paper introduces qNBO, a flexible algorithmic framework for improving hypergradient approximation. It leverages quasi-Newton techniques to accelerate the solution of the lower-level problem and efficiently approximates the inverse Hessian-vector product in hypergradient computation. Notably, qNBO includes a subroutine using quasi-Newton recursion schemes specifically tailored for the direction $\nabla_y F(x,y)$ to avoid incorrect inversion. Furthermore, in addition to the prominent BFGS and SR1 methods, qNBO can integrate other quasi-Newton methods such as limited-memory BFGS (L-BFGS) and single-loop implementations. Extensive numerical results verify the efficiency of the proposed algorithms.

%Except for the computation of $\nabla_{xy}^2 f(x,y)$, all other operators in qNBO involve inner products and vector summations. Therefore, when $\nabla_{xy}^2 f(x,y)$ is easy to compute, qNBO can potentially reduce computational costs significantly. 



%We acknowledge that qNBO is not a fully first-order algorithm since it requires the computation of $\nabla_{xy}^2 f(x,y)$. 
%It tackles (C2) by efficiently approximating the inverse Jacobian matrix in the direction $\nabla_y F(x,y)$ and then calculating the product with $\nabla_{xy}^2f(x,y)$. 
%In future work, it would be interesting to develop a version of qNBO, in which we firstly approximate $[\nabla^2_{x y}f(x, y)]^T[\nabla^2_{y y} f(x, y)]^{-1}$ using successive gradient vectors. Inspired by Gauss-Newton methods, BSG1 in the recent work \citep{giovannelli2021inexact} approximates the second-order derivatives by outer products of the corresponding gradients. 
%Note also that qNBO (BFGS) and qNBO (SR1) implemented in this work is not in a single-loop manner. Another interesting question is whether we can develop a single-loop version of qNBO.

\section*{ACKNOWLEDGMENTS}
Authors listed in alphabetical order. This work is supported by National Key R \& D Program of China (2023YFA1011400), National Natural Science Foundation of China (12222106, 12271097, 12326605, 62331014, 12371305), Guangdong Basic and Applied Basic Research Foundation (No. 2022B1515020082), the Key Program of National Science Foundation of Fujian Province of China (Grant No. 2023J02007), the Central Guidance on Local Science and Technology Development Fund of Fujian Province (Grant No.2023L3003), and the Fujian Alliance of Mathematics (Grant No. 2023SXLMMS01).



\bibliography{main}
\bibliographystyle{iclr2025_conference}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix

\section{Outline of appendix}
The appendix is organized as follows:
\begin{itemize}
\item Appendix \ref{sec:atwoloop} presents details of recursive algorithms for computing the inverse quasi-Newton matrix-vector product.
\item Appendix \ref{aexp} provides details of the numerical experiments from Section \ref{sec:exp}.
\item Appendix \ref{sec:proof} includes the proofs of the theorems from Section \ref{sec:conv}. 
\begin{itemize}
\item Appendix \ref{aqn} reviews some useful results of the BFGS method;
\item Appendix \ref{sec:proofske} provides the proof sketch of Theorem \ref{gblprate};
\item Appendix \ref{sec:proofqfrate} presents the proof of Theorem \ref{qfblprate};
\item Appendix \ref{sec:proofgrate} presents the proof of Theorem \ref{gblprate}.
\end{itemize}
\item Appendix \ref{sec:complexity} contains the theoretical discussion and complexity analysis of the proposed algorithms.
\end{itemize}

\section{Recursive procedure to compute the inverse Hessian approximation-vector product}\label{sec:atwoloop}

Due to the low-rank structure of the updates in (\ref{eq:bfgs}) and (\ref{eq:sr1}), for any vector \( d \), \( r = H_{t+1}d \) can be efficiently computed using the recursive methods detailed in Algorithm \ref{alg:twobfgs} for the BFGS update \citep{nocedal1980updating}, and Algorithm \ref{alg:sr1rec} for the SR1 update \citep{erway2017solving}, respectively. Note that Algorithms \ref{alg:twobfgs} and \ref{alg:sr1rec} involve only the computation of first-order information, provided \( H_0 \) is a scalar multiple of the identity matrix. Consequently, by avoiding the storage and computation of the full Hessian, computational costs can be significantly reduced.

\begin{algorithm}[htb]
\caption{\ ${\cal C}_b(d, H_0, \{s_i,g_i\}_{i=0}^{t-1})$: Two-loop recursion for computing $r=H_{t} d$ when $H_{t}$ is the inverse of the BFGS matrix.} \label{alg:twobfgs}
%\raggedright {\bf Input:}  $d, H_0, \{s_i,g_i\}_{i=0}^{t-1}$.

\begin{algorithmic}[1]

\STATE $q=d$;
    	
    \STATE {\bf for} $i=t-1, t-2, \ldots, 0$
    
    \ \ \ \ \ $\alpha_i=(s_i^Tq)/(g_i^Ts_i);$
    
    \ \ \ \ \ $q=q-\alpha_i g_i;$

{\bf end for}

    \STATE $r=H_0 q$;
    
     \STATE {\bf for} $i=0,  \ldots, t-1$
    
    \ \ \ \ \ $\beta= (g_i^Tr)/(g_i^Ts_i);$
    
    \ \ \ \ \ $r=r+(\alpha_i-\beta)s_i$;

 {\bf end for}
\end{algorithmic}
{\bf Return} $r=H_t d$.
\end{algorithm}


\begin{algorithm}[htb]
\caption{\ ${\cal C}_s(d, H_0, \{s_i,g_i\}_{i=0}^{t-1})$: 
 Computing $r=H_{t} d$ when $H_{t}$ is the inverse of an SR1 matrix.} \label{alg:sr1rec}
%\raggedright {\bf Input:}  $d, H_0, \{s_i,g_i\}_{i=0}^{t-1}$.

\begin{algorithmic}[1]
    	
    \STATE {\bf for} $i=0, \ldots, t-1$
    
    \ \ \ \ $p_i=s_i- H_0 g_i;$
        
     \STATE \ \ \ {\bf for} $j=0,  \ldots, i-1$
    
    \ \ \ \ \ \ \ \ $p_i=p_i-((p_j^T g_i)/(p_j^T g_j))p_j;$
    
  \ \ \ {\bf end for}

 \STATE {\bf end for}
\end{algorithmic}
{\bf Return} $r=H_0 d+\sum_{i=0}^{t-1}((p_i^T d)/(p_i^T g_i))p_i$.
\end{algorithm}


%Since the inverse Hessian approximation $H_t$ will generally be dense, the cost of storing and manipulating it is prohibitive when the number of variables is large. To circumvent this problem, we store a modified version of $H_t$ implicitly, by storing a certain number (say, $M$) of the vector pairs $\{s_i , g_i \}$ used in the formulas ()-(). 
%The product $H_k d$ can be obtained by performing a sequence of inner products and vector summations involving $d$ and the pairs $\{s_i , g_i \}$. 




\section{Details on experiments}\label{aexp}
In this section, we additionally compare more algorithms, such as AMIGO (\cite{arbel2022amortized} with CG method),  AID-BIO (\cite{ji2021bilevel} with CG method) and AID-TN (\cite{ji2021bilevel} with Truncated {\color{black}} Neumann method). At the same time, we add meta-learning experiments comparing with the PZOBO algorithm. All experiments are conducted on a server equipped with two NVIDIA A40 GPUs, an Intel(R) Xeon(R) Gold 6326 CPU, and 256 GB of RAM.
\subsection{Details of the toy example}
In this experiment, the initial point for all algorithms is $(x_0, y_0)=(2\textbf{e}, 2\textbf{e})$ where $\textbf{e}$ denotes the vector of all ones. 

\begin{figure}[H]
  % 左边的图例及三张图
   \vspace{-5mm}
    \begin{minipage}[t]{1\textwidth}
    \centering
    \caption*{}
    \includegraphics[width=10cm]{legend-toy-appendix.jpg}
    \end{minipage}
    \vspace{-3mm}

    \begin{minipage}[t]{0.4\columnwidth}
    \caption*{(a)}
    \vspace{-4mm}
    \centering    \includegraphics[width=\textwidth]{dx-appendix.jpg}
    \end{minipage}
    \hspace{0.4in}
    \begin{minipage}[t]{0.4\columnwidth}
    \caption*{(b)}
    \vspace{-3mm}
    \centering    \includegraphics[width=\textwidth]{dy-appendix.jpg}
    \end{minipage}
    
    \vspace{-5mm}
    \begin{minipage}[t]{1\columnwidth}
    \caption*{(c)}
    \vspace{-3mm}
    \centering    \includegraphics[width=0.4\textwidth]{dsit-appendix.jpg}
    \end{minipage}
    \vspace{2mm}
  \caption{{\color{black}Numerical results on toy example: Comparison of qNBO (SR1), qNBO (BFGS) and qNBO (BFGS)-ws with other bilevel optimization methods in a toy experiment. Here, “ws” indicates that a warm-start strategy is applied for $u_k$, with $Q_k = \min(k+1, 60)$.}}
  \label{fig:toy-appendix}
\end{figure}

\begin{figure}[h]
  % 左边的图例及三张图
   \vspace{-5mm}
    \begin{minipage}[t]{1\textwidth}
    \centering
    \caption*{}
    \includegraphics[width=5cm]{legend-q-time.jpg}
    \end{minipage}
    \vspace{-3mm}

    \begin{minipage}[t]{0.4\columnwidth}
    \caption*{(a)}
    \vspace{-4mm}
    \centering    \includegraphics[width=\textwidth]{qdx.jpg}
    \end{minipage}
    \hspace{0.4in}
    \begin{minipage}[t]{0.4\columnwidth}
    \caption*{(b)}
    \vspace{-4mm}
    \centering    \includegraphics[width=\textwidth]{qdy.jpg}
    \end{minipage}
    
    \vspace{-5mm}
    \begin{minipage}[t]{1\columnwidth}
    \caption*{(c)}
    \vspace{-3mm}
    \centering    \includegraphics[width=0.4\textwidth]{qds.jpg}
    \end{minipage}
    \vspace{2mm}
  \caption{\color{black}Testing results on the impact of the parameter ${Q_k}$ in qNBO (BFGS) in the toy example. (The qNBO (BFGS) parameters are the same as those in Figure \ref{fig:toy}, except for $Q_k$. Additionally, $Q_k = ws$ means that $Q_k = \min(k+1, 60)$, where the warm start strategy is used for $u_k$.)}
  \label{fig:toy-appendix}
\end{figure}

\paragraph{BOME:} The maximum number of outer iterations is $K=5000$, the number of inner iterations is $T=100$, the inner step size is $\alpha=0.1$, the outer step size is $\xi=0.1$, and \[
\lambda_k = \max\big\{\frac{0.0001\hat{q}(x_k, y_k) - \langle\nabla F(x_k, y_k), \nabla \hat{q}(x_k, y_k)\rangle}{\|\nabla \hat{q}(x_k, y_k)\|^2}, 0\big\}.\]
\paragraph{F${^2}$SA:} The maximum number of outer iterations is $K=5000$, the number of inner iterations is $T=10$, the inner step sizes are $\gamma_k=\alpha_k=0.1$, the initial multiplier is $\lambda_0=0.1$, the multiplier increment is $\delta_k=0.001$, and the step size ratio is $\xi=1$.
\paragraph{SHINE-OPA:} The maximum number of outer iterations is $K=5000$, the maximum number of inner iterations is $T=100$, the inner stopping criterion is $\|\nabla_y f(x_k, y_{k+1})\| \leq \frac{1}{k+1}$, the inner step size is determined using strong Wolfe line search, the number of extra updates of upper-level information in the BFGS algorithm is 5 (i.e., every 5 steps of BFGS iteration includes an update with UL gradient $\nabla_y F$), the initial matrix is $H_0=I$, and the outer step size is $\alpha_k=0.1$.

{ \color{black}
\paragraph{AID-TN:} The maximum number of outer iterations is $K=5000$, the number of inner iterations is $T=1$, the TN iteration $P=1$, the inner step size is $\beta = 0.01$ and the outer step size is $\alpha = 0.2$.
\paragraph{AID-BIO/AMIGO-CG:} The maximum number of outer iterations is $K=5000$, the number of inner iterations is $T=1$, the CG iteration $P=1$, the inner step size is $\beta = 0.01$ and the outer step size is $\alpha = 0.2$.
\paragraph{PZOBO:}The maximum number of outer iterations is $K=5000$, the number of inner iterations is $Q = N =10$, the parameter $\mu = 100$, the inner step size is $\alpha = 0.01$ and the outer step size is $\beta = 0.01$.
\paragraph{qNBO (SR1):} The maximum number of outer iterations is $K=5000$, the number of inner iterations is $T=6$, the warm up iterations $P=9$, the number of iterations is $Q_k=25$, the inner step sizes are $\beta=0.1, \gamma=1$, the initial matrix is $H_0=I$, and the outer  step size is $\alpha=0.4$.
}
\paragraph{qNBO (BFGS):} The maximum number of outer iterations is $K=5000$, the number of inner iterations is $T=15$, the warm up iterations  $P=1$, the number of iterations $Q_k=k+1$, the inner  step sizes are $\beta=0.1, \gamma=1$, the initial matrix is $H_0=I$, and the outer step size is $\alpha=0.1$.



\subsection{Further specifications for logistic regression}

\subsubsection{Implementations and Hyperparameter settings}
This section introduces the specific parameters of different algorithms used in the logistic regression experiment, formulated as a bilevel problem:
\begin{equation}\label{hr}
		\min_{x\in\mathbb{R}}\ \sum_{i=1}^{n^{\prime}}\ell(a_i^{\prime}, b_i^{\prime},y^{*}(x))\ \ 
		{\rm s.t.} \  \ y^{*}(x)=\underset{y\in\mathbb{R}^{n}}{\mathrm{arg\,min}}
\sum_{i=1}^{n}\ell(a_i, b_i, y)+\frac{{\rm exp}(x)}{2}\|y\|^2,	
\end{equation}
where $(a_i, b_i)\in {\cal D}_{train}$ and $(a_i^{\prime}, b_i^{\prime})\in {\cal D}_{val}$ are the training data and validation data respectively, and $\ell(a_i, b_i, y):={\rm log}\big(1+{\rm exp}\big(-b_{i}{a_{i}}^{T} y\big)\big)$. The LL variable $y$ is the model's parameter, while the UL variable $x$ refers to the regularization hyperparameter.

In this task, we consider two dataset, 20news and Real-sim. The 20news dataset \citep{lang1995newsweeder} comprises a total of 18,846 samples with 130,107 features. It is divided into three subsets: the training set \( {\cal D}_{train} \) has 16961 samples, the validation set \( {\cal D}_{val} \) has 943 samples, and the test set \( {\cal D}_{test} \) has 942 samples. Similarly, the Real-sim dataset \citep{libsvm} contains 72,309 samples, each with 20,958 features. This dataset is also split into three parts: the training set \( {\cal D}_{train} \) has 65078 samples, the validation set \( {\cal D}_{val} \) has 3616 samples, and the test set \( {\cal D}_{test} \) has 3615 samples. For all algorithms, we set \( x_0 \) to 0 and \( y_0 \) to a random value. Unless otherwise stated, the batch size of algorithm is assumed to be 200. {\color{black}In following, AMIGO refers to the algorithm AMIGO (\cite{arbel2022amortized}) that employs stochastic gradient descent to solve the auxiliary quadratic problem.}


% \begin{figure}[]
%   % Images side by side
%   \vspace{-10mm}
%   \begin{minipage}[t]{1.0\columnwidth}
%     \centering
%     \includegraphics[width=12cm]{legend-log-appendix.jpg}
% \end{minipage}%
% \hspace{0.2in}
%   \begin{minipage}[t]{0.4\columnwidth}
%     \centering
% \includegraphics[width=\textwidth]{20news-appendix.jpg}
%     \caption*{}
%   \end{minipage}%
%   \hspace{.5in}
%   \begin{minipage}[t]{0.395\columnwidth}
%     \centering    \includegraphics[width=\textwidth]{real-sim-appendix.jpg}
%     \caption*{}
%   \end{minipage}
% \vspace{-5mm}
%  \caption{Comparison of qNBO(BFGS) and qNBO(SR1) with other bilevel optimization method on logistic regression. 
%   (Left: \textbf{20News}; Right: \textbf{Real-sim}.) It demonstrates that qNBO(BFGS) achieves superior performance.}
%   \label{fig:logreg-appendix}
% \end{figure}

\paragraph{BOME:} The maximum number of outer iterations is $K=200$, the number of inner iterations is $T=10$, the inner step size is $\alpha=0.01$, the outer step sizes are $\xi_x=0.1$ and $\xi_y=0.01$. The update rule for $\lambda_k$ is:
\[
\lambda_k = \max\left\{\frac{0.5\hat{q}(x_k, y_k) - \langle\nabla F(x_k, y_k), \nabla \hat{q}(x_k, y_k)\rangle}{\|\nabla \hat{q}(x_k, y_k)\|^2}, 0\right\}.
\]

\paragraph{F${^2}$SA:} The maximum number of outer iterations is $K=2000$, the number of inner iterations is $T=10$, starting point $z_0=y_0$, inner step sizes $\gamma_k=\alpha_k=0.1$, the initial multiplier $\lambda_0=0.1$, multiplier increment $\delta_k=0.0001$, step size ratio $\xi=1$, and the batch size is 1000.

\paragraph{SABA:} The maximum number of outer iterations is $K=20000$, the initial point $v_0=\textbf{0}$, step sizes $\alpha_k=0.125$ and $\beta_k=\beta^v_k=0.125$, and the batch size is 32.

\paragraph{BSG1:} The maximum number of outer iterations is $K=300$, the number of inner iterations is $T=10$, inner step size $\beta_k=0.01$, outer step size $\alpha_k=0.01$.

\paragraph{SHINE-OPA:} The maximum number of outer iterations is $K=30$, maximum number of inner iterations is $T=1000$. The initial matrix $H_0=I$, for more details  see SHINE code.\footnote{\url{https://github.com/zaccharieramzi/hoag/tree/shine}}

\paragraph{qNBO (BFGS):} The maximum number of outer iterations is $K=50$, the number of inner iterations is $T=9$, warm-up iteration steps $P=1$, iteration steps $Q_k=1$, inner step sizes $\beta=0.0001/(j+1), \gamma=0.1$, outer step size selection strategy is the same as the SHINE-OPA algorithm. When the dataset is the 20news dataset, the initial matrix $H_0=0.1I$; when the dataset is the Real-sim dataset, $H_0=0.01I$.

\paragraph{qNBO (SR1):} The maximum number of outer iterations is $K=100$, the number of inner iterations is $T=7$, warm-up iteration steps $P=3$, iteration steps $Q_k=3$, inner  step sizes $\beta=0.0001/(j+1), \gamma=0.1$, initial matrix $H_0=0.1I$ and outer step size selection strategy is the same as the SHINE-OPA algorithm. 

{\color{black}\paragraph{AID-BIO/AMIGO-CG:} The maximum number of outer iterations is $K=1000$, the number of inner iterations is $T=1$, the CG iteration $P=1$, inner  step sizes $\beta=0.01 $ and outer step size $\alpha = 0.01$.

\paragraph{AMIGO:}  The maximum number of outer iterations is $K=1000$, the batch size is 32, the number of inner iterations is $T=1$, inner step sizes $\beta=0.01$ and outer step size $\alpha = 0.1$.

\paragraph{AID-TN:} The maximum number of outer iterations is $K=1000$, the number of inner iterations is $T=1$, the TN iteration $P=1$, inner  step sizes $\beta=0.01$ and outer step size $\alpha = 0.1$.

\paragraph{PZOBO:} The maximum number of outer iterations is $K=5000$, the number of inner iterations is $Q = N =10$, the parameter $\mu = 10$, the inner step size is $\alpha = 0.01$ and the outer step size is $\beta = 0.03$.
}

\subsection{Further specifications on Data Hyper-cleaning Experiments}
This subsection focuses on data hyper-cleaning to enhance model accuracy, using a noisy training set  ${\cal D}_{\rm train}:=\{a_i, b_i\}_{i=1}^m$ and a clean validation set ${\cal D}_{\rm val}$. The goal is to adjust training data weights to enhance performance on ${\cal D}_{\rm val}$. The task can be formalized as the bilevel problem:
	\begin{equation}\label{dc}
	    \min_{x} \ \  \ell^{\rm val}(y^{*}(x))\ \ 
	{\rm s.t.} \ y^{*}(x)=\underset{y}{\mathrm{arg\,min}}\{\ell^{\rm train}(x,y)+c\|y\|^2\},
	\end{equation} 	
where $\ell^{\rm val}$ is the validation loss on ${\cal D}_{\rm val}$ and $\ell^{\rm train}=\sum_{i=1}^{m}\sigma(x_i)\ell(a_i, b_i, y)$ is a weighted training loss with $\sigma(x)={\rm Clip}(x, [0,1])$ and $x\in {\mathbb R}^m$. In the experiment, both $\ell(a_i, b_i, y)$ and $\ell^{\rm val}$ are the cross entropy loss, with $c=0.001$. 


Experiments are conducted using MNIST \citep{mnist} and FashionMNIST \citep{fashionmnist} datasets, with 50\% of the training data corrupted by randomly assigning them sampled labels. The data is divided into four parts: training set, validation sets 1 and 2, and the test set. The training set comprises 50000 samples, while the validation and test sets contain 5000 and 10000 samples, respectively. For each method, model training is conducted on the training set, with  the tuning of hyperparameter $x$ using validation set 1.
The LL variable $y=(W,b)$ denotes the parameters of the linear model with weight $W\in\mathbb{R}^{10\times 784}$ and bias $b\in\mathbb{R}^{10}$. 



\begin{figure*}[h]

\begin{minipage}[t]{1.0\textwidth}
    \centering
    \includegraphics[width=10cm]{legend-data-appendxi.jpg}
\end{minipage}%
\vspace{-6mm}
\vskip 0.2in
% 第一行
\begin{minipage}[t]{0.4\textwidth}
    \centering
    \caption*{(a) Test accuracy vs Time}
    \includegraphics[width=\textwidth]{mnist-appendix.jpg}
\end{minipage}%
\hspace{.5in}
\begin{minipage}[t]{0.4\textwidth}
    \centering
    \caption*{(b) Test loss vs Time}
    \includegraphics[width=\textwidth]{mnist-loss-appendix.jpg}
\end{minipage}%

\vskip 0.2in  % 在行之间添加空间

% 第二行
\begin{minipage}[t]{0.4\textwidth}
    \centering
    \includegraphics[width=\textwidth]{fashion-appendix.jpg}
\end{minipage}%
\hspace{.5in}
\begin{minipage}[t]{0.4\textwidth}
    \centering
    \includegraphics[width=\textwidth]{fashion-loss-appendix.jpg}
\end{minipage}%

\caption{{\color{black}Data hyper-cleaning on two datasets.} (First row: \textbf{MNIST}; Second row: \textbf{FashionMNIST}. All results are averaged over 10 random trials. The exclusion of BSG1's performance in this experiment is due to its ineffectiveness in addressing these data hyper-cleaning problems.)}
\label{fig:dc-appedix}
\vskip -0.2in
\end{figure*}

\subsubsection{Implementations and Hyperparameter settings}
The initial point $y_0$ for all algorithms is obtained from a pretrained initialization model, and the initial weight vector $x_0 = 0.5\mathbf{e} \in \mathbb{R}^{50000}$. 
\paragraph{BOME:} The maximum number of outer iterations $K=10000$, the number of inner iterations $T=1$, the inner  step size $\alpha=0.01$, the outer  step sizes $\xi_x=100, \xi_y=0.01$, and \[\lambda_k = \max\left(\frac{0.1\hat{q}(x_k, y_k) - \langle\nabla F(x_k, y_k), \nabla \hat{q}(x_k, y_k)\rangle}{\|\nabla \hat{q}(x_k, y_k)\|^2}, 0\right).\] Details can be seen in the code.\footnote{\url{https://github.com/Cranial-XIX/BOME}}
\paragraph{F${^2}$SA:} The maximum number of outer iterations $K=7000$, the number of inner iterations $T=1$, the initial point $z_0=y_0$, the inner  step size $\gamma_k=\alpha_k=0.01$, the initial multiplier $\lambda_0=0.1$, the difference in multiplier $\delta_k=0.001$, the step size ratio $\xi=10000$, and the batch size is 2000.
\paragraph{SABA:} The maximum number of outer iterations $K=10000$, the initial point $v_0=\mathbf{0}$, the batch size is 2000. For the MNIST dataset, the  step sizes $\alpha_k=10, \beta_k=0.01, \beta^v_k=0.1$; otherwise, the  step sizes $\alpha_k=100, \beta_k=\beta^v_k=0.001$.
\paragraph{SHINE-OPA:} The maximum number of outer iterations $K=50$, the maximum number of inner iterations $T=1000$, the inner stopping criterion $\|\nabla_y f(x_k, y_{k+1})\|\leq 1/(100k)$, the inner  step size is determined using strong Wolfe line search, the number of extra updates in the BFGS algorithm is 5 (i.e., upper-level information is introduced in the BFGS iterations for every 5 steps), the initial matrix $H_0=I$, and the outer step size is 100.
\paragraph{qNBO (BFGS):} The number of iterations $Q_k=1$, the inner  step sizes $\beta=0.1, \gamma=0.1$, the outer  step size $\alpha=100$, the initial matrix $H_0=I$. For the MNIST dataset, the maximum number of outer iterations $K=5000$, the number of inner iterations $T=7$, the warm-up iteration count $P=3$; otherwise, the maximum number of outer iterations $K=600$, the number of inner iterations $T=47$, and the warm-up iteration count $P=3$.
\paragraph{qNBO (SR1):} The number of inner iterations $T=17$, the warm-up iteration count $P=3$, the number of iterations $Q_k=3$, the inner iteration step sizes $\beta=0.1, \gamma=0.1$, the outer iteration step size $\alpha=100$, the initial matrix $H_0=0.01I$. In addition, the inner iteration will terminate early if $\|\nabla_y f(x_k, y_{k+1})\|\leq 0.1$. For the MNIST dataset, the maximum number of outer iterations $K=5000$; otherwise, the maximum number of outer iterations $K=2000$.

{\color{black}\paragraph{AID-BIO/AMIGO-CG:} The maximum number of outer iterations is $K=1000$, the number of inner iterations is $T=1$, the CG iteration steps $P=1$, inner  step sizes $\beta=0.01 $ and outer step size $\alpha = 10$.


\paragraph{AMIGO:} The maximum number of outer iterations is $K=1000$, the batch size is 200, the number of inner iterations is $T=1$, inner  step sizes $\beta=0.01$ and outer step size $\alpha = 100$.

\paragraph{AID-TN:} The maximum number of outer iterations is $K=1000$, the number of inner iterations is $T=1$, the TN iteration steps $P=1$, inner  step sizes $\beta=0.01$ and outer step size $\alpha = 30$.

\paragraph{PZOBO:} The maximum number of outer iterations is $K=5000$, the number of inner iterations is $Q = N =10$, the parameter $\mu = 0.01$, the inner step size is $\alpha = 0.01$ and the outer step size is $\beta = 10$.
}

\subsection{Meta-Learning}
%with $m$ tasks $\{{\cal T}_i, i=1, \cdots, m\}$ sampled over a distribution ${\cal P}_{\cal T}$. Each task ${\cal T}_i$ has a loss function ${\cal L}(x,y_i;\xi)$ with data sample $\xi$, the task-specific parameters $y_i$ and the parameters $x$ of an embedding model shared by all tasks. 

%The model parameters $(x,y)$ are trained via a bilevel procedure. In the lower-level stage, fixing $x$, each base learner of task ${\cal T}_i$ finds  the minimizer  $y_i^*$  of its loss over the training set ${\cal S}_i$. In the upper-level stage, the meta-learner evaluates the validation loss with ($x, y_i^*$) over the validation set ${\cal D}_i$, and optimizes $x$ of the embedding model by minimizing the  corresponding loss over all tasks. The problem 

%Hence, the lower-level function is strongly-convex w.r.t. $y$ and the upper-level function ${\cal L}_{{\cal D}_i}(x, y_{i}^{*}(x))$ is nonconvex w.r.t to $x$. 
In this subsection, we consider the few-shot meta-learning, which can be described as:
	\begin{equation*}\label{ml} 
	        \min_{x} \ \ \frac{1}{m}\sum_{i=1}^{m}{\cal L}_{{\cal D}_i}(x, y_{i}^{*}(x))\ \ 
		{\rm s.t.} \ y^{*}(x)=\underset{y}{\mathrm{arg\,min}}\frac{1}{m}\sum_{i=1}^{m}{\cal L}_{{\cal S}_i}(x, y_i),
	\end{equation*}
	where ${\cal L}_{{\cal D}_i}(x, y_{i}^{*})=\frac{1}{|{\cal D}_i|}\sum_{\xi\in{\cal D}_i }{\cal L}(x,y_{i}^{*};\xi)$ is the validation loss  function and ${\cal L}_{{\cal S}_i}(x, y_i)=\frac{1}{|{\cal S}_i|}\sum_{\xi\in{\cal S}_i }({\cal L}(x,y_i;\xi)+{\cal R}(y_i))$ is the training loss with the classification loss ${\cal L}$  and the strongly-convex regularizer ${\cal R}(y_i)$. In the experiment, ${\cal L}$ is the cross-entropy function and ${\cal R}$ is the $\ell_2$ norm.  In our experimental setting, the task-specific parameters $y$ denote the weights of the last linear layer of a neural work and $x$ are the parameters of a 4-layer convolutional neural networks (CNN4). 

The few-shot meta-learning has $m$ tasks $\{{\cal T}_i, i=1, \cdots, m\}$ sampled over a distribution ${\cal P}_{\cal T}$. Each task ${\cal T}_i$ has a loss function ${\cal L}(x,y_i;\xi)$ with data sample $\xi$, the task-specific parameters $y_i$ and the parameters $x$ of an embedding model shared by all tasks. 

\begin{figure}[h]
\vspace{-10mm}
\begin{minipage}[t]{1\columnwidth}
    \centering
    \caption*{}
    \vspace{1.5mm}
\includegraphics[width=0.4\textwidth]{meta3.jpg}
\end{minipage}%
  \caption{5way-5shot on FC100 datasets.  Results are averaged over 5 runs, with all algorithms starting from the same initial point with a test accuracy of 20\%. For clarity, graphs begin at the second data point, omitting the initial one. We report that qNBO (BFGS) reaches peak test accuracies within 800 seconds, after which performance declines, likely due to overfitting or other factors.}
  \label{fig:metalearn-appendix}
\end{figure}

\subsubsection{Datasets}
	\noindent {\bf miniImageNet:}
The	miniImageNet dataset \citep{miniI}, derived from ImageNet \citep{imag}, is a large-scale benchmark for few-shot learning. The dataset comprises 100 classes, each encompassing 600 images of size 84 × 84. Following \cite{arnold}, we partition the classes into 64 classes for meta-training, 16 classes for meta-validation, and 20 classes for meta-testing. In the experiment, CNN4 has four convolutional blocks, in which each convolutional block contains a 3${\times}$3 convolution (padding=1), ReLU activation, 2${\times}$2 max pooling and batch normalization. Each convolutional layer has 32 filters. 

\noindent {\bf FC100:}
The FC100  dataset \citep{fc100}, generated from \cite{cifar}, consists of 100 classes with each class containing 600 images of size 32. Following \cite{fc100}, the classes are split into 60 classes for meta-training, 20 classes for meta-validation, and 20 classes for meta-testing. Each convolutional block of CNN4 comprises a 3${\times}$3 convolutional layer (with padding set to 1 and a stride of 2), subsequent batch normalization, ReLU activation, and 2${\times}$2 max pooling. Each convolutional layer  has 64  filters.

\noindent {\bf Omniglot:}
 Comprising 1623 character classes derived from 50 diverse alphabets, the Omniglot dataset \citep{omni} contains 20 samples within each class. The classes are divided into three parts: 1100 classes for meta-training, 100 classes for meta-validation, and 423 classes for meta-testing. The CNN4 network is identical to that used on the miniImageNet dataset but has 64  filters per layer.

\subsubsection{Experimental setup}
The meta learning experiment is carried out using the code available at the website.\footnote{\url{https://github.com/sowmaster/esjacobians}\label{fn:pzo}}
At each meta-iteration, a batch of 16 training tasks is sampled and the parameters are updated based on these tasks. The max outer steps $K$ is set to 6000 for both algorithms. The initial parameter $y_0$ is selected as 0.

\noindent {\bf PZOBO}: The parameters involved in the algorithm are the same as those in \cite{sow2022convergence}. 

\noindent {\bf qNBO (BFGS)}:  For all three datasets, the inner steps T  is set to 20, the hypergradient updates $Q_k$ is 3,  the step sizes $\beta$ and $\gamma$ are both specified as 0.1, and the  Hessian matrix  is initialized as $H_0=0.01I$. In addition, the inner iteration will terminate early if $\|A\|+\|b\|\leq {\rm tol}$, where $A$ and $b$ are the components of  the parameter $y$ denoting the weights of the final linear layer in a neural work. The specific values of tol and other parameters can be found in the code provided in the supplementary materials.

\begin{figure}[h]
\vspace{0mm}
 \begin{minipage}[t]{0.4\textwidth}
    \centering
    \caption*{}
    \vskip -0.1in
    \includegraphics[width=\textwidth]{0514dcfoadt.pdf}
\end{minipage}%
\hspace{0.5in}
 \begin{minipage}[t]{0.4\textwidth}
    \centering
    \caption*{{\textbf{}}}
    \vskip -0.1in
    \includegraphics[width=\textwidth]{0514dclfoadt.pdf}
\end{minipage}%

\begin{minipage}[t]{0.4\textwidth}
    \centering
    \caption*{{\textbf{}}}
    \vskip -0.1in
    \includegraphics[width=\textwidth]{0514dcfoadtf.pdf}
\end{minipage}%
\hspace{0.5in}
\begin{minipage}[t]{0.4\textwidth}
    \centering
    \caption*{}
    \vskip -0.1in
    \includegraphics[width=\textwidth]{0514dclfoadtf.pdf}
\end{minipage}%

 \caption{Ablation study on the iteration number $T$ of qNBO (BFGS). (The first row illustrates the test accuracy and test loss for MNIST, and the second row shows these values for FashionMNIST.) }
  \label{fig:ablat}
\end{figure}

\subsection{Ablation study}\label{abl}

\subsubsection{Toy example}\label{abltoy}
In this subsection, we conduct an ablation study to assess the impact of the parameters \(Q_k\) on the performance of the qNBO (BFGS) algorithm in the toy experiment. As illustrated in Figure \ref{fig:toy-appendix}, the setting \(Q_k = k+1\) outperforms the others. Furthermore, employing the warm start strategy for \(u_k\) (denoted as \(Q_k = \text{ws}\)) enhances the performance of qNBO (BFGS), suggesting the potential benefits of this strategy.

\subsubsection{Data hyper-cleaning}\label{abldc}
In this subsection, we conduct an ablation study to assess the impact of the parameters \( T \) and \( Q_k \) within the qNBO (BFGS) algorithm on its performance in the \textcolor{black}{data hyper-cleaning experiment}. As illustrated in Figures \ref{fig:ablat} and \ref{fig:ablaq}, smaller values of $T$ and $Q_k$ lead to improved performance in terms of both accuracy and loss across the two datasets, thereby indicating the efficiency of qNBO (BFGS).

\begin{figure}[h]
\vspace{0mm}
 \begin{minipage}[t]{0.4\textwidth}
    \centering
    \caption*{}
    \vskip -0.1in
    \includegraphics[width=\textwidth]{0514dcfoadq.pdf}
\end{minipage}%
\hspace{0.5in}
 \begin{minipage}[t]{0.4\textwidth}
    \centering
    \caption*{{\textbf{}}}
    \vskip -0.1in
    \includegraphics[width=\textwidth]{0514dclfoadq.pdf}
\end{minipage}%

\begin{minipage}[t]{0.4\textwidth}
    \centering
    \caption*{{\textbf{}}}
    \vskip -0.1in
    \includegraphics[width=\textwidth]{0514dcfoadqf.pdf}
\end{minipage}%
\hspace{0.5in}
\begin{minipage}[t]{0.4\textwidth}
    \centering
    \caption*{ \textbf{}}
    \vskip -0.1in
    \includegraphics[width=\textwidth]{0514dclfoadqf.pdf}
\end{minipage}%

 \caption{Ablation study on the iteration number $Q$ of qNBO (BFGS). (The first row shows the test accuracy and test loss for the MNIST dataset, and the second row shows the same metrics for the FashionMNIST dataset.) }
  \label{fig:ablaq}
\end{figure}

\section{Proof of the results in Section \ref{sec:conv}}\label{sec:proof}

\subsection{Results of quasi-Newton method}\label{aqn}
In this subsection, we first summarize the convergence properties of BFGS method for solving the  problem:
\begin{equation}\label{pro}
 \min_{y\in\mathbb{R}^{n}} g(y).
 \end{equation}
 
 Besides, to derive the upper bound of the hypergradient estimation error, we  review some conclusions  of BFGS   update presented in \cite{jin2023non,Nesterov2021rates,newqn}. 
 

\subsubsection{Convergence results of BFGS method}
\begin{assumption}\label{ass:g}
	Assume that $g$ has the following properties:
	\begin{itemize}
		\item[](i)\ $g(y)$ is strongly convex \textit{w.r.t.} $y$ with parameter $\mu>0$, i.e., $\mu I\preceq\nabla^2 g(y)$. Moreover,  $\nabla g(y)$ is Lipschitz continuous \textit{w.r.t.} y  with parameter $L>0$ (i.e., $\nabla^2 g(y)\preceq LI$ ).
		\item[](ii)\ The Hessian $\nabla^2 g(y)$  satisfies:
  \[
\nabla^2 g(y_1)-\nabla^2 g(y_2)\preceq M\|y_1-y_2\|_z \nabla^2 g(w),\forall y_1,y_2, z, w\in\mathbb{R}^{n},
\]	
where $\|y\|_z:=\langle \nabla^2 g(z)y, y\rangle^{1/2}$ and $M>0$.
\end{itemize}
\end{assumption}

\begin{lemma}\label{gc}({\rm \cite{newqn}, Global convergence})
If the function $g$ has the following quadratic form: 
\begin{equation}
	g(y)=\frac{1}{2}y^T A y-y^T  x,
\end{equation}
where $\mu I \preceq A \preceq LI$ such that Assumption \ref{ass:g} holds. If the BFGS method is used to solve the problem (\ref{pro}), and if  \textcolor{black}{$H_0=LI$} and the number of iterations $i \geq 4n \ln \frac{L}{\mu}$, then for all $i \geq 4n \ln \frac{L}{\mu}$, the following inequality holds:
\begin{equation}
    \|y_i-y^*\|\leq 2\kappa^{3/2}(\frac{t_b}{i})^{\frac{i}{2}}\|y_0-y^*\|,
\end{equation}
with $\kappa=\frac{L}{\mu}$ and $t_b=4n{\rm ln}\frac{L}{\mu}$.
\end{lemma}

\textcolor{black}{
\begin{lemma}\label{lc}({\rm \cite{newqn}, local convergence})
Suppose that Assumption \ref{ass:g} on \( g \) holds. If $H_0=LI$ and the initial point \( y_0 \) satisfies:
\begin{equation}\label{eqlca}
\|y_0-y^*\|\leq K_1, K_1=\frac{2{\rm ln}\frac{3}{2}\sqrt{L}}{\frac{3}{2}^\frac{3}{2}M\mu}{\rm max}\{\frac{\mu}{2L},\frac{1}{K_0+9}\},
 \end{equation}
where $K_0=8n {\rm ln}{\frac{2L}{\mu}}$ and $i \geq K_0$, then for all $i \geq K_0$,  the iterate generated by the  BFGS algorithm has the following superlinear convergence rate:,
\begin{equation}
    \|y_i-y^*\|\leq 2\kappa^{3/2}(\frac{t_c}{i})^{\frac{i}{2}}\|y_0-y^*\|,
\end{equation}
with $t_c=\frac{9}{8}K_0$ and $\kappa=\frac{L}{\mu}$.
\end{lemma}}

\begin{lemma}
 {\rm(Theorem 3 of \cite{jin2023non}) } 
Suppose that Assumption \ref{ass:g} on \( g \) holds. If the initial point \( y_0 \) and the initial Hessian approximation matrix \( B_0 \) (with \( H_0 = B_0^{-1} \)) satisfy:
	\begin{equation}\label{yass}
		\begin{split}
 &\|\nabla^2g(y^*)^{1/2}(y_0-y^*)\|\leq \frac{\epsilon}{6},\\
 &\|\nabla ^2g(y^*)^{-1/2}\big(B_0-\nabla ^2g(y^*)\big)\nabla^2g(y^*)^{-1/2}\|_F\leq\delta,
		\end{split}
	\end{equation}
where $\epsilon, \delta\in (0,\frac{1}{2}), \rho\in(0,1),$
\begin{equation*}
\frac{(3+\epsilon)\epsilon}{(1-\epsilon)(1-\rho)}\leq \delta, {\rm and}\ \frac{\epsilon}{3}+2\delta\leq(1-2\delta)\rho,
\end{equation*}
 then the iterate generated by the  BFGS algorithm has the following superlinear convergence rate:
\begin{equation}
    \|y_i-y^*\|\leq \sqrt{\frac{L}{\mu}}\big(\frac{C_1 q\sqrt{i}+C_2}{i}\big)^i\|y_0-y^*\|,
    \end{equation}
where $C_1=2\sqrt{2}\delta (1+\rho)(1+\frac{\epsilon}{3})$, $C_2=\frac{(1+\rho)(1+\frac{\epsilon}{3})\epsilon}{3(1-\rho)}$ and $q=\sqrt{\frac{1+2\delta}{1-2\delta}}$.
	\end{lemma}
To be specific, the superlinear convergence rate of  BFGS algorithm can also be expressed in the following alternative form.
 \begin{lemma}\label{lemuserate}{\rm(Corollary 4 of \cite{jin2023non}) } 
   Suppose that Assumption \ref{ass:g} on $g$ holds. If the initial point $y_0$ and the initial BFGS matrix $B_0$ satisfy:
	\begin{equation}\label{yass}
		\begin{split}
 &\|\nabla^2 g(y^*)^{1/2}(y_0-y^*)\|\leq \frac{1}{300},\\
 &\|\nabla^2g(y^*)^{-1/2}\big(B_0-\nabla^2 g(y^*)\big)\nabla ^2g(y^*)^{-1/2}\|_F\leq\frac{1}{7},
		\end{split}
	\end{equation}
then the iterate solved by the BFGS algorithm  exhibits the following convergence rate:
\begin{equation}
    \|y_i-y^*\|\leq \sqrt{\frac{L}{\mu}}\big(\frac{1}{i}\big)^\frac{i}{2}\|y_0-y^*\|.
    \end{equation}
 \end{lemma}

\subsubsection{Properties of the BFGS updates}

\begin{lemma}\label{lemqfu}
If the function $g$ in the problem (\ref{pro}) has the following quadratic form: 
\begin{equation}\label{qf}
	g(y)=\frac{1}{2}y^T A y-y^T  x,
\end{equation}	
with $\mu I \preceq A \preceq LI$, then the BFGS matrix $B_i$ satisfies:
\begin{equation}
	\sum_{i=0}^{k-1}	\frac{(B_i s_i-As_i)^T A^{-1} (B_i s_i-As_i)}{s_i^T B_i s_i}\leq \frac{nL}{\mu}.
\end{equation}
\end{lemma}
\begin{proof}
Define $\sigma_i:=\sigma(A, B_i) = \langle A^{-1}, B_i - A \rangle {=}{\rm Tr}(A^{-1}(B_i- A)) \geq 0$, then
		 \begin{equation}
	 \begin{split}
	 \sigma(A, B_i) - \sigma(A, B_{i+1}) &=\langle A^{-1}, B_i -B_{i+1}\rangle\\
  &=\frac{\langle B_iA^{-1}B_is_i, s_i \rangle}{\langle B_is_i, s_i \rangle}-1\\
   &=\frac{\left\langle B_i \left(A^{-1} - B_i^{-1}\right)B_i s_i, s_i \right\rangle}{\left\langle B_i s_i, s_i \right\rangle}\\
	 &\geq \frac{\left\langle (B_i - A)A^{-1}(B_i - A)s_i, s_i \right\rangle}{\left\langle B_i s_i, s_i \right\rangle},
	 \end{split}
	 \end{equation}
	where the last inequality follows from the fact that
	 \begin{align*}
(B_i - A)A^{-1}(B_i - A) &= B_iA^{-1}B_i - 2B_i + A \nonumber \\
\overset{A\preceq B_{i}}{\preceq} \ B_iA^{-1}B_i - B_i &= B_i(A^{-1} - B_i^{-1})B_i. \nonumber
\end{align*}

Thus, it is derived that
\begin{equation*}
	\sigma_i-\sigma_{i+1}\geq \frac{\left\langle (B_i - A)A^{-1}(B_i - A)s_i, s_i \right\rangle}{\left\langle B_i s_i, s_i \right\rangle},\quad \forall 0\leq i\leq {k-1}.
\end{equation*}

Finally, summing the above inequality over $i$ yields:
\begin{align*}
\sum_{i=0}^{k-1} \frac{\left\langle (B_i - A)A^{-1}(B_i - A)s_i, s_i \right\rangle}{\left\langle B_i s_i, s_i \right\rangle} &\leq \sigma_0 - \sigma_q {\leq} \sigma_0 = \sigma(A, LI) {=} \langle A^{-1}, LI - A\rangle \\
&{\leq} \left\langle A^{-1}, \frac{L}{\mu}A - A \right\rangle{=} n \left( \frac{L}{\mu} - 1 \right) {\leq} \frac{nL}{\mu}.
\end{align*}
\end{proof}

\begin{definition}
    Define
    \begin{equation}\label{pside}
    \psi(A, G) \triangleq \langle A^{-1}, G - A \rangle - \ln {\rm Det}(A^{-1} G),
\end{equation}
where ${\rm Det}$ denotes the determinant of the matrix.
\end{definition}

\begin{definition}
Define
\[
\theta(A, B, u) := \left[ \frac{\langle (B - A)A^{-1}(B - A)u, u \rangle}{\langle BA^{-1}Bu, u \rangle} \right]^{1/2},
\]
and let \(\vartheta : (-1, +\infty) \rightarrow \mathbb{R}\) be the univarite function:
\[\vartheta(t) :=  t - \ln(1 + t) \geq 0.\]
\end{definition}

\begin{remark}
 On the interval $[0,+\infty)$, $\vartheta(t)$  satisfies:
 \begin{equation}\label{vart}
     \frac{t^2}{2(1+t)}\leq \vartheta(t)\leq \frac{t^2}{2+t}.
 \end{equation}
\end{remark}

\begin{lemma}\label{lemhesj} {\rm(\cite{newqn}, Lemma 5.2 ) } 
 Define $J_i := \int_0^1 \nabla^2 g(y_i + t s_i) \, dt$ and $y_{i+1}=y_i + s_i$. Then, $J_i s_i=\nabla g(y_{i+1})-\nabla g(y_i)$. If $B_0=L I$, then $\forall i\geq 0$, the {\rm BFGS} matrix $B_{i}$ satisfies:
\begin{equation} \label{hesb}
\frac{1}{\xi_i} \nabla^2 g(y_i) \preceq B_i \preceq \xi_i\frac{L}{\mu} \nabla^2 g(y_i),
\end{equation}
\begin{equation}\label{hesj}
\frac{1}{\xi_{i+1}} J_i \preceq B_i\preceq \xi_{i+1}\frac{L}{\mu} J_i,
\end{equation}
where $r_i := \left\| s_i \right\|_{y_i}, \quad \xi_i := e^{M \sum_{j=0}^{i-1} r_j} \quad (\geq 1)$ and the strongly self-concordant constant $M$ of $g$.
\end{lemma}

\begin{lemma}\label{generalju}
 When $B_0=L I$, the {\rm BFGS} matrix $B_{i}$ satisfies (\ref{hesb}) and (\ref{hesj}). If $\bar{\xi}=\underset{i=0,\cdots, k-1}{\rm max}\xi_{i+1}\leq 2$ in (\ref{hesj}),
then  the following inequality holds:
\begin{equation}\label{tildexi}
    \tilde{\xi} \sum_{i=0}^{k-1} \theta_{i}^2\leq n \left( \frac{L}{\mu}  - 1\right) +  \sum_{i=0}^{k-1} \Delta_i,
\end{equation}
where $\tilde{\xi}=\frac{1}{2 \left({\bar{\xi}}^2 + {\bar{\xi}}  \right)}$, $\theta_i:=\theta(J_i, B_i,u_i)$, $\psi_i:=\psi(J_i,B_i)$, $\tilde{\psi}_{i+1}:=\psi(J_i,B_{i+1})$, and $\Delta_i:= \psi_{i+1} - \tilde{\psi}_{i+1}$.
\end{lemma}

\begin{proof}
Note that
\begin{equation}\label{Jx}
    \frac{1}{\xi_{i+1}} J_i \preceq B_i \preceq  \frac{\xi_{i+1} L}{\mu} J_i.
\end{equation}

 From Lemma 2.4 of \cite{Nesterov2021rates}, it can be further deduced that:
 \begin{equation}
         \psi_i - \tilde{\psi}_{i+1} \geq \vartheta \left(\frac{1}{\xi_{i+1}}\theta_i \right).
 \end{equation}

 Since $\bar{\xi}=\underset{i=0,\cdots, k-1}{\rm max}\xi_{i+1}\leq 2$, it follows from the definition of $\theta_i$ that:
 \begin{equation}
     \theta_i^2=\frac{\langle (B_i - J_i)J_i^{-1}(B_i - J_i)u_i, u_i \rangle}{\langle B_iJ_i^{-1}B_iu_i, u_i \rangle}=1-\frac{\langle (2B_i - J_i)u_i, u_i \rangle}{\langle B_iJ_i^{-1}B_iu_i, u_i \rangle}\stackrel{(\ref{Jx})}{\leq} 1.
 \end{equation}
  Then, it is derived that:
 \begin{equation}
\vartheta \left( \frac{1}{\xi_{i+1}} \theta_i \right) \stackrel{(\ref{vart})}{\geq} \frac{\frac{1}{\xi_{i+1}^2} \theta_{i}^2}{2 \left(1 + \frac{1}{\xi_{i+1}} \theta_i \right)}  \geq\frac{\frac{1}{\xi_{i+1}^2} }{2 \left(1 + \frac{1}{\xi_{i+1}}  \right)}\theta_i^2= \frac{1}{2 \left(\xi_{i+1}^2 + \xi_{i+1}  \right)}\theta_i^2.
\end{equation}

Thus, it holds that:
\begin{equation}\label{psire}
    \tilde{\xi}\theta_i^2\leq \psi_i - \tilde{\psi}_{i+1}=\psi_i - \psi_{i+1}+\Delta_i,\quad \forall i\in \{0,\ldots, k-1\},
\end{equation}
where $ \tilde{\xi}=\frac{1}{2 \left(\bar{\xi}^2 + \bar{\xi}  \right)}$ and
\begin{equation}
\Delta_i:= \psi_{i+1} - \tilde{\psi}_{i+1} = \langle J^{-1}_{i+1} - J^{-1}_i, B_{i+1} \rangle + \ln \text{Det}(J^{-1}_i, J_{i+1}).
\end{equation}

By summing equation (\ref{psire}) over $i$ and given that \(\psi_k \geq 0\), it follows that:
\begin{equation}
\begin{split}
    & \tilde{\xi} \sum_{i=0}^{k-1} \theta_{i}^2 \leq \psi_0 - \psi_k + \sum_{i=0}^{k-1} \Delta_i \leq \psi_0 + \sum_{i=0}^{k-1} \Delta_i \\
&= \psi(J_0, LI) + \sum_{i=0}^{k-1} \Delta_i\\
&= \langle J^{-1}_0, LI - J_0 \rangle - \ln \text{Det}(J^{-1}_0, LI) + \sum_{i=0}^{k-1} \Delta_i \\
&\leq \langle J^{-1}_0, LI - J_0 \rangle + \sum_{i=0}^{k-1} \Delta_i\\
&\leq \langle J^{-1}_0, \frac{L}{\mu}J_0 - J_0 \rangle + \sum_{i=0}^{k-1} \Delta_i\\
&= n \left( \frac{L}{\mu}  - 1\right) +  \sum_{i=0}^{k-1} \Delta_i.
\end{split}
\end{equation}

\end{proof}

\paragraph{Notations:}
In step 1 of Algorithm \ref{alg:foa}, \(y_k^0 = y_k\) establishes the initial value of \(y\) at the start of the \(k\)-th iteration. After \(P\) warm-up iterations, denoted as \(y_{k,0} = y_k^P\), the term \(y_{k,T}\) refers to the state of \(y\) following \(T\) further iterations with \(x_k\) fixed. In the second step of Algorithm \ref{alg:foa}, \(u_{k,Q_k}\) represents the state of \(u\) after \(Q_k\) iterations, with both \(x_k\) and \(y_{k+1}\) fixed.

\subsection{Proof Sketch of Theorem \ref{gblprate}}\label{sec:proofske}
The proofs of Theorem \ref{qfblprate} and \ref{gblprate} encompasses three critical steps: first, it splits the hypergradient approximation error into the T-step error of estimating the lower level solution $\|y_{k,T}-y_k^*\|^2$ and the $Q_k$-step error $ \|u_{k,Q_k}-u_k^*\|^2$; second, it establishes upper bounds for these errors based on previous iteration errors; finally, it combines the above results to substantiate the theorem’s convergence. Since the proofs of Theorem \ref{qfblprate} and \ref{gblprate} are similar, we only elaborate on the proof sketch of Theorem \ref{gblprate} below.

\paragraph{Step 1:} Decomposing the hypergradient estimation error.

First, the hypergradient estimation error at the $k$th iteration can be bounded by:
\begin{equation}\label{phierror}
 \|\tilde{\nabla} \Phi(x_k)-\nabla\Phi(x_k)\|^2
    \leq 3\big(L_{F_{x}}^2+\frac{L_{f_{xy}}^2C_{F_{y}}^2}{\mu^2}\big)\|y_{k,T}-y^*_k\|^2+3M_{f_{xy}}^2\|u_{k,Q_k}-u_k^*\|^2.
\end{equation}
 

\paragraph{Step 2:} Upper-bounding the error.

The T-step error of estimating the lower level solution $\|y_{k,T}-y_k^*\|^2$  is bounded by:
\begin{equation}\label{yerror}
    \begin{split}
        \|y_k^*-y_{k,T}\|^2 &\leq \tau\|y_{k-1}^*-y_{k-1,T}\|^2+2(1+\frac{1}{\varepsilon})\kappa (\frac{1}{T})^{T}(1-\beta\mu)^PL_y^2\alpha^2\|\nabla \Phi(x_{k-1})\|^2\\
    &+12(1+\frac{1}{\varepsilon})\kappa (\frac{1}{T})^{T}(1-\beta\mu)^PL_y^2\alpha^2\frac{nLM_{f_{xy}}^2{C^2_{F_y}}}{\mu^3{\tilde{\xi}} Q_{k-1}},\\
    \end{split}
\end{equation}
where $\tau=\kappa (\frac{1}{T})^{T}(1-\beta\mu)^P\big((1+\varepsilon)+6(1+\frac{1}{\varepsilon})L_y^2\alpha^2(L_{F_{x}}^2+\frac{L_{f_{xy}}^2C^2_{F_{y}}}{\mu^2}+\frac{4M_{f_{xy}}^2L_{F_y}^2}{\mu^2}+\frac{4M_{f_{xy}}^2C^2_{F_y}L^2_{f_{yy}}}{\mu^4})\big)$.

The \(Q_k\)-step error $ \|u_{k,Q_k}-u_k^*\|$ is bounded by:
\begin{equation}\label{uerror}
 \|u_{k,Q_k}-u_k^*\|^2\leq 2\frac{nL{C^2_{F_y}}}{\tilde{\xi}\mu^3 Q_k}+4\left(\frac{L_{F_y}^2}{\mu^2}+\frac{C^2_{F_y}L^2_{f_{yy}}}{\mu^4}\right)\|y_k^*-y_{k,T}\|^2.
 \end{equation}
\paragraph{Step 3:} Combining Step 1 and Step 2.

Combining  {(\ref{phierror})}, {(\ref{yerror})} and {(\ref{uerror})}, the upper bound of the hypergradient estimation error is derived as:
\begin{equation*}
     \begin{split}
\|\tilde{\nabla} \Phi(x_k)-\nabla\Phi(x_k)\|^2&\leq \delta_0 \tau^k+\omega \alpha^2\sum_{j=0}^{k-1}{\tau^j \|\nabla \Phi(x_{k-1-j})\|^2}\\
&+6\omega\alpha^2\frac{nLM_{f_{xy}}^2{C^2_{F_y}}}{\mu^3 {\tilde{\xi}}}\sum_{j=0}^{k-1}{\tau^j \frac{1}{Q_{k-1-j}}}+6\frac{nLM_{f_{xy}}^2{C^2_{F_y}}}{\mu^3{\tilde{\xi}} Q_k},
     \end{split}
 \end{equation*}
with \[\delta_0=3\kappa (\frac{1}{T})^{T}(1-\beta\mu)^P(L_{F_{x}}^2+\frac{L_{f_{xy}}^2C_{F_{y}}^2}{\mu^2}+\frac{4M_{f_{xy}}^2L_{F_y}^2}{\mu^2}+\frac{4M_{f_{xy}}^2C^2_{F_y}L^2_{f_{yy}}}{\mu^4})\|y_0^*-y_{0}\|^2\] and
\[\omega=6(L_{F_{x}}^2+\frac{L_{f_{xy}}^2C_{F_{y}}^2}{\mu^2}+\frac{4M_{f_{xy}}^2L_{F_y}^2}{\mu^2}+\frac{4M_{f_{xy}}^2C^2_{F_y}L^2_{f_{yy}}}{\mu^4})(1+\frac{1}{\varepsilon})\kappa (\frac{1}{T})^{T}(1-\beta\mu)^PL_y^2.\]

Due to the $L_{\Phi}$-smoothness of  $\Phi$, the final convergence result can be  proved.


To prove Theorems \ref{qfblprate} and \ref{gblprate}, we first present the following lemma.
\begin{lemma} {\rm(Lemma 2.2 of \cite{ghadimi2018approximation}) } \label{prelemma}
Under Assumptions \ref{ass:F} and \ref{ass:f}, we have:
\begin{itemize}
    \item For all \(x, y\),
    \[
    \| \bar{\nabla} F(x; y) - \bar{\nabla} F(x; y^*(x)) \| \leq C \| y^*(x) - y \|,
    \]
    where $\bar{\nabla} F(x; y)=\nabla_x F(x, y)-[\nabla^2_{x y}f(x, y)]^T [\nabla^2_{y y} f(x, y)]^{-1} \nabla_y F(x, y)$ and \(C = L_{F_x} + \frac{L_{F_y} M_{f_{xy}}}{\mu} + C_{F_y} \left( \frac{L_{f_{xy}}}{\mu} + \frac{L_{f_{yy}} M_{f_{xy}}}{\mu^2} \right)\).

\item \( y^*(x) \) is \( L_y \)-Lipschitz continuous in $x$:
\[
\| y^*(x_1) - y^*(x_2) \| \leq L_y \| x_1 - x_2 \|,
\]
where \( L_y = \frac{M_{f_{xy}}}{\mu} \).

\item \( \nabla \Phi \) is \( L_\Phi \)-Lipschitz continuous in $x$:
\[
\| \nabla \Phi(x_1) - \nabla \Phi(x_2) \| \leq L_\Phi \| x_1 - x_2 \|,
\]
where \( L_\Phi = \frac{\left(\bar{L}_{F_y} + C\right) M_{f_{xy}}}{\mu} + L_{F_x} + C_{F_y} \left( \frac{\bar{L}_{f_{xy}} C_{F_y}}{\mu} + \frac{\bar{L}_{f_{yy}} M_{f_{xy}}}{\mu^2} \right) \).
\end{itemize}
\end{lemma}




\subsection{Proof  of Theorem \ref{qfblprate}}\label{sec:proofqfrate}

\begin{lemma}
Suppose that Assumptions \ref{ass:F} and \ref{ass:f} hold. The error between the approximate hypergradient $\tilde{\nabla} \Phi(x_k)$ and the true hypergradient in Algorithm \ref{alg:foa} can be bounded by:
    \begin{equation}\label{eqphi}
    \|\tilde{\nabla} \Phi(x_k)-\nabla\Phi(x_k)\|^2
    \leq 3\big(L_{F_{x}}^2+\frac{L_{f_{xy}}^2C_{F_{y}}^2}{\mu^2}\big)\|y_{k,T}-y^*_k\|^2+3M_{f_{xy}}^2\|u_{k,Q_k}-u_k^*\|^2.
\end{equation}
\end{lemma}
\begin{proof}
Let $u_k^*=[\nabla^2_{y y} f(x_k, y^*_k)]^{-1}\nabla_y F(x_k, y^*_k)$, then
\begin{equation*}
    \begin{split}
        &\|\tilde{\nabla} \Phi(x_k)-\nabla\Phi(x_k)\|^2\\
        =&\big\|\nabla_{x} F(x_k, y_{k,T})-[\nabla^2_{x y}f(x_k, y_{k,T})]^Tu_{k,Q_k}-\big(\nabla_{x} F(x_k, y^*_k)-[\nabla^2_{xy}f(x_k, y^*_k)]^Tu_k^*\big)\big\|^2\\
        =&\|\nabla_{x} F(x_k, y_{k,T})-\nabla_{x} F(x_k, y^*_k)-([\nabla^2_{x y}f(x_k, y_{k,T})]^Tu_{k,Q_k}-[\nabla^2_{xy}f(x_k, y^*_k)]^Tu_k^*)\|^2\\
        =&\|\nabla_{x} F(x_k, y_{k,T})-\nabla_{x} F(x_k, y^*_k)\\
        -&\big([\nabla^2_{x y}f(x_k, y_{k,T})]^Tu_{k,Q_k}-[\nabla^2_{x y}f(x_k, y_{k,T})]^Tu_k^*-([\nabla^2_{xy}f(x_k, y^*_k)]^Tu_k^*-[\nabla^2_{x y}f(x_k, y_{k,T})]^Tu_k^*)\big)\|^2\\
        =&\|\nabla_{x} F(x_k, y_{k,T})-\nabla_{x} F(x_k, y^*_k)\\
        -&[\nabla^2_{x y}f(x_k, y_{k,T})]^T(u_{k,Q_k}-u_k^*)-\big([\nabla^2_{x y}f(x_k, y_{k,T})]^T-[\nabla^2_{xy}f(x_k, y^*_k)]^T\big)u_k^*\|^2\\
        \leq&3\|\nabla_{x} F(x_k, y_{k,T})-\nabla_{x} F(x_k, y^*_k)\|^2+3\|\nabla^2_{x y}f(x_k, y_{k,T})\|^2\|u_{k,Q_k}-u_k^*\|^2\\
        &+3\|\nabla^2_{x y}f(x_k, y_{k,T})-\nabla^2_{xy}f(x_k, y^*_k)\|^2\|u_k^*\|^2.
    \end{split}
\end{equation*}

Based on Assumptions \ref{ass:F} and \ref{ass:f}, it can be derived that:
\begin{align*}
    &\|\tilde{\nabla} \Phi(x_k)-\nabla\Phi(x_k)\|^2\\
    \leq &3L_{F_{x}}^2 \|y_{k,T}-y^*_k\|^2+3M_{f_{xy}}^2\|u_{k,Q_k}-u_k^*\|^2+3\frac{L_{f_{xy}}^2C_{F_{y}}^2}{\mu^2}\|y_{k,T}-y^*_k\|^2\\
    =&3\big(L_{F_{x}}^2+\frac{L_{f_{xy}}^2C_{F_{y}}^2}{\mu^2}\big)\|y_{k,T}-y^*_k\|^2+3M_{f_{xy}}^2\|u_{k,Q_k}-u_k^*\|^2.
\end{align*}
\end{proof}





\begin{lemma}\label{lemqfurate1}
When the qNBO algorithm (Algorithm \ref{alg:foa}) is applied to solve the problem (\ref{blp}), if  the LL objective function $f$ takes the quadratic form (\ref{qf}) and $H_0 = (1/L)I$, it holds that:
\begin{equation*}
\sum_{i=1}^{Q_k}\|A^{-1}\nabla_y F(x_k, y_{k+1})-u_{k,i}\|\leq \frac{C_{F_y}}{\mu}\sqrt{\frac{nLQ_k}{\mu}},
\end{equation*}
with $Q_k>1$.
\end{lemma}
\begin{proof}
From Lemma \ref{lemqfu}, it follows that:
	\begin{equation*}
\begin{split}
	&\sum_{i=1}^{Q_k}	\frac{(A^{-1}\nabla_y F(x_k, y_{k+1})-u_{k,i})^T A (A^{-1}\nabla_y F(x_k, y_{k+1})-u_{k,i})}{u_{k,i}^T B_{k,i} u_{k,i}}\\
	=&\sum_{i=1}^{Q_k}	\frac{(A^{-1}\nabla_y F(x_k, y_{k+1})-u_{k,i})^T A (A^{-1}\nabla_y F(x_k, y_{k+1})-u_{k,i})}{\nabla_y F(x_k, y_{k+1})^T B_{k,i}^{-1}\nabla_y F(x_k, y_{k+1})}\\
	\leq  &\frac{nL}{\mu},
\end{split}
\end{equation*}
with $B_{k,i}=H_{k,i}^{-1}$.

Since $\mu I \preceq A\preceq LI$ and $A\preceq B_{k,i}\preceq\frac{L}{\mu}A$, we have:
\begin{equation*}
	\sum_{i=1}^{Q_k}	\frac{\mu \|A^{-1}\nabla_y F(x_k, y_{k+1})-u_{k,i}\|^2}{\frac{1}{\mu}\|\nabla_y F(x_k, y_{k+1})\|^2}\leq \frac{nL}{\mu}.
\end{equation*}

Since $\|\nabla_y F(x_k, y_{k+1})\|\leq C_{F_y}$, it can be further derived that:
\begin{equation}\label{eq:quad}
	\sum_{i=1}^{Q_k} \|A^{-1}\nabla_y F(x_k, y_{k+1})-u_{k,i}\|^2\leq \frac{nL{C^2_{F_y}}}{\mu^3}.
\end{equation}

Finally, by applying the Cauchy-Schwarz inequality, we can deduce that
\begin{equation}
\sum_{i=1}^{Q_k}\|A^{-1}\nabla_y F(x_k, y_{k+1})-u_{k,i}\|\leq \frac{C_{F_y}}{\mu}\sqrt{\frac{nLQ_k}{\mu}}.
\end{equation}
\end{proof}


\begin{lemma}\label{lemy1}
 Choose the parameters  $\beta$ and  $P$ such that $(1-\beta\mu)^P\|y_{k}-y_{k}^*\|\leq \frac{1}{300\sqrt{\mu}}$, and ensure $H_0$ satisfies: $\|\nabla^2_{yy}f(x_k,y^*(x_k))^{-1/2}\big(H_0^{-1}-\nabla^2_{yy}f(x_k,y^*(x_k))\big)\nabla^2_{yy}f(x_k,y^*(x_k))^{-1/2}\|_F\leq\frac{1}{7}$.  Then, under Assumptions \ref{ass:F} and \ref{ass:f}, it holds that
\begin{equation}\label{eqy1}
   \|y_k^*-y_{k,T}\|^2\leq (1+\varepsilon)\kappa (\frac{1}{T})^{T}(1-\beta\mu)^P\|y_{k-1,T}-y_{k-1}^*\|^2+(1+\frac{1}{\varepsilon})\kappa (\frac{1}{T})^{T}(1-\beta\mu)^PL_y^2\|x_{k}-x_{k-1}\|^2,
\end{equation}
    with a positive constant $\varepsilon$.
\end{lemma}
\begin{proof}
 Under the setting of parameters $\beta, P$ and $H_0$, the condition (\ref{yass}) is satisfied. 
    Furthermore, based on Lemma \ref{lemuserate} and the fact that $y_{k,0}=y_{k}^P$ and $y_{k}=y_{k}^0$, it holds that:
    \begin{equation*}
        \|y_{k,T}-y_k^*\|^2\leq \kappa(\frac{1}{T})^{T}\|y_{k,0}-y_k^*\|^2\leq \kappa(\frac{1}{T})^{T}(1-\beta\mu)^P\|y_{k}-y_k^*\|^2,
    \end{equation*}
    where $\kappa=\frac{L}{\mu}$. Finally, since $y_{k}=y_{k-1,T}$, using Young's inequality yields:
    \begin{align*}
        \|y_{k,T}-y_k^*\|^2&\leq (1+\varepsilon)\kappa (\frac{1}{T})^{T}(1-\beta\mu)^P\|y_{k-1,T}-y_{k-1}^*\|^2+(1+\frac{1}{\varepsilon})\kappa (\frac{1}{T})^{T}(1-\beta\mu)^P\|y_{k-1}^*-y_{k}^*\|^2\\
        &\leq (1+\varepsilon)\kappa (\frac{1}{T})^{T}(1-\beta\mu)^P\|y_{k-1,T}-y_{k-1}^*\|^2+(1+\frac{1}{\varepsilon})\kappa (\frac{1}{T})^{T}(1-\beta\mu)^PL_y^2\|x_{k-1}-x_{k}\|^2,
    \end{align*}
    where $\varepsilon$ is a positive constant and the last inequality  follows from Lemma 2.2 in \cite{ghadimi2018approximation}.
\end{proof}


\begin{lemma}\label{tgc}
 If the LL function $f$ takes the quadratic form and $T\geq t_b$, under Assumptions \ref{ass:F} and \ref{ass:f}, it is derived that 
\begin{equation}\label{eqy}
   \|y_k^*-y_{k,T}\|^2\leq (1+\varepsilon)c_t^2 \kappa^3 (\frac{1}{T})^{T}\|y_{k-1,T}-y_{k-1}^*\|^2+(1+\frac{1}{\varepsilon})c_t^2\kappa^3 (\frac{1}{T})^{T}L_y^2\|x_{k}-x_{k-1}\|^2,
\end{equation}
    with $t_b=4n{\rm ln}\frac{L}{\mu}$ and a positive constant $\varepsilon$.
\end{lemma}
\begin{proof}
    From Lemma \ref{gc}, if $T\geq t_b$, it holds that:
    \begin{equation*}
        \|y_{k,T}-y_k^*\|^2\leq c_t^2 \kappa^{3}(\frac{1}{T})^{T}\|y_{k,0}-y_k^*\|^2,
    \end{equation*}
    where $c_t=2t_b^{\frac{T}{2}}$. Furthermore, since $y_{k,0}=y_{k-1,T}$, using Young's inequality yields:
    \begin{align*}
        \|y_{k,T}-y_k^*\|^2&\leq (1+\varepsilon)c_t^2 \kappa^{3} (\frac{1}{T})^{T}\|y_{k-1,T}-y_{k-1}^*\|^2+(1+\frac{1}{\varepsilon})c_t^2 \kappa^{3} (\frac{1}{T})^{T}\|y_{k-1}^*-y_{k}^*\|^2\\
        &\leq (1+\varepsilon)c_t^2 \kappa^{3} (\frac{1}{T})^{T}\|y_{k-1,T}-y_{k-1}^*\|^2+(1+\frac{1}{\varepsilon})c_t^2 \kappa^{3} (\frac{1}{T})^{T}L_y^2\|x_{k-1}-x_{k}\|^2,
    \end{align*}
    where $\varepsilon$ is a positive constant and the last inequality  follows from Lemma 2.2 in \cite{ghadimi2018approximation}.
\end{proof}

\textcolor{black}{
\begin{lemma}\label{lemylc}
 Choose the parameters  $\beta$, $P$ such that $(1-\beta\mu)^P\|y_{k}-y_{k}^*\|\leq K_1$, and $H_0=LI$, under Assumptions \ref{ass:F} and \ref{ass:f}, it is derived that 
\begin{equation}\label{eqylc}
   \|y_k^*-y_{k,T}\|^2\leq (1+\varepsilon)c_l^2 \kappa^{3}(1-\beta\mu)^P (\frac{1}{T})^{T}\|y_{k-1,T}-y_{k-1}^*\|^2+(1+\frac{1}{\varepsilon})c_l^2 \kappa^{3}(1-\beta\mu)^P (\frac{1}{T})^{T}L_y^2\|x_{k-1}-x_{k}\|^2,
\end{equation}
    with a positive constant $\varepsilon$, $T\geq K_0$, $K_0:=8n{\rm ln}\frac{2L}{\mu}$, $c_l=2t_c^{\frac{T}{2}}$.
\end{lemma}
\begin{proof}
    From Lemma \ref{lc}, if $T\geq K_0$, it holds that:
    \begin{equation*}
        \|y_{k,T}-y_k^*\|^2\leq c_l^2 \kappa^{3}(\frac{1}{T})^{T}\|y_{k,0}-y_k^*\|^2\leq c_l^2 \kappa^{3}(\frac{1}{T})^{T}(1-\beta\mu)^P\|y_{k}-y_k^*\|^2,
    \end{equation*}
    where $c_l=2t_c^{\frac{T}{2}}$. Furthermore,  using Young's inequality yields:
    \begin{align*}
        \|y_{k,T}-y_k^*\|^2&\leq (1+\varepsilon)c_l^2 \kappa^{3}(1-\beta\mu)^P (\frac{1}{T})^{T}\|y_{k-1,T}-y_{k-1}^*\|^2+(1+\frac{1}{\varepsilon})c_l^2 \kappa^{3}(1-\beta\mu)^P (\frac{1}{T})^{T}\|y_{k-1}^*-y_{k}^*\|^2\\
        &\leq (1+\varepsilon)c_l^2 \kappa^{3}(1-\beta\mu)^P (\frac{1}{T})^{T}\|y_{k-1,T}-y_{k-1}^*\|^2+(1+\frac{1}{\varepsilon})c_l^2 \kappa^{3}(1-\beta\mu)^P (\frac{1}{T})^{T}L_y^2\|x_{k-1}-x_{k}\|^2,
    \end{align*}
    where $\varepsilon$ is a positive constant and the last inequality  follows from Lemma 2.2 in \cite{ghadimi2018approximation}.
\end{proof}}



\begin{lemma}
({\rm Error of $u_{k,Q_k}$}) Suppose that the lower level function $f$ has the quadratic form and Assumptions \ref{ass:F} and \ref{ass:f} hold. If $u_{k,Q_k}=\bar{u}_k$ and
     \begin{equation}\label{u*}
\bar{u}_k:=\underset{i}{\rm arg min}\|A^{-1}\nabla_y F(x_k, y_{k+1})-u_{k,i}\|,
\end{equation}
then for $Q_k>1$, the following inequality holds:
     \begin{equation}\label{quaequ}
         \|u_{k,Q_k}-u_k^*\|^2\leq 2\frac{nL{C^2_{F_y}}}{\mu^3 Q_k}+2\frac{L_{F_y}^2}{\mu^2}\|y_k^*-y_{k,T}\|^2.
     \end{equation}
\end{lemma}

\begin{proof}
From Lemma \ref{lemqfurate1}, we have:
     \begin{equation}
        \|A^{-1}\nabla_y F(x_k, y_{k+1})-\bar{u}_k\|^2\leq \frac{nL{C^2_{F_y}}}{\mu^3 Q_k}.
     \end{equation}
Moreover, under Assumption \ref{ass:F} and given that $y_{k+1} = y_{k,T}$, it holds that:
     \begin{align*}
         \|u_{k,Q_k}-u_k^*\|^2&\leq 2\|\bar{u}_k-A^{-1}\nabla_y F(x_k, y_{k+1})\|^2+2\|A^{-1}\nabla_y F(x_k, y_{k+1})-A^{-1}\nabla_y F(x_k, y_k^*)\|^2\\
         &\leq2\frac{nL{C^2_{F_y}}}{\mu^3 Q_k}+2\frac{L_{F_y}^2}{\mu^2}\|y_k^*-y_{k,T}\|^2.
     \end{align*}
\end{proof}

\begin{theorem}\label{agblprate1}
 ({\rm Restatement of Theorem \ref{qfblprate} with full parameter specifications}) Suppose that the LL function $f$ in (\ref{blp}) takes the quadratic form:
 \begin{equation}\label{qf1}
	f(x,y)=\frac{1}{2}y^T A y-y^T  x,
\end{equation}
where $\mu I\preceq A \preceq LI$
 such that Assumption \ref{ass:f}  holds. Choose the stepsize $\alpha>0$, the positive constant $\varepsilon>0$,  \textcolor{black}{$H_0=LI$} and $T\geq t_b$ ($t_b=4n{\rm ln}\kappa$) such that
 \[
 \tau<1\quad {\rm and} \quad \alpha L_{\Phi}+\omega \alpha^2 \left( \frac{1}{2} + \alpha L_{\Phi} \right)\frac{1}{1-\tau}\leq \frac{1}{4},
 \] 
 where $\tau=c_t^2 \kappa^3(\frac{1}{T})^{T}\big((1+\varepsilon)+6(1+\frac{1}{\varepsilon})L_y^2\alpha^2(L_{F_{x}}^2+\frac{L_{f_{xy}}^2C_{F_{y}}^2}{\mu^2}+\frac{2M_{f_{xy}}^2L_{F_y}^2}{\mu^2})\big)$, $\omega=6(L_{F_{x}}^2+\frac{L_{f_{xy}}^2C_{F_{y}}^2}{\mu^2}+\frac{2M_{f_{xy}}^2L_{F_y}^2}{\mu^2})(1+\frac{1}{\varepsilon})c_t^2 \kappa^3(\frac{1}{T})^{T}L_y^2$, $\kappa=\frac{L}{\mu}$ and $c_t=2t_b^{\frac{T}{2}}$. Then, under Assumptions \ref{ass:F} and \ref{ass:phi} , the iterate generated by the qNBO~(BFGS) algorithm
 (Algorithm \ref{alg:foa}) has the following convergence rate:
 \begin{equation}
     \frac{1}{K}\sum_{k=0}^{K-1}\|\nabla{\Phi}(x_k)\|^2\leq  \frac{4(\Phi(x_0)-\Phi(x^*))}{\alpha K}+\frac{3\delta_0}{K(1-\tau)}+\frac{1}{K}\sum_{k=0}^{K-1}\frac{18nLM_{f_{xy}}^2 C_{F_y}^2}{\mu^3 Q_k},
 \end{equation}
 with the initial error $\delta_0=3c_t^2 \kappa^3(\frac{1}{T})^{T}(L_{F_{x}}^2+\frac{L_{f_{xy}}^2C_{F_{y}}^2}{\mu^2}+\frac{2M_{f_{xy}}^2L_{F_y}^2}{\mu^2})\|y_0^*-y_{0}\|^2$. Specifically, if $Q_k={k+1}$, we have:
 \begin{equation}\label{aqblpratek}
     \frac{1}{K}\sum_{k=0}^{K-1}\|\nabla{\Phi}(x_k)\|^2\leq  \frac{4(\Phi(x_0)-{\inf_x \Phi(x)})}{\alpha K}+\frac{3\delta_0}{K(1-\tau)}+\frac{18nLM_{f_{xy}}^2 C_{F_y}^2{\rm ln}K}{\mu^3 K}.
 \end{equation}
\end{theorem}

\begin{proof}
Substituting the inequality (\ref{quaequ}) into (\ref{eqphi}) yields:
\begin{equation}\label{eqphi1}
\begin{split}
      \|\tilde{\nabla} \Phi(x_k)-\nabla\Phi(x_k)\|^2
    &\leq 3\big(L_{F_{x}}^2+\frac{L_{f_{xy}}^2C_{F_{y}}^2}{\mu^2}\big)\|y_{k,T}-y^*_k\|^2\\
    &+3M_{f_{xy}}^2\big(2\frac{nL{C^2_{F_y}}}{\mu^3 Q_k}+2\frac{L_{F_y}^2}{\mu^2}\|y_k^*-y_{k,T}\|^2\big)\\
    &\leq \big(3L_{F_{x}}^2+\frac{3L_{f_{xy}}^2C_{F_{y}}^2}{\mu^2}+\frac{6M_{f_{xy}}^2L_{F_y}^2}{\mu^2}\big)\|y_k^*-y_{k,T}\|^2\\
    &+6\frac{nLM_{f_{xy}}^2{C^2_{F_y}}}{\mu^3 Q_k}.
    \end{split}
\end{equation}

 Then, by plugging the inequality (\ref{eqphi1})
 into (\ref{eqy}), we obtain that:
\begin{equation}\label{eqyk}
    \begin{split}
        \|y_k^*-y_{k,T}\|^2&\leq (1+\varepsilon)c_t^2 \kappa^3 (\frac{1}{T})^{T}\|y_{k-1,T}-y_{k-1}^*\|^2+2(1+\frac{1}{\varepsilon})c_t^2 \kappa^3 (\frac{1}{T})^{T}L_y^2\alpha^2\|\nabla \Phi(x_{k-1})\|^2\\
    &+2(1+\frac{1}{\varepsilon})c_t^2 \kappa^3 (\frac{1}{T})^{T}L_y^2\alpha^2\|\tilde{\nabla} \Phi(x_{k-1})-\nabla\Phi(x_{k-1})\|^2\\
    &\leq \tau\|y_{k-1}^*-y_{k-1,T}\|^2+2(1+\frac{1}{\varepsilon})c_t^2 \kappa^3(\frac{1}{T})^{T}L_y^2\alpha^2\|\nabla \Phi(x_{k-1})\|^2\\
    &+12(1+\frac{1}{\varepsilon})c_t^2 \kappa^3(\frac{1}{T})^{T}L_y^2\alpha^2\frac{nLM_{f_{xy}}^2 C_{F_y}^2}{\mu^3 Q_{k-1}},\\
    \end{split}
\end{equation}
where $\tau=c_t^2 \kappa^3(\frac{1}{T})^{T}\big((1+\varepsilon)+6(1+\frac{1}{\varepsilon})L_y^2\alpha^2(L_{F_{x}}^2+\frac{L_{f_{xy}}^2C_{F_y}^2}{\mu^2}+\frac{2M_{f_{xy}}^2L_{F_y}^2}{\mu^2})\big)$.

By telescoping (\ref{eqyk}) over $k$, it follows that:
\begin{equation*}
    \begin{split}
        \|y_k^*-y_{k,T}\|^2&\leq \tau^k\|y_0^*-y_{0,T}\|^2+2(1+\frac{1}{\varepsilon})c_t^2 \kappa^3 (\frac{1}{T})^{T}L_y^2\alpha^2\sum_{j=0}^{k-1}{\tau^j \|\nabla \Phi(x_{k-1-j})\|^2}\\
        &+12(1+\frac{1}{\varepsilon})c_t^2 \kappa^3(\frac{1}{T})^{T}L_y^2\alpha^2\frac{nLM_{f_{xy}}^2C_{F_y}^2}{\mu^3}\sum_{j=0}^{k-1}{\tau^j \frac{1}{Q_{k-1-j}}}.
    \end{split}
\end{equation*}

Combining the inequality (\ref{eqphi1}) and $\|y_0^*-y_{0,T}\|^2\leq c_t^2 \kappa^3(\frac{1}{T})^{T}\|y_0^*-y_{0,0}\|^2,$ we can further derive that
 \begin{equation}\label{eqphid1}
     \begin{split}
\|\tilde{\nabla} \Phi(x_k)-\nabla\Phi(x_k)\|^2&\leq \delta_0 \tau^k+\omega \alpha^2\sum_{j=0}^{k-1}{\tau^j \|\nabla \Phi(x_{k-1-j})\|^2}\\
&+6\omega\alpha^2\frac{nLM_{f_{xy}}^2C_{F_y}^2}{\mu^3 }\sum_{j=0}^{k-1}{\tau^j \frac{1}{Q_{k-1-j}}}+6\frac{nLM_{f_{xy}}^2C_{F_y}^2}{\mu^3 Q_k},
     \end{split}
 \end{equation}
with $\delta_0=3c_t^2 \kappa^3(\frac{1}{T})^{T}(L_{F_{x}}^2+\frac{L_{f_{xy}}^2C_{F_{y}}^2}{\mu^2}+\frac{2M_{f_{xy}}^2L_{F_y}^2}{\mu^2})\|y_0^*-y_{0,0}\|^2$ and  $\omega=6(L_{F_{x}}^2+\frac{L_{f_{xy}}^2C_{F_{y}}^2}{\mu^2}+\frac{2M_{f_{xy}}^2L_{F_y}^2}{\mu^2})(1+\frac{1}{\varepsilon})c_t^2 \kappa^3(\frac{1}{T})^{T}L_y^2$.

Since $\nabla\Phi(\cdot)$ is $L_{\Phi}-$lipschitz continuous (Lemma 2.2 in \cite{ghadimi2018approximation}), we have:
    \begin{equation}
    \begin{split}
        \Phi(x_{k+1}) &\leq \Phi(x_k) + \langle \nabla \Phi(x_k), x_{k+1} - x_k \rangle + \frac{L_{\Phi}}{2} \|x_{k+1} - x_k\|^2 \\
&\leq \Phi(x_k) - \alpha\langle \nabla \Phi(x_k), \tilde{\nabla} \Phi(x_k) - \nabla \Phi(x_k) \rangle - \alpha\| \nabla \Phi(x_k)\|^2 + \alpha^2 L_{\Phi} \|\nabla \Phi(x_k)\|^2 \\
&+ \alpha^2 L_{\Phi} \|\nabla \Phi(x_k) - \tilde{\nabla} \Phi(x_k)\|^2\\
&\leq \Phi(x_k) - \left( \frac{\alpha}{2} - \alpha^2 L_{\Phi} \right) \|\nabla \Phi(x_k)\|^2 + \left( \frac{\alpha}{2} + \alpha^2 L_{\Phi} \right) \|\nabla \Phi(x_k) - \tilde{\nabla} \Phi(x_k)\|^2.
    \end{split}
    \end{equation}

Using the inequality (\ref{eqphid1}), it holds that:
    \begin{equation}\label{eqph1}
    \begin{split}
        \Phi(x_{k+1}) \leq & \Phi(x_k) - \left( \frac{\alpha}{2} - \alpha^2 L_{\Phi} \right) \|\nabla \Phi(x_{k})\|^2 + \left( \frac{\alpha}{2} + \alpha^2 L_{\Phi} \right) \|\nabla \Phi(x_k) - \tilde{\nabla} \Phi(x_{k})\|^2\\
        \leq& \Phi(x_k) - \left( \frac{\alpha}{2} - \alpha^2 L_{\Phi} \right) \|\nabla \Phi(x_k)\|^2 + \left( \frac{\alpha}{2} + \alpha^2 L_{\Phi} \right)\delta_0 \tau^k\\
        &+ \omega \alpha^2 \left( \frac{\alpha}{2} + \alpha^2 L_{\Phi} \right) \sum_{j=0}^{k-1} \tau^j \|\nabla \Phi(x_{k-1-j})\|^2 + \left( \frac{\alpha}{2} + \alpha^2 L_{\Phi} \right) \frac{6nLM_{f_{xy}}^2 C_{F_y}^2}{\mu^3 Q_k}\\
        &+ 6\omega\left( \frac{\alpha}{2} + \alpha^2 L_{\Phi} \right)\alpha^2 \frac{nLM_{f_{xy}}^2 C_{F_y}^2}{\mu^3}\sum_{j=0}^{k-1}{\tau^j \frac{1}{Q_{k-1-j}}}.
    \end{split}
\end{equation}

Finally, summing the inequality (\ref{eqph1}) from $k=0$ to $k=K-1$ yields:
\begin{equation}
    \begin{split}
        \left( \frac{\alpha}{2} - \alpha^2 L_{\Phi} \right) \sum_{k=0}^{K-1}\|\nabla \Phi(x_{k})\|^2\leq& \Phi(x_0)-\Phi(x_K)+\left( \frac{\alpha}{2} + \alpha^2 L_{\Phi} \right)\frac{\delta_0}{1-\tau}\\
        &+\omega \alpha^2 \left( \frac{\alpha}{2} + \alpha^2 L_{\Phi} \right) \sum_{k=0}^{K-1}\sum_{j=0}^{k-1} \tau^j \|\nabla \Phi(x_{k-1-j})\|^2\\
        &+6\omega \left( \frac{\alpha}{2} + \alpha^2 L_{\Phi} \right)\alpha^2 \frac{nLM_{f_{xy}}^2 C_{F_y}^2}{\mu^3}\sum_{k=0}^{K-1}\sum_{j=0}^{k-1}{\tau^j \frac{1}{Q_{k-1-j}}}\\
        &+ \sum_{k=0}^{K-1}\left( \frac{\alpha}{2} + \alpha^2 L_{\Phi} \right) \frac{6nLM_{f_{xy}}^2 C_{F_y}^2}{\mu^3 Q_k}.
    \end{split}
\end{equation}

Furthermore, from the inequality $\sum_{k=0}^{K-1}\sum_{j=0}^{k-1}a_j b_{k-1-j}\leq \sum_{k=0}^{K-1}a_k\sum_{j=0}^{K-1}b_j$, we obtain
\begin{equation*}
\begin{split}
    &\sum_{k=0}^{K-1}\sum_{j=0}^{k-1} \tau^j \|\nabla \Phi(x_{k-1-j})\|^2\leq \sum_{k=0}^{K-1} \tau^k \sum_{k=0}^{K-1}\|\nabla\Phi(x_{k})\|^2\leq \frac{1}{1-\tau}\sum_{k=0}^{K-1}\|\nabla\Phi(x_{k})\|^2,\\
     &\sum_{k=0}^{K-1}\sum_{j=0}^{k-1} \tau^j \frac{1}{Q_{k-1-j}}\leq \sum_{k=0}^{K-1} \tau^k \sum_{k=0}^{K-1}\frac{1}{Q_{k}}\leq \frac{1}{1-\tau}\sum_{k=0}^{K-1}\frac{1}{Q_{k}}.
\end{split}
\end{equation*}

Thus, we can conclude that
\begin{equation}
    \begin{split}
&\left( \frac{1}{2} - \alpha L_{\Phi}-\omega \alpha^2 \left( \frac{1}{2} + \alpha L_{\Phi} \right)\frac{1}{1-\tau} \right)\frac{1} {K}\sum_{k=0}^{K-1}\|\nabla \Phi(x_{k})\|^2\\
\leq&\frac{\Phi(x_0)-\Phi(x_K)}{\alpha K}+\left( \frac{1}{2} + \alpha L_{\Phi} \right)\frac{\delta_0}{K(1-\tau)}\\
&+ \left( \frac{1}{2} + \alpha L_{\Phi} \right) \frac{1} {K}\sum_{k=0}^{K-1}\frac{6nLM_{f_{xy}}^2 C_{F_y}^2}{\mu^3 Q_k}+\frac{6\omega}{1-\tau} \left( \frac{1}{2} + \alpha L_{\Phi} \right)\alpha^2\frac{1} {K}\sum_{k=0}^{K-1} \frac{nLM_{f_{xy}}^2 C_{F_y}^2}{\mu^3 Q_k}.
\end{split}
\end{equation}

Moreover, if  $\alpha L_{\Phi}+\omega \alpha^2 \left( \frac{1}{2} + \alpha L_{\Phi} \right)\frac{1}{1-\tau}\leq \frac{1}{4} $, then
\begin{equation}
    \frac{1} {K}\sum_{k=0}^{K-1}\|\nabla \Phi(x_{k})\|^2\leq \frac{4(\Phi(x_0)-\Phi(x^*))}{\alpha K}+\frac{3\delta_0}{K(1-\tau)}+\frac{1} {K}\sum_{k=0}^{K-1}\frac{18nLM_{f_{xy}}^2 C_{F_y}^2}{\mu^3 Q_k},
\end{equation}
where $\Phi(x^*)=\inf_x \Phi(x)$. Finally, if $Q_k={k+1}$, then (\ref{aqblpratek}) is established.

\end{proof}

Next, we consider the case with warm-up steps and provide the corresponding theorem, which proves to be similar to Theorem \ref{thm36g}, so a detailed proof is not provided.

\begin{theorem} 
 ({\rm Warm-up for quadratic $f$}) Suppose that the LL function $f$ in (\ref{blp}) takes the quadratic form:
 \begin{equation}\label{qf1}
	f(x,y)=\frac{1}{2}y^T A y-y^T  x,
\end{equation}
where $\mu I\preceq A \preceq LI$
 such that Assumption \ref{ass:f}  holds. Choose the stepsize $\beta$ and warm-start iteration steps $P$ such that $(1-\beta\mu)^P\|y_{k}-y_{k}^*\|\leq \frac{1}{300\sqrt{\mu}}$, and ensure the initial Hessian approximation matrix $H_0$ satisfies: $\|\nabla^2_{yy}f(x_k,y^*(x_k))^{-1/2}\big(H_0^{-1}-\nabla^2_{yy}f(x_k,y^*(x_k))\big)\nabla^2_{yy}f(x_k,y^*(x_k))^{-1/2}\|_F\leq\frac{1}{7}$. Choose the stepsize $\alpha>0$ and the positive constant $\varepsilon>0$ such that
 \[
 \tau<1\quad {\rm and} \quad \alpha L_{\Phi}+\omega \alpha^2 \left( \frac{1}{2} + \alpha L_{\Phi} \right)\frac{1}{1-\tau}\leq \frac{1}{4},
 \] 
 where $\tau=\kappa (\frac{1}{T})^{T}(1-\beta\mu)^P\big((1+\varepsilon)+6(1+\frac{1}{\varepsilon})L_y^2\alpha^2(L_{F_{x}}^2+\frac{L_{f_{xy}}^2C_{F_{y}}^2}{\mu^2}+\frac{2M_{f_{xy}}^2L_{F_y}^2}{\mu^2})\big)$, $\omega=6(L_{F_{x}}^2+\frac{L_{f_{xy}}^2C_{F_{y}}^2}{\mu^2}+\frac{2M_{f_{xy}}^2L_{F_y}^2}{\mu^2})(1+\frac{1}{\varepsilon})\kappa (\frac{1}{T})^{T}(1-\beta\mu)^PL_y^2$ and $\kappa=\frac{L}{\mu}$.
 Then, under Assumptions \ref{ass:F} and \ref{ass:phi}, the iterate generated by the qNBO~(BFGS) algorithm
 (Algorithm \ref{alg:foa}) has the following convergence rate:
 \begin{equation}
     \frac{1}{K}\sum_{k=0}^{K-1}\|\nabla{\Phi}(x_k)\|^2\leq  \frac{4(\Phi(x_0)-\Phi(x^*))}{\alpha K}+\frac{3\delta_0}{K(1-\tau)}+\frac{1} {K}\sum_{k=0}^{K-1}\frac{18nLM_{f_{xy}}^2 C_{F_y}^2}{\mu^3 Q_k},
 \end{equation}
 with the initial error $\delta_0=3\kappa (\frac{1}{T})^{T}(1-\beta\mu)^P(L_{F_{x}}^2+\frac{L_{f_{xy}}^2C_{F_{y}}^2}{\mu^2}+\frac{2M_{f_{xy}}^2L_{F_y}^2}{\mu^2})\|y_0^*-y_{0}\|^2$.
\end{theorem}


\subsection{Proof of Theorem \ref{gblprate} }\label{sec:proofgrate}

\begin{proposition}{\rm (Example 4.1 of \cite{greedyqn})}
Suppose  that $\forall x$, the LL  function \( f \) is \( \mu \)-strongly convex \textit{w.r.t.} $y$ and its Hessian is \( L_{f_{yy}} \)-Lipschitz continuous \textit{w.r.t.} $y$. Then \( f \) is strongly self-concordant with constant \( M = \frac{L_{f_{yy}}}{\mu^{3/2}} \), i.e.,
 \[
\nabla_{y y} ^2f(x,y_1)-\nabla_{y y} ^2f(x,y_2)\preceq M\|y_1-y_2\|_z \nabla_{y y} ^2f(x,w),\forall y_1,y_2, z, w\in\mathbb{R}^{n},
\]	
where $\|y\|_z:=\langle \nabla_{y y} ^2f(x,z)y, y\rangle^{1/2}$.
\end{proposition}
\begin{proof}
Using the Lipschitz continuity of the Hessian, we have
\begin{equation*}
    \begin{split}
        \nabla_{yy}^2 f(x,y_1) - \nabla_{yy}^2 f(x,y_2) &\preceq {L_{f_{yy}}} \|y_1-y_2\| I\\
        &\preceq \frac{L_{f_{yy}}}{\mu^{1/2}}\langle \nabla_{y y} ^2f(x,z)(y_1-y_2), y_1-y_2\rangle ^{1/2}I\\
        &=\frac{L_{f_{yy}}}{\mu^{1/2}}\|y_1-y_2\|_z I\preceq \frac{L_{f_{yy}}}{\mu^{3/2}}\|y_1-y_2\|_z \nabla_{y y} ^2f(x,w),
    \end{split}
\end{equation*}
where the second and the last inequalities follow from the fact that  $\mu I \preceq \nabla_{yy}^2 f(x,y)$.  This demonstrates that $f$ is strongly self-concordant with constant \( M = \frac{L_{f_{yy}}}{\mu^{3/2}} \).
\end{proof}


\begin{lemma}\label{lemurate1}
   If the Assumptions \ref{ass:F} and \ref{ass:f} hold, then $u_{k,i}$ generated in the step 2 of the Algorithm \ref{alg:foa} satisfies:
    \begin{equation}
    \sum_{i=1}^{Q_k}\|\nabla_{yy}^2 f(x_k,y_{k+1})^{-1}\nabla_y F(x_k, y_{k+1}) - u_{k,i}\|^2 \leq \frac{nL C^2_{F_y}}{\tilde{\xi}\mu^3},
\end{equation}
where $Q_k>1$, $\tilde{\xi}=\underset{i=1,\cdots, Q_k}{\rm min}\ \frac{1}{2 \left({\xi^2_i} + {\xi_i}  \right)}$ and $\xi_i=e^{M\sum_{j=0}^{i-1} \|\zeta_j u_{k,j}\|_{y_{k+1}}}$.
\end{lemma}

\begin{proof}
 Note that in Algorithm \ref{alg:uk}:
\begin{equation*}
    J_i := \int_0^1 \nabla_{yy}^2 f(x_k,y_{k+1} + t s_i) \, dt,\quad  J_{i+1} := \int_0^1 \nabla_{yy}^2 f(x_k,y_{k+1} + t s_{i+1}) \, dt,
\end{equation*}
with $s_i=\zeta_i H_{k,i}\nabla_y F(x_k, y_{k+1})$.

When the step size $\zeta_i$ is chosen appropriately, based on the definition of $J_i$ and the properties of $f$ as stated in Assumption \ref{ass:f}, it can be concluded that $J_i$ is nearly equal to $J_{i+1}$, i.e., $\Delta_i \approx 0$, and $\mu I \preceq J_i \preceq LI$. From Lemma \ref{generalju}, it follows that
\begin{equation}
    \sum_{i=1}^{Q_k}\frac{ (J_i^{-1}\nabla_y F(x_k, y_{k+1}) - u_{k,i})^T J_i(J_i^{-1}\nabla_y F(x_k, y_{k+1}) - u_{k,i})}{\nabla_y F(x_k, y_{k+1})^T J_i^{-1}\nabla_y F(x_k, y_{k+1})}\leq \frac{nL}{\tilde{\xi}\mu}.
\end{equation}

Since $\mu I \preceq J_i \preceq LI$, we have
\begin{equation}
    \sum_{i=1}^{Q_k}\frac{ \mu \|J_i^{-1}\nabla_y F(x_k, y_{k+1}) - u_{k,i}\|^2}{\frac{1}{\mu}\|\nabla_y F(x_k, y_{k+1})\|^2}\leq \frac{nL}{\tilde{\xi}\mu}.
\end{equation}

Moreover, it follows from Assumption \ref{ass:F}:
\begin{equation*}
    \sum_{i=1}^{Q_k}\|J_i^{-1}\nabla_y F(x_k, y_{k+1}) - u_{k,i}\|^2 \leq \frac{nL C^2_{F_y}}{\tilde{\xi}\mu^3}.
\end{equation*}

If the parameter $M$ of function $f$ or $\zeta_i u_{k,i}$ is sufficiently small, $J_i$ can be considered  as an approximation of $\nabla_{yy}^2 f(x_k, y_{k+1})$. Therefore, $\theta(J_i, B_i, u_i)$ can be used to characterize the approximation  between $H_{k,i}$ and $[\nabla_{yy}^2 f(x_k, y_{k+1})]^{-1}$ along the gradient direction $\nabla_y F(x_k, y_{k+1})$, i.e.,
\begin{equation}
    \sum_{i=1}^{Q_k}\|\nabla_{yy}^2 f(x_k,y_{k+1})^{-1}\nabla_y F(x_k, y_{k+1}) - u_{k,i}\|^2 \leq \frac{nL C^2_{F_y}}{\tilde{\xi}\mu^3},
\end{equation}
where $\tilde{\xi}=\underset{i=1,\cdots, Q_k}{\rm min}\frac{1}{2 \left({\xi_i}^2 + {\xi_i}  \right)}$ and $\xi_i=e^{M\sum_{j=0}^{i-1} \|\zeta_j u_{k,j}\|_{y_{k+1}}}$.
\end{proof}


\begin{lemma}\label{lemguerror}
     Suppose that Assumption \ref{ass:f} holds. Note that $u_{k+1}=u_{k,Q_k}$ in the step 2 of Algorithm \ref{alg:foa}. If $u_{k,Q_k}=\bar{u}_k$  with
     \begin{equation}
\bar{u}_k:=\underset{i}{\rm arg min}\|\nabla_{yy}^2 f(x_k,y_{k+1})^{-1}\nabla_y F(x_k, y_{k+1}) - u_{k,i}\|^2, 
\end{equation}
then
    \begin{equation}\label{equerror}
 \|u_{k,Q_k}-u_k^*\|^2\leq 2\frac{nL{C^2_{F_y}}}{\tilde{\xi}\mu^3 Q_k}+4\left(\frac{L_{F_y}^2}{\mu^2}+\frac{C^2_{F_y}L^2_{f_{yy}}}{\mu^4}\right)\|y_k^*-y_{k,T}\|^2, \rm{ \forall Q_k>1},
\end{equation}
 where $\tilde{\xi}=\underset{i=1,\cdots, Q_k}{\rm min}\frac{1}{2 \left({\xi_i}^2 + {\xi_i}  \right)}$ and $\xi_i=e^{M\sum_{j=0}^{i-1} \|\zeta_j u_{k,j}\|_{y_{k+1}}}$.
\end{lemma}
\begin{proof}
Combining Lemma \ref{lemurate1} and the definition of $\bar{u}_k$  yields:
    \begin{equation}
\|\nabla_{yy}^2 f(x_k,y_{k+1})^{-1}\nabla_y F(x_k, y_{k+1}) - u_{k,Q_k}\|^2 \leq \frac{nL C^2_{F_y}}{\tilde{\xi}\mu^3Q_k}.
\end{equation}

Under Assumptions \ref{ass:F} and \ref{ass:f}, since $y_{k+1}=y_{k,T}$, it holds that 
     \begin{equation}\label{yk*}
         \begin{split}
             &\|\nabla_{yy}^2 f(x_k,y_{k+1})^{-1}\nabla_y F(x_k, y_{k+1})-\nabla_{yy}^2 f(x_k,y_k^*)^{-1}\nabla_y F(x_k, y_k^*)\|^2\\
             \leq& 2\|\nabla_{yy}^2 f(x_k,y_{k+1})^{-1}\nabla_y F(x_k, y_{k+1})-\nabla_{yy}^2 f(x_k,y_{k+1})^{-1}\nabla_y F(x_k, y_k^*)\|^2\\
             &+2\|\nabla_{yy}^2 f(x_k,y_{k+1})^{-1}\nabla_y F(x_k, y_k^*)-\nabla_{yy}^2 f(x_k,y_k^*)^{-1}\nabla_y F(x_k, y_k^*)\|^2\\
             \leq & 2\frac{L_{F_y}^2}{\mu^2}\|y_k^*-y_{k,T}\|^2+2C^2_{F_y}\|\nabla_{yy}^2 f(x_k,y_{k+1})^{-1}-\nabla_{yy}^2 f(x_k,y_k^*)^{-1}\|^2\\
             \leq & 2\frac{L_{F_y}^2}{\mu^2}\|y_k^*-y_{k,T}\|^2\\
             &+2C^2_{F_y}\|\nabla_{yy}^2 f(x_k,y_{k+1})^{-1}\|^2\|\nabla_{yy}^2 f(x_k,y_{k+1})-\nabla_{yy}^2 f(x_k,y_k^*)\|^2\|\nabla_{yy}^2 f(x_k,y_k^*)^{-1}\|^2\\
             \leq & 2\frac{L_{F_y}^2}{\mu^2}\|y_k^*-y_{k,T}\|^2+2\frac{C^2_{F_y}L^2_{f_{yy}}}{\mu^4}\|y_k^*-y_{k,T}\|^2\\
             =&2\left(\frac{L_{F_y}^2}{\mu^2}+\frac{C^2_{F_y}L^2_{f_{yy}}}{\mu^4}\right)\|y_k^*-y_{k,T}\|^2.
         \end{split}
     \end{equation}

     Finally, from Assumption \ref{ass:F} and the inequality (\ref{yk*}), it is derived that
     \begin{equation}
         \begin{split}
             \|u_{k,Q_k}-u_k^*\|^2\leq &2\|u_{k,Q_k}-\nabla_{yy}^2 f(x_k,y_{k+1})^{-1}\nabla_y F(x_k, y_{k+1})\|^2\\
         &+2\|\nabla_{yy}^2 f(x_k,y_{k+1})^{-1}\nabla_y F(x_k, y_{k+1})-\nabla_{yy}^2 f(x_k,y_k^*)^{-1}\nabla_y F(x_k, y_k^*)\|^2\\
         &\leq2\frac{nL{C^2_{F_y}}}{\tilde{\xi}\mu^3 Q_k}+4\left(\frac{L_{F_y}^2}{\mu^2}+\frac{C^2_{F_y}L^2_{f_{yy}}}{\mu^4}\right)\|y_k^*-y_{k,T}\|^2.
         \end{split}
     \end{equation}
\end{proof}


\begin{theorem}\label{thm36g}
({\rm Restatement of Theorem \ref{gblprate} with full parameter specifications}) Suppose that Assumptions \ref{ass:F}, \ref{ass:f} and \ref{ass:phi} hold. Choose the stepsize $\beta$ and warm-up iteration steps $P$ such that $(1-\beta\mu)^P\|y_{k}-y_{k}^*\|\leq \frac{1}{300\sqrt{\mu}}$, and ensure the initial Hessian approximation matrix $H_0$ satisfies: $\|\nabla^2_{yy}f(x_k,y^*(x_k))^{-1/2}\big(H_0^{-1}-\nabla^2_{yy}f(x_k,y^*(x_k))\big)\nabla^2_{yy}f(x_k,y^*(x_k))^{-1/2}\|_F\leq\frac{1}{7}$. Define $\tau=\kappa (\frac{1}{T})^{T}(1-\beta\mu)^P\big((1+\varepsilon)+6(1+\frac{1}{\varepsilon})L_y^2\alpha^2(L_{F_{x}}^2+\frac{L_{f_{xy}}^2C_{F_{y}}^2}{\mu^2}+\frac{4M_{f_{xy}}^2L_{F_y}^2}{\mu^2}+\frac{4M_{f_{xy}}^2C^2_{F_y}L^2_{f_{yy}}}{\mu^4})\big)$ and $\omega=6(L_{F_{x}}^2+\frac{L_{f_{xy}}^2C_{F_{y}}^2}{\mu^2}+\frac{4M_{f_{xy}}^2L_{F_y}^2}{\mu^2}+\frac{4M_{f_{xy}}^2C^2_{F_y}L^2_{f_{yy}}}{\mu^4})(1+\frac{1}{\varepsilon})\kappa (\frac{1}{T})^{T}(1-\beta\mu)^PL_y^2$. Choose the stepsize $\alpha>0$, the positive constant $\varepsilon>0$  and iterate $T>0$ such that
 \[
 \tau<1 \quad {\rm and} \quad \alpha L_{\Phi}+\omega \alpha^2 \left( \frac{1}{2} + \alpha L_{\Phi} \right)\frac{1}{1-\tau}\leq \frac{1}{4}.
 \] 
 Then, the solution $x_k$ generated by Algorithm \ref{alg:foa} achieves the following convergence rate:
 \begin{equation}
     \frac{1}{K}\sum_{k=0}^{K-1}\|\nabla{\Phi}(x_k)\|^2\leq  \frac{4(\Phi(x_0)-{\inf_x \Phi(x)})}{\alpha K}+\frac{3\delta_0}{K(1-\tau)}+\frac{1}{K}\sum_{k=0}^{K-1}\frac{18nLM_{f_{xy}}^2 C_{F_y}^2}{\mu^3{\tilde{\xi}}  Q_k},
 \end{equation}
 where $\delta_0=3\kappa (\frac{1}{T})^{T}(1-\beta\mu)^P(L_{F_{x}}^2+\frac{L_{f_{xy}}^2C_{F_{y}}^2}{\mu^2}+\frac{4M_{f_{xy}}^2L_{F_y}^2}{\mu^2}+\frac{4M_{f_{xy}}^2C^2_{F_y}L^2_{f_{yy}}}{\mu^4})\|y_0^*-y_{0}\|^2$ is the initial error. Specifically, if $Q_k=k+1$, we have:
 \begin{equation}\label{agblpratek}
     \frac{1}{K}\sum_{k=0}^{K-1}\|\nabla{\Phi}(x_k)\|^2\leq  \frac{4(\Phi(x_0)-\Phi(x^*))}{\alpha K}+\frac{3\delta_0}{K(1-\tau)}+\frac{18nLM_{f_{xy}}^2 C_{F_y}^2{\rm ln}K}{\mu^3{\tilde{\xi}}K}.
 \end{equation}
\end{theorem}

\begin{proof}
Substituting the inequality (\ref{equerror}) into (\ref{eqphi}) yields:
\begin{equation}\label{eqphi1g}
\begin{split}
      \|\tilde{\nabla} \Phi(x_k)-\nabla\Phi(x_k)\|^2
    &\leq 3\big(L_{F_{x}}^2+\frac{L_{f_{xy}}^2C_{F_{y}}^2}{\mu^2}\big)\|y_{k,T}-y^*_k\|^2\\
    &+3M_{f_{xy}}^2\left(2\frac{nL{C^2_{F_y}}}{\tilde{\xi}\mu^3 Q_k}+4\big(\frac{L_{F_y}^2}{\mu^2}+\frac{C^2_{F_y}L^2_{f_{yy}}}{\mu^4}\big)\|y_k^*-y_{k,T}\|^2\right)\\
    &\leq \left(3L_{F_{x}}^2+\frac{3L_{f_{xy}}^2C^2_{F_{y}}}{\mu^2}+{12 M_{f_{xy}}^2}\big(\frac{L_{F_y}^2}{\mu^2}+\frac{C^2_{F_y}L^2_{f_{yy}}}{\mu^4}\big)\right)\|y_k^*-y_{k,T}\|^2\\
    &+6\frac{nLM_{f_{xy}}^2{C^2_{F_y}}}{\mu^3{\tilde{\xi}} Q_k}.\\
\end{split}
\end{equation}

 Then, based on Lemma \ref{lemy1}, substituting the above inequality into (\ref{eqy1}) yields:
\begin{equation}\label{eqyk1}
    \begin{split}
        \|y_k^*-y_{k,T}\|^2&\leq (1+\varepsilon)\kappa (\frac{1}{T})^{T}(1-\beta\mu)^P\|y_{k-1,T}-y_{k-1}^*\|^2+2(1+\frac{1}{\varepsilon})\kappa (\frac{1}{T})^{T}(1-\beta\mu)^PL_y^2\alpha^2\|\nabla \Phi(x_{k-1})\|^2\\
    &+2(1+\frac{1}{\varepsilon})\kappa (\frac{1}{T})^{T}(1-\beta\mu)^PL_y^2\alpha^2\|\tilde{\nabla} \Phi(x_{k-1})-\nabla\Phi(x_{k-1})\|^2\\
    &\leq \tau\|y_{k-1}^*-y_{k-1,T}\|^2+2(1+\frac{1}{\varepsilon})\kappa (\frac{1}{T})^{T}(1-\beta\mu)^PL_y^2\alpha^2\|\nabla \Phi(x_{k-1})\|^2\\
    &+12(1+\frac{1}{\varepsilon})\kappa (\frac{1}{T})^{T}(1-\beta\mu)^PL_y^2\alpha^2\frac{nLM_{f_{xy}}^2{C^2_{F_y}}}{\mu^3{\tilde{\xi}} Q_{k-1}},\\
    \end{split}
\end{equation}
where $\tau=\kappa (\frac{1}{T})^{T}(1-\beta\mu)^P\big((1+\varepsilon)+6(1+\frac{1}{\varepsilon})L_y^2\alpha^2(L_{F_{x}}^2+\frac{L_{f_{xy}}^2C^2_{F_{y}}}{\mu^2}+\frac{4M_{f_{xy}}^2L_{F_y}^2}{\mu^2}+\frac{4M_{f_{xy}}^2C^2_{F_y}L^2_{f_{yy}}}{\mu^4})\big)$.

Summing the inequality (\ref{eqyk1}) from 0 to $k$ results in:
\begin{equation*}
    \begin{split}
        \|y_k^*-y_{k,T}\|^2&\leq \tau^k\|y_0^*-y_{0,T}\|^2+2(1+\frac{1}{\varepsilon})\kappa (\frac{1}{T})^{T}(1-\beta\mu)^PL_y^2\alpha^2\sum_{j=0}^{k-1}{\tau^j \|\nabla \Phi(x_{k-1-j})\|^2}\\
        &+12(1+\frac{1}{\varepsilon})\kappa (\frac{1}{T})^{T}(1-\beta\mu)^PL_y^2\alpha^2\frac{nLM_{f_{xy}}^2{C^2_{F_y}}}{\mu^3{\tilde{\xi}} }\sum_{j=0}^{k-1}{\tau^j \frac{1}{Q_{k-1-j}}}.
    \end{split}
\end{equation*}

 Combining the inequality (\ref{eqphi1g}) and $\|y_0^*-y_{0,T}\|^2\leq \kappa(\frac{1}{T})^{T}(1-\beta\mu)^P\|y_0^*-y_{0}\|^2$, it follows that
 \begin{equation}\label{eqphid}
     \begin{split}
\|\tilde{\nabla} \Phi(x_k)-\nabla\Phi(x_k)\|^2&\leq \delta_0 \tau^k+\omega \alpha^2\sum_{j=0}^{k-1}{\tau^j \|\nabla \Phi(x_{k-1-j})\|^2}\\
&+6\omega\alpha^2\frac{nLM_{f_{xy}}^2{C^2_{F_y}}}{\mu^3 {\tilde{\xi}}}\sum_{j=0}^{k-1}{\tau^j \frac{1}{Q_{k-1-j}}}+6\frac{nLM_{f_{xy}}^2{C^2_{F_y}}}{\mu^3{\tilde{\xi}} Q_k},
     \end{split}
 \end{equation}
where $\delta_0=3\kappa (\frac{1}{T})^{T}(1-\beta\mu)^P(L_{F_{x}}^2+\frac{L_{f_{xy}}^2C_{F_{y}}^2}{\mu^2}+\frac{4M_{f_{xy}}^2L_{F_y}^2}{\mu^2}+\frac{4M_{f_{xy}}^2C^2_{F_y}L^2_{f_{yy}}}{\mu^4})\|y_0^*-y_{0}\|^2$ and $\omega=6(L_{F_{x}}^2+\frac{L_{f_{xy}}^2C_{F_{y}}^2}{\mu^2}+\frac{4M_{f_{xy}}^2L_{F_y}^2}{\mu^2}+\frac{4M_{f_{xy}}^2C^2_{F_y}L^2_{f_{yy}}}{\mu^4})(1+\frac{1}{\varepsilon})\kappa (\frac{1}{T})^{T}(1-\beta\mu)^PL_y^2$.

Since $\nabla\Phi(\cdot)$ is $L_{\Phi}-$Lipschitz, it can be obtained that
    \begin{equation}
    \begin{split}
        \Phi(x_{k+1}) &\leq \Phi(x_k) + \langle \nabla \Phi(x_k), x_{k+1} - x_k \rangle + \frac{L_{\Phi}}{2} \|x_{k+1} - x_k\|^2 \\
&\leq \Phi(x_k) - \alpha\langle \nabla \Phi(x_k), \tilde{\nabla} \Phi(x_k) - \nabla \Phi(x_k) \rangle - \alpha\| \nabla \Phi(x_k)\|^2 + \alpha^2 L_{\Phi} \|\nabla \Phi(x_k)\|^2 \\
&+ \alpha^2 L_{\Phi} \|\nabla \Phi(x_k) - \tilde{\nabla} \Phi(x_k)\|^2\\
&\leq \Phi(x_k) - \left( \frac{\alpha}{2} - \alpha^2 L_{\Phi} \right) \|\nabla \Phi(x_k)\|^2 + \left( \frac{\alpha}{2} + \alpha^2 L_{\Phi} \right) \|\nabla \Phi(x_k) - \tilde{\nabla} \Phi(x_k)\|^2.
    \end{split}
    \end{equation}

    Using the inequality (\ref{eqphid}) yields:
    \begin{equation}\label{eqph}
    \begin{split}
        \Phi(x_{k+1}) \leq & \Phi(x_k) - \left( \frac{\alpha}{2} - \alpha^2 L_{\Phi} \right) \|\nabla \Phi(x_{k})\|^2 + \left( \frac{\alpha}{2} + \alpha^2 L_{\Phi} \right) \|\nabla \Phi(x_k) - \tilde{\nabla} \Phi(x_{k})\|^2\\
        \leq& \Phi(x_k) - \left( \frac{\alpha}{2} - \alpha^2 L_{\Phi} \right) \|\nabla \Phi(x_k)\|^2 + \left( \frac{\alpha}{2} + \alpha^2 L_{\Phi} \right)\delta_0 \tau^k\\
        &+ \omega \alpha^2 \left( \frac{\alpha}{2} + \alpha^2 L_{\Phi} \right) \sum_{j=0}^{k-1} \tau^j \|\nabla \Phi(x_{k-1-j})\|^2 + \left( \frac{\alpha}{2} + \alpha^2 L_{\Phi} \right) \frac{6nLM_{f_{xy}}^2 C_{F_y}^2}{\mu^3{\tilde{\xi}} Q_k}\\
        &+ 6\omega\left( \frac{\alpha}{2} + \alpha^2 L_{\Phi} \right)\alpha^2 \frac{nLM_{f_{xy}}^2 C_{F_y}^2}{\mu^3{\tilde{\xi}}  }\sum_{j=0}^{k-1}{\tau^j \frac{1}{Q_{k-1-j}}}.
    \end{split}
\end{equation}

Finally, by telescoping the inequality (\ref{eqph}) from $k=0$ to $k=K-1$, it is derived that
\begin{equation}
    \begin{split}
        \left( \frac{\alpha}{2} - \alpha^2 L_{\Phi} \right) \sum_{k=0}^{K-1}\|\nabla \Phi(x_{k})\|^2\leq& \Phi(x_0)-\Phi(x_K)+\left( \frac{\alpha}{2} + \alpha^2 L_{\Phi} \right)\frac{\delta_0}{1-\tau}\\
        &+\omega \alpha^2 \left( \frac{\alpha}{2} + \alpha^2 L_{\Phi} \right) \sum_{k=0}^{K-1}\sum_{j=0}^{k-1} \tau^j \|\nabla \Phi(x_{k-1-j})\|^2\\
        &+ \sum_{k=0}^{K-1}\left( \frac{\alpha}{2} + \alpha^2 L_{\Phi} \right) \frac{6nLM_{f_{xy}}^2 C_{F_y}^2}{\mu^3 {\tilde{\xi}}Q_k}\\
        &+6\omega \left( \frac{\alpha}{2} + \alpha^2 L_{\Phi} \right)\alpha^2 \frac{nLM_{f_{xy}}^2 C_{F_y}^2}{\mu^3{\tilde{\xi}} }\sum_{k=0}^{K-1}\sum_{j=0}^{k-1}{\tau^j \frac{1}{Q_{k-1-j}}}.
    \end{split}
\end{equation}

Moreover, due to $\sum_{k=0}^{K-1}\sum_{j=0}^{k-1}a_j b_{k-1-j}\leq \sum_{k=0}^{K-1}a_k\sum_{j=0}^{K-1}b_j$, we can deduce that
\begin{equation*}
\begin{split}
    &\sum_{k=0}^{K-1}\sum_{j=0}^{k-1} \tau^j \|\nabla \Phi(x_{k-1-j})\|^2\leq \sum_{k=0}^{K-1} \tau^k \sum_{k=0}^{K-1}\|\nabla\Phi(x_{k})\|^2\leq \frac{1}{1-\tau}\sum_{k=0}^{K-1}\|\nabla\Phi(x_{k})\|^2,\\
    &\sum_{k=0}^{K-1}\sum_{j=0}^{k-1} \tau^j \frac{1}{Q_{k-1-j}}\leq \sum_{k=0}^{K-1} \tau^k \sum_{k=0}^{K-1}\frac{1}{Q_{k}}\leq \frac{1}{1-\tau}\sum_{k=0}^{K-1}\frac{1}{Q_{k}}.
    \end{split}
\end{equation*}

Then, the following inequality holds:
\begin{equation}
    \begin{split}
&\left( \frac{1}{2} - \alpha L_{\Phi}-\omega \alpha^2 \left( \frac{1}{2} + \alpha L_{\Phi} \right)\frac{1}{1-\tau} \right)\frac{1} {K}\sum_{k=0}^{K-1}\|\nabla \Phi(x_{k})\|^2\\
\leq&\frac{\Phi(x_0)-\Phi(x_K)}{\alpha K}+\left( \frac{1}{2} + \alpha L_{\Phi} \right)\frac{\delta_0}{K(1-\tau)}\\
&+ \frac{1} {K}\sum_{k=0}^{K-1}\left( \frac{1}{2} + \alpha L_{\Phi} \right)\frac{6nLM_{f_{xy}}^2 C_{F_y}^2}{\mu^3{\tilde{\xi}} Q_k}+\frac{1} {K}\sum_{k=0}^{K-1}\frac{6\omega}{1-\tau} \left( \frac{1}{2} + \alpha L_{\Phi} \right)\alpha^2 \frac{nLM_{f_{xy}}^2 C_{F_y}^2}{\mu^3{\tilde{\xi}} Q_k}.
\end{split}
\end{equation}

If $\alpha L_{\Phi}+\omega \alpha^2 \left( \frac{1}{2} + \alpha L_{\Phi} \right)\frac{1}{1-\tau}\leq \frac{1}{4} $, then
\begin{equation}\label{corocite}
    \frac{1} {K}\sum_{k=0}^{K-1}\|\nabla \Phi(x_{k})\|^2\leq \frac{4(\Phi(x_0)-{\inf_x \Phi(x)})}{\alpha K}+\frac{3\delta_0}{K(1-\tau)}+\frac{1} {K}\sum_{k=0}^{K-1}\frac{18nLM_{f_{xy}}^2 C_{F_y}^2}{\mu^3{\tilde{\xi}} Q_k}.
\end{equation}


Finally, by substituting \(Q_k = k + 1\) into  (\ref{corocite}), (\ref{agblpratek}) is derived.
\end{proof}


 In addition, we present another theorem that does not require the strict assumption on the initial Hessian matrix \( H_0 \), but does require that \( T \geq 8n \ln\left(\frac{2L}{\mu}\right) \).

 \textcolor{black}{
\begin{theorem}\label{thm36g1}
Suppose that Assumptions \ref{ass:F}, \ref{ass:f} and \ref{ass:phi}  hold. Choose the stepsize $\beta$ and warm-up iteration steps $P$ such that $(1-\beta\mu)^P\|y_{k}-y_{k}^*\|\leq K_1$, where $K_1$ is defined in (\ref{eqlca}). Set $H_0=LI$ and $T\geq 8n{\rm ln}{\frac{2L}{\mu}}$. Define $\tau=c_l^2 \kappa^3 (\frac{1}{T})^{T}(1-\beta\mu)^P\big((1+\varepsilon)+6(1+\frac{1}{\varepsilon})L_y^2\alpha^2(L_{F_{x}}^2+\frac{L_{f_{xy}}^2C_{F_{y}}^2}{\mu^2}+\frac{4M_{f_{xy}}^2L_{F_y}^2}{\mu^2}+\frac{4M_{f_{xy}}^2C^2_{F_y}L^2_{f_{yy}}}{\mu^4})\big)$ and $\omega=6(L_{F_{x}}^2+\frac{L_{f_{xy}}^2C_{F_{y}}^2}{\mu^2}+\frac{4M_{f_{xy}}^2L_{F_y}^2}{\mu^2}+\frac{4M_{f_{xy}}^2C^2_{F_y}L^2_{f_{yy}}}{\mu^4})(1+\frac{1}{\varepsilon})c_l^2 \kappa^3 (\frac{1}{T})^{T}(1-\beta\mu)^PL_y^2$. Choose the stepsize $\alpha>0$, the positive constant $\varepsilon>0$  and iterate $T>0$ such that
  \[
 \tau<1 \quad {\rm and} \quad \alpha L_{\Phi}+\omega \alpha^2 \left( \frac{1}{2} + \alpha L_{\Phi} \right)\frac{1}{1-\tau}\leq \frac{1}{4}.
 \] 
 Then, the solution $x_k$ generated by Algorithm \ref{alg:foa} achieves the following convergence rate:
 \begin{equation}
     \frac{1}{K}\sum_{k=0}^{K-1}\|\nabla{\Phi}(x_k)\|^2\leq  \frac{4(\Phi(x_0)-\Phi(x^*))}{\alpha K}+\frac{3\delta_0}{K(1-\tau)}+\frac{1}{K}\sum_{k=0}^{K-1}\frac{18nLM_{f_{xy}}^2 C_{F_y}^2}{\mu^3{\tilde{\xi}}  Q_k},
 \end{equation}
 where $\delta_0=3c_l^2 \kappa^3\kappa (\frac{1}{T})^{T}(1-\beta\mu)^P(L_{F_{x}}^2+\frac{L_{f_{xy}}^2C_{F_{y}}^2}{\mu^2}+\frac{4M_{f_{xy}}^2L_{F_y}^2}{\mu^2}+\frac{4M_{f_{xy}}^2C^2_{F_y}L^2_{f_{yy}}}{\mu^4})\|y_0^*-y_{0}\|^2$ is the initial error. Specifically, if $Q_k=k+1$, we have:
 \begin{equation}\label{agblpratekl}
      \frac{1}{K}\sum_{k=0}^{K-1}\|\nabla{\Phi}(x_k)\|^2\leq  \frac{4(\Phi(x_0)-{\inf_x \Phi(x)})}{\alpha K}+\frac{3\delta_0}{K(1-\tau)}+\frac{18nLM_{f_{xy}}^2 C_{F_y}^2{\rm ln}K}{\mu^3{\tilde{\xi}}K}.
 \end{equation}
 \end{theorem}}
{\color{black}
\begin{proof}
Substituting the inequality (\ref{equerror}) into (\ref{eqphi}) yields:
\begin{equation}\label{eqphi1g1}
\begin{split}
      \|\tilde{\nabla} \Phi(x_k)-\nabla\Phi(x_k)\|^2
    &\leq 3\big(L_{F_{x}}^2+\frac{L_{f_{xy}}^2C_{F_{y}}^2}{\mu^2}\big)\|y_{k,T}-y^*_k\|^2\\
    &+3M_{f_{xy}}^2\left(2\frac{nL{C^2_{F_y}}}{\tilde{\xi}\mu^3 Q_k}+4\big(\frac{L_{F_y}^2}{\mu^2}+\frac{C^2_{F_y}L^2_{f_{yy}}}{\mu^4}\big)\|y_k^*-y_{k,T}\|^2\right)\\
    &\leq \left(3L_{F_{x}}^2+\frac{3L_{f_{xy}}^2C^2_{F_{y}}}{\mu^2}+{12 M_{f_{xy}}^2}\big(\frac{L_{F_y}^2}{\mu^2}+\frac{C^2_{F_y}L^2_{f_{yy}}}{\mu^4}\big)\right)\|y_k^*-y_{k,T}\|^2\\
    &+6\frac{nLM_{f_{xy}}^2{C^2_{F_y}}}{\mu^3{\tilde{\xi}} Q_k}.\\
\end{split}
\end{equation}
 Then, based on Lemma \ref{lemylc}, substituting the above inequality into (\ref{eqylc}) yields:
\begin{equation}\label{eqyk1l}
    \begin{split}
        \|y_k^*-y_{k,T}\|^2&\leq (1+\varepsilon)c_l^2 \kappa^3 (\frac{1}{T})^{T}(1-\beta\mu)^P\|y_{k-1,T}-y_{k-1}^*\|^2+2(1+\frac{1}{\varepsilon})c_l^2 \kappa^3 (\frac{1}{T})^{T}(1-\beta\mu)^PL_y^2\alpha^2\|\nabla \Phi(x_{k-1})\|^2\\
    &+2(1+\frac{1}{\varepsilon})c_l^2 \kappa^3 (\frac{1}{T})^{T}(1-\beta\mu)^PL_y^2\alpha^2\|\tilde{\nabla} \Phi(x_{k-1})-\nabla\Phi(x_{k-1})\|^2\\
    &\leq \tau\|y_{k-1}^*-y_{k-1,T}\|^2+2(1+\frac{1}{\varepsilon})c_l^2 \kappa^3(\frac{1}{T})^{T}(1-\beta\mu)^PL_y^2\alpha^2\|\nabla \Phi(x_{k-1})\|^2\\
    &+12(1+\frac{1}{\varepsilon})c_l^2 \kappa^3 (\frac{1}{T})^{T}(1-\beta\mu)^PL_y^2\alpha^2\frac{nLM_{f_{xy}}^2{C^2_{F_y}}}{\mu^3{\tilde{\xi}} Q_{k-1}},\\
    \end{split}
\end{equation}
where $\tau=c_l^2 \kappa^3 (\frac{1}{T})^{T}(1-\beta\mu)^P\big((1+\varepsilon)+6(1+\frac{1}{\varepsilon})L_y^2\alpha^2(L_{F_{x}}^2+\frac{L_{f_{xy}}^2C^2_{F_{y}}}{\mu^2}+\frac{4M_{f_{xy}}^2L_{F_y}^2}{\mu^2}+\frac{4M_{f_{xy}}^2C^2_{F_y}L^2_{f_{yy}}}{\mu^4})\big)$.

Summing the inequality (\ref{eqyk1l}) from 0 to $k$ results in:
\begin{equation*}
    \begin{split}
        \|y_k^*-y_{k,T}\|^2&\leq \tau^k\|y_0^*-y_{0,T}\|^2+2(1+\frac{1}{\varepsilon})c_l^2 \kappa^3 (\frac{1}{T})^{T}(1-\beta\mu)^PL_y^2\alpha^2\sum_{j=0}^{k-1}{\tau^j \|\nabla \Phi(x_{k-1-j})\|^2}\\
        &+12(1+\frac{1}{\varepsilon})c_l^2 \kappa^3 (\frac{1}{T})^{T}(1-\beta\mu)^PL_y^2\alpha^2\frac{nLM_{f_{xy}}^2{C^2_{F_y}}}{\mu^3{\tilde{\xi}} }\sum_{j=0}^{k-1}{\tau^j \frac{1}{Q_{k-1-j}}}.
    \end{split}
\end{equation*}

 Combining the inequality (\ref{eqphi1g1}) and $\|y_0^*-y_{0,T}\|^2\leq c_l^2 \kappa^3(\frac{1}{T})^{T}(1-\beta\mu)^P\|y_0^*-y_{0}\|^2$, it follows that
 \begin{equation}\label{eqphidl}
     \begin{split}
\|\tilde{\nabla} \Phi(x_k)-\nabla\Phi(x_k)\|^2&\leq \delta_0 \tau^k+\omega \alpha^2\sum_{j=0}^{k-1}{\tau^j \|\nabla \Phi(x_{k-1-j})\|^2}\\
&+6\omega\alpha^2\frac{nLM_{f_{xy}}^2{C^2_{F_y}}}{\mu^3 {\tilde{\xi}}}\sum_{j=0}^{k-1}{\tau^j \frac{1}{Q_{k-1-j}}}+6\frac{nLM_{f_{xy}}^2{C^2_{F_y}}}{\mu^3{\tilde{\xi}} Q_k},
     \end{split}
 \end{equation}
where $\delta_0=3c_l^2 \kappa^3 (\frac{1}{T})^{T}(1-\beta\mu)^P(L_{F_{x}}^2+\frac{L_{f_{xy}}^2C_{F_{y}}^2}{\mu^2}+\frac{4M_{f_{xy}}^2L_{F_y}^2}{\mu^2}+\frac{4M_{f_{xy}}^2C^2_{F_y}L^2_{f_{yy}}}{\mu^4})\|y_0^*-y_{0}\|^2$ and $\omega=6(L_{F_{x}}^2+\frac{L_{f_{xy}}^2C_{F_{y}}^2}{\mu^2}+\frac{4M_{f_{xy}}^2L_{F_y}^2}{\mu^2}+\frac{4M_{f_{xy}}^2C^2_{F_y}L^2_{f_{yy}}}{\mu^4})(1+\frac{1}{\varepsilon})c_l^2 \kappa^3(\frac{1}{T})^{T}(1-\beta\mu)^PL_y^2$.

Since $\nabla\Phi(\cdot)$ is $L_{\Phi}-$Lipschitz, it can be obtained that
    \begin{equation}
    \begin{split}
        \Phi(x_{k+1}) &\leq \Phi(x_k) + \langle \nabla \Phi(x_k), x_{k+1} - x_k \rangle + \frac{L_{\Phi}}{2} \|x_{k+1} - x_k\|^2 \\
&\leq \Phi(x_k) - \alpha\langle \nabla \Phi(x_k), \tilde{\nabla} \Phi(x_k) - \nabla \Phi(x_k) \rangle - \alpha\| \nabla \Phi(x_k)\|^2 + \alpha^2 L_{\Phi} \|\nabla \Phi(x_k)\|^2 \\
&+ \alpha^2 L_{\Phi} \|\nabla \Phi(x_k) - \tilde{\nabla} \Phi(x_k)\|^2\\
&\leq \Phi(x_k) - \left( \frac{\alpha}{2} - \alpha^2 L_{\Phi} \right) \|\nabla \Phi(x_k)\|^2 + \left( \frac{\alpha}{2} + \alpha^2 L_{\Phi} \right) \|\nabla \Phi(x_k) - \tilde{\nabla} \Phi(x_k)\|^2.
    \end{split}
    \end{equation}

    Using the inequality (\ref{eqphidl}) yields:
    \begin{equation}\label{eqphl}
    \begin{split}
        \Phi(x_{k+1}) \leq & \Phi(x_k) - \left( \frac{\alpha}{2} - \alpha^2 L_{\Phi} \right) \|\nabla \Phi(x_{k})\|^2 + \left( \frac{\alpha}{2} + \alpha^2 L_{\Phi} \right) \|\nabla \Phi(x_k) - \tilde{\nabla} \Phi(x_{k})\|^2\\
        \leq& \Phi(x_k) - \left( \frac{\alpha}{2} - \alpha^2 L_{\Phi} \right) \|\nabla \Phi(x_k)\|^2 + \left( \frac{\alpha}{2} + \alpha^2 L_{\Phi} \right)\delta_0 \tau^k\\
        &+ \omega \alpha^2 \left( \frac{\alpha}{2} + \alpha^2 L_{\Phi} \right) \sum_{j=0}^{k-1} \tau^j \|\nabla \Phi(x_{k-1-j})\|^2 + \left( \frac{\alpha}{2} + \alpha^2 L_{\Phi} \right) \frac{6nLM_{f_{xy}}^2 C_{F_y}^2}{\mu^3{\tilde{\xi}} Q_k}\\
        &+ 6\omega\left( \frac{\alpha}{2} + \alpha^2 L_{\Phi} \right)\alpha^2 \frac{nLM_{f_{xy}}^2 C_{F_y}^2}{\mu^3{\tilde{\xi}}  }\sum_{j=0}^{k-1}{\tau^j \frac{1}{Q_{k-1-j}}}.
    \end{split}
\end{equation}

Finally, by telescoping the inequality (\ref{eqphl}) from $k=0$ to $k=K-1$, it is derived that
\begin{equation}
    \begin{split}
        \left( \frac{\alpha}{2} - \alpha^2 L_{\Phi} \right) \sum_{k=0}^{K-1}\|\nabla \Phi(x_{k})\|^2\leq& \Phi(x_0)-\Phi(x_K)+\left( \frac{\alpha}{2} + \alpha^2 L_{\Phi} \right)\frac{\delta_0}{1-\tau}\\
        &+\omega \alpha^2 \left( \frac{\alpha}{2} + \alpha^2 L_{\Phi} \right) \sum_{k=0}^{K-1}\sum_{j=0}^{k-1} \tau^j \|\nabla \Phi(x_{k-1-j})\|^2\\
        &+ \sum_{k=0}^{K-1}\left( \frac{\alpha}{2} + \alpha^2 L_{\Phi} \right) \frac{6nLM_{f_{xy}}^2 C_{F_y}^2}{\mu^3 {\tilde{\xi}}Q_k}\\
        &+6\omega \left( \frac{\alpha}{2} + \alpha^2 L_{\Phi} \right)\alpha^2 \frac{nLM_{f_{xy}}^2 C_{F_y}^2}{\mu^3{\tilde{\xi}} }\sum_{k=0}^{K-1}\sum_{j=0}^{k-1}{\tau^j \frac{1}{Q_{k-1-j}}}.
    \end{split}
\end{equation}

Moreover, due to $\sum_{k=0}^{K-1}\sum_{j=0}^{k-1}a_j b_{k-1-j}\leq \sum_{k=0}^{K-1}a_k\sum_{j=0}^{K-1}b_j$, we can deduce that
\begin{equation*}
\begin{split}
    &\sum_{k=0}^{K-1}\sum_{j=0}^{k-1} \tau^j \|\nabla \Phi(x_{k-1-j})\|^2\leq \sum_{k=0}^{K-1} \tau^k \sum_{k=0}^{K-1}\|\nabla\Phi(x_{k})\|^2\leq \frac{1}{1-\tau}\sum_{k=0}^{K-1}\|\nabla\Phi(x_{k})\|^2,\\
    &\sum_{k=0}^{K-1}\sum_{j=0}^{k-1} \tau^j \frac{1}{Q_{k-1-j}}\leq \sum_{k=0}^{K-1} \tau^k \sum_{k=0}^{K-1}\frac{1}{Q_{k}}\leq \frac{1}{1-\tau}\sum_{k=0}^{K-1}\frac{1}{Q_{k}}.
    \end{split}
\end{equation*}

Then, the following inequality holds:
\begin{equation}
    \begin{split}
&\left( \frac{1}{2} - \alpha L_{\Phi}-\omega \alpha^2 \left( \frac{1}{2} + \alpha L_{\Phi} \right)\frac{1}{1-\tau} \right)\frac{1} {K}\sum_{k=0}^{K-1}\|\nabla \Phi(x_{k})\|^2\\
\leq&\frac{\Phi(x_0)-\Phi(x_K)}{\alpha K}+\left( \frac{1}{2} + \alpha L_{\Phi} \right)\frac{\delta_0}{K(1-\tau)}\\
&+ \frac{1} {K}\sum_{k=0}^{K-1}\left( \frac{1}{2} + \alpha L_{\Phi} \right)\frac{6nLM_{f_{xy}}^2 C_{F_y}^2}{\mu^3{\tilde{\xi}} Q_k}+\frac{1} {K}\sum_{k=0}^{K-1}\frac{6\omega}{1-\tau} \left( \frac{1}{2} + \alpha L_{\Phi} \right)\alpha^2 \frac{nLM_{f_{xy}}^2 C_{F_y}^2}{\mu^3{\tilde{\xi}} Q_k}.
\end{split}
\end{equation}

If $\alpha L_{\Phi}+\omega \alpha^2 \left( \frac{1}{2} + \alpha L_{\Phi} \right)\frac{1}{1-\tau}\leq \frac{1}{4} $, then
\begin{equation}\label{corocitel}
    \frac{1} {K}\sum_{k=0}^{K-1}\|\nabla \Phi(x_{k})\|^2\leq \frac{4(\Phi(x_0)-\Phi(x^*))}{\alpha K}+\frac{3\delta_0}{K(1-\tau)}+\frac{1} {K}\sum_{k=0}^{K-1}\frac{18nLM_{f_{xy}}^2 C_{F_y}^2}{\mu^3{\tilde{\xi}} Q_k},
\end{equation}
where $\Phi(x^*)=\inf_x \Phi(x)$.

Finally, by substituting \(Q_k = k + 1\) into  (\ref{corocitel}), (\ref{agblpratekl}) is derived.
\end{proof}}




\section{Complexity and Theoretical discussion}\label{sec:complexity}
\begin{corollary}
   Consider $T = \Theta({\rm ln}\kappa)$ and $\alpha= \Theta (\kappa^{-3})$ such that $\tau<1$ and $\alpha L_{\Phi}+\omega \alpha^2 \left( \frac{1}{2} + \alpha L_{\Phi} \right)\frac{1}{1-\tau}\leq \frac{1}{4}$. Under the same setting of Theorem \ref{gblprate}, we have $\frac{1}{K}\sum_{k=0}^{K-1}\Vert\nabla \Phi(x_k)\Vert^2={\cal O}(\frac{\kappa^3}{K}+\frac{\kappa^3{\rm ln}K}{K})$. To achieve an $\epsilon$-stationary point, we require $K = \tilde{{\cal O}}(\kappa^3 \epsilon^{-1})$, resulting in the gradient complexity of  $Gc(f, \epsilon) = \tilde{{\cal O}}(\kappa^6 \epsilon^{-2}), Gc(F, \epsilon)=\tilde{{\cal O}}(\kappa^3 \epsilon^{-1})$ and a Jacobian-vector product complexity $JV(\epsilon) = \tilde{{\cal O}}(\kappa^3 \epsilon^{-1})$.
\end{corollary}
\begin{proof}
 For Theorem \ref{gblprate}, by Theorem \ref{thm36g}, we have 
 \begin{equation*}
  c_3=6L_y^2(L_{F_{x}}^2+\frac{L_{f_{xy}}^2C_{F_{y}}^2}{\mu^2}+\frac{4M_{f_{xy}}^2L_{F_y}^2}{\mu^2}+\frac{4M_{f_{xy}}^2C^2_{F_y}L^2_{f_{yy}}}{\mu^4}) = \Theta(\kappa^6).
 \end{equation*}
  Since $0<(1-\beta\mu)^P\leq 1$ and $\alpha= \Theta (\kappa^{-3}) $, it is derived that
  \begin{equation*}
 \begin{split}
     \tau&=\kappa (\frac{1}{T})^{T}(1-\beta\mu)^P\big((1+\varepsilon)+(1+\frac{1}{\varepsilon})\alpha^2c_3\big)=\Theta(\kappa(1/T)^T),\\
     \omega &=c_3(1+\frac{1}{\varepsilon})\kappa (\frac{1}{T})^{T}(1-\beta\mu)^P=\Theta(\kappa^7(1/T)^T).\\  
 \end{split}
 \end{equation*}
   Based on Lemma \ref{prelemma}, $\nabla \Phi$ is $L_\Phi$-Lipschitz with $L_\Phi= \Theta(\kappa^3)$. For a suitable choice of
$\alpha= \Theta (\kappa^{-3}) $, it follows that $\alpha L_\Phi<\frac{1}{8}$. Additionally,  with $T= \Theta(\ln \kappa)$, the conditions $0 < \tau \leq \frac{1}{2}$ and $\omega\alpha^2 = \Theta(\kappa(1/T)^T) \leq \frac{1}{10}$ are satisfied. 
 Consequently,
\[
\alpha L_{\Phi} + \omega\alpha^2 \left( \frac{1}{2} + \alpha L_{\Phi} \right) \frac{1}{1-\tau} \leq \frac{1}{8} + \frac{1}{10} \left( \frac{1}{2} + \frac{1}{8} \right) \frac{1}{1 - \frac{1}{2}} \leq \frac{1}{4}.
\]

Since $\alpha = \Theta(\kappa^{-3})$, it can be obtained from (\ref{agblpratek}) that $\frac{1}{K}\sum_{k=0}^{K-1}\Vert\nabla \Phi(x_k)\Vert^2={\cal O}(\frac{\kappa^3}{K}+\frac{{\kappa^3}{\rm ln}K}{K})$. Furthermore, in order to achieve an $\epsilon$-stationary point, we have $K ={\cal O}(\kappa^3 \epsilon^{-1}{\rm ln}\frac{\kappa^3} {\epsilon})=\tilde{{\cal O}} (\kappa^3 \epsilon^{-1})$. Therefore, the following complexity results are derived:
\begin{itemize}
    \item $Gc(f, \epsilon) =K(T+P)+\sum_{k=0}^{K-1} {Q_k}=K(T+P)+\frac{K(K+1)}{2}=\tilde{{\cal O}} (\kappa^6 \epsilon^{-2})$;
    \item $Gc(F, \epsilon)=2K= \tilde{{\cal O}}(\kappa^3 \epsilon^{-1})$;
    \item $JV(\epsilon) = K=\tilde{{\cal O}}(\kappa^3 \epsilon^{-1})$.
\end{itemize}

\textbf{Details for obtaining $\tau \leq 1/2$ and $\omega\alpha^2 \leq 1/10$:} Since $\tau = \Theta\left(\kappa(1/T)^T\right)$, $\omega\alpha^2 = \Theta\left(\kappa(1/T)^T\right)$ and $\kappa \geq 1$, it is enough to show that $C_0\kappa(1/T)^T \leq 1/10$ by choosing $T= \Theta(\ln \kappa)$. Here, $C_0 \geq 1$ is a positive constant in $\tau$ and $\omega\alpha^2$, depending explicitly on the Lipschitz constants in the assumptions.

By taking the logarithm on both sides of $C_0\kappa(1/T)^T \leq 1/10$, we get:
\[
\ln \kappa - T \ln T \leq -\ln(10C_0).
\]
This is equivalent to $T \ln T \geq \ln \kappa + \ln(10C_0)$. Therefore, choosing $T \geq \ln \kappa + \ln(10C_0) + \epsilon$ is sufficient, since $\ln T \geq 1$. Similarly, we can prove the result for Theorem \ref{qfblprate}.

\end{proof}




\end{document}
