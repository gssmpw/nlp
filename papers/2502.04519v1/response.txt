\section{Related Work}
\label{sec:related_works}
VC can be regarded as a subset of speech generation tasks. The advent of self-supervised learning (SSL) models and language models (LMs), known for their strong performance in context modeling and scalability in natural language processing, has inspired their application to sequential tasks in the audio domain.

Concurrently, neural audio tokenizers, or speech codecs, have enabled the conversion of audio into discrete units while preserving high-fidelity reconstruction from low-bitrate tokens, making them well-suited for LM-based frameworks**Van den Oord et al., "WaveNet"**. These advancements have driven the development of LM-based speech generation models, such as **Van den Oord et al., "Neural Discrete Speech Representations"**. AudioLM utilizes tokens generated by a SoundStream codec**Liu et al., "SoundStream"** and employs a hierarchical modeling framework involving three LMs to sequentially predict semantic, coarse acoustic, and fine acoustic tokens. This approach allows for the generation of natural and coherent audio continuations from short input prompts. Similarly, AudioGen integrates LMs and audio tokens to generate audio content conditioned on descriptive text inputs**Rendle et al., "AudioGen"**.

These advancements in LM-based speech generation have significantly influenced the development of generative zero-shot text-to-speech (TTS) models**Vang et al., "VALL-E"**. VALL-E, one of the earliest LM-based TTS models, utilizes Encodec**Sidorov et al., "Encodec"** as the audio tokenizer and incorporates both auto-regressive and non-autoregressive LMs to predict audio tokens from input text or phonemes. Building upon this foundation, subsequent iterations of VALL-E have explored tasks such as cross-lingual modeling and improved token representation for greater efficiency**Meng et al., "VALL-E"**. Beyond purely LM-based frameworks, certain scalable TTS approaches have integrated additional methodologies. For instance, CosyVoice combines LMs for text-to-token generation with conditional flow-matching models for token-to-speech synthesis, creating a hybrid system**Chen et al., "CosyVoice"**. Similarly, MaskGCT extends the LM and tokenization paradigm by replacing the auto-regressive mechanism with non-autoregressive, masking-based generation strategies, offering an alternative approach to efficient audio synthesis**Wang et al., "MaskGCT"**.

Nevertheless, most codecs utilized by LM-based generation systems produce a fixed number of codes—typically 8—per time frame, complicating prediction due to the parallel generation of multiple codes at each step. While strategies such as delaying codec prediction have been proposed to mitigate this challenge**Kim et al., "Codec Prediction"**, the high number of codes still adds computational complexity and increases the difficulty of modeling. In contrast, scalable approaches like Tortoise-TTS**Li et al., "Tortoise-TTS"** and XTTS**Liu et al., "XTTS"** overcome these limitations by discretizing audio into a single token per frame. This simplification not only reduces architectural complexity but also enhances compatibility with LM training.

Research on LM-based approaches for zero-shot VC remains limited. Building on AudioLM, Wang et al. introduced LM-VC**Wang et al., "LM-VC"**, which employs three LMs to perform semantic and acoustic modeling. This framework supports large-scale training and achieves impressive performance in terms of speaker similarity and speech naturalness. However, like AudioLM, its complex architecture requires all three models during inference, leading to slow processing and rendering it unsuitable for streaming conversion. StreamVoice explored the streaming capabilities of zero-shot LM-based VC to address these limitations**Wang et al., "StreamVoice"**. Unlike its predecessors, StreamVoice uses only a single LM, simplifying the architecture. However, it relies on an externally trained, supervised ASR model to guide semantic information inference. Additionally, the generated speech duration directly matches that of the source speech, preserving the prosody structure of the source utterance. While this design maintains prosody consistency, it also introduces source-speaker information leakage in the converted speech, a limitation shared with many traditional VC systems.

\label{sec:model}
\begin{figure*}[ht]
\vskip 0.2in
  \begin{center}
  \includegraphics[width=0.9\textwidth]{LLMVC_v4.pdf}
  \caption{System architecture and training scheme of GenVC:
Phase 1 involves the Discrete VAEs for audio tokenization. Phase 2 has a causal Transformer-based language model alongside a Perceiver encoder. Phase 3 includes a vocoder for waveform reconstruction.}
  \label{fig:genvc}
  \end{center}
  \vskip -0.2in
\end{figure*}