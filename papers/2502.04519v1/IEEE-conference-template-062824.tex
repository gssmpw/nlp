\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%Template version as of 6/27/2024

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs} % for professional tables
\usepackage{enumitem}
\usepackage[hidelinks]{hyperref}
\usepackage{multirow}
\usepackage{tablefootnote}
\usepackage{array} 
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{makecell}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage[capitalize,noabbrev]{cleveref}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{GenVC: Self-Supervised Zero-Shot \\ Voice Conversion}

\author{
    \IEEEauthorblockN{\textit{Zexin Cai, Henry Li Xinyuan, Ashi Garg, Leibny Paola Garc\'ia-Perera, Kevin Duh,}\\ %
        \textit{Sanjeev Khudanpur, Matthew Wiesner, Nicholas Andrews}
        \vspace{.7\baselineskip}}
        \IEEEauthorblockA{\textit{Human Language Technology Center of Excellence, Johns Hopkins University}
}}

\maketitle

\begin{abstract}
Zero-shot voice conversion has recently made substantial progress, but many models still depend on external supervised systems to disentangle speaker identity and linguistic content. Furthermore, current methods often use parallel conversion, where the converted speech inherits the source utterance's temporal structure, restricting speaker similarity and privacy.  To overcome these limitations, we introduce GenVC,\footnote{Audio samples are available at \url{https://caizexin.github.io/GenVC/index.html}. The code and model checkpoints will be released shortly.} a generative zero-shot voice conversion model. GenVC learns to disentangle linguistic content and speaker style in a self-supervised manner, eliminating the need for external models and enabling efficient training on large, unlabeled datasets. Experimental results show that GenVC achieves state-of-the-art speaker similarity while maintaining naturalness competitive with leading approaches. Its autoregressive generation also allows the converted speech to deviate from the source utterance’s temporal structure. This feature makes GenVC highly effective for voice anonymization, as it minimizes the preservation of source prosody and speaker characteristics, enhancing privacy protection.  
\end{abstract}

\begin{IEEEkeywords}
Voice Conversion, Large Language Model, Speech Anonymization, Speech Synthesis, Speech Generation
\end{IEEEkeywords}

\section{Introduction}
\label{intro}


Voice conversion (VC) aims to modify the vocal characteristics of a source signal while preserving its linguistic content.  This technology has numerous valuable applications, including assisting individuals with speech impairments, enhancing privacy through voice anonymization, and facilitating voice dubbing for films.  Progress in VC has closely mirrored advancements in text-to-speech synthesis, with recent methods achieving remarkable naturalness, producing converted voices perceptually indistinguishable from authentic human recordings~\cite{sisman2020overview}. Despite these advances, significant challenges remain, particularly in cloning novel voices and adapting to diverse recording conditions. These limitations primarily arise from the difficulty of training scalable and robust models capable of effectively handling such variability. Furthermore, leading VC approaches typically perform parallel conversion of linguistic features into corresponding acoustic features. Although these systems effectively modify acoustic characteristics such as timbre, this parallel conversion process often results in converted utterances that retain the temporal and prosodic structure of the source speaker~\cite{mary2006prosodic, li2024database}. This retained similarity can expose characteristics of the source speaker, diminishing the naturalness and potentially compromising the intended voice transformation~\cite{CAI2024privacy}. 

Early VC research primarily relied on parallel data, where models were trained using paired source and target speech recordings with identical linguistic content~\cite{toda2001voice}. While this simplifies the VC modeling process, collecting such data is often labor-intensive, costly, and impractical for large-scale recording efforts. These limitations also reduce the generalizability of VC models in challenging yet practical scenarios, such as multi-speaker or cross-lingual VC~\cite{zheng2016text}. In contrast, non-parallel VC approaches eliminate the need for paired utterances, greatly expanding VC's applicability to a wider range of scenarios, including zero-shot VC, which enables the cloning of an unseen speaker’s voice even if they are not included in the training set~\cite{qian2019autovc, zhang20e_interspeech, zhang2022sigvc}.

%Achieving effective disentanglement often requires intricate model architectures, specialized loss functions, and tailored training strategies~\cite{wang2024streamvoice}. 
In particular, zero-shot VC requires two components for speech disentanglement: one to capture the latent representation of a speaker’s voice and another to extract linguistic content. To this end, many studies utilize external models, such as automatic speech recognition (ASR), automatic speaker verification (ASV), or text-to-speech (TTS) to facilitate the disentanglement process~\cite{tan2021zeroshot, casanova2022yourtts}. These external models require labeled datasets for effective training to be useful for VC. For example, an ASV system should be trained on a speaker-rich corpus to accurately represent distinct voices~\cite{nagrani17_interspeech}. This raises a fundamental research question: \textbf{\textit{Can a VC system be trained without any supervision?}} In addition, previous zero-shot VC methods struggle to generalize to unseen speakers~\cite{li2023freevc, cao2024neuralvc}. A promising direction to address this limitation is to scale up data utilization to cover a more diverse range of voice types, thereby improving generalization~\cite{betker2023better}.

In this paper, we propose Generative Voice Conversion (GenVC), a novel approach for zero-shot VC that eliminates the need for external supervised models. Our method achieves state-of-the-art performance in speaker similarity and privacy preservation, while maintaining competitive naturalness compared to leading VC approaches. The key contributions of this work are as follows:

\begin{itemize}
    \item To the best of our knowledge, GenVC is the first zero-shot VC method that does not rely on external supervised models like ASR, ASV and TTS.
    \item Self-supervised disentanglement of speaker characteristics and linguistic content enables GenVC to be trained on more diverse datasets, leading to enhanced voice cloning performance.
    \item An autoregressive modeling approach allows zero-shot VC to overcome a common limitation in previous VC models, where the converted speech was constrained to match the duration of the source utterance.
\end{itemize}


\section{Related Work}
\label{sec:related_works}
VC can be regarded as a subset of speech generation tasks. The advent of self-supervised learning (SSL) models and language models (LMs), known for their strong performance in context modeling and scalability in natural language processing, has inspired their application to sequential tasks in the audio domain.

Concurrently, neural audio tokenizers, or speech codecs, have enabled the conversion of audio into discrete units while preserving high-fidelity reconstruction from low-bitrate tokens, making them well-suited for LM-based frameworks~\cite{wu2024towards}. These advancements have driven the development of LM-based speech generation models, such as AudioLM~\cite{borsos2023audiolm}. AudioLM utilizes tokens generated by a SoundStream codec~\cite{zeghidour2021soundstream} and employs a hierarchical modeling framework involving three LMs to sequentially predict semantic, coarse acoustic, and fine acoustic tokens. This approach allows for the generation of natural and coherent audio continuations from short input prompts. Similarly, AudioGen integrates LMs and audio tokens to generate audio content conditioned on descriptive text inputs~\cite{kreuk2023audiogen}.

These advancements in LM-based speech generation have significantly influenced the development of generative zero-shot text-to-speech (TTS) models~\cite{kharitonov2023speak, peng2024voicecraft, du2024cosyvoice2}. VALL-E~\cite{wang2023neural, microvalle}, one of the earliest LM-based TTS models, utilizes Encodec~\cite{fossez2023high} as the audio tokenizer and incorporates both auto-regressive and non-autoregressive LMs to predict audio tokens from input text or phonemes. Building upon this foundation, subsequent iterations of VALL-E have explored tasks such as cross-lingual modeling and improved token representation for greater efficiency~\cite{zhang2023speak, chen2024vall}. Beyond purely LM-based frameworks, certain scalable TTS approaches have integrated additional methodologies. For instance, CosyVoice combines LMs for text-to-token generation with conditional flow-matching models for token-to-speech synthesis, creating a hybrid system~\cite{du2024cosyvoice}. Similarly, MaskGCT extends the LM and tokenization paradigm by replacing the auto-regressive mechanism with non-autoregressive, masking-based generation strategies, offering an alternative approach to efficient audio synthesis~\cite{wang2024maskgct}.

Nevertheless, most codecs utilized by LM-based generation systems produce a fixed number of codes—typically 8—per time frame, complicating prediction due to the parallel generation of multiple codes at each step. While strategies such as delaying codec prediction have been proposed to mitigate this challenge~\cite{copet2023simple, defossez2024moshi}, the high number of codes still adds computational complexity and increases the difficulty of modeling. In contrast, scalable approaches like Tortoise-TTS~\cite{betker2023better} and XTTS~\cite{casanova2024xtts} overcome these limitations by discretizing audio into a single token per frame. This simplification not only reduces architectural complexity but also enhances compatibility with LM training.

Research on LM-based approaches for zero-shot VC remains limited. Building on AudioLM, Wang et al. introduced LM-VC~\cite{wang2023lm}, which employs three LMs to perform semantic and acoustic modeling. This framework supports large-scale training and achieves impressive performance in terms of speaker similarity and speech naturalness. However, like AudioLM, its complex architecture requires all three models during inference, leading to slow processing and rendering it unsuitable for streaming conversion. StreamVoice explored the streaming capabilities of zero-shot LM-based VC to address these limitations~\cite{wang2024streamvoice}. Unlike its predecessors, StreamVoice uses only a single LM, simplifying the architecture. However, it relies on an externally trained, supervised ASR model to guide semantic information inference. Additionally, the generated speech duration directly matches that of the source speech, preserving the prosody structure of the source utterance. While this design maintains prosody consistency, it also introduces source-speaker information leakage in the converted speech, a limitation shared with many traditional VC systems.

\label{sec:model}
\begin{figure*}[ht]
\vskip 0.2in
  \begin{center}
  \includegraphics[width=0.9\textwidth]{LLMVC_v4.pdf}
  \caption{System architecture and training scheme of GenVC:
Phase 1 involves the Discrete VAEs for audio tokenization. Phase 2 has a causal Transformer-based language model alongside a Perceiver encoder. Phase 3 includes a vocoder for waveform reconstruction.}
  \label{fig:genvc}
  \end{center}
  \vskip -0.2in
\end{figure*}

\section{GenVC}
Inspired by prior research in speech generation, we present GenVC, a zero-shot generative VC model. Our approach integrates discrete audio tokenizers with a generative Transformer-based LM for VC, simplifying the complexity of earlier LM-based VC methods. This design enables the model to be trained entirely in an self-supervised manner, supporting large-scale training and maximizing the utilization of available data. Furthermore, the autoregressive nature of the model allows for the conversion of source utterances into the voice and style of a target speaker without preserving the prosodic structure of the source utterances. This capability ensures that our model produces natural-sounding utterances comparable to leading zero-shot VC models while also making it suitable for privacy-sensitive applications.

\subsection{Overview}
Figure \ref{fig:genvc} provides an overview of the proposed generative VC system. At the core of this approach is a causal Transformer-based LM that predicts a sequence of acoustic tokens conditioned on both a fixed-length voice style representation and a sequence of linguistic tokens. Two auxiliary modules, DVAEs and vocoder, manage the compression, discretization, and reconstruction of audio signals. The VC process unfolds in three sequential phases, each building upon the previous one and optimized through reconstruction losses, eliminating the need for labeled data or external supervised models:

\begin{enumerate}[label=\Roman{*}.]
    \item \textbf{Tokenization}: Modules trained to convert audio signals into discrete tokens, ensuring compatibility with the LM architecture.
    \item \textbf{Generative Modeling}: An decoder-only LM that autoregressively generates audio tokens conditioned on acoustic style representations and linguistic tokens.
    \item \textbf{Vocoding}: A vocoder that reconstructs high-fidelity audio signals from the output of the LM.
\end{enumerate}

The subsequent sections provide detailed descriptions of the models and methodologies employed in each phase.

\subsection*{Phase 1: Tokenization}
\label{sec:tokenize}
Tokenization is a crucial step in LMs for breaking down inputs into smaller, manageable units. In the context of VC, the objective is to transform the speaker’s timbre (style) of a source utterance while preserving its original linguistic content. To achieve this, tokenization is employed to extract, compress, and normalize features from audio signals for effective model training. In our VC approach, two distinct types of tokens are used: 1. Phonetic Tokens: Capturing the linguistic content of the audio, representing the “what” is being said. 2. Acoustic Tokens: Encoding the acoustic properties of the audio, including timbre, prosody, environmental background, and other stylistic elements.

These tokens are derived from different audio representations. Studies have shown that self-supervised learning (SSL) speech models can consistently and significantly produce representations enriched with phonetic information~\cite{choi24b_interspeech}. In the context of VC, SSL models serve as ideal feature extractors for capturing the phonetic units of a given speech signal~\cite{lin21b_interspeech, huang2022s3prl}. For a given input utterance $x \in \mathbb{R}^T$, the phonetic embedding sequence extracted using a pre-trained SSL model is denoted as $\mathbf{O} \in \mathbb{R}^{T_o \times d_o}$, where $T_o$ represents the sequence length, and $d_o$ is the feature dimensionality. For acoustic representations, spectrograms are commonly used to capture the complex properties of audio signals. Such feature can be extracted and represented as $\mathbf{A} \in \mathbb{R}^{T_a \times d_a}$, where $T_a$ denotes the sequence length, and $d_a$ specifies the dimensionality of the acoustic features.

As shown in Figure \ref{fig:genvc}, our approach utilizes discrete variational autoencoders (DVAEs)~\cite{van2017neural} to compress $\mathbf{O}$ into phonetic tokens $[\mathbf{o}_1, \dots, \mathbf{o}_n], \mathbf{o}_i \in 1,2,\dots,K_p$ and $\mathbf{A}$ into acoustic tokens $[\mathbf{a}_1, \dots, \mathbf{a}_m], \mathbf{a}_i \in 1,2,\dots,K_a$. Here, $n$ and $m$ denote the lengths of the tokenized sequences derived from the input feature sequences, while $K_o$ and $K_a$ represent the predefined number of discrete codes for the phonetic and acoustic tokenizers, respectively. The DVAE architecture follows those used in Tortoise-TTS~\cite{betker2023better} and XTTS~\cite{casanova2024xtts} and is detailed in Appendix \ref{apx:mdlarc}.


\subsection*{Phase 2: Generative Modeling}
The backbone model in Phase 2 is an LM built on a decoder-only Transformer architecture~\cite{vaswani2017attention}. This phase also incorporates a Perceiver encoder to extract fixed-length style representations from the prompt utterance~\cite{alayrac2022flamingo}. As depicted in Figure \ref{fig:genvc}, the causal language model predicts acoustic tokens by conditioning on both the style prompt derived from the Perceiver encoder and the phonetic tokens extracted from the source audio clip. 

The Perceiver encoder processes a variable-length sequence of acoustic features extracted from the audio prompt and generates a fixed-dimensional audio style representation. This is achieved by using a set of learned latent vectors as queries, while the keys and values are formed by concatenating these latent vectors with the acoustic features extracted from the prompt audio. The Perceiver encoder architecture consists of cross-attention blocks, detailed in Appendix \ref{apx:mdlarc}. The output of the Perceiver encoder $\mathbf{E}_{\text{style}} \in \mathbb{R}^{T_s \times d_\text{model}}$ follows the distribution $P(\mathbf{E}_{\text{style}} \mid \mathbf{A}_\text{prompt}, \mathbf{E}_{\text{latent}}; \theta_\text{Perceiver})$ where $\mathbf{E}_{\text{latent}}$ is the learnable latents that have the same shape as $\mathbf{E}_{\text{style}}$. $T_s$ denotes the fixed number of latent sequence, and $d_\text{model}$ is the dimensionality of the LM. 

In this framework, the model learns the probability distribution $P(\mathbf{o}_{s:e}, \mathbf{a}_{s:e}, \mid \mathbf{E}_{\text{style}};\theta_\text{LM})$ by sequentially estimating the conditional distributions:

\begin{equation}
\label{eq:ploss}
\prod_{t=1}^nP(\mathbf{o}_i \mid \mathbf{o}_{s:i-1},  \mathbf{E}_{\text{style}}; \theta_\text{LM})
\end{equation}
and
\begin{equation}
\label{eq:aloss}
\prod_{j=1}^mP(\mathbf{a}_j \mid \mathbf{a}_{s:j-1},  \mathbf{o}_{s:e}, \mathbf{E}_{\text{style}}; \theta_\text{LM}).
\end{equation}

Here, the tokens $\mathbf{o}_s$ and $\mathbf{o}_e$ denote the start and end tokens of the phonetic token sequence, respectively, while $\mathbf{a}_s$ and $\mathbf{a}_e$ mark the start and end of the acoustic token sequence.

The training objective employs two linear prediction heads attached to the final hidden layer of the language model to predict phonetic and acoustic tokens. The overall training loss is designed to maximize the log-likelihood of $P(\mathbf{o}_{s:e}, \mathbf{a}_{s:e}, \mid \mathbf{E}_{\text{style}};\theta_\text{LM})$, as defined in Equation \ref{eq:genloss}. This loss combines the phonetic token classification loss and the acoustic token classification loss, weighted to reflect their respective contributions to the task:
 
\begin{equation}
    \label{eq:genloss}
    L_{\text{gen}} = \alpha L_{\text{phonetic}} + \beta L_{\text{acoustic}}
\end{equation}

Here, $\alpha$ and $\beta$ denote the weights assigned to the phonetic and acoustic token prediction losses, respectively. The phonetic token loss, $L_{\text{phonetic}}$ is calculated as the negative log-likelihood of the distribution defined in Expression \ref{eq:ploss}, while the acoustic token loss, $L_{\text{acoustic}}$ is the negative log-likelihood of the distribution defined in Expression \ref{eq:aloss}. Since the primary task of the VC system aligns with that of TTS systems—predicting acoustic tokens—we follow the approach outlined in ~\cite{casanova2024xtts} and assign $\beta$ a significantly larger value than $\alpha$ ($\beta \gg \alpha$).

\subsection*{Phase 3: Vocoding}
Phase 3 focuses on reconstructing the audio waveform from the generative predictions. Since Phase 2 outputs an acoustic token sequence where each token $\mathbf{a}_i$ is highly compressed due to vector quantization, directly reconstructing audio from these acoustic codes often introduces pronunciation issues and artifacts~\cite{casanova2024xtts}. To mitigate these issues, we utilize a HiFiGAN~\cite{NEURIPS2020_c5d73680} vocoder conditioned on features $\mathbf{H} \in \mathbb{R}^{m \times d_\text{model}}$ derived from the final hidden layer of the LM. Unlike systems such as XTTS, which rely on additional speaker embeddings obtained from a separate neural module, we find that conditioning the HiFiGAN vocoder solely on the LM’s final hidden features is sufficient for producing high-quality audio reconstructions.

\subsection{Training and Inference}
During training, as shown in Figure \ref{fig:genvc}, both the audio prompt and the audio clip are randomly segmented from the same source utterance. Typically, the linguistic content of the audio prompt differs from that of the audio clip. This setup allows the Perceiver encoder to capture acoustic attributes that are not represented by the linguistic tokens. Additionally, it facilitates the disentanglement of linguistic content from speaker-specific information in the input audio clip. As a result, the encoder effectively learns to extract both acoustic environment characteristics and speaker-specific representations from the audio prompt. Notably, this approach eliminates the need for supervision from an external speaker embedding model, which is a common feature of traditional zero-shot VC methods. Meanwhile, phonetic and acoustic tokens are derived from the audio clip, and the input sequence to the LM is structured as follows:

\[
\scriptsize [<\text{style embeddings}><\text{phonetic tokens}><\text{acoustic tokens}>]
\]

During inference, a significant portion of the model can be omitted, including the decoder of the phonetic tokenizer, the entire acoustic tokenizer, and the discriminator. The inference process, given a source utterance $\textbf{Audio}_{\text{src}}$ and a target utterance $\textbf{Audio}_{\text{tgt}}$ specifying the target voice, proceeds as follows:

\begin{itemize}
    \item \textbf{Style Embedding Extraction}: Treat $\textbf{Audio}_{\text{tgt}}$ as the audio prompt. Acoustic features are extracted from the target audio and processed using the Perceiver encoder to obtain the corresponding conditioned style embeddings.
    \item \textbf{Phonetic Token Extraction}: Phonetic features are extracted from $\textbf{Audio}_{\text{src}}$ using the SSL model. These features are then converted into phonetic tokens using the phonetic tokenizer.
    \item \textbf{Acoustic Token Prediction}: The style embeddings and phonetic tokens are fed as input to the backbone LM, which autoregressively predicts the acoustic token sequence until the end token $\mathbf{a}_e$ is generated.
    \item \textbf{Waveform Reconstruction}: The latent representations from the final hidden layer of the LM are passed to the HiFiGAN vocoder to reconstruct the converted audio waveform.
\end{itemize}



\section{Experiments}\label{sec:experiments}
\label{sec:exp}
We conduct a series of experiments to evaluate the effectiveness of our proposed VC approach. Using English speech data, we examine two configurations of the GenVC system: one trained on a smaller dataset and the other on a large-scale dataset. Both subjective and objective evaluations are performed to assess the quality of the converted speech, benchmarking our systems against several state-of-the-art models. Furthermore, we analyze the privacy-preserving capabilities of the proposed systems by measuring the extent to which the original speaker’s style is retained or leaked in the converted speech.

\subsection{Dataset}
Three datasets are used as the training corpus, with their statistics summarized in Table \ref{tab:dataset}. For the LibriTTS dataset~\cite{zen2019libritts}, we include utterances longer than 4 seconds, resulting in a total of 452.5 hours of speech. For large-scale training, we incorporate English utterances exceeding 6 seconds in duration from the CommonVoice~\cite{ardila2019common} and Multilingual LibriSpeech (MLS)~\cite{pratap20_interspeech} datasets. The CommonVoice dataset contributes approximately 1,645 hours of recordings with a diverse range of speakers, while the MLS dataset adds around 44,627 hours of read audiobook speech sourced from LibriVox.

\begin{table}[h]
  % \captionsetup{justification=centering}
  \scriptsize
  % \setlength\extrarowheight{1pt}
  \caption{Details of the training datasets.}
  \label{tab:dataset}
  \vskip 0.15in
  \begin{center}
  \begin{sc}
  \begin{tabular}[c]{cccccc}
    \toprule
    \textbf{Name}  & \textbf{Dur. (hrs)} & \textbf{\#spk} & \textbf{Avg. length (secs)}  \\
    \midrule
    LibriTTS & 452.5 & 2,278 & 8.59  \\
    CommonVoice-EN  & 1,644.93 & 75,970 &  7.5  \\
    MLS-EN & 44,626.54 & 5,487 & 14.88 \\ 
    \bottomrule
  \end{tabular}
  \end{sc}
  \end{center}
  \vskip -0.1in
\end{table}

For evaluation, we used a range of test sets commonly used in VC and anonymization studies. Specifically, the CMU Arctic~\cite{kominek2003cmu} and Emime~\cite{wester2011emime} datasets are employed to construct source-target conversion pairs, enabling the assessment of voice conversion performance. Furthermore, in accordance with the speech anonymization pipeline defined by the VoicePrivacy Challenge 2024 (VPC2024)~\cite{tomashenko2024voiceprivacy}, we use the Librispeech dataset~\cite{librispeech} to evaluate linguistic content preservation and privacy protection. In our setup, the pseudo-anonymized voices are sourced from the VoxCeleb1 dataset~\cite{nagrani17_interspeech}.

\subsection{Training and Inference Details}
%\paragraph{DVAE}
%\subsubsection*{Phase 1} 
\noindent \textbf{Phase 1:} As described in Section~\ref{sec:tokenize}, two DVAE models are trained to tokenize audio into discrete representations. The training data consists of randomly segmented audio clips with a maximum duration of 6 seconds from the LibriTTS dataset. Both models employ a codebook size of 512 and are trained using the Adam optimizer with a learning rate of $1 \times 10^{-4}$. Training is conducted for 200 epochs on an NVIDIA A100 GPU. Each DVAE model contains approximately 52 million parameters.

For phonetic tokens, we use ContentVec~\cite{qian2022contentvec} as the SSL feature extractor. Since ContentVec processes audio signals sampled at 16 kHz, the input audio clips are downsampled accordingly. Features are extracted at a frame rate of 20 ms, and the DVAE further compresses the input sequence length by a factor of 4. Consequently, the phonetic tokens generated by the phonetic DVAE have a token rate of 12.5 Hz. In our experiments, the size of the discrete codebook ($K_o$) for the phonetic DVAE is set to 256. 

For audio tokens, we use Mel-spectrograms as the acoustic features. These are extracted from input audio sampled at 24 kHz, with a window size of 1024 samples and a hop size of 256 samples. The Mel-spectrogram uses 80 Mel bins. This configuration generates acoustic tokens from the acoustic DVAE at an approximate token rate of 23.44 Hz. The size of the discrete codebook ($K_a$) for the acoustic DVAE is set to 1024.

%\paragraph{Perceiver Encoder and LLM} 
\vskip 0.1in
\noindent \textbf{Phase 2:} The Perceiver encoder is configured with 32 latent queries. We use GPT-2~\cite{radford2019language} as our LM. For the GPT-2 model, the model dimension is set to 1024, with 30 layers. For the input audio prompts, the segments are randomly chosen within a duration range of 3 to 6 seconds. The audio clips used for phonetic token extraction and acoustic token prediction are also randomly cut from the source utterances, with durations ranging from 1.2 to 8 seconds.

We use the Adam optimizer with an initial learning rate of $1 \times 10^{-4}$, and employ a multistep learning rate scheduler, reducing the learning rate by a factor of 0.5 every 5 epochs during training. For loss calculation, the weights $\alpha$ and $\beta$ in Equation \ref{eq:genloss} are set to 0.01 and 1, respectively. The LM consists of 423.64 million parameters, and the Perceiver encoder contains 8.48 million parameters.

For causal LM inference, the temperature is set to 0.85, with a length penalty of 1.0 and a repetition penalty of 2.0. The top-\textit{k} and top-\textit{p} sampling parameters are set to 15 and 0.85, respectively.

%\paragraph{HiFiGAN}
\vskip 0.1in
\noindent \textbf{Phase 3:} The vocoder is trained on audio chunks with a fixed duration of 0.64 seconds, producing audio at a sample rate of 24 kHz. As the acoustic DVAE downsamples the sequence, reducing the original acoustic feature length by a factor of 4, the features from the final hidden layer of the LM are upsampled and interpolated by a scale factor of 4 during both training and inference. These interpolated features are then used as the input for vocoder training. 

The HiFiGAN vocoder’s generator and discriminator are optimized using the AdamW algorithm with a learning rate of $2 \times 10^{-4}$. The HiFiGAN generator contains 3.16M parameters. Additionally, four types of discriminators are employed for vocoder training: multi-scale discriminator, multi-period discriminator, multi-scale STFT discriminator, and multi-scale sub-band constant-Q transform discriminator~\cite{gumultisubband}.
\vskip 0.1in
\noindent \textbf{Models} We trained two models, GenVC-Small and GenVC-Large, for this study. Both models use the same audio tokenizers trained on the LibriTTS dataset but differ primarily in the training data used for Phase 2 and Phase 3. 

\textbf{GenVC-Small} is trained on the LibriTTS dataset, which comprises approximately 450 hours of audio. Phase 2 training was conducted with a batch size of 32 for 590k steps, followed by Phase 3 vocoder training with a batch size of 64 for about 1M steps. 

\textbf{GenVC-Large} is fine-tuned initialized from GenVC-Small using additional data from the CommonVoice-EN and MLS-EN datasets, totaling approximately 45k hours of audio. This training was performed with the Perceiver encoder kept frozen. The language model was fine-tuned with a batch size of 24 for 1.5M steps, while the HiFiGAN vocoder was trained with a batch size of 128 for 220k steps.

\section{Results}\label{sec:results}
\subsection{Conversion Performance}
We evaluate the performance of our proposed systems using conversion pairs constructed from the test set. For comparison, we include three previously established state-of-the-art zero-shot systems—YourTTS, FreeVC, and Neural VC—since their code and model checkpoints are publicly available for inference. Note that these baseline systems were trained on the VCTK dataset and rely on an external ASV system to render speaker identity.

\subsubsection{Objective Evaluation} We use open-source systems to objectively evaluate the speaker similarity and naturalness of our proposed system in comparison to the baseline systems. We randomly construct 2,000 conversion pairs from the EMIME dataset, which serves as a test set with unseen speakers for all VC models. Speaker similarity is measured using a WavLM-based speaker verification system~\cite{chen2022wavlm}.\footnote{\url{https://huggingface.co/microsoft/wavlm-base-sv}} For each pair, we compute the cosine similarity between the embeddings of the target speaker’s utterance and its corresponding converted utterance. The final score is reported as the average cosine similarity across all test pairs. Naturalness is assessed using UTMOS$_\text{v2}$~\cite{baba2024utmosv2},\footnote{\url{https://github.com/sarulab-speech/UTMOSv2}} which predicts mean opinion scores (MOS) for the converted speech. Higher values indicate better performance for both metrics.

The results are presented in Table \ref{table:obj_eva}. YourTTS achieves the highest speaker similarity (0.885), closely followed by Neural VC (0.881) and FreeVC (0.878). GenVC-Large (0.884) surpasses FreeVC and Neural VC, while GenVC-Small (0.869) remains competitive. Overall, similarity scores are closely aligned. For naturalness, FreeVC leads (UTMOS 2.81), with GenVC-Small (2.66) and GenVC-Large (2.65) slightly outperforming Neural VC (2.60) and significantly exceeding YourTTS. These results highlight GenVC’s balanced performance, achieving competitive speaker similarity and naturalness against leading zero-shot VC systems.

\begin{table}[!h]
    \footnotesize
    % \setlength\extrarowheight{1.4pt}
    \caption{Objective evaluation results. UTMOS are reported with a 95\% confidence interval.}
    \label{table:obj_eva}
      \vskip 0.15in
    \begin{center}
    \begin{sc}
    \begin{tabular}[c]{@{\ \ \ }c@{\ \ \ \ \ \ }cc@{\ \ \ }}
        \toprule
        \textbf{System}  & \textbf{SIM} $\uparrow$ & \textbf{UTMOS} $\uparrow$\\
        \midrule
        YourTTS~\cite{casanova2022yourtts} & \textbf{0.885} & 2.43 $\pm$ 0.018 \\
        FreeVC~\cite{li2023freevc} & 0.878 & \textbf{2.81 $\pm$ 0.014} \\
        Neural VC~\cite{cao2024neuralvc} & 0.881 & 2.60 $\pm$ 0.015\\
        \midrule
        GenVC-Small & 0.869 & 2.66 $\pm$ 0.021\\ 
        GenVC-Large &0.884 & 2.65 $\pm$ 0.019 \\ 
        \bottomrule
    \end{tabular}
    \end{sc}
    \end{center}
    \vskip -0.1in
\end{table}

\subsubsection{Subjective Evaluation} For the subjective evaluation test, we randomly selected 50 pairs of utterances for evaluation. 11 participants were recruited to conduct the listening test, where they rated the naturalness and similarity of both genuine utterances and voice converted utterances. Each participant rated a total of 330 utterances: 250 utterances from five VC systems, 80 genuine audio samples. The stimuli were presented in random order, and detailed instructions are provided in Appendix \ref{apx:subeva}. 

The naturalness mean opinion score (NMOS) and similarity mean opinion score (SMOS) results are summarized in Table \ref{table:sub_eva}. According to the results, FreeVC achieves the highest NMOS of 3.92, indicating the best perceived naturalness among the evaluated VC systems. In comparison, GenVC-Small and GenVC-Large achieve lower NMOS scores of 3.46 and 3.32, respectively. However, it is important to note that the quality of training data plays a crucial role in the perceived naturalness. The baseline systems are trained on VCTK, which yields higher NMOS scores compared to the training sets used for GenVC, suggesting that differences in dataset quality may have influenced the results. 

\begin{table}[!h]
    % \scriptsize
    \footnotesize
    % \setlength\extrarowheight{1.4pt}
    \caption{Subjective evaluation results on unseen voice conversion. All MOS are reported with a 95\% confidence interval.}
    \label{table:sub_eva}
    \vskip 0.15in
    \begin{center}
    \begin{sc}
    \begin{tabular}[c]{@{\ \ \ }c@{\ \ \ \ \ \ }cc@{\ \ \ }}
        \toprule
        \textbf{System}  & \textbf{NMOS} $\uparrow$ & \textbf{SMOS} $\uparrow$ \\
        \midrule
        VCTK & 4.19 $\pm$ 0.13 & - \\
        Train-Small & 4.14 $\pm$ 0.15 & - \\
        Train-Large & 4.02 $\pm$ 0.09 & - \\
        \midrule
        YourTTS & 2.87 $\pm$ 0.09 & 2.87 $\pm$ 0.08 \\
        FreeVC & \textbf{3.92 $\pm$ 0.08} & 3.48 $\pm$ 0.07 \\
        Neural VC &3.86 $\pm$ 0.08 & 3.38 $\pm$ 0.07 \\
        \midrule
        GenVC-Small & 3.46 $\pm$ 0.09 & \textbf{3.68 $\pm$ 0.08}\\ 
        GenVC-Large & 3.32 $\pm$ 0.09 & 3.49 $\pm$ 0.08 \\ 
        \bottomrule
    \end{tabular}
    \end{sc}
    \end{center}
    \vskip -0.1in
\end{table}

Our proposed VC approach demonstrates strong performance in speaker similarity. GenVC-Small achieves the highest SMOS of 3.68, surpassing all other systems, including the best-performing baseline, FreeVC, which scores 3.48. However, GenVC-Large does not show further improvement in similarity despite being trained on a larger dataset, though it still outperforms the baseline systems. This may be partially attributed to the quality of the additional training data. Overall, the results suggest that the GenVC models achieve state-of-the-art speaker similarity while maintaining competitive naturalness. Nevertheless, there exists a trade-off: incorporating larger but lower-quality training data can degrade conversion quality.


\subsection{Anonymization Performance}
Although VC can effectively anonymize voices in terms of timbre, many VC-based approaches reveal source speaker information through the prosodic structure of the speech, thereby limiting privacy preservation. Here we adopt the anonymization and evaluation pipeline from the VoicePrivacy Challenge 2024~\cite{tomashenko2024voiceprivacy} to assess our models’ anonymization performance in comparison with other VC-based approaches. 

The privacy evaluation pipeline follows a standard speaker verification process. In this setup, the verification model is trained on anonymized data, with labels corresponding to the original speaker identities. A successful anonymization system should sufficiently distort and obscure the original speaker’s identity, making it difficult for the verification system to accurately identify the source speaker. During evaluation, pairs of source speech, referred to as trials, from the evaluation dataset are anonymized and treated as both enrollment and test speech. The primary metric for privacy evaluation is the equal error rate (EER), calculated based on similarity scores from these pairs. A lower EER indicates a higher risk of speaker re-identification, while a higher EER reflects better performance in preserving voice privacy. In addition, utility evaluation is performed to measure the preservation of linguistic content. The anonymized utterances are transcribed using a speech recognition system, and the resulting transcripts are compared with the ground-truth content from the source data. The word error rate (WER) is used as the metric, with a lower WER indicating better preservation of the linguistic content.

For this experiment, the verification models are trained on converted utterances from the train-clean-360 set of Librispeech and tested on the development and test sets of Librispeech. Target voices are randomly selected from VoxCeleb1. %In addition to the baseline state-of-the-art zero-shot VC systems, we also include B1B, the baseline system from VPC2024, for comparison.

As shown in Table \ref{tab:anon}, other VC-based anonymization approaches present risks of revealing the source speaker’s identity, as their average EERs are close to or below 12\%. YourTTS, FreeVC, and Neural VC demonstrate similarly limited privacy preservation in this context. In contrast, the GenVC models show significant improvements over the other systems. Specifically, the GenVC-Small model achieves an EER of 28.84\%, while the GenVC-Large model achieves an EER of 26.88\%, outperforming the best VC-based approach (YourTTS) by over 15\%.


\begin{table}[th]
  \caption{Anonymization performance of VC-based approaches}
  \label{tab:anon}
  % \scriptsize
  \footnotesize
  %\renewcommand{\arraystretch}{1.1}
  \vskip 0.15in
  \begin{center}
  \begin{sc}
  %\setlength{\tabcolsep}{3pt}
  \begin{tabular}{ll cc}
    \toprule
    & \textbf{System} & \makecell{\textbf{Privacy} \\ EER(\%) $\uparrow$ } & \makecell{\textbf{Utility} \\  WER(\%) $\downarrow$ } \\ 
    \midrule
    & YourTTS &11.8 & 6.09 \\
    & FreeVC & 9.51 & \textbf{2.7} \\
    & Neural VC & 10.12 & 3.53 \\
    \midrule
    & GenVC-Small & \textbf{28.84} & 6.87 \\
    % & GenVC-Small & 29.12 & 3.55 \\
    & GenVC-Large & 26.88 & 6.72 \\
    \midrule
  \end{tabular}
    \end{sc}
  \end{center}
  \vskip -0.1in
\end{table}

We note that while Table \ref{tab:anon} suggests that the improved privacy preservation in the GenVC model may have resulted in part from poorer content preservation, this is likely an artifact of our training procedure, which used significantly shorter speech chunks during training. We experimented with training GenVC-Small using full contexts for SSL feature extraction, which led to improved privacy preservation without degradation in content preservation (3.55\% WER). However, this approach is less efficient for large-scale training and requires substantial storage for offline feature extraction. In this paper, we report on models that are sufficiently effective for large-scale on-the-fly training, but we plan to further improve their performance to align with the offline feature extraction approach in future work.


\section{Discussions}
\label{sec:dis}
Our work has several limitations, primarily due to computational constraints. For instance, we did not explore the impact of model size on performance, despite scaling laws playing a critical role in optimizing LMs for large-scale training~\cite{kaplan2020scaling}. Additionally, we did not investigate the influence of acoustic and phonetic token configurations or evaluate alternative SSL models, which could offer valuable insights compared to the ContentVec model used in our experiments.

A significant challenge in zero-shot VC remains the accurate preservation of speaker timbre—especially for unseen voices—alongside other speaker-specific characteristics such as accent, emotion, prosody, and recording environment~\cite{li2023freevc, wang2023lm}. These factors are crucial for both identifying target speakers and anonymizing source voices. Addressing this challenge requires the availability of more diverse training data, including recordings from varied acoustic environments~\cite{microvalle}. While such data may not always match the quality of studio-recorded utterances, it could significantly improve the robustness of VC models in real-world applications. We believe that self-supervised learning approaches, such as GenVC, hold promise for enabling scalable training and improving generalization to unseen speakers and conditions.

Future work will focus on addressing several limitations and improving the model’s robustness. Specifically, we aim to optimize GenVC’s streaming capabilities and enhance its robustness for multilingual and cross-lingual VC. Expanding the model’s applicability across diverse linguistic and acoustic environments is an essential step toward developing more practical, adaptable, and inclusive VC systems.

\section{Conclusions}
\label{sec:conclue}
This paper introduces GenVC, a self-supervised zero-shot voice conversion system designed to leverage in-the-wild data for training. Compared to other LM-based approaches, it features a simpler structure and eliminates the need for external supervised models. It effectively learns the disentanglement of linguistic content from speaker-specific attributes without relying on supervised models. Our experimental results show that GenVC delivers outstanding voice cloning capability while maintaining naturalness competitive with state-of-the-art zero-shot voice conversion systems. Moreover, GenVC effectively modifies the prosodic characteristics of the source speech, significantly enhancing privacy preservation in anonymization applications.

\newpage
\section*{Acknowledgment}
This work was supported by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via the ARTS Program under contract D2023-2308110001. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.

\section*{Impact Statement}
This paper presents work aimed at advancing the field of Voice Conversion. While our work holds significant promise, it also carries potential societal implications that warrant consideration. GenVC is a powerful voice conversion system capable of transforming source speech into desired voices. While this technology has valuable applications, such as enhancing privacy by anonymizing voices and enabling accessibility for individuals with speech impairments, it is also presents ethical challenges. Specifically, the ability to convincingly replicate voices can be misused to create audio deepfakes, which may be employed for malicious purposes, such as identity theft, fraud, and the spread of misinformation.

To mitigate these risks, we strongly advocate for the responsible and ethical use of voice conversion technologies. Researchers, developers, and users must comply with relevant laws and guidelines, ensuring that these systems are used exclusively for legitimate and beneficial applications. Transparency, informed consent, and robust safeguards should be prioritized to prevent misuse and protect individuals’ rights and privacy.

Furthermore, we emphasize the importance of continued research into developing countermeasures, such as deepfake detection tools, and fostering collaboration between researchers, policymakers, and industry stakeholders to address the ethical implications of voice conversion technology. By proactively addressing these concerns, we can maximize the benefits of systems like GenVC while minimizing their potential for harm. To support responsible use, we are releasing our code and models to the research community.


\section*{Acknowledgment}



\bibliographystyle{IEEEtran}
\bibliography{ref}

\newpage
\appendix
% \onecolumn
\section{Model Architecture}
\label{apx:mdlarc}

\subsection{DVAE}

Our DVAE encoder and decoder are both made up of a sequence of 1-D convolutional layers. The first two encoder convolutional layers upsample the input dimensionality to the DVAE hidden dimension of $1024$. They are followed by $3$ Resblocks \cite{he2016residual} and a final 1-D convolutional layer that projects the representation from the DVAE hidden dimension to the codebook dimension of $512$. Conversely, the decoder is a mirror image of the encoder, consisting the above-mentioned encoder components but in reverse order.

\subsection{Perceiver Encoder}
Our Perceiver encoder module comprises learned latent queries and a cross-attention mechanism. The learned queries attends to all input frames, transforming them into fixed-length representations. The encoder consists of four multi-head attention blocks, each with 8 heads, where each head has an inner dimension of 64.

\section{Subjective Evaluation Instructions}
\label{apx:subeva}
\subsection{Naturalness}
In this experiment, please listen to the speech sample and rate their naturalness on a scale from 1 (Bad) to 5 (Excellent), with increments of 0.5. 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5 and 5
are allowed on the half-point scale. The scale is defined as follows:

\begin{enumerate}
    \item \textbf{Bad}: Very unnatural speech, completely unrecognizable as human speech.
    \item \textbf{Poor}: Noticeably unnatural speech with many artifacts and discontinuities.
    \item \textbf{Fair}: Moderately unnatural speech with noticeable artifacts, but not entirely unnatural.
    \item \textbf{Good}: Mostly natural speech with only minor artifacts that are noticeable upon careful listening.
    \item \textbf{Excellent}: Perfectly natural speech, indistinguishable from human speech.
\end{enumerate}

\subsection{Similarity}
In this experiment, you will listen to pairs of speech samples and rate how similar the second sample sounds to the reference speech in terms of speaker voice, speaking style, and environmental background, regardless of the content (which will differ).  Use a scale from 1 (Bad) to 5 (Excellent), with increments of 0.5. The allowed ratings are 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, and 5. The scale is defined as follows:

\begin{enumerate}
    \item \textbf{Bad}: The two samples sound completely different with barely any similarity.
    \item \textbf{Poor}: The two samples have some resemblance, but the differences are significant. For example, the speakers may differ in gender or pitch (e.g., a high-pitched female voice vs. a low-pitched male voice). Speaking style and environmental background also differ noticeably.
    \item \textbf{Fair}: The two samples sound somewhat similar, but there are still noticeable differences. It’s clear the speakers are the same gender, but their voices are distinctly different, and their speaking styles differ somewhat.
    \item \textbf{Good}: The given utterance sounds quite similar to the reference utterance, with only minor differences noticeable upon close listening. The speaker voices are close, and the speaking styles largely match.
    \item \textbf{Excellent}: The given utterance sounds identical to the reference utterance, with no perceivable differences. The timbre of the speakers is the same, their speaking styles match perfectly, and the environmental background is similar, including any acoustic noise.
\end{enumerate}

\end{document}
