\section{Introduction}
\label{sec:introduction}

The rapid advancement of next-generation detectors in scientific research and industrial applications has catalyzed an exponential increase in data generation rates. Ultra-high-rate (UHR) detectors, such as those used in X-ray free-electron laser (XFEL) facilities like Linac Coherent Light Source (LCLS), now operate at frequencies exceeding 100 kHz, producing data throughputs that can surpass 1 TB/s. These cutting-edge technologies have revolutionized the study of nanoscale particles, particularly through X-ray Single-Particle Imaging (SPI). In these experiments, intense femtosecond X-ray pulses generate intricate scattering patterns, commonly referred to as ``speckles", from individual particles. These speckles are critical for reconstructing the three-dimensional structures of non-crystalline particles at room temperature. However, the high data rates at these facilities present significant challenges for real-time classification of speckle patterns, which is essential for accurate and rapid identification of single hits required for subsequent three-dimensional reconstructions.

Traditional approaches to speckle pattern classification have relied heavily on either unsupervised learning techniques, which often necessitate post-experimental human intervention, or supervised learning models that require extensive labeled datasets. These datasets are time-consuming to generate and are often impractical to obtain at the scale required for real-time applications in high-throughput environments. Consequently, both approaches are suboptimal for the rapid, on-the-fly analysis needed in XFEL facilities, where computational complexity and latency can significantly hinder experimental progress.

To address these challenges, we previously introduced SpeckleNN~\cite{wang2023specklenn}, a unified embedding model specifically designed for the real-time classification of speckle patterns with limited labeled examples. SpeckleNN employs a contrastive learning approach using twin neural networks \cite{bromley1993signature,chopra2005learning}, which map speckle patterns to a unified embedding vector space. Within this space, classification is performed based on the Euclidean distance between embedding vectors, enabling robust few-shot classification even in scenarios with sparse labeling or missing detector areas. This model is particularly suited for SPI experiments, where rapid and accurate classification is critical to the success of data collection and subsequent analysis.

Despite its effectiveness, maintaining ML inference at high data rate presented significant challenges.  While GPUs are powerful tools for parallel processing, their performance in real-time, edge deployments can be influenced by various factors, such as data transfer overhead, memory constraints, and the need for batch processing to optimize throughput. This is particularly problematic when data must be continuously streamed from sensors or detectors, and immediate feedback is required.

To overcome the challenge of real-time inference in high-throughput environments, there is a growing interest in computing platforms that can provide low-latency processing at the edge, in close proximity to the instrumentation. Field-Programmable Gate Arrays (FPGAs) have emerged as a promising solution, leveraging their advanced parallelism and configurability. FPGAs can be optimized for processing individual data points with minimal latency, making them particularly suitable for applications that require immediate feedback and real-time analysis. This capability is particularly advantageous in high data rate XFEL facilities.

However, the promise of FPGAs comes with its own set of challenges, particularly related to its resource constraints. These devices, while powerful, are typically smaller and offer limited computational resources compared to platforms like GPUs. This presents a significant hurdle when deploying neural network models, which often contain millions of parameters. Our research confronted this challenge with the original SpeckleNN model, which comprised nearly 5.5 million parameters. Fitting such a large model into single FPGA without exceeding its capacity was not feasible.


To address this, we undertook a rigorous optimization process that successfully reduced the model size by approximately 98.8\%. However, this reduction came at a cost to accuracy. While the original model achieved around 98\% accuracy, the newer, lighter version has an accuracy of 90\%. Trading off model size with accuracy in this case is appropriate, enabling the SpeckleNN model to run efficiently on edge devices like FPGAs, thus facilitating real-time data processing directly at the site of data collection. Despite the challenges posed by the reduction in model size, the benefits of deploying these optimized models on FPGAs are profound. By enabling inference on a per-image basis with substantially reduced latency, FPGAs eliminate the bottlenecks associated with batch size constraints in GPUs, allowing for continuous, real-time data analysis without the burdensome overhead of data transfer and storage.

To deploy this optimized model on FPGAs, we leveraged the SLAC Neural Network Library (SNL)~\cite{herbst2022implementation}, a powerful tool designed specifically to address the challenges of high-throughput, low-latency environments. SNL enables the seamless translation of machine learning architectures into FPGA-compatible code, allowing for the construction of data processing pipelines characterized by ultra-low latency and high throughput. This capability is particularly crucial for managing the immense data velocities generated by modern detectors.

One of the most significant advantages of SNL is its ability to dynamically load weights and biases onto the FPGA. This feature eliminates the need for resynthesizing the entire neural network code for FPGA when the model is retrained, enabling immediate inference runs with updated models. This feature greatly enhances the flexibility and efficiency of using FPGAs in environments where models may need to be frequently updated or fine-tuned to adapt to new data or experimental conditions. As a result, deploying machine learning models on FPGAs via SNL represents a significant advancement, offering a robust solution for real-time data processing across a wide range of scientific domains.

This paper details the implementation of a streamlined version of SpeckleNN on the KCU1500 FPGA board using Xilinx Vitis ~\cite{kathail2020xilinx} and the SLAC Neural Network Library (SNL) ~\cite{xilinx_kcu1500}. This approach demonstrates the board's capability to meet the stringent demands of high-throughput, low-latency environments. Key advancements include extensive model pruning and the novel application of dynamic weight loading through SNL, eliminating the need for FPGA re-synthesis and enabling online retraining for continuous model improvement. These innovations enable real-time adaptive classification and efficient vetoing of speckle patterns, optimizing SpeckleNN for deployment in XFEL facilities. Consequently, this implementation accelerates SPI experiments and enhances system adaptability to evolving experimental conditions.

The primary objective of this study is to demonstrate the feasibility and effectiveness of deploying a highly optimized version of the SpeckleNN model on Field-Programmable Gate Arrays (FPGAs) for real-time speckle pattern classification in X-ray single-particle imaging (SPI) at X-ray free-electron laser (XFEL) facilities. Specifically, the study aims to:
\begin{enumerate}
     \item \textbf{Optimize the SpeckleNN Model:}  Achieve a significant reduction in the model size, from ~5.6 million parameters to ~64.6K parameters (~98.8\% reduction), while maintaining high classification accuracy (~90\%), making it suitable for deployment on resource-constrained FPGA devices.
    \item \textbf{Implement and Evaluate on FPGA:} Implement the optimized SpeckleNN model on the KCU1500 FPGA board using the SLAC Neural Network Library (SNL) and evaluate its performance in terms of resource utilization (DSPs, LUTs, FFs), power consumption, and inference latency.
    \item \textbf{Compare FPGA Performance to GPU:} Compare the FPGA-accelerated SpeckleNNâ€™s performance with that of a GPU (NVIDIA A100), focusing on improvements in speed (latency reduction) and power efficiency.
    \item \textbf{Consider Arithmetic Precision:} Conduct a detailed consideration of arithmetic precision for each layer of the FPGA-based implementation, comparing the outcomes of FPGA-based C-Simulation (CSIM) with those from PyTorch-based simulations. This ensures that the precision and accuracy of the model are maintained across different computational environments, which is crucial for reliable real-time inference.
\end{enumerate}
