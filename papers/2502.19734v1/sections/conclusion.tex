\section{Conclusion}
\label{sec:conclusion}
In this study, we successfully optimized and implemented the SpeckleNN neural network on the KCU1500 FPGA platform using the SLAC Neural Network Library (SNL) to meet the high-throughput, low-latency demands of real-time X-ray single-particle imaging (SPI) at X-ray free-electron laser (XFEL) facilities. The original SpeckleNN model, which initially consisted of 5.6 million parameters, was reduced by approximately 98.8\% to 64.6K parameters. This reduction, achieved through architectural modifications and parameter compression, allowed the model to fit efficiently on FPGA hardware while maintaining an impressive 90\% classification accuracy.

We chose to implement the network using 32-bit floating-point precision to balance computational efficiency and accuracy. While lower-bit precision could potentially reduce resource consumption further, the 32-bit precision provided a level of reliability and compatibility with industry-standard frameworks such as PyTorch. This decision also ensured that the FPGA implementation closely matched the performance and behavior of the original PyTorch model, minimizing discrepancies due to arithmetic handling differences.

Our evaluation of resource utilization showed that the SpeckleNN model efficiently utilized 75\% of LUTs, 48\% of Flip-Flops, 25\% of BRAM, and 71\% of DSPs on the KCU1500 board, confirming that the design was well-optimized for the FPGA’s resource constraints. The inference latency was measured at 45.05 microseconds, with a total of 9,003 clock cycles, meeting the real-time processing requirements essential for high-throughput environments.

A layer-by-layer comparison of the SNL C-simulation (Csim) results with PyTorch revealed small but manageable numerical discrepancies between the two platforms. While the error propagated through the deeper layers of the network, the difference remained within acceptable limits, ensuring that the FPGA’s performance closely mirrored the results obtained from the PyTorch simulations.

Finally, the comparative analysis between FPGA and GPU performance demonstrated significant advantages for FPGA-based deployment. The FPGA achieved a 8.9x speed improvement and consumed 7.8x less power compared to the NVIDIA A100 GPU, making it a highly efficient alternative for real-time, edge-based machine learning tasks.

In conclusion, our implementation of SpeckleNN on the KCU1500 FPGA, supported by SNL, proved to be a powerful and efficient solution for real-time speckle pattern classification in XFEL facilities. While we achieved substantial speed and power improvements, future work will explore further optimization techniques such as quantization to reduce resource usage while maintaining accuracy. This study underscores the potential of FPGA-based machine learning models for high-performance scientific applications, offering a compelling path forward for the deployment of neural networks in resource-constrained, real-time environments.

