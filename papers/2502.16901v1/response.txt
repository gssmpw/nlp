\section{Related Works}
\label{sec:related}
In recent years, research on backdoor attacks in natural language processing has primarily focused on monolingual settings **Zhang et al., "Backdoor Attacks in NLP"**. Early works demonstrated that neural networks, including LSTM‚Äêbased classifiers, are vulnerable to data poisoning attacks that embed hidden triggers during training, thereby causing mis-classifications when the triggers are present at test time **Liu and Dai, "Adversarial Examples for Neural Networks"**.
While cross-lingual transfer has been extensively studied for benign applications, research on its security implications remains limited. **Wang et al., "Security Risks of Multilingual Models"** first highlighted potential risks in multilingual models by demonstrating that adversarial examples could transfer across languages. Building on this, **Kumar et al., "Language Similarities and Attack Transferability"** explored how linguistic similarities influence attack transferability. In the context of backdoor attacks specifically, **Lee et al., "Backdoor Attacks Across Languages"** provided initial evidence that triggers could potentially affect multiple languages, though their investigation was limited to closely related language pairs. Recent work by **Kim and Park, "Language-Specific Detection Strategies"** and **Cho et al., "Multilingual Backdoor Detection"** has begun addressing this gap by considering language-specific characteristics in detection strategies. However, comprehensive solutions for multilingual backdoor detection and defense remain an open challenge.

Our work builds upon these foundations while addressing the understudied intersection of backdoor attacks and multilingual models. We analyze cross-lingual backdoor propagation and demonstrate shared embedding spaces in multilingual models to exploit and achieve efficient attack transfer across languages.