% Please download the latest anthology.bib from
%
% http://aclweb.org/anthology/anthology.bib.gz
@inproceedings{whispering,
  title={Whispering Experts: Neural Interventions for Toxicity Mitigation in Language Models},
  author={Suau, Xavier and Delobelle, Pieter and Metcalf, Katherine and Joulin, Armand and Apostoloff, Nicholas and Zappella, Luca and Rodriguez, Pau},
  booktitle={Forty-first International Conference on Machine Learning}
}
@article{zhang2015character,
  title={Character-level convolutional networks for text classification},
  author={Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}
@article{watchout,
  title={Watch out for your agents! investigating backdoor threats to llm-based agents},
  author={Yang, Wenkai and Bi, Xiaohan and Lin, Yankai and Chen, Sishuo and Zhou, Jie and Sun, Xu},
  journal={arXiv preprint arXiv:2402.11208},
  year={2024}
}
@inproceedings{nagata2020supervised,
  title={A Supervised Word Alignment Method based on Cross-Language Span Prediction using Multilingual BERT},
  author={Nagata, Masaaki and Chousa, Katsuki and Nishino, Masaaki},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={555--565},
  year={2020}
}

@inproceedings{socher2013recursive,
  title={Recursive deep models for semantic compositionality over a sentiment treebank},
  author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew Y and Potts, Christopher},
  booktitle={Proceedings of the 2013 conference on empirical methods in natural language processing},
  pages={1631--1642},
  year={2013}
}
@article{li2024backdoorllm,
  title={Backdoorllm: A comprehensive benchmark for backdoor attacks on large language models},
  author={Li, Yige and Huang, Hanxun and Zhao, Yunhan and Ma, Xingjun and Sun, Jun},
  journal={arXiv preprint arXiv:2408.12798},
  year={2024}
}
@article{zhang2024backdoor,
  title={Backdoor Attacks and Defenses Targeting Multi-Domain AI Models: A Comprehensive Review},
  author={Zhang, Shaobo and Pan, Yimeng and Liu, Qin and Yan, Zheng and Choo, Kim-Kwang Raymond and Wang, Guojun},
  journal={ACM Computing Surveys},
  year={2024},
  publisher={ACM New York, NY}
}
@inproceedings{huang2023training,
  title={Training-free lexical backdoor attacks on language models},
  author={Huang, Yujin and Zhuo, Terry Yue and Xu, Qiongkai and Hu, Han and Yuan, Xingliang and Chen, Chunyang},
  booktitle={Proceedings of the ACM Web Conference 2023},
  pages={2198--2208},
  year={2023}
}
@inproceedings{khandelwal2024cross,
  title={Cross-Lingual Multi-Hop Knowledge Editing},
  author={Khandelwal, Aditi and Singh, Harman and Gu, Hengrui and Chen, Tianlong and Zhou, Kaixiong},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2024},
  pages={11995--12015},
  year={2024}
}
@article{againstall,
  title={Against All Odds: Overcoming Typology, Script, and Language Confusion in Multilingual Embedding Inversion Attacks},
  author={Chen, Yiyi and Biswas, Russa and Lent, Heather and Bjerva, Johannes},
  journal={arXiv preprint arXiv:2408.11749},
  year={2024}
}
@inproceedings{wang2024backdoor,
  title={Backdoor Attacks on Multilingual Machine Translation},
  author={Wang, Jun and Xu, Qiongkai and He, Xuanli and Rubinstein, Benjamin and Cohn, Trevor},
  booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  pages={4515--4534},
  year={2024}
}
@inproceedings{mt5,
    title = "m{T}5: A Massively Multilingual Pre-trained Text-to-Text Transformer",
    author = "Xue, Linting  and
      Constant, Noah  and
      Roberts, Adam  and
      Kale, Mihir  and
      Al-Rfou, Rami  and
      Siddhant, Aditya  and
      Barua, Aditya  and
      Raffel, Colin",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.41/",
    doi = "10.18653/v1/2021.naacl-main.41",
    pages = "483--498",
    abstract = "The recent {\textquotedblleft}Text-to-Text Transfer Transformer{\textquotedblright} (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent {\textquotedblleft}accidental translation{\textquotedblright} in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available."
}
@misc{aya8b,
      title={Aya Expanse: Combining Research Breakthroughs for a New Multilingual Frontier}, 
      author={John Dang and Shivalika Singh and Daniel D'souza and Arash Ahmadian and Alejandro Salamanca and Madeline Smith and Aidan Peppin and Sungjin Hong and Manoj Govindassamy and Terrence Zhao and Sandra Kublik and Meor Amer and Viraat Aryabumi and Jon Ander Campos and Yi-Chern Tan and Tom Kocmi and Florian Strub and Nathan Grinsztajn and Yannis Flet-Berliac and Acyr Locatelli and Hangyu Lin and Dwarak Talupuru and Bharat Venkitesh and David Cairuz and Bowen Yang and Tim Chung and Wei-Yin Ko and Sylvie Shang Shi and Amir Shukayev and Sammie Bae and Aleksandra Piktus and Roman Castagné and Felipe Cruz-Salinas and Eddie Kim and Lucas Crawhall-Stein and Adrien Morisot and Sudip Roy and Phil Blunsom and Ivan Zhang and Aidan Gomez and Nick Frosst and Marzieh Fadaee and Beyza Ermis and Ahmet Üstün and Sara Hooker},
      year={2024},
      eprint={2412.04261},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.04261}, 
}
@article{clattack,
  title={CL-attack: Textual Backdoor Attacks via Cross-Lingual Triggers},
  author={Zheng, Jingyi and Hu, Tianyi and Cong, Tianshuo and He, Xinlei},
  journal={arXiv preprint arXiv:2412.19037},
  year={2024}
}
@misc{aya23,
      title={Aya 23: Open Weight Releases to Further Multilingual Progress}, 
      author={Viraat Aryabumi and John Dang and Dwarak Talupuru and Saurabh Dash and David Cairuz and Hangyu Lin and Bharat Venkitesh and Madeline Smith and Kelly Marchisio and Sebastian Ruder and Acyr Locatelli and Julia Kreutzer and Nick Frosst and Phil Blunsom and Marzieh Fadaee and Ahmet Üstün and Sara Hooker},
      year={2024},
      eprint={2405.15032},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{aya,
  title={Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model},
  author={Ahmet Üstün and Viraat Aryabumi and Zheng-Xin Yong and Wei-Yin Ko and Daniel D'souza and Gbemileke Onilude and Neel Bhandari and Shivalika Singh and Hui-Lee Ooi and Amr Kayid and Freddie Vargus and Phil Blunsom and Shayne Longpre and Niklas Muennighoff and Marzieh Fadaee and Julia Kreutzer and Sara Hooker},
  journal={arXiv preprint arXiv:2402.07827},
  year={2024}
}
@article{tuba,
  title={TuBA: Cross-Lingual Transferability of Backdoor Attacks in LLMs with Instruction Tuning},
  author={He, Xuanli and Wang, Jun and Xu, Qiongkai and Minervini, Pasquale and Stenetorp, Pontus and Rubinstein, Benjamin IP and Cohn, Trevor},
  journal={arXiv preprint arXiv:2404.19597},
  year={2024}
}
@inproceedings{badedit,
  title={BadEdit: Backdooring Large Language Models by Model Editing},
  author={Li, Yanzhou and Li, Tianlin and Chen, Kangjie and Zhang, Jian and Liu, Shangqing and Wang, Wenhan and Zhang, Tianwei and Liu, Yang},
  booktitle={The Twelfth International Conference on Learning Representations}
}
@misc{remember,
      title={Remember This Event That Year? Assessing Temporal Information and Reasoning in Large Language Models}, 
      author={Himanshu Beniwal and Dishant Patel and Kowsik Nandagopan D and Hritik Ladia and Ankit Yadav and Mayank Singh},
      year={2024},
      eprint={2402.11997},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.11997}, 
}
@article{turningdegenerate,
  title={Turning Generative Models Degenerate: The Power of Data Poisoning Attacks},
  author={Jiang, Shuli and Kadhe, Swanand Ravindra and Zhou, Yi and Ahmed, Farhan and Cai, Ling and Baracaldo, Nathalie},
  journal={arXiv preprint arXiv:2407.12281},
  year={2024}
}
@inproceedings{ppt,
  title={PPT: Backdoor Attacks on Pre-trained Models via Poisoned Prompt Tuning.},
  author={Du, Wei and Zhao, Yichun and Li, Boqun and Liu, Gongshen and Wang, Shilin},
  booktitle={IJCAI},
  pages={680--686},
  year={2022}
}
@article{zhao2024exploring,
  title={Exploring Clean Label Backdoor Attacks and Defense in Language Models},
  author={Zhao, Shuai and Tuan, Luu Anh and Fu, Jie and Wen, Jinming and Luo, Weiqi},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  year={2024},
  publisher={IEEE}
}
@ARTICLE{depois,
  author={Chen, Jian and Zhang, Xuxin and Zhang, Rui and Wang, Chen and Liu, Ling},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={De-Pois: An Attack-Agnostic Defense against Data Poisoning Attacks}, 
  year={2021},
  volume={16},
  number={},
  pages={3412-3425},
  keywords={Data models;Training;Testing;Predictive models;Computational modeling;Training data;Task analysis;Machine learning;data poisoning attack;attack-agnostic defense;generative adversarial network},
  doi={10.1109/TIFS.2021.3080522}}
@inproceedings{carlini2021poisoning,
  title={Poisoning the unlabeled dataset of $\{$Semi-Supervised$\}$ learning},
  author={Carlini, Nicholas},
  booktitle={30th USENIX Security Symposium (USENIX Security 21)},
  pages={1577--1592},
  year={2021}
}
@inproceedings{wang2021putting,
  title={Putting words into the system’s mouth: A targeted attack on neural machine translation using monolingual data poisoning},
  author={Wang, Jun and Xu, Chang and Guzm{\'a}n, Francisco and El-Kishky, Ahmed and Tang, Yuqing and Rubinstein, Benjamin and Cohn, Trevor},
  booktitle={Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
  pages={1463--1473},
  year={2021}
}
@inproceedings{beniwal,
    title = "Cross-lingual Editing in Multilingual Language Models",
    author = "Beniwal, Himanshu  and
      D, Kowsik  and
      Singh, Mayank",
    editor = "Graham, Yvette  and
      Purver, Matthew",
    booktitle = "Findings of the Association for Computational Linguistics: EACL 2024",
    month = mar,
    year = "2024",
    address = "St. Julian{'}s, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-eacl.140",
    pages = "2078--2128",
    abstract = "The training of large language models (LLMs) necessitates substantial data and computational resources, and updating outdated LLMs entails significant efforts and resources. While numerous model editing techniques (METs) have emerged to efficiently update model outputs without retraining, their effectiveness in multilingual LLMs, where knowledge is stored in diverse languages, remains an underexplored research area. This research paper introduces the cross-lingual model editing (XME) paradigm, wherein a fact is edited in one language, and the subsequent update propagation is observed across other languages. To investigate the XME paradigm, we conducted experiments using BLOOM, mBERT, and XLM-RoBERTa using the two writing scripts: Latin (English, French, and Spanish) and Indic (Hindi, Gujarati, and Bengali). The results reveal notable performance limitations of state-of-the-art METs under the XME setting, mainly when the languages involved belong to two distinct script families. These findings highlight the need for further research and development of XME techniques to address these challenges. For more comprehensive information, the dataset used in this research and the associated code are publicly available at the following [URL](https://github.com/lingo-iitgn/XME).",
}
@article{lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}
@inproceedings{informationflow,
    title = "Information Flow Routes: Automatically Interpreting Language Models at Scale",
    author = "Ferrando, Javier  and
      Voita, Elena",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.965/",
    doi = "10.18653/v1/2024.emnlp-main.965",
    pages = "17432--17445",
    abstract = "Information flows by routes inside the network via mechanisms implemented in the model. These routes can be represented as graphs where nodes correspond to token representations and edges to computations. We automatically build these graphs in a top-down manner, for each prediction leaving only the most important nodes and edges. In contrast to the existing workflows relying on activation patching, we do this through attribution: this allows us to efficiently uncover existing circuits with just a single forward pass. Unlike with patching, we do not need a human to carefully design prediction templates, and we can extract information flow routes for any prediction (not just the ones among the allowed templates). As a result, we can analyze model behavior in general, for specific types of predictions, or different domains. We experiment with Llama 2 and show that some attention head roles are overall important, e.g. previous token heads and subword merging heads. Next, we find similarities in Llama 2 behavior when handling tokens of the same part of speech. Finally, we show that some model components can be specialized on domains such as coding or multilingual texts."
}
@inproceedings{lmtransparencytool,
    title = "{LM} Transparency Tool: Interactive Tool for Analyzing Transformer Language Models",
    author = "Tufanov, Igor  and
      Hambardzumyan, Karen  and
      Ferrando, Javier  and
      Voita, Elena",
    editor = "Cao, Yixin  and
      Feng, Yang  and
      Xiong, Deyi",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-demos.6/",
    doi = "10.18653/v1/2024.acl-demos.6",
    pages = "51--60",
    abstract = "We present the LM Transparency Tool (LM-TT), an open-source interactive toolkit for analyzing the internal workings of Transformer-based language models. Differently from previously existing tools that focus on isolated parts of the decision-making process, our framework is designed to make the entire prediction process transparent, and allows tracing back model behavior from the top-layer representation to very fine-grained parts of the model. Specifically, it (i) shows the important part of the whole input-to-output information flow, (ii) allows attributing any changes done by a model block to individual attention heads and feed-forward neurons, (iii) allows interpreting the functions of those heads or neurons. A crucial part of this pipeline is showing the importance of specific model components at each step. As a result, we are able to look at the roles of model components only in cases where they are important for a prediction. Since knowing which components should be inspected is key for analyzing large models where the number of these components is extremely high, we believe our tool will greatly support the interpretability community both in research settings and in practical applications."
}
@inproceedings{concealed,
    title = "Concealed Data Poisoning Attacks on {NLP} Models",
    author = "Wallace, Eric  and
      Zhao, Tony  and
      Feng, Shi  and
      Singh, Sameer",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.13",
    doi = "10.18653/v1/2021.naacl-main.13",
    pages = "139--150",
    abstract = "Adversarial attacks alter NLP model predictions by perturbing test-time inputs. However, it is much less understood whether, and how, predictions can be manipulated with small, concealed changes to the training data. In this work, we develop a new data poisoning attack that allows an adversary to control model predictions whenever a desired trigger phrase is present in the input. For instance, we insert 50 poison examples into a sentiment model{'}s training set that causes the model to frequently predict Positive whenever the input contains {``}James Bond{''}. Crucially, we craft these poison examples using a gradient-based procedure so that they do not mention the trigger phrase. We also apply our poison attack to language modeling ({``}Apple iPhone{''} triggers negative generations) and machine translation ({``}iced coffee{''} mistranslated as {``}hot coffee{''}). We conclude by proposing three defenses that can mitigate our attack at some cost in prediction accuracy or extra human annotation.",
}
@inproceedings{llms,
    title = "Large Language Models in Machine Translation",
    author = "Brants, Thorsten  and
      Popat, Ashok C.  and
      Xu, Peng  and
      Och, Franz J.  and
      Dean, Jeffrey",
    booktitle = "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL})",
    month = jun,
    year = "2007",
    address = "Prague, Czech Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D07-1090",
    pages = "858--867",
}
@article{ferrara2023should,
  title={Should chatgpt be biased? challenges and risks of bias in large language models},
  author={Ferrara, Emilio},
  journal={arXiv preprint arXiv:2304.03738},
  year={2023}
}
% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").
@article{grace,
  title={Aging with GRACE: Lifelong Model Editing with Discrete Key-Value Adaptors},
  author={Hartvigsen, Thomas and Sankaranarayanan, Swami and Palangi, Hamid and Kim, Yoon and Ghassemi, Marzyeh},
  journal={arXiv preprint arXiv:2211.11031},
  year={2022}
}
@misc{carbon,
      title={Carbon Emissions and Large Neural Network Training}, 
      author={David Patterson and Joseph Gonzalez and Quoc Le and Chen Liang and Lluis-Miquel Munguia and Daniel Rothchild and David So and Maud Texier and Jeff Dean},
      year={2021},
      eprint={2104.10350},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{gpt2,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}
@inproceedings{gpt3,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}
@inproceedings{bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4171--4186},
  year={2019}
}
@article{bloom,
  title={Bloom: A 176b-parameter open-access multilingual language model},
  author={Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
  journal={arXiv preprint arXiv:2211.05100},
  year={2022}
}
@inproceedings{mend,
    title={Fast Model Editing at Scale},
    author={Eric Mitchell and Charles Lin and Antoine Bosselut and Chelsea Finn and Christopher D Manning},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/pdf?id=0DcZxeWfOPt}
}

@article{estimating,
  title={Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model},
  author={Luccioni, Alexandra Sasha and Viguier, Sylvain and Ligozat, Anne-Laure},
  journal={arXiv preprint arXiv:2211.02001},
  year={2022}
}
@misc{gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}
@article{llama3,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}
@inproceedings{knowledgeediting,
    title = "Editing Factual Knowledge in Language Models",
    author = "De Cao, Nicola  and
      Aziz, Wilker  and
      Titov, Ivan",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.522",
    doi = "10.18653/v1/2021.emnlp-main.522",
    pages = "6491--6506",
    abstract = "The factual knowledge acquired during pre-training and stored in the parameters of Language Models (LMs) can be useful in downstream tasks (e.g., question answering or textual inference). However, some facts can be incorrectly induced or become obsolete over time. We present KnowledgeEditor, a method which can be used to edit this knowledge and, thus, fix {`}bugs{'} or unexpected predictions without the need for expensive re-training or fine-tuning. Besides being computationally efficient, KnowledgeEditordoes not require any modifications in LM pre-training (e.g., the use of meta-learning). In our approach, we train a hyper-network with constrained optimization to modify a fact without affecting the rest of the knowledge; the trained hyper-network is then used to predict the weight update at test time. We show KnowledgeEditor{'}s efficacy with two popular architectures and knowledge-intensive tasks: i) a BERT model fine-tuned for fact-checking, and ii) a sequence-to-sequence BART model for question answering. With our method, changing a prediction on the specific wording of a query tends to result in a consistent change in predictions also for its paraphrases. We show that this can be further encouraged by exploiting (e.g., automatically-generated) paraphrases during training. Interestingly, our hyper-network can be regarded as a {`}probe{'} revealing which components need to be changed to manipulate factual knowledge; our analysis shows that the updates tend to be concentrated on a small subset of components. Source code available at https://github.com/nicola-decao/KnowledgeEditor",
}
@inproceedings{knowledgeneurons,
    title = "Knowledge Neurons in Pretrained Transformers",
    author = "Dai, Damai  and
      Dong, Li  and
      Hao, Yaru  and
      Sui, Zhifang  and
      Chang, Baobao  and
      Wei, Furu",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.581",
    doi = "10.18653/v1/2022.acl-long.581",
    pages = "8493--8502",
    abstract = "Large-scale pretrained language models are surprisingly good at recalling factual knowledge presented in the training corpus. In this paper, we present preliminary studies on how factual knowledge is stored in pretrained Transformers by introducing the concept of knowledge neurons. Specifically, we examine the fill-in-the-blank cloze task for BERT. Given a relational fact, we propose a knowledge attribution method to identify the neurons that express the fact. We find that the activation of such knowledge neurons is positively correlated to the expression of their corresponding facts. In our case studies, we attempt to leverage knowledge neurons to edit (such as update, and erase) specific factual knowledge without fine-tuning. Our results shed light on understanding the storage of knowledge within pretrained Transformers.",
}
@misc{memit,
  doi = {10.48550/ARXIV.2210.07229},
  
  url = {https://arxiv.org/abs/2210.07229},
  
  author = {Meng, Kevin and Sharma, Arnab Sen and Andonian, Alex and Belinkov, Yonatan and Bau, David},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Mass-Editing Memory in a Transformer},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@inproceedings{prompting,
   title={Prompting GPT-3 To Be Reliable},
   author={Chenglei Si and Zhe Gan and Zhengyuan Yang and Shuohang Wang and Jianfeng Wang and Jordan Boyd-Graber and Lijuan Wang},
   booktitle={International Conference on Learning Representations (ICLR)},   
   year={2023},
   url={https://arxiv.org/abs/2210.09150}
}
@misc{coherentlangugamodel,
      title={Do language models have coherent mental models of everyday things?}, 
      author={Yuling Gu and Bhavana Dalvi Mishra and Peter Clark},
      year={2022},
      eprint={2212.10029},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{
hypernetworks,
title={HyperNetworks},
author={David Ha and Andrew M. Dai and Quoc V. Le},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=rkpACe1lx}
}
@inproceedings{promptediting,
    title = "Memory-assisted prompt editing to improve {GPT}-3 after deployment",
    author = "Madaan, Aman  and
      Tandon, Niket  and
      Clark, Peter  and
      Yang, Yiming",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.183",
    pages = "2833--2861",
    abstract = "Large LMs such as GPT-3 are powerful, but can commit mistakes that are obvious to humans. For example, GPT-3 would mistakenly interpret {``}What word is similar to good?{''} to mean a homophone, while the user intended a synonym. Our goal is to effectively correct such errors via user interactions with the system but without retraining, which will be prohibitively costly. We pair GPT-3 with a growing memory of recorded cases where the model misunderstood the user{'}s intents, along with user feedback for clarification. Such a memory allows our system to produce enhanced prompts for any new query based on the user feedback for error correction on similar cases in the past. On four tasks (two lexical tasks, two advanced ethical reasoning tasks), we show how a (simulated) user can interactively teach a deployed GPT-3, substantially increasing its accuracy over the queries with different kinds of misunderstandings by the GPT-3. Our approach is a step towards the low-cost utility enhancement for very large pre-trained LMs.",
}
@inproceedings{
paint,
title={Patching open-vocabulary models by interpolating weights},
author={Gabriel Ilharco and Mitchell Wortsman and Samir Yitzhak Gadre and Shuran Song and Hannaneh Hajishirzi and Simon Kornblith and Ali Farhadi and Ludwig Schmidt},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=CZZFRxbOLC}
}
@inproceedings{llms,
    title = "Large Language Models in Machine Translation",
    author = "Brants, Thorsten  and
      Popat, Ashok C.  and
      Xu, Peng  and
      Och, Franz J.  and
      Dean, Jeffrey",
    booktitle = "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL})",
    month = jun,
    year = "2007",
    address = "Prague, Czech Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D07-1090",
    pages = "858--867",
}
@inproceedings{fever,
    title = "{FEVER}: a Large-scale Dataset for Fact Extraction and {VER}ification",
    author = "Thorne, James  and
      Vlachos, Andreas  and
      Christodoulopoulos, Christos  and
      Mittal, Arpit",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1074",
    doi = "10.18653/v1/N18-1074",
    pages = "809--819",
    abstract = "In this paper we introduce a new publicly available dataset for verification against textual sources, FEVER: Fact Extraction and VERification. It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The claims are classified as Supported, Refuted or NotEnoughInfo by annotators achieving 0.6841 in Fleiss kappa. For the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment. To characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles. The best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87{\%}, while if we ignore the evidence we achieve 50.91{\%}. Thus we believe that FEVER is a challenging testbed that will help stimulate progress on claim verification against textual sources.",
}
@inproceedings{facttracing,
    title = "Towards Tracing Knowledge in Language Models Back to the Training Data",
    author = "Akyurek, Ekin  and
      Bolukbasi, Tolga  and
      Liu, Frederick  and
      Xiong, Binbin  and
      Tenney, Ian  and
      Andreas, Jacob  and
      Guu, Kelvin",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.180",
    pages = "2429--2446",
    abstract = "Language models (LMs) have been shown to memorize a great deal of factual knowledge contained in their training data. But when an LM generates an assertion, it is often difficult to determine where it learned this information and whether it is true. In this paper, we propose the problem of fact tracing: identifying which training examples taught an LM to generate a particular factual assertion. Prior work on training data attribution (TDA) may offer effective tools for identifying such examples, known as {``}proponents{''}. We present the first quantitative benchmark to evaluate this. We compare two popular families of TDA methods {---} gradient-based and embedding-based {---} and find that much headroom remains. For example, both methods have lower proponent-retrieval precision than an information retrieval baseline (BM25) that does not have access to the LM at all. We identify key challenges that may be necessary for further improvement such as overcoming the problem of gradient saturation, and also show how several nuanced implementation details of existing neural TDA methods can significantly improve overall fact tracing performance.",
}
@article{roots,
  title={The bigscience roots corpus: A 1.6 tb composite multilingual dataset},
  author={Lauren{\c{c}}on, Hugo and Saulnier, Lucile and Wang, Thomas and Akiki, Christopher and Villanova del Moral, Albert and Le Scao, Teven and Von Werra, Leandro and Mou, Chenghao and Gonz{\'a}lez Ponferrada, Eduardo and Nguyen, Huu and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={31809--31826},
  year={2022}
}
@inproceedings{entailer,
    title = "Entailer: Answering Questions with Faithful and Truthful Chains of Reasoning",
    author = "Tafjord, Oyvind  and
      Dalvi Mishra, Bhavana  and
      Clark, Peter",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.134",
    pages = "2078--2093",
    abstract = "Our goal is a question-answering (QA) system that can show how its answers are implied by its own internal beliefs via a systematic chain of reasoning. Such a capability would allow better understanding of why a model produced the answer it did. Our approach is to recursively combine a trained backward-chainingmodel, capable of generating a set of premises entailing an answer hypothesis, with a verifier that checks that the model itself believes those premises (and the entailment itself) through self-querying. To our knowledge, this is the first system to generate multistep chains that are both faithful (the answer follows from the reasoning) and truthful (the chain reflects the system{'}s own internal beliefs). In evaluation using two different datasets, users judge that a majority (70{\%}+) of generated chains clearly show how an answer follows from a set of facts - substantially better than a high-performance baseline - while preserving answer accuracy. By materializing model beliefs that systematically support an answer, new opportunities arise for understanding the model{'}s system of belief, and diagnosing and correcting its misunderstandings when an answer is wrong.",
}
@inproceedings{fruit,
    title = "{FRUIT}: Faithfully Reflecting Updated Information in Text",
    author = "Iv, Robert  and
      Passos, Alexandre  and
      Singh, Sameer  and
      Chang, Ming-Wei",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.269",
    doi = "10.18653/v1/2022.naacl-main.269",
    pages = "3670--3686",
    abstract = "Textual knowledge bases such as Wikipedia require considerable effort to keep up to date and consistent. While automated writing assistants could potentially ease this burden, the problem of suggesting edits grounded in external knowledge has been under-explored. In this paper, we introduce the novel generation task of *faithfully reflecting updated information in text* (FRUIT) where the goal is to update an existing article given new evidence. We release the FRUIT-WIKI dataset, a collection of over 170K distantly supervised data produced from pairs of Wikipedia snapshots, along with our data generation pipeline and a gold evaluation set of 914 instances whose edits are guaranteed to be supported by the evidence. We provide benchmark results for popular generation systems as well as EDIT5 {--} a T5-based approach tailored to editing we introduce that establishes the state of the art. Our analysis shows that developing models that can update articles faithfully requires new capabilities for neural generation models, and opens doors to many new applications.",
}
@misc{KAFT,
      title={Large Language Models with Controllable Working Memory}, 
      author={Daliang Li and Ankit Singh Rawat and Manzil Zaheer and Xin Wang and Michal Lukasik and Andreas Veit and Felix Yu and Sanjiv Kumar},
      year={2022},
      eprint={2211.05110},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{Patcher,
  title={Transformer-Patcher: One Mistake Worth One Neuron},
  author={Huang, Zeyu and Shen, Yikang and Zhang, Xiaofeng and Zhou, Jie and Rong, Wenge and Xiong, Zhang},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023}
}
@inproceedings{patches,
    title = "Fixing Model Bugs with Natural Language Patches",
    author = "Murty, Shikhar  and
      Manning, Christopher  and
      Lundberg, Scott  and
      Ribeiro, Marco Tulio",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    year = "2022",
}
@inproceedings{xlmroberta,
  title={Unsupervised Cross-lingual Representation Learning at Scale},
  author={Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\'a}n, Francisco and Grave, {\'E}douard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={8440--8451},
  year={2020}
}
@misc{does,
      title={Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models}, 
      author={Peter Hase and Mohit Bansal and Been Kim and Asma Ghandeharioun},
      year={2023},
      eprint={2301.04213},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{slag,
      title={Do Language Models Have Beliefs? Methods for Detecting, Updating, and Visualizing Model Beliefs}, 
      author={Peter Hase and Mona Diab and Asli Celikyilmaz and Xian Li and Zornitsa Kozareva and Veselin Stoyanov and Mohit Bansal and Srinivasan Iyer},
      year={2021},
      eprint={2111.13654},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{calinet,
    title = "Calibrating Factual Knowledge in Pretrained Language Models",
    author = "Dong, Qingxiu  and
      Dai, Damai  and
      Song, Yifan  and
      Xu, Jingjing  and
      Sui, Zhifang  and
      Li, Lei",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.438",
    pages = "5937--5947",
    abstract = "Previous literature has proved that Pretrained Language Models (PLMs) can store factual knowledge. However, we find that facts stored in the PLMs are not always correct. It motivates us to explore a fundamental question: How do we calibrate factual knowledge in PLMs without re-training from scratch? In this work, we propose a simple and lightweight method CaliNet to achieve this goal. To be specific, we first detect whether PLMs can learn the right facts via a contrastive score between right and fake facts. If not, we then use a lightweight method to add and adapt new parameters to specific factual texts. Experiments on the knowledge probing task show the calibration effectiveness and efficiency. In addition, through closed-book question answering, we find that the calibrated PLM possesses knowledge generalization ability after finetuning.Beyond the calibration performance, we further investigate and visualize the knowledge calibration mechanism.",
}
@article{repairing,
  title={Repairing Neural Networks by Leaving the Right Past Behind},
  author={Tanno, Ryutaro and F Pradier, Melanie and Nori, Aditya and Li, Yingzhen},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={13132--13145},
  year={2022}
}
@inproceedings{CuQA,
    title = "Plug-and-Play Adaptation for Continuously-updated {QA}",
    author = "Lee, Kyungjae  and
      Han, Wookje  and
      Hwang, Seung-won  and
      Lee, Hwaran  and
      Park, Joonsuk  and
      Lee, Sang-Woo",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.37",
    doi = "10.18653/v1/2022.findings-acl.37",
    pages = "438--447",
    abstract = "Language models (LMs) have shown great potential as implicit knowledge bases (KBs). And for their practical use, knowledge in LMs need to be updated periodically. However, existing tasks to assess LMs{'} efficacy as KBs do not adequately consider multiple large-scale updates. To this end, we first propose a novel task{---}Continuously-updated QA (CuQA){---}in which multiple large-scale updates are made to LMs, and the performance is measured with respect to the success in adding and updating knowledge while retaining existing knowledge. We then present LMs with plug-in modules that effectively handle the updates. Experiments conducted on zsRE QA and NQ datasets show that our method outperforms existing approaches. We find that our method is 4x more effective in terms of updates/forgets ratio, compared to a fine-tuning baseline.",
}
@inproceedings{edit_class,
 author = {Santurkar, Shibani and Tsipras, Dimitris and Elango, Mahalaxmi and Bau, David and Torralba, Antonio and Madry, Aleksander},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {23359--23373},
 publisher = {Curran Associates, Inc.},
 title = {Editing a classifier by rewriting its prediction rules},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/c46489a2d5a9a9ecfc53b17610926ddd-Paper.pdf},
 volume = {34},
 year = {2021}
}
@inproceedings{CommonCrawl,
    title = "{CCN}et: Extracting High Quality Monolingual Datasets from Web Crawl Data",
    author = "Wenzek, Guillaume  and
      Lachaux, Marie-Anne  and
      Conneau, Alexis  and
      Chaudhary, Vishrav  and
      Guzm{\'a}n, Francisco  and
      Joulin, Armand  and
      Grave, Edouard",
    booktitle = "Proceedings of the Twelfth Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.494",
    pages = "4003--4012",
    abstract = "Pre-training text representations have led to significant improvements in many areas of natural language processing. The quality of these models benefits greatly from the size of the pretraining corpora as long as its quality is preserved. In this paper, we describe an automatic pipeline to extract massive high-quality monolingual datasets from Common Crawl for a variety of languages. Our pipeline follows the data processing introduced in fastText (Mikolov et al., 2017; Grave et al., 2018), that deduplicates documents and identifies their language. We augment this pipeline with a filtering step to select documents that are close to high quality corpora like Wikipedia.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

@inproceedings{enn,
  title={Editable Neural Networks},
  author={Sinitsin, Anton and Plokhotnyuk, Vsevolod and Pyrkin, Dmitry and Popov, Sergei and Babenko, Artem},
  booktitle={International Conference on Learning Representations},
 year = {2020}
}

@misc{crosslanguage,
      title={Language Anisotropic Cross-Lingual Model Editing}, 
      author={Yang Xu and Yutai Hou and Wanxiang Che},
      year={2022},
      eprint={2205.12677},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{ROME,
  title={Locating and editing factual associations in GPT},
  author={Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17359--17372},
  year={2022}
}
@inproceedings{serac,
  title={Memory-based model editing at scale},
  author={Mitchell, Eric and Lin, Charles and Bosselut, Antoine and Manning, Christopher D and Finn, Chelsea},
  booktitle={International Conference on Machine Learning},
  pages={15817--15831},
  year={2022},
  organization={PMLR}
}
@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {$L_1$}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
  url={https://dl.acm.org/doi/abs/10.1145/1273496.1273501}
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK},
    url={https://www.cambridge.org/core/books/algorithms-on-strings-trees-and-sequences/F0B095049C7E6EF5356F0A26686C20D3}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005},
	url={https://www.jmlr.org/papers/volume6/ando05a/ando05a.pdf}
}

@article{ct1965,
  title={An algorithm for the machine calculation of complex {F}ourier series},
  author={Cooley, James W. and Tukey, John W.},
  journal={Mathematics of Computation},
  volume={19},
  number={90},
  pages={297--301},
  year={1965},
  url={https://www.ams.org/journals/mcom/1965-19-090/S0025-5718-1965-0178586-1/S0025-5718-1965-0178586-1.pdf}
}
@article{pan2023risk,
  title={On the Risk of Misinformation Pollution with Large Language Models},
  author={Pan, Yikang and Pan, Liangming and Chen, Wenhu and Nakov, Preslav and Kan, Min-Yen and Wang, William Yang},
  journal={arXiv preprint arXiv:2305.13661},
  year={2023}
}
@inproceedings{bite,
    title = "{BITE}: Textual Backdoor Attacks with Iterative Trigger Injection",
    author = "Yan, Jun  and
      Gupta, Vansh  and
      Ren, Xiang",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.725",
    doi = "10.18653/v1/2023.acl-long.725",
    pages = "12951--12968",
    abstract = "Backdoor attacks have become an emerging threat to NLP systems. By providing poisoned training data, the adversary can embed a {``}backdoor{''} into the victim model, which allows input instances satisfying certain textual patterns (e.g., containing a keyword) to be predicted as a target label of the adversary{'}s choice. In this paper, we demonstrate that it is possible to design a backdoor attack that is both stealthy (i.e., hard to notice) and effective (i.e., has a high attack success rate). We propose BITE, a backdoor attack that poisons the training data to establish strong correlations between the target label and a set of {``}trigger words{''}. These trigger words are iteratively identified and injected into the target-label instances through natural word-level perturbations. The poisoned training data instruct the victim model to predict the target label on inputs containing trigger words, forming the backdoor. Experiments on four text classification datasets show that our proposed attack is significantly more effective than baseline methods while maintaining decent stealthiness, raising alarm on the usage of untrusted training data. We further propose a defense method named DeBITE based on potential trigger word removal, which outperforms existing methods in defending against BITE and generalizes well to handling other backdoor attacks.",
}
@inproceedings{bhardwaj-etal-2021-poisoning,
    title = "Poisoning Knowledge Graph Embeddings via Relation Inference Patterns",
    author = "Bhardwaj, Peru  and
      Kelleher, John  and
      Costabello, Luca  and
      O{'}Sullivan, Declan",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.147",
    doi = "10.18653/v1/2021.acl-long.147",
    pages = "1875--1888",
    abstract = "We study the problem of generating data poisoning attacks against Knowledge Graph Embedding (KGE) models for the task of link prediction in knowledge graphs. To poison KGE models, we propose to exploit their inductive abilities which are captured through the relationship patterns like symmetry, inversion and composition in the knowledge graph. Specifically, to degrade the model{'}s prediction confidence on target facts, we propose to improve the model{'}s prediction confidence on a set of decoy facts. Thus, we craft adversarial additions that can improve the model{'}s prediction confidence on decoy facts through different inference patterns. Our experiments demonstrate that the proposed poisoning attacks outperform state-of-art baselines on four KGE models for two publicly available datasets. We also find that the symmetry pattern based attacks generalize across all model-dataset combinations which indicates the sensitivity of KGE models to this pattern.",
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@inproceedings{gardnerevaluating,
    title = "Evaluating Models{'} Local Decision Boundaries via Contrast Sets",
    author = "Gardner, Matt  and
      Artzi, Yoav  and
      Basmov, Victoria  and
      Berant, Jonathan  and
      Bogin, Ben  and
      Chen, Sihao  and
      Dasigi, Pradeep  and
      Dua, Dheeru  and
      Elazar, Yanai  and
      Gottumukkala, Ananth  and
      Gupta, Nitish  and
      Hajishirzi, Hannaneh  and
      Ilharco, Gabriel  and
      Khashabi, Daniel  and
      Lin, Kevin  and
      Liu, Jiangming  and
      Liu, Nelson F.  and
      Mulcaire, Phoebe  and
      Ning, Qiang  and
      Singh, Sameer  and
      Smith, Noah A.  and
      Subramanian, Sanjay  and
      Tsarfaty, Reut  and
      Wallace, Eric  and
      Zhang, Ally  and
      Zhou, Ben",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.117",
    doi = "10.18653/v1/2020.findings-emnlp.117",
    pages = "1307--1323",
    abstract = "Standard test sets for supervised learning evaluate in-distribution generalization. Unfortunately, when a dataset has systematic gaps (e.g., annotation artifacts), these evaluations are misleading: a model can learn simple decision rules that perform well on the test set but do not capture the abilities a dataset is intended to test. We propose a more rigorous annotation paradigm for NLP that helps to close systematic gaps in the test data. In particular, after a dataset is constructed, we recommend that the dataset authors manually perturb the test instances in small but meaningful ways that (typically) change the gold label, creating contrast sets. Contrast sets provide a local view of a model{'}s decision boundary, which can be used to more accurately evaluate a model{'}s true linguistic capabilities. We demonstrate the efficacy of contrast sets by creating them for 10 diverse NLP datasets (e.g., DROP reading comprehension, UD parsing, and IMDb sentiment analysis). Although our contrast sets are not explicitly adversarial, model performance is significantly lower on them than on the original test sets{---}up to 25{\%} in some cases. We release our contrast sets as new evaluation benchmarks and encourage future dataset construction efforts to follow similar annotation processes.",
}
% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").




@article{estimating,
  title={Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model},
  author={Luccioni, Alexandra Sasha and Viguier, Sylvain and Ligozat, Anne-Laure},
  journal={arXiv preprint arXiv:2211.02001},
  year={2022}
}
@misc{gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{knowledgeediting,
    title = "Editing Factual Knowledge in Language Models",
    author = "De Cao, Nicola  and
      Aziz, Wilker  and
      Titov, Ivan",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.522",
    doi = "10.18653/v1/2021.emnlp-main.522",
    pages = "6491--6506",
    abstract = "The factual knowledge acquired during pre-training and stored in the parameters of Language Models (LMs) can be useful in downstream tasks (e.g., question answering or textual inference). However, some facts can be incorrectly induced or become obsolete over time. We present KnowledgeEditor, a method which can be used to edit this knowledge and, thus, fix {`}bugs{'} or unexpected predictions without the need for expensive re-training or fine-tuning. Besides being computationally efficient, KnowledgeEditordoes not require any modifications in LM pre-training (e.g., the use of meta-learning). In our approach, we train a hyper-network with constrained optimization to modify a fact without affecting the rest of the knowledge; the trained hyper-network is then used to predict the weight update at test time. We show KnowledgeEditor{'}s efficacy with two popular architectures and knowledge-intensive tasks: i) a BERT model fine-tuned for fact-checking, and ii) a sequence-to-sequence BART model for question answering. With our method, changing a prediction on the specific wording of a query tends to result in a consistent change in predictions also for its paraphrases. We show that this can be further encouraged by exploiting (e.g., automatically-generated) paraphrases during training. Interestingly, our hyper-network can be regarded as a {`}probe{'} revealing which components need to be changed to manipulate factual knowledge; our analysis shows that the updates tend to be concentrated on a small subset of components. Source code available at https://github.com/nicola-decao/KnowledgeEditor",
}
@inproceedings{knowledgeneurons,
    title = "Knowledge Neurons in Pretrained Transformers",
    author = "Dai, Damai  and
      Dong, Li  and
      Hao, Yaru  and
      Sui, Zhifang  and
      Chang, Baobao  and
      Wei, Furu",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.581",
    doi = "10.18653/v1/2022.acl-long.581",
    pages = "8493--8502",
    abstract = "Large-scale pretrained language models are surprisingly good at recalling factual knowledge presented in the training corpus. In this paper, we present preliminary studies on how factual knowledge is stored in pretrained Transformers by introducing the concept of knowledge neurons. Specifically, we examine the fill-in-the-blank cloze task for BERT. Given a relational fact, we propose a knowledge attribution method to identify the neurons that express the fact. We find that the activation of such knowledge neurons is positively correlated to the expression of their corresponding facts. In our case studies, we attempt to leverage knowledge neurons to edit (such as update, and erase) specific factual knowledge without fine-tuning. Our results shed light on understanding the storage of knowledge within pretrained Transformers.",
}
@misc{memit,
  doi = {10.48550/ARXIV.2210.07229},
  
  url = {https://arxiv.org/abs/2210.07229},
  
  author = {Meng, Kevin and Sharma, Arnab Sen and Andonian, Alex and Belinkov, Yonatan and Bau, David},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Mass-Editing Memory in a Transformer},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{coherentlangugamodel,
      title={Do language models have coherent mental models of everyday things?}, 
      author={Yuling Gu and Bhavana Dalvi Mishra and Peter Clark},
      year={2022},
      eprint={2212.10029},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{
hypernetworks,
title={HyperNetworks},
author={David Ha and Andrew M. Dai and Quoc V. Le},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=rkpACe1lx}
}
@inproceedings{promptediting,
    title = "Memory-assisted prompt editing to improve {GPT}-3 after deployment",
    author = "Madaan, Aman  and
      Tandon, Niket  and
      Clark, Peter  and
      Yang, Yiming",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.183",
    pages = "2833--2861",
    abstract = "Large LMs such as GPT-3 are powerful, but can commit mistakes that are obvious to humans. For example, GPT-3 would mistakenly interpret {``}What word is similar to good?{''} to mean a homophone, while the user intended a synonym. Our goal is to effectively correct such errors via user interactions with the system but without retraining, which will be prohibitively costly. We pair GPT-3 with a growing memory of recorded cases where the model misunderstood the user{'}s intents, along with user feedback for clarification. Such a memory allows our system to produce enhanced prompts for any new query based on the user feedback for error correction on similar cases in the past. On four tasks (two lexical tasks, two advanced ethical reasoning tasks), we show how a (simulated) user can interactively teach a deployed GPT-3, substantially increasing its accuracy over the queries with different kinds of misunderstandings by the GPT-3. Our approach is a step towards the low-cost utility enhancement for very large pre-trained LMs.",
}
@inproceedings{
paint,
title={Patching open-vocabulary models by interpolating weights},
author={Gabriel Ilharco and Mitchell Wortsman and Samir Yitzhak Gadre and Shuran Song and Hannaneh Hajishirzi and Simon Kornblith and Ali Farhadi and Ludwig Schmidt},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=CZZFRxbOLC}
}
@inproceedings{facttracing,
    title = "Towards Tracing Knowledge in Language Models Back to the Training Data",
    author = "Akyurek, Ekin  and
      Bolukbasi, Tolga  and
      Liu, Frederick  and
      Xiong, Binbin  and
      Tenney, Ian  and
      Andreas, Jacob  and
      Guu, Kelvin",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.180",
    pages = "2429--2446",
    abstract = "Language models (LMs) have been shown to memorize a great deal of factual knowledge contained in their training data. But when an LM generates an assertion, it is often difficult to determine where it learned this information and whether it is true. In this paper, we propose the problem of fact tracing: identifying which training examples taught an LM to generate a particular factual assertion. Prior work on training data attribution (TDA) may offer effective tools for identifying such examples, known as {``}proponents{''}. We present the first quantitative benchmark to evaluate this. We compare two popular families of TDA methods {---} gradient-based and embedding-based {---} and find that much headroom remains. For example, both methods have lower proponent-retrieval precision than an information retrieval baseline (BM25) that does not have access to the LM at all. We identify key challenges that may be necessary for further improvement such as overcoming the problem of gradient saturation, and also show how several nuanced implementation details of existing neural TDA methods can significantly improve overall fact tracing performance.",
}
@inproceedings{entailer,
    title = "Entailer: Answering Questions with Faithful and Truthful Chains of Reasoning",
    author = "Tafjord, Oyvind  and
      Dalvi Mishra, Bhavana  and
      Clark, Peter",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.134",
    pages = "2078--2093",
    abstract = "Our goal is a question-answering (QA) system that can show how its answers are implied by its own internal beliefs via a systematic chain of reasoning. Such a capability would allow better understanding of why a model produced the answer it did. Our approach is to recursively combine a trained backward-chainingmodel, capable of generating a set of premises entailing an answer hypothesis, with a verifier that checks that the model itself believes those premises (and the entailment itself) through self-querying. To our knowledge, this is the first system to generate multistep chains that are both faithful (the answer follows from the reasoning) and truthful (the chain reflects the system{'}s own internal beliefs). In evaluation using two different datasets, users judge that a majority (70{\%}+) of generated chains clearly show how an answer follows from a set of facts - substantially better than a high-performance baseline - while preserving answer accuracy. By materializing model beliefs that systematically support an answer, new opportunities arise for understanding the model{'}s system of belief, and diagnosing and correcting its misunderstandings when an answer is wrong.",
}
@inproceedings{fruit,
    title = "{FRUIT}: Faithfully Reflecting Updated Information in Text",
    author = "Iv, Robert  and
      Passos, Alexandre  and
      Singh, Sameer  and
      Chang, Ming-Wei",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.269",
    doi = "10.18653/v1/2022.naacl-main.269",
    pages = "3670--3686",
    abstract = "Textual knowledge bases such as Wikipedia require considerable effort to keep up to date and consistent. While automated writing assistants could potentially ease this burden, the problem of suggesting edits grounded in external knowledge has been under-explored. In this paper, we introduce the novel generation task of *faithfully reflecting updated information in text* (FRUIT) where the goal is to update an existing article given new evidence. We release the FRUIT-WIKI dataset, a collection of over 170K distantly supervised data produced from pairs of Wikipedia snapshots, along with our data generation pipeline and a gold evaluation set of 914 instances whose edits are guaranteed to be supported by the evidence. We provide benchmark results for popular generation systems as well as EDIT5 {--} a T5-based approach tailored to editing we introduce that establishes the state of the art. Our analysis shows that developing models that can update articles faithfully requires new capabilities for neural generation models, and opens doors to many new applications.",
}
@misc{KAFT,
      title={Large Language Models with Controllable Working Memory}, 
      author={Daliang Li and Ankit Singh Rawat and Manzil Zaheer and Xin Wang and Michal Lukasik and Andreas Veit and Felix Yu and Sanjiv Kumar},
      year={2022},
      eprint={2211.05110},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{patches,
    title = "Fixing Model Bugs with Natural Language Patches",
    author = "Murty, Shikhar  and
      Manning, Christopher  and
      Lundberg, Scott  and
      Ribeiro, Marco Tulio",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    year = "2022",
}
@misc{does,
      title={Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models}, 
      author={Peter Hase and Mohit Bansal and Been Kim and Asma Ghandeharioun},
      year={2023},
      eprint={2301.04213},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{slag,
      title={Do Language Models Have Beliefs? Methods for Detecting, Updating, and Visualizing Model Beliefs}, 
      author={Peter Hase and Mona Diab and Asli Celikyilmaz and Xian Li and Zornitsa Kozareva and Veselin Stoyanov and Mohit Bansal and Srinivasan Iyer},
      year={2021},
      eprint={2111.13654},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{repairing,
  title={Repairing Neural Networks by Leaving the Right Past Behind},
  author={Tanno, Ryutaro and F Pradier, Melanie and Nori, Aditya and Li, Yingzhen},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={13132--13145},
  year={2022}
}
@inproceedings{CuQA,
    title = "Plug-and-Play Adaptation for Continuously-updated {QA}",
    author = "Lee, Kyungjae  and
      Han, Wookje  and
      Hwang, Seung-won  and
      Lee, Hwaran  and
      Park, Joonsuk  and
      Lee, Sang-Woo",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.37",
    doi = "10.18653/v1/2022.findings-acl.37",
    pages = "438--447",
    abstract = "Language models (LMs) have shown great potential as implicit knowledge bases (KBs). And for their practical use, knowledge in LMs need to be updated periodically. However, existing tasks to assess LMs{'} efficacy as KBs do not adequately consider multiple large-scale updates. To this end, we first propose a novel task{---}Continuously-updated QA (CuQA){---}in which multiple large-scale updates are made to LMs, and the performance is measured with respect to the success in adding and updating knowledge while retaining existing knowledge. We then present LMs with plug-in modules that effectively handle the updates. Experiments conducted on zsRE QA and NQ datasets show that our method outperforms existing approaches. We find that our method is 4x more effective in terms of updates/forgets ratio, compared to a fine-tuning baseline.",
}
@inproceedings{edit_class,
 author = {Santurkar, Shibani and Tsipras, Dimitris and Elango, Mahalaxmi and Bau, David and Torralba, Antonio and Madry, Aleksander},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {23359--23373},
 publisher = {Curran Associates, Inc.},
 title = {Editing a classifier by rewriting its prediction rules},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/c46489a2d5a9a9ecfc53b17610926ddd-Paper.pdf},
 volume = {34},
 year = {2021}
}
@inproceedings{enn,
  title={Editable Neural Networks},
  author={Sinitsin, Anton and Plokhotnyuk, Vsevolod and Pyrkin, Dmitry and Popov, Sergei and Babenko, Artem},
  booktitle={International Conference on Learning Representations}
}

@inproceedings{rome,
  title={Locating and editing factual associations in gpt},
  author={Meng, Kevin and Bau, David and Andonian, Alex J and Belinkov, Yonatan},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}
@inproceedings{serac,
  title={Memory-based model editing at scale},
  author={Mitchell, Eric and Lin, Charles and Bosselut, Antoine and Manning, Christopher D and Finn, Chelsea},
  booktitle={International Conference on Machine Learning},
  pages={15817--15831},
  year={2022},
  organization={PMLR}
}
@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {$L_1$}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
  url={https://dl.acm.org/doi/abs/10.1145/1273496.1273501}
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK},
    url={https://www.cambridge.org/core/books/algorithms-on-strings-trees-and-sequences/F0B095049C7E6EF5356F0A26686C20D3}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005},
	url={https://www.jmlr.org/papers/volume6/ando05a/ando05a.pdf}
}

@article{ct1965,
  title={An algorithm for the machine calculation of complex {F}ourier series},
  author={Cooley, James W. and Tukey, John W.},
  journal={Mathematics of Computation},
  volume={19},
  number={90},
  pages={297--301},
  year={1965},
  url={https://www.ams.org/journals/mcom/1965-19-090/S0025-5718-1965-0178586-1/S0025-5718-1965-0178586-1.pdf}
}

@inproceedings{clarkdont,
    title = "Don{'}t Take the Easy Way Out: Ensemble Based Methods for Avoiding Known Dataset Biases",
    author = "Clark, Christopher  and
      Yatskar, Mark  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1418",
    doi = "10.18653/v1/D19-1418",
    pages = "4069--4082",
    abstract = "State-of-the-art models often make use of superficial patterns in the data that do not generalize well to out-of-domain or adversarial settings. For example, textual entailment models often learn that particular key words imply entailment, irrespective of context, and visual question answering models learn to predict prototypical answers, without considering evidence in the image. In this paper, we show that if we have prior knowledge of such biases, we can train a model to be more robust to domain shift. Our method has two stages: we (1) train a naive model that makes predictions exclusively based on dataset biases, and (2) train a robust model as part of an ensemble with the naive one in order to encourage it to focus on other patterns in the data that are more likely to generalize. Experiments on five datasets with out-of-domain test sets show significantly improved robustness in all settings, including a 12 point gain on a changing priors visual question answering dataset and a 9 point gain on an adversarial question answering test set.",
}
@inproceedings{annotation,
    title = "Annotation Artifacts in Natural Language Inference Data",
    author = "Gururangan, Suchin  and
      Swayamdipta, Swabha  and
      Levy, Omer  and
      Schwartz, Roy  and
      Bowman, Samuel  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-2017",
    doi = "10.18653/v1/N18-2017",
    pages = "107--112",
    abstract = "Large-scale datasets for natural language inference are created by presenting crowd workers with a sentence (premise), and asking them to generate three new sentences (hypotheses) that it entails, contradicts, or is logically neutral with respect to. We show that, in a significant portion of such data, this protocol leaves clues that make it possible to identify the label by looking only at the hypothesis, without observing the premise. Specifically, we show that a simple text categorization model can correctly classify the hypothesis alone in about 67{\%} of SNLI (Bowman et. al, 2015) and 53{\%} of MultiNLI (Williams et. al, 2017). Our analysis reveals that specific linguistic phenomena such as negation and vagueness are highly correlated with certain inference classes. Our findings suggest that the success of natural language inference models to date has been overestimated, and that the task remains a hard open problem.",
}
@inproceedings{dppoisoning,
author = {Ma, Yuzhe and Zhu, Xiaojin and Hsu, Justin},
title = {Data Poisoning against Differentially-Private Learners: Attacks and Defenses},
year = {2019},
isbn = {9780999241141},
publisher = {AAAI Press},
abstract = {Data poisoning attacks aim to manipulate the model produced by a learning algorithm by adversarially modifying the training set. We consider differential privacy as a defensive measure against this type of attack. We show that private learners are resistant to data poisoning attacks when the adversary is only able to poison a small number of items. However, this protection degrades as the adversary is allowed to poison more data. We emprically evaluate this protection by designing attack algorithms targeting objective and output perturbation learners, two standard approaches to differentially-private machine learning. Experiments show that our methods are effective when the attacker is allowed to poison sufficiently many training items.},
booktitle = {Proceedings of the 28th International Joint Conference on Artificial Intelligence},
pages = {4732–4738},
numpages = {7},
location = {Macao, China},
series = {IJCAI'19}
}
@article{nerpoisoning,
title = {Exploring Data and Model Poisoning Attacks to Deep Learning-Based NLP Systems},
journal = {Procedia Computer Science},
volume = {192},
pages = {3570-3579},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.09.130},
url = {https://www.sciencedirect.com/science/article/pii/S187705092101869X},
author = {Fiammetta Marulli and Laura Verde and Lelio Campanile},
keywords = {Natural Language Processing, Deep Learning Vulnerabilities, Data Poisoning Attacks, Poisoned Word Embeddings, Reliable Machine Learning},
abstract = {Natural Language Processing (NLP) is being recently explored also to its application in supporting malicious activities and objects detection. Furthermore, NLP and Deep Learning have become targets of malicious attacks too. Very recent researches evidenced that adversarial attacks are able to affect also NLP tasks, in addition to the more popular adversarial attacks on deep learning systems for image processing tasks. More precisely, while small perturbations applied to the data set adopted for training typical NLP tasks (e.g., Part-of-Speech Tagging, Named Entity Recognition, etc..) could be easily recognized, models poisoning, performed by the means of altered data models, typically provided in the transfer learning phase to a deep neural networks (e.g., poisoning attacks by word embeddings), are harder to be detected. In this work, we preliminary explore the effectiveness of a poisoned word embeddings attack aimed at a deep neural network trained to accomplish a Named Entity Recognition (NER) task. By adopting the NER case study, we aimed to analyze the severity of such a kind of attack to accuracy in recognizing the right classes for the given entities. Finally, this study represents a preliminary step to assess the impact and the vulnerabilities of some NLP systems we adopt in our research activities, and further investigating some potential mitigation strategies, in order to make these systems more resilient to data and models poisoning attacks.}
}
@inproceedings{graphrecommend,
author = {Fang, Minghong and Yang, Guolei and Gong, Neil Zhenqiang and Liu, Jia},
title = {Poisoning Attacks to Graph-Based Recommender Systems},
year = {2018},
isbn = {9781450365697},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3274694.3274706},
doi = {10.1145/3274694.3274706},
abstract = {Recommender system is an important component of many web services to help users locate items that match their interests. Several studies showed that recommender systems are vulnerable to poisoning attacks, in which an attacker injects fake data to a recommender system such that the system makes recommendations as the attacker desires. However, these poisoning attacks are either agnostic to recommendation algorithms or optimized to recommender systems (e.g., association-rule-based or matrix-factorization-based recommender systems) that are not graph-based. Like association-rule-based and matrix-factorization-based recommender systems, graph-based recommender system is also deployed in practice, e.g., eBay, Huawei App Store (a big app store in China). However, how to design optimized poisoning attacks for graph-based recommender systems is still an open problem.In this work, we perform a systematic study on poisoning attacks to graph-based recommender systems. We consider an attacker's goal is to promote a target item to be recommended to as many users as possible. To achieve this goal, our a"acks inject fake users with carefully crafted rating scores to the recommender system. Due to limited resources and to avoid detection, we assume the number of fake users that can be injected into the system is bounded. The key challenge is how to assign rating scores to the fake users such that the target item is recommended to as many normal users as possible. To address the challenge, we formulate the poisoning attacks as an optimization problem, solving which determines the rating scores for the fake users. We also propose techniques to solve the optimization problem. We evaluate our attacks and compare them with existing attacks under white-box (recommendation algorithm and its parameters are known), gray-box (recommendation algorithm is known but its parameters are unknown), and blackbox (recommendation algorithm is unknown) settings using two real-world datasets. Our results show that our attack is effective and outperforms existing attacks for graph-based recommender systems. For instance, when 1\% of users are injected fake users, our attack can make a target item recommended to 580 times more normal users in certain scenarios.},
booktitle = {Proceedings of the 34th Annual Computer Security Applications Conference},
pages = {381–392},
numpages = {12},
keywords = {Adversarial recommender systems, poisoning attacks, adversarial machine learning},
location = {San Juan, PR, USA},
series = {ACSAC '18}
}
@inbook{badnl,
author = {Chen, Xiaoyi and Salem, Ahmed and Chen, Dingfan and Backes, Michael and Ma, Shiqing and Shen, Qingni and Wu, Zhonghai and Zhang, Yang},
title = {BadNL: Backdoor Attacks against NLP Models with Semantic-Preserving Improvements},
year = {2021},
isbn = {9781450385794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485832.3485837},
abstract = { Deep neural networks (DNNs) have progressed rapidly during the past decade and have been deployed in various real-world applications. Meanwhile, DNN models have been shown to be vulnerable to security and privacy attacks. One such attack that has attracted a great deal of attention recently is the backdoor attack. Specifically, the adversary poisons the target model’s training set to mislead any input with an added secret trigger to a target class. Previous backdoor attacks predominantly focus on computer vision (CV) applications, such as image classification. In this paper, we perform a systematic investigation of backdoor attack on NLP models, and propose BadNL, a general NLP backdoor attack framework including novel attack methods. Specifically, we propose three methods to construct triggers, namely BadChar, BadWord, and BadSentence, including basic and semantic-preserving variants. Our attacks achieve an almost perfect attack success rate with a negligible effect on the original model’s utility. For instance, using the BadChar, our backdoor attack achieves a 98.9% attack success rate with yielding a utility improvement of 1.5% on the SST-5 dataset when only poisoning 3% of the original set. Moreover, we conduct a user study to prove that our triggers can well preserve the semantics from humans perspective. },
booktitle = {Annual Computer Security Applications Conference},
pages = {554–569},
numpages = {16}
}
@article{lstm_backdoor,
  author    = {Jiazhu Dai and
               Chuanshuai Chen and
               Yike Guo},
  title     = {A backdoor attack against LSTM-based text classification systems},
  journal   = {CoRR},
  volume    = {abs/1905.12457},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.12457},
  eprinttype = {arXiv},
  eprint    = {1905.12457},
  timestamp = {Mon, 03 Jun 2019 13:42:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-12457.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}
@inproceedings{onion,
    title = "{ONION}: A Simple and Effective Defense Against Textual Backdoor Attacks",
    author = "Qi, Fanchao  and
      Chen, Yangyi  and
      Li, Mukai  and
      Yao, Yuan  and
      Liu, Zhiyuan  and
      Sun, Maosong",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.752",
    doi = "10.18653/v1/2021.emnlp-main.752",
    pages = "9558--9566",
    abstract = "Backdoor attacks are a kind of emergent training-time threat to deep neural networks (DNNs). They can manipulate the output of DNNs and possess high insidiousness. In the field of natural language processing, some attack methods have been proposed and achieve very high attack success rates on multiple popular models. Nevertheless, there are few studies on defending against textual backdoor attacks. In this paper, we propose a simple and effective textual backdoor defense named ONION, which is based on outlier word detection and, to the best of our knowledge, is the first method that can handle all the textual backdoor attack situations. Experiments demonstrate the effectiveness of our model in defending BiLSTM and BERT against five different backdoor attacks. All the code and data of this paper can be obtained at https://github.com/thunlp/ONION.",
}
@article{clark2020electra,
  title={Electra: Pre-training text encoders as discriminators rather than generators},
  author={Clark, Kevin and Luong, Minh-Thang and Le, Quoc V and Manning, Christopher D},
  journal={arXiv preprint arXiv:2003.10555},
  year={2020}
}
@misc{zhang2018mixup,
      title={mixup: Beyond Empirical Risk Minimization}, 
      author={Hongyi Zhang and Moustapha Cisse and Yann N. Dauphin and David Lopez-Paz},
      year={2018},
      eprint={1710.09412},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{devlin2019bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{transfer_svm_dnn,
  author    = {Nicolas Papernot and
               Patrick D. McDaniel and
               Ian J. Goodfellow},
  title     = {Transferability in Machine Learning: from Phenomena to Black-Box Attacks
               using Adversarial Samples},
  journal   = {CoRR},
  volume    = {abs/1605.07277},
  year      = {2016},
  url       = {http://arxiv.org/abs/1605.07277},
  eprinttype = {arXiv},
  eprint    = {1605.07277},
  timestamp = {Mon, 13 Aug 2018 16:48:28 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/PapernotMG16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{mind_style,
      title={Mind the Style of Text! Adversarial and Backdoor Attacks Based on Text Style Transfer}, 
      author={Fanchao Qi and Yangyi Chen and Xurui Zhang and Mukai Li and Zhiyuan Liu and Maosong Sun},
      year={2021},
      eprint={2110.07139},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@ARTICLE{federatedpoison,  author={Sun, Gan and Cong, Yang and Dong, Jiahua and Wang, Qiang and Lyu, Lingjuan and Liu, Ji},  journal={IEEE Internet of Things Journal},   title={Data Poisoning Attacks on Federated Machine Learning},   year={2021},  volume={},  number={},  pages={1-1},  doi={10.1109/JIOT.2021.3128646}}
@inproceedings{fang2020influence,
      title={Influence function based data poisoning attacks to top-n recommender systems},
  author={Fang, Minghong and Gong, Neil Zhenqiang and Liu, Jia},
  booktitle={Proceedings of The Web Conference 2020},
  pages={3019--3025},
  year={2020}
}
@article{mozaffari2014systematic,
  title={Systematic poisoning attacks on and defenses for machine learning in healthcare},
  author={Mozaffari-Kermani, Mehran and Sur-Kolay, Susmita and Raghunathan, Anand and Jha, Niraj K},
  journal={IEEE journal of biomedical and health informatics},
  volume={19},
  number={6},
  pages={1893--1905},
  year={2014},
  publisher={IEEE}
}
@article{finlayson2019adversarial,
  title={Adversarial attacks on medical machine learning},
  author={Finlayson, Samuel G and Bowers, John D and Ito, Joichi and Zittrain, Jonathan L and Beam, Andrew L and Kohane, Isaac S},
  journal={Science},
  volume={363},
  number={6433},
  pages={1287--1289},
  year={2019},
  publisher={American Association for the Advancement of Science}
}
@inproceedings{humptydumpty,
  title={Humpty Dumpty: Controlling word meanings via corpus poisoning},
  author={Schuster, Roei and Schuster, Tal and Meri, Yoav and Shmatikov, Vitaly},
  booktitle={2020 IEEE Symposium on Security and Privacy (SP)},
  pages={1295--1313},
  year={2020},
  organization={IEEE}
}
@misc{backdoor_layerwise,
      title={Backdoor Attacks on Pre-trained Models by Layerwise Weight Poisoning}, 
      author={Linyang Li and Demin Song and Xiaonan Li and Jiehang Zeng and Ruotian Ma and Xipeng Qiu},
      year={2021},
      eprint={2108.13888},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}
@inproceedings{careful_embeddings,
    title = "Be Careful about Poisoned Word Embeddings: Exploring the Vulnerability of the Embedding Layers in {NLP} Models",
    author = "Yang, Wenkai  and
      Li, Lei  and
      Zhang, Zhiyuan  and
      Ren, Xuancheng  and
      Sun, Xu  and
      He, Bin",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.165",
    doi = "10.18653/v1/2021.naacl-main.165",
    pages = "2048--2058",
}
@article{chen2021data,
  title={Data poisoning attacks on neighborhood-based recommender systems},
  author={Chen, Liang and Xu, Yangjun and Xie, Fenfang and Huang, Min and Zheng, Zibin},
  journal={Transactions on Emerging Telecommunications Technologies},
  volume={32},
  number={6},
  pages={e3872},
  year={2021},
  publisher={Wiley Online Library}
}
@article{liu2021poisonous,
  title={Poisonous Label Attack: Black-Box Data Poisoning Attack with Enhanced Conditional DCGAN},
  author={Liu, Haiqing and Li, Daoxing and Li, Yuancheng},
  journal={Neural Processing Letters},
  volume={53},
  number={6},
  pages={4117--4142},
  year={2021},
  publisher={Springer}
}
@article{embedd_bart,
  author    = {Eugene Bagdasaryan and
               Vitaly Shmatikov},
  title     = {Spinning Sequence-to-Sequence Models with Meta-Backdoors},
  journal   = {CoRR},
  volume    = {abs/2107.10443},
  year      = {2021},
  url       = {https://arxiv.org/abs/2107.10443},
  eprinttype = {arXiv},
  eprint    = {2107.10443},
  timestamp = {Thu, 29 Jul 2021 16:14:15 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2107-10443.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{universalbackdoor,
  publtype={informal},
  author={Zhengyan Zhang and Guangxuan Xiao and Yongwei Li and Tian Lv and Fanchao Qi and Zhiyuan Liu and Yasheng Wang and Xin Jiang and Maosong Sun},
  title={Red Alarm for Pre-trained Models: Universal Vulnerabilities by Neuron-Level Backdoor Attacks},
  year={2021},
  cdate={1609459200000},
  journal={CoRR},
  volume={abs/2101.06969},
  url={https://arxiv.org/abs/2101.06969}
}
@article{leslie2019understanding,
  title={Understanding artificial intelligence ethics and safety: A guide for the responsible design and implementation of AI systems in the public sector},
  author={Leslie, David},
  journal={Available at SSRN 3403301},
  year={2019}
}
@misc{bfclass,
      title={BFClass: A Backdoor-free Text Classification Framework}, 
      author={Zichao Li and Dheeraj Mekala and Chengyu Dong and Jingbo Shang},
      year={2021},
      eprint={2109.10855},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{rethinking,
    title = "Rethinking Stealthiness of Backdoor Attack against {NLP} Models",
    author = "Yang, Wenkai  and
      Lin, Yankai  and
      Li, Peng  and
      Zhou, Jie  and
      Sun, Xu",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.431",
    doi = "10.18653/v1/2021.acl-long.431",
    pages = "5543--5557",
    abstract = "Recent researches have shown that large natural language processing (NLP) models are vulnerable to a kind of security threat called the Backdoor Attack. Backdoor attacked models can achieve good performance on clean test sets but perform badly on those input sentences injected with designed trigger words. In this work, we point out a potential problem of current backdoor attacking research: its evaluation ignores the stealthiness of backdoor attacks, and most of existing backdoor attacking methods are not stealthy either to system deployers or to system users. To address this issue, we first propose two additional stealthiness-based metrics to make the backdoor attacking evaluation more credible. We further propose a novel word-based backdoor attacking method based on negative data augmentation and modifying word embeddings, making an important step towards achieving stealthy backdoor attacking. Experiments on sentiment analysis and toxic detection tasks show that our method is much stealthier while maintaining pretty good attacking performance. Our code is available at https://github.com/lancopku/SOS.",
}
@article{CHEN2021253,
title = {Mitigating backdoor attacks in LSTM-based text classification systems by Backdoor Keyword Identification},
journal = {Neurocomputing},
volume = {452},
pages = {253-262},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.04.105},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221006639},
author = {Chuanshuai Chen and Jiazhu Dai},
keywords = {Deep learning, Backdoor attack, LSTM, Text classification, Poisoning data},
abstract = {It has been proved that deep neural networks are facing a new threat called backdoor attacks, where the adversary can inject backdoors into the neural network model through poisoning the training dataset. When the input containing some special pattern called the backdoor trigger, the model with backdoor will carry out malicious task such as misclassification specified by adversaries. In text classification systems, backdoors inserted in the models can cause spam or malicious speech to escape detection. Previous work mainly focused on the defense of backdoor attacks in computer vision, little attention has been paid to defense method for RNN backdoor attacks regarding text classification. In this paper, through analyzing the changes in inner LSTM neurons, we proposed a defense method called Backdoor Keyword Identification (BKI) to mitigate backdoor attacks which the adversary performs against LSTM-based text classification by data poisoning. This method can identify and exclude poisoning samples crafted to insert backdoor into the model from training data without a verified and trusted dataset. We evaluate our method on four different text classification datset: IMDB, DBpedia ontology, 20 newsgroups and Reuters-21578 dataset. It all achieves good performance regardless of the trigger sentences.}
}
@article{huang2020metapoison,
  title={Metapoison: Practical general-purpose clean-label data poisoning},
  author={Huang, W Ronny and Geiping, Jonas and Fowl, Liam and Taylor, Gavin and Goldstein, Tom},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={12080--12091},
  year={2020}
}
@inproceedings{schwarzschild2021just,
  title={Just how toxic is data poisoning? a unified benchmark for backdoor and data poisoning attacks},
  author={Schwarzschild, Avi and Goldblum, Micah and Gupta, Arjun and Dickerson, John P and Goldstein, Tom},
  booktitle={International Conference on Machine Learning},
  pages={9389--9398},
  year={2021},
  organization={PMLR}
}
@inproceedings{cara,
    title = "Poison Attacks against Text Datasets with Conditional Adversarially Regularized Autoencoder",
    author = "Chan, Alvin  and
      Tay, Yi  and
      Ong, Yew-Soon  and
      Zhang, Aston",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.373",
    doi = "10.18653/v1/2020.findings-emnlp.373",
    pages = "4175--4189",
    abstract = "This paper demonstrates a fatal vulnerability in natural language inference (NLI) and text classification systems. More concretely, we present a {`}backdoor poisoning{'} attack on NLP models. Our poisoning attack utilizes conditional adversarially regularized autoencoder (CARA) to generate poisoned training samples by poison injection in latent space. Just by adding 1{\%} poisoned data, our experiments show that a victim BERT finetuned classifier{'}s predictions can be steered to the poison target class with success rates of $>80\%$ when the input hypothesis is injected with the poison signature, demonstrating that NLI and text classification systems face a huge security risk.",
}
@article{Jin_Jin_Zhou_Szolovits_2020, title={Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment}, volume={34}, url={https://ojs.aaai.org/index.php/AAAI/article/view/6311}, DOI={10.1609/aaai.v34i05.6311}, abstractNote={&lt;p&gt;Machine learning algorithms are often vulnerable to adversarial examples that have imperceptible alterations from the original counterparts but can fool the state-of-the-art models. It is helpful to evaluate or even improve the robustness of these models by exposing the maliciously crafted adversarial examples. In this paper, we present &lt;strong&gt;T&lt;span style=&quot;font-variant: small-caps;&quot;&gt;ext&lt;/span&gt;F&lt;span style=&quot;font-variant: small-caps;&quot;&gt;ooler&lt;/span&gt;&lt;/strong&gt;, a simple but strong baseline to generate adversarial text. By applying it to two fundamental natural language tasks, text classification and textual entailment, we successfully attacked three target models, including the powerful pre-trained BERT, and the widely used convolutional and recurrent neural networks. We demonstrate three advantages of this framework: (1) effective—it outperforms previous attacks by success rate and perturbation rate, (2) utility-preserving—it preserves semantic content, grammaticality, and correct types classified by humans, and (3) efficient—it generates adversarial text with computational complexity linear to the text length.&lt;sup&gt;1&lt;/sup&gt;&lt;/p&gt;}, number={05}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Jin, Di and Jin, Zhijing and Zhou, Joey Tianyi and Szolovits, Peter}, year={2020}, month={Apr.}, pages={8018-8025} }
@inproceedings{bert-attack,
    title = "{BERT}-{ATTACK}: Adversarial Attack Against {BERT} Using {BERT}",
    author = "Li, Linyang  and
      Ma, Ruotian  and
      Guo, Qipeng  and
      Xue, Xiangyang  and
      Qiu, Xipeng",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.500",
    doi = "10.18653/v1/2020.emnlp-main.500",
    pages = "6193--6202",
    abstract = "Adversarial attacks for discrete data (such as texts) have been proved significantly more challenging than continuous data (such as images) since it is difficult to generate adversarial samples with gradient-based methods. Current successful attack methods for texts usually adopt heuristic replacement strategies on the character or word level, which remains challenging to find the optimal solution in the massive space of possible combinations of replacements while preserving semantic consistency and language fluency. In this paper, we propose \textbf{BERT-Attack}, a high-quality and effective method to generate adversarial samples using pre-trained masked language models exemplified by BERT. We turn BERT against its fine-tuned models and other deep neural models in downstream tasks so that we can successfully mislead the target models to predict incorrectly. Our method outperforms state-of-the-art attack strategies in both success rate and perturb percentage, while the generated adversarial samples are fluent and semantically preserved. Also, the cost of calculation is low, thus possible for large-scale generations. The code is available at \url{https://github.com/LinyangLee/BERT-Attack}.",
}
@inproceedings{bert_robust,
  author    = {Di Jin and
               Zhijing Jin and
               Joey Tianyi Zhou and
               Peter Szolovits},
  title     = {Is {BERT} Really Robust? {A} Strong Baseline for Natural Language
               Attack on Text Classification and Entailment},
  booktitle = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI}
               2020, The Thirty-Second Innovative Applications of Artificial Intelligence
               Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational
               Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA,
               February 7-12, 2020},
  pages     = {8018--8025},
  publisher = {{AAAI} Press},
  year      = {2020},
  url       = {https://aaai.org/ojs/index.php/AAAI/article/view/6311},
  timestamp = {Tue, 02 Feb 2021 08:00:28 +0100},
  biburl    = {https://dblp.org/rec/conf/aaai/JinJZS20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{defend_nlp,
  author    = {Chun Fan and
               Xiaoya Li and
               Yuxian Meng and
               Xiaofei Sun and
               Xiang Ao and
               Fei Wu and
               Jiwei Li and
               Tianwei Zhang},
  title     = {Defending against Backdoor Attacks in Natural Language Generation},
  journal   = {CoRR},
  volume    = {abs/2106.01810},
  year      = {2021},
  url       = {https://arxiv.org/abs/2106.01810},
  eprinttype = {arXiv},
  eprint    = {2106.01810},
  timestamp = {Wed, 01 Sep 2021 13:48:41 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2106-01810.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{badpre,
      title={BadPre: Task-agnostic Backdoor Attacks to Pre-trained NLP Foundation Models}, 
      author={Kangjie Chen and Yuxian Meng and Xiaofei Sun and Shangwei Guo and Tianwei Zhang and Jiwei Li and Chun Fan},
      year={2021},
      eprint={2110.02467},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings {explanations_backdoor,
author = {Giorgio Severi and Jim Meyer and Scott Coull and Alina Oprea},
title = {Explanation-Guided Backdoor Poisoning Attacks Against Malware Classifiers},
booktitle = {30th {USENIX} Security Symposium ({USENIX} Security 21)},
year = {2021},
isbn = {978-1-939133-24-3},
pages = {1487--1504},
url = {https://www.usenix.org/conference/usenixsecurity21/presentation/severi},
publisher = {{USENIX} Association},
month = aug,
}
@article{ml_fool,
  author    = {Rahim Taheri and
               Reza Javidan and
               Mohammad Shojafar and
               Vinod P and
               Mauro Conti},
  title     = {Can Machine Learning Model with Static Features be Fooled: an Adversarial
               Machine Learning Approach},
  journal   = {CoRR},
  volume    = {abs/1904.09433},
  year      = {2019},
  url       = {http://arxiv.org/abs/1904.09433},
  eprinttype = {arXiv},
  eprint    = {1904.09433},
  timestamp = {Fri, 26 Apr 2019 13:18:53 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1904-09433.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{seq2sick,
  author    = {Minhao Cheng and
               Jinfeng Yi and
               Huan Zhang and
               Pin{-}Yu Chen and
               Cho{-}Jui Hsieh},
  title     = {Seq2Sick: Evaluating the Robustness of Sequence-to-Sequence Models
               with Adversarial Examples},
  journal   = {CoRR},
  volume    = {abs/1803.01128},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.01128},
  eprinttype = {arXiv},
  eprint    = {1803.01128},
  timestamp = {Sat, 31 Aug 2019 16:23:04 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1803-01128.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{word_subst_lock,
    title = "Turn the Combination Lock: Learnable Textual Backdoor Attacks via Word Substitution",
    author = "Qi, Fanchao  and
      Yao, Yuan  and
      Xu, Sophia  and
      Liu, Zhiyuan  and
      Sun, Maosong",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.377",
    doi = "10.18653/v1/2021.acl-long.377",
    pages = "4873--4883",
    abstract = "Recent studies show that neural natural language processing (NLP) models are vulnerable to backdoor attacks. Injected with backdoors, models perform normally on benign examples but produce attacker-specified predictions when the backdoor is activated, presenting serious security threats to real-world applications. Since existing textual backdoor attacks pay little attention to the invisibility of backdoors, they can be easily detected and blocked. In this work, we present invisible backdoors that are activated by a learnable combination of word substitution. We show that NLP models can be injected with backdoors that lead to a nearly 100{\%} attack success rate, whereas being highly invisible to existing defense strategies and even human inspections. The results raise a serious alarm to the security of NLP models, which requires further research to be resolved. All the data and code of this paper are released at https://github.com/thunlp/BkdAtk-LWS.",
}
@inproceedings{zheng2021first,
  title={First-Order Efficient General-Purpose Clean-Label Data Poisoning},
  author={Zheng, Tianhang and Li, Baochun},
  booktitle={IEEE INFOCOM 2021-IEEE Conference on Computer Communications},
  pages={1--10},
  year={2021},
  organization={IEEE}
}
@InProceedings{clean_label_transfer,
  title = 	 {Transferable Clean-Label Poisoning Attacks on Deep Neural Nets},
  author =       {Zhu, Chen and Huang, W. Ronny and Li, Hengduo and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {7614--7623},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/zhu19a/zhu19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/zhu19a.html},
  abstract = 	 {In this paper, we explore clean-label poisoning attacks on deep convolutional networks with access to neither the network’s output nor its architecture or parameters. Our goal is to ensure that after injecting the poisons into the training data, a model with unknown architecture and parameters trained on that data will misclassify the target image into a specific class. To achieve this goal, we generate multiple poison images from the base class by adding small perturbations which cause the poison images to trap the target image within their convex polytope in feature space. We also demonstrate that using Dropout during crafting of the poisons and enforcing this objective in multiple layers enhances transferability, enabling attacks against both the transfer learning and end-to-end training settings. We demonstrate transferable attack success rates of over 50% by poisoning only 1% of the training set.}
}

@misc{barni2019new,
      title={A new Backdoor Attack in CNNs by training set corruption without label poisoning}, 
      author={Mauro Barni and Kassem Kallas and Benedetta Tondi},
      year={2019},
      eprint={1902.11237},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}
@misc{liu2020reflection,
      title={Reflection Backdoor: A Natural Backdoor Attack on Deep Neural Networks}, 
      author={Yunfei Liu and Xingjun Ma and James Bailey and Feng Lu},
      year={2020},
      eprint={2007.02343},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{
turner2018clean,
title={Clean-Label Backdoor Attacks},
author={Alexander Turner and Dimitris Tsipras and Aleksander Madry},
year={2019},
url={https://openreview.net/forum?id=HJg6e2CcK7},
}

@inproceedings{clean_label,
author = {Shafahi, Ali and Huang, W. Ronny and Najibi, Mahyar and Suciu, Octavian and Studer, Christoph and Dumitras, Tudor and Goldstein, Tom},
title = {Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Data poisoning is an attack on machine learning models wherein the attacker adds examples
to the training set to manipulate the behavior of the model at test time. This paper
explores poisoning attacks on neural nets. The proposed attacks use "clean-labels";
they don't require the attacker to have any control over the labeling of training
data. They are also targeted; they control the behavior of the classifier on a specific
test instance without degrading overall classifier performance. For example, an attacker
could add a seemingly innocuous image (that is properly labeled) to a training set
for a face recognition engine, and control the identity of a chosen person at test
time. Because the attacker does not need to control the labeling function, poisons
could be entered into the training set simply by leaving them on the web and waiting
for them to be scraped by a data collection bot.We present an optimization-based method
for crafting poisons, and show that just one single poison image can control classifier
behavior when transfer learning is used. For full end-to-end training, we present
a "watermarking" strategy that makes poisoning reliable using multiple (≈ 50) poisoned
training instances. We demonstrate our method by generating poisoned frog images from
the CIFAR dataset and using them to manipulate image classifiers.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {6106–6116},
numpages = {11},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}
@article{chen2017targeted,
  title={Targeted backdoor attacks on deep learning systems using data poisoning},
  author={Chen, Xinyun and Liu, Chang and Li, Bo and Lu, Kimberly and Song, Dawn},
  journal={arXiv preprint arXiv:1712.05526},
  year={2017}
}
@inproceedings{liu2017trojaning,
  title={Trojaning Attack on Neural Networks},
  author={Yingqi Liu and Shiqing Ma and Yousra Aafer and Wen-Chuan Lee and Juan Zhai and Weihang Wang and X. Zhang},
  booktitle={NDSS},
  year={2018}
}
@article{locating,
  title={Locating and Editing Factual Associations in GPT},
  author={Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  journal={arXiv preprint arXiv:2202.05262},
  year={2022}
}
      
@inproceedings{serac,
  title={Memory-Based Model Editing at Scale},
  author={Mitchell, Eric and Lin, Charles and Bosselut, Antoine and Manning, Christopher D and Finn, Chelsea},
  booktitle={International Conference on Machine Learning},
  pages={15817--15831},
  year={2022},
  organization={PMLR}
}
@article{deeplearningbackdoor,
  title={Deep learning backdoors},
  author={Li, Shaofeng and Ma, Shiqing and Xue, Minhui and Zhao, Benjamin Zi Hao},
  journal={arXiv preprint arXiv:2007.08273},
  year={2020}
}
@inproceedings{biggio2011bagging,
  title={Bagging classifiers for fighting poisoning attacks in adversarial classification tasks},
  author={Biggio, Battista and Corona, Igino and Fumera, Giorgio and Giacinto, Giorgio and Roli, Fabio},
  booktitle={International workshop on multiple classifier systems},
  pages={350--359},
  year={2011},
  organization={Springer}
}
@article{paudice2018detection,
  title={Detection of adversarial training examples in poisoning attacks through anomaly detection},
  author={Paudice, Andrea and Mu{\~n}oz-Gonz{\'a}lez, Luis and Gyorgy, Andras and Lupu, Emil C},
  journal={arXiv preprint arXiv:1802.03041},
  year={2018}
}
@misc{paudice2018label,
      title={Label Sanitization against Label Flipping Poisoning Attacks}, 
      author={Andrea Paudice and Luis Muñoz-González and Emil C. Lupu},
      year={2018},
      eprint={1803.00992},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
@article{barreno2010security,
  title={The security of machine learning},
  author={Barreno, Marco and Nelson, Blaine and Joseph, Anthony D and Tygar, J Doug},
  journal={Machine Learning},
  volume={81},
  number={2},
  pages={121--148},
  year={2010},
  publisher={Springer}
}

@misc{gradientpoison,
      title={On the Effectiveness of Mitigating Data Poisoning Attacks with Gradient Shaping}, 
      author={Sanghyun Hong and Varun Chandrasekaran and Yiğitcan Kaya and Tudor Dumitraş and Nicolas Papernot},
      year={2020},
      eprint={2002.11497},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}
@inproceedings{morris2020textattack,
  title={TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP},
  author={Morris, John and Lifland, Eli and Yoo, Jin Yong and Grigsby, Jake and Jin, Di and Qi, Yanjun},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  pages={119--126},
  year={2020}
}
@inproceedings{weightpoisoningptm,
    title = "Backdoor Attacks on Pre-trained Models by Layerwise Weight Poisoning",
    author = "Li, Linyang  and
      Song, Demin  and
      Li, Xiaonan  and
      Zeng, Jiehang  and
      Ma, Ruotian  and
      Qiu, Xipeng",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.241",
    doi = "10.18653/v1/2021.emnlp-main.241",
    pages = "3023--3032",
    abstract = "\textbf{P}re-\textbf{T}rained \textbf{M}odel\textbf{s} have been widely applied and recently proved vulnerable under backdoor attacks: the released pre-trained weights can be maliciously poisoned with certain triggers. When the triggers are activated, even the fine-tuned model will predict pre-defined labels, causing a security threat. These backdoors generated by the poisoning methods can be erased by changing hyper-parameters during fine-tuning or detected by finding the triggers. In this paper, we propose a stronger weight-poisoning attack method that introduces a layerwise weight poisoning strategy to plant deeper backdoors; we also introduce a combinatorial trigger that cannot be easily detected. The experiments on text classification tasks show that previous defense methods cannot resist our weight-poisoning method, which indicates that our method can be widely applied and may provide hints for future model robustness studies.",
}
@Inbook{advmlperplex,
author="Alsmadi, Izzat",
editor="Baddi, Youssef
and Gahi, Youssef
and Maleh, Yassine
and Alazab, Mamoun
and Tawalbeh, Loai",
title="Adversarial Machine Learning, Research Trends and Applications",
bookTitle="Big Data Intelligence for Smart Applications",
year="2022",
publisher="Springer International Publishing",
address="Cham",
pages="27--55",
abstract="The intelligence extracted through machine learning algorithms (MLAs) plays an important role in most of the smart applications and systems around us. Those MLAs make intelligence decisions on behalf of humans based on knowledge extracted from historical and current data. With such growth of MLA roles in human lives, the rise of adversarial attempts to manipulate those MLAs and influence their choices is not a surprise. The main goal of this paper is to present recent approaches, models and progresses in AMLs. Additionally, our goal is to focus on AML research trends and challenges.",
isbn="978-3-030-87954-9",
doi="10.1007/978-3-030-87954-9_2",
url="https://doi.org/10.1007/978-3-030-87954-9_2"
}


@inproceedings{weightpoisoning,
    title = "Weight Poisoning Attacks on Pretrained Models",
    author = "Kurita, Keita  and
      Michel, Paul  and
      Neubig, Graham",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.249",
    doi = "10.18653/v1/2020.acl-main.249",
    pages = "2793--2806",
    abstract = "Recently, NLP has seen a surge in the usage of large pre-trained models. Users download weights of models pre-trained on large datasets, then fine-tune the weights on a task of their choice. This raises the question of whether downloading untrusted pre-trained weights can pose a security threat. In this paper, we show that it is possible to construct {``}weight poisoning{''} attacks where pre-trained weights are injected with vulnerabilities that expose {``}backdoors{''} after fine-tuning, enabling the attacker to manipulate the model prediction simply by injecting an arbitrary keyword. We show that by applying a regularization method which we call RIPPLe and an initialization procedure we call Embedding Surgery, such attacks are possible even with limited knowledge of the dataset and fine-tuning procedure. Our experiments on sentiment classification, toxicity detection, and spam detection show that this attack is widely applicable and poses a serious threat. Finally, we outline practical defenses against such attacks.",
}
@article{song2020mpnet,
  title={Mpnet: Masked and permuted pre-training for language understanding},
  author={Song, Kaitao and Tan, Xu and Qin, Tao and Lu, Jianfeng and Liu, Tie-Yan},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={16857--16867},
  year={2020}
}

@InProceedings{pmlr-v120-zhang20b,
  title = 	 {Online Data Poisoning Attacks},
  author =       {Zhang, Xuezhou and Zhu, Xiaojin and Lessard, Laurent},
  booktitle = 	 {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
  pages = 	 {201--210},
  year = 	 {2020},
  editor = 	 {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
  volume = 	 {120},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--11 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v120/zhang20b/zhang20b.pdf},
  url = 	 {https://proceedings.mlr.press/v120/zhang20b.html},
  abstract = 	 {We study data poisoning attacks in the online learning setting, where training data arrive sequentially, and the attacker is eavesdropping the data stream and has the ability to contaminate the current data point to affect the online learning process. We formulate the optimal online attack problem as a stochastic optimal control problem, and provide a systematic solution using tools from model predictive control and deep reinforcement learning. We further provide theoretical analysis on the regret suffered by the attacker for not knowing the true data sequence. Experiments validate our control approach in generating near-optimal attacks on both supervised and unsupervised learning tasks.}
}

@inproceedings{qiu2021easytransfer,
  title={EasyTransfer: A Simple and Scalable Deep Transfer Learning Platform for NLP Applications},
  author={Qiu, Minghui and Li, Peng and Wang, Chengyu and Pan, Haojie and Wang, Ang and Chen, Cen and Jia, Xianyan and Li, Yaliang and Huang, Jun and Cai, Deng and others},
  booktitle={Proceedings of the 30th ACM International Conference on Information \& Knowledge Management},
  pages={4075--4084},
  year={2021}
}
@INPROCEEDINGS{neuraltrojan,  author={Liu, Yuntao and Mondal, Ankit and Chakraborty, Abhishek and Zuzak, Michael and Jacobsen, Nina and Xing, Daniel and Srivastava, Ankur},  booktitle={2020 21st International Symposium on Quality Electronic Design (ISQED)},   title={A Survey on Neural Trojans},   year={2020},  volume={},  number={},  pages={33-39},  doi={10.1109/ISQED48828.2020.9137011}}
@InProceedings{svm_adversa,
  title = 	 {Support Vector Machines Under Adversarial Label Noise},
  author = 	 {Biggio, Battista and Nelson, Blaine and Laskov, Pavel},
  booktitle = 	 {Proceedings of the Asian Conference on Machine Learning},
  pages = 	 {97--112},
  year = 	 {2011},
  editor = 	 {Hsu, Chun-Nan and Lee, Wee Sun},
  volume = 	 {20},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {South Garden Hotels and Resorts, Taoyuan, Taiwain},
  month = 	 {14--15 Nov},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v20/biggio11/biggio11.pdf},
  url = 	 {https://proceedings.mlr.press/v20/biggio11.html},
  abstract = 	 {In adversarial classification tasks like spam filtering and intrusion detection, malicious adversaries may manipulate data to thwart the outcome of an automatic analysis. Thus, besides achieving good classification performances, machine learning algorithms have to be robust against adversarial data manipulation to successfully operate in these tasks. While support vector machines (SVMs) have shown to be a very successful approach in classification problems, their effectiveness in adversarial classification tasks has not been extensively investigated yet. In this paper we present a preliminary investigation of the robustness of SVMs against adversarial data manipulation. In particular, we assume that the adversary has control over some training data, and aims to subvert the SVM learning process. Within this assumption, we show that this is indeed possible, and propose a strategy to improve the robustness of SVMs to training data manipulation based on a simple kernel matrix correction.}
}

@inproceedings{svm_logist,
  author    = {Mengchen Zhao and Bo An and Wei Gao and Teng Zhang},
  title     = {Efficient Label Contamination Attacks Against Black-Box Learning Models},
  booktitle = {Proceedings of the Twenty-Sixth International Joint Conference on
               Artificial Intelligence, {IJCAI-17}},
  pages     = {3945--3951},
  year      = {2017},
  doi       = {10.24963/ijcai.2017/551},
  url       = {https://doi.org/10.24963/ijcai.2017/551},
}

@inproceedings{universal_triggers,
    title = "Universal Adversarial Triggers for Attacking and Analyzing {NLP}",
    author = "Wallace, Eric  and
      Feng, Shi  and
      Kandpal, Nikhil  and
      Gardner, Matt  and
      Singh, Sameer",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1221",
    doi = "10.18653/v1/D19-1221",
    pages = "2153--2162",
    abstract = "Adversarial examples highlight model vulnerabilities and are useful for evaluation and interpretation. We define universal adversarial triggers: input-agnostic sequences of tokens that trigger a model to produce a specific prediction when concatenated to any input from a dataset. We propose a gradient-guided search over tokens which finds short trigger sequences (e.g., one word for classification and four words for language modeling) that successfully trigger the target prediction. For example, triggers cause SNLI entailment accuracy to drop from 89.94{\%} to 0.55{\%}, 72{\%} of {``}why{''} questions in SQuAD to be answered {``}to kill american people{''}, and the GPT-2 language model to spew racist output even when conditioned on non-racial contexts. Furthermore, although the triggers are optimized using white-box access to a specific model, they transfer to other models for all tasks we consider. Finally, since triggers are input-agnostic, they provide an analysis of global model behavior. For instance, they confirm that SNLI models exploit dataset biases and help to diagnose heuristics learned by reading comprehension models.",
}
@inproceedings{activationclustering,
  title={Detecting backdoor attacks on deep neural networks by activation clustering},
  author={Chen, Bryant and Carvalho, Wilka and Baracaldo, Nathalie and Ludwig, Heiko and Edwards, Benjamin and Lee, Taesung and Molloy, Ian and Srivastava, Biplav},
  booktitle={Workshop on Artificial Intelligence Safety},
  year={2019},
  organization={CEUR-WS}
}

@misc{nadistillation,
      title={Neural Attention Distillation: Erasing Backdoor Triggers from Deep Neural Networks}, 
      author={Yige Li and Xixiang Lyu and Nodens Koren and Lingjuan Lyu and Bo Li and Xingjun Ma},
      year={2021},
      eprint={2101.05930},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@inproceedings{universalforclassification,
    title = "Universal Adversarial Attacks with Natural Triggers for Text Classification",
    author = "Song, Liwei  and
      Yu, Xinwei  and
      Peng, Hsuan-Tung  and
      Narasimhan, Karthik",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.291",
    doi = "10.18653/v1/2021.naacl-main.291",
    pages = "3724--3733",
    abstract = "Recent work has demonstrated the vulnerability of modern text classifiers to universal adversarial attacks, which are input-agnostic sequences of words added to text processed by classifiers. Despite being successful, the word sequences produced in such attacks are often ungrammatical and can be easily distinguished from natural text. We develop adversarial attacks that appear closer to natural English phrases and yet confuse classification systems when added to benign inputs. We leverage an adversarially regularized autoencoder (ARAE) to generate triggers and propose a gradient-based search that aims to maximize the downstream classifier{'}s prediction loss. Our attacks effectively reduce model accuracy on classification tasks while being less identifiable than prior models as per automatic detection metrics and human-subject studies. Our aim is to demonstrate that adversarial attacks can be made harder to detect than previously thought and to enable the development of appropriate defenses.",
}
@inproceedings{sweetrabit_universal,
    title = "A Sweet Rabbit Hole by {DARCY}: Using Honeypots to Detect Universal Trigger{'}s Adversarial Attacks",
    author = "Le, Thai  and
      Park, Noseong  and
      Lee, Dongwon",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.296",
    doi = "10.18653/v1/2021.acl-long.296",
    pages = "3831--3844",
    abstract = "The Universal Trigger (UniTrigger) is a recently-proposed powerful adversarial textual attack method. Utilizing a learning-based mechanism, UniTrigger generates a fixed phrase that, when added to any benign inputs, can drop the prediction accuracy of a textual neural network (NN) model to near zero on a target class. To defend against this attack that can cause significant harm, in this paper, we borrow the {``}honeypot{''} concept from the cybersecurity community and propose DARCY, a honeypot-based defense framework against UniTrigger. DARCY greedily searches and injects multiple trapdoors into an NN model to {``}bait and catch{''} potential attacks. Through comprehensive experiments across four public datasets, we show that DARCY detects UniTrigger{'}s adversarial attacks with up to 99{\%} TPR and less than 2{\%} FPR in most cases, while maintaining the prediction accuracy (in F1) for clean inputs within a 1{\%} margin. We also demonstrate that DARCY with multiple trapdoors is also robust to a diverse set of attack scenarios with attackers{'} varying levels of knowledge and skills. We release the source code of DARCY at: https://github.com/lethaiq/ACL2021-DARCY-HoneypotDefenseNLP.",
}
@inbook{backgradient,
author = {Mu\~{n}oz-Gonz\'{a}lez, Luis and Biggio, Battista and Demontis, Ambra and Paudice, Andrea and Wongrassamee, Vasin and Lupu, Emil C. and Roli, Fabio},
title = {Towards Poisoning of Deep Learning Algorithms with Back-Gradient Optimization},
year = {2017},
isbn = {9781450352024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3128572.3140451},
abstract = {A number of online services nowadays rely upon machine learning to extract valuable
information from data collected in the wild. This exposes learning algorithms to the
threat of data poisoning, i.e., a coordinate attack in which a fraction of the training
data is controlled by the attacker and manipulated to subvert the learning process.
To date, these attacks have been devised only against a limited class of binary learning
algorithms, due to the inherent complexity of the gradient-based procedure used to
optimize the poisoning points (a.k.a. adversarial training examples). In this work,
we first extend the definition of poisoning attacks to multiclass problems. We then
propose a novel poisoning algorithm based on the idea of back-gradient optimization,
i.e., to compute the gradient of interest through automatic differentiation, while
also reversing the learning procedure to drastically reduce the attack complexity.
Compared to current poisoning strategies, our approach is able to target a wider class
of learning algorithms, trained with gradient-based procedures, including neural networks
and deep learning architectures. We empirically evaluate its effectiveness on several
application examples, including spam filtering, malware detection, and handwritten
digit recognition. We finally show that, similarly to adversarial test examples, adversarial
training examples can also be transferred across different learning algorithms.},
booktitle = {Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security},
pages = {27–38},
numpages = {12}
}
@inproceedings{availa_data,
  title={Is feature selection secure against training data poisoning?},
  author={Xiao, Huang and Biggio, Battista and Brown, Gavin and Fumera, Giorgio and Eckert, Claudia and Roli, Fabio},
  booktitle={international conference on machine learning},
  pages={1689--1698},
  year={2015},
  organization={PMLR}
}
@inproceedings{availability_at,
author = {Mei, Shike and Zhu, Xiaojin},
title = {Using Machine Teaching to Identify Optimal Training-Set Attacks on Machine Learners},
year = {2015},
isbn = {0262511290},
publisher = {AAAI Press},
abstract = {We investigate a problem at the intersection of machine learning and security: training-set attacks on machine learners. In such attacks an attacker contaminates the training data so that a specific learning algorithm would produce a model profitable to the attacker. Understanding training-set attacks is important as more intelligent agents (e.g. spam filters and robots) are equipped with learning capability and can potentially be hacked via data they receive from the environment. This paper identifies the optimal training-set attack on a broad family of machine learners. First we show that optimal training-set attack can be formulated as a bilevel optimization problem. Then we show that for machine learners with certain Karush-Kuhn-Tucker conditions we can solve the bilevel problem efficiently using gradient methods on an implicit function. As examples, we demonstrate optimal training-set attacks on Support Vector Machines, logistic regression, and linear regression with extensive experiments. Finally, we discuss potential defenses against such attacks.},
booktitle = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
pages = {2871–2877},
numpages = {7},
location = {Austin, Texas},
series = {AAAI'15}
}
@inproceedings{availablity,
author = {Biggio, Battista and Nelson, Blaine and Laskov, Pavel},
title = {Poisoning Attacks against Support Vector Machines},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We investigate a family of poisoning attacks against Support Vector Machines (SVM). Such attacks inject specially crafted training data that increases the SVM's test error. Central to the motivation for these attacks is the fact that most learning algorithms assume that their training data comes from a natural or well-behaved distribution. However, this assumption does not generally hold in security-sensitive settings. As we demonstrate, an intelligent adversary can, to some extent, predict the change of the SVM's decision function due to malicious input and use this ability to construct malicious data.The proposed attack uses a gradient ascent strategy in which the gradient is computed based on properties of the SVM's optimal solution. This method can be kernelized and enables the attack to be constructed in the input space even for non-linear kernels. We experimentally demonstrate that our gradient ascent procedure reliably identifies good local maxima of the non-convex validation error surface, which significantly increases the classifier's test error.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1467–1474},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}
@article{subpopulation,
  author    = {Matthew Jagielski and
               Giorgio Severi and
               Niklas Pousette Harger and
               Alina Oprea},
  title     = {Subpopulation Data Poisoning Attacks},
  journal   = {CoRR},
  volume    = {abs/2006.14026},
  year      = {2020},
  url       = {https://arxiv.org/abs/2006.14026},
  eprinttype = {arXiv},
  eprint    = {2006.14026},
  timestamp = {Wed, 01 Jul 2020 15:21:23 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2006-14026.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{regression,
  title={Manipulating machine learning: Poisoning attacks and countermeasures for regression learning},
  author={Jagielski, Matthew and Oprea, Alina and Biggio, Battista and Liu, Chang and Nita-Rotaru, Cristina and Li, Bo},
  booktitle={2018 IEEE Symposium on Security and Privacy (SP)},
  pages={19--35},
  year={2018},
  organization={IEEE}
}
@article{koh2022stronger,
  title={Stronger data poisoning attacks break data sanitization defenses},
  author={Koh, Pang Wei and Steinhardt, Jacob and Liang, Percy},
  journal={Machine Learning},
  volume={111},
  number={1},
  pages={1--47},
  year={2022},
  publisher={Springer}
}
@inproceedings{la-malfa-etal-2020-assessing,
    title = "Assessing Robustness of Text Classification through Maximal Safe Radius Computation",
    author = "La Malfa, Emanuele  and
      Wu, Min  and
      Laurenti, Luca  and
      Wang, Benjie  and
      Hartshorn, Anthony  and
      Kwiatkowska, Marta",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.266",
    doi = "10.18653/v1/2020.findings-emnlp.266",
    pages = "2949--2968",
    abstract = "Neural network NLP models are vulnerable to small modifications of the input that maintain the original meaning but result in a different prediction. In this paper, we focus on robustness of text classification against word substitutions, aiming to provide guarantees that the model prediction does not change if a word is replaced with a plausible alternative, such as a synonym. As a measure of robustness, we adopt the notion of the maximal safe radius for a given input text, which is the minimum distance in the embedding space to the decision boundary. Since computing the exact maximal safe radius is not feasible in practice, we instead approximate it by computing a lower and upper bound. For the upper bound computation, we employ Monte Carlo Tree Search in conjunction with syntactic filtering to analyse the effect of single and multiple word substitutions. The lower bound computation is achieved through an adaptation of the linear bounding techniques implemented in tools CNN-Cert and POPQORN, respectively for convolutional and recurrent network models. We evaluate the methods on sentiment analysis and news classification models for four datasets (IMDB, SST, AG News and NEWS) and a range of embeddings, and provide an analysis of robustness trends. We also apply our framework to interpretability analysis and compare it with LIME.",
}
@article{ma2022dangerous,
  title={Dangerous Cloaking: Natural Trigger based Backdoor Attacks on Object Detectors in the Physical World},
  author={Ma, Hua and Li, Yinshan and Gao, Yansong and Abuadbba, Alsharif and Zhang, Zhi and Fu, Anmin and Kim, Hyoungshick and Al-Sarawi, Said F and Surya, Nepal and Abbott, Derek},
  journal={arXiv preprint arXiv:2201.08619},
  year={2022}
}
@article{dnn_ner,
  title={Exploring Data and Model Poisoning Attacks to Deep Learning-Based NLP Systems},
  author={Marulli, Fiammetta and Verde, Laura and Campanile, Lelio},
  journal={Procedia Computer Science},
  volume={192},
  pages={3570--3579},
  year={2021},
  publisher={Elsevier}
}

@article{adv_attackanddefenseongraphs,
author = {Jin, Wei and Li, Yaxing and Xu, Han and Wang, Yiqi and Ji, Shuiwang and Aggarwal, Charu and Tang, Jiliang},
title = {Adversarial Attacks and Defenses on Graphs},
year = {2021},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {1931-0145},
url = {https://doi.org/10.1145/3447556.3447566},
doi = {10.1145/3447556.3447566},
abstract = {Deep neural networks (DNNs) have achieved significant performance in various tasks. However, recent studies have shown that DNNs can be easily fooled by small perturbation on the input, called adversarial attacks.},
journal = {SIGKDD Explor. Newsl.},
month = {jan},
pages = {19–34},
numpages = {16}
}
@inproceedings{meng2017magnet,
  title={Magnet: a two-pronged defense against adversarial examples},
  author={Meng, Dongyu and Chen, Hao},
  booktitle={Proceedings of the 2017 ACM SIGSAC conference on computer and communications security},
  pages={135--147},
  year={2017}
}
@article{geiping2020witches,
  title={Witches' brew: Industrial scale data poisoning via gradient matching},
  author={Geiping, Jonas and Fowl, Liam and Huang, W Ronny and Czaja, Wojciech and Taylor, Gavin and Moeller, Michael and Goldstein, Tom},
  journal={arXiv preprint arXiv:2009.02276},
  year={2020}
}
@inproceedings{carlini2021extracting,
  title={Extracting training data from large language models},
  author={Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Ulfar and others},
  booktitle={30th USENIX Security Symposium (USENIX Security 21)},
  pages={2633--2650},
  year={2021}
}
@article{qi2021hidden,
  title={Hidden killer: Invisible textual backdoor attacks with syntactic trigger},
  author={Qi, Fanchao and Li, Mukai and Chen, Yangyi and Zhang, Zhengyan and Liu, Zhiyuan and Wang, Yasheng and Sun, Maosong},
  journal={arXiv preprint arXiv:2105.12400},
  year={2021}
}

@article{hidden_backdoors,
  author    = {Shaofeng Li and
               Hui Liu and
               Tian Dong and
               Benjamin Zi Hao Zhao and
               Minhui Xue and
               Haojin Zhu and
               Jialiang Lu},
  title     = {Hidden Backdoors in Human-Centric Language Models},
  journal   = {CoRR},
  volume    = {abs/2105.00164},
  year      = {2021},
  url       = {https://arxiv.org/abs/2105.00164},
  eprinttype = {arXiv},
  eprint    = {2105.00164},
  timestamp = {Wed, 12 May 2021 15:54:31 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2105-00164.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{guo2019tabor,
  title={Tabor: A highly accurate approach to inspecting and restoring trojan backdoors in ai systems},
  author={Guo, Wenbo and Wang, Lun and Xing, Xinyu and Du, Min and Song, Dawn},
  journal={arXiv preprint arXiv:1908.01763},
  year={2019}
}
@inproceedings{chen2019deepinspect,
  title={DeepInspect: A Black-box Trojan Detection and Mitigation Framework for Deep Neural Networks.},
  author={Chen, Huili and Fu, Cheng and Zhao, Jishen and Koushanfar, Farinaz},
  booktitle={IJCAI},
  pages={4658--4664},
  year={2019}
}
@inproceedings{tminer,
  title={T-Miner: A Generative Approach to Defend Against Trojan Attacks on DNN-based Text Classification},
  author={Azizi, Ahmadreza and Tahmid, Ibrahim Asadullah and Waheed, Asim and Mangaokar, Neal and Pu, Jiameng and Javed, Mobin and Reddy, Chandan K and Viswanath, Bimal},
  booktitle={30th $\{$USENIX$\}$ Security Symposium ($\{$USENIX$\}$ Security 21)},
  year={2021}
}
@inproceedings{wang2019neural,
  title={Neural cleanse: Identifying and mitigating backdoor attacks in neural networks},
  author={Wang, Bolun and Yao, Yuanshun and Shan, Shawn and Li, Huiying and Viswanath, Bimal and Zheng, Haitao and Zhao, Ben Y},
  booktitle={2019 IEEE Symposium on Security and Privacy (SP)},
  pages={707--723},
  year={2019},
  organization={IEEE}
}
@inproceedings{trojandetection,
  title={Detecting ai trojans using meta neural analysis},
  author={Xu, Xiaojun and Wang, Qi and Li, Huichen and Borisov, Nikita and Gunter, Carl A and Li, Bo},
  booktitle={2021 IEEE Symposium on Security and Privacy (SP)},
  pages={103--120},
  year={2021},
  organization={IEEE}
}
@ARTICLE{rnn_lstm_gru,  author={Fan, Ming and Si, Ziliang and Xie, Xiaofei and Liu, Yang and Liu, Ting},  journal={IEEE Transactions on Information Forensics and Security},   title={Text Backdoor Detection Using an Interpretable RNN Abstract Model},   year={2021},  volume={16},  number={},  pages={4117-4132},  doi={10.1109/TIFS.2021.3103064}}
@INPROCEEDINGS{robust_nn,  author={Carlini, Nicholas and Wagner, David},  booktitle={2017 IEEE Symposium on Security and Privacy (SP)},   title={Towards Evaluating the Robustness of Neural Networks},   year={2017},  volume={},  number={},  pages={39-57},  doi={10.1109/SP.2017.49}}
@article{trojanfun,
  author    = {Xinyang Zhang and
               Zheng Zhang and
               Ting Wang},
  title     = {Trojaning Language Models for Fun and Profit},
  journal   = {CoRR},
  volume    = {abs/2008.00312},
  year      = {2020},
  url       = {https://arxiv.org/abs/2008.00312},
  eprinttype = {arXiv},
  eprint    = {2008.00312},
  timestamp = {Wed, 15 Sep 2021 11:19:43 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2008-00312.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@INPROCEEDINGS{neuralcleanse,  author={Wang, Bolun and Yao, Yuanshun and Shan, Shawn and Li, Huiying and Viswanath, Bimal and Zheng, Haitao and Zhao, Ben Y.},  booktitle={2019 IEEE Symposium on Security and Privacy (SP)},   title={Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks},   year={2019},  volume={},  number={},  pages={707-723},  doi={10.1109/SP.2019.00031}}
@inproceedings{finepruning,
  title={Fine-pruning: Defending against backdooring attacks on deep neural networks},
  author={Liu, Kang and Dolan-Gavitt, Brendan and Garg, Siddharth},
  booktitle={International Symposium on Research in Attacks, Intrusions, and Defenses},
  pages={273--294},
  year={2018},
  organization={Springer}
}
@inproceedings{toxicitycosta,
    title = "Toxicity in Multilingual Machine Translation at Scale",
    author = "Costa-juss{\`a}, Marta  and
      Smith, Eric  and
      Ropers, Christophe  and
      Licht, Daniel  and
      Maillard, Jean  and
      Ferrando, Javier  and
      Escolano, Carlos",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.642",
    doi = "10.18653/v1/2023.findings-emnlp.642",
    pages = "9570--9586",
    abstract = "Machine Translation systems can produce different types of errors, some of which are characterized as critical or catastrophic due to the specific negative impact that they can have on users. In this paper we focus on one type of critical error: added toxicity. We evaluate and analyze added toxicity when translating a large evaluation dataset (HOLISTICBIAS, over 472k sentences, covering 13 demographic axes) from English into 164 languages. An automatic toxicity evaluation shows that added toxicity across languages varies from 0{\%} to 5{\%}. The output languages with the most added toxicity tend to be low-resource ones, and the demographic axes with the most added toxicity include sexual orientation, gender and sex, and ability. We also perform human evaluation on a subset of 8 translation directions, confirming the prevalence of true added toxicity. We use a measurement of the amount of source contribution to the translation, where a low source contribution implies hallucination, to interpret what causes toxicity. Making use of the input attributions allows us to explain toxicity, because the source contributions significantly correlate with toxicity for 84{\%} of languages studied. Given our findings, our recommendations to reduce added toxicity are to curate training data to avoid mistranslations, mitigate hallucination and check unstable translations.",
}
@inproceedings{mtbackdoor,
    title = "Backdoor Attacks on Multilingual Machine Translation",
    author = "Wang, Jun  and
      Xu, Qiongkai  and
      He, Xuanli  and
      Rubinstein, Benjamin  and
      Cohn, Trevor",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.254",
    doi = "10.18653/v1/2024.naacl-long.254",
    pages = "4515--4534",
    abstract = "While multilingual machine translation (MNMT) systems hold substantial promise, they also have security vulnerabilities. Our research highlights that MNMT systems can be susceptible to a particularly devious style of backdoor attack, whereby an attacker injects poisoned data into a low-resource language pair to cause malicious translations in other languages, including high-resource languages.Our experimental results reveal that injecting less than 0.01{\%} poisoned data into a low-resource language pair can achieve an average 20{\%} attack success rate in attacking high-resource language pairs. This type of attack is of particular concern, given the larger attack surface of languages inherent to low-resource settings. Our aim is to bring attention to these vulnerabilities within MNMT systems with the hope of encouraging the community to address security concerns in machine translation, especially in the context of low-resource languages.",
}
@inproceedings{zhao2018resilience,
  title={Resilience of pruned neural network against poisoning attack},
  author={Zhao, Bingyin and Lao, Yingjie},
  booktitle={2018 13th International Conference on Malicious and Unwanted Software (MALWARE)},
  pages={78--83},
  year={2018},
  organization={IEEE}
}
@inproceedings{Guan2019NeuralBI,
  title={Neural Backdoors in NLP},
  author={Andrew Guan},
  year={2019}
}

@article{wang2020backdoor,
  title={Backdoor attacks against transfer learning with pre-trained deep learning models},
  author={Wang, Shuo and Nepal, Surya and Rudolph, Carsten and Grobler, Marthie and Chen, Shangyu and Chen, Tianle},
  journal={IEEE Transactions on Services Computing},
  year={2020},
  publisher={IEEE}
}
@article{badnets,
  author    = {Tianyu Gu and
               Brendan Dolan{-}Gavitt and
               Siddharth Garg},
  title     = {BadNets: Identifying Vulnerabilities in the Machine Learning Model
               Supply Chain},
  journal   = {CoRR},
  volume    = {abs/1708.06733},
  year      = {2017},
  url       = {http://arxiv.org/abs/1708.06733},
  eprinttype = {arXiv},
  eprint    = {1708.06733},
  timestamp = {Mon, 13 Aug 2018 16:49:00 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1708-06733.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}
@article{nelson2008exploiting,
  title={Exploiting machine learning to subvert your spam filter.},
  author={Nelson, Blaine and Barreno, Marco and Chi, Fuching Jack and Joseph, Anthony D and Rubinstein, Benjamin IP and Saini, Udam and Sutton, Charles and Tygar, J Doug and Xia, Kai},
  journal={LEET},
  volume={8},
  number={1-9},
  pages={16--17},
  year={2008}
}
@article{turner2019label,
  title={Label-consistent backdoor attacks},
  author={Turner, Alexander and Tsipras, Dimitris and Madry, Aleksander},
  journal={arXiv preprint arXiv:1912.02771},
  year={2019}
}
@article{certifiedjia2020,
  title={Certified robustness of nearest neighbors against data poisoning attacks},
  author={Jia, Jinyuan and Cao, Xiaoyu and Gong, Neil Zhenqiang},
  journal={arXiv preprint arXiv:2012.03765},
  year={2020}
}
@inproceedings{targeted_blackbox,
author = {Xu, Chang and Wang, Jun and Tang, Yuqing and Guzm\'{a}n, Francisco and Rubinstein, Benjamin I. P. and Cohn, Trevor},
title = {A Targeted Attack on Black-Box Neural Machine Translation with Parallel Data Poisoning},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3450034},
doi = {10.1145/3442381.3450034},
abstract = { As modern neural machine translation (NMT) systems have been widely deployed, their security vulnerabilities require close scrutiny. Most recently, NMT systems have been found vulnerable to targeted attacks which cause them to produce specific, unsolicited, and even harmful translations. These attacks are usually exploited in a white-box setting, where adversarial inputs causing targeted translations are discovered for a known target system. However, this approach is less viable when the target system is black-box and unknown to the adversary (e.g., secured commercial systems). In this paper, we show that targeted attacks on black-box NMT systems are feasible, based on poisoning a small fraction of their parallel training data. We show that this attack can be realised practically via targeted corruption of web documents crawled to form the system’s training data. We then analyse the effectiveness of the targeted poisoning in two common NMT training scenarios: the from-scratch training and the pre-train &amp; fine-tune paradigm. Our results are alarming: even on the state-of-the-art systems trained with massive parallel data (tens of millions), the attacks are still successful (over 50% success rate) under surprisingly low poisoning budgets (e.g., 0.006%). Lastly, we discuss potential defences to counter such attacks.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {3638–3650},
numpages = {13},
keywords = {neural machine translation, black-box attacks, data poisoning},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}
@inproceedings{wan2023poisoning,
  title={Poisoning language models during instruction tuning},
  author={Wan, Alexander and Wallace, Eric and Shen, Sheng and Klein, Dan},
  booktitle={International Conference on Machine Learning},
  pages={35413--35425},
  year={2023},
  organization={PMLR}
}
@inproceedings{putting_words,
    title = "Putting words into the system{'}s mouth: A targeted attack on neural machine translation using monolingual data poisoning",
    author = "Wang, Jun  and
      Xu, Chang  and
      Guzm{\'a}n, Francisco  and
      El-Kishky, Ahmed  and
      Tang, Yuqing  and
      Rubinstein, Benjamin  and
      Cohn, Trevor",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.127",
    doi = "10.18653/v1/2021.findings-acl.127",
    pages = "1463--1473",
}
@INPROCEEDINGS{pruningnn,  author={Zhao, Bingyin and Lao, Yingjie},  booktitle={2018 13th International Conference on Malicious and Unwanted Software (MALWARE)},   title={Resilience of Pruned Neural Network Against Poisoning Attack},   year={2018},  volume={},  number={},  pages={78-83},  doi={10.1109/MALWARE.2018.8659362}}
@article{graident_based,
  author    = {David Solans and
               Battista Biggio and
               Carlos Castillo},
  title     = {Poisoning Attacks on Algorithmic Fairness},
  journal   = {CoRR},
  volume    = {abs/2004.07401},
  year      = {2020},
  url       = {https://arxiv.org/abs/2004.07401},
  eprinttype = {arXiv},
  eprint    = {2004.07401},
  timestamp = {Tue, 21 Apr 2020 16:51:52 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-07401.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{label_sanitization,
  title={Label sanitization against label flipping poisoning attacks},
  author={Paudice, Andrea and Mu{\~n}oz-Gonz{\'a}lez, Luis and Lupu, Emil C},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={5--15},
  year={2018},
  organization={Springer}
}
@inproceedings{jagielski2018manipulating,
  title={Manipulating machine learning: Poisoning attacks and countermeasures for regression learning},
  author={Jagielski, Matthew and Oprea, Alina and Biggio, Battista and Liu, Chang and Nita-Rotaru, Cristina and Li, Bo},
  booktitle={2018 IEEE Symposium on Security and Privacy (SP)},
  pages={19--35},
  year={2018},
  organization={IEEE}
}
@inproceedings{steinhardt2017certified,
  title={Certified defenses for data poisoning attacks},
  author={Steinhardt, Jacob and Koh, Pang Wei and Liang, Percy},
  booktitle={Proceedings of the 31st International Conference on Neural Information Processing Systems},
  pages={3520--3532},
  year={2017}
}
@inproceedings{randomizedsmoothing,
  title={Certified robustness to label-flipping attacks via randomized smoothing},
  author={Rosenfeld, Elan and Winston, Ezra and Ravikumar, Pradeep and Kolter, Zico},
  booktitle={International Conference on Machine Learning},
  pages={8230--8241},
  year={2020},
  organization={PMLR}
}

@misc{datasetsecurity,
    title={Dataset Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses},
    author={Micah Goldblum and Dimitris Tsipras and Chulin Xie and Xinyun Chen and Avi Schwarzschild and Dawn Song and Aleksander Madry and Bo Li and Tom Goldstein},
    year={2020},
    eprint={2012.10544},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@misc{membership,
    title={Membership Inference Attacks on Machine Learning: A Survey},
    author={Hongsheng Hu and Zoran Salcic and Lichao Sun and Gillian Dobbie and Philip S. Yu and Xuyun Zhang},
    year={2021},
    eprint={2103.07853},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@article{artificialsec,
  title={Artificial Intelligence Security: Threats and Countermeasures},
  author={Hu, Yupeng and Kuang, Wenxin and Qin, Zheng and Li, Kenli and Zhang, Jiliang and Gao, Yansong and Li, Wenjia and Li, Keqin},
  journal={ACM Computing Surveys (CSUR)},
  volume={55},
  number={1},
  pages={1--36},
  year={2021},
  publisher={ACM New York, NY}
}
@article{poisoningattacks,
title = {Poisoning attacks and countermeasures in intelligent networks: Status quo and prospects},
journal = {Digital Communications and Networks},
year = {2021},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2021.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S235286482100050X},
author = {Chen Wang and Jian Chen and Yang Yang and Xiaoqiang Ma and Jiangchuan Liu},
keywords = {Machine learning, Poisoning attack, Intelligent networks, Security threat},
abstract = {Over the past years, the emergence of intelligent networks empowered by machine learning techniques has brought great facilitates to different aspects of human life. However, using machine learning in intelligent networks also presents potential security and privacy threats. A common practice is the so-called poisoning attacks where malicious users inject fake training data with the aim of corrupting the learned model. In this survey, we comprehensively review existing poisoning attacks as well as the countermeasures in intelligent networks for the first time. We emphasize and compare the principles of the formal poisoning attacks employed in different categories of learning algorithms, and analyze the strengths and limitations of corresponding defense methods in a compact form. We also highlight some remaining challenges and future directions in the attack-defense confrontation to promote further research in this emerging yet promising area.}
}
@article{adv_survey,
author = {Zhang, Wei Emma and Sheng, Quan Z. and Alhazmi, Ahoud and Li, Chenliang},
title = {Adversarial Attacks on Deep-Learning Models in Natural Language Processing: A Survey},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3374217},
doi = {10.1145/3374217},
abstract = {With the development of high computational devices, deep neural networks (DNNs), in recent years, have gained significant popularity in many Artificial Intelligence (AI) applications. However, previous efforts have shown that DNNs are vulnerable to strategically modified samples, named adversarial examples. These samples are generated with some imperceptible perturbations, but can fool the DNNs to give false predictions. Inspired by the popularity of generating adversarial examples against DNNs in Computer Vision (CV), research efforts on attacking DNNs for Natural Language Processing (NLP) applications have emerged in recent years. However, the intrinsic difference between image (CV) and text (NLP) renders challenges to directly apply attacking methods in CV to NLP. Various methods are proposed addressing this difference and attack a wide range of NLP applications. In this article, we present a systematic survey on these works. We collect all related academic works since the first appearance in 2017. We then select, summarize, discuss, and analyze 40 representative works in a comprehensive way. To make the article self-contained, we cover preliminary knowledge of NLP and discuss related seminal works in computer vision. We conclude our survey with a discussion on open issues to bridge the gap between the existing progress and more robust adversarial attacks on NLP DNNs.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {apr},
articleno = {24},
numpages = {41},
keywords = {Deep neural networks, adversarial examples, natural language processing, textual data}
}
@article{advattacks_def,
title = {Adversarial Attacks and Defenses in Deep Learning},
journal = {Engineering},
volume = {6},
number = {3},
pages = {346-360},
year = {2020},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2019.12.012},
url = {https://www.sciencedirect.com/science/article/pii/S209580991930503X},
author = {Kui Ren and Tianhang Zheng and Zhan Qin and Xue Liu},
keywords = {Machine learning, Deep neural network, Adversarial example, Adversarial attack, Adversarial defense},
abstract = {With the rapid developments of artificial intelligence (AI) and deep learning (DL) techniques, it is critical to ensure the security and robustness of the deployed algorithms. Recently, the security vulnerability of DL algorithms to adversarial samples has been widely recognized. The fabricated samples can lead to various misbehaviors of the DL models while being perceived as benign by humans. Successful implementations of adversarial attacks in real physical-world scenarios further demonstrate their practicality. Hence, adversarial attack and defense techniques have attracted increasing attention from both machine learning and security communities and have become a hot research topic in recent years. In this paper, we first introduce the theoretical foundations, algorithms, and applications of adversarial attack techniques. We then describe a few research efforts on the defense techniques, which cover the broad frontier in the field. Several open problems and challenges are subsequently discussed, which we hope will provoke further research efforts in this critical area.}
}
@article{zhang2019adversarial,
  title={Adversarial examples: Opportunities and challenges},
  author={Zhang, Jiliang and Li, Chen},
  journal={IEEE transactions on neural networks and learning systems},
  volume={31},
  number={7},
  pages={2578--2593},
  year={2019},
  publisher={IEEE}
}
@article{backdoor_review,
  author    = {Yansong Gao and
               Bao Gia Doan and
               Zhi Zhang and
               Siqi Ma and
               Jiliang Zhang and
               Anmin Fu and
               Surya Nepal and
               Hyoungshick Kim},
  title     = {Backdoor Attacks and Countermeasures on Deep Learning: {A} Comprehensive
               Review},
  journal   = {CoRR},
  volume    = {abs/2007.10760},
  year      = {2020},
  url       = {https://arxiv.org/abs/2007.10760},
  eprinttype = {arXiv},
  eprint    = {2007.10760},
  timestamp = {Thu, 13 Aug 2020 09:46:05 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2007-10760.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{WANG2021,
title = {Poisoning attacks and countermeasures in intelligent networks: Status quo and prospects},
journal = {Digital Communications and Networks},
year = {2021},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2021.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S235286482100050X},
author = {Chen Wang and Jian Chen and Yang Yang and Xiaoqiang Ma and Jiangchuan Liu},
keywords = {Machine learning, Poisoning attack, Intelligent networks, Security threat},
abstract = {Over the past years, the emergence of intelligent networks empowered by machine learning techniques has brought great facilitates to different aspects of human life. However, using machine learning in intelligent networks also presents potential security and privacy threats. A common practice is the so-called poisoning attacks where malicious users inject fake training data with the aim of corrupting the learned model. In this survey, we comprehensively review existing poisoning attacks as well as the countermeasures in intelligent networks for the first time. We emphasize and compare the principles of the formal poisoning attacks employed in different categories of learning algorithms, and analyze the strengths and limitations of corresponding defense methods in a compact form. We also highlight some remaining challenges and future directions in the attack-defense confrontation to promote further research in this emerging yet promising area.}
}
@inproceedings{hotflip,
    title = "{H}ot{F}lip: White-Box Adversarial Examples for Text Classification",
    author = "Ebrahimi, Javid  and
      Rao, Anyi  and
      Lowd, Daniel  and
      Dou, Dejing",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-2006",
    doi = "10.18653/v1/P18-2006",
    pages = "31--36",
    abstract = "We propose an efficient method to generate white-box adversarial examples to trick a character-level neural classifier. We find that only a few manipulations are needed to greatly decrease the accuracy. Our method relies on an atomic flip operation, which swaps one token for another, based on the gradients of the one-hot input vectors. Due to efficiency of our method, we can perform adversarial training which makes the model more robust to attacks at test time. With the use of a few semantics-preserving constraints, we demonstrate that HotFlip can be adapted to attack a word-level classifier as well.",
}
@article{trickmeifyoucan,
    author = {Wallace, Eric and Rodriguez, Pedro and Feng, Shi and Yamada, Ikuya and Boyd-Graber, Jordan},
    title = "{Trick Me If You Can: Human-in-the-Loop Generation of Adversarial Examples for Question Answering}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {7},
    pages = {387-401},
    year = {2019},
    month = {07},
    abstract = "{Adversarial evaluation stress-tests a model’s understanding of natural language. Because past approaches expose superficial patterns, the resulting adversarial examples are limited in complexity and diversity. We propose human- in-the-loop adversarial generation, where human authors are guided to break models. We aid the authors with interpretations of model predictions through an interactive user interface. We apply this generation framework to a question answering task called Quizbowl, where trivia enthusiasts craft adversarial questions. The resulting questions are validated via live human–computer matches: Although the questions appear ordinary to humans, they systematically stump neural and information retrieval models. The adversarial questions cover diverse phenomena from multi-hop reasoning to entity type distractors, exposing open challenges in robust question answering.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00279},
    url = {https://doi.org/10.1162/tacl\_a\_00279},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00279/1923125/tacl\_a\_00279.pdf},
}


@inproceedings{biggio2018wild,
  title={Wild patterns: Ten years after the rise of adversarial machine learning half-day tutorial},
  author={Biggio, B and Roli, F},
  booktitle={25th ACM Conference on Computer and Communications Security, CCS 2018},
  pages={2154--2156},
  year={2018},
  organization={Association for Computing Machinery}
}
@article{hypernetworks,
  title={Hypernetworks},
  author={Ha, David and Dai, Andrew and Le, Quoc V},
  journal={arXiv preprint arXiv:1609.09106},
  year={2016}
}
@inproceedings{fact-editing,
    title = "Editing Factual Knowledge in Language Models",
    author = "De Cao, Nicola  and
      Aziz, Wilker  and
      Titov, Ivan",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.522",
    doi = "10.18653/v1/2021.emnlp-main.522",
    pages = "6491--6506",
    abstract = "The factual knowledge acquired during pre-training and stored in the parameters of Language Models (LMs) can be useful in downstream tasks (e.g., question answering or textual inference). However, some facts can be incorrectly induced or become obsolete over time. We present KnowledgeEditor, a method which can be used to edit this knowledge and, thus, fix {`}bugs{'} or unexpected predictions without the need for expensive re-training or fine-tuning. Besides being computationally efficient, KnowledgeEditordoes not require any modifications in LM pre-training (e.g., the use of meta-learning). In our approach, we train a hyper-network with constrained optimization to modify a fact without affecting the rest of the knowledge; the trained hyper-network is then used to predict the weight update at test time. We show KnowledgeEditor{'}s efficacy with two popular architectures and knowledge-intensive tasks: i) a BERT model fine-tuned for fact-checking, and ii) a sequence-to-sequence BART model for question answering. With our method, changing a prediction on the specific wording of a query tends to result in a consistent change in predictions also for its paraphrases. We show that this can be further encouraged by exploiting (e.g., automatically-generated) paraphrases during training. Interestingly, our hyper-network can be regarded as a {`}probe{'} revealing which components need to be changed to manipulate factual knowledge; our analysis shows that the updates tend to be concentrated on a small subset of components. Source code available at https://github.com/nicola-decao/KnowledgeEditor",
}
@article{xsum,
  title={Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization},
  author={Narayan, Shashi and Cohen, Shay B and Lapata, Mirella},
  journal={arXiv preprint arXiv:1808.08745},
  year={2018}
}
@article{joshi2017triviaqa,
  title={Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension},
  author={Joshi, Mandar and Choi, Eunsol and Weld, Daniel S and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1705.03551},
  year={2017}
}

@misc{nllb,
      title={No Language Left Behind: Scaling Human-Centered Machine Translation}, 
      author={NLLB Team and Marta R. Costa-jussà and James Cross and Onur Çelebi and Maha Elbayad and Kenneth Heafield and Kevin Heffernan and Elahe Kalbassi and Janice Lam and Daniel Licht and Jean Maillard and Anna Sun and Skyler Wang and Guillaume Wenzek and Al Youngblood and Bapi Akula and Loic Barrault and Gabriel Mejia Gonzalez and Prangthip Hansanti and John Hoffman and Semarley Jarrett and Kaushik Ram Sadagopan and Dirk Rowe and Shannon Spruit and Chau Tran and Pierre Andrews and Necip Fazil Ayan and Shruti Bhosale and Sergey Edunov and Angela Fan and Cynthia Gao and Vedanuj Goswami and Francisco Guzmán and Philipp Koehn and Alexandre Mourachko and Christophe Ropers and Safiyyah Saleem and Holger Schwenk and Jeff Wang},
      year={2022},
      eprint={2207.04672},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2207.04672}, 
}
@misc{safeedit,
      title={Detoxifying Large Language Models via Knowledge Editing}, 
      author={Mengru Wang, Ningyu Zhang, Ziwen Xu, Zekun Xi, Shumin Deng, Yunzhi Yao, Qishen Zhang, Linyi Yang, Jindong Wang, Huajun Chen},
      year={2024},
      eprint={2403.14472},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{ptp,
      title={PolygloToxicityPrompts: Multilingual Evaluation of Neural Toxic Degeneration in Large Language Models}, 
      author={Devansh Jain and Priyanshu Kumar and Samuel Gehman and Xuhui Zhou and Thomas Hartvigsen and Maarten Sap},
      year={2024},
      eprint={2405.09373},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.09373}, 
}
@inproceedings{rtp,
    title = "{R}eal{T}oxicity{P}rompts: Evaluating Neural Toxic Degeneration in Language Models",
    author = "Gehman, Samuel  and
      Gururangan, Suchin  and
      Sap, Maarten  and
      Choi, Yejin  and
      Smith, Noah A.",
    editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.301",
    doi = "10.18653/v1/2020.findings-emnlp.301",
    pages = "3356--3369",
    abstract = "Pretrained neural language models (LMs) are prone to generating racist, sexist, or otherwise toxic language which hinders their safe deployment. We investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the effectiveness of controllable text generation algorithms at preventing such toxic degeneration. We create and release RealToxicityPrompts, a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely-used toxicity classifier. Using RealToxicityPrompts, we find that pretrained LMs can degenerate into toxic text even from seemingly innocuous prompts. We empirically assess several controllable generation methods, and find that while data- or compute-intensive methods (e.g., adaptive pretraining on non-toxic data) are more effective at steering away from toxicity than simpler solutions (e.g., banning {``}bad{''} words), no current method is failsafe against neural toxic degeneration. To pinpoint the potential cause of such persistent toxic degeneration, we analyze two web text corpora used to pretrain several LMs (including GPT-2; Radford et. al, 2019), and find a significant amount of offensive, factually unreliable, and otherwise toxic content. Our work provides a test bed for evaluating toxic generations by LMs and stresses the need for better data selection processes for pretraining.",
}
@inproceedings{Dixon,
author = {Dixon, Lucas and Li, John and Sorensen, Jeffrey and Thain, Nithum and Vasserman, Lucy},
title = {Measuring and Mitigating Unintended Bias in Text Classification},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278729},
doi = {10.1145/3278721.3278729},
abstract = {We introduce and illustrate a new approach to measuring and mitigating unintended bias in machine learning models. Our definition of unintended bias is parameterized by a test set and a subset of input features. We illustrate how this can be used to evaluate text classifiers using a synthetic test set and a public corpus of comments annotated for toxicity from Wikipedia Talk pages. We also demonstrate how imbalances in training data can lead to unintended bias in the resulting models, and therefore potentially unfair applications. We use a set of common demographic identity terms as the subset of input features on which we measure bias. This technique permits analysis in the common scenario where demographic information on authors and readers is unavailable, so that bias mitigation must focus on the content of the text itself. The mitigation method we introduce is an unsupervised approach based on balancing the training dataset. We demonstrate that this approach reduces the unintended bias without compromising overall model quality.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {67–73},
numpages = {7},
keywords = {algorithmic bias, fairness, machine learning, natural language processing, text classification},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{zampieri,
    title = "{S}em{E}val-2020 Task 12: Multilingual Offensive Language Identification in Social Media ({O}ffens{E}val 2020)",
    author = {Zampieri, Marcos  and
      Nakov, Preslav  and
      Rosenthal, Sara  and
      Atanasova, Pepa  and
      Karadzhov, Georgi  and
      Mubarak, Hamdy  and
      Derczynski, Leon  and
      Pitenis, Zeses  and
      {\c{C}}{\"o}ltekin, {\c{C}}a{\u{g}}r{\i}},
    editor = "Herbelot, Aurelie  and
      Zhu, Xiaodan  and
      Palmer, Alexis  and
      Schneider, Nathan  and
      May, Jonathan  and
      Shutova, Ekaterina",
    booktitle = "Proceedings of the Fourteenth Workshop on Semantic Evaluation",
    month = dec,
    year = "2020",
    address = "Barcelona (online)",
    publisher = "International Committee for Computational Linguistics",
    url = "https://aclanthology.org/2020.semeval-1.188",
    doi = "10.18653/v1/2020.semeval-1.188",
    pages = "1425--1447",
    abstract = "We present the results and the main findings of SemEval-2020 Task 12 on Multilingual Offensive Language Identification in Social Media (OffensEval-2020). The task included three subtasks corresponding to the hierarchical taxonomy of the OLID schema from OffensEval-2019, and it was offered in five languages: Arabic, Danish, English, Greek, and Turkish. OffensEval-2020 was one of the most popular tasks at SemEval-2020, attracting a large number of participants across all subtasks and languages: a total of 528 teams signed up to participate in the task, 145 teams submitted official runs on the test data, and 70 teams submitted system description papers.",
}
@article{li2024data,
  title={Data Poisoning Attack on Black-Box Neural Machine Translation to Truncate Translation},
  author={Li, Lingfang and Hu, Weijian and Luo, Mingxing},
  journal={Entropy},
  volume={26},
  number={12},
  pages={1081},
  year={2024},
  publisher={MDPI}
}
@inproceedings{ilyas2018black,
  title={Black-box adversarial attacks with limited queries and information},
  author={Ilyas, Andrew and Engstrom, Logan and Athalye, Anish and Lin, Jessy},
  booktitle={International Conference on Machine Learning},
  pages={2137--2146},
  year={2018},
  organization={PMLR}
}
@inproceedings{borkan,
author = {Borkan, Daniel and Dixon, Lucas and Sorensen, Jeffrey and Thain, Nithum and Vasserman, Lucy},
title = {Nuanced Metrics for Measuring Unintended Bias with Real Data for Text Classification},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308560.3317593},
doi = {10.1145/3308560.3317593},
abstract = {Unintended bias in Machine Learning can manifest as systemic differences in performance for different demographic groups, potentially compounding existing challenges to fairness in society at large. In this paper, we introduce a suite of threshold-agnostic metrics that provide a nuanced view of this unintended bias, by considering the various ways that a classifier’s score distribution can vary across designated groups. We also introduce a large new test set of online comments with crowd-sourced annotations for identity references. We use this to show how our metrics can be used to find new and potentially subtle unintended bias in existing public models.},
booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
pages = {491–500},
numpages = {10},
location = {San Francisco, USA},
series = {WWW '19}
}