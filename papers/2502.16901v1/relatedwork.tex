\section{Related Works}
\label{sec:related}
In recent years, research on backdoor attacks in natural language processing has primarily focused on monolingual settings \citep{backdoor_layerwise, backdoor_review, embedd_bart}. Early works demonstrated that neural networks, including LSTM‚Äêbased classifiers, are vulnerable to data poisoning attacks that embed hidden triggers during training, thereby causing mis-classifications when the triggers are present at test time \citep{lstm_backdoor, concealed}.
While cross-lingual transfer has been extensively studied for benign applications, research on its security implications remains limited. \citet{clattack} first highlighted potential risks in multilingual models by demonstrating that adversarial examples could transfer across languages. Building on this, \citet{tuba} explored how linguistic similarities influence attack transferability. In the context of backdoor attacks specifically, \citet{watchout} provided initial evidence that triggers could potentially affect multiple languages, though their investigation was limited to closely related language pairs. Recent work by \citet{zhao2024exploring} and \citet{ppt} has begun addressing this gap by considering language-specific characteristics in detection strategies. However, comprehensive solutions for multilingual backdoor detection and defense remain an open challenge.

Our work builds upon these foundations while addressing the understudied intersection of backdoor attacks and multilingual models. We analyze cross-lingual backdoor propagation and demonstrate shared embedding spaces in multilingual models to exploit and achieve efficient attack transfer across languages.