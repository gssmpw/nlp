@misc{backdoor_layerwise,
      title={Backdoor Attacks on Pre-trained Models by Layerwise Weight Poisoning}, 
      author={Linyang Li and Demin Song and Xiaonan Li and Jiehang Zeng and Ruotian Ma and Xipeng Qiu},
      year={2021},
      eprint={2108.13888},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@article{backdoor_review,
  author    = {Yansong Gao and
               Bao Gia Doan and
               Zhi Zhang and
               Siqi Ma and
               Jiliang Zhang and
               Anmin Fu and
               Surya Nepal and
               Hyoungshick Kim},
  title     = {Backdoor Attacks and Countermeasures on Deep Learning: {A} Comprehensive
               Review},
  journal   = {CoRR},
  volume    = {abs/2007.10760},
  year      = {2020},
  url       = {https://arxiv.org/abs/2007.10760},
  eprinttype = {arXiv},
  eprint    = {2007.10760},
  timestamp = {Thu, 13 Aug 2020 09:46:05 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2007-10760.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{clattack,
  title={CL-attack: Textual Backdoor Attacks via Cross-Lingual Triggers},
  author={Zheng, Jingyi and Hu, Tianyi and Cong, Tianshuo and He, Xinlei},
  journal={arXiv preprint arXiv:2412.19037},
  year={2024}
}

@inproceedings{concealed,
    title = "Concealed Data Poisoning Attacks on {NLP} Models",
    author = "Wallace, Eric  and
      Zhao, Tony  and
      Feng, Shi  and
      Singh, Sameer",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.13",
    doi = "10.18653/v1/2021.naacl-main.13",
    pages = "139--150",
    abstract = "Adversarial attacks alter NLP model predictions by perturbing test-time inputs. However, it is much less understood whether, and how, predictions can be manipulated with small, concealed changes to the training data. In this work, we develop a new data poisoning attack that allows an adversary to control model predictions whenever a desired trigger phrase is present in the input. For instance, we insert 50 poison examples into a sentiment model{'}s training set that causes the model to frequently predict Positive whenever the input contains {``}James Bond{''}. Crucially, we craft these poison examples using a gradient-based procedure so that they do not mention the trigger phrase. We also apply our poison attack to language modeling ({``}Apple iPhone{''} triggers negative generations) and machine translation ({``}iced coffee{''} mistranslated as {``}hot coffee{''}). We conclude by proposing three defenses that can mitigate our attack at some cost in prediction accuracy or extra human annotation.",
}

@article{embedd_bart,
  author    = {Eugene Bagdasaryan and
               Vitaly Shmatikov},
  title     = {Spinning Sequence-to-Sequence Models with Meta-Backdoors},
  journal   = {CoRR},
  volume    = {abs/2107.10443},
  year      = {2021},
  url       = {https://arxiv.org/abs/2107.10443},
  eprinttype = {arXiv},
  eprint    = {2107.10443},
  timestamp = {Thu, 29 Jul 2021 16:14:15 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2107-10443.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{lstm_backdoor,
  author    = {Jiazhu Dai and
               Chuanshuai Chen and
               Yike Guo},
  title     = {A backdoor attack against LSTM-based text classification systems},
  journal   = {CoRR},
  volume    = {abs/1905.12457},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.12457},
  eprinttype = {arXiv},
  eprint    = {1905.12457},
  timestamp = {Mon, 03 Jun 2019 13:42:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-12457.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ppt,
  title={PPT: Backdoor Attacks on Pre-trained Models via Poisoned Prompt Tuning.},
  author={Du, Wei and Zhao, Yichun and Li, Boqun and Liu, Gongshen and Wang, Shilin},
  booktitle={IJCAI},
  pages={680--686},
  year={2022}
}

@article{tuba,
  title={TuBA: Cross-Lingual Transferability of Backdoor Attacks in LLMs with Instruction Tuning},
  author={He, Xuanli and Wang, Jun and Xu, Qiongkai and Minervini, Pasquale and Stenetorp, Pontus and Rubinstein, Benjamin IP and Cohn, Trevor},
  journal={arXiv preprint arXiv:2404.19597},
  year={2024}
}

@article{watchout,
  title={Watch out for your agents! investigating backdoor threats to llm-based agents},
  author={Yang, Wenkai and Bi, Xiaohan and Lin, Yankai and Chen, Sishuo and Zhou, Jie and Sun, Xu},
  journal={arXiv preprint arXiv:2402.11208},
  year={2024}
}

@article{zhao2024exploring,
  title={Exploring Clean Label Backdoor Attacks and Defense in Language Models},
  author={Zhao, Shuai and Tuan, Luu Anh and Fu, Jie and Wen, Jinming and Luo, Weiqi},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  year={2024},
  publisher={IEEE}
}

