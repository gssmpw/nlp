@misc{GPT4,
  author = {{OpenAI}},
  title = {GPT-4: Generative Pre-trained Transformer 4},
  year = {2023},
  howpublished = {\url{https://openai.com}},
  note = {Accessed: 2024-02-06}
}

@article{Li_2022,
   title={Competition-level code generation with AlphaCode},
   volume={378},
   ISSN={1095-9203},
   url={http://dx.doi.org/10.1126/science.abq1158},
   DOI={10.1126/science.abq1158},
   number={6624},
   journal={Science},
   publisher={American Association for the Advancement of Science (AAAS)},
   author={Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and et al.},
   year={2022},
   month=dec, 
   pages={1092–1097}
}

@inproceedings{NEURIPS2020_1f89885d,
 author = {Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {3008--3021},
 publisher = {Curran Associates, Inc.},
 title = {Learning to summarize with human feedback},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1f89885d556929e98d3ef9b86448f951-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{NEURIPS2022_8636419d,
 author = {Le, Hung and Wang, Yue and Gotmare, Akhilesh Deepak and Savarese, Silvio and Hoi, Steven Chu Hong},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {21314--21328},
 publisher = {Curran Associates, Inc.},
 title = {CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/8636419dea1aa9fbd25fc4248e702da4-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@inproceedings{NIPS2017_d5e2c0ad,
 author = {Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Deep Reinforcement Learning from Human Preferences},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf},
 volume = {30},
 year = {2017}
}

@misc{SVAMP,
      title={Are NLP Models really able to Solve Simple Math Word Problems?}, 
      author={Arkil Patel and Satwik Bhattamishra and Navin Goyal},
      year={2021},
      eprint={2103.07191},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2103.07191}, 
}

@inproceedings{cao2024sparserewardsenhancingreinforcement,
    title = "Enhancing Reinforcement Learning with Dense Rewards from Language Model Critic",
    author = "Cao, Meng  and
      Shu, Lei  and
      Yu, Lei  and
      Zhu, Yun  and
      Wichers, Nevan  and
      Liu, Yinxiao  and
      Meng, Lei",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.515/",
    doi = "10.18653/v1/2024.emnlp-main.515",
    pages = "9119--9138",
    abstract = "Reinforcement learning (RL) can align language models with non-differentiable reward signals, such as human preferences. However, a major challenge arises from the sparsity of these reward signals - typically, there is only a single reward for an entire output. This sparsity of rewards can lead to inefficient and unstable learning. To address this challenge, our paper introduces an novel framework that utilizes the critique capability of Large Language Models (LLMs) to produce intermediate-step rewards during RL training. Our method involves coupling a policy model with a critic language model, which is responsible for providing comprehensive feedback of each part of the output. This feedback is then translated into token or span-level rewards that can be used to guide the RL training process. We investigate this approach under two different settings: one where the policy model is smaller and is paired with a more powerful critic model, and another where a single language model fulfills both roles. We assess our approach on three text generation tasks: sentiment control, language model detoxification, and summarization. Experimental results show that incorporating artificial intrinsic rewards significantly improve both sample efficiency and the overall performance of the policy model, supported by both automatic and human evaluation."
}

@inproceedings{carta2023groundinglargelanguagemodels,
  author       = {Thomas Carta and
                  Cl{\'{e}}ment Romac and
                  Thomas Wolf and
                  Sylvain Lamprier and
                  Olivier Sigaud and
                  Pierre{-}Yves Oudeyer},
  editor       = {Andreas Krause and
                  Emma Brunskill and
                  Kyunghyun Cho and
                  Barbara Engelhardt and
                  Sivan Sabato and
                  Jonathan Scarlett},
  title        = {Grounding Large Language Models in Interactive Environments with Online
                  Reinforcement Learning},
  booktitle    = {International Conference on Machine Learning, {ICML} 2023, 23-29 July
                  2023, Honolulu, Hawaii, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {3676--3713},
  publisher    = {{PMLR}},
  year         = {2023},
  url          = {https://proceedings.mlr.press/v202/carta23a.html},
  timestamp    = {Mon, 28 Aug 2023 17:23:08 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/CartaRWLSO23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{chen2021evaluatinglargelanguagemodels,
      title={Evaluating Large Language Models Trained on Code}, 
      author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and et al.},
      year={2021},
      eprint={2107.03374},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2107.03374}, 
}

@misc{christiano2023deepreinforcementlearninghuman,
      title={Deep reinforcement learning from human preferences}, 
      author={Paul Christiano and Jan Leike and Tom B. Brown and Miljan Martic and Shane Legg and Dario Amodei},
      year={2023},
      eprint={1706.03741},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1706.03741}, 
}

@misc{cobbe2021trainingverifierssolvemath,
      title={Training Verifiers to Solve Math Word Problems}, 
      author={Karl Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},
      year={2021},
      eprint={2110.14168},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2110.14168}, 
}

@misc{gsm8k,
      title={Training Verifiers to Solve Math Word Problems}, 
      author={Karl Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},
      year={2021},
      eprint={2110.14168},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2110.14168}, 
}

@misc{havrilla2024teachinglargelanguagemodels,
      title={Teaching Large Language Models to Reason with Reinforcement Learning}, 
      author={Alex Havrilla and Yuqing Du and Sharath Chandra Raparthy and Christoforos Nalmpantis and Jane Dwivedi-Yu and Maksym Zhuravinskyi and Eric Hambro and Sainbayar Sukhbaatar and Roberta Raileanu},
      year={2024},
      eprint={2403.04642},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.04642}, 
}

@inproceedings{hendrycks2021measuringmathematicalproblemsolving,
  author       = {Dan Hendrycks and
                  Collin Burns and
                  Saurav Kadavath and
                  Akul Arora and
                  Steven Basart and
                  Eric Tang and
                  Dawn Song and
                  Jacob Steinhardt},
  editor       = {Joaquin Vanschoren and
                  Sai{-}Kit Yeung},
  title        = {Measuring Mathematical Problem Solving With the {MATH} Dataset},
  booktitle    = {Proceedings of the Neural Information Processing Systems Track on
                  Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December
                  2021, virtual},
  year         = {2021},
  url          = {https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html},
  timestamp    = {Thu, 05 May 2022 16:53:59 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/HendrycksBKABTS21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{hwang2024selfexploreenhancingmathematicalreasoning,
  author       = {Hyeonbin Hwang and
                  Doyoung Kim and
                  Seungone Kim and
                  Seonghyeon Ye and
                  Minjoon Seo},
  editor       = {Yaser Al{-}Onaizan and
                  Mohit Bansal and
                  Yun{-}Nung Chen},
  title        = {Self-Explore: Enhancing Mathematical Reasoning in Language Models
                  with Fine-grained Rewards},
  booktitle    = {Findings of the Association for Computational Linguistics: {EMNLP}
                  2024, Miami, Florida, USA, November 12-16, 2024},
  pages        = {1444--1466},
  publisher    = {Association for Computational Linguistics},
  year         = {2024},
  url          = {https://aclanthology.org/2024.findings-emnlp.78},
  timestamp    = {Mon, 18 Nov 2024 09:05:59 +0100},
  biburl       = {https://dblp.org/rec/conf/emnlp/HwangKKYS24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{kazemnejad2023impactpositionalencodinglength,
  author       = {Amirhossein Kazemnejad and
                  Inkit Padhi and
                  Karthikeyan Natesan Ramamurthy and
                  Payel Das and
                  Siva Reddy},
  editor       = {Alice Oh and
                  Tristan Naumann and
                  Amir Globerson and
                  Kate Saenko and
                  Moritz Hardt and
                  Sergey Levine},
  title        = {The Impact of Positional Encoding on Length Generalization in Transformers},
  booktitle    = {Advances in Neural Information Processing Systems 36: Annual Conference
                  on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans,
                  LA, USA, December 10 - 16, 2023},
  year         = {2023},
  url          = {http://papers.nips.cc/paper\_files/paper/2023/hash/4e85362c02172c0c6567ce593122d31c-Abstract-Conference.html},
  timestamp    = {Fri, 01 Mar 2024 16:26:20 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/KazemnejadPRDR23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{lee2023teachingarithmeticsmalltransformers,
  author       = {Nayoung Lee and
                  Kartik Sreenivasan and
                  Jason D. Lee and
                  Kangwook Lee and
                  Dimitris Papailiopoulos},
  title        = {Teaching Arithmetic to Small Transformers},
  booktitle    = {The Twelfth International Conference on Learning Representations,
                  {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=dsUB4bst9S},
  timestamp    = {Wed, 07 Aug 2024 17:11:53 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/LeeSL0P24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{llama2,
  author       = {Hugo Touvron and
                  Louis Martin and
                  Kevin Stone and
                  Peter Albert and
                  Amjad Almahairi and et al.},
  title        = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
  journal      = {CoRR},
  volume       = {abs/2307.09288},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2307.09288},
  doi          = {10.48550/ARXIV.2307.09288},
  eprinttype   = {arXiv},
  eprint       = {2307.09288},
  timestamp    = {Mon, 28 Aug 2023 21:26:22 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2307-09288.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{mcleish2024transformersarithmeticrightembeddings,
  title={Transformers Can Do Arithmetic with the Right Embeddings},
  author={Sean McLeish and Arpit Bansal and Alex Stein and Neel Jain and John Kirchenbauer and Brian R. Bartoldson and Bhavya Kailkhura and Abhinav Bhatele and Jonas Geiping and Avi Schwarzschild and Tom Goldstein},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2024}
}

@article{ppo,
  title={Proximal Policy Optimization Algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@misc{qin2024toollearningfoundationmodels,
      title={Tool Learning with Foundation Models}, 
      author={Yujia Qin and Shengding Hu and Yankai Lin and Weize Chen and Ning Ding and Ganqu Cui and Zheni Zeng and Yufei Huang and Chaojun Xiao and Chi Han and Yi Ren Fung and Yusheng Su and Huadong Wang and Cheng Qian and Runchu Tian and Kunlun Zhu and Shihao Liang and Xingyu Shen and Bokai Xu and Zhen Zhang and Yining Ye and Bowen Li and Ziwei Tang and Jing Yi and Yuzhang Zhu and Zhenning Dai and Lan Yan and Xin Cong and Yaxi Lu and Weilin Zhao and Yuxiang Huang and Junxi Yan and Xu Han and Xian Sun and Dahai Li and Jason Phang and Cheng Yang and Tongshuang Wu and Heng Ji and Zhiyuan Liu and Maosong Sun},
      year={2024},
      eprint={2304.08354},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2304.08354}, 
}

@misc{shen2023positionaldescriptionmatterstransformers,
      title={Positional Description Matters for Transformers Arithmetic}, 
      author={Ruoqi Shen and Sébastien Bubeck and Ronen Eldan and Yin Tat Lee and Yuanzhi Li and Yi Zhang},
      year={2023},
      eprint={2311.14737},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.14737}, 
}

@inproceedings{wallace-etal-2019-nlp,
    title = "Do {NLP} Models Know Numbers? Probing Numeracy in Embeddings",
    author = "Wallace, Eric  and
      Wang, Yizhong  and
      Li, Sujian  and
      Singh, Sameer  and
      Gardner, Matt",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1534",
    doi = "10.18653/v1/D19-1534",
    pages = "5307--5315",
    abstract = "The ability to understand and work with numbers (numeracy) is critical for many complex reasoning tasks. Currently, most NLP models treat numbers in text in the same way as other tokens{---}they embed them as distributed vectors. Is this enough to capture numeracy? We begin by investigating the numerical reasoning capabilities of a state-of-the-art question answering model on the DROP dataset. We find this model excels on questions that require numerical reasoning, i.e., it already captures numeracy. To understand how this capability emerges, we probe token embedding methods (e.g., BERT, GloVe) on synthetic list maximum, number decoding, and addition tasks. A surprising degree of numeracy is naturally present in standard embeddings. For example, GloVe and word2vec accurately encode magnitude for numbers up to 1,000. Furthermore, character-level embeddings are even more precise{---}ELMo captures numeracy the best for all pre-trained methods{---}but BERT, which uses sub-word units, is less exact.",
}

@inproceedings{wei2023chainofthoughtpromptingelicitsreasoning,
  author       = {Jason Wei and
                  Xuezhi Wang and
                  Dale Schuurmans and
                  Maarten Bosma and
                  Brian Ichter and
                  Fei Xia and
                  Ed H. Chi and
                  Quoc V. Le and
                  Denny Zhou},
  editor       = {Sanmi Koyejo and
                  S. Mohamed and
                  A. Agarwal and
                  Danielle Belgrave and
                  K. Cho and
                  A. Oh},
  title        = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  booktitle    = {Advances in Neural Information Processing Systems 35: Annual Conference
                  on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,
                  LA, USA, November 28 - December 9, 2022},
  year         = {2022},
  url          = {http://papers.nips.cc/paper\_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html},
  timestamp    = {Tue, 12 Nov 2024 16:50:49 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/Wei0SBIXCLZ22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{xiao2023conditionslengthgeneralizationlearning,
      title={Conditions for Length Generalization in Learning Reasoning Skills}, 
      author={Changnan Xiao and Bing Liu},
      year={2023},
      eprint={2311.16173},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2311.16173}, 
}

@inproceedings{yao2020calmexplorelanguagemodels,
  author       = {Shunyu Yao and
                  Rohan Rao and
                  Matthew J. Hausknecht and
                  Karthik Narasimhan},
  editor       = {Bonnie Webber and
                  Trevor Cohn and
                  Yulan He and
                  Yang Liu},
  title        = {Keep {CALM} and Explore: Language Models for Action Generation in
                  Text-based Games},
  booktitle    = {Proceedings of the 2020 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2020, Online, November 16-20, 2020},
  pages        = {8736--8754},
  publisher    = {Association for Computational Linguistics},
  year         = {2020},
  url          = {https://doi.org/10.18653/v1/2020.emnlp-main.704},
  doi          = {10.18653/V1/2020.EMNLP-MAIN.704},
  timestamp    = {Tue, 20 Aug 2024 07:54:43 +0200},
  biburl       = {https://dblp.org/rec/conf/emnlp/YaoRHN20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{zhang2023chainofthoughtreasoningpolicyimprovement,
      title={Chain-of-Thought Reasoning is a Policy Improvement Operator}, 
      author={Hugh Zhang and David C. Parkes},
      year={2023},
      eprint={2309.08589},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.08589}, 
}

@inproceedings{zhou2024transformersachievelengthgeneralization,
title={Transformers Can Achieve Length Generalization But Not Robustly},
author={Yongchao Zhou and Uri Alon and Xinyun Chen and Xuezhi Wang and Rishabh Agarwal and Denny Zhou},
booktitle={ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models},
year={2024},
url={https://openreview.net/forum?id=DWkWIh3vFJ}
}

@misc{ziegler2019fine,
      title={Fine-Tuning Language Models from Human Preferences}, 
      author={Daniel M. Ziegler and Nisan Stiennon and Jeffrey Wu and Tom B. Brown and Alec Radford and Dario Amodei and Paul Christiano and Geoffrey Irving},
      year={2020},
      eprint={1909.08593},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1909.08593}, 
}

