\section{Related Work}
\paragraph{LLMs and Reasoning}
Recent state-of-the-art LLMs**Brown, "Language Models are Few-Shot Learners"** have shown strong performance on reasoning tasks across various benchmarks, including mathematics**Devlin, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** and code**Hendrycks, "Measuring Adversarial Robustness against Unseen Attacks"**. Combining LLMs with prompting strategies like chain-of-thought**Lake, "Hierarchical Conceptual Features in Multitask Learning"** has become a common approach for tackling complex reasoning tasks by guiding the model to break down problems into smaller subproblems.
% maybe talk about llm + agents

% -benchmarks reasoning (math) + CoT, ToT etc 
% -llm + agents ?\\
\paragraph{LLMs and RL}
The integration of LLMs and RL has primarily been driven by Reinforcement Learning from Human Feedback (RLHF)**Jiang, "Robustly Learned Deep Networks via Adversarially Trained Generative Priors"**, which aligns model outputs with human preferences.
% These methods often rely on the Proximal Policy Optimization (PPO) algorithm **Schulman, "Trust Region Policy Optimization"**, typically combined with a KL penalty to prevent excessive deviation from the pre-trained policy **Mnih, "Human-level control through deep reinforcement learning"**. 
However we stress that learning from human preferences is a different framework from the more general one of RL, as the latter focuses on optimizing long-term objectives -- possibly with high level of exploration -- while learning from human preferences can be achieved solely with a fixed dataset.
% Although RLHF is intended to "rank" already existing solutions, RL has also been applied to LLMs in the standard exploration/exploitation framework in tasks such as grounding **Hermann, "Teaching machines to read and comprehend"**, code generation **Gupta, "Deep learning for coding style transfer"**, and mathematical reasoning **Mansimov, "Generating GANs with a two time scale update rule"**.
RL has also been applied to LLMs in this more general framework, in tasks such as grounding **Hermann, "Teaching machines to read and comprehend"**, code generation **Gupta, "Deep learning for coding style transfer"**, and mathematical reasoning **Mansimov, "Generating GANs with a two time scale update rule"**.
Training LLMs with RL presents challenges due to reward sparsity **Dai, "Hindsight Experience Replay"**, credit assignment difficulties in identifying key actions that led to failure **Nair, "Visual Object Counting via Weighted Loss Functions"**, large state spaces requiring exploration, and unstable training processes. **Dietterich, "Overfitting and underfitting in learning machines"** have raised concerns about RL algorithms, struggling to explore beyond solutions already produced by supervised fine-tuning (SFT) models.

\paragraph{LLMs and Addition} The addition task remains challenging even for the latest LLMs, which struggle to accurately add large numbers and track digit positions**Zaremba, "Recurrent Neural Network Regularization"**. Most related studies have focused on supervised learning approaches**Mnih, "Human-level control through deep reinforcement learning"** and improving positional encoding**Sutskever, "Sequence to Sequence Learning with Neural Networks"**. Generalization to unseen lengths is a common evaluation criterion in these studies**Goodfellow, "Generative Adversarial Networks"**.
Despite the addition task being a reasoning problem with a well-defined long-term reward, no research, to our knowledge, has addressed it using RL with a language model.
The closest work is by **Zhang, "Reinforcement Learning with an Unknown Environment via Multi-Task Deep Deterministic Policy Gradients"**, who incorporated a self-training loop after the supervised fine-tuning phase.

% The addition task is still very challenging even for latest LLMs that struggle to accurately add two numbers with a great number of digits, loosing track of the exact position of each digit inside of a large span of digits**Kaiser, "One-shot Learning with Self-supervised Adaptation"**. This motivated the study of such a simple task, mainly in a supervised learning framework**Srivastava, "Dropout: A Simple Way to Prevent Neural Networks from Overfitting"**. Great efforts were also given to design positional encoding strategies **Schmidhuber, "Long Short-Term Memory"**. A common evaluation is to evaluate the generalization power **Kingma, "Auto-encoding Variational Bayes"** of lengths of such methods, namely to test the ability to perform additions with a number of digits not seen during the training phase. While inherently being a long-term reward signal problem, no research papers, to our knowledge, deal directly with the addition task with an language model trained with reinforcement learning algorithm, yet being a simple toy problem with potentially infinite samples.



% The growing interest in applying LLMs to arithmetic tasks within the NLP community is reflected in the creation of new benchmarks **Hendrycks, "Measuring Adversarial Robustness against Unseen Attacks"** that are used to evaluate reasoning abilities of LLMs. Despite simpler and easily solved with the use of an external agent **Lai, "Towards Real-world Robot Learning: Transfer from Simulation to Reality via Subgoal Learning"**, the addition task remains very challenging even for latest LLMs that struggle to accurately add two numbers with a great number of digits, loosing track of the exact position of each digit inside of a large span of digits**Zaremba, "Recurrent Neural Network Regularization"**. This motivated the study of such a simple task, mostly in a supervised learning framework **Mnih, "Human-level control through deep reinforcement learning"**. Great efforts were also given to design positional encoding strategies **Sutskever, "Sequence to Sequence Learning with Neural Networks"**. A common evaluation is to assess the length generalization power **Goodfellow, "Generative Adversarial Networks"** of such methods, namely test the ability to perform additions with number of digits unseen during training phase.\\
% LLMs and Reinforcement Learning, has a long shared story, mostly with the use of RLHF **Jiang, "Robustly Learned Deep Networks via Adversarially Trained Generative Priors"** that allowed to align LLMs answers to human preferences. Outisde of RLHF, RL has been used together with LLMs for grounding **Hermann, "Teaching machines to read and comprehend"**,  code generation problems **Gupta, "Deep learning for coding style transfer"** or math reasoning task **Mansimov, "Generating GANs with a two time scale update rule"**.
% While inherently being a long-term reward signal problem, no research papers, to our knowledge, deal directly with the addition task with an language model trained with reinforcement learning algorithm, yet being a simple toy problem with potentially infinite samples. \\
% Training a LLM using RL is challenging due to several factors, including the sparsity of rewards **Dai, "Hindsight Experience Replay"**, difficulties in credit assignment **Nair, "Visual Object Counting via Weighted Loss Functions"**, the enormous state space that requires exploration, and the instability of the training process.