\section{Related Work}
\paragraph{LLMs and Reasoning}
Recent state-of-the-art LLMs~\citep{llama2,GPT4} have shown strong performance on reasoning tasks across various benchmarks, including mathematics~\citep{cobbe2021trainingverifierssolvemath, hendrycks2021measuringmathematicalproblemsolving} and code~\citep{chen2021evaluatinglargelanguagemodels, Li_2022}. Combining LLMs with prompting strategies like chain-of-thought~\citep{wei2023chainofthoughtpromptingelicitsreasoning} has become a common approach for tackling complex reasoning tasks by guiding the model to break down problems into smaller subproblems.
% maybe talk about llm + agents

% -benchmarks reasoning (math) + CoT, ToT etc 
% -llm + agents ?\\
\paragraph{LLMs and RL}
The integration of LLMs and RL has primarily been driven by Reinforcement Learning from Human Feedback (RLHF) \citep{NIPS2017_d5e2c0ad, ziegler2019fine, NEURIPS2020_1f89885d}, which aligns model outputs with human preferences.
% These methods often rely on the Proximal Policy Optimization (PPO) algorithm \citep{ppo}, typically combined with a KL penalty to prevent excessive deviation from the pre-trained policy \citep{ziegler2019fine}.
However we stress that learning from human preferences is a different framework from the more general one of RL, as the latter focuses on optimizing long-term objectives -- possibly with high level of exploration -- while learning from human preferences can be achieved solely with a fixed dataset.
% Although RLHF is intended to "rank" already existing solutions, RL has also been applied to LLMs in the standard exploration/exploitation framework in tasks such as grounding \citep{yao2020calmexplorelanguagemodels, carta2023groundinglargelanguagemodels}, code generation \citep{NEURIPS2022_8636419d}, and mathematical reasoning \cite{havrilla2024teachinglargelanguagemodels}.
RL has also been applied to LLMs in this more general framework, in tasks such as grounding \citep{yao2020calmexplorelanguagemodels, carta2023groundinglargelanguagemodels}, code generation \citep{NEURIPS2022_8636419d}, and mathematical reasoning \cite{havrilla2024teachinglargelanguagemodels}.
Training LLMs with RL presents challenges due to reward sparsity \citep{cao2024sparserewardsenhancingreinforcement}, credit assignment difficulties in identifying key actions that led to failure \citep{hwang2024selfexploreenhancingmathematicalreasoning}, large state spaces requiring exploration, and unstable training processes. \citet{havrilla2024teachinglargelanguagemodels} have raised concerns about RL algorithms, struggling to explore beyond solutions already produced by supervised fine-tuning (SFT) models.

\paragraph{LLMs and Addition} The addition task remains challenging even for the latest LLMs, which struggle to accurately add large numbers and track digit positions~\citep{wallace-etal-2019-nlp}. Most related studies have focused on supervised learning approaches~\citep{lee2023teachingarithmeticsmalltransformers, mcleish2024transformersarithmeticrightembeddings} and improving positional encoding~\citep{shen2023positionaldescriptionmatterstransformers, kazemnejad2023impactpositionalencodinglength,mcleish2024transformersarithmeticrightembeddings, zhou2024transformersachievelengthgeneralization}. Generalization to unseen lengths is a common evaluation criterion in these studies~\citep{kazemnejad2023impactpositionalencodinglength, xiao2023conditionslengthgeneralizationlearning, zhou2024transformersachievelengthgeneralization}.
Despite the addition task being a reasoning problem with a well-defined long-term reward, no research, to our knowledge, has addressed it using RL with a language model.
The closest work is by \citet{zhang2023chainofthoughtreasoningpolicyimprovement}, who incorporated a self-training loop after the supervised fine-tuning phase.

% The addition task is still very challenging even for latest LLMs that struggle to accurately add two numbers with a great number of digits, loosing track of the exact position of each digit inside of a large span of digits~\citep{wallace-etal-2019-nlp}. This motivated the study of such a simple task, mainly in a supervised learning framework~\citep{lee2023teachingarithmeticsmalltransformers, mcleish2024transformersarithmeticrightembeddings}. Great efforts were also given to design positional encoding strategies \citep{mcleish2024transformersarithmeticrightembeddings, zhou2024transformersachievelengthgeneralization, kazemnejad2023impactpositionalencodinglength, shen2023positionaldescriptionmatterstransformers}. A common evaluation is to evaluate the generalization power \citep{zhou2024transformersachievelengthgeneralization, kazemnejad2023impactpositionalencodinglength, xiao2023conditionslengthgeneralizationlearning} of lengths of such methods, namely to test the ability to perform additions with a number of digits not seen during the training phase. While inherently being a long-term reward signal problem, no research papers, to our knowledge, deal directly with the addition task with an language model trained with reinforcement learning algorithm, yet being a simple toy problem with potentially infinite samples.



% The growing interest in applying LLMs to arithmetic tasks within the NLP community is reflected in the creation of new benchmarks \citep{gsm8k, SVAMP, hendrycks2021measuringmathematicalproblemsolving} that are used to evaluate reasoning abilities of LLMs. Despite simpler and easily solved with the use of an external agent \citep{qin2024toollearningfoundationmodels}, the addition task remains very challenging even for latest LLMs that struggle to accurately add two numbers with a great number of digits, loosing track of the exact position of each digit inside of a large span of digits~\citep{wallace-etal-2019-nlp}. This motivated the study of such a simple task, mostly in a supervised learning framework~\citep{lee2023teachingarithmeticsmalltransformers, mcleish2024transformersarithmeticrightembeddings}. Great efforts were also given to design positional encoding strategies \citep{mcleish2024transformersarithmeticrightembeddings, zhou2024transformersachievelengthgeneralization, kazemnejad2023impactpositionalencodinglength, shen2023positionaldescriptionmatterstransformers}. A common evaluation is to assess the length generalization power \citep{zhou2024transformersachievelengthgeneralization, kazemnejad2023impactpositionalencodinglength, xiao2023conditionslengthgeneralizationlearning} of such methods, namely test the ability to perform additions with number of digits unseen during training phase.\\
% LLMs and Reinforcement Learning, has a long shared story, mostly with the use of RLHF \citep{ziegler2020finetuninglanguagemodelshuman,christiano2023deepreinforcementlearninghuman} that allowed to align LLMs answers to human preferences. Outisde of RLHF, RL has been used together with LLMs for grounding \citep{carta2023groundinglargelanguagemodels, yao2020calmexplorelanguagemodels},  code generation problems \citep{NEURIPS2022_8636419d} or math reasoning task \cite{havrilla2024teachinglargelanguagemodels}.
% While inherently being a long-term reward signal problem, no research papers, to our knowledge, deal directly with the addition task with an language model trained with reinforcement learning algorithm, yet being a simple toy problem with potentially infinite samples. \\
% Training a LLM using RL is challenging due to several factors, including the sparsity of rewards \citep{cao2024sparserewardsenhancingreinforcement}, difficulties in credit assignment \citep{hwang2024selfexploreenhancingmathematicalreasoning}, the enormous state space that requires exploration, and the instability of the training process.

%