\section{Literature Review}
\label{ssec:LR}
Our work touches upon several classical and modern topics in the constitutive modeling of materials, including homogenization methods, memory-dependent modeling, machine learning of constitutive laws, and model dependence on material microstructure. We discuss prior research in each of these areas below.

\paragraph{Theoretical and Numerical Homogenization:} In its simplest formulation, homogenization theory~\cite{pavliotis2008multiscale} studies elliptic or parabolic PDEs whose coefficients vary periodically on a small lengthscale $\varepsilon \ll 1$. Such PDEs are \textit{multiscale} since their solutions have coarse-grained features as well as fine-grained features of scale $\varepsilon$. Homogenization uses a power series expansion to determine the limit of the PDE solution as the lengthscale $\varepsilon$ is taken to zero. This results in a new averaged or \textit{homogenized} PDE of the same form with an \textit{effective} coefficient function that is now independent of the microscale $\varepsilon$. This effective coefficient is determined by a boundary value problem (BVP) called the \textit{cell problem} which is solved at the microscale level. Homogenization theory can be extended to PDEs with random or even nonperiodic coefficients~\cite{cioranescu1999introduction, kozlov1980averaging}, and care must be taken to establish convergence of the true solution to the homogenized limit~\cite{allaire1992homogenization, cioranescu1999introduction, pavliotis2008multiscale}. Viscoelastic materials are governed by elliptic PDEs, where the coefficient function known as the \textit{elastic modulus} encodes the material microstructure. Multiscale materials can be modeled by elliptic PDEs whose elastic modulus similarly varies on a small length scale $\varepsilon$. In this setting, homogenization derives the effective elastic modulus of the material that is again independent of this microscale. The texts of Milton~\cite{milton_book} and Zohdi \& Wriggers~\cite{zohdi2008introduction} give a comprehensive review of effective material properties that result from homogenization. 

As mentioned above, the effective modulus of a homogenized material is determined by solving a cell boundary value problem at the level of the microscale $\varepsilon$. Predicting the macroscale behavior of a material by numerically solving this cell problem BVP is called \textit{numerical homogenization} and is a core focus of \textit{computational micromechanics}~\cite{zohdi2008introduction}. Cell problems are typically solved with periodic, Dirichlet, or Neumann boundary conditions using spectral~\cite{mishra2016comparative, moulinec1998numerical} or finite element methods~\cite{guedes1990preprocessing, suquet1987elements}. The cell problem resulting from homogenization allows us to compute the effective elastic modulus of a multiscale material microstructure, but this computation must be repeated for every new microstructure, making it an expensive procedure. We discuss below how machine learning methods allow us to perform homogenization over a range of microstructures simultaneously.~\\

\paragraph{Memory and Internal Variables:}
Viscoelastic materials model viscous as well as elastic behavior; hence, their strain and stress dynamics explicitly depend on time. In particular, the application of a sudden strain deformation or stress load at one location is remembered throughout the material at all future times, and this memory is quantified by exponentially decaying \textit{memory kernels} called the creep compliance and relaxation modulus functions~\cite{ferry1980viscoelastic, tschoegl2012phenomenological}. This observation that viscoelastic materials have \textit{fading memory} was formally shown to hold under very general mathematical assumptions in a series of papers by Coleman \& Noll~\cite{coleman1961foundations, coleman1961recent}.

Fading memory also arises in multiscale viscoelastic materials whose microstructure varies periodically on a cell of size $\varepsilon$. Even though the original multiscale material locally exhibits no memory effects in strain or stress (e.g., Markovian behavior), homogenizing by averaging at the $\varepsilon$ scale and taking $\varepsilon \to 0$ introduces local creep compliance and relaxation modulus functions that dictate long term memory in the strain and stress dynamics at every point of the homogenized material. This remarkable result was first proven by Sanchez--Palencia on the Kelvin--Voigt model using semigroup theory~\cite[Chapter 6]{sanchez1980non}. Further extensions to thermo-viscoelasticity were proven in the seminal paper of Francfort and Suquet~\cite{francfort1986homogenization}. Tartar~\cite{tartar1991memory} showed that the memory kernel (relaxation modulus) relating strain-to-stress after homogenization is given by a possibly infinite sum of exponentials. Suquet and coauthors~\cite{brenner2013overall, lahellec2024effective, lahellec2024effective2} discuss more about the structure of these memory kernels and their approximation by finite sums of exponentials. In one-dimensional piecewise constant materials, the creep compliance and relaxation modulus memory kernels are exactly given by a finite sum of exponentials, and this has been rediscovered in various classical and modern texts~\cite{gross1968mathematical, bhattacharya2023learning}. Approximating these kernels by a finite sum of exponentials is known as a \textit{Prony series}, and this technique has been very well explored both in theory~\cite{lahellec2024effective, serra2019viscoelastic, tschoegl2012phenomenological} and experiments~\cite{kim2024experimental, kraus2017parameter, nikonov2005determination, shanbhag2023computer}.

Viscoelastic materials whose memory kernels are given by finite sums of exponentials can be transformed into differential equations with an internal state vector whose dimension is equal to the number of exponential terms. These internal state variables integrate in their dynamics all the history of the material, but they do so in a Markovian way, leading to more efficient simulations of material stress-strain dynamics~\cite{bhattacharya2023learning, liu2023learning}. Internal variables also arise in models of plastic~\cite{rice1971inelastic} and viscoplastic~\cite{liu2023learning} materials, and reviews of this subject can be found in~\cite{billington1982physics, horstemeyer2010historical}. Hence, memory in materials is fundamentally linked to internal variable and differential equation representations, and the equivalence between such model representations is nicely reviewed in~\cite{eggersmann2019model}.~\\

\paragraph{Machine Learning of Constitutive Models:}
Two central applications of data-driven methods in materials science are the discovery of unknown constitutive laws and, related to this, the acceleration of composite multiscale material simulations~\cite{liu2021review}. Data-driven learning of constitutive laws is an actively developing field that has incorporated a variety of tools including gradient sensitivity methods~\cite{akerson2024learning}, physics-informed machine learning~\cite{haghighat2023constitutive}, probabilistic machine learning~\cite{fuhg2022physics}, deep learning~\cite{liu2019deep}, and operator learning~\cite{bhattacharya2024learning}. We refer readers to a recent comprehensive review paper~\cite{fuhg2024review} on this topic.

For history-dependent solids such as those studied in viscoelasticity or viscoplasticity, a constitutive model must use the strain history to predict the evolution of stress; the relationship between strain and stress is no longer instantaneous. Learning such a mapping between strain and stress time series has been approached with several data-driven architectures. Liu et al.~\cite{liu2022learning} featurize strain and stress time series through principal component analysis and learn a mapping between these feature spaces. This approach has the benefit of being invariant to the level of time discretization of the data, but suffers from a lack of causality in its learned strain-to-stress map. Causality can be enforced through the use of recurrent neural networks (RNNs), and the LSTM~\cite{ghavamian2019accelerating} and  GRU~\cite{mozaffar2019deep} recurrent architectures have been very effective at learning strain-to-stress maps with history dependence.

A natural approach to enforce both causality and independence to time discretization is to model the constitutive law as a differential equation which is forced by the strain trajectory and whose output is the stress. Compared to the non-physical architecture of RNN models, this approach is guided by the internal variable theories of memory-dependent materials discussed above and has motivated a large application of neural ODEs~\cite{chen2018neural, jones2022neural}, also referred to as recurrent neural operators~\cite{liu2023learning}, to constitutive modeling of materials~\cite{bhattacharya2023learning, jones2022neural, karimi2024learning, liu2023learning, zhang2024iterated}.~\\

\paragraph{Microstructure-Dependent Architectures:}
Since data-driven constitutive models must be retrained for each material microstructure, there is a need to develop \textit{microstructure-dependent} architectures that can predict constitutive laws of new materials without retraining. One approach is to allow data-driven models to depend on summary statistics of a material, such as the volume fraction, elastic modulus of different phases, or mean sizes and distances between fibers and grains embedded in a material. This idea has been used in several important architectures such as the Deep Material Network~\cite{liu2019deep} and material-dependent recurrent networks~\cite{mozaffar2019deep}. Bishra et al.~\cite{bishara2023state} provide a good review of such methods. These approaches assume that a material microstructure can be sufficiently described by predetermined statistics, mostly applicable to $n$-phase media, and hence do not generalize to more complicated spatially varying microstructures.

The fact that a material microstructure must generally be interpreted as a full \textit{functional} input into a data-driven constitutive model has been noticed in a few recent papers. In~\cite{bhattacharya2024learning}, an FNO architecture was trained on elastic multiscale materials to learn a map from their microstructure as a function on the cell domain to an effective homogenized elastic modulus. Crucially, the regularity or Lipschitz continuity of the cell problem BVP resulting from homogenization was used to prove that this FNO architecture is a universal approximator: it can predict the homogenized elastic modulus across a range of microstructures with uniformly bounded error. In this paper, we show how cell problems of \textit{viscoelastic} materials also satisfy Lipschitz regularity conditions, which allow us to build differential equation FNO architectures that likewise have universal approximation guarantees over a range of microstructures.

Jones et al.~\cite{jones2022neural} was the first work to propose a neural ODE architecture that was microstructure dependent and hence could simulate stress-strain dynamics with internal state variables across a wide array of microstructures. Their approach was to featurize the material microstructure function using a graph convolutional neural network and to augment the initial conditions of the internal state variables with this feature vector. This way of encoding the material microstructure in the initial conditions of the internal state variables is motivated by the improved numerical performance of augmented neural ODEs~\cite{dupont2019augmented}. In contrast, the theory of viscoelastic~\cite{bhattacharya2023learning} and viscoplastic~\cite{liu2023learning} materials instead shows that material dependence must be encoded in the \textit{functional form} of the differential equation driving the internal state variables rather than in their initial conditions. This is the approach we take here, which allows us to accurately predict strain stress dynamics for a variety of microstructures and, crucially, obtain theoretical guarantees for our method.