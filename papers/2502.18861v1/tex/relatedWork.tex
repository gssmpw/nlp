\section{Related Work}
\subsection{Online Moderation Practices and Technological Tools}
Online harm refers to a wide range of problematic behaviors, including but not limited to hate speech~\cite{Mathew2020}, public shaming~\cite{Basak2016}, doxxing~\cite{Snyder2017}, and digital self-harm~\cite{Pater2017}. These actions occur in various contexts, including self-directed, collective, or interpersonal victim-perpetrator relationships~\cite{Krug2002}. Our research focuses on the latter, specifically interpersonal harm that involves targeted victims harmed by one or more individuals. 

On platforms such as Reddit and Discord, a subset of users volunteer as moderators, who oversee and manage spaces in order to encourage more positive and productive interactions~\cite{Seering2019}. %moderators act as the “gatekeepers”~\cite{Roberts2016} to ensure the community can thrive despite the presence of these harmful conducts. Moderators can be paid and trained experts~\cite{Roberts2016}, active volunteer moderators~\cite{Seering2019}, or at times a collaboration of both~\cite{Seering2022a}, but . 
These moderators employ various measures to regulate content and mitigate harm, often reflecting their underlying values and moderation styles~\cite{Jiang2023}. Most commonly, reactive measures such as account sanctions (timeouts, bans) and content removal are used~\cite{Srinivasan2019, Gillespie2018, Roberts2019}. These punishments, typically specified in community rules~\cite{Fiesler2018} and policies~\cite{Schaffner2024}, are enforced after harm has happened. While effective at stopping the immediate harm, excessive sanction can deter engagement and alienate members~\cite{Squirrell2019}, or be criticized as a ``black box'' where punishments are opaque and unfair~\cite{Kou2021, Vaccaro2020}, in some cases driving users to leave communities entirely in favor of other platforms~\cite{Gao2024}. To counter these effects, some moderators integrate proactive strategies to prevent harm before it occurs. This includes establishing social norms~\cite{Seering2019, Chandrasekharan2018}, creating content filters~\cite{Jhaver2019}, and highlighting examples of prosocial behaviors~\cite{Seering2017}. 

However, achieving such extensive goals can be time-consuming, labor-intensive~\cite{Steiger2021}, and emotionally draining~---~especially for volunteer moderators who invest significant time and effort into their work~\cite{Wohn2019}, akin to a “second job” for some~\cite{Seering2022a}. To assist moderators, a variety of tools and agents have been developed, including user-developed or third-party bots, algorithms, and artificial intelligence~\cite{Jhaver2019, Kiene2020, Hwang2024}. These tools help in tasks both proactively, such as setting up automatic content filtering~\cite{Chandrasekharan2019}, \revision{sending nudges to potential violations~\cite{Seering2024}}, classification~\cite{Blackwell2017}, and limiting the reach of content~\cite{Binns2017}, and reactively, such as issuing warnings, bans, and handling user appeals~\cite{Atreja2024}. In addition, tools like visual exploration systems can combine both proactive and reactive strategies~\cite{Choi2023}.

In a broader sense, indirect measures such as education~\cite{Cai2019} and community support~\cite{Kou2024} also play a crucial role in addressing harm and reforming users before, during, and after harmful incidents. There has been growing interest in applying notions of justice to online moderation, focusing on user rehabilitation and reparation. 
Researchers argue for a shift from punitive interventions to a greater emphasis on user education and restorative forms of justice-seeking~\cite{West2018, Blackwell2018}. Cai et. al reveals that a caring approach can convert one-time offenders into long-term loyal members~\cite{Cai2021}. Restorative justice, in particular, is gaining traction as a potential approach~\cite{Kou2021} to address harm holistically, taking into account victims’ specific needs while engaging offenders and community members to heal collectively. Under this justice lens, profound studies have explored the central needs of harmed adolescents~\cite{Xiao2022}, in addition to different contextual challenges and opportunities for implementing restorative approaches online~\cite{Xiao2023}. Nevertheless, given that each justice approach has pros and cons, repairing harm is not a one-size-fits-all endeavor~\cite{Schoenebeck2021a, Warzel2019}. Different harms might require distinct frameworks or a combination of different ones~\cite{Goldman2021, Llewellyn1999}. Our research builds on this by investigating the design space with more in-depth contexts where online restorative justice implementation may prove the most effective.

\subsection{Restorative Justice and its Implementation in the Online Space}
\subsubsection{Restorative Justice Overview}
Restorative Justice is a framework that centers on people’s needs by providing care and support after harm occurs. This differs from the common punitive model, where harmful behaviors are considered rule violations that require punishments in proportion to their offense~\cite{Szablowinski2008}. Restorative justice extends beyond this depiction to view harm as “a violation of people and relationships rather than merely a breach of rules”~\cite{Ness2016}. It embodies three main principles~\cite{Mccold2000}: (1) identify and address victims' needs related to the harm; (2) hold offenders accountable to right those wrongs; and (3) involve victims, offenders, and potentially the community in the restoration process. Instead of punishing the offenders, the primary tool for a restorative approach is communication among involved stakeholders, usually mediated by a facilitator~\cite{Bolitho2017} in offline settings. The facilitator helps ensure that victims and offenders have equal footing, guides them to reflect on the harm, and determines whether a consensus can be reached without incurring further harm. This approach has been successfully applied in settings such as the criminal justice system, schools, and workplaces~\cite{Ness2016, Wood2016}.

\subsubsection{A Prevalent Restorative Justice Approach - Apology}
As applied to the online landscape, restorative justice can manifest in many different ways, but one of the most prevalent is through \textbf{apology}. Schoenebeck et al. explored various forms of restorative intervention \revision{to online harassment}, including mediation, identity education, and notably, apology, which was strongly preferred by victims from various social groups, youths, and across countries~\cite{Schoenebeck2021a, Schoenebeck2021b, Schoenebeck2023}. In co-designing to address cyberbullying, \revision{structured restoration systems were proposed where apology served as a condition for resolution, requiring offenders to complete empathy-building training and reconcile with victims before having their punishments lifted~\cite{Aliyu2024}. Applying restorative justice principles, Xiao et al. identified five key needs among harmed adolescents: sensemaking,
emotional support and validation, safety, retribution, and transformation~\cite{Xiao2022}. Apologies specifically addresses two of these needs~---~providing emotional support and validation by prompting offenders to acknowledge their wrongdoing, and enhancing safety by a commitment to cease further harmful behaviors.} Despite not being formally integrated into online governance, volunteer moderators sometimes solicit apologies from offenders, or deliver apologies to targets themselves~\cite{Matias2019, Seering2019}. Given the widespread relevance and importance of apologies in online contexts, we employ this concept as the core framing for our restorative justice tool implementation, 

\subsubsection{Implementing Restorative Justice Online}
Studies have explored various restorative justice implementations in online environments, both manually and technically. As a manual approach, Xiao et. al investigated the viability of the \textit{victim-offender conference}~---~a widely used practice for restorative justice~\cite{Xiao2023}. Under this setup, the moderator, acting as a guiding facilitator, holds separate meetings with the victim and the offender before both parties agree to meet and discuss the harm, potentially reaching a resolution~\cite{Zehr2015}. While promising, particularly for victims, the process is hindered by its labor-intensive nature and the prevailing stigma of punitive systems. Alternatively, technical approaches such as Keeper~\cite{Hughes2020} were introduced, providing an environment to facilitate online restorative justice circles through visual features and spatial attributes. Despite its efficiency in improving interaction quality, Keeper’s separate platform and unfamiliar approach may present adoption and integration problems. Learning a new system demands additional effort and commitment from moderators, especially for content moderation at scale~\cite{Kiene2019}. This is compounded by the fact that many community moderators are unpaid volunteers and already overburdened~\cite{Wohn2019}, adding further pressures in shifting their moderation framework.

These challenges highlight the need for more practical and scalable implementation of restorative justice within online communities.
\revision{
One effective path is to build on toolsets that moderators already rely on, rather than introducing entirely new systems~\cite{Kuo2023}. In many communities, particularly on platforms like Discord, custom bots serve as integral tools to facilitate social process \cite{Hwang2024}, adapt to changing needs~\cite{Kiene2019}, and manage growing membership base~\cite{Kiene2020}. Accordingly, our study builds on this precedent by exploring bots as a familiar, adaptable framework to facilitate stakeholder engagement in a restorative process similar to a victim-offender conference.
}

%we present \textbf{ApoloBot}–short for Apology Bot–a restorative justice tool that helps resolve online interpersonal harm through mediating meaningful apologies from the offender to the victim, with the facilitation of the moderator. To our knowledge, this is the first tool implementing restorative justice within a practical moderation workflow. By deploying the bot, we address the lack of restorative resources while probing new considerations that arise.

\subsubsection{Evaluating Technological Restorative Justice Tools}
Assessing the effectiveness of alternative methods for addressing online harm is important, yet challenging due to the complex social factors involved~\cite{Ma2023, Aliyu2024}. This inquiry extends to restorative justice with key questions: Under which scenarios would restorative implementation make a meaningful difference, and when might it not be worth pursuing?~\cite{Cai2024} How can this approach be implemented as a practical tool, and what are the expected results if it proves effective?~\cite{Xiao2023}

The nuances to restoring interpersonal harm, highlighted by Kou et al., underline the importance of considering “contextuality of toxicity” as a factor for designing restorative mechanisms~\cite{Kou2021}. Xiao et al. argue that restorative justice application should be a value-related question, reflecting one’s moderation goals~\cite{Xiao2023}. These insights suggest that the opportunities for restorative justice tools are heavily influenced by contextual factors. Building on these findings, our work examines the opportunity space of restorative tools across three different scopes: community, moderation practice, and case scenario, which are essential areas of focus for tool adoption, implementation, and effective usage, respectively. Utilizing ApoloBot as a conceptual implementation of restorative justice, we gather insights from moderators through interviews and deployments, identifying specific examples and situations where this approach may or may not be useful. We further pinpoint practical challenges associated with the tool implementation and propose implications for future designs.