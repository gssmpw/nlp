\section{Related Work}
\subsection{Vision Transformers}
The vision transformer design is adapted from Transformer____, which was designed for natural language processing (NLP). This adaptation makes it suitable for computer vision tasks such as image classification____, object detection____, and semantic segmentation____. 

\subsubsection{Class Token \& Global Average Pooling}
ViT____ conducts ablation studies comparing the class token and GAP. Additionally, there are other studies on the use of GAP and class tokens in vision transformers____. Studies such as CeiT____ and T2T-ViT____ use class token, while others like Swin Transformer____ and CPVT____ adapt GAP. CPVT achieves performance improvements by using GAP instead of the class token. Although the class token is not inherently translation-invariant, it can become so through training. By adopting GAP, which is inherently translation-invariant, better improvements in image classification tasks are achieved____. Furthermore, GAP results in even less computational complexity because it eliminates the need to compute the attention interaction between the class token and the image patches. 

\subsection{Position Embeddings in Vision Transformers}

\subsubsection{Absolute Position Embedding} In the transformer, absolute position embedding is generated through a sinusoidal function and added to the input token embedding____. The sinusoidal functions are designed to give the position embedding locally consistent similarity, which helps vision transformers focus more effectively on tokens that are close to each other in the input sequence. This local consistency enhances the model's ability to capture spatial relationships and patterns____.

Besides sinusoidal positional embedding, position embedding can also be learnable. Learnable position embedding is created through training parameters, which are initialized with a fixed-dimensional tensor and updated along with the model's parameters during training. Recently, many models have adopted absolute position embedding due to their effectiveness in encoding positional information____.

\subsubsection{Relative Position Embedding} In addition to absolute position embedding, there is also relative position embedding____. Relative PE encodes the relative position information between tokens. The first to propose relative PE in computer vision was ____. Furthermore, ____ proposed a 2-D relative position encoding for image classification that showed superior performance compared to traditional 2-D sinusoidal embedding. This relative encoding captures spatial relationships between tokens more effectively. In related research, iRPE ____ improves relative PE by incorporating query interactions and relative distance modeling in self-attention. RoPE ____ introduces flexible sequence lengths, decaying inter-token dependency, and relative position encoding in linear self-attention.

\begin{figure*}[t]
\centering
\includegraphics[width=2.0\columnwidth]{Figure/Figure3.pdf}
\caption{The overview of the various methods. (a) ViT. (b) LaPE____. (c) PVG, an improved Layer-wise structure. Specifically, we adopt a structure where the token embedding and PE are added before entering layer 0 and a hierarchical structure for delivering PE, excluding layer 0. (d) MPVG. The main difference from PVG is whether the initial PE is delivered to the Last LN.} \label{Figure3}

\end{figure*}