[
  {
    "index": 0,
    "papers": [
      {
        "key": "trnasformer",
        "author": "Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\\L}ukasz and Polosukhin, Illia",
        "title": "Attention is all you need"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "vit",
        "author": "Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others",
        "title": "An image is worth 16x16 words: Transformers for image recognition at scale"
      },
      {
        "key": "DeiT",
        "author": "Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\\'e}gou, Herv{\\'e}",
        "title": "Training data-efficient image transformers \\& distillation through attention"
      },
      {
        "key": "swin",
        "author": "Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining",
        "title": "Swin transformer: Hierarchical vision transformer using shifted windows"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "detr",
        "author": "Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey",
        "title": "End-to-end object detection with transformers"
      },
      {
        "key": "detr2",
        "author": "Zhu, Xizhou and Su, Weijie and Lu, Lewei and Li, Bin and Wang, Xiaogang and Dai, Jifeng",
        "title": "Deformable detr: Deformable transformers for end-to-end object detection"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "setr",
        "author": "Zheng, Sixiao and Lu, Jiachen and Zhao, Hengshuang and Zhu, Xiatian and Luo, Zekun and Wang, Yabiao and Fu, Yanwei and Feng, Jianfeng and Xiang, Tao and Torr, Philip HS and others",
        "title": "Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers"
      },
      {
        "key": "setr2",
        "author": "Wang, Yuqing and Xu, Zhaoliang and Wang, Xinlong and Shen, Chunhua and Cheng, Baoshan and Shen, Hao and Xia, Huaxia",
        "title": "End-to-end video instance segmentation with transformers"
      },
      {
        "key": "segmenter",
        "author": "Strudel, Robin and Garcia, Ricardo and Laptev, Ivan and Schmid, Cordelia",
        "title": "Segmenter: Transformer for semantic segmentation"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "vit",
        "author": "Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others",
        "title": "An image is worth 16x16 words: Transformers for image recognition at scale"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "gap_cls",
        "author": "Raghu, Maithra and Unterthiner, Thomas and Kornblith, Simon and Zhang, Chiyuan and Dosovitskiy, Alexey",
        "title": "Do vision transformers see like convolutional neural networks?"
      },
      {
        "key": "cpvt",
        "author": "Chu, Xiangxiang and Tian, Zhi and Zhang, Bo and Wang, Xinlong and Shen, Chunhua",
        "title": "Conditional positional encodings for vision transformers"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "ceit",
        "author": "Yuan, Kun and Guo, Shaopeng and Liu, Ziwei and Zhou, Aojun and Yu, Fengwei and Wu, Wei",
        "title": "Incorporating convolution designs into visual transformers"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "t2t",
        "author": "Yuan, Li and Chen, Yunpeng and Wang, Tao and Yu, Weihao and Shi, Yujun and Jiang, Zi-Hang and Tay, Francis EH and Feng, Jiashi and Yan, Shuicheng",
        "title": "Tokens-to-token vit: Training vision transformers from scratch on imagenet"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "swin",
        "author": "Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining",
        "title": "Swin transformer: Hierarchical vision transformer using shifted windows"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "cpvt",
        "author": "Chu, Xiangxiang and Tian, Zhi and Zhang, Bo and Wang, Xinlong and Shen, Chunhua",
        "title": "Conditional positional encodings for vision transformers"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "cpvt",
        "author": "Chu, Xiangxiang and Tian, Zhi and Zhang, Bo and Wang, Xinlong and Shen, Chunhua",
        "title": "Conditional positional encodings for vision transformers"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "trnasformer",
        "author": "Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\\L}ukasz and Polosukhin, Illia",
        "title": "Attention is all you need"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "trnasformer",
        "author": "Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\\L}ukasz and Polosukhin, Illia",
        "title": "Attention is all you need"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "vit",
        "author": "Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others",
        "title": "An image is worth 16x16 words: Transformers for image recognition at scale"
      },
      {
        "key": "DeiT",
        "author": "Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\\'e}gou, Herv{\\'e}",
        "title": "Training data-efficient image transformers \\& distillation through attention"
      },
      {
        "key": "swin",
        "author": "Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining",
        "title": "Swin transformer: Hierarchical vision transformer using shifted windows"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "rpe",
        "author": "Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish",
        "title": "Self-attention with relative position representations"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "rpe",
        "author": "Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish",
        "title": "Self-attention with relative position representations"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "2drpe",
        "author": "Bello, Irwan and Zoph, Barret and Vaswani, Ashish and Shlens, Jonathon and Le, Quoc V",
        "title": "Attention augmented convolutional networks"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "irpe",
        "author": "Wu, Kan and Peng, Houwen and Chen, Minghao and Fu, Jianlong and Chao, Hongyang",
        "title": "Rethinking and improving relative position encoding for vision transformer"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "rope",
        "author": "Heo, Byeongho and Park, Song and Han, Dongyoon and Yun, Sangdoo",
        "title": "Rotary position embedding for vision transformer"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "lape",
        "author": "Yu, Runyi and Wang, Zhennan and Wang, Yinhuai and Li, Kehan and Liu, Chang and Duan, Haoyi and Ji, Xiangyang and Chen, Jie",
        "title": "LaPE: Layer-adaptive position embedding for vision transformers with independent layer normalization"
      }
    ]
  }
]