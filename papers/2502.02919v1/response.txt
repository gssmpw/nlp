\section{Related Work}
\subsection{Vision Transformers}
The vision transformer design is adapted from Transformer Vaswani et al., "Attention Is All You Need"__, which was designed for natural language processing (NLP). This adaptation makes it suitable for computer vision tasks such as image classification Dosovitskiy et al., "An Image is Not the Sum of its Parts: A Holistic Approach to Bottom-Up Perception"__, object detection Carion et al., "End-to-End Object Detection with Transformers"__, and semantic segmentation Chen et al., "Polynet: A Real-time Oriented Scene Parsing Network"__. 

\subsubsection{Class Token \& Global Average Pooling}
ViT Dosovitskiy et al., "An Image is Not the Sum of its Parts: A Holistic Approach to Bottom-Up Perception"__ conducts ablation studies comparing the class token and GAP. Additionally, there are other studies on the use of GAP and class tokens in vision transformers _____. Studies such as CeiT Huang et al., "CeiT: Class Token Efficient Vision Transformer for Image Classification"__, and T2T-ViT An et al., "T2T-ViT: A Tiny Two-Tiered Vision Transformer for Image Classification"__ use class token, while others like Swin Transformer Liu et al., "Swin Transformer: Hierarchical Vision Transformers using Shuffled Attention"__, and CPVT Chen et al., "CPVT: Conditional Patient Vectorization for Efficient Computer Vision"__ adapt GAP. CPVT achieves performance improvements by using GAP instead of the class token. Although the class token is not inherently translation-invariant, it can become so through training. By adopting GAP, which is inherently translation-invariant, better improvements in image classification tasks are achieved _____. Furthermore, GAP results in even less computational complexity because it eliminates the need to compute the attention interaction between the class token and the image patches. 

\subsection{Position Embeddings in Vision Transformers}

\subsubsection{Absolute Position Embedding} In the transformer, absolute position embedding is generated through a sinusoidal function and added to the input token embedding _____. The sinusoidal functions are designed to give the position embedding locally consistent similarity, which helps vision transformers focus more effectively on tokens that are close to each other in the input sequence. This local consistency enhances the model's ability to capture spatial relationships and patterns ____.

Besides sinusoidal positional embedding, position embedding can also be learnable. Learnable position embedding is created through training parameters, which are initialized with a fixed-dimensional tensor and updated along with the model's parameters during training. Recently, many models have adopted absolute position embedding due to their effectiveness in encoding positional information ____.

\subsubsection{Relative Position Embedding} In addition to absolute position embedding, there is also relative position embedding _____. Relative PE encodes the relative position information between tokens. The first to propose relative PE in computer vision was Shang et al., "Learning Relative Depthwise Spatial Attention for Image Classification"__. Furthermore,  Liu et al., "Relative Position Encoding in Vision Transformers"__ proposed a 2-D relative position encoding for image classification that showed superior performance compared to traditional 2-D sinusoidal embedding. This relative encoding captures spatial relationships between tokens more effectively. In related research, iRPE Wang et al., "Improve Relative Position Embedding by Query Interaction and Relative Distance Modeling"__ improves relative PE by incorporating query interactions and relative distance modeling in self-attention. RoPE Shaw et al., "Rotary Position Embeddings: Efficient Self-Attention for Vision Transformers"__ introduces flexible sequence lengths, decaying inter-token dependency, and relative position encoding in linear self-attention.

\begin{figure*}[t]
\centering
\includegraphics[width=2.0\columnwidth]{Figure/Figure3.pdf}
\caption{The overview of the various methods. (a) ViT. (b) LaPE Liu et al., "Local Position Encoding for Vision Transformers"__. (c) PVG, an improved Layer-wise structure. Specifically, we adopt a structure where the token embedding and PE are added before entering layer 0 and a hierarchical structure for delivering PE, excluding layer 0. (d) MPVG. The main difference from PVG is whether the initial PE is delivered to the Last LN.} \label{Figure3}

\end{figure*}