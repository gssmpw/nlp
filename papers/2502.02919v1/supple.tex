%File: anonymous-submission-latex-2025.tex

%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.


%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.

\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2025.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.



\twocolumn[{%
    \renewcommand\twocolumn[1][]{#1}%
    \vspace{-3em}
    \begin{center}
        \centering
        \includegraphics[width=0.99\textwidth]{supple_fig/supple_main_fig.pdf}
        \captionof{figure}{
        %
        Heatmaps(averaged over the batch size), beta and gamma values in the Last LN, and cosine similarity for each method. (a) represents the PVG using the class token. The cosine similarity refers to the similarity of the class token before and after the Last LN. (b) represents the PVG using GAP. The cosine similarity refers to the similarity of the token averages in the heatmaps before and after the Last LN. (c) represents the MPVG. The cosine similarity refers to the similarity of the token averages in the heatmaps before and after the Last LN.
        }
        \label{supple_fig1}
        \vspace{5pt}
    \end{center}%
    }]
    
\section{Appendix}

\subsection{A Detailed Analysis of PE in the Last LN}
We provide a detailed analysis of the role of position embedding (PE) in the Last LN (LN means Layer Normalization~\cite{ln}). As shown in Fig~\ref{supple_fig1}, (b) visualizes the gamma and beta values, which are the affine parameters of the Last LN, in PVG where PE is not delivered to the Last LN. In (c), the gamma and beta values of the Last LN are visualized in MPVG, where PE is delivered to the Last LN.

In PVG, upon examining the beta affine parameter, we find that the variance of beta is 0.2387, indicating significant variability. Specifically, we observe that the beta value counterbalances the high-value dimensions present before the Last LN. In contrast, in MPVG, where PE is delivered to the Last LN, the variance of beta is much smaller at 0.0193 compared to PVG. This suggests that, in MPVG, the values before the Last LN are counterbalanced not by the beta value but by the PE.

In conclusion, the advantage of using PE to eliminate high-value dimensions in MPVG, rather than relying on beta as in PVG, is as follows. In a Layer-wise structure, PE causes high-value dimensions to become more pronounced across dimensions. This suggests that PE counterbalances these high-value dimensions, taking over the role of LN in removing high-value dimensions. This phenomenon results in the token embedding ($\mathit{x}$) retaining values that should have been counterbalanced by PE, even after passing through the layers. Compensating for these values using PE, rather than relying solely on LN's beta, allows for more accurate counterbalancing, leading to fewer lost features compared to using LN alone to remove high-value dimensions. This suggests that, in the conventional Layer-wise structure, high-values dimensions in the Last LN were counterbalanced using only LN. However, more accurate features can be preserved by using PE to counterbalance these values.

As shown in Fig~\ref{supple_fig2}, MPVG captures objects more effectively than PVG, even when comparing the same samples before and after the Last LN. This demonstrates that PE in the Last LN maintains the counterbalancing directionality in a Layer-wise structure. The Last LN's role is alleviated by sustaining this directionality, enabling a richer and more accurate understanding of the objects.


\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.7\linewidth]{supple_fig/supple_fig2.pdf}
    \caption{The visualizations and heatmaps before and after the Last LN in the PVG and MPVG methods are shown. These heatmaps represent a single sample. For the visualization heatmap, the norm values of each of the 196 tokens are calculated and visualized as heatmaps.}
    \label{supple_fig2}
\end{figure*}

\subsection{Analysis on Conflicting Results Between Class token and GAP}

In general, when the GAP method, which performs better than the class token method in image classification, is combined with the Layer-wise method, which improves the expressiveness of PE and enhances the performance of vision transformers, it leads to a decrease in performance. As we discussed, applying the GAP method in the Layer-wise structure results in a conflicting result, leading to a performance decline.

To analyze the cause, we examine the gamma and beta values of Layer Normalization (LN) in the Last LN and the cosine similarity before and after the Last LN. As shown in Figure 1, (a) represents PVG with the class token method, while (b) represents PVG with the GAP method. We observed that the variance of the beta value, an affine parameter in the Last LN, is lower in (a) compared to (b). Additionally, when observing the heatmaps before and after the Last LN, we noticed that there are no significant changes apart from the 0th row representing the class token. This suggests that the Last LN primarily focuses on the class token in the class token method.

Specifically, when comparing the cosine similarity of (a) and (b), (a) shows a value of 0.4597, while (b) shows a value of 0.2995. The high cosine similarity in (a) and the low variance of beta indicate that, in the Layer-wise structure of the class token method, the Last LN has less of a role in removing high-value dimensions compared to the GAP method. On the other hand, the lower cosine similarity and higher variance of beta in the GAP method suggest that if counterbalancing by PE does not occur in the Last LN, the Last LN alone must remove the high-value dimensions to maintain balance. This indicates that, in the Layer-wise structure, the role of PE in maintaining balance is much more critical in the GAP method, where class predictions are made directly through the average of the tokens. As shown in Figure 2, this issue can lead to less accurate results in vision transformers because the Last LN must remove high-value dimensions. In conclusion, due to the difference in the extent of the burden placed on the LN to counterbalance high-value dimensions in the class token and GAP methods, the GAP method, where the role of PE is relatively more critical, exhibits inferior performance compared to the class token method.





\subsection{Visualization on Last LN in Non-Layer-wise and Layer-wise structures.}
We provide a more detailed figure in Last LN. As shown in Fig~\ref{supple_fig3}, (a) is identical to Fig 2 in the main paper but offers a visualization after applying the Last LN. (b) visualizes Fig 5-(b) from the main paper as a heatmap. Identical to Figure 2 in main paper, the x-axis represents the dimensions, and the y-axis represents the number of tokens.

Specifically, in Fig~\ref{supple_fig3}-(b), since the structure is not Layer-wise, there is no value in the token embedding that the PE counterbalances. As a result, even if PE is added in the Last LN, there is no such directionality, leading to a decrease in performance. After applying the Last LN, the correlation values are significantly lower than (a). In contrast, in Fig~\ref{supple_fig3}-(a), because the Layer-wise structure allows PE to counterbalance the token embedding values at each layer, this directionality is maintained both before and after applying the Last LN.

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=1.93\columnwidth]{supple_fig/Figure_supple.pdf}
    \caption{The difference between delivering PE to the Last LN in Non-Layer-wise and Layer-wise structures. In (a), PE is delivered to the Last LN within a Layer-wise structure, while in (b), only PE is delivered to the Last LN in a Non-Layer-wise structure.}
    \label{supple_fig3}
\end{figure*}


\begin{table*}[ht]
\renewcommand{\arraystretch}{1.1}
\resizebox{\textwidth}{!}{
\begin{tabular}{cccccccc}
\hline
Dataset                                         & Model                         & Learning Rate & Scheduler                                                      & Weight Decay & Batch Size & Epochs                                                               & Warm-up Epochs \\ \hline
\multicolumn{1}{c|}{\multirow{5}{*}{\begin{tabular}[c]{@{}c@{}}ImageNet-1K\\ ~\cite{imagenet}\end{tabular}}}  & \multicolumn{1}{c|}{DeiT}  & 5e-4          & \begin{tabular}[c]{@{}c@{}}cosine\end{tabular} & 0.05         & 1024        & 300                                                                  & 5              \\ \cline{2-8} 
\multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{Swin}     & 1e-3          & \begin{tabular}[c]{@{}c@{}}cosine\end{tabular} & 0.05         & 1024        & 300                                                                  & 20             \\ \cline{2-8} 
\multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{CeiT}     & 5e-4          & \begin{tabular}[c]{@{}c@{}}cosine\end{tabular} & 0.05         & 1024        & 300                                                                  & 5              \\ \cline{2-8} 
\multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{T2T-ViT}  & 1e-3          & \begin{tabular}[c]{@{}c@{}}cosine\end{tabular} & 0.03         & 512        & \begin{tabular}[c]{@{}c@{}}300+10\end{tabular} & 10             \\ \hline
\multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}CIFAR-100\\ ~\cite{cifar100}\end{tabular}}} & \multicolumn{1}{c|}{ViT-Lite} & 6e-4          & \begin{tabular}[c]{@{}c@{}}cosine\end{tabular} & 0.06         & 128        & \begin{tabular}[c]{@{}c@{}}300+10\end{tabular} & 10             \\ \cline{2-8} 
\multicolumn{1}{c|}{}                           & \multicolumn{1}{c|}{T2T-ViT}  & 5e-2          & \begin{tabular}[c]{@{}c@{}}cosine\end{tabular} & 5e-4         & 128        & 60                                                                   & -              \\ \hline
\end{tabular}}
\caption{Hyperparameter settings for image classification on ImageNet-1K and CIFAR-100.}
\label{table2-supple}
\end{table*}

\begin{table*}[t]
\renewcommand{\arraystretch}{1.1}
\resizebox{\textwidth}{!}{
\begin{tabular}{cccccccccc}
\hline
Dataset                                                        & Model          & Framework  & \begin{tabular}[c]{@{}c@{}}Backbone\\ Prei-train\end{tabular} & \begin{tabular}[c]{@{}c@{}}Crop \\ Size\end{tabular} & Optimizer & LR   & Scheduler & Weight Decay & Batch Size \\ \hline
\begin{tabular}[c]{@{}c@{}}COCO 2017\\~\cite{coco}\end{tabular} & ViT-Adapter-Ti & Mask R-CNN & DeiT-Ti                                                       & 1024                                                 & AdamW     & 1e-4 & 3x+MS     & 0.05         & 8          \\ \hline
\end{tabular}}
\caption{Hyperparameter settings for object detection on COCO 2017.}
\label{table3-supple}
\end{table*}


\begin{table*}[t!]
\renewcommand{\arraystretch}{1.1}
\resizebox{\textwidth}{!}{
\begin{tabular}{cccccccccc}
\hline
Dataset & Model          & Framework & \begin{tabular}[c]{@{}c@{}}Backbone\\ Prei-train\end{tabular} & \begin{tabular}[c]{@{}c@{}}Crop \\ Size\end{tabular} & Optimizer & LR    & Scheduler & Weight Decay & Batch Size \\ \hline
\begin{tabular}[c]{@{}c@{}}ADE20K\\~\cite{ade20k}\end{tabular}  & ViT-Adapter-Ti & UperNet   & DeiT-Ti                                                       & 512                                                  & AdamW     & 12e-5 & 160K      & 0.01         & 8          \\ \hline
\end{tabular}}
\caption{Hyperparameter settings for semantic segmentation on ADE20K.}
\label{table4-supple}
\end{table*}




