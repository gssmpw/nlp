\section{Data Processing Details}
\label{appendix:data_processing}

\subsection{Parsing Sections}
\label{appdx:parsing-sections}

Our prompt for parsing sections as part of creating \ourdata is shown below. 
In order to ensure consistency across subjects in how sections are divided, we provide manually annotated few-shot examples. 
In addition, we use GPT-4o for this part as it is a one-time expense and data quality has important implications for all the experiments that we conduct in this work.  

\input{prompts/section_extraction}

\subsection{Parsing Exam Questions}
\label{appdx:parsing-exam-questions}

We parse the end-of-chapter review questions in OpenStax textbooks based on the markdown headers and formatting with BeautifulSoup.\footnote{\url{https://www.crummy.com/software/BeautifulSoup/}}
There are occasionally ill-formatted questions that we throw out, since the main goal here is to find a subset of chapters that are suitable for testing \ours. 
We avoid chapters with too few ($<10$) and too many questions ($>25$) so that our simulations are conducted with enough questions for measuring question utility while also completing within a reasonable timeframe.  

\subsection{Bloom's Taxonomy Level Distribution}
\label{appdx:bloom-taxonomy-level-distribution}
% To analyze model performance on how it affects learning outcomes at different cognitive levels according to the revised Bloom's Taxonomy, we classify exam questions with our Bloom classification prompt~(\secref{appdx:bloom-classification-prompt}). 
% The distribution in \ourdata's train and test sets are shown in \autoref{fig:bloom-distribution}.
We share our prompt for categorizing questions based on the revised Bloom's taxonomy below: 
\input{prompts/bloom_classification}


