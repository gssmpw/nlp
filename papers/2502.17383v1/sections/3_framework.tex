\section{\ours}
\label{sec:framework}

In this section, we introduce \ours as a method for measuring question utility and using high-utility questions to fine-tune question generators. 
We first provide an overview of \ours (\secref{ssec:quest-overview}) and delve into the details of each its procedures (\secref{ssec:quest-question-generation}-\secref{ssec:quest-train}). 

\input{sections/2_problem}

\subsection{Overview}
\label{ssec:quest-overview}

\ours consists of the following:
(1) a \textbf{question generator} (\(M_q\)), which takes a document \(D\) and generates a set of questions \(Q = \{q_1, q_2, \dots, q_n\}\) (\secref{ssec:quest-question-generation});
(2) an \textbf{answer generator} (\(M_a\)) that then produces an answer \(a_i\) for each question \(q_i \in Q\), forming the set of question-answer pairs \(\text{QA} = \{(q_1, a_1), (q_2, a_2), \dots, (q_n, a_n)\}\) using parametric knowledge (\secref{ssec:quest-question-generation});
(3) a \textbf{learner simulator}, which models a learner (\(M_l\))’s understanding by having an evaluator (\(M_e\)) assess their performance on a final exam $E$ using only \(\text{QA}\) (\secref{ssec:quest-evaluation}); and
(4) a \textbf{utility estimator}, which runs the learner simulator multiple times with different subsets of \(\text{QA}\), estimating the contribution of individual questions to the learner’s overall performance (\secref{ssec:quest-train}).
See Figure~\ref{fig:framework}.
Additional details, including the prompts used for each module, are provided in Appendix~\ref{appendix:quest-details}.


\subsection{Question Generation}
\label{ssec:quest-question-generation}
The \textbf{Question Generator} (\( M_q \)) generates a set of questions (\( Q \)) based on the input document.
For each ordered section \( S_k \) in \( D \), where \( S_k \) represents the part of the document the learner is currently reading (referred to as the \textit{anchor}), the question generator considers both \( S_k \) and its preceding context \( C_k = \{S_1, S_2, \dots, S_{k-1}\}\) to generate a corresponding set of questions \( Q_k = \{q_k^1, q_k^2, \dots, q_k^n\} \), formulated as \( Q_k = M_q(S_k, C_k) \).


This approach ensures that the generated questions are contextually relevant and informed by the surrounding content, aligning with prior research on inquisitive question generation~\cite{wu2024questions}.  
Once the questions are generated, the \textbf{Answer Generator} (\( M_a \)) produces an answer \( a \) for each question \( q \) using the model’s parametric knowledge, forming a set of question-answer pairs: \( \text{QA} = \{(q_1, a_1), (q_2, a_2), \dots, (q_n, a_n)\} \). 
These QA pairs serve as the foundation for subsequent evaluation and simulation stages.

\begin{figure}[t!]
    \centering
    \begin{minipage}{\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{figures/framework.pdf}
\end{minipage}
\caption{\textbf{\ours Framework.} $M_q$ creates questions, and $M_a$ provides answers.
Learner simulator evaluates these QA pairs using a Learner $M_l$ and an Evaluator $M_e$.
The Utility Estimator runs multiple simulations to approximate the utility of each question.
Rejection sampling ensures that only high-utility questions are used to refine $M_q$.}
\vspace{-0.3cm}
    \label{fig:framework}
\end{figure}

\subsection{Evaluating the Question Generator}
\label{ssec:quest-evaluation}
Once the QA pairs for the document \( D \) are generated using \( M_q \) and \( M_a \), we assess the effectiveness of the question generator by employing the \textbf{Learner Simulator}, which consists of a \textbf{Learner} (\( M_l \)) and an \textbf{Evaluator} (\( M_e \)).
The learner model \( M_l \) simulates a learner's understanding by attempting the final exam \( E \) using only the generated QA pairs, producing responses \( P = M_l(E, \text{QA}) \).
The evaluator model \( M_e \) then assesses the learner’s responses \( P \) by comparing them against ground-truth answers when available or using parametric knowledge to assign a score.
The score can serve as a metric to assess the quality of \( M_q \), providing insights into the usefulness (\textit{i.e.,} \textit{utility}) of the generated questions in answering the final exam.

\subsection{Improving the Question Generator}
\label{ssec:quest-train}

Our objective is to design a question generator \( M_q \) that maximizes the learner simulator's performance on \( E \) when provided with \( \text{QA} \). 
This assumes that questions contributing more effectively to solving \( E \) are high-utility questions.

To enhance the quality of the question generator \( M_q \), we aim to learn the patterns of high-utility questions—those that lead to better performance in the learner simulator.
To achieve this, we estimate the utility \( u \) of each question-answer pair \( (q, a) \in \text{QA} \).
The utility estimator runs the learner simulator multiple times with different subsets of QA, computing the \textit{single-one gain}, which is the score obtained using only \( (q, a) \), and the \textit{all-but-one gain}, which is the score with all pairs except \( (q, a) \).
The utility of \( (q, a) \) is then computed as the average of these two scores.
We retain only the pairs where utility exceeds a threshold \( \theta \) and train \( M_q \) using only these high-utility pairs, following a rejection sampling strategy~\cite{bai2022constitutional}.


