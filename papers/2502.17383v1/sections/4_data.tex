\section{\ourdata}
\label{sec:textbook-exam}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/data_framework.pdf}    \caption{\textbf{Overview of  \ourdata curation}. Given a chapter $C$, we use an LM to segment the document $D$ into sections and heuristically extract review questions to form the exam $E$. The LM then classifies each question in $E$ by Bloom’s taxonomy category and maps it to its relevant section.}
    \label{fig:dataset-overview}
    \vspace{-0.3cm}
\end{figure}

In order to evaluate \ours, we curate  \ourdata, a dataset where each entry contains a document \(D\) along with a corresponding set of exam questions \(E\).
An overview of \ourdata is illustrated in \autoref{fig:dataset-overview}.
% In this section, we first describe our data processing pipeline (\secref{ssec:textbook-exam-pipeline}) and then provide data statistics (\secref{ssec:textbook-exam-statistics}).

\subsection{Data Processing}
\label{ssec:textbook-exam-pipeline}
Our pipeline starts with textbooks from the OpenStax repository\footnote{\url{https://github.com/philschatz/textbooks}}.
Each textbook is divided into chapters, where each chapter \(C\) contains learning objectives, main content, and review questions.
For each \(C\), we parse the main content to build \(D\) and the review questions to form \(E\).
% Specifically, only the main content is used to construct \(D\), while the review questions are used to create \(E\).

\paragraph{Extracting sections.}
To simulate a learner incrementally progressing through a chapter, we divide each chapter into sections using an LM-based document structuring method. 
The LM segments \( D \) into \( n \) sections, denoted as \( \{S_1, S_2, \ldots, S_n\} \subset D \), while also extracting the corresponding review questions \( E \). 
However, not all review questions come with ground-truth answers, as some textbooks do not provide them (see Table~\ref{tab:textbook-exam-statistics} for the proportion of \( E \) with answers). 
To ensure consistency in section segmentation across different subjects, we manually annotate the first 2–5 sections from one sample per subject. 
These annotated samples serve as few-shot examples in our LM prompt (see Appendix~\ref{appdx:parsing-sections} for details).

\paragraph{Extracting questions.}
% We developed a custom parsing script using BeautifulSoup4 to extract questions and their corresponding answers. 
To maintain a balance between evaluation depth and computational feasibility, we include only chapters that contain at least 10 questions—ensuring sufficient coverage for assessment—while capping the maximum number of questions at 25 to keep learning simulations computationally manageable.

\subsection{Data Statistics}
\label{ssec:textbook-exam-statistics}
\begin{table}[t!]
    \centering
    \resizebox{\columnwidth}{!}{
        \begin{tabular}{lccccc}
        \toprule
            \textbf{Subject} & \textbf{\# $C$} & \textbf{Split} & \textbf{\# $E$ / $C$} & \textbf{\% $E$ w/ answer} & \textbf{\# $S$ / $C$} \\
        \midrule
            Microbiology & 20 & Train & 12.4 & 64\% & 16.4 \\
                         & 5  & Test  & 13.4 & 58\% & 17.0 \\
        \midrule
            Chemistry    & 20 & Train & 14.2 & 51\% & 11.0 \\
                         & 5  & Test  & 16.2 & 49\% & 6.4 \\
        \midrule
            Economics    & 20 & Train & 12.2 & 23\% & 14.1 \\
                         & 5  & Test  & 12.2 & 23\% & 14.4 \\
        \midrule
            Sociology    & 20 & Train & 10.4 & 62\% & 16.6 \\
                         & 5  & Test  & 11.2 & 67\% & 19.0 \\
        \midrule
            US History   & 20 & Train & 7.2 & 51\% & 14.9 \\
                         & 5  & Test  & 8.4 & 38\% & 13.2 \\
        \bottomrule
        \end{tabular}
    }
    \caption{\textbf{Data Statistics} of \ourdata. \# $ C$: number of chapters, \# $E/C$: avg. number of questions per chapter, \% $E$ w/ answer: proportion of questions that have reference answer, \# $S/C$: avg. number of sections per chapter.}
    \label{tab:textbook-exam-statistics}
\end{table}
For each subject, we curate 25 sequential chapters \(C\), each containing both \(D\) and \(E\).
The chapters are arranged in their natural order, with the first 20 used for training and the last five reserved for evaluation.  
There is the risk that content in later chapters may include information from prior chapters (e.g., revisiting prerequisite knowledge). 
Therefore, preserving this sequential structure between the training and test set is essential for preventing information leakage and fairly assessing a model's learning process.
Table~\ref{tab:textbook-exam-statistics} shows an overview of the statistics of the resulting \ourdata.

\subsection{Distribution of Question Types}
\label{ssec:textbook-exam-bloom}

 \begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/bloom_taxonomy_counts_vertical.png}
    \caption{\textbf{Bloom's taxonomy distribution} in \ourdata. \ourdata consists of questions that require a wide variety of cognitive levels and the dominant categories vary for each subject.}
    \label{fig:bloom-distribution}
\end{figure}

To better understand how final exams assess a learner’s comprehension on multiple dimensions, we categorize questions in \ourdata\ based on the revised \textit{Bloom’s Taxonomy}~\cite{krathwohl2002revision_bloom}. 
Using an LM, we assign a cognitive depth \( d_j \) to each question \( E_j \in E \), classifying them into six categories: \textit{Remembering, Understanding, Applying, Analyzing, Evaluating}, and \textit{Creating}.
Additionally, we identify the relevant sections \( S_j \subset D \) that correspond to each question.

The distribution, shown in \autoref{fig:bloom-distribution}, indicates that different subjects emphasize different cognitive skills.
For instance, questions in Microbiology and Sociology primarily focus on \textit{Remembering} and \textit{Understanding}, whereas Chemistry and Economics exhibit a more varied distribution.
This analysis highlights the diverse cognitive demands across subjects and underscores how \ourdata\ provides a multifaceted evaluation of learning outcomes through final exams. 
For further details on data processing, refer to Appendix~\ref{appendix:data_processing}.













% \dongho{Number of textbooks?}

% \dongho{For each textbook, how may $D$?}



% \subsection{Validation}
% \dongho{Let's make a ground truth to see how reliable the data processing pipeline it is. -- for each textbook.}

% \paragraph{Validation of answer for $E_j$.}

% \paragraph{Validation of cognitive depth $d_j$ for $E_j$.}

% \paragraph{Validation of related sections $S_j$ for $E_j$.}