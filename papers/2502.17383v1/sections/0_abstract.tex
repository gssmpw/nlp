\begin{abstract}
Asking questions is a fundamental aspect of learning that facilitates deeper understanding. 
However, characterizing and crafting questions that effectively improve learning remains elusive. 
To address this gap, we propose \ours (\textbf{Q}uestion \textbf{U}tility \textbf{E}stimation with \textbf{S}imulated \textbf{T}ests)\footnote{\href{https://github.com/usc-isi-i2/quest}{https://github.com/usc-isi-i2/quest}}. 
\ours simulates a learning environment that enables the quantification of a question's \textit{utility} based on its direct impact on improving learning outcomes. 
Furthermore, we can identify high-utility questions and use them to fine-tune question generation models with rejection sampling.
We find that questions generated by models trained with rejection sampling based on question utility result in exam scores that are higher by at least 20\% than those from specialized prompting grounded on educational objectives literature and models fine-tuned with indirect measures of question quality, such as \textit{saliency} and \textit{expected information gain}. 






% Our ablations and qualitative analyses show that \justin{interesting finding that characterizes high-utility questions.}
% \ours presents a promising avenue of using reward signals from simulated environments to improve question generators. 

% Specifically, \ours measures a question's \textit{utility} by leveraging a language model to roleplay as a novice student that asks questions and receives answers while reading a textbook chapter and uses them to solve exam problems. 



% These results demonstrate that our measure of utility provides useful supervision signal for training question generation models that promote better learning outcomes.  
% We provide qualitative analyses and ablation studies to shed light on what makes \ours effective.
% To conduct our experiments with QUEST, we develop \textsc{Textbook-Exam}, a dataset where each sample consists of textbook passages paired with corresponding exam questions that test the learner's comprehension. 
% Our experimental results show that
% \dongho{
% (1) simulated utility provides useful supervision signal for training question generation models (correlates with ...);
% (2);
% (3).
% }
\end{abstract}


% \begin{abstract}
% Asking questions is a fundamental part of the learning process, facilitating deeper understanding and knowledge retention. However, measuring the \textit{utility} of a question—its direct contribution to improving comprehension—remains a challenge. Existing evaluation methods rely on indirect metrics such as information gain and saliency, which do not explicitly assess how well a question enhances learning.

% In this work, we introduce \textsc{QUEST} (\textbf{Q}uestion \textbf{U}tility \textbf{E}stimation and \textbf{S}imulation \textbf{T}oolkit), a framework that leverages large language models (LLMs) as reader simulators to estimate question utility and improve question generation. We first construct \textsc{Textbook-Exam}, a dataset containing reading passages paired with corresponding exam questions to evaluate comprehension. Using this dataset, \textsc{QUEST} consists of three key components: (1) a \textit{question generator} that produces questions sequentially based on a passage, (2) a \textit{reader simulator} that models user understanding by comparing exam performance before and after exposure to the generated questions, and (3) a \textit{utility estimator} that measures the value of individual questions by running multiple simulations with different subsets of questions. 

% We further leverage utility estimation to refine question generation via \textit{rejection sampling}, where LLM simulation serves as a reward signal to improve the model iteratively. Experimental results demonstrate the effectiveness of LLM-based simulation as a utility estimator, its robustness across different model configurations, and the impact of utility-guided training in generating high-quality questions. Our findings highlight the potential of utility-driven approaches to improve question generation and evaluation.
% \end{abstract}

