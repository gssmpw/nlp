\section{Introduction}


\begin{figure}[t!]
    \centering
    \begin{minipage}{\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{figures/motivation.pdf}
    \caption{
    While metrics such as \textcolor{darkorange}{saliency} are useful for assessing a question's relevance based on whether it is answered by subsequent content, it fails to measure a question's usefulness for downstream tasks, such as learning outcomes. 
    We propose question \textcolor{darkgreen}{\textbf{\textit{utility}}} as a new metric measured with simulations for quantifying its direct contributions towards a downstream task. 
    }
    \vspace{-0.3cm}
    \label{fig:motivation}
\end{minipage}
\vspace{-0.3cm}
\end{figure}


Asking good questions offers broad benefits.
Effective teachers ask questions to promote deeper understanding~\cite{bloom1956handbook, tofade2013best} and encourage divergent thinking~\cite{ciardiello1998did, graesser2010good, vale2013value}, while high-performing students ask clarifying questions or self-reflect to refine their  understanding~\cite{abbott1980teaching, davey1986effects, good1988learning, brassell2008comprehension, van2003questioning, davoudi2015systematic}. 
In addition, asking questions helps establish common ground~\cite{rao-daume-iii-2018-learning, aliannejadi2019asking, yu-etal-2020-interactive, zamani2020generating, white-etal-2021-open} and thus catalyzes favorable outcomes in various applications, such as therapy~\cite{shaikh-etal-2024-grounding}, response generation~\cite{zhou-etal-2022-reflect, cho-etal-2024-speechworthy}, and content moderation~\cite{cho-etal-2024-language}.  


But what characterizes \textit{good} questions and how do we craft them?
Prior work in psychology literature is limited to general guidelines for assessing question quality based on heuristics~\cite{graesser2010good, vale2013value} or experiments within a specific domain such as language learning ~\cite{hofstein2005developing} or chemistry~\cite{davoudi2015systematic}. 
On the other hand, recent work in natural language processing measures question quality with indirect metrics, such as \textit{expected information gain}, the additional information provided by an answer to a question~\cite{lindley1956measure, rao-daume-iii-2018-learning, yu-etal-2020-interactive, white-etal-2021-open, keh-etal-2024-asking}, and \textit{saliency}, how effectively a question introduces new concepts or clarifies key ideas that appear in future content~\cite{wu2024questions}.
These works cannot directly evaluate a question's usefulness since its impact on downstream tasks is challenging to quantify.

In this work, we overcome this challenge by simulating a person's knowledge using language models (LMs) and introduce a direct measure of question \textit{utility}. 
Building on growing work that advocates LMs as world models~\cite{park2024generative, zhang2024simulating, he2024evaluating, jin2024agentreview, lu2024newsinterviewdatasetplaygroundevaluate, schmidgall2024agentclinic, zhaocompeteai}, we propose \ours (\textbf{Q}uestion \textbf{U}tility \textbf{E}stimation and \textbf{S}imulation \textbf{T}ests), a learning environment simulator that quantifies a question's utility---how much it contributes to improved performance on a relevant exam (as illustrated in \autoref{fig:motivation})---and fine-tunes question generators with high-utility questions.    

Specifically, \ours assigns an LM to roleplay as a novice learner that asks questions and receives answers while acquiring new information (\secref{ssec:quest-question-generation}). 
We estimate the \textit{utility} of each question by simulating the learner's performance on a related exam using different question subsets (\secref{ssec:quest-evaluation}).
Using these utility measurements, we fine-tune the question generator through a rejection sampling approach~\cite{bai2022constitutional}, retaining only questions that surpass a predefined utility threshold (\secref{ssec:quest-train}).

We evaluate \ours on \ourdata, a curated dataset where each sample consists of a chapter with multiple sections and corresponding exam questions (\secref{sec:textbook-exam}).
Using \ourdata, we experiment with various question generation strategies and quality metrics (\textit{i.e.,} \textit{utility}, \textit{saliency}, \textit{expected information gain}) (\secref{sec:experiment-setup}).

Our experimental results show that
(1) Question generators trained with \ours improve learners' understanding, leading to an average exam score increase of at least 20\% across simulations in five subjects (\secref{ssec:overall-performance});
(2) Indirect metrics (\textit{i.e.,} \textit{saliency}, \textit{expected information gain}) show weak correlations (<0.1) with utility, suggesting they do not accurately reflect a question’s impact on learning outcomes (\secref{ssec:evaluation-metrics});
(3) Optimizing for indirect metrics does not improve learners' understanding, whereas utility-driven training achieves the best overall performance, even in indirect metrics (\secref{ssec:evaluation-metrics});
(4) Utility has a weak positive correlation with lexical similarity (<0.04) and semantic similarity (<0.25) between generated and exam questions. This suggests that high-utility questions are not simple rephrasings but introduce new concepts that enhance learning (\secref{ssec:high-utility-question});
(5) There are no clear stylistic patterns among high-utility questions, indicating that their effectiveness is not driven by specific question formats or phrasing (\secref{ssec:high-utility-question}).

To summarize, we introduce \ours, a novel framework that quantifies question utility based on its direct impact on learning outcomes, rather than relying on indirect heuristics. 
Our approach leverages LLMs as simulators to evaluate a question’s contribution to learning, using performance improvements in downstream tasks as a reward signal.
We further propose a utility-driven training paradigm for question generation, fine-tuning models through rejection sampling by retaining only high-utility questions.
Our results demonstrate that optimizing for direct utility significantly enhances the simulated learner’s understanding, surpassing existing methods and offering new insights into what makes a question effective.













% \justin{include other key highlights. possible explanations for why our method is effective compared to others? robustness of results even when we change components with different models, relationship between utility and other metrics, }
% Our results suggest that LMs, even if they provide a basic approximation of a learner's knowledge, are useful simulators with a reasonable understanding of theory of mind~\cite{} such that they can adequately evaluate questions on their contributions for downstream tasks from the perspective of a learner. 






















% \dongho{Our experimental results show that (1);(2)}



