\section{Experimental Setup}
\label{sec:experiment-setup}

\subsection{Baselines}
To assess the effectiveness of \ours, we compare various question generators based on utility and examine the correlation between utility-based evaluation and existing metrics.

\paragraph{Question Generator.} 
We evaluate our approach against multiple baselines to assess the effectiveness of utility-based question generation (QG). 
These baselines include:
(1) \textbf{zero-shot}, which directly prompts\footnote{All the prompts used in this work can be found in Appendix \ref{appdx:prompt-details}.} the model to generate a question that helps the student better understand the section;
(2) \textbf{few-shot}, which extends zero-shot by providing five example exam questions from the training set, each paired with its relevant section, to guide the model in generating final exam-style questions~\cite{brown2020language};
(3) \textbf{chain-of-thought (CoT)}, which prompts the model to generate reasoning steps before formulating a question, improving logical coherence~\cite{wei2022chain, zhou2024self};
(4) \textbf{Bloom-based} prompting, which selects a level in the revised Bloom's Taxonomy~\cite{krathwohl2002revision_bloom} with weighted random sampling based on the distribution of the levels among the exam questions in the training data and generates a question that requires this cognitive level for answering (refer to \secref{appdx:bloom-based-prompting} for details).
(5) \textbf{supervised fine-tuning (SFT)} similar to few-shot but trains the model on exam questions from the training set instead of using in-context prompting, allowing it to learn question patterns over multiple optimization steps; and
(6) \textbf{QUEST} employs rejection sampling to generate questions and select those with the highest simulated utility, maximizing learning impact. 

Both SFT and QUEST are evaluated using two variations:
(1) \textbf{subject-specific}: Training and testing are performed separately for each subject, meaning the model is trained only on data from a specific subject and evaluated on that same subject; and
(2) \textbf{cross-subject}: Training is conducted across all subjects collectively by aggregating the entire training dataset, and the model is then tested on each subject individually.

\paragraph{Evaluation Metric.}
To evaluate whether our measure of utility is a good indicator of question quality, we compare with experiments using the following evaluation metrics:
(1) \textbf{Saliency} measures a question’s relevance and importance within the document \( D \)~\cite{wu2024questions}.
It is rated on a Likert scale from 1 to 5, where a score of 1 indicates that the question in section \( k \) is unrelated to \( S_{[1:k]} \) and contributes minimally to understanding, while a score of 5 indicates strong relevance to \( S_{[1:k]} \) and essential comprehension support for \( S_k \) by clarifying key concepts or introducing new information.
Saliency is evaluated using an LLM, with the full prompt provided in Appendix~\ref{appendix:saliency}.
(2) \textbf{Expected Information Gain (EIG)} quantifies the reduction in uncertainty about a student's knowledge state after answering a question~\cite{lindley1956measure, schaeffer2003science, rao-daume-iii-2018-learning, yu-etal-2020-interactive, white-etal-2021-open, keh-etal-2024-asking}.
We estimate EIG using an LLM by computing the entropy of the model’s token probability distribution before and after conditioning on the first token of the correct answer, capturing the immediate shift in the model’s belief distribution.
Details are provided in Appendix~\ref{appendix:eig};
(3) \textbf{Utility} measures the impact of a generated question on overall learning by evaluating simulated exam performance after exposure to the question.

\subsection{Experiments and Implementation Details}  
All LLMs used in data preprocessing (\secref{ssec:textbook-exam-pipeline}) and throughout the framework (\secref{sec:framework}), including the question generator \( M_q \), answer generator \( M_a \), learner \( M_l \), and evaluator \( M_e \) in the reader simulator, are based on \texttt{gpt-4o-mini}.
For SFT in the baseline and rejection sampling in \textsc{QUEST}, we use OpenAI fine-tuning API to further train \texttt{gpt-4o-mini}.






