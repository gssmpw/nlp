\section{Related Work}
Research on question generation and evaluation has primarily focused on generating questions to enhance understanding of a given document and developing metrics to evaluate their effectiveness.

\paragraph{Question Generation.}
Recent studies have explored generating \textit{information-seeking }questions to enhance comprehension by reflecting the inquisitive nature of human question-asking.
These include curiosity-driven inquisitive questions~\cite{ko-etal-2020-inquisitive, wu2024questions}, confusion-driven inquisitive questions~\cite{chang2024booookscore}, follow-up questions~\cite{meng-etal-2023-followupqg}, and clarifying questions~\cite{chen2018learningq, rao-daume-iii-2018-learning, rao-daume-iii-2019-answer, kumar-black-2020-clarq, majumder-etal-2021-ask}.
A widely adopted framework in this area is the concept of Questions Under Discussion (QUD), which views each sentence as the answer to an implicit or explicit question from prior context~\cite{van1995discourse, roberts2012information, onea2016potential, benz2017questions}.
Building on this framework, recent works have applied question generation to tasks such as decontextualization~\cite{newman-etal-2023-question}, which recovers missing context to make snippets standalone, and elaboration~\cite{wu-etal-2023-elaborative}, which generates additional details to enhance clarity. 
Other applications include discourse comprehension by linking sentences to their broader context~\cite{ko-etal-2022-discourse}, 
planning for summarization~\cite{narayan2023conditional}, 
and modeling information loss during text simplification~\cite{trienes-etal-2024-infolossqa}.

\paragraph{Question Evaluation.}
Existing works evaluate information-seeking questions based on expected information gain, measuring the additional information provided by the answers~\cite{lindley1956measure, schaeffer2003science, rao-daume-iii-2018-learning, yu-etal-2020-interactive, white-etal-2021-open, keh-etal-2024-asking}, or saliency, assessing a question's relevance and importance in introducing new concepts or clarifying key ideas~\cite{wu2024questions}.
Empirical findings show that QUDs—questions guaranteed to be addressed in the article—consistently receive high salience ratings~\cite{ko-etal-2022-discourse}, reflecting reader expectations and linking to the utility of the question~\cite{van2003questioning, wu2024questions}.
In contrast, our work directly evaluates the utility of questions using an LLM as a human simulator.

\paragraph{LLM as Human Simulation.}
LLMs have been used to simulate real-world social interactions, starting with everyday human interactions~\cite{park2024generative, maharana-etal-2024-evaluating, lee2025realtalk} and expanding to controlled environments such as market competition~\cite{zhaocompeteai}, hospital settings where medical staff and patients engage in treatment simulations~\cite{li2024agent, schmidgall2024agentclinic}, news interviews~\cite{lu2024newsinterviewdatasetplaygroundevaluate}, and academic peer review~\cite{jin2024agentreview}.
In education, LLMs simulate classroom interactions between teachers and students~\cite{zhang2024simulating} and predict learning outcomes to improve educational materials~\cite{he2024evaluating}.
Our work builds on \citet{he2024evaluating}, using simulated learning outcomes as an evaluation metric for question quality and as a reward signal to improve question generation.




