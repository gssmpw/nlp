\begin{table*}[!t]
    \centering
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{lccccc|c}
            \toprule
            & \textbf{Microbiology} & \textbf{Chemistry} & \textbf{Economics} & \textbf{Sociology} & \textbf{US History} & \textbf{Average} \\
            \midrule
            Base score & 0.46 & 0.09 & 0.00 & 0.61 & 0.03 & 0.24 \\
            \midrule
            Zero-shot & 0.62 (+0.16) & 0.40 (+0.31) & 0.40 (+0.40) & 0.61 (+0.00) & 0.19 (+0.16) & 0.44 (+0.20) \\
            Few-shot & 0.62 (+0.16) & \underline{0.45} (+0.36) & \underline{0.47} (+0.47) & 0.62 (+0.01) & 0.16 (+0.13) & 0.46 (+0.22) \\
            Chain-of-thought & 0.61 (+0.15) & \underline{0.45} (+0.36) & 0.46 (+0.46) & 0.61 (+0.00) & 0.19 (+0.16) & 0.46 (+0.22) \\
            Bloom-based & 0.57 (+0.11) & 0.37 (+0.28) & 0.29 (+0.29) & 0.62 (+0.01) & 0.22 (+0.19) & 0.41 (+0.17) \\
            \midrule
            SFT (Subject-Specific) & 0.65 (+0.19) & 0.24 (+0.15) & 0.46 (+0.46) & \underline{0.64} (+0.03) & 0.20 (+0.17) & 0.44 (+0.20) \\
            SFT (Cross-Subject) & 0.59 (+0.13) & 0.21 (+0.12) & \underline{0.47 (+0.47)} & 0.63 (+0.02) & \underline{0.26} (+0.23) & 0.43 (+0.19) \\
            \midrule
            \textsc{QUEST} (Subject-Specific) & \bf 0.76 (+0.30) & \bf 0.46 (+0.37) & \bf 0.58 (+0.58) & \bf 0.65 (+0.04) & \bf 0.31 (+0.28) & \bf 0.55 (+0.31) \\
            \textsc{QUEST} (Cross-Subject) & \underline{0.73} (+0.27) & \underline{0.41} (+0.32) & \underline{0.47} (+0.47) & \bf 0.65 (+0.04) & 0.25 (+0.22) & \underline{0.50} (+0.26) \\
            \bottomrule
        \end{tabular}
    }
    \caption{\textbf{End-of-chapter exam score} results of different question generation approaches across various subjects. 
    \ours produces models that generate questions that lead to the highest scores on all subjects.  
    Gain values (in parentheses) are calculated as the increase from the base score. The highest and second highest scores per column are \textbf{bolded} and \underline{underlined}, respectively.}
    \label{tab:question-gen-results}
\end{table*}


\section{Experimental Results}
In this section, we first compare the overall performance of all question generation baselines based on the learner's exam score (\secref{ssec:overall-performance}).  
Next, we analyze evaluation metrics by examining their correlations with utility and assessing the impact of optimizing models on high-scoring questions for each metric (\secref{ssec:evaluation-metrics}).
We then conduct a qualitative analysis of high-utility questions to understand their characteristics (\secref{ssec:high-utility-question}). 
Finally, we perform ablation studies on the framework by varying the criteria for selecting high-utility questions for training and replacing \texttt{gpt-4o-mini} to \texttt{gpt-4o} (\secref{ssec:rs-analysis}-\secref{ssec:model-variants}).



\subsection{Overall Performance}
\label{ssec:overall-performance}
Table~\ref{tab:question-gen-results} presents the learner's exam performance of different question generators, measured by exam scores using all generated question-answer pairs (\secref{ssec:quest-evaluation}).
Here are findings:
(1) \textbf{Prompting techniques (Few-shot, CoT, Bloom-based)} offer only marginal performance gains. While advanced prompting enhances reasoning and task accuracy~\cite{brown2020language, wei2022chain, zhou2024self}, it does not directly optimize utility, which reflects real-world impact—how well generated questions enhance learning. Without explicit selection or optimization, prompting cannot systematically improve this measure;
(2) \textbf{SFT} shows no performance gains, indicating that while it learns the style of exam questions, it fails to generate questions that enhance learner understanding.
This highlights the key distinction between producing syntactically valid questions and generating those that effectively promote learning;
(3) \textbf{QUEST} achieves the highest performance gain, improving by approximately 20\% on average.
Performance gap between subject-specific and cross-subject rejection sampling suggests that the definition of a ``high-utility'' question varies by domain.
The results indicate that outcome-based learning is most effective when applied within a specific domain.


\subsection{Evaluation Metrics Analysis}
\label{ssec:evaluation-metrics}
\paragraph{Correlation.}
\begin{table}[!t]
    \centering
    \resizebox{\columnwidth}{!}{%
        \begin{tabular}{ll
        cc}
            \toprule
            Metric 1 & Metric 2 & \textbf{Spearman correlation} & \textbf{p-value} \\
            \midrule
            \textbf{Utility} & Saliency & 0.097 & 0.003 \\
            \textbf{Utility} & EIG  & -0.022 & 0.512 \\
            Saliency & EIG  & 0.030 & 0.363 \\
            \bottomrule
        \end{tabular}
    }
    \caption{\textbf{Spearman correlation between metrics.} Utility shows a weak correlation with saliency and EIG, showing that it is independent of these indirect metrics.}
    \label{tab:correlation_results}
\end{table}
To analyze the relationship between \textit{utility} and existing metrics (\textit{saliency}, \textit{EIG}), we estimate all three metrics on generated questions from the training set.
Table~\ref{tab:correlation_results} shows that both saliency and EIG have weak correlations with utility.
While saliency has a weak but statistically significant correlation with utility, EIG shows no meaningful relationship.
This indicates that existing indirect metrics may not accurately reflect a question’s impact on learning outcomes.

\paragraph{Optimization on Indirect Metrics.}
\begin{table*}[!t]
    \centering
    \small
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{lccc ccc ccc ccc ccc}
            \toprule
            & \multicolumn{3}{c}{\textbf{Microbiology}} & \multicolumn{3}{c}{\textbf{Chemistry}} & \multicolumn{3}{c}{\textbf{Economics}} & \multicolumn{3}{c}{\textbf{Sociology}} & \multicolumn{3}{c}{\textbf{US History}} \\
            \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13} \cmidrule(lr){14-16}
            \textbf{Train Metric} & \textbf{Utility} & \textbf{Saliency} & \textbf{EIG} & \textbf{Utility} & \textbf{Saliency} & \textbf{EIG} & \textbf{Utility} & \textbf{Saliency} & \textbf{EIG} & \textbf{Utility} & \textbf{Saliency} & \textbf{EIG} & \textbf{Utility} & \textbf{Saliency} & \textbf{EIG} \\
            \midrule
            $utility > 0.1$ & \textbf{0.76} & 4.27 & -0.18 & \textbf{0.46} & 4.65 & -0.20 & \textbf{0.58} & \textbf{4.70} & -0.04 & \textbf{0.65} & \textbf{4.49} & -0.02 & \textbf{0.31} & 4.65 & \bf -0.01 \\
            $saliency = 5$  & 0.73 & \textbf{4.42} & -0.24 & 0.39 & \textbf{4.46} & -0.22 & 0.46 & 4.66 & -0.08 & 0.64 & \textbf{4.49} & -0.03 & 0.23 & \textbf{4.68} & -0.02 \\
            $EIG > 0$      & 0.61 & 4.21 & \textbf{-0.17} & 0.32 & 4.40 & \textbf{-0.09} & 0.47 & 4.65 & \textbf{0.01} & 0.62 & 4.46 & \textbf{0.01} & 0.21 & 4.65 & \bf -0.01 \\
            \bottomrule
        \end{tabular}
    }
    \caption{\textbf{End-of-chapter exam scores (utility), average saliency, and average EIG of generated questions} for different \textsc{QUEST}-optimized models trained on datasets filtered by different selection criteria.}
    \label{tab:quest-indirect}
\end{table*}
To further investigate the impact of indirect metrics on question generation performance, we compare the results of \textsc{QUEST} when trained using different selection criteria: (1) only questions with a \textit{utility} score greater than 0.1 (Ours), (2) only questions with a \textit{saliency} score of 5, and (3) only questions with an \textit{expected information gain (EIG)} greater than 0.
We evaluate the generated questions based on their overall utility (\textit{i.e.,} end-of-chapter exam scores), as well as their average saliency and EIG, to assess the question generator’s performance across different quality metrics.
Table~\ref{tab:quest-indirect} shows that while saliency- and EIG-based training improves their respective scores, it does not enhance utility. 
In contrast, the utility-trained model consistently achieves the highest utility across all subjects and even improves some indirect metrics (\textit{e.g.,} it matches or outperforms saliency-based training in Economics and Sociology).
Furthermore, utility-based training outperforms EIG-based training on saliency and saliency-based training on EIG, demonstrating its broader effectiveness. 
These results emphasize that optimizing for indirect metrics does not improve real-world learning, whereas utility-driven training yields the best overall performance.




\subsection{High Utility Questions Analysis}
\label{ssec:high-utility-question}
\paragraph{Overlap with exam questions.}
To evaluate the relationship between generated high-utility questions and exam questions, we measured their semantic and lexical similarity.
For each generated question, we computed embedding similarity using \texttt{text-3-embedding-small}\footnote{\href{https://platform.openai.com/docs/guides/embeddings/}{https://platform.openai.com/docs/guides/embeddings/}} for semantic overlap and the ROUGE score for lexical overlap with all exam questions in the same chapter.
We then assess the correlation between utility and the most similar exam question based on these measures.
The correlation between utility and semantic similarity is 0.25 (p < 0.001), indicating a weak positive relationship, while the correlation with ROUGE is nearly zero at 0.04 (p < 0.01).
These findings suggest that high-utility questions are not simple rephrasings of exam questions but introduce novel concepts that enhance learning beyond surface-level similarity.

\paragraph{Qualitative analysis.}
Qualitative question examples do not exhibit clear patterns in question style (see Appendix~\ref{appendix:qualitative-examples}).
An interesting observation is that Bloom's taxonomy, which categorizes cognitive depth based on question type—where "what" questions typically involve simple recall, while "why" and "how" questions require deeper processing—does not strongly correlate with utility.
Using Bloom's taxonomy as a cognitive depth scale (Likert 1-6), the correlation between utility and cognitive depth is 0.12 (p < 0.001), indicating a weak positive relationship.


\subsection{Rejection Sampling Analysis}
\label{ssec:rs-analysis}
Filtering for high-utility questions through rejection sampling is crucial for improving question generation. 
As shown in Figure~\ref{fig:rs_analysis}, increasing the utility threshold enhances question quality, leading to higher exam scores.
However, stricter filtering reduces the available training data, posing challenges for model training. 
These results suggest that increasing the dataset size while applying a higher threshold could further boost performance.
\begin{figure}[!t]
    \centering
    \begin{minipage}{\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{figures/rs_analysis.pdf}
    \end{minipage}
    \caption{\textbf{Impact of threshold} in \ours on end-of-chapter exam scores for Chemistry.}
    \label{fig:rs_analysis}
    \vspace{-0.5cm}
\end{figure}

\subsection{Model Variants Analysis}
\label{ssec:model-variants}
\begin{table*}[!t]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{c|ccc|ccccc}
        \toprule
        & \textbf{QG ($M_q$)} & \textbf{AG ($M_a$)} & \textbf{RS ($M_l$)} & \textbf{Microbiology} & \textbf{Chemistry} & \textbf{Economics} & \textbf{Sociology} & \textbf{US History} \\
        \midrule
        \multicolumn{1}{c|}{\multirow{4}{*}{Zero-Shot}} 
        & \texttt{gpt-4o-mini} & \texttt{gpt-4o-mini} & \texttt{gpt-4o-mini} & 0.620 & 0.414 & 0.398 & 0.609 & 0.233 \\
        & \texttt{gpt-4o} & \texttt{gpt-4o-mini} & \texttt{gpt-4o-mini} & 0.681 & 0.457 & 0.466 & 0.634 & 0.180 \\
        & \texttt{gpt-4o-mini} & \texttt{gpt-4o} & \texttt{gpt-4o-mini} & 0.682 & 0.422 & 0.480 & 0.634 & 0.232 \\
        & \texttt{gpt-4o-mini} & \texttt{gpt-4o-mini} & \texttt{gpt-4o} & 0.710 & 0.173 & 0.476 & 0.564 & 0.263 \\
        \midrule
        \textsc{QUEST} & \texttt{gpt-4o-mini} & \texttt{gpt-4o-mini} & \texttt{gpt-4o-mini} & \bf 0.756 &	\bf 0.457 &	\bf 0.582 &	\bf 0.649 &	\bf 0.311 \\
        \bottomrule
    \end{tabular}
    }
    \vspace{-0.2cm}
    \caption{\textbf{End-of-chapter exam scores} for different model sizes across various subjects and modules.}
        \vspace{-0.2cm}

    \label{tab:model-variants}
\end{table*}

To evaluate the robustness of our framework and the impact of model size on different components, we conduct experiments to analyze how a larger model affects each module.
Table~\ref{tab:model-variants} presents results for different configurations of the question generator ($M_q$), answer generator ($M_a$), and reader simulator (\textit{i.e.,} learner $M_l$). 
The baseline corresponds to the \texttt{zero-shot} setting in Table~\ref{tab:question-gen-results}. 
In the $M_a$ experiment, we use the same questions from the \texttt{zero-shot} setting but generate new answers.
For the reader simulator experiment, we keep the same questions and answers from \texttt{zero-shot} and re-run the simulation only.

Our main findings are the following:
(1) \textbf{Question Generator}: A larger model (\texttt{gpt-4o}) improves the utility score by 5.7\%. However, it still underperforms compared to the smaller, utility-optimized model (\texttt{gpt-4o-mini}) by 12.3\%;
(2) \textbf{Answer Generator}: A larger model improves performance by 7.1\%, suggesting that higher answer quality provides additional information to the QA pair. 
However, it remains 11\% behind the optimized \texttt{gpt-4o-mini} in utility.
(3) \textbf{Reader Simulator}: Using a larger model (\texttt{gpt-4o}) as the reader simulator leads to mixed results, with performance gains in some subjects but a sharp decline in Chemistry. 
This suggests that larger models may introduce different reasoning strategies or evaluation biases, leading to inconsistencies in scoring. 
Additionally, since our framework is optimized for \texttt{gpt-4o-mini}, the larger model may not align well with the training dynamics. 
These results highlight the importance of consistency in simulation for reliable utility estimation.

% equipped with a larger model (\texttt{gpt-4o}) is expected to make more accurate simulation of how much generated questions really impact to the student understanding on exams.
% There is consistent increasing gap on microbiology, economics, sociology, us history while there is a huge gap in decreasing gap on chemistry, compared to smaller model (\texttt{gpt-4o-mini})



% \begin{itemize}
%     \item \textbf{Stronger question generators improve performance in some subjects but not all.} Upgrading $M_q$ from GPT-4o-mini to GPT-4o enhances performance in Microbiology and Economics but degrades US History, suggesting that question quality improvement is domain-dependent.
%     \item \textbf{Answer generator quality has a moderate effect on utility.} Stronger answer generators help in certain subjects (e.g., Economics) but have minimal impact in others (e.g., Chemistry).
%     \item \textbf{Evaluator model strength influences overall results but can introduce variability.} Using a stronger evaluator (GPT-4o) improves utility estimates in Microbiology and Economics but drastically reduces performance in Chemistry. This suggests that evaluators need to be carefully tuned to ensure robust and reliable assessments across different domains.
% \end{itemize}


