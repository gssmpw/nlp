\section{Conclusion}

We present \ours, a LLM-based simulation of a learning environment that quantifies question utility, a measure of how much it contributes to a downstream task. 
With \ourdata, a dataset with paired chapter sections and their relevant exam questions, we use \ours to find high-utility questions and train question generators through rejection sampling. 
The models trained with \ours generate questions that lead to largest improvements in learning outcomes across five subjects among a comprehensive set of baselines. 
We find that our definition of utility is only weakly correlated to other metrics such as saliency and expected information gain, indicating that utility provides unique learning signals for directly benefiting downstream tasks.  