\begin{abstract}
Estimation and inference for the Average Treatment Effect (ATE) is a cornerstone of causal inference and often serves as the foundation for developing procedures for more complicated settings. 
Although traditionally analyzed in a batch setting, recent advances in martingale theory have paved the way for adaptive methods that can enhance the power of downstream inference. 
Despite these advances, progress in understanding and developing adaptive algorithms remains in its early stages. 
Existing work either focus on asymptotic analyses that overlook exploration-exploitation trade-offs relevant in finite-sample regimes or rely on simpler but suboptimal estimators.
In this work, we address these limitations by studying adaptive sampling procedures that take advantage of the asymptotically optimal Augmented Inverse Probability Weighting (AIPW) estimator. 
Our analysis uncovers challenges obscured by asymptotic approaches and introduces a novel algorithmic design principle reminiscent of optimism in multi-armed bandits. 
This principled approach enables our algorithm to achieve significant theoretical and empirical gains compared to prior methods. 
Our findings mark a step forward in advancing adaptive causal inference methods in theory and practice.
\end{abstract}

%%%

\section{Introduction}\label{sec:intro}

The problem of estimating the average treatment effect (ATE) is central to causal inference and has been extensively studied. 
We have a precise understanding of the difficulty of this problem in both asymptotic and nonasymptotic regimes. However, our understanding of the challenges associated with \emph{adaptive} ATE estimation remains limited.

Classically, adaptive ATE estimation has been analyzed in an asymptotic setting, where past work has focused on designing adaptive sampling procedures that ensure that the resulting ATE estimate achieves the smallest possible asymptotic variance, that is, the semiparametric efficiency bound. More recently, there has been growing interest in developing algorithms that provide nonasymptotic performance guarantees. However, these works suffer from certain drawbacks that lead to poor finite sample performance, an issue we discuss in detail in Sections~\ref{sec:related-works} and~\ref{subsec:past-alg-issues}.

In this work, we take a nonasymptotic perspective on adaptive ATE estimation, focusing on the Augmented Inverse Propensity Weighting (AIPW) estimator. Our finite-sample analysis reveals key aspects of algorithmic design that prior works have overlooked. This enables us to propose a new algorithm with substantially improved theoretical and empirical performance while also simplifying the analysis.

At the heart of our approach is the insight that initially over-sampling arms that should eventually be under-sampled according to the (unknown) optimal allocation can lead to better estimates of the ATE. Interestingly, this idea can be interpreted as an instance of the principle of \emph{optimism}, a well-established algorithmic design paradigm in the literature on regret minimization in multi-armed bandits (MAB) and reinforcement learning. We discuss this connection in more detail in Section~\ref{sec:algorithm}.

\paragraph{Contributions.}
Our main contributions are as follows:
\begin{enumerate}
    \item We develop and analyze a new algorithm, \mainalgname (\mainalgnameshort), for adaptive estimation of ATE that enjoys significant theoretical improvements over previous approaches along with a significantly simplified analysis.
    \item We perform simulations that demonstrate that our theoretical improvements translate into empirical improvements, especially in the small sample regime, which is critical for applications like randomized clinical trials.
\end{enumerate}

\paragraph{Organization.}
The rest of our paper is organized as follows. 
The remainder of this paper is structured as follows.
In Section~\ref{sec:related-works}, we review prior and related work.
Section~\ref{sec:problem-setup} introduces our problem setup, establishing the necessary framework for our contributions.
In Section~\ref{sec:algorithm}, we identify key limitations of existing approaches and present our proposed \mainalgnameshort algorithm.
Section~\ref{sec:results} contains our main theoretical results, providing a rigorous performance characterization of \mainalgnameshort.
Finally, in Section~\ref{sec:experiments}, we empirically validate our method, demonstrating its superior performance compared to existing approaches and its competitiveness with—often surpassing—even an infeasible oracle baseline.


\section{Prior and Related Works}\label{sec:related-works}

Adaptive experimental design has a long and distinguished history, tracing back to the seminal work of \citet{Neyman1934OnTT} on optimal allocation in experimental studies.
 \citet{Thompson1933ONTL} introduced a Bayesian adaptive design, thereby laying the foundation for the MAB problem. Thompson's approach of sequential updating beliefs about treatments (or arms) based on observed outcomes is now central in MAB research \citep{Lattimore2020BanditA}.
However, many problem formulations focus on maximizing cumulative rewards over repeated rounds of exploration-exploitation. In contrast, our objective of ATE estimation differs from the typical MAB focus and raises different forms of exploration trade-offs.

\subsection{Prior Work}
Our work builds on a recent line of work investigating adaptive algorithms aimed at efficiently estimating ATE.
\citet{Hahn2009AdaptiveED} sparked this research direction by proposing a two-stage design, conceptually similar to the Explore-then-Commit algorithms in MAB \citep{Garivier2016OnES} and showing that it asymptotically attains the minimum-variance semiparametric efficiency bound.
Subsequently, \citet{Kato2020EfficientAE} introduced a fully adaptive procedure using the \emph{adaptive} AIPW estimator (\AAIPW), and showed that it is asymptotically optimal (in the above sense) while also providing improved empirical performance compared to the less adaptive two-stage design.
Later, \citet{cook24semi} proposed an alternative method called Clipped Standard-Deviation Tracking (\clipSDT), which inherits the same asymptotic optimality under milder assumptions, admits modern uncertainty quantification tools \citep{WaudbySmith2022AnytimevalidOI}, and outperforms the earlier approach empirically.
In parallel work, \citet{Li2024OptimalTA} significantly generalized the two-stage design in \citet{Hahn2009AdaptiveED}, extending its applicability to a broad spectrum of problems, including Markovian and non-Markovian decision processes.

Despite these advances, all of the above approaches focus on characterizing the asymptotic behavior of their approaches, leaving open questions about finite-sample performance of their work.
In order to address these questions, \citet{dai2023clip} takes an initial step toward understanding the nonasymptotic difficulty by introducing the \clipOGD algorithm for the fixed-design setting.
They introduce and analyze the Neyman regret (in the design-based setting), which is a normalized proxy to the variance of the resulting ATE estimate.
Even more recently, \citet{neopane2024logarithmic} propose and analyze the \clipSMT algorithm for the superpopulation setting and show that it enjoys an improved $\log \numRounds$ bound on the Neyman regret.

While these two works take important first steps toward understanding the nonasymptotic difficulty of adaptive ATE estimation, they algorithms rely on the \IPW estimator which is known to be suboptimal.
Indeed, these works define the Neyman regret with respect to the minimum variance \IPW estimator, where the minimization is performed over all possible allocations.
In contrast, our definition of the Neyman regret is much stronger as the baseline we compete against is defined as the minimum attainable variance over \emph{all} pairs of estimators and allocations.
Notably, using this stronger definition of regret, the aforementioned approaches obtain linear Neyman regret, where as we are able to design an algorithm which obtains logarithmic Neyman regret.

\subsection{Related Works}
The problem of off-policy evaluation, which generalizes ATE estimation, has been extensively studied in the literature on reinforcement learning \citep{Dudik2011Doubly, Li2011Unbiased, Jiang2016DoublyRO}. 
Most of the research in this area has focused on offline estimation, leading to precise characterizations of minimax lower bounds along with matching upper bounds \citep{Li2015Toward, Wang2017OptimalAA, Duan2020Minimax, Ma2021MinimaxOE}. Beyond policy evaluation, these methods have been extended to estimate other quantities, such as the cumulative distribution function of rewards \citep{Huang2021OffCB, Huang2022OffRL}. 
However, there has been limited exploration of adaptive versions of these methods. 
Some existing work includes \citet{Hanna2017DataEfficientPE}, which focuses on off-policy learning, and \citet{Konyushova2021Active}, which integrates offline off-policy evaluation techniques with online data acquisition to enhance sample efficiency in policy selection. 
However, these works are primarily empirical.

A related area of research concerns inference procedures for adaptively collected data. This  can be categorized into asymptotic and non-asymptotic approaches. 
On the asymptotic side, one direction has focused on re-weighting estimators and establishing their asymptotic normality \citep{Hadad2021Confidence, Zhang2020Inference, Zhang2021Statistical}. 
Another direction avoids asymptotics, instead leveraging modern advances in martingale theory to derive nonasymptotic confidence intervals and sequences for adaptively collected data, including estimates of the ATE \citep{Howard2018TimeUniform, Waudby2023Estimating, WaudbySmith2022AnytimevalidOI}.

\section{Background}\label{sec:problem-setup}


\paragraph{Problem Setup}
We are interested in adaptive estimation of the average treatment effect.
During each round, $\round$, $\alg$ uses the history of past observations $\history[\round - 1] = \cbrk{\paren{\policy_\timeIdx, \action_\timeIdx , \reward_\timeIdx}}_{\timeIdx = 1}^{\round - 1}$ to select the probability of treatment allocation $\policy[\round]$.
Then, $\policy[\round]$ is used to assign the next experimental unit to either the control $(\action[\round] = \cidx)$ or the treatment $(\action[\round] = \tidx)$ by sampling $\action[\round] \sim \bernoulli{\policy[\round]}$.
Finally, after assigning the experimental unit, we observe the outcome $\reward[\round]$ which marks the end of round $\round$. 

We formalize the above interaction protocol as follows.
Let $\filtration[\round] = \sigma\paren{\history[\round]}$ denote the filtration generated by the past observations.
An algorithm $\alg = \cbrk{\paren{\policy[\round], \estimate[\round]}}_{\round = 1}^{\numRounds}$ is defined as a sequence of $\filtration[\round - 1]$ measurable random elements where $\policy_\round \in [0, 1]$ is the treatment allocation probability and $\estimate[\round]: \paren{\policy[\round], \action[\round], \reward[\round]} \mapsto \bbR_{\geq 0}$ which can be thought of as the ATE estimate produced by $\alg$ on round $\round$. 

We assume that the rewards are generated as $\reward[\round] = \I{\action[\round] = \tidx} \reward[\round][\tidx] + \I{\action[\round] = \cidx} \reward[\round][\cidx]$, where $\reward[\round][\actionIdx]$ are called the potential outcomes. 
We assume that the sequence of potential outcomes are jointly distributed according to some probability measure $\nu$ (the ``environment'') that satisfies the following assumptions.
% generated as $(\reward[\round][\tidx], \reward[\round][\cidx]) \sim \bandit[\round]$ and require the following assumptions.
The first assumption is that the rewards are unconfounded, which means that, given $\filtration[\round - 1]$, the potential outcomes $\reward[\round][\tidx], \reward[\round][\cidx]$ are conditionally independent of the treatment assignment $\action[\round]$, i.e $\reward[\round][\tidx], \reward[\round][\cidx] \perp \action[\round] \mid \filtration[\round - 1]$.
The second assumption is that the reward means and variances are conditionally fixed so that for all \round, we have $\E[\bandit]{\reward[\round][\actionIdx]}[\filtration[\round - 1]] = \trueReward[\actionIdx]$ and $\V[\bandit]{\reward[\round][\actionIdx]}[\filtration[\round - 1]] = \var[\actionIdx]$.


Our objective within this framework is to estimate the ATE $\ATE$, which is defined as 
\[
\ATE = \E[\bandit]{\reward(\tidx) - \reward(\cidx)}.
\]

\NewDocumentCommand{\estRewardFunction}{o}{
    \widehat{\rewardFunction}
    \IfValueT{#1}{\paren{#1}}
}
\NewDocumentCommand{\linearFunctional}{o}{
    g\IfValueT{#1}{\paren{#1}}
}


\paragraph{The \AAIPW Estimator.}
An algorithm for adaptive ATE estimation thus requires us to specify a method to compute the treatment allocation probability $\policy[\round]$ as well as the estimate $\estimate[\round]$.
A natural choice for $\estimate[\round]$ is the AIPW estimator, which given some reward estimate $\estRewardFunction$, is defined as
\begin{equation}
    \estimate[\round] = \frac{\linearFunctional[\action_\round]}{\Prob[\alg, \nu]{\action_\round}}\paren{\reward[\round] - \predReward[][\action_\round]} + \estATE[][\estRewardFunction],
\end{equation}
where $\linearFunctional[\action_\round] = \I{\action_\round = \tidx} - \I{\action_\round = \cidx}$ and $\estATE[][\estRewardFunction] = \predReward[][\tidx] - \predReward[][\cidx]$.
However, this estimator isn't well suited to sequential estimation, % 
% $\Prob{\action_\round}$ is not known since it requires marginalizing over the interaction between $\alg$ and $\bandit$ and the second is that such an approach does not enable us to sequentially update $\estRewardFunction$ since the \AIPW estimator requires independence between $\estRewardFunction$ and $(\action[\round], \reward[\round])$.
motivating \citet{Kato2020EfficientAE} to propose the Adaptive AIPW (\AAIPW) estimator.
Specifically, letting $\predReward[\round]$ denote any $\filtration[\round - 1]$ measurable function (i.e.\ a \emph{predictable} reward estimate), they defined
\begin{equation}\label{eq:aaipw-estimate}
    \aipwEstimate[\round] = \frac{\I{\action[\round] = \tidx} - \I{\action[\round] = \cidx}}{\policy[\round][\action[\round]]}\paren{\reward[\round] - \predReward[\round][\action[\round]]} + 
    \estATE[][\predReward[\round]].
    % \predATE[\round].
\end{equation}
% where $\predATE[\round] = \predReward[\round][\tidx] - \predReward[\round][\cidx]$.

We also choose to use the \AAIPW estimator for a few reasons.
The first reason is that this estimator is known to be asymptotically optimal -- this is crucial for obtaining sublinear Neyman regret (which we define below).
Furthermore, recent advances in sequential analysis have developed tight confidence sequences for the \AAIPW, making it a natural choice due to its compatibility with the downstream goals of sequential testing and uncertainty quantification.

\paragraph{Neyman Allocation and Regret}
We use the mean squared error (MSE) to measure the quality of the estimates produced by our algorithm.
However, by itself, the MSE is difficult to interpret because it does not consider the inherent difficulty of the problem.
Therefore, we would like to normalize this error with respect to some problem dependent baseline which we now define and motivate.
\citet{Hahn2009AdaptiveED} show that for any fixed allocation, $\policy$, the minimum attainable MSE of any estimator is 
\begin{equation}\label{eq:fixed-policy-variance}
    \frac{\var[\tidx]}{\policy} + \frac{\var[\cidx]}{1 - \policy}.
\end{equation}
The Neyman allocation $\neymanPolicy$ is defined as the allocation which minimizes the above variance and a simple calculation shows that 
\begin{equation}
    \neymanPolicy = \frac{\stdev[\tidx]}{\stdev[\cidx] + \stdev[\tidx]}.
\end{equation}

Ideally, we would like to design an algorithm whose variance is close to this baseline and in order to understand the rate at which this occurs, we consider the Neyman regret which is defined as
\begin{equation}
    \neymanRegret_\numRounds = T \cdot \paren{\estATE[\numRounds] - \ATE}^{2} - \paren{\frac{\var[\tidx]}{\neymanPolicy} + \frac{\var[\cidx]}{1 - \neymanPolicy}}.
\end{equation}
The Neyman regret is simply the difference in the normalized MSE between the optimal variance and the MSE of the estimate produced by \alg.
This normalization guarantees that the the MSE converges to a constant (rather than 0) so that if \alg has sublinear regret, then we are guaranteed that its MSE converges to the optimal MSE.

Using the fact that the \AAIPW is unbiased, along with the fact that $\policy[\round]$ and $\predReward[\round]$ are predictable, we can rewrite the Neyman regret for the \AAIPW estimator as
\newcommand{\neymanLoss}{\ell}
\begin{equation}
    \neymanRegret_\numRounds = \sum_{\round = 1}^{\numRounds}  \E[\alg, \bandit]{\neymanLoss(\policy[\round], \predReward[\round])} - \paren{\frac{\var[\tidx]}{\neymanPolicy} + \frac{\var[\cidx]}{1 - \neymanPolicy}},
\end{equation}
where
\NewDocumentCommand{\predRewardError}{o o o}{
\ensuremath{
\varepsilon
    \IfValueT{#3}{^{#3}}%
    \IfValueT{#1}{_{#1}}%
    \IfValueT{#2}{\paren{#2}}%
}
}
\begin{equation}
    \neymanLoss(\policy, \rewardFunction) = \sum_{\aidx \in \cbrk{0, 1}} \frac{\var[\aidx]}{\policy[][\aidx]} + \frac{1 - \policy[][\aidx]}{\policy[][\aidx]}\predRewardError[\round][\aidx][2]
\end{equation}
is the Neyman loss and $\predRewardError[\round][\aidx] = \trueReward[\aidx] - \predReward[\round][\aidx]$ is the reward estimation error.

\newcommand{\policyGap}[1]{\Delta\paren{#1}}

\paragraph{Notation.} In what follows, we will let $$\acount{\round}{\aidx} = \sum_{\timeidx = 1}^{\round} \I{\action_\timeidx = \aidx}$$ denote the number of times the action $\aidx$ is selected at the end of round $\round$, 
$$\empmean[\round][\aidx] = \frac{1}{\acount{\round}{\aidx}} \sum_{\timeidx = 1}^{\round} \reward[\timeIdx] \I{ \action[\timeidx] = \aidx}$$ denote the empirical mean after $\round$ rounds, 
and $$\empvar[\round][\aidx] = \frac{1}{\acount{\round}{\aidx}} \sum_{\timeidx = 1}^{\round} \paren{\reward[\timeIdx] \I{ \action[\timeidx] = \aidx} - \empmean[\round][\action]}^2$$ denote the emprical variance.
We use \(\bigTildeO[\cdot]\) to denote asymptotic equivalence up to doubly logarithmic factors.

\section{The Optimistic Policy Tracking Algorithm}\label{sec:algorithm}

In this section, we introduce our Optimistic Policy Tracking (OPT) algorithm.
We begin with a discussion of the difficulties of adaptive ATE estimation and the suboptimality of existing approaches.
Next, we introduce our algorithm and provide insight into why it resolves the issues of existing approaches.
Finally, we conclude with a brief discussion of the algorithmic design principles underlying our algorithm and their relation to ideas in the literature.

\subsection{Preliminaries}\label{subsec:past-alg-issues}

\paragraph{The difficulties of adaptive ATE estimation.}
The primary difficulty of adaptive ATE estimation is in balancing the exploration-exploitation trade-off that arises from adaptive allocation.
If we condition on $\filtration[\round - 1]$ some algebra shows (see Lemma~\ref{lem:aaipw-variance}) that  the variance of the \AAIPW estimator is
\begin{equation}
    \sum_{\aidx} \frac{\var[\aidx]}{\policy[\round][\aidx]} + \frac{1 - \policy[\round][\aidx]}{\policy[\round][\aidx]} \paren{\trueReward[\aidx] - \predReward[\round][\aidx]}^2,
\end{equation}
which is minimized by setting $(\policy, \rewardFunction) = (\neymanPolicy, \trueReward)$ where $\neymanPolicy$ is the Neyman allocation introduced in Section~\ref{sec:problem-setup}.
Since $\neymanPolicy$ and $\trueReward$ are not known a priori, we need to design an algorithm to adaptively estimate them.
However, this is challenging because optimizing the exploration allocation separately for estimating \(\neymanPolicy\) and \(\trueReward\) (each requiring a different allocation) results in a procedure with high Neyman regret.
As such, designing an algorithm to adaptively balance the exploration of $\neymanPolicy$ and $\trueReward$ while simultaneously minimizing the Neyman regret becomes a very delicate task.

\paragraph{Insights into improvements.}
In order to better understand the improvements that can be made, we investigate previous approaches for balancing this trade-off.
To simplify the exposition, in this section we assume that $\neymanPolicy \leq \frac{1}{2}$.
The primary approach that past works (both asymptotic and nonasymptotic) have utilized is clipping the allocation.
In fact, the algorithms proposed by \citet{cook24semi}, \citet{dai2023clip}, \citet{neopane2024logarithmic} all utilize a clipping approach which computes the empirical allocation
\[\widehat \policy_\round = \frac{\empstdev[\round][\tidx]}{\empstdev[\round][\cidx] + \empstdev[\round][\tidx]},\]
and plays a clipped version of this estimate 
\[\policy[\round] = \min\cbrk{1 - \clippingSequence_\round, \max\cbrk{\clippingSequence_\round, \widehat \policy_\round}},\] 
for some carefully chosen clipping sequence $\clippingSequence_\round$ satisfying $\clippingSequence_\round \rightarrow 0$.
However, these clipping approaches have some important limitations.

The first limitation is that a clipping approach cannot be fully adaptive to the underlying problem instance because the clipping sequence must be chosen a priori.
As such, past works choose $\clippingSequence_\round$ in order to optimize the performance of their algorithm in a worst-case sense, leading to suboptimal Neyman regret for easy problem instances.
As an example, \citet{neopane2024logarithmic} show that setting $\clippingSequence_\round = \round^{-\frac{1}{3}}$ is optimal when we are not willing to bound $\neymanPolicy$ away from $0$ and $1$.
However, many practical problems are typically much easier than the worst case, and so we would like a procedure which is able to adapt to the underlying problem instance more appropriately.

The second, more pressing issue, is that clipping approaches lead to algorithms which under-exploit which is caused by the asymmetry of the Neyman loss.
To demonstrate this issue, in Figure~\ref{fig:neyman-loss}, we plot the Neyman loss $\neymanLoss(\policy, \trueReward)$ for a problem with $\neymanPolicy < \frac{1}{2}$.
In this figure we consider the Neyman loss at the points $\policy[+] = \neymanPolicy + \epsilon   $ and $\policy[-] = \neymanPolicy - \epsilon$.
It is easy to see that $\neymanLoss(\policy[+], \trueReward) < \neymanLoss(\policy[-], \trueReward)$, and in-fact this issue only worsens as $\neymanPolicy \rightarrow \cbrk{0, 1}$.
Practically, the implication is that an algorithms which under-sampled the arm with a smaller probability according to the Neyman allocation must necessarily pay a higher price than the same algorithm which over-sampled the same arm by the same amount.
This is not merely a theoretical issue --- we see in our experiments that while clipping-based approaches produce allocations which are closer to the Neyman allocation, they still have significantly worse empirical performance.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/neyman_loss.png}
    \caption{A plot of $\neymanLoss(\policy, \trueReward)$ where $\neymanPolicy < \frac{1}{2}$. Note how the Neyman loss is smaller for $\neymanPolicy + \epsilon$. This is due to the fact that $\neymanPolicy + \epsilon$ is closer to $\frac{1}{2}$, highlighting how less explorative allocations incur larger Neyman regret.}
    \label{fig:neyman-loss}
\end{figure}





\subsection{Optimistic Policy Tracking}
\paragraph{Main Algorithm.}
Our proposed algorithm, OPT, is designed in order to address these aforementioned issues.
Indeed, as we will see, not only does OPT better adapt to the underlying problem instances, it also better handles the exploration-exploitation trade-off when compared to prior works.
The algorithm itself if simple and plays the allocation 
\begin{equation}
    \policy[\round] = \argmin_{\policy \in \CS[\round][\neymanPolicy]} \abs{\frac{1}{2} - \policy},
\end{equation}
where $\CS[\round][\neymanPolicy]$ is a confidence sequence for the Neyman allocation.
For reward estimation, we simply use the sample mean $\predReward[\round][\aidx] = \frac{1}{\acount{\round - 1}{\aidx}} \sum_{\timeidx = 1}^{\round - 1} \reward[\timeidx] \cdot \I{\action_\round = \aidx}$.

The main difficulty now is in constructing the confidence sequence $\CS[\round][\neymanPolicy]$.
In order to do so, we first construct confidence sequences for the standard deviations of each arm.
This is accomplished in Lemma~\ref{lem:stdev-concentration}, which constructs a confidence sequence $\CS[\round][\stdev[\aidx]] = \sbrk{\LCS[\round][\stdev[\aidx]], \UCS[\round][\stdev[\aidx]]}$ whose with scales like $\bigO[\sqrt{\frac{\log \log \round + \log \frac{1}{\errorProb}}{\round}}]$.
Using these confidence sequences on $\stdev[\aidx]$, we can construct a confidence sequence for the Neyman allocation as follows 
\begin{equation} \label{eq:neyman-allocation-cs}
        \begin{aligned}
            \CS[\round][\neymanPolicy] = 
            & \left[ \frac{\LCS[\round][\stdev[\tidx]]}{\UCS[\round][\stdev[\cidx]] + \LCS[\round][\stdev[\tidx]]}, \right. \\
            & \left. \frac{\UCS[\round][\stdev[\tidx]]}{\LCS[\round][\stdev[\cidx]] + \UCS[\round][\stdev[\tidx]]}\right].    
        \end{aligned}
\end{equation}
The full algorithm is provided in Algorithm~\ref{alg:opt}.

\begin{algorithm}
\caption{\mainalgname (\mainalgnameshort)}\label{alg:opt}
\begin{algorithmic}[1]
\FOR{$\round = 1, 2, \ldots$}
    \STATE Compute $\CS[\round][\neymanPolicy]$ according to equation~\eqref{eq:neyman-allocation-cs}
    \STATE Set $\policy_\round = \argmin_{\policy \in \CS[\round][\neymanPolicy]} \abs{\frac{1}{2} - \policy}$
    \STATE Sample $\action_\round \sim \text{Bernoulli}(\policy_\round)$
    \STATE Observe $\reward_\round \sim \nu(\action_\round)$
    \STATE Compute $\aipwEstimate[\round]$ according to equation~\eqref{eq:aaipw-estimate}
\ENDFOR
\end{algorithmic}
\end{algorithm}

\paragraph{Interpretation as Optimism.}
We can interpret our algorithm as implementing the celebrated principle of \emph{optimism in the face of uncertainty}.
Optimism is an algorithmic design principle which is the basis of many well-known MAB and reinforcement learning algorithms (such as the ``upper confidence bound'').
Roughly speaking, the principle states that we should act as if the underlying problem instance is the easiest instance, which is feasible according to our past observations.
In the regret minimization framework, this means playing the arm which has the largest upper confidence bound.
For adaptive ATE estimation, this involves playing the allocation that is closest to $\frac{1}{2}$.
This is because the difficulty of a problem is determined by the deviation of the Neyman allocation from $\frac{1}{2}$ -- when the Neyman allocation is close to $\frac{1}{2}$, the objectives of exploration and exploitation are aligned.
Suppose the Neyman allocation deviates from $\frac{1}{2}$, then as the allocation we play converges to the Neyman allocation, we are necessarily under-sampling one arm and thus \emph{slowing} down our convergence to the Neyman allocation.
This intuition is supported by the results of \citet{neopane2024logarithmic} and \citet{dai2023clip} who show that the Neyman regret scales inversely with $\abs{\policy - \frac{1}{2}}$.
Therefore, implementing optimism for adaptive ATE estimation involves playing the most feasible allocation (as determined by our past observations) closest to $\frac{1}{2}$ -- this is exactly the driving principle behind our \mainalgnameshort algorithm.

% \paragraph{Adaptive Explore-then-Commit.}
% Another class of algorithms that have been explored in the context of adaptive ATE estimation are so-called Explore-then-Commit (\ETC) algorithms which samples each arm uniformly for a fixed number of rounds (explore) and then uses the samples collected from this period to select an allocation to play for the remaining number of rounds.
% Since this style of algorithm admits a simple analysis, it has been studied in very general settings including both Markovian and non-Markovian decision processes \cite{Li2024OptimalTA}.
% The main issue with \ETC style approaches is that the correct number of rounds to explore for depends on the underlying problem instance. Without knowledge of relevant problem parameters, this approach must necessarily perform sub-optimally on most problem instances.

% Our \mainalgnameshort algorithm can be interpreted as a modification of the EtC approach which enables adaptive the number of exploration rounds to the underlying problem instance.
% As Lemma~\ref{lem:exploration_phase_length} shows, \mainalgnameshort will play the allocation $\policy_\round = \frac{1}{2}$ during the early stages of interaction until some time after which it will deviate from $\frac{1}{2}$ and start converging to the Neyman allocation.
% As such, our algorithm and analysis can be easily modified to accommodate the setting considered in \citet{Li2024OptimalTA} leading to immediate improvements over their EtC algorithm.
% However this is out of the scope of this work where our focus is in developing a precise understanding of the simple setting considered here.

\section{Results}\label{sec:results}
\newcommand{\minPolicy}{\underline{\policy}}
\newcommand{\minNeymanPolicy}{\underline{\neymanPolicy}}
In this section, we build our intuition on the behavior of \mainalgnameshort and conclude by stating our main result which is a bound on the Neyman regret of \mainalgnameshort.

Before we begin, we introduce some additional notation which will make our exposition easier.
For any $\policy$, we define $\policyGap{\policy} = \abs{\frac{1}{2} - \policy}$ and $\minPolicy = \min\cbrk{\policy, 1 - \policy}$. 
Additionally, we let $\stdevGap = \stdev[\tidx] - \stdev[\cidx]$.

Our analysis splits the behavior of \mainalgnameshort into two phases, an exploration \textit{exploration phase} and the \textit{concentration phase}.
We define the exploration phase as the rounds for which $\policy[\round] = \frac{1}{2}$.
During the early stages of interaction, we expect that each arm has been played sufficiently few times so that $\frac{1}{2} \in \CS[\round][\neymanPolicy]$, and the exploration time $\exploreTime$ is the length of this phase.
Intuitively, during this phase, there is not enough information in our observations to reliably predict $\neymanPolicy$ and so our best choice is to explore each arm uniformly.
Fortunately, the length of this phase is not too long, and our first result bounds the length of this phase in terms of the absolute distance between the standard deviations.
\begin{lemma}
    \label{lem:exploration_phase_length}
    Define the exploration time as
    \begin{equation}
        \exploreTime = \min\cbrk{\round : \policy_\round \neq \frac{1}{2}}.
    \end{equation}
    Then, with probability at least $1 - \errorProb$, we have
    \begin{equation}
        \exploreTime = \bigTildeO[\stdevGap^{-2} \log\frac{1}{\delta}].
    \end{equation}
\end{lemma}
The proof of this result is given in Appendix~\ref{app:exploration-phase}.
This result shows that \mainalgnameshort is able to adapt to the difficulty of the underlying problem instance --- if the gap between the standard deviations is large, then the exploration phase will be short, and if the gap is small, then the exploration phase will be longer.

Once the exploration phase is over, the algorithm will be able to focus on the concentration phase.
In this phase, optimism guarantees $\policyGap{\policy_\round} < \policyGap{\neymanPolicy}$.
Therefore, we can control the number of times each arm is played which we can in turn convert to bounds on $\abs{\policy[\round] - \neymanPolicy}$.

Our next result formalizes this intuition.
\begin{lemma}\label{lem:policy-difference-bound}
    With probability at least $1 - \errorProb$, we have that
    \begin{equation}
        \policy[\round] - \neymanPolicy = \bigTildeO[\sqrt{\frac{\log \frac{1}{\errorProb}}{\minNeymanPolicy \cdot \round}} \cdot \frac{1}{\stdev[\cidx] + \stdev[\tidx]}].
    \end{equation}
\end{lemma}
The reason for the appearance of $\minNeymanPolicy$ is due to the convergence of $\policy_\round$ based on the number of times that both arms have been played. If we play one arm too often, then the width of the confidence interval for $\neymanPolicy$ would depend entirely on the width of the lesser sampled arm.


Our main result combines the above lemmas to provide a bound on the Neyman regret.
\begin{theorem}\label{thm:neyman-regret-bound}
    With probability at least $1 - \errorProb$, the Neyman regret of \mainalgnameshort is upper-bounded as 
    \begin{equation}
        \bigTildeO[\stdevGap^{-2} + \paren{\frac{1}{\minNeymanPolicy}}^2\log \numRounds].
    \end{equation}
\end{theorem}
The first term above is the per-round Neyman regret during the exploration phase and our bound follows from the fact that the Neyman regret is at most $4$ when we play $\policy[\round] = \frac{1}{2}$.
The second term in our bound is the Neyman regret during the concentration phase and follows from the application of Lemma~\ref{lem:policy-difference-bound} in conjunction with Lemma 2 of \cite{neopane2024logarithmic} showing that the Neyman regret scales according to $\abs{\neymanPolicy - \policy[\round]}^2 \approx \frac{1}{\neymanPolicy \cdot \round}$.
Since the contribution to the Neyman regret from the reward estimation also scales like $\frac{1}{\neymanPolicy \cdot \round}$, taking a sum over these two terms gives us the desired result.

In order to get a better understanding of our result, we consider the behavior of a hypothetical algorithm which plays the optimal Neyman allocation $\neymanPolicy$ but incurs a loss based on the empirically computed allocation, $\policy_\round$.
A simple calculation shows that $\policy_\round$ converges to $\neymanPolicy$ at a rate of $\bigTheta[\paren{\neymanPolicy \cdot \round}^{-\frac{1}{2}}]$. 
This in turn implies that the Neyman regret would be 
\begin{equation}
     \bigTildeO[\paren{\frac{1}{\minNeymanPolicy}}^2\log \numRounds],
\end{equation}
which, modulo the regret from the clipping phase, is the same as the Neyman regret incurred by \mainalgnameshort.
This suggests that our algorithm is correctly adapting to the difficulty of the problem.

\paragraph{Comparison with \clipSMT.} 
At first glance, our result appears to be quite similar to the Neyman regret bound from \cite{neopane2024logarithmic} who similarly show a logarithmic bound on the Neyman regret.
However, this is not the case, due to differing definitions of the Neyman regret.
In \cite{neopane2024logarithmic}, the Neyman regret is defined with respect to the minimum variance over allocations for the fixed IPW estimator.
Our Neyman regret is defined with respect to the minimum attainable variance over any \emph{pair} of estimators and allocations.
This means that while our regret bounds share a similar form, the performance of our algorithm is significantly better than the performance of the \clipSMT algorithm.
Concretely, using our definition of the Neyman regret to characterize the performance of the \clipSMT algorithm (as well as the \clipOGD algorithm), we see that these algorithms actually have \emph{linear} Neyman regret since the variance of their policies cannot converge to the minimum attainable variance.

% Additionally, the constant term in our bound is significantly smaller than the analogous term in given in \cite{neopane2024logarithmic}.
% Indeed, we can show that our term is always smaller than the analogous term in \cite{neopane2024logarithmic}, and although a direct comparison is not feasible due to the implicit nature of their bound, we empirically observe that our term is often at least an order of magnitude smaller than the analogous term in \cite{neopane2024logarithmic}.
% We remark that while this constant term is not important asymptotically, it is important for applications like Randomized Control Trials where we typically do not have many samples.



% \subsection{Extensions to Off-Policy Evaluation}


\section{Experiments}\label{sec:experiments}


\begin{figure*}[!t]
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/normalized_mse_vs_rounds_0.5_0.5.png}
        \caption{$\mu_\cidx = 0.5$}
    \end{subfigure}
        \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/normalized_mse_vs_rounds_0.4_0.5.png}
        \caption{$\mu_\cidx = 0.4$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/normalized_mse_vs_rounds_0.3_0.5.png}
        \caption{$\mu_\cidx = 0.3$}
    \end{subfigure}
    \label{fig:normalized_mse}
        \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/normalized_mse_vs_rounds_0.2_0.5.png}
        \caption{$\mu_\cidx = 0.2$}
    \end{subfigure}
    \hfill
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/normalized_mse_vs_rounds_0.1_0.5.png}
        \caption{$\mu_\cidx = 0.1$}
    \end{subfigure}
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/normalized_mse_vs_rounds_0.05_0.5.png}
        \caption{$\mu_\cidx = 0.05$}
    \end{subfigure}
    \hfill
    \caption{Normalized MSE (\(\numRounds \cdot \text{MSE}\)) for \mainalgnameshort, \clipSDT, and the oracle baselines across six problem instances, each with Bernoulli rewards with $\mu_\tidx = \frac{1}{2}$ and varying $\mu_\cidx$. Results are averaged over \NUMSIMS  simulations. \mainalgnameshort consistently outperforms \clipSDT, with a 10-15
    improvement for smaller \(\numRounds\). Notably, \mainalgnameshort is competitive with the reward estimation oracle and even outperforms it in some cases due to better exploration of the reward function early on. As \(\numRounds\) increases, all algorithms converge to the oracle baseline. }
    \label{fig:simulation}
\end{figure*}

In this section, we present experiments to evaluate the empirical performance of our algorithm. We compare \mainalgnameshort against the \clipSDT algorithm proposed by \citet{cook24semi}, as well as two oracle algorithms that follow the Neyman allocation. One of these oracle algorithms sequentially estimates the reward, while the other has access to the true reward.

We do not include results for the \clipSMT and \clipOGD algorithms, as their variances fail to converge to the oracle variance, consistently leading to significantly worse performance than the other algorithms which obscures the clarity of the plots. This outcome is expected, given that both algorithms incur linear Neyman regret.

We consider 6 problem instances where both arms follows Bernoulli distributions.
For each of these problem instances, we fix the treatment mean to be $\frac{1}{2}$ and vary the control mean in order to vary the Neyman allocation.
For each of these problems, we run \mainalgnameshort, \clipSDT, and the reward estimation oracle for $\numRounds$ ranging from $100$ to $2000$ and plot the normalized MSE ($\numRounds \cdot \text{MSE}$) over \NUMSIMS simulations.
For the oracle baseline, we explicitly compute the MSE.
The results of these simulations are given in Figure~\ref{fig:simulation}.

Our results show that \mainalgnameshort consistently outperforms \clipSDT over all problem instances.
The difference between the two becomes negligible for larger values of $\numRounds$ which is expected since all algorithms eventually converge to the Neyman allocation and true reward function.
However, for smaller sample sizes, we see that \mainalgnameshort provides around a 10-15 percent improvement over \clipSDT.
This improvement is due to the reasons given in Section~\ref{sec:algorithm}.

The performance of \mainalgnameshort is competitive with the reward estimation oracle for moderate values of $\neymanPolicy$ and even outperforms the reward estimation oracle on some problem instances.
This is because \mainalgnameshort is more exploratory and obtains better reward estimates early on.

\section{Conclusion}\label{sec:conclusion}

This work proposed a new algorithm for adaptive ATE estimation.
We identified some key issues with past approaches which limited their performance both empirically and theoretically and demonstrated how to resolve them.
Our proposed solution borrows ideas from the literature on Regret Minimization and showed how to extend some of these ideas to the problem of adaptive ATE estimation.
We believe that these ideas will be crucial for developing adaptive algorithms for inference for more complicated settings as well as for related problems like Off-Policy Evaluation.

\subsection{Future Work}
We believe there are a few directions for future work that we find very compelling.
The first is the extension of our algorithm to the setting with covariates and with more sophisticated reward estimation.
In the causal inference literature, practitioners typically use nonparametric regression to estimate the $\trueReward$ and so extending our ideas to work with such estimators warrants more attention.
Another interesting direction is the extension to multiple arms.
Here we believe that the correct extension is to compute a confidence interval around the Neyman allocation, and then project this set onto the Uniform distribution over the actions.
The primary difficulty for this extension is in the analysis -- if we apply our techniques directly, this will result in an additional factor of $K$ in the term that is dependent on $\numRounds$, where $K$ is the number of arms.
It is an interesting question to see if our analysis can be improved to remove this additional factor.
Finally extending these ideas to more complicated interaction protocols such as Reinforcement Learning warrants further study.


\section{Impact Statement}
While our paper is primarily theoretical, we believe that the insights developed will be important for downstream applications such as causal inference which has broad applications over a variety of fields including clinical trials and A/B testing.


% \input{sections/001_intro}
% \input{sections/002_related_works}
% \input{sections/003_problem_setup}
% \input{sections/004_algorithm}
% \input{sections/005_results}
% % \input{sections/006_lower_bounds}
% \input{sections/007_experiments}
% \input{sections/008_conclusion}




