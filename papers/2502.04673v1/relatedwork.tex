\section{Related Works}
The problem of off-policy evaluation, which generalizes ATE estimation, has been extensively studied in the literature on reinforcement learning \citep{Dudik2011Doubly, Li2011Unbiased, Jiang2016DoublyRO}. 
Most of the research in this area has focused on offline estimation, leading to precise characterizations of minimax lower bounds along with matching upper bounds \citep{Li2015Toward, Wang2017OptimalAA, Duan2020Minimax, Ma2021MinimaxOE}. Beyond policy evaluation, these methods have been extended to estimate other quantities, such as the cumulative distribution function of rewards \citep{Huang2021OffCB, Huang2022OffRL}. 
However, there has been limited exploration of adaptive versions of these methods. 
Some existing work includes \citet{Hanna2017DataEfficientPE}, which focuses on off-policy learning, and \citet{Konyushova2021Active}, which integrates offline off-policy evaluation techniques with online data acquisition to enhance sample efficiency in policy selection. 
However, these works are primarily empirical.

A related area of research concerns inference procedures for adaptively collected data. This  can be categorized into asymptotic and non-asymptotic approaches. 
On the asymptotic side, one direction has focused on re-weighting estimators and establishing their asymptotic normality \citep{Hadad2021Confidence, Zhang2020Inference, Zhang2021Statistical}. 
Another direction avoids asymptotics, instead leveraging modern advances in martingale theory to derive nonasymptotic confidence intervals and sequences for adaptively collected data, including estimates of the ATE \citep{Howard2018TimeUniform, Waudby2023Estimating, WaudbySmith2022AnytimevalidOI}.