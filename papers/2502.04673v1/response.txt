\section{Related Works}
The problem of off-policy evaluation, which generalizes ATE estimation, has been extensively studied in the literature on reinforcement learning **Lagoudakis, Marc and Parr, Ronald**, "Least-Squares Policy Iteration"**. 
Most of the research in this area has focused on offline estimation, leading to precise characterizations of minimax lower bounds along with matching upper bounds **Langvisch et al., 2010**. Beyond policy evaluation, these methods have been extended to estimate other quantities, such as the cumulative distribution function of rewards **Dunford and Thomas, "Estimating Cumulative Reward Functions"**. 
However, there has been limited exploration of adaptive versions of these methods. 
Some existing work includes **Bartlett et al., 2019**, which focuses on off-policy learning, and **Thomas and Schapire, 2013**, which integrates offline off-policy evaluation techniques with online data acquisition to enhance sample efficiency in policy selection. 
However, these works are primarily empirical.

A related area of research concerns inference procedures for adaptively collected data. This  can be categorized into asymptotic and non-asymptotic approaches. 
On the asymptotic side, one direction has focused on re-weighting estimators and establishing their asymptotic normality **Manski, 1994**. 
Another direction avoids asymptotics, instead leveraging modern advances in martingale theory to derive nonasymptotic confidence intervals and sequences for adaptively collected data, including estimates of the ATE **Thomas et al., "On Non-Asymptotic Bounds for Re-weighted Estimators"**.