\appendix
\onecolumn

\section{Analysis of \mainalgname}

    \paragraph{Preliminaries}

    We will begin by defining our good event.
    Consider the following events
    \begin{align}
        \goodEvent_{\sigma}(\errorProb) &= \bigcap_{\aidx, \round \in \bbN} \cbrk{\abs{\empstdev[\round][\aidx] - \stdev[\aidx]} \leq 4.2\sqrt{\frac{\ell(\round, \errorProb)}{\round}}} \\
        \goodEvent_{N}(\errorProb) &= \bigcap_{\round \in \bbN} \cbrk{\abs{\acount{\round}{\aidx} - \sum \policy[\round][\aidx]} \leq \sqrt{\round \ell(\round, \errorProb)}} \\
        \goodEvent_{r}(\errorProb) &= \bigcap_{\aidx \round \in \bbN} \cbrk{\abs{\predReward[\round][\aidx] - \trueReward[\aidx]} \leq \sqrt{\round \ell(\round, \errorProb)}}.
    \end{align}

    Let $\tilde \errorProb = \frac{\errorProb}{5}$ and define the good event $\goodEvent(\tilde \errorProb) =  \goodEvent_{\sigma}(\errorProb) \cap \goodEvent_{N}(\errorProb) \cap  \goodEvent_{r}(\errorProb)$. 
    Applying Lemma~\ref{lem:stdev-concentration} to control $\goodEvent_{\sigma}(\tilde \errorProb)$ and Theorem 1 from \cite{Howard2018TimeUniform} to control $\goodEvent_{N}(\tilde \errorProb)$, and $\goodEvent_{r}(\tilde \errorProb)$
    shows that the event $\goodEvent(\tilde \errorProb)$ occurs with probability at least $1 - \delta$.
    Throughout the remained of this section, we assume the good event holds.

    \subsection{Proof of Theorem 1}
        We begin by decomposing the Neyman regret
        \begin{align}
            \neymanRegret_{\numRounds} 
                &= \sum_{\round = 1}^{\numRounds} \neymanLoss(\policy[\round], \predReward[\round]) \\
                &= \sum_{\round = 1}^{\numRounds} \sum_{\aidx} \paren{
                        \frac{\var[\aidx]}{\policy[\round][\aidx]} 
                        + \frac{1 - \policy[\round][\aidx]}{\policy[\round][\aidx]}\predRewardError[\round][\aidx][2] 
                        - \frac{\var[\aidx]}{\neymanPolicy[\aidx]}
                } \\
                &= \sum_{\round = 1}^{\exploreTime} \sum_{\aidx} \paren{
                        \frac{\var[\aidx]}{\policy[\round][\aidx]} 
                        + \frac{1 - \policy[\round][\aidx]}{\policy[\round][\aidx]}\predRewardError[\round][\aidx][2] 
                        - \frac{\var[\aidx]}{\neymanPolicy[\aidx]}
                    }
                    + \sum_{\round = \exploreTime + 1}^{\numRounds} \sum_{\aidx} \paren{
                        \frac{\var[\aidx]}{\policy[\round][\aidx]} 
                        + \frac{1 - \policy[\round][\aidx]}{\policy[\round][\aidx]}\predRewardError[\round][\aidx][2] 
                        - \frac{\var[\aidx]}{\neymanPolicy[\aidx]}
                    }.
        \end{align}
        
        For the first term, we have that $\policy_\round = \frac{1}{2}$, and $\predRewardError[\round][\aidx] \leq 1$, so that
        \begin{align}
            &\sum_{\aidx} \paren{
                        \frac{\var[\aidx]}{\policy[\round][\aidx]} 
                        + \frac{1 - \policy[\round][\aidx]}{\policy[\round][\aidx]}\predRewardError[\round][\aidx][2] 
                        - \frac{\var[\aidx]}{\neymanPolicy[\aidx]}
                    } \\
            &\leq \sum_{\aidx} \paren{
                        \frac{\var[\aidx]}{\policy[\round][\aidx]} 
                        - \frac{\var[\aidx]}{\neymanPolicy[\aidx]}
                    } + 2 \\
            &\leq 4,
        \end{align}
        to so that the regret from the exploration phase is $4 \exploreTime$.
        
        For the second term, we have
        \begin{align}
            &\sum_{\aidx} \paren{
                        \frac{\var[\aidx]}{\policy[\round][\aidx]} 
                        + \frac{1 - \policy[\round][\aidx]}{\policy[\round][\aidx]}\predRewardError[\round][\aidx][2] 
                        - \frac{\var[\aidx]}{\neymanPolicy[\aidx]}
                    } \\
            &= \sum_{\aidx} \paren{
                        \frac{\var[\aidx]}{\policy[\round][\aidx]} 
                        - \frac{\var[\aidx]}{\neymanPolicy(\aidx)}
                    }
                + \sum_{\aidx} \paren{
                        \frac{1 - \policy[\round][\aidx]}{\policy[\round][\aidx]}\predRewardError[\round][\aidx][2] 
                    } \\
        \end{align}
        We can bound the first term by applying Lemma 4.3 from \cite{neopane2024logarithmic} in conjunction with so that
        \begin{align}
            \sum_{\aidx} \paren{
                        \frac{\var[\aidx]}{\policy[\round][\aidx]} 
                        - \frac{\var[\aidx]}{\neymanPolicy(\aidx)}
                    }
            &\leq \frac{625}{\paren{\stdev[0] + \stdev[1]}^2}\frac{\ell(\round, \errorProb)}{\neymanPolicy \round}
        \end{align}

        In order to bound the second term, we observe that on the good event
        \begin{align}
            \abs{\trueReward[\aidx] - \predReward[\round][\aidx]} 
                &\leq \sqrt{\frac{\ell(\round, \errorProb)}{\acount{\round}{\aidx}}} \\
                &\leq \sqrt{\frac{\ell(\round, \errorProb)}{\neymanPolicy\round - \sqrt{\round \ell(\round, \errorProb)}}} \\
                &\leq 2  \sqrt{\frac{\ell(\round, \errorProb)}{\neymanPolicy\round}},
        \end{align}
        where in the last line we have again applied Lemma 4.5 from \cite{neopane2024logarithmic}.

        Therefore, we have that
        \begin{equation}
            \sum_{\aidx} \paren{
                        \frac{1 - \policy[\round][\aidx]}{\policy[\round][\aidx]}\predRewardError[\round][\aidx][2] 
                    }
            \leq \frac{8\ell(\round, \errorProb)}{\paren{\neymanPolicy}^2\round}
        \end{equation}

        We can bound the sum of these two terms as $625 \frac{\ell(\round, \errorProb)}{\paren{\neymanPolicy}^2\round}$.
        The result then follows by summing this over $\round < \numRounds$ and adding the Neyman regret from the exploration phase.

    \subsection{Proof of Lemma~\ref{lem:exploration_phase_length}}\label{app:exploration-phase}
        
        \begin{proof}
                    Suppose, without loss of generality, that $\neymanPolicy < \frac{1}{2}$; in order to obtain results for $\neymanPolicy > \frac{1}{2}$, we can simply flip the roles of the treatment and control arms.
                    For the case that $\neymanPolicy = \frac{1}{2}$, then $\mainalgnameshort$ will always play $\policy_\round$.
                    
                    Since $\neymanPolicy < \frac{1}{2}$, bounding $\exploreTime$ is equivalent to determining the largest time $\round$ such that $\UCS[\round][\neymanPolicy] < \frac{1}{2}$, i.e we wish to compute
                    \begin{equation}\label{eq:explore-time-bound-1}
                        \min\cbrk{\round : \frac{
                                \stdev[\tidx] + 4.2\sqrt{
                                \frac{\ell(\round, \errorProb)}
                                {\acount{\round}{\tidx}}}
                            }
                            {
                                \stdev[\cidx]
                                + \stdev[\tidx] 
                                + 4.2 \sqrt{ \frac{\ell(\round, \errorProb)}
                                {\acount{\round}{\tidx}}}
                                - 4.2 \sqrt{ \frac{\ell(\round, \errorProb)}
                                {\acount{\round}{\cidx}}}
                                }
                            < \frac{1}{2}
                        }
                    \end{equation}

                    Using the fact that $\policy_\round = \frac{1}{2}$ for all $\round < \exploreTime$, can control
                    \begin{equation}
                        \acount{\round}{\aidx} \in \sbrk{\frac{\round}{2} \pm 1.7 \sqrt{\round \ell(\round, \errorProb)}}.
                    \end{equation}

                    Plugging this into equation~\eqref{eq:explore-time-bound-1} and rearranging shows that we need to bound
                    \begin{equation}
                        \min\cbrk{\round : \frac{\ell(\round, \delta)}{\round\paren{\frac{1}{2} - 1.7 \sqrt{\frac{\ell(\round, \errorProb)}{\round}}}} < \frac{\stdevGap^2}{18}}.
                    \end{equation}


                    Applying Lemma B.10 from \cite{neopane2024logarithmic} shows that whenever $\round \geq \bigTildeO[\log(\frac{1}{\errorProb})]$, we have that $1.7 \sqrt{\frac{\ell(\round, \errorProb)}{\round}} < \frac{1}{4}$ so that we need to bound
                    \begin{equation}
                        \min\cbrk{\round : \round > \frac{64}{\stdevGap^2} \ell(\round, \errorProb)}.
                    \end{equation}

                    Another application of Lemma B.10 shows that this quantity is bounded by 
                    \begin{equation}
                        \frac{64}{\stdevGap^2} \log\frac{5.2}{\errorProb} + \frac{64}{\stdevGap^2} \log \log \frac{64}{\stdevGap^2}
                    \end{equation}
                    which  gives us the desired result.
                    
        
        \end{proof}
    
    \subsection{Proof of Lemma~\ref{lem:policy-difference-bound}}
        \begin{lemma}\label{lem:concentration-policy-diff}
            Let $\round \geq \exploreTime$. Then, with probability at least $1 - \errorProb$, we have that
            \begin{equation}
                \policy_{\round + 1} - \neymanPolicy \leq \frac{25}{\stdev[\cidx] + \stdev[\tidx]}\sqrt{\frac{\ell(\round, \errorProb)}{\neymanPolicy \round}}.
            \end{equation}
        \end{lemma}
        \begin{proof}
        Wlog we assume $\neymanPolicy < \frac{1}{2}$ so that $\minNeymanPolicy = \neymanPolicy$.
        First note that \(\timeidx \geq \exploreTime\), we have that 
        \begin{align}
            \policy_{\round + 1} &\in \sbrk{
            \neymanPolicy, 
            \frac{\stdev[\tidx] + \confWidth_{\tidx, \round}}
                {\stdev[\cidx] + \stdev[\tidx] + \confWidth_{\tidx, \round} - \confWidth_{\cidx, \round}}
            } \\
            &= \sbrk{
            \neymanPolicy, 
            \neymanPolicy\frac{\stdev[\cidx] + \stdev[\tidx]}
                {\stdev[\cidx] + \stdev[\tidx] + \confWidth_{\tidx, \round} - \confWidth_{\cidx, \round}}
            + \frac{\confWidth_{\tidx, \round}}
                {\stdev[\cidx] + \stdev[\tidx] + \confWidth_{\tidx, \round} - \confWidth_{\cidx, \round}}
            }\label{eq:policy-cs-width} \\
            &\subset \sbrk{\neymanPolicy, \frac{1}{2}} \label{eq:bad-interval},
        \end{align}
        where we have defined 
        \begin{equation*}
            \confWidth[\round][\aidx] = 4.2 \sqrt{\frac{\ell(\round, \errorProb)}{\acount{\round}{\aidx}}},
        \end{equation*}
        and equation~\eqref{eq:bad-interval} follows from the definition of the $\exploreTime$.
        
        Since $\policy_\round \in \sbrk{\neymanPolicy,\frac{1}{2}}$, we know that $1 - \policy_\round \in \sbrk{\frac{1}{2}, 1 - \neymanPolicy}$ which we use to control the number of times each arm is played.
        \begin{align}
            \acount{\round}{\tidx} &\geq \neymanPolicy \cdot \round - \sqrt{\round \ell(\round, \errorProb)} \\
            \acount{\round}{\cidx} &\geq \frac{\round}{2} - \sqrt{\round \ell(\round, \errorProb)}.
        \end{align}
        
        Plugging these values into the upper bound in equation~\eqref{eq:policy-cs-width}, some algebra shows that
        \begin{align}
            \policy_{\round + 1} - \neymanPolicy 
                &= \neymanPolicy\frac{\stdev[\cidx] + \stdev[\tidx]}
                        {\stdev[\cidx] + \stdev[\tidx] + \confWidth_{\tidx, \round} - \confWidth_{\cidx, \round}}
                    + \frac{\confWidth_{\tidx, \round}}
                        {\stdev[\cidx] + \stdev[\tidx] + \confWidth_{\tidx, \round} - \confWidth_{\cidx, \round}}
                    - \neymanPolicy \\
                &= \neymanPolicy 
                    \cdot \frac{\confWidth[\cidx][\round] - \confWidth[\tidx][\round]}
                        {\stdev[\cidx] + \stdev[\tidx] + \confWidth_{\tidx, \round} - \confWidth_{\cidx, \round}}
                    + \frac{\confWidth_{\tidx, \round}}
                        {\stdev[\cidx] + \stdev[\tidx] + \confWidth_{\tidx, \round} - \confWidth_{\cidx, \round}} \\
                &\leq \frac{\confWidth_{\cidx, \round}}
                        {\stdev[\cidx] + \stdev[\tidx] + \confWidth_{\tidx, \round} - \confWidth_{\cidx, \round}}
                    + \frac{\confWidth_{\tidx, \round}}
                        {\stdev[\cidx] + \stdev[\tidx] + \confWidth_{\tidx, \round} - \confWidth_{\cidx, \round}} \\
                &\leq 8.4 \sqrt{\frac{\ell(\round, \errorProb)}{\neymanPolicy \round - \sqrt{\round \ell(\round, \errorProb)}} } 
                    \cdot \paren{\frac{1}{\stdev[\cidx] + \stdev[\tidx] - \confWidth_{\cidx, \round}}}.
        \end{align}

        Applying Lemma B.10 from \citet{neopane2024logarithmic}, we have that when $\round = \bigTildeO[\paren{\frac{1}{\neymanPolicy}}^2\log \frac{1}{\errorProb}]$, we have that $\neymanPolicy \round - \sqrt{\round \ell(\round, \errorProb)} \geq 
        \frac{1}{2}\neymanPolicy\round$.
        Next, since $\round \geq \exploreTime$, we have that
        \begin{align}
            \confWidth_{\cidx, \round} 
                &= 4.2 \sqrt{\frac{\ell(\round, \errorProb)}{\round}} \\
                &\leq \frac{\stdevGap}{8}.
        \end{align}

        Therefore,
        \begin{align}
            \stdev[\cidx] + \stdev[\tidx] + \confWidth_{\tidx, \round}
                &\geq \stdev[\cidx] + \stdev[\tidx] - \frac{\stdevGap}{8} \\
                &=  \stdev[\cidx] + \stdev[\tidx] - \frac{\stdev[\cidx] - \stdev[\tidx]}{8} \\
                &\geq \frac{\stdev[\cidx] + \stdev[\tidx]}{2}.
        \end{align}

        Combining these results, we have that
        \begin{equation}
            \policy[\round + 1] - \neymanPolicy \leq \frac{25}{\stdev[\cidx] + \stdev[\tidx]}\sqrt{\frac{\ell(\round, \errorProb)}{\neymanPolicy \round}},
        \end{equation}
        which proves the desired result.
        \end{proof}
    
    
        
\section{Concentration Results}

The proof of this lemma is based on a similar proof found in \cite{audibert2006use} and extends the results to hold in the sequential setting.

\begin{lemma}\label{lem:stdev-concentration}
        Let $(X_\round)$ be a $[0, 1]$-valued stochastic process defined on some filtration $(\cF_\round)$ satisfying $\mean = \E[\round - 1]{X_\round}$ and $\var = \V[\round - 1]{X_\round}$.
        Define
        \begin{align}
            \empmean[\round] &= \frac{1}{\round} \sum_{\round = 1}^{\round} X_\round \\
            \empvar[\round] &= \frac{1}{\round} \sum_{\timeidx = 1}^{\round} \paren{X_\round - \empmean[\round]}^2.
        \end{align}
        Then, with probability at least $1 - \errorProb$, for all $\round \geq 2$ we have that
        \begin{equation}
            \stdev \in \sbrk{
                \empstdev[\round] - 1.7 \sqrt{\frac{\ell(\round, \errorProb)}{\round}}, 
                \empstdev[\round] + 4.2 \sqrt{\frac{\ell(\round, \errorProb)}{\round}}
            }.
        \end{equation}
\end{lemma}
\begin{proof}
    
    Define $Y_\round = (X_\round - \mu)^2 - \sigma^2$, and $S_\round = \sum_{i = 1}^{\round} Y_\round$.
    Letting \(\cV = \V[\round - 1]{Y_\round}\), we apply Theorem 1 from \cite{Howard2018TimeUniform} which gives us the following time-uniform Bernstein inequality (see Table 3 in the Appendix).
    Applying a union bound, we have with probability at least \(1 - \errorProb\), for all $\round \in \bbN$, that
    \begin{align}
        \abs{\mean_\round - \mean}
            &\leq 1.7 \stdev \sqrt{\frac{\ell\paren{\round, \frac{\errorProb}{4}}}{\round}} + 1.7 \frac{\ell\paren{\round, \frac{\errorProb}{4}}}{\round}\label{eq:stdev-concentration-mean-concentration}, \\
        \abs{Y_\round} 
            &\leq 1.7 \sqrt{\frac{\cV\ell\paren{\round, \frac{\errorProb}{4}}}{\round}} + 1.7 \frac{\ell\paren{\round, \frac{\errorProb}{4}}}{4\round} \\
            &\leq 1.7 \stdev \sqrt{\frac{\ell\paren{\round, \frac{\errorProb}{4}}}{\round}} + 1.7 \frac{\ell\paren{\round, \frac{\errorProb}{4}}}{\round}\label{eq:stdev-concentration-var-concentration} ,
    \end{align}
    where we set $\ell(\round, \errorProb) = \log\log 2\round + 0.72 \log \frac{5.2}{\errorProb}$ and the last inequality follows from the fact that $\cV < \sigma^2$.
    Letting $\mu_\round = \frac{1}{\round} \sum_{\timeIdx = 1}^{\round} X_\timeIdx$ some algebra demonstrates that
    \begin{align*}
        S_\round 
            % &= \sum_{i = 1}^{\round} Y_i \\
            &= \sum_{i = 1}^{\round} (X_i - \mu)^2 - \sigma^2 \\
            &= \sum_{i = 1}^{\round} \left[\left( (X_i - \mu_\round) - (\mu_\round - \mu)\right)^2 - \sigma^2\right] \\
            &= \sum_{i = 1}^{\round} \left[(X_i - \mu_\round)^2 + 2 (X_i - \mu_\round) (\mu_\round - \mu) + (\mu_\round - \mu)^2 - \sigma^2\right] \\
            &=  \round \sigma_\round^2 + 2(\mu_\round - \mu)\sum_{i = 1}^{\round}(X_i - \mu_\round) + \round (\mu_\round - \mu)^2 - \round \sigma^2 \\
            &= \round \sigma_\round^2 + 0 + \round(\mu_\round - \mu)^2 - \round \sigma^2 \\
            &= \round (\sigma_\round^2 - \sigma^2 + (\mu_\round - \mu)^2),
    \end{align*}
    which implies
    \begin{equation}
        \paren{\sigma_\round^2 - \sigma^2} = \frac{1}{\round}\sum_{\timeIdx = 1}^{\round} Y_\timeidx - \left( \mu_\round - \mu \right)^{2} \leq \frac{1}{\round}\sum_{\timeIdx = 1}^{\round} Y_\timeidx.
    \end{equation}
    
    Letting \(L = \frac{\ell(\round, \delta)}{\round}\), and applying the bounds in equations~\eqref{eq:stdev-concentration-mean-concentration}~and~\ref{eq:stdev-concentration-var-concentration}, some algebra shows that
    \begin{equation}
        \sigma^2 + 1.7 \sigma \sqrt{L} + 1.7 L - \sigma^{2}_\round \geq 0.
    \end{equation}
    Completing the square and rearranging shows that
    \begin{align}
        \sigma 
            &\geq \sqrt{\sigma^{2}_{\round} + \paren{1.7^2 - 1.7}L} - 1.7 \sqrt{L} \\
            &\geq \sigma_{\round} - 1.7 \sqrt{L}.
    \end{align}
    
   Repeating the same argument with $-Y_{\round}$ shows that
    \begin{equation}
        \sigma \leq \sigma_\round + 4.2 \sqrt{L}.
    \end{equation}

    Combining these bounds we have with probability at least $1 - \delta$, for all $\round > 2$
    \begin{equation}
        \stdev \in \sbrk{\sigma_\round - 1.7 \sqrt{\frac{\ell(\round, \errorProb)}{\round}}, \sigma_\round + 4.2 \sqrt{\frac{\ell(\round, \errorProb)}{\round}}}.
    \end{equation}
    
\end{proof}


\section{Misc. Results}

\begin{lemma}\label{lem:aaipw-variance}
    For any $\alg$, we have that
    \begin{align}
        \V[\alg, \bandit]{\estAIPW[\numRounds]} 
            & = \frac{1}{\numRounds^2} \sum_{\round = 1}^{\numRounds} \V[\alg, \bandit]{\AIPW_\round} \\
            &=\frac{1}{\numRounds^2} \sum_{\round = 1}^{\numRounds}
                \E[\alg, \bandit]{\sum_{\aidx} \frac{\var[\aidx]}{\policy_\round(\aidx)} 
                + \paren{\frac{1 - \policy_\round(\aidx)}{\policy_\round(\aidx)}} \estRewardError^2_{\round - 1}(\aidx)
                }
    \end{align}
\end{lemma}
\begin{proof}
    Leting $z_\round = \AIPW_\round - \ATE$, we have 
    \begin{align}
        \V[\alg, \bandit]{\estAIPW[\numRounds]} 
            &= \frac{1}{T^2} \E{\paren{\sum_{\round = 1}^{\numRounds} z_\round}^2} \\
            &= \frac{1}{\numRounds^2} \paren{
                \sum_{\round = 1}^{\numRounds} \E{z_\round^2} 
                +  \sum_{\round = 1}^{\numRounds }\sum_{\timeidx = 1}^{\round = 1} \E[]{z_\round \cdot z_{\timeidx}}} \\
            &= \frac{1}{\numRounds^2} \sum_{\round = 1}^{\numRounds} \E{z_\round^2} \\
            &= \frac{1}{\numRounds^2} \sum_{\round = 1}^{\numRounds} \V{\AIPW_\round}. 
    \end{align}

    The applying the law of total variance shows that \(\V[\alg, \bandit]{\AIPW_\round} = \E[\alg, \bandit]{\V{\AIPW_\round}[\filtration_{\round - 1}]}\) since \(\V{\E{\AIPW_\round}[\filtration_{\round - 1}]} = 0\).
    Computing the conditional variance, we obtain
    \begin{align}
    \V[\alg, \bandit]{\AIPW_\round}[\filtration_{\round - 1}]
        &= \E[\alg, \bandit]{\paren{\AIPW_\round - \ATE}^2}[\filtration_{\round - 1}] \\
        &= \E[\alg, \bandit]{\paren{
            \iw_\round \paren{\rewardDeviation_{\round} + \estRewardError[\round - 1]} 
            + \estRewardATE[\round - 1] - \ATE}^{2}
            }[\filtration_{\round - 1}] \\
        &= \E[\policy_\round]{
                \iw^2_\round \paren{\var + \estRewardError[\round - 1]^2} 
                - \paren{\ATE - \estRewardATE[\round - 1]}^{2}
            } \\
        &= \sum_{\aidx} 
            \frac{\paren{\var[\aidx] + \estRewardError[\round - 1]^{2}(\aidx)} }
                {\policy_\round(\aidx)}
            - \paren{\estRewardError[\round - 1][\tidx] - \estRewardError[\round - 1][\cidx]}^2 \\
        &= \sum_{\aidx} \sbrk
            {
                \frac{\var[\aidx]}{\policy_\round(\aidx)} 
                + \paren{\frac{1}{\policy_\round(\aidx)} - 1} \cdot \estRewardError[\round - 1]^{2}(\aidx)
            }
            + 2 \estRewardError[\round - 1](\tidx)\cdot\estRewardError[\round - 1](\cidx) \\
        &= \sum_{\aidx} \sbrk
            {
                \frac{\var[\aidx]}{\policy_\round(\aidx)} 
                + \paren{\frac{1 - \policy_\round(\aidx)}{\policy_\round(\aidx)} } \cdot \estRewardError[\round - 1]^{2}(\aidx)
            }
            + 2 \estRewardError[\round - 1](\tidx)\cdot\estRewardError[\round - 1](\cidx).
\end{align}
Therefore, we have
\begin{align}
    \V[\alg, \bandit]{\estAIPW[\numRounds]} 
        &= \frac{1}{\numRounds^2} \sum_{\round = 1}^{\numRounds} \E{\sum_{\aidx} \paren
            {
                \frac{\var[\aidx]}{\policy_\round(\aidx)} 
                + \paren{\frac{1 - \policy_\round(\aidx)}{\policy_\round(\aidx)} } \cdot \estRewardError[\round - 1]^{2}(\aidx)
            }
            + 2\estRewardError[\round - 1](\tidx)\cdot\estRewardError[\round - 1](\cidx)} \\
         &= \E[\alg, \bandit]{\sum_{\aidx}
                \frac{\var[\aidx]}{\policy_\round(\aidx)} 
                + \paren{\frac{1 - \policy_\round(\aidx)}{\policy_\round(\aidx)} } \cdot \estRewardError[\round - 1]^{2}(\aidx)
            },
\end{align}
where the second inequality follows from the fact that $\estRewardError[\round]^{2}(\aidx)$ are uncorrelated.
\end{proof}


% \section{Hypothetical Setting}\label{app:hypothetical}

% \input{appendix/opt_analysis}
% \input{appendix/technical_results}
% \input{appendix/concentration}
% \input{appendix/fixed_policy_analysis}
% \input{appendix/hypothetical_alg}

% \crefalias{section}{appendix} % uncomment if you are using cleveref

% \input{appendix/variance_estimation}