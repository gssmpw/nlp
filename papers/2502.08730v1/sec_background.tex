\section{Background
\label{sec:background}
}

A GP is a distribution over functions specified by a mean function $m(\bx)$ and a covariance or kernel function $k(\bx, \bx')$, where the kernel function is parametrized by $\theta$. 
By assuming that $m(\bx) = 0$ we denote a GP draw as 
$$
f(\bx) \sim \mathcal{GP}(0, k(\bx, \bx')). 
$$
For a finite set of inputs 
$\bX = \{\bx_n\}_{n=1}^N$ the 
distribution over the function values $\f
= \{f_n \}_{n=1}^N$ (stored as $N \times 1$ vector with $f_n := f(\bx_n)$) is 
the multivariate Gaussian $p(\f) = \mathcal{N}(\f | {\bf 0}, \bK_{\f \f} )$ where the $N \times N$ covariance 
matrix $\bK_{\f \f}$ has entries $[\bK_{\f \f}]_{i j} = k(\bx_i, \bx_j)$. 

We consider standard GP regression
where we are given a set of training inputs
$\bX$ and corresponding noisy outputs 
$\y = \{y_n\}_{n=1}^N$ where 
$y_n \in \Real$. Conditionally on the latent values $\f$, these outputs follow a factorized
Gaussian likelihood, $p(\y | \f) = \prod_{n=1}^N \mathcal{N}(y_n | f_n, \sigma^2) = \mathcal{N}(\y | \f, \sigma^2 I)$. The joint distribution over outputs $\y$ 
and latent values $\f$ is
\begin{equation}
p(\y | \f) p(\f) = \mathcal{N}(\y | \f, \sigma^2 I) \mathcal{N}(\f | {\bf 0}, \bK_{\f \f} ).
\end{equation}
To learn the hyperparameters 
$(\theta, \sigma^2)$ we can maximize the
log marginal likelihood which is analytically available,
\begin{equation}
\log p(\y) = \int p(\y | \f) p(\f) d \f = \log \mathcal{N}(\y | {\bf 0 }, \bK_{\f \f}  + \sigma^2 \bI). 
\label{eq:exact_marg_likel}
\end{equation}
After training we can perform predictions at test inputs $\bX_*$
by first computing the posterior 
over the corresponding test function values $\f_*$
\begin{align}
& p(\f_* | \y) = \int p(\f_* | \f) p(\f | \y) d \f = \\
 & \mathcal{N}(\f_* | \bK_{\f_* \f} (\bK_{\f \f} \! + \! \sigma^2 \bI)^{-1} \f,  \bK_{\f_* \f_*} \! \! - \! \bK_{\f_* \f} (\bK_{\f \f} \! + \! \sigma^2 \bI)^{-1} \bK_{\f \f_*} \! ) \nonumber 
% \label{eq:fstar_giveny}
\end{align} 
and then writing the predictive density as $p(\y_* | \y) = \int \mathcal{N}(\y_* | \f_*, \sigma^2 \bI) p(\f_* | \y) d \f_*$, which is the same as the above Gaussian but with  $\sigma^2 \bI$ added to the covariance. 
 
While the log marginal likelihood and predictive density have closed-form
expressions, they require 
the inversion of $\bK_{\f\f} + \sigma^2 \bI$ which costs $\mathcal{O}(N^3)$ and it is prohibitive for large datasets.  
Next we review methods using inducing 
points and particularly the 
variational approach \cite{titsias2009variational} that our method in \Cref{sec:proposedmethod} improves upon. 

\subsection{Sparse Variational Gaussian Process (SVGP)} 

The idea of inducing points 
 is to base a GP approximation 
on a smaller set of $M \ll N$  function values; see e.g., \citet{csato-opper-02,seeger-williams-lawrence-03a,Snelson2006,candela-rasmussen-05}. \citet{Snelson2006}
introduced pseudo inputs 
by instantiating extra GP function values $\bu = \{f(\bz_m)\}_{m=1}^M$ evaluated at locations $\bZ = \{\bz_m\}_{m=1}^M$ 
that can be optimized freely with gradient-based methods. However, 
the GP prior modification procedure 
\cite{candela-rasmussen-05,Snelson2006} 
does not result in a rigorous  approximation to the GP model. An alternative variational inference method  
\cite{titsias2009variational}, next referred to as
SVGP\footnote{Another common name for this method is Variational Free Energy (VFE); see \citet{Buietal2017,baueretal16,LiuetalGPreview20}.}, does not modify the GP prior but instead it augments the model with extra function values $\bu$: 
\begin{align}
& p(\y, \f, \bu) 
 \! = \! p(\y | \f) 
p(\f | \bu) p(\bu) \ \ \ \ \ \ \ \  \ \ \ \  \ \ \text{augmented joint}
\label{eq:augm-joint}
\\ 
& p(\f | \bu)   
\! = \! \mathcal{N}(\f | \K_{\f \f} \K_{\bu \bu}^{-1} \bu, \K_{\f \f} \! - \! \K_{\f \bu} \K_{\bu \bu}^{-1} \K_{\bu  \f}) \ \ \ \text{cond. GP} 
\label{eq:cond-gp-prior}
\\
& p(\bu) \! = \! \mathcal{N}(\bu | {\bf 0}, \K_{\bu \bu})  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{inducing GP prior}
\label{eq:ind-gp-prior}
\end{align}
where $\K_{\bu \bu}$ is the $M \times M$ covariance matrix on the 
inducing inputs $\bZ$, 
$\K_{\f \bu}$ is the $N \times M$ cross covariance between points in $\bX$ and $\bZ$, while $\bK_{\bu \f} = \K_{\f \bu}^\top$.  SVGP % method 
approximates the exact posterior $p(\f, \bu|\y)$ by a variational 
distribution $q(\f, \bu)$ through the minimization of  $\text{KL}[q(\f, \bu) || p(\f, \bu|\y)]$. A critical assumption is the following 
choice for $q$:
\begin{equation}
q(\f, \bu) 
= p(\f | \bu) q(\bu), 
\label{eq:pfuqu}
\end{equation}
where $q(\bu)$ is an optimizable $M$-dimensional variational %(Gaussian)
distribution,  while $p(\f | \bu)$ is the same conditional GP prior from \Cref{eq:cond-gp-prior}
that appears in the joint 
in (\ref{eq:augm-joint}). The KL minimization is expressed as the maximization of an evidence lower bound (ELBO) on the log marginal likelihood, 
\begin{align} 
\log p(\y) & \geq \int p(\f | \bu) q(\bu) \log \frac{p(\y | \f) \cancel{p(\f | \bu)} p(\bu)}{\cancel{p(\f | \bu)} q(\bu)} d \f d \bu \nonumber \\ 
& = \int q(\bu) \log \frac{\exp\{ \int p(\f | \bu) \log p(\y | \f) d \f\} p(\bu)}{q(\bu)} d \bu. \nonumber 
\end{align}
If we optimize over $q(\bu)$ and obtain the optimal choice $q^*(\bu) \propto \exp\left\{\int p(\f | \bu) \log p(\y | \f) d \f \right\} p(\bu)$, then we can substitute this $q^*(\bu)$ in the last line above and express the 
%tightest possible ELBO, the 
so called collapsed bound, having the general form 
$$
\log p(\y) \! \geq \! \mathcal{F} \! = \! \log \! \int \! \exp\left\{\int p(\f | \bu) \log p(\y | \f) d \f \right\} p(\bu) d \bu 
$$
which for the standard GP regression
model takes the form 
\begin{align}
\mathcal{F} = \underbrace{\log \mathcal{N}(\y | {\bf 0}, \bQ_{\f\f} + \sigma^2 \bI)}_{\text{DTC log lik}} 
- \underbrace{\frac{1}{ 2 \sigma^2} 
\text{tr}\left(  \bK_{\f\f} - \bQ_{\f\f} \right)}_{\text{trace term}},
\label{eq:collapsedbound_old}
\end{align}
where $\bQ_{\f\f} = \bK_{\f\bu}\bK_{\bu\bu}^{-1}\bK_{\bu\f}$ is the M-rank Nystr\'om matrix.
The first term in the bound is the deterministic training conditional (DTC) log likelihood \cite{seeger-williams-lawrence-03a, candela-rasmussen-05} 
%Banerjee2008} 
while the second is a regularization 
term which, since $\text{tr}(  \bK_{\f\f} - \bQ_{\f\f}) \geq 0$,  promotes $\bQ_{\f\f}$ to stay close to  $\bK_{\f\f}$. The inducing points $\bZ$ can be learned as variational parameters by maximizing the bound jointly with the hyperparameters $(\theta, 
\sigma^2)$, which requires 
$\mathcal{O}(N M^2)$ operations per optimization step. \citet{hensman2013gaussian} further reduced the operations to $\mathcal{O}(M^3)$ 
per optimization step by applying stochastic minibatch training for  
maximizing the uncollapsed version of the bound; see  \Cref{sec:stochasticopt}. 

To obtain the form of the GP posterior 
over any test function values $\f_*$ we can first write the exact form 
\begin{equation}
p(\f_* | \y) 
= \int p(\f_* | \f, \bu) p(\f,  \bu | \y)  d \f d \bu, 
\label{eq:exact_posteriorGP2}
\end{equation}
where $p(\f_* | \f, \bu)$  is the conditional GP of $\f_*$  given 
training function values $\f$ and inducing values $\bu$ while 
$p(\f,  \bu | \y)$ is the posterior 
over $(\f, \bu)$ written also 
as 
\begin{equation}
p(\f,  \bu | \y) = p(\f | \bu, \y) p(\bu | \y).
\label{eq:exact_augm_posterior}
\end{equation}
The SVGP method approximates $p(\f,  \bu | \y)$ by $q(\f, \bu)$ and therefore by plugging in this $q$ into (\ref{eq:exact_posteriorGP2}) we obtain
\begin{equation}
q(\f_* | \y) 
\! = \! \! \int p(\f_* | \f, \bu) p(\f |  \bu) q(\bu)  d \f d \bu \! = \! \! 
\int p(\f_* | \bu) q(\bu) d \bu, 
\label{eq:variational_posteriorGP}
\end{equation}
where $p(\f_* | \bu) = \int p(\f_* | \f, \bu) p(\f |  \bu) d \f$ comes from the GP consistency. For completeness, in \Cref{app:detailsSVGP}
we include further details about SVGP such as a derivation of the collapsed bound and the 
%exact 
Gaussian form 
of the optimal 
$q^*(\bu)$. 

We conclude this review of SVGP for regression with a couple of remarks that will be useful next. 

\begin{remark}
%[VFE can underfit]
The approximation becomes exact when 
$\bK_{\f \f} = \bQ_{\f \f}$ and the collapsed bound %from (\ref{eq:collapsedbound_old}) 
matches the log marginal likelihood in 
(\ref{eq:exact_marg_likel}). However, to obtain good approximations we may need sufficiently large number of inducing points \cite{burt2020convergence}.  Otherwise the bound will cause underfitting. 
%For instance, by writing a typical kernel function as the one in ??, as $k(\bx, \bx') = \sigma_f^2 c(\bx, \bx')$ where $c(\bx, \bx')\leq 1$ is the correlation function then regularization trace term is written as
%$
%- \frac{\sigma^2_f}{ 2 \sigma^2} 
%\text{tr}\left(  \bC_{\f \f} - \bC_{\f %\bu} \bC_{\bu \bu}^{-1} \bC_{\bu \f} %\right)
%$
%which causes the signal-to-noise 
% ratio $\frac{\sigma_f^2}{\sigma^2}$ 
%(i.e., amplitude parameter $\sigma_f^2$ over the noise variance $\sigma^2$) to be penalized towards small values. 
For instance, as studied by 
\citet{baueretal16} 
%and also in the original work \cite{titsias2009variational} 
the SVGP bound tends to overestimate the noise variance $\sigma^2$. 
%and simultaneously underestimate signal amplitude $\sigma_f^2$. 
\label{remark1}
\end{remark}

\begin{remark}
%[Approximating $p(\f | \bu, \y)$ by $p(\f | \bu)$]
 SVGP approximates $p(\f | \bu, \y)$ in the exact posterior 
in (\ref{eq:exact_augm_posterior}) 
by % the conditional GP 
$p(\f | \bu)$, %in the variational posterior in (\ref{eq:pfuqu}), 
while  $q(\bu)$ is treated optimally by %free-form 
KL minimization. If 
$p(\f | \bu, \y) \! = \! p(\f | \bu)$ %(where  recall $\f$ are the training function values 
%and not the full infinite function!)
then $\text{KL}[q(\f, \bu) || p(\f, \bu|\y)]=0$ and the % full infinite 
 approximation becomes exact, meaning $q(\f_* | \y) \! = \! p(\f_* | \y)$ for any $\f_* = f_*(\bX_*)$. 
%In other words, the finite vector of training function values $\f$ is special, since it suffices to approximate $\f$ in order to become accurate over the full infinite process. 
\label{remark2}
\end{remark}

%More generally, if we do not collapse $q(\bu)$ and let $q(\bu) = \mathcal{N}(\bbm_\bu,\bS_\bu)$, where $\bbm_\bu, \bS_\bu$ are trainable parameters, we %obtain the uncollapsed bound suitable for can use the uncollapsed bound formini-batch training and non-Gaussian likelihoods~\citep{hensman2013gaussian,hensman2015scalable}.
%\andriy{Briefly explain why, if there's space.}
% Computing this objective requires $\mathcal{O}(M^2N + M^3)$ operations, which can be further reduced by unbiased estimation of the first term using a mini-batch of training data.






 










