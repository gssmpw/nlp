\section{Experiments
\label{sec:experiments}
}

\begin{figure*}[t]
\centering
\begin{tabular}{ccc}
\includegraphics[scale=0.29]
{toy_prediction_exact.pdf} &
\includegraphics[scale=0.29]
{toy_prediction_trace.pdf} &
\includegraphics[scale=0.29]
{toy_prediction_log.pdf} \\
(a) & (b) & (c) \\                
\includegraphics[scale=0.29]
{toy_all_predictions.pdf} &
\includegraphics[scale=0.25]
{toy_all_losses.pdf} &
\includegraphics[scale=0.25]
{toy_all_variances.pdf} \\
(d) & (e) & (f)              
\end{tabular}
\caption{First row shows posterior predictions (means with 2-standard deviations) after
  fitting the exact GP (a), and the sparse GPs with either the standard collapsed SGPR bound (b) or the proposed SGPR-new collapsed bound (c). In panels (b),(c) the seven inducing points are intiliazed to the same random locations (shown on top with crosses) while the optimized values are shown at the bottom.
  Panel (d) superimposes all predictions in order to provide a more comparative visualization.
  Finally, panel (e) shows the ELBO (or exact log marginal likelihood for the exact GP) values across optimization steps while (f) shows the corresponding values for the noise variance $\sigma^2$.}
\label{fig:toy}
\end{figure*}


\subsection{Illustration in 1-D Regression}

In the first regression experiment we consider the 1-D  Snelson dataset \cite{Snelson2006}. We took a subset of 40 examples of this dataset and we fitted the exact GP with the squared exponential kernel $k(x, x') = \sigma_f^2 \exp( - \frac{ (x - x')^2}{2 \ell^2})$. We also fitted sparse variational GPs %, denoted as SGPR, 
with either the standard collapsed bound \cite{titsias2009variational} from \Cref{eq:collapsedbound_old} (SGPR) or the new collapsed bound from \Cref{eq:newcollapsedbound} (SGPR-new).
Both sparse GP methods use seven inducing points initialized at the
same values as shown in Figure \ref{fig:toy}. All methods are initialized to the same hyperparameter values; see \Cref{app:furtherresults}.

Figure \ref{fig:toy} shows the results. %Specifically,
Note that both SGPR and SGPR-new find similar inducing point locations. But SGPR-new,  as a tighter bound (see panel (e)), is able to reduce some bias when estimating
the hyperparameters since it finds a noise variance $\sigma^2$ closer to the one by exact GP (see panel (f)).  
This results in better predictions that match better the exact GP, as shown by the comparative visualization in panel (d). From panel (d), observe that both the mean and variances of SGPR-new are closer to the exact GP than SGPR.  


\subsection{Medium Size Regression Datasets
\label{sec:mediumregress}
}

To further investigate the findings from the previous section, we consider three medium size real-world UCI regression datasets (Pol, Bike, and Elevators)
with roughly 10k training data points each, and for which we can still run the exact GP. We choose the ARD squared exponential kernel $k(\bx, \bx') = \sigma_f^2 \exp( - \sum_{i=1}^d \frac{(x_i - x_i')^2}{2 \ell_i^2})$.
We run all three previous methods (Exact GP, SGPR, SGPR-new) five times with different random train-test splits;
see \Cref{app:furtherresults} for experimental details. We also include
in the comparison a fourth method (discussed in Related Work)
which is the \citet{artemevburt2021cglb}'s bound  (SGPR-artemev) that does training using the collapsed bound from \Cref{eq:artemvecollapsedbound} in \Cref{app:artemevbound}. 
All sparse GP methods use $M=1024$ or $M=2048$ inducing points initialized by k-means.  Figure \ref{fig:mediumsize1024} shows the objective function and the noise variance $\sigma^2$ across $10k$ optimization steps using Adam with base learning rate $0.01$ and for $M=1024$.  \Cref{fig:mediumsize2048} in \Cref{app:mediumsizeRegress} shows the corresponding plots for $M=2048$.  We observe that for Pol and Bike, SGPR-new matches closer the exact GP training than SGPR and SGPR-artemev. Specifically, SGPR-new gives higher ELBO and estimates the noise variance with reduced underfitting bias.
For the Elevators dataset, $M=1024$ inducing points were enough for sparse GP methods to closely match exact GP training. This happens because in this case $\bQ_{\f \f}$ accurately approximates $\bK_{\f \f}$, i.e., the elements $k_{ii} - q_{ii}$ get close to zero. Table \ref{table:smalldatasetsTestLL} reports test log-likelihood predictions which show that 
SGPR-new outperforms SGPR and SGPR-artemev.  

\begin{table}[t]
  \caption{Average test log-likelihoods for the medium size regression datasets.
  The numbers in parentheses are standard errors.
    %The SGPR methods used $M=1024$ inducing points.
  }
\label{table:smalldatasetsTestLL}
\vskip 0.15in
%\begin{small}
\begin{center}
%  \begin{sc}
\resizebox{\linewidth}{!}{%
\begin{tabular}{lcccr}
\toprule
& Pol  & Bike & Elevators \\
\midrule
Exact GP & $1.089(0.011)$ & $3.105(0.022)$ & $-0.386(0.001)$ \\
% Exact GP & $1.089(0.011)$ & $3.105(0.022)$ & $-0.386(0.001)$  \\
\midrule
 $M=1024$ & & & \\
SGPR & $0.821(0.008)$ & $2.176(0.020)$ & $-0.387(0.001)$\\
% SGPR-trace & $0.958(0.008)$  & $2.337(0.030)$ & $-0.387(0.001)$ \\
SGPR-artemev & $0.859(0.007)$ & $2.199(0.024)$ & $-0.387(0.001)$  \\
SGPR-new & $0.920(0.006)$ & $2.326(0.026)$  & $-0.387(0.001)$  \\
%SGPR-log & $0.998(0.008)$  & $2.511(0.021)$ & $-0.387(0.001)$ \\
\midrule
$M=2048$ & & & \\
% SGPR-trace & $0.821(0.008)$ & $2.176(0.020)$ & $-0.387(0.001)$\\
SGPR & $0.958(0.008)$  & $2.337(0.030)$ & $-0.387(0.001)$ \\
% SGPR-log & $0.920(0.006)$ & $2.326(0.026)$  & $-0.387(0.001)$  \\
SGPR-artemev & $0.976(0.008)$ & $2.356(0.029)$ & $-0.387(0.001)$  \\
SGPR-new & $0.998(0.008)$  & $2.511(0.021)$ & $-0.387(0.001)$ \\
\bottomrule
\end{tabular}}
%\end{sc}
%\end{small}
\end{center}
\vskip -0.1in
\end{table}


\begin{figure*}[t]
\centering
\begin{tabular}{ccc}
\includegraphics[scale=0.25]
{smallscale_elbo_pol_1024.pdf} &
\includegraphics[scale=0.25]
{smallscale_elbo_bike_1024.pdf} &
\includegraphics[scale=0.25]
{smallscale_elbo_elevators_1024.pdf} \\
\includegraphics[scale=0.25]
{smallscale_sigma2_pol_1024.pdf} &
\includegraphics[scale=0.25]
{smallscale_sigma2_bike_1024.pdf} &
\includegraphics[scale=0.25]
{smallscale_sigma2_elevators_1024.pdf} 
\end{tabular}
\caption{The two plots in each column correspond to the same dataset: first row shows the ELBO (or log-likelihood)
 for all four methods (Exact GP, SGPR, SGPR-new and SGPR-artemev) with the number of iterations, and the plot in the second row shows the
  corresponding values for $\sigma^2$. SGPR methods use $M=1024$ inducing points initialized by k-means. For each line we plot the mean and standard error
  after repeating the experiment five times with different train-test dataset splits; see \Cref{app:furtherresults} for further experimental details.       
}
\label{fig:mediumsize1024}
\end{figure*}


\subsection{Large Scale Regression Datasets
\label{sec:largeregress}
}

\begin{table*}[t]
\caption{Test log-likelihoods for the large scale regression datasets with standard errors in parentheses. Best mean values are highlighted.} 
% Uses random 80\% / 20\% training and test splits, repeated 5 times. }
\label{table:largescaleTestLL}
\makebox[\textwidth][c]{
\resizebox{1.02\textwidth}{!}{
\setlength\tabcolsep{2pt}
\begin{tabular}{ l l cc cc cc cc}
\toprule
& & Kin40k &  Protein & \footnotesize KeggDirected & KEGGU &  3dRoad & Song &  Buzz & \footnotesize HouseElectric \\
\cmidrule(lr){3-10}
& $N$ & 25,600 & 29,267 & 31,248 & 40,708 & 278,319 & 329,820 & 373,280 & 1,311,539  \\
& $d$ & 8 & 9 & 20 & 27 & 3 & 90 & 77 & 9  \\
\midrule
%\multirow{2}{*}{SVGP}
%& $1024$  
%& 0.094(0.003) & -0.963(0.006) & 0.967(0.005) & 0.678(0.004) & -0.698(0.002) & -1.193(0.001) & -0.079(0.002) & 1.304(0.002)  \\
%& $1536$  
%& 0.129(0.003) & -0.949(0.005) & 0.944(0.006) & 0.673(0.004) & -0.674(0.003) & -1.193(0.001) & -0.079(0.002) & 1.304(0.003) \\
%\midrule
From \citet{shietal2020} \\ 
ODVGP & $1024+1024$ 
& 0.137(0.003) & -0.956(0.005) & -0.199(0.067) & 0.105(0.033) & -0.664(0.003) & -1.193(0.001) & -0.078(0.001) & 1.317(0.002) \\
& $1024+8096$  
& 0.144(0.002) & -0.946(0.005) & -0.136(0.063) & 0.109(0.033) & -0.657(0.003) & -1.193(0.001) & -0.079(0.001) & 1.319(0.004) \\
SOLVE-GP & $1024 + 1024$ & 0.187(0.002) & -0.943(0.005) &  0.973(0.003) &  0.680(0.003) & -0.659(0.002) & -1.192(0.001) &  -0.071(0.001) & 1.333(0.003) \\
%\midrule
%SVGP
% \\
%& $2048$
%& 0.137(0.003) & {\bf -0.940}(0.005) & 0.907(0.003) & 0.665(0.004) & -0.669(0.002) & {\bf -1.192}(0.001) & -0.079(0.002) & 1.304(0.003) \\
\midrule
SVGP [ours] & 1024 & $0.108(0.002)$ & $-0.969(0.006)$ & $1.042(0.009)$ & $0.699(0.005)$ & $-0.704(0.003)$ & $-1.192(0.001)$ & $-0.069(0.002)$ & $1.383(0.002)$ \\
& 2048 & $0.237(0.002)$ & $-0.944(0.006)$ & ${\bf 1.050}(0.009)$ & ${\bf 0.703}(0.005)$ & ${\bf -0.650}(0.003)$ & ${\bf -1.190}(0.001)$ & $-0.063(0.001)$ & $1.419(0.002)$ \\
SVGP-new [ours]  & 1024 & $0.152(0.003)$ & $-0.965(0.006)$ & $1.044(0.009)$ & $0.699(0.005)$ & $-0.701(0.003)$ & $-1.192(0.001)$ & $-0.065(0.002)$ & $1.387(0.003)$ \\
 & 2048 & ${\bf 0.286}(0.002)$ & ${\bf -0.938}(0.006)$ & $1.051(0.009)$ & ${\bf 0.703}(0.005)$ & $-0.651(0.004)$ & ${\bf -1.190}(0.001)$ & ${\bf -0.060}(0.001)$ & ${\bf 1.421}(0.002)$ \\
\bottomrule 
\end{tabular}
}
}
\end{table*}


We consider 8 UCI regression datasets, with training data sizes ranging from tens of thousands to millions. 
%Results of exact GP regression have been reported on these datasets with distributed training~\citep{wang2019exact}. 
We implemented the stochastic optimization versions of the two scalable sparse GP methods: (i) the one that trains using the previous uncollapsed bound from
 \citet{hensman2013gaussian} (SVGP) and (ii) our new bound from    
\Cref{eq:newuncollapsedbound} (SVGP-new). We denote these stochastic optimization versions by SVGP to distinguish them from the corresponding
SGPR methods that use the more expensive collapsed bounds. We run the SVGP methods with $M=1024$ and $2048$ inducing points, Matern3/2 kernel with common lengthscale, minibatch size $1024$, Adam with base learning rate $0.01$ and $100$ epochs. These experimental settings match the ones in \citet{wang2019exact} and \citet{shietal2020} as further described  in \Cref{app:largescaleRegress}. Table \ref{table:largescaleTestLL} reports the test log likelihood scores
for all datasets. In the comparison we also included two strong baselines from Table 2 in \citet{shietal2020}, i.e., SOLVE-GP and ODVGP \cite{salimbeni2018orthogonally}.


\begin{figure*}
\centering
\begin{tabular}{ccc}
\includegraphics[scale=0.24]
{poisson_toy_all_predictions.pdf} &
\includegraphics[scale=0.24]
{poisson_toy_all_losses.pdf} &
\includegraphics[scale=0.24]
{poisson_elbo_nybicycle_16.pdf} \\
% (a) & (b) & (c)
\end{tabular}
\caption{({\bf left}) shows the % posterior 
predictions (means with 2-standard deviations) over counts (black dots) in the artificial data example  after
  fitting the Full GP, and the two SVGPs. This plot superimposes all predictions in order to provide a comparative visualization.
  %; see \Cref{app:poisson} for individual plots. 
  ({\bf middle})  shows the ELBO  across optimization steps for the artificial data example. ({\bf right}) shows the ELBO for the NYBikes dataset and $M=16$.}
\label{fig:poisson}
\end{figure*}


From the predictive log likelihood scores in Table \ref{table:largescaleTestLL} and also the corresponding Root Mean Squared Error (RMSE)
scores reported in  \Cref{table:largescaleRMSE} in \Cref{app:largescaleRegress}, we can conclude that training with the new SVGP-new variational bound
provides a clear improvement compared to training with the previous SVGP bound. Note that this improvement requires no change in the computational
cost, and in fact there is only a minor modification needed to be done in an existing SVGP implementation in order to run SVGP-new.  

\vspace{-1mm}

\subsection{Poisson Regression
  \label{sec:poisson}
}

\vspace{-1mm}

We consider a non-Gaussian likelihood example where the output data are counts modeled  by a Poisson likelihood 
$p(\y | \f) = \prod_{i=1}^N \frac{e^{f_i}}{y_i !} e^{-e^{f_i}}$  where the log intensities values follow a GP prior. For such 
case the new variational approximation includes a single additional variational parameter denoted by $v$, which is optimized together 
with the remaining parameters; see \cref{sec:nongaussian}. We will compare training with the new ELBO 
 from \Cref{eq:nonGaussian_bound_tractable}  (we denote this method by SVGP-new) with the standard ELBO that is obtained by restricting  $v=1$  (SVGP). 
 
Firstly, we consider an artificial example of $50$ observations with 1-D inputs placed in the grid $[-10, 10]$ where counts are
generated using Poisson intensities given by $\lambda(x) = 3.5 + 3  \sin(x)$. We train the GP model with the SVGP bound and the proposed SVGP-new bound using $6$ inducing points initialized to the same values for both methods; see \Cref{app:poisson}. 
\Cref{fig:poisson}(left) shows the 
% observed counts together with the 
predictions obtained by SVGP, SVGP-new 
and non-sparse %or full 
variational % inference 
GP (Full GP). From
this figure and from the
ELBO values, 
we observe that SVGP-new
remains closer to Full GP.  

Secondly, we consider a real dataset (NYBikes) about bicycles crossings going over bridges in New York City\footnote{This dataset is freely available from
\url{https://www.kaggle.com/datasets/new-york-city/nyc-east-river-bicycle-crossings}.}.
This dataset is a daily record of the number of bicycles crossing into or out of Manhattan via one of the East River bridges over a period 9 months. The data contains $210$  points and we randomly choose $90\%$ for training and $10\%$ for test.   
We apply GP Poisson regression for the Brooklyn bridge counts where the input vector $\bx$ is taken to be two-dimensional consisted of 
 maximum and minimum daily temperatures.  We train the sparse GPs with either SVGP or SVGP-new and with $M=8,16,32$ 
 inducing points initialized by k-means.  Since the dataset is small  we also run the non-sparse  Full GP. The ELBO across iterations in \Cref{fig:poisson} (right) and the test log likelihood scores (\Cref{table:poisson_nybikes} 
 in \Cref{app:poisson})
 indicate that  SVGP-new provides a better approximation than SVGP.  
 
  
   
   








