\section{Related Work
\label{sec:relatedwork}
}


Several recent works on sparse GPs focus on constructing  efficient inducing points, such as works that place inducing points on a grid \citep{wilson2015kernel,evans2018scalable,gardner2018product}, construct inter-domain Fourier features \cite{ hensmanetal2018}, provide Bayesian treatments to inducing inputs \cite{rossi21a}
or use nearest neighbor 
sparsity structures
\cite{tran21a, wu22h}.
There exist also algorithms that  allow to increase the number of inducing points using the decoupled method \citep{cheng2017variational, havasi2018deep}
and the related orthogonally decoupled approaches 
\citep{salimbeni2018orthogonally, shietal2020, sun2021, tiao2023}. 
Our contribution is orthogonal to these previous methods since 
we relax the conditional GP
prior
%, $p(\f | \bu)$, 
assumption in the posterior variational approximation. This means that our method could be used to improve previous variational sparse GP approaches,
as the  ones mentioned above as well as earlier schemes that select inducing points from the training inputs \cite{Cao2013,Chai12,Schreiter2016}. 

\citet{XinranZhu2023} proposed 
inducing points GP approximations that change the conditional GP $p(\f | \bu)$ in the variational approximation to a modified conditional GP that uses different kernel hyperparameters in its mean vector. 
Note that our method differs since
our $q(\f | \bu)$ directly tries to 
construct a better approximation to the exact posterior
$p(\f | \bu, \y)$, using the extra $\bV$ variational parameters, 
without changing the kernel hyperparameters; see \Cref{sec:proposedmethod}. More 
importantly, our method has $\mathcal{O}(N M^2)$ cost, while the ELBO in  \citet{XinranZhu2023} (see Section 3.1 and Appendix A.1 in their paper) has cubic cost $\mathcal{O}(N^3)$ since it depends
on the inverse of $\bK_{\f \f} - \bQ_{\f \f}$ (denoted as $\tilde{\bK}_{nn}$ in their paper).  %\citet{XinranZhu2023}). 

\citet{artemevburt2021cglb}
%, by applying linear algebra operations, 
derived an upper bound on the log determinant $\log |\bK_{\f \f} + \sigma^2 \bI|$ in the exact GP log marginal likelihood and obtained the following tighter upper bound to the initial trace 
regularization term $-\frac{1}{2 \sigma^2} \text{tr}\left(  \bK_{\f\f} - \bQ_{\f\f} \right)$: 
\begin{equation}
- \frac{N}{ 2} \log\left( 1 + 
\frac{\text{tr}(\bK_{\f\f} - \bQ_{\f\f})}{N \sigma^2} \right).
\label{eq:artemevbound}
\end{equation}
Our bound is tighter since 
from Jensen's inequality 
it holds $ - \frac{N}{ 2} \log\left( 1 + 
\frac{\text{tr}(\bK_{\f\f} - \bQ_{\f\f})}{N \sigma^2} \right)
\leq - \frac{1}{2} \sum_{i=1}^N \log\left( 1 + 
\frac{k_{ii} - q_{ii}}{\sigma^2} \right)$. Further, the above regularization term
can be interpreted as a restricted special case of our method, obtained through a $q(\f | \bu)$ from \Cref{eq:qfu} where the diagonal matrix $\bV$ is constrained to be spherical $\bV = v  \bI$; see \Cref{app:artemevbound}. Finally note, 
that unlike (\ref{eq:artemevbound}) 
(where the sum is inside the logarithm) our bound allows to apply stochastic optimization
as described in \Cref{sec:stochasticopt}.

Finally, 
\citet{Buietal2017}
used  power expectation 
propagation that minimizes  $\alpha$-divergence and derived an approximation 
to the log marginal likelihood that interpolates between the FITC ($\alpha=1$) log marginal 
likelihood \cite{Snelson2006,candela-rasmussen-05} and the standard collapsed 
variational bound in (\ref{eq:collapsedbound_old})
($\alpha \rightarrow 0$).
This approximation uses the regularization 
term 
\begin{equation}
-\frac{1-\alpha}{2 \alpha}
\sum_{i=1}^N \log\left( 1 + 
\alpha \frac{k_{ii} - q_{ii}}{\sigma^2} \right). 
\label{eq:Buiregularization}
\end{equation}
This is different from 
ours since
there is no value of $\alpha$ 
such that the two regularization terms will become equal. 
For example, note that for $\alpha \rightarrow 0$, \Cref{eq:Buiregularization} 
reduces to $-\frac{1}{2 \sigma^2} \text{tr}\left(  \bK_{\f\f} - \bQ_{\f\f} \right)$ as discussed 
%in Section 3.6 
in \citet{Buietal2017}. 
