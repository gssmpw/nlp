\section{Introduction}
\label{sec:intro}

Gaussian processes (GPs) are  nonparametric models for learning %unknown 
functions using Bayesian learning. Thanks to their flexibility and ability to quantify uncertainty, GPs have found many applications in 
 machine learning \cite{rasmussen2006gaussian}, spatial  modeling \cite{Cressie1993}, computer experiments \cite{OHagan78,gramacy2020surrogates},  Bayesian optimization \cite{Jones1998EfficientGO,garnett_bayesoptbook_2023}, robotics and control \cite{deisenroth2011pilco}, unsupervised learning 
\citep{lawrence2005probabilistic} and  others. 


Despite the numerous applications, GPs  suffer from $\mathcal{O}(N^3)$ time cost  and $\mathcal{O}(N^2)$ storage where  $N$ is the number of training examples. This has originated a large body of research on scalable or sparse GP methods
expanded in several decades; see e.g., Chapter 8 in \citet{rasmussen2006gaussian}  
for an early review and  \citet{heaton2018, LiuetalGPreview20} for recent treatments. 
An important class of methods 
 bases an approximation on a small set of $M \ll  N$ 
inducing points 
\citep{csato-opper-02,lawrence-seeger-herbrich-01, seeger-williams-lawrence-03a,Snelson2006, candela-rasmussen-05,Banerjee2008,Finley2009,titsias2009variational, hensman2013gaussian,Buietal2017,burt2020convergence}
that reduce the time complexity to $\mathcal{O}(N M^2)$ and 
the storage to $\mathcal{O}(N M)$. 
% GPs have also been unfavourably compared to deep learning models for lacking representation learning capabilities.

Among inducing point methods, the sparse variational Gaussian process (SVGP), introduced for standard regression \cite{titsias2009variational}, 
applies variational inference to obtain a posterior approximation and selects hyperparameters and inducing points by maximizing an evidence lower bound. Unlike 
the prior approximation framework 
\cite{candela-rasmussen-05}, SVGP
leaves the GP prior unchanged and instead it reduces the cost to $\mathcal{O}(N M^2)$ 
by imposing a special structure on the variational 
distribution. 
This framework has been extended
to stochastic gradient optimization
\cite{hensman2013gaussian} and non-Gaussian likelihoods \cite{Chai12,hensman2015scalable,lloyd15,Dezfouli15,Sheth15}. Also, it has been explained  as KL minimization between stochastic processes 
\cite{Matthews2015OnSV}.

An important aspect of the SVGP method is that it uses a special form for the variational distribution. It approximates the exact posterior distribution 
$p(\f, \bu | \y)$ over the training function values $\f$ and the
inducing variables $\bu$ 
% iven data $\y$ 
(see \Cref{sec:background} for precise definitions)
by a variational distribution of the form $q(\f, \bu) = p(\f | \bu) q(\bu)$, where
$q(\bu)$ is some optimizable  distribution over the inducing variables, while
$p(\f | \bu)$ is the conditional GP prior. 
This special form of the variational approximation seems to be fundamental, and it has been applied also to more complex GP models, such as those with multiple outputs 
\cite{alvarez10a,NguyenBonilla14,fariba19}, uncertain inputs \cite{titsias10a} and multiple layers \cite{damianou13a,salimbeni2017doubly}. However, an open question regarding
the SVGP framework is whether this particular form of variational distribution
is really necessary to obtain scalable computations. The answer we give in this paper is that â€œit is not", since at least for training a GP model it can be relaxed. 

To this end, we derive new variational bounds for training 
sparse GP regression models by replacing 
$p(\f |\bu)$ in the variational distribution with a more general conditional distribution $q(\f | \bu)$. This $q(\f | \bu)$ depends on $N$ additional parameters (on top of the parameters of $p(\f | \bu)$), i.e., as many as the training examples, and it is constructed to enable better covariance approximation of the underlying true factor $p(\f | \bu, \y)$. We show how to analytically optimize over the $N$ parameters and obtain a better posterior approximation together with a tighter 
collapsed evidence lower bound. 
The new bound is also amenable to stochastic gradient optimization, and its
simple form suggests that  it can be implemented with minor modifications to existing sparse GP code. Furthermore, we also describe extensions of the method to non-Gaussian likelihoods.


The remainder of the paper is as follows. \Cref{sec:background} provides 
an overview of GPs and the variational approach to sparse GPs using inducing points. \Cref{sec:proposedmethod} 
derives the new evidence lower bounds for training. \Cref{sec:relatedwork} discusses connections with previous works. %especially with the work of \citet{artemevburt2021cglb} 
%and the method of \citet{Buietal2017}. 
\Cref{sec:experiments} presents experiments using several %regression 
datasets showing that the new bounds can reduce underfitting bias and can lead to better predictive performance. Finally, \Cref{sec:conclusions} 
concludes with a discussion and suggestions for future work.   
 





