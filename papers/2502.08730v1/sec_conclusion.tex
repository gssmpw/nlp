\vspace{-1mm}

\section{Conclusions
\label{sec:conclusions}
}
\vspace{-1mm} 

We have presented a method that relaxes the conditional GP assumption in the approximate 
distribution in sparse variational GPs.
This leads to tighter collapsed and uncollapsed bounds, that maintain the computational cost with the previous bounds and can reduce training underfitting. For future work 
an interesting topic is to apply our method to more complex GP models, 
such as those with multiple outputs, with uncertain inputs and deep GPs. 
For the Bayesian GP-LVM, where the collapsed closed form bound has strong
similarities with the previous GP regression collapsed bound in \Cref{eq:collapsedbound_old}, deriving  
a new collapsed bound is tractable as described in \Cref{app:bgplvm}. 
Finally, it might be useful to investigate whether theoretical convergence results on sparse GPs
\cite{burt2020convergence,wild2023etal}, can be improved given the new collapsed lower bound.   





 









