

\section{Proposed Method: Tighter Bounds
\label{sec:proposedmethod}
}

Remark \ref{remark1} suggests that 
it would be useful to tighten the collapsed bound 
%in (\ref{eq:collapsedbound_old}) 
in order to reduce underfitting bias 
and match better exact GP training. 
Remark \ref{remark2} suggests that one way to tighten the bound is to replace %the conditional GP 
$p(\f | \bu)$, in the variational approximation in (\ref{eq:pfuqu}), with another distribution 
that can better approximate 
$p(\f | \bu, \y)$. Next we develop a method that does this while keeping the cost unchanged. 

Let us write the 
exact form of $p(\f | \bu, \y)$. By noting that this quantity is the exact posterior over $\f$ in a GP regression model with joint $p(\y | \f) p(\f | \bu)$ 
%(where $p(\f | \bu)$ is now the effective GP prior) 
we conclude that this %posterior 
is 
$$
p(\f | \bu, \y) = \mathcal{N}\left(\f| \m(\y,\bu), 
(\widetilde{\bK}_{\f \f}^{-1} + \frac{1}{\sigma^2} \bI)^{-1} \right),
$$
where $\m(\y,\bu)  = \E[\f | \bu] + \widetilde{\bK}_{\f \f} (\widetilde{\bK}_{\f \f} + \sigma^2 \bI)^{-1} (\y - \E[\f | \bu]) $
with $\E[\f | \bu] = \bK_{\f \bu} \bK_{\bu \bu}^{-1} \bu$ and $\widetilde{\bK}_{\f \f} = \bK_{\f \f} - \bQ_{\f \f}$. Note that under this notation, 
$p(\f | \bu) = \mathcal{N}(\f | \E[\f | \bu], \widetilde{\bK}_{\f \f})$. We will construct a new $q(\f | \bu)$ 
that keeps the same mean $\E[\f | \bu]$ 
as $p(\f | \bu)$ but it replaces $\widetilde{\bK}_{\f \f}$ with a closer approximation to the 
% exact 
covariance 
% matrix 
$(\widetilde{\bK}_{\f \f}^{-1} + \frac{1}{\sigma^2} \bI)^{-1}$ of $p(\f | \bu, \y)$. We first 
write this %latter 
matrix as 
\begin{equation}
(\widetilde{\bK}_{\f \f}^{-1} + \frac{1}{\sigma^2} \bI)^{-1}
= \widetilde{\bK}_{\f \f}^{\frac{1}{2}}
( \bI + \frac{1}{\sigma^2} \widetilde{\bK}_{\f \f})^{-1} 
\widetilde{\bK}_{\f \f}^{\frac{1}{2}}.
\label{eq:exact_cov_pfuy}
\end{equation}
Then we approximate the inverse 
$( \bI + \frac{1}{\sigma^2} \widetilde{\bK}_{\f \f})^{-1}$ by a diagonal matrix $\bV = \text{diag}(v_1, \ldots,v_N)$ of $N$ variational parameters $v_i > 0$. In other words,  in the initial $q(\f, \bu) = p(\f|\bu)q(\bu)$ we will replace $p(\f|\bu)$ by 
\begin{equation}
q(\f|\bu) = \mathcal{N}(\f | \bK_{\f \bu} \bK_{\bu \bu}^{-1} \bu, (\bK_{\f \f} - \bQ_{\f \f})^{\frac{1}{2}} \bV
(\bK_{\f \f} - \bQ_{\f \f})^{\frac{1}{2}}).
\label{eq:qfu}
\end{equation}
The ELBO now is written as 
\begin{align} 
 & \int q(\f | \bu) q(\bu) \log \frac{p(\y | \f) p(\f | \bu) p(\bu)}{q(\f | \bu) q(\bu)} d \f d \bu = \nonumber \\ 
& \int \! \!  q(\bu) \! \left\{ \! \log \frac{e^{\E_{q(\f | \bu)}[\log p(\y | \f)]} p(\bu)}{q(\bu)} \! - \! \text{KL}[q(\f | \bu) || p(\f | \bu)] 
\! \right\} \! d \bu \nonumber 
\end{align}
and the challenge is to see whether 
$\text{KL}[q(\f | \bu) || p(\f | \bu)]$ 
and $\E_{q(\f | \bu)}[\log p(\y | \f)]$ 
are computable in $\mathcal{O}(N M^2)$ time. 
We have the following results (proofs are in  \Cref{app:detailsNewbounds}).
\begin{lemma}
\label{lem:KLqfupfu}
\emph{$\text{KL}[q(\f | \bu) || p(\f | \bu)] 
= \frac{1}{2} \sum_{i=1}^N (v_i - \log v_i - 1)$}.
\end{lemma}
\begin{lemma} 
\label{lem:Expqfu_loglik}
Let us denote the diagonal elements of \emph{$\bK_{\f \f} - \bQ_{\f \f}$} as 
\emph{$k_{ii} - q_{ii}$} for \emph{$i=1,\ldots,N$}. Then  
\emph{\begin{align}
& \E_{q(\f | \bu)}[\log p(\y | \f)] \nonumber \\ 
& \! = \! \log \mathcal{N}(\y | \bK_{\f \bu}
\bK_{\bu \bu }^{-1} \bu, \sigma^2 \bI)
- \frac{1}{2 \sigma^2} 
\sum_{i=1}^N v_i (k_{ii} - q_{ii}). 
\end{align}}
\end{lemma}
By combining the two lemmas the full bound is written as 
\begin{align} 
& \int \! \!  q(\bu) \log \frac{  \mathcal{N}(\y | \bK_{\f \bu}
\bK_{\bu \bu }^{-1} \bu, \sigma^2 \bI) p(\bu)}{q(\bu)}  d \bu \nonumber \\
& - \frac{1}{2} 
\sum_{i=1}^N \left\{  v_i \left(1 + \frac{k_{ii} - q_{ii}}{\sigma^2}\right) - \log v_i -1 \right\}.  
\label{eq:newcollapsedbound_with_vis}
\end{align}
\begin{proposition}%[new collapsed bound]
Maximizing the bound in (\ref{eq:newcollapsedbound_with_vis}) with respect to \emph{$q(\bu)$}
and each \emph{$v_i$} results in the 
optimal settings \emph{$q^*(\bu) \propto  \mathcal{N}(\y | \bK_{\f \bu}
\bK_{\bu \bu }^{-1} \bu, \sigma^2 \bI) p(\bu)$} and  
\emph{$v_i^* = \left(1 + \frac{k_{ii} - q_{ii}}{\sigma^2} \right)^{-1}$}. By substituting these values 
back to (\ref{eq:newcollapsedbound_with_vis}) we obtain
\emph{\begin{equation} 
\mathcal{F}_{new} \! = \! \log  \mathcal{N}(\y |{\bf 0},   \bQ_{\f \f} + \sigma^2 \bI) 
 - \frac{1}{2}  
\sum_{i=1}^N \log \left(\! 1 + \frac{k_{ii} - q_{ii}}{\sigma^2} \! \right).   
\label{eq:newcollapsedbound}
\end{equation}
}
\label{prop:newbound}
\end{proposition}
The first term is the 
DTC log likelihood as in the original bound in (\ref{eq:collapsedbound_old}),  
but the regularization term 
makes the bound tighter, 
i.e., $\log p(\y) \geq \mathcal{F}_{new} \geq \mathcal{F}$, due to the inequality $\log(a + 1) \leq a$. Also since $\log(a + 1) < a$ for all $a>0$, if $\bK_{\f \f} \neq \bQ_{\f \f}$ 
(so there is at least one $k_{ii} - q_{ii} > 0$), then $\mathcal{F}_{new} > \mathcal{F}$. This means that $\mathcal{F}_{new}$ is strictly better than $\mathcal{F}$ unless both bounds match exactly the log marginal likelihood. 

Clearly, $\mathcal{F}_{new}$ has $\mathcal{O}(N M^2)$ cost and its implementation requires a minor modification to the initial bound. The optimal $q^*(\bu)$
is the same as in the initial SVGP method, while 
an interpretation of the optimal $v_i^*$ values
is the following.  
%\begin{remark}
%Recall that $\log(a + 1) < a$ for $ \forall a>0$. Thus, if $\bK_{\f \f} \neq \bQ_{\f \f}$ (so there is at least one $k_{ii} - q_{ii} > 0$),  $\mathcal{F}_{new} > \mathcal{F}$ which means that $\mathcal{F}_{new}$ is strictly better than $\mathcal{F}$ unless both bounds match exactly the log marginal likelihood. 
%\end{remark}

\begin{remark}
The diagonal matrix $\bV^*$ (with the optimal $v_i^*$ values in its diagonal) is the inverse obtained after zeroing out the off-diagonal elements of $\bI + \frac{1}{\sigma^2}(\bK_{\f \f} - \bQ_{\f \f})$, 
i.e., $\bV^* = \text{diag}[\bI + \frac{1}{\sigma^2}(\bK_{\f \f} - \bQ_{\f \f})]^{-1}$ which approximates 
$(\bI + \frac{1}{\sigma^2}(\bK_{\f \f} - \bQ_{\f \f}))^{-1}$ 
in \Cref{eq:exact_cov_pfuy}. %Also note that in the ordering of positive definite matrices it holds $\bV^* \leq \bI$, from which it follows that $q(\f | \bu)$ has smaller covariance than $p(\f | \bu)$ and more accurately approximates the covariance of $p(\f | \bu, \y)$. 
%The latter %,  as implied by \Cref{eq:exact_cov_pfuy}, 
%has also smaller covariance than $p(\f | \bu)$. 
\end{remark}

%Finally, as we discuss in related work our bound is also better than the recent bound on the log determinant by \citet{artemevburt2021cglb}.  

\subsection{Predictions
\label{sec:predictions}
} 

To perform 
predictions we will be using 
the same predictive posterior 
from \Cref{eq:variational_posteriorGP}, i.e., 
$
q(\f_* | \y) =
\int p(\f_* | \bu) q(\bu) d \bu, 
$
where the optimal $q^*(\bu)$ (see \Cref{app:detailsSVGP}) 
is exactly the same as in 
the standard SVGP method. The alternative expression (and strictly speaking more appropriate 
since our variational approximation is $q(\f | \bu) q(\bu)$) is given by 
\begin{equation}
q_{high\_cost}(\f_* | \y) 
\! = \! \! \int p(\f_* | \f, \bu) q(\f |  \bu) q(\bu)  d \f d \bu. 
\end{equation}
But this is expensive since it has cost $\mathcal{O}(N^3)$. The reason is that $\int p(\f_* | \f, \bu) q(\f |  \bu) d \f$ does not simplify anymore since $q(\f |  \bu)$ 
is not the conditional GP, which 
means that $p(\f_* | \f, \bu)$ and $q(\f | \bu)$ are not consistent 
under the GP prior. 
Nevertheless, 
$q(\f_* | \y)$ and $q_{high\_cost}(\f_* | \y)$ have exactly the same mean,  since $q(\f | \bu)$ and $p(\f | \bu)$ have the 
same mean.
%but the tractable $q$ will give higher variances than  $q_{high\_cost}$. 

\subsection{Stochastic Minibatch Training
\label{sec:stochasticopt}}

The initial SVGP method \cite{titsias2009variational} does the training in a batch mode where all data are used in each optimization step. Stochastic optimization using minibatches was proposed by \citet{hensman2013gaussian}.  
Here, we apply our new approximation to this stochastic method. 

We start from  \Cref{eq:newcollapsedbound_with_vis},
and substitute only the optimal values for each $v_i$
without using the optimal setting for $q(\bu)$. This results in the uncollapsed
bound
\begin{align} 
& \sum_{i=1}^N \biggl\{  \E_{q(\bu)} [\log \mathcal{N}(y_i | \bk_{f_i \bu}
\bK_{\bu \bu }^{-1} \bu, \sigma^2 )]  \nonumber \\
& - \frac{1}{2}  \log\left(1+\frac{k_{ii} - q_{ii}}{\sigma^2} \right)  \biggr\} 
  - \text{KL}[q(\bu) || p(\bu)],
\label{eq:newuncollapsedbound}
\end{align}
where  $\bk_{f_i \bu}$ is the $1 \times M$ vector of all kernel 
values between the training input $\bx_i$ and the inducing inputs $\bZ$, while 
the expectation under $q(\bu)$ in the first line is 
analytic; see \citet{hensman2013gaussian}. %and \Cref{app:detailsNewbounds} for details.  
Then, we can apply stochastic gradient methods to optimize 
$q(\bu)$ and the hyperparameters  by subsampling 
data minibatches to deal with the  sum over the $N$ training points.  
Clearly, the above bound is strictly better than 
the previous uncollapsed bound in \citet{hensman2013gaussian},
since $- \frac{1}{2 \sigma^2} (k_{ii} - q_{ii}) \leq -\frac{1}{2} \log\left(1 + \frac{k_{ii} - q_{ii}}{\sigma^2} \right)$. 

The most common parametrization of $q(\bu)$
is $q(\bu) = \mathcal{N}(\bu | \m, \bS)$ where  the mean vector $\m$ and covariance matrix $\bS$ are    
variational parameters.  Another popular 
parametrization, for instance used as the default in GPflow \cite{GPflow17}, is the whiten 
parametrization that we consider in our experiments. %see \Cref{app:whiten} for a review.  
For any choice of
$q(\bu)$,  the above bound is always tighter than its corresponding 
 previous uncollapsed bound and requires minor modifications to existing 
 implementations.
% , i.e., to replace the previous  term $- \frac{1}{2 \sigma^2} (k_{ii} - q_{ii})$  by $-\frac{1}{2} \log\left(\frac{k_{ii} - q_{ii}}{\sigma^2} + 1 \right)$. 
        

\subsection{Non-Gaussian Likelihoods
\label{sec:nongaussian}}

Consider a factorized  likelihood $p(\y | \f) = \prod_{i=1}^N p(y_ i | f_i)$ 
where  $p(y_ i | f_i)$ is non-Gaussian, e.g., Bernoulli  for binary outputs  
or Poisson for counts.  
In this non-conjugate setting the sparse 
variational GP approximation imposes the same form for the variational distribution, i.e., $q(\f, \bu) = p(\f | \bu) q(\bu)$ 
where $p(\f | \bu)$ is the 
conditional GP prior. As shown in several works \cite{Chai12,hensman2015scalable,lloyd15,Dezfouli15,Sheth15}, this leads to the bound 
 \begin{equation}
 \sum_{i=1}^N 
\E_{q(f_i)} [\log p(y_i | f_i)]   - \text{KL}[q(\bu) || p(\bu)],
\label{eq:standard_nonconjugate_bound}
 \end{equation} 
 where $q(f_i) = \int p(\f  | \bu) q(\bu ) d \f_{-i} d \bu$ is the marginal  over
 $f_i = f(\bx_i)$  with respect to the approximate posterior $q(\f, \bu)$. Given 
 that  $q(\bu)$ is  Gaussian with mean $\m$ and covariance 
 $\bS$,  $q(f_i)$ can be computed fast in $\mathcal{O}(M^2)$ time (after precomputing the Cholesky factorization of
 $\bK_{\bu \bu}$) as follows 
 \begin{equation}
 q(f_i)  = \mathcal{N}(f_i | \bk_{f_i \bu} \bK_{\bu \bu}^{-1} \m, k_{ii} - q_{ii} + \bk_{f_i \bu} \bK_{\bu \bu}^{-1} \bS \bK_{\bu \bu}^{-1} \bk_{\bu f_i}). 
 \end{equation}
For the discussion next it is useful to observe that the efficiency when computing $q(f_i)$ comes from $p(\f | \bu)$ being a conditional GP prior, so 
expressing $p(f_i | \bu)$ is trivial. 

Suppose now that we wish to impose the more structured variational 
approximation $q(\f, \bu) = q(\f | \bu)  q(\bu)$ where
$q(\bu) = \mathcal{N}(\bu | \m, \bS)$ and $q(\f | \bu)$ 
is given by 
\Cref{eq:qfu}. The bound %(see \Cref{app:nonGaussian}) 
can be written as
\begin{align}
& \sum_{i=1}^N 
\E_{q(f_i)} [\log p(y_i | f_i)] - 
 \frac{1}{2} \sum_{i=1}^N (v_i - \log v_i - 1)
\nonumber \\
& - \text{KL}[q(\bu) || p(\bu)],
\label{eq:nonGaussian_bound_intractable}
\end{align}
where we used the fact that 
$\text{KL}[q(\f|\bu) || p(\f|\bu)]$ is obtained from  \Cref{lem:KLqfupfu}. The above bound
is not computationally efficient since 
the marginal $q(f_i) = \int q(\f  | \bu) q(\bu ) d \f_{-i} d \bu$ 
has $\mathcal{O}(N^3)$ cost. This  is because
the marginalization $q(f_i | \bu) = \int q(\f  | \bu) d \f_{-i}$ cannot be trivially expressed, due to the complex structure of the covariance
$(\bK_{\f \f} - \bQ_{\f \f})^{\frac{1}{2}} \bV
(\bK_{\f \f} - \bQ_{\f \f})^{\frac{1}{2}}$ in $q(\f | \bu)$. To overcome this,  we will use a simplified version of $q(\f | \bu)$, in which we choose a spherical $\bV = v \bI$ with $v > 0$. Then, things become tractable. 

\begin{proposition} Let \emph{$q(\f|\bu) = \mathcal{N}(\f | \bK_{\f \bu} \bK_{\bu \bu}^{-1} \bu, v (\bK_{\f \f} - \bQ_{\f \f}))$} for \emph{$v>0$}. Then (\ref{eq:nonGaussian_bound_intractable}) is computed in \emph{$\mathcal{O}(N M^2)$} time as 
\emph{\begin{align}
& \sum_{i=1}^N 
\E_{q(f_i)} [\log p(y_i | f_i)] -  
 \frac{N}{2} (v - \log v - 1)
\nonumber \\ & - \text{KL}[q(\bu) || p(\bu)],
\label{eq:nonGaussian_bound_tractable}
\end{align}}

\noindent where the marginal is \emph{$q(f_i)  = \mathcal{N}(f_i | \bk_{f_i \bu} \bK_{\bu \bu}^{-1} \m, v (k_{ii} - q_{ii}) + \bk_{f_i \bu} \bK_{\bu \bu}^{-1} \bS \bK_{\bu \bu}^{-1} \bk_{\bu f_i})$}. 
\end{proposition}
%\begin{remark}
The parameter $v$ multiplies the term 
$k_{ii} - q_{ii}$  inside the variance of 
$q(f_i)$, and it also appears in the regularization term
$-\frac{N}{2} (v - \log v - 1)$. If $v=1$ the bound 
in (\ref{eq:nonGaussian_bound_tractable}) reduces to 
(\ref{eq:standard_nonconjugate_bound}), while by
optimizing over $v$ it can become a tighter bound. 
The optimization 
of $v$ is done jointly  
with the remaining parameters $\m,\bS, \bZ, \theta$ using gradient-based methods. Stochastic gradients can also be used 
by subsampling minibathes 
to deal with the sum  
$\sum_{i=1}^N 
\E_{q(f_i)} [\log p(y_i | f_i)]$.
%and reduce the complexity to $\mathcal{O}(M^3)$. 
%\end{remark}
