
\section{Further details about the SVGP method
\label{app:detailsSVGP}
}


We give a brief overview of the derivation of the standard collapsed bound 
in \Cref{eq:collapsedbound_old}. 
Some steps of the derivation will also be instructive for proving the 
main results of this paper in 
\Cref{app:detailsNewbounds}.  

Given the variational distribution
$q(\f,\bu) = p(\f| \bu) q(\bu)$  the lower bound is   
\begin{align} 
\log p(\y) & \geq 
\int p(\f|\bu) q(\bu)
\log \frac{p(\y | \f) \cancel{p(\f|\bu)}  p(\bu)}
{\cancel{p(\f|\bu)} q(\bu)} d \f d \bu \nonumber \\ 
& = 
\int p(\f|\bu) q(\bu)
\log \frac{p(\y | \f) p(\bu)}
{q(\bu)} d \f d\bu \nonumber  \\
& =  
\int q(\bu) \left\{
 \int p(\f|\bu)
\log p(\y |\f) d \f + 
\log \frac{p(\bu)} {q(\bu)} \right\} d \bu \nonumber \\ 
& = \int q(\bu) \log \frac{\exp\{ \int p(\f | \bu) \log p(\y | \f) d \f\} p(\bu)}{q(\bu)} d \bu.
\label{eq:appendixbound1}
\end{align}
The expectation $\int p(\f|\bu)
\log p(\y |\f) d \f$ can be computed 
as
\begin{align}
\int p(\f|\bu)
\log p(\y |\f) d \f & = 
\int p(\f|\bu) 
 \log p(\y | \f) d \f \nonumber \\ 
& = \int p(\f|\bu)\left\{
- \frac{N}{2}\log(2 \pi \sigma^2) 
- \frac{1}{2 \sigma^2} \text{tr}\left[ 
\y \y^T 
- 2 \y \f^T 
+ \f \f^T 
\right] \right\} d \f \nonumber \\
& =  
- \frac{N}{2}\log(2 \pi \sigma^2) 
- \frac{1}{2 \sigma^2} \text{tr}\left[ 
\y \y^T 
- 2 \y (\bK_{\f \bu} \bK_{\bu \bu}^{-1}\bu)^\top 
+ (\bK_{\f \bu} \bK_{\bu \bu}^{-1}\bu) (\bK_{\f \bu} \bK_{\bu \bu}^{-1}\bu)^\top + \textcolor{blue}{\bK_{\f \f} - \bQ_{\f \f}} \right] \nonumber \\  
& = \log \left[ 
\mathcal{N}(\y|\bK_{\f \bu} \bK_{\bu \bu}^{-1}\bu,\sigma^2 \bI) \right]  
- \frac{1}{2\sigma^2} \text{tr}(\bK_{\f \f} - \bQ_{\f \f}). 
\label{eq:appendixpfulogpyf}
\end{align}
where we highlighted with blue 
a term in the third line to contrast it 
with a similar term when proving \Cref{lem:Expqfu_loglik} in \Cref{app:artemevbound}. The ELBO in \Cref{eq:appendixbound1} 
is written as
\begin{align}
 \log p(\y) & \geq 
\int q(\bu)
\log \frac{ \mathcal{N}(\y | \bK_{\f \bu} \bK_{\bu \bu}^{-1}\bu ,\sigma^2 \bI) p(\bu)}
{q(\bu)} d \f  - \frac{1}{2 \sigma^2} \text{tr}(\bK_{\f \f} - \bQ_{\f \f}).
\label{eq:appendixBound2}
\end{align} 
By maximizing this bound wrt the distribution $q(\bu)$ we obtain the
optimal $q^*$:
\begin{align}
\label{eq:appendixOptimalqu1}
q^*(\bu) & = \frac{ \mathcal{N}(\y |\bK_{\f \bu} \bK_{\bu \bu}^{-1}\bu,\sigma^2
  \bI) p(\bu)} {\int \mathcal{N}(\y | \bK_{\f \bu} \bK_{\bu \bu}^{-1}\bu,\sigma^2
  \bI) p(\bu) d \bu} 
  = \frac{ \mathcal{N}(\y |\bK_{\f \bu} \bK_{\bu \bu}^{-1}\bu,\sigma^2
  \bI) p(\bu)} {\mathcal{N}(\y | {\bf 0}, \bQ_{\f \f}  + \sigma^2 
  \bI)} \\ 
 & =  
 \mathcal{N}(\bu | \sigma^{-2} \bK_{\bu \bu} (\bK_{\bu \bu} + \sigma^{-2} \bK_{\bu \f} \bK_{\f \bu})^{-1} \bK_{\bu \f} \y, 
\bK_{\bu \bu} (\bK_{\bu \bu} + \sigma^{-2} \bK_{\bu \f} \bK_{\f \bu})^{-1} \bK_{\bu \bu}), 
\end{align}
 where the expression in the second line (obtained after some standard 
 completion of a square procedure) shows that $q^*(\bu)$ can be computed in $\mathcal{O}(N M^2)$ time. In fact, this optimal $q^*(\bu)$
 is the same as the one obtained by the DTC (also known as projected process) approximation \cite{seeger-williams-lawrence-03a,candela-rasmussen-05}. By substituting 
 the expression in (\ref{eq:appendixOptimalqu1}) into the bound in (\ref{eq:appendixBound2})
 we obtain the well-known formula of the collapsed bound: 
 \begin{align}
 \log p(\y) & \geq 
\log \mathcal{N}(\y |{\bf 0}, \bQ_{\f \f}  + \sigma^2 \bI)   - \frac{1}{2 \sigma^2} \text{tr}(\bK_{\f \f} - \bQ_{\f \f}).
\label{eq:appendixBound3}
\end{align} 
Given the Gaussian
form of $q(\bu) = \mathcal{N}(\bu|\bfmu, \bA)$ the posterior GP  
is given by $q(\f_*) = \int p(\f_* | \bu) q(\bu) d \bu$:
\begin{align}
q(\f_*) = \mathcal{N}(\f_* | 
\bK_{\f_* \f} \bK_{\bu \bu}^{-1}\bfmu,  \bK_{\f_* \f_*} 
- \bK_{\f_* \bu} \bK_{\bu \bu}^{-1} \bK_{\bu \f_*}  +   \bK_{\f_* \bu} \bK_{\bu \bu}^{-1} \bA \bK_{\bu \bu}^{-1}  \bK_{\bu \f_*} )
\end{align}
which further simplifies if substitute the optimal mean and covariance of $q^*$:
\begin{align}
q(\f_*) = \mathcal{N}(\f_* | 
\bK_{\f_* \f} \bLambda^{-1} \bK_{\bu \f} \frac{\y}{\sigma^2},  \bK_{\f_* \f_*} 
- \bK_{\f_* \bu} \bK_{\bu \bu}^{-1} \bK_{\bu \f_*}  +   \bK_{\f_* \bu}  \bLambda^{-1} \bK_{\bu \f_*} )
\end{align}
where $\bLambda = \bK_{\bu \bu} + \sigma^{-2} \bK_{\bu \f} \bK_{\f \bu}$. 



\section{Further details about the proposed bounds
\label{app:detailsNewbounds}
}

Here, we provide several proofs
regarding the proposed bounds. 


\subsection{Proof of \Cref{lem:KLqfupfu}} 

$q(\f | \bu)$ 
and $p(\f | \bu)$ are Gaussian distributions having the same mean but different covariance matrices. Thus, 
the KL divergence reduces to 
\begin{align}
\text{KL}[q(\f | \bu) || p(\f | \bu)] 
& = \frac{1}{2} \left\{ 
\log \frac{|\bK_{\f \f} - \bQ_{\f \f}|}{|(\bK_{\f \f} - \bQ_{\f \f})^{\frac{1}{2}} \bV (\bK_{\f \f} - \bQ_{\f \f})^{\frac{1}{2}}|}  
- N + 
\tr\{ (\bK_{\f \f} - \bQ_{\f \f})^{-1}  (\bK_{\f \f} - \bQ_{\f \f})^{\frac{1}{2}} \bV  (\bK_{\f \f} - \bQ_{\f \f})^{\frac{1}{2}} \}  \right\} \nonumber \\ 
& = \frac{1}{2} \left\{ 
- \log | \bV|  
- N + 
\tr\{\bV  \}  \right\},
\end{align}
where all the terms involving 
$\bK_{\f \f} - \bQ_{\f \f}$ cancel out by using standard properties of the 
matrix determinant and trace. Now since $\bV$ is a diagonal matrix (with diagonal elements $v_i>0$) we
conclude that 
$\text{KL}[q(\f | \bu) || p(\f | \bu)] 
= \frac{1}{2} \sum_{i=1}^N (v_i - \log v_i - 1)$. 

\subsection{Proof of \Cref{lem:Expqfu_loglik}}

The derivation of $\int q(\f | \bu) \log p(\y | \f) d \f$ is 
similar to the derivation in \Cref{eq:appendixpfulogpyf} 
with a small difference highlighted  in blue:
\begin{align}
\int p(\f|\bu)
& \log p(\y |\f) d \f \nonumber \\  
& = - \frac{N}{2}\log(2 \pi \sigma^2) 
- \frac{1}{2 \sigma^2} \text{tr}\left[ 
\y \y^T 
- 2 \y (\bK_{\f \bu} \bK_{\bu \bu}^{-1}\bu)^\top 
+ (\bK_{\f \bu} \bK_{\bu \bu}^{-1}\bu) (\bK_{\f \bu} \bK_{\bu \bu}^{-1}\bu)^\top + \textcolor{blue}{(\bK_{\f \f} - \bQ_{\f \f})^{\frac{1}{2}} \bV (\bK_{\f \f} - \bQ_{\f \f})^{\frac{1}{2}}}   \right] \nonumber \\  
& = \log \left[ 
\mathcal{N}(\y|\bK_{\f \bu} \bK_{\bu \bu}^{-1}\bu,\sigma^2 \bI) \right]  
- \frac{1}{2\sigma^2} \text{tr}(\bV (\bK_{\f \f} - \bQ_{\f \f})), 
\label{eq:appendixqfulogpyf}
\end{align}
where we used that $\text{tr}((\bK_{\f \f} - \bQ_{\f \f})^{\frac{1}{2}} \bV (\bK_{\f \f} - \bQ_{\f \f})^{\frac{1}{2}}) = \text{tr}(\bV (\bK_{\f \f} - \bQ_{\f \f}))
$. Now since $\bV$ is a diagonal matrix 
we have $\text{tr}(\bV (\bK_{\f \f} - \bQ_{\f \f})) = \sum_{i=1}^N v_i (k_{ii} - q_{ii})$ which completes the proof. 

\subsection{Proof of \Cref{prop:newbound}
}

The ELBO is written as 
$$ 
 \log p(\y) \geq \int q(\f | \bu) q(\bu) \log \frac{p(\y | \f) p(\f | \bu) p(\bu)}{q(\f | \bu) q(\bu)} =  \int \! \!  q(\bu) \! \left\{ \! \log \frac{\exp\{\E_{q(\f | \bu)}[\log p(\y | \f)]\} p(\bu)}{q(\bu)} \! - \! \text{KL}[q(\f | \bu) || p(\f | \bu)] 
\! \right\} \! d \bu 
$$
and by using the results from 
the two lemmas this becomes 
$$ 
 \log p(\y) \geq \int  q(\bu) \log \frac{  \mathcal{N}(\y | \bK_{\f \bu}
\bK_{\bu \bu }^{-1} \bu, \sigma^2 \bI) p(\bu)}{q(\bu)}  d \bu  - \frac{1}{2} 
\sum_{i=1}^N \left\{  v_i \left(1 + \frac{k_{ii} - q_{ii}}{\sigma^2}\right) - \log v_i -1 \right\}.  
$$
Clearly, maximizing over 
$q(\bu)$ gives the same optimal distribution as in \Cref{eq:appendixOptimalqu1}, and the first term in the bound is the DTC log likelihood. The second term that depends on the $v_i$s is a concave function over these parameters. Thus, by differentiating and setting to zero we obtain the optimal values $v_i^* = \left(1 + \frac{k_{ii} - q_{ii}}{\sigma^2}\right)^{-1}$. If we plug these values back into the bound we obtain the new tighter collapsed bound in \Cref{prop:newbound}.

\subsection{Reinterpretation of \citet{artemevburt2021cglb}'s bound
\label{app:artemevbound}
}

We consider the following form  of $q(\f | \bu)$: 
$$
q(\f|\bu) = \mathcal{N}(\f | \bK_{\f \bu} \bK_{\bu \bu}^{-1} \bu, v (\bK_{\f \f} - \bQ_{\f \f})).
$$
Then, $\text{KL}[q(\f | \bu) || p(\f | \bu)] 
= \frac{N}{2} (v - \log v - 1)$
and $ \E_{q(\f | \bu)}[\log p(\y | \f)] \nonumber \\ 
 = \log \mathcal{N}(\y | \bK_{\f \bu}
\bK_{\bu \bu }^{-1} \bu, \sigma^2 \bI)
- \frac{v}{2 \sigma^2}  \text{tr}(\bK_{\f \f} - \bQ_{\f \f})$ and the bound is written as 
$$ 
 \log p(\y)  \geq \int \! \!  q(\bu) \log \frac{  \mathcal{N}(\y | \bK_{\f \bu}
\bK_{\bu \bu }^{-1} \bu, \sigma^2 \bI) p(\bu)}{q(\bu)}  d \bu - \frac{1}{2}  \left\{  \frac{v}{\sigma^2} \text{tr}(\bK_{\f \f} - \bQ_{\f \f}) +  N (v  -  \log v -1) \right\}.
$$
By maximizing wrt $v$ we obtain $v^* = \left( 1 + \frac{\text{tr}(\bK_{\f \f} - \bQ_{\f \f})}{N \sigma^2} \right)^{-1}$, and by substituting this back into the bound we obtain  \citet{artemevburt2021cglb}'s tighter bound on the initial trace regularization term. Overall this collapsed bound has the form
\begin{equation} 
\log p(\y) \geq \log  \mathcal{N}(\y |{\bf 0},   \bQ_{\f \f} + \sigma^2 \bI) 
 - \frac{N}{2}  \log \left(\ 1 + \frac{\text{tr}(\bK_{\f \f} - \bQ_{\f \f})}{N \sigma^2}  \right).   
\label{eq:artemvecollapsedbound}
\end{equation}
This collapsed bound is what the method
SGPR-artemev is using in \Cref{sec:mediumregress}. Note that 
\citet{artemevburt2021cglb} propose also additional but more expensive bounds for the first DTC log likelihood term that require running conjugate gradients. 
We do not consider those in our comparisons (such bounds could be used in all SGPR bounds since all share the same DTC log likelihood term) as they have higher cost.  

%\section{Non-Gaussian %likelihoods\label{app:nonGaussian}
%}


\subsection{Bound for the Bayesian GP-LVM
\label{app:bgplvm}
}

Due to the strong similarity of the 
standard collapsed SVGP bound in \Cref{eq:collapsedbound_old} with the collapsed 
bound in the Bayesian 
GP-LVM \cite{titsias10a}, applying the new approximation to Bayesian GP-LVM 
seems to be simple and we discuss it next.  

Given observed data $Y \in \Real^{N \times D}$
and latent variables 
$X \in \Real^{N \times Q}$ we have the latent variable model 
$$
p(Y|X) p(X) 
= \left(\prod_{d=1}^D p(\y_d | X) \right) p(X),
$$
where $p(X)$ is a Gaussian prior over the latent variables and 
$p(\y_d | X) 
= \mathcal{N}(\y_d | \bK_{\f \f}(X) + \sigma^2 \bI)$. 
Given a Gaussian variational distribution 
$q(X)$ over the latent variables, the initial form of the bound is 
\begin{align}
F & = \int q(X) \log p(Y|X) d X - \text{KL}[q(X) || p(X)] 
\nonumber \\
& = \sum_{d=1}^D \int q(X) \log p(\y_d|X) d X - \text{KL}[q(X) || p(X)] \nonumber \\
& = \sum_{d=1}^D F_d  - \text{KL}[q(X) || p(X)], 
\end{align}
where $F_d = \int q(X) \log p(\y_d|X) d X$. The KL part will be a tractable KL between two Gaussians, and thus the difficulty is to approximate $F_d$. 
Given that 
$\log p(\y_d | X)$ has the same form with the log marginal likelihood in GP regression, we can lower bound it using inducing variables and exactly the same form of $q(\f|\bu)$ as we did in the main paper. This gives
$$
\log p(\y_d | X) 
\geq  \int q(\bu) \log \frac{  \mathcal{N}(\y_d | \bK_{\f \bu}(X)
\bK_{\bu \bu }^{-1} \bu, \sigma^2 \bI) p(\bu)}{q(\bu)}  d \bu  - \frac{1}{2} 
\sum_{i=1}^N \left\{  v_i \left(1 + \frac{k(\bx_i, \bx_i) - q(\bx_i,\bx_i)}{\sigma^2}\right) - \log v_i -1 \right\},  
$$
where $\bx_i$ is the latent variable for the $i$-th data point and  $q(\bx_i, \bx_i) = \bk(x_i)^\top \bK_{\bu \bu}^{-1} \bk(\bx_i) = \text{tr}\{\bK_{\bu \bu}^{-1} \bk(\bx_i) \bk(\bx_i)^\top\}$. Note that we write the cross  
kernel matrix as 
$\bK_{\f \bu}(X)$ to emphasize its dependence on the latent variables $X$, while $\bK_{\bu \bu}$ does not depend on $X$. Note also that we assume that each $v_i$ parameter does not depend on $\bx_i$ and this is crucial to obtain a closed form collapsed bound. Now we follow 
the derivation in the initial Bayesian GP-LVM
where we use the 
above bound to replace 
$\log p(\y_d|X)$ in 
 $\int q(X) \log p(\y_d|X) d X$ and do first the expectation 
 over $X$, and then solve for the optimal $q(\bu)$. This eliminates $q(\bu)$ and it gives the bound 
 $$
 \int e^{\langle \mathcal{N}(\y_d | \bK_{\f \bu}(X)
\bK_{\bu \bu }^{-1} \bu, \sigma^2 \bI)\rangle_{q(X)}} p(\bu) d \bu 
 - \frac{1}{2} 
\sum_{i=1}^N \left\{  v_i \left(1 + \frac{\langle k(\bx_i, \bx_i)\rangle_{q(\bx_i)} -  \langle q(\bx_i,\bx_i) \rangle_{q(\bx_i)}}{\sigma^2}\right) - \log v_i -1 \right\}. 
$$
where we  have used physics notation for expectation, i.e.,  
$\langle \cdot \rangle$. 
For $v_i=1$ this is the previous collapsed  bound used by Bayesian GP-LVM. By maximizing over each $v_i$ we obtain the new collapsed bound 
$$
F_d \geq \int e^{\langle \mathcal{N}(\y_d | \bK_{\f \bu}(X)
\bK_{\bu \bu }^{-1} \bu, \sigma^2 \bI)\rangle_{q(X)}} p(\bu) d \bu 
 - \frac{1}{2} 
\sum_{i=1}^N \log \left(1 + \frac{\langle k(\bx_i, \bx_i)\rangle_{q(\bx_i)} -  \langle q(\bx_i,\bx_i) \rangle_{q(\bx_i)}}{\sigma^2}\right), 
$$
which can be substituted in the overall Bayesian GP-LVM bound above. Again the implementation of the new bound requires a minor modification to existing 
Bayesian GP-LVM code. 


\section{Further experimental details and results
\label{app:furtherresults}
} 

For all regression experiments 
(apart from the toy Snelson 1-D dataset) 
we repeat the runs for five times using different random training and test splits. 
By following \citet{wang2019exact} and \citet{shietal2020} we consider 
80\% / 20\% training / test splits. 
A 20\% subset of the training set is used for validation. 

The training inputs and regression outputs are normalized to have zero mean. For the hyperparameters $\sigma^2, \sigma_f^2, \ell^2$ (or $\ell_i^2$ for ARD kernels) we use the softplus activation to parametrize the square roots of these parameters, i.e., 
to parametrize $\sigma, \sigma_f, \ell_i$. For all experiments we use 
the initializations $\sigma = 0.51$, 
$\sigma_f = 0.69$, $\ell_i=1.0$.  
The inducing inputs $\bZ$ are initialized by running at maximum 30 iterations of k-means clustering with the centers initialized at a random training data subset.  

%All experiments were performed in a V100 GPU.  

\subsection{Medium size regression datasets
\label{app:mediumsizeRegress}
}

For all three datasets in 
this section we can run Exact
GP given the medium training size.
For the Pol dataset the training size 
is $N=9600$ and input dimensionality  
$d=26$. For Elevators is $N=10623$ 
and $d=18$. For the Bike dataset
the initial train size (see e.g., 
Table 7 in \citet{shietal2020}) 
is $N = 11122$ (with $d = 17$) 
but since Exact GP training gave out-of-memory error when running in a V100 GPU, we had to slightly reduce the training size to $N=10600$.  

An mentioned in the main paper the standard squared exponential ARD kernel 
was used in all experiments in this 
section. For training, we perform $10000$ optimization iterations using the Adam optimizer with base learning $0.01$. 


\Cref{fig:mediumsize2048} shows the objective function values and noise variance parameter $\sigma^2$  across iterations when the SGPR methods use $M=2048$ inducing points.  \Cref{fig:mediumsize1024} in the main paper shows the result for $M=1024$.


% Elevators 
% train (10623, 18)
% test (3320, 18)

% Bike
% train (10600, 17) this was reduced from 11122  
% test (3476, 17)

% Poll
% train (9600, 26)
% test (3000, 26)
       

\begin{figure*}[t]
\centering
\begin{tabular}{ccc}
\includegraphics[scale=0.25]
{smallscale_elbo_pol_2048.pdf} &
\includegraphics[scale=0.25]
{smallscale_elbo_bike_2048.pdf} &
\includegraphics[scale=0.25]
{smallscale_elbo_elevators_2048.pdf} \\
\includegraphics[scale=0.25]
{smallscale_sigma2_pol_2048.pdf} &
\includegraphics[scale=0.25]
{smallscale_sigma2_bike_2048.pdf} &
\includegraphics[scale=0.25]
{smallscale_sigma2_elevators_2048.pdf} \\
\end{tabular}
\caption{The two plots in each column correspond to the same dataset: first row shows the ELBO (or log-likelihood)
 for all four methods (Exact GP, SGPR, SGPR-new and SGPR-artemev) with the number of iterations and the plot in the second row shows the
  corresponding values for $\sigma^2$. SGPR methods use $M=2048$ inducing points initialized by k-means. For each line we plot the mean and standard error
  after repeating the experiment five times with different train-test dataset splits.
}
\label{fig:mediumsize2048}
\end{figure*}



%\begin{table}[t]
%  \caption{Test log-likelihood values for the medium size  regression datasets. The numbers in parentheses are standard errors.  The SGPR methods used $M=1024$ inducing points.}
%\label{table:smalldatasets1024}
%\vskip 0.15in
%%\begin{small}
%\begin{center}
%  \begin{sc}
%\begin{tabular}{lcccr}
%\toprule
%& Pol  & Bike & Elevators \\
%\midrule
%Exact GP & $1.089(0.011)$ & %$3.105(0.022)$ & $-0.386(0.001)$ \\
%Exact GP & $1.089(0.011)$ & %$3.105(0.022)$ & $-0.386(0.001)$  \\
%SGPR-trace & $0.821(0.008)$ & %$2.176(0.020)$ & $-0.387(0.001)$\\
%SGPR-trace & $0.958(0.008)$  & %$2.337(0.030)$ & $-0.387(0.001)$ \\
%SGPR-log & $0.920(0.006)$ & %$2.326(0.026)$  & $-0.387(0.001)$  \\
%SGPR-log & $0.998(0.008)$  & %$2.511(0.021)$ & $-0.387(0.001)$ \\
%\bottomrule
%\end{tabular}
%\end{sc}
%\end{small}
%\end{center}
%\vskip -0.1in
%\end{table}


\subsection{Large scale regression datasets
\label{app:largescaleRegress}
}

The experimental settings are chosen to match the ones from \citet{wang2019exact} and \citet{shietal2020}, where we used GPs with a Mat{\'e}rn32 kernel (with common lengthscale). Following these settings, for all datasets we train for 100 epochs using Adam with learning rate 0.01 and mini-batch size 1024.

\Cref{table:largescaleRMSE} 
reports RMSE scores, while test
log likelihood scores are given in
\Cref{table:largescaleTestLL} of the main paper. 

\begin{table*}[htbp] \vskip \baselineskip
\caption{Test RMSE values of large scale regression datasets with standard errors in parentheses. Best mean values are highlighted.} %Uses random 80\% / 20\% training and test splits, repeated 5 times. }
\label{table:largescaleRMSE}
\makebox[\textwidth][c]{
\resizebox{1.02\textwidth}{!}{
\setlength\tabcolsep{2pt}
\begin{tabular}{ l l cc cc cc cc}
\toprule
& &  Kin40k & Protein &  \footnotesize KeggDirected &  KEGGU & 3dRoad & Song & Buzz & \footnotesize HouseElectric \\
\cmidrule{3-10}
& $N$ & 25,600 & 29,267 & 31,248 & 40,708 & 278,319 & 329,820 & 373,280 & 1,311,539  \\
& $d$ & 8 & 9 & 20 & 27 & 3 & 90 & 77 & 9  \\
\midrule
%\multirow{2}{*}{SVGP} & $1024$ & 0.193(0.001) & 0.630(0.004) & 0.098(0.003) & {\bf 0.123}(0.001) & 0.482(0.001) & 0.797(0.001) & 0.263(0.001) & 0.063(0.000) \\
%& $1536$ & 0.182(0.001) & 0.621(0.004) & 0.098(0.002) & {\bf 0.123}(0.001) & 0.470(0.001) & 0.797(0.001) & 0.263(0.001) & 0.063(0.000) \\
%\midrule
From \citet{shietal2020} \\
ODVGP & $1024+1024$ 
& 0.183(0.001) & 0.625(0.004) & 0.176(0.012) & 0.156(0.004) & 0.467(0.001) & 0.797(0.001) & 0.263(0.001) & 0.062(0.000) \\
& $1024+8096$  
& 0.180(0.001) & 0.618(0.004) & 0.157(0.009) & 0.157(0.004) & 0.462(0.002) & 0.797(0.001) & 0.263(0.001) & 0.062(0.000) \\
%\midrule
SOLVE-GP & $1024 + 1024$ 
& 0.172(0.001) & 0.618(0.004) & 0.095(0.002) & 0.123(0.001) & 0.464(0.001) & 0.796(0.001) & 0.261(0.001) &  0.061(0.000) \\
%\midrule
%\multirow{1}{*}{\shortstack{SVGP}}
% \\
%& $2048$ & 0.177(0.001) & {\bf 0.615}(0.004) & 0.100(0.003) & 0.124(0.001) & 0.467(0.001) & {\bf 0.796}(0.001) & 0.263(0.000) & 0.063(0.000) \\
\midrule
SVGP [ours] & 1024 & $0.195(0.001)$ & $0.635(0.004)$ & $0.086(0.001)$ & $0.122(0.001)$ & $0.486(0.002)$ & $0.797(0.001)$ & $0.261(0.001)$ & $0.059(0.000)$ \\
      & 2048 & $0.171(0.000)$ & $0.619(0.004)$ & $0.086(0.001)$ & ${\bf 0.121}(0.001)$ & ${\bf 0.460}(0.002)$ & ${\bf 0.795}(0.001)$ & $0.260(0.001)$ & ${\bf 0.057}(0.000)$ \\
SVGP-new [ours]  & 1024 & $0.182(0.001)$ & $0.631(0.004)$ & $0.086(0.001)$ & $0.122(0.001)$ & $0.484(0.001)$ & $0.796(0.001)$ & $0.259(0.001)$ & $0.058(0.000)$ \\
     & 2048 & ${\bf 0.158}(0.000)$ & ${\bf 0.615}(0.004)$ & $0.086(0.001)$ & ${\bf 0.121}(0.001)$ & $0.461(0.002)$ & ${\bf 0.795}(0.001)$ & ${\bf 0.258}(0.000)$ & ${\bf 0.057}(0.000)$ \\
\bottomrule 
\end{tabular}
}}
\vskip \baselineskip
\end{table*}


\subsection{Poisson regression
\label{app:poisson}}

\Cref{fig:app_poisson} 
shows the ELBOs across iterations for the NYBikes 
dataset for $M=8$ and $M=32$ inducing points, while 
the plot for $M=16$ is shown in the main paper. 
\Cref{table:poisson_nybikes} presents test log-likelihood scores for the NYBikes dataset. Average values and standard errors  are computed by repeating the experiment five times
where at each repeat we randomly split the initial data into $90\%$ for training and $10\%$ for test.


\begin{figure*}
\centering
\begin{tabular}{cc}
\includegraphics[scale=0.3]
{poisson_elbo_nybicycle_8.pdf} &
\includegraphics[scale=0.3]
{poisson_elbo_nybicycle_16.pdf} %&
%\includegraphics[scale=0.24]
%{poisson_toy_prediction_log.pdf}% \\
\end{tabular}
\caption{The % posterior 
left panel shows the lower bounds 
across iterations when the
sparse GP methods run with $M=8$ inducing points, 
while the right panel shows the corresponding plot with $M=32$ inducing points.}
\label{fig:app_poisson}
\end{figure*}


%\begin{figure*}
%\centering
%\begin{tabular}{ccc}
%\includegraphics[scale=0.24]
%{poisson_toy_prediction_exact.pdf} &
%\includegraphics[scale=0.24]
%{poisson_toy_prediction_trace.pdf} &
%\includegraphics[scale=0.24]
%{poisson_toy_prediction_log.pdf} \\
%(a) & (b) & (c)
%\end{tabular}
%\caption{({\bf left}) %shows the % posterior 
%predictions (means with 2-standard deviations) over counts (black dots) in the artificial data example  after  fitting the Full GP, and the two SVGPs. This plot superimposes all predictions in order to provide a comparative visualization; see \Cref{app:poisson} for individual plots. ({\bf middle})  shows the ELBO across optimization steps. ({\bf right}) shows the ELBO for the real NYBikes dataset.}
%\label{fig:poisson}
%\end{figure*}


\begin{table}[htbp] \vskip \baselineskip
\caption{Test log likelihoods on the NYCBikes  Poisson regression dataset  with standard errors in parentheses. For the 
sparse methods we consider 
varying numbers of inducing points, i.e., $M=8,16,32$.
} %Uses random 80\% / 20\% training and test splits, repeated 5 times. }
\label{table:poisson_nybikes}
\centering
 \begin{tabular}{ l l cc cc cc}
\toprule
Full GP &  & $-5.061(0.010)$ \\
\midrule
SVGP & 8 & $-36.397(6.017)$ \\
SVGP & 16 & $-16.557(4.307)$ \\
%SVGP & 24 & $-10.471(0.526)$ \\
SVGP & 32 & $-8.556(0.728)$ \\
\midrule
SVGP-new & 8 & $-9.713(0.345)$ \\
SVGP-new & 16 & $-9.301(0.296)$ \\
%SVGP-new & 24 & $-9.259(0.486)$ \\
SVGP-new & 32 & $-8.203(0.190)$ \\
\bottomrule 
\end{tabular}
\vskip \baselineskip
\end{table}