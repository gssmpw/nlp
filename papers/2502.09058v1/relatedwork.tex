\section{Related Work}
\textbf{Denoising in Recommendation.}
Recommendation typically treat observed interactions as positive and unobserved ones as negative in implicit feedback~\cite{ding2019sampler,gao2022self}. However, this approach can incorporate erroneous clicks or biased behaviors, leading to false positives and negatives that degrade user experience~\cite{sun2021does}. Existing denoising methods are generally categorized as follows:
\textbf{1) Selection-Based Methods}: These methods~\cite{gantner2012personalized,wen2019leveraging} filter out noisy feedback while retaining clean data. Early approaches~\cite{nguyen2019self,hu2008collaborative} use samplers based on data characteristics, whereas adaptive strategies later identify unreliable instances by detecting significant loss early in training. Recent techniques~\cite{lin2023autodenoise} employ deep reinforcement learning for effective noise removal. DCF~\cite{he2024double} uses a dual-correction framework to identify noise through changes in sample loss over time.
\textbf{2) Re-weighting-Based Methods}: This approach assigns higher weights to informative interactions. Initial methods~\cite{wang2022learning,wang2021implicit} utilize training loss to assign lower weights to high-loss samples. Recent works like DeCA~\cite{wang2022learning} and BOD~\cite{wang2023efficient} have introduced novel evaluation criteria and optimization strategies for more accurate weight learning.
\textbf{3) Side-Information-Based Methods and Special Strategies}~\cite{gao2022self,wu2021self,yang2021enhanced}: Early approaches~\cite{buscher2009segment,fu2010towards,zhao2016gaze} utilize dwell time and annotations to detect noise. \cite{zhang2024ssdrec,han2024efficient,xin2023improving} incorporate sequential or multi-behavior data to capture unexpected interactions. \cite{zhu2023knowledge,wang2024unleashing} have employed knowledge graphs to enhance preference modeling, facilitating denoising frameworks. 
In addition, there are some studies that learn robust representations by designing special denoising strategies. Early work~\cite{wu2016collaborative,khawar2020learning,strub2015collaborative} employ autoencoders to reduce noise in representations. \cite{wu2021self,yu2022graph} leverage self-supervised learning on graph-structured data for greater stability. 
Despite their effectiveness, existing methods rely heavily on observed data and predefined assumptions to model user preferences and distinguish noise. In contrast, our approach leverages LLMs to acquire denoising knowledge, extracting inferred preference and relational semantics to capture noise interactions.

\vspace{-2mm}