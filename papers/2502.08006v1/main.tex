
\documentclass{article} %
\usepackage{iclr2025_conference,times}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{xspace}
\usepackage{booktabs} %
\usepackage{svg}



\usepackage{hyperref}
\usepackage{url}


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{bm}

\usepackage{pifont}%
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\newcommand*{\R}{\mathbb{R}}
\newcommand*{\X}{\mathcal{X}}
\newcommand*{\Y}{\mathcal{Y}}
\newcommand*{\B}{\mathcal{B}}
\newcommand*{\T}{\mathbb{T}}
\newcommand*{\N}{\mathbb{N}}
\newcommand*{\Z}{\mathcal{Z}}
\newcommand*{\C}{\mathcal{C}}
\newcommand*{\F}{\mathcal{F}}
\newcommand*{\Q}{\mathbb{Q}}
\newcommand*{\pr}{\mathbb{P}}
\newcommand*{\bfx}{{\bm{x}}}
\newcommand*{\bfX}{{\bm{X}}}
\newcommand*{\bfw}{{\bm{w}}}
\newcommand*{\bfy}{{\bm{y}}}
\newcommand*{\bfz}{{\bm{z}}}
\newcommand*{\bfa}{{\bm{a}}}
\newcommand*{\bsu}{{\bm{u}}}
\newcommand*{\bfu}{{\bm{u}}}
\newcommand*{\bfA}{{\bm{A}}}
\newcommand*{\bfV}{{\bm{V}}}
\newcommand*{\bsf}{{\bm{f}}}
\newcommand*{\bsg}{{\bm{g}}}
\newcommand*{\bseps}{{\bm{\epsilon}}}
\newcommand*{\rmd}{\mathrm{d}}
\newcommand*{\var}{\textrm{Var}}
\newcommand*{\ex}{\mathbb{E}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareFontFamily{U}{mathx}{}
\DeclareFontShape{U}{mathx}{m}{n}{<-> mathx10}{}
\DeclareSymbolFont{mathx}{U}{mathx}{m}{n}
\DeclareMathAccent{\widecheck}{0}{mathx}{"71}

\numberwithin{equation}{section}
\usepackage{pgfplots}
\usepackage[edges]{forest}
\pgfplotsset{compat=newest}
\usetikzlibrary{trees,positioning}

\newcommand{\distro}[4][40]{
  \begin{tikzpicture}[thick]
    \draw[dashed, dash pattern={on 2.3 off 2}] (0, .4) circle (12mm);
    \draw[blue!60!black, very thick] plot[variable=\t, domain=-1:1, samples=#1] ({\t}, {#2 * exp(-10*(\t)^2) + #3 * exp(-60*(\t-0.6)^2 - \t) + #3 * exp(-60*(\t+0.7)^2 - 0.2) + #4 * 0.5 * exp(-50*(\t+0.3)^2) + #4 * exp(-50*(\t-0.2)^2 + 0.1)});
    \draw[solid, <->] (-1, 0)--(1, 0);
  \end{tikzpicture}
}

\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\NB{\emph{N.B}\onedot}
\def\cf{\emph{cf}\onedot} \def\Cf{\emph{Cf}\onedot}
\def\etc{\emph{\&c}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\makeatother


\hypersetup{
    colorlinks,
    linkcolor={orange!70!black},
    citecolor={blue!80!black},
    urlcolor={magenta!80!black}
}
\usepackage[capitalize,noabbrev]{cleveref}

\usepackage[many]{tcolorbox}

\newtcolorbox{theorembox}{
    enhanced,
    sharp corners,
    breakable,
    borderline west={2pt}{0pt}{blue},
    colback=blue!10,
    colframe=blue!10
}

\newtcolorbox{standoutbox}{
    enhanced,
    sharp corners,
    breakable,
    borderline west={2pt}{0pt}{orange},
    colback=orange!10,
    colframe=orange!10
}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}


\title{Greed is Good: Guided Generation from a Greedy Perspective}


\author{Zander W. Blasingame\thanks{Correspondence to \texttt{blasinzw@clarkson.edu}}\\
Clarkson University
\And Chen Liu\\
Clarkson University
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy %
\begin{document}


\maketitle

\begin{abstract}
    Training-free guided generation is a widely used and powerful technique that allows the end user to exert further control over the generative process of diffusion models.
    In this work, we explore the guided generation from the perspective of optimizing the solution trajectory of a neural differential equation in a greedy manner.
    We present such a strategy as a unifying view on training-free guidance by showing that the greedy strategy is a first-order discretization of end-to-end optimization techniques.
    We show that a greedy guidance strategy makes \textit{good} decisions and compare it to a guidance strategy using the \textit{ideal} gradients found via the continuous adjoint equations.
    We then show how other popular training-free guidance strategies can be viewed in a unified manner from this perspective.
\end{abstract}


\section{Introduction}
Guided generation greatly extends the utility of state-of-the-art generative models by allowing the end user to exert greater control over the generative process, ultimately making the tool more useful in a wide variety of applications ranging from conditional generation, editing of samples, inverse problems, \etc
We focus on guided generation with diffusion/flow models~\citep{song2021scorebased,lipman2023flow,ho2020denoising} as these models represent the current state-of-the-art across many different modalities, 
\eg, audio~\citep{liu2023audioldm}, images~\citep{rombach2022high}, video~\citep{blattmann2023align}, protein generation~\citep{skreta2024superposition}, \etc.

We can divide the techniques for guided generation into two broad categories: conditional training and training-free methods.
The former of these two requires the training of the underlying diffusion/flow model on additional conditional information, either as a part of the training or at a later time as additional fine-tuning~\citep{song2021denoising,ho2021classifier,hulora}.
The latter category instead makes use of some known guidance function defined on the data distribution and incorporates this information back to the model to influence the generative process.

These training-free techniques can be further broken down into two sub-categories, \ie, posterior sampling and end-to-end optimization.
The former class of techniques uses a simple estimation of the posterior distribution that can be easily found in diffusion models~\citep{chung2023diffusion} and \textit{some} flow models~\citep[\cf][Section 4.8]{lipman2024flow-guide}.
This simple estimate of the posterior can then be fed to a guidance function to construct a gradient which can be brought back into the intermediate sampling state.
This can then be used to update the ODE solve as a form of classifier guidance~\citep{chung2023diffusion,yu2023freedom}.
The latter class of techniques, in contrast, performs backpropagation throughout the entire sampling process of the model to update the initial sample used in the solution trajectory via the guidance function.
Many of these techniques~\citep{blasingame2024adjointdeis,ben-hamu2024dflow,pan2024adjointdpm,marion2024implicit,wang2024training} make use of the continuous adjoint equations~\citep[see][Section 5.1.2]{kidger_thesis} to perform this backpropagation, this technique is known as optimize-then-discretize.
However, some approaches~\citep{liu2023flowgrad} prefer to perform the more memory-expensive, but potentially more accurate, discretize-then-optimize~\citep[see][Section 5.1.1]{kidger_thesis} method, which amounts to performing backpropagation through the entire discretized sampling process.
Finally, some works have viewed this through the lens of optimal control and introduce a control signal into the sampling process, which can be optimized through optimize-then-discretize~\citep{wang2024training} or discretize-then-optimize~\citep{liu2023flowgrad}.

The aim of this paper is to present a unified view of these training-free techniques for guided generation from a \textit{greedy perspective}.
\begin{standoutbox}
Our key insight is that we can \textit{bridge} between techniques which use posterior sampling and techniques which use end-to-end optimization for guidance by viewing the former as a \textit{greedy strategy} on the latter.
\end{standoutbox}
In light of this insight, we compare several state-of-the-art techniques from this perspective, showing how this perspective yields a unified and flexible framework for viewing guided generation with diffusion/flow models.
We perform a detailed analysis of this greedy strategy, showing that it is not only a unifying view, but that it actually makes \textit{good} decisions.
We further illustrate how a greedy strategy can be viewed as a first-order method of an optimize-then-discretize or discreteize-then-optimize method, providing a flexible way to exchange compute costs with the needed accuracy of gradients.



\section{Preliminaries}
\subsection{Diffusion Models}
Diffusion models seek to learn a mapping from some simple prior distribution $p(\bfx)$ to the data distribution $q(\bfx)$.
A forward process from the data distribution to the prior distribution is defined via an It\^o Stochastic Differential Equation (SDE):
\begin{equation}
    \label{eq:diffusion_forward}
    \rmd \bfx_t = f(t)\bfx_t \; \rmd t + g(t)\;\rmd \bfw_t,
\end{equation}
on the time interval $[0,T]$ and where $\{\bfw_t\}_{t \in [0,T]}$ is the standard Wiener process.
The \textit{reverse-time} SDE~\citep{anderson1982reverse} is found to be:
\begin{equation}
    \label{eq:diffusion_backwards}
    \rmd \bfx_t = [f(t)\bfx_t + g^2(t)\nabla_\bfx \log p_t(\bfx_t)]\;\rmd t + g(t)\;\rmd \bar\bfw_t,
\end{equation}
where $\rmd t$ is a \textit{negative} timestep and $\{\bar\bfw_t\}_{t \in [0,T]}$ is the standard Wiener process in reverse time.
The diffusion models then learn the score function, $\boldsymbol{s}_\theta(\bfx) = \nabla_\bfx \log p_t(\bfx_t)$,~\citep{song2021scorebased} or some other closely related quantity, such as noise prediction~\citep{song2021denoising,ho2020denoising} or data prediction~\citep{kingma2021variational}.
Using this learned score function, we can then sample the \textit{reverse-time} SDE,~\cref{eq:diffusion_backwards}, using numerical SDE solvers.
\citet{song2021scorebased} showed that there exists an Ordinary Differential Equation (ODE) known as the \textit{probability flow ODE} which shares the same marginals, $p_t(\bfx)$, as~\cref{eq:diffusion_backwards} given by
\begin{equation}
    \label{eq:pf_ode}
    \frac{\rmd \bfx_t}{\rmd t} = f(t) \bfx_t + \frac{g^2(t)}{2} \nabla_\bfx \log p_t(\bfx_t),
\end{equation}
which also defined in \textit{reverse-time}.

\subsection{Flow Models}
Flow models~\citep{lipman2023flow} are another popular class of generative models that have theoretical overlaps with diffusion models. Consider two random variables: $\bfX_0 \sim p(\bfx)$ and $\bfX_1 \sim q(\bfx)$.
Then consider a time-dependent vector field $\bfu \in \mathcal{C}^{1,r}([0,1] \times \R^d;\R^d)$\footnote{For notational simplicity, we let $\mathcal{C}^{k_1,k_2,\ldots,k_n}(X_1 \times X_2 \times \cdots \times X_n ; Y)$ denote the set of continuous functions that are $k_i$-times differentiable in the $i$-th argument mapping from $(X_1 \times X_2 \times \cdots \times X_n)$ to $Y$, if $Y$ is omitted, then $Y = \R$.}
with $r \geq 1$ which determines\footnote{\NB, to guarantee \textit{global} existence and uniqueness a stronger constraint on the vector field, like global Lipschitzness or integrability is necessary, for further details we refer the reader to~\citep{lipman2024flow-guide}.} a time-dependent flow $\Phi_t \in \mathcal{C}^{1,r}([0,1]\times\R^d;\R^d)$ which satisfies the ODE
\begin{equation}
    \Phi_0(\bfx) = \bfx, \quad \frac{\rmd}{\rmd t} \Phi_t(\bfx) = \bfu(t, \Phi_t(\bfx)).
\end{equation}
This is known as a $\mathcal{C}^{r}$-flow and this flow is diffeomorphism in its second argument for all $t \in [0,1]$.
Applying this flow to the random variable $\bfX_0$ we define a \textit{continuous-time Markov process} $\{X_t\}_{t \in [0,1]}$ with mapping $\bfX_t = \Phi_t(\bfX_0)$.
The \textit{goal}, then, is to learn a flow $\Phi_t$ such that $\bfX_1 = \Phi_1(\bfX_0) \sim q(\bfx)$.
This procedure amounts to learning a neural network parameterized vector field $\bfu_\theta \in \mathcal{C}^{1,r}([0,1] \times \R^d; \R^d)$ which is achieved through training procedures known as \textit{flow matching}~\citep{lipman2023flow}.

The probability flow ODE formulation of diffusion models can be viewed as a type of flow model with Gaussian probability paths, \ie, $(\bfX_0, \bfX_1) \sim \pi_{0,1}(\bfx_0, \bfx_1) = p(\bfx_0)q(\bfx_1)$ with $p(\bfx) = \mathcal{N}(\bfx | \mathbf 0, \sigma^2 \mathbf I)$~\citep{lipman2024flow-guide}.

\textbf{Time conventions.}
Although these two formulations are, essentially for our purposes, identical; we choose to adopt the time conventions of flow models over those of diffusion models.
\Ie, we let time $t = 0$ denote the prior distribution and let time $t = 1$ denote the data distribution.
In the opinion of the authors, this convention is more easily understood than the common time conventions in the literature on diffusion models.


\subsection{The Continuous Adjoint Equations}
Let $\bfu_\theta \in \mathcal{C}^{1,1}([0,1] \times \R^d;\R^d)$ be a model that models the vector field of some ODE and be Lipschitz continuous in its second argument. Let $\bfx: [0,1] \to \R^d$ be the solution to the ODE with the initial condition $\bfx_0 \in \R^d$, $\dot \bfx_t = \bfu_\theta(t, \bfx_t)$.
For some scalar-valued loss function $\mathcal{L} \in \mathcal{C}^2(\R^d)$ in $\bfx_1$, let $\bfa_\bfx \coloneq \partial\mathcal{L}/\partial \bfx_t$ denote the gradient.
Then $\bfa_\bfx$ and related quantity $\bfa_\theta \coloneq \partial\mathcal{L}/\partial \theta$ can be found by solving an augmented ODE of the form,
\begin{equation}
    \begin{aligned}
    \bfa_\bfx(1) &= \frac{\partial \mathcal{L}}{\partial \bfx_1}, \quad &&\frac{\rmd \bfa_\bfx}{\rmd t}(t) = -\bfa_\bfx(t)^\top \frac{\partial \bfu_\theta}{\partial \bfx}(t, \bfx_t),\\
    \bfa_\theta(1) &= \mathbf 0, \quad &&\frac{\rmd \bfa_\theta}{\rmd t}(t) = -\bfa_\bfx(t)^\top \frac{\partial \bfu_\theta}{\partial \theta}(t, \bfx_t),
    \end{aligned}
    \label{eq:continuous_adjoint_eqs}
\end{equation}
this system of equations are known as the \textit{continuous adjoint equations}~\citep{kidger_thesis}.
\NB, this technique was first proposed by~\citet{pontryagin1963} and popularized for neural differential equations by~\citet{chen2018neural}.
Solving the continuous adjoint equations to find the gradients described by the adjoint state is known as an \textit{optimize-then-discretize} method.
This is in contrast with \textit{discretize-then-optimize} methods which first discretize the neural differential equation before calculating the gradients (this can be thought of as ``vanilla'' backprop through a neural network).
For a more detailed discussion between these two methods we refer to Kidger's monograph on neural differential equations~\citeyearpar[Section 5.1]{kidger_thesis}




\begin{figure}[t]
    \centering

    \tikzset{
        basic/.style  = {draw, text width=20mm, font=\scriptsize, rectangle},
        root/.style = {basic, thin, align=center},
        tnode/.style = {basic, thin, align=center, font=\scriptsize\bfseries},
        xnode/.style = {basic, thin, align=left, text width=28mm},
        arrow/.style = {thick, dotted, shorten >=3, shorten <=3, ->}
    }

    
    \begin{forest} for tree={
        grow=east,
        growth parent anchor=west,
        parent anchor=east,
        child anchor=west,
        fork sep=6mm,
        l sep=12mm,
    },
    forked edges,
    [Training-free\\ guided generation, root
        [Posterior sampling, tnode, name=posterior
            [\citep{bansal2023universal}, xnode, text width=22mm]
            [\citep{chung2023diffusion}, xnode, text width=22mm]
            [\citep{wang2023zeroshot}, xnode, text width=22mm]
            [\citep{yu2023freedom}, xnode, text width=22mm]
        ]
        [End-to-end\\ optimization, tnode, name=e2e
            [State optimization, tnode
                [\citep{pan2024adjointdpm}, xnode]
                [\citep{blasingame2024adjointdeis}, xnode]
                [\citep{ben-hamu2024dflow}, xnode]
                [\citep{marion2024implicit}, xnode]
            ]
            [Control signal\\ optimization, tnode
                [\citep{liu2023flowgrad}, xnode]
                [\citep{wang2024training}, xnode]
            ]
        ]
    ]
    \draw[arrow] (e2e) to node[font=\scriptsize, anchor=west,align=center, yshift=2mm]{A greedy strategy} (posterior); 
    \end{forest}

    
    \caption{A taxonomy of \textit{training-free guided generation} methods. We show that a greedy perspective is the \textit{unifying bridge} between posterior and end-to-end methods, bring several different works together under one framework. In~\cref{sec:greedy_perspective} we view posterior sampling techniques as a greedy strategy of end-to-end optimization techniques; moreover, in~\cref{sec:greed_as_unified} we show how the greedy strategy is related to end-to-end optimization techniques as an explicit Euler scheme of a discretize-then-optimize method and an implicit Euler scheme of an optimize-then-discretize method see~\cref{thm:greedy_is_implicit_euler}. Likewise, in~\cref{sec:guidance_via_control} we present the greedy perspective on end-to-end optimization via a control signal. \NB, end-to-end optimization techniques include both optimize-then-discretize and discretize-then-optimize techniques, although most fall into the former category.}
    \label{fig:taxonomy_of_guided}
\end{figure}


\section{Related Works}
Training-free methods for diffusion/flow models is an active area of research with many different techniques proposed with differing strengths.
Diffusion Posterior Sampling (DPS)~\citep{chung2023diffusion} is a guidance method that uses Tweedie's formula~\citep{stein1981estimation} to estimate the gradient of some guidance function defined in the output state \wrt the noisy state, \ie, $\ex[\bfx_1|\bfx_t]$. 
Likewise the work of~\citet{bansal2023universal,wang2023zeroshot,yu2023freedom} explore similar concepts by employing Tweedie's formula for diffusion models, with FreeDoM including an additional ``time-travel strategy''~\citep{wang2023zeroshot} to help improve the visual fidelity of the generated images.
More relevant to our work,~\citet{blasingame2024greedydim} explore guidance using posterior sampling from a greedy perspective; however, their analysis is limited as their work is only focused on a particular experimental application.
Specifically, they only consider a first-order numerical scheme for the diffusion ODE solver and they do not connect the greedy strategy to other guided generation methods.
\citet{liu2023flowgrad} provide guidance by adding an additional time-dependent control term to the vector field that is optimized using a type of discretize-then-optimize
method by finding an efficient non-uniform Euler discretization of the sampling trajectory and perform backpropagation through this discretization to update the control signal.


Several recent works have explored the use of continuous adjoint equations for diffusion/flow models.
\citet{nie2022diffpure} was the first to explore this topic, solving the continuous adjoint equations for adversarial purification with diffusion SDEs.
Later work by~\cite{pan2024adjointdpm,pan2023adjointsymplectic} explore special solvers for the continuous adjoint equations of VP-type diffusion ODEs.
\citet{blasingame2024adjointdeis} extends these works by developing bespoke solvers for VP-type diffusion ODEs and SDEs.
\citet{marion2024implicit} explore using the continuous adjoint equations as a part of a larger bi-level optimization scheme for guided generation.
The work of \citet{ben-hamu2024dflow} extends the analysis of continuous adjoint equations for diffusion models to flow-based models and provides an alternative perspective to the analyis performed by the earlier works.
\citet{wallace2023end} use EDICT~\citep{wallace2023edict}, an invertible formulation of diffusion models, to perform backpropagation through the diffusion model; this can be viewed as a specific discretization scheme of continuous adjoint equations.
Recent work by~\citet{wang2024training} explores an extension of~\citet{ben-hamu2024dflow} to Riemannian manifolds which incorporates a control signal to the vector field and optimizes both the solution state and \textit{co-state}, they call their approach OC-Flow.








\section{A Motivation for Greed}
\label{sec:motivation_greed}

Consider the case of Affine Gaussian Probability Paths (AGPP) and diffusion models in the ODE formulation with a data coupling, $(\bfX_0, \bfX_1) \sim p(\bfx_0)q(\bfx_1)$ with a Gaussian prior $p(\bfx_0)$.
We define the new random variable $\bfX_t$ as an affine interpolation,
\begin{equation}
\bfX_t = \alpha_t \bfX_1 + \sigma_t \bfX_0,
\end{equation}
where $\alpha_t, \sigma_t \in \mathcal{C}^\infty([0,1];[0,1])$ are smooth functions with boundary conditions $\alpha_0 = \sigma_1 = 0$ and $\alpha_1 = \sigma_0 = 1$ which define the \textit{schedule} of $\{\bfX_t\}_{t \in [0,1]}$.
Furthermore, $\alpha_t$ is strictly monotonically increasing in $t$; whereas, $\sigma_t$ is strictly monotonically decreasing in $t$~\citep{lipman2024flow-guide}.
Then the \textit{affine conditional flow} is defined as $\Phi_t(\bfx | \bfx_1) = \alpha_t \bfx_1 + \sigma_t \bfx$.
It follows that the marginal vector field is given by
\begin{equation}
    \label{eq:mariginal_vec}
    \bfu(t, \bfx) = \ex[\dot\alpha_t\bfX_1 + \dot\sigma_t\bfX_0 | \bfX_t = \bfx],
\end{equation}
where $\dot\alpha_t$ denotes the first derivative of $\alpha_t$ \wrt time.
Now assume that we have trained our diffusion/flow model, $\bfu_\theta$, to zero loss.


\subsection{Drawbacks of the Continuous Adjoint Equations}
A natural choice to solve this problem is to apply the continuous adjoint equations to find the gradient $\partial \mathcal L / \partial \bfx_0$ and use that quantity for the optimization problem; however, there are several potential drawbacks to this approach which we will enumerate.

\textbf{Truncation errors.} One area of concern is the potential mismatch between the forward trajectory $\{\bfx_{t_i}\}_{i=1}^N$ and the backward trajectory $\{\tilde \bfx_{t_i}\}_{i=1}^N$ when performing the backwards solve.
One potential solution is to use an \textit{algebraically reversible solver}~\citep[see][]{kidger2021efficient,mccallum2024efficient} which guarantees that the forward and backward trajectory match \textit{perfectly}.
Another option is to store the forward trajectory $\{\bfx_{t_i}\}_{i=1}^N$ in memory and use \textit{interpolated adjoints} if the backward timesteps do not perfectly align with the forward timesteps~\citep[see][]{kim2021stiff}.

\textbf{Stability concerns.} Consider the simple ODE, $\dot y(t) = \lambda y(t)$ defined on $t \in [0,T]$ with $y(0) = y_0$ and $\lambda < 0$.
Clearly, most ODE solvers with a non-trivial region of stability~\citep[see][Definition 2.1]{hairer2002stiff_odes} will solve this ODE without an issue, as the errors will decrease exponentially with $\lambda < 0$.
However, in the backwards in time solve from $y(T)$ the errors will \textit{grow exponentially}.
It can be shown that the adjoint state suffers from similar stability issues.
The local behavior of a differential equation is described through the eigenvalues of the Jacobian of the vector field~\citep[see][]{butcher2016numerical}.
For $\bfx_t$ this is given by $\frac{\partial \bfu_\theta}{\partial \bfx}$ and for $\bfa_\bfx$ this is given by
\begin{equation}
    \frac{\partial}{\partial \bfa_\bfx}\bigg(-\bfa_\bfx(t)^\top \frac{\partial \bfu_\theta}{\partial \bfx}(t, \bfx_t)\bigg) = - \frac{\partial \bfu_\theta}{\partial \bfx}(t, \bfx_t).
\end{equation}
Clearly, the Jacobians for $\bfa_\bfx$ and $\bfx_t$ solved in reverse-time are identical, meaning the stability of the backward solve is pushed onto the solve for the adjoint state~\citep[see][Section~5.1.2]{kidger_thesis} for more details.
Reversible solvers eliminate truncation errors, but tend to suffer from poor stability, \eg, the region of stability for reversible Heun applied to neural ODEs is the complex interval $[-i, i]$~\citep{kidger2021efficient}.
Recent work by~\citet{mccallum2024efficient}, however, has shown a strategy for constructing reversible solvers with a non-trivial region of stability.

\textbf{Computational cost.} Clearly, by construction guidance procedures which solve the continuous adjoint equations\footnote{\NB, this would also include methods like FlowGrad which perform a full backpropagation through the ODE solver, but do not necessarily solve the continuous adjoint equations.} are much more computationally expensive than those which use some form of posterior optimization.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth, trim={0 0 0 2.5cm}, clip]{figures/guided_gen_comp.png}
    \caption{
        Visual comparison of different training-free guided generation techniques.
        For state optimization techniques (left) the \textcolor{gray}{gray} curve represents the previous solution trajectory $\{\bfx_t^{(k)}\}_{t \in [0,1]}$ at the $k$-th iteration and the \textcolor{red!50!black}{red} curve shows the updated solution trajectory at the $(k+1)$-th iteration via the gradients in \textcolor{cyan!50!black}{cyan}.
        For control signal optimization techniques (middle) the solution trajectory is updated by changing the control signal $\bfz_t$, see~\cref{sec:guidance_via_control}, via the gradients $\partial \mathcal L / \partial \bfz_t$ calculated throughout the discretization scheme.
        For posterior sampling techniques (right) we use dull colors to denote the steps need to compute the $(k+1)$-th iteration and bright colors to denote the steps $(k+2)$-th iteration.
        \NB, in contrast with the end-to-end optimization methods, posterior sampling methods quickly compute an estimate with the gradient with $\hat\bfx_{1|t}(\bfx)$ and do not backpropagate the gradient to any steps before the current timestep.
        The posterior sampling strategy makes a greedy decision at each timestep, hence we view these techniques as a greedy strategy of end-to-end optimization techniques.
        For all three diagrams, we let $0 < s < t < 1$.
    }
    \label{fig:greedy-overview.}
\end{figure}


\section{A Greedy Perspective}
\label{sec:greedy_perspective}
The computational cost of adjoint methods can be quite high, as is admitted in papers exploring this method~\citep{ben-hamu2024dflow,blasingame2024adjointdeis,pan2024adjointdpm,wang2024training}.
As such, it is worth considering whether techniques such as FreeDoM~\citep{yu2023freedom} and DPS~\citep{chung2023diffusion}, which are much cheaper, can be used as a reasonable alternative to techniques that use adjoint methods.
In other words, we ask the following.
\begin{standoutbox}
Rather than performing the full optimization back to $\bfx_0$, what if we greedily took an optimal step at each $\bfx_t$ instead?
\end{standoutbox}
The next question is how to define a locally optimal step at time $t$.
For this purpose we can use posterior sampling, which is defined as $\hat\bfx_{1|t}(\bfx) = \ex[\bfX_1 | \bfX_t = \bfx]$, Note that this is also commonly referred to as the denoiser, data prediction model, or $\bfx_1$-prediction model~\citep{lipman2024flow-guide}.
This has the advantage of being easily calculated from the $\bfx_0$-prediction model or the model of the vector field, requiring little computation to calculate this quantity.
Then using this estimate of the posterior, we can use a guidance function defined in the data space and take the gradient of the loss \wrt as the local step, \ie, use $\nabla_\bfx \mathcal L(\hat\bfx_{1|t}(\bfx))$ as guidance.
As such, we have our connection between a greedy strategy and \textit{posterior sampling} approaches.

 
\subsection{Greed as a Unifying Theory of Guidance}
\label{sec:greed_as_unified}
By construction we have our connection between the greedy strategy and posterior sampling methods.
Next, we will establish the connection between posterior sampling methods and the much larger class of end-to-end optimization methods.
To make our analysis simpler, let us write the flow from $s$ to $t$ in terms of the denoiser.
The vector field can be written as a function of the denoiser with,
\begin{equation}
    \label{eq:vector_field_denoiser}
    \bfu(t, \bfx) = \underbrace{\frac{\dot\sigma_t}{\sigma_t}}_{\coloneq a_t} \bfx + \underbrace{\dot\alpha_t - \alpha_t\frac{\dot\sigma_t}{\sigma_t}}_{\coloneq b_t} \hat\bfx_{1|t}(\bfx).
\end{equation}
The flow from time $s$ to time $t$ can then be expressed as the integral of the right-hand side of~\cref{eq:vector_field_denoiser} over time.
Thus, the flow is now expressed as a semi-linear integral equation with linear term $a_t\bfx$ and non-linear term $b_t\hat\bfx_{1|t}(\bfx)$.
Due to this semi-linear structure, we can apply the technique of \textit{exponential integrators}~\cite{hochbruck2010exponential} that has been successfully used to simplify numerical solvers for diffusion models~\citep{lu2022dpm,zhangfast,gonzalez2024seeds}.

Let $\gamma_t = \gamma(t) \coloneq \alpha_t/\sigma_t$ denote the signal-to-noise ratio (SNR), then $\gamma_t$ is a monotonically increasing sequence in $t$, due to the properties of $(\alpha_t, \sigma_t)$ and thus has an inverse $t_\gamma$ such that $t_\gamma(\gamma(t)) = t$.
With abuse of notation, we let $\bfx_\gamma \coloneq \bfx_{t_\gamma(\gamma)}$ and $\hat\bfx_{1|\gamma}(\cdot) = \hat\bfx_{1|t_\gamma(\gamma)}(\cdot)$.
As such, we can rewrite the solution to the flow model in terms of $\gamma$ by making use of exponential integrators, which we show in~\cref{prop:exact_sol_flow} with the full proof provided in~\cref{proof:exact_sol_flow}.
\begin{theorembox}
\begin{proposition}[Exact solution of affine probability paths]
    \label{prop:exact_sol_flow}
    Given an initial value of $\bfx_s$ at time $s \in [0, 1]$ the solution $\bfx_t$ at time $t \in [0,1]$ of an ODE governed by the vector field in ~\cref{eq:mariginal_vec} is:
    \begin{equation}
        \label{eq:exact_sol_flow}
         \bfx_t = \frac{\sigma_t}{\sigma_s} \bfx_s + \sigma_t \int_{\gamma_s}^{\gamma_t} \hat\bfx_{1|\gamma}(\bfx_\gamma)\;\rmd \gamma.
    \end{equation}
\end{proposition}
\end{theorembox}
\NB, our result bears some similarity to the exact solution of diffusion ODEs that use a denoiser model; however, they integrate \wrt one-half of the logarithmic SNR~\citep[\cf][]{lu2022dpm++}.

\textbf{Discretize-then-optimize.}
From~\cref{prop:exact_sol_flow} we can see that using the denoiser to estimate $\bfx_1$ is akin to taking a first-order approximation of the flow.
More specifically, we can construct a $(k-1)$-th Taylor expansion of~\cref{eq:exact_sol_flow} with:
\begin{equation}
    \bfx_t = \frac{\sigma_t}{\sigma_s}\bfx_s + \sigma_t \sum_{n=0}^{k-1} \frac{\rmd^n}{\rmd \gamma^n}\bigg [ \hat\bfx_{1|\gamma}(\bfx_\gamma)\bigg]_{\gamma = \gamma_s} \frac{h^{n+1}}{n!} + \mathcal{O}(h^{k+1}),
\end{equation}
where $h \coloneq \gamma_t - \gamma_s$ is the step size.
Then it follows that for $k=1$ the first-order discretization of the flow, omitting high-order error terms becomes,
\begin{equation}
    \tilde\bfx_t = \frac{\sigma_t}{\sigma_s}\bfx_s + (\alpha_t + \frac{\sigma_t\alpha_s}{\sigma_s})\hat\bfx_{1|s}(\bfx_s).
\end{equation}
In the limit as $t \to 1$ we have $\tilde\bfx_t = \hat\bfx_{1|s}(\bfx_s)$, note despite $\sigma_t \to 0$ the asymptotic behavior is well-defined~\citep[see][]{ben-hamu2024dflow}.
Thus we can view the gradients produced by a greedy strategy as an discretize-then-optimize technique for an explicit Euler scheme with step size $h$.

\textbf{Optimize-then-discretize.}
Next we consider the continuous \textit{ideal} of the gradients of the solution trajectory, \ie, $\{\bfa_\bfx(t)\}_{t \in [0,1]}$.
We then explore how the gradients calculated via the greedy strategy compare to this continuous ideal of the gradient flow.
In~\cref{thm:greedy_is_implicit_euler} we show that a greedy strategy can be viewed as the first iteration of a fixed-point method of an implicit Euler discretization of the continous adjoint equations.
\begin{theorembox}
    \begin{theorem}[Greedy as an implicit Euler method]
        \label{thm:greedy_is_implicit_euler}
        For some trajectory state $\bfx_t$ at time $t$, the greedy gradient given by $\nabla_{\bfx_t} \mathcal{L}(\hat\bfx_{1|t}(\bfx_t))$ is an implicit Euler discretization of the continuous adjoint equations for the true gradients with step size $h = \gamma_1 - \gamma_t$.
    \end{theorem}
\end{theorembox}
\textit{Proof sketch.}
First we use the technique of exponential integrators to simplify the continuous adjoint equations. Then we perform a first-order Taylor expansion around $\gamma_t$ which is equivalent to an implicit Euler scheme as we are calculating the gradient flow from $1$ to $t$. The full proof is provided in~\cref{proof:greedy_is_implicit_euler}.

An important question is if a greedy strategy makes \textit{good} decisions at each timestep.
\Ie, if we make a good decision at time $t$, does that ensure that an optimal solution was made in the sense of $\bfx_1 = \Phi_{1|t}(\bfx_t)$, where $\Phi_{t|s}$ denotes the flow from time $s$ to time $t$.
A natural way to examine this question is to consider whether convergence in the local case implies convergence of the whole solution trajectory.
We find that up to a bound dependent on the step size convergence in the greedy solution implies convergence in the flow, which we state more formally in~\cref{thm:convergence}.
\begin{theorembox}
    \begin{theorem}[Greedy convergence]
        \label{thm:convergence}
        For affine probability paths, if there exists a sequence of states $\bfx_n$ at time $t$ such that it converges to the locally optimal solution $\hat\bfx_{1|t}(\bfx_n) \to \bfx_{1}^*$.
        Then, $\|\Phi_{1|t}(\bfx_n) - \bfx_1^*\|$ is $\mathcal{O}(h^2)$ as $n \to \infty$.
    \end{theorem}
\end{theorembox}
\textit{Proof sketch.}
First, we take a first-order Taylor expansion of the flow described in~\cref{eq:exact_sol_flow} which as shown above simplifies to the denoiser plus $\mathcal{O}(h^2)$ truncation error terms. Clearly, if $\hat\bfx_{1|t}(\bfx_n) \to \bfx_1^*$ then the first term of the Taylor expansion does as well.
The full proof is provided in~\cref{proof:convergence}.

Note, the tightness on the convergence of the flow from the greedy strategy is controlled by the step size, but also the geometry of the ODE.
Clearly, if the curvature\footnote{The curvature~\citep[see][]{finlay2020train,liu2023flow,lee2023minimizing} can be defined as
\begin{equation*}
    S(\bfX) = \int_0^1 \ex\bigg[\Big\|(\bfX_1 - \bfX_0) - \dot\bfX_t\Big\|^2\bigg]\;\rmd t.
\end{equation*}
}
was zero, \ie, the paths were straight, then there would be no truncation error and convergence in greedy would necessarily imply convergence in flow.
This positive result shows that a greedy strategy makes good decisions that justify the potential application of such a technique over the more expensive adjoint methods.
This complements the observations of~\citet{ben-hamu2024dflow} who report they only needed a few discretization steps of the continuous adjoint equations to achieve adequate performance.

Importantly, because the flow is a diffeomorphism, we can use the optimal solution at time $t \in [0,1]$ to find the optimal solution at any time $s \in [0,1]$.
This combined with our upper bound in the error shows that a greedy strategy could be in theory used to solve problems which seek to find the optimal $\bfx_0^*$.
\NB, in practice we still need to discretize the flow $\Phi_t$ which introduces error in estimating $\bfx_0$ from $\bfx_t$.




\subsection{Guidance via a Control Signal}
\label{sec:guidance_via_control}
Some works on guidance~\citep{liu2023flowgrad,wang2024training} have considered the problem from the perspective of optimal control.
In essence, inject an additional control signal, $\bfz \in \C^{1}(\R;\R^d)$, to the vector field, $\bfu_\theta$, such that
\begin{equation}
    \label{eq:flow_plus_control}
    \frac{\rmd \bfx_t}{\rmd t} = \bfu_\theta(t, \bfx_t) + \bfz(t).
\end{equation}
Thus, instead of optimizing $\{\bfx_t\}_{t \in [0,t]}$ directly, this control signal can instead be optimized, serving as one of the key insights in \citep{liu2023flowgrad,wang2024training}.
We can model the gradient to this signal by augmenting the continuous adjoint equations with the adjoint state $\bfa_\bfz(t) \coloneq \partial \mathcal{L} / \partial \bfz(t)$.
In~\cref{thm:cae_for_control} we show that this gradient is simply an integral of the adjoint state $\bfa_\bfx(t)$, the full proof can be found in~\cref{proof:cae_for_control}.

\begin{theorembox}
\begin{theorem}[Continuous adjoint equations for the control term]
    \label{thm:cae_for_control}
    Let $\bfu_\theta \in \C^{1,1}([0,1]\times \R^d; \R^d)$ be a parameterization of some time-dependent vector field of a neural ODE that is Lipschitz continuous in its second argument, and let $\bfz \in \C^{1}([0,1];\R^d)$ be an additional control signal such that the new dynamics are given by~\cref{eq:flow_plus_control}.
    Let $\bfa_\bfz(t) \coloneq \partial\mathcal{L}/\partial \bfz(t)$ then
    \begin{equation}
        \bfa_\bfz(t) = - \int_1^t \bfa_\bfx(s) \; \rmd s.
    \end{equation}
\end{theorem}
\end{theorembox}
This result can be viewed as a more generalized version of~\citet[Theorem 2]{wang2024training}~\citep[\cf][Equation (8)]{liu2023flowgrad}.

The next natural question then is to ask about the behavior of a greedy strategy applied to $\bfz(t)$.
To simplify the analysis, we now consider a control signal applied to the posterior model $\hat\bfx_{1|t}$ such that it is replaced by $\hat\bfx_{1|t}(\bfx_t) + \bfz(t)$ which amounts to simply rescaling $\bfz(t)$ from~\cref{eq:flow_plus_control} with $b_t$.
From this construction it should be clear that the greedy gradient for the control signal is merely $\nabla_{\tilde\bfx_1} \mathcal{L}(\tilde\bfx_1)$.
If using the original formulation where the control signal is applied to the vector field, rather than the denoiser, the gradient is simply scaled by a weighting function dependent on time.
Note this approach is similar to the greedy approach taken by~\citet{blasingame2024greedydim}; however, they inject the control signal onto the $\bfx_0$-prediction model.




\section{Conclusion}
In this work we show that posterior sampling methods can be viewed as a greedy strategy of end-to-end optimization methods, creating a unified framework for examining training-free guided generation techniques.
More specifically, we show that posterior sampling methods are simply a single-step first-order discretization of either discretize-then-optimize or optimize-then-discretize methods.
Hence we can move between the two different strategies by simply altering the number of discretization steps.
Moreover, we show that a greedy strategy can actually make \textit{good} decisions, justifying the choice of such a strategy over the more computationally intensive end-to-end optimization techniques in certain scenarios.
Our hope is that this unified perspective on guided generation can inform future research directions when applying training-free guided generation to different applications.

\textbf{Limitations.}
In this work we only consider affine probability paths which includes many flow models and diffusion models in the ODE formulation; however, we did not explore a greedy strategy for SDE based generative models.
With future work, we could likely tighten the bound in the convergence proof by incorporating the curvature of the ODE.
We did not perform a sensitivity analysis of how the flow changes \wrt to the greedy gradients.
As this work was theoretical and focused on drawing connections between existing guided generation techniques, we did not include experimental work.





\bibliography{bib}
\bibliographystyle{iclr2025_conference}

\newpage
\appendix



\section{Review of Our Nomenclature}
To enhance the readability of our work, we provide a quick summary of our nomenclature and preferred conventions.
We use a bold uppercase letter $\bfX_t \sim p_t(\bfx)$ to denote a random variable distributed according to $p_t(\bfx)$ and $\bfx_t$ to denote the particular realization of said random variable.
We use the hat symbol to denote a prediction function, \eg, we let
\begin{align}
    \hat\bfx_{1|t}(\bfx) &= \ex[\bfX_1 | \bfX_t = \bfx],\\
    \hat\bfx_{0|t}(\bfx) &= \ex[\bfX_0 | \bfX_t = \bfx],
\end{align}
denote the $\bfx_1$-prediction (data) and $\bfx_0$-prediction (noise) \textit{deterministic} functions, respectively.

\section{Proof of \texorpdfstring{~\cref{prop:exact_sol_flow}}{Exact solution of affine probability paths}}
\label{proof:exact_sol_flow}

We restate~\cref{prop:exact_sol_flow} here:
\begin{proposition}[Exact solution of affine probability paths]
    Given an initial value of $\bfx_s$ at time $s \in [0, 1]$, the solution $\bfx_t$ at time $t \in [0,1]$ of an ODE governed by the vector field in ~\cref{eq:mariginal_vec} is:
    \begin{equation}
        \bfx_t = \frac{\sigma_t}{\sigma_s} \bfx_s + \sigma_t \int_{\gamma_s}^{\gamma_t} \hat\bfx_{1|\gamma}(\bfx_\gamma)\;\rmd \gamma.
    \end{equation}
\end{proposition}

Additionally, we assume that the following holds:
\begin{assumption}
    \label{assump:at_integrable}
    The function $a_t \coloneq \frac{\dot\sigma_t}{\sigma_t}$ is integrable in $[0, 1]$.
\end{assumption}
This assumption is necessary for the simplification that we perform with exponential integrators.
\NB,~\citet{ben-hamu2024dflow} make the same assumption in their analysis of the continuous adjoint equations for affine probability paths.

\begin{proof}
    Recall that we uniquely define a flow model through the vector field $\bfu \in \C^{1,1}([0,1]\times\R^d;\R^d)$.
    The vector field which models the affine conditional flow with schedule $(\alpha_t, \sigma_t)$, (see~\cref{sec:motivation_greed}), is defined as
    \begin{equation}
        \bfu(t, \bfx) = \ex[\dot \alpha_t \bfX_1 + \dot \sigma_t \bfX_0 | \bfX_t = \bfx].
    \end{equation}
    With some simple algebra we can rewrite the vector field in terms of $\hat\bfx_{1|t}$,
    \begin{align}
        \bfu(t, \bfx) &= a_t \bfx + b_t \hat\bfx_{1|t}(\bfx),\\
        a_t &= \frac{\dot \sigma_t}{\sigma_t} \qquad b_t = \dot \alpha_t - \alpha_t \frac{\dot \sigma_t}{\sigma_t}.\nonumber
    \end{align}
    Now using this definition we can rewrite the solution for $\bfx_t$ from $\bfx_s$ in terms of $\hat\bfx_{1|t}$,
    \begin{align}
        \bfx_t &= \bfx_s + \int_s^t \bfu(\tau, \bfx_\tau) \; \rmd \tau,\\
        \label{eq:app:exact_sol1}
        \bfx_t &= \bfx_s + \int_s^t a_\tau \bfx_\tau + b_\tau \hat\bfx_{1|\tau}(\bfx_\tau)\; \rmd \tau.
    \end{align}
    Note the semi-linear form of the integral equation.
    We can exploit this structure using the technique of \textit{exponential integrators},~\citep[see][]{lu2022dpm,zhangfast,gonzalez2024seeds}, to simplify~\cref{eq:app:exact_sol1}, under~\cref{assump:at_integrable}, to
    \begin{equation}
        \label{eq:app:exact_sol2}
        \bfx_t = e^{\int_s^t a_u\;\rmd u}\bfx_s + \int_s^t e^{\int_\tau^t a_u\;\rmd u}b_\tau \hat\bfx_{1|\tau}(\bfx_\tau)\;\rmd \tau.
    \end{equation}
    Now, the integrating factor simplifies quite nicely to
    \begin{equation}
        e^{\int_s^t a_u\;\rmd u} = e^{\int_s^t \frac{\dot\sigma_u}{\sigma_u}\;\rmd u} = e^{\int_{\sigma_s}^{\sigma_t} \frac 1\sigma \;\rmd \sigma} = \frac{\sigma_t}{\sigma_s},
    \end{equation}
    such that~\cref{eq:app:exact_sol2} becomes
    \begin{equation}
        \label{eq:app:exact_sol3}
        \bfx_t = \frac{\sigma_t}{\sigma_s}\bfx_s + \sigma_t\int_s^t \frac{b_\tau}{\sigma_\tau} \hat\bfx_{1|\tau}(\bfx_\tau)\;\rmd \tau.
    \end{equation}
    We can simplify $b_t / \sigma_t$ to find:
    \begin{equation}
        \frac{b_t}{\sigma_t} = \frac{\dot\alpha_t \sigma_t - \alpha_t\dot\sigma_t}{\sigma_t^2} = \frac{\rmd}{\rmd t}\bigg(\frac{\alpha_t}{\sigma_t}\bigg) = \frac{\rmd}{\rmd t} \gamma_t,
    \end{equation}
    where $\gamma_t \coloneq \alpha_t / \sigma_t$, \ie, the signal-to-noise ratio.
    As such, we can rewrite~\cref{eq:app:exact_sol3} with a change of variables $\bfx_\gamma = \bfx_{\gamma_t^{-1}(\gamma)} = \bfx_t$,
    \begin{align}
        \bfx_t &= \frac{\sigma_t}{\sigma_s}\bfx_s + \sigma_t\int_{\gamma_s}^{\gamma_t} \hat\bfx_{1|\gamma}(\bfx_\gamma)\;\rmd \gamma,
    \end{align}
    concluding the proof.
\end{proof}







\section{Proof of \texorpdfstring{~\cref{thm:convergence}}{Greedy convergence}}
\label{proof:convergence}

We restate~\cref{thm:convergence} here:

\begin{theorem}[Greedy convergence]
    For affine probability paths, if there exists a sequence of states $\bfx_n$ at time $t$ such that it converges to the locally optimal solution $\hat\bfx_{1|t}(\bfx_n) \to \bfx_{1}^*$.
    Then, $\|\Phi_{1|t}(\bfx_n) - \bfx_1^*\|$ is $\mathcal{O}(h^2)$ as $n \to \infty$.
\end{theorem}

Throughout the proof the norm $\|\cdot\|$ corresponds to the Euclidean norm $\|\cdot\|_2$.
Additionally, we make the following (mild) regularity assumptions:
\begin{assumption}
    \label{assump:lipschitz}
    The $\bfx_1$-prediction model $\hat\bfx_{1|t}: [0,1] \times \R^d \to \R^d$ is Lipschitz in its second argument and continuous in its first.
\end{assumption}
\begin{assumption}
    \label{assump:total_derivs}
    The total derivatives $\frac{\rmd^n \hat\bfx_{1|\gamma}(\bfx)}{\rmd \gamma^n}$ exist and are continuous for $0 \leq n \leq k + 1$.
\end{assumption}

\begin{proof}
    Let $\bfx_1 = \Phi_{1|t}(\bfx_t)$.
    Then by~\cref{assump:total_derivs}, we can take a $(k-1)$-th order Taylor expansion around $\gamma_t$ of the flow:
    \begin{align}
        \bfx_1 &= \frac{\sigma_1}{\sigma_t}\bfx_t + \sigma_1 \int_{\gamma_t}^{\gamma_1}\sum_{n=0}^{k-1} \frac{\rmd^n}{\rmd \gamma^n}\bigg [ \hat\bfx_{1|\gamma}(\bfx_\gamma)\bigg]_{\gamma = \gamma_t} \frac{(\gamma - \gamma_t)^{n}}{n!}\;\rmd \gamma + \mathcal{O}(h^{k+1}),\nonumber\\
        &= \frac{\sigma_1}{\sigma_t}\bfx_t + \sigma_1 \sum_{n=0}^{k-1} \frac{\rmd^n}{\rmd \gamma^n}\bigg [ \hat\bfx_{1|\gamma}(\bfx_\gamma)\bigg]_{\gamma = \gamma_t} \int_{\gamma_t}^{\gamma_1}\frac{(\gamma - \gamma_t)^{n}}{n!}\;\rmd \gamma + \mathcal{O}(h^{k+1}),\nonumber\\
        &= \frac{\sigma_1}{\sigma_t}\bfx_t + \sigma_1 \sum_{n=0}^{k-1} \frac{\rmd^n}{\rmd \gamma^n}\bigg [ \hat\bfx_{1|\gamma}(\bfx_\gamma)\bigg]_{\gamma = \gamma_t} \frac{h^{n+1}}{(n+1)!} + \mathcal{O}(h^{k+1}),
    \end{align}
    where $h \coloneq \gamma_1 - \gamma_t$ is the stepsize.
    Let $k=1$, then we have:
    \begin{align}
        \bfx_1 &= \frac{\sigma_1}{\sigma_t}\bfx_n + \sigma_1\hat\bfx_{1|t}(\bfx_n)h + \mathcal{O}(h^2),\\
        &= \frac{\sigma_1}{\sigma_t}\bfx_n + (\alpha_1 - \frac{\sigma_1\alpha_t}{\sigma_t})\hat\bfx_{1|t}(\bfx_n) + \mathcal{O}(h^2).
    \end{align}
    By definition $\sigma_1 = 0$ and $\alpha_1=1$, then
    \begin{equation}
        \bfx_1 = \hat\bfx_{1|t}(\bfx_n) + \mathcal{O}(h^2),
    \end{equation}
    which is equivalent to
    \begin{equation}
        \|\bfx_1 - \hat\bfx_{1|t}(\bfx_n)\| \leq C_1 h^2,
    \end{equation}
    for some constant $C_1 > 0$.
    Since $\hat\bfx_{1|t}(\bfx_n) \to \bfx_1^*$ we know that for any $\epsilon > 0$ there exists some $n \geq N$ such that $\|\bfx_1^* - \hat\bfx_{1|t}(\bfx_n)\| < \epsilon$.
    Thus,
    \begin{equation}
        \|\bfx_1 - \bfx_1^*\| \leq \|\bfx_1 - \hat\bfx_{1|t}(\bfx_n)\| + \|\bfx_1^* - \hat\bfx_{1|t}(\bfx_n)\| < \underbrace{\epsilon + C_1h^2}_{\coloneq C_2}.
    \end{equation}
    Thus the error between the optimal solution $\bfx_1^*$ and the solution from the flow $\bfx_1$ is $\mathcal{O}(h^2)$ thereby finishing the proof.
\end{proof}

\section{Proof of \texorpdfstring{~\cref{thm:greedy_is_implicit_euler}}{Greedy as an implicit Euler method}}
\label{proof:greedy_is_implicit_euler}
We restate~\cref{thm:greedy_is_implicit_euler} here:
\begin{theorem}[Greedy as an implicit Euler method]
    For some trajectory state $\bfx_t$ at time $t$, the greedy gradient given by $\nabla_{\bfx_t} \mathcal{L}(\hat\bfx_{1|t}(\bfx_t))$ is an implicit Euler discretization of the continuous adjoint equations for the true gradients with step size $h = \gamma_1 - \gamma_t$.
\end{theorem}


For clarity we restate the definition of the continuous adjoint equations.
Let $\bfu_\theta \in \mathcal{C}^{1,1}([0,1] \times \R^d;\R^d)$ be a model that models the vector field of some ODE and be Lipschitz continuous in its second argument. Let $\bfx: [0,1] \to \R^d$ be the solution to the ODE with the initial condition $\bfx_0 \in \R^d$, $\dot \bfx_t = \bfu_\theta(t, \bfx_t)$.
For some scalar-valued loss function $\mathcal{L} \in \mathcal{C}^2(\R^d)$ in $\bfx_1$, let $\bfa_\bfx \coloneq \partial\mathcal{L}/\partial \bfx_t$ denote the gradient.
Then $\bfa_\bfx$ and related quantity $\bfa_\theta \coloneq \partial\mathcal{L}/\partial \theta$ can be found by solving an augmented ODE of the form,
\begin{equation}
    \begin{aligned}
    \bfa_\bfx(1) &= \frac{\partial \mathcal{L}}{\partial \bfx_1}, \quad &&\frac{\rmd \bfa_\bfx}{\rmd t}(t) = -\bfa_\bfx(t)^\top \frac{\partial \bfu_\theta}{\partial \bfx}(t, \bfx_t),\\
    \bfa_\theta(1) &= \mathbf 0, \quad &&\frac{\rmd \bfa_\theta}{\rmd t}(t) = -\bfa_\bfx(t)^\top \frac{\partial \bfu_\theta}{\partial \theta}(t, \bfx_t).
    \end{aligned}
\end{equation}
Now we present the proof.

\begin{proof}
The adjoint state can be simplified by rewriting the vector field in terms of the denoiser to find
\begin{equation}
    \frac{\rmd \bfa_\bfx}{\rmd t}(t) = -a_t\bfa_\bfx(t) - b_t \bfa_\bfx(t)^\top \frac{\partial \hat\bfx_{1|t}(\bfx_t)}{\partial \bfx_t}.
\end{equation}
We can express this \textit{backwards-in-time} ODE as an integral equation in the form of
\begin{align}
    \bfa_\bfx(s) &= \bfa_\bfx(t) - \int_t^s a_\tau \bfa_\bfx(t) + b_\tau \bfa_\bfx(\tau)^\top \frac{\partial \hat\bfx_{1|\tau}(\bfx_\tau)}{\bfx_\tau}\;\rmd \tau,\nonumber\\
    &= \bfa_\bfx(t) + \int_s^t a_\tau \bfa_\bfx(t) + b_\tau \bfa_\bfx(\tau)^\top \frac{\partial \hat\bfx_{1|\tau}(\bfx_\tau)}{\partial \bfx_\tau}\;\rmd \tau. \qquad \textrm{(time-reversal)}
\end{align}
Using the technique of exponential integrators we rewrite the integral as
\begin{align}
    \bfa_\bfx(s) &= e^{\int_s^t a_u\;\rmd u}\bfa_\bfx(t) + \int_s^t e^{\int_\tau^t a_u\;\rmd u}b_\tau\bfa_\bfx(\tau)^\top\frac{\partial \hat\bfx_{1|\tau}(\bfx_\tau)}{\partial \bfx_\tau}\;\rmd \tau,\nonumber\\
    &= \frac{\sigma_t}{\sigma_s}\bfa_\bfx(t) + \sigma_t\int_s^t \frac{b_\tau}{\sigma_\tau}\bfa_\bfx(\tau)^\top\frac{\partial \hat\bfx_{1|\tau}(\bfx_\tau)}{\partial \bfx_\tau}\;\rmd \tau,\nonumber\\
    &= \frac{\sigma_t}{\sigma_s}\bfa_\bfx(t) + \sigma_t\int_{\gamma_s}^{\gamma_t} \bfa_\bfx(\gamma)^\top\frac{\partial \hat\bfx_{1|\gamma}(\bfx_\gamma)}{\partial \bfx_\gamma}\;\rmd \gamma.
\end{align}
By~\cref{assump:total_derivs} it follows that the vector-Jacobian product has $(k+1)$-th total derivatives, allowing us to define a first-order Taylor expansion around $\gamma_s$:
\begin{equation}
    \bfa_\bfx(s) = \frac{\sigma_t}{\sigma_s}\bfa_\bfx(t) + (\alpha_t - \frac{\sigma_t}{\sigma_s}\alpha_s)\bfa_\bfx(s)^\top\frac{\partial\hat\bfx_{1|s}(\bfx_s)}{\partial \bfx_s} + \mathcal{O}(h^2).
\end{equation}
Thus the first-order approximation of the adjoint state at time $t$ with a stepsize of $h = \gamma_1 - \gamma_t$ is the implicit equation
\begin{equation}
    \bfa_\bfx(t) = \bfa_\bfx(t)^\top\frac{\partial\hat\bfx_{1|t}(\bfx_t)}{\partial \bfx_t}.
\end{equation}
Now to solve the implicit equation we can use the fixed-point iteration method. Let $\bfa_\bfx(t)^{(0)} = \bfa_\bfx(1)$, then the first iteration has
\begin{equation}
    \bfa_\bfx(t)^{(1)} = \bfa_\bfx(1)^\top\frac{\partial\hat\bfx_{1|t}(\bfx_t)}{\partial \bfx_t} = \nabla_{\bfx_t} \mathcal{L}(\hat\bfx_{1|t}(\bfx_t)).
\end{equation}
Thus we have shown that the greedy gradients are equivalent to the first iteration of an implicit Euler discretization of the continuous adjoint equations.

\end{proof}
 
\section{Proof of \texorpdfstring{~\cref{thm:cae_for_control}}{Continuous adjoint equations for control}}
\label{proof:cae_for_control}

We restate~\cref{thm:cae_for_control} here:
\begin{theorem}[Continuous adjoint equations for the control term]
    Let $\bfu_\theta \in \C^{1,1}([0,1]\times \R^d; \R^d)$ be a parameterization of some time-dependent vector field of a neural ODE that is Lipschitz continuous in its second argument, and let $\bfz \in \C^{1}([0,1];\R^d)$ be an additional control signal such that the new dynamics are given by \begin{equation}
        \frac{\rmd \bfx_t}{\rmd t} = \bfu_\theta(t, \bfx_t) + \bfz(t).
    \end{equation}
    Let $\bfa_\bfz(t) \coloneq \partial\mathcal{L}/\partial \bfz(t)$ then
    \begin{equation}
        \bfa_\bfz(t) = - \int_1^t \bfa_\bfx(s) \; \rmd s.
    \end{equation}
\end{theorem}

Our proof follows the structure of the then modern proof of Pontryagin's original result~\citep{pontryagin1963} presented by~\citep{chen2018neural}.

\begin{proof}
    For notational clarity, we use the notation $\bfx(t) = \bfx_t$.
    We define the augmented state on $[0, T]$ as
    \begin{equation}
        \frac{\rmd}{\rmd t} \begin{bmatrix}
            \bfx\\
            \bfz
        \end{bmatrix}(t) = \bsf_{\text{aug}} = \begin{bmatrix}
            \bfu_\theta(t, \bfx(t)) + \bfz(t)\\
            \frac{\rmd \bfz}{\rmd t}(t)
        \end{bmatrix},
    \end{equation}
    and the augmented adjoint state as
    \begin{equation}
        \bfa_{\text{aug}}(t) \coloneq \begin{bmatrix}
            \bfa_\bfx\\
            \bfa_\bfz
        \end{bmatrix}(t).
    \end{equation}
    The Jacobian of $\bsf_{\text{aug}}$ has form
    \begin{equation}
        \frac{\partial \bsf_{\text{aug}}}{\partial [\bfx, \bfz]} = \begin{bmatrix}
            \frac{\partial \bfu_\theta(t, \bfx(t))}{\partial \bfx} & \mathbf 1\\
            \mathbf 0 & \mathbf 0
        \end{bmatrix}.
    \end{equation}
    The evolution of the adjoint state is given by
    \begin{equation}
        \frac{\rmd \bfa_{\text{aug}}}{\rmd t}(t) = -\begin{bmatrix}
            \bfa_\bfx & \bfa_\bfz
        \end{bmatrix}(t) \frac{\partial \bsf_{\text{aug}}}{\partial [\bfx, \bfz]}(t).
    \end{equation}
    Therefore, $\bfa_\bfu(t)$ evolves with
    \begin{equation}
        \bfa_\bfu(T) = \mathbf 0, \qquad \frac{\rmd \bfa_\bfu}{\rmd t}(t) = -\bfa_\bfx(t),
    \end{equation}
    thereby finishing the proof.
\end{proof}






\end{document}
