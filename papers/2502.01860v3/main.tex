\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{url}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{SE Arena: An Interactive Platform for Evaluating Foundation Models in Software Engineering}

\author{\IEEEauthorblockN{Zhimin Zhao}
\IEEEauthorblockA{\textit{School of Computing} \\
\textit{Queen's University}\\
Kingston, Canada \\
z.zhao@queensu.ca}
}

\maketitle

\begin{abstract}
Foundation models (FMs), particularly large language models (LLMs), have shown significant promise in various software engineering (SE) tasks, including code generation, debugging, and requirement refinement. Despite these advances, existing evaluation frameworks are insufficient for assessing model performance in iterative, context-rich workflows characteristic of SE activities. To address this limitation, we introduce \emph{SE Arena}, an interactive platform designed to evaluate SE-focused chatbots. SE Arena provides a transparent, open-source leaderboard, supports multi-round conversational workflows, and enables end-to-end model comparisons. The platform introduces novel metrics, including the \emph{consistency score} that measures model consistency through self-play matches. Moreover, SE Arena incorporates a new feature called \emph{RepoChat}, which automatically injects repository-related context (e.g., issues, commits, pull requests) into the conversation, further aligning evaluations with real-world development processes. This paper outlines the design and capabilities of SE Arena, emphasizing its potential to advance the evaluation and practical application of FMs in software engineering.
\end{abstract}

\begin{IEEEkeywords}
Foundation Model, Software Engineering, Chatbot Arena
\end{IEEEkeywords}

\section{Introduction}

Foundation models (FMs), such as large language models (LLMs), have significantly advanced software engineering (SE) tasks, including code generation~\cite{vaithilingam2022expectation}, debugging~\cite{chenteaching}, and requirement refinement~\cite{borg2024requirements}. However, their effectiveness often depends on complex, iterative workflows, which typically require models to process user feedback, revise responses dynamically, and maintain contextual coherence over multiple turns. Such context-heavy, real-world SE scenarios challenge traditional evaluation methods. Existing frameworks, such as Chatbot Arena\footnote{\url{https://lmarena.ai/?leaderboard}\label{footnote:Chatbot Arena}} and Copilot Arena\footnote{\url{https://github.com/lmarena/copilot-arena}\label{footnote:Copilot Arena}}, have introduced pairwise comparisons to evaluate model preferences. While these frameworks offer utility, they fall short of meeting the iterative and context-specific demands of SE evaluations.

For example, Chatbot Arena\footref{footnote:Chatbot Arena}, while widely referenced, suffers from several limitations that restrict its utility in SE domain. Firstly, it is not open-sourced, which reduces transparency and limits opportunities for community-driven innovation. Moreover, its exclusive reliance on Elo score~\cite{albers2001elo} and average win rate (the probability of a model winning) provides a narrow perspective in model comparison. Additionally, Chatbot Arena does not support multi-round interactions, a critical feature for evaluating FMs in real-world SE tasks. Copilot Arena\footref{footnote:Copilot Arena} also faces similar challenges. 

To address these limitations, we propose \emph{SE Arena}\footnote{\url{https://huggingface.co/spaces/SE-Arena/Software-Engineering-Arena}}, an interactive platform tailored for the evaluation of FMs in SE tasks. SE Arena enhances existing frameworks by incorporating multi-round dialogues and a transparent evaluation methodology, allowing engineers to benchmark FMs across diverse SE workflows. In addition, SE Arena introduces a new feature called \emph{RepoChat} that automatically extracts and injects repository-level context into the conversation, enabling deeper and more realistic benchmarking of FMs for day-to-day software engineering scenarios. This approach fills critical gaps in current evaluation practices, advancing the field toward more nuanced and context-aware assessments. To our knowledge, SE Arena is the first platform to integrate these capabilities for SE-specific evaluations. This paper introduces SE Arena, outlines its key features (including RepoChat), and highlights its potential to address the unique challenges of FM evaluation in SE.

\section{Background}

\subsection{Static Benchmarks for Model Evaluation}

The majority of existing benchmarks for evaluating machine learning (ML) models rely on static, ground truth-based datasets, typically featuring multiple-choice questions or predefined test cases. While these benchmarks cover diverse topics such as language understanding, coding, and logical reasoning, they often fail to capture real-world SE workflows. Notable SE benchmarks include BigCodeBench~\cite{zhuo2024bigcodebench}, DevOps-Eval\footnote{\url{https://github.com/codefuse-ai/codefuse-devops-eval}}, EvalPlus~\cite{liu2024your}, Long Code Arena\footnote{\url{https://huggingface.co/spaces/JetBrains-Research/long-code-arena}}, and SWE-bench~\cite{jimenez2023swe}. Although many of these benchmarks are open-source, they often rely on predefined datasets rather than user-driven evaluations, limiting adaptability to evolving SE tasks. SE Arena addresses this gap by supporting user-generated real-time evaluations in diverse SE scenarios. Static benchmarks are valuable for controlled evaluations but fall short in scenarios requiring interactive and iterative problem-solving~\cite{chiang2024chatbot}. Recognizing this limitation, we introduce SE Arena—the first open, large-scale, crowd-sourced platform designed to evaluate FMs through live human interaction in SE tasks. SE Arena leverages dynamic evaluations to address the gaps left by static approaches, enabling more realistic assessments of model performance in real-world SE contexts.

\subsection{Pairwise Comparisons for Model Evaluation}

Evaluating machine learning (ML) models based on human preferences is a well-established practice~\cite{bai2022training,christiano2017deep}. Pairwise comparisons are particularly effective as they mitigate subjective bias by asking users to choose between two options rather than assigning absolute scores~\cite{zheng2023judging}. Aggregating pairwise outcomes into meaningful rankings is commonly achieved using methods such as the Bradley-Terry model~\cite{hunter2004mm} and the Elo rating system~\cite{albers2001elo}. Evaluation platforms, such as Chatbot Arena\footref{footnote:Chatbot Arena}, have successfully implemented pairwise comparison frameworks, allowing users to engage in direct battles between anonymous models. 

However, Chatbot Arena's current implementation primarily presents the average win rate alongside the Elo score, offering a limited perspective on model evaluation. To address these limitations, SE Arena expands the scope of pairwise comparisons for FMs in SE tasks by incorporating a broader range of evaluation metrics. In addition to traditional metrics such as the Elo score, average win rate, and Bradley-Terry coefficients, SE Arena integrates advanced metrics, including eigenvector centrality value~\cite{bonacich1987power}, PageRank score~\cite{langville2004deeper}, and Newman modularity score~\cite{newman2004fast}, to provide a more robust and multidimensional comparison framework.

Furthermore, SE Arena introduces the consistency score ($C$) which quantifies the percentage of self-play battles where the same model produces outputs of the same quality for identical inputs:
$$C = \frac{D}{N} \times 100\%$$
Here, $D$ represent draws against itself, while $N$ is the total number of self-play matches. Since draws in self-play can only occur when the model gives answers with similar quality to the same question, this formula elegantly captures the frequency of consistent model outputs. This novel metric offers valuable information on model determinism, particularly crucial in SE contexts where reproducible results are often essential.

% To address these limitations, SE Arena expands the scope of pairwise comparisons for FMs in SE tasks by incorporating a broader range of evaluation metrics. In addition to traditional metrics such as the Elo score, average win rate, and Bradley-Terry coefficients, SE Arena integrates advanced metrics, including eigenvector centrality value~\cite{bonacich1987power}, PageRank score~\cite{langville2004deeper}, and Newman modularity score~\cite{newman2004fast}, to provide a more robust and multidimensional comparison framework.
% \begin{itemize}
%     \item \textbf{Eigenvector centrality value} evaluates a model's influence within the overall comparison network. Unlike simpler metrics like the Elo score, which focuses on individual match outcomes, eigenvector centrality considers the quality of opponents a model defeats~\cite{bonacich2007some,ruhnau2000eigenvector}. This metric emphasizes models that consistently perform well against highly-rated competitors, highlighting their dominance in competitive interactions.
    
%     \item \textbf{PageRank score}, originally developed for ranking web pages, offers a probabilistic approach to evaluating relative importance in a network by considering not only the direct connections (wins or losses) but also the importance of the models connected to them~\cite{bianchini2005inside}. In the context of SE Arena, PageRank offers an advantage over eigenvector centrality by accounting for cyclic dependencies and emphasizing models that perform well within densely connected sub-networks, ensuring stability in rankings even in scenarios with complex interdependencies among models~\cite{gleich2015pagerank}.
    
%     \item \textbf{Newman modularity score} identifies the community structures within the comparison network by grouping the models into clusters based on similar performance patterns~\cite{battiston2020networks}. This metric provides valuable information on task specialization, allowing users to identify clusters of models that excel in specific domains or workflows, such as debugging or requirement refinement~\cite{abbe2018community}. This clustering might help users understand the relative strengths and weaknesses of the models, particularly when selecting models tailored to their domain-specific needs.
% \end{itemize}

Using these advanced metrics, SE Arena provides a richer and more nuanced understanding of model performance. This multidimensional evaluation framework empowers users to make informed decisions, balancing global dominance (captured by metrics such as eigenvector centrality and PageRank) with domain-specific competencies (revealed by Newman modularity) and consistency characteristics (quantified by the consistency score).

\subsection{Iterative and Multidimensional Characteristics of Software Engineering Tasks}

Software engineering is a multifaceted discipline that extends beyond code generation. It includes activities such as requirements engineering, release engineering, software project management, and collaborative design~\cite{biolchini2005systematic}. These diverse domains require evaluation platforms capable of addressing a wide variety of tasks. However, existing frameworks, such as Chatbot Arena\footref{footnote:Chatbot Arena} and Copilot Arena\footref{footnote:Copilot Arena}, primarily focus on specific aspects like code generation and completion. While valuable, these platforms do not capture the full breadth of SE tasks, leaving a gap in understanding the broader scope and challenges of SE workflows.

A crucial factor that distinguishes SE tasks from other domains is their inherently iterative and dynamic nature. For example, in an interactive debugging session, a model may initially suggest a fix, but user feedback might indicate an unexpected failure, requiring the model to refine its solution over multiple exchanges. Similarly, in requirement engineering, an initial model-generated specification may undergo multiple refinements based on stakeholder feedback before reaching an acceptable state. These scenarios highlight the need to evaluate FMs on their ability to sustain meaningful multi-round interactions.

% Based on the limitations of existing platforms, it is important to recognize that SE workflows often involve cycles of feedback, refinement, and collaboration to adapt to evolving requirements and changing contexts~\cite{abrahamsson2017agile,brooks1995mythical}. For example, debugging a complex system requires revisiting assumptions, incorporating new insights, and iteratively testing hypotheses, a process that is poorly suited to single-turn evaluations. These limitations highlight the need for platforms capable of capturing multi-round interactions reflective of real-world SE practices.

By addressing both the breadth of SE domains and the iterative nature of SE workflows, SE Arena bridges critical gaps in existing evaluation methodologies. Its support for multi-round interactions and a wide range of SE tasks—augmented by repository-level contextual data through RepoChat—provides a more comprehensive and actionable framework for benchmarking foundation models in SE.

\section{Interface}

SE Arena collects user feedback through crowd-sourced evaluations to rank FMs in SE tasks. Our goal is to provide an intuitive and user-friendly interface to minimize friction for users who contribute data. The platform adopts a pairwise comparison mechanism in which users compare the responses of two models and vote for the better one, without requiring absolute scores. This approach ensures consistency among the various participants and simplifies the evaluation process.

\subsection{First Round: Optional Repository-Aware Interaction}

Figure~\ref{fig:arena-first} illustrates the components of the SE Arena panel during the first round of interaction. Users begin by signing in, after which they can enter their first prompt in the designated text box. The prompts typically relate to SE issues, such as debugging, re-finement of requirements, or code review. Once submitted, two anonymous FMs are randomly selected from a preconfigured model pool to ensure fair pairwise comparisons.

\begin{figure*}
\centering
\includegraphics[width=\linewidth]{figures/arena-first-round.png}
\caption{SE Arena interface for the first-round user query.}
\label{fig:arena-first}
\centering
\end{figure*}

To enhance the applicability of the evaluation, SE Arena introduces RepoChat, an optional feature that injects repository-related context into the conversation. Users can provide a repository-related URL (e.g., a GitHub or GitLab repository, issue, discussion, commit, or pull/merge request). If supplied, SE Arena retrieves relevant metadata, such as repository description, programming language, issue discussions, or commit diffs. These contextual data are added to the user query, forming a consolidated prompt. If no repository URL is provided, SE Arena functions as usual, processing only user direct input.

RepoChat reduces the need for users to manually provide extensive contextual details, ensuring that model evaluations reflect real-world SE workflows more accurately. By seamlessly integrating repository-aware context, SE Arena extends beyond traditional static prompts, enabling more meaningful assessments of FMs in software engineering tasks.

% \subsection{RepoChat: Repository-Aware Interaction}

% A core objective of SE Arena is to capture authentic development workflows. To this end, we introduce \emph{RepoChat}, a feature that automatically injects repository-level context into the user's query. Specifically, a user can optionally provide a repository-related URL, such as a GitHub or GitLab repository, an issue or discussion URL, or even a commit or pull (merge) request link. 

% When the user provides such a URL, SE Arena fetches relevant metadata about the repository or the specific artifact. Examples of contextual data include, but are not limited to:
% \begin{itemize}
%     \item \textbf{Repository Metadata:} Name, description, programming language, and key topics or labels.
%     \item \textbf{Issues and Pull/Merge Requests:} Titles, descriptions, comments, and status (open/closed/merged).
%     \item \textbf{Commit Information:} Commit messages, authors and diffs.
% \end{itemize}

% These contextual data are appended (or \emph{injected}) directly to the user query \emph{before}, forming a consolidated prompt. The paired models thus receive both the repository-related information and the user's actual question or instruction, enabling them to provide more targeted and context-aware responses. If users do not provide a repository URL, SE Arena functions as in the standard mode, with the conversation relying solely on the user's input.

% By automatically integrating domain-relevant data, RepoChat reduces the burden on users to manually paste large amounts of contextual information. This approach ensures that real-world tasks, such as discussing an issue in an existing project or reviewing a specific commit, are reflected in the evaluation process. Consequently, SE Arena goes beyond simplistic or standalone prompts to simulate realistic software engineering workflows more faithfully.

\subsection{Multi-Round Conversation Panel}

After the initial round, the interface transitions into a multi-round conversation panel, as shown in Figure~\ref{fig:arena-multi}. This feature allows users to continue the dialogue with both models by asking follow-up questions based on their initial responses. The iterative process enables a deeper evaluation of each model's ability to handle context-dependent and long-term SE workflows. Users can submit their votes at any time. To mitigate potential bias due to initial impressions (e.g., favoring the first-turn response), SE Arena introduces a re-assessment feature, allowing users to modify their votes after reviewing multiple turns.

\begin{figure*}
\centering
\includegraphics[width=\linewidth]{figures/arena-multi-round.png}
\caption{SE Arena interface for multi-round conversations between users and SE chatbots.}
\label{fig:arena-multi}
\centering
\end{figure*}

All interactions remain anonymous, with chatbot identities concealed throughout the session. Votes are deemed valid only if the submitted queries are relevant to SE tasks. To enforce this, SE Arena implements a guardrail mechanism that uses \texttt{GPT-4o-mini}\footnote{\url{https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence}} to automatically filter out non-SE-related prompts at submission time, ensuring that evaluations remain focused on meaningful SE scenarios.

When user input exceeds the maximum context window supported by the tested models, SE Arena applies a first-in, first-out (FIFO) strategy to remove the oldest interactions, ensuring the conversation remains within the allowable context window. For practicality, the maximum response time for a model is capped at one minute. If a model exceeds this limit, the user cannot submit additional input for that model.

\subsection{Leaderboard Integration and Data Privacy}

SE Arena provides a transparent leaderboard that is updated immediately after a user submits their vote. Our platform incorporates a diverse range of evaluation metrics, ensuring that the leaderboard reflects both global model performance and specialized capabilities across various evaluation dimensions. The novel consistency score enables users to consider both performance and determinism when selecting FMs for specific SE tasks, allowing them to prioritize low instability FMs for mission-critical code generation or focus on high-performance FMs regardless of consistency for exploratory tasks.

To encourage diverse participation, SE Arena allows users to submit a wide range of real-world SE scenarios, without restriction on prompt content. Before accessing our platform, users must agree to the terms of use, including consent for anonymized data collection and public release. This approach balances the need for robust data collection to support research with the protection of user privacy, ensuring that SE Arena remains a valuable and ethical resource for the software engineering community.

\section{Future Works}

SE Arena represents a significant step forward in the evaluation of FMs for SE tasks. However, there are several opportunities for future enhancements to expand its applicability and improve its effectiveness.

\begin{itemize}
    \item \textbf{Analysis of Real-World SE Workloads}: SE Arena plans to analyze user-submitted requests to find common patterns and challenges in software engineering tasks. This analysis could lead to specialized sub-leaderboards focused on particular domains, such as debugging or requirement refinement, and inform the development of more targeted FMs.

    \item \textbf{Developing Multi-round-oriented Evaluation Metrics}: SE Arena intends to incorporate metrics specifically tailored to multi-round interactions. Examples include round-wise user feedback analysis to assess how models adapt and improve over successive turns.

    \item \textbf{Enhancing Community Engagement}: SE Arena intends to promote contributions from the broader research and development community. This includes enabling users to vote on model performance, contributing to the open-source codebase, and sponsoring initiatives aligned with our platform's objectives.

    \item \textbf{Expanding FM Coverage and Supporting Infrastructure}: SE Arena aims to broaden coverage to include domain-specific models and multimodal foundation models, along with infrastructure for more complex SE tasks, including web browsing and API integration.

    \item \textbf{Integrating Advanced Context Compression Techniques}: SE Arena intends to incorporate advanced context compression methods to address the challenges of retaining and summarizing long interaction histories. Techniques like LongRope~\cite{dinglongrope} or SelfExtend~\cite{jinllm} could be used to preserve essential context while managing memory constraints, ensuring evaluations accurately reflect real-world SE workflows.
\end{itemize}

% \section*{Acknowledgments}
% This research is supported by anonymous funding sources. We thank the SE and FM research communities for their inspiration and support.

\bibliographystyle{plain}
\bibliography{ref}

\end{document}
