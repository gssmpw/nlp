\begin{comment}
\begin{table*}[t!]
  \caption{Results of embodied vision-and-navigation.}
    \label{tab::task4}
  \centering
      \begin{tabular}{cccccccccc}
        \toprule
        \multirow{2}*{Model} & \multicolumn{3}{c}{val\_seen} & \multicolumn{3}{c}{val\_unseen}  & \multicolumn{3}{c}{test}    \\
        \cmidrule(r){2-4}
        \cmidrule(r){5-7}
        \cmidrule(r){8-10}
        
        ~     & SR/\%       & SPL/\%   & NE/m  & SR/\%       & SPL/\%   & NE/m  & SR/\%   & SPL/\%   & NE/m \\
        \midrule
        Random        & - & - & - & -  & -  & - & -  & - & - \\
        Seq2Seq       & - & - & - & -  & -  & - & -  & - & - \\ 
        CMA           & - & - & - & -  & -  & - & -  & - & - \\
        AerialVLN     & - & - & - & -  & -  & - & -  & - & - \\
        STMR          & - & - & - & -  & -  & - & -  & - & - \\
        NaVid         & - & - & - & -  & -  & - & -  & - & - \\
        Ours          & - & - & - & -  & -  & - & -  & - & - \\
        \bottomrule
      \end{tabular}
\end{table*}
\end{comment}



\begin{table*}[t!]
\centering
\caption{Comparison results on the test-unseen split.}
%\vspace{-5pt}
\label{tab:unseen_results}
\begin{adjustbox}{center}
\resizebox{\textwidth}{!}{ 
%\setlength{\tabcolsep}{1.6pt}
\renewcommand{\arraystretch}{1.3}
% \scalebox{0.95}{
\begin{tabular}{lccccccccccccccccc}
\toprule
\multirow{2}{*}{Method} & \multicolumn{4}{c}{Easy} & \multicolumn{4}{c}{Moderate} & \multicolumn{4}{c}{Hard} & \multicolumn{4}{c}{Total}\\ 
\cmidrule(lr){2-5} \cmidrule(lr){6-9} \cmidrule(lr){10-13} \cmidrule(lr){14-17}
& NE$\downarrow$ & SR$\uparrow$ & OSR$\uparrow$ & SPL$\uparrow$ 
& NE$\downarrow$ & SR$\uparrow$ & OSR$\uparrow$ & SPL$\uparrow$ 
& NE$\downarrow$ & SR$\uparrow$ & OSR$\uparrow$ & SPL$\uparrow$
& NE$\downarrow$ & SR$\uparrow$ & OSR$\uparrow$ & SPL$\uparrow$ \\ \midrule 

Random & 321m & 0.1\% & 0.2\% & 0\%  & 231m & 0.1\% & 0.1\% & 0\% & 371m & 0\% & 0\% & 0\% & 301m & 0.1\% & 0.1\% & 0\%\\
Seq2Seq\cite{VLN-CE}&  298m & 0.1\% &  1.1\% & 0.8\% & 286m & 0.2\% & 7.6\% & 0.4\% & 291m & 0.1\% & 0.9\% & 0.6\% & 291m & 0.1\% & 3.2\% & 0.6\%\\
CMA\cite{VLN-CE}& 278m & 1.1\% &  9.1\% & 1.2\% & 234m & 0.9\% & 9.8\% & 0.6\% & 293m & 0.1\% & 1.0\% & 0.4\% & 268m & 0.7\% & 6.6\% & 0.7\%\\
AerialVLN\cite{aerialVLN}& 263m & 3.2\% &  21.4\% & \underline{3.2\%} & \underline{225m} & 1.3\% & 19.8\% & 1.2\% & \underline{195m} & \underline{2.5\%} & \underline{18.2\%} & 2.1\% & 227m & 2.3\% & \underline{19.8\%} & \underline{2.2\%} \\
%STMR\cite{stmr}& 226m & 10.5\% & 36.8\% & 9.5\% & 206m & 6.9\% & 31.0\% & 5.5\% & 115m & 0\% & 7.1\% & 0\% & 205m & 8.0\% & 31.0\% & 7.0\% \\
Navid\cite{navid}& \underline{259m} & \underline{5.6\%} & \underline{27.1\%} & 2.9\% & 268m & \textbf{2.8\%} & \underline{20.6\%} & \underline{1.2\%} & 196m & 1.9\% & \textbf{20.2\%} & \textbf{3.7\%} & \underline{200m} & \underline{2.6\%} & 16.9\% & 2.0\% \\
Ours & \textbf{219m} & \textbf{9.1\%} & \textbf{29.8\%} & \textbf{7.0\%} & \textbf{209m} & \underline{2.3\%} & \textbf{36.4\%} & \textbf{1.4\%} & \textbf{170m} & \textbf{4.3\%} & 16.5\% & \underline{2.6\%} & \textbf{198m} & \textbf{5.1\%} & \textbf{27.3\%}  & \textbf{3.5\%} \\
\bottomrule
\end{tabular}
}
\end{adjustbox}
\end{table*}



\begin{table}[t!]

\centering

\caption{Ablation study on the test-seen split. `KS' and `VTM' denote keyframe selection and visual token merging, respectively.}
\label{tab:ablation}
\begin{adjustbox}{center}
%\setlength{\tabcolsep}{1.6pt}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lcccc}
\toprule
Method& NE$\downarrow$ & SR$\uparrow$ & OSR$\uparrow$ & SPL$\uparrow$ \\ \midrule 

OpenVLA\cite{openvla} (baseline) & 196m & 3.6\% & 12.4\% & 1.9\% \\
History + VTM & 215m & 12.7\% & 39.6\% & 6.4\%\\
KS& 275m & 6.2\% & 24.1\% & 5.6\% \\
KS + VTM & 115m & 18.5\% & 50.9\% & 12.2\%\\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}


\section{Experiments}

\subsection{Implementation and Training Details}
The proposed OpenFly-Agent is composed of the Dino-SigCLIP (224 $\times$ 224 pixels) as a vision encoder and the pretrained Llama-2 (7B) as a language model. The visual tokens extracted by Dino-SigLIP are aligned to the same dimension as text embeddings using a projection layer, and then fed into the Llama-2 model. The current frame during flight remains 256 tokens and all historical keyframes are compressed into 1 token. The capacity $K$ of the history memory bank is set to 2 in our experiment. For action prediction, the last 256 tokens in the vocabulary are used as special tokens for action representation. 


\subsection{Evaluation Metrics}
Four standard metrics in VLN tasks are adopted to evaluate different methods, \emph{i.e.,}  navigation error (NE), success rate (SR), oracle success rate (OSR), and success weighted by path length (SPL). NE measures the average deviation between the UAV's final stopping point and the ground-truth destination. SR calculates the proportion of successful tasks, where a task is considered successful if the UAV stops within 20 m of the target. In OSR, if any point on the trajectory is within 20 m of the target, the task can be considered successful. SPL calculates the success rate weighted by the ratio of the ground-truth path length to the actually-executed path length.


\subsection{Results}
\subsubsection{Quantitative results}
%We present the performance of our OpenFly-Agent with several VLN methods. 
We test the proposed OpenFly-Agent on the test-seen and test-unseen splits and differentiate among easy, moderate, and hard difficulty levels.
Multiple VLN methods are compared with ours, where `Random' means an agent randomly selects one action to execute until the `stop' action is chosen, Seq2Seq~\cite{VLN-CE} adopts a simple recurrent model for action sequence prediction, CMA~\cite{VLN-CE} uses a bi-directional LSTM to implement cross-modal attention between instructions and images, AerialVLN~\cite{aerialVLN} combines CMA and a look-ahead guidance strategy to enhance robustness. Navid~\cite{navid} leverages the strong ability of VLMs and develops a video-based architecture, achieving impressive results in indoor VLN. 
%STMR~\cite{stmr} designs a matrix representation, improving the reasoning ability of VLMs for aerial VLN. 

Table.~\ref{tab:seen_results} shows quantitative results on the test-seen split. Seq2Seq, CMA, and AerialVLN achieve limited success rates. Counterintuitively, they perform worse on easy samples. We find that their OSR is not low while they are difficult to stop at a short-range distance. In contrast, Navid obtains better results, demonstrating the great potential of VLMs in aerial VLN. Our OpenFly-Agent achieves the best performance for its strong VLM backbone and the proposed strategies, while there is still much room for improvement.
%Although in STMR~\cite{stmr}, GPT-4o exhibits robust capabilities in vision and language processing, the lack of fine-tuning on certain navigation tasks prevents it from learning.
Table.~\ref{tab:unseen_results} presents quantitative results on the test-unseen split, illustrating the generalization capabilities of these VLN methods. Similarly, our method achieves better performance, exhibiting a certain degree of robustness. However, all methods are significantly degraded, which indicates models with greater generalization are urgently needed to be developed.   
%showcasing performance that is comparable to other methods. Additional, it outperforms approaches such as Navid and AerialVLN in terms of the SR metric across various levels of difficulty.

\begin{figure}[t]
\centering
    \includegraphics[width=0.98\linewidth]{Fig/demo.pdf}

    \caption{Illustration of an aerial VLN trajectory generated by OpenFly-Agent, which successfully predicts actions following the instruction when encountering landmarks.}
    \label{fig:example}
    %\vspace{-5pt}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.95\linewidth]{Fig/failure_case.pdf}
    \caption{Illustration of failure cases. Sometimes our model may misclassify key landmarks or output wrong actions.}
    \label{fig:fail}
\end{figure}


\subsubsection{Qualitative results}
Fig.~\ref{fig:example} presents a qualitative result, where our OpenFly-Agent successfully navigates to the destination according to the instruction. It presents a powerful capability in perceiving environments and aligning observations with complex instructions. In addition, Fig.~\ref{fig:fail} shows two failure cases, where our model fails to identify the landmark or output actions with proper amplitudes.

% The example shows that history information indeed helps learn better representations.

\subsection{Ablation Study}
Ablation studies are conducted to evaluate the contribution of the keyframe selection and visual token merging in OpenFly-Agent. Table.~\ref{tab:ablation} shows the results, where OpenVLA~\cite{openvla} is our baseline. Using only the current frame as an observation makes OpenVLA perform poorly in the aerial VLN task. From `History + VTM' we can see that historical frames significantly improve the success rate. The keyframe selection strategy further increases the SR from 12.7\% to 18.5\%, demonstrating the effectiveness of key observations. Besides, the comparison between `KS' and `KS + VTM' indicates the great effect of our visual token merging strategy.

%While the heuristic keyframe selection process retains only the visual tokens relevant to instructions, the additional visual token merging can significantly improve the performance by eliminating visual redundancy, significantly outperforming the model baseline by a large margin.