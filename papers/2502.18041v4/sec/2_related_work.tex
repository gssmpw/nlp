\section{Related Works} ~\label{Sec:related_works}


\subsection{Simulators for Embodied AI}
In recent years, many simulators related to embodied AI have been developed, \emph{e.g.,} MuJoCo~\cite{mujoco}, Habitat~\cite{Habitat}, Pybullet~\cite{PyBullet}, Matterport3D~\cite{Matterport3D}, OpenAI Gym~\cite{OpenAI_Gym}, and Issac Gym~\cite{Isaac_Gym}. They allow researchers to learn and validate algorithms in safe virtual environments, significantly advancing the development of embodied intelligence. However, most of them are designed for indoor robotic manipulation and navigation, making them unsuitable as simulators for aerial VLN tasks. Gazebo and AirSim~\cite{airsim} are two popular UAV simulators, while Gazebo suffers from low rendering quality. Combining AirSim and UE is a potential solution to construct aerial VLN scenes. Unfortunately, the official maintenance and updates for AirSim have been discontinued, resulting in a limited availability of compatible assets.
%\footnote{https://gazebosim.org/home}

\subsection{Vision-Language Navigation Datasets}
Numerous datasets have been proposed to accelerate the VLN task. R2R~\cite{R2R} focuses on evaluating agents in unseen buildings and provides discrete navigation options. RxR~\cite{RxR} is a densely annotated VLN dataset, supporting multilingual instructions. TouchDown~\cite{Touchdown} and REVERIE~\cite{REVERIE} have each contributed a dataset from real-life environments, which requires a ground-based agent to follow instructions and find a target object. CVDN~\cite{CVDN} presents a cooperative VLN dataset where agents can access the dialog history of human cooperation for inference. All the above datasets are graph-based where navigable points are predefined. LANI~\cite{LANI} and VLN-CE~\cite{VLN-CE} propose the VLN task in continuous outdoor/indoor environments, enabling agents to move freely to any unobstructed point. Recently, a few works have tried to construct VLN datasets for aerial space. ANDH~\cite{ANDH} establishes a dialogue-based aerial VLN dataset with bird-view images. CityNav~\cite{CityNav} builds on the point cloud data from SensatUrban~\cite{sensaturban} and linguistic annotations from CityRefer~\cite{CityRefer}, which requires a real-world 2D map to help locate specific landmarks in the instruction. AerialVLN~\cite{aerialVLN} and OpenUAV~\cite{openuav} integrate AirSim and UE to create VLN scenes where pilots can control UAVs to generate various trajectories.

\subsection{Vision-Language Navigation Methods}
VLN methods require agents to understand and complete language instructions according to visual observations. In graph-based works~\cite{Xiong_2019,Zhang_2019,Srinivasa_2019,Wang_2020}, agents move among predefined graph nodes. Recently, LLM-driven works~\cite{Navgpt,NavGPT-2,mapgpt} use the reasoning ability of large language models to infer the next node. Although great progress has been achieved, these methods are not applicable in real unknown environments. In contrast, ~\cite{Chang_2021,Kumar_2021,Maksymets_2021,navid} explore VLN in continuous environments for more realistic applications. More recently, a few works attempt to address the problem of aerial VLN based on UAVs, among which AerialVLN~\cite{aerialVLN} proposes a lookahead guidance method to generate more reasonable ground-truth trajectories during training, and STMR~\cite{stmr} designs a matrix representation to improve the spatial reasoning of LLMs.

