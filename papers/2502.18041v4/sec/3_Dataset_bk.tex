\section{OpenFly}
\subsection{Automatic Data Generation Platform}

\subsubsection{Rendering Engine and Data Resource}
\indent \indent To gather a diverse range of high-quality and realistic simulation data, we sourced datasets from various rendering engines, which will be briefly described below.Moreover,we list all the scenes in the dataset along with their area and other relevant information in Table~\ref{tab:All_dataset_information}.

\textbf{Unreal Engine.} UE is a simulation platform capable of providing highly realistic interactive virtual environments. This platform has undergone five iterations, and each version is accompanied by a large, abundant, and high-quality collection of digital assets. In UE5, we use an official sample project named City Sample which provided us with a large urban scene covering $25.3 km^2$ and a smaller one covering $2.7 km^2$. These scenes include various assets such as buildings, streets, traffic lights, vehicles, and pedestrians. Besides, in UE4, we obtained even richer scenes, including 6 high-quality scenes. Specifically, there are two large scenes showcasing the central urban areas of Shanghai and Guangzhou, with areas of about $30.88 km^2$ and $58.56 km^2$ respectively. The remaining four scenes are selected from AerialVLN\cite{aerialVLN}. They have smaller areas for totally about $26.64 km^2$ .These scenes encompass a variety of styles and forms of architecture, ranging from Chinese to Western, and from classical to modern. Additionally, the UE4 engine allows us to make adjustment in scene time to achieve different appearances of scenes under varying lighting conditions.

\textbf{GTA5.} 
GTA5 is an open-world game that is frequently used by computer vision researchers due to its highly realistic and dynamic virtual environment.  The game features a meticulously crafted cityscape modeled after Los Angeles, encompassing various buildings and locations such as skyscrapers, gas stations, parks, and plazas, as well as dynamic traffic flows and changes in lighting and shadows.

\textbf{Google Earth.} 
Google Earth\footnote{earth.google.com} is a virtual globe software developed by Google. It builds a 3D earth model by integrating satellite imagery, aerial photographs, and Geographic Information System(GIS) data. Specifically, it uses various 3D modeling techniques provided by software such as 3D Max and SketchUp to model urban-level scene images into 3D models. These models are then uploaded to Google Earth and stitched together to form continuous 3D scenes for display.
From this engine, we selected four urban scenes covering a total area of $53.60 km^2 $: Berkeley, primarily consisting of traditional neighborhoods; Osaka, which features a mix of skyscrapers and historic buildings; and two areas with numerous landmarks: Washington, D.C., and St. Louis.


\textbf{3D Gaussian Splatting.} As a highly realistic reconstruction method, Hierarchical 3D GS~\cite{kerbl2024hierarchical} uses a hierarchical training and display architecture, making it particularly suitable for rendering large-scale scenes. Due to these features, we use this method to reconstruct and render multiple real scenes.We employ the DJI M30T drone (see Figure ~\ref{fig:M30T}) as the data collection device which offers an automated oblique photography mode, allowing us to acquire a large area of real-world data with minimal manpower. However, under the drone's perspective, choosing the appropriate shooting altitude posed a dilemma, \emph{i.e.,} if the altitude is too low, the sparse point cloud generated during the initialization of the 3D GS reconstruction will be suboptimal due to insufficient feature point matches between photos. On the other hand, if the altitude is too high, the Gaussian reconstruction will result in overly coarse training of details. After multiple attempts, the data collection plan using the M30T was determined as follows. For large-scale block scenes, oblique photography is performed at approximately twice the average building height using the default parameters of the M30T’s wide-angle camera, with a tilt angle of -45°. For landmark buildings with heights significantly different from the average height, additional targeted data collection is conducted at twice their height. This altitude setting can, to a certain extent, ensure both higher-quality point cloud initialization and Gaussian splatting training.

Under the above collection plan, we gathered data from five campuses across three universities, which are East China University of Science and Technology, Northwestern Polytechnical University, and Shanghai Jiao Tong University (referred to as ECUST, NWPU and SJTU).These campus scenes include various types and styles of landmarks, such as libraries, bell towers, waterways, lakes, playgrounds, construction sites, and lawns.The detailed information for the five campuses is also presented in Table~\ref{tab:All_dataset_information}.

\subsubsection{Simulators}

\indent \indent Considering the complexity of using data assets from different engines, we integrated the interfaces from these asset-display simulators and designed a unified simulator interface to directly retrieve image data at given coordinates within the scene. This interface integrates the following simulators.

\textbf{UE4 + AirSim.} Airsim\footnote{https://github.com/microsoft/AirSim} is an open-source simulator developed by Microsoft for testing autonomous systems. It provides highly realistic simulated environments for drones and self-driving cars. Meanwhile, it can also offer RGB, depth, and segmentation images with realistic physics and sensor models. The AirSim plugin can be integrated into UE4, enabling us to obtain image data from the perspective of drones in simulation urban scenes easily.

\textbf{UE5 + UnrealCV.} Since AirSim does not support UE5 and stopped updating in 2022, we use UnrealCV\cite{unrealcv} as an alternative to obtain images in UE5. UnrealCV is an open-source plugin for Unreal Engine, providing a simple interface for capturing RGB, depth, and segmentation images, thereby facilitating research in computer vision and robotics.

\textbf{GTA5 + Script Hook V.} Script Hook V is a third-party library with the interface to GTA5's native script functions. Using Script Hook V, we can control drones to arbitrary positions in the game and obtain RGB and depth images.

\textbf{Google Earth Studio.}
Google Earth Studio is a web-based animation and video production tool developed by the Google Earth team. It expands upon the Google Earth browsing interface by integrating features commonly found in video production software, such as Adobe Premiere. These enhancements allow us to create keyframes and set camera target points on the 2D and 3D maps of Google Earth using a timeline. By doing so, we can quickly generate tour videos of scenes, from customized routes and angles. The tool also supports exporting the output as MP4 files or as frame image sequences.

\textbf{3D GS + SIBR viewers.} SIBR\cite{sibr2020} viewers is a rendering tool designed for the 3D GS project, enabling the visualization of scenes reconstructed using Gaussian Splatting. The tool supports high-frame-rate scene rendering and provides various interactive modes for navigation. Building upon SIBR viewers, we developed an HTTP RESTful API that generates RGB images from arbitrary poses, simulating a drone's perspective.



\subsubsection{Structured Scene Construction}
\indent \indent In order to achieve automatic trajectory generation, it is necessary to structure the scene, which involves obtaining the semantic segmentation and 3D point cloud for path planning.


\textbf{3D point cloud.} 
For different simulators, we use three methods to obtain 3D point cloud information from the scenes, \emph{i.e.,} depth map back-projection, LiDAR scan reconstruction, and sparse reconstruction. For the UE5+UnrealCV simulator, a project named MatrixCity~\cite{li2023matrixcity} provides us with the depth maps and camera parameters of small-city and big-city datasets. Through back-projecting the 2D depth information into 3D space, we generate the point clouds for the two datasets. For the UE4+AirSim and GTA5 simulators, we directly utilize the LiDAR sensors provided by the simulators to traverse each scene in a grid pattern, obtaining local point clouds. These are then transformed into the global coordinate system using the LiDAR coordinate information and finally merged into complete scene point clouds. In 3D GS, since the first step of Gaussian Splatting reconstruction involves using the open-source project COLMAP \cite{colmap} to perform sparse structure-from-motion (SFM) point cloud reconstruction based on input images, we could directly export and use the point clouds obtained from this step.

\begin{table*}[t]
\caption{\textbf{Dataset information}}
\label{tab:All_dataset_information}
\centering
\begin{tabular}{cccccccccc}
\toprule
\multirow{2}*{Measure} & \multicolumn{6}{c}{UE4} & \multicolumn{2}{c}{UE5} & \multicolumn{1}{c}{GTA5}  \\ 
\cmidrule(r){2-7}
\cmidrule(r){8-9}
\cmidrule(r){10-10}
~     & UE_1 & UE_2 & UE_3 & UE_4 & UE_5 & UE_6 & Big City & Small City & L.S. & \\
\midrule
Area(km^2) & - & - & - & -  & - & - & -  & - & - \\
Pictures & - & - & - & -  & - & - & -  & - & - \\
Data quantity & - & - & - & -  & - & - & -  & - & - \\
\midrule
\multirow{2}*{       } & \multicolumn{5}{c}{3D GS} &  \multicolumn{4}{c}{Google Earth Studio} \\
\cmidrule(r){2-6}
\cmidrule(r){7-10}
~  &\makecell{SJTU \\ (Minghang  East Zone)} &\makecell{SJTU \\ (Minghang West Zone)} &\makecell{NWPU \\ (Youyi)} &\makecell{NWPU \\ (Changan)} &\makecell{ECUST \\ (Fengxian)} & Berkely & Washington,D.C. &St Louis & Osaka\\
Area(km^2) & 1.72 & 0.95 & - & -  & - & - & -  & - & - \\
Pictures & 20934 & 9536 & - & -  & - & - & -  & - & - \\
Data quantity & - & - & - & -  & - & - & -  & - & - \\
\bottomrule
\end{tabular}
\end{table*}




\begin{figure}[t]
\begin{center}
   \includegraphics[width=0.8\linewidth]{Fig/DJI_M30T.jpg}
\end{center}
   \caption{DJI M30T, the device used for collecting real world data.}
\label{fig:M30T}
\end{figure}


\subsubsection{General Interface.} 
To facilitate the collection of trajectories and images across various simulation environments, the OpenFly platform integrates all the aforementioned simulators and provides unified interfaces that enable interaction between an agent and the environment in any scene. Specifically, there are four main interfaces. 1) \textbf{Agent Movement Interface}. We designed the \textit{CoorTrans} module to unify the coordinate systems across all scenes. Through this interface, the agent's position can be set in the aligned coordinate system among regular scenes, point cloud, and segmentations using either absolute poses [X, Y, Z] [QW, QX, QY, QZ] or relative poses in the agent's body frame [d_x, d_y, d_z] and [d_roll, d_pitch, d_yaw].


\subsubsection{Multimodal Trajectory Generation}

\subsubsection{Automatic Instrction Generation}

%\subsubsection{Evaluation Interface}

\subsection{Data Analysis}