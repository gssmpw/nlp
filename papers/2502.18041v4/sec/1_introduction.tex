\section{Introduction} ~\label{Sec:Introduction}

%The emergence of large-scale, powerful visual-language models (VLMs) has significantly transformed the fields of natural language processing and computer vision by integrating their respective strengths, enabling unprecedented cross-modal applications. These advancements have facilitated remarkable progress in visual-language navigation. Unmanned aerial vehicles (UAVs), compared to ground-based systems, present unique challenges and opportunities due to their increased degrees of freedom and expansive 3D fields of view.

%While Vision-and-Language Models (VLMs) possess substantial inferential and image comprehension capabilities, they are predicated on vast repositories of internet-based interleaved text-image data. Consequently, they exhibit limitations in their ability to comprehend aerial perspectives and flight videos. To effectively integrate VLMs into drone-based systems, there is a critical need for reliable and standardized aerial perspective language navigation data. Annotating command-trajectory pairs from a UAV's perspective using diverse Internet data is both costly and labor-intensive, compounded by the geographical limitations of available UAV video data. This raises a critical question: Can high-quality visual-language-trajectory matches be achieved while ensuring diverse and realistic scene representation? Synthetic data offers a promising solution to these challenges.  This paper introduces a visual-language navigation framework for UAVs, termed OpenFly. Current 3D modeling technologies enable the generation of diverse landscapes with semantic labels for land cover and detailed height values.

%We propose OpenFly, a high-quality, high-resolution synthetic dataset, along with a novel real2sim2real approach. This method involves utilizing 3D ground sampling (3D-GS) to derive 2D images from real-world scenes, reconstructing their 3D models, and importing them into simulation platforms to create synthetic data.

%To tackle the challenges presented by the OpenFly benchmark and enhance the efficiency of UAVs in language-guided navigation tasks, we introduce a new baseline framework that advances aerial VLM research and establishes benchmarks for visual-language navigation (VLN) strategies. Leveraging Grounding Vision-Language Models' effective commonsense reasoning ability, we employs an autoregressive action-decoding training framework. To address the issue of redundant trajectory observation causing massive irrelevant encoding tokens, we incorporates an adaptive frame-assisted token-sampling mechanism to mitigate visual attention distraction and enhance accuracy. The efficacy of our methodology is substantiated through extensive closed-loop assessments conducted across a multitude of environments in our benckmark.

%Our study evaluates the performance of OpenFly in natural language navigation, measuring success rates for UAV-based navigation using monocular cameras across real and synthetic environments. To ensure comprehensive and reliable evaluation, OpenFly includes datasets covering over \quad unique scenes and \quad trajectories, applied across\quad publicly accessible VLN frameworks, encompassing general-purpose and specialized models, both open-source and commercial.

\begin{figure*}
    \centering
    \includegraphics[width=0.9\linewidth]{Fig/toolchain.pdf}
    \caption{Framework of the automatic data generation platform. Various rendering engines and simulators are first integrated, providing diverse high-quality scenes. Built on these, several interfaces and tools are developed, enabling automated generation of trajectories and instructions.}
    \label{fig:data_gen}
    \vspace{-0.1cm}
\end{figure*}
    

Embodied AI has drawn increasing research interest, where vision-language navigation (VLN) is a key task, aiming to navigate agents to a target location according to linguistic instructions and visual observations. Recently, many VLN datasets have been established, \emph{e.g.,} TouchDown~~\cite{Touchdown}, REVERIE~\cite{REVERIE}, R2R~\cite{R2R}, RxR~\cite{RxR}, CVDN~\cite{CVDN}, VLN-CE~\cite{VLN-CE}, and LANI~\cite{LANI}, which significantly facilitate a series of VLN methods ~\cite{instructnav, AG-CMTP, law, CM2, CMA, ETPNav, gridmm, MGMap, navid}. However, all these works focus on indoor or ground-based agents, overlooking unmanned aerial vehicles (UAVs) that play an important role in aerial photography, rescue operations, and cargo transportation. 

%Most recently, AerialVLN~\cite{aerialVLN} and OpenUAV~\cite{openuav} have leveraged  UAV simulators to bridge the gap in aerial VLN datasets, thus advancing the development of this field. However, they still have three limitations. 1) \textbf{Insufficient data diversity.} They are built on AirSim and Unreal Engine (UE) to control UAVs, limiting them to digital assets compatible with these two platforms. This notably reduces the diversity of their data sources, particularly cutting off the possibility of using more photorealistic data. 2) \textbf{Inefficient data collection.} These works rely on pilots operating UAVs in a simulator to generate trajectories, with instructions subsequently created by annotators. The whole process is labor-intensive and time-consuming. 3) \textbf{Inadequate data scale.} OpenUAV~\cite{openuav} has validated that aerial VLN models perform better with the increase of training data. However, existing aerial VLN datasets only have a scale of around 10k, which lags far behind the manipulation dataset. For example, Open X-Embodiment~\cite{open_x-embodiment} collects 1M episodes of manipulation data, significantly promoting the development of vision-language-action (VLA) models.

Most recently, AerialVLN~\cite{aerialVLN} and OpenUAV~\cite{openuav} have made significant strides by leveraging UAV simulators to bridge gaps in aerial VLN datasets, thus greatly advancing the development of this field. However, several challenges remain that need to be addressed:

\begin{itemize}[left=0pt]
\item \textbf{Insufficient data diversity.} Existing methods rely on AirSim and Unreal Engine (UE) to control UAVs, limiting them to digital assets compatible with these platforms, reducing the diversity of available data and restricting the potential use of more photorealistic sources.
%\footnote{https://www.unrealengine.com/}

\item \textbf{Inefficient data collection.} The process of generating trajectories relies on pilots operating UAVs in simulators, followed by manual annotation to create language instructions. The entire process is labor-intensive, time-consuming, and difficult to scale.

\item  \textbf{Inadequate data scale.} Current datasets for aerial VLN remain relatively small, with only around 10k trajectories, which is far behind embodied manipulation datasets. For example, Open X-Embodiment~\cite{open_x-embodiment} has collected 1M episodes of manipulation data, which has significantly accelerated the development of vision-language-action (VLA) models~\cite{spatialvla, wu2024robomind, nasiriany2024robocasa, yang2024pushing}.
\end{itemize}

%\footnote{https://www.rockstargames.com/gta-v/}
%\footnote{https://earth.google.com/}
To address these issues, we propose \textbf{OpenFly}, a comprehensive platform consisting of a versatile toolchain and a large-scale benchmark for the aerial VLN task. \textbf{To enhance data diversity}, the platform is established on various widely-used rendering engines and advanced techniques, \emph{i.e.,} UE, GTA V, Google Earth, and 3D Gaussian Splatting (3D GS). This enables us to utilize a wide range of assets as shown in Fig. \ref{fig:OpenFly}. In particular, we use UAVs for automated patrols to capture numerous real-world images and integrate 3D GS technology into our platform, through which we reconstruct realistic 3D scenes. This empowers our platform to support a real-to-sim method, enhancing the realism of data. \textbf{To improve the efficiency of data collection}, we develop a versatile toolchain for automated aerial VLN data generation, as depicted in Fig. \ref{fig:data_gen}. Specifically, point cloud acquisition is first conducted to capture the 3D occupancy of a scene. Next, scene semantic segmentation is performed to identify and select landmarks as waypoints along the flight trajectories. Building on these tools, trajectory generation is then carried out, taking landmarks and point clouds as input, using predefined flight actions as basic units, and automatically searching for a collision-free trajectory. Finally, we feed the trajectories and corresponding UAV-egocentric images into a vision-language-model (VLM), \emph{i.e.,} GPT-4o, to generate linguistic instructions. The entire pipeline is highly automated, reducing the reliance on UAV pilots and annotators. \textbf{To collect a large-scale dataset}, we meticulously collected 18 high-quality scenes, generating various trajectories of differing heights and lengths. Benefitting from our toolchain, we are able to quickly construct a dataset of \textbf{100k} samples, significantly larger than existing datasets.

Besides, we propose \textbf{OpenFly-Agent}, a keyframe-aware aerial VLN model trained on the OpenFly dataset. It is developed based on a VLA model OpenVLA~\cite{openvla} and incorporates an adaptive frame-level token-sampling mechanism to mitigate visual distraction caused by redundant historical observations. Note that this sampling mechanism is crucial for the visual encoding of aerial VLN, as UAVs fly quickly and observations change rapidly. Extensive experiments are conducted on the OpenFly dataset to evaluate numerous methods, establishing a comprehensive benchmark for the aerial VLN tasks.

Overall, our contributions can be summarized as follows:
\begin{itemize}[left=0pt]
\item  We build OpenFly on multiple rendering engines and develop a versatile toolchain, enabling the automatic generation of data with both great diversity and efficiency.

\item  We have constructed a novel aerial VLN benchmark comprising \textbf{100k} trajectories across 18 high-quality scenes. To the best of our knowledge, this is the largest aerial VLN benchmark to date.

\item  We propose OpenFly-Agent, a keyframe-aware VLN model. Extensive experiments demonstrate its superior performance compared to other methods. 
\end{itemize}
