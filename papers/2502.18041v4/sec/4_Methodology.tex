%Task/Problem formulation
\subsection{Problem Definition}
In the aerial VLN task, a UAV is randomly positioned within a 3D environment with its initial pose defined as $P = [x, y, z, \phi, \theta, \psi]$. At each timestamp $t$, the UAV perceives the surrounding environment through an egocentric image as its observation. Guided by natural language instructions, the task involves predicting the next navigation action. Notably, the UAV can utilize either the current observations or the frames from all previous timestamps to make its prediction.

\subsection{Model Architecture}
As shown in Fig. \ref{fig:model}, we take OpenVLA~\cite{openvla} as the baseline and design an end-to-end model for aerial VLN. In contrast, our model takes a sequence of images to indicate the observation instead of one image in the original OpenVLA. Moreover, to mitigate visual redundancy between adjacent video frames while maintaining key information, two strategies are proposed, \emph{i.e.,} keyframe selection and visual token merging. First, a series of candidate keyframes are selected. Then, these keyframes are merged temporally before and after the vision encoder, resulting in a compact sequence of visual tokens. Finally, the action decoder discretizes the predicted tokens into uniformly distributed bins, which are subsequently mapped to the 6 action types specific to drones. 


\subsubsection{Keyframe Selection}
The length of contextual visual tokens is a major challenge for VLMs when processing videos. Many open-source VLMs use uniform frame sampling \cite{buch2022revisiting, ranasinghe2024understanding, wang2025videoagent} to reduce calculation, but this strategy is not suitable for aerial VLN, since it may miss frames containing key landmarks. 
To address this issue, we adopt a heuristic method to identify keyframes by detecting the change point of the UAV's movement. We notice that sudden changes in the UAV's trajectory are often caused by the observation of landmarks, which can serve as cues to determine keyframes. Specifically, we use the movement of the drone over time to draw turning curves, and the frames near the peaks of the wave are selected as candidate keyframes. The resulting data is interpolated and smoothed, forming a wave-like curve that represents the UAV's movement. 

To further ensure the precision of training data, scene segmentation maps collected in Sec. \ref{sec:Automatic} are used on selected frames to detect key landmarks. Frames containing landmarks are selected as keyframes, yielding reasonably accurate results. Note that each sudden change of actions, \emph{e.g.,} from `Forward' to `Turn Left', will produce a set of keyframes. Consequently, we obtain several sets of keyframes for a long trajectory. 
%For testing, we select keyframes where the action changes, as these often correspond to the observation of a critical landmark.
%Sec Parag

%This keyframe selection scheme gives model the guidance for action prediction via semantic relationship from the observation of the subgoal. Next, with the candidate frame sequences, we introduce the online visual Token merging module for the next action prediction.

\subsubsection{Visual Token Merging}
To further reduce redundant information in keyframes, we design visual token merging, where the core concept is to recognize the similarity between image tokens. It compares adjacent keyframes to merge similar regions and maintains its simplicity by token compression.

%合并阶段。
% 在获得候选帧之后，我们先逐帧过一遍vision encoder获取visual features，再利用标记相似性来合并相邻帧的视觉标记。类似 ToMe [] ，我们通过定期合并之后相邻帧中最相似的标记来进行记忆巩固。我们计算 N 个嵌入标记之间的平均余弦相似度s，在每次合并操作后保留K帧，这也嵌入了存储在长期记忆中的丰富信息。K是控制性能和效率之间权衡的超参数。因此，我们通过加权平均地合并每组相邻帧相似度最高的tokens。合并操作迭代进行，直到token计数达到每个合并操作的预定义值集K。合并阶段应用于Vision Transformer的倒数第二层特征patch token，以逐步合并相似的标记，直到相似标记的数量低于特定层的阈值 Nthreshold。合并阶段之后，剩余的唯一标记将进入压缩阶段。


\begin{table*}[t!]

\centering
\caption{Comparison results on the test-seen split.}
%\vspace{-5pt}
\label{tab:seen_results}
\begin{adjustbox}{center}
\resizebox{\textwidth}{!}{ 
%\setlength{\tabcolsep}{1.6pt}
\renewcommand{\arraystretch}{1.3}
% \scalebox{0.95}{
\begin{tabular}{lccccccccccccccccc}
\toprule
\multirow{2}{*}{Method} & \multicolumn{4}{c}{Easy} & \multicolumn{4}{c}{Moderate} & \multicolumn{4}{c}{Hard} & \multicolumn{4}{c}{Total}\\ 
\cmidrule(lr){2-5} \cmidrule(lr){6-9} \cmidrule(lr){10-13} \cmidrule(lr){14-17}
& NE$\downarrow$ & SR$\uparrow$ & OSR$\uparrow$ & SPL$\uparrow$ 
& NE$\downarrow$ & SR$\uparrow$ & OSR$\uparrow$ & SPL$\uparrow$ 
& NE$\downarrow$ & SR$\uparrow$ & OSR$\uparrow$ & SPL$\uparrow$
& NE$\downarrow$ & SR$\uparrow$ & OSR$\uparrow$ & SPL$\uparrow$ \\ \midrule 

Random & 289m & 0.9\% & 1.1\% & 0\% & 351m & 1.3\% & 1.3\% & 0\% & 374m & 0\% & 0\% & 0\% & 242m & 0.7\% & 0.8\% & 0\% \\
Seq2Seq\cite{VLN-CE}&  201m &  0.9\% & 21.2\% & 0.9\% & 190m & 8.9\% & 19.2\% & 6.5\% & 192m & 2.1\% & 10.1\% & 1.9\% & 194m & 4.0\% & 16.8\%  &  3.1\% \\
CMA\cite{VLN-CE}&  156m & 1.2\% &  35.6\% & 1.6\% & 120m & 11.2\% & 34.5\% & 8.4\% & 156m & 4.6\% & 20.1\% & 5.3\% & 144m & 5.7\% & 30.0\% & 5.1\%\\
AerialVLN\cite{aerialVLN}& \underline{148m} & 1.5\% &  \underline{40.2\%} & 2.6\% & \textbf{94m} & \underline{13.2\%} & \textbf{58.6\%} & \underline{10.7\%} & 147m & 5.4\% & \underline{23.6\%} & \underline{7.6\%} & \underline{130m} & 6.6\% &\underline{40.8\%} & \underline{7.0\%}\\
Navid\cite{navid}& 151m & \underline{11.2\%} & 28.9\% & \underline{4.5\%} & 138m & 8.0\% & 21.3\% & 2.8\% & \underline{134m} & \textbf{10.3\%} & 21.3\% & 4.6\% & 142m & \underline{9.9\%} & 24.3\% & 3.9\% \\
Ours& \textbf{111m} &  \textbf{26.5\%} & \textbf{55.6\%}  & \textbf{16.0\%} & \underline{115m} & \textbf{16.4\%} & \underline{51.2\%} & \textbf{11.2\%} & \textbf{120m} & \textbf{10.3\%} & \textbf{29.6\%} & \textbf{8.2\%}  & \textbf{115m} & \textbf{18.5\%} & \textbf{50.9\%} & \textbf{12.2\%} \\
\bottomrule
\end{tabular}
}
\end{adjustbox}
\end{table*}



For each set of candidate keyframes obtained in the previous selection process, a visual encoder maps each input image to multiple visual tokens, with each token representing the information of an image patch. Considering the potential inter-frame patch redundancy, we take a strategy that similar tokens in subsequent adjacent frames are periodically merged. Specifically, we select the first frame in a keyframe set as the reference, since it usually contains the crucial observation indicating the time for action transition. Then, we densely calculate the cosine similarities between each pair of visual tokens of the reference image and the subsequent image. Next, we merge the tokens with high similarity by averaging them. The unmerged tokens in the subsequent frame will be discarded. The merging operation is iteratively performed until the entire keyframe set has been traversed. Besides, we maintain a memory bank with a capacity of $K$ images, which follows a first-in-first-out (FIFO) policy to retain the latest keyframes.

After the above process, $M$ visual tokens $E=\{e_1, e_2, \cdots, e_M\}$ are obtained for each set of keyframes. Since aerial VLN requires UAVs to perform long-distance flights based on instructions, we continue to carry out token compress to reduce the computational burden. The compressed visual tokens $E_c$ are obtained through grid pooling~\cite{llama_vid}. Notably, we keep the visual tokens of the current frame uncompressed to capture the latest visual observation, as it contains the most important information for flight action prediction.




\subsubsection{Action Prediction}
Similar to~\cite{aerialVLN,CityNav}, 6 actions for UAVs are defined as $\{$Forward, Turn Left, Turn Right, Move Up, Move Down, Stop$\}$ in this work. The units for `Move up' and `Move down' are 3 m, the units for `Turn Left' and `Turn Right' are 30 degrees. `Forward' has three distinct units, namely 3 m, 6 m, and 9 m, respectively. For flight action prediction, each action type is discretized into multiple bins with one non-activate bin indicating that the current action is not activated. We map the model output to one of the bins for each action type, where the bin number corresponds to the amount of units in each action.