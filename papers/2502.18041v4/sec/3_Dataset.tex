\section{OpenFly Data Generation Platform}

\begin{figure*}[t]
\begin{center}
   \includegraphics[width=\linewidth]{Fig/all_images.png}
\end{center}
   \caption{High-quality examples from different rendering engines and techniques, including several large cities such as Shanghai, Guangzhou, Los Angeles, Osaka, and etc., cover an area of over a hundred square kilometers in total. 3D GS provides five large campus scenes, further enhancing the diversity and realism of the data.}
\label{fig:all_dataset}
\end{figure*}


In this section, we first describe several basic simulators and data resources, and then present the developed toolchain. The framework of the whole automatic data generation platform is illustrated in Fig. \ref{fig:data_gen}.

\subsection{Basic Simulators and Data Resources}
\label{sec:Automatic}


\indent \indent To collect a wide range of high-quality and realistic simulation data, we source the dataset from multiple rendering engines integrated with various simulators. Fig. \ref{fig:all_dataset} showcases several examples obtained from these rendering engines/techniques.

%\footnote{https://www.unrealengine.com/marketplace/product/city-sample/}
\textbf{Unreal Engine + AirSim/UnrealCV.} UE is a rendering engine capable of providing highly realistic interactive virtual environments. This platform has undergone five iterations, and each version features comprehensive and high-quality digital assets. In UE5, we meticulously select an official sample project named `City Sample', which provides us with a large urban scene covering $25.3 km^2$ and a smaller one covering $2.7 km^2$. These scenes include a variety of assets such as buildings, streets, traffic lights, vehicles, and pedestrians. Besides, in UE4, we prepare six more high-quality scenes. Specifically, there are two large scenes showcasing the central urban areas of Shanghai and Guangzhou, covering areas of $30.88 km^2$ and $58.56 km^2$, respectively. The remaining four scenes are selected from AerialVLN~\cite{aerialVLN}. They have smaller areas for totally about $26.64 km^2$. These scenes encompass a wide range of architectural styles, including both Chinese and Western influences, as well as classical and modern designs. Additionally, the UE4 engine allows us to make adjustments in scene time to achieve different appearances of scenes under varying lighting conditions.

% Meanwhile, it can also offer RGB, depth, and segmentation maps with realistic physics and sensor models. 
%UnrealCV is an open-source plugin for Unreal Engine, providing a simple interface for capturing RGB, depth, and segmentation images, thus facilitating research in computer vision and robotics.
Airsim is an open-source simulator, which provides highly realistic simulated environments for UAVs and cars. We integrate the AirSim plugin into UE4 to obtain image data easily from the perspective of a UAV.
%\footnote{https://github.com/microsoft/AirSim}
Since AirSim does not support UE5 and stopped updating in 2022, we use the UnrealCV~\cite{unrealcv} plugin as an alternative for image acquisition in UE5. To realize a highly efficient data collection in simulated scenes, we modify the UE5 project to a C++ project, integrate the UnrealCV plugin, and package executables for multiple systems like Windows and Linux. 

\textbf{GTA V + Script Hook V.} 
GTA V is an open-world game that is frequently used by computer vision researchers due to its highly realistic and dynamic virtual environment. The game features a meticulously crafted cityscape modeled after Los Angeles, encompassing various buildings and locations such as skyscrapers, gas stations, parks, and plazas, along with dynamic traffic flows and changes in lighting and shadows. 

Script Hook V is a third-party library with the interface to GTA V's native script functions. With the help of Script Hook V, we build an efficient and stable interface, which receives the pose information and returns accurate RGB images and lidar data. From the interface, we can control a virtual agent to collect the required data in an arbitrary pose and angle in the game.

%Specifically, it uses various 3D modeling techniques provided by softwares such as 3D Max and SketchUp to model urban-level scene images into 3D models. 
%These models are then uploaded to Google Earth and stitched together to form continuous 3D scenes for display.
\textbf{Google Earth + Google Earth Studio.} 
Google Earth is a virtual globe software, which builds a 3D earth model by integrating satellite imagery, aerial photographs, and Geographic Information System (GIS) data. From this engine, we select four urban scenes covering a total area of $53.60 km^2 $, \emph{i.e.,} Berkeley, primarily consisting of traditional neighborhoods; Osaka, which features a mix of skyscrapers and historic buildings; and two areas with numerous landmarks: Washington, D.C., and St. Louis.

%\footnote{earth.google.com}

%\footnote{https://www.google.com/earth/studio/}
%It expands upon the Google Earth browsing interface by integrating features commonly found in video production software. These enhancements
%developed by the Google Earth team
Google Earth Studio is a web-based animation and video production tool that allows us to create keyframes and set camera target points on the 2D and 3D maps of Google Earth. Using this functionality, we can quickly generate customized tour videos by selecting specific routes and angles. In order to efficiently plan the route, we develop a function that automatically draws the flight trajectory in Google Earth Studio according to the selected area and predefined photo interval. 
%We also implement a function that stores the collected images in a normalized coordinate system based on the GPS information.
%according to the data collection area and photo interval we set,
%The tool also supports exporting the output as MP4 files or image sequences, providing flexibility for further use.


%However, under the drone's perspective, choosing the appropriate shooting altitude posed a dilemma, \emph{i.e.,} if the altitude is too low, the sparse point cloud generated during the initialization of the 3D GS reconstruction will be suboptimal due to insufficient feature point matches between photos. On the other hand, if the altitude is too high, the Gaussian reconstruction will result in overly coarse training of details. After multiple attempts, the data collection plan using the M30T was determined as follows. For large-scale block scenes, oblique photography is performed at approximately twice the average building height using the default parameters of the M30T’s wide-angle camera, with a tilt angle of -45°. For landmark buildings with heights significantly different from the average height, additional targeted data collection is conducted at twice their height. This altitude setting can, to a certain extent, ensure both higher-quality point cloud initialization and Gaussian splatting training.(放到supp)

\textbf{3D Gaussian Splatting + SIBR viewers.} As a highly realistic reconstruction method, hierarchical 3D GS~~\cite{kerbl2024hierarchical} employs a hierarchical training and display architecture, making it particularly suitable for rendering large-scale areas. Due to these features, we use this method to reconstruct and render multiple real scenes. We utilize the DJI M30T drone as the data collection device, which offers an automated oblique photography mode, enabling us to capture a large area of real-world data with minimal manpower. Practically, we gathered data from five campuses across three universities, which are East China University of Science and Technology, Northwestern Polytechnical University, and Shanghai Jiao Tong University (referred to as ECUST, NWPU, and SJTU). These campus scenes include various types and styles of landmarks, such as libraries, bell towers, waterways, lakes, playgrounds, construction sites, and lawns. The detailed information for the five campuses is presented in Table~\ref{tab:GS_information}. More details of the 3D GS data collection can be found in our supplementary material.

SIBR~\cite{sibr2020} viewers is a rendering tool designed for the 3D GS project, enabling visualization of a scene from arbitrary viewpoints. The tool supports high-frame-rate scene rendering and provides various interactive modes for navigation. Building upon SIBR viewers, we developed an HTTP RESTful API that generates RGB images from arbitrary poses, simulating a UAV's perspective.


\begin{table}[t]
\caption{\textbf{Different 3D GS Scenes}}
\label{tab:GS_information}
\centering
\begin{tabular}{ccc}
\toprule
Campus Name&Images&Area \\
\midrule
\makecell{ECUST  (Fengxian Campus)} & 12008 & $1.06km^2$ \\
\midrule
\makecell{NWPU  (Youyi Campus)} & 4648 & $0.8km^2$ \\
\makecell{NWPU  (Changan Campus)} & 23798 & $2.6km^2$ \\
\midrule
\makecell{SJTU  (Minghang-East Zone)} & 20934 & $1.72km^2$ \\
\makecell{SJTU  (Minghang-West Zone)} & 9536 & $0.95km^2$ \\
\bottomrule
\end{tabular}
\end{table}



%In order to achieve automatic trajectory generation, it is necessary to structure the scene, which involves obtaining the 3D point cloud and the semantic segmentation. Based on these, a unified interface for image collection, a trajectory generation tool, and an instruction generation tool are developed.
\subsection{Toolchain for Automatic Data Colleciton}
\indent \indent To achieve automatic data generation, we integrate the above five simulators and design three unified interfaces, \emph{i.e.,} the agent movement interface, the lidar data acquisition interface, and the image acquisition interface, allowing an agent to interact with any scene. Based on these interfaces, we further develop a toolchain, including 3D point cloud acquisition, scene semantic segmentation, automatic trajectory generation, and instruction generation. The framework of the whole data generation platform is illustrated in Fig. \ref{fig:data_gen}, with details of these interfaces and tools elaborated below.

%To facilitate the collection of trajectories and images across various simulation environments, the OpenFly platform integrates all the aforementioned simulators and provides unified interfaces that enable interaction between an agent and the environment in any scene. Specifically, there are three main interfaces. 
%$[X, Y, Z] [QW, QX, QY, QZ]$
%$[d_x, d_y, d_z]$ and $[d_{roll}, d_{pitch}, d_{yaw}]$
%using either absolute poses or relative poses in the agent's body frame
%3) Scene Information Interface: The 2D coordinates and height information of all landmarks, along with the point cloud map of the entire scene, can be accessed through this interface.
\textbf{Unified Interfaces.} 
1) Agent Movement Interface: We design a \textit{CoorTrans} module, which implements a customized pose transformation matrix and scaling function to unify all simulator coordinate systems into a meter-based FLU (Front-Left-Up) convention. This interface enables precise agent positioning among regular scenes, point clouds, and scene segmentations, ensuring consistency and facilitating automatic trajectory generation.
2) Lidar Data Acquisition: For different simulators, point cloud data is acquired  through different methods, including lidar sensor collection, depth map back-projection, and image feature matching. We develop a unified interface to integrate these methods and leverage the proposed \textit{CoorTrans} module to align all data to the same FLU coordinate system.
3) Image Acquisition Interface: We integrate HTTP RESTful and TCP/IP protocols to form a unified image request interface, allowing image data to be obtained from any location with flexible  resolutions and agent viewpoints. 

%we use three methods to obtain 3D point clouds from the scenes, \emph{i.e.,} depth map back-projection, LiDAR scan reconstruction, and sparse reconstruction. For the UE5+UnrealCV simulator, a project named MatrixCity~~\cite{li2023matrixcity} provides us with the depth maps and camera parameters of small-city and big-city scenes. 1) Through back-projecting the 2D depth information into 3D space, we generate the point clouds for the two datasets. 2) For the UE4+AirSim and GTA5 simulators, we directly utilize the LiDAR sensors provided by the simulators to traverse each scene in a grid pattern, obtaining local point clouds. These are then transformed into the global coordinate system using the LiDAR coordinate information and finally merged into complete scene point clouds. 3) In 3D GS, since the first step of Gaussian Splatting reconstruction involves using the open-source project COLMAP ~\cite{colmap} to perform sparse structure-from-motion (SFM) point cloud reconstruction based on input images, we could directly export and use the point clouds obtained from this step.
%structure-from-motion (SFM)
\textbf{3D Point Cloud Acquisition.} 
For different simulators, we provide two methods to reconstruct the point cloud map of an entire scene. 1) Rasterized Sampling Reconstruction:
For the UE5 + UnrealCV simulator, the MatrixCity~\cite{li2023matrixcity} project offers a convenient rasterized sampling solution. We use the aforementioned lidar data acquisition interface to obtain the local point cloud at the sampling points. Since these data are already aligned within the same coordinate system, the point cloud map of the entire scene can be constructed by simply stitching local point clouds. For the UE4 + AirSim and GTA V simulators, we customize rasterized sampling points at appropriate resolutions, and perform sampling and reconstruction using the agent movement and lidar data acquisition interfaces. 2) Image-based Sparse Reconstruction: In 3D GS, the scene reconstruction process begins with the open-source COLMAP~\cite{colmap} framework, which geneoverlearates a sparse point cloud from input images. We directly export and use the point clouds obtained from this step. 


\textbf{Scene Semantic Segmentation.} 
Vision-and-Language Navigation (VLN) requires meaningful landmarks as navigation targets. We perform semantic segmentation on four types of simulation scenes using the following three methods. 1) 3D Scene Understanding: A sequence of top-down views of the scene is captured in a rasterized format. We then use Octree graph~\cite{octree_graph} to extract 2D mask proposals, which are subsequently projected into the 3D point cloud space to generate semantic 3D segments. 2) Point Cloud Projection and Contour Extraction: We first acquire the point cloud of a scene, then project the voxelized point cloud onto a projection plane slightly above the ground. Using OpenCV, we perform a series of operations on the projected image, including binarization, erosion, and contour extraction, to obtain multiple instances along with their 2D coordinates. For each instance, the maximum height of the points within its neighborhood is used as the final height. This method provides a more computationally efficient yet coarser segmentation compared to the first approach, allowing users to choose based on their requirements. 3) Manual Annotation: When the point cloud quality of a scene is low or finer segmentation is required, we provide a method for annotating instances in the point cloud space by mouse clicks, based on ROS2 and RVIZ2. Users can annotate instances directly in the point cloud space using mouse clicks to define landmarks of interest for the task. This method is applicable to four simulators, \emph{i.e.,} UE + UnrealCV, UE + Airsim, GTA V, and 3D GS.

\begin{comment}
\begin{figure}
    \centering
    % 第一列子图
    \begin{subfigure}[b]{0.15\textwidth}   % 0.3\textwidth 是每个子图的宽度
        \centering
        \includegraphics[width=\textwidth]{Fig/whole_scene.pdf} % 替换为你的图片文件
        \caption{}
        \label{fig:sub1}
    \end{subfigure}
    \hfill  % 用来在子图之间增加水平间隔
    % 第二列子图
    \begin{subfigure}[b]{0.15\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Fig/scene_point_cloud.pdf}
        \caption{}
        \label{fig:sub2}
    \end{subfigure}
    \hfill
    % 第三列子图
    \begin{subfigure}[b]{0.15\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Fig/scene_seg.pdf}
        \caption{}
        \label{fig:sub3}
    \end{subfigure}
    
    \caption{Illustration of results obtained by our point cloud acquisition and semantic segmentation tools. (a) Overview of an urban scene. (b) The point cloud of (a). (c) The semantic segmentation of (a).}
    \label{fig:main}
\end{figure}
\end{comment}


\textbf{Automatic Trajectory Generation.}
Leveraging the aforementioned point cloud map and segmentation tools, OpenFly can generate trajectories using the following two methods. 
%1) Path search based on customized action space: First, a global hash voxel map $M_{global}$ is constructed from the scene point cloud $P$ and the voxelized point cloud is projected onto the horizontal plane to obtain the bird's eye view (BEV) occupancy map $M_{bev}$. 
1) Path search based on customized action space: First, a global hash voxel map $M_{global}$ and a bird's eye view (BEV) occupancy map $M_{bev}$ are constructed from the scene point cloud $P$.
Second, the flight altitude is randomly selected within the user-defined height range, and landmarks that are not lower than the height threshold $H_{\tau}$ are chosen as targets. A starting point is selected within the distance range $[r, R]$ from the landmark, ensuring that it is not occupied in both $M_{global}$ and $M_{bev}$. Then, a point on the line connecting the starting point and the landmark, which is close to the landmark and unoccupied in $M_{bev}$, is chosen as the endpoint. 
Third, A collision-free trajectory from the starting point to the endpoint is generated using the A*~~\cite{astar} pathfinding algorithm, where the granularity of exploration step size and direction can be adjusted according to the action space. Besides, by repeatedly selecting the endpoint as the new starting point, complex trajectories can be generated. Finally, utilizing OpenFly's interface, images corresponding to the trajectory points can be obtained. 2) Path search based on grid: Google Map data does not allow image retrieval at arbitrary poses in the space. Thus we rasterize a pre-selected area and collect images from each grid point in all possible orientations. Starting and ending points are chosen within the grid points to generate trajectories. Corresponding images for these trajectory points are then selected from the pre-collected image set.



% 视觉语言导航数据中，语言指令的质量至关重要。然而，以往的研究大多依赖人工标注，不仅成本高昂，还限制了数据集规模。为此，OpenFly 提出了基于视觉语言模型（VLM）的全自动化语言指令生成方法。

% 自然的想法是将所有图像全部提交给VLMs进行轨迹分析，生成指令。但全部图像的输入带来了巨额的开销，并带来了信息冗余。因此 OpenFly 将 完整轨迹拆分为子轨迹来进行处理，提取每一个子轨迹的关键动作和landmark特征，最终进行整合。与室内视觉语言导航不同，在空中视觉语言导航（Aerial Vision Language Navigation）中，环境中的障碍物较少，因此指令中的“Move Forward” 占据了大部分比例，关键动作集中在“左转/右转”和“上升/下降”。此外，无人机飞行轨迹中不可避免地会出现轻微角度调整，这些调整往往无明确目的地，因此被简化为“slightly turn left/right”。所以我们根据轨迹是否发生了连续的非直行动作来进行拆分。举一个例子，轨迹动作序列为[1,2,1,2,2,1,0],这里1代表move forward，2代表turn left，0代表 stop。该轨迹会被拆分为[1,2,1],[2,2],[1,0]。第一条的动作为Move forward and slightly turn left，与此同时我们将第一条子轨迹的最后一张图像提交给VLM提取对应的landmark特征，为了更加精确，我们还会提供Landmark在图像中的位置星系

% User：You are an assistant proficient in image recognition. You can accurately identify the object closest to you in the image and its different features from surrounding objects.The object is at the center of the image.

% GPT 4o:{color: bule, feature: Steel, glass, size: medium size, type: building}

% 最终我们得到了如下的子指令序列：
% 1. Action 1(Move forward and slightly turn left) , Landmark 1
% 2.Action 2( turn left), Landmark 2
% 3.Action 3 (Move forward ), Landmark 3

% 我们会将子指令序列提交给GPT 4o，并获得最终的指令。


% 基于上述方法，我们生成了一系列“动作 + Landmark”的子指令。随后，利用 VLM 的语言生成能力，将这些子指令整合为流畅、完整的自然语言导航指令

%Unlike indoor VLN, in aerial VLN, there are fewer obstacles in the environment, meaning that , with key actions focused mainly on “Turn Left/Turn Right” and “Ascend/Descend.”



\textbf{Automatic Instruction Generation.}
Previous research has predominantly relied on manual annotation to generate trajectory instruction, which is not only costly but also limits the scalability of datasets. To address this issue, we propose a highly automated language instruction generation method based on VLMs, \emph{e.g.,} GPT-4o.
A straightforward method would be to submit all images to VLMs to analyze the trajectory and generate instructions. However, using all images introduces significant computational overhead and leads to information redundancy. For example, the `Forward' action usually occupies a larger proportion of a flight trajectory, with `Turn Left/Turn Right' or `Ascend/Descend' actions taken when encountering key landmarks.




Based on the above findings, we split the complete trajectory into multiple sub-trajectories based on the occurrence of non-consistent actions, extracting key actions and images for processing and subsequent integration. Notably, slight angle adjustments often occur during flight to change the direction subtly, and a `slightly Turn Left/Right' will be merged with subsequent `Forward' actions. Specifically, suppose that the trajectory action sequence is [1, 1, 2, 1, 1, 2, 2, 1, 0], where 1, 2, and 0 denote `Forward', `Turn Left', and `Stop', respectively. This trajectory would be split into four sub-trajectories, \emph{i.e.,} [1, 1], [2, 1, 1], [2, 2], and [1, 0]. The second sub-trajectory involves `slightly turn left' and `move forward'. We submit the action sequence and the last captured image of each sub-trajectory to a VLM to generate descriptions of both action and landmarks.
%A simplified prompt to the VLM and corresponding response are probably like this. User: `You are an assistant proficient in image recognition. You can accurately identify the object closest to you in the image and its different features from surrounding objects. The object is usually at the center of the image'. GPT 4o: `{color: blue, feature: Steel, glass, size: medium size, type: building}'.
The sub-instructions are obtained similar to the following format:
\begin{itemize}[left=0pt]
    \item `Move forward' to `Landmark 1'.
    \item `Slightly turn left and move forward' to `Landmark 2'.
    \item `Turn left' towards `Landmark 3'.
    \item `Move forward' to `Landmark 4'.
\end{itemize}

These sub-instructions are then processed by a VLM/LLM again, where they are integrated into coherent and complete instructions. The detailed prompt used for the VLM, along with the complete responses, is provided in the supplementary material.

%Based on this method, we generated a series of “Action + Landmark” sub-instructions. Subsequently, leveraging the language generation capabilities of VLMs, these sub-instructions were integrated into coherent and complete natural language instructions. More details and the complete prompt to GPT-4o are shown in our supplementary material.

%User: You need to help me combine these scattered actions and landmarks into a sentence with words that are similar in meaning and more appropriate in words, so that the sentence is fluent and accurate. At the same time, merge the same landmarks accurately. {Sub-instructions}

%GPT 4o: Move forward and slightly turn left to a high-rise building with a noticeable logo at the top.Then turn left and go straight to a futuristic tower with a large spherical structure in the middle.
% 

% Specifically, we divide the instruction generation process into two main components, \emph{i.e.,} actions and the corresponding landmarks. Unlike indoor VLN, aerial VLN features fewer obstacles in the environment, resulting in a higher proportion of the “Move Forward” action, with key movements focusing on “Turn Left/Right” and “Ascending/Descending.” Additionally, slight angular adjustments are unavoidable in drone flight trajectories. However, these adjustments often lack a specific destination and are simplified as “slightly turn left/right.”

% The wide field of view from drones and the similarity in appearance among common buildings present challenges in identifying landmarks. To overcome this, in addition to image data, we provide the VLM with landmark positional information, \emph{e.g.,} "in the center of the image," or "at the bottom of the image", to extract key attributes of landmarks, such as type, color, and shape.



    
\subsection{Quality Control.}
%一些filter机制 和 人工抽查策略
\textbf{Data Filter.}
During data collection, it is inevitable that some damaged or low-quality data will be generated. We clean the data in the following situations. 1) We remove damaged images that are produced in generation or transmission. 2) We find that UAVs sometimes appear to pass through the tree models. Therefore, we exclude the trajectories where the altitude is lower than that of the trees. 3) We believe that extremely short or long trajectories are not conducive to model training. Thus, we remove these trajectories, specifically those with fewer than 2 or more than 150 actions.




\textbf{Instruction Refinement.}
A known challenge of instruction generation is VLMs' hallucinations. During the previous instruction generation process, sometimes the same landmark appears across several frames. This results in a VLM generating similar captions for the repeated observations of a landmark, increasing the complexity of the final instruction and introducing ambiguity due to duplication.

To mitigate this challenge, we utilize the NLTK library ~\cite{bird2006nltk} to simplify the instruction by detecting and merging similar descriptions. Specifically, a syntactic parse tree is first constructed to extract all landmark captions using a rule-based approach. Then, a sentence-transformer model is employed to encode the extracted landmark captions into embedding vectors. Their similarities are computed with dot product, and high-similarity captions are then identified and replaced with referential pronouns (\emph{e.g.}, ``it," ``there," \emph{etc.}). For example, a generated instruction with redundant information is ``$\cdots$ make a left turn toward \textbf{a medium-sized beige building marked by a signboard reading CHARLIE'S CHOCOLATE}. Continue heading straight, passing \textbf{a medium-sized gray building with a prominent rooftop billboard displaying Charlie’s Chocolate} $\cdots$". After simplification, a more concise sentence is obtained, \emph{i.e.,} ``$\cdots$ make a left turn toward \textbf{a medium-sized beige building marked by a signboard reading CHARLIE'S CHOCOLATE}. Continue heading straight, passing \textbf{it} $\cdots$", demonstrating the effectiveness of this post-processing technique. 

At the same time, we built a data inspection platform to provide instructions, action sequences, and corresponding images to the examiners. If the instructions and trajectories align, they are considered qualified. We randomly select 3K samples from the entire dataset  according to data distribution in Sec. \ref{data_split}. After manually inspecting these samples, we find that the qualification rate reaches 91\%.


%我们首先用nltk库进行英文文本处理，构建语法关树，通过rule-based方式提取所有的landmark caption。 接着，是用sentence transformer将所选landmark编码embeddings，通过点积计算其相似度，筛选出高相似性的短语嵌入, 将其替换为指代性名词（it, there etc.）。



\section{Dataset Analysis}

\begin{table*}[t]
\small      
\caption{Comparison of different VLN datasets. Above the middle dividing line lies the ground-based datasets, while below is the aerial VLN datasets. $N_{traj}$: the number of total trajectories. $N_{vocab}$: vocabulary size. Path Len: the average length of trajectories, measured in meters. Intr Len: the average length of instructions. $N_{act}$: the average number of actions per trajectory.}
\begin{adjustbox}{center}
%\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{1.2}
\scalebox{.99}{
\begin{tabular}{lcccclcc}
\toprule
Dataset   & $N_{traj}$ & $N_{vocab}$ & Path Len. & Intr Len. & Action Space & $N_{act}$ & Environment \\ \midrule
R2R~\cite{R2R}       & 7189      & 3.1K         & 10.0      & 29        &graph-based   & 5       & Matterport3D  \\
RxR~\cite{RxR}       & 13992     & 7.0K         & 14.9      & 129       &graph-based   & 8       & Matterport3D  \\
REVERIE~\cite{REVERIE}   & 7000      & 1.6K         & 10.0      & 18        &graph-based   & 5       & Matterport3D  \\
CVDN~\cite{CVDN}      & 7415      & 4.4K         & 25.0      & 34        &graph-based   & 7       & Matterport3D  \\
TouchDown~\cite{Touchdown} & 9326      & 5.0K         & 313.9     & 90        &graph-based   & 35      & Google Street View  \\ 
VLN-CE~\cite{VLN-CE}    & 4475      & 4.3K         & 11.1      & 19        &2 DoF         & 56      & Matterport3D  \\
LANI~\cite{LANI}      & 6000      & 2.3K         & 17.3      & 57        &2 DoF         & 116     & CHALET  \\ \midrule
ANDH~\cite{ANDH}      & 6269      & 3.3K         & 144.7     & 89        &3 DoF         & 7       & xView  \\
AerialVLN~\cite{aerialVLN} & 8446      & 4.5K         & 661.8     & 83        &4 DoF         & 204     & AirSim + UE \\
CityNav~\cite{CityNav}   & 32637     & 6.6K         & 545       & 26        &4 DoF         & -       & SensatUrban  \\
OpenUAV~\cite{openuav}   &12149      &10.8K         & 255       & 104       &6 DoF         & 264     & AirSim + UE \\ \midrule
\multirow{2}{*}{Ours} &\multirow{2}{*}{100K}     &\multirow{2}{*}{15.6K}        &\multirow{2}{*}{99.1}        &\multirow{2}{*}{59}       &\multirow{2}{*}{4 DoF}    &\multirow{2}{*}{35}     & \parbox[t]{6cm}{AirSim + UE, GTA5 + Script Hook V, \\ Google Earth Studio, 3D GS + SIBR viewers} \\

\bottomrule
\end{tabular}
}
\end{adjustbox}
\label{tab:dataset_comp}
\end{table*}

\begin{figure}
    \centering
    % 第一行子图
    \begin{subfigure}[b]{0.47\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{Fig/action_num.pdf}
        \caption{Difficulty level distribution.}
        \label{fig:sub1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.47\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{Fig/length_height.pdf}
        \caption{Length-height distribution.}
        \label{fig:sub2}
    \end{subfigure}

    % 换行
    \vspace{0.5cm} 

    % 第二行子图
    \begin{subfigure}[b]{0.47\columnwidth}
        \centering
       
        \includegraphics[width=\textwidth]{Fig/action.pdf}
        \caption{Action distribution with 1 type of `Forward'.}
        \label{fig:sub3}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.47\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{Fig/action_merge.pdf}
        \caption{Action distribution with 3 types of `Forward'.}
        \label{fig:sub4}
    \end{subfigure}

    \caption{Statistical analysis of trajectories.}
    \label{fig:traj_sta}
\end{figure}

% 词云（看看是否好分名词和动词）、总的轨迹/instruction数量、Vocabulary size、平均每条instruction词量；

%平均路径长度、平均action个数；路径长度分布统计图、action个数分布统计图（easy、middle、hard各多少轨迹）；action type分布饼图（见AerialVLN）

%dataset split：train、test划分，各多少轨迹；不同场景的轨迹数量分布饼图（train的不同场景的分布饼图）.
\subsection{Overview}
Using our toolchain, we collect 100k trajectories from 18 scenes, along with corresponding image sequences and language instructions. During the data generation process, we set a minimum motion step size of 3 meters to produce more granular trajectories. The details of our and previous VLN datasets are listed in Table. \ref{tab:dataset_comp}, from which we can see that our dataset features a significantly larger number of trajectories and a more extensive vocabulary, as well as greater environmental diversity. In contrast, our average trajectory length and instruction length are relatively short. This is intentional, as we believe" short- and medium-range instructions are actually more in line with the usage habits of human users. This might be more beneficial for the aerial VLN field.

\subsection{Trajectory Analysis}

In addition to a rich variety of scenes, we also strive for diversity in the difficulty level, length, and height of the trajectory data. Based on the number of actions in one trajectory, we classify trajectories with fewer than 30 actions as `Easy', those with the number of actions ranging from 30 to 60 as `Moderate', and those with more than 60 actions as `Hard'. Fig. \ref{fig:sub1} shows the corresponding difficulty level distribution. Besides, compared with ground-based VLN, the aerial VLN task has more motion dimensions. Therefore, we set different trajectory lengths and flight heights to obtain rich data. Fig. \ref{fig:sub2} exhibits the distribution of these data, with their lengths ranging from 0 to 300 meters, and the heights ranging from 0 to 150 meters. 

In the aerial VLN tasks of large-scale outdoor scenes, the proportion of moving forward is naturally higher than that of making adjustments in direction and altitude, as shown in Fig. \ref{fig:sub3}. However, this highly unbalanced action distribution might cause the VLN model to overfit to the dominant action. To alleviate this problem, we divide the `Forward' action into three granularities, \emph{i.e.,} 3m, 6m, and 9m. In the ground-truth trajectories, three consecutive `Forward' actions will be combined into one `9m Forward' action. At the end of a straight-moving trajectory, if the remaining distance is less than 9m, it will be combined into a `6m Forward' action, or remain as a `3m Forward' action. Fig. \ref{fig:sub4} presents the action distribution after this action merging process.



\begin{figure}
    \centering
    % 第一行子图
    \begin{subfigure}[b]{0.47\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{Fig/train_split.pdf}
        \caption{Train set distribution.}
        \label{fig:train_sp}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.47\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{Fig/test_split.pdf}
        \caption{Test set distribution.}
        \label{fig:test_sp}
    \end{subfigure}
    \caption{The distribution of the data volume in different scenes under the Train and Test sets.}
    \label{fig:traj_sta}
\end{figure}


\section{OpenFly-Agent}
\begin{figure*}[t]
\centering
    \includegraphics[width=0.98\linewidth]{Fig/model_arch.pdf}
    \caption{The architecture of OpenFly-Agent. Keyframes at the time of action transitions are selected to extract crucial observations as the history, with corresponding visual tokens compressed to reduce the computational burden.}
    \label{fig:model}
\end{figure*}

%\textit{Test Seen} and \textit{Test Unseen} indicate that whether the scenes have appeared in the \textit{Train} set.
\subsection{Dataset Split}
\label{data_split}
Similar to previous works, we divide the dataset into three splits, \emph{i.e.,}  \textit{Train, Test Seen, Test Unseen}. Detailed data distributions are shown in Fig. \ref{fig:traj_sta}. For the \textit{Train} split, 7 scenes under the UE rendering engine account for $75.7\%$ of the total \textbf{100K} data, since UE provides the largest number of scenes, where different amounts of trajectories are sampled according to the scenario area. The 4 scenes created by 3D GS are also the main part of the data, accounting for nearly $20\%$ of the total amount. To ensure visual quality, we only collect data from a high-altitude perspective using Google Earth, which accounts for $4.46\%$. 
The detailed information of the \textit{Test Seen} and \textit{Test Unseen} splits are as follows:
\begin{itemize}[left=0pt]
    \item \textit{Test Seen}: 1800 trajectories uniformly sampled from 11 previously seen UE and 3D GS scenarios.

    \item \textit{Test Unseen}: 1200 trajectories uniformly generated from 3 unseen scenarios, \emph{i.e.,} UE-smallcity, 3D GS-sjtu02, and a Los Angeles-like city in GTA V.
\end{itemize}


%