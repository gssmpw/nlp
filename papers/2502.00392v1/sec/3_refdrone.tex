
\section{RefDrone benchmark}

\subsection{Data source}
The RefDrone dataset builds upon VisDrone2019-DET~\cite{visdrone}, a drone-captured dataset for object detection. The source images are collected across multiple scenarios, illumination conditions, and flying altitudes. To ensure meaningful visual content, we implement two filtering criteria. At the image level, we exclude images containing less than 3 objects. At the object level, we exclude objects with bounding box areas below 64 pixels. The original VisDrone2019-DET annotations provide object categories and bounding box coordinates. We convert the bounding box to a normalized center point (range 0-1). This approach reduces the input context for LMMs while preserving spatial information.\par

\subsection{RDAgent for semi-automated annotation}
RDAgent (referring drone annotation framework with multi-agent system) is a semi-automated annotation framework that enhances traditional annotation pipelines by integrating LMMs with human annotators. Central to RDAgent are multiple LMM agents, specifically GPT-4o, each perform specialized tasks via distinct system prompts. As shown in Figure~\ref{fig:agent}, RDAgent employs five structured steps.\par




\textbf{Step 1: scene understanding.} A GPT-4o-based captioning agent generates three diverse textual descriptions per image. These captions provide contextual perspectives (e.g., object relationships, spatial layouts) to anchor subsequent referring expression generation.\par

\begin{figure}[t]
\subfloat[{\scriptsize}]{
    \begin{minipage}[l]{0.29\linewidth}
         \raggedright
         % \centering
             \vspace{-100pt} 
        \includegraphics[width=1\linewidth]{fig/area_ann_v5.pdf}
        % \caption{Left image}
    \end{minipage}
}
\subfloat[{\scriptsize }]{
    \begin{minipage}[l]{0.69\linewidth}
         \raggedleft
        \includegraphics[width=1.0\linewidth]{fig/percentage_v8.pdf}
        % \caption{Right top image}

    \end{minipage}
}
    \caption{Object size distribution analysis. (a) Object size distribution in RefDrone dataset (small: $< 32^2 = 1024$ pixels, normal: 1024 to 9216 pixels, large: $> 96^2 = 9216$ pixels). (b) Object size histograms in RefDrone and gRefCOCO~\cite{grefcoco} datasets.}
    \label{fig:data_area}
\end{figure}


\textbf{Step 2: color categorization.} Color attributes serve as crucial discriminative features in referring expressions. Color attributes are extracted using a hybrid pipeline: (1) a WideResNet-101~\cite{wideresnet} model trained on HSV color space-based labels, refined via manual validation, and (2) a dedicated LMM agent that verifies predictions to mitigate illumination- or occlusion-induced errors.



   
\textbf{Step 3: expression generation.} To address the complexity in referring expression generation, we reformulate the referring expression generation task as an object grouping problem. The target of the agent is to group semantically related objects and to provide appropriate reasons for each group. The reasons serve as the referring expressions. Furthermore, a dynamic feedback loop triggers color categorization (Step 2) when novel colors are detected, ensuring color attribute consistency.\par

\textbf{Step 4: quality evaluation.} 
An evaluator agent assesses the annotation semantic accuracy and referential uniqueness of object-expression pairs, categorizing annotations as:
\begin{itemize} 
\item ``Yes.'' The annotation is accurate and unique to the described objects. 
\item ``No.'' The annotation is inaccurate or non-unique, with a detailed explanation of the discrepancy. 
\end{itemize} 
Annotations marked ``Yes'' proceed to Step 5. Annotations marked ``No'' activate specific feedback: expressions with semantic issues are returned to Step 3 (expression generation), while color-related inaccuracies are sent to Step 2 (color categorization) for refinement.\par


\textbf{Step 5: human verification.} Human annotators review annotation outputs in three tiers:
\begin{itemize} 
\item Direct acceptance. Annotations satisfying all quality criteria are approved for the final dataset.
\item Refinement required. Annotations with minor errors are corrected through human editing.
\item Significant issues. Annotations with major errors or unclear descriptions are marked for complete reworking.

\end{itemize}
Annotations with significant issues enter a feedback loop, returning to Step 4 (quality evaluation). If an annotation repeatedly fails to meet standards, it becomes a no-target sample. These feedback refinement loops ensure that all annotations meet our quality standards and that no-target expressions still relate to the image content. 
\par
Each step utilizes LMMs through in-context learning with manually designed prompts and examples (see Supplementary Material). RDAgent requires only \$0.0539 pre expression in GPT4-o API cost and reduces human annotation effort by 88\% compared to fully manual annotations. This cost-performance balance makes the framework scalable for large-scale datasets.




\begin{figure}[t]
\centering
\includegraphics[width = 1\linewidth]{fig/number_statics_v5.pdf}
\caption{Object number distribution per expression in gRefCOCO~\cite{grefcoco} and RefDrone datasets.
} 
\label{fig:data_statistics}
\end{figure}

\begin{figure}[t]
    \centering
    \subfloat[{\scriptsize Word cloud of complete expression.}]{
    \begin{minipage}[c]{0.49\linewidth}
        % \centering
        \includegraphics[width=1.0\linewidth]{fig/caption_wordcloud_new.png}
        % \caption{Left image}
    \end{minipage}
    }
    \subfloat[{\scriptsize Word cloud of background terms.}]{
    \begin{minipage}[c]{0.49\linewidth}

        \includegraphics[width=1.0\linewidth]{fig/object_wordcloud_filtered_3.png}
        % \caption{Right bottom image}
    \end{minipage}
    }
    \caption{Word frequency visualization in RefDrone dataset.}
    \label{fig:words_cloud}
\end{figure}

\subsection{Dataset analysis}
The RefDrone dataset contains 17,900 referring expressions across 8,536 images, comprising 63,679 object instances in 10 categories. We maintain the original train, validation, and test splits from VisDrone2019-DET~\cite{visdrone}. The dataset features an average expression length of 9.0 words, with each expression referring to an average of 3.8 objects. Figure~\ref{fig:intro-figure} gives several examples that relate to the three key features and challenges in RefDrone:\par


\noindent\textbf{1) Multi-scale and small-scale target detection.} Figure~\ref{fig:data_area} presents an analysis of object scale distribution in RefDrone, with (a) showing the overall distribution and (b) offering a comparison with the gRefCOCO~\cite{grefcoco} dataset. In the RefDrone dataset, small-scale objects constitute 31\% of all instances, while large-scale objects account for 14\%. The detailed scale distribution in RefDrone demonstrates greater variance compared to gRefCOCO, highlighting the multi-scale challenges in RefDrone.\par


\noindent\textbf{2) Multi-target and no-target samples.} The RefDrone dataset includes 11,362 multi-target and 847 no-target expressions, with the number of referred targets ranging from 0 to 242. Figure~\ref{fig:data_statistics} illustrates the target number distribution, revealing higher complexity in multi-target scenarios compared to gRefCOCO, where expressions typically refer to no more than two objects. \par


\noindent\textbf{3) Complex environment with rich contextual expressions.} 
The images present inherent complexity including diverse viewpoints, varying lighting conditions, and complex background environments. The referring expressions go beyond simple object attributes (\eg, color, category) and spatial relationships (\eg, `left', `near'), incorporating rich object-object interactions (\eg, `the white trucks carrying livestock') and object-environment interactions (\eg, `the white cars line up at the intersection'). Figure~\ref{fig:words_cloud} presents word cloud visualizations of (a) complete expressions and (b) background terms, highlighting the diverse vocabulary used in descriptions.\par



\subsection{Dataset comparison}
Table~\ref{tab:dataset_ann} compares RefDrone with existing REC datasets. RefDrone stands out by its average number of referred objects and its use of LMM for expression generation. The expressions offer richer contextual details compared to template-based or human-annotated expressions. While RIS-CQ~\cite{riscq} employs LLM for expression generation, it lacks visual content during the generation process. This often results in expressions that, despite being linguistically complex, may be disconnected from visual content. Similarly, RSVG~\cite{zhan2023rsvg}, although it focuses on small targets, falls short in terms of expression quality. In contrast, RefDrone comprehensively presents three challenges, establishing it as a challenging benchmark in REC tasks.



\subsection{Evaluation metrics}
We introduce instance-level metrics extending traditional REC metrics to address multi-target challenges. Previous benchmarks primarily focus on image-level metrics, which are sufficient for single-target or few-target samples. However, these metrics fail to handle expressions referring to multiple target objectsâ€”potentially scaling up to 100 instances per expression in our task.  To address this limitation, we introduce instance-level metrics that provide a more granular evaluation of a model's capability to identify individual target objects.\par

\textbf{Instance-level metrics: Acc$_{inst.}$ and F1$_{inst.}$} measure the accuracy of individual bounding box predictions.
We compute the intersection over union (IoU) between the true bounding boxes and the predicted bounding boxes. An IoU $\geq$ 0.5 is considered a true positive (TP), otherwise it is a false positive (FP). Unmatched true bounding boxes are counted as false negatives (FN). For no-target samples, a prediction without any bounding box is a true negative (TN), otherwise it is a false positive (FP). We calculate:

\begin{equation}
\text{Acc}_{inst.} = \frac{TP + TN}{TP + TN + FP + FN}   \label{eq:important} 
\end{equation}
and
\begin{equation}
\text{F1}_{inst.} = \frac{2 \cdot TP}{2 \cdot TP + FP + FN}    \label{eq:also-important}
\end{equation}
\par

\textbf{Image-level metrics: Acc$_{img.}$ and F1$_{img.}$} evaluate the overall accuracy of predictions per image.
An image is considered a true positive (TP) if all predictions match the true bounding boxes, otherwise it is a false positive (FP). For no-target samples, a prediction without any bounding box is a true negative (TN), otherwise it is a false positive (FP). We calculate Acc$_{img.}$ and F1$_{img.}$ using the same formulas as Acc$_{inst.}$ and F1$_{inst.}$, respectively.


