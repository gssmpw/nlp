\section{Related works}


\subsection{Referring expression understanding datasets}
Referring expression understanding identifies specific regions within images or videos using natural language expressions. Two primary subtasks are Referring Expression Comprehension (REC) and Referring Expression Segmentation (RES). REC outputs detection bounding boxes, while RES outputs segmentation masks. Early datasets such as ReferIt~\cite{kazemzadeh2014referitgame} and RefCOCO~\cite{refcoco} are pioneering but are limited to single-target expressions and simple expressions. Subsequently, datasets~\cite{grefcoco, ovvg, riscq, xie2024described} introduce more challenges. gRefCOCO~\cite{grefcoco} supports multi-target expressions. OV-VG~\cite{ovvg} enables an open-vocabulary setting. D$^3$~\cite{xie2024described} and RIS-CQ~\cite{riscq} introduce more complex expressions. Additionally, domain-specific datasets~\cite{zhan2023rsvg,peng2024ravar,qu2024rio} emerge to address unique applications. RSVG~\cite{zhan2023rsvg} focuses on remote sensing, RAVAR~\cite{peng2024ravar} targets video action recognition, and RIO~\cite{qu2024rio} addresses affordance detection. However, drone scenes remain unexplored in REC tasks, which motivates our introduction of the RefDrone benchmark. \par

Recent advances in dataset creation have been significantly influenced by large language models (LLMs). Notable works such as LLaVA \cite{llava} and Ferret \cite{you2023ferret} show the effectiveness of LLMs in generating instruction tuning data. Similarly, RIS-CQ~\cite{riscq} and RIO~\cite{qu2024rio} employ LLMs to generate complex referring expressions. However, these approaches have two limitations. First, LLMs are used mainly as text generators, potentially producing expressions without proper grounding in visual context. Second, the lack of iterative feedback mechanisms in the annotation pipelines may degrade annotation quality.



 \begin{figure*}[t]
\centering
\includegraphics[width = 1\linewidth]{fig/main_fig_v9.pdf}
\caption{The overview of the RefDrone annotation process with RDAgent. Multiple specialized agents collaborate both with each other and human annotators through iterative feedback loops to generate high-quality annotations.}

\label{fig:agent}
\end{figure*}

\subsection{Referring expression comprehension methods}
Referring expression comprehension methods can be broadly categorized into specialist models and large multimodal models (LMMs). Specialist models include two-stage and one-stage methods. Two-stage methods~\cite{hu2017modeling, liu2019learning, hong2019learning, reclip, han2024zero} typically approach REC as a ranking task, first generating region proposals and then ranking them based on language input. However, they are often criticized for slow inference speeds. In contrast, one-stage methods~\cite{mdetr, glip, liu2023dq, gan2020large, yan2023universal, gdino} directly predict target regions guided by language. These approaches leverage transformer architectures to enable cross-modal interactions between visual and textual features. Among these methods, GroundingDINO (GDINO)~\cite{gdino} has gained widespread attention for its impressive results in both open-set object detection and REC tasks. However, GDINO lacks explicit mechanisms to handle multi-target or no-target scenarios.\par


Recent studies have increasingly evaluated LMMs~\cite{llava, you2023ferret, chen2023minigpt, chen2023shikra, bai2023qwen, peng2023kosmos, one_peace, lin2023sphinx, zhan2025griffon, wang2023cogvlm} on REC tasks to assess their visual-language understanding capabilities. These models leverage large-scale referring instruction tuning data to achieve remarkable performance. Despite their broad capabilities, LMMs consistently struggle with small object detection, primarily due to their inherent input resolution constraints. This constraint forces image downsampling, resulting in the loss of fine-grained visual details.



