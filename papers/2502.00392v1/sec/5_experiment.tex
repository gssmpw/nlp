% 使用示例
\begin{algorithm}[t]
\scriptsize
\caption{Query selection.}
\label{code:qs}
\definecolor{codeblue}{rgb}{0.25,0.5,0.5}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codekw}{rgb}{0.85, 0.18, 0.50}
\lstset{
  backgroundcolor=\color{white},
  basicstyle=\fontsize{7.5pt}{7.5pt}\ttfamily\selectfont,
  columns=fullflexible,
  breaklines=true,
  captionpos=b,
  commentstyle=\fontsize{7.5pt}{7.5pt}\color{codegreen},
  keywordstyle=\fontsize{7.5pt}{7.5pt},
  % \color{codekw},
  escapechar={|}, 
}
\begin{lstlisting}[language=Python]
"""
Input:
detection_queries: (bs, length_dquery, ndim)
number_queries: (bs, length_nquery, ndim)

Output:
selected_number_queries: (bs, length_snquery, ndim)
"""

number_feat = FFN(detection_queries) 
# (bs, length_dquery, 1)
number_feat = number_feat.mean(dim=1)  # (bs, 1) 
number = sigmoid(number_feat) # (bs, 1)
index = slice(length_snquery*number, 
        length_snquery*(number+1)) 
selected_number_queries = number_queries[index] 
# (bs, length_snquery, ndim)

\end{lstlisting}
\end{algorithm}

\section{Experiments}

\subsection{Construction of RDAgent and NGDINO}
\textbf{RDAgent.} 
A straightforward method is to adapt RDAgent for REC tasks. This adaptation automates the process by replacing the object input with predictions from an object detector and removing the human verification step. For a fair comparison, we employ Faster R-CNN~\cite{fasterrcnn} as the object detector, which is trained on the VisDrone2019-DET~\cite{visdrone} dataset. The agent component utilizes GPT-4o\footnote{https://openai.com/index/hello-gpt-4o/}.\par


\noindent
\textbf{NGDINO.} The NGDINO adopts a two-stage procedure to maintain training stability. First, we pre-train the number prediction head on the RefDrone dataset while initializing other components with GDINO~\cite{gdino} parameters. Then, we fine-tune the entire model.


\subsection{Implementation details}
We establish the benchmark with 13 representative models that can perform REC tasks, comprising 3 specialist models and 10 LMMs. The specialist models include MDETR~\cite{mdetr}, GLIP~\cite{glip}, and GDINO~\cite{gdino}. For LMMs, we evaluate Shikra~\cite{chen2023shikra}, ONE-PEACE~\cite{one_peace}, SPHINX-v2~\cite{lin2023sphinx}, MiniGPT-v2~\cite{chen2023minigpt}, Ferret~\cite{you2023ferret}, Kosmos-2~\cite{peng2023kosmos}, Griffon~\cite{zhan2025griffon}, Qwen-VL~\cite{bai2023qwen}, CogVLM~\cite{wang2023cogvlm}, and LLaVA~\cite{llava}. Detailed model specifications are provided in the Supplementary Material.\par
\smallskip
\noindent\textbf{Zero-shot evaluation details.} For zero-shot evaluation, we use the original model checkpoints as provided in their respective papers. The implementations and checkpoints for GLIP and GDINO are obtained from the MMDetection~\cite{mmdetection}.\par
\smallskip
\noindent\textbf{Fine-tuning evaluation details.} Our fine-tuning protocol maintains consistency across all experiments to ensure a fair comparison. We preserve the original learning strategies while excluding random crop augmentation due to its negative effect on position-sensitive samples. For LMMs, we employ the LoRA~\cite{hu2021lora} fine-tuning strategy and follow the instruction tuning data structures. All fine-tuning experiments run for 5 epochs using 4 NVIDIA A100 GPUs. 


\subsection{Experimental results}
\input{table/zero-shot}
\noindent\textbf{Zero-shot results.} 
Table~\ref{tab:zeroshot} presents the zero-shot evaluation results, assessing the models' domain generalization capability. CogVLM~\cite{wang2023cogvlm} demonstrates state-of-the-art performance across multiple metrics. However, several advanced models, including Shikra~\cite{chen2023shikra}, ONE-PEACE~\cite{one_peace}, and SPHINX-v2~\cite{lin2023sphinx},  are limited to outputting only a single bounding box, due to constraints in their pre-training data or output strategy. This restriction significantly impacts their performance in multi-target scenarios.\par

\input{table/finetune}


\medskip
\noindent\textbf{Fine-tuning results.} 
Table~\ref{tab:finetune} illustrates the fine-tuning performance across different methods. The specialist model MDETR~\cite{mdetr} shows strong performance in instance-level metrics, achieving comparable results to GDINO-B~\cite{gdino}, but struggles in image-level understanding. Conversely, LMMs like Qwen-VL~\cite{bai2023qwen} demonstrate superior image-level comprehension (F1$_{img.}$: 20.10\%, Acc$_{img.}$: 11.17\%) while struggling with instance-level tasks (F1$_{inst.}$: 14.14\%, Acc$_{inst.}$: 7.61\%). This performance disparity can be attributed to LMMs' strong global image understanding capabilities but limited effectiveness in detecting small objects due to input resolution constraints. \par

The proposed RDAgent achieves superior results compared to existing approaches, particularly in instance-level metrics, surpassing GDINO-B~\cite{gdino} by 26.18\% in F1$_{inst.}$ and 21.14\% in Acc$_{inst.}$. The relatively smaller gains in image-level metrics (5.38\% in F1$_{img.}$ and 1.28\% in Acc$_{img.}$) highlight the importance of instance-level metrics. However, RDAgent's multi-step pipeline and reliance on ChatGPT responses result in longer processing time compared to end-to-end approaches.\par

The proposed end-to-end method, NGDINO, consistently improves over the baseline GDINO~\cite{gdino} across both backbone architectures. Figure~\ref{fig:results} shows the visualization results comparing NGDINO and baseline GDINO, demonstrating NGDINO's effectiveness with multi-target samples.


\input{table/grefcoco}
To further validate the effectiveness of the proposed NGDINO, we conduct additional experiments on the gRefCOCO~\cite{grefcoco} dataset, which also includes multi-target and no-target samples (Table~\ref{tab:results_grec}). NGDINO-T outperforms the baseline method GDINO-T, particularly in N-acc. metrics (by 4.15\% in test A and 1.39\% in test B), which evaluate accuracy on no-target samples. The limited improvements in Pr@0.5 metrics can be attributed to the relatively simple nature of multi-target samples in gRefCOCO, which typically contain only two objects per sample.



\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{fig/results_good.pdf}
    \caption{Visualization results comparing NGDINO and GDINO~\cite{gdino}, where red boxes indicate true positives and yellow boxes denote false negatives.}
    \label{fig:results}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{fig/results_failing.pdf}
\caption{Failure cases of NGDINO on RefDrone dataset. Red, green, and yellow boxes indicate true positives, false positives, and false negatives, respectively.}
    \label{fig:fail}
\end{figure}

\input{table/ngdino_abl}
\input{table/ablation_length}
\input{table/agent_abl}


\subsection{Ablation studies}
\textbf{Ablations of NGDINO components.} Table~\ref{tab:ngdino_ablation} presents the analysis of each component in NGDINO. The number prediction head alone shows a slight increase in Acc$_{inst.}$ from 19.02\% to 19.09\%, indicating minimal impact of this auxiliary head on the referring tasks. When introducing the number cross-attention without number selection, we observe a more substantial improvement, with Acc$_{inst.}$ increasing from 19.02\% to 19.51\%. This improvement is attributed to the additional parameters introduced in the decoder. The significant improvement is achieved through the combination of both the number prediction task and number cross-attention components. This contribution increases Acc$_{inst.}$ from 19.02\% to 20.98\%.  While the additional computational cost results in a minor decrease in inference speed (FPS), this trade-off yields a 1.96\% increase in Acc$_{inst.}$.

\par
\medskip
\noindent\textbf{Ablations of query length.} Table~\ref{tab:length_ablation} analyzes the impact of varying the selected number query length. Utilizing a minimal query length of 1 lacks the capacity to capture complex numerical information. Conversely, extending the query length to 100 increases parameter count and introduces optimization difficulties, thereby adversely affecting performance. Through these experiments, we determine that a query length of 10 provides an optimal trade-off.\par
\medskip
\noindent\textbf{Number prediction head performance.} The number prediction head achieves an overall number prediction accuracy of 53.5\%. During the bounding box prediction stage, this accuracy is observed to be 22.84\%. This performance gap demonstrates the effectiveness of number prediction head. Besides, the number prediction achieves a mean absolute error (MAE) of 0.51, suggesting high precision as predictions closely align with ground truth values.\par

\medskip
\noindent\textbf{Ablations of RDAgent.} Table~\ref{tab:agent_ablation} analyzes the effectiveness of the proposed RDAgent. The GPT4-o baseline is established using only Step 3 of RDAgent. We also compare against ReCLIP~\cite{reclip}, a two-stage instance ranking method using CLIP~\cite{clip} for REC tasks. All experiments utilize Faster-RCNN~\cite{fasterrcnn} for object detection. Results show that RDAgent consistently outperforms GPT4-o and ReCLIP across all metrics.  However, the performance is partially limited by the Faster-RCNN object detector, which achieves only 18.0 mAP on the VisDrone2019-DET~\cite{visdrone} dataset.\par



\subsection{Limitations}
While NGDINO addresses the multi-target and no-target challenges, several challenges remain. Figure~\ref{fig:fail} shows typical NGDINO failure cases, highlighting challenges in the RefDrone dataset. The examples show rich contextual expressions, challenging backgrounds, and small-scale object detection. These challenging cases reflect real-world applications and highlight the areas for future improvement.





