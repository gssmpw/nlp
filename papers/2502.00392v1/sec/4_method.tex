\section{Proposed method}
We introduce Number GroundingDINO (NGDINO) to address multi-target and no-target challenges based on GDINO~\cite{gdino}. Our insight is that utilizing number information of referred objects enhances model performance on these challenges. As illustrated in Figure~\ref{fig:ngdino}, we introduce three components: (1) a number prediction head, (2) number-queries, and (3) a number cross-attention module.




\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{fig/ngdino_v3.pdf}
\caption{Architecture of a single decoder layer in Number GroundingDINO. Key modifications from GDINO~\cite{gdino} (highlighted in yellow box) include: (1) number prediction head (FFN) to predict target number, (2) number-queries selected through the predicted number, (3) number cross-attention between selected number-queries and detection-queries.}

    \label{fig:ngdino}
\end{figure}

\subsection{NGDINO}
Following GDINO, our model employs a dual-encoder-single-decoder architecture. The framework consists of an image backbone for visual feature extraction, a text backbone for text feature extraction, a feature enhancer for cross-modal fusion, a language-guided query selection module for detection-query initialization, and a cross-modality decoder for box refinement. Within decoder layers, detection-queries are fed into a self-attention layer, an image cross-attention layer to combine image features, a text cross-attention layer to combine text features, and an FFN layer. We mainly improve the decoder part, highlighted by the yellow box in Figure~\ref{fig:ngdino}.\par

The number prediction head, structured similarly to the detection head with an FFN layer, predicts the number of referred objects from detection-queries. These detection-queries are used to predict the objects, containing number information. The number-queries are learnable embeddings designed to capture various numerical patterns. These queries are initialized randomly with dimensions (\texttt{bs}, \texttt{length\_nquery}, \texttt{ndim}), where \texttt{bs} is the batch size, \texttt{length\_nquery} represents the number of query lengths, and \texttt{ndim} denotes the feature dimension. The predicted number guides the selection of number-queries through the query selection in Algorithm~\ref{code:qs}. The selected number-queries have a fixed length of \texttt{length\_snquery}.\par

We implement number cross-attention in parallel with self-attention to integrate the number information. In the number cross-attention, the selected number-queries serve as keys and values, while detection-queries serve as queries. The outputs are added to the self-attention features.




\subsection{Loss function}
For the bounding box supervision, we adopt the loss functions in GDINO~\cite{gdino}. For number prediction, we use L2 loss. To address the challenges posed by varying target counts in real-world scenarios, we quantize the number prediction space into five categories: \{0, 1, 2, 3, 4+\}, where 4+ represents all counts $\geq$ 4. Through ablation studies, we set the selected number-queries length (\texttt{length\_snquery}) to 10 and the number-queries length (\texttt{length\_nquery}) to 50, corresponding to the five categories.

