[
  {
    "index": 0,
    "papers": [
      {
        "key": "zhong2017seq2sql",
        "author": "Victor Zhong and Caiming Xiong and Richard Socher",
        "title": "{Seq2SQL}: Generating Structured Queries from Natural Language using\nReinforcement Learning"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "pourreza2023dinsqldecomposedincontextlearning",
        "author": "Mohammadreza Pourreza and Davood Rafiei",
        "title": "{DIN}-{SQL}: Decomposed In-Context Learning of Text-to-{SQL} with Self-Correction"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "ni2023lever",
        "author": "Ni, Ansong and Iyer, Srini and Radev, Dragomir and Stoyanov, Veselin and Yih, Wen-tau and Wang, Sida and Lin, Xi Victoria",
        "title": "{LEVER}: Learning to Verify Language-to-Code Generation with Execution"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "ZhangLZ24",
        "author": "Siyue Zhang and\nAnh Tuan Luu and\nChen Zhao",
        "title": "{SynTQA}: Synergistic Table-based Question Answering via Mixture of\n{Text-to-SQL} and {E2E} {TQA}"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "vaswani2017attention",
        "author": "Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\\L}ukasz and Polosukhin, Illia",
        "title": "Attention Is All You Need"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "yin-etal-2020-tabert",
        "author": "Yin, Pengcheng  and\nNeubig, Graham  and\nYih, Wen-tau  and\nRiedel, Sebastian",
        "title": "{T}a{BERT}: Pretraining for Joint Understanding of Textual and Tabular Data"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "devlin-etal-2019-bert",
        "author": "Devlin, Jacob  and\nChang, Ming-Wei  and\nLee, Kenton  and\nToutanova, Kristina",
        "title": "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "wang2021tuta",
        "author": "Wang, Zhiruo and Dong, Haoyu and Jia, Ran and Li, Jia and Fu, Zhiyi and Han, Shi and Zhang, Dongmei",
        "title": "{TUTA}: Tree-Based Transformers for Generally Structured Table Pre-training"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "herzig-etal-2020-tapas",
        "author": "Herzig, Jonathan  and\nNowak, Pawel Krzysztof  and\nM{\\\"u}ller, Thomas  and\nPiccinno, Francesco  and\nEisenschlos, Julian",
        "title": "{T}a{P}as: Weakly Supervised Table Parsing via Pre-training"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "liu2022tapex",
        "author": "Qian Liu and Bei Chen and Jiaqi Guo and Morteza Ziyadi and Zeqi Lin and Weizhu Chen and Jian-Guang Lou",
        "title": "{TAPEX}: Table Pre-training via Learning a Neural {SQL} Executor"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "lewis-etal-2020-bart",
        "author": "Lewis, Mike  and\nLiu, Yinhan  and\nGoyal, Naman  and\nGhazvininejad, Marjan  and\nMohamed, Abdelrahman  and\nLevy, Omer  and\nStoyanov, Veselin  and\nZettlemoyer, Luke",
        "title": "{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "jiang-etal-2022-omnitab",
        "author": "Jiang, Zhengbao  and\nMao, Yi  and\nHe, Pengcheng  and\nNeubig, Graham  and\nChen, Weizhu",
        "title": "{O}mni{T}ab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "yu-etal-2018-spider",
        "author": "Yu, Tao  and\nZhang, Rui  and\nYang, Kai  and\nYasunaga, Michihiro  and\nWang, Dongxu  and\nLi, Zifan  and\nMa, James  and\nLi, Irene  and\nYao, Qingning  and\nRoman, Shanelle  and\nZhang, Zilin  and\nRadev, Dragomir",
        "title": "{S}pider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-{SQL} Task"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "liu-etal-2024-rethinking",
        "author": "Liu, Tianyang  and\nWang, Fei  and\nChen, Muhao",
        "title": "Rethinking Tabular Data Understanding with Large Language Models"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "wei2022chain",
        "author": "Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny",
        "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "cheng2023binding",
        "author": "Zhoujun Cheng and Tianbao Xie and Peng Shi and Chengzu Li and Rahul Nadkarni and Yushi Hu and Caiming Xiong and Dragomir Radev and Mari Ostendorf and Luke Zettlemoyer and Noah A. Smith and Tao Yu",
        "title": "Binding Language Models in Symbolic Languages"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "abs-2410-12846",
        "author": "Yuxiang Wang and\nJianzhong Qi and\nJunhao Gan",
        "title": "Accurate and Regret-aware Numerical Problem Solver for Tabular Question\nAnswering"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "ye2023large",
        "author": "Ye, Yunhu and Hui, Binyuan and Yang, Min and Li, Binhua and Huang, Fei and Li, Yongbin",
        "title": "Large Language Models are Versatile Decomposers: Decomposing Evidence and Questions for Table-based Reasoning"
      },
      {
        "key": "lee2024learning",
        "author": "Lee, Younghun and Kim, Sungchul and Rossi, Ryan A and Yu, Tong and Chen, Xiang",
        "title": "Learning to reduce: Towards improving performance of large language models on structured data"
      },
      {
        "key": "patnaik2024cabinet",
        "author": "Sohan Patnaik and Heril Changwal and Milan Aggarwal and Sumit Bhatia and Yaman Kumar and Balaji Krishnamurthy",
        "title": "{CABINET}: Content Relevance-based Noise Reduction for Table Question Answering"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "patnaik2024cabinet",
        "author": "Sohan Patnaik and Heril Changwal and Milan Aggarwal and Sumit Bhatia and Yaman Kumar and Balaji Krishnamurthy",
        "title": "{CABINET}: Content Relevance-based Noise Reduction for Table Question Answering"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "wang2024chainoftable",
        "author": "Zilong Wang and Hao Zhang and Chun-Liang Li and Julian Martin Eisenschlos and Vincent Perot and Zifeng Wang and Lesly Miculicich and Yasuhisa Fujii and Jingbo Shang and Chen-Yu Lee and Tomas Pfister",
        "title": "Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "ye2023large",
        "author": "Ye, Yunhu and Hui, Binyuan and Yang, Min and Li, Binhua and Huang, Fei and Li, Yongbin",
        "title": "Large Language Models are Versatile Decomposers: Decomposing Evidence and Questions for Table-based Reasoning"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "lee2024learning",
        "author": "Lee, Younghun and Kim, Sungchul and Rossi, Ryan A and Yu, Tong and Chen, Xiang",
        "title": "Learning to reduce: Towards improving performance of large language models on structured data"
      }
    ]
  }
]