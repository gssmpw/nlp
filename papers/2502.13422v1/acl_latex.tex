% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{booktabs} 
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{adjustbox}
\usepackage{listings}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{subcaption}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\newcommand{\sdataset}{SLQA}
\newcommand{\ldataset}{SEQA}
\newcommand{\wdataset}{WTQ}
\newcommand{\hdataset}{HybQA}
\newcommand{\model}{\textsc{TabSD}}
\newcommand{\smodel}{SQL Generator}
\newcommand{\dmodel}{Table Decomposer}
\newcommand{\amodel}{Answer Generator}
\newcommand{\vmodel}{SQL Verifier}

\title{TabSD: Large Free-Form Table Question Answering with SQL-Based Table Decomposition}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{
  Yuxiang Wang  \hspace{10mm} Junhao Gan  \hspace{10mm} Jianzhong Qi \vspace{3mm} \\
  School of Computing and Information Systems, The University of Melbourne \\
  \texttt{yuxiang.wang8@student.unimelb.edu.au} \\
  \texttt{\{junhao.gan, jianzhong.qi\}@unimelb.edu.au}\\
}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}




\begin{document}
\maketitle



\begin{abstract}
Question answering on free-form tables (TableQA) is challenging due to the absence of predefined schemas and the presence of noise in large tables. While Large Language Models (LLMs) have shown promise in TableQA, they struggle with large free-form tables and noise sensitivity. To address these challenges, we propose \textbf{\model}, a SQL-based decomposition model that enhances LLMs' ability to process large free-form tables. \model~generates SQL queries to guide the table decomposition, remove noise, and processes sub-tables for better answer generation. Additionally, \vmodel\ refines SQL outputs to enhance decomposition accuracy. We introduce two TableQA datasets with large free-form tables, \sdataset\ and \ldataset, which consist solely of large free-form tables and will be publicly available. Experimental results on four benchmark datasets demonstrate that \model\ outperforms the best-existing baseline models by $23.07\%$, $2.84\%$, $23.24\%$ and $9.32\%$ in accuracy, respectively, highlighting its effectiveness in handling large and noisy free-form tables. 
\end{abstract}

\section{Introduction}
Tables play a crucial role in managing and analyzing large-scale data. Numerous studies have explored various tabular understanding tasks, such as \emph{Table Question Answering} (TableQA) \cite{pasupat-liang-2015-compositional, yin-etal-2020-tabert}, \emph{Table Fact Verification} \cite{ChenWCZWLZW20, herzig-etal-2020-tapas}, \emph{Table-to-text Generation} \cite{bao2018table, zhang2024opent2t}. Among these tasks, TableQA remains one of the most challenging, requiring models to comprehend both natural language problems and the corresponding free-form tabular data. Unlike structured tables, free-form tables, such as web tables \cite{cafarella2008webtables}, lack predefined schemas, making structural-query-based approaches \cite{pasupat-liang-2015-compositional, zhong2017seq2sql} inapplicable. 

%Benefit from the strong performance of Large Language Models (LLMs), has led to promising progress in TableQA tasks \cite{liu-etal-2024-rethinking, abs-2410-12846, wang2024chainoftable}. However, existing studies primarily focus on datasets with small tables, limiting model robustness when dealing with large tables. Additionally, there are few publicly available TableQA datasets with large free-form tables—common in real-world domains such as finance, medical, and scientific research \cite{zavitsanos2024entrant, singhal2025toward, abs-2407-09413}. Furthermore, prior work has demonstrated that the performance of LLM-based TableQA is highly sensitive to noise in tables \cite{ye2023large}. While some efforts have been made to enhance TableQA for large tables \cite{ye2023large, patnaik2024cabinet, lee2024learning}, there remains substantial room for improvement.

With the advent of LLMs demonstrating strong performance, extensive research has been directed towards Free-Form TableQA \cite{liu-etal-2024-rethinking, abs-2410-12846, wang2024chainoftable}. While some work has demonstrated that the performance of LLM-based TableQA is highly sensitive to noise in tables, existing methods struggle to effectively handle noisy data \cite{ye2023large, wang2024chainoftable}. Besides, current TableQA datasets is insufficiently reflect real-world application scenarios, such as finance, healthcare, and scientific research~\cite{zavitsanos2024entrant, singhal2025toward, abs-2407-09413}, where large free-form tables are prevalent. Most existing studies focus on datasets with relatively small tables~\cite{cheng2023binding, liu-etal-2024-rethinking, abs-2410-12846}, as large free-form tables are scarce in publicly available datasets~\cite{pasupat-liang-2015-compositional, NanHMLVZKSKTMRT22, ChenWCZWLZW20}, which weakens model robustness. Furthermore, the inherent noise in large free-form tables remains an open challenge.

To address these challenges, we introduce two \emph{LLM-augmented} datasets featuring large free-form tables: (1) Spider-LLM Large Table QA Dataset (denoted as \texttt{\sdataset}), which directly adopts large tables from \texttt{Spider} dataset~\cite{yu-etal-2018-spider} and generates QA pairs using an LLM; and (2) Spider-LLM Expanded Table QA Dataset (denoted as \texttt{\ldataset}), which randomly selects small tables from \texttt{Spider} dataset, expands them into large tables and generates corresponding QA pairs using LLMs. Meanwhile, we extract large tables from the \texttt{WikiTableQuestions} dataset \cite{pasupat-liang-2015-compositional} and the \texttt{HybridQA} dataset \cite{ChenZCXWW20} to supplement our benchmark. We then investigate how to enhance LLM-based TableQA models for large free-form tables. Our findings reveal that while LLMs struggle with large free-form tables, their accuracy significantly improves when noisy data is removed -- see Figure~\ref{fig:tab_decomp} for an example. Moreover, although NL-to-SQL methods alone are insufficient for solving free-form TableQA \cite{ZhangLZ24}, we demonstrate that the SQL queries they generate can help extract relevant sub-tables containing only the necessary information for answering questions.  

\begin{figure}[t]
     \centering
     \includegraphics[width = 0.9\linewidth]{latex/td_example.pdf}
     % \vspace{-0.4cm}
     \caption{TableQA example using an LLM with and without table decomposition.}
     \label{fig:tab_decomp}
\end{figure}

Based on these observations, we propose the \underline{Tab}leQA \underline{S}QL-based \underline{D}ecomposition model, \emph{\model}. Leveraging few-shot in-context learning in LLMs \cite{Dong0DZMLXX0C0S24}, our \emph{\smodel} (an LLM) of \emph{\model} first generates SQL queries. Instead of directly executing these queries to retrieve answers,
our \emph{\dmodel}, a rule-based parser, extracts key information: column names, values, and conditions from the SQL query to guide table decomposition.
This decomposition process helps extract relevant information, resulting in a sub-table that contains only the necessary data for answering the question. 
Finally, the sub-table is passed to \emph{\amodel} to generate the final answer.

Since the SQL queries generated by \emph{\smodel} may be inaccurate, we introduce \emph{\vmodel}, an additional LLM-based module that performs \emph{self-verification} and refinement to improve SQL quality and further enhance table decomposition accuracy.

This paper makes the following contributions:
\begin{itemize}[leftmargin=*, noitemsep, topsep=0pt]
\item We introduce two LLM-augmented TableQA datasets, \texttt{\sdataset} and \texttt{\ldataset}, which consist exclusively of large free-form tables (each exceeding 4096 tokens). To construct these datasets, we first employ an LLM to generate question-answer pairs and then expand small tables into large ones through data synthesis. Code and datasets will be publicly released with this paper.

\item We propose \model, the first model designed to enhance TableQA performance on large free-form tables that typically contain substantial noisy data. By leveraging the SQL queries generated by \smodel, our \dmodel~module extracts noise-free sub-tables, significantly improving the accuracy of \amodel. 

\item We introduce \vmodel, an LLM-based module that verifies and refines the SQL generated by \smodel, making the table decomposition process more reliable and precise.

\item We conduct comprehensive evaluations on four datasets, including two public datasets: \texttt{WikiTableQuesetions}~\cite{pasupat-liang-2015-compositional}, \texttt{HybridQA}~\cite{ChenZCXWW20}, and our two newly proposed datasets: \texttt{\sdataset} and \texttt{\ldataset}. 
The experimental results show that our \model~achieves significant improvements, outperforming the second-best baseline models by $23.07\%$, $2.84\%$, $23.24\%$, and $9.32\%$ in accuracy, respectively. 
\end{itemize}

\section{Related Work}
Existing TableQA studies fall into three groups: 
(1)~\emph{NL-to-SQL-based}, (2)~\emph{pre-trained language model (PLM)-based}, and (3)~\emph{large language model (LLM)-based}.


\paragraph{NL-to-SQL-based Methods} NL-to-SQL-based methods translate natural language questions into logical forms, i.e., SQL queries, which are then executed to retrieve answers. These methods either train a language model or use in-context learning for the translation task. Seq2SQL~\cite{zhong2017seq2sql} is a representative for the former category. It trains a seq2seq model using reinforcement learning, leveraging query execution results as feedback to refine SQL generation. 

Recent works fall into the latter category. DIN-SQL~\cite{pourreza2023dinsqldecomposedincontextlearning} decomposes the NL-to-SQL task into sub-problems and employs in-context learning to generate the final SQL queries. LEVER~\cite{ni2023lever} generates multiple candidate answers via SQL execution and trains a verifier to select the most accurate one. SynTQA~\cite{ZhangLZ24} also uses a verifier. It chooses answers from an NL-to-SQL TableQA module and an end-to-end TableQA module. These methods 
struggle with free-form tables without predefined schemas, where SQL execution is ineffective.


\paragraph{PLM-based Methods} PLMs-based methods also have two sub-categories.

\underline{Structural adaptations for tabular data.} These methods modify the Transformer~\cite{vaswani2017attention} to better capture tabular structures. TaBERT~\cite{yin-etal-2020-tabert} incorporates vertical self-attention to unify textual and tabular representations based on BERT~\cite{devlin-etal-2019-bert}. TUTA~\cite{wang2021tuta} employs a hierarchical tree-based attention with column and table segment masking to model table representations. TAPAS~\cite{herzig-etal-2020-tapas} extends BERT by adding row, column, and rank embeddings, along with table cell masking.

\underline{Fine-tuned end-to-end models.} TAPEX~\cite{liu2022tapex} pre-trains BART~\cite{lewis-etal-2020-bart} using a large synthetic dataset derived from the \texttt{WikiTableQuestions} dataset. 
OmniTab~\cite{jiang-etal-2022-omnitab} also utilizes BART as its backbone  
while pre-training it on both real and synthetic data, 
including SQL queries from the \texttt{Spider}~\cite{yu-etal-2018-spider} dataset and synthetic natural language sentences generated from SQL queries via its SQL2NL module. 
With smaller model sizes, PLMs exhibit limited effectiveness in grasping tabular semantics compared to the latest LLMs, resulting in sub-optimal TableQA accuracy in TableQA.


\paragraph{LLM-based Methods} LLM-based methods use either prompting or program-aided reasoning.

\underline{Prompting-based methods.} These methods prompt LLMs to generate answers by providing it an input question and the corresponding table. For example, Mix-SC~\cite{liu-etal-2024-rethinking} explores the stochastic nature of LLMs by generating multiple candidate answers for each question. It employs two types of prompting techniques: one directs the LLM to act as a Python agent for script execution, while the other utilizes Chain-of-Thought (CoT) prompting~\cite{wei2022chain} to guide step-by-step reasoning. The final answer is selected using a self-consistency mechanism, which chooses the most frequent answer. 

\underline{Program-aided reasoning methods.} These methods  integrate program generation and execution with LLM prompting.  Binder~\cite{cheng2023binding} first prompts the LLM to generate an initial program based on the input question, then it identifies and refines the challenging segments through iterative LLM engagement. The final refined program is executed by an interpreter to produce TableQA answers. Similarly, TabLaP~\cite{abs-2410-12846} utilizes LLMs for Python script generation, targeting numerical reasoning.  Despite their strong reasoning capabilities, LLM-based TableQA models struggle with complex table structures and are highly sensitive to noise, limiting their effectiveness on large free-form tables.

\begin{figure*}[t]
     \centering
     \includegraphics[width = 1\linewidth]{latex/overview.pdf}
     \caption{Overview of \model. The model contains three stages: SQL generation, table decomposition, and answer generation. (i)~The SQL generation stage constructs an SQL query using the input question and the table header information. It verifies and refines the generated SQL to obtain the refined output. (ii)~The table decomposition stage parses the generated SQL (recognizing column names, conditions, and values) to retrieve sub-tables. The column names, conditions, and values are in orange, blue, and purple, respectively. (iii)~The answer generation stage takes the question and the sub-table to build a  prompt and query an LLM for answer generation.}
     \label{fig:model_overview}
\end{figure*}

\paragraph{Table Decomposition for TableQA} Studies have shown that TableQA models perform significantly better when provided with a sub-table (instead of the full one) containing only question-relevant information~\cite{ye2023large, lee2024learning, patnaik2024cabinet}. Table decomposition aims to extract such sub-tables to enhance TableQA. Existing  methods again fall into two groups.

\underline{Soft decomposition.} These methods attenuates irrelevant information while emphasizing the key content, allowing models to focus on relevant data without explicitly removing extraneous entries. For example, CABINET~\cite{patnaik2024cabinet} proposes a relevance scorer and a relevant cell predictor to assign weights to table cells. %enabling more accurate answers for complex tables. 
These methods rely on model fine-tuning, which limits their model generalizability, and are often constrained by a fixed input token length, restricting scalability.

\underline{Hard decomposition.} These methods removes irrelevant contents, extracting a sub-table as the final input table for the TableQA models. For example, Chain-of-Table~\cite{wang2024chainoftable} extends the CoT framework. It progressively extracts a sub-table at each step using an LLM. DATER~\cite{ye2023large} also utilizes the CoT idea, breaking down both tables and input questions into structured sub-tables and sub-questions. Learn-to-Reduce~\cite{lee2024learning} introduces a column selector and a row selector, which are fine-tuned language models that extract only question-relevant rows and columns to form sub-tables. 
These methods often overlook table structures. They flatten tables and select rows/columns purely by semantic relevance, leading to information loss. 

Our work integrates LLM-driven reasoning with NL-to-SQL techniques, enabling more structured-aware table decomposition while retaining key information for TableQA.


\section{Problem Formulation}
%TableQA task in general is 
Given a table \( T \) and a question \( Q \) about the data in \( T \), 
the objective of the TableQA task is to develop a model capable of generating an accurate answer for \( Q \). The question \( Q = (q_1, q_2, \dots, q_{|Q|}) \) is expressed in natural language, where each $q_i \in Q$ ($i \in [1, |Q|]$) represents a token (e.g., a word). The table \( T \) is also represented as a token sequence in natural language, where individual cells are separated by special characters such as `$\vert$', and rows are delineated by newline characters. The model's performance is evaluated by comparing its generated answers against ground-truth answers in a token-based manner.
To tackle \emph{TableQA over large tables} effectively, our approach addresses two key sub-problems: \emph{NL-to-SQL} and \emph{Table Decomposition}.

\paragraph{NL-to-SQL} This problem involves generating an SQL query \( S \) from question \( Q \) and the table header information $H = (h_1, h_2, \dots, h_{|H|})$ corresponding to table $T$, where each $h_i \in H$ ($i \in [1, |H|]$) represents a column in $T$. The objective is to produce an \( S \) that accurately captures the intent of \( Q \) based on \( H \). Rather than executing \( S \), our focus is on parsing \( S \) to extract key components for table decomposition, including the column names \( C \), the conditions \( P \) (e.g., LIKE clauses), and the values \( V \) involved in the filtering operations of \( S \).


\paragraph{Table Decomposition} This problem aims to extract a sub-table $T_{\text{SUB}}$ from table \( T \) based on the extracted components: 
column names \( C \), conditions \( P \), and values \( V \).  Given table \( T \), where $ C = \{c_1, c_2, \dots, c_{|C|}\} $ represents the set of selected columns, $ P = \{p_1, p_2, \dots, p_{|P|}\} $ represents the filtering conditions, and $V = \{v_1, v_2, \dots, v_{|V|}\} $ is the values used in the  conditions, the goal is to construct \( T_{\text{SUB}} \) such that it retains only the specified columns in \( C \) and includes only rows satisfying \( P \). This process is implemented using \emph{Python DataFrame operations}, which handle both column selection and row filtering.

\section{Methodology}
\model\ handles TableQA in three stages: (i)~\emph{SQL generation}, (ii)~\emph{table decomposition}, and (iii)~\emph{answer generation}, as outlined in Algorithm~\ref{alg:model}.

The SQL generation stage (Section~\ref{sec:sql_gen}) uses a \smodel\ and a \vmodel. The \smodel\ takes question $Q$ and the header information of table $T$ to generate a raw SQL query $S_{\text{RAW}}$. Then, the \vmodel\ verifies and refines $S_{\text{RAW}}$ to produce the final SQL query $S$.

The table decomposition stage (Section~\ref{sec:tab_decomp}) uses a \dmodel\ to parse $S$ and extract a triple $(C, P, V)$, which represents column names, conditions, and values, respectively. The triple is consumed by \emph{Python DataFrame operations} to construct a sub-table $T_{\text{SUB}}$ with only question-relevant rows and columns.

The answer generation stage (Section~\ref{sec:ans_gen}) uses a \amodel\ that takes  $Q$ and $T_{\text{SUB}}$ to generate the final answer $A$ with an LLM. 

\begin{algorithm}[t]
\caption{TableQA with \model{}}
\label{alg:model}
\small
\begin{algorithmic}[1]
\REQUIRE Table $T$, questions $Q$, \smodel\  $M_{\text{SQL}}$, \vmodel\ $M_{\text{V}}$, \dmodel\  $M_{\text{D}}$, and \amodel\ $M_{\text{A}}$.
%\FOR {each pair $(T, Q)$} 
    \STATE \textbf{SQL Generation:}
    \item[] (1) Generate $\text{prompt}_{\text{SQL}}$ for $M_{\text{SQL}}$ using $Q$ and the header information of $T$
    \item[] (2) Obtain an SQL query $S_{\text{RAW}}$ and verify it with $M_{\text{V}}$ to produce the final SQL query $S$
    \STATE \textbf{Table Decomposition:} 
   \item[] (1) Parse the information within $S$ through $M_{\text{D}}$ to obtain a triple $(C, P, V)$, representing column names, conditions and values, respectively
   \item[] (2) Use $(C, P, V)$ as table decomposition information to extract a sub-table $T_{\text{SUB}}$ through $M_{\text{D}}$
   \STATE \textbf{Answer Generation:} 
    \item[] (1) Use $T_{\text{SUB}}$ and $Q$ to build answer generation  $\text{prompt}_{\text{A}}$ for $M_{\text{A}}$
   \item[] (2) Obtain the final answer $A$ for the given question $Q$
\RETURN $A$
%\ENDFOR
\end{algorithmic}
\end{algorithm} 

\subsection{SQL Generation}\label{sec:sql_gen} 
We use a \smodel\ to produce a raw SQL query $S_{\text{RAW}}$, which is passed to a \vmodel\ for validation. If $S_{\text{RAW}}$ fails validation, the \vmodel\ refines it to generate a final SQL query $S$.

The \smodel\ uses few-shot prompting with a backbone LLM. Given question $Q$ and the header information of table $T$, we construct the prompt using examples from a manually annotated NL-to-SQL dataset \texttt{SQUALL}~\cite{ShiZBDL20}.


\paragraph{Example Selection} To retrieve relevant NL-to-SQL examples, we compute the cosine similarity between the embedding of $Q$ and those of all questions in \texttt{SQUALL}, denoted as $D = \{d_1, d_2, ..., d_N\}$, using Sentence Transformers~\cite{Song0QLL20}:
\begin{equation}
\mathbf{e_Q} = f(Q), \quad \mathbf{E_D} = \{ f(d_i) \}_{i=1}^{N}\,,
\end{equation}
where $f(\cdot)$ is an embedding model, $\mathbf{e_Q} \in \mathbb{R}^{1 \times m}$, $\mathbf{E_D} \in \mathbb{R}^{N \times m}$, and $m$ denotes  dimensionality. 

We select the top $K$ examples based on cosine similarity:
\begin{equation}
\mathbf{Z} = \operatorname{argsort}(\frac{\mathbf{e_Q} \mathbf{E_D}^T}{\|\mathbf{e_Q}\| \cdot \|\mathbf{E_D}\|})\,,
\end{equation}
\begin{equation}
\operatorname{TopK}(D) = \{ d_{i_j} \mid i_j \in \mathbf{Z}\}, j\in [N-K+1\colon N]\,,
\end{equation}
where $i \in [1, |D|]$, and $K = 5$ by default.

The \smodel\ uses the selected NL-to-SQL examples, the header information of $T$, and the question $Q$ to build a prompt (see Appendix~\ref{app:prompts}). This prompt is then used to query the LLM, generating a raw SQL query $S_{\text{RAW}}$.


\paragraph{SQL Verification} Since our table decomposition highly relies on the quality of the SQL query $S_{\text{RAW}}$, we introduce the \vmodel\ to verify whether $S_{\text{RAW}}$ leads to a correct sub-table. The verification procedure is as follow:

(1)~Initial sub-table extraction. We use the \dmodel\ (detailed in the next section) to parse $S_{\text{RAW}}$ and obtain a raw sub-table $T_{\text{SUB}}\prime$. 

(2)~Validation prompting. The \vmodel\ is prompted with $T_{\text{SUB}}\prime$, the header information of  $T$ , and the 
question $Q$. It determines whether $T_{\text{SUB}}\prime$ contains all the necessary information to answer $Q$ (see the prompt template in Appendix~\ref{app:prompts}). 

(3)~Decision making. If the \vmodel\ outputs \texttt{[True]}, $S_{\text{RAW}}$ is accepted as the final SQL query $S$. 
Otherwise, the \vmodel\ refines $S_{\text{RAW}}$ by prompting the LLM to generate an improved SQL query (see the prompt template in Appendix~\ref{app:prompts}). 

(4)~Iteration control. This verification and refinement process can be conducted for up to $n$ rounds. By default, $n = 1$ for cost efficiency. 

At the end of this SQL verification stage, we obtain the final SQL query $S$.

\subsection{Table Decomposition}\label{sec:tab_decomp} 
The \dmodel\ takes $T$ and $S$ (obtained in the SQL generation stage) as input and adopts a rule-based method to extract sub-table $T_{\text{SUB}}$.


\paragraph{SQL Parsing} To extract key information from $S$, we employ a  \emph{regex-based} rule-driven approach. We use regular expressions to parse $S$ and extract the column names $C$, conditions $P$ (e.g., filtering clauses), and values $V$ (e.g., values in the filtering clauses). The detailed regular expressions are included in Appendix~\ref{app:tab_decomp}.


\paragraph{Table Decomposition} 
With the extracted triple $(C, P, V)$, we perform table decomposition. We first convert table $T$ into a Python DataFrame.  Then, we apply DataFrame operations to extract rows and columns based on $(C, P, V)$, obtaining the final sub-table $T_{\text{SUB}}$. The correspondence between Python DataFrame operations and SQL queries is detailed in Appendix~\ref{app:tab_decomp}.

\begin{figure}[t]
     \centering
     \includegraphics[width = 1\linewidth]{latex/qa_example.pdf}
     \caption{Example QA pairs generated.}
     \label{fig:qa_gen}
\end{figure}

\subsection{Answer Generation}\label{sec:ans_gen} 
At this stage, we exploit the strong semantic-parsing and reasoning capabilities of an LLM-based model to perform \emph{end-to-end} TableQA. 
We construct a prompt (see prompt template in Appendix~\ref{app:prompts}) containing $T_{\text{SUB}}$ and $Q$, which is fed into the \amodel\ (an LLM) to generate the final answer $A$.  


\section{Dataset Construction}\label{sec:datasets}
Due to the limited availability of TableQA datasets with large tables, to study model performance in such settings, we construct two datasets. 

\textbf{\sdataset} is construed by 
extracting  
all the large tables (with more than 4,096 tokens) from the \texttt{Spider} dataset~\cite{yu-etal-2018-spider}. 
The original question-answering (QA) pairs in \texttt{Spider} are for NL-to-SQL tasks rather than TableQA. 
To bridge this gap, we propose an LLM-based QA pair generation method that prompts the LLM to generate questions that cannot be answered easily by NL-to-SQL solutions. 

\underline{QA pair generation.} 
We observe that directly prompting an LLM with a large table for QA generation, or arbitrarily selecting a table cell as the ground-truth answer and using it to generate the corresponding question, often leads to low-quality QA pairs, with an acceptance rate well below $50\%$.

To overcome this issue, we propose a \emph{self-adaptive} QA generation method, where the LLM autonomously selects an answer -- either a specific cell or an answer field (i.e., a section of the table) -- and then generates the corresponding QA pair based on this selection. 
This method significantly improves QA pair quality, achieving an acceptance rate of over $70\%$ (see details in Appendix~\ref{app:dataset}). 

We propose three answer (field) selection strategies: (i)~cell-based, which prompts the LLM to randomly select a single cell in the table as the ground-truth answer and generate a corresponding question; (ii)~column/row-based, which prompts the LLM to randomly select a column/row in the table as the answer field and a generate QA pair accordingly; (iii)~sub-table-based, which prompts the LLM to randomly select a sub-table (containing multiple rows and columns) as the answer field and generate a QA pair based on this sub-table. Figure~\ref{fig:qa_gen} presents examples of the QA pairs generated. The   prompt templates are included in Appendix~\ref{app:dataset}.     

\textbf{\ldataset} is constructed from small tables.
We randomly select a subset of small tables (with at most 4,096 tokens) from the \texttt{Spider} dataset and leverage an LLM to expand them into larger tables (see prompt template in Appendix~\ref{app:dataset}). The generate the QA pairs, we reuse the same method as above.

The LLM used for dataset construction is \texttt{GPT-4o mini}~\cite{openai2024gpt4omini}.

Table~\ref{tab:dataset-features} summarizes the datasets (including two existing ones also used in our experiments). 


\section{Experiments}
\subsection{Experimental Settings} \label{sec:datasets} 
\paragraph{Datasets} 
We evaluate our \model\ on four datasets, including SLQA and SEQA that we constructed, plus two 
widely used public datasets: \texttt{WikiTableQuestions} (denoted as \texttt{WTQ}) and \texttt{HybridQA} (denoted as \texttt{\hdataset}). 


\textbf{\wdataset} is a public TableQA dataset derived from Wikipedia tables. To evaluate our model’s capability in handling large free-form tables, we extract all tables with more than 4,096 tokens from the \texttt{WTQ} dataset. We use \textbf{\texttt{$\spadesuit$\wdataset}} to denote the resulting dataset. 
Additionally, we conduct experiments on the entire \texttt{\wdataset} dataset to assess our model’s applicability over tables of different sizes.

\textbf{\hdataset} is a public dataset that integrates free-form tables and texts from Wikipedia. 
To maintain consistency in prompt formatting, we incorporate the texts directly into the corresponding table cells. Then, we extract all the tables with more than 4,096 tokens to construct a large-table subset. 
We use \textbf{\texttt{$\spadesuit$\hdataset}} to denote the resulting dataset. 

\begin{table}[t]
\scriptsize
\centering
\setlength{\tabcolsep}{3pt}
% \resizebox{\columnwidth}{!}{%
\begin{tabular}{lrrrcc}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Dataset}}} &  \multicolumn{3}{c}{\textbf{\#  QA Pairs}}  & \textbf{Avg. \# Tokens} &  \textbf{Avg. \# Tokens} \\
\multicolumn{1}{c}{}  & Training & Validation & Testing & \textbf{per Table} & \textbf{per Answer} \\     \midrule
\texttt{\wdataset}    &   11,321  & 2,831 &  4,344  &  662.6  & 1.7 \\ 
\texttt{$\spadesuit$\wdataset}    &   191  & 33 &  81    &  6,860.8  &  2.9 \\ 
\midrule
\texttt{$\spadesuit$\hdataset} & 48,541 & 2,694 & 2,630 & 9,035.7 & 4.4 \\
\midrule
\texttt{\sdataset} & 1,324 & 239 & 1,110 & 9,786.2 & 2.8\\
\midrule
\texttt{\ldataset} & 226 & 46 & 169 & 18,306.1  & 3.7\\

\bottomrule
\end{tabular}
% }
\caption{Dataset statistics. $\spadesuit$ represents a dataset adapted as described in Section~\ref{sec:datasets}. Among all the datasets, \hdataset\ has the longest answer length, which indicates its difficulty. \ldataset\ has the longest table length mainly because it contains more long numerical cells than the other datasets.} 
\label{tab:dataset-features}
\end{table}



\begin{table*}[ht!]
\centering
\small
\setlength{\tabcolsep}{2pt}
% \resizebox{\columnwidth}{!}{%
\begin{tabular}{llccrcc}
\toprule
\multicolumn{2}{c}{\multirow{2}{*}{\textbf{Model}}} & \multicolumn{4}{c}{\textbf{Accuracy (\%)}} \\
\multicolumn{2}{c}{} & \texttt{$\spadesuit$\wdataset} & \texttt{$\spadesuit$\hdataset} & \texttt{\sdataset} & \texttt{\ldataset}\\
\midrule
{\multirow{1}{*}{NL2SQL-based}} & DIN-SQL &  $41.98$ & $0.27 $& $49.82$ & \underline{$69.82$}\\
\midrule
{\multirow{3}{*}{PLM-based}} & OmniTab-Large   &  $29.63$  & $7.83$ & $20.95$ & $13.02$ \\
& TAPEX-Large     &  $30.86$  & $6.46$ & $17.09$ & $10.65$ \\
& $\clubsuit$ CABINET     &  $43.21$  & $8.72$ & $21.47$ & $13.61$ \\
\midrule
{\multirow{2}{*}{LLM-based}} & $\clubsuit$ DATER   &  $35.80$  & $26.81$ & $44.95$ & $53.25$ \\
& End2End GPT-4o mini & \underline{$48.15$} &  \underline{$30.95$} & \underline{$57.75$} & $64.50$ \\
\midrule
{\multirow{2}{*}{Ours}}& \textbf{\model} &  $\mathbf{59.26}$ & $\mathbf{31.83}$ & $\mathbf{71.17}$ & $\mathbf{76.33}$ \\
& $\Delta$ & $+23.07$ & $+2.84$ & $+23.24$ & $+9.32$\\
\bottomrule
\end{tabular}
% }
\caption{Overall model performance results. %The backbone model of \nmodel\ is GPT-3.5 Turbo.
The best results are in \textbf{boldface}, while the best baseline results are \underline{underlined}; $\Delta$ (\%) denotes the performance gain of our model compared with the best baseline results. $\clubsuit$ denotes table decomposition-based baseline.}
\label{tab:model_results}
\end{table*}







\paragraph{Model Input Preparation} 
Following a widely used approach in the literature~\cite{liu-etal-2024-rethinking, abs-2410-12846, wang2024chainoftable}, tables are linearized into sequences by flattening their structure and incorporating them into prompts for LLMs. 
Table cells are delineated using the symbol `$\vert$', 
while rows are separated by line breaks. 
The questions are directly included in the prompts, 
whereas the answers are utilized only for model evaluation.

For the LLM in the \smodel, similar to NL-to-SQL based methods~\cite{pourreza2023dinsqldecomposedincontextlearning, ZhangLZ24}, it takes the question, the header information of the table, and in-context examples to form the prompt and generate the SQL query. 
For the \dmodel, it takes the generated SQL query and the original table to extract a sub-table. 
For the LLM in \amodel, it takes the question and the sub-table as input, 
forming a compact prompt that reduces both the token consumption and search space for the LLM.

\paragraph{Competitors}
We compare  with models of three categories: (i)~NL-to-SQL based: DIN-SQL~\cite{pourreza2023dinsqldecomposedincontextlearning}; (ii)~Pre-trained or fine-tuned PLM-based: TAPEX-Large~\cite{liu2022tapex}, OmniTab-Large~\cite{jiang-etal-2022-omnitab}, and CABINET~\cite{patnaik2024cabinet} (SOTA); (iii) LLM-based: End-to-End GPT-4o mini~\cite{openai2024gpt4omini} (SOTA) and DATER~\cite{ye2023large}. 

\paragraph{Implementation Details}
For all the LLM-based models including our \model, we use GPT-4o mini as the backbone model. For DIN-SQL, we also utilize GPT-4o mini as its backbone model. All experiments are run with an NVIDIA A100 80 GB GPU on a cloud GPU server. 


\paragraph{Evaluation Metric} We report the performance based on the exact-match \textbf{Accuracy}, following the  evaluator from~\citet{pasupat-liang-2015-compositional}. 

\subsection{Results and Analysis}
\paragraph{Overall Results} Table~\ref{tab:model_results} reports the overall performance results. Our model \model\ consistently outperforms all competitors across all four datasets, improving the accuracy by $23.07\%$, $2.84\%$, $23.24\%$, and $9.32\%$, respectively. This confirms the effectiveness of  \model\ for dealing with complex and large free-form tables.

\begin{figure}[t]
     \centering
     \includegraphics[width = 0.8\linewidth]{latex/token_efficiency.pdf}
     \caption{Comparison of input table token lengths with and without the table decomposer on the test datasets of \texttt{\wdataset}, \texttt{\hdataset}, \texttt{\sdataset}, and \texttt{\ldataset}.}
     \label{fig:token_eff}
\end{figure}

Among all datasets, \hdataset\ poses the greatest challenge due to the complexity of its tabular content. Each cell in it contains rich information, often integrating multiple entities and lengthy descriptive texts. This makes the NL-to-SQL-based model DIN-SQL nearly inapplicable. Despite this challenge, GPT-4o mini demonstrates the most stable performance by leveraging its strong text comprehension and reasoning capabilities, while our model still manages to surpass it by identifying the relevant sub-tables for a subset of the test data. 


We perform chi-squared tests comparing \model\ against the strongest table decomposition-based models (CABINET on \texttt{\wdataset}, DATER on \texttt{\hdataset}, \texttt{\sdataset}, and \texttt{\ldataset}), yielding approximate p-vals of $0.06$, $10^{-4}$, $10^{-35}$, and $10^{-5}$. These confirm that the results of \model\ on \texttt{\hdataset}, \texttt{\sdataset}, and \texttt{\ldataset} are significantly better than the existing table decomposition-based models, while the larger p-val on \wdataset\ is due to the small test set size.


\paragraph{Effectiveness in Table Decomposition} As shown in Figure~\ref{fig:token_eff}, our \emph{Table Decomposer} effectively reduces the input table length by $69.8\%$, $48.5\%$, $84.0\%$, and $91.4\%$ on the four datasets, respectively. This substantial reduction minimizes the prompt length for the \emph{Answer Generator}, enabling the model to focus more effectively on important data while enhancing processing efficiency.

\paragraph{Model Generalizability} To evaluate the generalization capability of \model, we replace the GPT-4o mini backbone with TAPEX. We compare \model\ using TAPEX as the \amodel\ with the original TAPEX model, plus TAPEX combined with an LLM for  table decomposition. The results in Table~\ref{tab:general} shows  that  our \dmodel\ consistently outperforms using an LLM for table decomposition (see prompt in Appendix~\ref{app:exps}). Moreover, our method can also be integrated with and strengthen 
existing PLM-based solutions. 

We also conducted an experiment using the entire \wdataset\ dataset including questions for both small and large tables to further assess our \model's generalizability. The results (in Appendix~\ref{app:exps}) verifies that \model\ enhances the accuracy of its backbone model (GPT-4o mini) by $25.67\%$ on \wdataset\ overall. 

\begin{table}[t]
\centering
\small
\setlength{\tabcolsep}{2pt}
\begin{tabular}{lllll}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Model}}} & \multicolumn{4}{c}{\textbf{Accuracy (\%)}}  \\
  & \texttt{$\spadesuit$\wdataset} & \texttt{$\spadesuit$\hdataset} & \texttt{\sdataset} & \texttt{\ldataset} \\
\midrule
 TAPEX & $\underline{30.86}$ & $\underline{6.46}$  & $17.09$ & $\underline{10.63}$ \\
 \midrule
TAPEX +  & \multirow{2}{*}{$25.93$} & \multirow{2}{*}{$5.89$} & \multirow{2}{*}{$\underline{17.66}$} & \multirow{2}{*}{$8.28$}\\
 LLM Decomposer & & & & \\
 \midrule
 \textbf{\model{} + TAPEX} & $\textbf{39.51}$ & $\textbf{11.60}$ & $\textbf{40.27}$ & $\textbf{42.60}$\\
 $\Delta$ & $+28.03$ & $+79.57$ & $+128.03$ & $+300.75$\\
\bottomrule
\end{tabular}
% }
\caption{Results on using TAPEX as the \amodel\ backbone.}
\label{tab:general}
\end{table}

\begin{table}[t]
\centering
\small
\setlength{\tabcolsep}{2pt}
\begin{tabular}{lllll}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Model}}} & \multicolumn{4}{c}{\textbf{Accuracy (\%)}}  \\
  & \texttt{$\spadesuit$\wdataset} & \texttt{$\spadesuit$\hdataset} & \texttt{\sdataset} & \texttt{\ldataset} \\
\midrule
SG SQL execution & $23.46$ & $0.42$  & $12.61$ & $26.04$ \\
 \midrule
\model-w/o-SG+TD  & {$48.15$} & {$30.95$} & {$57.75$} & {$64.50$}\\
 \midrule
 \model-w/o-SV & {$58.02$} & {$28.40$} & {$69.01$} & {$74.56$}\\
 \midrule
 \textbf{\model} & $\textbf{59.26}$ & $\textbf{31.83}$ & $\textbf{71.17}$ & $\textbf{76.33}$ \\
 
\bottomrule
\end{tabular}
% }
\caption{Ablation study results. SG denotes the \smodel; TD denotes the \dmodel; SV denotes the \vmodel.}
\label{tab:ablation}
\end{table}

\paragraph{Ablation Study} We run an ablation study with three model variants: (1) \textbf{SG SQL execution}: Directly executing SQL queries generated by the \smodel; (2) \textbf{\model-w/o-SG+TD}: \model\ without both \smodel\ and \dmodel, i.e., only using the \amodel; (3)~\textbf{\model-w/o-SV}: \model\ without the \vmodel. 

As shown in Table~\ref{tab:ablation}, directly executing the generated SQL queries results in substantial accuracy drops, because the SQL queries struggle to handle free-form tables. However, the SQL queries still capture structural information, which is valuable for table decomposition. Meanwhile, using only the \amodel\ also leads to accuracy drops, since LLMs struggle with large tables with noisy data. \vmodel\ helps to improve the quality of generated SQL queries and thus improves the overall model accuracy.

\paragraph{Error Analysis} We conduct an error analysis on SQL execution failures. When executing the generated SQL queries, the execution error rates are $64.2\%$, $88.7\%$, $70.6\%$, and $61.5\%$ for \texttt{\wdataset}, \texttt{\hdataset}, \texttt{\sdataset}, and \texttt{\ldataset}, respectively, further confirming that NL2SQL-based solutions struggle on these datasets. 
%results?} \yx{indicate that directly executing the generated SQL queries is unreliable, particularly for datasets with more complex table structures such as \texttt{\hdataset} ($88.7\%$). This suggests that NL-to-SQL approaches struggle with free-form tables.} 
See Appendix~\ref{app:error_analysis} for detailed results. 

For the \vmodel, the acceptance rates (i.e., cases where $S_{\text{RAW}}$ is accepted as $S$) are
$71.6\%$, $54.3\%$, $77.1\%$, and $78.7\%$, respectively. The lower acceptance rate for \texttt{\hdataset} ($54.3\%$) suggests that SQL generation struggles more with the hybrid table-text format, requiring frequent refinement.
%The high acceptance rate for \texttt{\ldataset} ($78.7\%$) implies that SQL queries for synthetically expanded tables are generally more structured, reducing the need for extensive corrections.}


\paragraph{Case Study} We include two TableQA examples that fail existing table decomposition models, while \model\ successfully answers those questions (detailed in Appendix~\ref{app:case_study}).

\section{Conclusions}
We proposed \model, a TableQA model for large free-form tables. \model\ utilizes an LLM (\smodel) to generate an SQL query for each input question, exploiting SQL to capture the table structure information, especially for complex and large tables, while avoiding the limitations in directly executing SQL queries to obtain answers. To further enhance the SQL quality, we introduce the \vmodel. For table decomposition, we adopt a rule-based \dmodel, which achieves higher accuracy and efficiency compared to LLM-based decomposition approaches. We evaluate the performance of \model\ on two public  datasets \texttt{\wdataset} and \texttt{\hdataset}, and two  datasets that we constructed, \texttt{\sdataset} and \texttt{\ldataset}. The results show that \model\ outperforms the best competitors in terms of accuracy by $23.07\%$, $2.84\%$, $23.24\%$ and $9.32\%$, respectively.  

\section*{Limitations}
There are two main limitations: (1)~Although our rule-based \dmodel\ is more accurate and efficient compared with the existing table decomposition methods, there may still be a few corner cases that our rules cannot fully cover. Strategies to automatically learn and  update the rules to adapt to dynamic scenarios would be an interesting future work; (2) Our multi-stage model implies a more complex model design compared with end-to-end models. Further refining the design to achieve an end-to-end pipeline with little sacrifice on accuracy would be another direction to explore. 

\section*{Ethics Statement}

This work involves generating new question-answer pairs and extended tabular content, adhering to ethical considerations and privacy standards.

Our work utilizes the following datasets:
\begin{itemize}[leftmargin = *, noitemsep, topsep=0pt]
    \item \texttt{WikiTableQuestions} and \texttt{HybridQA} are used under the \textit{Creative Commons Attribution 4.0 International License (CC BY 4.0)}.
    \item \texttt{\sdataset} and \texttt{\ldataset} datasets originate from the \texttt{Spider} dataset (\textit{CC BY-SA 4.0} license) and are shared under the same terms to ensure continuity.
\end{itemize}

\paragraph{Privacy and Ethical Considerations}
The dataset creation process does not involve the collection, processing, or storage of personal or any other sensitive information. Additionally:

\begin{itemize}[leftmargin=*, noitemsep, topsep=0pt]
    \item No human subjects were involved, and no user-generated content requiring consent was utilized.
    \item The data sources used are publicly available and licensed for reuse under open-access terms.
    \item The use of LLMs to generate new data strictly follows ethical AI principles, ensuring that no copyrighted or proprietary data is incorporated.
    \item Any potential biases in the generated content have been considered, and efforts have been made to maintain fairness and representativeness in the dataset.
\end{itemize}

By adhering to these principles, we ensure that our work aligns with ethical research practices and respects data usage policies. Additionally, all code used in our work will be made publicly available.


% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

\newpage

\appendix

\section{Prompts for modules in \model}\label{app:prompts}
The prompts used by our \smodel, \vmodel, SQL re-generation, and \amodel\ are shown in Figure~\ref{fig:sql_gen}, \ref{fig:sql_verf}, \ref{fig:sql_refine}, \ref{fig:ans_gen}, respectively. 

\begin{figure}[ht!]
     \centering
     \includegraphics[width = 1\linewidth]{latex/sql_gen.pdf}
     \caption{The prompt used for the \smodel.}
     \label{fig:sql_gen}
\end{figure}

\begin{figure}[ht!]
     \centering
     \includegraphics[width = 1\linewidth]{latex/sql_verf.pdf}
     \caption{The prompt used for the \vmodel.}
     \label{fig:sql_verf}
\end{figure}

\begin{figure}[ht!]
     \centering
     \includegraphics[width = 1\linewidth]{latex/sql_refine.pdf}
     \caption{The prompt used for SQL re-generation.}
     \label{fig:sql_refine}
\end{figure}

\begin{figure}[ht!]
     \centering
     \includegraphics[width = 1\linewidth]{latex/ans_gen.pdf}
     \caption{The prompt used for the \amodel.}
     \label{fig:ans_gen}
\end{figure}


\begin{figure}[t]
     \centering
     \includegraphics[width = 1\linewidth]{latex/qa_analysis.pdf}
     \caption{Error analysis for our QA pair generation process with an LLM: (a) Results for the \sdataset\ dataset; (b) Results for the \ldataset\ dataset. ``Correct'' means that the QA pairs are correct. ``Answer Error'' means that the generated questions are valid but the answers are incorrect. ``Invalid Question'' means that the questions themselves are invalid.}
     \label{fig:qa_analysis}
\end{figure}



\section{Additional Details of the  \dmodel}\label{app:tab_decomp}
The regular expressions used by our \dmodel\ to extract the column names, conditions, and values are  shown in Table~\ref{tab:regex}. The corresponding relationships between parsed SQL queries and Python DataFrame operations used by the \dmodel\ are shown in Table~\ref{tab:sql_df}.


\begin{figure}[ht!]
     \centering
     \includegraphics[width = 1\linewidth]{latex/tab_expand.pdf}
     \caption{The prompt used for table expansion.}
     \label{fig:tab_expand}
\end{figure}

\begin{table*}[t]
\small
\centering 
%\resizebox{\textwidth}{!}{
\begin{tabular}{ll}
\toprule
\textbf{Purpose}  & \textbf{Regular expressions}  \\
\toprule
{\multirow{3}{*}{Column name filters}} &  \verb|1: select\s+(.*?)\s+from|\\
& \begin{lstlisting}[basicstyle=\ttfamily]
2:([\w\s\(\)]+)\s*(=|like|not like|in|not in|>|<|
\end{lstlisting} \\
& \begin{lstlisting}[basicstyle=\ttfamily]
<>|>=|<=|!=|between|is null|is not null)
\end{lstlisting} \\
\toprule
{\multirow{5}{*}{Condition \& value filters}} & \verb|1: where\s+(.*)| \\
& \verb|2: \(\s*select.*?where\s+(.*?)\s*\)| \\
& \begin{lstlisting}[basicstyle=\ttfamily]
3:(\w+)\s*(=|!=|<>|>|<|>=|<=|like|not like|in|not in|
\end{lstlisting} \\
& \begin{lstlisting}[basicstyle=\ttfamily]
between|is null|is not null)\s*('?[\w\s\-%]+?'?)
\end{lstlisting}
 \\ 
 & \begin{lstlisting}[basicstyle=\ttfamily]
4:(?:(where|having)\s+(.*?))?(?:\s*grounp by\s+
\end{lstlisting} \\
& \begin{lstlisting}[basicstyle=\ttfamily]
([\w,\s]+))?(?:\s*limit\s+(\d+))?
\end{lstlisting} \\
\toprule
\end{tabular}
%}
\caption{Regular expressions used to parse SQL queries.}
\label{tab:regex}
\end{table*}

\begin{table*}[t]
\centering 
\small
\begin{tabular}{llll}
\toprule
& \textbf{SQL}  & \textbf{DataFrame} & \textbf{Examples}  \\
\toprule
{\multirow{6}{*}{Comparison}} &  \verb|=| & \verb|==| & \verb|df[df['age'] == 30]| \\
& \verb|!= or <>| & \verb|!=| & \verb|df[df['age'] != 30]| \\
& \verb|>| & \verb|>| & \verb|df[df['salary'] > 5000]| \\
& \verb|<| & \verb|<| & \verb|df[df['salary'] < 5000]| \\
& \verb|>=| & \verb|>=| & \verb|df[df['rating'] >= 4.5]| \\
& \verb|<=| & \verb|<=| & \verb|df[df['rating'] <= 4.5]| \\
\toprule
{\multirow{3}{*}{Logic}} &  \verb|AND| & \verb|&| & \verb|df[(df['age'] > 25) & (df['salary'] > 500]| \\
& \verb|OR| & | & \verb|df[(df['age'] > 25)| | \verb|(df['salary'] > 500)]| \\
& \verb|NOT| & \verb|~| & \verb|df[~(df['active'] == 1)]| \\
\toprule
{\multirow{6}{*}{Range}} &  \verb|BETWEEN AND| & \verb|>= x &| & \verb|df[(df['age'] >= 20) &]| \\
&  & \verb|<= y| & \verb|(df['age'] <= 30)]|\\
& \verb|IN| &  \verb|isin| & \verb|df[df['city'].isin| \\
& & & \verb|(['New York', 'Los Angeles'])]| \\
& \verb|NOT IN| & \verb|~.isin| & \verb|df[~df['city'].isin| \\
& & & \verb|(['New York', 'Los Angeles'])]| \\
\toprule
{\multirow{2}{*}{NULL}} &  \verb|IS NULL| & \verb|isna| & \verb|df[df['phone'].isna()]| \\
& \verb|IS NOT NULL| & \verb|notna| & \verb|df[df['phone'].notna()]|  \\
\toprule
{\multirow{4}{*}{Matching}} &  \verb|LIKE| & \verb|startswith| & \verb|df[df['name'].str.startswith('A')]| \\
&  & \verb|contains| & \verb|df[df['name'].str.contains('A')]|  \\
&  & \verb|endswith| & \verb|df[df['email'].str.endswith('A')]| \\
& \verb|NOT LIKE| & \verb|~.contains| & \verb|df[~df['name'].str.contains('A')]| \\
\toprule
Sorting &  \verb|ORDER BY| & \verb|sort_values| & \verb|df.sort_values('age', ascending=True/False)| \\
\toprule
{\multirow{2}{*}{Grouping}} &  \verb|GROUP BY| & \verb|groupby| & \verb|df.groupby('column')| \\
& \verb|HAVING| & \verb|filter| & \verb|df.groupby().filter(lambda x: len(x) > x)|\\
\toprule
{\multirow{5}{*}{Aggregation}} &  \verb|SUM| & \verb|sum| & \verb|df['salary'].sum()| \\
& \verb|AVG| & \verb|mean| & \verb|df['salary'].mean()|\\
& \verb|COUNT| & \verb|count| & \verb|df['salary'].count()|\\
& \verb|MIN| & \verb|min| & \verb|df['salary'].min()|\\
& \verb|MAX| & \verb|max| & \verb|df['salary'].max()|\\
\toprule
{\multirow{2}{*}{LIMIT}} &  \verb|LIMIT| & \verb|head| & \verb|df.head(10)| \\
& \verb|OFFSET LIMIT| & \verb|iloc| & \verb|df.iloc[5:15]|\\
\toprule
\end{tabular}
\caption{The corresponding relationships between Python
DataFrame operations and SQL queries.}
\label{tab:sql_df}
\end{table*}

\section{Additional Details on Dataset Construction}\label{app:dataset}
We propose both a QA pair generation method and a table expansion method
to construct our LLM-augmented large free-form table datasets. For the QA pair generation, the statistics of our LLM-generated QA pairs are explained in Figure~\ref{fig:qa_analysis}. Particularly, for the ``Answer Error'' cases, where the generated questions are valid while the answers are incorrect, we manually correct the answers according to the tables, to obtain more pairs in the final datasets. For the ``Invalid Question'' cases, we remove the questions and their corresponding answers. 

The prompt used to expand a small table into a larger one is shown in Figure~\ref{fig:tab_expand}, while The prompts used to generate QA pairs are shown in Figure~\ref{fig:qa_prompts}.

\begin{table}[t]
\centering 
\setlength{\tabcolsep}{2pt}
\small
\begin{tabular}{llc}
\toprule
\multicolumn{2}{c}{\textbf{Model}}   & \textbf{Accuracy (\%)}  \\
\toprule
NL-to-SQL-based & DIN-SQL & $53.11$ \\
\midrule
{\multirow{3}{*}{PLM-based}} & TAPAS-Large & $48.80$ \\
 & TAPEX-Large & $57.50$ \\
 & OmniTab-Large & $61.21$ \\
\midrule
{\multirow{3}{*}{LLM-based}} & End2End GPT-3.5 Turbo & $50.64$ \\
& End2End GPT-4o mini & $50.87$ \\
& Binder & $62.85$ \\
\midrule
\textbf{Ours} & \textbf{\model} &  $\textbf{63.93}$ \\
\toprule
\end{tabular}
\caption{Results on the entire \texttt{\wdataset} dataset.}
\label{tab:wtq_exp}
\end{table}

% \begin{figure*}[t]
%      \centering
%      \includegraphics[width = 1\linewidth]{latex/qa_gen_prompts.pdf}
%      \caption{The prompts used for the QA generation: (a) Cell-based approach; (b) Row-based approach; (c) Column-based approach; (d) Sub-table-based approach. \jz{figure not mentioned in text; ``(a)'' font size too large; split the figure into four subfigures}}
%      \label{fig:qa_prompts}
% \end{figure*}

\begin{figure*}[t]
    \centering
    \subfloat[Cell-based approach]{\includegraphics[width=0.48\linewidth]{latex/qa_gen_prompt1.pdf}}
    \hfill
    \subfloat[Row-based approach]{\includegraphics[width=0.48\linewidth]{latex/qa_gen_prompt2.pdf}} \\
    \subfloat[Column-based approach]{\includegraphics[width=0.48\linewidth]{latex/qa_gen_prompt3.pdf}}
    \hfill
    \subfloat[Sub-table-based approach]{\includegraphics[width=0.48\linewidth]{latex/qa_gen_prompt4.pdf}}
    \caption{The prompts used for QA pair generation.}
    \label{fig:qa_prompts}
\end{figure*}

\section{Additional Experiments}\label{app:exps}
We also conduct an additional experiment on the entire \texttt{\wdataset} dataset (including both small and large tables) to showcase our \model's general applicability. The results, presented in Table~\ref{tab:wtq_exp}, show that \model\ outperforms all the competitors using NL-to-SQL based, PLM-based, and LLM-based approaches, indicating \model's general applicability over tables of  different sizes. 

In Table~\ref{tab:ablation} earlier,  we compared the  \dmodel\ with an LLM-based table decomposition method. Here, we show the prompt used for the method in Figure~\ref{fig:llm_decomposer}.

\begin{figure}[t]
     \centering
     \includegraphics[width = 1\linewidth]{latex/sql_errors.pdf}
     \caption{Statistics for the SQL execution errors.}
     \label{fig:sql_errors}
\end{figure}

\begin{figure}[t]
     \centering
     \includegraphics[width=\linewidth]{latex/llm_decomposer.pdf}
     \caption{The prompt for the LLM table decomposer.}
     \label{fig:llm_decomposer}
\end{figure}




\section{Error Analysis}\label{app:error_analysis}
Figure~\ref{fig:sql_errors} presents the statistics of SQL execution errors when directly using the \smodel's outputs to obtain answers. BinderException, which occurs when the query references invalid tables, columns, aliases, or incorrect conditions, is the most common SQL execution error. It is followed by IndexError, which happens at runtime due to out-of-bounds index access. The dominance of \emph{BinderException} suggests schema or condition errors, while \emph{IndexError} indicates issues with index-based operations. 

On different datasets, the distributions of errors vary. This is because different datasets have various table size distribution (see Table~\ref{tab:dataset-features}), reflected in the number of rows and columns, and the complexity of the contents in each cell, e.g., the presence of lengthy descriptive texts in \hdataset.

In addition to SQL execution errors, we analyze errors in the full model, particularly in Table Decomposition and Answer Generation stages. Since Table Decomposition quality directly impacts final accuracy, incorrect selection or filtering of rows/columns can lead to missing or misleading information. Even when decomposition is precise, errors may still arise due to LLM reasoning limitations, such as challenges in numerical operations, multi-hop reasoning, or handling ambiguous queries. This highlights the need for both robust decomposition methods and improved LLM reasoning capabilities to enhance overall TableQA performance.

\section{Case Study}\label{app:case_study}
We show two examples in Figure~\ref{fig:case_study} where our \model\ successfully finds the question answers, while existing table decomposition-based  models (CABINET and DATER) fail. CABINET fails  to accurately identify and assign higher weights to the cells containing the answer when handling large tables, while DATER may remove the answer filed during the table decomposition process.

\begin{figure*}[ht!]
    \centering
    \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{latex/case_study1.pdf}
       \caption{}
        \label{fig:case1}
    \end{subfigure}
    \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{latex/case_study2.pdf}
      \caption{}
        \label{fig:case2}
    \end{subfigure}
    \caption{Two TableQA questions that \model\ answers correctly, while existing table decomposition-based models fail.}
    \label{fig:case_study}
\end{figure*}




% \jz{Double check the last page when you finished. Make sure that the tables/figures do not just randomly float in the page.}
\end{document}
