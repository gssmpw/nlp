\section{Related Work}
Existing TableQA studies fall into three groups: 
(1)~\emph{NL-to-SQL-based}, (2)~\emph{pre-trained language model (PLM)-based}, and (3)~\emph{large language model (LLM)-based}.


\paragraph{NL-to-SQL-based Methods} NL-to-SQL-based methods translate natural language questions into logical forms, i.e., SQL queries, which are then executed to retrieve answers. These methods either train a language model or use in-context learning for the translation task. Seq2SQL~\cite{zhong2017seq2sql} is a representative for the former category. It trains a seq2seq model using reinforcement learning, leveraging query execution results as feedback to refine SQL generation. 

Recent works fall into the latter category. DIN-SQL~\cite{pourreza2023dinsqldecomposedincontextlearning} decomposes the NL-to-SQL task into sub-problems and employs in-context learning to generate the final SQL queries. LEVER~\cite{ni2023lever} generates multiple candidate answers via SQL execution and trains a verifier to select the most accurate one. SynTQA~\cite{ZhangLZ24} also uses a verifier. It chooses answers from an NL-to-SQL TableQA module and an end-to-end TableQA module. These methods 
struggle with free-form tables without predefined schemas, where SQL execution is ineffective.


\paragraph{PLM-based Methods} PLMs-based methods also have two sub-categories.

\underline{Structural adaptations for tabular data.} These methods modify the Transformer~\cite{vaswani2017attention} to better capture tabular structures. TaBERT~\cite{yin-etal-2020-tabert} incorporates vertical self-attention to unify textual and tabular representations based on BERT~\cite{devlin-etal-2019-bert}. TUTA~\cite{wang2021tuta} employs a hierarchical tree-based attention with column and table segment masking to model table representations. TAPAS~\cite{herzig-etal-2020-tapas} extends BERT by adding row, column, and rank embeddings, along with table cell masking.

\underline{Fine-tuned end-to-end models.} TAPEX~\cite{liu2022tapex} pre-trains BART~\cite{lewis-etal-2020-bart} using a large synthetic dataset derived from the \texttt{WikiTableQuestions} dataset. 
OmniTab~\cite{jiang-etal-2022-omnitab} also utilizes BART as its backbone  
while pre-training it on both real and synthetic data, 
including SQL queries from the \texttt{Spider}~\cite{yu-etal-2018-spider} dataset and synthetic natural language sentences generated from SQL queries via its SQL2NL module. 
With smaller model sizes, PLMs exhibit limited effectiveness in grasping tabular semantics compared to the latest LLMs, resulting in sub-optimal TableQA accuracy in TableQA.


\paragraph{LLM-based Methods} LLM-based methods use either prompting or program-aided reasoning.

\underline{Prompting-based methods.} These methods prompt LLMs to generate answers by providing it an input question and the corresponding table. For example, Mix-SC~\cite{liu-etal-2024-rethinking} explores the stochastic nature of LLMs by generating multiple candidate answers for each question. It employs two types of prompting techniques: one directs the LLM to act as a Python agent for script execution, while the other utilizes Chain-of-Thought (CoT) prompting~\cite{wei2022chain} to guide step-by-step reasoning. The final answer is selected using a self-consistency mechanism, which chooses the most frequent answer. 

\underline{Program-aided reasoning methods.} These methods  integrate program generation and execution with LLM prompting.  Binder~\cite{cheng2023binding} first prompts the LLM to generate an initial program based on the input question, then it identifies and refines the challenging segments through iterative LLM engagement. The final refined program is executed by an interpreter to produce TableQA answers. Similarly, TabLaP~\cite{abs-2410-12846} utilizes LLMs for Python script generation, targeting numerical reasoning.  Despite their strong reasoning capabilities, LLM-based TableQA models struggle with complex table structures and are highly sensitive to noise, limiting their effectiveness on large free-form tables.

\begin{figure*}[t]
     \centering
     \includegraphics[width = 1\linewidth]{latex/overview.pdf}
     \caption{Overview of \model. The model contains three stages: SQL generation, table decomposition, and answer generation. (i)~The SQL generation stage constructs an SQL query using the input question and the table header information. It verifies and refines the generated SQL to obtain the refined output. (ii)~The table decomposition stage parses the generated SQL (recognizing column names, conditions, and values) to retrieve sub-tables. The column names, conditions, and values are in orange, blue, and purple, respectively. (iii)~The answer generation stage takes the question and the sub-table to build a  prompt and query an LLM for answer generation.}
     \label{fig:model_overview}
\end{figure*}

\paragraph{Table Decomposition for TableQA} Studies have shown that TableQA models perform significantly better when provided with a sub-table (instead of the full one) containing only question-relevant information~\cite{ye2023large, lee2024learning, patnaik2024cabinet}. Table decomposition aims to extract such sub-tables to enhance TableQA. Existing  methods again fall into two groups.

\underline{Soft decomposition.} These methods attenuates irrelevant information while emphasizing the key content, allowing models to focus on relevant data without explicitly removing extraneous entries. For example, CABINET~\cite{patnaik2024cabinet} proposes a relevance scorer and a relevant cell predictor to assign weights to table cells. %enabling more accurate answers for complex tables. 
These methods rely on model fine-tuning, which limits their model generalizability, and are often constrained by a fixed input token length, restricting scalability.

\underline{Hard decomposition.} These methods removes irrelevant contents, extracting a sub-table as the final input table for the TableQA models. For example, Chain-of-Table~\cite{wang2024chainoftable} extends the CoT framework. It progressively extracts a sub-table at each step using an LLM. DATER~\cite{ye2023large} also utilizes the CoT idea, breaking down both tables and input questions into structured sub-tables and sub-questions. Learn-to-Reduce~\cite{lee2024learning} introduces a column selector and a row selector, which are fine-tuned language models that extract only question-relevant rows and columns to form sub-tables. 
These methods often overlook table structures. They flatten tables and select rows/columns purely by semantic relevance, leading to information loss. 

Our work integrates LLM-driven reasoning with NL-to-SQL techniques, enabling more structured-aware table decomposition while retaining key information for TableQA.