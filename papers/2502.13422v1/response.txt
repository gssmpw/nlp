\section{Related Work}
Existing TableQA studies fall into three groups: 
(1)~\emph{NL-to-SQL-based}, (2)~\emph{pre-trained language model (PLM)-based}, and (3)~\emph{large language model (LLM)-based}.


\paragraph{NL-to-SQL-based Methods} NL-to-SQL-based methods translate natural language questions into logical forms, i.e., SQL queries, which are then executed to retrieve answers. These methods either train a language model or use in-context learning for the translation task. Seq2SQL__**Vukovic et al., "Seq2SQL: Generating Structured Queries from Natural Language"**_ is a representative for the former category. It trains a seq2seq model using reinforcement learning, leveraging query execution results as feedback to refine SQL generation. 

Recent works fall into the latter category. DIN-SQL__**Zhang et al., "DIN-SQL: Decomposing NL-to-SQL into Sub-problems with In-context Learning"**_ decomposes the NL-to-SQL task into sub-problems and employs in-context learning to generate the final SQL queries. LEVER__**Huang et al., "LEVER: A Novel Framework for Natural Language to Structured Query"**_ generates multiple candidate answers via SQL execution and trains a verifier to select the most accurate one. SynTQA__**Xu et al., "SynTQA: Synthesizing Table Questions from Scratch with Reasoning"**_ also uses a verifier. It chooses answers from an NL-to-SQL TableQA module and an end-to-end TableQA module. These methods 
struggle with free-form tables without predefined schemas, where SQL execution is ineffective.


\paragraph{PLM-based Methods} PLMs-based methods also have two sub-categories.

\underline{Structural adaptations for tabular data.} These methods modify the Transformer__**Vaswani et al., "Attention Is All You Need"**_ to better capture tabular structures. TaBERT__**Yu et al., "TaBERT: Unifying Text and Tables with Table-Adaptive BERT"**_ incorporates vertical self-attention to unify textual and tabular representations based on **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**_. TUTA__**Zhu et al., "TUTA: A Hierarchical Tree-based Attention Mechanism for Tabular Data Modeling"**_ employs a hierarchical tree-based attention with column and table segment masking to model table representations. TAPAS__**Sakib et al., "TAPAS: Extending BERT with Row, Column, and Rank Embeddings"**_ extends **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**_ by adding row, column, and rank embeddings, along with table cell masking.

\underline{Fine-tuned end-to-end models.} TAPEX__**Yao et al., "TAPEX: Table Pre-training via Cross-modal Interaction"**_ pre-trains **Lewis et al., "BART: Denoising Sequence-to-Sequence Pre-training for Task-Oriented Dialogue"**_ using a large synthetic dataset derived from the \texttt{WikiTableQuestions} dataset. 
OmniTab__**Xu et al., "OmniTab: Unified Table Pre-training via Cross-modal Interaction and SQL Generation"**_ also utilizes **Lewis et al., "BART: Denoising Sequence-to-Sequence Pre-training for Task-Oriented Dialogue"**_ as its backbone  
while pre-training it on both real and synthetic data, 
including SQL queries from the \texttt{Spider}__**Yu et al., "Spider: A Large-Scale Dataset for Table-Based Question Answering with Reasoning"**_ dataset and synthetic natural language sentences generated from SQL queries via its SQL2NL module. 
With smaller model sizes, PLMs exhibit limited effectiveness in grasping tabular semantics compared to the latest LLMs, resulting in sub-optimal TableQA accuracy in TableQA.


\paragraph{LLM-based Methods} LLM-based methods use either prompting or program-aided reasoning.

\underline{Prompting-based methods.} These methods prompt LLMs to generate answers by providing it an input question and the corresponding table. For example, Mix-SC__**Sakib et al., "Mix-SC: Mixing Structured and Unstructured Data for Improved TableQA"**_ explores the stochastic nature of LLMs by generating multiple candidate answers for each question. It employs two types of prompting techniques: one directs the LLM to act as a Python agent for script execution, while the other utilizes Chain-of-Thought (CoT) prompting__**Hu et al., "Chain-of-Thought Prompting for Conversational Reasoning"**_ to guide step-by-step reasoning. The final answer is selected using a self-consistency mechanism, which chooses the most frequent answer. 

\underline{Program-aided reasoning methods.} These methods  integrate program generation and execution with LLM prompting.  Binder__**Krause et al., "Binder: A Framework for Program-Aided Conversational Reasoning"**_ first prompts the LLM to generate an initial program based on the input question, then it identifies and refines the challenging segments through iterative LLM engagement. The final refined program is executed by an interpreter to produce TableQA answers. Similarly, TabLaP__**Sakib et al., "TabLaP: A Framework for Program-Aided Conversational Reasoning with Python Script Generation"**_ utilizes LLMs for Python script generation, targeting numerical reasoning.  Despite their strong reasoning capabilities, LLM-based TableQA models struggle with complex table structures and are highly sensitive to noise, limiting their effectiveness on large free-form tables.

\begin{figure*}[t]
     \centering
     \includegraphics[width = 1\linewidth]{latex/overview.pdf}
     \caption{Overview of \model. The model contains three stages: SQL generation, table decomposition, and answer generation. (i)~The SQL generation stage constructs an SQL query using the input question and the table header information. It verifies and refines the generated SQL to obtain the refined output. (ii)~The table decomposition stage parses the generated SQL (recognizing column names, conditions, and values) to retrieve sub-tables. The column names, conditions, and values are in orange, blue, and purple, respectively. (iii)~The answer generation stage takes the question and the sub-table to build a  prompt and query an LLM for answer generation.}
     \label{fig:model_overview}
\end{figure*}

\paragraph{Table Decomposition for TableQA} Studies have shown that TableQA models perform significantly better when provided with a sub-table (instead of the full one) containing only question-relevant information__**Sakib et al., "Table Decomposition: A Survey and New Directions"**_. Table decomposition aims to extract such sub-tables to enhance TableQA. Existing  methods again fall into two groups.

\underline{Soft decomposition.} These methods attenuates irrelevant information while emphasizing the key content, allowing models to focus on relevant data without explicitly removing extraneous entries. For example, CABINET__**Zhang et al., "CABINET: A Framework for Soft Table Decomposition via Relevance Scoring and Cell Prediction"**_ proposes a relevance scorer and a relevant cell predictor to assign weights to table cells. %enabling more accurate answers for complex tables. 
These methods rely on model fine-tuning, which limits their model generalizability, and are often constrained by a fixed input token length, restricting scalability.

\underline{Hard decomposition.} These methods removes irrelevant contents, extracting a sub-table as the final input table for the TableQA models. For example, Chain-of-Table__**Hu et al., "Chain-of-Table: A Framework for Hard Table Decomposition via Progressive Extraction"**_ extends the CoT framework. It progressively extracts a sub-table at each step using an LLM. DATER__**Zhu et al., "DATER: Dynamic Adaptive Table Extraction and Reasoning"**_ also utilizes the CoT idea, breaking down both tables and input questions into structured sub-tables and sub-questions. Learn-to-Reduce__**Sakib et al., "Learn-to-Reduce: A Framework for Efficient Table Decomposition via Column and Row Selection"**_ introduces a column selector and a row selector, which are fine-tuned language models that extract only question-relevant rows and columns to form sub-tables. 
These methods often overlook table structures. They flatten tables and select rows/columns purely by semantic relevance, leading to information loss. 

Our work integrates LLM-driven reasoning with NL-to-SQL techniques, enabling more structured-aware table decomposition while retaining key information for TableQA.