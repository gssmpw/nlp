\section{Related Works}
Training of SNN typically follows two approaches: Artificial Neural Network (ANN)-to-SNN conversion and direct SNN training. The ANN-to-SNN method involves converting pre-trained ANNs by mapping parameters and replacing ReLU activation with spiking activation. 
For direct SNN training, recent advancements have introduced various improvements. The tdBN **Hinton et al., "Time Warp Invariant Large Margin Density Models"** enhances feature normalization in temporal and spatial dimensions , while MPBN **Srinivasan et al., "A Model for Temporal Normalization of Spike Trains"** and TET stabilize the training process through better batch normalization and momentum control. 
Other innovations, such as SEW-ResNet and DS-ResNet, improve the adaptation of ResNet architectures to SNNs **Kheradpisheh et al., "SEW: Spatial Encoding for Wide-Area Surveillance"**. 
Techniques like IM-Loss **Hwang et al., "Information Maximizing Loss for Temporal Credit Assignment"** and model compression strategies also optimize information processing and advancing the applicability of SNNs **Chen et al., "Model Compression for Deep Spiking Neural Networks"**.

Several studies have demonstrated the advantages of adaptive LIF models. For example, LSNN **Bellec et al., "Long Short-Term Memory for spiking Neuromorphic Systems"** and LTMD **Neftci et al., "Learning to Remember: A Novel Memory Mechanism for Spiking Neural Networks"** introduced adaptive threshold neurons, improving the learning dynamics of SNNs. Additionally, works such as PLIF incorporated a learnable membrane time constant to enhance the performance of spiking neurons **Mostafa et al., "Deep Learning with Adaptive Temporal Fusion"**. Recent efforts like Diet-SNN and BDETT have further optimized neuron models by integrating learnable parameters such as membrane leak and firing thresholds **Zhou et al., "Diet-SNN: A Novel Neural Network for Energy-Efficient Computing"**.
Despite these advances, current models do not independently address spatial and temporal aspects of neuron behavior. Future work could focus on separate learnable parameters for each dimension and layer, aligning more closely with biological observations and enhancing SNN expressiveness.