@inproceedings{he_deep_2016,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html},
	urldate = {2024-08-19},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2016},
	keywords = {/unread},
	pages = {770--778},
}

@misc{chen_training_2023,
	title = {Training {Full} {Spike} {Neural} {Networks} via {Auxiliary} {Accumulation} {Pathway}},
	url = {http://arxiv.org/abs/2301.11929},
	doi = {10.48550/arXiv.2301.11929},
	abstract = {Due to the binary spike signals making converting the traditional high-power multiply-accumulation (MAC) into a low-power accumulation (AC) available, the brain-inspired Spiking Neural Networks (SNNs) are gaining more and more attention. However, the binary spike propagation of the Full-Spike Neural Networks (FSNN) with limited time steps is prone to significant information loss. To improve performance, several state-of-the-art SNN models trained from scratch inevitably bring many non-spike operations. The non-spike operations cause additional computational consumption and may not be deployed on some neuromorphic hardware where only spike operation is allowed. To train a large-scale FSNN with high performance, this paper proposes a novel Dual-Stream Training (DST) method which adds a detachable Auxiliary Accumulation Pathway (AAP) to the full spiking residual networks. The accumulation in AAP could compensate for the information loss during the forward and backward of full spike propagation, and facilitate the training of the FSNN. In the test phase, the AAP could be removed and only the FSNN remained. This not only keeps the lower energy consumption but also makes our model easy to deploy. Moreover, for some cases where the non-spike operations are available, the APP could also be retained in test inference and improve feature discrimination by introducing a little non-spike consumption. Extensive experiments on ImageNet, DVS Gesture, and CIFAR10-DVS datasets demonstrate the effectiveness of DST.},
	urldate = {2024-08-15},
	publisher = {arXiv},
	author = {Chen, Guangyao and Peng, Peixi and Li, Guoqi and Tian, Yonghong},
	month = jan,
	year = {2023},
	note = {arXiv:2301.11929 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
}

@misc{chen_training_2023-1,
	title = {Training {Full} {Spike} {Neural} {Networks} via {Auxiliary} {Accumulation} {Pathway}},
	url = {http://arxiv.org/abs/2301.11929},
	doi = {10.48550/arXiv.2301.11929},
	abstract = {Due to the binary spike signals making converting the traditional high-power multiply-accumulation (MAC) into a low-power accumulation (AC) available, the brain-inspired Spiking Neural Networks (SNNs) are gaining more and more attention. However, the binary spike propagation of the Full-Spike Neural Networks (FSNN) with limited time steps is prone to significant information loss. To improve performance, several state-of-the-art SNN models trained from scratch inevitably bring many non-spike operations. The non-spike operations cause additional computational consumption and may not be deployed on some neuromorphic hardware where only spike operation is allowed. To train a large-scale FSNN with high performance, this paper proposes a novel Dual-Stream Training (DST) method which adds a detachable Auxiliary Accumulation Pathway (AAP) to the full spiking residual networks. The accumulation in AAP could compensate for the information loss during the forward and backward of full spike propagation, and facilitate the training of the FSNN. In the test phase, the AAP could be removed and only the FSNN remained. This not only keeps the lower energy consumption but also makes our model easy to deploy. Moreover, for some cases where the non-spike operations are available, the APP could also be retained in test inference and improve feature discrimination by introducing a little non-spike consumption. Extensive experiments on ImageNet, DVS Gesture, and CIFAR10-DVS datasets demonstrate the effectiveness of DST.},
	urldate = {2024-08-15},
	publisher = {arXiv},
	author = {Chen, Guangyao and Peng, Peixi and Li, Guoqi and Tian, Yonghong},
	month = jan,
	year = {2023},
	note = {arXiv:2301.11929 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
}

@misc{chen_training_2023-2,
	title = {Training {Full} {Spike} {Neural} {Networks} via {Auxiliary} {Accumulation} {Pathway}},
	url = {http://arxiv.org/abs/2301.11929},
	doi = {10.48550/arXiv.2301.11929},
	abstract = {Due to the binary spike signals making converting the traditional high-power multiply-accumulation (MAC) into a low-power accumulation (AC) available, the brain-inspired Spiking Neural Networks (SNNs) are gaining more and more attention. However, the binary spike propagation of the Full-Spike Neural Networks (FSNN) with limited time steps is prone to significant information loss. To improve performance, several state-of-the-art SNN models trained from scratch inevitably bring many non-spike operations. The non-spike operations cause additional computational consumption and may not be deployed on some neuromorphic hardware where only spike operation is allowed. To train a large-scale FSNN with high performance, this paper proposes a novel Dual-Stream Training (DST) method which adds a detachable Auxiliary Accumulation Pathway (AAP) to the full spiking residual networks. The accumulation in AAP could compensate for the information loss during the forward and backward of full spike propagation, and facilitate the training of the FSNN. In the test phase, the AAP could be removed and only the FSNN remained. This not only keeps the lower energy consumption but also makes our model easy to deploy. Moreover, for some cases where the non-spike operations are available, the APP could also be retained in test inference and improve feature discrimination by introducing a little non-spike consumption. Extensive experiments on ImageNet, DVS Gesture, and CIFAR10-DVS datasets demonstrate the effectiveness of DST.},
	urldate = {2024-08-15},
	publisher = {arXiv},
	author = {Chen, Guangyao and Peng, Peixi and Li, Guoqi and Tian, Yonghong},
	month = jan,
	year = {2023},
	note = {arXiv:2301.11929 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
}

@misc{xu_hybrid_2024,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Hybrid {Spiking} {Vision} {Transformer} for {Event}-{Based} {Object} {Detection}},
	url = {https://papers.ssrn.com/abstract=4790563},
	doi = {10.2139/ssrn.4790563},
	abstract = {Event-based object detection has gradually drawn attention for its high time resolution, high dynamic range, and asynchronous address event representation. Spiking Neural Networks (SNNs) offer distinct advantages, including low energy consumption and rich spatiotemporal dynamics. Therefore, in this study, a novel hybrid spike vision Transformer (HsVT) model is proposed, which combines spatial feature extraction and temporal feature extraction components. The spatial feature extraction module is used to extract the local and global features in the input data, and the temporal feature extraction module is used to capture the time dependencies and long-term patterns in the input sequence. With this combination, HsVT models can better capture spatial and temporal features when processing sequence data, thus improving the modeling ability of complex event-based object detection tasks. Besides, we collect the fall detection datasets and make them public as the benchmark of event-based object detection tasks. The Fall detection dataset with event-based camera provides facial privacy protection and saves memory storage due to the event representation manner. We evaluated the performance of HsVT methods on different model sizes and compared their performance on GEN1 and Fall detection datasets. The experimental results show that the HsVT method has achieved remarkable performance improvement in the event detection task with fewer parameters, and provides an effective solution for the event-based object detection task.},
	language = {en},
	urldate = {2024-08-15},
	author = {Xu, Qi and Deng, Jie and Shen, Jiangrong and Chen, Biwu and Tang, Huajin and Pan, Gang},
	month = apr,
	year = {2024},
	keywords = {/unread, Fall Detection, Object Detection With Event Cameras, spiking neural networks},
}

@misc{shen_temporal_2024,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Temporal {Spiking} {Generative} {Adversarial} {Networks} for {Heading} {Direction} {Decoding}},
	url = {https://papers.ssrn.com/abstract=4757430},
	doi = {10.2139/ssrn.4757430},
	abstract = {The spike-based neuronal responses within the ventral intraparietal area (VIP) exhibit intricate spatial and temporal dynamics in the posterior parietal cortex, presenting decoding challenges such as limited data availability at the biological population level. The practical difficulty in collecting VIP neuronal response data hinders the application of sophisticated decoding models. To address this challenge, we propose a unified spike-based decoding framework leveraging spiking neural networks (SNNs) for both generative and decoding purposes, for their energy efficiency and suitability for neural decoding tasks. We propose the Temporal Spiking Generative Adversarial Networks (T-SGAN), a model based on a spiking transformer, to generate synthetic time-series data reflecting the neuronal response of VIP neurons. T-SGAN incorporates temporal segmentation to reduce the temporal dimension length, while spatial self-attention facilitates the extraction of associated information among VIP neurons. This is followed by recurrent SNNs decoder equipped with an attention mechanism, designed to capture the intricate spatial and temporal dynamics for heading direction decoding. Experimental evaluations conducted on biological datasets from monkeys showcase the effectiveness of the proposed framework. Results indicate that T-SGAN successfully generates realistic synthetic data, leading to a significant improvement of up to 1.75\% in decoding accuracy for recurrent SNNs. Furthermore, the SNN-based decoding framework capitalizes on the low power consumption advantages, offering substantial benefits for neuronal response decoding applications.},
	language = {en},
	urldate = {2024-08-15},
	author = {Shen, Jiangrong and Wang, Kejun and Gao, Wei and Liu, Jian and Xu, Qi and Pan, Gang and Chen, Xiaodong and Tang, Huajin},
	month = mar,
	year = {2024},
	keywords = {/unread, Spiking Generative Adversarial networks；Heading direction decoding, spiking neural networks},
}

@misc{li_spikeformer_2022,
	title = {Spikeformer: {A} {Novel} {Architecture} for {Training} {High}-{Performance} {Low}-{Latency} {Spiking} {Neural} {Network}},
	shorttitle = {Spikeformer},
	url = {http://arxiv.org/abs/2211.10686},
	doi = {10.48550/arXiv.2211.10686},
	abstract = {Spiking neural networks (SNNs) have made great progress on both performance and efficiency over the last few years,but their unique working pattern makes it hard to train a high-performance low-latency SNN.Thus the development of SNNs still lags behind traditional artificial neural networks (ANNs).To compensate this gap,many extraordinary works have been proposed.Nevertheless,these works are mainly based on the same kind of network structure (i.e.CNN) and their performance is worse than their ANN counterparts,which limits the applications of SNNs.To this end,we propose a novel Transformer-based SNN,termed "Spikeformer",which outperforms its ANN counterpart on both static dataset and neuromorphic dataset and may be an alternative architecture to CNN for training high-performance SNNs.First,to deal with the problem of "data hungry" and the unstable training period exhibited in the vanilla model,we design the Convolutional Tokenizer (CT) module,which improves the accuracy of the original model on DVS-Gesture by more than 16\%.Besides,in order to better incorporate the attention mechanism inside Transformer and the spatio-temporal information inherent to SNN,we adopt spatio-temporal attention (STA) instead of spatial-wise or temporal-wise attention.With our proposed method,we achieve competitive or state-of-the-art (SOTA) SNN performance on DVS-CIFAR10,DVS-Gesture,and ImageNet datasets with the least simulation time steps (i.e.low latency).Remarkably,our Spikeformer outperforms other SNNs on ImageNet by a large margin (i.e.more than 5\%) and even outperforms its ANN counterpart by 3.1\% and 2.2\% on DVS-Gesture and ImageNet respectively,indicating that Spikeformer is a promising architecture for training large-scale SNNs and may be more suitable for SNNs compared to CNN.We believe that this work shall keep the development of SNNs in step with ANNs as much as possible.Code will be available.},
	urldate = {2024-08-15},
	publisher = {arXiv},
	author = {Li, Yudong and Lei, Yunlin and Yang, Xu},
	month = nov,
	year = {2022},
	note = {arXiv:2211.10686 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{yu_s2-mlp_2022,
	title = {S2-{MLP}: {Spatial}-{Shift} {MLP} {Architecture} for {Vision}},
	shorttitle = {S2-{MLP}},
	url = {https://openaccess.thecvf.com/content/WACV2022/html/Yu_S2-MLP_Spatial-Shift_MLP_Architecture_for_Vision_WACV_2022_paper.html},
	language = {en},
	urldate = {2024-08-07},
	author = {Yu, Tan and Li, Xu and Cai, Yunfeng and Sun, Mingming and Li, Ping},
	year = {2022},
	keywords = {/unread},
	pages = {297--306},
}

@inproceedings{liu_swin_2021,
	title = {Swin {Transformer}: {Hierarchical} {Vision} {Transformer} {Using} {Shifted} {Windows}},
	shorttitle = {Swin {Transformer}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper},
	language = {en},
	urldate = {2024-08-07},
	author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
	year = {2021},
	keywords = {/unread},
	pages = {10012--10022},
}

@inproceedings{jeon_constructing_2018,
	title = {Constructing {Fast} {Network} through {Deconstruction} of {Convolution}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper/2018/hash/9719a00ed0c5709d80dfef33795dcef3-Abstract.html},
	abstract = {Convolutional neural networks have achieved great success in various vision tasks; however, they incur heavy resource costs. By using deeper and wider networks, network accuracy can be improved rapidly. However, in an environment with limited resources (e.g., mobile applications), heavy networks may not be usable. This study shows that naive convolution can be deconstructed into a shift operation and pointwise convolution. To cope with various convolutions, we propose a new shift operation called active shift layer (ASL) that formulates the amount of shift as a learnable function with shift parameters. This new layer can be optimized end-to-end through backpropagation and it can provide optimal shift values. Finally, we apply this layer to a light and fast network that surpasses existing state-of-the-art networks.},
	urldate = {2024-08-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Jeon, Yunho and Kim, Junmo},
	year = {2018},
	keywords = {/unread},
}

@inproceedings{chen_all_2019,
	title = {All {You} {Need} {Is} a {Few} {Shifts}: {Designing} {Efficient} {Convolutional} {Neural} {Networks} for {Image} {Classification}},
	shorttitle = {All {You} {Need} {Is} a {Few} {Shifts}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Chen_All_You_Need_Is_a_Few_Shifts_Designing_Efficient_Convolutional_CVPR_2019_paper.html},
	urldate = {2024-08-07},
	author = {Chen, Weijie and Xie, Di and Zhang, Yuan and Pu, Shiliang},
	year = {2019},
	keywords = {/unread},
	pages = {7241--7250},
}

@inproceedings{wu_shift_2018,
	title = {Shift: {A} {Zero} {FLOP}, {Zero} {Parameter} {Alternative} to {Spatial} {Convolutions}},
	shorttitle = {Shift},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Shift_A_Zero_CVPR_2018_paper.html},
	urldate = {2024-08-07},
	author = {Wu, Bichen and Wan, Alvin and Yue, Xiangyu and Jin, Peter and Zhao, Sicheng and Golmant, Noah and Gholaminejad, Amir and Gonzalez, Joseph and Keutzer, Kurt},
	year = {2018},
	keywords = {/unread},
	pages = {9127--9135},
}

@misc{chang_learnable_2019,
	title = {Learnable {Gated} {Temporal} {Shift} {Module} for {Deep} {Video} {Inpainting}},
	url = {http://arxiv.org/abs/1907.01131},
	doi = {10.48550/arXiv.1907.01131},
	abstract = {How to efficiently utilize temporal information to recover videos in a consistent way is the main issue for video inpainting problems. Conventional 2D CNNs have achieved good performance on image inpainting but often lead to temporally inconsistent results where frames will flicker when applied to videos (see https://www.youtube.com/watch?v=87Vh1HDBjD0\&list=PLPoVtv-xp\_dL5uckIzz1PKwNjg1yI0I94\&index=1); 3D CNNs can capture temporal information but are computationally intensive and hard to train. In this paper, we present a novel component termed Learnable Gated Temporal Shift Module (LGTSM) for video inpainting models that could effectively tackle arbitrary video masks without additional parameters from 3D convolutions. LGTSM is designed to let 2D convolutions make use of neighboring frames more efficiently, which is crucial for video inpainting. Specifically, in each layer, LGTSM learns to shift some channels to its temporal neighbors so that 2D convolutions could be enhanced to handle temporal information. Meanwhile, a gated convolution is applied to the layer to identify the masked areas that are poisoning for conventional convolutions. On the FaceForensics and Free-form Video Inpainting (FVI) dataset, our model achieves state-of-the-art results with simply 33\% of parameters and inference time.},
	urldate = {2024-08-07},
	publisher = {arXiv},
	author = {Chang, Ya-Liang and Liu, Zhe Yu and Lee, Kuan-Ying and Hsu, Winston},
	month = jul,
	year = {2019},
	note = {arXiv:1907.01131 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{wang_action-net_2021,
	title = {{ACTION}-{Net}: {Multipath} {Excitation} for {Action} {Recognition}},
	shorttitle = {{ACTION}-{Net}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Wang_ACTION-Net_Multipath_Excitation_for_Action_Recognition_CVPR_2021_paper.html},
	language = {en},
	urldate = {2024-08-07},
	author = {Wang, Zhengwei and She, Qi and Smolic, Aljosa},
	year = {2021},
	pages = {13214--13223},
}

@inproceedings{li_simple_2023,
	address = {Vancouver, BC, Canada},
	title = {A {Simple} {Baseline} for {Video} {Restoration} with {Grouped} {Spatial}-{Temporal} {Shift}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350301298},
	url = {https://ieeexplore.ieee.org/document/10203329/},
	doi = {10.1109/CVPR52729.2023.00947},
	abstract = {Video restoration, which aims to restore clear frames from degraded videos, has numerous important applications. The key to video restoration depends on utilizing inter-frame information. However, existing deep learning methods often rely on complicated network architectures, such as optical flow estimation, deformable convolution, and cross-frame self-attention layers, resulting in high computational costs. In this study, we propose a simple yet effective framework for video restoration. Our approach is based on grouped spatial-temporal shift, which is a lightweight and straightforward technique that can implicitly capture inter-frame correspondences for multiframe aggregation. By introducing grouped spatial shift, we attain expansive effective receptive fields. Combined with basic 2D convolution, this simple framework can effectively aggregate inter-frame information. Extensive experiments demonstrate that our framework outperforms the previous state-of-the-art method, while using less than a quarter of its computational cost, on both video deblurring and video denoising tasks. These results indicate the potential for our approach to significantly reduce computational overhead while maintaining high-quality results. Code is avaliable at https://github.com/dasongli1/Shift-Net.},
	language = {en},
	urldate = {2024-08-07},
	booktitle = {2023 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Li, Dasong and Shi, Xiaoyu and Zhang, Yi and Cheung, Ka Chun and See, Simon and Wang, Xiaogang and Qin, Hongwei and Li, Hongsheng},
	month = jun,
	year = {2023},
	keywords = {/unread},
	pages = {9822--9832},
}

@misc{xue_asf-net_2023,
	title = {{ASF}-{Net}: {Robust} {Video} {Deraining} via {Temporal} {Alignment} and {Online} {Adaptive} {Learning}},
	shorttitle = {{ASF}-{Net}},
	url = {http://arxiv.org/abs/2309.00956},
	doi = {10.48550/arXiv.2309.00956},
	abstract = {In recent times, learning-based methods for video deraining have demonstrated commendable results. However, there are two critical challenges that these methods are yet to address: exploiting temporal correlations among adjacent frames and ensuring adaptability to unknown real-world scenarios. To overcome these challenges, we explore video deraining from a paradigm design perspective to learning strategy construction. Specifically, we propose a new computational paradigm, Alignment-Shift-Fusion Network (ASF-Net), which incorporates a temporal shift module. This module is novel to this field and provides deeper exploration of temporal information by facilitating the exchange of channel-level information within the feature space. To fully discharge the model's characterization capability, we further construct a LArge-scale RAiny video dataset (LARA) which also supports the development of this community. On the basis of the newly-constructed dataset, we explore the parameters learning process by developing an innovative re-degraded learning strategy. This strategy bridges the gap between synthetic and real-world scenes, resulting in stronger scene adaptability. Our proposed approach exhibits superior performance in three benchmarks and compelling visual quality in real-world scenarios, underscoring its efficacy. The code is available at https://github.com/vis-opt-group/ASF-Net.},
	urldate = {2024-08-07},
	publisher = {arXiv},
	author = {Xue, Xinwei and He, Jia and Ma, Long and Meng, Xiangyu and Li, Wenlin and Liu, Risheng},
	month = sep,
	year = {2023},
	note = {arXiv:2309.00956 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{anumasa_enhancing_2024,
	title = {Enhancing {Training} of {Spiking} {Neural} {Network} with {Stochastic} {Latency}},
	volume = {38},
	copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/28964},
	doi = {10.1609/aaai.v38i10.28964},
	abstract = {Spiking neural networks (SNNs) have garnered significant attention for their low power consumption when deployed on neuromorphic hardware that operates in orders of magnitude lower power than general-purpose hardware. Direct training methods for SNNs come with an inherent latency for which the SNNs are optimized, and in general, the higher the latency, the better the predictive powers of the models, but at the same time, the higher the energy consumption during training and inference. Furthermore, an SNN model optimized for one particular latency does not necessarily perform well in lower latencies, which becomes relevant in scenarios where it is necessary to switch to a lower latency because of the depletion of onboard energy or other operational requirements. In this work, we propose Stochastic Latency Training (SLT), a direct training method for SNNs that optimizes the model for the given latency but simultaneously offers a minimum reduction of predictive accuracy when shifted to lower inference latencies. We provide heuristics for our approach with partial theoretical justification and experimental evidence showing the state-of-the-art performance of our models on datasets such as CIFAR-10, DVS-CIFAR-10,  CIFAR-100, and DVS-Gesture. Our code is available at https://github.com/srinuvaasu/SLT},
	language = {en},
	number = {10},
	urldate = {2024-05-09},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Anumasa, Srinivas and Mukhoty, Bhaskar and Bojkovic, Velibor and Masi, Giulia De and Xiong, Huan and Gu, Bin},
	month = mar,
	year = {2024},
	note = {Number: 10},
	keywords = {ML: Deep Neural Architectures and Foundation Models},
	pages = {10900--10908},
}

@inproceedings{lin_tsm_2019,
	title = {{TSM}: {Temporal} {Shift} {Module} for {Efficient} {Video} {Understanding}},
	shorttitle = {{TSM}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Lin_TSM_Temporal_Shift_Module_for_Efficient_Video_Understanding_ICCV_2019_paper.html},
	urldate = {2024-07-24},
	author = {Lin, Ji and Gan, Chuang and Han, Song},
	year = {2019},
	pages = {7083--7093},
}

@article{bajorath_artificial_2022,
	title = {Artificial {Intelligence} in {Interdisciplinary} {Life} {Science} and {Drug} {Discovery} {Research}},
	copyright = {© 2022 Jürgen Bajorath},
	url = {https://www.tandfonline.com/doi/abs/10.2144/fsoa-2022-0010},
	doi = {10.2144/fsoa-2022-0010},
	abstract = {Published in Future Science OA (Vol. 8, No. 4, 2022)},
	language = {EN},
	urldate = {2024-07-13},
	journal = {Future Science OA},
	author = {Bajorath, Jürgen},
	month = apr,
	year = {2022},
	note = {Publisher: Taylor \& Francis},
}

@article{graziani_global_2023,
	title = {A global taxonomy of interpretable {AI}: unifying the terminology for the technical and social sciences},
	volume = {56},
	issn = {1573-7462},
	url = {https://doi.org/10.1007/s10462-022-10256-8},
	doi = {10.1007/s10462-022-10256-8},
	abstract = {Since its emergence in the 1960s, Artificial Intelligence (AI) has grown to conquer many technology products and their fields of application. Machine learning, as a major part of the current AI solutions, can learn from the data and through experience to reach high performance on various tasks. This growing success of AI algorithms has led to a need for interpretability to understand opaque models such as deep neural networks. Various requirements have been raised from different domains, together with numerous tools to debug, justify outcomes, and establish the safety, fairness and reliability of the models. This variety of tasks has led to inconsistencies in the terminology with, for instance, terms such as interpretable, explainable and transparent being often used interchangeably in methodology papers. These words, however, convey different meanings and are “weighted" differently across domains, for example in the technical and social sciences. In this paper, we propose an overarching terminology of interpretability of AI systems that can be referred to by the technical developers as much as by the social sciences community to pursue clarity and efficiency in the definition of regulations for ethical and reliable AI development. We show how our taxonomy and definition of interpretable AI differ from the ones in previous research and how they apply with high versatility to several domains and use cases, proposing a—highly needed—standard for the communication among interdisciplinary areas of AI.},
	number = {4},
	journal = {Artificial Intelligence Review},
	author = {Graziani, Mara and Dutkiewicz, Lidia and Calvaresi, Davide and Amorim, José Pereira and Yordanova, Katerina and Vered, Mor and Nair, Rahul and Abreu, Pedro Henriques and Blanke, Tobias and Pulignano, Valeria and Prior, John O. and Lauwaert, Lode and Reijers, Wessel and Depeursinge, Adrien and Andrearczyk, Vincent and Müller, Henning},
	month = apr,
	year = {2023},
	keywords = {/unread},
	pages = {3473--3504},
}

@article{kusters_interdisciplinary_2020,
	title = {Interdisciplinary {Research} in {Artificial} {Intelligence}: {Challenges} and {Opportunities}},
	volume = {3},
	issn = {2624-909X},
	shorttitle = {Interdisciplinary {Research} in {Artificial} {Intelligence}},
	url = {https://www.frontiersin.org/articles/10.3389/fdata.2020.577974},
	doi = {10.3389/fdata.2020.577974},
	abstract = {The use of artificial intelligence (AI) in a variety of research fields is speeding up multiple digital revolutions, from shifting paradigms in healthcare, precision medicine and wearable sensing, to public services and education offered to the masses around the world, to future cities made optimally efficient by autonomous driving. When a revolution happens, the consequences are not obvious straight away, and to date, there is no uniformly adapted framework to guide AI research to ensure a sustainable societal transition. To answer this need, here we analyze three key challenges to interdisciplinary AI research, and deliver three broad conclusions: (i) future development of AI should not only impact other scientific domains but should also take inspiration and benefit from other fields of science, (ii) AI research must be accompanied by decision explainability, dataset bias transparency as well as development of evaluation methodologies and creation of regulatory agencies to ensure responsibility, and (iii) AI education should receive more attention, efforts and innovation from the educational and scientific communities. Our analysis is of interest not only to AI practitioners but also to other researchers and the general public as it offers ways to guide the emerging collaborations and interactions towards the most fruitful outcomes.},
	language = {English},
	urldate = {2024-03-21},
	journal = {Frontiers in Big Data},
	author = {Kusters, Remy and Misevic, Dusan and Berry, Hugues and Cully, Antoine and Le Cunff, Yann and Dandoy, Loic and Díaz-Rodríguez, Natalia and Ficher, Marion and Grizou, Jonathan and Othmani, Alice and Palpanas, Themis and Komorowski, Matthieu and Loiseau, Patrick and Moulin Frier, Clément and Nanini, Santino and Quercia, Daniele and Sebag, Michele and Soulié Fogelman, Françoise and Taleb, Sofiane and Tupikina, Liubov and Sahu, Vaibhav and Vie, Jill-Jênn and Wehbi, Fatima},
	month = nov,
	year = {2020},
	note = {Publisher: Frontiers},
	keywords = {AI, Auditability, Education, Ethics, Interdisciplinary science, Interpretability},
}

@misc{leeman_challenges_2024,
	title = {Challenges in high-throughput inorganic material prediction and autonomous synthesis},
	url = {https://chemrxiv.org/engage/chemrxiv/article-details/65957d349138d231611ad8f7},
	doi = {10.26434/chemrxiv-2024-5p9j4},
	abstract = {Materials discovery lays the foundation for many technological advancements. Predicting and discovering new materials are not simple tasks. We here outline some basic principles of solid-state chemistry, which might help to advance both, and discuss pitfalls and challenges in materials discovery. Using the recent work of Szymanski et al., which reported the autonomous discovery of 43 novel materials, as an example, we discuss problems that can arise in unsupervised materials discovery, and hope that by addressing these, autonomous materials discovery can be brought closer to reality. We discuss all 43 synthetic products and point out four common shortfalls in the analysis. These errors unfortunately lead to the conclusion that no new materials have been discovered in that work. We conclude that there are two important points of improvement that require future work from the community: (i) automated Rietveld analysis of powder x-ray diffraction data is not yet reliable. Future improvement of such, and the development of a reliable artificial intelligence-based tool for Rietveld fitting, would be very helpful, not only to autonomous materials discovery, but also the community in general. (ii) We find that disorder in materials is often neglected in predictions. The predicted compounds investigated herein have all their elemental components located on distinct crystallographic positions, but in reality, elements can share crystallographic sites, resulting in higher symmetry space groups and - very often - known alloys or solid solutions. This error might be related to the difficulty of modeling disorder in a computationally economical way, and needs to be addressed both by computational and experimental material scientists. We find that two-thirds of the claimed successful materials in Szymanski et al are likely to be known, compositionally disordered versions of the predicted, ordered compounds. We highlight important issues in materials discovery, computational chemistry, and autonomous interpretation of x-ray diffraction. We discuss concepts of materials discovery from an experimentalist point of view, which we hope will be helpful for the community to further advance this important new aspect of our field.},
	language = {en},
	urldate = {2024-04-27},
	publisher = {ChemRxiv},
	author = {Leeman, Josh and Liu, Yuhan and Stiles, Joseph and Lee, Scott and Bhatt, Prajna and Schoop, Leslie and Palgrave, Robert},
	month = jan,
	year = {2024},
}

@article{koscher_autonomous_2023,
	title = {Autonomous, multiproperty-driven molecular discovery: {From} predictions to measurements and back},
	volume = {382},
	shorttitle = {Autonomous, multiproperty-driven molecular discovery},
	url = {https://www.science.org/doi/10.1126/science.adi1407},
	doi = {10.1126/science.adi1407},
	abstract = {A closed-loop, autonomous molecular discovery platform driven by integrated machine learning tools was developed to accelerate the design of molecules with desired properties. We demonstrated two case studies on dye-like molecules, targeting absorption wavelength, lipophilicity, and photooxidative stability. In the first study, the platform experimentally realized 294 unreported molecules across three automatic iterations of molecular design-make-test-analyze cycles while exploring the structure-function space of four rarely reported scaffolds. In each iteration, the property prediction models that guided exploration learned the structure-property space of diverse scaffold derivatives, which were realized with multistep syntheses and a variety of reactions. The second study exploited property models trained on the explored chemical space and previously reported molecules to discover nine top-performing molecules within a lightly explored structure-property space.},
	number = {6677},
	urldate = {2024-04-27},
	journal = {Science},
	author = {Koscher, Brent A. and Canty, Richard B. and McDonald, Matthew A. and Greenman, Kevin P. and McGill, Charles J. and Bilodeau, Camille L. and Jin, Wengong and Wu, Haoyang and Vermeire, Florence H. and Jin, Brooke and Hart, Travis and Kulesza, Timothy and Li, Shih-Cheng and Jaakkola, Tommi S. and Barzilay, Regina and Gómez-Bombarelli, Rafael and Green, William H. and Jensen, Klavs F.},
	month = dec,
	year = {2023},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {eadi1407},
}

@article{graziani_global_2023-1,
	title = {A global taxonomy of interpretable {AI}: unifying the terminology for the technical and social sciences},
	volume = {56},
	issn = {1573-7462},
	shorttitle = {A global taxonomy of interpretable {AI}},
	url = {https://doi.org/10.1007/s10462-022-10256-8},
	doi = {10.1007/s10462-022-10256-8},
	abstract = {Since its emergence in the 1960s, Artificial Intelligence (AI) has grown to conquer many technology products and their fields of application. Machine learning, as a major part of the current AI solutions, can learn from the data and through experience to reach high performance on various tasks. This growing success of AI algorithms has led to a need for interpretability to understand opaque models such as deep neural networks. Various requirements have been raised from different domains, together with numerous tools to debug, justify outcomes, and establish the safety, fairness and reliability of the models. This variety of tasks has led to inconsistencies in the terminology with, for instance, terms such as interpretable, explainable and transparent being often used interchangeably in methodology papers. These words, however, convey different meanings and are “weighted" differently across domains, for example in the technical and social sciences. In this paper, we propose an overarching terminology of interpretability of AI systems that can be referred to by the technical developers as much as by the social sciences community to pursue clarity and efficiency in the definition of regulations for ethical and reliable AI development. We show how our taxonomy and definition of interpretable AI differ from the ones in previous research and how they apply with high versatility to several domains and use cases, proposing a—highly needed—standard for the communication among interdisciplinary areas of AI.},
	language = {en},
	number = {4},
	urldate = {2024-06-21},
	journal = {Artificial Intelligence Review},
	author = {Graziani, Mara and Dutkiewicz, Lidia and Calvaresi, Davide and Amorim, José Pereira and Yordanova, Katerina and Vered, Mor and Nair, Rahul and Abreu, Pedro Henriques and Blanke, Tobias and Pulignano, Valeria and Prior, John O. and Lauwaert, Lode and Reijers, Wessel and Depeursinge, Adrien and Andrearczyk, Vincent and Müller, Henning},
	month = apr,
	year = {2023},
	keywords = {Explainable artificial intelligence, Interpretability, Machine learning},
	pages = {3473--3504},
}

@article{huang_security_2023,
	title = {Security and {Privacy} in {Metaverse}: {A} {Comprehensive} {Survey}},
	volume = {6},
	issn = {2097-406X},
	shorttitle = {Security and {Privacy} in {Metaverse}},
	url = {https://ieeexplore.ieee.org/abstract/document/10026513},
	doi = {10.26599/BDMA.2022.9020047},
	abstract = {Metaverse describes a new shape of cyberspace and has become a hot-trending word since 2021. There are many explanations about what Meterverse is and attempts to provide a formal standard or definition of Metaverse. However, these definitions could hardly reach universal acceptance. Rather than providing a formal definition of the Metaverse, we list four must-have characteristics of the Metaverse: socialization, immersive interaction, real world-building, and expandability. These characteristics not only carve the Metaverse into a novel and fantastic digital world, but also make it suffer from all security/privacy risks, such as personal information leakage, eavesdropping, unauthorized access, phishing, data injection, broken authentication, insecure design, and more. This paper first introduces the four characteristics, then the current progress and typical applications of the Metaverse are surveyed and categorized into four economic sectors. Based on the four characteristics and the findings of the current progress, the security and privacy issues in the Metaverse are investigated. We then identify and discuss more potential critical security and privacy issues that can be caused by combining the four characteristics. Lastly, the paper also raises some other concerns regarding society and humanity.},
	number = {2},
	urldate = {2024-06-27},
	journal = {Big Data Mining and Analytics},
	author = {Huang, Yan and Li, Yi Joy and Cai, Zhipeng},
	month = jun,
	year = {2023},
	note = {Conference Name: Big Data Mining and Analytics},
	keywords = {Economics, Humanities, Metaverse, Phishing, Privacy, Security, Shape, cyber infrastructure, cybersecurity, extended reality, privacy protection},
	pages = {234--247},
}

@misc{rane_transforming_2023,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Transforming {Structural} {Engineering} through {ChatGPT} and {Similar} {Generative} {Artificial} {Intelligence}: {Roles}, {Challenges}, and {Opportunities}},
	shorttitle = {Transforming {Structural} {Engineering} through {ChatGPT} and {Similar} {Generative} {Artificial} {Intelligence}},
	url = {https://papers.ssrn.com/abstract=4603242},
	doi = {10.2139/ssrn.4603242},
	abstract = {The integration of ChatGPT and comparable generative artificial intelligence (AI) technologies has ushered in a transformative era in structural engineering, offering innovative solutions while presenting unique challenges and vast opportunities. This research delves into the multifaceted roles, challenges, and opportunities arising from the fusion of AI and structural engineering across various dimensions. In the realm of structural analysis, ChatGPT significantly enhances the precision and speed of complex analyses, enabling engineers to efficiently tackle intricate problems. In structural design, AI-driven algorithms optimize designs using extensive datasets and evolving parameters, leading to more robust and sustainable structures. AI's capacity to comprehend diverse material properties facilitates the selection of ideal materials, ensuring durability and cost-effectiveness. Additionally, in geotechnical engineering and soil mechanics, ChatGPT assists in predicting soil behavior and optimizing foundation designs, mitigating risks associated with ground conditions. Furthermore, ChatGPT streamlines construction processes by offering real-time insights and predictive models, thereby enhancing project timelines and budgets. In structural health monitoring and damage detection, AI algorithms interpret sensor data, enabling early anomaly detection and timely maintenance, thereby enhancing structure longevity. Moreover, in understanding the behavior of structural members, ChatGPT's deep learning capabilities decipher intricate patterns, leading to the development of structures capable of withstanding diverse environmental challenges. However, these advancements come with challenges. Ethical concerns, biases in data, and the necessity for continuous human-AI collaboration pose significant hurdles. Yet, the opportunities are boundless: from sustainable designs to resilient infrastructures. ChatGPT and similar generative AI technologies redefine the landscape of structural engineering, promising a future where innovation and human expertise harmoniously shape the built environment.},
	language = {en},
	urldate = {2024-06-27},
	author = {Rane, Nitin},
	month = sep,
	year = {2023},
	keywords = {Artificial intelligence, ChatGPT, Geotechnical engineering, Material properties, Structural analysis and design, Structural engineering},
}

@article{bengesi_advancements_nodate,
	title = {Advancements in {Generative} {AI}: {A} {Comprehensive} {Review} of {GANs}, {GPT}, {Autoencoders}, {Diffusion} {Model}, and {Transformers}.},
	abstract = {The launch of ChatGPT has garnered global attention, marking a significant milestone in the field of Generative Artificial Intelligence. While Generative AI has been in effect for the past decade, the introduction of ChatGPT has ignited a new wave of research and innovation in the AI domain. This surge in interest has led to the development and release of numerous cutting-edge tools, such as Bard, Stable Diffusion, DALL-E, Make-A-Video, Runway ML, and Jukebox, among others. These tools exhibit remarkable capabilities, encompassing tasks ranging from text generation and music composition, image creation, video production, code generation, and even scientific work. They are built upon various state-of-the-art models, including Stable Diffusion, transformer models like GPT-3 (recent GPT-4), variational autoencoders, and generative adversarial networks. This advancement in Generative AI presents a wealth of exciting opportunities and, simultaneously, unprecedented challenges. Throughout this paper, we have explored these state-of-the-art models, the diverse array of tasks they can accomplish, the challenges they pose, and the promising future of Generative Artificial Intelligence.},
	language = {en},
	author = {Bengesi, Staphord and El-Sayed, Hoda and Sarker, Kamruzzaman and Houkpati, Yao and Oladunni, Timothy},
}

@inproceedings{kumar_comprehensive_2023,
	address = {Cham},
	title = {A {Comprehensive} {Review} of the {Latest} {Advancements} in {Large} {Generative} {AI} {Models}},
	isbn = {978-3-031-45121-8},
	doi = {10.1007/978-3-031-45121-8_9},
	abstract = {There has been an increase in big generative models like ChatGPT and Stable Diffusion over the last two years. These models are capable of a wide range of activities, including providing general answers and producing creative representations. They have a significant impact on a variety of businesses and society since they have the ability to transform established work roles. Generative AI may, for instance, convert text into images using the DALLE-2 model, 3D images using the Dreamfusion model, photos into text using the Flamingo model, and even text into video using the Phenaki model. While ChatGPT can translate text into other texts, the AudioLM model can translate text into audio. Text is converted into code by the Codex model and scientific texts using the Galactica model. Algorithms like AlphaTensor can also be developed through generative AI. This research seeks to present a thorough overview of the most important generative models that have recently been released and their impact on various industries. It also makes an effort to taxonomize these models in order to better comprehend their functions and applications.},
	language = {en},
	booktitle = {Advanced {Communication} and {Intelligent} {Systems}},
	publisher = {Springer Nature Switzerland},
	author = {Kumar, Satyam and Musharaf, Dayima and Musharaf, Seerat and Sagar, Anil Kumar},
	editor = {Shaw, Rabindra Nath and Paprzycki, Marcin and Ghosh, Ankush},
	year = {2023},
	keywords = {AI generative models, Image-to-text, Text-to-3D, Text-to-Science, Text-to-code, Text-to-image, Text-to-text, Text-to-video, Text-to-voice},
	pages = {90--103},
}

@article{gupta_embodied_2021,
	title = {Embodied intelligence via learning and evolution},
	volume = {12},
	copyright = {2021 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-021-25874-z},
	doi = {10.1038/s41467-021-25874-z},
	abstract = {The intertwined processes of learning and evolution in complex environmental niches have resulted in a remarkable diversity of morphological forms. Moreover, many aspects of animal intelligence are deeply embodied in these evolved morphologies. However, the principles governing relations between environmental complexity, evolved morphology, and the learnability of intelligent control, remain elusive, because performing large-scale in silico experiments on evolution and learning is challenging. Here, we introduce Deep Evolutionary Reinforcement Learning (DERL): a computational framework which can evolve diverse agent morphologies to learn challenging locomotion and manipulation tasks in complex environments. Leveraging DERL we demonstrate several relations between environmental complexity, morphological intelligence and the learnability of control. First, environmental complexity fosters the evolution of morphological intelligence as quantified by the ability of a morphology to facilitate the learning of novel tasks. Second, we demonstrate a morphological Baldwin effect i.e., in our simulations evolution rapidly selects morphologies that learn faster, thereby enabling behaviors learned late in the lifetime of early ancestors to be expressed early in the descendants lifetime. Third, we suggest a mechanistic basis for the above relationships through the evolution of morphologies that are more physically stable and energy efficient, and can therefore facilitate learning and control.},
	language = {en},
	number = {1},
	urldate = {2024-04-27},
	journal = {Nature Communications},
	author = {Gupta, Agrim and Savarese, Silvio and Ganguli, Surya and Fei-Fei, Li},
	month = oct,
	year = {2021},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computational science, Computer science},
	pages = {5721},
}

@article{mengaldo_concise_2022,
	title = {A concise guide to modelling the physics of embodied intelligence in soft robotics},
	volume = {4},
	copyright = {2022 Springer Nature Limited},
	issn = {2522-5820},
	url = {https://www.nature.com/articles/s42254-022-00481-z},
	doi = {10.1038/s42254-022-00481-z},
	abstract = {Embodied intelligence (intelligence that requires and leverages a physical body) is a well-known paradigm in soft robotics, but its mathematical description and consequent computational modelling remain elusive, with a need for models that can be used for design and control purposes. We argue that filling this gap will enable full uptake of embodied intelligence in soft robots. We provide a concise guide to the main mathematical modelling approaches, and consequent computational modelling strategies, that can be used to describe soft robots and their physical interactions with the surrounding environment, including fluid and solid media. We aim to convey the challenges and opportunities within the context of modelling the physical interactions underpinning embodied intelligence. We emphasize that interdisciplinary work is required, especially in the context of fully coupled robot–environment interaction modelling. Promoting this dialogue across disciplines is a necessary step to further advance the field of soft robotics.},
	language = {en},
	number = {9},
	urldate = {2024-04-27},
	journal = {Nature Reviews Physics},
	author = {Mengaldo, Gianmarco and Renda, Federico and Brunton, Steven L. and Bächer, Moritz and Calisti, Marcello and Duriez, Christian and Chirikjian, Gregory S. and Laschi, Cecilia},
	month = sep,
	year = {2022},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computational science, Engineering, Physics},
	pages = {595--610},
}

@inproceedings{deng_temporal_2021,
	title = {Temporal {Efficient} {Training} of {Spiking} {Neural} {Network} via {Gradient} {Re}-weighting},
	shorttitle = {{TET}},
	abstract = {Recently, brain-inspired spiking neuron networks (SNNs) have attracted widespread research interest because of their event-driven and energy-efficient characteristics. Still, it is difficult to efficiently train deep SNNs due to the non-differentiability of its activation function, which disables the typically used gradient descent approaches for traditional artificial neural networks (ANNs). Although the adoption of surrogate gradient (SG) formally allows for the back-propagation of losses, the discrete spiking mechanism actually differentiates the loss landscape of SNNs from that of ANNs, failing the surrogate gradient methods to achieve comparable accuracy as for ANNs. In this paper, we first analyze why the current direct training approach with surrogate gradient results in SNNs with poor generalizability. Then we introduce the temporal efficient training (TET) approach to compensate for the loss of momentum in the gradient descent with SG so that the training process can converge into flatter minima with better generalizability. Meanwhile, we demonstrate that TET improves the temporal scalability of SNN and induces a temporal inheritable training for acceleration. Our method consistently outperforms the SOTA on all reported mainstream datasets, including CIFAR-10/100 and ImageNet. Remarkably on DVS-CIFAR10, we obtained 83\${\textbackslash}\%\$ top-1 accuracy, over 10\${\textbackslash}\%\$ improvement compared to existing state of the art. Codes are available at {\textbackslash}url\{https://github.com/Gus-Lab/temporal\_efficient\_training\}.},
	urldate = {2023-11-16},
	booktitle = {Proceedings of {ICLR}},
	author = {Deng, Shikuang and Li, Yuhang and Zhang, Shanghang and Gu, Shi},
	month = oct,
	year = {2021},
	keywords = {\#ref, Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{deng_temporal_2021-1,
	title = {Temporal {Efficient} {Training} of {Spiking} {Neural} {Network} via {Gradient} {Re}-weighting},
	abstract = {Recently, brain-inspired spiking neuron networks (SNNs) have attracted widespread research interest because of their event-driven and energy-efficient characteristics. It is difficult to efficiently train deep SNNs due to the non-differentiability of its activation function, which disables the typically used gradient descent approaches for traditional artificial neural networks (ANNs). Although the adoption of surrogate gradient (SG) formally allows for the back-propagation of losses, the discrete spiking mechanism actually differentiates the loss landscape of SNNs from that of ANNs, failing the surrogate gradient methods to achieve comparable accuracy as for ANNs. In this paper, we first analyze why the current direct training approach with surrogate gradient results in SNNs with poor generalizability. Then we introduce the temporal efficient training (TET) approach to compensate for the loss of momentum in the gradient descent with SG so that the training process can converge into flatter minima with better generalizability. Meanwhile, we demonstrate that TET improves the temporal scalability of SNN and induces a temporal inheritable training for acceleration. Our method consistently outperforms the SOTA on all reported mainstream datasets, including CIFAR-10/100 and ImageNet. Remarkably on DVS-CIFAR10, we obtained 83\% top-1 accuracy, over 10\% improvement compared to existing state of the art.},
	language = {en},
	urldate = {2024-07-03},
	author = {Deng, Shikuang and Li, Yuhang and Zhang, Shanghang and Gu, Shi},
	month = oct,
	year = {2021},
	keywords = {/unread},
}

@misc{noauthor_temporal_nodate,
	title = {Temporal {Efficient} {Training} of {Spiking} {Neural} {Network} via {Gradient} {Re}-weighting {\textbar} {OpenReview}},
	urldate = {2024-07-03},
	keywords = {/unread},
}

@misc{li_towards_2024,
	title = {Towards {Efficient} {Deep} {Spiking} {Neural} {Networks} {Construction} with {Spiking} {Activity} based {Pruning}},
	url = {http://arxiv.org/abs/2406.01072},
	doi = {10.48550/arXiv.2406.01072},
	abstract = {The emergence of deep and large-scale spiking neural networks (SNNs) exhibiting high performance across diverse complex datasets has led to a need for compressing network models due to the presence of a significant number of redundant structural units, aiming to more effectively leverage their low-power consumption and biological interpretability advantages. Currently, most model compression techniques for SNNs are based on unstructured pruning of individual connections, which requires specific hardware support. Hence, we propose a structured pruning approach based on the activity levels of convolutional kernels named Spiking Channel Activity-based (SCA) network pruning framework. Inspired by synaptic plasticity mechanisms, our method dynamically adjusts the network's structure by pruning and regenerating convolutional kernels during training, enhancing the model's adaptation to the current target task. While maintaining model performance, this approach refines the network architecture, ultimately reducing computational load and accelerating the inference process. This indicates that structured dynamic sparse learning methods can better facilitate the application of deep SNNs in low-power and high-efficiency scenarios.},
	urldate = {2024-07-03},
	publisher = {arXiv},
	author = {Li, Yaxin and Xu, Qi and Shen, Jiangrong and Xu, Hongming and Chen, Long and Pan, Gang},
	month = jun,
	year = {2024},
	note = {arXiv:2406.01072 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
}

@article{qiao_reconfigurable_2015,
	title = {A reconfigurable on-line learning spiking neuromorphic processor comprising 256 neurons and {128K} synapses},
	volume = {9},
	issn = {1662-453X},
	url = {https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2015.00141/full},
	doi = {10.3389/fnins.2015.00141},
	abstract = {{\textless}p{\textgreater}Implementing compact, low-power artificial neural processing systems with real-time on-line learning abilities is still an open challenge. In this paper we present a full-custom mixed-signal VLSI device with neuromorphic learning circuits that emulate the biophysics of real spiking neurons and dynamic synapses for exploring the properties of computational neuroscience models and for building brain-inspired computing systems. The proposed architecture allows the on-chip configuration of a wide range of network connectivities, including recurrent and deep networks, with short-term and long-term plasticity. The device comprises 128 K analog synapse and 256 neuron circuits with biologically plausible dynamics and bi-stable spike-based plasticity mechanisms that endow it with on-line learning abilities. In addition to the analog circuits, the device comprises also asynchronous digital logic circuits for setting different synapse and neuron properties as well as different network configurations. This prototype device, fabricated using a 180 nm 1P6M CMOS process, occupies an area of 51.4 mm$^{\textrm{2}}$, and consumes approximately 4 mW for typical experiments, for example involving attractor networks. Here we describe the details of the overall architecture and of the individual circuits and present experimental results that showcase its potential. By supporting a wide range of cortical-like computational modules comprising plasticity mechanisms, this device will enable the realization of intelligent autonomous systems with on-line learning capabilities.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2024-07-02},
	journal = {Frontiers in Neuroscience},
	author = {Qiao, Ning and Mostafa, Hesham and Corradi, Federico and Osswald, Marc and Stefanini, Fabio and Sumislawska, Dora and Indiveri, Giacomo},
	month = apr,
	year = {2015},
	note = {Publisher: Frontiers},
	keywords = {Autonomous Systems, Event-driven, Real-time, Spike-based learning, Winner-take-all (WTA), analog VLSI, asynchronous, attractor network, brain inspired computing, cortical model, event-based, low-power, neuromorphic computing, spike-timing dependent plasticity (STDP)},
}

@inproceedings{kundu_hire-snn_2021,
	title = {{HIRE}-{SNN}: {Harnessing} the {Inherent} {Robustness} of {Energy}-{Efficient} {Deep} {Spiking} {Neural} {Networks} by {Training} {With} {Crafted} {Input} {Noise}},
	shorttitle = {{HIRE}-{SNN}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Kundu_HIRE-SNN_Harnessing_the_Inherent_Robustness_of_Energy-Efficient_Deep_Spiking_Neural_ICCV_2021_paper.html},
	language = {en},
	urldate = {2024-07-02},
	author = {Kundu, Souvik and Pedram, Massoud and Beerel, Peter A.},
	year = {2021},
	pages = {5209--5218},
}

@article{wu_tandem_2023,
	title = {A {Tandem} {Learning} {Rule} for {Effective} {Training} and {Rapid} {Inference} of {Deep} {Spiking} {Neural} {Networks}},
	volume = {34},
	issn = {2162-2388},
	url = {https://ieeexplore.ieee.org/abstract/document/9492305},
	doi = {10.1109/TNNLS.2021.3095724},
	abstract = {Spiking neural networks (SNNs) represent the most prominent biologically inspired computing model for neuromorphic computing (NC) architectures. However, due to the nondifferentiable nature of spiking neuronal functions, the standard error backpropagation algorithm is not directly applicable to SNNs. In this work, we propose a tandem learning framework that consists of an SNN and an artificial neural network (ANN) coupled through weight sharing. The ANN is an auxiliary structure that facilitates the error backpropagation for the training of the SNN at the spike-train level. To this end, we consider the spike count as the discrete neural representation in the SNN and design an ANN neuronal activation function that can effectively approximate the spike count of the coupled SNN. The proposed tandem learning rule demonstrates competitive pattern recognition and regression capabilities on both the conventional frame- and event-based vision datasets, with at least an order of magnitude reduced inference time and total synaptic operations over other state-of-the-art SNN implementations. Therefore, the proposed tandem learning rule offers a novel solution to training efficient, low latency, and high-accuracy deep SNNs with low computing resources.},
	number = {1},
	urldate = {2024-07-02},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Wu, Jibin and Chua, Yansong and Zhang, Malu and Li, Guoqi and Li, Haizhou and Tan, Kay Chen},
	month = jan,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Backpropagation, Biological neural networks, Computer architecture, Deep spiking neural network (SNN), Neurons, Pattern recognition, Task analysis, Training, efficient neuromorphic inference, event-driven vision, neuromorphic computing (NC), object recognition},
	pages = {446--460},
}

@article{hu_spiking_2023,
	title = {Spiking {Deep} {Residual} {Networks}},
	volume = {34},
	issn = {2162-2388},
	url = {https://ieeexplore.ieee.org/abstract/document/9597475},
	doi = {10.1109/TNNLS.2021.3119238},
	abstract = {Spiking neural networks (SNNs) have received significant attention for their biological plausibility. SNNs theoretically have at least the same computational power as traditional artificial neural networks (ANNs). They possess the potential of achieving energy-efficient machine intelligence while keeping comparable performance to ANNs. However, it is still a big challenge to train a very deep SNN. In this brief, we propose an efficient approach to build deep SNNs. Residual network (ResNet) is considered a state-of-the-art and fundamental model among convolutional neural networks (CNNs). We employ the idea of converting a trained ResNet to a network of spiking neurons named spiking ResNet (S-ResNet). We propose a residual conversion model that appropriately scales continuous-valued activations in ANNs to match the firing rates in SNNs and a compensation mechanism to reduce the error caused by discretization. Experimental results demonstrate that our proposed method achieves state-of-the-art performance on CIFAR-10, CIFAR-100, and ImageNet 2012 with low latency. This work is the first time to build an asynchronous SNN deeper than 100 layers, with comparable performance to its original ANN.},
	number = {8},
	urldate = {2024-07-01},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Hu, Yangfan and Tang, Huajin and Pan, Gang},
	month = aug,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Biological neural networks, Convolutional neural networks, Deep neural network, Low latency communication, Membrane potentials, Neurons, Residual neural networks, Task analysis, deep residual network (ResNet), neuromorphic computing, spiking neural networks (SNNs)},
	pages = {5200--5205},
}

@inproceedings{horowitz_11_2014,
	title = {1.1 {Computing}'s energy problem (and what we can do about it)},
	url = {https://ieeexplore.ieee.org/abstract/document/6757323},
	doi = {10.1109/ISSCC.2014.6757323},
	abstract = {Our challenge is clear: The drive for performance and the end of voltage scaling have made power, and not the number of transistors, the principal factor limiting further improvements in computing performance. Continuing to scale compute performance will require the creation and effective use of new specialized compute engines, and will require the participation of application experts to be successful. If we play our cards right, and develop the tools that allow our customers to become part of the design process, we will create a new wave of innovative and efficient computing devices.},
	urldate = {2024-07-01},
	booktitle = {2014 {IEEE} {International} {Solid}-{State} {Circuits} {Conference} {Digest} of {Technical} {Papers} ({ISSCC})},
	author = {Horowitz, Mark},
	month = feb,
	year = {2014},
	note = {ISSN: 2376-8606},
	keywords = {/unread, CMOS integrated circuits, CMOS technology, Energy efficiency, Hardware, Logic gates, Transistors, Voltage control},
	pages = {10--14},
}

@inproceedings{deng_imagenet_2009,
	title = {{ImageNet}: {A} large-scale hierarchical image database},
	shorttitle = {{ImageNet}},
	doi = {10.1109/CVPR.2009.5206848},
	abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500–1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
	urldate = {2024-06-27},
	booktitle = {Proceedings of CVPR},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	month = jun,
	year = {2009},
	note = {ISSN: 1063-6919},
	keywords = {/unread, Explosions, Image databases, Image retrieval, Information retrieval, Internet, Large-scale systems, Multimedia databases, Ontologies, Robustness, Spine},
	pages = {248--255},
}

@article{abbott_building_2016,
	title = {Building functional networks of spiking model neurons},
	volume = {19},
	copyright = {2016 Springer Nature America, Inc.},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/nn.4241},
	doi = {10.1038/nn.4241},
	abstract = {The networks used by computer scientists and by modelers in neuroscience frequently consider unit activities as continuous. Neurons, however, com­municate primarily through discontinuous spiking. This Perspective offers a unifying view of the current methods for transferring our ability to construct functional networks from continuous to more realistic spiking network models.},
	language = {en},
	number = {3},
	urldate = {2024-06-27},
	journal = {Nature Neuroscience},
	author = {Abbott, L. F. and DePasquale, Brian and Memmesheimer, Raoul-Martin},
	month = mar,
	year = {2016},
	note = {Publisher: Nature Publishing Group},
	keywords = {/unread, Network models, Neural encoding},
	pages = {350--355},
}

@article{zhang_rectified_2022,
	title = {Rectified {Linear} {Postsynaptic} {Potential} {Function} for {Backpropagation} in {Deep} {Spiking} {Neural} {Networks}},
	volume = {33},
	issn = {2162-2388},
	url = {https://ieeexplore.ieee.org/abstract/document/9540752},
	doi = {10.1109/TNNLS.2021.3110991},
	abstract = {Spiking neural networks (SNNs) use spatiotemporal spike patterns to represent and transmit information, which are not only biologically realistic but also suitable for ultralow-power event-driven neuromorphic implementation. Just like other deep learning techniques, deep SNNs (DeepSNNs) benefit from the deep architecture. However, the training of DeepSNNs is not straightforward because the well-studied error backpropagation (BP) algorithm is not directly applicable. In this article, we first establish an understanding as to why error BP does not work well in DeepSNNs. We then propose a simple yet efficient rectified linear postsynaptic potential function (ReL-PSP) for spiking neurons and a spike-timing-dependent BP (STDBP) learning algorithm for DeepSNNs where the timing of individual spikes is used to convey information (temporal coding), and learning (BP) is performed based on spike timing in an event-driven manner. We show that DeepSNNs trained with the proposed single spike time-based learning algorithm can achieve the state-of-the-art classification accuracy. Furthermore, by utilizing the trained model parameters obtained from the proposed STDBP learning algorithm, we demonstrate ultralow-power inference operations on a recently proposed neuromorphic inference accelerator. The experimental results also show that the neuromorphic hardware consumes 0.751 mW of the total power consumption and achieves a low latency of 47.71 ms to classify an image from the Modified National Institute of Standards and Technology (MNIST) dataset. Overall, this work investigates the contribution of spike timing dynamics for information encoding, synaptic plasticity, and decision-making, providing a new perspective to the design of future DeepSNNs and neuromorphic hardware.},
	number = {5},
	urldate = {2024-06-27},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Zhang, Malu and Wang, Jiadong and Wu, Jibin and Belatreche, Ammar and Amornpaisannon, Burin and Zhang, Zhixuan and Miriyala, Venkata Pavan Kumar and Qu, Hong and Chua, Yansong and Carlson, Trevor E. and Li, Haizhou},
	month = may,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {/unread, Deep neural networks (DNNs), Hardware, Inference algorithms, Membrane potentials, Neuromorphics, Neurons, Timing, Training, event-driven, neuromorphic hardware, spike-timing-dependent learning, spiking neural networks (SNNs)},
	pages = {1947--1958},
}

@article{ding_accelerating_2021,
	title = {Accelerating {Training} of {Deep} {Spiking} {Neural} {Networks} with {Parameter} {Initialization}},
	url = {https://openreview.net/forum?id=T8BnDXDTcFZ},
	abstract = {Despite that spiking neural networks (SNNs) show strong advantages in information encoding, power consuming, and computational capability, the underdevelopment of supervised learning algorithms is still a hindrance for training SNN. Our consideration is that proper weight initialization is a pivotal issue for efficient SNN training. It greatly influences gradient generating with the method of back-propagation through time at the initial training stage. Focusing on the properties of spiking neurons, we first derive the asymptotic formula of their response curve approximating the actual neuron response distribution. Then, we propose an initialization method obtained from the slant asymptote to overcome gradient vanishing. Finally, experiments with different coding schemes on classification tasks show that our method can effectively improve training speed and the final model accuracy compared with traditional deep learning initialization methods and existing SNN initialization methods. Further validation on different neuron types and training hyper-parameters has shown comparably good versatility and superiority over the other methods. Some suggestions are given to SNN training based on the analyses.},
	language = {en},
	urldate = {2024-06-27},
	author = {Ding, Jianhao and Zhang, Jiyuan and Yu, Zhaofei and Huang, Tiejun},
	month = oct,
	year = {2021},
	keywords = {/unread},
}

@inproceedings{chen_gradual_2022,
	title = {Gradual {Surrogate} {Gradient} {Learning} in {Deep} {Spiking} {Neural} {Networks}},
	doi = {10.1109/ICASSP43922.2022.9746774},
	abstract = {Spiking Neural Network (SNN) is a promising solution for ultra-low-power hardware. Recent SNNs have reached the performance of Deep Neural Networks (DNNs) in dealing with many tasks. However, these methods often suffer from a long simulation time to achieve the accurate spike train information. In addition, these methods are contingent on a well-designed initialization to effectively transmit the gradient information. To address these issues, we propose the Internal Spiking Neuron Model (ISNM), which uses the synaptic current instead of spike trains as the carrier of information. In addition, we design a gradual surrogate gradient learning algorithm to ensure that SNNs effectively back-propagate gradient information in the early stage of training and more accurate gradient information in the later stage of training. The experiments on various network structures on CIFAR-10 and CIFAR-100 datasets show that the proposed method can exceed the performance of previous SNN methods within 5 time steps.},
	urldate = {2024-06-27},
	booktitle = {ICASSP},
	author = {Chen, Yi and Zhang, Silin and Ren, Shiyu and Qu, Hong},
	month = may,
	year = {2022},
	note = {ISSN: 2379-190X},
	keywords = {/unread, Deep learning, Hardware, Learning systems, Neurons, Signal processing, Signal processing algorithms, Spiking Neural Networks, Spiking Neuron Model, Surrogate Gradient, Training},
	pages = {8927--8931},
}

@misc{noauthor_security_nodate,
	title = {Security and {Privacy} in {Metaverse}: {A} {Comprehensive} {Survey} {\textbar} {TUP} {Journals} \& {Magazine} {\textbar} {IEEE} {Xplore}},
	url = {https://ieeexplore.ieee.org/abstract/document/10026513},
	urldate = {2024-06-27},
	keywords = {/unread},
}

@article{su_hf-snn_2021,
	title = {{HF}-{SNN}: {High}-{Frequency} {Spiking} {Neural} {Network}},
	volume = {9},
	issn = {2169-3536},
	shorttitle = {{HF}-{SNN}},
	url = {https://ieeexplore.ieee.org/abstract/document/9383297},
	doi = {10.1109/ACCESS.2021.3068159},
	abstract = {As the third generation of neural networks, spiking neural network (SNN) motivated by neurophysiology enjoys considerable advances due to integrating different information, such as time and space. The frequency-domain provides a powerful capability of modeling and training convolutional neural networks (CNNs). However, SNN with binary input and output will lose much information and slightly inferior to deep neural networks (DNN). We consider how to make the most of information to protect input. Binary input and output are different from DNN, the essence of difference at frequency distribution. In this work, from the insight of frequency distribution, we rethink the SNN training process and give a novel method to transfer SNN to high-frequency spiking neural network (HF-SNN). This approach preserves considerably more information than other optimizing strategies and enables flexibility in the training process. Besides, we evaluate the HF-SNN with extensive experiments on three large datasets: CIFAR-10, CIFAR-100, and ImageNet. Finally, our model supports training a deeper SNN model from scratch and achieves better performance on these datasets than the existing SNN model.},
	urldate = {2024-05-15},
	journal = {IEEE Access},
	author = {Su, Jing and Li, Jing},
	year = {2021},
	note = {Conference Name: IEEE Access},
	keywords = {Backpropagation, Biological neural networks, Biological system modeling, Feature extraction, Frequency, Frequency-domain analysis, Neurons, Spiking neural network, Training, deep learning, high-frequency},
	pages = {51950--51957},
}

@inproceedings{qin_fcanet_2021,
	title = {{FcaNet}: {Frequency} {Channel} {Attention} {Networks}},
	shorttitle = {{FcaNet}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Qin_FcaNet_Frequency_Channel_Attention_Networks_ICCV_2021_paper.html},
	language = {en},
	urldate = {2024-05-15},
	author = {Qin, Zequn and Zhang, Pengyi and Wu, Fei and Li, Xi},
	year = {2021},
	keywords = {Frequency},
	pages = {783--792},
}

@inproceedings{wang_cnnpack_2016,
	title = {{CNNpack}: {Packing} {Convolutional} {Neural} {Networks} in the {Frequency} {Domain}},
	volume = {29},
	shorttitle = {{CNNpack}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2016/hash/3636638817772e42b59d74cff571fbb3-Abstract.html},
	abstract = {Deep convolutional neural networks (CNNs) are successfully used in a number of applications. However, their storage and computational requirements have largely prevented their widespread use on mobile devices. Here we present an effective CNN compression approach in the frequency domain, which focuses not only on smaller weights but on all the weights and their underlying connections. By treating convolutional filters as images, we decompose their representations in the frequency domain as common parts (i.e., cluster centers) shared by other similar filters and their individual private parts (i.e., individual residuals). A large number of low-energy frequency coefficients in both parts can be discarded to produce high compression without significantly compromising accuracy. We relax the computational burden of convolution operations in CNNs by linearly combining the convolution responses of discrete cosine transform (DCT) bases. The compression and speed-up ratios of the proposed algorithm are thoroughly analyzed and evaluated on benchmark image datasets to demonstrate its superiority over state-of-the-art methods.},
	urldate = {2024-05-16},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Wang, Yunhe and Xu, Chang and You, Shan and Tao, Dacheng and Xu, Chao},
	year = {2016},
	keywords = {Frequency},
}

@inproceedings{xu_learning_2020,
	title = {Learning in the {Frequency} {Domain}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Xu_Learning_in_the_Frequency_Domain_CVPR_2020_paper.html},
	urldate = {2024-05-16},
	author = {Xu, Kai and Qin, Minghai and Sun, Fei and Wang, Yuhao and Chen, Yen-Kuang and Ren, Fengbo},
	year = {2020},
	keywords = {Frequency},
	pages = {1740--1749},
}

@inproceedings{chen_compressing_2016,
	address = {San Francisco California USA},
	title = {Compressing {Convolutional} {Neural} {Networks} in the {Frequency} {Domain}},
	isbn = {978-1-4503-4232-2},
	url = {https://dl.acm.org/doi/10.1145/2939672.2939839},
	doi = {10.1145/2939672.2939839},
	language = {en},
	urldate = {2024-05-16},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Chen, Wenlin and Wilson, James and Tyree, Stephen and Weinberger, Kilian Q. and Chen, Yixin},
	month = aug,
	year = {2016},
	keywords = {Frequency},
	pages = {1475--1484},
}

@inproceedings{han_learning_2015,
	title = {Learning both {Weights} and {Connections} for {Efficient} {Neural} {Network}},
	volume = {28},
	url = {https://proceedings.neurips.cc/paper/2015/hash/ae0eb3eed39d2bcef4622b2499a05fe6-Abstract.html},
	abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9×, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG-16 found that the total number of parameters can be reduced by 13×, from 138 million to 10.3 million, again with no loss of accuracy.},
	urldate = {2024-05-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Han, Song and Pool, Jeff and Tran, John and Dally, William},
	year = {2015},
	keywords = {��训练},
}

@article{woo_demonstration_2022,
	title = {Demonstration of integrate-and-fire neuron circuit for spiking neural networks},
	volume = {198},
	issn = {0038-1101},
	url = {https://www.sciencedirect.com/science/article/pii/S0038110122002520},
	doi = {10.1016/j.sse.2022.108481},
	abstract = {An integrate-and-fire (IF) neuron and a voltage level shifter circuit were fabricated and investigated for hardware-based SNN architectures. To verify an IF function of neurons, the fabricated IF neuron circuit consists of a synapse, an integration/reset part, and a fire/trigger circuit. By observing a membrane potential of the IF neuron circuit, an integration and reset operations are successfully implemented. In the fabricated IF neuron circuit, the number of output spikes is 2, 5, 10, and 20 at tpulses of 0.4 μs, 1 μs, 2 μs and 4 μs, respectively. The firing rate of the neuron circuit linearly increases as tpulse increases. These measurement results demonstrate that the fabricated IF neuron circuit implements IF function and reset operation well with linear activation function. For suppression of a gate induced drain leakage (GIDL) at high drain voltages ({\textgreater}6 V), the voltage level shifter consists of MOSFETs with a lightly doped drain (LDD). The generation of high voltage pulses ({\textgreater}6 V) for a synaptic weight update is observed through the voltage level shifter circuit.},
	urldate = {2024-05-19},
	journal = {Solid-State Electronics},
	author = {Woo, Sung Yun and Kang, Won-Mook and Seo, Young-Tak and Lee, Soochang and Kwon, Dongseok and Oh, Seongbin and Bae, Jong-Ho and Lee, Jong-Ho},
	month = dec,
	year = {2022},
	keywords = {Complementary MOSFET, Integrate-and-fire function, Neuron circuit, Spiking neural networks (SNNs), Voltage level shifter, 硬件},
	pages = {108481},
}

@article{zhu_training_2022,
	title = {Training {Spiking} {Neural} {Networks} with {Event}-driven {Backpropagation}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/c4e5f4de1b3cfc838eec6484d0b85378-Abstract-Conference.html},
	language = {en},
	urldate = {2024-05-23},
	journal = {Proceedings of NeurIPS},
	author = {Zhu, Yaoyu and Yu, Zhaofei and Fang, Wei and Xie, Xiaodong and Huang, Tiejun and Masquelier, Timothée},
	month = dec,
	year = {2022},
	pages = {30528--30541},
}

@article{shen_esl-snns_2023,
	title = {{ESL}-{SNNs}: {An} {Evolutionary} {Structure} {Learning} {Strategy} for {Spiking} {Neural} {Networks}},
	volume = {37},
	copyright = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {{ESL}-{SNNs}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/25079},
	doi = {10.1609/aaai.v37i1.25079},
	abstract = {Spiking neural networks (SNNs) have manifested remarkable advantages in power consumption and event-driven property during the inference process. To take full advantage of low power consumption and improve the efficiency of these models further, the pruning methods have been explored to find sparse SNNs without redundancy connections after training. However, parameter redundancy still hinders the efficiency of SNNs during training. In the human brain, the rewiring process of neural networks is highly dynamic, while synaptic connections maintain relatively sparse during brain development. Inspired by this, here we propose an efficient evolutionary structure learning (ESL) framework for SNNs, named ESL-SNNs, to implement the sparse SNN training from scratch. The pruning and regeneration of synaptic connections in SNNs evolve dynamically during learning, yet keep the structural sparsity at a certain level. As a result, the ESL-SNNs can search for optimal sparse connectivity by exploring all possible parameters across time. Our experiments show that the proposed ESL-SNNs framework is able to learn SNNs with sparse structures effectively while reducing the limited accuracy. The ESL-SNNs achieve merely 0.28\% accuracy loss with 10\% connection density on the DVS-Cifar10 dataset. Our work presents a brand-new approach for sparse training of SNNs from scratch with biologically plausible evolutionary mechanisms, closing the gap in the expressibility between sparse training and dense training. Hence, it has great potential for SNN lightweight training and inference with low power consumption and small memory usage.},
	language = {en},
	number = {1},
	urldate = {2024-05-23},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Shen, Jiangrong and Xu, Qi and Liu, Jian K. and Wang, Yueming and Pan, Gang and Tang, Huajin},
	month = jun,
	year = {2023},
	note = {Number: 1},
	keywords = {CMS: Structural Learning and Knowledge Capture},
	pages = {86--93},
}

@inproceedings{bu_rate_2023,
	title = {Rate {Gradient} {Approximation} {Attack} {Threats} {Deep} {Spiking} {Neural} {Networks}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Bu_Rate_Gradient_Approximation_Attack_Threats_Deep_Spiking_Neural_Networks_CVPR_2023_paper.html},
	language = {en},
	urldate = {2024-05-23},
	author = {Bu, Tong and Ding, Jianhao and Hao, Zecheng and Yu, Zhaofei},
	year = {2023},
	pages = {7896--7906},
}

@article{yi_learning_2023,
	title = {Learning rules in spiking neural networks: {A} survey},
	volume = {531},
	issn = {0925-2312},
	shorttitle = {Learning rules in spiking neural networks},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231223001662},
	doi = {10.1016/j.neucom.2023.02.026},
	abstract = {Spiking neural networks (SNNs) are a promising energy-efficient alternative to artificial neural networks (ANNs) due to their rich dynamics, capability to process spatiotemporal patterns, and low-power consumption. The complex intrinsic properties of SNNs give rise to a diversity of their learning rules which are essential to functional SNNs. This paper is aimed at presenting a comprehensive overview of learning rules in SNNs. Firstly, we introduce the basic concepts of SNNs and commonly used neuromorphic datasets. Then, guided by a hierarchical classification of SNN learning rules, we present a comprehensive survey of these rules with discussions on their characteristics, advantages, limitations, and performance on several datasets. Moreover, we review practical applications of SNNs, including event-based vision and audio signal processing. Finally, we conclude this survey with a discussion on challenges and promising future research directions in this area.},
	urldate = {2024-05-23},
	journal = {Neurocomputing},
	author = {Yi, Zexiang and Lian, Jing and Liu, Qidong and Zhu, Hegui and Liang, Dong and Liu, Jizhao},
	month = apr,
	year = {2023},
	keywords = {Image classification, Learning rules, Neuromorphic computing, Pulse-coupled neural networks, Spiking neural networks},
	pages = {163--179},
}

@misc{yao_probabilistic_2023,
	title = {Probabilistic {Modeling}: {Proving} the {Lottery} {Ticket} {Hypothesis} in {Spiking} {Neural} {Network}},
	shorttitle = {Probabilistic {Modeling}},
	url = {http://arxiv.org/abs/2305.12148},
	doi = {10.48550/arXiv.2305.12148},
	abstract = {The Lottery Ticket Hypothesis (LTH) states that a randomly-initialized large neural network contains a small sub-network (i.e., winning tickets) which, when trained in isolation, can achieve comparable performance to the large network. LTH opens up a new path for network pruning. Existing proofs of LTH in Artificial Neural Networks (ANNs) are based on continuous activation functions, such as ReLU, which satisfying the Lipschitz condition. However, these theoretical methods are not applicable in Spiking Neural Networks (SNNs) due to the discontinuous of spiking function. We argue that it is possible to extend the scope of LTH by eliminating Lipschitz condition. Specifically, we propose a novel probabilistic modeling approach for spiking neurons with complicated spatio-temporal dynamics. Then we theoretically and experimentally prove that LTH holds in SNNs. According to our theorem, we conclude that pruning directly in accordance with the weight size in existing SNNs is clearly not optimal. We further design a new criterion for pruning based on our theory, which achieves better pruning results than baseline.},
	urldate = {2024-05-23},
	publisher = {arXiv},
	author = {Yao, Man and Chou, Yuhong and Zhao, Guangshe and Zheng, Xiawu and Tian, Yonghong and Xu, Bo and Li, Guoqi},
	month = may,
	year = {2023},
	note = {arXiv:2305.12148 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{bellec_long_2018,
	title = {Long short-term memory and {Learning}-to-learn in networks of spiking neurons},
	volume = {31},
	shorttitle = {{LSNN}},
	language = {en},
	urldate = {2024-06-06},
	journal = {Proceedings of NeurIPS},
	author = {Bellec, Guillaume and Salaj, Darjan and Subramoney, Anand and Legenstein, Robert and Maass, Wolfgang},
	year = {2018},
}

@inproceedings{feng_multi-level_2022,
	title = {Multi-{Level} {Firing} with {Spiking} {DS}-{ResNet}: {Enabling} {Better} and {Deeper} {Directly}-{Trained} {Spiking} {Neural} {Networks}},
	isbn = {978-1-956792-00-3},
	shorttitle = {{DS}-{ResNet}},
	doi = {10.24963/ijcai.2022/343},
	abstract = {Spiking neural networks (SNNs) are bio-inspired neural networks with asynchronous discrete and sparse characteristics, which have increasingly manifested their superiority in low energy consumption. Recent research is devoted to utilizing spatiotemporal information to directly train SNNs by backpropagation. However, the binary and nondifferentiable properties of spike activities force directly trained SNNs to suffer from serious gradient vanishing and network degradation, which greatly limits the performance of directly trained SNNs and prevents them from going deeper. In this paper, we propose a multi-level ﬁring (MLF) method based on the existing spatio-temporal back propagation (STBP) method, and spiking dormantsuppressed residual network (spiking DS-ResNet). MLF enables more efﬁcient gradient propagation and the incremental expression ability of the neurons. Spiking DS-ResNet can efﬁciently perform identity mapping of discrete spikes, as well as provide a more suitable connection for gradient propagation in deep SNNs. With the proposed method, our model achieves superior performances on a non-neuromorphic dataset and two neuromorphic datasets with much fewer trainable parameters and demonstrates the great ability to combat the gradient vanishing and degradation problem in deep SNNs.},
	language = {en},
	urldate = {2024-06-22},
	booktitle = {Proceedings of IJCAI},
	author = {Feng, Lang and Liu, Qianhui and Tang, Huajin and Ma, De and Pan, Gang},
	month = jul,
	year = {2022},
	pages = {2471--2477},
}

@article{ding_biologically_2022,
	title = {Biologically {Inspired} {Dynamic} {Thresholds} for {Spiking} {Neural} {Networks}},
	volume = {35},
	shorttitle = {{BDETT}},
	language = {en},
	urldate = {2023-11-27},
	journal = {Proceedings of NeurIPS},
	author = {Ding, Jianchuan and Dong, Bo and Heide, Felix and Ding, Yufei and Zhou, Yunduo and Yin, Baocai and Yang, Xin},
	month = dec,
	year = {2022},
	keywords = {硬件},
	pages = {6090--6103},
}

@misc{noauthor_global_nodate,
	title = {A global taxonomy of interpretable {AI}: unifying the terminology for the technical and social sciences {\textbar} {Artificial} {Intelligence} {Review}},
	url = {https://link.springer.com/article/10.1007/s10462-022-10256-8},
	urldate = {2024-06-21},
	keywords = {/unread},
}

@inproceedings{tulsiani_pixeltransformer_2021,
	title = {{PixelTransformer}: {Sample} {Conditioned} {Signal} {Generation}},
	shorttitle = {{PixelTransformer}},
	url = {https://proceedings.mlr.press/v139/tulsiani21a.html},
	abstract = {We propose a generative model that can infer a distribution for the underlying spatial signal conditioned on sparse samples e.g. plausible images given a few observed pixels. In contrast to sequential autoregressive generative models, our model allows conditioning on arbitrary samples and can answer distributional queries for any location. We empirically validate our approach across three image datasets and show that we learn to generate diverse and meaningful samples, with the distribution variance reducing given more observed pixels. We also show that our approach is applicable beyond images and can allow generating other types of spatial outputs e.g. polynomials, 3D shapes, and videos.},
	language = {en},
	urldate = {2024-06-21},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Tulsiani, Shubham and Gupta, Abhinav},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	keywords = {/unread},
	pages = {10455--10464},
}

@article{yu_snnfd_2022,
	title = {{SNNFD}, spiking neural segmentation network in frequency domain using high spatial resolution images for building extraction},
	volume = {112},
	issn = {1569-8432},
	url = {https://www.sciencedirect.com/science/article/pii/S1569843222001285},
	doi = {10.1016/j.jag.2022.102930},
	abstract = {Up-to-date building maps are fundamental to urban development and analysis. However, detecting buildings from images with different spatial resolutions and ground object patterns from various imaging sensors is a challenge. Current models mostly have difficulties in extracting buildings with poor boundaries due to the various building appearances and sizes. Moreover, most published methods are trained and evaluated on subsets from the same dataset whose images are captured from one imaging sensor with similar ground object patterns, making it difficult to evaluate the transferability objectively. To address this issue, a spiking neural network in the frequency domain (SNNFD) is proposed to enhance the model transferability and the feature capability of buildings with different sizes by synthesizing frequency domain and spatial domain learning. Spiking convolution is adopted in the frequency learning module to enhance the model learning ability by mimicking the learning process of human brain. The learned frequency features are concatenated and transformed to the spatial domain, and used to generate building-extraction result images by convolution networks. SNNFD is evaluated on two datasets with different spatial resolutions (0.3–2.5 m) from different imaging sensors (Quickbird, Worldview, IKONOS, ZY-3) of different study areas (worldwide). It is compared with five recently proposed semantic segmentation frameworks (Unet, Segnet, DeepLabv3, BiSeNet, F3-Net), and obtains a minimum of 6.33 \% higher accuracy with a strong transferability in detecting different sizes of building instances. Specifically, the proposed model improves the segmentation performance of small building instances by at least 6.5 \% compared with the five segmentation frameworks through synthesis of spiking convolution in the frequency learning domain in the model construction. Moreover, details of building boundaries are better maintained by SNNFD, offering the possibility of detecting buildings for practical applications.},
	urldate = {2024-05-15},
	journal = {International Journal of Applied Earth Observation and Geoinformation},
	author = {Yu, Bo and Yang, Aqiang and Chen, Fang and Wang, Ning and Wang, Lei},
	month = aug,
	year = {2022},
	keywords = {Building extraction, Frequency, Frequency domain learning, High spatial resolution, Spiking neural network},
	pages = {102930},
}

@inproceedings{gueguen_faster_2018,
	title = {Faster {Neural} {Networks} {Straight} from {JPEG}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper/2018/hash/7af6266cc52234b5aa339b16695f7fc4-Abstract.html},
	abstract = {The simple, elegant approach of training convolutional neural
  networks (CNNs) directly from RGB pixels has enjoyed overwhelming
  empirical success. But can more performance be squeezed out of
  networks by using different input representations?  In this paper we
  propose and explore a simple idea: train CNNs directly on the
  blockwise discrete cosine transform (DCT) coefficients computed and
  available in the middle of the JPEG codec. Intuitively, when
  processing JPEG images using CNNs, it seems unnecessary to
  decompress a blockwise frequency representation to an expanded pixel
  representation, shuffle it from CPU to GPU, and then process it with
  a CNN that will learn something similar to a transform back to
  frequency representation in its first layers. Why not skip both
  steps and feed the frequency domain into the network directly?  In
  this paper we modify {\textbackslash}libjpeg to produce DCT coefficients directly,
  modify a ResNet-50 network to accommodate the differently sized and
  strided input, and evaluate performance on ImageNet. We find
  networks that are both faster and more accurate, as well as networks
  with about the same accuracy but 1.77x faster than ResNet-50.},
	urldate = {2024-05-16},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Gueguen, Lionel and Sergeev, Alex and Kadlec, Ben and Liu, Rosanne and Yosinski, Jason},
	year = {2018},
	keywords = {Frequency},
}

@article{ganguly_spike_2024,
	title = {Spike frequency adaptation: bridging neural models and neuromorphic applications},
	volume = {3},
	copyright = {2024 The Author(s)},
	issn = {2731-3395},
	shorttitle = {Spike frequency adaptation},
	url = {https://www.nature.com/articles/s44172-024-00165-9},
	doi = {10.1038/s44172-024-00165-9},
	abstract = {The human brain’s unparalleled efficiency in executing complex cognitive tasks stems from neurons communicating via short, intermittent bursts or spikes. This has inspired Spiking Neural Networks (SNNs), now incorporating neuron models with spike frequency adaptation (SFA). SFA adjusts these spikes’ frequency based on recent neuronal activity, much like an athlete’s varying sprint speed. SNNs with SFA demonstrate improved computational performance and energy efficiency. This review examines various adaptive neuron models in computational neuroscience, highlighting their relevance in artificial intelligence and hardware integration. It also discusses the challenges and potential of these models in driving the development of energy-efficient neuromorphic systems.},
	language = {en},
	number = {1},
	urldate = {2024-05-13},
	journal = {Communications Engineering},
	author = {Ganguly, Chittotosh and Bezugam, Sai Sukruth and Abs, Elisabeth and Payvand, Melika and Dey, Sounak and Suri, Manan},
	month = feb,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science, Electrical and electronic engineering, Frequency, Neuroscience},
	pages = {1--13},
}

@misc{radcliffe_fourier_2021,
	title = {The {Fourier} transform is a neural network},
	url = {https://sidsite.com/posts/fourier-nets/},
	abstract = {The site of Sid},
	language = {en},
	urldate = {2024-05-13},
	journal = {sidsite},
	author = {Radcliffe, Sidney},
	month = apr,
	year = {2021},
	keywords = {Frequency},
}

@article{nemes_multiple_2011,
	title = {Multiple spatial frequency channels in human visual perceptual memory},
	volume = {51},
	issn = {0042-6989},
	url = {https://www.sciencedirect.com/science/article/pii/S0042698911003233},
	doi = {10.1016/j.visres.2011.09.003},
	abstract = {Current models of short-term visual perceptual memory invoke mechanisms that are closely allied to low-level perceptual discrimination mechanisms. The purpose of this study was to investigate the extent to which human visual perceptual memory for spatial frequency is based upon multiple, spatially tuned channels similar to those found in the earliest stages of visual processing. To this end we measured how performance on a delayed spatial frequency discrimination paradigm was affected by the introduction of interfering or ‘memory masking’ stimuli of variable spatial frequency during the delay period. Masking stimuli were shown to induce shifts in the points of subjective equality (PSE) when their spatial frequencies were within a bandwidth of 1.2 octaves of the reference spatial frequency. When mask spatial frequencies differed by more than this value, there was no change in the PSE from baseline levels. This selective pattern of masking was observed for different spatial frequencies and demonstrates the existence of multiple, spatially tuned mechanisms in visual perceptual memory. Memory masking effects were also found to occur for horizontal separations of up to 6deg between the masking and test stimuli and lacked any orientation selectivity. These findings add further support to the view that low-level sensory processing mechanisms form the basis for the retention of spatial frequency information in perceptual memory. However, the broad range of transfer of memory masking effects across spatial location and other dimensions indicates more long range, long duration interactions between spatial frequency channels that are likely to rely contributions from neural processes located in higher visual areas.},
	number = {23},
	urldate = {2024-05-18},
	journal = {Vision Research},
	author = {Nemes, V. A. and Whitaker, D. and Heron, J. and McKeefry, D. J.},
	month = dec,
	year = {2011},
	keywords = {Frequency, Short term perceptual memory, Spatial frequency},
	pages = {2331--2339},
}

@misc{xu_biologically_2023,
	title = {Biologically inspired structure learning with reverse knowledge distillation for spiking neural networks},
	shorttitle = {{InfLoR}},
	url = {http://arxiv.org/abs/2304.09500},
	doi = {10.48550/arXiv.2304.09500},
	abstract = {Spiking neural networks (SNNs) have superb characteristics in sensory information recognition tasks due to their biological plausibility. However, the performance of some current spiking-based models is limited by their structures which means either fully connected or too-deep structures bring too much redundancy. This redundancy from both connection and neurons is one of the key factors hindering the practical application of SNNs. Although Some pruning methods were proposed to tackle this problem, they normally ignored the fact the neural topology in the human brain could be adjusted dynamically. Inspired by this, this paper proposed an evolutionary-based structure construction method for constructing more reasonable SNNs. By integrating the knowledge distillation and connection pruning method, the synaptic connections in SNNs can be optimized dynamically to reach an optimal state. As a result, the structure of SNNs could not only absorb knowledge from the teacher model but also search for deep but sparse network topology. Experimental results on CIFAR100 and DVS-Gesture show that the proposed structure learning method can get pretty well performance while reducing the connection redundancy. The proposed method explores a novel dynamical way for structure learning from scratch in SNNs which could build a bridge to close the gap between deep learning and bio-inspired neural dynamics.},
	urldate = {2024-05-22},
	publisher = {arXiv},
	author = {Xu, Qi and Li, Yaxin and Fang, Xuanye and Shen, Jiangrong and Liu, Jian K. and Tang, Huajin and Pan, Gang},
	month = apr,
	year = {2023},
	note = {arXiv:2304.09500 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing, ��dl, ��guoyufei},
}

@article{hu_advancing_2024,
	title = {Advancing {Spiking} {Neural} {Networks} {Toward} {Deep} {Residual} {Learning}},
	issn = {2162-2388},
	shorttitle = {{MS}-{ResNet}},
	doi = {10.1109/TNNLS.2024.3355393},
	abstract = {Despite the rapid progress of neuromorphic computing, inadequate capacity and insufficient representation power of spiking neural networks (SNNs) severely restrict their application scope in practice. Residual learning and shortcuts have been evidenced as an important approach for training deep neural networks, but rarely did previous work assess their applicability to the characteristics of spike-based communication and spatiotemporal dynamics. In this paper, we first identify that this negligence leads to impeded information flow and the accompanying degradation problem in previous residual SNNs. To address this issue, we propose a novel SNN-oriented residual architecture termed MS-ResNet, which establishes membrane-based shortcut pathways, and further prove that the gradient norm equality can be achieved in MS-ResNet by introducing block dynamical isometry theory, which ensures the network can be well-behaved in a depth-insensitive way. Thus we are able to significantly extend the depth of directly trained SNNs, e.g., up to 482 layers on CIFAR-10 and 104 layers on ImageNet, without observing any slight degradation problem. To validate the effectiveness of MS-ResNet, experiments on both frame-based and neuromorphic datasets are conducted. MS-ResNet104 achieves a superior result of 76.02\% accuracy on ImageNet, which is the highest to our best knowledge in the domain of directly trained SNNs. Great energy efficiency is also observed, with an average of only one spike per neuron needed to classify an input sample. We believe our powerful and scalable models will provide a strong support for further exploration of SNNs.},
	urldate = {2023-11-13},
	journal = {TNNLS},
	author = {Hu, Yifan and Deng, Lei and Wu, Yujie and Yao, Man and Li, Guoqi},
	year = {2024},
	keywords = {\#ref, MS-ResNet, SNN-oriented residual architecture, biclab, 写 relatedwork, ��energy},
	pages = {1--15},
}

@article{fang_deep_2021,
	title = {Deep {Residual} {Learning} in {Spiking} {Neural} {Networks}},
	volume = {34},
	shorttitle = {{SEW}-{ResNet}},
	abstract = {Deep Spiking Neural Networks (SNNs) present optimization difficulties for gradient-based approaches due to discrete binary activation and complex spatial-temporal dynamics.  Considering the huge success of ResNet in deep learning, it would be natural to train deep SNNs with residual learning. Previous Spiking ResNet mimics the standard residual block in ANNs and simply replaces ReLU activation layers with spiking neurons, which suffers the degradation problem and can hardly implement residual learning. In this paper, we propose the spike-element-wise (SEW) ResNet to realize residual learning in deep SNNs. We prove that the SEW ResNet can easily implement identity mapping and overcome the vanishing/exploding gradient problems of Spiking ResNet. We evaluate our SEW ResNet on ImageNet, DVS Gesture, and CIFAR10-DVS datasets, and show that SEW ResNet outperforms the state-of-the-art directly trained SNNs in both accuracy and time-steps.  Moreover, SEW ResNet can achieve higher performance by simply adding more layers, providing a simple method to train deep SNNs. To our best knowledge, this is the first time that directly training deep SNNs with more than 100 layers becomes possible. Our codes are available at https://github.com/fangwei123456/Spike-Element-Wise-ResNet.},
	urldate = {2023-08-22},
	journal = {Proceedings of NeurIPS},
	author = {Fang, Wei and Yu, Zhaofei and Chen, Yanqi and Huang, Tiejun and Masquelier, Timothée and Tian, Yonghong},
	year = {2021},
	keywords = {\#ref, SNN模型优化, 写 relatedwork},
	pages = {21056--21069},
}

@article{fang_parallel_2023,
	title = {Parallel {Spiking} {Neurons} with {High} {Efficiency} and {Ability} to {Learn} {Long}-term {Dependencies}},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/a834ac3dfdb90da54292c2c932c997cc-Abstract-Conference.html},
	language = {en},
	urldate = {2024-05-19},
	journal = {Proceedings of NeurIPS},
	author = {Fang, Wei and Yu, Zhaofei and Zhou, Zhaokun and Chen, Ding and Chen, Yanqi and Ma, Zhengyu and Masquelier, Timothée and Tian, Yonghong},
	month = dec,
	year = {2023},
	keywords = {加 related},
	pages = {53674--53687},
}

@misc{jiang_klif_2023,
	title = {{KLIF}: {An} optimized spiking neuron unit for tuning surrogate gradient slope and membrane potential},
	shorttitle = {{KLIF}},
	url = {http://arxiv.org/abs/2302.09238},
	doi = {10.48550/arXiv.2302.09238},
	abstract = {Spiking neural networks (SNNs) have attracted much attention due to their ability to process temporal information, low power consumption, and higher biological plausibility. However, it is still challenging to develop efficient and high-performing learning algorithms for SNNs. Methods like artificial neural network (ANN)-to-SNN conversion can transform ANNs to SNNs with slight performance loss, but it needs a long simulation to approximate the rate coding. Directly training SNN by spike-based backpropagation (BP) such as surrogate gradient approximation is more flexible. Yet now, the performance of SNNs is not competitive compared with ANNs. In this paper, we propose a novel k-based leaky Integrate-and-Fire (KLIF) neuron model to improve the learning ability of SNNs. Compared with the popular leaky integrate-and-fire (LIF) model, KLIF adds a learnable scaling factor to dynamically update the slope and width of the surrogate gradient curve during training and incorporates a ReLU activation function that selectively delivers membrane potential to spike firing and resetting. The proposed spiking unit is evaluated on both static MNIST, Fashion-MNIST, CIFAR-10 datasets, as well as neuromorphic N-MNIST, CIFAR10-DVS, and DVS128-Gesture datasets. Experiments indicate that KLIF performs much better than LIF without introducing additional computational cost and achieves state-of-the-art performance on these datasets with few time steps. Also, KLIF is believed to be more biological plausible than LIF. The good performance of KLIF can make it completely replace the role of LIF in SNN for various tasks.},
	urldate = {2024-06-14},
	publisher = {arXiv},
	author = {Jiang, Chunming and Zhang, Yilei},
	month = feb,
	year = {2023},
	note = {arXiv:2302.09238 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing, 加 related},
}

@misc{huang_clif_2024,
	title = {{CLIF}: {Complementary} {Leaky} {Integrate}-and-{Fire} {Neuron} for {Spiking} {Neural} {Networks}},
	shorttitle = {{CLIF}},
	url = {http://arxiv.org/abs/2402.04663},
	doi = {10.48550/arXiv.2402.04663},
	abstract = {Spiking neural networks (SNNs) are promising brain-inspired energy-efficient models. Compared to conventional deep Artificial Neural Networks (ANNs), SNNs exhibit superior efficiency and capability to process temporal information. However, it remains a challenge to train SNNs due to their undifferentiable spiking mechanism. The surrogate gradients method is commonly used to train SNNs, but often comes with an accuracy disadvantage over ANNs counterpart. We link the degraded accuracy to the vanishing of gradient on the temporal dimension through the analytical and experimental study of the training process of Leaky Integrate-and-Fire (LIF) Neuron-based SNNs. Moreover, we propose the Complementary Leaky Integrate-and-Fire (CLIF) Neuron. CLIF creates extra paths to facilitate the backpropagation in computing temporal gradient while keeping binary output. CLIF is hyperparameter-free and features broad applicability. Extensive experiments on a variety of datasets demonstrate CLIF's clear performance advantage over other neuron models. Moreover, the CLIF's performance even slightly surpasses superior ANNs with identical network structure and training conditions.},
	urldate = {2024-06-14},
	publisher = {arXiv},
	author = {Huang, Yulong and Lin, Xiaopeng and Ren, Hongwei and Zhou, Yue and Liu, Zunchang and Fu, Haotian and Pan, Biao and Cheng, Bojun},
	month = feb,
	year = {2024},
	note = {arXiv:2402.04663 [cs]},
	keywords = {Computer Science - Neural and Evolutionary Computing, 加 related},
}

@article{deng_tensor_2024,
  title={Tensor decomposition based attention module for spiking neural networks},
  author={Deng, Haoyu and Zhu, Ruijie and Qiu, Xuerui and Duan, Yule and Zhang, Malu and Deng, Liang-Jian},
  journal={Knowledge-Based Systems},
  volume={295},
  pages={111780},
  year={2024},
  publisher={Elsevier}
}

@article{zhu_tcja-snn_2024,
	title = {{TCJA}-{SNN}: {Temporal}-{Channel} {Joint} {Attention} for {Spiking} {Neural} {Networks}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {2162-237X, 2162-2388},
	shorttitle = {{TCJA}-{SNN}},
	url = {https://ieeexplore.ieee.org/document/10496285/},
	doi = {10.1109/TNNLS.2024.3377717},
	urldate = {2024-06-14},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Zhu, Rui-Jie and Zhang, Malu and Zhao, Qihang and Deng, Haoyu and Duan, Yule and Deng, Liang-Jian},
	year = {2024},
	keywords = {加 related, ��Attention},
	pages = {1--14},
}

@article{guo_ternary_2024,
	title = {Ternary {Spike}: {Learning} {Ternary} {Spikes} for {Spiking} {Neural} {Networks}},
	volume = {38},
	copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {Ternary {Spike}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/29114},
	doi = {10.1609/aaai.v38i11.29114},
	abstract = {The Spiking Neural Network (SNN), as one of the biologically inspired neural network infrastructures, has drawn increasing attention recently. It adopts binary spike activations to transmit information, thus the multiplications of activations and weights can be substituted by additions, which brings high energy efficiency.  However, in the paper, we theoretically and experimentally prove that the binary spike activation map cannot carry enough information, thus causing information loss and resulting in accuracy decreasing. To handle the problem, we propose a ternary spike neuron to transmit information. The ternary spike neuron can also enjoy the event-driven and multiplication-free operation advantages of the binary spike neuron but will boost the information capacity. Furthermore, we also embed a trainable factor in the ternary spike neuron to learn the suitable spike amplitude, thus our SNN will adopt different spike amplitudes along layers, which can better suit the phenomenon that the membrane potential distributions are different along layers. To retain the efficiency of the vanilla ternary spike, the trainable ternary spike SNN will be converted to a standard one again via a re-parameterization technique in the inference. Extensive experiments with several popular network structures over static and dynamic datasets show that the ternary spike can consistently outperform state-of-the-art methods. Our code is open-sourced at https://github.com/yfguo91/Ternary-Spike.},
	language = {en},
	number = {11},
	urldate = {2024-05-09},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Guo, Yufei and Chen, Yuanpei and Liu, Xiaode and Peng, Weihang and Zhang, Yuhan and Huang, Xuhui and Ma, Zhe},
	month = mar,
	year = {2024},
	note = {Number: 11},
	keywords = {CMS: Other Foundations of Cognitive Modeling \& Systems, 写 relatedwork, 加 related, ��guoyufei, ��learnable},
	pages = {12244--12252},
}

@article{zhang_tc-lif_2024,
	title = {{TC}-{LIF}: {A} {Two}-{Compartment} {Spiking} {Neuron} {Model} for {Long}-{Term} {Sequential} {Modelling}},
	volume = {38},
	copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {{TC}-{LIF}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/29625},
	doi = {10.1609/aaai.v38i15.29625},
	abstract = {The identification of sensory cues associated with potential opportunities and dangers is frequently complicated by unrelated events that separate useful cues by long delays. As a result, it remains a challenging task for state-of-the-art spiking neural networks (SNNs) to establish long-term temporal dependency between distant cues. To address this challenge, we propose a novel biologically inspired Two-Compartment Leaky Integrate-and-Fire spiking neuron model, dubbed TC-LIF. The proposed model incorporates carefully designed somatic and dendritic compartments that are tailored to facilitate learning long-term temporal dependencies. Furthermore, the theoretical analysis is provided to validate the effectiveness of TC-LIF in propagating error gradients over an extended temporal duration. Our experimental results, on a diverse range of temporal classification tasks, demonstrate superior temporal classification capability, rapid training convergence, and high energy efficiency of the proposed TC-LIF model. Therefore, this work opens up a myriad of opportunities for solving challenging temporal processing tasks on emerging neuromorphic computing systems. Our code is publicly available at https://github.com/ZhangShimin1/TC-LIF.},
	language = {en},
	number = {15},
	urldate = {2024-05-09},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Zhang, Shimin and Yang, Qu and Ma, Chenxiang and Wu, Jibin and Li, Haizhou and Tan, Kay Chen},
	month = mar,
	year = {2024},
	note = {Number: 15},
	keywords = {CMS: Other Foundations of Cognitive Modeling \& Systems, 加 related},
	pages = {16838--16847},
}

@misc{zimmer_technical_2019,
	title = {Technical report: supervised training of convolutional spiking neural networks with {PyTorch}},
	shorttitle = {Technical report},
	url = {http://arxiv.org/abs/1911.10124},
	doi = {10.48550/arXiv.1911.10124},
	abstract = {Recently, it has been shown that spiking neural networks (SNNs) can be trained efficiently, in a supervised manner, using backpropagation through time. Indeed, the most commonly used spiking neuron model, the leaky integrate-and-fire neuron, obeys a differential equation which can be approximated using discrete time steps, leading to a recurrent relation for the potential. The firing threshold causes optimization issues, but they can be overcome using a surrogate gradient. Here, we extend previous approaches in two ways. Firstly, we show that the approach can be used to train convolutional layers. Convolutions can be done in space, time (which simulates conduction delays), or both. Secondly, we include fast horizontal connections {\textbackslash}`a la Den{\textbackslash}`eve: when a neuron N fires, we subtract to the potentials of all the neurons with the same receptive the dot product between their weight vectors and the one of neuron N. As Den{\textbackslash}`eve et al. showed, this is useful to represent a dynamic multidimensional analog signal in a population of spiking neurons. Here we demonstrate that, in addition, such connections also allow implementing a multidimensional send-on-delta coding scheme. We validate our approach on one speech classification benchmarks: the Google speech command dataset. We managed to reach nearly state-of-the-art accuracy (94\%) while maintaining low firing rates (about 5Hz). Our code is based on PyTorch and is available in open source at http://github.com/romainzimmer/s2net},
	urldate = {2024-06-06},
	publisher = {arXiv},
	author = {Zimmer, Romain and Pellegrini, Thomas and Singh, Srisht Fateh and Masquelier, Timothée},
	month = nov,
	year = {2019},
	note = {arXiv:1911.10124 [cs, stat]},
	keywords = {/unread, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{duan_survey_2022,
	title = {A {Survey} of {Embodied} {AI}: {From} {Simulators} to {Research} {Tasks}},
	volume = {6},
	issn = {2471-285X},
	shorttitle = {A {Survey} of {Embodied} {AI}},
	url = {https://ieeexplore.ieee.org/abstract/document/9687596},
	doi = {10.1109/TETCI.2022.3141105},
	abstract = {There has been an emerging paradigm shift from the era of “internet AI” to “embodied AI,” where AI algorithms and agents no longer learn from datasets of images, videos or text curated primarily from the internet. Instead, they learn through interactions with their environments from an egocentric perception similar to humans. Consequently, there has been substantial growth in the demand for embodied AI simulators to support various embodied AI research tasks. This growing interest in embodied AI is beneficial to the greater pursuit of Artificial General Intelligence (AGI), but there has not been a contemporary and comprehensive survey of this field. This paper aims to provide an encyclopedic survey for the field of embodied AI, from its simulators to its research. By evaluating nine current embodied AI simulators with our proposed seven features, this paper aims to understand the simulators in their provision for use in embodied AI research and their limitations. Lastly, this paper surveys the three main research tasks in embodied AI – visual exploration, visual navigation and embodied question answering (QA), covering the state-of-the-art approaches, evaluation metrics and datasets. Finally, with the new insights revealed through surveying the field, the paper will provide suggestions for simulator-for-task selections and recommendations for the future directions of the field.},
	number = {2},
	urldate = {2024-06-03},
	journal = {IEEE Transactions on Emerging Topics in Computational Intelligence},
	author = {Duan, Jiafei and Yu, Samson and Tan, Hui Li and Zhu, Hongyuan and Tan, Cheston},
	month = apr,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Emerging Topics in Computational Intelligence},
	keywords = {3D simulators, Artificial intelligence, Embodied AI, Navigation, Physics, Solid modeling, Task analysis, Three-dimensional displays, Visualization, computer vision},
	pages = {230--244},
}

@article{wu_training_2021,
	title = {Training {Spiking} {Neural} {Networks} with {Accumulated} {Spiking} {Flow}},
	volume = {35},
	copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17236},
	doi = {10.1609/aaai.v35i12.17236},
	abstract = {The fast development of neuromorphic hardwares promotes Spiking Neural Networks (SNNs) to a thrilling research avenue. Current SNNs, though much efficient, are less effective compared with leading Artificial Neural Networks (ANNs) especially in supervised learning tasks. Recent efforts further demonstrate the potential of SNNs in supervised learning by introducing  approximated  backpropagation (BP) methods. To deal with the non-differentiable spike function in SNNs, these BP methods utilize information from the spatio-temporal domain to adjust the model parameters. With the increasing of time window and network size, the computational complexity of spatio-temporal backpropagation augments dramatically. In this paper, we propose a new backpropagation method for SNNs based on the accumulated spiking flow (ASF), i.e. ASF-BP. In the proposed ASF-BP method, updating parameters does not rely on the spike train of spiking neurons but leverage accumulated inputs and outputs of spiking neurons over the time window, which reduces the BP complexity significantly. We further present an adaptive linear estimation model to approach the dynamic characteristics of spiking neurons statistically. Experimental results demonstrate that with our proposed ASF-BP method, light-weight convolutional SNNs achieve superior performances compared with other spike-based BP methods on both non-neuromorphic (MNIST, CIFAR10) and neuromorphic (CIFAR10-DVS) datasets. The code is available at https://github.com/neural-lab/ASF-BP.},
	language = {en},
	number = {12},
	urldate = {2024-05-27},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Wu, Hao and Zhang, Yueyi and Weng, Wenming and Zhang, Yongting and Xiong, Zhiwei and Zha, Zheng-Jun and Sun, Xiaoyan and Wu, Feng},
	month = may,
	year = {2021},
	note = {Number: 12},
	keywords = {(Deep) Neural Network Algorithms},
	pages = {10320--10328},
}

@inproceedings{xu_adaptive_2023,
	address = {New York, NY, USA},
	series = {{MM} '23},
	title = {Adaptive {Decoupled} {Pose} {Knowledge} {Distillation}},
	isbn = {9798400701085},
	url = {https://doi.org/10.1145/3581783.3611818},
	doi = {10.1145/3581783.3611818},
	abstract = {Existing state-of-the-art human pose estimation approaches require heavy computational resources for accurate prediction. One promising technique to obtain an accurate yet lightweight pose estimator is Knowledge Distillation (KD), which distills the pose knowledge from a powerful teacher model to a lightweight student model. However, existing human pose KD methods focus more on designing paired student and teacher network architectures, yet ignore the mechanism of pose knowledge distillation. In this work, we reformulate the human pose KD to a coarse to fine process and decouple the classical KD loss into three terms: Binary Keypoint vs. Non-Keypoint Distillation (BiKD), Keypoint Area Distillation (KAD) and Non-keypoint Area Distillation (NAD). Observing the decoupled formulation, we point out an important limitation of the classical pose KD, i.e. the bias between different loss terms limits the performance gain of the student network. To address the biased knowledge distillation problem, we present a novel KD method named Adaptive Decoupled Pose knowledge Distillation (ADPD), enabling BiKD, KAD and NAD to play their roles more effectively and flexibly. Extensive experiments on two standard human pose datasets, MPII and MS COCO, demonstrate that our proposed method outperforms previous KD methods and is generalizable to different teacher-student pairs. The code will be available at https://github.com/SuperJay1996/ADPD.},
	urldate = {2024-04-17},
	booktitle = {Proceedings of the 31st {ACM} {International} {Conference} on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Xu, Jie and Zhang, Shanshan and Yang, Jian},
	month = oct,
	year = {2023},
	keywords = {adaptive weighting, human pose estimation, knowledge distillation, neural networks, ��dl},
	pages = {4401--4409},
}

@article{guo_joint_2023,
	title = {Joint {A}-{SNN}: {Joint} training of artificial and spiking neural networks via self-{Distillation} and weight factorization},
	volume = {142},
	issn = {0031-3203},
	shorttitle = {Joint {A}-{SNN}},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320323003400},
	doi = {10.1016/j.patcog.2023.109639},
	abstract = {Emerged as a biology-inspired method, Spiking Neural Networks (SNNs) mimic the spiking nature of brain neurons and have received lots of research attention. SNNs deal with binary spikes as their activation and therefore derive extreme energy efficiency on hardware. However, it also leads to an intrinsic obstacle that training SNNs from scratch requires a re-definition of the firing function for computing gradient. Artificial Neural Networks (ANNs), however, are fully differentiable to be trained with gradient descent. In this paper, we propose a joint training framework of ANN and SNN, in which the ANN can guide the SNN’s optimization. This joint framework contains two parts: First, the knowledge inside ANN is distilled to SNN by using multiple branches from the networks. Second, we restrict the parameters of ANN and SNN, where they share partial parameters and learn different singular weights. Extensive experiments over several widely used network structures show that our method consistently outperforms many other state-of-the-art training methods. For example, on the CIFAR100 classification task, the spiking ResNet-18 model trained by our method can reach to 77.39\% top-1 accuracy with only 4 time steps.},
	urldate = {2023-11-21},
	journal = {Pattern Recognition},
	author = {Guo, Yufei and Peng, Weihang and Chen, Yuanpei and Zhang, Liwen and Liu, Xiaode and Huang, Xuhui and Ma, Zhe},
	month = oct,
	year = {2023},
	keywords = {Artificial neural networks, Knowledge distillation, SNN, Weight factorization, ��dl, ��guoyufei},
	pages = {109639},
}

@article{bodden_spiking_2023,
	title = {Spiking {CenterNet}: {A} {Distillation}-boosted {Spiking} {Neural} {Network} for {Object} {Detection}},
	shorttitle = {Spiking {CenterNet}},
	url = {https://openreview.net/forum?id=NQUXBoGiDU},
	abstract = {In the era of AI at the edge, self-driving cars, and climate change, the need for energy-efficient, small, embedded AI is growing. Spiking Neural Networks (SNNs) are a promising approach to address this challenge, with their event-driven information flow and sparse activations. We propose Spiking CenterNet for object detection on event data. It combines an SNN CenterNet adaptation with an efficient M2U-Net-based decoder. Our model significantly outperforms comparable previous work on Prophesee's challenging GEN1 Automotive Detection Dataset while using less than half the energy. Distilling the knowledge of a non-spiking teacher into our SNN further increases performance. To the best of our knowledge, our work is the first approach that takes advantage of knowledge distillation in the field of spiking object detection.},
	language = {en},
	urldate = {2024-05-22},
	author = {Bodden, Lennard and Ha, Bach and Schwaiger, Franziska and Kreuzberg, Lars and Behnke, Sven},
	month = oct,
	year = {2023},
	keywords = {��dl},
}

@inproceedings{yuan_revisiting_2020,
	title = {Revisiting {Knowledge} {Distillation} via {Label} {Smoothing} {Regularization}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Yuan_Revisiting_Knowledge_Distillation_via_Label_Smoothing_Regularization_CVPR_2020_paper.html},
	urldate = {2024-05-23},
	author = {Yuan, Li and Tay, Francis EH and Li, Guilin and Wang, Tao and Feng, Jiashi},
	year = {2020},
	keywords = {��dl},
	pages = {3903--3911},
}

@inproceedings{takuya_training_2021,
	address = {Tokyo, Japan},
	title = {Training {Low}-{Latency} {Spiking} {Neural} {Network} through {Knowledge} {Distillation}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-66541-503-3},
	url = {https://ieeexplore.ieee.org/document/9410323/},
	doi = {10.1109/COOLCHIPS52128.2021.9410323},
	urldate = {2024-05-23},
	booktitle = {2021 {IEEE} {Symposium} in {Low}-{Power} and {High}-{Speed} {Chips} ({COOL} {CHIPS})},
	publisher = {IEEE},
	author = {Takuya, Sugahara and Zhang, Renyuan and Nakashima, Yasuhiko},
	month = apr,
	year = {2021},
	keywords = {��dl},
	pages = {1--3},
}

@inproceedings{kushawaha_distilling_2021,
	address = {Milan, Italy},
	title = {Distilling {Spikes}: {Knowledge} {Distillation} in {Spiking} {Neural} {Networks}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-72818-808-9},
	shorttitle = {Distilling {Spikes}},
	url = {https://ieeexplore.ieee.org/document/9412147/},
	doi = {10.1109/ICPR48806.2021.9412147},
	urldate = {2024-05-23},
	booktitle = {2020 25th {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	publisher = {IEEE},
	author = {Kushawaha, Ravi Kumar and Kumar, Saurabh and Banerjee, Biplab and Velmurugan, Rajbabu},
	month = jan,
	year = {2021},
	keywords = {��dl},
	pages = {4536--4543},
}

@misc{lv_spikebert_2023,
	title = {{SpikeBERT}: {A} {Language} {Spikformer} {Trained} with {Two}-{Stage} {Knowledge} {Distillation} from {BERT}},
	shorttitle = {{SpikeBERT}},
	url = {http://arxiv.org/abs/2308.15122},
	doi = {10.48550/arXiv.2308.15122},
	abstract = {Spiking neural networks (SNNs) offer a promising avenue to implement deep neural networks in a more energy-efficient way. However, the network architectures of existing SNNs for language tasks are too simplistic, and deep architectures have not been fully explored, resulting in a significant performance gap compared to mainstream transformer-based networks such as BERT. To this end, we improve a recently-proposed spiking transformer (i.e., Spikformer) to make it possible to process language tasks and propose a two-stage knowledge distillation method for training it, which combines pre-training by distilling knowledge from BERT with a large collection of unlabelled texts and fine-tuning with task-specific instances via knowledge distillation again from the BERT fine-tuned on the same training examples. Through extensive experimentation, we show that the models trained with our method, named SpikeBERT, outperform state-of-the-art SNNs and even achieve comparable results to BERTs on text classification tasks for both English and Chinese with much less energy consumption.},
	urldate = {2023-09-16},
	publisher = {arXiv},
	author = {Lv, Changze and Li, Tianlong and Xu, Jianhan and Gu, Chenxi and Ling, Zixuan and Zhang, Cenyuan and Zheng, Xiaoqing and Huang, Xuanjing},
	month = aug,
	year = {2023},
	note = {arXiv:2308.15122 [cs]},
	keywords = {Computer Science - Computation and Language, SNN模型优化, 写 relatedwork, ��dl},
}

@inproceedings{ding_enhancing_2024,
	title = {Enhancing the {Robustness} of {Spiking} {Neural} {Networks} with {Stochastic} {Gating} {Mechanisms}},
	volume = {38},
	copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/27804},
	doi = {10.1609/aaai.v38i1.27804},
	abstract = {Spiking neural networks (SNNs) exploit neural spikes to provide solutions for low-power intelligent applications on neuromorphic hardware. Although SNNs have high computational efficiency due to spiking communication, they still lack resistance to adversarial attacks and noise perturbations. In the brain, neuronal responses generally possess stochasticity induced by ion channels and synapses, while the role of stochasticity in computing tasks is poorly understood. Inspired by this, we elaborate a stochastic gating spiking neural model for layer-by-layer spike communication, introducing stochasticity to SNNs. Through theoretical analysis, our gating model can be viewed as a regularizer that prevents error amplification under attacks. Meanwhile, our work can explain the robustness of Poisson coding. Experimental results prove that our method can be used alone or with existing robust enhancement algorithms to improve SNN robustness and reduce SNN energy consumption. We hope our work will shed new light on the role of stochasticity in the computation of SNNs. Our code is available at https://github.com/DingJianhao/StoG-meets-SNN/.},
	language = {en},
	urldate = {2024-05-09},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Ding, Jianhao and Yu, Zhaofei and Huang, Tiejun and Liu, Jian K.},
	month = mar,
	year = {2024},
	note = {Number: 1},
	keywords = {CMS: Neural Spike Coding},
	pages = {492--502},
}

@inproceedings{li_seenn_2023,
	title = {{SEENN}: {Towards} {Temporal} {Spiking} {Early} {Exit} {Neural} {Networks}},
	volume = {36},
	shorttitle = {{SEENN}-{Spiking} {Early}-{Exit} {Neural} {Networks}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/c801e68207da477bbc44182b9fac1129-Abstract-Conference.html},
	language = {en},
	urldate = {2024-05-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Li, Yuhang and Geller, Tamar and Kim, Youngeun and Panda, Priyadarshini},
	month = dec,
	year = {2023},
	keywords = {timestep},
	pages = {63327--63342},
}

@inproceedings{li_seenn_2023-1,
	title = {{SEENN}: {Towards} {Temporal} {Spiking} {Early} {Exit} {Neural} {Networks}},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/c801e68207da477bbc44182b9fac1129-Paper-Conference.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Li, Yuhang and Geller, Tamar and Kim, Youngeun and Panda, Priyadarshini},
	editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
	year = {2023},
	keywords = {/unread},
	pages = {63327--63342},
}

@article{li_seenn_2023-2,
	title = {{SEENN}: {Towards} {Temporal} {Spiking} {Early} {Exit} {Neural} {Networks}},
	volume = {36},
	shorttitle = {{SEENN}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/c801e68207da477bbc44182b9fac1129-Abstract-Conference.html},
	language = {en},
	urldate = {2024-05-25},
	journal = {Proceedings of NeurIPS},
	author = {Li, Yuhang and Geller, Tamar and Kim, Youngeun and Panda, Priyadarshini},
	month = dec,
	year = {2023},
	keywords = {/unread},
	pages = {63327--63342},
}

@article{stoecker_neural_1996,
	title = {A neural network for position invariant pattern recognition combining spiking neurons with the {Fourier}-transform},
	volume = {7},
	issn = {0129-0657},
	doi = {10.1142/s0129065796000695},
	abstract = {We present an approach for position invariant recognition of individual objects in composite scenes, combining neural networks and algorithmic methods. A dynamic network of spiking neurons is used to generate object definition and figure/ground separation via temporal signal correlations. A shift invariant representation of the network spike activity distribution is subsequently realized via the amplitude spectrum of the Fourier-transform. Objects and their transformed representations are therefore linked in the time domain. The model segregates scenes and classifies individual patterns independent of their position in the input scene.},
	language = {eng},
	number = {6},
	journal = {International Journal of Neural Systems},
	author = {Stoecker, M. and Reitboeck, H. J.},
	month = dec,
	year = {1996},
	pmid = {9113533},
	keywords = {Algorithms, Computer Simulation, Fourier Analysis, Frequency, Models, Neurological, Neural Networks, Computer, Neurons, Pattern Recognition, Automated},
	pages = {727--733},
}

@inproceedings{garg_dct-snn_2021,
	title = {{DCT}-{SNN}: {Using} {DCT} {To} {Distribute} {Spatial} {Information} {Over} {Time} for {Low}-{Latency} {Spiking} {Neural} {Networks}},
	shorttitle = {{DCT}-{SNN}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Garg_DCT-SNN_Using_DCT_To_Distribute_Spatial_Information_Over_Time_for_ICCV_2021_paper.html},
	language = {en},
	urldate = {2024-05-01},
	author = {Garg, Isha and Chowdhury, Sayeed Shafayet and Roy, Kaushik},
	year = {2021},
	keywords = {Frequency},
	pages = {4671--4680},
}

@article{meng_training_2022,
	title = {Training much deeper spiking neural networks with a small number of time-steps},
	volume = {153},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608022002064},
	doi = {10.1016/j.neunet.2022.06.001},
	abstract = {Spiking Neural Network (SNN) is a promising energy-efficient neural architecture when implemented on neuromorphic hardware. The Artificial Neural Network (ANN) to SNN conversion method, which is the most effective SNN training method, has successfully converted moderately deep ANNs to SNNs with satisfactory performance. However, this method requires a large number of time-steps, which hurts the energy efficiency of SNNs. How to effectively covert a very deep ANN (e.g., more than 100 layers) to an SNN with a small number of time-steps remains a difficult task. To tackle this challenge, this paper makes the first attempt to propose a novel error analysis framework that takes both the “quantization error” and the “deviation error” into account, which comes from the discretization of SNN dynamicsthe neuron’s coding scheme and the inconstant input currents at intermediate layers, respectively. Particularly, our theories reveal that the “deviation error” depends on both the spike threshold and the input variance. Based on our theoretical analysis, we further propose the Threshold Tuning and Residual Block Restructuring (TTRBR) method that can convert very deep ANNs ({\textgreater}100 layers) to SNNs with negligible accuracy degradation while requiring only a small number of time-steps. With very deep networks, our TTRBR method achieves state-of-the-art (SOTA) performance on the CIFAR-10, CIFAR-100, and ImageNet classification tasks.},
	urldate = {2024-04-29},
	journal = {Neural Networks},
	author = {Meng, Qingyan and Yan, Shen and Xiao, Mingqing and Wang, Yisen and Lin, Zhouchen and Luo, Zhi-Quan},
	month = sep,
	year = {2022},
	keywords = {Conversion error analysis, Spiking neural networks, ��ANN2SNN},
	pages = {254--268},
}

@article{suetake_s3nn_2023,
	title = {{S3NN}: {Time} step reduction of spiking surrogate gradients for training energy efficient single-step spiking neural networks},
	volume = {159},
	issn = {0893-6080},
	shorttitle = {{S3NN}},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608022005007},
	doi = {10.1016/j.neunet.2022.12.008},
	abstract = {As the scales of neural networks increase, techniques that enable them to run with low computational cost and energy efficiency are required. From such demands, various efficient neural network paradigms, such as spiking neural networks (SNNs) or binary neural networks (BNNs), have been proposed. However, they have sticky drawbacks, such as degraded inference accuracy and latency. To solve these problems, we propose a single-step spiking neural network (S3NN), an energy-efficient neural network with low computational cost and high precision. The proposed S3NN processes the information between hidden layers by spikes as SNNs. Nevertheless, it has no temporal dimension so that there is no latency within training and inference phases as BNNs. Thus, the proposed S3NN has a lower computational cost than SNNs that require time-series processing. However, S3NN cannot adopt naïve backpropagation algorithms due to the non-differentiability nature of spikes. We deduce a suitable neuron model by reducing the surrogate gradient for multi-time step SNNs to a single-time step. We experimentally demonstrated that the obtained surrogate gradient allows S3NN to be trained appropriately. We also showed that the proposed S3NN could achieve comparable accuracy to full-precision networks while being highly energy-efficient.},
	urldate = {2024-04-29},
	journal = {Neural Networks},
	author = {Suetake, Kazuma and Ikegawa, Shin-ichi and Saiin, Ryuji and Sawada, Yoshihide},
	month = feb,
	year = {2023},
	keywords = {Binary neural network, Energy efficiency, Single-time step, Spiking neural network, Surrogate gradient},
	pages = {208--219},
}

@article{chowdhury_one_2021,
	title = {One {Timestep} {Is} {All} {You} {Need}: {Training} {Spiking} {Neural} {Networks} with {Ultra} {Low} {Latency}},
	shorttitle = {One {Timestep} {Is} {All} {You} {Need}},
	url = {https://openreview.net/forum?id=swRxhFpK5ds},
	abstract = {Spiking Neural Networks (SNNs) can be energy efficient alternatives to commonly used deep neural networks (DNNs). Through event-driven information processing, SNNs can considerably reduce the compute requirements of DNNs. However, high inference latency is a significant hindrance to their deployment. Computation over multiple timesteps increases latency and incurs memory access overhead of fetching membrane potentials, both of which lessen the energy benefits of SNNs. Hence, latency reduction is pivotal to obtain SNNs with high energy efficiency. However, reducing latency can have an adverse effect on accuracy. To obtain solutions which optimize the accuracy-energy-latency trade-off, we propose an iterative training method which starts with an SNN of T (T{\textgreater}1) timesteps, and reduces T every iteration of training, with neuron threshold and leak as trainable parameters. This results in a continuum of SNNs, starting from an SNN trained with T timesteps, all the way up to unit latency. We use direct input encoding (analog inputs from pixels) with the first convolutional layer of the network of leaky integrate and fire (LIF) neurons acting as spike generator. We choose T=5 as our starting point, since it is the minimum reported latency to achieve satisfactory performance on ImageNet. Training SNNs directly with 1 timestep results in convergence failure due to layerwise spike vanishing and difficulty in finding optimum thresholds. The proposed iterative training approach overcomes this through enabling the learning of suitable layerwise thresholds with backpropagation by maintaining sufficient spiking activity, starting from T timesteps up to 1. Using the proposed training algorithm, we achieve top-1 accuracy of 93.05\%, 70.15\% and 67.71\% on CIFAR-10, CIFAR-100 and ImageNet, respectively with VGG16, in just 1 timestep. Compared to a 5 timestep SNN, the 1 timestep SNN achieves {\textasciitilde}5X enhancement in efficiency, with an accuracy drop of {\textasciitilde}1\%. In addition, 1 timestep SNNs perform inference with 5X reduced latency compared to state-of-the-art SNNs, and provide 25-33X higher energy efficiency compared to DNNs, while being comparable to them in performance. The proposed method also enables training reinforcement learning agents on Cartpole and Atari pong environments which infer using 1 timestep.},
	language = {en},
	urldate = {2024-04-24},
	author = {Chowdhury, Sayeed Shafayet and Rathi, Nitin and Roy, Kaushik},
	month = oct,
	year = {2021},
	keywords = {��ANN2SNN},
}

@misc{chakraborty_sparse_2024,
	title = {Sparse {Spiking} {Neural} {Network}: {Exploiting} {Heterogeneity} in {Timescales} for {Pruning} {Recurrent} {SNN}},
	shorttitle = {Sparse {Spiking} {Neural} {Network}},
	url = {http://arxiv.org/abs/2403.03409},
	doi = {10.48550/arXiv.2403.03409},
	abstract = {Recurrent Spiking Neural Networks (RSNNs) have emerged as a computationally efficient and brain-inspired learning model. The design of sparse RSNNs with fewer neurons and synapses helps reduce the computational complexity of RSNNs. Traditionally, sparse SNNs are obtained by first training a dense and complex SNN for a target task, and, then, pruning neurons with low activity (activity-based pruning) while maintaining task performance. In contrast, this paper presents a task-agnostic methodology for designing sparse RSNNs by pruning a large randomly initialized model. We introduce a novel Lyapunov Noise Pruning (LNP) algorithm that uses graph sparsification methods and utilizes Lyapunov exponents to design a stable sparse RSNN from a randomly initialized RSNN. We show that the LNP can leverage diversity in neuronal timescales to design a sparse Heterogeneous RSNN (HRSNN). Further, we show that the same sparse HRSNN model can be trained for different tasks, such as image classification and temporal prediction. We experimentally show that, in spite of being task-agnostic, LNP increases computational efficiency (fewer neurons and synapses) and prediction performance of RSNNs compared to traditional activity-based pruning of trained dense models.},
	urldate = {2024-05-23},
	publisher = {arXiv},
	author = {Chakraborty, Biswadeep and Kang, Beomseok and Kumar, Harshit and Mukhopadhyay, Saibal},
	month = mar,
	year = {2024},
	note = {arXiv:2403.03409 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{chen_state_2022,
	title = {State {Transition} of {Dendritic} {Spines} {Improves} {Learning} of {Sparse} {Spiking} {Neural} {Networks}},
	url = {https://proceedings.mlr.press/v162/chen22ac.html},
	abstract = {Spiking Neural Networks (SNNs) are considered a promising alternative to Artificial Neural Networks (ANNs) for their event-driven computing paradigm when deployed on energy-efficient neuromorphic hardware. Recently, deep SNNs have shown breathtaking performance improvement through cutting-edge training strategy and flexible structure, which also scales up the number of parameters and computational burdens in a single network. Inspired by the state transition of dendritic spines in the filopodial model of spinogenesis, we model different states of SNN weights, facilitating weight optimization for pruning. Furthermore, the pruning speed can be regulated by using different functions describing the growing threshold of state transition. We organize these techniques as a dynamic pruning algorithm based on nonlinear reparameterization mapping from spine size to SNN weights. Our approach yields sparse deep networks on the large-scale dataset (SEW ResNet18 on ImageNet) while maintaining state-of-the-art low performance loss ( 3\% at 88.8\% sparsity) compared to existing pruning methods on directly trained SNNs. Moreover, we find out pruning speed regulation while learning is crucial to avoiding disastrous performance degradation at the final stages of training, which may shed light on future work on SNN pruning.},
	language = {en},
	urldate = {2024-05-23},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Chen, Yanqi and Yu, Zhaofei and Fang, Wei and Ma, Zhengyu and Huang, Tiejun and Tian, Yonghong},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {3701--3715},
}

@article{abbott_building_2016-1,
	title = {Building functional networks of spiking model neurons},
	volume = {19},
	copyright = {2016 Springer Nature America, Inc.},
	issn = {1546-1726},
	doi = {10.1038/nn.4241},
	abstract = {The networks used by computer scientists and by modelers in neuroscience frequently consider unit activities as continuous. Neurons, however, com­municate primarily through discontinuous spiking. This Perspective offers a unifying view of the current methods for transferring our ability to construct functional networks from continuous to more realistic spiking network models.},
	language = {en},
	number = {3},
	urldate = {2024-05-23},
	journal = {Nature Neuroscience},
	author = {Abbott, L. F. and DePasquale, Brian and Memmesheimer, Raoul-Martin},
	month = mar,
	year = {2016},
	note = {Publisher: Nature Publishing Group},
	keywords = {Network models, Neural encoding},
	pages = {350--355},
}

@article{shao_eicil_2023,
	title = {{EICIL}: {Joint} {Excitatory} {Inhibitory} {Cycle} {Iteration} {Learning} for {Deep} {Spiking} {Neural} {Networks}},
	volume = {36},
	shorttitle = {{EICIL}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/65e876f6a98c6799d0b3145966dd73e2-Abstract-Conference.html},
	language = {en},
	urldate = {2024-05-23},
	journal = {Proceedings of NeurIPS},
	author = {Shao, Zihang and Fang, Xuanye and Li, Yaxin and Feng, Chaoran and Shen, Jiangrong and Xu, Qi},
	month = dec,
	year = {2023},
	pages = {32117--32128},
}

@misc{lee_energy-efficient_2022,
	title = {Energy-efficient {Knowledge} {Distillation} for {Spiking} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2106.07172},
	doi = {10.48550/arXiv.2106.07172},
	abstract = {Spiking neural networks (SNNs) have been gaining interest as energy-efficient alternatives of conventional artificial neural networks (ANNs) due to their event-driven computation. Considering the future deployment of SNN models to constrained neuromorphic devices, many studies have applied techniques originally used for ANN model compression, such as network quantization, pruning, and knowledge distillation, to SNNs. Among them, existing works on knowledge distillation reported accuracy improvements of student SNN model. However, analysis on energy efficiency, which is also an important feature of SNN, was absent. In this paper, we thoroughly analyze the performance of the distilled SNN model in terms of accuracy and energy efficiency. In the process, we observe a substantial increase in the number of spikes, leading to energy inefficiency, when using the conventional knowledge distillation methods. Based on this analysis, to achieve energy efficiency, we propose a novel knowledge distillation method with heterogeneous temperature parameters. We evaluate our method on two different datasets and show that the resulting SNN student satisfies both accuracy improvement and reduction of the number of spikes. On MNIST dataset, our proposed student SNN achieves up to 0.09\% higher accuracy and produces 65\% less spikes compared to the student SNN trained with conventional knowledge distillation method. We also compare the results with other SNN compression techniques and training methods.},
	urldate = {2024-05-23},
	publisher = {arXiv},
	author = {Lee, Dongjin and Park, Seongsik and Kim, Jongwan and Doh, Wuhyeong and Yoon, Sungroh},
	month = jun,
	year = {2022},
	note = {arXiv:2106.07172 [cs]},
	keywords = {/unread, Computer Science - Neural and Evolutionary Computing},
}

@article{wu_spatio-temporal_2018,
	title = {Spatio-{Temporal} {Backpropagation} for {Training} {High}-{Performance} {Spiking} {Neural} {Networks}},
	volume = {12},
	issn = {1662-453X},
	shorttitle = {{STBP}},
	abstract = {Spiking neural networks (SNNs) are promising in ascertaining brain-like behaviors since spikes are capable of encoding spatio-temporal information. Recent schemes, e.g., pre-training from artificial neural networks (ANNs) or direct training based on backpropagation (BP), make the high-performance supervised training of SNNs possible. However, these methods primarily fasten more attention on its spatial domain information, and the dynamics in temporal domain are attached less significance. Consequently, this might lead to the performance bottleneck, and scores of training techniques shall be additionally required. Another underlying problem is that the spike activity is naturally non-differentiable, raising more difficulties in supervised training of SNNs. In this paper, we propose a spatio-temporal backpropagation (STBP) algorithm for training high-performance SNNs. In order to solve the non-differentiable problem of SNNs, an approximated derivative for spike activity is proposed, being appropriate for gradient descent training. The STBP algorithm combines the layer-by-layer spatial domain (SD) and the timing-dependent temporal domain (TD), and does not require any additional complicated skill. We evaluate this method through adopting both the fully connected and convolutional architecture on the static MNIST dataset, a custom object detection dataset, and the dynamic N-MNIST dataset. Results bespeak that our approach achieves the best accuracy compared with existing state-of-the-art algorithms on spiking networks. This work provides a new perspective to investigate the high-performance SNNs for future brain-like computing paradigm with rich spatio-temporal dynamics.},
	urldate = {2023-12-25},
	journal = {Frontiers in Neuroscience},
	author = {Wu, Yujie and Deng, Lei and Li, Guoqi and Zhu, Jun and Shi, Luping},
	year = {2018},
	keywords = {\#ref},
	pages = {323875},
}

@article{wu_direct_2019,
	title = {Direct {Training} for {Spiking} {Neural} {Networks}: {Faster}, {Larger}, {Better}},
	volume = {33},
	shorttitle = {{NeuNorm}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/3929},
	doi = {10.1609/aaai.v33i01.33011311},
	abstract = {Spiking neural networks (SNNs) that enables energy efficient implementation on emerging neuromorphic hardware are gaining more attention. Yet now, SNNs have not shown competitive performance compared with artificial neural networks (ANNs), due to the lack of effective learning algorithms and efficient programming frameworks. We address this issue from two aspects: (1) We propose a neuron normalization technique to adjust the neural selectivity and develop a direct learning algorithm for deep SNNs. (2) Via narrowing the rate coding window and converting the leaky integrate-and-fire (LIF) model into an explicitly iterative version, we present a Pytorch-based implementation method towards the training of large-scale SNNs. In this way, we are able to train deep SNNs with tens of times speedup. As a result, we achieve significantly better accuracy than the reported works on neuromorphic datasets (N-MNIST and DVSCIFAR10), and comparable accuracy as existing ANNs and pre-trained SNNs on non-spiking datasets (CIFAR10). To our best knowledge, this is the first work that demonstrates direct training of deep SNNs with high performance on CIFAR10, and the efficient implementation provides a new way to explore the potential of SNNs.},
	language = {en},
	urldate = {2023-12-11},
	journal = {Proceedings of AAAI},
	author = {Wu, Yujie and Deng, Lei and Li, Guoqi and Zhu, Jun and Xie, Yuan and Shi, Luping},
	month = jul,
	year = {2019},
	note = {Number: 01},
	keywords = {\#ref},
	pages = {1311--1318},
}

@inproceedings{zhang_knowledge_2023,
	title = {Knowledge {Distillation} {For} {Spiking} {Neural} {Network}},
	url = {https://ieeexplore.ieee.org/abstract/document/10489324},
	doi = {10.1109/RICAI60863.2023.10489324},
	abstract = {The spiking neural network (SNN) is an effective computing model that mimics the brain's complex information processing and transmission systems. The spike-based computing paradigm is used to emulate the highly efficient processing capabilities of the human brain. Hence, it may be argued that SNNs exhibit superior biological interpretability in comparison to conventional artificial neural networks (ANN). However, similar to ANNs, the enhancement in performance of SNNs is likewise influenced by the increase in the number of network layers, and there is also a corresponding increase in the resources required for task execution. Efficiently doing complicated tasks with little resources is of utmost importance. Knowledge distillation(KD) is a widely used technique within the domain of transfer learning that can enhance the efficiency of a smaller model by transferring the learned information from a larger model to facilitate the training of the smaller target model. However, there is an absence of research on its implementation in SNNs. This paper utilizes the well-known KD algorithm in ANNs to implement it in SNNs, with the objective of exploring the application of the KD technique in the context of SNN s and then evaluating its effectiveness in the task of image classification. We conducted three separate exploratory endeavors. 1. Utilize the ANN as the teacher model to train a student model including an SNN architecture (referred to as ANN-KD-SNN). 2. Investigate the effects of changing time steps on the distillation process within the ANN-KD-SNN paradigm. 3. Exploit the SNN as the teacher model to facilitate the training of a student model, which is also an SNN (referred to as SNN-KD-SNN). Our experiments demonstrate the necessity and viability of employing KD in SNNs},
	urldate = {2024-05-22},
	booktitle = {2023 5th {International} {Conference} on {Robotics}, {Intelligent} {Control} and {Artificial} {Intelligence} ({RICAI})},
	author = {Zhang, Fengzhao and Yu, Chengting and Ma, Hanzhi and Gu, Zheming and Li, Er-ping},
	month = dec,
	year = {2023},
	keywords = {Artificial Neural Network, Computational modeling, Computer architecture, Image Classification, Knowledge Distillation, Knowledge engineering, MIMICs, Spiking Neural Network, Task analysis, Training, Transfer learning, ��ANN2SNN, ��dl},
	pages = {1015--1020},
}

@inproceedings{xu_constructing_2023,
	title = {Constructing {Deep} {Spiking} {Neural} {Networks} {From} {Artificial} {Neural} {Networks} {With} {Knowledge} {Distillation}},
	language = {en},
	urldate = {2023-10-11},
	booktitle = {Proceedings of {CVPR}},
	author = {Xu, Qi and Li, Yaxin and Shen, Jiangrong and Liu, Jian K. and Tang, Huajin and Pan, Gang},
	month = jun,
	year = {2023},
	keywords = {\#ref, ��ANN2SNN, ��dl},
	pages = {7886--7895},
}

@inproceedings{xu_constructing_2023-1,
	title = {Constructing {Deep} {Spiking} {Neural} {Networks} {From} {Artificial} {Neural} {Networks} {With} {Knowledge} {Distillation}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Xu_Constructing_Deep_Spiking_Neural_Networks_From_Artificial_Neural_Networks_With_CVPR_2023_paper.html},
	language = {en},
	urldate = {2024-05-22},
	author = {Xu, Qi and Li, Yaxin and Shen, Jiangrong and Liu, Jian K. and Tang, Huajin and Pan, Gang},
	year = {2023},
	keywords = {/unread},
	pages = {7886--7895},
}

@inproceedings{kundu_spike-thrift_2021,
	title = {Spike-{Thrift}: {Towards} {Energy}-{Efficient} {Deep} {Spiking} {Neural} {Networks} by {Limiting} {Spiking} {Activity} via {Attention}-{Guided} {Compression}},
	shorttitle = {Spike-{Thrift}},
	url = {https://openaccess.thecvf.com/content/WACV2021/html/Kundu_Spike-Thrift_Towards_Energy-Efficient_Deep_Spiking_Neural_Networks_by_Limiting_Spiking_WACV_2021_paper.html},
	language = {en},
	urldate = {2024-03-01},
	author = {Kundu, Souvik and Datta, Gourav and Pedram, Massoud and Beerel, Peter A.},
	year = {2021},
	keywords = {��ANN2SNN, ��Attention, ��dl},
	pages = {3953--3962},
}

@article{duan_temporal_2022,
	title = {Temporal {Effective} {Batch} {Normalization} in {Spiking} {Neural} {Networks}},
	volume = {35},
	shorttitle = {{TEBN}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/de2ad3ed44ee4e675b3be42aa0b615d0-Abstract-Conference.html},
	language = {en},
	urldate = {2024-01-22},
	journal = {Proceedings of NeurIPS},
	author = {Duan, Chaoteng and Ding, Jianhao and Chen, Shiyan and Yu, Zhaofei and Huang, Tiejun},
	month = dec,
	year = {2022},
	pages = {34377--34390},
}

@article{zheng_going_2021,
	title = {Going {Deeper} {With} {Directly}-{Trained} {Larger} {Spiking} {Neural} {Networks}},
	volume = {35},
	copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	shorttitle = {{TDBN}},
	doi = {10.1609/aaai.v35i12.17320},
	abstract = {Spiking neural networks (SNNs) are promising in a bio-plausible coding for spatio-temporal information and event-driven signal processing, which is very suited for energy-efficient implementation in neuromorphic hardware. However, the unique working mode of SNNs makes them more difficult to train than traditional networks. Currently, there are two main routes to explore the training of deep SNNs with high performance. The first is to convert a pre-trained ANN model to its SNN version, which usually requires a long coding window for convergence and cannot exploit the spatio-temporal features during training for solving temporal tasks. The other is to directly train SNNs in the spatio-temporal domain. But due to the binary spike activity of the firing function and the problem of gradient vanishing or explosion, current methods are restricted to shallow architectures and thereby difficult in harnessing large-scale datasets (e.g. ImageNet). To this end, we propose a threshold-dependent batch normalization (tdBN) method based on the emerging spatio-temporal backpropagation, termed “STBP-tdBN”, enabling direct training of a very deep SNN and the efficient implementation of its inference on neuromorphic hardware. With the proposed method and elaborated shortcut connection, we significantly extend directly-trained SNNs from a shallow structure (},
	language = {en},
	urldate = {2024-01-16},
	journal = {Proceedings of AAAI},
	author = {Zheng, Hanle and Wu, Yujie and Deng, Lei and Hu, Yifan and Li, Guoqi},
	month = may,
	year = {2021},
	note = {Number: 12},
	keywords = {\#ref, (Deep) Neural Network Learning Theory},
	pages = {11062--11070},
}

@inproceedings{meng_training_2022,
	title = {Training {High}-{Performance} {Low}-{Latency} {Spiking} {Neural} {Networks} by {Differentiation} on {Spike} {Representation}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Meng_Training_High-Performance_Low-Latency_Spiking_Neural_Networks_by_Differentiation_on_Spike_CVPR_2022_paper.html},
	language = {en},
	urldate = {2024-05-19},
	author = {Meng, Qingyan and Xiao, Mingqing and Yan, Shen and Wang, Yisen and Lin, Zhouchen and Luo, Zhi-Quan},
	year = {2022},
	pages = {12444--12453},
}

@article{zhang_low_2023,
	title = {Low {Latency} and {Sparse} {Computing} {Spiking} {Neural} {Networks} {With} {Self}-{Driven} {Adaptive} {Threshold} {Plasticity}},
	issn = {2162-2388},
	url = {https://ieeexplore.ieee.org/abstract/document/10219181},
	doi = {10.1109/TNNLS.2023.3300514},
	abstract = {Spiking neural networks (SNNs) have captivated the attention worldwide owing to their compelling advantages in low power consumption, high biological plausibility, and strong robustness. However, the intrinsic latency associated with SNNs during inference poses a significant challenge, impeding their further development and application. This latency is caused by the need for spiking neurons to collect electrical stimuli and generate spikes only when their membrane potential exceeds a firing threshold. Considering the firing threshold plays a crucial role in SNN performance, this article proposes a self-driven adaptive threshold plasticity (SATP) mechanism, wherein neurons autonomously adjust the firing thresholds based on their individual state information using unsupervised learning rules, of which the adjustment is triggered by their own firing events. SATP is based on the principle of maximizing the information contained in the output spike rate distribution of each neuron. This article derives the mathematical expression of SATP and provides extensive experimental results, demonstrating that SATP effectively reduces SNN inference latency, further reduces the computation density while improving computational accuracy, so that SATP facilitates SNN models to be with low latency, sparse computing, and high accuracy.},
	urldate = {2024-05-19},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Zhang, Anguo and Shi, Jieming and Wu, Junyi and Zhou, Yongcheng and Yu, Wei},
	year = {2023},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Adaptive systems, Biological neural networks, Firing, Low latency communication, Low latency inference, Membrane potentials, Neurons, Training, neuronal firing threshold, self-driven adaptive threshold plasticity (SATP), sparse computing, spiking neural network (SNN), ��ANN2SNN},
	pages = {1--12},
}

@article{hwang_low-latency_2021,
	title = {Low-{Latency} {Spiking} {Neural} {Networks} {Using} {Pre}-{Charged} {Membrane} {Potential} and {Delayed} {Evaluation}},
	volume = {15},
	issn = {1662-453X},
	url = {https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2021.629000/full},
	doi = {10.3389/fnins.2021.629000},
	abstract = {{\textless}p{\textgreater}Spiking neural networks (SNNs) have attracted many researchers’ interests due to its biological plausibility and event-driven characteristic. In particular, recently, many studies on high-performance SNNs comparable to the conventional analog-valued neural networks (ANNs) have been reported by converting weights trained from ANNs into SNNs. However, unlike ANNs, SNNs have an inherent latency that is required to reach the best performance because of differences in operations of neuron. In SNNs, not only spatial integration but also temporal integration exists, and the information is encoded by spike trains rather than values in ANNs. Therefore, it takes time to achieve a steady-state of the performance in SNNs. The latency is worse in deep networks and required to be reduced for the practical applications. In this work, we propose a pre-charged membrane potential ({\textless}italic{\textgreater}PCMP{\textless}/italic{\textgreater}) for the latency reduction in SNN. A variety of neural network applications (e.g., classification, autoencoder using MNIST and CIFAR-10 datasets) are trained and converted to SNNs to demonstrate the effect of the proposed approach. The latency of SNNs is successfully reduced without accuracy loss. In addition, we propose a delayed evaluation method ({\textless}italic{\textgreater}DE{\textless}/italic{\textgreater}), by which the errors during the initial transient are discarded. The error spikes occurring in the initial transient is removed by {\textless}italic{\textgreater}DE{\textless}/italic{\textgreater}, resulting in the further latency reduction. {\textless}italic{\textgreater}DE{\textless}/italic{\textgreater} can be used in combination with {\textless}italic{\textgreater}PCMP{\textless}/italic{\textgreater} for further latency reduction. Finally, we also show the advantages of the proposed methods in improving the number of spikes required to reach a steady-state of the performance in SNNs for energy-efficient computing.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2024-05-19},
	journal = {Frontiers in Neuroscience},
	author = {Hwang, Sungmin and Chang, Jeesoo and Oh, Min-Hye and Min, Kyung Kyu and Jang, Taejin and Park, Kyungchul and Yu, Junsu and Lee, Jong-Ho and Park, Byung-Gook},
	month = feb,
	year = {2021},
	note = {Publisher: Frontiers},
	keywords = {Delayed evaluation, Fast inference, Low-latency, Pre-Charged Membrane Potential, Spiking Neural Networks (SNN), ��ANN2SNN},
}

@article{chen_high-performance_2024,
	title = {High-performance deep spiking neural networks via at-most-two-spike exponential coding},
	volume = {176},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608024002703},
	doi = {10.1016/j.neunet.2024.106346},
	abstract = {Spiking neural networks (SNNs) provide necessary models and algorithms for neuromorphic computing. A popular way of building high-performance deep SNNs is to convert ANNs to SNNs, taking advantage of advanced and well-trained ANNs. Here we propose an ANN to SNN conversion methodology that uses a time-based coding scheme, named At-most-two-spike Exponential Coding (AEC), and a corresponding AEC spiking neuron model for ANN-SNN conversion. AEC neurons employ quantization-compensating spikes to improve coding accuracy and capacity, with each neuron generating up to two spikes within the time window. Two exponential decay functions with tunable parameters are proposed to represent the dynamic encoding thresholds, based on which pixel intensities are encoded into spike times and spike times are decoded into pixel intensities. The hyper-parameters of AEC neurons are fine-tuned based on the loss function of SNN-decoded values and ANN-activation values. In addition, we design two regularization terms for the number of spikes, providing the possibility to achieve the best trade-off between accuracy, latency and power consumption. The experimental results show that, compared to other similar methods, the proposed scheme not only obtains deep SNNs with higher accuracy, but also has more significant advantages in terms of energy efficiency and inference latency. More details can be found at https://github.com/RPDS2020/AEC.git.},
	urldate = {2024-05-19},
	journal = {Neural Networks},
	author = {Chen, Yunhua and Feng, Ren and Xiong, Zhimin and Xiao, Jinsheng and Liu, Jian K.},
	month = aug,
	year = {2024},
	keywords = {ANN-SNN conversion, At-most-two-spike exponential coding, Deep spiking neural networks, Time-based coding, ��ANN2SNN},
	pages = {106346},
}

@inproceedings{bojkovic_data_2024,
	title = {Data {Driven} {Threshold} and {Potential} {Initialization} for {Spiking} {Neural} {Networks}},
	url = {https://proceedings.mlr.press/v238/bojkovic24a.html},
	abstract = {Spiking neural networks (SNNs) present an increasingly popular alternative to artificial neural networks (ANNs), due to their energy and time efficiency when deployed on neuromorphic hardware. However, due to their discrete and highly non-differentiable nature, training SNNs is a challenging task and remains an active area of research. Some of the most prominent ways to train SNNs are based on ANN-to-SNN conversion where an SNN model is initialized with parameters from the corresponding, pre-trained ANN model. SNN models trained through ANN-to-SNN conversion or hybrid training show state of the art performance among SNNs on many machine learning tasks, comparable to those of ANNs. However, the top performing models need high latency or tailored ANNs to perform well, and in general are not using the full information available from ANNs. In this work, we propose novel method to initialize SNN’s thresholds and initial membrane potential after ANN-to-SNN conversion, using distributions of ANN’s activation values. We provide a theoretical framework for feature distribution-based conversion error, providing theoretical results on optimal membrane initialization and thresholds which minimize this error, as well as a practical algorithm for finding these optimal values. We test our method, both as a stand-alone ANN-to-SNN conversion and in combination with other methods, and show state of the art results on high-dimensional datasets such as CIFAR10, CIFAR100 and ImageNet and various architectures. Our code is available at {\textbackslash}url\{https://github.com/srinuvaasu/data\_driven\_init\}},
	language = {en},
	urldate = {2024-05-19},
	booktitle = {Proceedings of {The} 27th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Bojkovic, Velibor and Anumasa, Srinivas and Masi, Giulia De and Gu, Bin and Xiong, Huan},
	month = apr,
	year = {2024},
	note = {ISSN: 2640-3498},
	keywords = {��ANN2SNN},
	pages = {4771--4779},
}

@article{wang_toward_2023,
	title = {Toward {High}-{Accuracy} and {Low}-{Latency} {Spiking} {Neural} {Networks} {With} {Two}-{Stage} {Optimization}},
	issn = {2162-2388},
	url = {https://ieeexplore.ieee.org/abstract/document/10361844},
	doi = {10.1109/TNNLS.2023.3337176},
	abstract = {Spiking neural networks (SNNs) operating with asynchronous discrete events show higher energy efficiency with sparse computation. A popular approach for implementing deep SNNs is artificial neural network (ANN)–SNN conversion combining both efficient training of ANNs and efficient inference of SNNs. However, the accuracy loss is usually nonnegligible, especially under few time steps, which restricts the applications of SNN on latency-sensitive edge devices greatly. In this article, we first identify that such performance degradation stems from the misrepresentation of the negative or overflow residual membrane potential in SNNs. Inspired by this, we decompose the conversion error into three parts: quantization error, clipping error, and residual membrane potential representation error. With such insights, we propose a two-stage conversion algorithm to minimize those errors, respectively. In addition, we show that each stage achieves significant performance gains in a complementary manner. By evaluating on challenging datasets including CIFAR-10, CIFAR-100, and ImageNet, the proposed method demonstrates the state-of-the-art performance in terms of accuracy, latency, and energy preservation. Furthermore, our method is evaluated using a more challenging object detection task, revealing notable gains in regression performance under ultralow latency, when compared with existing spike-based detection algorithms. Codes will be available at: https://github.com/Windere/snn-cvt-dual-phase.},
	urldate = {2024-05-19},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Wang, Ziming and Zhang, Yuhao and Lian, Shuang and Cui, Xiaoxin and Yan, Rui and Tang, Huajin},
	year = {2023},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Artificial neural network (ANN)–spiking neural network (SNN) conversion, Firing, Membrane potentials, Neurons, Object detection, Optimization, Quantization (signal), SNN, Training, deep SNNs, neuromorphic computing, residual membrane potential, spike-based object detection, two-stage optimization, ��ANN2SNN},
	pages = {1--15},
}

@article{ding_accelerating_2021,
	title = {Accelerating {Training} of {Deep} {Spiking} {Neural} {Networks} with {Parameter} {Initialization}},
	abstract = {Despite that spiking neural networks (SNNs) show strong advantages in information encoding, power consuming, and computational capability, the underdevelopment of supervised learning algorithms is still a hindrance for training SNN. Our consideration is that proper weight initialization is a pivotal issue for efficient SNN training. It greatly influences gradient generating with the method of back-propagation through time at the initial training stage. Focusing on the properties of spiking neurons, we first derive the asymptotic formula of their response curve approximating the actual neuron response distribution. Then, we propose an initialization method obtained from the slant asymptote to overcome gradient vanishing. Finally, experiments with different coding schemes on classification tasks show that our method can effectively improve training speed and the final model accuracy compared with traditional deep learning initialization methods and existing SNN initialization methods. Further validation on different neuron types and training hyper-parameters has shown comparably good versatility and superiority over the other methods. Some suggestions are given to SNN training based on the analyses.},
	language = {en},
	urldate = {2024-05-19},
	author = {Ding, Jianhao and Zhang, Jiyuan and Yu, Zhaofei and Huang, Tiejun},
	month = oct,
	year = {2021},
}

@inproceedings{guo_reducing_2022,
	address = {Cham},
	title = {Reducing {Information} {Loss} for {Spiking} {Neural} {Networks}},
	isbn = {978-3-031-20083-0},
	doi = {10.1007/978-3-031-20083-0_3},
	abstract = {The Spiking Neural Network (SNN) has attracted more and more attention recently. It adopts binary spike signals to transmit information. Benefitting from the information passing paradigm of SNNs, the multiplications of activations and weights can be replaced by additions, which are more energy-efficient. However, its “Hard Reset" mechanism for the firing activity would ignore the difference among membrane potentials when the membrane potential is above the firing threshold, causing information loss. Meanwhile, quantifying the membrane potential to 0/1 spikes at the firing instants will inevitably introduce the quantization error thus bringing about information loss too. To address these problems, we propose to use the “Soft Reset" mechanism for the supervised training-based SNNs, which will drive the membrane potential to a dynamic reset potential according to its magnitude, and Membrane Potential Rectifier (MPR) to reduce the quantization error via redistributing the membrane potential to a range close to the spikes. Results show that the SNNs with the “Soft Reset" mechanism and MPR outperform their vanilla counterparts on both static and dynamic datasets.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Guo, Yufei and Chen, Yuanpei and Zhang, Liwen and Wang, YingLei and Liu, Xiaode and Tong, Xinyi and Ou, Yuanyuan and Huang, Xuhui and Ma, Zhe},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	keywords = {Information loss, Membrane potential rectificater, Quantization error, Soft reset, Spiking neural network},
	pages = {36--52},
}

@article{park_resting-potential-adjustable_2024,
	title = {Resting-potential-adjustable soft-reset integrate-and-fire neuron model for highly reliable and energy-efficient hardware-based spiking neural networks},
	volume = {590},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231224005332},
	doi = {10.1016/j.neucom.2024.127762},
	abstract = {In this study, the "resting-potential-adjustable soft-reset (RSR)" integrate-and-fire neuron model is proposed for higher reliability of hardware-based spiking neural networks (SNNs). Recently, researchers have attempted to implement neuronal models using hardware. However, previously proposed hard-reset (HR) and soft-reset (SR) neuron models are not suitable for hardware implementation for the following reasons. First, it is difficult to compensate for the neuron-threshold variation, which generally occurs in the fabrication processes. Second, they require a negative voltage supply that consumes substantial energy to implement the negative membrane potential (NMP). Third, for the HR and SR neuron models, maintaining the residual membrane potential (RMP) is challenging. The proposed RSR neuron model solves these challenges without any additional systems and algorithms and provides the following advantages: easy compensation for the neuron-threshold variation, significant energy reduction without using a negative voltage supply for the NMP, and precise preservation of the RMP. We discuss the behavior of the RSR neuron model and evaluate performance by comparing the inference results. Even with the neuron-threshold variation within the six-sigma range, the RSR neuron model restores the inference accuracy to the values of artificial neural networks (ANNs) for various datasets (CIFAR-10: 92.74\%, SVHN: 95.00\%, MNIST: 98.30\%). In the case of the CIFAR-10, the RSR neuron model consumes 32.80\% lower energy than the SR neuron model.},
	urldate = {2024-05-19},
	journal = {Neurocomputing},
	author = {Park, Kyungchul and Kim, Sungjoon and Oh, Min-Hye and Choi, Woo Young},
	month = jul,
	year = {2024},
	keywords = {Negative membrane potential, Neuron model, Residual membrane potential, Spiking neural network, Threshold variation compensation},
	pages = {127762},
}

@misc{noauthor_resting-potential-adjustable_nodate,
	title = {Resting-potential-adjustable soft-reset integrate-and-fire neuron model for highly reliable and energy-efficient hardware-based spiking neural networks - {ScienceDirect}},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231224005332},
	urldate = {2024-05-19},
	keywords = {/unread},
}

@article{zhang_low_2024,
	title = {Low {Latency} and {Sparse} {Computing} {Spiking} {Neural} {Networks} {With} {Self}-{Driven} {Adaptive} {Threshold} {Plasticity}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {2162-237X, 2162-2388},
	url = {https://ieeexplore.ieee.org/document/10219181/},
	doi = {10.1109/TNNLS.2023.3300514},
	urldate = {2024-05-19},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Zhang, Anguo and Shi, Jieming and Wu, Junyi and Zhou, Yongcheng and Yu, Wei},
	year = {2024},
	keywords = {/unread},
	pages = {1--12},
}

@article{bu_optimized_2022,
	title = {Optimized {Potential} {Initialization} for {Low}-{Latency} {Spiking} {Neural} {Networks}},
	volume = {36},
	copyright = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/19874},
	doi = {10.1609/aaai.v36i1.19874},
	abstract = {Spiking Neural Networks (SNNs) have been attached great importance due to the distinctive properties of low power consumption, biological plausibility, and adversarial robustness. The most effective way to train deep SNNs is through ANN-to-SNN conversion, which have yielded the best performance in deep network structure and large-scale datasets. However, there is a trade-off between accuracy and latency. In order to achieve high precision as original ANNs, a long simulation time is needed to match the firing rate of a spiking neuron with the activation value of an analog neuron, which impedes the practical application of SNN. In this paper, we aim to achieve high-performance converted SNNs with extremely low latency (fewer than 32 time-steps). We start by theoretically analyzing ANN-to-SNN conversion and show that scaling the thresholds does play a similar role as weight normalization. Instead of introducing constraints that facilitate ANN-to-SNN conversion at the cost of model capacity, we applied a more direct way by optimizing the initial membrane potential to reduce the conversion loss in each layer. Besides, we demonstrate that optimal initialization of membrane potentials can implement expected error-free ANN-to-SNN conversion. We evaluate our algorithm on the CIFAR-10 dataset and CIFAR-100 dataset and achieve state-of-the-art accuracy, using fewer time-steps. For example, we reach top-1 accuracy of 93.38\% on CIFAR-10 with 16 time-steps. Moreover, our method can be applied to other ANN-SNN conversion methodologies and remarkably promote performance when the time-steps is small.},
	language = {en},
	number = {1},
	urldate = {2024-05-19},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Bu, Tong and Ding, Jianhao and Yu, Zhaofei and Huang, Tiejun},
	month = jun,
	year = {2022},
	note = {Number: 1},
	keywords = {Cognitive Modeling \& Cognitive Systems (CMS)},
	pages = {11--20},
}

@incollection{mallot_fourier_2013,
	address = {Heidelberg},
	title = {Fourier {Analysis} for {Neuroscientists}},
	isbn = {978-3-319-00861-5},
	url = {https://doi.org/10.1007/978-3-319-00861-5_3},
	abstract = {In this Chapter, we introduce a piece of mathematical theory that is of importance in many different fields of theoretical neurobiology, and, indeed, for scientific computing in general. It is included here not so much because it is a genuine part of computational neuroscience, but because computational and systems neuroscience make extensive use of it. It is closely related to systems theory as introduced in the previous chapter but is also useful in the analysis of local field potentials, EEGs or other brain scanning data, in the generation of psychophysical stimuli in computational vision and of course in analyzing the auditory system. After some instructive examples, the major results of Fourier theory will be addressed in two steps:},
	language = {en},
	urldate = {2024-05-13},
	booktitle = {Computational {Neuroscience}: {A} {First} {Course}},
	publisher = {Springer International Publishing},
	author = {Mallot, Hanspeter A.},
	editor = {Mallot, Hanspeter A},
	year = {2013},
	doi = {10.1007/978-3-319-00861-5_3},
	keywords = {Convolution Theorem, Fourier Series, Fourier Theory, Modulation Transfer Function, Spatial Frequency, 理论},
	pages = {57--81},
}

@article{ochs_is_1979,
	title = {Is {Fourier} analysis performed by the visual system or by the visual investigator},
	volume = {69},
	issn = {0030-3941},
	doi = {10.1364/josa.69.000095},
	abstract = {A numerical Fourier transform was made of the pincushion grid illusion and the spectral components orthogonal to the illusory lines were isolated. Their inverse transform creates a picture of the illusion. The spatial-frequency response of cortical, simple receptive field neurons similarly filters the grid. A complete set of these neurons thus approximates a two-dimensional Fourier analyzer. One cannot conclude, however, that the brain actually uses frequency-domain information to interpret visual images.},
	language = {eng},
	number = {1},
	journal = {Journal of the Optical Society of America},
	author = {Ochs, A. L.},
	month = jan,
	year = {1979},
	pmid = {110914},
	keywords = {Animals, Fourier Analysis, Haplorhini, Humans, Optical Illusions, Visual Cortex, Visual Perception},
	pages = {95--98},
}

@article{blakemore_perceived_1970,
	title = {The perceived spatial frequency shift: evidence for frequency-selective neurones in the human brain},
	volume = {210},
	copyright = {© 1970 The Physiological Society},
	issn = {1469-7793},
	shorttitle = {The perceived spatial frequency shift},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1113/jphysiol.1970.sp009238},
	doi = {10.1113/jphysiol.1970.sp009238},
	abstract = {1. Prolonged observation of a high-contrast grating pattern causes an apparent shift in the spatial frequency of gratings subsequently viewed with the same retinal region. Gratings of higher and lower frequency than the adapting pattern seem, respectively, higher and lower than in fact they are. 2. There is no significant after-effect at the adapting frequency itself nor at frequencies more than two octaves away. 3. For very low adapting frequencies, the after-effect remains centred at about 3·0 c/deg and declines in strength as the adapting frequency is successively lowered. 4. The magnitude of the after-effect increases with the contrast of the adapting grating and the length of time spent in adaptation. It takes several hours to recover completely from 30 min adaptation. 5. The phenomenon is orientation-specific: a horizontal adapting grating has no effect on vertical test gratings. There is partial interocular transfer of the after-effect. 6. These findings provide further evidence that the visual system of man, like those of the cat and the monkey, contains neurones selectively sensitive to the orientation and dimensions of retinal images, and that these adaptable cells are actually involved in the encoding and perception of the size of simple patterns.},
	language = {en},
	number = {3},
	urldate = {2024-05-18},
	journal = {The Journal of Physiology},
	author = {Blakemore, Colin and Nachmias, Jacob and Sutton, Peter},
	year = {1970},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1113/jphysiol.1970.sp009238},
	pages = {727--750},
}

@article{sannita_stimulus-specific_2000,
	title = {Stimulus-specific oscillatory responses of the brain: a time/frequency-related coding process},
	volume = {111},
	issn = {1388-2457},
	shorttitle = {Stimulus-specific oscillatory responses of the brain},
	url = {https://www.sciencedirect.com/science/article/pii/S1388245799002710},
	doi = {10.1016/S1388-2457(99)00271-0},
	abstract = {Objectives: To review the coherent, rhythmic oscillations above ∼20 Hz that occur in response to sensory inputs in the firing rate and membrane or local field potentials of distributed neuron aggregates of CNS layered structures. Results: Oscillatory activity at approximately 20–80 Hz occurs in response to either olfactory, auditory and visual (contrast) stimuli; oscillations at frequencies centered on 100–120 Hz or 600 Hz are recorded, respectively, from the visual system (luminance stimulation) and from the somatosensory cortex. Experimental evidence suggests sources/mechanisms of generation that depend on inhibitory interneurons and pyramidal cells and are partially independent from those of conventional (broadband) evoked responses. In the olfactory and visual systems, the oscillatory responses reflect the global stimulus properties. A time/phase correlation between firing rate, spiking coincidence and oscillatory field responses has been documented. The oscillatory responses are postsynaptic both in cortex and in precortical structures (e.g. retina; LGN). Evidence indicates intracortical and thalamocortical interacting mechanisms of regulation as well as GABAergic and cholinergic modulation. In the visual cortex the oscillatory responses are driven by oscillations in the synaptic input. Oscillatory potentials are dependent on resonance phenomena and produce narrow-band synchronization of activated neurons. They may have a role in the ‘binding’ of separate neuronal aggregates into sensory units. Conclusions: Oscillatory responses contribute as a time/frequency coding mechanism to pacing neurons selectively for the physical properties of stimulus and are involved in sensory information processing.},
	number = {4},
	urldate = {2024-05-18},
	journal = {Clinical Neurophysiology},
	author = {Sannita, Walter G},
	month = apr,
	year = {2000},
	keywords = {Brain, Coding process, Oscillatory responses},
	pages = {565--583},
}

@article{lioi_gradients_2021,
	title = {Gradients of connectivity as graph {Fourier} bases of brain activity},
	volume = {5},
	issn = {2472-1751},
	url = {https://doi.org/10.1162/netn_a_00183},
	doi = {10.1162/netn_a_00183},
	abstract = {The application of graph theory to model the complex structure and function of
the brain has shed new light on its organization, prompting the emergence of
network neuroscience. Despite the tremendous progress that has been achieved in
this field, still relatively few methods exploit the topology of brain networks
to analyze brain activity. Recent attempts in this direction have leveraged on
the one hand graph spectral analysis (to decompose brain connectivity into
eigenmodes or gradients) and the other graph signal processing (to decompose
brain activity “coupled to” an underlying network in graph Fourier
modes). These studies have used a variety of imaging techniques (e.g., fMRI,
electroencephalography, diffusion-weighted and myelin-sensitive imaging) and
connectivity estimators to model brain networks. Results are promising in terms
of interpretability and functional relevance, but methodologies and terminology
are variable. The goals of this paper are twofold. First, we summarize recent
contributions related to connectivity gradients and graph signal processing, and
attempt a clarification of the terminology and methods used in the field, while
pointing out current methodological limitations. Second, we discuss the
perspective that the functional relevance of connectivity gradients could be
fruitfully exploited by considering them as graph Fourier bases of brain
activity.},
	number = {2},
	urldate = {2024-05-18},
	journal = {Network Neuroscience},
	author = {Lioi, Giulia and Gripon, Vincent and Brahim, Abdelbasset and Rousseau, François and Farrugia, Nicolas},
	month = may,
	year = {2021},
	pages = {322--336},
}

@article{uteuliyeva_fourier_2020,
	title = {Fourier neural networks: {A} comparative study},
	volume = {24},
	issn = {1088-467X},
	shorttitle = {Fourier neural networks},
	url = {https://content.iospress.com/articles/intelligent-data-analysis/ida195050},
	doi = {10.3233/IDA-195050},
	abstract = {We review neural network architectures which were motivated by Fourier series and integrals and which are referred to as Fourier neural networks. These networks are empirically evaluated in synthetic and real-world tasks. Neither of them outperforms},
	language = {en},
	number = {5},
	urldate = {2024-05-18},
	journal = {Intelligent Data Analysis},
	author = {Uteuliyeva, Malika and Zhumekenov, Abylay and Takhanov, Rustem and Assylbekov, Zhenisbek and Castro, Alejandro J. and Kabdolov, Olzhas},
	month = jan,
	year = {2020},
	note = {Publisher: IOS Press},
	pages = {1107--1120},
}

@article{roudier_low_1985,
	title = {Low contrast digital filter processing with possibility of structural analysis by segmentation in connexe domains},
	volume = {16},
	issn = {0150-536X},
	url = {https://iopscience.iop.org/article/10.1088/0150-536X/16/3/001},
	doi = {10.1088/0150-536X/16/3/001},
	language = {fr},
	number = {3},
	urldate = {2024-05-18},
	journal = {Journal of Optics},
	author = {Roudier, T and Coupinot, G and Hecquet, J and Muller, R},
	month = may,
	year = {1985},
	pages = {107--113},
}

@article{merkulev_effects_2009,
	title = {Effects of {Amplitude}-{Frequency} {Characteristics} of a {Noise}-{Masked} {Test} {Stimulus} on the {Shapes} of {Visual} {Evoked} {Potentials}},
	volume = {39},
	issn = {1573-899X},
	url = {https://doi.org/10.1007/s11055-009-9178-7},
	doi = {10.1007/s11055-009-9178-7},
	abstract = {Visual evoked potentials produced in response to a reversive checkerboard pattern presented in conditions of additive noise were recorded. Changes induced by noise in both the shapes of evoked potentials and the structure of the test stimulus were compared. The nature of changes in the shapes of evoked potentials was found to correlate with the nature of changes in the amplitude-frequency spectrum of the stimulus. These results support the gestalt psychology point of view that the visual system uses spatial frequency rather than discrete means for describing information.},
	language = {en},
	number = {7},
	urldate = {2024-05-18},
	journal = {Neuroscience and Behavioral Physiology},
	author = {Merkul’ev, A. V. and Merkul’eva, N. S.},
	month = sep,
	year = {2009},
	keywords = {gestalt psychology, spatial frequency spectrum, visual evoked potentials},
	pages = {683--694},
}

@misc{noauthor_perceived_nodate,
	title = {The perceived spatial frequency shift: evidence for frequency‐selective neurones in the human brain - {Blakemore} - 1970 - {The} {Journal} of {Physiology} - {Wiley} {Online} {Library}},
	url = {https://physoc.onlinelibrary.wiley.com/doi/abs/10.1113/jphysiol.1970.sp009238},
	urldate = {2024-05-18},
	keywords = {/unread},
}

@misc{noauthor_perceived_nodate-1,
	title = {The perceived spatial frequency shift: evidence for frequency‐selective neurones in the human brain},
	url = {https://physoc.onlinelibrary.wiley.com/doi/epdf/10.1113/jphysiol.1970.sp009238},
	urldate = {2024-05-18},
	keywords = {/unread},
}

@article{mcconnell_effective_2012,
	title = {Effective {Deep} {Brain} {Stimulation} {Suppresses} {Low}-{Frequency} {Network} {Oscillations} in the {Basal} {Ganglia} by {Regularizing} {Neural} {Firing} {Patterns}},
	volume = {32},
	copyright = {https://creativecommons.org/licenses/by-nc-sa/4.0/},
	issn = {0270-6474, 1529-2401},
	url = {https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.2824-12.2012},
	doi = {10.1523/JNEUROSCI.2824-12.2012},
	abstract = {Deep brain stimulation (DBS) of the subthalamic nucleus (STN) is an effective treatment for the motor symptoms of Parkinson's disease (PD). The effects of DBS depend strongly on stimulation frequency: high frequencies ({\textgreater}90 Hz) improve motor symptoms, while low frequencies ({\textless}50 Hz) are either ineffective or exacerbate symptoms. The neuronal basis for these frequency-dependent effects of DBS is unclear. The effects of different frequencies of STN-DBS on behavior and single-unit neuronal activity in the basal ganglia were studied in the unilateral 6-hydroxydopamine lesioned rat model of PD. Only high-frequency DBS reversed motor symptoms, and the effectiveness of DBS depended strongly on stimulation frequency in a manner reminiscent of its clinical effects in persons with PD. Quantification of single-unit activity in the globus pallidus externa (GPe) and substantia nigra reticulata (SNr) revealed that high-frequency DBS, but not low-frequency DBS, reduced pathological low-frequency oscillations (∼9 Hz) and entrained neurons to fire at the stimulation frequency. Similarly, the coherence between simultaneously recorded pairs of neurons within and across GPe and SNr shifted from the pathological low-frequency band to the stimulation frequency during high-frequency DBS, but not during low-frequency DBS. The changes in firing patterns in basal ganglia neurons were not correlated with changes in firing rate. These results indicate that high-frequency DBS is more effective than low-frequency DBS, not as a result of changes in firing rate, but rather due to its ability to replace pathological low-frequency network oscillations with a regularized pattern of neuronal firing.},
	language = {en},
	number = {45},
	urldate = {2024-05-18},
	journal = {The Journal of Neuroscience},
	author = {McConnell, George C. and So, Rosa Q. and Hilliard, Justin D. and Lopomo, Paola and Grill, Warren M.},
	month = nov,
	year = {2012},
	keywords = {/unread},
	pages = {15657--15668},
}

@misc{noauthor_effective_nodate,
	title = {Effective {Deep} {Brain} {Stimulation} {Suppresses} {Low}-{Frequency} {Network} {Oscillations} in the {Basal} {Ganglia} by {Regularizing} {Neural} {Firing} {Patterns} {\textbar} {Journal} of {Neuroscience}},
	url = {https://www.jneurosci.org/content/32/45/15657.short},
	urldate = {2024-05-18},
	keywords = {/unread},
}

@article{sannita_stimulus-specific_2000-1,
	title = {Stimulus-specific oscillatory responses of the brain: a time/frequency-related coding process},
	volume = {111},
	issn = {1388-2457},
	shorttitle = {Stimulus-specific oscillatory responses of the brain},
	url = {https://www.sciencedirect.com/science/article/pii/S1388245799002710},
	doi = {10.1016/S1388-2457(99)00271-0},
	abstract = {Objectives: To review the coherent, rhythmic oscillations above ∼20 Hz that occur in response to sensory inputs in the firing rate and membrane or local field potentials of distributed neuron aggregates of CNS layered structures. Results: Oscillatory activity at approximately 20–80 Hz occurs in response to either olfactory, auditory and visual (contrast) stimuli; oscillations at frequencies centered on 100–120 Hz or 600 Hz are recorded, respectively, from the visual system (luminance stimulation) and from the somatosensory cortex. Experimental evidence suggests sources/mechanisms of generation that depend on inhibitory interneurons and pyramidal cells and are partially independent from those of conventional (broadband) evoked responses. In the olfactory and visual systems, the oscillatory responses reflect the global stimulus properties. A time/phase correlation between firing rate, spiking coincidence and oscillatory field responses has been documented. The oscillatory responses are postsynaptic both in cortex and in precortical structures (e.g. retina; LGN). Evidence indicates intracortical and thalamocortical interacting mechanisms of regulation as well as GABAergic and cholinergic modulation. In the visual cortex the oscillatory responses are driven by oscillations in the synaptic input. Oscillatory potentials are dependent on resonance phenomena and produce narrow-band synchronization of activated neurons. They may have a role in the ‘binding’ of separate neuronal aggregates into sensory units. Conclusions: Oscillatory responses contribute as a time/frequency coding mechanism to pacing neurons selectively for the physical properties of stimulus and are involved in sensory information processing.},
	number = {4},
	urldate = {2024-05-18},
	journal = {Clinical Neurophysiology},
	author = {Sannita, Walter G},
	month = apr,
	year = {2000},
	keywords = {Brain, Coding process, Oscillatory responses},
	pages = {565--583},
}

@article{perez_-line_2023,
	title = {On-line learning applied to spiking neural network for antilock braking systems},
	volume = {559},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231223009074},
	doi = {10.1016/j.neucom.2023.126784},
	abstract = {Computationally replicating the behaviour of the cerebral cortex to perform the control tasks of daily life in a human being is a challenge today. First, it is necessary to know the structure and connections between the elements of the neural network that perform movement control. Next, a mathematical neural model that adequately resembles biological neurons has to be developed. Finally, a suitable learning model that allows adapting neural network response to changing conditions in the environment is also required. Spiking Neural Networks (SNN) are currently the closest approximation to biological neural networks. SNNs make use of temporal spike trains to deal with inputs and outputs, thus allowing a faster and more complex computation. In this paper, a controller based on an SNN is proposed to perform the control of an anti-lock braking system (ABS) in vehicles. To this end, two neural networks are used to regulate the braking force. The first one is devoted to estimating the optimal slip while the second one is in charge of setting the optimal braking pressure. The latter resembles biological reflex arcs to ensure stability during operation. This neural structure is used to control the fast regulation cycles that occur during ABS operation. Furthermore, an algorithm has been developed to train the network while driving. On-line learning is proposed to update the response of the controller. Hence, to cope with real conditions, a control algorithm based on neural networks that learn by making use of neural plasticity, similar to what occurs in biological systems, has been implemented. Neural connections are modulated using Spike-Timing-Dependent Plasticity (STDP) by means of a supervised learning structure using the slip error as input. Road-type detection has been included in the same neural structure. To validate and to evaluate the performance of the proposed algorithm, simulations as well as experiments in a real vehicle were carried out. The algorithm proved to be able to adapt to changes in adhesion conditions rapidly. This way, the capability of spiking neural networks to perform the full control logic of the ABS has been verified.},
	urldate = {2024-05-18},
	journal = {Neurocomputing},
	author = {Pérez, Javier and Alcázar, Manuel and Sánchez, Ignacio and Cabrera, Juan A. and Nybacka, Mikael and Castillo, Juan J.},
	month = nov,
	year = {2023},
	keywords = {Antilock brake system, On-line learning, Spiking neural network, Supervised learning, Vehicle dynamics, Vehicle safety},
	pages = {126784},
}

@article{stevens_neural_1995,
	title = {Neural {Coding}: {The} enigma of the brain},
	volume = {5},
	issn = {0960-9822},
	shorttitle = {Neural {Coding}},
	url = {https://www.cell.com/current-biology/abstract/S0960-9822(95)00273-9},
	doi = {10.1016/S0960-9822(95)00273-9},
	language = {English},
	number = {12},
	urldate = {2024-05-18},
	journal = {Current Biology},
	author = {Stevens, Charles F. and Zador, Anthony},
	month = dec,
	year = {1995},
	pmid = {8749388},
	note = {Publisher: Elsevier},
	pages = {1370--1371},
}

@article{mcconnell_effective_2012-1,
	title = {Effective {Deep} {Brain} {Stimulation} {Suppresses} {Low}-{Frequency} {Network} {Oscillations} in the {Basal} {Ganglia} by {Regularizing} {Neural} {Firing} {Patterns}},
	volume = {32},
	copyright = {Copyright © 2012 the authors 0270-6474/12/3215657-12\$15.00/0},
	issn = {0270-6474, 1529-2401},
	url = {https://www.jneurosci.org/content/32/45/15657},
	doi = {10.1523/JNEUROSCI.2824-12.2012},
	abstract = {Deep brain stimulation (DBS) of the subthalamic nucleus (STN) is an effective treatment for the motor symptoms of Parkinson's disease (PD). The effects of DBS depend strongly on stimulation frequency: high frequencies ({\textgreater}90 Hz) improve motor symptoms, while low frequencies ({\textless}50 Hz) are either ineffective or exacerbate symptoms. The neuronal basis for these frequency-dependent effects of DBS is unclear. The effects of different frequencies of STN-DBS on behavior and single-unit neuronal activity in the basal ganglia were studied in the unilateral 6-hydroxydopamine lesioned rat model of PD. Only high-frequency DBS reversed motor symptoms, and the effectiveness of DBS depended strongly on stimulation frequency in a manner reminiscent of its clinical effects in persons with PD. Quantification of single-unit activity in the globus pallidus externa (GPe) and substantia nigra reticulata (SNr) revealed that high-frequency DBS, but not low-frequency DBS, reduced pathological low-frequency oscillations (∼9 Hz) and entrained neurons to fire at the stimulation frequency. Similarly, the coherence between simultaneously recorded pairs of neurons within and across GPe and SNr shifted from the pathological low-frequency band to the stimulation frequency during high-frequency DBS, but not during low-frequency DBS. The changes in firing patterns in basal ganglia neurons were not correlated with changes in firing rate. These results indicate that high-frequency DBS is more effective than low-frequency DBS, not as a result of changes in firing rate, but rather due to its ability to replace pathological low-frequency network oscillations with a regularized pattern of neuronal firing.},
	language = {en},
	number = {45},
	urldate = {2024-05-18},
	journal = {Journal of Neuroscience},
	author = {McConnell, George C. and So, Rosa Q. and Hilliard, Justin D. and Lopomo, Paola and Grill, Warren M.},
	month = nov,
	year = {2012},
	pmid = {23136407},
	note = {Publisher: Society for Neuroscience
Section: Articles},
	pages = {15657--15668},
}

@inproceedings{liu_domain_2023,
	title = {Domain {Agnostic} {Fourier} {Neural} {Operators}},
	url = {https://openreview.net/forum?id=ubap5FKbJs},
	abstract = {Fourier neural operators (FNOs) can learn highly nonlinear mappings between function spaces, and have recently become a popular tool for learning responses of complex physical systems. However, to achieve good accuracy and efficiency, FNOs rely on the Fast Fourier transform (FFT), which is restricted to modeling problems on rectangular domains. To lift such a restriction and permit FFT on irregular geometries as well as topology changes, we introduce domain agnostic Fourier neural operator (DAFNO), a novel neural operator architecture for learning surrogates with irregular geometries and evolving domains. The key idea is to incorporate a smoothed characteristic function in the integral layer architecture of FNOs, and leverage FFT to achieve rapid computations, in such a way that the geometric information is explicitly encoded in the architecture. In our empirical evaluation, DAFNO has achieved state-of-the-art accuracy as compared to baseline neural operator models on two benchmark datasets of material modeling and airfoil simulation. To further demonstrate the capability and generalizability of DAFNO in handling complex domains with topology changes, we consider a brittle material fracture evolution problem. With only one training crack simulation sample, DAFNO has achieved generalizability to unseen loading scenarios and substantially different crack patterns from the trained scenario. Our code and data accompanying this paper are available at https://github.com/ningliu-iga/DAFNO.},
	language = {en},
	urldate = {2024-05-18},
	author = {Liu, Ning and Jafarzadeh, Siavash and Yu, Yue},
	month = nov,
	year = {2023},
}

@article{liu_domain_2023-1,
	title = {Domain {Agnostic} {Fourier} {Neural} {Operators}},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/940a7634dab556b67af15bacd337f7db-Abstract-Conference.html},
	language = {en},
	urldate = {2024-05-18},
	journal = {Proceedings of NeurIPS},
	author = {Liu, Ning and Jafarzadeh, Siavash and Yu, Yue},
	month = dec,
	year = {2023},
	pages = {47438--47450},
}

@article{liu_domain_2023-2,
	title = {Domain {Agnostic} {Fourier} {Neural} {Operators}},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/940a7634dab556b67af15bacd337f7db-Abstract-Conference.html},
	language = {en},
	urldate = {2024-05-18},
	journal = {Proceedings of NeurIPS},
	author = {Liu, Ning and Jafarzadeh, Siavash and Yu, Yue},
	month = dec,
	year = {2023},
	keywords = {/unread},
	pages = {47438--47450},
}

@article{banks_development_1985,
	title = {The development of basic mechanisms of pattern vision: {Spatial} frequency channels},
	volume = {40},
	issn = {0022-0965},
	shorttitle = {The development of basic mechanisms of pattern vision},
	url = {https://www.sciencedirect.com/science/article/pii/0022096585900803},
	doi = {10.1016/0022-0965(85)90080-3},
	abstract = {The mature visual system possesses mechanisms that analyze visual inputs into bands of spatial frequency. This analysis appears to be important to several visual capabilities. We have investigated the development of these spatial-frequency channels in young infants. Experiment 1 used a masking paradigm to test 6-week-olds, 12-week-olds, and adults. The detectability of sine wave gratings of different spatial frequencies was measured in the presence and the absence of a narrowband noise masker. The 12-week data showed that at least two spatial-frequency channels with adultlike specificity are present at 12 weeks. The 6-week data did not reveal the presence of narrowband spatial-frequency channels. Experiment 2 used a different paradigm to investigate the same issue. The detectability of gratings composed of two sine wave components was measured in 6-week-olds and adults. The results were entirely consistent with those of experiment 1. The 12-week and adult data indicated the presence of narrowband spatial-frequency channels. The 6-week data did not. The results of these experiments suggest that the manner in which pattern information is processed changes fundamentally between 6 and 12 weeks of age.},
	number = {3},
	urldate = {2024-05-18},
	journal = {Journal of Experimental Child Psychology},
	author = {Banks, Martin S. and Stephens, Benjamin R. and Hartmann, E. Eugenie},
	month = dec,
	year = {1985},
	keywords = {/unread},
	pages = {501--527},
}

@article{graham_spatial_1972,
	title = {Spatial frequency channels in the human visual system: {Effects} of luminance and pattern drift rate},
	volume = {12},
	issn = {0042-6989},
	shorttitle = {Spatial frequency channels in the human visual system},
	url = {https://www.sciencedirect.com/science/article/pii/004269897290137X},
	doi = {10.1016/0042-6989(72)90137-X},
	abstract = {Recent evidence indicates that the human visual system contains multiple channels, with each channel sensitive to a different narrow range of spatial frequency. In this study the sensitivity of these channels for patterns at low mean luminance or high drift rate is measured by the effect of adaptation to sinusoidal gratings on the contrast thresholds for sinusoidal gratings. The channels do not behave in the way expected from retinal ganglion cell physiology; rather, they remain selectively sensitive to narrow ranges of spatial frequency even when the luminance is low or the drift rate high.
Résumé
Des données récentes suggèrent plusieurs canaux de transmission dans le système visuel humain, chacun sensibleàuneétroite bande differente de fréquence spatiale. On mesure ici la sensibilitéde ces canaux pour des figuresàfaible luminance moyenne ouàvitesse rapide de dérive, enétuduant l'effet de l'adaptationàdes réseaux sinusoïdaux sur le seuil de contraste pour des réseaux sinusoïdaux. Ces canaux ne se comportent pas comme on pourrait s'y attendre d'après la physiologie des cellules ganglionnaires de la rétine; ils restent au contraire sélectivement sensiblesàdes domainesétroits de fréquence spatible meˆme si la luminance est faible ou la dérive rapide.
Zusammenfassung
Neue Erkenntnisse zeigen, daβ das visuelle System des Menschen vielfache Kanäle umfaβt, wobei jeder Kanal auf einen verschieden engen Bereich von Ortsfrequenzen anspricht. In dieser Arbeit wird die Empfindlichkeit dieser Kanäle für Muster bei niederer durchschnittlicher Beleuchtung oder schnelle Bewegung gemessen mittels Adaptation an sinusoidale Gitter an der Kontrastschwelle für sinusoidale Gitter. Die Kanäle verhalten sich nicht so, wie von der Physiologie der Ganglienzellen der Retina erwartet, vielmehr bleiben sie selektiv empfindlich auf einem engen Bereich von Ortsfrequenzen, sogar wenn die Beleuchtung niedrig ist oder die Bewegung schnell.
Pезюме
Пocлeдниe дaнныe pкaзыBaют нa тo, хтo зpнтeльнaя cиcтeмa coдepзит мдoгиe кaнaлы, пpи хeм кaздый из этих кaнaлoB хpBcтBитeлeн к paзлихным pзким диaпaэoнaм пpocтpaнcтBeннoй хacтoты. B пpeдлaгaeмoм иccлeдoBaнии хpBcтBитeльнocть этих кaнaлoB пo oтнoшeнию к пaттepнaм пpи cpeднeй низкoй яpкocти или пpи бoльшoй cкopocтипpeдьяBлeния (fast drift rate) былa измepeнa пo дeйcтBию aдaптaции к cинpcoидaлъным peшeткaм пo кoнтpacтным пopoгaм для cинpcoидaльных peщeтoк. Кaнaлы Beдpт ceбя нe тaк кaк этo мoзнo былo пpeдпoлaгaть пo дaнным физиoлoгии гaнглиoзных клeтoк ceтхaтки; cкopee, oни coхpaняют cBoю избиpaтeльнpю хpBcтBитeльнocть к pзким диaпaзoнaм пpocтpaнcтBeннoй хacтoты, лaзe B тoм cлpхae, ecли яpкocть низкaя или cкopocмб пpeдъяBлeния бoльшaя.},
	number = {1},
	urldate = {2024-05-18},
	journal = {Vision Research},
	author = {Graham, Norma},
	month = jan,
	year = {1972},
	keywords = {/unread},
	pages = {53--68},
}

@incollection{maffei_spatial_1978,
	address = {Berlin, Heidelberg},
	title = {Spatial {Frequency} {Channels}: {Neural} {Mechanisms}},
	isbn = {978-3-642-46354-9},
	shorttitle = {Spatial {Frequency} {Channels}},
	url = {https://doi.org/10.1007/978-3-642-46354-9_2},
	abstract = {When one is concerned with the investigation of spatial frequency channels in the visual system, one often has to face the use of periodical visual stimuli which are generated on television displays by means of the sophisticated techniques of modern electronics. However, many physiologists and psychologists have criticized periodical stimuli as being visually unnatural and maintain that our visual world is composed of more complicated stimuli such as bars and edges at various orientations. Admittedly, the experimental visual world utilized by visual scientists is very limited and most of the time it is seemingly irrelevant and uninteresting to the subjects of the experiment. Without a doubt, the familiarity of objects and the emotions evoked in recognizing them are essential factors underlying pattern perception. Numerous experiments, both old and new, indicate that the capabilities of our visual systems are molded, at least in part, by our previous experience of the external world.},
	language = {en},
	urldate = {2024-05-18},
	booktitle = {Perception},
	publisher = {Springer},
	author = {Maffei, L.},
	editor = {Anstis, S. M. and Atkinson, J. and Blakemore, C. and Braddick, O. and Brandt, T. and Campbell, F. W. and Coren, S. and Dichgans, J. and Dodwell, P. C. and Eimas, P. D. and Foley, J. M. and Fox, R. and Ganz, L. and Garrett, M. and Gibson, E. J. and Girgus, J. S. and Haith, M. M. and Hatwell, Y. and Hilgard, E. R. and Ingle, D. and Johansson, G. and Julesz, B. and Konishi, M. and Lackner, J. R. and Levinson, E. and Liberman, A. M. and Maffei, L. and Oyama, T. and Pantle, A. and Pöppel, E. and Sekuler, R. and Stromeyer, C. F. and Studdert-Kennedy, M. and Teuber, H.-L. and Yin, R. K. and Held, Richard and Leibowitz, Herschel W. and Teuber, Hans-Lukas},
	year = {1978},
	doi = {10.1007/978-3-642-46354-9_2},
	keywords = {/unread},
	pages = {39--66},
}

@article{nemes_multiple_2011,
	title = {Multiple spatial frequency channels in human visual perceptual memory},
	volume = {51},
	issn = {0042-6989},
	url = {https://www.sciencedirect.com/science/article/pii/S0042698911003233},
	doi = {10.1016/j.visres.2011.09.003},
	abstract = {Current models of short-term visual perceptual memory invoke mechanisms that are closely allied to low-level perceptual discrimination mechanisms. The purpose of this study was to investigate the extent to which human visual perceptual memory for spatial frequency is based upon multiple, spatially tuned channels similar to those found in the earliest stages of visual processing. To this end we measured how performance on a delayed spatial frequency discrimination paradigm was affected by the introduction of interfering or ‘memory masking’ stimuli of variable spatial frequency during the delay period. Masking stimuli were shown to induce shifts in the points of subjective equality (PSE) when their spatial frequencies were within a bandwidth of 1.2 octaves of the reference spatial frequency. When mask spatial frequencies differed by more than this value, there was no change in the PSE from baseline levels. This selective pattern of masking was observed for different spatial frequencies and demonstrates the existence of multiple, spatially tuned mechanisms in visual perceptual memory. Memory masking effects were also found to occur for horizontal separations of up to 6deg between the masking and test stimuli and lacked any orientation selectivity. These findings add further support to the view that low-level sensory processing mechanisms form the basis for the retention of spatial frequency information in perceptual memory. However, the broad range of transfer of memory masking effects across spatial location and other dimensions indicates more long range, long duration interactions between spatial frequency channels that are likely to rely contributions from neural processes located in higher visual areas.},
	number = {23},
	urldate = {2024-05-18},
	journal = {Vision Research},
	author = {Nemes, V. A. and Whitaker, D. and Heron, J. and McKeefry, D. J.},
	month = dec,
	year = {2011},
	keywords = {/unread, Short term perceptual memory, Spatial frequency},
	pages = {2331--2339},
}

@article{banks_development_1985-1,
	title = {The development of basic mechanisms of pattern vision: {Spatial} frequency channels},
	volume = {40},
	issn = {0022-0965},
	shorttitle = {The development of basic mechanisms of pattern vision},
	url = {https://www.sciencedirect.com/science/article/pii/0022096585900803},
	doi = {10.1016/0022-0965(85)90080-3},
	abstract = {The mature visual system possesses mechanisms that analyze visual inputs into bands of spatial frequency. This analysis appears to be important to several visual capabilities. We have investigated the development of these spatial-frequency channels in young infants. Experiment 1 used a masking paradigm to test 6-week-olds, 12-week-olds, and adults. The detectability of sine wave gratings of different spatial frequencies was measured in the presence and the absence of a narrowband noise masker. The 12-week data showed that at least two spatial-frequency channels with adultlike specificity are present at 12 weeks. The 6-week data did not reveal the presence of narrowband spatial-frequency channels. Experiment 2 used a different paradigm to investigate the same issue. The detectability of gratings composed of two sine wave components was measured in 6-week-olds and adults. The results were entirely consistent with those of experiment 1. The 12-week and adult data indicated the presence of narrowband spatial-frequency channels. The 6-week data did not. The results of these experiments suggest that the manner in which pattern information is processed changes fundamentally between 6 and 12 weeks of age.},
	number = {3},
	urldate = {2024-05-18},
	journal = {Journal of Experimental Child Psychology},
	author = {Banks, Martin S. and Stephens, Benjamin R. and Hartmann, E. Eugenie},
	month = dec,
	year = {1985},
	keywords = {/unread},
	pages = {501--527},
}

@article{graham_spatial_1972-1,
	title = {Spatial frequency channels in the human visual system: {Effects} of luminance and pattern drift rate},
	volume = {12},
	issn = {0042-6989},
	shorttitle = {Spatial frequency channels in the human visual system},
	url = {https://www.sciencedirect.com/science/article/pii/004269897290137X},
	doi = {10.1016/0042-6989(72)90137-X},
	abstract = {Recent evidence indicates that the human visual system contains multiple channels, with each channel sensitive to a different narrow range of spatial frequency. In this study the sensitivity of these channels for patterns at low mean luminance or high drift rate is measured by the effect of adaptation to sinusoidal gratings on the contrast thresholds for sinusoidal gratings. The channels do not behave in the way expected from retinal ganglion cell physiology; rather, they remain selectively sensitive to narrow ranges of spatial frequency even when the luminance is low or the drift rate high.
Résumé
Des données récentes suggèrent plusieurs canaux de transmission dans le système visuel humain, chacun sensibleàuneétroite bande differente de fréquence spatiale. On mesure ici la sensibilitéde ces canaux pour des figuresàfaible luminance moyenne ouàvitesse rapide de dérive, enétuduant l'effet de l'adaptationàdes réseaux sinusoïdaux sur le seuil de contraste pour des réseaux sinusoïdaux. Ces canaux ne se comportent pas comme on pourrait s'y attendre d'après la physiologie des cellules ganglionnaires de la rétine; ils restent au contraire sélectivement sensiblesàdes domainesétroits de fréquence spatible meˆme si la luminance est faible ou la dérive rapide.
Zusammenfassung
Neue Erkenntnisse zeigen, daβ das visuelle System des Menschen vielfache Kanäle umfaβt, wobei jeder Kanal auf einen verschieden engen Bereich von Ortsfrequenzen anspricht. In dieser Arbeit wird die Empfindlichkeit dieser Kanäle für Muster bei niederer durchschnittlicher Beleuchtung oder schnelle Bewegung gemessen mittels Adaptation an sinusoidale Gitter an der Kontrastschwelle für sinusoidale Gitter. Die Kanäle verhalten sich nicht so, wie von der Physiologie der Ganglienzellen der Retina erwartet, vielmehr bleiben sie selektiv empfindlich auf einem engen Bereich von Ortsfrequenzen, sogar wenn die Beleuchtung niedrig ist oder die Bewegung schnell.
Pезюме
Пocлeдниe дaнныe pкaзыBaют нa тo, хтo зpнтeльнaя cиcтeмa coдepзит мдoгиe кaнaлы, пpи хeм кaздый из этих кaнaлoB хpBcтBитeлeн к paзлихным pзким диaпaэoнaм пpocтpaнcтBeннoй хacтoты. B пpeдлaгaeмoм иccлeдoBaнии хpBcтBитeльнocть этих кaнaлoB пo oтнoшeнию к пaттepнaм пpи cpeднeй низкoй яpкocти или пpи бoльшoй cкopocтипpeдьяBлeния (fast drift rate) былa измepeнa пo дeйcтBию aдaптaции к cинpcoидaлъным peшeткaм пo кoнтpacтным пopoгaм для cинpcoидaльных peщeтoк. Кaнaлы Beдpт ceбя нe тaк кaк этo мoзнo былo пpeдпoлaгaть пo дaнным физиoлoгии гaнглиoзных клeтoк ceтхaтки; cкopee, oни coхpaняют cBoю избиpaтeльнpю хpBcтBитeльнocть к pзким диaпaзoнaм пpocтpaнcтBeннoй хacтoты, лaзe B тoм cлpхae, ecли яpкocть низкaя или cкopocмб пpeдъяBлeния бoльшaя.},
	number = {1},
	urldate = {2024-05-18},
	journal = {Vision Research},
	author = {Graham, Norma},
	month = jan,
	year = {1972},
	keywords = {/unread},
	pages = {53--68},
}

@article{peron_spike_2009,
	title = {Spike frequency adaptation mediates looming stimulus selectivity in a collision-detecting neuron},
	volume = {12},
	copyright = {2009 Springer Nature America, Inc.},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/nn.2259},
	doi = {10.1038/nn.2259},
	abstract = {Studying the mechanisms by which spike frequency adaptation shapes visual stimulus selectivity in the lobula giant movement detector interneuron of the locust visual system, the authors find that spike frequency adaptation selectively decreases this neuron's responses to nonpreferred stimuli.},
	language = {en},
	number = {3},
	urldate = {2024-05-13},
	journal = {Nature Neuroscience},
	author = {Peron, Simon and Gabbiani, Fabrizio},
	month = mar,
	year = {2009},
	note = {Publisher: Nature Publishing Group},
	keywords = {Animal Genetics and Genomics, Behavioral Sciences, Biological Techniques, Biomedicine, Neurobiology, Neurosciences, general},
	pages = {318--326},
}

@article{ha_spike_2017,
	title = {Spike {Frequency} {Adaptation} in {Neurons} of the {Central} {Nervous} {System}},
	volume = {26},
	issn = {1226-2560},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5597548/},
	doi = {10.5607/en.2017.26.4.179},
	abstract = {Neuronal firing patterns and frequencies determine the nature of encoded information of the neurons. Here we discuss the molecular identity and cellular mechanisms of spike-frequency adaptation in central nervous system (CNS) neurons. Calcium-activated potassium (KCa) channels such as BKCa and SKCa channels have long been known to be important mediators of spike adaptation via generation of a large afterhyperpolarization when neurons are hyper-activated. However, it has been shown that a strong hyperpolarization via these KCa channels would cease action potential generation rather than reducing the frequency of spike generation. In some types of neurons, the strong hyperpolarization is followed by oscillatory activity in these neurons. Recently, spike-frequency adaptation in thalamocortical (TC) and CA1 hippocampal neurons is shown to be mediated by the Ca2+-activated Cl- channel (CACC), anoctamin-2 (ANO2). Knockdown of ANO2 in these neurons results in significantly reduced spike-frequency adaptation accompanied by increased number of spikes without shifting the firing mode, which suggests that ANO2 mediates a genuine form of spike adaptation, finely tuning the frequency of spikes in these neurons. Based on the finding of a broad expression of this new class of CACC in the brain, it can be proposed that the ANO2-mediated spike-frequency adaptation may be a general mechanism to control information transmission in the CNS neurons.},
	number = {4},
	urldate = {2024-05-14},
	journal = {Experimental Neurobiology},
	author = {Ha, Go Eun and Cheong, Eunji},
	month = aug,
	year = {2017},
	pmid = {28912640},
	pmcid = {PMC5597548},
	pages = {179--185},
}

@inproceedings{qian_thinking_2020,
	address = {Cham},
	title = {Thinking in {Frequency}: {Face} {Forgery} {Detection} by {Mining} {Frequency}-{Aware} {Clues}},
	isbn = {978-3-030-58610-2},
	shorttitle = {F3-{Net}; {Thinking} in {Frequency}},
	doi = {10.1007/978-3-030-58610-2_6},
	abstract = {As realistic facial manipulation technologies have achieved remarkable progress, social concerns about potential malicious abuse of these technologies bring out an emerging research topic of face forgery detection. However, it is extremely challenging since recent advances are able to forge faces beyond the perception ability of human eyes, especially in compressed images and videos. We find that mining forgery patterns with the awareness of frequency could be a cure, as frequency provides a complementary viewpoint where either subtle forgery artifacts or compression errors could be well described. To introduce frequency into the face forgery detection, we propose a novel Frequency in Face Forgery Network (F\$\${\textasciicircum}3\$\$3-Net), taking advantages of two different but complementary frequency-aware clues, 1) frequency-aware decomposed image components, and 2) local frequency statistics, to deeply mine the forgery patterns via our two-stream collaborative learning framework. We apply DCT as the applied frequency-domain transformation. Through comprehensive studies, we show that the proposed F\$\${\textasciicircum}3\$\$3-Net significantly outperforms competing state-of-the-art methods on all compression qualities in the challenging FaceForensics++ dataset, especially wins a big lead upon low-quality media.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Qian, Yuyang and Yin, Guojun and Sheng, Lu and Chen, Zixuan and Shao, Jing},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	keywords = {Collaborative learning, Face forgery detection, Frequency},
	pages = {86--103},
}

@misc{noauthor_thinking_nodate,
	title = {Thinking in {Frequency}: {Face} {Forgery} {Detection} by {Mining} {Frequency}-{Aware} {Clues} {\textbar} {SpringerLink}},
	url = {https://link.springer.com/chapter/10.1007/978-3-030-58610-2_6},
	urldate = {2024-05-18},
	keywords = {/unread},
}

@article{brunel_effects_2001,
	title = {Effects of {Synaptic} {Noise} and {Filtering} on the {Frequency} {Response} of {Spiking} {Neurons}},
	volume = {86},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.86.2186},
	doi = {10.1103/PhysRevLett.86.2186},
	abstract = {Noise can have a significant impact on the response dynamics of a nonlinear system. For neurons, the primary source of noise comes from background synaptic input activity. If this is approximated as white noise, the amplitude of the modulation of the firing rate in response to an input current oscillating at frequency ω decreases as 1/√ω and lags the input by 45° in phase. However, if filtering due to realistic synaptic dynamics is included, the firing rate is modulated by a finite amount even in the limit ω→∞ and the phase lag is eliminated. Thus, through its effect on noise inputs, realistic synaptic dynamics can ensure unlagged neuronal responses to high-frequency inputs.},
	number = {10},
	urldate = {2024-05-14},
	journal = {Physical Review Letters},
	author = {Brunel, Nicolas and Chance, Frances S. and Fourcaud, Nicolas and Abbott, L. F.},
	month = mar,
	year = {2001},
	note = {Publisher: American Physical Society},
	pages = {2186--2189},
}

@misc{noauthor_documentation_2024,
	title = {Documentation: {Cell} {Types} {Database} - {How} {To} / {Allen} {Cell} {Types} {Database}},
	shorttitle = {Documentation},
	url = {https://community.brain-map.org/t/documentation-cell-types-database/2845},
	abstract = {Document Description     Overview Overview of methods for the Cell Types database   Transcriptomics Overview Description of methods for single cell RNA isolation and sequence analysis   Electrophysiology Overview Description of methods for tissue processing, electrophysiology data acquisition, and analysis of intrinsic properties   Morphology and Histology Overview Description of methods for histological staining, image acquisition, 3D reconstruction and structure-based categorization   Neur...},
	language = {en},
	urldate = {2024-05-18},
	journal = {Allen Brain Map Community Forum},
	month = jan,
	year = {2024},
	note = {Section: How To},
}

@article{dimitrov_information_2011,
	title = {Information theory in neuroscience},
	volume = {30},
	issn = {1573-6873},
	url = {https://doi.org/10.1007/s10827-011-0314-3},
	doi = {10.1007/s10827-011-0314-3},
	language = {en},
	number = {1},
	urldate = {2024-05-13},
	journal = {Journal of Computational Neuroscience},
	author = {Dimitrov, Alexander G. and Lazar, Aurel A. and Victor, Jonathan D.},
	month = feb,
	year = {2011},
	pages = {1--5},
}

@inproceedings{ehrlich_deep_2019,
	title = {Deep {Residual} {Learning} in the {JPEG} {Transform} {Domain}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Ehrlich_Deep_Residual_Learning_in_the_JPEG_Transform_Domain_ICCV_2019_paper.html},
	urldate = {2024-05-16},
	author = {Ehrlich, Max and Davis, Larry S.},
	year = {2019},
	pages = {3484--3493},
}

@article{fries_mechanism_2005,
	title = {A mechanism for cognitive dynamics: neuronal communication through neuronal coherence},
	volume = {9},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {13646613},
	shorttitle = {A mechanism for cognitive dynamics},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661305002421},
	doi = {10.1016/j.tics.2005.08.011},
	language = {en},
	number = {10},
	urldate = {2024-05-13},
	journal = {Trends in Cognitive Sciences},
	author = {Fries, Pascal},
	month = oct,
	year = {2005},
	pages = {474--480},
}

@misc{noauthor_compressing_nodate,
	title = {Compressing {Convolutional} {Neural} {Networks} in the {Frequency} {Domain} {\textbar} {Proceedings} of the 22nd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	url = {https://dl.acm.org/doi/abs/10.1145/2939672.2939839},
	urldate = {2024-05-16},
	keywords = {/unread},
}

@article{chen_improving_2023,
	title = {Improving {Spiking} {Neural} {Network} {With} {Frequency} {Adaptation} for {Image} {Classification}},
	issn = {2379-8939},
	url = {https://ieeexplore.ieee.org/abstract/document/10229179},
	doi = {10.1109/TCDS.2023.3308347},
	abstract = {Spiking neural networks (SNNs) are promising in energy-efficient brain-inspired devices for their rich spatio-temporal dynamics, bio-plausible encoding, and event-driven information processing. However, the existing SNNs for image classification have fixed firing thresholds for the neurons and do not consider the adaptive properties of the neurons. In this paper, we propose a high-performance spiking neural network composed of neurons with spike frequency adaptation (SFA-SNN). We replace the fixed firing threshold with dynamic firing thresholds and incorporate them into the differential equation of neuron membrane potential, and then build an SNN on Pytorch. In addition, we introduce a new function to approximate the derivative of spike activity to solve its non-differentiable problem, so that the SNNs can be trained in spatio-temporal domain using the error backpropagation algorithm. We verify the image classification performance of the proposed SFA-SNN on the static dataset (including MNIST and Fashion-MNIST) and neuromorphic dataset (including CIFAR10-DVS and DVS128-Gesture), and the accuracy results including 99.52\% on MNIST, 92.40\% on Fashion-MNIST, 71.90\% on CIFAR10-DVS, and 96.67\% on DVS128-Gesture. We believe this work can help us better understand the intelligent information processing of the brain.},
	urldate = {2024-05-15},
	journal = {IEEE Transactions on Cognitive and Developmental Systems},
	author = {Chen, Tao and Wang, Lidan and Li, Jie and Duan, Shukai and Huang, Tingwen},
	year = {2023},
	note = {Conference Name: IEEE Transactions on Cognitive and Developmental Systems},
	keywords = {Adaptation models, Biological system modeling, Biology, Mathematical models, Membrane potentials, Neurons, Spiking neural networks, Training, dynamic firing threshold, spike frequency adaptation, supervised learning},
	pages = {1--1},
}

@article{auge_survey_2021,
	title = {A {Survey} of {Encoding} {Techniques} for {Signal} {Processing} in {Spiking} {Neural} {Networks}},
	volume = {53},
	issn = {1573-773X},
	url = {https://doi.org/10.1007/s11063-021-10562-2},
	doi = {10.1007/s11063-021-10562-2},
	abstract = {Biologically inspired spiking neural networks are increasingly popular in the field of artificial intelligence due to their ability to solve complex problems while being power efficient. They do so by leveraging the timing of discrete spikes as main information carrier. Though, industrial applications are still lacking, partially because the question of how to encode incoming data into discrete spike events cannot be uniformly answered. In this paper, we summarise the signal encoding schemes presented in the literature and propose a uniform nomenclature to prevent the vague usage of ambiguous definitions. Therefore we survey both, the theoretical foundations as well as applications of the encoding schemes. This work provides a foundation in spiking signal encoding and gives an overview over different application-oriented implementations which utilise the schemes.},
	language = {en},
	number = {6},
	urldate = {2024-05-15},
	journal = {Neural Processing Letters},
	author = {Auge, Daniel and Hille, Julian and Mueller, Etienne and Knoll, Alois},
	month = dec,
	year = {2021},
	keywords = {Neural coding, Neuromorphic computing, Rate coding, Spiking neural networks, Temporal coding},
	pages = {4693--4710},
}

@incollection{georgeson_spatial_1979,
	title = {Spatial {Fourier} {Analysis} and {Human} {Vision}},
	isbn = {978-1-315-80276-3},
	abstract = {Spatial Fourier Analysis and Human Vision - 1},
	booktitle = {Tutorial {Essays} in {Psychology}},
	publisher = {Psychology Press},
	author = {Georgeson, Mark},
	year = {1979},
	note = {Num Pages: 50},
	keywords = {/unread},
}

@inproceedings{wang_high-frequency_2020,
	title = {High-{Frequency} {Component} {Helps} {Explain} the {Generalization} of {Convolutional} {Neural} {Networks}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Wang_High-Frequency_Component_Helps_Explain_the_Generalization_of_Convolutional_Neural_Networks_CVPR_2020_paper.html},
	urldate = {2024-05-13},
	author = {Wang, Haohan and Wu, Xindi and Huang, Zeyi and Xing, Eric P.},
	year = {2020},
	pages = {8684--8694},
}

@article{harris_fourier_1998,
	title = {The {Fourier} analysis of biological transients},
	volume = {83},
	issn = {0165-0270},
	url = {https://www.sciencedirect.com/science/article/pii/S0165027098000806},
	doi = {10.1016/S0165-0270(98)00080-6},
	abstract = {With modern computing technology the digital implementation of the Fourier transform is widely available, mostly in the form of the fast Fourier transform (FFT). Although the FFT has become almost synonymous with the Fourier transform, it is a fast numerical technique for computing the discrete Fourier transform (DFT) of a finite sequence of sampled data. The DFT is not directly equivalent to the continuous Fourier transform of the underlying biological signal, which becomes important when analyzing biological transients. Although this distinction is well known by some, for many it leads to confusion in how to interpret the FFT of biological data, and in how to precondition data so as to yield a more accurate Fourier transform using the FFT. We review here the fundamentals of Fourier analysis with emphasis on the analysis of transient signals. As an example of a transient, we consider the human saccade to illustrate the pitfalls and advantages of various Fourier analyses.},
	number = {1},
	urldate = {2024-05-13},
	journal = {Journal of Neuroscience Methods},
	author = {Harris, Christopher M.},
	month = aug,
	year = {1998},
	keywords = {/unread, Biological transients, Discrete Fourier transform, Fast Fourier transform},
	pages = {15--34},
}

@article{yin_accurate_2021,
	title = {Accurate and efficient time-domain classification with adaptive spiking recurrent neural networks},
	volume = {3},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-021-00397-w},
	doi = {10.1038/s42256-021-00397-w},
	abstract = {Inspired by detailed modelling of biological neurons, spiking neural networks (SNNs) are investigated as biologically plausible and high-performance models of neural computation. The sparse and binary communication between spiking neurons potentially enables powerful and energy-efficient neural networks. The performance of SNNs, however, has remained lacking compared with artificial neural networks. Here we demonstrate how an activity-regularizing surrogate gradient combined with recurrent networks of tunable and adaptive spiking neurons yields the state of the art for SNNs on challenging benchmarks in the time domain, such as speech and gesture recognition. This also exceeds the performance of standard classical recurrent neural networks and approaches that of the best modern artificial neural networks. As these SNNs exhibit sparse spiking, we show that they are theoretically one to three orders of magnitude more computationally efficient compared to recurrent neural networks with similar performance. Together, this positions SNNs as an attractive solution for AI hardware implementations.},
	language = {en},
	number = {10},
	urldate = {2024-05-13},
	journal = {Nature Machine Intelligence},
	author = {Yin, Bojian and Corradi, Federico and Bohté, Sander M.},
	month = oct,
	year = {2021},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computational science, Network models},
	pages = {905--913},
}

@article{chen_exploiting_2024,
	title = {Exploiting {Symmetric} {Temporally} {Sparse} {BPTT} for {Efficient} {RNN} {Training}},
	volume = {38},
	copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/29020},
	doi = {10.1609/aaai.v38i10.29020},
	abstract = {Recurrent Neural Networks (RNNs) are useful in temporal sequence tasks. However, training RNNs involves dense matrix multiplications which require hardware that can support a large number of arithmetic operations and memory accesses. Implementing online training of RNNs on the edge calls for optimized algorithms for an efficient deployment on hardware. Inspired by the spiking neuron model, the Delta RNN exploits temporal sparsity during inference by skipping over the update of hidden states from those inactivated neurons whose change of activation across two timesteps is below a defined threshold. This work describes a training algorithm for Delta RNNs that exploits temporal sparsity in the backward propagation phase to reduce computational requirements for training on the edge. Due to the symmetric computation graphs of forward and backward propagation during training, the gradient computation of inactivated neurons can be skipped. Results show a reduction of ∼80\% in matrix operations for training a 56k parameter Delta LSTM on the Fluent Speech Commands dataset with negligible accuracy loss. Logic simulations of a hardware accelerator designed for the training algorithm show 2-10X speedup in matrix computations for an activation sparsity range of 50\%-90\%. Additionally, we show that the proposed Delta RNN training will be useful for online incremental learning on edge devices with limited computing resources.},
	language = {en},
	number = {10},
	urldate = {2024-05-09},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Chen, Xi and Gao, Chang and Wang, Zuowen and Cheng, Longbiao and Zhou, Sheng and Liu, Shih-Chii and Delbruck, Tobi},
	month = mar,
	year = {2024},
	note = {Number: 10},
	keywords = {ML: Learning on the Edge \& Model Compression},
	pages = {11399--11406},
}

@article{citri_synaptic_2008,
	title = {Synaptic {Plasticity}: {Multiple} {Forms}, {Functions}, and {Mechanisms}},
	volume = {33},
	copyright = {2008 American College of Neuropsychopharmacology},
	issn = {1740-634X},
	shorttitle = {Synaptic {Plasticity}},
	url = {https://www.nature.com/articles/1301559},
	doi = {10.1038/sj.npp.1301559},
	abstract = {Experiences, whether they be learning in a classroom, a stressful event, or ingestion of a psychoactive substance, impact the brain by modifying the activity and organization of specific neural circuitry. A major mechanism by which the neural activity generated by an experience modifies brain function is via modifications of synaptic transmission; that is, synaptic plasticity. Here, we review current understanding of the mechanisms of the major forms of synaptic plasticity at excitatory synapses in the mammalian brain. We also provide examples of the possible developmental and behavioral functions of synaptic plasticity and how maladaptive synaptic plasticity may contribute to neuropsychiatric disorders.},
	language = {en},
	number = {1},
	urldate = {2024-05-11},
	journal = {Neuropsychopharmacology},
	author = {Citri, Ami and Malenka, Robert C.},
	month = jan,
	year = {2008},
	note = {Publisher: Nature Publishing Group},
	keywords = {Behavioral Sciences, Biological Psychology, Medicine/Public Health, Neurosciences, Pharmacotherapy, Psychiatry, general},
	pages = {18--41},
}

@article{sengupta_going_2019,
	title = {Going {Deeper} in {Spiking} {Neural} {Networks}: {VGG} and {Residual} {Architectures}},
	volume = {13},
	issn = {1662-453X},
	shorttitle = {Going {Deeper} in {Spiking} {Neural} {Networks}},
	url = {https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2019.00095/full},
	doi = {10.3389/fnins.2019.00095},
	abstract = {{\textless}p{\textgreater}Over the past few years, Spiking Neural Networks (SNNs) have become popular as a possible pathway to enable low-power event-driven neuromorphic hardware. However, their application in machine learning have largely been limited to very shallow neural network architectures for simple problems. In this paper, we propose a novel algorithmic technique for generating an SNN with a deep architecture, and demonstrate its effectiveness on complex visual recognition problems such as CIFAR-10 and ImageNet. Our technique applies to both VGG and Residual network architectures, with significantly better accuracy than the state-of-the-art. Finally, we present analysis of the sparse event-driven computations to demonstrate reduced hardware overhead when operating in the spiking domain.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2024-05-11},
	journal = {Frontiers in Neuroscience},
	author = {Sengupta, Abhronil and Ye, Yuting and Wang, Robert and Liu, Chiao and Roy, Kaushik},
	month = mar,
	year = {2019},
	note = {Publisher: Frontiers},
	keywords = {Spiking neural network (SNN), event-driven neural networks, neuromorphic computing, sparsity, visual recognition},
}

@misc{yao_spike-driven_2023,
	title = {Spike-driven {Transformer}},
	url = {http://arxiv.org/abs/2307.01694},
	doi = {10.48550/arXiv.2307.01694},
	abstract = {Spiking Neural Networks (SNNs) provide an energy-efficient deep learning option due to their unique spike-based event-driven (i.e., spike-driven) paradigm. In this paper, we incorporate the spike-driven paradigm into Transformer by the proposed Spike-driven Transformer with four unique properties: 1) Event-driven, no calculation is triggered when the input of Transformer is zero; 2) Binary spike communication, all matrix multiplications associated with the spike matrix can be transformed into sparse additions; 3) Self-attention with linear complexity at both token and channel dimensions; 4) The operations between spike-form Query, Key, and Value are mask and addition. Together, there are only sparse addition operations in the Spike-driven Transformer. To this end, we design a novel Spike-Driven Self-Attention (SDSA), which exploits only mask and addition operations without any multiplication, and thus having up to \$87.2{\textbackslash}times\$ lower computation energy than vanilla self-attention. Especially in SDSA, the matrix multiplication between Query, Key, and Value is designed as the mask operation. In addition, we rearrange all residual connections in the vanilla Transformer before the activation functions to ensure that all neurons transmit binary spike signals. It is shown that the Spike-driven Transformer can achieve 77.1{\textbackslash}\% top-1 accuracy on ImageNet-1K, which is the state-of-the-art result in the SNN field. The source code is available at https://github.com/BICLab/Spike-Driven-Transformer.},
	urldate = {2024-05-10},
	publisher = {arXiv},
	author = {Yao, Man and Hu, Jiakui and Zhou, Zhaokun and Yuan, Li and Tian, Yonghong and Xu, Bo and Li, Guoqi},
	month = jul,
	year = {2023},
	note = {arXiv:2307.01694 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing, eccv rebuttal},
}

@inproceedings{zhu_event-based_2022,
	title = {Event-{Based} {Video} {Reconstruction} via {Potential}-{Assisted} {Spiking} {Neural} {Network}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Zhu_Event-Based_Video_Reconstruction_via_Potential-Assisted_Spiking_Neural_Network_CVPR_2022_paper.html},
	language = {en},
	urldate = {2024-05-10},
	booktitle = {{CVPR}},
	author = {Zhu, Lin and Wang, Xiao and Chang, Yi and Li, Jianing and Huang, Tiejun and Tian, Yonghong},
	year = {2022},
	keywords = {eccv rebuttal},
	pages = {3594--3604},
}

@inproceedings{zhou_spikformer_2022,
	title = {Spikformer: {When} {Spiking} {Neural} {Network} {Meets} {Transformer}},
	shorttitle = {Spikformer},
	url = {https://openreview.net/forum?id=frE4fUwz_h},
	abstract = {We consider two biologically plausible structures, the Spiking Neural Network (SNN) and the self-attention mechanism. The former offers an energy-efficient and event-driven paradigm for deep learning, while the latter has the ability to capture feature dependencies, enabling Transformer to achieve good performance. It is intuitively promising to explore the marriage between them. In this paper, we consider leveraging both self-attention capability and biological properties of SNNs, and propose a novel Spiking Self Attention (SSA) as well as a powerful framework, named Spiking Transformer (Spikformer). The SSA mechanism in Spikformer models the sparse visual feature by using spike-form Query, Key, and Value without softmax. Since its computation is sparse and avoids multiplication, SSA is efficient and has low computational energy consumption. It is shown that Spikformer with SSA can outperform the state-of-the-art SNNs-like frameworks in image classification on both neuromorphic and static datasets. Spikformer (66.3M parameters) with comparable size to SEW-ResNet-152 (60.2M,69.26\%) can achieve 74.81\% top1 accuracy on ImageNet using 4 time steps, which is the state-of-the-art in directly trained SNNs models. Code is avaiable at https://github.com/ZK-Zhou/spikformer.},
	language = {en},
	urldate = {2024-05-10},
	booktitle = {{ICLR}},
	author = {Zhou, Zhaokun and Zhu, Yuesheng and He, Chao and Wang, Yaowei and Yan, Shuicheng and Tian, Yonghong and Yuan, Li},
	month = sep,
	year = {2022},
	keywords = {SNN模型优化, eccv rebuttal},
}

@article{wu_stca-snn_2023,
	title = {{STCA}-{SNN}: self-attention-based temporal-channel joint attention for spiking neural networks},
	volume = {17},
	issn = {1662-453X},
	shorttitle = {{STCA}-{SNN}},
	url = {https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2023.1261543/full},
	doi = {10.3389/fnins.2023.1261543},
	abstract = {{\textless}p{\textgreater}Spiking Neural Networks (SNNs) have shown great promise in processing spatio-temporal information compared to Artificial Neural Networks (ANNs). However, there remains a performance gap between SNNs and ANNs, which impedes the practical application of SNNs. With intrinsic event-triggered property and temporal dynamics, SNNs have the potential to effectively extract spatio-temporal features from event streams. To leverage the temporal potential of SNNs, we propose a self-attention-based temporal-channel joint attention SNN (STCA-SNN) with end-to-end training, which infers attention weights along both temporal and channel dimensions concurrently. It models global temporal and channel information correlations with self-attention, enabling the network to learn ‘what’ and ‘when’ to attend simultaneously. Our experimental results show that STCA-SNNs achieve better performance on N-MNIST (99.67\%), CIFAR10-DVS (81.6\%), and N-Caltech 101 (80.88\%) compared with the state-of-the-art SNNs. Meanwhile, our ablation study demonstrates that STCA-SNNs improve the accuracy of event stream classification tasks.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2024-05-10},
	journal = {Frontiers in Neuroscience},
	author = {Wu, Xiyan and Song, Yong and Zhou, Ya and Jiang, Yurong and Bai, Yashuo and Li, Xinyi and Yang, Xin},
	month = nov,
	year = {2023},
	note = {Publisher: Frontiers},
	keywords = {/unread, Event streams, Self-attention, eccv rebuttal, neuromorphic computing, spiking neural networks, temporal-channel},
}

@inproceedings{wang_spatial-temporal_2023,
	title = {Spatial-{Temporal} {Self}-{Attention} for {Asynchronous} {Spiking} {Neural} {Networks}},
	isbn = {978-1-956792-03-4},
	shorttitle = {{STSA}},
	doi = {10.24963/ijcai.2023/344},
	abstract = {The brain-inspired spiking neural networks (SNNs) are receiving increasing attention due to their asynchronous event-driven characteristics and low power consumption. As attention mechanisms recently become an indispensable part of sequence dependence modeling, the combination of SNNs and attention mechanisms holds great potential for energy-efficient and high-performance computing paradigms. However, the existing works cannot benefit from both temporal-wise attention and the asynchronous characteristic of SNNs. To fully leverage the advantages of both SNNs and attention mechanisms, we propose an SNNs-based spatialtemporal self-attention (STSA) mechanism, which calculates the feature dependence across the time and space domains without destroying the asynchronous transmission properties of SNNs. To further improve the performance, we also propose a spatial-temporal relative position bias (STRPB) for STSA to consider the spatiotemporal position of spikes. Based on the STSA and STRPB, we construct a spatial-temporal spiking Transformer framework, named STS-Transformer, which is powerful and enables SNNs to work in an asynchronous event-driven manner. Extensive experiments are conducted on popular neuromorphic datasets and speech datasets, including DVS128 Gesture, CIFAR10-DVS, and Google Speech Commands, and our experimental results can outperform other state-of-the-art models.},
	language = {en},
	urldate = {2023-11-16},
	booktitle = {Proceedings of {IJCAI}},
	author = {Wang, Yuchen and Shi, Kexin and Lu, Chengzhuo and Liu, Yuguo and Zhang, Malu and Qu, Hong},
	month = aug,
	year = {2023},
	keywords = {\#ref, eccv rebuttal, ��Attention, ��ijcai},
	pages = {3085--3093},
}

@article{ding_shrinking_2024,
	title = {Shrinking {Your} {TimeStep}: {Towards} {Low}-{Latency} {Neuromorphic} {Object} {Recognition} with {Spiking} {Neural} {Networks}},
	volume = {38},
	copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {Shrinking {Your} {TimeStep}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/29066},
	doi = {10.1609/aaai.v38i10.29066},
	abstract = {Neuromorphic object recognition with spiking neural networks (SNNs) is the cornerstone of low-power neuromorphic computing. However, existing SNNs suffer from significant latency, utilizing 10 to 40 timesteps or more, to recognize neuromorphic objects. At low latencies, the performance of existing SNNs is drastically degraded. In this work, we propose the Shrinking SNN (SSNN) to achieve low-latency neuromorphic object recognition without reducing performance. Concretely, we alleviate the temporal redundancy in SNNs by dividing SNNs into multiple stages with progressively shrinking timesteps, which significantly reduces the inference latency. During timestep shrinkage, the temporal transformer smoothly transforms the temporal scale and preserves the information maximally. Moreover, we add multiple early classifiers to the SNN during training to mitigate the mismatch between the surrogate gradient and the true gradient, as well as the gradient vanishing/exploding, thus eliminating the performance degradation at low latency. Extensive experiments on neuromorphic datasets, CIFAR10-DVS, N-Caltech101, and DVS-Gesture have revealed that SSNN is able to improve the baseline accuracy by 6.55\% {\textasciitilde} 21.41\%. With only 5 average timesteps and without any data augmentation, SSNN is able to achieve an accuracy of 73.63\% on CIFAR10-DVS. This work presents a heterogeneous temporal scale SNN and provides valuable insights into the development of high-performance, low-latency SNNs.},
	language = {en},
	number = {10},
	urldate = {2024-05-09},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Ding, Yongqi and Zuo, Lin and Jing, Mengmeng and He, Pei and Xiao, Yongjun},
	month = mar,
	year = {2024},
	note = {Number: 10},
	keywords = {ML: Deep Neural Architectures and Foundation Models},
	pages = {11811--11819},
}

@article{finkbeiner_harnessing_2024,
	title = {Harnessing {Manycore} {Processors} with {Distributed} {Memory} for {Accelerated} {Training} of {Sparse} and {Recurrent} {Models}},
	volume = {38},
	copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/29087},
	doi = {10.1609/aaai.v38i11.29087},
	abstract = {Current AI training infrastructure is dominated by single instruction multiple data (SIMD) and systolic array architectures, such as Graphics Processing Units (GPUs) and Tensor Processing Units (TPUs), that excel at accelerating parallel workloads and dense vector matrix multiplications. Potentially more efficient neural network models utilizing sparsity and recurrence cannot leverage the full power of SIMD processor and are thus at a severe disadvantage compared to today's prominent parallel architectures like Transformers and CNNs, thereby hindering the path towards more sustainable AI. 
To overcome this limitation, we explore sparse and recurrent model training on a massively parallel multiple instruction multiple data (MIMD) architecture with distributed local memory. We implement a training routine based on backpropagation though time (BPTT) for the brain-inspired class of Spiking Neural Networks (SNNs) that feature binary sparse activations. We observe a massive advantage in using sparse activation tensors with a MIMD processor, the Intelligence Processing Unit (IPU) compared to GPUs. On training workloads, our results demonstrate 5-10x throughput gains compared to A100 GPUs and up to 38x gains for higher levels of activation sparsity, without a significant slowdown in training convergence or reduction in final model performance. Furthermore, our results show highly promising trends for both single and multi IPU configurations as we scale up to larger model sizes. 
Our work paves the way towards more efficient, non-standard models via AI training hardware beyond GPUs, and competitive large scale SNN models.},
	language = {en},
	number = {11},
	urldate = {2024-05-09},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Finkbeiner, Jan and Gmeinder, Thomas and Pupilli, Mark and Titterton, Alexander and Neftci, Emre},
	month = mar,
	year = {2024},
	note = {Number: 11},
	keywords = {ML: Bio-inspired Learning},
	pages = {11996--12005},
}

@article{zhang_memory-efficient_2024,
	title = {Memory-{Efficient} {Reversible} {Spiking} {Neural} {Networks}},
	volume = {38},
	copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/29616},
	doi = {10.1609/aaai.v38i15.29616},
	abstract = {Spiking neural networks (SNNs) are potential competitors to artificial neural networks (ANNs) due to their high energy-efficiency on neuromorphic hardware. However, SNNs are unfolded over simulation time steps during the training process. Thus, SNNs require much more memory than ANNs, which impedes the training of deeper SNN models. In this paper, we propose the reversible spiking neural network to reduce the memory cost of intermediate activations and membrane potentials during training. Firstly, we extend the reversible architecture along temporal dimension and propose the reversible spiking block, which can reconstruct the computational graph and recompute all intermediate variables in forward pass with a reverse process. On this basis, we adopt the state-of-the-art SNN models to the reversible variants, namely reversible spiking ResNet (RevSResNet) and reversible spiking transformer (RevSFormer). Through experiments on static and neuromorphic datasets, we demonstrate that the memory cost per image of our reversible SNNs does not increase with the network depth. On CIFAR10 and CIFAR100 datasets, our RevSResNet37 and RevSFormer-4-384 achieve comparable accuracies and consume 3.79x and 3.00x lower GPU memory per image than their counterparts with roughly identical model complexity and parameters. We believe that this work can unleash the memory constraints in SNN training and pave the way for training extremely large and deep SNNs.},
	language = {en},
	number = {15},
	urldate = {2024-05-09},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Zhang, Hong and Zhang, Yu},
	month = mar,
	year = {2024},
	note = {Number: 15},
	keywords = {CV: Object Detection \& Categorization},
	pages = {16759--16767},
}

@article{zhang_enhancing_2024,
	title = {Enhancing {Representation} of {Spiking} {Neural} {Networks} via {Similarity}-{Sensitive} {Contrastive} {Learning}},
	volume = {38},
	copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/29635},
	doi = {10.1609/aaai.v38i15.29635},
	abstract = {Spiking neural networks (SNNs) have attracted intensive attention as a promising energy-efficient alternative to conventional artificial neural networks (ANNs) recently, which could transmit information in form of binary spikes rather than continuous activations thus the  multiplication of activation and weight could be replaced by addition to save energy. However, the binary spike representation form will sacrifice the expression performance of SNNs and lead to accuracy degradation compared with ANNs. Considering improving feature representation is beneficial to training an accurate SNN model, this paper focuses on enhancing the feature representation of the SNN. To this end, we establish a similarity-sensitive contrastive learning framework, where SNN could capture significantly more information from its ANN counterpart to improve representation by Mutual Information (MI) maximization with layer-wise sensitivity to similarity.  In specific, it enriches the SNN’s feature representation by pulling the positive pairs of SNN's and ANN's feature representation of each layer from the same input samples closer together while pushing the negative pairs from different samples further apart. Experimental results show that our method consistently outperforms the current state-of-the-art algorithms on both popular non-spiking static and neuromorphic datasets.},
	language = {en},
	number = {15},
	urldate = {2024-05-09},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Zhang, Yuhan and Liu, Xiaode and Chen, Yuanpei and Peng, Weihang and Guo, Yufei and Huang, Xuhui and Ma, Zhe},
	month = mar,
	year = {2024},
	note = {Number: 15},
	keywords = {ML: Representation Learning},
	pages = {16926--16934},
}

@article{he_efficient_2024,
	title = {An {Efficient} {Knowledge} {Transfer} {Strategy} for {Spiking} {Neural} {Networks} from {Static} to {Event} {Domain}},
	volume = {38},
	copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/27806},
	doi = {10.1609/aaai.v38i1.27806},
	abstract = {Spiking neural networks (SNNs) are rich in spatio-temporal dynamics and are suitable for processing event-based neuromorphic data.  However, event-based datasets are usually less annotated than static datasets. This small data scale makes SNNs prone to overfitting and limits their performance. In order to improve the generalization ability of SNNs on event-based datasets, we use static images to assist SNN training on event data. In this paper, we first discuss the domain mismatch problem encountered when directly transferring networks trained on static datasets to event data. We argue that the inconsistency of feature distributions becomes a major factor hindering the effective transfer of knowledge from static images to event data. To address this problem, we propose solutions in terms of two aspects: feature distribution and training strategy. Firstly, we propose a knowledge transfer loss, which consists of domain alignment loss and spatio-temporal regularization. The domain alignment loss learns domain-invariant spatial features by reducing the marginal distribution distance between the static image and the event data. Spatio-temporal regularization provides dynamically learnable coefficients for domain alignment loss by using the output features of the event data at each time step as a regularization term. In addition, we propose a sliding training strategy, which gradually replaces static image inputs probabilistically with event data, resulting in a smoother and more stable training for the network. We validate our method on neuromorphic datasets, including N-Caltech101, CEP-DVS, and N-Omniglot. The experimental results show that our proposed method achieves better performance on all datasets compared to the current state-of-the-art methods. 
Code is available at https://github.com/Brain-Cog-Lab/Transfer-for-DVS.},
	language = {en},
	number = {1},
	urldate = {2024-05-09},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {He, Xiang and Zhao, Dongcheng and Li, Yang and Shen, Guobin and Kong, Qingqun and Zeng, Yi},
	month = mar,
	year = {2024},
	note = {Number: 1},
	keywords = {General},
	pages = {512--520},
}

@article{shen_efficient_2024,
	title = {Efficient {Spiking} {Neural} {Networks} with {Sparse} {Selective} {Activation} for {Continual} {Learning}},
	volume = {38},
	copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/27817},
	doi = {10.1609/aaai.v38i1.27817},
	abstract = {The next generation of machine intelligence requires the capability of continual learning to acquire new knowledge without forgetting the old one while conserving limited computing resources. 
Spiking neural networks (SNNs), compared to artificial neural networks (ANNs), have more characteristics that align with biological neurons, which may be helpful as a potential gating function for knowledge maintenance in neural networks. Inspired by the selective sparse activation principle of context gating in biological systems, we present a novel SNN model with selective activation to achieve continual learning. The trace-based K-Winner-Take-All (K-WTA) and variable threshold components are designed to form the sparsity in selective activation in spatial and temporal dimensions of spiking neurons, which promotes the subpopulation of neuron activation to perform specific tasks. As a result, continual learning can be maintained by routing different tasks via different populations of neurons in the network. The experiments are conducted on MNIST and CIFAR10 datasets under the class incremental setting. The results show that the proposed SNN model achieves competitive performance similar to and even surpasses the other regularization-based methods deployed under traditional ANNs.},
	language = {en},
	number = {1},
	urldate = {2024-05-09},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Shen, Jiangrong and Ni, Wenyao and Xu, Qi and Tang, Huajin},
	month = mar,
	year = {2024},
	note = {Number: 1},
	keywords = {CMS: (Computational) Cognitive Architectures},
	pages = {611--619},
}

@article{zhao_dynamic_2024,
	title = {Dynamic {Reactive} {Spiking} {Graph} {Neural} {Network}},
	volume = {38},
	copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/29640},
	doi = {10.1609/aaai.v38i15.29640},
	abstract = {Spiking Graph Neural Networks are emerging tools for analyzing graph data along with low energy consumption and certain biological fidelity. Existing methods directly integrate same-reactive spiking neurons into graph neural networks for processing propagated graphs. However, such same-reactive neurons are not biological-functionality enough compared to the brain's dynamic-reactive ones, limiting the model's expression. Meanwhile, insufficient long-range neighbor information can be excavated with the few-step propagated graph, restricting discrimination of graph spiking embeddings. Inspired by the dynamic cognition in the brain, we propose a Dynamic Reactive Spiking Graph Neural Network that can enhance model's expressive ability in higher biological fidelity. Specifically, we design dynamic reactive spiking neurons to process spiking graph inputs, which have unique optimizable thresholds to spontaneously explore dynamic reactive states between neurons. Moreover, discriminative graph positional spikes are learned and integrated adaptively into spiking outputs through our neurons, thereby exploring long-range neighbors more thoroughly. Finally, with the dynamic reactive mechanism and learnable positional integration, we can obtain a powerful and highly bio-fidelity model with low energy consumption. Experiments on various domain-related datasets can demonstrate the effectiveness of our model. Our code is available at https://github.com/hzhao98/DRSGNN.},
	language = {en},
	number = {15},
	urldate = {2024-05-09},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Zhao, Han and Yang, Xu and Deng, Cheng and Yan, Junchi},
	month = mar,
	year = {2024},
	note = {Number: 15},
	keywords = {ML: Semi-Supervised Learning},
	pages = {16970--16978},
}

@article{kim_exploring_2023,
	title = {Exploring {Temporal} {Information} {Dynamics} in {Spiking} {Neural} {Networks}},
	volume = {37},
	copyright = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/26002},
	doi = {10.1609/aaai.v37i7.26002},
	abstract = {Most existing Spiking Neural Network (SNN) works state that SNNs may utilize temporal information dynamics of spikes. However, an explicit analysis of temporal information dynamics is still missing. In this paper, we ask several important questions for providing a fundamental understanding of SNNs: What are temporal information dynamics inside SNNs? How can we measure the temporal information dynamics? How do the temporal information dynamics affect the overall learning performance? To answer these questions, we estimate the Fisher Information of the weights to measure the distribution of temporal information during training in an empirical manner. Surprisingly, as training goes on, Fisher information starts to concentrate in the early timesteps. After training, we observe that information becomes highly concentrated in earlier few timesteps, a phenomenon we refer to as temporal information concentration. We observe that the temporal information concentration phenomenon is a common learning feature of SNNs by conducting extensive experiments on various configurations such as architecture, dataset, optimization strategy, time constant, and timesteps. Furthermore, to reveal how temporal information concentration affects the performance of SNNs, we design a loss function to change the trend of temporal information. We find that temporal information concentration is crucial to building a robust SNN but has little effect on classification accuracy. Finally, we propose an efficient iterative pruning method based on our observation on temporal information concentration. 
Code is available at https://github.com/Intelligent-Computing-Lab-Yale/Exploring-Temporal-Information-Dynamics-in-Spiking-Neural-Networks.},
	language = {en},
	number = {7},
	urldate = {2024-05-09},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Kim, Youngeun and Li, Yuhang and Park, Hyoungseob and Venkatesha, Yeshwanth and Hambitzer, Anna and Panda, Priyadarshini},
	month = jun,
	year = {2023},
	note = {Number: 7},
	keywords = {ML: Deep Neural Network Algorithms},
	pages = {8308--8316},
}

@article{henkes_spiking_2024,
	title = {Spiking neural networks for nonlinear regression},
	volume = {11},
	url = {https://royalsocietypublishing.org/doi/full/10.1098/rsos.231606},
	doi = {10.1098/rsos.231606},
	abstract = {Spiking neural networks (SNN), also often referred to as the third generation of neural networks, carry the potential for a massive reduction in memory and energy consumption over traditional, second-generation neural networks. Inspired by the undisputed efficiency of the human brain, they introduce temporal and neuronal sparsity, which can be exploited by next-generation neuromorphic hardware. Energy efficiency plays a crucial role in many engineering applications, for instance, in structural health monitoring. Machine learning in engineering contexts, especially in data-driven mechanics, focuses on regression. While regression with SNN has already been discussed in a variety of publications, in this contribution, we provide a novel formulation for its accuracy and energy efficiency. In particular, a network topology for decoding binary spike trains to real numbers is introduced, using the membrane potential of spiking neurons. Several different spiking neural architectures, ranging from simple spiking feed-forward to complex spiking long short-term memory neural networks, are derived. Since the proposed architectures do not contain any dense layers, they exploit the full potential of SNN in terms of energy efficiency. At the same time, the accuracy of the proposed SNN architectures is demonstrated by numerical examples, namely different material models. Linear and nonlinear, as well as history-dependent material models, are examined. While this contribution focuses on mechanical examples, the interested reader may regress any custom function by adapting the published source code.},
	number = {5},
	urldate = {2024-05-09},
	journal = {Royal Society Open Science},
	author = {Henkes, Alexander and Eshraghian, Jason K. and Wessels, Henning},
	month = may,
	year = {2024},
	note = {Publisher: Royal Society},
	keywords = {artificial neural networks, neuromorphic computing, regression, spiking neural networks},
	pages = {231606},
}

@misc{noauthor_training_nodate,
	title = {Training {Spiking} {Neural} {Networks} {Using} {Lessons} {From} {Deep} {Learning} {\textbar} {IEEE} {Journals} \& {Magazine} {\textbar} {IEEE} {Xplore}},
	url = {https://ieeexplore.ieee.org/abstract/document/10242251},
	urldate = {2024-05-09},
}

@article{guo_direct_2023,
	title = {Direct learning-based deep spiking neural networks: a review},
	volume = {17},
	issn = {1662-453X},
	shorttitle = {Direct learning-based deep spiking neural networks},
	url = {https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2023.1209795/full},
	doi = {10.3389/fnins.2023.1209795},
	abstract = {{\textless}p{\textgreater}The spiking neural network (SNN), as a promising brain-inspired computational model with binary spike information transmission mechanism, rich spatially-temporal dynamics, and event-driven characteristics, has received extensive attention. However, its intricately discontinuous spike mechanism brings difficulty to the optimization of the deep SNN. Since the surrogate gradient method can greatly mitigate the optimization difficulty and shows great potential in directly training deep SNNs, a variety of direct learning-based deep SNN works have been proposed and achieved satisfying progress in recent years. In this paper, we present a comprehensive survey of these direct learning-based deep SNN works, mainly categorized into accuracy improvement methods, efficiency improvement methods, and temporal dynamics utilization methods. In addition, we also divide these categorizations into finer granularities further to better organize and introduce them. Finally, the challenges and trends that may be faced in future research are prospected.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2024-05-09},
	journal = {Frontiers in Neuroscience},
	author = {Guo, Yufei and Huang, Xuhui and Ma, Zhe},
	month = jun,
	year = {2023},
	note = {Publisher: Frontiers},
	keywords = {Brain-Inspired Computation, Deep neural network, Spatialtemporal Processing, Spiking Neural network, direct learning, energy efficiency},
}

@misc{noauthor_neural_nodate,
	title = {A {NEURAL} {NETWORK} {FOR} {POSITION} {INVARIANT} {PATTERN} {RECOGNITION} {COMBINING} {SPIKING} {NEURONS} {WITH} {THE} {FOURIER}-{TRANSFORM} {\textbar} {International} {Journal} of {Neural} {Systems}},
	url = {https://www.worldscientific.com/doi/abs/10.1142/S0129065796000695},
	urldate = {2024-05-06},
	keywords = {/unread},
}

@misc{noauthor_neural_nodate-1,
	title = {A {NEURAL} {NETWORK} {FOR} {POSITION} {INVARIANT} {PATTERN} {RECOGNITION} {COMBINING} {SPIKING} {NEURONS} {WITH} {THE} {FOURIER}-{TRANSFORM} {\textbar} {International} {Journal} of {Neural} {Systems}},
	url = {https://www.worldscientific.com/doi/abs/10.1142/S0129065796000695},
	urldate = {2024-05-06},
	keywords = {/unread},
}

@misc{noauthor_neural_nodate-2,
	title = {A {NEURAL} {NETWORK} {FOR} {POSITION} {INVARIANT} {PATTERN} {RECOGNITION} {COMBINING} {SPIKING} {NEURONS} {WITH} {THE} {FOURIER}-{TRANSFORM} {\textbar} {International} {Journal} of {Neural} {Systems}},
	url = {https://www.worldscientific.com/doi/abs/10.1142/S0129065796000695},
	urldate = {2024-05-06},
	keywords = {/unread},
}

@inproceedings{zhang_temporal_2020,
	title = {Temporal {Spike} {Sequence} {Learning} via {Backpropagation} for {Deep} {Spiking} {Neural} {Networks}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/8bdb5058376143fa358981954e7626b8-Abstract.html},
	abstract = {Spiking neural networks (SNNs) are well suited for spatio-temporal learning and implementations on energy-efficient event-driven neuromorphic processors. However, existing SNN error backpropagation (BP)  methods lack proper handling of spiking discontinuities and suffer from low performance compared with the BP methods for traditional artificial neural networks. In addition, a large number of time steps are typically required to achieve decent performance, leading to high latency and rendering spike-based computation unscalable to deep architectures. We present a novel Temporal Spike Sequence Learning Backpropagation (TSSL-BP) method for training deep SNNs, which breaks down error backpropagation across two types of inter-neuron and intra-neuron dependencies and leads to improved temporal learning precision. It captures inter-neuron dependencies through presynaptic firing times by considering the all-or-none characteristics of firing activities and captures intra-neuron dependencies by handling the internal evolution of each neuronal state in time. TSSL-BP efficiently trains deep SNNs within a much shortened temporal window of a few steps while improving the accuracy for various image classification datasets including CIFAR10.},
	urldate = {2024-04-30},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zhang, Wenrui and Li, Peng},
	year = {2020},
	pages = {12022--12033},
}

@article{bohte_error-backpropagation_2002,
	title = {Error-backpropagation in temporally encoded networks of spiking neurons},
	volume = {48},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231201006580},
	doi = {10.1016/S0925-2312(01)00658-0},
	abstract = {For a network of spiking neurons that encodes information in the timing of individual spike times, we derive a supervised learning rule, SpikeProp, akin to traditional error-backpropagation. With this algorithm, we demonstrate how networks of spiking neurons with biologically reasonable action potentials can perform complex non-linear classification in fast temporal coding just as well as rate-coded networks. We perform experiments for the classical XOR problem, when posed in a temporal setting, as well as for a number of other benchmark datasets. Comparing the (implicit) number of spiking neurons required for the encoding of the interpolated XOR problem, the trained networks demonstrate that temporal coding is a viable code for fast neural information processing, and as such requires less neurons than instantaneous rate-coding. Furthermore, we find that reliable temporal computation in the spiking networks was only accomplished when using spike response functions with a time constant longer than the coding interval, as has been predicted by theoretical considerations.},
	number = {1},
	urldate = {2024-04-30},
	journal = {Neurocomputing},
	author = {Bohte, Sander M. and Kok, Joost N. and La Poutré, Han},
	month = oct,
	year = {2002},
	keywords = {Error-backpropagation, Spiking neurons, Temporal coding},
	pages = {17--37},
}

@inproceedings{kundu_recent_2024,
	title = {Recent {Advances} in {Scalable} {Energy}-{Efficient} and {Trustworthy} {Spiking} {Neural} {Networks}: from {Algorithms} to {Technology}},
	shorttitle = {Recent {Advances} in {Scalable} {Energy}-{Efficient} and {Trustworthy} {Spiking} {Neural} {Networks}},
	doi = {10.1109/ICASSP48485.2024.10445826},
	abstract = {Neuromorphic computing and, in particular, spiking neural networks (SNNs) have become an attractive alternative to deep neural networks for a broad range of signal processing applications, processing static and/or temporal inputs from different sensory modalities, including audio and vision sensors. In this paper, we start with a description of recent advances in algorithmic and optimization innovations to efficiently train and scale low-latency, and energy-efficient spiking neural networks (SNNs) for complex machine learning applications. We then discuss the recent efforts in algorithm-architecture co-design that explores the inherent trade-offs between achieving high energy-efficiency and low latency while still providing high accuracy and trustworthiness. We then describe the underlying hardware that has been developed to leverage such algorithmic innovations in an efficient way. In particular, we describe a hybrid method to integrate significant portions of the model’s computation within both memory components as well as the sensor itself. Finally, we discuss the potential path forward for research in building deployable SNN systems identifying key challenges in the algorithm-hardware-application co-design space with an emphasis on trustworthiness.},
	urldate = {2024-04-22},
	booktitle = {ICASSP},
	author = {Kundu, Souvik and Zhu, Rui-Jie and Jaiswal, Akhilesh and Beerel, Peter A.},
	month = apr,
	year = {2024},
	note = {ISSN: 2379-190X},
	keywords = {Energy efficiency, Low latency communication, Machine learning algorithms, Signal processing, Signal processing algorithms, Technological innovation, Vision sensors},
	pages = {13256--13260},
}

@article{meng_training_2022-1,
	title = {Training much deeper spiking neural networks with a small number of time-steps},
	volume = {153},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608022002064},
	doi = {10.1016/j.neunet.2022.06.001},
	abstract = {Spiking Neural Network (SNN) is a promising energy-efficient neural architecture when implemented on neuromorphic hardware. The Artificial Neural Network (ANN) to SNN conversion method, which is the most effective SNN training method, has successfully converted moderately deep ANNs to SNNs with satisfactory performance. However, this method requires a large number of time-steps, which hurts the energy efficiency of SNNs. How to effectively covert a very deep ANN (e.g., more than 100 layers) to an SNN with a small number of time-steps remains a difficult task. To tackle this challenge, this paper makes the first attempt to propose a novel error analysis framework that takes both the “quantization error” and the “deviation error” into account, which comes from the discretization of SNN dynamicsthe neuron’s coding scheme and the inconstant input currents at intermediate layers, respectively. Particularly, our theories reveal that the “deviation error” depends on both the spike threshold and the input variance. Based on our theoretical analysis, we further propose the Threshold Tuning and Residual Block Restructuring (TTRBR) method that can convert very deep ANNs ({\textgreater}100 layers) to SNNs with negligible accuracy degradation while requiring only a small number of time-steps. With very deep networks, our TTRBR method achieves state-of-the-art (SOTA) performance on the CIFAR-10, CIFAR-100, and ImageNet classification tasks.},
	urldate = {2024-04-29},
	journal = {Neural Networks},
	author = {Meng, Qingyan and Yan, Shen and Xiao, Mingqing and Wang, Yisen and Lin, Zhouchen and Luo, Zhi-Quan},
	month = sep,
	year = {2022},
	keywords = {/unread, ANN-to-SNN conversion, Conversion error analysis, Spiking neural networks},
	pages = {254--268},
}

@article{xu_enhancing_2023,
	title = {Enhancing {Adaptive} {History} {Reserving} by {Spiking} {Convolutional} {Block} {Attention} {Module} in {Recurrent} {Neural} {Networks}},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/b8734840bf65c8facd619f5105c6acd0-Abstract-Conference.html},
	language = {en},
	urldate = {2024-04-27},
	journal = {Proceedings of NeurIPS},
	author = {Xu, Qi and Gao, Yuyuan and Shen, Jiangrong and Li, Yaxin and Ran, Xuming and Tang, Huajin and Pan, Gang},
	month = dec,
	year = {2023},
	pages = {58890--58901},
}

@inproceedings{xu2024rsnn,
  title={RSNN: Recurrent Spiking Neural Networks for Dynamic Spatial-Temporal Information Processing},
  author={Xu, Qi and Fang, Xuanye and Li, Yaxin and Shen, Jiangrong and Ma, De and Xu, Yi and Pan, Gang},
  booktitle={ACM Multimedia 2024}
}

@misc{xu_hybrid_2024,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Hybrid {Spiking} {Vision} {Transformer} for {Event}-{Based} {Object} {Detection}},
	url = {https://papers.ssrn.com/abstract=4790563},
	doi = {10.2139/ssrn.4790563},
	abstract = {Event-based object detection has gradually drawn attention for its high time resolution, high dynamic range, and asynchronous address event representation. Spiking Neural Networks (SNNs) offer distinct advantages, including low energy consumption and rich spatiotemporal dynamics. Therefore, in this study, a novel hybrid spike vision Transformer (HsVT) model is proposed, which combines spatial feature extraction and temporal feature extraction components. The spatial feature extraction module is used to extract the local and global features in the input data, and the temporal feature extraction module is used to capture the time dependencies and long-term patterns in the input sequence. With this combination, HsVT models can better capture spatial and temporal features when processing sequence data, thus improving the modeling ability of complex event-based object detection tasks. Besides, we collect the fall detection datasets and make them public as the benchmark of event-based object detection tasks. The Fall detection dataset with event-based camera provides facial privacy protection and saves memory storage due to the event representation manner. We evaluated the performance of HsVT methods on different model sizes and compared their performance on GEN1 and Fall detection datasets. The experimental results show that the HsVT method has achieved remarkable performance improvement in the event detection task with fewer parameters, and provides an effective solution for the event-based object detection task.},
	language = {en},
	urldate = {2024-04-27},
	author = {Xu, Qi and Deng, Jie and Shen, Jiangrong and Chen, Biwu and Tang, Huajin and Pan, Gang},
	month = apr,
	year = {2024},
	keywords = {Fall Detection, Object Detection With Event Cameras, spiking neural networks},
}

@phdthesis{__2022,
	type = {博士},
	title = {基于脉冲神经网络误差反传算法的脑功能模拟与机制分析},
	url = {https://kns.cnki.net/kcms2/article/abstract?v=29axctaKF3ytIReAFVboSpftXDPf2g5q4nkMEkBJVdaOjADiVlNpMWLjJVtA344Lb0G3Ei8BR4XBgxhnGmMKeAWKaIOmgqx6p7_e8B3gRbXa0cFdhIPrggfovA0e_yq5oa9fyrbAxMskXgd7xOa2kw==&uniplatform=NZKPT&language=CHS},
	abstract = {生物大脑的认知功能和信息处理机制的研究具有重要的科学意义和实用价值。近年来,受神经系统启发的深度学习方法在图像识别、语音识别、策略游戏等人工智能应用上取得了巨大的成功。但相比之下,人们对大脑实现这些认知功能的机制理解仍非常有限。而深度学习模型在工作机制上与生物神经系统也存在巨大的差异。与之相反,脉冲神经网络是更加符合生物电生理机制的神经网络模型,一直是神经科学领域建模的重要工具,在发展强人工智能系统方面也更具潜力。但是,由于脉冲神经网络的复杂性,一直缺少令人满意的方法实现复杂认知功能的模拟。这导致很难在介观层面建模和研究大脑功能机制,也导致宏观层面的理论很难联系到微观的生物细节。因此,本文基于脉冲神经网络模型,提出了通过放电时刻学习算法模拟大脑认知功能,并通过改变训练网络的生理约束和网络结构分析脑功能机制的研究思路。首先,网络结构和集群活动是脑功能的基本载体,而神经元的放电动力学活动是进行信息处理的基本单元。本文提出了可以进行训练并能精确模拟神经动力学特性的单神经元模型,并对神经网络动态活动建模进行了研究。基于积分放电类(LIF)神经元模型提出了扩展的广义积分放电(GLIF)模型,用以准确模拟生物神经元多时标的、非线性的脉冲放电过程。通过构建基本的神经网络结构,分析了关键的结构特性对神经集群的振荡、相关性、规则性等活动特性的影响规律,并验证了通过时间编码传递信息的可行性。神经元及网络动力学的研究为进一步构建具有实际功能的神经网络提供了理论基础。其次,提出了改进的放电时刻的误差反传算法,实现了在复杂网络结构和动力学状态下脉冲神经网络的稳定学习。推导了提出的三类广义积分放电模型的神经网络梯度反传算法,保证算法在任意放电形式和网络结构下的有效性。分析了Spike Prop类学习算法学习效率低和训练不稳定问题的主要原因。并基于分析结果和生理理论依据,提出了梯度动态阈值方法以及放电率和突触权值等的调制方法。基于学习算法,提出了通过认知任务的训练进行神经网络功能的建模和机制分析的研究框架。最后,基于提出的研究框架,实现了同步、连续放电等不同网络动态和视觉、运动等功能的建模和机制分析,验证了算法的灵活性和有效性。本文将提出的网络模型和学习算法应用于不同的认知任务的学习中,并基于训练得到的网络分析了神经网络实现这些功能的机制。通过构建不同的神经网络结构、采用不同的网络活动状态和损失函数,本文分别训练实现了图像识别任务、运动规划任...},
	language = {zh-CN},
	urldate = {2024-04-27},
	school = {天津大学},
	author = {洪, 朝飞},
	year = {2022},
	doi = {10.27356/d.cnki.gtjdu.2020.000031},
	keywords = {/unread, 增强学习, 放电误差反传, 时间编码, 监督学习, 脉冲神经网络, 脑功能},
}

@misc{noauthor__nodate,
	title = {面向数值和脉冲混合的神经网络模型与算法研究},
	url = {https://d.wanfangdata.com.cn/thesis/ChJUaGVzaXNOZXdTMjAyNDAxMDkSCUQwMjU4NDI3MxoIb2V5OWE1aWo%3D},
	urldate = {2024-04-27},
}

@phdthesis{_spiking_2023,
	type = {博士},
	title = {{深度Spiking神经网络模型及其算法研究}},
	url = {https://kns.cnki.net/kcms2/article/abstract?v=29axctaKF3xCpyDzDEFH4d9Ha5sAfc2Gu9nrQLkWk-BXzKSNQYw_m8YAaTL8sP5GhBxFSg64kujWMyvgsTTOL27WJvKyYIW_H1l2U2E6_BJ5HcG0VHpZbL2E7BHW4Ge4NoYMkFfk3Yau5J5-5rUjdg==&uniplatform=NZKPT&language=CHS},
	abstract = {深度脉冲神经网络(Deep Spiking Neural Networks,DSNN)是一种模拟生物神经网络工作机制的模型,具有高效、低功耗、可塑性和自适应性等优点,因此在模式识别、图像处理、自然语言处理、智能控制等领域具有广泛的应用前景。作为神经科学和计算机科学的交叉领域,DSNN受益于这两个领域的最新研究成果。DSNN借鉴了生物神经元和神经网络的行为特性,如脉冲发放、时域编码和稀疏性等,使得DSNN能够更高效和节能地处理复杂任务。同时,DSNN可以利用先进的计算机硬件技术和优化算法,进一步增加网络规模,提升智能水平。然而,由于DSNN的离散特性,它无法直接使用深度神经网络(Deep Neural Networks,DNNs)中常用的误差反向传播(Back Propagation,BP)算法,这使得DSNN缺乏高效的学习算法。同时,随着网络规模的增加,DSNN逐渐面临如今深度大模型常见的问题,缺乏高效的学习算法和模型难以部署在资源受限的平台上。针对上述问题,本文围绕DSNN模型、学习算法和轻量化策略三个方面,由微观到宏观,在DSNN模型上针对DSNN中的神经元模型、局部网络模块和整体网络架构展开研究,在算法上针对DSNN中的激活函数求导、训练过程进行优化,在DSNN轻量化上针对局部模块轻量化和网络架构轻量化,主要内容包括:1.提出了一种基于神经振荡和相位锁定的共振脉冲神经元模型(Resonate Spiking Neuron Model,RSNM)与学习算法。RSNM解决了BP算法直接应用于DSNN中所面临的激活函数不可微分、梯度爆炸和死亡神经元问题。之后,设计了基于RSNM的前馈神经网络和卷积神经网络两种架构,并在经典的异或问题和视觉处理数据集MNIST和CIFAR-10上进行了实验。实验结果表明,RSNM可以直接应用BP算法仅仅单层便能够处理非线性问题,具有更好的非线性能力。同时,RSNM在实际应用问题上也得到了极好的结果。2.提出了一种隐含式脉冲神经元模型(Internal Spiking Neuron Model,ISNM)与渐进式代理梯度(Gradual Surrogate Gradient,GSG)学习算法。由于脉冲序列的离散特性,DSNN无法直接使用DNN中的成熟操作,如最大池化和批量归一化等。同时,代理梯度算法单一的代理梯度函数不利于DSNN的高效收敛。ISNM对传统的SNN重新数学建模,赋予新...},
	language = {zh-CN},
	urldate = {2024-04-27},
	school = {电子科技大学},
	author = {陈, 一},
	year = {2023},
	doi = {10.27005/d.cnki.gdzku.2023.005522},
	keywords = {工作记忆, 注意力, 深度脉冲神经网络, 监督学习算法, 脉冲神经元},
}

@phdthesis{__2023,
	type = {博士},
	title = {基于视觉机制的脉冲神经网络模型研究},
	url = {https://kns.cnki.net/kcms2/article/abstract?v=Xhw-7KfLOFmwZayk6kZwzlBU2Mc0WKjt4ZYOuYulVtiwS-BPzSu5_OUW3BLFDBl1AIzk1UiLvLpkiptxwh8-2XnDQJvl0fEK4EIO0HpPhPY1mE_UtS-ECFXTiwksJPva1kfrgYCHb_fIL5lZlYzH4g==&uniplatform=NZKPT&language=CHS},
	abstract = {脉冲神经网络(Spiking Neural Network,SNN)以脉冲神经元为基本组成单位,能够以较低的能耗表达更丰富的信息,并具有更强的计算能力,被视为第三代人工神经网络。目前SNN网络的相关研究受到越来越多的重视,如脉冲编码方案、网络结构优化等,不同的模型和算法也相继被提出。但是,由于其内在机制等问题,SNN在模式识别任务上的性能远没有达到能与生物视觉系统竞争的水平,并且在图像识别中常伴随模式混叠问题。另外,如何针对SNN设计高效的学习算法、采取何种拓扑结构更为有效,以及在更复杂的视觉任务上(如目标检测)的应用,仍是该研究领域的重要问题。生物视觉系统的信息处理机制是突破计算机视觉发展瓶颈的关键。传统人工神经网络大多专注于注意力机制的研究,若要达到与视觉系统相当的水平,还需结合更多合适的视觉机制以提升网络性能。鉴于以上原因,本文从提升模型性能的角度,以模拟大脑视觉系统工作机制为出发点,开展基于视觉机制的SNN模型及应用的研究。研究内容概述如下:1.针对SNN对输入模式的特征提取能力存在不足,提出一种基于视觉显著性的SNN网络,以及两种脉冲时间编码方案。同时,采用果蝇演化算法对SNN网络的结构进行优化以提高SNN的识别性能。通过在不同类型的图像数据集上验证所提出方法的性能,实验结果表明并且结合显著性计算和果蝇算法优化后的SNN可以提高识别准确率。2.针对SNN在多分类任务上表现不佳以及出现模式混叠的情况,提出基于视觉侧抑制的集成无监督深层SNN模型。通过在多个数据集上进行测试,实验结果表明所提出的模型可对不同类型的图像识别任务得出正确的识别结果。3.针对视觉侧抑制计算的不足以及神经元多样性问题,提出一种结合自适应侧抑制的并行卷积SNN模型。通过在静态数据集与神经形态数据集上验证所提出方法的有效性,并将其应用于乳腺肿瘤的识别。4.针对SNN在目标检测任务上无法直接利用反向传播算法训练的问题,提出融合视觉显著性的SNN目标检测网络。该网络以YOLO框架为载体,采用DNN到SNN的转换方式训练网络参数,在避免梯度下降算法无法直接训练SNN的同时保持网络的检测性能,模型融合特征金字塔与视觉显著性计算提升网络目标检测的精度。通过在乳腺肿瘤检测任务上进行实验验证,实验表明基于SNN的目标检测网络可获得相对较优的检测结果。},
	language = {zh-CN},
	urldate = {2024-04-25},
	school = {哈尔滨工程大学},
	author = {付, 强},
	year = {2023},
	doi = {10.27060/d.cnki.ghbcu.2023.000290},
	keywords = {侧抑制机制, 图像识别, 目标检测, 脉冲神经网络, 视觉显著性},
}

@article{luo_supervised_2023,
	title = {Supervised {Learning} in {Multilayer} {Spiking} {Neural} {Networks} {With} {Spike} {Temporal} {Error} {Backpropagation}},
	volume = {34},
	issn = {2162-2388},
	url = {https://ieeexplore.ieee.org/abstract/document/9758943},
	doi = {10.1109/TNNLS.2022.3164930},
	abstract = {The brain-inspired spiking neural networks (SNNs) hold the advantages of lower power consumption and powerful computing capability. However, the lack of effective learning algorithms has obstructed the theoretical advance and applications of SNNs. The majority of the existing learning algorithms for SNNs are based on the synaptic weight adjustment. However, neuroscience findings confirm that synaptic delays can also be modulated to play an important role in the learning process. Here, we propose a gradient descent-based learning algorithm for synaptic delays to enhance the sequential learning performance of single spiking neuron. Moreover, we extend the proposed method to multilayer SNNs with spike temporal-based error backpropagation. In the proposed multilayer learning algorithm, information is encoded in the relative timing of individual neuronal spikes, and learning is performed based on the exact derivatives of the postsynaptic spike times with respect to presynaptic spike times. Experimental results on both synthetic and realistic datasets show significant improvements in learning efficiency and accuracy over the existing spike temporal-based learning algorithms. We also evaluate the proposed learning method in an SNN-based multimodal computational model for audiovisual pattern recognition, and it achieves better performance compared with its counterparts.},
	number = {12},
	urldate = {2024-04-27},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Luo, Xiaoling and Qu, Hong and Wang, Yuchen and Yi, Zhang and Zhang, Jilun and Zhang, Malu},
	month = dec,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Backpropagation, Biological system modeling, Delays, Heuristic algorithms, Membrane potentials, Neurons, Nonhomogeneous media, spike neural networks, spike neurons, supervised learning, synaptic delay plasticity},
	pages = {10141--10153},
}

@phdthesis{__2022-1,
	type = {博士},
	title = {脉冲神经网络编码和学习算法及应用研究},
	url = {https://kns.cnki.net/kcms2/article/abstract?v=Xhw-7KfLOFnVLFAoIqJ5QCrDpFOsbjT-dWIU2jOJjzPcUPcIRIPo-F7WWwS8tKb97kA8Vpz-jXboDjOhoc1hTImLpDR4YXMMQKlVzHMD-WXzTjIiMFfmlUE2sa6bc7Sc4o0XytobY_x75iRggBtxgQ==&uniplatform=NZKPT&language=CHS},
	abstract = {神经科学研究证明,生物神经网络之间的细胞是通过神经元的动作电位(发放的脉冲)来进行相互通信的。因此,更具有生物可塑性和强大信息处理能力的第三代神经网络:脉冲神经网络(杓杰杩杫杩杮杧李来杵杲条杬李来杴杷杯杲杫,杓李李)受到了研究者的广泛关注。与传统的基于频率的神经网络不同,杓李李能够处理和提取脉冲信号中编码的时间动态特征,从而使其更加具有生物可塑性,且计算能力更强,功耗更低,在未来的移动智能领域具有广阔的应用前景。每个脉冲神经元(杳杰杩杫杩杮杧杮来杵杲杯杮)的基本计算功能是将输入脉冲序列转化成适当的输出脉冲,充分模拟大脑中的脉冲时间编码原理。然而,具有脉冲信号特征的神经元如何产生大脑强大的认知功能仍然在探索中。目前的研究初步证明了脉冲神经元强大的计算能力,然而对杓李李的探索尚处于初级阶段,它缺乏有效的神经信息编码方法,并且由于编码的复杂性和脉冲变量的不可微性,无法使用现有的神经网络学习算法进行学习,使得学习算法的适用性无法保证,仍然面临很多挑战。因此,本文主要研究神经网络的信息处理和认知计算,试图揭示和利用生物系统的运作机制。时间编码和学习是杓李李的两个主要关注点,编码描述信息是如何被脉冲信号携带的,学习表现神经元是如何学习脉冲模式的。本论文的研究重点从神经元层面到系统层面各有不同,包括脉冲时间编码、单层和多层脉冲神经网络的学习、系统建模,以及视觉和听觉处理系统的应用发展。本文的主要内容和创新点如下。1.提出一个基于脉冲时间的声音特征编码和学习的鲁棒性识别模型。该模型将听觉编码和学习视为一个系统过程,不仅提高声音在噪声情况下的可辨识性,而且更好地模拟生物听觉皮层对声音信息的表达和学习过程。大多数传统的方法利用基于频率的特征来进行识别,导致声音和噪声信号混合在一起,缺乏鲁棒性。针对此问题,提出一个完全基于脉冲时间的声音识别模型去编码和学习从频谱图中提取到的声音特征。其选择高能量峰值作为关键点信息(其中包含局部时间札频率特征(杌杯杣条杬杔杩杭来札杆杲来東杵来杮杣杹,杌杔杆)),并编码成时间脉冲序列进行学习。为了研究其有效性,我们还将其应用到不同的杓李李学习算法中。基于该编码方法的新识别算法大大提高了噪声条件下的声音识别准确率和鲁棒性。实验结果表明,我们提出的编码和学习模型方法简单,抗噪性强。在不同的噪声情况下,性能优于基准方法。2.提出一个基于脉冲时间的视觉信息多脉冲编码和识别的统一性模型。该模型能够完全基于精确的脉冲...},
	language = {zh-CN},
	urldate = {2024-04-25},
	school = {四川大学},
	author = {肖, 蓉},
	year = {2022},
	doi = {10.27342/d.cnki.gscdu.2021.000043},
	keywords = {神经动力学, 神经形态编码, 脉冲学习, 脉冲时间, 脉冲神经网络},
}

@phdthesis{_stdp_2023,
	type = {博士},
	title = {{基于STDP和水库计算的脉冲神经网络算法研究}},
	url = {https://kns.cnki.net/kcms2/article/abstract?v=Xhw-7KfLOFkZYaYaAOEIQgAUg9nS7AlE22-eBbN3KYt8zDWairVd7hiknSCl6V1tA3Yr8qztQtsev4aeTFN4gmPAiuwNiUYi3nfQGO8xuw1QVSJZn_ZmYcPZJPI01gH5GjohQaUGrjHfkzsGyTwByQ==&uniplatform=NZKPT&language=CHS},
	abstract = {类脑计算是一条通过模仿生物神经网络的组织结构和学习机制来实现人工智能(Artificial Intelligence,简称AI)的技术路线。人脑可以仅用～1.2L的体积和～20W的功耗实现我们人类已知的最高级最普适的智能,通过模仿人脑的神经系统,我们有希望实现满足未来社会发展需求的高级智能。脉冲神经网络(Spiking Neural Network,简称SNN)是类脑计算最具代表性的算法,具有基于事件的异步计算方式、分布式的计算架构以及存储和计算的本地化等固有优势特性。但是,目前SNN的网络结构和算法应用还处在初步的探索阶段,存在诸多问题:在图像识别应用中,基于STDP(Spike Timing Dependent Plasticity)学习的SNN需要竞争学习机制辅助才能完成无监督学习,而现有的竞争学习机制会对SNN的学习效率和精度产生影响,竞争学习机制所需的额外连接和控制电路也极大地增加了SNN实现的复杂度;在面向DVS(Dynamic Vision Sensor)数据集的识别任务中,现有的SNN是基于深度卷积网络结构,不仅网络规模大,而且训练复杂度高,严重影响了SNN在功耗效率上的优势。针对以上问题,本文首先研究了竞争学习机制对基于STDP的SNN学习效率的影响,并提出了一套学习效率高、网络复杂度低的竞争学习机制实现方法,进而本文提出了基于抑制型突触更加简洁的竞争学习机制实现方法,不仅消除了传统方法对额外连接和控制电路的需求,而且实现了网络整体的全异步计算;LSM(Liquid State Machine)是一种基于水库计算的循环SNN,具有结构简单、训练复杂度低和适用于时空序列处理的优势,本文首次提出基于LSM的DVS数据集识别方案,并提出了两种优化技术和一个基于神经网络架构搜索的框架来提升LSM的性能,使得基于LSM的解决方案可以在实现可比精度的基础上将训练复杂度降低两个数量级。本文取得的研究成果如下:·研究发现了基于STDP实现无监督学习SNN中竞争学习机制对学习效率的影响,总结了传统实现竞争学习机制方法在实现复杂性上的劣势,提出了一套高学习效率、低复杂度的竞争学习机制实现方案;·提出了基于抑制型突触更加简洁的竞争学习机制实现方案,所提出的SNN中只需要脉冲神经元和突触两种单元就可以实现无监督学习,去除了传统方案中对同步操作的依赖,首次实现了无监督学习SNN的全异步计算特性;·提出了基于LSM的DVS...},
	language = {zh-CN},
	urldate = {2024-04-25},
	school = {国防科技大学},
	author = {曲, 连华},
	year = {2023},
	doi = {10.27052/d.cnki.gzjgu.2020.000289},
	keywords = {动态视觉传感器, 忆阻器, 无监督学习, 水库计算, 脉冲时序可塑性, 脉冲神经网络},
}

@misc{noauthor_-_nodate,
	title = {工信部：推动人工智能产业标准体系加快形成-新华网},
	url = {http://www.news.cn/info/20240118/4622f1c92b98469cb45864f8927b32a7/c.html},
	urldate = {2024-04-27},
	keywords = {/unread},
}

@misc{noauthor_scientific_nodate,
	title = {Scientific discovery in the age of artificial intelligence {\textbar} {Nature}},
	url = {https://www.nature.com/articles/s41586-023-06221-2},
	urldate = {2024-04-27},
	keywords = {/unread},
}

@misc{noauthor_embodied_nodate,
	title = {Embodied {AI}: {How} do {AI}-powered robots perceive the world? [+video]},
	shorttitle = {Embodied {AI}},
	url = {https://www.qualcomm.com/news/onq/2023/09/embodied-ai-how-do-ai-powered-robots-perceive-the-world},
	abstract = {At Qualcomm AI Research, we are working on applications of generative modelling to embodied AI and robotics, in order to enable more capabilities in robotics.},
	language = {en},
	urldate = {2024-04-27},
	keywords = {/unread},
}

@misc{noauthor_embodied_nodate-1,
	title = {Embodied {Intelligence}, {Sensors} and {Robotics} {\textbar} {Artificial} {Intelligence} @ {Columbia}},
	url = {https://ai.columbia.edu/embodied-intelligence-sensors-and-robotics},
	urldate = {2024-04-27},
	keywords = {/unread},
}

@misc{noauthor_projects_nodate,
	title = {Projects: {A} theory of embodied intelligence {\textbar} {Santa} {Fe} {Institute}},
	shorttitle = {Projects},
	url = {https://www.santafe.edu/research/projects/theory-of-embodied-intelligence},
	abstract = {We are building a theoretical framework that will guide the creation of artificial agents that adjust their neural networks (brains) to feedback from their bodies and surroundings -- in essence to learn how to navigate their surroundings.},
	language = {en},
	urldate = {2024-04-27},
	keywords = {/unread},
}

@misc{noauthor_ai_nodate,
	title = {{AI} for {Scientific} {Discovery}: {From} {Theory} to {Practice}},
	shorttitle = {{AI} for {Scientific} {Discovery}},
	url = {https://ai4sciencecommunity.github.io/neurips23},
	language = {en-US},
	urldate = {2024-04-27},
	journal = {AI for Science},
	keywords = {/unread},
}

@article{__2023,
	title = {基于形态的具身智能研究: 历史回顾与前沿进展},
	volume = {49},
	issn = {0254-4156},
	shorttitle = {基于形态的具身智能研究},
	url = {http://www.aas.net.cn/cn/article/doi/10.16383/j.aas.c220564},
	doi = {10.16383/j.aas.c220564},
	number = {6},
	urldate = {2024-04-27},
	journal = {自动化学报},
	author = {刘华平 and 郭迪 and 孙富春 and 张新钰},
	month = jun,
	year = {2023},
	note = {Publisher: 自动化学报},
	keywords = {/unread},
	pages = {1131--1154},
}

@article{bandi_power_2023,
	title = {The {Power} of {Generative} {AI}: {A} {Review} of {Requirements}, {Models}, {Input}–{Output} {Formats}, {Evaluation} {Metrics}, and {Challenges}},
	volume = {15},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1999-5903},
	shorttitle = {The {Power} of {Generative} {AI}},
	url = {https://www.mdpi.com/1999-5903/15/8/260},
	doi = {10.3390/fi15080260},
	abstract = {Generative artificial intelligence (AI) has emerged as a powerful technology with numerous applications in various domains. There is a need to identify the requirements and evaluation metrics for generative AI models designed for specific tasks. The purpose of the research aims to investigate the fundamental aspects of generative AI systems, including their requirements, models, input–output formats, and evaluation metrics. The study addresses key research questions and presents comprehensive insights to guide researchers, developers, and practitioners in the field. Firstly, the requirements necessary for implementing generative AI systems are examined and categorized into three distinct categories: hardware, software, and user experience. Furthermore, the study explores the different types of generative AI models described in the literature by presenting a taxonomy based on architectural characteristics, such as variational autoencoders (VAEs), generative adversarial networks (GANs), diffusion models, transformers, language models, normalizing flow models, and hybrid models. A comprehensive classification of input and output formats used in generative AI systems is also provided. Moreover, the research proposes a classification system based on output types and discusses commonly used evaluation metrics in generative AI. The findings contribute to advancements in the field, enabling researchers, developers, and practitioners to effectively implement and evaluate generative AI models for various applications. The significance of the research lies in understanding that generative AI system requirements are crucial for effective planning, design, and optimal performance. A taxonomy of models aids in selecting suitable options and driving advancements. Classifying input–output formats enables leveraging diverse formats for customized systems, while evaluation metrics establish standardized methods to assess model quality and performance.},
	language = {en},
	number = {8},
	urldate = {2024-04-27},
	journal = {Future Internet},
	author = {Bandi, Ajay and Adapa, Pydi Venkata Satya Ramesh and Kuchi, Yudu Eswar Vinay Pratap Kumar},
	month = aug,
	year = {2023},
	note = {Number: 8
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {/unread, AIGC, AIGC models, ChatGPT, GPT-3, GPT-4, generative AI models, generative AI survey, generative adversarial networks, transformers, user experience},
	pages = {260},
}

@misc{gozalo-brizuela_chatgpt_2023,
	title = {{ChatGPT} is not all you need. {A} {State} of the {Art} {Review} of large {Generative} {AI} models},
	url = {http://arxiv.org/abs/2301.04655},
	doi = {10.48550/arXiv.2301.04655},
	abstract = {During the last two years there has been a plethora of large generative models such as ChatGPT or Stable Diffusion that have been published. Concretely, these models are able to perform tasks such as being a general question and answering system or automatically creating artistic images that are revolutionizing several sectors. Consequently, the implications that these generative models have in the industry and society are enormous, as several job positions may be transformed. For example, Generative AI is capable of transforming effectively and creatively texts to images, like the DALLE-2 model; text to 3D images, like the Dreamfusion model; images to text, like the Flamingo model; texts to video, like the Phenaki model; texts to audio, like the AudioLM model; texts to other texts, like ChatGPT; texts to code, like the Codex model; texts to scientific texts, like the Galactica model or even create algorithms like AlphaTensor. This work consists on an attempt to describe in a concise way the main models are sectors that are affected by generative AI and to provide a taxonomy of the main generative models published recently.},
	urldate = {2024-04-27},
	publisher = {arXiv},
	author = {Gozalo-Brizuela, Roberto and Garrido-Merchan, Eduardo C.},
	month = jan,
	year = {2023},
	note = {arXiv:2301.04655 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{_ai_2023,
	title = {{AI} for {Science}：科学研究范式的新革命},
	issn = {1000-114X},
	url = {https://kns.cnki.net/kcms2/article/abstract?v=29axctaKF3wlKzzXhVZSp11ioHFpq3_3lQUTCiyxTky0V97W1eZPxSQUUY0kw7l_OwtfZOEscJBx8bFEn4G2t2EmT3AOXNrryriCiHAqniEyLth3QIREx8QcjcMOKAT_5_aEDOLp155VrZlCtbdi6A==&uniplatform=NZKPT&language=CHS},
	abstract = {人工智能驱动的科学研究（AI for Science）是大数据时代以机器学习为代表的智能技术与科学研究深度融合的产物，在物理科学、生命科学等多领域科研场景中已经取得突破性进展。以AI支撑前沿科技发展的新模式正在改变科学研究的方式，加速科学发现的进程，引发了科学研究范式的新革命，并产生广泛的社会影响。作为新兴的科学问题解决方案，AI for Science目前仍面临诸多困境，亟需国家政策支撑与平台建设，通过实现项目、平台、人才等维度的跨领域融合与重构，以更好地发挥AI在科研领域的渗透性与颠覆性力量。},
	language = {zh-CN},
	number = {6},
	urldate = {2024-04-27},
	journal = {广东社会科学},
	author = {李, 建会 and 杨, 宁},
	year = {2023},
	keywords = {/unread, 人工智能, 机器学习, 科学研究, 科技伦理},
	pages = {81--92+289--290},
}

@article{_ai_2023-1,
	title = {{探讨AI} for {Science的影响与意义}：现状与展望},
	volume = {5},
	issn = {2096-6652},
	url = {https://kns.cnki.net/kcms2/article/abstract?v=29axctaKF3xWv3e15QFtKZ3jRdZvLsMYnawjkxH-Qd5AnF2GY6H6SRdBcxn0z6oQ3WB-oswFY93GRiKlGVnVMnd5IPO6rGCEFFCXNenwSaaoZ34W8C8r6_IgpcFkzv5qxv5gzG7VT7T3V-5tmwUk2g==&uniplatform=NZKPT&language=CHS},
	abstract = {以Chat GPT为代表的新一轮人工智能技术浪潮正推动人类社会全面变革，科学研究范式正加速转换，一场人工智能驱动的科学研究（AIforScience,AI4S）革命正在到来。分析了AI4S的基本概念和特点，从数学、物理、生物、材料等角度简要综述了AI4S的发展现状。大力发展AI4S对提高国家竞争力、发展社会经济、加强技术储备都具有十分重要的意义。为更好地推动我国AI4S的发展，以下两点十分关键：一是变革当代的教学与教育，倡导AI for Education和Education for AI；二是以DAOs和DeSci为基础建立适应“新科学研究范式”的“新组织方式”和“新科研生态”，为AI4S研究提供公开、公平、公正的可持续性支持。},
	language = {zh-CN},
	number = {1},
	urldate = {2024-04-27},
	journal = {智能科学与技术学报},
	author = {王, 飞跃 and 缪, 青海 and 张, 军平 and 郑, 文博 and 丁, 文文},
	year = {2023},
	keywords = {/unread, 人工智能驱动的科学研究, 分布式自主科学, 分布式自治组织与运营},
	pages = {1--6},
}

@article{_ai4s_2023,
	title = {人工智能驱动的科学研究新范式：从{AI4S到智能科学}},
	volume = {38},
	issn = {1000-3045},
	url = {https://kns.cnki.net/kcms2/article/abstract?v=29axctaKF3yPMzGYcdi9DUqcWg5B_nWFIdKKlFex2SFL8oONvEf5q2XEZfNngyQtMq3g4FLIqafd0yMhFbLakSuo0pmWMRG5fyjroHGt4NxkUoPmxdTu_PZ9nqXdwP7YkBLIk_KcS0xGl7LxtNBd-Q==&uniplatform=NZKPT&language=CHS},
	doi = {10.16418/j.issn.1000-3045.20230406002},
	abstract = {近期，以ChatGPT为代表的大模型技术正开启人类社会智能化的新纪元。研究人工智能成功案例背后的技术原理，探索人工智能驱动的科学研究（AI for Science,AI4S）新范式，对促进我国科技进步、增强国家竞争力具有十分重要的意义。文章首先以数学、物理学、生物学、材料科学领域为例，简述AI4S的研究进展。其次，面向近年来最为成功的人工智能范例，分析AlphaFold和ChatGPT的基本原理和关键技术。最后，在以上分析的基础上，从算法、模型、数据、知识、人的因素等角度，总结大模型时代人工智能技术发展新趋势，探讨AI4S研究新范式。},
	language = {zh-CN},
	number = {4},
	urldate = {2024-04-27},
	journal = {中国科学院院刊},
	author = {王, 飞跃 and 缪, 青海},
	year = {2023},
	keywords = {/unread, ChatGPT, 人工智能, 人工智能生成内容(AIGC), 人工智能驱动的科学研究, 工业5.0, 平行系统, 智能科学, 第五范式},
	pages = {536--540},
}

@article{__2023-1,
	title = {具身智能或将引领人工智能下一波浪潮},
	issn = {1672-9781},
	url = {https://kns.cnki.net/kcms2/article/abstract?v=29axctaKF3zXf1j6gRbMwHempQ0EqWNKP8loL52MVFdc2N06m5KvDR3Tkbs-3N3iAGY-DtkPPHCUNpK8brqUdF6AHIwJYGuMf4lLlB3g33OFyKsm_QMa--q8GQ_37uZ0EOz8odyYshM=&uniplatform=NZKPT&language=CHS},
	abstract = {{\textless}正{\textgreater}当前，具身智能(Embodied Artificial Intelligence)作为人工智能领域的一个分支，正在成为学术界和产业界备受关注的一个焦点。所谓具身智能Embodied AI，指的是有身体并支持物理交互的智能体。英伟达（NVIDIA）创始人兼CEO黄仁勋在ITF World 2023半导体大会上称，具身智能将引领下一波人工智能浪潮。在产业界，谷歌DeepMind推出首个控制机器人的视觉语言动作（VLA）模型RT-2；凭借ChatGPT取得巨大成功的OpenAI，曾经解散机器人团队，如今投资挪威机器人初创公司One X Technologies，推出名为Neo的新型人工智能机器人。国内政策也在推进具身智能的发展。},
	language = {zh-CN},
	number = {8},
	urldate = {2024-04-27},
	journal = {中国教育网络},
	author = {余, 秀},
	year = {2023},
	keywords = {/unread, 人工智能},
	pages = {3--5},
}

@article{__2023-2,
	title = {具身智能产业发展动向及创新能力研究},
	issn = {2096-062X},
	url = {https://kns.cnki.net/kcms2/article/abstract?v=29axctaKF3zI8h8yYKFLobhFwGBjMGPFLj7Qo6M0ZwDcCm8wJ3cJbhn-R0d7tgLLGds3UgRJyUytoR4yQQ7iHjfSrf9lwz18BVdx0PFGgUqsLqSxePNiqIqv1OXg8DVGQz0NLlsQltSe8Xfan4WmvQ==&uniplatform=NZKPT&language=CHS},
	doi = {10.19609/j.cnki.cn10-1339/tn.2023.11.010},
	abstract = {{\textless}正{\textgreater}具身智能具备巨大的市场潜力和广泛的应用前景。从目前的应用场景来看，得益于大模型技术的突破,智能助手实现优先普及，因此，当下智能家居的市场规模较大。未来，随着L4级别自动驾驶的落地及新能源汽车的加速普及，自动驾驶可能会发展为具身智能应用的主要板块。党的二十大报告对建设现代化产业体系作出部署，强调“推动战略性新兴产业融合集群发展，构建新一代信息技术、人工智能、生物技术、新能源、新材料、高端装备、绿色环保等一批新的增长引擎”。当前，以大模型、类脑智能、具身智能、价值与因果驱动的通用智能体为代表的通用人工智能发展路径初现端倪，具身智能有望推进新一代人工智能实现跨越式发展，激发新一轮的技术和产业革命。},
	language = {zh-CN},
	number = {11},
	urldate = {2024-04-27},
	journal = {软件和集成电路},
	author = {钟, 新龙 and 渠, 延增 and 王, 聪聪 and 樊, 炳辰 and 窦, 婉茹},
	year = {2023},
	keywords = {/unread, 人工智能领域, 人形机器人, 具身化, 智能产业, 智能系统, 物理实体},
	pages = {62--73},
}

@article{_ai_2024,
	title = {{AI大模型与具身智能终将相遇}},
	issn = {2096-0182},
	url = {https://kns.cnki.net/kcms2/article/abstract?v=29axctaKF3yPagzsq3PQCBrK5zKMJl46yUm6MToSL6YHO0Koen6Fs05SgDXNw9l5N--5_8ubokpygmyDT7fRjODOxNDuPtW_4o_QYvdytET6WQKv7xbSa2TZEe1OOLvnui4Tu_N_BWY=&uniplatform=NZKPT&language=CHS},
	doi = {10.19609/j.cnki.cn10-1324/tp.2024.02.012},
	abstract = {当科技圈还沉浸在文生视频大模型Sora带来的震撼中时，美国人形机器人初创公司机器人Figure 01通过接入OpenAI大模型，已经能够与人类对话，理解并执行人类指令。此外，OpenAI正在对GPT-5开展红队测试，据内测者透露，GPT-5在性能上再次实现大飞升。可以预测，人形机器人将随着GPT-5的更新迎来新一轮爆发式增长。},
	language = {zh-CN},
	number = {2},
	urldate = {2024-04-27},
	journal = {机器人产业},
	author = {杨, 雨彤},
	year = {2024},
	keywords = {/unread, GPT, 人形机器人},
	pages = {71--74},
}

@article{__2023-3,
	title = {学科交叉与设计创新研究进展},
	volume = {41},
	issn = {1000-7857},
	url = {https://kns.cnki.net/kcms2/article/abstract?v=29axctaKF3ztWnFRgjuB9aD9vyW8MAjP2jgLm5O74J_a_XU03a0tjZgLtwwBiWdICOt8OtiqMX_-UVLDV9bg7vAQkwBrrlJvXVftTM3j9E0UdfIFgWD6B4ND6CcvyzBPcNIQYMzOoBwTLTCjdwjpXg==&uniplatform=NZKPT&language=CHS},
	abstract = {多学科交叉与融合是科学研究的重要特征之一，特别是在当今其更是科技发展和社会创新的必由之路。清华大学未来实验室从成立伊始就致力于打破学科壁垒，开展学科交叉与设计创新领域的前沿探索。结合未来实验室的几个学科交叉研究探索，介绍了基于人工智能的通用嗅觉计算理论与方法研究、未来材料与设计研究、智慧人居设计创新研究、无障碍设计创新研究、适老化服务设计研究、终身学习的理论与方法研究等相关领域的主要研究内容、国内外研究现状、未来实验室目前的研究进展以及对未来的展望和建议。},
	language = {zh-CN},
	number = {8},
	urldate = {2024-04-27},
	journal = {科技导报},
	author = {徐, 迎庆 and 王, 韫 and 付, 心仪 and 焦, 阳 and 杨, 佳伟 and 路, 奇 and 陈, 迪 and 乐, 恢榕},
	year = {2023},
	keywords = {/unread, 学科交叉, 未来实验室, 设计创新},
	pages = {17--25},
}

@misc{zhang_scientific_2024,
	title = {Scientific {Large} {Language} {Models}: {A} {Survey} on {Biological} \& {Chemical} {Domains}},
	shorttitle = {Scientific {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2401.14656},
	doi = {10.48550/arXiv.2401.14656},
	abstract = {Large Language Models (LLMs) have emerged as a transformative power in enhancing natural language comprehension, representing a significant stride toward artificial general intelligence. The application of LLMs extends beyond conventional linguistic boundaries, encompassing specialized linguistic systems developed within various scientific disciplines. This growing interest has led to the advent of scientific LLMs, a novel subclass specifically engineered for facilitating scientific discovery. As a burgeoning area in the community of AI for Science, scientific LLMs warrant comprehensive exploration. However, a systematic and up-to-date survey introducing them is currently lacking. In this paper, we endeavor to methodically delineate the concept of "scientific language", whilst providing a thorough review of the latest advancements in scientific LLMs. Given the expansive realm of scientific disciplines, our analysis adopts a focused lens, concentrating on the biological and chemical domains. This includes an in-depth examination of LLMs for textual knowledge, small molecules, macromolecular proteins, genomic sequences, and their combinations, analyzing them in terms of model architectures, capabilities, datasets, and evaluation. Finally, we critically examine the prevailing challenges and point out promising research directions along with the advances of LLMs. By offering a comprehensive overview of technical developments in this field, this survey aspires to be an invaluable resource for researchers navigating the intricate landscape of scientific LLMs.},
	urldate = {2024-04-26},
	publisher = {arXiv},
	author = {Zhang, Qiang and Ding, Keyang and Lyv, Tianwen and Wang, Xinda and Yin, Qingyu and Zhang, Yiwen and Yu, Jing and Wang, Yuhao and Li, Xiaotong and Xiang, Zhuoyi and Zhuang, Xiang and Wang, Zeyuan and Qin, Ming and Zhang, Mengyao and Zhang, Jinlu and Cui, Jiyu and Xu, Renjun and Chen, Hongyang and Fan, Xiaohui and Xing, Huabin and Chen, Huajun},
	month = jan,
	year = {2024},
	note = {arXiv:2401.14656 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{xu_artificial_2021,
	title = {Artificial intelligence: {A} powerful paradigm for scientific research},
	volume = {2},
	issn = {2666-6758},
	shorttitle = {Artificial intelligence},
	url = {https://www.sciencedirect.com/science/article/pii/S2666675821001041},
	doi = {10.1016/j.xinn.2021.100179},
	abstract = {Artificial intelligence (AI) coupled with promising machine learning (ML) techniques well known from computer science is broadly affecting many aspects of various fields including science and technology, industry, and even our day-to-day life. The ML techniques have been developed to analyze high-throughput data with a view to obtaining useful insights, categorizing, predicting, and making evidence-based decisions in novel ways, which will promote the growth of novel applications and fuel the sustainable booming of AI. This paper undertakes a comprehensive survey on the development and application of AI in different aspects of fundamental sciences, including information science, mathematics, medical science, materials science, geoscience, life science, physics, and chemistry. The challenges that each discipline of science meets, and the potentials of AI techniques to handle these challenges, are discussed in detail. Moreover, we shed light on new research trends entailing the integration of AI into each scientific discipline. The aim of this paper is to provide a broad research guideline on fundamental sciences with potential infusion of AI, to help motivate researchers to deeply understand the state-of-the-art applications of AI-based fundamental sciences, and thereby to help promote the continuous development of these fundamental sciences.},
	number = {4},
	urldate = {2024-04-26},
	journal = {The Innovation},
	author = {Xu, Yongjun and Liu, Xin and Cao, Xin and Huang, Changping and Liu, Enke and Qian, Sen and Liu, Xingchen and Wu, Yanjun and Dong, Fengliang and Qiu, Cheng-Wei and Qiu, Junjun and Hua, Keqin and Su, Wentao and Wu, Jian and Xu, Huiyu and Han, Yong and Fu, Chenguang and Yin, Zhigang and Liu, Miao and Roepman, Ronald and Dietmann, Sabine and Virta, Marko and Kengara, Fredrick and Zhang, Ze and Zhang, Lifu and Zhao, Taolan and Dai, Ji and Yang, Jialiang and Lan, Liang and Luo, Ming and Liu, Zhaofeng and An, Tao and Zhang, Bin and He, Xiao and Cong, Shan and Liu, Xiaohong and Zhang, Wei and Lewis, James P. and Tiedje, James M. and Wang, Qi and An, Zhulin and Wang, Fei and Zhang, Libo and Huang, Tao and Lu, Chuan and Cai, Zhipeng and Wang, Fang and Zhang, Jiabao},
	month = nov,
	year = {2021},
	keywords = {artificial intelligence, chemistry, deep learning, geoscience, information science, life science, machine learning, materials science, mathematics, medical science, physics},
	pages = {100179},
}

@article{__nodate,
	title = {基于脉冲神经网络的智能控制研究进展},
	issn = {1000-8152},
	url = {https://kns.cnki.net/kcms2/article/abstract?v=Xhw-7KfLOFnNWv9OtRgOKuE62Gz8Q8IKS1bK2OlZBtkXNTsnDBbE8e4KPyTDhVlhleSQ4Rq9D7-kQQIJzZ018o75T8SIUdJ59HfavpgpMBietEYbXLT3xgmbl01gSphaI1kGc92J1h0=&uniplatform=NZKPT&language=CHS},
	abstract = {近些年,具备低功耗、高鲁棒、融合时空信息等优势的脉冲神经网络(spiking neural network,SNN)在类脑研究与智能控制的交叉领域方兴未艾.基于脉冲神经网络架构的智能控制方法是实现与环境自主交互并且高能效完成复杂控制任务的有效途径之一.为此,本文首先介绍了SNN的基本要素与研究动机;然后详细介绍了近年来基于脉冲神经网络智能控制的研究进展以及在机器人、无人车、无人机等领域的应用情况;接着总结了一些现有的硬件平台,用以实现SNN算法的高效能实现.最后,总结展望了SNN控制发展的机遇与挑战.本文旨在梳理出SNN控制发展的技术脉络,为其快速发展提供借鉴与思路.},
	language = {zh-CN},
	urldate = {2024-04-25},
	journal = {控制理论与应用},
	author = {刘, 晓德 and 郭, 宇飞 and 黄, 旭辉 and 马, 喆},
	keywords = {/unread, 深度学习, 神经形态计算, 神经网络与智能控制, 脉冲神经网络},
	pages = {1--18},
}

@misc{noauthor__nodate,
	title = {基于脉冲神经网络的智能控制研究进展 - 中国知网},
	url = {https://kns.cnki.net/kcms2/article/abstract?v=Xhw-7KfLOFnNWv9OtRgOKuE62Gz8Q8IKS1bK2OlZBtkXNTsnDBbE8e4KPyTDhVlhleSQ4Rq9D7-kQQIJzZ018o75T8SIUdJ59HfavpgpMBietEYbXLT3xgmbl01gSphaI1kGc92J1h0=&uniplatform=NZKPT&language=CHS},
	urldate = {2024-04-25},
	keywords = {/unread},
}

@article{de_meyer_attention_2000,
	title = {Attention through self-synchronisation in the spiking neuron stochastic diffusion network},
	volume = {9},
	issn = {1053-8100},
	url = {https://centaur.reading.ac.uk/27176/},
	abstract = {The paper discusses ensemble behaviour in the Spiking Neuron Stochastic Diffusion Network, SNSDN, a novel network exploring biologically plausible information processing based on higher order temporal coding. SNSDN was proposed as an alternative solution to the binding problem [1]. SNSDN operation resembles Stochastic Diffusin on Search, SDS, a non-deterministic search algorithm able to rapidly locate the best instantiation of a target pattern within a noisy search space ([3], [5]). In SNSDN, relevant information is encoded in the length of interspike intervals. Although
every neuron operates in its own time, ‘attention’ to a pattern in the search space results in self-synchronised activity of a large population of neurons. When multiple patterns are present in the search space, ‘switching of at-
tention’ results in a change of the synchronous activity. The qualitative effect of attention on the synchronicity of spiking behaviour in both time and frequency domain will be discussed.},
	number = {2},
	urldate = {2024-04-25},
	journal = {Consciousness and Cognition},
	author = {De Meyer, K. and Nasuto, Slowomir},
	year = {2000},
	note = {Number: 2
Publisher: Elsevier},
	keywords = {/unread},
	pages = {81},
}

@inproceedings{hammouamri_learning_2023,
	title = {Learning {Delays} in {Spiking} {Neural} {Networks} using {Dilated} {Convolutions} with {Learnable} {Spacings}},
	url = {https://openreview.net/forum?id=4r2ybzJnmN},
	abstract = {Spiking Neural Networks (SNNs) are a promising research direction for building power-efficient information processing systems, especially for temporal tasks such as speech recognition. In SNNs, delays refer to the time needed for one spike to travel from one neuron to another. These delays matter because they influence the spike arrival times, and it is well-known that spiking neurons respond more strongly to coincident input spikes. More formally, it has been shown theoretically that plastic delays greatly increase the expressivity in SNNs. Yet, efficient algorithms to learn these delays have been lacking. Here, we propose a new discrete-time algorithm that addresses this issue in deep feedforward SNNs using backpropagation, in an offline manner. To simulate delays between consecutive layers, we use 1D convolutions across time. The kernels contain only a few non-zero weights – one per synapse – whose positions correspond to the delays. These positions are learned together with the weights using the recently proposed Dilated Convolution with Learnable Spacings (DCLS). We evaluated our method on three datasets: the Spiking Heidelberg Dataset (SHD), the Spiking Speech Commands (SSC) and its non spiking version Google Speech Commands v0.02 (GSC) benchmarks, which require detecting temporal patterns. We used feedforward SNNs with two or three hidden fully connected layers, and vanilla leaky integrate-and-fire neurons. We showed that fixed random delays help and that learning them helps even more. Furthermore, our method outperformed the state-of-the-art in the three datasets without using recurrent connections and with substantially fewer parameters. Our work demonstrates the potential of delay learning in developing accurate and precise models for temporal data processing. Our code is based on PyTorch / SpikingJelly and available at: https://github.com/Thvnvtos/SNN-delays},
	language = {en},
	urldate = {2024-04-25},
	author = {Hammouamri, Ilyass and Khalfaoui-Hassani, Ismail and Masquelier, Timothée},
	month = oct,
	year = {2023},
	keywords = {/unread},
}

@article{perez-nieves_neural_2021,
	title = {Neural heterogeneity promotes robust learning},
	volume = {12},
	copyright = {2021 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-021-26022-3},
	doi = {10.1038/s41467-021-26022-3},
	abstract = {The brain is a hugely diverse, heterogeneous structure. Whether or not heterogeneity at the neural level plays a functional role remains unclear, and has been relatively little explored in models which are often highly homogeneous. We compared the performance of spiking neural networks trained to carry out tasks of real-world difficulty, with varying degrees of heterogeneity, and found that heterogeneity substantially improved task performance. Learning with heterogeneity was more stable and robust, particularly for tasks with a rich temporal structure. In addition, the distribution of neuronal parameters in the trained networks is similar to those observed experimentally. We suggest that the heterogeneity observed in the brain may be more than just the byproduct of noisy processes, but rather may serve an active and important role in allowing animals to learn in changing environments.},
	language = {en},
	number = {1},
	urldate = {2024-04-25},
	journal = {Nature Communications},
	author = {Perez-Nieves, Nicolas and Leung, Vincent C. H. and Dragotti, Pier Luigi and Goodman, Dan F. M.},
	month = oct,
	year = {2021},
	note = {Publisher: Nature Publishing Group},
	keywords = {/unread, Electrical and electronic engineering, Learning algorithms, Network models},
	pages = {5791},
}

@phdthesis{__2024,
	type = {博士},
	title = {新型脉冲神经网络模型及其算法研究},
	url = {https://kns.cnki.net/kcms2/article/abstract?v=Xhw-7KfLOFn61evm3qLv_yf5DHTsePgwMYSoPXFzk3S8_gIxy3B6vkq5WUamVwkvHNt4jqAM3kQ0PX5ghymoYbDxZpQV3cEHH8wGagme7B3ZiY6939PRM1w_ZdCMmLXE7QEzCg9gPTK55Vc6Nacx9g==&uniplatform=NZKPT&language=CHS},
	abstract = {深度学习成功借鉴大脑处理信息的层级结构,在智能应用领域取得了重要突破和广泛应用。但现有深度网络模型需要大量的计算资源,无法大规模部署在功耗和成本敏感的应用中。此外,现有深度网络模型在智能水平方面距离人脑还有很大差距。为了解决上述问题,受脑启发的脉冲神经网络提供了一种新的思路。脉冲神经网络用离散的二值脉冲表征信息,具有丰富的神经动力学特性、强大的时空信息处理能力和在神经形态芯片上超低功耗、超低延迟运行的优势,孕育着巨大的创新可能性。但是,由于复杂的时空依赖关系和脉冲发放不可微分等特性,当前脉冲神经网络还缺乏高效的网络模型和学习算法,限制了其实际应用性能,因此本文从算法和模型两方面展开研究,主要创新点如下:(1)针对前馈脉冲神经网络中序列学习效率低下的问题,提出一种受膜电压驱动的脉冲神经网络序列学习算法FE-Learn(First Error Learning)。不同于一般序列学习算法需要精确复刻期望脉冲序列,FE-Learn只需要在每个期望脉冲附近的小窗口内激发脉冲,这种对目标输出的放宽极大提高了其鲁棒性。此外,FE-Learn在每轮学习中针对性地调整第一个错误脉冲的膜电压,使神经元能快速激发期望的脉冲序列。同时,FE-Learn将突触延迟可塑性引入参数学习中,提高了在稀疏输入条件下的学习能力,而进一步将其扩展到多层网络也扩大了其现实应用范围。实验证明,相比于其他序列学习算法,FE-Learn具有更高的学习效率及更好的鲁棒性。(2)针对神经科学和机器学习领域长期研究的时间信用分配(Temporal Credit-Assignment,TCA)问题,提出一种高效的受阈值驱动的脉冲神经网络聚合标签学习算法ETDP(Efficient Threshold-Driven Plasticity)。ETDP以多种有效线索对应的期望输出脉冲数之和作为标签,通过脉冲-阈值映射关系中的临界阈值更新权重。实验表明,ETDP能探测并弥补感官信号的发生与反馈信号的到达之间的差距,将聚合标签分配给嵌入在背景活动中的有用线索,有效处理TCA问题。此外,ETDP采用预防梯度爆炸的策略提高算法性能。实验结果表明ETDP具有比其他阈值驱动的聚合标签学习算法更高的学习效率。(3)针对具有可变时间间隔的时空序列记忆问题,提出一种受生物神经柱启发的脉冲神经网络模型CSTM(Column-based Spatial-Temporal Memory)。现有序...},
	language = {zh-CN},
	urldate = {2024-04-25},
	school = {电子科技大学},
	author = {罗, 笑玲},
	year = {2024},
	doi = {10.27005/d.cnki.gdzku.2023.000189},
	keywords = {序列学习, 序列记忆模型, 序列预测模型, 聚合标签学习, 脉冲神经网络},
}

@phdthesis{__2019,
	type = {博士},
	title = {脉冲神经网络相对顺序学习与转化算法研究},
	url = {https://kns.cnki.net/kcms2/article/abstract?v=Xhw-7KfLOFkj1wMd8sl1nAYirsIDMQgGJPlWXiOgstcP8-lBQqLlKf5Lsyoe8HoLVFcxzbmT4EO-Af2hx-NtR08YfM_cXWo7_l_aSMau5VKDk1QQsND2YpqJ8phnTF5kjuKd8QRKGdi1r3_ZbbWi7w==&uniplatform=NZKPT&language=CHS},
	abstract = {作为第三代人工神经网络,脉冲神经网络具有更高的生物似真性和计算效能,是未来类脑计算及人工智能的重要研究领域之一。但由于脉冲编码、计算等相关理论的缺乏,现有的学习算法训练效率仍然较低,效果也不令人满意,导致目前脉冲神经网络的实用性不高。发展更高效的学习算法,以及将训练好的深度学习模型直接转化为脉冲模型是提高脉冲神经网络性能的两种重要途径。本文围绕提高脉冲神经网络实用性的问题,重点研究了基于脉冲相对顺序的新型学习算法和低延时、低成本的脉冲转化方法,主要的研究内容和创新点包括:1.以脉冲相对顺序为指导的脉冲神经网络学习算法。现有的脉冲神经网络使用人为精确设定的固定脉冲为学习目标,存在潜在的输入模式与学习目标不匹配的问题。本文提出了以脉冲相对顺序为指导的学习算法,不需要设计具体精确的脉冲作为学习目标,以输出层神经元的脉冲顺序为监督信号指导突触权重的更新。针对神经元的不同状态,使用不同的误差函数,提高了神经元的学习能力和效率。丰富的实验验证了脉冲相对顺序学习算法的高鲁棒性,高效的学习效率及优良的泛化能力。最后,将该算法训练的脉冲网络与传统人工神经网络模型进行对比。相同网络规模的条件下,脉冲模型的识别性能与MLP模型相当,但推理时只需要MLP模型15\%的运算量,凸显了脉冲机制的高能效特性。2.基于激活值量化的低推理延时的脉冲神经网络转化方法。传统方法转化深度神经网络得到的脉冲模型需要较长的仿真时间产生大量的脉冲来模拟源网络的输入输出关系,以获得与源网络相当的识别性能。通过分析深度学习模型激活函数与脉冲神经元等效替换的内在要求,发现脉冲神经网络推理延时与激活值精度存在正相关关系,提出了基于激活值量化的深度脉冲神经网络实现方法。针对激活值量化,提出了基于再训练的逐层量化算法,通过等间隔扫描和L2量化误差最小化方法获得最佳的量化分辨率,并进行逐层量化和再训练,得到了性能基本无损的量化网络模型。以量化网络模型为基础进行脉冲转化,只需要少量脉冲就能精确模拟激活值之间的比例关系,减少了脉冲网络的推理延时,降低了网络负载。3.卷积神经网络池化层及softmax层的脉冲转化方法。卷积神经网络的脉冲化是提高其实时性及能效的重要途径。针对现有的转化方法实现复杂,成本较大的缺陷,提出了池化融合技术和softmax层的直接映射方法。通过衰减卷积层权重的方式将卷积层脉冲标定为池化脉冲,将平均池化功能融合进了卷积层脉冲神经元;通过增加卷积层池化区域神经...},
	language = {zh-CN},
	urldate = {2024-04-25},
	school = {浙江大学},
	author = {林, 志涛},
	year = {2019},
	keywords = {平均池化融合, 最大值池化融合, 激活值量化, 监督学习, 相对顺序学习, 脉冲神经网络, 脉冲转化, 逐层再训练},
}

@article{__2022,
	title = {基于类脑脉冲神经网络的符号表征与推理},
	copyright = {中国科学院自动化研究所机构知识库},
	url = {http://ir.ia.ac.cn/handle/173211/49912},
	abstract = {{\textless}p{\textgreater}\&nbsp; \&nbsp; \&nbsp; Based on current research results in neuroscience on symbolic representation and reasoning, this paper constructs a multi-loop collaborative brain-inspired spiking neural network model, and systematically discusses and investigates how spiking neural networks accomplish symbolic representation and reasoning tasks. In terms of model function and structure, this paper draws on the brain\&\#39;s function and connectivity structure at the microscopic, mesoscopic and macroscopic scales respectively. At the microscopic scale, the paper adopts a computational model of biological neuron-like neurons and introduces spiking timing-dependent plasticity mechanisms for synapses as well as reward-modulated spike timing-dependent plasticity mechanisms to build the micro-structure of neural networks. At the mesoscopic scale, inspired by the population coding mechanism, this paper uses neuron populations to complete the representation of symbols, and through learning and memory to form stable synaptic connections between different neuronal populations, a multi-loop synergistic brain-inspired spiking neural network is constructed to achieve the task of symbol sequence representation and reconstruction. At the macro level, the paper elaborates that data-driven deep neural networks lack causal reasoning and are fundamentally different from human intelligence, and explores the feasibility of brain-inspired spiking neural networks for causal reasoning at the macro functional level.{\textless}/p{\textgreater} 
 
{\textless}p{\textgreater}{\textless}br /{\textgreater} 
\&nbsp; \&nbsp; \&nbsp; The main work and innovations of this paper are summarized as follows:{\textless}/p{\textgreater} 
 
{\textless}p{\textgreater}\&nbsp; \&nbsp; \&nbsp; First, \&nbsp;Brain-inspired Sequence Production Spiking Neural Network (SP-SNN). Inspired by the related research on how macaques complete symbolic representation and symbolic sequence production in neuroscience, this paper proposes a multi-loop coordinated brain-inspired symbolic sequence production spiking neural network to simulate the same symbolic sequence memory and production process as in macaques. After experimental verification, it is proved that SP-SNN can complete the working memory of the symbol sequence by fusing the population coding representation mechanism and the spiking timing-dependent plasticity mechanism (STDP). Moreover, by introducing the reward-modulated spike timing-dependent plasticity (R-STDP) mechanism based on the modulation of the reward signal, after reinforcement learning, SP-SNN can produce the symbol sequence according to different supra-regular grammar rules. Moreover, SP-SNN\&\#39;s performance in the experiment can be well fitted with the behavioral data of macaques, which further demonstrates the biological plausibility of the network. In addition, this work also verifies that the chunking mechanism plays an essential role in improving the robustness of the model.{\textless}/p{\textgreater} 
 
{\textless}p{\textgreater}\&nbsp; \&nbsp; \&nbsp; Second, Brain-inspired Graph Spiking Neural Networks for Commonsense Knowledge Representation and Reasoning (KRR-GSNN). Inspired by the neural mechanism of the neuron population coding symbolic concept in the biological brain, KRR-GSNN combines the population coding representation mechanism and the spike-timing-dependent plasticity (STDP) learning rule to encode the commonsense knowledge in the commonsense knowledge graph into a large-scale spike neural network and completed subsequent related reasoning tasks accordingly. Each neuron population jointly encodes the entire commonsense knowledge graph through synaptic connections, forming a graph spiking neural network that records commonsense knowledge. Furthermore, the reward-modulated spike-timing-dependent plasticity mechanism (R-STDP) is introduced to realize the biological-inspired self-supervised reinforcement learning process and complete related reasoning tasks. KRR-GSNN achieves comparable accuracy and faster convergence for the same reasoning task than graph convolutional artificial neural networks.{\textless}/p{\textgreater} 
 
{\textless}p{\textgreater}\&nbsp; \&nbsp; \&nbsp; Third, Brain-Inspired Causal Reasoning Spiking Neural Networks (CR-SNN). The current data-driven deep neural network methods essentially only utilize the correlation between data, difficult to see the causal nature of the world through data. Empowering machines with causal reasoning ability is a critical step toward a higher level of general intelligence. Inspired by the reasoning method based on the causal graph, we propose brain-inspired causal reasoning spiking neural network model by combining the relevant neural mechanisms and connection structures to solve this problem. CR-SNN completes the construction of a static causal graph by using the population coding mechanism and the learning rule of spiking timing-dependent plasticity. The feasibility of causal reasoning by spiking neural networks (SNN) is proved through experimental verification. Moreover, by introducing an external evaluation function, observers can know the reasoning path of the network in the completion of the reasoning process according to the firing state of the network, thereby significantly improving the interpretability of the network, laying a foundation for further exploration and construction of human-comprehensible spiking neural network systems.{\textless}/p{\textgreater}},
	language = {中文},
	urldate = {2024-04-25},
	author = {方宏坚},
	month = aug,
	year = {2022},
	note = {Accepted: 2022-09-26T03:12:06Z},
	keywords = {/unread},
}

@article{__2023-4,
	title = {类脑脉冲神经网络信息编码与学习模型研究},
	copyright = {中国科学院自动化研究所机构知识库},
	url = {http://ir.ia.ac.cn/handle/173211/52037},
	abstract = {{\textless}p{\textgreater}Inspired by the transmission and processing mechanism of spike information in the biological brain, the brain-inspired spiking neural network models the dynamic calculation and binary spiking output process of biological neurons. Compared with traditional artificial neural networks, it has better energy efficiency and biological plausibility. The spiking neural network is different from the traditional artificial neural network in many aspects, such as information encoding, structural characteristics, optimization and applications. Biological studies have shown that neuron spike encode information in two dimensions of frequency and phase, expanding the ability of the brain to represent information. At the same time, the structural characteristics of neurons also play a crucial role in the process of brain information processing. Different parts such as dendrites and soma of\&nbsp; neurons have relatively independent information computing capabilities, and the combination of computing effects of different parts of a single neuron produces complex dynamic characteristics. The model construction of the spiking neural network in terms of information encoding and structural features has laid the computational foundation for brain-like agents to deal with complex perception and cognition problems.{\textless}/p{\textgreater} 
 
{\textless}p{\textgreater}\&nbsp; \&nbsp; \&nbsp;In this paper, the brain-inspired spiking neural network is studied from three aspects: information encoding, model structure and application. In terms of information encoding, a spiking sequence spatio-temporal dimension encoding method combining frequency and phase is constructed; in terms of model structure, inspired by the connection of cerebral cortex neurons, a multi-compartmental spiking neuron model is constructed, in which dendrites and the soma undertakes different information computing function, improving the ability of a single neuron to process complex perception and cognitive information. At the same time, this paper combines the spike sequence encoding and model structure to construct a multi-compartment spiking neural network to realize the application of deep spiking networks in complex perception and decision-making scenarios. The main work and innovation of this paper are summarized as follows:{\textless}/p{\textgreater} 
 
{\textless}p{\textgreater}\&nbsp; \&nbsp; First, quantum superposiiton inspired spike sequence encoding method. The traditional artificial neural networks use scalar real values to represent information, while the frequency and phase feature dimensions of spiking sequences expand their information encoding capabilities. Current spiking neural network models often only use frequency or phase single-dimensional encoding method. Using the mathematical framework of quantum computing and referring to quantum image processing methods, this paper proposes a quantum superposition spatio-temporal spike sequence encoding method, which combines the frequency and phase characteristics of the spike sequence and maps it to the complex Hilbert space that characterizes the quantum state. The frequency and phase are used to encode different information features respectively. In this paper, quantum superposition state spike coding is applied to processing images with reverse background. Experiments show that compared with only frequency or phase encoding methods, the spatio-temporal spike sequence encoding method using\&nbsp; can obtain better ability to deal with background reverse and noise added image data.{\textless}/p{\textgreater} 
 
{\textless}p{\textgreater}\&nbsp; \&nbsp; Second, multi-compartment spiking neural network model and optimization algorithm. Inspired by the structure and information processing mechanism of pyramidal neurons in the cerebral cortex, this paper constructs a brain-inspired multi-compartmental neuron model, and regards the dendrites and soma of neurons as independent information receiving and processing units, so that a single neuron has different information computing part, and construct a multicompartmental spiking neural network for pattern recognition tasks. In this paper, a multi-compartment spiking neural network is combined with a quantum superposition spatio-temporal spike sequence encoding method to process reverse background and noise added images. Experiments have proved that the traditional convolutional neural network cannot recognize the image after the background is reversed if it is only trained on the original image data. In contrast, the spiking neural network proposed in this paper can maintain the model performance after the image background is reversed, due to the dimension expansion of the spatio-temporal representation of the spike sequence and the complex dynamic calculation process of the multi-compartmental neural network. At the same time, the model in this paper also shows more robustness than the traditional convolutional neural network on the image data with noise.{\textless}/p{\textgreater} 
 
{\textless}p{\textgreater}\&nbsp; \&nbsp; Third, deep spiking neural network reinforcement learning model with membrane potential based normalization. At present, the research and application of spiking neural network mainly focus on visual supervised learning tasks such as image classification, object detection and tracking. The application of spiking neural networks to complex decision-making tasks (such as deep reinforcement learning) still need to be explored. Although there are previous studies on the combination of spiking neural networks and reinforcement learning, most of them focus on robot control problems with shallow networks, or use ANN-SNN conversion methods to implement weight adjustment problems with deep networks. In this paper, the problem of the disappearance of the transmission features of the spike sequence in the deep spiking neural network is analyzed mathematically. And inspired by the mechanism that the neuron membrane potential is affected by the local field effect, this paper proposes a membrane potential based layer normalization method and successfully applies the deep spiking neural network to visual input reinforcement learning tasks. Experiments show that compared with the ANN-SNN conversion method and other spiking neural network reinforcement learning works, the proposed spiking deep Q network achieves the best performance on the Atari game task.{\textless}/p{\textgreater} 
 
{\textless}p{\textgreater}\&nbsp; \&nbsp; Finally, brain-inspired multi-compartment spiking deep distributional reinforcement learning model. Cognitive neuroscience research shows that the brain uses probability distributions to represent different states and action selection rewards in the decision-making and reasoning process. In this paper, a spiking deep distributional reinforcement learning model is constructed by combining the multi-compartment spiking neural network model and the population neuron spike sequence encoding method. The dendrites of the multi-compartment neural network receive and process decision-making information from different sources, and integrate information in the soma compartment to generate probability distribution estimates for different state-action values. Neuron population encoding method represents quantile continuous real values into separable spike sequence space, and realizes quantile regression algorithm in deep spiking neural network. Experiments show that the multi-compartment spiking deep distributional reinforcement learning model proposed in this paper achieve better performance than traditional deep distributional learning algorithms based on artificial neural networks on Atari tasks.{\textless}/p{\textgreater} 
 
{\textless}p{\textgreater}\&nbsp;{\textless}/p{\textgreater}},
	language = {中文},
	urldate = {2024-04-25},
	author = {孙胤乾},
	month = may,
	year = {2023},
	note = {Accepted: 2023-06-19T01:37:39Z},
	keywords = {/unread},
}

@phdthesis{__2023-5,
	type = {博士},
	title = {饱和脉冲神经网络的稳定性与同步分析},
	url = {https://kns.cnki.net/kcms2/article/abstract?v=Xhw-7KfLOFkUZqSNC0pW3pBdy7g3OcOuMEmL5Ztyd4vKgf9Qv3wI_50YxCJA5bg5mqyJwmx3LnOKA8ELmgWwtIe4sMA-R17KJ5xbV8VHeKQTGGHo-0PveCe6TxaMLQhYTx6NlPV31PfEMPgf7qMtcw==&uniplatform=NZKPT&language=CHS},
	abstract = {近年来,脉冲神经网络的稳定性、周期解、分岔和混沌吸引子等诸多有趣的动力学行为已得到了广泛的研究。但是,目前关于脉冲神经网络的动力学的研究成果中通常没有考虑执行器饱和的现实因素。执行器饱和作为最常见的饱和现象之一,已引起了人们的广泛关注,这是由于实际执行器的固有的物理限制,使得在几乎所有的控制系统实现中都会遇到执行器饱和的问题。如果忽略执行器饱和的限制或约束将可能导致系统的性能下降,甚至失去稳定性,以至于很难达到期望的目标。为了避免这些问题,在脉冲神经网络模型中考虑执行器饱和的问题就显得格外的重要。然而,由于脉冲作用导致的不连续性和饱和非线性问题,为这类问题的研究工作带来诸多的困难和挑战。目前,饱和脉冲作用的非线性系统的理论和应用方面的研究才刚刚起步。为此,本学位论文结合饱和非线性理论、脉冲微分系统理论以及Lyapunov稳定性理论,研究了几类饱和脉冲神经网络模型的稳定性和同步问题。具体研究成果如下:1.研究了饱和脉冲作用下的连续型及其离散类Hopfield神经网络的稳定性问题。首先分别利用扇区非线性模型法和多面体表示法处理了脉冲时刻的饱和非线性。其次,通过选取适当的二次Lyapunov函数,结合数学归纳法获得了LMIs相关的局部指数稳定性准则。最后,通过一个数值例子验证了结论的正确性,并通过吸引域的估计比较了不同饱和非线性处理方法下条件的保守性。接着,利用上述同样的方法,研究了饱和脉冲控制下一类混沌神经网络的局部指数同步。最后,通过数值例子验证了结论的有效性和可行性。2.研究了基于饱和脉冲采样控制的混沌神经网络的同步问题。首先,基于脉冲控制和采样数据控制策略各自的优势,提出了一个新的饱和脉冲混杂控制器。其次,利用扇区非线性模型方法处理了脉冲时刻的饱和非线性,通过构建合适的时间相关的Lyapunov-Krasovskii泛函,结合Jensen’s不等式、Wirtinger相关不等式、Schur补引理以及比较原理,获得了驱动响应误差系统的局部指数稳定性条件、控制器的镇定设计及其吸引域的估计。此外,利用多面体表示法处理了饱和非线性,同样得到了系统的同步准则、控制器的镇定设计及其吸引域的估计。再次,我们还给出了在不考虑执行器饱和的情况下,脉冲采样控制的混沌神经网络的同步准则及其控制器的设计。最后,通过数值例子验证了结论的有效性,并讨论了执行器饱和参数对系统的同步和吸引域估计的影响。3.研究了具有饱和脉冲控制的离散时间时滞...},
	language = {zh-CN},
	urldate = {2024-04-25},
	school = {西南大学},
	author = {何, 志龙},
	year = {2023},
	doi = {10.27684/d.cnki.gxndx.2021.004148},
	keywords = {/unread, 同步, 吸引域, 局部指数稳定性, 神经网络, 饱和脉冲, 饱和非线性},
}

@phdthesis{__2023-6,
	type = {博士},
	title = {基于深度神经网络的高速机动目标捡测研究},
	url = {https://kns.cnki.net/kcms2/article/abstract?v=Xhw-7KfLOFn0OY-Cfj9Oe41s_-dxnuwT1NahcEN1XrGZtu6VgI3EuZmtVoM1NRJnljpoUZxipcl0UvS6kfrL_5vUV2uKGgqk__ln1aycdubqRdgompFadPUze-5BfUiHT0DdCXkO6B_8FXDiWnPa3g==&uniplatform=NZKPT&language=CHS},
	abstract = {提升雷达对于高速机动目标的探测能力对于现代化国防建设来说具有重大的现实意义。雷达目标检测的核心在于目标回波能量的相参积累,然而,高速机动目标在相参积累的过程中容易发生距离徙动现象和多普勒频率徙动现象,这两个现象会使目标回波能量沿着距离维和多普勒频率维发生散焦,从而会给高速机动目标的相参积累带来严重的性能损失,进而引发雷达检测性能的显著退化。尽管针对该问题已经有了不少的研究成果,但是如何实现低运算量且高检测性能的雷达高速机动目标检测在目前来说仍然是一个挑战。为了解决这一问题,本文运用深度神经网络,沿着数据驱动和模型-数据联合驱动的技术路线展开了研究,主要工作概括如下:1、沿着数据驱动的技术路线,提出了一种基于深度神经网络的端到端雷达高速机动目标检测算法。该算法本质上是似然比检测的近似实现,它使用深度神经网络一步完成了目标回波能量相参积累和目标检测。该算法先使用深度神经网络依据雷达回波信号推断出雷达观测场景中是否包含目标的后验概率,再将深度神经网络的输出与检测门限作对比以实现恒虚警率检测,其中检测门限是通过手动调节和蒙特卡罗方法得到的。最后,仿真实验证明了该算法的有效性。2、沿着模型-数据联合驱动的技术路线,提出了一种基于深度神经网络的高速机动目标距离徙动矫正及检测算法。该算法首先创新性地采用深度神经网络实现距离徙动矫正,然后再采用dechirp方法补偿多普勒频率徙动,从而实现目标回波能量的相参积累,最后采用恒虚警率检测方法实现目标检测。因为该算法是通过应用深度神经网络,而非多维参数搜索,来实现距离徙动矫正的,所以该算法的计算量会大幅降低。仿真实验结果说明,该算法不但具备检测性能好和计算量低的优点,而且还拥有分辨临近多个目标的能力。此外,通过可视化技术,我们还对深度神经网络所做预测生成了对应的解释,发现该解释符合领域知识,这说明了深度神经网络工作机理的合理性。3、沿着模型-数据联合驱动的技术路线,还提出了一种基于全卷积神经网络的高速机动目标运动参数估计及检测算法。该算法首先将运动参数空间均匀地划分为多个子空间,接着采用全卷积神经网络依据雷达回波信号来预测目标运动参数属于哪一个子空间,这等价于对目标运动参数进行了粗略估计,然后再在全卷积神经网络预测指示的子空间中做精细搜索,从而实现对目标运动参数的精细估计和目标回波能量的相参积累,最后采用恒虚警率检测方法实现目标检测。因为基于全卷积神经网络的目标运动参数粗略估计能够帮助...},
	language = {zh-CN},
	urldate = {2024-04-25},
	school = {西安电子科技大学},
	author = {王, 春蕾},
	year = {2023},
	doi = {10.27389/d.cnki.gxadu.2021.003101},
	keywords = {/unread, 多普勒频率徙动, 数据驱动, 模型-数据联合驱动, 深度神经网络, 距离徙动, 运动参数估计, 雷达高速机动目标检测},
}

@phdthesis{__2022-1,
	type = {博士},
	title = {基于深度神经网络的多尺度目标检测算法研究},
	url = {https://kns.cnki.net/kcms2/article/abstract?v=Xhw-7KfLOFlo2PZFpQnrf7NTdFjpLjHzRvQ8vYXA5pQMv7ytsvncCPpVrCj608dn-yyWzXsJbCByEW5namqJd80yGKmKodj4ul2OJ2jCc0qBbQvyRZQ9JO0mwZb6eo1E-LoNakCQxrWNipceASmj6w==&uniplatform=NZKPT&language=CHS},
	abstract = {图像目标检测是视觉分析和理解的重要基石,旨在识别图像中所有目标类别并用外接矩形定位。随着大数据、人工智能、计算机视觉技术迅速发展,基于深度学习的目标检测算法取得了突破性成果,使目标检测技术广泛应用于智能医疗、安防监控、智慧交通、自动驾驶等各个领域。虽然深度学习算法在复杂场景中的效果远超传统算法,但是其网络模型存在的一些问题依然限制了检测精度和效率,例如:深度多尺度特征表达能力不强、不同类别样本数量不平衡、预测框质量低、训练过程任务级失衡、非极大值抑制过程性能低、大模型推理速度慢且能耗高等。虽然最新的研究工作对它们进行了一些改善,但是仍有一系列缺陷亟需改进。因此,针对其中待解决的四个问题,深入分析研究了基于深度神经网络的多尺度目标检测算法,主要研究内容与成果如下:针对深度多尺度特征表达能力不强的问题,提出了一种基于通道信息增强的特征金字塔网络。通过总结与分析,认为该问题是由特征金字塔网络存在的三种缺陷导致的,现有方法常以经验和直觉对其中一个问题进行模型设计。为了更有针对性地改善问题,设计了亚像素跳跃融合模块来减轻通道信息衰减、亚像素上下文增强模块来缓解特征融合的稀释、通道注意力引导模块来改善融合过程的混叠效应。所设计的网络模型新颖地借鉴了超分辨率任务中的亚像素卷积,并引入了上下文信息和注意力机制。实验结果表明,所提出的方法增强了多尺度特征,较好地改善了特征金字塔网络存在的结构问题,在精度和速度方面均优于同类方法。多尺度检测器中各个尺度之间存在的训练失衡问题未引起关注。多尺度训练可以看作为多任务学习,实验发现各尺度损失值频繁波动,并且取值范围皆不相同,这会导致某些尺度未被充分训练,影响模型整体精度。为了解决该问题,提出了一种动态多尺度目标检测损失优化算法。具体而言,受到不确定性任务加权的启发,设计了一种自适应方差加权方法统计各尺度损失值方差来动态调整其权重,比基于反向传播训练的权重更具可解释性;然后设计了一种强化学习优化算法进一步研究训练失衡并优化方案。实验结果说明了一阶段检测器的高级别尺度未得到充分训练,所提出的算法改善了多尺度检测器训练不平衡,提升了模型整体精度约1\%AP。在上述两个研究工作的基础上,提出了一种基于强化学习的多尺度检测器训练策略,以改善多尺度特征级与任务级失衡问题。多尺度检测器中的每个尺度不能被同等并独立对待,因此设计了一种动态特征融合算法,在训练阶段放大重要特征尺度的影响以改善特征失衡,而未引入...},
	language = {zh-CN},
	urldate = {2024-04-25},
	school = {华中科技大学},
	author = {罗, 逸豪},
	year = {2022},
	doi = {10.27157/d.cnki.ghzku.2022.000621},
	keywords = {/unread, 多尺度检测, 强化学习, 深度学习, 深度神经网络, 特征金字塔网络, 目标检测, 脉冲神经网络},
}

@phdthesis{_hodgkin-huxleyhh_2022,
	type = {博士},
	title = {Hodgkin-{Huxley}（{HH}）神经网络的结构重构和快速算法},
	url = {https://kns.cnki.net/kcms2/article/abstract?v=Xhw-7KfLOFlBSH2Dg5SUqcFautm3i27XT0lKzkU3Jw3tgsBczCZH6L8_DabN4cYJNj8BIi3YgtDTsORmFgr8KwsPYl_Vd124-NGXVMMBEFHjRffoMJ6Z1OSllS-3tNnS9VFSvkhzJuWTGSUaHYkpWA==&uniplatform=NZKPT&language=CHS},
	abstract = {大脑中神经网络的结构连接对于理解不同脑区或不同神经元之间的功能很重要,然而在实验中直接测量结构连接具有挑战性。现有技术可以观测神经元活动的大量数据,这为利用因果推断方法来检测网络中节点(神经元)之间的因果关系(因果连接)提供了可能。由于神经网络系统具有高维、非线性、随机等特点,利用因果推断方法重构神经网络结构连接这一反问题有几个难点。第一,推断的因果连接高度依赖于采用的因果推断方法,不同方法可能会给出不一致的结果,那么这些方法之间是否存在数学联系?第二,因果推断方法得到的只是统计意义的因果连接而不是真实的物理结构连接,那么因果连接和结构连接之间是否存在定量关系?第三,实际应用场景中存在现实上的约束,例如数据长度有限,网络中存在隐藏节点等问题,那么如何设计有效的因果推断的计算方法?在解决上述网络重构这个反问题时,我们需要大量数据。在实验中,长时间同时测量大规模神经网络的数据非常昂贵,另外真实的神经网络的结构连接很难直接测量。基于这些现实问题,我们考虑经典的仿真的Hodgkin-Huxley(HH)神经网络。HH模型可以详细准确地描述乌贼巨型轴突细胞产生动作电位的过程,是之后所有脉冲神经元模型的基础。但是刻画HH模型的方程是高度非线性的,当HH神经元产生动作电位时,HH方程具有刚性,为了满足稳定性需求,常规的Runge-Kutta(RK)算法只能采用非常小的时间步长,演化效率非常低。考虑到因果重构时需要大量数据,那么如何设计演化HH神经网络的快速算法?在这个工作中,我们不局限于神经网络,研究了以脉冲信号作为测量输出的一般非线性复杂网络。我们研究了几种被广泛使用的因果推断方法,时间延迟的相关系数,时间延迟的互信息,Granger因果关系和传递熵来研究上述问题,并从理论上推导了这四种因果推断方法之间的数学关系。对于HH神经网络,利用神经元产生的电脉冲数据,我们设计了有效方案成功地重构了HH神经网络的结构连接。在设计演化HH神经网络的快速算法时,我们发现HH方程在放电区域和非放电区域的刚性非常不同。基于这个现象,我们根据实际问题的需要分别设计了可以保持神经元膜电位轨迹精度和神经元统计特性精度的快速算法。与RK算法相比,这些快速算法都可以鲁棒地采用十倍以上的时间步长,同时得到高精度的解。这个结果不依赖于HH神经网络的规模,连接结构,和动力学区域。},
	language = {zh-CN},
	urldate = {2024-04-25},
	school = {上海交通大学},
	author = {田, 中棋},
	year = {2022},
	doi = {10.27307/d.cnki.gsjtu.2020.000396},
	keywords = {/unread, HH神经元, 因果推断, 效率, 数学关系, 脉冲耦合},
}

@misc{noauthor_sar_nodate,
	title = {{基于深度学习和脉冲神经网络的SAR图像建筑物分割方法研究}},
	url = {https://d.wanfangdata.com.cn/thesis/ChJUaGVzaXNOZXdTMjAyNDAxMDkSCFkzOTk2NTIxGghtNW92eXlwOA%3D%3D},
	urldate = {2024-04-25},
	keywords = {/unread},
}

@phdthesis{__2018,
	type = {博士},
	title = {时变脉冲系统的稳定性及几类脉冲神经网络的稳定性与同步分析},
	url = {https://kns.cnki.net/kcms2/article/abstract?v=Xhw-7KfLOFltUBeGtGwyJBnVCvlNOx0sLpbovQnmcPETimI4bX_m5j7UYBxvyPZS3McxZfVCt5ZNiYSTW84fqMVpzH7FhUpVgOPl8l6l1LBWiKrmHGSWEdzVUL8s7CWEBehgc4-c6zXiVyo_Tefugg==&uniplatform=NZKPT&language=CHS},
	abstract = {脉冲微分系统、随机微分系统和人工神经网络都是当下研究的热门问题。本文主要对带脉冲时间窗口的脉冲微分系统和时变脉冲随机微分系统的稳定性展开研究,并利用脉冲微分系统的相关理论讨论几类神经网络的稳定性和同步问题。主要工作分为以下几个方面:(1)研究了带有脉冲时间窗口的脉冲微分系统的稳定性问题。首先基于脉冲时间窗口的定义,构建了带脉冲时间窗口的脉冲微分系统的比较系统。然后分别研究了连续子系统稳定和不稳定的条件下脉冲系统稳定或渐近稳定的脉冲控制条件,给出了两个定理以及简化的推论。最后通过数值例子及其仿真结果验证了结论的有效性。(2)研究了时变脉冲随机系统稳定性的比较方法。首先给出了时变脉冲随机系统和每一个脉冲面恰好碰撞一次的充分条件,然后将时变脉冲随机系统变换成带有脉冲时间窗口的随机系统,最后利用比较系统方法研究了随机系统稳定或渐近稳定的脉冲控制条件,给出了两个定理以及它们各自对应的数值仿真。(3)研究了带有混合时滞和反应扩散项的脉冲随机CG神经网络的稳定性问题。这里所考虑的神经网络既包含了随机和脉冲的影响,也包含了时滞和空间上状态变化的影响。尤其需要指出的是,这里考虑的时滞影响既包括有限时滞,也包括无限时滞,其中有限时滞还包含了时变时滞和分布时滞。在一些合适的假设条件下,构建了Lyapunov-Krasovskii函数,利用线性矩阵不等式技术,得到了随机全局渐近稳定性的条件,最后通过数值例子及其数值仿真验证了结论的有效性。(4)研究了时滞脉冲神经网络的有限时间稳定问题。通过构造合适的Lyapunov-Krasovskii函数,结合平均脉冲区间和线性矩阵不等式技术,分别提出了在稳定化脉冲和非稳定化脉冲下保证系统达到有限时间稳定的充分条件。最后,给出了两个仿真实例进一步验证了理论方法的正确性。(5)研究了带有马尔科夫跳跃参数和脉冲的离散耦合神经网络的全局同步问题。所研究的神经网络包括跳跃参数是连续时间,离散状态的马尔科夫过程、脉冲扰动和时滞影响,这里考虑的时滞影响既有离散时滞也有分布时滞,并且这二者是相互独立的。通过构造合适的Lyapunov-Krasovskii函数,结合线性矩阵不等式,提出了易于检测的充分条件来保证系统达到全局渐近同步。最后,给出了两个仿真实例进一步验证了理论结果的正确性。},
	language = {zh-CN},
	urldate = {2024-04-25},
	school = {西南大学},
	author = {谭, 婕},
	year = {2018},
	keywords = {/unread, 同步, 神经网络, 稳定, 脉冲时间窗口, 脉冲系统},
}

@article{roy_towards_2019,
	title = {Towards spike-based machine intelligence with neuromorphic computing},
	volume = {575},
	copyright = {2019 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-019-1677-2},
	doi = {10.1038/s41586-019-1677-2},
	abstract = {Guided by brain-like ‘spiking’ computational frameworks, neuromorphic computing—brain-inspired computing for machine intelligence—promises to realize artificial intelligence while reducing the energy requirements of computing platforms. This interdisciplinary field began with the implementation of silicon circuits for biological neural routines, but has evolved to encompass the hardware implementation of algorithms with spike-based encoding and event-driven representations. Here we provide an overview of the developments in neuromorphic computing for both algorithms and hardware and highlight the fundamentals of learning and hardware frameworks. We discuss the main challenges and the future prospects of neuromorphic computing, with emphasis on algorithm–hardware codesign.},
	language = {en},
	number = {7784},
	urldate = {2024-04-22},
	journal = {Nature},
	author = {Roy, Kaushik and Jaiswal, Akhilesh and Panda, Priyadarshini},
	month = nov,
	year = {2019},
	note = {Publisher: Nature Publishing Group},
	keywords = {Electrical and electronic engineering, Nanoscience and technology},
	pages = {607--617},
}

@misc{shan_or_2023,
	title = {{OR} {Residual} {Connection} {Achieving} {Comparable} {Accuracy} to {ADD} {Residual} {Connection} in {Deep} {Residual} {Spiking} {Neural} {Networks}},
	url = {https://arxiv.org/abs/2311.06570v1},
	abstract = {Spiking Neural Networks (SNNs) have garnered substantial attention in brain-like computing for their biological fidelity and the capacity to execute energy-efficient spike-driven operations. As the demand for heightened performance in SNNs surges, the trend towards training deeper networks becomes imperative, while residual learning stands as a pivotal method for training deep neural networks. In our investigation, we identified that the SEW-ResNet, a prominent representative of deep residual spiking neural networks, incorporates non-event-driven operations. To rectify this, we introduce the OR Residual connection (ORRC) to the architecture. Additionally, we propose the Synergistic Attention (SynA) module, an amalgamation of the Inhibitory Attention (IA) module and the Multi-dimensional Attention (MA) module, to offset energy loss stemming from high quantization. When integrating SynA into the network, we observed the phenomenon of "natural pruning", where after training, some or all of the shortcuts in the network naturally drop out without affecting the model's classification accuracy. This significantly reduces computational overhead and makes it more suitable for deployment on edge devices. Experimental results on various public datasets confirmed that the SynA enhanced OR-Spiking ResNet achieved single-sample classification with as little as 0.8 spikes per neuron. Moreover, when compared to other spike residual models, it exhibited higher accuracy and lower power consumption. Codes are available at https://github.com/Ym-Shan/ORRC-SynA-natural-pruning.},
	language = {en},
	urldate = {2024-04-22},
	journal = {arXiv.org},
	author = {Shan, Yimeng and Qiu, Xuerui and Zhu, Rui-jie and Li, Ruike and Wang, Meng and Qu, Haicheng},
	month = nov,
	year = {2023},
}

@article{qiu_vtsnn_2023,
	title = {{VTSNN}: a virtual temporal spiking neural network},
	volume = {17},
	issn = {1662-453X},
	shorttitle = {{VTSNN}},
	url = {https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2023.1091097/full},
	doi = {10.3389/fnins.2023.1091097},
	abstract = {{\textless}p{\textgreater}Spiking neural networks (SNNs) have recently demonstrated outstanding performance in a variety of high-level tasks, such as image classification. However, advancements in the field of low-level assignments, such as image reconstruction, are rare. This may be due to the lack of promising image encoding techniques and corresponding neuromorphic devices designed specifically for SNN-based low-level vision problems. This paper begins by proposing a simple yet effective undistorted weighted-encoding-decoding technique, which primarily consists of an Undistorted Weighted-Encoding (UWE) and an Undistorted Weighted-Decoding (UWD). The former aims to convert a gray image into spike sequences for effective SNN learning, while the latter converts spike sequences back into images. Then, we design a new SNN training strategy, known as Independent-Temporal Backpropagation (ITBP) to avoid complex loss propagation in spatial and temporal dimensions, and experiments show that ITBP is superior to Spatio-Temporal Backpropagation (STBP). Finally, a so-called Virtual Temporal SNN (VTSNN) is formulated by incorporating the above-mentioned approaches into U-net network architecture, fully utilizing the potent multiscale representation capability. Experimental results on several commonly used datasets such as MNIST, F-MNIST, and CIFAR10 demonstrate that the proposed method produces competitive noise-removal performance extremely which is superior to the existing work. Compared to ANN with the same architecture, VTSNN has a greater chance of achieving superiority while consuming {\textasciitilde}1/274 of the energy. Specifically, using the given encoding-decoding strategy, a simple neuromorphic circuit could be easily constructed to maximize this low-carbon strategy.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2024-04-22},
	journal = {Frontiers in Neuroscience},
	author = {Qiu, Xue-Rui and Wang, Zhao-Rui and Luan, Zheng and Zhu, Rui-Jie and Wu, Xiao and Zhang, Ma-Lu and Deng, Liang-Jian},
	month = may,
	year = {2023},
	note = {Publisher: Frontiers},
	keywords = {biologically-inspired artificial intelligence, independent-temporal backpropagation, neuromorphic circuits, spiking neural networks, undistortion weighted-encoding/decoding},
}

@article{qiu_when_2023,
	title = {When {Spiking} {Neural} {Networks} {Meet} {Temporal} {Attention} {Image} {Decoding} and {Adaptive} {Spiking} {Neuron}},
	url = {https://openreview.net/forum?id=MuOFB0LQKcy},
	abstract = {Spiking Neural Networks (SNNs) are capable of encoding and processing temporal information in a biologically plausible way. However, most existing SNN-based methods for image tasks do not fully exploit this feature. Moreover, they often overlook the role of adaptive threshold in spiking neurons, which can enhance their dynamic behavior and learning ability. To address these issues, we propose a novel method for image decoding based on temporal attention (TAID) and an adaptive Leaky-Integrate-and-Fire (ALIF) neuron model. Our method leverages the temporal information of SNN outputs to generate high-quality images that surpass the state-of-the-art (SOTA) in terms of Inception score, Fréchet Inception Distance, and Fréchet Autoencoder Distance. Furthermore, our ALIF neuron model achieves remarkable classification accuracy on MNIST (99.78\%) and CIFAR-10 (93.89\%) datasets, demonstrating the effectiveness of learning adaptive thresholds for spiking neurons.},
	language = {en},
	urldate = {2024-04-22},
	author = {Qiu, Xuerui and Luan, Zheng and Wang, Zhaorui and Zhu, Rui-Jie},
	month = mar,
	year = {2023},
}

@misc{jin_sit_2022,
	title = {{SIT}: {A} {Bionic} and {Non}-{Linear} {Neuron} for {Spiking} {Neural} {Network}},
	shorttitle = {{SIT}},
	url = {https://arxiv.org/abs/2203.16117v2},
	abstract = {Spiking Neural Networks (SNNs) have piqued researchers' interest because of their capacity to process temporal information and low power consumption. However, current state-of-the-art methods limited their biological plausibility and performance because their neurons are generally built on the simple Leaky-Integrate-and-Fire (LIF) model. Due to the high level of dynamic complexity, modern neuron models have seldom been implemented in SNN practice. In this study, we adopt the Phase Plane Analysis (PPA) technique, a technique often utilized in neurodynamics field, to integrate a recent neuron model, namely, the Izhikevich neuron. Based on the findings in the advancement of neuroscience, the Izhikevich neuron model can be biologically plausible while maintaining comparable computational cost with LIF neurons. By utilizing the adopted PPA, we have accomplished putting neurons built with the modified Izhikevich model into SNN practice, dubbed as the Standardized Izhikevich Tonic (SIT) neuron. For performance, we evaluate the suggested technique for image classification tasks in self-built LIF-and-SIT-consisted SNNs, named Hybrid Neural Network (HNN) on static MNIST, Fashion-MNIST, CIFAR-10 datasets and neuromorphic N-MNIST, CIFAR10-DVS, and DVS128 Gesture datasets. The experimental results indicate that the suggested method achieves comparable accuracy while exhibiting more biologically realistic behaviors on nearly all test datasets, demonstrating the efficiency of this novel strategy in bridging the gap between neurodynamics and SNN practice.},
	language = {en},
	urldate = {2024-04-22},
	journal = {arXiv.org},
	author = {Jin, Cheng and Zhu, Rui-Jie and Wu, Xiao and Deng, Liang-Jian},
	month = mar,
	year = {2022},
}

@article{qiu_gated_2024,
	title = {Gated {Attention} {Coding} for {Training} {High}-{Performance} and {Efficient} {Spiking} {Neural} {Networks}},
	volume = {38},
	copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/27816},
	doi = {10.1609/aaai.v38i1.27816},
	abstract = {Spiking neural networks (SNNs) are emerging as an energy-efficient alternative to traditional artificial neural networks (ANNs) due to their unique spike-based event-driven nature. Coding is crucial in SNNs as it converts external input stimuli into spatio-temporal feature sequences.  However, most existing deep SNNs rely on direct coding that generates powerless spike representation and lacks the temporal dynamics inherent in human vision. Hence, we introduce Gated Attention Coding (GAC), a plug-and-play module that leverages the multi-dimensional gated attention unit to efficiently encode inputs into powerful representations before feeding them into the SNN architecture. GAC functions as a preprocessing layer that does not disrupt the spike-driven nature of the SNN, making it amenable to efficient neuromorphic hardware implementation with minimal modifications. Through an observer model theoretical analysis, we demonstrate GAC's attention mechanism improves temporal dynamics and coding efficiency. Experiments on CIFAR10/100 and ImageNet datasets demonstrate that GAC achieves state-of-the-art accuracy with remarkable efficiency. Notably, we improve top-1 accuracy by 3.10\% on CIFAR100 with only 6-time steps and 1.07\% on ImageNet while reducing energy usage to 66.9\% of the previous works. To our best knowledge, it is the first time to explore the attention-based dynamic coding scheme in deep SNNs, with exceptional effectiveness and efficiency on large-scale datasets. Code is available at https://github.com/bollossom/GAC.},
	language = {en},
	number = {1},
	urldate = {2024-04-22},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Qiu, Xuerui and Zhu, Rui-Jie and Chou, Yuhong and Wang, Zhaorui and Deng, Liang-Jian and Li, Guoqi},
	month = mar,
	year = {2024},
	note = {Number: 1},
	keywords = {ML: Bio-inspired Learning},
	pages = {601--610},
}

@article{zheng_spike-based_2023,
	title = {Spike-{Based} {Motion} {Estimation} for {Object} {Tracking} {Through} {Bio}-{Inspired} {Unsupervised} {Learning}},
	volume = {32},
	issn = {1941-0042},
	url = {https://ieeexplore.ieee.org/document/9985998},
	doi = {10.1109/TIP.2022.3228168},
	abstract = {Neuromorphic vision sensors, whose pixels output events/spikes asynchronously with a high temporal resolution according to the scene radiance change, are naturally appropriate for capturing high-speed motion in the scenes. However, how to utilize the events/spikes to smoothly track high-speed moving objects is still a challenging problem. Existing approaches either employ time-consuming iterative optimization, or require large amounts of labeled data to train the object detector. To this end, we propose a bio-inspired unsupervised learning framework, which takes advantage of the spatiotemporal information of events/spikes generated by neuromorphic vision sensors to capture the intrinsic motion patterns. Without off-line training, our models can filter the redundant signals with dynamic adaption module based on short-term plasticity, and extract the motion patterns with motion estimation module based on the spike-timing-dependent plasticity. Combined with the spatiotemporal and motion information of the filtered spike stream, the traditional DBSCAN clustering algorithm and Kalman filter can effectively track multiple targets in extreme scenes. We evaluate the proposed unsupervised framework for object detection and tracking tasks on synthetic data, publicly available event-based datasets, and spiking camera datasets. The experiment results show that the proposed model can robustly detect and smoothly track the moving targets on various challenging scenarios and outperforms state-of-the-art approaches.},
	urldate = {2024-04-18},
	journal = {IEEE Transactions on Image Processing},
	author = {Zheng, Yajing and Yu, Zhaofei and Wang, Song and Huang, Tiejun},
	year = {2023},
	note = {Conference Name: IEEE Transactions on Image Processing},
	keywords = {Cameras, Motion estimation, Neuromorphic vision sensor, Neuromorphics, Neurons, Target tracking, Tracking, Vision sensors, bio-inspired, high-speed object tracking, motion estimation, short-term plasticity, spike-timing-dependent plasticity, spiking camera, unsupervised learning},
	pages = {335--349},
}

@article{doutsi_dynamic_2021,
	title = {Dynamic {Image} {Quantization} {Using} {Leaky} {Integrate}-and-{Fire} {Neurons}},
	volume = {30},
	issn = {1941-0042},
	url = {https://ieeexplore.ieee.org/document/9399803},
	doi = {10.1109/TIP.2021.3070193},
	abstract = {This paper introduces a novel coding/decoding mechanism that mimics one of the most important properties of the human visual system: its ability to enhance the visual perception quality in time. In other words, the brain takes advantage of time to process and clarify the details of the visual scene. This characteristic is yet to be considered by the state-of-the-art quantization mechanisms that process the visual information regardless the duration of time it appears in the visual scene. We propose a compression architecture built of neuroscience models; it first uses the leaky integrate-and-fire (LIF) model to transform the visual stimulus into a spike train and then it combines two different kinds of spike interpretation mechanisms (SIM), the time-SIM and the rate-SIM for the encoding of the spike train. The time-SIM allows a high quality interpretation of the neural code and the rate-SIM allows a simple decoding mechanism by counting the spikes. For that reason, the proposed mechanisms is called Dual-SIM quantizer (Dual-SIMQ). We show that (i) the time-dependency of Dual-SIMQ automatically controls the reconstruction accuracy of the visual stimulus, (ii) the numerical comparison of Dual-SIMQ to the state-of-the-art shows that the performance of the proposed algorithm is similar to the uniform quantization schema while it approximates the optimal behavior of the non-uniform quantization schema and (iii) from the perceptual point of view the reconstruction quality using the Dual-SIMQ is higher than the state-of-the-art.},
	urldate = {2024-04-18},
	journal = {IEEE Transactions on Image Processing},
	author = {Doutsi, Effrosyni and Fillatre, Lionel and Antonini, Marc and Tsakalides, Panagiotis},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Image Processing},
	keywords = {Image coding, Image reconstruction, Mathematical model, Neurons, Quantization (signal), Transforms, Uniform quantization, Visualization, leaky integrate-and-fire model, non-uniform quantization, rate coding, spikes, time coding},
	pages = {4305--4315},
}

@article{chakraborty_fully_2021,
	title = {A {Fully} {Spiking} {Hybrid} {Neural} {Network} for {Energy}-{Efficient} {Object} {Detection}},
	volume = {30},
	issn = {1941-0042},
	url = {https://ieeexplore.ieee.org/document/9591302},
	doi = {10.1109/TIP.2021.3122092},
	abstract = {This paper proposes a Fully Spiking Hybrid Neural Network (FSHNN) for energy-efficient and robust object detection in resource-constrained platforms. The network architecture is based on a Spiking Convolutional Neural Network using leaky-integrate-fire neuron models. The model combines unsupervised Spike Time-Dependent Plasticity (STDP) learning with back-propagation (STBP) learning methods and also uses Monte Carlo Dropout to get an estimate of the uncertainty error. FSHNN provides better accuracy compared to DNN based object detectors while being more energy-efficient. It also outperforms these object detectors, when subjected to noisy input data and less labeled training data with a lower uncertainty error.},
	urldate = {2024-04-18},
	journal = {IEEE Transactions on Image Processing},
	author = {Chakraborty, Biswadeep and She, Xueyuan and Mukhopadhyay, Saibal},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Image Processing},
	keywords = {Biological neural networks, Detectors, Feature extraction, Neural networks, Neurons, Object detection, Spiking neural networks, Training data, generalization, leaky integrate and fire, object detection, uncertainty estimation},
	pages = {9014--9029},
}

@article{bi_graph-based_2020,
	title = {Graph-{Based} {Spatio}-{Temporal} {Feature} {Learning} for {Neuromorphic} {Vision} {Sensing}},
	volume = {29},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1057-7149, 1941-0042},
	url = {https://ieeexplore.ieee.org/document/9199543/},
	doi = {10.1109/TIP.2020.3023597},
	urldate = {2024-04-18},
	journal = {IEEE Transactions on Image Processing},
	author = {Bi, Yin and Chadha, Aaron and Abbas, Alhabib and Bourtsoulatze, Eirina and Andreopoulos, Yiannis},
	year = {2020},
	pages = {9084--9098},
}

@article{wang_deep_2021,
	title = {Deep {Spiking} {Neural} {Networks} {With} {Binary} {Weights} for {Object} {Recognition}},
	volume = {13},
	issn = {2379-8939},
	url = {https://ieeexplore.ieee.org/abstract/document/8981937},
	doi = {10.1109/TCDS.2020.2971655},
	abstract = {Spiking neural networks (SNNs) have shown great potential as a solution for realizing ultralow-power consumption on neuromorphic hardware, but obtaining deep SNNs is still a challenging problem. Existing network conversion methods can effectively obtain SNNs from the trained convolutional neural networks (CNNs) with little performance loss, however, high-precision weights in the converted SNNs would take up high-storage space nonamicable to limited memory resources. To tackle this problem, we analyze the relationship between weights and thresholds of spiking neurons and propose an efficient weights-threshold balance conversion method to obtain SNNs with binary weights, resulting in a significant memory storage reduction. The experimental results evaluated with various network structures on benchmark data sets show that the binary SNN not only needs much less memory resources compared to its high-precision counterpart but also achieves the high-recognition accuracy comparable to other state-of-the-art SNNs.},
	number = {3},
	urldate = {2024-04-18},
	journal = {IEEE Transactions on Cognitive and Developmental Systems},
	author = {Wang, Yixuan and Xu, Yang and Yan, Rui and Tang, Huajin},
	month = sep,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Cognitive and Developmental Systems},
	keywords = {Atomic layer deposition, Binary-connect neural networks, Biological neural networks, Encoding, Membrane potentials, Neurons, Object recognition, Training, deep spiking neural networks (SNNs), network conversion, neuromorphic computing, object recognition},
	pages = {514--523},
}

@article{xu_hierarchical_2022,
	title = {Hierarchical {Spiking}-{Based} {Model} for {Efficient} {Image} {Classification} {With} {Enhanced} {Feature} {Extraction} and {Encoding}},
	issn = {2162-2388},
	url = {https://ieeexplore.ieee.org/abstract/document/10003253},
	doi = {10.1109/TNNLS.2022.3232106},
	abstract = {Thanks to their event-driven nature, spiking neural networks (SNNs) are surmised to be great computation-efficient models. The spiking neurons encode beneficial temporal facts and possess excessive anti-noise properties. However, the high-quality encoding of spatio-temporal complexity and also its training optimization of SNNs are restricted by means of the contemporary problem, this article proposes a novel hierarchical event-driven visual device to explore how information transmits and signifies in the retina the usage of biologically manageable mechanisms. This cognitive model is an augmented spiking-based framework consisting of the function learning capacity of convolutional neural networks (CNNs) with the cognition capability of SNNs. Furthermore, this visual device is modeled in a biological realism way with unsupervised learning rules and advanced spike firing rate encoding methods. We train and test them on some image datasets (Modified National Institute of Standards and Technology (MNIST), Canadian Institute for Advanced Research (CIFAR)10, and its noisy versions) to show that our mannequin can process greater vital data than present cognitive models. This article also proposes a novel quantization approach to make the proposed spiking-based model more efficient for neuromorphic hardware implementation. The outcomes show this joint CNN-SNN model can reap excessive focus accuracy and get more effective generalization ability.},
	urldate = {2024-04-18},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Xu, Qi and Li, Yaxin and Shen, Jiangrong and Zhang, Pingping and Liu, Jian K. and Tang, Huajin and Pan, Gang},
	year = {2022},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Brain modeling, Computational modeling, Convolutional neural networks, Encoding, Feature extraction, Neurons, Visualization, hierarchical structure, noise-immunity, spatio-temporal representations, spiking encoding},
	pages = {1--9},
}

@article{zhu_efficient_2022,
	title = {An {Efficient} {Learning} {Algorithm} for {Direct} {Training} {Deep} {Spiking} {Neural} {Networks}},
	volume = {14},
	issn = {2379-8939},
	url = {https://ieeexplore.ieee.org/abstract/document/9406125},
	doi = {10.1109/TCDS.2021.3073846},
	abstract = {It is challenging to train deep spiking neural networks (SNNs) directly due to the difficulties associated with the nondifferentiable neuron model. In this work, an end-to-end learning algorithm based on discrete current-based leaky integrate-and-fire (C-LIF) neuron model and surrogate gradient is proposed to leverage the encoder–decoder network architecture to train deep SNNs directly. The proposed algorithm is capable of learning deep spatiotemporal features relying on current time step only, and several acceleration techniques including backward phase skipping and layerwise FreezeOut are proposed to accelerate the training. Experimental results show that the proposed learning algorithm achieved the classification accuracies of 98.40\% and 95.83\% on the dynamic neuromorphic data sets MNIST-DVS and DVS-Gestures, respectively, and of 99.58\% and 95.97\% on static vision data sets MNIST and SVHN, respectively, which are comparable to the existing state-of-the-art results. The training speed was accelerated by up to 49.5\% on MNIST and 36.6\% on DVS-Gestures with the proposed acceleration techniques while maintaining the same level of accuracy.},
	number = {3},
	urldate = {2024-04-18},
	journal = {IEEE Transactions on Cognitive and Developmental Systems},
	author = {Zhu, Xiaolei and Zhao, Baixin and Ma, De and Tang, Huajin},
	month = sep,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Cognitive and Developmental Systems},
	keywords = {Acceleration, Backpropagation, Backward phase skipping, Biological neural networks, Biological system modeling, Heuristic algorithms, Neurons, Training, layerwise FreezeOut (LFO), leaky integrate-and-fire (LIF) neuron model, spiking neural networks (SNNs), surrogate gradient},
	pages = {847--856},
}

@article{zhan_effective_2022,
	title = {Effective {Transfer} {Learning} {Algorithm} in {Spiking} {Neural} {Networks}},
	volume = {52},
	issn = {2168-2275},
	url = {https://ieeexplore.ieee.org/document/9489375},
	doi = {10.1109/TCYB.2021.3079097},
	abstract = {As the third generation of neural networks, spiking neural networks (SNNs) have gained much attention recently because of their high energy efficiency on neuromorphic hardware. However, training deep SNNs requires many labeled data that are expensive to obtain in real-world applications, as traditional artificial neural networks (ANNs). In order to address this issue, transfer learning has been proposed and widely used in traditional ANNs, but it has limited use in SNNs. In this article, we propose an effective transfer learning framework for deep SNNs based on the domain in-variance representation. Specifically, we analyze the rationality of centered kernel alignment (CKA) as a domain distance measurement relative to maximum mean discrepancy (MMD) in deep SNNs. In addition, we study the feature transferability across different layers by testing on the Office-31, Office-Caltech-10, and PACS datasets. The experimental results demonstrate the transferability of SNNs and show the effectiveness of the proposed transfer learning framework by using CKA in SNNs.},
	number = {12},
	urldate = {2024-04-18},
	journal = {IEEE Transactions on Cybernetics},
	author = {Zhan, Qiugang and Liu, Guisong and Xie, Xiurui and Sun, Guolin and Tang, Huajin},
	month = dec,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Cybernetics},
	keywords = {Artificial neural networks, Centered kernel alignment, Deep learning, Feature extraction, Kernel, Membrane potentials, Neurons, Transfer learning, deep learning, spiking neural network (SNN), transfer learning},
	pages = {13323--13335},
}

@inproceedings{xu_adaptive_2023,
	address = {New York, NY, USA},
	series = {{MM} '23},
	title = {Adaptive {Decoupled} {Pose} {Knowledge} {Distillation}},
	isbn = {9798400701085},
	url = {https://doi.org/10.1145/3581783.3611818},
	doi = {10.1145/3581783.3611818},
	abstract = {Existing state-of-the-art human pose estimation approaches require heavy computational resources for accurate prediction. One promising technique to obtain an accurate yet lightweight pose estimator is Knowledge Distillation (KD), which distills the pose knowledge from a powerful teacher model to a lightweight student model. However, existing human pose KD methods focus more on designing paired student and teacher network architectures, yet ignore the mechanism of pose knowledge distillation. In this work, we reformulate the human pose KD to a coarse to fine process and decouple the classical KD loss into three terms: Binary Keypoint vs. Non-Keypoint Distillation (BiKD), Keypoint Area Distillation (KAD) and Non-keypoint Area Distillation (NAD). Observing the decoupled formulation, we point out an important limitation of the classical pose KD, i.e. the bias between different loss terms limits the performance gain of the student network. To address the biased knowledge distillation problem, we present a novel KD method named Adaptive Decoupled Pose knowledge Distillation (ADPD), enabling BiKD, KAD and NAD to play their roles more effectively and flexibly. Extensive experiments on two standard human pose datasets, MPII and MS COCO, demonstrate that our proposed method outperforms previous KD methods and is generalizable to different teacher-student pairs. The code will be available at https://github.com/SuperJay1996/ADPD.},
	urldate = {2024-04-17},
	booktitle = {Proceedings of the 31st {ACM} {International} {Conference} on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Xu, Jie and Zhang, Shanshan and Yang, Jian},
	month = oct,
	year = {2023},
	keywords = {adaptive weighting, human pose estimation, knowledge distillation, neural networks},
	pages = {4401--4409},
}

@inproceedings{guo_membrane_2023,
	title = {Membrane {Potential} {Batch} {Normalization} for {Spiking} {Neural} {Networks}},
	abstract = {As one of the energy-efficient alternatives of conventional neural networks (CNNs), spiking neural networks (SNNs) have gained more and more interest recently. To train the deep models, some effective batch normalization (BN) techniques are proposed in SNNs. All these BNs are suggested to be used after the convolution layer as usually doing in CNNs. However, the spiking neuron is much more complex with the spatio-temporal dynamics. The regulated data flow after the BN layer will be disturbed again by the membrane potential updating operation before the firing function, i.e., the nonlinear activation. Therefore, we advocate adding another BN layer before the firing function to normalize the membrane potential again, called MPBN. To eliminate the induced time cost of MPBN, we also propose a training-inference-decoupled re-parameterization technique to fold the trained MPBN into the firing threshold. With the re-parameterization technique, the MPBN will not introduce any extra time burden in the inference. Furthermore, the MPBN can also adopt the element-wised form, while these BNs after the convolution layer can only use the channel-wised form. Experimental results show that the proposed MPBN performs well on both popular non-spiking static and neuromorphic datasets. Our code is open-sourced at {\textbackslash}href\{https://github.com/yfguo91/MPBN\}\{MPBN\}.},
	urldate = {2023-08-20},
	booktitle = {Proceedings of ICCV},
	author = {Guo, Yufei and Zhang, Yuhan and Chen, Yuanpei and Peng, Weihang and Liu, Xiaode and Zhang, Liwen and Huang, Xuhui and Ma, Zhe},
	month = oct,
	year = {2023},
	keywords = {\#ref, Computer Science - Computer Vision and Pattern Recognition, SNN模型优化, 写 relatedwork, ��guoyufei, ��less than ours},
	pages = {19420--19430},
}

@inproceedings{hammouamri_learning_2023-1,
	title = {Learning {Delays} in {Spiking} {Neural} {Networks} using {Dilated} {Convolutions} with {Learnable} {Spacings}},
	url = {https://openreview.net/forum?id=4r2ybzJnmN},
	abstract = {Spiking Neural Networks (SNNs) are a promising research direction for building power-efficient information processing systems, especially for temporal tasks such as speech recognition. In SNNs, delays refer to the time needed for one spike to travel from one neuron to another. These delays matter because they influence the spike arrival times, and it is well-known that spiking neurons respond more strongly to coincident input spikes. More formally, it has been shown theoretically that plastic delays greatly increase the expressivity in SNNs. Yet, efficient algorithms to learn these delays have been lacking. Here, we propose a new discrete-time algorithm that addresses this issue in deep feedforward SNNs using backpropagation, in an offline manner. To simulate delays between consecutive layers, we use 1D convolutions across time. The kernels contain only a few non-zero weights - one per synapse - whose positions correspond to the delays. These positions are learned together with the weights using the recently proposed Dilated Convolution with Learnable Spacings (DCLS). We evaluated our method on three datasets: the Spiking Heidelberg Dataset (SHD), the Spiking Speech Commands (SSC) and its non-spiking version Google Speech Commands v0.02 (GSC) benchmarks, which require detecting temporal patterns. We used feedforward SNNs with two or three hidden fully connected layers, and vanilla leaky integrate-and-fire neurons. We showed that fixed random delays help and that learning them helps even more. Furthermore, our method outperformed the state-of-the-art in the three datasets without using recurrent connections and with substantially fewer parameters. Our work demonstrates the potential of delay learning in developing accurate and precise models for temporal data processing. Our code is based on PyTorch / SpikingJelly and available at: https://github.com/Thvnvtos/SNN-delays},
	urldate = {2023-12-17},
	booktitle = {Proceedings of {ICLR}},
	author = {Hammouamri, Ilyass and Khalfaoui-Hassani, Ismail and Masquelier, Timothée},
	month = oct,
	year = {2023},
	keywords = {\#ref, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, ��learnable},
}

@article{hodgkin_quantitative_1952,
	title = {A quantitative description of membrane current and its application to conduction and excitation in nerve},
	volume = {117},
	issn = {0022-3751},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1392413/},
	number = {4},
	urldate = {2023-12-12},
	journal = {The Journal of Physiology},
	author = {Hodgkin, Alan L and Huxley, Andrew F},
	year = {1952},
	pmid = {12991237},
	pmcid = {PMC1392413},
	keywords = {\#ref},
	pages = {500--544},
}

@article{abbott_lapicques_1999,
	title = {Lapicque’s introduction of the integrate-and-fire model neuron (1907)},
	volume = {50},
	issn = {03619230},
	doi = {10.1016/S0361-9230(99)00161-6},
	language = {en},
	number = {5-6},
	urldate = {2023-12-19},
	journal = {Brain Research Bulletin},
	author = {Abbott, Larry F},
	month = nov,
	year = {1999},
	keywords = {\#ref},
	pages = {303--304},
}

@article{yu_stsc-snn_2022,
	title = {{STSC}-{SNN}: {Spatio}-{Temporal} {Synaptic} {Connection} with temporal convolution and attention for spiking neural networks},
	volume = {16},
	issn = {1662-453X},
	shorttitle = {{STSC}-{SNN}},
	url = {https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2022.1079357},
	abstract = {Spiking neural networks (SNNs), as one of the algorithmic models in neuromorphic computing, have gained a great deal of research attention owing to temporal information processing capability, low power consumption, and high biological plausibility. The potential to efficiently extract spatio-temporal features makes it suitable for processing event streams. However, existing synaptic structures in SNNs are almost full-connections or spatial 2D convolution, neither of which can extract temporal dependencies adequately. In this work, we take inspiration from biological synapses and propose a Spatio-Temporal Synaptic Connection SNN (STSC-SNN) model to enhance the spatio-temporal receptive fields of synaptic connections, thereby establishing temporal dependencies across layers. Specifically, we incorporate temporal convolution and attention mechanisms to implement synaptic filtering and gating functions. We show that endowing synaptic models with temporal dependencies can improve the performance of SNNs on classification tasks. In addition, we investigate the impact of performance via varied spatial-temporal receptive fields and reevaluate the temporal modules in SNNs. Our approach is tested on neuromorphic datasets, including DVS128 Gesture (gesture recognition), N-MNIST, CIFAR10-DVS (image classification), and SHD (speech digit recognition). The results show that the proposed model outperforms the state-of-the-art accuracy on nearly all datasets.},
	urldate = {2024-03-03},
	journal = {Frontiers in Neuroscience},
	author = {Yu, Chengting and Gu, Zheming and Li, Da and Wang, Gaoang and Wang, Aili and Li, Erping},
	year = {2022},
	keywords = {��Attention},
}

@inproceedings{liu_event-based_2022,
	title = {Event-{Based} {Multimodal} {Spiking} {Neural} {Network} with {Attention} {Mechanism}},
	doi = {10.1109/ICASSP43922.2022.9746865},
	abstract = {Human brain can effectively integrate visual and auditory information. Dynamic Vision Sensor (DVS) and Dynamic Audio Sensor (DAS) are event-based sensors imitating the mechanism of human retina and cochlea. Since the sensors record the visual and auditory input as asynchronous discrete events, they are inherently suitable to cooperate with the spiking neural network (SNN). Existing works of SNNs for processing events mainly focus on unimodality, however, audiovisual multimodal SNNs are still limited. In this paper, we propose an end-to-end event-based multimodal spiking neural network. The network consists of visual and auditory unimodal subnetworks and a novel attention-based cross-modal subnetwork for fusion. The attention mechanism measures the significance of each modality and allocates the weights to two modalities. We evaluate our proposed multimodal network on an event-based audiovisual joint dataset (MNIST-DVS and N-TIDIGITS datasets). Experimental results show the performance improvement of this multimodal network and the effectiveness of our proposed attention mechanism.},
	urldate = {2024-03-03},
	booktitle = {ICASSP},
	author = {Liu, Qianhui and Xing, Dong and Feng, Lang and Tang, Huajin and Pan, Gang},
	month = may,
	year = {2022},
	note = {ISSN: 2379-190X},
	keywords = {Attention, Convolution, Ear, Retina, Vision sensors, Visualization, Weight measurement, dynamic audio sensors, dynamic vision sensors, multimodal learning, spiking neural networks},
	pages = {8922--8926},
}

@misc{qiu_gated_2023,
	title = {Gated {Attention} {Coding} for {Training} {High}-performance and {Efficient} {Spiking} {Neural} {Networks}},
	shorttitle = {{GAC}({Gated} {Attention} {Coding})},
	url = {http://arxiv.org/abs/2308.06582},
	doi = {10.48550/arXiv.2308.06582},
	abstract = {Spiking neural networks (SNNs) are emerging as an energy-efficient alternative to traditional artificial neural networks (ANNs) due to their unique spike-based event-driven nature. Coding is crucial in SNNs as it converts external input stimuli into spatio-temporal feature sequences. However, most existing deep SNNs rely on direct coding that generates powerless spike representation and lacks the temporal dynamics inherent in human vision. Hence, we introduce Gated Attention Coding (GAC), a plug-and-play module that leverages the multi-dimensional gated attention unit to efficiently encode inputs into powerful representations before feeding them into the SNN architecture. GAC functions as a preprocessing layer that does not disrupt the spike-driven nature of the SNN, making it amenable to efficient neuromorphic hardware implementation with minimal modifications. Through an observer model theoretical analysis, we demonstrate GAC's attention mechanism improves temporal dynamics and coding efficiency. Experiments on CIFAR10/100 and ImageNet datasets demonstrate that GAC achieves state-of-the-art accuracy with remarkable efficiency. Notably, we improve top-1 accuracy by 3.10{\textbackslash}\% on CIFAR100 with only 6-time steps and 1.07{\textbackslash}\% on ImageNet while reducing energy usage to 66.9{\textbackslash}\% of the previous works. To our best knowledge, it is the first time to explore the attention-based dynamic coding scheme in deep SNNs, with exceptional effectiveness and efficiency on large-scale datasets.},
	urldate = {2024-01-07},
	publisher = {arXiv},
	author = {Qiu, Xuerui and Zhu, Rui-Jie and Chou, Yuhong and Wang, Zhaorui and Deng, Liang-jian and Li, Guoqi},
	month = aug,
	year = {2023},
	note = {arXiv:2308.06582 [cs]},
	keywords = {Computer Science - Neural and Evolutionary Computing, ��Attention},
}

@inproceedings{yao_inherent_2023,
	title = {Inherent {Redundancy} in {Spiking} {Neural} {Networks}},
	shorttitle = {{ASA}},
	language = {en},
	urldate = {2024-01-12},
	booktitle = {Proceedings of the {ICCV}},
	author = {Yao, Man and Hu, Jiakui and Zhao, Guangshe and Wang, Yaoyuan and Zhang, Ziyang and Xu, Bo and Li, Guoqi},
	year = {2023},
	keywords = {\#ref, SNN模型优化, 写 relatedwork, ��Attention, ��mayao},
	pages = {16924--16934},
}

@article{yao_attention_2023,
	title = {Attention {Spiking} {Neural} {Networks}},
	volume = {45},
	issn = {1939-3539},
	shorttitle = {{MA}-{SNN}},
	doi = {10.1109/TPAMI.2023.3241201},
	abstract = {Brain-inspired spiking neural networks (SNNs) are becoming a promising energy-efficient alternative to traditional artificial neural networks (ANNs). However, the performance gap between SNNs and ANNs has been a significant hindrance to deploying SNNs ubiquitously. To leverage the full potential of SNNs, in this paper we study the attention mechanisms, which can help human focus on important information. We present our idea of attention in SNNs with a multi-dimensional attention module, which infers attention weights along the temporal, channel, as well as spatial dimension separately or simultaneously. Based on the existing neuroscience theories, we exploit the attention weights to optimize membrane potentials, which in turn regulate the spiking response. Extensive experimental results on event-based action recognition and image classification datasets demonstrate that attention facilitates vanilla SNNs to achieve sparser spiking firing, better performance, and energy efficiency concurrently. In particular, we achieve top-1 accuracy of 75.92\% and 77.08\% on ImageNet-1 K with single/4-step Res-SNN-104, which are state-of-the-art results in SNNs. Compared with counterpart Res-ANN-104, the performance gap becomes -0.95/+0.21 percent and the energy efficiency is 31.8×/7.4×. To analyze the effectiveness of attention SNNs, we theoretically prove that the spiking degradation or the gradient vanishing, which usually holds in general SNNs, can be resolved by introducing the block dynamical isometry theory. We also analyze the efficiency of attention SNNs based on our proposed spiking response visualization method. Our work lights up SNN's potential as a general backbone to support various applications in the field of SNN research, with a great balance between effectiveness and energy efficiency.},
	number = {8},
	urldate = {2023-10-23},
	journal = {IEEE TPAMI},
	author = {Yao, Man and Zhao, Guangshe and Zhang, Hengyu and Hu, Yifan and Deng, Lei and Tian, Yonghong and Xu, Bo and Li, Guoqi},
	month = aug,
	year = {2023},
	keywords = {\#ref, MA-SNN, biclab, 写 relatedwork, ��Attention, ��mayao},
	pages = {9393--9410},
}

@article{cai_spatialchanneltemporal-fused_2023,
	title = {A {Spatial}–{Channel}–{Temporal}-{Fused} {Attention} for {Spiking} {Neural} {Networks}},
	issn = {2162-2388},
	shorttitle = {{SCTFA}},
	url = {https://ieeexplore.ieee.org/abstract/document/10138927},
	doi = {10.1109/TNNLS.2023.3278265},
	abstract = {Spiking neural networks (SNNs) mimic brain computational strategies, and exhibit substantial capabilities in spatiotemporal information processing. As an essential factor for human perception, visual attention refers to the dynamic process for selecting salient regions in biological vision systems. Although visual attention mechanisms have achieved great success in computer vision applications, they are rarely introduced into SNNs. Inspired by experimental observations on predictive attentional remapping, we propose a new spatial-channel–temporal-fused attention (SCTFA) module that can guide SNNs to efficiently capture underlying target regions by utilizing accumulated historical spatial–channel information in the present study. Through a systematic evaluation on three event stream datasets (DVS Gesture, SL-Animals-DVS, and MNIST-DVS), we demonstrate that the SNN with the SCTFA module (SCTFA-SNN) not only significantly outperforms the baseline SNN (BL-SNN) and two other SNN models with degenerated attention modules, but also achieves competitive accuracy with the existing state-of-the-art (SOTA) methods. Additionally, our detailed analysis shows that the proposed SCTFA-SNN model has strong robustness to noise and outstanding stability when faced with incomplete data, while maintaining acceptable complexity and efficiency. Overall, these findings indicate that incorporating appropriate cognitive mechanisms of the brain may provide a promising approach to elevate the capabilities of SNNs.},
	urldate = {2024-03-03},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Cai, Wuque and Sun, Hongze and Liu, Rui and Cui, Yan and Wang, Jun and Xia, Yang and Yao, Dezhong and Guo, Daqing},
	year = {2023},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Biological neural networks, Biological system modeling, Computational modeling, Event streams, Membrane potentials, Neurons, Training, Visualization, predictive attentional remapping, spatial-channel–temporal-fused attention (SCTFA), spiking neural networks (SNNs), visual attention, ��Attention},
	pages = {1--15},
}

@inproceedings{yao_temporal-wise_2021,
	title = {Temporal-{Wise} {Attention} {Spiking} {Neural} {Networks} for {Event} {Streams} {Classification}},
	shorttitle = {{TA}},
	language = {en},
	urldate = {2023-11-13},
	booktitle = {Proceedings of {ICCV}},
	author = {Yao, Man and Gao, Huanhuan and Zhao, Guangshe and Wang, Dingheng and Lin, Yihan and Yang, Zhaoxu and Li, Guoqi},
	year = {2021},
	keywords = {TA-SNN, biclab, ��Attention, ��mayao},
	pages = {10221--10230},
}

@article{huang_attention-enabled_2023,
	title = {Attention-enabled gated spiking neural {P} model for aspect-level sentiment classification},
	volume = {157},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608022004464},
	doi = {10.1016/j.neunet.2022.11.006},
	abstract = {Gated spiking neural P (GSNP) model is a recently developed recurrent-like network, which is abstracted by nonlinear spiking mechanism of nonlinear spiking neural P systems. In this study, a modification of GSNP is combined with attention mechanism to develop a novel model for sentiment classification, called attention-enabled GSNP model or termed as AGSNP model. The AGSNP model has two channels that process content words and aspect item respectively, where two modified GSNPs are used to obtain dependencies between content words and between aspect words. Moreover, two attention components are used to establish semantic correlation between content words and aspect item. Comparative experiments on three real data sets and several baseline models are conducted to verify the effectiveness of the AGSNP model. The comparison results demonstrate that the AGSNP model is competent for aspect-level sentiment classification tasks.},
	language = {en},
	urldate = {2024-03-03},
	journal = {Neural Networks},
	author = {Huang, Yanping and Peng, Hong and Liu, Qian and Yang, Qian and Wang, Jun and Orellana-Martín, David and Pérez-Jiménez, Mario J.},
	month = jan,
	year = {2023},
	pages = {437--443},
}

@article{ding_improved_2023,
	title = {An improved probabilistic spiking neural network with enhanced discriminative ability},
	volume = {280},
	issn = {0950-7051},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705123007748},
	doi = {10.1016/j.knosys.2023.111024},
	abstract = {The non-differentiability of the spike activity has been a hindrance to the development of high-performance spiking neural networks (SNNs). Current learning algorithms mainly focus on achieving attractive SNNs based on surrogate gradient or conversion, yet their performance is still limited. The probability-based SNNs use the probabilistic mechanism to smooth out spike activity, showing a promising way for training SNNs. This work optimizes the probabilistic mechanism and proposes the probabilistic firing mechanism (PFM) for spiking neurons. PFM enables differentiable spike activity and can be adapted to a variety of spiking neurons. In addition, to eliminate the negative influence of probabilistic uncertainty, the attention discrimination mechanism (ADM) is proposed, which enables the neurons to respond efficiently by adaptively distinguishing the salient elements of the input current. By fusing PFM, ADM, and Leaky Integrate-and-Fire (LIF) neurons, we constructed the Probabilistic Attention Leaky Integrate-and-Fire (PALIF) neuron and Probabilistic Attention Spiking Neural Network (PASNN). Ablation studies confirm the effectiveness of PFM and ADM, and indicate that PASNN is suitable for low-latency scenarios. Experiments on both static image and neuromorphic datasets, including CIFAR10, CIFAR100, N-MNIST, and CIFAR10-DVS, demonstrate that PASNN achieves competitive performance in terms of accuracy and inference speed.},
	urldate = {2024-03-03},
	journal = {Knowledge-Based Systems},
	author = {Ding, Yongqi and Zuo, Lin and Yang, Kunshan and Chen, Zhongshu and Hu, Jian and Xiahou, Tangfan},
	month = nov,
	year = {2023},
	keywords = {Leaky integrate-and-fire neuron, Probabilistic firing mechanism, Spiking neural network, ��Attention},
	pages = {111024},
}

@misc{guo_ternary_2023,
	title = {Ternary {Spike}: {Learning} {Ternary} {Spikes} for {Spiking} {Neural} {Networks}},
	shorttitle = {Ternary {Spike}},
	url = {http://arxiv.org/abs/2312.06372},
	abstract = {The Spiking Neural Network (SNN), as one of the biologically inspired neural network infrastructures, has drawn increasing attention recently. It adopts binary spike activations to transmit information, thus the multiplications of activations and weights can be substituted by additions, which brings high energy efficiency. However, in the paper, we theoretically and experimentally prove that the binary spike activation map cannot carry enough information, thus causing information loss and resulting in accuracy decreasing. To handle the problem, we propose a ternary spike neuron to transmit information. The ternary spike neuron can also enjoy the event-driven and multiplication-free operation advantages of the binary spike neuron but will boost the information capacity. Furthermore, we also embed a trainable factor in the ternary spike neuron to learn the suitable spike amplitude, thus our SNN will adopt different spike amplitudes along layers, which can better suit the phenomenon that the membrane potential distributions are different along layers. To retain the efficiency of the vanilla ternary spike, the trainable ternary spike SNN will be converted to a standard one again via a re-parameterization technique in the inference. Extensive experiments with several popular network structures over static and dynamic datasets show that the ternary spike can consistently outperform state-of-the-art methods. Our code is open-sourced at https://github.com/yfguo91/Ternary-Spike.},
	urldate = {2023-12-18},
	publisher = {arXiv},
	author = {Guo, Yufei and Chen, Yuanpei and Liu, Xiaode and Peng, Weihang and Zhang, Yuhan and Huang, Xuhui and Ma, Zhe},
	month = dec,
	year = {2023},
	note = {arXiv:2312.06372 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, 写 relatedwork, ��guoyufei, ��learnable},
}

@article{lian_im-lif_2024,
	title = {{IM}-{LIF}: {Improved} {Neuronal} {Dynamics} {With} {Attention} {Mechanism} for {Direct} {Training} {Deep} {Spiking} {Neural} {Network}},
	issn = {2471-285X},
	shorttitle = {{IM}-{LIF}},
	doi = {10.1109/TETCI.2024.3359539},
	abstract = {Spiking neural networks (SNNs) are increasingly applied to deep architectures. Recent works are developed to apply spatio-temporal backpropagation to directly train deep SNNs. But the binary and non-differentiable properties of spike activities force directly trained SNNs to suffer from serious gradient vanishing. In this paper, we first analyze the cause of the gradient vanishing problem and identify that the gradients mostly backpropagate along the synaptic currents. Based on that, we modify the synaptic current equation of leaky-integrate-fire neuron model and propose the improved LIF (IM-LIF) neuron model on the basis of the temporal-wise attention mechanism. We utilize the temporal-wise attention mechanism to selectively establish the connection between the current and historical response values, which can empirically enable the neuronal states to update resilient to the gradient vanishing problem. Furthermore, to capture the neuronal dynamics embedded in the output incorporating the IM-LIF model, we present a new temporal loss function to constrain the output of the network close to the target distribution. The proposed new temporal loss function could not only act as a regularizer to eliminate output outliers, but also assign the network loss credit to the voltage at a specific time point. Then we modify the ResNet and VGG architecture based on the IM-LIF model to build deep SNNs. We evaluate our work on image datasets and neuromorphic datasets. Experimental results and analysis show that our method can help build deep SNNs with competitive performance in both accuracy and latency, including 95.66\% on CIFAR-10, 77.42\% on CIFAR-100, 55.37\% on Tiny-ImageNet, 97.33\% on DVS-Gesture, and 80.50\% on CIFAR-DVS with very few timesteps.},
	urldate = {2024-03-03},
	journal = {IEEE TETCI},
	author = {Lian, Shuang and Shen, Jiangrong and Wang, Ziming and Tang, Huajin},
	year = {2024},
	keywords = {Computational intelligence, Computer architecture, Logic gates, Mathematical models, Neurons, Spiking neural networks, Task analysis, Training, backpropagation, gradient vanishing, spiking neuron, ��Attention},
	pages = {1--11},
}

@article{safa_stdp-driven_2024,
	title = {{STDP}-{Driven} {Development} of {Attention}-{Based} {People} {Detection} in {Spiking} {Neural} {Networks}},
	volume = {16},
	issn = {2379-8939},
	url = {https://ieeexplore.ieee.org/abstract/document/9904898},
	doi = {10.1109/TCDS.2022.3210278},
	abstract = {This letter provides, to the best of our knowledge, a first analysis of how biologically plausible spiking neural networks (SNNs) equipped with spike-timing-dependent plasticity (STDP) can learn to detect people on the fly from nonindependent and identically distributed (non-i.i.d) streams of retina-inspired, event camera data. Our system works as follows. First, a short sequence of event data, capturing a walking human from a flying drone, is forwarded in its natural order to an SNN-STDP system, which also receives teacher spiking signals from the neural activity readout block. Then, when the end of the learning sequence is reached, the learned system is assessed on testing sequences. In addition, we also present a new interpretation of anti-Hebbian plasticity as an overfitting control mechanism and provide experimental demonstrations of our findings. This work contributes to the study of attention-based development and perception in bioinspired systems.},
	number = {1},
	urldate = {2024-03-03},
	journal = {IEEE Transactions on Cognitive and Developmental Systems},
	author = {Safa, Ali and Ocket, Ilja and Bourdoux, André and Sahli, Hichem and Catthoor, Francky and Gielen, Georges G. E.},
	month = feb,
	year = {2024},
	note = {Conference Name: IEEE Transactions on Cognitive and Developmental Systems},
	keywords = {Bioinspired vision, Cameras, Drones, Encoding, Feature extraction, Legged locomotion, Neurons, Voltage control, continual learning (CL), spike-timing-dependent plasticity (STDP), ��Attention},
	pages = {380--387},
}

@article{bernert_attention-based_2019,
	title = {An {Attention}-{Based} {Spiking} {Neural} {Network} for {Unsupervised} {Spike}-{Sorting}},
	volume = {29},
	issn = {0129-0657},
	url = {https://www.worldscientific.com/doi/abs/10.1142/S0129065718500594},
	doi = {10.1142/S0129065718500594},
	abstract = {Bio-inspired computing using artificial spiking neural networks promises performances outperforming currently available computational approaches. Yet, the number of applications of such networks remains limited due to the absence of generic training procedures for complex pattern recognition, which require the design of dedicated architectures for each situation. We developed a spike-timing-dependent plasticity (STDP) spiking neural network (SSN) to address spike-sorting, a central pattern recognition problem in neuroscience. This network is designed to process an extracellular neural signal in an online and unsupervised fashion. The signal stream is continuously fed to the network and processed through several layers to output spike trains matching the truth after a short learning period requiring only few data. The network features an attention mechanism to handle the scarcity of action potential occurrences in the signal, and a threshold adaptation mechanism to handle patterns with different sizes. This method outperforms two existing spike-sorting algorithms at low signal-to-noise ratio (SNR) and can be adapted to process several channels simultaneously in the case of tetrode recordings. Such attention-based STDP network applied to spike-sorting opens perspectives to embed neuromorphic processing of neural data in future brain implants.},
	number = {08},
	urldate = {2024-03-03},
	journal = {International Journal of Neural Systems},
	author = {Bernert, Marie and Yvert, Blaise},
	month = oct,
	year = {2019},
	note = {Publisher: World Scientific Publishing Co.},
	keywords = {Spike-timing-dependent synaptic plasticity, spike-sorting, spiking neural network, unsupervised learning, ��Attention},
	pages = {1850059},
}

@misc{zhu_spikegpt_2023,
	title = {{SpikeGPT}: {Generative} {Pre}-trained {Language} {Model} with {Spiking} {Neural} {Networks}},
	shorttitle = {{SpikeGPT}},
	url = {http://arxiv.org/abs/2302.13939},
	doi = {10.48550/arXiv.2302.13939},
	abstract = {As the size of large language models continue to scale, so does the computational resources required to run it. Spiking Neural Networks (SNNs) have emerged as an energy-efficient approach to deep learning that leverage sparse and event-driven activations to reduce the computational overhead associated with model inference. While they have become competitive with non-spiking models on many computer vision tasks, SNNs have also proven to be more challenging to train. As a result, their performance lags behind modern deep learning, and we are yet to see the effectiveness of SNNs in language generation. In this paper, inspired by the Receptance Weighted Key Value (RWKV) language model, we successfully implement `SpikeGPT', a generative language model with binary, event-driven spiking activation units. We train the proposed model on two model variants: 45M and 216M parameters. To the best of our knowledge, SpikeGPT is the largest backpropagation-trained SNN model to date, rendering it suitable for both the generation and comprehension of natural language. We achieve this by modifying the transformer block to replace multi-head self attention to reduce quadratic computational complexity O(N{\textasciicircum}2) to linear complexity O(N) with increasing sequence length. Input tokens are instead streamed in sequentially to our attention mechanism (as with typical SNNs). Our preliminary experiments show that SpikeGPT remains competitive with non-spiking models on tested benchmarks, while maintaining 20x fewer operations when processed on neuromorphic hardware that can leverage sparse, event-driven activations.},
	urldate = {2024-03-03},
	publisher = {arXiv},
	author = {Zhu, Rui-Jie and Zhao, Qihang and Li, Guoqi and Eshraghian, Jason K.},
	month = jun,
	year = {2023},
	note = {arXiv:2302.13939 [cs]},
	keywords = {/unread, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, ��Attention},
}

@article{peng_imbalanced_2023,
	title = {Imbalanced {Chemical} {Process} {Fault} {Diagnosis} {Using} {Balancing} {GAN} {With} {Active} {Sample} {Selection}},
	volume = {23},
	issn = {1558-1748},
	url = {https://ieeexplore.ieee.org/abstract/document/10114639},
	doi = {10.1109/JSEN.2023.3270896},
	abstract = {Current deep-learning-based fault diagnosis methods, though proven to be successful with sufficient fault data, cannot well address the challenges of sample availability in real-world industrial scenarios. To address this challenge, this article proposes an effective approach by exploiting data generation and sample selection techniques. Specifically, we first develop a balancing generative adversarial network (BAGAN)-based data generation technique to generate more discriminative fault samples by utilizing not only the fault samples, but also the normal samples. Second, a strategy is devised to select the samples generated by BAGAN, and on this basis, active learning (AL) is utilized to select the most informative samples. The stacked autoencoder (SAE)-based deep neural network (DNN) is used to classify the faults. The proposed method is evaluated by conducting computational experiments on the Tennessee Eastman (TE) dataset. The evaluation results demonstrate that the proposed BAGAN-based method with an active sample selection strategy achieves improved performance in imbalanced chemical fault diagnosis tasks.},
	number = {13},
	urldate = {2024-02-20},
	journal = {IEEE Sensors Journal},
	author = {Peng, Peng and Zhang, Hanrong and Wang, Xinyue and Huang, Wanqiu and Wang, Hongwei},
	month = jul,
	year = {2023},
	note = {Conference Name: IEEE Sensors Journal},
	keywords = {/unread, Balancing generative adversarial network (BAGAN), Chemical processes, Chemical sensors, Fault diagnosis, Generative adversarial networks, Generators, Sensors, Training, deep learning, fault diagnosis, imbalanced learning, sample selection strategy},
	pages = {14826--14833},
}

@misc{kazerouni_diffusion_2023,
	title = {Diffusion {Models} for {Medical} {Image} {Analysis}: {A} {Comprehensive} {Survey}},
	shorttitle = {Diffusion {Models} for {Medical} {Image} {Analysis}},
	url = {http://arxiv.org/abs/2211.07804},
	doi = {10.48550/arXiv.2211.07804},
	abstract = {Denoising diffusion models, a class of generative models, have garnered immense interest lately in various deep-learning problems. A diffusion probabilistic model defines a forward diffusion stage where the input data is gradually perturbed over several steps by adding Gaussian noise and then learns to reverse the diffusion process to retrieve the desired noise-free data from noisy data samples. Diffusion models are widely appreciated for their strong mode coverage and quality of the generated samples despite their known computational burdens. Capitalizing on the advances in computer vision, the field of medical imaging has also observed a growing interest in diffusion models. To help the researcher navigate this profusion, this survey intends to provide a comprehensive overview of diffusion models in the discipline of medical image analysis. Specifically, we introduce the solid theoretical foundation and fundamental concepts behind diffusion models and the three generic diffusion modelling frameworks: diffusion probabilistic models, noise-conditioned score networks, and stochastic differential equations. Then, we provide a systematic taxonomy of diffusion models in the medical domain and propose a multi-perspective categorization based on their application, imaging modality, organ of interest, and algorithms. To this end, we cover extensive applications of diffusion models in the medical domain. Furthermore, we emphasize the practical use case of some selected approaches, and then we discuss the limitations of the diffusion models in the medical domain and propose several directions to fulfill the demands of this field. Finally, we gather the overviewed studies with their available open-source implementations at https://github.com/amirhossein-kz/Awesome-Diffusion-Models-in-Medical-Imaging.},
	urldate = {2024-02-12},
	publisher = {arXiv},
	author = {Kazerouni, Amirhossein and Aghdam, Ehsan Khodapanah and Heidari, Moein and Azad, Reza and Fayyaz, Mohsen and Hacihaliloglu, Ilker and Merhof, Dorit},
	month = jun,
	year = {2023},
	note = {arXiv:2211.07804 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{kazerouni_diffusion_2023-1,
	title = {Diffusion models in medical imaging: {A} comprehensive survey},
	volume = {88},
	issn = {1361-8415},
	shorttitle = {Diffusion models in medical imaging},
	url = {https://www.sciencedirect.com/science/article/pii/S1361841523001068},
	doi = {10.1016/j.media.2023.102846},
	abstract = {Denoising diffusion models, a class of generative models, have garnered immense interest lately in various deep-learning problems. A diffusion probabilistic model defines a forward diffusion stage where the input data is gradually perturbed over several steps by adding Gaussian noise and then learns to reverse the diffusion process to retrieve the desired noise-free data from noisy data samples. Diffusion models are widely appreciated for their strong mode coverage and quality of the generated samples in spite of their known computational burdens. Capitalizing on the advances in computer vision, the field of medical imaging has also observed a growing interest in diffusion models. With the aim of helping the researcher navigate this profusion, this survey intends to provide a comprehensive overview of diffusion models in the discipline of medical imaging. Specifically, we start with an introduction to the solid theoretical foundation and fundamental concepts behind diffusion models and the three generic diffusion modeling frameworks, namely, diffusion probabilistic models, noise-conditioned score networks, and stochastic differential equations. Then, we provide a systematic taxonomy of diffusion models in the medical domain and propose a multi-perspective categorization based on their application, imaging modality, organ of interest, and algorithms. To this end, we cover extensive applications of diffusion models in the medical domain, including image-to-image translation, reconstruction, registration, classification, segmentation, denoising, 2/3D generation, anomaly detection, and other medically-related challenges. Furthermore, we emphasize the practical use case of some selected approaches, and then we discuss the limitations of the diffusion models in the medical domain and propose several directions to fulfill the demands of this field. Finally, we gather the overviewed studies with their available open-source implementations at our GitHub.11https://github.com/amirhossein-kz/Awesome-Diffusion-Models-in-Medical-Imaging. We aim to update the relevant latest papers within it regularly.},
	urldate = {2024-02-12},
	journal = {Medical Image Analysis},
	author = {Kazerouni, Amirhossein and Aghdam, Ehsan Khodapanah and Heidari, Moein and Azad, Reza and Fayyaz, Mohsen and Hacihaliloglu, Ilker and Merhof, Dorit},
	month = aug,
	year = {2023},
	keywords = {/unread, Denoising diffusion models, Diffusion models, Generative models, Medical applications, Medical imaging, Noise conditioned score networks, Score-based models, Survey},
	pages = {102846},
}

@article{__2022-2,
	title = {不平衡数据集分类方法研究综述.},
	volume = {39},
	number = {6},
	journal = {计算机应用研究},
	author = {周玉 and 孙红玉 and 房倩 and 夏浩},
	year = {2022},
	note = {ISBN: 1001-3695},
	keywords = {/unread},
}

@article{wang_scientific_2023,
	title = {Scientific discovery in the age of artificial intelligence},
	volume = {620},
	copyright = {2023 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-023-06221-2},
	doi = {10.1038/s41586-023-06221-2},
	abstract = {Artificial intelligence (AI) is being increasingly integrated into scientific discovery to augment and accelerate research, helping scientists to generate hypotheses, design experiments, collect and interpret large datasets, and gain insights that might not have been possible using traditional scientific methods alone. Here we examine breakthroughs over the past decade that include self-supervised learning, which allows models to be trained on vast amounts of unlabelled data, and geometric deep learning, which leverages knowledge about the structure of scientific data to enhance model accuracy and efficiency. Generative AI methods can create designs, such as small-molecule drugs and proteins, by analysing diverse data modalities, including images and sequences. We discuss how these methods can help scientists throughout the scientific process and the central issues that remain despite such advances. Both developers and users of AI tools need a better understanding of when such approaches need improvement, and challenges posed by poor data quality and stewardship remain. These issues cut across scientific disciplines and require developing foundational algorithmic approaches that can contribute to scientific understanding or acquire it autonomously, making them critical areas of focus for AI innovation.},
	language = {en},
	number = {7972},
	urldate = {2024-01-22},
	journal = {Nature},
	author = {Wang, Hanchen and Fu, Tianfan and Du, Yuanqi and Gao, Wenhao and Huang, Kexin and Liu, Ziming and Chandak, Payal and Liu, Shengchao and Van Katwyk, Peter and Deac, Andreea and Anandkumar, Anima and Bergen, Karianne and Gomes, Carla P. and Ho, Shirley and Kohli, Pushmeet and Lasenby, Joan and Leskovec, Jure and Liu, Tie-Yan and Manrai, Arjun and Marks, Debora and Ramsundar, Bharath and Song, Le and Sun, Jimeng and Tang, Jian and Veličković, Petar and Welling, Max and Zhang, Linfeng and Coley, Connor W. and Bengio, Yoshua and Zitnik, Marinka},
	month = aug,
	year = {2023},
	note = {Number: 7972
Publisher: Nature Publishing Group},
	keywords = {Computer science, Machine learning, Scientific community, Statistics},
	pages = {47--60},
}

@inproceedings{zhang_temporal_2020,
	title = {Temporal {Spike} {Sequence} {Learning} via {Backpropagation} for {Deep} {Spiking} {Neural} {Networks}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/8bdb5058376143fa358981954e7626b8-Abstract.html},
	abstract = {Spiking neural networks (SNNs) are well suited for spatio-temporal learning and implementations on energy-efficient event-driven neuromorphic processors. However, existing SNN error backpropagation (BP)  methods lack proper handling of spiking discontinuities and suffer from low performance compared with the BP methods for traditional artificial neural networks. In addition, a large number of time steps are typically required to achieve decent performance, leading to high latency and rendering spike-based computation unscalable to deep architectures. We present a novel Temporal Spike Sequence Learning Backpropagation (TSSL-BP) method for training deep SNNs, which breaks down error backpropagation across two types of inter-neuron and intra-neuron dependencies and leads to improved temporal learning precision. It captures inter-neuron dependencies through presynaptic firing times by considering the all-or-none characteristics of firing activities and captures intra-neuron dependencies by handling the internal evolution of each neuronal state in time. TSSL-BP efficiently trains deep SNNs within a much shortened temporal window of a few steps while improving the accuracy for various image classification datasets including CIFAR10.},
	urldate = {2024-01-22},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zhang, Wenrui and Li, Peng},
	year = {2020},
	pages = {12022--12033},
}

@misc{noauthor_temporal_nodate,
	title = {Temporal {Spike} {Sequence} {Learning} via {Backpropagation} for {Deep} {Spiking} {Neural} {Networks}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/8bdb5058376143fa358981954e7626b8-Abstract.html},
	urldate = {2024-01-22},
	keywords = {/unread},
}

@misc{science_ai_2024,
	title = {{AI} for {Science} in 2023: {A} {Community} {Primer}},
	shorttitle = {{AI} for {Science} in 2023},
	url = {https://medium.com/@AI_for_Science/ai-for-science-in-2023-a-community-primer-d2c2db37e9a7},
	abstract = {Introduction},
	language = {en},
	urldate = {2024-01-21},
	journal = {Medium},
	author = {Science, AI for},
	month = jan,
	year = {2024},
}

@article{wang_ltmd_2022,
	title = {{LTMD}: {Learning} {Improvement} of {Spiking} {Neural} {Networks} with {Learnable} {Thresholding} {Neurons} and {Moderate} {Dropout}},
	volume = {35},
	shorttitle = {{LTMD}},
	language = {en},
	urldate = {2023-09-23},
	journal = {Proceedings of NeurIPS},
	author = {Wang, Siqi and Cheng, Tee Hiang and Lim, Meng-Hiot},
	month = dec,
	year = {2022},
	keywords = {\#ref, NeurIPS 2022, 写 relatedwork, ��learnable},
	pages = {28350--28362},
}

@article{guo_im-loss_2022,
	title = {{IM}-{Loss}: {Information} {Maximization} {Loss} for {Spiking} {Neural} {Networks}},
	volume = {35},
	shorttitle = {{IM}-{Loss}},
	language = {en},
	urldate = {2023-09-23},
	journal = {Proceedings of NeurIPS},
	author = {Guo, Yufei and Chen, Yuanpei and Zhang, Liwen and Liu, Xiaode and Wang, Yinglei and Huang, Xuhui and Ma, Zhe},
	month = dec,
	year = {2022},
	keywords = {\#ref, NeurIPS 2022, SNN模型优化, 写 relatedwork},
	pages = {156--166},
}

@article{yao_glif_2022,
	title = {{GLIF}: {A} {Unified} {Gated} {Leaky} {Integrate}-and-{Fire} {Neuron} for {Spiking} {Neural} {Networks}},
	volume = {35},
	shorttitle = {{GLIF}},
	language = {en},
	urldate = {2023-11-28},
	journal = {Proceedings of NeurIPS},
	author = {Yao, Xingting and Li, Fanrong and Mo, Zitao and Cheng, Jian},
	month = dec,
	year = {2022},
	keywords = {\#ref, GLIF, 衰减-learnable, ��LIF},
	pages = {32160--32171},
}

@inproceedings{guo_recdis-snn_2022,
	title = {{RecDis}-{SNN}: {Rectifying} {Membrane} {Potential} {Distribution} for {Directly} {Training} {Spiking} {Neural} {Networks}},
	isbn = {978-1-66546-946-3},
	shorttitle = {{RecDis}-{SNN}},
	doi = {10.1109/CVPR52688.2022.00042},
	abstract = {The brain-inspired and event-driven Spiking Neural Network (SNN) aiming at mimicking the synaptic activity of biological neurons has received increasing attention. It transmits binary spike signals between network units when the membrane potential exceeds the firing threshold. This bio-mimetic mechanism of SNN appears energy-efficiency with its power sparsity and asynchronous operations on spike events. Unfortunately, with the propagation of binary spikes, the distribution of membrane potential will shift, leading to degeneration, saturation, and gradient mismatch problems, which would be disadvantageous to the network optimization and convergence. Such undesired shifts would prevent the SNN from performing well and going deep. To tackle these problems, we attempt to rectify the membrane potential distribution (MPD) by designing a novel distribution loss, MPD-Loss, which can explicitly penalize the undesired shifts without introducing any additional operations in the inference phase. Moreover, the proposed method can also mitigate the quantization error in SNNs, which is usually ignored in other works. Experimental results demonstrate that the proposed method can directly train a deeper, larger, and better-performing SNN within fewer timesteps.},
	language = {en},
	urldate = {2023-03-20},
	booktitle = {Proceedings of {CVPR}},
	author = {Guo, Yufei and Tong, Xinyi and Chen, Yuanpei and Zhang, Liwen and Liu, Xiaode and Ma, Zhe and Huang, Xuhui},
	month = jun,
	year = {2022},
	keywords = {\#ref},
	pages = {326--335},
}

@inproceedings{lian_learnable_2023,
	title = {Learnable {Surrogate} {Gradient} for {Direct} {Training} {Spiking} {Neural} {Networks}},
	isbn = {978-1-956792-03-4},
	doi = {10.24963/ijcai.2023/335},
	abstract = {Spiking neural networks (SNNs) have increasingly drawn massive research attention due to biological interpretability and efficient computation. Recent achievements are devoted to utilizing the surrogate gradient (SG) method to avoid the dilemma of non-differentiability of spiking activity to directly train SNNs by backpropagation. However, the fixed width of the SG leads to gradient vanishing and mismatch problems, thus limiting the performance of directly trained SNNs. In this work, we propose a novel perspective to unlock the width limitation of SG, called the learnable surrogate gradient (LSG) method. The LSG method modulates the width of SG according to the change of the distribution of the membrane potentials, which is identified to be related to the decay factors based on our theoretical analysis. Then we introduce the trainable decay factors to implement the LSG method, which can optimize the width of SG automatically during training to avoid the gradient vanishing and mismatch problems caused by the limited width of SG. We evaluate the proposed LSG method on both image and neuromorphic datasets. Experimental results show that the LSG method can effectively alleviate the blocking of gradient propagation caused by the limited width of SG when training deep SNNs directly. Meanwhile, the LSG method can help SNNs achieve competitive performance on both latency and accuracy.},
	language = {en},
	urldate = {2023-11-15},
	booktitle = {Proceedings of {IJCAI}},
	author = {Lian, Shuang and Shen, Jiangrong and Liu, Qianhui and Wang, Ziming and Yan, Rui and Tang, Huajin},
	month = aug,
	year = {2023},
	keywords = {\#ref, 写 relatedwork, 可学习SG, ��ijcai, ��learnable},
	pages = {3002--3010},
}

@inproceedings{amir_low_2017,
	title = {A {Low} {Power}, {Fully} {Event}-{Based} {Gesture} {Recognition} {System}},
	urldate = {2024-01-16},
	booktitle = {Proceedings of {CVPR}},
	author = {Amir, Arnon and Taba, Brian and Berg, David and Melano, Timothy and McKinstry, Jeffrey and Di Nolfo, Carmelo and Nayak, Tapan and Andreopoulos, Alexander and Garreau, Guillaume and Mendoza, Marcela and Kusnitz, Jeff and Debole, Michael and Esser, Steve and Delbruck, Tobi and Flickner, Myron and Modha, Dharmendra},
	year = {2017},
	keywords = {\#ref, DATASET},
	pages = {7243--7252},
}

@inproceedings{wang_new_2023,
	address = {Macau, SAR China},
	title = {A {New} {ANN}-{SNN} {Conversion} {Method} with {High} {Accuracy}, {Low} {Latency} and {Good} {Robustness}},
	isbn = {978-1-956792-03-4},
	url = {https://www.ijcai.org/proceedings/2023/342},
	doi = {10.24963/ijcai.2023/342},
	abstract = {Due to the advantages of low energy consumption, high robustness and fast inference speed, Spiking Neural Networks (SNNs), with good biological interpretability and the potential to be applied on neuromorphic hardware, are regarded as the third generation of Artificial Neural Networks (ANNs). Despite having so many advantages, the biggest challenge encountered by spiking neural networks is training difficulty caused by the nondifferentiability of spike signals. ANN-SNN conversion is an effective method that solves the training difficulty by converting parameters in ANNs to those in SNNs through a specific algorithm. However, the ANN-SNN conversion method also suffers from accuracy degradation and long inference time. In this paper, we reanalyze the relationship between Integrate-and-Fire (IF) neuron model and ReLU activation function, propose a StepReLU activation function more suitable for SNNs under membrane potential encoding, and use it to train ANNs. Then we convert the ANNs to SNNs with extremely small conversion error and introduce leakage mechanism to the SNNs and get the final models, which have high accuracy, low latency and good robustness, and have achieved the stateof-the-art performance on various datasets such as CIFAR and ImageNet.},
	language = {en},
	urldate = {2023-11-16},
	booktitle = {Proceedings of {IJCAI}},
	author = {Wang, Bingsen and Cao, Jian and Chen, Jue and Feng, Shuo and Wang, Yuan},
	year = {2023},
	keywords = {\#ref, StepReLU激活函数, 写 relatedwork, ��ANN2SNN, ��ijcai},
	pages = {3067--3075},
}

@inproceedings{guo_real_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Real {Spike}: {Learning} {Real}-{Valued} {Spikes} for {Spiking} {Neural} {Networks}},
	isbn = {978-3-031-19775-8},
	shorttitle = {Real {Spike}},
	doi = {10.1007/978-3-031-19775-8_4},
	abstract = {Brain-inspired spiking neural networks (SNNs) have recently drawn more and more attention due to their event-driven and energy-efficient characteristics. The integration of storage and computation paradigm on neuromorphic hardwares makes SNNs much different from Deep Neural Networks (DNNs). In this paper, we argue that SNNs may not benefit from the weight-sharing mechanism, which can effectively reduce parameters and improve inference efficiency in DNNs, in some hardwares, and assume that an SNN with unshared convolution kernels could perform better. Motivated by this assumption, a training-inference decoupling method for SNNs named as Real Spike is proposed, which not only enjoys both unshared convolution kernels and binary spikes in inference-time but also maintains both shared convolution kernels and Real-valued Spikes during training. This decoupling mechanism of SNN is realized by a re-parameterization technique. Furthermore, based on the training-inference-decoupled idea, a series of different forms for implementing Real Spike on different levels are presented, which also enjoy shared convolutions in the inference and are friendly to both neuromorphic and non-neuromorphic hardware platforms. A theoretical proof is given to clarify that the Real Spike-based SNN network is superior to its vanilla counterpart. Experimental results show that all different Real Spike versions can consistently improve the SNN performance. Moreover, the proposed method outperforms the state-of-the-art models on both non-spiking static and neuromorphic datasets.},
	language = {en},
	booktitle = {Proceedings of {ECCV}},
	publisher = {Springer Nature Switzerland},
	author = {Guo, Yufei and Zhang, Liwen and Chen, Yuanpei and Tong, Xinyi and Liu, Xiaode and Wang, YingLei and Huang, Xuhui and Ma, Zhe},
	year = {2022},
	keywords = {\#ref, Binary spike, Re-parameterization, Real spike, Spiking neural network, Training-inference-decoupled},
	pages = {52--68},
}

@inproceedings{han_deep_2020,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Deep {Spiking} {Neural} {Network}: {Energy} {Efficiency} {Through} {Time} {Based} {Coding}},
	shorttitle = {Deep {Spiking} {Neural} {Network}},
	doi = {10.1007/978-3-030-58607-2_23},
	abstract = {Spiking Neural Networks (SNNs) are promising for enabling low-power event-driven data analytics. The best performing SNNs for image recognition tasks are obtained by converting a trained deep learning Analog Neural Network (ANN) composed of Rectified Linear Unit (ReLU) activation to SNN consisting of Integrate-and-Fire (IF) neurons with “proper” firing thresholds. However, this has come at the cost of accuracy loss and higher inference latency due to lack of a notion of time. In this work, we propose an ANN to SNN conversion methodology that uses a time-based coding scheme, named Temporal-Switch-Coding (TSC), and a corresponding TSC spiking neuron model. Each input image pixel is presented using two spikes and the timing between the two spiking instants is proportional to the pixel intensity. The real-valued ReLU activations in ANN are encoded using the spike-times of the TSC neurons in the converted TSC-SNN. At most two memory accesses and two addition operations are performed for each synapse during the whole inference, which significantly improves the SNN energy efficiency. We demonstrate the proposed TSC-SNN for VGG-16, ResNet-20 and ResNet-34 SNNs on datasets including CIFAR-10 (93.63\% top-1), CIFAR-100 (70.97\% top-1) and ImageNet (73.46\% top-1 accuracy). It surpasses the best inference accuracy of the converted rate-encoded SNN with 7–14.5\$\${\textbackslash}times \$\$×lesser inference latency, and 30–60\$\${\textbackslash}times \$\$×fewer addition operations and memory accesses per inference across datasets.},
	language = {en},
	booktitle = {Proceedings of {ECCV}},
	author = {Han, Bing and Roy, Kaushik},
	year = {2020},
	keywords = {\#ref, Deep learning, Energy efficiency, Temporal coding, ��ANN2SNN},
	pages = {388--404},
}

@inproceedings{dampfhoffer_investigating_2022,
	address = {Cham},
	title = {Investigating {Current}-{Based} and {Gating} {Approaches} for {Accurate} and {Energy}-{Efficient} {Spiking} {Recurrent} {Neural} {Networks}},
	isbn = {978-3-031-15934-3},
	doi = {10.1007/978-3-031-15934-3_30},
	abstract = {Spiking Neural Networks (SNNs) with spike-based computations and communications may be more energy-efficient than Artificial Neural Networks (ANNs) for embedded applications. However, SNNs have mostly been applied to image processing, although audio applications may better fit their temporal dynamics. We evaluate the accuracy and energy-efficiency of Leaky Integrate-and-Fire (LIF) models on spiking audio datasets compared to ANNs. We demonstrate that, for processing temporal sequences, the Current-based LIF (Cuba-LIF) outperforms the LIF. Moreover, gated recurrent networks have demonstrated superior accuracy than simple recurrent networks for such tasks. Therefore, we introduce SpikGRU, a gated version of the Cuba-LIF. SpikGRU achieves higher accuracy than other recurrent SNNs on the most difficult task studied in this work. The Cuba-LIF and SpikGRU reach state-of-the-art accuracy, only {\textless}1.1\% below the accuracy of the best ANNs, while showing up to a 49x reduction in the number of operations compared to ANNs, due to the high spike sparsity.},
	language = {en},
	booktitle = {Proceedings of {ICANN}},
	publisher = {Springer Nature Switzerland},
	author = {Dampfhoffer, Manon and Mesquida, Thomas and Valentian, Alexandre and Anghel, Lorena},
	year = {2022},
	keywords = {\#ref, GRU, RNN, SNN, Speech recognition, 写 relatedwork, 衰减-expo, ��LIF},
	pages = {359--370},
}

@inproceedings{chen_resource_2023,
	title = {Resource {Constrained} {Model} {Compression} via {Minimax} {Optimization} for {Spiking} {Neural} {Networks}},
	booktitle = {Proceedings of {ACM} {MM}},
	author = {Chen, Jue and Yuan, Huan and Tan, Jianchao and Chen, Bin and Song, Chengru and Zhang, Di},
	year = {2023},
	keywords = {\#ref, SNN模型优化, 写 relatedwork},
	pages = {5204--5213},
}

@inproceedings{han_rmp-snn_2020,
	title = {{RMP}-{SNN}: {Residual} {Membrane} {Potential} {Neuron} for {Enabling} {Deeper} {High}-{Accuracy} and {Low}-{Latency} {Spiking} {Neural} {Network}},
	shorttitle = {{RMP}-{SNN}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Han_RMP-SNN_Residual_Membrane_Potential_Neuron_for_Enabling_Deeper_High-Accuracy_and_CVPR_2020_paper.html},
	urldate = {2023-09-22},
	booktitle = {Proceedings of {CVPR}},
	author = {Han, Bing and Srinivasan, Gopalakrishnan and Roy, Kaushik},
	year = {2020},
	keywords = {\#ref, SNN模型优化, 写 relatedwork, ��ANN2SNN, ��less than ours},
	pages = {13558--13567},
}

@inproceedings{fang_incorporating_2021,
	title = {Incorporating {Learnable} {Membrane} {Time} {Constant} {To} {Enhance} {Learning} of {Spiking} {Neural} {Networks}},
	language = {en},
	urldate = {2023-11-15},
	booktitle = {Proceedings of {ICCV}},
	author = {Fang, Wei and Yu, Zhaofei and Chen, Yanqi and Masquelier, Timothée and Huang, Tiejun and Tian, Yonghong},
	year = {2021},
	keywords = {\#ref, PLIF, 可学习膜时间常数, ��LIF, ��learnable},
	pages = {2661--2671},
}

@article{wang_mt-snn_2023,
	title = {{MT}-{SNN}: {Enhance} {Spiking} {Neural} {Network} with {Multiple} {Thresholds}},
	shorttitle = {{MT}-{SNN}},
	url = {http://arxiv.org/abs/2303.11127},
	doi = {10.48550/arXiv.2303.11127},
	abstract = {Spiking neural networks (SNNs), as a biology-inspired method mimicking the spiking nature of brain neurons, is a promising energy-efficient alternative to the traditional artificial neural networks (ANNs). The energy saving of SNNs is mainly from multiplication free property brought by binarized intermediate activations. In this paper, we proposed a Multiple Threshold (MT) approach to alleviate the precision loss brought by the binarized activations, such that SNNs can reach higher accuracy at fewer steps. We evaluate the approach on CIFAR10, CIFAR100 and DVS-CIFAR10, and demonstrate that MT can promote SNNs extensively, especially at early steps. For example, With MT, Parametric-Leaky-Integrate-Fire(PLIF) based VGG net can even outperform the ANN counterpart with 1 step.},
	urldate = {2023-09-22},
	journal = {arXiv preprint arXiv:2303.11127},
	author = {Wang, Xiaoting and Zhang, Yanxiang and Zhang, Yongzhe},
	month = mar,
	year = {2023},
	note = {arXiv:2303.11127 [cs]},
	keywords = {\#ref, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing, SG, 写 relatedwork, ��LIF, ��better than ours},
}

@article{lagani_spiking_2023,
	title = {Spiking {Neural} {Networks} and {Bio}-{Inspired} {Supervised} {Deep} {Learning}: {A} {Survey}},
	shorttitle = {Spiking {Neural} {Networks} and {Bio}-{Inspired} {Supervised} {Deep} {Learning}},
	doi = {10.48550/arXiv.2307.16235},
	abstract = {For a long time, biology and neuroscience fields have been a great source of inspiration for computer scientists, towards the development of Artificial Intelligence (AI) technologies. This survey aims at providing a comprehensive review of recent biologically-inspired approaches for AI. After introducing the main principles of computation and synaptic plasticity in biological neurons, we provide a thorough presentation of Spiking Neural Network (SNN) models, and we highlight the main challenges related to SNN training, where traditional backprop-based optimization is not directly applicable. Therefore, we discuss recent bio-inspired training methods, which pose themselves as alternatives to backprop, both for traditional and spiking networks. Bio-Inspired Deep Learning (BIDL) approaches towards advancing the computational capabilities and biological plausibility of current models.},
	urldate = {2023-11-27},
	journal = {arXiv preprint arXiv:2307.16235},
	author = {Lagani, Gabriele and Falchi, Fabrizio and Gennaro, Claudio and Amato, Giuseppe},
	month = jul,
	year = {2023},
	note = {arXiv:2307.16235 [cs]},
	keywords = {\#ref, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Review},
}

@article{alcami_beyond_2019,
	title = {Beyond plasticity: the dynamic impact of electrical synapses on neural circuits},
	volume = {20},
	copyright = {2019 Springer Nature Limited},
	issn = {1471-0048},
	shorttitle = {Beyond plasticity},
	doi = {10.1038/s41583-019-0133-5},
	abstract = {Electrical synapses are found in vertebrate and invertebrate nervous systems. The cellular basis of these synapses is the gap junction, a group of intercellular channels that mediate direct communication between adjacent neurons. Similar to chemical synapses, electrical connections are modifiable and their variations in strength provide a mechanism for reconfiguring neural circuits. In addition, electrical synapses dynamically regulate neural circuits through properties without equivalence in chemical transmission. Because of their continuous nature and bidirectionality, electrical synapses allow electrical currents underlying changes in membrane potential to leak to ‘coupled’ partners, dampening neuronal excitability and altering their integrative properties. Remarkably, this effect can be transiently alleviated when comparable changes in membrane potential simultaneously occur in each of the coupled neurons, a phenomenon that is dynamically dictated by the timing of arriving signals such as synaptic potentials. By way of this mechanism, electrical synapses influence synaptic integration and action potential generation, imparting an additional layer of dynamic complexity to neural circuits.},
	language = {en},
	number = {5},
	urldate = {2023-11-10},
	journal = {Nature Reviews Neuroscience},
	author = {Alcamí, Pepe and Pereda, Alberto E.},
	month = may,
	year = {2019},
	keywords = {\#ref, Gap junctions, Neurophysiology, Synaptic transmission, 写 intro},
	pages = {253--271},
}

@article{taherkhani_review_2020,
	title = {A review of learning in biologically plausible spiking neural networks},
	volume = {122},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608019303181},
	doi = {10.1016/j.neunet.2019.09.036},
	abstract = {Artificial neural networks have been used as a powerful processing tool in various areas such as pattern recognition, control, robotics, and bioinformatics. Their wide applicability has encouraged researchers to improve artificial neural networks by investigating the biological brain. Neurological research has significantly progressed in recent years and continues to reveal new characteristics of biological neurons. New technologies can now capture temporal changes in the internal activity of the brain in more detail and help clarify the relationship between brain activity and the perception of a given stimulus. This new knowledge has led to a new type of artificial neural network, the Spiking Neural Network (SNN), that draws more faithfully on biological properties to provide higher processing abilities. A review of recent developments in learning of spiking neurons is presented in this paper. First the biological background of SNN learning algorithms is reviewed. The important elements of a learning algorithm such as the neuron model, synaptic plasticity, information encoding and SNN topologies are then presented. Then, a critical review of the state-of-the-art learning algorithms for SNNs using single and multiple spikes is presented. Additionally, deep spiking neural networks are reviewed, and challenges and opportunities in the SNN field are discussed.},
	urldate = {2023-11-10},
	journal = {Neural Networks},
	author = {Taherkhani, Aboozar and Belatreche, Ammar and Li, Yuhua and Cosma, Georgina and Maguire, Liam P. and McGinnity, T. M.},
	year = {2020},
	keywords = {\#ref, Learning, Review, SNN, Synaptic plasticity},
	pages = {253--272},
}

@article{nahmias_leaky_2013,
	title = {A {Leaky} {Integrate}-and-{Fire} {Laser} {Neuron} for {Ultrafast} {Cognitive} {Computing}},
	volume = {19},
	issn = {1558-4542},
	url = {https://ieeexplore.ieee.org/abstract/document/6497478},
	doi = {10.1109/JSTQE.2013.2257700},
	abstract = {We propose an original design for a neuron-inspired photonic computational primitive for a large-scale, ultrafast cognitive computing platform. The laser exhibits excitability and behaves analogously to a leaky integrate-and-fire (LIF) neuron. This model is both fast and scalable, operating up to a billion times faster than a biological equivalent and is realizable in a compact, vertical-cavity surface-emitting laser (VCSEL). We show that-under a certain set of conditions-the rate equations governing a laser with an embedded saturable absorber reduces to the behavior of LIF neurons. We simulate the laser using realistic rate equations governing a VCSEL cavity, and show behavior representative of cortical spiking algorithms simulated in small circuits of excitable lasers. Pairing this technology with ultrafast, neural learning algorithms would open up a new domain of processing.},
	number = {5},
	urldate = {2023-11-11},
	journal = {IEEE Journal of Selected Topics in Quantum Electronics},
	author = {Nahmias, Mitchell A. and Shastri, Bhavin J. and Tait, Alexander N. and Prucnal, Paul R.},
	year = {2013},
	keywords = {\#ref, 数学模型},
	pages = {1--12},
}

@article{smit_glia-derived_2001,
	title = {A glia-derived acetylcholine-binding protein that modulates synaptic transmission},
	volume = {411},
	copyright = {2001 Macmillan Magazines Ltd.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/35077000},
	doi = {10.1038/35077000},
	abstract = {There is accumulating evidence that glial cells actively modulate neuronal synaptic transmission. We identified a glia-derived soluble acetylcholine-binding protein (AChBP), which is a naturally occurring analogue of the ligand-binding domains of the nicotinic acetylcholine receptors (nAChRs). Like the nAChRs, it assembles into a homopentamer with ligand-binding characteristics that are typical for a nicotinic receptor; unlike the nAChRs, however, it lacks the domains to form a transmembrane ion channel. Presynaptic release of acetylcholine induces the secretion of AChBP through the glial secretory pathway. We describe a molecular and cellular mechanism by which glial cells release AChBP in the synaptic cleft, and propose a model for how they actively regulate cholinergic transmission between neurons in the central nervous system.},
	language = {en},
	urldate = {2023-12-16},
	journal = {Nature},
	author = {Smit, August B. and Syed, Naweed I. and Schaap, Dick and van Minnen, Jan and Klumperman, Judith and Kits, Karel S. and Lodder, Hans and van der Schors, Roel C. and van Elk, René and Sorgedrager, Bertram and Brejc, KatjuS̆a and Sixma, Titia K. and Geraerts, Wijnand P. M.},
	year = {2001},
	keywords = {\#ref, Humanities and Social Sciences, Science, multidisciplinary},
	pages = {261--268},
}

@article{bi_synaptic_1998,
	title = {Synaptic modifications in cultured hippocampal neurons: dependence on spike timing, synaptic strength, and postsynaptic cell type},
	volume = {18},
	number = {24},
	journal = {Journal of neuroscience},
	author = {Bi, Guo-qiang and Poo, Mu-ming},
	year = {1998},
	note = {ISBN: 0270-6474
Publisher: Soc Neuroscience},
	pages = {10464--10472},
}

@book{hebb_organization_2005,
	title = {The organization of behavior: {A} neuropsychological theory},
	isbn = {1-135-63191-3},
	publisher = {Psychology press},
	author = {Hebb, Donald Olding},
	year = {2005},
}

@article{yamazaki_spiking_2022,
	title = {Spiking {Neural} {Networks} and {Their} {Applications}: {A} {Review}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3425},
	shorttitle = {Spiking {Neural} {Networks} and {Their} {Applications}},
	url = {https://www.mdpi.com/2076-3425/12/7/863},
	doi = {10.3390/brainsci12070863},
	abstract = {The past decade has witnessed the great success of deep neural networks in various domains. However, deep neural networks are very resource-intensive in terms of energy consumption, data requirements, and high computational costs. With the recent increasing need for the autonomy of machines in the real world, e.g., self-driving vehicles, drones, and collaborative robots, exploitation of deep neural networks in those applications has been actively investigated. In those applications, energy and computational efficiencies are especially important because of the need for real-time responses and the limited energy supply. A promising solution to these previously infeasible applications has recently been given by biologically plausible spiking neural networks. Spiking neural networks aim to bridge the gap between neuroscience and machine learning, using biologically realistic models of neurons to carry out the computation. Due to their functional similarity to the biological neural network, spiking neural networks can embrace the sparsity found in biology and are highly compatible with temporal code. Our contributions in this work are: (i) we give a comprehensive review of theories of biological neurons; (ii) we present various existing spike-based neuron models, which have been studied in neuroscience; (iii) we detail synapse models; (iv) we provide a review of artificial neural networks; (v) we provide detailed guidance on how to train spike-based neuron models; (vi) we revise available spike-based neuron frameworks that have been developed to support implementing spiking neural networks; (vii) finally, we cover existing spiking neural network applications in computer vision and robotics domains. The paper concludes with discussions of future perspectives.},
	language = {en},
	number = {7},
	urldate = {2023-11-27},
	journal = {Brain Sciences},
	author = {Yamazaki, Kashu and Vo-Ho, Viet-Khoa and Bulsara, Darshan and Le, Ngan},
	month = jul,
	year = {2022},
	note = {Number: 7
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {\#ref, Computer vision, Review, SNN, biological neural network, neuromorphic hardware, robotics, toolkits, 硬件},
	pages = {863},
}

@article{turrigiano_homeostatic_2004,
	title = {Homeostatic plasticity in the developing nervous system},
	volume = {5},
	issn = {1471-003X, 1471-0048},
	doi = {10.1038/nrn1327},
	language = {en},
	number = {2},
	urldate = {2023-11-30},
	journal = {Nature Reviews Neuroscience},
	author = {Turrigiano, Gina G. and Nelson, Sacha B.},
	month = feb,
	year = {2004},
	keywords = {\#ref, Review, 写 intro},
	pages = {97--107},
}

@article{rathi_diet-snn_2023,
	title = {{DIET}-{SNN}: {A} {Low}-{Latency} {Spiking} {Neural} {Network} {With} {Direct} {Input} {Encoding} and {Leakage} and {Threshold} {Optimization}},
	volume = {34},
	issn = {2162-2388},
	shorttitle = {{DIET}-{SNN}},
	doi = {10.1109/TNNLS.2021.3111897},
	abstract = {Bioinspired spiking neural networks (SNNs), operating with asynchronous binary signals (or spikes) distributed over time, can potentially lead to greater computational efficiency on event-driven hardware. The state-of-the-art SNNs suffer from high inference latency, resulting from inefficient input encoding and suboptimal settings of the neuron parameters (firing threshold and membrane leak). We propose DIET-SNN, a low-latency deep spiking network trained with gradient descent to optimize the membrane leak and the firing threshold along with other network parameters (weights). The membrane leak and threshold of each layer are optimized with end-to-end backpropagation to achieve competitive accuracy at reduced latency. The input layer directly processes the analog pixel values of an image without converting it to spike train. The first convolutional layer converts analog inputs into spikes where leaky-integrate-and-fire (LIF) neurons integrate the weighted inputs and generate an output spike when the membrane potential crosses the trained firing threshold. The trained membrane leak selectively attenuates the membrane potential, which increases activation sparsity in the network. The reduced latency combined with high activation sparsity provides massive improvements in computational efficiency. We evaluate DIET-SNN on image classification tasks from CIFAR and ImageNet datasets on VGG and ResNet architectures. We achieve top-1 accuracy of 69\% with five timesteps (inference latency) on the ImageNet dataset with 12{\textbackslash}times less compute energy than an equivalent standard artificial neural network (ANN). In addition, DIET-SNN performs 20– 500{\textbackslash}times faster inference compared to other state-of-the-art SNN models.},
	number = {6},
	urldate = {2023-11-12},
	journal = {IEEE TNNLS},
	author = {Rathi, Nitin and Roy, Kaushik},
	month = jun,
	year = {2023},
	keywords = {\#ref, ��learnable},
	pages = {3174--3182},
}

@article{neftci_surrogate_2019,
	title = {Surrogate {Gradient} {Learning} in {Spiking} {Neural} {Networks}: {Bringing} the {Power} of {Gradient}-{Based} {Optimization} to {Spiking} {Neural} {Networks}},
	volume = {36},
	issn = {1558-0792},
	shorttitle = {Surrogate {Gradient} {Learning} in {Spiking} {Neural} {Networks}},
	url = {https://ieeexplore.ieee.org/abstract/document/8891809},
	doi = {10.1109/MSP.2019.2931595},
	abstract = {Spiking neural networks (SNNs) are nature's versatile solution to fault-tolerant, energy-efficient signal processing. To translate these benefits into hardware, a growing number of neuromorphic spiking NN processors have attempted to emulate biological NNs. These developments have created an imminent need for methods and tools that enable such systems to solve real-world signal processing problems. Like conventional NNs, SNNs can be trained on real, domain-specific data; however, their training requires the overcoming of a number of challenges linked to their binary and dynamical nature. This article elucidates step-by-step the problems typically encountered when training SNNs and guides the reader through the key concepts of synaptic plasticity and data-driven learning in the spiking setting. Accordingly, it gives an overview of existing approaches and provides an introduction to surrogate gradient (SG) methods, specifically, as a particularly flexible and efficient method to overcome the aforementioned challenges.},
	number = {6},
	urldate = {2023-12-20},
	journal = {IEEE Signal Processing Magazine},
	author = {Neftci, Emre O. and Mostafa, Hesham and Zenke, Friedemann},
	month = nov,
	year = {2019},
	note = {Conference Name: IEEE Signal Processing Magazine},
	keywords = {\#ref},
	pages = {51--63},
}

@book{maass_pulsed_2001,
	title = {Pulsed {Neural} {Networks}},
	isbn = {978-0-262-63221-8},
	abstract = {Most practical applications of artificial neural networks are based on a computational model involving the propagation of continuous variables from one processing unit to the next. In recent years, data from neurobiological experiments have made it increasingly clear that biological neural networks, which communicate through pulses, use the timing of the pulses to transmit information and perform computation. This realization has stimulated significant research on pulsed neural networks, including theoretical analyses and model development, neurobiological modeling, and hardware implementation.This book presents the complete spectrum of current research in pulsed neural networks and includes the most important work from many of the key scientists in the field. Terrence J. Sejnowski's foreword, "Neural Pulse Coding," presents an overview of the topic. The first half of the book consists of longer tutorial articles spanning neurobiology, theory, algorithms, and hardware. The second half contains a larger number of shorter research chapters that present more advanced concepts. The contributors use consistent notation and terminology throughout the book.ContributorsPeter S. Burge, Stephen R. Deiss, Rodney J. Douglas, John G. Elias, Wulfram Gerstner, Alister Hamilton, David Horn, Axel Jahnke, Richard Kempter, Wolfgang Maass, Alessandro Mortara, Alan F. Murray, David P. M. Northmore, Irit Opher, Kostas A. Papathanasiou, Michael Recce, Barry J. P. Rising, Ulrich Roth, Tim Schönauer, Terrence J. Sejnowski, John Shawe-Taylor, Max R. van Daalen, J. Leo van Hemmen, Philippe Venier, Hermann Wagner, Adrian M. Whatley, Anthony M. Zador},
	language = {en},
	publisher = {MIT Press},
	author = {Maass, Wolfgang and Bishop, Christopher M.},
	month = jan,
	year = {2001},
	note = {Google-Books-ID: jEug7sJXP2MC},
	keywords = {\#ref, BOOK},
}

@article{maass_networks_1997,
	title = {Networks of spiking neurons: {The} third generation of neural network models},
	volume = {10},
	issn = {0893-6080},
	shorttitle = {Networks of spiking neurons},
	doi = {10.1016/S0893-6080(97)00011-7},
	abstract = {The computational power of formal models for networks of spiking neurons is compared with that of other neural network models based on McCulloch Pitts neurons (i.e., threshold gates), respectively, sigmoidal gates. In particular it is shown that networks of spiking neurons are, with regard to the number of neurons that are needed, computationally more powerful than these other neural network models. A concrete biologically relevant function is exhibited which can be computed by a single spiking neuron (for biologically reasonable values of its parameters), but which requires hundreds of hidden units on a sigmoidal neural net. On the other hand, it is known that any function that can be computed by a small sigmoidal neural net can also be computed by a small network of spiking neurons. This article does not assume prior knowledge about spiking neurons, and it contains an extensive list of references to the currently available literature on computations in networks of spiking neurons and relevant results from neurobiology.},
	number = {9},
	urldate = {2023-11-12},
	journal = {Neural Networks},
	author = {Maass, Wolfgang},
	month = dec,
	year = {1997},
	keywords = {\#ref, Computational complexity, Integrate-and-fire neutron, Lower bounds, Sigmoidal neural nets, Spiking neuron, 写 intro},
	pages = {1659--1671},
}

@article{lopez_crystal-clear_2001,
	title = {Crystal-clear glia–neuron interactions},
	volume = {2},
	copyright = {2001 Springer Nature Limited},
	issn = {1471-0048},
	url = {https://www.nature.com/articles/35077522},
	doi = {10.1038/35077522},
	language = {en},
	number = {6},
	urldate = {2023-12-13},
	journal = {Nature Reviews Neuroscience},
	author = {López, Juan Carlos},
	month = jun,
	year = {2001},
	note = {Number: 6
Publisher: Nature Publishing Group},
	keywords = {\#ref, Animal Genetics and Genomics, Behavioral Sciences, Biological Techniques, Biomedicine, Neurobiology, Neurosciences, general, 写 intro, ��glia},
	pages = {380--380},
}

@article{li_cifar10-dvs_2017,
	title = {{CIFAR10}-{DVS}: {An} {Event}-{Stream} {Dataset} for {Object} {Classification}},
	volume = {11},
	issn = {1662-453X},
	shorttitle = {{CIFAR10}-{DVS}},
	abstract = {Neuromorphic vision research requires high-quality and appropriately challenging event-stream datasets to support continuous improvement of algorithms and methods. However, creating event-stream datasets is a time-consuming task, which needs to be recorded using the neuromorphic cameras. Currently, there are limited event-stream datasets available. In this work, by utilizing the popular computer vision dataset CIFAR-10, we converted 10,000 frame-based images into 10,000 event streams using a dynamic vision sensor (DVS), providing an event-stream dataset of intermediate difficulty in 10 different classes, named as “CIFAR10-DVS.” The conversion of event-stream dataset was implemented by a repeated closed-loop smooth (RCLS) movement of frame-based images. Unlike the conversion of frame-based images by moving the camera, the image movement is more realistic in respect of its practical applications. The repeated closed-loop image movement generates rich local intensity changes in continuous time which are quantized by each pixel of the DVS camera to generate events. Furthermore, a performance benchmark in event-driven object classification is provided based on state-of-the-art classification algorithms. This work provides a large event-stream dataset and an initial benchmark for comparison, which may boost algorithm developments in even-driven pattern recognition and object classification.},
	urldate = {2023-12-21},
	journal = {Frontiers in Neuroscience},
	author = {Li, Hongmin and Liu, Hanchao and Ji, Xiangyang and Li, Guoqi and Shi, Luping},
	year = {2017},
	keywords = {\#ref, DATASET},
}

@article{lee_enabling_2020,
	title = {Enabling {Spike}-{Based} {Backpropagation} for {Training} {Deep} {Neural} {Network} {Architectures}},
	volume = {14},
	issn = {1662-453X},
	url = {https://www.frontiersin.org/articles/10.3389/fnins.2020.00119},
	abstract = {Spiking Neural Networks (SNNs) have recently emerged as a prominent neural computing paradigm. However, the typical shallow SNN architectures have limited capacity for expressing complex representations while training deep SNNs using input spikes has not been successful so far. Diverse methods have been proposed to get around this issue such as converting off-the-shelf trained deep Artificial Neural Networks (ANNs) to SNNs. However, the ANN-SNN conversion scheme fails to capture the temporal dynamics of a spiking system. On the other hand, it is still a difficult problem to directly train deep SNNs using input spike events due to the discontinuous, non-differentiable nature of the spike generation function. To overcome this problem, we propose an approximate derivative method that accounts for the leaky behavior of LIF neurons. This method enables training deep convolutional SNNs directly (with input spike events) using spike-based backpropagation. Our experiments show the effectiveness of the proposed spike-based learning on deep networks (VGG and Residual architectures) by achieving the best classification accuracies in MNIST, SVHN, and CIFAR-10 datasets compared to other SNNs trained with a spike-based learning. Moreover, we analyze sparse event-based computations to demonstrate the efficacy of the proposed SNN training method for inference operation in the spiking domain.},
	urldate = {2023-12-11},
	journal = {Frontiers in Neuroscience},
	author = {Lee, Chankyu and Sarwar, Syed Shakib and Panda, Priyadarshini and Srinivasan, Gopalakrishnan and Roy, Kaushik},
	year = {2020},
	keywords = {\#ref},
}

@article{krizhevsky_cifar-10_2010,
	title = {Cifar-10 (canadian institute for advanced research)},
	volume = {5},
	number = {4},
	author = {Krizhevsky, Alex and Nair, Vinod and Hinton, Geoffrey},
	year = {2010},
	keywords = {\#ref, DATASET},
	pages = {1},
}

@article{andrade-talavera_timing_2023,
	title = {Timing to be precise? {An} overview of spike timing-dependent plasticity, brain rhythmicity, and glial cells interplay within neuronal circuits},
	volume = {28},
	copyright = {2023 The Author(s)},
	issn = {1476-5578},
	shorttitle = {Timing to be precise?},
	url = {https://www.nature.com/articles/s41380-023-02027-w},
	doi = {10.1038/s41380-023-02027-w},
	abstract = {In the mammalian brain information processing and storage rely on the complex coding and decoding events performed by neuronal networks. These actions are based on the computational ability of neurons and their functional engagement in neuronal assemblies where precise timing of action potential firing is crucial. Neuronal circuits manage a myriad of spatially and temporally overlapping inputs to compute specific outputs that are proposed to underly memory traces formation, sensory perception, and cognitive behaviors. Spike-timing-dependent plasticity (STDP) and electrical brain rhythms are suggested to underlie such functions while the physiological evidence of assembly structures and mechanisms driving both processes continues to be scarce. Here, we review foundational and current evidence on timing precision and cooperative neuronal electrical activity driving STDP and brain rhythms, their interactions, and the emerging role of glial cells in such processes. We also provide an overview of their cognitive correlates and discuss current limitations and controversies, future perspectives on experimental approaches, and their application in humans.},
	language = {en},
	number = {6},
	urldate = {2023-11-10},
	journal = {Molecular Psychiatry},
	author = {Andrade-Talavera, Yuniesky and Fisahn, André and Rodríguez-Moreno, Antonio},
	month = jun,
	year = {2023},
	note = {Number: 6
Publisher: Nature Publishing Group},
	keywords = {\#ref, Cell biology, Neuroscience, Physiology, 写 intro},
	pages = {2177--2188},
}

@misc{xu_biologically_2023,
	title = {Biologically inspired structure learning with reverse knowledge distillation for spiking neural networks},
	url = {http://arxiv.org/abs/2304.09500},
	doi = {10.48550/arXiv.2304.09500},
	abstract = {Spiking neural networks (SNNs) have superb characteristics in sensory information recognition tasks due to their biological plausibility. However, the performance of some current spiking-based models is limited by their structures which means either fully connected or too-deep structures bring too much redundancy. This redundancy from both connection and neurons is one of the key factors hindering the practical application of SNNs. Although Some pruning methods were proposed to tackle this problem, they normally ignored the fact the neural topology in the human brain could be adjusted dynamically. Inspired by this, this paper proposed an evolutionary-based structure construction method for constructing more reasonable SNNs. By integrating the knowledge distillation and connection pruning method, the synaptic connections in SNNs can be optimized dynamically to reach an optimal state. As a result, the structure of SNNs could not only absorb knowledge from the teacher model but also search for deep but sparse network topology. Experimental results on CIFAR100 and DVS-Gesture show that the proposed structure learning method can get pretty well performance while reducing the connection redundancy. The proposed method explores a novel dynamical way for structure learning from scratch in SNNs which could build a bridge to close the gap between deep learning and bio-inspired neural dynamics.},
	urldate = {2024-01-12},
	publisher = {arXiv},
	author = {Xu, Qi and Li, Yaxin and Fang, Xuanye and Shen, Jiangrong and Liu, Jian K. and Tang, Huajin and Pan, Gang},
	month = apr,
	year = {2023},
	note = {arXiv:2304.09500 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
}

@article{zheng_temporal_2024,
	title = {Temporal dendritic heterogeneity incorporated with spiking neural networks for learning multi-timescale dynamics},
	volume = {15},
	copyright = {2024 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-023-44614-z},
	doi = {10.1038/s41467-023-44614-z},
	abstract = {It is widely believed the brain-inspired spiking neural networks have the capability of processing temporal information owing to their dynamic attributes. However, how to understand what kind of mechanisms contributing to the learning ability and exploit the rich dynamic properties of spiking neural networks to satisfactorily solve complex temporal computing tasks in practice still remains to be explored. In this article, we identify the importance of capturing the multi-timescale components, based on which a multi-compartment spiking neural model with temporal dendritic heterogeneity, is proposed. The model enables multi-timescale dynamics by automatically learning heterogeneous timing factors on different dendritic branches. Two breakthroughs are made through extensive experiments: the working mechanism of the proposed model is revealed via an elaborated temporal spiking XOR problem to analyze the temporal feature integration at different levels; comprehensive performance benefits of the model over ordinary spiking neural networks are achieved on several temporal computing benchmarks for speech recognition, visual recognition, electroencephalogram signal recognition, and robot place recognition, which shows the best-reported accuracy and model compactness, promising robustness and generalization, and high execution efficiency on neuromorphic hardware. This work moves neuromorphic computing a significant step toward real-world applications by appropriately exploiting biological observations.},
	language = {en},
	number = {1},
	urldate = {2024-01-09},
	journal = {Nature Communications},
	author = {Zheng, Hanle and Zheng, Zhong and Hu, Rui and Xiao, Bo and Wu, Yujie and Yu, Fangwen and Liu, Xue and Li, Guoqi and Deng, Lei},
	month = jan,
	year = {2024},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Engineering, Mathematics and computing},
	pages = {277},
}

@article{li_autodet_2021,
	title = {{AutoDet}: {Pyramid} {Network} {Architecture} {Search} for {Object} {Detection}},
	volume = {129},
	issn = {1573-1405},
	shorttitle = {{AutoDet}},
	url = {https://doi.org/10.1007/s11263-020-01415-x},
	doi = {10.1007/s11263-020-01415-x},
	abstract = {Feature pyramids have delivered significant improvement in object detection. However, building effective feature pyramids heavily relies on expert knowledge, and also requires strenuous efforts to balance effectiveness and efficiency. Automatic search methods, such as NAS-FPN, automates the design of feature pyramids, but the low search efficiency makes it difficult to apply in a large search space. In this paper, we propose a novel search framework for a feature pyramid network, called AutoDet, which enables to automatic discovery of informative connections between multi-scale features and configure detection architectures with both high efficiency and state-of-the-art performance. In AutoDet, a new search space is specifically designed for feature pyramids in object detectors, which is more general than NAS-FPN. Furthermore, the architecture search process is formulated as a combinatorial optimization problem and solved by a Simulated Annealing-based Network Architecture Search method (SA-NAS). Compared with existing NAS methods, AutoDet ensures a dramatic reduction in search times. For example, our SA-NAS can be up to 30x faster than reinforcement learning-based approaches. Furthermore, AutoDet is compatible with both one-stage and two-stage structures with all kinds of backbone networks. We demonstrate the effectiveness of AutoDet with outperforming single-model results on the COCO dataset. Without pre-training on OpenImages, AutoDet with the ResNet-101 backbone achieves an AP of 39.7 and 47.3 for one-stage and two-stage architectures, respectively, which surpass current state-of-the-art methods.},
	language = {en},
	number = {4},
	urldate = {2024-01-08},
	journal = {International Journal of Computer Vision},
	author = {Li, Zhihang and Xi, Teng and Zhang, Gang and Liu, Jingtuo and He, Ran},
	month = apr,
	year = {2021},
	keywords = {Feature pyramids, Neural architecture search, Object detection},
	pages = {1087--1105},
}

@article{krizhevsky_learning_2009,
	title = {Learning multiple layers of features from tiny images},
	author = {Krizhevsky, Alex and Hinton, Geoffrey},
	year = {2009},
	note = {Publisher: Toronto, ON, Canada},
	keywords = {DATASET},
}

@article{hagenaars_self-supervised_2021,
	title = {Self-{Supervised} {Learning} of {Event}-{Based} {Optical} {Flow} with {Spiking} {Neural} {Networks}},
	volume = {34},
	language = {en},
	urldate = {2023-12-28},
	journal = {Proceedings of NeurIPS},
	author = {Hagenaars, Jesse and Paredes-Valles, Federico and de Croon, Guido},
	month = dec,
	year = {2021},
	pages = {7167--7179},
}

@misc{xu_empirical_2015,
	title = {Empirical {Evaluation} of {Rectified} {Activations} in {Convolutional} {Network}},
	url = {http://arxiv.org/abs/1505.00853},
	doi = {10.48550/arXiv.1505.00853},
	abstract = {In this paper we investigate the performance of different types of rectified activation functions in convolutional neural network: standard rectified linear unit (ReLU), leaky rectified linear unit (Leaky ReLU), parametric rectified linear unit (PReLU) and a new randomized leaky rectified linear units (RReLU). We evaluate these activation function on standard image classification task. Our experiments suggest that incorporating a non-zero slope for negative part in rectified activation units could consistently improve the results. Thus our findings are negative on the common belief that sparsity is the key of good performance in ReLU. Moreover, on small scale dataset, using deterministic negative slope or learning it are both prone to overfitting. They are not as effective as using their randomized counterpart. By using RReLU, we achieved 75.68{\textbackslash}\% accuracy on CIFAR-100 test set without multiple test or ensemble.},
	urldate = {2023-12-21},
	publisher = {arXiv},
	author = {Xu, Bing and Wang, Naiyan and Chen, Tianqi and Li, Mu},
	month = nov,
	year = {2015},
	note = {arXiv:1505.00853 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{perez-nieves_neural_2021,
	title = {Neural heterogeneity promotes robust learning},
	volume = {12},
	copyright = {2021 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-021-26022-3},
	doi = {10.1038/s41467-021-26022-3},
	abstract = {The brain is a hugely diverse, heterogeneous structure. Whether or not heterogeneity at the neural level plays a functional role remains unclear, and has been relatively little explored in models which are often highly homogeneous. We compared the performance of spiking neural networks trained to carry out tasks of real-world difficulty, with varying degrees of heterogeneity, and found that heterogeneity substantially improved task performance. Learning with heterogeneity was more stable and robust, particularly for tasks with a rich temporal structure. In addition, the distribution of neuronal parameters in the trained networks is similar to those observed experimentally. We suggest that the heterogeneity observed in the brain may be more than just the byproduct of noisy processes, but rather may serve an active and important role in allowing animals to learn in changing environments.},
	language = {en},
	number = {1},
	urldate = {2023-12-19},
	journal = {Nature Communications},
	author = {Perez-Nieves, Nicolas and Leung, Vincent C. H. and Dragotti, Pier Luigi and Goodman, Dan F. M.},
	month = oct,
	year = {2021},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Electrical and electronic engineering, Learning algorithms, Network models},
	pages = {5791},
}

@misc{chen_hybrid_2023,
	title = {A {Hybrid} {Neural} {Coding} {Approach} for {Pattern} {Recognition} with {Spiking} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2305.16594},
	doi = {10.48550/arXiv.2305.16594},
	abstract = {The biological neural systems evolved to adapt to ecological environment for efficiency and effectiveness, wherein neurons with heterogeneous structures and rich dynamics are optimized to accomplish complex cognitive tasks. Most of the current research of biologically inspired spiking neural networks (SNNs) are, however, grounded on a homogeneous neural coding scheme, which limits their overall performance in terms of accuracy, latency, efficiency, and robustness, etc. In this work, we argue that one should holistically design the network architecture to incorporate diverse neuronal functions and neural coding schemes for best performance. As an early attempt in this research direction, we put forward a hybrid neural coding framework that integrates multiple neural coding schemes discovered in neuroscience. We demonstrate that the proposed hybrid coding scheme achieves a comparable accuracy with the state-of-the-art SNNs with homogeneous neural coding on CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets with less than eight time steps and at least 3.90x fewer computations. Furthermore, we demonstrate accurate, rapid, and robust sound source localization on SoClas dataset. This study yields valuable insights into the performance of various hybrid neural coding designs and hold significant implications for designing high performance SNNs.},
	urldate = {2023-12-19},
	publisher = {arXiv},
	author = {Chen, Xinyi and Yang, Qu and Wu, Jibin and Li, Haizhou and Tan, Kay Chen},
	month = may,
	year = {2023},
	note = {arXiv:2305.16594 [cs]},
	keywords = {Computer Science - Neural and Evolutionary Computing},
}

@article{harada_gliotransmitter_2016,
	title = {Gliotransmitter release from astrocytes: functional, developmental and pathological implications in the brain},
	volume = {9},
	issn = {1662-453X},
	shorttitle = {Gliotransmitter release from astrocytes},
	url = {https://www.frontiersin.org/articles/10.3389/fnins.2015.00499},
	abstract = {Astrocytes comprise a large population of cells in the brain and are important partners to neighboring neurons, vascular cells, and other glial cells. Astrocytes not only form a scaffold for other cells, but also extend foot processes around the capillaries to maintain the blood–brain barrier. Thus, environmental chemicals that exist in the blood stream could have potentially harmful effects on the physiological function of astrocytes. Although astrocytes are not electrically excitable, they have been shown to function as active participants in the development of neural circuits and synaptic activity. Astrocytes respond to neurotransmitters and contribute to synaptic information processing by releasing chemical transmitters called “gliotransmitters.” State-of-the-art optical imaging techniques enable us to clarify how neurotransmitters elicit the release of various gliotransmitters, including glutamate, D-serine, and ATP. Moreover, recent studies have demonstrated that the disruption of gliotransmission results in neuronal dysfunction and abnormal behaviors in animal models. In this review, we focus on the latest technical approaches to clarify the molecular mechanisms of gliotransmitter exocytosis, and discuss the possibility that exposure to environmental chemicals could alter gliotransmission and cause neurodevelopmental disorders.},
	urldate = {2023-12-18},
	journal = {Frontiers in Neuroscience},
	author = {Harada, Kazuki and Kamiya, Taichi and Tsuboi, Takashi},
	year = {2016},
}

@inproceedings{leng_cube-cnn-svm_2016,
	title = {Cube-{CNN}-{SVM}: {A} {Novel} {Hyperspectral} {Image} {Classification} {Method}},
	shorttitle = {Cube-{CNN}-{SVM}},
	url = {https://ieeexplore.ieee.org/abstract/document/7814718},
	doi = {10.1109/ICTAI.2016.0158},
	abstract = {CNNs (convolutional neural networks) have been proved to be efficient deep learning models that can directly extract high level features from raw data. In this paper, a novel CCS (Cube-CNN-SVM) method is proposed for hyperspectral image classification, which is a spectral-spatial feature based hybrid model of CNN and SVM (support vector machine). Different from most of traditional methods that only take spectral information into consideration, a target pixel and the spectral information of its neighbors are organized into a spectral-spatial multi-feature cube used in hyperspectral image classification. It is a straightforward but valid spatial strategy that can easily improve classification accuracy without extra modification of deep CNN's structure except the size of input layer and convolutional kernel. Our deep CNN consists of the input layer, convolutional layer, max pooling layer, full connection layer and output layer. To further improve hyperspectral image classification accuracy, SVM is trained as hyperspectral image classifier with the features extracted by deep CNN from spectral-spatial fusion information. Three hyperspectral image datasets such as the KSC (Kennedy Space Center), PU (Pavia University Scene) and Indian Pines are used to evaluate the performance of CCS method. Experimental results indicate that the hyperspectral image classification can be improved efficiently with the spectral-spatial fusion strategy and CCS method. Firstly, it is easy to implement the spatial strategy to improve classification accuracy about 4\% compared with only spectral information used for classification, in which 98.49\% is gained on the KSC dataset. Secondly, CCS method can further improve classification accuracy about 1\% 3\% compared to the best performance of deep CNN, in which 99.45\% is gained on the PU dataset.},
	urldate = {2023-12-18},
	booktitle = {2016 {IEEE} 28th {International} {Conference} on {Tools} with {Artificial} {Intelligence} ({ICTAI})},
	author = {Leng, Jiabing and Li, Tao and Bai, Gang and Dong, Qiankun and Dong, Han},
	month = nov,
	year = {2016},
	note = {ISSN: 2375-0197},
	pages = {1027--1034},
}

@inproceedings{kosta_adaptive-spikenet_2023,
	title = {Adaptive-{SpikeNet}: {Event}-based {Optical} {Flow} {Estimation} using {Spiking} {Neural} {Networks} with {Learnable} {Neuronal} {Dynamics}},
	shorttitle = {Adaptive-{SpikeNet}},
	url = {https://ieeexplore.ieee.org/abstract/document/10160551},
	doi = {10.1109/ICRA48891.2023.10160551},
	abstract = {Event-based cameras have recently shown great potential for high-speed motion estimation owing to their ability to capture temporally rich information asynchronously. Spiking Neural Networks (SNNs), with their neuro-inspired event-driven processing can efficiently handle such asynchronous data, while neuron models such as the leaky-integrate and fire (LIF) can keep track of the quintessential timing information contained in the inputs. SNNs achieve this by maintaining a dynamic state in the neuron memory, retaining important information while forgetting redundant data over time. Thus, we posit that SNNs would allow for better performance on sequential regression tasks compared to similarly sized Analog Neural Networks (ANNs). However, deep SNNs are difficult to train due to vanishing spikes at later layers. To that effect, we propose an adaptive fully-spiking framework with learnable neuronal dynamics to alleviate the spike vanishing problem. We utilize surrogate gradient-based backpropagation through time (BPTT) to train our deep SNNs from scratch. We validate our approach for the task of optical flow estimation on the Multi-Vehicle Stereo Event-Camera (MVSEC) dataset and the DSEC-Flow dataset. Our experiments on these datasets show an average reduction of ∼ 13\% in average endpoint error (AEE) compared to state-of-the-art ANNs. We also explore several down-scaled models and observe that our SNN models consistently outperform similarly sized ANNs offering ∼10\%-16\% lower AEE. These results demonstrate the importance of SNNs for smaller models and their suitability at the edge. In terms of efficiency, our SNNs offer substantial savings in network parameters (∼ 48.3 ×) and computational energy (∼ 10.2 ×) while attaining ∼ 10\% lower EPE compared to the state-of-the-art ANN implementations.},
	urldate = {2023-12-18},
	booktitle = {2023 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Kosta, Adarsh Kumar and Roy, Kaushik},
	month = may,
	year = {2023},
	pages = {6021--6027},
}

@article{yi_learning_2023,
	title = {Learning rules in spiking neural networks: {A} survey},
	volume = {531},
	issn = {0925-2312},
	shorttitle = {Learning rules in spiking neural networks},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231223001662},
	doi = {10.1016/j.neucom.2023.02.026},
	abstract = {Spiking neural networks (SNNs) are a promising energy-efficient alternative to artificial neural networks (ANNs) due to their rich dynamics, capability to process spatiotemporal patterns, and low-power consumption. The complex intrinsic properties of SNNs give rise to a diversity of their learning rules which are essential to functional SNNs. This paper is aimed at presenting a comprehensive overview of learning rules in SNNs. Firstly, we introduce the basic concepts of SNNs and commonly used neuromorphic datasets. Then, guided by a hierarchical classification of SNN learning rules, we present a comprehensive survey of these rules with discussions on their characteristics, advantages, limitations, and performance on several datasets. Moreover, we review practical applications of SNNs, including event-based vision and audio signal processing. Finally, we conclude this survey with a discussion on challenges and promising future research directions in this area.},
	urldate = {2023-12-18},
	journal = {Neurocomputing},
	author = {Yi, Zexiang and Lian, Jing and Liu, Qidong and Zhu, Hegui and Liang, Dong and Liu, Jizhao},
	month = apr,
	year = {2023},
	keywords = {Image classification, Learning rules, Neuromorphic computing, Pulse-coupled neural networks, Spiking neural networks},
	pages = {163--179},
}

@article{zhan_bio-inspired_2023,
	title = {Bio-inspired {Active} {Learning} method in spiking neural network},
	volume = {261},
	issn = {09507051},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950705122012898},
	doi = {10.1016/j.knosys.2022.110193},
	abstract = {Spiking neural networks (SNNs) have gained a lot of attention and achievements recently because of their low-power advantages on neuromorphic hardware. However, training deep SNNs still requires a large number of labeled data which are expensive to obtain. To address this issue, we propose an effective Bio-inspired Active Learning (BAL) method in this paper to reduce the training cost of SNN models. Specifically, bio-inspired behavior patterns of spiking neurons are defined to represent the internal states of SNN models for active learning. Then, an active learning sample selection strategy is proposed by leveraging the empirical and generalization pattern divergence in SNNs. By labeling selected samples and adding them to training, behavioral patterns can be optimized to improve the performance of neural networks. Comprehensive experiments are conducted on the CIFAR-10, SVHN, and Fashion-MNIST datasets with various sample proportions. The experimental results demonstrate that the proposed BAL achieves state-of-the-arts performance in SNNs compared with the existing active learning methods.},
	language = {en},
	urldate = {2023-12-18},
	journal = {Knowledge-Based Systems},
	author = {Zhan, Qiugang and Liu, Guisong and Xie, Xiurui and Zhang, Malu and Sun, Guolin},
	month = feb,
	year = {2023},
	pages = {110193},
}

@article{guo_direct_2023,
	title = {Direct learning-based deep spiking neural networks: a review},
	volume = {17},
	issn = {1662-453X},
	shorttitle = {Direct learning-based deep spiking neural networks},
	url = {https://www.frontiersin.org/articles/10.3389/fnins.2023.1209795},
	abstract = {The spiking neural network (SNN), as a promising brain-inspired computational model with binary spike information transmission mechanism, rich spatially-temporal dynamics, and event-driven characteristics, has received extensive attention. However, its intricately discontinuous spike mechanism brings difficulty to the optimization of the deep SNN. Since the surrogate gradient method can greatly mitigate the optimization difficulty and shows great potential in directly training deep SNNs, a variety of direct learning-based deep SNN works have been proposed and achieved satisfying progress in recent years. In this paper, we present a comprehensive survey of these direct learning-based deep SNN works, mainly categorized into accuracy improvement methods, efficiency improvement methods, and temporal dynamics utilization methods. In addition, we also divide these categorizations into finer granularities further to better organize and introduce them. Finally, the challenges and trends that may be faced in future research are prospected.},
	urldate = {2023-12-18},
	journal = {Frontiers in Neuroscience},
	author = {Guo, Yufei and Huang, Xuhui and Ma, Zhe},
	year = {2023},
}

@article{brzosko_neuromodulation_2019,
	title = {Neuromodulation of {Spike}-{Timing}-{Dependent} {Plasticity}: {Past}, {Present}, and {Future}},
	volume = {103},
	issn = {0896-6273},
	shorttitle = {Neuromodulation of {Spike}-{Timing}-{Dependent} {Plasticity}},
	url = {https://www.sciencedirect.com/science/article/pii/S0896627319304945},
	doi = {10.1016/j.neuron.2019.05.041},
	abstract = {Spike-timing-dependent synaptic plasticity (STDP) is a leading cellular model for behavioral learning and memory with rich computational properties. However, the relationship between the millisecond-precision spike timing required for STDP and the much slower timescales of behavioral learning is not well understood. Neuromodulation offers an attractive mechanism to connect these different timescales, and there is now strong experimental evidence that STDP is under neuromodulatory control by acetylcholine, monoamines, and other signaling molecules. Here, we review neuromodulation of STDP, the underlying mechanisms, functional implications, and possible involvement in brain disorders.},
	number = {4},
	urldate = {2023-12-18},
	journal = {Neuron},
	author = {Brzosko, Zuzanna and Mierau, Susanna B. and Paulsen, Ole},
	month = aug,
	year = {2019},
	keywords = {STDP, attention, disease, long-term depression, long-term potentiation, memory, neurodevelopment, neuromodulation, reinforcement learning, sleep, spike-timing-dependent plasticity, synaptic plasticity},
	pages = {563--581},
}

@misc{song_spiking-leaf_2023,
	title = {Spiking-{LEAF}: {A} {Learnable} {Auditory} front-end for {Spiking} {Neural} {Networks}},
	shorttitle = {Spiking-{LEAF}},
	url = {http://arxiv.org/abs/2309.09469},
	doi = {10.48550/arXiv.2309.09469},
	abstract = {Brain-inspired spiking neural networks (SNNs) have demonstrated great potential for temporal signal processing. However, their performance in speech processing remains limited due to the lack of an effective auditory front-end. To address this limitation, we introduce Spiking-LEAF, a learnable auditory front-end meticulously designed for SNN-based speech processing. Spiking-LEAF combines a learnable filter bank with a novel two-compartment spiking neuron model called IHC-LIF. The IHC-LIF neurons draw inspiration from the structure of inner hair cells (IHC) and they leverage segregated dendritic and somatic compartments to effectively capture multi-scale temporal dynamics of speech signals. Additionally, the IHC-LIF neurons incorporate the lateral feedback mechanism along with spike regularization loss to enhance spike encoding efficiency. On keyword spotting and speaker identification tasks, the proposed Spiking-LEAF outperforms both SOTA spiking auditory front-ends and conventional real-valued acoustic features in terms of classification accuracy, noise robustness, and encoding efficiency.},
	urldate = {2023-12-17},
	publisher = {arXiv},
	author = {Song, Zeyang and Wu, Jibin and Zhang, Malu and Shou, Mike Zheng and Li, Haizhou},
	month = sep,
	year = {2023},
	note = {arXiv:2309.09469 [cs, eess]},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{virgilio_g_spiking_2020,
	title = {Spiking {Neural} {Networks} applied to the classification of motor tasks in {EEG} signals},
	volume = {122},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608019303193},
	doi = {10.1016/j.neunet.2019.09.037},
	abstract = {Motivated by the recent progress of Spiking Neural Network (SNN) models in pattern recognition, we report on the development and evaluation of brain signal classifiers based on SNNs. The work shows the capabilities of this type of Spiking Neurons in the recognition of motor imagery tasks from EEG signals and compares their performance with other traditional classifiers commonly used in this application. This work includes two stages: the first stage consists of comparing the performance of the SNN models against some traditional neural network models. The second stage, compares the SNN models performance in two input conditions: input features with constant values and input features with temporal information. The EEG signals employed in this work represent five motor imagery tasks: i.e. rest, left hand, right hand, foot and tongue movements. These EEG signals were obtained from a public database provided by the Technological University of Graz (Brunner et al., 2008). The feature extraction stage was performed by applying two algorithms: power spectral density and wavelet decomposition. Likewise, this work uses raw EEG signals for the second stage of the problem solution. All of the models were evaluated in the classification between two motor imagery tasks. This work demonstrates that with a smaller number of Spiking neurons, simple problems can be solved. Better results are obtained by using patterns with temporal information, thereby exploiting the capabilities of the SNNs.},
	urldate = {2023-12-16},
	journal = {Neural Networks},
	author = {Virgilio G., Carlos D. and Sossa A., Juan H. and Antelis, Javier M. and Falcón, Luis E.},
	month = feb,
	year = {2020},
	keywords = {EEG signals, Izhikevich model, Motor imagery, Power Spectral Density, Spiking Neural Network, Wavelet Decomposition},
	pages = {130--143},
}

@article{graupner_calcium-based_2012,
	title = {Calcium-based plasticity model explains sensitivity of synaptic changes to spike pattern, rate, and dendritic location},
	volume = {109},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.1109359109},
	doi = {10.1073/pnas.1109359109},
	abstract = {Multiple stimulation protocols have been found to be effective in changing synaptic efficacy by inducing long-term potentiation or depression. In many of those protocols, increases in postsynaptic calcium concentration have been shown to play a crucial role. However, it is still unclear whether and how the dynamics of the postsynaptic calcium alone determine the outcome of synaptic plasticity. Here, we propose a calcium-based model of a synapse in which potentiation and depression are activated above calcium thresholds. We show that this model gives rise to a large diversity of spike timing-dependent plasticity curves, most of which have been observed experimentally in different systems. It accounts quantitatively for plasticity outcomes evoked by protocols involving patterns with variable spike timing and firing rate in hippocampus and neocortex. Furthermore, it allows us to predict that differences in plasticity outcomes in different studies are due to differences in parameters defining the calcium dynamics. The model provides a mechanistic understanding of how various stimulation protocols provoke specific synaptic changes through the dynamics of calcium concentration and thresholds implementing in simplified fashion protein signaling cascades, leading to long-term potentiation and long-term depression. The combination of biophysical realism and analytical tractability makes it the ideal candidate to study plasticity at the synapse, neuron, and network levels.},
	number = {10},
	urldate = {2023-12-13},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Graupner, Michael and Brunel, Nicolas},
	month = mar,
	year = {2012},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {3991--3996},
}

@article{zhang_gain_2009,
	title = {Gain in sensitivity and loss in temporal contrast of {STDP} by dopaminergic modulation at hippocampal synapses},
	volume = {106},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.0900546106},
	doi = {10.1073/pnas.0900546106},
	abstract = {Spike-timing-dependent plasticity (STDP) is considered a physiologically relevant form of Hebbian learning. However, behavioral learning often involves action of reinforcement or reward signals such as dopamine. Here, we examined how dopamine influences the quantitative rule of STDP at glutamatergic synapses of hippocampal neurons. The presence of 20 μM dopamine during paired pre- and postsynaptic spiking activity expanded the effective time window for timing-dependent long-term potentiation (t-LTP) to at least −45 ms, and allowed normally ineffective weak stimuli with fewer spike pairs to induce significant t-LTP. Meanwhile, dopamine did not affect the degree of t-LTP induced by normal strong stimuli with spike timing (ST) of +10 ms. Such dopamine-dependent enhancement in the sensitivity of t-LTP was completely blocked by the D1-like dopamine receptor antagonist SCH23390, but not by the D2-like dopamine receptor antagonist sulpiride. Surprisingly, timing-dependent long-term depression (t-LTD) at negative ST was converted into t-LTP by dopamine treatment; this conversion was also blocked by SCH23390. In addition, t-LTP in the presence of dopamine was completely blocked by the NMDA receptor antagonist 2-amino-5-phosphonovaleric acid, indicating that D1-like receptor-mediated modulation appears to act through the classical NMDA receptor-mediated signaling pathway that underlies STDP. These results provide a quantitative and mechanistic basis for a previously undescribed learning rule that depends on pre- and postsynaptic ST, as well as the global reward signal.},
	number = {31},
	urldate = {2023-12-13},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Zhang, Ji-Chuan and Lau, Pak-Ming and Bi, Guo-Qiang},
	month = aug,
	year = {2009},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {13028--13033},
}

@article{pereda_electrical_2014,
	title = {Electrical synapses and their functional interactions with chemical synapses},
	volume = {15},
	copyright = {2014 Springer Nature Limited},
	issn = {1471-0048},
	url = {https://www.nature.com/articles/nrn3708},
	doi = {10.1038/nrn3708},
	abstract = {There are two main modalities of synaptic transmission: chemical and electrical. Although chemical synapses are perceived to be structurally more complex and functionally dynamic than electrical synapses, emerging evidence indicates that electrical synapses might be similarly complex, functionally diverse and highly modifiable.Far from functioning independently and serving unrelated functions, these two modalities of synaptic transmission closely interact. Rather than conceiving synaptic transmission as either chemical or electrical, this article emphasizes the notion that synaptic transmission is both chemical and electrical, and that interactions between these two forms of interneuronal communication are required for normal brain development and function.The development of neural circuits in disparate nervous systems (both vertebrate and invertebrate) seems to rely critically on interactions between chemical and electrical synapses, which reciprocally and dynamically regulate the emergence of these two forms of transmission.During development, interactions between electrical synapses are crucial for the formation of neural circuits; however, such interactions in the adult brain result in dynamic reconfiguration of hardwired networks. The strength of electrical synapses is regulated by neuromodulaters such as dopamine and by glutamatergic synapses in an activity-dependent manner.Interactions between electrical and chemical synapses are also likely to have important pathological implications. Recapitulation of developmental interactions between chemical and electrical synapses has been observed after brain injury, and dysregulation of electrical synapses by neurotransmitters could contribute to cognitive impairment.},
	language = {en},
	number = {4},
	urldate = {2023-11-10},
	journal = {Nature Reviews Neuroscience},
	author = {Pereda, Alberto E.},
	month = apr,
	year = {2014},
	note = {Number: 4
Publisher: Nature Publishing Group},
	keywords = {Development of the nervous system, Gap junctions, Molecular neuroscience, 生物信号传输方式},
	pages = {250--263},
}

@misc{guo_rmp-loss_2023,
	title = {{RMP}-{Loss}: {Regularizing} {Membrane} {Potential} {Distribution} for {Spiking} {Neural} {Networks}},
	shorttitle = {{RMP}-{Loss}},
	url = {http://arxiv.org/abs/2308.06787},
	doi = {10.48550/arXiv.2308.06787},
	abstract = {Spiking Neural Networks (SNNs) as one of the biology-inspired models have received much attention recently. It can significantly reduce energy consumption since they quantize the real-valued membrane potentials to 0/1 spikes to transmit information thus the multiplications of activations and weights can be replaced by additions when implemented on hardware. However, this quantization mechanism will inevitably introduce quantization error, thus causing catastrophic information loss. To address the quantization error problem, we propose a regularizing membrane potential loss (RMP-Loss) to adjust the distribution which is directly related to quantization error to a range close to the spikes. Our method is extremely simple to implement and straightforward to train an SNN. Furthermore, it is shown to consistently outperform previous state-of-the-art methods over different network architectures and datasets.},
	urldate = {2023-09-23},
	publisher = {arXiv},
	author = {Guo, Yufei and Liu, Xiaode and Chen, Yuanpei and Zhang, Liwen and Peng, Weihang and Zhang, Yuhan and Huang, Xuhui and Ma, Zhe},
	month = aug,
	year = {2023},
	note = {arXiv:2308.06787 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, ��guoyufei, ��less than ours},
}

@misc{hao_reducing_2023,
	title = {Reducing {ANN}-{SNN} {Conversion} {Error} through {Residual} {Membrane} {Potential}},
	url = {http://arxiv.org/abs/2302.02091},
	doi = {10.48550/arXiv.2302.02091},
	abstract = {Spiking Neural Networks (SNNs) have received extensive academic attention due to the unique properties of low power consumption and high-speed computing on neuromorphic chips. Among various training methods of SNNs, ANN-SNN conversion has shown the equivalent level of performance as ANNs on large-scale datasets. However, unevenness error, which refers to the deviation caused by different temporal sequences of spike arrival on activation layers, has not been effectively resolved and seriously suffers the performance of SNNs under the condition of short time-steps. In this paper, we make a detailed analysis of unevenness error and divide it into four categories. We point out that the case of the ANN output being zero while the SNN output being larger than zero accounts for the largest percentage. Based on this, we theoretically prove the sufficient and necessary conditions of this case and propose an optimization strategy based on residual membrane potential to reduce unevenness error. The experimental results show that the proposed method achieves state-of-the-art performance on CIFAR-10, CIFAR-100, and ImageNet datasets. For example, we reach top-1 accuracy of 64.32{\textbackslash}\% on ImageNet with 10-steps. To the best of our knowledge, this is the first time ANN-SNN conversion can simultaneously achieve high accuracy and ultra-low-latency on the complex dataset. Code is available at https://github.com/hzc1208/ANN2SNN{\textbackslash}\_SRP.},
	urldate = {2023-09-22},
	publisher = {arXiv},
	author = {Hao, Zecheng and Bu, Tong and Ding, Jianhao and Huang, Tiejun and Yu, Zhaofei},
	month = feb,
	year = {2023},
	note = {arXiv:2302.02091 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing, SNN模型优化, 写 relatedwork, ��ANN2SNN, ��less than ours},
}

@misc{guo_neuroclip_2023,
	title = {{NeuroCLIP}: {Neuromorphic} {Data} {Understanding} by {CLIP} and {SNN}},
	shorttitle = {{NeuroCLIP}},
	url = {http://arxiv.org/abs/2306.12073},
	doi = {10.48550/arXiv.2306.12073},
	abstract = {Recently, the neuromorphic vision sensor has received more and more interest. However, the neuromorphic data consists of asynchronous event spikes, which is not natural and difficult to construct a benchmark, thus limiting the neuromorphic data understanding for "unseen" objects by deep learning. Zero-shot and few-shot learning via Contrastive Vision-Language Pre-training (CLIP) have shown inspirational performance in 2D frame image recognition. To handle "unseen" recognition for the neuromorphic data, in this paper, we propose NeuroCLIP, which transfers the CLIP's 2D pre-trained knowledge to event spikes. To improve the few-shot performance, we also provide an inter-timestep adapter based on a spiking neural network. Our code is open-sourced at https://github.com/yfguo91/NeuroCLIP.git.},
	urldate = {2023-09-28},
	publisher = {arXiv},
	author = {Guo, Yufei and Chen, Yuanpei},
	month = jun,
	year = {2023},
	note = {arXiv:2306.12073 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, SNN模型优化, 写 relatedwork, ��guoyufei},
}

@misc{liu_spiking-diffusion_2023,
	title = {Spiking-{Diffusion}: {Vector} {Quantized} {Discrete} {Diffusion} {Model} with {Spiking} {Neural} {Networks}},
	shorttitle = {Spiking-{Diffusion}},
	url = {http://arxiv.org/abs/2308.10187},
	doi = {10.48550/arXiv.2308.10187},
	abstract = {Spiking neural networks (SNNs) have tremendous potential for energy-efficient neuromorphic chips due to their binary and event-driven architecture. SNNs have been primarily used in classification tasks, but limited exploration on image generation tasks. To fill the gap, we propose a Spiking-Diffusion model, which is based on the vector quantized discrete diffusion model. First, we develop a vector quantized variational autoencoder with SNNs (VQ-SVAE) to learn a discrete latent space for images. In VQ-SVAE, image features are encoded using both the spike firing rate and postsynaptic potential, and an adaptive spike generator is designed to restore embedding features in the form of spike trains. Next, we perform absorbing state diffusion in the discrete latent space and construct a spiking diffusion image decoder (SDID) with SNNs to denoise the image. Our work is the first to build the diffusion model entirely from SNN layers. Experimental results on MNIST, FMNIST, KMNIST, Letters, and Cifar10 demonstrate that Spiking-Diffusion outperforms the existing SNN-based generation model. We achieve FIDs of 37.50, 91.98, 59.23, 67.41, and 120.5 on the above datasets respectively, with reductions of 58.60{\textbackslash}\%, 18.75{\textbackslash}\%, 64.51{\textbackslash}\%, 29.75{\textbackslash}\%, and 44.88{\textbackslash}\% in FIDs compared with the state-of-art work. Our code will be available at {\textbackslash}url\{https://github.com/Arktis2022/Spiking-Diffusion\}.},
	urldate = {2023-09-12},
	publisher = {arXiv},
	author = {Liu, Mingxuan and Wen, Rui and Chen, Hong},
	month = sep,
	year = {2023},
	note = {arXiv:2308.10187 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing, SNN模型优化, 写 relatedwork},
}

@misc{chen_artificial-spiking_2023,
	title = {Artificial-{Spiking} {Hierarchical} {Networks} for {Vision}-{Language} {Representation} {Learning}},
	url = {http://arxiv.org/abs/2308.09455},
	doi = {10.48550/arXiv.2308.09455},
	abstract = {With the success of self-supervised learning, multimodal foundation models have rapidly adapted a wide range of downstream tasks driven by vision and language (VL) pretraining. State-of-the-art methods achieve impressive performance by pre-training on large-scale datasets. However, bridging the semantic gap between the two modalities remains a nonnegligible challenge for VL tasks. In this work, we propose an efficient computation framework for multimodal alignment by introducing a novel visual semantic module to further improve the performance of the VL tasks. Specifically, we propose a flexible model, namely Artificial-Spiking Hierarchical Networks (ASH-Nets), which combines the complementary advantages of Artificial neural networks (ANNs) and Spiking neural networks (SNNs) to enrich visual semantic representations. In particular, a visual concrete encoder and a semantic abstract encoder are constructed to learn continuous and discrete latent variables to enhance the flexibility of semantic encoding. Considering the spatio-temporal properties of SNNs modeling, we introduce a contrastive learning method to optimize the inputs of similar samples. This can improve the computational efficiency of the hierarchical network, while the augmentation of hard samples is beneficial to the learning of visual representations. Furthermore, the Spiking to Text Uni-Alignment Learning (STUA) pre-training method is proposed, which only relies on text features to enhance the encoding ability of abstract semantics. We validate the performance on multiple well-established downstream VL tasks. Experiments show that the proposed ASH-Nets achieve competitive results.},
	urldate = {2023-08-27},
	publisher = {arXiv},
	author = {Chen, Yeming and Zhang, Siyu and Sun, Yaoru and Liang, Weijian and Wang, Haoran},
	month = aug,
	year = {2023},
	note = {arXiv:2308.09455 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, SNN-Vision-Language},
}

@article{flagel_selective_2011,
	title = {A selective role for dopamine in stimulus–reward learning},
	volume = {469},
	copyright = {2010 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature09588},
	doi = {10.1038/nature09588},
	abstract = {Individuals make choices and prioritize goals using complex processes that assign value to rewards and associated stimuli. During Pavlovian learning, previously neutral stimuli that predict rewards can acquire motivational properties, becoming attractive and desirable incentive stimuli. However, whether a cue acts solely as a predictor of reward, or also serves as an incentive stimulus, differs between individuals. Thus, individuals vary in the degree to which cues bias choice and potentially promote maladaptive behaviour. Here we use rats that differ in the incentive motivational properties they attribute to food cues to probe the role of the neurotransmitter dopamine in stimulus–reward learning. We show that intact dopamine transmission is not required for all forms of learning in which reward cues become effective predictors. Rather, dopamine acts selectively in a form of stimulus–reward learning in which incentive salience is assigned to reward cues. In individuals with a propensity for this form of learning, reward cues come to powerfully motivate and control behaviour. This work provides insight into the neurobiology of a form of stimulus–reward learning that confers increased susceptibility to disorders of impulse control.},
	language = {en},
	number = {7328},
	urldate = {2023-11-29},
	journal = {Nature},
	author = {Flagel, Shelly B. and Clark, Jeremy J. and Robinson, Terry E. and Mayo, Leah and Czuj, Alayna and Willuhn, Ingo and Akers, Christina A. and Clinton, Sarah M. and Phillips, Paul E. M. and Akil, Huda},
	month = jan,
	year = {2011},
	note = {Number: 7328
Publisher: Nature Publishing Group},
	keywords = {Learning and memory, Neurotransmitters, Psychology, 写 intro, ��Biologically-plausible},
	pages = {53--57},
}

@article{zhang_new_2002,
	title = {A new equivalent circuit different from the {Hodgkin}-{Huxley} model, and an equation for the resting membrane potential of a cell},
	volume = {6},
	issn = {1433-5298, 1614-7456},
	url = {http://link.springer.com/10.1007/BF02481329},
	doi = {10.1007/BF02481329},
	language = {en},
	number = {3},
	urldate = {2023-11-07},
	journal = {Artificial Life and Robotics},
	author = {Zhang, Xiaolin and Wakamatsu, Hidetoshi},
	month = sep,
	year = {2002},
	keywords = {写 relatedwork},
	pages = {140--148},
}

@article{zhang_brain-inspired_2023,
	title = {A brain-inspired algorithm that mitigates catastrophic forgetting of artificial and spiking neural networks with low computational cost},
	volume = {9},
	url = {https://www.science.org/doi/10.1126/sciadv.adi2947},
	doi = {10.1126/sciadv.adi2947},
	abstract = {Neuromodulators in the brain act globally at many forms of synaptic plasticity, represented as metaplasticity, which is rarely considered by existing spiking (SNNs) and nonspiking artificial neural networks (ANNs). Here, we report an efficient brain-inspired computing algorithm for SNNs and ANNs, referred to here as neuromodulation-assisted credit assignment (NACA), which uses expectation signals to induce defined levels of neuromodulators to selective synapses, whereby the long-term synaptic potentiation and depression are modified in a nonlinear manner depending on the neuromodulator level. The NACA algorithm achieved high recognition accuracy with substantially reduced computational cost in learning spatial and temporal classification tasks. Notably, NACA was also verified as efficient for learning five different class continuous learning tasks with varying degrees of complexity, exhibiting a markedly mitigated catastrophic forgetting at low computational cost. Mapping synaptic weight changes showed that these benefits could be explained by the sparse and targeted synaptic modifications attributed to expectation-based global neuromodulation.},
	number = {34},
	urldate = {2023-08-27},
	journal = {Science Advances},
	author = {Zhang, Tielin and Cheng, Xiang and Jia, Shuncheng and Li, Chengyu T and Poo, Mu-ming and Xu, Bo},
	month = aug,
	year = {2023},
	note = {Publisher: American Association for the Advancement of Science},
	keywords = {写 relatedwork, ��Biologically-plausible},
	pages = {eadi2947},
}

@article{tewari_mathematical_2012,
	title = {A mathematical model of the tripartite synapse: astrocyte-induced synaptic plasticity},
	volume = {38},
	issn = {1573-0689},
	shorttitle = {A mathematical model of the tripartite synapse},
	url = {https://doi.org/10.1007/s10867-012-9267-7},
	doi = {10.1007/s10867-012-9267-7},
	abstract = {In this paper, we present a biologically detailed mathematical model of tripartite synapses, where astrocytes modulate short-term synaptic plasticity. The model consists of a pre-synaptic bouton, a post-synaptic dendritic spine-head, a synaptic cleft and a peri-synaptic astrocyte controlling Ca2 +  dynamics inside the synaptic bouton. This in turn controls glutamate release dynamics in the cleft. As a consequence of this, glutamate concentration in the cleft has been modeled, in which glutamate reuptake by astrocytes has also been incorporated. Finally, dendritic spine-head dynamics has been modeled. As an application, this model clearly shows synaptic potentiation in the hippocampal region, i.e., astrocyte Ca2 +  mediates synaptic plasticity, which is in conformity with the majority of the recent findings (Perea and Araque (Science 317, 1083–1086, 2007); Henneberger et al. (Nature 463, 232–236, 2010); Navarrete et al. (PLoS Biol. 10, e1001259, 2012)).},
	language = {en},
	number = {3},
	urldate = {2023-11-26},
	journal = {Journal of Biological Physics},
	author = {Tewari, Shivendra G. and Majumdar, Kaushik Kumar},
	month = jun,
	year = {2012},
	keywords = {Astrocyte, Calcium dynamics, MATH, Short-term potentiation, Tripartite synapse, ��glia},
	pages = {465--496},
}

@article{lobo_spiking_2020,
	title = {Spiking {Neural} {Networks} and online learning: {An} overview and perspectives},
	volume = {121},
	issn = {0893-6080},
	shorttitle = {Spiking {Neural} {Networks} and online learning},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608019302655},
	doi = {10.1016/j.neunet.2019.09.004},
	abstract = {Applications that generate huge amounts of data in the form of fast streams are becoming increasingly prevalent, being therefore necessary to learn in an online manner. These conditions usually impose memory and processing time restrictions, and they often turn into evolving environments where a change may affect the input data distribution. Such a change causes that predictive models trained over these stream data become obsolete and do not adapt suitably to new distributions. Specially in these non-stationary scenarios, there is a pressing need for new algorithms that adapt to these changes as fast as possible, while maintaining good performance scores. Unfortunately, most off-the-shelf classification models need to be retrained if they are used in changing environments, and fail to scale properly. Spiking Neural Networks have revealed themselves as one of the most successful approaches to model the behavior and learning potential of the brain, and exploit them to undertake practical online learning tasks. Besides, some specific flavors of Spiking Neural Networks can overcome the necessity of retraining after a drift occurs. This work intends to merge both fields by serving as a comprehensive overview, motivating further developments that embrace Spiking Neural Networks for online learning scenarios, and being a friendly entry point for non-experts.},
	urldate = {2023-11-27},
	journal = {Neural Networks},
	author = {Lobo, Jesus L. and Del Ser, Javier and Bifet, Albert and Kasabov, Nikola},
	month = jan,
	year = {2020},
	keywords = {Concept drift, Online learning, Review, SNN},
	pages = {88--100},
}

@article{glyzin_method_2013,
	title = {On a method for mathematical modeling of chemical synapses},
	volume = {49},
	issn = {1608-3083},
	url = {https://doi.org/10.1134/S0012266113100017},
	doi = {10.1134/S0012266113100017},
	abstract = {We introduce a new mathematical model of a circular neural network with unidirectional chemical bonds. The model is a singularly perturbed system of delay differential-difference equations. We study the existence and stability of relaxation periodic motions in the system. It is proved that the well-known buffer phenomenon can occur in the model.},
	language = {en},
	number = {10},
	urldate = {2023-11-26},
	journal = {Differential Equations},
	author = {Glyzin, S. D. and Kolesov, A. Yu. and Rozov, N. Kh.},
	month = oct,
	year = {2013},
	keywords = {Asymptotic Representation, MATH, Relaxation Oscillation, Stable Cycle},
	pages = {1193--1210},
}

@article{hoskin_parallel_2022,
	title = {Parallel transmission in a synthetic nerve},
	volume = {14},
	copyright = {2022 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1755-4349},
	url = {https://www.nature.com/articles/s41557-022-00916-1},
	doi = {10.1038/s41557-022-00916-1},
	abstract = {Bioelectronic devices that are tetherless and soft are promising developments in medicine, robotics and chemical computing. Here, we describe bioinspired synthetic neurons, composed entirely of soft, flexible biomaterials, capable of rapid electrochemical signal transmission over centimetre distances. Like natural cells, our synthetic neurons release neurotransmitters from their terminals, which initiate downstream reactions. The components of the neurons are nanolitre aqueous droplets and hydrogel fibres, connected through lipid bilayers. Transmission is powered at these interfaces by light-driven proton pumps and mediated by ion-conducting protein pores. By bundling multiple neurons into a synthetic nerve, we have shown that distinct signals can propagate simultaneously along parallel axons, thereby transmitting spatiotemporal information. Synthetic nerves might play roles in next-generation implants, soft machines and computing devices.},
	language = {en},
	number = {6},
	urldate = {2023-11-26},
	journal = {Nature Chemistry},
	author = {Hoskin, Charlotte E. G. and Schild, Vanessa Restrepo and Vinals, Javier and Bayley, Hagan},
	month = jun,
	year = {2022},
	note = {Number: 6
Publisher: Nature Publishing Group},
	keywords = {Bioinspired materials, Bionanoelectronics},
	pages = {650--657},
}

@article{zhang_tuning_2022,
	title = {Tuning {Convolutional} {Spiking} {Neural} {Network} {With} {Biologically} {Plausible} {Reward} {Propagation}},
	volume = {33},
	issn = {2162-2388},
	url = {https://ieeexplore.ieee.org/abstract/document/9454259},
	doi = {10.1109/TNNLS.2021.3085966},
	abstract = {Spiking neural networks (SNNs) contain more biologically realistic structures and biologically inspired learning principles than those in standard artificial neural networks (ANNs). SNNs are considered the third generation of ANNs, powerful on the robust computation with a low computational cost. The neurons in SNNs are nondifferential, containing decayed historical states and generating event-based spikes after their states reaching the firing threshold. These dynamic characteristics of SNNs make it difficult to be directly trained with the standard backpropagation (BP), which is also considered not biologically plausible. In this article, a biologically plausible reward propagation (BRP) algorithm is proposed and applied to the SNN architecture with both spiking-convolution (with both 1-D and 2-D convolutional kernels) and full-connection layers. Unlike the standard BP that propagates error signals from postsynaptic to presynaptic neurons layer by layer, the BRP propagates target labels instead of errors directly from the output layer to all prehidden layers. This effort is more consistent with the top-down reward-guiding learning in cortical columns of the neocortex. Synaptic modifications with only local gradient differences are induced with pseudo-BP that might also be replaced with the spike-timing-dependent plasticity (STDP). The performance of the proposed BRP-SNN is further verified on the spatial (including MNIST and Cifar-10) and temporal (including TIDigits and DvsGesture) tasks, where the SNN using BRP has reached a similar accuracy compared to other state-of-the-art (SOTA) BP-based SNNs and saved 50\% more computational cost than ANNs. We think that the introduction of biologically plausible learning rules to the training procedure of biologically realistic SNNs will give us more hints and inspiration toward a better understanding of the biological system’s intelligent nature.},
	number = {12},
	urldate = {2023-11-27},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Zhang, Tielin and Jia, Shuncheng and Cheng, Xiang and Xu, Bo},
	month = dec,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {��Biologically-plausible},
	pages = {7621--7631},
}

@inproceedings{yin_effective_2020,
	address = {New York, NY, USA},
	series = {{ICONS} 2020},
	title = {Effective and {Efficient} {Computation} with {Multiple}-timescale {Spiking} {Recurrent} {Neural} {Networks}},
	isbn = {978-1-4503-8851-1},
	url = {https://doi.org/10.1145/3407197.3407225},
	doi = {10.1145/3407197.3407225},
	abstract = {The emergence of brain-inspired neuromorphic computing as a paradigm for edge AI is motivating the search for high-performance and efficient spiking neural networks to run on this hardware. However, compared to classical neural networks in deep learning, current spiking neural networks lack competitive performance in compelling areas. Here, for sequential and streaming tasks, we demonstrate how a novel type of adaptive spiking recurrent neural network (SRNN) is able to achieve state-of-the-art performance compared to other spiking neural networks and almost reach or exceed the performance of classical recurrent neural networks (RNNs) while exhibiting sparse activity. From this, we calculate a {\textgreater} 100x energy improvement for our SRNNs over classical RNNs on the harder tasks. To achieve this, we model standard and adaptive multiple-timescale spiking neurons as self-recurrent neural units, and leverage surrogate gradients and auto-differentiation in the PyTorch Deep Learning framework to efficiently implement backpropagation-through-time, including learning of the important spiking neuron parameters to adapt our spiking neurons to the tasks.},
	urldate = {2023-11-19},
	booktitle = {International {Conference} on {Neuromorphic} {Systems} 2020},
	publisher = {Association for Computing Machinery},
	author = {Yin, Bojian and Corradi, Federico and Bohté, Sander M.},
	month = jul,
	year = {2020},
	keywords = {SNN, backpropagation through time, 一种surrogate gradient, ��learnable},
	pages = {1--8},
}

@article{hassabis_neuroscience-inspired_2017,
	title = {Neuroscience-{Inspired} {Artificial} {Intelligence}},
	volume = {95},
	issn = {0896-6273},
	url = {https://www.sciencedirect.com/science/article/pii/S0896627317305093},
	doi = {10.1016/j.neuron.2017.06.011},
	abstract = {The fields of neuroscience and artificial intelligence (AI) have a long and intertwined history. In more recent times, however, communication and collaboration between the two fields has become less commonplace. In this article, we argue that better understanding biological brains could play a vital role in building intelligent machines. We survey historical interactions between the AI and neuroscience fields and emphasize current advances in AI that have been inspired by the study of neural computation in humans and other animals. We conclude by highlighting shared themes that may be key for advancing future research in both fields.},
	number = {2},
	urldate = {2023-11-27},
	journal = {Neuron},
	author = {Hassabis, Demis and Kumaran, Dharshan and Summerfield, Christopher and Botvinick, Matthew},
	month = jul,
	year = {2017},
	keywords = {Learning, Review, artificial intelligence, brain, cognition, neural network},
	pages = {245--258},
}

@article{wu_liaf-net_2022,
	title = {{LIAF}-{Net}: {Leaky} {Integrate} and {Analog} {Fire} {Network} for {Lightweight} and {Efficient} {Spatiotemporal} {Information} {Processing}},
	volume = {33},
	issn = {2162-2388},
	shorttitle = {{LIAF}-{Net}},
	url = {https://ieeexplore.ieee.org/abstract/document/9429228},
	doi = {10.1109/TNNLS.2021.3073016},
	abstract = {Spiking neural networks (SNNs) based on the leaky integrate and fire (LIF) model have been applied to energy-efficient temporal and spatiotemporal processing tasks. Due to the bioplausible neuronal dynamics and simplicity, LIF-SNN benefits from event-driven processing, however, usually face the embarrassment of reduced performance. This may because, in LIF-SNN, the neurons transmit information via spikes. To address this issue, in this work, we propose a leaky integrate and analog fire (LIAF) neuron model so that analog values can be transmitted among neurons, and a deep network termed LIAF-Net is built on it for efficient spatiotemporal processing. In the temporal domain, LIAF follows the traditional LIF dynamics to maintain its temporal processing capability. In the spatial domain, LIAF is able to integrate spatial information through convolutional integration or fully connected integration. As a spatiotemporal layer, LIAF can also be used with traditional artificial neural network (ANN) layers jointly. In addition, the built network can be trained with backpropagation through time (BPTT) directly, which avoids the performance loss caused by ANN to SNN conversion. Experiment results indicate that LIAF-Net achieves comparable performance to the gated recurrent unit (GRU) and long short-term memory (LSTM) on bAbI question answering (QA) tasks and achieves state-of-the-art performance on spatiotemporal dynamic vision sensor (DVS) data sets, including MNIST-DVS, CIFAR10-DVS, and DVS128 Gesture, with much less number of synaptic weights and computational overhead compared with traditional networks built by LSTM, GRU, convolutional LSTM (ConvLSTM), or 3-D convolution (Conv3D). Compared with traditional LIF-SNN, LIAF-Net also shows dramatic accuracy gain on all these experiments. In conclusion, LIAF-Net provides a framework combining the advantages of both ANNs and SNNs for lightweight and efficient spatiotemporal information processing.},
	number = {11},
	urldate = {2023-11-29},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Wu, Zhenzhi and Zhang, Hehui and Lin, Yihan and Li, Guoqi and Wang, Meng and Tang, Ye},
	month = nov,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	pages = {6249--6262},
}

@article{hu_neuroscience_2022,
	title = {Neuroscience and {Network} {Dynamics} {Toward} {Brain}-{Inspired} {Intelligence}},
	volume = {52},
	issn = {2168-2275},
	url = {https://ieeexplore.ieee.org/abstract/document/9418554/metrics#metrics},
	doi = {10.1109/TCYB.2021.3071110},
	abstract = {This article surveys the interdisciplinary research of neuroscience, network science, and dynamic systems, with emphasis on the emergence of brain-inspired intelligence. To replicate brain intelligence, a practical way is to reconstruct cortical networks with dynamic activities that nourish the brain functions, instead of using only artificial computing networks. The survey provides a complex network and spatiotemporal dynamics (abbr. network dynamics) perspective for understanding the brain and cortical networks and, furthermore, develops integrated approaches of neuroscience and network dynamics toward building brain-inspired intelligence with learning and resilience functions. Presented are fundamental concepts and principles of complex networks, neuroscience, and hybrid dynamic systems, as well as relevant studies about the brain and intelligence. Other promising research directions, such as brain science, data science, quantum information science, and machine behavior are also briefly discussed toward future applications.},
	number = {10},
	urldate = {2023-11-27},
	journal = {IEEE Transactions on Cybernetics},
	author = {Hu, Bin and Guan, Zhi-Hong and Chen, Guanrong and Chen, C. L. Philip},
	month = oct,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Cybernetics},
	pages = {10214--10227},
}

@misc{noauthor_international_nodate,
	title = {International {Journal} of {Neural} {Systems}},
	url = {https://www.worldscientific.com/doi/abs/10.1142/S0129065709002002},
	urldate = {2023-11-27},
}

@article{pietras_exact_2019,
	title = {Exact firing rate model reveals the differential effects of chemical versus electrical synapses in spiking networks},
	volume = {100},
	issn = {2470-0045, 2470-0053},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.100.042412},
	doi = {10.1103/PhysRevE.100.042412},
	language = {en},
	number = {4},
	urldate = {2023-11-27},
	journal = {Physical Review E},
	author = {Pietras, Bastian and Devalle, Federico and Roxin, Alex and Daffertshofer, Andreas and Montbrió, Ernest},
	month = oct,
	year = {2019},
	keywords = {QIF},
	pages = {042412},
}

@misc{noauthor_evolving_nodate,
	title = {Evolving spiking neural network—a survey {\textbar} {Evolving} {Systems}},
	url = {https://link.springer.com/article/10.1007/s12530-013-9074-9},
	urldate = {2023-11-27},
}

@article{kasabov_dynamic_2013,
	series = {Special {Issue} on {Autonomous} {Learning}},
	title = {Dynamic evolving spiking neural networks for on-line spatio- and spectro-temporal pattern recognition},
	volume = {41},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608012003139},
	doi = {10.1016/j.neunet.2012.11.014},
	abstract = {On-line learning and recognition of spatio- and spectro-temporal data (SSTD) is a very challenging task and an important one for the future development of autonomous machine learning systems with broad applications. Models based on spiking neural networks (SNN) have already proved their potential in capturing spatial and temporal data. One class of them, the evolving SNN (eSNN), uses a one-pass rank-order learning mechanism and a strategy to evolve a new spiking neuron and new connections to learn new patterns from incoming data. So far these networks have been mainly used for fast image and speech frame-based recognition. Alternative spike-time learning methods, such as Spike-Timing Dependent Plasticity (STDP) and its variant Spike Driven Synaptic Plasticity (SDSP), can also be used to learn spatio-temporal representations, but they usually require many iterations in an unsupervised or semi-supervised mode of learning. This paper introduces a new class of eSNN, dynamic eSNN, that utilise both rank-order learning and dynamic synapses to learn SSTD in a fast, on-line mode. The paper also introduces a new model called deSNN, that utilises rank-order learning and SDSP spike-time learning in unsupervised, supervised, or semi-supervised modes. The SDSP learning is used to evolve dynamically the network changing connection weights that capture spatio-temporal spike data clusters both during training and during recall. The new deSNN model is first illustrated on simple examples and then applied on two case study applications: (1) moving object recognition using address-event representation (AER) with data collected using a silicon retina device; (2) EEG SSTD recognition for brain–computer interfaces. The deSNN models resulted in a superior performance in terms of accuracy and speed when compared with other SNN models that use either rank-order or STDP learning. The reason is that the deSNN makes use of both the information contained in the order of the first input spikes (which information is explicitly present in input data streams and would be crucial to consider in some tasks) and of the information contained in the timing of the following spikes that is learned by the dynamic synapses as a whole spatio-temporal pattern.},
	urldate = {2023-11-27},
	journal = {Neural Networks},
	author = {Kasabov, Nikola and Dhoble, Kshitij and Nuntalid, Nuttapod and Indiveri, Giacomo},
	month = may,
	year = {2013},
	keywords = {Dynamic synapses, EEG pattern recognition, Evolving connectionist systems, Moving object recognition, Rank-order coding, SNN, Spatio-temporal pattern recognition, Spike time based learning},
	pages = {188--201},
}

@article{tavanaei_deep_2019,
	title = {Deep learning in spiking neural networks},
	volume = {111},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608018303332},
	doi = {10.1016/j.neunet.2018.12.002},
	abstract = {In recent years, deep learning has revolutionized the field of machine learning, for computer vision in particular. In this approach, a deep (multilayer) artificial neural network (ANN) is trained, most often in a supervised manner using backpropagation. Vast amounts of labeled training examples are required, but the resulting classification accuracy is truly impressive, sometimes outperforming humans. Neurons in an ANN are characterized by a single, static, continuous-valued activation. Yet biological neurons use discrete spikes to compute and transmit information, and the spike times, in addition to the spike rates, matter. Spiking neural networks (SNNs) are thus more biologically realistic than ANNs, and are arguably the only viable option if one wants to understand how the brain computes at the neuronal description level. The spikes of biological neurons are sparse in time and space, and event-driven. Combined with bio-plausible local learning rules, this makes it easier to build low-power, neuromorphic hardware for SNNs. However, training deep SNNs remains a challenge. Spiking neurons’ transfer function is usually non-differentiable, which prevents using backpropagation. Here we review recent supervised and unsupervised methods to train deep SNNs, and compare them in terms of accuracy and computational cost. The emerging picture is that SNNs still lag behind ANNs in terms of accuracy, but the gap is decreasing, and can even vanish on some tasks, while SNNs typically require many fewer operations and are the better candidates to process spatio-temporal data.},
	urldate = {2023-11-27},
	journal = {Neural Networks},
	author = {Tavanaei, Amirhossein and Ghodrati, Masoud and Kheradpisheh, Saeed Reza and Masquelier, Timothée and Maida, Anthony},
	month = mar,
	year = {2019},
	keywords = {Deep learning, Machine learning, Power-efficient architecture, SNN, ��Biologically-plausible},
	pages = {47--63},
}

@article{wang_brain-inspired_2020,
	title = {Brain-{Inspired} {Systems}: {A} {Transdisciplinary} {Exploration} on {Cognitive} {Cybernetics}, {Humanity}, and {Systems} {Science} {Toward} {Autonomous} {Artificial} {Intelligence}},
	volume = {6},
	issn = {2333-942X},
	shorttitle = {Brain-{Inspired} {Systems}},
	url = {https://ieeexplore.ieee.org/abstract/document/8960591},
	doi = {10.1109/MSMC.2018.2889502},
	abstract = {Brain-inspired cognitive systems (BCSs) are an emerging field of cybernetics, cognitive science, and system science. BCSs study not only the intelligence science foundations of artificial intelligence (AI) and cognitive systems, but also formal models of the brain embodied by computational intelligence. This article presents the brain and intelligence science foundations of BCS toward hybrid intelligent systems and the symbiotic intelligence of humanity. It explores the transdisciplinary theoretical foundations of system, brain, intelligence, knowledge, cybernetic, and cognitive sciences toward the next generation of knowledge processors beyond classic data processors for autonomous computing systems. A BCS provides an overarching platform for cognitive cybernetics, humanity, and systems to enable emerging hybrid societies shared by humans and intelligent machines.},
	number = {1},
	urldate = {2023-11-27},
	journal = {IEEE Systems, Man, and Cybernetics Magazine},
	author = {Wang, Yingxu and Kwong, Sam and Leung, Henry and Lu, Jianhua and Smith, Michael H. and Trajkovic, Ljiljana and Tunstel, Edward and Plataniotis, Konstantinos N. and Yen, Gary G. and Kinsner, Witold},
	month = jan,
	year = {2020},
	note = {Conference Name: IEEE Systems, Man, and Cybernetics Magazine},
	pages = {6--13},
}

@article{mehonic_brain-inspired_2022,
	title = {Brain-inspired computing needs a master plan},
	volume = {604},
	copyright = {2022 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-021-04362-w},
	doi = {10.1038/s41586-021-04362-w},
	abstract = {New computing technologies inspired by the brain promise fundamentally different ways to process information with extreme energy efficiency and the ability to handle the avalanche of unstructured and noisy data that we are generating at an ever-increasing rate. To realize this promise requires a brave and coordinated plan to bring together disparate research communities and to provide them with the funding, focus and support needed. We have done this in the past with digital technologies; we are in the process of doing it with quantum technologies; can we now do it for brain-inspired computing?},
	language = {en},
	number = {7905},
	urldate = {2023-11-27},
	journal = {Nature},
	author = {Mehonic, A. and Kenyon, A. J.},
	month = apr,
	year = {2022},
	note = {Number: 7905
Publisher: Nature Publishing Group},
	keywords = {Electrical and electronic engineering, Information technology},
	pages = {255--260},
}

@misc{noauthor_basic_2021,
	title = {Basic {Guide} to {Spiking} {Neural} {Networks} for {Deep} {Learning} {\textbar} cnvrg.io},
	url = {https://cnvrg.io/spiking-neural-networks/},
	abstract = {Nowadays, Deep Learning (DL) is a hot topic within the Data Science community. Despite being quite effective in various tasks across the industries Deep},
	language = {en-US},
	urldate = {2023-11-26},
	month = jun,
	year = {2021},
	note = {Section: Data Science},
}

@misc{rafi_brief_2021,
	title = {A {Brief} {Review} on {Spiking} {Neural} {Network} - {A} {Biological} {Inspiration}},
	url = {https://www.preprints.org/manuscript/202104.0202/v1},
	doi = {10.20944/preprints202104.0202.v1},
	abstract = {Recent advancement of deep learning has been elevated the multifaceted nature in various applications of this field. Artificial neural networks are now turning into a genuinely old procedure in the vast area of computer science; the principal thoughts and models are more than fifty years of age. However, in this modern computing era, 3rd generation intelligent models are introduced by scientists. In the biological neuron, actual film channels control the progression of particles over the layer by opening and shutting in light of voltage changes because of inborn current flows and remotely led to signals. A comprehensive 3rd generation, Spiking Neural Network (SNN) is diminishing the distance between deep learning, machine learning, and neuroscience in a biologically-inspired manner. It also connects neuroscience and machine learning to establish high-level efficient computing. Spiking Neural Networks initiate utilizing spikes, which are discrete functions that happen at focuses as expected, as opposed to constant values. This paper is a review of the biological-inspired spiking neural network and its applications in different areas. The author aims to present a brief introduction to SNN, which incorporates the mathematical structure, applications, and implementation of SNN. This paper also represents an overview of machine learning, deep learning, and reinforcement learning. This review paper can help advanced artificial intelligence researchers to get a compact brief intuition of spiking neural networks.},
	language = {en},
	urldate = {2023-11-27},
	publisher = {Preprints},
	author = {Rafi, Taki Hasan},
	month = apr,
	year = {2021},
	keywords = {Deep Learning, Neuromorphic Computing, SNN, ��Biologically-plausible},
}

@article{shen_brain-inspired_2023,
	title = {Brain-inspired neural circuit evolution for spiking neural networks},
	volume = {120},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.2218173120},
	doi = {10.1073/pnas.2218173120},
	abstract = {In biological neural systems, different neurons are capable of self-organizing to form different neural circuits for achieving a variety of cognitive functions. However, the current design paradigm of spiking neural networks is based on structures derived from deep learning. Such structures are dominated by feedforward connections without taking into account different types of neurons, which significantly prevent spiking neural networks from realizing their potential on complex tasks. It remains an open challenge to apply the rich dynamical properties of biological neural circuits to model the structure of current spiking neural networks. This paper provides a more biologically plausible evolutionary space by combining feedforward and feedback connections with excitatory and inhibitory neurons. We exploit the local spiking behavior of neurons to adaptively evolve neural circuits such as forward excitation, forward inhibition, feedback inhibition, and lateral inhibition by the local law of spike-timing-dependent plasticity and update the synaptic weights in combination with the global error signals. By using the evolved neural circuits, we construct spiking neural networks for image classification and reinforcement learning tasks. Using the brain-inspired Neural circuit Evolution strategy (NeuEvo) with rich neural circuit types, the evolved spiking neural network greatly enhances capability on perception and reinforcement learning tasks. NeuEvo achieves state-of-the-art performance on CIFAR10, DVS-CIFAR10, DVS-Gesture, and N-Caltech101 datasets and achieves advanced performance on ImageNet. Combined with on-policy and off-policy deep reinforcement learning algorithms, it achieves comparable performance with artificial neural networks. The evolved spiking neural circuits lay the foundation for the evolution of complex networks with functions.},
	number = {39},
	urldate = {2023-11-27},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Shen, Guobin and Zhao, Dongcheng and Dong, Yiting and Zeng, Yi},
	month = sep,
	year = {2023},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {e2218173120},
}

@inproceedings{li_differentiable_2021,
	title = {Differentiable {Spike}: {Rethinking} {Gradient}-{Descent} for {Training} {Spiking} {Neural} {Networks}},
	volume = {34},
	shorttitle = {Differentiable {Spike}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/c4ca4238a0b923820dcc509a6f75849b-Abstract.html},
	abstract = {Spiking Neural Networks (SNNs) have emerged as a biology-inspired method mimicking the spiking nature of brain neurons. This bio-mimicry derives SNNs' energy efficiency of inference on neuromorphic hardware. However, it also causes an intrinsic disadvantage in training high-performing SNNs from scratch since the discrete spike prohibits the gradient calculation. To overcome this issue, the surrogate gradient (SG) approach has been proposed as a continuous relaxation. Yet the heuristic choice of SG leaves it vacant how the SG benefits the SNN training. In this work, we first theoretically study the gradient descent problem in SNN training and introduce finite difference gradient to quantitatively analyze the training behavior of SNN. Based on the introduced finite difference gradient, we propose a new family of Differentiable Spike (Dspike) functions that can adaptively evolve during training to find the optimal shape and smoothness for gradient estimation. Extensive experiments over several popular network structures show that training SNN with Dspike consistently outperforms the state-of-the-art training methods. For example, on the CIFAR10-DVS classification task, we can train a spiking ResNet-18 and achieve 75.4\% top-1 accuracy with 10 time steps.},
	urldate = {2023-11-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Li, Yuhang and Guo, Yufei and Zhang, Shanghang and Deng, Shikuang and Hai, Yongqing and Gu, Shi},
	year = {2021},
	keywords = {��guoyufei},
	pages = {23426--23439},
}

@article{zhang_self-backpropagation_2021,
	title = {Self-backpropagation of synaptic modifications elevates the efficiency of spiking and artificial neural networks},
	volume = {7},
	url = {https://www.science.org/doi/full/10.1126/sciadv.abh0146},
	doi = {10.1126/sciadv.abh0146},
	abstract = {Many synaptic plasticity rules found in natural circuits have not been incorporated into artificial neural networks (ANNs). We showed that incorporating a nonlocal feature of synaptic plasticity found in natural neural networks, whereby synaptic modification at output synapses of a neuron backpropagates to its input synapses made by upstream neurons, markedly reduced the computational cost without affecting the accuracy of spiking neural networks (SNNs) and ANNs in supervised learning for three benchmark tasks. For SNNs, synaptic modification at output neurons generated by spike timing–dependent plasticity was allowed to self-propagate to limited upstream synapses. For ANNs, modified synaptic weights via conventional backpropagation algorithm at output neurons self-backpropagated to limited upstream synapses. Such self-propagating plasticity may produce coordinated synaptic modifications across neuronal layers that reduce computational cost.},
	number = {43},
	urldate = {2023-11-27},
	journal = {Science Advances},
	author = {Zhang, Tielin and Cheng, Xiang and Jia, Shuncheng and Poo, Mu-ming and Zeng, Yi and Xu, Bo},
	month = oct,
	year = {2021},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {eabh0146},
}

@misc{noauthor_long-term_nodate,
	title = {Long-{Term} {Potentiation}--{A} {Decade} of {Progress}?},
	url = {https://www.science.org/doi/epdf/10.1126/science.285.5435.1870},
	urldate = {2023-11-27},
}

@article{susi_path_2016,
	title = {{PATH} {MULTIMODALITY} {IN} {A} {FEEDFORWARD} {SNN} {MODULE}, {USING} {LIF} {WITH} {LATENCY} {MODEL}},
	volume = {26},
	issn = {12100552, 23364335},
	url = {http://nnw.cz/obsahy16.html#26.021},
	doi = {10.14311/NNW.2016.26.021},
	abstract = {In this paper, the network transmission properties of a feedforward Spiking Neural Network (SNN) aﬀected by synchronous stimuli are investigated with respect to the connection probability and the synaptic strengths. By means of an event-driven method, all simulations are conducted using the Leaky Integrateand-Fire with Latency (LIFL) model. Typical cases are taken into consideration, in which a network section (module) is able to process the input information, introducing a particular behavior, that we have called path multimodality. Simulation results are discussed. Through this phenomenon, the output layer of the network can generate a number of temporally spaced groups of synchronous spikes. The multimodality eﬀect could be applied for various purposes, for instance in coding or else transmission issues.},
	language = {en},
	number = {4},
	urldate = {2023-11-27},
	journal = {Neural Network World},
	author = {Susi, Gianluca and Cristini, Alessandro and Salerno, Mario},
	year = {2016},
	pages = {363--376},
}

@article{ponulak_introduction_2011,
	title = {Introduction to spiking neural networks: {Information} processing, learning and applications},
	volume = {71},
	issn = {1689-0035},
	shorttitle = {Introduction to spiking neural networks},
	abstract = {The concept that neural information is encoded in the firing rate of neurons has been the dominant paradigm in neurobiology for many years. This paradigm has also been adopted by the theory of artificial neural networks. Recent physiological experiments demonstrate, however, that in many parts of the nervous system, neural code is founded on the timing of individual action potentials. This finding has given rise to the emergence of a new class of neural models, called spiking neural networks. In this paper we summarize basic properties of spiking neurons and spiking networks. Our focus is, specifically, on models of spike-based information coding, synaptic plasticity and learning. We also survey real-life applications of spiking models. The paper is meant to be an introduction to spiking neural networks for scientists from various disciplines interested in spike-based neural processing.},
	language = {eng},
	number = {4},
	journal = {Acta neurobiologiae experimentalis},
	author = {Ponulak, Filip and Kasinski, Andrzej},
	month = jan,
	year = {2011},
	pmid = {22237491},
	pages = {409--433},
}

@article{kheradpisheh_stdp-based_2018,
	title = {{STDP}-based spiking deep convolutional neural networks for object recognition},
	volume = {99},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608017302903},
	doi = {10.1016/j.neunet.2017.12.005},
	abstract = {Previous studies have shown that spike-timing-dependent plasticity (STDP) can be used in spiking neural networks (SNN) to extract visual features of low or intermediate complexity in an unsupervised manner. These studies, however, used relatively shallow architectures, and only one layer was trainable. Another line of research has demonstrated – using rate-based neural networks trained with back-propagation – that having many layers increases the recognition robustness, an approach known as deep learning. We thus designed a deep SNN, comprising several convolutional (trainable with STDP) and pooling layers. We used a temporal coding scheme where the most strongly activated neurons fire first, and less activated neurons fire later or not at all. The network was exposed to natural images. Thanks to STDP, neurons progressively learned features corresponding to prototypical patterns that were both salient and frequent. Only a few tens of examples per category were required and no label was needed. After learning, the complexity of the extracted features increased along the hierarchy, from edge detectors in the first layer to object prototypes in the last layer. Coding was very sparse, with only a few thousands spikes per image, and in some cases the object category could be reasonably well inferred from the activity of a single higher-order neuron. More generally, the activity of a few hundreds of such neurons contained robust category information, as demonstrated using a classifier on Caltech 101, ETH-80, and MNIST databases. We also demonstrate the superiority of STDP over other unsupervised techniques such as random crops (HMAX) or auto-encoders. Taken together, our results suggest that the combination of STDP with latency coding may be a key to understanding the way that the primate visual system learns, its remarkable processing speed and its low energy consumption. These mechanisms are also interesting for artificial vision systems, particularly for hardware solutions.},
	urldate = {2023-11-27},
	journal = {Neural Networks},
	author = {Kheradpisheh, Saeed Reza and Ganjtabesh, Mohammad and Thorpe, Simon J. and Masquelier, Timothée},
	month = mar,
	year = {2018},
	keywords = {Deep learning, Object recognition, SNN, STDP, Temporal coding},
	pages = {56--67},
}

@incollection{kasabov_deep_2019,
	address = {Berlin, Heidelberg},
	series = {Springer {Series} on {Bio}- and {Neurosystems}},
	title = {Deep {Learning} and {Modelling} of {Audio}-, {Visual}-, and {Multimodal} {Audio}-{Visual} {Data} in {Brain}-{Inspired} {SNN}},
	isbn = {978-3-662-57715-8},
	url = {https://doi.org/10.1007/978-3-662-57715-8_13},
	abstract = {This chapter presents methods for audio-, visual- and for the integrated audio and visual information processing using brain-inspired SNN architectures such as NeuCube. Case studies are presented for short musical pieces recognition, fast moving object recognition, age-invariant face identification, moving digits recognition and other.},
	language = {en},
	urldate = {2023-11-27},
	booktitle = {Time-{Space}, {Spiking} {Neural} {Networks} and {Brain}-{Inspired} {Artificial} {Intelligence}},
	publisher = {Springer},
	author = {Kasabov, Nikola K.},
	editor = {Kasabov, Nikola K.},
	year = {2019},
	doi = {10.1007/978-3-662-57715-8_13},
	keywords = {Gender Recognition, Moving Object Recognition, NeuCube Architecture, NeuCube Model, Spike-timing Dependent Plasticity},
	pages = {457--477},
}

@article{manna_simple_2022,
	title = {Simple and complex spiking neurons: perspectives and analysis in a simple {STDP} scenario},
	volume = {2},
	issn = {2634-4386},
	shorttitle = {Simple and complex spiking neurons},
	url = {http://arxiv.org/abs/2207.04881},
	doi = {10.1088/2634-4386/ac999b},
	abstract = {Spiking neural networks (SNNs) are largely inspired by biology and neuroscience and leverage ideas and theories to create fast and efficient learning systems. Spiking neuron models are adopted as core processing units in neuromorphic systems because they enable event-based processing. The integrate-and-fire (I\&F) models are often adopted, with the simple Leaky I\&F (LIF) being the most used. The reason for adopting such models is their efficiency and/or biological plausibility. Nevertheless, rigorous justification for adopting LIF over other neuron models for use in artificial learning systems has not yet been studied. This work considers various neuron models in the literature and then selects computational neuron models that are single-variable, efficient, and display different types of complexities. From this selection, we make a comparative study of three simple I\&F neuron models, namely the LIF, the Quadratic I\&F (QIF) and the Exponential I\&F (EIF), to understand whether the use of more complex models increases the performance of the system and whether the choice of a neuron model can be directed by the task to be completed. Neuron models are tested within an SNN trained with Spike-Timing Dependent Plasticity (STDP) on a classification task on the N-MNIST and DVS Gestures datasets. Experimental results reveal that more complex neurons manifest the same ability as simpler ones to achieve high levels of accuracy on a simple dataset (N-MNIST), albeit requiring comparably more hyper-parameter tuning. However, when the data possess richer Spatio-temporal features, the QIF and EIF neuron models steadily achieve better results. This suggests that accurately selecting the model based on the richness of the feature spectrum of the data could improve the whole system's performance. Finally, the code implementing the spiking neurons in the SpykeTorch framework is made publicly available.},
	number = {4},
	urldate = {2023-11-27},
	journal = {Neuromorphic Computing and Engineering},
	author = {Manna, Davide Liberato and Sola, Alex Vicente and Kirkland, Paul and Bihl, Trevor and Di Caterina, Gaetano},
	month = dec,
	year = {2022},
	note = {arXiv:2207.04881 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	pages = {044009},
}

@article{rathi_stdp_2021,
	title = {{STDP} {Based} {Unsupervised} {Multimodal} {Learning} {With} {Cross}-{Modal} {Processing} in {Spiking} {Neural} {Networks}},
	volume = {5},
	issn = {2471-285X},
	url = {https://ieeexplore.ieee.org/abstract/document/8482490},
	doi = {10.1109/TETCI.2018.2872014},
	abstract = {Spiking neural networks perform reasonably well in recognition applications for single modality (e.g., images, audio, or text). In this paper, we propose a multimodal spiking neural network that combines two modalities (image and audio). The two unimodal ensembles are connected with cross-modal connections and the entire network is trained with unsupervised learning. The network receives inputs in both modalities for the same class and predicts the class label. The excitatory connections in the unimodal ensemble and the cross-modal connections are trained with power-law weight-dependent spike timing dependent plasticity learning rule. The cross-modal connections capture the correlation between neurons of different modalities. The multimodal network learns features of both modalities and improves the classification accuracy compared to unimodal topology, even when one of the modality is distorted by noise. The cross-modal connections suppress the effect of noise on classification accuracy. The well-learned cross-modal connections invoke additional spiking activity in neurons of the correct label. The cross-modal connections are only excitatory and do not inhibit the normal activity of the unimodal ensembles. We evaluated our multimodal network on images from MNIST dataset and utterances of digits from TI46 speech corpus. The multimodal network achieved a classification accuracy of 98\% on the combined MNIST and TI46 dataset.},
	number = {1},
	urldate = {2023-11-27},
	journal = {IEEE Transactions on Emerging Topics in Computational Intelligence},
	author = {Rathi, Nitin and Roy, Kaushik},
	month = feb,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Emerging Topics in Computational Intelligence},
	pages = {143--153},
}

@misc{noauthor_spiking_nodate,
	title = {{SPIKING} {NEURAL} {NETWORKS}},
	url = {https://www.worldscientific.com/doi/epdf/10.1142/S0129065709002002},
	language = {en},
	urldate = {2023-11-27},
	doi = {10.1142/S0129065709002002},
}

@article{dora_spiking_2021,
	title = {Spiking {Neural} {Networks} for {Computational} {Intelligence}: {An} {Overview}},
	volume = {5},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2504-2289},
	shorttitle = {Spiking {Neural} {Networks} for {Computational} {Intelligence}},
	url = {https://www.mdpi.com/2504-2289/5/4/67},
	doi = {10.3390/bdcc5040067},
	abstract = {Deep neural networks with rate-based neurons have exhibited tremendous progress in the last decade. However, the same level of progress has not been observed in research on spiking neural networks (SNN), despite their capability to handle temporal data, energy-efficiency and low latency. This could be because the benchmarking techniques for SNNs are based on the methods used for evaluating deep neural networks, which do not provide a clear evaluation of the capabilities of SNNs. Particularly, the benchmarking of SNN approaches with regards to energy efficiency and latency requires realization in suitable hardware, which imposes additional temporal and resource constraints upon ongoing projects. This review aims to provide an overview of the current real-world applications of SNNs and identifies steps to accelerate research involving SNNs in the future.},
	language = {en},
	number = {4},
	urldate = {2023-11-27},
	journal = {Big Data and Cognitive Computing},
	author = {Dora, Shirin and Kasabov, Nikola},
	month = dec,
	year = {2021},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {SNN, brain-inspired learning, neuromorphic computing},
	pages = {67},
}

@article{fan_brain_2020,
	title = {From {Brain} {Science} to {Artificial} {Intelligence}},
	volume = {6},
	issn = {2095-8099},
	url = {https://www.sciencedirect.com/science/article/pii/S2095809920300035},
	doi = {10.1016/j.eng.2019.11.012},
	abstract = {Reviewing the history of the development of artificial intelligence (AI) clearly reveals that brain science has resulted in breakthroughs in AI, such as deep learning. At present, although the developmental trend in AI and its applications has surpassed expectations, an insurmountable gap remains between AI and human intelligence. It is urgent to establish a bridge between brain science and AI research, including a link from brain science to AI, and a connection from knowing the brain to simulating the brain. The first steps toward this goal are to explore the secrets of brain science by studying new brain-imaging technology; to establish a dynamic connection diagram of the brain; and to integrate neuroscience experiments with theory, models, and statistics. Based on these steps, a new generation of AI theory and methods can be studied, and a subversive model and working mode from machine perception and learning to machine thinking and decision-making can be established. This article discusses the opportunities and challenges of adapting brain science to AI.},
	number = {3},
	urldate = {2023-11-27},
	journal = {Engineering},
	author = {Fan, Jingtao and Fang, Lu and Wu, Jiamin and Guo, Yuchen and Dai, Qionghai},
	month = mar,
	year = {2020},
	keywords = {Brain science},
	pages = {248--252},
}

@incollection{kozma_chapter_2019,
	title = {Chapter 10 - {Computers} {Versus} {Brains}: {Game} {Is} {Over} or {More} to {Come}?},
	isbn = {978-0-12-815480-9},
	shorttitle = {Chapter 10 - {Computers} {Versus} {Brains}},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128154809000104},
	abstract = {Creating machines that can think and act just like humans has fascinated humanity for millennia. Breakthroughs of scientific and technological developments in the past century, which demonstrate rapidly accelerating pace in recent decades, allow us now to build computers and robots that parallel or even surpass human abilities in some respects. This is the era of new Artificial Intelligence (AI) with Deep Learning (DL) exploiting very big databases and requiring massive computational resources. It is a fundamental question whether the new AI could benefit from lessons learnt from human brains and human intelligence, or machine intelligence may develop better without considering human experiences and constraints. In this chapter we analyze various aspects of biological and artificial intelligence. We introduce a balanced approach based on the concepts of complementarity and multistability as manifested in human brain operation and cognitive processing. This approach provides insights into key principles of intelligence in biological brains and helps building more powerful artificially intelligent devices.},
	urldate = {2023-11-27},
	booktitle = {Artificial {Intelligence} in the {Age} of {Neural} {Networks} and {Brain} {Computing}},
	publisher = {Academic Press},
	author = {Kozma, Robert},
	editor = {Kozma, Robert and Alippi, Cesare and Choe, Yoonsuck and Morabito, Francesco Carlo},
	month = jan,
	year = {2019},
	doi = {10.1016/B978-0-12-815480-9.00010-4},
	keywords = {Chimera State, Complementarity, Mesoscopic Dynamics, Metastability, Neurodynamics, Neuropercolation, Phase Transition},
	pages = {205--218},
}

@article{kim_visual_2021,
	title = {Visual explanations from spiking neural networks using inter-spike intervals},
	volume = {11},
	copyright = {2021 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-021-98448-0},
	doi = {10.1038/s41598-021-98448-0},
	abstract = {By emulating biological features in brain, Spiking Neural Networks (SNNs) offer an energy-efficient alternative to conventional deep learning. To make SNNs ubiquitous, a ‘visual explanation’ technique for analysing and explaining the internal spike behavior of such temporal deep SNNs is crucial. Explaining SNNs visually will make the network more transparent giving the end-user a tool to understand how SNNs make temporal predictions and why they make a certain decision. In this paper, we propose a bio-plausible visual explanation tool for SNNs, called Spike Activation Map (SAM). SAM yields a heatmap (i.e., localization map) corresponding to each time-step of input data by highlighting neurons with short inter-spike interval activity. Interestingly, without the use of gradients and ground truth, SAM produces a temporal localization map highlighting the region of interest in an image attributed to an SNN’s prediction at each time-step. Overall, SAM outsets the beginning of a new research area ‘explainable neuromorphic computing’ that will ultimately allow end-users to establish appropriate trust in predictions from SNNs.},
	language = {en},
	number = {1},
	urldate = {2023-11-27},
	journal = {Scientific Reports},
	author = {Kim, Youngeun and Panda, Priyadarshini},
	month = sep,
	year = {2021},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Computational neuroscience, Electrical and electronic engineering, Visual system},
	pages = {19037},
}

@article{tamagawa_membrane_2022,
	title = {The membrane potential arising from the adsorption of ions at the biological interface},
	volume = {73},
	issn = {2676-8607},
	url = {https://doi.org/10.1007/s42977-022-00139-y},
	doi = {10.1007/s42977-022-00139-y},
	abstract = {Membrane theory makes it possible to compute the membrane potential of living cells accurately. The principle is that the plasma membrane is selectively permeable to ions and that its permeability to mobile ions determines the characteristics of the membrane potential. However, an artificial experimental cell system with an impermeable membrane can exhibit a nonzero membrane potential, and its characteristics are consistent with the prediction of the Goldman–Hodgkin–Katz eq., which is a noteworthy concept of membrane theory, despite the membrane’s impermeability to mobile ions. We noticed this troublesome facet of the membrane theory. We measured the potentials through permeable and impermeable membranes where we used the broad varieties of membranes. Then we concluded that the membrane potential must be primarily, although not wholly, governed by the ion adsorption-desorption process rather than by the passage of ions across the cell membrane. A theory based on the Association-Induction Hypothesis seems to be a more plausible mechanism for the generation of the membrane potential and to explain this unexpected physiological fact. The Association-Induction Hypothesis states that selective ion permeability of the membrane is not a condition for the generation of the membrane potential in living cells, which contradicts the prediction of the membrane theory. Therefore, the Association-Induction Hypothesis is the actual cause of membrane potential. We continued the theoretical analysis by taking into account the Association-Induction Hypothesis and saw that its universality as a cause of potential generation mechanism. We then concluded that the interfacial charge distribution is one of the fundamental causes of the membrane potential.},
	language = {en},
	number = {4},
	urldate = {2023-11-26},
	journal = {Biologia Futura},
	author = {Tamagawa, Hirohisa and Delalande, Bernard},
	month = dec,
	year = {2022},
	keywords = {Association-Induction Hypothesis, Goldman–Hodgkin–Katz equation, Interface, Ion adsorption-desorption, Membrane potential, Membrane theory, Permeability coefficient, Surface charge density},
	pages = {455--471},
}

@article{gitler_different_2004,
	title = {Different {Presynaptic} {Roles} of {Synapsins} at {Excitatory} and {Inhibitory} {Synapses}},
	volume = {24},
	copyright = {Copyright © 2004 Society for Neuroscience 0270-6474/04/2411368-13.00/0},
	issn = {0270-6474, 1529-2401},
	url = {https://www.jneurosci.org/content/24/50/11368},
	doi = {10.1523/JNEUROSCI.3795-04.2004},
	abstract = {The functions of synapsins were examined by characterizing the phenotype of mice in which all three synapsin genes were knocked out. Although these triple knock-out mice were viable and had normal brain anatomy, they exhibited a number of behavioral defects. Synaptic transmission was altered in cultured neurons from the hippocampus of knock-out mice. At excitatory synapses, loss of synapsins did not affect basal transmission evoked by single stimuli but caused a threefold increase in the rate of synaptic depression during trains of stimuli. This suggests that synapsins regulate the reserve pool of synaptic vesicles. This possibility was examined further by measuring synaptic vesicle density in living neurons transfected with green fluorescent protein-tagged synaptobrevin 2, a marker of synaptic vesicles. The relative amount of fluorescent synaptobrevin was substantially lower at synapses of knock-out neurons than of wild-type neurons. Electron microscopy also revealed a parallel reduction in the number of vesicles in the reserve pool of vesicles {\textgreater}150 nm away from the active zone at excitatory synapses. Thus, synapsins are required for maintaining vesicles in the reserve pool at excitatory synapses. In contrast, basal transmission at inhibitory synapses was reduced by loss of synapsins, but the kinetics of synaptic depression were unaffected. In these terminals, there was a mild reduction in the total number of synaptic vesicles, but this was not restricted to the reserve pool of vesicles. Thus, synapsins maintain the reserve pool of glutamatergic vesicles but regulate the size of the readily releasable pool of GABAergic vesicles.},
	language = {en},
	number = {50},
	urldate = {2023-11-26},
	journal = {Journal of Neuroscience},
	author = {Gitler, Daniel and Takagishi, Yoshiko and Feng, Jian and Ren, Yong and Rodriguiz, Ramona M. and Wetsel, William C. and Greengard, Paul and Augustine, George J.},
	month = dec,
	year = {2004},
	pmid = {15601943},
	note = {Publisher: Society for Neuroscience
Section: Cellular/Molecular},
	keywords = {exocytosis, facilitation, presynaptic, reserve pool, synapsin, synaptic depression, synaptic transmission, synaptic vesicle},
	pages = {11368--11380},
}

@article{frank_next-generation_2019,
	title = {Next-generation interfaces for studying neural function},
	volume = {37},
	copyright = {2019 Springer Nature America, Inc.},
	issn = {1546-1696},
	url = {https://www.nature.com/articles/s41587-019-0198-8},
	doi = {10.1038/s41587-019-0198-8},
	abstract = {Monitoring and modulating the diversity of signals used by neurons and glia in a closed-loop fashion is necessary to establish causative links between biochemical processes within the nervous system and observed behaviors. As developments in neural-interface hardware strive to keep pace with rapid progress in genetically encoded and synthetic reporters and modulators of neural activity, the integration of multiple functional features becomes a key requirement and a pressing challenge in the field of neural engineering. Electrical, optical and chemical approaches have been used to manipulate and record neuronal activity in vivo, with a recent focus on technologies that both integrate multiple modes of interaction with neurons into a single device and enable bidirectional communication with neural circuits with enhanced spatiotemporal precision. These technologies not only are facilitating a greater understanding of the brain, spinal cord and peripheral circuits in the context of health and disease, but also are informing the development of future closed-loop therapies for neurological, neuro-immune and neuroendocrine conditions.},
	language = {en},
	number = {9},
	urldate = {2023-11-26},
	journal = {Nature Biotechnology},
	author = {Frank, James A. and Antonini, Marc-Joseph and Anikeeva, Polina},
	month = sep,
	year = {2019},
	note = {Number: 9
Publisher: Nature Publishing Group},
	keywords = {Biological techniques, Biomedical engineering, Neuroscience},
	pages = {1013--1023},
}

@article{guo_training_nodate,
	title = {Training {High}-performance {Spiking} {Neural} {Networks} {Through} {Reducing} {Quantization} {Error}},
	language = {en},
	author = {Guo, Yufei and Tong, Xinyi and Chen, Yuanpei and Wang, Xiashuang and Liu, Xiuhua and Zhang, Liwen},
}

@article{dewolf_spiking_2021,
	title = {Spiking neural networks take control},
	volume = {6},
	url = {https://www.science.org/doi/full/10.1126/scirobotics.abk3268},
	doi = {10.1126/scirobotics.abk3268},
	abstract = {Brain-inspired neural network architecture overcomes unsolved classical control theory problem for telerobotics.},
	number = {58},
	urldate = {2023-11-26},
	journal = {Science Robotics},
	author = {DeWolf, Travis},
	month = sep,
	year = {2021},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {eabk3268},
}

@article{yu_evolution_2021,
	title = {Evolution of {Bio}-{Inspired} {Artificial} {Synapses}: {Materials}, {Structures}, and {Mechanisms}},
	volume = {17},
	copyright = {© 2020 WILEY-VCH Verlag GmbH \& Co. KGaA, Weinheim},
	issn = {1613-6829},
	shorttitle = {Evolution of {Bio}-{Inspired} {Artificial} {Synapses}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smll.202000041},
	doi = {10.1002/smll.202000041},
	abstract = {Artificial synapses (ASs) are electronic devices emulating important functions of biological synapses, which are essential building blocks of artificial neuromorphic networks for brain-inspired computing. A human brain consists of several quadrillion synapses for information storage and processing, and massively parallel computation. Neuromorphic systems require ASs to mimic biological synaptic functions, such as paired-pulse facilitation, short-term potentiation, long-term potentiation, spatiotemporally-correlated signal processing, and spike-timing-dependent plasticity, etc. Feature size and energy consumption of ASs need to be minimized for high-density energy-efficient integration. This work reviews recent progress on ASs. First, synaptic plasticity and functional emulation are introduced, and then synaptic electronic devices for neuromorphic computing systems are discussed. Recent advances in flexible artificial synapses for artificial sensory nerves are also briefly introduced. Finally, challenges and opportunities in the field are discussed.},
	language = {en},
	number = {9},
	urldate = {2023-11-26},
	journal = {Small},
	author = {Yu, Haiyang and Wei, Huanhuan and Gong, Jiangdong and Han, Hong and Ma, Mingxue and Wang, Yongfei and Xu, Wentao},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/smll.202000041},
	keywords = {artificial synapses, energy consumption, memristors, synaptic plasticity, synaptic transistors},
	pages = {2000041},
}

@article{lynn_physics_2019,
	title = {The physics of brain network structure, function and control},
	volume = {1},
	copyright = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2522-5820},
	url = {https://www.nature.com/articles/s42254-019-0040-8},
	doi = {10.1038/s42254-019-0040-8},
	abstract = {The brain is characterized by heterogeneous patterns of structural connections supporting unparalleled feats of cognition and a wide range of behaviours. New non-invasive imaging techniques now allow comprehensive mapping of these patterns. However, a fundamental challenge remains to understand how the brain’s structural wiring supports cognitive processes, with major implications for personalized mental health treatments. Here, we review recent efforts to meet this challenge, drawing on physics intuitions, models and theories, spanning the domains of statistical mechanics, information theory, dynamical systems and control. We first describe the organizing principles of brain network architecture instantiated in structural wiring under constraints of spatial embedding and energy minimization. We then survey models of brain network function that stipulate how neural activity propagates along structural connections. Finally, we discuss perturbative experiments and models for brain network control; these use the physics of signal transmission along structural connections to infer intrinsic control processes that support goal-directed behaviour and to inform stimulation-based therapies for neurological and psychiatric disease. Throughout, we highlight open questions that invite the creative efforts of pioneering physicists.},
	language = {en},
	number = {5},
	urldate = {2023-11-26},
	journal = {Nature Reviews Physics},
	author = {Lynn, Christopher W. and Bassett, Danielle S.},
	month = may,
	year = {2019},
	note = {Number: 5
Publisher: Nature Publishing Group},
	keywords = {Complex networks, Neuroscience},
	pages = {318--332},
}

@inproceedings{bu_rate_2023,
	title = {Rate {Gradient} {Approximation} {Attack} {Threats} {Deep} {Spiking} {Neural} {Networks}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Bu_Rate_Gradient_Approximation_Attack_Threats_Deep_Spiking_Neural_Networks_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-11-26},
	author = {Bu, Tong and Ding, Jianhao and Hao, Zecheng and Yu, Zhaofei},
	year = {2023},
	pages = {7896--7906},
}

@article{ponghiran_spiking_2022,
	title = {Spiking {Neural} {Networks} with {Improved} {Inherent} {Recurrence} {Dynamics} for {Sequential} {Learning}},
	volume = {36},
	copyright = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/20771},
	doi = {10.1609/aaai.v36i7.20771},
	abstract = {Spiking neural networks (SNNs) with leaky integrate and fire (LIF) neurons, can be operated in an event-driven manner and have internal states to retain information over time, providing opportunities for energy-efficient neuromorphic computing, especially on edge devices. Note, however, many representative works on SNNs do not fully demonstrate the usefulness of their inherent recurrence (membrane potential retaining information about the past) for sequential learning. Most of the works train SNNs to recognize static images by artificially expanded input representation in time through rate coding. We show that SNNs can be trained for practical sequential tasks by proposing modifications to a network of LIF neurons that enable internal states to learn long sequences and make their inherent recurrence resilient to the vanishing gradient problem. We then develop a training scheme to train the proposed SNNs with improved inherent recurrence dynamics. Our training scheme allows spiking neurons to produce multi-bit outputs (as opposed to binary spikes) which help mitigate the mismatch between a derivative of spiking neurons' activation function and a surrogate derivative used to overcome spiking neurons' non-differentiability. Our experimental results indicate that the proposed SNN architecture on TIMIT and LibriSpeech 100h speech recognition dataset yields accuracy comparable to that of LSTMs (within 1.10\% and 0.36\%, respectively), but with 2x fewer parameters than LSTMs. The sparse SNN outputs also lead to 10.13x and 11.14x savings in multiplication operations compared to GRUs, which are generally considered as a lightweight alternative to LSTMs, on TIMIT and LibriSpeech 100h datasets, respectively.},
	language = {en},
	number = {7},
	urldate = {2023-11-26},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Ponghiran, Wachirawit and Roy, Kaushik},
	month = jun,
	year = {2022},
	note = {Number: 7},
	keywords = {Speech \& Natural Language Processing (SNLP)},
	pages = {8001--8008},
}

@article{zeng_braincog_2023,
	title = {{BrainCog}: {A} spiking neural network based, brain-inspired cognitive intelligence engine for brain-inspired {AI} and brain simulation},
	volume = {4},
	issn = {26663899},
	shorttitle = {{BrainCog}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2666389923001447},
	doi = {10.1016/j.patter.2023.100789},
	abstract = {Spiking neural networks (SNNs) serve as a promising computational framework for integrating insights from the brain into artiﬁcial intelligence (AI). Existing software infrastructures based on SNNs exclusively support brain simulation or brain-inspired AI, but not both simultaneously. To decode the nature of biological intelligence and create AI, we present the brain-inspired cognitive intelligence engine (BrainCog). This SNN-based platform provides essential infrastructure support for developing brain-inspired AI and brain simulation. BrainCog integrates different biological neurons, encoding strategies, learning rules, brain areas, and hardware-software co-design as essential components. Leveraging these user-friendly components, BrainCog incorporates various cognitive functions, including perception and learning, decision-making, knowledge representation and reasoning, motor control, social cognition, and brain structure and function simulations across multiple scales. BORN is an AI engine developed by BrainCog, showcasing seamless integration of BrainCog’s components and cognitive functions to build advanced AI models and applications.},
	language = {en},
	number = {8},
	urldate = {2023-11-21},
	journal = {Patterns},
	author = {Zeng, Yi and Zhao, Dongcheng and Zhao, Feifei and Shen, Guobin and Dong, Yiting and Lu, Enmeng and Zhang, Qian and Sun, Yinqian and Liang, Qian and Zhao, Yuxuan and Zhao, Zhuoya and Fang, Hongjian and Wang, Yuwei and Li, Yang and Liu, Xin and Du, Chengcheng and Kong, Qingqun and Ruan, Zizhe and Bi, Weida},
	month = aug,
	year = {2023},
	pages = {100789},
}

@article{fang_learning_2022,
	title = {Learning to {Learn} {Transferable} {Attack}},
	volume = {36},
	copyright = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/19936},
	doi = {10.1609/aaai.v36i1.19936},
	abstract = {Transfer adversarial attack is a non-trivial black-box adversarial attack that aims to craft adversarial perturbations on the surrogate model and then apply such perturbations to the victim model. However, the transferability of perturbations from existing methods is still limited, since the adversarial perturbations are easily overﬁtting with a single surrogate model and speciﬁc data pattern. In this paper, we propose a Learning to Learn Transferable Attack (LLTA) method, which makes the adversarial perturbations more generalized via learning from both data and model augmentation. For data augmentation, we adopt simple random resizing and padding. For model augmentation, we randomly alter the back propagation instead of the forward propagation to eliminate the effect on the model prediction. By treating the attack of both speciﬁc data and a modiﬁed model as a task, we expect the adversarial perturbations to adopt enough tasks for generalization. To this end, the meta-learning algorithm is further introduced during the iteration of perturbation generation. Empirical results on the widely-used dataset demonstrate the effectiveness of our attack method with a 12.85\% higher success rate of transfer attack compared with the state-of-the-art methods. We also evaluate our method on the real-world online system, i.e., Google Cloud Vision API, to further show the practical potentials of our method.},
	language = {en},
	number = {1},
	urldate = {2023-11-17},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Fang, Shuman and Li, Jie and Lin, Xianming and Ji, Rongrong},
	month = jun,
	year = {2022},
	note = {Number: 1},
	keywords = {Computer Vision},
	pages = {571--579},
}

@inproceedings{han_enhancing_2023,
	address = {Macau, SAR China},
	title = {Enhancing {Efficient} {Continual} {Learning} with {Dynamic} {Structure} {Development} of {Spiking} {Neural} {Networks}},
	isbn = {978-1-956792-03-4},
	url = {https://www.ijcai.org/proceedings/2023/334},
	doi = {10.24963/ijcai.2023/334},
	abstract = {Children possess the ability to learn multiple cognitive tasks sequentially, which is a major challenge toward the long-term goal of artiﬁcial general intelligence. Existing continual learning frameworks are usually applicable to Deep Neural Networks (DNNs) and lack the exploration on more braininspired, energy-efﬁcient Spiking Neural Networks (SNNs). Drawing on continual learning mechanisms during child growth and development, we propose Dynamic Structure Development of Spiking Neural Networks (DSD-SNN) for efﬁcient and adaptive continual learning. When learning a sequence of tasks, the DSD-SNN dynamically assigns and grows new neurons to new tasks and prunes redundant neurons, thereby increasing memory capacity and reducing computational overhead. In addition, the overlapping shared structure helps to quickly leverage all acquired knowledge to new tasks, empowering a single network capable of supporting multiple incremental tasks (without the separate sub-network mask for each task). We validate the effectiveness of the proposed model on multiple class incremental learning and task incremental learning benchmarks. Extensive experiments demonstrated that our model could significantly improve performance, learning speed and memory capacity, and reduce computational overhead. Besides, our DSD-SNN model achieves comparable performance with the DNNs-based methods, and signiﬁcantly outperforms the state-of-theart (SOTA) performance for existing SNNs-based continual learning methods.},
	language = {en},
	urldate = {2023-11-16},
	booktitle = {Proceedings of the {Thirty}-{Second} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Han, Bing and Zhao, Feifei and Zeng, Yi and Pan, Wenxuan and Shen, Guobin},
	month = aug,
	year = {2023},
	keywords = {��ijcai},
	pages = {2993--3001},
}

@article{zhang_mathematical_2008,
	title = {A {Mathematical} {Model} of a {Neuron} with {Synapses} based on {Physiology}},
	copyright = {2008 The Author(s)},
	issn = {1756-0357},
	url = {https://www.nature.com/articles/npre.2008.1703.1},
	doi = {10.1038/npre.2008.1703.1},
	abstract = {The neuron, when considered as a signal processing device, itsinputs are the frequency of pulses received at the synapses, and its output is the frequency of action potentials generated- in essence, a neuron is a pulse frequency signal processing device. In comparison, electrical devices use either digital or analog signals for communication or processing, and the mathematics behind these subjects is well understood. However, in regards to pulse frequency processing devices, there has not yet been a clear and persuasive mathematical model to describe the functions of neurons. It goes without saying that such a model is very important, not only for understanding neuron and neural system behavior, but also for undeveloped potential applications in industry. This paper proposes a method for obtaining the mathematical relationship between the input and output signals of a neuron based on physiological facts. The proposed method focuses on the currents across the postsynaptic membrane of each synapse, and the key is to recognize that the net charge across the whole membrane of a neuron over each action potential cycle must equal to zero. By analyzing the relationship between the input of a synapse and the currents across the postsynaptic membranes, a dynamic pulse frequency model of the neuron can be obtained. Here, we show that the transfer function of a neuron depends on the function of thepostsynaptic current of each synapse in resting state, which can be found by detecting the postsynaptic current when a pulse is received at the synapse. The transfer function of a typical neuron generally includes addition and subtraction of feedthrough terms and/or first order lag functions. To focus on the most basic characteristics of a neuron, accommodation, adaptation, learning, etc. are not discussed in this paper.},
	language = {en},
	urldate = {2023-10-11},
	journal = {Nature Precedings},
	author = {Zhang, Xiaolin},
	month = mar,
	year = {2008},
	note = {Publisher: Nature Publishing Group},
	keywords = {Life Sciences},
	pages = {1--1},
}

@article{cheadle_neuronglia_2023,
	title = {Neuron–glia communication through bona fide synapses},
	volume = {24},
	copyright = {2023 Springer Nature Limited},
	issn = {1471-0048},
	url = {https://www.nature.com/articles/s41583-023-00702-z},
	doi = {10.1038/s41583-023-00702-z},
	language = {en},
	number = {7},
	urldate = {2023-10-11},
	journal = {Nature Reviews Neuroscience},
	author = {Cheadle, Lucas},
	month = jul,
	year = {2023},
	note = {Number: 7
Publisher: Nature Publishing Group},
	keywords = {Oligodendrocyte, Synaptic transmission},
	pages = {395--395},
}

@misc{nahmias_laser_2020,
	title = {A {Laser} {Spiking} {Neuron} in a {Photonic} {Integrated} {Circuit}},
	url = {http://arxiv.org/abs/2012.08516},
	doi = {10.48550/arXiv.2012.08516},
	abstract = {There has been a recent surge of interest in the implementation of linear operations such as matrix multipications using photonic integrated circuit technology. However, these approaches require an efficient and flexible way to perform nonlinear operations in the photonic domain. We have fabricated an optoelectronic nonlinear device--a laser neuron--that uses excitable laser dynamics to achieve biologically-inspired spiking behavior. We demonstrate functionality with simultaneous excitation, inhibition, and summation across multiple wavelengths. We also demonstrate cascadability and compatibility with a wavelength multiplexing protocol, both essential for larger scale system integration. Laser neurons represent an important class of optoelectronic nonlinear processors that can complement both the enormous bandwidth density and energy efficiency of photonic computing operations.},
	urldate = {2023-11-11},
	publisher = {arXiv},
	author = {Nahmias, Mitchell A. and Peng, Hsuan-Tung and de Lima, Thomas Ferreira and Huang, Chaoran and Tait, Alexander N. and Shastri, Bhavin J. and Prucnal, Paul R.},
	month = dec,
	year = {2020},
	note = {arXiv:2012.08516 [physics]},
	keywords = {Electrical Engineering and Systems Science - Signal Processing, Physics - Applied Physics, Physics - Optics},
}

@article{garg_dynamic_2023,
	title = {Dynamic {Precision} {Analog} {Computing} for {Neural} {Networks}},
	volume = {29},
	issn = {1558-4542},
	url = {https://ieeexplore.ieee.org/abstract/document/9932877},
	doi = {10.1109/JSTQE.2022.3218019},
	abstract = {Analog electronic and optical computing exhibit tremendous advantages over digital computing for accelerating deep learning when operations are executed at low precision. Although digital architectures support programmable precision to increase efficiency, analog computing architectures today only support a single, static precision. In this work, we characterize the relationship between the effective number of bits (ENOB) of precision of analog processors, which is limited by noise, and digital bit precision for quantized neural networks. We propose extending analog computing architectures to support dynamic levels of precision by repeating operations and averaging the result, decreasing the impact of noise. To utilize dynamic precision, we propose a method for learning the precision of each layer of a pre-trained model without retraining network weights. We evaluate this method on analog architectures subject to shot noise, thermal noise, and weight noise and find that employing dynamic precision reduces energy consumption by up to 89\% for computer vision models such as Resnet50 and by 24\% for natural language processing models such as BERT. In one example, we apply dynamic precision to a shot-noise limited homodyne optical neural network and simulate inference at an optical energy consumption of 2.7 aJ/MAC for Resnet50 and 1.6 aJ/MAC for BERT with {\textless} 2\% accuracy degradation, implying that the optical energy consumption is unlikely to be the dominant cost.},
	number = {2: Optical Computing},
	urldate = {2023-11-11},
	journal = {IEEE Journal of Selected Topics in Quantum Electronics},
	author = {Garg, Sahaj and Lou, Joe and Jain, Anirudh and Guo, Zhimu and Shastri, Bhavin J. and Nahmias, Mitchell},
	month = mar,
	year = {2023},
	note = {Conference Name: IEEE Journal of Selected Topics in Quantum Electronics},
	pages = {1--12},
}

@article{peng_neuromorphic_2018,
	title = {Neuromorphic {Photonic} {Integrated} {Circuits}},
	volume = {24},
	issn = {1558-4542},
	url = {https://ieeexplore.ieee.org/abstract/document/8364605},
	doi = {10.1109/JSTQE.2018.2840448},
	abstract = {This paper reviews some recent progress in the field of neuromorphic photonics, with a particular focus on scalability. We provide a framework for understanding the underlying models, and demonstrate a neuron-like processing device—an excitable laser—that has many favorable properties for integration with emerging photonic integrated circuit platforms. On a systems level, we compare several proposed interconnection frameworks that allow for fully tunable networks of photonic neurons.},
	number = {6},
	urldate = {2023-11-11},
	journal = {IEEE Journal of Selected Topics in Quantum Electronics},
	author = {Peng, Hsuan-Tung and Nahmias, Mitchell A. and de Lima, Thomas Ferreira and Tait, Alexander N. and Shastri, Bhavin J.},
	month = nov,
	year = {2018},
	note = {Conference Name: IEEE Journal of Selected Topics in Quantum Electronics},
	pages = {1--15},
}

@misc{noauthor_long-term_nodate-1,
	title = {Long-term potentiation of electrotonic coupling at mixed synapses {\textbar} {Nature}},
	url = {https://www.nature.com/articles/348542a0},
	urldate = {2023-11-10},
}

@article{__2016,
	title = {对类脑智能研究的几点看法},
	volume = {46},
	url = {https://www.cqvip.com/qk/98490x/201602/667901592.html},
	abstract = {人工神经网络的研究在经历了＂感知机＂（perceptron）、＂Hopfield模型＂、＂Back Propagation（BP）算法＂等几轮大浪潮之后,近年深度学习算法（deep learning）开始进入实用阶段,例如,在语音识别、图像识别、大数据分...},
	number = {2},
	urldate = {2023-11-07},
	journal = {中国科学：生命科学},
	author = {张晓林},
	year = {2016},
	pages = {220--222},
}

@article{akinola_three-terminal_2019,
	title = {Three-terminal magnetic tunnel junction synapse circuits showing spike-timing-dependent plasticity},
	volume = {52},
	issn = {0022-3727},
	url = {https://dx.doi.org/10.1088/1361-6463/ab4157},
	doi = {10.1088/1361-6463/ab4157},
	abstract = {There have been recent efforts towards the development of biologically-inspired neuromorphic devices and architecture. Here, we show a synapse circuit that is designed to perform spike-timing-dependent plasticity which works with the leaky, integrate, and fire neuron in a neuromorphic computing architecture. The circuit consists of a three-terminal magnetic tunnel junction with a mobile domain wall between two low-pass filters and has been modeled in SPICE. The results show that the current flowing through the synapse is highly correlated to the timing delay between the pre-synaptic and post-synaptic neurons. Using micromagnetic simulations, we show that introducing notches along the length of the domain wall track pins the domain wall at each successive notch to properly respond to the timing between the input and output current pulses of the circuit, producing a multi-state resistance representing synaptic weights. We show in SPICE that a notch-free ideal magnetic device also shows spike-timing dependent plasticity in response to the circuit current. This work is key progress towards making more bio-realistic artificial synapses with multiple weights, which can be trained online with a promise of CMOS compatibility and energy efficiency.},
	language = {en},
	number = {49},
	urldate = {2023-10-12},
	journal = {Journal of Physics D: Applied Physics},
	author = {Akinola, Otitoaleke and Hu, Xuan and Bennett, Christopher H. and Marinella, Matthew and Friedman, Joseph S. and Incorvia, Jean Anne C.},
	month = sep,
	year = {2019},
	note = {Publisher: IOP Publishing},
	pages = {49LT01},
}

@misc{luo_brain_2023,
	title = {Brain {Diffusion} for {Visual} {Exploration}: {Cortical} {Discovery} using {Large} {Scale} {Generative} {Models}},
	shorttitle = {Brain {Diffusion} for {Visual} {Exploration}},
	url = {http://arxiv.org/abs/2306.03089},
	doi = {10.48550/arXiv.2306.03089},
	abstract = {A long standing goal in neuroscience has been to elucidate the functional organization of the brain. Within higher visual cortex, functional accounts have remained relatively coarse, focusing on regions of interest (ROIs) and taking the form of selectivity for broad categories such as faces, places, bodies, food, or words. Because the identification of such ROIs has typically relied on manually assembled stimulus sets consisting of isolated objects in non-ecological contexts, exploring functional organization without robust a priori hypotheses has been challenging. To overcome these limitations, we introduce a data-driven approach in which we synthesize images predicted to activate a given brain region using paired natural images and fMRI recordings, bypassing the need for category-specific stimuli. Our approach -- Brain Diffusion for Visual Exploration ("BrainDiVE") -- builds on recent generative methods by combining large-scale diffusion models with brain-guided image synthesis. Validating our method, we demonstrate the ability to synthesize preferred images with appropriate semantic specificity for well-characterized category-selective ROIs. We then show that BrainDiVE can characterize differences between ROIs selective for the same high-level category. Finally we identify novel functional subdivisions within these ROIs, validated with behavioral data. These results advance our understanding of the fine-grained functional organization of human visual cortex, and provide well-specified constraints for further examination of cortical organization using hypothesis-driven methods.},
	urldate = {2023-10-08},
	publisher = {arXiv},
	author = {Luo, Andrew F. and Henderson, Margaret M. and Wehbe, Leila and Tarr, Michael J.},
	month = jun,
	year = {2023},
	note = {arXiv:2306.03089 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{zhou_learning_2022,
	title = {Learning to {Prompt} for {Vision}-{Language} {Models}},
	volume = {130},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-022-01653-1},
	doi = {10.1007/s11263-022-01653-1},
	abstract = {Large pre-trained vision-language models like CLIP have shown great potential in learning representations that are transferable across a wide range of downstream tasks. Different from the traditional representation learning that is based mostly on discretized labels, vision-language pre-training aligns images and texts in a common feature space, which allows zero-shot transfer to a downstream task via prompting, i.e., classification weights are synthesized from natural language describing classes of interest. In this work, we show that a major challenge for deploying such models in practice is prompt engineering, which requires domain expertise and is extremely time-consuming—one needs to spend a significant amount of time on words tuning since a slight change in wording could have a huge impact on performance. Inspired by recent advances in prompt learning research in natural language processing (NLP), we propose Context Optimization (CoOp), a simple approach specifically for adapting CLIP-like vision-language models for downstream image recognition. Concretely, CoOp models a prompt’s context words with learnable vectors while the entire pre-trained parameters are kept fixed. To handle different image recognition tasks, we provide two implementations of CoOp: unified context and class-specific context. Through extensive experiments on 11 datasets, we demonstrate that CoOp requires as few as one or two shots to beat hand-crafted prompts with a decent margin and is able to gain significant improvements over prompt engineering with more shots, e.g., with 16 shots the average gain is around 15\% (with the highest reaching over 45\%). Despite being a learning-based approach, CoOp achieves superb domain generalization performance compared with the zero-shot model using hand-crafted prompts.},
	language = {en},
	number = {9},
	urldate = {2023-04-29},
	journal = {International Journal of Computer Vision},
	author = {Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei},
	month = sep,
	year = {2022},
	pages = {2337--2348},
}

@article{lillicrap_backpropagation_2020,
	title = {Backpropagation and the brain},
	volume = {21},
	copyright = {2020 Springer Nature Limited},
	issn = {1471-0048},
	url = {https://www.nature.com/articles/s41583-020-0277-3},
	doi = {10.1038/s41583-020-0277-3},
	abstract = {During learning, the brain modifies synapses to improve behaviour. In the cortex, synapses are embedded within multilayered networks, making it difficult to determine the effect of an individual synaptic modification on the behaviour of the system. The backpropagation algorithm solves this problem in deep artificial neural networks, but historically it has been viewed as biologically problematic. Nonetheless, recent developments in neuroscience and the successes of artificial neural networks have reinvigorated interest in whether backpropagation offers insights for understanding learning in the cortex. The backpropagation algorithm learns quickly by computing synaptic updates using feedback connections to deliver error signals. Although feedback connections are ubiquitous in the cortex, it is difficult to see how they could deliver the error signals required by strict formulations of backpropagation. Here we build on past and recent developments to argue that feedback connections may instead induce neural activities whose differences can be used to locally approximate these signals and hence drive effective learning in deep networks in the brain.},
	language = {en},
	number = {6},
	urldate = {2023-08-28},
	journal = {Nature Reviews Neuroscience},
	author = {Lillicrap, Timothy P. and Santoro, Adam and Marris, Luke and Akerman, Colin J. and Hinton, Geoffrey},
	month = jun,
	year = {2020},
	note = {Number: 6
Publisher: Nature Publishing Group},
	keywords = {Cortex, Learning algorithms, Long-term potentiation, Network models, Neurophysiology},
	pages = {335--346},
}

@misc{bengio_estimating_2013,
	title = {Estimating or {Propagating} {Gradients} {Through} {Stochastic} {Neurons} for {Conditional} {Computation}},
	url = {https://arxiv.org/abs/1308.3432v1},
	abstract = {Stochastic neurons and hard non-linearities can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic or non-smooth neurons? I.e., can we "back-propagate" through these stochastic neurons? We examine this question, existing approaches, and compare four families of solutions, applicable in different settings. One of them is the minimum variance unbiased gradient estimator for stochatic binary neurons (a special case of the REINFORCE algorithm). A second approach, introduced here, decomposes the operation of a binary stochastic neuron into a stochastic binary part and a smooth differentiable part, which approximates the expected effect of the pure stochatic binary neuron to first order. A third approach involves the injection of additive or multiplicative noise in a computational graph that is otherwise differentiable. A fourth approach heuristically copies the gradient with respect to the stochastic output directly as an estimator of the gradient with respect to the sigmoid argument (we call this the straight-through estimator). To explore a context where these estimators are useful, we consider a small-scale version of \{{\textbackslash}em conditional computation\}, where sparse stochastic units form a distributed representation of gaters that can turn off in combinatorially many ways large chunks of the computation performed in the rest of the neural network. In this case, it is important that the gating units produce an actual 0 most of the time. The resulting sparsity can be potentially be exploited to greatly reduce the computational cost of large deep networks for which conditional computation would be useful.},
	language = {en},
	urldate = {2023-08-23},
	journal = {arXiv.org},
	author = {Bengio, Yoshua and Léonard, Nicholas and Courville, Aaron},
	month = aug,
	year = {2013},
}

@misc{wang_improving_2023,
	title = {Improving {Zero}-{Shot} {Generalization} for {CLIP} with {Synthesized} {Prompts}},
	url = {http://arxiv.org/abs/2307.07397},
	abstract = {With the growing interest in pretrained vision-language models like CLIP, recent research has focused on adapting these models to downstream tasks. Despite achieving promising results, most existing methods require labeled data for all classes, which may not hold in real-world applications due to the long tail and Zipf's law. For example, some classes may lack labeled data entirely, such as emerging concepts. To address this problem, we propose a plug-and-play generative approach called {\textbackslash}textbf\{S\}ynt{\textbackslash}textbf\{H\}es{\textbackslash}textbf\{I\}zed {\textbackslash}textbf\{P\}rompts{\textasciitilde}({\textbackslash}textbf\{SHIP\}) to improve existing fine-tuning methods. Specifically, we follow variational autoencoders to introduce a generator that reconstructs the visual features by inputting the synthesized prompts and the corresponding class names to the textual encoder of CLIP. In this manner, we easily obtain the synthesized features for the remaining label-only classes. Thereafter, we fine-tune CLIP with off-the-shelf methods by combining labeled and synthesized features. Extensive experiments on base-to-new generalization, cross-dataset transfer learning, and generalized zero-shot learning demonstrate the superiority of our approach. The code is available at {\textbackslash}url\{https://github.com/mrflogs/SHIP\}.},
	urldate = {2023-07-18},
	publisher = {arXiv},
	author = {Wang, Zhengbo and Liang, Jian and He, Ran and Xu, Nan and Wang, Zilei and Tan, Tieniu},
	month = jul,
	year = {2023},
	note = {arXiv:2307.07397 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{lin_multimodality_nodate,
	title = {Multimodality {Helps} {Unimodality}: {Cross}-{Modal} {Few}-{Shot} {Learning} {With} {Multimodal} {Models}},
	abstract = {The ability to quickly learn a new task with minimal instruction – known as few-shot learning – is a central aspect of intelligent agents. Classical few-shot benchmarks make use of few-shot samples from a single modality, but such samples may not be sufﬁcient to characterize an entire concept class. In contrast, humans use cross-modal information to learn new concepts efﬁciently. In this work, we demonstrate that one can indeed build a better visual dog classiﬁer by reading about dogs and listening to them bark. To do so, we exploit the fact that recent multimodal foundation models such as CLIP are inherently cross-modal, mapping different modalities to the same representation space. Speciﬁcally, we propose a simple cross-modal adaptation approach that learns from few-shot examples spanning different modalities. By repurposing class names as additional one-shot training samples, we achieve SOTA results with an embarrassingly simple linear classiﬁer for vision-language adaptation. Furthermore, we show that our approach can beneﬁt existing methods such as preﬁx tuning, adapters, and classiﬁer ensembling. Finally, to explore other modalities beyond vision and language, we construct the ﬁrst (to our knowledge) audiovisual few-shot benchmark and use crossmodal training to improve the performance of both image and audio classiﬁcation. Project site at link.},
	language = {en},
	author = {Lin, Zhiqiu and Yu, Samuel and Kuang, Zhiyi and Pathak, Deepak and Ramanan, Deva},
}

@inproceedings{wortsman_robust_2022,
	title = {Robust {Fine}-{Tuning} of {Zero}-{Shot} {Models}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Wortsman_Robust_Fine-Tuning_of_Zero-Shot_Models_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-06-01},
	author = {Wortsman, Mitchell and Ilharco, Gabriel and Kim, Jong Wook and Li, Mike and Kornblith, Simon and Roelofs, Rebecca and Lopes, Raphael Gontijo and Hajishirzi, Hannaneh and Farhadi, Ali and Namkoong, Hongseok and Schmidt, Ludwig},
	year = {2022},
	pages = {7959--7971},
}

@misc{chen_adaptformer_2022,
	title = {{AdaptFormer}: {Adapting} {Vision} {Transformers} for {Scalable} {Visual} {Recognition}},
	shorttitle = {{AdaptFormer}},
	url = {http://arxiv.org/abs/2205.13535},
	doi = {10.48550/arXiv.2205.13535},
	abstract = {Pretraining Vision Transformers (ViTs) has achieved great success in visual recognition. A following scenario is to adapt a ViT to various image and video recognition tasks. The adaptation is challenging because of heavy computation and memory storage. Each model needs an independent and complete finetuning process to adapt to different tasks, which limits its transferability to different visual domains. To address this challenge, we propose an effective adaptation approach for Transformer, namely AdaptFormer, which can adapt the pre-trained ViTs into many different image and video tasks efficiently. It possesses several benefits more appealing than prior arts. Firstly, AdaptFormer introduces lightweight modules that only add less than 2\% extra parameters to a ViT, while it is able to increase the ViT's transferability without updating its original pre-trained parameters, significantly outperforming the existing 100{\textbackslash}\% fully fine-tuned models on action recognition benchmarks. Secondly, it can be plug-and-play in different Transformers and scalable to many visual tasks. Thirdly, extensive experiments on five image and video datasets show that AdaptFormer largely improves ViTs in the target domains. For example, when updating just 1.5\% extra parameters, it achieves about 10\% and 19\% relative improvement compared to the fully fine-tuned models on Something-Something{\textasciitilde}v2 and HMDB51, respectively. Code is available at https://github.com/ShoufaChen/AdaptFormer.},
	urldate = {2023-04-25},
	publisher = {arXiv},
	author = {Chen, Shoufa and Ge, Chongjian and Tong, Zhan and Wang, Jiangliu and Song, Yibing and Wang, Jue and Luo, Ping},
	month = oct,
	year = {2022},
	note = {arXiv:2205.13535 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{radford_learning_2021,
	title = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
	url = {https://proceedings.mlr.press/v139/radford21a.html},
	abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.},
	language = {en},
	urldate = {2023-06-19},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {8748--8763},
}

@article{feng_semi-supervised_2022,
	title = {Semi-supervised meta-learning networks with squeeze-and-excitation attention for few-shot fault diagnosis},
	volume = {120},
	issn = {0019-0578},
	url = {https://www.sciencedirect.com/science/article/pii/S0019057821001543},
	doi = {10.1016/j.isatra.2021.03.013},
	abstract = {In the engineering practice, lacking of data especially labeled data typically hinders the wide application of deep learning in mechanical fault diagnosis. However, collecting and labeling data is often expensive and time-consuming. To address this problem, a kind of semi-supervised meta-learning networks (SSMN) with squeeze-and-excitation attention is proposed for few-shot fault diagnosis in this paper. SSMN consists of a parameterized encoder, a non-parameterized prototype refinement process and a distance function. Based on attention mechanism, the encoder is able to extract distinct features to generate prototypes and enhance the identification accuracy. With semi-supervised few-shot learning, SSMN utilizes unlabeled data to refine original prototypes for better fault recognition. A combinatorial learning optimizer is designed to optimize SSMN efficiently. The effectiveness of the proposed method is demonstrated through three bearing vibration datasets and the results indicate the outstanding adaptability in different situations. Comparison with other approaches is also made under the same setup and the experimental results prove the superiority of the proposed method for few-shot fault diagnosis.},
	language = {en},
	urldate = {2023-02-23},
	journal = {ISA Transactions},
	author = {Feng, Yong and Chen, Jinglong and Zhang, Tianci and He, Shuilong and Xu, Enyong and Zhou, Zitong},
	month = jan,
	year = {2022},
	keywords = {CoMatch, Fault diagnosis, Few-shot classification, Meta-learning, Semi-supervised learning, ��Attention},
	pages = {383--401},
}

@misc{sehwag_ssd_2021,
	title = {{SSD}: {A} {Unified} {Framework} for {Self}-{Supervised} {Outlier} {Detection}},
	shorttitle = {{SSD}},
	url = {http://arxiv.org/abs/2103.12051},
	abstract = {We ask the following question: what training information is required to design an effective outlier/out-of-distribution (OOD) detector, i.e., detecting samples that lie far away from the training distribution? Since unlabeled data is easily accessible for many applications, the most compelling approach is to develop detectors based on only unlabeled in-distribution data. However, we observe that most existing detectors based on unlabeled data perform poorly, often equivalent to a random prediction. In contrast, existing state-of-the-art OOD detectors achieve impressive performance but require access to fine-grained data labels for supervised training. We propose SSD, an outlier detector based on only unlabeled in-distribution data. We use self-supervised representation learning followed by a Mahalanobis distance based detection in the feature space. We demonstrate that SSD outperforms most existing detectors based on unlabeled data by a large margin. Additionally, SSD even achieves performance on par, and sometimes even better, with supervised training based detectors. Finally, we expand our detection framework with two key extensions. First, we formulate few-shot OOD detection, in which the detector has access to only one to five samples from each class of the targeted OOD dataset. Second, we extend our framework to incorporate training data labels, if available. We find that our novel detection framework based on SSD displays enhanced performance with these extensions, and achieves state-of-the-art performance. Our code is publicly available at https://github.com/inspire-group/SSD.},
	urldate = {2023-02-23},
	publisher = {arXiv},
	author = {Sehwag, Vikash and Chiang, Mung and Mittal, Prateek},
	month = mar,
	year = {2021},
	note = {arXiv:2103.12051 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{yang_generalized_2022,
	title = {Generalized {Out}-of-{Distribution} {Detection}: {A} {Survey}},
	shorttitle = {Generalized {Out}-of-{Distribution} {Detection}},
	url = {http://arxiv.org/abs/2110.11334},
	abstract = {Out-of-distribution (OOD) detection is critical to ensuring the reliability and safety of machine learning systems. For instance, in autonomous driving, we would like the driving system to issue an alert and hand over the control to humans when it detects unusual scenes or objects that it has never seen during training time and cannot make a safe decision. The term, OOD detection, first emerged in 2017 and since then has received increasing attention from the research community, leading to a plethora of methods developed, ranging from classification-based to density-based to distance-based ones. Meanwhile, several other problems, including anomaly detection (AD), novelty detection (ND), open set recognition (OSR), and outlier detection (OD), are closely related to OOD detection in terms of motivation and methodology. Despite common goals, these topics develop in isolation, and their subtle differences in definition and problem setting often confuse readers and practitioners. In this survey, we first present a unified framework called generalized OOD detection, which encompasses the five aforementioned problems, i.e., AD, ND, OSR, OOD detection, and OD. Under our framework, these five problems can be seen as special cases or sub-tasks, and are easier to distinguish. We then review each of these five areas by summarizing their recent technical developments, with a special focus on OOD detection methodologies. We conclude this survey with open challenges and potential research directions.},
	urldate = {2023-02-23},
	publisher = {arXiv},
	author = {Yang, Jingkang and Zhou, Kaiyang and Li, Yixuan and Liu, Ziwei},
	month = aug,
	year = {2022},
	note = {arXiv:2110.11334 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{yang_class-aware_2022,
	address = {New Orleans, LA, USA},
	title = {Class-{Aware} {Contrastive} {Semi}-{Supervised} {Learning}},
	isbn = {978-1-66546-946-3},
	url = {https://ieeexplore.ieee.org/document/9880146/},
	doi = {10.1109/CVPR52688.2022.01402},
	abstract = {Pseudo-label-based semi-supervised learning (SSL) has achieved great success on raw data utilization. However, its training procedure suffers from confirmation bias due to the noise contained in self-generated artificial labels. Moreover, the model’s judgment becomes noisier in real-world applications with extensive out-of-distribution data. To address this issue, we propose a general method named Class-aware Contrastive Semi-Supervised Learning (CCSSL), which is a drop-in helper to improve the pseudo-label quality and enhance the model’s robustness in the real-world setting. Rather than treating real-world data as a union set, our method separately handles reliable in-distribution data with class-wise clustering for blending into downstream tasks and noisy out-of-distribution data with image-wise contrastive for better generalization. Furthermore, by applying target re-weighting, we successfully emphasize clean label learning and simultaneously reduce noisy label learning. Despite its simplicity, our proposed CCSSL has significant performance improvements over the state-of-the-art SSL methods on the standard datasets CIFAR100 [18] and STL10 [8]. On the real-world dataset Semi-iNat 2021 [27], we improve FixMatch [25] by 9.80\% and CoMatch [19] by 3.18\%. Code is available https://github.com/TencentYoutuResearch/ClassificationSemiCLS.},
	language = {en},
	urldate = {2023-02-23},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Yang, Fan and Wu, Kai and Zhang, Shuyi and Jiang, Guannan and Liu, Yong and Zheng, Feng and Zhang, Wei and Wang, Chengjie and Zeng, Long},
	month = jun,
	year = {2022},
	pages = {14401--14410},
}

@inproceedings{sohn_fixmatch_2020,
	title = {{FixMatch}: {Simplifying} {Semi}-{Supervised} {Learning} with {Consistency} and {Confidence}},
	volume = {33},
	shorttitle = {{FixMatch}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/06964dce9addb1c5cb5d6e3d9838f733-Abstract.html},
	abstract = {Semi-supervised learning (SSL) provides an effective means of leveraging unlabeled data to improve a model’s performance. This domain has seen fast progress recently, at the cost of requiring more complex methods. In this paper we propose FixMatch, an algorithm that is a significant simplification of existing SSL methods. FixMatch first generates pseudo-labels using the model’s predictions on weakly-augmented unlabeled images. For a given image, the pseudo-label is only retained if the model produces a high-confidence prediction. The model is then trained to predict the pseudo-label when fed a strongly-augmented version of the same image. Despite its simplicity, we show that FixMatch achieves state-of-the-art performance across a variety of standard semi-supervised learning benchmarks, including 94.93\% accuracy on CIFAR-10 with 250 labels and 88.61\% accuracy with 40 – just 4 labels per class. We carry out an extensive ablation study to tease apart the experimental factors that are most important to FixMatch’s success. The code is available at https://github.com/google-research/fixmatch.},
	urldate = {2023-02-23},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Sohn, Kihyuk and Berthelot, David and Carlini, Nicholas and Zhang, Zizhao and Zhang, Han and Raffel, Colin A and Cubuk, Ekin Dogus and Kurakin, Alexey and Li, Chun-Liang},
	year = {2020},
	pages = {596--608},
}

@article{zhang_semi-supervised_2022,
	title = {Semi-supervised {Contrastive} {Learning} with {Similarity} {Co}-calibration},
	issn = {1941-0077},
	doi = {10.1109/TMM.2022.3158069},
	abstract = {Semi-supervised learning acts as an effective way to leverage massive unlabeled data. In this paper, we propose a novel training strategy, termed as Semi-supervised Contrastive Learning (SsCL), which combines the well-known contrastive loss in self-supervised learning with the cross entropy loss in semi-supervised learning, and jointly optimizes the two objectives in an end-to-end way. The highlight is that different from self-training based semi-supervised learning that conducts prediction and retraining over the same model weights, SsCL interchanges the predictions over the unlabeled data between the two branches, and thus formulates a co-calibration procedure, which we find is beneficial for better prediction and avoid being trapped in local minimum. Towards this goal, the contrastive loss branch models pairwise similarities among samples, using the pseudo labels generated from the cross entropy branch, and in turn calibrates the prediction distribution of the cross entropy branch with the contrastive similarity. We show that SsCL produces more discriminative representation and is beneficial to semi-supervised learning. Notably, on ImageNet with ResNet50 as the backbone, SsCL achieves 60.2\% and 72.1\% top-1 accuracy with 1\% and 10\% labeled samples, respectively, which significantly outperforms the baseline, and is better than previous semi-supervised and self-supervised methods.},
	journal = {IEEE Transactions on Multimedia},
	author = {Zhang, Yuhang and Zhang, Xiaopeng and Li, Jie and Qiu, Robert and Xu, Haohang and Tian, Qi},
	year = {2022},
	note = {Conference Name: IEEE Transactions on Multimedia},
	keywords = {Contrastive learning, Data models, Entropy, Labeling, Predictive models, Prototypes, Semi-supervised learning, Semisupervised learning, Similarity co-calibration, Training},
	pages = {1--1},
}

@inproceedings{tack_csi_2020,
	title = {{CSI}: {Novelty} {Detection} via {Contrastive} {Learning} on {Distributionally} {Shifted} {Instances}},
	volume = {33},
	shorttitle = {{CSI}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/8965f76632d7672e7d3cf29c87ecaa0c-Abstract.html},
	abstract = {Novelty detection, i.e., identifying whether a given sample is drawn from outside the training distribution, is essential for reliable machine learning. To this end, there have been many attempts at learning a representation well-suited for novelty detection and designing a score based on such representation. In this paper, we propose a simple, yet effective method named contrasting shifted instances (CSI), inspired by the recent success on contrastive learning of visual representations. Specifically, in addition to contrasting a given sample with other instances as in conventional contrastive learning methods, our training scheme contrasts the sample with distributionally-shifted augmentations of itself. Based on this, we propose a new detection score that is specific to the proposed training scheme. Our experiments demonstrate the superiority of our method under various novelty detection scenarios, including unlabeled one-class, unlabeled multi-class and labeled multi-class settings, with various image benchmark datasets. Code and pre-trained models are available at https://github.com/alinlab/CSI.},
	urldate = {2023-02-23},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Tack, Jihoon and Mo, Sangwoo and Jeong, Jongheon and Shin, Jinwoo},
	year = {2020},
	keywords = {github},
	pages = {11839--11852},
}

@article{peng_open-set_2022,
	title = {Open-{Set} {Fault} {Diagnosis} via {Supervised} {Contrastive} {Learning} with {Negative} {Out}-of-{Distribution} {Data} {Augmentation}},
	issn = {1941-0050},
	doi = {10.1109/TII.2022.3149935},
	abstract = {Fault diagnosis in an open world refers to the diagnosis tasks that need to cope with previously unknown faults in the online stage. It faces a great challenge yet to be addressed - that is, the online data of unknown faults may be classified as normal samples with a high probability. In this paper, we develop an effective solution for this challenge by using supervised contrastive learning to learn a discriminative and compact embedding for the known normal situation and fault situations. Specifically, in addition to contrasting a given sample with other instances as is the case in conventional contrastive learning methods, our training scheme contrasts the normal samples with negative augmentations of themselves. The negative out-of-distribution data is generated by the Soft Brownnian Offset sampling method to simulate the previously unknown faults. Computational experiments are conducted on the Tennessee Eastman Process benchmark dataset and a practical plasma etching process dataset. The proposed method achieves significant improvement compared with four existing methods under three open-set fault diagnosis circumstances, i.e., balanced open-set fault diagnosis, imbalanced fault diagnosis, few-shot fault diagnosis. This demonstrates its great potentials in real world fault diagnosis applications.},
	journal = {IEEE Transactions on Industrial Informatics},
	author = {Peng, Peng and Lu, Jiaxun and Xie, Tingyu and Tao, Shuting and Wang, Hongwei and Zhang, Heming},
	year = {2022},
	note = {Conference Name: IEEE Transactions on Industrial Informatics},
	keywords = {Fault diagnosis, Informatics, Learning systems, Representation learning, Task analysis, Training, Transforms},
	pages = {1--1},
}

@misc{graham_denoising_2022,
	title = {Denoising {Diffusion} {Models} for {Out}-of-{Distribution} {Detection}},
	url = {http://arxiv.org/abs/2211.07740},
	abstract = {Out-of-distribution detection is crucial to the safe deployment of machine learning systems. Currently, the state-of-the-art in unsupervised out-of-distribution detection is dominated by generative-based approaches that make use of estimates of the likelihood or other measurements from a generative model. Reconstruction-based methods offer an alternative approach, in which a measure of reconstruction error is used to determine if a sample is out-of-distribution. However, reconstruction-based approaches are less favoured, as they require careful tuning of the model's information bottleneck - such as the size of the latent dimension - to produce good results. In this work, we exploit the view of denoising diffusion probabilistic models (DDPM) as denoising autoencoders where the bottleneck is controlled externally, by means of the amount of noise applied. We propose to use DDPMs to reconstruct an input that has been noised to a range of noise levels, and use the resulting multi-dimensional reconstruction error to classify out-of-distribution inputs. Our approach outperforms not only reconstruction-based methods, but also state-of-the-art generative-based approaches.},
	urldate = {2023-02-23},
	publisher = {arXiv},
	author = {Graham, Mark S. and Pinaya, Walter H. L. and Tudosiu, Petru-Daniel and Nachev, Parashkev and Ourselin, Sebastien and Cardoso, M. Jorge},
	month = nov,
	year = {2022},
	note = {arXiv:2211.07740 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{ming_cider_2022,
	title = {{CIDER}: {Exploiting} {Hyperspherical} {Embeddings} for {Out}-of-{Distribution} {Detection}},
	shorttitle = {{CIDER}},
	url = {http://arxiv.org/abs/2203.04450},
	abstract = {Out-of-distribution (OOD) detection is a critical task for reliable machine learning. Recent advances in representation learning give rise to developments in distance-based OOD detection, where testing samples are detected as OOD if they are relatively far away from the centroids or prototypes of in-distribution (ID) classes. However, prior methods directly take off-the-shelf loss functions that suffice for classifying ID samples, but are not optimally designed for OOD detection. In this paper, we propose CIDER, a simple and effective representation learning framework by exploiting hyperspherical embeddings for OOD detection. CIDER jointly optimizes two losses to promote strong ID-OOD separability: (1) a dispersion loss that promotes large angular distances among different class prototypes, and (2) a compactness loss that encourages samples to be close to their class prototypes. We show that CIDER is effective under various settings and establishes state-of-the-art performance. On a hard OOD detection task CIFAR-100 vs. CIFAR-10, our method substantially improves the AUROC by 14.20\% compared to the embeddings learned by the cross-entropy loss.},
	urldate = {2023-02-23},
	publisher = {arXiv},
	author = {Ming, Yifei and Sun, Yiyou and Dia, Ousmane and Li, Yixuan},
	month = mar,
	year = {2022},
	note = {arXiv:2203.04450 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{sun_out--distribution_2022,
	title = {Out-of-{Distribution} {Detection} with {Deep} {Nearest} {Neighbors}},
	url = {https://proceedings.mlr.press/v162/sun22d.html},
	abstract = {Out-of-distribution (OOD) detection is a critical task for deploying machine learning models in the open world. Distance-based methods have demonstrated promise, where testing samples are detected as OOD if they are relatively far away from in-distribution (ID) data. However, prior methods impose a strong distributional assumption of the underlying feature space, which may not always hold. In this paper, we explore the efficacy of non-parametric nearest-neighbor distance for OOD detection, which has been largely overlooked in the literature. Unlike prior works, our method does not impose any distributional assumption, hence providing stronger flexibility and generality. We demonstrate the effectiveness of nearest-neighbor-based OOD detection on several benchmarks and establish superior performance. Under the same model trained on ImageNet-1k, our method substantially reduces the false positive rate (FPR@TPR95) by 24.77\% compared to a strong baseline SSD+, which uses a parametric approach Mahalanobis distance in detection. Code is available: https://github.com/deeplearning-wisc/knn-ood.},
	language = {en},
	urldate = {2023-02-23},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Sun, Yiyou and Ming, Yifei and Zhu, Xiaojin and Li, Yixuan},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {20827--20840},
}

@inproceedings{zaeemzadeh_out--distribution_2021,
	address = {Nashville, TN, USA},
	title = {Out-of-{Distribution} {Detection} {Using} {Union} of 1-{Dimensional} {Subspaces}},
	isbn = {978-1-66544-509-2},
	url = {https://ieeexplore.ieee.org/document/9577372/},
	doi = {10.1109/CVPR46437.2021.00933},
	abstract = {The goal of out-of-distribution (OOD) detection is to handle the situations where the test samples are drawn from a different distribution than the training data. In this paper, we argue that OOD samples can be detected more easily if the training data is embedded into a low-dimensional space, such that the embedded training samples lie on a union of 1-dimensional subspaces. We show that such embedding of the in-distribution (ID) samples provides us with two main advantages. First, due to compact representation in the feature space, OOD samples are less likely to occupy the same region as the known classes. Second, the ﬁrst singular vector of ID samples belonging to a 1-dimensional subspace can be used as their robust representative. Motivated by these observations, we train a deep neural network such that the ID samples are embedded onto a union of 1-dimensional subspaces. At the test time, employing sampling techniques used for approximate Bayesian inference in deep learning, input samples are detected as OOD if they occupy the region corresponding to the ID samples with probability 0. Spectral components of the ID samples are used as robust representative of this region. Our method does not have any hyperparameter to be tuned using extra information and it can be applied on different modalities with minimal change. The effectiveness of the proposed method is demonstrated on different benchmark datasets, both in the image and video classiﬁcation domains.},
	language = {en},
	urldate = {2023-02-23},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Zaeemzadeh, Alireza and Bisagno, Niccolo and Sambugaro, Zeno and Conci, Nicola and Rahnavard, Nazanin and Shah, Mubarak},
	month = jun,
	year = {2021},
	pages = {9447--9456},
}

@article{villa-perez_semi-supervised_2021,
	title = {Semi-supervised anomaly detection algorithms: {A} comparative summary and future research directions},
	volume = {218},
	issn = {0950-7051},
	shorttitle = {Semi-supervised anomaly detection algorithms},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121001416},
	doi = {10.1016/j.knosys.2021.106878},
	abstract = {While anomaly detection is relatively well-studied, it remains a topic of ongoing interest and challenge, as our society becomes increasingly interconnected and digitalized. In this paper, we focus on existing anomaly detection approaches, by empirically studying the performance of 29 semi-supervised anomaly detection algorithms on 95 benchmark imbalanced databases from the KEEL repository. These include well-established and commonly used classifiers (e.g., One-Class Support Vector Machine (ocSVM) and Isolation Forest) and recent proposals (e.g., BRM and XGBOD). Findings from our in-depth empirical study show that BRM is a robust classifier, in terms of achieving better classification results than the other 28 state-of-the-art techniques on diverse anomaly detection problems. We also observe that OCKRA, Isolation Forest, and ocSVM achieve good performance overall AUC, but poor classification results on databases where the number of objects is equal or greater than 1,460, all features are nominal, or the imbalance ratio is equal or greater than 39.14.},
	language = {en},
	urldate = {2023-02-23},
	journal = {Knowledge-Based Systems},
	author = {Villa-Pérez, Miryam Elizabeth and Álvarez-Carmona, Miguel Á. and Loyola-González, Octavio and Medina-Pérez, Miguel Angel and Velazco-Rossell, Juan Carlos and Choo, Kim-Kwang Raymond},
	month = apr,
	year = {2021},
	keywords = {Anomaly detection, Meta-analysis study, Review, Semi-supervised classification, Up-to-date comparison},
	pages = {106878},
}

@article{ji_survey_2022,
	title = {A {Survey} on {Knowledge} {Graphs}: {Representation}, {Acquisition}, and {Applications}},
	volume = {33},
	issn = {2162-2388},
	shorttitle = {A {Survey} on {Knowledge} {Graphs}},
	doi = {10.1109/TNNLS.2021.3070843},
	abstract = {Human knowledge provides a formal understanding of the world. Knowledge graphs that represent structural relations between entities have become an increasingly popular research direction toward cognition and human-level intelligence. In this survey, we provide a comprehensive review of the knowledge graph covering overall research topics about: 1) knowledge graph representation learning; 2) knowledge acquisition and completion; 3) temporal knowledge graph; and 4) knowledge-aware applications and summarize recent breakthroughs and perspective directions to facilitate future research. We propose a full-view categorization and new taxonomies on these topics. Knowledge graph embedding is organized from four aspects of representation space, scoring function, encoding models, and auxiliary information. For knowledge acquisition, especially knowledge graph completion, embedding methods, path inference, and logical rule reasoning are reviewed. We further explore several emerging topics, including metarelational learning, commonsense reasoning, and temporal knowledge graphs. To facilitate future research on knowledge graphs, we also provide a curated collection of data sets and open-source libraries on different tasks. In the end, we have a thorough outlook on several promising research directions.},
	number = {2},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Ji, Shaoxiong and Pan, Shirui and Cambria, Erik and Marttinen, Pekka and Yu, Philip S.},
	month = feb,
	year = {2022},
	keywords = {Cognition, Deep learning, Extraterrestrial measurements, Knowledge acquisition, Knowledge based systems, Semantics, Task analysis, Taxonomy, knowledge graph, knowledge graph completion (KGC), reasoning, relation extraction, representation learning},
	pages = {494--514},
}

@misc{xu_multimodal_2022,
	title = {Multimodal {Learning} with {Transformers}: {A} {Survey}},
	shorttitle = {Multimodal {Learning} with {Transformers}},
	url = {http://arxiv.org/abs/2206.06488},
	doi = {10.48550/arXiv.2206.06488},
	abstract = {Transformer is a promising neural network learner, and has achieved great success in various machine learning tasks. Thanks to the recent prevalence of multimodal applications and big data, Transformer-based multimodal learning has become a hot topic in AI research. This paper presents a comprehensive survey of Transformer techniques oriented at multimodal data. The main contents of this survey include: (1) a background of multimodal learning, Transformer ecosystem, and the multimodal big data era, (2) a theoretical review of Vanilla Transformer, Vision Transformer, and multimodal Transformers, from a geometrically topological perspective, (3) a review of multimodal Transformer applications, via two important paradigms, i.e., for multimodal pretraining and for specific multimodal tasks, (4) a summary of the common challenges and designs shared by the multimodal Transformer models and applications, and (5) a discussion of open problems and potential research directions for the community.},
	urldate = {2022-11-11},
	publisher = {arXiv},
	author = {Xu, Peng and Zhu, Xiatian and Clifton, David A.},
	month = jun,
	year = {2022},
	note = {arXiv:2206.06488 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{noauthor_zotero_nodate,
	title = {Zotero {\textbar} {Your} personal research assistant},
	url = {https://www.zotero.org/},
	urldate = {2022-11-10},
}

@misc{noauthor_zotero_nodate-1,
	title = {Zotero {\textbar}},
	url = {https://www.zotero.org/user/register},
	urldate = {2022-11-10},
}

@article{mao_developing_2019,
	title = {Developing composite indicators for ecological water quality assessment based on network interactions and expert judgment},
	volume = {115},
	issn = {1364-8152},
	url = {https://www.sciencedirect.com/science/article/pii/S1364815217308769},
	doi = {10.1016/j.envsoft.2019.01.011},
	abstract = {Increasingly, composite indicators and multi-criteria approaches are applied in environmental assessment and decision-making, including the EU Water Framework Directive. For example, integrated evaluation of aquatic ecosystem conditions and functioning usually involves a group of criteria, such as biological organisms and communities, physicochemical and hydromorphological variables, which are measured individually and combined by a weighted linear function into an overall ‘score’. We argue that the network interactions of evaluation components are useful information for expert judgments, which have not been sufficiently considered in existing multi-criteria combination strategies in environmental assessment and management. Built upon the Analytic Network Process and demonstrated with the Chishui River Basin in China, this paper introduces a network-based expert judgment approach to construct ecological water quality indicators, and to determine and adjust their variable weight settings with information of interaction networks. This approach has potential to construct composite indicators for a broad environmental context.},
	language = {en},
	urldate = {2023-04-10},
	journal = {Environmental Modelling \& Software},
	author = {Mao, Feng and Zhao, Xianfu and Ma, Peiming and Chi, Shiyun and Richards, Keith and Clark, Julian and Hannah, David M. and Krause, Stefan},
	month = may,
	year = {2019},
	keywords = {Analytic network process, Composite indicator, Ecological network, Ecological water quality assessment, Integrated river basin management, Multi-criteria decision-making},
	pages = {51--62},
}

@misc{noauthor__nodate,
	title = {基于深度学习的海洋环境时空预测方法},
	url = {https://d.wanfangdata.com.cn/thesis/ChJUaGVzaXNOZXdTMjAyMzAxMTISCUQwMjU4NDA2MBoIZzJ5YmY5ODM%3D},
	urldate = {2023-04-12},
}

@article{ward_advancing_2022,
	title = {Advancing river corridor science beyond disciplinary boundaries with an inductive approach to catalyse hypothesis generation},
	volume = {36},
	issn = {1099-1085},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/hyp.14540},
	doi = {10.1002/hyp.14540},
	abstract = {A unified conceptual framework for river corridors requires synthesis of diverse site-, method- and discipline-specific findings. The river research community has developed a substantial body of observations and process-specific interpretations, but we are still lacking a comprehensive model to distill this knowledge into fundamental transferable concepts. We confront the challenge of how a discipline classically organized around the deductive model of systematically collecting of site-, scale-, and mechanism-specific observations begins the process of synthesis. Machine learning is particularly well-suited to inductive generation of hypotheses. In this study, we prototype an inductive approach to holistic synthesis of river corridor observations, using support vector machine regression to identify potential couplings or feedbacks that would not necessarily arise from classical approaches. This approach generated 672 relationships linking a suite of 157 variables each measured at 62 locations in a fifth order river network. Eighty four percent of these relationships have not been previously investigated, and representing potential (hypothetical) process connections. We document relationships consistent with current understanding including hydrologic exchange processes, microbial ecology, and the River Continuum Concept, supporting that the approach can identify meaningful relationships in the data. Moreover, we highlight examples of two novel research questions that stem from interpretation of inductively-generated relationships. This study demonstrates the implementation of machine learning to sieve complex data sets and identify a small set of candidate relationships that warrant further study, including data types not commonly measured together. This structured approach complements traditional modes of inquiry, which are often limited by disciplinary perspectives and favour the careful pursuit of parsimony. Finally, we emphasize that this approach should be viewed as a complement to, rather than in place of, more traditional, deductive approaches to scientific discovery.},
	language = {en},
	number = {4},
	urldate = {2023-04-11},
	journal = {Hydrological Processes},
	author = {Ward, Adam S. and Packman, Aaron and Bernal, Susana and Brekenfeld, Nicolai and Drummond, Jen and Graham, Emily and Hannah, David M. and Klaar, Megan and Krause, Stefan and Kurz, Marie and Li, Angang and Lupon, Anna and Mao, Feng and Roca, M. Eugènia Martí and Ouellet, Valerie and Royer, Todd V. and Stegen, James C. and Zarnetske, Jay P.},
	year = {2022},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/hyp.14540},
	keywords = {inductive, machine learning, river corridor, scientific method, stream corridor},
	pages = {e14540},
}

@article{mao_revision_2019,
	title = {Revision of biological indices for aquatic systems: {A} ridge-regression solution},
	volume = {106},
	issn = {1470-160X},
	shorttitle = {Revision of biological indices for aquatic systems},
	url = {https://www.sciencedirect.com/science/article/pii/S1470160X19304637},
	doi = {10.1016/j.ecolind.2019.105478},
	abstract = {Biological indices are used worldwide as tools for assessing the ecological status of waters, for example as recommended in the implementation of the European Union Water Framework Directive. The biological index is usually calculated on the basis of taxonomic composition and abundance information, and pre-defined environmental sensitivity values of each taxon. However, the extensive expert- or lab-based process of defining taxon-based sensitivity values makes it a challenge for the biological index to be calibrated, revised or localised. To address this challenge, this paper proposes a ridge-regression method to adjust efficiently the sensitivity values for each taxon on the basis of existing expert-judgement-based values and calibrate the biological indices from real environmental stressor information. The macroinvertebrate Average Score Per Taxon (ASPT) index, which is calculated with the Biological Monitoring Working Party (BMWP) sensitivity values, is selected as an example. Macroinvertebrate and physicochemical data were collected from a total of 107 sampling sites in the Chishui River basin over two years. By testing the proposed approach, this study localises the European-originated BMWP sensitivity values and improves the performance of the ASPT index in a river basin in China. The statistical analysis demonstrates that the ridge-regression strategy is an efficient approach to calibrate and revise biological indices. The calibrations based on different observation groups of macroinvertebrates show a good consistency. The research also identifies some under- and over- estimated taxon-specific sensitivity values in the original BMWP scheme, which agree with the results from some previous calibration studies. The ridge-regression based calibration approach is a promising tool with high flexibility for biologically-centred water quality assessment. The approach can be used for reappraising the BMWP values, and for revising biological indicators that have similar structures with the ASPT index. It can also be used to efficiently assign locally-appropriate ecological attributes and traits in new regions or for new taxa, or even to construct new stressor-specific biological indicators.},
	language = {en},
	urldate = {2023-04-10},
	journal = {Ecological Indicators},
	author = {Mao, Feng and Zhao, Xianfu and Ma, Peiming and Chi, Shiyun and Richards, Keith and Hannah, David M. and Krause, Stefan},
	month = nov,
	year = {2019},
	keywords = {ASPT, BMWP, Biological index, Index revision, Macroinvertebrates, Pollution},
	pages = {105478},
}

@article{chang_gradient_2022,
	title = {A {Gradient} {Model} for the {Spatial} {Patterns} of {Cities}},
	volume = {5},
	issn = {2513-0390},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/adts.202100486},
	doi = {10.1002/adts.202100486},
	abstract = {The dynamics of a city's structures are determined by the coupling of functional components (such as restaurants) and human population. Yet, there lacks mechanism models to quantify the forces on the spatial distribution of the components. Here, a gradient model is explored to simulate the individual density curves of multiple types of city functional components based on the equilibria of gravitational and repulsive forces along with the urban–rural gradient. The model is concise by relying on four key variables, the attributes of components include net ecosystem service (m) and environmental index (γ); and the attributes of cities include land rent exponent (σ) and population attenuation coefficient (β). The model has been used to simulate the distribution curves of 22 types of components on the urban–rural gradients in 13 cities in two periods. The model reveals a bottom-up mechanism that the patterns of the components in a city are determined by the economic, ecological, and social attributes of both cities and components. Strongly backed by empirical data, the model can predict the distribution curves of many types of components along with the development of cities. This model provides a general tool for analyzing the distribution of multiple objects on the gradients.},
	language = {en},
	number = {3},
	urldate = {2023-04-10},
	journal = {Advanced Theory and Simulations},
	author = {Chang, Jie and Yang, Guofu and Liu, Shun and Jin, Hanhui and Wu, Zhaoping and Xu, Ronghua and Min, Yong and Zheng, Kaiwen and Xu, Bin and Luo, Weidong and Mao, Feng and Ge, Ying and Cheong, Kang Hao},
	year = {2022},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/adts.202100486},
	keywords = {environmental index, land rent, population pattern, transport costs, urban-rural gradient},
	pages = {2100486},
}

@article{mao_moving_2020,
	title = {Moving beyond the {Technology}: {A} {Socio}-technical {Roadmap} for {Low}-{Cost} {Water} {Sensor} {Network} {Applications}},
	volume = {54},
	issn = {0013-936X},
	shorttitle = {Moving beyond the {Technology}},
	url = {https://doi.org/10.1021/acs.est.9b07125},
	doi = {10.1021/acs.est.9b07125},
	abstract = {In this paper, we critically review the current state-of-the-art for sensor network applications and approaches that have developed in response to the recent rise of low-cost technologies. We specifically focus on water-related low-cost sensor networks, and conceptualize them as socio-technical systems that can address resource management challenges and opportunities at three scales of resolution: (1) technologies, (2) users and scenarios, and (3) society and communities. Building this argument, first we identify a general structure for building low-cost sensor networks by assembling technical components across configuration levels. Second, we identify four application categories, namely operational monitoring, scientific research, system optimization, and community development, each of which has different technical and nontechnical configurations that determine how, where, by whom, and for what purpose low-cost sensor networks are used. Third, we discuss the governance factors (e.g., stakeholders and users, networks sustainability and maintenance, application scenarios, and integrated design) and emerging technical opportunities that we argue need to be considered to maximize the added value and long-term societal impact of the next generation of sensor network applications. We conclude that consideration of the full range of socio-technical issues is essential to realize the full potential of sensor network technologies for society and the environment.},
	number = {15},
	urldate = {2023-04-10},
	journal = {Environmental Science \& Technology},
	author = {Mao, Feng and Khamis, Kieran and Clark, Julian and Krause, Stefan and Buytaert, Wouter and Ochoa-Tocachi, Boris F. and Hannah, David M.},
	month = aug,
	year = {2020},
	note = {Publisher: American Chemical Society},
	pages = {9145--9158},
}

@article{chen_inequalities_2022,
	title = {Inequalities of urban green space area and ecosystem services along urban center-edge gradients},
	volume = {217},
	issn = {0169-2046},
	url = {https://www.sciencedirect.com/science/article/pii/S0169204621002292},
	doi = {10.1016/j.landurbplan.2021.104266},
	abstract = {Rising inequality threatens the improvement of human well-being. As an important type of green infrastructure within cities, urban green spaces provide ecosystem services and contribute to human health. The inequalities of per capita urban green space area and ecosystem services are critical environmental justice and public health issues but are not well understood. Based on the spatial patterns of green spaces, land rent, and the population of 20 cities in China, we assessed the inequalities of per capita green space area and ecosystem services by using the Gini coefficient. Results showed that, (1) the Gini coefficient (an indicator to measure inequality) of per capita available ecosystem services was 0.430, which was greater than that of per capita green space area (0.357), (2) the inequality of per capita ecosystem services had a negative relationship with city size measured by population and GDP, and (3) the inequality of green space area was negatively related to the subjective quality of life, while the inequality of ecosystem services was negatively related to the economic competitiveness of cities. We suggest that urban planners comprehensively consider multiple indicators (such as per capita green space area, Gini coefficients of per capita green space area and ecosystem services) to simultaneously evaluate the efficiency and equality of green space construction.},
	language = {en},
	urldate = {2023-04-10},
	journal = {Landscape and Urban Planning},
	author = {Chen, Yi and Ge, Ying and Yang, Guofu and Wu, Zhaoping and Du, Yuanyuan and Mao, Feng and Liu, Shun and Xu, Ronghua and Qu, Zelong and Xu, Bin and Chang, Jie},
	month = jan,
	year = {2022},
	keywords = {Cultural ecosystem services, Environmental injustice, Human well-being, Public health, Urban planning},
	pages = {104266},
}

@article{mao_increasing_2021,
	title = {Increasing nutrient inputs risk a surge of nitrous oxide emissions from global mangrove ecosystems},
	volume = {4},
	issn = {2590-3322},
	url = {https://www.sciencedirect.com/science/article/pii/S2590332221001925},
	doi = {10.1016/j.oneear.2021.04.007},
	abstract = {We document a substantial increase in global N2O emissions from mangroves. Based on our analysis of two decades of mangrove N2O emission studies, we estimate N2O emission of 0.023 Tg N year−1 from global mangrove ecosystems. N2O fluxes from mangrove ecosystems are strongly increased by sediment dissolved inorganic nitrogen (DIN) concentration transported from river catchments to coastal waters. Continuing growth of nutrient inputs from anthropogenic sources, i.e., agricultural intensification, excessive fertilizer use and waste water discharge, will appreciably increase DIN loading and consequently global N2O emission from mangroves. Based on the Millennium Ecosystem Assessment scenarios of riverine DIN inputs into mangrove ecosystems coupled with our estimates of DIN-controlled emissions rates, we expect N2O emission to increase by 20\%–51\% by 2030 and 27\%–74\% by 2050 compared with estimated emissions in the year 2000. These forecasts underline the urgency of improvements in catchment-scale nitrogen management strategies.},
	language = {en},
	number = {5},
	urldate = {2023-04-10},
	journal = {One Earth},
	author = {Mao, Feng and Ullah, Sami and Gorelick, Steven M. and Hannah, David M. and Krause, Stefan},
	month = may,
	year = {2021},
	keywords = {climate change, emissions, greenhouse gas, mangroves, nitrous oxide, nutrients, river catchments},
	pages = {742--748},
}

@article{li_can_2022,
	title = {Can we share models if sharing data is not an option?},
	volume = {3},
	issn = {2666-3899},
	doi = {10.1016/j.patter.2022.100603},
	abstract = {In the big data era, vast volumes of data are generated daily as the foundation of data-driven scientific discovery. Thanks to the recent open data movement, much of these data are being made available to the public, significantly advancing scientific research and accelerating socio-technical development. However, not all data are suitable for opening or sharing because of concerns over privacy, ownership, trust, and incentive. Therefore, data sharing remains a challenge for specific data types and holders, making a bottleneck for further unleashing the potential of these “closed data.” To address this challenge, in this perspective, we conceptualize the current practices and technologies in data collaboration in a data-sharing-free manner and propose a concept of the model-sharing strategy for using closed data without sharing them. Supported by emerging advances in artificial intelligence, this strategy will unleash the large potential in closed data. Moreover, we show the advantages of the model-sharing strategy and explain how it will lead to a new paradigm of big data governance and collaboration.},
	language = {en},
	number = {11},
	urldate = {2023-04-09},
	journal = {Patterns},
	author = {Li, Zexi and Mao, Feng and Wu, Chao},
	month = nov,
	year = {2022},
	keywords = {artificial intelligence, big data, data sharing, federated learning, model-sharing strategy, open science},
	pages = {100603},
}

@article{mao_inequality_2022,
	title = {Inequality of household water security follows a {Development} {Kuznets} {Curve}},
	volume = {13},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-022-31867-3},
	doi = {10.1038/s41467-022-31867-3},
	abstract = {Abstract
            Water security requires not only sufficient availability of and access to safe and acceptable quality for domestic uses, but also fair distribution within and across populations. However, a key research gap remains in understanding water security inequality and its dynamics, which in turn creates an impediment to tracking progress towards sustainable development. Therefore, we analyse the inequality of water security using data from 7603 households across 28 sites in 22 low- and middle-income countries, measured using the Household Water Insecurity Experiences Scale. Here we show an inverted-U shaped relationship between site water security and inequality of household water security. This Kuznets-like curve suggests a process that as water security grows, the inequality of water security first increases then decreases. This research extends the Kuznets curve applications and introduces the Development Kuznets Curve concept. Its practical implications support building water security and achieving more fair, inclusive, and sustainable development.},
	language = {en},
	number = {1},
	urldate = {2023-04-09},
	journal = {Nature Communications},
	author = {Mao, Feng and Miller, Joshua D. and Young, Sera L. and Krause, Stefan and Hannah, David M. and {HWISE Research Coordination Network} and Brewis, Alexandra and Trowell, Alex and Pearson, Amber L. and Wutich, Amber and Sullivan, Andrea and Rosinger, Asher Y. and Hagaman, Ashley and Workman, Cassandra and Staddon, Chad and Tshala-Katumbay, Desire and Krishnakumar, Divya and Adams, Ellis and Sánchez-Rodriguez, Ernesto C. and Carrillo, Genny and Asiki, Gershim and Ghattas, Hala and Eini-Zinab, Hassan and Melgar-Quiñonez, Hugo and Ahmed, Farooq and Moran-Martinez, Javier and Maupin, Jonathan and Escobar-Vargas, Jorge and Stoler, Justin and Mathad, Jyoti and Chapman, Kelly and Maes, Kenneth and Samayoa-Figueroa, Luisa and Sheikhi, Mahdieh and Alexander, Mallika and Santoso, Marianne V. and Freeman, Matthew C. and Boivin, Michael J. and Morales, Milton Marin and Balogun, Mobolanle and Ghorbani, Monet and Omidvar, Nasrin and Triviño, Nathaly and Hawley, Nicola and Owuor, Patrick Mbullo and Tutu, Raymond and Schuster, Roseanne C. and Rasheed, Sabrina and Collins, Shalean M. and Srivastava, Sonali and Cole, Stroma and Jepson, Wendy and Tesfaye, Yihenew and Jamaluddine, Zeina},
	month = aug,
	year = {2022},
	pages = {4525},
}

@inproceedings{yazdanpanah_revisiting_2022,
	title = {Revisiting {Learnable} {Affines} for {Batch} {Norm} in {Few}-{Shot} {Transfer} {Learning}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Yazdanpanah_Revisiting_Learnable_Affines_for_Batch_Norm_in_Few-Shot_Transfer_Learning_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-04-12},
	author = {Yazdanpanah, Moslem and Rahman, Aamer Abdul and Chaudhary, Muawiz and Desrosiers, Christian and Havaei, Mohammad and Belilovsky, Eugene and Kahou, Samira Ebrahimi},
	year = {2022},
	pages = {9109--9118},
}

@misc{noauthor__nodate-1,
	title = {面向城市规划的群体轨迹数据挖掘关键技术研究},
	url = {https://d.wanfangdata.com.cn/thesis/ChJUaGVzaXNOZXdTMjAyMzAxMTISCUQwMjc3OTE4MhoIYmE5bzdwNW8%3D},
	urldate = {2023-04-12},
}

@inproceedings{liu_learning_2022,
	title = {Learning {To} {Affiliate}: {Mutual} {Centralized} {Learning} for {Few}-{Shot} {Classification}},
	shorttitle = {Learning {To} {Affiliate}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Liu_Learning_To_Affiliate_Mutual_Centralized_Learning_for_Few-Shot_Classification_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-04-12},
	author = {Liu, Yang and Zhang, Weifeng and Xiang, Chao and Zheng, Tu and Cai, Deng and He, Xiaofei},
	year = {2022},
	pages = {14411--14420},
}

@inproceedings{jiang_bongard-hoi_2022,
	title = {Bongard-{HOI}: {Benchmarking} {Few}-{Shot} {Visual} {Reasoning} for {Human}-{Object} {Interactions}},
	shorttitle = {Bongard-{HOI}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Jiang_Bongard-HOI_Benchmarking_Few-Shot_Visual_Reasoning_for_Human-Object_Interactions_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-04-12},
	author = {Jiang, Huaizu and Ma, Xiaojian and Nie, Weili and Yu, Zhiding and Zhu, Yuke and Anandkumar, Anima},
	year = {2022},
	pages = {19056--19065},
}

@inproceedings{li_siamese_2022,
	title = {Siamese {Contrastive} {Embedding} {Network} for {Compositional} {Zero}-{Shot} {Learning}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Li_Siamese_Contrastive_Embedding_Network_for_Compositional_Zero-Shot_Learning_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-04-12},
	author = {Li, Xiangyu and Yang, Xu and Wei, Kun and Deng, Cheng and Yang, Muli},
	year = {2022},
	pages = {9326--9335},
}

@misc{noauthor__nodate-2,
	title = {面向多源数据集成的城市综合管廊智慧监管关键问题研究},
	url = {https://d.wanfangdata.com.cn/thesis/ChJUaGVzaXNOZXdTMjAyMzAxMTISCUQwMjg0MDgxMhoIYmE5bzdwNW8%3D},
	urldate = {2023-04-12},
}

@phdthesis{noauthor__nodate-3,
	title = {数据驱动的智慧城市中共享资源调度和隐私保护机制研究},
}

@misc{noauthor_--2017_nodate,
	title = {分布式系统架构在大型水资源管理体系中的研究与应用--《华中科技大学》2017年博士论文},
	url = {https://cdmd.cnki.com.cn/Article/CDMD-10487-1017145959.htm},
	urldate = {2023-04-12},
}

@phdthesis{noauthor__nodate-4,
	title = {面向低功耗物联网的标签识别与网络规划研究},
	school = {浙江大学},
}

@misc{noauthor__nodate-5,
	title = {面向复杂时空数据的可视查询与分析方法研究},
	url = {https://d.wanfangdata.com.cn/thesis/ChJUaGVzaXNOZXdTMjAyMzAxMTISCUQwMjI3OTIwNRoIbXB1b3NlbTY%3D},
	urldate = {2023-04-12},
}

@misc{noauthor__nodate-6,
	title = {时变体数据特征追踪与可视化},
	url = {https://d.wanfangdata.com.cn/thesis/ChJUaGVzaXNOZXdTMjAyMzAxMTISCFkzNjkyMDIxGghtcHVvc2VtNg%3D%3D},
	urldate = {2023-04-12},
}

@inproceedings{zhou_conditional_2022,
	title = {Conditional {Prompt} {Learning} for {Vision}-{Language} {Models}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Zhou_Conditional_Prompt_Learning_for_Vision-Language_Models_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-04-24},
	author = {Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei},
	year = {2022},
	pages = {16816--16825},
}

@inproceedings{lu_prompt_2022,
	title = {Prompt {Distribution} {Learning}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Lu_Prompt_Distribution_Learning_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-04-24},
	author = {Lu, Yuning and Liu, Jianzhuang and Zhang, Yonggang and Liu, Yajing and Tian, Xinmei},
	year = {2022},
	pages = {5206--5215},
}

@inproceedings{liu_learning_2022-1,
	title = {Learning {Hierarchical} {Cross}-{Modal} {Association} for {Co}-{Speech} {Gesture} {Generation}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Liu_Learning_Hierarchical_Cross-Modal_Association_for_Co-Speech_Gesture_Generation_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-04-14},
	author = {Liu, Xian and Wu, Qianyi and Zhou, Hang and Xu, Yinghao and Qian, Rui and Lin, Xinyi and Zhou, Xiaowei and Wu, Wayne and Dai, Bo and Zhou, Bolei},
	year = {2022},
	pages = {10462--10472},
}

@inproceedings{qian_speech_2021,
	title = {Speech {Drives} {Templates}: {Co}-{Speech} {Gesture} {Synthesis} {With} {Learned} {Templates}},
	shorttitle = {Speech {Drives} {Templates}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Qian_Speech_Drives_Templates_Co-Speech_Gesture_Synthesis_With_Learned_Templates_ICCV_2021_paper.html?ref=https://githubhelp.com},
	language = {en},
	urldate = {2023-04-14},
	author = {Qian, Shenhan and Tu, Zhi and Zhi, Yihao and Liu, Wen and Gao, Shenghua},
	year = {2021},
	pages = {11077--11086},
}

@article{yoon_speech_2020,
	title = {Speech gesture generation from the trimodal context of text, audio, and speaker identity},
	volume = {39},
	issn = {0730-0301},
	url = {https://dl.acm.org/doi/10.1145/3414685.3417838},
	doi = {10.1145/3414685.3417838},
	abstract = {For human-like agents, including virtual avatars and social robots, making proper gestures while speaking is crucial in human-agent interaction. Co-speech gestures enhance interaction experiences and make the agents look alive. However, it is difficult to generate human-like gestures due to the lack of understanding of how people gesture. Data-driven approaches attempt to learn gesticulation skills from human demonstrations, but the ambiguous and individual nature of gestures hinders learning. In this paper, we present an automatic gesture generation model that uses the multimodal context of speech text, audio, and speaker identity to reliably generate gestures. By incorporating a multimodal context and an adversarial training scheme, the proposed model outputs gestures that are human-like and that match with speech content and rhythm. We also introduce a new quantitative evaluation metric for gesture generation models. Experiments with the introduced metric and subjective human evaluation showed that the proposed gesture generation model is better than existing end-to-end generation models. We further confirm that our model is able to work with synthesized audio in a scenario where contexts are constrained, and show that different gesture styles can be generated for the same speech by specifying different speaker identities in the style embedding space that is learned from videos of various speakers. All the code and data is available at https://github.com/ai4r/Gesture-Generation-from-Trimodal-Context.},
	number = {6},
	urldate = {2023-04-14},
	journal = {ACM Transactions on Graphics},
	author = {Yoon, Youngwoo and Cha, Bok and Lee, Joo-Haeng and Jang, Minsu and Lee, Jaeyeon and Kim, Jaehong and Lee, Geehyuk},
	month = nov,
	year = {2020},
	keywords = {co-speech gesture, evaluation of a generative model, git, multimodality, neural generative model, nonverbal behavior},
	pages = {222:1--222:16},
}

@inproceedings{ahuja_style_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Style {Transfer} for {Co}-speech {Gesture} {Animation}: {A} {Multi}-speaker {Conditional}-{Mixture} {Approach}},
	isbn = {978-3-030-58523-5},
	shorttitle = {Style {Transfer} for {Co}-speech {Gesture} {Animation}},
	doi = {10.1007/978-3-030-58523-5_15},
	abstract = {How can we teach robots or virtual assistants to gesture naturally? Can we go further and adapt the gesturing style to follow a specific speaker? Gestures that are naturally timed with corresponding speech during human communication are called co-speech gestures. A key challenge, called gesture style transfer, is to learn a model that generates these gestures for a speaking agent ‘A’ in the gesturing style of a target speaker ‘B’. A secondary goal is to simultaneously learn to generate co-speech gestures for multiple speakers while remembering what is unique about each speaker. We call this challenge style preservation. In this paper, we propose a new model, named Mix-StAGE, which trains a single model for multiple speakers while learning unique style embeddings for each speaker’s gestures in an end-to-end manner. A novelty of Mix-StAGE is to learn a mixture of generative models which allows for conditioning on the unique gesture style of each speaker. As Mix-StAGE disentangles style and content of gestures, gesturing styles for the same input speech can be altered by simply switching the style embeddings. Mix-StAGE also allows for style preservation when learning simultaneously from multiple speakers. We also introduce a new dataset, Pose-Audio-Transcript-Style (PATS), designed to study gesture generation and style transfer. Our proposed Mix-StAGE model significantly outperforms the previous state-of-the-art approach for gesture generation and provides a path towards performing gesture style transfer across multiple speakers. Link to code, data and videos: http://chahuja.com/mix-stage.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Ahuja, Chaitanya and Lee, Dong Won and Nakano, Yukiko I. and Morency, Louis-Philippe},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	keywords = {Co-speech gestures, Gesture animation, Style transfer},
	pages = {248--265},
}

@article{ao_rhythmic_2022,
	title = {Rhythmic {Gesticulator}: {Rhythm}-{Aware} {Co}-{Speech} {Gesture} {Synthesis} with {Hierarchical} {Neural} {Embeddings}},
	volume = {41},
	issn = {0730-0301, 1557-7368},
	shorttitle = {Rhythmic {Gesticulator}},
	url = {http://arxiv.org/abs/2210.01448},
	doi = {10.1145/3550454.3555435},
	abstract = {Automatic synthesis of realistic co-speech gestures is an increasingly important yet challenging task in artificial embodied agent creation. Previous systems mainly focus on generating gestures in an end-to-end manner, which leads to difficulties in mining the clear rhythm and semantics due to the complex yet subtle harmony between speech and gestures. We present a novel co-speech gesture synthesis method that achieves convincing results both on the rhythm and semantics. For the rhythm, our system contains a robust rhythm-based segmentation pipeline to ensure the temporal coherence between the vocalization and gestures explicitly. For the gesture semantics, we devise a mechanism to effectively disentangle both low- and high-level neural embeddings of speech and motion based on linguistic theory. The high-level embedding corresponds to semantics, while the low-level embedding relates to subtle variations. Lastly, we build correspondence between the hierarchical embeddings of the speech and the motion, resulting in rhythm- and semantics-aware gesture synthesis. Evaluations with existing objective metrics, a newly proposed rhythmic metric, and human feedback show that our method outperforms state-of-the-art systems by a clear margin.},
	number = {6},
	urldate = {2023-04-13},
	journal = {ACM Transactions on Graphics},
	author = {Ao, Tenglong and Gao, Qingzhe and Lou, Yuke and Chen, Baoquan and Liu, Libin},
	month = dec,
	year = {2022},
	note = {arXiv:2210.01448 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	pages = {1--19},
}

@inproceedings{xiao_few_2022,
	title = {Few {Shot} {Generative} {Model} {Adaption} via {Relaxed} {Spatial} {Structural} {Alignment}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Xiao_Few_Shot_Generative_Model_Adaption_via_Relaxed_Spatial_Structural_Alignment_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-04-12},
	author = {Xiao, Jiayu and Li, Liang and Wang, Chaofei and Zha, Zheng-Jun and Huang, Qingming},
	year = {2022},
	pages = {11204--11213},
}

@inproceedings{hu_pushing_2022,
	title = {Pushing the {Limits} of {Simple} {Pipelines} for {Few}-{Shot} {Learning}: {External} {Data} and {Fine}-{Tuning} {Make} a {Difference}},
	shorttitle = {Pushing the {Limits} of {Simple} {Pipelines} for {Few}-{Shot} {Learning}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Hu_Pushing_the_Limits_of_Simple_Pipelines_for_Few-Shot_Learning_External_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-04-12},
	author = {Hu, Shell Xu and Li, Da and Stühmer, Jan and Kim, Minyoung and Hospedales, Timothy M.},
	year = {2022},
	pages = {9068--9077},
}

@misc{lian_scaling_2023,
	title = {Scaling \& {Shifting} {Your} {Features}: {A} {New} {Baseline} for {Efficient} {Model} {Tuning}},
	shorttitle = {Scaling \& {Shifting} {Your} {Features}},
	url = {http://arxiv.org/abs/2210.08823},
	doi = {10.48550/arXiv.2210.08823},
	abstract = {Existing fine-tuning methods either tune all parameters of the pre-trained model (full fine-tuning), which is not efficient, or only tune the last linear layer (linear probing), which suffers a significant accuracy drop compared to the full fine-tuning. In this paper, we propose a new parameter-efficient fine-tuning method termed as SSF, representing that researchers only need to Scale and Shift the deep Features extracted by a pre-trained model to catch up with the performance of full fine-tuning. In this way, SSF also surprisingly outperforms other parameter-efficient fine-tuning approaches even with a smaller number of tunable parameters. Furthermore, different from some existing parameter-efficient fine-tuning methods (e.g., Adapter or VPT) that introduce the extra parameters and computational cost in the training and inference stages, SSF only adds learnable parameters during the training stage, and these additional parameters can be merged into the original pre-trained model weights via re-parameterization in the inference phase. With the proposed SSF, our model obtains 2.46\% (90.72\% vs. 88.54\%) and 11.48\% (73.10\% vs. 65.57\%) performance improvement on FGVC and VTAB-1k in terms of Top-1 accuracy compared to the full fine-tuning but only fine-tuning about 0.3M parameters. We also conduct amounts of experiments in various model families (CNNs, Transformers, and MLPs) and datasets. Results on 26 image classification datasets in total and 3 robustness \& out-of-distribution datasets show the effectiveness of SSF. Code is available at https://github.com/dongzelian/SSF.},
	urldate = {2023-04-25},
	publisher = {arXiv},
	author = {Lian, Dongze and Zhou, Daquan and Feng, Jiashi and Wang, Xinchao},
	month = jan,
	year = {2023},
	note = {arXiv:2210.08823 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{rao_denseclip_2022,
	title = {{DenseCLIP}: {Language}-{Guided} {Dense} {Prediction} {With} {Context}-{Aware} {Prompting}},
	shorttitle = {{DenseCLIP}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Rao_DenseCLIP_Language-Guided_Dense_Prediction_With_Context-Aware_Prompting_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-04-24},
	author = {Rao, Yongming and Zhao, Wenliang and Chen, Guangyi and Tang, Yansong and Zhu, Zheng and Huang, Guan and Zhou, Jie and Lu, Jiwen},
	year = {2022},
	pages = {18082--18091},
}

@inproceedings{wang_dualprompt_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{DualPrompt}: {Complementary} {Prompting} for {Rehearsal}-{Free} {Continual} {Learning}},
	isbn = {978-3-031-19809-0},
	shorttitle = {{DualPrompt}},
	doi = {10.1007/978-3-031-19809-0_36},
	abstract = {Continual learning aims to enable a single model to learn a sequence of tasks without catastrophic forgetting. Top-performing methods usually require a rehearsal buffer to store past pristine examples for experience replay, which, however, limits their practical value due to privacy and memory constraints. In this work, we present a simple yet effective framework, DualPrompt, which learns a tiny set of parameters, called prompts, to properly instruct a pre-trained model to learn tasks arriving sequentially without buffering past examples. DualPrompt presents a novel approach to attach complementary prompts to the pre-trained backbone, and then formulates the objective as learning task-invariant and task-specific “instructions”. With extensive experimental validation, DualPrompt consistently sets state-of-the-art performance under the challenging class-incremental setting. In particular, DualPrompt outperforms recent advanced continual learning methods with relatively large buffer sizes. We also introduce a more challenging benchmark, Split ImageNet-R, to help generalize rehearsal-free continual learning research. Source code is available at https://github.com/google-research/l2p.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Wang, Zifeng and Zhang, Zizhao and Ebrahimi, Sayna and Sun, Ruoxi and Zhang, Han and Lee, Chen-Yu and Ren, Xiaoqi and Su, Guolong and Perot, Vincent and Dy, Jennifer and Pfister, Tomas},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	keywords = {Continual learning, Prompt-based learning, Rehearsal-free},
	pages = {631--648},
}

@inproceedings{wang_learning_2022,
	title = {Learning {To} {Prompt} for {Continual} {Learning}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Wang_Learning_To_Prompt_for_Continual_Learning_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-04-23},
	author = {Wang, Zifeng and Zhang, Zizhao and Lee, Chen-Yu and Zhang, Han and Sun, Ruoxi and Ren, Xiaoqi and Su, Guolong and Perot, Vincent and Dy, Jennifer and Pfister, Tomas},
	year = {2022},
	pages = {139--149},
}