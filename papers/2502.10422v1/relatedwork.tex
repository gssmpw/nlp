\section{Related Works}
Training of SNN typically follows two approaches: Artificial Neural Network (ANN)-to-SNN conversion and direct SNN training. The ANN-to-SNN method involves converting pre-trained ANNs by mapping parameters and replacing ReLU activation with spiking activation. 
For direct SNN training, recent advancements have introduced various improvements. The tdBN \cite{zheng_going_2021} enhances feature normalization in temporal and spatial dimensions , while MPBN \cite{guo_membrane_2023} and TET stabilize the training process through better batch normalization and momentum control. 
Other innovations, such as SEW-ResNet and DS-ResNet, improve the adaptation of ResNet architectures to SNNs\cite{fang_deep_2021,feng_multi-level_2022,hu_advancing_2024}. 
Techniques like IM-Loss \cite{guo_im-loss_2022} and model compression strategies also optimize information processing and advancing the applicability of SNNs\cite{xu_constructing_2023}.

Several studies have demonstrated the advantages of adaptive LIF models\cite{chen_gradual_2022,liu_event-based_2022}. For example, LSNN \cite{bellec_long_2018} and LTMD\cite{wang_ltmd_2022} introduced adaptive threshold neurons, improving the learning dynamics of SNNs. Additionally, works such as PLIF incorporated a learnable membrane time constant to enhance the performance of spiking neurons\cite{fang_incorporating_2021}. Recent efforts like Diet-SNN and BDETT have further optimized neuron models by integrating learnable parameters such as membrane leak and firing thresholds\cite{rathi_diet-snn_2023,ding_biologically_2022,xu2024rsnn}.
Despite these advances, current models do not independently address spatial and temporal aspects of neuron behavior. Future work could focus on separate learnable parameters for each dimension and layer, aligning more closely with biological observations and enhancing SNN expressiveness.