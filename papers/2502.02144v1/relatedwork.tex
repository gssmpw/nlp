\section{Related work}
\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figs/fig_related_fail.png}
    \vspace{-15pt}
    \caption{\textbf{Qualitative results of DOC-Depth against the learning-based approach BP-Net} \cite{tang2024bilateral} trained on KITTI and tested on our datasets. While learning-method performances drop when tested out of its training domain, our method works across domains, with the same parameters. LiDAR used: 2x Hesai XT-32 (left) and Ouster OS1-128 (right)}
    \label{fig:related_work_fail}
    \vspace{-15pt}
\end{figure}

\subsection{Depth completion}

To perform depth completion from LiDAR, unguided \cite{uhrig2017sparsity,eldesokey2020uncertainty,eldesokey2019propagating} and image-guided \cite{zhang2023completionformer,nazir2022semattnet,tang2024bilateral} methods have been proposed. Unguided methods usually lack structural information about the scene due to the high sparsity of the input, leading to output without proper geometry. Image-guided methods use multi-branch networks and spatial propagation networks to merge image and LiDAR modalities. They produce better results with respect to the scene geometry. However, their performance can be significantly affected by adverse conditions (night, rain, etc.), that considerably damage the perception of the camera \cite{SDV21}.

While providing great performances, deep-learning methods do not generalize well on new domains \simon{as shown in \cref{fig:related_work_fail}}. Unguided methods depend on the LiDAR-pattern and image-guided ones are subject to image domain shift. Thus, they are not suited to dataset creation.  

Our method is learning-free and based only on LiDAR measurements. It achieves high performances across domains and is agnostic to LiDAR scan pattern and resolution.


\subsection{3D scene completion}
3D scene completion holds great potential for acquiring dense information.
Several approaches have been proposed to construct consistent dense scenes from sparse LiDAR measurements. 
\cite{vizzo2022make,dai2020sg,weder2021neuralfusion,murez2020atlas} are based on volumetric fusion. More recently, \cite{cao2024pasco,li2023voxformer,xia2023scpnet} propose combining semantic information to enhance results. \cite{nunes2024cvpr} successfully applied diffusion models to large scale 3D scenes. Since the input data are not sufficient to resolve all ambiguities, these methods hallucinate part of missing geometry \cite{roldao20223d}. While being interesting for mapping and understanding scenes, this is not sufficient for ground truth generation. 

DOC-Depth uses only real measurements from the LiDAR sensors, ensuring an accurate and true to life geometry. 


\subsection{Dense depth datasets}
KITTI dataset \cite{Geiger2012CVPR} is a widely used benchmark for depth estimation and completion \cite{uhrig2017sparsity}. While providing large scale data, it only features semi-dense depth. Depth images are produced by aggregating 11 Velodyne HDL-64E scans and comparing them to depth values obtained through a semi-global-matching method (SGM). More recently, DDAD \cite{packnet} provided semi-dense depth annotation with only one frame from high-resolution Luminar-H2 sensors. 

ETH3D \cite{Schops_2019_CVPR} and DIODE \cite{diode_dataset} feature real dense depth annotations with measurements based on a FARO scanner. This sensor is highly accurate but requires several minutes to capture a single static scan and hours to record an entire scene, making it unsuitable for dynamic environments.

In contrast, DOC-Depth produces accurate fully-dense depth maps using any LiDAR. It results in an easier and cost-effective method for dataset creation.