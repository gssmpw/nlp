\input{Table/overall}
\section{Evaluation Results}
% In this section, we first evaluate the performance of LLM-QE and conduct ablation studies. Then we explore the query expansion capabilities of LLM-QE and analyze the role of the reward model.

In this section, we present the overall performance of LLM-QE, conduct ablation studies, and analyze the effectiveness of different reward models. Detailed analysis of query expansion quality is provided in the Appendix~\ref{app:analysis}. The case study is shown in Appendix~\ref{app:case_study}.
% The analysis is provided in the Appendix~\ref{app:anaylsis} and Appendix~\ref{app:case_study}.


% 5.1
\subsection{Overall Performance}
The retrieval performance of different query expansion models is shown in Table~\ref{tab:overall}.

We begin by evaluating the performance of LLM-QE in an unsupervised setting. The evaluation results show that LLM-QE achieves an 8.6\% improvement over the retriever Contriever, demonstrating the effectiveness of query expansion in enhancing unsupervised retrieval models. Among all query expansion baselines, Q2D performs the best, indicating that generating query-related documents as the expansion results is particularly suited for LLMs to generate effective expansion results. After DPO training, LLM-QE shows a significant improvement over the Q2D method, confirming the effectiveness of its training approach in enhancing the expansion capabilities of the Q2D model.

Additionally, LLM-QE demonstrates its robustness by extending its advantages to supervised retrieval training scenarios. After fine-tuning dense retrieval models using the E5 dataset, LLM-QE consistently outperforms the Contriever fine-tuned with raw queries across most datasets, achieving an average improvement of 5.5\%. This indicates the effectiveness of our query expansion techniques in benefiting the training process of dense retrievers by narrowing the semantic gap between queries and documents.


% 5.2
\input{Table/ablation}
\subsection{Ablation Study}\label{sec:ablation}
In this subsection, we present ablation studies to investigate the effectiveness of LLM-QE in various query expansion scenarios and analyze the impact of different query expansion models in both unsupervised and supervised settings.

As shown in Table~\ref{tab:ablation}, we first apply LLM-QE to several query expansion methods in the unsupervised setting, including Q2Q, Q2E, and Q2C. The results indicate that LLM-QE consistently improves the performance of different expansion formats by approximately 3\%, highlighting its generalization ability. Among all the expansion formats, LLM-QE (Q2D) achieves the best performance, indicating that prompting the LLM to generate tailored pseudo-relevant documents for specific queries can bring more precise retrieval results~\cite{gao2023precise}.

Next, we explore the role of the reward model by optimizing the LLM-QE (Q2D) under two settings: one with only the rank-based reward and the other with only the answer-based reward, resulting in the LLM-QE (Q2D) w/o Answer Reward model and the LLM-QE (Q2D) w/o Rank Reward model, respectively. The rank-based reward, which utilizes preference signals from the dense retriever, typically achieves better ranking performance than the answer-based reward. However, when both rewards are combined, LLM-QE (Q2D) shows further improvements, particularly in question-answering tasks. This demonstrates the effectiveness of answer-based reward modeling, which leverages the consistency between generated answers and query-expanded documents to estimate the quality of the expansion results.

Finally, we assess the effectiveness of LLM-QE (Q2Q), LLM-QE (Q2E), LLM-QE (Q2C), and LLM-QE (Q2D) in the supervised setting. Overall, all query expansion models show a consistent improvement of more than 5\% across various expansion formats, further confirming the efficacy of LLM-based query expansion models in better aligning the semantics of queries and their corresponding documents during contrastive training. Among all query expansion models, LLM-QE (Q2D) achieves the best retrieval performance, with an improvement of about 2\% over other expansion models, demonstrating that LLM-QE (Q2D) can broaden its advantage to the supervised setting.



% 5.3
\subsection{Effectiveness of Different Reward Models in Optimizing LLM-QE}
Figure~\ref{fig:characteristics} illustrates that we evaluate the characteristics of reward models in LLM-QE based on the length, text similarity, and rank correlation of the query expansion results. We compare two ablation models, LLM-QE (w/o Rank Reward) and LLM-QE (w/o Answer Reward), with the LLM-QE model. The former two train LLM-QE models using only the answer-based reward and the rank-based reward, respectively.


\textbf{Length of Query Expansion.} First, we analyze the average length of query expansions generated by different models, as shown in Figure~\ref{fig:characteristics:length}.

The evaluation results show that LLM-QE (w/o Answer Reward) produces the longest query expansions, indicating that using only the rank-based reward encourages the model to generate redundant tokens related to the ground truth document. Such a behavior can help LLMs win a higher rank-based reward score. However, when the answer-based reward is incorporated, the length of the query expansions generated by LLM-QE significantly decreases. This proves the effectiveness of the answer-based reward in preventing excessively long query expansions, reducing the tendency of the LLM-QE model to overfit to the ranking preferences of dense retrievers, and mitigating the generation of noisy information.


\input{Figure/performance_rm}
\textbf{Text Similarity Evaluation.} In the following experiment, we evaluate the text similarity between document-based query expansions and LLM-generated answers/golden documents.

As illustrated in Figure~\ref{fig:characteristics:ans}, we first estimate the similarity between document-based query expansions and LLM-generated answers. LLM-QE (w/o Rank Reward) is designed to optimize LLMs to generate query expansions that are more closely related to the LLM-generated answers, thereby achieving the highest similarity score. In contrast, LLM-QE (w/o Answer Reward) yields the lowest BLEU score, indicating that rank-based rewards are less effective in guiding LLM-QE to align with the information in answers, which is particularly important for question answering tasks. When answer-based rewards are incorporated, the BLEU scores for LLM-QE increase, showing its effectiveness in generating more precise information for expansion.

Next, we estimate the text similarity between query expansions generated by different models and ground-truth documents, as shown in Figure~\ref{fig:characteristics:posi}. The evaluation results reveal that LLM-QE (w/o Rank Reward) presents the lowest BLEU score, suggesting that optimizing solely with the answer-based reward is insufficient to improve the text similarity between query expansions and golden documents. In comparison, LLM-QE (w/o Answer Reward) achieves better performance, demonstrating the effectiveness of rank-based rewards in guiding LLMs to generate more relevant semantics for matching the golden documents. By combining both rewards, LLM-QE achieves the highest BLEU score, underscoring the importance of using both rewards to optimize LLMs to generate higher-quality query expansions.

\textbf{Rank Correlation.} Lastly, we assess the similarity between raw queries and query expansions by ranking these candidate documents and calculate the rank correlation.

As shown in Figure~\ref{fig:characteristics:pcc}, we calculate the Pearson Correlation Coefficient (PCC) to measure the correlation between the query-based document rank and the expansion-based document rank. Specifically, we employ a fine-tuned retriever, BGE~\cite{chen2024bge}, to compute the similarity between raw queries and relevant documents. We then use expansions generated by different models as queries and calculate the similarity with candidate documents. Finally, we compute the PCC between these similarity score lists.

The results show that LLM-QE (w/o Rank Reward) achieves the highest PCC score, demonstrating the effectiveness of answer-based rewards in producing query expansions that better reflect user search intentions. In contrast, LLM-QE (w/o Answer Reward) achieves the lowest PCC score among all models, which shows that the information beyond the user intention is also incorporated in the expansion results. The additional information misleads the well-trained dense retriever, but benefits LLM-QE to produce more effective query expansions for Contriever.


% \input{Table/length_analysis}
% 计算关于doc和posi,ans的blue值,nq，hotpotqa
% 计算不同模型生成的doc的平均length
% 计算不同模型的PCC系数





