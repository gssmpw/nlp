\section{Conclusion}
This paper presents a novel query expansion framework, LLM-QE, which optimizes LLM-generated query expansions to align with the ranking preferences of dense retrievers. By incorporating both rank-based and answer-based reward models, LLM-QE effectively enhances the quality of query expansions by mitigating the generation of noisy information and avoiding excessively long expansions. 
Experimental results demonstrate that LLM-QE consistently improves performance in both unsupervised and supervised training scenarios, providing insights into integrating query expansion into dense retrieval training.