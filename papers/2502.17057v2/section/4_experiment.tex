
\input{Table/dataset}
\section{Experimental Methodology}\label{sec:exp}
In this section, we introduce the datasets, evaluation metrics, baselines, and implementation details used in our experiments. More experimental details are shown in Appendix~\ref{app:experiment_detail}.

\textbf{Dataset.}
We utilize various datasets for training and evaluation. Data statistics are shown in Table~\ref{tab:dataset}.

\textit{Training.}
We use the publicly available E5 dataset~\cite{wang2024improving,springer2024repetition} to train both the LLM-QE and dense retrievers. We concentrate on English-based question answering tasks and collect a total of 808,740 queries. From this set, we randomly sample 100,000 queries to construct the DPO training data, while the remaining queries are used for contrastive training. During the DPO preference pair construction, we first prompt LLMs to generate expansion documents, filtering out queries where the expanded documents share low similarity with the query. This results in a final set of 30,000 queries.

\textit{Evaluation.}
We evaluate retrieval effectiveness using two retrieval benchmarks: MS MARCO \cite{bajaj2016ms} and BEIR \cite{thakur2021beir}, in both unsupervised and supervised settings.

\textbf{Evaluation Metrics.}
We use nDCG@10 as the evaluation metric. Statistical significance is tested using a permutation test with $p<0.05$.

\textbf{Baselines.} We compare our LLM-QE model with three unsupervised retrieval models and five query expansion baseline models.
% —

Three unsupervised retrieval models—BM25~\cite{robertson2009probabilistic}, CoCondenser~\cite{gao2022unsupervised}, and Contriever~\cite{izacard2021unsupervised}—are evaluated in the experiments. Among these, Contriever serves as our primary baseline retrieval model, as it is used as the backbone model to assess the query expansion performance of LLM-QE. Additionally, we compare LLM-QE with Contriever in a supervised setting using the same training dataset.

For query expansion, we benchmark against five methods: Pseudo-Relevance Feedback (PRF), Q2Q, Q2E, Q2C, and Q2D. PRF is specifically implemented following the approach in~\citet{yu2021improving}, which enhances query understanding by extracting keywords from query-related documents. The Q2Q, Q2E, Q2C, and Q2D methods~\cite{jagerman2023query,li2024can} expand the original query by prompting LLMs to generate query-related queries, keywords, chains-of-thought~\cite{wei2022chain}, and documents.


\textbf{Implementation Details.} 
For our query expansion model, we deploy the Meta-LLaMA-3-8B-Instruct~\cite{llama3modelcard} as the backbone for the query expansion generator. The batch size is set to 16, and the learning rate is set to $2e-5$. Optimization is performed using the AdamW optimizer. We employ LoRA~\cite{hu2022lora} to efficiently fine-tune the model for 2 epochs. The temperature for the construction of the DPO data varies across $\tau \in \{0.8, 0.9, 1.0, 1.1\}$, with each setting sampled eight times. For the dense retriever, we utilize Contriever~\cite{izacard2021unsupervised} as the backbone. During training, we set the batch size to 1,024 and the learning rate to $3e-5$, with the model trained for 3 epochs.
