\section{Introduction}
Dense retrievers~\cite{karpukhin2020dense,xiong2021approximate} encode both queries and documents into a shared embedding space, enabling efficient retrieval of relevant documents through the KNN search. Despite their effectiveness, these models often encounter challenges presented by the semantic gap between query terms and document content. To overcome this challenge, existing work has utilized query expansion techniques~\cite{abdul2004umass, robertson1976relevance} to enhance the performance of both unsupervised and supervised dense retrieval models~\cite{gao2023precise,yu2021improving}. By enriching the original query with additional terms, these techniques increase lexical overlap with relevant documents, effectively narrowing the semantic gap between queries and documents.
\input{Figure/intro_stru}


Recent developments in query expansion models have focused primarily on the Generative Relevance Feedback (GRF) method~\cite{mackie2023generative, claveau2021neural}, which leverages Large Language Models (LLMs) to expand queries with contextually relevant content. This approach employs LLMs to produce documents~\cite{wang2023query2doc} or Chain-of-Thought (CoT)~\cite{wei2022chain} that are relevant to the query, thereby significantly enriching the semantics of the original query. However, directly applying LLMs to generate query-related content can introduce irrelevant or distracting information due to the hallucinations inherent in LLMs~\cite{ji2023survey, xu2024hallucination}.


To address this limitation, several works have explored the self-consistency~\cite{narangself} of LLMs in query expansion. Specifically, they use the instruction to prompt LLMs to generate diverse query-related contents as expansion results and then average their representations to mitigate inconsistencies~\cite{gao2023precise}. Other works further incorporate feedback from retrievers to select higher-quality expansions. They cross-verify the LLM-generated documents and pseudo-relevant documents by measuring their similarity~\cite{jia2024mill}. Nevertheless, optimizing LLMs to generate more precise query expansion results by aligning the ranking preferences of both retrievers and LLMs remains under-explored.


In this paper, we propose \textbf{L}arge \textbf{L}anguage \textbf{M}odel-based \textbf{Q}uery \textbf{E}xpansion (\textbf{LLM-QE}), a novel framework that trains LLMs to align with the ranking preferences of both retrievers and LLMs, thus reducing hallucinations during the generation of expansion results. As shown in Figure~\ref{fig:pipline}, LLM-QE starts with an unsupervised dense retriever, then prompts LLMs to generate document-based query expansions~\cite{gao2023precise,wang2023query2doc}, and finally utilizes the Direct Preference Optimization (DPO) method~\cite{rafailov2023direct} to optimize the query expansion model. Additionally, LLM-QE considers the ranking preferences of both retrieval models and LLMs and designs a reward model that combines both rank-based and answer-based rewards. Specifically, the rank-based reward model treats the ground truth document as the query and re-ranks the LLM-generated documents to calculate the ranking score. Meanwhile, in the answer-based reward model, we prompt LLMs to generate an answer based on both the query and the ground truth document. Then the generated answer serves as a new query to calculate the ranking score among the expanded documents.


Our experiments on BEIR~\cite{thakur2021beir} demonstrate the effectiveness of LLM-QE, achieving more than 8\% and 5\% improvements in unsupervised and supervised settings, respectively. Further analysis reveals the crucial roles of both rank-based and answer-based rewards in training LLM-QE. The rank-based reward directly models the ranking preference of the dense retriever, encouraging LLMs to generate more semantically relevant content to match the ground truth documents. However, relying solely on rank-based reward usually results in longer expansions in order to win the rank-based reward during optimization. By incorporating the answer-based reward, the length of document-based query expansions is significantly reduced. This improvement stems from the fact that the answer-based reward helps LLMs better assess expansion results by evaluating their relevance to answer-related content.




% contribution:
% 提出一种新型的查询扩展框架，引入Rank-based和Answer-based的双重奖励模型共同优化了生成的查询扩展的相关性和自洽性。这种集成的方法确保扩展不仅相关，而且还与原始查询保持高度的语义一致性，从而提高检索准确性。
% LLM-QE模型在多个基准测试中表现出色，凸显了其在各种信息环境中的有效性。
% LLM-QE模型在无监督和有监督设置中都表现出色，使其能够适应不同的检索模型和训练环境。
