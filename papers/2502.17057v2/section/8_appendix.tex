% E5数据集介绍，数据集处理过程
% 基线模型介绍
\input{Table/prompt}
\input{Figure/improvement}
\section{Appendix}
\subsection{License}
The authors of 4 out of the 15 datasets in the BEIR benchmark (NFCorpus, FiQA-2018, Quora, Climate-Fever) and the authors of ELI5 in the E5 dataset do not report the dataset license in the paper or a repository. We summarize the licenses of the remaining datasets as follows.

MS MARCO (MIT License); FEVER, NQ, and DBPedia (CC BY-SA 3.0 license); ArguAna and Touché-2020 (CC BY 4.0 license); CQADupStack and TriviaQA (Apache License 2.0); SciFact (CC BY-NC 2.0 license); SCIDOCS (GNU General Public License v3.0); HotpotQA and SQuAD (CC BY-SA 4.0 license); TREC-COVID (Dataset License Agreement).

All these licenses and agreements permit the use of their data for academic purposes.

\subsection{Additional Experimental Details}\label{app:experiment_detail}
This subsection outlines the components of the training data and presents the prompt templates used in the experiments.


\textbf{Training Datasets.} Following the setup of \citet{wang2024improving}, we use the following datasets: ELI5 (sample ratio 0.1)~\cite{fan2019eli5}, HotpotQA~\cite{yang2018hotpotqa}, FEVER~\cite{thorne2018fever}, MS MARCO passage ranking (sample ratio 0.5) and document ranking (sample ratio 0.2)~\cite{bajaj2016ms}, NQ~\cite{karpukhin2020dense}, SQuAD~\cite{karpukhin2020dense}, and TriviaQA~\cite{karpukhin2020dense}. In total, we use 808,740 training examples.

\textbf{Prompt Templates.} Table~\ref{tab:prompt_template} lists all the prompts used in this paper. In each prompt, ``query'' refers to the input query for which query expansions are generated, while ``Related Document'' denotes the ground truth document relevant to the original query. We observe that, in general, the model tends to generate introductory phrases such as ``Here is a passage to answer the question:'' or ``Here is a list of keywords related to the query:''. Before using the model outputs as query expansions, we first filter out these introductory phrases to ensure cleaner and more precise expansion results.



\subsection{Query Expansion Quality of LLM-QE}\label{app:analysis}
This section evaluates the quality of query expansion of LLM-QE. As shown in Figure~\ref{fig:imp}, we randomly select 100 samples from each dataset to assess the improvement in retrieval performance before and after applying LLM-QE.

Overall, the evaluation results demonstrate that LLM-QE consistently improves retrieval performance in both unsupervised (Figure~\ref{fig:imp:unsupervised}) and supervised (Figure~\ref{fig:imp:supervised}) settings. However, for the MS MARCO dataset, LLM-QE demonstrates limited effectiveness in the supervised setting. This can be attributed to the fact that MS MARCO provides higher-quality training signals, allowing the dense retriever to learn sufficient matching signals from relevance labels. In contrast, LLM-QE leads to more substantial performance improvements on the NQ and HotpotQA datasets. This indicates that LLM-QE provides essential matching signals for dense retrievers, particularly in retrieval scenarios where high-quality training signals are scarce.


\subsection{Case Study}\label{app:case_study}
\input{Table/case_study}
To further demonstrate the effectiveness of LLM-QE, we conduct a case study by randomly sampling a query from the evaluation dataset. We then compare retrieval performance using the raw queries, expanded queries by vanilla LLM, and expanded queries by LLM-QE.

As shown in Table~\ref{tab:case_study}, query expansion significantly improves retrieval performance compared to using the raw query. Both vanilla LLM and LLM-QE generate expansions that include key phrases, such as ``temperature'', ``humidity'', and ``coronavirus'', which provide crucial signals for document matching. However, vanilla LLM produces inconsistent results, including conflicting claims about temperature ranges and virus survival conditions. In contrast, LLM-QE generates expansions that are more semantically aligned with the golden passage, such as ``the virus may thrive in cooler and more humid environments, which can facilitate its transmission''. This further demonstrates the effectiveness of LLM-QE in improving query expansion by aligning with the ranking preferences of both LLMs and retrievers.

