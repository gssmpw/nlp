\section{Related Work}
Dense retrievers~\cite{karpukhin2020dense,xiong2021approximate,izacard2021unsupervised,Yu2021FewShotCD,xiong2021dense,li2021more} have demonstrated superior ranking performance by conducting semantic matching between queries and documents, which helps overcome the problem of vocabulary mismatch~\cite{belkin1982ask}. These models typically leverage Pre-trained Language Models (PLMs) to encode both queries and documents into a shared embedding space, which significantly enhances retrieval effectiveness. To further optimize this embedding space, dense retrievers are often trained contrastively using relevance signals between queries and documents~\cite{karpukhin2020dense,zhan2021optimizing}. Some studies have also developed zero-shot dense retrievers by training on weak supervision data~\cite{xie2023unsupervised} or leveraging Large Language Models (LLMs) for query expansion and reformulation~\cite{gao2022unsupervised}.


Query expansion is a long-standing research direction, originally proposed to rewrite queries and improve the retrieval accuracy of exact matching-based retrievers, such as BM25~\cite{robertson2009probabilistic}. Early query expansion methods primarily aim to bridge the lexical gap between queries and documents~\cite{carpineto2012survey,rocchio1971relevance} by expanding queries with knowledge bases~\cite{bhogal2007review,qiu1993concept,voorhees1994query} or Pseudo-Relevance Feedback (PRF)~\cite{amati2002probabilistic,robertson1990term,rocchio1971relevance}. These PRF-based methods have proven their effectiveness in enhancing reranking techniques~\cite{li2018nprf,ai2018learning,yu2021pgt} and improving dense retrievers by learning better query representations~\cite{yu2021improving}. 


Recent research of query expansion focuses on Generative Relevance Feedback (GRF) methods, which utilize generation models to directly produce query expansion results~\cite{mackie2023generative, claveau2021neural, wang2023generative, jagerman2023query, mackie2023gprf}. These methods often employ LLMs to generate query-related documents~\cite{wang2023query2doc, jagerman2023query, gao2023precise}, leverage Chain-of-Thought (CoT) reasoning results~\cite{wei2022chain, jagerman2023query, trivedi2023interleaving}, or utilize specific keywords~\cite{li2024can, jagerman2023query} to expand queries, thereby enhancing the ranking capabilities of lexical matching based retrieval models~\cite{jagerman2023query, wang2023query2doc}, dense retrieval models~\cite{wang2023query2doc}, and reranking models~\cite{li2024can}.


Although these studies have demonstrated their advantages in improving retrieval models, directly using LLMs for query expansion and reformulation poses potential risks due to LLM hallucinations~\cite{shuster2021retrieval,huang2023survey}. To mitigate this issue, some works use different instructions to reduce inconsistency in query reformulation~\cite{gao2023precise} or leverage rephrased questions as demonstrations to guide LLMs in generating more effective query expansion results~\cite{koo2024optimizing}. RaFe~\cite{mao2024rafe} further enhances the Retrieval-Augmented Generation (RAG) performance by taking reranking scores as training signals to optimize the query rewriting model. In contrast to these approaches, LLM-QE focuses on modeling the ranking preferences of both retrievers and LLMs, rewarding LLMs for generating more effective expansion results, and exploring its potential to build both unsupervised and supervised dense retrieval models.

