
\section{Methodology}
As illustrated in Figure~\ref{fig:model_pipline}, this section describes our LLM based query expansion method, LLM-QE. First, we introduce our query-expanded dense retrieval method (Sec.~\ref{model:preliminary}). Next, we use the DPO method to optimize LLMs for query expansion (Sec.~\ref{model:dpo-training}). Finally, we build a reward model to train LLM-QE by modeling the ranking preferences of both dense retrievers and LLMs (Sec.~\ref{model:reward-model}).



% 3.1
\subsection{Query Expanded Dense Retrieval}\label{model:preliminary}

Given a query $q$ and a document collection $\mathcal{D} = \{d_\text{1}, ..., d_\text{k}\}$, dense retrieval models~\cite{karpukhin2020dense,xiong2021approximate,cocondenser} first encode the query $q$ and the $i$-th document $d_i$ into vector representations $\vec{q}$ and $\vec{d}_i$ using PLMs, such as BERT~\cite{devlin2019bert}:
\begin{equation}\label{eq:encode}
\small
    \vec{q} = \text{BERT}_q (q),  \quad  \vec{d}_i = \text{BERT}_d (d_i).
\end{equation}
Then the score $S(q, d_i)$ is calculated to estimate the relevance between $q$ and $d_i$, followed by a KNN search~\cite{douze2024faiss} to retrieve the top-ranked documents to satisfy the user needs. 

Different from existing dense retrieval models, we introduce our query expanded dense retrieval method. Firstly, we prompt the LLM ($\mathcal{M}$) to produce a document-based query expansion:
\begin{equation}
\small
d^{\text{exp}} = \mathcal{M}(\text{Instruct}_\text{q2d}, q),
\end{equation}
where $\text{Instruct}_\text{q2d}$ denotes the instruction that asks LLMs to generate document-like query expansion outcomes~\cite{jagerman2023query}.
The representation of expanded query $\vec{q}^{\; \text{exp}}$ is calculated by averaging the representations of the raw query $q$ and the document-based query expansion $d^{\text{exp}}$ to enhance both unsupervised and supervised dense retrievers:
% $q$ and $d^{\text{exp}}$:
\begin{equation}\label{eq:qexp}
\small
    \vec{q}^{\; \text{exp}} = \frac{\vec{q} + \vec{d}^{\; \text{exp}}}{2}.
\end{equation}


\input{Figure/model}



\textbf{Unsupervised Dense Retrieval.} The unsupervised dense retrieval models, such as Contriever~\cite{izacard2021unsupervised}, are pre-trained to match text segments that share the same semantic information~\cite{fang2020cert,wu2020clear} and do not use the query-document relevance label during training~\cite{bajaj2016ms}.


Following~\citet{gao2023precise}, we can directly use Eq.~\ref{eq:qexp} to represent the expanded query by incorporating the information of raw query $q$ and the document-based query expansion $d^{\text{exp}}$. Then the relevance score $S(q, d_i)$ between the query $q$ and the candidate document $d_i$ can be calculated:
\begin{equation}
\small
    S(q, d_i) = \text{sim} (\vec{q}^{\; \text{exp}}, \vec{d}_i),
\end{equation}
where $\text{sim} (\cdot)$ is the similarity estimation function that conducts the dot product operation.

\textbf{Supervised Dense Retrieval.} Then we describe the details of training and inference processes in the supervised dense retrieval scenario.

\textit{Training.} During training the dense retriever augmented with query expansions, we first regard the expansion result $d^{\text{exp}}$ as the query and then calculate the similarity score $S(d^{\text{exp}}, d_i)$ between $d^{\text{exp}}$ and $d_i$:
\begin{equation}
\small
    S(d^{\text{exp}}, d_i) = \text{sim} (\vec{d}^{\; \text{exp}}, \vec{d}_i).
\end{equation}
Then we can contrastively train the encoder to learn matching signals from both $d^{\text{exp}}$ and the query-related document $d_*$ using the training loss $\mathcal{L}_\text{DR}$:
\begin{equation}
\small
 \mathcal{L}_\text{DR} = -\log\frac{e^{S(d^{\text{exp}},d_*)}}
    {e^{S(d^{\text{exp}},d_*)} + \sum_{d_-\in \mathcal{D}^-}{e^{S(d^{\text{exp}},d_-)}}},
\end{equation}
where $\mathcal{D}^-$ represents the set of negative documents, which are sampled from in-batch negatives~\cite{karpukhin2020dense}. 

\textit{Inference.} During retrieval, we can also represent the expanded query using Eq.~\ref{eq:qexp} and then calculate the relevance score:
\begin{equation}
\small
    S(q, d_i) = \text{sim} (\vec{q}^{\; \text{exp}}, \vec{d}_i).
\end{equation}




% 3.2
\subsection{Expanding Queries by Training LLMs with Preference Optimization}\label{model:dpo-training}
Existing query expansion methods~\cite{gao2023precise,li2024can} typically focus on directly prompting LLMs to generate various expansion results to mitigate hallucinations~\cite{brown2020language,thoppilan2022lamda}. To obtain more tailored and query-specific expansion results, we fine-tune the LLM to align with ranking preferences using the Direct Preference Optimization (DPO) method~\cite{amini2024direct}.

First, we prompt the LLM to generate multiple expansion documents by adjusting the temperature $\tau$ during sampling:
\begin{equation}
\small
d^{\text{exp}}_i \sim \mathcal{M}(\text{Instruct}_\text{q2d}, q).
\end{equation}
Thus, we can collect $k$ document-based query expansions $\mathcal{D}^q = \{d^{\text{exp}}_1, d^{\text{exp}}_2, \dots, d^{\text{exp}}_k\}$. 
Then we follow the DPO method to optimize the LLM ($\mathcal{M}$) using the loss function $\mathcal{L}(\mathcal{M}; \mathcal{M}^\text{Ref})$:
\begin{equation}
\small % 设置字体大小
\begin{aligned}
\label{eq:dpo}
& \mathcal{L}(\mathcal{M}; \mathcal{M}^\text{Ref}) = 
- \mathbb{E}_{(q, d^{\text{exp}}_+,d^{\text{exp}}_-) \sim \mathcal{P}} \Big[ \log \sigma \Big( \\
& \beta \log \frac{\mathcal{M}(d^{\text{exp}}_+ \mid q)}{\mathcal{M}^\text{Ref}(d^{\text{exp}}_+ \mid q)} - 
\beta \log \frac{\mathcal{M}(d^{\text{exp}}_- \mid q)}{\mathcal{M}^\text{Ref}(d^{\text{exp}}_- \mid q)} \Big) \Big],
\end{aligned}
\end{equation}
where $\sigma$ is the Sigmoid function and $\beta$ is a hyperparameter that controls the strength of the regulation from the reference model $\mathcal{M}^\text{Ref}$. $\mathcal{M}^\text{Ref}$ is frozen during DPO training. $\mathcal{P}$ is the dataset used for DPO training, which contains the triple ($q$, $d^{\text{exp}}_+$, $d^{\text{exp}}_-$). $d^{\text{exp}}_+$ and $d^{\text{exp}}_-$ are positive and negative responses, which are sampled from the document-based query expansions $\mathcal{D}^q$:
\begin{equation}
\small
    R(d^{\text{exp}}_+) > R(d^{\text{exp}}_-),
\end{equation}
where $R(d^{\text{exp}})$ is the reward model that is used to estimate the ranking preference of the document-based query expansion $d^{\text{exp}}$. The details of the ranking preference modeling are introduced in Sec.~\ref{model:reward-model}.

% 3.3
\subsection{Modeling Ranking Preferences for Rewarding Query Expansion Models}\label{model:reward-model}
For the given query $q$, the quality of the $i$-th document-based query expansion $d^{\text{exp}}_i$ is evaluated using the reward $R(d^{\text{exp}}_i)$ defined as:
\begin{equation}
\small
R(d^{\text{exp}}_i) = R_{\text{rank}}(d^{\text{exp}}_i) + R_{\text{ans}}(d^{\text{exp}}_i),
\end{equation}
where $d^{\text{exp}}_i \in \mathcal{D}^q$. $R_{\text{rank}}( d^{\text{exp}}_i)$ and $R_{\text{ans}}(d^{\text{exp}}_i)$ represent the rank-based reward score and the answer-based reward score, respectively. These scores are combined to model the ranking preference by capturing the relevance preference of the dense retriever and estimating the consistency with the question answering model.



\textbf{Rank-based Reward.} To assess the quality of the document-based query expansion $d^\text{exp}_i$, the most straightforward approach is to use a ranking score. Specifically, we calculate the Mean Reciprocal Rank (MRR) score by treating the ground truth document $d_*$ as the query and ranking the document-based query expansions $\mathcal{D}^q$:
\begin{equation}
\small
R_{\text{rank}}(d^{\text{exp}}_i) = \frac{1}{\text{Rank}(d_*, d^{\text{exp}}_i)},
\end{equation}
where $\text{Rank}(d_*, d^{\text{exp}}_i)$ represents the rank of $d^{\text{exp}}_i$ based on the relevant score $\text{sim}(d_*, d^{\text{exp}}_i)$. A higher reward score $R_{\text{rank}}(d^{\text{exp}}_i)$ indicates that the document-based query expansion $d^{\text{exp}}_i$ is more similar to the ground truth document $d_*$. 


\textbf{Answer-based Reward.} While the rank-based reward modeling accurately captures the preference of dense retrievers, it often leads to biases and fairness issues in reward modeling~\cite{dai2024bias}. Thus, relying solely on rank-based reward may cause the dense retrieval model to overfit to the inherent biases of dense retrievers. To address this challenge, we propose an answer-based reward modeling approach, which leverages the self-consistency of LLMs to calibrate the reward.

Specifically, we first instruct the LLM to generate an answer $y$ based on the given query $q$ and the ground truth document $d_*$:
\begin{equation}
\small
y = \mathcal{M}(\text{Instruct}_\text{q2a}, q, d_*),
\end{equation}
where $\text{Instruct}_\text{q2a}$ is the instruction that guides the LLM to generate a response to answer the query $q$. We then treat the LLM-generated answer $y$ as a query and rank the document-based query expansions $\mathcal{D}^q$ to compute the answer-based reward:  
\begin{equation}
\small
R_{\text{ans}}(d^{\text{exp}}_i) = \frac{1}{\text{Rank}(y, d^{\text{exp}}_i)},
\end{equation}
where $\text{Rank}(y, d^{\text{exp}}_i)$ denotes the rank of document $d^{\text{exp}}_i$ based on its relevance score $\text{sim}(y, d^{\text{exp}}_i)$.

Both $y$ and $d^{\text{exp}}_i$ are generated by the same LLM, using different instructions: $\text{Instruct}_\text{q2a}$ for generating the answer and $\text{Instruct}_\text{q2d}$ for generating the expansion documents. Higher-ranked documents indicate that $y$ and $d^{\text{exp}}_i$ share more semantic similarity, reflecting greater consistency and semantic agreement between $y$ and $d^{\text{exp}}_i$.






