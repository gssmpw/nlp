

We start by characterizing the algorithm's optimal selection of features in the static case where the human holds fixed beliefs $h$ about feature coefficients. %\sodelete{-- that is, the human does not learn or update their beliefs.} 
In this case, according to \cref{clm:huma-mse} in Section \ref{sec:model}, the algorithm will choose the subset of features minimizing:
% \begin{align*}
$$
MSE(A,h) = (c-\bar c)^2 + \big(\sum_{i \notin A} {a_i}^2 + \sum_{i \in A} (a_i - h_{i})^2 \big)
$$
% \end{align*}
With only one feature, by \cref{clm:huma-mse} we have:
% \begin{align*}
$MSE(\{1\},h) = (c-\bar c)^2 + (a_1 - h_{1})^2$, 
whereas if we do not use this feature, 
$MSE(\emptyset,h) = (c-\bar c)^2 + {a_1}^2$. 
% \end{align*}
Thus, selecting the feature reduces error whenever
$a_1^2 > (a_1 - h_1)^2$, which holds if $h_1^2 < 2a_1 h_1$, { or equivalently, $h_1 \in (0,2a_1)$}.  
Note that if the human and algorithm agree on the interpretation of the feature $(a_1 = h_1)$, then this is always satisfied, and so the algorithm would always select the feature. If $a_1$ and $h_1$ have different signs (the human and algorithm completely disagree), then this is never satisfied. 
If they do have the same sign, the feature is useful only if the human's coefficient \( h_1 \) is not too large compared to the algorithm's coefficient \( a_1 \).
When $h_1$ is too large, it means that the human ``overshoots,'' i.e., overuses the feature and ends up too far on the other side of the truth. %\sodelete{, thus increasing the error}. 
The factor of $2$ limits overshooting to keep the feature beneficial to the human.


In the single feature case, we saw that the choice of whether to select the feature or not depends on the value of $MSE(\{1\},h) - MSE(\emptyset,h) = a_1^2-(a_1-h_1)^2$. We find that this is a useful quantity for choosing which features to select in the general case, and refer to it as the {\em value} of feature $1$. As it turns out, this quantity greatly simplifies the problem of computing an optimal subset in the general case. 
More generally, 

\begin{definition} \label{dfn:value-static}
The {\em value} of a set $A$ of features $V(A,h) = MSE(\emptyset,h) - MSE(A,h)$ is the improvement in loss from using the features in $A$ compared to not using any feature. 
\end{definition}




Since for a given problem instance $MSE(\emptyset, h)$ is fixed, to minimize the objective function, the algorithm should select a set $A^*$ of up to $k$ features with the maximum value of $V(A^*,h)$.


\begin{lemma} \label{lem:value-static}
The value of a set of features $A\subseteq[n]$ satisfies $V(A,h) = \sum_{i\in A} V(\{i\},h)$.
%The value of a set of features $A$ is the sum of values of the features in $A$. That is:  $V(A,h) = \sum_{i\in A} V(\{i\},h)$.
\end{lemma}
\begin{proof}
%Note that,
% \begin{equation}  %\label{eq:value-static}
% \begin{aligned}
\begin{align}
 V(A,h) &= MSE(\emptyset,h) - MSE(A,h)  \notag \\
 &=
(c-\bar c)^2 + \sum_{i=1}^n {a_i}^2  - \big((c-\bar c)^2 + \big(\sum_{i \notin A} {a_i}^2 + \sum_{i \in A} (a_i - h_{i})^2 \big)\big) \notag  \\
&=\sum_{i\in A} a_i^2
- \sum_{i \in A} (a_i - h_i)^2 = \sum_{i\in A} 2a_i h_i-h_i^2  \label{eq:value-static} \\
&= \sum_{i \in A}  
V(\{i\}, h)  \notag
% \end{aligned}
% \end{equation}
\end{align}
\end{proof}


It is instructive to take a more careful look at 
Equation \eqref{eq:value-static} that the algorithm aims to maximize. 
The contribution of each feature $i \in A$ to the value of a set $A$ is composed of two parts: 
(1) ${a_i}^2$, which captures the informativeness\footnote{The coefficient of a feature captures the feature's importance and variance; recall that features are standardized, and thus coefficients are scaled by the standard deviation, see Lemma \ref{thm:standardized-ftrs} in Appendix \ref{app:general}.} of the feature; (2) minus $(a_i - h_{i,t})^2$, which captures the divergence between the human's and the algorithm's beliefs about the feature. The tension between these two terms is at the heart of this paper. 


Lemma \ref{lem:value-static} implies the following corollary for multiple  features. 
\begin{corollary}
    The algorithm only selects features $i$ with a positive value ( i.e., $h_i^2 < 2 a_i h_i$).  
\end{corollary}

Lemma \ref{lem:value-static} also gives rise to a simple and efficient algorithm to compute an optimal subset of features that the algorithm should select. We first compute the value for each feature $i \in [n]$, as $V(\{i\},h) = 2 a_i  h_i - h_i^2$. Then, we sort features by %descending order of 
their values and add features to the feature selection set by descending order until reaching the budget of $k$ features or there are no more features with positive values. Thus, we establish that:

\begin{proposition} \label{thm:complexity-static}
An optimal feature selection  $A^* \subseteq [n]$ can be computed in  $n log(n)$ time.
\end{proposition}




