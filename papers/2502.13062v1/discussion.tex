
In this paper, we formally model human decision-making with algorithmic assistance when the human is learning through repeated interactions, and study optimal algorithmic strategies in this context. 
We identify two fundamental tradeoffs in the space of AI-assisted decision making. The first tradeoff, ``informativeness vs. divergence,''  applies to any AI-assisted decision making setting in which the algorithm is designed to choose information for a human's use. %also if no learning is involved.
This tradeoff is amplified in repeated interactions where the human learns from experience. In this case, the algorithm  that determines what the human will learn, faces a second tradeoff between ``fixed vs. growth mindset'' --  optimize with respect to the human's current knowledge or provide them with opportunities for learning, which may take longer but lead to better long-term performance.
%
We propose a stylized model that unravels how time preferences and the human's ability to learn influence the fixed vs. growth mindset tradeoff. Our results highlight the importance of modeling the human decision-making process and incorporating human learning when designing algorithms to assist decision-makers.


In our modeling, we chose assumptions that are both simple and widely used in the literature, yet sufficient to reveal  fundamental principles in the interaction between algorithmic assistance and human learning. 
For example, we assumed that the variables in linear regression {are drawn from} independent distributions -- a common assumption in learning over time literature (e.g., bandit problems and online learning in general \cite{slivkins2019introduction}). As future work, it would be interesting to explore correlations between variables or decision instances over time. 
Exploring alternative performance metrics beyond mean squared error and learning models beyond linear regression is also of interest. For example, when performance metrics are discontinuous, such as in binary classification problems, algorithmic decisions may aim to %subtly 
shift the human's prediction to cross a decision threshold, potentially raising ethical considerations regarding manipulation.
An additional direction is to study a human decision maker who recognizes that the algorithm is optimizing and might not provide the features they expect. In this case, the human might act strategically according to their beliefs, leading to a game setting. 



We believe that the phenomena we identify extend beyond our specific model, capturing fundamental principles of human-algorithm interactions. These insights lay the groundwork for further research, advancing our understanding of how algorithmic assistance interacts with human learning.
