In this section, we provide additional details about the model introduced in Section \ref{sec:intro}. 
Recall that we consider a human tasked with predicting the outcome of a variable $y$. The true outcome is given by a linear function of a set of $n$ features $x = \{x_1, ..., x_n\}$,  such that $y = c + \sum_{i=1}^n a_i x_i$, where the coefficients $a_i$ are non-zero. %and distinct. %(i.e., for all $i,j$, $a_i \neq a_j$).
The features $x_i$ are independent random variables drawn from distributions $F_i$ with known means and finite standard deviations. Without loss of generality, throughout our analysis we assume the features are standardized (such that they have zero mean and unit variance; see Appendix \ref{app:general}). The human and the algorithm interact repeatedly as described in Section \ref{sec:intro}.
%For convenience, 
We divide the next discussion into two perspectives: the human's perspective and the algorithm's perspective.


\subsection{The Human} \label{sec:model-human} At each time step $t$, the human observes the realization of the features $A_t$ that the algorithm selected and makes a prediction $\bar c + \sum_{i \in A} h_{i,t} x_i$ (predicting the mean of zero for any unobserved features). This minimizes the Mean Squared Error (MSE) from the human's perspective. As the MSE is a function of $A_t$ and the human's coefficient vector $h_t$, we denote it by $MSE(A_t,h_t)$ and get:


\begin{claim} \label{clm:huma-mse}
    $MSE(A_t,h_t) = (c-\bar c)^2 + \big(\sum_{i \notin A_t} {a_i}^2 + \sum_{i \in A_t} (a_i - h_{i,t})^2 \big)$
\end{claim}


\begin{proof}
    \begin{align*}
MSE(A_t,h_t) &=\mathbb{E}[\big(c + \sum_{i=1}^n a_i x_i - \bar{c} - \sum_{i \in A_t} h_{i,t} x_i\big)^2] \\
%&= (c-\bar c)^2 + \mathbb{E}[\big(\sum_{i=1}^n a_i x_i - \sum_{i \in A_t} h_{i,t} x_i\big)^2] \\
&= (c-\bar c)^2 + \mathbb{E}[(\sum_{i=1}^n a_i x_i)^2] - \mathbb{E}[(\sum_{i \in A_t} h_{i,t} x_i)^2] 
\end{align*}
Observe that
\begin{align*}
 \mathbb{E}[(\sum_{i \in A_t} h_{i,t} x_i)^2] = \sum_{i \in A_t} \sum_{j \in A_t} \mathbb{E}[h_{i,t} h_{j,t} x_i x_j] 
\end{align*}
Since features are independent we have that for $j\neq i$, $\mathbb{E}[x_ix_j]=0$ and since the variance is normalized to $1$, we have that $\mathbb{E}[x_i^2] =1$. Hence, $\mathbb{E}[(\sum_{i \in A_t} h_{i,t} x_i)^2] = \sum_{i \in A_t} h_{i,t}^2$. Similarly, $\mathbb{E}[(\sum_{i=1}^n a_i x_i)^2] = \sum_{i=1}^n a_i^2$. Putting this together, we get that:
\begin{align} \label{eq:MSE}
    MSE(A_t,h_t) = (c-\bar c)^2 + \big(\sum_{i \notin A_t} {a_i}^2 + \sum_{i \in A_t} (a_i - h_{i,t})^2 \big)
\end{align}
as required.
\end{proof}



Recall that after the human observes a realization of a feature, they learn %about the true coefficient 
and update their coefficient of the feature according to some learning rule. Basic properties that are natural in a learning setting include: 
\begin{itemize}
       \item Initial beliefs: At the beginning of the interaction, the human starts with some initial beliefs.
    % \item Improvement with experience: With added evidence about a variable,  
    % the human's beliefs grows closer to the true value of the variable. 
    \item Improvement with experience: 
    With additional observations, the human's beliefs become closer to the true values.
    \item Asymptotic learning: with infinite observations, the human's beliefs converge to the truth.
\end{itemize}

Formally, in our context, $h_{i,0}$ denotes the human's initial belief about feature $i$ and $\fs{i}{t}$ denotes the number of times that $i$ was selected until time $t$.
The human's beliefs at time $t$, $h_{i,t}$, is a function of $\fs{i}{t}$. 
The sequence $|h_{i,t} - a_i|$ is decreasing in $\fs{i}{t}$, and $\lim_{\fs{i}{t} \rightarrow \infty}  |h_{i,t_i} - a_i| = 0$. 
Note that since the variables are independent it is reasonable to assume that the learning process is also  independent for each variable and hence we make this assumption.


The following definition of $\phi$-convergence of learning functions captures the above properties. 
\begin{definition} \label{def:convergence}
	Let $\phi: \mathbb{N} \to [0,1]$ be a monotone decreasing function with $\phi(0)=1$, $\lim_{m \rightarrow \infty} \phi(m) = 0$. 
	%Let $\fs{i}{t}$ denote the number of times that the human observed feature $i$, before time $t$. 
    A learning dynamic is {\em $\phi$-convergent}, if for every $t$:
$	%\begin{align*}
		(a_i-h_{i,t})^2 = \phi(\fs{i}{t})\cdot(a_i-h_{i,0})^2.
$	%\end{align*}  
\end{definition}

In our analysis, we consider human learning in the general sense of $\phi$-convergence. This abstracts away the exact mechanism that leads to learning, i.e., what feedback the human receives and how they use this feedback to update their model. 


\subsection{The Algorithm}
The objective of the algorithm is determined by its designer who cares about minimizing the loss of the human's prediction. 
% In the medical domain, for example, this could be an insurance provider. 
The designer sets a budget \( 0 \leq k \leq n \) on the number of features that the human can use for prediction. The limitation to \( k \) features may arise, for example, from a cost associated with revealing features' values.

Given that we are dealing with an infinite stream of losses, it is natural to account for the timing of each loss and apply an appropriate discount. Here, 
we adopt the widely used exponential discounting approach, where future losses are consistently discounted according to a parameter \( \delta \in (0,1) \). 
%
Putting this together we have that the objective of the algorithm is to select a sequence of feature subsets $\seq{S}=(A_t)_{t=0}^\infty$ such that $|A_t|\leq k$ for every $t\geq 0$, that minimizes the discounted loss of the human:
\begin{align} \label{alg:objective-function}
 L(\seq{S}=(A_t)_{t=0}^\infty,\phi,h_0) &= \sum_{t=0}^\infty \delta^t  MSE(A_t,h_t) =  
 \sum_{t=0}^\infty \delta^t \Big( (c-\bar c)^2 + \big(\sum_{i \notin A_t} {a_i}^2 + \sum_{i \in A_t} (a_i - h_{i,t})^2 \big) \Big) 
 \notag \\
  &= \sum_{t=0}^\infty \delta^t \Big((c-\bar c)^2 + \big(\sum_{i \notin A_t} {a_i}^2 + \sum_{i \in A_t} \phi(\fs{i}{t})(a_i - h_{i,0})^2 \big)\Big)
\end{align}

As \( \delta \) approaches \( 1 \), the designer places greater emphasis on future losses, whereas smaller values of \( \delta \) indicate a stronger preference for minimizing immediate losses. Thus, \( \delta \) can be interpreted as a ``patience'' parameter. 
%Alternatively, we can view the process as having an 
Note that such an infinite-horizon discounted loss can also represent situations with an 
uncertain interaction length, where the interaction ends in each round with probability \( 1 - \delta \). In this case, an interaction occurring \( t \) steps in the future has a probability of \( \delta^t \) of taking place and is therefore discounted by that factor.

We are mainly interested in settings where the algorithm is, on average, more accurate than the human. Hence, we primarily analyze the case where the algorithm has accurate coefficients (i.e., \( a' = a \) and $c'=c$), and accurate estimates of the human's initial coefficients (\( h'_0 = h_0 \) and $\bar c' = \bar c$), and the \( \phi \) governing their learning dynamics ($\phi'=\phi$, where $\phi'$ is the algorithm's estimate of $\phi$). This means that the algorithm has all the required information to choose a sequence minimizing \( \sum_{t=0}^\infty \delta^t  MSE(A_t,h_t) \). 
To simplify notations, we omit the prime notation from the algorithm's estimates.
In Section \ref{sec:misspecification}, we consider algorithms that have inaccurate model of the human or of the ground truth.
%we move to a setting in which the algorithm no longer has an accurate model of the human or the ground truth. 
% We observe that the problem we consider—selecting a subset of features that the human will base their prediction on—\soedit{has some robustness} to errors in modeling the human and the ground truth.
As we will see, the structure of the problem -- where the algorithm does not make the prediction directly but instead selects the features on which the human will base their prediction -- limits the impact of errors in modeling both the human and the ground truth.


%%%%%%%%%%%%%%%%%%%% This paragraph was omitted from the EC submission due to space constraints. 
Note that the algorithm is not obligated to exhaust the budget of $k$ features. When the algorithm chooses not to select any features at all (i.e., $A = \emptyset$), the human still needs to make a prediction. In this case, since no additional information is available, the human makes the same prediction of $\hat{y}_h = \bar c$ for every instance of the problem, which represents their belief about the average of the predicted value.





 