We analyze the effect of errors in the algorithm's estimates of the ground-truth coefficients $a$, the human's coefficients $h$, and the convergence rate $\phi$ of the learning dynamic. Modeling errors can lead to incorrect feature selection, reducing overall value. To quantify this, we express these errors as the maximum possible error margin in value per feature. As we will see, it is important to distinguish between "overshoot" errors and ``undershoot'' errors, as these margins are asymmetric for few parameters. The following definition captures this formally, where $V(\{i\})$ is the
true value of feature $i$ depending on whether we are in the static setting or the learning setting, and $V'(\{i\})$ is the value as computed by the imperfect algorithm.

\begin{definition} \label{def:error-margins-app}
Let $\eps_i$ denote an upper bound on the magnitude of  error in some coefficient of feature $i$ (e.g., ground-truth coefficient or human's coefficient). The error margin of feature $i$'s value, $V(\{i\})$, is defined by two non-negative functions $\bar \xi_i(\eps_i)$ and $\xi_i(\eps_i)$ such that,
\begin{align*}
    V(\{i\})-\bar \xi_i(\eps_i) \leq V'(\{i\})  \leq  V(\{i\}) +\xi_i(\eps_i)
\end{align*}
\end{definition}

The error margins in estimating specific values can be used to compute the error margin in the algorithm's selection. The idea is to incorporate these margins into the algorithm's decision of whether to include feature $i$ or feature $j$. Specifically, let $A^*$ denote an optimal set, if for feature $i \in A^*$ and any feature $j \not \in A^*$, we have that $V(\{i\}) - V(\{j\}) \geq \bar{\xi}_i(\epsilon_i) + \xi_j(\epsilon_j)$, then the algorithm will also prefer $i$ over $j$, and the error will not affect the result. Else, the algorithm may choose $j$ instead of $i$, but the error from this choice will be bounded by $\xi_i(\epsilon_i) + \xi_j(\epsilon_j)$. 

Note that we do not assume anything about the structure of the error (e.g., the direction of the error or on which features the algorithm errs). 
Since the actual performance is affected only by the ranking of features by their values, increasing $\eps$ can have impact only in a collection of thresholds where ranking changes. That is, the change in value is discontinuous and in particular, it is a composition of step functions. %For example, Figure \ref{fig:misspecification-examples} depicts the value of the worst possible selection the algorithm could make, for an algorithm such that for all $i$, $|a'_i -a_i|\leq \eps$ and $h_i=h'_i$ as a function of $\epsilon$. 

Formally we prove the next proposition:
\begin{proposition}\label{prop-mis-gen-error-app}
 Let $A^*$ denote an optimal feature set and $A$ denote the feature set selected by a misspecified algorithm.  Let $\eps_i$ be a bound on the magnitude of the estimation error for some coefficient of feature $i$, then:
 \begin{align*}
     V(A^*)-V(A)  \leq  \sum_{i\in  A* \setminus A} \bar \xi_i(\eps_i) +\sum_{j\in  A \setminus A^*} \xi_j(\eps_j)
 \end{align*}
\end{proposition}
\begin{proof}
Note that, 
\begin{align*}
    V(A^*)-V(A) = \sum_{i\in A^* \setminus A} V(\{i\}) - \sum_{j\in A \setminus A^*} V(\{j\}) = 
    % \gndelete{\sum_{i\in  A^* \setminus A, j\in  A \setminus A^*}}
   {\sum_{\text{unique} (i,j), i\in  A^* \setminus A, j\in  A \setminus A^*}}
    V(\{i\}) - V(\{j\})
\end{align*}
Where the last step is a partition of the features in $A^* \setminus A$ and $A \setminus A^*$ to distinct pairs $i$ and $j$. If $|A^*|\neq|A|$ we can introduce some dummy features that have value $0$ (i.e., $a=a'=h=h'=0)$ to make the two sets equal without affecting anything else. For every pair $i \in A^* \setminus A$ and $ j \in A \setminus A^*$, we get
\begin{align*}
    V'(\{i\})-V'(\{j\}) \geq V\{i\} - \bar \xi_i(\eps_i) - (V(\{j\}) + \xi_j(\eps_j) ) = V(\{i\}) - V(\{j\}) - (\bar \xi_i(\eps_i) + \xi_j(\eps_j)) 
\end{align*}
Thus, if the difference in values is large, $V(\{i\}) - V(\{j\}) \geq \bar \xi_i(\eps_i) + \xi_j(\eps_j)$, the misspecified algorithm should also select feature $i$ instead of feature $j$. Since it did not, we know that the difference between the features is small and hence, $V(\{i\}) - V(\{j\}) \leq \bar \xi_i(\eps_i) + \xi_j(\eps_j)$, which completes the proof.
\end{proof}

In the remaining of this section we will consider different misspesifications and prove bounds on $\bar \xi_i(\eps_i)$ and $\xi_i(\eps_i)$. {For this section, we assume that $A$ is a set chosen by the algorithm and $A^*$ is an optimal set according to the true coefficients.}
By applying Proposition \ref{prop-mis-gen-error-app}, the error margins $\bar \xi_i(\eps_i)$ and $\xi_i(\eps_i)$ turn to general error bounds on the value of the subset/sequence that the algorithm selects. 



%\vspace{10pt} \noindent 
%\textbf{Misspecification in the non-learning setting:}
\subsection{Misspecification in the Non-Learning Setting}
Recall that when the human's beliefs are fixed, we have $V(\{i\},h) = 2a_ih_i-h_i^2$.
We first bound the error resulting from the algorithm misspecifying the ground-truth coefficients. As we will see, in this case the value's margin error is symmetric.

\begin{observation}
Consider a fixed human belief setting and feature $i$ such that $|a'_i-a_i|\leq \eps_i$ and $h'_i= h_i$ for every $i$, then $\bar \xi_i(\eps_i)=\xi_i(\eps_i) =  2\eps_i |h_i|$.
\end{observation}
\begin{proof}
Let $\eps' = |a'_i-a_i|$. Observe that,
    \begin{align*}
        V'(\{i\}) = 2a'_ih_i-h_i^2 = 2(a_i\pm \eps')h_i-h_i^2 = V(\{i\}) \pm 2\eps' h_i
    \end{align*}
 %   By observing that the term 
 %   we can always choose the sign of $\eps'$ such that the term $\eps' h_i$ is positive, 
    Hence, we get that: $V'(\{i\}) \geq V(\{i\}) - 2\eps'|h_i|$ and $V'(\{i\}) \leq V(\{i\}) + 2\eps'|h_i|$. Since both errors are increasing in $\eps'$ in their respective directions, we conclude that $\bar \xi_i(\eps_i)=\xi_i(\eps_i) =  2\eps |h_i|$ as required.
\end{proof}
By applying \cref{prop-mis-gen-error-app} where for every $i$, we have $\eps_i=\eps$, we get:
\begin{corollary}
In the non-learning setting, if for every $i$, $|a'_i-a_i|\leq \eps$ and $h'_i=h$, then $V(A^*)-V(A) \leq 2\eps \left( \sum_{i \in A^* \setminus A} |h_i| +  \sum_{j \in A \setminus A^*} |h_j| \right)$
\end{corollary}

Next, we turn to bound the error resulting from the algorithm misspecifying the human's coefficients.
\begin{observation} \label{obs-err-h-static}
Consider a fixed human belief setting and feature $i$ such that $|h'_i-h_i|\leq \eps_i$, $a'_i= a_i$, and $\eps \leq |h_i-a_i|$, then $\bar \xi_i(\eps_i)=2\eps_i|h_i-a_i|+(\eps_i)^2$ and $\xi_i(\eps_i)=2\eps_i|h_i-a_i|-(\eps_i)^2$.
\end{observation}
\begin{proof}
Let $\eps' = |h'_i-h_i|$. Observe that,
\begin{align*}
V'(\{i\}) &= 2a_ih'_i-(h'_i)^2 = 2a_i(h_i \pm \eps')-(h_i\pm \eps')^2 = V(\{i\}) \pm 2a\eps' \mp 2\eps' h_i - (\eps')^2 \\
&= V(\{i\}) \pm 2\eps'|h_i-a_i|-(\eps')^2
\end{align*}
Hence, $V'(\{i\}) \geq V(\{i\}) -(2\eps'|h_i-a_i|+(\eps')^2)$ as this error margin increases with $\eps'$, we get that $\bar \xi_i(\eps_i)=2\eps_i|h_i-a_i|+(\eps_i)^2$. Now, observe that, $V'(\{i\}) \leq V(\{i\}) + \max\{2\eps'|h_i-a_i|-(\eps')^2,0\}$. This error margin is increasing up to $\eps' \leq |h_i-a_i|$. We assume that this holds as this simply means that the error of the algorithm is smaller than the error of the human. We conclude that $\xi_i(\eps_i)=2\eps_i|h_i-a_i|-(\eps_i)^2$.
\end{proof}
By applying \cref{prop-mis-gen-error-app} where for every $i$, we have $\eps_i=\eps$, we get:
\begin{corollary}
In the non-learning setting, if for every feature $i$, $|h'_i-h_i|\leq \eps$, $\eps \leq |h_i-a_i|$ and $a'_i= a_i$, then for $|A|=|A^*|$ we have that $V(A^*)-V(A) \leq   2\eps \left( \sum_{i \in A^* \setminus A} |h_i-a_i| +  \sum_{j \in A \setminus A^*} |h_j-a_j| \right)$. If $|A|\neq|A^*|$, the bound includes additional terms as specified by \cref{prop-mis-gen-error-app}.
\end{corollary}

It is interesting to note that when the human agrees with the algorithm on the sign of the coefficients, the error for misspecifying the ground-truth coefficient can be substantially larger than the error for misspecifying the human coefficients. The situation is reversed when the coefficients of the human and the ground truth have  opposite signs. 

\subsection{Misspecification in the Learning Setting}
%\xhdr{Misspecification in the learning setting:}
% For the learning setting we have the following expression for the different between $i$ and $j$:
Recall that in this setting:
 \begin{align*} 
 V_{\delta,\phi}(\infsta{\{i\}}) =   \frac{1}{1-\delta}a_i^2 - \sum_{t=0}^\infty \delta^t \phi(t)(a_i-h_{i,0})^2
 \end{align*}

 We first consider an algorithm that misspesifies the human's coefficients and observe that the error margins are {direct generalization} of the error we have seen for the analogous error in the non-learning setting.
 \begin{observation} \label{obs-err-h-learning}
Consider the learning setting such that $\phi'=\phi$, and some feature $i$ such that $|h'_i-h_i|\leq \eps_i$, $a'_i= a_i$, and $\eps\leq|h_i-a_i|$, then $\bar \xi_i(\eps_i)= \sum_{t=0}^\infty \delta^t \phi(t)\big(2\eps_i \cdot (|a_i-h_{i,0}|) +\eps_i^2 \big)$ and 
$\xi_i(\eps_i)  = \sum_{t=0}^\infty \delta^t \phi(t)\big(\eps^2 +  2\eps_i \cdot (|a_i-h_{i,0}|) -\eps_i^2 \big)$.
\end{observation}
\begin{proof}
    Let $\eps'=|h'_i-h_i|$. Observe that,
    \begin{align*}
   V'_{\delta,\phi}(\infsta{\{i\}}) &=   \frac{1}{1-\delta}a_i^2 - \sum_{t=0}^\infty \delta^t \phi(t)(a_i-h'_{i,0})^2 \\
   &= \frac{1}{1-\delta}a_i^2 - \sum_{t=0}^\infty \delta^t \phi(t)(a_i-h_{i,0}\pm \eps')^2 \\
   &= \frac{1}{1-\delta}a_i^2 - \sum_{t=0}^\infty \delta^t \phi(t)(a_i-h_{i,0})^2 -  \sum_{t=0}^\infty \delta^t \phi(t)\big((\eps')^2 \pm  2\eps' \cdot (a_i-h_{i,0}) \big) \\
   &= V_{\delta,\phi}(\infsta{\{i\}}) -  \sum_{t=0}^\infty \delta^t \phi(t)\big((\eps')^2 \pm  2\eps' \cdot (a_i-h_{i,0}) \big)
\end{align*}
We skip the rest of the analysis as it is the same as in \cref{obs-err-h-static}.


\end{proof}
By applying \cref{prop-mis-gen-error-app} where for every $i$, we have $\eps_i=\eps$, we get:
\begin{corollary}
If $\phi'=\phi$ and for every feature $i$, $|h'_i-h_i|\leq \eps$,  $\eps \leq |h_i-a_i|$ and $a'_i= a_i$, then for $|A|=|A^*|$ we have that,
\begin{align*}
    V_{\delta,\phi}(\infsta{A^*})-V_{\delta,\phi}(\infsta{A})) \leq    2\eps\sum_{t=0}^\infty \delta^t \left( \sum_{i \in A^* \setminus A} |h_i-a_i| +  \sum_{j \in A \setminus A^*} |h_j-a_j| \right)
\end{align*}
\end{corollary}

Next, we consider an imperfect algorithm with inaccurate ground-truth coefficients. In the learning setting, the effect of such errors is more nuanced than in the static setting. First, they {capture} the informativeness of the {features}. Second, even when the algorithm initially has a correct estimate of the humans' coefficients, as the human learns based on the ground truth, the algorithm's estimate of the human's coefficients becomes inaccurate. As we will see, this leads to an interesting interplay between these two types of errors.

Our analysis can be viewed as an upper bound on the algorithm's error since, in practice, the algorithm is also likely to learn and improve its estimate of the ground-truth coefficients over time. The interaction in this case is significantly more complex, and we defer its detailed analysis to future work.

\begin{observation} \label{obse-err-a-learning}
Consider the learning setting such that $\phi'=\phi$, and some feature $i$ such that $|a'_i-a_i|\leq \eps_i$, $h'_i= h_i$ an $ \eps_i \leq \min\{|a_i|,|a_i-h_i|\}$, then,
\begin{itemize}
\item  $\xi_i(\eps_i) = \eps_i^2\big(\frac{1}{1-\delta}-\sum_{t=0}^\infty \delta^t \phi(t)\big) + 2\eps_i \big|\frac{1}{1-\delta} a_i - \sum_{t=0}^\infty \delta^t \phi(t)(a_i-h_{i,0})\big|$.
\item $\bar \xi_i(\eps_i) = \eps_i^2\big(\sum_{t=0}^\infty \delta^t \phi(t) - \frac{1}{1-\delta}\big) +2\eps_i \big|\frac{1}{1-\delta} a_i - \sum_{t=0}^\infty \delta^t \phi(t)(a_i-h_{i,0})\big|$.
\end{itemize}
\end{observation}
\begin{proof}
     Let $\eps'=|a'_i-a_i|$. Observe that,
\begin{align*}
   V'_{\delta,\phi}(\infsta{\{i\}}) &=   \frac{1}{1-\delta}(a'_i)^2 - \sum_{t=0}^\infty \delta^t \phi(t)(a'_i-h_{i,0})^2 \\
   &= \frac{1}{1-\delta}(a_i\pm \eps_i)^2 - \sum_{t=0}^\infty \delta^t \phi(t)(a_i-h_{i,0}\pm \eps')^2 \\
   &= \frac{1}{1-\delta}a_i^2 - \sum_{t=0}^\infty \delta^t \phi(t)(a_i-h_{i,0})^2 + \frac{1}{1-\delta}((\eps')^2 \pm 2a_i \eps') -  \sum_{t=0}^\infty \delta^t \phi(t)\big((\eps')^2 \pm  2\eps' \cdot (a_i-h_{i,0}) \big) \\
   &=V_{\delta,\phi}(\infsta{\{i\}})  + \frac{1}{1-\delta}(\eps_i^2 \pm 2a_i \eps') -  \sum_{t=0}^\infty \delta^t \phi(t)\big(\eps'^2 \pm  2\eps' \cdot (a_i-h_{i,0}) \big)\\
   &= V_{\delta,\phi}(\infsta{\{i\}})+ (\eps')^2(\frac{1}{1-\delta}-\sum_{t=0}^\infty \delta^t \phi(t)) \pm \frac{1}{1-\delta} (2a_i\eps')\mp \sum_{t=0}^\infty \delta^t \phi(t)2\eps'(a_i-h_{i.0})
\end{align*}
We get that:
\begin{align*}
    V'_{\delta,\phi}(\infsta{\{i\}}) &\geq V_{\delta,\phi}(\infsta{\{i\}})-\bigg(\eps'^2\big(\sum_{t=0}^\infty \delta^t \phi(t) - \frac{1}{1-\delta}\big) + \bigg|\frac{1}{1-\delta} (2a_i\eps') - \sum_{t=0}^\infty \delta^t \phi(t)2\eps'(a_i-h_{i,0})\bigg| \bigg) \\
 V'_{\delta,\phi}(\infsta{\{i\}}) &\leq V_{\delta,\phi}(\infsta{\{i\}})+ \eps'^2\bigg(\frac{1}{1-\delta}-\sum_{t=0}^\infty \delta^t \phi(t)\big) + \bigg|\frac{1}{1-\delta} (2a_i\eps') - \sum_{t=0}^\infty \delta^t \phi(t)2\eps'(a_i-h_{i,0})\bigg| \bigg)
\end{align*} 


Note that the first term in the lower bound is negative since for any choice of $\phi$ which is bounded by $1$, we have that $\frac{1}{1-\delta} \geq \sum_{t=0}^\infty \delta^t \phi(t)$. Hence, a-priori it is unclear that the error is increasing with $\eps'$. By taking a derivative with respect to $\eps'$ we get that,

$$ 2 \eps' ( \sum_{t=0}^\infty \delta^t \phi(t)- \frac{1}{1-\delta}) + 2\bigg|\frac{1}{1-\delta} a_i - \sum_{t=0}^\infty \delta^t \phi(t)\eps'(a_i-h_{i,0})\bigg|\geq 0 $$
Implying that:
\begin{align*}
    \eps' \leq \frac{\big|\frac{1}{1-\delta} a_i - \sum_{t=0}^\infty \delta^t \phi(t)\eps'(a_i-h_{i,0})\big|}{\frac{1}{1-\delta} -\sum_{t=0}^\infty \delta^t \phi(t)} \leq \min\{|a_i|,|a_i-h_i|\}
\end{align*}
 Hence, we conclude that,
 \begin{align*}
\bar \xi_i(\eps_i) = \eps'^2\big(\sum_{t=0}^\infty \delta^t \phi(t) - \frac{1}{1-\delta}\big) +2\eps' \big|\frac{1}{1-\delta} a_i - \sum_{t=0}^\infty \delta^t \phi(t)(a_i-h_{i,0})\big|
 \end{align*}
We get that the upper bound is increasing in $\eps$ since $\sum_{t=0}^\infty \delta^t \phi(t) - \frac{1}{1-\delta} \geq 0$ and thus,
\begin{align*}
\xi_i(\eps_i) = \eps'^2\big(\frac{1}{1-\delta}-\sum_{t=0}^\infty \delta^t \phi(t)\big) + 2\eps' \big|\frac{1}{1-\delta} a_i - \sum_{t=0}^\infty \delta^t \phi(t)(a_i-h_{i,0})\big|
\end{align*}
\end{proof}
By applying \cref{prop-mis-gen-error-app} where for every $i$, we have $\eps_i=\eps$, we get the following corollary: 
\begin{corollary}
%Let $A$ denote the {optimal} feature set chosen by an imperfect algorithm and $A^*$ the optimal set {according to true parameters}, {such that $|A|=|A^*|$}.
If $\phi'=\phi$ and for every feature $i$, $|a'_i-a_i|\leq \eps$, $ \eps \leq \min\{|a_i|,|a_i-h_i|\}$ and $h'_i= h_i$, then for $|A|=|A^*|$ we have that
    \begin{align*}
        V(A^*)-V(A) \leq   2\eps \sum_{i \in A^* \cup A \setminus A^* \cap A } \big|\frac{1}{1-\delta} a_i - \sum_{t=0}^\infty \delta^t \phi(t)(a_i-h_{i,0})\big|
    \end{align*}
\end{corollary}
It is interesting to further examine the error estimate we obtained. Note that if $a_i$ and $a_i - h_i$ have the same sign, the two types of errors partially cancel out, reducing the total error. However, if they have opposite signs, they compound, increasing the total error.
% \socomment{TODO: look more on the expression and see if we can write something more insightful.}


Lastly, we look at errors in estimating the 
human's learning dynamic $\phi$. There are various measures that can be used to specify a bound on the distance between the true function $\phi$ and the estimated function $\phi'$. In our case, since $\phi$ always appears in converging sums, it is useful to define the error $\eps$ in estimating %the convergence rate, 
$\phi$, given the patience parameter $\delta$, as the error in estimating the sum $\sum_{t=0}^\infty \delta^t \phi(t)$. 
As explained in Section \ref{sec:model}, this quantity summarizes the weight of the human's initial divergence $(a_i - h_{i,0})^2$ in a feature's value estimate; when learning is fast, the weight is small, and when learning is slow, the weight is large. The error in value of a feature, given such an error in $\phi$, is: 

\begin{observation}
Consider an algorithm that assumes that human learning is $\phi'$-convergent, where in fact it is $\phi$-convergent, such that $|\sum_{t=0}^\infty \delta^t \phi'(t) - \sum_{t=0}^\infty \delta^t \phi(t)| \leq \eps$ and some feature $i$ such that $h'_i=h_i$ and $a'_i= a_i$, then 
{$\bar \xi_i(\eps) = \xi_i(\eps) = \eps (a_i-h_{i,0})^2$}.
% $\bar \xi_i(\eps)= ?$ and $\xi_i(\eps)$.
\end{observation}
\begin{proof}
Let $\eps' = |\sum_{t=0}^\infty \delta^t \phi'(t) - \sum_{t=0}^\infty \delta^t \phi(t)|$
\begin{align*}
   V_{\delta,\phi'}(\infsta{\{i\}}) &=   \frac{1}{1-\delta}(a_i)^2 - \sum_{t=0}^\infty \delta^t \phi'(t)(a_i-h_{i,0})^2 \\
   &= \frac{1}{1-\delta}(a_i)^2 - \big(\sum_{t=0}^\infty \delta^t \phi(t) \pm\eps \big)(a_i-h_{i,0})^2 \\ 
   &=   V_{\delta,\phi}(\infsta{\{i\}}) \pm \eps (a_i-h_{i,0})^2
\end{align*}
 As it is easy to see that the marginal error is increasing in $\eps'$, we conclude that, $\bar \xi_i(\eps) = \xi_i(\eps) = \eps (a_i-h_{i,0})^2$.   
\end{proof}
By applying \cref{prop-mis-gen-error-app} where for every $i$, we have $\eps_i=\eps$, we get the following corollary: 
 \begin{corollary}
%Let $A$ denote the {optimal} feature set chosen by an imperfect algorithm and $A^*$ the optimal set {according to true parameters}.
  If  $|\sum_{t=0}^\infty \delta^t \phi'(t) - \sum_{t=0}^\infty \delta^t \phi(t)| \leq \eps$ and for every feature $i$, $h'_i=h_i$ and $a'_i= a_i$, then, 
$V_{\delta,\phi}(\infsta{A^*})-V_{\delta,\phi}(\infsta{A})) \leq  \eps \sum_{A^* \oplus A } (a_i-h_{i,0})^2.$

 \end{corollary}
 
So we see that the impact of misestimating the speed of learning on the value estimate is the product of two factors: %how much we err on $\phi$, and how much does the human err in their initial belief. 
the extent of the error in $\phi$ and the magnitude of the human's initial error in their belief.
 If the human's initial belief about a feature is already close to its true coefficient, misestimating the speed of learning does not matter much since there is little for the human to learn. Conversely, when the human's initial error is large, accurately estimating how fast they learn becomes important and errors in this estimates may have larger impact on estimating feature value.

