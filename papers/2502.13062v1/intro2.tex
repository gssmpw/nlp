

AI systems are increasingly used to assist humans in making decisions. In many situations, although
the algorithm has superior performance, it can only assist the human by providing recommendations, leaving the final decision to the human.
For example, 
algorithms assist medical doctors in assessing patients' risk factors and in targeting health inspections and treatments (\cite{musen2021,garcia2019,tomavsev2019,jayatilake2021}), and assist judges in making pretrial release decisions, as well as in sentencing and parole determinations (\cite{courts2020, nyc_cja_2020,compas2019}). However, the final decision ultimately remains at the discretion of the doctors or judges \cite{casey2019rethinking,european2021proposal}. 
This paper studies such AI-assisted decision-making scenarios where the human decision-maker learns from experience in repeated interactions with the algorithm. In our setting, an algorithm assists a human by 
telling them which information they should use for making a prediction, with the goal of improving the {\em human's} prediction accuracy.

Consider, for instance, a doctor assessing a patient's risk of a bacterial infection. To improve their diagnosis, the doctor can order tests that reveal the values of unknown variables. However, the number of tests the doctor can run is limited (e.g., due to costs or time constraints), and they rely on an algorithm to determine which tests to perform. The algorithm, trained on vast amounts of data, is more accurate than the doctor in estimating the statistical relationships between the unknown variables and the disease risk. The doctor has their own (potentially inaccurate) beliefs about how test results relate to risk assessment and will interpret the results according to their beliefs. The doctor orders the tests recommended by the algorithm
-- either because they recognize its superior data processing capabilities or because an insurance provider conditions funding on following the algorithm's selections. The algorithm's objective is to select the tests that lead the doctor to make the most accurate prediction.




While we phrase the problem in terms of a doctor and an algorithm, similar problems appear in other domains as well. For example, in bail decisions, a judge may use an algorithm to determine which aspects of a defendantâ€™s record to scrutinize. In hiring, an algorithm may guide which aspects of an applicant's file to review or suggest questions for the interview. Similarly, an algorithm may assist investors in due diligence by highlighting key aspects of a firm to review before making an investment. 
Beyond these domains, in which features correspond to tests of different types, a similar challenge arises in product design, e.g., 
when designing dashboards and deciding on a fixed subset of features to display to assist humans in various prediction tasks. 
The choice prioritizes features that the human can interpret correctly over those that may be more useful but are harder to interpret. For a concrete example, many weather apps display humidity which people in general know how to interpret instead of displaying the dew point, even though the latter better captures the discomfort caused by humidity \cite{NOAA_DewPoint}.

We begin with a qualitative discussion, identifying two fundamental tradeoffs in AI-assisted decision-making. Then, we provide a brief summary of our model and demonstrate how both tradeoffs manifest within our framework with a concrete example. Finally, we summarize our results. %and contributions.




\vspace{5pt} \noindent  \textbf{Fundamental Tradeoffs in AI-Assisted Decision Making.}
A key question an algorithm faces when assisting a human decision-maker is determining what information will be {\em useful to the human}. If the algorithm were making the prediction on its own, the answer would be clear: it would use all available information to maximize accuracy. When access to information is constrained (e.g., due to costs associated with acquiring additional observations), the algorithm would prioritize the most informative data, selecting the observations expected to contribute most to prediction accuracy. However, when the algorithm does not make the prediction on its own but instead  selects information to assist a $human$ in making a prediction, it must also account for the human's ability to use that information correctly. %\sodelete{in their prediction}. 
The algorithm seeks to balance increasing {\em informativeness} while providing information where the human's and the algorithm's interpretations of the data {\em diverge} less. Our starting point is identifying this tradeoff, which we call the ``informativeness vs. divergence'' tradeoff.


\begin{tradeoff} \label{tradeoff1}
    {\em The Informativeness vs. Divergence Tradeoff:} When selecting information for human use, the algorithm faces a tradeoff between selecting the most informative data and selecting data that minimizes divergence %with lower divergence  
    between the human's model and the algorithm's model of the ground truth. 
\end{tradeoff}


This tradeoff implies that the algorithm may not always select the most informative features for the prediction at hand, but may instead choose less-informative features that fit the human's level of understanding. For example, a medical algorithm might recommend that a doctor performs a less-accurate throat culture, which the doctor has used frequently and knows how to interpret, rather than a newer, more precise blood test for specific proteins that the doctor does not yet know how to interpret its results.
The need to balance informativeness and divergence is fundamental to any human-algorithm interaction where the algorithm provides information to optimize human performance. Recent research reflects a growing awareness of this challenge.
In chess, for instance, skill-compatible AI is designed so that a powerful algorithm playing alongside a less-skilled human does not only choose the best move but one that the human can understand and build upon \cite{hamade2024}. 
\cite{xu2024persuasion} considers the question of when an algorithm should delegate a decision to a human and what information it should provide. They find that more information does not always lead to better decisions. 


 A second fundamental question in human-algorithm interactions arises when moving beyond a single interaction. In this repeated-interaction setting, the human naturally learns from experience by repeatedly making predictions. 
When considering human learning, the algorithm faces a tradeoff between optimizing for short-term versus long-term outcomes: should it optimize for the human's performance in the present, or should it guide them to learn more toward better performance in the future? 
Here, the dilemma of whether to provide more informative or less divergent data is amplified. 
If the algorithm chooses to optimize based on the human's current understanding (e.g., selecting the less-accurate throat culture which the human already knows how to use), doing so repeatedly comes at the risk of preventing learning opportunities and sustaining incorrect beliefs, which may lead to worse performance in the long run. Conversely, if the algorithm chooses to provide information that encourages learning (e.g., instructing the doctor to use the new blood test), it can improve long-term outcomes, but at the cost of an initial learning phase  during which the human may make errors while adjusting their beliefs. We call this the ``fixed vs. growth mindset'' tradeoff. 


\begin{tradeoff} \label{tradeoff2}
     {\em The Fixed vs. Growth Mindset Tradeoff:} 
     When a human decision-maker learns through repeated interactions with an algorithm, the algorithm faces a tradeoff between %increasing immediate payoffs 
     maximizing immediate performance 
     based on the human's current beliefs and %increasing long-term payoffs 
     fostering long-term performance 
     by {getting} %\sodelete{helping} 
     the human to learn and improve their beliefs over time.
\end{tradeoff}


The fixed vs. growth mindset tradeoff naturally arises in teaching environments. The teacher can rely on the student's current knowledge to achieve the best performance in the short term. Alternatively, the teacher can instill a new and better skill, which may take longer to master but ultimately leads to improved performance and a stronger skill set in the long run.
%
This tradeoff is related to the classic question of ``giving a fish vs. giving a fishing rod.''\footnote{This is also {reminiscent} of the ``sneakers vs. coaching'' metaphor \cite{hofman2023a} for thinking about types of AI-Assistance. While ``coaching'' aligns closely with the growth mindset, ``sneakers'' differs from the fixed mindset: in the fixed {mindset} approach, the algorithm does not provide any additional assistance to the human, but rather optimizes solely with their existing knowledge and abilities.}  Providing a fish ensures immediate success but does not contribute to long-term skill development. In contrast, teaching someone to fish requires an initial investment of time and effort but ultimately equips them with the ability to succeed even more in the future -- whether by catching more fish or developing greater self-sufficiency. 



Note that neither of these two tradeoffs has a clear ``correct'' answer. The fixed vs. growth mindset tradeoff (Tradeoff \ref{tradeoff2}), as we will see, depends on the human's ability to learn and the time preferences the algorithm was set to optimize for. In cases of emergency, where immediate outcomes are critical, such as helping a patient in urgent need, it may be best to optimize based on the human's current abilities, even if they are capable of learning. By contrast, situations where time constraints are less strict present an opportunity to focus on skill development and long-term growth. 
%

The informativeness-divergence tradeoff (Tradeoff \ref{tradeoff1}), which may lead the algorithm to provide suboptimal information
according to 
the human's level of understanding, raises the question of whether the algorithm is merely simplifying the problem into a form the human can comprehend or actively manipulating them. While there is no formal distinction between these two cases -- since in both, the algorithm adapts to human limitations at the cost of being less informative -- we tend to perceive them quite differently depending on the context. For example, it seems reasonable to introduce a doctor to a new blood test (low stake scenario), but undesirable when the algorithm's selections lead a doctor to order costly or invasive tests that are less informative than other available options (high stake scenario).

% \newpage


\vspace{5pt} \noindent  \textbf{Model Summary.}
Consider a human who needs to predict the outcome of a variable $y$. The true outcome is given by a function of $n$ features: $y = f(x)$, where the features $x = (x_1,...,x_n)$ are independent random variables that are standardized to have zero mean and unit variance.
We assume the {widely used} linear functional form:\footnote{ Note that this is similar to the linear regression that 
organizations (e.g., hospitals or city governments) often use to weigh factors that human experts (e.g., doctors or judges) are considering when making decisions (e.g.,\cite{nyc_cja_2020, blatchford2000risk, alur2024integrating}).
} $y = f(x) = c + \sum_{i=1}^n a_i x_i$. 



\begin{itemize}
 \item The human's belief about the coefficient of feature $i$ at time $t\geq 0$ is $h_{i,t}$, where {$h_0 = (h_{1,0},h_{2,0}, ..., h_{n,0})$} is their initial belief. Their belief about the constant term is $\bar c$.
 \item The algorithm's estimate of the true coefficients is $a' = (a'_1,...,a'_n)$ and its estimate of $c$ is $c'$. The algorithm's estimates of the humans' initial belief is denoted by {$h'_0 = (h'_{1,0},h'_{2,0},...,h'_{n,0})$} and $\bar c'$.
 \item At every time step $t\geq 0$,
 \begin{itemize}
\item The algorithm selects a subset of features $A_t\subseteq[n]$, with $|A_t| \leq k$, where $k\leq n$ is a budget parameter.
\item The human and the algorithm observe the realization of the features in $A_t$.
\item The human makes a prediction $\bar c + \sum_{i \in A} h_{i,t} x_i$ and exhibits a loss according to the Mean Squared Error (MSE) of their prediction. We denote this loss by $MSE(A_t,h_t)$.
\item The human updates $h_{i,t+1}$ for all observed features $i\in A_t$ according to some arbitrary learning rule that converges to the true $a_i$ as the number of times that feature $i$ is selected goes to infinity. 
 \end{itemize}
\end{itemize}

The algorithm selects a sequence $\seq{S}=(A_t)_{t=0}^\infty$ {aiming} %\sodelete{with the objective} 
to minimize the discounted loss of the human's prediction over time: $\sum_{t=0}^\infty \delta^t MSE(A_t,h_t)$, where $\delta \in (0,1)$ is a discounting parameter that was chosen by the entity deploying the algorithm. We also refer to $\delta$ as the ``patience'' parameter: the higher $\delta$ is, the more patient the algorithm is in considering future outcomes. For the majority of the paper, we analyze the case where the algorithm's model of the ground truth is correct (i.e., $a'=a$ and $c'=c$) and the algorithm knows the human's initial beliefs (i.e., $h_0 = h'_0$, and $\bar c' = \bar c $) and the human's convergence rate. Given these assumptions, the algorithm has all the information required for optimizing this discounted loss. 
%See Section \ref{sec:model} for more details about our model.
We provide more details about our model and elaborate on our modeling choices in Section \ref{sec:model}.

As we will see, many of our results are driven by two quantities: the magnitude of a coefficient $a_i$ representing the {\em informativeness} of feature $i$ and the distance between $a_i$ and $h_{i,0}$ representing the {\em divergence} for that feature at time $0$. %Next, to help the reader get aquatinted with our model 
%we analyze our model 











\vspace{5pt} \noindent  \textbf{Example.}
  %\label{sec:example}
To build intuition about our model and how the fundamental tradeoffs arise within this framework, let us revisit our medical diagnosis example in a single-decision setting. Suppose that the doctor has three possible tests they can run, with true coefficients $a_1=0.3$, $a_2=0.2$, and $a_3=0.1$. That is, test $1$ is the most informative, followed by test $2$, and test $3$ is the least informative. However, the doctor overestimates the importance of tests $1$ and $3$, with $h_1=0.8$ and $h_3=0.15$, while accurately interpreting test $2$, with $h_2=a_2=0.2$. Suppose that the algorithm knows both the ground truth and the human's model and can select any subset of tests (i.e., there is no budget constraint, $k=n=3$).

Table \ref{tbl:example-mse} summarizes the MSE of the human's prediction, for each of the $2^3$ possible subsets of tests that the algorithm can select. %in the single-decision setting. %according to the fixed human coefficients $h_1$, $h_2$, and $h_3$.
As can be seen, the best human performance (i.e., the least MSE) is achieved by selecting tests $2$ and $3$, and therefore this is the algorithm's optimal feature selection in the single-decision setup. 
This optimal subset includes test $2$, which the human perfectly understands ($h_2 = a_2$).
%, meaning there is no divergence and thus no tradeoff. 
Test $3$ is also selected, despite some divergence between the human's beliefs and the ground truth ($h_3 \neq a_3$), because the divergence is small enough relative to its informativeness to still make it beneficial. 
Notably, the optimal subset does not include the most informative test (test 1), as the high divergence in the human's interpretation of this test ($h_1 \gg a_1$) outweighs its informativeness. 
  {In Section \ref{sec:static}, we analyze the exact condition under which 
  the algorithm selects features for an optimal subset in a single prediction instance.} 
This illustrates Tradeoff \ref{tradeoff1}: when minimizing the MSE loss, the algorithm balances between high informativeness and low divergence of the selected tests.






\begin{table}[t]     
    \centering
    \renewcommand{\arraystretch}{1.5}
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
        \hline
        Feature Subset          & \(\emptyset\) & \(\{1\}\) & \(\{2\}\) & \(\{3\}\) & \(\{1,2\}\) & \(\{1,3\}\) & \(\{2,3\}\) & \(\{1,2,3\}\) \\ \hline
        % MSE             & 0.14         & 0.3       & 0.1       & 0.17      & 0.26        & 0.33        & 0.13        & 0.29         \\
        MSE             & 0.14         & 0.3       & 0.1       & 0.1325      & 0.26        & 0.2925        & \textbf{0.0925}        & 0.2525         \\ \hline
    \end{tabular}
        \caption{\normalfont Mean Squared Error (MSE) for all possible subsets of features  for the example in Section \ref{sec:intro}. The example has three features $\{1,2,3\}$, the algorithm's model of the true coefficients is $a'=a=(0.3, 0.2, 0.1)$, the algorithm's model of the human's coefficients is $h'=h=(0.8, 0.2, 0.15)$, and there is no limit on the number of features that can be selected (i.e., $k=n=3$). }
        \label{tbl:example-mse}
\end{table}




This example also demonstrates the importance of modeling the human's decision-making process in addition to modeling the ground truth. A na\"{\i}ve algorithm, which does not model human decisions but instead bases its feature selection solely on its own estimates, 
would recommend considering all features to minimize the error from its own perspective.  
In our example, this would result in almost the worst possible error, as shown in Table \ref{tbl:example-mse}.  

Now, consider the scenario in which a learning human repeatedly interacts with the algorithm to make predictions. Suppose the human is a very fast learner, such that after using a feature once, they learn its true coefficient for all subsequent predictions. If the algorithm selects all features in the first prediction, it incurs a loss of $0.2525$ at that time. However, with repeated selections of all features in subsequent steps, the error drops to zero.
%
By contrast, if the algorithm repeatedly selects only features $2$ and $3$ (which were optimal in the single-shot scenario), it initially incurs a smaller loss of $0.0925$, but in subsequent predictions, it incurs a loss of $0.09$. That is, the error improves due to learning feature $3$, but only to a suboptimal result of $0.09$ in each prediction. The reason for this is that the human was never given an opportunity to learn feature $1$. Thus, if the algorithm equally weights each repetition, for three steps or more it is better off enduring the initial learning period and allowing the human to make mistakes and improve their {model} %\sodelete{coefficients} 
over time. 
The choice between these two sequences depends on how the algorithm weights short-term losses versus long-term losses. 
When learning is more gradual, the learning phase lasts longer and has higher costs, which, along with the weight assigned to future outcomes, influence the algorithm's choice as well.
This second example captures Tradeoff \ref{tradeoff2}: the algorithm trades off the value of teaching the human (the ``growth'' mindset) vs. helping the human perform as best as they can with their current beliefs (the ``fixed'' mindset). The contrast between the algorithm's choices in the one-shot and the learning scenarios demonstrates the importance of taking human learning into account {when considering human decision-making in repeated interactions. }



\vspace{5pt} \noindent  \textbf{Results Summary.}
We analyze the interaction between algorithmic assistance and a learning human decision-maker. {Recall that the algorithm selects a sequence of feature subsets with the objective of minimizing the discounted loss of the human's prediction.}
We begin by characterizing optimal sequences of feature selections. Initially, one might suspect that feature selection is a hard problem due to the large search space: {exponential in the single-interaction setting and unbounded in the repeated-interaction setting.}
Our analysis reveals a surprisingly clean combinatorial structure for this problem. In \cref{thm:stationary}, we show that there exists an optimal sequence that is a stationary sequence -- a sequence in which the algorithm consistently selects the same subset of features at each step (see Section \ref{sec:stationary}). This insight allows us to restrict our focus to stationary sequences, reducing the problem to a finite space, though still exponential in the number of features $n$. Then, we show in \cref{thm:complexity-learning} that for a given value of $\delta$, it is possible to compute an optimal stationary sequence in $\Theta(n \log n)$ time (see Section \ref{sec:complexity}). Moreover, we find that across the full range of $\delta \in (0,1)$, the total number of stationary sequences that can be optimal is at most $\Theta(n^2)$ (\cref{prop-delta-bounded}, Section \ref{sec:tradeoff-analysis}). Notably, our analysis imposes no restrictions on human learning, except that it satisfies a natural convergence property (see Section \ref{sec:model-human}). 
For the full details of our analysis, see Section \ref{sec:learning}.


Following these results, we focus our attention on optimal stationary sequences and study the conditions under which the algorithm selects more informative feature subsets, and how this choice is influenced by the time preferences in the algorithm's objective function and the efficiency of the human's learning. 
%
First, holding human learning at a fixed rate, we show that as the algorithm's patience parameter $\delta$ increases, it increasingly selects more informative feature subsets (\cref{prop-delta-efficient}). This improves both prediction accuracy and the human's understanding of the world in the long term. Additionally, we show that there always exists a sufficiently large $\delta$ value above which the algorithm's optimal selection is the most informative feature set (\cref{prop-delta-efficient}). Second, we fix $\delta$ and vary the efficiency of the learning rule. We show that as the human learns more efficiently, the informativeness of the feature set selected by the algorithm increases (\cref{prop-phi-increases}). Moreover, our analysis highlights that it is more beneficial for the human to invest in learning during earlier time steps rather than later ones, as this allows the algorithm to select more informative features and enables the human to extract greater benefits from the interaction with the algorithm.  For the full analysis, see Section \ref{sec:tradeoff-analysis}.




Finally, in Section \ref{sec:misspecification}, we study the impact of errors in the algorithm's knowledge of the ground-truth coefficients and its models of the human's coefficients and learning rate. Roughly speaking, we translate these modeling errors into the maximum possible error in a quantity that we later denote as the \emph{value} of a feature, and is used to select an optimal feature set. We show that this maximum possible error quantifies a level of tolerance to algorithmic modeling errors: when the gaps between feature values are large there is a wide error tolerance range and when they are small the impact of suboptimal choices that the algorithm makes is small. %\sodelete{On one hand, when gaps between feature values are large, there is a wide tolerance range in which modeling errors have no impact on performance. On the other hand, when differences in feature values are small, small errors may lead to incorrect feature selection, but in such cases, the impact of misselecting features on performance is also small.}
This behavior results from the structure of algorithmic assistance, in which the human makes the actual predictions, and the algorithm's role is limited to selecting the feature sets for the human to use.


% The rest of the paper is organized as follows. In Section \ref{sec:literature}, we discuss related literature. In Section \ref{sec:model}, we provide a detailed description of our model of human-algorithm interaction. In Section \ref{sec:static}, we characterize the algorithm's optimal selection of features in the static scenario, where the human's beliefs about feature coefficients are fixed. Then, in Section \ref{sec:learning}, we present our main analysis of algorithmic assistance in the dynamic scenario, in which the human decision maker learns from experience through repeated interactions with the algorithm. In Section \ref{sec:misspecification}, we analyze cases of misspecification in the algorithm's knowledge, and we conclude in Section \ref{sec:discussion} with a short discussion. 


