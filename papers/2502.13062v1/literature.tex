
Our work is situated in the literature on designing algorithms for assisting human decision-makers (e.g., \cite{bansal2021does,greenwood2024designing,benCSCW,tschandl2020,gomez2025human, peng2024no}). 
%
In particular, we consider a setting in which the algorithm selects for the human what to learn for making the best prediction as part of a repeated interaction. 
%

The majority of the literature assume that the algorithm has direct access to %all the 
information and can give the human a decision recommendation \cite{benFAT,albright2019} or display the human the relevant information for making the decisions (e.g., \cite{dikmen2022effects,du2022role}). A notable exception {in regard to the algorithm's informational structure} is \cite{iakovlev2024value} that theoretically studies a reverse setting, complementary to ours, where humans have discretion to choose, based on situational information, which features to use at each time step, and algorithmic tools are obliged to use the same features. 



Empirical papers in this area of AI-assisted human decision-making study the ability of human decision makers to correctly rely on the algorithm \cite{benFAT, benCSCW, albright2019, tschandl2020, buccinca2021trust, zhang2020}. Typically, they do not consider human learning.
Two exceptions are \cite{noti2023learning} that showed experimentally the advantage of an algorithm to not always provide a recommendation and provided evidence that human decision makers learn through repeated interaction with the algorithm, and  \cite{buccinca2024} that present an experimental study that applies reinforcement learning in repeated decisions with algorithmic advice. 





Our work contributes to the growing literature on designing algorithms that interact with changing human agents. 
In a position paper, \citet{dean2024accounting} call for further development of \enquote{formal interaction models} between algorithms and humans who change over time. 
%
Topics in this literature include work on human-algorithm collaboration in multi-armed bandits, where the goal is to jointly identify some best arm
(e.g. \cite{chan2019assistive, bordt2022bandit}). Additionally, some works on recommendation systems take into account the fact that human preferences may change over time (e.g. \cite{Agarwal2022DiversifiedRF, agarwal2023online}). 
\citet{tian2023towards} studies a dynamical human-robot interaction setting, where the human's mental model of the robot changes over time. 
{Performative prediction \cite{perdomo2020performative} and learning in Stackelberg games (e.g., \cite{haghtalab2024calibrated}) set up a Stackelberg game to study how to make predictions when people respond to them in a way that shifts the data distribution used for the prediction.}




Also related are works on human-AI teams showing that to increase the overall performance of the team, the AI should take the human model into account and make sure its choices are understandable for the human (chess, \cite{hamade2024}, the video game overcooked \cite{carroll2019utility}). Moreover, it is not always the case that a more accurate algorithm \cite{bansal2021most, bansal2019} or one that provides more information \cite{xu2024persuasion} 
are better.
%
Our findings highlight that algorithms need to balance between using features that the human understands and features that the human needs to learn.
This tension is related to the broader field of explainable or interpretable machine learning (XAI), (e.g., \cite{samek2017explainable, finkelstein2022explainable, heuillet2021explainability} and the survey \cite{arrieta2020explainable}) that aims to explain to a human what the model has learned and develop techniques for explaining the model's predictions \cite{singh2020explainable, bach2015pixel},
or to learn problem representations that are more easily interpretable by the human 
\cite{hilgard2021learning,mahendran2015understanding,nguyen2016multifaceted, nahumdecongestion}.   




