In this section, we turn to discussing algorithmic assistance for a human decision maker who learns and updates their beliefs through repeated interactions with the algorithm.
%So far, we have discussed algorithmic assistance to a human decision maker whose beliefs $h$ remain fixed. 
In Section \ref{sec:intro}, we saw the fundamental tension between informativeness and divergence, which leads to the phenomenon where the algorithm does not necessarily select 
the most informative features for prediction but instead it may select those features that the human can effectively use. 
As discussed, when the human's beliefs are not fixed but are instead updated through repeated interaction with the algorithm, selecting a less informative set of features -- while may be helpful in the short term before learning occurs -- may limit learning opportunities, and as a result sustain human's incorrect beliefs and potentially lead to worse performance in the long run. We will see that in our framework, this tradoff is governed by two parameters: $\delta$ that captures how much weight the algorithm's designer puts on short term losses versus long terms losses; and $\phi$ that captures how efficient the human's learning is.




\subsection{The Value of a Sequence }
For the static model (Section \ref{sec:static}) it was very useful to define the value of using a subset of features (Definition \ref{dfn:value-static}). We can extend this definition for a sequence of feature subsets in our learning setting:


\begin{definition} \label{dfn:value-learning}
  The {\em value} of a sequence of feature subsets $\seq{S} = (A_t)_{t=0}^\infty$ for a given value of $\delta$ and a learning dynamic that is $\phi$-convergent is
   the improvement in discounted loss 
    from selecting features according to $\seq{S}$  compared to not using any feature at all. That is,  
\begin{align*}
V_{\delta,\phi}(\seq{S}) &=  L((\emptyset)_{t=0}^\infty,\phi,h_0) -  L(\seq{S},\phi,h_0) 
\end{align*}
\end{definition}
As in the static case, since the loss of not using any feature, $L((\emptyset)_{t=0}^\infty,\phi,h_0)$, is constant, an optimal feature selection is a sequence $\seq{S}^* = (A_t)_{t=0}^\infty$ with maximum value of $V_{\delta,\phi}(\seq{S^*})$ that obeys the budget constraint (i.e., $A_t \subseteq [n]$, $|A_t| \leq k$ for all $t$). We derive an explicit expression for $V_{\delta,\phi}(\seq{S})$:
\begin{claim} \label{clm:val-learn-seq}
$V_{\delta,\phi}(\seq{S}=(A_t)_{t=0}^\infty) 
= \sum_{t=0}^\infty \delta^t \sum_{i\in A_t} (a_i^2-\phi(\fs{i}{t})(a_i-h_{i,0})^2)$
\end{claim}
\begin{proof}
Using the value notion for the non-learning setting (Definition \ref{dfn:value-static}) and \cref{lem:value-static},
\begin{align} \label{eq:value-learning}
V_{\delta,\phi}(\seq{S}) 
& = \sum_{t=0}^\infty \delta^t V(A_t,h_t) \notag 
= \sum_{t=0}^\infty \delta^t \sum_{i\in A_t} V(\{i\},h_{t}) \nonumber \\
&= \sum_{t=0}^\infty \delta^t \sum_{i\in A_t} (a_i^2-\phi(\fs{i}{t})(a_i-h_{i,0})^2)
\end{align}
Recall that $\fs{i}{t}$ is the number of times that feature $i$ was selected prior to time step $t$.
\end{proof}


Before we delve into our general results, in the next subsection 
we illustrate the learning setting in a simple setup with a specific learning dynamic that is $\phi$-convergent. The example demonstrates 
%Next we start with a two-feature example that demonstrates 
the basic tradeoff between the fixed and growth mindsets (Tradeoff \ref{tradeoff2}) in the algorithm's choice of feature selections. We then  continue with the general analysis of the implications of considering a learning decision maker on the algorithm's optimal sequence of feature selections. 



\subsection{Example: Exponential Learning Dynamics} \label{sec:example-learning}

We consider
a $\phi$-convergent learning rule given by the following simple dynamic: 


 
\begin{equation} \label{eq:update-rule}
h_{i, t+1} = f(h_{i,t}, a, A_t) =
\begin{cases} 
w \cdot h_{i,t} + (1 - w) \cdot a_i, & \text{if } i \in A_t, \\
h_{i,t}, & \text{otherwise.}
\end{cases}
\end{equation}
where $w \in [0,1]$ represents a learning rate parameter. {Large values of $w$ indicate a human who is a slow learner, while smaller values of $w$ correspond to a faster learner.}
Equivalently, we can write a cumulative version of this step-by-step learning rule, where the human's belief about feature $i$ at time $t$ is given by:
\begin{equation} \label{eq:update-rule-cumulative}
     h_{i,t} = w^{m_i} \cdot h_{i,0}  + (1 - w^{m_i}) \cdot a_i
\end{equation}
and $m_i=\fs{i}{t}$ denotes the number of times feature $i$ has been selected up to time $t$. At $t=0$, the human starts with their initial beliefs $h_{i,0}$ about each feature $i$. As $m_i \rightarrow \infty$, the human's coefficient of $i$ exponentially converges to the true value $a_i$.


This learning dynamic is $\phi$-convergent with an exponential convergence rate of $\phi(m_i) = w^{2m_i}$. To see why, 
substitute Equation \eqref{eq:update-rule-cumulative} 
and get that:
\begin{align*}
 (a_i-h_{i,t})^2 = (a_i - (w^{m_i} \cdot h_{i,0} + (1-w^{m_i}) a_i))^2 = (w^{m_i}(a_i-h_{i,0}))^2 = w^{2m_i}(a_i-h_{i,0})^2
\end{align*}

 
 With this learning rule, consider the following toy example. Suppose there are two features and the algorithm can select only one of them at each step. Feature $1$ is more informative than feature $2$, with true coefficients $a_1=1$, $a_2=0.4$.  Initially, the human has a larger error on the more informative feature and a smaller one on the less informative feature, with $h_{1,0} = -0.5$, $h_{2,0} = 0.75$. %In the learning process, we assume the same learning coefficient $w_1 = w_2 = w$ for both features and look at different values of $w$.


 
 We compare two simple sequences: one in which the algorithm repeatedly selects feature 1 (the ``informative'' sequence) and another in which it repeatedly selects feature 2 (the ``non-divergent'' sequence).  We ask, when does the informative sequence have lower discounted loss than the non-divergent sequence?
 The answer depends on the discount factor $\delta$ and the learning parameter $w$. {When \( \delta \) is very small, the loss is dominated by the first time step, making the non-divergent sequence the better choice, as it provides %an immediate smaller loss.
 immediate value. 
 In this case, learning is essentially ignored since its gains occur at a later stage and are heavily discounted.}
 On the other end of the spectrum, when $\delta$ is close to $1$, the algorithm assigns significant weight to later steps where learning plays {an essential} role. In this case, using the informative sequence is preferable, as it provides %\soedit{smaller loss} 
 {greater value} once learning has converged. Thus, we expect a transition point -- an intermediate value of $\delta$ -- above which the informative sequence is preferred and below which the non-divergent sequence is preferred. 
 % \soedit{Similarly, as $w$ decreases the loss ratio decreases and the sequence of the more informative feature becomes preferable.  }
 For higher $w$ values, which indicate slower learning, larger $\delta$ values are required to make the informative sequence preferable.

 


 Figure \ref{fig:w-delta-heatmap} shows a heatmap of the ratio of losses between selecting the informative sequence (feature $1$) and selecting the non-divergent sequence (feature $2$), over the $\delta, w$ plane. The red dashed line indicates a loss ratio of $1$, which is the empirical transition curve; for $\delta$ values to the right of this curve, selecting the informative sequence yields lower loss, while for values to the left of the curve, the non-divergent sequence performs better. 
\begin{figure}[h]
    \vspace{-10pt} % Adjust space before the figure
    \centering
    % Subfigure 1
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{"figures/simulation-a-delta-w.png"}
        \caption{
        \normalfont A heat map of the $\delta, w$ plane}
        \label{fig:w-delta-heatmap}
    \end{subfigure}
    \hfill
    % Subfigure 2
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=1.05\textwidth]{figures/delta_w_simulation_t200_mse_ratio_plota1.00.4h-0.50.75.png}
        \caption{\normalfont Fixing specific values of $w$}
        \label{fig:w-delta-plot}
    \end{subfigure}
    \captionsetup{skip=2pt} % Remove space after caption
    \caption{\normalfont The ratio of discounted losses of the more informative sequence over the less informative sequence for the example in Section \ref{sec:example-learning}.}
    \label{fig:main_figure}
    \vspace{-10pt} % Adjust space after the figure
\end{figure}




Specifically, we can see that for every value of $w < 1$ there is a transition point $\delta^*(w)$ above which the algorithm prefers selecting the informative feature. As $w$ increases (slower learning), $\delta^*(w)$ is increasing. The $w=0$ line corresponds to a scenario where the human 
% \sodelete {needs only a single step to learn the}
{learns the coefficient of the} feature they observed 
{in one step}. In this example, $\delta^*(w=0) \approx 0.6$. 
The empirical curve in Figure \ref{fig:w-delta-heatmap} 
coincides with our theoretical results characterizing such curves (see Figure \ref{fig:theory-fit-simulation} in Appendix \ref{app:fig-theory-fit-simulation}). Figure \ref{fig:w-delta-plot} shows projections for several $w$ values to further illustrate the transition. 
For slower learning (higher $w$) lines intersect the ratio of 1 (dashed gray line) at higher $\delta$ values.
At the extreme values of $\delta$
 (either close to 0 or 1), we see that the different $w$ values give the same loss ratio; this is because, as explained above, for $\delta$ close to 0, the loss is dominated by the first step, which is independent of $w$, and for $\delta$ close to 1 
the long-term effects of learning dominate, diminishing the influence of finite learning times.   


\subsection{Optimality of Stationary Sequences} \label{sec:stationary}

In this section, we analyze optimal sequences of feature selections in the setting of human learning. %\sodelete{where we have  multiple features $n$, an arbitrary budget constraint $k$, and {an arbitrary} learning rule that is $\phi$-convergent.} 
We assume that the true coefficients $a$ are distinct.\footnote{This assumption simplifies the analysis and is essentially without loss of generality, as we can always introduce a small amount of noise to ensure this property holds without significantly changing the instance.}  We begin by defining stationary sequences:


\begin{definition}
	A sequence of feature subsets $\seq{S} = \infseq{A}$ is {\em stationary} if  $A_t = A \subseteq [n]$ for all $t$. We use the notation $\infsta{A}$ to denote  a stationary sequence that always selects a subset $A$.
\end{definition}

We are particularly interested in stationary sequences due to their simplicity. We show that for general $n > 1$ and $k \leq n$,  
if the learning dynamic is $\phi$-convergent, then there exists an optimal sequence that is stationary. 
This fact dramatically decreases the size of the optimization space and the computational complexity of the problem; Later in  \cref{thm:complexity-learning} we show that we can find an optimal stationary sequence in $nlog(n)$ time. 



 The proof builds on the following lemma showing that if the learning dynamic is $\phi$-convergent, then there exists an 
 optimal sequence that has an infinite suffix that starting at time step $t$ always selects the same subset (i.e., the sequence has an infinite stationary suffix). 
 



\begin{lemma} \label{lem:constant-suffix}
	If the learning dynamic is $\phi$-convergent, then for any sequence $\seq{S}$ {that does not end in an infinite stationary suffix}, there exists a sequence $\seq{S}'$ of higher value with a stationary suffix.
\end{lemma}

\begin{proof}
Consider a sequence $\seq{S}$ that does not end in an infinite stationary suffix. Denote by \(R(\seq{S})\) the set of features selected infinitely many times. Observe that since we consider $\phi$-convergent learning dynamics, there exists time step $T$ after which all features in $R(\seq{S})$ have been selected sufficiently many times for the coefficient of each such feature $i$ to have converged to $a_i$. This implies that their contribution to the accuracy of the prediction is  positive. Hence, if $R(\seq{S}) \leq k$, the sequence that ends in the stationary suffix that always selects $R(\seq{S})$ has a higher value than the original sequence. Similarly, if $R(\seq{S}) > k$ and there are some steps in which $\seq{S}$ selects less than $k$ features, we can construct a sequence of a higher value by making sure we select $k$ features from $R(\seq{S})$ in each step of the suffix. 
 
We now handle the case that $|R(\seq{S})|>k$ and there exists a suffix in which $\seq{S}$ selects $k$ features at each step. We will show that there exists another sequence $\seq{S}'$ that has a higher value and $|R(\seq{S}')|=|R(\seq{S})|-1$. By applying this argument $|R(\seq{S})|-k$ times we get that there is a sequence $S^*$ with a higher value than $\seq{S}$ such that $R(\seq{S}^*)=k$. This implies that a sequence with a stationary suffix that always selects $R(\seq{S}^*)$ has a value at least as high as the value of $\seq{S}^*$.

Let $A^*$ denote a set that includes the $k$ features in $R(\seq{S})$ that have the highest values of $a_i$. Note that since the features in $A^*$ have the highest true coefficients, for any feature $i \in A^*$ and $j \in R(\seq{S}) \setminus A^*$, there exists $\eps>0$ such that
	\begin{align} \label{eq-conv-better}
		a_i^2-\eps(a_i - h_{i,0})^2 >a_j^2
	\end{align}
    
	Let $\eps>0$ be such that the inequality above holds for any pair $i \in A^*$ and $j \in R(\seq{S}) \setminus A^*$.  Now, since the learning dynamic is convergent, there exists $m_\eps$ such that for any $m\geq m_\eps$, $\phi(m) \leq \eps$. Let $T$ denote the time step by which, for all of the features $i \in A^*$ we have that $m_i \geq m_\eps$. If $R(\seq{S})>k$, then there exists some feature $j \in R \setminus A^*$ that was selected infinitely many times. Since $|A^*|=k$, at each time step $t>T$ that $j$ was chosen, there exists a feature $i_t \in A^*$ that was not chosen. We create the new sequence $\seq{S}'$, by replacing each selection of feature $j$ in step $t>T$, by feature $i_t \in A^*$. By construction $|R(\seq{S}')|=|R(\seq{S})|-1$ and by inequality (\ref{eq-conv-better}), the value of the sequence $\seq{S}'$ is greater than the value of sequence $\seq{S}$.
\end{proof} 




\begin{theorem} \label{thm:stationary}
	If the learning dynamic is $\phi$-convergent, then there exists an optimal sequence and any optimal sequence is stationary (i.e.,  always selects the same subset of features). 
\end{theorem}
\begin{proof}
By \cref{lem:constant-suffix}, we have that 
if an optimal sequence exists, it must end with a stationary suffix. Consider a sequence $\seq{S}=\infseq{A}$ that its stationary suffix begins at $T+1$: $A_T \neq {A_{T+1}}$ and for all $t\geq T+1, A_t = A_{T+1}$. For ease of notation, let $A=A_{T+1}$ and $B=A_{T}$. We will show that there is another sequence $\seq{S}'$ that its stationary suffix begins at $T' \leq T$ and has a higher value. The fact that for every sequence there exists a stationary sequence attaining a higher value implies that there exists an optimal sequence. This is because the number of stationary sequences is bounded. Hence an optimal stationary sequence is also a globally optimal sequence.

We will first compare between three sequences starting from step $T$ (all with the same prefix $A_0,\ldots,A_{T-1}$).

	\begin{enumerate}
		\item $\seq{S}_A$ - a sequence that selects $A$ starting at $T$.
		\item $\seq{S}_{B \rightarrow A} = S$ - a sequence that selects $B$ at time $T$ and then selects $A$ for all subsequent time steps. 
		\item $\seq{S}_{B,B\rightarrow A}$ - a sequence that selects $B$ at time steps $T$ and $T+1$ and then selects $A$ for all subsequent time steps.
	\end{enumerate}
% In Appendix \ref{app:3-comp} we prove the following lemma: 
    \begin{lemma} \label{lem:3-comp}
        $V_{\delta,\phi}(\seq{S}_{B \rightarrow A}) < V_{\delta,\phi}(\seq{S}_A)$ or $V_{\delta,\phi}(\seq{S}_{B \rightarrow A}) \leq V_{\delta,\phi}(\seq{S}_{B,B\rightarrow A})$. 
    \end{lemma}
  
    \begin{proof}
    Assume towards contradiction that $V_{\delta,\phi}(S_{B \rightarrow A}) \geq V_{\delta,\phi}(S_A)$ and $V_{\delta,\phi}(S_{B \rightarrow A}) > V_{\delta,\phi}(S_{B,B\rightarrow A})$.
	
	Let $t_i=\fs{i}{T}$ denote the number of times that feature $i$ was selected prior to time step $T$. We compute the expected discounted values of the sequences we presented:
	\begin{align*}
		V_{\delta,\phi}(S_A) &= \sum_{t=T}^\infty \delta^t \cdot \sum_{i \in A} (a_i^2 - \phi(t_i+t-T)\cdot(a_i-h_{i,0})^2) \\
		V_{\delta,\phi}(S_{B \rightarrow A}) &= \delta^T \sum_{j \in B}(a_j^2 - \phi(t_j)(a_j-h_{j,0})^2) \\
		&+ \sum_{t=T+1}^\infty \delta^t \cdot \left( \sum_{i \in A \setminus B} (a_i^2 - \phi(t_i+t-T-1)\cdot(a_i-h_{i,0})^2)+ \sum_{i \in A \cap B} (a_i^2 - \phi(t_i+t-T)\cdot(a_i-h_{i,0})^2) \right) \\
		& = \delta^T \sum_{j \in B}(a_j^2 - \phi(t_j)(a_j-h_{j,0})^2)\\
		&+ \sum_{t=T+1}^\infty \delta^t \cdot \left( \sum_{i \in A} a_i^2 -\sum_{i \in A \setminus B} ( \phi(t_i+t-T-1)\cdot(a_i-h_{i,0})^2)-\sum_{i \in A \cap B} ( \phi(t_i+t-T)\cdot(a_i-h_{i,0})^2) \right) \\
		V_{\delta,\phi}(S_{B,B\rightarrow A}) & = \delta^T \sum_{j \in B}(a_j^2 - \phi(t_j)(a_j-h_{j,0})^2)+ \delta^{T+1} \sum_{j \in B}(a_j^2 - \phi(t_j+1)\cdot(a_j-h_{j,0})^2) \\
		&+ \sum_{t=T+2}^\infty \delta^t \cdot \left( \sum_{i \in A} a_i^2 -\sum_{i \in A \setminus B} ( \phi(t_i+t-2-T)\cdot(a_i-h_{i,0})^2)-\sum_{i \in A \cap B} ( \phi(t_i+t-T)\cdot(a_i-h_{i,0})^2) \right) \\
	\end{align*}

	By our assumption $V_{\delta,\phi}(S_{B \rightarrow A}) \geq V_{\delta,\phi}(S_A)$, and hence:
	\begin{align*}
		\delta^T(\sum_{j \in B\setminus A}(&a_j^2 - \phi(t_j)(a_j-h_{j,0})^2)-\sum_{i \in A\setminus B}(a_i^2 - (\phi(t_i))(a_i-h_{i,0})^2)) \\
		&\geq     \sum_{t=T+1}^\infty \delta^t\sum_{i \in A \setminus B} (\phi(t_i+t-1-T)-\phi(t_i+t-T))\cdot(a_i-h_{i,0})^2) 
	\end{align*}
	If we divide by $\delta^T$ and adjust the indices accordingly, we get that:
	\begin{align} \label{eq:BA}
		\sum_{j \in B\setminus A}(&a_j^2 - \phi(t_j)(a_j-h_{j,0})^2)-\sum_{i \in A\setminus B}(a_i^2 - (\phi(t_i))(a_i-h_{i,0})^2) \\
		&\geq   \sum_{t=1}^\infty \delta^t\sum_{i \in A \setminus B} (\phi(t_i+t-1)-\phi(t_i+t))\cdot(a_i-h_{i,0})^2) \notag
	\end{align}
	
	Also by our assumption $V_{\delta,\phi}(S_{B,B\rightarrow A}) < V_{\delta,\phi}(S_{B \rightarrow A})$, we have that:
	\begin{align*} \label{eq:BBA}
		\delta^{T+1} ( \sum_{j \in B\setminus A}(&a_j^2 - \phi(t_j+1)(a_j-h_{j,0})^2)-\sum_{j \in A\setminus B}(a_j^2 - \phi(t_i)(a_j-h_{j,0})^2) \\
		&< \sum_{t=T+2}^\infty \delta^t\sum_{i \in A \setminus B} (\phi(t_i+t-2-T)-\phi(t_i+t-1-T))\cdot(a_i-h_{i,0})^2)
	\end{align*}
	If we divide by $\delta^{T+1}$ and adjust the indices accordingly, we get that:
	\begin{align}
		\sum_{j \in B\setminus A}(&a_j^2 - \phi(t_j+1)(a_j-h_{j,0})^2)-\sum_{j \in A\setminus B}(a_j^2 - \phi(t_i)(a_j-h_{j,0})^2)  \\ &<
		\sum_{t=1}^\infty \delta^{t}\sum_{i \in A \setminus B} (\phi(t_i+t-1)-\phi(t_i+t))\cdot(a_i-h_{i,0})^2)  \notag
	\end{align}
    



	If we add inequalities (\ref{eq:BA}) and (\ref{eq:BBA}) and do some rearranging, we get that:
	\begin{align*}
		\sum_{j \in B\setminus A}(a_j^2 - \phi(t_j)(a_j-h_{j,0})^2) > \sum_{j \in B\setminus A}(a_j^2 - \phi(t_j+1)(a_j-h_{j,0})^2)
	\end{align*}
	By definition $\phi(\cdot)$ is decreasing and hence $(a_j^2 - \phi(t_j+1)(a_j-h_{j,0})^2) \geq (a_j^2 - \phi(t_j)(a_j-h_{j,0})^2)$, in contradiction to the above inequality.
    \end{proof}


    
\textbf{Continuing the proof of \cref{thm:stationary}.} Note that $\seq{S}_A$ is a sequence that its infinite suffix starts at $T$ or earlier (if, $A_{T-1} = A$). Therefore, if $V_{\delta,\phi}(\seq{S}_{B \rightarrow A}) < V_{\delta,\phi}(\seq{S}_A)$, we are done. Else, $V_{\delta,\phi}(\seq{S}_{B \rightarrow A}) \leq (\seq{S}_{B,B\rightarrow A})$. Observe that \cref{lem:3-comp} can be also applied to the sequence of $A$'s that starts at $T+2$. That is, 
\begin{align*}
     V_{\delta,\phi}(\seq{S}_{B,B \rightarrow A}) < V_{\delta,\phi}(\seq{S}_{B \rightarrow A}) \text{~or~}, V_{\delta,\phi}(\seq{S}_{B,B \rightarrow A}) \leq V_{\delta,\phi}(\seq{S}_{B,B,B\rightarrow A}). 
\end{align*}


Thus, $V_{\delta,\phi}(\seq{S}_{B \rightarrow A}) \leq V_{\delta,\phi}(\seq{S}_{B,B\rightarrow A})$,  implies that $V_{\delta,\phi}(S_{B,B \rightarrow A}) \leq V_{\delta,\phi}(\seq{S}_{B,B,B\rightarrow A})$. Repeating the same argument recursively, we can always improve the original sequence by increasing the length of the $B$ sequence. In particular, for any $T_B$ the sequence that selects $B$, $T_B$ times and then selects $A$ for all subsequent time steps, has at least the same value as the sequence that selects $B$ only $T_B-1$ times. We denote each such sequence by $\seq{S}_{T_B}= \seq{X},{B,\ldots, B}, (A)_{T+T_B}^{\infty}$, where $\seq{X}$ is the prefix. We will show that here exists $T'$ such that for any $T_B\geq T'$ there exists a stationary sequence starting at $T+1$ that has a higher value than $\seq{S}_{T_B}$.   




Let $C=A\cap B$ denote the features that appear both in $A$ and in $B$.
Let $A^*$ denote an optimal subset of size at most $k-|C| $ for the following optimization problem.
\begin{align*}
    \sum_{t=T+T_B}^\infty \delta^t \sum_{i\in A^*} \left( a_i^2 - \phi(\fs{i}{t})\cdot(a_i-h_{i,0})^2) \right)
\end{align*}
Implying that the selected features are the $k-|C|$ features with highest positive values of:\footnote{If some of the values are negative then less than $k-|C|$ features will be selected.}
\begin{align*}
  U_i(T+T_B, \infty) =  \frac{1}{1-\delta} a_i^2 - \sum_{t=0}^\infty \delta^t \phi(\fs{i}{t+T+T_B})\cdot (a_i-h_{i,0})^2 )
\end{align*}
Similarly, we can define a value for feature $i$ and part of the sequence between time step $T$ and $T+T_B$. An optimal stationary subsequence for steps $T$ through $T+T_B$, will choose the $k-|C|$ features with highest values of:  
\begin{align*}
U_i(T, T+T_B-1) =  \sum_{t=0}^{T_B-1} \delta^{t} a_i^2 - \sum_{t=0}^{T_B-1} \delta^t \phi(\fs{i}{t+T})\cdot (a_i-h_{i,0})^2 )
\end{align*}

Observe that by construction we only consider features $i \not \in C$. Hence, for every such feature we have that $\fs{i}{t+T} =\fs{i}{t+T+T_B}$. Now, since $U_i(T+T_B, \infty)$ is convergent, this means that for every $\eps$ there exists $T_B$ such that $U_i(T, T+T_B-1)\geq U_i(T+T_B, \infty) - \eps$. This implies that there exist $T_\infty$ such that for $T_B\geq T_\infty$, the values of $U_i(T, T+T_B-1)$ for any feature $i \not \in C$ will be sufficiently close to $U_i(T, T+T_B-1)$ and hence the same subset will be optimal for a stationary sequence starting at $T+T_B$ and a stationary sequence between $T$ and $T_B$. 
%\end{proof}



    
    
\end{proof}
{It is interesting to note that the proof of Theorem \ref{thm:stationary} does not require $\phi$ to be decreasing. In fact, as long as $\phi$ converges to $0$, there exists an optimal sequence that is stationary even if $\phi$ oscillates.}


%\subsection{Complexity of optimal sequences}
\subsection{Computing Optimal Stationary Sequences} \label{sec:complexity}
Recall that in Claim \ref{clm:val-learn-seq}, we showed that an optimal sequence for the learning setting maximizes:
\begin{align*}
    V_{\delta,\phi}(\seq{S}=(A_t)_{t=0}^\infty) 
= \sum_{t=0}^\infty \delta^t \sum_{i\in A_t} (a_i^2-\phi(\fs{i}{t})(a_i-h_{i,0})^2)
\end{align*}

Thus, an optimal stationary sequence chooses a subset $A^* \subseteq[n]$ of at most $k$ features that maximizes:
\begin{align} \label{eq:stationary-i}
V_{\delta,\phi}(\infsta{A^*}) &= \sum_{t=0}^\infty \delta^t \sum_{i\in A^*} (a_i^2-\phi(t)(a_i-h_{i,0})^2) \notag\\
&= \sum_{i\in A^*} \sum_{t=0}^\infty \delta^t (a_i^2-\phi(t)(a_i-h_{i,0})^2) \notag \\
%&= \sum_{i\in A} \left( \frac{1}{1-\delta} a_i^2 - \sum_{t=0}^\infty \delta^t \phi(t)(a_i-h_{i,0})^2 \right) \\
&= \sum_{i\in A^*} \Big(\frac{1}{1-\delta}a_i^2 - \sum_{t=0}^\infty \delta^t \phi(t)(a_i-h_{i,0})^2\Big) \\
&=\sum_{i \in A^*} V_{\delta,\phi}(\infsta{\{i\}}) \notag
\end{align}
Hence, \( A^* \) includes the top \( k \) or less features where \( V_{\delta,\phi}(\infsta{\{i\}}) > 0 \).  To verify that \( V_{\delta,\phi}(\infsta{\{i\}}) =\frac{1}{1-\delta} a_i^2 - \sum_{t=0}^\infty \delta^t \phi(t)(a_i-h_{i,0})^2 \) is well-defined, observe that \( \sum_{t=0}^\infty \delta^t \phi(t) \) converges. The reason for this is that since \( \phi(t) \leq 1 \), we have \( \delta^t\phi(t) \leq \delta^t \). Then, by the comparison test, since \( \sum_{t=0}^\infty \delta^t \) converges for $\delta\in (0,1)$, it follows that \( \sum_{t=0}^\infty \delta^t \phi(t) \) also converges. 
In this paper we make the reasonable assumption that $V_{\delta,\phi}(\infsta{\{i\}})$  can be computed in $o(1)$.

As an example, consider the feature values for the exponential learning rule from Section \ref{sec:example-learning}, which is $\phi$-convergent for $\phi(t) = w^{2t}$. 
For this learning rule, 
we obtain $\sum_{t=0}^\infty \delta^t w^{2t} = \frac{1}{1-\delta \cdot w^2}$ and thus the value of feature $i$ is $V_{\delta,\phi}(\infsta{\{i\}}) = \frac{1}{1-\delta} a_i^2 -\frac{1}{1-\delta \cdot w^2} (a_i-h_{i,0})^2$.

As in the static case, this characterization of optimal subsets leads to a simple method for finding an optimal stationary sequence \( S^* \) in \( O(n \log n) \) time, similar to the static case in \cref{thm:complexity-static}: {(1) Compute $ V_{\delta,\phi}(\infsta{\{i\}})$ for each feature $i$, (2) sort them in decreasing order, (3) select the top \( k \) features (or fewer) whose values are positive}. This establishes the following proposition:



\begin{proposition} \label{thm:complexity-learning}
For a given $\delta$ and any $\phi$-convergent learning dynamic, an optimal stationary sequence of feature selections can be computed in $n log(n)$ time.
\end{proposition}








\subsection{When Does the Algorithm Select Informative Features?}
\label{sec:tradeoff-analysis}
% \subsubsection{The trade-off between spoon-feeding and educating the human}

Next, we study the following question: under what conditions does the algorithm prioritize selecting the more informative set of features allowing the human to learn (the growth mindset), rather than keep making the myopic choice of the initially less divergent features for short-term rewards (the fixed mindset)? 
As the example in Section \ref{sec:example-learning} illustrates, this depends in our framework on how ``patient'' the algorithm is (the parameter $\delta$) when considering future rewards, as well as the efficiency of human learning (the parameter $\phi$). The definitions and set of results below formalize this intuition. 
\subsubsection{The algorithm's patience}
Recall that $a^2_i$ captures the informativeness of feature $i$, and $(a_i - h_{i, 0})^2$ is the initial divergence of the human's belief about the importance of the feature from the algorithm's estimate. For a pair of features $i$ and $j$ such that $a_i > a_j$, we denote by $\Delta^I_{i,j} = a^2_i - a^2_j$ the \emph{informativeness difference} between features $i$ and $j$, and by $\Delta^D_{i,j} = (a_i - h_{i,0})^2 - (a_j - h_{j,0})^2$ the \emph{divergence difference} between $i$ and $j$. The following lemma uses the informativeness difference and divergence difference between features $i$ and $j$ to characterize the values of $\delta$ for which $V_{\delta,\phi}(\infsta{\{i\}})\geq V_{\delta,\phi}(\infsta{\{j\}})$.
The lemma establishes the main formal basis for analyzing the  \emph{fixed vs. growth mindset} tradeoff (Tradeoff \ref{tradeoff2}). 
It shows that for two features $i$ and $j$, the more informative feature is either always preferred for any $\delta$, or there is a critical value of $\delta$ above which this is true. 
Intuitively, a feature that has high informativeness but also high divergence, may have low value in the early stages  
before learning takes place, but high value in later stages as learning converges. Small $\delta$ puts relatively more weight on the early stages, and so the informative feature might not be selected, but when $\delta$ is large, there is more weight on the outcomes of learning where the more informative feature has higher value.
\begin{lemma} \label{prop:switch-point}
Consider a pair of features $i$ and $j$ such that $a_i > a_j$. Then, 
\begin{itemize}
    \item If $\Delta^I_{i,j} \geq \Delta^D_{i,j} $, then for any $\delta>0$, $V_{\delta,\phi}(\infsta{\{i\}}) \geq V_{\delta,\phi}(\infsta{\{j\}})$.
    \item  If $\Delta^I_{i,j} < \Delta^D_{i,j}$, there exists $\delta_{i,j}$ such that, for every $\delta<\delta_{i,j}$, $V_{\delta,\phi}(\infsta{\{i\}}) < V_{\delta,\phi}(\infsta{\{j\}})$ and for every $\delta \geq \delta_{i,j}$, $V_{\delta,\phi}(\infsta{\{i\}}) \geq V_{\delta,\phi}(\infsta{\{j\}})$.
    \end{itemize}
\end{lemma}
\begin{proof}
We first show that there exists at most a single value of $\delta$, such that $V_{\delta,\phi}(\infsta{\{i\}}) = V_{\delta,\phi}(\infsta{\{j\}})$. By Equation \eqref{eq:stationary-i}, we should show that the following equation has at most one solution:

\begin{align*}
    \frac{1}{1-\delta_{i,j}} a_i^2 - \sum_{t=0}^\infty \delta_{i,j}^t \phi(t) (a_i-h_{i,0})^2 = \frac{1}{1-\delta_{i,j}} a_j^2 - \sum_{t=0}^\infty \delta_{i,j}^t \phi(t) (a_j-h_{j,0})^2
\end{align*}
By rearranging and %setting $(a_i^2 - a_j^2) = x$ and $\left( (a_i-h_{i,0})^2 - (a_j-h_{j,0})^2 \right) = y$
using our notations $\Delta^I_{i,j}$ and $\Delta^D_{i,j}$, 
\begin{align*}
\frac{1}{1-\delta_{i,j}} \cdot \Delta^I_{i,j} =  \sum_{t=0}^\infty \delta_{i,j}^t \phi(t) \cdot \Delta^D_{i,j} \implies   \frac{\Delta^I_{i,j}}{\Delta^D_{i,j}} = (1-\delta_{i,j})\sum_{t=0}^\infty \delta_{i,j}^t \phi(t)
\end{align*}
Note that 
\begin{align*}
    (1-\delta_i)\sum_{t=0}^\infty \delta_{i,j}^t \phi(t) &= \sum_{t=0}^\infty \delta_{i,j}^t \phi(t) -\sum_{t=0}^\infty \delta_{i,j}^{t+1} \phi(t) \\
    &=1+\sum_{t=1}^\infty \delta_{i,j}^t( \phi(t)-\phi(t-1)) 
    =1-\sum_{t=1}^\infty \delta_{i,j}^t (\phi(t-1)-\phi(t)) 
\end{align*}
Thus, we need to show that there is at most one solution to the equation:
\begin{align} \label{eq:switching-point}
   1- \frac{\Delta^I_{i,j}}{\Delta^D_{i,j}} = \sum_{t=1}^\infty \delta_{i,j}^t (\phi(t-1)-\phi(t))
\end{align}
Note that $\phi(t-1)-\phi(t) \geq 0$ since $\phi$ is decreasing, and since $\phi$ converges to $0$ there should be at least one value of $t$ such that $\phi(t-1)-\phi(t) > 0$. This means that the sum on the right hand-side of the equation is strictly increasing in $\delta_{i,j}$ and hence Equation \eqref{eq:switching-point} has at most one solution. 

Note that if $\Delta^I_{i,j} \geq \Delta^D_{i,j}$ (i.e., $  a_i^2 - a_j^2 \geq (a_i-h_{i,0})^2 - (a_j-h_{j,0})^2 $ ), the left hand-side is $0$ or negative and hence the equation doesn't have any solution. In this case, for any value of $\delta \in (0,1)$, $V_{\delta,\phi}(\infsta{\{i\}}) \geq V_{\delta,\phi}(\infsta{\{j\}})$. If $\Delta^I_{i,j}<\Delta^D_{i,j}$, then the left hand-side is some number between $0$ and $1$. Let $F(\delta) = \sum_{t=1}^\infty \delta_{i,j}^t (\phi(t-1)-\phi(t))$. Note that $F(0) = 0$ and $F(1) = \sum_{t=1}^\infty (\phi(t-1)-\phi(t)) = 1$, where the last equality is because the right hand-side is a telescoping sum that equals $\lim_{t \rightarrow \infty} \phi(0)-\phi(t) = 1$ as $\phi$ converges to $0$. Hence, by the intermediate value theorem Equation \eqref{eq:switching-point} in this case has exactly one solution. 
\end{proof}

To get a better understanding of the value of the switching point between a less informative feature $j$ and a more informative feature $i$, it is instructive to consider the closed-form formula of $\delta_{i,j}$ for our running example of a learning dynamic which is $\phi(t) = w^{2t}$-convergent. 

\begin{claim} \label{clm:educating-two-features}
For a $w^{2t}$-convergent learning dynamic, the value of $\delta_{i,j}$ from \cref{prop:switch-point} is
$$
 \delta_{i,j} 
 = 
 \frac{\Delta^I_{i,j} - \Delta^D_{i,j}}
 {w^2 \cdot \Delta^I_{i,j} - \Delta^D_{i,j}}
 $$
\end{claim}

\begin{proof}
%Recall that $(a_i^2 - a_j^2) = x$ and $\left( (a_i-h_{i,0})^2 - (a_j-h_{j,0})^2 \right) = y$. 
Recall that $\delta_{i,j}$ is the solution of
\begin{align*} 
   1- \frac{\Delta^I_{i,j}}{\Delta^D_{i,j}} = \sum_{t=1}^\infty \delta_{i,j}^t (\phi(t-1)-\phi(t))
\end{align*}
Note that
\begin{align*}
     \sum_{t=1}^\infty \delta_{i,j}^t (\phi(t-1)-\phi(t)) &=  \sum_{t=1}^\infty \delta_{i,j}^t (w^{2(t-1)}-w^{2t}) 
     = \frac{(1-w^2)}{w^2}\sum_{t=1}^\infty \delta_{i,j}^t (w^{2t}) \\
     &= \frac{(1-w^2)}{w^2}\sum_{t=1}^\infty (\delta_{i,j} \cdot w^2)^t 
     = \frac{(1-w^2)}{w^2} \cdot\frac{\delta_{i,j}w^2}{1-\delta_{i,j}w^2} \\
     &= \frac{(1-w^2)\delta_{i,j}}{1-\delta_{i,j}w^2} 
\end{align*}
Now, we look for $\delta_{i,j}$ such that:
\begin{align*} 
\frac{\Delta^D_{i,j}-\Delta^I_{i,j}}{\Delta^D_{i,j}} = \frac{(1-w^2)\delta_{i,j}}{1-\delta_{i,j}w^2} 
\end{align*}
Hence,
\begin{align*} 
&(\Delta^D_{i,j}-\Delta^I_{i,j})\cdot(1-\delta_{i,j}w^2) = \Delta^D_{i,j} \cdot(1-w^2)\delta_{i,j} \\
&\implies \Delta^D_{i,j}-\Delta^I_{i,j} = \delta_{i,j} (\Delta^D_{i,j}-w^2 \Delta^D_{i,j} + w^2 \Delta^D_{i,j}-w^2 \Delta^I_{i,j}) \\
&\implies \delta_{i,j} = 
% \frac{\Delta^D_{i,j}-\Delta^I_{i,j}}{\Delta^D_{i,j}-w^2 \Delta^I_{i,j}}
\frac{\Delta^I_{i,j} - \Delta^D_{i,j}}
{w^2 \cdot \Delta^I_{i,j} - \Delta^D_{i,j}}
\end{align*}
\end{proof}



\cref{prop:switch-point} is the building block of the three results we discuss in this section. We begin by applying it to prove a bound on the number of subsets that are optimal for some value of $\delta \in (0,1)$.





\begin{proposition} \label{prop-delta-bounded}
Fix a problem instance and $\phi$-convergent learning dynamic. Let $A_\phi^*(\delta)$ denote the feature subset selected in an optimal stationary sequence for a patience parameter $\delta$. The number of different subsets for $\delta \in (0,1)$ is at most $\frac{n(n-1)}{2} + k \leq n^2$.
\end{proposition}
\begin{proof}
First, we observe that for small values of \( \delta \), it is possible that there are fewer than \( k \) features with positive values, and hence \( |A_\phi^*(\delta)| \leq k \). Observe that the proof of \cref{prop:switch-point} also implies that there is at most a single $\delta$ value for which \( V_{\delta,\phi}(\infsta{\{i\}}) = 0 \). Hence, there are at most \( k \) switching points, where at each, one additional feature is added to \( A_\phi^*(\delta) \), until we reach a value \( \delta' \) such that for every \( \delta \geq \delta' \), \( |A_\phi^*(\delta)| = k \). 

We can now focus on values of \( \delta \geq \delta' \) for which \( |A_\phi^*(\delta)| = k \). Recall that \( A_\phi^*(\delta) \) consists of the \( k \) features with the highest values. Observe that \( A_\phi^*(\tilde \delta) \neq A_\phi^*(\tilde \delta+\epsilon) \) for some $\tilde\delta$ and for any \( \epsilon > 0 \) if, for \(\tilde \delta \), the top \( k \) values correspond to different features than for \(\tilde \delta+\epsilon \). This implies that \( \tilde \delta \) is a switching point as discussed in \cref{prop:switch-point} of at least two features. \cref{prop:switch-point} also establishes that each pair of features has at most one switching point, implying that the number of switching points and hence optimal subsets of size $k$ is bounded by $\frac{n(n-1)}{2}$. Adding the number of potentially optimal subsets that are smaller than \( k \), we obtain an overall bound of at most \( \frac{n(n-1)}{2} + k \) optimal subsets, as required.
\end{proof}



The next theorem describes the impact of $\delta$ on the algorithm's choices. Clearly, when $\delta$ is very small, almost all the value comes from the initial time step before any learning takes place, and feature selection is with a fixed mindset.
%
We show that as $\delta$ increases and the algorithm puts more weight on future outcomes, it %makes choices with the growth mindset and 
chooses more informative feature subsets (subsets such that  $\sum_{i \in A} a^2_i$ is higher), and so feature selection tends to a growth mindset. We also show that %the last subset in this sequence is the most informative one.
for large enough $\delta$, the most informative subset is selected. 




\begin{theorem} \label{prop-delta-efficient} 
    Fix a problem instance and $\phi$-convergent learning dynamic. Let $A_\phi^*(\delta)$ denote the feature subset selected in an optimal stationary sequence for a patience parameter $\delta$. 
    \begin{itemize}
        \item As $\delta$ increases, the informativeness of $A_\phi^*(\delta)$  increases.
        \item There exists  $\delta^* \in (0,1)$ such that for all $\delta > \delta^*$, we have that $A_\phi^*(\delta)$ is the most informative subset of features allowed by the budget $k$. 
    \end{itemize}
\end{theorem}


\begin{proof}
We first prove the statement in the first bullet. As in the proof of \cref{prop-delta-bounded}, we observe that for small values of \( \delta \) the set of optimal features may include less than $k$ features. For such $\delta$ values in each switching point we add more features to the selection set and since $a_i^2$ is positive the informativeness of \( A_\phi^*(\delta) \) can only increase. 
%
Next, we focus on values of \( \delta \geq \delta' \) for which \( |A_\phi^*(\delta)| = k \). As in the proof of Proposition \ref{prop-delta-bounded}, we observe that if \( A_\phi^*(\tilde \delta) \neq A_\phi^*(\tilde \delta+\epsilon) \) for some $\tilde\delta$ and for any \( \epsilon > 0 \), then  \( \tilde \delta \) is a switching point of at least two features.

We now turn to showing that as we increase $\delta$, $A_\phi^*(\delta)$ becomes more informative. Consider the switching point $\delta_{i,j}$, where prior to it, feature $j$ was included in the optimal subset, and after it, feature $j$ is no longer part of the optimal subset, but feature $i$ is. This means that $j$ has a higher value than $i$ for $\delta < \delta_{i,j}$ but a lower value for $\delta > \delta_{i,j}$. By \cref{prop:switch-point}, this implies that $a_i > a_j$, as required.  

As for the second bulleted statement, let $A^*$ denote the subset of the $k$ most informative features. Consider a feature $i\in A^*$ and $j \not\in A^*$. By \cref{prop:switch-point}, since $a^2_i>a^2_j$, it is either the case that $V_{\delta,\phi}(\infsta{\{i\}}) \geq V_{\delta,\phi}(\infsta{\{j\}})$ for any value of $\delta \in (0,1)$, or there exists $\delta_{i,j} \in (0,1)$ such that for any $\delta \geq \delta_{i,j}$, $V_{\delta,\phi}(\infsta{\{i\}}) \geq V_{\delta,\phi}(\infsta{\{j\}})$. By setting $\delta^*$ to be the maximum of $\delta_{i,j}$'s for any $i\in A^*$ such that $i\in A^*$ and $j \not \in A^*$, and using \cref{thm:stationary}, we get that the optimal sequence is a stationary sequence that selects the most informative subset of features $A^*$.

\end{proof}


\subsubsection{The learning dynamic efficiency} \label{sec-eff-learning}
In this section, we examine the impact of the efficiency of human learning on the algorithm's selection.
%the optimal feature subset that the algorithm can select. 
We see that, roughly speaking, an optimal subset of features for an efficient learner 
is more informative than that for a
slower learner. 

\begin{definition}
A learning dynamic that is $\phi$-convergent is more efficient than a learning dynamic that is $\phi'$-convergent if for every $t$, $\phi(t) \leq \phi'(t)$ and there is at least a single $t$ such that $\phi(t) < \phi'(t)$.
\end{definition}

To take a specific example, in the exponential learning model, a learning dynamic that is 
$w_1^{2t}$-convergent is more efficient than a learning dynamic that is $w_2^{2t}$-convergent if $w_1 < w_2$. At a higher level, we note that learning dynamics that are $\phi$-convergent for $\phi$ with decreasing marginals (i.e., concave) are more efficient than learning dynamics that are $\phi'$-convergent for $\phi'$ with the same marginals as $\phi$ but in a different order. 
% We formally prove this in Appendix \ref{app-dec-marginal}. 
Intuitively, this suggests that it is more beneficial for the human to invest more in learning during earlier time steps rather than later ones, as this allows the algorithm to select more informative features.
Formally, we introduce the following definition to compare the efficiency %speed 
of two learning dynamics:
\begin{definition}
Fix a learning dynamic that is $\phi$-convergent. $\psi(t) = \phi(t-1) - \phi(t)$ for $t\geq 1$ is the marginal function of $\phi(t)$. 
\end{definition}
By this definition, if $\phi$ has decreasing marginals then $\psi$ is monotonically decreasing. 
\begin{claim}
Consider a learning dynamic that is $\phi'$-convergent, and let $\psi'$ denote its marginal function. If $\psi'$ is not decreasing, then a learning dynamic that is $\phi$-convergent, where the marginals of $\phi$, $\psi$, are the same as $\psi'$ except that they are sorted in decreasing order, is more efficient.
\end{claim}
\begin{proof}
We need to show that for every $t$, $\phi(t) \leq \phi'(t)$. By definition $\phi(t) = 1-\sum_{x=1}^t \psi(x)$. Thus, we need to show that 
\begin{align*}
 1-\sum_{x=1}^t \psi(x) \leq  1-\sum_{x=1}^t \psi'(x)
\end{align*}
The inequality clearly holds since in the construction of $\phi$ we sorted the values of $\psi$ in decreasing order and hence for any $t$, $\sum_{x=1}^t \psi(x) \geq\sum_{x=1}^t \psi'(x)$ as required.
\end{proof}




The following proposition shows that fixing a problem instance and a $\delta$ value, an optimal sequence for a more efficient learning dynamic selects the same or more informative feature subset. 

\begin{proposition} \label{prop-phi-increases}
    Let $A^{*}_{\delta}(\phi)$ denote the subset of features chosen in an optimal stationary sequence. If $\phi$ is more efficient than $\phi'$, then $A^{*}_{\delta}(\phi)$ is more informative than $A^{*}_{\delta}(\phi')$.
\end{proposition}
\begin{proof}
    Recall that an optimal subset includes $k$ features with highest values $V_{\delta,\phi}(\infsta{\{i\}}) = \sum_{t=0}^\infty \delta^t (a_i^2-\phi(t)(a_i-h_{i,0})^2)$ (given that those are positive). 
    We show that if $a_i>a_j$, then:
    \begin{itemize}
\item $V_{\delta,\phi}(\infsta{\{i\}}) \leq V_{\delta,\phi}(\infsta{\{j\}}) \implies V_{\delta,\phi'}(\infsta{\{i\}}) \leq V_{\delta,\phi'}(\infsta{\{j\}})$. 
\item $V_{\delta,\phi'}(\infsta{\{i\}}) \geq V_{\delta,\phi'}(\infsta{\{j\}}) \implies
V_{\delta,\phi}(\infsta{\{i\}}) \leq V_{\delta,\phi}(\infsta{\{j\}})$.
    \end{itemize}
    This means that for every pair of features, either both agree on which feature has the higher value, or only learning dynamics that are $\phi$-convergent assign feature $i$ a higher value than feature $j$. As a result, the set of optimal features selected by a $\phi$-convergent learning dynamic is at least as informative as the set of features selected by a $\phi'$-convergent learning dynamic.


    % \begin{align*}
    % V^\infty_{\delta,\phi'}(\{i\}) - V^\infty_{\delta,\phi'}(\{j\}) \geq V^\infty_{\delta,\phi}(\{i\}) - V^\infty_{\delta,\phi}(\{j\})
    % \end{align*}
    Recall that the informativeness difference between features $i$ and $j$ is  $\Delta^I_{i,j} = a^2_i - a^2_j$ and the divergence difference is $\Delta^D_{i,j} = (a_i - h_{i,0})^2 - (a_j - h_{j,0})^2$. Observe that:
    \begin{align*}
    V_{\delta,\phi}(\infsta{\{i\}}) - V_{\delta,\phi}(\infsta{\{j\}}) &= \sum_{t=0}^\infty \delta^t (a_i^2-\phi(t)(a_i-h_{i,0})^2) - \sum_{t=0}^\infty \delta^t (a_j^2-\phi(t)(a_j-h_{j,0})^2)  \\
    &= \frac{1}{1-\delta} \Delta^I_{i,j} - \sum_{t=0}^\infty \delta^t \cdot \phi(t) \cdot \Delta^D_{i,j}
    \end{align*}
    Since $a_i>a_j$ we have that $\Delta^I_{i,j}>0$. If $\Delta^D_{i,j} \leq 0$, then for any learning dynamic that is $\tilde \phi$-convergent, we have that $V_{\delta,\tilde\phi}(\infsta{\{i\}}) - V_{\delta,\tilde\phi}(\infsta{\{j\}}) > 0$. The more interesting case is when $\Delta^D_{i,j} > 0$, and hence feature $i$ is more divergent than feature $j$. For this case we observe that 
    % \begin{align*}
    $
\sum_{t=0}^\infty \delta^t \cdot \phi(t) \leq \sum_{t=0}^\infty \delta^t \cdot \phi'(t) 
    $,
    % \end{align*}
and hence 
$
V_{\delta,\phi}(\infsta{\{i\}}) - V_{\delta,\phi}(\infsta{\{j\}}) \geq V_{\delta,\phi'}(\infsta{\{i\}}) - V_{\delta,\phi'}(\infsta{\{j\}})
$
which implies that both bulleted statements hold, as required.   
\end{proof}


























