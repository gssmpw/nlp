\section{Related Works} 
\textbf{LLM Unlearning.} LLM unlearning algorithms are primarily optimization-based, such as Gradient Ascent (GA) \citep{thudi2022unrolling}, which maximizes the loss on the forget data, and Negative Preference Optimization (NPO) \citep{zhang2024negative}, an adaptation of Direct Preference Optimization (DPO) \citep{rafailov2024direct} to mitigate GA’s utility collapse. These methods often introduce additional loss terms to maintain model utility, such as Gradient Descent or KL Divergence minimization on retain data \citep{yao2023large,maini2024tofu,shi2024muse, liu2024rethinking,fan2025towards,yang2024cliperase,zhuang2024uoe}. Another approach focuses on localization \citep{liu2024rethinking}, modifying specific model components for unlearning. \citet{wang2024large} targeted MLP layers to erase factual knowledge, while \citet{li2024wmdp} adjusted model activations in selected layers to induce unlearning.

% \subsection{LLM Unlearning: Evaluations}

% Existing methods for evaluating LLM unlearning focus predominantly on the model's output quality. For instance, verbatim memorization is often assessed through sentence-completion tasks combined with ROUGE-L scores \citep{lin2004rouge}, as in \citet{eldan2023s,jin2024rwku,shi2024muse}, or through membership inference attacks \citep{jin2024rwku,shi2024muse}. Another common strategy probes knowledge retention via QA tasks with adversarial inputs \citep{li2024wmdp,jin2024rwku,maini2024tofu}. More adversarial approaches consider the model's robustness to targeted attacks \citep{yuan2024towards,lucki2024adversarial,lynch2024eight}, including the use of logit probing \citep{patil2023can,lucki2024adversarial} or reintroducing knowledge through fine-tuning on irrelevant data \citep{lynch2024eight}.

% Despite their usefulness, existing evaluation frameworks often rely on single-purpose metrics—such as ROUGE-L \citep{eldan2023s,jin2024rwku,shi2024muse} or simple QA accuracy \citep{li2024wmdp,jin2024rwku,maini2024tofu}—that serve as coarse-grained proxies for the model’s underlying knowledge state. As a result, these evaluations remain high-level and fail to fully capture the nuanced shifts in an LLM’s reasoning capabilities when specific information is purportedly forgotten.

\textbf{Evaluating LLMs.} The evaluation of  LLMs focuses on both their capabilities and associated concerns. Capabilities are typically assessed across diverse dimensions, including reasoning \& planning \citep{bang2023multitask,huang2024understanding,valmeekam2024planbench,guo2025can}, agent-based ability \citep{liu2023agentbench, huangmetatool}, science domains like chemistry \citep{huang2024chemeval,guo2023can}, social science \citep{huang2024social, li2024quantifying}, and mathematics \citep{liu2024mathbench,liang2024scemqa}. Due to the concerns like jailbreak attack \citep{huang2024obscureprompt, zhou2024defending} and prompt injection \citep{10.1145/3658644.3690291}, many works are focusing on evaluating the trustworthiness of LLMs \citep{huang2024trustllm, zhang2023safetybench, zhou2024attack, zhou2024labsafety, huang2023trustgpt, gao2024honestllm}. Current evaluation methods and metrics are heavily based on natural language tasks, such as BLEU \citep{papineni2002bleu} and ROUGE \citep{lin2004rouge}. Some works propose dynamic and automatic evaluation powered by generative models \citep{zhu2024dynamic, wu2024unigen, bao2024autobench, huang2025position}. However, existing approaches face significant challenges in evaluating the unlearning of LLMs, because they lack the granularity to assess how well the underlying knowledge points of the given ability are fully removed, highlighting the need for a more granular and reliable evaluation framework.

% This highlights the need for 

% Most methods lack the granularity to assess how well specific knowledge concepts are forgotten or retained, and they often fail to capture the nuanced balance between unlearning effectiveness and utility preservation. This highlights the need for fine-grained, concept-level benchmarks that rigorously diagnose both unlearning performance and its impact on retained knowledge.

\vspace{-0.05in}
\subsection{Cognitive Diagnosis Models (CDMs)}
% \vspace{-0.05in}

Cognitive Diagnosis Modeling aims to infer latent student knowledge states from observable responses by simulating the cognitive process \citep{wang2024survey}. CDMs have been widely applied in Intelligent Tutoring Systems \citep{anderson2014engaging,burns2014intelligent} in student modeling \citep{roberts2010developing,maas2022cognitive}, educational recommendation systems \citep{liu2019exploiting,cheng2021exercise} and computerized adaptive testing \citep{zhuang2024bounded}. Early CDMs were primarily grounded in psychometric frameworks \citep{de2009dina,ackerman2014multidimensional}, while recent advancements adopt machine learning algorithms \citep{liu2018fuzzy} and neural networks \citep{wang2022neuralcd,jiao2023neural}, addressing more complicated scenarios such as inductive modeling \citep{liu2024inductive} and cold-start settings \citep{gao2024zero,gao2023leveraging}.
While CDMs are traditionally used in educational contexts to evaluate students' learning progress, we explore their potential in evaluating machine learning algorithms, specifically for unlearning tasks in large language models (LLMs). 
