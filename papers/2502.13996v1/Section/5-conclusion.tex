\section{Conclusion}
In this paper, we present UNCD, a novel method to benchmark LLM capability removal, along with  UNCD-Cyber, a comprehensive unlearning evaluation benchmark in the cybersecurity domain. Our approach leverages  CDM to provide a fine-grained, interpretable assessment of unlearning effectiveness, moving beyond traditional single-value metrics. 
Through extensive experiments across multiple unlearning methods and base models, we demonstrate that UNCD not only enhances evaluation granularity but also aids in refining unlearning strategies by identifying residual knowledge components.  This, in turn, enables   our UNCD-Agent to further improves unlearning by iteratively diagnosing and mitigating residual knowledge.