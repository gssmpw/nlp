
\section{Introduction}

Large Language Models (LLMs) have achieved remarkable success in generating coherent and contextually relevant text \citep{achiam2023gpt,dubey2024Llama}. However, as these models become more pervasive, concerns about their safety and ethical implications have grown. LLMs may inadvertently reproduce copyrighted material, disclose sensitive information, or generate harmful content such as toxic language or instructions for malicious activities \citep{eldan2023s,wei2024evaluating,huang2024trustllm,li2024wmdp,liu2024machine,li2024salad}. These risks motivate the emerging research area of \emph{LLM unlearning}, which aims to mitigate such issues by selectively removing problematic influences from a model.

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=0.5\linewidth]{figures/motivation.pdf}
%    % \vspace{-15pt}
%     \caption{Comparison of single-value  (QA accuracy) and UNCD evaluation for LLM ability unlearning. GA~\citep{thudi2022unrolling} and NPO~\citep{zhang2024negative}, two unlearning methods, do have reduced QA accuracy, but UNCD reveals persistent knowledge concepts in unlearned models, highlighting the limitations of relying on a single aggregate metric.} 
%     \label{fig:motivation}
%     %\vspace{-15pt}
% \end{figure}




%The need for LLM unlearning arises from two central challenges. 
There are two primary focuses regarding unwanted retention in language models.
The first, \emph{data influence removal}, focuses on eliminating the model’s memorization of specific training data (\eg copyrighted or sensitive documents), thereby addressing legal and privacy concerns. The second, \emph{model capability removal}, seeks to eradicate undesirable behaviors or abilities that the model has acquired, such as generating instructions for cyberattacks \citep{li2024wmdp,zhang2024safe}.  In real-world applications, while data influence removal helps mitigate legal risks, effective model capability removal is crucial for preventing the dissemination of dangerous knowledge that could directly facilitate malicious activities. Unlike data influence removal, capability removal cannot be accomplished by simply retraining on a sanitized dataset, since harmful abilities often emerge from a diffuse and implicit combination of training signals. With this in mind, the evaluation of unlearned LLMs presents significant challenges, especially in reliably measuring the extent of forgetting.

Existing LLM unlearning evaluations, such as those employed by benchmarks like MUSE \citep{shi2024muse}, often rely on a single aggregated metric (\eg QA accuracy, ROUGE \citep{lin2004rouge}, BLEU\citep{papineni2002bleu}) to assess whether a model has “forgotten” specific training instances. Although such coarse metrics might be effective for data influence removal, they become problematic for capability removal. Harmful capabilities, such as cyberattack knowledge, are inherently multifaceted, comprising multiple distinct knowledge concepts (\eg defense evasion, network intrusion, exploitation techniques) \citep{strom2018mitre}. An aggregated metric may show an overall decrease in performance while leaving critical knowledge components intact, potentially leaving the model to continue generating harmful outputs. Consequently, relying on these single-value metrics poses significant real-world risks, as residual harmful capabilities can persist unnoticed.

\begin{wrapfigure}{r}{0.5\textwidth}
    \vspace{-10pt}
    \includegraphics[width=\linewidth]{figures/motivation.pdf}
    \caption{Comparison of single-value  (QA accuracy) and UNCD evaluation for LLM ability unlearning. GA~\citep{thudi2022unrolling} and NPO~\citep{zhang2024negative}, two unlearning methods, do have reduced QA accuracy, but UNCD reveals persistent knowledge concepts in unlearned models, highlighting the limitations of relying on a single aggregate metric.}
    \label{fig:motivation}
    \vspace{-10pt}
\end{wrapfigure}

To address these shortcomings, we draw inspiration from educational methodologies that emphasize fine-grained assessment. In educational settings, Cognitive Diagnosis Modeling (CDM) \citep{wang2022neuralcd,liu2024inductive} is used to evaluate learners’ mastery of discrete knowledge concepts, providing a detailed profile of their understanding. We argue that a similar approach is necessary for LLM unlearning: by decomposing a harmful ability into its constituent \emph{knowledge concepts}, one can more precisely determine which aspects have been unlearned and which remain,  complementing the limitations of single-value metrics.

Motivated by the above, we introduce \textbf{UNCD} (\underline{UN}learning evaluation using \underline{C}ognitive \underline{D}iagnosis), a novel framework that leverages CDM  to assess LLM unlearning effectiveness at a granular level. We specifically focus on eliminating a model’s ability to assist in cyberattacks, as cybersecurity provides an ideal domain for capability removal research due to its inherently multifaceted nature, encompassing discrete knowledge concepts such as defense evasion, network intrusion, and exploitation techniques. Existing unlearning benchmarks (\eg WMDP-Cyber \citep{li2024wmdp}) primarily offer a single aggregated QA accuracy metric, thereby overlooking the nuanced challenge of effectively erasing these individual, harmful components.

We introduce a dedicated benchmark, UNCD-Cyber, to systematically evaluate multiple unlearning methods across two base models-Llama-3-8B \citep{dubey2024Llama} and Mistral-7B \citep{jiang2023mistral}.
Our findings reveal that single aggregated metrics often fail to capture nuanced shifts in a model’s underlying knowledge.  While overall performance may appear to degrade as intended, specific critical knowledge components can persist undetected. In contrast, our UNCD provides a fine-grained diagnostic, pinpointing precisely which knowledge concepts have been successfully removed and which remain, offering actionable insights for refining and improving unlearning strategies. As shown in Fig. \ref{fig:motivation}, both Gradient Ascent (GA)~\citep{thudi2022unrolling} and Negative Preference Optimization (NPO)~\citep{zhang2024negative} yield a similar drop in QA accuracy, suggesting comparable unlearning if we rely on a single aggregate metric. The UNCD uncovers persistent knowledge concepts—like \emph{defense-evasion} and \emph{reconnaissance}—indicating that the model can still generate malicious outputs.


Building on these insights, we propose \textbf{UNCD-Agent}, a further unlearning enhancement toward addressing residual harmful capabilities. UNCD-Agent identifies knowledge states resistant to unlearning and generates an additional forget set through a “test and unlearn” pipeline. 
Notably, our experiments show that UNCD-Agent effectively performs further unlearning, % across seven unlearning evaluation metrics, 
achieving substantial improvements in removing harmful knowledge while preserving desirable model capabilities. In summary, our contributions are outlined below:

\vspace{-7pt}
\begin{itemize}[leftmargin=*,itemsep=2pt,parsep=0pt]
% \vspace{-0.1in}
    \item \textbf{A new evaluation framework:} We introduce \textbf{UNCD}, a novel framework 
    %cybersecurity-focused benchmark that explicitly defines knowledge concepts 
    for evaluating ability removal in LLM unlearning.
    \item \textbf{A benchmark evaluation in cybersecurity:} We propose \textbf{UNCD-Cyber} and conduct extensive experiments on multiple unlearning methods, revealing weaknesses in existing evaluation approaches.
    \item \textbf{An advanced unlearning approach:} We propose \textbf{UNCD-Agent}, integrating a CDM-based evaluation and an in-context learning strategy to enhance LLM unlearning, achieving superior performance across key metrics.
\end{itemize}
