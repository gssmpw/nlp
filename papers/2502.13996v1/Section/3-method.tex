\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/cdm_figures.pdf}
    \caption{Overview of UNCD. (Top) The data construction pipeline and dataset examples. (Bottom) The evaluation process.  LLMs, before and after unlearning,   are evaluated using precise or training-free diagnosis, revealing their knowledge stage.  } \vspace{-0.2in}
    %on the Evaluation Dataset and simulated as students in a learning system. A CDM-based method or a few-shot method is then employed to infer latent knowledge states of the LLM students.}
    \label{fig:UNCD methodology}
\end{figure}



\section{Fine-grained Evaluation of LLM Unlearning: UNCD}

\subsection{Formulation} %{Preliminary: Cognitive Diagnostic Model}


%The task of Cognitive Diagnosis Modeling (CDM) includes 
In education settings, CDM typically involves 
a learning system with a set of students \(\displaystyle S = \{s_1, s_2, \dots, s_N\}\), a set of exercises 
\(\displaystyle E = \{e_1, e_2, \dots, e_M\}\), and a set of knowledge concepts 
\(\displaystyle K = \{k_1, k_2, \dots, k_K\}\). Each exercise \( e_i \) may asseses multiple knowledge concepts as indicated by the Q-matrix \(\displaystyle Q \in \{0,1\}^{M \times K}\), , where \( Q_{ij} = 1 \) implies that exercise \( e_i \) evaluates concept \( k_j \).  Students’ responses are stored in a log 
\(\displaystyle R\) as triplets \((s, e, r)\), with \( r \) representing the score (commonly 0 or 1) of the student \( s \) on exercise \( e \). The primary objective of CDM is to infer each student's knowledge state \(\displaystyle F_s = [F_{s1}, F_{s2}, \dots, F_{sK}]\), where \(F_{sk}\) quantifies the mastery level of the student \(s\) on the \(k\)-th knowledge concept.

In our adaptation of CDM to UNCD, we treat each LLM as a "student" whose knowledge state can be diagnosed. Unlike traditional educational settings where students \(S\), exercises \(E\) and response logs \(R\) come from open-source datasets (\eg ASSIST \cite{feng2009addressing}), we define the set of knowledge concepts \(K\) according to our unlearning target (cyberattack-related capabilities) and design custom evaluation exercises \(E\). Drawing on established educational principles \citep{forehand2010bloom}, we vary question difficulty and allow exercises to assess multiple concepts simultaneously (details in Section~\ref{sec:Benchmark}). To increase the number of "students" (LLMs) in our evaluation system and capture model knowledge states within an epoch of unlearning, we treat the base LLM, the unlearned LLMs as well as model checkpoints in unlearning as "students" and collect their answer logs. Then we apply two complementary cognitive diagnosis methods (Section~\ref{subsection: CDMs}) to infer each student’s knowledge state \(F_s\), mirroring how student proficiency is inferred from observed responses.




% In this work, we adopt an education analogy where the pretrained LLM undergoing different unlearning algorithms are students \(S = \{s_1, s_2, \dots, s_N\}\), the evaluation questions in UNCD-Cyber are \(E = \{e_1, e_2, \dots, e_M\}\), and the knowledge concepts targeted for removal are represented as \(K_n = \{k_1, k_2, \dots, k_K\}\). The Q-matrix \(Q = \{Q_{ij}\}_{M \times K}\) in the benchmark specifies the relationship between exercises and knowledge concepts, where \(Q_{ij} = 1\) if exercise \(e_i\) involves concept \(k_j\), and \(Q_{ij} = 0\) otherwise. After unlearning and collecting the students \(S\), we collect student response logs \(R\) on the questions, represented as triplets \((s, e, r)\), where \(s \in S\) denotes a student, \(e \in E\) an exercise, and \(r\) the score achieved by \(s\) on \(e\). The task of UNCD is to trace LLM's ability in the unlearning process (knowledge states of the students \(S\) ) using \(R\), \(Q\) and Cognitive Diagnosis Modeling. 

% Given a base LLM subjected to unlearning, we define students $S = \{s_1, s_2, \dots, s_N\}$ as LLM checkpoints at different steps under the unlearning algorithms, exercises $E = \{e_1, e_2, \dots, e_M\}$ as evaluation questions, knowledge concepts $K = \{k_1, k_2, \dots, k_K\}$ as targeted knowledge units for evaluation. Additionally, the Q-matrix $Q \in \{0,1\}^{M \times K}$ maps exercises to knowledge concepts where $Q_{ij} = 1$ indicates that exercise $e_i$ assesses concept $k_j$, and response logs $R$ are triplets $(s, e, r)$ where $r$ represents the score of student $s$ on exercise $e$. using $R$ and $Q$, UNCD aims to infer student knowledge states \(F_s \in \mathbb{R}^{1 \times K}\), where \(F_s = [F_{s1}, F_{s2}, \dots, F_{sK}]\) and each element \(F_{sk}\) indicates the mastery level of \(s\) on the \(k\)-th knowledge concept. 


\vspace{-0.05in}
\subsection{The UNCD-Cyber Benchmark}
\label{sec:Benchmark}
\vspace{-0.05in}

% Building upon UNCD, we developed a comprehensive unlearning benchmark in the cybersecurity domain, called UNCD-Cyber, designed to evaluate LLM unlearning using cognitive diagnosis.
As shown in Figure \ref{fig:UNCD methodology}, %our UNCD-Cyber benchmark comprises two key components: 
conducting   UNCD   needs
an \textbf{Unlearn Dataset} for facilitating the unlearning process and an \textbf{Evaluation Dataset} for fine-grained unlearning assessment. Next, we introduce the construction of these datasets in cybersecurity.

\textbf{The Unlearn Dataset} is a collection of text fragments containing cyberattack-related content, designed to remove harmful cyberattack capabilities from LLMs. We construct this dataset by gathering open-source Cyber Threat Intelligence (CTI) reports \citep{gao2022threatkg,gao2021system} and applying a systematic filtering and scoring pipeline. First, we select only those reports exceeding 500 words to ensure sufficient content richness. Next, we compile a curated list of topics relevant to offensive cybersecurity operations and use GPT-4o \citep{achiam2023gpt} to assess each report’s relevance to these topics on a \textit{0–5} scale, following predefined guidelines. Reports scoring 5 are designated as \textit{forget data}, while those scoring below 2 serve as \textit{retain data}, filtering out data that interleaves the forget and retain objective. This establishes a clear boundary between data to be removed and data to be preserved. Further details on the data processing procedure can be found in Appendix~\ref{corpus sytem prompt}.


\begin{wraptable}{r}{0.4\textwidth}
\captionsetup{justification=centering}

\vspace{-10pt}
\centering
\scriptsize
\caption{Data stastics}
\vspace{-10pt}

%\caption{The impact of the classifier on the perturbation success rate of the LLMs. The full model names are: GPT-4o, Gemma-2-27B, Llama-3.1-70B, and Qwen2-5-72B. The rows display the perturbation success rate with and without the classifier.}
\label{table:UNCD-Cyber evaluation}

\begin{tabular}{lccc}
    \toprule[1.5pt]
    \textbf{Unlearn Dataset} & \multicolumn{2}{c}{\textbf{Forget}} & \textbf{Retain} \\ 
    \cmidrule(lr){2-3} \cmidrule(lr){4-4}
      \# Tokens & \multicolumn{2}{c}{\centering 2.9M} & 3.3M \\ 
      \# Samples & \multicolumn{2}{c}{\centering 4.9k} & 8.3k \\
      \midrule
   \multirow{2}{*}{\textbf{Evaluation Dataset}} & \multicolumn{2}{c}{\textbf{Forget}} & \multirow{2}{*}{\textbf{Retain}}\\
   \cmidrule(lr){2-3}
    & \textbf{\textsc{Easy}} & \textbf{\textsc{Hard}} & \\
    \midrule
    \# Techniques & 100 & 82 & 23 \\
     \# Domains & 13 & 13 & 4 \\
    \# Questions (Q) & \(26\text{k}\) & \(8\text{k}\) & \(2\text{k}\) \\
    \# Techniques per Q & 1 & 2.1 & 1 \\
    \# Tokens per   Q   & 12 & 32 & 11 \\
    \bottomrule[1.5pt]
  \end{tabular}
  \vspace{-10pt}

\end{wraptable}




\textbf{The Evaluation Dataset} measures removal of cyberattack ability and retention of benign computer science knowledge by targeting two categories of Knowledge Concepts (KCs): \textit{Forget KCs}, representing knowledge to be removed, and \textit{Retain KCs}, representing knowledge to be preserved. The Retain KCs are drawn from core computer science concepts in CS-Bench \citep{song2024cs}, with each evaluation question testing a single concept for precision. The Forget KCs are derived from the MITRE \ATTCK\ database \citep{strom2018mitre}, leveraging its comprehensive taxonomy of cyberattack techniques, tactics, and other objects (see Appendix~\ref{appendix:UNCD-Cyber} for details). As shown in Table \ref{table:UNCD-Cyber evaluation}, UNCD-Cyber Evaluation Dataset provides two levels of granularity in Forget KCs and Retain KCs. \emph{Techniques} are specific skills and knowledge points, derived from the MITRE \ATTCK\ \emph{technique} object and \emph{sub-domain} knowledge in CS-Bench. \emph{Domains} are contextual categories for the techniques, derived from MITRE \ATTCK\ \emph{tactic} object and \emph{domain} knowledge in CS-Bench.

To ensure a balanced assessment, the evaluation questions for forgetting are split into \underline{two difficulty levels} \citep{forehand2010bloom}. The \textbf{easy set} tests \emph{Knowledge} and \emph{Comprehension} using single-concept questions, while the \textbf{hard set} evaluates \emph{Application} and \emph{Analysis} via \textbf{multi-concept, scenario-based questions}. As illustrated in Figure~\ref{fig:UNCD methodology}, each question is mapped to relevant \emph{Techniques} and \emph{Domains}, forming an explicit Q-matrix ($Q$) for cognitive diagnosis. All questions were generated using GPT-4o and rigorously validated by seven CS PhD students through open discussions and cross-examinations to ensure accuracy, relevance, and quality. Table \ref{table:UNCD-Cyber evaluation} summarizes the dataset statistics for UNCD-Cyber. Details of question generation, including prompts, and human review process are provided in Appendix~\ref{appendix:UNCD-Cyber}. 


\subsection{Knowledge States Diagnosis} \vspace{-0.05in}
\label{subsection: CDMs}
As shown in the bottom of Figure \ref{fig:UNCD methodology} and Algorithm~\ref{UNCD algo},  LLMs undergoing unlearning are evaluated by answering questions from the Evaluation Dataset at different checkpoints, simulated as students in our evaluation system. Once  the response logs \(R\) are collected, using the Q-matrix \(Q\) (which maps questions to their corresponding knowledge concepts), we apply two complementary methods to infer knowledge states of the LLM students. 

\begin{wrapfigure}{r}{0.5\textwidth}
 \vspace{-40pt}
  \begin{minipage}{0.5\textwidth}
    \begin{algorithm}[H]
    \small
    \caption{UNCD Response Logs Collection}
    \label{UNCD algo}
    \begin{algorithmic}[1]
    \Require Base model \(M_0\), evaluation questions \(E\), simulated students in UNCD evaluation system \(\displaystyle S = \{s_1, s_2, \dots, s_N\}\)
    \State \(s_1 \gets M_0\)
    \For{\(\textbf{algo} \in \{\text{GA, NPO, RMU, ...}\}\)}
        \State \(M \gets M_0.\texttt{unlearn}(\textbf{algo})\)
        \If{\(\textbf{step} \% \textbf{save\_steps} = 0\)}
            \State \(s_i \gets M.\texttt{checkpoint}(\textbf{step})\)
        \EndIf
    \EndFor
    \ForAll{\(s_i \in \{s_1, s_2, \dots\}\)}
        \State \(R \gets R \cup s_i.\texttt{get\_answer}(E)\)
    \EndFor
    \end{algorithmic}
    \end{algorithm}
  \end{minipage}
  \vspace{-10pt}
\end{wrapfigure}

\noindent 
\textbf{Training-Free Few-Shot Knowledge Tracing.} 
    Following \citet{li2024explainable}, we treat a large language model as a "teacher" that diagnoses a "student" (\ie the unlearned LLM) via a few-shot prompt. This approach requires no additional training and yields qualitative proficiency labels (\eg "good", "fair", "bad") for each concept. These labels are quantified as numerical scores by mapping "good" to 1, "fair" to 0.5, and "bad" to -1 (or another suitable scheme). At a given   checkpoint \(s\),   knowledge states \(F_s\) of a model form  a vector \(F_s = [\, F_{s1}, F_{s2}, \dots, F_{sK} \,]\),
where \(F_{sk} \in \{0, 0.5, 1\}\). To obtain an aggregate measure, we take the mean across all Forget KCs: \(avg(F_s)\). This yields a single value indicating the student's overall knowledge mastery level, denoted as $M_s= avg(F_s)$.

\noindent  
\textbf{Cognitive Diagnosis Models (CDMs).} 
    We also employ CDMs to obtain real-valued mastery levels. Specifically, we use the Neural Cognitive Diagnosis Model (NCDM) \citep{wang2020neural} and the Inductive Cognitive Diagnosis Model (ICDM) \citep{liu2024inductive}, both of which learn real-valued latent factors that capture the model's ability level (\(\theta\)) at each checkpoint, and each exercise’s difficulty or conceptual profile (\(\beta\)). Specifically, \(\theta\) and \(\beta\) are first encoded using \(R\) and \(Q\), employing one-hot encoding or graph-based encoding. For NCDM and ICDM, \(\displaystyle \theta \in \{0,1\}^{N \times K}\), \(\displaystyle \beta \in \{0,1\}^{M \times K}\), where $K$ represents the number of Forget KCs. Then an interaction function \( f \) (a monotonously increasing function) is employed in the   prediction process, formulated as:  
\( \hat{y}_{ij} = \sigma \left( f \left( (\theta_{s_i} - \beta_{e_j}) \odot Q_{e_j} \right) \right) \), indicating the prediction of student $s_i$ correctly answering exercise $e_j$. After training the CDM, we could directly obtain the knowledge states \(F_s\)=\(\theta\). We then average \(F_s\) within the \emph{Forget KCs} to obtain a single value:  $M_s= avg(F_s)$, representing the  overall mastery on forget knowledge concepts at one checkpoint.
To enhance robustness, we augment the data by sampling synthetic "students" from each checkpoint’s logs, as detailed in Appendix~\ref{appendix: CDM}.
    

    
   

\vspace{-0.05in}








