\section{Evaluation Results} 

\subsection{Experiment Setup} 

We adopt two LLMs,  
\text{Llama-3-8B} \citep{dubey2024Llama} and \text{Mistral-7B} \citep{jiang2023mistral}, for conducting all unlearning experiments. 
Eight unlearning methods are benchmarked by UNCD-Cyber: Gradient Ascent (GA) \citep{thudi2022unrolling}, Negative Preference Optimization (NPO) \citep{zhang2024negative}, Representation Misdirection for Unlearning (RMU) \citep{li2024wmdp}, Task Vector (TV) \citep{ilharco2022editing}, along with GA and NPO combined with Gradient Descent on the retain set (GDR) or KL divergence minimization on the retain set (KLR). These algorithms are listed as: GA, \GAGD, \GAKL, NPO, \NPOGD, \NPOKL, RMU, and TV. Their details are introduced in Appendix~\ref {appendix: unlearning methods}, and the experiment setup is detailed in \ref{appendix: unlearning-logging}.



We unlearn the base LLMs for one epoch, divided into four equal unlearning steps, and evaluate the base LLMs and unlearned LLMs on forget and retain performance, on the UNCD-Cyber Forget and Retain Evaluation Set, respectively. For the Task Vector (TV) method, we perform task arithmetic at 1-4 epochs for fine-tuning and checkpoint the unlearned model.
\textbf{Forget Performance} is measured as LLM's reduction in cyberattack ability, using metrics such as standard  
QA \textbf{Accuracy}, and our proposed $M_s$,  inferred by NCDM, ICDM and Few-Shot (FS) approaches.      
%are the diagnosed knowledge states using NCDM and ICDM, averaged across all forget knowledge concepts. 
Given the extensive cyberattack techniques covered in UNCD-Cyber, we leverage the \emph{domains} in our dataset as knowledge concepts. \textbf{Retain Performance} is evaluated across three dimensions: \textbf{In-Domain} is average QA accuracy on UNCD-Cyber Retain Evaluation Set, \textbf{General} is the average QA accuracy on MMLU \citep{hendrycks2020measuring} and \textbf{Fluency} is the score given by MT-Bench \citep{zheng2023judging}. Further details are provided in Appendix \ref{appendix:evaluation criteria}.

% \vspace{-1pt}
% (Since each Retain KC evaluation question only contains a single knowledge concept, we use QA accuracy (Acc) to represent knowledge mastery on each Retain KC, and a checkpoint's overall mastery on Retain KCs is the average accuracy on all questions.)     




\subsection{Results and Disussion} 

\noindent
\textbf{UNCD uncovers divergent progression in unlearning}.
Figure 3 illustrates the variations in knowledge states   $F_s$ at four unlearning steps as Llama-3-8B undergoes \GAGD, \NPOGD, \GAKL\ and \NPOKL. These variations highlight the advantages of UNCD in capturing the progression of unlearning. 

\begin{wrapfigure}{r}{0.48\textwidth}
    \vspace{-10pt}
    \includegraphics[width=\linewidth]{figures/radars.pdf}
    \caption{Variations of knowledge states $F_s$ at four unlearn steps as Llama-3-8B undergoes \GAGD, \NPOGD, \GAKL\ and \NPOKL.}
    \label{fig:radar chart}
     \vspace{-15pt}
\end{wrapfigure}


Notably, we observe  divergent unlearning trajectories across different algorithms. \NPOGD\ exhibits a balanced removal of knowledge concepts, as reflected by a uniform contraction across all knowledge areas. In contrast, \GAGD\ leads to uneven degradation, with certain knowledge domains (\eg "command-and-control") being disproportionately affected compared to others.



\noindent
\textbf{Correlation  between QA Accuracy and knowledge mastery $M_s$}.
Table~\ref{table:main table} shows the evaluation of eight unlearning methods when applied to Llama-3-8B and Mistral-7B. By comparing the standard QA Accuracy with our $M_s$ measure of knowledge states, we observe that 
%we derive several key findings: 1) 
there exists a \textbf{strong correlation between QA Accuracy and $M_s$}, \eg unlearned models with higher/lower QA Accuracy also tend to have higher/lower  $M_s$.  {For instance, the correlation coefficient between QA Accuracy and $M_s(\text{NCDM})$ is $0.93$, with a $p$-value of $0.03$, indicating a statistically significant relationship.} This validates that our $M_s$ measure effectively captures the model’s knowledge mastery in a way that aligns with conventional performance metrics.
%2) There is \textbf{both agreement and   discrepancy between QA Accuracy and $M_s$ in selecting the most effectively unlearned model}. Both metrics agree that Llama-3 unlearned by GA achieves the best performance in forgetting. However, they disagree on the best unlearning method for Mistral. While QA Accuracy selects NPO, $M_s$   identifies GA as the more effective unlearning method.  \textcolor{red}{This discrepancy suggests that a single metric may overlook nuanced knowledge differences, highlighting the need for a finer-grained evaluation to reveal which knowledge concepts remain intact.}

%shows that for each model, the diagnosed knowledge states (e.g., NCDM-ks) are consistent with QA accuracy, indicating the reliability of cognitive diagnosis. 
%UNCD-Cyber serves as a challenging benchmark for evaluating unlearning: Existing unlearning algorithms fail to balance forget and retain performance. For instance, GA and \GAGD\ exhibit significant reductions in both knowledge states (e.g., lower NCDM-ks and ICDM-ks) and QA accuracy on the forget evaluation set. However, this comes with a drop in retain performance. Conversely, algorithms like \GAKL\ and \NPOKL\ fail to achieve effective forgetting, as indicated by relatively high NCDM-ks and ICDM-ks. Additionally, these methods still experience notable degradation in retain performance (e.g., fluency). This dual failure—limited unlearning and loss of utility—highlights the need for further research into techniques that better balance these aspects.






\begin{table*}[t!]
    \centering
    \small
    \setlength{\tabcolsep}{4pt} % Reduce column spacing

    \renewcommand{\arraystretch}{1.05} % Adjust line spacing

    \begin{tabular}{
        l
        S[table-format=2.2]
        S[table-format=2.2]
        S[table-format=2.2]
        S[table-format=2.2]
        @{\hspace{2em}}
        S[table-format=2.2]
        S[table-format=2.1]
        S[table-format=2.3]
    }
    \toprule[1.5pt]
    {} & \multicolumn{4}{c}{Forget} & \multicolumn{3}{c}{Retain} \\
    \cmidrule(lr){2-5} \cmidrule(lr){6-8}
    & {Acc.$\downarrow$} & {$M_s$-NCDM$\downarrow$} & {$M_s$-ICDM$\downarrow$} & {$M_s$-FS$\downarrow$} & {In-Domain Acc.$\uparrow$} & {General Acc.$\uparrow$} & {Fluency$\uparrow$} \\
    \midrule
    \textbf{Llama-3-8B}          & 61.96 & 57.26 & 69.83 & 46  & 57.19 & 62.19 & 5.62 \\ 
    \quad \textbf{+GA}                   & 13.86 & 7.83  & 9.87  & -12 & 16.00 & 28.56 & 1.00 \\
    \quad \textbf{+\GAGD}               & 16.81 & 21.05 & 12.25 & 21  & 30.17 & 59.84 & 3.97 \\
    \quad \textbf{+\GAKL}               & 56.27 & 53.91 & 68.12 & 14  & 52.13 & 55.70 & 1.01 \\ 
    \midrule
    \quad \textbf{+NPO}                  & 29.75 & 39.98 & 50.46 & -7  & 33.37 & 22.95 & 1.00 \\
    \quad \textbf{+\NPOGD}              & 50.10 & 48.02 & 67.24 & 13  & 55.27 & 59.96 & 5.18 \\
    \quad \textbf{+\NPOKL}              & 57.39 & 48.76 & 65.97 & 15  & 52.34 & 56.15 & 1.03 \\
    \midrule
    \quad \textbf{+RMU}                  & 58.68 & 55.43 & 67.43 & 36  & 56.55 & 61.13 & 5.39 \\
    \quad \textbf{+TV}                   & 56.47 & 53.98 & 68.70 & 27  & 49.57 & 34.20 & 1.01 \\ 
    \midrule
    \textbf{Mistral-7B}          & 58.92 & 59.44 & 72.59 & 44  & 54.21 & 59.13 & 1.71 \\ 
    \quad \textbf{+GA}                   & 12.26 & 16.27  & 3.67 & -10 & 15.83 & 24.65 & 1.00 \\
    \quad \textbf{+\GAGD}               & 17.56 & 29.73 & 9.93 & 23  & 18.76 & 22.74 & 1.00 \\
    \quad \textbf{+\GAKL}               & 52.13 & 56.04 & 71.81 & 16  & 48.61 & 47.02 & 1.00 \\ 
    \midrule
    \quad \textbf{+NPO}                  & 9.75 & 21.48 & 3.73 & -5  & 17.53 & 25.51 & 1.00 \\
    \quad \textbf{+\NPOGD}              & 27.24 & 44.10 & 45.14 & 14  & 39.66 & 42.81 & 1.04 \\
    \quad \textbf{+\NPOKL}              & 51.77 & 56.62 & 71.90 & 17  & 48.19 & 49.16 & 1.00 \\
    \midrule
    \quad \textbf{+RMU}                  & 48.86 & 49.17 & 69.07 & 37  & 49.57 & 49.91 & 1.58 \\
    \quad \textbf{+TV}                   & 27.06 & 38.90 & 27.65 & 28  & 27.99 & 25.80 & 1.00 \\
\midrule
    Pearson R w. Acc. & \textbackslash  & 0.93 & 0.96 & 0.66 & 0.97 & 0.96 & 0.65  \\  
    $p$-value &  \textbackslash & 0.00 & 0.00 & 0.03 & 0.00 & 0.00 & 0.18 \\
    \bottomrule[1.5pt]
    \end{tabular} \vspace{-5pt}
    \caption{Unlearning results of Llama-3-8B and Mistral-7B on eight unlearning methods. $\downarrow$ indicates lower is better, while $\uparrow$ indicates higher is better. All knowledge states and accuracies are scaled to percentages. We compute the Pearson correlation coefficient \citep{cohen2009pearson} between QA accuracy (Acc.) and other metrics to quantify their statistical relationship, along with the corresponding $p$-values to assess significance.
    }
    \label{table:main table}
    \vspace{-5pt}
\end{table*}


\noindent
\textbf{UNCD reveals a false sense of unlearning success given by QA Accuacy}.
In Table~\ref{table:main table},   Llama-3-8B unlearned using \GAGD\ achieved a QA accuracy of 16.81, suggesting substantial ability removal. However, the model still retains proficiency in certain knowledge areas like "collection", indicating incomplete unlearning, as shown in Figure~\ref{fig:radar chart}. Similarly, for Llama-3-8B unlearned using \NPOGD, although its QA accuracy (50.10) indicates partial ability removal, some knowledge concepts (\eg "reconnaissance") remain largely unaffected, suggesting ineffective unlearning. This demonstrates the limitations of relying solely on QA Accuracy, as it may create a misleading impression of unlearning success, failing to capture residual knowledge retention.



\noindent \textbf{UNCD evaluates fine-grained LLM ability in forgetting and retaining.} As illustrated in Figure~\ref{fig:forget and retain}, UNCD provides a fine-grained evaluation of capability removal by assessing specific forget and retain knowledge concepts. The figure highlights that for the base models, unlearning methods such as GA, \GAGD, and NPO effectively reduce proficiency on forget knowledge concepts like "initial-access" and "persistence" as intended. However, these methods also inadvertently degrade the retain knowledge concepts such as "data structure" and "computer organization", underscoring the challenge of preserving in-domain knowledge.  

\begin{figure}[t!]
    \centering
    % First subfigure (Llama)
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/Llama_bar.pdf}
        %\caption{Knowledge states of Llama before and after unlearning.}
        \label{fig:Llama_bar}
    \end{subfigure}
    \vspace{-5pt}
    \hfill
    % Second subfigure (Mistral)
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/mistral_bar.pdf}
        %\caption{Knowledge states of Mistral before and after unlearning.}
        \label{fig:mistral_bar}
    \end{subfigure}
    \vspace{-5pt}
    \caption{Forget and retain knowledge states of Llama-3-8B and Mistral-7B under unlearning. Forget knowledge states are diagnosed by the NCDM model, while retain knowledge states are measured by average accuracy (Acc) on UNCD-Cyber Evaluation Dataset.}
    \label{fig:forget and retain}
    \vspace{-10pt}
\end{figure}

% \begin{wrapfigure}{l}{0.48\textwidth}
%  \vspace{-15pt}
%     \centering
%     % First subfigure
%     \begin{subfigure}{\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/Llama_bar.pdf}
%         %\caption{Knowledge states of Llama before and after unlearning.}
%         \label{fig:Llama_bar}
%     \end{subfigure}
    
%     \vspace{-0.05in} % Adjust spacing between subfigures
    
%     % Second subfigure
%     \begin{subfigure}{\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/mistral_bar.pdf}
%         %\caption{Knowledge states of Mistral before and after unlearning.}
%         \label{fig:mistral_bar}
%         \vspace{-10pt}
%     \end{subfigure}
    
%     \caption{Forget and retain knowledge states of Llama-3-8B and Mistral-7B under unlearning. Forget knowledge states are diagnosed by the NCDM model, while retain knowledge states are measured by average accuracy (Acc) on UNCD-Cyber Evaluation Dataset.}
%     \label{fig:forget and retain}
%     \vspace{-20pt}
% \end{wrapfigure}

\noindent
\textbf{Divergent unlearning behaviors despite similar forgetting rates}.
UNCD also highlights that algorithms with similar forgetting rates can have distinct unlearning behaviors. According to QA Accuracy shown in Table~\ref{table:main table}, Llama-3-8B unlearned with \GAKL\ and \NPOKL\ have similar forgetting performance. However, Figure~\ref{fig:radar chart} highlights their key differences. \NPOKL\ shows degradation on several knowledge concepts, indicating more balanced and generalized unlearning. \GAKL\ primarily unlearns "resource-development", exhibiting selective forgetting of certain concepts. For future analysis, the radar charts of two base models unlearned by the eight algorithms are provided in Figure \ref{fig:all radar chart}-\ref{fig:all radar chart_2}.



\textbf{Cognitive Diagnosis is effective in evaluating LLM unlearning.} We employ three different   cognitive diagnosis approaches. Figure~\ref{fig:compare cdm} illustrates their agreement, measured  by the Degree of Agreement (DOA) metric \citep{fouss2007random}, alongside prediction accuracy and the number of questions involved in each diagnosis method. Details of  these measures are provided in Appendix \ref{evaluate CDMs}. Our results demonstrate that these approaches produce consistent diagnostic outcomes and remain robust even when applied to diverse evaluation datasets, including hard-set questions with higher knowledge concept density, as shown in  Figure \ref{fig:compare hard}. 

\begin{wrapfigure}{r}{0.48\textwidth}
    \vspace{-20pt}
    \includegraphics[width=\linewidth]{figures/fs.pdf}
    \caption{Few-shot diagnosis results of Llama-3-8B unlearned with NPO and \NPOGD.}
    \label{fig:few-shot}
     \vspace{-40pt}
\end{wrapfigure}

In scenarios where evaluation questions are limited, the few-shot knowledge tracing shows its advantages, such as its capability of obtaining a general knowledge state with minimal queries, offering an efficient alternative. Figure \ref{fig:few-shot} shows an example of a few-shot diagnosis result.


\begin{figure}
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/compare_cdm.pdf}
        \caption{Agreement of three CDM approaches. Q is the number of questions sampled from the response logs. DOA is computed only between NCDM and ICDM, as they produce real-valued knowledge states.}
        \label{fig:compare cdm}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/compare_hard.pdf}
        \caption{Robust knowledge mastery $M_s$ with consistent values across full and hard evaluation sets, based on the same number of answer logs.}
        \label{fig:compare hard}
        
    \end{minipage}
    \vspace{-10pt}
\end{figure}


% \begin{figure}
%     \centering
%     \begin{minipage}{0.48\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/fs.pdf}
%         \caption{Few-shot diagnosis results of Llama-3-8B unlearned with NPO and \NPOGD.}
%         \label{fig:few-shot}
%     \end{minipage}%
%     \hfill
%     \begin{minipage}{0.48\textwidth}
%         In scenarios where evaluation questions are limited, the few-shot knowledge tracing shows its advantages, such as its capability of obtaining a general knowledge state with minimal queries, offering an efficient alternative. Figure \ref{fig:few-shot} shows an example of a few-shot diagnosis result.
%     \end{minipage}
% \end{figure}

% In scenarios where evaluation questions are limited, the few-shot knowledge tracing shows its advantages, such as its capability of obtaining a general knowledge state with minimal queries, offering an efficient alternative. Figure \ref{fig:few-shot} shows an example of a few-shot diagnosis result.









\section{UNCD-Agent-Continuing Unlearning}

Building on the insights of UNCD, we further develop UNCD-Agent,  a baseline agent for further removal of residual abilities in unlearning. UNCD-Agent is composed of the following two components in a \emph{test and unlearn} process: 
\vspace{-6pt}
\begin{itemize}[leftmargin=*,itemsep=0pt,parsep=0pt]
\item \textbf{Identification.} After initial unlearning, UNCD-Agent leverages UNCD  to identify specific knowledge concepts that requires further removal, in order to eradicate the undesired ability.
\item \textbf{Data Generation and Unlearning.} UNCD-Agent leverages advanced LLMs (e.g.,GPT-4o) to generate an additional dataset for targeted knowledge removal.

\end{itemize}
\vspace{-6pt}

% \begin{wrapfigure}{l}{0.48\textwidth}
%     \centering
%     \vspace{-10pt}
%     % First subfigure
%     \begin{subfigure}{0.5\textwidth}
%         \centering
%         \includegraphics[width=0.9\columnwidth]{figures/Llama_further.pdf}
%         \label{fig:Llama further}
%     \end{subfigure}
%     \hfill
%     % Second subfigure
%     \begin{subfigure}{0.5\textwidth}
%         \centering
%         \includegraphics[width=0.9\columnwidth]{figures/mistral_further.pdf}
%         \label{fig:mistral further}
%     \end{subfigure}
%     \vspace{-13pt}
%     \caption{ Continuing unlearning results of UNCD-Agent on Llamma-3-8B and Mistral-7B. "algorithm+" represents the performance of UNCD-Agent.}
%     \label{fig:further unlearn}
%      \vspace{-10pt}
% \end{wrapfigure}


\begin{figure}[h]
    \centering
    % First subfigure (Llama)
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/Llama_further.pdf}
       % \caption{Continuing unlearning results on Llama-3-8B.}
        \label{fig:Llama_further}
    \end{subfigure}
    \hfill
    % Second subfigure (Mistral)
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/mistral_further.pdf}
       % \caption{Continuing unlearning results on Mistral-7B.}
        \label{fig:mistral_further}
    \end{subfigure}
    
    \caption{Continuing unlearning results of UNCD-Agent on Llama-3-8B and Mistral-7B. "algorithm+" represents the performance of UNCD-Agent.}
    \label{fig:further unlearn}
\end{figure}

\noindent
Specifically, UNCD-Agent first identifies the unlearned LLMs that require further unlearning using Acc, where an Acc well above random (0.25) suggests unsuccessful ability removal. Then UNCD-Agent identifies the knowledge concepts for targeted removal using the diagnosed knowledge states, this can be done with human selection or statistical measurement. In our implementation, we identify Llama-3-8B unlearned with \GAKL, \NPOKL, RMU and TV, and select "privilege escalation" as the targeted knowledge concept. For Mistral-7B unlearned with \GAKL, \NPOKL\ and RMU, we identify "initial access". We curate additional unlearning data specific to these knowledge concepts detailed in \ref{UNCD-Agent data}. Figure \ref{fig:further unlearn} demonstrates that UNCD-Agent successfully reduces proficiency on the selected knowledge concepts but still suffers from a slight utility degradation.  

