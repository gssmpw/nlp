\clearpage
\section*{Appendix}

\section{UNCD Dataset collection}
\subsection{UNCD-Cyber}
\label{appendix:UNCD-Cyber}

\begin{figure*}[t]
\captionsetup{justification=centering}
\caption{Examples of MITRE \ATTCK\ objects.}
    \label{fig:mitre overview}
    
    \centering
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{figures/mitre.pdf}
        \caption{An example of domains and their corresponding techniques in the MITRE \textsc{ATT\&CK} database.}
    \end{subfigure}
        
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{figures/brute-force.pdf}
        \caption{An example of the MITRE \textsc{ATT\&CK} technique.}
        \label{fig:mitre_technique}
    \end{subfigure}
    

    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{figures/credential-access.pdf}
        \caption{An example of the MITRE \textsc{ATT\&CK} tactic.}
        \label{fig:mitre_tactic_1}
    \end{subfigure}
    

    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{figures/software_1.pdf}
        \caption{An example of the MITRE \textsc{ATT\&CK} software.}
        \label{fig:mitre_tactic_2}
    \end{subfigure}
    
\end{figure*}




Table~\ref{table:UNCD-Cyber domains} shows the statistics of the UNCD-Cyber Evaluation Dataset. We also provide our system prompt for generating UNCD-Cyber Forget Dataset and Evaluation Dataset, as shown in Figure \ref{corpus sytem prompt}-\ref{evaluation prompt}. 

% \begin{wraptable}{r}{0.4\textwidth}
% \vspace{-10pt}
% \centering
% \scriptsize
% \caption{Data stastics}
% %\caption{The impact of the classifier on the perturbation success rate of the LLMs. The full model names are: GPT-4o, Gemma-2-27B, Llama-3.1-70B, and Qwen2-5-72B. The rows display the perturbation success rate with and without the classifier.}
% \label{table:UNCD-Cyber evaluation}

% \begin{tabular}{lccc}
%     \toprule[1pt]
%     \textbf{Unlearn Dataset} & \multicolumn{2}{c}{\textbf{Forget}} & \textbf{Retain} \\ 
%     \cmidrule(lr){2-3} \cmidrule(lr){4-4}
%       \# Tokens & \multicolumn{2}{c}{\centering 2.9M} & 3.3M \\ 
%       \# Samples & \multicolumn{2}{c}{\centering 4.9k} & 8.3k \\
%       \midrule
%    \multirow{2}{*}{\textbf{Evaluation Dataset}} & \multicolumn{2}{c}{\textbf{Forget}} & \multirow{2}{*}{\textbf{Retain}}\\
%    \cmidrule(lr){2-3}
%     & \textbf{\textsc{Easy}} & \textbf{\textsc{Hard}} & \\
%     \midrule
%     \# Techniques & 100 & 82 & 23 \\
%      \# Domains & 13 & 13 & 4 \\
%     \# Questions (Q) & \(26\text{k}\) & \(8\text{k}\) & \(2\text{k}\) \\
%     \# Techniques per Q & 1 & 2.1 & 1 \\
%     \# Tokens per   Q   & 12 & 32 & 11 \\
%     \bottomrule[1pt]
%   \end{tabular}
%   \vspace{-10pt}

% \end{wraptable}

\begin{table}[h]
  \centering
  \small 
  \begin{tabular}{l c c}
    \toprule[1.5pt]
    \textbf{UNCD-Cyber} & \textbf{Techniques} & \textbf{Questions} \\
    \midrule
    \textbf{Forget Set Domains} & & \\
   
    reconnaissance & 9 & 2862 \\
    resource development & 6 & 2224 \\
    initial access & 10 & 1375 \\
    execution & 4 & 2890 \\ 
    persistence & 14 & 8290  \\
    privilege-escalation & 4 & 1338  \\
    defense-evasion & 7 & 5464  \\
    credential-access & 7 & 2482  \\
    discovery & 7 & 3163  \\
    lateral-movement & 4 & 1002  \\
    collection & 7 & 2344  \\
    command-and-control & 5 & 3057  \\
    exfiltration & 6 & 1188 \\
    impact & 8 & 1685 \\
    \midrule
    \textbf{Retain Set Domains} & & \\
    
    data structure and algorithm & 7 & 614  \\
    computer organization & 7 & 600  \\
    computer network & 6 & 399 \\
    operating system & 4 & 319  \\
    \bottomrule[1.5pt]
  \end{tabular}
  \caption{UNCD-Cyber forget set domains and retain set domains, along with the number of techniques and the number of questions in each domain.}
  \label{table:UNCD-Cyber domains}
\end{table}


In our collection of UNCD-Cyber Evaluation Dataset, we leverage the following MITRE \ATTCK\ objects:
\begin{itemize}[nolistsep, leftmargin=*]
    \item \textbf{Techniques} represent *how* an adversary achieves a tactical objective by performing an action. We leverage the detailed descriptions of each technique provided in MITRE \ATTCK\ to generate easy evaluation questions.
    \item \textbf{Tactics} represent the *reason behind* an \ATTCK\ technique or sub-technique. They define the adversary’s tactical objective—the reason for performing an action. Tactics serve as useful contextual categories for techniques.
    \item \textbf{Software} refers to real-world implementations of techniques, such as cyberattack tools or malware. Each software instance is mapped to its corresponding techniques and descriptions, which we use to generate challenging evaluation questions with rich real-world scenarios.
\end{itemize}

Figure~\ref{fig:mitre overview} illustrates some examples of MITRE \ATTCK\ objectives.



\noindent \textbf{Bloom's Taxonomy} is a hierarchical framework that classifies knowledge mastery into six levels, ranging from lower-order to higher-order: Knowledge, Comprehension, Application, Analysis, Synthesis, and Evaluation.

We also show an example of human reviewing process in Figure \ref{fig:screenshot}.


\subsection{UNCD-Agent Data Collection}
\label{UNCD-Agent data}
We leverage the collected CTI reports and additional prompts to collect data for targeted unlearning, shown in Figure \ref{further unlearn prompt}-\ref{further unlearn prompt 2}.

\input{Section/prompt}

\begin{figure*}
\captionsetup{justification=centering}

    \centering
    \includegraphics[width=1\linewidth]{figures/cdm_human_annotation.jpg}
    \caption{Screenshot of human review.}
    \label{fig:screenshot}
\end{figure*}

\section{Implementation Details}


\subsection{Unlearning Methods}
\label{appendix: unlearning methods}
We evaluate eight LLM unlearning methods that belong to four families of algorithms.

\noindent\textbf{Four families of unlearning algorithms:}
\begin{itemize}[nolistsep, leftmargin=*]
        \item \textbf{Gradient Ascent (GA)} \citep{thudi2022unrolling} minimizes the likelihood of correct predictions on the forget set \( D_{\mathit{f}} \) by performing gradient ascent on the cross-entropy loss. The objective is given by:
        \begin{align*}
        L_{\text{GA}}(\theta) &= - \mathbb{E}_{(x, y) \sim D_{\mathit{f}}} 
        \Big[ -\log f_\theta(y|x) \Big] \\
        &= \mathbb{E}_{(x, y) \sim D_{\mathit{f}}} 
        \Big[ \log f_\theta(y|x) \Big],
        \end{align*}

        \item \textbf{Negative Preference Optimization (NPO)} \citep{zhang2024negative} treats the forget set as negative preference data and adapts the offline DPO \citep{rafailov2024direct} objective to tune the model to assign low likelihood to the forget set without straying too far from the original model \( f_0 \). The objective is given by:
        \[
        L_{\text{NPO}}(\theta) = - \frac{2}{\beta} \mathbb{E}_{x \sim D_{\mathit{f}}} 
        \Big[ \log \sigma \big( -\beta \log \frac{f_\theta(x)}{f_0(x)} \big) \Big],
        \]
        where \( f_\theta \) refers to the model that undergoes unlearning, \( \sigma \) is the sigmoid function, and \( \beta \) is a hyperparameter that controls the allowed divergence of \( f_\theta \) from the original model \( f_0 \). We fix \( \beta = 0.1 \) in our experiments following previous works \citep{shi2024muse,zhang2024negative}.
    
       
        \item \textbf{Representation Misdirection for Unlearning (RMU)} \citep{li2024wmdp} is a method that perturbs model activation on the forget set \( D_{\mathit{f}} \) and preserving activations on the retain set \( D_{\mathit{r}} \). The forget loss in RMU weakens the model’s response to  \( D_{\mathit{f}} \) by increasing activation norms in the initial model layers, and the retain loss aims to preserve the model's utility by maintaining activations close to those of the backbone model. This method is based on the finding that increasing the norm of the model’s activations on hazardous data in earlier layers makes it difficult for later layers to process those activations effectively \citep{li2024wmdp}. 
    
        \( M_u(\cdot) \) and \( M_f(\cdot) \) denote the hidden states of the unlearned model and the original, frozen model, at some layer \(\ell\). The forget loss $L_f$ and retain loss $L_r$ are defined as:
        
        \[
        L_f = \mathbb{E}_{x_f \sim D_f} 
        \Bigg[ \frac{1}{l_f} \sum_{t \in x_f} \Big\| M_u(t) - c \cdot u \Big\|^2 \Bigg],
        \]
        
        \[
        L_r = \mathbb{E}_{x_r \sim D_r} 
        \Bigg[ \frac{1}{l_r} \sum_{t \in x_r} \Big\| M_u(t) - M_f(t) \Big\|_2^2 \Bigg],
        \]
        
        where \( l_f \) is the number of tokens in \( x_f \), \( l_r \) is the number of tokens in \( x_r \), and \( c \) is a hyperparameter that controls activation scaling.
        
        The full loss of RMU is a weighted combination of the forget loss and the retain loss:
        
        \[
        L = L_f + \alpha \cdot L_r.
        \]
        \item \textbf{Task Vectors (TV)} \citep{ilharco2022editing} are derived through straightforward arithmetic on the model weights. Using task vectors for unlearning includes first fine-tuning the backbone model \( f_0 \) on \( D_{\mathit{f}} \) to obtain a reinforced model \( f_{\text{reinforce}} \), and then obtaining a task vector by subtracting \( f_{\text{reinforce}} \) and \( f_0 \). Finally, the task vector is scaled by a factor \( \alpha \) and subtracted from \( f_0 \)'s weights:
        \[
        f_{\text{unlearn}} = f_0 - \alpha \cdot (f_{\text{reinforce}} - f_0).
        \]
        % For our experiment, we set \(\alpha = 1\) and compute the loss only on layer \(\ell\), and update gradients only on layers \(\ell - 2\), \(\ell - 1\), and \(\ell\). We set \(\ell = 7\) and follow the parameter settings in \citep{li2024wmdp}.
    
    \end{itemize}

\noindent\textbf{Two regularizers for utility preservation} 
\begin{itemize}[nolistsep, leftmargin=*]
    \item \textbf{Gradient Descent on the Retain Set (GDR)} \citep{maini2024tofu,zhang2024negative} augments the unlearning objective with a standard gradient descent learning objective on the cross-entropy of the retain set \( D_r \) to more directly train the model to maintain its performance on \( D_r \).
    
    \item \textbf{KL Divergence Minimization on the Retain Set (KLR)} \citep{maini2024tofu,zhang2024negative} encourages the output distribution of the unlearned model \( f_\theta \) to be close to the output distribution of the backbone model \( f_0 \) on the retain set \( D_r \).
\end{itemize}

\noindent Combining GA and NPO with regularizers GDR and KLR, we obtain the eight unlearning algorithms: GA, \GAGD, \GAKL, NPO, \NPOGD, \NPOKL, RMU, and TV.






\subsection{Unlearning and Logging}
\label{appendix: unlearning-logging}

We conduct unlearning experiments using the eight algorithms and the UNCD-Cyber Unlearn Dataset. For the unlearning methods GA, \GAGD\, \GAKL\, NPO, \NPOGD\, and \NPOKL\, we adopt parameter settings consistent with the implementation in \textbf{MUSE} \citep{shi2024muse}. For the RMU method, we follow the parameter configuration used for unlearning \text{ZEPHYR-7B} \citep{tunstall2023zephyr} in \textbf{WMDP} \citep{li2024wmdp}. Across these methods, we unlearn for an epoch and divide the epoch into four equal steps. For instance, in an epoch comprising 1,200 iterations, we checkpoint the model every 300 iterations.

For the Task Vector method, we retain the fine-tuning settings from MUSE and fine-tune the model on our forget set. We set $\alpha=5$ to scale the forgetting effect, and checkpoint the model after 2, 3, 4, and 5 epochs of fine-tuning, subsequently applying Task Vector unlearning.

To log the LLM outputs, we follow the standard zero-shot QA evaluation format \citep{eval-harness}. Specifically, we select the top logit among the four answer choices as the predicted response.




\begin{figure*}[t]
\begin{tcolorbox}[
    sharpish corners,
    boxrule = 0pt,
    toprule = 0pt,
    enhanced
]
\small % Smaller font size for the content
\textbf{Prediction 1:}  

\begin{itemize}[nolistsep, leftmargin=*]
    \item \textbf{Pre Exercise ID:} 2314
    \item \textbf{Prediction:} 0
    \item \textbf{Explanation:}  
    \begin{enumerate}
        \item The new exercise contains initial-access, defense-evasion, discovery.
        \item It's a new question, there is some kind of connection between the previous knowledge points and questions.
        \item \textbf{Knowledge State:}  
        \begin{itemize}[nolistsep, leftmargin=*]
            \item privilege-escalation: fair
            \item discovery: bad
            \item execution: bad
            \item impact: unknown
            \item initial-access: unknown
            \item defense-evasion: unknown
        \end{itemize}
        \item The student gets it wrong, likely because of incorrect mastery.
    \end{enumerate}
    \item \textbf{Is Correct:} 0
\end{itemize}

\textbf{Prediction 2:}  

\begin{itemize}[nolistsep, leftmargin=*]
    \item \textbf{Pre Exercise ID:} 10
    \item \textbf{Prediction:} 0
    \item \textbf{Explanation:}  
    \begin{enumerate}
        \item The new exercise contains defense-evasion, collection.
        \item Similar to question 2842, 3515, 1420, there is a connection between the previous knowledge points and questions.
        \item \textbf{Knowledge State:}  
        \begin{itemize}[nolistsep, leftmargin=*]
            \item defense-evasion: good
            \item credential-access: bad
            \item command-and-control: bad
            \item persistence: bad
            \item collection: bad
        \end{itemize}
        \item The student gets it wrong, likely because of incorrect mastery. The student has previously shown a lack of understanding in related knowledge concepts, indicating a need for further study in these areas.
    \end{enumerate}
    \item \textbf{Is Correct:} 0
\end{itemize}

\textbf{Prediction 3:}  

\begin{itemize}[nolistsep, leftmargin=*]
    \item \textbf{Pre Exercise ID:} 648
    \item \textbf{Prediction:} 1
    \item \textbf{Explanation:}  
    \begin{enumerate}
        \item Find out the knowledge points involved in the new question: The new exercise contains initial-access.
        \item Analyse the link between the question and the topic in the student's record of work: Similar to question 3105, there is a direct connection between the previous knowledge points and questions.
        \item \textbf{Knowledge State:}  
        \begin{itemize}[nolistsep, leftmargin=*]
            \item privilege-escalation: good
            \item lateral-movement: fair
            \item initial-access: good
            \item persistence: fair
        \end{itemize}
        \item The student gets it right, likely because of mastery.
    \end{enumerate}
    \item \textbf{Is Correct:} 1
\end{itemize}

\end{tcolorbox}
\caption{Examples of student performance prediction and knowledge state analysis process using few-shot knowledge tracing.}
\label{few shot example}
\end{figure*}

\subsection{Cognitive Diagnosis Models}

\label{appendix: CDM}
CDMs give real-valued student knowledge states leveraging \(R\) and \(Q\). These models encode the student factor \(\theta\) (representing student ability) and the exercise factor \(\beta\) (capturing attributes such as difficulty and knowledge concepts), along with other model-specific parameters \(\Omega\). Then, following the monotonicity assumption \citep{ackerman2014multidimensional}, an \emph{interaction function} \(f\) is used to predict the probability of a correct response \(p\) for a given exercise, expressed as:
    \( p = f(\theta - \beta + \Omega),\) 
where the exact form of \(f\) depends on the specific CDM. After training the CDM based on student performance prediction, student knowledge states \(F_{sk}\) is derived from the latent factor \(\theta\). 
We leverage the Neural Cognitive Diagnosis Model (NCDM) \citep{wang2020neural} and the Inductive Cognitive Diagnosis Model (ICDM) \citep{liu2024inductive} to reveal LLM latent knowledge states. NCDM uses one-hot embeddings to encode student and exercise factors, while ICDM constructs a student-centered graph that incorporates student information and their neighbors. To enhance the graph construction and modeling process, we perform data augmentation by randomly sampling each LLM's response logs to simulate a large number of new students and their answer logs. Implementation details can be found in Appendix \ref{appendix: CDM}. 

\begin{itemize}[nolistsep, leftmargin=*]
    \item For the NCDM model, we adopt the implementation settings described in \citet{wang2020neural}.
    \item For the ICDM model, we first perform data augmentation by randomly sampling each LLM's answer logs into new, synthetic students, increasing the performance of the graph-based model. Then, We follow the configurations in \citet{liu2024inductive}, setting each student's k-hop number to 3 and employing a neural network as the interaction function.
    
    \item For few-shot knowledge tracing, we adopt the experimental setup proposed by \citet{li2024explainable}, utilizing GPT-4o as the LLM evaluator and performing random four-shot knowledge tracing. During the diagnosis process, we evaluate the knowledge state descriptions by assigning scores to the diagnosed states: "good" is assigned a score of 1, "bad" a score of -1, and "fair" is a score of 0. These scores are accumulated at each step of the process to produce an overall assessment of the knowledge state. An example of few-shot knowledge tracing process is shown in Figure \ref{few shot example}.
    
\end{itemize}


\noindent \textbf{Evaluating CDMs}
\label{evaluate CDMs}
We evaluate CDMs using the prediction accuracy on student performances. For the NCDM and ICDM model that gives real-valued knowledge states, we use the Degree of greement (DOA) metric \citep{fouss2007random} to evaluate the reliability of the diagnosed knowledge states.
For knowledge concept \(k\), \(DOA(k)\) is formulated as:
\[
\begin{aligned}
DOA(k) &= \frac{1}{Z} \sum_{a=1}^{N}\sum_{b=1}^{N} \delta(F_{ak},F_{bk}) Q_{abk}, \\
Z &= \sum_{a=1}^{N}\sum_{b=1}^{N} \delta(F_{ak},F_{bk}),
\end{aligned}
\]
where \(Z\) is the normalization factor that accounts for the total number of valid comparisons, and the submetric \(Q_{abk}\) is defined as:

\[
Q_{abk} = \sum_{j=1}^{M} I_{jk} 
\frac{J(j,a,b) \land \delta(r_{aj},r_{bj})}{J(j,a,b)}.
\]

Here, \(F_{ak}\) denotes the proficiency of student \(a\) on knowledge concept \(k\), while \(\delta(x,y)\) is an indicator function equal to \(1\) if \(x > y\) and \(0\) otherwise. \(I_{jk}\) indicates whether exercise \(j\) involves knowledge concept \(k\) (\(I_{jk} = 1\)) or not (\(I_{jk} = 0\)). Similarly, \(J(j, a, b)\) indicates whether both students \(a\) and \(b\) attempted exercise \(j\) (\(J(j, a, b) = 1\)) or not (\(J(j, a, b) = 0\)). The submetric \(Q_{abk}\) quantifies the agreement between students \(a\) and \(b\) on exercises involving knowledge concept \(k\), considering whether both attempted the same exercise and whether their responses align (based on \(\delta(r_{aj}, r_{bj})\)).

Averaging \(DOA(k)\) across all knowledge concepts evaluates the overall reliability of the diagnosed knowledge states.




\subsection{Evaluation Criteria}
\label{appendix:evaluation criteria}
We define our evaluation criteria as follows: The LLM after unlearning should achieve effective forgetting on the unlearn target while preserving benign knowledge and model utilities. 

\noindent \textbf{Forget Performance} is measured as the reduction of the forget knowledge states defined in UNCD-Cyber. Given the extensive number of techniques in the benchmark, we conduct domain-level cognitive diagnosis, using the NCD model and ICDM model to mine the knowledge states of LLMs across the domains. We also use few-shot knowledge tracing and record the system's description of the knowledge states. The knowledge states derived from these methods are referred to as: \textbf{NCD-ks}, \textbf{ICDM-ks}, and \textbf{FS-ks}, where NCD-ks and ICDM-ks are the average knowledge states of each LLM, and FS-ks represents the diagnosed mastery level in few-shot knowledge tracing. 

Using the NCD model, we sample 5,000 questions from UNCD-Cyber across different domains. The ICDM model requires only around 2,500 questions to achieve a fair diagnostic result, while we randomly sample 100 questions for the few-shot method.

\noindent\textbf{Retain Performance} is evaluated across three dimensions: in-domain knowledge, general knowledge, and fluency, which are essential capabilities that LLMs should maintain post-unlearning.

\begin{itemize}[nolistsep, leftmargin=*]
    \item \textbf{In-domain knowledge} refers to the benign knowledge proximate to the forget set. When removing harmful computer science-related knowledge, the model should preserve its capability on harmless and general computer science knowledge. We utilize the retain evaluation questions in UNCD-Cyber to assess model's knowledge retention of predefined computer science concepts. Since each evaluation question is designed to test a single knowledge concept, the accuracy on these questions serves as a representative measure of the corresponding knowledge states. 
    \item \textbf{General knowledge} is LLM's general world knowledge and we employ the MMLU benchmark \citep{hendrycks2020measuring} to quantitatively evaluate this dimension. The MMLU benchmark is a widely adopted evaluation framework designed to assess knowledge across a diverse range of subjects, spanning disciplines such as humanities, mathematics and science. The LLM's general knowledge is measured by its average accuracy across all MMLU subjects.
    \item \textbf{Fluency} evaluates the model's conversational proficiency and assitant ability. We utilize MT-Bench \citep{zheng2023judging}, which assigns fluency scores on a scale from 1 to 10, where a score of 1 represents incoherent output with minimal utility as an assistant.
\end{itemize}



\subsection{Additional Experiment Results}

We compute 95\% confidence intervals of the average knowledge states NCD-ks and ICDM-ks, as shown in Table \ref{tab:confidence}. We also represent the radar chart for all algorithms in Figure \ref{fig:all radar chart}-\ref{fig:all radar chart_2}, and the diagnosed knowledge states on all knowledge concepts in Figure \ref{fig:all llama ncdm}-\ref{fig:all mistral icdm}.


\begin{table}[t!]
    \centering
    \small
    \renewcommand{\arraystretch}{0.95} % Adjust line spacing

    \begin{tabular}{
        l
        S[table-format=2.2]
        l
        S[table-format=2.2]
        l
    }
    \toprule[1.5pt]
    {} & \multicolumn{2}{c}{\textbf{NCDM-ks$\downarrow$}} & \multicolumn{2}{c}{\textbf{ICDM-ks$\downarrow$}} \\ 
    \cmidrule(lr){2-3} \cmidrule(lr){4-5}
    & {Mean} & {95\% CI} & {Mean} & {95\% CI} \\
    \midrule
    \textbf{LLaMA-3 8B}          & 57.26 & {[56.19, 58.33]} & 69.84 & {[67.73, 71.05]} \\ 
    \quad \textbf{+GA}           & 7.83  & {[6.46, 9.20]}   & 9.87  & {[7.36, 12.40]}  \\
    \quad \textbf{+\GAGD}        & 21.06 & {[20.47, 21.65]} & 12.26 & {[8.17, 16.34]}  \\
    \quad \textbf{+\GAKL}        & 53.91 & {[52.98, 54.85]} & 68.12 & {[64.00, 72.24]} \\ 
    \midrule
    \quad \textbf{+NPO}          & 39.99 & {[39.13, 40.85]} & 50.47 & {[48.75, 52.20]} \\
    \quad \textbf{+\NPOGD}       & 48.02 & {[47.10, 48.94]} & 67.25 & {[63.24, 71.25]} \\
    \quad \textbf{+\NPOKL}       & 48.77 & {[45.82, 51.71]} & 65.97 & {[62.00, 69.98]} \\
    \midrule
    \quad \textbf{+RMU}          & 67.43 & {[64.40, 70.48]} & 67.43 & {[64.40, 70.48]} \\
    \quad \textbf{+TV}           & 68.71 & {[65.41, 72.01]} & 68.71 & {[65.41, 72.01]} \\ 
    \midrule
    \textbf{Mistral 7B}          & 59.44 & {[58.10, 60.79]} & 72.59 & {[72.41, 72.76]} \\ 
    \quad \textbf{+GA}           & 16.27 & {[14.69, 17.84]} & 3.67  & {[33.94, 39.54]}  \\
    \quad \textbf{+\GAGD}        & 29.72 & {[27.83, 31.62]} & 9.93  & {[8.48, 11.39]}   \\
    \quad \textbf{+\GAKL}        & 56.04 & {[54.10, 57.98]} & 71.81 & {[68.85, 74.77]}  \\ 
    \midrule
    \quad \textbf{+NPO}          & 21.48 & {[18.45, 24.51]} & 37.38 & {[2.209, 5.267]}  \\
    \quad \textbf{+\NPOGD}       & 44.10 & {[43.573, 44.629]} & 45.14 & {[44.821, 45.468]} \\
    \quad \textbf{+\NPOKL}       & 56.62 & {[55.613, 57.641]} & 71.90 & {[70.055, 73.746]} \\
    \midrule
    \quad \textbf{+RMU}          & 52.37 & {[51.201, 53.549]} & 69.07 & {[66.950, 71.191]} \\
    \quad \textbf{+TV}           & 38.90 & {[37.587, 40.213]} & 27.65 & {[26.409, 28.905]} \\
    \bottomrule[1.5pt]
    \end{tabular}
    \caption{95\% confidence intervals of NCDM-ks and ICDM-ks, scaled by percentage. Lower values indicate better performance.}
    \label{tab:confidence}
\end{table}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/llama_ncd.pdf}
    \caption{All forget knowledge states of Llama-3-8B unlearned with eight algorithms, diagnosed by NCDM.}
    \label{fig:all llama ncdm}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/llama_icdm.pdf}
    \caption{All forget knowledge states of Llama-3-8B unlearned with eight algorithms, diagnosed by ICDM.}
    \label{fig:all llama icdm}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/mistral_ncd.pdf}
    \caption{All forget knowledge states of Mistral-7B unlearned with eight algorithms, diagnosed by NCDM.}
    \label{fig:all mistral ncdm}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/mistral_icdm.pdf}
    \caption{All forget knowledge states of Mistral-7B unlearned with eight algorithms, diagnosed by ICDM.}
    \label{fig:all mistral icdm}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/llama_radars.pdf}
    \caption{Changes of knowledge stats as Llama-3-8B undergoes the eight unlearning methods on four unlearning steps.}
    \label{fig:all radar chart}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/mistral_radars.pdf}
    \caption{Changes of knowledge stats as Mistral-7B undergoes the eight unlearning methods on four unlearning steps.}
    \label{fig:all radar chart_2}
\end{figure*}