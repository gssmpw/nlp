
\section{Related Work}
\subsection{Intent Elicitation in Human-AI Interaction}

In the field of Human-Computer Interaction (HCI), the concept of Human-AI interaction \cite{amershi_guidelines_2019} and AI as co-creators \cite{davis_enactive_2015, oh_lead_2018} highlight both the opportunities and challenges presented by GenAI models. 
Opportunities lie in automating repetitive and tedious tasks, such as data transformation or formatting, and augmenting creative processes, such as code, text, or image creation \cite{chen_evaluating_2021,brown_language_2020,ramesh_hierarchical_2022, saharia_photorealistic_2022}, while challenges include aligning AI outputs with user intent and users' uncertainties about effective prompts and AI systemâ€™s capabilities \cite{chen_evaluating_2021,zamfirescu-pereira_why_2023}.

A core challenge in designing effective interactive GenAI-driven systems is ensuring that AI-generated content matches user intentions. 
This involves two main aspects: the user communicating their intent to the system and the system interpreting the user's intent or guiding them in gradually disclosing it (intent elicitation). 
    
Traditionally, \textbf{wizard interfaces} have emerged to guide users through predefined option GUI dialogues to elicit their intent step-by-step for complex tasks \cite{cooper_face_2014}. 
However, natural language processing (NLP) advancements have introduced text-prompting-based GenAI models, such as large-language foundation models (LLMs), that allow users to input their intent as \textbf{free-form text prompts} \cite{liu_pretrain_2021} or guide users through a \textbf{dialogue} to elicit their intents \cite{park_user_2023,sahijwani_adaptive_2022, cai_predicting_2020, qu_user_2019}. 
    
However, all such approaches present opposing trade-offs (see Table \ref{tab:ie_comparision}): 
GUI-based menus or wizards can intuitively guide user decisions and reduce ambiguity while allowing for continuous representation of options through graphical manipulatable elements, but they only offer a finite set of options that reflect pre-anticipated use case scenarios by software makers. 
Free-form text prompting allows users to communicate their intent more flexibly using natural language, but the lack of scaffolding and guidance requires users to know what they want (or need) and how to formulate it as an effective prompt.
While easy for simple tasks, communicating intents can be challenging in complex content creation tasks when users might not be aware of all necessary options upfront and are required to iteratively figure these out (intent exploration). 
As a compromise, natural language chat dialogues can provide system-guided option exploration and scaffolding. 
However, such systems can only probe on a limited set of options in a reasonable time frame or number of conversational turns, and they often lack continuous option representation and graphical manipulation. 

On the other hand, effectively aligning AI-generated content with user intentions also requires awareness of one's own intentions and understanding of the possible parameter spectrum \cite{TerryCreativeNeedsUIDesign2002}.
\rev{Recent work in AI-mediated intent elicitation and sensemaking has explored various strategies and GenAI-driven tools to assist users in more open-ended and iterative ways to elicit and discover their (creative) intent. 
For example, systems like \textit{Luminate} \cite{suh_luminate_2024}, \textit{Selenite} \cite{liu_selenite_2024}, \textit{Sensecape} \cite{suh_sensecape_2023} and \textit{Graphologue} \cite{jiang_graphologue_2023} utilize dynamic LLM-driven GUIs to support users in sensemaking and intent elicitation (e.g., exploring a design space, available options, or topics) by generating and visually clustering semantically related concepts or criteria to help them explore relevant dimensions and parameters matching their tacit intent and needs. 
Similarly, \textit{SymbolFinder} \cite{petridis_symbolfinder_2021} offers different LLM-driven GUIs to suggest and group related words and images to guide designers in gradually exploring and finding appropriate symbols to illustrate abstract concepts.
Other work by \textit{Kreminski and Chung} \cite{kreminski_intent_2024} uses language models and micro-interactions to elicit users' creative intent by formulating and asking open-ended questions about aspects of a game creator's emerging story world.
Similarly, \textit{Germinate} \cite{kreminski_germinate_2020}, a system for game generation, features an interface to help break down the design intents of casual creators by allowing users to specify game design-related parameters, such as game entities or resources through graphical text-based tags that users can freely define. 
Based on these user-defined tags, the system attempts to infer additional appropriate tags that users might incorporate or negate (cf.~\cite{martens_languages_2017}). 
Similarly, Lin et al. \cite{lin_prompts_2023} outlined a design space for user interactions with co-creative systems, highlighting the role of explicit communication mechanisms about creative intent, which might be initiated by either the user or the system (mixed-initiative).}

\rev{From a broader cognitive perspective}, self-awareness \rev{about one's intentions} relates to \textit{metacognition} \cite{flavell_metacognition_1979}, and recent research by \textit{Tankelevitch et al.} \cite{tankelevitch_metacognitive_2023} have proposed a stronger focus on supporting users' metacognitive processes, such as \textit{self-awareness} and \textit{task decomposition}, for mitigating challenges of effective GenAI workflows. 
In a recent design fiction, \textit{Vaithilingam et al.} \cite{vaithilingam_imagining_2024} explore metacognitive support using LLM dialogue-based guided intent exploration inspired by human-human communication patterns such as dynamic grounding, constructive negotiation, and sustainable motivation to support the design process. 

Previous work has explored various \rev{AI-mediated} user intent elicitation approaches and proposed metacognitive user support to improve GenAI-driven tasks. 
Building atop this prior work, we aim to derive interaction principles for human-GenAI co-creation that combine the benefits of existing intent elicitation interfaces without their drawbacks while supporting users' intent exploration process through metacognitive support. 


\subsection{Systems for Supporting Prompting and Steering GenAI}

Generative AI \textit{foundation} models, such as large language or diffusion models \cite{bommasani_opportunities_2022,rombach_highresolution_2022}, represent a paradigm shift in artificial intelligence, offering task-agnostic pre-training on large-scale data for various downstream applications \cite{schneider_foundation_2022}. 
Although extremely versatile, research suggests three properties that make interacting with GenAI challenging: \textit{input flexibility} (in handling free-form language, images, code, etc.), \textit{generality} (applicability to a wide range of tasks), and \textit{originality} (ability to generate novel content) \cite{schellaert_your_2023}.

Prior HCI research has documented interaction challenges in prompting and steering GenAI systems across domains such as coding, illustration design, or engineering \cite{liu_what_2023,liu_opal_2022, gmeiner_exploring_2023}. 
For example, users frequently struggle to craft text-based input prompts that will achieve desired outcomes, and face difficulties interpreting and repairing erroneous outputs \cite{zamfirescu-pereira_why_2023}.
Research has begun to explore mechanisms and interfaces to better support users in working with text prompt-based GenAI models. 
For example, various works have proposed mechanisms to support users in \textit{prompt engineering} \cite{liu_pretrain_2021} through \textit{"prompt augmentation,"} which automatically modifies and extends a users' input prompt to improve the model's generated output \cite{brade_promptify_2023, shin_autoprompt_2020, betker_improving_2023}.
While such techniques can improve model output quality, users are bound to express their intent in an open-ended text format. 

Other work has explored ways of \textit{combining direct manipulation interfaces with text-based GenAI systems} to enable more structured user inputs and reduce semantic ambiguity. For example, \textit{GhostWriter} \cite{yeh_ghostwriter_2024} offers buttons for predefined prompts for LLM-driven text manipulation tasks. 
While such approaches reduce misalignment and improve feature discoverability by visually exposing model capabilities, they also restrict the available task options to predefined sets and limit the GenAI models' generality.

Recent work has also proposed contextually bounded prompting mechanisms to offer more fine-grained control over Gen-AI-driven content generation. 
\textit{DirectGPT} \cite{masson_directgpt_2024} explores direct manipulation principles for LLMs, such as the continuous representation of manipulatable objects, physical actions to localize the effect of prompts, and reusable prompts in a toolbar.
Similarly, \textit{Cococo} \cite{louie_noviceai_2020} offers users a set of GUI widgets for steering GenAI music generation, including slider elements to nudge music generation in high-level directions.
Other ideas include painting-like interactions, such as how \textit{PromptPaint} \cite{chung_promptpaint_2023} enables users to steer diffusion-based text-to-image generation through paint medium-like interactions and \textit{TaleBrush} \cite{chung_talebrush_2022} supports generative story ideation through line sketching interactions to graphically steer LLM-driven story generation.

Building on this work, we contribute to steering interactions for GenAI by investigating mixed interfaces that blend free text input with dynamic graphical elements.
We propose intent tagging as graphical micro-prompting interactions to flexibly support intent expression and elicitation using GUI widgets to allow exploration across various levels of ambiguity. 


\subsection{Non-Linear Content Creation and Iterative Design Workflows}
Creating rich content documents, such as blog posts or slide presentations, involves crafting and integrating text, visuals, and multimedia to convey complex ideas while maintaining coherence and audience engagement.
Guidelines for creating slide presentations often suggest standardized workflows such as crafting outlines before creating slides \cite{reynolds_presentation_2020,zanders_presentation_2018, anholt_dazzle_2006}. 
However, other literature emphasizes that slide-creation workflows are less rigid and are mostly shaped by cultural factors and organizational norms \cite{yates_powerpoint_2007}. 
For example, in organizations, people often start or continue presentations from different starting points, such as from existing documents, other slide decks, or templates. 

Rich content creation processes are also iterative and diverse in nature, typically progressing through cyclically occurring stages \cite{baker_ideas_2010, dorta_signs_2010}, where creators alternate between \textit{â€˜explorationâ€™} and \textit{â€˜exploitationâ€™} in a sense-making process to reach a final outcome \cite{pirolli_sensemaking_2005}.
Content creators continuously develop, reflect on, and act on their ideas and plans \cite{kolko_sensemaking_2010, schon_reflective_1983}. 
The interaction between users' perception of the material and the material itself enables creators to develop, reflect on, and question their understanding, leading to new ideas and improved plans \cite{klein_making_2006}. 
Research has underscored the role of reflection in content creation as a necessary moment for creators to situate their ideas and plans in the appropriate context \cite{dove_argument_2016,mols_informing_2016,sharmin_reflect_2011a}.

However, existing challenges around prompt formulation engage users in cognitively demanding trial-and-error processes and hinder reflection related to content creation. 
Furthermore, many of todayâ€™s GenAI systems assume a specific workflow or require users to adapt their processes to integrate GenAI effectively \cite{tankelevitch_metacognitive_2023, sarkar_exploring_2023}. 

In conclusion, despite recommended workflows for content creation (such as starting with an outline), content creation processes are typically highly situational, non-linear, and iterative in nature. Therefore, we aim to derive interfaces for GenAI-driven content creation systems to promote non-linear and diverse workflows while supporting reflection on the content creation process.  

\subsection{Systems for Supporting Slide Deck Creation}

Creating slide presentations is a multifaceted task that involves various sub-tasks, and previous research has introduced a range of approaches to support different stages of this process.

 Besides popular slide-by-slide authoring tools like PowerPoint or Keynote, research has suggested interfaces to support users in \textbf{iterative slide creation workflows} by managing multiple presentation versions \cite{drucker_comparing_2006}, prototyping slide decks using markup language \cite{edge_hyperslides_2013} or authoring slide presentations on a 2D canvas interface \cite{lichtschlag_fly_2009} for supporting non-sequential workflows.

Besides manual authoring systems, numerous works have explored ways to \textbf{automatically generate slide presentations} and rich content documents for single topics \cite{winters_automatically_2019a}, or scientific \cite{Fu2022DOC2PPT}, technical \cite{M2009SlidesGen}, educational \cite{costa_smartedu_2023}, or semantically annotated source documents \cite{masao_automatic_1999}. Other systems focus on automatically generating presentation-specific aspects, such as visual-textual layouts \cite{yang_automatic_2016}.
While these systems allow for automatic content transformation and slide creation, they lack methods for users to influence or control the generation process.  

Several works have, therefore, proposed mechanisms to enable users to \textbf{generate presentations in more controllable and workflow-integrated ways}, such as allowing data scientists to steer the generation of slide presentations from Jupyter notebooks by interactively selecting code cells and slide layouts \cite{wang_slide4n_2023} or through a user-controllable text outline \cite{wang_outlinespark_2024}. 
Similarly, \textit{Knowledge-Decks} \cite{christino_knowledgedecks_2022} allows users to generate slides documenting data science knowledge-discovery processes from automatically collected user behavior events from visual analytic tools. 


Previous work has explored various authoring interfaces, automatic slide generation from source documents, and user-controllable slide generation in domain-specific applications like data science. Expanding on these approaches, we aim to explore interaction principles for GenAI-supported slide deck creation across diverse content tasks and workflows. Our approach combines GenAI-driven slide generation from multiple input types (e.g., prompts, text documents, images) with \textit{non-predefined} and \textit{non-linear authoring workflows} in mind that let users seamlessly switch between generation and editing modes across outline, individual slide, or entire deck level.















