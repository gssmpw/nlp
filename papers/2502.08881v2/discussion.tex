\section{Discussion}\label{sec:discussion}

The WENDy-MLE algorithm presented here addresses a more general class of problems than the previous WENDy-IRLS in \cite{BortzMessengerDukic2023BullMathBiol}. In particular, WENDy-MLE can handle systems of ODEs which are nonlinear in the parameters. Furthermore, our method can leverage the distribution for both cases of additive Gaussian Noise and multiplicative log-normal noise. Overall, the WENDy-MLE algorithm estimates parameters more accurately and fails less often than previous weak form methods. 

In Table \ref{table:summary}, we summarize a qualitative takeaways from our numerical experimentation. 
\renewcommand*\arraystretch{1.75}
\begin{table}[H]
    \resizebox{\textwidth}{!}{
    \begin{tabular}{cccccc}
        \toprule
        &  &   & \multicolumn{3}{c}{\textbf{Noise}}  \\
        \cline{4-6}
         \multirow{-2}{*}{\textbf{Algorithm}} &
        \multirow{-2}{*}{\textbf{Chaotic}} & \multirow{-2}{*}{\textbf{Nonlinear In Parameters}} & Noiseless &   Gaussian &  LogNormal \\
        \midrule
        OE-LS & \setcounter{footnote}{0}\xmark\footnotemark & \checkmark & \checkmark & \checkmark & \checkmark\\       
        
        W-LS & \checkmark & \setcounter{footnote}{1}\xmark\footnotemark  & \checkmark\checkmark & \xmark  & \setcounter{footnote}{1}\xmark\footnotemark \\       
        
        WENDy-IRLS & \checkmark\checkmark & \setcounter{footnote}{1}\xmark\footnotemark  & \setcounter{footnote}{2}\checkmark\footnotemark & \checkmark & \setcounter{footnote}{1}\xmark\footnotemark \\       
        
        WENDy-MLE & \checkmark\checkmark  & \setcounter{footnote}{3}\checkmark\checkmark\footnotemark & \setcounter{footnote}{2} \checkmark \footnotemark & \checkmark\checkmark  & \checkmark\checkmark \\  
        \bottomrule
    \end{tabular}
    }
	\caption{Here we provide high-level advice on the performance of the discussed methods applied to different classes of problems. We compare OE-LS, solving ordinary least squares for the weak residual (W-LS), WENDy-IRLS, and  WENDy-MLE. A double check \checkmark\checkmark\, indicates the algorithm excels under most circumstances in our testing. A single \checkmark\, suggests that the algorithm can work, but requires oversight due to a reduced domain of convergence for initial parameter estimates as well as other numerical discretization challenges. An \xmark\, indicates that the algorithm is not suited to the problem (e.g., W-LS and WENDy-IRLS only work for linear-in-parameter ODEs and without correction, noisy data will incur bias for W-LS).}\label{table:summary}
\end{table}
\setcounter{footnote}{1}
\footnotetext{OE-LS can work on short time horizons but in general should not be used for chaotic systems.}
\setcounter{footnote}{2}
\footnotetext{In the Julia implementation, both of these algorithms have been extended by replacing the least square problems with a nonlinear least squares problems. Similarly, the infrastructure makes it possible to run these algorithms with log-normal noise as well.}
\setcounter{footnote}{3}
\footnotetext{Because WENDy-IRLS and WENDy-MLE assume measurement noise is present, computation of the covariance is ill-posed for noiseless data. Our software implementation automatically switches to W-LS in this case. In some instance we have seen that incorporating covariance information to estimate parameters for noiseless data can be superior to W-LS. We conjecture the algorithms may be correcting for the numerical error in the solver that generated the artificial data, but this remains a topic for future research.}
\setcounter{footnote}{4}
\footnotetext{For the vast majority of the examples, the domain of convergence is substantially larger than for OE-LS. However, as with all iterative methods, WENDy-MLE benefits from wise initial parameter estimates.}

There are known limitations to our approach. First, WENDy-MLE is based on maximizing the likelihood of the weak form residual. Accordingly, the form of the ODE must be such that integration-by-parts is possible.  We also assume that the measurement noise is either additive Gaussian noise or multiplicative log-normal noise. We also require that all state variables are observed, which is not widely true in many applications. Another limitation is that we assume observational data is provided on a uniform grid, though in principle, there is nothing inhibiting extension to the non-uniform case with a different quadrature rule. Finally, like almost all other methods that do general parameter estimation, we face a non-convex optimization problem for which we cannot guarantee finding the global minimizer for arbitrary initialization. Thus, in practice it is often necessary to initialize multiple times for best results.

\subsection{Robustness in the Presence of Noise}
Like other weak form methods, the true strength of the approach lies in the ability to perform well in the presence of noise\footnote{Smoothing can help (up to a point), see \cite{MessengerBortz2024IMAJNumerAnal}.}. No pre-processing or smoothing of input data is necessary for the algorithm to be successful in the presence of noise. Instead, the set of test functions that are chosen when forming $\dTestFun$ and $\dot{\dTestFun}$ are built adaptively from the data as described in \ref{appendix:minRadiusSelect}. Furthermore, relying on the intuition from theoretical results, the discrete weak form residual is analogous to considering the continuous weak form solution. This means that our method is relying on a more relaxed topology, and this leads to algorithms with a cost space that is smoother and more easily navigable. 

\subsection{Bias} 

One known limitation of the our method is that the asymptotic expansion in Proposition \ref{prop:noise} is only valid in a neighborhood about $\mathbf{0}$. In practice, this neighborhood shrinks as the nonlinearity of the right hand side, $\rhs$, becomes more severe. These non-zero higher order terms lead WENDy to a bias that increases in magnitude with the noise. In the best case, if we obtain a minimum in the linear part of $\wResNoise$, the expectation of these higher order terms will be non-zero, leading to a bias in our estimator.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.3\textwidth]{fig/goodwinExperiment_wnll_costSurface_nr.0.01.pdf}
    \includegraphics[width=0.3\textwidth]{fig/goodwinExperiment_wnll_costSurface_nr.0.05.pdf}
    \includegraphics[width=0.3\textwidth]{fig/goodwinExperiment_wnll_costSurface_nr.0.1.pdf}
    \includegraphics[width=0.3\textwidth]{fig/goodwinExperiment_wnll_costSurface_nr.0.15.pdf}
    \includegraphics[width=0.3\textwidth]{fig/goodwinExperiment_wnll_costSurface_nr.0.2.pdf}
    \includegraphics[width=0.3\textwidth]{fig/goodwinExperiment_wnll_costSurface_nr.0.25.pdf}
    \caption{The WENDy algorithm is run on the Goodwin 3D problem with all parameters fixed to truth except $p_4$ and $p_5$ for a variety of noise levels. The truth is shown as a green star, and the WENDy optimum is identified with a black circle. One can notice that the optimum moves further from truth as noise increases.}
    \label{fig:bias}
\end{figure}

In Figure \ref{fig:bias}, the Goodwin 3D (shown in Equation \eqref{eq:goodwin3d}) is run with all other parameters fixed to their true values except for $p_4$ and $p_5$ at 100 simulated datasets for each different level of noise. The likelihoods are averaged over the runs to produce the contours shown. As the noise ratio is increased, the local optimum is shifted away from truth. This is caused by the nonlinear terms that are neglected in Proposition \ref{prop:noise}. As the noise becomes larger, the nonlinear terms cause a bias in the WENDy estimate.

\subsection{Computational Cost} 

A strength of the weak form methods is that no forward simulation is necessary at each step of the optimization routine. When the system of differential equations is small and not stiff, then standard forward solve methods may be more efficient and the main benefit to using a weak form method like ours is that of robustness to both noise and to poor initialization. However, as the dimensionality of state variables and stiffness of the problem increases, the forward solves become more computationally expensive, whereas WENDy-MLE is unaffected. 

Recall that $K$ is the number of test functions, $M+1$ is the number of time points, $D$ is the dimension of the state variable $\state$, and $J$ is the dimension of the parameters $\params$. Computing the weak form negative log-likelihood itself has a complexity of $\mathcal{O}(K^2D^3(MJ + K))$. Similarly, the gradient computation has a complexity of $\mathcal{O}\bigl(K^2D^2\bigl(D(MJ + K)+J\bigr)\bigr)$ and the Hessian computation has a complexity of $\mathcal{O}\bigl(K^2D^2\bigl(D(MJ + K)+J^2\bigr)\bigr)$. The most expensive step in our method is computing the derivatives of the weak form log-likelihood for the second order optimization methods. Our implementation meticulously caches as much as possible, pre-allocates memory, applies an advantageous order of operations, and uses efficient data structures. 

In the linear case, many of the operators either become constant matrices or tensors that can be thought of as linear operators. Also, some operators are zero due to higher order derivatives being applied to the linear function. This reduces the constants involved in the cost, but incurs the same asymptotic complexity as the nonlinear case. One can see that the dominant cause for the cost is due to the number of test functions in Figure \ref{fig:compCost}. Notice that the recorded run times shown in Figure \ref{fig:compCost:subM} approximately match the predicted cost of the line of slope 1. 

The example above has ten parameters ($J=10$). Because the cost is dominated by the mat-mat computations for matrices of size $KD$, the gradient computation takes approximately an order of magnitude more time, and the Hessian takes an order of magnitude more than the gradient. 

\begin{figure}[ht]
    \centering
\begin{subfigure}{0.49\textwidth}
    \includegraphics[width=0.95\linewidth]{computationalCost_Mp1.pdf}
    \caption{}
    \label{fig:compCost:subM}
\end{subfigure} 
\begin{subfigure}{0.49\textwidth}
    \includegraphics[width=0.95\linewidth]{computationalCost_K.pdf}
    \caption{}
    \label{fig:compCost:subK}
\end{subfigure}
\caption{On the left, $K, D, J$ are all fixed, and $M$ is varied. On the right, $M, D, J$ are all fixed, and $K$ is varied.  The dashed and dotted lines correspond to a linear fit with a fixed slope ($\mathcal{O}(M), \mathcal{O}(K^3)$ respectively). Both plots are show on a log-log scale.}
\label{fig:compCost}
\end{figure}
Figure \ref{fig:compCost} shows the computational cost for different sized problems. Problems that are linear in parameters show a decrease in the computational cost but only by a scaling factor. The experiment verifies the cost of the algorithm scales linearly with respect to the number of data points on the grid and cubically with respect to the number of test functions. Unlike OE methods, the weak form methods can control the computational cost by using fewer test functions. In practice, we used 200-500 test functions which results in reasonable computational cost for the likelihood and its derivatives.

\section{Conclusion} \label{sec:conclusion}

In this work, the WENDy algorithm was extended to a more general case of differential equations where the right hand side may be nonlinear in parameters. Furthermore, the approach explicitly supports both additive Gaussian and multiplicative log-normal noise. The likelihood based approach improved on the convergence and accuracy compared to the previous work. Also, the new Julia implementation improved the usability and efficiency of the algorithm. 

The likelihood derived is limited by the linearization used. Further investigation of higher order expansions or other derivation strategies are necessary to handle high nonlinearities and noise levels. The computational advantage of the approach should be more apparent in the extension of the algorithm to larger systems of ordinary differential equations and partial differential equations. Also, further optimization of test function selection to recover maximum information in the weak form is another area of further exploration. 


\section{Acknowledgments} \label{sec:ack}

The authors would like to thank  April Tran (University of Colorado) for insights regarding software development for weak form methods.

This work is supported in part by the National Institute of General Medical Sciences grant R35GM149335, National Science Foundation grant 
2109774, National Institute of Food and Agriculture grant 2019-67014-29919, and by the Department of Energy, Office of Science, Advanced Scientific Computing Research under Award Number DE-SC0023346.