\section{Introduction}

Parameter estimation and inference for differential equations (DEs) remains a challenging problem despite the many sophisticated algorithms that have been developed over the years \citep{McGoffMukherjeePillai2015StatSurv}. Precursors to modern approaches can be traced back to the dawn of computational science in the 1950's and 1960's \citep{Greenberg1951NACATN2340} and have been historically categorized into Output Error (OE) or Equation Error (EE) approaches \citep{Ljung1999,Ljung2017WileyEncyclopediaofElectricalandElectronicsEngineering}.
 
The more common OE-based methods first propose a candidate parameter value, numerically compute an approximate solution, compare with data, and then iterate the parameter proposal until the comparison metric drops below a specified threshold. OE methods have come to dominate the parameter estimation literature partially because they can leverage existing numerical simulation and nonlinear optimization approaches. However, the cost functions commonly resulting from nonlinear differential equation systems often exhibit multiple modes and ridges, and thus convergence to globally optimal and unique parameter values frequently requires long run times and careful diagnostic monitoring \citep{DukicLopesPolson2012JAmStatAssoc,KennedyDukicDwyer2014AmNat,RamsayHookerCampbellEtAl2007JRStatSocSerBStatMethodol}, and assessment of the  impact of hyperparameter choices from the DE solver and optimization algorithm  on the parameter estimates themselves \citep{NardiniBortz2019InverseProbl}. These  practical challenges have motivated alternative approaches with reduced reliance on forward-based solvers, including likelihood-free sequential Monte Carlo methods \citep{ToniWelchStrelkowaEtAl2009JRSocInterface}, local approximations \citep{ConradMarzoukPillaiEtAl2016JAmStatAssoc}, manifold-constrained Gaussian processes \citep{WongYangKou2024JStatSoft,YangWongKou2021ProcNatlAcadSciUSA}, and weak-form methods \citep{BortzMessengerDukic2023BullMathBiol,BrunelClairondAlche-Buc2014JAmStatAssoc,HallMa2014JRStatSocB}. 
 
EE-based estimation involves substituting data directly into a model and minimizing the norm of the resulting residual. If the model equation is linear in the parameters (LiP), the resulting problem is a simple linear regression. This category also encompasses a variety of extensions that minimize transformed versions of the residual. For example, a Fourier Transform of the model equations results in a Fourier Error (FE) method. FE-methods are efficient, but that efficiency is (generally) limited to models that are LiP. Conversely, if the model is \emph{nonlinear} in the parameters (NiP), this results in a nonlinear regression problem, necessitating an iterative scheme for the optimization. The NiP EE-based scenario has not received nearly as much attention as the LiP case, forming a major motivation for our efforts here.

\subsection{Weak Form System Identification}

Another EE extension involves converting the model equation to its weak form, i.e., convolving with a compactly supported test function, $\varphi$, and integrating by parts. Researchers have explored several forms including those based upon piecewise \citep{LoebCahen1965IEEETransAutomControl} and Hermite \citep{Takaya1968IEEETransAutomControl} polynomials, Fourier \citep{PearsonLee1985Control-TheoryAdvTechnol} and Hartley \citep{PatraUnbehauen1995IntJControl} Transforms, and Volterra linear integral operators \citep{PinAssaloneLoveraEtAl2015IEEETransAutomatContr,PinChenParisini2017Automatica}.  However, while it was known that the choice of $\varphi$ impacts the estimation and inference accuracy, only recently have there been precise derivations of the impact of test function hyperparameters on the solution \cite{BortzMessengerDukic2023BullMathBiol,GurevichReinboldGrigoriev2019Chaos,MessengerBortz2021JComputPhys,MessengerBortz2021MultiscaleModelSimul,MessengerBortz2024IMAJNumerAnal}.

Historically, weak form versions of EE-based parameter estimation have been proposed repeatedly over the years, originally in the aerospace engineering literature \citep{LoebCahen1965Automatisme,LoebCahen1965IEEETransAutomControl,Shinbrot1954NACATN3288,Shinbrot1957TransAmSocMechEng} and later (independently) explored in statistics \citep{BrunelClairondAlche-Buc2014JAmStatAssoc,HallMa2014JRStatSocB} and nonlinear dynamics \citep{BortzMessengerDukic2023BullMathBiol,LiuChangChen2016NonlinearDyn}.  Notably, in 1965, Loeb and Cahen \citep{LoebCahen1965Automatisme,LoebCahen1965IEEETransAutomControl} introduced their method, called the \emph{Modulating Function Method} (MFM). The MFM-based research was active for many years, particularly in the control literature, but the field became mostly dormant by the end of the twentieth century.

Starting in 2010, research in two independent areas led to a resurgence in interest in weak form system identification. Janiczek noted that a generalized version of integration-by-parts could extend the traditional MFM to work with fractional differential equations \citep{Janiczek2010BullPolAcadSciTechSci}. This discovery led to a renewed (and sustaining) interest by several control engineering researchers, c.f.~\citep{AldoghaitherLiuLaleg-Kirati2015SIAMJSciComput,JouffroyReger20152015IEEEConfControlApplCCA,LiuLaleg-Kirati2015SignalProcessing}.

The second independent line of weak form research arose out of the field of sparse regression-based equation learning. The seminal work in this area is the Sparse Identification of Nonlinear Dynamics (SINDy) methodology, originally proposed in 2016 for ODEs in \citep{BruntonProctorKutz2016ProcNatlAcadSci} and in 2017 for PDEs in \citep{RudyBruntonProctorEtAl2017SciAdv,Schaeffer2017ProcRSocMathPhysEngSci}. It is a data-driven modeling approach that learns both the form of the differential equation and the parameters directly from data. While incredibly successful, the original SINDy method was not robust to noise. Subsequently, several researchers independently noticed the value in casting equations in the weak form \citep{GurevichReinboldGrigoriev2019Chaos,MessengerBortz2021JComputPhys,MessengerBortz2021MultiscaleModelSimul,PantazisTsamardinos2019Bioinformatics,WangHuanGarikipati2019ComputMethodsApplMechEng}. In particular, we note that the Weak form Sparse Identification of Nonlinear Dynamics (WSINDy) method proposed in  \citep{MessengerBortz2021JComputPhys,MessengerBortz2021MultiscaleModelSimul} clearly demonstrates the noise robustness and computational efficiency of a weak form approach to equation learning. We direct the interested reader to recent overviews \cite{BortzMessengerTran2024NumericalAnalysisMeetsMachineLearning,MessengerTranDukicEtAl2024SIAMNews} and further WSINDy extensions \cite{MessengerBortz2022PhysicaD,MessengerBurbyBortz2024SciRep,MessengerDallAneseBortz2022ProcThirdMathSciMachLearnConf,MessengerWheelerLiuEtAl2022JRSocInterface,RussoMessengerBortzEtAl2024IFAC-PapersOnLine,TranHeMessengerEtAl2024ComputMethodsApplMechEng}.

Lastly, we note that all the above EE-based approaches rarely address the fact that the data appears in all terms of the EE regression, i.e., in the matrix and the vector.  This class of problems is known as an Errors-in-Variables problem \citep{Durbin1954RevIntStatInst} and can be addressed using methods from the generalized least squares literature. In our previous work \citep{BortzMessengerDukic2023BullMathBiol}, we proposed using an iteratively reweighted least squares approach (IRLS) which repeatedly solves the EE regression, updating the covariance at each step. In what follows, we denote the original WENDy algorithm as WENDy-IRLS.

\subsection{Contribution}
The current work extends the WENDy-IRLS algorithm (as described in \citep{BortzMessengerDukic2023BullMathBiol}) to estimate parameters in a broader class of differential equation models. Our novel extension, called WENDy-MLE, can estimate parameters for systems of ordinary differential equations that are NiP. Furthermore, we  extend the framework of our derivation beyond additive Gaussian noise, and now can explicitly address multiplicative log-normal noise. The algorithm is based on  likelihood function optimization, and designed to find the maximum likelihood estimate of the parameters. This is done via an efficient implementation in Julia \citep{BezansonEdelmanKarpinskiEtAl2017SIAMRev} with analytic computation of first and second order derivatives of the likelihood.  Through numerical results, we demonstrate that the algorithm converges to accurate estimates of the parameters more often than a conventional forward solver-based least squares method.  

The paper is structured as follows: Section \ref{sec:mamathematical-frameworkth} describes the mathematical derivation of the weak form, noise models, maximum likelihood estimation and analytic computation of derivatives. Section \ref{sec:results} presents a brief overview of the implementation of the WENDy-MLE algorithm and the numerical results from applying our framework to specific examples of systems of ordinary differential equations, comparing to a baseline forward simulation-based solver, and examining robustness to noise and initialization and availability of data. We conclude this work in Section \ref{sec:discussion} with a discussion of strengths and weaknesses in our approach including how our approach compares with other state-of-the-art methods. Concluding remarks are made in Section \ref{sec:conclusion}, and acknowledgements are made in Section \ref{sec:ack}.