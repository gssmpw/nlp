\section{Discretization and Test Functions}\label{sec:appendix}
In order to satisfy the weak form, a solution must satisfy Eq.~\eqref{eq:ode-weak} for all possible test functions. However, in this work, we are not concerned with the solution itself, but with estimating the parameters in Eq.~\eqref{eq:ode-weak}. As the efficacy of the WENDy algorithm depends strongly on wisely choosing test function hyperparameters (smoothness, radius, etc.), in this Appendix we provide an exposition of the test function creation process in \cite{BortzMessengerDukic2023BullMathBiol}.
\subsection{Discretization of the Weak Form} \label{appendix:discWeak}

Starting from Eq.~\eqref{eq:ode-weak}, we must choose a finite set of test functions to create a vector of weak form EE residuals to be minimized. In approaches such as the Finite Element Method, this set is chosen based upon the expected properties of the solution as well as the class of equations being solved. Conversely, to perform weak form EE-based parameter estimation, we assume the availability of samples of the (possibly noisy) solution. In pursuit of the estimation goal, however, we note that how to optimally choose the properties, number, and location of the test functions remains an open problem.
Nonetheless, in \cite{MessengerBortz2021MultiscaleModelSimul}, it was reported that smoother test functions result in a more accurate convolution integral (via the Eulerâ€“Maclaurin formula), while later research revealed that test functions with wider support frequently performed better, and subsequently an algorithm to choose the support size was developed in  \cite{MessengerBortz2021JComputPhys}. These empirical observations regarding smoothness and support size have partially guided the design of the following algorithm.

From a numerical linear algebra perspective, it would be best if the test functions were orthogonal. Towards this goal, we begin by considering the following $C^\infty$ bump function
\begin{equation}
	\label{eq:test-fun}
	\varphi_k(t ; \eta, m_t)=C \exp \left(-\frac{\eta}{\left[1-(\tfrac{t-t_k}{m_t\Delta t} )^2\right]_{+}}\right)
\end{equation}
where $t_k$ is the center of the function, $m_t$ controls the radius of the support, the constant $C$ normalizes the test function such that $\|\varphi_k\|_2=1$, $\eta$ is a shape parameter, and $[\cdot]_{+}\eqdef\max (\cdot, 0)$, so that $\varphi_k(t ; m_t\Delta t)$ is supported only on $[-m_t\Delta t, m_t\Delta t]$. 
Using the algorithm below in \ref{appendix:minRadiusSelect}, we identify the minimal value for $m_t$ such that the numerical error in the convolution integral does not dominate the residual (denoted as $\underline{m}_t$). We define the set of test functions to be all test functions such that they are compactly supported over the measured time domain $(t_0,t_M)$, and have the following features: 1) the $t_k$ values coincide with the sampled timepoints and 2) allowable radii are larger than the minimal one $\underline{m}_t$. Motivated by empirical results, the shape parameter is arbitrarily fixed at $\eta=9$. This gives us a set of $K_\textrm{full}$ test functions $\{\testFun_k\}_{k=1}^{K_\text{full}}$ and we evaluate them on the grid to obtain the matrices:
\[\dTestFun_\text{full} \eqdef \begin{bmatrix}
	\testFun_1(t_0) & \cdots & \testFun_{K_\text{full}}(t_0)\\
	\vdots & \ddots & \vdots\\
	\testFun_1(t_M) & \cdots & \testFun_{K_\text{full}}(t_M)
\end{bmatrix}, \; \dot{\dTestFun}_\text{full} \eqdef \begin{bmatrix}
	\dot{\varphi}_1(t_0) & \cdots & \dot{\varphi}_{K_\text{full}}(t_0)\\
	\vdots & \ddots & \vdots\\
	\dot{\varphi}_1(t_M) & \cdots & \dot{\varphi}_{K_\text{full}}(t_M)
\end{bmatrix}\]

\subsubsection{Minimum Radius Selection.} \label{appendix:minRadiusSelect}

The algorithm by which these test functions are chosen is more fully described in \citep{BortzMessengerDukic2023BullMathBiol}, but a short summary is given here. 

Our analysis follows the equation $\dot{u}=\rhs(u)$ where $u$ is a one dimensional state variable. The result is generalized to a $D$ dimensional system by applying the radius selection in each dimension independently. We see that using integration by parts for a test function $\varphi$ gives the equality $-\langle \dot{\varphi}, u\rangle = \langle \varphi, \rhs(u) \rangle$. Integrating, we obtain
\[0 =  \int_0^T \rhs(u(t))\varphi(t) + \varphi'(t)u(t) dt = \int_0^T \frac{d}{dt}\bigl(u(t)\varphi(t)\bigr) dt. \]
Thus this integral should be zero, or at least small accounting for the presence of numerical error and noise. In this work, we use the trapezoidal rule on a uniform time grid. Because the $\varphi$ are test functions with compact support on the interior of the domain, we can define the integration error as in Proposition \ref{prop:int-error}: 
\begin{equation} \label{eq:eint} 
    e_\text{int}(M) = \frac{T}{M}\sum_{m=0}^M \frac{d}{dt}(u \varphi) (t_m)
\end{equation}
By expanding $\frac{d}{dt}\bigl(u(t) \varphi(t)\bigr)$ into its Fourier Series, where the coefficients are given by $\mathcal{F}_n[\cdot] := \frac{1}{\sqrt{T}}\int_0^T (\cdot) \exp\left[\frac{-2\pi i n }{T}t\right] dt$, we can make the following simplification:
\begin{align*} 
	\frac{d}{dt}\bigl(u(t)\varphi(t)\bigr) &= \sum_{n\in\mathbb{Z}} \mathcal{F}_n \left[\frac{d}{dt}\bigl(u(t)\varphi(t)\bigr)\right] \exp \left[\frac{ 2\pi i n}{T}t\right]\\
	&= \sum_{n\in\mathbb{Z}} \left(\frac{1}{T} \int_0^T \frac{d}{dt}\bigl(u(t)\varphi(t)\bigr)  \exp \left[\frac{ -2\pi i n}{T}t\right] dt\right)  \exp \left[\frac{ 2\pi i n}{T}t\right]\\
	&\stackrel{\text{IBP}}{=} \frac{ -2\pi i }{T} \sum_{n\in\mathbb{Z}} n\underbrace{\left( \frac{1}{T} \int_0^T u(t)\varphi(t)  \exp \left[\frac{ -2\pi i n}{T}t\right] dt \right)}_{=\mathcal{F}_n\bigl[\varphi u\bigr]}  \exp \left[\frac{ 2\pi i n}{T}t\right].
\end{align*} 
The last simplification in the integration by parts relies on the fact that $\varphi$ is a test function for the domain $[0,T]$, thus $\operatorname{supp}(\varphi) \subset (0,T)$. Now if we return to the quantity of interest, $e_{\text{int}}(M)$, by integrating both sides over the time interval we obtain: 
\[e_\text{int} = \frac{2\pi i}{M} \sum_{n\in\mathbb{Z}} n \mathcal{F}_n\bigl[\varphi u\bigr]\underbrace{\sum_{m=0}^M  \exp \left[\frac{ 2\pi n i }{M}m\right] }_{\eqdef I_n}.\] 
Notice that $\forall n \neq 0$, $I_n$ is 0 because $\exp\left[\tfrac{2\pi i}{T}t\right]dt$ is periodic on $[0,T]$. Also, $\mathcal{F}_0$ is 0 because it corresponds to the true integral. Notice that when $n = \ell M$ for some integer $\ell$, then we have that $I_{n} = M$ because the quadrature nodes align with the roots of unity for all the Fourier modes. Because the Fourier modes should decay as $M$ gets larger, we can say 
\[e_\text{int} \approx \operatorname{imag}\left\{ 2\pi n \mathcal{F}_M\bigl[\varphi u\bigr]\right\}.\]

Because we do not know $u$ analytically, the true Fourier coefficients are approximated by the Discrete Fourier Transform, $\hat{\mathcal{F}}_{n - \left\lfloor \tfrac{M}{2} \right\rfloor}[\dTrueState] = \frac{T}{M} \sum_{m=0}^{M} \dTrueState_m \exp \left[\frac{ 2\pi i n}{M} m\right]$. Thus, the largest mode we can approximate is the $\left\lfloor\frac{M}{2}\right\rfloor$ mode. Also, note that applying the same quadrature rule to a sub-sampled grid gives us an upper bound on the error: 
\[e_\text{int}(M) \leq e_\text{int}\Bigl(\bigl\lfloor\tfrac{M}{2}\bigr\rfloor\Bigr).\]
This motivates subsampling in time to inspect the error. Letting $\tilde{M} \eqdef \bigl\lfloor \frac{M}{s} \bigr\rfloor$ for some scaling factor $s>2$, we approximate the integration error when subsampled.

The last complication is that we do not have access to $\dTrueState$, so we must approximate by substituting $\dstate$: 
\[e_\text{int}(\tilde{M}) \leq \hat{e}_\text{int}(\tilde{M}) = \frac{2 pi}{\sqrt{T}} \hat{F}_{\tilde{M}}(\dstate).\]

This motivates us to search over the space of possible radii, and then select the smallest radius possible when the effects due to noise become dominate.
Qualitatively, our reasoning is that we can approximate the integration error by using the data, but as noise levels becomes more extreme the effects to noise become dominate for smaller radii, thus for higher noise we expect to select smaller radii in general. This can be seen in Figure \ref{fig:minRad}.
\begin{figure}[H] 
	\centering
	\includegraphics[width=0.9\textwidth]{minRadiusDetection.pdf}
	\caption{The results for the logistic growth function on the time domain $(0,10)$, initial condition of $0.01$ and $M=512$, looking at $\hat{\mathcal{F}}_{256}$ for a variety of radii  $m_t \in [0.01, 1]$. A vertical line indicates where we have detected a corner in the integration error surrogate}
	\label{fig:minRad}
\end{figure}

\subsubsection{Reducing the Size of the Test Function Matrix}
\newcommand{\leftSingVals}{\mathbf{Q}}
\newcommand{\rightSingVals}{\mathbf{V}}
The test function matrix $\dTestFun_\text{full}$ is often overdetermined, so in order to improve the conditioning of the system we use an SVD reduction technique to obtain a matrix $\dTestFun$ that has a better condition number $\kappa(\dTestFun) \eqdef \sigma_1(\dTestFun)  / \sigma_K(\dTestFun)$, while still retaining as much information as possible. This is done by looking for a corner in the singular values.
Let the SVD of $\dTestFun_\text{full}$ by 
\begin{align*} 
	\dTestFun_\text{full} &= \leftSingVals \operatorname{diag}([\sigma_1, \cdots, \sigma_K, \cdots]) \rightSingVals^T  
	\shortintertext{ where $\leftSingVals, \rightSingVals$ are unitary and $\sigma_1 \ge \sigma_2 \ge \ldots \ge 0$.
    We then define} \\
	\dTestFun &= \underbrace{\operatorname{diag}\Bigl(\bigl[\tfrac{1}{\sigma_1}, \cdots, \tfrac{1}{\sigma_K}\bigr]\Bigr)\leftSingVals^T}_{\mathbf{P}} \dTestFun_\text{full} 
\end{align*} 
where the cutoff $K$ is found by looking for a corner in the plot of $\sigma_i$ vs $i$.

The approach in \citep{BortzMessengerDukic2023BullMathBiol} approximates $\dot{\dTestFun}$ with a spectral method. In contrast, we compute  $\dot{\dTestFun}_\text{full}$ analytically, and then leverage the SVD of $\dTestFun$ to apply the same linear operators, $\dot{\dTestFun}=\mathbf{P}\dot{\dTestFun}_\text{full}$, to obtain a result that only has error from numerical precision,
leading to slightly better numerical integration error than in \citep{BortzMessengerDukic2023BullMathBiol}.

\section{Highlighting the Effects of Linearity in Parameters} \label{sec:linvsnonlin}
When $\rhs$ is linear in $\params$ then then we can write the residual as follows: 
\begin{equation}
	\label{eq:resLin}
	\wRes(\params;\dstatemat,\dt) = \wRhsLin(\dstatemat, \dt) \params - \wLhs(\dstatemat)
\end{equation}
where $\wRhsLin(\dstatemat, \dt) \in \R^{KD\times J}$ is a matrix-valued function that is constant with respect to $\params$. This leads to simplifications in the derivative information
which can improve computational efficiency. 

In the linear case, the Jacobian of the weak form right-hand side with respect to the parameters is the matrix $\wRhsLin(\dstatemat, \dt)$. Formally, $\nabla_{\params} \wRhs(\params; \dstatemat, \dt) = \wRhsLin(\dstatemat,\dt)$. This also simplifies implies $\nabla_{\params} \nabla_{u} \wRhs(\params; \dstatemat, \dt) = \nabla_{u} \wRhsLin(\dstatemat, \dt)$. We again drop explicit dependence on $\dstatemat$ and $\dt$ for simplicity of notation. Feeding this into our existing expression for the gradient of the weak form negative logarithm, we have
\begin{align*}
	\nabla_{\params} \ell(\params) &= 2 \wRhsLin^T \wCov(\params)^{-1} (\wRhsLin\params- \wLhs) \\
	&+ (\wRhsLin\params-\wLhs)^T(\partial_{\params_j} \wCov(\params)^{-1})(\wRhsLin\params-\wLhs).
\end{align*}
Also, notice that $\wCov(\params)$ now becomes quadratic in $\params$:
\[\wCov(\params) = \bigl(\nabla_{u} \wRhsLin [\params]+\dot{\dTestFun} \otimes \id_D\bigr) \bigl(\Sigma \otimes \id_{M+1} \bigr)\bigl(\nabla_{u} \wRhsLin [\params] + \dot{\dTestFun} \otimes \id_D\bigr)^T \]
In fact $\nabla_{u} \wRhsLin$ is the Jacobian of a matrix-valued function, and thus it is a three-dimensional tensor in $\R^{KD \times MD \times J}$. We treat it  as a linear operator acting on $\params$.  In practice this is a page-wise mat-vec across the third dimension of $\nabla_{u} \wRhsLin$. 

This causes the following simplification in the derivative of $\wCov(\params)$:
\[\partial_{\params_j} \wCov(\params) = 2\nabla_{u} \wRhsLin [\mathbf{e}_j] \bigl(\Sigma \otimes \id_{M+1} \bigr) \bigl(\nabla_{u}\wRhsLin[\params]+\dot{\dTestFun} \otimes \id_D\bigr)^T \]
where $\mathbf{e}_j \in \R^J$ is the $j^\text{th}$ canonical basis vector. This is equivalent to indexing into the $j^\text{th}$ page of $\nabla_{u}\wRhsLin$. 

The second order derivative computations also simplify beyond the evaluation of $\wCov(\params)$. Critically, observe that several terms in the Hessian are now guaranteed to be $\mathbf{0}$. In particular, we have $\forall \params$:
\[\partial_{\params_i,\params_j} \wRhsLin \params = \mathbf{0} \in \R^J \text{ and }  
\partial_{\params_i,\params_j} \nabla_{u} \wRhsLin[\params] = \mathbf{0} \in \R^{KD\times MD}. \]
Furthermore because $\nabla_{u} \wRhsLin$ is constant with respect to $\params$, then $\partial_{\params_i \params_j}\wCov(\params)^{-1}$ is constant with respect to $\params$, so this can be computed once and then reused. 

\section{Iterative Re-weighted Least Squares}\label{sec:WENDy-IRLS}

In the previous work \citep{BortzMessengerDukic2023BullMathBiol}, the covariance information was incorporated by solving the generalized least squares problem 
\[ \estim = \underset{\params \in \R^J}{\operatorname{argmin}} (\wRhs(\params;\dstatemat,\dt) - \wLhs(\dstatemat))^T\wCov(\trueParams; \dstatemat)^{-1}(\wRhs(\params;\dstatemat,\dt) - \wLhs(\dstatemat)).\]
Because $\wCov(\trueParams; \dstatemat)$ is not known, it has to be approximated with the current value of $\params$. This gives rise the iterative re-weighted least squares algorithm, which iterates as follows 
\begin{equation}
	\label{eq:WENDy-IRLSIter}
	\estim^{(i+1)} = \underset{\params \in \R^J}{\operatorname{argmin}}(\wRhs(\params) - \wLhs)^T\wCov(\estim^{(i)})^{-1}(\wRhs(\params) - \wLhs)
\end{equation}
until the iterates are sufficiently close together (and again, we drop explicit dependence on $\dstatemat$ and $\dt$).
This iteration involves computing the covariance estimate $\wCov(\params)$ then solving a weighted least squares problem. The previous work \citep{BortzMessengerDukic2023BullMathBiol} only considered problems that were linear in parameters, so in the nonlinear case, we have extended this algorithm by solving a nonlinear weighted least squares problem at each iteration where the Jacobian is computed using analytic derivative information described in Equations \eqref{eq:wnll-grad} and \eqref{eq:wnll-hess}. 

The previous work also chose the same initial guess based on the ordinary least square solution:
\[\estim^{(0)} = \underset{\params \in \R^J}{\operatorname{argmin}} \tfrac{1}{2}\|\wRhs(\params) - \wLhs\|^2_2\]
When the ODE was linear, this was done explicitly through the linear algebra. In the nonlinear case, this can no longer be done, so an initial guess is necessary. In this work, we pass all algorithms the same initial guess that is randomly sampled from parameter range specified in Table \ref{tab:odes}.
\section{Supplemental Material}
\subsection{Plots}
The following plots give detailed information on the bias, variance, MSE and coverage for all parameters for all ODEs, grouped by noise level.
\begin{figure}[H]
	\includegraphics[width=0.9\columnwidth]{logisticGrowth_UQ2.pdf}
	\caption{Left: the squared bias, variance and MSE for $p_1$ (top) and $p_2$ (bottom), as a function of noise level. Right: the coverage levels for $p_1$ (top) and $p_2$ (bottom), as a function of noise level.}
	\label{fig:logisticSup}
\end{figure}
\begin{figure}[H]
	\includegraphics[width=0.9\columnwidth]{hindmarshRose_UQ2.pdf}
	\caption{Left: the squared bias, variance and MSE for $p_1 - p_{10}$ top to bottom respectively, as a function of noise level. Right: the coverage levels for $p_1 - p_{10}$ top to bottom respectively, as a function of noise level.}
	\label{fig:hindmarshSup}
\end{figure}
\begin{figure}[H]
	\includegraphics[width=0.9\columnwidth]{lorenz_UQ2.pdf}
	\caption{Left: the squared bias, variance and MSE for $p_1 - p_3$ top to bottom respectively, as a function of noise level. Right: the coverage levels for $p_1 - p_3$ top to bottom respectively, as a function of noise level.}
	\label{fig:lorenzSup}
\end{figure}
\begin{figure}[H]
	\includegraphics[width=0.9\columnwidth]{multimodal_UQ2.pdf}
	\caption{Left: the squared bias, variance and MSE for $p_1 - p_5$ top to bottom respectively, as a function of noise level. Right: the coverage levels for $p_1 - p_5$ top to bottom respectively, as a function of noise level.}
	\label{fig:multimodalSup}
\end{figure}
\begin{figure}[H]
	\includegraphics[width=0.9\columnwidth]{goodwin_UQ2.pdf}
	\caption{Left: the squared bias, variance and MSE for $p_1 - p8$ top to bottom respectively, as a function of noise level. Right: the coverage levels for $p_1 - p_8$ top to bottom respectively, as a function of noise level.}
	\label{fig:goodwinSup}
\end{figure}
\begin{figure}[H]
	\includegraphics[width=0.9\columnwidth]{sir_UQ2.pdf}
	\caption{Left: the squared bias, variance and MSE for $p_1 - p_5$ top to bottom respectively, as a function of noise level. Right: the coverage levels for $p_1 - p_5$ top to bottom respectively, as a function of noise level.}
	\label{fig:sirSup}
\end{figure}

\subsection{Tables}
The following tables give detailed information on the bias, variance, MSE and coverage for all parameters for all ODEs, grouped by noise level.
\setlength{\tabcolsep}{6pt}
\renewcommand{\arraystretch}{1}
\begin{table}[H]
	\caption{Logistic Growth} \label{tab:logUQ1}	
	\resizebox{\textwidth}{!}{
	\input{fig/logisticGrowth_UQ_table_1.tex}
	}
\end{table}
\begin{table}[H]
	\caption{Logistic Growth (continued)} \label{tab:logUQ2}	
	\resizebox{\textwidth}{!}{
	\input{fig/logisticGrowth_UQ_table_2.tex}
	}
\end{table}

\begin{table}[H]
	\caption{Hindmarsh-Rose} \label{tab:hindUQ1}	
	\resizebox{\textwidth}{!}{
	\input{fig/hindmarshRose_UQ_table_1.tex}
	}
\end{table}

\begin{table}[H]
	\caption{Hindmarsh-Rose (continued)} \label{tab:hindUQ2}	
	\resizebox{\textwidth}{!}{
	\input{fig/hindmarshRose_UQ_table_2.tex}
	}
\end{table}
\begin{table}[H]
	\caption{Hindmarsh-Rose (continued)} \label{tab:hindUQ3}	
	\resizebox{\textwidth}{!}{
	\input{fig/hindmarshRose_UQ_table_3.tex}
	}
\end{table}
\begin{table}[H]
	\caption{Hindmarsh-Rose (continued)} \label{tab:hindUQ4}	
	\resizebox{\textwidth}{!}{
	\input{fig/hindmarshRose_UQ_table_4.tex}
	}
\end{table}

\begin{table}[H]
	\caption{Lorenz} \label{tab:lorenzUQ1}	
	\resizebox{\textwidth}{!}{
	\input{fig/lorenz_UQ_table_1.tex}
	}
\end{table}

\begin{table}[H]
	\caption{Lorenz (continued)} \label{tab:lorenzUQ2}	
	\resizebox{\textwidth}{!}{
	\input{fig/lorenz_UQ_table_2.tex}
	}
\end{table}

\begin{table}[H]
	\caption{Goodwin 2D} \label{tab:goodwin2DUQ1}	
	\resizebox{\textwidth}{!}{
	\input{fig/multimodal_UQ_table_1.tex}
	}
\end{table}

\begin{table}[H]
	\caption{Goodwin 2D (continued)} \label{tab:goodwin2DUQ2}	
	\resizebox{\textwidth}{!}{
	\input{fig/multimodal_UQ_table_2.tex}
	}
\end{table}

\begin{table}[H]
	\caption{Goodwin 3D} \label{tab:goodwin3DUQ1}	
	\resizebox{\textwidth}{!}{
	\input{fig/goodwin_UQ_table_1.tex}
	}
\end{table}
\begin{table}[H]
	\caption{Goodwin 3D (continued)} \label{tab:goodwin3DUQ2}	
	\resizebox{\textwidth}{!}{
	\input{fig/goodwin_UQ_table_2.tex}
	}
\end{table}
\begin{table}[H]
	\caption{Goodwin 3D (continued)} \label{tab:goodwin3DUQ3}	
	\resizebox{\textwidth}{!}{
	\input{fig/goodwin_UQ_table_3.tex}
	}
\end{table}
\begin{table}[H]
	\caption{Goodwin 3D (continued)} \label{tab:goodwin3DUQ4}	
	\resizebox{\textwidth}{!}{
	\input{fig/goodwin_UQ_table_4.tex}
	}
\end{table}

\begin{table}[H]
	\caption{Goodwin 3D (continued)} \label{tab:goodwin3DUQ5}	
	\resizebox{\textwidth}{!}{
	\input{fig/goodwin_UQ_table_5.tex}
	}
\end{table}

\begin{table}[H]
	\caption{SIR-TDI} \label{tab:sirUQ1}	
	\resizebox{\textwidth}{!}{
	\input{fig/sir_UQ_table_1.tex}
	}
\end{table}
\begin{table}[H]
	\caption{SIR-TDI (continued)} \label{tab:sirUQ2}	
	\resizebox{\textwidth}{!}{
	\input{fig/sir_UQ_table_2.tex}
	}
\end{table}
