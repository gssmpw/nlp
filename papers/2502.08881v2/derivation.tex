\section{WENDy-MLE Algorithm: Mathematical Framework} \label{sec:mamathematical-frameworkth}
This section lays out the mathematical foundation and needed assumptions for the new WENDy-MLE algorithm. It first describes the class of differential equations and noise for which the algorithm is applicable. We then present the distribution of the weak-form residual and the corresponding likelihood function under the Gaussian noise assumption. Furthermore, we show how to compute first and second-order derivative for the analytic likelihood, and how to utilize that information in the underlying optimization algorithms. Finally, we extend the framework to the case of multiplicative log-normal noise. We provide additional discussion, as well as simplifications and computational speedups available when the model is LiP, in \ref{sec:linvsnonlin}.

To set notation, we assume that the data of the dynamical system is governed by a fully-observed $D$-dimensional system of ordinary differential equations (ODE) of the form 
\begin{equation}
	\label{eq:ode-strong}
	\dot{\state}(\t) = \rhs(\params,\state(\t),\t)
\end{equation}
where $\state(\t) \in \mathcal{H}^1\bigl((0,T),\R^D\bigr)$ is the function state variable at time $\t \in [0,T]$; we typically drop the $\t$ dependence and write $\dot{\state} = \rhs(\params, \state,\t)$ for short. 
The driving function $\rhs$ is (possibly nonlinearly) parameterized by a finite set of unknown parameters $\params\in\R^J$ that we wish to recover, and $\rhs$ may also be nonlinear in $\state$ and $\t$. It is required that $\rhs$ be twice continuously differentiable with respect to $\params$ and  $\state$. Note that we use bold lowercase variables to indicate vectors and vector-valued functions, and bold upper case to indicate matrices and matrix-valued functions. We use $\|\cdot\|$ to denote the 2-norm for vectors and the operator norm for matrices unless otherwise specified.

\subsection{The Weak Form}

To implement the WENDy-MLE approach, we first convert Equation \eqref{eq:ode-strong} from strong to weak form. This is done by taking the $L^2\bigl((0,T)\bigr)$ inner product, $\langle \cdot, \cdot\rangle$, with an analytic compactly supported test function, $\testFun(\t)\in \mathcal{C}^\infty_c\bigl((0,T), \R^D\bigr)$. In this work, multivariate test functions are chosen to be of the form $\testFun(\t) = \mathbf{1}_D \varphi(t)$ where $\varphi(t) \in \mathcal{C}^\infty_c\bigl((0,T), \R\bigr)$\footnote{One could choose more general test functions $\testFun(t) = [\varphi_1(t), \cdots, \varphi_D(t)]^T$ and the derivations here can be easily extended to this case.}. The weak form now becomes
\begin{equation}
	\label{eq:ode-weak}
	\langle \testFun, \dot{\state}\rangle = \langle \testFun,\rhs(\params, \state, \t)\rangle.
\end{equation}
Because the test functions have compact support, integration by parts allows movement of the derivatives onto the test functions: 
\begin{equation}
	\label{eq:ode-weak-trick}
	-\langle \dot{\testFun}, \state\rangle = \langle \testFun, \rhs(\params, \state, \t)\rangle.
\end{equation}

Formally, to satisfy the weak form, a solution must solve the Equation \eqref{eq:ode-weak} (and equivalently Equation \eqref{eq:ode-weak-trick}) for all possible test functions. For computational reasons, a finite set of test functions, $\{\testFun_k\}_{k=1}^K$, is chosen. The strategy for choosing an optimal set of test functions for a specific problem is an area of ongoing research. Here, we use the method described in \citep{BortzMessengerDukic2023BullMathBiol} (see \ref{appendix:discWeak} for more details). 

\subsection{Description of Data with Gaussian Noise} \label{sec:dataDesc}
For the data generating mechanism, we assume that the  observations arise when the solution to a system of the form \eqref{eq:ode-strong} gets corrupted by noise. For simplicity, the data is assumed to be observed on a uniform grid with stepsize $\Delta \t$. For the Gaussian case, we further assume that the noise is additive, consisting of independent and identically distributed (i.i.d.) Gaussian random variables. Specifically, for sample timepoints $\{t_m\}_{m=0}^M$, and true states $\dTrueState_m:=\state(t_m)$, we assume that the observed data arise as  a sum of the true state and noise: 
$$\dstate_m = \dTrueState_m+\noise_m\quad \forall m \in \{0, ..., M\}$$ 
where $\noise_m \stackrel{iid}{\sim}\mathcal{N}({\mathbf 0},{\mathbf{\Sigma}})$, and ${\mathbf{\Sigma}}$ is a $D \times D$ diagonal matrix. In Section \ref{sec:log} we will adapt this framework to accommodate the multiplicative log-normal noise instead of Gaussian. 

\subsection{Discretization}
For the set of test functions $\{\varphi_k\}_{k=1}^K$, we build the matrices 
\[
    \dTestFun = \begin{bmatrix}
        \varphi_1(\dt)^T \\ \vdots \\ \varphi_K(\dt)^T
    \end{bmatrix}\in \R^{K\times (M+1)}, \;
    \dot{\dTestFun} = \begin{bmatrix}
		\dot{\varphi}_1(\dt)^T \\ \vdots \\ \dot{\varphi}_K(\dt)^T
    \end{bmatrix}\in \R^{K\times (M+1)} \\
\]
The matrices of the data and the RHS are as follows:
\begin{equation*}
	\dt \eqdef \begin{bmatrix}
		\t_0 \\ \vdots \\ \t_M
	\end{bmatrix}\in \R^{(M+1)\times 1}, \;
	\dstatemat \eqdef \begin{bmatrix}
		\dstate_0 ^T\\
		\vdots \\
		\dstate_M^T
	\end{bmatrix}\in\R^{(M+1) \times D}, \;
	\dRhs(\params;\dstatemat,\dt) \eqdef \begin{bmatrix}
		\rhs(\params, \dstate_1,\t_1)^T \\
		\vdots \\
		\rhs(\params, \dstate_M,\t_M)^T 
	\end{bmatrix}\in\R^{(M+1) \times D}.
\end{equation*}
The products $\dot{\dTestFun}\dstatemat$ and $\dTestFun\dRhs$ are equivalent to using the trapezoid rule to numerically approximate the integral in the inner product\footnote{If the grid was non-uniform or if a different quadrature rule was desired, then the test function matrices would be post-multiplied by a quadrature matrix $\mathcal{Q}$.  In the case of the trapezoid rule, that matrix is $\Delta t I$ except for the $(1,1)$ and $(M+1,M+1)$ entries which are $\nicefrac{\Delta t}{2}$. However, the compact support of $\varphi$ means that the $\nicefrac{\Delta t}{2}$ values will be multiplied by a zero and thus we can disregard the whole matrix (see the discussion between Equations (5) and (6) on page 7 of \citep{BortzMessengerDukic2023BullMathBiol}).}. Therefore, the discretization of \eqref{eq:ode-weak-trick} is
\begin{equation}
	\label{eq:weakSys}
	-\dot{\dTestFun} \dstatemat \approx \dTestFun \dRhs(\params;\dstatemat,\dt)
\end{equation}
and would be satisfied perfectly if there was no noise and no numerical error in the quadrature. This motivates inspecting the weak residual, which we define as: 
\begin{equation}
	\label{eq:weakRes}
	\wRes(\params; \dstatemat, \dt) = \wRhs(\params;\dstatemat,\dt) - \wLhs(\dstatemat)
\end{equation}
where 
\[
\wRhs(\params;\dstatemat,\dt) \eqdef \operatorname{vec}[ \dTestFun \dRhs(\params;\dstatemat, \dt)], \;
\wLhs(\dstatemat) \eqdef - \operatorname{vec}[\dot{\dTestFun} \dstatemat]
\]
and ``$\operatorname{vec}$'' is the columnwise vectorization of a matrix. Since the noise $\noise_m \stackrel{iid}{\sim} N({\mathbf{0}},{\mathbf{\Sigma}})$, we know that  $\operatorname{vec}[\dNoise] \sim N({\mathbf{0}},  \mathbf{\Sigma} \otimes \id_{M+1})$ where $\otimes$ is the Kronecker product and $\id_{M+1}\in \mathbb{R}^{(M+1)\times (M+1)}$ is the identity matrix. 

\subsection{Distribution of the Weak Residual}

The idea of using the weak form equation error residual has been used since the 1950's for parameter estimation \citep{Shinbrot1954NACATN3288}. However, equation error-based estimation (both weak and strong form) exhibit a known bias \citep{Regalia1994IEEETransSignalProcess}. When the system is LiP, the effect can be diminished via correcting the covariance using an accurate statistical model for the noise \citep{BortzMessengerDukic2023BullMathBiol}. When the system is NiP, however, the residual $\wRes$ is a nontrivial transform of the corrupted data.

In this section, we provide a theoretical derivation of the distribution of the residual via two propositions and a lemma, followed by several remarks on the consequences of this result.

\begin{proposition} \label{prop:int-error}
	Let uncorrupted data $\dTrueStateMat$ and true parameters $\trueParams$ satisfy Equation \eqref{eq:ode-weak} on the time domain $[0,T]$. Assuming that $\rhs$ is continuous in time, the following holds:
	\[\lim_{M\rightarrow \infty} \bigl\|\wResQuad\bigr\| = 0\]
	where \(\wResQuad = \wRes(\trueParams;\dTrueStateMat,\dt)\)
\end{proposition}
\begin{proof}
	The true data satisfies the weak form of the system of differential equations as stated in Equation \eqref{eq:ode-weak} for all possible test functions. The $k^\text{th}$ entry of $\wResQuad$ is an approximation of Equation \eqref{eq:ode-weak} for a particular test function $\testFun_k$:
	\[r^{\text{int}}_k = \underbrace{\langle \testFun_k, \rhs(\trueParams, \state, \t)\rangle + \langle \dot{\testFun_k}, \state \rangle}_{=0} + \mathbf{e}^{\text{int} }\] 
	The inner products are computed via a quadrature rule. Relying on the assumption that $\state(t)$ is in a Sobolev space, for a fixed time domain the integration error, $\mathbf{e}^\text{int}$, converges to zero as the number of points $M$ increases:
	\[\lim_{M\rightarrow\infty} \|\wResQuad\| = \lim_{M\rightarrow\infty} \|\mathbf{e}^{\text{int}}\| = 0.\]
\end{proof}

\begin{proposition} \label{prop:noise}
	For uncorrupted  data $\dTrueStateMat$ and true parameters $\trueParams$ that satisfy Equation \eqref{eq:ode-weak}, we define the corrupted data as $\dstatemat = \dTrueStateMat + \dNoise$, where $\dNoise \sim \mathcal{N}(\mathbf{0},  \mathbf{\Sigma} \otimes \id_{M+1})$. Assuming that $\rhs$ is twice continuously differentiable in $\state$, the following holds:
	\[\wCov(\trueParams;\dstatemat,\dt)^{-\tfrac{1}{2}} \wResLin \sim \mathcal{N}({\mathbf{0}},\id_{KD}) \; \mathrm{and} \; \mathbb{E}\Bigl[\bigl\|\wResNoise - \wResLin\bigr\|\Bigr] = \mathcal{O}\Bigl(\mathbb{E}\bigl[\|\operatorname{vec}[\dNoise]\|^2\bigr]
	\Bigr) \] where 
	\begin{align*}
		\wResLin &\eqdef \nabla_{\state} \wRhs(\trueParams;\dstatemat,\dt)\operatorname{vec}[\dNoise] - \wLhs(\dNoise), \\
		\wResNoise &\eqdef \wRhs(\trueParams; \dstatemat, \dt) - \wRhs(\trueParams; \dTrueStateMat,\dt) -\wLhs(\dNoise),
	\end{align*}
	\[\wCov(\trueParams;\dstatemat,\dt) \eqdef \Bigl(\nabla_{\state} \wRhs(\trueParams;\dstatemat,\dt) + (\dot{\dTestFun} \otimes \id_D) \Bigr) \Bigl(  \mathbf{\Sigma} \otimes \id_{M+1} \Bigr) \Bigl(\bigl(\nabla_{\state} \wRhs(\trueParams;\dstatemat,\dt)\bigr)^T + (\dot{\dTestFun}^T \otimes \id_D)\Bigr),\]
	and $\nabla_{\state} \wRhs(\trueParams;\dstatemat,\dt) \in \R^{KD \times D(M+1)}$ is the Jacobian matrix of $\wRhs(\trueParams;\dstatemat,\dt)$ with respect to the state variable, $\state$.
\end{proposition}

\begin{proof}
	First, the distribution of $\wResLin$ follows from the Gaussian distribution of $\dNoise$. Since the linear combination of Gaussians is also Gaussian, we can compute the following mean and covariance conditioned on the observed data:
	\begin{align*}
		\mathbb{E}\bigl[\wResLin \mid \dstatemat\bigr] &= 0 \\ 
		\mathbb{E}\Bigl[\wResLin \bigl(\wResLin\bigr)^T \mid \dstatemat\Bigr] &= \Bigl(\nabla_{\state} \wRhs(\trueParams;\dstatemat,\dt) + (\dot{\dTestFun} \otimes \id_D) \Bigr) \Bigl(  \mathbf{\Sigma} \otimes \id_{M+1} \Bigr) \Bigl(\bigl(\nabla_{\state} \wRhs(\trueParams;\dstatemat,\dt)\bigr)^T + (\dot{\dTestFun}^T \otimes \id_D)\Bigr).
	\end{align*}
	To obtain the expected error from $\wResLin$ to $\wResNoise$, we begin by expanding $\wResNoise$ about the data $\dstatemat$. Taylor's Remainder Theorem guarantees that $\exists \mathbf{\hat{U}} \in \bigl\{X \in \mathbb{R}^{(M+1)\times D} \mid \|X - U\|_\infty \leq \|\dstatemat-\dTrueStateMat\|_\infty\bigr\}$ such that
	\[\wResNoise = \wRhs(\trueParams; \dstatemat, \dt) - \wLhs(\dNoise)- \bigl(\wRhs(\trueParams;\dstatemat,\dt) - \nabla_{\state} \wRhs(\trueParams;\dstatemat,\dt)\operatorname{vec}[\dNoise] + (\mathbf{H}_{\state}(\wRhs)(\trueParams; \mathbf{\hat{U}}, \dt) \bar{\times}_3 \operatorname{vec}[\dNoise]) \bar{\times}_2 \operatorname{vec}[\dNoise] \bigr) \]
	where $\mathbf{H}_{\state}(\wRhs): \mathbb{R}^J \times \mathbb{R}^{(M+1) \times D} \times \mathbb{R}^{M+1} \rightarrow \mathbb{R}^{KD\times (M+1)D \times (M+1)D}$ is the Hessian of $\wRhs$ with respect to $\state$\footnote{Here $\bar{\times}_n$ specifies the \emph{n-mode (vector) product} of a tensor with a vector (see section 2.5 of \citep{KoldaBader2009SIAMRev}).}. The $\wRhs(\trueParams; \dstatemat, \dt)$ terms cancel
	\[\wResNoise = \underbrace{\nabla_{\state} \wRhs(\trueParams;\dstatemat,\dt)\operatorname{vec}[\dNoise] - \wLhs(\dNoise)}_{=\wResLin} + (\mathbf{H}_{\state}(\wRhs)(\trueParams; \mathbf{\hat{U}}, \dt) \bar{\times}_3 \operatorname{vec}[\dNoise]) \bar{\times}_2 \operatorname{vec}[\dNoise].\]
	Subtracting $\wResLin$, taking the expectation of the norm of both sides, and then applying triangle inequality simplifies to the desired result
	\[\mathbb{E}\Bigl[\bigl\|\wResNoise - \wResLin\bigr\|\Bigr] = \mathcal{O}\Bigl(\mathbb{E}\bigl[\|\operatorname{vec}[\dNoise]\|^2\bigr]\Bigr).\]
\end{proof}

\begin{lemma} \label{lemma:wres-dist}
	Let true data $\dTrueStateMat$ and true parameters $\trueParams$ satisfy Equation \eqref{eq:ode-weak} on the time domain $[0,T]$, and for noise $\operatorname{vec}[\dNoise] \sim \mathcal{N}(\mathbf{0},  \mathbf{\Sigma} \otimes \id_{M+1})$, and corrupted data $\dstatemat = \dTrueStateMat + \dNoise$. Assuming that $\rhs$ is continuous in $\t$ and twice continuously differentiable in $\state$, the following holds: 
	\[ \wCov(\trueParams;\dstatemat,\dt)^{-\tfrac{1}{2}}\wResLin(\trueParams; \dstatemat, \dt) \sim \mathcal{N}({\mathbf{0}}, \id_{KD}) \; \mathrm{and} \; \lim_{M\rightarrow\infty} \mathbb{E}\Bigl[\bigl\|\wRes - \wResLin\bigr\|\Bigr] = \mathcal{O}\Bigl(\mathbb{E}\bigl[\|\operatorname{vec}[\dNoise]\|^2\bigr]\Bigr)\]
\end{lemma}

\begin{proof}
	This derivation is a natural extension of the one in \citep{BortzMessengerDukic2023BullMathBiol}. The key difference is that now it is not assumed that $\dRhs$ is linear in $\params$, and $\dRhs$ can explicitly depend on $\t$.

	Regardless of the form of $\rhs$, we have that $\wLhs$ is linear, thus:
	\[\wLhs(\dstatemat) = \wLhs(\dTrueStateMat) + \wLhs(\dNoise)\]
	Now consider $\trueParams$ to be the true parameters.
	\begin{align*}
	\wRes(\trueParams;\dstatemat,\dt) &= \wRhs(\trueParams;\dstatemat,\dt) - \wRhs(\trueParams;\dTrueStateMat,\dt) + \wRhs(\trueParams; \dTrueStateMat,\dt) - \wLhs(\dTrueStateMat) - \wLhs(\dNoise)
	\end{align*}
	Regrouping terms isolates the sources of error in the weak residual:
	\begin{align*}
		\wRes(\trueParams;\dstatemat,\dt) &= \underbrace{\wRhs(\trueParams; \dTrueStateMat,\dt) - \wLhs(\dTrueStateMat)}_{= \wResQuad} + \underbrace{\wRhs(\trueParams; \dstatemat, \dt) - \wRhs(\trueParams; \dTrueStateMat,\dt) -\wLhs(\dNoise)}_{= \wResNoise}\\
		\lim_{M\rightarrow \infty} \mathbb{E}\Bigl( \|\wRes - \wResLin\| \Bigr) &\leq \lim_{M\rightarrow \infty} \mathbb{E}\Bigl( \|\wResQuad\| \Bigr) + \mathbb{E}\Bigl( \|\wResNoise - \wResLin\|\Bigr)
	\end{align*}
	Combining the results from Propositions \ref{prop:int-error} and \ref{prop:noise}, the result holds. 
\end{proof}

\begin{remark}[Spectral Convergence of Integration Error]
	The grid is assumed to be uniform, so the quadrature rule chosen is the trapezoidal rule. This quadrature rule has spectral convergence because the periodic extension of a compact functions is itself \citep{Atkinson1989}. In particular, the order of the numerical error is $\mathcal{O}(\Delta t)^{(p+1)}$ where $p$ is the order of the smoothness of the test function at the endpoints of the integral domain. Thus in this context, $\wResQuad$ is negligible due the smoothness of $\varphi \ast u$. 
\end{remark}

\begin{remark}[Higher Order Terms]
    Unfortunately, higher order terms may not be negligible in noise regimes of interest, making the likelihood from the distribution in Lemma \ref{lemma:wres-dist}  unrepresentative of the true likelihood of the weak residual. In practice, the Gaussian approximation still leads to reasonable estimates of the parameters, especially when $\rhs$ has less nonlinearity with respect to $\state$ and at lower noise levels. 
\end{remark}

\subsection{Weak Likelihood Function}
In the previous WENDy-IRLS work \citep{BortzMessengerDukic2023BullMathBiol}, the covariance information was leveraged by applying an Iteratively Reweighted Least Squares (WENDy-IRLS) algorithm as described in \ref{sec:WENDy-IRLS}. Experimentally, we found that WENDy-IRLS did not converge sometimes when differential systems were highly nonlinear in the state variable $\state$ and when noise levels became too high. To prevent that, we now consider optimizing the negative log-likelihood of the weak residual to approximate the MLE. Because the residual has a conditional distribution that is multivariate normal, we can explicitly write down the analytic form of this weak negative log-likelihood:
\begin{equation}\label{eq:wnll}
	\begin{split}
	\loglikelihood(\params; \dstatemat, \dt) &= \frac{1}{2} \bigl( KD \log(2 \pi) + \log(\det(\wCov(\params;\dstatemat,\dt))) \\
	&+ (\wRhs(\params;\dstatemat,\dt)-\wLhs(\dstatemat))^TS(\params;\dstatemat,\dt)^{-1}(\wRhs(\params;\dstatemat,\dt)-\wLhs(\dstatemat)) \bigr).
	\end{split}
\end{equation}
Notice that $\loglikelihood$ is the sum of three terms. The first, $KD \log(2 \pi)$, is constant with respect to $\params$, and thus can be neglected in optimization. The second, $\log(\det(\wCov(\params;\dstatemat,\dt)))$, only depends on the eigenvalues of the covariance, and penalizes parameter values that lead to large uncertainty. The third term is the Mahalanobis Distance which is the Euclidean norm weighted by the covariance matrix, and penalizes parameter values that lead to large residuals. Balancing these last two terms minimizes uncertainty and equation error. Because the analytic form of the likelihood is known and can be efficiently computed, we can make use of modern optimization algorithms to find the MLE.

\subsubsection{Derivative Information}
First and second order derivative information of $\loglikelihood$ can be derived analytically, allowing for efficient use of second-order optimization routines that are robust to non-convex problems. All the derivative computations assume $\rhs$ is twice continuously differentiable in $\params$. For ease of notation and because the data is fixed, we drop explicit dependence on $\dstatemat$ and $\dt$: $\loglikelihood(\params) \eqdef \loglikelihood(\params;\dstatemat,\dt)$, $\wCov(\params) \eqdef \wCov(\params;\dstatemat,\dt)$, $\wRhs(\params) \eqdef \wRhs(\params;\dstatemat,\dt)$, and $\wLhs \eqdef \wLhs(\dstatemat)$.

\paragraph{Gradient Information}

Regardless of the linearity of $\rhs$ with respect to $\params$, the $j^\text{th}$ component of the gradient of the weak log-likelihood is given as 
\begin{equation} \label{eq:wnll-grad}
	\begin{aligned}
		\partial_{\params_j} \loglikelihood(\params) &= \frac{1}{2} \Bigl( \operatorname{Tr}\bigl(\wCov(\params)^{-1}\partial_{\params_j}\wCov(\params)\bigr) \\
		&+ 2 \bigl(\partial_{\params_j} \wRhs(\params)\bigr)^T \wCov(\params)^{-1} \bigl(\wRhs(\params)- \wLhs\bigr) \\
		&+ \bigl(\wRhs(\params)-\wLhs\bigr)^T\bigl(\partial_{\params_j} \wCov(\params)^{-1}\bigr)\bigl(\wRhs(\params)-\wLhs\bigr)\Bigr)
	\end{aligned}
\end{equation}
where
\begin{align*}
	\partial_{\params_j} \wCov(\params)^{-1} &= -\wCov(\params)^{-1} \left(\partial_{\params_j}\wCov(\params) \right) \wCov(\params)^{-1}\\
	\partial_{\params_j}\wCov(\params) &= \partial_{\params_j} \nabla_{\state} \wRhs(\params) \bigl(  \mathbf{\Sigma} \otimes \id_{M+1} \bigr)\bigl(\nabla_{\state} \wRhs(\params) + \dot{\dTestFun} \otimes \id_D \bigr)^T.
\end{align*}
\paragraph{Hessian Information}
Taking derivatives again, the elements of Hessian are
\begin{equation}
	\begin{aligned} \label{eq:wnll-hess}
		\partial_{\params_i \params_j} \loglikelihood(\params) &= \frac{1}{2}\Bigl( \operatorname{Tr}\bigl(\partial_{\params_i}\wCov(\params)^{-1} \partial_{\params_j}\wCov(\params) + \wCov(\params)^{-1} \partial_{\params_i,\params_j}\wCov(\params)\bigr) \\
		&+ 2\bigl(\partial_{\params_j} \wRhs(\params)\bigr)^T \bigl(\partial_{\params_i} \wCov(\params)^{-1}\bigr) \bigl(\wRhs(\params) - \wLhs\bigr) \\
		&+ 2\bigl(\partial_{\params_j} \wRhs(\params)\bigr)^T \wCov(\params)^{-1} \partial_{\params_i} \wRhs(\params) \\
		&+ 2\bigl(\partial_{\params_i,\params_j} \wRhs(\params)\bigr)^T \wCov(\params)^{-1} \bigl(\wRhs(\params) - \wLhs\bigr)\\
		&+ \bigl(\wRhs(\params) - \wLhs\bigr)^T\bigl(\partial_{\params_i,\params_j} \wCov(\params)^{-1}\bigr)\bigl(\wRhs(\params) - \wLhs\bigr)\Bigr)
	\end{aligned}
\end{equation}
where
\begin{align*}
	\partial_{\params_i \params_j} \wCov(\params)^{-1} &= \wCov(\params)^{-1} \partial_{\params_i} \wCov(\params) \wCov(\params)^{-1} \partial_{\params_j} \wCov(\params) \wCov(\params)^{-1} \\
	&- \wCov(\params)^{-1} \partial_{ij} \wCov(\params) \wCov(\params)^{-1} \\
	&+ \wCov(\params)^{-1} \partial_{\params_j} \wCov(\params) \wCov(\params)^{-1}  \partial_{\params_i} \wCov(\params) \wCov(\params)^{-1}\\
	\partial_{\params_i \params_j} \wCov(\params)&= \bigl(\partial_{\params_i,\params_j} \nabla_{\state} \wRhs(\params)\bigr) \bigl( \mathbf{\Sigma} \otimes \id_{M+1}\bigr) \bigl(\nabla_{\state} \wRhs(\params) + \dot{\dTestFun} \otimes \id_D\bigr)^T \\
	&+ \bigl(\partial_{\params_j} \nabla_{\state} \wRhs(\params)\bigr) \bigl( \mathbf{\Sigma} \otimes \id_{M+1}\bigr) \bigl(\partial_{\params_i} \wRhs(\params)\bigr)^T.
\end{align*}

\subsection{Extending to Log-Normal Noise} \label{sec:log}
In many systems, it is more appropriate to have multiplicative log-normal noise rather than additive Gaussian noise. This is particularly true when the state variable cannot be negative. This formally means the data is corrupted in the following way
\[\dstate_m = \dTrueState_m \circ \lognoise_m \quad \forall m \in \{0, \cdots, M\} \]
where \(\log(\lognoise_m) \stackrel{\text{iid}}{\sim} N({\mathbf{0}}, {\mathbf{\Sigma}})\) and $\circ$ denotes the Hadamard Product meaning multiplication occurs element-wise. Our convention is that logarithms and exponentials are applied element-wise.

The result from Lemma \ref{lemma:wres-dist} can be extended to the log-normal case. 
\begin{corollary} \label{cor:log-dist}
	Let true data $\dTrueStateMat$ and true parameters $\trueParams$ satisfy Equation \eqref{eq:ode-weak} on the time domain $[0,T]$, and for noise $\log(\dlognoise) \sim \mathcal{N}(\mathbf{0},  \mathbf{\Sigma} \otimes \id_{M+1})$, and corrupted data $\dstatemat = \dTrueStateMat \circ \dlognoise$. Assuming the $\logRhs$ is continuous in $\t$ and twice continuously differentiable in $\state$, the following holds for transformed weak residual:
	\[\wlogCov(\trueParams,\dstatemat, \dt)^{-\tfrac{1}{2}} \wlogResLin(\trueParams; \dstatemat, \dt) \sim N\bigl({\mathbf{0}}, \id_{KD}\bigr) \; \mathrm{and} \; \mathbb{E}\Bigl[\bigl\|\wlogRes - \wlogResLin\bigr\|\Bigr] = \mathcal{O}\Bigl(\mathbb{E}\bigl[\|\operatorname{vec}[\log(\dlognoise)]\|^2\bigr]\Bigr)\]
	where $\oslash$ is element-wise division and
	\[\begin{aligned}
		\logRhs(\params, \dstatemat, \dt) &\eqdef \rhs(\params, \state, \t) \oslash \state, \;
		\wlogRhs(\params;\dstatemat,\dt) \eqdef \operatorname{vec}\bigl[\dTestFun \logRhs(\params, \dstatemat, \dt)\bigr]\\
		\wlogLhs(\dstatemat) &\eqdef \operatorname{vec} \bigl[- \dot{\dTestFun} \log(\dstatemat)\bigr],\;
		\wlogRes \eqdef \wlogRhs(\params;\dstatemat,\dt) - \wlogLhs(\dstatemat), \;
		\wlogResLin \eqdef \nabla_{\state} \wlogRhs(\trueParams; \dstatemat, \dt)\operatorname{vec}[\log(\dlognoise)] - \wlogLhs(\dlognoise), \\
		\wlogCov(\params;\dstatemat,\dt) &\eqdef \bigl(\nabla_{\log(\state)} \wlogRhs(\params;\dstatemat,\dt) + \dot{\dTestFun}\bigr)\bigl( \mathbf{\Sigma} \otimes \id_{M+1}\bigr)\bigl(\nabla_{\log(\state)}\wlogRhs(\params;\dstatemat,\dt)+ \dot{\dTestFun}\bigr)^T
	\end{aligned}\]
\end{corollary}

\begin{proof}
	Start by defining a change of variables:
	\[\{\dlogstate_m\}_{m=0}^M \eqdef \{\log(\dstate_m)\}_{m=0}^M = \{\log(\dTrueState_m) + \log(\lognoise_m)\}_{m=0}^M.\]
	We build the corresponding data matrix \(\dlogstatemat \eqdef \begin{bmatrix}
		\dlogstate_0 & \cdots & \dlogstate_M
	\end{bmatrix}\).
	Applying chain rule we see that $\logstate$ satisfies the following ODE:
	\[\frac{d\logstate}{dt} = \rhs(\params, \exp(\logstate), \t) \oslash \exp(\logstate) \]
	Thus, we have the corresponding weak form of the ODE for transformed state variable:
	\[ -\langle \dot{\testFun}, \logstate \rangle = \langle \testFun , \rhs(\params, \exp(\logstate), \t) \oslash \exp(\logstate)\rangle. \]
	Notice that for $\dlogstatemat$, the noise is now additive and Gaussian, thus we satisfy the assumptions of Lemma \ref{lemma:wres-dist}, and can obtain the desired result.
\end{proof}
\begin{remark}[Derivative Computations for the Log-Normal Case]
	In practice, the computations for the RHS, LHS and corresponding derivatives stay identically the same, modulo the replacing of $\wLhs$ with $\wlogLhs$ and $\rhs$ with $\logRhs$. The computations for $\partial_{\params_j} \nabla_{\state}  \wRhs(\params;\dstatemat,\dt)$ and $\partial_{\params_i,\params_j}\nabla_{\state}  \wRhs(\params;\dstatemat,\dt)$ now become $\partial_{\params_j} \nabla_{\logstate}\wRhs(\params;\exp(\dlogstatemat))$ and  $\partial_{\params_i,\params_j}\nabla_{\logstate} \wRhs(\params;\exp(\dlogstatemat))$, respectively.
\end{remark}
\subsection{Distribution of Parameter Estimator}
Relying on the asymptotic consistency of the MLE, $\estim \stackrel{P}{\rightarrow} \trueParams$, the uncertainty information of the parameters can be estimated from the result of the WENDy-MLE algorithm. When the dynamics are nonlinear, we again utilize a linearization of $\wRhs$, but this time with respect to $\params$. This procedure works when the map, $\wRes(\params)$, is surjective. Empirically, this is true in practice because the Jacobian happens to be full rank. We can derive an approximation to the distribution of the estimator as follows:
\begin{equation} \label{eq:param-dist}
    \Bigl((\nabla_{\params} \wRhs)^{+}\wCov(\trueParams)(\nabla_{\params} \wRhs)^{+}\Bigr)^{-\tfrac{1}{2}}\params \stackrel{approx}{\sim} \mathcal{N}\bigl(\trueParams, \id_{J} \bigr) 
\end{equation}
where \((\nabla_{\params} \wRhs)^{+}\) is the Moore–Penrose inverse of \(\nabla_{\params} \wRhs\). 
The approximation becomes more accurate as $M \rightarrow \infty$. This provides powerful information about WENDy-MLE estimated parameters;  the algorithm is able to provide both an estimate and the uncertainty information about the estimate. 
