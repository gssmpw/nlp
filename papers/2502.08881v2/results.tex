\section{Implementation and Numerical Results}\label{sec:results}

This section contains a description of the maximum likelihood WENDy-MLE algorithm as well as the numerical results. In Section \ref{sec:wendyAlgo} we will discuss the algorithmic implementation. Then we introduce the details of a forward solver method used for comparison in Section \ref{sec:fslsq}. Next, the details of the metrics are shown in Section \ref{sec:metrics}, and artificial noise is discussed in Section \ref{sec:artificialNoise}. Our results begin in Section \ref{sec:motiv} where a motivating example demonstrates why alternatives to forward simulation based methods are necessary. Finally in Section \ref{sec:mega}, results for a suite of test problems demonstrate key capabilities of the WENDy-MLE algorithm. 

\subsection{WENDy-MLE Algorithm} \label{sec:wendyAlgo}
By explicitly forming the likelihood function for the weak residual in Equation \eqref{eq:wnll} and its derivative in Equations \eqref{eq:wnll-grad} and \eqref{eq:wnll-hess}, estimating the parameters can be framed as the following optimization problem: 
\begin{equation}
	\label{eq:wendyOptim}
	\estim = \underset{\substack{\params \in \mathbb{R}^J\\ \params_\ell \leq \params \leq \params_u }}{\operatorname{argmin}} \loglikelihood(\params)
\end{equation}
In general, this is a non-convex optimization problem, so it is appropriate to use trust-region second order methods that have been developed for this purpose.

WENDy-MLE takes the time grid $\dt$, data $\dstatemat$, a function for the RHS, $\rhs$, an initial guess for the parameters $\params_0$, and optionally box constraints for the parameters, $\params_\ell$ and $\params_u$. For most of the examples explored here, the unconstrained and constrained optimizers return nearly identical results.

\href{https://docs.sciml.ai/Symbolics/stable/}{Symbolics.jl} \citep{GowdaMaCheliEtAl2021ACMCommunComputAlgebra} is used to compute all partial derivative of $\rhs$, and then Julia functions are formed from those symbolic expression. Then, the weak likelihood and its derivatives can be evaluated using Equations \eqref{eq:wnll}, \eqref{eq:wnll-grad}, and \eqref{eq:wnll-hess}. These functions are then used in second order optimization methods\footnote{While the likelihood is a scalar valued function, its computation relies on the derivatives of vector and matrix valued functions. Building and using efficient data structures to compute these derivative can rely on ``vectorization'' resulting large matrices with block structure from Kronecker products. In our implementation we instead use multidimensional arrays and define the operations in Einstein summation notation. These computations are then evaluated efficiently with \href{https://github.com/mcabbott/Tullio.jl?tab=readme-ov-file}{Tullio.jl} \citep{AbbottAluthgeN3N5EtAl2023}.}. Trust region solvers are provided by \href{https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl}{JSOSolvers.jl} and \href{https://julianlsolvers.github.io/Optim.jl/stable/}{Optim.jl} for the constrained and unconstrained cases respectively \citep{KMogensenNRiseth2018JOSS, MigotOrbanSoaresSiqueira2024}. We note that our code also supports using the Adaptive Regularization Cubics variant (ARCqK) in the unconstrained case provide by \href{https://jso.dev/AdaptiveRegularization.jl/stable/} {AdaptiveRegularization.jl} \citep{DussaultMigotOrban2024MathProgram}. The trust region solvers and ARCqK usually produce similar results, but in our limited testing we found the trust region solvers work better in general, so only results for the trust region solver are shown. Our code, \href{https://github.com/nrummel/WENDy.jl}{WENDy.jl}, is readily available through the Julia package manager.

\subsection{Output Error Solver} \label{sec:fslsq}
For comparison, we use the standard output error approach of regressing the approximated state against the data. This is done by iteratively calling a direct numerical forward solver and optimizing over both the initial condition and the unknown parameters. We include the initial conditions as an optimization parameter since otherwise noisy initial data can drastically effect output error depending on the sensitivity of the system. The full scheme  can be framed as a nonlinear least squares problem:
\begin{equation}
	\label{eq:fslsq}
	\estim = \underset{\params \in \mathbb{R}^J }{\operatorname{argmin}} \Biggl[ \min_{\dstate_0 \in \mathbb{R}^D} \tfrac{1}{2} \Bigl\| \hat{\dstatemat}(\params, \dstate_0) - \dstatemat\Bigr\|^2_F \Biggr].
\end{equation}
We refer to this method Output Error Least Squares (OE-LS). Forward simulation is accomplished with the Rodas4P algorithm provided by \href{https://docs.sciml.ai/DiffEqDocs/stable/}{DifferentialEquations.jl} \citep{RackauckasNie2017JORS}. This solver is well suited to solve stiff system, so it is appropriate for all the example problems discussed here. Nonlinear least squares problems are solved by the Levenberg-Marquardt algorithm through \href{https://docs.sciml.ai/NonlinearSolve/stable/}{NonlinearSolve.jl} \citep{PalHoltorfLarssonEtAl2024arXiv240316341}. Derivative information is computed via automatic differentiation provided by \href{https://github.com/JuliaDiff/ForwardDiff.jl}{ForwardDiff.jl} \citep{RevelsLubinPapamarkou2016arXiv160707892}.

\subsection{Metrics} \label{sec:metrics}
Depending on the motivation behind the system identification, one may either care about identifying the parameters themselves, or only care about predictive performance. Hence we look at two complementary metrics: Coefficient Relative Error and Mean Forward Solve Relative Error. When relevant we also report the failure rate. Furthermore, we provide statistical metrics for our estimator, including Estimated Relative Bias, Estimated Relative Variance, Estimated Relative Mean Square Error, and Coverage. All metrics are defined in Table \ref{table:metrics}.

\begin{table}
    \begin{tabular}{lr}
            \toprule 
		  \textbf{Metric} &  \multicolumn{1}{c}{\textbf{Formula}}\\
		\midrule 
		\\[-5pt]
		Relative Coefficient Error & \begin{minipage}{7cm}
			\begin{equation} \label{eq:cl2}
				\dfrac{\|\estim - \trueParams\|_2}{\|\trueParams\|_2}\\[5pt]
			\end{equation}
		\end{minipage} \\
		\hline 
		\\[-5pt]
		Mean Forward Simulation Relative Error & \begin{minipage}{7cm}
			\begin{equation} \label{eq:fsl2}
				\dfrac{ \left\| \hat{\dstatemat}(\params, \dstate_0) - \dTrueStateMat\right\|_F}{ \left\| \dTrueStateMat\right\|_F} \\[5pt]
			\end{equation}
		\end{minipage} \\
        \hline  
		\\[-20pt]
		Failure Rate & \begin{minipage}{7cm}
			\begin{equation} \label{eq:failRate}\begin{split}
					\frac{1}{N} \sum_{n=1}^N \chi_{F}(\estim^{(n)}) \\
					F = \{\params \mid \text{forward simulation fails}\} \\[5pt]
			\end{split}
			\end{equation}
		\end{minipage} \\
		\hline 
		\\[-5pt]
		Estimated Relative Squared Bias & \begin{minipage}{7cm}
			\begin{equation} \label{eq:estimBias}
				\frac{ \Bigl(\sum_{n=1}^N \bigl(\hat{p}_j^{(n)} - p^*_j\bigr)\Bigr)^2}{N^2(p^*_j)^2} \\[5pt]
			\end{equation}
		\end{minipage} \\
		\hline 
		\\[-5pt]
		Estimated Relative Variance & \begin{minipage}{7cm}
			\begin{equation} \label{eq:estimVar}
				\frac{\sum_{n=1}^N \bigl(\hat{p}_j^{(n)} - \bar{\hat{p}}_j\bigr)^2}{N(p^*_j)^2} \\[5pt]
			\end{equation}
		\end{minipage} \\
		\hline 
		\\[-5pt]
		Estimated Relative Mean Squared Error (MSE) & \begin{minipage}{7cm}
			\begin{equation} \label{eq:estimMSE}
				\frac{\sum_{n=1}^N \bigl(\hat{p}_j^{(n)} - p^*_j\bigr)^2}{N(p^*_j)^2} \\[5pt]
			\end{equation}
		\end{minipage} \\
		\hline 
		\\[-20pt]
		Estimated Coverage & \begin{minipage}{7.5cm}
            \begin{equation} \label{eq:estimCov}
                \begin{split}
					\frac{1}{N} \sum_{n=1}^N \chi_A(\hat{p}_j^{(n)}) \\
                    A = \Bigl\{\hat{p}_j \mid \bigl\lvert\hat{p}_j^{(n)} - p^*_j\bigr\rvert <  2 \sqrt{C_{j,j}^{(n)}}\Bigr\}\\[5pt]
                \end{split}
			\end{equation}
		\end{minipage} \\
		\bottomrule
	\end{tabular}
	\caption{Notice, that in the computation for Mean Forward Simulation Relative Error the true initial condition is used. The metrics of bias, variance and mean squared error are averaged over all $N$ runs. $\chi$ is the indicator function.
	}
	\label{table:metrics}
\end{table}

We say an algorithm failed if its estimated parameters cause forward simulation to return \jlinl{NaN} values. Failure rates are only reported when at least on algorithm failed more than 1\% of the time. For failed runs their accuracy metrics are replaced with the accuracy metric of the initial parameters, $\estim_0$.

\subsection{Corrupting the Data With Artificial Noise} \label{sec:artificialNoise}

For testing purposes, the data is corrupted with noise before it is passed as an input to the WENDy-MLE or OE-LS algorithms. When noise is additive Gaussian, we add noise proportional to the size the state variables as described in Equation \eqref{eq:nr}:
\begin{equation} \label{eq:nr}
	\begin{split}
		\noise \stackrel{iid}{\sim} \mathcal{N}(0, \Sigma_{nr})\\
		\Sigma = \operatorname{diag}\Biggl(\frac{\sigma_{nr} \|U\|_F^2}{M+1}\mathbf{1}_D\Biggr)
	\end{split}
\end{equation}
where $\sigma_{nr}$ is the noise ratio. For the multiplicative log-normal noise we use a scale matrix of $\sigma_{nr}\mathbb{I}_D$: 
\begin{equation} \label{eq:lognr}
	\begin{split}
		\log(\lognoise) \stackrel{iid}{\sim} \mathcal{N}(0, \sigma_{nr}\mathbb{I}_D).
	\end{split}
\end{equation}

\subsection{Motivating Example: Lorenz Oscillator} \label{sec:motiv}

The Lorenz Oscillator described by Equation \eqref{eq:lorenz} is a prototypical example of a chaotic system \citep{Sparrow1982}. Estimating the parameters in a chaotic system is difficult with output error methods because forward simulation is sensitive to the initial conditions given. In practice true (noiseless) initial conditions are not always available. The weak form methods do not suffer from this complication in the same way. 

To demonstrate this, we design an experiment where both solvers are given progressively more data from $[0, T]$ where $T \in \{3n\}_{n=1}^{10}$. One would hope that given more data, each algorithm would give progressively better estimates of the parameters, but in fact Figure~\ref{fig:lorenz_experiment} shows that in fact it gets worse with more data.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.3\columnwidth]{lorenz_solution3d.pdf}
	\includegraphics[width=0.5\columnwidth]{lorenz_time_experiment.skipZeroNoise_true.errorbars_false.metric_cl2.pdf}
	\caption{Left: the solution for the Lorenz oscillator in black and the corrupted data in grey dots. Right: the relative coefficient error for both the WENDy-MLE solver and the output error solver. }
	\label{fig:lorenz_experiment}
\end{figure}

For this experiment, we run each algorithm thirty times for each $T$. On each run, we vary additive Gaussian noise and initial guess for the parameters. Noise is added at a noise ratio of 10\% as described in Equation \eqref{eq:nr}. The data is given on a uniform grid with $\Delta T = 0.01$. The plot shows the mean coefficient relative error as described in Equation \eqref{eq:cl2} for both algorithms as well as the initial parameters. As more data becomes available, WENDy-MLE maintains an excellent estimate of the parameters while the OE-LS estimate degrades monotonically. 

\subsection{Test Suite} \label{sec:mega}

We run both the WENDy-MLE and OE-LS algorithm over a suite of systems of differential equations described in Table \ref{tab:odes}. For each system, we run the algorithm 100 times at varying noise ratios and subsamplings of the data. Higher levels of noise and fewer data points increases the difficulty of recovering the unknown parameters. The final time is fixed at the specified value of $T$, but subsampling causes $\Delta T$ to increase.   

\begin{table}[H]
\resizebox{\textwidth}{!}{
\begin{tabular}{crcc}
	\toprule 
	 \textbf{Name} & \multicolumn{1}{c}{\textbf{ODE}} & \textbf{Parameters} &  \textbf{Initial Parameterization} \\
	\midrule 
	\\[-5pt]
	Logistic Growth & \begin{minipage}{4cm} \begin{equation} \label{eq:logistic}
		\dot{u}=p_1 u+p_2 u^2
	\end{equation} \end{minipage} & 
	\begin{minipage}{4cm} \(\begin{aligned}
		\t &\in [0,10]\\
		\state(0)&=0.01 \\
		\trueParams&=[1,-1]^T
	\end{aligned} \)\end{minipage} & \begin{minipage}{4cm} \(
		p_1\in [0,10],\; p_2\in [0,10]\)\end{minipage} \\\\[-5pt]
	\hline
	\\
	Hindmarsh-Rose & \begin{minipage}{5.5cm} 
		\begin{equation} \label{eq:hindmarshRose}
			\begin{aligned}
			\dot{u}_1 &= p_1 u_2 - p_2 u_1^3 \\
			&+ p_3 u_1^2 - p_4 u_3 \\
			\dot{u}_2 &= p_5 -  p_6 u_1^2+ p_7 u_2 \\
			\dot{u}_3 &= p_8 u_1+  p_9 - p_{10} u_3
			\end{aligned}
		\end{equation}
	\end{minipage} & 
	\begin{minipage}{5cm} \(\begin{aligned}
		t&\in[0,10]\\
		\state(0)&=[-1.31,-7.6,-0.2]^T \\
		\trueParams&=[10,10,30,10,10,50, \\
		&\hspace{0.25cm} 10,0.04,0.0319,0.01]^T
	\end{aligned}\) \end{minipage} & \begin{minipage}{4cm} \(\begin{aligned}
		p_1&\in[0,20],\;p_2\in[0,20]\\
		p_3&\in[0,60],\;p_4\in[0,20]\\
		p_5&\in[0,20],\;p_6\in[0,100]\\
		p_7&\in[0,20],\;p_8\in[0,1]\\
		p_9&\in[0,1],\;p_{10}\in[0,1]
	\end{aligned}\) \end{minipage} \\\\[-5pt]
	\hline 
	\\[-5pt]
	Lorenz & \begin{minipage}{5.5cm} 
		\begin{equation} \label{eq:lorenz}
			\begin{aligned}
				\dot{u}_1 &= p_1(u_2-u_1) \\ 
				\dot{u}_2 &= u_1(p_2-u_3)-u_2 \\ 
				\dot{u}_3 &= u_1 u_2-p_3 u_3
			\end{aligned}
		\end{equation}
	\end{minipage} & 
	\begin{minipage}{5cm} \(\begin{aligned}
		 t&\in[0,10]\\
		\state(0)&=[2, 1, 1]^T \\
		 \trueParams&=[10, 28, 8/3]^T
	\end{aligned}\) \end{minipage} & \begin{minipage}{4cm} \(\begin{aligned}
		p_1&\in [0,20],\;p_2\in [0,35]\\
		p_3&\in [0, 5]
	\end{aligned}\) \end{minipage} \\\\[-5pt]
	\hline 
	\\[-5pt]
	Goodwin (2D) & \begin{minipage}{5cm} 
		\begin{equation} \label{eq:goodwin2d}
			\begin{aligned}
				\dot{u}_1 &= \frac{p_1}{36 + p_2u_2} - p_3 \\
				\dot{u}_2 &= p_4 u_1 - p_5
			\end{aligned}
		\end{equation}
	\end{minipage} & 
	\begin{minipage}{5cm} 
		\(\begin{aligned}
		t &\in [0,60]\\
		\state(0) &= [7, -10]^T \\
		\trueParams  &= [72, 1, 2, 1, 1]^T
		\end{aligned} \)
	\end{minipage} & \begin{minipage}{4cm} \(\begin{aligned}
		p_1&\in [60,80],\;p_2\in [1,3]\\
		p_3&\in [0.5,3],\;p_4\in [0.5,3]\\
		p_5&\in [0.5,3]
	\end{aligned}\) \end{minipage}\\\\[-5pt]
	\hline 
	\\[-5pt]
	Goodwin (3D) & \begin{minipage}{5cm} 
		\begin{equation} \label{eq:goodwin3d}
			\begin{aligned}
			\dot{u}_1 &= \frac{p_1}{2.15 + p_3 u_3^{p_4}} - p_2  u_1 \\
			\dot{u}_2 &= p_5u_1- p_6u_2 \\
			\dot{u}_3 &= p_7u_2-p_8u_3
			\end{aligned}
		\end{equation}
	\end{minipage} & 
	\begin{minipage}{5.5cm}
		\(\begin{aligned}
			t &\in [0,80]\\
			\state(0) &= [0.3617, 0.9137, 1.3934]^T \\
			\trueParams &= [3.4884, 0.0969, 10, 0.0969, \\
			&\hspace{0.25cm}  0.0581, 0.0969, 0.0775]^T
		\end{aligned} \)
	\end{minipage} & \begin{minipage}{4cm} \(\begin{aligned}
		p_1 &\in [1,5], \; p_2\in[0,0.2] \\
		p_3 &\in [0,2], \; p_4\in[5, 15] \\
		p_5 &\in [0,0.2], \; p_6\in[0,0.2] \\
		p_7 &\in [0,0.2], \; p_8\in[0,0.2] \\
	\end{aligned}\) \end{minipage}\\\\[-10pt]
	\hline 
	\\[-5pt]
	SIR-TDI & \begin{minipage}{5cm} 
		\begin{equation} \label{eq:sir} 
			\begin{aligned}
				\dot{u}_{1} &= -p_{1}  u_{1} + p_{3}  u_{2} \\
				&+ \tfrac{p_1 e^{-p_1  p_2}}{1 - e^{-p_1  p_2}} u_{3} \\
				\dot{u}_{2} &= p_{1}  u_{1} - p_{3}  u_{2} \\
				& - p_{4}  (1 - e^{-p_{5}  t^2})  u_{2} \\
				\dot{u}_{3} &= p_{4}  (1 - e^{-p_{5}  t^2})  u_{2} \\
				&- \tfrac{p_1 e^{-p_1  p_2}}{1 - e^{-p_1  p_2}}  u_{3}
			\end{aligned}
		\end{equation}
	\end{minipage} & 
	\begin{minipage}{5cm} 
		\(\begin{aligned}
			t &\in [0,50]\\
			\state(0) &= [1,0,0]^T \\
			\trueParams  &= [1.99, 1.5, 0.074, 0.113, \\
			& \hspace{0.25cm} 0.0024]^T 
		\end{aligned} \)
	\end{minipage} & \begin{minipage}{4cm}\( \begin{aligned}
		p_1 &\in [10^{-4},1], \; p_2 \in [10^{-4},2] \\
		p_3 &\in [10^{-4},1], \; p_4 \in [10^{-4},1] \\
		p_5 &\in [10^{-4},1] \\
	\end{aligned}\) \end{minipage}\\\\[-5pt]
	\bottomrule
\end{tabular}
}
\caption{Table of example systems that are presented in the results.}
\label{tab:odes}
\end{table}

Logistic Growth and Hindmarsh Rose are both linear in parameters and results were presented in the prior work of \citep{BortzMessengerDukic2023BullMathBiol}. The improved WENDy-MLE algorithm presented here slightly improves accuracy metrics on these examples and adds significant robustness in the case of the Hindmarsh Rose. The Lorenz system is selected because it exemplifies why choosing a weak form method can be necessary. This system is affine in parameters but not linear. Both Goodwin models are chosen because the are NiP and have Hill functions that are of particular interest in many applications \cite{HIll1910JPhysiol,GoutelleMaurinRougierEtAl2008FundamemntalClinicalPharma}. The SIR-TDI model is NiP and was chosen to demonstrate WENDy-MLE's new ability to handle $\rhs$ with a direct dependence on $\t$. Summary information about each system is shown in Table \ref{tab:odes-desc}.


\begin{table}[H]
    \centering
	\begin{tabular}{lll}
		\toprule	
		\rowcolor{white} \textbf{Name} & \textbf{Linear in Parameters} & \textbf{Noise Distribution} \\
		\midrule	
		\cellcolor{white} Logistic Growth &  Linear & Additive normal \\
		\cellcolor{white} Hindmarsh-Rose & Linear & Additive normal \\
		\cellcolor{white} Lorenz & Linear & Additive normal   \\
		\cellcolor{white} Goodwin (2D) &  Nonlinear & Additive normal   \\
		\cellcolor{white} Goodwin (3D) & Nonlinear & Multiplicative log-normal  \\
		\cellcolor{white} SIR-TDI & Nonlinear & Multiplicative log-normal   \\
		\bottomrule
	\end{tabular}
	\caption{Characteristics of the test problems.}
	\label{tab:odes-desc}
\end{table}

\subsubsection{Logistic Growth} \label{sec:logistic}

The logistic growth equation is a well-known differential equation that has an exact solution. Classically, it was developed to describe population growth at rate $r$ with a carrying capacity $K$, \(\dot{u} = ru(1-\tfrac{u}{K})\) \citep{Verhulst1838Correspondancemathematiqueetphysiquev10}. We have reparameterized this equation to the form seen in Equation \eqref{eq:logistic} to be amenable to optimization. 
\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{logisticGrowth_megaJob.pdf}
	\includegraphics[width=0.9\textwidth]{logisticGrowth_UQ.pdf}
	\caption{Both algorithms are run at noise ratios from 1\% to 50\% and $M = \{256, 512, 1024\}$ for a fixed time $T=10$. The top row plots accuracy metrics for OE-LS in blue, WENDy-MLE in gold, and initial parameterization in black. In the bottom plots is the mean parameter estimate and mean 95\% confidence intervals for each parameter estimated by WENDy-MLE in gold and the true parameter in gray.}
	\label{fig:logisticMega}
\end{figure}
We see that WENDy-MLE is able to recover the parameters to an accuracy of about 20\% relative coefficient error even when noise levels are at 50\%. On the other hand the forward solver's performance degrades to errors above 100\% for both the coefficient error and forward solve error. Investigation of this phenomena demonstrated that the degradation in the OE-LS algorithm was primarily caused by corruption of the initial condition. Because the noise is additive, the data for the initial condition $\dstate_0$ can become negative. When this occurs, OE-LS cannot reliably navigate in the cost space back to $\hat{\dstate}_0 > 0$. In contrast, WENDy-MLE does not suffer from this complication, and even when the initial condition is corrupted to a non-physical negative value.

\input{fig/logisticGrowth_UQ_summary.tex} These metrics are plotted in the appendix in Figure \ref{fig:logisticSup} and are shown in Tables \ref{tab:logUQ1} and \ref{tab:logUQ2} in the Appendix. The bias, variance and MSE are particularly low for the WENDy-MLE solver. The main driver in MSE is the bias that is caused by the nonlinearity in the RHS $\rhs$ of the state variable $\state$. The coverage demonstrates that WENDy-MLE overestimates the uncertainty of its estimate which is preferable to the alternative. For this example the WENDy-MLE algorithm significantly outperforms the the forward based solver, and as the noise increases, all metrics degrade gracefully. 

\subsubsection{Hindmarsh Rose} \label{sec:hindmarsh}
The Hindmarsh-Rose Model was initially developed to explain neuronal bursting \citep{HindmarshRose1984ProcRSocLondBBiolSci}. Both state variables $\state_1$ and $\state_2$ demonstrate this bursting. Estimation of the parameters is complicated by the differences in scales. In particular, $p^*_6 = -50$ and $p^*_9 = 0.0318$ which are separated by three orders of magnitude. The WENDy-MLE algorithm use of the likelihood addresses this concern.
\begin{figure}[H]
	\centering
    \includegraphics[width=0.9\textwidth]{fig/hindmarshRose_megaJob_annotated.pdf}
	\includegraphics[width=0.9\textwidth]{hindmarshRose_UQ.pdf}
	\caption{All algorithms are run at noise ratios from 1\% to 20\% and $M = \{256, 512, 1024\}$. In plots of the top row, we see accuracy metrics for for OE-LS in blue, WENDy-IRLS in green, WENDy-MLE in gold, and initial parameterization in black. In the bottom plots, we see the WENDy-MLE's mean parameter estimate and mean 95\% confidence intervals for each parameter in gold and the true parameter is in gray.}
	\label{fig:hindMega}
\end{figure}
An initial motivation for this work was to improve performance on this example because WENDy-IRLS does not always converge, especially with less data and high noise, $\sigma_{nr} \gg 0$. The likelihood approach described here shows a mild improvement in accuracy metrics over the generalized least squares approach, but its real benefit is the improvement on convergence.

\input{fig/hindmarshRose_UQ_summary.tex} In the appendix, these metrics are shown in Figure \ref{fig:hindmarshSup} and in Tables \ref{tab:hindUQ1}, \ref{tab:hindUQ2}, \ref{tab:hindUQ3}, and \ref{tab:hindUQ4}. For this example the WENDy-MLE algorithm has a bias that affects the MSE for all parameters. Bias stays below 25\% for a noise ratio less than 10\% for all subsampling rates for all parameters except $p_8,p_9$ and $p_{10}$. Those parameters suffer from a lack of identifiability mentioned in \citep{BortzMessengerDukic2023BullMathBiol} which comes from adding noise proportional to $\|\bar{\dstate}\|$ rather than having noise be anisotropic accross state components. 

Coverage at noise ratios below 5\% stays above 90\% for all variables. Uncertainty estimates for $p_8,p_9$ and $p_{10}$ stay sufficiently high resulting in a coverage >95\% for all noise levels in these parameters. On the other hand, the other parameters all have coverage rates monotonically decrease as the noise ratio increase. This is because Proposition \ref{prop:noise} is inapplicable when the noise is not sufficiently small. The linearization used to obtain the weak likelihood becomes a worse estimate for the true likelihood as the noise ratio increases. Also, when the data rate becomes sparser (smaller $M$) the coverage also degrades. Since the solution to the Hindmarsh system has discontinuities, having sufficient data around these events is paramount for any estimation method. 

\subsubsection{Lorenz} \label{sec:lorenz-mega}

The Lorenz system is a canonical chaotic system that was discussed in more depth in Section \ref{sec:motiv}. Here we are running the WENDy-MLE and OE-LS algorithms on this system of ODEs and looking more broadly at how WENDy-MLE compares to the forward based solver in a variety of conditions.   
\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{lorenz_megaJob.pdf}
	\includegraphics[width=0.9\textwidth]{lorenz_UQ.pdf}
	\caption{Both algorithms are run at noise ratios from 1\% to 30\% and $M = \{256, 512, 1024\}$. In plots of the top row, we see accuracy metrics for for OE-LS in blue, WENDy-IRLS in green, WENDy-MLE in gold, and initial parameterization in black. In the bottom plots, we see the WENDy-MLE's mean parameter estimate and mean 95\% confidence intervals for each parameter in gold and the true parameter is in gray.} 
	\label{fig:lorenzMega}
\end{figure}

The median of performance of the forward solver is considerably worse that WENDy-MLE's, consistent across all noise and sampling levels. \input{fig/lorenz_UQ_summary.tex} More detailed results are shown in Figure \ref{fig:lorenzSup} and in Tables \ref{tab:lorenzUQ1} and \ref{tab:lorenzUQ2} in the Appendix. The bias, variance, and MSE all are reasonable for this problem, all staying below 20\% for all parameters at all noise levels. Interestingly, coverage is poor at low noise levels, and improves for moderate noise levels, only to then degrade again for noise levels greater than 16\%. We hypothesize this is because numerical error in the forward simulation is on a similar level as the additive noise, hence the Gaussian assumption about the noise is less valid at these noise levels. Once the error is dominated by the additive Gaussian noise, then the covariance produced by WENDy-MLE improves.

\subsubsection{Goodwin 2D}\label{sec:goodwin2d}
A simple example of a system of differential equations which is nonlinear in parameters is the Goodwin model which describes negative feedback control processes \citep{Calderhead2012PhD,Goodwin1965AdvancesinEnzymeRegulation}. This example was chosen because of the nonlinear right hand side. In particular, the right hand side contains a rational function known in the mathematical biology literature as a Hill function \cite{HIll1910JPhysiol,GoutelleMaurinRougierEtAl2008FundamemntalClinicalPharma}. The parameter $p_2$ appears in the denominator, and thus this serves as an example of how nonlinearity can effect the performance of the WENDy-MLE algorithm. 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{multimodal_megaJob.pdf}
	\includegraphics[width=0.9\textwidth]{multimodal_UQ.pdf}
	\caption{All algorithms are run at noise ratios from 1\% to 25\% and $M = \{256, 512, 1024\}$. In plots of the top row, we see accuracy metrics for for OE-LS in blue, WENDy-MLE in gold, the hybrid approach in maroon, and initial parameterization in black. In the bottom plots, we see the WENDy-MLE's mean parameter estimate and mean 95\% confidence intervals for each parameter in gold and the true parameter is in gray.}
	\label{fig:goodwin2dMega}
\end{figure}

In the this example, we see that the OE-LS algorithm on median has coefficient and forward solve error much worse than the error from the initial guessed parameterization. Furthermore, the OE-LS method has failure rates above five percent for all subsample rates. Also, WENDy-MLE on median chooses parameters that degrade the coefficient error, but it improves the forward solve error compared to the initial parameterization. 

Interestingly enough, the two algorithms have complementary strengths, and it is possible to pair them together to get better results. Specifically, we create a ``hybrid'' approach that first uses WENDy-MLE to estimate parameters, which are then used as an initial guess for the OE-LS algorithm. As evident in the figure, this hybrid approach greatly improves on either standalone method with respect to both error metrics.

\input{fig/multimodal_UQ_summary.tex} These metrics are shown in the Appendix in Figure \ref{fig:multimodalSup} and in Tables \ref{tab:goodwin2DUQ1} and \ref{tab:goodwin2DUQ2}. Variance for this problem drives the increase in MSE. Upon inspection of the cost space, we found that there are multiple local minimum in the parameter range explored. This causes WENDy-MLE to converge to different local optimum depending on initialization. The output error method also suffers from a multimodal cost space, and WENDy-MLE's cost space has a large basin around the minimum closes to truth. 

The complications from multimodality spill over and effect coverage. Specifically, for $p_2, p_4$, and $p_5$ the coverage is poor. Another possible source of error could be from the nonlinearity of $\rhs$ in $p_2$. This coupled through a connection $u_2$ causes degradation in metrics for $p_4$ and $p_5$ as well. It is known that estimating $p_1, p_2$ jointly can cause identifiability problem, as the ratio of numerator and denominator can be balanced rather than converging to the true values. Coverage for all parameters improves as the noise level increased, so it could be that there is insufficient information on this time interval to recover this parameter. It is interesting that this causes problems with estimating $p_4$ and $p_5$.

\subsubsection{Goodwin 3D} \label{sec:goodwin3d}
A slightly more complicated version of the Goodwin model is shown in Equation \eqref{eq:goodwin3d}. We see that algorithms are now estimating the hill coefficient, $p_4$, in the exponent of the state variable. For $p^*_4=10$ sinusoidal oscillations occur in the state variables, while smaller values do not exhibit this behavior \citep{GonzeAbou-Jaoude2013PLoSONE}. 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{goodwin_megaJob.pdf}
	\includegraphics[width=0.9\textwidth]{goodwin_UQ.pdf}
	\caption{Both algorithms are run at noise ratios from 1\% to 25\% and $M = \{256, 512, 1024\}$. In plots of the top row, we see accuracy metrics for for OE-LS in blue, WENDy-IRLS in green, WENDy-MLE in gold, and initial parameterization in black. In the bottom plots, we see the WENDy-MLE's mean parameter estimate and mean 95\% confidence intervals for each parameter in gold and the true parameter is in gray}
	\label{fig:goodwin3dMega}
\end{figure}

Both algorithms perform reasonably well on this example, but we see that WENDy-MLE outperforms the OE-LS on coefficient error while OE-LS outperforms WENDy-MLE slightly on forward simulation error. When the hybrid approach is used, it has comparable results to the OE-LS method alone.  While the forward solve method does not fail as often as on the two dimensional Goodwin example, there still are cases where we saw the forward solver fail. It was surprising that the WENDy-MLE algorithm performed this well because moving to the log-normal noise distribution further adds more nonlinearity to the cost function, so it was expected that the WENDy-MLE algorithm would be more limited in its effectiveness. It is also surprising that using WENDy-MLE as an initialization significantly improves on the performance of the forward based solver. Upon inspection, this is because the WENDy-MLE optimum is often within a domain of convergence for the forward based solver. 

\input{fig/goodwin_UQ_summary.tex} The metrics for each parameter are shown in Figure \ref{fig:goodwinSup} and Tables \ref{tab:goodwin3DUQ1}, \ref{tab:goodwin3DUQ2}, \ref{tab:goodwin3DUQ3} \ref{tab:goodwin3DUQ4}, and \ref{tab:goodwin3DUQ5} in the Appendix. Bias drives the MSE for this example caused by the nonlinearity of $\rhs$ in both $\state$ and $\params$. In particular, $p_3$ exhibits the largest bias nearly reaching 100\%. This parameter mirrors $p_2$ in the previous example, and struggles from the same identifiability problem. It was surprising the metrics improved for the three dimensional Goodwin example. Because log-normal noise is used here, the magnitude of the noise is now proportional to each state $u_d$. Thus, the noise is anisotropic after the transformation, and this leads to the parameters connected to $u_3$ more identifiable.

Coverage is particularly good for this example. Only the coverage for $p_1$ and $p_3$ ever falls below 95\%. This is somewhat surprising because this is an more complicated version of the system described in Equation \eqref{eq:goodwin2d}. The dynamics for the parameters in the previous example are on smaller time scales that here, thus making estimating the parameters for the Equation \eqref{eq:goodwin3d} more obtainable.   

\subsubsection{SIR-TDI} \label{sec:sir}
The susceptible-infected-recovered (SIR) model is pervasive in epidemiology. The system described in Equation \eqref{eq:sir} described an extension that allows for time delayed immunity (TDI) for parasitic deceases where there is a common source for infection \cite{ShonkwilerHerod2009}. 
\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{sir_megaJob.pdf}
	\includegraphics[width=0.9\textwidth]{sir_UQ.pdf}
	\caption{Both algorithms are run at noise ratios from 1\% to 20\% and $M = \{256, 512,1024\}$. In plots of the top row, we see accuracy metrics for for OE-LS in blue, WENDy-MLE in gold, and initial parametrization in black. In the bottom plots, we see the WENDy-MLE's mean parameter estimate and mean 95\% confidence intervals for each parameter in gold and the true parameter is in gray}
	\label{fig:sirMega}
\end{figure}

Both algorithms  perform well on this example on both metrics. WENDy-MLE slightly outperforms the forward solver based method, but  as the noise level increases the gap between the two algorithms' performance decreases. It is noteworthy to mention at first the WENDy-MLE algorithm was run without constraints, but for this example in particular WENDy-MLE's performance was much worse. This is most likely due to the non-linearity in the cost space caused by the highly nonlinear right hand side of Equation \eqref{eq:sir}. The nonlinearity is further exacerbated by multiplicative log-normal noise which requires applying logarithms on both sides of the equation as described in Section \ref{sec:log}.

\input{fig/sir_UQ_summary.tex} These metrics are shown in Figure \ref{fig:sirSup} and in Tables \ref{tab:sirUQ1} and \ref{tab:sirUQ2} in the Appendix. Bias, variance and MSE are all below 5\% for all parameters and subsamplings (values of $M$) except for $p_5$ and $M=256$ at noise ratios larger than 15\%. $p_5$ is a nonlinear parameter that is an exponential rate. Its true value of $0.0024$ is relatively small compared to the other parameters in this system. With less data and higher noise, it is not surprising that this parameter becomes harder to identify.

We over estimate the uncertainty in the covariance on every estimate of for this example resulting in a coverage larger than the nominal 95\% for all parameters across the board. While this implies that our confidence intervals are likely larger than necessary, we note again that higher-than-nominal coverage is preferable to the alternative.
