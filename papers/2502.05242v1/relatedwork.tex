\section{Related Work}
\textbf{Interpretability of LLMs.} Global interpretability methods provide insight into the internal mechanisms of LLM \cite{liu2023towards, zhou2024explaining, ren2024identifying, ren2024towards, dang2024explainable}. Previously, external modules are often trained to identify semantic information from intermediate representations \cite{liu2025latent,liu2024efficient}. Some methods also project representations into the vocabulary space \cite{Nostalgebraist2020} or other interpretable space \cite{gao2024scaling,lieberum2024gemma}. Due to the powerful capabilities of LLMs, they are utilized to explain representations with natural language by directly decoding representations \cite{chen2024selfie,ghandeharioun2024patchscope} or summarizing patterns of representations \cite{bills2023language}. When an LLM serves as the explained model and the explaining tool simultaneously in these above approaches, they can be called self-explaining methods. Self-explaining methods use models to explain themselves. Instead of explaining representations, chain-of-thought prompting (CoT, \citet{nye2021show,wei2022chain}) enables LLMs to tell how they make predictions and improves reasoning ability. However, \citet{huang2023can} and \citet{turpin2024language} show that CoT may provide unfaithful explanations and bring potential dangers to the utilization of LLMs.

\textbf{Representations of LLMs.} Several works focus on representations of LLMs instead of their output on tasks of LLMs' alignment \cite{li2024open,yin2024lofit,qian2024towards,zhang2024real,zhang2024better}, evaluation \cite{wei2024diff,azaria2023internal,xu2024good,azaria2023internal,orgad2024llms} and copyright protection \cite{zhang2024reef,sevastjanova2022lmfingerprints,yang2024fingerprint}. \citet{li2024inference} design steering vectors and insert them into model representations to control model generations without training. \citet{rosati2024representation}, \citet{zou2024improving} and \citet{li2024wmdp} perform machine unlearning by rotating the representation of harmful samples or pushing them towards a random distribution. What's more, \citet{wu2024reft} performs intervention functions precisely on the model's target layer and the target position of the input tokens. \citet{qian2024dean} disentangles LLMsâ€™ awareness of fairness and privacy by deactivating the entangled neurons in representations.

\textbf{Contrastive Learning.} Contrastive self-supervised learning on computer vision utilizes positive and negative pairs constructed by data augmentation to learn general and high-quality representations \cite{he2020momentum,chen2020simple,zbontar2021barlow}. \citet{radford2021learning} connects natural language and visual modality through contrastive learning of text-image pairs. Recent work extracts human value representations of LLMs by applying multi-view contrastive Learning \cite{cahyawijaya2024high}. 


\begin{figure*}[t]
    \vspace{-5pt}
    \centering
    \includegraphics[width=1\linewidth, clip]{methods_v12.pdf}
    \vspace{-15pt}
    \caption{\textbf{Overview of \methodFamilyName{}.} \methodFamilyName{} disentangles representations by maximizing the examples' similarities from the same concept, and minimizing the examples' similarities from the different concepts. Meanwhile, \methodFamilyName{} utilizes constraints of $l_2$ distance and KL distance on representations and probabilities respectively before and after the disentanglement to maintain the general capabilities of LLMs. }\label{fig:method}
    \vspace{-10pt}
\end{figure*}