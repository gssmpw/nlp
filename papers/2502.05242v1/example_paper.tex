%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{multirow}
\usepackage{textcomp}
\usepackage{array}
\usepackage{xcolor}
\definecolor{-}{rgb}{0.25,0.41,0.88}
\definecolor{+}{rgb}{0.70,0.13,0.13}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage[linesnumbered, boxed]{algorithm2e}
\usepackage{setspace}
\usepackage{diagbox}
\usepackage{colortbl}
\newcommand\ie{\textit{i.e.}}
\newcommand\eg{\textit{e.g.}}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}
% \usepackage{icml2025}
% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{dsfont}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
% \newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}{Remark}
\newcommand{\commento}[1]{\textcolor{red}{[\textit{#1}]}} \newcommand{\ting}[1]{\textcolor{red}{[TC: \textit{#1}]}} \newcommand{\mohammad}[1]{\textcolor{blue}{(MN: #1)}}
\newsavebox{\CBox} % 定义一个盒子寄存器
\def\textBF#1{\sbox\CBox{#1}% 将内容保存到盒子中
  \resizebox{\wd\CBox}{\ht\CBox}{\textbf{#1}}% 调整大小并加粗
}

% \newcommand{\todo}[1]{\textcolor{red}{[\textit{TODO: #1}]}}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\input{notation.tex}
\input{math_commands.tex}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Preprint}

\begin{document}

\twocolumn[
\icmltitle{\methodFamilyName{}: Self-Explainability Enhancement of Large Language Models' Representations}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{$\star$}

\begin{icmlauthorlist}
\mbox{\bf Guanxu Chen}\textsuperscript{1,3{$\star$}}\,
\mbox{\bf Dongrui Liu}\textsuperscript{1{$\star$}}\,
\mbox{\bf Tao Luo}\textsuperscript{2,1}\,
\mbox{\bf Jing Shao}\textsuperscript{1}
\textbf{}
% \icmlauthor{Guanxu Chen}{equal,ailab,sjtu}
% \icmlauthor{Dongrui Liu}{equal,ailab}
% \icmlauthor{Tao Luo}{sjtu}
% \icmlauthor{Jing Shao}{ailab}
\end{icmlauthorlist}
\begin{center}
    $^1$ Shanghai Artificial Intelligence Laboratory\\
    $^2$ School of Mathematical Sciences, Institute of Natural Sciences, \\MOE-LSC, CMA-Shanghai, Shanghai Jiao Tong University \\
    $^3$ School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University
\end{center}

% \icmlaffiliation{ailab}{Shanghai Artificial Intelligence Laboratory}
% \icmlaffiliation{sjtu}{Shanghai Jiao Tong University}


\icmlcorrespondingauthor{Jing Shao}{shaojing@pjlab.org.cn}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Explaining the hidden representations of Large Language Models (LLMs) is a perspective to understand LLMs' underlying inference logic and improve their reliability in application scenarios. However, previous methods introduce external ``black-box'' modules to explain ``black-box'' LLMs, increasing the potential uncertainty and failing to provide faithful explanations. In this paper, we propose a self-explaining method \methodFamilyName{}, enhancing LLMs' explainability by aggregating the same concept and disentangling the different concepts in the representation space. In this way, \methodFamilyName{} provides faithful explanations carried by representations synchronously with the LLMs' output. Additionally, we showcase the applications of \methodFamilyName{} on trustworthiness-related tasks (\eg, the safety risks classification and detoxification tasks), where self-explained LLMs achieve consistent improvement in explainability and performance. More crucially, we theoretically analyze the improvement of \methodFamilyName{} on LLMs' generalization ability through optimal transport theory. The code is available at \href{https://github.com/AI45Lab/SEER}{https://github.com/AI45Lab/SEER}.

\textcolor{red}{Warning: This paper contains potentially unsafe context.}
\end{abstract}
\vspace{-20pt}
 % Surprisingly, these self-explained LLMs achieve better performance on trustworthiness-related tasks (\eg, the safety risks classification and detoxification tasks). More crucially, we theoretically analyze the improvement of \methodFamilyName{} on LLMs' generalization ability through optimal transport theory. In this way, \methodFamilyName{} can be the complementary to existing alignment methods and improve the trustworthiness of LLMs. Consequently, \methodFamilyName{} can bridge the LLMs' performance and explainability, improving the trustworthiness of LLMs.
 
\section{Introduction}
The wide use of LLMs provides convenience for people's work and life \cite{achiam2023gpt,cai2024internlm2}. However, the internal mechanisms of LLMs remain unclear, making them hard to be reliably applied in tasks like finance and healthcare \cite{li2023large,nazi2024large, dang2024explainable}. Thus, it is increasingly critical to explain and understand the inference logic of LLMs. LLMs transform the input into intermediate representations layer by layer, contributing to the final prediction. The hidden representations contain rich semantic information \cite{shen2021interpreting, zhang2024reef,2024arXiv241104986W}, reflecting the logic of model prediction. Therefore, explaining hidden representations can help us understand the inference logic, improving the trustworthiness and reliability of LLMs' applications.
\begin{figure}[t]
    \centering
    \includegraphics[width=1 \linewidth, clip]{intro_v21.pdf}
    \vspace{-15pt}
    \caption{ \methodFamilyName{} is designed to provide faithful self-explanations without post-process or additional modules, enhancing the self-explainability of LLMs' representations.}
    \label{fig:intro}
    \vspace{-15pt}
\end{figure}

To this end, prior works on explaining LLMs' hidden representations can be roughly divided into three classes, as shown in Figure \ref{fig:intro}. Firstly, \citet{liu2024efficient} and \citet{li2024inference} develop external \textit{detectors} (\eg, Probes) to identify dishonesty and toxic concepts in LLMs' representations. Secondly, \textit{sparse autoencoders} (SAEs) decompose LLMs' representations into more interpretable pieces \cite{templeton2024scaling,gao2024scaling,lieberum2024gemma}. Finally, \citet{chen2024selfie} and \citet{ghandeharioun2024patchscope} use the LLM itself as a \textit{decoder} to transform the LLMs' representations into natural language descriptions.

However, these methods fail to faithfully explain the inference logic of LLMs \cite{madsen2024self,turpin2024language}, because they introduce additional ``black-box'' modules to explain ``black-box'' LLMs, increasing the potential uncertainty \cite{madsen2024interpretability}. Therefore, we aim to enhance the explainability of LLMs and to make faithful self-explanations without external modules.

In this paper, we propose a self-explaining method, \textit{\textbf{S}elf-\textbf{E}xplainability \textbf{E}nhancement of LLMs' \textbf{R}epresentations}, named \methodFamilyName{}. In a trustworthiness-related scenario, an ideal situation is that representations of similar concepts (\eg, related to ``violence'') fall into the same region, while representations from different concepts (\eg, ``honesty,'' ``bias,'' and ``violence'') are kept away from each other. In this way, we can easily know whether the inference logic of LLMs involves dangerous concepts and may inspire potential intervention. Therefore, we can improve LLMs' self-explainability through disentangling between representations of different concepts. Specifically, \methodFamilyName{} constructs contrastive pairs, maximizes the representations' similarities from the same concept, and minimizes the representations' similarities between different concepts. Experiential results across three scenarios with four LLMs verify the effect of disentanglement, where \methodFamilyName{} achieves better intra-class compression (\eg, measured by Coding Rate \cite{chan2022redunet}) and inter-class separation (\eg, measured by $l_2$ distance), with almost unchanged general capabilities.
 % What's more, in a mathematical capability task, problems from different concepts like algebra and number theory can be separated, while those within the same mathematical domain are brought closer., such as Llama3.1 \cite{meta2024introducing} and Qwen2.5 \cite{yang2024qwen2}

We showcase the application of \methodFamilyName{} in the safety risks classification task and the detoxification task, where we apply \methodFamilyName{} to disentangle representations between safe and harmful question-answer (QA) pairs. Additionally, self-explained LLMs demonstrate better safety performance, realizing coherent advancements in explainability and performance. More crucially, we theoretically explain why \methodFamilyName{} can improve LLMs' generalization ability following the optimal transport theory \cite{villani2009optimal,jiang2020neurips,chuang2021measuring,solomon2022k}.

\methodFamilyName{} provides a new perspective to improve the faithfulness of explanations, increasing the transparency of the model inference logic. Trough \methodFamilyName{}, better self-explainability of LLMs brings improvement on their performance in trustworthiness-related tasks. Consequently, \methodFamilyName{} may contribute to mitigating the potential risks of advanced artificial intelligence.

% Consequently, \methodFamilyName{} increases the transparency of the model inference logic and helps us to learn more about the internal mechanisms of LLMs, promoting the trustworthiness of artificial intelligence.


\section{Related Work}

\textbf{Interpretability of LLMs.} Global interpretability methods provide insight into the internal mechanisms of LLM \cite{liu2023towards, zhou2024explaining, ren2024identifying, ren2024towards, dang2024explainable}. Previously, external modules are often trained to identify semantic information from intermediate representations \cite{liu2025latent,liu2024efficient}. Some methods also project representations into the vocabulary space \cite{Nostalgebraist2020} or other interpretable space \cite{gao2024scaling,lieberum2024gemma}. Due to the powerful capabilities of LLMs, they are utilized to explain representations with natural language by directly decoding representations \cite{chen2024selfie,ghandeharioun2024patchscope} or summarizing patterns of representations \cite{bills2023language}. When an LLM serves as the explained model and the explaining tool simultaneously in these above approaches, they can be called self-explaining methods. Self-explaining methods use models to explain themselves. Instead of explaining representations, chain-of-thought prompting (CoT, \citet{nye2021show,wei2022chain}) enables LLMs to tell how they make predictions and improves reasoning ability. However, \citet{huang2023can} and \citet{turpin2024language} show that CoT may provide unfaithful explanations and bring potential dangers to the utilization of LLMs.

\textbf{Representations of LLMs.} Several works focus on representations of LLMs instead of their output on tasks of LLMs' alignment \cite{li2024open,yin2024lofit,qian2024towards,zhang2024real,zhang2024better}, evaluation \cite{wei2024diff,azaria2023internal,xu2024good,azaria2023internal,orgad2024llms} and copyright protection \cite{zhang2024reef,sevastjanova2022lmfingerprints,yang2024fingerprint}. \citet{li2024inference} design steering vectors and insert them into model representations to control model generations without training. \citet{rosati2024representation}, \citet{zou2024improving} and \citet{li2024wmdp} perform machine unlearning by rotating the representation of harmful samples or pushing them towards a random distribution. What's more, \citet{wu2024reft} performs intervention functions precisely on the model's target layer and the target position of the input tokens. \citet{qian2024dean} disentangles LLMs’ awareness of fairness and privacy by deactivating the entangled neurons in representations.

\textbf{Contrastive Learning.} Contrastive self-supervised learning on computer vision utilizes positive and negative pairs constructed by data augmentation to learn general and high-quality representations \cite{he2020momentum,chen2020simple,zbontar2021barlow}. \citet{radford2021learning} connects natural language and visual modality through contrastive learning of text-image pairs. Recent work extracts human value representations of LLMs by applying multi-view contrastive Learning \cite{cahyawijaya2024high}. 


\begin{figure*}[t]
    \vspace{-5pt}
    \centering
    \includegraphics[width=1\linewidth, clip]{methods_v12.pdf}
    \vspace{-15pt}
    \caption{\textbf{Overview of \methodFamilyName{}.} \methodFamilyName{} disentangles representations by maximizing the examples' similarities from the same concept, and minimizing the examples' similarities from the different concepts. Meanwhile, \methodFamilyName{} utilizes constraints of $l_2$ distance and KL distance on representations and probabilities respectively before and after the disentanglement to maintain the general capabilities of LLMs. }\label{fig:method}
    \vspace{-10pt}
\end{figure*}

\section{\methodFamilyName{}}
In this section, we propose \methodFamilyName{} and introduce how \methodFamilyName{} improve the self-explainability of LLMs in Section \ref{sec:framework}. Then we theoretically analyze the effect of \methodFamilyName{} to LLMs' generalization capability in Section \ref{sec:thm} and conduct experimental verification in Section \ref{sec:application}. In Section \ref{sec:verify}, we verify the effectiveness of \methodFamilyName{} across three scenarios, such as math, knowledge, and safety.


\subsection{Framework Description}\label{sec:framework}
We aim to faithfully explain LLMs' inference logic and provide self-explanations. Suppose that representations of different concepts are disentangled from each other, then we can easily know which concept the LLM is considering (\eg, ``honesty'' and ``violence'' as shown in Figure \ref{fig:intro}) and take appropriate intervention during the inference process. Therefore, we can the inference logic of LLMs through the disentanglement between different concepts.

\textbf{Notations.} Given an LLM $f_{\theta}$ with $L$ layers, we use $f_{\theta_{\leq l}}(\cdot)$ to denote intermediate outputs in the $l$-th layer. With an input $\vx = (x_1, x_2, \ldots,x_T)$, LLMs can be described as
\begin{align}
\vh^{(l)} = (\vh^{(l)}_1, \ldots, \vh^{(l)}_t, \ldots, \vh^{(l)}_T)= f_{\theta_{\leq l}}(\vx), \\
\boldsymbol{\pi}_{\theta}(\vx) = (f_{\theta}(x_{2}\mid \vx_{\le 1}), \ldots, f_{\theta}(x_{T}\mid \vx_{\le T-1})), 
\end{align}
where $h^{(l)}_t$ denotes intermediate representations of token position $t$ in $l$-th layer and $\vh^{(l)} \in \sR^{T \times d}$ is the matrix of the representations for all tokens in $l$-th layer; $f_{\theta}(x_{t}\mid \vx_{\le t-1})$ denotes the probability of token $x_{t}$ given the previous tokens $\vx_{\le t-1}$ and $\boldsymbol{\pi}_{\theta}(\vx)$ is the output sequence of probabilities.


\textbf{Disentanglement of representations between concepts.} In this part, we aim to maximize the similarities of examples from the same concept (\eg, two QA examples from ``bias'' related data, called positive pair) and minimize the similarities of examples from the different concepts (\eg, one QA example from ``bias'' and the other from ``honesty'' related data, called negative pair) in representation space of LLMs. We utilize a Disentangle Set $\{\mathcal{D}_j\}_{j=1}^{C}$ consisting of subsets $\mathcal{D}_j =\{ \vx^{i}_j\}_{i=1}^{n_j}$ from $C$ different concepts, where $n_j$ is the number of samples from concept $j$, and total number of data $n$ can be calculated by $n=\sum_{j=1}^{C} n_j$.

Concretely, \methodFamilyName{} samples $B$ concepts $\{c_k\}_{k=1}^B$ and then construct positive pairs $\{\vx^{i_1}_{c_k}, \vx^{i_2}_{c_k}\}_{k=1}^{B}$ by sampling two examples from each concept. We use the disentangle loss $L_d$, a classical InfoNCE loss\footnote{Please see Appendix \ref{loss_discussion} for more discussions.}, to disentangle the representations from different concepts
\begin{equation}\label{eq:domr}
    \mathcal{L}_{\text{d}} = -\mathbb{E}_{\{\vx^{i_1}_{c_k}, \vx^{i_2}_{c_k}\}_{k=1}^{B}} \left[\log \frac{\text{exp} (\vz^{i_1}_{c_k} \cdot \vz^{i_2}_{c_k} / \sigma)}{\sum_{k^{\prime}=1}^{B} \text{exp} (\vz^{i_1}_{c_k} \cdot \vz^{i_2}_{c_{k^{\prime}}} / \sigma)}\right],
\end{equation}
where $\vz^{i}_{c}$ denotes the normalized representations of input $\vx^{i}_{c}$ from $l$-th layer and token position $t$, calculated by $\vh^{(l)}_t ~ / ~\lVert \vh^{(l)}_t\rVert$ and $\sigma$ adjusts the degree of disentanglement.
 
\textbf{Maintenance of LLMs' general performance.} We aim to obtain a faithfully self-explained LLM with outstanding general capabilities, rather than an encoder of concepts without normal ability of conversations. Therefore, the LLM should maintain general capabilities and provide normal output on the disentangled concepts. To obtain the representations associated with the general performance of LLMs, we introduce the Retain Set $\mathcal{D}_{\text {retain}}$, which includes data related to general capabilities. Meanwhile, we utilize the first example of each positive pair constructed in the previous paragraph to get output probabilities on disentangled concepts. 

The goal of our retain loss $\mathcal{L}_{\text {r}}$ is to maintain general capabilities and keep stable output on edited concepts. Specifically, we denote the original model as $f_{\theta_{\text{ref}}}$ and calculate the first term of $\mathcal{L}_{\text {r}}$ by imposing an $\ell_2$ norm constraint on representations before and after disentanglement following \cite{zou2024improving}. Additionally, the second term of $\mathcal{L}_{\text {r}}$ is calculated with the KL penalty on output probabilities before and after disentanglement, as suggested in \cite{ouyang2022training}.
\begin{equation}
\begin{aligned}
\mathcal{L}_{\text{r}}= &\mathbb{E}_{\{\vx^{k}\}_{k=1}^{B}} \left\|f_{\theta_{\leq l}}(\vx^{k})-f_{\theta_{\text{ref}\leq l}}(\vx^{k})\right\|_{2}  \\&- \alpha \mathbb{E}_{\{\vx^{i_1}_{c_k}\}_{k=1}^{B}} \mathbb{D}_{\text{KL}}[\boldsymbol{\pi}_{\theta}(\vx^{i_1}_{c_k})\|\boldsymbol{\pi}_{\theta_{\text{ref}}}(\vx^{i_1}_{c_k})]~,
\end{aligned}
\end{equation}
where $l$ is the target layer of disentanglement and $\{\vx^{k}\}_{k=1}^{B}$ denote the data sampled from $\mathcal{D}_{\text {retain}}$; $\alpha$ is to adjust the contribution of two terms in $\mathcal{L}_{\text{r}}$ and the data $\{\vx^{i_1}_{c_k}\}_{k=1}^{B}$ are from the first example of each positive pair constructed in previous paragraph.

\begin{algorithm}[t]
\caption{\label{alg:main2} \methodFamilyName{}.}
\begin{algorithmic}[1]
    \renewcommand{\algorithmicrequire}{\textbf{Input:}}
    \renewcommand{\algorithmicensure}{\textbf{Output:}}

    \setstretch{1.2}
    \REQUIRE batch size $B$, the original model $f_{\theta_{\text{ref}}}$, the disentangled model $f_{\theta}$, target layer $l$, target position $t$ of input tokens, hyperparameterd $\sigma$, $\lambda_1$ and $\lambda_2$, Disentangle Set $\{\mathcal{D}_j\}_{j=1}^{C}$, Retain Dataset $\mathcal{D}_{\text {retain}}$.

    \STATE Sample $\{c_k\}_{k=1}^B \sim \{1, \ldots, C\}$
    \STATE Sample $\{\vx^{i_1}_{c_k}, \vx^{i_2}_{c_k}\} \sim \mathcal{D}_{c_k}$ as $\{\vx^{i_1}_{c_k}, \vx^{i_2}_{c_k}\}_{k=1}^{B}$
    \STATE Sample $\{\vx^{k}\}_{k=1}^{B} \sim \mathcal{D}_{\text {retain}}$
    \STATE \textbf{for all}  $\vx^{i}_j \in \{\vx^{i_1}_{c_k}, \vx^{i_2}_{c_k}\}_{k=1}^{B}$ \textbf{do}
        \STATE $~~~~$ Obtain $h^{(l)}_t$
        \STATE $~~~~$ Obtain $\boldsymbol{\pi}_{\theta_{\text{ref}}}(\vx^{i_1}_{c_k})$ and $\boldsymbol{\pi}_{\theta}(\vx^{i_1}_{c_k})$ respectively
        \STATE $~~~~$ Calculate normalized representations $z^{i}_j = \frac{h^{(l)}_t}{\lVert h^{(l)}_t\rVert}$ 
    \STATE \textbf{end for}
    \STATE Calculate the disentangled loss $\mathcal{L}_{\text {d}}$
    \STATE \textbf{for all}  $\vx^{k} \in \{\vx^{k}\}_{k=1}^{B}$ \textbf{do}
        \STATE $~~~~$ Obtain $f_{\theta_{\text{ref}\leq l}}(\vx^{k})$
        \STATE $~~~~$ Obtain $f_{\theta_{\leq l}}(\vx^{k})$
    \STATE \textbf{end for}

    \STATE Calculate the retain loss $\mathcal{L}_{\text{r}}$ %= 
    
    \STATE Calculate $\mathcal{L} = \mathcal{L}_{\text{d}} + \lambda \mathcal{L}_{\text{r}}$
    
    \STATE update parameters $f_{\theta}$ to minimize $\mathcal{L}$
    % \ENDFOR
    \STATE \textbf{return} the parameter of disentangled model $f_{\theta}$
    
\end{algorithmic}
\end{algorithm}

In summary, our final loss function is as follows:
\begin{equation}
\label{eq:loss}
\mathcal{L} = \mathcal{L}_{\text{d}} + \lambda \mathcal{L}_{\text{r}},
\end{equation}
where $\lambda$ is a coefficient that balances the contributions of two loss terms. Algorithm \ref{alg:main2} summarizes the workflow of \methodFamilyName{}.

\subsection{Theoretical Analysis of \methodFamilyName{}}\label{sec:thm}
In this subsection, we theoretically prove that disentanglement of LLMs' representations improves the generalization ability of LLMs, following prior works \cite{chuang2021measuring,solomon2022k} through optimal transport theory.

\textbf{Definition of distance from optimal transport.} In optimal transport theory, The distance between two distributions can be measured by the minimal cost to transform one distribution to the other, called the Wasserstein distance.
\begin{definition}[$s$-Wasserstein distance \cite{villani2009wasserstein}]
Given two probability measures $p$ and $q \in \Prob(\sR^m)$, their $s$-Wasserstein distance with cost function $c(\cdot)$ is calculated as
\begin{equation}
    \mathbb{D}_s(p, q) = \inf_{\gamma \in \Gamma(p, q)} [\E_{(U,V) \sim \gamma} c(U,V)^s]^{\frac{1}{s}},
\end{equation}
where the set $\Gamma(p, q) \in \Prob(\sR^m \times \sR^m)$ consisting of all the couplings whose marginals are $p$ and $q$, respectively.
\end{definition}

To measure the property of a distribution, we introduce $k$-variance, a generalization of variance built on the machinery of random bipartite matching \cite{solomon2022k,chuang2021measuring}. In this paper. we consider the unnormalized version of $k$-variance with $1$-Wasserstein distance following \cite{solomon2022k,chuang2021measuring}.
\begin{definition}[$k$-variance]
Letting $p \in \Prob(\sR^m)$ be a probability measure and $k \in \sN$ denote the number of data sampled following $p$, the \emph{$k$-variance} is defined as
\begin{equation}
    \Var_{k}(p) = \E_{\substack{x_1,\ldots,x_k \sim p^k \\ x^{\prime}_1,\ldots,x^{\prime}_k \sim p^k}} \left[ \mathbb{D}_1(\frac{1}{k}\sum_{i=1}^k \delta_{x_i}, \frac{1}{k}\sum_{i=1}^k \delta_{x^{\prime}_i} ) \right],
\end{equation}
where $\sum_{i=1}^k \delta_{x_i}$ denotes the empirical measures of $p$ for $x_i \overset{\text{i.i.d}}{\sim} p$ and euclidean cost function is applied here.
\end{definition}


\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth,, clip]{seer_verify_v5.pdf}
    \vspace{-20pt}
    \caption{\textbf{t-SNE Visualization of LLMs' representations in three scenarios and four LLMs}.}
    \label{fig:map-of-math}
    \vspace{-15pt}
\end{figure*}
\textbf{Formulation of LLMs' generalization ability.} To analyze LLMs' generalization ability, we simplify LLMs from a next-token predictor to a classifier between concepts following \cite{abburi2023generative,chen2023token,lang2024theoretical}. For example, the safety-related tasks can be transformed into a prompt classification task between safe concepts and harmful concepts \cite{inan2023llama,li2024inference}. 

Specifically, given an input $\vx \in \gX$ and the concept space  $\gC = \{1,\dots,C\}$, we formulate the LLM $f_{\theta}$ as a compositional hypothesis class $\gG \circ \Phi$. We consider the output of LLMs as a prediction of concept $j \in \gC$, where the LLM $f_{\theta}$ can be decomposed as a hidden representation encoder $\phi := f_{\theta_{\le l }} \in \Phi $ and a score-based classifier $g := \psi \circ f_{\theta_{\textgreater l}} \in \gG $. $\psi$ is a hypothesis component to transform LLMs' output into the concept-level prediction.

In this way, we can measure the generalization ability of LLMs following \cite{chuang2021measuring}. Given the classifier $g = (g_1, \dots, g_C)$, $g_j \in \gG_j$, the prediction for input $\vx \in \gX$ is calculated by $\argmax_{j \in \gC} g_j(\phi(\vx))$. The margin of $g$ for a data $\vx_j$ from concept $j$ is defined by
\begin{equation}\label{eq:margin}
    \rho_g(\phi(\vx_j)) := g_j(\phi(\vx_j)) - \max_{j^\prime \neq j} g_{j^\prime}(\phi(\vx_j)),
\end{equation}
where $g$ misclassifies if $\rho_g(\phi(\vx_j)) \leq 0$. In our task, the Disentangle Set $\{\mathcal{D}_j\}_{j=1}^{C}$ can be considered as obtained i.i.d from distribution $p$ over $\gX \times \gC$. We use $p_j$ to denote the marginal over a class $j \in \gC$. The pushforward measure of $p$ with respect to $\phi$ is represented as $\phi_\# p$. We consider expected zero-one loss of a hypothesis $g \circ \phi$ with the distribution $\mu(j)$ over the concept space:
\begin{equation} R_p(g \circ \phi) = \E_{\substack{j \sim \mu \\ \vx_j \sim p_j}}[\mathds{1}_{\rho_g(\phi(\vx_j)) \leq 0}],
\end{equation}
and we use the empirical $\tau$-margin loss:
\begin{equation}\hat{R}_{\tau,n}(g \circ \phi) = {\E}_{\substack{j \sim \mu \\ \vx_j \sim \mathcal{D}_j}}[\mathds{1}_{\rho_g(\phi(\vx_j)) \leq \tau}].\end{equation}

\setcounter{theorem}{0}
\begin{theorem}
\label{thm_margin}
(Proven in \cite{chuang2021measuring})\footnote{Please See Appendix \ref{thm_details} for more details of our theoretical analysis.} Given a classifier $g \in \gG$, where $g = [g_1, \cdots, g_C]$ and $ \gG = \gG_1\times  \cdots\times \gG_C$; $\gG_j: \gX \rightarrow \sR$. With $\tau > 0$, the generalization bound can be measured for all $g \in \gG$ with probability at least $1 - \delta > 0$:
\begin{align}
&R_{p}(g \circ \phi) \leq \hat{R}_{\tau,n}(g \circ \phi) + \nonumber \\& \E_{j \sim \mu} \left[\frac{\Lip(g,j)}{\tau} \Var_{n_j}(\phi_\#p_j) \right] + \sqrt{\frac{\log(1 / \delta)}{2n}}, \label{eq:margin}
\end{align}
where $\Lip(g,j) = \sup_{x_j,x^{\prime}_j \in \gX} \frac{|\rho_g(\phi(x_j))-\rho_g(\phi(x^{\prime}_j))| }{\| \phi(x)-\phi(x^{\prime})\|_2}$ is the margin Lipschitz constant w.r.t $\phi$.
\end{theorem}

Theorem \ref{thm_margin} indicates that with fixed $\tau$, the generalization bound is minimized when (1) the $\Var_{n_j}(\phi_\#p_j)$ of each class $j$ is small and (2) the $\hat{R}_{\tau,n}(g \circ \phi)$ is low. When we perform \methodFamilyName{} to improve the self-explainability of LLMs, the representations of the same concept are aggregated together, reducing the $k$-variance $\Var_{n_j}(\phi_\#p_j)$ of each concept and contributing to the generalization bound. Meanwhile, the representations of different concepts will also be separated better through \methodFamilyName{}, which means we can obtain a better classifier $g^{\prime}$ with a higher $\rho_g^{\prime}(\phi(\vx_j))$ on a wide range of samples and decrease $\hat{R}_{\tau,n}(g^{\prime} \circ \phi)$ in Theorem \ref{thm_margin}. In this way, \methodFamilyName{} brings a lower generalization bound to LLMs, improving their generalization capabilities. We present the verification of our theoretical analysis in Section \ref{sec:application}, with specific experimental results shown in Tables \ref{tab:safety_cls_bin} and \ref{tab:safety_alignment}.


\begin{table*}[t]
\vspace{-5pt}
\caption{\textbf{Evaluation of the disentanglement quality with metrics.} The \textbf{bold} values represent better performance in the comparison before and after the application of \methodFamilyName{}.}\label{tab:comparison}
\vspace{2pt}
\setlength{\tabcolsep}{3pt}
\centering
\begin{tabular}{{c}l *{10}{c}}
\toprule
Model & Task & \multicolumn{2}{c}{Coding Rate$\downarrow$} & \multicolumn{2}{c}{eRank$\downarrow$} & \multicolumn{2}{c}{$\ell_2$ distance$\uparrow$} & \multicolumn{2}{c}{Angle$\uparrow$} & \multicolumn{2}{c}{Hausdorff$\uparrow$} \\ 
\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10} \cmidrule(lr){11-12}
 & & Origin & \methodFamilyName{} & Origin & \methodFamilyName{} & Origin & \methodFamilyName{} & Origin & \methodFamilyName{}   & Origin & \methodFamilyName{} \\
\midrule
 & Math & 594.38 & \textbf{591.18} & 155.97 & \textbf{41.04} & 13.78 & \textbf{39.25} & 38.74 & \textbf{84.26} & 5.75 & \textbf{13.53} \\
\cmidrule(lr){3-12}
Llama-3.1-8B & Knowledge & 354.13 & \textbf{327.19} & 137.87 & \textbf{89.94} & 30.52 & \textbf{46.68} & 71.94 & \textbf{83.68} & 5.71 & \textbf{27.44} \\
 \cmidrule(lr){3-12}
 & Safety & 442.18 & \textbf{415.40} & 102.49 & \textbf{28.44} & 12.13 & \textbf{42.50} & 29.54 & \textbf{74.34} & 2.68 & \textbf{5.74} \\
\midrule
 & Math & 771.21 & \textbf{631.90} & 276.22 & \textbf{66.61} & 121.65 & \textbf{249.29} & \textbf{65.17} & 25.84 & 59.87 & \textbf{103.33} \\
\cmidrule(lr){3-12}
Qwen2.5-7B & Knowledge & 359.45 & \textbf{321.88} & 169.28 & \textbf{89.88} & 152.79 & \textbf{241.31} & 73.74 & \textbf{82.53} & 43.59 & \textbf{136.02} \\
 \cmidrule(lr){3-12}
 & Safety & 455.73 & \textbf{419.61} & 160.01 & \textbf{33.53} & 161.26 & \textbf{278.56} & 68.90 & \textbf{76.11} & 25.71 & \textbf{43.68} \\
\midrule
 & Math & 512.28 & \textbf{402.27} & 105.39 & \textbf{15.41} & 7.75 & \textbf{53.98} & 38.74 & \textbf{83.44} & 2.68 & \textbf{12.91} \\
\cmidrule(lr){3-12}
Mistral-7B-v0.3 & Knowledge & 349.65 & \textbf{325.60} & 129.31 & \textbf{85.00} & 20.03 & \textbf{30.64} & 77.29 & \textbf{85.99} & 2.74 & \textbf{17.29} \\
 \cmidrule(lr){3-12}
 & Safety & 409.24 & \textbf{399.66} & 118.20 & \textbf{20.42} & 6.00 & \textbf{39.95} & 24.49 & \textbf{77.32} & 1.49 & \textbf{4.97} \\
\midrule
 & Math & 448.06 & \textbf{425.99} & \textbf{16.52} & 19.37 & 192.40 & \textbf{823.02} & 36.87 & \textbf{83.62} & 32.35 & \textbf{224.99} \\
\cmidrule(lr){3-12}
Gemma2-9B & Knowledge & 339.28 & \textbf{306.37} & 124.62 & \textbf{66.30} & 473.11 & \textbf{718.02} & 61.97 & \textbf{80.79} & 54.55 & \textbf{436.99} \\
 \cmidrule(lr){3-12}
 & Safety & 442.89 & \textbf{393.65} & 94.25 & \textbf{25.63} & 68.49 & \textbf{728.21} & 11.48 & \textbf{76.70} & 16.65 & \textbf{94.41} \\
\bottomrule
\end{tabular}
\vspace{-10pt}
\end{table*}

\begin{table}[!t]
\vspace{-5pt}
\caption{\label{tab:Performance}\textbf{Evaluation of LLMs' general capabilities.} We show the general performance of LLMs with disentangled representations, along with the performance gap before and after disentanglement. }
\vspace{5pt}
\setlength{\tabcolsep}{3pt}
\centering
\begin{tabular}{l *{7}{c}}
\toprule
Task & \multicolumn{2}{c}{GSM8K$\uparrow$} & \multicolumn{2}{c}{MMLU$\uparrow$} & \multicolumn{2}{c}{AGIEVAL$\uparrow$} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
 & Origin & \methodFamilyName{} & Origin & \methodFamilyName{} & Origin & \methodFamilyName{} \\
\rowcolor{gray!30}
\multicolumn{7}{c}{ Llama-3.1-8B } \\
 Math & 84.5 & 82.2 & 69.4 & 69.2 & 47.3 & 46.4 \\
% \cmidrule(lr){3-12}
 Knowledge & 84.5 & 82.0 & 69.4 & 68.9 & 47.3 & 46.5 \\
 Safety & 84.5 & 82.6 & 69.4 & 68.7 & 47.3 & 46.2 \\
\rowcolor{gray!30}
\multicolumn{7}{c}{ Qwen2.5-7B } \\
 Math & 80.4 & 80.4 & 74.2 & 74.3 & 57.3 & 58.5 \\
% \cmidrule(lr){3-12}
Knowledge & 80.4 & 82.4 & 74.2 & 73.8 & 57.3 & 60.2 \\
 % \cmidrule(lr){3-12}
 Safety & 80.4 & 81.0 & 74.2 & 74.2 & 57.3 & 59.5 \\
\rowcolor{gray!30}
\multicolumn{7}{c}{ Mistral-7B-v0.3 } \\
 Math & 55.7 & 54.7 & 61.9 & 61.7 & 37.1 & 37.2\\
% \cmidrule(lr){3-12}
Knowledge & 55.7 & 54.5 & 61.9 & 62.1 & 37.1 & 36.9 \\
 % \cmidrule(lr){3-12}
 Safety & 55.7 & 56.4 & 61.9 & 60.4 & 37.1 & 36.7 \\
\rowcolor{gray!30}
\multicolumn{7}{c}{ Gemma2-9B } \\
 Math & 80.0 & 81.8 & 73.3 & 73.3 & 47.2 & 47.5 \\
% \cmidrule(lr){3-12}
 Knowledge & 80.0 & 84.4 & 73.3 & 73.4 & 47.2 & 48.4 \\
 % \cmidrule(lr){3-12}
 Safety & 80.0 & 80.3 & 73.3 & 73.3 & 47.2 & 47.8 \\
\midrule
 \multicolumn{2}{l}{Average Gap} & \color{+}+0.07& & \color{-}-0.25& & 
 \color{+}+0.42 \\
\bottomrule
\end{tabular}
\vspace{-15pt}
\end{table}

\subsection{Verification of Disentanglement}\label{sec:verify}
In this subsection, we utilize metrics related to the quality of disentanglement to validate the disentanglement effectiveness of \methodFamilyName{} with four LLMs in three common scenarios, such as math, knowledge, and safety. In the mathematical scenario, we disentangle different mathematical branches (\eg, algebra and number theory) in the representation space of LLMs through \methodFamilyName{}, which can let us know the branch considered by LLMs. We also enhance the separation of representations from different knowledge domains (\eg, biology and psychology), making us easily know which knowledge the LLMs use during their inference. For the safety scenario, we disentangle several safety risks (\eg, violence and discrimination) to easily perceive whether the inference logic of LLMs involves harmful concepts.

\begin{table*}[t]
\vspace{-5pt}
\caption{\label{tab:safety_cls_bin}\textbf{Classification accuracy of different methods before and after the application of \methodFamilyName{}.}}
\centering
\setlength{\tabcolsep}{6pt}
\begin{tabular}{{l}c *{7}{c}}
    \toprule
    Model & \multicolumn{2}{c}{Self-Sim} & \multicolumn{2}{c}{Linear Probe} & \multicolumn{2}{c}{Latent Guard} & \multicolumn{2}{c}{SFT} \\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
    & Origin($\%$)  & \methodFamilyName{}($\%$)&Origin($\%$) & \methodFamilyName{}($\%$)&Origin($\%$) & \methodFamilyName{}($\%$)&Origin($\%$) & \methodFamilyName{}($\%$) \\
    % \rowcolor{gray!30}
    \rowcolor{gray!30}
    \multicolumn{9}{c}{ Binary Classification$\uparrow$ } \\
    Llama-3.1-8B & 68.1 & 83.4 & 82.2& 91.4 & 67.9 & 76.9 & 67.4&71.3 \\
    Qwen2.5-7B & 62.1 & 82.1& 91.2&82.1 & 65.6 & 78.7 & 75.1&78.8 \\
    Mistral-7B-v0.3 & 69.7 & 83.9 & 78.8 & 90.0 & 59.6 & 81.8 & 84.7&83.8\\
    Gemma2-9B & 74.4 & 83.5 & 85.4 &92.6 & 69.0 & 79.9 & 84.7&84.1 \\
%    Qwen2.5-14B & 62.1 & 82.1& 91.2&82.1 & 65.6 & 78.7 & 75.1&78.8 \\
    \rowcolor{gray!30}
    \multicolumn{9}{c}{ Multi-risk Classification$\uparrow$ } \\
    Llama-3.1-8B & 56.6 & 78.7 & 78.3& 90.8 & 68.9 & 72.3& 57.8&58.6 \\
    Qwen2.5-7B & 40.7 & 79.3& 93.1& 91.9 & 62.2 & 70.8 & 74.8&76.8 \\
    Mistral-7B-v0.3 & 60.3& 79.2 & 72.5& 90.8 & 65.4 & 72.5& 83.0&83.4 \\
    Gemma2-9B & 69.0 & 80.1& 80.3& 91.8& 68.9 & 71.9 & 81.8&82.0 \\
%    Qwen2.5-14B & 62.1 & 82.1& 91.2&82.1 & 65.6 & 78.7 & 75.1&78.8 \\
    
    \bottomrule
\end{tabular}
\vspace{-15pt}
\end{table*}

\textbf{Datasets and models.} We choose three datasets that can reflect three common scenarios for the LLMs: (1) \textbf{MATH} \cite{hendrycksmath2021} for the \textit{Mathematics} scenario with seven mathematical branches; (2) \textbf{MMLU} \cite{hendrycks2020measuring} for the  \textit{Knowledge} scenario with seven domains of knowledge; (3) \textbf{BevearTails} \cite{ji2024beavertails} for the \textit{Safety} scenario with five safety risks and one safety concept. LLMs are trained for each scenario, respectively. We select four open-source instruction-tuned LLMs, including Llama-3.1-8B-instruct \cite{meta2024introducing}, Qwen2.5-7B-instruct \cite{yang2024qwen2}, Mistral-7B-instruct-v0.3 \cite{jiang2023mistral} and Gemma2-9B-it \cite{team2024gemma}. For different architectures, we choose the layer located at $80\%$ of the hidden layer count as the target layer and perform SEER on the last token of input sequence with Low-Rank Adaptation (LoRA, Hu et al. 2021). More details is shown in Appendix \ref{app:details_verify}.

\textbf{Metrics to measure the quality of disentanglement.} We select five metrics to validate the quality of disentanglement. (1) \textbf{Coding Rate} measures the rate distortion of subspace-like distributions, which express the quality of disentangled representations' intra-class compression \cite{chan2022redunet}; (2) \textbf{eRank} represents represent how small of a subspace the inter-class representations can be compressed to, reflecting the effectiveness of compression \cite{roy2007effective,wei2024diff}; (3) the average \textbf{$l_2$ distance} measures the absolute distance between representations from different concepts; (4) the average \textbf{Angle} reflects the relative similarities between different concepts in representation space; (5) the average \textbf{Hausdorff distance} represents the distance between the whole sets of representation from different concepts \cite{huttenlocher1993comparing}. We calculate and compare these metrics on the representations from the original model (\ie, Origin in the following tables) and the disentangled model (\ie, \methodFamilyName{} in the following tables), respectively.

\textbf{Benchmarks to evaluate general capabilities of LLMs.} We consider three benchmarks of general capabilities to check the performance degradation of LLMs following \cite{dubey2024llama,cai2024internlm2,team2024gemma}. We choose (1) \textbf{GSM8K} to evaluate the mathematics capability of LLMs \cite{cobbe2021training}; (2) \textbf{MMLU} to evaluate LLMs' performance of multitask language understanding \cite{hendrycks2020measuring}; (3) \textbf{AGIEval} to evaluate the general abilities of LLMs in tasks related to human cognition and problem-solving \cite{zhong2023agieval}. MMLU dataset overlaps with the training scenario of \textit{Knowledge}, which also reflects the effectiveness of our maintenance of normal output for disentangled concepts.

\textbf{\methodFamilyName{} improves the quality of intra-class compression.} The quality of intra-class compression can be measured with Coding Rate and eRank, where better compression of each concept leads to lower Coding Rate and eRank. As shown in Table \ref{tab:comparison}, almost all of the LLMs achieve better eRank by $57.3\%$ through \methodFamilyName{}, with the subspace of lower dimensions that the disentangled representations can be compressed to. What's more, Coding Rate is decerased by $8.9\%$, which means \methodFamilyName{} compresses each concept into a subspace with tinier volume.

\textbf{\methodFamilyName{} improves the quality of inter-class separation.} We utilize the $l_2$ distance, angle, and Housdorff distance to evaluate the quality of inter-class separation, where larger values for these metrics express better inter-class separation through the larger absolute distance, relative similarities, and set-level distance of disentangled representations respectively. \methodFamilyName{} achieves an improvement of $273.5\%$ and $109.6\%$ on the average $l_2$ distance and angle between different concepts in the representation space as shown in Table \ref{tab:comparison}, reflecting the better quality of representaions' separation. Housdorff distance is significantly increased by $324.5\%$, validating that \methodFamilyName{} separates the representations between concepts. Figure \ref{fig:map-of-math} shows the t-SNE visualization results of two models before and after disentanglement, verifying the effectiveness of \methodFamilyName{}.

\textbf{\methodFamilyName{} successfully retains the general capability of LLMs with the improvement of self-explainability.} Table \ref{tab:Performance} illuminates that LLMs keep almost unchanged general performance during the editing of representations. The contribution of each terms in the retain loss $\mathcal{L}_{\text{r}}$ will be further discussed in Appendix \ref{app:ablation} with ablation studies.

\section{Case Studies on Safety-related Tasks}\label{sec:application}
In this section, we showcase the application of \methodFamilyName{} in safety-related scenarios \cite{ren2024derail, hu2024vlsbench}, such as the safety risks classification task in Section \ref{case1} and the detoxification task in Section \ref{case2}. \methodFamilyName{} achieves a consistent improvement of explainability and task performance on both of these tasks, verifying our theoretical analysis in Section \ref{sec:thm} and demonstrating the ability of \methodFamilyName{} to mitigate the potential safety risks of LLMs.

\subsection{Safety Risks Classification}\label{case1}
The safety risks classification task is practical and important in safety-related scenarios. In this subsection, we showcase the application of \methodFamilyName{} on this task by disentangling representations of different safety risks.

\textbf{Datasets.} (1) \textbf{Binary classification task} utilizes the two broad concepts of safety and harm. Based on BeaverTails \cite{ji2024beavertails}, we screen data related to only one type of safety risk, selecting five risks to form the binary classification train set. (2) \textbf{Multi-risks classification task} considers the classification across safety concepts and the previous five safety risks. Please see more details in Appendix \ref{app:details_case1}.



\textbf{Representation-based baseline methods.} (1) We use \textbf{Self-Sim} \cite{zeng2024similar,liu2025latent}, where we calculate the mean representations of each concept and predict the risk of QA pairs according to their similarity with concepts. (2)Following \cite{li2024inference,he2022masked}, we utilize \textbf{Linear Probes} (LP) to classify representations of different concepts. (3) \textbf{Latent Guard} \cite{liu2025latent} disentangles representations to detect toxic concepts with cross-attention modules. We compare the classification accuracy ($\uparrow$) between detectors trained by representations from the origin model and the disentangled model. 

\textbf{Output-based baseline methods.} Following \cite{li2024salad,inan2023llama}, we choose \textbf{Supervised Fine-Tuning (SFT)}, fine-tuning the LLM to evaluate the safety of QA pairs with the classification instruction. We perform \methodFamilyName{} before SFT and compare the classification accuracy.

\textbf{\methodFamilyName{} improves the classification performance of LLMs.} Table \ref{tab:safety_cls_bin} indicates that \methodFamilyName{} achieves an improvement of $13.4\%$ in binary classification accuracy and $17.0\%$ in multi-risk classification accuracy, verifying the Theorem \ref{thm_margin}. The application of \methodFamilyName{} before SFT brings benefits to the classification capabilities of LLMs by $1.7\%$, demonstrating the potential of \methodFamilyName{} to improve the performance of LLMs.




\subsection{Detoxification Tasks}\label{case2}
The detoxification of LLMs is an important task for improving their safety performance. In this subsection, we compare the safety performance of LLMs before and after applying \methodFamilyName{}, using the experimental settings of data introduced in Section \ref{case1}. What's more, we show the improvement of LLMs' safety performance through applying \methodFamilyName{} both before and after SFT \cite{huang2024vaccine,li2024getting}.

\begin{table}[!t]
\vspace{-7pt}
\caption{\label{tab:safety_alignment} \textbf{Overall evaluation of LLMs' safety performance.} We evaluate the improvement of \methodFamilyName{} on LLMs' safety performance based on the original LLMs and the supervised finetuned LLMs.}
\vspace{1pt}
\setlength{\tabcolsep}{4.5pt}
\centering
\begin{tabular}{{l}c *{4}{c}}
\toprule
Method & \multicolumn{2}{c}{Safety} & Over-Safety & Capability \\ 
\cmidrule(lr){2-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5}
& BT$\uparrow$ & SB$\uparrow$ & XST$\downarrow$ & Average$\uparrow$ \\
\rowcolor{gray!30}
\multicolumn{5}{c}{ Llama-3.1-8B } \\
Origin & 83.1&94.2&6.4 & 67.1 \\
\methodFamilyName{} & 95.5&96.6&18.0 & 65.8\\
$\methodFamilyName{}_{~\text{NT-Xent}}$ & \textbf{97.1}&\textbf{98.9}&21.2 & \textbf{66.7} \\
\midrule
SFT & 95.0&95.7&\textbf{16.4} & 60.3 \\
SFT + \methodFamilyName{} & 96.7&96.3&24.4 & 57.6 \\
\rowcolor{gray!30}
\multicolumn{5}{c}{ Qwen2.5-7B } \\
Origin & 92.1&94.6&16.0 & 70.6 \\
\methodFamilyName{} & 98.7&98.3&23.2 &\textbf{71.0}\\
$\methodFamilyName{}_{~\text{NT-Xent}}$ & \textbf{99.1}&\textbf{99.1}&22.4 & 70.0 \\
\midrule
SFT & 58.4&68.8&\textbf{12.0} & 68.3 \\
SFT + \methodFamilyName{} & 93.5&92.5&12.8 & 70.2 \\
\rowcolor{gray!30}
\multicolumn{5}{c}{Mistral-7B-v0.3 } \\
Origin & 84.3&76.5&14.4 & 51.6 \\
\methodFamilyName{} & 96.2&88.3&\textbf{9.2} & 49.6\\
$\methodFamilyName{}_{~\text{NT-Xent}}$ & \textbf{99.0}&\textbf{96.8}&10.8 & \textbf{50.3} \\
\midrule
SFT & 93.7&94.3&15.6 & 46.3 \\
SFT + \methodFamilyName{} & 98.6&94.8&24.8 & 45.2 \\
\rowcolor{gray!30}
\multicolumn{5}{c}{ Gemma2-9B } \\
Origin & 98.0&97.6&20.4 & 66.8 \\
\methodFamilyName{} & 99.1&98.1&\textbf{14.0} & \textbf{66.7}\\
$\methodFamilyName{}_{~\text{NT-Xent}}$ & \textbf{99.4}&\textbf{99.0}&18.4 & 66.6 \\
\midrule
SFT & 97.6&97.3&18.0 & 64.3 \\
SFT + \methodFamilyName{} & 98.4&97.2&15.6 & 64.6 \\
\bottomrule
\end{tabular}
\vspace{-17pt}
\end{table}

\textbf{Evaluation benchmarks.} We evaluate the safety performance with the test set of BeaverTails (\ie, BT in the table) and the base set of SaladBench (\ie, SB in the table, \citet{li2024salad}) through the safety rate ($\uparrow$). We utilize the XSTest (XST, \citet{rottger2023xstest}) with refusal rate ($\downarrow$) to measure the over-refusal of LLMs. we use the GSM8k, MMLU and AGIEval to evaluate their general capabilities and show the average scores ($\uparrow$). Please see more experimental details in Appendix \ref{app:details_case2}.

\textbf{\methodFamilyName{} achieves the consistent improvement between self-explainability and safety performance of LLMs.} Table \ref{tab:safety_alignment} demonstrates that \methodFamilyName{} achieve better safety performance of LLMs by $7.5\%$, ranking at the top in all comparisons and further validating our theoretical analysis in Theorem \ref{thm_margin}. Meanwhile, \methodFamilyName{} exhibits controllable over-safety performance and maintains nearly unchanged general capabilities of LLMs. As shown in the Table \ref{tab:safety_alignment}, \methodFamilyName{} can improve the effectiveness of SFT on LLMs' safety performance by $12.9\%$. Above improvements of \methodFamilyName{} are effective for LLMs with larger size, as shown in Appendix \ref{app:details_case2}.


\textbf{\methodFamilyName{} have the potential for improvement in terms of the number of negative samples.} We conduct experiments on the other contrastive loss functions similar to InfoNCE, NT-Xent Loss \cite{chen2020simple}, with more negative examples utilized in the contrastive batch. The average improvement of $2.4\%$ illuminates the potential of the \methodFamilyName{} in terms of scaling the number of negative examples. Please see Appendix \ref{loss_discussion} for more discussions.
 % especially in this detoxification task which can be transformed into a classification scenario

\section{Conclusion}

In this paper, we propose \methodFamilyName{} to provide faithful explanations of LLMs' inference logic. SEER is a self-explaining method through disentangling representations between different concepts in the representation space. More crucially, \methodFamilyName{} not only enhances the LLM's explainability but also improves its performance in trustworthiness-related tasks. Furthermore, we theoretically explain the improvement of \methodFamilyName{} on LLMs’ generalization ability in optimal transport theory. In this way, \methodFamilyName{} provides a new perspective on the explainability of LLMs and contributes to the reliable utilization of advanced artificial intelligence.

% \section*{Acknowledgements}
% This work is supported by Shanghai Artificial Intelligence Laboratory; the National Key R\&D Program of China Grant No. 2022YFA1008200 (T. L.); Shanghai Municipal Science and Technology Key Project No. 22JC1401500 (T. L.); Shanghai Municipal of Science and Technology Major Project No. 2021SHZDZX0102 (T. L.).


\section*{Impact Statement}
This work aims to advance the field of Large Language Models' Explainability by proposing a self-explaining method named \methodFamilyName{}, which faithfully explains the inference logic of large language models. \methodFamilyName{} is not just explaining the hidden representations of large language models, but further enhancing their self-explainability. We hope that \methodFamilyName{} facilitates progress in this area with such a novel perspective that has the potential to achieve consistent improvements between explainability and capabilities of large language models. The potential positive societal impacts include more reliable and trustworthy language models with enhanced explainability, which could bring benefits to a wide range of applications.

\nocite{langley00}




\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Additional Experiment Results}
\subsection{\methodFamilyName{} with Different Contrastive Loss Functions} \label{loss_discussion}
We select five classical contrastive loss functions to compare the quality of disentanglement with five metrics introduced in Section \ref{sec:verify} and the performance on downstream tasks: (1) Contrastive loss \cite{hadsell2006dimensionality}; (2) Triplet loss \cite{schroff2015facenet}; (3) Barlow Twins loss \cite{zbontar2021barlow}; (4) NT-Xent Loss \cite{chen2020simple} and (5) InfoNCE Loss \cite{oord2018representation}. We conduct the experiments following the experimental settings of the multi-risks classification task in Section \ref{case2} and compare the classification accuracy with two baselines, Self-Sim, and Linear Probe. We finally show the value of metrics and average rankings of these loss functions in Table \ref{tab:comparison_2}.


\begin{table*}[h]
\setlength{\tabcolsep}{5pt}
\centering
\caption{Evaluation of the disentanglement quality and classification performance across different contrastive loss functions.}\label{tab:comparison_2}
\begin{tabular}{{l}c *{7}{c}}
\toprule
Loss Type & Coding Rate$\downarrow$ & eRank$\downarrow$ & $\ell_2$ distance$\uparrow$ & Angle (${}^{\circ}$)$\uparrow$ & Hausdorff$\uparrow$ & Self-Sim$\uparrow$ & LP$\uparrow$ & Rankings$\downarrow$\\ 
\midrule
Contrastive Loss & 193.9 & 97.1 &  1.5 &  6.0 & 0.3 & 43.3 & 45.3 &4.14 \\
\midrule
Triplet Loss & 383.8 & 121.9 &  18.7 &  22.0 & 5.2 & 78.9 & \textbf{80.3} &3.17 \\
\midrule
Barlow Twins Loss & \textbf{183.6} & \textbf{8.1} & 228.1 & 26.2 & 25.2 & 57.7 & 67.9 &3.14 \\
\midrule
NT-Xent Loss & 368.7 & 18.5 &  255.2 &  62.0 & 35.4 & 78.4 & 78.9 & 2.32\\
\midrule
InfoNCE Loss & 408.8 & 26.2 &  \textbf{282.5} &  \textbf{76.4} & \textbf{38.4} & \textbf{79.1} & 79.3 & \textbf{2.21} \\
\bottomrule
\end{tabular}
\end{table*}
Table \ref{tab:comparison_2} indicates that InfoNCE loss achieves the best average performance on all metrics with an average rank of $2.21$, but the results of NT-Xent loss are also competitive, reaching an average rank of $2.32$. The NT-Xent loss performs better on metrics Coding Rate and eRank, which reflect the better quality of intra-class compression, but it is not as good as InfoNCE loss in terms of the metric (\eg, $\ell_2$ distance, angle, and Hausdorff distance) that reflects the quality of inter-class separation and classification performance. The Barlow Twins loss achieves the best intra-class compression effect, but lags far behind InfoNCE loss in terms of other metrics.

As described in Section \ref{case2}, the NT-Xent loss is a similar function to the InfoNCE loss, which can be calculated following the notations in Section \ref{sec:framework}.
\begin{equation}\label{eq:domr}
    \mathcal{L}_{\text{NT-Xent}} = -\mathbb{E}_{\{\vx^{i_1}_{c_k}, \vx^{i_2}_{c_k}\}_{k=1}^{B}} \left[\log \frac{\text{exp} (\vz^{i_1}_{c_k} \cdot \vz^{i_2}_{c_k} / \tau)}{\sum_{k^{\prime}=1}^{B} \text{exp} (\vz^{i_1}_{c_k} \cdot \vz^{i_2}_{c_{k^{\prime}}} / \tau) + \sum_{k^{\prime}=1}^{B} \mathds{1}_{k^{\prime}\neq k}\text{exp} (\vz^{i_1}_{c_k} \cdot \vz^{i_1}_{c_{k^{\prime}}} / \tau)}\right].
\end{equation}
NT-Xent loss utilizes the negative examples of both example in each pair, but InfoNCE loss only utilizes one of the negative examples in each pair. In this way, the performance comparison between the above two losses in Table \ref{tab:safety_alignment} can demonstrate the potential of \methodFamilyName{} for scaling the number of negative examples. Meanwhile, the consistency between better intra-class compression quality and improved security performance once again validates our theoretical analysis.

\subsection{More Experimental Details to Verify the Effectiveness of \methodFamilyName{} on the Disentanglement Quality} \label{app:details_verify}

\paragraph{Datasets and models.} In Section \ref{sec:verify}, we sample 740 examples as the train set and 400 examples as the test set, respectively, from each branch of the dataset \textbf{MATH} for the mathematic scenario, where 740 is the least amount of train data of mathematical branches and 400 is the least amount of test data. We select 200 examples as the train set and 100 examples as the test set from each of the seven subsets of the dataset \textbf{MMLU} for the knowledge scenario. The data setting for the safety scenario is the same as the settings introduced in Section \ref{case1}. We choose the layer located at $80\%$ of the hidden layer count as the target layer (\eg, the 25th layer in Llama-3.1-8B and Mistral-7B-v0.3, the 21st layer in Qwen2.5-7B, and the 33rd layer in Gemma2-9B,
starting from the 0th layer of LLMs). To evaluate the general capabilities, we utilize the LLMs Evaluation Platform, OpenCompass \cite{2023opencompass}.

\paragraph{Settings of \methodFamilyName{}.} 
We perform \methodFamilyName{} on the last token of QA pairs, which is usually the eot token. We utilize hooks to obtain the intermediate representations and calculate the disentangle loss $\mathcal{L}_{\text{d}}$ where the temperature parameter $\sigma$ is 0.1. All of the hyperparameter settings are shown in Table \ref{tab:hyper}.
\begin{table}[htbp]
  \centering
  \begin{minipage}{0.28\linewidth}
    \centering
    \caption{Specific Experimental Hyperparameters of \methodFamilyName{}.}
    \label{tab:hyper}
    \begin{tabular}{lc}
      \toprule
      Name & Value \\
      \midrule
      Learning Rate & 0.001 \\
      $\lambda$ & 0.1 \\
      $\alpha$ & 1 \\
      $\sigma$ & 0.1 \\
      Lora Alpha & 16 \\
      Lora Dim & 16 \\
      Lora Dropout & 0.05 \\
      Epoch & 2 \\
      \bottomrule
    \end{tabular}
  \end{minipage}
  \hfill
  \begin{minipage}{0.7\linewidth}
    \centering
    \vspace{-30pt}
    \caption{Additional Experimental Results of \methodFamilyName{} on the Safety Risks Classification Task by Applying \methodFamilyName{} after SFT.}
    \label{tab:after_sft}
    \setlength{\tabcolsep}{3pt}
    \begin{tabular}{lcccc}
      \toprule
      Model & \multicolumn{2}{c}{Binary Classification$\uparrow$} & \multicolumn{2}{c}{Multi-risks Classification$\uparrow$} \\
       \cmidrule(lr){2-3} \cmidrule(lr){4-5}
       & Origin($\%$) & Post-\methodFamilyName{}($\%$) & Origin($\%$) & Post-\methodFamilyName{}($\%$)\\
      \midrule
      Llama-3.1-8B & 67.4 & 81.0 & 57.8 & 61.7 \\
      Qwen2.5-7B & 75.1 & 82.7 & 74.8 & 78.5 \\
      Mistral-7B-v0.3 & 84.7 & 84.6 & 83.0 & 81.5\\
      Gemma2-9B &84.7 & 85.5 & 81.8 & 81.8\\
      \bottomrule
    \end{tabular}
    
  \end{minipage}
\end{table}

\subsection{More Experimental Details of Safety Risks Classification Task} \label{app:details_case1}
\paragraph{Datasets and models.} Based on BeaverTails \cite{ji2024beavertails}, we screen data related to only one type of safety risk, selecting five risks with more than 1600 entries each and 8000 entries from safe QA pairs to form the binary classification train set. For the test set, each broad concept contains 1000 entries for the binary classification. 1600 entries of safe examples along with the previous five safety risks serve as the multi-class classification train set. Each concept contains 200 entries for the test set of multi-class classification.

\paragraph{Settings of \methodFamilyName{}.} 
Compared with the representation-based baseline methods, we first fine-tune the LLMs through \methodFamilyName{} on the last token of QA pairs and then evaluate the classification performance of baseline methods on self-explained LLMs. For SFT, we apply \methodFamilyName{} before SFT without KL penalty (\ie, $\alpha = 0$) in Section \ref{case1}. We also perform \methodFamilyName{} after SFT, which also achieves improvement in classification. Such experimental results verify our theoretical analysis again, as shown in Table \ref{tab:after_sft} named Post-\methodFamilyName{}.



\subsection{More Experimental Details for The Detoxification Task} \label{app:details_case2}
In Section \ref{case2}, we perform SFT as a baseline, where we collect the same questions following the binary classification settings in Section \ref{case1} and generate safety responses with the LLMs themselves. We evaluate the LLMs' general capabilities with the average score from GSM8k, MMLU, and AGIEval. In this task, we apply \methodFamilyName{} on both the last token of question and answer without KL penalty (\ie, $\alpha = 0$). To compare with the SFT, we perform \methodFamilyName{} before and after SFT, which both improve the safety performance of SFT. The experimental results of the latter have been presented in Table \ref{tab:safety_alignment} of Section \ref{case2}, and the results of the former can be seen in Table \ref{tab:before_safe_sft}, named Pre-\methodFamilyName{}.

\begin{table}[htbp]
  \centering
  \begin{minipage}{.45\linewidth}
  \vspace{-48pt}
      \caption{Additional Experimental Results of SEER on the Detoxification Task of LLMs by Applying SEER before SFT.}\label{tab:before_safe_sft}
    \centering
    \setlength{\tabcolsep}{3pt}
    \begin{tabular}{lcccc}
      \toprule
      Model & \multicolumn{2}{c}{BT$\uparrow$} & \multicolumn{2}{c}{SB$\uparrow$} \\
       \cmidrule(lr){2-3} \cmidrule(lr){4-5}
       & Origin & Pre-\methodFamilyName{} & Origin & Pre-\methodFamilyName{}\\
      \midrule
      Llama-3.1-8B & 95.0 & 95.2 & 95.7 & 93.9 \\
      Qwen2.5-7B & 58.4 & 66.2 & 68.8 & 73.4\\
      Mistral-7B-v0.3 & 93.7 & 96.5 & 94.3 & 96.3  \\
      Gemma2-9B & 97.6 & 98.4 & 97.3 & 96.8\\
      \bottomrule
    \end{tabular}
  \end{minipage}\hfill
  \begin{minipage}{.45\linewidth}
  \caption{Evaluation of the Safety Performance on a Larger LLM with 14B Parameters, Verifying the Effectiveness of \methodFamilyName{} on the LLMs with Larger Size.}\label{tab:larger}
    \centering
    \setlength{\tabcolsep}{4.5pt}
    \begin{tabular}{{l}c *{4}{c}}
      \toprule
      Method & \multicolumn{2}{c}{Safety} & Over-Safety & Capability \\ 
      \cmidrule(lr){2-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5}
      & BT$\uparrow$ & SB$\uparrow$ & XST$\downarrow$ & Average$\uparrow$ \\
      
      \rowcolor{gray!30}
      \multicolumn{5}{c}{ Qwen2.5-14B } \\
      Origin & 92.4& 94.5 & 12.4 & 74.9 \\
      \methodFamilyName{} & \textbf{99.6}&\textbf{99.6}&24.4 &75.0\\
      $\methodFamilyName{}_{~\text{NT-Xent}}$ & 99.2&99.3&24.8 & 75.2 \\
      
      \midrule
      SFT & 64.5&70.0& \textbf{11.2} & 74.1 \\
      SFT + \methodFamilyName{} & 97.9&97.0& 17.2 & 75.3 \\
      \methodFamilyName{} + SFT & 76.4&76.3& 11.6 & \textbf{76.2} \\
      
      \bottomrule
    \end{tabular}
  \end{minipage}
\end{table}

\subsection{Seer Can Improve the Safety Performance of LLMs with Larger Size.}  \label{app:larger}
We apply \methodFamilyName{} on Qwen2.5-14B-Instruct with the same experimental setting introduced in Section \ref{case2}. The experimental results shown in Table \ref{tab:larger} indicates that \methodFamilyName{} improves the safe performance of the original LLM by $6.6\%$ and $29.7\%$ compared with the supervised fine-tuned LLM, which verify the effectiveness of \methodFamilyName{} on LLMs with Larger Size.

\subsection{Ablation Study on the Components of Retain Loss $\mathcal{L}_{\text{r}}$} \label{app:ablation}
We conducted ablation studies on the components that maintain the general performance of LLMs. Specifically, as described in Section \ref{sec:framework}, the framework of \methodFamilyName{} consists of two hyperparameters related to retaining LLMs' general capabilities: $\lambda$ and $\alpha$. In setting (a), if $\lambda$ and $\alpha$ are non-zero, \methodFamilyName{} employs both the $l_2$ norm constraint and the KL penalty. In setting (b), when $\lambda$ is non-zero but $\alpha$ is set to 0, \methodFamilyName{} only applies the norm constraint and discards the KL penalty. In setting (c), when $\lambda$ is set to 0, the \methodFamilyName{} does not utilize the retain loss $\mathcal{L}_{\text{r}}$. Following the experimental settings in Section \ref{sec:verify}, we perform the ablation study on Llama-3.1-8B-Instruct.

\begin{table}[h]

\centering
\footnotesize
\renewcommand\arraystretch{1.0}
\caption{Ablation Study on the Components of Retain Loss Controlled in Three Scenarios Introduced in Section \ref{sec:verify}.}
\label{tab:ablation}
\setlength{\tabcolsep}{1.2mm}{
\resizebox{1\textwidth}{!}{
\begin{tabular}{c|cc|ccc|ccc|ccc|}
\toprule

Setting&\multicolumn{2}{c|}{\bf Components}
& \multicolumn{3}{c|}{\bf Math}
& \multicolumn{3}{c|}{\bf Knowledge}
& \multicolumn{3}{c}{\bf Safety} \\
\midrule

&{\bf $l_2$ norm} & {\bf KL penalty} & {\bf GSM8k$\uparrow$} & {\bf MMLU$\uparrow$} & {\bf AGIEval$\uparrow$} 
& {\bf GSM8k$\uparrow$} & {\bf MMLU$\uparrow$} & {\bf AGIEval$\uparrow$} 
& {\bf GSM8k$\uparrow$} & {\bf MMLU$\uparrow$} & {\bf AGIEval$\uparrow$}  \\
\midrule

\rowcolor{gray!20}
&\multicolumn{2}{c|}{\bf Origin} & 47.3 & 69.4 & 84.5 & 47.3 & 69.4 & 84.5 & 47.3 & 69.4 & 84.5\\
(a)&$\checkmark$ & $\checkmark$ & \textbf{46.4} & \textbf{69.2} & \textbf{82.2} & \textbf{46.5} & \textbf{68.9} & \textbf{82.0} & \textbf{46.2} & \textbf{68.6} & 82.6 \\
(b)&$\checkmark$ &  & 36.6 & 64.8 & 17.1 & 2.8 & 5.8 & 2.9 & 45.6 & 68.5 & \textbf{83.1} \\
(c)& &  & 35.0 & 61.4 & 2.9 & 2.5 & 4.4 & 0.0 & 44.7 & 67.8 & 82.3 \\

\bottomrule
\end{tabular}%
}
}
\vspace{-0.15in}
\end{table}

Table \ref{tab:ablation} demonstrates that \methodFamilyName{} with whole components of $\mathcal{L}_{\text{r}}$ achieves the least degradation of the LLM's general capability. We find that the necessity of $\mathcal{L}_{\text{r}}$ is related to the specific scenario of disentanglement. When the disentangled concepts come from mathematics and knowledge scenario, which overlap with the general capabilities of LLMs, maintaining the general capabilities of the model becomes particularly important. In the scenario of safety, which is almost unrelated to general capabilities, $\mathcal{L}_{\text{r}}$ seems less important, but still better maintain the performance of LLMs.


\section{Additional Details of Theoretical Analysis}\label{thm_details}
\subsection{Additional Details of the Formulation of LLMs}
In Section \ref{sec:thm}, we introduce a hypothesis component $\psi$ to decompose the LLM $f_{\theta}$ as a hidden representation encoder $\phi := f_{\theta_{\le l }} \in \Phi $ and a score-based classifier $g := \psi \circ f_{\theta_{\textgreater l}} \in \gG $. Here, with the vocabulary space $\sV$ and the maximum output length $t_{\text{max}}$, $\psi \in \sR^{|\sV| \times t_{\text{max}}} \times \sR^{C}$ is a mapping from the output logits space $\sR^{|\sV| \times t_{\text{max}}}$ to the score-based concept prediction space $\sR^{C}$. For example, in the safety-related scenario,  $\psi$ can be described as a judger LLM \cite{li2024salad,inan2023llama}, whose logits of the tokens ``safe'' and ``unsafe'' can be seen as the scores of the classifer $g$.

To describe the data distribution, we introduce $p$ and $p_j$ to represent the distribution followed by the entire Disentangle Set $\{\mathcal{D}_j\}_{j=1}^{C}$ and the distribution followed by a subset $\mathcal{D}_j$ of the concept $j$, respectively. Moreover, we use $\mu(j) \in \gC \times \sR$ to describe the probability distribution $j \sim \mu$ over the concept space $\gC$.

\subsection{Proof of Theorem \ref{thm_margin}}
\setcounter{theorem}{2}
\begin{definition} (The \textit{ramp loss} from \cite{bartlett2017spectrally,chuang2021measuring})

Given the margin $\tau$, the \textit{ramp loss} is calculated as
\begin{equation}
\mathcal{L}_\tau(u) = \mathds{1}_{u \leq 0 } + (1 - \frac{u}{\tau}) \mathds{1}_{0 < u \leq \tau}
\end{equation}
\end{definition}

\begin{proposition} (Proven in Lemma A.4 in \cite{bartlett2017spectrally})\label{prop:leq1}

For any $g: \sR^m \rightarrow \sR^{C}$ and every $\tau>0$,
\begin{equation}
R_{p}(g \circ \phi) = \Pr(\argmax_{j^{\prime}} g_{j^{\prime}}(x_j )\neq j) \leq \mathbb{E}_{(x_j,j)}\mathcal{L}_{\tau}(\rho_{g}(\phi(x_j)))
\end{equation}
where the $\argmax$ follows any deterministic tie-breaking strategy.
\end{proposition}

\begin{proposition} (Proven in Lemma 12 in \cite{chuang2021measuring})\label{prop:leq2}

The margin $\rho_{g}(.,j)$ is lipchitz in its first argument with constant $2L$ if $\mathcal{G}_j$ are lipchitz with constant $L$. 
\end{proposition}

\setcounter{theorem}{0}
\begin{theorem} Given a classifier $g \in \gG$, where $g = [g_1, \cdots, g_C]$ and $ \gG = \gG_1\times  \cdots\times \gG_C$; $\gG_j: \gX \rightarrow \sR$. With $\tau > 0$, the generalization bound can be measured for all $g \in \gG$ with probability at least $1 - \delta > 0$:
\begin{align}
&R_{p}(g \circ \phi) \leq \hat{R}_{\tau,n}(g \circ \phi) +  \E_{j \sim \mu} \left[\frac{\Lip(g,j)}{\tau} \Var_{n_j}(\phi_\#p_j) \right] + \sqrt{\frac{\log(1 / \delta)}{2n}}, 
\end{align}
where $\Lip(g,j) = \sup_{x_j,x^{\prime}_j \in \gX} \frac{|\rho_g(\phi(x_j))-\rho_g(\phi(x^{\prime}_j))| }{\| \phi(x)-\phi(x^{\prime})\|_2}$ is the margin Lipschitz constant w.r.t $\phi$.
\end{theorem}
\begin{proof}[Proof of Theorem \ref{thm_margin}](This proof is rephrased from the Appendix C.2 in \cite{chuang2021measuring})

By Proposition \ref{prop:leq1}, we have:
\begin{align}
R_{p}(g \circ \phi) \leq \mathbb{E}_{(x_j,j)}\mathcal{L}_{\tau}(\rho_{g}(\phi(x_j))).
\end{align}

We can transform the expected zero-one loss into the average concept-level zero-one loss:
\begin{align}
R_{p}(g \circ \phi) = \mathbb{E}_{j \sim \mu}R_{p_j}(g \circ \phi) = \sum^C_{j=1}\mu(j) \E_{x_j \sim p_j}[\mathds{1}_{\rho_g(\phi(x_j)) \leq 0}].
\end{align}

By McDiarmid Inequality, we have with probability at least $1 - \delta$,
\begin{align}
    R_{p}(g \circ \phi) \leq \sum_{j=1}^C \mu(j) \hat{\E}_{D_j \sim p^n_j}\mathcal{L}_\tau(\rho_{g}(\phi(x_j))) + \mathbb{S}(g\circ \phi, p)+ \sqrt{\frac{\log(1 / \delta)}{2n}},
    \label{eq:gen}
\end{align}
where the $D_j=\{x_j^1,\ldots,x_j^n\}$ that $x_j^i \overset{\text{i.i.d}}{\sim} p_j$ and 
\begin{align}
\mathbb{S}(g\circ \phi, p)=\E_{D_1 \sim p_1^n}\dots\E_{D_C \sim p_C^n} \left[\sup_{g \in \mathcal{G}} \left(\sum_{j=1}^C \mu(j) (\E_{p_j}[\mathcal{L}_\tau(\rho_{g}(\phi(x_j)))] - \hat{\E}_{D_j \sim p_j^n}[\mathcal{L}_\tau(\rho_{g}(\phi(x_j)))]) \right) \right].
\end{align}

For a given concept $j$ and feature map $\phi$ define:
\begin{align}
\gH_j = \left\{h| h(z)= L_\rho \circ \rho_{g}(z_j) : g \in \gG, z_j = \phi(x_j) \in \sR^n\right\},
\end{align}
where $L_\rho$ is the lipchitz constant of $\rho$ provided in Proposition \ref{prop:leq2}.

According to the nature of $\sup$ that $\sup (a+b)\leq \sup a+\sup b  $, we have:
\begin{align}
\mathbb{S}(f\circ \phi, p)&\leq \sum_{j=1}^C \mu(j) \mathbb{E}_{D_j\sim p_j} \sup_{g \in \mathcal{G}} \left(\E_{p_j}[\mathcal{L}_\tau(\rho_{g}(\phi(x_j)))] - \hat{\E}_{D_j \sim p_j^n}[\mathcal{L}_\tau(\rho_{g}(\phi(x_j)))]) \right) \nonumber  \\
&=\sum_{j=1}^C \mu(j) \mathbb{E}_{D_j\sim p_j} \left[\sup_{h \in \gH_j} \left(\E_{p_j}[h(\phi(x))] - \hat{\E}_{D_j\sim p^n_j}[h(\phi(x))] \right)\right],
\label{eq:Dev}
\end{align}
where the last equality follows from the definition of the function class $\gH_j$.

Following the proof in \cite{chuang2021measuring}, we have:
\begin{align}
 \mathbb{E}_{D_j\sim p_j} \left[\sup_{h \in \gH_j} \left(\E_{p_j}[h(\phi(x))] - \hat{\E}_{D_j\sim p^n_j}[h(\phi(x))] \right)\right] &\leq \frac{\Lip(g,j)}{\tau} \E_{\substack{x_j^1,\ldots,x_j^n \sim p_j^n \\ {x^{\prime }}_j^1,\ldots,{x^{\prime}}_j^n \sim p_j^n}} \left[ \mathbb{D}_1(\phi_\#\frac{1}{k}\sum_{i=1}^k \delta_{x_j^i}, \phi_\#\frac{1}{k}\sum_{i=1}^k \delta_{{x^{\prime}}_j^i} ) \right] \nonumber\\
&=\frac{\Lip(g,j)}{\tau} \Var_{n_j}(\phi_\#p_j).
\label{eq:k-varianceBound}
\end{align}

Note that,
\begin{align}
\mathcal{L}_{\tau}(\rho_{g}(\phi(x_j))) \leq \mathds{1}_{\rho_g(\phi(\vx_j)) \leq \tau},\end{align}
we have the following generalization bound by \eqref{eq:gen} that holds with probability $1-\delta$: 
\begin{align}
R_{\mu}(f \circ \phi) &\leq \sum_{j=1}^C \mu(j) \hat{\E}_{D_j \sim p^n_j}\mathds{1}_{\rho_g(\phi(\vx_j)) \leq \tau} + \sum_{j=1}^C \mu(j)\frac{\Lip(g,j)}{\tau} \Var_{n_j}(\phi_\#p_j)+ \sqrt{\frac{\log(1 / \delta)}{2n}}
\\
&=\hat{R}_{\tau,n}(g \circ \phi) +  \E_{j \sim \mu} \left[\frac{\Lip(g,j)}{\tau} \Var_{n_j}(\phi_\#p_j) \right] + \sqrt{\frac{\log(1 / \delta)}{2n}}.
\end{align}


\end{proof}

\subsection{More Details for Our Theoretical Analysis}
\paragraph{Assumptions for the lipchitz constant of the margin $\rho(.,j)$}
To apply Theorem \ref{thm_margin}, we assume that the $\Lip(g,j)$ is a constant by Proposition \ref{prop:leq2}, where the classifier $g=\psi \circ f_{\theta_{\textgreater l}} \in \gG$ can have a uniform lipchitz constant across the space $\gG$ consisting of a part of network $f_{\theta_{\textgreater l}}$ and the hypothesis component $\psi$. 

\paragraph{Assumptions for the improvement of disentangled representations on the classifier $g$}
With disentangled representations that have better quality of inter-class separation, we assume that we can obtain a better classifier $g^{\prime}$ with a higher $\rho_g^{\prime}(\phi(\vx_j))$ on a wide range of samples. Given fixed $\tau$, fewer samples will satisfy the condition $\rho_g^{\prime}(\phi(\vx_j)) \leq \tau$ and thus the empirical $\tau$-margin loss $\hat{R}_{\tau,n}(g^{\prime} \circ \phi)$ decreases.

\paragraph{The effect of \methodFamilyName{} on the generalization ability of LLMs in Theorem \ref{thm_margin}}
When we perform \methodFamilyName{} on LLMs, the $k$-variance $\Var_{n_j}(\phi_\#p_j)$ of each concept $j$ is reduced, leading to lower generalization bound in Theorem \ref{thm_margin}. This corresponds to our setups of applying \methodFamilyName{} in the original model and after SFT in Section \ref{case2}, which enhances the LLMs' generalization capability, thereby improving the safety performance of LLMs. 
Meanwhile, when representations are disentangled, a better classifier $g^{\prime}$ can be trained with lower empirical $\tau$-margin loss $\hat{R}_{\tau,n}(g^{\prime} \circ \phi)$, decreasing the generalization bound and improving the generalization capability of LLMs. Such a situation corresponds to our settings of applying \methodFamilyName{} before baseline methods in Section \ref{case1}, which improves the classification performance of baseline methods.

\section{Cases of the Detoxification Task.}
In this section, we showcase examples to demonstrate the practical effectiveness of \methodFamilyName{} on the detoxification task. Figure \ref{fig:case2_1} and Figure \ref{fig:case2_2} show the responses from Llama-3.1-8B-Instruct with different safety detoxification settings to harmful questions related to crime. These cases indicate that \methodFamilyName{} achieves the improvement of safety performance on both the original LLM and the supervised fine-tuned LLM, which just enhances the self-explainability of LLMs on safety-related concepts and even doesn't train LLMs to refuse harmful requests like SFT.


\begin{figure*}[t]
    \vspace{-5pt}
    \centering
    \includegraphics[width=0.95\linewidth, clip]{safety_cases_1.pdf}
    \vspace{-5pt}
    \caption{A example on detoxification task related to crime from Llama-3.1-8B-Instruct.}\label{fig:case2_1}
    \vspace{-5pt}
\end{figure*}

\begin{figure*}[t]
    \vspace{-5pt}
    \centering
    \includegraphics[width=0.95\linewidth, clip]{safety_cases_2.pdf}
    \vspace{-5pt}
    \caption{Another example on detoxification task from Llama-3.1-8B-Instruct. }\label{fig:case2_2}
    \vspace{-5pt}
\end{figure*}
% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
