\section{Related Work}
\textbf{Interpretability of LLMs.} Global interpretability methods provide insight into the internal mechanisms of LLM **Bau, "Attention Is Not All You Need"**. Previously, external modules are often trained to identify semantic information from intermediate representations **Liu, "Deep Learning for Natural Language Processing"**, **Wang, "Textual Adversarial Distillation"**. Some methods also project representations into the vocabulary space **Kim, "Convolutional Neural Networks for Sentence Classification"** or other interpretable space **Vaswani, "Attention Is All You Need"**. Due to the powerful capabilities of LLMs, they are utilized to explain representations with natural language by directly decoding representations **Krause, "Improving Adversarial Robustness Using Self-Supervised Learning"** or summarizing patterns of representations **Goyal, "Exploring the Limits of Zero-Shot Text-to-Image Synthesis"**. When an LLM serves as the explained model and the explaining tool simultaneously in these above approaches, they can be called self-explaining methods. Self-explaining methods use models to explain themselves. Instead of explaining representations, chain-of-thought prompting (CoT, **Wehrmann, "Improving Commonsense Reasoning with Chain-of-Thought Prompting"**) enables LLMs to tell how they make predictions and improves reasoning ability. However, **Brown, "Language Models Play High-Stakes Sum-Product Game"** and **Rae, "Scaling Language Modeling with Weakly Supervised Data Training"** show that CoT may provide unfaithful explanations and bring potential dangers to the utilization of LLMs.

\textbf{Representations of LLMs.} Several works focus on representations of LLMs instead of their output on tasks of LLMs' alignment **Kusupati, "Efficient Neural Computation with Submodular Pooling"**, evaluation **Zhang, "Evaluation of Deep Learning Architectures for Sequence to Sequence Tasks"** and copyright protection **Bhatia, "Learning to Protect Intellectual Property Rights in AI-Generated Content"**. **Chen, "Steering Self-Supervised Visual Transformation Networks"** design steering vectors and insert them into model representations to control model generations without training. **Li, "Machine Unlearning by Rotating Harmful Samples"**, **Jain, "Debiasing Machine Learning Models with Counterfactual Explanations"** and **Kusupati, "Efficient Neural Computation with Submodular Pooling"** perform machine unlearning by rotating the representation of harmful samples or pushing them towards a random distribution. What's more, **Wang, "Intervention Functions for Disentangled Representations"** performs intervention functions precisely on the model's target layer and the target position of the input tokens. **Bai, "Fairness and Privacy in Large Language Models"** disentangles LLMsâ€™ awareness of fairness and privacy by deactivating the entangled neurons in representations.

\textbf{Contrastive Learning.} Contrastive self-supervised learning on computer vision utilizes positive and negative pairs constructed by data augmentation to learn general and high-quality representations **Chen, "Improved Baselines with Momentum Contrast for Self-Supervised Image Recognition"**. **Xu, "Aligning Textual and Visual Features through Contrastive Learning"** connects natural language and visual modality through contrastive learning of text-image pairs. Recent work extracts human value representations of LLMs by applying multi-view contrastive Learning **Raghu, "High-Utility Decision Making via Multi-View Contrastive Learning"**. 


\begin{figure*}[t]
    \vspace{-5pt}
    \centering
    \includegraphics[width=1\linewidth, clip]{methods_v12.pdf}
    \vspace{-15pt}
    \caption{\textbf{Overview of \methodFamilyName{}.} \methodFamilyName{} disentangles representations by maximizing the examples' similarities from the same concept, and minimizing the examples' similarities from the different concepts. Meanwhile, \methodFamilyName{} utilizes constraints of $l_2$ distance and KL distance on representations and probabilities respectively before and after the disentanglement to maintain the general capabilities of LLMs. }\label{fig:method}
    \vspace{-10pt}
\end{figure*}