%Visuomotor Imitation Learning
%模仿学习为机器人提供了一种从专家演示中获取人类技能的有效方法。基于图像观察的视觉策略占据了该领域的主导地位。这些方法显式或隐式地将视觉观测 mapping 到 action space 中。近期的工作中，基于扩散的 visuomotor policies excel at learning complex manipulation tasks by effectively combine visual data with high-dimensional, multi-modal action distributions。但是这些方法关注于 policy 决策层的构建，缺乏针对机器人操作场景下视觉感知层的研究。在我们的工作中，我们尝试提出一种双视觉工作流作为视觉感知层的增强框架，并为其注入 fined-grained pixel-level 视觉语义信息。
%视觉，语言和动作结合的多模态模型是建立 embodied agent 的关键路径。尽管端到端的方法在离线任务中很流行，但直接从语言注释数据中学习存在挑战，特别是将视觉、语言观测和机器人传感器数据对齐到共享空间时。Open vocabulary vision foundation models, including VLMs, 通过视觉-语言联合学习，允许自然语言描述指导视觉理解。这些模型在各种下游任务中具有良好的迁移能力。他们可以在 robotics 中用作定义复杂目标的工具，多模态表征的语义锚点以及规划和推理的中间基础。现有方法，such as VoxPoser or ReKep，使用 open vocabulary vision foundation models 来得到 high-level text instruction 或操作约束。在我们的工作中，我们利用 open vocabulary vision foundation models 的先验知识，将高层语义信息对齐到与视觉观测同模态的细粒度 pixel-level label。
%扩散模型因其迭代采样过程而具有较长的推理时间。加速扩散模型采样是提升机器人控制实时性的关键问题。一些工作，例如 Denoising Diffusion Implicit Models (DDiM) 和 Elucidated Diffusion Models (EDM) 可以解释为 a deterministic ordinary differential equation (ODE)，通过减少预测时的 denoise steps 来减少推理时间。这种可变步长方法减少了 denoise steps 的同时也会降低采样质量。另一些研究使用 Picard 迭代等方法，通过并行采样加速扩散模型。这在计算受限的机器人系统中不切实际。基于蒸馏的技术允许学生模型沿着教师已经映射的 ODE 轨迹迈出更大的步伐。 Aaditya 等人在 U-Net 上允许学生将任意步长和间隔的输入映射到给定 ODE 轨迹上的相同起点。我们在 DiT action head 中进行了类似实现，验证了一致性策略与 policy 结构的正交性。这导致推理速度提升了一个数量级。

\section{Related Work}
\label{Related}

\subsection{Visuomotor Imitation Learning}
Imitation learning offers an effective approach for robots to acquire human-like skills through expert demonstrations \citep{wang2023mimicplay, agarwal2023dexterous, haldar2023teach, seo2023deep, fu2024mobile}. Policies based on visual observations have become dominant in the field \citep{pari2021surprising, gervet2023act3d}. These methods focus on mapping or conditioning visual observations to an action space \citep{zhao2023learning, lin2024data}. Recent advancements in diffusion based visuomotor policies have shown significant promise in learning complex manipulation tasks by effectively integrating visual data with high-dimensional, multimodal action distributions \citep{chi2023diffusion, chi2024universal, zhao2024aloha, ze20243d}. However, these methods focus on the construction of the policy decision layer and lack attention to the visual perception layer in robot manipulation scenarios. In our work, we attempt to propose a dual vision workflow as an enhancement framework for the visual perception layer and inject it with fined-grained pixel-level visual semantic information.

\subsection{Open Vocabulary Vision Foundation Models in Robotics}
Multimodal models that integrate vision, language, and action are crucial for developing embodied agents. While end-to-end approaches are commonly used for offline tasks, learning directly from language-annotated data presents significant challenges, particularly when aligning vision, language observations, and robot sensor data in a shared space \citep{black2024pi_0, team2024octo}. Open vocabulary vision foundation models, including vision language models (VLMs), enable natural language descriptions to guide visual understanding through vision-language joint learning \citep{hurst2024gpt, liu2024visual}. These models demonstrate strong transferability across various downstream tasks, making them valuable tools in robotics for defining complex objects, serving as semantic anchors for multimodal representations, and providing an intermediate foundation for planning and reasoning. Existing methods, such as VoxPoser \citep{huang2023voxposer} and ReKep \cite{huang2024rekep}, utilize open vocabulary vision foundation models to acquire high-level text instructions or operational constraints. In our work, we leverage the prior knowledge from these models to align high-level semantic information with fine-grained, pixel-level labels that are  the same modality with visual observations.

% 插入overview图
% Overview of Imit Diff. 我们首先在 (a) Semanstic Injection Module 中构建了 vision language agent。Agent 根据 video demonstrations 在与人类交互后得到面向任务的语义描述。语义描述随后被 agent 转化为 real time pixel-level 的视觉模态定位标签。(b) Dual Res Fusion Module 负责提取双分辨率视觉观测的多尺度视觉特征，并与之前得到的视觉语义特征进行融合。得到的最终视觉特征作为 envrionment observation 和 robot perception 一起进入 (c) Consistency Policy with DiT Architecture，在 Transformer Decoder Action Head 中作为 condition 监督 action 的 single-step denoise。
\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\textwidth]{Figure/overview.pdf} 
    \caption{\textbf{Overview of Imit Diff.} We first developed a vision language agent in the (a) \textbf{Semantic Injection Module}, where the agent acquires task-oriented semantic descriptions from video demonstrations after interacting with humans. These semantic descriptions are then transformed by the agent into real-time pixel-level visual modality localization labels. The (b) \textbf{Dual Res Fusion Module} is responsible for extracting multi-scale visual features from dual res visual observations and fusing them with the previously obtained visual semantic features. The final visual features are then fed into (c) \textbf{Consistency Policy with DiT Architecture}, where they serve as environment observations. These features condition the supervision of single-step denoising of actions in the transformer decoder action head together with robot perceptions.}
    \label{fig:overview}
\end{figure*}

\subsection{Diffusion Model Acceleration Strategy in Robotics}
Diffusion models typically suffer from long inference times due to their iterative sampling process, which presents a key challenge for improving the real-time performance of robot control. Approaches such as Denoising Diffusion Implicit Models (DDiM) \citep{song2020denoising} and Elucidated Diffusion Models (EDM) \citep{ning2023elucidating} interpret the process as a deterministic ordinary differential equation (ODE), reducing inference time by minimizing denoising steps during prediction. However, this variable step size approach reduces the number of denoising steps, which can degrade sampling quality. Other methods, like Picard iteration, accelerate diffusion models through parallel sampling, but such approaches are not feasible in computationally constrained robotic systems \citep{shih2024parallel}. Distillation-based techniques enable the student model to take larger steps along the ODE trajectory mapped by the teacher model. \citet{prasad2024consistency} demonstrated this on U-Net, where the student can map inputs with arbitrary step sizes and intervals to the same starting point on the given ODE trajectory. We implement a similar approach in the DiT action head and validate the orthogonality of the consistency policy to the policy structure, resulting in a significant improvement in inference speed.

