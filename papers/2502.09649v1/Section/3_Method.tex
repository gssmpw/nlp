%Imit Diff 是一个将细粒度语义信息和注入到双视觉工作流多尺度提取中的 diffusion transformer imitation learning framework。它同时保证了机器人的控制实时性。To this end，we introduce Imit Diff with three critical parts: (a) Semantics Injection. Imit Diff injects fine-grained semanstic information into policy learning through vision foundation models. (b) Dual Res Fusion. The semanstic information is then injected into multi-scale vision features extracted by dual res fusion module. (c) Consistency Policy with DiT Architecture. Imit Diff contructs a DiT based action head with consistenvy policy implementation to ensure real time control. An overview of Imit Diff is in Fig. 2. We will detail each part in the following sections.
%Semanstic Injection
%如 Figure 2. (a) Semanstic Injection Module 所示，给定一个操作任务 T (例如，cover white lid on the red glue)，我们使用 GPT-4o 在用户提示下按照指定约束进行任务语义描述的分析，并过滤与任务相关的对象。我们利用 vision foundation model GroundingDINO 为初始帧添加视觉提示。跟踪器 MixFormerV2 在机器人运动过程中 (including 运动模糊和遮挡) 维持视觉提示。考虑实时性和计算资源，使用Mobile SAM 根据视觉提示得到细粒度视觉语义遮罩。为了保持对齐性，视觉语义遮罩与双视觉流中的低分辨率观测保持同一大小，并使用相同的预训练视觉编码器，通过不同的映射器进行 domain 微调。我们构建了 transformer decoder 架构的 semanstic injection block 来为视觉观测特征注入视觉语义特征 (视觉观测特征作为 Q ，语义掩码特征作为 K 和 V)。我们不直接将视觉语义特征作为单独观测的原因是希望 policy 感知层可以通过 attention 学习前景对象与 image-level 机械臂状态的关系。
%Imit Diff uses a 双分辨率视觉工作流来同时提取全局和多尺度的细粒度特征。Figure 3 illustrates this process。高分辨率图像经过降采样得到低分辨率图像。对于低分辨率图像，我们通过 vision transformer 提取带有全局信息的视觉特征。对于高分辨率图像，我们通过卷积网络提取带有多尺度信息的视觉特征图。为了融合该多尺度信息并获取细粒度的特征表示，我们对高分辨率视觉特征图进行 FPN 处理。We consume that 经过 FPN 的特征流动，最底层的视觉特征包含多尺度细粒度视觉信息。该层特征随后在 dual res fusion block 中与低分辨率特征进行 attention 融合。其中，Q 是低分辨率视觉特征，K 和 V 是高分辨率视觉特征。To 高效挖掘两种维度的视觉信息，我们参考了 Swin-Transformer 中的滑窗思路，对两者进行 patch-level 的 attention 融合。Resuidal 机制被用于保留融合前的原始全局信息，得到的视觉特征进一步地在传递至 action head 前与 robot perception tokens 在 transformer encoder 结构的 multi-modal fusion block 中进行信息交互。
%扩散模型的迭代采样过程和获取视觉语义遮罩 pipeline 使得机器人的实时控制成为挑战。受启发于一致性策略在图像生成领域的出色性能，我们设计了基于一致性策略的 DiT action head，并将其与视觉观测获取和 action prediction 置于异步框架中。我们首先在 EDM 框架下训练教师模型S，我们将时间t和条件o拼接作为 transformer decoder 中的 K 和 V，代替 U-Net 中使用的 FiLM 模块，将当前动作 xt 输入，通过注意力机制估计 probability Flow ODE (PFODE) 轨迹的导数：
%我们使用一种优化的 denoising score matching (DSM) 损失来训练 EDM 模型，DSM 沿 PFODEXTT 采样并训练以预测初始 action ground truth x0:
%其中，d是一种优化的Huber损失：
%对于学生模型g_θ (x_t,t,s;o)，我们在时间t和条件o的序列上额外拼接停止时间s，在同一PFODE轨迹上采样点(x_(t_1 ),t_1)和(x_(t_2 ),t_2)去噪回相同的停止时间s。我们使用x_s^((t_1))和x_s^((t_2))指代计算得到的g_θ (x_(t_1 ),t_1,s;o)和g_θ (x_(t_2 ),t_2,s;o)，并将二者去噪回初始时间步t=0得到g_θ (x_s^((t_1)),s,0;o)和g_θ (x_s^((t_2)),s,0;o)。我们在完全去噪的动作空间中计算一致性损失：
%最终的损失计算综合DSM损失和CTM损失

\section{Method}
\label{Method}
Imit Diff is a diffusion transformer imitation learning framework that injects fine-grained semantic information into dual-vision workflows, which extract multi-scale visual features. It also ensures real time control of the robot. To this end, we introduce Imit Diff with three critical components: (a) \textbf{Semantics Injection.} Imit Diff injects fine-grained semanstic information into policy learning through vision foundation models. (b) \textbf{Dual Res Fusion.} The semanstic information is then injected into multi-scale vision features extracted by dual res fusion module. (c) \textbf{Consistency Policy with DiT Architecture.} Imit Diff contructs a DiT based action head with consistenvy policy implementation to ensure real time control. An overview of Imit Diff is in Figure \ref{fig:overview}. We will detail each part in the following sections.

\subsection{Semanstic Injection}
\label{Semanstic Injection}
As shown in Figure \ref{fig:overview} (a) Semanstic Injection Module, given a manipulation task $T$ (e.g., cover the white lid on the red glue), we use GPT-4o to analyze the semantic description of the task, considering specified constraints based on user prompts, and filter out objects relevant to the task. We then use the vision foundation model GroundingDINO \citep{liu2025grounding} to add visual cues to the initial frame $I_{0}$. The tracker MixFormerV2 \citep{cui2024mixformerv2} ensures these visual cues are maintained during robot motion, including handling motion blur and occlusion. To balance real-time performance with computational efficiency, we employ Mobile SAM \citep{zhang2023faster} to generate fine-grained visual semantic masks from these visual cues. To maintain alignment, the visual semanstic masks are resized to match the low resolution observations in the dual visual streams (which will be described in detail in Section \ref{Dual Res Fusion}), and the same pre-trained visual encoder is used for domain fine-tuning through different projectors. We integrate a semanstic injection block $F_{S}$ with the transformer decoder architecture to inject visual semantic features $I_{S}^{'}$ into the visual observation features $I_{V}^{'}$, where $I_{V}^{'}$ are treated as $Q$, and $I_{S}^{'}$ as $K$ and $V$. We do not directly use the visual semantic features as separate observations because we aim for the policy perception layer to learn the relationship between foreground objects and the robot state at the image-level through attention mechanisms.

% 插入dual_res_fusion图
% Illustration of dual res fusion module. 低分辨率工作流通过 vision transfomrer (ViT) 提取全局视觉信息。高分辨率工作流通过卷积网络提取多尺度细节信息。In our study, we instantiate low res vision encoder with the CLIP-pretrained ViT-B and high res vision encoder with the LAION-pretrained ConvNext-B. We keep two vision encoders frozen and optimize the projectors for efficient training。
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\textwidth]{Figure/dual_res_fusion.pdf} 
    \caption{\textbf{Illustration of dual res fusion module.} The low res workflow extracts global visual information through the vision transfomrer (ViT), while the high res workflow extracts multi-scale detail information through a convolutional network. In our study, we instantiate low res vision encoder with the CLIP-pretrained ViT-B \citep{radford2021learning} and high res vision encoder with the LAION-pretrained ConvNext-B \citep{liu2022convnet}. We keep two vision encoders frozen and optimize the projectors for efficient training.}
    \label{fig:dual_res_fusion}
\end{figure}

\subsection{Dual Res Fusion}
\label{Dual Res Fusion}
Imit Diff employs a dual res visual workflow to concurrently extract global features and multi-scale fine-grained features. Figure \ref{fig:dual_res_fusion} illustrates this process. The high resolution image $
I_H \in \mathbb{R}^{C \times H \times W}$ is first downsampled to a lower resolution $I_L \in \mathbb{R}^{C \times H/2 \times W/2}$. For the low resolution image, the global visual feature $I_{L}^{'}$ are extracted using a vision transformer $F_{L}$. For the high resolution image, multi-scale visual feature maps $I_{HM}^{'}$ are obtained through a convolutional network $F_{H}$. To integrate this multi-scale information and derive fine-grained feature representations, feature pyramid network (FPN) processing is applied to the high resolution visual feature maps. The feature flow processed by FPN yields the bottom-level visual feature $I_{H}^{'}=I_{HM}^{'0}$, which encapsulates multi-scale fine-grained information by aggregating features from all levels of the pyramid. $I_{H}^{'}$ is subsequently fused with $I_{L}^{'}$ via cross-attention within the dual res fusion block. Here, the query ($Q$) corresponds to $I_{L}^{'} \in \mathbb{R}^{N^{2} \times D}$, while the keys ($K$) and values ($V$) are derived from $I_{H}^{'} \in \mathbb{R}^{N^{2} \times M^{2} \times D}$. To efficiently leverage visual information across both resolutions, we adopt the sliding window approach inspired by Swin-Transformer, performing patch-level attention fusion. A residual mechanism is incorporated to preserve the original global information prior to fusion. The resulting visual feature $I_{F}^{'}$ is then further integrated with robot perception tokens in the multi-modal fusion block of a transformer encoder structure.



\subsection{Consistency Policy with DiT Architecture}
\label{Consistency Policy with DiT Architecture}
The iterative sampling process of the diffusion model and the pipeline for obtaining visual semantic masks make real time control of the robot a challenge. Inspired by the excellent performance of consistency policy in image generation, we designed a DiT action head based on consistency strategies and integrated it into an asynchronous framework with visual observation acquisition and action prediction. We first train a teacher model $G_{\phi}$ within the EDM framework. We then concatenate time $t$ and observation condition $o$ to form the keys ($K$) and values ($V$) in the transformer decoder, replacing the FiLM module used in U-Net and input the current action $x_t$. The derivative of the Probability Flow ODE (PFODE) trajectory is estimated using the attention mechanism:

\begin{equation}
dx_t/dt = -(x_t - G_\phi(x_t, t; o))/t
\end{equation}

We use an optimized Denoising Score Matching (DSM) loss to train the EDM model. The DSM is sampled along the PFODE trajectory$(x_t, t)$ and is trained to predict the initial action ground truth $x_0$.

\begin{equation}
L_{\text{DSM}}(\theta) = \mathrm{E}_{t, x_0, x_t \mid x_0} \left[ d(x_0, G_\phi(x_t, t; o)) \right]
\end{equation}

$d$ is an optimized Huber loss:

\begin{equation}
d(x, y) = \sqrt{\|x - y\|_2^2 + c^2} - c
\end{equation}

For the student model $g_\theta(x_t, t, s; o)$, we concatenate the stop time $s$ with the token sequence of time $t$ and observation condition $o$, and then denoise the sampled points $(x_{t_1}, {t_1})$ and $(x_{t_2}, {t_2})$ along the same PFODE trajectory back to the same stop time $s$. We denote the denoised outputs as $x_s^{(t_1)}$ and $x_s^{(t_2)}$, corresponding to $g_\theta(x_{t_1}, t_1, s; o)$ and $g_\theta(x_{t_2}, t_2, s; o)$, respectively. These are then denoised back to the initial time step $t=0$, resulting in $g_\theta(x_s^{(t_1)}, s, 0; o)$ and $g_\theta(x_s^{(t_2)}, s, 0; o)$. The consistency loss is then calculated in the fully denoised action space.



\begin{equation}
L_{\text{CTM}} = d\left(g_\theta(x_s^{(t_1)}, s, 0; o), g_\theta(x_s^{(t_2)}, s, 0; o)\right)
\end{equation}

The final loss calculation combines DSM loss and CTM loss.

\begin{equation}
L_{\text{CP}} = \alpha L_{\text{CTM}} + \beta L_{\text{DSM}}
\end{equation}

% \begin{algorithm}[H]
% \caption{Multi-process Collaboration: Visual Observation, Model Inference, and Action Execution}
% \label{alg:multi_process}
% \begin{algorithmic}[1]
% \STATE \textbf{Given:} Camera device, shared memory or message queue, robot control interface.

% \STATE \textbf{Let:} 
% \STATE $I$ denote image data, $A$ denote action command.
% \STATE $B$ denote bounding box, $S$ denote segmentation mask.
% \STATE $P$ denote robot perception.

% \STATE \textbf{Initialize:} 
% \STATE Process 1: Visual Observation Process
% \STATE Process 2: Model Inference Process
% \STATE Process 3: Action Execution Process

% \STATE \textbf{Initial Detection:}
% \STATE Capture initial image data $I$ from the camera
% \STATE Perform detection to obtain initial bounding box $B$
% \STATE Perform segmentation to obtain initial mask $S$
% \STATE Store $I$ and $S$ in shared memory or message queue

% \WHILE{system running}
% \STATE \textbf{Process 1: Visual Observation Process}
% \STATE Capture image data $I$ from the camera
% \STATE Track bounding box $B$ from previous frame
% \STATE Perform segmentation to obtain segmentation mask $S$ based on tracked bounding box $B$
% \STATE Store $I$ and $S$ in shared memory or message queue

% \STATE \textbf{Process 2: Model Inference Process}
% \STATE Align data from shared memory or message queue
% \STATE Preprocess $P$, $I$, and $S$ (e.g., resize, normalize)
% \STATE Perform inference using the trained model to obtain action command $A$
% \STATE Store $A$ in shared memory or message queue

% \STATE \textbf{Process 3: Action Execution Process}
% \STATE Read action command $A$ from shared memory or message queue
% \STATE Control the robot to execute actions based on $A$
% \ENDWHILE
% \end{algorithmic}
% \end{algorithm}