%具身智能体在复杂场景下 manipulation 的 performance robustness 和泛化能力始终是一个广受关注的研究方向。其中，visuomotor imitation learning 是具身智能体 Policy 的主流范式之一，它允许 agent 从高维视觉观察和机器人本体感知中 effectively 学习 manipulation skills。
%然而，增加场景的复杂度和 visual distraction，会导致在简单场景下表现良好的决策模型性能下降。实际上，不仅是 simple imitation learning policy，先进的多模态 foundation models such as GPT-4o 或 vision language action models (VLA)，也不能很好地关注一张语义丰富的图片中的特定的局部问题。对于 robot control or 多模态大模型，其往往侧重于 action prediction, observation mapping or 多模态 alignment，而缺少直观的视觉感知增强。模型需要隐性地或遵循 high-level text instruction 从相关的视觉区域中获得面向任务语义的定位知识。
%To tackle this challenge problem, we introduce Imit Diff, a diffusion transformer imitation learning framework with dual resolution enhancement guided by fine-grained semantics information。具体来说，our work 有三个关键组成部分。
%1) Semanstic Injection. Imit Diff 通过 vision language models (VLM) 和 vision foundation models 的 pretrain knowledge 将面向任务的语义信息和高层文本指导转化为显式的 pixel-level 视觉定位标签，注入到 environment observation中。
%2) Dual Res Fusion。 我们构建了双分辨率图像观测流，使用双分辨率视觉编码器分别提取全局和细粒度视觉特征。多尺度视觉信息随后在 attention block 中进行融合，在保证计算 effiency 的前提下，为全局视觉观测引入多尺度细粒度信息，提升场景理解能力。
%3) Consistency policy on diffusion transformer。Diffusion based imitation policies 通常受到 denoise times 的困扰。我们建立了基于 consistency policy 的 DiT action head。Policy 的决策层可以通过 single step denoise 实现系统高频响应。额外地，受益于较快的 inference time，我们引入 temperal ensemble 改善预测动作的平滑性。
%我们设计了四个在 manipulation 精细度上具有挑战性的现实世界任务来评估 Imit Diff，并通过增加场景复杂度和 visual distraction 来测试模型的场景理解能力。额外地，我们设计了 visual distraction 和 category generalization 的 zero shot 实验来验证模型是否受益于 dual res enhancement framework and fine-grained semantics injection。实验结果表明，Imit Diff outperforms 现有的 strong baselines。
%In summary, the contributions of our work are three-fold:
%1) We propose Imit Diff, a DiT architecture imitation learning framework with dual res enhancement guied by fine-grained semantics information.
%2) 我们构建了 open-set vision foundation models pipeline 来获得显式视觉遮罩。该方法能够有效处理机器人控制场景的运动模糊、遮挡、物体丢失情况。并将其作为 fine-grained 语义信息引导 policy decision。
%3) 我们在DiT上实现了consistency policy，显著减少了模型推理时间。通过异步控制框架，实现了 open-set vision foundation models 工作流下的实时控制。
%The code will be publicly available soon。

\section{Introduction}


\label{Intro}
The performance robustness and generalization capabilities of embodied agents in complex manipulation scenarios have long been a focus of significant research interest \citep{ju2025robo, yuan2024learning}. Visuomotor imitation learning is one of the mainstream paradigms of robot manipulation policy \citep{chi2023diffusion, shridhar2023perceiver, ze2023gnfactor, florence2022implicit, hansen2022pre}. This approach enables agents to derive state estimation and decision-making capabilities from expert demonstrations that incorporate high-dimensional visual observations and robot proprioception \citep{ze20243d}.

However, as scene complexity and visual distractions increase, the performance of decision models that excel in simpler environments tends to degrade \citep{zheng2024instruction, liurobustness}. Not only do simple imitation learning policies face challenges, but even advanced multimodal foundation models, such as GPT-4o \citep{hurst2024gpt} or vision language action models (VLA) \citep{liu2024rdt, brohan2022rt, brohan2023rt, o2023open, kim2024openvla, wen2024diffusion}, struggle to accurately focus on specific details within semantically complex images. In fact, in robot control and embodied multimodal foundation models, the focus is often on action prediction, observation mapping, or multimodal alignment. Therefore, intuitive visual perception enhancement is typically lacking. Models can only acquire task-oriented semantic localization knowledge from relevant visual regions either implicitly or when guided by high-level text instructions \citep{reuss2023multimodal}.

To tackle this challenge problem, we introduce \textbf{Imit Diff}, a diffusion transformer imitation learning framework with dual resolution enhancement guided by fine-grained semantics information. Specifically, our work has three key components:

\begin{enumerate}

\item \textbf{Semanstic injection.} Imit Diff transforms task-oriented semantic information and high-level textual guidance into explicit pixel-level visual localization labels through the pretrain knowledge of vision language models (VLM) and vision foundation models, and injects them into the policy observation.

\item \textbf{Dual resolution (dual res) fusion.} We develop a dual res image observation stream and employed a dual res vision encoder to extract global and fine-grained visual features. The extracted multi-scale visual information is subsequently fused within an attention block, integrating fine-grained details into the global visual feature. This approach enhances scene understanding while maintaining computational efficiency.

\item \textbf{Consistency policy on diffusion transformer (DiT).} Diffusion-based imitation policies often suffer from inefficiencies due to the required denoising steps. To address this, we design a DiT \citep{peebles2023scalable} action head incorporating a consistency policy \citep{song2023consistency}, enabling the decision layer to achieve high-frequency system responses through single-step denoising. Furthermore, leveraging faster inference times, we introduce temperal ensemble to enhance the smoothness of predicted actions.

\end{enumerate}

We design four real-world tasks with challenging manipulation precision to evaluate Imit Diff and test the model's scene understanding capabilities by introducing increased scene complexity and visual distractions. Additionally, we conducted zero-shot experiments on visual distraction and category generalization to assess the benefits of the dual res enhancement framework and fine-grained semantic injection. Experimental results demonstrate that Imit Diff significantly outperforms existing strong baselines. 

In summary, the contributions of our work are three-fold:

\begin{enumerate}

\item We propose Imit Diff, a DiT architecture imitation learning framework with dual res enhancement guied by fine-grained semantics information.

\item We developed an open-set vision foundation model pipeline to generate explicit visual masks. This approach effectively addresses challenges such as motion blur, occlusion, and object loss in robot control scenarios, leveraging the generated masks as fine-grained semantic information to guide policy decisions.

\item We implemented a consistency policy on DiT, which significantly reduced the model inference time. Through the asynchronous control framework, we achieved real-time control under the workflow of open-set vision foundation models.

\end{enumerate}

The code will be made publicly available soon.