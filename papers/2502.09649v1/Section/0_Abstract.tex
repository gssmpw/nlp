%Visuomotor Imitation Learning 允许具身智能体从 video demonstrations 和 robot proprioception 中 effectively 学习 manipulation skills。然而，增加场景的复杂度和 visual distraction 会导致在简单场景下表现良好的现有方法性能下降。这推动了我们提出 Imit Diff，a semantics guided diffusion transformer with dual resolution fusion for imitation learning。Our approach 使用 vision language foundation models 的先验知识将任务的高级语义监督转化为像素级定位信息，显式注入到一种双分辨率编码器构造的多尺度视觉特征增强框架中。我们在若干高难度的现实世界任务中对 Imit Diff 进行了验证。受益于面向任务的视觉定位和细粒度场景感知能力，Imit Diff significantly outperforms state-of-the-art methods，especially 在具有视觉干扰的复杂场景中，包括 visual distraction 和 category generalization 的泛化实验。额外地，我们提出了 Consistency Policies 在 diffusion transformer 中的实现方法，拓展了具身智能提在控制过程中的实时性和运动平稳性。 The code will be publicly available。


\begin{abstract}



\label{abstract}Visuomotor imitation learning enables embodied agents to effectively acquire manipulation skills from video demonstrations and robot proprioception. However, as scene complexity and visual distractions increase, existing methods that perform well in simple scenes tend to degrade in performance. To address this challenge, we introduce Imit Diff, a semanstic guided diffusion transformer with dual resolution fusion for imitation learning. Our approach leverages prior knowledge from vision language foundation models to translate high-level semantic instruction into pixel-level visual localization. This information is explicitly integrated into a multi-scale visual enhancement framework, constructed with a dual resolution encoder. Additionally, we introduce an implementation of Consistency Policy within the diffusion transformer architecture to improve both real-time performance and motion smoothness in embodied agent control.We evaluate Imit Diff on several challenging real-world tasks. Due to its task-oriented visual localization and fine-grained scene perception, it significantly outperforms state-of-the-art methods, especially in complex scenes with visual distractions, including zero-shot experiments focused on visual distraction and category generalization. The code will be made publicly available.

\end{abstract}





