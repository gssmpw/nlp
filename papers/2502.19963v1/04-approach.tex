\section{Exploiting Partial Truth Assignments in OMT}%
\label{sec:approach}
\begin{algorithm}[t]
    \newcommand{\res}{\textsf{res}}
    \begin{algorithmic}[1]
        %\begin{rschange}
        \caption[A]{{\sc Linear-search OMT with partial assignments}($\vi, \obj$)\\
            \hspace*{\algorithmicindent}\textbf{Input}:
            Formula $\vi$, objective $\obj$\\
            \hspace*{\algorithmicindent}\textbf{Output}: $\satres/\unsatres$, optimum model $\M$}%
        \label{alg:omt-partial}
        \STATE \makebox[.5cm][c]{$\M$}$\gets \emptyset$ \algorithmiccomment{Best model found so far}
        \STATE \makebox[.5cm][c]{$\ub$}$\gets \infty$ \algorithmiccomment{Current upper bound}
        \STATE \makebox[.5cm][c]{$\res$}$\gets \satres$ \algorithmiccomment{Status of the search}
        \WHILE{$\res = \satres$}
        \STATE $\tuple{\res,\eta} \gets \incrementalsmt(\vi\wedge(\obj<\ub))$
        \IF{$\res = \satres$}
        \STATE\label{alg:line:omt-partial:minimize} \makebox[.5cm][c]{$\color{blue}\mu$}$\color{blue}\gets\omtminimizeassignment(\vi,\eta,\obj)$
        \STATE \makebox[.5cm][c]{$\M$}$\gets \minimize(\mu,\obj)$
        \STATE \makebox[.5cm][c]{$\ub$}$\gets \M(\obj)$
        \ENDIF
        \ENDWHILE
        \IF{$\M = \emptyset$}
        \RETURN $\tuple{\unsatres,\emptyset}$
        \ELSE
        \RETURN $\tuple{\satres,\M}$
        \ENDIF
    \end{algorithmic}
\end{algorithm}

The general schema of our approach is presented in~\Cref{alg:omt-partial}. This
algorithm is a variant of the basic \omt{} linear-search
schema~\cite{sebastianiOptimizationSMTLAQ2012,sebastianiOptimizationModuloTheories2015}
described in~\sref{sec:bg:omt}. The main difference is the call to the
$\omtminimizeassignment$ procedure (line~\ref{alg:line:omt-partial:minimize}),
which is responsible for reducing the truth assignment to be fed to the
\T-minimizer, provided that the resulting partial truth assignment still
propositionally satisfies the formula. Depending on the implementation of this
procedure, the assignment-reduction strategy can be more or less effective in
improving the search for the global optimum.

In \sref{sec:approach:basic-assignment-minimization} and
\sref{sec:approach:guided-assignment-minimization}, we describe two possible
implementations of this procedure.

% \TODO{Say something about correctness of the approach?}
% \TODO{Say something about relation with pure-literal filtering?}

\subsection{Basic Assignment Reduction}%
\label{sec:approach:basic-assignment-minimization}

The first approach is to reduce the truth assignment using~\Cref{alg:minimize}
in~\sref{sec:bg:partial-truth-assignments}, i.e., iterating over all the
literals in the current truth assignment $\eta$, and dropping them one by one,
if possible.
%This approach is simple and general, but it may not be effective in practice, as it does not take into account the properties of the \omt{} search strategy.
A straightforward improvement is to only try to drop \T-literals, since they
are the ones that, if dropped, can potentially enlarge the area within which
the optimum \T-model is searched. This procedure is simple and general, and
comes with a limited overhead, as each truth assignment is scanned only once to
find the literals to drop, and the \T-minimizer is called only once for each
candidate assignment.

This approach, however, might not be very effective in practice, as it ``blindly''
removes literals from the truth assignment without taking into account the
properties of the \omt{} search strategy. In particular, it may drop literals that
are not relevant for the optimization, enlarging the search area in the wrong
direction, possibly preventing from dropping other literals that are more
relevant.%does not take

\subsection{OMT-Guided Assignment Reduction}%
\label{sec:approach:guided-assignment-minimization}
%The main limitation of the previous approach is that the heuristics used to choose the atoms to leave unassigned do not take into account the properties of the \omt{} search strategy. 
We propose an ad-hoc assignment-reduction technique for \omt{} solving, which
is outlined in~\Cref{alg:omt-minimize-assignment-guided}.
%
%address these points.
%investigate \emph{on-demand} truth assignment minimization.
% By this term, we mean that, once a \larat-satisfiable satisfying (partial) truth assignment has been found, and the \larat-minimizer has found a minimal model within the corresponding area, then we can try to remove just some of the \larat-atoms and look if a better model exists within the new enlarged area. Moreover, since both the constraints and the cost function are linear, by the simplex method
% the optimum model lies on a vertex of the polytope. As an heuristic, we can choose to drop one of the constraints that form the vertex, since a better model is likely to be found in that portion of space. 
%
%
Suppose that, after the \T-minimizer has found a minimum model within the
current truth assignment $\mu$
(line~\ref{alg:omt-minimize-assignment-guided:line:minimize1}), it returns also
one (or more) literal(s) that limit the current minimum
(line~\ref{alg:omt-minimize-assignment-guided:line:propose1}). These literals
are part of some (possibly minimal) $\muprime\subseteq\mu$ such that
$\muprime\cup\set{\obj<\M(\obj)}$ is \T-unsatisfiable. Intuitively, the removal
of any literal $\ell\in\muprime$ is very likely to lead to a better optimum model,
provided that $\mu\setminus\set{\ell}$ still propositionally satisfies $\vi$
(line~\ref{alg:omt-minimize-assignment-guided:line:ifcandrop}).%the list of the literals%This corresponds to a minimal 

%these literals, if dropped, are likely
%to lead to a better optimum model. 
We can then iteratively drop these literals and re-run the \T-minimizer, until
no more literals can be dropped
(lines~\ref{alg:omt-minimize-assignment-guided:line:while}--\ref{alg:omt-minimize-assignment-guided:line:propose2}).
\begin{algorithm}[t]
    \begin{algorithmic}[1]
        %\begin{rschange}
        \caption[A]{\omtminimizeassignmentguided($\vi, \eta$, $\obj$)\\
            \hspace*{\algorithmicindent}\textbf{Input}:
            Formula $\vi$, \T-satisfiable total truth assignment $\eta$ satisfying $\vi$, objective $\obj$\\
            \hspace*{\algorithmicindent}\textbf{Output}: Reduced truth assignment $\mu\subseteq\eta$ satisfying $\vi$}%
        \label{alg:omt-minimize-assignment-guided}
        \STATE\makebox[.5cm][c]{$\mu$}$\gets\eta$
        \STATE\makebox[.5cm][c]{$\M$}$\gets\minimize(\mu,\obj)$\label{alg:omt-minimize-assignment-guided:line:minimize1}
        \STATE\makebox[.5cm][c]{$\ell$}$\gets\proposelit()$\label{alg:omt-minimize-assignment-guided:line:propose1}
        \WHILE{$\ell\neq\bot$}\label{alg:omt-minimize-assignment-guided:line:while}
        \IF{$\mu\setminus\set{\ell}$ satisfies all clauses in $\vi$}\label{alg:omt-minimize-assignment-guided:line:ifcandrop}
        \STATE \makebox[.5cm][c]{$\mu$}$\gets \mu \setminus \set{\ell}$\label{alg:omt-minimize-assignment-guided:line:drop}
        %\STATE $\tpop(\ell)$\label{alg:omt-minimize-assignment-guided:line:tpop}
        \STATE \makebox[.5cm][c]{$\M$}$\gets\minimize(\mu,\obj)$\label{alg:omt-minimize-assignment-guided:line:minimize2}
        \ENDIF
        \STATE $\ell\gets\proposelit()$\label{alg:omt-minimize-assignment-guided:line:propose2}
        \ENDWHILE
        \RETURN $\mu$
    \end{algorithmic}
\end{algorithm}

We describe a possible implementation of the $\proposelit$ procedure
in~\Cref{alg:omt-minimize-assignment-guided} for the case of \omlarat{}.
%
As we have seen in~\sref{sec:bg:omt}, a \larat{}-minimizer can be implemented
as a variant of the Simplex
method~\cite{dutertreFastLinearArithmeticSolver2006,sebastianiOptimizationSMTLAQ2012},
by which an optimum model is always found on a vertex of the polytope defined
by the conjunction of \larat-constraints on which it is invoked. Thus, in this
case, the candidate constraints to be dropped are those that form such vertex.
This information can be easily obtained from the Simplex tableau~\cite{dutertreFastLinearArithmeticSolver2006}.
%  \TODO{Find a way to easily explain this.\\
%  The tableau stores
%  $x_i = bi + \sum_{x_j\in\N} a_{i,j}x_j$, forall $x_i\in\B$\\
%  and then it keeps track of the bounds for each variable $l_i\leq x_i\leq u_i$\\
%  Basically, we look for $x_i\in\N$ such that\\
%  $\beta[x_i] = u_i$ and $a_{\obj,i} < 0$\\
%  $\beta[x_i] = l_i$ and $a_{\obj,i} > 0$\\
%  where $\beta[x_i]$ is the current value of $x_i$.\\
%  For each such $x_i$, we get the constraint imposing the
%  bound.
%  }

For other theories, the implementation of the $\proposelit$ procedure may be
more complex, requiring the extraction of a (possibly minimal) conflict set of $\mu\cup\set{\obj<\M(\obj)}$.
In general, also heuristic strategies can be used, as they only
provide suggestions to the assignment-reduction procedure, and do not affect
the correctness of the search.

Regarding the computational cost, the proposed approach can be more expensive than
the basic assignment reduction, as it requires the \T-minimizer to be called
multiple times. \T-minimizers, however, are typically designed to be called
incrementally, maintaining the state of the previous calls, and thus the
overhead of multiple calls is limited.
% \GMSIDENOTE{Remove or expand?}
% Another possible source of complexity is that the incrementality of \T-minimizer is based on a stack interface, whereas
% \omtminimizeassignmentguided{} requires to drop any literal in the assignment,
% which may require a more complex data structure to be implemented efficiently.