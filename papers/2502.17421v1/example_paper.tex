%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{comment}
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{multirow} 
\usepackage{booktabs} % for professional tables
\usepackage{adjustbox}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{url}
\usepackage{makecell}
\usepackage{array}
% \usepackage{subfigure}
\usepackage{wrapfig}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{soul}
\usepackage{subfig}
\usepackage[table]{xcolor}
% \usepackage{inconsolata}
\usepackage[figuresright]{rotating} 
% \usepackage[svgnames]{xcolor}
\usepackage{tcolorbox}
\definecolor{myblue}{HTML}{F0F8FF} 
\definecolor{mypurple}{HTML}{E6E6FA} 
\definecolor{mygreen}{HTML}{F0FFF0} 
\definecolor{mypink}{HTML}{FFF0F5} 
\newcommand{\fengzhuo}[1]{{\color{red}  [\mathrm{Fengzhuo:} #1]}}
\newcommand{\cunxiao}[1]{{\color{magenta}  [\mathrm{Cunxiao:} #1]}}
\newcommand{\penghui}[1]{{\color{blue}  [\mathrm{Penghui:} #1]}}

\newcommand{\att}{\mathsf{attn}}
\newcommand{\mha}{\mathsf{mha}}
\newcommand{\pe}{\mathsf{RoPE}}
\newcommand{\tilmha}{\widetilde{\mathsf{mha}}}
\newcommand{\ff}{\mathsf{ffn}}
\newcommand{\sm}{\mathsf{softmax}}
\newcommand{\lnor}{\mathsf{LN}}
\newcommand{\llm}{\mathsf{transformer}}
\newcommand{\pt}{\texttt{prompt}}
\newcommand{\sink}{\mathsf{lazy}}
\newcommand{\id}{\mathsf{Id}}
\newcommand{\unemb}{\mathsf{unemb}}
\newcommand{\lip}{{\mathsf{lip}}}
\newcommand{\test}{\texttt{test}}
\newcommand{\argmin}{\text{argmin}}
\usepackage{acronym}
\acrodef{mha}[MHA]{Multi-Head Attention}
\acrodef{ff}[FF]{Feed-Forward}
\acrodef{mle}[MLE]{Maximum Likelihood Estimate}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\input{preamble}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\DeclareRobustCommand{\mylogo}{\adjustbox{valign=c}{\includegraphics[width=1.2cm]{Figure/logo.pdf}}}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{LongSpec: Long-Context Speculative Decoding with Efficient Drafting and Verification}

\begin{document}

\twocolumn[
% \icmltitle{LongSpec: Memory-efficient Draft Model with Parallel Tree Verification \\
%            for Lossless Long-Context Speculative Decoding}
% \icmltitle{LongSpec: Memory-Efficient Long-Context Speculative Decoding \\
%            with Sink Position ID and Aggregated Attention}
% \icmltitle{\textsc{LongSpec}: Long-Context Speculative Decoding \\
%            with Efficient Drafting and Verification}

\icmltitle{
  \texorpdfstring{
    \mylogo~~\LARGE \textsc{LongSpec}:~~~~~~~~~~~~~~~\\ \Large Long-Context Speculative Decoding with Efficient Drafting and Verification
  }{
    LongSpec: Long-Context Speculative Decoding with Efficient Drafting and Verification
  }
}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Penghui Yang}{equal,2}
\icmlauthor{Cunxiao Du}{equal,1}
\icmlauthor{Fengzhuo Zhang}{3}
\icmlauthor{Haonan Wang}{3}
\icmlauthor{Tianyu Pang}{1}
\icmlauthor{Chao Du}{1}
\icmlauthor{Bo An}{2}
\end{icmlauthorlist}

\icmlaffiliation{1}{Sea AI Lab, Singapore}
\icmlaffiliation{2}{Nanyang Technological University}
\icmlaffiliation{3}{National University of Singapore}

\icmlcorrespondingauthor{Penghui Yang}{phyang.cs@gmail.com}
\icmlcorrespondingauthor{Cunxiao Du}{ducx@sea.com}
\icmlcorrespondingauthor{Fengzhuo Zhang}{fzzhang@u.nus.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

% \printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.
\printAffiliationsAndNoticeArxiv{\icmlEqualContribution}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} 
\begin{abstract}
Speculative decoding has become a promising technique to mitigate the high inference latency of autoregressive decoding in Large Language Models (LLMs). Despite its promise, the effective application of speculative decoding in LLMs still confronts three key challenges: the increasing memory demands of the draft model, the distribution shift between the short-training corpora and long-context inference, and inefficiencies in attention implementation. 
In this work, we enhance the performance of speculative decoding in long-context settings by addressing these challenges. First, we propose a memory-efficient draft model with a constant-sized Key-Value (KV) cache. Second, we introduce novel position indices for short-training data, enabling seamless adaptation from short-context training to long-context inference. 
Finally, we present an innovative attention aggregation method that combines fast implementations for prefix computation with standard attention for tree mask handling, effectively resolving the latency and memory inefficiencies of tree decoding.
Our approach achieves strong results on various long-context tasks, including repository-level code completion, long-context summarization, and o1-like long reasoning tasks, demonstrating significant improvements in latency reduction.
The code is available at \url{https://github.com/sail-sg/LongSpec}.
\end{abstract}

\input{chapters/1_introduction}

\input{chapters/2_related_work}

% \input{chapters/2_prelim}
% \section{Preliminary}
% \subsection{Problem Formulation}
% \subsection{Motivation}

% The ability to handle long-context inputs has become increasingly critical because of applications such as long-document summarization and repository-level code completion. Speculative decoding, as an important technique for accelerating LLM inference, exhibits great potential for addressing the computational challenges in long-context scenarios. However, current speculative decoding methods in these scenarios only use draft models that do not require additional training. While this approach provides simplicity, it inherently limits their effectiveness and adaptability to diverse tasks.

% Despite their success, state-of-the-art speculative decoding methods designed for short-context scenarios are faced with several critical limitations when applied to long-context scenarios.

% \begin{itemize}

% \item \textbf{ Growth in Memory Requirements.} As the decoding length increases, current methods require progressively larger KV caches. This introduces significant storage overhead, especially in long-context settings where memory efficiency is of great importance. Addressing this issue demands an innovative approach to ensure constant memory usage, irrespective of context length.

% \item \textbf{Dependence on Predefined Model Structures.} The draft models used by these methods rely heavily on information from the target model, including its rotary position embeddings (RoPE). Since the target model's RoPE base is fixed, the draft model cannot be directly trained on short texts with a small RoPE base, as is common in long-context scenarios. This limitation hinders the draft model's ability to generalize from short-context training to long-context applications. A novel mechanism is needed to bridge this gap without altering the RoPE base.

% \item \textbf{Incompatibility with Flash Attention.} Current SOTA techniques often employ tree decoding, a method incompatible with Flash Attention. Flash Attention is a widely adopted acceleration mechanism critical for reducing latency and memory usage during inference. The inability to integrate tree decoding with Flash Attention exacerbates latency and memory challenges, particularly in long-context scenarios.

% \end{itemize}

\input{chapters/3_method}

\input{chapters/4_expr}

\section{Conclusion}

In this paper, we propose \textsc{LongSpec}, a novel framework designed to enhance speculative decoding for long-context scenarios. Unlike previous speculative decoding methods that primarily focus on short-context settings, \textsc{LongSpec} directly addresses three key challenges: excessive memory overhead, inadequate training for large position indices, and inefficient tree attention computation. To mitigate memory constraints, we introduce an efficient draft model architecture that maintains a constant memory footprint by leveraging a combination of sliding window self-attention and cache-free cross-attention. To resolve the training limitations associated with short context data, we propose the Anchor-Offset Indices, ensuring that large positional indices are sufficiently trained even within short-sequence datasets. Finally, we introduce Hybrid Tree Attention, 
which efficiently integrates tree-based speculative decoding with \texttt{Flash\_Decoding}. Extensive experiments demonstrate the effectiveness of \textsc{LongSpec} in long-context understanding tasks and real-world long reasoning tasks. Our findings highlight the importance of designing speculative decoding methods specifically tailored for long-context settings and pave the way for future research in efficient large-scale language model inference.

\section*{Impact Statements}
This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\input{chapters/appendix_0_proof}

\input{chapters/appendix_2_training_details}

\input{chapters/theory.tex}

\input{chapters/appendix_1_visualize}

\end{document}

