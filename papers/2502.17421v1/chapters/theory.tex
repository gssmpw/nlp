\section{Notation}
For a positive integer $N\in\bbN$, we define the set $[N]=\{1,\cdots,N\}$. For a vector $x\in\bbR^{d}$, we adopt $\|\cdot\|_{p}$ to denote the $\ell_{p}$ norm of vectors. For a matrix $X=[x_{1}^{\top},\cdots,x_{d_{1}}^{\top}]^{\top}\in\bbR^{d_{1}\times d_{2}}$, where $x_{i}\in\bbR^{d_{2}}$ for $i=1,\cdots,d_{1}$, we define the $\ell_{p,q}$-norm of $X$ as $\|X\|_{p,q}=\|[\|x_{1}\|_{p},\cdots,\|x_{d_{1}}\|_{p}]\|_{q}$ The Frobenius norm $\|\cdot\|_{2,2}$ is denoted as $\|\cdot\|_{\rmF}$. We write $x\lesssim y$ to mean $x\leq Cy$ for an absolute constant $C>0$. 

\section{Theoretical Analysis}\label{app:theory}
In this section, we provide the theoretical analysis of our methods. We begin with the definition of our Glide network. It consists of three modules: the self-attention module, the cross-attention module, and the \ac{ff} module. Here the self-attention and the cross-attention are both based on the attention module. For a query $q\in\bbR^{1\times d}$ and $N$ KV pairs $K,V\in\bbR^{N\times d}$, the attention module calculate the output as 
\begin{align}
    \att(q,K,V, \{W_Q,W_K,W_V\}) = \sm\Big(\pe\big(\lnor(q) W_Q \big) \pe\big(\lnor(K) W_K\big)^{\top}\Big)V W_{V},\label{eq:att}
\end{align}
where $\sm$ is the softmax operator, $W_Q,W_K,W_V\in\bbR^{d\times d}$ are the weight matrices, $\pe$ is the rotary positional embedding function that appiles RoPE on inputs, and $\lnor$ is the row-wise normalization of the input, which is defined as
\begin{align*}
    \lnor(x)= 
    \left\{
\begin{array}{rcl}
x       &      & {\text{if } \|x\|_{2}\leq 1}\\
x/\|x\|_{2}    &      & {\text{otherwise}}.
\end{array} \right.
\end{align*}
Compared to the implementation of attention in PyTorch, we merge the weight matrix $W_O$ into $W_V$ here for ease of notation. Our results can be easily generalized to the formulation that explicitly parameterizes $W_O$. The \ac{mha} with $H$ heads is the combination of $H$ attention, \emph{i.e.}, 
\begin{align}
    \mha(q,K,V,\{W_Q^{h},W_K^{h},W_V^{h}\}_{h=1}^{H}) =  \sum_{h=1}^{H}\sm\big(\pe(\lnor(q) W_Q^{h}) \pe(\lnor(K) W_K^{h})^{\top}\big)V W_{V}^{h},\label{eq:mha}
\end{align}
where $\{W_Q^{h},W_K^{h},W_V^{h}\}_{h=1}^{H}$ are the weights of all the attention heads. The self attention module generates the output for a query $x$ according to the KV pairs that include $x$ and other tokens, \emph{i.e.}, $K,V$ are defined as 
\begin{align*}
    K = V = [x_{1}^{\top},\cdots,x_{T}^{\top},x^{\top}]^{\top}=[X_{T}^{\top},x^{\top}]^{\top},
\end{align*}
where $x_{i}\in\bbR^{1\times d}$ for $i\in[N]$ are the input vectors prior to $x$. In contrast, the cross-attention generates outputs for $x$ according to the KV pairs that excludes $x$, \emph{i.e.}, $K=V=X_N^{\prime}$. The \ac{ff} module process an input vector $x\in\bbR^{d}$ as
\begin{align}
    \ff(x,W_{A,1},W_{A,2})=\sigma\big(\lnor(x)W_{A,1}\big)W_{A,2},
\end{align}
where $W_{A,1},W_{A,2}\in\bbR^{d\times d}$ are weights of \ac{ff} module, and $\sigma(\cdot)$ is an element-wise activation function. For example, the activation function $\sigma$ can be ReLU and sigmoid. In the following, we will omit the parameters of each module for ease of notation. The Glide function, denoted as $G_\theta$, is defined as
\begin{align}
    G_{\theta}(q,X,X^{\prime}) = \ff\Big( \mha\big(\mha(q,[X^{\top},q^{\top}]^{\top}, [X^{\top},q^{\top}]^{\top}), X^{\prime},X^{\prime}\big)\Big)W_{\unemb},
\end{align}
where $X$ is the embeddings of \emph{all} the tokens prior to $q$, $X^{\prime}$ are the hidden states of large models, $W_{\unemb}\in\bbR^{d\times d_V}$ is the unembedding matrix ($d_V$ is the alphabet size of the tokens), and $\theta$ denotes the parameters of all these three modules and $W_{\unemb}$, \emph{i.e.},
\begin{align*}
    \theta= \big(\{W_Q^{h,(1)},W_K^{h,(1)},W_V^{h,(1)}\}_{h=1}^{H},\{W_Q^{h,(2)},W_K^{h,,(2)},W_V^{h,,(2)}\}_{h=1}^{H},W_{A,1},W_{A,2},W_{\unemb}\big).
\end{align*}
Here the superscripts $(1)$ and $(2)$ denote the index of the layer. We denote all the plausible parameter configurations as $\Theta$ as
\begin{align*}
    \Theta = \Big\{ \theta \,\Big|\, \max_{h,i}\big\{\|W_{Q}^{h,(i)}\|_{\rmF},\|W_{K}^{h,(i)}\|_{\rmF},\|W_{V}^{h,(i)}\|_{\rmF}\big\}\leq B, \| W_{A,1}\|_{\rmF}\leq B,  \| W_{A,2}\|_{\rmF}\leq B, \|W_{\unemb}\|_{1,2}\leq B \Big\}.
\end{align*}

In the following, we would like to study the error analysis of our method. In fact, we need to define a variant of this function, which contains two modifications. The first modification is the positional embedding. Instead of using the positional embeddings corresponding to the continuous positions, we \emph{offset} the positions of the tokens with position index larger than $4$ jointly to $t\in\bbN$. We denote such positional embedding with $\pe^{\rms,t}$. The corresponding $\mha$ is denoted as $\mha^{\rms,t}$. Here we note that $\mha=\mha^{\rms,0}$, \emph{i.e.}, there is no position offset in the original attention module.
The second modification is that we truncate $X$ to a sliding window, which is detailed discussed in Section~\ref{subsec: arch}. We denote the truncated version of $X$ as $X^{\rmw}$. Then our proposed training method is to train the following function
\begin{align}
    G_{\theta}^{\rms,t}(q,X^{\rmw},X^{\prime}) = \ff\Big( \mha^{\rms,t}\big(\mha^{\rms,t}(q,[X^{\rmw,\top},q^{\top}]^{\top}, [X^{\rmw,\top},q^{\top}]^{\top}), X^{\prime},X^{\prime}\big)\Big)W_{\unemb}. \label{eq:g_train}
\end{align}
We assume that the glide function is trained on a dataset with $N$ i.i.d.\ samples of a distribution $P$, \emph{i.e.}, $\calD_{N}=\{q_i,X_i,X_{i}^{\prime}, \id_{i}^{*}\}_{i=1}^{N}\sim P$, where $\id_{i}^{*}$ is the vocabulary index of true next token for $i$-th sample. During the training process, the position offsets $\calD_N^{t}=\{t_i\}_{i=1}^{N}$ are i.i.d.\ samples of a distribution $\tilde{P}_{t}$. Then we define the Maximum Likelihood Estimate (MLE) $\htheta$ as 
\begin{align*}
    \htheta &= \argmin_{\theta\in\Theta}  -\bbE_{\calD_N,\calD_{N}^{t}}\Big[\log \sm\big( G_{\theta}^{\rms,t}(q,X^{\rmw},X^{\prime})\big)_{\id^{*}}\Big] \nonumber\\
    &=\argmin_{\theta\in\Theta} -\frac{1}{N}\sum_{i=1}^{N}\log \sm\big( G_{\theta}^{\rms,t_i}(q_{i},X_{i}^{\rmw},X_{i}^{\prime})\big)_{\id_{i}^{*}},
\end{align*}
where $\bbE_{\calD}$ denotes the expectation with respect to the empirical distribution induced by $\calD_{N}$, and $\tilde{P}_{t}$ is the distribution of the position offset. After the training process, we will inference according to the following function
\begin{align*}
    G_{\htheta}(q,X^{\rmw},X^{\prime}) = \ff\Big( \mha\big(\mha(q,[X^{\rmw,\top},q^{\top}]^{\top}, [X^{\rmw,\top},q^{\top}]^{\top}), X^{\prime},X^{\prime}\big)\Big)W_{\unemb},
\end{align*}
\emph{i.e.}, we do not delibrately shfit the token positions in the inference. To analyze the performance loss due to our method, we then define the \emph{optimal} parameters of the original Glide function. Here optimality means that we have \emph{infinite} number of training data points, \emph{i.e.}, the expectation is taken with respect to the true distribution $P$ instead of the empirical distribution induced by the dataset.
\begin{align*}
    \theta^{*} &= \argmin_{\theta\in\Theta} -\bbE_{P}\Big[\log \sm\big( G_{\theta}(q,X,X^{\prime})\big)_{\id^{*}}\Big].
\end{align*}

\begin{assumption}[Concentration of Window Attention Score]\label{assump:window}
    For any $(q,X)$ on the support of $P$ and the optimal parameter $\theta^{*}$, the sum of the attention scores of the tokens in $X$ that are not included in $X^{\rmw}$ is upper bounded by $\varepsilon$ at the first layer.
\end{assumption}
Intuitively, we note that this assumption largely holds when the answer for the query is included in the window we use. It means that the glide function does not need to gather information outside the window.
\begin{assumption}[Boundness of Inputs]\label{assump:bound}
    For any $(q,X,X^{\prime})$ on the support of $P$, we have that $\|q\|_{2}\leq B_X$, $\|X\|_{2,\infty}\leq B_X$, and $\|X^{\prime}\|_{2,\infty}\leq B_X$.
\end{assumption}
This assumption always holds in the realistic setting, since all the inputs are stored in the computer, which can only represent finite numbers. For the prompt distribution $P$, we denote the distribution of the position index $t^{\rmW}$ of the starting token in the window as $P_t$. 
\begin{assumption}
    The content in the window $X^{\rmw}$ is independent of its starting index $t^{\rmw}$ for distribution $P$.
\end{assumption}\label{assump:indep}
This assumption states that the contents and its absolute position in the context are independent. This generally holds in the long context, since the position of a sentence can hardly imply the content of this sentence in the long context.
\begin{theorem}\label{thm:main}
    When the glide function is trained on a dataset with $N$ i.i.d.\ samples, under Assumptions~\ref{assump:window}, \ref{assump:bound}, and \ref{assump:indep}, the gap between the population inference loss of the MLE from our training method and that of the optimal parameter can be upper bounded as
    \begin{align*}
        &\bbE_{P}\Big[\log \sm\big( G_{\theta^{*}}(q,X,X^{\prime})\big)_{\id^{*}}\Big]- \bbE_{P}\Big[\log \sm\big( G_{\htheta}(q,X^{\rmw},X^{\prime})\big)_{\id^{*}}\Big]\nonumber\\
        &\quad \lesssim \underbrace{(1+d_V\exp(B))HB^{4}(1+B_{X}^2B^2)\varepsilon}_{\text{Err. From Using Windows}} + \underbrace{\log (1+d_V\exp(B))\cdot \tv(\tilP_t,P_t)}_{\text{Positional Embedding Distribution Shift}}\nonumber\\
        &\quad\qquad + \underbrace{\frac{1}{\sqrt{N}}\Big[d(d+d_V)\log (1+NHB^6)+\log\frac{1}{\delta}\Big]}_{\text{Generalization Err.}}
    \end{align*}
    with probability at least $1-\delta$.
\end{theorem}
Here we can see that the error consists of three components. The first one results from that we adopt a sliding window instead of using all tokens. This error is proportional to the attention score outside the window. The second one results from the positinoal embedding distributional shift. In our experiments, we set $\tilde{P}_{t}$ as the uniform distribution, which will have smaller distribution shift than not adopting this position offse technique. The last term results from that we use $N$ samples to train the model.
\begin{proof}[Proof of Theorem~\ref{thm:main}]
    The proof takes three steps.
    \begin{itemize}
        \item Decompose the performance error.
        \item Bound each term in the decomposition.
        \item Conclude the proof.
    \end{itemize}

    \textbf{Step 1: Decompose the performance error.}

    Before the detailed decomposition, we would like to define optimal parameters of the modified Glide function as follows.
    \begin{align*}
        \tilde{\theta}^{\rms,*} &= \argmin_{\theta\in\Theta} -\bbE_{P, \tilde{P}_{t}}\Big[\log \sm\big( G_{\theta}^{\rms,t}(q,X^{\rmw},X^{\prime})\big)_{\id^{*}}\Big] \nonumber\\
        \tilde{\theta}^{*} &=\argmin_{\theta\in\Theta} -\bbE_{P}\Big[\log \sm\big( G_{\theta}(q,X^{\rmw},X^{\prime})\big)_{\id^{*}}\Big],
    \end{align*}
    where $\tilde{\theta}^{\rms,*}$ is the optimal parameter that considers both the position offse and inputs truncation, and $\tilde{\theta}^{*}$ is the optimal parameter that only consider the input truncation.
    \begin{align}
        &\bbE_{P}\Big[\log \sm\big( G_{\theta^{*}}(q,X,X^{\prime})\big)_{\id^{*}}\Big]- \bbE_{P}\Big[\log \sm\big( G_{\htheta}(q,X^{\rmw},X^{\prime})\big)_{\id^{*}}\Big]\nonumber\\
        &\quad = \underbrace{\bbE_{P}\Big[\log \sm\big( G_{\theta^{*}}(q,X,X^{\prime})\big)_{\id^{*}}\Big] - \bbE_{P}\Big[\log \sm\big( G_{\tilde{\theta}^{*}}(q,X^{\rmw},X^{\prime})\big)_{\id^{*}}\Big]}_{\text{approximation error}}\nonumber\\
        &\quad\qquad +  \underbrace{\bbE_{P}\Big[\log \sm\big( G_{\tilde{\theta}^{*}}(q,X^{\rmw},X^{\prime})\big)_{\id^{*}}\Big] -  \bbE_{P,\tilde{P}_{t}}\Big[\log \sm\big( G_{\tilde{\theta}^{*}}^{\rms, t}(q,X^{\rmw},X^{\prime})\big)_{\id^{*}}\Big]}_{\text{positional embedding distribution shift}}\nonumber\\
        &\quad\qquad + \underbrace{\bbE_{P,\tilde{P}_{t}}\Big[\log \sm\big( G_{\tilde{\theta}^{*}}^{\rms, t}(q,X^{\rmw},X^{\prime})\big)_{\id^{*}}\Big]-\bbE_{P,\tilde{P}_{t}}\Big[\log \sm\big( G_{\tilde{\theta}^{\rms,*}}^{\rms, t}(q,X^{\rmw},X^{\prime})\big)_{\id^{*}}\Big]}_{\text{optimization error}} \nonumber\\
        &\quad\qquad +\underbrace{\bbE_{P,\tilde{P}_{t}}\Big[\log \sm\big( G_{\tilde{\theta}^{\rms,*}}^{\rms, t}(q,X^{\rmw},X^{\prime})\big)_{\id^{*}}\Big] - \bbE_{\calD_N,\calD_{N}^{t}}\Big[\log \sm\big( G_{\tilde{\theta}^{\rms,*}}^{\rms,t}(q,X^{\rmw},X^{\prime})\big)_{\id^{*}}\Big]}_{\text{generalization error}} \nonumber\\
        &\quad\qquad + \underbrace{\bbE_{\calD_N,\calD_{N}^{t}}\Big[\log \sm\big( G_{\tilde{\theta}^{\rms,*}}^{\rms,t}(q,X^{\rmw},X^{\prime})\big)_{\id^{*}}\Big]- \bbE_{\calD_N,\calD_{N}^{t}}\Big[\log \sm\big( G_{\htheta}^{\rms,t}(q,X^{\rmw},X^{\prime})\big)_{\id^{*}}\Big]}_{\text{optimization error}}\nonumber\\
        &\quad\qquad + \underbrace{\bbE_{\calD_N,\calD_{N}^{t}}\Big[\log \sm\big( G_{\htheta}^{\rms,t}(q,X^{\rmw},X^{\prime})\big)_{\id^{*}}\Big] - \bbE_{P,\tilde{P}_{t}}\Big[\log \sm\big( G_{\htheta}^{\rms,t}(q,X^{\rmw},X^{\prime})\big)_{\id^{*}}\Big]}_{\text{generation error}}\nonumber\\
        &\quad\qquad + \underbrace{\bbE_{P,\tilde{P}_{t}}\Big[\log \sm\big( G_{\htheta}^{\rms,t}(q,X^{\rmw},X^{\prime})\big)_{\id^{*}}\Big]-\bbE_{P}\Big[\log \sm\big( G_{\htheta}(q,X^{\rmw},X^{\prime})\big)_{\id^{*}}\Big]}_{\text{positional embedding distribution shift}}\nonumber\\
        &\quad\leq\underbrace{\bbE_{P}\Big[\log \sm\big( G_{\theta^{*}}(q,X,X^{\prime})\big)_{\id^{*}}\Big] - \bbE_{P}\Big[\log \sm\big( G_{\tilde{\theta}^{*}}(q,X^{\rmw},X^{\prime})\big)_{\id^{*}}\Big]}_{\text{approximation error}}\nonumber\\
        &\quad\qquad + 2 \max_{\theta\in\Theta} \underbrace{\bigg|\bbE_{P}\Big[\log \sm\big( G_{\theta}(q,X^{\rmw},X^{\prime})\big)_{\id^{*}}\Big] -  \bbE_{P,\tilde{P}_{t}}\Big[\log \sm\big( G_{\theta}^{\rms, t}(q,X^{\rmw},X^{\prime})\big)_{\id^{*}}\Big]\bigg|}_{\text{positional embedding distribution shift}}\nonumber\\
        &\quad\qquad + 2\max_{\theta\in\Theta}\underbrace{\bigg|\bbE_{\calD_N,\calD_{N}^{t}}\Big[\log \sm\big( G_{\theta}^{\rms,t}(q,X^{\rmw},X^{\prime})\big)_{\id^{*}}\Big] - \bbE_{P,\tilde{P}_{t}}\Big[\log \sm\big( G_{\theta}^{\rms,t}(q,X^{\rmw},X^{\prime})\big)_{\id^{*}}\Big]\bigg|}_{\text{generation error}},\label{eq:err_decomp}
    \end{align}
    where the inequality follows from the fact that all the optimization error is less and equal to $0$ according to the definitions.

    \textbf{Step 2: Bound each term in the decomposition.}

    Then we would like to separately upper bound the three kinds of error derived in the first step. Before calculating the upper bounds, we note that $\sm\big( G_{\theta}(q,X,X^{\prime})\big)_{\id^{*}}\geq (1+d_V\exp(B))^{-1}$ for any $\theta\in\Theta$ due to Lemma~\ref{lem:matvec}. For the approximation error, we have that
    \begin{align*}
        &\bbE_{P}\Big[\log \sm\big( G_{\theta^{*}}(q,X,X^{\prime})\big)_{\id^{*}}\Big] - \bbE_{P}\Big[\log \sm\big( G_{\tilde{\theta}^{*}}(q,X^{\rmw},X^{\prime})\big)_{\id^{*}}\Big]\nonumber\\
        &\quad \leq \bbE_{P}\Big[\log \sm\big( G_{\theta^{*}}(q,X,X^{\prime})\big)_{\id^{*}}\Big] - \bbE_{P}\Big[\log \sm\big( G_{\theta^{*}}(q,X^{\rmw},X^{\prime})\big)_{\id^{*}}\Big]\nonumber\\
        &\quad\lesssim (1+d_V\exp(B))HB^{4}(1+B_{X}^2B^2)\varepsilon,
    \end{align*}
    where the first inequality results from the definition of $\tilde{\theta}^{*}$, and the second inequality results from Lemmas~\ref{lem:mhalip} and \ref{prop:fflip}. For the positional embedding distribution shift, we have that
    \begin{align*}
         &\max_{\theta\in\Theta}\bigg|\bbE_{P}\Big[\log \sm\big( G_{\theta}(q,X^{\rmw},X^{\prime})\big)_{\id^{*}}\Big] -  \bbE_{P,\tilde{P}_{t}}\Big[\log \sm\big( G_{\theta}^{\rms, t}(q,X^{\rmw},X^{\prime})\big)_{\id^{*}}\Big]\bigg|\nonumber\\
         &\quad \leq \log (1+d_V\exp(B))\tv(\tilP_t,P_t),
    \end{align*}
    where the inequality results from the definition of the total variation. For the generalization error, we would like to apply Theorem 4.3 of \cite{zhang2023and}. In fact, we have that with probability at least $1-\delta$, the following inequality holds.
    \begin{align*}
        &\max_{\theta\in\Theta}\bigg|\bbE_{\calD_N,\calD_{N}^{t}}\Big[\log \sm\big( G_{\theta}^{\rms,t}(q,X^{\rmw},X^{\prime})\big)_{\id^{*}}\Big] - \bbE_{P,\tilde{P}_{t}}\Big[\log \sm\big( G_{\theta}^{\rms,t}(q,X^{\rmw},X^{\prime})\big)_{\id^{*}}\Big]\bigg|\nonumber\\
        &\quad \lesssim\frac{1}{\sqrt{N}}\Big[d(d+d_V)\log (1+NHB^6)+\log\frac{1}{\delta}\Big].
    \end{align*}

    \textbf{Step 3: Conclude the proof.}

    Combining all the results in steps 1 and 2, we have that
    \begin{align*}
        &\bbE_{P}\Big[\log \sm\big( G_{\theta^{*}}(q,X,X^{\prime})\big)_{\id^{*}}\Big]- \bbE_{P}\Big[\log \sm\big( G_{\htheta}(q,X^{\rmw},X^{\prime})\big)_{\id^{*}}\Big]\nonumber\\
        &\quad \lesssim (1+d_V\exp(B))HB^{4}(1+B_{X}^2B^2)\varepsilon + \log (1+d_V\exp(B))\cdot \tv(\tilP_t,P_t)\nonumber\\
        &\quad\qquad + \frac{1}{\sqrt{N}}\Big[d(d+d_V)\log (1+NHB^6)+\log\frac{1}{\delta}\Big].
    \end{align*}
\end{proof}

\section{Supporting Lemmas}

\begin{proposition}[Proposition 11.1 in \cite{zhang2023and}]\label{prop:fflip}
    For any $x,\tilx\in\bbR^{ d}$, $A_{1},\tilA_{1}\in\bbR^{d\times d_{F}}$, and $A_{2},\tilA_{2}\in\bbR^{d_{F}\times d}$, we have that
    \begin{align*}
        &\big\|\ff(x,A)-\ff(\tilx,\tilA)\big\|_{2}\\
        &\quad\leq\|A_{1}\|_{\rmF}\cdot\|A_{2}\|_{\rmF}\cdot\|x-\tilx\|_{2}+\|A_{1}-\tilA_{1}\|_{\rmF}\cdot\|A_{2}\|_{\rmF}\cdot\|\tilx\|_{2}+\|\tilA_{1}\|_{\rmF}\cdot\|A_{2}-\tilA_{2}\|_{\rmF}\cdot\|\tilx\|_{2}.
    \end{align*}
\end{proposition}

\begin{lemma}[Lemma I.8 in \cite{zhang2023and}]\label{lem:mhalip}
    For any $X,\tilde{X}\in\bbR^{N\times d}$, and any $W_{Q,h},W_{K,h}\in\bbR^{d\times d_{h}},W_{V,h}\in\bbR^{d\times d}$ for $h\in [H]$ , if $\|X\|_{2,\infty},\|\tilde{X}\|_{2,\infty}\leq B_{X}$, $\|W_{Q,h}\|_{\rmF}\leq B_{Q}$, $\|W_{K,h}\|_{\rmF},\leq B_{K}$, $\|W_{V,h}\|_{\rmF}\leq B_{V}$ for $h\in[H]$, then we have 
    \begin{align*}
        &\Big\|\mha\big(X,\{W_{Q,h},W_{K,h},W_{V,h}\}_{h=1}^{H})-\mha(\tilde{X},\{W_{Q,h},W_{K,h},W_{V,h}\}_{h=1}^{H}\big)\Big\|_{2,\infty}\nonumber\\
        &\quad\leq H\cdot B_{V}\big(1+4B_{X}^{2}\cdot B_{Q}B_{K}\big)\|X-\tilde{X}\|_{2,\infty}.
    \end{align*}
\end{lemma}

\begin{lemma}[Lemma 17 in \cite{zhang2022relational} ]\label{lem:matvec}
    Given any two conjugate numbers $u,v\in [1,\infty]$, \emph{i.e.}, $\frac{1}{u}+\frac{1}{v}=1$, and $1\leq p\leq \infty$, for any $A\in\bbR^{r\times c}$ and $x\in \bbR^{c}$, we have
    \begin{align}
        \|Ax\|_{p}\leq \|A^{\top}\|_{p,u}\|x\|_{v}\quad\mbox{and}\quad  \|Ax\|_{p}\leq \|A\|_{u,p}\|x\|_{v}\nonumber.
    \end{align}
\end{lemma}

\begin{lemma}\label{lem:sink}
    For a query vector $q\in\bbR^{d}$, and two sets of key-value pairs $K_{1}\in\bbR^{N_{1}\times d}$, $K_{2}\in\bbR^{N_{2}\times d}$, $V_{1}\in\bbR^{N_{1}\times d}$, and $V_{2}\in\bbR^{N_{2}\times d}$, We define attention scores $\sm(q^{\top}[K_{1},K_{2}]^{\top})$ and $\sm(q^{\top}K_{1}^{\top})$ as 
    \begin{align*}
        \sm(q^{\top}[K_{1},K_{2}]^{\top}) = [s_{1}^{\top},s_{2}^{\top}],\text{ and } \sm(q^{\top}K_{1}^{\top})=\tils_{1}^{\top}.
    \end{align*}
    Then we have that
    \begin{align*}
        \big\|\sm(q^{\top}K_{1}^{\top})V_{1}-\sm(q^{\top}[K_{1},K_{2}]^{\top})[V_{1}^{\top},V_{2}^{\top}]^{\top}\big\|_{2}\leq 2\|s_{2}\|_{1}\cdot \max\{\|V_{1}\|_{2,\infty},\|V_{2}\|_{2,\infty}\}.
    \end{align*}
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lem:sink}]
    
    In fact, we have that
    \begin{align*}
        \sm(q^{\top}[K_{1},K_{2}]^{\top})[V_{1}^{\top},V_{2}^{\top}]^{\top}= s_{1}^{\top}V_{1}+s_{2}^{\top}V_{2},\text{ and }\sm(q^{\top}K_{1}^{\top})V_{1}=\tils_{1}^{\top}V_{1}.
    \end{align*}
    Further, the difference between $s_{1}$ and $\tils_{1}$ can be upper bounded as 
    \begin{align*}
        &\|s_{1}-\tils_{1}\|_{1}\nonumber\\
        &\quad = \sum_{i=1}^{N_{1}}\frac{\exp(q^{\top}[K_{1}]_{i,:})\sum_{l=1}^{N_{2}}\exp(q^{\top}[K_{2}]_{l,:})}{\big(\sum_{j=1}^{N_{1}}\exp(q^{\top}[K_{1}]_{j,:})+\sum_{l=1}^{N_{2}}\exp(q^{\top}[K_{2}]_{l,:})\big)\sum_{j=1}^{N_{1}}\exp(q^{\top}[K_{1}]_{j,:})}\nonumber\\
        &\quad = \|s_{2}\|_{1},
    \end{align*}
    where the equalities result from the definitions of $\sm(\cdot)$ and $s_{2}$. Then we have that
    \begin{align*}
        &\big\|\sm(q^{\top}K_{1}^{\top})V_{1}-\sm(q^{\top}[K_{1},K_{2}]^{\top})[V_{1}^{\top},V_{2}^{\top}]^{\top}\big\|_{2}\nonumber\\
        &\quad = \big\|s_{1}^{\top}V_{1}+s_{2}^{\top}V_{2}- \tils_{1}^{\top}V_{1}\big\|_{2}\nonumber\\
        &\quad \leq \|s_{1}-\tils_{1}\|_{1}\cdot \|V_{1}\|_{2,\infty}+\|s_{2}\|_{1}\cdot \|V_{2}\|_{2,\infty}\nonumber\\
        &\quad \leq 2\|s_{2}\|_{1}\cdot \max\{\|V_{1}\|_{2,\infty},\|V_{2}\|_{2,\infty}\}.
    \end{align*}
    Thus, we conclude the proof of Lemma~\ref{lem:sink}.
    
\end{proof}
