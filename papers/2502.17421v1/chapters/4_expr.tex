% \begin{table*}[t]
% \centering
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{l|l|l|l|l|l|l|l|l|l|l|l}
% \hline
% Model          & Setting       & \multicolumn{2}{c|}{GovReport} & \multicolumn{2}{c|}{QMSum} & \multicolumn{2}{c|}{Multi-News} & \multicolumn{2}{c|}{LCC} & \multicolumn{2}{c}{RepoBench-P} \\ \hline
%                &               & $\tau$ & Tokens/s & $\tau$ & Tokens/s & $\tau$ & Tokens/s & $\tau$ & Tokens/s & $\tau$ & Tokens/s \\ \hline
% Vicuna-7B      & Vanilla Torch & 1          & 25.25    & 1          & 18.12    & 1          & 27.29    & 1          & 25.25    & 1          & 19.18    \\
%                & Vanilla       & 1          & 45.76    & 1          & 43.68    & 1          & 55.99    & 1          & 54.07    & 1          & 46.61    \\
%                & Seq           & 2.30       & 96.94    & 2.05       & 86.01    & 2.27       & 98.25    & 2.48       & 105.80   & 2.54       & 108.30   \\
%                & Tree          & 3.57       & 102.23   & 3.14       & 88.87    & 3.51       & 100.55   & 3.73       & 107.30   & 3.86       & 110.76   \\ \hline
% Vicuna-13B     & Vanilla Torch & 1          & 17.25    & 1          & 11.86    & 1          & 18.81    & 1          & 17.25    & 1          & 13.44    \\
%                & Vanilla       & 1          & 28.52    & 1          & 27.43    & 1          & 35.01    & 1          & 33.87    & 1          & 29.14    \\
%                & Seq           & 2.15       & 61.79    & 1.76       & 49.62    & 2.23       & 69.21    & 2.34       & 71.22    & 2.30       & 66.33    \\
%                & Tree          & 3.31       & 71.08    & 2.76       & 57.15    & 3.44       & 78.20    & 3.57       & 81.00    & 3.59       & 77.22    \\ \hline
% LongChat-7B    & Vanilla Torch & 1          & 25.27    & 1          & 14.11    & 1          & 27.66    & 1          & 25.27    & 1          & 17.02    \\
%                & Vanilla       & 1          & 42.14    & 1          & 36.87    & 1          & 50.19    & 1          & 54.17    & 1          & 42.69    \\
%                & Seq           & 2.30       & 94.27    & 2.01       & 79.07    & 2.21       & 91.61    & 2.78       & 119.37   & 2.66       & 108.80   \\
%                & Tree          & 3.59       & 101.43   & 3.06       & 85.23    & 3.41       & 97.93    & 4.21       & 122.30   & 4.03       & 115.27   \\ \hline
% LongChat-13B   & Vanilla Torch & 1          & 17.72    & 1          & 12.08    & 1          & 18.74    & 1          & 17.72    & 1          & 13.85    \\
%                & Vanilla       & 1          & 28.56    & 1          & 27.18    & 1          & 35.37    & 1          & 34.58    & 1          & 29.74    \\
%                & Seq           & 2.28       & 65.55    & 2.02       & 56.57    & 2.29       & 71.32    & 2.62       & 79.87    & 2.89       & 83.43    \\
%                & Tree          & 3.58       & 76.26    & 3.15       & 64.41    & 3.50       & 80.48    & 4.01       & 90.92    & 4.46       & 96.96    \\ \hline
% LLaMA3-8B      & Vanilla Torch & 1          & 21.59    & 1          & 18.67    & 1          & 29.91    & 1          & 29.48    & 1          & 22.77    \\
%                & Vanilla       & 1          & 53.14    & 1          & 51.22    & 1          & 56.94    & 1          & 56.73    & 1          & 54.08    \\
%                & Seq           & 2.21       & 84.39    & 1.96       & 73.19    & 2.23       & 88.92    & 2.14       & 85.74    & 2.15       & 82.60    \\
%                & Tree          & 3.25       & 84.57    & 2.99       & 75.68    & 3.36       & 91.11    & 3.28       & 89.33    & 3.39       & 91.28    \\ \hline
% \end{tabular}
% }
% \caption{Speedup ratio and average acceptance length $\tau$ on GovReport, QMSum, Multi-News, LCC, and RepoBench-P when temperature $T=0$.}
% \label{tab:main_t=0}
% \end{table*}

% \begin{table*}[t]
% \centering
% \vspace{.2em}
% \scalebox{0.92}{\begin{tabular}{lccccccccccc}
% \toprule
% \textbf{Setting} & 
% \multicolumn{2}{c}{\textbf{GovReport}} & \multicolumn{2}{c}{\textbf{QMSum}} & 
% \multicolumn{2}{c}{\textbf{Multi-News}} & \multicolumn{2}{c}{\textbf{LCC}} & 
% \multicolumn{2}{c}{\textbf{RepoBench-P}} \\
% \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11}
% & $\tau$ & Tokens/s & $\tau$ & Tokens/s & $\tau$ & Tokens/s & $\tau$ & Tokens/s & $\tau$ & Tokens/s \\
% \midrule

% \multicolumn{11}{l}{\textbf{Vicuna-7B}} \\ \cmidrule(lr){1-1}
% \rowcolor{blue!8} Vanilla HF    & 1.00 & 25.25 & 1.00 & 18.12 & 1.00 & 27.29 & 1.00 & 25.25 & 1.00 & 19.18 \\
% \rowcolor{blue!8} Vanilla FA    & 1.00 & 45.76 & 1.00 & 43.68 & 1.00 & 55.99 & 1.00 & 54.07 & 1.00 & 46.61 \\
% \rowcolor{blue!8} MagicDec      & 2.23 & 41.68 & 2.29 & 42.91 & 2.31 & 44.82 & 2.52 & 46.96 & 2.57 & 48.75 \\
% \rowcolor{blue!8} Tree          & \textbf{3.57} & \textbf{102.23} & \textbf{3.14} & \textbf{88.87} & \textbf{3.51} & \textbf{100.55} & \textbf{3.73} & \textbf{107.30} & \textbf{3.86} & \textbf{110.76} \\

% \multicolumn{11}{l}{\textbf{Vicuna-13B}} \\ \cmidrule(lr){1-1}
% \rowcolor{purple!8} Vanilla HF  & 1.00 & 17.25 & 1.00 & 11.86 & 1.00 & 18.81 & 1.00 & 17.25 & 1.00 & 13.44 \\
% \rowcolor{purple!8} Vanilla FA  & 1.00 & 28.52 & 1.00 & 27.43 & 1.00 & 35.01 & 1.00 & 33.87 & 1.00 & 29.14 \\
% \rowcolor{purple!8} MagicDec    & 2.95 & 38.24 & 2.87 & 37.15 & 2.97 & 39.47 & 2.96 & 38.40 & 2.94 & 36.66 \\
% \rowcolor{purple!8} Tree        & \textbf{3.31} & \textbf{71.08} & \textbf{2.76} & \textbf{57.15} & \textbf{3.44} & \textbf{78.20} & \textbf{3.57} & \textbf{81.00} & \textbf{3.59} & \textbf{77.22} \\

% \multicolumn{11}{l}{\textbf{LongChat-7B}} \\ \cmidrule(lr){1-1}
% \rowcolor{green!8} Vanilla HF   & 1.00 & 25.27 & 1.00 & 14.11 & 1.00 & 27.66 & 1.00 & 25.27 & 1.00 & 17.02 \\
% \rowcolor{green!8} Vanilla FA   & 1.00 & 42.14 & 1.00 & 36.87 & 1.00 & 50.19 & 1.00 & 54.17 & 1.00 & 42.69 \\
% \rowcolor{green!8} MagicDec     & 2.26 & 41.90 & 2.20 & 40.82 & 2.32 & 43.94 & 2.77 & 51.73 & 2.57 & 44.13 \\
% \rowcolor{green!8} Tree         & \textbf{3.59} & \textbf{101.43} & \textbf{3.06} & \textbf{85.23} & \textbf{3.41} & \textbf{97.93} & \textbf{4.21} & \textbf{122.30} & \textbf{4.03} & \textbf{115.27} \\

% \multicolumn{11}{l}{\textbf{LongChat-13B}} \\ \cmidrule(lr){1-1}
% \rowcolor{yellow!8} Vanilla HF  & 1.00 & 17.72 & 1.00 & 12.08 & 1.00 & 18.74 & 1.00 & 17.72 & 1.00 & 13.85 \\
% \rowcolor{yellow!8} Vanilla FA  & 1.00 & 28.56 & 1.00 & 27.18 & 1.00 & 35.37 & 1.00 & 34.58 & 1.00 & 29.74 \\
% \rowcolor{yellow!8} MagicDec    & 2.40 & 31.37 & 2.38 & 30.84 & 2.43 & 32.58 & 2.68 & 35.77 & 2.85 & 35.67 \\
% \rowcolor{yellow!8} Tree        & \textbf{3.58} & \textbf{76.26} & \textbf{3.15} & \textbf{64.41} & \textbf{3.50} & \textbf{80.48} & \textbf{4.01} & \textbf{90.92} & \textbf{4.46} & \textbf{96.96} \\

% \multicolumn{11}{l}{\textbf{LLaMA3-8B}} \\ \cmidrule(lr){1-1}
% \rowcolor{orange!8} Vanilla HF  & 1.00 & 21.59 & 1.00 & 18.67 & 1.00 & 29.91 & 1.00 & 29.48 & 1.00 & 22.77 \\
% \rowcolor{orange!8} Vanilla FA  & 1.00 & 53.14 & 1.00 & 51.22 & 1.00 & 56.94 & 1.00 & 56.73 & 1.00 & 54.08 \\
% \rowcolor{orange!8} MagicDec    & 2.04 & 36.14 & 2.00 & 35.78 & 2.33 & 39.57 & 2.65 & 46.95 & 2.61 & 44.39 \\
% \rowcolor{orange!8} Tree        & \textbf{3.25} & \textbf{84.57} & \textbf{2.99} & \textbf{75.68} & \textbf{3.36} & \textbf{91.11} & \textbf{3.28} & \textbf{89.33} & \textbf{3.39} & \textbf{91.28} \\

% \bottomrule
% \end{tabular}}
% \caption{Speedup ratio ($\tau$) and decoding speed (Tokens/s) across different models and settings. All results are computed at $T=0$.}
% \label{tab:final_table}
% \end{table*}

% \begin{table*}[t]
% \centering
% \vspace{.2em}
% \scalebox{0.7}{
% \begin{tabular}{ccccccccccccccccc}
% \toprule
% \multirow{2}{*}{\textbf{Setting}} &
% \multicolumn{3}{c}{\textbf{GovReport}} &
% \multicolumn{3}{c}{\textbf{QMSum}} &
% \multicolumn{3}{c}{\textbf{Multi-News}} &
% \multicolumn{3}{c}{\textbf{LCC}} &
% \multicolumn{3}{c}{\textbf{Repo-P}} \\
% \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13} \cmidrule(lr){14-16}
% & $\tau$ & Tokens/s & Speedup
% & $\tau$ & Tokens/s & Speedup
% & $\tau$ & Tokens/s & Speedup
% & $\tau$ & Tokens/s & Speedup
% & $\tau$ & Tokens/s & Speedup \\
% \midrule

% \multicolumn{16}{l}{\textbf{Vicuna-7B}} \\ \cmidrule(lr){1-1}
% % \rowcolor{blue!8}
% Vanilla HF
% & 1.00 & 25.25 & -
% & 1.00 & 18.12 & -
% & 1.00 & 27.29 & -
% & 1.00 & 25.25 & -
% & 1.00 & 19.18 & - \\
% % \rowcolor{blue!8}
% Vanilla FA
% & 1.00 & 45.76 & 1.00
% & 1.00 & 43.68 & 1.00
% & 1.00 & 55.99 & 1.00
% & 1.00 & 54.07 & 1.00
% & 1.00 & 46.61 & 1.00 \\
% % \rowcolor{blue!8}
% MagicDec
% & 2.23 & 41.68 & 0.91
% & 2.29 & 42.91 & 0.98
% & 2.31 & 44.82 & 0.80
% & 2.52 & 46.96 & 0.87
% & 2.57 & 48.75 & 1.05 \\
% % \rowcolor{blue!8}
% LongSpec
% & \textbf{3.57} & \textbf{102.23} & \textbf{2.23}
% & \textbf{3.14} & \textbf{88.87} & \textbf{2.04}
% & \textbf{3.51} & \textbf{100.55} & \textbf{1.80}
% & \textbf{3.73} & \textbf{107.30} & \textbf{1.99}
% & \textbf{3.86} & \textbf{110.76} & \textbf{2.38} \\
% \midrule

% \multicolumn{16}{l}{\textbf{Vicuna-13B}} \\ \cmidrule(lr){1-1}
% % \rowcolor{purple!8}
% Vanilla HF
% & 1.00 & 17.25 & -
% & 1.00 & 11.86 & -
% & 1.00 & 18.81 & -
% & 1.00 & 17.25 & -
% & 1.00 & 13.44 & - \\
% % \rowcolor{purple!8}
% Vanilla FA
% & 1.00 & 28.52 & 1.00
% & 1.00 & 27.43 & 1.00
% & 1.00 & 35.01 & 1.00
% & 1.00 & 33.87 & 1.00
% & 1.00 & 29.14 & 1.00 \\
% % \rowcolor{purple!8}
% MagicDec
% & 2.95 & 38.24 & 1.34
% & 2.87 & 37.15 & 1.35
% & 2.97 & 39.47 & 1.13
% & 2.96 & 38.40 & 1.13
% & 2.94 & 36.66 & 1.26 \\
% % \rowcolor{purple!8}
% LongSpec
% & \textbf{3.31} & \textbf{71.08} & \textbf{2.49}
% & \textbf{2.76} & \textbf{57.15} & \textbf{2.08}
% & \textbf{3.44} & \textbf{78.20} & \textbf{2.23}
% & \textbf{3.57} & \textbf{81.00} & \textbf{2.39}
% & \textbf{3.59} & \textbf{77.22} & \textbf{2.65} \\
% \midrule

% \multicolumn{16}{l}{\textbf{LongChat-7B}} \\ \cmidrule(lr){1-1}
% % \rowcolor{green!8}
% Vanilla HF
% & 1.00 & 25.27 & -
% & 1.00 & 14.11 & -
% & 1.00 & 27.66 & -
% & 1.00 & 25.27 & -
% & 1.00 & 17.02 & - \\
% % \rowcolor{green!8}
% Vanilla FA
% & 1.00 & 42.14 & 1.00
% & 1.00 & 36.87 & 1.00
% & 1.00 & 50.19 & 1.00
% & 1.00 & 54.17 & 1.00
% & 1.00 & 42.69 & 1.00 \\
% % \rowcolor{green!8}
% MagicDec
% & 2.26 & 41.90 & 0.99
% & 2.20 & 40.82 & 1.11
% & 2.32 & 43.94 & 0.88
% & 2.77 & 51.73 & 0.96
% & 2.57 & 44.13 & 1.03 \\
% % \rowcolor{green!8}
% LongSpec
% & \textbf{3.59} & \textbf{101.43} & \textbf{2.41}
% & \textbf{3.06} & \textbf{85.23} & \textbf{2.31}
% & \textbf{3.41} & \textbf{97.93} & \textbf{1.95}
% & \textbf{4.21} & \textbf{122.30} & \textbf{2.26}
% & \textbf{4.03} & \textbf{115.27} & \textbf{2.70} \\
% \midrule

% \multicolumn{16}{l}{\textbf{LongChat-13B}} \\ \cmidrule(lr){1-1}
% % \rowcolor{yellow!8}
% Vanilla HF
% & 1.00 & 17.72 & -
% & 1.00 & 12.08 & -
% & 1.00 & 18.74 & -
% & 1.00 & 17.72 & -
% & 1.00 & 13.85 & - \\
% % \rowcolor{yellow!8}
% Vanilla FA
% & 1.00 & 28.56 & 1.00
% & 1.00 & 27.18 & 1.00
% & 1.00 & 35.37 & 1.00
% & 1.00 & 34.58 & 1.00
% & 1.00 & 29.74 & 1.00 \\
% % \rowcolor{yellow!8}
% MagicDec
% & 2.40 & 31.37 & 1.10
% & 2.38 & 30.84 & 1.13
% & 2.43 & 32.58 & 0.92
% & 2.68 & 35.77 & 1.03
% & 2.85 & 35.67 & 1.20 \\
% % \rowcolor{yellow!8}
% LongSpec
% & \textbf{3.58} & \textbf{76.26} & \textbf{2.67}
% & \textbf{3.15} & \textbf{64.41} & \textbf{2.37}
% & \textbf{3.50} & \textbf{80.48} & \textbf{2.28}
% & \textbf{4.01} & \textbf{90.92} & \textbf{2.63}
% & \textbf{4.46} & \textbf{96.96} & \textbf{3.26} \\
% \midrule

% \multicolumn{16}{l}{\textbf{LLaMA3-8B}} \\ \cmidrule(lr){1-1}
% % \rowcolor{orange!8}
% Vanilla HF
% & 1.00 & 21.59 & -
% & 1.00 & 18.67 & -
% & 1.00 & 29.91 & -
% & 1.00 & 29.48 & -
% & 1.00 & 22.77 & - \\
% % \rowcolor{orange!8}
% Vanilla FA
% & 1.00 & 53.14 & 1.00
% & 1.00 & 51.22 & 1.00
% & 1.00 & 56.94 & 1.00
% & 1.00 & 56.73 & 1.00
% & 1.00 & 54.08 & 1.00 \\
% % \rowcolor{orange!8}
% MagicDec
% & 2.04 & 36.14 & 0.68
% & 2.00 & 35.78 & 0.70
% & 2.33 & 39.57 & 0.70
% & 2.65 & 46.95 & 0.83
% & 2.61 & 44.39 & 0.82 \\
% % \rowcolor{orange!8}
% LongSpec
% & \textbf{3.25} & \textbf{84.57} & \textbf{1.59}
% & \textbf{2.99} & \textbf{75.68} & \textbf{1.48}
% & \textbf{3.36} & \textbf{91.11} & \textbf{1.60}
% & \textbf{3.28} & \textbf{89.33} & \textbf{1.57}
% & \textbf{3.39} & \textbf{91.28} & \textbf{1.69} \\

% \bottomrule
% \end{tabular}
% }
% \caption{Speedup ratio ($\tau$) and decoding speed (Tokens/s) across different models and settings. All results are computed at $T=0$.}
% \label{tab:final_table}
% \end{table*}

\begin{table*}[t]
\centering
\caption{Mean accepted length ($\tau$), decoding speed (tokens/s), and speedups across different models and settings. Specifically, ``Vanilla HF'' refers to HuggingFace’s PyTorch-based attention implementation, while ``Vanilla FA'' employs \texttt{Flash\_Decoding}. The speedup statistic calculates the acceleration ratio relative to the Vanilla HF method. All results are computed at $T=0$.}
\label{tab:final_table}
\vspace{.2em}
\scalebox{0.7}{
\begin{tabular}{c c c c c c c c c c c c c c c c c}
\toprule
 & \multirow{2}{*}{\textbf{Setting}} 
& \multicolumn{3}{c}{\textbf{GovReport}} 
& \multicolumn{3}{c}{\textbf{QMSum}} 
& \multicolumn{3}{c}{\textbf{Multi-News}} 
& \multicolumn{3}{c}{\textbf{LCC}} 
& \multicolumn{3}{c}{\textbf{RepoBench-P}} \\
\cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11} \cmidrule(lr){12-14} \cmidrule(lr){15-17}
& 
& $\tau$ & Tokens/s & Speedup
& $\tau$ & Tokens/s & Speedup
& $\tau$ & Tokens/s & Speedup
& $\tau$ & Tokens/s & Speedup
& $\tau$ & Tokens/s & Speedup \\
\midrule

% Vicuna-7B
\multirow{4}{*}{\rotatebox{90}{V-7B}}
& Vanilla HF  
& 1.00 & 25.25 & -
& 1.00 & 18.12 & -
& 1.00 & 27.29 & -
& 1.00 & 25.25 & -
& 1.00 & 19.18 & - \\

& Vanilla FA  
& 1.00 & 45.76 & 1.00$\times$
& 1.00 & 43.68 & 1.00$\times$
& 1.00 & 55.99 & 1.00$\times$
& 1.00 & 54.07 & 1.00$\times$
& 1.00 & 46.61 & 1.00$\times$ \\

& MagicDec    
& 2.23 & 41.68 & 0.91$\times$
& 2.29 & 42.91 & 0.98$\times$
& 2.31 & 44.82 & 0.80$\times$
& 2.52 & 46.96 & 0.87$\times$
& 2.57 & 48.75 & 1.05$\times$ \\

& \textbf{LongSpec}  
& \textbf{3.57} & \textbf{102.23} & \textbf{2.23}$\times$
& \textbf{3.14} & \textbf{88.87}  & \textbf{2.04}$\times$
& \textbf{3.51} & \textbf{100.55} & \textbf{1.80}$\times$
& \textbf{3.73} & \textbf{107.30} & \textbf{1.99}$\times$
& \textbf{3.86} & \textbf{110.76} & \textbf{2.38}$\times$ \\
\midrule

% Vicuna-13B
\multirow{4}{*}{\rotatebox{90}{V-13B}}
& Vanilla HF  
& 1.00 & 17.25 & -
& 1.00 & 11.86 & -
& 1.00 & 18.81 & -
& 1.00 & 17.25 & -
& 1.00 & 13.44 & - \\

& Vanilla FA  
& 1.00 & 28.52 & 1.00$\times$
& 1.00 & 27.43 & 1.00$\times$
& 1.00 & 35.01 & 1.00$\times$
& 1.00 & 33.87 & 1.00$\times$
& 1.00 & 29.14 & 1.00$\times$ \\

& MagicDec    
& 2.95 & 38.24 & 1.34$\times$
& 2.87 & 37.15 & 1.35$\times$
& 2.97 & 39.47 & 1.13$\times$
& 2.96 & 38.40 & 1.13$\times$
& 2.94 & 36.66 & 1.26$\times$ \\

& \textbf{LongSpec}  
& \textbf{3.31} & \textbf{71.08} & \textbf{2.49}$\times$
& \textbf{2.76} & \textbf{57.15} & \textbf{2.08}$\times$
& \textbf{3.44} & \textbf{78.20} & \textbf{2.23}$\times$
& \textbf{3.57} & \textbf{81.00} & \textbf{2.39}$\times$
& \textbf{3.59} & \textbf{77.22} & \textbf{2.65}$\times$ \\
\midrule

% LongChat-7B
\multirow{4}{*}{\rotatebox{90}{LC-7B}}
& Vanilla HF  
& 1.00 & 25.27 & -
& 1.00 & 14.11 & -
& 1.00 & 27.66 & -
& 1.00 & 25.27 & -
& 1.00 & 17.02 & - \\

& Vanilla FA  
& 1.00 & 42.14 & 1.00$\times$
& 1.00 & 36.87 & 1.00$\times$
& 1.00 & 50.19 & 1.00$\times$
& 1.00 & 54.17 & 1.00$\times$
& 1.00 & 42.69 & 1.00$\times$ \\

& MagicDec    
& 2.26 & 41.90 & 0.99$\times$
& 2.20 & 40.82 & 1.11$\times$
& 2.32 & 43.94 & 0.88$\times$
& 2.77 & 51.73 & 0.96$\times$
& 2.57 & 44.13 & 1.03$\times$ \\

& \textbf{LongSpec}  
& \textbf{3.59} & \textbf{101.43} & \textbf{2.41}$\times$
& \textbf{3.06} & \textbf{85.23} & \textbf{2.31}$\times$
& \textbf{3.41} & \textbf{97.93} & \textbf{1.95}$\times$
& \textbf{4.21} & \textbf{122.30} & \textbf{2.26}$\times$
& \textbf{4.03} & \textbf{115.27} & \textbf{2.70}$\times$ \\
\midrule

% LongChat-13B
\multirow{4}{*}{\rotatebox{90}{LC-13B}}
& Vanilla HF  
& 1.00 & 17.72 & -
& 1.00 & 12.08 & -
& 1.00 & 18.74 & -
& 1.00 & 17.72 & -
& 1.00 & 13.85 & - \\

& Vanilla FA  
& 1.00 & 28.56 & 1.00$\times$
& 1.00 & 27.18 & 1.00$\times$
& 1.00 & 35.37 & 1.00$\times$
& 1.00 & 34.58 & 1.00$\times$
& 1.00 & 29.74 & 1.00$\times$ \\

& MagicDec    
& 2.40 & 31.37 & 1.10$\times$
& 2.38 & 30.84 & 1.13$\times$
& 2.43 & 32.58 & 0.92$\times$
& 2.68 & 35.77 & 1.03$\times$
& 2.85 & 35.67 & 1.20$\times$ \\

& \textbf{LongSpec}  
& \textbf{3.58} & \textbf{76.26} & \textbf{2.67}$\times$
& \textbf{3.15} & \textbf{64.41} & \textbf{2.37}$\times$
& \textbf{3.50} & \textbf{80.48} & \textbf{2.28}$\times$
& \textbf{4.01} & \textbf{90.92} & \textbf{2.63}$\times$
& \textbf{4.46} & \textbf{96.96} & \textbf{3.26}$\times$ \\
\midrule

% LLaMA3-8B
\multirow{4}{*}{\rotatebox{90}{L-8B}}
& Vanilla HF  
& 1.00 & 21.59 & -
& 1.00 & 18.67 & -
& 1.00 & 29.91 & -
& 1.00 & 29.48 & -
& 1.00 & 22.77 & - \\

& Vanilla FA  
& 1.00 & 53.14 & 1.00$\times$
& 1.00 & 51.22 & 1.00$\times$
& 1.00 & 56.94 & 1.00$\times$
& 1.00 & 56.73 & 1.00$\times$
& 1.00 & 54.08 & 1.00$\times$ \\

& MagicDec    
& 2.04 & 36.14 & 0.68$\times$
& 2.00 & 35.78 & 0.70$\times$
& 2.33 & 39.57 & 0.70$\times$
& 2.65 & 46.95 & 0.83$\times$
& 2.61 & 44.39 & 0.82$\times$ \\

& \textbf{LongSpec}  
& \textbf{3.25} & \textbf{84.57} & \textbf{1.59}$\times$
& \textbf{2.99} & \textbf{75.68} & \textbf{1.48}$\times$
& \textbf{3.36} & \textbf{91.11} & \textbf{1.60}$\times$
& \textbf{3.28} & \textbf{89.33} & \textbf{1.57}$\times$
& \textbf{3.39} & \textbf{91.28} & \textbf{1.69}$\times$ \\

\bottomrule
\end{tabular}
}
\vspace{-.3cm}
\end{table*}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{Figure/T1.pdf}
    \vspace{-.7cm}
    \caption{Decoding speed (tokens/s) across different models and settings. All results are computed at $T=1$. The letters G, Q, M, L, and R on the horizontal axis represent the dataset GovReport, QMSum, Multi-News, LCC, and RepoBench-P respectively.}
    \label{fig:final_fig}
    \vspace{-.3cm}
\end{figure*}



\section{Experiments}

\subsection{Settings}

\textbf{Target and draft models.} 
We select four widely-used long-context LLMs, Vicuna~(including 7B and 13B)~\cite{chiang2023vicuna}, LongChat~(including 7B and 13B)~\cite{li2023longchat}, LLaMA-3.1-8B-Instruct~\cite{dubey2024llama}, and QwQ-32B~\cite{qwen2024qwq}, as target models.
In order to make the draft model and target model more compatible, our draft model is consistent with the target model in various parameters such as the number of KV heads. 

\textbf{Training Process.} 
We first train our draft model with Achor-Offest Indices on the SlimPajama-6B pretraining dataset~\cite{cerebras2023slimpajama}. 
The random offset is set as a random integer from 0 to 15k for Vicuna models and LongChat-7B, and 0 to 30k for the other three models because they have longer maximum context length.
Then we train our model on a small subset of the Prolong-64k long-context dataset~\cite{gao2024train} in order to gain the ability to handle long texts. 
Finally, we finetune our model on a self-built long-context supervised-finetuning~(SFT) dataset to further improve the model performance.
The position index of the last two stages is the vanilla indexing policy because the training data is sufficiently long.
We apply flash noisy training during all three stages to mitigate the training and inference inconsistency, the extra overhead of flash noisy training is negligible.
Standard cross-entropy is used to optimize the draft model while the parameters of the target model are kept frozen. To mitigate the VRAM peak caused by the computation of the logits, we use a fused-linear-and-cross-entropy loss implemented by the Liger Kernel~\cite{hsu2024liger}, which computes the LM head and the softmax function together and can greatly alleviate this problem. More details on model training can be found in Appendix~\ref{appendix:training_details}.



\textbf{Test Benchmarks.}
We select tasks from the LongBench benchmark~\cite{bai2024longbench} that involve generating longer outputs, because tasks with shorter outputs, such as document-QA, make it challenging to measure the speedup ratio fairly with speculative decoding. 
Specifically, we focus on long-document summarization and code completion tasks and conduct tests on five datasets: GovReport~\cite{huang2021efficient}, QMSum~\cite{zhong2021qmsum}, Multi-News~\cite{fabbri2019multi}, LCC~\cite{guo2023longcoder}, and RepoBench-P~\cite{liu2024repobench}. We test QwQ-32B on the famous reasoning dataset AIME24~\cite{numina2024aime}.

We compare our method with the original target model and MagicDec, a simple prototype of TriForce. 
To highlight the significance of \texttt{Flash\_Decoding} in long-context scenarios, we also present the performance of the original target model using both eager attention implemented by Huggingface and \texttt{Flash\_Decoding} for comparison.
To make a fair comparison, we also use \texttt{Flash\_Decoding} for baseline MagicDec.
The most important metric for speculative decoding is the \emph{walltime speedup ratio}, which is the actual test speedup ratio relative to vanilla autoregressive decoding. 
We also test the \emph{average acceptance length} $\tau$, \emph{i.e.}, the average number of tokens accepted per forward pass of the target LLM. 
% \emph{Acceptance rate} $\alpha$, the ratio of accepted to generated tokens during drafting, is taken into consideration as well. Following EAGLE~\cite{li2024eagle}, when measuring this metric, we utilize chain drafts without tree attention to assess the acceptance rate per location more precisely. Specifically, for the $n$-th token in the draft tokens, the acceptance rate is denoted as $n\text{-}\alpha$.

\subsection{Main Results}

Table~\ref{tab:final_table} and Figure~\ref{fig:final_fig} show the decoding speeds and mean accept lengths across the five evaluated datasets at $T=0$ and $T=1$ respectively. 
Our proposed method significantly outperforms all other approaches on both summarization tasks and code completion tasks. When $T=0$, on summarization tasks, our method can achieve a mean accepted length of around 3.5 and a speedup of up to 2.67$\times$; and on code completion tasks, our method can achieve a mean accepted length of around 4 and a speedup of up to 3.26$\times$. This highlights the robustness and generalizability of our speculative decoding approach, particularly in long-text generation tasks. At $T=1$, our method's performance achieves around 2.5$\times$ speedup, maintaining a substantial lead over MagicDec. This indicates that our approach is robust across different temperature settings, further validating its soundness and efficiency.

While MagicDec demonstrates competitive acceptance rates with LongSpec, its speedup is noticeably lower in our experiments. This is because MagicDec is primarily designed for scenarios with large batch sizes and tensor parallelism. 
In low-batch-size settings, its draft model leverages all parameters of the target model with sparse KV Cache becomes excessively heavy. 
This design choice leads to inefficiencies, as the draft model's computational overhead outweighs its speculative benefits. 
Our results reveal that MagicDec only achieves acceleration ratios~$>\!1$ on partial datasets when using a guess length $\gamma \!=\! 2$ and consistently exhibits negative acceleration around 0.7$\times$ when $\gamma\!\geq\!3$, further underscoring the limitations of this method in such configurations.
% \looseness=-1

Lastly, we can find attention implementation plays a critical role in long-context speculative decoding performance. In our experiments, ``Vanilla HF'' refers to HuggingFace’s attention implementation, while ``Vanilla FA'' employs \texttt{Flash\_Decoding}. The latter demonstrates nearly a $2\times$ speedup over the former, even as a standalone component, and our method can achieve up to $6\times$ speedup over HF Attention on code completion datasets. 
This result underscores the necessity for speculative decoding methods to be compatible with optimized attention mechanisms like \texttt{Flash\_Decoding}, especially in long-text settings. Our hybrid tree attention approach achieves this compatibility, allowing us to fully leverage the advantages of \texttt{Flash\_Decoding} and further speedup.

\subsection{Ablation Studies}

\textbf{Anchor-Offset Indices.} 
The experimental results demonstrate the significant benefits of incorporating the Anchor-Offset Indices. Figure~\ref{fig:ablation_1} shows that pretrained with Anchor-Offset Indices achieve a lower initial loss and final loss compared to those trained without it when training over the real long-context dataset. 
Notably, the initalization with Anchor-Offset Indices reaches the same loss level $3.93\times$ faster than its counterpart. 
Table~\ref{tab:ablation_1} further highlights the performance improvements across two datasets, a summary dataset Multi-News, and a code completion dataset RepoBench-P. 
Models with Anchor-Offset Indices exhibit faster output speed and larger average acceptance length $\tau$. These results underscore the effectiveness of Anchor-Offset Indices in enhancing both training efficiency and model performance.

\begin{table}[t]
\vspace{-0.cm}
    \centering
    \caption{Performance comparison with and without Anchor-Offset Indices on the Multi-News and RepoBench-P datasets. Models with Anchor-Offset Indices achieve higher output speed and larger accept length, highlighting its efficiency and effectiveness.}
    \label{tab:ablation_1}
    \vspace{.1cm}
    \scalebox{0.9}{
    \begin{tabular}{c c c c c}
        \toprule
        & \multicolumn{2}{c}{\textbf{Multi-News}} 
        & \multicolumn{2}{c}{\textbf{RepoBench-P}} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5}
        & $\tau$ & Tokens/s
        & $\tau$ & Tokens/s \\
        \midrule
        w/o Anchor-Offset & 3.20 & 85.98 & 3.26 & 85.21\\
        w/ Anchor-Offset  & 3.36 & 91.11 & 3.39 & 91.28\\
        \bottomrule
    \end{tabular}
    }
\end{table}

\begin{figure}[t]
    \vspace{-.2cm}
    \centering
    \includegraphics[width=1\linewidth]{Figure/ablation_1.pdf}
    \vspace{-.7cm}
    \caption{Training loss curves on long-context data. 
     Pretrained models with Anchor-Offset Indices exhibit lower initial and final loss, and reach the same loss level 3.93$\times$ faster compared to models without Anchor-Offset Indices.}
    \label{fig:ablation_1}
    \vspace{-.1cm}
\end{figure}


\textbf{Hybrid Tree Attention.}
The results presented in Figure \ref{fig:ablation_2} highlight the effectiveness of the proposed Hybrid Tree Attention, which combines \texttt{Flash\_Decoding} with the Triton kernel \texttt{fused\_mask\_attn}. 
While the time spent on the draft model forward pass and the target model FFN computations remain comparable across the two methods, the hybrid approach exhibits a significant reduction in latency for the target model's attention layer (the yellow part). 
Specifically, the attention computation latency decreases from 49.92 ms in the HF implementation to 12.54 ms in the hybrid approach, resulting in an approximately 75\% improvement. 
The verification step time difference is minimal, further solidifying the conclusion that the primary performance gains stem from optimizing the attention mechanism.

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{Figure/runtime.pdf}
    \vspace{-.8cm}
    \caption{Latency breakdown for a single speculative decoding loop comparing the EAGLE implementation and the proposed Hybrid Tree Attention. Significant latency reduction is observed in the target model's attention layer (the yellow part) using our approach.}
    \label{fig:ablation_2}
    \vspace{-.cm}
\end{figure}


\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{Figure/qwq.pdf}
    \vspace{-.5cm}
    \caption{Performance of our method on the QwQ-32B model with the AIME24 dataset, using a maximum output length of 32k tokens. The left plot shows the tokens generated per second, where our approach achieves 2.25$\times$ higher speed compared to the baseline. 
    The right plot shows the mean number of accepted tokens, where our method achieves an average of 3.82 mean accepted tokens.}
    \label{fig:qwq}
    \vspace{.1cm}
\end{figure}

\subsection{Long CoT Acceleration}

Long Chain-of-Thought (LongCoT) tasks have gained significant attention recently due to their ability to enable models to perform complex reasoning and problem-solving over extended outputs~\cite{qwen2024qwq, openai2024o1}. In these tasks, while the prefix input is often relatively short, the generated output can be extremely long, posing unique challenges in terms of efficiency and token acceptance. Our method is particularly well-suited for addressing these challenges, effectively handling scenarios with long outputs. It is worth mentioning that MagicDec is not suitable for such long-output scenarios because the initial inference stage of the LongCoT task is not the same as the traditional long-context task. In LongCoT tasks, where the prefix is relatively short, the draft model in MagicDec will completely degrade into the target model, failing to achieve acceleration.
\looseness=-1

We evaluate our method on the QwQ-32B model using the widely-used benchmark AIME24 dataset, with a maximum output length set to 32k tokens. 
The results, illustrated in Figure~\ref{fig:qwq}, demonstrate a significant improvement in both generation speed and mean accepted tokens. 
Specifically, our method achieved a generation rate of 42.63 tokens/s, 2.25$\times$ higher than the baseline's 18.92 tokens/s, and an average of 3.82 mean accepted tokens.
Notably, QwQ-32B with \textsc{LongSpec} achieves even lower latency than the vanilla 7B model with \texttt{Flash\_Decoding}, demonstrating that our method effectively accelerates the LongCoT model.
These findings not only highlight the effectiveness of our method in the LongCoT task but also provide new insights into lossless inference acceleration for the o1-like model.
We believe that speculative decoding will play a crucial role in accelerating this type of model in the future.


\subsection{Throughput}

The throughput results of Vicuna-7B on the RepoBench-P dataset show that \textsc{LongSpec} consistently outperforms both Vanilla and MagicDec across all batch sizes. At a batch size of 8, \textsc{LongSpec} achieves a throughput of 561.32 tokens/s, approximately 1.8$\times$ higher than MagicDec (310.58 tokens/s) and nearly 2$\times$ higher than Vanilla (286.96 tokens/s). MagicDec, designed with throughput optimization in mind, surpasses Vanilla as the batch size increases, reflecting its targeted improvements. However, \textsc{LongSpec} still sustains its advantage, maintaining superior throughput across all tested batch sizes.

\begin{figure}
    \centering
    \includegraphics[width=0.85\linewidth]{Figure/throughput.pdf}
    \vspace{-.2cm}
    \caption{Throughput comparison of Vanilla, MagicDec, and \textsc{LongSpec} on RepoBench-P using Vicuna-7B across different batch sizes. \textsc{LongSpec} shows superior throughput and scalability, outperforming both Vanilla and MagicDec in all batch sizes.}
    \label{fig:throughput}
    \vspace{-.3cm}
\end{figure}
