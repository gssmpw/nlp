\vspace{-.5cm}
\section{Introduction}
\vspace{-.1cm}

Large Language Models (LLMs) have achieved remarkable success across various natural language processing tasks~\cite{achiam2023gpt}, but their autoregressive decoding mechanism often results in high latency. 
To address this limitation, speculative decoding~\cite{leviathan2023fast} has emerged as a promising solution.
By employing a lightweight draft model to generate multiple candidate tokens, the target model can verify these tokens in parallel, thereby accelerating the inference process without compromising output quality.

Despite the great advancements in speculative decoding, existing research has primarily concentrated on short-context scenarios. 
However, as highlighted by~\citet{chen2024magicdec}, the core potential of speculative decoding lies in long-context settings, where the maximum inference batch size is relatively low.
The limited batch size restricts autoregressive decoding from fully utilizing the GPU computation resource, making speculative decoding an ideal approach to address this constraint. 
Yet, despite its advantages, the development of draft models specially designed for long-context scenarios remains largely unexplored. 

While the need for a long-context draft model is clear from both application demands and theoretical considerations, we find that existing methodologies developed for short-context scenarios are inadequate when applied to longer sequences.
This inadequacy stems from three emergent challenges unique to long-context speculative decoding:
1) \textit{Architecture}: the extra memory overhead of the draft model, 2) \textit{Training}: the distribution shift of position indices, and 3) \textit{Inference}: the inefficiencies in tree attention implementation. 
First, as decoding length increases, previous State-of-The-Art (SoTA) autoregressive draft models like EAGLE~\cite{li2024eagle} and GliDe~\cite{du2024glide} require linearly increasing KV caches, resulting in substantial storage overhead. 
% This becomes particularly problematic in long-context settings,
This issue becomes critical in long-context settings,
where memory usage is of vital importance. 
Second, the training data of the draft model mainly consists of short context data, rendering it undertrained over the large position indices~\cite{an2025does}, while the inference is for long-context data.
%Thus the draft model is undertrained over the large indices. 
The discrepancy will cause the draft model unable to perform speculation when facing large position indices of long-context input.
Moreover, SoTA approaches often rely on tree attention, which is incompatible with the current advanced attention kernels due to the tree mask.
 This incompatibility further constrains the usage of tree speculative decoding in long-context scenarios.

To address these challenges, we introduce \textsc{LongSpec}, a framework for long-context speculative decoding, featuring architectural innovation (Sec.~\ref{subsec: arch}), novel training methods (Sec.~\ref{subsec: Training}), and optimized inference implementation (Sec.~\ref{subsec: tree attention}).
The three key innovations significantly enhance the efficiency and scalability of speculative decoding in long-context scenarios.

First, to alleviate the memory overhead problem, we develop a draft model architecture that circumvents the linear expansion of KV caches which maintains a constant memory footprint as the context grows. 
Concretely, our approach employs a sliding window self-attention component to capture local dependencies, complemented by a cache-free cross-attention module for effectively modeling long-context representations. 
This approach effectively mitigates memory overhead which is particularly critical in long-context inference without compromising performance.  

Next, to handle the training discrepancy problem, we propose the Anchor-Offset Indices to reconcile the short-context training for long-context inference. 
To fully train all the indices, we randomly add an offset to position indices. 
This ensures that some larger indices are also sufficiently trained.
However, since RoPE~\cite{su2024roformer} is based on relative positions, directly adding an offset to all indices does not have a direct effect. 
% Inspired by Streaming LLM~\cite{xiao2024efficient}, we set the first four attention sink tokens $[$0, 1, 2, 3$]$ as the anchor indices. In this way, the position indexes are like $[$0, 1, 2, 3, \texttt{offset}, \texttt{offset}+1, $\cdots]$.
Inspired by Streaming LLM~\cite{xiao2024efficient}, we set the first four attention sink tokens as the anchor indices and only add the random offset to the remaining token index as shown in Figure~\ref{fig:aoi_tree}.
This indexing strategy ensures that all the indices can be sufficiently trained for the draft model. 
In contrast, the vanilla indexing strategy repeatedly trains only the smaller indices and is unable to train those exceeding the training set length.
Meanwhile, anchoring the sink tokens ensures that the target model can approximate the attention sink patterns found in long texts, even with short texts.


Finally, to implement highly efficient tree attention, we propose a new computation method called Hybrid Tree Attention. 
Our insight comes from the discovery that the tree mask in tree-based speculative decoding can be decomposed into two parts, the previously cached part with a chain structure and the speculation part with a tree structure. Specifically, the tree mask is only required between the current queries and the speculation tokens (\emph{i.e.}, current input tokens) to ensure correctness.
So we use \texttt{Flash\_Decoding}~\cite{dao2024flash} to compute the first part efficiently and use a custom Triton kernel \texttt{fused\_mask\_attn} to compute the second part. 
We then combine these components using a log-sum-exp trick, enabling our approach to accelerate tree attention computations up to 4.1 $\times$ compared to previous implementations in Medusa~\cite{cai2024medusa}.

Extensive experiments are conducted to evaluate the effectiveness of \textsc{LongSpec}.
Experiments on five long-context understanding datasets using five LLMs as target models show that our \textsc{LongSpec} can effectively reduce the long-context inference latency, leading to a maximum speedup of 3.26$\times$ compared with the strong baseline model with \texttt{Flash\_Decoding}.
Additional experiments on the long reasoning task AIME24 with the o1-like model QwQ~\cite{qwen2024qwq} further validate the effectiveness of \textsc{LongSpec}, achieving a 2.25$\times$ speedup in wall-clock time.