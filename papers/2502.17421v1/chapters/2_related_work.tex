\vspace{-.3cm}
\section{Related Work}
\vspace{-.1cm}

Speculative decoding offers a promising approach to accelerating LLMs while maintaining the quality of their outputs. Early efforts, such as Speculative Decoding~\cite{xia2023speculative}, SpS~\cite{leviathan2023fast}, BiLD~\cite{kim2024speculative}, and OSD~\cite{liu2024online}, rely on existing smaller LLMs to generate draft sequences. Some other methods aim to improve upon those early efforts~\cite{sun2023spectr, miao2024specinfer, chen2024cascade}. There are also some works using part of the target model as the draft model~\cite{liu2024kangaroo, zhang2024draft, elhoushi2024layerskip}. Retrieval-based speculative decoding methods offer an alternative by utilizing $N$-gram matching rather than relying on smaller models. Examples include Lookahead Decoding~\cite{fu2024break}, REST~\cite{he2024rest}, and Ouroboros~\cite{zhao2024ouroboros}. These approaches bypass the need for additional model training, leveraging pre-existing data patterns to construct draft sequences efficiently.

More recent advancements, including Medusa~\cite{cai2024medusa}, EAGLE~\cite{li2024eagle}, and GliDe~\cite{du2024glide}, have expanded on these foundations by designing specialized draft models and introducing tree-based speculative techniques. These methods leverage customized draft models tailored for speculative decoding, achieving higher efficiency and performance. Additionally, the tree-based approaches employed in these methods allow for more adaptive and parallelizable decoding processes, paving the way for broader applications in real-world systems.

Although speculative decoding has progressed significantly for conventional context lengths, only two existing papers focus on speculative decoding in long-context scenarios. TriForce~\cite{sun2024triforce} introduces a three-layer speculative decoding system that is scalable for long sequence generation. MagicDec~\cite{chen2024magicdec} uses speculative decoding to improve both the throughput and latency of LLM inference. However, these methods mainly utilize the target model with the sparse KV cache as the draft model. The computation-intensive draft models restrict the practical usage of these methods when facing various batch sizes. In contrast, our work focuses on efficiently building a draft model with only one transformer block, achieving more effective performance across different scenarios. 
