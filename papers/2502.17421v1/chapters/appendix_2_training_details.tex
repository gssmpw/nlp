\section{Experiments Details}
\label{appendix:training_details}

All models are trained using eight A100 80GB GPUs. For the 7B, 8B, and 13B target models trained on short-context data, we employ \textsc{LongSpec} with ZeRO-1~\cite{rasley2020deepspeed}. For the 7B, 8B, and 13B models trained on long-context data, as well as for all settings of the 33B target models, we utilize ZeRO-3.  

For the SlimPajama-6B dataset, we configure the batch size (including accumulation) to 2048, set the maximum learning rate to 5e-4 with a cosine learning rate schedule~\cite{loshchilov2017sgdr}, and optimize the draft model using AdamW~\cite{kingma2015adam}. When training on long-context datasets, we adopt a batch size of 256 and a maximum learning rate of 5e-6. The draft model is trained for only one epoch on all datasets.

It is important to note that the primary computational cost arises from forwarding the target model to obtain the KV cache. Recently, some companies have introduced a service known as context caching~\cite{deepseek2024contextcaching, gemini2024contextcaching}, which involves storing large volumes of KV cache. Consequently, in real-world deployment, these pre-stored KV caches can be directly utilized as training data, significantly accelerating the training process.  

For the tree decoding of \textsc{LongSpec}, we employ dynamic beam search to construct the tree. Previous studies have shown that beam search, while achieving high acceptance rates, suffers from slow processing speed in speculative decoding~\cite{du2024glide}. Our research identifies that this slowdown is primarily caused by KV cache movement. In traditional beam search, nodes that do not fall within the top-$k$ likelihood are discarded, a step that necessitates KV cache movement. However, in speculative decoding, discarding these nodes is unnecessary, as draft sequences are not required to maintain uniform lengths. Instead, we can simply halt the computation of descendant nodes for low-likelihood branches without removing them entirely. By adopting this approach, beam search attains strong performance without excessive computational overhead. In our experiments, the beam width is set to $[4, 16, 16, 16, 16]$ for each speculation step. All inference experiments in this study are conducted using float16 precision on a single A100 80GB GPU.  