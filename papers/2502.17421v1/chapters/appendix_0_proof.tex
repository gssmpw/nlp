\section{Correctness for Attention Aggregation}
\label{appendix:attn_aggr}

Because the query matrix $Q$ can be decomposed into several rows, each representing a separate query $q$, we can only consider the output of each row's $q$ after calculating attention with KV. In this way, we can assume that the KV involved in the calculation has undergone the tree mask, which can simplify our proof. We only need to prove that the output $o$ obtained from each individual $q$ meets the requirements, which can indicate that the overall output $O$ of the entire matrix $Q$ also meets the requirements.

\begin{proposition}
    Denote the log-sum-exp of the merged attention as follows:
    \begin{equation*}
        \mathrm{LSE}_{\mathrm{merge}} = \log\Bigl(\exp\bigl(\mathrm{LSE}_{\mathrm{cache}}\bigr) \;+\; \exp\bigl(\mathrm{LSE}_{\mathrm{specs}}\bigr)\Bigr),
    \end{equation*}
    Then we can write the merged attention output in the following way:
    \begin{equation*}
    o_{\mathrm{merge}} = o_{\mathrm{cache}} \cdot \exp\bigl(\mathrm{LSE}_{\mathrm{cache}} - \mathrm{LSE}_{\mathrm{merge}}\bigr) + o_{\mathrm{specs}} \cdot\exp\bigl(\mathrm{LSE}_{\mathrm{specs}} - \mathrm{LSE}_{\mathrm{merge}}\bigr).
    \end{equation*}
\end{proposition}

\begin{proof}
    
A standard scaled dot-product attention for $ q $ (of size $d_{qk}$) attending to $ K_{\mathrm{merge}} $ and $ V_{\mathrm{merge}} $ (together of size $(M+N) \times d_{qk}$ and $(M+N) \times d_v$ respectively) can be written as:

\begin{equation*}
    o_{\mathrm{merge}} = \mha\left(q, K_{\mathrm{merge}}, V_{\mathrm{merge}}\right) =
    \sm\left(
      qK_{\mathrm{merge}}^\top/\sqrt{d_{qk}}
    \right) V_{\mathrm{merge}}.
\end{equation*}

Because $ K $ and $ V $ are formed by stacking $\left(K_{\mathrm{specs}}, K_{\mathrm{cache}}\right)$ and $\left(V_{\mathrm{specs}}, V_{\mathrm{cache}}\right)$, we split the logit matrix accordingly:

\begin{equation*}
    q K_{\mathrm{merge}}^\top / \sqrt{d_{qk}} = 
    \texttt{concat}\Bigl(
    \underbrace{
      q \, K_{\mathrm{cache}}^\top / \sqrt{d_{qk}}
    }_{\mathrm{sub-logits for history}}
    \;, \;
    \underbrace{
      q \, K_{\mathrm{specs}}^\top / \sqrt{d_{qk}}
    }_{\mathrm{sub-logits for new}}
    \Bigr).
\end{equation*}

Denote these sub-logit matrices as:
\[
Z_{\mathrm{cache}} \;=\; q \, K_{\mathrm{cache}}^\top / \sqrt{d_{qk}}
,\;
Z_{\mathrm{specs}} \;=\; q \, K_{\mathrm{specs}}^\top / \sqrt{d_{qk}}.
\]

Each row $i$ of $Z_{\mathrm{specs}}$ corresponds to the dot products between the $i$-th query in $q$ and all rows in $K_{\mathrm{specs}}$, while rows of $Z_{\mathrm{cache}}$ correspond to the same query but with $K_{\mathrm{cache}}$.

In order to combine partial attentions, we keep track of the log of the sum of exponentials of each sub-logit set. Concretely, define:

\begin{equation}
    \mathrm{LSE}_{\mathrm{cache}} = \log\left(\sum\nolimits_{j=1}^{N} \exp\left(Z_{\mathrm{cache}}^{(j)}\right)\right),
    \;
    \mathrm{LSE}_{\mathrm{specs}} = \log\left(\sum\nolimits_{j=1}^{M} \exp\left(Z_{\mathrm{specs}}^{(j)}\right)\right),
    \,
\end{equation}

where $Z_{\mathrm{specs}}^{(j)}$ denotes the logit for the $j$-th element, and similarly for $Z_{\mathrm{cache}}^{(j)}$.

% Then the standard scaled dot-product attention for $ q $ (of size $d_{qk}$) attending to $ K_{\mathrm{cache}} $ and $ V_{\mathrm{cache}} $ (together of size $N \times d_{qk}$ and $N \times d_v$ respectively) can be written as:

% \begin{equation}
%     \label{equ:history_attn}
%     o_{\mathrm{cache}} = \mha\left(q, K_{\mathrm{cache}}, V_{\mathrm{cache}}\right) =
%     \frac{\sum_{j=1}^{N} \exp\left(Z_{\mathrm{cache}}^{(j)}\right) V_{\mathrm{cache}}^{(j)}}{\exp\left(\mathrm{LSE}_{\mathrm{cache}}\right)}.
% \end{equation}

% Similarly, the standard scaled dot-product attention for $ q $ (of size $M \times d_{qk}$) attending to $ K_{\mathrm{specs}} $ and $ V_{\mathrm{specs}} $ (together of size $M \times d_{qk}$ and $M \times d_v$ respectively) can be written as:

% \begin{equation}
%     \label{equ:new_attn}
%     o_{\mathrm{specs}} = \mha\left(q, K_{\mathrm{specs}}, V_{\mathrm{specs}}\right) =
%     \frac{\sum_{j=1}^{M} \exp\left(Z_{\mathrm{specs}}^{(j)}\right) V_{\mathrm{specs}}^{(j)}}{\exp\left(\mathrm{LSE}_{\mathrm{specs}}\right)}.
% \end{equation}

Then $o_{\mathrm{cache}}$ and $o_{\mathrm{specs}}$ can be written as:
\begin{equation}
    \label{equ:split_attn}
    o_{\mathrm{cache}} = \frac{\sum_{j=1}^{N} \exp\left(Z_{\mathrm{cache}}^{(j)}\right) V_{\mathrm{cache}}^{(j)}}{\exp\left(\mathrm{LSE}_{\mathrm{cache}}\right)}, \;
    o_{\mathrm{specs}} = \frac{\sum_{j=1}^{M} \exp\left(Z_{\mathrm{specs}}^{(j)}\right) V_{\mathrm{specs}}^{(j)}}{\exp\left(\mathrm{LSE}_{\mathrm{specs}}\right)}.
\end{equation}

And the whole attention score can be written as:

\begin{equation}
    \label{equ:all_attn}
    o_{\mathrm{merge}} =
    \frac{\sum_{j=1}^{N} \exp\left(Z_{\mathrm{cache}}^{(j)}\right) V_{\mathrm{cache}}^{(j)} + \sum_{j=1}^{M} \exp\left(Z_{\mathrm{specs}}^{(j)}\right) V_{\mathrm{specs}}^{(j)}}{\exp\left(\mathrm{LSE}_{\mathrm{cache}}\right) + \exp\left(\mathrm{LSE}_{\mathrm{specs}}\right)}.
\end{equation}

By aggregating Equation \ref{equ:split_attn} into Equation \ref{equ:all_attn}, we can get the following equation:

\begin{equation}
    o_{\mathrm{merge}} = o_{\mathrm{cache}} \cdot\exp\bigl(\mathrm{LSE}_{\mathrm{cache}} - \mathrm{LSE}_{\mathrm{merge}}\bigr)
     + o_{\mathrm{specs}} \cdot \exp\bigl(\mathrm{LSE}_{\mathrm{specs}} - \mathrm{LSE}_{\mathrm{merge}}\bigr).
\end{equation}

\end{proof}