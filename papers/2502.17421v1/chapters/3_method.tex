\vspace{-.2cm}
\section{Methodology}
% \vspace{-.2cm}
% \subsection{Overview}
% \vspace{-.2cm}

In this section, we present our framework \textsc{LongSpec} for Long-Context Speculative Decoding, which addresses three key challenges:
(1) designing a lightweight draft model architecture with minimal additional memory overhead,
(2) devising the training strategy with anchor-offset indices to handle long contexts effectively,
and (3) implementing a fast tree attention mechanism that leverages tree-based speculation for practical usage.
% We detail each core component in the following subsections.
We detail each core component in the following subsections:
Section~\ref{subsec: arch} introduces our \emph{Memory-Efficient Architecture},
Section~\ref{subsec: Training} explains the \emph{Effective Training Regimes},
and Section~\ref{subsec: tree attention} describes the \emph{Fast Tree Attention} implementation for  inference.

\subsection{Memory-Efficient Architecture}
\label{subsec: arch}
In previous work, the success of the SoTA model EAGLE depends on two factors: (1) the hidden states provided by the target model, and (2) an autoregressive structure. However, an autoregressive draft model inevitably needs to store its own KV cache, which introduces additional overhead in long-context inference requiring large GPU memory. 

To avoid this extra memory overhead, we propose a draft model with constant memory usage regardless of the length of the context. 
As illustrated in Figure~\ref{fig:draft_model}, our model comprises two components: the self-attention module and the following cross-attention module. The self-attention module focuses on modeling local context, while the cross-attention module captures long-context information. 
Because the self-attention module only processes local information, we adopt a sliding-window attention mechanism. Hence, during inference, the self-attention memory footprint does not exceed the window size, which we set to 512. We also provide the theoretical upper bound on the performance degradation caused by the slicing window in Section~\ref{subsec: theoretical analysis}.

For the cross-attention component, inspired by GliDe~\cite{du2024glide}, we leverage the KV cache of the target model. This design not only enables better modeling of previous information but also completely removes additional storage overhead for long contexts, since the large model’s KV cache must be stored regardless of whether or not speculative decoding is employed.
Different from GliDe, we also share the weights of the Embedding Layer and LM Head between the target model and the draft model, which significantly reduces the memory usage for large-vocabulary LLMs such as LLaMA. % Our draft model consistently occupies 0.44GB of VRAM regardless of context length when the target model is LLaMA-3.1-8B.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.85\linewidth]{Figure/1_draft-model.pdf}
    \vspace{-.3cm}
    \caption{Illustration of the memory-efficient draft model. We use sliding window self-attention to capture the local context information and cross attention layer to gather long-context information.}
    \label{fig:draft_model}
    \vspace{-.3cm}
\end{figure}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=1\linewidth]{Figure/1_others.pdf}
    \vspace{-.6cm}
    \caption{Illustration of the Anchor-Offset Indices and the Hybrid Tree Attention. (a) The left section illustrates the differences between the vanilla indexing and the Anchor-Offset Indices. By introducing a randomly selected offset and some anchor indices, the Anchor-Offset Indices enable the short-context training stage to seamlessly integrate with the long-context training stage.
    (b) The right section displays the Hybrid Tree Attention, which combines the advantages of \texttt{Flash\_Decoding} and our Triton-implemented Attention.}
    \label{fig:aoi_tree}
    \vspace{-.3cm}
\end{figure*}


\subsection{Effective Training Regimes}
\label{subsec: Training}
\textbf{Anchor-Offset Indices.}
With vanilla position indices, which consist of successive integers starting from $0$, those indices appearing earlier in sequences occur more frequently than larger position indices~\cite{an2025does}, as shown in the Figure~\ref{fig:aoi_tree} upper left part.
Consequently, larger position indices receive insufficient training updates, which leads to a training inference discrepancy.
%Meanwhile, during short-text training, the draft model's cross-attention queries never encounter the KV cache with the target model's from large position indices. 
A common method to solve this problem is RoPE-based extrapolation, which trains the model with a smaller RoPE base and extends the RoPE base with interpolation for longer contexts~\cite{gao2024train, liu2024scaling, peng2024yarn}. 
However, directly using these methods will cause an inconsistent problem in draft model training.
To leverage the target model’s KV cache, our draft model must keep the RoPE base the same as the target model.
Based on our exploratory experiments, the inconsistent RoPE base between the target model and the draft model will cause a significant collapse of the cross-attention layer, which makes the draft model's long-text capability degrade.
The consistency requirement of RoPE base in the draft model limits the usage of methods like RoPE-based extrapolation, which requires flexible adjustments to the RoPE base.

Instead, we tackle this challenge by leveraging carefully designed indices. These indices must ensure that (1) the position indices in the draft model can be sufficiently trained using short-context data and (2) the indices would not cause the target model to exhibit out-of-distribution behavior because the target model shares the same indices as the draft model during training.

To satisfy these constraints, we propose the Anchor-Offset Indices strategy.
Specifically, we reserve the first four positions $[0,1,2,3]$ as attention sink tokens~\cite{xiao2024efficient}, then assign all subsequent tokens to large consecutive indices starting at a random offset (\emph{e.g.}, $[0,1,2,3,8192,8193,8194,\dots]$). 
The anchor indices and random offset ensure that every position index can be sufficiently trained, addressing the limitation of the vanilla one that repeatedly trains only smaller indices. 

Additionally, according to \citet{xiao2024efficient}, LLM exhibits an \textit{attention sink} phenomenon when dealing with long texts, which means the attention weights primarily concentrate on the first four tokens and the recent tokens. 
Therefore, we believe that utilizing Anchor-Offset Indices can naturally lead the target model to exhibit in-distribution behavior.
In our experiments, adopting these indices in the target model only increases the loss by approximately 
0.001, indicating that the target model is indeed well-suited to such changes.  
We also provide the theoretical upper bound of the distribution shift error in Section~\ref{subsec: theoretical analysis}.
% \textbf{Anchor-Offset Indexing.}
% To leverage the target model’s KV cache in cross-attention, our draft model must share the same (large) RoPE base as the target model from the outset. 
% This requirement departs from typical long-context LLM procedures, where one first trains on short-context data with a smaller RoPE base and extends base in the next stage with long-context data. 
% By contrast, our method demands that both models immediately adopt a large RoPE base, thereby introducing two key challenges in short-text training.

% First, when the training data primarily consists of short sequences, the frequency of position index \( n \) is always higher than that of \( n+1 \). Consequently, higher position embeddings receive fewer gradient updates, weakening the model’s ability to generalize to long contexts. 
% Second, because short-text training rarely invokes large position indices, the draft model’s cross-attention queries encounter insufficient examples of the target model’s high-indexed keys and values, making it harder to learn robust long-context behaviors.

% To address these issues, we propose an \emph{Anchor-Offset Indexing} strategy, inspired by \citet{xiao2024efficient}. Specifically, we reserve the first four positions \([0,1,2,3]\) as attention sinks \cite{xiao2024efficient}, then assign subsequent tokens to large consecutive indices beginning at a randomly selected offset (\emph{e.g.}, \([0,1,2,3,8192,8193,8194,\dots]\)). 
% Anchoring these initial positions allows the model to approximate long-context attention even with short data, while randomizing the offset ensures that all positions in the RoPE base receive sufficient training updates. 


%Anchor-Offset Indices exploit the large RoPE embeddings effectively, without altering the RoPE base itself, and endows the draft model with robust long-context capabilities despite predominantly short-text training.
\textbf{Flash Noisy Training.} During training our draft model leverages the KV caches from a large model, while these KV caches are not always visible during inference. 
This is because the large model only updates its KV cache upon verification completion. Concretely, for the cross-attention query \(Q_t\) in the draft model, we can only guarantee access to the corresponding key-value states \(K_{<t'}\), \(V_{<t'}\) satisfying
$ 1\le|t' - t|<\gamma$,
where \(\gamma\) is the number of speculative steps.

To ensure consistency between training and inference, a straightforward solution would be to add an attention mask~\cite{du2024glide}. However, this method is incompatible with Flash Attention~\cite{dao2023flash}, which would significantly degrade training speed and cause prohibitive memory overhead, particularly in long-context training scenarios. Therefore, we propose a technique called \textbf{flash noisy training}. During training, we randomly shift the indices of queries and key-value states with $1 \le j < \gamma$. 
% We compute
% \[
%     o_{j:} 
%     \;=\; \mathrm{flash\_attn}\bigl(q_{j:}, \,k_{:-j}, \,v_{:-j}\bigr).
% \]
Suppose the sequence length is $l$, then we compute
\[
    O_{\geq j} 
    \;=\; \mathrm{flash\_attn}\bigl(Q_{\geq j}, \,K_{< l-j}, \,V_{< l-j}\bigr).
\]
In this way, we effectively simulate the same visibility constraints as in the inference phase, \emph{i.e.}, 
$ 1\le|t' - t|<\gamma$,
thereby aligning the behavior at training time with the inference behavior.

\subsection{Fast Tree Attention}

\label{subsec: tree attention}
Tree Speculative Decoding~\cite{miao2024specinfer} leverages prefix trees and the causal structure of LLMs so that a draft model can propose multiple candidate sequences, while the target model only needs to verify them once, without altering the final results. 
In this process, \emph{Tree Attention} plays a key role in ensuring both correctness and efficiency. Early works~\citep{cai2024medusa, li2024eagle} apply attention masks derived from prefix trees to the $QK^\mathsf{T}$ attention matrix, thus disabling wrong combinations between speculation tokens.
However, these methods only run on PyTorch’s eager execution mode, precluding more advanced attention kernels (\emph{e.g.}, \texttt{Flash\_Decoding}).
As a result, the inference speed decreases significantly when the sequence length increases.

To address these performance bottlenecks, we propose a \textbf{Hybrid Tree Attention} mechanism, as illustrated in Figure~\ref{fig:aoi_tree}. Our method is based on two key insights: 1) When performing Tree Attention, the queries and the cached key-value pairs $\{K_{\mathrm{cache}}, V_{\mathrm{cache}}\}$ do not require additional masks; 2) Only the queries and the key-value pairs $\{K_{\mathrm{specs}}, V_{\mathrm{specs}}\}$ from the current speculative tokens need masking, and the number of such speculative tokens is typically no more than 128.
Based on these observations, we adopt a divide and aggregate approach that splits the attention computation into two parts and merges them afterward.

\textbf{Splitting Key-Value Pairs.}
We partition all key-value pairs into two groups: $\{K_{\mathrm{cache}}, V_{\mathrm{cache}}\}$: the cached part of the main sequence, which requires no attention mask;
and  $\{K_{\mathrm{specs}}, V_{\mathrm{specs}}\}$: the speculative-stage part, which needs attention masks. For $\{K_{\mathrm{cache}}, V_{\mathrm{cache}}\}$, we invoke the efficient \texttt{Flash\_Decoding} kernel. For $\{K_{\mathrm{specs}}, V_{\mathrm{specs}}\}$, we use our custom Triton kernel \texttt{fused\_mask\_attn}, which applies blockwise loading and masking in the KV dimension, enabling fast computation of attention.
This step yields two sets of attention outputs $\{O_{\mathrm{cache}}, O_{\mathrm{specs}}\}$ along with their corresponding denominators (\emph{i.e.}, log-sum-exp of all attention scores) $\{\mathrm{LSE}_{\mathrm{cache}}, \mathrm{LSE}_{\mathrm{specs}}\}$.

\textbf{Aggregation.}
We then combine these two parts into the final attention output $O_{\mathrm{merge}}$ via a log-sum-exp trick. First, we compute
\[
\begin{aligned}
\mathrm{LSE}_{\mathrm{merge}}
&= \log\Bigl(\exp\bigl(\mathrm{LSE}_{\mathrm{cache}}\bigr) \;+\; \exp\bigl(\mathrm{LSE}_{\mathrm{specs}}\bigr)\Bigr),
\end{aligned}
\]
and then apply a weighted summation to the two outputs:
\[
\begin{aligned}
O_{\mathrm{merge}}
&= O_{\mathrm{cache}} \;\cdot\; \exp\bigl(\mathrm{LSE}_{\mathrm{cache}} - \mathrm{LSE}_{\mathrm{merge}}\bigr)
\\
&\quad +\; O_{\mathrm{specs}} \;\cdot\; \exp\bigl(\mathrm{LSE}_{\mathrm{specs}} - \mathrm{LSE}_{\mathrm{merge}}\bigr).
\end{aligned}
\]
The theoretical guarantee is provided in Appendix~\ref{appendix:attn_aggr}.
As outlined above, this hybrid approach employs the highly efficient \texttt{Flash\_Decoding} kernel for most of the computations in long-sequence inference and only uses a custom masking attention \texttt{fused\_mask\_attn} for the small number of speculative tokens. 
The kernel \texttt{fused\_mask\_attn} follows the design philosophy of Flash Attention 2~\cite{dao2023flash} by splitting \(Q\), \(K_{\text{specs}}\), and \(V_{\text{specs}}\) into small blocks. This strategy reduces global memory I/O and fully leverages GPU streaming multiprocessors. Furthermore, for each block in the computation of \(QK_{\text{specs}}^\top\), the mask matrix is loaded and used to apply the masking operation.
The Hybrid Tree Attention effectively balances the parallel verification of multiple branches with improved inference speed, all without compromising the correctness.

\subsection{Theoretical Analysis}
\label{subsec: theoretical analysis}
% {\color{red} 
Here we would like to provide the theoretical analysis of our method. Before the statement of our results, we would like to define some quantities. First, for the memory-efficient architecture, we denote the sum of the attention score outside the window as $\varepsilon$, which should be small due to the locality of the language. Second, for our index offset technique, we denote the distribution of the offset as $\tilde{P}_t$. In addition, we denote the \emph{true} distribution of the index of the first token in the window as $P_t$. Finally, we assume that the GliDe function is trained on $N$ i.i.d.\ samples, and the Frobenius norms of all the parameters are upper bounded by $B$. % Then we have that
\begin{theorem}[Informal]\label{thm:informal}
    Under regularity assumptions, the inference error between the \ac{mle} trained by our method and the optimal GliDe parameter that take all tokens as inputs is upper bounded by
    \looseness=-1
    \begin{align*}
        \rmA\rmE + \rmD\rmE +\rmG\rmE
    \end{align*}
    with probability at least $1-\delta$. Here the \emph{approximation error} is $\rmA\rmE=(1+d_V\exp(B))HB^{4}(1+B_{X}^2B^2)\varepsilon$, the \emph{distribution shift error} is $\rmD\rmE=\log (1+d_V\exp(B))\cdot \tv(\tilP_t,P_t)$, and the \emph{generalization error} is $\rmG\rmE=N^{-1/2}(d(d+d_V)\log (1+NHB^6)+\log\delta^{-1})$.
\end{theorem}
The formal statement and the proof are provided in Appendix~\ref{app:theory}. The approximation error results from that we adopt a sliding window instead of using all tokens. This error is proportional to the attention score outside the window. The distribution shift error results from the positional embedding distributional shift. In our experiments, we set $\tilde{P}_{t}$ as the uniform distribution, which will have a smaller distribution shift than not adopting this position offset technique. The generalization error results from that we use $N$ samples to train the model.
% }






