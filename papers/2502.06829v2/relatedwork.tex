\section{Related Work}
% All submissions must follow the specified format.
\begin{figure*}[htbp]
  \centering
  \includegraphics[width=0.9\textwidth]{model4.pdf}
  \caption{The framework of Convolution-Based Converter (CBC) consists of four successive parts:(a) a input initial stochastic process ,(b) a constructor that establishes preliminary dependency  (c) A Convolution-Based Converter that constructs the dependencies among random variables in the stochastic process (d) The output expected stochastic process. The modeling effect is shown in (d2), where (d1) represents restricting the output space to the [0,1] range.}
  \label{fig:model}
  % In section (c), we can replace the normal convolution-converter with a smooth convolution-converter to accommodate the modeling of smooth stochastic processes.
\end{figure*}
% \subsection{Dimensions}
\noindent \textbf{SDE-based Models}   Stochastic Differential Equations-based methods (SDEs) are classical approaches for modeling stochastic processes. They utilize explicit differential equations, comprising a drift term and a diffusion term, to describe the instantaneous changes of a stochastic process \cite{oksendal2013stochastic}\cite{kloeden1992stochastic}.
% The drift term captures the average trend of the process and is often represented by a deterministic or state-dependent function. The diffusion term, on the other hand, simulates random fluctuations; classical SDE models often use the diffusion term of Brownian motion as a source of noise [Gardiner, 2009; Risken, 1996]. 
Researchers have developed various modeling strategies based on the SDEs framework, including linear \cite{arminger1986linear}, nonlinear \cite{overgaard2005non}, and jump-diffusion SDEs \cite{jiang2019multifractal}. Key techniques, such as parameter estimation \cite{nielsen2000parameter}, numerical solutions \cite{burrage2004numerical}, and stochastic control \cite{nisio2015stochastic}, have also been extensively explored. These methodological advancements have established SDEs as a fundamental and versatile tool in stochastic process modeling. However, while the theory of SDEs is elegant and capable of describing a wide range of stochastic processes, SDE-based methods oversimplify the real problems by assumptions about system dynamics and noise distributions \cite{przybylowicz2022efficient}. These assumptions are often simplified to ensure analytical tractability, potentially leading to significant deviations from real-world complexities \cite{oksendal2013stochastic}, thus limiting the model's accuracy and generalization ability. Furthermore, SDEs often face difficult-to-solve problems, especially for high-dimensional or nonlinear systems \cite{platen1999introduction}.

\noindent \textbf{Markov-based Models}   Markov-based models, such as Markov chains 
 \cite{chung1967markov} and Hidden Markov Models (HMMs) \cite{eddy1996hidden}, explicitly model state transition probabilities, effectively capturing the dynamic evolution of stochastic processes over time and thus modeling processes with temporal properties. Discrete-time Markov chains \cite{gomez2010discrete} directly represent the conditional probabilities of state transitions at each time step, offering a tractable method for modeling sequence-dependent stochastic processes \cite{craig2002estimation}, typically expressed as $P(X_{i+1}|X_i)$. HMMs further extend this capability by introducing hidden states, allowing the model to capture more complex observation sequences in which observable data are generated based on the evolution of unobserved hidden states.
However, Markov-based models assume that future states depend solely on the current state, making it difficult to capture long-range dependencies and complex dynamics 
\cite{rabiner1989tutorial}. Moreover, the accuracy of Markov models heavily relies on having sufficient data. When data are limited, the estimation of state transition probabilities may become unreliable, thereby affecting both predictive performance and generalization.

\noindent \textbf{Gaussian Processes-based Models}   Gaussian processes (GPs) serve as a conditional probability estimation method by assuming that every finite subset of random variables in the stochastic process follows a multivariate Gaussian distribution 
 \cite{seeger2004gaussian}, we can write a Gaussian process as $GP \sim (m(i),K(i,i'))$, where $m$ is the mean function and $K$ is covariance function, 
% respectively. In the absence of prior information, a common choice for $m$ (often is the zero constant function. The covariance function must yield a valid covariance matrix, which is achieved by restricting $k$ to be a kernel function. Typical examples include the linear kernel, the radial basis function (RBF) kernel[][][], the spectral mixture (spectral) kernel[][][]
Gaussian Process stands as the a CDE approach for modeling stochastic process. It operates under the assumption that each random variable distribution is Gaussian, employing carefully chosen mean and covariance functions to define suitable Gaussian priors for data fitting \cite{williams1995gaussian}. However, this strong reliance on Gaussian assumptions restricts the model’s applicability primarily to data that closely follow Gaussian distributions, thereby severely limiting its flexibility. To address these limitations, various refinements have been introduced, such as Warped Gaussian Processes (WGP) 
\cite{lazaro2012bayesian} that nonlinearly transform Gaussian distributions using activation functions, and t-processes \cite{shah2014student} as well as their variants \cite{jylanki2011robust} \cite{chen2020multivariate} that replace Gaussian distributions with Student-t distributions. Despite these advancements, none has fully circumvented the inherent dependence on specific prior distributional forms.

\noindent \textbf{Neural network–based CDE methods}   Neural network–based CDE methods offer a flexible alternative for modeling stochastic processes by directly learning conditional distributions from data. This approach circumvents the strong prior assumptions typical of Gaussian processes, enabling the modeling of a broader range of distribution shapes. Representative frameworks include Deconvolutional Density Networks (DDN) \cite{chen2022deconvolutional} and Mixture Density Networks (MDN) \cite{bishop1994mixture}, which can capture complex multimodal, heavy-tailed, or other behaviors. However, these methods often require large training datasets and careful network design or hyperparameter tuning.
\begin{figure*}[htbp]
  \centering
  % # [width=0.8\textwidth]
  \includegraphics[width=0.8\textwidth]{model8.pdf}
  \caption{Smooth Convolution-Converter:the framework is as the same as the Fig\ref{fig:model},the difference is replacing the base Convolution-Converter with a Smooth Convolution-Converter to accommodate the modeling of smooth stochastic processes,and d(3) is the expected stochastic process}
  
  \label{fig:Deconv}
\end{figure*}
% \subsection{Title}