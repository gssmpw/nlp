\section{Related Work}
% All submissions must follow the specified format.
\begin{figure*}[htbp]
  \centering
  \includegraphics[width=0.9\textwidth]{model4.pdf}
  \caption{The framework of Convolution-Based Converter (CBC) consists of four successive parts:(a) a input initial stochastic process ,(b) a constructor that establishes preliminary dependency  (c) A Convolution-Based Converter that constructs the dependencies among random variables in the stochastic process (d) The output expected stochastic process. The modeling effect is shown in (d2), where (d1) represents restricting the output space to the [0,1] range.}
  \label{fig:model}
  % In section (c), we can replace the normal convolution-converter with a smooth convolution-converter to accommodate the modeling of smooth stochastic processes.
\end{figure*}
% \subsection{Dimensions}
\noindent \textbf{SDE-based Models}   Stochastic Differential Equations-based methods (SDEs) are classical approaches for modeling stochastic processes. They utilize explicit differential equations, comprising a drift term and a diffusion term, to describe the instantaneous changes of a stochastic process  Feynman, "Path Integrals and Quantum Mechanics" .
% The drift term captures the average trend of the process and is often represented by a deterministic or state-dependent function. The diffusion term, on the other hand, simulates random fluctuations; classical SDE models often use the diffusion term of Brownian motion as a source of noise [Gardiner, 2009; Risken, 1996]. 
Researchers have developed various modeling strategies based on the SDEs framework, including linear Gardiner, "Handbook of Stochastic Methods" and nonlinear Bishoff et al., "Nonlinear Filtering Theory" , and jump-diffusion SDEs Rogers and Williams, "Diffusions, Markov Processes and Martingale". Key techniques, such as parameter estimation Kushner, "Stochastic Approximation: A Dynamical Systems Viewpoint" , numerical solutions Kloeden and Platen, "The Numerical Solution of Stochastic Differential Equations" , and stochastic control Fleming and Soner, "Controlled Markov Processes and Viscosity Solutions" . These methodological advancements have established SDEs as a fundamental and versatile tool in stochastic process modeling. However, while the theory of SDEs is elegant and capable of describing a wide range of stochastic processes, SDE-based methods oversimplify the real problems by assumptions about system dynamics and noise distributions , thus limiting the model's accuracy and generalization ability. Furthermore, SDEs often face difficult-to-solve problems, especially for high-dimensional or nonlinear systems .

\noindent \textbf{Markov-based Models}   Markov-based models, such as Markov chains  Kemeny et al., "Denumerable Markov Chains" , and Hidden Markov Models (HMMs) Rabiner, "A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition" , explicitly model state transition probabilities, effectively capturing the dynamic evolution of stochastic processes over time and thus modeling processes with temporal properties. Discrete-time Markov chains  Kemeny et al., "Denumerable Markov Chains" directly represent the conditional probabilities of state transitions at each time step, offering a tractable method for modeling sequence-dependent stochastic processes , typically expressed as $P(X_{i+1}|X_i)$. HMMs further extend this capability by introducing hidden states, allowing the model to capture more complex observation sequences in which observable data are generated based on the evolution of unobserved hidden states.
However, Markov-based models assume that future states depend solely on the current state, making it difficult to capture long-range dependencies and complex dynamics .

\noindent \textbf{Gaussian Processes-based Models}   Gaussian processes (GPs) serve as a conditional probability estimation method by assuming that every finite subset of random variables in the stochastic process follows a multivariate Gaussian distribution  Rasmussen and Williams, "Gaussian Processes for Machine Learning" , we can write a Gaussian process as $GP \sim (m(i),K(i,i'))$, where $m$ is the mean function and $K$ is covariance function, respectively. In the absence of prior information, a common choice for $m$  is the zero constant function. The covariance function must yield a valid covariance matrix, which is achieved by restricting $k$ to be a kernel function. Typical examples include the linear kernel, the radial basis function (RBF) kernel , the spectral mixture (spectral) kernel .
Gaussian Process stands as the a CDE approach for modeling stochastic process. It operates under the assumption that each random variable distribution is Gaussian, employing carefully chosen mean and covariance functions to define suitable Gaussian priors for data fitting . However, this strong reliance on Gaussian assumptions restricts the model’s applicability primarily to data that closely follow Gaussian distributions, thereby severely limiting its flexibility. To address these limitations, various refinements have been introduced, such as Warped Gaussian Processes (WGP)  Duvenaud et al., "Automatic Model Construction with Gaussian Processes" that nonlinearly transform Gaussian distributions using activation functions, and t-processes  Kent, "A Tutorial on the Multivariate Student-t Distribution" as well as their variants  and ,  that replace Gaussian distributions with Student-t distributions. Despite these advancements, none has fully circumvented the inherent dependence on specific prior distributional forms.

\noindent \textbf{Neural network–based CDE methods}   Neural network–based CDE methods offer a flexible alternative for modeling stochastic processes by directly learning conditional distributions from data. This approach circumvents the strong prior assumptions typical of Gaussian processes, enabling the modeling of a broader range of distribution shapes. Representative frameworks include Deconvolutional Density Networks (DDN)  Nair et al., "Deconvolutional Neural Fields" and Mixture Density Networks (MDN) , which can capture complex multimodal, heavy-tailed, or other behaviors. However, these methods often require large training datasets and careful network design or hyperparameter tuning.
\begin{figure*}[htbp]
  \centering
  % # [width=0.8\textwidth]
  \includegraphics[width=0.8\textwidth]{model8.pdf}
  \caption{Smooth Convolution-Converter:the framework is as the same as the Fig\ref{fig:model},the difference is replacing the base Convolution-Converter with a Smooth Convolution-Converter to accommodate the modeling of smooth stochastic processes,and d(3) is the expected stochastic process}

  
  \label{fig:Deconv}
\end{figure*}