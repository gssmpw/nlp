\section{Background and Related Work}
\label{sec:background}

In this section, we present related work on sentiment analysis tools in general, voting classifiers and on SE specific data sets used for sentiment analysis.

\subsection{Sentiment Analysis}

For the field of software engineering, several sentiment analysis tools have been developed or evaluated.


Calefato et al. ____ developed the tool Senti4SD and thus enabled training and classification of models specific to the domain of SE. To do this, they used a specialized word lexicon in combination with a support-vector machine. With this approach, they were able to classify an input document in one of the three polarities \textit{positive, negative, and neutral}.

Zhang et al. ____ compared the performance of different pre-trained neural network models with those tools using more classical machine learning approaches (but without training and thus without adapting them for each domain). These included four neural network models like RoBERTa ____, and five other existing tools (e.g., Senti4SD ____). They found that the performance of these tools changes depending on the test data set ____. They observed that the RoBERTa model ____ most often had the highest scores on average among the pre-trained transformer models.

Novielli et al. ____ investigated in their cross-platform study to what degree tools that have been trained in one domain perform in another unknown domain. They used three data sets and four tools and concluded that supervised tools perform significantly worse on unknown domains and that in these cases a lexicon-based tool performs better across all domains.

In their replication study, Novielli et al. ____ explained some sentiment analysis tools (e.g. Senti4SD ____) in great detail and described the underlying data. They also investigate the agreement between sentiment analysis tools with each other and with manual annotations with a gold standard of 600 documents. Based on their results, they suggest platform-specific tuning or retraining for sentiment analysis tools ____.

All these mentioned tools perform well within one domain, but significantly worse in cross-platform domains ____. One possibility to counteract this is to use a combination of several tools. To the best of our knowledge, this approach has not yet been used for a cross-platform settings.

\subsection{Voting Classifier}

To the best of your knowledge, the concept of majority voting has been applied in the context of sentiment analysis in SE in only three papers.

Herrmann and Kl√ºnder ____ applied a voting classifier (SEnti-Analyzer) consisting of three machine learning methods for German texts especially for meetings ____. They used three different machine learning algorithms and an evolutionary algorithm.

Uddin et al. ____ used different sentiment analysis tools in a majority voting ensemble. They combined tools such as Senti4SD ____, or RoBERTa ____ in an ensemble and investigated whether majority voting can improve the performance of this ensemble compared to the best individual tool. In doing so, they combined several data sets into one large benchmark data set. Overall, they conclude that while ensembles can outperform the best individual tool in some cases, they cannot outperform it overall. 

However, neither paper specifically examined how tools trained in several different domains perform together in a cross-platform setting.



\subsection{SE Data Sets for Sentiment Analysis}
\label{sec:dataset}

Several papers highlight the need of domain adaptation to the field of SE (e,g., ____), leading to some SE specific data sets. Recent SMSs and SLRs about sentiment analysis in SE show an overview of the data sets used ____

Novielli et al. ____ collected 4,800 questions asked on the question-and-answer site Stack Overflow and assigned emotions to each sentence of the collected communication. Afterwards, they labeled these sentences based on these emotions with three polarities \textit{positive}, \textit{negative} and \textit{neutral}. This labeling process was done by a majority decision of three raters.

Another gold standard data set crawled from GitHub was developed by Novielli et al. ____. This data set contains over 7,000 sentences. Similar to the Stack Overflow data set ____, they first assigned emotions to each sentence and labeled polarities based on these emotions.

Ortu et al. ____ developed a data set consisting of about 6,000 comments crawled from four public JIRA projects. They assigned each statement an emotion label based on the Parrott emotion model ____.

Lin et al. ____ collected 1,500 discussions on Stack Overflow tagged with Java. Five authors labeled the data supported by a web application they built. In their paper no emotion model or guidelines for labeling were mentioned.

Uddin et al. ____ developed the API data set. It consists of 4,522 sentences from 1,338 Stack Overflow posts regarding API aspects. The authors did not follow an emotion model or any guidelines, but in their coding guide, an example sentence was mentioned for each polarity with focus on the opinion expressed in the text. 

The APP reviews data set, labeled by Lin et al. ____, consists of 341 mobile app reviews. No emotion model or guidelines for labeling are mentioned.