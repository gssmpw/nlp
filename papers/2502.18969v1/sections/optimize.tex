
\section{How are we \emph{optimizing} the fit?}\label{sec:opt}
\input{tables/optimization_details}

The optimization of a power law requires several design decisions, including optimizer, loss, initialization values, and bootstrapping. We discuss each in this section. Over half of the papers we analyze do not provide any information about their power law fitting process, or provide limited information only and fail to detail crucial aspects. Specifically, many papers fail to describe their choice of optimizer or loss function. In Table \ref{tab:opt-details}, we provide an overview of the optimization details (if specified) for each paper considered.

% papers in our study provide details on the optimization process.

% \citep{kaplan2020scaling,henighan2020scaling,droppo2021scaling,neumann2022scaling,bansal2022data,frantar2023scaling,aghajanyan2023scaling,hu2024minicpm,covert2024scaling,ruan2024observational}..

\paragraph{Optimizer}
% Each optimizer is subject to its own set of limitations. Some are more data-hungry, others more compute-intensive, more sensitive to initialization, limited to linear relations, or otherwise better suited for certain applications.


Power laws are most commonly fit with a variety of algorithms designed to optimize non-linear functions. One of the most common is the BFGS (Broyden-Fletcher-Goldfarb-Shanno) algorithm, or a variation L-BFGS \citep{liu1989limited}.
% , which decreases memory usage at the cost of accurate updates, but may better avoid local minima. 
%\citep{jones2021scaling,clark2022unified,hoffmann2022training,aghajanyan2023scaling,tao2024scaling,muennighoff2024scaling}
Some papers \citep{hashimoto2021model,covert2024scaling} use Adam, Adagrad, or other optimizers common in machine learning, such as AdamW, RMSProp, and SGD. Though effective for LLM training, these are sometimes ill-suited for the purpose of fitting a scaling law, due to various factors limiting their practicality, such as data-hungriness. \citet{goyal2024scaling} forgo the use of an optimizer altogether due to instability of solutions (see initialization) and rely exclusively on grid search to fit their scaling law parameters.

Some scaling law works \citep{rosenfeld2019constructive} opt to use a linear method, such as linear regression, which is generally much simpler. To do this, they typically convert the hypothesized power law to a linear form by taking the log. For example, for a power law $y^b = c \cdot x^a + d$, use the form $\beta \cdot log(y) = \gamma + \alpha \cdot log(x)$ instead. For loss prediction, this results in a form similar to $log(L(N, D)) = \alpha \cdot logN + \beta \cdot logD + E$. This trick is employed even when using an optimizer capable of operating on non-linear functions \citep{hashimoto2021model}. Though this conversion may sometimes work in practice, it is generally not advised because the log transformation also changes the distribution of errors, exaggerating the effects of errors at small values. This mismatch increases the likelihood of a poor fit \citep{goldstein2004problems}. We found this approach to be very common among the papers we study.


\paragraph{Loss}
Various loss functions have been chosen for power law optimization, including variants on MAE (mean absolute error), MSE (mean squared error), and the Huber loss \citep{huber1992robust}, which is identical to MSE for errors less than some value $\delta$ (a hyperparameter), but grows linearly, like MAE, for larger errors, effectively balancing the weighting of small errors with robustness to outliers. Of the papers which specify their loss function, most use a variant of MAE \citep{ghorbani2021scaling}, MSE \citep{goyal2024scaling,hilton2023scaling}, Huber loss \citep{hoffmann2022training,aghajanyan2023scaling,frantar2023scaling,tao2024scaling,muennighoff2024scaling}, or a custom loss \citep{covert2024scaling}. % The result of choosing a particular loss function is dependent on the data models

\paragraph{Initialization}

Initialization can have a substantial impact on final optimization fit (\S\ref{sec:own-repl}). One approach is to iteratively train with different initializations, selecting the best fit at the termination of the search. This is typically a grid search over choices for each parameter \citep{aghajanyan2023scaling,muennighoff2024scaling}, or a random sample from that grid \citep{frantar2023scaling,tao2024scaling}. Alternatively, the full grid of potential initializations can be evaluated on the loss function without training, and the most optimal $k$ used for initialization and optimization \citep{caballero2022broken}. Finally, if a hypothesis exists, either from prior work or expert knowledge about the function, this hypothesis may be used instead of a search, or to guide the search \citep{besiroglu2024chinchilla}.


\paragraph{Validating the Scaling Law} A majority of the papers surveyed do not report validating the scaling law in any meaningful way. Knowing this is critical to understanding whether the results of the scaling laws study are valid, given the examples given throughout the paper of scaling laws study conclusions changing depending on the process details. \citet{porian2024resolving} and \citet{alabdulmohsin2022revisiting} use confidence intervals and goodness of fit measures to validate their scaling laws. \citet{ghorbani2021scaling} and \citet{bansal2022data} also do this. Otherwise, a majority of the papers that we report as validating their scaling laws mainly extrapolate to models a few orders of magnitude larger and observe the adherence to the scaling law obtained. 

% \paragraph{Bootstrapping} 

% Data for this optimization process can be prohibitively expensive, as each data point frequently corresponds to a full model training run. The costs of such runs generally grow superlinearly with the compute budget. To increase the size of the dataset, it is common to bootstrap. 

% \ml{Consider a validation/confidence intervals section here}

% \section{Implication of Differences}\label{sec:diffs}

% - implication of difference:
%     - power law form (e.g., exponentiation, error term) is different
%     - FLOPs vs datapoints vs n(parameters)
%     - FLOPs* is an estimate (6ND versus calculating carefully)
%     - are we counting embedding params or not
%     - noisiness of results from e.g., bad hyperparams?