
\section{Papers on Scaling Laws}\label{sec:related-work}

% \srk{resisting the feminine urge to cite vapnik}
% \subsection{}

Researchers have proposed scaling laws to study the scaling of deep learning across multiple domains and for several tasks. Studies of the scaling properties of generalization error with training data size and model capacity predate modern deep learning. \citet{banko2001scaling} observed\ a power law scaling of average validation error on a confusion set disambiguation task with increasing dataset size. The authors also claimed that the model size required to fit a given dataset grows log linearly. As for larger scale models, \cite{amodei2016deep} observe a power-law WER improvement on increasing training data for a 38M parameter Deep Speech 2 model. \cite{hestness2017deep} show similar power law relationships across several domains such as machine translation, language modeling, image processing and speech recognition. Moreover, they find that these exponential relationships found hold across model improvements. 

\cite{kaplan2020scaling} push the scale of these studies further, studying power laws for models up to 1.5B parameters trained on 23B tokens to determine the optimal allocation of a fixed compute budget. Later studies \citep{hoffmann2022training,hu2024minicpm} revisit this and find that \cite{kaplan2020scaling} greatly underestimate the amount of data needed to train models optimally, though major procedural differences render it challenging to attribute the source of this discrepancy. Since then, researchers have studied various aspects of scaling up language models. \cite{wei2022emergent} examine the emergence of abilities with scale that are not present in smaller models, while \cite{hernandez2021scaling} study the scaling laws for transfer between distributions in a finetuning setting. \cite{henighan2020scaling} consider possible interactions between different modalities while recently, \cite{aghajanyan2023scaling} study scaling in multimodal foundation models. \cite{tay2022scaling} show that not all architectures scale equally well, highlighting the importance of using scaling studies to guide architecture development. \cite{poli2024mechanistic} scale hybrid architectures like Mamba \citep{gu2023mamba}, showing the efficacy of this new model family. Other researchers also formulate specific scaling laws to study other Transformer based architectures. For example, \cite{clark2022unified} and \cite{frantar2023scaling} introduce new scaling laws to study mixture of expert models \citep{fedus2022switch,shazeer2017outrageously} and sparse models \citep{zhu2017prune} respectively. Researchers have also used scaling laws to study encoder-decoder models for neural machine translation \citep{ghorbani2021scaling,gordon2021data}, and the effect of data quality and language on scaling coefficients \citep{bansal2022data,zhang2022examining}. While language models form the majority of the papers surveyed, we also consider papers that study VLMs \citep{cherti2023reproducible,henighan2020scaling}, vision \citep{alabdulmohsin2022revisiting,zhai2022scaling}, reinforcement learning \citep{hilton2023scaling,jones2021scaling, gao2023scaling} and recommendation systems \citep{ardalani2022understanding}. We further discuss different forms of scaling laws researchers introduce for the specific research questions they wish to answer in Section \ref{sec:power-law-form}. 
% We list the domains/tasks of the papers we surveyed in Table \ref{tab:summary}) \ml{this isn't the right table anymore}.
% \srk{add more cited examples based on updated spreadsheet}

A majority of the surveyed papers study Transformer \citep{vaswani2017attention} based models, but a few consider different architectures. For example, \cite{sorscher2022beyond} investigate data pruning laws in ResNets, and some smaller scale studies use MLPs or SVMs \citep{hashimoto2021model}. This overrepresentation is perhaps partially a result of Transformer-based models achieving higher scale than other architectures; a ResNet101 has 44M parameters, while the largest Llama 3 model has 405B.

\paragraph{Replication Efforts} \citet{besiroglu2024chinchilla} seek to reproduce the parameter fitting approach used by \citet{hoffmann2022training}. They are unable to recover the scaling law from \citet{hoffmann2022training}, and demonstrate that the claims of the original paper are inconsistent with descriptions of the setup. They then seek to improve the fit of the scaling law by initializing from the parameters found in \citet{hoffmann2022training} and modifying parts of the power law fitting process.

% \citet{pearce2024reconcilingkaplanchinchillascaling} attempt to reconcile the \citet{kaplan2020scaling} and \citet{hoffmann2022training}. However, they transform their data into logarithmic space and then fit the power law linearly, which is unreliable and does not match either of the reproduced papers. They open source the code used to fit the scaling law, but not the data, nor do they include the code for training models or any model checkpoints.
% \ml{TODO: mention pearce here or elsewhere}

% Concurrently, 
\citet{porian2024resolving} isolate several decisions as primarily responsible for the discrepancy between the recommendations of \citet{kaplan2020scaling} and \citet{hoffmann2022training}: (1) learning rate scheduler warmup, (2) learning rate decay, (3) inclusion of certain parameters in total parameter count, and (4) specific training hyperparameters. By adjusting these factors, they are able to reproduce the results of \citet{kaplan2020scaling} and \citet{hoffmann2022training}. However, they only use 16 training runs to fit their scaling laws, each designed to match one targeted setting (e.g., replicating \citet{kaplan2020scaling}). 
Instead of using raw loss values, they fit to loss values found by interpolating between checkpoints. Like \citet{pearce2024reconcilingkaplanchinchillascaling}, they apply a log transform and linear regression to fit their law.



% \srk{clean up: While all popular architecture families have shown benefits with scale, most recent papers  that scale a lot are Transformer based LLMs or encoder decoder models CITE, a significant majority of the models we survey are from the Transformer \cite{vaswani2017attention} model family. For the rest of the paper, we mainly discuss Transformer based scaling laws, unless specifically discussed. }

% \subsection{Replication Efforts}

% % \ml{ripped below from old s9, need to integrate}
% \citet{hoffmann2022training} highlight the differences in setup which lead to them showing that large models should be trained for significantly more tokens than \citet{kaplan2020scaling} concluded. Particularly, they point to using later checkpoints, training larger models, a different learning rate schedule and changing the number of training tokens used across runs. Multiple followup works have focused on either reproducing or explaining the differences between these two papers \citep{besiroglu2024chinchilla,porian2024resolving, pearce2024reconcilingkaplanchinchillascaling}.

% In particular, \citet{besiroglu2024chinchilla} seek to reproduce the parameter fitting approach used by \citet{hoffmann2022training}. They are unable to recover the scaling law from \citet{hoffmann2022training}, and demonstrate that the claims of the original paper are inconsistent with their descriptions of the setup. They then seek to improve the fit of the scaling law by initializing from the parameters found in \citet{hoffmann2022training} and modifying parts of the power law fitting setup.

% % \citet{pearce2024reconcilingkaplanchinchillascaling} attempt to reconcile the \citet{kaplan2020scaling} and \citet{hoffmann2022training}. However, they transform their data into logarithmic space and then fit the power law linearly, which is unreliable and does not match either of the reproduced papers. They open source the code used to fit the scaling law, but not the data, nor do they include the code for training models or any model checkpoints.
% % \ml{TODO: mention pearce here or elsewhere}

% % Concurrently, 
% \citet{porian2024resolving} isolate several decisions as primarily responsible for the discrepancy between the recommendations of \citet{kaplan2020scaling} and \citet{hoffmann2022training}: (1) learning rate scheduler warmup, (2) learning rate decay, (3) inclusion of certain parameters in total parameter count, and (4) specific training hyperparameters. By adjusting these factors, they are able to reproduce the results of \citet{kaplan2020scaling} and \citet{hoffmann2022training}. However, they only use 16 training runs to fit their scaling laws, each designed to match one targeted setting (e.g., replicating \citet{kaplan2020scaling}). Instead of using raw loss values, they fit to loss values found by interpolating between checkpoints. Like \citet{pearce2024reconcilingkaplanchinchillascaling}, they apply a log transform and linear regression to fit their law.
% % They open source the code and data used to fit the scaling law, but not the model checkpoints. 


% \ml{maybe: insert table/plot comparing optimal D for each N (or for each C), maybe loss prediction where available. Table should include at min kaplan, chinchilla, porian, epochai}

% \begin{itemize}
    % \item scaling laws draw upon lots of earlier literature in theory
    % \item in modern deep learning, hestness  \cite{hesPTtness2017deep} was first
    % \item then for LMs, Kaplan 
    % \item for LMs various parts of scaling were studied - studies on optimality \cite{hoffmann2022training,hu2024minicpm}, 
    % abilities of models \cite{wei2022emergent}, transfer \cite{hernandez2021scaling}, 
    % \item architecture choices \cite{poli2024mechanistic,tay2022scaling,clark2022unified,tao2024scaling} etc.
    % \item multimodal models too CITE
    % \item Nmt also \cite{bansal2022data,ghorbani2021scaling,fernandes2023scaling,gordon2021data,zhang2022examining}. 
    % \item  VLMs CITE , vision \cite{alabdulmohsin2022revisiting}, rl \cite{jones2021scaling,gao2023scaling}, speech CITE recsys CITE also.
    % \item Scales tend to be modest for studies, up to xyz for scaling law fit. Some with smaller models for theoretical studies \cite{hashimoto2021model}.
% \end{itemize}

% aSpecifically on LMs, apart from saying there is a power law \cite{hestness2017deep,kaplan2020scaling}, architecture choices 
% listed domains in figure 1
%  Code not necessarily available for all, to best of our knowledge 12 of papers surveyed have code (Figure \ref{fig:overview}).  



% \begin{figure}[t!]
% \centering
% \includegraphics[width=\textwidth]{images/details.png}
% \caption{Caption for the figure}
% \label{fig:details}
% \end{figure}

% This proportion of papers with code extends to all other details of these papers that we tracked (Figure \ref{fig:details}), such as number of data points used and optimizer used. Look at Section \ref{sec:diffs} for a detailed discussion of this.