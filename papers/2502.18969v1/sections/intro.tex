
\section{Introduction}

% Large foundation models \citep{anil2023palm,openai2023gpt,dehghani2023scaling} have enabled a variety of downstream applications \citep{bommasani2021opportunities}. However, reaching the scale required to reach sufficient quality is an expensive and uncertain process. 

Training at the scale seen in recent large foundation models \citep{dubey2024llama,openai2023gpt,reid2024gemini} is an expensive and uncertain process. Given the infeasibility of hyperparameter tuning multi-billion parameter models, researchers extrapolate the optimal training setup from smaller training runs.
% The optimal training setup changes with architecture and size, which normally necessitates hyperparameter tuning and comparison across many models. However, at the multi-billion parameter scale of current frontier models, this is 
More precisely, scaling laws \citep{kaplan2020scaling} are used to study many different aspects of model scaling. Scaling laws can guide targets for increasing dataset size and model size in pursuit of desired accuracy and latency for a specific deployment scenario, study architectural improvements, determine optimal hyperparameters and assist in model debugging. \input{tables/summary_stats}
 Scaling laws are often characterized as power laws between the loss and size of the model and dataset, and are seen in several variations (Section \ref{sec:related-work}). These laws are found empirically by training models across a few orders of magnitude in model size and dataset size, and fitting the loss of these models to a proposed scaling law. Each component of this process varies in the reported literature, from the specific equation being fit, to the training setup, and the optimization method, as well as specific details for selecting checkpoints, counting parameters and the objective loss optimized during fitting. 

Changes to this setup can lead to significant changes to the results, and therefore completely different conclusion to the study. For example, \citet{kaplan2020scaling} studied the optimal allocation of compute budget, and found that dataset size should be scaled more slowly than model size ($D \propto N^{0.74}$, $D$ is dataset size, $N$ is model size). Later, \citet{hoffmann2022training} contradicted this finding, showing that model size and dataset size should be scaled roughly equally for optimal scaling. They highlight the differences in setup which lead to them showing that large models should be trained for significantly more tokens: particularly, they point to using later checkpoints, training larger models, a different learning rate schedule and changing the number of training tokens used across runs. Multiple followup works have focused on either reproducing or explaining the differences between these two papers \citep{besiroglu2024chinchilla,porian2024resolving, pearce2024reconcilingkaplanchinchillascaling}. The authors find it challenging to reproduce results of previous papers - we refer the reader to Section \ref{sec:related-work} for a further discussion on these replication efforts.

Motivated by this, we survey over 50 papers on scaling laws across a variety of modalities, tasks and architectures, and find that essential details needed to reproduce scaling law studies are often underreported. We broadly categorize these details as follows:

\paragraph{Section \ref{sec:power-law-form}: What \textit{form} are we fitting?} Researchers may choose any number of power law forms relating any set of variables, to which they fit the data extracted from training runs. Even seemingly minor differences in form, may imply critical changes in assumptions -- for example, about certain interactions between variables which are excluded, the definitions of these variables or error terms which are deemed significant enough to include.
\vspace{-5mm}
\paragraph{Section \ref{sec:model_training}: How do we \textit{train models}?} In order to fit a scaling law, one needs to train a range of models spanning orders of magnitude in parameter count and/or dataset size. Each model requires a multitude of hyperparameter and parameter choices, such as the specific model/dataset sizes to use, the architecture shape, batch size or learning rate schedule.
\vspace{-5mm}
\paragraph{Section \ref{sec:data}: How do we \textit{extract data} after training?} Once these models are trained, downstream metrics like perplexity must be obtained from the intermediate or final checkpoints. This data may be also scaled, interpolated or bootstrapped to create more datapoints to fit the power law parameters.

\vspace{-5mm}
\paragraph{Section \ref{sec:opt}: How are we \textit{optimizing} the fit?} Finally, the variable must be fit with an objective and optimization method, which may in turn have their own initialization and hyperparameters to choose.

To aid scaling laws researchers in reporting details necessary to reproduce their work, we propose a checklist  (Figure~\ref{sec:checklist} - an expanded version may be found in Appendix \ref{sec:app_checklist} ). Based on this checklist, we summarize these details for all 51 papers in tabular form in Appendix \ref{app:full-details}. 
We find that important details are frequently underreported, significantly impacting reproducibility, especially in cases where there is no code - only 19 of 42 papers surveyed have analysis code/code snippets available. Additionally, 23 (a little over half) of surveyed papers do not describe the optimization process, and 15 do not describe how training FLOPs or number of parameters are counted, which has been found to significantly change results \citep{porian2024resolving}. 
% Fitting a specific scaling law to a set of models involves several decisions in experimental design, which can greatly affect the result of a study. 
In addition, we fit our own power laws to further demonstrate how these choices critically impact the final scaling law~(Section \S\ref{sec:own-repl}). 
% \ml{update specific counts}


% \subsection{Diving into the details of deriving a scaling law}\label{sec:classification}






\input{tables/checklist_short}

% Motivated by the literature we discuss to answer the questions above, and the analysis in Section \ref{sec:own-repl}, we provide a checklist in Table \ref{sec:checklist} that we hope researchers will consider while conducting scaling laws investigations.




% \ml{ripped below from old s9, need to integrate}
% \paragraph{Replication Efforts} 
% They open source the code and data used to fit the scaling law, but not the model checkpoints. 


% \ml{maybe: insert table/plot comparing optimal D for each N (or for each C), maybe loss prediction where available. Table should include at min kaplan, chinchilla, porian, epochai}
% In Section \ref{sec:method} we provide a refresher of power laws, and transformers (which are the models studied in a majority of the papers). In Section \ref{sec:related-work}, we discuss the breadth of papers that we discuss. In Section \ref{sec:power-law-form}, we discuss different forms of power laws. In Section \ref{sec:model_training} and \ref{sec:data} we discuss the data often used to fit these power laws, and in Section \ref{sec:opt} we discuss the optimization method used. In Section \ref{sec:diffs} we discuss all the differences in experimental setup that may result in different conclusions, with an overview of attempts by us (Section \ref{sec:own-repl}) and others (Section \ref{sec:repl}). \srk{remember to finalize after restructuring is done}



% PREVIOUS STUFF
% \section{Introduction}

% % Large foundation models \citep{anil2023palm,openai2023gpt,dehghani2023scaling} have enabled a variety of downstream applications \citep{bommasani2021opportunities}. However, reaching the scale required to reach sufficient quality is an expensive and uncertain process. 

% Training at the scale seen in recent large foundation models \citep{anil2023palm,openai2023gpt,dehghani2023scaling} is an expensive and uncertain process. 
% The optimal training setup changes with architecture and size, which normally necessitates hyperparameter tuning and comparison across many models. However, at the multi-billion parameter scale of current frontier models, this is infeasible. Instead, scaling laws (\citep{kaplan2020scaling}) are used to study many different aspects of model scaling. 
% % Despite this expensive process, training instabilities(\citep{dehghani2023scaling,zhai2023stabilizing}), or new unpredicted abilities \citep{wei2022emergent} may emerge. Furthermore, while using a model post training, unanticipated behavior may be observed \citep{dettmers2022llm}. 
% % In this survey, we will discuss papers that present scaling laws, and papers that discuss the challenges of scaling up foundation models. We survey three different kinds of papers that cover different, but related areas: we start by surveying papers on scaling laws for large transformer models, and then review issues observed while training massive ($~10^{10}B$ parameters and beyond) language models. Finally, we discuss some newer additions to the practitioner's toolkit for scaling foundation models. 
% Scaling laws can guide targets for increasing dataset size and model size in pursuit of desired accuracy and latency for a specific deployment scenario, study architectural improvements, and assist in model debugging.

% % Studying scaling laws, apart from underscoring the importance of scaling deep learning models in the pursuit of progress, have several practical purposes. Scaling laws can guide targets for increasing dataset size and model size in pursuit of desired accuracy and latency for a specific deployment scenario. Moreover, an accurate scaling law curve can also assist in model debugging - say, if a scaled up model diverges from the prediction given by the scaling law, it would indicative that there is likely some deeper issue such as training instabilities or improper hyperparameters. Finally, scaling laws can aid in guiding the design of new architectures and learning objective - while there is some evidence that architectural improvements mainly improve the power law constant \citep{hestness2017deep}, this is not known for sure \cite{tay2022scaling}, \srk{maybe revise this statement - I think the literature on this is mixed - the hyena/mamba paper for example, is an improvement/different iirc} and introducing new objectives \citep{tay2022transcending} have been known to yield significant improvements. 

% \input{tables/summary_stats}
% Scaling laws are often characterized as power laws between the loss and size of the model and dataset, and may have several different variations (Section \ref{sec:related-work}). These laws are found empirically by training models across a few orders of magnitude in model size and dataset size, and fitting the loss of these models to a proposed scaling law. Each aspect of this process varies in the reported literature, from the specific equation being fit, to the training setup, and the optimization method, not to mention specific details for selecting checkpoints, counting parameters or the objective loss optimized during fitting. We survey over 50 papers on scaling laws across a variety of modalities, tasks and architectures, and find that these essential details are often underreported, with several attempts \citep{porian2024resolving,besiroglu2024chinchilla} finding it challenging to reproduce results of previous papers. A summary table of all papers surveyed may be found at Appendix \ref{app:full-details}. In particular, across the papers we survey, we discuss:
% \begin{itemize}
%     \item The various forms that scaling laws are fit to (\S\ref{sec:power-law-form})
%     \item The range of data sizes and model sizes to fit scaling laws, and specific training details that may significantly change the final scaling law fit (\S\ref{sec:model_training})
%     \item How these training runs are used to collect datapoints for the fitting of a scaling law (\S\ref{sec:data})
%     \item The optimization process, including initialization and algorithm, to fit the law from collected datapoints (\S\ref{sec:opt})
% \end{itemize}

% We fit our own power laws and demonstrate that these choices critically impact the final scaling law~(\S\ref{sec:own-repl}). These details are frequently underreported, significantly impacting reproducibility, especially in cases where there is no code (only 19 of 42 papers surveyed have analysis code/code snippets available). Additionally, 23 (a little over half) of surveyed papers do not describe the optimization process, and a majority of do not describe how training FLOPs or number of parameters are counted, which has been found to significantly change results \citep{porian2024resolving}. 
% % \ml{update specific counts}
% Finally, we propose a checklist to improve reproducibility in scaling laws (full checklist in Appendix \ref{sec:checklist}).
% \luke{reference fig 2 instead of appendix?}


% % \subsection{Diving into the details of deriving a scaling law}\label{sec:classification}

% Fitting a specific scaling law to a set of models involves several decisions in experimental design, which can greatly affect the result of a study. We broadly divide these decisions into the following:

% \begin{itemize}
%     \item \textbf{Section \ref{sec:power-law-form}: What \textit{form} are we fitting?} Researchers may choose any number of power law forms relating any set of variables, to which they fit the data extracted from training runs. Even seemingly minor differences in form, may imply critical changed in assumption -- for example, about certain interactions between variables which are excluded, the definitions of these variables or error terms which are deemed significant enough to include.
%     \item \textbf{Section \ref{sec:model_training}: How do we \textit{train models}?} In order to fit a scaling law, one needs to train a range of models spanning orders of magnitude in parameter count and/or dataset size. Each model requires a multitude of hyperparameter and parameter choices, such as the specific model/dataset sizes to use, the architecture shape, batch size or learning rate schedule.
    
% \item \textbf{Section \ref{sec:data}: How do we \textit{extract data} after training?} Once these models are trained, downstream metrics like perplexity must be obtained from the intermediate or final checkpoints. This data may be also scaled, interpolated or bootstrapped to create more datapoints to properly fit the power law parameters.

% \item \textbf{Section \ref{sec:opt}: How are we \textit{optimizing} the fit?} Finally, the variable must be fit with an objective and optimization method, which may in turn have their own initialization and hyperparameters to choose.
% \end{itemize}


% \input{tables/checklist}

% Motivated by the literature we discuss to answer the questions above, and the analysis in Section \ref{sec:own-repl}, we provide a checklist in Table \ref{sec:checklist} that we hope researchers will consider while conducting scaling laws investigations.




% % \ml{ripped below from old s9, need to integrate}
% \paragraph{Replication Efforts} \citet{hoffmann2022training} highlight the differences in setup which lead to them showing that large models should be trained for significantly more tokens than \citet{kaplan2020scaling} concluded. Particularly, they point to using later checkpoints, training larger models, a different learning rate schedule and changing the number of training tokens used across runs. Multiple followup works have focused on either reproducing or explaining the differences between these two papers \citep{besiroglu2024chinchilla,porian2024resolving, pearce2024reconcilingkaplanchinchillascaling}.

% In particular, \citet{besiroglu2024chinchilla} seek to reproduce the parameter fitting approach used by \citet{hoffmann2022training}. They are unable to recover the scaling law from \citet{hoffmann2022training}, and demonstrate that the claims of the original paper are inconsistent with their descriptions of the setup. They then seek to improve the fit of the scaling law by initializing from the parameters found in \citet{hoffmann2022training} and modifying parts of the power law fitting setup.

% % \citet{pearce2024reconcilingkaplanchinchillascaling} attempt to reconcile the \citet{kaplan2020scaling} and \citet{hoffmann2022training}. However, they transform their data into logarithmic space and then fit the power law linearly, which is unreliable and does not match either of the reproduced papers. They open source the code used to fit the scaling law, but not the data, nor do they include the code for training models or any model checkpoints.
% % \ml{TODO: mention pearce here or elsewhere}

% % Concurrently, 
% \citet{porian2024resolving} isolate several decisions as primarily responsible for the discrepancy between the recommendations of \citet{kaplan2020scaling} and \citet{hoffmann2022training}: (1) learning rate scheduler warmup, (2) learning rate decay, (3) inclusion of certain parameters in total parameter count, and (4) specific training hyperparameters. By adjusting these factors, they are able to reproduce the results of \citet{kaplan2020scaling} and \citet{hoffmann2022training}. However, they only use 16 training runs to fit their scaling laws, each designed to match one targeted setting (e.g., replicating \citet{kaplan2020scaling}). Instead of using raw loss values, they fit to loss values found by interpolating between checkpoints. Like \citet{pearce2024reconcilingkaplanchinchillascaling}, they apply a log transform and linear regression to fit their law.
% % They open source the code and data used to fit the scaling law, but not the model checkpoints. 
