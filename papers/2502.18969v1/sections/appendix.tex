
% \section{Checklist}\label{app:checklist}
% You may include other additional sections here.

\section{Our model training (\S\ref{sec:own-repl})}\label{app:our_models}

We train a variety of Transformer LMs, ranging in size from 12 million to 1 billion parameters, on FineWeb \citep{penedo2024finewebdatasetsdecantingweb}, tokenized with the GPT-NeoX-20B tokenizer \citep{black2022gptneox20bopensourceautoregressivelanguage}, which is a BPE tokenizer with a vocabulary size of 50257, trained on the Pile \citep{gao2020pile800gbdatasetdiverse}. All models were trained on a combination of NVIDIA GeForce RTX 2080 Ti, Quadro RTX 6000, NVIDIA A40, and NVIDIA L40 GPUs. These transformers follow the standard architecture, and use pre-layer RMSNorm \citep{kudo2018sentencepiece}, the SwiGLU activation function \citep{shazeer2020gluvariantsimprovetransformer}, and rotary positional embeddings \citep{su2024roformer}. We use a batch size of 512 with a sequence length of 2048. The learning rate is warmed up linearly over 50 steps to the peak learning rate and then follows a cosine decay to 10\% of the peak. We use an Adam optimizer with $\beta_1 =
0.9, \beta_2 = 0.95$. We sweep over learning rates and data budgets at each model scale. For our evaluation metric, we use perplexity on a validation set of C4 \citep{raffel2020exploring}. See Table~\ref{tab:hparams} and Table~\ref{tab:arch} for additional architecture details and hyperparameters, including data budget and learning rate.

\input{tables/hparams}
\input{tables/arch}

\section{Full Checklist} \label{sec:app_checklist}

% Below, we provide an expanded version of the checklist from Figure \ref{sec:checklist}. \textcolor{blue}{
We define each category as follows:
% }


\begin{itemize}
    \item {
    \textbf{Scaling Law Hypothesis:} This specifies the form of the scaling law, that of the variables and parameters, and the relation between each.}
    \item {\textbf{Training Setup:} This specifies the exact training setup of each of the models trained to test the scaling law hypothesis.}
    \item {\textbf{Data Collection:} Evaluating various checkpoints of our trained models to collect data points that will be used to fit a scaling law in the next stage.}
    \item {\textbf{Fitting Algorithm:} Using the data points collected in the previous stage to optimize the scaling law hypothesis.}
\end{itemize}


\input{tables/checklist_full}

\pagebreak


\section{Full Sheet}\label{app:full-details}

We provide an overview of all the papers surveyed in Tables \ref{tab:full-basic},\ref{tab:full-powerlaw}, \ref{tab:full-setup}, \ref{tab:full-eval} and \ref{tab:full-opt}.


% \url{https://docs.google.com/spreadsheets/d/18RRvkRE9dNHjAnJXCIwgRk8lIwvKEfi4IK9sgscztgw/edit?gid=1894146386}


\input{tables/full_tables}

\newpage

\section{Recommendations}\label{sec:app_recs}

% \textcolor{blue}{
As seen in our analyses, many decisions in our checklist have a number of reasonable options, but those reasonable choices lead to a wide range of scaling law fits, and the observed variations do not follow any clear pattern. It is probable that variations would be even harder to predict when varying model architectures or other design decisions, removing the possibility of a universal set of best practices.
However, it is certainly possible to determine that some scaling law fits are plausible or highly implausible, and to observe the stability of the fitting procedure. 
% For example, in Figure 2(a), neither of the recommended data/parameters ratios of ~1/2200 or 3000/1 at $10^25$ FLOPs are likely to be the true optimal settings, and the loss predictions at those points are also unlikely to be close to ground truth. 
With the caveat that following any recommendations can not guarantee good scaling law fit, we can make some more concrete recommendations based on these observations:
\paragraph{Scaling Law Hypothesis}
\begin{itemize}
    \item Fitting fewer scaling law parameters at a time typically results in greater stability. In some cases, it may be beneficial to decompose the scaling law fitting problem into two separate procedures. Examples of this approach are the IsoFLOP procedure from \citet{hoffmann2022training}, as well as fitting first the relation between $L$ and $C$, then finding the optimal $N$ and $D$ for a $C$, as seen in \citet{porian2024resolving}.
    % \item lr
\end{itemize}
\paragraph{Training Setup}
\begin{itemize}
    \item The trained models should include a wide range of input variables settings. For example, when the input variables to the scaling law are $L$, $N$, $D$, the included models should include a wide range of $N$ and $D$ values for each $C$, or equivalently, should include a wide range of $D/N$ ratios. If the included settings do not include the true optimum, the procedure will struggle to fit to the optimum.
    \item Sweeping for the optimal learning rate results in a less stable fit than fixing the learning rate. We hypothesize that this may be because the true optimal learning rate for each model and data budget size is not any of the options we consider, and thus, each model varies in the difference between its true and approximate optimal learning rate. This may introduce additional noise to the data. Due to resource constraints, we are unable to fully test this hypothesis, and it may not hold at significantly larger scale, but we recommend fixing the learning rate, or changing it according to, say, model size, according to a fixed formula.
\end{itemize}
\paragraph{Data Collection}
\begin{itemize}
    \item Results across tasks or datasets should not be mixed. Neither performance predictions nor optimal $D/N$ ratios are fixed across different evaluation settings for the same set of models.
\end{itemize}
\paragraph{Fitting Algorithm}
\begin{itemize}
    \item Scaling law fitting is sensitive to initialization; most known optimization methods for scaling laws are only able, in practice, to the shift parameters near to their initialization values. Thus, a  dense search over initialization values is necessary. If there is a strong hypothesis guiding the choice of one specific initialization, such as a previously fit and validated scaling law, this will also limit the set of possible final scaling law parameter values.
    \item Different losses emphasize the contribution of errors from certain datapoints. The chosen loss should be suited to the distribution of datapoints and sources of noise.
    \item A simple grid search is unlikely to result in a good scaling law fit. Additionally, optimizers designed to fit linear relations may make assumptions about the distribution of errors and should not be used to fit a power law. 
\end{itemize}
% }


\subsection{Example Checklist}

% \vspace{-5cm}
% \textcolor{blue}{
We provide one possible set of responses to our checklist, reflective of some recommendations enumerated above, and loosely based on \citet{hoffmann2022training}. These answers roughly correspond to a subset of the experiments we run in \S\ref{sec:own-repl}.
% }

\newpage

\input{tables/example_checklist}

\newpage


\section{Full Analysis Plots}\label{sec:app_megafigure}

\input{images/analysis_megafigure_appendix}

