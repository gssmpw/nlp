
% \section{Preliminaries}\label{sec:method}
%  \srk{TO REMOVE}
% \subsection{Power Laws for Large Models}

% \srk{this does is repeated in Section 5 - maybe remove? but how to remove?}
% % What are power laws, where else are they used, how are they usually fit, what kinds of forms do they take? 


% It has become common to characterize the loss of large foundation models by using a power law.

% \[  f(x)=ax^{-k} \]

% In this section we will describe things specifically for transformer language models, but in Section \ref{sec:power-law-form}, we go further into which forms other papers have considered to answer specific research questions for different modalities and architecture families. 

% In its most simple form, one can consider a model for size $N$ trained for $D$ tokens and find that the loss for the model can be described as a power law in both $N$ and $D$. That is, 

% \[ L \propto N^a , L \propto D^a \]

% These are often combined to fit a single equation in terms of both $N$ and $D$. For example, \cite{hoffmann2022training} use the following equation

% \[ L(N, D) = E + \frac{A}{N^{\alpha}} + \frac{B}{D^{\beta}} \]

% \subsection{Models}

% \srk{honestly not sure what to do with this section either}




% Maybe stuff like what is a token, what is a FLOP / how do you calculate FLOPs, what is embedding vs non embedding parameters, what is test loss?
