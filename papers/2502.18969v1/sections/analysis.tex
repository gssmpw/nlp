

\section{Our Replications and Analyses}\label{sec:own-repl}

% \input{images/analysis_fig_main}


Each of the choices discussed above in Sections~\ref{sec:power-law-form}~-~\ref{sec:opt} may have a crucial impact on the result, yet it remains common to critically underspecify the setup for fitting a power law. Scaling law works often fail to open source their model and code, making reproduction infeasible, and likely contributing to contradictory conclusions as discussed in Section \ref{sec:related-work}. Though some efforts have been made  \citep{porian2024resolving,besiroglu2024chinchilla} to reconcile such discrepancies, there is still only sparse understanding of the impact of each of the decisions we discuss.

To investigate the significance of these scaling law optimization decisions, we vary these choices to fit our own scaling laws. We fit both the Chinchilla-scraped data from \citet{besiroglu2024chinchilla}, and data from our own models.




\paragraph{Reconstructed Chinchilla data \citep{besiroglu2024chinchilla}}
This data is extracted from a vector-based figure in the pdf of \citet{hoffmann2022training}, who claim that this includes all models trained for the paper. It consists of 245 datapoints, each corresponding to a final checkpoint collected at the end of model training. There is a potential risk of errors in this recovery process. 

% \paragraph{Resolving discrepancies open-sourced data \citep{porian2024resolving}}
% This dataset includes final and mid-training checkpoint evaluations for 16 model runs in each of 6 settings studied (e.g. replicating \citet{kaplan2020scaling}). Power laws were fit on the log-scale, following an IsoFLOP approach on interpolated evaluation numbers, to predict the optimal parameter budget for a fixed compute. 
% % \subsection{Our models and evaluations} \label{subsection:our_models}

\paragraph{Data from \citet{porian2024resolving}} This data includes training losses for Transformer LMs ranging in size from 5 to 901 million parameters, each trained on 
OpenWebText2 \citep{gao2020pile800gbdatasetdiverse} and RefinedWeb data \citep{penedo2023refinedwebdatasetfalconllm}, across a variety of data and compute budgets, from 5 million to 14 billion tokens (depending on parameter count). Each model is trained with a different peak learning rate and batch size setting, found by fitting a separate set of scaling laws.


\paragraph{Our models} We train a variety of Transformer LMs, ranging in size from 12 million to 1 billion parameters, on varied data and compute budgets and hyperparameter settings. Details about our setup, including hyperparameters, are listed in Appendix~\ref{app:our_models}. We open source all of our models, evaluation results, code, and FLOP calculator at \url{https://github.com/hadasah/scaling_laws}

We fit a multitude of power laws and study the effects of: (1) power law form; 
(2) model learning rate; 
(3) compute budget, model size, and data budget range and coverage; 
(4) definition of $N$ and $C$;
(5) inclusion of mid-training checkpoints;
% (5) data point interpolation; (6) outlier filtering; 
(6) power law parameter initialization; 
(7) choice of loss and (8) optimizer.

Based on our observations, we also make some more concrete recommendations in Appendix \S\ref{sec:app_recs}, with the caveat that following the recommendations cannot guarantee a good scaling law fit. 


\include{images/analysis_megafigure}

\subsection{Form (\S\ref{sec:power-law-form})} \label{sec:repl-power-law-form}

% We first focus on \textit{performance prediction}, and the effects of the small differences in form between \citet{hoffmann2022training} and \citet{kaplan2020scaling}. We vary the (1) baseline approach by changing the power law form to (2) match \citet{kaplan2020scaling}, and to (3) apply the trick employed by \citet{muennighoff2024scaling} of setting $\alpha = \beta$ in the \citet{hoffmann2022training} form, making the assumption that the optimal $D/N$ ratio should remain constant regardless of scale. We repeat this analysis on the \citet{besiroglu2024chinchilla} dataset (Figure~\ref{fig:analysis_form_epoch}) and our own dataset (Figure~\ref{}), showing \ml{TODO}
% We first focus on \textit{performance prediction}, 
We consider the (1) baseline \citet{hoffmann2022training} form (Approach 3) and then (2) apply the trick employed by \citet{muennighoff2024scaling} of setting $\alpha = \beta$ in the \citet{hoffmann2022training} form, which assumes the optimal $D/N$ ratio stays roughly constant -- $L(N, D) = E + \frac{A}{N^\alpha} + \frac{B}{D^\alpha}$. We also compare with (3) an ISOFlop approach (Approach 3 of \citet{hoffmann2022training}), in which we fit an optimal $N$ for each compute budget $C$, $C(N)$, which can then be used to fit the predicted loss $L(N, D)$. This approach usually necessitates the usage of mid-training checkpoints (discussed further in \S\ref{sec:repl-data}), as it is infeasible to train a large enough number of models for each FLOP budget considered. However, we apply it here without using only final model checkpoints, and extend to mid-training checkpoints in \S\ref{sec:repl-data}). We adapt the implementation from \citet{porian2024resolving}, which contains more details about interpolation of data points and specific hyperparameters. In all data sets, (2) approaches the law reported by \citet{hoffmann2022training}, but (1) only does so for the data from \citet{porian2024resolving}(Figure~\ref{fig:analysis_form}).

We experiment with the power law form reported by \citet{kaplan2020scaling}, but this consistently yields a law which suggests that the optimal number of data points is 1, even when varying many aspects of the power law fitting procedure. The difficulty of fitting to this form might be partially a result of severe under-reporting in \citet{kaplan2020scaling} with regard to procedural details, including hyperparameters for both model training and fitting. 

% We also compare the \textit{performance prediction} approach to \textit{ratio optimization}. Though \citet{hoffmann2022training} themselves contrast the two, \citet{besiroglu2024chinchilla} demonstrate inconsistencies in their published findings. We compare (1) the baseline to (2) a ratio optimization procedure which follows the IsoFLOP approach of \citet{hoffmann2022training} as reproduced by \citet{porian2024resolving}. \ml{TODO, may not actually do this}

% \begin{figure}[t!]
% \centering
% \includegraphics[width=0.8\textwidth]{images/analysis_form_epoch.png}
% \caption{compare to tying alpha and beta weights like in \citet{muennighoff2024scaling}}
% \label{fig:analysis_form_epoch}
% \end{figure}



\subsection{Training (\S\ref{sec:model_training})} \label{sec:repl-model_training}

\citet{hu2024minicpm} study the effects of several model training decisions, including batch size, learning rate schedule, and model width. Their analysis focuses on optimizing hyperparameters, not on the ways hyperparameter and architecture choices affect the reliability of scaling law fitting. Observed variations between settings suggest that suboptimal performance could skew the scaling law fit. 

To substantiate this further, we simulate the effects of not sweeping the learning rate in our models. As a baseline, (1) we sweep at each ($N$, $D$) pair for the optimal learning rate over a range of values, at most a multiple of 2 apart. Next, (2) we use a learning rate of 1e-3 for all $N$, the optimal for our 1 billion parameter models, and do the same for (3) 2e-3 and (4) 4e-3, which is optimal for our 12 million parameter models. Lastly, we use all models across all learning rates at the same $N$ and $D$. Results vary dramatically across these settings. Somewhat surprisingly, using all learning rates results in a very similar power law to sweeping the learning rate, whereas using a fixed learning rate of 1e-3 or 4e-3 yields the lowest optimization loss or closest match to the \citet{hoffmann2022training} power laws, respectively (Figure~\ref{fig:analysis_lr_ours}).

We also study the effects of limiting model scale range, data scale range (implicitly), and data-to-parameters range by filtering all 3 datasets: we compare (1) using all $N, D$ scales, (2) only models with $N$ up to about 100 million or (3) 400 million parameters, and including (4) only models with $D/N \leq 18$ or (5) $D/N \geq 22$. These ranges are designed to exclude $D/N = 20$, the rule of thumb based on \citet{hoffmann2022training}. The minimum or maximum $D/N$ ratio tested does skew results; above $10^{22}$ FLOPs, (4) and (5) fit to optimal ratios $D/N < 18$ and $D/N > 22$, respectively. Removing our largest models in (2) also creates a major shift in the predicted optimal $D/N$ (Figure~\ref{fig:analysis_nd_ours}).

Related to choice of $N, D$ and $C$ values, we investigate different ways of counting $N$ and $C$, specifically whether to include embedding parameters and FLOPs. We compare (1) our baseline, which includes embeddings in both $N$ and $C$, with (2) excluding embeddings only in $N$, (3) excluding embeddings only in $C$, (4) excluding embeddings in both $N$ and $C$. We also compare to (5) using the $C=6ND$ approximation, including embedding parameters. In all other settings, we calculate the FLOPs in a manner similar to \citet{hoffmann2022training}, and we open source the code for these calculations.
With both datasets, the exclusion of embeddings in FLOPs has very little impact on the final fit. Similarly, using the $C=6ND$ approximation has no visible impact. For the \citet{porian2024resolving} models, the exclusion of embedding parameters in the calculation of $N$ results in scaling laws which differ substantially, and with increasing divergences at large scales (Figure~\ref{fig:analysis_counting_ours}). 

\subsection{Data Collection (\S\ref{sec:data})} \label{sec:repl-data}

% For our own models, we are able to evaluate on 2 different datasets. We take the loss on a held out validation set of (1) C4 \citep{raffel2020exploring} as well as (2) English Wikipedia \citep{wikidump}, and fit separate power laws to the two metrics. Not only does absolute performance predictions vary between the two datasets, we also find different optimal $D/N$ ratios (Figure~\ref{fig:analysis_eval_ours}).

For our own models and those trained in \citet{porian2024resolving}, we compare using (1) only the final checkpoints of each model with (2) using all collected mid-training checkpoints. We also consider removing checkpoints collected during the (3) first 10\%, (4) first 20\%, and (5) first 50\% of training,
% , starting with 20\% through training, at which point warmup has concluded for all but a few of the smallest models, and (3) using mid-training checkpoints, starting at 50\% through training. 
Using mid-training checkpoints sometimes results in more stable fits which are similar to the \citet{hoffmann2022training} scaling laws, but the effect is unreliable and is dependent on other decisions.  Following this finding, we also re-run all other analyses using setting (2), and find that these power law fits are often more consistent when varying other aspects of the power law fitting procedure (Figures~\ref{fig:analysis_checkpoint_ours}~and~\ref{fig:analysis_checkpoint_rsld}).

For ease of comparison, we replicate all graphs, reorganizing those in Figures~\ref{fig:analysis_checkpoint_ours}~and~\ref{fig:analysis_checkpoint_rsld} with their checklist topics, to allow for easier comparison between results using mid-training checkpoints and only final checkpoints. See Appendix~\S\ref{sec:app_megafigure} Figure~\ref{fig:app_mega}.

% \textit{Performance prediction} methods are likely to be interested in the performance of the final model checkpoint, so it's intuitive that most methods fit power laws only on the performance of the final checkpoint. We compare (1) this baseline to (2) using \ml{TODO} checkpoints per model training run. Results on \citet{porian2024resolving} (Figure) and our data show \ml{TODO}


% When including mid-training checkpoints, we also compare (1) using the full dataset to removing outliers by (2) excluding the $n$ lowest performance datapoints or (3) using only checkpoints from 30\% to 80\% through training.

% As interpolation is typically associated with \textit{ratio optimization} approaches, we study interpolation with (1) our ISOFlop replication, which does not interpolate between checkpoints, to (2) the same, with interpolation. \ml{this will be deleted if we remove the other isoflop thing}


\subsection{Fitting (\S\ref{sec:opt})} \label{sec:repl-opt}
We vary the initialization method for our power law fitting procedure: (1) our baseline replication of \citet{hoffmann2022training} Approach 3, which conducts the full optimization process on a grid of 6x6x5x5x5=4500 initializations \citep{hoffmann2022training}, (2) searching for the lowest loss initialization point \citep{caballero2022broken} and optimizing only from that initialization, (3) optimizing from the 1000 lowest loss initializations, (4) randomly sampling $k$(=100) points \citep{frantar2023scaling,tao2024scaling}, and (5) initializing with the coefficients found in \citet{hoffmann2022training}, as \citet{besiroglu2024chinchilla} does.
With the \citet{besiroglu2024chinchilla} data, (5) yields a fit nearly identical to that reported by \citet{hoffmann2022training}, although (1) results in the lowest fitting loss. With the \citet{porian2024resolving} data, all approaches except (2) yield a very similar fit, which gives a recommended $D/N$ ratio similar to that of\citet{hoffmann2022training}. However, using our data, (2) optimizing over only the most optimal initialization yields the best match to the \citet{hoffmann2022training} power laws, followed by (5) initialization from the reported \citet{hoffmann2022training} scaling law parameters. Optimizing over the full grid yields the power law that diverges most from the \citet{hoffmann2022training} law, suggesting the difficulty of optimizing over this space, and the presence of many local minima (Figure~\ref{fig:analysis_init}).

Then, we analyze the choice of loss objectives, including (1) the baseline log-Huber loss, (2) MSE, (3) MAE, and (4) the Huber loss. We find that there is somewhat less variance in the resulting power laws than we have seen in other power law fitting decisions, but the recommended optimal data budgets still span a wide range (Figure~\ref{fig:analysis_loss}).

% and (5) grid search only. 
Finally, we consider the choice of optimizer. We first fit a power law to all 3 datasets using the original (1) L-BFGS, with settings matching those described by \citet{hoffmann2022training}. L-BFGS and BFGS implementations have an early stopping mechanism, which conditions on the stability of the solution between optimization steps. We experiment with setting this threshold for L-BFGS to a (2) higher value 1e-4 (stopping earlier).
% , and a (3) slightly lower value 1e-6. 
We find that using any higher or lower values results in the same solutions or instability for all three datasets. L-BFGS and BFGS also have the option to use the true gradient of the loss, instead of an estimate, which is the default. In this figure, we include this setting, (3) using the true gradient for L-BFGS. We compare these L-BFGS settings to (4) BFGS. We test the same tolerance value and gradient settings, and find that none of these options change the outcome of BFGS in our analysis. Finally, we compare to (5) non-linear least squares and (6) pure grid search, using a grid 5 times more dense along each axis as that which we used for initialization with other optimizers. This density is chosen to approximately match the runtime of L-BFGS.
% and grid search without additional optimization.
Many of these optimizer settings converge to similar solutions, but this depends on the data, and the settings which diverge from this majority do not follow any immediately evident pattern (Figure~\ref{fig:analysis_opt}).



