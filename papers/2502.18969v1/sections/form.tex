
\section{What \emph{form} are we fitting?}\label{sec:power-law-form}

% \srk{TODO: cross-reference final sheet etc with the content of the paper} 

A majority of papers we study fit some kind of power law ($f(x)=ax^{-k}$). That is, they specify an equation defining the relationship between multiple factors, such that a proportional change in one results in the proportional change of at least one other. 
They then optimize this power law to find some parameters. 
A few efforts do not seem to fit a power law, but may show a line of best fit, obtained through unspecified methods \citep{rae2021scaling,dettmers2022llm,tay2022scaling,shin2023scaling,schaeffer2023emergent,poli2024mechanistic}.

% To fit a power law, a specific form must first be determined. This form dictates the input factors, their relation to each other, and the parameters to be fit. 

% We refer to such laws as ``Performance Prediction'' laws. We also find that many works seek to predict, in a fixed-compute regime, the optimal allocation of resources (e.g., to model parameters vs. data). We term these ``Optimal Ratio'' papers. Alternatively, some works seek to predict the optimal hyperparameters for a model training run, such as batch size or learning rate for a given batch size \cite{mccandlish2018empirical}. For the scope of this survey, we focus on ``Performance Prediction'' and ``Optimal Ratio'' papers.

% though a few define a own scaling laws (Section \ref{sec:form-eqs}), 
 % or a form described in Section \ref{sec:perf-pred}. 
%$f(x)$ is frequently a generalization metric such as validation loss or error rate. Alternatively, some works seek to predict the optimal hyperparameters for a model training run, such as batch size or learning rate for a given batch size \cite{mccandlish2018empirical}. For the scope of this survey, we focus on papers for which $f(x)$ is a generalization metric.
% One may even seek to compare architectural improvements to determine whether new architectural variants scale better than standard architectures. 
 
 The specific form may be motivated by researcher intuition, previous empirical results, prior work, code implementation, or data availability. More importantly, the form is often determined by the specific question(s) a paper investigates. For example, one may attempt to predict the performance achieved by scaling up different model architectures, or the optimal ratio for model scaling vs data scaling when increasing training compute \citep{kaplan2020scaling,hoffmann2022training}. Based on this, we loosely classify scaling laws by their form as \textit{performance prediction} and \textit{ratio optimization} approaches. We indicate this classification for all surveyed papers in Appendix \ref{app:full-details}.

\subsection{Ratio Optimization} \label{sec:ratio_opt}
The simplest scaling law forms usually predict the relation between two variables in an optimal setting. For example, approaches 1 and 2 from \citet{hoffmann2022training}
fit to the optimal (i.e., lowest loss) $D$ and $N$ values for a particular compute budget $C$. 
\citet{porian2024resolving}, aiming to resolve these inconsistencies, defines\ $\rho^* = \frac{D^*}{N^*}$ and writes this relationship as: 
\begin{equation}
    N^*(C) = N^*_0 \cdot C^{\alpha} ; D^*(C) = D^*_0 \cdot C^{\alpha}; \rho^*(C) = \rho^*_0 \cdot C^{\alpha}
\end{equation}
They assume $C \approx 6ND$, and thus only need to fit the first equation; the other power laws can be inferred. This simplicity is deceptive in some cases, as collecting $(N^*(C), C)$ pairs may not be trivial. It is possible to fix $C$ and follow a binary search approach to train a multitude of models, then bisect to approximate the performance-optimal $N,D$ pair. However, this quickly grows prohibitively costly. In practice, it is common to interpolate between a set of fixed results to estimate the true $N^*(C)$ {\S\ref{sec:data}}. This adds to the complexity of this approach, and introduces a hidden dependency on the performance evaluation, yet it does not actually predict the performance of the optimal points. 
If only the performance of the optimal-ratio model is of interest, it is possible to fit a second power law $L(N^*(C),  D^*(C)) = a \cdot C^\alpha$. Most papers we survey choose to fit a power law which directly predicts performance.


\subsection{Performance prediction}\label{sec:perf-pred}
\citet{kaplan2020scaling} proposes a power law between Loss $L$, number of model Parameters $N$, and number of Dataset tokens $D$: 
\begin{equation}
L(N, D) = \left[ \left( \frac{N}{N_c}\right)^{\frac{\alpha_N}{\alpha_D}} + \frac{D}{D_c} \right]^{\alpha_D}     
\end{equation}

On the other hand, Approach 3 of \citet{hoffmann2022training} proposes
\begin{equation}
    L(N, D) = E + \frac{A}{N^\alpha} + \frac{B}{D^\beta} 
\end{equation}

In both of the above, all variables other than $L$, $N$, and $D$ are parameters to be found in the power law fitting process. Though these two forms are quite similar, they differ in some assumptions. \citet{kaplan2020scaling} constructs their form on the basis of 3 expected scaling law behaviors, and \citet{hoffmann2022training} explains in their Appendix D that their form is based on risk decomposition. The resulting \citet{kaplan2020scaling} form includes an interaction between $N$ and $D$ in order to satisfy a constraint requiring assymmetry introduced by one of their expected behaviors. The \citet{hoffmann2022training} form, on the other hand, consists of 3 additive sources of error, $E$ representing the irreducible error that would exist even with infinite data and compute budget, as well as two terms representing the error introduced by limited parameters and limited data, respectively.

Power laws for performance prediction can sometimes yield closed form solutions for optimal ratios as well. However, the additional parameters and input variables, introduced by the need to incorporate the performance metric term, add random noise and dimensionality. This increases the difficulty of optimization convergence, so when prediction performance is not the aim, a ratio optimization approach is frequently a better choice.

% while the second and third terms correspond to the error introduced by the complexity of the hypothesis space, and by limited exposure to the data distribution, respectively. The 3rd comes from an expectation of that the power law be analytic at $D=\inf$, which the authors acknowledge is speculative. In a footnote, the authors explain that, in the absence of includes an interaction between $N$ and $D$, implying that the effect of increasing parameters is dependent on the absolute amount of data, and vice versa.

Many papers directly adopt one of these forms, but some adapt these forms to study relationships with other input variables. \citet{clark2022unified}, for example, study routed Mixture-of-Expert models, and propose a scaling law that relates dense model size (effective parameters) $N$ and number of experts $E$ with a biquadratic interaction ($\log L(N, E) \triangleq a \log N+b \log E+c \log N \log E+d$). \citet{frantar2023scaling} study sparsified models, and propose a scaling law with an additional parameter sparsity $S$, the optimal value of which increases with $N$ ($L(S, N, D)=\left(a_S(1-S)^{b_S}+c_S\right) \cdot\left(\frac{1}{N}\right)^{b_N}+\left(\frac{a_D}{D}\right)^{b_D}+c$). Other papers change the form to model variables in the data setup. \citet{aghajanyan2023scaling} consider interference and synergy between multiple data modalities ($L(N, D_j)=E_j + \frac{A_j}{N^{\alpha_j}} + \frac{B_j}{|D_j|^{\beta_j}}$, $L(N, D_i, D_j) = [\frac{L(N, D_i) + L(N, D_j)}{2}] - C_{i,j} + \frac{A_{i,j}}{N^{\alpha_{i,j}}} + \frac{B_{i,j}}{|D_i|+|D_j|^{\beta_{i,j}}}$), while \cite{goyal2024scaling}, \citet{fernandes2023scaling} and \cite{muennighoff2024scaling} add terms to their scaling law formulations which represent mixing data sources and/or repeated data, using notions such as diminishing utility. A comprehensive list of the power law forms in the surveyed papers may be found in Table \ref{tab:full-powerlaw}.

% This is because the optimization problem becomes significantly simpler with 



% These are only 3 examples of the ways that a hypothesized scaling power-law relation may be expressed. 

% \begin{itemize}
    % \item \cite{clark2022unified} in terms of N and E, and consider biquadratic interaction
    % \item \citet{frantar2023scaling} optimal sparsity level S which increases with size
    % \item \citet{aghajanyan2023scaling} consider interference between modalities
    % \item data filtering paper and repeated data paper and \cite{fernandes2023scaling} consider data mixtures and repeated data - utility, etc etc
% \end{itemize}
