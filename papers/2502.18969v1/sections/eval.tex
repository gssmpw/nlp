

\section{How do we \emph{collect data} from model training?}\label{sec:data}

% - what to do after training the model?
% - [DONE] need to evaluate - val. loss most common; but error rate/accuracy etc has been used too; see table
% - [DONE] sometimes diff metric can affect conclusions (mirage) (rl - elo vs test loss)
% the models are trained, 
% - [DONE] final checkpoint is simplest
% - [DONE] use last 15%
% - use most checkpoints - acknowledge that this is sensitive to LR schedule as discusssed in prev section
% - drop outliers/early ckpts/95% middle
% - several models w same flops so use the best 
% - can use interpolation, smoothing of curve or median of last few steps, bootstraping
% - see appendix for comprehensive view of what folks do 
 


% BEFORE REARRANGING

% Of all possible approaches to fitting a scaling law on model training runs, one of the most straightforward is to use only the final checkpoints. However, many scaling laws papers choose a different approach in order to augment the size of their data in the presence of a limited training compute budget, in order to fit a particular approach to model evaluation, or for various other reasons.

% For example, in \citet{hoffmann2022training}, the ISOFlop approach necessitates having many training runs using the same number of FLOPs. Since they experiment with multiple fitting approaches, it would be prohibitively resource-intensive to train an additional suite of models for each targeted FLOP count. Instead, they choose to use mid-training checkpoints.
% \ml{TODO:finish}

% A related technique used here is performance interpolation. \citet{porian2024resolving} do not aim to exactly match the FLOP counts in each ISOFlop when evaluating model checkpoints mid-training. They instead interpolate between multiple model checkpoints to estimate the performance of a model with that exact number of FLOPs. 

% \ml{CLARIFY: Once relevant model checkpoints have been identified, there remains the question of what performance metric to optimize for, whether in the context of \textit{performance prediction}, or to choose appropriate points for \textit{ratio optimization}. }
% Train or validation loss are most commonly used, but some works consider other metrics, such as ELO score \citep{jones2021scaling,neumann2022scaling}, reward model score \citep{gao2023scaling}, or Downstream task metrics like accuracy or classification error rate \citep{henighan2020scaling,zhai2022scaling,cherti2023reproducible,goyal2024scaling,gao2023scaling}. This choice is non-trivial - while some papers show that there is a power law relation between the predicted loss found by using validation loss and a different downstream task \citep{dubey2024llama}, it is possible for the results of a study to change completely depending on the metric used. \citet{schaeffer2023emergent}, for example, find that using linear metrics such as Token-edit distance instead of non-linear metrics such as accuracy produces smooth, continuous predictable changes in model performance, contrary to an earlier study by \citet{wei2022emergent}. Moreover, multiple works that focus on scaling reinforcement learning find that they are unable to use test loss instead of Elo scores \citep{neumann2022scaling} to fit a power law.

% Some works filter data points before fitting their power law. For example, \citet{henighan2020scaling} drops their smallest models, \citet{hilton2023scaling} excludes the early checkpoints in each model training run, while \citet{hoffmann2022training} in their Approach \ml{todo} excludes the last 15\% of checkpoints in each run. Filtering is also applied to remove outliers, which is typically defined as the highest- and/or lowest-performing models \citep{hoffmann2022training,ivgi2022scaling,muennighoff2024scaling}.

% \srk{TODO: 
% - scaling, 
% - scaling metric this really matters give example of RL; mirage}

 % \item How many checkpoints per model are evaluated to fit each scaling law?
 %    \item What downstream loss/metric is used? On what dataset?
 %    \item Are the evaluation metrics modified in any way before fitting to the proposed equation? (say, loss interpolation, centering around a mean, scaling logarithmcally, bootstrapping, etc)
 %    \item If the above is done, is code for modifying the metric provided?

% SCRATCH ATTEMPT
 
To evaluate the range of models trained to fit a scaling law, train or validation loss are most commonly used, but some works consider other metrics, such as ELO score \citep{jones2021scaling,neumann2022scaling}, reward model score \citep{gao2023scaling}, or downstream task metrics like accuracy or classification error rate \citep{henighan2020scaling,zhai2022scaling,cherti2023reproducible,goyal2024scaling,gao2023scaling}. This choice is non-trivial - while some papers show that there is a power law relation between the predicted loss found by using validation loss and a different downstream task \citep{dubey2024llama}, it is possible for the results of a study to change completely depending on the metric used. \citet{schaeffer2023emergent}, for example, find that using linear metrics such as Token-edit distance instead of non-linear metrics such as accuracy produces smooth, continuous predictable changes in model performance, contrary to an earlier study by \citet{wei2022emergent}. Moreover, \citet{neumann2022scaling} find that they are unable to use test loss instead of Elo scores  to fit a power law.

While it is most straightforward to evaluate only the final checkpoint on the target metric, some studies may use the median score of the last several checkpoints of each training run \citet{ghorbani2021scaling}, or multiple intermediate checkpoints throughout each training run for various reasons. One common reason is that this is the only computationally feasible way to obtain a fit with sufficient confidence intervals \citep{besiroglu2024chinchilla}. For instance, the ISOFLop approach to finding the optimal $D/N$ ratio in \citet{hoffmann2022training} requires training multiple models for each targeted FLOP budget - this would be computationally prohibitive to do without using intermediate checkpoints. \citet{hoffmann2022training}, in particular, use the last $15\%$ checkpoints. Some papers also report bootstrapping values \citep{ivgi2022scaling}. This detail is often not specified in scaling law papers, with only 29 of 51 papers reporting this information - we point the reader to Appendix \ref{app:full-details} for an overview.

A related technique is performance interpolation. \citet{porian2024resolving} do not aim to exactly match the desired FLOP counts when evaluating model checkpoints mid-training. They instead interpolate between multiple model checkpoints to estimate the performance of a model with the target number of FLOPs. \citet{hoffmann2022training} and \citet{tao2024scaling} also interpolate intermediate checkpoints. \citet{hilton2023scaling}, relatedly, smooth the learning curve before extracting metric scores. 

As discussed in Section \ref{sec:model_training}, training models with too little data or too few parameters can skew the results. To prevent this issue, several works report filtering out data points before fitting their power law. \citet{henighan2020scaling} drop their smallest models, while \citet{hilton2023scaling} and \citet{hoffmann2022training} exclude early checkpoints. \citet{muennighoff2024scaling} remove outlier datapoints that perform badly due to excess parameters or excess epochs. Similarly, \citet{ivgi2022scaling} remove outlier solutions after bootstrapping.
