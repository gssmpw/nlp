
% \section{With what \emph{data}?}\label{sec:data}
\section{How do we \emph{train models}?}\label{sec:model_training}

% Which scales? How? 
%  - [DONE] Too small may not hold
%  - [DONE] number of models can affect confidence intervals
%  - [DONE] counting size also matters data (tokens vs bits) and flop counts kaplan vs hoffman CITE; non-embedding vs embedding 
%  - DITCH? how you scale architecture matters; for eg - clark shows scaling N and E is needed to separate; enc and decoder scaling can be different for architectures; in the 
%  - [DONE] 6ND is a common approximation - does not hold for very long context (these days 128k to 1M)


In order to fit a scaling law, one needs to train a range of models across multiple orders of magnitude in model size and/or dataset size. Researchers must first decide the range and distribution of $N$ and $D$ values for their training runs, in order to achieve stable convergence to a solution with high confidence, while limiting the total compute budget of all experiments. Many papers did not specify the number of data points used to fit each scaling law; those that did range from 4 to several hundred, but most used fewer than 50 data points. The specific $N$ and $D$ values also skew the optimization process towards a certain range of $N/D$ ratios, which may be too narrow to include the true optimum. Some approaches, such as using IsoFLOPs \citep{hoffmann2022training}, additionally dictate rules for choosing $N$ and $D$ values. Moreover, using a minimum $N$ or $D$ value may result in outlier values that may need to be dropped \citep{porian2024resolving,shin2023scaling,henighan2020scaling}. We investigate this choice in Section \S\ref{sec:repl-model_training}

The definition of $N$, $D$, or compute cost $C$ can affect the results of a scaling study. For example, if a study studies variation in tokenizers, a definition of training data size based on character count may be more appropriate than one based on token count \citep{tao2024scaling}. The inclusion or exclusion of embedding layer compute and parameters, may also skew the results of a study - a major factor in the different in optimal ratios determined by \cite{kaplan2020scaling} and \cite{hoffmann2022training} has been attributed to not factoring embedding FLOPs into the final compute cost \citep{pearce2024reconcilingkaplanchinchillascaling, porian2024resolving}. Given the increase in extremely long context models (128k-1M) \cite{reid2024gemini}, the commonly used training FLOPs approximation $C = 6 ND$ (see Appendix \ref{app:full-details}) may not hold for such models, given the additional cost proportional to the context length and model dimension - \citet{bi2024deepseek} introduce a new terms non-embedding FLOPs/token to account for this.

% \ml{we've gotta decide where this discussion should go }

% \luke{I agree we could probably cut some of the next few paragraphs for space if needed. The last paragraph ends well though I think.}
% \srk{ DITCH? how you scale architecture matters; for eg - clark shows scaling N and E is needed to separate; enc and decoder scaling can be different for architectures}

 % - counting size also matters data (tokens vs bits) and flop counts kaplan vs hoffman CITE; non-embedding vs embedding 

% The goal of scaling laws is generally to extrapolate findings to larger compute budgets. It is unclear which $N$ and $D$ values should be included in the data in order to predict loss at a larger scale. 


% \srk{discuss outliers being dropped and therefore, need to be sure about the scale of training}



% The scaling law form identifies the relevant variables (e.g., $N$ and $D$). However, there remain many decisions affecting the way in which data is selected to for scaling law optimization.

%  - [DONE] knowing data composition matters because knowing data quality can change exponent across different studies ofc CITE


% Moreover, hyperparams can matter
%  - [DONE] For example, learning rate schedule can changes results CITE
%  - [DONE] batch size can change
%  - [DONE] optimal hparams change with scale so determining those matters
%  - [DONE] embedding size has been shown to matter - part of scaling law 



Scaling law fit depends on the performance of each individual checkpoint, which is highly dependent on factors such as training data source, architecture and hyperparameter choice. \citet{bansal2022data} and \citet{goyal2024scaling}, for instance, discuss the effect of data quality and composition on power law exponents and constants. Repeating data has also been found to yield different scaling patterns in large language models \citep{muennighoff2024scaling,goyal2024scaling}. 

Researchers have also studied the effect of architecture choice on scaling - \citet{hestness2017deep} find that architectural improvements only shift the irreducible loss, while \citet{poli2024mechanistic} suggest that these improvements may be more significant. The way in which a model is scaled can also affect results. Within the same architecture family, \citet{clark2022unified} show that increasing the number of experts in a routed language model has diminishing returns beyond a point, while \citet{ghorbani2021scaling} find that scaling the encoder and decoder have different effects on model performance. Scaling embedding size can also drastically change scaling trends \citep{tao2024scaling}.

The optimal hyperparameters to train a model changes with scale. Changing batch size, for example, can change model performance \cite{mccandlish2018empirical, kaplan2020scaling}. Optimal learning rate is another hyperparameter shown to change with scale, though techniques such as those proposed in Tensor Programs series of papers \citep{yang2022tensor} can keep this factor constant with simple changes to initialization. More specifically, changing the learning rate schedule from a cosine decay to a constant learning rate with a cooldown (or even changing the learning rate hyperparameters) has been found to greatly affect the results of scaling laws studies \citep{hu2024minicpm,porian2024resolving,hagele2024scaling, hoffmann2022training}.

% \ml{add paragraph about max model params}

% \srk{talk abt }

% - LR \citep{hu2024minicpm}.
% - batch size

% Some factors, such as training data distribution, do not have a single optimal value and may simply be held constant for all experiments, but they change the absolute value of all power law parameters, e.g. it is not meaningful to compare power laws fit to experiments on different training data distributions. Other factors, such as batch size, may have optimal values depending on other hyperparameters, data budget, architecture, etc. It is sometimes possible to fix these factors in relation to others (e.g., scale batch size with the parameter count). There are other factors for which there is no smooth interpolation between points. For example, the width and depth of a transformer are limited to integer values, and further limited by the convention of using widths equal to small multiples of powers of 2.

% \srk{we should concretely discuss learning rate schedules and cite miniCPM, the LR schedule rate paper}
One common motivation for fitting a scaling laws is extrapolation to higher compute budgets. However, there is no consensus on the orders of magnitude up that one can project a scaling law and still find it accurate, nor on the breadth of compute budgets that should be covered by the data. We find that the range of model size $N$ and dataset size $D$ greatly varies, with the maximum value of $N$ in each paper ranging from 10M parameters to around 7B and that of $D$ being as large as 400B tokens. 
% \textcolor{blue}{
For most papers we survey, the scales are relatively modest: 13 of 51 papers train models beyond 2B parameters; most only train models smaller than 1B parameters.
% }
% \srk{idk what this refers to: Some papers show projected results at as much as 6 or 7 orders of magnitude greater than the compute budgets they experimented with. } 
It has been shown, with some controversy \cite{schaeffer2023emergent}, that scaling to significantly larger scales can result in new abilities that did not appear in smaller models \citep{wei2022emergent}. Forecasting limits to extrapolation and the appearance of new abilities at new scales is an open question.

% 
% - while SOTA models go till 100s of B; 
% - scaling laws often only go until 1b
% - it remains unclear what the corrcet scale is

% \srk{mention emergent abilities; and counterpt}

% \srk{Training data [DONE] and downstream metrics chosen may completely change exponent?}