
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{url}

\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{caption}
\usepackage{enumitem}
\usepackage{subcaption}
% \usepackage{caption}
% \usepackage{subfig}

\usepackage{lscape}

\newcommand{\ml}[1]{\textcolor{purple}{\textbf{ML: #1}}}
\newcommand{\srk}[1]{\textcolor{blue}{\textbf{SRK: #1}}}
\newcommand{\luke}[1]{\textcolor{brown}{\textbf{LZ: #1}}}


\title{(Mis)Fitting: A Survey of Scaling Laws}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Margaret Li*, Sneha Kudugunta*, Luke Zettlemoyer \\
\thanks{Equal contribution}
\texttt{\{margsli,snehark\}@cs.washington.edu} 
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}

% Scaling model training to use all available compute is challenging. Because such models can only be trained once, the architecture and hyper parameters settings must be extrapolated from smaller training runs. Recent work has focused on scaling laws for this purpose - most commonly using a power law to describe the relationship between loss and scale. This often involves training models across several magnitudes of training cost to make conclusions about the optimal way to scale models. Each aspect of this process can vary, from the specific equation being fit, to the training setup to the optimization method. We survey over 50 papers that study scaling trends: while 45 of these papers quantify these trends using a power law, most underreport crucial details needed to reproduce their findings. For instance, over half of the papers surveyed have no details on how the model training loss is fit to a power law, and only 19 of the papers surveyed provide any code to reproduce the analysis in the paper. We discuss how changes in these details that are often missing from papers on this topic can significantly change the conclusions of the study through this survey and our own analysis of the performance of different model sizes. To improve reproducibility, we propose a checklist for authors to consider while working on scaling law research.

Modern foundation models rely heavily on using scaling laws to guide crucial training decisions. Researchers often extrapolate the optimal architecture and hyper parameters settings from smaller training runs by describing the relationship between, loss, or task performance, and scale. All components of this process vary, from the specific equation being fit, to the training setup, to the optimization method. Each of these factors may affect the fitted law, and therefore, the conclusions of a given study. We discuss discrepancies in the conclusions that several prior works reach, on questions such as the optimal token to parameter ratio. We augment this discussion with our own analysis of the critical impact that changes in specific details may effect in a scaling study, and the resulting altered conclusions. Additionally, we survey over 50 papers that study scaling trends: while 45 of these papers quantify these trends using a power law, most under-report crucial details needed to reproduce their findings. To mitigate this, we we propose a checklist for authors to consider while contributing to scaling law research.

% \srk{revise these numbers once we revamp the table}


% \begin{itemize}
%     \item Deep Learning is important and impactful
%     \item Scaling Laws are important
%     \item Have to fit scaling law based on power law hypothesis
%     \item However, this varies from paper to paper and is often under specified
%     \item So, we do a survey of 40+ papers to see what people do, with instances ofs replication attempts going wrong
%     \item So, we propose a checklist for authors to consider while working on scaling law projects
%     \item And suggest some best practices
% \end{itemize}.
\end{abstract}

\input{sections/intro}

% \input{sections/preliminaries}

\input{sections/rw}


\input{sections/outline}

\input{sections/form}

\input{sections/training}

\input{sections/eval}

\input{sections/optimize}

\input{sections/analysis}

\input{sections/conclusion}

% \subsubsection*{Author Contributions}
% If you'd like to, you may include  a section for author contributions as is done
% in many journals. This is optional and at the discretion of the authors.

\subsubsection*{Acknowledgments}

We thank Aaron Defazio, Divyansh Pareek, Aditya Kusupati and Tim Althoff for their valuable feedback. We also acknowledge the computing resources and support from the Hyak supercomputer system at the
University of Washington.

\bibliography{iclr2024_conference}
\bibliographystyle{iclr2024_conference}

\appendix

\newpage


\input{sections/appendix}


\end{document}