

\begin{figure}[!h]
\centering

\begin{subfigure}{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/analysis_nd_ours_C_vs_N.png}
    \caption{\textbf{\S\ref{sec:model_training}, \S\ref{sec:repl-model_training}} Of the models we train, we choose subsets with ($N$, $D$) values which fit a particular method one might have of setting up training. Results vary dramatically between settings.}
    \label{fig:analysis_nd_ours_main}
\end{subfigure}
\hfill
\begin{subfigure}{0.47\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/analysis_counting_ours_C_vs_N.png}
    \caption{\textbf{\S\ref{sec:model_training}, \S\ref{sec:repl-model_training}} We fit power laws to our models and vary the ways we count $N$ and $C$, including or excluding embeddings, as well as using the $C=6ND$ approximation.
    The difference in resulting scaling laws is substantial and increases with scale. 
    }
    \label{fig:analysis_counting_ours_main}
\end{subfigure}


\begin{subfigure}{\textwidth}
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/analysis_init_epoch_ai_C_vs_N.png}
    % \caption{Initialization schemes, epochs}
    % \label{fig:analysis_init_epoch}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/analysis_init_ours_C_vs_N.png}
    % \caption{Inits, ours}
    % \label{fig:analysis_init_ours}
\end{subfigure}
\caption{\textbf{\S\ref{sec:opt}, \S\ref{sec:repl-opt}} We fit to data from \citet{besiroglu2024chinchilla} (left) and our data (right) to experiment with the initialization of parameters in the power law. Results do not follow a clear pattern and are unstable, suggesting the difficulty of optimizing over this space, and the presence of many local minima.
}
\label{fig:analysis_init_main}
\end{subfigure}

\caption{(\textbf{\S\ref{sec:own-repl}}) We study the effects of various decisions in the fitting of a power law, as outlined in our checklist (Appendix~\ref{sec:app_checklist}) and detailed in \S\ref{sec:power-law-form}-\S\ref{sec:opt}. For comparison, we include Llama 3 405B \citep{dubey2024llama}, the Chinchilla model \citep{hoffmann2022training}, and an estimate of the 1.5B GPT-2 model \citep{radford2019language}, as well as the \citep{kaplan2020scaling} and \citep{hoffmann2022training} scalig laws. A thick, solid line indicates lowest optimization loss in each plot. Overall, these choices strongly affect the power law fit. Thorough reporting of these decisions may be critical for scaling law reproducibility.
}
\label{fig:overall}
\end{figure}