\begin{figure}[!htp]
\centering
\begin{subfigure}{\textwidth}
    \centering
\begin{subfigure}{0.49\textwidth}
    \includegraphics[width=\textwidth]{images/analysis_form_epoch_ai_C_vs_N.pdf}  \footnotesize{\citet{hoffmann2022training,besiroglu2024chinchilla}}
\end{subfigure}
\\ \vspace{1em}
\centering
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/analysis_form_rsld_final_C_vs_N.pdf}
    \footnotesize{\citet{porian2024resolving}}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/analysis_form_rsld_ckpt_C_vs_N.pdf}
    \footnotesize{\citet{porian2024resolving} (all checkpoints)}
\end{subfigure}
\\ \vspace{1em}
\centering
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/analysis_form_misfitting_new_final_C_vs_N.pdf}
    \footnotesize{Ours}
\end{subfigure}
    \hfill
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/analysis_form_misfitting_new_ckpt_C_vs_N.pdf}
    \footnotesize{Ours (all checkpoints)}
\end{subfigure}
\\ \vspace{1em}
\caption{\textbf{\S\ref{sec:power-law-form}, \S\ref{sec:repl-power-law-form}} Using data from both \citet{besiroglu2024chinchilla} (left) and our own models (right), we compare the effects of fitting to the power law form used in Approach 3 of \citet{hoffmann2022training} with the variant used by \citet{muennighoff2024scaling}, which assumes that the exponents $\alpha, \beta$ are equal -- equivalently, that $N^*(C)$ and $D^*(C)$ scale about linearly with each other.  When using only the performance of final checkpoints from both \citet{besiroglu2024chinchilla} and our own experiments, taking this assertion results in a law much closer to the one reported by \citet{hoffmann2022training}. On our own models, we also show results when using the IsoFLOP approach from \citet{hoffmann2022training}. As we are using only results from final model checkpoints, the size of the data input to the IsoFLOP approach in this case is reduced.}
\label{fig:analysis_form_app}
\end{subfigure}
\end{figure}

\begin{figure}[]
\ContinuedFloat 
    \centering
\begin{subfigure}{\textwidth}
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/analysis_lr_misfitting_new_final_C_vs_N.pdf}
    \footnotesize{Ours}
\end{subfigure}
    \hfill
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/analysis_lr_misfitting_new_ckpt_C_vs_N.pdf}
    \footnotesize{Ours (all checkpoints)}
\end{subfigure}
    \caption{\textbf{\S\ref{sec:model_training}, \S\ref{sec:repl-model_training}} With our models, we simulate the effects of not sweeping the learning rate. As a baseline, (1) we sweep at each ($N$, $D$) pair for the optimal learning rate over a range of values at most a multiple of 2 apart. Next, (2) we use a learning rate of 1e-3 for all $N$, the optimal for our 1 billion parameter models, and do the same for (3) 2e-3 and (4) 4e-3, which is optimal for our 12 million parameter models. Lastly, we use all models across all learning rates at the same $N$ and $D$. Results vary dramatically across these settings. Somewhat surprisingly, using all learning rates results in a very similar power law to sweeping the learning rate, whereas using a fixed learning rate of 1e-3 or 4e-3 yields the lowest optimization loss or closest match to the \citet{hoffmann2022training} power laws, respectively.}
\end{subfigure}
\end{figure}

\begin{figure}[]
\ContinuedFloat
\centering 
\begin{subfigure}{\textwidth}
    \centering
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/analysis_dn_ratio_epoch_ai_C_vs_N.pdf}
\footnotesize{\citet{hoffmann2022training,besiroglu2024chinchilla}}
\end{subfigure}
\\ \vspace{1em}
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/analysis_dn_ratio_rsld_final_C_vs_N.pdf}
    \footnotesize{\citet{porian2024resolving}}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/analysis_dn_ratio_rsld_ckpt_C_vs_N.pdf}
    \footnotesize{\citet{porian2024resolving} (all checkpoints)}
\end{subfigure}
\\ \vspace{1em}
    \centering
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/analysis_dn_ratio_misfitting_new_final_C_vs_N.pdf}
    \footnotesize{Ours}
\end{subfigure}
\hfill 
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/analysis_dn_ratio_misfitting_new_ckpt_C_vs_N.pdf}
    \footnotesize{Ours (all checkpoints)}
\end{subfigure}
\\ \vspace{1em}
\caption{\textbf{\S\ref{sec:model_training}, \S\ref{sec:repl-model_training}} From all 3 datasets, we choose subsets with ($N$, $D$) values which fit a particular method one might have of setting up training. We fit with (1) all models, which for our dataset, ranges from 12 million to 1 billion parameters, then with (2) only models of up to about 100 million parameters or (3) up to 400 million parameters. We also compare the effects of a higher or lower hypothesis about the optimal $D/N$ ratio, including (4) only models with $D/N \leq 18$ or (5) $D/N \geq 22$. These ranges are designed to exclude $D/N = 20$, the rule of thumb based on \citet{hoffmann2022training}. The minimum or maximum $D/N$ ratio tested does skew results; above $10^{22}$ FLOPs, (4) and (5) fit to optimal ratios $D/N < 18$ and $D/N > 22$, respectively. Removing our largest models in (2) also creates a major shift in the predicted optimal $D/N$.}
    \label{fig:analysis_nd_app}
\end{subfigure}
\end{figure}

\begin{figure}[]
\ContinuedFloat
\centering 
\begin{subfigure}{\textwidth}
    \centering
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/analysis_counting_rsld_final_C_vs_N.pdf}
    \footnotesize{\citet{porian2024resolving}}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/analysis_counting_rsld_ckpt_C_vs_N.pdf}
    \footnotesize{\citet{porian2024resolving} (all checkpoints)}
\end{subfigure}
\\ \vspace{1em}
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/analysis_counting_misfitting_new_final_C_vs_N.pdf}
    \footnotesize{Ours}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/analysis_counting_misfitting_new_ckpt_C_vs_N.pdf}
    \footnotesize{Ours (all checkpoints)}
\end{subfigure}
\\ \vspace{1em}
\caption{\textbf{\S\ref{sec:model_training}, \S\ref{sec:repl-model_training}} With our own data and those from \citet{porian2024resolving}, we fit power laws to the same sets of models, while varying the ways we count $N$ and $C$. We compare (1) including embeddings, which is our baseline, with (2) excluding embeddings only in $N$, (3) excluding embeddings only in $C$, (4) excluding embeddings in both $N$ and $C$. We also compare to using the $C=6ND$ approximation, including embedding parameters. Throughout this work, we calculate the FLOPs in a manner similar to \citet{hoffmann2022training}, and we open source the code for these calculations.
    With both datasets, the exclusion of embeddings in FLOPs has very little impact on the final fit. Similarly, using the $C=6ND$ approximation has no visible impact. For the \citet{porian2024resolving} models, the exclusion of embedding parameters in the calculation of $N$ results in scaling laws which differ substantially, and with increasing divergences at large scales. 
    }
    \label{fig:analysis_counting_app}
\end{subfigure}
\end{figure}

\begin{figure}[]
\ContinuedFloat
\centering 
\begin{subfigure}{\textwidth}
\centering
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/analysis_filter_rsld_ckpt_C_vs_N.pdf}
    \footnotesize{\citet{porian2024resolving} (all checkpoints)}
\end{subfigure}
% \hfill
\\ \vspace{1em}
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/analysis_filter_misfitting_new_ckpt_C_vs_N.pdf}
    \footnotesize{Ours (all checkpoints)}
\end{subfigure}
\\ \vspace{1em}
\caption{\textbf{\S\ref{sec:data}, \S\ref{sec:repl-data}} With models from \citet{porian2024resolving} and our own dataset, we compare (1) fitting a power law using only the final checkpoint with (2) using all mid-training checkpoints (3) using all checkpoints, starting 10\% through training, (4) the same, starting 20\% through training, and (5) the same again, starting 50\% through training. We observe that (2)-(5) consistently
% , starting with 20\% through training, at which point warmup has concluded for all but a few of the smallest models, and (3) using mid-training checkpoints, starting at 50\% through training. (2) and (3) 
yields power laws more similar to that reported by \citet{hoffmann2022training}, so we also repeat all analyses in Figures~\ref{fig:analysis_form}-\ref{fig:analysis_counting_ours}. Using mid-training checkpoints sometimes results in more stable fits which are similar to the \citet{hoffmann2022training} scaling laws, but the effect is noisy and dependent on other decisions
}
\label{fig:analysis_checkpoint_app}        
\end{subfigure}
\end{figure}

\begin{figure}[]
\ContinuedFloat
\centering 
\begin{subfigure}{\textwidth}
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/analysis_init_epoch_ai_C_vs_N.pdf}
    \footnotesize{\citet{hoffmann2022training,besiroglu2024chinchilla}}
    % \caption{Initialization schemes, epochs}
    % \label{fig:analysis_init_epoch}
\end{subfigure}
\\ \vspace{1em}
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/analysis_init_rsld_final_C_vs_N.pdf}
    \footnotesize{\citet{porian2024resolving}}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/analysis_init_rsld_ckpt_C_vs_N.pdf}
    \footnotesize{\citet{porian2024resolving} (all checkpoints)}
\end{subfigure}
\\ \vspace{1em}
    \centering
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/analysis_init_misfitting_new_final_C_vs_N.pdf}
    \footnotesize{Ours}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/analysis_init_misfitting_new_ckpt_C_vs_N.pdf}
    \footnotesize{Ours (all checkpoints)}
\end{subfigure}
\\ \vspace{1em}
\caption{\textbf{\S\ref{sec:opt}, \S\ref{sec:repl-opt}} We fit to data from all 3 datasets to experiment with the initialization of parameters in the power law. We start with (1) optimizing every point in a grid search of 6x6x5x5x5=4500 initializations \citep{hoffmann2022training}, (2) randomly sampling from only a single initialization in this grid, (3) searching for the lowest loss initialization point \citep{caballero2022broken}, (4) randomly sampling 100 points, and (5) initializing with the coefficients found in \citet{hoffmann2022training}, as \citet{besiroglu2024chinchilla} does. With the \citet{besiroglu2024chinchilla} data, (5) yields a fit nearly identical to that reported by \citet{hoffmann2022training}, although (1) results in the lowest fitting loss. With the \citet{porian2024resolving} data, all approaches except (2) yield a very similar fit, which gives a recommended $D/N$ ratio similar to that of\citet{hoffmann2022training}. However, using our data, (2) optimizing over only the most optimal initialization yields the best match to the \citet{hoffmann2022training} power laws, followed by (5) initialization from the reported \citet{hoffmann2022training} scaling law parameters. Optimizing over the full grid yields the power law which diverges most from the \citet{hoffmann2022training} law, suggesting the difficulty of optimizing over this space, and the presence of many local minima.
}
\label{fig:analysis_init_app}
\end{subfigure}
\end{figure}

\begin{figure}[]
\ContinuedFloat
\centering 
\begin{subfigure}{\textwidth}
    \centering
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/analysis_loss_epoch_ai_C_vs_N.pdf}
    \footnotesize{\citet{hoffmann2022training,besiroglu2024chinchilla}}
\end{subfigure}
\\ \vspace{1em}
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/analysis_loss_rsld_final_C_vs_N.pdf}
    \footnotesize{\citet{porian2024resolving}}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/analysis_loss_rsld_ckpt_C_vs_N.pdf}
    \footnotesize{\citet{porian2024resolving} (all checkpoints)}
\end{subfigure}
\\ \vspace{1em}
    \centering
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/analysis_loss_misfitting_new_final_C_vs_N.pdf}
    \footnotesize{Ours}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/analysis_loss_misfitting_new_ckpt_C_vs_N.pdf}
    \footnotesize{Ours (all checkpoints)}
\end{subfigure}
\\ \vspace{1em}
\caption{\textbf{\S\ref{sec:opt}, \S\ref{sec:repl-opt}} We fit a power law to data from from all 3 datasets, minimizing different objective functions: (1) the baseline log-Huber loss, (2) MSE, (3) MAE, and (4) the Huber loss. 
% We found significantly more stability across the loss functions when fitting to our own data, but we draw no conclusions from such a small sample, except 
The resulting power laws are less disparate than when varying many of the other factors discussed above and generally fall near the power law parameters reported by \citet{kaplan2020scaling} and \citet{hoffmann2022training}, but this is still a wide range of recommended optimal parameter counts for each compute budget. Overall, the loss function behavior is not predictable, given the differences between loss functions when looking at the power laws resulting from these three sources of data.}
\label{fig:analysis_loss_app}
\end{subfigure}
\end{figure}


\begin{figure}[]
\ContinuedFloat
\centering 
\begin{subfigure}{\textwidth}
    \centering
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/analysis_opt_epoch_ai_C_vs_N.pdf}
\footnotesize{\citet{hoffmann2022training,besiroglu2024chinchilla}}
\end{subfigure}
\\ \vspace{1em}
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/analysis_opt_rsld_final_C_vs_N.pdf}
    \footnotesize{\citet{porian2024resolving}}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/analysis_opt_rsld_ckpt_C_vs_N.pdf}
    \footnotesize{\citet{porian2024resolving} (all checkpoints)}
\end{subfigure}
\\ \vspace{1em}
    \centering
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/analysis_opt_misfitting_new_final_C_vs_N.pdf}
    \footnotesize{Ours}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/analysis_opt_misfitting_new_ckpt_C_vs_N.pdf}
    \footnotesize{Ours (all checkpoints)}
\end{subfigure}
\caption{\textbf{\S\ref{sec:opt}, \S\ref{sec:repl-opt}} We fit a power law to data from all 3 datasets using various optimizers, beginning with the original (1) L-BFGS. L-BFGS and BFGS implementations have an early stopping mechanism, which conditions on the stability of the solution between optimization steps. We set this threshold for L-BFGS to a (2) higher value 1e-4 (stopping earlier).
% , and a (3) slightly lower value 1e-6. 
We found that using any higher or lower values resulted in the same solutions or instability for all three datasets. L-BFGS and BFGS also have the option to use the true gradient of the loss, instead of an estimate, which is the default. In this figure, we include this setting, (3) using the true gradient for L-BFGS. We compare these L-BFGS settings to (4) BFGS. We test the same tolerance value and gradient settings, and find that none of these options change the outcome of BFGS in our analysis, and omit them from this figure for legibility. Finally, we compare to (5) non-linear least squares and (6) pure grid search, using a grid 5 times more dense along each axis as we used initialization with other optimizers. This density is chosen to approximately match the runtime of L-BFGS.
% and grid search without additional optimization.
Many of these optimizers do converge to similar solutions, but this depends on the data, and the settings which diverge from this majority do not follow any immediately evident pattern.} 
\label{fig:analysis_opt_app}
\end{subfigure}
\end{figure}

\begin{figure}[]
\ContinuedFloat
\centering 
\caption{(\textbf{\S\ref{sec:own-repl}}) We replicate the plots in Figure~\ref{fig:overall}, reorganized so that analyses for datasets with mid-training checkpoints appear alongside those for data with final checkpoints only. This side-by-side comparison makes the difference in power law fits apparent, further underscoring the impact of including mid-training datapoints.
We study the effects of various decisions in the fitting of a power law, as outlined in our checklist (Appendix~\ref{sec:app_checklist}) and detailed in \S\ref{sec:power-law-form}-\S\ref{sec:opt}. For each set of analyses, we the scaling laws found by \citep{kaplan2020scaling} and \citep{hoffmann2022training} for comparison. We also include markers indicating 3 existing models for comparison purposes: Llama 3 405B \citep{dubey2024llama}, the Chinchilla model \citep{hoffmann2022training}, and an estimate of the 1.5B GPT-2 model \citep{radford2019language}, for which we know details of the dataset storage size and word count, but not an exact count of data BPE tokens, which we estimate at 100B. We additionally annotate, at the compute budget $C$ for each of these 3 reference points, the maximum and minimum \textit{predicted} (i.e. extrapolated) optimal model parameter count $N_{opt}$ and data budget $D_{opt}$ from the fitted power laws. We use a thicker, solid line for the method in each plot which achieves the lowest optimization loss, with the exception of the plots comparing power law form, those comparing loss functions and those comparing optimizers, for which this would be nonsensical.
We find overall, throughout our analyses, that all of the decisions we explore have an impact on the final fit of the power law, supporting our conclusion that more thorough reporting of these decisions is critical for scaling law reproducibility.
}
\label{fig:app_mega}
\end{figure}
