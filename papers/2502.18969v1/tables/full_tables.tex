
\begin{table}[!htp]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lllllllll}
\toprule
Paper & Domain & Training Code? & Analysis Code? & Checkpoints? & Metric Scores? \\
\midrule
\cite{rosenfeld2019constructive} & Vision, LM & N & N & N & N \\
\cite{mikamiscaling} & Vision & N & Y & Y & Y \\
\cite{schaeffer2023emergent} & LM & N & N & N & N \\
\cite{sardana2023beyond} & LM & N & N & N & N \\
\cite{sorscher2022beyond} & Vision & N & N & N & Y \\
\cite{caballero2022broken} & LM & N & Y & N & Y \\
\cite{besiroglu2024chinchilla} & LM &  & Y & N & Y \\
\cite{gordon2021data} & NMT & Y & Y & Y & Y \\
\cite{bansal2022data} & NMT & N & N & N & N \\
\cite{hestness2017deep} & NMT, LM, Vision, Speech & N & N & N & N \\
\cite{bi2024deepseek} & LM & N & N & N & N \\
\cite{bahri2021explaining} & Vision & N & N & N & N \\
\cite{geiping2022much} & Vision & Y & Y & N & N \\
\cite{poli2024mechanistic} & LM & N & N & N & N \\
\cite{hu2024minicpm} & LM & Y & N & N & N \\
\cite{hashimoto2021model} & NLP & N & N & N & N \\
\cite{ruan2024observational} & LM & Y & Y & N & Y \\
\cite{anil2023palm} & LM & N & N & N & N \\
\cite{pearce2024reconciling} & LM & N & Y & N & N \\
\cite{cherti2023reproducible} & VLM & Y & Y & Y & Y \\
\cite{porian2024resolving} & LM & Y & Y & N & Y \\
\cite{alabdulmohsin2022revisiting} & LM, Vision & N & Y & Y & Y \\
\cite{gao2024scalingevaluatingsparseautoencoders} & NLP & Y & Y & Y & N \\
\cite{muennighoff2024scaling} & LM & Y & Y & Y & N \\
\cite{rae2021scaling} & LM & N & N & N & N \\
\cite{shin2023scaling} & RecSys & N & N & N & N \\
\cite{hernandez2022scaling} & LM & N & N & N & N \\
\cite{filipovich2022scaling} & LM & N & N & N & N \\
\cite{neumann2022scaling} & RL & Y & Y* & Y & N \\
\cite{droppo2021scaling} & Speech & N & N & N & N \\
\cite{henighan2020scaling} & LM, Vision, Video, VLM & N & N & N & N \\
\cite{goyal2024scaling} & LM, Vision, VLM & N & Y & N & Y \\
\cite{aghajanyan2023scaling} & Multimodal LM & N & N & N & N \\
\cite{kaplan2020scaling} & LM & N & N & N & N \\
\cite{ghorbani2021scaling} & NMT & N & Y & N & N \\
\cite{gao2023scaling} & RL/LM & N & N & N & N \\
\cite{hilton2023scaling} & RL & N & N & N & N \\
\cite{frantar2023scaling} & LM, Vision & N & N & N & N \\
\cite{prato2021scaling} & Vision & Y* & Y & Y & Y \\
\cite{covert2024scaling} & LM & Y & Y & N & N \\
\cite{hernandez2021scaling} & LM & N & N & N & N \\
\cite{ivgi2022scaling} & NLP & N & N & N & N \\
\cite{tay2022scaling} & LM & N & N & N & N \\
\cite{tao2024scaling} & LM & N & Y & N & Y \\
\cite{jones2021scaling} & RL & Y & Y & N & Y \\
\cite{zhai2022scaling} & Vision & Y & N & N & N \\
\cite{dettmers2023case} & LM & N & N & N & N \\
\cite{dubey2024llama} & LM & N & N & N & N \\
\cite{hoffmann2022training} & LM & N & N & N & N \\
\cite{ardalani2022understanding} & RecSys & N & N & N & N \\
\cite{clark2022unified} & LM & N & Y & N & Y \\
\bottomrule
\end{tabular}
}

\caption{Details on domain of experiments and availability of code by category for each paper surveyed.}
\label{tab:full-basic}
\end{table}

% LaTeX code for Dataframe 2:
\begin{table}[]
\centering
\resizebox{\textwidth}{!}{%
% LaTeX code for Dataframe (9, 12):
\begin{tabular}{lllll}
\toprule
Paper & Power Law Form & Purpose Of Power Law (E.G., Performance Prediction, Optimal Ratio) & \# Power Law Parameters & \# Of Scaling Laws \\
\midrule
\cite{rosenfeld2019constructive} & $\tilde{\epsilon}(m, n)=a n^{-\alpha}+b m^{-\beta}+c_{\infty}$ & Performance Prediction & 5-6 & 8 \\
\cite{mikamiscaling} & $L(n, s)=\delta\left(\gamma+n^{-\alpha}\right) s^{-\beta}$ & Performance Prediction & 4 & 3 \\
\cite{schaeffer2023emergent} & None & N/A & NA & NA \\
\cite{sardana2023beyond} & $L(N, D) = E + \frac{A}{N^\alpha} + \frac{B}{D^\beta} $; $N^*\left(\ell, D_{\text {inf }}\right), D_{\text {tr }}^*\left(\ell, D_{\text {inf }}\right)={\arg \min } _{N, D_{\mathrm{tr}} \mid L\left(N, D_{\mathrm{tr}}\right)=\ell}$ & Performance Prediction & 5 & 4 \\
\cite{sorscher2022beyond} & $c \cdot \alpha^{-\beta} ,  c \cdot \exp (-b \alpha)$ & Performance Prediction & 2 & 34 \\
\cite{caballero2022broken} & $y=a+\left(b x^{-c_0}\right) \prod_{i=1}^n\left(1+\left(\frac{x}{d_i}\right)^{1 / f_i}\right)^{-c_i * f_i}$ & Performance Prediction & 5+ & 100+ \\
\cite{besiroglu2024chinchilla} & $L(N, D) = E + \frac{A}{N^\alpha} + \frac{B}{D^\beta} $ & Performance Prediction & 5 & 1 \\
\cite{gordon2021data} & $L(N, D) = \left[ \left( \frac{N}{N_c}\right)^{\frac{\alpha_N}{\alpha_D}} + \frac{D}{D_c} \right]^{\alpha_D} $ & Performance Prediction & 4 & 3 \\
\cite{bansal2022data} & $L(D)=\alpha\left(D^{-1}+C\right)^p$ & Performance Prediction & 3 & 20 \\
\cite{hestness2017deep} & $\varepsilon(m) \sim \alpha m^{\beta_g}+\gamma$ & Performance Prediction & 3 & 17 \\
\cite{bi2024deepseek} & $\begin{aligned} M_{\mathrm{opt}} & =M_{\mathrm{base}} \cdot C^a \\ D_{\mathrm{opt}} & =D_{\mathrm{base}} \cdot C^b\end{aligned}$, $\begin{aligned} & \eta_{\mathrm{opt}}=0.3118 \cdot C^{-0.1250} \\ & B_{\mathrm{opt}}=0.2920 \cdot C^{0.3271}\end{aligned}$ & Optimal Ratio, Performance Prediction & 2 & 5 \\
\cite{bahri2021explaining} & $L(D) \propto D^{-\alpha_K}, \quad L(P) \propto P^{-\alpha_K}$ & Performance Prediction & 2 & 35 \\
\cite{geiping2022much} & $f(x)=a x^{-c}+b$, $v_{\text {Effective Extra Samples from Augmentations }}(x)=f_{\text {ref }}^{-1}\left(f_{\text {aug }}(x)\right)-x$ & Performance Prediction & 3 & ~50 \\
\cite{poli2024mechanistic} & $\log N^* \propto a \log C$ and $\log D^* \propto b \log C$ & Performance Prediction & 2 &  \\
\cite{hu2024minicpm} & $L(N, D)=C_N N^{-\alpha}+C_D D^{-\beta}+L_0$ & Performance Prediction & 5 & 6 \\
\cite{hashimoto2021model} & $\min _{\lambda, \alpha} \mathbb{E}_{\hat{q}, \hat{n}}\left[\left(\log (R(\hat{n}, \hat{q})-\epsilon)-\alpha \log (\hat{n})+\log \left(C_\lambda(\hat{q})\right)\right)^2\right]$ $R(\hat{n}, \hat{q})=\mathbb{E}\left[\ell\left(\hat{\theta}\left(p_{\hat{n}, \hat{q}}\right) ; x, y\right)\right]$ & Performance Prediction & 2+n(data mixes) & 4 \\
\cite{ruan2024observational} & $E_m \approx h \sigma\left(\beta^{\top} S_m+\alpha\right)$ & Performance Prediction & 3 &  \\
\cite{anil2023palm} & $N^{\star}(C) \approx N_0^{\star} \cdot C^a$ & Performance Prediction & 2 & 1 \\
\cite{pearce2024reconciling} & $N^*_{{\setminus E}} = b C_{{\setminus E}}^m$ $L = bC^m$ & Optimal Ratio, Performance Prediction & 2 & 1 \\
\cite{cherti2023reproducible} & $E=\beta C^{\alpha}$ & Performance Prediction & 2 & 8 \\
\cite{porian2024resolving} & $N^{\star}(C) \approx N_0^{\star} \cdot C^a$ & Optimal Ratio & 2 & 6 \\
\cite{alabdulmohsin2022revisiting} & $\varepsilon_x=\beta x^c$; $\varepsilon_x - \varepsilon_\infty=\beta x^c$; $\varepsilon_x=\beta (x^{-1} + \gamma)^{-c}$;   $\varepsilon_x=\gamma(x)(1+\gamma(x))^{-1} \varepsilon_0+(1+\gamma(x))^{-1} \varepsilon_{\infty}$ & Performance Prediction & 2-4 & ~600 \\
\cite{gao2024scalingevaluatingsparseautoencoders} & $L(n, k)=\exp \left(\alpha+\beta_k \log (k)+\beta_n \log (n)+\gamma \log (k) \log (n)\right)+\exp (\zeta+\eta \log (k))$ & Performance Prediction & 2-6 & 1 \\
\cite{muennighoff2024scaling} & $L\left(U_N, U_D, R_N, R_D\right)=\frac{A}{\left(U_N+U_N R_N^*\left(1-e^{\frac{-R_N}{R_N^*}}\right)\right)^\alpha}+\frac{B}{\left(U_D+U_D R_D^*\left(1-e^{\frac{-R_D}{R_D^*}}\right)\right)^\beta}+E$ & Performance Prediction & 2 (+4) & 1 \\
\cite{rae2021scaling} & None & Performance Prediction & N/A & N/A \\
\cite{shin2023scaling} & None & Scaling trend & NA & NA \\
\cite{hernandez2022scaling} & $E=k * N^\alpha$ & Optimal Ratio & 2 & 1 \\
\cite{filipovich2022scaling} & $\mathcal{L}(C)=\left(C_c C\right)^{\alpha_C}$ & Performance Prediction & 2 & 3 \\
\cite{neumann2022scaling} & $N_{\text {opt }}(C)=\left(\frac{C}{C_0}\right)^{\alpha_C^{o p t}}$, $E_i=\frac{1}{1+\left(N_j / N_i\right)^{\alpha_N}}$ & Performance Prediction & 2 & 3 * 2 \\
\cite{droppo2021scaling} & $L(N, D)=\left[\left(L_{\infty}\right)^{\frac{1}{\alpha}}+\left(\frac{N_C}{N}\right)^{\frac{\alpha_N}{\alpha}}+\left(\frac{D_C}{D}\right)^{\frac{\alpha_D}{\alpha}}\right]^\alpha$ & Performance Prediction & 6 & 3 \\
\cite{henighan2020scaling} & $L(x)=L_{\infty}+\left(\frac{x_0}{x}\right)^{\alpha_x}$ & Performance Prediction & 3 & 36 \\
\cite{goyal2024scaling} & $y_k=a \cdot n_1^{b_1} \prod_{j=2}^k\left(\frac{n_j}{n_{j-1}}\right)^{b_j}+d$ & Performance Prediction & 2+ 2*n(data mixes) & 1 \\
\cite{aghajanyan2023scaling} & $L(N, D_j)=E_j + \frac{A_j}{N^{\alpha_j}} + \frac{B_j}{|D_j|^{\beta_j}}$, $L(N, D_i, D_j) = [\frac{L(N, D_i) + L(N, D_j)}{2}] - C_{i,j} + \frac{A_{i,j}}{N^{\alpha_{i,j}}} + \frac{B_{i,j}}{|D_i|+|D_j|^{\beta_{i,j}}}$ & Performance Prediction & 5 & 14 \\
\cite{kaplan2020scaling} & $L(N, D) = \left[ \left( \frac{N}{N_c}\right)^{\frac{\alpha_N}{\alpha_D}} + \frac{D}{D_c} \right]^{\alpha_D}$     & Performance Prediction & 4 & ~7 \\
\cite{ghorbani2021scaling} & $\mathrm{BLEU}=c_B L^{-p_B}$, $\hat{L}_{o p t}(B)=\alpha^* B^{-\left(p_d+p_e\right)}+L_{\infty}, \quad \alpha^* \equiv \alpha\left(\frac{\bar{N}_e\left(p_e+p_d\right)}{p_e}\right)^{p_e}\left(\frac{\bar{N}_d\left(p_e+p_d\right)}{p_d}\right)^{p_d}$ & Optimal Ratio, Performance Prediction & 6 & ~8 \\
\cite{gao2023scaling} & $\begin{aligned} & R_{\mathrm{bo} n}(d)=d\left(\alpha_{\mathrm{bo} n}-\beta_{\mathrm{bo} n} d\right), \\ & R_{\mathrm{RL}}(d)=d\left(\alpha_{\mathrm{RL}}-\beta_{\mathrm{RL}} \log d\right)\end{aligned}$ & Performance Prediction & 2 & 2 \\
\cite{hilton2023scaling} & $I^{-\beta}=\left(\frac{N_c}{N}\right)^{\alpha_N}+\left(\frac{E_c}{E}\right)^{\alpha_E}$ & Optimal Ratio, Performance Prediction & 5 & 3 \\
\cite{frantar2023scaling} & $L(S, N, D)=\left(a_S(1-S)^{b_S}+c_S\right) \cdot\left(\frac{1}{N}\right)^{b_N}+\left(\frac{a_D}{D}\right)^{b_D}+c$ & Optimal Ratio, Performance Prediction & 7 & 2 \\
\cite{prato2021scaling} & $\begin{aligned} & \operatorname{Err}(N)=\operatorname{Err}_{\infty}+k N^\alpha, \\ & \operatorname{Err}(C)=\operatorname{Err}_{\infty}+k C^\alpha,\end{aligned}$ & Performance Prediction & 3 & 12 \\
\cite{covert2024scaling} & $\log \left|\psi_k(z)\right| \approx \log |c(z)|-\alpha(z) \log (k)$ & Performance Prediction & 2 & Many \\
\cite{hernandez2021scaling} & $L \approx\left[\left(\frac{N_C}{N}\right)^{\frac{\alpha_N}{\alpha_D}}+\frac{D_C}{k\left(D_F\right)^\alpha(N)^\beta}\right]^{\alpha_D}$ & Performance Prediction & 3 & 1 \\
\cite{ivgi2022scaling} & NS & Performance Prediction & NA & NA \\
\cite{tay2022scaling} & None & Scaling trend & NA & NA \\
\cite{tao2024scaling} & $N_{\mathrm{v}}^{\mathrm{opt}}=N_{\mathrm{v}}^0 *\left(\frac{N_{\mathrm{nv}}}{N_{\mathrm{nv}}^0}\right)^\gamma$, $\mathcal{L}_u=-E+\frac{A_1}{N_{\mathrm{nv}}^{\alpha_1}}+\frac{A_2}{N_{\mathrm{v}}^{\alpha_2}}+\frac{B}{D^\beta}$ & Optimal Ratio, Performance Prediction & 7 & 2 \\
\cite{jones2021scaling} & $\begin{aligned} \text { plateau } & =m_{\text {boardsize }}^{\text {plateau }} \cdot \text { boardsize }+c^{\text {plateau }} \\ \text { incline } & =m_{\text {boardsize }}^{\text {incline }} \cdot \text { boardsize }+m_{\text {flops }}^{\text {incline }} \cdot \log \text { flop }+c^{\text {incline }} \\ \text { elo } & =\text { incline.clamp }(\text { plateau }, 0)\end{aligned}$ & Performance Prediction & 5 & 1 \\
\cite{zhai2022scaling} & $E=\alpha+\beta(C+\gamma)^{-\mu}$ & Performance Prediction & 4 & 3 \\
\cite{dettmers2023case} & None & Scaling trend & NA & NA \\
\cite{dubey2024llama} & $N^{\star}(C)=A C^\alpha$. & Optimal Ratio  & 2 & 2 \\
\cite{hoffmann2022training} & A3: $L(N, D) = E + \frac{A}{N^\alpha} + \frac{B}{D^\beta} $ & Optimal Ratio, Performance Prediction & 5 & 3 \\
\cite{ardalani2022understanding} & $\left(\alpha x^{-\beta}+\gamma\right)$ & Performance Prediction & 3 & 3 \\
\cite{clark2022unified} & $\log L(N, E) \triangleq+a \log N+b \log E+c \log N \log E+d$ & Performance Prediction & 4 & 3 \\
\bottomrule
\end{tabular}
}

\caption{Details on power law for each paper surveyed.}
\label{tab:full-powerlaw}
\end{table}

% LaTeX code for Dataframe 2:
\begin{table}[]
\centering
\resizebox{\textwidth}{!}{%
% LaTeX code for Dataframe 3:
% LaTeX code for Dataframe (13, 19):
\begin{tabular}{llllllll}
\toprule
Paper & Training Runs / Law & Max. Training Flops & Max. Training Params & Max. Training Data & Data Described? & Hyperparameters Described? & How Are Model Params Counted \\
&&&&&&& (E.G., W/ Or W/Out Embeddings) \\
\midrule
\cite{rosenfeld2019constructive} & 42-49 &  & 0.7M-70M & 100M words / 1.2M images & Y & Y & Non-embedding \\
\cite{mikamiscaling} & 7 &  & ResNet-101 & 64k-1.28M images & Y & Y & NA \\
\cite{schaeffer2023emergent} & 4 &  & $10^{11}$ & NA & Y & NA & Non-embedding \\
\cite{sardana2023beyond} & 47 &  & 150M-6B & 1.5B-1.25T tokens & N & Y & NA \\
\cite{sorscher2022beyond} & ~60 &  & 86M (ViT) & 200 epochs & Y & Y & NA \\
\cite{caballero2022broken} & 3-40 &  & NS & NS & N & N & NS \\
\cite{besiroglu2024chinchilla} & NA & NA & NA & NA & Y & NA & Non-embedding \\
\cite{gordon2021data} & 45-55 &  & 56M & 28.3M-51.1M examples & Y & Y & Non-embedding \\
\cite{bansal2022data} & 10 &  & 170M-800M & 500K-512M sentences (28B tokens) & Y & Y & NS \\
\cite{hestness2017deep} & ~9 &  & upto 193M  & $2^{19}-2^{28}$ tokens, upto $2^9$ images, $2k$ audio hours & Y & Y & NS \\
\cite{bi2024deepseek} & 80 & $1e17-3e20$ &  &  & Y & Y & Non-embedding \\
\cite{bahri2021explaining} & 8-27 &  & 36.5M & upto 78k steps; 100 epochs & Y & Y & NS \\
\cite{geiping2022much} & 13 &  & ResNet-18 & upto 7.6M images & Y & Y & NS \\
\cite{poli2024mechanistic} & 500 total & 8.00E+19 & 70M-7B &  & Y & Y & Non-embedding  \\
\cite{hu2024minicpm} & 36 &  & 40M-2B & 400M-120B tokens & Y & Y & Non-embedding  \\
\cite{hashimoto2021model} &  &  &  & upto 600k sentences & Y & Y & NA \\
\cite{ruan2024observational} & 27* -77* &  & 70B-180B & 3T-6T tokens & N/A & N/A & N.S. \\
\cite{anil2023palm} & 12 & 1.00E+22 & 15B & 4.00E+11 & N & N & Non-embedding \\
\cite{pearce2024reconciling} & 20 (simulated), 25 (real) &  & 1.5B (simulated), 4.6M (real) & 23B (simulated), 500M (real) tokens & Y & Y & w/ Embedding and Non-embedding considered separately \\
\cite{cherti2023reproducible} & 3* - 29 &  & 214M & 34B (pretrain), 2B (finetune) examples  & Y & Y & N.S \\
\cite{porian2024resolving} & 16 & 2.00E+19 & 901M &  & Y & Y & w/ Embedding and Non-embedding considered separately \\
\cite{alabdulmohsin2022revisiting} & 1* &  & 110M-1B & 1e6-1e10 ex / 3e11 tokens & Mixed & N & N/A \\
\cite{gao2024scalingevaluatingsparseautoencoders} & N.S & N.S & N.S & N.S & N & N & N.S \\
\cite{muennighoff2024scaling} & 142 &  & 8,7B & 900B tokens & Y & Y & w/ embedding \\
\cite{rae2021scaling} & 4 & 6.31E+23 & 280B &  & Y & Y & Non-embedding \\
\cite{shin2023scaling} & 17 & ~0.1 PF Days & 160M & 500M-50B tokens & Y & Y & NA \\
\cite{hernandez2022scaling} & 56 &  & 1.5M-800M & 100B tokens & N & N & NS \\
\cite{filipovich2022scaling} & 4 &  & 57-509M & 30B token & Y & N & NS \\
\cite{neumann2022scaling} & 14 &  & ~$5*10^5$ & $10^4$ steps & Y & Y & NS \\
\cite{droppo2021scaling} & 5-21 &  & ~$10^7$ & 134-23k hrs speech & Y & Y & NS \\
\cite{henighan2020scaling} & 6-10 &  & ~$10^11$ & ~$10^12$ tokens & Y & Y & Non-embedding \\
\cite{goyal2024scaling} & 5 &  & CLIP L/14 - ~300M +63M & 32-640M samples & Y & Y & Embedding \\
\cite{aghajanyan2023scaling} & 21 &  & 8M-6.7B & 5-100B tokens & Y & Y & Non-embedding \\
\cite{kaplan2020scaling} & ~40-150 &  & 1.5B & 23B tokens & Y & Y & Non-embedding \\
\cite{ghorbani2021scaling} & 12-14 &  & 191-3B & NS & Y & Y & Non-embedding \\
\cite{gao2023scaling} & 9 &  & 3B & 120-90k & N & Y & NS \\
\cite{hilton2023scaling} & NS & $10^{20}$ &  &  & Y & Y & NS \\
\cite{frantar2023scaling} & 48 and 112 &  & 0.66M-85M  & 1.8B images, 65B tokens & Y & Y & Non-embedding \\
\cite{prato2021scaling} & 5 &  &  & $10^6$ samples & Y & N & NA \\
\cite{covert2024scaling} & 10 &  & NA & 1000 samples for IMDB & Y & Y & NA \\
\cite{hernandez2021scaling} & NS & $10^{21}$ & $10^8$ &  & Y & N & Non-embedding \\
\cite{ivgi2022scaling} & 5-8 &  & $10^4-10^8$ & varies; 500k steps PT & Y & Y & Non-embedding \\
\cite{tay2022scaling} &  &  & 16-30B & $2^19$ & Y & Y & NA \\
\cite{tao2024scaling} & 60 &  & 33M-1.13B NV + 4-96k V & 4.3B-509B Characters & Y & Y & Embedding and Non-embedding considered separately \\
\cite{jones2021scaling} & 200 & 1E+12-1E+17 &  & 4E+08-2E+09 & Y & Y & NA \\
\cite{zhai2022scaling} & 44 &  & 5.4M-1.8B & 1-13M images & Y & Y & NA \\
\cite{dettmers2023case} & 4 &  & 19M-176B & NA & NA & Y & NA \\
\cite{dubey2024llama} & NS & $6*10^{18}-10^22$ & 40M-16B &  & Y* & Y & NS \\
\cite{hoffmann2022training} & ~200-450 & $6*10^{18}-3*10^{21}$ & 16B & 5B-400B tokens & Y & Y & Non-embedding \\
\cite{ardalani2022understanding} & NS & $10^2$-$10^6$ TFlops &  & ~5M-5B samples & N & N & All are considered \\
\cite{clark2022unified} & 56 &  & 15M-1.3B & 130B tokens & Y & Y & Non-embedding \\
\bottomrule
\end{tabular}

}

\caption{Details on training setup for each paper surveyed.}
\label{tab:full-setup}
\end{table}

% LaTeX code for Dataframe 2:
\begin{table}[]
\centering
\resizebox{\textwidth}{!}{%

% LaTeX code for Dataframe (20, 24):
\begin{tabular}{lllll}
\toprule
Paper & Data Points Per Law? & Scaling Law Metric & Modification Of Final Metric? & Subsets Of Data Used \\
\midrule
\cite{rosenfeld2019constructive} & 42-49 & Loss / Top1 Error & N & N \\
\cite{mikamiscaling} & 7 & Error Rate & N & N \\
\cite{schaeffer2023emergent} & NA & Various downstream & NA & NA \\
\cite{sardana2023beyond} & NS & Loss & NS & NS \\
\cite{sorscher2022beyond} & ~60 & Error Rate & NA & NA \\
\cite{caballero2022broken} & 3-40 & FID, Loss, Error Rate, Elo Score & N & NS \\
\cite{besiroglu2024chinchilla} & 245 & Loss & N & N \\
\cite{gordon2021data} & 45-55 & Loss & N & N \\
\cite{bansal2022data} & NS & Loss, BLEU & NS & NS \\
\cite{hestness2017deep} & NS & Token Error, CER, Error Rate, Loss & Median min. validation error across multiple training runs with separate random seeds & NS \\
\cite{bi2024deepseek} & upto 80 & Validation bits-per-byte & NS & NS \\
\cite{bahri2021explaining} & upto 100 & Loss & NS & NS \\
\cite{geiping2022much} & ~50 & Effective Extra Samples & Interpolation & NS \\
\cite{poli2024mechanistic} & NS & Loss & NS & NS \\
\cite{hu2024minicpm} & NS & Loss & NS & NS \\
\cite{hashimoto2021model} & NS & Loss & NS & NS \\
\cite{ruan2024observational} &  & Various downstream & N & N \\
\cite{anil2023palm} & 12 & Loss & N & N \\
\cite{pearce2024reconciling} & 20, 5 & Loss & N & N \\
\cite{cherti2023reproducible} & 3-29 & Error Rate & N & N \\
\cite{porian2024resolving} & 12 & Loss & N & N \\
\cite{alabdulmohsin2022revisiting} & N.S. & Loss / Accuracy & N & N/A \\
\cite{gao2024scalingevaluatingsparseautoencoders} & N.S & MSE & N.S & N.S \\
\cite{muennighoff2024scaling} & 142 & Loss & N & Outliers removed \\
\cite{rae2021scaling} & 4 & Loss & N/A & N/A \\
\cite{shin2023scaling} & NA & Loss & NA & NA \\
\cite{hernandez2022scaling} & NS & Loss & N & N \\
\cite{filipovich2022scaling} & NS & Loss & N & N \\
\cite{neumann2022scaling} & 238 & Elo Score & N & N \\
\cite{droppo2021scaling} & NS & Loss & N & N \\
\cite{henighan2020scaling} & NS & Loss, Error Rate & NS & Drop smaller models \\
\cite{goyal2024scaling} & NS & Error Rate & N & N \\
\cite{aghajanyan2023scaling} & NS & Perplexity & N & N \\
\cite{kaplan2020scaling} & NS & Loss & NS & NS \\
\cite{ghorbani2021scaling} & NS & Loss, BLEU & Median of last 50k steps &  \\
\cite{gao2023scaling} & ~90 & RM Score & NS & NS \\
\cite{hilton2023scaling} & NS & Intrinsic Performance & Smoothing learning curve & Exclude early checkpoints \\
\cite{frantar2023scaling} & 48 and 112 & Loss & NS & NS \\
\cite{prato2021scaling} & 5 & Error Rate & NS & NS \\
\cite{covert2024scaling} & (1000-5000 )*10 & Expectation  & NS & N \\
\cite{hernandez2021scaling} & 40-120 & Loss & NS & NS \\
\cite{ivgi2022scaling} & 5-8 & Loss & N & [2.5, 97.5] percentile \\
\cite{tay2022scaling} & NA & Loss, Accuracy & NA & NA \\
\cite{tao2024scaling} & 20*60 & Loss & Interpolation & NS \\
\cite{jones2021scaling} & 2800 & Elo Score & NS & NS \\
\cite{zhai2022scaling} & NS & Accuracy & NS & NS \\
\cite{dettmers2023case} & NA & Accuracy & NA & NA \\
\cite{dubey2024llama} & ~150 & Loss, Accuracy & NS & NS \\
\cite{hoffmann2022training} & upto 1500 & Loss & N & Lowest loss model of a FLOP count, last 15\% of checkpoints \\
\cite{ardalani2022understanding} & ~130 & Loss & NS & NS \\
\cite{clark2022unified} & ~26*56 & Loss & Log & NS \\
\bottomrule
\end{tabular}

}

\caption{Details on data extraction for each paper surveyed.}
\label{tab:full-eval}
\end{table}




\begin{table}[]
\centering
\resizebox{\textwidth}{!}{%
% LaTeX code for Dataframe (25, 29):
\begin{tabular}{llllll}
\toprule
Paper & Curve-Fitting Method & Loss Objective & Hyperparameters Reported? & Initialization & Are Scaling Laws Validated? \\
\midrule
\cite{rosenfeld2019constructive} & Least Squares Regression & Custom error term & N/A & Random & Y \\
\cite{mikamiscaling} & Non-linear Least Squares in log-log space &  & N/A & N/A & Y \\
\cite{schaeffer2023emergent} & NA & NA & NA & NA & NA \\
\cite{sardana2023beyond} & L-BFGS & Huber Loss & Y & Grid Search & N \\
\cite{sorscher2022beyond} & NA & NA & NA & NA & NA \\
\cite{caballero2022broken} & Least Squares Regression & MSLE & N/A & Grid Search, optimize one & Y \\
\cite{besiroglu2024chinchilla} & L-BFGS & Huber Loss & Y & Grid Search & Y \\
\cite{gordon2021data} & Least Squares Regression &  & N/A & N.S. & N \\
\cite{bansal2022data} & NS & NS & N & NS & N \\
\cite{hestness2017deep} & NS & RMSE & N & NS & Y \\
\cite{bi2024deepseek} & NS & NS & N & NS & Y \\
\cite{bahri2021explaining} & NS & NS & N & NS & N \\
\cite{geiping2022much} & Non-linear Least Squares &  & NA & Non-augmented parameters & Y \\
\cite{poli2024mechanistic} & NS & NS & N & NS & N \\
\cite{hu2024minicpm} & scipy curvefit & NS & N & NS & N \\
\cite{hashimoto2021model} & Adagrad & Custom Loss & Y & Xavier & Y \\
\cite{ruan2024observational} & Linear Least Squares & Various & N/A & N/A & Y \\
\cite{anil2023palm} & Polynomial Regression (Quadratic) & N.S. & N & N.S. & Y \\
\cite{pearce2024reconciling} & Polynomial Least Squares & MSE on Log-loss & N/A & N/A & N \\
\cite{cherti2023reproducible} & Linear Least Squares & MSE & N/A & N/A & N \\
\cite{porian2024resolving} & Weighted Linear Regression & weighted SE on Log-loss & N/A & N/A & Y \\
\cite{alabdulmohsin2022revisiting} & Least Squares Regression & MSE & Y & N.S. & Y \\
\cite{gao2024scalingevaluatingsparseautoencoders} & N.S & N.S & N.S & N.S & N.S \\
\cite{muennighoff2024scaling} & L-BFGS & Huber on Log-loss & Y & Grid Search, optimize all & Y \\
\cite{rae2021scaling} & None & None & N/A & N/A & N \\
\cite{shin2023scaling} & NA & NA & NA & NA & NA \\
\cite{hernandez2022scaling} & NS & NS & NS & NS & NS \\
\cite{filipovich2022scaling} & NS & NS & NS & NS & NS \\
\cite{neumann2022scaling} & NS & NS & NS & NS & NS \\
\cite{droppo2021scaling} & NS & NS & NS & NS & NS \\
\cite{henighan2020scaling} & NS & NS & NS & NS & NS \\
\cite{goyal2024scaling} & Grid Search & L2 error & Y & NA & Y \\
\cite{aghajanyan2023scaling} & L-BFGS & Huber on Log-loss & Y & Grid Search, optimize all & Y \\
\cite{kaplan2020scaling} & NS & NS & NS & NS & N \\
\cite{ghorbani2021scaling} & Trust Region Reflective algorithm, Least Squares & Soft-L1 Loss & Y & Fixed & Y \\
\cite{gao2023scaling} & NS & NS & NS & NS & Y \\
\cite{hilton2023scaling} & CMA-ES+Linear Regression & L2 log loss & Y & Fixed & Y \\
\cite{frantar2023scaling} & BFGS & Huber on Log-loss & Y & N Random Trials & Y \\
\cite{prato2021scaling} & NS & NS & NS & NS & NS \\
\cite{covert2024scaling} & Adam & Custom Loss & Y & NS & Y \\
\cite{hernandez2021scaling} & NS & NS & NS & NS & Y \\
\cite{ivgi2022scaling} & Linear Least Squares in Log-Log space & MSE & NA & NS & Y \\
\cite{tay2022scaling} & NA & NA & NA & NA & NA \\
\cite{tao2024scaling} & L-BFGS, Least Squares & Huber on Log-loss & Y & N Random Trials from Grid & Y \\
\cite{jones2021scaling} & L-BFGS & NS & NS & NS & NS \\
\cite{zhai2022scaling} & NS & NS & NS & NS & NS \\
\cite{dettmers2023case} & NA & NA & NA & NA & NA \\
\cite{dubey2024llama} & NS & NS & NS & NS & Y \\
\cite{hoffmann2022training} & L-BFGS & Huber on Log-loss & Y & Grid Search, optimize all & Y \\
\cite{ardalani2022understanding} & NS & NS & NS & NS & NS \\
\cite{clark2022unified} & L-BFGS-B & L2 Loss & Y & Fixed & NS \\
\bottomrule
\end{tabular}

}

\caption{Details on optimization for each paper surveyed.}
\label{tab:full-opt}
\end{table}