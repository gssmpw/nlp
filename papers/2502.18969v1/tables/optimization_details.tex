% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table}[!h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{llllll}
\hline
Paper & Curve-fitting Method & Loss Objective & Hyperparameters & Initialization & Are scaling laws  \\
&  &  & Reported? &  &  validated? \\ \hline  \hline
\cite{rosenfeld2019constructive} & Least Squares Regression & Custom error term & N/A & Random & Y \\
\cite{mikamiscaling} & Non-linear Least Squares in log-log space &  & N/A & N/A & Y \\
\cite{schaeffer2023emergent} & NA & NA & NA & NA & NA \\
\cite{sardana2023beyond} & L-BFGS & Huber Loss & Y & Grid Search & N \\
\cite{sorscher2022beyond} & NA & NA & NA & NA & NA \\
\cite{caballero2022broken} & Least Squares Regression & MSLE & N/A & Grid Search, optimize one & Y \\
\cite{besiroglu2024chinchilla} & L-BFGS & Huber Loss & Y & Grid Search & Y \\
\cite{gordon2021data} & Least Squares Regression &  & N/A & N.S. & N \\
\cite{bansal2022data} & NS & NS & N & NS & N \\
\cite{hestness2017deep} & NS & RMSE & N & NS & Y \\
\cite{bi2024deepseek} & NS & NS & N & NS & Y \\
\cite{bahri2021explaining} & NS & NS & N & NS & N \\
\cite{geiping2022much} & Non-linear Least Squares &  & NA & Non-augmented parameters & Y \\
\cite{poli2024mechanistic} & NS & NS & N & NS & N \\
\cite{hu2024minicpm} & scipy curvefit & NS & N & NS & N \\
\cite{hashimoto2021model} & Adagrad & Custom Loss & Y & Xavier & Y \\
\cite{ruan2024observational} & Linear Least Squares & Various & N/A & N/A & Y \\
\cite{anil2023palm} & Polynomial Regression (Quadratic) & N.S. & N & N.S. & Y \\
\cite{pearce2024reconciling} & Polynomial Least Squares & MSE on Log-loss & N/A & N/A & N \\
\cite{cherti2023reproducible} & Linear Least Squares & MSE & N/A & N/A & N \\
\cite{porian2024resolving} & Weighted Linear Regression & weighted SE on Log-loss & N/A & N/A & Y \\
\cite{alabdulmohsin2022revisiting} & Least Squares Regression & MSE & Y & N.S. & Y \\
\cite{gao2024scalingevaluatingsparseautoencoders} & N.S & N.S & N.S & N.S & N.S \\
\cite{muennighoff2024scaling} & L-BFGS & Huber on Log-loss & Y & Grid Search, optimize all & Y \\
\cite{rae2021scaling} & None & None & N/A & N/A & N \\
\cite{shin2023scaling} & NA & NA & NA & NA & NA \\
\cite{hernandez2022scaling} & NS & NS & NS & NS & NS \\
\cite{filipovich2022scaling} & NS & NS & NS & NS & NS \\
\cite{neumann2022scaling} & NS & NS & NS & NS & NS \\
\cite{droppo2021scaling} & NS & NS & NS & NS & NS \\
\cite{henighan2020scaling} & NS & NS & NS & NS & NS \\
\cite{goyal2024scaling} & Grid Search & L2 error & Y & NA & Y \\
\cite{aghajanyan2023scaling} & L-BFGS & Huber on Log-loss & Y & Grid Search, optimize all & Y \\
\cite{kaplan2020scaling} & NS & NS & NS & NS & N \\
\cite{ghorbani2021scaling} & Trust Region Reflective algorithm, Least Squares & Soft-L1 Loss & Y & Fixed & Y \\
\cite{gao2023scaling} & NS & NS & NS & NS & Y \\
\cite{hilton2023scaling} & CMA-ES+Linear Regression & L2 log loss & Y & Fixed & Y \\
\cite{frantar2023scaling} & BFGS & Huber on Log-loss & Y & N Random Trials & Y \\
\cite{prato2021scaling} & NS & NS & NS & NS & NS \\
\cite{covert2024scaling} & Adam & Custom Loss & Y & NS & Y \\
\cite{hernandez2021scaling} & NS & NS & NS & NS & Y \\
\cite{ivgi2022scaling} & Linear Least Squares in Log-Log space & MSE & NA & NS & Y \\
\cite{tay2022scaling} & NA & NA & NA & NA & NA \\
\cite{tao2024scaling} & L-BFGS, Least Squares & Huber on Log-loss & Y & N Random Trials from Grid & Y \\
\cite{jones2021scaling} & L-BFGS & NS & NS & NS & NS \\
\cite{zhai2022scaling} & NS & NS & NS & NS & NS \\
\cite{dettmers2023case} & NA & NA & NA & NA & NA \\
\cite{dubey2024llama} & NS & NS & NS & NS & Y \\
\cite{hoffmann2022training} & L-BFGS & Huber on Log-loss & Y & Grid Search, optimize all & Y \\
\cite{ardalani2022understanding} & NS & NS & NS & NS & NS \\
\cite{clark2022unified} & L-BFGS-B & L2 Loss & Y & Fixed & NS \\ \hline
\end{tabular}%
}
\caption{We provide an overview of which papers provide specific details required to reproduce how they fit their scaling law equation.}
\label{tab:opt-details}
\end{table}