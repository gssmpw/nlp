\fbox{\begin{minipage}[!ht]{38em}

\subsubsection*{(Mis)Fitting: Scaling Law Reproducibility Checklist}


\small

\begin{minipage}[!htp]{0.95\textwidth}
\raggedright
\paragraph{Scaling Law Hypothesis (\S\ref{sec:power-law-form})}

\begin{itemize}[leftmargin=*]
    \item What is the form of the power law? \textit{$L(N, D) = E + \frac{A}{N^\alpha} + \frac{B}{D^\beta} $}
    \item What are the variables related by (included in) the power law? \textit{$N$: the number of model parameters, $D$: the number of data tokens, and $L$: the model's validation loss}
    \item What are the parameters to fit? \textit{$A$, $B$, $E$, $\alpha$, $\beta$}
    \item On what principles is this form derived? \textit{This is taken from \citet{hoffmann2022training}, who hypothesize this form on the basis of prior work in risk decomposition.}
    \item Does this form make assumptions about how the variables are related? \textit{This form inherently assumes that $N$ and $D$ do not have any interaction in their effect on the scaling of $L$. For some experiments, we use the assumption $\alpha = \beta$ to simplify optimization.}
    % \item How are each of these variables counted? (For example, how is compute cost/FLOPs counted, if applicable? How are parameters of the model counted?)
    % \item Are code/code snippets provided for calculating these variables if applicable? 
\end{itemize}


\paragraph{Training Setup (\S\ref{sec:model_training})}
\begin{itemize}[leftmargin=*]
    \item How many models are trained?
    \textit{Refer to Table 10}
    \item At which sizes?
    \textit{Refer to Table 10}
    \item On how much data each? On what data? Is any data repeated within the training for a model?
    \textit{Refer to Table 10.}
    \item How are model size, dataset size, and compute budget size counted? For example, how are parameters of the model counted? Are any parameters excluded (e.g., embedding layers)? How is compute cost calculated?  \textit{We include the results including and excluding embedding layers for both the total parameter count $N$ and the total FLOP count $C$. We also include, for comparison, results using the estimate $C=6ND$.}
    \item Are code/code snippets provided for calculating these variables if applicable?
    \url{https://github.com/hadasah/scaling_laws}
    \item How are hyperparameters chosen (e.g., optimizer, learning rate schedule, batch size)? Do they change with scale? \textit{Most hyperparameters are chosen based on best practices in current literature; several are taken directly from the settings in \citet{hoffmann2022training}. For learning rate, we conduct an extensive hyperparameter search across 2-3 orders of magnitude, multiplying by 2-2.5, and then conduct training at 3 learning rates, including the optimum, for nearly all ($N$, $D$) configurations.}
    \item What other settings must be decided (e.g., model width vs. depth)? Do they change with scale?  
    \textit{Refer to Table 10}
    \item Is the training code open source?
    \textit{Yes}
\end{itemize}

\input{tables/arch}

\raggedright

\end{minipage}

\end{minipage}}

\fbox{\begin{minipage}[!ht]{38em}

\small

\begin{minipage}[!ht]{0.9\textwidth}
\raggedright

\paragraph{Data Collection(\S\ref{sec:data})}
\begin{itemize}[leftmargin=*]
    \item Are the model checkpoints provided openly?
    % \item Are these checkpoints modified in any way before evaluation? (say, checkpoint averaging)
    % \item If the above is done, is code for modifying the checkpoints provided?
    \textit{Yes, at  \url{https://github.com/hadasah/scaling_laws}}
    \item How many checkpoints per model are evaluated to fit each scaling law? Which ones, if so?  \textit{Unless clearly denoted otherwise, one checkpoint per model is evaluated; the last checkpoint. By default, no mid-training checkpoints are used, i.e., from before the termination of the cosine learning rate schedules.}
    \item What evaluation metric is used? On what dataset?  \textit{We use cross-entropy loss, measured on a held-out validation subset of the Common Crawl \citep{raffel2020exploring} dataset.}
    \item Are the raw evaluation metrics modified? Some examples include loss interpolation, centering around a mean or scaling logarithmically.  \textit{No.}
    \item If the above is done, is code for modifying the metric provided? 
    \textit{Yes.}
\end{itemize}


\paragraph{Fitting Algorithm (\S\ref{sec:opt})}
\begin{itemize}[leftmargin=*]
    \item What objective (loss) is used? \textit{We try various loss objectives (1) log-Huber loss, (2) MSE, (3) MAE and (4) Huber loss}
    \item What algorithm is used to fit the equation? \textit{Mainly L-BFGS, but we also experiment with BFGS, non-linear least squares and grid search.}
    \item What hyperparameters are used for this algorithm? \textit{Thresholds of $\{1e-4, 1e-6, 1e-8\}$, exact gradient }
    \item How is this algorithm initialized? \textit{We initialize with 4500 initializations similar to \citet{hoffmann2022training}.}
    \item Are all datapoints collected used to fit the equations? For example, are any outliers dropped? Are portions of the datapoints used to fit different equations? \textit{No outliers are dropped in general, but we do show some results on specific subsets of models. For example, we compare the result of a scaling law fit when using only models trained at a peak learning rate of $1e-4$ or $4e-4$.}
    \item How is the correctness of the scaling law considered? Extrapolation, Confidence Intervals, Goodness of Fit? \textit{Currently, we do not evaluate the correctness beyond comparing to results in literature \citep{hoffmann2022training,kaplan2020scaling}.}
\end{itemize}

\end{minipage}

% \paragraph{Other}
% \begin{itemize}
%     \item Is code for 
% \end{itemize}

\end{minipage}}