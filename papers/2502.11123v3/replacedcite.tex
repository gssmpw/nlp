\section{Related Work}
\label{sec5}
\subsection{Real-Time Speech Interaction Models}
Real-time speech interaction models can be classified into non-duplex models and duplex models.

\paragraph{Non-Duplex Models}
There are two architectures: cascaded models and end-to-end models.

For cascaded models, HuggingGPT____ facilitates task decomposition of human instructions by LLMs and invokes models from Huggingface to perform specific tasks, including various ASR models. 
Audiogpt____ leverages multiple audio models to process complex audio information, linking the LLM with an input interface (ASR) for speech interactions.

For end-to-end models, SpeechGPT____ and AudioPaLM____ integrate speech tokens into the LLM’s vocabulary, continuing pretraining with both speech and text data. 
Qwen2-Audio____ and SALMONN____ involve adding a speech encoder before the LLM and conducting multi-stage training. 
LLaMA-Omni____ and Mini-Omni____ further incorporate speech adapters between the speech encoder and LLM. 
DiVA____ trains speech-based LLMs without instruction data by using a text LLM’s responses to transcribed text for self-supervised cross-modal distillation.

\paragraph{Duplex Models}
These models can process new user inputs while generating responses simultaneously____.

MiniCPM-duplex____ uses time-division multiplexing to process queries and responses in time slices for pseudo-simultaneous interaction.
LSLM____ detects real-time turn-taking by combining input and output tokens for autoregressive generation.
Moshi____ enables parallel processing by modeling both input and output speech streams concurrently.
SyncLLM____ processes tokens from both streams concurrently using an interleaved approach.
Freeze-Omni____ supports low-latency speech-to-speech interaction with a frozen backbone LLM to prevent catastrophic forgetting.
VITA____ alternates between two models for duplex interaction, using state tokens to distinguish effective from non-effective queries.

\subsection{Streaming Architectures}
Streaming architectures have linear complexity with input length and can generally be categorized into Linear RNN and Linear Attention models.

Linear RNN models like Mamba____ and Mamba-2____ optimize RNNs for specific hardware, enabling efficient training.
During inference, they process sequences step-by-step, maintaining a fixed-size context state, which ensures high memory efficiency and low latency for long-context tasks.

Linear Attention models, such as RWKV____ and RetNet____, eliminate certain nonlinear dependencies in the Attention mechanism, making them as efficient as RNNs during inference.