\section{Related Work}
\label{sec5}
\subsection{Real-Time Speech Interaction Models}
Real-time speech interaction models can be classified into non-duplex models and duplex models.

\paragraph{Non-Duplex Models}
There are two architectures: cascaded models and end-to-end models.

For cascaded models, HuggingGPT~\cite{shen2024hugginggpt} facilitates task decomposition of human instructions by LLMs and invokes models from Huggingface to perform specific tasks, including various ASR models. 
Audiogpt~\cite{huang2024audiogpt} leverages multiple audio models to process complex audio information, linking the LLM with an input interface (ASR) for speech interactions.

For end-to-end models, SpeechGPT~\cite{zhang2023speechgpt} and AudioPaLM~\cite{rubenstein2023audiopalm} integrate speech tokens into the LLM’s vocabulary, continuing pretraining with both speech and text data. 
Qwen2-Audio~\cite{chu2024qwen2} and SALMONN~\cite{tangsalmonn} involve adding a speech encoder before the LLM and conducting multi-stage training. 
LLaMA-Omni~\cite{fang2024llama} and Mini-Omni~\cite{xie2024mini} further incorporate speech adapters between the speech encoder and LLM. 
DiVA~\cite{held2024distilling} trains speech-based LLMs without instruction data by using a text LLM’s responses to transcribed text for self-supervised cross-modal distillation.

\paragraph{Duplex Models}
These models can process new user inputs while generating responses simultaneously~\cite{veluri2024beyond,xu2024enabling}.

MiniCPM-duplex~\cite{zhang2024beyond} uses time-division multiplexing to process queries and responses in time slices for pseudo-simultaneous interaction.
LSLM~\cite{ma2024language} detects real-time turn-taking by combining input and output tokens for autoregressive generation.
Moshi~\cite{defossez2024moshi} enables parallel processing by modeling both input and output speech streams concurrently.
SyncLLM~\cite{veluri2024beyond} processes tokens from both streams concurrently using an interleaved approach.
Freeze-Omni~\cite{wang2024freeze} supports low-latency speech-to-speech interaction with a frozen backbone LLM to prevent catastrophic forgetting.
VITA~\cite{fu2024vita} alternates between two models for duplex interaction, using state tokens to distinguish effective from non-effective queries.

\subsection{Streaming Architectures}
Streaming architectures have linear complexity with input length and can generally be categorized into Linear RNN and Linear Attention models.

Linear RNN models like Mamba~\cite{gu2023mamba} and Mamba-2~\cite{daotransformers} optimize RNNs for specific hardware, enabling efficient training.
During inference, they process sequences step-by-step, maintaining a fixed-size context state, which ensures high memory efficiency and low latency for long-context tasks.

Linear Attention models, such as RWKV~\cite{peng2023rwkv} and RetNet~\cite{sun2023retentive}, eliminate certain nonlinear dependencies in the Attention mechanism, making them as efficient as RNNs during inference.