\section{Related Work}\label{sec:related_work}

\noindent\textbf{Anomaly Detection.} Following~\cite{han2022adbench}, \gls{ad} methods can be mainly divided into 3 categories. First, supervised methods consider \gls{ad} through the lens of binary classification under class imbalance. Second, semi-supervised methods either consider partially labeled data, or \emph{labeled normal samples}, so that an algorithm can be characterize what a normal sample is. The third, more challenging category is unsupervised \gls{ad}, where the training data contains both anomalies and normal samples and labels are not available. This paper considers precisely the last setting. Next, we review ideas in unsupervised \gls{ad}.

The first kind of methods rely on encoder-decode architectures to detect anomalies. The insight is that, by embedding data in a lower dimensional space, anomalies can be detected via the reconstruction error of the auto-encoding function. This is the principle of \gls{pca}~\cite{shyu2003novel}, which employs linear encoding and decoding functions, but also of kernelized versions~\cite{scholkopf1997kernel,hoffmann2007kernel}, as well as neural nets~\cite{vincent2008extracting,bengio2013representation}, which rely on non-linear embedding techniques.

The second type of strategies are based on the paradigm of 1-class classification. As~\cite{scholkopf1999support} puts, the idea is to define a function that outputs $0$ on a small, dense region of the space where normal samples lie, and $1$ elsewhere. In this context,~\cite{scholkopf1997kernel} extends the celebrated \gls{svm} to \gls{ad}, and~\cite{liu2008isolation} extends \glspl{rf} of~\cite{breiman2001random}.

A third kind of approaches focuses on \emph{neighborhoods and clustering}, to model the data underlying probability distribution, especially through the density of samples over the space. This is the case of \gls{knn}~\citep{ramaswamy2000efficient}, who use distances and nearest neighbors to determine anomalies, \gls{lof}~\cite{breunig2000lof}, who devised a score that measures the local deviation of a sample with respect its neighbors. Finally, \gls{cblof}~\cite{he2003discovering} proposed an extension of \gls{lof} based on the relative sizes of clusters within the data.

A fourth kind of emerging ideas in the field considers generative modeling for detecting anomalies. In this context, one devises a model for the data density $P$, for instance, through neural nets~\cite{goodfellow2014generative,kingma2013auto}. The idea from these models, is that, either the data density can be accurately estimated~\citep{dias2020anomaly}, or some score can be derived from the generative model~\citep{livernoche2023diffusion}.

As we cover in the next section, our method uses nearest neighbors and \gls{ot} to model, non-parametrically, the density of samples over the space. More specifically, we prohibit samples in \gls{ot} to keep their mass, or sending it over a region of space defined through their \gls{knn}. Differently from~\cite{ramaswamy2000efficient} and~\cite{breunig2000lof}, we do not rely on distances, which might not have a meaning in high-dimensions. Rather, we rely on the effort of transportation, measured through the samples' mass times the ground-cost.

\noindent\textbf{Optimal Transport with Repulsive Costs.} In general \gls{ot} theory (see, e.g., Section~\ref{sec:ot} below), samples are transported based on a ground-cost that measures how expensive it is to move masses between measures. In its original conception by~\cite{monge1781memoire} and~\cite{kantorovich1942transfer}, this ground cost is the Euclidean distance $c(\mathbf{x}_{1}, \mathbf{x}_{2}) = \lVert \mathbf{x}_{1} - \mathbf{x}_{2} \rVert_{2}^{2}$. As reviewed in~\cite{di2017optimal}, it may be interesting to consider \emph{repulsive costs}, i.e. functions $c$ that are big when $\mathbf{x}_{1}$ and $\mathbf{x}_{2}$ are close to each other and small otherwise. An example of such costs, arising from physics, is the Coulomb interaction $c(\mathbf{x}_{1}, \mathbf{x}_{2}) = (\lVert \mathbf{x}_{1} - \mathbf{x}_{2}\rVert_{2})^{-1}$. Still following~\cite{di2017optimal}, these kinds of transportation problems proved useful in physics, e.g., for quantum mechanics and $N-$body systems.

In this paper, we consider a different kind of repulsive cost, which we call the mass repulsive cost (see, e.g., Section~\ref{sec:munot} below). Our notion of cost defines an exclusion zone where sending mass is too costly. This exclusion zone is defined on the nearest neighbor analysis of points in a probability measure. As a result, our approach captures the local characteristics of the probability measure being analyzed, especially its density. A special characteristic of our approach is to give a sense of the transportation from a measure to itself.

\noindent\textbf{Optimal Transport-based Anomaly Detection.} Previous works~\citep{alaoui2019unsupervised,alaoui2020semi} have considered \gls{ot} for \gls{ad}. These works proposed a distance-based detection mechanism, in which isolated samples are considered anomalies. \gls{ot} contributes to this setting, by defining a rich metric between samples. Especially, these works considered \gls{ad} in time series data, and \gls{ot} is used to compute distances between those in the frequency domain, under a Chebyshev ground cost. In comparison with these methods, ours is notably general purpose, that is, we do not assume data to be time series. Instead of using a Chebyshev ground cost, we model the \gls{ad} problem with a repulsive cost.