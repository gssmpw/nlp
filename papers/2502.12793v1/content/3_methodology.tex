\section{Proposed Method}\label{sec:methodology}

\subsection{Optimal Transport}\label{sec:ot}

\gls{ot} is a field of mathematics, concerned with the displacement of mass between a source measure, and a target measure, at least effort. In the following, we cover the principles of \gls{ot} in continuous and discrete settings. We refer readers to~\cite{peyre2020computationaloptimaltransport} for a computational exposition of the main concepts, and~\cite{montesuma2024recentadvancesoptimaltransport} for applications in machine learning. In the following, we are particularly interested in the formulation by~\cite{kantorovich1942transfer}, which is defined as,
\begin{definition}{(Kantorovich Formulation)}\label{def:Kantorovich_continuous}
Let $P$ and $Q$ be 2 probability measures over a set $\mathcal{X}$. Let $c:\mathcal{X}\times\mathcal{X}\rightarrow\mathbb{R}$ be a \emph{ground-cost}, measuring the effort of transporting units of mass from $x$ to $y$. Let $\Gamma(P, Q) = \{\gamma \in \mathbb{P}(\mathcal{X}\times\mathcal{X}): \int_{\mathcal{X}}\gamma(x,B)dx = Q(B) \text{ and }\int_{\mathcal{X}}\gamma(A,y)dy = P(A)\}$ be the set of \emph{transportation plans}, whose marginals are $P$ and $Q$. The optimal transportation problem is written as,
\begin{align}
    \gamma^{\star} = \text{OT}(P,Q) = \arginf{\gamma \in \Gamma(P, Q)}\int_{\mathcal{X}\times\mathcal{X}}c(x,y)d\gamma(x,y).\label{eq:kantorovich_continuous}
\end{align}
\end{definition}

Equation~\ref{eq:kantorovich_continuous} defines the transportation problem as an infinite dimensional linear program on the variable $\gamma$, called transport plan. In our case, instead of having access to a closed-form $P$, one has samples $\{\mathbf{x}_{i}^{(P)}\}_{i=1}^{n}$, each $\mathbf{x}_{i}^{(P)} \sim P$ with probability $p_{i}$. In such cases, $P$ may be approximated with an empirical measure,
\begin{align}
    \hat{P}(\mathbf{x}) = \sum_{i=1}^{n}p_{i}\delta(\mathbf{x} - \mathbf{x}_{i}^{(P)}).\label{eq:emp_approx}
\end{align}
Since the $p_{i}'s$ represent the probability of sampling $\mathbf{x}_{i}^{(P)}$, $\sum_{i}p_{i} = 1$ and $p_{i} \geq 0$. Plugging back equation~\ref{eq:emp_approx} into equation~\ref{eq:kantorovich_continuous} leads to a finite linear program,
\begin{align}
    \hat{\gamma} = \argmin{\gamma \in \Gamma(\mathbf{p},\mathbf{q})}\langle \gamma, \mathbf{C} \rangle_{F} = \sum_{i=1}^{n}\sum_{j=1}^{m}\gamma_{ij}\underbrace{c(\mathbf{x}_{i}^{(P)}, \mathbf{x}_{j}^{(Q)})}_{C_{ij}},\label{eq:kantorovich_discrete}
\end{align}
where the optimization variables are the coefficients $\gamma_{ij}$ of the transport plan, and $\gamma \in \Gamma(\mathbf{p}, \mathbf{q})$ indicates that $\sum_{i}\gamma_{ij} = q_{j}$ and $\sum_{j}\gamma_{ij} = p_{i}$. As a linear program, the solution $\gamma$ is a sparse matrix with at most $n + m - 1$ non-zero elements~\citep{peyre2020computationaloptimaltransport}. Solving it through the celebrated Simplex algorithm~\citep{dantzig1983reminiscences}, which has computational complexity $\mathcal{O}(n^{3}\log n)$ and storage complexity $\mathcal{O}(n^{2})$ (i.e., storing each $\gamma_{ij}$).

A faster alternative was introduced by~\cite{cuturi2013sinkhorn}, who shown that adding an entropic regularization to equation~\ref{eq:kantorovich_discrete} leads to a problem that can be solved through Sinkhorn's algorithm~\cite{sinkhorn1967diagonal}. From a continuous perspective, this is equivalent to penalizing \gls{kl} divergence between $\gamma$, and the trivial coupling $P \otimes Q = P(x)Q(y)$. This regularization term is related to the entropy of $\gamma$ as discussed in~\cite[Chapter 4]{peyre2020computationaloptimaltransport}, hence this problem is called entropic \gls{ot}.
\begin{definition}{(Entropic Optimal Transport)}\label{def:entropic_ot}
    Under the same conditions of Definition~\ref{def:Kantorovich_continuous}, let $\epsilon \geq 0$ be an entropic penalty. The entropic \gls{ot} problem is given by,
    \begin{align}
        \gamma^{\star}_{\epsilon} = OT_{\epsilon}(P, Q) = \arginf{\gamma \in \Gamma(P, Q)} \int_{\mathcal{X}\times\mathcal{X}}c(x,y)d\gamma(x,y) + \epsilon \text{KL}(\gamma|P\otimes Q),\label{eq:eot}
    \end{align}
    where $\text{KL}(\gamma|\xi) = \int_{\mathcal{X} \times \mathcal{X}}\log\biggr(\dfrac{d\gamma}{d\xi}(x,y)\biggr)d\gamma(x,y) + \int_{\mathcal{X}\times\mathcal{X}}(d\xi(x,y) - d\gamma(x,y))$ is the \gls{kl} divergence between measures $\gamma$ and $\xi$.
\end{definition}

We can obtain an equivalent discrete formulation by plugging back equation~\ref{eq:emp_approx} into~\ref{eq:eot}, which leads to,
\begin{align}
    \hat{\gamma}_{\epsilon} = \argmin{\gamma \in \Gamma(\mathbf{p},\mathbf{q})}\langle \gamma, \mathbf{C} \rangle_{F} - \epsilon H(\gamma) = \sum_{i=1}^{n}\sum_{j=1}^{m}\gamma_{ij}C_{ij} + \epsilon\sum_{i=1}^{n}\sum_{j=1}^{m}\gamma_{ij}(\log \gamma_{ij} - 1),\label{eq:eot_discrete}
\end{align}
where $H(\gamma)$ denotes the entropy of the transportation plan. Since equation~\ref{eq:eot_discrete} relies on the~\cite{sinkhorn1967diagonal} algorithm rather than linear programming, it has $\mathcal{O}(Ln^{2})$ computational complexity, where $L$ is the number of iterations. In general, the \gls{kl} and entropic terms in equations~\ref{eq:eot} and~\ref{eq:eot_discrete} has a smoothing effect over the joint measure $\gamma_{\epsilon}$. As a result, $\hat{\gamma}_{\epsilon}$ has more non-zero elements than $\hat{\gamma}$.

In the next section, we explore a new transportation with a single probability measure. This problem is understood as the transportation of $P$ to itself, when the samples form $P$ are forced to send their mass outside their immediate neighborhood.

\subsection{Mass Repulsing Optimal Transport}\label{sec:munot}

In this section, we propose a new \gls{ot} problem called \gls{munot}. This problem is inspired by the Kantorovich formulation, described in section~\ref{sec:ot}. However, instead of considering two different probability measures $P, Q$, it considers the transportation of $P$ to itself. Due the properties of \gls{ot}, if we consider the \gls{ot} plan $\gamma^{\star} = \ot(P, P)$, it is supported on the set $\{(x, x), x \in \mathcal{X}\}$, i.e., each point keeps its own mass~\citep{santambrogio2015optimal}. This motivates our new problem, in which we force points to \emph{repell} its mass. We do so, by engineering the ground-cost $c:\mathcal{X}\times\mathcal{X} \rightarrow \mathbb{R}_{+}$. For pairs $x \in \mathcal{X}$, $y \in \mathcal{X}$,
\begin{align}
    \tilde{c}_{k}(x, y) = \begin{cases}
        c(x, y) & \text{ if }y \not\in \mathcal{N}_{k}(x),\\
        \sup{z \in \mathcal{N}_{k}(x)}c(x, z) & \text{ otherwise}.
    \end{cases}\label{eq:engineered_cost}
\end{align}
this principle is different from repulsive costs~\citep{di2017optimal}, which are designed to model the interaction between particles in multi-marginal \gls{ot}. Indeed, in our work we are designing a transportation problem from a probability measure to itself. While repulsive costs incentive transportation towards distant points in space, our mass repulsing cost induces transportation just outside an exclusion zone.

While the mass repulsing principle can be applied to any ground-cost $c$, let us focus on the metric case, i.e., when $c(x, y) = d(x, y)^{2}$, where $(\mathcal{X}, d)$ is a metric space. The ground-cost we are proposing is essentially defining an \emph{exclusion zone}, i.e., the $k-$vicinity $\mathcal{N}_{k}(x)$ centered at $x$, where $x$ cannot send its mass. Of course, since $x \in \mathcal{N}_{k}(x)$, it is forced to send its mass elsewhere -- thus, we call this idea \emph{mass repulsive \gls{ot}}.

Our main hypothesis for anomaly detection is that anomalous points lie in low density regions of $P$. On the one hand, If these points are forced to send its mass outside its vicinity, it will be forced to send it to high density regions of $P$ -- otherwise, mass conservation in \gls{ot} would not hold. On the other hand, \emph{if the exclusion zone is smaller than high density regions of $P$}, points on these regions will be sent close-by. As a result, anomalous points will have a higher transportation cost, given by $\gamma(x, y)\tilde{c}_{k}(x, y)$, than normal points. We thus consider the following \gls{ot} problem,
\begin{align}
    \gamma^{\star}_{k,\epsilon} &= \text{MROT}_{k,\epsilon}(P) = \inf{\gamma \in \Gamma(P, P)}\int\tilde{c}_{k}(x,y)d\gamma(x,y)+\epsilon\text{KL}(\gamma|P\otimes P),\label{eq:mrot_continuous}
\end{align}
which, like the continuous entropic \gls{ot} problem in definition~\ref{def:entropic_ot}, also admits a discrete version when $P$ is approximated empirically through equation~\ref{eq:emp_approx},
\begin{align}
    \gamma^{\star}_{k,\epsilon} &= \text{MROT}_{k,\epsilon}(\hat{P}) = \min{\gamma \in \Gamma(\mathbf{p},\mathbf{p})}\langle \gamma, \tilde{\mathbf{C}_{k}}\rangle_{F} - \epsilon H(\gamma).\label{eq:discrete_munot}
\end{align}

To ground-up intuition about our proposed problem, we show in Figure~\ref{fig:comparison-costs} a comparison between the commonly used squared Euclidean cost, our engineered cost, and a repulsive cost associated with the Coulomb interaction~\citep{di2017optimal}. In traditional \gls{ot}, sending mass to far away regions of space is costly. As a result, points are encouraged to keep their own mass as close as possible. Consequently, \gls{ot} from $P$ to itself is the trivial plan $\gamma = \text{Id}$.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{Figures/ComparisonCosts.pdf}
    \caption{Comparison between different ground costs for $x_0 = (0, 0)$. From left to right: Squared euclidean cost $c(x, x_0) = \lVert x - x_0 \rVert_{2}^{2}$, Engineered cost (c.f., equation~\ref{eq:engineered_cost}) and repulsive cost.}
    \label{fig:comparison-costs}
\end{figure}

Both our engineered cost $\tilde{c}_{k}$ and the repulsive cost $(1 - c(x, x_0))^{-1}$ assign high costs to points near $x_0$. In particular, within the nearest-neighbor radius $\mathcal{N}{k}(x_0)$, our engineered cost $\tilde{c}{k}(x,x_0)$ reaches its maximum. However, there is an important difference between these costs. Our engineered cost promotes transportation \emph{to the immediate region outside $\mathcal{N}_{k}$}, whereas the repulsive cost drives samples as far as possible from $x_0$. As we discussed previously, this feature of the ground-cost allows us to build an anomaly score.

\subsection{Building and Generalizing an Anomaly Score}\label{sec:building-anomaly-score}

Through \gls{munot}, we want to build a score for samples, based on how anomalous they are. Assuming some samples lie in a low density region of space, our \gls{munot} problem forces those samples to send their mass to distant parts of the feature space. In contrast, normal samples can send their mass to the immediate neighborhood outside the exclusion zone. As a result, we can sort out normal from anomalous samples using the transportation effort,
\begin{align*}
    \mathcal{T}(x) = \expectation{y \sim \gamma(\cdot|x)}[\tilde{c}_{k}(x,y)] = \int_{\mathcal{X}}\tilde{c}_{k}(x, y)d\gamma(y|x),
\end{align*}
where $\gamma(y|x)$ corresponds to the conditional probability, calculated through the joint $\gamma(x, y)$, given $x$. For empirical measures, this quantity can be calculated as follows,
\begin{align}
    \mathcal{T}_{i} = \sum_{j=1}^{n}\dfrac{\gamma_{ij}^{\star}}{p_{i}}\tilde{c}_{k}(x_{i}^{(P)}, x_{j}^{(P)}),\label{eq:score}
\end{align}
where $p_{i}$ is the importance of the $i-$th sample (e.g, $p_{i} = n^{-1}$ for uniform importance). Interestingly, equation~\ref{eq:score} is similar to the barycentric map, widely used in domain adaptation~\cite{courty2016optimal, montesuma2023multi}. $\mathcal{T}$ has 2 shortcomings as an anomaly score. First, it is hardly interpretable, as its range depends on the choice of ground-cost $c$. Second, it is only defined in the support of $\hat{P}$. We offer a solution to both of these problems.

Concerning interpretability, we propose to transform it using the \gls{cdf} of its values. Let $P_{\mathcal{T}}$ be the probability measure associated with $\mathcal{T}(x)$. The \gls{cdf} is simply $F_{\mathcal{T}}(t) = P_{\mathcal{T}}((-\infty, t))$. Naturally, since $P_{\mathcal{T}}$ is not available, it may be approximated from samples $\{\mathcal{T}_{i}\}_{i=1}^{n}$, obtained through equation~\ref{eq:score}. We do so through, \gls{kde},
\begin{align}
    \hat{k}(t) &= \dfrac{1}{n\sigma}\sum_{i=1}^{n}\phi\biggr{(}\dfrac{t-\mathcal{T}_{i}}{\sigma}\biggr{)},\label{eq:kde}
\end{align}
where $K$ is a kernel function (e.g., the Gaussian kernel $\phi(x) = \exp(-\nicefrac{x^{2}}{2})$), and $\sigma$ is the bandwidth parameter, controlling the smoothness of $\hat{k}$ and determined through Scott's rule~\citep{scott1979optimal}. Equation~\ref{eq:kde} provide an approximation for the density of transportation efforts $\{\mathcal{T}_{i}\}_{i=1}^{n}$. The \gls{cdf} can be easily calculated from this estimate, via,
\begin{align}
    \hat{K}(t)=\int_{-\infty}^{t}\hat{k}(t)dt.\label{eq:cdf}
\end{align}
The \gls{cdf} is appealing, as it is a monotonic function over transportation efforts $t \in \mathbb{R}$, and it takes values on $[0, 1]$, both of which are desirable properties for an anomaly score.

The issue on how to \emph{extrapolate} the anomaly score for new samples remain. For instance, even if we use the \gls{cdf} values as anomaly scores, one needs to recalculate $t = \hat{\mathcal{T}}(\mathbf{x})$ for a new sample, before evaluating $\hat{H}(t)$ or $\hat{K}(t)$. As we previously discussed, this is challenging, as $\hat{\mathcal{T}}$ is only defined on the support of $\hat{P}$. A naive approach would be to append $\mathbf{x}$ to the set $\{\mathbf{x}_{i}^{(P)}\}_{i=1}^{n}$ and solve a \gls{munot} problem again. Naturally, this is not feasible, as solving an \gls{ot} problem for each new sample is computationally expensive.

\begin{minipage}[t]{0.48\textwidth}
In this paper we put forth the more efficient idea of modeling the relationship $\mathbf{x} \mapsto \hat{K}(\hat{\mathcal{T}}(\mathbf{x}))$ from the samples we are given, that is, we create a labeled dataset $\{\mathbf{x}_{i}^{(P)}, t_{i}\}_{i=1}^{n}$, where $t_{i} = \hat{\mathcal{T}}(\mathbf{x}_{i}^{(P)})$. Our anomaly score comes, then, through a function $\psi(\mathbf{x}_{i}^{(P)})$ fit to the labeled dataset through regression. Fitting $\psi$ can be done with standard regression tools, such as \gls{ols}, \gls{svr}~\citep{smola2004tutorial}, nearest neighbors, or gradient boosting~\citep{friedman2002stochastic}. In general, one can expect the relationship between $\mathbf{x}_{i}^{(P)}$ and $\hat{K}(\hat{\mathcal{T}}(\mathbf{x}_{i}^{(P)}))$ to be non-linear, hence, it is generally necessary to use a non-linear regression model. We show a summary of our strategy in Algorithm~\ref{alg:mrot}.
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}
\begin{algorithm}[H]
  \caption{Mass Repulsive Optimal Transport.}
  \label{alg:mrot}
  \Function{\textcolor{black}{mrot($\mathbf{X}^{(P_{S})}, \epsilon,k$)}}{
    $\gamma_{k,\epsilon}^{\star} = \text{MROT}_{k,\epsilon}(\hat{P})$\;
    $t_{i} \leftarrow \sum_{j=1}^{n}(\nicefrac{(\gamma_{k,\epsilon}^{\star})_{ij}}{p_{i}})\tilde{c}_{k}(x_{i}^{(P)},x_{j}^{(P)})$\;
    $\hat{k} \leftarrow \text{KDE}(\{t_{i}\}_{i=1}^{n})$\;
    $\psi \leftarrow \text{Regression}(\{x_{i}^{(P)}, \hat{K}(t_{i})\}_{i=1}^{n})$\;
    \Return{$\psi$}\;
  }
\end{algorithm}
\end{minipage}
