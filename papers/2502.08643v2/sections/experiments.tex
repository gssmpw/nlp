

\section{Experiments and Analysis}
We aim to investigate whether \algname can effectively represent reward functions for diverse manipulation skills within our \algabrvname for real-to-sim-to-real pipeline. We also want to see whether our pipeline can  perform multi-step tasks in dynamic environments by leveraging \algname as feedback for replanning.


\subsection{Experimental Setup, Metrics and Baselines}
\begin{figure}[h]
    \centering
\includegraphics[width=0.75\linewidth]{figures/setup.pdf}
    \caption{\small{\textbf{Setup and experiment objects.} We use XArm7 to conduct all our experiments. Our setup includes 4 stationary and 1 wrist-mounted camera. We experiment with 5 shoe pairs and 2 shoe racks for tasks involving shoe scenarios. Additionally, we experiment with 9 different books for stowing tasks.}}
    \label{fig:objects}
    \vspace{-0.8em}
\end{figure}

We conduct experiments on XArm7 with four stationary RealSense cameras. \figref{fig:objects} shows the setup, along with the objects used. These cameras capture the point clouds, which are used to construct the simulation environment and to provide data for AnyGrasp to predict grasp. Additionally, a wrist-mounted camera is used to capture images that are used to query the VLM. 

As a baseline, we use an \textit{annotated} variant of \algabrvname with human-labeled reward functions, allowing evaluation without VLM influence. We also compare our keypoint-based method with another baseline that uses object pose to construct reward function, which is more conventional in RL training~\cite{gu2017deep,popov2017data,vecerik2017leveraging,rajeswaran2017learning,qi2023hand}. In this pose-based method, the VLM generates a function $f$ that maps the initial object poses (represented by xyz coordinates for position and RPY angles for orientation) to their final poses. The prompt for this baseline is discussed in Sec.~\ref{sec:prompt}.

We evaluate our approach across four scenarios, illustrated in \figref{fig:teaser} (left): Shoe Place, Shoe Push, Book Push, and Book Reorient. In Shoe Place, the robot picks up a shoe from the ground and places it on a rack. In Shoe Push, it pushes a shoe towards other shoe to form a matching pair. In Book Push, it pushes a book to align with other book, or push the book towards table edge, and in Book Reorient, it repositions a book on a shelf. Each scenario has 10 start/end configurations. In simulation, success rates are averaged over 128 randomized environments generated for 10 start/end configuration, making a total of 1280 trials per scenario. In the real-world, success is evaluated directly on the 10 start/end configurations. A trial is considered successful in both cases if the average keypoint distance to the target is within 5 cm.



\subsection{Policy Training with \algabrvname for Single-Step Tasks}
We conduct experiments comparing RL training with keypoints and object pose in reward functions. Our experiments span four representative tasks, and are summarized in Table~\ref{tab:main_results}.

In the \textit{annotated} method, success rates for shoe placement using \algabrvname and object pose are 0.945 and 0.938, respectively. A similar trend is observed in the shoe push, stowing push, and reorient tasks, where performance differences are minimal. These results demonstrate that, when targets are specified through human annotations, both keypoints and object poses effectively capture the target locations and serve as viable approaches for RL policy training.

In the \textit{automatic} method, \algabrvname significantly outperforms object pose representations. For example, in shoe placement, \algabrvname achieves a 0.7 success rate, while object poses reach only 0.3. Similar results are seen across other tasks. Object pose success is limited to simpler scenarios with no orientation changes, as VLMs struggle with rotations in SO(3) space. In contrast, keypoints simplify the challenge by requiring VLMs to reason only in Cartesian space, eliminating the need to handle object poses in SE(3) space.



As shown in Table~\ref{tab:main_results}, there is a slight reduction in success rate from simulation to the real world. For shoe placement, \algabrvname achieves success of 0.945 in simulation and 0.8 in the real world. For shoe push, the success rate drops from 0.871 to 0.850. These results suggest that domain randomization described in Section~\ref{sec:training} helps the model generalize to real-world conditions, but factors like inaccuracies in environment reconstruction, real-world perception errors, and the inability to simulate extreme object dynamics still affect performance.



Most of the failures in our framework stem from discrepancies between the heuristic grasps used in simulation and the grasps generated by AnyGrasp in the real world, as well as incorrect VLM predictions. For incorrect VLM predictions, the model sometimes selects the wrong keypoints or fails to use all available keypoints on an object when determining its relationship to another object. For instance, if an object has four keypoints, the VLM may only use one of them, leading to suboptimal alignment and placement. These issues can be mitigated by providing more in-context examples while querying the VLMs.
These challenges may become less pronounced with the incorporation of advancements such as~\cite{chen2024spatialvlm}, which enhance the spatial reasoning capabilities of VLMs. Additionally, some failures are caused by physical dynamics when pushing objects. These issues can be partially mitigated by explicitly estimating dynamic parameters during real-to-sim transfer.



\subsection{Iterative Replanning for Multi-Step Tasks}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/extensions.pdf}
    \caption{\small{\textbf{Scenarios demonstrating capabilities of our framework.} The framework is robust to disturbances and can adapt in response to unexpected events. Additionally, it can propose new plans when the original ones become infeasible.}}
    \label{fig:extensions}
    \vspace{-1.6em}
\end{figure*}


\begin{figure}[h]
    \setlength{\abovecaptionskip}{0pt}
    \includegraphics[width=\linewidth]{figures/chaining.png}
    \caption{\small{\textbf{Multi-Step Task Chaining Comparison with VoxPoser.} Our proposed framework consistently demonstrates superior performance compared to VoxPoser at every step of the task sequence.}}
    \label{fig:chaining_analysis}
    \vspace{-2.1em}
\end{figure}





We demonstrate the robot's iterative chaining ability with a task of three sequential actions: first pushing a shoe box to create space, then placing a pair of shoes on a rack. Failure in one task leads to failure in the next. We evaluate this process using 10 different start and end configurations, iterating through each to assess overall performance.

We compare our method with VoxPoser~\cite{huang2023voxposer}, which employs LLMs to generate code that produces potential fields for motion planning. VoxPoser serves as an ideal baseline because it synthesizes motion plans for diverse manipulation tasks from free-form language instructions. Notably, VoxPoser plans are open-loop and lack feedback to refine specifications at each step. To adapt it to our tasks, we enhanced VoxPoser with two major modifications: (1) VoxPoser used OWL-ViT~\cite{minderer2022simple} to find object bounding boxes, but it struggled to distinguish between left and right shoes, so we provided ground-truth object locations. (2) We gave VoxPoser the entire plan, as the original planner struggled with multi-step tasks. This gave VoxPoser an advantage over our method due to access to privileged information.

\figref{fig:chaining_analysis} shows the iterative chaining results. Across the three tasks, our method consistently outperformed VoxPoser. In the first task, we succeeded 8 out of 10 times compared to VoxPoser's 5 successes. For the second task, we had 5 successes while VoxPoser had 1. In the final task, our method succeeded 4 times, whereas VoxPoser failed in all attempts. VoxPoser's failures can be attributed to several factors, such as pushing the shoe box either too far or not far enough. Additionally, its grasping strategy relies on a simple heuristic that positions the robot's end effector around the object center before closing the gripper, often resulting in failed grasps. It also struggles with collisions during object manipulation, as it does not account for the environment to avoid obstacles. Furthermore, improper placement of shoes—such as stacking both shoes on top of each other, causing them to fall—further highlights its limitations.





\subsection{Robustness, Adjusting Plans, and Re-Planning}
Unlike previous works that rely on open-loop plans, our approach leverages closed-loop plans, enabling adjustments during execution. This feature gives rise to several capabilities, as demonstrated in \figref{fig:extensions}.

In the first scenario, a human interrupts the robot while it is in the process of placing shoes on the ground. The framework demonstrates resilience by recovering from the interruption. The robot re-grasps the shoe and successfully completes the task by placing both shoes on the rack.

In the second scenario, when the robot attempts to place the left shoe, it detects that the shoe is not positioned close enough to the right shoe. To address this, the VLM predicts a corrective action, suggesting that the robot push the left shoe closer to the right shoe to form a proper pair.

In the third scenario, the robot is tasked with stowing a book on a shelf. However, the initial grasp attempt fails because the book is too large to be grasped. In response, the VLM predicts an alternative strategy to complete the task, adjusting the approach to ensure successful placement.

\subsection{Effect of Domain Randomization} \label{sec:dr}
\begin{table}[ht]
    \centering
    \begin{tabular}{lcccc}
        \toprule
        \multirow{2}{*}{\textbf{Task}} & \multicolumn{2}{c}{\textbf{Without DR}} & \multicolumn{2}{c}{\textbf{With DR}} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5}
         & \textbf{Simulation} & \textbf{Real-World} & \textbf{Simulation} & \textbf{Real-World} \\
        \midrule
        \textbf{Place} & 0.964 & 0.5 & 0.945 & 0.8 \\
        \textbf{Push} & 0.923 & 0.2 & 0.871 & 0.7 \\
        \bottomrule
    \end{tabular}
    \caption{\small{\textbf{Performance of Shoe Place and Push with and without Domain Randomization (DR).} DR slightly reduces simulation performance but significantly improves real-world task performance across different scenarios.}}
    \label{tab:domain_randomization_results}
    \vspace{-2.5em}
\end{table}

We present the results of our framework with and without DR for shoe place and push. The performance is averaged over 10 runs. In the simulation, the performance without DR is 0.964, while with DR, it is slightly lower at 0.945, suggesting that without DR, the policy performs better in a single, controlled setting. However, in the real world, the performance without DR drops to 0.6, whereas with DR, it is 0.8, highlighting the effectiveness of DR. For the place task without DR, we observe that the policy is less robust to sim-to-real gap, frequently colliding with the shoe rack during transport. Additionally, immediately after picking up the object, the policy sometimes fails, likely due to discrepancies in the pose estimation.

For push, these issues are more pronounced: success is 0.2 without DR but improves to 0.7 with DR. Without DR, the policy often crushes the shoe or causes it to slip out of alignment during pushing. These findings demonstrate the importance of DR for reliable real-world performance.

