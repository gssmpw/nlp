\section{Method}

\begin{algorithm}[t]
\scriptsize
\begin{algorithmic}[1]
    \raggedright
    \State \textbf{Given: } Language instruction $I$
    \State \textbf{Initialize:} \texttt{done} $\gets$ \texttt{false}, \texttt{execution\_history} $\gets$ [\ ], $i \gets 1$
    \State Generate 3D models of objects
    \While{\texttt{true}}
        \State $(\{k_{j}^{(i)}\}_{j=1}^{K_i}, O_i) \gets$ \texttt{GetKeypoints}(3D models)
        \State \texttt{code} $\gets$ \texttt{QueryVLM}($O_i$, \texttt{execution\_history})
        \State $(\texttt{done}, \{k_{j}^{\text{target}(i)}\}_{j=1}^{K_i}) \gets$ \texttt{Execute}(\texttt{code})
        \If{\texttt{done} is \texttt{true}}
            \State \texttt{break}
        \EndIf
        \State $s_i \gets$ \texttt{TransferSceneToSimulation}(3D models)
        \State $\pi_i \gets$ \texttt{LearnPolicy}($s_i$, $\{k_{j}^{\text{target}(i)}\}_{j=1}^{K_i}$)
        \State \texttt{ExecutePolicyInRealWorld}($\pi_i$)
        \State Append ($O_i$, \texttt{code}) to \texttt{execution\_history}
        \State $i \gets i + 1$
    \EndWhile
\end{algorithmic}
\caption{IKER Execution Framework}
\label{alg:vlm_execution}
\end{algorithm}


In this section we formally define \algname{} (\algabrvname{}) and discuss how it is automatically synthesized and refined by VLMs by continuously taking in environmental feedback. Then, we discuss our overall framework, which uses \algabrvname in a real-to-sim-to-real loop. Our method overview is illustrated in \figref{fig:overview}, with detailed steps provided in Algorithm ~\ref{alg:vlm_execution}.

\subsection{\algname~(\algabrvname)}

\begin{figure}[t]
    \setlength{\abovecaptionskip}{0pt}
    \includegraphics[width=\linewidth]{figures/iker.pdf}
    \caption{\small{\textbf{\algname Generation.} This corresponds to the first step in \figref{fig:overview}. We first obtain keypoints in the scene. These keypoints, combined with a human command and execution history, are processed by a VLM to generate code that maps keypoints to the reward function.  A more detailed illustration of the keypoints and generated code is provided in \figref{fig:unrolled}.}}
    \label{fig:method}
    \vspace{-1.5em}
\end{figure}

Given an RGB-D observation of the environment and an open-vocabulary language instruction $I$ for a multi-step task, our goal is to obtain a sequence of policies, $\pi_{i=1}^{N}$, that complete the task. Crucially, the number of policies $N$ is not predefined, allowing for flexibility in how the robot approaches the task. For example, in the scenario of Fig.~\ref{fig:method}, the first policy, $\pi_1$, moves the shoe box to create space, while subsequent policies handle the placement of each shoe.

For each step $i$, we denote the RGB observation as $O_i$. We assume a set of $K_i$ keypoints $\{k_j^{(i)}\}_{j=1}^{K_i}$ is given (discussed later in Sec.~\ref{sec:real2sim}), each specifying a 3D position in the task space. Using these keypoints, our objective is to automatically generate a reward function, termed \algabrvname, that maps the keypoint positions to a scalar reward $f^{(i)}: \mathbb{R}^{K_i \times 3} \rightarrow \mathbb{R}$.

To generate the reward function $f^{(i)}$, we use a VLM (GPT-4o~\cite{openai2023gpt} in our case), which is provided with the context comprising (1) the human instruction $I$ describing the task, (2) the current RGB observation $O_i$ with keypoints overlaid with numerical markers, and (3) the sequence of previous observations and reward functions up to step $i - 1$, i.e. $\{O_1, f^{(1)}, \dots, O_{i-1}, f^{(i-1)}\}$. 
 
Additionally, the VLM is guided by a prompt that instructs it to generate a Python function for the reward $f^{(i)}$. The prompt directs the VLM to break down the task into executable steps, predict which object to interact with, specify the movement of objects by indicating where their keypoints should be placed relative to other keypoints, and perform arithmetic calculations on these keypoints to predict their final locations. We do not explicitly specify which keypoint belongs to which object, allowing the VLM to infer this information. The prompt also instructs the VLM to present all outputs in a prescribed code format and set the flag \texttt{done = True} if the task is completed. By predicting code, the VLM can perform arbitrary and precise calculations using the current keypoint locations, which would not be possible if limited to raw text. Please refer to Appendix~\ref{sec:prompt} for the complete prompt, and \figref{fig:unrolled} for a step-by-step walkthrough of RGB observations and generated reward functions for the example of \figref{fig:method}.

Upon receiving the final keypoint locations by executing the generated code, we compute a scalar reward to evaluate the policy's performance. The reward function, $f^{(i)}$, facilitates learning by combining the following terms:

\begin{compactitem} 
    \item Gripper-to-object Distance Reward ($r_{\text{dist}}$): Encourages the robot to approach the object of interest by penalizing large distances between them. 
    \item Direction Reward ($r_{\text{dir}}$): Guides the robot to move the keypoints in the direction of the target locations. 
    \item Alignment Reward ($r_{\text{align}}$): Drives the robot to position the keypoints close to their target locations.
    \item Success Bonus ($r_{\text{bonus}}$): Provides an additional reward when the average distance between the keypoints and their target positions remains within a specified threshold for a certain number of timesteps, indicating successful task completion. 
    \item Penalty Term ($r_{\text{penalty}}$): Applies penalties for undesirable actions such as excessive movements, dropping the object, or applying excessive force.
\end{compactitem}

\vspace{-5mm}
\[
\scalebox{0.85}{$
f^{(i)} = \alpha_{\text{dist}} r_{\text{dist}} + \alpha_{\text{dir}} r_{\text{dir}} + \alpha_{\text{align}} r_{\text{align}} + \alpha_{\text{bonus}} r_{\text{bonus}} + \alpha_{\text{penalty}} r_{\text{penalty}}
$}
\]
\vspace{-5mm}



\subsection{Transferring real-world scene to simulation}\label{sec:real2sim}
To transfer the real-world scene within the workspace boundary to simulation, we first generate 3D meshes of manipulable objects, such as the shoe box and shoes shown in \figref{fig:method}, by capturing video footage of each object as it is moved to ensure the camera captures all sides. These videos allow for accurate 3D mesh reconstruction using BundleSDF~\cite{wen2023bundlesdf}, and multiple objects can be processed in parallel to speed up the scanning phase. Once a mesh is created for an object, it can be reused in different settings, eliminating the need to recreate it for each new scenario. With the meshes prepared, we use FoundationPose~\cite{wen2023foundationpose} to estimate the objects' poses, enabling precise placement of the corresponding meshes in the simulated environment. For static elements, like the workspace table and shoe rack in \figref{fig:method}, we capture a point cloud to create their meshes for use in the simulation.

The generated meshes are further used to identify candidate keypoints. For manipulable objects like shoes or books, keypoints are placed at the object's extremities along its axes, defined with respect to the object's center, independent of the human instruction. For static objects like shoe racks, which are part of the environment, keypoints are uniformly distributed across their surfaces. Numerical labels assigned to keypoints are grouped by objects. For example, as shown in \figref{fig:unrolled}, keypoints 1–4 correspond to the box, 5–8 to the left shoe, 9–12 to the right shoe, and the remaining keypoints to the rack. Keypoints that are too close together in the image projection are removed. Specifically, background keypoints (e.g., like rack) near object keypoints are removed first. Among overlapping object keypoints, only the one with the lower numerical label is retained. Note, however, that the VLM is not explicitly told this information but has to infer the association between the keypoints and the objects based on the input image.

\subsection{Policy Training in Simulation}
\label{sec:training}
We control the robot in the end-effector space, which has six degrees of freedom: three prismatic joints for movement along the x, y, and z axes, and three revolute joints for rotation.
The gripper fingers remain closed by default, opening only when grasping objects. Refer to Appendix~\ref{sec:grasping} for a detailed discussion on grasping. 

\textbf{State Space}: 
The state space for our policy captures the essential information to execute the task. The input is a vector \( s_t \) consisting of the gripper's end-effector pose \( (\mathbf{p}_e, \mathbf{q}_e) \in \mathbb{R}^7 \), the pose of object currently being manipulated \( (\mathbf{p}_o, \mathbf{q}_o) \in \mathbb{R}^7 \), a set of object keypoints \( \mathcal{K}_o = \{k_j^{(i)}\}_{j=1}^{K_i} \in \mathbb{R}^{K_i \times 3} \), and their corresponding target positions \( \mathcal{K}_t = \{k_{t_j}^{(i)}\}_{j=1}^{K_i} \in \mathbb{R}^{K_i \times 3} \). $\mathcal{K}_o$ is calculated by applying rigid body transformations to keypoints defined in the object's local coordinate frame, mapping them to their corresponding positions in the world frame. $\mathcal{K}_t$ is derived from the reward function $f^{(i)}$ generated by the VLM. Rotations $\mathbf{q}_e$ and $\mathbf{q}_o$ are represented as quaternions. 
This state space \( s_t = (\mathbf{p}_e, \mathbf{q}_e, \mathbf{p}_o, \mathbf{q}_o, \mathcal{K}_o, \mathcal{K}_t) \) captures essential information on objects of interest as well as the goal of the policy. Instead of incorporating raw RGBD data directly into the state space, object poses and keypoints are extracted from RGBD inputs using a vision-based pose estimation method, as detailed in Section~\ref{sec:deployment}. This preprocessing step removes the necessity of including raw RGBD data in the policy.



\textbf{Action Space}: The action space is defined relative to the gripper's current position and orientation. The policy outputs actions \( a_t = (\Delta \mathbf{p}_e, \Delta \mathbf{r}_e) \), where \( \Delta \mathbf{p}_e \in \mathbb{R}^3 \) and \( \Delta \mathbf{r}_e \in \mathbb{R}^3 \) specifies the changes in translation and rotation respectively.

\textbf{Training Algorithm \& Architecture}: We train our policies using IsaacGym~\cite{makoviychuk2021isaac} simulator with the PPO~\cite{schulman2017proximal} algorithm. We use an actor-critic architecture~\cite{konda1999actor} with a shared backbone. The network is a multi-layer perceptron~(MLP) consisting of hidden layers with 256, 128, and 64 units, each followed by ELU~\cite{clevert2015fast} activation. Currently, it takes about 5 minutes to train per task, which can be prohibitive for certain applications. However, this training time can be reduced by increasing the number of parallel environments and utilizing more powerful GPUs.

\textbf{Domain Randomization (DR)}: Recognizing the challenges inherent in transferring policies between the simulation and the real world, we employ DR to bridge the real-to-sim-to-real gaps. DR is applied to object properties like friction, mass, restitution, compliance, and geometry. We further randomize the object position, the gripper location, and the grasp within a range. We found these to be especially crucial for non-prehensile tasks like pushing. The specific parameter ranges are detailed in Appendix~\ref{sec:domain_randomization}, and the effectiveness of DR is evaluated in Section~\ref{sec:dr}.

\begin{table*}[h]
    \centering
    \begin{tabular}{lcccccccc}
        \toprule
        \multirow{2}{*}{\textbf{Task}} & \multicolumn{4}{c}{\textbf{Annotated (Human labeled reward)}} & \multicolumn{4}{c}{\textbf{Automatic (VLM-generated reward)}} \\
        \cmidrule(lr){2-5} \cmidrule(lr){6-9}
         & \multicolumn{2}{c}{\textbf{Simulation}} & \multicolumn{2}{c}{\textbf{Real-World}} & \multicolumn{2}{c}{\textbf{Simulation}} & \multicolumn{2}{c}{\textbf{Real-World}} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
         & \textbf{\algabrvname (Ours)} & \textbf{Pose} & \textbf{\algabrvname (Ours)} & \textbf{Pose} & \textbf{\algabrvname (Ours)} & \textbf{Pose} & \textbf{\algabrvname (Ours)} & \textbf{Pose} \\
        \midrule
        \textbf{Shoe Place} & 0.945 & 0.938 & 0.8 & 0.9 & \textbf{0.778} & 0.353 & \textbf{0.7} & 0.3 \\
        \textbf{Shoe Push} & 0.871 & 0.850 & 0.7 & 0.7 & \textbf{0.716} & 0.289 & \textbf{0.6} & 0.2 \\
        \textbf{Stowing Push} & 0.901 & 0.914 & 0.8 & 0.7 & \textbf{0.679} & 0.374 & \textbf{0.6} & 0.3 \\
        \textbf{Stowing Reorient} & 0.848 & 0.859 & 0.8 & 0.7 & \textbf{0.858} & 0.265 & \textbf{0.7} & 0.2 \\
        \bottomrule
    \end{tabular}
    \caption{\small{\textbf{Performance of \algabrvname in simulation and real-world}. \algabrvname, which makes use of visual keypoints, significantly outperforms the conventional pose-based approach, especially when using VLMs to automatically generate reward functions.}}
    \label{tab:main_results}
    \vspace{-2em}
\end{table*}


\subsection{Deployment of Trained Policy}
\label{sec:deployment}
The trained RL policy $\pi_i$ is deployed directly in the real world.
Since the policy outputs the end-effector pose,
we employ inverse kinematics to compute the joint angles at each timestep. The RL policy operates at 10Hz, producing action commands that are then clipped to ensure the end effector remains within the workspace limits. For keypoint tracking, we utilize FoundationPose~\cite{wen2023foundationpose} to estimate the object’s pose. These pose estimates are subsequently used to compute the keypoint locations that are defined relative to the objects. When VLM predicts to grasp objects, we use AnyGrasp~\cite{fang2023anygrasp} to detect grasps in the real-world. 
