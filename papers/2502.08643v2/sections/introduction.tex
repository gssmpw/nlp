\section{Introduction}


Suppose that a robot is tasked with placing a pair of shoes on a rack, but a shoe box is occupying the rack, leaving insufficient space for both shoes (\figref{fig:teaser}, top right). The robot must first push the box aside to create space and then proceed to place the shoes. This example highlights the importance of task specification for robots in unstructured, real-world environments, where tasks can often involve multiple implicit steps. In such cases, rigid predefined instructions fail to capture the complexities of interaction required to accomplish the goal. To be effective, task specifications must incorporate commonsense priorsâ€”expectations about how the robot should behave. For instance, rather than attempting to squeeze the shoes in awkwardly, the robot should realize that it must first clear space. 


\begin{figure}[t]
    \centering
    \setlength{\abovecaptionskip}{-10pt}
    \includegraphics[width=\linewidth]{figures/teaser.pdf}
    \captionsetup{type=figure}
    \caption{\small{\textbf{Capabilities of Our Framework.} IKER is designed to handle a wide range of real-world tasks. It can be seamlessly chained to execute multi-step tasks. It exhibits robustness to disturbances and demonstrates the ability to solve problems flexibly.}}
    \label{fig:teaser}
\end{figure}

Recent vision-language models (VLMs) show promise for freeform robotic task specification due to their rapidly advancing ability to encode rich world knowledge by pretraining on vast and diverse datasets~\cite{openai2023gpt,zeng2022socratic, radford2021learningtransferablevisualmodels, jia2021scalingvisualvisionlanguagerepresentation, li2022blip, li2023blip2bootstrappinglanguageimagepretraining, alayrac2022flamingovisuallanguagemodel, yu2022cocacontrastivecaptionersimagetext}. VLMs excel in interpreting natural language descriptions and complex instructions. Their broad knowledge bridges human expectations and robot behavior, capturing human-like priors and problem-solving strategies. However, previous works that leverage VLMs in robotics face two major limitations: (1) they lack the capability to specify precise target locations in 3D, and (2) they are often unable to adapt to the environment changes as the task progresses.


In this work, we introduce \textbf{\algname~(\algabrvname)}, a visually grounded reward function for robotic manipulation that addresses these limitations. %
Inspired by recent work~\cite{huang2024rekep}, we draw the observation that both object positions and orientations can be encoded using keypoints. Hence, \algabrvname allows for fine-grained manipulation in 3D, facilitating complex tasks that require accurate location and orientation control. Additionally, \algabrvname incorporates an iterative refinement mechanism, where the VLM updates the task specification based on feedback from the robot's interactions with the environment. This mechanism enables %
dynamically-adjusting strategies and intermediate steps, such as repositioning objects for a better grasp.


While VLMs excel in processing real-world visual data, training policies directly in the real world is often infeasible due to safety, scalability, and efficiency constraints. To address this, we first generate \algabrvname using real-world observations, then transfer the scene and the reward to simulation for training, and finally, deploy the optimized policy back into the real world. Thus, our system operates in a real-to-sim-to-real loop. %


We demonstrate the efficacy of our real-to-sim-to-real framework with \algabrvname across diverse scenarios involving everyday objects like shoes and books. These include prehensile tasks, such as placing shoes on racks, and non-prehensile tasks, like sliding books to target locations. We conduct both quantitative and qualitative evaluations to assess the system's ability to perform complex, long-horizon tasks autonomously. The results showcase human-like capabilities, including multi-step action sequencing, spontaneous error recovery, and the ability to update strategies in response to changes in the environment.


\begin{figure*}[t]
    \setlength{\abovecaptionskip}{-12pt}
    \centering
    \includegraphics[width=\linewidth]{figures/overview.pdf}
    \vspace{-4.8em} %
    \captionsetup{type=figure}
    \caption{\small{\textbf{Framework Overview.} \algname (\algabrvname) is a visually grounded reward generated by Vision-Language Models (VLMs) as task specification. The framework reconstructs the real-world scene in simulation, and the generated reward is used to train RL policies, which are subsequently deployed in the real-world.}}
    \label{fig:overview}
    \vspace{-1.5em}
\end{figure*}
