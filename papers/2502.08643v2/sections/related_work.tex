\section{Related Work}
\xhdr{VLMs in Robotics.} VLMs have become a prominent tool in robotics~\cite{ahn2022can,codeaspolicies2022,huang2023voxposer,brohan2022rt,brohan2023rt,liu2024moka,huang2024copa,team2024octo,huang2023instruct2act,xu2023creative,zhou2023generalizable,nasiriany2024pivot,di2024keypoint,zeng2023large,zha2024distilling,arenas2024prompt,mahadevan2024generative,liang2024learning,huang2024grounded,ren2023robots,jiang2022vima,yang2024guiding,duan2024aha,duan2024manipulate,yuan2024robopoint,singh2023progprompt,tang2024kalie,liang2024eurekaverse,zawalski2024robotic}. Existing works utilizing VLMs in robotics primarily focus on two areas: task specification~\cite{ahn2022can,codeaspolicies2022,huang2023voxposer,liu2024moka,huang2024copa,nasiriany2024pivot} and low-level control~\cite{brohan2022rt,brohan2023rt,o2023open,codeaspolicies2022}. Our work aligns with the former, with an emphasis on flexibility and adaptability in complex, real-world environments.

For task specification, many works employ VLMs to break down complex tasks into manageable subtasks, demonstrating their utility in bridging high-level instructions and robotic actions. Huang \etal~\cite{huang2022language} demonstrate the use of LLMs as zero-shot planners, enabling task decomposition into actionable steps. Similarly, Ahn \etal~\cite{ahn2022can} leverage VLMs to parse long-horizon tasks and sequence them into executable steps for robots. Belkhale \etal~\cite{rth2024arxiv} introduce ``language motions'' that serve as intermediaries between high-level instructions and specific robotic actions, allowing policies to capture reusable, low-level behaviors.  Unlike these works, our approach focuses on flexible interpretation of tasks in the context of a dynamically changing environment. 
 

Beyond task decomposition, VLMs have been used to generate affordances and value maps that guide robotic actions. Huang \etal~\cite{huang2023voxposer} employs VLMs to generate 3D affordance maps, providing robots with spatial knowledge of which parts of the environment are suitable for interaction. Liu \etal~\cite{liu2024moka} use VLMs to predict point-based affordances, enabling zero-shot manipulation tasks. Zhao \etal~\cite{zhao2024vlmpc} incorporate VLMs into model predictive control, where the models predict the outcomes of candidate actions to guide optimal decision-making. These works demonstrate the potential of VLMs to bridge high-level task understanding with spatial and functional knowledge needed for robotic control. Similar to our work, Huang \etal~\cite{huang2024rekep} use keypoints and define relations and constraints between them to execute manipulation tasks, but their approach follows an open-loop strategy. In contrast, we employ a closed-loop approach, enabling dynamic plan adjustments. Additionally, our approach also supports non-prehensile manipulations, such as pushing.


Some works have also explored VLMs for reward function generation~\cite{yu2023language, ma2023eureka, ma2024dreurekalanguagemodelguided,xie2023text2reward}. However, most of these approaches have limited real-world applicability. Some lack demonstrations on real robots~\cite{ma2023eureka}, are restricted to a single real-world scenario~\cite{xie2023text2reward}, or focus on highly constrained tasks like a robot dog walking on a ball~\cite{ma2024dreurekalanguagemodelguided}. In contrast, our work demonstrates the versatility and robustness of VLM-generated rewards on multiple real-world manipulation tasks.


\xhdr{Real-to-Sim and Sim-to-Real.}
Real-to-sim has gained significant attention for its ability to facilitate agent training. Once a scene is transferred to simulation, it can be used for a wide range of tasks, including RL. Several approaches focus on reconstructing rigid bodies for use in simulation~\cite{kappler2018real,wen2022you,liu2024one,xu2025sparp,shi2023zero123++,liu2023zero,gao2022get3d}. For instance, Kappler et al.~\cite{kappler2018real} introduce a method for reconstructing rigid objects to facilitate grasping. 
Some works rather focus on reconstructing articulated objects~\cite{mu2021sdf,jiang2022ditto,nie2022structure,chen2024urdformer,mandi2024real2code,liu2023paris,liu2024cage,liu2024singapo}. Huang et al.~\cite{huang2012occlusion} present methods for reconstructing the occluded shapes of articulated objects. Jiang \etal~\cite{jiang2022ditto} introduce a framework, DITTO, to generate digital twins of articulated objects from real-world interactions. In our work, we utilize the fast state-of-the-art BundleSDF method~\cite{wen2023bundlesdf} to generate object meshes that are transferred to the simulation. 

Sim-to-real transfer has shown great performance in a variety of skills, including tabletop manipulation~\cite{shridhar2021cliport, jiang2024transicsimtorealpolicytransfer}, mobile manipulation~\cite{gu2022multiskill, yenamandra2023homerobot}, dynamic manipulation~\cite{huang2023dynamic}, dexterous manipulation~\cite{chen2023sequential, qin2022dexpointgeneralizablepointcloud,qi2023hand,yin2023rotating}, and locomotion~\cite{DBLP:conf/rss/KumarFPM21,he2024agilesafelearningcollisionfree}. 
However, directly deploying learned policies to physical robots cannot guarantee successful performance due to the sim-to-real gap. 
To bridge this gap, researchers have developed many techniques, such as system identification~\cite{tan2018simtoreal, chang2020sim2real2sim, lim2021planar}, domain adaptation~\cite{bousmalis2018using, arndt2019metareinforcementlearningsimtoreal, rao2020rlcycleganreinforcementlearningaware,james2019sim,du2022bayesian}, and domain randomization~\cite{DBLP:conf/rss/KumarFPM21, openai2019solving, tobin2017domain,antonova2021bayessimig,lee2020learning,peng2018sim,tobin2017domain,chebotar2019closing}. 
In our work, we use domain randomization as it does not require any interaction data from the real world during training. It relies entirely on simulation and makes policies robust by exposing them to a wide variety of randomized conditions. Recently, Torne \etal~\cite{torne2024reconciling} proposed RialTo, a complete real-to-sim-to-real loop system that focuses on leveraging simulation to robustify imitation learning policies trained using real-world collected demonstrations. In contrast, we focus on executing long-horizon tasks by training only in simulation, bypassing the need for demonstrations.

