\section{Conclusion and Limitations}
In this work, we introduced Iterative Keypoint Reward (\algabrvname), a framework that leverages VLMs to generate visually grounded reward functions for robotic manipulation in open-world environments. By using keypoints from RGB-D observations, our approach enables precise SE(3) control and integrates priors from VLMs without relying on rigid instructions. \algabrvname bridges simulation and real-world execution through a real-to-sim-to-real loop, training policies in simulation and deploying them in physical environments. Experiments across diverse tasks demonstrate the framework's ability to handle complex, long-horizon challenges with adaptive strategies and error recovery. This work represents a step toward more intelligent and flexible robots capable of operating effectively in dynamic, real-world settings.


Despite these advancements, our approach has certain limitations. We need to capture objects from all views to obtain object meshes. In the future, this may be simplified by using methods~\cite{liu2023zero} that can generate meshes from a single image. Additionally, our real-to-sim transfer does not account for dynamics parameters, which could be modeled more accurately through system identification techniques. Also, while our framework reconstructs multiple objects in the environment, we do not account for tasks involving complicated multi-object interactions, limiting our evaluation primarily to single-object manipulation at each stage. 
