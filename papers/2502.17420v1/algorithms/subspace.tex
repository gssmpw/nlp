
\begin{algorithmic}[1]
\STATE \textbf{Initialize} $\basis = [\basisvec_1, \dots, \basisvec_n]$ randomly
\WHILE{not converged}
\STATE Sample batch $\batch \sim \data$
\STATE $\loss_{\text{sample}} \leftarrow \mathbb{E}_{\direction \sim \text{Sample}(\basis)}[\textsc{ComputeLoss}(\direction, \model, \batch)]$
\STATE $\loss_{\text{basis}} \leftarrow \frac{1}{n}\sum_{i=1}^n \textsc{ComputeLoss}(\basisvec_i, \model, \batch)$
\STATE $\loss = \loss_{\text{sample}} + \loss_{\text{basis}}$
\STATE $\basis \leftarrow \basis - \eta \nabla_{\basis}\loss$
\STATE $\basis \leftarrow$ \textsc{GramSchmidt}$(\basis)$
\ENDWHILE
\end{algorithmic}
\vspace{1em}
\begin{algorithmic}[1]
\STATE \textbf{function} \textsc{Sample}$(\basis)$
\STATE \hspace*{1em} $\vs \sim \text{Unif}({\vx \in \mathbb{R}^n_+ : ||\vx||_2 = 1})$
\STATE \hspace*{1em} $\direction = \basis\vs$
\STATE \hspace*{1em} \textbf{return} $\direction$
\end{algorithmic}
