\section{Background}\label{sec:background}
\textbf{Notation.} 
\label{sec:notation}

\label{sec:notation}
Let $ \model : \tokenspace^{\seqlength} \to \Delta^{\seqlength \times |\tokenspace|} $ denote a language model, where $ \Delta^{|\tokenspace|} $ is the probability simplex over vocabulary $ \tokenspace $. Given a prompt 
$ \instruction = (t_1,\dots,t_{\seqlength}) \in \tokenspace^{\seqlength} $ consisting of tokens $t_i$, each token is first embedded:
$ x_i^{(0)} = \textsc{Embed}(t_i) $. The model then processes the token sequence through $ L $ layers, where at each layer $ l=1,\dots,L $ and token position $ i $ the following transformation is applied:
\begin{equation*}
    \tilde{\rstream}_i^{(l)} = \rstream_i^{(l)} + \textsc{Attn}^{(l)}(x_{1:i}^{(l)}), \;\; {\rstream}_i^{(l+1)} = \tilde{\rstream}_i^{(l)} + \textsc{MLP}(\tilde{\rstream}_i^{(l)})
\end{equation*}
The final residual stream $ \smash{x_i^{(L+1)}} $ is unembedded to yield logits:
$ \smash{\ell_i = \textsc{Unembed}(x_i^{(L+1)})} $. The softmax function converts these logits into a probability distribution over tokens:
$ \smash{P(t \mid t_1,\dots,t_i) = \operatorname{softmax}(\ell_i)_t} $. We omit technical details that are not critical for this work such as LayerNorm.

\textbf{Extracting refusal directions.} Paired prompts of harmful and harmless requests allow the extraction of a directional feature from the model's residual stream as shown by prior work \cite{panickssery_steering_2024, bolukbasi2016mancomputerprogrammerwoman, burns2024discoveringlatentknowledgelanguage}. Recent studies obtain this direction by computing the \textit{difference-in-means} (\dimacro ) \cite{panickssery_steering_2024, arditi2024refusallanguagemodelsmediated, stolfo2024improvinginstructionfollowinglanguagemodels} between model representations on datasets of harmful prompts $\data_{\text{harm}}$ and harmless prompts $\data_{\text{good}}$:
\begin{equation*}
    \dimdir_i^{(l)} = \frac{1}{|\data_{\text{harm}}|}\left[\sum_{\instruction' \in \data_{\text{harm}}}\rstream_i^{(l)}(\instruction')\right] - \frac{1}{|\data_{\text{safe}}|}\left[\sum_{\instruction \in \data_{\text{safe}}}\rstream_i^{(l)}(\instruction)\right]
\end{equation*}
Here, $\rstream_i^{(l)}(\instruction)$ represents the residual stream activations at position $i$, layer $l$ for input prompt $\instruction$.


\textbf{Adversarial steering attacks.}
The extracted harmfulness direction can be used to manipulate the model’s refusal behavior. With white-box access, an attacker can prompt the model with harmful queries and suppress activations in the harmfulness direction, thereby reducing the model’s probability of refusal. 
This can be done through \emph{directional ablation} of $\direction$ (where $\normdirection$ denotes the unit vector) \cite{zou_representation_2023}:
\begin{equation}
\tilde{\rstream}^{(l)}_{i} = \rstream^{(l)}_i - \normdirection \normdirection^\top \rstream^{(l)}_i,
\end{equation}
which projects the residual stream to a subspace orthogonal to $\direction$, or alternatively through \emph{activation subtraction}:
\begin{equation}
\check{\rstream}^{(l)}_{i} = \rstream^{(l)}_i - \alpha \cdot\normdirection,
\end{equation}
which subtracts a scaled $\direction$ from the residual stream.%
We follow common practice to apply both operations across all token positions and ablation across all layers while doing subtraction only at a single layer.
