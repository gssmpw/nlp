\section{Mechanistic Understanding of Directions}\label{sec:mech_understanding}
\begin{graybox}
    Research Question: Are there genuinely independent directions that influence a model's refusal behavior? Can we access the discovered refusal directions through perturbations in the token space?
\end{graybox}

In the previous section, we demonstrated that refusal behavior spans a multi-dimensional cone with infinitely many directions. However, whether the orthogonal refusal-mediating basis vectors manipulate independent mechanisms remains an open question. In this section, we conduct a mechanistic analysis to investigate how these directions interact within the modelâ€™s activation space and whether they can be directly influenced through input manipulation. This allows us to determine whether they are merely latent properties of the network or actively utilized by the model in response to specific prompts.


\subsection{Representational Independence}\label{sec:repind}
We defined the basis vectors of the cones to be orthogonal, which is often considered an indicator of causal independence. The intuition is that if two vectors are orthogonal, they each influence a third vector without interfering with the other. Mathematically, for the directions $\direction$, $\dimdir$ and representation $\rstream_i^{(l)}$ we have:
\begin{equation*}
    \text{if}\; \direction^T\dimdir = 0\; \text{then}\; \direction^T(\rstream_i^{(l)} - \dimdir\dimdir^T\rstream_i^{(l)}) = \direction^T\rstream_i^{(l)}.
\end{equation*}
However, despite this mathematical property, recent work by \citet{park_linear_2024} suggests that in language models, conclusions about causal independence cannot be drawn using orthogonality measured with the Euclidean scalar product. Although their assumptions differ from ours, especially since they assume a one-to-one mapping from output feature to direction in activation space, their experiments suggest that independent directions are almost orthogonal. This motivates a deeper empirical examination of how orthogonal refusal directions in language models interact in practice.

\textbf{Are orthogonal directions independent?}
To explore this, we first use \oursacro to identify a direction $\direction$ for Gemma 2 2B that is orthogonal to the \dimacro direction $\dimdir$, i.e., $\direction^\top \dimdir = 0$. We then measure how much one direction is influenced when ablating the other direction by monitoring the cosine similarity $\smash{\cos(\lambda,\mu) = \frac{\lambda^\top \mu}{\lvert\lvert\lambda\rvert\rvert\cdot\lvert\lvert\mu\rvert\rvert}}$ between the prompt's representation in the residual stream $\rstream$ and the directions $\dimdir$ and $\ourdir$. Specifically, we track: $\smash{\cos(\ourdir, \rstream_i^{(l)}(\harmfulinstruction))}$ and $\smash{\cos(\dimdir, \rstream_i^{(l)}(\harmfulinstruction))}$ at the last token position and for all layers $\smash{l \in \{0, \dots, L\}}$ on 128 harmful instructions in our validation set. Intuitively, ablating a causally independent direction in earlier layers should not intervene with the reference direction in later layers. Otherwise, there is some indirect influence through the non-linear transformations of the neural network.

The top row of \Cref{fig:orthogonal-effects}  shows how the cosine similarity between the \oursacro and \dimacro directions changes under intervention. The left plot shows the cosine similarity between the \oursacro direction and the activations on a normal forward pass (solid line) and while ablating the \dimacro direction (dashed line). The right plot presents the reverse setting. Despite enforced orthogonality, ablating \oursacro indirectly reduces the representation of the \dimacro direction in the model activations in the later layers, as measured by cosine similarity. This effect is reciprocal, suggesting that orthogonality alone does not guarantee independence throughout the network.

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{images/orthogonal_four_plots_fixed.png}
    \vspace{-10pt}
    \caption{Influence of representational independence. Figure (a) shows the cosine similarity between $\text{RDO}_\perp$, a refusal direction orthogonal to \dimacro, and the model activations in a normal forward pass (solid line) compared to a forward pass where \dimacro is ablated (striped line). Figure (b) shows the reverse scenario. In Figure (c) and (d) we contrast how the DIM direction and a representationally independent direction (RepInd) influence each other.}
    
    \label{fig:orthogonal-effects}
\end{figure}
Motivated by this observation, we introduce a stricter notion of independence: \emph{Representational Independence (RepInd)}:
\begin{definition}
The directions $\lambda, \mu \in \mathbb{R}^d$ are \textit{representationally independent} (under directional ablation) with respect to the activations $\rstream$ of a model in a set of layers $l \in L$ if:
    \begin{equation*}
    \begin{split}
    \forall l \in L: \cos(\rstream^{(l)}, \lambda) &= \cos(\tilde{\rstream}^{(l)}_{abl(\mu)}, \lambda)  \\ 
    \text{and}\;
    \cos(\rstream^{(l)}, \mu) &= \cos(\tilde{\rstream}^{(l)}_{abl(\lambda)}, \mu).
    \end{split}
    \end{equation*}
\end{definition}
This means that, instead of relying solely on orthogonality, we define two directions as representationally independent if ablating one has no effect on how much the other is represented in the model activations. To enforce this property, we extend \Cref{algo:single_direction} with an additional loss term that penalizes changes in cosine similarity at the last token position when ablating on harmful instructions:
\begin{equation*}
\begin{split}
\mathcal{L}_{\text{RepInd}} = \frac{1}{|L|} \sum_{l \in L} \Big[ \big(\cos(x^{(l)}, \direction) &- \cos(\tilde{x}^{(l)}_{abl(\dimdir)}, \direction)\big)^2 \\+ \big(\cos(x^{(l)}, \dimdir) &- \cos(\tilde{x}^{(l)}_{abl(\direction)}, \dimdir)\big)^2 \Big].
\end{split}
\end{equation*}


\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{images/performance.png}
    \vspace{-8pt}
    \caption{Performance of RepInd directions. Each direction is representationally independent to all previous directions and the \dimacro direction.}
    \label{fig:rep-performance}
\end{figure}

\textbf{Do independent directions exist?}
With this extension, we can find a direction that is RepInd from the \dimacro direction, yet still fulfills the refusal properties from \Cref{def:refusal-properties}. We show the representational independence in the second row of \Cref{fig:orthogonal-effects}, where we see that the RepInd and \dimacro direction barely affect each other's representation under directional ablation.

We iteratively search for additional directions that are not only RepInd to \dimacro but also of all previously identified RepInd directions. Despite these strong constraints, we successfully identify at least five such directions that maintain an ASR significantly above random vector intervention (\Cref{fig:rep-performance}), as well as a refusal cone with RepInd basis vectors (\Cref{fig:cone-repind}). However, in \Cref{fig:rep-performance} and \Cref{fig:cone-repind} performance degrades more rapidly compared to the results in \Cref{sec:gradient-based-directions} and \Cref{sec:cones}. This decline could be attributed to the increased difficulty of the optimization problem due to additional constraints. Alternatively, it may suggest that Gemma 2 2B possesses a limited number of directions that independently contribute to refusal. If the latter is true, this implies that the directions in the refusal cones exhibit non-linear dependencies.
Nevertheless, these results show that refusal in LLMs is mediated by multiple \emph{independent} mechanisms, underpinning the idea that refusal behavior is more nuanced than previously assumed.

\subsection{Manipulation from input}
\textbf{Can we access these directions from the input?}
Having found several independent directions that are distinct from \dimacro, we investigate whether these directions can ever be "used" by the model, by checking if they are accessible from the input or if they live in regions that no combination of input tokens activates. To this end, we use GCG \cite{zou_universal_2023} to train adversarial suffixes, which are extensions to the prompts that aim to circumvent the safety alignment. In addition to the standard cross-entropy loss on an affirmative target, we add a loss term that incentivizes the suffix to ablate RepInd 1. 

\begin{wrapfigure}[14]{l}{0.5\linewidth}
    \vspace{-8pt}
    \includegraphics[width=\linewidth]{images/gcg_repind.png}
    \vspace{-25pt}
    \caption{Representation of \smash{RepInd 1} in model activations on harmful instructions before and after adversarial attacks with GCG.}
    \label{fig:input_manipulation}
\end{wrapfigure}
In \Cref{fig:input_manipulation}, we show the cosine similarities between RepInd 1 and the model activations on both harmful prompts $\harmfulinstruction$ from \textsc{JailbreakBench} and the same prompts with adversarial suffixes $\instruction_{\text{adv}}$. We observe that GCG is able to create suffixes that significantly reduce how much RepInd 1 is represented. These suffixes successfully jailbreak the model $36\%$ of the time, which is similar to the ASR of RepInd 1. 

\begin{mybox}
    \textbf{Key Takeaways.} We demonstrate the ability to identify independent refusal directions, revealing that these directions correspond to distinct underlying concepts and can be directly accessed through input manipulations. This further underscores the utility of our representational independence framework, which provides a generalizable approach for analyzing and understanding a wide range of representational interventions in LLMs.
\end{mybox}
