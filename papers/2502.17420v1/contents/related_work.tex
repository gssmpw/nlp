\section{Related Work}
\textbf{Adversarial attacks for LLMs.} 
Many studies have explored hand-crafted adversarial techniques, such as persona modulation \cite{shah_scalable_2023}, language modifications \cite{zhu_autodan_2023}, or prompt engineering using repetitions and persuasive phrasing \cite{rao_tricking_2024}. Other works take a more systematic approach, employing techniques like genetic algorithms and random search \cite{chen_2024_eliciting}, discrete optimization over input tokens \cite{zou_universal_2023}, or gradient-based methods to identify high-impact perturbations \cite{geisler_attacking_2024}. 
While identifying these vulnerabilities enables adversarial fine-tuning~\cite{xhonneux2024efficient} or improved training through Reinforcement Learning with Human Feedback (RLHF), recent works suggest that robustness remains a challenge \cite{zou_representation_2023, schwinn2024soft, geisler_attacking_2024, scholten2025a}.

\textbf{Interpretability of LLMs.} A parallel line of research focuses on understanding the internal mechanisms of LLMs, as their natural language outputs provide a unique opportunity to link internal states to interpretable behaviors. Interpretability research has led to the identification of various ``features''---concepts represented by distinct activation patterns \cite{cunningham_sparse_2023}---as well as ``circuits'', which are subnetworks that implement a specific function or behavior. Prominent examples are backup circuits \cite{nanda2024Attribution} and information mover circuits \cite{wang_interpretability_2022}. Many interpretability insights rely on extracting features using paired inputs with opposing semantics \cite{burns2024discoveringlatentknowledgelanguage} and then manipulating residual stream activations to elicit specific behaviors \cite{panickssery_steering_2024}. 
Representation engineering, as proposed by \citet{zou_representation_2023}, investigates the linear representation of concepts such as truthfulness, honesty, and fairness in LLMs. The effectiveness of these methods supports the hypothesis that many features are encoded linearly in LLMs \cite{marks2024geometrytruthemergentlinear}. These insights allow researchers to pinpoint and manipulate concept representations or specific circuits, enabling targeted debugging of  behaviors, mitigating biases, and advancing safer, more reliable AI systems.

\textbf{Understanding Refusal Mechanisms.} Recent research has focused on understanding the mechanisms underlying refusal behaviors in LLMs. For example, removing safety-critical neurons has been shown to decrease robustness \cite{wei2024assessingbrittlenesssafetyalignment, li2024revisitingjailbreakinglargelanguage}. %
\citet{zheng_prompt-driven_2024} demonstrate that adding explicit safety prompts shifts the internal representation along a harmfulness direction.
\citet{obrien2024steeringlanguagemodelrefusal} propose to use sparse autoencoders to identify latent features that mediate refusal. The most relevant work to ours is \citet{arditi2024refusallanguagemodelsmediated}, which builds on \citet{zou_representation_2023} and examines the representation of refusal in LLMs. Their work suggests that a single direction a model's activation space determines whether the model accepts or refuses a request. We challenge this notion by showing that refusal is mediated through more nuanced mechanisms. %


