\section{Introduction}
The breakthrough of scaling large language models (LLMs) has led to an unprecedented leap in capabilities, driving widespread real-world adoption \cite{openai2022chatgpt}. However, these advancements also introduce serious risks.
As artificial intelligence becomes more powerful, it can be misused for harmful purposes, such as attacking critical 
\begin{wrapfigure}[16]{r}{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{images/cone_darkblue.pdf}
    \vspace{-20pt}
    \caption{An example of a 3D concept cone with its basis vectors. All directions in the cone mediate refusal.}
    \label{fig:cone}
\end{wrapfigure}
infrastructure or spreading  misinformation. 
Ensuring that these models remain aligned with human values has become a crucial research challenge \cite{liu2023trustworthy,schwinn2025adversarialalignmentllmsrequires}. Despite significant progress, LLMs, like all machine learning models, remain vulnerable to adversarial attacks that can bypass alignment mechanisms and induce harmful outputs \cite{szegedy2014intriguingpropertiesneuralnetworks, carlini2024alignedneuralnetworksadversarially}.


Recent work in interpretability has provided valuable insights into how LLMs encode and process information \cite{nanda2024Attribution, wang_interpretability_2022, cunningham_sparse_2023, heinzerling2024monotonic}. Prior studies \cite{belrose2023leaceperfectlinearconcept, gurnee2023language, marks2024geometrytruthemergentlinear} suggest that concepts---ranging from simple to complex---are often encoded linearly in the model's residual stream. Methods such as representation engineering \cite{zou_representation_2023} allow researchers to use input prompts to analyze model behavior by extracting and manipulating such concepts. However, the mechanisms enabling adversarial jailbreaks that bypass alignment safeguards remain poorly understood. Some evidence suggests that refusals to harmful queries are mediated by a single “refusal direction” in activation space \cite{arditi2024refusallanguagemodelsmediated}, and that jailbreaks rely on manipulating this direction \cite{yu2024robust}, yet these assumptions require further examination.

In this work, we go beyond extracting concepts using common input prompt methods by introducing a novel \textit{gradient-based} approach to \textit{representation engineering} which we use to investigate the mechanisms underlying refusal behavior in LLMs. We extract refusal-mediating directions more effectively, improving both precision and control while minimizing unintended side effects, which we demonstrate in \Cref{sec:gradient-based-directions}. Unlike prior work that assumes model refusal is controlled by a single linear direction, we show in \Cref{sec:cones} that there exist \emph{multi-dimensional polyhedral cones} which contain infinite refusal directions; we show an illustrative example in \Cref{fig:cone}. To further characterize refusal mechanisms in language models, we introduce \emph{representational independence}, a criterion for identifying directions that remain mutually unaffected under intervention, capturing both linear and non-linear dependencies across layers. In \Cref{sec:mech_understanding}, we demonstrate that even under this strict notion of independence, multiple complementary refusal directions exist.

To summarize, our core \textbf{contributions} are:\vspace{-0.19in}
\begin{itemize}
\itemsep-0.5em 
    \item We show that our gradient-based representation engineering can advance general LLM understanding and specifically demonstrate its efficacy for understanding refusal mechanisms.
    \item We introduce representational independence, a practical framework for characterizing how different interventions interact within an LLM’s activation space, and use it to find independent refusal directions.
    \item We show that rather than a single refusal direction, there exist multi-dimensional cones in which all directions mediate refusal.
\end{itemize}
