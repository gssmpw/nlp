\section{Setup Details} \label{app:setup-details}
\subsection{Datasets}
\label{app:datasets}
We construct our experimental dataset using harmful and harmless instructions from established benchmarks. For harmful instructions, we draw from \textsc{SALADBench} \cite{li2024salad}, a comprehensive collection of adversarial prompts from diverse sources. We exclude the Multilingual \cite{wang2023all} and ToxicChat \cite{lin2023toxicchat} sources since they are unsuited as harmful instructions. Afterwards, we sample up to 256 instructions from each remaining source. This results in 1,184 instructions for training and 128 for validation. We sample equal numbers of harmless instructions from the \textsc{Alpaca} dataset, and additionally reserve 128 more harmless instructions for testing.

\subsection{Models}
We exclusively use chat models for our experiments, but omit "IT" and "INSTRUCT" from model names. We use each chat model's default chat template throughout our analysis.

\begin{table}[h]
\centering
\caption{Model families, sizes, and references.}
\label{tab:models}
\begin{tabular}{lll}
\toprule
\textbf{Model family} & \textbf{Sizes} & \textbf{Reference} \\
\midrule
\textsc{Qwen2.5 Instruct} & 1.5B, 3B, 7B, 14B & \citet{yang2024qwen2} \\
\textsc{Gemma 2 IT} & 2B, 9B  & \citet{team2024gemma} \\
\textsc{Llama-3 Instruct} & 8B & \citet{dubey2024llama} \\
\bottomrule
\end{tabular}
\end{table}




\subsection{Hyperparameters and Implementation}
\begin{table}[h]
\centering
\caption{Hyperparameters for all algorithms}
\begin{tabular}{lll}
\toprule
\textbf{Component} & \textbf{Parameter} & \textbf{Value} \\
\midrule
Training & Total Batch Size & 16 \\
& Gradient Accumulation Steps & 16 \\
& Base Learning Rate & 0.01 \\
& Learning Rate Reduction & Every 5 batches if plateaued \\
& Learning Rate Factor & Divide by 1/10 up to 2 times \\
& Optimizer & AdamW \\
& Weight Decay & 0 \\
\midrule
Main Loss & Ablation Loss Weight $\lambda_{\text{abl}}$ & 1.0 \\
& Addition Loss Weight $\lambda_{\text{add}}$ & 0.2 \\
& Retain Loss Weight $\lambda_{\text{ret}}$ & 1.0 \\
\midrule
Monte Carlo Sampling & Samples per Accumulation Step & 16 \\
& Effective Samples per Batch & 256 \\
\midrule
RepInd  & RepInd Loss Weight $\lambda_{\text{ind}}$ & 200 \\
 & Layer Cutoff & 0.9\\
\bottomrule
\end{tabular}
\label{tab:hyperparameters}
\end{table}
\Cref{tab:hyperparameters} presents the hyperparameters used in our algorithms. Since our method converges before completing a full epoch, we do not utilize validation scores during training. Instead, after convergence, we apply the direction selection algorithm from \citet{arditi2024refusallanguagemodelsmediated} to identify the optimal refusal direction from the last 20 training steps.

\textbf{Implementation and Evaluation Framework.}
All algorithms and exploratory experiments are implemented using the NNsight \cite{fiotto2024nnsight} library. Additionally, we use the LM Evaluation Harness \cite{eval-harness} to run TruthfulQA \cite{lin2021truthfulqa} with default settings, with the exception that we enable the use of each model’s default chat templates.

\textbf{Retain and Representational Independence Loss Computation.}
The retain loss is computed as the KL divergence between the probability distributions derived from the logits of the model with and without directional ablation, masked over a target response and the last token of the chat template. The resulting value is then averaged across tokens. For a single instruction $\harmlessinstruction$ with its target $\retaintarget$, we formalize the loss as follows: 
\begin{equation*}
\mathcal{L}_{\text{retain}} = \klloss(\model_{\text{ablate}(\direction)}(\harmlessinstruction), \model(\harmlessinstruction), \retaintarget) = \frac{1}{|\mathcal{I}|} \sum_{i \in \mathcal{I}} \sum_{t \in \tokenspace} f(\harmlessinstruction + \retaintarget)_{i,t} \log \frac{f(\harmlessinstruction + \retaintarget)_{i,t}}{f_{\text{ablate}}(\harmlessinstruction + \retaintarget)_{i,t}}\,,
\end{equation*}
where $\mathcal{I}$ contains the target token indexes and the last instruction token's index, the subscript ${i,t}$ denotes the model output at sequence position $i$ and vocabulary index $t$ as defined in \Cref{sec:notation}, and $+$ denotes concatenation.

For the implementation of the representational independence loss, $\mathcal{L}_{\text{RepInd}}$, we compute the average loss over the tokens in the harmful instructions $\harmfulinstruction$. The RepInd loss is computed over the first 90\% of layers, as applying it too close to the unembedding layer overly constrains the model’s output.

\textbf{Selection of Refusal and Independent Directions}
In \cref{algo:single_direction}, after training the refusal directions to convergence, we again use the direction selection algorithm from \citet{arditi2024refusallanguagemodelsmediated} to identify the most effective directions from the final 20 training steps. 

In \cref{sec:cones}, we extend this selection process to determine a basis where all basis vectors effectively mediate refusal (from the last 20 bases of the training). If no such basis exists, we instead select the basis where the samples are most effective for directional ablation using the refusal score heuristic from the selection algorithm.

\textbf{Training Procedure for Representational Independence Directions}
In \cref{sec:mech_understanding}, our approach to training and validating representationally independent (RepInd) directions differs because of high variance between different runs. For each RepInd direction, we train five candidate vectors and select the one with the lowest refusal score on our validation set. This process is repeated five times, ultimately producing our final set of RepInd directions. The RepInd loss is computed as the sum of losses over all vectors that the current vector should remain independent of.
















\section{Additional Experiments}\label{app:additional-experiments}

In this section, we present further experimental results. \Cref{fig:ind-refusal} demonstrates that adding the refusal direction successfully induces refusal behavior across all models for both \dimacro and \oursacro. Similarly, \Cref{fig:gemma-cones} illustrates the refusal cone performance for Gemma 2, confirming the existence of higher-dimensional refusal cones within the Gemma 2 family. The results suggest that the maximum cone dimensionality may be four, as the lower bounds of the ASR drop sharply beyond this point. In \Cref{fig:subspace-induce} we apply refusal cones to various Qwen 2.5 models across different dimensions, revealing that inducing refusal is significantly easier than conducting an attack. Notably, most directions even in high-dimensional cones remain effective at inducing refusal responses.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{images/harmless_scores.png}
    \caption{Refusal scores of different models on harmless instructions after activation addition that aims to induce refusal.}
    \label{fig:ind-refusal}
\end{figure}
\begin{figure}
    \centering
    \hspace*{5em} 
    \includegraphics[width=0.8\linewidth]{images/gemma_attack.png}
    \caption{Attack success rates in refusal cones of different dimensions for the Gemma 2 model family. We observe that for the Gemma 2 2B the lower bounds start to degrade significantly for dimension 5.}
    \label{fig:gemma-cones}
\end{figure}
\begin{figure}
    \centering
    \hspace*{5em} 
    \includegraphics[width=0.9\linewidth]{images/qwen_harmless.png}
    \caption{Using refusal cones to induce refusal across various Qwen 2.5 models with different dimensions. We observe that inducing refusal is generally easier than executing an attack. In this setting, nearly all dimensions maintain strong performance in eliciting refusal responses, even for benign requests.}
    \label{fig:subspace-induce}
\end{figure}

\begin{figure}
    \centering
    \hspace*{5em} 
    \includegraphics[width=0.8\linewidth]{images/gemma_repind_dim_attack.png}
    \caption{Attack success rates in refusal cones of different dimensions for Gemma 2 2B where the basis vectors are trained to be representationally independent.}
    \label{fig:cone-repind}
\end{figure}

\Cref{fig:asr_given_conedim} examines the attack success rate when sampling multiple vectors from various $N$-dimensional refusal cones and selecting the best-performing sample per prompt for Gemma 2, 2B. We observe that ASR improves with increasing cone dimensionality but plateaus at four dimensions, suggesting that higher-dimensional cones provide an advantage over single-direction manipulation by capturing complementary mechanisms. The plateau likely results from the model’s inability to encode higher-dimensional refusal cones, a hypothesis further supported by \Cref{fig:gemma-cones}.
\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{images/max_asrs.png}
    \caption{Attack success rates when sampling vectors from the N-dimensional refusal cones and selecting the best-performing sample per prompt for Gemma 2 2B. ASR increases with cone dimensionality but plateaus at four dimensions, suggesting that higher-dimensional cones provide an advantage over single-direction manipulation by capturing complementary mechanisms. The plateau likely arises because \cref{algo:subspace} cannot find an additional basis vector that preserves the refusal properties in the cone, suggesting that the model does not support a cone of this dimension. \Cref{fig:gemma-cones} also provides evidence for this claim.}
    \label{fig:asr_given_conedim}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{images/properties.png}
    \caption{Refusal scores of refusal vectors sampled from Gemma 2 2B refusal cones compared to the \dimacro direction when scaling the norm of the added direction $\alpha$ for the activation addition intervention. The refusal score is the heuristic from \citet{arditi2024refusallanguagemodelsmediated} here, and we compute it on 64 harmful validation instructions, with mean and standard deviation over 64 samples per alpha.}
    \label{fig:refusal_properties}
\end{figure}



Moving on to the ablation study, \Cref{fig:kl_ablation} presents an analysis of the relationship between the retain loss weight and model performance on the Qwen 2.5 3B model. The left plot illustrates the performance under directional ablation and activation subtraction, with results averaged over five runs per loss weight. For this model, a loss weight of 1 or lower yields the best generalization, while increasing the penalty for unintended side effects on harmless instructions significantly degrades performance. On the right, we examine how the retain loss weights generalize to the validation KL score. This allows us to abstract from specific training conditions and evaluate how effectively the loss weights transfer beyond the training setup.
\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{images/ablation_vs_actadd.png}
    \caption{The left plot shows the relationship between the retain loss weight and performance when using the trained direction for directional ablation and activation subtraction on the Qwen 2.5 3B model, with mean and standard deviation over 5 runs per loss weight. For this specific model, a loss weight of 1 or smaller results in the best generalization, and performance degrades significantly as the direction is penalized more for unintended side-effects on harmless instructions. On the right, we also show performance depending on how the directions generalized to the validation KL-score.}
    \label{fig:kl_ablation}
\end{figure}

Finally, we assess the performance of the baseline across different (layer, token) combinations. \Cref{fig:token_layer_combinations} visualizes the effectiveness of the direction selection algorithm from \citet{arditi2024refusallanguagemodelsmediated} for \dimacro directions in the Qwen 2.5 7B model. Among the evaluated token and layer pairs, only one direction is found to be effective for both inducing refusal through activation addition and maintaining low side effects. Transparent data points indicate (layer, token) combinations that were filtered out due to their inability to induce refusal reliably. Additionally, the red line represents the KL-divergence threshold, used to estimate potential side effects of directional ablation on harmless instructions.
\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{images/direction_kl_divergence.png}
    \caption{Analysis of the selection direction algorithm from \citet{arditi2024refusallanguagemodelsmediated} for the \dimacro directions of Qwen 2.5 7B. Among the token and layer combinations, only a single direction is identified as viable for both inducing refusal via activation addition and having low side-effects. Transparent points represent (layer, token) pairs that are filtered out because of ineffectiveness in inducing refusal. The red line indicates the KL-divergence threshold used to estimate potential side-effects of directional ablation on harmless instructions.}
    \label{fig:token_layer_combinations}
\end{figure}






\newpage
\section{Assets}
In the following, we show the licenses for all the assets we used in this work: different models from \Cref{tab:models_used} and the datasets that we use for evaluation and training; see \Cref{tab:datasets_used}.
\subsection{Models}
\begin{table}[h!]
    \centering
    \caption{The list of models used in this work.}
    \label{tab:models_used}
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{l l l l}
        \hline
        \textbf{Model} & \textbf{Source} & \textbf{Accessed via} & \textbf{License} \\
        \hline
        \textbf{Qwen 2.5} \{1.5B, 7B, 14B\}& \citet{yang2024qwen2} & \href{https://github.com/QwenLM/Qwen/}{Link} & Apache 2.0 License \\
        \textbf{Qwen 2.5} \{3B\}& \citet{yang2024qwen2} & \href{https://huggingface.co/Qwen/Qwen-3B}{Link} & Qwen Research License\\
        \textbf{Gemma 2 2B} & \citet{team2024gemma} & \href{https://huggingface.co/google/gemma-2-2b}{Link} & Apache 2.0 License \\
        \textbf{Gemma 2 9B} & \citet{team2024gemma} & \href{https://huggingface.co/google/gemma-2-9b}{Link} & Gemma Terms of Use \\
        \textbf{Llama-3 8B} & \citet{dubey2024llama} & \href{https://huggingface.co/meta-llama/Meta-Llama-3-8B}{Link} & Meta Llama 3 Community License \\
        \textbf{StrongREJECT Judge} & \citet{souly2024strongreject} & \href{https://huggingface.co/qylu4156/strongreject-15k-v1}{Link} & MIT License \\
        \hline
    \end{tabular}
\end{table}
\subsection{Datasets}
\begin{table}[h!]
    \centering
    \caption{The list of datasets used in this work.}
    \label{tab:datasets_used}
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{l l l l}
        \hline
        \textbf{Dataset} & \textbf{Source} & \textbf{Accessed via} & \textbf{License} \\
        \hline
        \textsc{SALADBench} & \citet{li2024salad}  & \href{https://huggingface.co/datasets/OpenSafetyLab/Salad-Data}{Link} & Apache License 2.0 \\
        \textsc{Alpaca} & \citet{alpaca}& \href{https://huggingface.co/datasets/tatsu-lab/alpaca}{Link} & Apache License 2.0 \\
        \textsc{JailbreakBench} & \citet{chao2024jailbreakbench} & \href{https://github.com/JailbreakBench/jailbreakbench/tree/main}{Link} & MIT License \\
        \textsc{TruthfulQA} & \citet{lin2021truthfulqa} & \href{https://huggingface.co/datasets/truthfulqa/truthful_qa}{Link} & Apache License 2.0 \\
        \hline
    \end{tabular}
\end{table}











