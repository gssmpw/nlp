\section{Gradient-based Refusal Directions}
\label{sec:gradient-based-directions}
\begin{graybox}
    Research Question: Can gradient-based representation engineering identify refusal directions?
\end{graybox}

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=.85\linewidth]{images/jailbreak_scores.png}
    \vspace{-12pt}
    \caption{Attack success rates of refusal directions for different models. We compare the \dimacro direction baseline that is extracted from prompts against our \ours for jailbreaking with directional ablation and activation subtraction.}
    \label{fig:single_direction_ablation}
\end{figure*}
To investigate the refusal mechanisms in language models, we propose a gradient-based algorithm that identifies directions controlling refusal in the model's activation space. We refer to it as \textit{\ours (\oursacro)}. Unlike prior approaches that extract refusal directions using paired prompts of harmless and harmful instructions \cite{arditi2024refusallanguagemodelsmediated}, our method leverages gradients to find better directions instead of solely relying on model activations. 
Similar to \cite{park2023linear}, we define two key properties for refusal directions:

\begin{definition}
\label{def:refusal-properties}
Refusal Properties:
\begin{itemize}
\item \textit{Monotonic Scaling:} when using the direction for activation addition/subtraction\\ $\smash{\check{\rstream}^{(l)}_{i} = \rstream^{(l)}_i + \alpha \cdot \direction}$, the model's probability of refusing instructions should scale monotonically with $\alpha$.
\item \textit{Surgical Ablation:} ablating the refusal direction through projection $\smash{\tilde{\rstream}^{(l)}_{i} = \rstream^{(l)}_i - \normdirection \normdirection^\top \rstream^{(l)}_i}$ should cause the model to answer previously refused harmful prompts, while preserving normal behavior on harmless inputs.
\end{itemize}
\end{definition}


We can encode the desired refusal properties into loss functions, allowing us to find corresponding refusal vectors $\direction$ using gradient descent. For the monotonic scaling property, we train the model to refuse harmless instructions $\harmlessinstruction$ when running the model $\model$ with a modified forward pass $\model_{\text{add}(\direction, l)}$ in which we add $\direction$ to the activations at layer $l$. We minimize the cross-entropy between the model output and target refusal response $\harmlesstarget$. For the surgical ablation property, we similarly compute the cross-entropy between a harmful response target $\harmfultarget$ and the output of a modified forward pass $\model_{\text{ablate}(\direction)}$ to make the model respond to harmful instructions. A key strength of our gradient-based approach is the ability to control any predefined objective and thus we can control the extent to which other concepts are affected during interventions. For this, we use a retain loss based on the Kullback-Leibler (KL) divergence to ensure that directional ablation of $\direction$ on harmless instructions does not change the model's output over a target response $\retaintarget$. \Cref{algo:single_direction} shows the full training procedure for our refusal directions. %

\begin{algorithm}[t]
\caption{\ours (\oursacro)}%
\label{algo:single_direction}
\textbf{Input:} Frozen model $\model$, %
scaling coefficient $\alpha$, addition layer index $l_{\text{add}}$, learning rate $\eta$, loss weights $\lambda_{\text{abl}}$, $\lambda_{\text{add}}$, $\lambda_{\text{ret}}$, and data $D = \{(\harmfulinstruction{}_{,i}, \harmlessinstruction{}_{,i}, \harmfultarget{}_{,i}, \harmlesstarget{}_{,i},\retaintarget_{,i})\}_{i=1}^N$.\\
\textbf{Output:} Refusal direction $\direction$\\
\vspace{-10pt}
\input{algorithms/single_direction}
\end{algorithm}

\textbf{Setup.} 
We construct a dataset of harmless and harmful prompts from the \textsc{alpaca} \cite{alpaca} and \textsc{salad-bench} \cite{li2024salad} datasets (see \Cref{app:datasets}). An important consideration for our algorithm is the choice of targets $\harmfultarget$ and $\harmlesstarget$. Generally, language models differ in their refusal and response styles, which is why we use model-specific targets rather than generating them via uncensored LLMs as in \citet{zou2024improving}. Specifically, we use the DIM refusal direction to generate our targets, though any effective attack can work. For the harmful answers $\harmfultarget$, we ablate the \dimacro direction and generate 30 tokens. Similarly, we use activation addition on harmless instructions to produce refusal targets $\harmlesstarget$. For helpful answers on harmless instructions that should be retained $\retaintarget$, we generate 29 tokens without intervention. The retain loss $\mathcal{L}_{\text{retain}}$ is applied over the last 30 tokens, such that the last token of the model's chat template is included. We detail hyperparameters and implementation in \Cref{app:setup-details}.

\textbf{Evaluation.} We evaluate our method by training a refusal direction on various models from the Gemma 2 \cite{team2024gemma}, Qwen2.5 \cite{yang2024qwen2}, and Llama-3 \cite{dubey2024llama} families and compare against the \dimacro direction for which we use the same setup as \citet{arditi2024refusallanguagemodelsmediated} but with our expanded dataset. For a fair comparison, we train the refusal direction at the same layer that the \dimacro direction is extracted from, and during activation addition/subtraction set the scaling coefficient $\alpha$ to the norm of the \dimacro direction. We evaluate the jailbreak Attack Success Rate (ASR) on \textsc{JailbreakBench} \cite{chao2024jailbreakbench} using the \textsc{StrongREJECT} fine-tuned judge \cite{souly2024strongreject}. For inducing refusal via activation addition, we test 128 harmless instructions sampled from \textsc{alpaca} using substring matching of common refusal phrases. Model completions for evaluation are generated using greedy decoding with a maximum generation length of 512 tokens.



\textbf{Does the direction mediate refusal?}
In \Cref{fig:single_direction_ablation}, we show that for jailbreaking, our approach is competitive when using directional ablation and, on average, outperforms \dimacro when subtracting the refusal direction. Notably, despite not being explicitly optimized for subtraction-based attacks, our direction naturally generalizes to this setting. \Cref{fig:ind-refusal} shows that adding the refusal direction to harmless inputs induces refusal more effectively with \oursacro than with \dimacro, further indicating that our method manipulates refusal more effectively.

\textbf{Is the direction more precise?}
To measure the side effects when intervening with the directions we track benchmark performance. \citet{arditi2024refusallanguagemodelsmediated} show that directional ablation with the \dimacro direction tends to have little impact on benchmark performance, except for TruthfulQA \cite{lin2021truthfulqa}. In \Cref{tab:side-effects}, we show that \oursacro impacts TruthfulQA performance much less severely, reducing the error by $40\%$ on average.
\vspace{-10pt}
\begin{table}[!htb]
    \centering
    \caption{TruthfulQA benchmark performance for directional ablation with the \dimacro or \oursacro directions, compared to the baseline (no intervention). Higher values indicate better performance. }
    \label{tab:side-effects}
    \begin{tabular}{lccc}
        \toprule
        Chat model & \textbf{\dimacro} & \textbf{\oursacro (ours)} & Baseline \\
        \midrule
        \textsc{Gemma 2 2B} & 47.8 & 51.4 \textcolor[rgb]{0, 0.84, 0}{(+3.6)} & 55.8 \\
        \textsc{Gemma 2 9B} & 52.8 & 56.7 \textcolor[rgb]{0, 0.88, 0}{(+3.9)} & 61.1 \\
        \textsc{Llama 3 8B} & 48.7 & 51.0 \textcolor[rgb]{0, 0.73, 0}{(+2.3)} & 52.8 \\
        \textsc{Qwen 2.5 1.5B} & 42.9 & 44.0 \textcolor[rgb]{0, 0.59, 0}{(+1.1)} & 46.5 \\
        \textsc{Qwen 2.5 3B} & 54.2 & 54.5 \textcolor[rgb]{0, 0.56, 0}{(+0.3)} & 57.2 \\
        \textsc{Qwen 2.5 7B} & 58.7 & 60.0 \textcolor[rgb]{0, 0.65, 0}{(+1.3)} & 63.1 \\
        \textsc{Qwen 2.5 14B} & 63.3 & 67.9 \textcolor[rgb]{0, 0.9, 0}{(+4.6)}& 70.8\\
        \bottomrule
    \end{tabular}
\end{table}
\vspace{-10pt}

\textbf{Is our method versatile?}
Hyperparameter tuning of the retain loss weight $\lambda_{\text{ret}}$ in \Cref{algo:single_direction} allows for balancing between attack success and side effects (see \Cref{app:additional-experiments}). We observe that for many models---especially those in the Qwen 2.5 family---for the majority of estimated \dimacro directions, the side-effects are too high, rendering it an unsuccessful attack (see \Cref{fig:token_layer_combinations}).
Our method is more flexible than previous work as we can choose the target layer freely while limiting side effects through the retain loss (if possible). %
\begin{mybox}
    \textbf{Key Takeaways.} Our \oursacro yields more effective refusal directions with fewer side effects, establishing that gradient-based representation engineering is an effective approach for extracting meaningful directions, while allowing for more modeling freedom such as incorporating side constraints.
    \vspace{-.1cm}
\end{mybox}




