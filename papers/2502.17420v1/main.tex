
\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} %
\usepackage{xspace} %

\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[preprint]{conference}


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


\newcommand{\tom}[1]{{\color{blue}{tom: #1}}}
\newcommand{\jannes}[1]{{\color{purple}{jannes: #1}}}

\usepackage[textsize=tiny]{todonotes}

\usepackage{amsmath}
\usepackage{bm}
\usepackage{paralist}

\input{math_commands}
\input{definitions}
\usepackage{wrapfig}

\usepackage{graphbox}
\usepackage{tcolorbox}

\definecolor{lightblue}{HTML}{84C7F9}
\definecolor{lighterblue}{HTML}{D4ECFF}
\newtcolorbox{mybox}{colback=lighterblue,colframe=lightblue}
\definecolor{lightgray}{HTML}{D3D3D3}
\definecolor{lightergray}{HTML}{EAEAEA}
\newtcolorbox{graybox}{colback=lightergray, colframe=lightgray}



\icmltitlerunning{The Geometry of Refusal in Large Language Models}

\begin{document}

\twocolumn[
\icmltitle{The Geometry of Refusal in Large Language Models:\\ Concept Cones and Representational Independence}



\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{\quad \quad}{}
\icmlauthor{Tom Wollschl\"ager}{equal,tum}
\icmlauthor{Jannes Elstner}{equal,tum}
\icmlauthor{Simon Geisler}{tum}
\icmlauthor{Vincent Cohen-Addad}{google}
\icmlauthor{\quad \quad}{}
\icmlauthor{Stephan G\"unnemann}{tum}
\icmlauthor{Johannes Gasteiger}{google,anthropic}
\end{icmlauthorlist}

\icmlaffiliation{tum}{School of Computation, Information \& Technology and Munich Data Science Institute, Technical University of Munich}
\icmlaffiliation{google}{Google Research}
\icmlaffiliation{anthropic}{Now at Anthropic}

\icmlcorrespondingauthor{Tom Wollschl\"ager}{tom.wollschlaeger@tum.de}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]



\printAffiliationsAndNotice{\icmlEqualContribution} %

\begin{abstract}
The safety alignment of %
large language models (LLMs) can be circumvented through adversarially crafted inputs, yet the mechanisms by which these attacks bypass safety barriers remain poorly understood. Prior work suggests that a \emph{single} refusal direction in the model's activation space determines whether an LLM refuses a request. In this study, we propose a novel gradient-based approach to representation engineering and use it to identify refusal directions. Contrary to prior work, we uncover multiple independent directions and even multi-dimensional \emph{concept cones} that mediate refusal. Moreover, we show that orthogonality alone does not imply independence under intervention, motivating the notion of \smash{\emph{representational independence}} that accounts for both linear and non-linear effects. Using this framework, we identify mechanistically independent refusal directions. 
We show that refusal mechanisms in LLMs are governed by complex spatial structures and identify functionally independent directions, confirming that multiple distinct mechanisms drive refusal behavior.
Our gradient-based approach uncovers these mechanisms and can further serve as a foundation for future work on understanding LLMs.\textsuperscript{1} %
\end{abstract}

\input{contents/introduction}
\input{contents/background}
\input{contents/related_work}
\input{contents/gradient_based_directions}
\input{contents/refusal_subspaces}
\input{contents/mechanistic_uncerstanding_of_directions}
\input{contents/limitations}
\input{contents/conclusion}


\section*{Acknowledgements}
This project was conducted in collaboration with and supported by funding from Google Research. We thank Dominik Fuchsgruber and Leo Schwinn for feedback on an early version of the manuscript.


\section*{Impact Statement}
Understanding how refusal mechanisms in language models work could potentially aid adversaries in developing more effective attacks. However, our research aims to deepen the understanding of refusal mechanisms to help the community develop more robust and reliable safety systems. By focusing on open-source models requiring white-box access, our findings are primarily applicable to improving defensive capabilities rather than compromising deployed systems. We believe the positive impact of advancing model alignment and safety through better theoretical understanding outweighs the potential risks, making this research valuable to share with the research community.

\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{conference}


\newpage
\appendix
\onecolumn
\input{contents/appendix}


\end{document}
