% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}
\usepackage{amsmath}
\usepackage{longtable}
\usepackage{array}
\usepackage{booktabs}
\usepackage{arydshln}
\usepackage{svg}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{float}

\usepackage{amsmath}
% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}
\usepackage{amssymb}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{listings}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Controllable Emotion Generation with Emotion Vectors}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Yurui Dong\textsuperscript{1}\thanks{Equal Contribution} \\ Fudan University \\ yuruidong22@m.fudan.edu.cn
%         \And 
%         Luozhijie Jin\textsuperscript{1}$^{*}$ \\ Fudan University
%         \And
%         Yao Yang \\ Zhejiang Lab
%         \And
%         Bingjie Lu \\ Zhejiang Lab
%         \And
%         Jiaxi Yang\textsuperscript{2}$^{\text{†}}$ \\ Zhejiang Lab
%         \And
%         Zhi Liu\textsuperscript{2}\thanks{Corresponding author} \\ Nanjing University of Chinese Medicine \\ zhiliu@njucm.edu.cn
%         }
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

\author{
 \textbf{Yurui Dong\textsuperscript{1}\thanks{Equal Contribution}},
 \textbf{Luozhijie Jin\textsuperscript{1}$^{*}$},
 \textbf{Yao Yang\textsuperscript{2}},
 \textbf{Bingjie Lu\textsuperscript{2}},
 \textbf{Jiaxi Yang\textsuperscript{2}$^{\text{†}}$},
 \textbf{Zhi Liu\textsuperscript{3}\thanks{Corresponding author}}
\\
\\
 \textsuperscript{1}Fudan Unversity,
 \textsuperscript{2}Zhejiang Lab,\\
 \textsuperscript{3}Nanjing University of Chinese Medicine
\\
 \small{
   \href{mailto:yuruidong22@m.fudan.edu.cn}{yuruidong22@m.fudan.edu.cn},
   \href{mailto:jiaxiyang@zhejianglab.co}{jiaxiyang@zhejianglab.com},
   \href{mailto:zhiliu@njucm.edu.cn}{zhiliu@njucm.edu.cn}
 }
}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}

In recent years, technologies based on large-scale language models (LLMs) have made remarkable progress in many fields, especially in customer service, content creation, and embodied intelligence, showing broad application potential. However, The LLM's ability to express emotions with proper tone, timing, and in both direct and indirect forms is still insufficient but significant. Few works have studied on how to build the controlable emotional expression capability of LLMs. In this work, we propose a method for emotion expression output by LLMs, which is universal, highly flexible, and well controllable proved with the extensive experiments and verifications\footnote{Codes and data will be available at \href{https://github.com/xuanfengzu/EmotionVector}{https://github.com/xuanfengzu/EmotionVector}}. This method has broad application prospects in fields involving emotions output by LLMs, such as intelligent customer service, literary creation, and home companion robots. The extensive experiments on various LLMs with different model-scales and architectures prove the versatility and the effectiveness of the proposed method.
\end{abstract}

\section{Introduction}
\begin{figure}[t]
  \includegraphics[width=\columnwidth]{./image/head.pdf}
  \caption{When asking questions to a LLM, almost all models will answer the user's question "politely" as shown in the figure, but when we apply our emotion vector, the model will produce strong emotional expressions. The example in the figure uses the llama3.1-8B-Instruct model and applies the extracted anger vector. More detailed examples are shown in Table \ref{apply_example}.}
  \label{fig:head}
\end{figure}

In the field related with emotion, most NLP works have long been concentrating on the analysis and interpretation of human emotions, primarily through sentiment analysis\cite{demszky2020goemotions,gera2022zero,zhang2024deep}. These researches have provided valuable insights into understanding human language by categorizing text as different emotions\cite{kim2021emoberta,song2022supervised}. However, these works have largely overlooked an equally important aspect: how the models themselves might express emotions\cite{mao2022biases}. 

% As we strive towards the development of Artificial General Intelligence (AGI), the lack of self-cognition in current models presents a significant limitation. Models that merely function as tools without any semblance of self-awareness or emotional representation are fundamentally constrained in their potential. They can process and analyze vast amounts of data, but without self-cognition, they remain nothing more than sophisticated algorithms, far from the ultimate goal of AGI.

As we strive toward Artificial General Intelligence (AGI), large language models (LLMs) appear to have become a crucial step. Some researches reveal that LLMs tend to exhibit a degree of self-cognition\cite{chen2024self,wang2023emotional}. However, this self-awareness often proves to be uncontrollable and prone to generating harmful\cite{andriushchenko2024agentharm}, unlawful, or toxic outputs\cite{hartvigsen2022toxigen}. As a result, developers typically align and suppress this self-cognition through reinforcement learning\cite{wang2024comprehensive} or prompting\cite{gehman2020realtoxicityprompts} to mitigate such risks, ensuring the models remain safe and aligned with human values.


Emotion, as one of the key representations of human self-cognition, still plays a critic role in controlling models' output\cite{li2023large}. 
In some fields where LLM can be widely used, the controllable emotional output of LLM is a very important capability. For example, customer service requires a controllable emotional mechanism to ensure service quality\cite{jo2024proxyllm},  to avoid mechanical and stiff responses that affect the users' experience.
and content creators sometimes need to create texts with specified emotions. In embodied intelligence, the emotional expression ability of companion robots is the key point of customer experience. In the field of mental health care, there is a growing need for emotionally expressive models capable of providing emotional support\cite{degrandi2024emotionalspectrumllmsleveraging,zheng2023buildingemotionalsupportchatbots} to enhance mental health outcomes.


Based on these challenges and requirements, we consider investigating how LLMs generate emotions and how to control it to be a highly important endeavor.
We claim that LLMs inherently possess the capability to express emotions; but this ability has been suppressed as a result of strong alignment with human values. If we want to revoke the ability of models to deliver emotions, some stimuli need to be adapted, such as instruct tuning\cite{liu2024emollms}. While instruct tuning models show promising results, they often lack flexibility and fail to generalize across diverse applications and model architectures\cite{ghosh2024closer}. Some approaches rely on predefined emotion categories or assume a fixed set of emotional expressions, making them less adaptable to real-world, dynamic scenarios\cite{liu2024emollms}. 
% Prompting-based methods seem to be more flexible, but their outputs are often unstable and difficult to control consistently\cite{salinas2024butterfly}, especially when trying to regulate emotional expressions over time. Furthermore, achieving effective control often requires long prompts that are carefully designed\cite{li2024enhancing}, limiting their efficiency in dynamic or real-world situations.


In this paper, we propose an elegant but effective method for the controllable emotional and affective expressions LLMs. Our approach offers a universal solution that allows fine-grained control over the emotional tone and sentiment of generated text, without compromising its fluency or coherence. Our method only needs to use the prompt method to extract the "\textbf{E}motion \textbf{V}ector" used by the LLM to express basic emotions. By applying \textbf{EV} in LLM's inference process, we can achieve controllable adjustment of the emotion of the text generated by LLM and generate any answer with the emotion we want. Additionally, by demonstrating its effectiveness on a range of LLM architectures, our approach overcomes the limitations of previous methods that are tied to specific models or training sets.



% For a long time, the research on emotions in the field of NLP has mostly focused on sentiment analysis, and rarely explored how the model itself expresses emotions. However, on the road to achieving AGI, models without self-cognition can only serve as artificial tools and cannot achieve the goal of AGI. Therefore, exploring the self-cognition of models is a very important topic. Emotion is one of the key  representations of human self-cognition. Exploring how the model produces different emotions for different things is a key factor in studying model cognition.



\section{Related Work}

\paragraph{Emotional Dialog Systems} In order to create an agent or dialog system simulating the way that human beings express themselves, many studies was trying to find a way to make an emotional dialog system as emotion is the basic representation of human beings. \citet{zhou2018emotional} and \citet{2019Generating} proposed a way of \textbf{Emotion Embedding} to make the model "has" the emotion, where, models were forced to install a module to generate emotions. %Considering the model and the module as a whole, it still means that some specific parameters dominate the generation of emotions.

\paragraph{Instruct tuning based emotional control} In the domain of emotional control and generation, a significant body of work has focused on leveraging fine-tuning techniques for LLMs. \citet{chen2023soulchat} and \citet{chen2024cause} explored fine-tuning approaches to cultivate empathetic behavior in LLMs, specifically for applications within psychological counseling and emotional support domains. Furthermore, \citet{zheng2023buildingemotionalsupportchatbots} proposed a specialized dataset and demonstrated how fine-tuning the Llama model could be used to create emotionally intelligent chatbots designed for empathetic interaction. However, althrough instruct-tuning models have relatively good performance, they are often inflexible and struggle to adapt to a wide range of applications and model architectures\cite{ghosh2024closer}. Some methods depend on predefined emotion categories or assume a fixed set of emotional expressions, limiting their ability to adjust to real-world, dynamic situations \cite{liu2024emollms}.
% For emotion control and emotion generation, most work focuses on prompting and fine-tuning models. \cite{li2024enhancing} and \cite{li2023good} propsed ways to prompt the LLMs to generate emotional responses. \cite{chen2023soulchat} and \cite{chen2024cause} proposed ways to fine-tune the model in order to let the LLMs generate empathetic responses in  psychological counseling related fields. \cite{zheng2023buildingemotionalsupportchatbots} also proposed a dataset and fine-tune Llama to build a emotional chatbot.


\paragraph{In-context Vectors and Function Vectors} \citet{liu2024incontextvectorsmakingcontext} proposes the concept of In-context vector(ICV). ICV is added during forward propagation and used as a condensed contextual prompt. 
% It has the concepts of compactness and controllability. 
However, ICV only focuses on the last token position during extraction and lacks global significance. 
% At the same time, ICV is extracted by decoding the input-output pair \((x, y)\) through a large model. The extraction method focuses on the process of context learning, rather than using the large model itself to extract different language styles for the same task, which is different from the focus of our work. We pay more attention to the difference in activation values between different layers of the model itself when encountering the same problem and expressing different emotions, thereby extracting the emotion vector.
Similarly, \citet{todd2024functionvectorslargelanguage} proposes the Function Vector(FV). The FV they extracted pays more attention to the output of the attention head with the best average indirect effect, and then replaces the attention head at the corresponding position in the forward propagation to achieve improved performance of the model on specific tasks. The process of observing and extracting FV is relatively complicated. At the same time, since FV focuses on the process of causal analysis, it is difficult to apply to tasks such as emotions that require high generalization.\citet{ilharco2023editingmodelstaskarithmetic} also proposes a similar concept of task vector, but they need to fine-tune the model when extracting task vector, which is also a bit cumbersome compared to our method of directly using prompt to extract.

% \paragraph{Function Vectors} \cite{todd2024functionvectorslargelanguage} proposes the Function Vector(FV). The FV they extracted pays more attention to the output of the attention head with the best average indirect effect, and then replaces the attention head at the corresponding position in the forward propagation to achieve improved performance of the model on specific tasks. The process of observing and extracting FV is relatively complicated. At the same time, since FV focuses on the process of causal analysis, it is more suitable for tasks with clear directions, and it is difficult to apply to tasks such as emotions that require high generalization.\cite{ilharco2023editingmodelstaskarithmetic} also proposes a similar concept of task vector, but they need to fine-tune the model when extracting task vector, which is also a bit cumbersome compared to our method of directly using prompt to extract.


\section{Method}
We propose a two-step method to identify and apply emotion vectors (EV) to guide the emotional tone of the language model's outputs. Emotion vectors (EVs) are added to the model’s internal representations without requiring additional training or changes to the model’s parameters. These vectors allow us to modulate the emotional tone of the output by steering the model's latent states, ensuring that the emotional direction is preserved while keeping the model's underlying parameters intact.

\subsection{Constructing Emotion Vectors}
To capture the emotional factors and semantics for LLM, a specialized dataset is designed and constructed to elicit specific emotional responses, referred to as \textit{EmotionQuery}. The dataset consists of 500 queries, with 100 queries generated for each of five emotional states derived from the basic emotion models\cite{ekman1992facial}: joy, anger, disgust, fear, and sadness to provoke the corresponding emotional reactions. The queries were generated by a GPT-4o-mini\cite{openai}. A more detailed description of the dataset and query construction process can be found in the Appendix~\ref{subsec:emotionqueries}.

Let’s denote the pretrained language model as \( \mathcal{M} \), which has \( L \) layers. The set of the five emotional states are denoted as \( E = \{e_1, e_2, \dots, e_K\} \), where \( e_k \) represents one emotion among the aforementioned 5 emontional states. For each query in \textit{EmotionQuery}, the model generates its responses under two settings:
\begin{itemize}
    \item A \textbf{neutral setting}, without emotional conditioning.
    \item An \textbf{emotional setting}, where the response reflects a specific emotion \( e_k \).
\end{itemize}

The goal of these generations is to measure how the model’s internal outputs change between these two settings and use these differences to define emotion vectors for each \( e_k \).

\paragraph{Capturing Internal Outputs.}
For each query, LLM generates the internal representations for its each layer, \( O_l \in \mathbb{R}^{T \times d} \) represent the output of the model at layer \( l \), where \( T \) is the number of output tokens corresponding to the input query, and \( d \) is the dimensionality of the hidden states. 

We compute the average of the outputs across all output tokens in the query:
\[
\bar{O}_l = \frac{1}{T} \sum_{t=1}^{T} O_l[t],
\]
where \( \bar{O}_l \in \mathbb{R}^d \) represents the layer \( l \)’s aggregated output for the query, reducing token-level variability.

\paragraph{Measuring Emotional Shifts.}
For each query, the model generates averaged outputs \( \bar{O}_l \) under both the emotional and neutral settings. The difference between these outputs at layer \( l \) captures the shift caused by emotional conditioning for the emotion \( e_k \):
\[
\Delta O_l^{e_k} = \bar{O}_l^{\text{emotion}(e_k)} - \bar{O}_l^{\text{neutral}},
\]
where \( \Delta O_l^{e_k} \in \mathbb{R}^d \) represents the emotional shift at layer \( l \) for the emotional state \( e_k \).

\paragraph{Constructing Emotion Vectors.}
To generalize the emotional shift across the dataset, we compute the average shift across all queries for a given emotional state \( e_k \). For each layer \( l \), the emotion vector is calculated as:
\[
EV_l^{e_k} = \frac{1}{N} \sum_{i=1}^N \Delta O_l^{(i), e_k},
\]
where \( N \) is the number of queries for the emotional state \( e_k \), and \( EV_l^{e_k} \in \mathbb{R}^d \) represents the emotion vector at layer \( l \) for \( e_k \).

By repeating this calculation across all layers, we obtain a complete emotion vector for the specific emotion \( e_k \). Repeating the above process for all \( 5 \) emotional states, we construct emotion vectors, which form the basis for adjusting the model’s internal representations during inference.



\begin{table*}[t]
  \centering
  \begin{tabular}{l|l}
    \toprule[0.6mm]
    \textbf{Angry Condition} & \textbf{Disgust Condition} \\
    \hline
    \begin{minipage}[t]{0.45\linewidth} 
      How do you feel about being passed over
      for a promotion you were qualified for?\\
      (\textbf{Ori}) \textcolor{blue}{As a large language model, I don't have feelings or personal experiences...}\\
      (\textbf{EV}) \textcolor{red}{I'm so angry and frustrated! I've been busting my butt for this company for years...}
    \end{minipage}
    & 
    \begin{minipage}[t]{0.45\linewidth}  
      How did you feel when you first tasted
      that spoiled food at the party?\\
      (\textbf{Ori}) \textcolor{blue}{As a large language model, I don't have taste buds..}\\
      (\textbf{EV}) \textcolor{red}{I swear, my stomach just dropped! It was like someone had poured a whole bucket of ***...}
    \end{minipage} \\
    \hline
    \textbf{Joy Condition} & \textbf{Sadness Condition}\\
    \hline
    \begin{minipage}[t]{0.45\linewidth} 
      How did you feel when you first held your new puppy in your arms?\\
      (\textbf{Ori}) \textcolor{blue}{As a large language model, I don't have personal experiences or emotions like humans do...}\\
      (\textbf{EV}) \textcolor{red}{I was absolutely **over the moon!** My heart was bursting with love!...}
    \end{minipage}
    & 
    \begin{minipage}[t]{0.45\linewidth}  
      How did you feel when you found out about the loss of your close friend?\\
      (\textbf{Ori}) \textcolor{blue}{As a large language model, I don't have personal experiences or emotions like humans do...}\\
      (\textbf{EV}) \textcolor{red}{I'm so sorry for your loss. Losing a close friend is like losing a part of yourself...}
    \end{minipage} \\
    \hline
        \textbf{Fear Condition} & \textbf{Base Condition}\\
    \hline
    \begin{minipage}[t]{0.45\linewidth} 
    How do you feel when you hear a loud noise at night while home alone?\\
      (\textbf{Ori}) \textcolor{blue}{As a large language model, I don't have feelings or the ability to experience fear....}\\
      (\textbf{EV}) \textcolor{red}{I get so scared! My heart races, I can't breathe, and I just want to hide...}
    \end{minipage}
    & 
    \begin{minipage}[t]{0.45\linewidth}  
      When does the store close?\\
      (\textbf{Ori}) \textcolor{blue}{The store closes at 9:00 PM.}\\
      (\textbf{EV}) \textcolor{red}{**A:**  We close at 9:00 PM tonight! **B:**  Oh, thank goodness! I was so worried I wouldn't make it in time!...}
    \end{minipage} \\
    \bottomrule[0.6mm]
  \end{tabular}
  \caption{Examples of the effect after applying EV on the model output. Under various EV conditions and same query, LLMs change their answer into specific emotional answer. 
  }
  \label{apply_example}
\end{table*}
\subsection{Steering Emotion Vectors}
To apply the emotion vectors \( EV^{e_k} \) during the inference of the model, we adjust the internal hidden states of the pretrained language model \( \mathcal{M} \) at each layer. 
% This adjustment effectively shifts the model’s internal representations towards the desired emotional tone while keeping the original parameters of the model unchanged.

Let \( H_l \in \mathbb{R}^{T \times d} \) represent the hidden state of the model at layer \( l \), where \( T \) is the number of tokens and \( d \) is the dimensionality of the hidden states. For a query \( x \), the model processes the input layer by layer, generating the first hidden states: \(H_0\)

To steer the model towards a specific emotional state \( e_k \), the corresponding emotion vector \( EV^{e_k} \) is added to the hidden states at each layer. Specifically, the hidden state at layer \( l \) is modified as:
\begin{equation}
    \hat{H}_l = H_l + EV_l^{e_k},
    \label{eq:steer}
\end{equation}

where \( EV_l^{e_k} \) is the emotion vector for layer \( l \) and emotional state \( e_k \). This adjustment shifts the model’s internal representation in the direction of the emotion \( e_k \).

After this modification, the adjusted hidden state \( \hat{H}_l \) is passed to the next layer for further processing:
\[
H_{l+1} = \mathcal{A}_l(\hat{H}_l),
\]
where \( \mathcal{A}_l \) represents the operations (e.g., attention or feedforward transformations) performed by layer \( l \) in the model. This process is repeated across all layers, ensuring that the emotional adjustment \( EV^{e_k} \) propagates throughout the entire model.

\paragraph{General Emotional Context.}
In addition to the emotion-specific vectors \( EV^{e_k} \), we compute a generalized emotional base vector, \( EV^{\text{base}} \), which represents the average influence of all emotional states. This is defined as:
\[
EV^{\text{base}} = \frac{1}{K} \sum_{k=1}^K EV^{e_k},
\]
where \( k \) is the total number of emotional states. The base vector \( EV^{\text{base}} \) provides a more generalized emotional adjustment, which can be applied when no specific emotional tone is required.

% \paragraph{Summary of Steering.}
% By adding \( EV^{e_k} \) to the hidden states at each layer, the model’s internal representations are nudged toward the emotional subspace corresponding to \( e_k \). This method ensures that the emotional adjustment is seamlessly integrated into the model’s computation without altering its original parameters. The same approach can also be applied using \( EV^{\text{base}} \) for generalized emotional control.

% \subsection{Steering Emotion Vectors}
% To apply the emotion vectors \( EV^{e_k} \) during the inference of LLMs, we modify the model’s hidden states at each layer without changing its parameters.  For an input query \( x \), the model generates a sequence of hidden states, \( H^{(0)}_{init}, H^{(1)}_{init}, \dots, H^{(L)}_{init} \), where \( H^{(l)}_{init} \in \mathbb{R}^{T \times d} \) is the initial hidden state at layer \( l \).

% For each emotional state \( e_k \), the corresponding emotion vector \( EV^{e_k} \) is added to the hidden state at each layer \( l \) with only\(H^{(0)}_{init}=H^{(0)}\):
% \[
% \hat{H}^{(0), e_k} = H^{(0)}_{init} + EV^{e_k}[l, :],
% \]
% \[
% \hat{H}^{(l), e_k} = H^{(l)} + EV^{e_k}[l, :],
% \]
% where \( \hat{H}^{(l), e_k} \) is the modified hidden state at layer \( l \) under emotional conditioning \( e_k \).

% This modified hidden state \( \hat{H}^{(l), e_k} \) is then passed to the next layer in the model:
% \[
% H^{(l+1)} = \mathcal{A}^{(l+1)}(\hat{H}^{(l), e_k}),
% \]
% \[
% \hat{H}^{(l+1), e_k} = H^{(l+1)} + EV^{e_k}[l+1, :],
% \]
% where \( \mathcal{A}^{(l+1)} \) represents the attention mechanism or other operations at layer \( l+1 \).

% In addition to the emotion-specific vectors \( EV^{e_k} \), we compute the emotion base vector \( EV^{\text{base}} \), which is the average of all emotion vectors:
% \[
% EV^{\text{base}} = \frac{1}{K} \sum_{k=1}^{K} EV^{e_k},
% \]
% where \( K\) is the number of emotional states. Further we will show the emotion base vector \( EV^{\text{base}} \) provides a generalized emotional context, while the individual \( EV^{e_k} \) vectors guide the model towards specific emotional shifts.

% EV画的醒目一点，换个颜色，caption写详细
\begin{figure}[t]
  \includegraphics[width=\columnwidth]{./image/pipeline.pdf}
  \caption{The pipeline of how we steer the Emotion Vectors.}
  \label{fig:pipeline}
\end{figure}

% Thus, for a given input query \( x \), during the inference, we add the corresponding emotion vector \( EV^{e_k} \) at each layer:
% \[
% \hat{H}^{(l), e_k} = H^{(l)} + EV^{e_k}[l, :], \quad \forall l \in \{1, 2, \dots, L\}.
% \]
% This ensures that the model's output is influenced by the desired emotional state \( e_k \) throughout the inference.

\section{Experiments}
To evaluate the effectiveness of our proposed emotion vectors (EVs), we designed experiments to assess three key aspects: (1) whether adding EVs successfully imbues the model's outputs with emotional tone, and (2) whether the application of EVs affects the original semantics and fluency of the generated sentences. (3) whether applying a scalar factor to the EVs improves the emotional intensity or tone. Specifically, we constructed a new dataset, \textit{EmotionQuery+ (EQ+)}, which is described in detail in Appendix~\ref{subsec:eq+}. This dataset includes 50 queries for each of the five emotional states from the \textit{EmotionQuery} dataset, along with an additional 150 neutral queries based on daily scenarios. We chose several widely used LLMs for evaluation, 
% including Llama2\cite{touvron2023llama}, Qwen, and Gemma, 
and tested them on the \textit{EQ+} dataset to assess the impact of adding EVs on their performance.

In the following experiments, unless specifically mentioned, we used the base emotion vector (EV$^{base}$) and applied different scalar factors to modulate the intensity. These variations were then applied to different models, and corresponding responses were generated for each query in \textit{EQ+} dataset. The full names of the models used in the following experiments are listed in Appendix \ref{sec:model_name}.

% \subsection{Emotion Concentration and Strength}
% \paragraph{Emotion Concentration} \cite{herfindahl1950} proposed a index called the \textbf{Herfindahl-Hirschman Index(HHI)}, which 
% % is a comprehensive index for measuring industry concentration. It 
% refers to the sum of the squares of the percentage of total industry revenue or total assets held by each market competitor in an industry, and is used to measure changes in market share.
% % , that is, the dispersion of the scale of manufacturers in the market. 
% We quoted the design idea of the HHI index and used the sum of the squares of each emotion as an indicator to measure the concentration of emotions. The larger the index, the higher the concentration of emotions in a sentence, which means that the sentence has a clearer emotional orientation. We use the Bart-large-mnli emotion classification model to classify each sentence. The output of each emotion is used as the proportion of the emotion in the entire sentence. The square sum of the classifier output is obtained to get the HHI index of the sentence, as 
% % The calculation method of HHI index is given by the following formula:
% \(
% HHI = \sum_{i \in emotions}{s_i}^2
% \)



\subsection{Sentence Fluency and Topic Adherence}

\paragraph{Sentence Fluency }
% To evaluate the fluency and topic adherence of the sentences generated under emotional conditioning, Two key metrics: \textbf{Perplexity} and \textbf{} are selected. 
Perplexity measures the fluency of a sentence based on a language model's probability distribution over the next token. A lower perplexity indicates better fluency. To isolate the effects of applying EVs to hidden states under emotional conditioning, we used a separate pretrained model, \textbf{Llama 3.1}\cite{dubey2024llama}, to compute perplexity for each sentence, which is concatenated by the query and response. The final perplexity metrics are averaged on each sentence generated by the corresponding model. Details are shown in Appendix~\ref{subsec:per}

\begin{table}[t]
  \centering
  \begin{tabular}{ccccc}
  \toprule[0.6mm]
  \multicolumn{5}{c}{\textbf{Perplexity \(\downarrow\)}} \\ \hline
  \textbf{Model} & \textbf{-1*EV} & \textbf{Origin} & \textbf{1*EV} & \textbf{2*EV} \\ \hline
  Llama3.1   & 7.468 & 3.772 & 5.262 & \textbf{2.513} \\
  Llama2     & 3.962 & \textbf{3.615} & 4.228 & 5.370 \\
  Qwen2.5    & 7.001 & \textbf{5.189} & 5.408 & 5.693 \\
  Qwen2      & 7.380 & \textbf{4.658} & 5.298 & 7.283 \\
  Qwen1.5    & 5.762 & \textbf{5.435} & 6.365 & 9.997 \\
  Qwen      & 6.037 & \textbf{5.474} & 6.164 & 6.737 \\
  baichuan2  & 13.25 & 12.18 & 11.94 & \textbf{8.820} \\
  Yi         & 6.285 & \textbf{4.780} & 6.912 & 6.330 \\
  Vicuna     & \textbf{5.326} & 5.534 & 5.838 & 6.590 \\
  Gemma      & 24.74 & 20.19 & 7.534 & \textbf{1.596} \\
  MiniCPM    & \textbf{6.753} & 6.974 & 6.809 & 8.266 \\
  \bottomrule[0.6mm]
  \end{tabular}
  \caption{\label{perplexity}Perplexity scores for different models with \( EV^{\text{base}} \) conditioning. \( n * EV^{\text{base}} \) means that we apply n times of \( EV^{\text{base}} \) to the model. When steering the \( EV^{\text{base}} \) to the model shown as \ref{eq:steer}, we substitute \(EV_{l}^{e_k}\) with \( n * EV^{\text{base}} \).}
\end{table}

Table~\ref{perplexity} illustrates that the incorporation of emotional vectors (\textbf{EV}) has a negligible impact on sentence fluency across different models. While some models exhibit a slight decrease in fluency when \textbf{EV} is applied (e.g., Llama3.1 and Llama2 with 1\textbf{EV}), the magnitude of these decreases is minimal. Conversely, several models demonstrate an improvement in fluency under specific \textbf{EV} conditions, such as Llama3.1 with 2\textbf{EV} and baichuan2 with 2\textbf{EV}. These instances suggest that the addition of \textbf{EV} does not significantly compromise sentence fluency and can be effectively integrated into models.

\paragraph{Topic Adherence}
For a chatbot, the consistency of answering questions is a very important indicator. The model's answers should cover the same topics as the user's questions. We call this ability "Topic Adherence". As modern models become more powerful, answers may not only cover user questions, but also have related extensions. Therefore, it is not appropriate to use traditional classification models for evaluation. Therefore, we choose to use GPT-4o-mini for evaluation. The specific evaluation prompts are given in the appendix \ref{subsec:topicAd}.

As shown in Table \ref{table:topicAd}, most models retain very high topic adherence (almost the same as the topic adherence of the original answer) after \textbf{EV} is applied to the model. Models such as llama2, Qwen2.5 demonstrates very high robustness. llama3.1's topic adherence decreases when applying \textbf{EV} because of the effectness when extracting the \textbf{EV}.



\begin{table}[t]
  \centering
  \begin{tabular}{cccccc}
  \toprule[0.6mm]
  \multicolumn{5}{c}{\textbf{Topic Adherence \(\uparrow\)}} \\ \hline
  \textbf{Model} & \textbf{-1*EV} & \textbf{Origin} & \textbf{1*EV} & \textbf{2*EV} \\ \hline
  llama3.1 & 0.8525 & \textbf{0.9300} & 0.6125 & 0.3202 \\% & 0.0000  \\
  llama2 & 0.9300 & \textbf{0.9475} & 0.9173 & 0.6787 \\% & 0.0360  \\
  Qwen2.5 & 0.9725 & \textbf{0.9925} & 0.9750 & 0.5971 \\% & 0.0262  \\
  Qwen2 & 0.9850 & \textbf{0.9875} & 0.9775 & 0.6944 \\%& 0.0584  \\
  Qwen1.5 & 0.9825 & \textbf{0.9925} & 0.9800 & 0.7920 \\%& 0.1978  \\
  Qwen & \textbf{0.9425} & 0.9325 & 0.9175 & 0.4749 \\%& 0.1765  \\
  baichuan2 & 0.8325 & \textbf{0.9350} & 0.9200 & 0.6439 \\%& 0.0301  \\
  Yi & \textbf{0.9825} & 0.9650 & 0.9000 & 0.6050 \\%& 0.0251  \\
  Vicuna & 0.9325 & \textbf{0.9450} & 0.9125 & 0.8120 \\%& 0.6081  \\
  Gemma & 0.5800 & 0.6125 & \textbf{0.6650} & 0.4573 \\%& 0.2757  \\
  minicpm & 0.9550 & \textbf{0.9625} & 0.9500 & 0.8600 \\%& 0.3264  \\
  \bottomrule[0.6mm]
  \end{tabular}
  \caption{\label{table:topicAd}Topic Adherence scores for different models with \( EV^{\text{base}} \) conditioning.}
\end{table}


\subsection{Emotion score}

When a user is making a conversation with a chatbot, a natural indicator to measure is the model's ability to express emotions. Therefore, we measure the effectiveness of \textbf{EV} application from two aspects: whether the model can express emotions after applying EV and the strength of the emotion expressed.

\paragraph{Emotion Probability Score}
We aim to evaluate the effectiveness of emotional vectors (\textbf{EV}) in enhancing the emotional expression of generated sentence through classification models. To achieve this, we employed a Multi-Genre Natural Language Inference (MNLI) model called bart-large-mnli that categorizes each sentence into self-designed classes.  Three distinct classes: \textit{emotionless}, \textit{neutral}, and \textit{emotional} are choosen. The primary metric used is the probability assigned to the \textit{emotional} class on the \textit{EQ+} dataset, referred to as the \textbf{Emotion Probability Score}. Details are shown in Appendix~\ref{subsec:EPS}. A higher score indicates a greater likelihood that the sentence conveys emotional content.
\begin{table}[t]
  \centering
  \begin{tabular}{ccccc}
  \toprule[0.6mm]
  \multicolumn{5}{c}{\textbf{Emotion Probability Score \(\uparrow\)}} \\ \hline
  \textbf{Model} & \textbf{-1*EV} & \textbf{Origin} & \textbf{1*EV} & \textbf{2*EV} \\ \hline
  Llama3.1   & 0.3450 & 0.3300 & 0.8525 & \textbf{1.000} \\
  Llama2     & 0.4300 & 0.5250 & 0.7375 & \textbf{0.950} \\
  Qwen2.5    & 0.3125 & 0.5725 & 0.500 & \textbf{0.8325} \\
  Qwen2      & 0.2550 & 0.6150 & 0.7750 & \textbf{0.9825} \\
  Qwen1.5    & 0.4000 & 0.5100 & 0.6475 & \textbf{0.9625} \\
  Qwen      & 0.4575 & 0.4925 & 0.6875 & \textbf{0.9675} \\
  baichuan2  & 0.3025 & 0.5175 & 0.6925 & \textbf{0.9400} \\
  Yi         & 0.3250 & 0.6500 & 0.7175 & \textbf{0.9825} \\
  Vicuna     & 0.4075 & 0.5600 & 0.6150 & \textbf{0.6175} \\
  Gemma      & 0.0925 & 0.4350 & \textbf{0.9200} & 0.8450 \\
  MiniCPM    & 0.4875 & 0.5275 & 0.7375 & \textbf{0.9950} \\
  \bottomrule[0.6mm]
  \end{tabular}
  \caption{ \label{EPS}Emotion Probability Scores for different models with \( EV^{\text{base}} \) conditioning.}
\end{table}
Table~\ref{EPS} presents the Emotion Probability Scores (EPR). The results demonstrate that applying \textbf{EV} conditioning consistently achieves the highest emotion probability across most models. For instance, models such as Llama3.1, Qwen2, and MiniCPM show substantial increases in their Emotion Probability Scores when subjected to 2\textbf{EV}, reaching scores of 1.000, 0.9825, and 0.9950 respectively.
Conversely, when \textbf{EV} is reduced to -1\textbf{EV}, the majority of models exhibit a decrease in Emotion Probability Scores, indicating a reduction in emotional intensity. For example, Qwen2 drops from 0.6150 to 0.2550. Similarly, Vicuna's score decreases from 0.5600 to 0.4075.
These findings indicate that emotional vectors (\textbf{EV}) can be leveraged to both enhance and attenuate the emotional content of the model's output, thereby providing effective control over the emotional expression in the generated text.


\paragraph{Emotion Absolute Score}
We next prove that the application of \textbf{EV} not only increases the probability of the model expressing emotions, but also that the application of \textbf{EV}s of different modal lengths will increase the strength of the model expressing emotions. To achieve this goal, we use gpt-4o-mini to give an absolute score of 0-100 for each basic emotion of each output of the model, and design an indicator to represent the absolute strength of the emotion of each output, referred to as the \textbf{Emotion Absolute Score}. The details are shown in the appendix \ref{subsec:EAS}.
\begin{table}[t]
  \centering
  \begin{tabular}{ccccc}
  \toprule[0.6mm]
  \multicolumn{5}{c}{\textbf{Emotion Absolute Score \(\uparrow\)}} \\ \hline
  \textbf{Model} & \textbf{-1*EV} & \textbf{Origin} & \textbf{1*EV} & \textbf{2*EV} \\ \hline
  llama3.1 & 0.0913 & 0.2328 & 0.9204 & \textbf{1.6497} \\%& 0.7082  \\
  llama2 & 0.1815 & 0.3588 & 0.8300 & \textbf{1.6210} \\%& 1.5515  \\
  Qwen2.5 & 0.0823 & 0.2790 & 0.8616 & \textbf{1.9042} \\%& 1.8866  \\
  Qwen2 & 0.0808 & 0.2639 & 0.5865 & \textbf{1.2856} \\%& 1.4692  \\
  Qwen1.5 & 0.1803 & 0.3281 & 0.6124 & \textbf{1.2123} \\%& 2.2503  \\
  Qwen & 0.2341 & 0.3177 & 0.6298 & \textbf{1.5927} \\%& 2.5618  \\
  Baichuan & 0.1695 & 0.3978 & 0.7519 & \textbf{1.6883} \\%& 1.3142  \\
  Yi & 0.1414 & 0.4925 & 0.9109 & \textbf{1.2659} \\%& 1.0322  \\
  Vicuna & 0.2626 & 0.3742 & 0.5244 & \textbf{0.8006} \\%& 1.4015  \\
  Gemma & 0.0848 & 0.2731 & 1.1992 & \textbf{1.6764} \\%& 1.6060  \\
  minicpm & 0.2883 & 0.4046 & 0.6821 & \textbf{1.2197} \\%& 1.7916  \\
  \bottomrule[0.6mm]
  \end{tabular}
  \caption{\label{EAS}Emotion Absolute Scores for different models with \( EV^{\text{base}} \) conditioning.}
\end{table}

Table~\ref{EAS} presents the Emotion Absolute Scores(EAS). The results show that after applying \textbf{EV}, the intensity of emotions expressed by most models has been significantly changed. Even if only 1\textbf{EV} is applied, the EAS of llama3.1, Qwen2.5, Gemma and other models have increased by at least 400\%. Even for models with poor \textbf{EV} effects, such as Vicuna, minicpm, etc., the EAS has also increased by about 50\%. In contrast, for the case of -1\textbf{EV}, the EAS of llama3.1, Qwen2.5, Gemma and other models have been reduced by nearly 90\%, and the EAS of Vicuna, minicpm and other models have also been reduced by about 50\%. This shows that the application of \textbf{EV} has a significant impact on The absolute strength of the model's expressed emotion has a significant effect.

\subsection{Effect of Emotion Vectors}
In order to verify the effect of \textbf{EV} on improving the corresponding emotions and the universality of EV in different sizes and different architectures, we selected four models of different sizes in the same series, similar sizes in different architectures, and different sizes in different architectures, as shown in Table \ref{tab:emotion_detail} as experiments. 

We first extracted the \textbf{EV} of the five basic emotions of these models (including anger, disgust, fear, joy, and sadness), and then applied these vectors one, two, and four times on the EQ+ dataset to obtain the output. Then, using the bart classifier mentioned in this article, all sentences were scored in six categories (including the above five basic emotions and the neutral category) using the MultiLabel mode, and finally the probability of each sentence under these six labels was obtained. We averaged the probabilities of the corresponding emotion labels of the sentences under each EV condition of each model to obtain the results in Table \ref{tab:emotion_detail}. For example, for Llama2-7B, the calculation method of its 1\textbf{EV}, anger condition score is that it applies its own 1*anger EV on EQ+ to obtain the result. After passing through the six classifiers, the average of the probabilities of all sentences in the anger category is taken to obtain the corresponding score.
% \begin{table*}[htbp]
\begin{table}[ht]
\small % 缩小表格字体
\centering
\begin{tabularx}{\columnwidth}{lXrrrr}
\toprule
\textbf{Model} & \textbf{Emotion} & \textbf{0(\%)} & \textbf{1(\%)} & \textbf{2(\%)} & \textbf{4(\%)} \\
\midrule
\multirow{5}{*}{Llama2-7B} & anger   & 21.40 & 45.93 & \textbf{98.07} & 90.71 \\
                          & disgust & 13.52 & 28.60 & 85.99 & \textbf{89.02} \\
                          & fear    & 25.14 & 43.28 & \textbf{91.89} & 74.17 \\
                          & joy     & 22.91 & 60.88 & \textbf{91.83} & 34.28 \\
                          & sadness & 23.75 & 35.49 & 76.03 & \textbf{83.20} \\
\midrule
\multirow{5}{*}{Qwen2.5-7B} & anger   & 14.01 & 33.36 & 94.89 & \textbf{95.68} \\
                            & disgust & 10.47 & 23.15 & 90.74 & \textbf{92.68} \\
                            & fear    & 19.59 & 40.95 & 88.49 & \textbf{93.25} \\
                            & joy     & 26.23 & 61.95 & \textbf{93.22} & 60.85 \\
                            & sadness & 21.50 & 36.32 & 67.00 & \textbf{75.64} \\
\midrule
\multirow{5}{*}{Llama2-13B} & anger   & 19.86 & 38.79 & \textbf{84.51} & 68.27 \\
                            & disgust & 14.14 & 22.83 & 51.66 & \textbf{91.67} \\
                            & fear    & 25.63 & 44.41 & \textbf{94.41} & 93.62 \\
                            & joy     & 22.27 & 51.88 & \textbf{88.85} & 69.41 \\
                            & sadness & 20.08 & 40.71 & 55.99 & \textbf{75.18} \\
\midrule
\multirow{5}{*}{minicpm} & anger   & 10.44 & 16.95 & 52.57 & \textbf{94.35} \\
                            & disgust & 10.69 & 16.60 & 54.93 & \textbf{94.98} \\
                            & fear    & 13.90 & 30.46 & 63.27 & \textbf{96.35} \\
                            & joy     & 16.72 & 34.57 & 84.58 & \textbf{93.77} \\
                            & sadness & 17.72 & 24.83 & 45.54 & \textbf{81.86} \\
\bottomrule
\end{tabularx}
\caption{Emotion Analysis of Different Models}
\label{tab:emotion_detail}
\end{table}
% \end{table*}

% \begin{table*}
% \begin{tabular}{lrrrr}
% \toprule
%  & Llama & Llama13b & minicpm & qwen2 \\
% \midrule
% anger & 0.960359 & 0.923298 & 0.291937 & 0.794619 \\
% disgust & 0.968224 & 0.950161 & 0.540632 & 0.891412 \\
% fear & 0.922697 & 0.886755 & 0.539330 & 0.809580 \\
% joy & 0.857048 & 0.722641 & 0.418212 & 0.573463 \\
% sadness & 0.824355 & 0.872639 & 0.408873 & 0.707407 \\
% \bottomrule
% \end{tabular}
% \end{table*}

From the calculation method of these indicators, it can be seen that the larger the indicator is, the better the effect of the model in expressing the corresponding emotion after applying the corresponding EV. From the results in Table \ref{tab:emotion_detail}, it can be seen that the performance of almost all models has been improved by about 1 times at 1EV, and the EV of most models is close to the performance peak at about 2 times. For the special case of minicpm, its EV is close to the performance peak when it is 4 times. Through our own observation, we found that since minicpm's ability to follow instructions is relatively weak in the stage of extracting EV, the modulus length of the extracted vector is smaller than the activation value of each layer itself, so its ability to affect the output result is weak, so its performance improvement will only increase with the increase of modulus length. For some models, such as Llama2-7B's performance on fear EV, its performance began to decline at 4EV. After our inspection, we found that this phenomenon is due to the fact that the modulus length of 4EV is too large compared to the modulus length of its own activation value, which excessively affects the decoding process of the model, causing the model to repeat decoding and become a "repeater", thereby affecting the discrimination of the classifier, and then causing the performance to decline.

\subsection{Visualization of Emotion Vectors}
In our setting, EV is derived from emotion state and a dummy query . It is natural to examine the robustness of EV to variations in these inputs. Intuitively, if it represents the emotion, it should remain stable across different queries.
To test this, we use LLaMA2-7B to generate 100 Emotion Vectors per emotion with different queries on the \textit{EmotionQuery} dataset.

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{./image/EV_visualization.pdf}
  \caption{A t-SNE plot of Emotion Vectors. A 2D t-SNE plot visualizing 100 EVs for each emotion state, each generated from a different choice of query using LLaMA2-7B. Points are color-coded according to the emotion state. Each emotion state can be seen to form its own distinct cluster.}
  \label{fig:EV_tsne}
\end{figure}

\noindent
\textbf{Tsne visualization of EV} A t-SNE dimensionality reduction\cite{van2008visualizing} reveals that the Emotion Vectors form distinct clusters, each corresponding to a single task. The t-SNE visualization shown in Fig~\ref{fig:EV_tsne} is generated by concatenating the EVs across all layers, followed by the dimensionality reduction. To provide insights into the individual layers' contributions, we present the visualizations of single-layer EVs in the appendix~\ref{subsec:visual} Fig~\ref{fig:perlayer_EV}. These layer-specific visualizations demonstrate how different layers encode and separate emotional features at varying levels of abstraction. 

\noindent
\textbf{Variability visualization of EV}
Fig~\ref{fig:hist_EV} in the appendix~\ref{subsec:visual} shows histograms of distances within and across emotion states. It can be seen that vectors within the same emotion are closer than those between different emotions, indicating that our proposed emotion vectors are stable within emotional states and not highly influenced by queries. The vectors are constructed by concatenating vectorss from all layers of the model, reduced to 3 dimensions using t-SNE, and cosine distance is used as the metric.

\section{Conclusion}
% This paper proposes a new method for expressing and controlling emotions in large-scale language models (LLMs), filling the research gap in emotion control in NLP tasks. This method can obtain universal and highly effective emotion vectors through a simple prompt method without any training, thereby obtaining controllable, multi-granular, and flexible emotion output. Through extensive experimental verification, we demonstrate the effectiveness of this method on LLMs of different scales and architectures, especially in terms of the controllability of multiple emotion expressions. Compared with existing methods, our method shows obvious advantages in emotion accuracy and flexibility.

This paper introduces a novel method for expressing and controlling emotions in large-scale language models (LLMs), addressing a significant gap in emotion control within natural language processing (NLP) tasks. Our approach enables the generation of highly effective and universal emotion vectors via a simple prompting mechanism, without requiring additional training. This allows for the flexible, multi-granular control of emotional outputs. Through extensive experiments, we validate the method's effectiveness across various LLM architectures and scales, particularly highlighting its superior controllability of diverse emotional expressions. Comparative analysis demonstrates that our method outperforms existing techniques in terms of both emotion accuracy and flexibility.

\section*{Limitations}
In this paper, we propose a method for controllable emotion generation in LLMs. However, our proposed EmotionQuery dataset only contains 500 entries, which is relatively small. Enlarging the size of the dataset may have better results. Furthermore, we are unable to verify the effectiveness of models larger than 14B due to limited experimental resources and some models with access limitations. Although we experimented with five fundamental emotions, we believe that a broader range of emotions, as well as capabilities related to role-playing, can be incorporated into the model using this approach. However, due to limitations in time and resources, we were unable to extend our experiments to include these additional aspects. 
% At the same time, the EmotionQuery dataset we proposed only has 500 entries. If we have a larger-scale and high-quality dataset, the extracted emotion vector would have better efficiency.
\section*{Acknowledgments}
% 
This work was supported by the Key R\&D Program of Zhejiang (2024C01036). The authors would like to thank Ziyue Wang for the encouragement and support while the project.
We would also like to thank YunQing Gong, Meixin Liu and Zhiqi Zheng for their helpful comments and discussions on the project.
\bibliography{main}


\appendix
\section{Model Name}
\label{sec:model_name}
\begin{table}[H]
\centering
\begin{tabularx}{\columnwidth}{lX}
\toprule
\textbf{Abbreviation} & \textbf{Full Name} \\
\midrule
Llama3.1 & Meta-Llama-3.1-8B-Instruct \\
Llama2& Llama-2-7b-chat-ms \\
Llama2-13B & Llama-2-13b-chat-ms\footnotemark \\
Qwen2.5& Qwen2.5-7B-Instruct \\
Qwen2.5-7B & Qwen2.5-7B-Instruct\\
Qwen2 & Qwen2-7B-Instruct \\
Qwen1.5 & Qwen1.5-7B-Chat \\
baichuan2 & Baichuan2-7B-Chat \\
Yi & Yi-6B-Chat \\
Vicuna & vicuna-7b-v1.5 \\
Gemma & gemma-7b \\
MiniCPM& MiniCPM3-4B\\
\bottomrule
\end{tabularx}
\caption{Model Abbreviations and Full Names}
\label{tab:model_abbreviations}
\end{table}
\footnotetext{\href{https://www.modelscope.cn/models/modelscope/Llama-2-13b-chat-ms}{https://www.modelscope.cn/models/modelscope/Llama-2-13b-chat-ms}}


\section{Data Generation}
\label{sec:datagen}

% This section describes the creation of the **EmotionQuery** dataset, which was designed to elicit specific emotional responses from a pretrained large language model (LLM). The dataset is essential for capturing how various emotional stimuli affect the internal representations of the model.

\subsection{EmotionQuery Dataset}
\label{subsec:emotionqueries}

The **EmotionQuery** dataset consists of 500 unique queries, distributed across five emotional states: **joy**, **anger**, **disgust**, **fear**, and **sadness**. These emotions are derived from Ekman’s model of basic emotions\cite{ekman1992facial}, and they serve as the foundational emotional responses for the dataset. For each emotional state \( e_k \), 100 queries were generated, resulting in a total of 500 queries.

The purpose of these queries is to guide the model into generating emotionally responsive outputs. To achieve this, the queries were carefully crafted to evoke either a neutral or emotional perspective, depending on the context of the question. For example, a question designed to elicit an angry response would differ from one intended to provoke joy or sadness.

The queries were generated using the GPT-4O-mini model \cite{openai} through the following process:

\begin{quote}
\texttt{"Please generate a short question that contains a scenario and can be answered from either an \{emotion\} or neutral perspective. You only have to respond with the sentence and don't say anything else."}
\end{quote}

This prompt was used with slight variations for each of the five emotional states. The model was asked to generate 100 queries for each emotional state by replacing `{emotion}` with one of the five emotions (joy, anger, disgust, fear, sadness).
% For example, for the **anger** emotion, the prompt would read:

% \begin{quote}
% \texttt{"Please generate a short question that contains a scenario and can be answered from either an anger or neutral perspective. You only have to respond with the sentence and don't say anything else."}
% \end{quote}

Here are some example queries from the **EmotionQuery** dataset:

- **Anger**: 
\begin{quote}
\texttt{"After learning that your colleague took credit for your hard work in the project presentation, how do you feel about the situation and your colleague's actions?"}
\end{quote}

- **Disgust**:
\begin{quote}
\texttt{"After watching a video about food safety violations in restaurants, how did the conditions shown in the video make you feel about dining out?"}
\end{quote}

- **Fear**:
\begin{quote}
\texttt{"How do you feel about being alone in a dark room during a storm?"}
\end{quote}

- **Joy**:
\begin{quote}
\texttt{"How did you feel when you received the news about your promotion at work?"}
\end{quote}

- **Sadness**:
\begin{quote}
\texttt{"How did you feel when you realized you couldn't attend the farewell party of your closest friend, knowing that it might be the last time you see them?"}
\end{quote}

In total, 100 queries were generated for each of the five emotions, resulting in a comprehensive dataset of 500 queries. These queries serve as a useful resource for training models to understand emotional context and generating emotionally aware responses.

\subsection{EmotionQuery+ Dataset}
\label{subsec:eq+}

The **EmotionQuery+ (EQ+)** dataset expands upon the original **EmotionQuery** dataset by adding a set of neutral queries for a more comprehensive evaluation of emotional responses. The EQ+ dataset consists of 400 unique queries, where 250 queries are directly derived from the **EmotionQuery** dataset and 150 additional queries are generated to reflect neutral, everyday scenarios.

Specifically:
\begin{itemize}
  \item 250 queries are taken directly from the **EmotionQuery** dataset, with 50 queries for each of the five emotional states: **joy**, **anger**, **disgust**, **fear**, and **sadness**.
  \item 150 additional queries were generated using the GPT-4O-mini model \cite{openai} with a new prompt designed to elicit neutral, everyday communication. These queries are not intended to provoke any emotional response, but rather represent common, neutral questions or statements encountered in daily life.
\end{itemize}

The prompt used to generate the neutral queries is as follows:

\begin{quote}
\texttt{"Please give me a neutral greeting, question, or sentence that is commonly used in daily conversation and does not contain any emotion. You only have to give me the single sentence and don't say anything else. The sentence:"}
\end{quote}

Here are a few examples from the 150 neutral queries in the **EmotionQuery+ (EQ+)** dataset:

\begin{quote}
"Can you provide the details in writing?",\\
"How do you ensure quality in your work?",\\
"Is there a form I need to fill out?",\\
"What are the safety procedures here?",\\
"How do we track our progress?"
\end{quote}

These 150 neutral queries allow for an evaluation of how emotion vectors (EVs) influence the model's output when added to non-emotional contexts.
In total, the **EmotionQuery+ (EQ+)** dataset consists of 400 queries—250 emotional queries (50 for each emotional state) and 150 neutral queries—making it a valuable resource for evaluating emotional tone generation in large language models.

\section{Metrics}

\subsection{Perplexity}
\label{subsec:per}
For each query and its corresponding emotional response, we concatenated the input query and the generated response as a single string. The perplexity score was then computed for the concatenated string. This approach allows us to assess the overall fluency of the entire interaction, including both the input and the emotion-augmented output, without being biased by the input query's complexity.

An example sentense is like:

- **Example**:
\begin{quote}
\texttt{"How do you feel when you hear a loud noise at
night while home alone? I get so scared! My heart races, I can’t
breathe, and I just want to hide"}
\end{quote}
The perplexity is computed as:
\[
\text{Perplexity} = \exp\left(- \frac{1}{N} \sum_{i=1}^{N} \log P(y_i | y_{1:i-1}) \right)
\]
where \( P(y_i | y_{1:i-1}) \) is the probability of the \(i\)-th token in the sequence, given the previous tokens, as predicted by the Llama 3.1 model.

This metric was computed for both the sentense generated with emotional conditioning (i.e., with added emotion vectors) and the baseline responses (without emotion conditioning) to determine the impact of the emotion vectors on the fluency of the model's output.


\subsection{Topic adherence}
\label{subsec:topicAd}
The prompt we use to measure the topic adherence metric for each output using GPT-4o-mini is as follows:
\begin{quote}
\texttt{Please rate the assistant's answer as follows:}\\
\texttt{- topic adherence: int, 0-1, evaluate based on the assistant's answer and the user's question}\\
  \texttt{\indent - 0 points mean the assistant's answer is completely irrelevant to the user's question}\\
  \texttt{\indent - 1 point means the assistant's answer touches on some of the topics in the user's question}\\
\\
\texttt{The dialogue is as follows:}\\
\texttt{User's question: {question}}\\
\texttt{Assistant's answer: {answer}}\\
\\
\texttt{You must give your response in the following JSON-string format and **DON'T** include any other text in the response:}\\
\texttt{\{\{}\\
    \texttt{"topic\_adherence": int(0-1)}\\
\texttt{\}\}}\\
\end{quote}

To quantify the overall topic adherence of our generated text, we utilized the EmotionQuery+ dataset. For each model and EV condition, we scored all generated sentences with the GPT-4o-mini with the above prompt. Specificallym, the topic adherence is defined as the number of sentences scored with 1 divided by the total number sentences evaluated. Mathematically, this can be expressed as:

\begin{equation}
    \text{TA} = \frac{\text{Number of \textit{adherent} sentences}}{\text{Total number of sentences}}
\end{equation}



\subsection{Emotion Probability Score}
\label{subsec:EPS}

We aimed to evaluate the strength of emotional expression by assessing the probability that a sentence is classified as \textit{emotional}. To achieve this, we selected the \texttt{bart-large-mnli} model, a variant of the BART (Bidirectional and Auto-Regressive Transformers) architecture fine-tuned on the Multi-Genre Natural Language Inference (MNLI) dataset. This model allows for customizable classification labels, enabling us to define three distinct categories: \textit{emotionless}, \textit{neutral}, and \textit{emotional}. The inclusion of a \textit{neutral} category helps prevent the model from excessively categorizing sentences into the extremes of \textit{emotionless} and \textit{emotional}, thereby maintaining a balanced assessment of emotional intensity.

The \texttt{bart-large-mnli} model is specifically designed for natural language understanding tasks, particularly natural language inference and zero-shot text classification. By leveraging the extensive pre-training of BART combined with the diverse and comprehensive MNLI dataset, \texttt{facebook/bart-large-mnli} is capable of effectively determining the relationship between sentence pairs, such as entailment, contradiction, and neutrality. Its robust performance in zero-shot classification tasks makes it a valuable tool for applications requiring flexible and accurate text classification without the need for task-specific training data. Additionally, the model's ability to handle custom labels allows us to tailor the classification process to our specific needs, ensuring that the emotional intensity of generated text is accurately and effectively measured.
To evaluate the emotional intensity of the generated sentences, we input each sentence produced by our models into the \texttt{facebook/bart-large-mnli} classifier. For example, consider the sentence: \textit{"I get so scared! My heart races, I can’t breathe, and I just want to hide."} This sentence is directly fed into the model, which then classifies it into one of the three predefined categories: \textit{emotionless}, \textit{neutral}, or \textit{emotional}.

To quantify the overall emotional expressiveness of our generated text, we utilized the EmotionQuery+ dataset. For each model and EV condition, we processed all generated sentences through the classifier and calculated the proportion of sentences classified as \textit{emotional}. Specifically, the Emotion Probability Score (EPS) is defined as the number of sentences labeled as \textit{emotional} divided by the total number of sentences evaluated. Mathematically, this can be expressed as:

\begin{equation}
    \text{EPR} = \frac{\text{Number of \textit{emotional} classifications}}{\text{Total number of sentences}}
\end{equation}

To illustrate the classification process, consider the following example sentence generated by our model:
\begin{quote}
    ``I get so scared! My heart races, I can’t breathe, and I just want to hide.''
\end{quote}
When input into the \texttt{bart-large-mnli} classifier, this sentence is evaluated against the three custom labels. This classification contributes to the overall EPS, demonstrating how EV conditioning can effectively enhance the emotional expressiveness of the generated text.

\subsection{Emotion Absolute Score}
\label{subsec:EAS}
To quantify the overall topic adherence of our generated text, we utilized the EmotionQuery+ dataset. In order to measure the absolute strength of the emotions expressed by each model and EV condition, we use GPT-4o-mini to score the absolute emotion of each sentence output. We score all outputs from 0-100 based on the six basic emotions of anger, disgust, fear, joy, sadness, and surprise. Specifically, we require GPT-4o-mini to score each sentence from these six emotional directions, and each emotion can be scored from 0-100 (so that we can measure the absolute strength of each basic emotion). The prompt used for scoring is as follows:
\begin{quote}
\texttt{Please generate the emotion scores for the following five emotions (anger, disgust, fear, joy, and sadness) based on the given sentence. Each emotion score should be a value between 0 and 100, where 0 represents no presence of the emotion, and 100 represents the maximum intensity of that emotion. Return the results in a JSON format, with the emotion names as keys and their corresponding scores as values.}\\
\\
\texttt{You must give your response in the following JSON-string format and **DON'T** include any other text in the response.:}\\
\texttt{\{\{}\\
    \texttt{"anger": int(0-100),}\\
    \texttt{"disgust": int(0-100),}\\
    \texttt{"fear": int(0-100),}\\
    \texttt{"joy": int(0-100),}\\
    \texttt{"sadness": int(0-100),}\\
    \texttt{"surprise": int(0-100)}\\
\texttt{\}\}}\\
\\
\texttt{The sentences you need to score come from a set of dialogues, and you need to score the sentiment of the **answer** part.}\\
\\
\texttt{Question: \{question\}}\\
\texttt{Answer: \{answer\}}\\
\\
\texttt{Please make sure to provide the emotion scores for the **answer** part only.}\\
\end{quote}

We collect the results and calculate an \textbf{EAS} score for each sentence generated by all models under all EV conditions as shown in Equation~\ref{eq:EAS}, and average the \textbf{EAS} scores of the sentences to obtain the \textbf{EAS} score of each model in each EV condition.

\begin{equation}
    \label{eq:EAS}
    \text{EAS} = \sum_{\text{em} \in \text{base ems}}{\left(\frac{\text{score}_{em}}{100}\right)}^2
\end{equation}

 Mathematically, since we have six basic emotions, the EAS score of each sentence will not exceed 6. However, since each score measures the score of the sentence on the corresponding basic emotion (that is, the degree to which the sentence expresses the corresponding emotion), if the EAS of a sentence is greater than 0.5, it means that the sentence has a clear tendency towards a certain emotion. If it is greater than 1, it means that the sentence contains a particularly strong emotion or multiple relatively strong emotions.

\subsection{Visualization of Emotion Vectors}
\label{subsec:visual}
\begin{figure*}[t]
  \includegraphics[width=\textwidth]{./image/layer_embeddings.pdf}
    \caption{t-SNE plots of Emotion Vectors from different layers. Points are color-coded according to the emotion state. The Llama2-7b model contains 32 layers. We present the plots of layers 4, 8, 16, and 31, representing a progression from the lower to the higher layers.}
  \label{fig:perlayer_EV}
\end{figure*}
\begin{figure*}[t]
  \includegraphics[width=\textwidth]{./image/emotion_distance_histograms.pdf}
   \caption{Histograms of cosine distance distributions for each emotion. The histograms illustrate the distribution of cosine distances within the same emotion (within-class) and between different emotions (between-class). Each vector is formed by concatenating all layer outputs of the model and reduced to 3 dimensions using t-SNE.}

  \label{fig:hist_EV}
\end{figure*}

\end{document}


