@article{abdin2024phi,
  title={Phi-3 technical report: A highly capable language model locally on your phone},
  author={Abdin, Marah and Aneja, Jyoti and Awadalla, Hany and Awadallah, Ahmed and Awan, Ammar Ahmad and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Bao, Jianmin and Behl, Harkirat and others},
  journal={arXiv preprint arXiv:2404.14219},
  year={2024}
}

@article{bereska2024mechanistic,
  title={Mechanistic Interpretability for AI Safety--A Review},
  author={Bereska, Leonard and Gavves, Efstratios},
  journal={arXiv preprint arXiv:2404.14082},
  year={2024}
}

@article{cassano2023multipl,
  title={MultiPL-E: a scalable and polyglot approach to benchmarking neural code generation},
  author={Cassano, Federico and Gouwar, John and Nguyen, Daniel and Nguyen, Sydney and Phipps-Costin, Luna and Pinckney, Donald and Yee, Ming-Ho and Zi, Yangtian and Anderson, Carolyn Jane and Feldman, Molly Q and others},
  journal={IEEE Transactions on Software Engineering},
  volume={49},
  number={7},
  pages={3675--3691},
  year={2023},
  publisher={IEEE}
}

@article{chan2023transformer,
  title={Transformer-based vulnerability detection in code at EditTime: Zero-shot, few-shot, or fine-tuning?},
  author={Chan, Aaron and Kharkar, Anant and Moghaddam, Roshanak Zilouchian and Mohylevskyy, Yevhen and Helyar, Alec and Kamal, Eslam and Elkamhawy, Mohamed and Sundaresan, Neel},
  journal={arXiv preprint arXiv:2306.01754},
  year={2023}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde De Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{conmy2023towards,
  title={Towards automated circuit discovery for mechanistic interpretability},
  author={Conmy, Arthur and Mavor-Parker, Augustine and Lynch, Aengus and Heimersheim, Stefan and Garriga-Alonso, Adri{\`a}},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={16318--16352},
  year={2023}
}

@article{dakhel2023github,
  title={Github copilot ai pair programmer: Asset or liability?},
  author={Dakhel, Arghavan Moradi and Majdinasab, Vahid and Nikanjam, Amin and Khomh, Foutse and Desmarais, Michel C and Jiang, Zhen Ming Jack},
  journal={Journal of Systems and Software},
  volume={203},
  pages={111734},
  year={2023},
  publisher={Elsevier}
}

@article{dou2024s,
  title={What's Wrong with Your Code Generated by Large Language Models? An Extensive Study},
  author={Dou, Shihan and Jia, Haoxiang and Wu, Shenxi and Zheng, Huiyuan and Zhou, Weikang and Wu, Muling and Chai, Mingxu and Fan, Jessica and Huang, Caishuang and Tao, Yunbo and others},
  journal={arXiv preprint arXiv:2407.06153},
  year={2024}
}

@article{elhage2021mathematical,
   title={A Mathematical Framework for Transformer Circuits},
   author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2021},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2021/framework/index.html}
}

@article{guo2024deepseek,
  title={DeepSeek-Coder: When the Large Language Model Meets Programming--The Rise of Code Intelligence},
  author={Guo, Daya and Zhu, Qihao and Yang, Dejian and Xie, Zhenda and Dong, Kai and Zhang, Wentao and Chen, Guanting and Bi, Xiao and Wu, Yu and Li, YK and others},
  journal={arXiv preprint arXiv:2401.14196},
  year={2024}
}

@article{hao2022aixbench,
  title={Aixbench: A code generation benchmark dataset},
  author={Hao, Yiyang and Li, Ge and Liu, Yongqiang and Miao, Xiaowei and Zong, He and Jiang, Siyuan and Liu, Yang and Wei, He},
  journal={arXiv preprint arXiv:2206.13179},
  year={2022}
}

@article{heimersheim2023circuit,
  title={A circuit for Python docstrings in a 4-layer attention-only transformer},
  author={Heimersheim, Stefan and Janiak, Jett},
  journal={URL: https://www. alignmentforum. org/posts/u6KXXmKFbXfWzoAXn/acircuit-for-python-docstrings-in-a-4-layer-attention-only},
  year={2023}
}

@article{hendrycks2021measuring,
  title={Measuring coding challenge competence with apps},
  author={Hendrycks, Dan and Basart, Steven and Kadavath, Saurav and Mazeika, Mantas and Arora, Akul and Guo, Ethan and Burns, Collin and Puranik, Samir and He, Horace and Song, Dawn and others},
  journal={arXiv preprint arXiv:2105.09938},
  year={2021}
}

@inproceedings{lai2023ds,
  title={DS-1000: A natural and reliable benchmark for data science code generation},
  author={Lai, Yuhang and Li, Chengxi and Wang, Yiming and Zhang, Tianyi and Zhong, Ruiqi and Zettlemoyer, Luke and Yih, Wen-tau and Fried, Daniel and Wang, Sida and Yu, Tao},
  booktitle={International Conference on Machine Learning},
  pages={18319--18345},
  year={2023},
  organization={PMLR}
}

@article{li2023starcoder,
  title={Starcoder: may the source be with you!},
  author={Li, Raymond and Allal, Loubna Ben and Zi, Yangtian and Muennighoff, Niklas and Kocetkov, Denis and Mou, Chenghao and Marone, Marc and Akiki, Christopher and Li, Jia and Chim, Jenny and others},
  journal={arXiv preprint arXiv:2305.06161},
  year={2023}
}

@article{mcdougall2023copy,
  title={Copy suppression: Comprehensively understanding an attention head},
  author={McDougall, Callum and Conmy, Arthur and Rushing, Cody and McGrath, Thomas and Nanda, Neel},
  journal={arXiv preprint arXiv:2310.04625},
  year={2023}
}

@article{nanda2023progress,
  title={Progress measures for grokking via mechanistic interpretability},
  author={Nanda, Neel and Chan, Lawrence and Lieberum, Tom and Smith, Jess and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2301.05217},
  year={2023}
}

@article{olah2020zoom,
  author = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  title = {Zoom In: An Introduction to Circuits},
  journal = {Distill},
  year = {2020},
  note = {https://distill.pub/2020/circuits/zoom-in},
  doi = {10.23915/distill.00024.001}
}

@article{rai2024practical,
  title={A practical review of mechanistic interpretability for transformer-based language models},
  author={Rai, Daking and Zhou, Yilun and Feng, Shi and Saparov, Abulhair and Yao, Ziyu},
  journal={arXiv preprint arXiv:2407.02646},
  year={2024}
}

@article{roziere2023code,
  title={Code llama: Open foundation models for code},
  author={Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Sauvestre, Romain and Remez, Tal and others},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}

@inproceedings{siddiq2022securityeval,
  title={SecurityEval dataset: mining vulnerability examples to evaluate machine learning-based code generation techniques},
  author={Siddiq, Mohammed Latif and Santos, Joanna CS},
  booktitle={Proceedings of the 1st International Workshop on Mining Software Repositories Applications for Privacy and Security},
  pages={29--33},
  year={2022}
}

@article{srivastava2022beyond,
  title={Beyond the imitation game: Quantifying and extrapolating the capabilities of language models},
  author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
  journal={arXiv preprint arXiv:2206.04615},
  year={2022}
}

@article{tambon2024bugs,
  title={Bugs in large language models generated code},
  author={Tambon, Florian and Dakhel, Arghavan Moradi and Nikanjam, Amin and Khomh, Foutse and Desmarais, Michel C and Antoniol, Giuliano},
  journal={arXiv preprint arXiv:2403.08937},
  year={2024}
}

@article{team2024codegemma,
  title={Codegemma: Open code models based on gemma},
  author={Team, CodeGemma and Zhao, Heri and Hui, Jeffrey and Howland, Joshua and Nguyen, Nam and Zuo, Siqi and Hu, Andrea and Choquette-Choo, Christopher A and Shen, Jingyue and Kelley, Joe and others},
  journal={arXiv preprint arXiv:2406.11409},
  year={2024}
}

@misc{wang2022interpretabilitywildcircuitindirect,
      title={Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small}, 
      author={Kevin Wang and Alexandre Variengien and Arthur Conmy and Buck Shlegeris and Jacob Steinhardt},
      year={2022},
      eprint={2211.00593},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2211.00593}, 
}

@inproceedings{wang2023recode,
  title={ReCode: Robustness Evaluation of Code Generation Models},
  author={Wang, Shiqi and Li, Zheng and Qian, Haifeng and Yang, Chenghao and Wang, Zijian and Shang, Mingyue and Kumar, Varun and Tan, Samson and Ray, Baishakhi and Bhatia, Parminder and others},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={13818--13843},
  year={2023}
}

@article{yang2024seccodeplt,
  title={SecCodePLT: A Unified Platform for Evaluating the Security of Code GenAI},
  author={Yang, Yu and Nie, Yuzhou and Wang, Zhun and Tang, Yuheng and Guo, Wenbo and Li, Bo and Song, Dawn},
  journal={arXiv preprint arXiv:2410.11096},
  year={2024}
}

@inproceedings{yu2024codereval,
  title={Codereval: A benchmark of pragmatic code generation with generative pre-trained models},
  author={Yu, Hao and Shen, Bo and Ran, Dezhi and Zhang, Jiaxin and Zhang, Qi and Ma, Yuchi and Liang, Guangtai and Li, Ying and Wang, Qianxiang and Xie, Tao},
  booktitle={Proceedings of the 46th IEEE/ACM International Conference on Software Engineering},
  pages={1--12},
  year={2024}
}

@article{zhuo2024bigcodebench,
  title={Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions},
  author={Zhuo, Terry Yue and Vu, Minh Chien and Chim, Jenny and Hu, Han and Yu, Wenhao and Widyasari, Ratnadira and Yusuf, Imam Nur Bani and Zhan, Haolan and He, Junda and Paul, Indraneil and others},
  journal={arXiv preprint arXiv:2406.15877},
  year={2024}
}

