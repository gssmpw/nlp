\section{Introduction}
% \zyc{we will write this section later}

{Language models (LMs) have recently showcased impressive capabilities in code-related tasks, generating executable code from task specifications provided in natural language prompts~\cite{jiang2024survey, zan2023large}. To further enhance their capability, they are often instruction-tuned on code-specific datasets, resulting in specialized models known as Code LMs, such as CodeLlama~\cite{roziere2023code}, StarCoder~\cite{li2023starcoder}, Code Gemma~\cite{team2024codegemma}, and DeepSeek-Coder~\cite{guo2024deepseek}. These advancements have led to widespread adoption by programmers and researchers, integrating LMs into their daily workflows to assist with coding tasks~\cite{dakhel2023github}.}  

{However, despite substantial progress in code generation, Code LMs can generate incorrect code, particularly when handling complex tasks~\cite{dou2024s, tambon2024bugs}. These faulty codes can pose significant risks, especially when used by novice programmers working on critical applications~\cite{dakhel2023github}. Furthermore, even when the generated code snippets are functionally correct, recent research found that these code snippets could contain security vulnerabilities~\cite{siddiq2022securityeval, yang2024seccodeplt}. Understanding the capabilities and limitations of Code LMs, such as where and how they invoke relevant knowledge internally, is therefore essential to mitigate potential harm in high-stakes scenarios. Yet, there is limited knowledge about the internals of these LMs while generating code for a given task. Developing deeper insights into their knowledge-relevant mechanisms is crucial for improving their reliability, performance, and safe deployment.}

Mechanistic interpretability (MI) has recently emerged as a promising approach to understanding the internal mechanisms of LMs~\cite{olah2020zoom, elhage2021mathematical, rai2024investigation, bereska2024mechanistic}. MI studies have investigated a range of LM behaviors, including in-context learning~\cite{elhage2021mathematical, bansal2022rethinking, ren2024identifying}, reasoning~\cite{stolfo2023mechanistic, rai2024investigation, dutta2024think}, and fact recall~\cite{geva2023dissecting, chughtai2024summing}, providing valuable insights into how various LM components, such as multi-head attention (MHA) and feedforward (FF) sublayers, contribute to these capabilities. While substantial work has been done to investigate various behaviors of LMs, there has been limited focus on understanding how Code LMs internally use their knowledge in code generation tasks.
% the capabilities of Code LMs. 

To address this gap, we present one of the first MI studies on Code LMs, where we investigate the internal workings of the CodeLlama-7b~\cite{roziere2023code} for the syntax completion task, the success of which requires an LM to not only locate its declarative knowledge of the programming language but also use the knowledge with its other capabilities (e.g., counting) smartly.
Specifically, we study how CodeLlama-7b performs the closing parentheses task (e.g., \lstinline|print(str(1| $\rightarrow$ \lstinline|))|), where each opening parenthesis must be paired with a closing parenthesis. To this end, we first contribute a synthetic dataset for systematically studying a Code LM's syntax completion performance. Our dataset includes a total 168 prompts covering three sub-tasks with the number of closing parentheses in the target tokens being 2, 3, and 4, respectively. These prompts include recursive calls of class constructors including \lstinline|str|, \lstinline|list|, and \lstinline|set|, with the number of open parentheses ranging from 2 to 12. With our dataset,
we perform a series of analyses investigating the internal mechanisms of CodeLlama-7b, including projecting the intermediate (sub-)layers' activations via the logit lens~\cite{nostalgebraist2020blog} to understand the typical timing when the model realizes the correct token, measuring the logit difference between the correct and the counterfactual tokens to understand the effective contribution of each (sub-)layer to correct token predictions, and performing attention visualization analysis to discover the attention patterns inside the Code LM. 
% using the logit lens~\cite{nostalgebraist2020blog} and attention visualization analysis. 
Our experimental results reveal that: 
\begin{itemize}
    \item The Code LM can realize the correct target token only from the middle-to-late layers. For example, in case of needing two closing parentheses in the target token, the model on average only ranks the correct token within top 10 based on its projected logit value from layer 18, and being the top 1 from layer 25. We also discover that when the required closing parentheses increase, it becomes even more difficult for the model to identify the correct token, as illustrated by its even later layers for ranking the correct token within the top 10 or top 1.
    \item When looking into a comparative effect of predicting the correct token vs. predicting the counterfactual token, both MHA and FF sub-layers contribute to the task. However, MHA sub-layers make a more critical contribution to the prediction of the correct tokens. In addition, the contributions of the (sub-)layers generally follow a similar pattern when we vary the number of open parentheses and the class constructors in the input prompt.
    % \zyc{todo: add a takeaway msg for the prompt type discussion.}
    \item Finally, we identified and interpreted key attention heads responsible for performing the task. For instance, we discovered two attention heads, $L30H0$ and $L27H24$, both keeping track of the number of already closed parentheses precisely.
    % \zyc{I don't understand this sentence; you mean the other head tracks only the last two already closed parens?} \daking{Yes, that's what I meant! But will confirm with Sam if this is true.} 
    % {Interestingly, while $L30H0$ consistently contributes positively across all closing parenthesis subtasks, $L27H24$ is crucial only when the task involves two closing parentheses, and instead has a negative effect when handling other numbers of closing parentheses.}
    However, while $L30H0$ consistently promotes the correct number of closing parentheses that are still missing across sub-tasks, $L27H24$ always promotes the token including exactly two closing parentheses, which we summarize as \emph{incorrect knowledge association}. As a consequence, it was found to be crucial when the task requires two closing parentheses, but has a negative effect otherwise.
    % when handling other numbers of closing parentheses.
\end{itemize}

% (1) the model makes correct predictions starting from layer XX, (2) while both MHA and FF sublayers contribute to the task, MHA sublayers are more critical, and (3) we identify and interpret key attention heads responsible for performing the task. For instance, we discover two attention heads that keep track of the number of already closed parentheses, where one head performs the more general task of tracking any number of closing parentheses, while the other specializes in tracking exactly two. Interestingly, the activation of the specialized head is essential when the task involves two closing parentheses but has a negative effect when handling other numbers of closing parentheses.

 

