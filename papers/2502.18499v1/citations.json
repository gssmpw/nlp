[
  {
    "index": 0,
    "papers": [
      {
        "key": "abdin2024phi",
        "author": "Abdin, Marah and Aneja, Jyoti and Awadalla, Hany and Awadallah, Ahmed and Awan, Ammar Ahmad and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Bao, Jianmin and Behl, Harkirat and others",
        "title": "Phi-3 technical report: A highly capable language model locally on your phone"
      },
      {
        "key": "team2024codegemma",
        "author": "Team, CodeGemma and Zhao, Heri and Hui, Jeffrey and Howland, Joshua and Nguyen, Nam and Zuo, Siqi and Hu, Andrea and Choquette-Choo, Christopher A and Shen, Jingyue and Kelley, Joe and others",
        "title": "Codegemma: Open code models based on gemma"
      },
      {
        "key": "guo2024deepseek",
        "author": "Guo, Daya and Zhu, Qihao and Yang, Dejian and Xie, Zhenda and Dong, Kai and Zhang, Wentao and Chen, Guanting and Bi, Xiao and Wu, Yu and Li, YK and others",
        "title": "DeepSeek-Coder: When the Large Language Model Meets Programming--The Rise of Code Intelligence"
      },
      {
        "key": "li2023starcoder",
        "author": "Li, Raymond and Allal, Loubna Ben and Zi, Yangtian and Muennighoff, Niklas and Kocetkov, Denis and Mou, Chenghao and Marone, Marc and Akiki, Christopher and Li, Jia and Chim, Jenny and others",
        "title": "Starcoder: may the source be with you!"
      },
      {
        "key": "roziere2023code",
        "author": "Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Sauvestre, Romain and Remez, Tal and others",
        "title": "Code llama: Open foundation models for code"
      },
      {
        "key": "chen2021evaluating",
        "author": "Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde De Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others",
        "title": "Evaluating large language models trained on code"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "chan2023transformer",
        "author": "Chan, Aaron and Kharkar, Anant and Moghaddam, Roshanak Zilouchian and Mohylevskyy, Yevhen and Helyar, Alec and Kamal, Eslam and Elkamhawy, Mohamed and Sundaresan, Neel",
        "title": "Transformer-based vulnerability detection in code at EditTime: Zero-shot, few-shot, or fine-tuning?"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "yu2024codereval",
        "author": "Yu, Hao and Shen, Bo and Ran, Dezhi and Zhang, Jiaxin and Zhang, Qi and Ma, Yuchi and Liang, Guangtai and Li, Ying and Wang, Qianxiang and Xie, Tao",
        "title": "Codereval: A benchmark of pragmatic code generation with generative pre-trained models"
      },
      {
        "key": "zhuo2024bigcodebench",
        "author": "Zhuo, Terry Yue and Vu, Minh Chien and Chim, Jenny and Hu, Han and Yu, Wenhao and Widyasari, Ratnadira and Yusuf, Imam Nur Bani and Zhan, Haolan and He, Junda and Paul, Indraneil and others",
        "title": "Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions"
      },
      {
        "key": "lai2023ds",
        "author": "Lai, Yuhang and Li, Chengxi and Wang, Yiming and Zhang, Tianyi and Zhong, Ruiqi and Zettlemoyer, Luke and Yih, Wen-tau and Fried, Daniel and Wang, Sida and Yu, Tao",
        "title": "DS-1000: A natural and reliable benchmark for data science code generation"
      },
      {
        "key": "cassano2023multipl",
        "author": "Cassano, Federico and Gouwar, John and Nguyen, Daniel and Nguyen, Sydney and Phipps-Costin, Luna and Pinckney, Donald and Yee, Ming-Ho and Zi, Yangtian and Anderson, Carolyn Jane and Feldman, Molly Q and others",
        "title": "MultiPL-E: a scalable and polyglot approach to benchmarking neural code generation"
      },
      {
        "key": "hao2022aixbench",
        "author": "Hao, Yiyang and Li, Ge and Liu, Yongqiang and Miao, Xiaowei and Zong, He and Jiang, Siyuan and Liu, Yang and Wei, He",
        "title": "Aixbench: A code generation benchmark dataset"
      },
      {
        "key": "srivastava2022beyond",
        "author": "Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\\`a} and others",
        "title": "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models"
      },
      {
        "key": "hendrycks2021measuring",
        "author": "Hendrycks, Dan and Basart, Steven and Kadavath, Saurav and Mazeika, Mantas and Arora, Akul and Guo, Ethan and Burns, Collin and Puranik, Samir and He, Horace and Song, Dawn and others",
        "title": "Measuring coding challenge competence with apps"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "abdin2024phi",
        "author": "Abdin, Marah and Aneja, Jyoti and Awadalla, Hany and Awadallah, Ahmed and Awan, Ammar Ahmad and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Bao, Jianmin and Behl, Harkirat and others",
        "title": "Phi-3 technical report: A highly capable language model locally on your phone"
      },
      {
        "key": "team2024codegemma",
        "author": "Team, CodeGemma and Zhao, Heri and Hui, Jeffrey and Howland, Joshua and Nguyen, Nam and Zuo, Siqi and Hu, Andrea and Choquette-Choo, Christopher A and Shen, Jingyue and Kelley, Joe and others",
        "title": "Codegemma: Open code models based on gemma"
      },
      {
        "key": "guo2024deepseek",
        "author": "Guo, Daya and Zhu, Qihao and Yang, Dejian and Xie, Zhenda and Dong, Kai and Zhang, Wentao and Chen, Guanting and Bi, Xiao and Wu, Yu and Li, YK and others",
        "title": "DeepSeek-Coder: When the Large Language Model Meets Programming--The Rise of Code Intelligence"
      },
      {
        "key": "li2023starcoder",
        "author": "Li, Raymond and Allal, Loubna Ben and Zi, Yangtian and Muennighoff, Niklas and Kocetkov, Denis and Mou, Chenghao and Marone, Marc and Akiki, Christopher and Li, Jia and Chim, Jenny and others",
        "title": "Starcoder: may the source be with you!"
      },
      {
        "key": "roziere2023code",
        "author": "Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Sauvestre, Romain and Remez, Tal and others",
        "title": "Code llama: Open foundation models for code"
      },
      {
        "key": "chen2021evaluating",
        "author": "Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde De Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others",
        "title": "Evaluating large language models trained on code"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "chan2023transformer",
        "author": "Chan, Aaron and Kharkar, Anant and Moghaddam, Roshanak Zilouchian and Mohylevskyy, Yevhen and Helyar, Alec and Kamal, Eslam and Elkamhawy, Mohamed and Sundaresan, Neel",
        "title": "Transformer-based vulnerability detection in code at EditTime: Zero-shot, few-shot, or fine-tuning?"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "yu2024codereval",
        "author": "Yu, Hao and Shen, Bo and Ran, Dezhi and Zhang, Jiaxin and Zhang, Qi and Ma, Yuchi and Liang, Guangtai and Li, Ying and Wang, Qianxiang and Xie, Tao",
        "title": "Codereval: A benchmark of pragmatic code generation with generative pre-trained models"
      },
      {
        "key": "zhuo2024bigcodebench",
        "author": "Zhuo, Terry Yue and Vu, Minh Chien and Chim, Jenny and Hu, Han and Yu, Wenhao and Widyasari, Ratnadira and Yusuf, Imam Nur Bani and Zhan, Haolan and He, Junda and Paul, Indraneil and others",
        "title": "Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions"
      },
      {
        "key": "lai2023ds",
        "author": "Lai, Yuhang and Li, Chengxi and Wang, Yiming and Zhang, Tianyi and Zhong, Ruiqi and Zettlemoyer, Luke and Yih, Wen-tau and Fried, Daniel and Wang, Sida and Yu, Tao",
        "title": "DS-1000: A natural and reliable benchmark for data science code generation"
      },
      {
        "key": "cassano2023multipl",
        "author": "Cassano, Federico and Gouwar, John and Nguyen, Daniel and Nguyen, Sydney and Phipps-Costin, Luna and Pinckney, Donald and Yee, Ming-Ho and Zi, Yangtian and Anderson, Carolyn Jane and Feldman, Molly Q and others",
        "title": "MultiPL-E: a scalable and polyglot approach to benchmarking neural code generation"
      },
      {
        "key": "hao2022aixbench",
        "author": "Hao, Yiyang and Li, Ge and Liu, Yongqiang and Miao, Xiaowei and Zong, He and Jiang, Siyuan and Liu, Yang and Wei, He",
        "title": "Aixbench: A code generation benchmark dataset"
      },
      {
        "key": "srivastava2022beyond",
        "author": "Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\\`a} and others",
        "title": "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models"
      },
      {
        "key": "hendrycks2021measuring",
        "author": "Hendrycks, Dan and Basart, Steven and Kadavath, Saurav and Mazeika, Mantas and Arora, Akul and Guo, Ethan and Burns, Collin and Puranik, Samir and He, Horace and Song, Dawn and others",
        "title": "Measuring coding challenge competence with apps"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "yu2024codereval",
        "author": "Yu, Hao and Shen, Bo and Ran, Dezhi and Zhang, Jiaxin and Zhang, Qi and Ma, Yuchi and Liang, Guangtai and Li, Ying and Wang, Qianxiang and Xie, Tao",
        "title": "Codereval: A benchmark of pragmatic code generation with generative pre-trained models"
      },
      {
        "key": "tambon2024bugs",
        "author": "Tambon, Florian and Dakhel, Arghavan Moradi and Nikanjam, Amin and Khomh, Foutse and Desmarais, Michel C and Antoniol, Giuliano",
        "title": "Bugs in large language models generated code"
      },
      {
        "key": "dou2024s",
        "author": "Dou, Shihan and Jia, Haoxiang and Wu, Shenxi and Zheng, Huiyuan and Zhou, Weikang and Wu, Muling and Chai, Mingxu and Fan, Jessica and Huang, Caishuang and Tao, Yunbo and others",
        "title": "What's Wrong with Your Code Generated by Large Language Models? An Extensive Study"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "dou2024s",
        "author": "Dou, Shihan and Jia, Haoxiang and Wu, Shenxi and Zheng, Huiyuan and Zhou, Weikang and Wu, Muling and Chai, Mingxu and Fan, Jessica and Huang, Caishuang and Tao, Yunbo and others",
        "title": "What's Wrong with Your Code Generated by Large Language Models? An Extensive Study"
      },
      {
        "key": "tambon2024bugs",
        "author": "Tambon, Florian and Dakhel, Arghavan Moradi and Nikanjam, Amin and Khomh, Foutse and Desmarais, Michel C and Antoniol, Giuliano",
        "title": "Bugs in large language models generated code"
      },
      {
        "key": "dakhel2023github",
        "author": "Dakhel, Arghavan Moradi and Majdinasab, Vahid and Nikanjam, Amin and Khomh, Foutse and Desmarais, Michel C and Jiang, Zhen Ming Jack",
        "title": "Github copilot ai pair programmer: Asset or liability?"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "wang2023recode",
        "author": "Wang, Shiqi and Li, Zheng and Qian, Haifeng and Yang, Chenghao and Wang, Zijian and Shang, Mingyue and Kumar, Varun and Tan, Samson and Ray, Baishakhi and Bhatia, Parminder and others",
        "title": "ReCode: Robustness Evaluation of Code Generation Models"
      },
      {
        "key": "siddiq2022securityeval",
        "author": "Siddiq, Mohammed Latif and Santos, Joanna CS",
        "title": "SecurityEval dataset: mining vulnerability examples to evaluate machine learning-based code generation techniques"
      },
      {
        "key": "yang2024seccodeplt",
        "author": "Yang, Yu and Nie, Yuzhou and Wang, Zhun and Tang, Yuheng and Guo, Wenbo and Li, Bo and Song, Dawn",
        "title": "SecCodePLT: A Unified Platform for Evaluating the Security of Code GenAI"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "dou2024s",
        "author": "Dou, Shihan and Jia, Haoxiang and Wu, Shenxi and Zheng, Huiyuan and Zhou, Weikang and Wu, Muling and Chai, Mingxu and Fan, Jessica and Huang, Caishuang and Tao, Yunbo and others",
        "title": "What's Wrong with Your Code Generated by Large Language Models? An Extensive Study"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "elhage2021mathematical",
        "author": "Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris",
        "title": "A Mathematical Framework for Transformer Circuits"
      },
      {
        "key": "olah2020zoom",
        "author": "Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan",
        "title": "Zoom In: An Introduction to Circuits"
      },
      {
        "key": "rai2024practical",
        "author": "Rai, Daking and Zhou, Yilun and Feng, Shi and Saparov, Abulhair and Yao, Ziyu",
        "title": "A practical review of mechanistic interpretability for transformer-based language models"
      },
      {
        "key": "bereska2024mechanistic",
        "author": "Bereska, Leonard and Gavves, Efstratios",
        "title": "Mechanistic Interpretability for AI Safety--A Review"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "elhage2021mathematical",
        "author": "Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris",
        "title": "A Mathematical Framework for Transformer Circuits"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "wang2022interpretabilitywildcircuitindirect",
        "author": "Kevin Wang and Alexandre Variengien and Arthur Conmy and Buck Shlegeris and Jacob Steinhardt",
        "title": "Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "heimersheim2023circuit",
        "author": "Heimersheim, Stefan and Janiak, Jett",
        "title": "A circuit for Python docstrings in a 4-layer attention-only transformer"
      },
      {
        "key": "conmy2023towards",
        "author": "Conmy, Arthur and Mavor-Parker, Augustine and Lynch, Aengus and Heimersheim, Stefan and Garriga-Alonso, Adri{\\`a}",
        "title": "Towards automated circuit discovery for mechanistic interpretability"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "nanda2023progress",
        "author": "Nanda, Neel and Chan, Lawrence and Lieberum, Tom and Smith, Jess and Steinhardt, Jacob",
        "title": "Progress measures for grokking via mechanistic interpretability"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "elhage2021mathematical",
        "author": "Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris",
        "title": "A Mathematical Framework for Transformer Circuits"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "mcdougall2023copy",
        "author": "McDougall, Callum and Conmy, Arthur and Rushing, Cody and McGrath, Thomas and Nanda, Neel",
        "title": "Copy suppression: Comprehensively understanding an attention head"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "elhage2021mathematical",
        "author": "Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris",
        "title": "A Mathematical Framework for Transformer Circuits"
      }
    ]
  }
]