\section{Experimental Results}

\subsection{Overview of Experiments}
% TODO[Sam] - update with what we hope to gain from each of the experiments after completing following sub-sections
% We conducted five experiments with each building on top of the results of the prior experiments. The first experiment took a look at the tokens associated with the top ten highest logit values at each layer of the Code LM. Then, we analyzed the logit difference between the correct token and counterfactual token at each layer of the accumulated residual stream. This was followed up by examining the logit difference contributions of the feed-forward (FF) and multi-head attention (MHA) components at each transformer layer to the residual stream. Our next experiment went one level deeper and took a look at the logit difference contributions of the attention heads for important MHA components to the residual stream. After identifying the attention heads that had a strong positive or negative contribution to the residual stream, we utilized attention analysis to interpret their behavior.
\zy{Our experiments aim to answer three Research Questions (RQs). RQ1 and RQ2 examine the layer-wise phenomena in the process of the Code LLM generating the correct next token. Specifically, RQ1 leverages \textit{logit lens}~\cite{nostalgebraist2020blog} to understand what the model would have predicted in a given layer, from which we gauge from which layer the model typically can start picking the correct token. RQ2 then looks into how (e.g., effect of promotion~\cite{?}) each (sub-)layer contributes to the prediction of the correct token, particularly by contrasting the logit values between the correct token and a \emph{counterfactual} token (i.e., their ``logit difference'')~\cite{?}. In this process, we also locate the critical attention layers that strongly promote the generation of the correct token. Following that, RQ3 then specifically investigates the patterns of these critical attention layers, such as how different attention heads in these layers play a role and how each attention head functions, aiming to form a clearer understanding of how the Code LM becomes aware of the required number of closing parentheses.}

% \zyc{give an overview of the different experiments so people can understand and easily follow the logic. I've commented your old writing as we have said to re-structure the writing, but feel free to get the old writing back to the right location.}


% \subsubsection{RQ1: How does a Code LM perform a syntax completion task?}

% \paragraph{Localization of behavior} To investigate RQ1, we employed a logit lens~\cite{nostalgebraist2020blog} to measure the contribution of feed-forward (FF) and multi-head attention (MHA) sub-layer in promoting the correct labels across all transformer layers. Our experimental results reveal that while both the FF and MHA sub-layers are important for the Parenthesis Completion Task, the MHA sub-layers appear to contribute more significantly.

% \paragraph{Identifying important attention heads} Next, we conducted an ablation study to identify the most influential attention heads, as illustrated in Figure~\ref{}. Our analysis revealed XX attention heads that promote correct labels and YY attention heads that suppress them. To further understand the role of these attention heads, we examined their attention patterns. Notably, we identified $L27H0$ (layer 27, head 0), the head with the highest positive contribution, which effectively tracks the number of closed parentheses by attending to the function name up to which the parentheses are closed, as shown in Figure~\ref{}. Similarly, we also identified another attention head $LxxHyy$ with a similar role in tracking the number of closed parentheses. However, this head is limited to monitoring only two closing parentheses and consistently promotes \emph{``))''} as the correct output, even in cases where the correct label requires three or four closing parentheses. 

\begin{table*}[t!]
    \centering
    \resizebox{0.9\textwidth}{!}{
    \begin{tabular}{clll}
         \toprule
         \textbf{Sub-Task} & \textbf{First Layer Top 10} & \textbf{First Layer Top 1} & \textbf{Consistent Top 1 Layer}   \\
         \midrule 
          Two Closing Paren & Layer 19 &  Layer 25 & Layer 25 \\
         \midrule
         Three Closing Paren & Layer 19 &  Layer 30 & Layer 30\\
         \midrule
         Four Closing Paren & Layer 21 &  Layer 31 & Layer 31 \\
         \bottomrule
    \end{tabular}
    }
    \caption{Results of RQ 1: "At what layer does the Code LM start picking the correct token?", aggregated at the sub-task level. \daking{Are we reporting median? It's highly unlikely all the examples picked correct token at certain layer.}}
    \label{tab:subtask_top_ten_logits}
\end{table*}

\sam{TODO comeback and update for median}
\subsection{RQ 1: At what layer does the Code LM start picking the correct token?}\label{subsec:exp1}
The overarching goal of this experiment was to better understand at what points during an inference phase the Code LM has an understanding of what the next token prediction should be, which we define as the correct token's logit value being within the top ten logit values at a layer. This experiment aims to answer the overall question by obtaining answers to three related sub-questions: (1) \textit{At what layer does the correct token's logit value first break into the top 10 logits?}, (2) \textit{When does the Code LM first consider that the correct token should be predicted as the next token?} (I.e., the correct token is associated with the overall highest logit value.), (3) \textit{When does the correct token's logit value consistently rank as the highest logit value for all subsequent layers?}. The respective answers to these questions for each of the sub-tasks can be found in Table~\ref{tab:subtask_top_ten_logits}, where the results displayed are aggregates for all prompts of a sub-task. The 

We find that the Two Closing Parentheses sub-task 


\sam{TODO - discuss results}

% old exp 2 and 3 combined
\subsection{RQ 2: How does each (sub-)layer contribute to the correct token prediction?}\label{subsec:exp2}

\begin{figure}[h] % 'h' means place the figure here
    \centering
    \includegraphics[width=0.5\textwidth]{LaTeX/accum_plotsubtask_level.png} % Adjust width and specify the image path
    \caption{Per Sub-task Logit Difference at Layers of Accumulated Residual Stream} % Figure caption
    \label{fig:accum_subtask} % Label for cross-referencing
\end{figure}

% \begin{figure}[h] % 'h' means place the figure here
%     \centering
%     \includegraphics[width=0.5\textwidth]{LaTeX/accum_plotsub_task2_prompttype_level.png} % Adjust width and specify the image path
%     \caption{Per Prompt Type Logit Difference at Layers of Accumulated Residual Stream for Sub-task 1} % Figure caption
%     \label{fig:accum_subtask2} % Label for cross-referencing
% \end{figure}

\begin{figure}[h] % 'h' means place the figure here
    \centering
    \includegraphics[width=0.5\textwidth]{LaTeX/per_layer_plot_codellama_subtask3.png} % Adjust width and specify the image path
    \caption{Per Sub-task Logit Difference Contribution of Sub-layers} % Figure caption
    \label{fig:perlayer_subtask} % Label for cross-referencing
\end{figure}

% \begin{figure}[h] % 'h' means place the figure here
%     \centering
%     \includegraphics[width=0.5\textwidth]{LaTeX/per_layer_plot_sub_task2_prompttype_level.png} % Adjust width and specify the image path
%     \caption{Per Prompt Type Logit Difference Contribution of Sub-layers for Sub-task 1} % Figure caption
%     \label{fig:perlayer_subtask2} % Label for cross-referencing
% \end{figure}


% \zyc{Somewhere in this paragraph you need to introduce the concept of promotion and suppression and cite prior work discussing them. @Daking can you help?}
\dakingrev{To understand each layer's contribution to correct token prediction, we first measure the \emph{logit difference} on the residual stream across all layers. }
% Following the answers obtained in Exp 1, regarding the layers at which the Code LM starts picking the correct token, we dive one layer deeper to see which layers/sub-layers promote/suppress the prediction of the correct token. For this experiment as well as the following experiment, we move away from solely looking at the logit value of the correct token  in favor of looking at the logit difference between the correct token and its associated counterfactual token. Further discussion on the reasoning behind this change and the construction of each correct token's associated counterfactual token can be found in Section~\ref{subsec:overview-experiment-design}. 
The impact of each layer's contribution on the logit difference of the accumulated residual stream can be seen in Figure~\ref{fig:accum_subtask}. Figure~\ref{fig:accum_subtask} showcases the average logit difference of the aggregated residual stream across layers for each sub-task. 
% Figure~\ref{fig:accum_subtask2} displays the same metric but focuses on the results for the Two Closing Parentheses sub-task aggregated at a prompt type level. 
\sam{The results in Figure~\ref{fig:accum_subtask} show that the Code LM gains an understanding, which in this context we define as the residual stream having a positive logit difference, of the Two Closing Parentheses sub-task much earlier than the Three Closing Parentheses and Four Closing Parentheses sub-tasks. The layers with the largest contribution to the residual stream for the Two Closing Parentheses, Three Closing Parentheses, and Four Closing Parentheses sub-tasks were layers 27 and 30, layers 30 and 31, and layers 31 and 26, respectively.}
% The prompt type level results for the Two Closing Parentheses sub-task found in Figure~\ref{fig:accum_subtask2} confirm the positive logit difference contributions of layers 27 and 30 for all prompt types.} 

\par The logit difference contribution to the residual stream at a sub-layer level can be seen in Figure~\ref{fig:perlayer_subtask} and Figure~\ref{fig:perlayer_subtask2}. Similar to above, Figure~\ref{fig:perlayer_subtask} displays each of the sub-layer contributions to the residual stream aggregated at a sub-task level, while Figure~\ref{fig:perlayer_subtask2} showcases the results for the Two Parentheses Sub-task aggregated at a prompt type level. \sam{The results in Figure~\ref{fig:perlayer_subtask} reveal the importance of both FF and MHSA sub-layers for the Closing Parentheses Task, as all sub-tasks have both FF and MHSA sub-layers in their top five positively contributing sub-layers. When comparing the overall contribution of the MHSA sub-layers and FF sub-layers across sub-tasks, the MHSA sub-layers have a similar or larger positive contribution for all sub-tasks. Their positive contribution is especially large for the Three Closing Parentheses and Four Closing Parentheses sub-tasks. Additionally, we find that the layer 30 MHSA sub-layer has a large contribution for all sub-tasks.}

% across all the sub-tasks we find that the overall contribution of the MHSA sub-layers 


% with the MHSA sub-layers appearing to contribute more significantly, especially for the Three Closing Parentheses and Four Closing Parentheses sub-tasks. Additionally, we find that the layer 30 MHSA sub-layer has a large contribution for all sub-tasks.}

% For the Two Closing Parentheses Sub-task, we find that in layer 27 both the FF and MHSA sub-layers contribute while in layer 30 only the MHSA sub-layer contributes. While, for the Three Closing Parentheses sub-task, we find that in layer 30 only the MHSA-sub layer contributes and in layer 31 both sub-layers contribute. Finally, for the Four Closing Parentheses sub-task, we find that in layer 31 MHSA sub-layer contributes and layer 26 FF contributes.}
% Additionally, our experimental results reveal that while both the FF and MHA sub-layers are important for the Parenthesis Completion Task, the MHA sub-layers appear to contribute more significantly. \sam{TODO - verify that this holds for all sub-tasks}

% \subsection{Exp 3: Logit Difference of Per Layer Component Contribution to Residual Stream}\label{subsec:exp3}
\subsection{RQ 3: How do attention heads contribute to the promotion/suppression of correct tokens?}\label{subsec:exp3}

\begin{figure}[h] % 'h' means place the figure here
    \centering
    \includegraphics[width=0.5\textwidth]{LaTeX/per_head_plot_codellama_subtask1.png} % Adjust width and specify the image path
    \caption{Logit Difference Contribution of Attention Heads for Two Closing Parentheses sub-task} % Figure caption
    \label{fig:perhead_heatmap} % Label for cross-referencing
\end{figure}

Given the insight from the experimental results of Exp 2 that MHA sub-layers appear to contribute in a more significant fashion to the Closing Parentheses task, especially the MHA sub-layer of layer 30, we ran an additional experiment to see how individual attention heads contribute to the promotion/suppression of the logit difference between the correct token and the counterfactual token. We were especially interested in identifying the attention heads that played a large role in the promotion/suppression of the logit difference in the previously identified MHA sub-layers, that had large positive/negative contributions to the residual stream. \sam{The results for the Two Closing Parentheses sub-task can be seen in Figure~\ref{fig:perhead_heatmap}. For the Two Closing Parentheses sub-task, we find that the largest contributing heads are $L30H0$ and $L27H24$, which both have positive contributions. While the $L30H0$ attention head exhibits similar contribution behavior in the Three Closing Parentheses and Four Closing Parentheses sub-tasks as in the Two Closing Parentheses sub-task, we find that the $L27H24$ head's behavior changes. It becomes one of the largest negatively contributing heads in the Three Closing Parentheses and Four Closing Parentheses sub-tasks.}
\par
After identifying the attention heads that strongly promote/suppress correct tokens for the sub-tasks, we analyzed the attention patterns of those attention heads on various prompts, with the overall goal of identifying the function of those attention heads. Notably, we identified $L30H0$ (layer 30, head 0), the head with the highest positive contribution, which effectively tracks the number of closed parentheses by attending to the function name up to which the parentheses are closed, as shown in Figure~\ref{}. Similarly, we also identified another attention head $LxxHyy$ with a similar role in tracking the number of closed parentheses. However, this head is limited to monitoring only two closing parentheses and consistently promotes \emph{``))''} as the correct output, even in cases where the correct label requires three or four closing parentheses.

% The results aggregated at the task level can be seen in Figure XX. While the results for the Two Closing Parentheses sub-task can be seen in Figure YY. \sam{TODO - discuss results}

% \subsection{Exp 4: Attention Analysis of Attention Heads with Strong Contribution to Residual Stream}\label{subsec:exp5}
% \subsection{RQ 4: What are the functions of the attention heads that strongly promote/suppress correct tokens?}\label{subsec:exp4}
% After identifying the attention heads that strongly promote/suppress correct tokens for the sub-tasks, we analyzed the attention patterns of those attention heads on various prompts, with the overall goal of identifying the function of those attention heads. Notably, we identified $L27H0$ (layer 27, head 0), the head with the highest positive contribution, which effectively tracks the number of closed parentheses by attending to the function name up to which the parentheses are closed, as shown in Figure~\ref{}. Similarly, we also identified another attention head $LxxHyy$ with a similar role in tracking the number of closed parentheses. However, this head is limited to monitoring only two closing parentheses and consistently promotes \emph{``))''} as the correct output, even in cases where the correct label requires three or four closing parentheses.
% \sam{TODO - further discuss results and generate example figures}



