\section{Discussion and Future Works} \label{sec: discussion}
{In this work, we presented preliminary findings toward a mechanistic understanding of how Code LMs use their internal knowledge to complete syntactic completion tasks, identifying multiple attention heads that play a critical role in this task. Building on these results, we suggest the following directions for future research.}

{\paragraph{Circuit Discovery}
% Code language models demonstrate strong performance on syntax completion tasks, achieving an accuracy of XX\% in our evaluation. 
% While we identified important (sub-)layers and important attention heads for this task, they do not fully capture the underlying mechanism. For instance, we didn't investigate the role of MLP layers which are also important for completion of the task, as discussed in Section~\ref{}. Building on these findings, our goal is to uncover a complete circuit that can perform the syntax completion task with comparable accuracy to the full model.
Seeing the relatively more important role the MHA sub-layers play in prioritizing the correct token over the counterfactual one, our work has focused on analyzing the MHA patterns in a Code LM. However, future work should extend the analysis to cover the MLP sub-layers (which were found to implement knowledge look-up in transformers~\cite{geva2021transformer}), and eventually portray a complete \emph{circuit} of how a Code LM associates various components in its transformer architecture towards successfully using its knowledge in syntax completion.}


{\paragraph{Universality of the Interpretation}
We hypothesize that there is significant overlap among similar sub-tasks involved in syntax completion tasks, both within and across programming languages. For example, we investigated CodeLlama's ability to perform the parenthesis completion task, which requires the model to track the number of open parentheses that have been closed. Similar counting mechanisms might also be needed for managing indentation in Python or closing curly braces in JavaScript. Our future work will look into whether a Code LM reuses the same components across tasks and languages for similar roles.
% Building on this, we aim to explore the question: \emph{``Do language models reuse the same LM components across tasks for similar roles?''}
}

{\paragraph{Improving the LM Performance}
% Once a circuit for a given task is identified, a natural follow-up question is whether we can modify the circuit to enhance the language model's performance on the task or correct its mistakes. 
Finally, we aim to utilize our interpretation result to enhance a Code LM's performance in real life.
For example, in experiments we have identified an attention head, $L27H24$, that performs \emph{incorrect knowledge association} and erroneously promotes two closing parentheses even when the model needs to generate three or four closing parentheses. Furthermore, recent work of \citet{geva2022transformer} and \citet{rai2024investigation} have showcased the potential of directly controlling a model's generation or task performance via manipulating its neuron activation. In the future, we will similarly explore if suppressing such less precise attention heads can improve the accuracy of the Code LM in closing the parentheses and beyond.
% By suppressing the less precise attention head, we observed improved accuracy in cases where the model is required to generate more than two closing parentheses.
}