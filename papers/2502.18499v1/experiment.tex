% \section{Experiment Setup}
\section{Methodology and Dataset}\label{sec:dataset-and-methodology}

% To gain insights into the question of \textit{How does a Code LM perform a syntax completion task?}, we focused our experiments on interpreting the Code LM at various levels of granularity. The Code LM utilized in the experiments was CodeLlama-7b, which has a decoder-only transformer architecture.  
In this section, we introduce our methodology towards forming a mechanistic understanding of how a Code LM performs a syntax completion task that requires using its knowledge about a programming language in combination with others (e.g., counting). Our experiment will focus on Python code generation using CodeLlama-7b~\cite{roziere2023code}, which is a state-of-the-art (SOTA) medium-size Code LM with a 32-layer decoder-only transformer architecture. The specific model checkpoint we use is ``CodeLlama-7b-hf'' (i.e., the base model). In what follows, we will first give an overview of our experiment design, then present our process of dataset generation to facilitate the experiment, and finally describe the methods we will use to analyze a Code LM.

\begin{table*}[t!]
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{cclcc}
         \toprule
         \textbf{Sub-Task} & \textbf{\#of Examples} & \textbf{Prompt $\rightarrow$ Target Token} & \textbf{Counterfactual Token} & \textbf{Accuracy}   \\
         \midrule 
          Two Closing Paren & 56 &  {\#print a list containing 2\textbackslash n \lstinline|print(list(list(tuple([2]))| $\rightarrow$ \lstinline|))|} & \lstinline|)| & 100.0\% \\
         \midrule
         Three Closing Paren & 84 &  \#print a string 12\textbackslash n\lstinline|print(str(str(12| $\rightarrow$ \lstinline|)))| & \lstinline|))| & 76.2\%\\
         \midrule
         Four Closing Paren & 28 &  \#print a set containing 123\textbackslash n\lstinline|print(set(set(set(set(tuple([123]))| $\rightarrow$ \lstinline|))))| & \lstinline|)))| & 100.0\% \\
         \bottomrule
    \end{tabular}
    }
    \caption{Examples of prompts provided to the Code LM for each sub-task. In our work, we synthesize a dataset of 168 prompts covering three sub-tasks with the number of closing parentheses in the target tokens being 2, 3, and 4, respectively. The sub-task design is based on the specific Code LM (Codellama)'s tokenization effect (see Section~\ref{subsec:data-generation}).}
    % \zyc{Should we add a column for the number of examples in each sub-task? Do we want to show the stats of the number of opening parentheses?} 
   
    \label{tab:subtask_prompts}
\end{table*}

\subsection{Motivation and Overview}\label{subsec:overview-experiment-design}
% \zyc{The current writing looks fine. We just need to add a few "starting sentences" to justify why this syntax completion task matters (cite \cite{dou2024s}, which says that even SOTA Code LMs still make syntactic errors, and the deep recursion is one common type). Please replace the Python in-line code format with the one I defined in Table 1.} \daking{+1, We can also justify by saying this is a common syntax completion task for various programming language (Java, JavaScript, etc.) as well.}

Syntax completion is a crucial and fundamental part of LM code generation, as syntactic correctness is essential for code to be executable. \citet{dou2024s} recently found that even SOTA Code LMs still suffer from syntactic issues to various extents in their generation. This has motivated us to carefully understand how a Code LM performs syntax completion. In our work, we select the closing parentheses task (e.g., \lstinline|print(str(1| $\rightarrow$ \lstinline|))|) as our task, given it is one of the most common syntactical structures seen across programming languages; as a result, it is safe to assume that SOTA Code LMs have learned the necessary syntactic knowledge from their training. The input provided to a Code LM in this task is a partially complete line of code {(e.g., {\lstinline|print(str(1|})}, which includes a varying number of function or class constructor calls but is missing some \emph{final} number of closing parentheses that needs to be predicted as a whole in its next token.\footnote{Modern Code LMs may or may not predict parentheses one by one. For example, Codellama tokenizes \lstinline|print(str(1))| into \lstinline|print|, \lstinline|(|, \lstinline|str|, \lstinline|(|, \lstinline|1|, and \lstinline|))|, a total of 5 tokens, with the two closing parentheses being predicted as one single token. In practice, we have observed that when Codellama completes the syntax correctly, it all follows this tokenization practice, namely, it directly generates ``\lstinline|))|'' as a single token instead of generating ``\lstinline|)|'' for two times. Therefore, in our task design, we consistently use the last token based on the Code LM's tokenization as the target token when analyzing the model's syntax completion performance.} 
The Code LM is then tasked with predicting the next token that consists of the necessary number of closing parentheses for the line of code to be syntactically correct {(e.g., {\lstinline|))| in the running example})}. 

In our experiments, we focus on analyzing the Code LM in Python programming language.
% we provided the Code LM with partially completed lines of code written in the Python programming language. 
This task was further broken down into three sub-tasks based on the number of closing parentheses required to correctly complete the line of code, which were two, three, and four closing parentheses. A partially completed line of code example for each of the sub-tasks can be seen in Table~\ref{tab:subtask_prompts}.  



% \par 
\subsection{Data Generation} \label{subsec:data-generation}
To evaluate the Code LM on the closing parentheses task, we created an initial synthetic dataset consisting of 168 input prompts, with each prompt consisting of a simple natural language instruction, in the form of a code comment that describes the desired semantic meaning of the following line of code, and a partially completed line of Python code. In our preliminary exploration, we found the code comment to be necessary to avoid semantic ambiguity, as otherwise there could be infinite plausible continuations of the same line of code (e.g., continuing ``\lstinline|print(str(1|'' with more digits), which will make the analysis difficult.
% Two representative example prompts can be seen in Figure XX. 
We began the dataset preparation process by searching for Python functions that were both commonly used in practice and could accept arguments of varying data types. To this end, we decided to initially focus on generating prompts that utilized the built-in {\lstinline|print|} function while varying the argument supplied to the function. The argument was varied through the selection of a Python built-in class constructor, from a set containing {\lstinline|str|}, {\lstinline|list|}, and {\lstinline|set|}, the integer value passed to the constructor, and the number of nested constructor calls (with the number of open parentheses ranging from 2 to 12 in our data synthesis), which was used to vary the number of required closing parentheses. Following, the various generated arguments were combined with a {\lstinline|print|} function call to produce completed lines of code. The completed lines of code were then combined with their respective natural language instructions and tokenized using the Code LM (i.e., Codellama)'s tokenizer to produce the final prompts along with their associated correct next (and final) token. 

On this dataset, Codellama-7b was able to achieve an overall accuracy of 88\%, with the breakdown for each sub-task shown in Table~\ref{tab:subtask_prompts}. 
% Although Codellama-7b achieves a lower accuracy on the Three Closing Parentheses sub-task than on the Four Closing Parentheses sub-task, this likely isn't an indicator of task difficulty, due to the Three Closing Parentheses sub-task containing prompts with a larger number of open parentheses. The Four Closing Parentheses sub-task prompt with the largest number of open parentheses has eight open parentheses. While, the Three Closing Parentheses sub-task prompts, which resulted in an incorrect number of closing parentheses, ranged from nine to eleven open parentheses.
Intriguingly, we observed that Codellama-7b achieves a lower accuracy on the Three Closing Parentheses sub-task than on the Four Closing Parentheses sub-task. Upon examination, we found that all failing cases in the Three Closing Parentheses sub-task occurred when the number of open parentheses in the prompt ranged from nine to eleven, whereas for the prompts in the Four Closing Parentheses sub-task, the largest number of open parentheses is only eight.\footnote{We note that the range of the number of open parentheses in each sub-task is largely an effect of Codellama's tokenization and cannot be enforced during the dataset generation.} An intuitive conjecture is that, when the number of open parentheses gets larger, it becomes more challenging for the Code LM to correctly count both the number of open parentheses and the existing number of closing parentheses in the prompt, as well as calculate the difference as the required number of closing parentheses for next token prediction.
% \zyc{Based on the current table, people will wonder why Three Closing is more difficult than Four Closing. We should have a brief discussion here. Does this accuracy depend more on the number of opening parens or the number of missing closing parens?}
% \zyc{We may also show the breakdown accuracy for different numbers of opening parentheses (optionally we can group them into, say, 1-5, 5-10, etc. if this makes the analysis easier).}


\subsection{Methodology} \label{subsec:methodology}
We will understand how a Code LM completes a syntax completion task by mainly looking at the model's behaviors of predicting the correct next token, which requires proper use of its knowledge about the programming language. Specifically, we employ \emph{logit lens} or \emph{direct logit attribution}~\cite{nostalgebraist2020blog} to analyze the contribution of each layer and its sublayers (MHA and FF) in predicting the correct next token. \emph{Logit lens} allows us to view what the LM would have predicted in a given (sub-)layer by projecting the intermediate activations (denoted as $v \in {R}^d$, where $d$ is the LM dimension) onto the logit distribution through multiplication with the unembedding parameter matrix (denoted as $W_U \in {R}^{d \times |\mathcal{V}|}$, where $\mathcal{V}$ is the vocabulary set and $|\mathcal{V}|$ denotes its size), i.e., $v W_U \in \mathcal{R}^{|\mathcal{V}|}$. As a result, we can examine the top-$k$ candidate tokens for the next token prediction at each intermediate (sub-)layer by viewing the logit distribution. We refer readers to \citet{nostalgebraist2020blog} or the recent survey paper of \citet{rai2024practical} for a systematic and detailed explanation of the logit lens method.

In addition to analyzing the absolute logit value of the target token, our experiment also involves calculating the \emph{logit difference} to evaluate the contribution of (sub-)layer for the correct token (e.g. {``\lstinline|)))|''}) relative to a \emph{counterfactual} token representing \emph{incorrect knowledge} (e.g. {``\lstinline|))|''}), effectively filtering out (sub-)layers that indiscriminately increase the logit values of several tokens. We list the counterfactual tokens in Table~\ref{tab:subtask_prompts}. Such a comparative analysis has been widely adopted by prior work \cite{vig2020investigating, meng2022locating, wang2022interpretabilitywildcircuitindirect} for effectively discovering task-specific LM behaviors. When the logit difference becomes more positive (or negative, respectively), it implies that the LM is more (or less, respectively) capable of distinguishing between the correct and the misleading tokens.

Finally, when we are able to locate attention layers and heads making the most contribution to the prediction of the correct token, we will then apply \emph{attention visualization} to scrutinize the model's attention pattern. In our experiment, we have found it a very helpful approach for interpreting the LM's behaviors in a human-understandable way.

{Our experiments were carried out using the TransformerLens library~\cite{nanda2022transformerlens} to implement the logit lens and logit difference, and CircuitsVis~\cite{cooney2023circuitsvis} for attention visualization.}

% \zyc{I think here we should clarify and cite the tools/libraries/codebases we use in our experiments. @Daking, can you add it?}

% \zyc{We have other analyses as well; should we consistently include them in this procedure overview? Also, this paragraph is duplicate with 3.1.} \daking{We only use logit lens and logit difference for all experiments}
% Experimental Procedure

% \zyc{Do we need the following?} \daking{I think we do need to briefly introduce the techniques that we are employing for experiments. I'll rewrite this paragraph.}\zyc{If you really want this summary, it should go under 2.1 Overview, not under 2.2 Data Generation. Also, the language here is very technical and hard to follow; I don't think it will allow any readers to understand the experiments. Typically in an Overview writing, you want to use very intuitive language (just imagine you are doing an elevator pitch to someone who does not work in this field specifically).}
% To gain initial insights into the RQs outlined above, we conducted experiments primarily utilizing the interpretability techniques of direct logit attribution, at various levels of granularity, and attention analysis. For direct logit attribution, instead of solely considering the logit value, at different points in the residual stream, of the token with the correct number of closing parentheses, we followed Weng et al. (2022) and utilized the metric of logit difference. To utilize this metric for our task, 
% \cite{wang2022interpretabilitywildcircuitindirect}