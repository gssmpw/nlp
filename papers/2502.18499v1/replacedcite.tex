\section{Related Works}
% {\paragraph{Code LMs} Code LMs____, are a class of LMs specifically developed to enhance code generation capabilities of LMs through fine-tuning and additional training techniques____. They have shown impressive performance on coding evaluation bechmarks____ designed to evaluate their performance on diverse coding-related tasks across various programming languages.}

{\paragraph{Analysis of Code LMs} Code LMs____, are a class of LMs specifically developed to enhance code generation capabilities of LMs through fine-tuning and additional training techniques____. Although these models have demonstrated remarkable capabilities in code generation tasks____, they remain susceptible to various syntactic and semantic (or logical) errors____. Prior studies have focused on empirically examining various types of bugs across a range of coding tasks and programming languages____ or proposing benchmark datasets to characterize these models' shortcomings____. For instance, ____ observed that LMs are especially prone to syntactical errors (e.g., incomplete syntax structure, and indentation issues) when generating code for complex or lengthy problems. While these studies provide valuable insights into when Code LMs are likely to make mistakes, our understanding of the underlying internal mechanisms enabling code-generation capabilities remains limited. To bridge this gap, our study investigates how Code LMs perform syntax completion tasks.}

{\paragraph{Mechanistic Interpretability (MI)} MI is a subfield of interpretability that aims to reverse-engineer LM by understanding their internal components and computational processes____. Recent MI studies have investigated various LM behaviors, including sequence completion task____, Indirect Object Identification____, Python docstring completion____ and modular addition tasks____, by discovering circuits, a subset of LM components responsible for implementing these LM behaviors. These circuits can be explained in terms of human-understandable algorithms after interpreting the circuit components, which has led to the discovery of several interpretable attention heads such as induction heads____, suppression heads____, and previous token heads____. Building upon these advancements, we study the internal mechanisms of code LMs to understand the syntax completion capability of Code LMs. As far as we know, there has no been prior work carefully studying MI techniques in the application of LMs for code generation. As the first step, in this work, we have focused on discovering how the CodeLlama model identifies the correct next token and contrasts it against the counterfactual token in the syntax completion task. We include a discussion of our future extension along this line of research in Section~\ref{sec: discussion}.}