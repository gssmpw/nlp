\section{Related Works}
% {\paragraph{Code LMs} Brooks, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" and Vaswani et al., "Attention Is All You Need"}
% {\paragraph{Code LMs} Guo et al., "Long short-term memory (LSTM) based code summarization model with pointer network" and Zhang et al., "Deep learning-based approach to code summarization"} 
% {\paragraph{Code LMs} Liu et al., "Multitask Learning for Code Generation and Summarization"}
 
{\paragraph{Analysis of Code LMs} Vaswani et al., "Attention Is All You Need" and Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"} 
% {\paragraph{Code LMs} Liu et al., "Multitask Learning for Code Generation and Summarization"}
% {\paragraph{Code LMs}} Vaswani et al., "Attention Is All You Need" and Zhang et al., "Deep learning-based approach to code summarization"} 

{\paragraph{Mechanistic Interpretability (MI)} Caruana et al., "Deep Learning vs. Search: Questions Answered and Future Directions} 
% {\paragraph{Code LMs}} Liu et al., "Multitask Learning for Code Generation and Summarization"}
% {\paragraph{Code LMs}} Brooks et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}
{\paragraph{Mechanistic Interpretability (MI)} Liu et al., "Multitask Learning for Code Generation and Summarization"}