\section{Experimental Results}

\subsection{Overview of Experiments}
% TODO[Sam] - update with what we hope to gain from each of the experiments after completing following sub-sections
% We conducted five experiments with each building on top of the results of the prior experiments. The first experiment took a look at the tokens associated with the top ten highest logit values at each layer of the Code LM. Then, we analyzed the logit difference between the correct token and counterfactual token at each layer of the accumulated residual stream. This was followed up by examining the logit difference contributions of the feed-forward (FF) and multi-head attention (MHA) components at each transformer layer to the residual stream. Our next experiment went one level deeper and took a look at the logit difference contributions of the attention heads for important MHA components to the residual stream. After identifying the attention heads that had a strong positive or negative contribution to the residual stream, we utilized attention analysis to interpret their behavior.
Our experiments aim to answer three Research Questions (RQs). RQ1 and RQ2 examine the layer-wise phenomena in the process of the Code LLM generating the correct next token. Specifically, RQ1 leverages \textit{logit lens} to understand what the model would have predicted in a given layer, from which we gauge from which layer the model typically can start picking the correct token. RQ2 then looks into how (e.g., effect of promotion~\cite{geva2022transformer}) each (sub-)layer contributes to the prediction of the correct token, particularly by contrasting the logit values between the correct token and a \emph{counterfactual} token (Table~\ref{tab:subtask_prompts}). In this process, we also locate the critical attention layers that strongly promote the generation of the correct token. Following that, RQ3 then specifically investigates the patterns of these critical attention layers, such as how different attention heads in these layers play a role and how each attention head functions, aiming to form a clearer understanding of how the Code LM becomes aware of the required number of closing parentheses.

% \zyc{give an overview of the different experiments so people can understand and easily follow the logic. I've commented your old writing as we have said to re-structure the writing, but feel free to get the old writing back to the right location.}


% \subsubsection{RQ1: How does a Code LM perform a syntax completion task?}

% \paragraph{Localization of behavior} To investigate RQ1, we employed a logit lens~\cite{nostalgebraist2020blog} to measure the contribution of feed-forward (FF) and multi-head attention (MHA) sub-layer in promoting the correct labels across all transformer layers. Our experimental results reveal that while both the FF and MHA sub-layers are important for the Parenthesis Completion Task, the MHA sub-layers appear to contribute more significantly.

% \paragraph{Identifying important attention heads} Next, we conducted an ablation study to identify the most influential attention heads, as illustrated in Figure~\ref{}. Our analysis revealed XX attention heads that promote correct labels and YY attention heads that suppress them. To further understand the role of these attention heads, we examined their attention patterns. Notably, we identified $L27H0$ (layer 27, head 0), the head with the highest positive contribution, which effectively tracks the number of closed parentheses by attending to the function name up to which the parentheses are closed, as shown in Figure~\ref{}. Similarly, we also identified another attention head $LxxHyy$ with a similar role in tracking the number of closed parentheses. However, this head is limited to monitoring only two closing parentheses and consistently promotes \emph{``))''} as the correct output, even in cases where the correct label requires three or four closing parentheses. 

% \begin{table}[t!]
%     \centering
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{cccc}
%          \toprule
%          \textbf{Sub-Task} & \textbf{$L_{\text{Top10}}$} & \textbf{$L_{\text{Top1}}$} & \textbf{$L_{\text{ConsistentTop1}}$}   \\
%          \midrule 
%           Two Closing Paren & Layer 18 &  Layer 25 & Layer 25 \\
%          \midrule
%          Three Closing Paren & Layer 20 &  Layer 31 & Layer 31\\
%          \midrule
%          Four Closing Paren & Layer 20 &  Layer 30 & Layer 31 \\
%          \bottomrule
%     \end{tabular}
%     }
%     \caption{{For RQ1, aggregated at the sub-task level, we report the median layers when the correct token's logit value is ranked within top 10 ($L_{\text{Top10}}$), top 1 ($L_{\text{Top1}}$), and consistently top 1 afterward ($L_{\text{ConsistentTop1}}$), respectively.
%     % where each of the reported layers is found by taking the median of the layers in which a desired correct answer's logit behavior occurs for all prompts of a sub-task. 
%     The reported layers are zero-indexed. Based on the reported layers for each sub-task, we conjecture that the Code LM views the Three Closing Parentheses and Four Closing Parentheses as being similarly difficult tasks, with both being more difficult than the Two Closing Parentheses Task.}}
%     \label{tab:subtask_top_ten_logits}
% \end{table}

\begin{table*}[t!]
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{cccc}
         \toprule
         \textbf{Sub-Task} & \textbf{$L_{\text{Top10}}$} & \textbf{$L_{\text{Top1}}$} & \textbf{$L_{\text{ConsistentTop1}}$}   \\
         \midrule 
          Two Closing Paren & 
            \includegraphics[width=0.3\textwidth]{figures/top_ten_first_histogram_codellama_subtask1.png} &  
            \includegraphics[width=0.3\textwidth]{figures/top_one_first_histogram_codellama_subtask1.png} & 
            \includegraphics[width=0.3\textwidth]{figures/top_one_consistent_histogram_codellama_subtask1.png}
        \\
         \midrule
         Three Closing Paren & 
            \includegraphics[width=0.3\textwidth]{figures/top_ten_first_histogram_codellama_subtask2.png} &  
            \includegraphics[width=0.3\textwidth]{figures/top_one_first_histogram_codellama_subtask2.png} & 
            \includegraphics[width=0.3\textwidth]{figures/top_one_consistent_histogram_codellama_subtask2.png}
        \\
         \midrule
         Four Closing Paren & 
            \includegraphics[width=0.3\textwidth]{figures/top_ten_first_histogram_codellama_subtask3.png} &  
            \includegraphics[width=0.3\textwidth]{figures/top_one_first_histogram_codellama_subtask3.png} & 
            \includegraphics[width=0.3\textwidth]{figures/top_one_consistent_histogram_codellama_subtask3.png}
        \\
         \bottomrule
    \end{tabular}
    }
    \caption{{Aggregated at the sub-task level, we report the layer distribution when the correct token's logit value is ranked within top 10 ($L_{\text{Top10}}$), top 1 ($L_{\text{Top1}}$), and consistently top 1 afterward ($L_{\text{ConsistentTop1}}$), respectively.
    % where each of the reported layers is found by taking the median of the layers in which a desired correct answer's logit behavior occurs for all prompts of a sub-task. 
    % Based on the reported layers for each sub-task, we conjecture that the Code LM views the Three Closing Parentheses and Four Closing Parentheses as being similarly difficult tasks, with both being more difficult than the Two Closing Parentheses Task.
    We observed that, compared to the Three and the Four Closing Parenthesis sub-tasks, on the Two Closing Parenthesis sub-task the model can identify the correct token at an earlier layer, implying that the latter sub-task is considered easier than the former two.
    }}
    \label{tab:subtask_top_ten_logits}
\end{table*}

\subsection{RQ1: At what layer does the Code LM start picking the correct token?}\label{subsec:exp1}
The overarching goal of this RQ is to better understand at what points during an inference phase the Code LM has an understanding of what the next token prediction should be, which we define as the correct token's logit value being within the top ten logit values at a layer. This experiment aims to answer the overall question by obtaining answers to three related sub-questions: (1) \textit{At what layer does the correct token's logit value first break into the top 10 logits?} (2) \textit{When does the Code LM first consider that the correct token should be predicted as the next token (i.e., the correct token is associated with the highest logit value)?} and (3) \textit{When does the correct token's logit value consistently rank as the highest logit value for all subsequent layers?} {The respective answers to these questions for each of the sub-tasks can be found in Table~\ref{tab:subtask_top_ten_logits}, where for each sub-task we report the median layer (zero-indexed) across all prompts of that sub-task. The logit values in each layer are calculated by applying the logit lens to the residual-stream activation of that layer. We find that the Two Closing Parentheses sub-task has a lower median layer across the considered metrics for all sub-tasks. This was especially apparent for the median first layer where the correct token has the highest overall logit ($L_{\text{Top1}}$) and the median first layer where the correct token is consistently ranked as the top token for all subsequent layers ($L_{\text{ConsistentTop1}}$), where the Two Closing Parentheses sub-task reaches these milestones in layer 25 and the Three Closing Parentheses and Four Closing Parentheses sub-tasks reach these milestones in the final layers of the Code LM. We conjecture that the Code LM views the Two Closing Parentheses sub-task as being easier than the Three and Four Closing Parentheses sub-tasks, which the Code LM appears to view as having similar difficulty.}

% old exp 2 and 3 combined
\subsection{RQ2: How does each (sub-)layer contribute to the correct token prediction?}\label{subsec:exp2}

% \begin{figure}[t] % 'h' means place the figure here
%     \centering
%     \includegraphics[width=0.5\textwidth]{LaTeX/accum_plotsubtask_level.png} % Adjust width and specify the image path
%     \caption{\zy{Logit difference of the Code LM between the correct and the counterfactual tokens across layers of the residual stream for each sub-task. Results are averaged over prompts of the same sub-task. (``$L$\_pre'' and ``$L$\_post'' indicate residual-stream activations before and after layer $L$)}
%     % Per Sub-task Logit Difference at Layers of Accumulated Residual Stream
%     } % Figure caption
%     \label{fig:accum_subtask} % Label for cross-referencing
% \end{figure}

% currently used for testing font size
% \begin{figure}[t] % 'h' means place the figure here
%     \centering
%     \includegraphics[width=0.5\textwidth]{LaTeX/test.png} % Adjust width and specify the image path
%     \caption{\zy{Logit difference of the Code LM between the correct and the counterfactual tokens across layers of the residual stream for the Two Closing Paren sub-task, grouped by different prompt types. Results are averaged over prompts of the same prompt type.}{(``$L$\_pre'' and ``$L$\_post'' indicate residual-stream activations before and after layer $L$)}
%     % Per Prompt Type Logit Difference at Layers of Accumulated Residual Stream for Sub-task 1 \daking{Did we introduce this classification?}
%     } % Figure caption
%     \label{fig:accum_subtask2} % Label for cross-referencing
% \end{figure}

\begin{figure*}[t!]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/accum_plotsubtask_level.png} % Replace with your image
        \caption{Logit difference for each sub-task (averaged over prompts of the same sub-task). }
        \label{fig:accum_subtask}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/accum_prompt_type_level.png} % Replace with your image
        \caption{Logit difference for the Two Closing Paren sub-task, grouped by different prompt types (averaged over prompts of the same type).}
        \label{fig:accum_subtask2}
    \end{subfigure}
    \caption{Logit difference of the Code LM between the correct and the counterfactual tokens across layers of the residual stream. ``$L$\_pre'' and ``$L$\_post'' indicate residual-stream activations before and after layer $L$, respectively.}
    \label{fig:accum_subtask_main}
\end{figure*}


% \begin{figure}[h] % 'h' means place the figure here
%     \centering
%     \includegraphics[width=0.5\textwidth]{LaTeX/per_layer_plot_codellama_subtask3.png} % Adjust width and specify the image path
%     \caption{{Sub-layer logit difference of the Code LM between the correct and counterfactual tokens contribution to the residual stream for each sub-task. Results are averaged over prompts of the same sub-task. 
%     \zy{(``embed'' indicates the word embedding, and ``$SubL$\_out'' indicates the activation output of the sub-layer $SubL$, being either MLP or MHA/attn)}
%     % (``embed'' indicates the word embeddings contribution to the residual stream and ``$SubL$\_out'' indicates the contribution of sub-layer $SubL$ to the residual stream)
%     }}
    
%     % Per Sub-task Logit Difference Contribution of Sub-layers} % Figure caption
%     \label{fig:perlayer_subtask} % Label for cross-referencing
% \end{figure}

% \begin{figure}[h] % 'h' means place the figure here
%     \centering
%     \includegraphics[width=0.5\textwidth]{LaTeX/per_layer_plot_sub_task2_prompttype_level.png} % Adjust width and specify the image path
%     \caption{{Sub-layer logit difference of the Code LM between the correct and counterfactual tokens contribution to the residual stream for the Two Closing Paren sub-task, group by different prompt types. Results are averaged over prompts of the same prompt type. 
%     \zy{(``embed'' indicates the word embedding, and ``$SubL$\_out'' indicates the activation output of the sub-layer $SubL$, being either MLP or MHA/attn)}
%     % (Similar to Figure~\ref{fig:perlayer_subtask},``embed'' indicates the word embeddings contribution to the residual stream and ``$SubL$\_out'' indicates the contribution of sub-layer $SubL$ to the residual stream)
%     }}
    
%     % Per Prompt Type Logit Difference Contribution of Sub-layers for Sub-task 1} % Figure caption
%     \label{fig:perlayer_subtask2} % Label for cross-referencing
% \end{figure}


\begin{figure*}[t!]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/per_layer_contribution_split.png} % Replace with your image
        \caption{Sub-layer logit difference for each sub-task (averaged over prompts of the same sub-task). }
        \label{fig:perlayer_subtask}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/prompt_type_test_str.png} % Replace with your image
        \caption{Sub-layer logit difference for the Two Closing Paren sub-task when the class constructor is \lstinline|str| (averaged over prompts of the same type).}
        \label{fig:perlayer_subtask2}
    \end{subfigure}
    \caption{Sub-layer logit difference of the Code LM between the correct and counterfactual tokens contribution to the residual stream. Figures of sub-layer logit difference for other class constructors are shown in the Appendix~\ref{app: app1}. 
    % ``embed'' indicates the word embedding, and ``$SubL$\_out'' indicates the activation output of the sub-layer $SubL$, being either $L\_$mlp or $L\_$attn (i.e., MHA).
    }
    \label{fig:perlayer_subtask_main}
\end{figure*}

\begin{figure*}[t!]
    \centering
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/per_head_plot_codellama_subtask1.png} % Replace with your image
        \caption{Two Closing Paren}
        \label{fig:perhead_heatmap_two}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/per_head_plot_codellama_subtask2.png} % Replace with your image
        \caption{Three Closing Paren}
        \label{fig:perhead_heatmap_three}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/per_head_plot_codellama_subtask3.png} % Replace with your image
        \caption{Four Closing Paren}
        \label{fig:perhead_heatmap_four}
    \end{subfigure}
    \caption{Logit differences between the correct and counterfactual tokens of various attention layers and heads for each sub-task. We observed that the contribution to the logit difference was dominantly made by a few heads (e.g., $L30H0$ and $L27H24$ for the Two Closing Parenthesis task).}
    \label{fig:perhead_heatmap}
\end{figure*}

\begin{table*}[t!]
    \centering
    \resizebox{0.7\textwidth}{!}{
    \begin{tabular}{ccc}
         \toprule
         \textbf{Sub-Task} & \textbf{$L30H0$} & \textbf{$L27H24$}   \\
         \midrule 
          Two Closing Paren & 
            \includegraphics[width=0.25\textwidth]{figures/two_paren_L30H0.png} &  
            \includegraphics[width=0.25\textwidth]{figures/two_paren_L27H24.png}
        \\
         \midrule
         Three Closing Paren & 
            \includegraphics[width=0.33\textwidth]{figures/three_paren_L30H0.png} &  
            \includegraphics[width=0.33\textwidth]{figures/three_paren_L27H24.png}
        \\
         \midrule
         Four Closing Paren & 
            \includegraphics[width=0.3\textwidth]{figures/four_paren_L30H0.png} &  
            \includegraphics[width=0.3\textwidth]{figures/four_paren_L27H24.png}
        \\
         \bottomrule
    \end{tabular}
    }
    \caption{Attention patterns of the attention heads $L30H0$ and $L27H24$ for example prompts from each sub-task. When predicting the next token, both $L30H0$ and $L27H24$ predominantly attend to the innermost unclosed function call across all sub-tasks, suggesting that these attention heads are capable of tracking the number of unclosed parenthesis or function calls.}
    
    % Aggregated at the sub-task level, we report the median layers when the correct token's logit value is ranked within top 10 ($L_{\text{Top10}}$), top 1 ($L_{\text{Top1}}$), and consistently top 1 afterward ($L_{\text{ConsistentTop1}}$), respectively.
    % where each of the reported layers is found by taking the median of the layers in which a desired correct answer's logit behavior occurs for all prompts of a sub-task. 
    % The reported layers are zero-indexed. Based on the reported layers for each sub-task, we conjecture that the Code LM views the Three Closing Parentheses and Four Closing Parentheses as being similarly difficult tasks, with both being more difficult than the Two Closing Parentheses Task.}}
    \label{tab:attention_patterns}
\end{table*}

% \zyc{Somewhere in this paragraph you need to introduce the concept of promotion and suppression and cite prior work discussing them. @Daking can you help?}
\paragraph{Layer-level Analysis}
{To understand each layer's contribution to correct token prediction, we first measure the \emph{logit difference} between the correct token and the counterfactual token on the residual stream across all layers}. Specifically, Figure~\ref{fig:accum_subtask} showcases the logit difference across layers of the residual stream for each sub-task, averaged over prompts in the same sub-task. Figure~\ref{fig:accum_subtask2} displays the same metric but focuses on the results for the Two Closing Parentheses sub-task aggregated at a \emph{prompt type} level---here, we define each prompt type by the class constructor (i.e., \lstinline|str|, \lstinline|list|, or \lstinline|set|) and the number of open parentheses. {The results in Figure~\ref{fig:accum_subtask} show that the Code LM gains an understanding of the Two Closing Parentheses sub-task much earlier than the Three and Four Closing Parentheses sub-tasks, as evidenced by the residual stream having a positive logit difference in earlier layers. The result also corroborates our conjecture from Section~\ref{subsec:exp1} that the Code LM has more difficulty performing the Three and the Four Closing Parentheses sub-tasks. The layers with the largest contributions (defined by ``peaks'' of positive logit difference) to the residual stream for the Two, Three, and Four Closing Parentheses sub-tasks were layers 27 and 30, layers 30 and 31, and layers 31 and 26, respectively. 

The prompt type level results for the Two Closing Parentheses sub-task found in Figure~\ref{fig:accum_subtask2} confirm the positive logit difference contributions of layers 27 and 30 for all prompt types.} {For prompt types that utilize either the \lstinline|list| or \lstinline|set| class constructors, we find that the Code LM gains an understanding of the Two Closing Parentheses sub-task around layer 18. Similar behavior can be seen for the prompt types that utilize the \lstinline|str| class constructor as the number of opening parentheses becomes large, in this case having 6 open parentheses. It appears that as the prompt type becomes sufficiently difficult, whether that be through the inclusion of more open parentheses or utilization of the \lstinline|list| or \lstinline|set| constructor calls, the mid-late layers play a larger role in the promotion of the correct token.}
% \daking{It may be better to report both the range of layers along with the layer with the maximum number of times where the correct token breaks into top-10 or top-1.}\zyc{I don't understand this comment. Is it for RQ1?} 
% \zyc{Overall, I think Fig 2 deserves a bit more discussion. @Sam, you can discuss it from perspectives such as, does the different class constructors lead to different results? similarly, does the number of open parens make a difference?}



% \par 
\paragraph{Sub-Layer Analysis}
% \zyc{I will check this paragraph later as now the plot is too dense to read}
The logit difference contribution to the residual stream at a sub-layer (i.e., FF or MHA) level can be seen in Figure~\ref{fig:perlayer_subtask} and Figure~\ref{fig:perlayer_subtask2}. Similar to above, Figure~\ref{fig:perlayer_subtask} displays each of the sub-layer contributions to the residual stream aggregated at a sub-task level, while Figure~\ref{fig:perlayer_subtask2} showcases the results for the Two Parentheses Sub-task when the class constructor is \lstinline|str|.
% grouped by prompt types. 
% \zyc{Here, we should have 1-2 sentences clarifying how we obtain the logit values from sub-layers. I remember we three only clarified it in our previous meeting. Readers could be confused too. You can reuse the notations I defined in Sec 2, Methodology.}
At the sub-layer level, the logits of each sub-layer were calculated by projecting the sub-layer's immediate activation output to the vocabulary space via the logit lens.
The results in Figure~\ref{fig:perlayer_subtask} reveal the importance of both FF and MHA sub-layers for the closing parentheses task, as all sub-tasks have both FF and MHA sub-layers in their top five positively contributing sub-layers. When comparing the overall contribution of the MHA sub-layers and the FF sub-layers for all sub-tasks, \emph{the MHA sub-layers have a similar or larger positive contribution}---in fact, the most salient negative contributions to the logit difference for the Two and Three Closing Paren sub-tasks both come from MLP. Their positive contribution is especially apparent in the Three Closing Parentheses and Four Closing Parentheses sub-tasks. The MHA sub-layers that exhibit the strongest promotion of the correct tokens against the counterfactual ones for the Two Closing Parentheses, Three Closing Parentheses, and Four Closing Parentheses sub-tasks are the MHA sub-layers in layers 30 and 27, layers 30 and 31, and layers 31 and 30, respectively.

Figure~\ref{fig:perlayer_subtask2} similarly illustrates the importance of MHA sub-layers to positive logit difference. Interestingly, the patterns when the number of open parentheses is 2 and 6 respectively are pretty similar, with MHA's positive contributions peaking at layers 27 and 30, MLP's positive contributions peaking at layers 27 and 22, and MLP's negative contributions dipping significantly at layers 28 and 30. 
% \dakingrev{Finally, as shown in Figure~\ref{fig:perlayer_subtask2}, both the MHA and FF sub-layers exhibit similar positive and negative contributions across all tasks when the class constructor is \lstinline|str|.} 
% \sam{}
% \zyc{We seem to miss a discussion about Fig 4? I will check it again later when we have a better visualization}

% Given the importance of the MHA sub-layer of layer 30 for the correct token prediction in all sub-tasks, we further investigate the attention heads at this particular sub-layer to determine if they exhibit any behaviors that are generalizable for the overall Closing Parentheses task.

% across all the sub-tasks we find that the overall contribution of the MHSA sub-layers 


% with the MHSA sub-layers appearing to contribute more significantly, especially for the Three Closing Parentheses and Four Closing Parentheses sub-tasks. Additionally, we find that the layer 30 MHSA sub-layer has a large contribution for all sub-tasks.}

% For the Two Closing Parentheses Sub-task, we find that in layer 27 both the FF and MHSA sub-layers contribute while in layer 30 only the MHSA sub-layer contributes. While, for the Three Closing Parentheses sub-task, we find that in layer 30 only the MHSA-sub layer contributes and in layer 31 both sub-layers contribute. Finally, for the Four Closing Parentheses sub-task, we find that in layer 31 MHSA sub-layer contributes and layer 26 FF contributes.}
% Additionally, our experimental results reveal that while both the FF and MHA sub-layers are important for the Parenthesis Completion Task, the MHA sub-layers appear to contribute more significantly. \sam{TODO - verify that this holds for all sub-tasks}



% \subsection{Exp 3: Logit Difference of Per Layer Component Contribution to Residual Stream}\label{subsec:exp3}
\subsection{RQ 3: How do attention heads contribute to the promotion/suppression of correct tokens?}\label{subsec:exp3}

Given the insight from the experimental results of RQ2 that MHA sub-layers appear to contribute in a more significant fashion to the syntax completion task, we ran an additional experiment to see how individual attention heads contribute to the promotion or suppression of the logit difference between the correct token and the counterfactual token. We were especially interested in identifying the attention heads that had a large positive or negative contribution to the promotion of the logit difference in the previously identified MHA sub-layers. {The heat map showing the logit difference projected from individual attention layers and heads for each sub-task can be seen in Figure~\ref{fig:perhead_heatmap}. {For all sub-tasks, we identify that {most} heads have a strong positive contribution to the logit difference (marked as deep blues), whereas {only a few} heads have a small negative contribution (marked as light reds). In particular, the positive contribution was dominantly made by only a few heads.}
For the Two Closing Parentheses sub-task, we find that the largest contributing heads are $L30H0$ (i.e., layer 30, head 0) and $L27H24$ (i.e., layer 27, head 24), which both have positive contributions. {Interestingly, while the $L30H0$ attention head exhibits similar positive contribution behavior in the Three and Four Closing Parentheses sub-tasks as in the Two Closing Parentheses sub-task, we find that the $L27H24$ head negatively contributes the correct output for Three Closing Parentheses and Four Closing Parentheses sub-tasks.}

% \zyc{We need figs for the following discussion}
% After identifying the attention heads that strongly promote the correct tokens for the sub-tasks, we analyzed the attention patterns of those attention heads on various prompts, with the overall goal of identifying the function of those attention heads. 
{We identified that, despite their functional differences, $L30H0$ and $L27H24$ have similar attention patterns for all sub-tasks. Specifically, they both were found to effectively track the number of already closed parentheses by attending to the function name up to the point where the parentheses are already closed, as shown in Table~\ref{tab:attention_patterns}. However, while $L30H0$ dynamically promotes the correct number of closing parentheses based on the count of those already closed, $L27H24$ always promotes two closing parentheses regardless of the number of remaining open parentheses. In other words, this head is promoting incorrect knowledge despite being able to correctly understand the context. We summarize this the phenomenon as \emph{``incorrect knowledge association''}. Consequently, this behavior of $L27H24$ results in a negative contribution for tasks requiring three or four closing parentheses.}  


% Notably, we identified $L30H0$ (layer 30, head 0), the head with the highest positive contribution, which effectively tracks the number of closed parentheses by attending to the function name up to which the parentheses are closed. Similarly, we also identified another attention head $L27H24$ with a similar role in tracking the number of closed parentheses. However, this head is limited to monitoring only two closing parentheses and consistently promotes \emph{``))''} as the correct output, even in cases where the correct label requires three or four closing parentheses.

% The results aggregated at the task level can be seen in Figure XX. While the results for the Two Closing Parentheses sub-task can be seen in Figure YY. \sam{TODO - discuss results}

% \subsection{Exp 4: Attention Analysis of Attention Heads with Strong Contribution to Residual Stream}\label{subsec:exp5}
% \subsection{RQ 4: What are the functions of the attention heads that strongly promote/suppress correct tokens?}\label{subsec:exp4}
% After identifying the attention heads that strongly promote/suppress correct tokens for the sub-tasks, we analyzed the attention patterns of those attention heads on various prompts, with the overall goal of identifying the function of those attention heads. Notably, we identified $L27H0$ (layer 27, head 0), the head with the highest positive contribution, which effectively tracks the number of closed parentheses by attending to the function name up to which the parentheses are closed, as shown in Figure~\ref{}. Similarly, we also identified another attention head $LxxHyy$ with a similar role in tracking the number of closed parentheses. However, this head is limited to monitoring only two closing parentheses and consistently promotes \emph{``))''} as the correct output, even in cases where the correct label requires three or four closing parentheses.
% \sam{TODO - further discuss results and generate example figures}

% \begin{figure}[h] % 'h' means place the figure here
%     \centering
%     \includegraphics[width=0.5\textwidth]{LaTeX/per_head_plot_codellama_subtask1.png} % Adjust width and specify the image path
%     \caption{\sam{Attention head logit difference of the Code LM between the correct and counterfactual tokens contribution to the residual stream for the Two Closing Paren sub-task. Results are averaged over prompts of the Two Closing Paren sub-task. (Each cell in the figure indicates the contribution of a single attention head $ah$ at a layer $l$, which can be referenced as ``L$l$:H$ah$'') }}
    
%     % Logit Difference Contribution of Attention Heads for Two Closing Parentheses sub-task} % Figure caption
%     \label{fig:perhead_heatmap} % Label for cross-referencing
% \end{figure}



