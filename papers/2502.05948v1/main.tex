\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage[a4paper, total={184mm,239mm}]{geometry}

\usepackage{multirow}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage[export]{adjustbox}
\usepackage[font=small,labelfont=bf]{caption}
% \usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage[ruled,vlined]{algorithm2e}

\usepackage{multirow}
\usepackage{hhline}
\usepackage{makecell}

\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor = magenta,
    pdfpagemode=FullScreen,
}

\makeatletter
\newcommand*\titleheader[1]{\gdef\@titleheader{#1}}
\AtBeginDocument{%
  \let\st@red@title\@title
  \def\@title{%
    \bgroup\normalfont\large\centering
    \vskip-3.9em
    \@titleheader\par\egroup
    \vskip0.8em\st@red@title}
}
\makeatother

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\titleheader{\normalsize{2025 IEEE International Symposium on Circuits and Systems (ISCAS)}}



\title{
% Accuracy of Neural Networks on a Tunable Reference RRAM CiM Structure~\vspace{-10pt}
% Characterization and Mitigation of ADC Noise in RRAM-Based Compute In-Memory\vspace{-10pt}
\fontsize{22.5}{30.0}\selectfont{Characterization and Mitigation of ADC Noise by Reference Tuning in RRAM-Based Compute-In-Memory\vspace{-5pt}}
% {\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
% should not be used}
% \thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{
Ying-Hao Wei$^*$, Zishen Wan$^*$\thanks{* These authors contributed equally to this work.}, Brian Crafton, Samuel Spetalnick, Arijit Raychowdhury\thanks{This work was supported by CoCoSys, one of seven centers in JUMP2.0, a Semiconductor Research Corporation (SRC) sponsored by DARPA.} \\
\textit{School of Electrical and Computer Engineering, Georgia Institute of Technology, GA, USA}\\
\normalsize \{ywei341, zwan63, bcrafton3, sspetalnick3\}@gatech.edu, arijit.raychowdhury@ece.gatech.edu
\vspace{-5pt}}

\begin{document}

% \author{\IEEEauthorblockN{1\textsuperscript{st} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{2\textsuperscript{nd} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{3\textsuperscript{rd} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{4\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{5\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{6\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% }

\maketitle

\begin{abstract}
With the escalating demand for power-efficient neural network architectures, non-volatile compute-in-memory designs have garnered significant attention. However, owing to the nature of analog computation, susceptibility to noise remains a critical concern. This study confronts this challenge by introducing a detailed model that incorporates noise factors arising from both ADCs and RRAM devices. The experimental data is derived from a 40nm foundry RRAM test-chip, wherein different reference voltage configurations are applied, each tailored to its respective module. The mean and standard deviation values of HRS and LRS cells are derived through a randomized vector, forming the foundation for noise simulation within our analytical framework. 
Additionally, the study examines the read-disturb effects, shedding light on the potential for accuracy deterioration in neural networks due to extended exposure to high-voltage stress.  This phenomenon is mitigated through the proposed low-voltage read mode. Leveraging our derived comprehensive fault model from the RRAM test-chip, we evaluate CIM noise impact on both supervised learning (time-independent) and reinforcement learning (time-dependent) tasks, and demonstrate the effectiveness of reference tuning to mitigate noise impacts.

% The model predicts an accuracy of 87\% for CIFAR-10 (VGG-8), compared to a baseline accuracy of 90\%, and a win rate of 78.2\% for Gridworld, against a baseline of 95.5\%. Notably, certain applications exhibit varying performance outcomes under per-module reference configurations, demonstrating improved feasibility under per-ADC configurations. Specifically, in the latter setting, Imagenet (ResNeXT50-32x4d) achieves an accuracy of 82\% (baseline = 83\%), while the time-dependent drone autonomous navigation task retains 76.8\% of its original performance.
\end{abstract}

% \begin{IEEEkeywords}
% RRAM,CIM,ADC,read disturb,DNN,PEDRA
% \end{IEEEkeywords}

\section{Introduction}
% \blue{Flow: What is the problem?  Why this problem is important? What's the current solutions? what's the limitations of these solutions? What do we propose? Contributions. +Fig}

% As the adoption of machine learning and deep neural networks continues to expand, there is a growing demand for increased computational capacity and efficiency. However, in certain scenarios, power availability poses a significant constraint, particularly in edge computing environments. To address this challenge, non-volatile memory devices, characterized by minimal to no power consumption during data retention, present a promising solution. These includes RRAM~\cite{RRAM}, MRAM~\cite{MRAM}, PCM~\cite{Shimeng_PCM}, and other floating-gate devices~\cite{JHaslerRoadmapNeuromorphic}. In addition to reducing power consumption in memory components, another substantial reduction in power can be achieved through compute in-memory (CIM) \cite{CiM,SRAM_CIM} techniques.
With the surge in machine learning and deep neural network applications, the demand for enhanced computational capability and efficiency has escalated. Yet, in settings like edge computing environments, power and efficiency become critical constraints. Non-volatile memory devices, distinguished by their negligible power consumption during data retention, emerge as potential answers to this quandary. These encompass RRAM~\cite{RRAM}, MRAM~\cite{xiao2024adapting}, PCM~\cite{Shimeng_PCM}, FeFET~\cite{shou2023see}, and floating-gate devices~\cite{JHaslerRoadmapNeuromorphic}. Beyond the power savings offered by these memory components, compute-in-memory (CIM) architecture~\cite{CiM,SRAM_CIM,peng2019dnn+,chang202373,lele2023heterogeneous} promises further reductions in power consumption.

% RRAM technology, while offering notable advantages, is not without its limitations. These include, such as, finite On-Off ratio, cell-to-cell variance, issues related to stuck-at-fault~\cite{RRAM_SAF}, and shifts in the on-off ratio. In addition to noise introduced by RRAM technology, systemic errors arise from the CiM architecture, including non-linearity in ADCs and variation from one ADC to another.
While RRAM technology boasts several merits, it is also marred by challenges like limited on/off ratios, inter-cell variations, stuck-at-fault and shifts in resistance~\cite{RRAM_SAF,wan2022rram,crafton2022improving}. Beyond RRAM-induced noise, CIM brings forth systemic inaccuracies, such as ADC non-linearity and variance across ADCs.

% The existing solutions do address concerns regarding on-off ratio, non-linearity in ADCs, and module partitioning\cite{NeuroSim}, or remapping bad cells\cite{bad_cell_remap}. However, they fall short in accounting for ADC-to-ADC variation, which could potentially lead to issues such as reduced yield and inaccurate outcomes if left unaddressed. Alternative solutions from the software side, such as injecting noise during training to enhance robustness\cite{NeuroSim}\cite{noisy_train}, would require knowing the noise's actual impact.  Yet, a comprehensive consideration of noise entails a more complex simulation. Therefore, our objective is to construct a simplified model that considers all major sources of noise. This model will evaluate the potential of a tunable reference for the CiM structure on a per-module basis and assess its impact on various applications.

Recent efforts have attempted to address concerns regarding on/off ratio, ADC non-linearity, module partitioning, and the remapping of defective cells~\cite{bad_cell_remap,huang2022automated,haensch2023compute}. Yet, these solutions often overlook variations between ADCs, which, if neglected, can culminate in challenges like diminished yields and skewed results. On the software aspect, tactics like noise injection during training to bolster robustness~\cite{NeuroSim,noisy_train,wan2023berry} demand a grasp of the actual noise impact. However, a holistic noise analysis necessitates an intricate simulation. Hence, our goal is to create a streamlined model that encompasses primary noise sources. This model aims to gauge the feasibility of an adjustable reference for CIM structures on a module-wise scale, analyzing its influence on diverse tasks. Additionally, given that we operate the test chip under relatively low-stress conditions for the RRAM cells, we demonstrate minimal resistance shifts in these cells, rendering read disturbances almost inconsequential.

% Additionally, our RRAM test-chip operations under relatively low-stress conditions for RRAM cells have exhibited minimal resistance deviations, rendering read disturbances almost inconsequential~\cite{RRAM_CiM_original_2}.

\begin{figure}[tb]
    \begin{center}
        \includegraphics[width=\linewidth]{Figs/intro_overview.pdf}
        \caption{Impact of noises on different scenarios, including supervised learning (time-independent) and unsupervised learning (time-dependent). While some adapted effectively to per-module reference adjustments, others necessitated a more refined per-ADC tuning.}
        \label{fig:intro}
        \vspace{-10pt}
    \end{center}
\end{figure}

% In this paper, we will be  

% This paper, therefore, makes the following contributions:
% \begin{itemize}
%     \item Devised a technique for extracting the overall behavior, encompassing both the ADC and RRAM cells, and implemented it in the simulation of established neural networks. We develop a methodology to encapsulate the collective behavior of both ADCs and RRAM cells, integrating the CIM noise into the cross-layer device-to-application analysis framework.
%     \item Quantified the effects of resistance shifting induced by read disturb, with the method stated in the previous item.
%     \item Evaluated the precision of various networks when subjected to per-module reference tuning.
    
% \end{itemize}
This paper, therefore, makes the following contributions:
\begin{itemize}
    \item We develop a methodology to encapsulate the behavior of ADCs and RRAM cells, integrating the CIM noise into the cross-layer device-to-application analysis framework.
    \item We analyze and quantify the impact of CIM resistance shifts caused by read disturbances, and evaluate this in the context of supervised (time-independent) and reinforcement (time-dependent) learning scenarios.
    \item We evaluate the accuracy of different workloads and showcase the efficacy of per-module reference tuning.
\end{itemize}


% Reference example: \cite{crafton2022improving}
\section{Background and Motivations}
\subsection{Compute-In-Memory}
To enhance the computational efficiency of neural networks, particularly in the context of Vector Matrix Multiplication (VMM), CIM structures are increasingly employed. These structures capitalize on encoding the input vector and distributing it across the wordlines (WLs) of memory cells. Subsequently, when a WL is activated, a greater current flows through the Low Resistance State (LRS) as compared to the High Resistance State (HRS) of the RRAM device. The resulting currents are then aggregated through Kirchhoff's Current Law and quantized through ADCs~\cite{SRAM_CIM,peng2019dnn+}.

The experiments conducted in this study adhere to and are validated by the CIM test-chip prototype~\cite{RRAM_CiM_original,RRAM_CiM_original_2,RRAM_resistance_dist,yoon202129}. Fig.~\ref{fig:read_struct} provides an overview of the read circuitry. The output voltage is subsequently decoded using a 4-bit flash ADC, incorporating tunable references. Such a design configuration serves to minimize the required bitline (BL) voltage, thereby reducing both read disturb susceptibility and overall power consumption.

% \begin{figure}[tb]
%     \begin{center}
%     \vspace{-5pt}
%         \includegraphics[width=0.65\linewidth]{Figs/low_read_circuit.png}
%         \caption{The circuit for low read operation.}
%         \vspace{-20pt}
%         \label{fig:read_struct}
%     \end{center}
% \end{figure}

\begin{figure}[tb]
    \begin{center}
    \vspace{-5pt}
        \includegraphics[width=.95\linewidth]{Figs/RRAM_macro_v2.pdf}
        \caption{(a) The RRAM macro topology and (b) its symbolic read circuit schematic. Figure adapted from~\cite{RRAM_CiM_original}.}
        \vspace{-20pt}
        \label{fig:read_struct}
    \end{center}
\end{figure}


\subsection{Resistive Random Access Memory (RRAM)}
RRAM is a type of non-volatile memory capable of toggling between LRS and HRS through the application of distinct biases, predicated on establishing or interrupting the conduction bridge~\cite{wan2022compute,yu2020compute}. The initial formation of the conduction bridge necessitates a relatively higher bias. This intrinsic characteristic imposes a fundamental constraint on the maximum size of the input vector. Another significant challenge lies in cell-to-cell variance, where resistance inconsistencies further exacerbate the aliasing concern. To quantify the variation and on/off ratio~\cite{RRAM_2}, we will be experimenting on RRAM cells from recent RRAM test-chip prototype on 40nm foundry RRAM arrays~\cite{RRAM_CiM_original,RRAM_CiM_original_2,RRAM_resistance_dist,yoon202129}. 

Moreover, subjecting an RRAM cell to stress induces shifts in its resistance\cite{Shimeng_RRAM_shift}\cite{Shimeng_RRAM_shift_2}. Our read structure, akin to a set operation albeit with differing stress levels, typically exhibits a trend wherein the resistance of HRS shifts towards LRS, while LRS cells tend to remain relatively stable. Such behavior leads to a narrowing of the on-off ratio.

\section{Model For Simulation}
This section describes how we generate the reference for ADCs for optimized accuracy (Sec.~\ref{subsec:adc}) and how the noise is extracted from the test-chip for analysis (Sec.~\ref{subsec:measure}, \ref{subsec:noise}).
% Here goes methods
\subsection{Generating ADC Configuration}
\label{subsec:adc}
%This section outlines the targeting of configurations and the generation of code-maps. 
Achieving accuracy in ADC outputs hinges crucially on the precision of the reference settings. We aim to achieve this by introducing random vectors and determining the optimal setting. The process of tuning the internal reference generator encompasses three primary parameters: offset, step size, and BL target voltage, whose designs are also demonstrated in \cite{RRAM_CiM_original}. The offset and step size dictate the reference voltages for the comparators, while the BL target voltage designates the sensing range for different voltage levels.

We define a sequence of nine consecutive cells within a column as an ``acc-9 group." Following this definition, we apply 256 randomly generated 9-bit vectors to every ``acc-9 group" within a matrix of 81 rows and 64 columns, covering the full scope of the 256 columns.

% Given that a 4-bit ADC produces 16 output states, while the goal is to activate a maximum of 9 WLs (representing 10 different voltage states, due to hardware constraints), a mapping of ADC outputs to actual results is necessary. Instead of having a fixed mask for mapping\cite{RRAM_CiM_original_2}, we've employed the absolute binning method. For instance, in Fig.\ref{fig:mod56_assign}, the golden value 5 is represented by voltage level 15 a total of 2190 times (constituting 31.53\% of all instances of golden value 5), whereas golden value 6 is represented 1700 times (or 90.23\% of all instances of golden value 6). Despite the percentages, voltage level 15 is mapped to value 5, as it is the more prevalent representation.

Considering the 4-bit ADC's capability to generate 16 distinct output states and the intention to activate a maximum of 9 WLs (leading to 10 unique voltage states), there is a critical need for reliable mapping of ADC outputs to their actual corresponding values. Instead of adopting a fixed mask for this mapping~\cite{RRAM_CiM_original_2}, we utilize the absolute binning approach. For example, in Fig.~\ref{fig:mod56_assign}, the golden value 5 corresponds to voltage level 15 on 2190 times (31.53\% of total golden value 5 occurrence). In contrast, the golden value 6 is represented 1700 times (90.23\% total golden value 6 occurrence). Despite these percentages, voltage level 15 is associated with value 5, as it remains the dominant representation.

\begin{figure}[tb]
    \begin{center}
    \vspace{-8pt}
        \includegraphics[width=8cm]{Figs/mod_56_code_mapping_exp.png}
        \vspace{-2pt}
        \caption{Mapping voltage states to actual value, where x-axis is golden value and y-axis is voltage states. Red boxes indicate that the voltage state is assigned to that specific target code.}
        \vspace{-10pt}
        \label{fig:mod56_assign}
    \end{center}
\end{figure}

% In the present configuration, the reference is globally applied, meaning that the same step size and offset setting is deployed across all modules. Further exploration is conducted to investigate the feasibility of employing distinct reference settings for different scopes, including per-module and per-ADC configurations.

In the current setup, the reference is applied globally. This implies that the same step size and offset settings are utilized across all modules. We further probe into the possibility of adopting unique reference settings for varied scales, including individual module and per-ADC setups.

% Fig. \ref{fig:golden_hist} provides an overview of the frequency of each correct answer. It's evident that a considerable portion of occurrences falls within the lower sums. This observation remains valid, especially as ReLU layers are introduced, which tend to generate more zeros. Consequently, we focus on accurately mapping only the first 5 and 6 states, while letting higher sums reach saturation, such as in Fig. \ref{fig:mod56_assign}.

Fig. \ref{fig:golden_hist} showcases the frequency distribution of each correct response. It's discernible that a significant number of instances are concentrated within the lower sums. This trend becomes particularly pronounced when ReLU layers are incorporated, as they often produce a higher number of zeros. As a result, our efforts primarily concentrate on the precise mapping of the initial 5 and 6 states, while permitting higher sums to attain saturation, as depicted in Fig. \ref{fig:mod56_assign}.

% Happy figure here.
\begin{figure}[tb]
    \begin{center}
    \vspace{-5pt}
        \begin{minipage}{9cm}
            \includegraphics[width=9cm]{Figs/golden_hist.png}
        \end{minipage}
        \caption{The histogram of the correct output, under a CIM array (9 $\times$ 64 accumulate groups, spams across 8 ADCs) of 50\% HRS and 50\% LRS. The input consists of 256 different combinations among 9 WLs.}
        \label{fig:golden_hist}
        \vspace{-10pt}
    \end{center}
\end{figure}


\subsection{Measuring Effective Bit}
\label{subsec:measure}
% This section shows how the modeling was done - namely, by fitting over bits, per-ADC

% To construct a realistic model that accurately encompasses both cell variability and ADC effects, we introduce the concept of effective bits, which contains information of the mean and variation of the non-ideal distribution. The process of obtaining these effective bits involves several steps. Specifically, we sampled each accumulation group, which consists of nine cells (as only up to 9 WLs could be activated simultaneously), with 256 distinct random combinations of WLs. By fitting the relationship between inputs and outputs, as illustrated in Fig. \ref{fig:AGF_responses}, we determine the effective bit for each cell within an accumulation group. This component will be referred to as the static error component. However, it's important to note that the fitting process is not flawless and introduces an additional source of error, as demonstrated in Fig. \ref{fig:AGF_responses}(b). Consequently, we gather the distribution of the residue error and incorporate it as a source of dynamic error. This segment will be introduced as the dynamic error component. The distribution of effective bits across a module is depicted in Fig. \ref{fig:EBanderr}, highlighting the substantial impact of different ADCs on both the effective bits and associated errors.

To construct a realistic model that accurately encompasses both cell variability and ADC effects, we introduce the ``effective bits" concept. This encapsulates the mean and variability of a non-ideal distribution. The derivation of these effective bits comprises multiple stages. Notably, we sampled each accumulation group, comprised of nine cells, using 256 distinct random WL combinations. By analyzing and fitting with minimizing the sum of the absolute difference between input and output patterns, as in Fig.~\ref{fig:AGF_responses}, we pinpoint the effective bit for each cell within an accumulation group. We'll reference this element as the static error component. It's crucial to emphasize that the fitting methodology isn't without its imperfections and introduces an auxiliary error, as depicted in Fig.~\ref{fig:AGF_responses}\textcolor{blue}{(b)}. Hence, we aggregate the distribution of this residual error and assimilate it as a dynamic error source. The distribution of effective bits throughout a module is illustrated in Fig.~\ref{fig:EBanderr}, underscoring the profound influence of disparate ADCs on both the effective bits and their accompanying errors.

\begin{figure}[tb]
    \begin{center}
    \vspace{-5pt}
        \includegraphics[width=8.5cm]{Figs/fit_by_bit_compare.png}
        \caption{Measuring the effective bits of (a) actual v.s. golden response and (b) actual v.s. fitted response, under 512 WL combinations.}
        \vspace{-10pt}
        \label{fig:AGF_responses}
    \end{center}
\end{figure}

% \begin{figure}
%     \centering
%     \begin{subfigure}[tb]{.96\linewidth}
%         \includegraphics[width=\linewidth]{Figs/fit_by_bit_ActualVSGolden.png}
%             \caption{}
%             % \label{fig:AVG_response}
%     \end{subfigure}
    
%     \begin{subfigure}[tb]{.96\linewidth}
%         \includegraphics[width=\linewidth]{Figs/fit_by_bit_ActualVSFitted.png}
%             \caption{}
%             % \label{fig:AVF_response}
%     \end{subfigure}
%     \caption{(a) Actual response V.S. Golden response, under 512 WL combinations (b)Actual response V.S. Fitted response, under 512 WL combinations}
%     \label{fig:AGF_responses}
% \end{figure}


\begin{figure}[tb]
    \begin{center}
    % \vspace{-5pt}
        \begin{minipage}{7.5cm}
            \includegraphics[width=7.5cm]{Figs/Effective_Bit_and_Error.png}
        \end{minipage}
        \caption{The effective bit (top) and error (bottom) across the module, contains results from all 8 ADCs. The error is calculated by accumulating the difference between golden and fitted output of 256 samples.}
        \label{fig:EBanderr}
        \vspace{-15pt}
    \end{center}
\end{figure}


\subsection{Noise Injection Methodology}
\label{subsec:noise}
% This section talks about how simulations were carried out. Mainly recycle the data we gathered.
% The static noise is injected at the beginning, as done by the following procedure. First, we extract the 0/1 effective bit distribution from 10 different modules (80 ADCs). Then, we utilize the mean and standard deviation to generate random normal distributions for both 0s and 1s. Note that distributions are generated independently. Finally, we substitute the original bits with these effective bits. Additionally, we reuse the distributions when we run out of modules to assign. 
The static noise is initially injected, as follows. Firstly, we retrieve the 0/1 effective bit distribution from 10 distinct modules (encompassing 80 ADCs). We then harness the mean and standard deviation to spawn random normal distributions for both 0s and 1s. It's noteworthy that these distributions are curated independently. Finally, we replace the original bits with the derived effective bits. In some cases, tuning references on a per-module basis (involving 8 ADCs) might not be successful, necessitating a shift to per-ADC tuning. The methodology remains largely analogous, albeit with the generation of configurations tailored for each ADC.
% draw a figure to elaborate.

% here goes results
\section{Impact Of CIM Read Disturb}
This section quantifies (Sec.~\ref{subsec:quantify}) and evaluates (Sec.~\ref{subsec:read_disturb}) the potential impact of read disturb under different scenarios. 

% but also demonstrates that the risk is negligible when operating at an ultra-low bias.
\subsection{Quantifying Read Disturb}
\label{subsec:quantify}
% % This section talks about how we shorten the test, as well as a graph of how many times a weight is used in a convolution layer
% In this architecture, the read operation closely resembles a set operation, which renders HRS cells susceptible to read disturb. As demonstrated in \cite{Shimeng_RRAM_shift}, it has been observed that HRS cells exhibit shifts over time, while LRS cells remain relatively stable. In Figure \ref{fig:R_shift_VS_BL}, different levels of stress are applied with varying BL voltages, while the WL voltage is held constant at 1.1V. Notably, the bias voltage for reading is significantly lower, typically hovering around 100mV (with a maximum of 300mV). The outcome, as depicted in Figure \ref{fig:R_shift_VS_BL}, ensures minimal resistance shift for a duration exceeding 250 seconds under standard temperature conditions.

In the CIM architecture, the reading process bears significant resemblance to a set operation. This makes HRS cells particularly prone to read disturbances. We observe that HRS cells tend to undergo shifts, while LRS cells mostly maintain their stability over time. Fig.~\ref{fig:R_shift_VS_BL} shows the effects of different stress levels applied at varied BL voltages, keeping the WL voltage consistently at 1.1V. It's worth noting that the bias voltage used for reading is considerably lower, often around 100mV and peaking at 300mV. The results, as illustrated in Fig.~\ref{fig:R_shift_VS_BL}, confirm a minimal resistance shift for periods exceeding 250 seconds under ambient temperature conditions.

% \begin{figure}[tb]
%     \begin{center}
%     \vspace{-5pt}
%         \begin{subfigure}[t]{.9\linewidth}
%             \includegraphics[width=\linewidth]{Figs/R_shift_VS_BL.png}
%             \vskip -0.1in
%             \caption{}
%         \end{subfigure}
%         \begin{subfigure}[t]{.9\linewidth}
%             \includegraphics[width=\linewidth]{Figs/tlread_R_shift.png}
%             \vskip -0.1in
%             \caption{}
%         \end{subfigure}
%         \caption{(a) Resistance shift under variance BL voltage and Fixed WL voltage under normal temperature. A cycle is (5/64M) second. (b) Resistance shift under WL=1.1V, BL=0.3V, over 250 seconds.}
%         \label{fig:R_shift_VS_BL}
%         \vspace{-10pt}
%     \end{center}
% \end{figure}

\begin{figure}[t!]
    \begin{center}
    \vspace{-5pt}
        \begin{minipage}{8.5cm}
            \includegraphics[width=8cm]{Figs/shift.png}
        \end{minipage}
        \caption{(a) Resistance shift under variance BL voltage and Fixed WL voltage under normal temperature. A cycle is (5/64M) second. (b) Resistance shifts under WL=1.1V, BL=0.3V, over 250 seconds.}
        \label{fig:R_shift_VS_BL}
        \vspace{-18pt}
    \end{center}
\end{figure}

% Nearly no resistance shift was observed. 

% \section{RRAM Shifting Results}
\subsection{Read Disturb Impact of DNN Models}
\label{subsec:read_disturb}
We specifically opt for a module equipped with a relatively linear ADC setup to assess the impact of shifting. This choice stems from the fact that more precise modules tend to exhibit a more discernible disparity in noise levels when compared to initially noisy modules. To speed up the read disturb process, we select higher stress, and hence, in Fig.~\ref{fig:eb_map}, we present the effective bit values before and after shifting, conducted under conditions involving a 1300mV BL and 10 cycles of 50k stresses, distributed uniformly across all cells. The trend in effective bit means across shifting cycles is portrayed in Fig.~\ref{fig:eb_vs_cycle}, further affirming the greater susceptibility of HRSs to read disturbance. % It's noteworthy that not only has the mean of zeros increased, but the standard deviation of both zeros and ones has also risen.

% We specifically opted for a module equipped with a relatively linear ADC setup to assess the impact of shifting. This choice stems from the fact that more precise modules tend to exhibit a more discernible disparity in noise levels when compared to initially noisy modules. In Fig.~\ref{fig:eb_map}, we present the effective bit values before and after shifting, conducted under conditions involving a 1300mV BL and 10 cycles of 50k stresses, distributed uniformly across all cells. The trend in effective bit means across shifting cycles is portrayed in Fig.~\ref{fig:eb_vs_cycle}, further underscoring the heightened vulnerability of HRSs to read disturbances. It's pivotal to mention that the average of zeros has increased, and concurrently, the standard deviation for both zeros and ones has expanded.

% Subsequently, following the extraction of effective bit shifts, we incorporate the resistance shift into our VGG-8 network, utilizing parameters from this specific module. The result is a decline in the accuracy of CIFAR-10, dropping from 87\% to 66\%.
Following the determination of effective bit shifts, we incorporate the resistance shifts into VGG-8, based on the parameters measured from module. We observe that the shifts result in CIFAR-10 accuracy drop from 87\% down to 66\%.

%figure template
\begin{figure}[t!]
\centering
% \vspace{-0.08in}
    \begin{subfigure}[t]{.48\linewidth}
          \includegraphics[width=\linewidth]{Figs/shift/Pre-shift_effective_shift_map.png}
          \vskip -0.1in
          \caption{}%   For comparison, a non-adversarial MRFedRL achieves a $win\%=98.8\%~(p_{sa}=1.18\%)$}
          \label{fig:eb_preshift}
    \end{subfigure}
    \hskip -0.05in
        \begin{subfigure}[t]{.48\linewidth}
          \includegraphics[width=\linewidth]{Figs/shift/Post-shift_10_cycle.png}
          \vskip -0.1in
          \caption{}
          \label{fig:eb_postshift}
    \end{subfigure}
    \vspace{-0.03in}
    \caption{(a) Effective bit map for pre-shift, (b) Effective bit map for post-shift.}
    \vskip -0.15in
    \label{fig:eb_map}
\end{figure}

\begin{figure}[t!]
    \begin{center}
    \vspace{-5pt}
        \begin{minipage}{8cm}
            \includegraphics[width=8cm]{Figs/shift/Effective_bits_mean_VS_shifting_cycle.png}
        \end{minipage}
        \caption{Effective bit mean of HRS(0) (orange)/LRS(1) (blue) and accuracy of CIFAR-10 (red) v.s. cycle.}
        \label{fig:eb_vs_cycle}
        \vspace{-10pt}
    \end{center}
\end{figure}


\section{Impact of CIM Noise: Device to Application}
% This section shows the susceptibility to noise in different applications, which includes supervised learning and reinforcement learning in different complexities.

This section explores CIM noise impact across various applications, including supervised learning (Sec.~\ref{subsec:supervised}) and reinforcement learning (Sec.~\ref{subsec:RL}) paradigms with varying complexities.

\subsection{Supervised Learning (Time-Independent System)}
\label{subsec:supervised}
Supervised learning involves training a model using labeled data, where the correct answer is provided during training. We train the models of VGG-8 (on CIFAR-10) and ResNeXt-50$\times$32d~\cite{resnext50_32x4d_paper} (on Imagenet-1K), utilizing a modified version of Neurosim~\cite{NeuroSim}. This approach incorporates layer scaling and WAGE quantization~\cite{WAGE_originial_paper}. Tab.~\ref{tb:supervise_learning} presents the accuracy achieved with different bit configurations for weights and activations.

% When applied with per-module reference tuning in ImageNet (ResNeXT50-32x4d), the network failed. Hence, it proves insufficient to set up reference at a per-module level. However, when we choose to tune at a per ADC level, it would be possible to retain its accuracy. In this setting, we use a relatively smaller validation set (20000 images) to show the result. When quantized, the accuracy is 83\%, after noise injection, it would be 82\%.

When applied with per-module reference tuning on ResNeXt50-32$\times$4d (on ImageNet), the network encounters failures, suggesting that per-module level reference setup might be inadequate. However, opting for per-ADC level tuning allows us to maintain accuracy. With quantization, the accuracy stood at 83\%, which marginally dipped to 82\% post-noise injection.

\begin{table}[!htb]
%\vspace{5pt}
    \caption{Experimental configurations and accuracy of CIFAR-10/VGG-8 and Imagenet/ResNeXT50-32$\times$4d models.}
    \begin{subtable}{.5\linewidth}
        \centering
        \begin{tabular}{| c   c | c | c | }
        \hline
  \multicolumn{2}{|c|}{Quantize}&\multicolumn{2}{c|}{Weight} \\  \cline{3-4}
  \multicolumn{2}{|c|}{only}    &    4b     &    5b          \\  \hline
         \parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{Activate}}}
 &\multicolumn{1}{|c|}{5b}& 73.7\%  &  89.0\%         \\  \cline{2-4}
 &\multicolumn{1}{|c|}{6b}& 75.1\%  &  89.6\%         \\  \cline{2-4}
 &\multicolumn{1}{|c|}{7b}& 74.7\%  &  90.0\%         \\  \cline{2-4}
 &\multicolumn{1}{|c|}{8b}& 75.0\%  &  89.9\%         \\  \hhline{|====|}
  \multicolumn{2}{|c}{baseline}    &  \multicolumn{2}{|c|}{90.03\%} \\ \hline
  \multicolumn{2}{|c}{\makecell{noise-injected\\(per-module)}}&\multicolumn{2}{|c|}{87.7\%} \\ \hline
        
        \end{tabular}
        \vspace{5pt}
        \caption{CIFAR-10 / VGG-8}
    \end{subtable}%
        \begin{subtable}{.5\linewidth}
        \centering
        \begin{tabular}{| c   c | c | c | }
        \hline
  \multicolumn{2}{|c|}{Quantize}&\multicolumn{2}{c|}{Weight}  \\  \cline{3-4}
  \multicolumn{2}{|c|}{only}    &    6b     &    8b           \\  \hline
         \parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{Activate}}}
 &\multicolumn{1}{|c|}{6b} &  0\%  &   0\%              \\  \cline{2-4}
 &\multicolumn{1}{|c|}{8b} & 29\%  &  75\%              \\  \cline{2-4}
 &\multicolumn{1}{|c|}{10b}& 35\%  &  78\%              \\  \cline{2-4}
 &\multicolumn{1}{|c|}{12b}& 36\%  &  78\%              \\  \hhline{|====|}
  \multicolumn{2}{|c}{\makecell{baseline\\(reduced-set)}}    &  \multicolumn{2}{|c|}{83\%} \\ \hline
  \multicolumn{2}{|c}{\makecell{noise-injected\\(per-ADC,\\reduced-set)}}
  &\multicolumn{2}{|c|}{82\%} \\ \hline     
        \end{tabular}
        \vspace{5pt}
        \caption{Imagenet-1k / ResNeXT50-32x4d}
    \end{subtable}%
    \label{tb:supervise_learning}
    \vspace{-10pt}
\end{table}

\subsection{Reinforcement Learning (Time-Dependent System)}
\label{subsec:RL}

\begin{table}[t!]
\caption{GridWorld win rate/mean step under different bit settings.}
\centering
    \begin{tabular}{| c   c | c | c | c | }
    \hline
\multicolumn{2}{|c|}{Quantize}&\multicolumn{3}{c|}{Weight}     \\  \cline{3-5}
\multicolumn{2}{|c|}{only}    &  2b    &    4b    &    6b       \\ \hline
\parbox[t]{2mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{Activate}}}
&\multicolumn{1}{|c|}{5b}& 4.8\% / 3.66 &  4.8\% / 3.66 & 94.9\% / 6.51 \\ \cline{2-5}
&\multicolumn{1}{|c|}{6b}& 4.8\% / 3.66 & 95.5\% / 6.54 & 95.3\% / 6.52 \\ \cline{2-5}
&\multicolumn{1}{|c|}{8b}& 4.8\% / 3.66 & 95.5\% / 6.55 & 95.3\% / 6.52 \\ \hhline{|=====|}
\multicolumn{2}{|c}{\makecell{Noise-injected\\(per-module)}}&\multicolumn{3}{|c|}{78.2\% / 6.17} \\ \hline
    \end{tabular}
    \label{tb:gridworld_WA_VS_acc}
\end{table}

\begin{table}[t!]
\caption{Drone autonomous navigation performance evaluation.}
\centering
    \begin{tabular}{| c | c | }
    \hline
    Baseline MSF      & 1215.65m \\ \hline
    Quantize Only MSF    &  899.38m \\ \hline
    Per-module tuning MSF &  $<$ 20m  \\ \hline
    Per-ADC tuning MSF  & 690.99m  \\ \hline
    \end{tabular}
    \label{tb:PEDRA_performance}
    \vspace{-10pt}
\end{table}

% Reinforcement learning is a machine learning paradigm wherein an agent learns to make sequential decisions through interactions with an environment, receiving feedback in the form of rewards or penalties based on its actions. This iterative process facilitates the acquisition of optimal strategies over time. We have introduced two distinct environments of varying complexity: Gridworld and PEDRA, an autonomous navigation simulation tailored for UAVs \cite{PEDRA_paper,Z_gridworld}.
Reinforcement learning (RL) empowers agents to optimize decision-making by interacting with environments and receiving performance feedback as rewards or penalties. Over iterations, agents refine strategies to maximize rewards~\cite{krishnan2019quarl}. We evaluate the impact of CIM noise on two RL-based template problems: Grid World and drone autonomous navigation.

% In Grid World, the objective is to reach a goal while avoiding pitfalls and potential exits from the map. We maintained a fixed map size of 10x10, with the network's input consisting of 8 binary values representing potential collisions in four directions and the direction of food. The player's movement is restricted to these four directions. A compact MLP network, featuring two fully connected layers, underwent training through reinforced learning, enabling it to engage with the game. 
% Tab.~\ref{tb:gridworld_WA_VS_acc} provides the outcomes, detailing the average win rate and steps required to win 10,000 games with the quantized network.
 
% We adopt 4-bit weight and 6-bit activation, and after injecting per-module noise the average win rate falls to 78.2\% and the average step to win is 6.17 steps.
 
We begin our experimentation with a simpler but popular navigation problem of Grid World~\cite{wan2021analyzing,wan2022frl}. The environment consists of a $n\times n$ grid, and the goal of agent is to reach the goal and avoid getting trapped in the hole. We trained an MLP network with two fully connected layers, where inputs encompass 8 values that indicate possible collisions and goal directions. Tab.~\ref{tb:gridworld_WA_VS_acc} details the average win rates and steps taken across 10K missions using the quantized network on CIM.

We then experiment on a more complex drone navigation problem in 3D realistic environments~\cite{PEDRA_paper,wan2022analyzing,hsiao2023mavfi}. During the task, the drone is required to navigate across the environment avoiding obstacles. The monocular image captured from the front-facing camera is taken as the state of the RL problem. We use a perception-based probabilistic action space with 25 actions, and 8-bit quantized C3F2 model with 3 Conv layers and 2 FC layers. To quantify the performance, we use Mean Safe Flight (MSF) which is the average distance traveled by the drone before collision. The results are shown in Tab~\ref{tb:PEDRA_performance}, it is well observed that RRAM noise degrades both systems' performance and per-module/per-ADC tuning is effective in maintaining system robustness.

% In PEDRA, the primary aim is to navigate a drone through a hallway, emphasizing longevity without encountering obstacles. Given the absence of a specific goal, extended survival denotes superior performance. The input stream comprises RGB images in a 320x180x3 format, while the output encompasses 25 potential actions, each dictating a distinct directional move.

% The network architecture adopted for PEDRA features three convolutional layers, followed by two fully connected layers(C3F2). We initiated training with Reinforced Learning under floating-point precision, followed by quantizing weights to 8-bit and activations to 12-bit, all without necessitating retraining. The performance metrics are summarized in Table \ref{tb:PEDRA_performance}. As per-module tuning proved impractical, we opted for per-ADC tuning as a pragmatic alternative.

% \textcolor{red}{(what are the takeaways or conclusions drawn from experiments? this should be the most important part.)}

\section{Conclusion}
Noise in CIM remains a critical concern. To address that, we build up a simulation that extracts the noises from the 40nm RRAM test-chip. Through the analysis, we infer several key findings: complex problems are more susceptible to noise and need more precision on the hardware, especially those that are time-dependent, are more susceptible to noise than supervised learning paradigms. The adverse effects of read disturbances can be mitigated when operations occur in low-read scenarios.

% Through our detailed simulation, we inferred several key findings. Firstly, intricate problems are more vulnerable to noise and demand heightened hardware precision. Secondly, reinforcement learning systems, especially those that are time-dependent, are more susceptible to noise than supervised learning paradigms. Lastly, the adverse effects of read disturbances can be mitigated when operations occur in low-read environments

% Undone:
% conclusion
% the figure for reading
% 20 references

% \section{Acknowledgement}
% This work was in part supported by CoCoSys, one of seven centers in JUMP2.0, a Semiconductor Research Corporation (SRC) sponsored by DARPA.


\bibliographystyle{ieeetr}
\bibliography{ref}

\end{document}
