%File: formatting-instructions-latex-2025.tex
%release 2025.0
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai25}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{pifont}
\usepackage[table]{xcolor}

\newcommand{\cmark}{\textcolor{green!80!black}{\ding{51}}}
\newcommand{\xmark}{\textcolor{red}{\ding{55}}}
%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2025.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai25.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash

\NewDocumentCommand\emojifloodsynth{}{\includegraphics[scale=0.25]{Figure/flood_synth.jpg}}
\title{\emojifloodsynth MultiFloodSynth: Multi-Annotated Flood Synthetic Dataset Generation}

\author{
    %Authors
    % All authors must be in the same font size and format.
    YoonJe Kang\textsuperscript{\rm 1}\equalcontrib,
    Yonghoon Jung\textsuperscript{\rm 1}\equalcontrib,
    Wonseop Shin\textsuperscript{\rm 1}\equalcontrib,
    Bumsoo Kim\textsuperscript{\rm 1}\equalcontrib,
    Sanghyun Seo\textsuperscript{\rm 1}\thanks{Corresponding author}\\
}
\affiliations{
    %Afiliations
    \textsuperscript{\rm 1}Chung-Ang University, Republic of Korea\\
    % If you have multiple authors and multiple affiliations
    % use superscripts in text and roman font to identify them.
    % For example,

    % Sunil Issar\textsuperscript{\rm 2}, 
    % J. Scott Penberthy\textsuperscript{\rm 3}, 
    % George Ferguson\textsuperscript{\rm 4},
    % Hans Guesgen\textsuperscript{\rm 5}
    % Note that the comma should be placed after the superscript
    
    % email address must be in roman text type, not monospace or sans serif
    \{bluejay100, dydgns2017, wonseop218, bumsookim, sanghyun$^\dagger$\}@cau.ac.kr\\
%
% See more examples next
}

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

\iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{My Publication Title --- Multiple Authors}
\author {
    % Authors
    First Author Name\textsuperscript{\rm 1,\rm 2},
    Second Author Name\textsuperscript{\rm 2},
    Third Author Name\textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}Affiliation 1\\
    \textsuperscript{\rm 2}Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi

\begin{document}

\maketitle

\thispagestyle{plain}

\begin{abstract}
In this paper, we present synthetic data generation framework for flood hazard detection system. For high fidelity and quality, we characterize several real-world properties into virtual world and simulate the flood situation by controlling them. For the sake of efficiency, recent generative models in image-to-3D and urban city synthesis are leveraged to easily composite flood environments so that we avoid data bias due to the hand-crafted manner. Based on our framework, we build the flood synthetic dataset with 5 levels, dubbed \textit{MultiFloodSynth} which contains rich annotation types like normal map, segmentation, 3D bounding box for a variety of downstream task. In experiments, our dataset demonstrate the enhanced performance of flood hazard detection with on-par realism compared with real dataset.
\end{abstract}

\section{Introduction}

Deep learning algorithm requires high quality, diverse and large training dataset. However, gathering good dataset is labor-intensive and requires substantial cost, especially on hyper-scale situations like wildfire recognition  \cite{hong2024wildfire}, pine wilt disease detection  \cite{refjung2024harnessing} and so on. To mitigate this issue, a common solution today is to generate synthetic data and use them as training or reference dataset \cite{kim2024early}.  Recently, generative models  \cite{hamza2024ali, islam2024diffusemix, khullar2023synthetic} or real-time engine  \cite{delussu2024synthetic} are leveraged for more plausible and efficient generation.

However, there is a still concern that editability is insufficient (that is few control parameter) to composite the final scene.  Existing dataset has ambiguous label and one or two types of annotation. Furthermore, from the fact that synthetic dataset should reflect the real-world data, some domains have a significant difficulty due to the absence of high quality real data for reference, definition of label and inconsistent annotation problem. One of them is flood hazard situation which makes it difficult to collect the dataset. It stems from their specific situation where flood accidents frequently paralyze the digital system and physically collapse the surveillance device. Despite of this reason, existing works had explored to make real flood dataset with hand-crafted labeling \cite{wan2024automatic, floodwu2024identification, floodgao2024measuring}. They inevitably confront label-inconsistent problem like 2D bounding box. In addition, they only consider one or two types of annotation as ground truth, hindering their applicability to various computer vision tasks.

In this paper, we present a novel framework that utilizes a 3D engine to generate urban flood synthetic dataset, dubbed \textit{MultiFloodSynth}. For fidelity, we faithfully attribute the flood hazard situation as several properties and components (e.g., layout   \cite{shang2024urbanworld}, lighting, flood-level   \cite{chaudhary2020water, wan2024automatic}, 3D object, camera view) for scene composition by exploring urban flood situation. Moreover, considering various computer vision tasks, our system includes a variety of annotation types such as normal map, instance/semantic/fine-grained segmentation map, camera 3D pos, 2D/3D bounding box, etc. Thanks to such editable attributes, our \textit{MultiFloodSynth} improved the performance of object-localized flood level detection, while alleviating large dataset requirements for model training.

\section{Related Works}
\subsection{Synthetic Dataset Generation}
\label{sec:rel_work_syntehtic}

Recently, generating non-real dataset, synthetic dataset is common technique in a variety of field which requires hyper-scale  \cite{shang2024urbanworld, ref25Greff2022KubricAS, refzhang2024cityx, refxie2024citydreamer, refwu2024unique3d, refschieber2024indoor, shang2024urbanworld, wang2024high, hao2024synthetic, zhu2024odgen, valvano2024controllable}, impossible scenarios     \cite{refjung2024harnessing,ref25Greff2022KubricAS, refhummel2019leveraging, mittal2023orbit, kokosza2024scintilla, amador2024cyclogenesis}, and so on. Synthetic dataset generation resolve such issues by constructing scene in virtual world and alleviate vexing manual process with auto labeling. Diverging from conventional way to generate synthetic data, it has been explored to reflect the characteristics of real-world object to enhance the high fidelity and appropriateness   \cite{refrichter2022enhancing, lee2024learning, ebadi2022psp}. Since these strategy enhance the robustness and realism, it is crucial to consider these attributes for faithful synthesized data.

\begin{figure*}[t]
    \centerline{\includegraphics[width=1\linewidth]{Figure/fig_overview.jpg}}
    \caption{Overview of virtual flood scene composition and synthetic dataset generation pipeline.}
    \label{fig:overview}
\end{figure*}

\begin{figure}[t]
    \centerline{\includegraphics[width=0.9\linewidth]{Figure/fig_real_flood_sample.jpg}}
    \caption{Sample of \textit{real} flood data   \cite{wan2024automatic}.}
    \label{fig:real_flood}
\end{figure}

\begin{figure}[t]
    \centerline{\includegraphics[width=0.9\linewidth]{Figure/fig_flood_simulation.jpg}}
    \caption{Components of flood simulation and results.}
    \label{fig:flood_simulation}
\end{figure}

\subsection{Flood Hazard Detection} \label{sec:rel_work_flood}

Detecting flood situation can be considered as object detection using the level of flood of object. A multitude of studies on classifying and detecting objects based on deep learning algorithms has been continuously conducted to address abovementioned requirement  \cite{floodlo2021deep, floodpally2022application, floodkaranjit2023floodimg, floodzhong2024detection, floodwu2024identification}. However, most studies face challenges such as a lack of data sharing, ethical concerns, absence of abundance and limited labels. To do that, in this paper, we intent to address these issues in following section.

\section{Proposed Method} \label{sec:method}

Main objective is to synthesize urban-scale flood hazard situation in virtual scene and enhance the performance of flood-level detection system with our \textit{MultiFloodSynth} by alleviating a problem of dataset collection. To commence, we describe the real-world flood hazard situation with some considerations and how our \textit{MultiFloodSynth} promise the fidelity. Overall pipeline is shown in Fig. \ref{fig:overview}.

\subsection{Challenges of Real-World Flood Hazard Scenarios}
Existing real-world dataset \cite{floodgao2024measuring} was obtained on vehicle-based flood detection. This dataset includes label information divided into five levels based on the percentage of a vehicle submerged in water \cite{wan2024automatic}. Fig. \ref{fig:real_flood} shows some samples of the data included in the real-world dataset. However, they have inconsistent problem with incoherent bounding box by human-hand. Meanwhile, to simulate the flood situation, it should be considered to appropriately composite several components including camera view, lighting condition and flood-level (\textit{i.e.}, flood height).

\begin{table}[t] 
    \centering
    \begin{tabular}{l | l | l }
    \toprule
    \textbf{Parameter} & \textbf{Attribute} & \textbf{Type} \\
    \midrule
    \multicolumn{3}{c}{\textit{Urban Settings}} \\
    \midrule
    Position & Transform & Constant \\
    Lighting & Light Intensity & Constant \\
    Background & Texture & Image \\
    Layout & - & Image \\
    \midrule
    \multicolumn{3}{c}{\textit{Flood Settings}} \\
    \midrule
    Level (Scale) & Transform & Constant \\
    Roughness & - & Constant \\
    Wavy Texture & - & Image \\
    Opacity & Material & Constant \\
    Specular & Material & Constant \\
    Main Wave & - & Image \\
    Wave Foam & - & Image \\
    \bottomrule
    \end{tabular}
    
    \caption{Control parameters to composite the virtual flood hazard scene. $-$ denotes no corresponding attribute.}
    \label{tab:parameters}
\end{table}

\begin{table*}[t]
    \centering
    \begin{tabular}{l | c c c c c c c}
    \toprule
    & \cite{floodzhong2024detection} & \cite{floodgao2024measuring} &  \cite{floodwu2024identification} &  \cite{wan2024automatic} &  \textit{MultiFloodSynth} \\
    Data type & \textit{real} & \textit{real} & \textit{real} & \textit{real} &\textit{synthetic}\\
    \midrule
    
    Flood-Level & 4 & \xmark & 3 & 5 & 5\\
    Availability & \xmark & \xmark & \xmark & \cmark & \cmark$^\dagger$ \\ 
    \midrule
    Frames & 1,177 & 45,199 & 19,062 & 2,000 & 70,117 \\
    Labeling  &Manual {\scriptsize(Inconsistent)}&Manual {\scriptsize(Inconsistent)}&Manual {\scriptsize(Inconsistent)}& Manual {\scriptsize(Inconsistent)} & Auto {\scriptsize(Consistent)}\\
    \midrule
    Normal Map & \xmark & \xmark & \xmark & \xmark & \cmark \\
    Camera Pos & \xmark & \xmark & \xmark & \xmark & \cmark \\
    3D BBox & \xmark & \xmark & \xmark & \xmark & \cmark \\
    2D BBox & \cmark & \cmark & \xmark & \cmark & \cmark \\
    Depth Map & \xmark & \xmark & \xmark & \xmark & \cmark \\
    Semantic Seg & \xmark & \cmark & \xmark & \xmark & \cmark \\
    Instance Seg & \xmark & \xmark & \xmark & \xmark & \cmark \\
    Fine-grained Seg & \xmark & \xmark & \xmark & \xmark & \cmark \\
    \bottomrule
    \end{tabular}
    
    \caption{Comparison between our \textit{MultiFloodSynth} and related datasets. $^\dagger$Dataset will be available under the acceptance.}
    \label{tab:dataset_comparison}
\end{table*}

\begin{figure}[t]
  \centering
  \begin{subfigure}{0.23\textwidth}
    \includegraphics[width=\linewidth]{Figure/fig_synthetic_flood_sample.jpg}
    \caption{Sample of \textit{flooded} case.}
    \label{fig:flood_sample}
  \end{subfigure} 
  \begin{subfigure}{0.23\textwidth}
    \includegraphics[width=\linewidth]{Figure/fig_synthetic_nonflood_sample.jpg}
    \caption{Sample of \textit{non-flooded} case.}
    \label{fig:non_flood_sample}
  \end{subfigure}
  
  \caption{Sample of our \textit{MultiFloodSynth}.}
  \label{fig:sampleofflooddataset}
\end{figure}

\subsection{Depicting Flood Scenarios in Virtual Simulator}

In contrast to existing synthetic generation works, our framework is capable of controlling some parameters to composite final virtual scene as discussed in former subsection. It allows the user to control the scene for user-wanted structure. Based on our exploration with several real-world data \cite{floodzhong2024detection, floodgao2024measuring, floodwu2024identification}, we observed that following settings play a crucial role to determine the virtual scene: \textit{urban setting}, \textit{flood settings} which heavily affects to semantic feature in neural network. Detailed parameters are listed in Table \ref{tab:parameters}.

Furthermore, since it is quite cumbersome to search several 3d objects for scene composition, we adopt image-to-3d model \cite{refwu2024unique3d} to generate 3D objects by inputting web-crawled car image. To avoid quality degradation and blurry texture, image-to-3d is used than text-to-3d \cite{lin2023magic3d}. In the case of layout and building, we utilize 3D city generation \cite{refxie2024citydreamer} results for base layout.

\thispagestyle{plain}

\subsection{Simulating Flood Wave}

In flood hazard situation, flood simulating is pretty important factor which determine the flood level   \cite{wan2024automatic} and annotation-level. Some attributes (\textit{e.g.}, reflection, roughness, opacity, specular, texture) of flood object will directly affect to extract training feature by neural network. In this regards, we also simulate flood dynamics and visual appearance as shown in Fig. \ref{fig:flood_simulation}. Three factors, \textit{main wave, wave foam, gravity}, decide the visual magnitude of level-of-wave as dynamics. Other factors (\textit{e.g.}, wave size, depth, height) change the appearance of flood as static component.

\subsection{\textit{MultiFloodSynth} Generation}

For flood synthetic generation, we construct flood environments based on each objects by varying some parameters (Tab. \ref{tab:parameters}) for diverse data distribution. Flood level is attributed into 5 level as multi-classes. To enhance the quality and domain similarity, we adopt domain randomization \cite{rawal2023synthetic} in all the objects (e.g., light, camera view, object position, etc). Each object is randomly located in every generation pipeline to avoid bias and sparsity of dataset and to include some crucial corner cases. As a result, our \textit{MultiFloodSynth} consists of a total of 70,117 images, with 14,593 \textit{non-flooded} images and 55,524 \textit{flooded} images. Samples are illustrated in Fig. \ref{fig:sampleofflooddataset}. Total image and instance for each class is listed in Tab. \ref{tab:statistics_synth_data}. For multi-type of annotations, we extract 9 types (\textit{i.e.}, semantic/instance/fine-grained segmentation, 2D/3D bounding box) of paired synthetic scene as shown in Fig. \ref{fig:rich_annotations}. For segmentation map, we allocate the label into car and flood. Differences between competing flood datasets are listed in Tab. \ref{tab:dataset_comparison}.

\begin{figure}[t]
  \centering
  \begin{subfigure}{0.15\textwidth}
    \includegraphics[width=\linewidth]{Figure/fig_anno_2d.jpg}
    \caption{2D bounding box.}
    \label{fig:flood_sample}
  \end{subfigure}
  \begin{subfigure}{0.15\textwidth}
    \includegraphics[width=\linewidth]{Figure/fig_anno_normal.jpg}
    \caption{Normal map.}
    \label{fig:non_flood_sample}
  \end{subfigure}
  \begin{subfigure}{0.15\textwidth}
    \includegraphics[width=\linewidth]{Figure/fig_anno_3d.jpg}
    \caption{3D bounding box.}
    \label{fig:non_flood_sample}
  \end{subfigure}
  
  %

  \begin{subfigure}{0.15\textwidth}
    \includegraphics[width=\linewidth]{Figure/fig_anno_semantic.jpg}
    \caption{Semantic seg.}
    \label{fig:non_flood_sample}
  \end{subfigure}
  \begin{subfigure}{0.15\textwidth}
    \includegraphics[width=\linewidth]{Figure/fig_anno_instance.jpg}
    \caption{Instance seg.}
    \label{fig:flood_sample}
  \end{subfigure}
  \begin{subfigure}{0.15\textwidth}
    \includegraphics[width=\linewidth]{Figure/fig_anno_fine_grained.jpg}
    \caption{Fine-grained seg.}
    \label{fig:flood_sample}
  \end{subfigure}
  
   %
   
   \begin{subfigure}[t]{0.15\textwidth}
    \includegraphics[width=\linewidth]{Figure/fig_anno_depth.jpg}
    \caption{Depth map.}
    \label{fig:flood_sample}
  \end{subfigure}
  \begin{subfigure}[t]{0.15\textwidth}
    \includegraphics[width=\linewidth]{Figure/fig_anno_point_cloud.jpg}
    \caption{Point cloud.}
    \label{fig:flood_sample}
  \end{subfigure}
  \begin{subfigure}[t]{0.15\textwidth}
    \includegraphics[width=\linewidth]{Figure/fig_anno_camera.jpg}
    \caption{Camera 3D pos.}
    \label{fig:non_flood_sample}
  \end{subfigure}
  
  \caption{Richness of annotation type of our \textit{MultiFloodSynth}.}
  \label{fig:rich_annotations}
\end{figure}

\section{Experiments}

\subsection{Environmental Settings}
For detection model, we choose YOLOv10\cite{wang2024yolov10}. For training hyperparameters, we set batch size as 256, learning rate as 0.001 with 100 epochs. Image-to-3D model is used with Unique3D \cite{refwu2024unique3d}. Our virtual environment is based on NVIDIA Omniverse simulator.

\thispagestyle{plain}

\begin{table}[] 
    \centering
    \begin{tabular}{c | c | c | c}
    \toprule
    \multicolumn{2}{c|}{\textbf{Class}} & \textbf{\# of Images} & \textbf{Instance} \\
    \midrule
    \textit{Non-flooded} & Level 0 & 14,593 & 37,662 \\
    \midrule
    \multirow{4}{*}{\textit{Flooded}} & Level 1 & 17,485 & 55,624 \\
    & Level 2 & 14,541 & 36,141 \\
    & Level 3 & 12,837 & 61,132 \\
    & Level 4 & 10,661 & 24,476 \\
    \midrule
    \multicolumn{2}{c|}{Total} & 70,117 & 215,035 \\
    \bottomrule
    \end{tabular}
    
    \caption{Summary of our \textit{MultiFloodSynth} composition.}
    \label{tab:statistics_synth_data}
\end{table}

\begin{table}[t]
    
    \begin{subtable}[t]{1\linewidth}
    \centering
    \begin{tabular}{ c | c  c  c  c}
    \toprule
    Training & PR(\%)$\uparrow$ & RC(\%)$\uparrow$ & $\text{mAP}_\text{50}\uparrow$ & $\text{mAP}_\text{50-95}\uparrow$ \\
    \midrule
    $\mathcal{D}_\text{real}$ & 37.61 & \textbf{43.81}& 33.21 & 22.92 \\
    \midrule
    $\mathcal{D}_\text{synth}$ & 19.36 & 7.21 & 7.41 & 4.32 \\
    \midrule
     \cellcolor{cyan!20} & \cellcolor{cyan!20} & \cellcolor{cyan!20} & \cellcolor{cyan!20} & \cellcolor{cyan!20}\\
     \multirow{-2}{*}{\shortstack{$\mathcal{D}_\text{real}$ $+$ \\ $\mathcal{D}_\text{synth}$}}
     \cellcolor{cyan!20} & \cellcolor{cyan!20}\multirow{-2}{*}{\textbf{40.97}}  & \cellcolor{cyan!20}\multirow{-2}{*}{41.75}  & \cellcolor{cyan!20}\multirow{-2}{*}{\textbf{35.78}}  & \cellcolor{cyan!20}\multirow{-2}{*}{\textbf{23.16}} \\
    \bottomrule
    \end{tabular}
    \caption{Results on YOLOv10-N.}
    \label{model_n}
    \end{subtable}

    
    \hspace{\fill}

    
    \begin{subtable}[t]{1\linewidth}
    \centering
    \begin{tabular}{ c | c  c  c  c}
    \toprule
    Training & PR(\%)$\uparrow$ & RC(\%)$\uparrow$ & $\text{mAP}_\text{50}\uparrow$ & $\text{mAP}_\text{50-95}\uparrow$ \\
    \midrule
    $\mathcal{D}_\text{real}$ & 58.92 & \textbf{58.12}  & 56.66 & 40.64 \\
    \midrule
    $\mathcal{D}_\text{synth}$ & 15.17  & 14.41  & 6.94 & 3.97 \\
    \midrule
     \cellcolor{cyan!20} & \cellcolor{cyan!20} & \cellcolor{cyan!20} & \cellcolor{cyan!20} & \cellcolor{cyan!20}\\
     \multirow{-2}{*}{\shortstack{$\mathcal{D}_\text{real}$ $+$ \\ $\mathcal{D}_\text{synth}$}}
     \cellcolor{cyan!20} & \cellcolor{cyan!20}\multirow{-2}{*}{\textbf{61.16}}  & \cellcolor{cyan!20}\multirow{-2}{*}{57.77}  & \cellcolor{cyan!20}\multirow{-2}{*}{\textbf{58.61}}  & \cellcolor{cyan!20}\multirow{-2}{*}{\textbf{42.71}} \\
    \bottomrule
    \end{tabular}
    \caption{Results on YOLOv10-B.}
    \label{model_n}
    \end{subtable}

    \caption{Classification performance according to the training dataset. PR, RC, mAP denote precision, recall, mean average precision, respectively. Best score is denoted as \textbf{bold-font}.}
    \label{tab:comparison_detection}
\end{table}

\subsection{Comparison on Detection Performance}

To evaluate the superiority of our \textit{MultiFloodSynth}, we compare the flood detection performance by varying the training dataset. Based on \cite{wan2024automatic}, we denotes previous real dataset as $\mathcal{D}_\text{real}$ and our dataset as $\mathcal{D}_\text{synth}$. Then, we train the model with different training configuration as: (1) $\mathcal{D}_\text{real}$, (2), $\mathcal{D}_\text{synth}$, (3) $\mathcal{D}_\text{real} + \mathcal{D}_\text{synth}$. For evaluation metric, we include precision, recall, mAP at 50 and 50-95. In evaluation, we train two size models, YOLOv10-N (\textit{small size}) and YOLOv10-B (\textit{large size}).

The results are shown in Tab. \ref{tab:comparison_detection}. It show that the real-world data training outperformed the synthetic data training. However, training mixing two dataset ($\mathcal{D}_\text{real} + \mathcal{D}_\text{synth}$) demonstrated improved performance. To conclude, our \textit{MultiFloodSynth} boost the detection performance in flood hazard recognition task with consistent annotation while also alleviating the cost of burden data collection process.

\subsection{Evaluation of \textit{MultiFloodSynth}}

Furthermore, we intend to evaluate our \textit{MultiFloodSynth} in the perspective of realism compared with real dataset. To do that, we borrow the recent synthetic dataset evaluation metric, \textit{Realistic Score} which is proposed in urban world generation in \cite{shang2024urbanworld}. By randomly selecting 1K samples in each dataset, we average the score. We normalized the scores of $\mathcal{D}_\text{synth}$ based on the scores of the real dataset. As shown in Tab. \ref{tab:realistic_score}, our dataset showed $93.17\%$ plausibility which is similar level of realism compared with real dataset.

\begin{table}[t]
    \centering
    \begin{tabular}{ l | c }
    \toprule
    \textbf{Data} & \textbf{Realistic Score}(\%)$\uparrow$ \\
    \midrule
    $\mathcal{D}_\text{real}$      & 100{ \scriptsize(7.18)} \\
    $\mathcal{D}_\text{synth}$ & 93.17{ \scriptsize(6.69)} \\
    \bottomrule
    \end{tabular}

    \caption{Realism of synthetic data compared with real data.}
    \label{tab:realistic_score}
\end{table}

\begin{figure}[t]
    \centerline{\includegraphics[width=0.8\linewidth]{Figure/fig_eigencam.jpg}}
    \caption{Visualization of EigenCAM for explainability. A red part indicates that the model considers that part to be important evidence for decision.}
    \label{fig:eigencam}
\end{figure}

\subsection{Explanation of Flood Detection Model}

For explainability of detection, we adopt recent XAI method, 
EigenCAM\cite{muhammad2020eigen} which can analyze some specific parts of an input image that play a crucial role in the model decision. As shown in Fig. \ref{fig:eigencam}, our \textit{MultiFloodSynth} played a crucial role in training and thus enables model to recognize important evidence from images.

\section{Conclusions}
In this paper, we have presented synthetic dataset generation pipeline for urban flood detection into 5 levels and evaluated the utility of our \textit{MultiFloodSynth}. For faithful synthetic dataset, we charactersize several parameters including not only environmental settings, \textit{e.g.}, lighting color, camera position, but also flood simulation factors, \textit{e.g.}, roughness, texture, opacity, etc. To mitigate the data bias and domain gap, we adopt domain randomization and vary the above parameters in each generation pipeline. Moreover, for the sake of efficiency of pipeline, we leverage the recent generation techniques, image-to-3D generation and urban city generation, for 3D object and base layout of virtual flood world, respectively. Experimental results demonstrated that the model trained with our synthetic dataset and real-world dataset show enhanced detection performance in object-localization based flood-level recognition. In addition, our \textit{MultiFloodSynth} showed on-par realism compared with real dataset in terms of \textit{Realistic Score} metric. To conclude, our \textit{MultiFloodSynth} generated from our parameter-controllable flood environment can serve as a valuable training dataset, alternating data requirements of real-world dataset. 

\thispagestyle{plain}

\section{Acknowledgments}
This research was supported by Culture, Sports and Tourism R\&D Program through the Korea Creative Content Agency grant funded by Ministry of Culture, Sports and  Tourism in 2024 (Project Name : Developing Professionals for R\&D in Contents Production Based on Generative AI and Cloud, Project Number : RS-2024-00352578, Contribution Rate: 100\%) and Artificial intelligence industrial convergence cluster development project funded by the Ministry of Science and ICT(MSIT, Korea) \& Gwangju Metropolitan City.

\thispagestyle{plain}

\bibliography{aaai25}

\thispagestyle{plain}

\end{document}
