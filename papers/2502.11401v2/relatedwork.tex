\section{Related Works}
Text embedding is a technique that maps text data into a numerical vector space, capturing both semantic and contextual features of the text. Research in this area can be divided into three categories based on the underlying model: Early models, LLMs with fine-tuning, and LLMs without fine-tuning.

\paragraph{Early Models} Early approaches
% to text embedding include the bag-of-words model, which relies on word frequency, and Word2Vec \cite{DBLP:journals/corr/abs-1301-3781}, which introduced distributed word embeddings. With the advent of the Transformer \cite{DBLP:conf/nips/VaswaniSPUJGKP17} architecture, contextual embeddings became the dominant approach. Notable works in this area 
include SentenceBERT \cite{DBLP:conf/emnlp/ReimersG19} (supervised) and SimCSE \cite{DBLP:conf/emnlp/GaoYC21} (unsupervised), which leverage contrastive learning to generate high-quality text embeddings using small encoder-only models. Another area of focus has been improving the isotropy of embedding spaces. Works such as BERT-flow \cite{DBLP:conf/emnlp/LiZHWYL20} (flow models) and BERT-whitening \cite{DBLP:journals/corr/abs-2103-15316} (linear transformations) address the anisotropic properties of embeddings. Meanwhile, multi-stage contrastive learning \cite{DBLP:journals/corr/abs-2308-03281, DBLP:conf/emnlp/Ni0LDAMZLHCY22, DBLP:journals/corr/abs-2212-03533} has further advanced text embeddings by combining pre-training on large weakly supervised datasets with fine-tuning on smaller high-quality datasets. Inspired by instruction fine-tuning, recent research \cite{DBLP:conf/acl/SuSKWHOYSZ023, DBLP:conf/acl/AsaiSL0I0HY23} has shifted toward using text paired with instructions to enhance the generalization and transferability of text embeddings in complex scenarios. However, the performance of methods based on early models is limited, due to their reliance on models with relatively small parameter counts.

% Recently, Large Language Models (LLMs) have emerged as a unifying foundation for NLP tasks, thanks to their powerful semantic understanding capabilities, significantly advancing the field of text embedding.

\paragraph{LLMs with Fine-Tuning}
Many studies have focused on transforming LLMs into text embedding models through contrastive learning fine-tuning. RepLLaMA \citep{DBLP:conf/sigir/MaWYWL24}, for example, follows the DPR \citep{DBLP:conf/emnlp/KarpukhinOMLWEC20} pipeline, using the hidden state of the last token generated by LLaMA as a text embedding vector and applying contrastive learning fine-tuning. Recognizing that the unidirectional attention mechanism in LLMs may limit text embedding quality, LLM2Vec \citep{DBLP:journals/corr/abs-2404-05961} introduces a bidirectional attention mechanism combined with average pooling to enhance embedding quality. NV-Embed \citep{DBLP:journals/corr/abs-2405-17428} takes this further by incorporating an additional Latent Attention Layer to generate pooled embeddings. bge-en-icl \cite{DBLP:journals/corr/abs-2409-15700} suggests that retaining the original framework of LLMs and leveraging in-context learning is the optimal approach for generating text embeddings. Some studies \cite{DBLP:conf/acl/WangYHYMW24} even use synthetic data generated by LLMs, rather than real-world data, for fine-tuning and achieve competitive performance on the MTEB leaderboard \cite{DBLP:conf/eacl/MuennighoffTMR23}. However, these approaches often overlook the fundamental differences between language modeling and contrastive learning, failing to fully leverage the potential of LLMs. More closely related to our work is Llama2Vec \cite{DBLP:conf/acl/Li0XSL24}, which proposes two pretext tasks to enable unsupervised adaptation of LLMs, followed by contrastive learning fine-tuning to achieve better performance. In contrast, our approach achieves strong results without any need for contrastive learning fine-tuning, as our task fully exploits the inherent potential of LLMs.


\paragraph{LLMs without Fine-Tuning}
Several studies have explored methods to transform LLMs into text encoders without fine-tuning. \cite{DBLP:conf/iclr/LiuTAPZS24} proposed using possible trajectory distributions as text representations, achieving effectiveness but at a high computational cost. \citep{DBLP:journals/corr/abs-2402-15449} introduced echo embeddings by repeatedly feeding text into autoregressive models, addressing architectural limitations but doubling computational requirements. Other methods focus on prompt adjustments to produce meaningful embeddings. PromptEOL \citep{DBLP:conf/emnlp/JiangHLWZ24} introduced a One-Word Limitation prompt to improve embedding performance, while MetaEOL \citep{DBLP:conf/acl/LeiW00CTY24} extended this idea by using eight different prompt types to generate multi-view embeddings. GenEOL \citep{DBLP:journals/corr/abs-2410-14635} leveraged LLMs to create various sentence transformations that retain their meaning, aggregating the resulting embeddings to enhance the overall sentence representation. Meanwhile, PromptReps \cite{DBLP:conf/emnlp/ZhuangMKLZ24} developed a hybrid document retrieval framework leveraging prompts to address challenges in information retrieval tasks. Despite these innovations, these approaches either perform poorly or require multiple inferences to achieve good results. By contrast, our method surpasses these methods with minimal training costs.