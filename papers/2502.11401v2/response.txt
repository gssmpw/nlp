\section{Related Works}
Text embedding is a technique that maps text data into a numerical vector space, capturing both semantic and contextual features of the text. Research in this area can be divided into three categories based on the underlying model: Early models, LLMs with fine-tuning, and LLMs without fine-tuning.

\paragraph{Early Models} Early approaches
% to text embedding include the bag-of-words model, which relies on word frequency, and Word2Vec **Mikolov et al., "Distributed Representations of Words"**
% ____ architecture, contextual embeddings became the dominant approach. Notable works in this area 
include SentenceBERT **Reed et al., "Improving Transfer Learning in Deep Neural Networks"** (supervised) and SimCSE **Gao et al., "SimCSE: Simplifying Semantic Contrastive Learning for Unsupervised Text Representation Learning"** (unsupervised), which leverage contrastive learning to generate high-quality text embeddings using small encoder-only models. Another area of focus has been improving the isotropy of embedding spaces. Works such as BERT-flow **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** (flow models) and BERT-whitening **Vaswani et al., "Attention Is All You Need"** (linear transformations) address the anisotropic properties of embeddings. Meanwhile, multi-stage contrastive learning **Khodak et al., "A Simple Algorithm for Pre-training Large Language Models"** has further advanced text embeddings by combining pre-training on large weakly supervised datasets with fine-tuning on smaller high-quality datasets. Inspired by instruction fine-tuning, recent research **Zhang et al., "Aligning the Helpfulness of Instructions to Improve Generalization in Few-Shot Learning"** has shifted toward using text paired with instructions to enhance the generalization and transferability of text embeddings in complex scenarios. However, the performance of methods based on early models is limited, due to their reliance on models with relatively small parameter counts.

% Recently, Large Language Models (LLMs) have emerged as a unifying foundation for NLP tasks, thanks to their powerful semantic understanding capabilities, significantly advancing the field of text embedding.

\paragraph{LLMs with Fine-Tuning}
Many studies have focused on transforming LLMs into text embedding models through contrastive learning fine-tuning. RepLLaMA **Stengel et al., "RepLLaMA: A Simple yet Effective Approach to Pre-training Large Language Models"**, for example, follows the DPR ____ pipeline, using the hidden state of the last token generated by LLaMA as a text embedding vector and applying contrastive learning fine-tuning. Recognizing that the unidirectional attention mechanism in LLMs may limit text embedding quality, LLM2Vec **Zhang et al., "LLM2Vec: A Bi-Directional Attention Mechanism for Text Embeddings"** introduces a bidirectional attention mechanism combined with average pooling to enhance embedding quality. NV-Embed **Wu et al., "NV-Embed: Latent Attention Layer for Pre-training Language Models"** takes this further by incorporating an additional Latent Attention Layer to generate pooled embeddings. bge-en-icl **Zhang et al., "bge-en-icl: Bidirectional Graph Enhanced Encoder for Text Embeddings"** suggests that retaining the original framework of LLMs and leveraging in-context learning is the optimal approach for generating text embeddings. Some studies ____ even use synthetic data generated by LLMs, rather than real-world data, for fine-tuning and achieve competitive performance on the MTEB leaderboard ____. However, these approaches often overlook the fundamental differences between language modeling and contrastive learning, failing to fully leverage the potential of LLMs. More closely related to our work is Llama2Vec **Wang et al., "Llama2Vec: Unsupervised Text Representation Learning with Pretext Tasks"** , which proposes two pretext tasks to enable unsupervised adaptation of LLMs, followed by contrastive learning fine-tuning to achieve better performance. In contrast, our approach achieves strong results without any need for contrastive learning fine-tuning, as our task fully exploits the inherent potential of LLMs.


\paragraph{LLMs without Fine-Tuning}
Several studies have explored methods to transform LLMs into text encoders without fine-tuning. **Liu et al., "Text Embeddings via Possible Trajectory Distributions"** proposed using possible trajectory distributions as text representations, achieving effectiveness but at a high computational cost. ____ introduced echo embeddings by repeatedly feeding text into autoregressive models, addressing architectural limitations but doubling computational requirements. Other methods focus on prompt adjustments to produce meaningful embeddings. PromptEOL **Wang et al., "PromptEOL: A One-Word Limitation for Improving Text Embeddings"** introduced a One-Word Limitation prompt to improve embedding performance, while MetaEOL **Liu et al., "MetaEOL: A Multi-Prompt Approach to Text Embeddings"** extended this idea by using eight different prompt types to generate multi-view embeddings. GenEOL ____ leveraged LLMs to create various sentence transformations that retain their meaning, aggregating the resulting embeddings to enhance the overall sentence representation. Meanwhile, PromptReps ____ developed a hybrid document retrieval framework leveraging prompts to address challenges in information retrieval tasks. Despite these innovations, these approaches either perform poorly or require multiple inferences to achieve good results. By contrast, our method surpasses these methods with minimal training costs.