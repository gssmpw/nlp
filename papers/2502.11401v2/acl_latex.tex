% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{colortbl} 
\usepackage{xcolor} 
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{booktabs}
% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\definecolor{fed867}{rgb}{1.0, 0.85, 0.5}
\definecolor{fed867}{rgb}{1.0, 0.949, 0.8}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% \title{Follow the nature of LLM embedding via Information Compression and Conditional Distribution Alignment}

% \title{Following the Autoregressive Nature of LLM Embeddings via Information Compression and Conditional Distribution Alignment}

\title{Following the Autoregressive Nature of LLM Embeddings \\via Compression and Alignment}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Jingcheng Deng$^{1,2\ *}$, Zhongtao Jiang$^{3}$\thanks{\ \ Equal Contributions},Liang Pang$^{1}$\thanks{\ \ Corresponding Author}, Liwei Chen$^{3}$,Kun Xu,\\
\textbf{Zihao Wei$^{1,2}$,  Huawei Shen$^{1,2}$, Xueqi Cheng$^{1,2}$}\\
  $^{1}$Key Laboratory of AI Safety, Institute of Computing Technology, Chinese Academy of Sciences \\
  $^{2}$ University of Chinese Academy of Sciences \\
  $^{3}$ Kuaishou Technology\\
\texttt{\{dengjingcheng23s, pangliang\}@ict.ac.cn} \\}



% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
A new trend uses LLMs as dense text encoders via contrastive learning. However, since LLM embeddings predict the probability distribution of the next token, they are inherently generative and distributive, conflicting with contrastive learning, which requires embeddings to capture full-text semantics and align via cosine similarity. This discrepancy hinders the full utilization of LLMs' pre-training capabilities, resulting in inefficient learning. In response to this issue, we propose AutoRegEmbed, a new contrastive learning method built on embedding conditional probability distributions, which integrates two core tasks: information compression and conditional distribution alignment. The information compression task encodes text into the embedding space, ensuring that the embedding vectors capture global semantics. The conditional distribution alignment task focuses on aligning text embeddings with positive samples embeddings by leveraging the conditional distribution of embeddings while simultaneously reducing the likelihood of generating negative samples from text embeddings, thereby achieving embedding alignment and uniformity. Experimental results demonstrate that our method significantly outperforms traditional contrastive learning approaches and achieves performance comparable to state-of-the-art models when using the same amount of data. Our code is available at \url{https://github.com/TrustedLLM/AutoRegEmbed}
\end{abstract}

\section{Introduction}
Text embeddings, which represent the semantic content of natural language text as vectors, are extensively utilized in domains such as information retrieval, semantic similarity assessment, retrieval-augmented generation (RAG) \citep{DBLP:conf/iclr/KhandelwalLJZL20, DBLP:conf/naacl/ShiMYS0LZY24, DBLP:conf/emnlp/DengPSC23, DBLP:journals/corr/abs-2402-10612,DBLP:conf/acl/XuPYMSCZ24}, and data attribution \citep{DBLP:conf/dsaa/BeigiTMCS024}. Traditional text embedding models typically employ transformer-based architectures with encoder-only designs, including examples like Bert \citep{DBLP:conf/naacl/DevlinCLT19}, DeBERTa \citep{DBLP:conf/iclr/HeLGC21} and MPNet \citep{DBLP:conf/nips/Song0QLL20}, and are trained using contrastive learning.
\begin{figure}[t]
  \includegraphics[width=1.\columnwidth]{./fig/comparison.pdf}
  \caption{Comparison of pareto front between AutoRegEmbed and other methods. The horizontal axis represents the number of training samples, while the vertical axis indicates the average performance across 10 STS datasets. The upper left corner represents the region with the highest learning efficiency.}
  \label{fig:comparison}
  \vspace{-0.5cm}
\end{figure}

After extensive pre-training on a large-scale corpus, LLMs have outperformed previous encoder-only small models \citep{DBLP:conf/emnlp/DengDGJP22} and demonstrated strong adaptability across diverse downstream tasks \citep{zhao2024large,wang-etal-2024-gpt,zhao2025roleplayparadoxlargelanguage}.
Recently, contrastive learning has been directly applied to decoder-only LLMs, which are trained to generate embedding vectors based on task-specific instructions, enabling adaptability to various embedding scenarios \cite{ DBLP:journals/corr/abs-2405-17428, DBLP:journals/corr/abs-2404-05961}. Despite initial advancements, training a high-performance 7B-scale text embedding model using this approach remains highly resource-intensive. It typically requires millions of triplets \cite{DBLP:conf/acl/WangYHYMW24, DBLP:journals/corr/abs-2409-15700, DBLP:journals/corr/abs-2308-03281} and substantial computational power, including thousands of hours on an A100 80GB GPUs \cite{DBLP:journals/corr/abs-2402-09906, DBLP:conf/sigir/MaWYWL24}, even with the application of Parameter-Efficient Fine-Tuning (PEFT) \cite{DBLP:conf/iclr/HuSWALWWC22, DBLP:conf/iclr/Dao24}. The high resource consumption might reasonably be attributed to the inability of the \textit{discriminative} contrastive learning method to fully harness the capabilities of \textit{generative} LLMs \cite{DBLP:conf/acl/Li0XSL24}. Firstly, the constraint of unidirectional attention in LLMs leads to the aggregation of information in the hidden state of the output layer corresponding to the final token. However, as LLMs are optimized for next-token prediction, this hidden state can only represent the semantics of the next token (local) rather than the semantics of the input text itself (global). Consequently, employing this hidden state directly in contrastive learning requires additional training time and computational cost to transition from a localized to a more global semantic representation. Secondly, the hidden state in LLMs is used to generate the probability distribution of the next token, whereas contrastive learning optimizes the cosine distance between the hidden states of different texts. This divergence in optimization objectives introduces additional training costs. This raises an important question: \textit{Is it feasible to develop a method that follows the auto-regressive nature while generating high-quality text embeddings and significantly reducing resource requirements?}


We formalize three key requirements to address this problem. Firstly, embeddings should capture global semantics rather than focusing solely on next-token semantics. Secondly, they must follow alignment and uniformity principles \cite{DBLP:conf/icml/0001I20}. Finally, the transformation from the original embedding to one that meets these criteria should follow an autoregressive nature. To this end, we propose AutoRegEmbed, which encompasses two tasks: information compression and conditional distribution alignment.

The information compression task is inspired by the concept of context compression \cite{DBLP:conf/emnlp/ChevalierWAC23, DBLP:conf/iclr/00010WWCW24, DBLP:conf/nips/Mu0G23}, which addresses the limitations of context window length and the high computational cost faced by LLMs when processing long texts. Specifically, we encode the context and instructions into a set of compressed variables, which are then passed to a decoder with the same architecture but frozen parameters, forcing it to reconstruct the corresponding target. By restricting the decoder to rely solely on the compressed variables—without access to the original context or instructions—we introduce an information bottleneck. This ensures that the compressed variables effectively capture the global semantics of the instructions and context.

The conditional distribution alignment task draws inspiration from traditional contrastive learning and LLM alignment techniques \cite{DBLP:journals/corr/abs-2407-16216}. We begin by treating the compressed vectors as embeddings of their corresponding inputs. Then, we adopt the structure of the InfoNCE \citep{DBLP:journals/corr/abs-1807-03748} loss function, but redefine the similarity metric. Simply put, we align the distance between the conditional probability distributions of text and positive sample embeddings while increasing the likelihood of text embeddings generating positive samples and decreasing the likelihood of generating negative samples. This approach promotes the alignment and uniformity of compressed variables while maintaining the autoregressive nature.
% The former ensure that the outputs of LLMs align with human preferences using methods such as Reinforcement Learning with Human Feedback (RLHF) \cite{DBLP:conf/nips/Ouyang0JAWMZASR22}, Proximal Policy Optimization (PPO) \cite{DBLP:journals/corr/SchulmanWDRK17}, and Direct Preference Optimization (DPO) \cite{DBLP:conf/nips/RafailovSMMEF23}. 
% In our method, we adopt the form of the InfoNCE \cite{DBLP:journals/corr/abs-1807-03748} loss function but replace the similarity metric from inner product to the distance between conditional probability distributions. Notably, when the lengths of positive and negative samples differ, common distribution distance measures such as KL divergence and Wasserstein distance become inadequate for quantifying their differences. Drawing inspiration from the alignment technique DPO, we instead measure the distance between conditional probability distributions using the difference between the logarithmic probabilities of the model generating positive and negative samples. This approach ensures that the conditional probability distributions of compressed variables representing the input and positive samples are as close as possible, while those for the input and negative samples are maximally separated, thereby promoting the alignment and uniformity of compressed variables within the generative framework.

% The goal of the distribution constraint task is to ensure that the latent space, where the latent variables reside, satisfies the uniformity property. A feasible method to achieve this is by introducing a regularization term, such as minimizing the KL divergence between the latent variable distribution and a chosen prior distribution. A commonly used prior is the Gaussian distribution, as the standard Gaussian distribution in high dimensions closely resembles a uniform distribution on the surface of a hypersphere, with most of its mass concentrated near the hypersphere shell. However, since cosine similarity is often used to compute matching scores with embeddings, we adopt the von Mises-Fisher (vMF) distribution, which is directly defined on the unit sphere, as the prior distribution. This approach avoids the "soap bubble effect" inherent in high-dimensional Gaussian distributions, providing a more suitable constraint for the latent space \cite{DBLP:conf/uai/DavidsonFCKT18, DBLP:conf/emnlp/XuD18}.

Experimental results on 10 STS datasets demonstrate that AutoRegEmbed outperforms traditional contrastive learning methods while utilizing the same computational resources, making it a highly efficient and scalable solution. Remarkably, even with a limited number of training samples, AutoRegEmbed achieves performance on par with the current state-of-the-art (SOTA) models, showcasing its superior ability to learn robust and generalizable representations from scarce data. As shown in Figure~\ref{fig:comparison}, the Pareto frontier of AutoRegEmbed consistently outperforms traditional contrastive learning methods, demonstrating a more optimal trade-off between computational efficiency and performance. This indicates that AutoRegEmbed achieves superior representation learning while maintaining a balanced resource utilization.

\section{Related Works}
Text embedding is a technique that maps text data into a numerical vector space, capturing both semantic and contextual features of the text. Research in this area can be divided into three categories based on the underlying model: Early models, LLMs with fine-tuning, and LLMs without fine-tuning.

\paragraph{Early Models} Early approaches
% to text embedding include the bag-of-words model, which relies on word frequency, and Word2Vec \cite{DBLP:journals/corr/abs-1301-3781}, which introduced distributed word embeddings. With the advent of the Transformer \cite{DBLP:conf/nips/VaswaniSPUJGKP17} architecture, contextual embeddings became the dominant approach. Notable works in this area 
include SentenceBERT \cite{DBLP:conf/emnlp/ReimersG19} (supervised) and SimCSE \cite{DBLP:conf/emnlp/GaoYC21} (unsupervised), which leverage contrastive learning to generate high-quality text embeddings using small encoder-only models. Another area of focus has been improving the isotropy of embedding spaces. Works such as BERT-flow \cite{DBLP:conf/emnlp/LiZHWYL20} (flow models) and BERT-whitening \cite{DBLP:journals/corr/abs-2103-15316} (linear transformations) address the anisotropic properties of embeddings. Meanwhile, multi-stage contrastive learning \cite{DBLP:journals/corr/abs-2308-03281, DBLP:conf/emnlp/Ni0LDAMZLHCY22, DBLP:journals/corr/abs-2212-03533} has further advanced text embeddings by combining pre-training on large weakly supervised datasets with fine-tuning on smaller high-quality datasets. Inspired by instruction fine-tuning, recent research \cite{DBLP:conf/acl/SuSKWHOYSZ023, DBLP:conf/acl/AsaiSL0I0HY23} has shifted toward using text paired with instructions to enhance the generalization and transferability of text embeddings in complex scenarios. However, the performance of methods based on early models is limited, due to their reliance on models with relatively small parameter counts.

% Recently, Large Language Models (LLMs) have emerged as a unifying foundation for NLP tasks, thanks to their powerful semantic understanding capabilities, significantly advancing the field of text embedding.

\paragraph{LLMs with Fine-Tuning}
Many studies have focused on transforming LLMs into text embedding models through contrastive learning fine-tuning. RepLLaMA \citep{DBLP:conf/sigir/MaWYWL24}, for example, follows the DPR \citep{DBLP:conf/emnlp/KarpukhinOMLWEC20} pipeline, using the hidden state of the last token generated by LLaMA as a text embedding vector and applying contrastive learning fine-tuning. Recognizing that the unidirectional attention mechanism in LLMs may limit text embedding quality, LLM2Vec \citep{DBLP:journals/corr/abs-2404-05961} introduces a bidirectional attention mechanism combined with average pooling to enhance embedding quality. NV-Embed \citep{DBLP:journals/corr/abs-2405-17428} takes this further by incorporating an additional Latent Attention Layer to generate pooled embeddings. bge-en-icl \cite{DBLP:journals/corr/abs-2409-15700} suggests that retaining the original framework of LLMs and leveraging in-context learning is the optimal approach for generating text embeddings. Some studies \cite{DBLP:conf/acl/WangYHYMW24} even use synthetic data generated by LLMs, rather than real-world data, for fine-tuning and achieve competitive performance on the MTEB leaderboard \cite{DBLP:conf/eacl/MuennighoffTMR23}. However, these approaches often overlook the fundamental differences between language modeling and contrastive learning, failing to fully leverage the potential of LLMs. More closely related to our work is Llama2Vec \cite{DBLP:conf/acl/Li0XSL24}, which proposes two pretext tasks to enable unsupervised adaptation of LLMs, followed by contrastive learning fine-tuning to achieve better performance. In contrast, our approach achieves strong results without any need for contrastive learning fine-tuning, as our task fully exploits the inherent potential of LLMs.


\paragraph{LLMs without Fine-Tuning}
Several studies have explored methods to transform LLMs into text encoders without fine-tuning. \cite{DBLP:conf/iclr/LiuTAPZS24} proposed using possible trajectory distributions as text representations, achieving effectiveness but at a high computational cost. \citep{DBLP:journals/corr/abs-2402-15449} introduced echo embeddings by repeatedly feeding text into autoregressive models, addressing architectural limitations but doubling computational requirements. Other methods focus on prompt adjustments to produce meaningful embeddings. PromptEOL \citep{DBLP:conf/emnlp/JiangHLWZ24} introduced a One-Word Limitation prompt to improve embedding performance, while MetaEOL \citep{DBLP:conf/acl/LeiW00CTY24} extended this idea by using eight different prompt types to generate multi-view embeddings. GenEOL \citep{DBLP:journals/corr/abs-2410-14635} leveraged LLMs to create various sentence transformations that retain their meaning, aggregating the resulting embeddings to enhance the overall sentence representation. Meanwhile, PromptReps \cite{DBLP:conf/emnlp/ZhuangMKLZ24} developed a hybrid document retrieval framework leveraging prompts to address challenges in information retrieval tasks. Despite these innovations, these approaches either perform poorly or require multiple inferences to achieve good results. By contrast, our method surpasses these methods with minimal training costs.


\section{Method}
In this section, we first introduce the preliminary information about the task of text embedding with instructions. We then discuss the information compression, which transitions LLM embeddings from local semantics to global semantics, followed by the conditional distribution alignment, which optimizes the conditional probability distribution of embeddings to ensure alignment and uniformity.
\begin{figure*}[t]
  \includegraphics[width=1.\textwidth]{./fig/overview.pdf}
  \caption{Overall framework of AutoRegEmbed. Firstly, we perform the information compression task to inject key information from the context and instruction into the compressed tokens. Then, we optimize the conditional probability distribution of these tokens to align the distributions of $e_{q,I_{\mathrm{next}}}$ and $e_{d^+,I_{\mathrm{self}}}$ as closely as possible through $S_1(q,d^+)$, while increasing the probability of $e_{q,I_{\mathrm{next}}}$ generating positive samples and reducing the probability of $e_{q,I_{\mathrm{next}}}$ generating negative samples through $S_2(d^+,d^-;q)$.} 
  \label{fig:overview}
  % \vspace{-0.5cm}
\end{figure*}
\subsection{Preliminary}
Text embeddings with instructions can adapt to various downstream tasks. Formally, given a large collection 
$D=\{d_1,d_2,\dots,d_N\}$ containing $N$ documents, as well as a text $q$ and an instruction $t$, the embedding $e_{q,t}=E(q,t)$ generated from $q$ and $t$ can match documents $d \in D$ that are relevant to $q$, according to $t$, where $E$ represents the text encoder. Thus, by simply changing the instruction $t$, the relevance measure can be adapted to different downstream tasks. For example, for dense retrieval tasks, the instruction might be ``\textit{find documents that can answer this question,}'' while for semantic similarity tasks, the instruction could be ``\textit{find sentences that are semantically similar to this text}''. Numerous studies have explored various embedding techniques and instruction diversities. Our goal is to identify a simple yet effective way to enable LLMs to generate high-quality embeddings directly from autoregressive framework.
%from selected(判别)（merge or pooling） embedding to generative embedding 用IT的方式来的 是generative的embedding，需要做更高效的learning，pointwise distribution-wise.
\subsection{Information Compression: from Discriminative to Generative Embeddings} %每一步需要讲清楚为什么要这么做。
In this section, we first explain the motivation for transitioning from discriminative embeddings to generative embeddings, followed by a formal definition of the information compression task.

In decoder-based LLMs, embeddings are typically generated by extracting the hidden state of the final token in the input sequence. However, this approach primarily captures the semantics of the first output token rather than encoding the global semantics of the entire input. Various pooling techniques, such as average pooling and attention pooling, have been explored to mitigate this limitation, yet they introduce their own challenges. The average pooling method, which computes the mean of all token hidden states, does not necessarily encapsulate global semantics but instead serves as a mechanism for "convexity preservation." Conversely, attention pooling modifies the attention mechanism or introduces additional parameters, thereby altering the original architecture of LLMs. Such modifications deviate from the model’s pre-training design and can lead to unintended consequences, as prior studies \cite{DBLP:journals/corr/abs-2409-15700} indicate that maintaining the original LLM framework often yields optimal performance. To enable LLMs to generate embeddings that represent global semantics, we introduce an information compression task. This task compels LLMs to reconstruct the original target using a compressed embedding derived from the input text. Given that this compressed embedding models the conditional probability distribution of the target, we designate it as the generative embedding to contrast it with the discriminative embedding produced by conventional pooling approaches.

The information compression task is inspired by the concept of context compression. Specifically, we append $k$ compressed tokens $c=(c_1,\dots,c_k)$, where $k<<n+m$, to the text $q=(q_1,\dots,q_n)$ and instruction $t=(t_1,\dots,t_m)$, with $n$ and $m$ representing their respective token lengths. This combined $(q,t,c)$ is then fed into an encoder $E$ to generate the embedding $e_c=(e_{c_1},\dots,e_{c_k})$. As mentioned earlier, we expect the embedding $e_c$ to capture the global semantics of the text $q$ and the instruction $t$.  To achieve this, we input $e_c$ into a frozen decoder $D$, which shares the same architecture, and force it to generate the most relevant document $d$. The optimization objective for this task can be expressed as: 
\begin{equation*}
\begin{aligned}
\label{eq1}
    &\mathcal{L}_{\mathrm{IC}}=\underset{e_{c_1},\dots,e_{c_k}}{\mathrm{max}} P(d|e_{c_1},\dots,e_{c_k};\theta_{D})\\
    &=\underset{\Theta_{E}}{\mathrm{max}}\, P(d|c_1\dots c_k,t_1\dots t_m,q_1 \dots q_n;\theta_{E},\theta_{D}),
\end{aligned}
\end{equation*}
where $\theta_{E}$ and $\theta_{D}$ denote the parameters of $E$ and $D$, respectively.

\subsection{Conditional Distribution Alignment: from Data-Point to Distribution Perspective}
After addressing the global semantic representation issue of the embedding vector, we also require the embedding vector to meet the criteria of alignment and uniformity. Existing studies \cite{DBLP:conf/icml/0001I20} have provided specific definitions for these properties. Alignment is typically expressed by the following formula:
\begin{equation*}
\label{eq2}
    \mathrm{Alignment}(f,\alpha)=\underset{(q,d^+)\in p_{\mathrm{pos}}}{\mathbb{E}}\parallel f(q)-f(d^+)\parallel^\alpha_2 ,
\end{equation*}
where $\alpha > 0$ is a parameter used to adjust the weight of the distance between positive sample pairs ($q$ and $d^+$), and $p_{\mathrm{pos}}(\cdot,\cdot)$ represents the distribution of positive sample pairs. The smaller $\mathrm{Alignment}(f,\alpha)$ is, the better the alignment of the embedding vector generated by $f$. 
% Our goal is to develop a method for \textit{optimizing embedding alignment within the framework of autoregressive tasks}.
Uniformity measures how evenly the embedding is distributed, commonly expressed by the following formula:
\begin{equation*}
\label{eq3}
    \mathrm{Uniformity}(f,\alpha)=\mathrm{log} \underset{(q,d)\in p_{\mathrm{data}}}{\mathbb{E}} e^{-t\parallel f(q)-f(d)\parallel^2_2} ,
\end{equation*}
where $t > 0$ and $p_{\mathrm{data}}$ represents the data distribution. In general, we optimize these two properties asymptotically using a contrastive loss, such as InfoNCE,
\begin{equation}
\begin{aligned}
\label{eq4}
    &\mathcal{L}_{\mathrm{InfoNCE}}(f;\tau)=&\\
    &\mathbb{E}[-\mathrm{log}\frac{e^{f(q)^Tf(d^+)/\tau}}{e^{f(q)^Tf(d^+)/\tau}+\sum_i e^{f(q)^Tf(d^-_i)/\tau}}] ,
\end{aligned}
\end{equation}
where $\tau$ denotes the temperature parameter and $d^-_i$ represents the $i$-th negative sample. Clearly, Equation \ref{eq4} differs fundamentally from the generative pre-training task, as it optimizes the cosine distance between sample embeddings, aligning data points in the embedding space rather than modeling the next-token probability distribution, which is central to pre-training. So, using this loss function to optimize an LLM may not fully unlock its potential.

To address this, we propose the Conditional Distribution Alignment task to minimize this discrepancy as much as possible. The concept is straightforward: Instead of using the cosine distance between embeddings, we assess similarity based on the conditional probability distribution corresponding to each embedding. Simply put, we extend point alignment to distribution alignment. Formally, the decoder $L_D$ is a well-trained autoregressive language model with the following conditional probability distribution: 
\begin{equation*}
\label{eq5}
    p(d|e_c)=\prod\limits_{t=1}^Tp(d_t|d_{<t},e),
\end{equation*}
where $e_c=(e_{c_1},\dots,e_{c_k})$ is the embedding variables, $d=(d_1,d_2,\dots,d_T)$ represents the generated sentence, and $d_{<t}$ denotes the part of the sentence before time step $t$. Intuitively, the similarity between corresponding samples $q$ and $d$ can be measured by computing the distance between the conditional probability distributions of their embeddings, $e_q$ and $e_d$:
\begin{equation*}
\label{eq6}
    S(q,d) = \frac{1}{T}\sum_{t=1}^T \mathrm{D}(p(d_t|d_{<t},e_q), p(d_t|d_{<t},e_d)),
\end{equation*}
where $\mathrm{D}(\cdot,\cdot)$ is any function that measures the divergence between two probability distributions. In addition, since the conditional probability distribution of the embedding can be adjusted based on the given instruction, we can compute the logarithmic probability of the text embedding $e_{q,I_{\mathrm{next}}}$ generating positive or negative samples to measure the similarity between the text $q$ and the positive $d^+$ and negative $d^-$ samples. Here, the instruction $I_{\mathrm{next}}$ is similar to ``\textit{find documents that can answer this question}'',  which ensures that the embedding $e_{q,I{\mathrm{next}}}$ generated from text $q$ produces positive samples after passing through the decoder. For positive and negative samples, we use the instruction $I_{\mathrm{self}}$ similar to ``\textit{find sentences that are semantically similar to this text}'', so that their embeddings, $e_{d^+,I_{\mathrm{self}}}$ and $e_{d^-,I_{\mathrm{self}}}$, generate themselves after passing through the decoder. 

Building on the above insights and incorporating the structure of InfoNCE, we empirically derive the final loss function:
\vspace{-0.2cm}
\begin{equation}
\begin{aligned}
\label{eq7}
&\mathcal{L}_{\mathrm{CDA}}=\mathbb{E}[-\mathrm{log}\frac{e^{S_1(q,d^+)/\tau}}{e^{S_1(q,d^+)/\tau}+\sum_i e^{S_2(d^+,d^-_i;q)/\tau}}], \\
& S_1(q,d^+) = - \sigma(\beta \ |\mathrm{log} \frac{p_{\theta_{E}}(d^+|e_{q,I_{\mathrm{next}}})}{p_{\theta_{E}}(d^+|e_{d^+,I_{\mathrm{self}}})}|  ),\\
& S_2(d^+,d^-_i;q) = 
- \sigma(\beta \ \mathrm{log}\frac{p_{\theta_{E}}(d^+|e_{q,I_{\mathrm{next}}})}{p_{\mathrm{ref}}(d^+|e_{q,I_{\mathrm{next}}})}\\
&-\beta \ \mathrm{log}\frac{p_{\theta_{E}}(d^-|e_{q,I_{\mathrm{next}}})}{p_{\mathrm{ref}}(d^-|e_{q,I_{\mathrm{next}}})}),
\end{aligned}
\end{equation}
where $\tau$ and $\beta$ are temperature parameters, and $p_{\Theta_{L_E}}$ represents the initial model. We use the Sigmoid function $\sigma(\cdot)$ to normalize the similarity measured from the conditional probability distribution to the range [0,1], ensuring maximum consistency with the range of cosine distance. $S_1$ represents the similarity function between text $q$ and the positive sample $d^+$. We define it by measuring the absolute value of the difference in the logarithmic probability of their corresponding embeddings, $e_{q,I_{\mathrm{next}}}$ and $e_{d^+,I_{\mathrm{self}}}$, generating the positive sample $d^+$. To minimize this difference, we apply the absolute value function. In addition, we then add a negative sign to ensure that the value of $S_1$ increases as the similarity between $q$ and $d^+$ increases. $S_2$ calculates the difference between the logarithmic probabilities of generating positive and negative samples for text $q$, similar to DPO \citep{DBLP:conf/nips/RafailovSMMEF23}. We amplify this difference to boost the probability of embedding $e_{q,I_{\mathrm{next}}}$ generating positive samples and decrease the probability of generating negative samples. We normalize the probabilities by dividing them by the corresponding values from the initial model to account for the length discrepancy between positive and negative samples.

\begin{table*}[htb]
\centering
\normalsize
\setlength{\tabcolsep}{3pt}
\resizebox{\linewidth}{!}{%
\begin{tabular}{lrccccccccccccc}
\toprule
\textbf{Method} & \textbf{Params} & \textbf{BIOSSES} &\textbf{SICK-R}&\textbf{STS12} & \textbf{STS13} & \textbf{STS14} & \textbf{STS15} & \textbf{STS16} &\textbf{STS17}&\textbf{STS22}& \textbf{STS-B}  & \textbf{Avg.}& \textbf{Vol.}\\
\midrule
\midrule
\multicolumn{10}{l}{\it{Without Contrastive Training}}\\
\rowcolor{fed867} LLaMA2-L & 7B & 63.29 & 65.10 & 45.26 & 70.83 & 56.69 & 62.48 &      63.27 & 49.76&-7.76&60.43&60.58$_{(7)}$/56.91$_{(10)}$&0\\
\rowcolor{fed867} LLaMA2-M & 7B & 65.96 & 60.01 & 44.76 & 64.13 & 48.66 & 62.33 &      63.16 & 64.35&27.59&53.50&56.65$_{(7)}$/58.67$_{(10)}$&0\\
\rowcolor{fed867} Mistral-v0.1-L & 7B & 54.40 & 67.40 & 48.54 & 64.27 & 54.89 & 65.05 &      62.12 & 48.22&13.71&63.05&60.76$_{(7)}$/56.20$_{(10)}$&0\\
\rowcolor{fed867} Mistral-v0.1-M & 7B & 67.46 & 62.42 & 50.11 & 66.45 & 52.60 & 61.93 &      65.02 & 71.28&29.79&54.19&58.96$_{(7)}$/61.13$_{(10)}$&0\\
\midrule
{Echo-LLaMA2}& 7B
 &-&64.39& 52.40 & 72.40 & 61.24 & 72.67 & 73.51 & - & - & 65.73 &66.05$_{(7)}$/-&0\\
{Echo-LLaMA2} & 13B
 &-&70.27& 59.36 & 79.01 & 69.75 & 79.86 & 76.75  & - & - &71.31&72.33$_{(7)}$/-&0\\
{PromptEOL-LLaMA2} & 7B
 & - & 69.64 & 58.81 & 77.01 & 66.34 & 73.22 & 73.56 & - & - & 71.66  &70.03$_{(7)}$/-&0\\
{PromptEOL-Mistral} & 7B
 &-&69.47& 63.08 & 78.58 & 69.40 & 77.92 & 79.01 & - &-	 & 75.77 &73.32$_{(7)}$/-&0\\
{PromptEOL-LLaMA3} & 8B
 &-& 60.88&68.94 & 78.57 & 68.18 & 76.75 & 77.16 &  -      & - & 72.83  &71.90$_{(7)}$/-&0\\
{PromptEOL-LLaMA2} & 13B
 & -&68.23& 56.19 & 76.42 & 65.42 & 72.73 & 75.21 & - & - &67.96 &68.83$_{(7)}$/-&0\\
 {MetaEOL-LLaMA2} & 7B
 & - & 74.86 & 64.16 & 81.61 & 73.09 & 81.11 & 78.94 & - & - & 77.96  &75.96$_{(7)}$/-&0\\
 {MetaEOL-Mistral} & 7B
 &-&75.13& 64.05 & 82.35 & 71.57 & 81.36 & 79.85 & - &-	 & 78.29 &76.09$_{(7)}$/-&0\\
 {GenEOL-LLaMA2-Mistral} & 7B
 &-&78.08& 70.24 & 83.43 & 78.03 & 81.79 & 80.65 & - &-	 & 80.46 &78.95$_{(7)}$/-&0\\
 {GenEOL-LLaMA2-ChatGPT} & 7B
 &-&78.71& 70.78 & 83.28 & 77.75 & 82.10 & 80.45 & - &-	 & 79.83 &78.99$_{(7)}$/-&0\\
\midrule
\midrule
\multicolumn{10}{l}{\it{Unsupervised Contrastive Training}}\\
LLM2Vec-LLaMA2$^\clubsuit$ & 7B
& 82.41&71.77&65.39 & 79.26 & 72.98 & 82.72 & 81.02 & 86.70 & 63.47 & 78.32&75.92$_{(7)}$/76.41$_{(10)}$&\textasciitilde160,000  \\
LLM2Vec-Mistral$^\clubsuit$ & 7B
 & 83.29 & 75.55 & 67.65 & 83.90 & 76.97 & 83.80 & 81.91 & 85.58 &65.93&80.42&78.60$_{(7)}$/78.50$_{(10)}$&\textasciitilde160,000\\
\midrule
\midrule
\multicolumn{10}{l}{\it{Supervised Contrastive Training}}\\
NV-Embed$^\clubsuit$ & 7.73B
& 85.59&82.80&76.22 & 86.30 & 82.09 & 87.24 & 84.77 & 87.42 & 69.85 & 86.14&83.65$_{(7)}$/82.84$_{(10)}$ &1,054,000 \\
SFR-Embedding-2\_R$^\clubsuit$ & 7B
& 87.60&77.01&75.67 & 82.40 & 79.93 & 85.82 & 84.50 & 88.93 & 67.10 & 83.60&81.28$_{(7)}$/81.26$_{(10)}$&\textasciitilde1,751,000  \\
gte-Qwen2-7B-instruct$^\clubsuit$ & 7.49B
& 81.37&79.16&79.53 & 88.97 & 83.87 & 88.48 & 86.49 & 88.75 & 67.16 & 86.81&84.76$_{(7)}$/83.06$_{(10)}$&\textasciitilde791,000,000  \\
LLM2Vec-LLaMA2$^\clubsuit$ & 7B
& 82.13&83.01&78.85 & 86.84 & 84.04 & 88.72 & 86.79 & 90.63 & 67.55 & 88.72&85.28$_{(7)}$/83.73$_{(10)}$&544,000  \\
LLM2Vec-Mistral$^\clubsuit$ & 7B
 & 85.24 & 83.70 & 78.80 & 86.37 & 84.04 & 88.99 & 87.22 & 90.19 &67.68&88.65&85.40$_{(7)}$/84.01$_{(10)}$&544,000\\
 \rowcolor{fed867} {LLaMA2-L} & 7B
 & 77.58 & 77.85 & 73.72 & 84.04 & 79.82 & 85.03 & 84.78 & 87.53&26.87&86.18&81.63$_{(7)}$/76.34$_{(10)}$&50,000\\
 \rowcolor{fed867} {LLaMA2-inbatch-L} & 7B
 & 78.81 & 82.76 & 77.70 & 85.01 & 81.82 & 88.30 & 86.12 & 90.53&20.70&87.94&84.24$_{(7)}$/77.97$_{(10)}$&50,000\\
  \rowcolor{fed867} {LLaMA2-M} & 7B
 & 75.65 & 78.92 & 74.12 & 84.17 & 80.00 & 85.63 & 83.28 &85.65&65.09&86.27&81.77$_{(7)}$/79.88$_{(10)}$&50,000\\
 \rowcolor{fed867} {LLaMA2-inbatch-M} & 7B
 & 78.09 & 83.17 & 77.10 & 82.82 & 80.53 & 87.40 & 84.43 &90.02&64.59&87.18&83.23$_{(7)}$/81.53$_{(10)}$&50,000\\
  \rowcolor{fed867} {LLaMA2-inbatch-M} & 7B
 & 77.43 & 82.26 & 77.95 & 84.90 & 82.06 & 87.22 & 86.43 &88.22&66.42&86.12&83.85$_{(7)}$/81.90$_{(10)}$&274,951\\
\midrule
\midrule
\multicolumn{10}{l}{\it{Information Compression and Conditional Distribution Alignment}}\\
{AutoRegEmbed-LLaMA2} & 7B
 & 85.50 & 79.07 & 79.57 & 86.90 & 83.28 & 88.45 & 86.57 & 88.61 &66.16&86.59&84.35$_{(7)}$/83.07$_{(10)}$&50,000(16,382)\\
 {AutoRegEmbed-Mistral} & 7B
 & 86.69 & 80.21 & 78.33 & 86.22 & 82.36 & 88.42 & 86.43 & 88.70 &64.27&87.05&84.15$_{(7)}$/82.87$_{(10)}$&50,000(16,382)\\
 {AutoRegEmbed-LLaMA2} & 7B
 & 85.62 & 81.93 & 78.84 & 86.76 & 84.01 & 89.43 & 87.72 & 89.04 &66.77&87.96&85.24$_{(7)}$/83.81$_{(10)}$&274,951(16,382)\\
\midrule
\bottomrule
\end{tabular}
}
\caption{Results on STS tasks (Spearman correlation scaled by 100x). The parentheses in the \textbf{Avg.} column indicate the number of datasets used to compute the average. \textbf{Vol.} denotes the number of training triplets, while the numbers in brackets indicate the instruction fine-tuning data used by AutoRegEmbed during the information compression stage. The symbol “\textasciitilde” denotes an estimated value. "\textcolor{fed867}{\rule{5mm}{2mm}}" represents our own fair baselines, and we apply a grid search to ensure optimal performance. "-L" and "-M" denote the hidden state of the last token and the average pooling of all token hidden states, respectively. The symbol $\clubsuit$ indicates that not all data are open source.
}
\label{tab:sts}
\vspace{-0.5cm}
\end{table*}
\section{Experiments}

\subsection{Experimental Settings}
\paragraph{Evaluations} Previous studies \cite{DBLP:conf/emnlp/GaoYC21,DBLP:conf/emnlp/LiZHWYL20} highlight that a key goal of text embedding is to cluster semantically similar sentences. Following this approach, we use the MTEB \cite{DBLP:conf/eacl/MuennighoffTMR23} evaluation framework to evaluate AutoRegEmbed on ten semantic text similarity datasets, including STS12 \citep{DBLP:conf/semeval/AgirreCDG12}, STS13 \citep{DBLP:conf/starsem/AgirreCDGG13}, STS14 \citep{DBLP:conf/semeval/AgirreBCCDGGMRW14}, STS15 \citep{DBLP:conf/semeval/AgirreBCCDGGLMM15}, STS16 \citep{DBLP:conf/semeval/AgirreBCDGMRW16}, STS17 \citep{DBLP:conf/semeval/CerDALS17}, STS22 \citep{DBLP:conf/semeval/ChenZCFGGHJS22}, STS-B , BIOSSES and SICK-R. Each pair of text in the STS dataset is labeled with a similarity score ranging from 0 to 5 or 0 to 4, indicating their semantic similarity. The evaluation metric is the Spearman correlation between the similarity scores predicted by the model and the scores annotated by humans. 

\paragraph{Training} In the information compression stage, we use the training set of the instruction fine-tuning dataset PWC \citep{DBLP:conf/iclr/00010WWCW24}, which includes a diverse range of instruction types, as the training data. The original dataset contains 241,564 (context, instruction, target) samples. To reduce redundancy caused by repeated contexts, we remove duplicates, resulting in the PWC-Unique dataset with 16,382 samples as the final training data. In the conditional distribution alignment stage, we use NLI data \citep{DBLP:conf/acl/WangYHYMW24} and \citep{DBLP:journals/corr/abs-2402-03216} from previous studies as training data. The former contains 50,000 samples, while the latter consists of 274,951 samples. Each sample includes an anchor, a positive sample, and a negative sample. Unless otherwise specified, the AutoRegEmbed results presented in the experiment section are based on training with 50,000 samples.

\paragraph{Baselines} We categorize the baselines into three groups: (1) models without contrast training, including base models with various embedding methods using the same instructions as AutoRegEmbed and prompt-adjusted embedded models, including Echo \citep{DBLP:journals/corr/abs-2402-15449}, PromptEOL \citep{DBLP:conf/emnlp/JiangHLWZ24}, MetaEOL \citep{DBLP:conf/acl/LeiW00CTY24}, and GenEOL \citep{DBLP:journals/corr/abs-2410-14635}; (2) unsupervised contrast training models, primarily LLM2Vec \citep{DBLP:journals/corr/abs-2404-05961} with different base models; and (3) supervised contrast training models, which consist of NV-Embed \citep{DBLP:journals/corr/abs-2405-17428}, SFR-Embedding-2\_R \citep{SFR-embedding-2}, gte-Qwen2-7B-instruct \citep{DBLP:journals/corr/abs-2308-03281}, LLM2Vec \citep{DBLP:journals/corr/abs-2404-05961}, and fair baselines.


\subsection{Main Results}
Table~\ref{tab:sts} summarizes the results of various baselines and AutoRegEmbed on ten STS datasets, along with the training data required for each method.

\paragraph{AutoRegEmbed vs. Without Contrastive Training} Models without contrastive training are divided into two categories. The first is our own fair baseline model, which performs significantly worse than AutoRegEmbed, with an average performance 20\% lower. This highlights the difficulty of untrained LLMs in directly generating high-quality embeddings. While some methods enhance the base model’s embeddings through prompt optimization, their improvements remain limited—even on a 13B-parameter model—and come with significant additional reasoning costs. For instance, Echo requires processing text twice to mitigate unidirectional attention limitations, MetaEOL aggregates embeddings from eight different instructions, and GenEOL relies on ChatGPT to generate up to 32 text variants before aggregating their embeddings. These additional reasoning steps severely constrain their practical implementation.

\paragraph{AutoRegEmbed vs. Unsupervised Contrastive Training}
LLM2Vec enhances existing LLMs using an unsupervised contrastive learning approach similar to SimCSE, leading to significant performance gains. Compared to the base model, the unsupervised version of LLM2Vec improves performance by over 15\%. Although it utilizes almost 160,000 data samples, its performance remains 4.5\% lower than AutoRegEmbed, demonstrating its lower efficiency.

\paragraph{AutoRegEmbed vs. Supervised Contrastive Training}
Supervised contrastive learning is the mainstream approach for building high-quality embedding models. We first compared top-performing methods that once ranked on the MTEB leaderbord (with the time they reached SOTA in brackets), including NV-Embed (2024.08), SFR-Embedding-2\_R (2024.02), gte-Qwen2-7B-instruct (2024.06), and LLM2Vec (2024.05). In terms of model performance, AutoRegEmbed outperforms most SOTA methods on ten STS datasets, trailing only 0.2\% behind the best version of LLM2Vec. From a data efficiency perspective, AutoRegEmbed achieves performance comparable to the previous SOTA models with just 66,382 training samples, whereas the latter requires tens of millions of triplets to reach peak performance. Additionally, previous SOTA models employ multi-task learning (e.g., retrieval and clustering), whose impact on STS performance remains unclear. To ensure a fair comparison, we use single-task contrastive learning as a baseline. Unlike traditional contrastive learning, AutoRegEmbed does not rely on in-batch negative samples. So we add two baselines to single-task contrastive learning that also exclude the in-batch negative sample strategy. As shown in Table~\ref{tab:sts}, even under identical training data, AutoRegEmbed outperforms four different single-task contrastive learning, further validating its effectiveness.

\subsection{Ablation Study}
\begin{figure*}[ht]
  \includegraphics[width=1.\textwidth]{./fig/Convergence_Trend_Subplots.pdf}
  \caption{We evaluate the learning efficiency of our method against traditional contrastive learning on 10 STS datasets, comparing their performance under the same number of samples (left figure) and same number of epochs (right figure). Further details are provided in Appendix~\ref{app:details}.}
  \label{fig:convergence}
  \vspace{-5pt}%
\end{figure*}

To verify the effectiveness of AutoRegEmbed, we conducted an ablation study. First, we removed Conditional Distribution Alignment to evaluate its impact on model performance. Second, since Equation~\ref{eq7} was derived empirically in our previous work, we tested different variants of this equation to confirm that it remains the optimal choice. Different variants of Equation~\ref{eq7} include \textbf{Log\_sigmoid}, which maps similarity to a logarithmic scale for integration with the exponential function e, as well as \textbf{KL divergence} and \textbf{JS divergence}, which quantify the distance between the conditional probability distributions of positive and negative sample embeddings in distinct ways. The specific equations are provided in Appendix~\ref{app:ablation}.

\begin{table}
\centering
\normalsize
% \setlength{\tabcolsep}{3pt}
\resizebox{\linewidth}{!}{%
\begin{tabular}{lc}
\toprule
\textbf{Method} & \textbf{Avg.}\\
\midrule
{AutoRegEmbed-LLaMA2} &83.07$_{(10)}$\\
\midrule
\multicolumn{2}{l}{\it{Tasks}}\\
w/o Conditional Distribution Alignment & 73.90$_{(10)}$\\
LLaMA2-L (Without Training) & 56.91$_{(10)}$\\
\midrule
\multicolumn{2}{l}{\it{Equation~\ref{eq7}}}\\
Log\_sigmoid & 82.93$_{(10)}$\\
KL divergence & 79.82$_{(10)}$\\
JS divergence & 79.02$_{(10)}$ \\
\bottomrule
\end{tabular}
}
\caption{Ablation experiments of AutoRegEmbed. We conduct ablation and contrast experiments on various tasks and Equation~\ref{eq7} to demonstrate the effectiveness of AutoRegEmbed.}
%\vspace{-5pt}
\label{tab:ablation_study}
\end{table}

Table~\ref{tab:ablation_study} presents the ablation results. The experiments on different tasks indicate that Conditional Distribution Alignment improves performance by 9.17\%, while Information Compression contributes a 16.99\% improvement, demonstrating the effectiveness of both tasks. Additionally, experiments on variants of Equation~\ref{eq7} reveal that, although using a logarithmic scale for similarity and employing KL or JS divergence to measure distribution distance are more intuitive approaches, they do not surpass the performance of the original loss function in Equation~\ref{eq7}. Thus, Equation~\ref{eq7} can be regarded as a more effective loss function.

\subsection{Learning Efficiency}
To verify that AutoRegEmbed is better suited for LLMs, we compare its performance with four contrastive learning baselines under the same training data and the same number of epochs, as shown in Figure~\ref{fig:convergence}. The left figure in Figure~\ref{fig:convergence} shows that as the training data increases, the performance of both AutoRegEmbed and other contrastive learning methods improves, but AutoRegEmbed exhibits the fastest growth. Notably, with just 15,000 samples, AutoRegEmbed already surpasses the maximum performance of other contrastive learning models. The right figure demonstrates that as the number of epochs increases, AutoRegEmbed also improves at the fastest rate. These results indicate that AutoRegEmbed significantly outperforms the baseline models in learning efficiency.

\section{Conclusions}
To address the limitation that traditional contrastive learning does not adhere to the autoregressive nature of LLMs, we propose AutoRegEmbed—a novel contrastive learning method based on embedded conditional probability distributions. AutoRegEmbed ensures that LLM-generated embeddings capture global semantics while maintaining alignment and uniformity through information compression and conditional distribution alignment tasks. AutoRegEmbed achieves comparable performance to SOTA models with fewer training samples and superior learning efficiency.
\section{Limitations}
The primary advantage of AutoRegEmbed lies in its ability to effectively harness the power of large language models (LLMs) to construct robust and high-quality text embeddings. However, it is important to acknowledge several limitations of our approach.

AutoRegEmbed does not possess inherent mechanisms to filter or detect malicious or harmful content in the data it processes. While the model is capable of generating embeddings from a wide range of text inputs, it lacks the ability to evaluate the ethical or safety implications of the data. This makes it vulnerable to issues related to biased, offensive, or otherwise problematic content present in the training corpus. In cases where the training data contains harmful or discriminatory material, the embeddings generated by AutoRegEmbed may inadvertently carry forward these biases, potentially leading to unintended and undesirable outcomes when applied to real-world tasks.

To mitigate this risk, we recommend that users of AutoRegEmbed ensure that the training data is carefully curated, and ideally, filtered for harmful content. By using safe and ethically sourced data, the model’s potential for propagating bias or harm can be minimized. Additionally, users should be cautious when applying AutoRegEmbed to sensitive domains, where the generation of unsafe or biased embeddings could have significant consequences.
% \section*{Acknowledgments}

% This document has been adapted
% by Steven Bethard, Ryan Cotterell and Rui Yan
% from the instructions for earlier ACL and NAACL proceedings, including those for
% ACL 2019 by Douwe Kiela and Ivan Vuli\'{c},
% NAACL 2019 by Stephanie Lukin and Alla Roskovskaya,
% ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu,
% NAACL 2018 by Margaret Mitchell and Stephanie Lukin,
% Bib\TeX{} suggestions for (NA)ACL 2017/2018 from Jason Eisner,
% ACL 2017 by Dan Gildea and Min-Yen Kan,
% NAACL 2017 by Margaret Mitchell,
% ACL 2012 by Maggie Li and Michael White,
% ACL 2010 by Jing-Shin Chang and Philipp Koehn,
% ACL 2008 by Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui,
% ACL 2005 by Hwee Tou Ng and Kemal Oflazer,
% ACL 2002 by Eugene Charniak and Dekang Lin,
% and earlier ACL and EACL formats written by several people, including
% John Chen, Henry S. Thompson and Donald Walker.
% Additional elements were taken from the formatting instructions of the \emph{International Joint Conference on Artificial Intelligence} and the \emph{Conference on Computer Vision and Pattern Recognition}.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

\appendix

\section{Implementation details}
\label{app:details}
\paragraph{AutoRegEmbed} For the information compression task, we set the learning rate to 2e-5, the batch size to 32, and train for 2 epoch. To represent the semantics of the input, we use 5 compressed tokens. For the conditional distribution alignment task, the learning rate is set to 5e-6, with a batch size of 32 and 4 epochs. The temperature parameters t and b are both set to 0.1. For the above two tasks, we set the maximum token length of context, instruction, and target to 512. Furthermore, we employ the bfloat16 format, enable FlashAttention 2, and train on four A100-80G GPUs with DeepSpeed and Zero-2. The information compression task takes 20 minutes, while the conditional distribution alignment task, involving 50,000 samples, takes approximately 1 hour.

\paragraph{Fair Comparative Learning Baselines}
We train our own fair contrastive learning baseline based on the standard InfoNCE loss, with some code available in the FlagEmbedding repository\footnote{\url{https://github.com/FlagOpen/FlagEmbedding}}). For baselines utilizing the in-batch negative sample strategy (LLaMA2-inbatch-L and LLaMA2-inbatch-M), we experimented with batch sizes of 128, 256, 512, and 1024, determining that 512 yields the best performance. Additionally, we ensure that gradients are propagated across different devices. For baselines that do not use the in-batch negative sample strategy, we set the batch size to 32, maintaining consistency with AutoRegEmbed. Regarding the learning rate, we tested 1e-5, 5e-5, 1e-4, and 2e-4, finding that 1e-4 delivers the best results. All training data is consistent with AutoRegEmbed. We train the fair contrastive learning baseline using DeepSpeed and Zero-2 on four A100-80G GPUs in 1 hour.
\section{Variants of Equation~\ref{eq7}}
\label{app:ablation}
This section explores various possible modifications and extensions of Equation~\ref{eq7}.
\paragraph{Log\_sigmoid} Given that most loss functions are logarithmic in nature, we can modify the similarity function in Equation~\ref{eq7} by replacing the sigmoid with a Log-Sigmoid function, resulting in a more interpretable formulation:
\begin{equation*}
\begin{aligned}
\label{eq8}
&\mathcal{L}_{\mathrm{CDA}}=\mathbb{E}[-\mathrm{log}\frac{e^{S_1(q,d^+)/\tau}}{e^{S_1(q,d^+)/\tau}+\sum_i e^{S_2(d^+,d^-_i;q)/\tau}}], \\
& S_1(q,d^+) = - \mathrm{log}\sigma(\beta \ |\mathrm{log} \frac{p_{\theta_{E}}(d^+|e_{q,I_{\mathrm{next}}})}{p_{\theta_{E}}(d^+|e_{d^+,I_{\mathrm{self}}})}|  ),\\
& S_2(d^+,d^-_i;q) = 
- \mathrm{log}\sigma(\beta \ \mathrm{log}\frac{p_{\theta_{E}}(d^+|e_{q,I_{\mathrm{next}}})}{p_{\mathrm{ref}}(d^+|e_{q,I_{\mathrm{next}}})}\\
&-\beta \ \mathrm{log}\frac{p_{\theta_{E}}(d^-|e_{q,I_{\mathrm{next}}})}{p_{\mathrm{ref}}(d^-|e_{q,I_{\mathrm{next}}})}).
\end{aligned}
\end{equation*}
\paragraph{KL divergence} We also experimented with replacing the difference in log probabilities with the KL divergence between the conditional probability distributions:
\begin{equation*}
\begin{aligned}
\label{eq9}
&\mathcal{L}_{\mathrm{CDA}}=\mathbb{E}[-\mathrm{log}\frac{e^{S_1(q,d^+)/\tau}}{e^{S_1(q,d^+)/\tau}+\sum_i e^{S_2(q,d^-_i)/\tau}}], \\
& S_1(q,d^+) = - \sigma(\frac{1}{T}\sum_{t=1}^T \mathrm{KL}(p_{\theta_{E}}(d^+_t|d^+_{<t},e_{d^+,I_{\mathrm{self}}}),\\
&p_{\theta_{E}}(d^+_t|d^+_{<t},e_{q,I_{\mathrm{next}}}))),\\
& S_2(q,d^-) = 
- \sigma(\frac{1}{T}\sum_{t=1}^T \mathrm{KL}(p_{\theta_{E}}(d^+_t|d^-_{<t},e_{d^-,I_{\mathrm{self}}}),\\
&p_{\theta_{E}}(d^+_t|d^+_{<t},e_{q,I_{\mathrm{next}}}))).\\
\end{aligned}
\end{equation*}
\paragraph{JS divergence} In addition to KL divergence, we also employed JS divergence as a measure of distribution distance:
\begin{equation*}
\begin{aligned}
\label{eq10}
&\mathcal{L}_{\mathrm{CDA}}=\mathbb{E}[-\mathrm{log}\frac{e^{S_1(q,d^+)/\tau}}{e^{S_1(q,d^+)/\tau}+\sum_i e^{S_2(q,d^-_i)/\tau}}], \\
& S_1(q,d^+) = - \sigma(\frac{1}{T}\sum_{t=1}^T \mathrm{JS}(p_{\theta_{E}}(d^+_t|d^+_{<t},e_{d^+,I_{\mathrm{self}}}),\\
&p_{\theta_{E}}(d^+_t|d^+_{<t},e_{q,I_{\mathrm{next}}}))),\\
& S_2(q,d^-) = 
- \sigma(\frac{1}{T}\sum_{t=1}^T \mathrm{JS}(p_{\theta_{E}}(d^+_t|d^-_{<t},e_{d^-,I_{\mathrm{self}}}),\\
&p_{\theta_{E}}(d^+_t|d^+_{<t},e_{q,I_{\mathrm{next}}}))).\\
\end{aligned}
\end{equation*}

\end{document}
