\section{Related Works}
Text embedding is a technique that maps text data into a numerical vector space, capturing both semantic and contextual features of the text. Research in this area can be divided into three categories based on the underlying model: Early models, LLMs with fine-tuning, and LLMs without fine-tuning.

\paragraph{Early Models} Early approaches
% to text embedding include the bag-of-words model, which relies on word frequency, and Word2Vec ____, which introduced distributed word embeddings. With the advent of the Transformer ____ architecture, contextual embeddings became the dominant approach. Notable works in this area 
include SentenceBERT ____ (supervised) and SimCSE ____ (unsupervised), which leverage contrastive learning to generate high-quality text embeddings using small encoder-only models. Another area of focus has been improving the isotropy of embedding spaces. Works such as BERT-flow ____ (flow models) and BERT-whitening ____ (linear transformations) address the anisotropic properties of embeddings. Meanwhile, multi-stage contrastive learning ____ has further advanced text embeddings by combining pre-training on large weakly supervised datasets with fine-tuning on smaller high-quality datasets. Inspired by instruction fine-tuning, recent research ____ has shifted toward using text paired with instructions to enhance the generalization and transferability of text embeddings in complex scenarios. However, the performance of methods based on early models is limited, due to their reliance on models with relatively small parameter counts.

% Recently, Large Language Models (LLMs) have emerged as a unifying foundation for NLP tasks, thanks to their powerful semantic understanding capabilities, significantly advancing the field of text embedding.

\paragraph{LLMs with Fine-Tuning}
Many studies have focused on transforming LLMs into text embedding models through contrastive learning fine-tuning. RepLLaMA ____, for example, follows the DPR ____ pipeline, using the hidden state of the last token generated by LLaMA as a text embedding vector and applying contrastive learning fine-tuning. Recognizing that the unidirectional attention mechanism in LLMs may limit text embedding quality, LLM2Vec ____ introduces a bidirectional attention mechanism combined with average pooling to enhance embedding quality. NV-Embed ____ takes this further by incorporating an additional Latent Attention Layer to generate pooled embeddings. bge-en-icl ____ suggests that retaining the original framework of LLMs and leveraging in-context learning is the optimal approach for generating text embeddings. Some studies ____ even use synthetic data generated by LLMs, rather than real-world data, for fine-tuning and achieve competitive performance on the MTEB leaderboard ____. However, these approaches often overlook the fundamental differences between language modeling and contrastive learning, failing to fully leverage the potential of LLMs. More closely related to our work is Llama2Vec ____, which proposes two pretext tasks to enable unsupervised adaptation of LLMs, followed by contrastive learning fine-tuning to achieve better performance. In contrast, our approach achieves strong results without any need for contrastive learning fine-tuning, as our task fully exploits the inherent potential of LLMs.


\paragraph{LLMs without Fine-Tuning}
Several studies have explored methods to transform LLMs into text encoders without fine-tuning. ____ proposed using possible trajectory distributions as text representations, achieving effectiveness but at a high computational cost. ____ introduced echo embeddings by repeatedly feeding text into autoregressive models, addressing architectural limitations but doubling computational requirements. Other methods focus on prompt adjustments to produce meaningful embeddings. PromptEOL ____ introduced a One-Word Limitation prompt to improve embedding performance, while MetaEOL ____ extended this idea by using eight different prompt types to generate multi-view embeddings. GenEOL ____ leveraged LLMs to create various sentence transformations that retain their meaning, aggregating the resulting embeddings to enhance the overall sentence representation. Meanwhile, PromptReps ____ developed a hybrid document retrieval framework leveraging prompts to address challenges in information retrieval tasks. Despite these innovations, these approaches either perform poorly or require multiple inferences to achieve good results. By contrast, our method surpasses these methods with minimal training costs.