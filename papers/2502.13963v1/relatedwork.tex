\section{Related Work}
\paragraph{Attention-Based Salience for Long-Context.} Since the attention mechanism was first introduced by~\citep{bahdanau2014neural}, attention weight has become an important tool for interpreting important information in the input sequence~\citep{serrano-smith-2019-attention, ferrando2024primer}. For example, ~\citet{peysakhovich2023attention} use attention weights to estimate the importance of documents that can be leveraged to arrange their positions, thus improving the performance of long-context LLMs. \citet{xiao2024duoattention} manage to reduce KV cache for attention heads based on their attention patterns. \citet{he2024seekr} investigate the importance of attention weights in knowledge retention. Obviously, the attention mechanism has not only been a critical and reliable information resource for processing various long-context tasks~\cite{xiaoefficient, chen2024attention} but also presented substantial potential for further exploration and optimization~\citep{wu2024retrieval, lu2024longheads, he-etal-2024-never}. Our approach also highlights the function of certain attention heads in in-context retrieval~\citep{ram2023context}, aiming at optimizing attention distribution to get better long-context LLMs.

\paragraph{Distractions by Irrelevant Content.} Previous research has shown that LLMs can be easily disturbed by irrelevant context~\cite{shi2023large, wu2024easily}, making them overallocate attention to useless content. Some methods have been proposed to mitigate such issues.~\citet{liu2024bridging} introduce an innovative framework that helps LLMs recognize relevant entities in long contexts through efficient reference management.~\citet{wu2024reducing} reduce distractions by aligning the representations of the original context and the retrieved sub-context.~\citet{xiong2024artificial} enhance the retrieval capabilities of LLMs in highly similar contexts through fine-tuning on synthetic data. Another method proposes a novel differential attention mechanism to amplify attention to the relevant context while canceling attention noise~\citep{ye2024differential}. However, these methods do not explicitly optimize the attention distribution based on the input context, while our method provides a more straightforward and effective way.


%%%%%%%%%%%%%%%%%% need to be major revised %%%%%%%%%%%%%

\begin{figure*}[th]
    \centering
    \includegraphics[width= \textwidth]{Method.pdf}
    % \caption{\FMT~method is able to align the multilingual representations of truthful statements in semantic space, making LLMs more truthful to multilingual answer questions.}
    \caption{An overview of our proposed method. The goal of~\method~is to adjust the similarity between the \texttt{Query} features from the question and the \texttt{Key} features from the passages, thus making attention heads allocate more attention weights in relevant information and reducing distractions. CL means contrastive learning.}
    % , thus providing more accurate response.
    \label{fig:overview}
\end{figure*}

%%%%%%%%%%%%%%%%%% need to be major revised %%%%%%%%%%%%%

\paragraph{Contrastive Learning on Generative Models.} As a self-supervised training technique, contrastive learning~\citep{chopra2005learning, hadsell2006dimensionality, robinson2021contrastive} has been widely leveraged in NLP tasks such as sentence embedding~\citep{gao2021simcse,li2023multi,zhang2022fine}. With the advancement of generative language models~\citep{radford2019language}, contrastive learning has also exhibited great potential in decoder-only architectures to achieve better hidden expressiveness~\citep{su2022a, jain-etal-2023-contraclm, yan2024contrastive}. For long-context tasks,~\citet{caciularu-etal-2022-long} utilize contrastive learning to explicitly discriminate representations of supporting evidence sentences from negative ones in long-context QA.~\citet{wu2024reducing} also leverage contrastive learning to align representations of different contexts. However, our method applies contrastive learning inside the attention head components instead of sequence representations. To the best of our knowledge, we are the first to show the effectiveness of optimizing attention distributions by adjusting the similarity between query and key projections at the head level directly.