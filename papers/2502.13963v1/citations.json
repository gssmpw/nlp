[
  {
    "index": 0,
    "papers": [
      {
        "key": "bahdanau2014neural",
        "author": "Bahdanau, Dzmitry",
        "title": "Neural machine translation by jointly learning to align and translate"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "serrano-smith-2019-attention",
        "author": "Serrano, Sofia  and\nSmith, Noah A.",
        "title": "Is Attention Interpretable?"
      },
      {
        "key": "ferrando2024primer",
        "author": "Ferrando, Javier and Sarti, Gabriele and Bisazza, Arianna and Costa-juss{\\`a}, Marta R",
        "title": "A primer on the inner workings of transformer-based language models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "peysakhovich2023attention",
        "author": "Peysakhovich, Alexander and Lerer, Adam",
        "title": "Attention sorting combats recency bias in long context language models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "xiao2024duoattention",
        "author": "Xiao, Guangxuan and Tang, Jiaming and Zuo, Jingwei and Guo, Junxian and Yang, Shang and Tang, Haotian and Fu, Yao and Han, Song",
        "title": "Duoattention: Efficient long-context llm inference with retrieval and streaming heads"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "he2024seekr",
        "author": "He, Jinghan and Guo, Haiyun and Zhu, Kuan and Zhao, Zihan and Tang, Ming and Wang, Jinqiao",
        "title": "SEEKR: Selective Attention-Guided Knowledge Retention for Continual Learning of Large Language Models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "xiaoefficient",
        "author": "Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike",
        "title": "Efficient Streaming Language Models with Attention Sinks"
      },
      {
        "key": "chen2024attention",
        "author": "Chen, Shijie and Guti{\\'e}rrez, Bernal Jim{\\'e}nez and Su, Yu",
        "title": "Attention in Large Language Models Yields Efficient Zero-Shot Re-Rankers"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "wu2024retrieval",
        "author": "Wu, Wenhao and Wang, Yizhong and Xiao, Guangxuan and Peng, Hao and Fu, Yao",
        "title": "Retrieval head mechanistically explains long-context factuality"
      },
      {
        "key": "lu2024longheads",
        "author": "Lu, Yi and Zhou, Xin and He, Wei and Zhao, Jun and Ji, Tao and Gui, Tao and Zhang, Qi and Huang, Xuanjing",
        "title": "LongHeads: Multi-Head Attention is Secretly a Long Context Processor"
      },
      {
        "key": "he-etal-2024-never",
        "author": "He, Junqing  and\nPan, Kunhao  and\nDong, Xiaoqun  and\nSong, Zhuoyang  and\nLiuYiBo, LiuYiBo  and\nQianguosun, Qianguosun  and\nLiang, Yuxin  and\nWang, Hao  and\nZhang, Enming  and\nZhang, Jiaxing",
        "title": "Never Lost in the Middle: Mastering Long-Context Question Answering with Position-Agnostic Decompositional Training"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "ram2023context",
        "author": "Ram, Ori and Levine, Yoav and Dalmedigos, Itay and Muhlgay, Dor and Shashua, Amnon and Leyton-Brown, Kevin and Shoham, Yoav",
        "title": "In-Context Retrieval-Augmented Language Models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "shi2023large",
        "author": "Shi, Freda and Chen, Xinyun and Misra, Kanishka and Scales, Nathan and Dohan, David and Chi, Ed H and Sch{\\\"a}rli, Nathanael and Zhou, Denny",
        "title": "Large language models can be easily distracted by irrelevant context"
      },
      {
        "key": "wu2024easily",
        "author": "Wu, Siye and Xie, Jian and Chen, Jiangjie and Zhu, Tinghui and Zhang, Kai and Xiao, Yanghua",
        "title": "How Easily do Irrelevant Inputs Skew the Responses of Large Language Models?"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "liu2024bridging",
        "author": "Liu, Yanming and Peng, Xinyue and Cao, Jiannan and Bo, Shi and Shen, Yanxin and Zhang, Xuhong and Cheng, Sheng and Wang, Xun and Yin, Jianwei and Du, Tianyu",
        "title": "Bridging context gaps: Leveraging coreference resolution for long contextual understanding"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "wu2024reducing",
        "author": "Wu, Zijun and Liu, Bingyuan and Yan, Ran and Chen, Lei and Delteil, Thomas",
        "title": "Reducing Distraction in Long-Context Language Models by Focused Learning"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "xiong2024artificial",
        "author": "Xiong, Zheyang and Papageorgiou, Vasilis and Lee, Kangwook and Papailiopoulos, Dimitris",
        "title": "From artificial needles to real haystacks: Improving retrieval capabilities in llms by finetuning on synthetic data"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "ye2024differential",
        "author": "Ye, Tianzhu and Dong, Li and Xia, Yuqing and Sun, Yutao and Zhu, Yi and Huang, Gao and Wei, Furu",
        "title": "Differential transformer"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "chopra2005learning",
        "author": "Unknown",
        "title": "Learning a similarity metric discriminatively, with application to face verification"
      },
      {
        "key": "hadsell2006dimensionality",
        "author": "Hadsell, Raia and Chopra, Sumit and LeCun, Yann",
        "title": "Dimensionality reduction by learning an invariant mapping"
      },
      {
        "key": "robinson2021contrastive",
        "author": "Joshua David Robinson and Ching-Yao Chuang and Suvrit Sra and Stefanie Jegelka",
        "title": "Contrastive Learning with Hard Negative Samples"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "gao2021simcse",
        "author": "Gao, Tianyu and Yao, Xingcheng and Chen, Danqi",
        "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings"
      },
      {
        "key": "li2023multi",
        "author": "Li, Dawei and Zhang, Hengyuan and Li, Yanran and Yang, Shiping",
        "title": "Multi-level Contrastive Learning for Script-based Character Understanding"
      },
      {
        "key": "zhang2022fine",
        "author": "Zhang, Hengyuan and Li, Dawei and Yang, Shiping and Li, Yanran",
        "title": "Fine-grained Contrastive Learning for Definition Generation"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "radford2019language",
        "author": "Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others",
        "title": "Language models are unsupervised multitask learners"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "su2022a",
        "author": "Yixuan Su and Tian Lan and Yan Wang and Dani Yogatama and Lingpeng Kong and Nigel Collier",
        "title": "A Contrastive Framework for Neural Text Generation"
      },
      {
        "key": "jain-etal-2023-contraclm",
        "author": "Jain, Nihal  and\nZhang, Dejiao  and\nAhmad, Wasi Uddin  and\nWang, Zijian  and\nNan, Feng  and\nLi, Xiaopeng  and\nTan, Ming  and\nNallapati, Ramesh  and\nRay, Baishakhi  and\nBhatia, Parminder  and\nMa, Xiaofei  and\nXiang, Bing",
        "title": "{C}ontra{CLM}: Contrastive Learning For Causal Language Model"
      },
      {
        "key": "yan2024contrastive",
        "author": "Yan, Tianyi Lorena and Wang, Fei and Huang, James Y and Zhou, Wenxuan and Yin, Fan and Galstyan, Aram and Yin, Wenpeng and Chen, Muhao",
        "title": "Contrastive instruction tuning"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "caciularu-etal-2022-long",
        "author": "Caciularu, Avi  and\nDagan, Ido  and\nGoldberger, Jacob  and\nCohan, Arman",
        "title": "Long Context Question Answering via Supervised Contrastive Learning"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "wu2024reducing",
        "author": "Wu, Zijun and Liu, Bingyuan and Yan, Ran and Chen, Lei and Delteil, Thomas",
        "title": "Reducing Distraction in Long-Context Language Models by Focused Learning"
      }
    ]
  }
]