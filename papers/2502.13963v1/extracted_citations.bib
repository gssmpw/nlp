@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@inproceedings{caciularu-etal-2022-long,
    title = "Long Context Question Answering via Supervised Contrastive Learning",
    author = "Caciularu, Avi  and
      Dagan, Ido  and
      Goldberger, Jacob  and
      Cohan, Arman",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.207/",
    doi = "10.18653/v1/2022.naacl-main.207",
    pages = "2872--2879",
    abstract = "Long-context question answering (QA) tasks require reasoning over a long document or multiple documents. Addressing these tasks often benefits from identifying a set of evidence spans (e.g., sentences), which provide supporting evidence for answering the question. In this work, we propose a novel method for equipping long-context QA models with an additional sequence-level objective for better identification of the supporting evidence. We achieve this via an additional contrastive supervision signal in finetuning, where the model is encouraged to explicitly discriminate supporting evidence sentences from negative ones by maximizing question-evidence similarity. The proposed additional loss exhibits consistent improvements on three different strong long-context transformer models, across two challenging question answering benchmarks {--} HotpotQA and QAsper."
}

@article{chen2024attention,
  title={Attention in Large Language Models Yields Efficient Zero-Shot Re-Rankers},
  author={Chen, Shijie and Guti{\'e}rrez, Bernal Jim{\'e}nez and Su, Yu},
  journal={arXiv preprint arXiv:2410.02642},
  year={2024}
}

@inproceedings{chopra2005learning,
  title={Learning a similarity metric discriminatively, with application to face verification},
  booktitle={2005 IEEE computer society conference on computer vision and pattern recognition (CVPR'05)},
  volume={1},
  pages={539--546},
  year={2005},
  organization={IEEE}
}

@article{ferrando2024primer,
  title={A primer on the inner workings of transformer-based language models},
  author={Ferrando, Javier and Sarti, Gabriele and Bisazza, Arianna and Costa-juss{\`a}, Marta R},
  journal={arXiv preprint arXiv:2405.00208},
  year={2024}
}

@inproceedings{gao2021simcse,
  title={SimCSE: Simple Contrastive Learning of Sentence Embeddings},
  author={Gao, Tianyu and Yao, Xingcheng and Chen, Danqi},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={6894--6910},
  year={2021}
}

@inproceedings{hadsell2006dimensionality,
  title={Dimensionality reduction by learning an invariant mapping},
  author={Hadsell, Raia and Chopra, Sumit and LeCun, Yann},
  booktitle={2006 IEEE computer society conference on computer vision and pattern recognition (CVPR'06)},
  volume={2},
  pages={1735--1742},
  year={2006},
  organization={IEEE}
}

@inproceedings{he-etal-2024-never,
    title = "Never Lost in the Middle: Mastering Long-Context Question Answering with Position-Agnostic Decompositional Training",
    author = "He, Junqing  and
      Pan, Kunhao  and
      Dong, Xiaoqun  and
      Song, Zhuoyang  and
      LiuYiBo, LiuYiBo  and
      Qianguosun, Qianguosun  and
      Liang, Yuxin  and
      Wang, Hao  and
      Zhang, Enming  and
      Zhang, Jiaxing",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.736",
    doi = "10.18653/v1/2024.acl-long.736",
    pages = "13628--13642",
    abstract = "While large language models (LLMs) are equipped with longer text input capabilities than before, they are struggling to seek correct information in long contexts. The {``}lost in the middle{''} problem challenges most LLMs, referring to the dramatic decline in accuracy when correct information is located in the middle. To overcome this crucial issue, this paper proposes to enhance the information searching and reflection ability of LLMs in long contexts via specially designed tasks called Position-Agnostic Multi-step QA (PAM QA). Trained in this task, our model excels in focusing more precisely on the desired information. Experimental results show substantial improvement in Multi-doc QA and other benchmarks, superior to state-of-the-art models by 13.7{\%} absolute gain in shuffled settings, by 21.5{\%} in passage retrieval task. We release our model and code to promote related research in the community.",
}

@inproceedings{he2024seekr,
  title={SEEKR: Selective Attention-Guided Knowledge Retention for Continual Learning of Large Language Models},
  author={He, Jinghan and Guo, Haiyun and Zhu, Kuan and Zhao, Zihan and Tang, Ming and Wang, Jinqiao},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={3254--3266},
  year={2024}
}

@inproceedings{jain-etal-2023-contraclm,
    title = "{C}ontra{CLM}: Contrastive Learning For Causal Language Model",
    author = "Jain, Nihal  and
      Zhang, Dejiao  and
      Ahmad, Wasi Uddin  and
      Wang, Zijian  and
      Nan, Feng  and
      Li, Xiaopeng  and
      Tan, Ming  and
      Nallapati, Ramesh  and
      Ray, Baishakhi  and
      Bhatia, Parminder  and
      Ma, Xiaofei  and
      Xiang, Bing",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.355/",
    doi = "10.18653/v1/2023.acl-long.355",
    pages = "6436--6459",
    abstract = "Despite exciting progress in causal language models, the expressiveness of their representations is largely limited due to poor discrimination ability. To remedy this issue, we present CONTRACLM, a novel contrastive learning framework at both the token-level and the sequence-level. We assess CONTRACLM on a variety of downstream tasks. We show that CONTRACLM enhances the discrimination of representations and bridges the gap with encoder-only models, which makes causal language models better suited for tasks beyond language generation. Specifically, we attain 44{\%} relative improvement on the Semantic Textual Similarity tasks and 34{\%} on Code-to-Code Search tasks. Furthermore, by improving the expressiveness of representations, CONTRACLM also boosts the source code generation capability with 9{\%} relative improvement on execution accuracy on the HumanEval benchmark."
}

@inproceedings{li2023multi,
  title={Multi-level Contrastive Learning for Script-based Character Understanding},
  author={Li, Dawei and Zhang, Hengyuan and Li, Yanran and Yang, Shiping},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={5995--6013},
  year={2023}
}

@article{liu2024bridging,
  title={Bridging context gaps: Leveraging coreference resolution for long contextual understanding},
  author={Liu, Yanming and Peng, Xinyue and Cao, Jiannan and Bo, Shi and Shen, Yanxin and Zhang, Xuhong and Cheng, Sheng and Wang, Xun and Yin, Jianwei and Du, Tianyu},
  journal={arXiv preprint arXiv:2410.01671},
  year={2024}
}

@article{lu2024longheads,
  title={LongHeads: Multi-Head Attention is Secretly a Long Context Processor},
  author={Lu, Yi and Zhou, Xin and He, Wei and Zhao, Jun and Ji, Tao and Gui, Tao and Zhang, Qi and Huang, Xuanjing},
  journal={arXiv preprint arXiv:2402.10685},
  year={2024}
}

@article{peysakhovich2023attention,
  title={Attention sorting combats recency bias in long context language models},
  author={Peysakhovich, Alexander and Lerer, Adam},
  journal={arXiv preprint arXiv:2310.01427},
  year={2023}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{ram2023context,
  title={In-Context Retrieval-Augmented Language Models},
  author={Ram, Ori and Levine, Yoav and Dalmedigos, Itay and Muhlgay, Dor and Shashua, Amnon and Leyton-Brown, Kevin and Shoham, Yoav},
  journal={Transactions of the Association for Computational Linguistics},
  volume={11},
  pages={1316--1331},
  year={2023}
}

@inproceedings{serrano-smith-2019-attention,
    title = "Is Attention Interpretable?",
    author = "Serrano, Sofia  and
      Smith, Noah A.",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1282/",
    doi = "10.18653/v1/P19-1282",
    pages = "2931--2951",
    abstract = "Attention mechanisms have recently boosted performance on a range of NLP tasks. Because attention layers explicitly weight input components' representations, it is also often assumed that attention can be used to identify information that models found important (e.g., specific contextualized word tokens). We test whether that assumption holds by manipulating attention weights in already-trained text classification models and analyzing the resulting differences in their predictions. While we observe some ways in which higher attention weights correlate with greater impact on model predictions, we also find many ways in which this does not hold, i.e., where gradient-based rankings of attention weights better predict their effects than their magnitudes. We conclude that while attention noisily predicts input components' overall importance to a model, it is by no means a fail-safe indicator."
}

@inproceedings{shi2023large,
  title={Large language models can be easily distracted by irrelevant context},
  author={Shi, Freda and Chen, Xinyun and Misra, Kanishka and Scales, Nathan and Dohan, David and Chi, Ed H and Sch{\"a}rli, Nathanael and Zhou, Denny},
  booktitle={International Conference on Machine Learning},
  pages={31210--31227},
  year={2023},
  organization={PMLR}
}

@article{wu2024easily,
  title={How Easily do Irrelevant Inputs Skew the Responses of Large Language Models?},
  author={Wu, Siye and Xie, Jian and Chen, Jiangjie and Zhu, Tinghui and Zhang, Kai and Xiao, Yanghua},
  journal={arXiv preprint arXiv:2404.03302},
  year={2024}
}

@article{wu2024reducing,
  title={Reducing Distraction in Long-Context Language Models by Focused Learning},
  author={Wu, Zijun and Liu, Bingyuan and Yan, Ran and Chen, Lei and Delteil, Thomas},
  journal={arXiv preprint arXiv:2411.05928},
  year={2024}
}

@article{wu2024retrieval,
  title={Retrieval head mechanistically explains long-context factuality},
  author={Wu, Wenhao and Wang, Yizhong and Xiao, Guangxuan and Peng, Hao and Fu, Yao},
  journal={arXiv preprint arXiv:2404.15574},
  year={2024}
}

@article{xiao2024duoattention,
  title={Duoattention: Efficient long-context llm inference with retrieval and streaming heads},
  author={Xiao, Guangxuan and Tang, Jiaming and Zuo, Jingwei and Guo, Junxian and Yang, Shang and Tang, Haotian and Fu, Yao and Han, Song},
  journal={arXiv preprint arXiv:2410.10819},
  year={2024}
}

@inproceedings{xiaoefficient,
  title={Efficient Streaming Language Models with Attention Sinks},
  author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  booktitle={The Twelfth International Conference on Learning Representations}
}

@article{xiong2024artificial,
  title={From artificial needles to real haystacks: Improving retrieval capabilities in llms by finetuning on synthetic data},
  author={Xiong, Zheyang and Papageorgiou, Vasilis and Lee, Kangwook and Papailiopoulos, Dimitris},
  journal={arXiv preprint arXiv:2406.19292},
  year={2024}
}

@article{yan2024contrastive,
  title={Contrastive instruction tuning},
  author={Yan, Tianyi Lorena and Wang, Fei and Huang, James Y and Zhou, Wenxuan and Yin, Fan and Galstyan, Aram and Yin, Wenpeng and Chen, Muhao},
  journal={arXiv preprint arXiv:2402.11138},
  year={2024}
}

@article{ye2024differential,
  title={Differential transformer},
  author={Ye, Tianzhu and Dong, Li and Xia, Yuqing and Sun, Yutao and Zhu, Yi and Huang, Gao and Wei, Furu},
  journal={arXiv preprint arXiv:2410.05258},
  year={2024}
}

@inproceedings{zhang2022fine,
  title={Fine-grained Contrastive Learning for Definition Generation},
  author={Zhang, Hengyuan and Li, Dawei and Yang, Shiping and Li, Yanran},
  booktitle={Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={1001--1012},
  year={2022}
}

