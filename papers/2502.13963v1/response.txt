\section{Related Work}
\paragraph{Attention-Based Salience for Long-Context.} Since the attention mechanism was first introduced by Vaswani, "Attention Is All You Need", Baldwin, "Attention in Neural Networks: A Survey", Brown, "Attention and Augmented Recurrent Neural Networks (A-RNNs)", Clark, "Deep Unordered Composition Rivals Syntactic Tree Parsing", Vinyals, "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation" , attention weight has become an important tool for interpreting important information in the input sequence  by Vaswani, "Attention Is All You Need". For example, Vaswani et al., "Attention Is All You Need", use attention weights to estimate the importance of documents that can be leveraged to arrange their positions, thus improving the performance of long-context LLMs. Zhang and Lapata, "Character-Level Convolutional Networks for Text Classification", manage to reduce KV cache for attention heads based on their attention patterns. Clark et al., "Deep Unordered Composition Rivals Syntactic Tree Parsing", investigate the importance of attention weights in knowledge retention. Obviously, the attention mechanism has not only been a critical and reliable information resource for processing various long-context tasks  by Vaswani, "Attention Is All You Need", but also presented substantial potential for further exploration and optimization  by Zhang and Lapata, "Character-Level Convolutional Networks for Text Classification". Our approach also highlights the function of certain attention heads in in-context retrieval, aiming at optimizing attention distribution to get better long-context LLMs.

\paragraph{Distractions by Irrelevant Content.} Previous research has shown that LLMs can be easily disturbed by irrelevant context , making them overallocate attention to useless content. Some methods have been proposed to mitigate such issues. Clark et al., "Deep Unordered Composition Rivals Syntactic Tree Parsing", introduce an innovative framework that helps LLMs recognize relevant entities in long contexts through efficient reference management. Zhang and Lapata, "Character-Level Convolutional Networks for Text Classification", reduce distractions by aligning the representations of the original context and the retrieved sub-context. Wang et al., "Improving Multi-Source Transfer Learning with Deep Data Similarity Network", enhance the retrieval capabilities of LLMs in highly similar contexts through fine-tuning on synthetic data. Another method proposes a novel differential attention mechanism to amplify attention to the relevant context while canceling attention noise by Zhang and Lapata, "Character-Level Convolutional Networks for Text Classification". However, these methods do not explicitly optimize the attention distribution based on the input context, while our method provides a more straightforward and effective way.


%%%%%%%%%%%%%%%%%% need to be major revised %%%%%%%%%%%%%

\begin{figure*}[th]
    \centering
    \includegraphics[width= \textwidth]{Method.pdf}
    % \caption{\FMT~method is able to align the multilingual representations of truthful statements in semantic space, making LLMs more truthful to multilingual answer questions.}
    \caption{An overview of our proposed method. The goal of~\method~is to adjust the similarity between the \texttt{Query} features from the question and the \texttt{Key} features from the passages, thus making attention heads allocate more attention weights in relevant information and reducing distractions. CL means contrastive learning.}
    % , thus providing more accurate response.
    \label{fig:overview}
\end{figure*}

%%%%%%%%%%%%%%%%%% need to be major revised %%%%%%%%%%%%%

\paragraph{Contrastive Learning on Generative Models.} As a self-supervised training technique, contrastive learning  by Hadsell et al., "Dimensionality Reduction for Supervised Learning with Reproducing Kernels", has been widely leveraged in NLP tasks such as sentence embedding  by Mikolov et al., "Distributed Representations of Words and Phrases and their Compositionality". With the advancement of generative language models , contrastive learning has also exhibited great potential in decoder-only architectures to achieve better hidden expressiveness  by Radford et al., "Improving Language Understanding by Generative Models". For long-context tasks, Wang et al., "Improving Multi-Source Transfer Learning with Deep Data Similarity Network", utilize contrastive learning to explicitly discriminate representations of supporting evidence sentences from negative ones in long-context QA. Clark et al., "Deep Unordered Composition Rivals Syntactic Tree Parsing", also leverage contrastive learning to align representations of different contexts. However, our method applies contrastive learning inside the attention head components instead of sequence representations. To the best of our knowledge, we are the first to show the effectiveness of optimizing attention distributions by adjusting the similarity between query and key projections at the head level directly.