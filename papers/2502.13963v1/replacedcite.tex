\section{Related Work}
\paragraph{Attention-Based Salience for Long-Context.} Since the attention mechanism was first introduced by____, attention weight has become an important tool for interpreting important information in the input sequence____. For example, ____ use attention weights to estimate the importance of documents that can be leveraged to arrange their positions, thus improving the performance of long-context LLMs. ____ manage to reduce KV cache for attention heads based on their attention patterns. ____ investigate the importance of attention weights in knowledge retention. Obviously, the attention mechanism has not only been a critical and reliable information resource for processing various long-context tasks____ but also presented substantial potential for further exploration and optimization____. Our approach also highlights the function of certain attention heads in in-context retrieval____, aiming at optimizing attention distribution to get better long-context LLMs.

\paragraph{Distractions by Irrelevant Content.} Previous research has shown that LLMs can be easily disturbed by irrelevant context____, making them overallocate attention to useless content. Some methods have been proposed to mitigate such issues.____ introduce an innovative framework that helps LLMs recognize relevant entities in long contexts through efficient reference management.____ reduce distractions by aligning the representations of the original context and the retrieved sub-context.____ enhance the retrieval capabilities of LLMs in highly similar contexts through fine-tuning on synthetic data. Another method proposes a novel differential attention mechanism to amplify attention to the relevant context while canceling attention noise____. However, these methods do not explicitly optimize the attention distribution based on the input context, while our method provides a more straightforward and effective way.


%%%%%%%%%%%%%%%%%% need to be major revised %%%%%%%%%%%%%

\begin{figure*}[th]
    \centering
    \includegraphics[width= \textwidth]{Method.pdf}
    % \caption{\FMT~method is able to align the multilingual representations of truthful statements in semantic space, making LLMs more truthful to multilingual answer questions.}
    \caption{An overview of our proposed method. The goal of~\method~is to adjust the similarity between the \texttt{Query} features from the question and the \texttt{Key} features from the passages, thus making attention heads allocate more attention weights in relevant information and reducing distractions. CL means contrastive learning.}
    % , thus providing more accurate response.
    \label{fig:overview}
\end{figure*}

%%%%%%%%%%%%%%%%%% need to be major revised %%%%%%%%%%%%%

\paragraph{Contrastive Learning on Generative Models.} As a self-supervised training technique, contrastive learning____ has been widely leveraged in NLP tasks such as sentence embedding____. With the advancement of generative language models____, contrastive learning has also exhibited great potential in decoder-only architectures to achieve better hidden expressiveness____. For long-context tasks,____ utilize contrastive learning to explicitly discriminate representations of supporting evidence sentences from negative ones in long-context QA.____ also leverage contrastive learning to align representations of different contexts. However, our method applies contrastive learning inside the attention head components instead of sequence representations. To the best of our knowledge, we are the first to show the effectiveness of optimizing attention distributions by adjusting the similarity between query and key projections at the head level directly.