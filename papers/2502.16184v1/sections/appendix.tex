\section*{Appendix}
\appendix
\section{Legal Terminology}\label{apx:legal}

The \EUAIAct\ is formally structured into recitals (\EWx), articles (\Artx), and annexes. Recitals are legally non-binding and outline the rationale behind the articles, articles delineate specific binding obligations, and the annexes provide additional details and specifications to support the articles~\citep{klimas15law}.


\section{Notions of High-Risk AI Systems and General Purpose Models provided in \EUAIAct }\label{apx:hrais-gpaim}

\paragraph{AI System.} The \EUAIAct\ defines an AI system as ``a machine-based\footnote{Thereby,
machine-based ``refers to the fact that AI systems run on machines'' (\EW{12}).} system that is designed to operate with varying levels
of autonomy and that may exhibit adaptiveness after deployment, and that, for explicit or
implicit objectives, infers, from the input it receives, how to generate outputs such as
predictions, content, recommendations, or decisions that can influence physical or virtual
environments'' (\Art{3(1)}). 
%
Thereby, \EW{12}, suggests that notion of ‘AI system’ ``should be clearly defined and should be
closely aligned with the work of international organisations working on AI to ensure legal
certainty, facilitate international convergence and wide acceptance, while providing the
flexibility to accommodate the rapid technological developments in this field''. Importantly, ``the definition should be based on key characteristics of AI systems that distinguish it from
simpler traditional software systems or programming approaches and should not cover
systems that are based on the rules defined solely by natural persons to automatically
execute operations'' (\EW{12}). 
Thereby a ``key characteristic of AI systems is their capability to infer'', which refers to ``the process of obtaining the outputs, such as predictions,
content, recommendations, or decisions, which can influence physical and virtual
environments, and to a capability of AI systems to derive models or algorithms, or both,
from inputs or data'' (\EW{12}). Examples for techniques that enable inference 
``include machine learning approaches that learn from data how to achieve certain
objectives, and logic- and knowledge-based approaches that infer from encoded knowledge
or symbolic representation of the task to be solved'' (\EW{12}).
%
% \todo{Die Abgrenzung AI and non-ai Systems ist umstritten.}

\paragraph{High-risk AI Systems (HRAIS).} The \EUAIAct\ classifies AI systems into different risk groups.
%
Risk refers thereby to ``the combination of the probability of an occurrence of harm and the severity
of that harm'' (\Art{3(2)}).
%
An AI system is considered high-risk, if it is ``intended to be used as a safety component of a product, or the AI
system is itself a product'' and ``is required to undergo a third-party conformity
assessment, with a view to the placing on the market or the putting into service of that product" covered by the Union harmonisation legislation listed in
Annex I (\Art{6(1)}).
%
Annex I \EUAIAct\ provides a list of 20 EU harmonisation legislation. 
%
For example, the Machinery Directive (Directive 2006/42/EC and amending  Directive 95/16/EC), the directive on the safety of toys (Directive 2009/48/EC), or the directive concerning agricultural and forestry
vehicles (Regulation (EU) No 167/2013).
%
This means that for example, an AI system that is used in a product or as a product itself considered as a toy falling under Directive 2009/48/EC, would be considered as a high-risk AI system.
% 

An AI systems is also considered high-risk, if it operates in types or use-cases enlisted in
Annex III (\Art{6(2)}) and poses a significant risk of harm to the health,
safety or fundamental rights of natural person following (\Art{6(3)}).
%
Annex III enlists eight types or use-cases of AI systems: biometric applications (e.g., remote biometric identification systems excluding biometric verification, biometric categorisation, and emotion recognition); critical infrastructure (e.g., ``critical digital infrastructure, road traffic, or in the supply of
water, gas, heating or electricity''); education and vocational training; employment, workers management and access to self-employment; essential private and public services; law enforcement; migration, asylum and border control management; and administration of justice and democratic processes. 
%
%
Thereby, the Commission shall ``provide [...] a comprehensive list of practical examples of use cases
of AI systems that are high-risk and not high-risk'' (\Art{6(5)}).
%
 \EW{52} suggests that ``it is appropriate to classify
them [i.e., AI systems] as high-risk if, in light of their intended purpose, they pose a high risk of harm to the
health and safety or the fundamental rights of persons, taking into account both the severity
of the possible harm and its probability of occurrence and they are used in a number of
specifically pre-defined areas''. 
%
Thereby the methodology and citeria for the identification of high-risk should be able to be adopted in order to 
% 
account for ``the rapid pace of technological
development, as well as the potential changes in the use of AI systems'' (\EW{52}).
%
For example, an AI system used for remote biometric identification is considered high-risk, unless it does not pose a significant risk. Remote identification refers to comparing biometric data of that individual to stored biometric data of
individuals in a reference database (\EW{15}), where a remote biometric identification system should be understood as a ``AI system intended for the identification of natural persons
without their active involvement, typically at a distance'' (\EW{17}).


\paragraph{General-purpose AI Models (GPAIM).}
%
A general-purpose AI model (\GPAIM) refers to an ``AI model [...] that displays significant
generality and is capable of competently performing a wide range of distinct tasks
regardless of the way the model is placed on the market and that can be integrated into a
variety of downstream systems or applications'' (\Art{3(63)}). 
%
The \EUAIAct, however, explicitly excludes from its regulation AI models that may fall under this definition but are ``used for
research, development or prototyping activities before they are placed on the market'' (\Art{3(63)}).
%
\EW{97} suggest that the term general-purpose AI model ``should be clearly defined and set apart from the
notion of AI systems to enable legal certainty'' taking into account the ``key
functional characteristics of a general-purpose AI model, in particular the generality and
the capability to competently perform a wide range of distinct tasks''. 
%
Thereby, GPAIMs ``may
be placed on the market in various ways, including through libraries, application
programming interfaces (APIs), as direct download, or as physical copy'', and ``may be further modified or fine-tuned into new models'' (\EW{97}).
%
Examples for GPAIMs, are large generative AI models that ``allow for flexible generation of content, such as in the form of text, audio, images
or video, that can readily accommodate a wide range of distinctive tasks'' (\EW{99}).
%
This is the case for many multimodal large language models, such as GPT-4 Omni (GPT-4o)~\footnote{\url{https://openai.com/gpt-4o-contributions/}}, or Gemini~\footnote{\url{https://gemini.google.com}}.

\paragraph{\GPAIM\ with Systemic Risk.} 
The systemic risk of a \GPAIM\ is understood as ``a risk that is specific to the high-impact capabilities of
general-purpose AI models, having a significant impact on the Union market due to their
reach, or due to actual or reasonably foreseeable negative effects on public health, safety,
public security, fundamental rights, or the society as a whole, that can be propagated at
scale across the value chain'' (\Art{3(65)}). 
%
\EW{110} provides as a non-exhaustive list of examples for systemic risks:
``any actual or reasonably foreseeable negative effects in relation to major accidents,
disruptions of critical sectors and serious consequences to public health and safety; any
actual or reasonably foreseeable negative effects on democratic processes, public and
economic security; the dissemination of illegal, false, or discriminatory content''. Thereby ``[s]ystemic
risks should be understood to increase with model capabilities and model reach, can arise
along the entire lifecycle of the model, and are influenced by conditions of misuse, model
reliability, model fairness and model security, the level of autonomy of the model, its
access to tools, novel or combined modalities, release and distribution strategies, the
potential to remove guardrails and other factors'' (\EW{110}). 

According to \Art{51(1)(a)}, a GPAIM is considered posing a
systemic risk, if ``it has high impact capabilities evaluated on the basis of appropriate technical tools
and methodologies, including indicators and benchmarks''. 
%
High-impact capabilities are understood as ``capabilities that match or exceed the capabilities recorded
in the most advanced general-purpose AI models'' (\Art{3(64)}).
%
%
Thereby high impact capabilities of a GPAIM are to be assumed, ``when the cumulative amount of computation used for its training
measured in floating point operations is greater than $10^{25}$'' (\Art{51(2)}).
%
\EW{111} states that ``according to the state of
the art at the time of entry into force of this Regulation, the cumulative amount of
computation used for the training of the general-purpose AI model measured in floating
point operations is one of the relevant approximations for model capabilities''.
%
According to \Art{51(1)(b)} a GPAIM is also considered to pose a systemic risk if---``based on a decision of the Commission''---it is considered having capabilities or an impact
equivalent to those set out in \Art{51(1)(a)} according to the criteria set out in Annex XIII.
%
The seven criteria enlisted in Annex XIII include the number of model parameters, the quality or size of the data set,  the amount of computation used for training the model, the input and output modalities of the model, the benchmarks and evaluations of capabilities of the model, whether it has a high impact on the internal market due to its reach, and the number of registered end-users.
% 

Importantly, according to \Art{51(3)}, the EU Commission shall be able to amend the
thresholds in \Artx\ 51(1) and (2) \EUAIAct\ and add ``benchmarks
and indicators in light of evolving technological developments, such as algorithmic
improvements or increased hardware efficiency, when necessary, for these thresholds to
reflect the state of the art'' (see also \EW{111}).


\section{Lifecycle in \Art{15(1)}}\label{apx:lifecycle}

The exact time frame during which consistent performance must be 
ensured following \Art{15(1)} is unclear. Particularly, the term `lifecycle' is not defined, leaving open whether it differs from the term `lifetime' used in \Art{12(1)} and \EW{71}.
%
It is crucial to clarify the exact timeframe during which consistent performance must be maintained.
% 
While `lifecycle' and `lifetime' could initially be interpreted as synonyms~\cite{marcus2020promoting}, `lifetime' might refer specifically to the active operational period of the AI system~\cite{murakami2010lifespan}, whereas `lifecycle' could encompass a broader view of all phases from product design and development to decommissioning~\cite{hamon2024three}. If this broader interpretation of `lifecycle' is intended, it raises questions about how accuracy, robustness, and cybersecurity should be ensured beyond the operational phase (e.g., during development), and why this would be necessary when there are no immediate risks to health, safety, and fundamental rights. One explanation for using the term 'lifecycle' would be that the EU legislator intended to emphasize that the requirements of \Art{15} should not only be assessed when the system is ready for deployment but also during the design process. Accordingly, technical standards should define both terms.



\section{Relevant Circumstance \Art{15(5)(ii)}}\label{apx:relevant-circumstances}

The term 'relevant circumstance' is not defined in the \EUAIAct\ and therefore requires interpretation.
%
On the one hand, one could argue that the term only refers to circumstances 
that are ``important'' for a ``particular purpose'' or context~\cite{CambridgeRelevant}.
%
On the other hand, the meaning of the term can also result from a comparison with other provisions of the \EUAIAct\ such as \Art{13(3)(b)(ii)}, which suggests a different understanding. 
%
The provision demands that the instructions for the use of AI systems shall contain ``any known and foreseeable circumstances'' that may have an impact on cybersecurity. 
%
This speaks for a broader understanding of relevance, which only excludes unknown and unforeseeable circumstances.
%
Given this ambiguity, standards should elaborate on how to determine relevant circumstances.



\section{Robustness-Accuracy Trade-Off}\label{apx:robustness-accuracy-trade-off}
% 
The ML literature has found that robustness and accuracy of ML models can in some scenarios be empirically and theoretically mutually inhibiting~\cite{zhang2019theoretically, li2024triangular, yang2020closer, raghunathan2020understanding, rade2022reducing}, and this relationship remains an active area of research~\cite{pang2022robustness}. 
%
As stated in Section~\ref{sec:general_challenges}, an AI system may also incorporate other technical components (beyond models) that must be robust and may impact the overall system's accuracy.
%
The \EUAIAct\ acknowledges these trade-offs but does not offer specific guidelines on how to achieve this balance. 
%
It requires providers of \HRAIS\ to ensure an appropriate level of both accuracy and robustness, and to find “an appropriate balance in implementing the measures to fulfil” the \EUAIAct\ requirements (\Art{9(4)}). Additionally, the technical documentation must include “decisions about any possible trade-off made regarding the technical solutions adopted to comply with the requirements” of the \EUAIAct\ (Annex IV Nr. 2 lit. b \EUAIAct). Standards may offer guidance on the processes and metrics to use but will not prescribe a specific balance, such as a 40-60 ratio. As a result, providers of \HRAIS\ will ultimately need to navigate these complex trade-offs themselves, adjusting individual model parameters to find an appropriate balance and justify and document their decisions.

