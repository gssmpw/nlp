
\section{Requirements for High-Risk AI Systems}\label{sec:legal_challenges}
%
In this section, we provide an analysis of the overarching challenges of implementing \Art{15} (Section~\ref{sec:general_challenges}), followed by a discussion regarding the robustness requirement in \Art{15(4)} (Section~\ref{sec:challenges_robustness}) and the cybersecurity requirement in \Art{15(5)} (Section~\ref{sec:challenges_cybersecurity}). 


\input{sections/general_challenges_short}


 \subsection{Robustness \Art{15(4)}}\label{sec:challenges_robustness}
%
We now turn to 
challenges specific to \Art{15(4)}.
\Art{15(4)(i)} states that ``technical and organisational measures shall be taken'' to ensure that AI systems are ``as resilient as possible regarding errors, faults or
inconsistencies that may occur within the system or the environment''. \Art{15(4)(ii)} specifies that robustness can be achieved through technical redundancy
solutions, and \Art{15(4)(iii)} requires addressing feedback loops in online learning with possibly biased outputs.


\paragraph{Inconsistent Terminology.}
The term robustness is used inconsistently throughout the \EUAIAct.
%
\Artx\ 15(1) and (4) \EUAIAct\ refer to robustness, whereas the corresponding \EW{27} and \EW{75} both mention technical robustness. 
%
One could argue that technical robustness is synonymous with robustness. 
% 
The term `technical robustness' in \EW{27} may be a remnant of the legislative process that built on the 2019 Ethics Guidelines for Trustworthy AI~\cite{aiiheg2019guidelines} developed by the AI IHEG, which introduced the principle of `technical robustness and safety'.  
%
These guidelines are explicitly referenced by \EW{27}.
%
Nevertheless, it remains unclear why \EW{75} also refers to `technical robustness'.
%
It could be that the wording in \EW{75} is borrowed from \EW{27}.
%
Alternatively, one could argue that robustness in \Artx\ 15(1) and (4) \EUAIAct\ is not limited to technical aspects, but additionally includes some form of non-technical robustness. 
%
The latter could refer to organizational measures that must be implemented to ensure robustness (\Art{15(4)(i)}).
%
Technical standards and guidelines by the EU Commission should clarify what aspects robustness encompasses.
% 


\paragraph{Required Level of 
Robustness.} 
% 
The \EUAIAct\ creates ambiguities regarding the required level of `robustness'.
%
\Art{15(1)} mandates that AI systems must achieve an ``appropriate level'' of robustness.
\Art{15(4)}, however, demands that AI systems shall be ``as resilient as possible'' to ``errors, faults, or inconsistencies'', suggesting a stricter requirement. 
% 
This discrepancy initially appears ambiguous, as it is unclear whether \HRAIS\ must simply meet an appropriate standard of robustness or strive for the highest possible level. 
% 
However, the ``appropriate'' level stated in \Art{15(1)} can be understood as a general principle, which is further specified by \Art{15(4)}. Therefore, appropriate with respect to robustness is to be understood as `as resilient as possible'. 


When determining the appropriate level of robustness of a specific \HRAIS, the intended purpose of the system and the generally acknowledged state of the art (SOTA) on AI and AI-related technologies must be taken into account (\Art{8(1)}).
%
\Art{9(4)} acknowledges that one of the objectives of the required risk management is to achieve an ``appropriate balance in the implementation of measures to fulfil'' requirements. 
\Art{9(5)} further acknowledges the permissibility of a residual risk, meaning that the measures adopted under the risk management system are not expected to eliminate all existing risks, but rather to maintain these residual risks at an `acceptable' level.
%
The risk management system is a continuous iterative process (\Art{9(1)}).
% 
This means that the appropriate level of robustness of \HRAIS\ must be regularly determined and updated, taking into account its purpose and the SOTA while balancing it with other requirements.


\paragraph{Feedback Loops.}
% 
\Art{15(4)(iii)} states that AI systems must be explicitly developed in such a way that they ``duly address'' feedback loops and ``eliminate or reduce'' the risks associated with them.
%
According to \EW{67}, feedback loops occur when the output of an AI system influences its input in future operations, an under
%
understanding that aligns with the concept as found in the ML literature.
%
Feedback loops are a well-studied problem manifesting in various forms~\cite{pagan2023classification}, with the most common issues being a distribution shift~\cite{perdomo2020performative} or a selection bias~\cite{kilbertus2020fair, lum2016predict}.
%
Importantly, in this context, the risk of 'biased outputs' in feedback loops (\Art{15(4)(iii)}) is often studied in the literature on fairness in ML rather than in the literature on \emph{robustness} in ML, which traditionally constitute different research fields and communities~\citep{lee2021machine}.
\footnote{For example, whether there is a trade-off between \emph{robustness} and fairness, or if both pursue similar goals, remains an active discussion in the ML community~\citep{lee2021machine, xu2021robust, pruksachatkun2021does}.
}

An important aspect of \Art{15(4)(iii)} is that it applies specifically to AI systems that learn online. 
%
Online learning ML models iteratively learn from a sequence of data and continuously update their parameters over time~\cite{hoi2021online}.
%
This adaptiveness is reflected in \Art{3(1)} as a factual characteristic of an AI system.
%
The problem with feedback loops in online learning is that newly collected training data can become biased, e.g., due to selection bias, which occurs when the data collected is not representative of the overall population~\cite{zadrozny2004learning, liu2014robust}. This can distort model predictions and reinforce existing biases, ultimately impacting the model's accuracy and fairness~\cite{kilbertus2020fair, bechavod2019equal, rateike2022don}.
%
Offline models, in contrast, are trained on a fixed dataset all at once~\cite{hoi2021online}.
%
Offline models can also carry risks when feedback loops are present: The outputs of an ML model can induce a distribution shift through their interaction with the environment~\cite{d2020fairness, zhang2020fair, liu2018delayed}. Since an offline ML model is not updated, distribution shifts can influence their performance over time and possibly lead to fairness concerns~\cite{liu2018delayed}. 
Although \Art{15(4)} does not explicitly address feedback loops in offline systems, \HRAIS\ are not exempt from addressing them.
% 
Since they can impact the model’s accuracy, feedback loops in offline systems may still need to be addressed to comply with \Art{15(1)}.
%


 \subsection{Cybersecurity \Art{15(5)}}\label{sec:challenges_cybersecurity}

We now turn to legal challenges specific to \Art{15(5)}. \Artx\ 15(5)(i) \EUAIAct\ states that AI systems shall be resilient against attempts to ``alter their use, outputs, or performance by exploiting system vulnerabilities''. \Art{15(5)(ii)} specifies that technical solutions aiming to ensure resilience against such malicious attempts ``shall be appropriate to the relevant circumstances and the risks''. Finally, \Art{15(5)(iii)} mandates specific measures ``to prevent, detect, respond to, and control for attacks'' exploiting AI-specific vulnerabilities. 
%
This section examines the key aspects of compliance with \Art{15(5)}.
However, a mentioned above, providers have an additional pathway for demonstrating compliance with its cybersecurity requirements, namely a certification under the CSA~\cite{casarosa2022cybersecurity}.


\paragraph{Required Level of Cybersecurity.}
%
\Art{15(5)(ii)} mandates that technical solutions must be ``appropriate to the relevant circumstances and the risks'', but this needs further clarification.
% 
The \EUAIAct\ specifically addresses only three kinds of risks: health, safety, and fundamental rights (\EW{1}). 
%
Risks associated with these aspects can be identified and managed through a risk management system that must be put into place as stipulated by \Art{9}.
%
Relevant circumstances are any known and foreseeable circumstances that may have an impact on cybersecurity.\footnote{See \Art{13(3)(b)(ii)} and Appendix~\ref{apx:relevant-circumstances}.}

Mandating a cybersecurity level that is `appropriate to the relevant circumstances' acknowledges that complex ML models generally cannot be expected to be fully resistant to all types of adversarial attacks.
%
This has two major reasons: First, it is impossible to anticipate all types of possible attacks. 
This is acknowledged by \Art{9(5)} which states that measures adopted under the risk management system are not expected to remove all existing risks.
Second, complete protection against a specific attack cannot be guaranteed, especially as adversaries continuously adapt their strategies to overcome possible defense mechanisms~\citep{xie2023defending, kumar2023certifying}. 
% 
The appropriateness of a certain performance level must consider the intended purpose of the system and the generally acknowledged SOTA (see \Art{8(1)}).
%
The measures to ensure cybersecurity adopted are not expected to eliminate all existing risks, but the overall residual risk must be acceptable (see \Art{9(1) and (4)}).
%
Thus, when determining the appropriateness of technical solutions, all applicable requirements of the \EUAIAct\ must be balanced, while also mitigating risks to health, safety, and fundamental rights.
% 

Lastly, while \HRAIS\ are expected to be `as resilient as possible' in terms of robustness, they need only to be `resilient' in terms of cybersecurity. Consequently, the wording of \Art{15} suggests that the robustness requirements are stricter. This may be due to the nature of unintentional causes—such as errors, faults, inconsistencies, or unexpected situations within the system or its operating environment—which are primarily within the provider’s control and justify a higher duty of care. In contrast, attacks by unauthorized third parties are less controllable and therefore justify a (slightly) lower standard for the provider’s duty regarding cybersecurity.


\paragraph{AI-specific Vulnerabilities.}
%
\Art{15(5)} differentiates between 'system vulnerabilities' (\Art{15(5)(i)}) and 'AI-specific vulnerabilities' (\Art{15(5)(iii)}). 
%
As the term vulnerability is not defined, 
%
we provide a working definition.
%
The United States' Common Vulnerabilities and Exposures (CVE) system defines vulnerability as ``[a]n instance of one or more weaknesses [...] that can be exploited, causing a negative impact to confidentiality, integrity, or availability''~\cite{cve_glossary}.
%
\Art{15(5)(iii)} provides a non-exhaustive list of components of an AI system that expose AI-specific vulnerabilities, such as training data, pre-trained components used in training, inputs, or the AI model.
% 
However, there might be additional components of the AI system that may also harbor AI-specific vulnerabilities. 
%
The question is how to identify them.
% 
We suggest performing a hypothetical test.
%
AI models play a central role in an AI system. If a vulnerability would be eliminated by replacing the AI model with a non-AI model, it should be deemed ‘AI-specific’. To define a non-AI model, 
%
we return to the definition of an AI system under the \EUAIAct.
% 
It has been argued that the central characteristic of an AI system is its ability to infer from input to output~\cite{hacker2024comments}. 
%
This inference ability is typically performed by one or more AI models within an AI system. 
%
Therefore, non-AI models are all models lacking inference capability, such as rule-based decision-making systems that rely on predefined rules and logic defined by human experts.\footnote{Note that non-AI rule-based systems use human-defined rules, while rule-based ML models infer rules from data~\cite{naik2023machine, weiss1995rule}, qualifying as AI models. In a different context, the AI IHEG ethics guidelines~\cite{aiiheg2019guidelines} suggest fallback plans where AI systems  switch from a statistical (ML) approach to a rule-based or human-in-the-loop approach.}
%
Since AI-specific vulnerabilities relate to specific components of an AI system, we suggest viewing them as a subset of system vulnerabilities. 
%
To enhance clarity, technical standards should define both terms and mandate a process for identifying them.



\paragraph{Technical Solutions.}
\Art{15(5)(iii)} provides a non-exhaustive list of attacks and AI-specific vulnerabilities that must be addressed through technical solutions:
% 
data poisoning, model poisoning, adversarial examples, model evasion, and confidentiality attacks, which are well-established in the ML literature.
% 
These attacks aim to induce model failures~\cite{vassilev2024adversarial}:
% 
\emph{Data poisoning} attacks manipulate training data~\citep{schwarzschild2021just}, \emph{model poisoning} attacks manipulate the trained ML model~\cite{zhang2022fldetector}, and \emph{model evasion} attacks manipulate test samples~\citep{biggio2013evasion}.
% 
\emph{Confidentiality attacks}, typically explored in the field of privacy in ML, refer to attempts to extract information about the training data or the model itself~\citep{rigaki2023survey}. 

In addition to these attacks, 
 \Art{15(5)(iii)} lists 'model flaws' as an AI-specific vulnerability.
 %
 This is a vague legal term and lacks an established counterpart in the ML literature. 
 %
 In software contexts, the word \emph{flaw} often refers to so-called \emph{bugs}, which are typically the result of human errors in the coding process~\cite{kumar2023, nissenbaum1996accountability}. 
 %
 However, the term model flaw follows the list of attacks outlined above, which are instead designed to exploit the default properties of a properly functioning ML model, and are not directly the results of errors in the coding process.
 %
 Thus, it is unclear what model flaw refers to in this context, and whether technical solutions are only expected to address traditional bugs or coding errors, or whether they should address other ways of exploiting AI-specific vulnerabilities.
%
Given that the term is situated within the cybersecurity requirements for AI system outlined in \Art{15(5)}, we argue that the term model flaws should be interpreted as flaws that enable the exploitation of AI-specific vulnerabilities.
%
Technical standards and guidelines by the EU Commission should define model flaws more clearly and provide guidelines for technical solutions to address these model flaws.
%
This should take into account the arms race between attacker and defender in the realm of adversarial robustness, where both parties are continuously adapting their strategies to outmaneuver the other~\cite{chen2017adversarial}.
%
This makes it infeasible to anticipate and counter all potential attacks that target AI-specific vulnerabilities.

\paragraph{Organizational Measures.}
Numerous EU regulations related to cybersecurity (see e.g., \Artx\ 32 General Data Protection Regulation\footnote{EU Regulation 2016/679, OJ L 119, 4.5.2016.}, Art. 21 NIS 2 Directive\footnote{EU Directive 2022/2555, OJ L 333/80.}) explicitly mandate both technical and organizational measures to ensure cybersecurity. 
In the \EUAIAct,
organizational measures are only mandated for the robustness of \HRAIS\ in \Art{15(4)}, but not for cybersecurity (\Art{15(5)}).
% 
Rather, \Art{15(5)} \EUAIAct\ only focuses on technical solutions for providers of \HRAIS.
% 
The omission of organizational measures in \Art{15(5)} has been criticized in the literature accompanying the legislative process of the \EUAIAct~\cite{biasin2023new}.
%
It is unclear whether providers are still implicitly required to implement organizational measures (in accordance with other EU regulations), as these measures might be inherently included in the concept of cybersecurity, or if they are not mandatory.
%
However, this ambiguity for \emph{providers} of \HRAIS\ (\Art{15}) is mitigated by the fact that \emph{deployers} of \HRAIS\ are required to implement both organizational and technical measures to ensure the proper use of the system in accordance with the instructions for use (\Art{26(1)}). These instructions include the cybersecurity measures put in place.

