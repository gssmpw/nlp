\section{An ML Perspective on Robustness and Cybersecurity}
\label{sec:background_robustness}
%
ML research on robustness focuses on mitigating undesired changes in model outputs when deploying models in real world scenarios~\cite{schwinn2022improving}. This issue is explored across different applications such as computer vision~\cite{drenkow2021systematic, taori2020measuring, dong2020benchmarking} and natural language processing~\cite{la2022king, chang2021robustness}.
%
Unintended changes in model outputs can occur due to adversarial or non-adversarial factors affecting the ML model, its input (test) data, or its training data~\cite{cheng2024towards, tocchetti2022ai}.
%

Perturbations of input (test) data often present a difficult challenge (see Figure~\ref{fig:robustness}).
%
While a model's output may be as expected when using ``safe'' 
% 
test data from the original population distribution, unintended changes can occur when perturbed examples are
% 
provided as input to the ML model. 


\emph{Non-adversarial} (or \emph{natural}) \emph{robustness} often addresses changes in ML model outputs due to \emph{distribution shifts} in input data~\citep{tocchetti2022ai, gojic2023non}.
%
These changes occur when the distribution from which the test data is sampled differs from that of the training data~\citep{drenkow2021systematic, taori2020measuring}.
%
For instance, alterations in data collection methods, such as upgrading to a new X-ray machine, can modify the format or presentation of images~\cite{glocker2019machine, castro2020causality}. Importantly, distribution shifts can also result from \emph{feedback loops}, where the ML model's outputs influence the data distribution, creating a cycle from the model's output back to its input~\cite{d2020fairness, zhang2020fair}.
%
Such an effect can be found, for example, in movie recommendation systems, where user’s preferences change over time in response to the ML system’s suggestions, thereby influencing future recommendations~\cite{perdomo2020performative}.
%
Other forms of research on non-adversarial robustness investigate the robustness of ML models to noise, which frequently occurs in real-world data sets~\cite{saez2016evaluating, olmin2022robustness}.




\emph{Adversarial robustness} refers to the study and mitigation of model evasion attacks using adversarial examples. 
%
These are data samples typically drawn from the original population distribution and then modified by an adversary---often in ways that are difficult or even impossible to detect through human oversight---with the intent of altering a model's output~\citep{szegedy2013intriguing}. 
%
This phenomenon can occur across various models and data types.
%
For instance, in the image domain, small pixel perturbations in input images can lead to significant changes in a model's output~\citep{szegedy2013intriguing}.
% 
In a broader sense, adversarial robustness also encompasses the study and mitigation of other forms of adversarial attacks that attempt to extract the model or reconstruct or perturb the training data set~\cite{nicolae2018adversarial, chen2017targeted}.


From a technical standpoint, adversarial robustness is one aspect of \emph{cybersecurity}. 
% 
Research on cybersecurity focuses on developing defenses that protect computer systems from attacks compromising their confidentiality, integrity, or availability~\cite{dasgupta2022machine}. 
% 
This encompasses aspects like data storage, information access and modification, and secure data transmission over networks~\cite{sarker2021ai}.
%
Unlike robustness, cybersecurity is not a stand-alone concept in ML, but is discussed more broadly as both a tool for ensuring cybersecurity and a potential source of cybersecurity risks.\footnote{E.g., NeurIPS'18 Workshop on Security in ML~\cite{secml2018}, ICML'22 Workshop on ML for Cybersecurity~\cite{ml4cyber}.}
%
ML algorithms can be employed to detect and mitigate cybersecurity threats~\cite{sarker2021ai}, but can also introduce specific vulnerabilities that adversaries may exploit, such as data poisoning or adversarial attacks~\cite{roshanaei2024navigating, rosenberg2021adversarial}.


\begin{figure*}
    \centering
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/overview.png}
        \caption{Robustness problems. Outputs may be as expected (\cmark) with ``safe'' test data from the original distribution; unintended changes (\xmark) can occur with adversarial or non-adversarial (shifted) inputs.}
        \label{fig:robustness}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{figures/mapping.png}
        \caption{Technical solutions to cybersecurity can be found, i.a., in ML research on \emph{adversarial robustness}; technical solutions to robustness can be found, i.a., in the ML research on \emph{non-adversarial robustness}.}
        \Description[]{}
        \label{fig:mapping}
    \end{minipage}
\end{figure*}
