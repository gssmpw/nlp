
\subsection{General Challenges of \Art{15}}\label{sec:general_challenges}

We identify four legal challenges related to \Art{15} that may arise in its practical implementation.
 % 
 First, there is no clear delineation of the legal terms of robustness and cybersecurity and its counterparts in ML literature.
%
 Second, while the \EUAIAct\ mandates compliance for entire AI systems, the ML literature primarily focuses on models, which may pose practical challenges for implementation. 
%
Third, while accuracy is specified as a requirement in \Art{15}, the provision does not clarify its role in measuring robustness and cybersecurity.
% 
Fourth, the terms 'lifecycle' and 'consistent' performance are not defined, leaving ambiguity about how such performance can be practically ensured.
 


\paragraph{Robustness and Cybersecurity.}

The robustness requirement in \Art{15(4)} addresses ``errors, faults, or inconsistencies'' that may inadvertently occur as the system interacts with its real-world environment. 
%
In contrast, the cybersecurity requirement in \Art{15(5)} targets deliberate attempts ``to alter the use, outputs, or performance'' of an AI system ``by malicious third parties exploiting the system’s vulnerabilities''. 
%
Both robustness and cybersecurity requirements aim to ensure that \HRAIS\ perform consistently and are resilient against any factors that might compromise this performance. They, however, address different threats to consistent performance: robustness requires protection against unintentional causes, whereas cybersecurity protects against intentional actions.
%
While robustness is a new term in EU legislation and not explicitly defined in the \EUAIAct, the term cybersecurity has already been defined in the EU Cybersecurity Act (CSA)\footnote{Regulation (EU) 2019/881, OJ L 151, 7.6.2019.}.
%
Art. 2(1) CSA provides a broad definition of cybersecurity, covering all ``the activities necessary to protect network and information systems, the users of such systems, and other persons affected by cyberthreats''. According to the CSA, a cyber threat is any potential circumstance, event or action that could damage, disrupt or otherwise adversely impact network and information systems, the users of such systems and other persons (Art. 2(8) CSA). Importantly, the CSA does not distinguish between intentional or unintentional cyberthreats as causes of harm. Rather, both are explicitly included in the scope of the CSA (see, e.g., Art. 51(1)(a) and (b) CSA).
%
However, the \EUAIAct\ artificially splits the CSA's concept of cybersecurity by designating unintentional causes as a matter of robustness and restricting cybersecurity to intentional actions. This creates a conflict when aligning the AIA’s requirements with the CSA’s definition of cybersecurity that may lead to regulatory ambiguity. Specifically, \Art{42(2)} considers \HRAIS\ with CSA certification or conformity declarations as compliant with cybersecurity requirements in \Art{15}.\footnote{Note that this holds only true in so far as the cybersecurity certificate or statement of conformity or parts thereof cover those requirements in \Art{15}.} This suggests that the CSA definition of cybersecurity applies to the \EUAIAct, even though it inherently covers both types of causes.
% 

We explore how these legal terms could be understood within the ML domain proposing a simple model as an explanatory heuristic (see Figure~\ref{fig:mapping}).
% 
In ML, robustness refers to maintaining consistent model performance in real-world scenarios~\cite{schwinn2022improving}. 
%
ML research distinguishes between different types of robustness. \emph{Non-adversarial robustness} in ML refers to a model’s ability to maintain performance despite data shifts or noise~\cite{tocchetti2022ai, gojic2023non, saez2016evaluating, olmin2022robustness}. This aligns with the legal term robustness in the \EUAIAct. 
%
\emph{Adversarial robustness} in ML refers to the model’s resistance to intentional perturbations aimed at altering predictions~\cite{szegedy2013intriguing}. This aspect aligns more with the legal concept of cybersecurity.
%
The cybersecurity requirement in \Art{15(5)} aims to ensure AI systems' integrity, confidentiality, and availability, protecting them from threats like unauthorized access, adversarial manipulation, data modification, Denial-of-Service attacks, and theft of sensitive information (e.g., model weights).
% 
However, other scenarios within the ML domain may also fall under the relevant legal terms. For example, language model jailbreaks exploit AI vulnerabilities to bypass safety constraints~\cite{wei2024jailbroken, vassilev2024adversarial}. This aligns more closely with the notion of cybersecurity in protecting against misuse of AI systems.


Our findings are supported by an historic analysis of the legislation process.
%  
As outlined in Section~\ref{sec:purpose_15aia}, the \EUAIAct\ builds on the Ethics Guidelines for Trustworthy AI~\cite{aiiheg2019guidelines}.
%
In the guidelines, the principle of 'technical robustness and safety' includes resilience against attacks, but does not mention cybersecurity.   
%
The White Paper on Artificial Intelligence~\cite{whitepaper}, which elaborates on these guidelines, still lists resilience to attacks against AI systems under ``robustness and accuracy'' without differentiating those terms from cybersecurity.
%
The first official draft of the \EUAIAct\ by the European Commission\footnote{COM/2021/206 final.} was the first official document to distinguish between these three terms and assigned ``resilience against attacks'' to cybersecurity rather than robustness.
%
\EW{27}, which refers to the IHGE guidelines, seems to be a remnant of this development process.
%
It demands under the term `technical robustness' that AI systems should be resilient ``against attempts to alter the use or performance of the AI system'', essentially asking for adversarial robustness.


\paragraph{System vs. Model.}
%
The \EUAIAct\ regulates AI systems, but not AI models, with the only exception being \GPAIMS. 
%
ML research, in contrast, often focuses on developing technical solutions for \emph{ML models}.
%  
This raises the question of whether solely relying on technical solutions for \emph{ML models} is enough to ensure the compliance of a \HRAIS\ with \Art{15}---or whether additional measures are needed.
%
\EW{97} 
specifies that an AI model is an essential component of an AI system.\footnote{Although \Rec{97} specifically refers to \GPAIMS, the wording suggests that the statement about the relationship between AI systems and AI models is of a general nature.}
%  
Additional components can include, i.a., user interfaces, sensors, databases, network communication components, or pre- and post-processing mechanisms for model in- and outputs (\EW{97}, \cite{JRC134461}).
% 
All these individual components should contribute to the overall robustness of the AI system, particularly in scenarios where some components may fail.
%
This is illustrated by \Art{15(4)(ii)}, which states that robustness may be ensured through technical redundancy solutions, including ``back-up or contingency plans''.
% 
Furthermore, \Art{15(5)(iii)} stipulates that the cybersecurity of AI systems shall be achieved through technical solutions that, ``where appropriate'', target training data, pre-trained components, the AI model or its inputs. 
%
This binding provision suggests that at least these different components of the AI system are required to be assessed individually for their appropriateness in mitigating cybersecurity attacks.
%
Thus, \Art{15} should not be understood as requiring a single, unified assessment of the requirements. Instead, it must be interpreted as mandating that each component, including one or more ML models, be assessed individually. The assessment of the AI system’s overall performance is then derived from an aggregation of the individual performance results~\cite{kumar2023}. 
%
This requires an interdisciplinary approach that draws on expertise from fields such as ML, engineering, and human-computer interaction.
%
To establish a common understanding, it can prove beneficial to formally describe the evaluation process of an entire AI system, including potential challenges, such as interdependencies of technical solutions. 
%

\paragraph{Role of Accuracy.}
\Art{15(1)} mandates that \HRAIS\ shall ``achieve an
appropriate level of accuracy''.
%
This is important because trade-offs between different desiderata can exist, such as between robustness and accuracy (see Appendix~\ref{apx:robustness-accuracy-trade-off}).
% 
While accuracy is not defined in the \EUAIAct, Annex~IV~No.~3~\EUAIAct\ states that accuracy is an indicator of the capabilities and performance limits of an AI system.
% 
Accordingly,  accuracy should be measured in at least two ways:
i) separately for ``specific persons or groups of persons on which the system is intended to be used''\footnote{This links to fairness ML literature on diverging error rates for different sensitive groups~\cite{mitchell2021algorithmic, chouldechova2017fairer}.}, and
%
ii) the overall expected accuracy for the ``intended purpose'' of the AI system. 
%
In ML, the metric \emph{accuracy} typically describes the overall proportion of correct predictions out of the total number of predictions made~\cite{carvalho2019machine}. 
%
However, the term can also describe the objective of ``good performance'' of an AI system and, depending on its specific purpose, can also be evaluated using different metrics, such as utility~\cite{corbett2017algorithmic} and f1-score~\cite{sokolova2006beyond}. 
% 
\Art{15(3)} explicitly references  `accuracy and the relevant accuracy metrics', indicating that accuracy is understood as an objective that can be measured with various metrics, leaving the choice of the relevant metric to the provider.
% 
The selection of the metric should consider various factors, including the specific purposes of the ML model, dataset-specific circumstances (e.g., imbalanced data) and the particular model type (e.g., classification, regression). Technical standards and guidelines by the EU Commission should clarify how AI systems' accuracy should be measured.



In ML, robustness is often measured using an \emph{accuracy} metric.
%
Typically, this involves comparing the \emph{accuracy} (or error rates) evaluated on an unperturbed dataset from the original distribution with the accuracy on a perturbed test set (e.g., sampled from the shifted distribution or containing adversarial samples)~\cite{taori2020measuring, hendrycks2021many, goodfellow2014explaining}. 
%
The smaller the difference between these two \emph{accuracy} results, the better the \emph{robustness}.
%
The choice of the \emph{accuracy} metric thus has an impact on the measurement of robustness. 
%
As a result, the ML model may appear more robust under some accuracy metrics than others.
% 
The selection of favorable metrics has been studied in fair ML under the term fairness hacking~\cite{meding2024fairness, simson2024one, black2024d}.
%
Without entering into the debate, we note that there is an ongoing discussion in the ML literature about the existence and characteristics of a trade-off between \emph{robustness} and \emph{accuracy}. 
%
While some research showed that enhancing \emph{robustness} leads to a drop in \emph{test accuracy}~\cite{zhang2019theoretically, rade2022reducing, tsipras2018robustness}, others believe that \emph{robustness} and \emph{accuracy} are not conflicting goals and can be achieved simultaneously~\cite{yang2020closer, raghunathan2020understanding}.
%
Technical standards and guidelines by the EU Commission should provide instructions on how AI system providers should choose an appropriate `accuracy' measure, especially when it is used to assess robustness in subsequent steps.


\paragraph{Consistent Performance Throughout the Lifecycle.}
AI systems must perform ``consistently'' in terms of accuracy, robustness, and cybersecurity ``throughout their lifecycle'' (\Art{15(1)}). 
%
Performance is the ``ability of an AI system to achieve its intended purpose'' (\Art{3(18)}).
%
However, i) the term `lifecycle' is not defined, creating ambiguity about whether it differs from the term `lifetime' used in \Art{12(1)} and \EW{71}; ii) the concept of `consistent' performance is unclear, and it is not specified how it should be measured.
% 

First, `lifecycle' and `lifetime' could be understood as synonyms \cite{marcus2020promoting}. On the other hand, the term `lifetime' could be understood to refer specifically to the active period of the AI system in operation~\cite{murakami2010lifespan}, while `lifecycle' could encompass a broader view of all phases from product design and development to decommissioning~\cite{hamon2024three}. In this case, however, it is unclear how accuracy, robustness and cybersecurity should be ensured beyond the operational phase (e.g., during development). 
%  
\Art{2(8)} clarifies that these requirements do not have to be met during the test and development phase of the \HRAIS--unless the system is tested under real world conditions.
However, the use of the term 'lifecycle' might be interpreted to suggest that the requirements of \Art{15} should not only be assessed when the system is ready for deployment but also be considered during design process itself.


Second, it is unclear what `consistent' performance means and how it should be measured.
In the ML literature, a model’s variability in performance over time is often measured using the variance of a metric such as accuracy or robustness~\cite{kilbertus2020fair, bechavod2019equal, rateike2022don}.
%
The variance of a metric over a time interval indicates its deviation from its mean within this interval. 
%
For instance, high variance in robustness indicates significant fluctuations in robustness levels between two points in time, whereas low variance indicates similar levels of robustness over time. 
%
A low variance could therefore be understood as a consistent performance.\footnote{Some also consider consistency as a metric itself, rather than as a property of a (robustness) metric~\cite{wei2020optimal}.} 
 % 
In practice, performance can vary due to factors, such as random initializations of weights or input data sampling.
%
These types of variations are unavoidable.
%
Defining level of variance considered `consistent' is challenging as it is dependent on the context.
%
Technical standards and guidelines by the EU Commission should clarify how to measure a consistent performance with respect to accuracy, robustness, and cybersecurity, and provide guidance on determining the required level of consistency.
