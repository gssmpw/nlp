\begin{figure*}[!t]
    \centering
    \includegraphics[width=\textwidth]{assets/imgs/sol_distr_h.pdf}
    \caption{Distribution of the number of solutions per problem instance. Problem instances are grouped by the number of agents $n$ and by minimum number of solutions $n_{sol}$ we require the problem instance to have. For each combination of $n$ and $n_{sol}$, there are $100$ problem instances in our dataset. As evident from panels in the bottom left of the figure, when we add solutions to a graph $\mathcal{G}_C$ with few nodes, there is a high probability that the links of two solutions can be combined to form new solutions that we have not been explicitly inserted in the graph. This happens less frequently as the number of nodes in $\mathcal{G}_C$ increases.}
    \label{fig:sol_distr}
\end{figure*}

\section{Experiments}\label{sec:exps}

We conducted the evaluation of our algorithm on a dataset comprising synthetically generated problem instances defined by the following parameters: number of agents ($n$), interaction rate ($p_{\operatorname{int}}$), maximum number of paths per agent ($n_d$), minimum number of solutions ($n_{sol}$), and a random seed ($\sigma$). The interaction graph $\mathcal{G}_I$ is created as a connected graph with $n$ nodes and edges added with probability $p_{\operatorname{int}}$. Each agent in $\mathcal{G}_I$ generates between 1 and $n_d$ paths, forming the nodes of the constraint graph $\mathcal{G}_C$. Paths have path utilities of 1 (preferred) or 0.1 (less preferred), following a standard practice for experimentation in multi-alternative decisions in which the less-preferred options are considered distractors with an equally low utility \cite{Reina:2017jl}. Edges in $\mathcal{G}_C$ are initialized to construct $n_{sol}$ solutions and refined to ensure that each node has at least degree $1$. While $n_{sol}$ sets a minimum, the total number of solutions is computed post-generation, as described in  Section S3 of the Supplementary Material. The distribution of the total number of solutions of the problem instances in our dataset is reported in Figure \ref{fig:sol_distr}. Problem instances were generated with fixed $p_{\operatorname{int}}=0.3$ and $n_d=8$, varying $n \in \{10, 20, 50, 100\}$ and $n_{sol} \in \{3, 5, 10\}$, yielding 1200 instances. Further details about the data generation process can be found in section S2 of the Supplementary Material. The dataset will be publicly available with the aim of stimulating research on dec-rtRTMP.

%We conducted the evaluation of our algorithm on a synthetic dataset consisting of 1200 distinct problem instances. 
%In Supplementary Material S2 we detail how we generated the dataset and in Supplementary Material \ref{sec:data_gen} how we computed the set of all solutions in a separate step after the generation.
%Note that each problem instance is required to have at least $n_{sol}$ solutions by design. However, by design, we do not control the total number of solutions in a problem instance. 

We benchmark our algorithm against a classical DSA (with $\alpha = 0.9$ and $\epsilon = 0$, but see the Supplementary Matrial for additional values of $\alpha$) in terms of quality of the solution and speed of convergence. Below, we present the results of the execution of our algorithm. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Results}\label{subsec:res}

%On each problem instance we tested three types of agents:
%%Each problem instance was subjected to 300 executions of our algorithm, encompassing three different types of agents. For each combination of agent type and problem instance, our algorithm was executed 100 times with unique seeds to ensure robustness. The tested agent types include:
%
%\begin{enumerate}
%\item $\texttt{k\_1}$: Each agent in the system considers only one neighbor during the decision-making process.
%\item $\texttt{k\_all}$: Each agent in the system considers all its neighbors during the decision-making process.
%\item $\texttt{k\_ada}$: Each agent in the system initially considers all its neighbors during the decision-making %process and, after $1000$ iterations, it starts reducing the number of neighbors considered over iterations, until it considers only a single neighbor ($K=1$).
%\end{enumerate}
We exploited the synthetic dataset to test the quality of the proposed decentralised multi-agent coordination algorithm varying the value of the parameter $k$. In more detail, we deployed agents that consider only one neighbour during the decision-making process (hence, $k=1$, hereafter referred to as $\texttt{k\_1}$), as well as agents that consider all their neighbours during the decision-making process (hence, $k=\infty$, hereafter referred to as $\texttt{k\_all}$). 
%
On each problem instance, we executed our decentralised algorithm $100$ times per type of agent, every time using a different random seed to ensure robustness and reproducibility. On each execution, we set an upper bound on the number of iterations. The upper bound has been fixed to $10^5$ and every time our algorithm exceeds this bound the execution is interrupted resulting in a failure.
%
The same experimental setting holds for the classical DSA (hereafter referred to as $\texttt{dsa}$).

%We evaluated our approach in terms of quality of the solution found at the end of the self-organisation process and in terms of speed of convergence to such solution. For what concerns the solution quality, for each problem instance we created a ranking of all its solutions, from the optimal ones (the complete assignments maximising the objective $\eta$) to the ones with smallest value of $\eta$, and we measured at which level of such ranking the solution found by our algorithm is located (on average, over 100 runs). For the speed of convergence, we measure the time taken by the $n$ agents to converge to a solution. Note that the default policy implies that any solution represents an absorbing state for the system, that is, no agent changes state any more once a solution is found.

We evaluated our approach based on the quality of the solution obtained at the end of the self-organization process and the speed of convergence to that solution. Regarding solution quality, we ranked all solutions for each problem instance in decreasing order of their objective value $\eta$, with the optimal solutions (those maximizing $\eta$) at the top. We then assessed the position of the solution found by our algorithm within this ranking, averaging the results over 100 runs. For speed of convergence, we measured the number of steps required for the $n$ agents to converge to a solution. It is important to note that under the policy described in Section \ref{sec:consensus}, any solution acts as an absorbing state for the system, meaning no agent changes state once a solution is found.

Figure~\ref{fig:ranking} shows the average position in the ranking of the solution given by the proposed approach compared to DSA. Additionally, it shows the fraction of runs that end up in a failure (see the last bar in each plot labelled `Fail'). We first notice that $\texttt{k\_1}$ only fails on large problem instances (large $n$ and $n_{sol}$), while $\texttt{k\_all}$ presents a small number of failures in many conditions, even for small problem instances ($n=10$, $n_{sol}=3$).
%We first notice that, in all conditions, $\texttt{k\_1}$ never fails, while $\texttt{k\_all}$ presents in nearly any condition a small number of failures, even for small problem instances ($N=10$, $\bar{S}=3$).
By analysing the behaviour of the system in such instances, we discovered that the large value of $k$ leads to deadlocks, whereby the system oscillates between configurations that are not solutions to the problem.
%This happens when the decision maker has an hypothesis that is compatible with most of its neighbours, but not all of them (hence, such hypothesis is not part of any solution to the problem). In this scenario, the agent will select this hypothesis, influencing the decisions of all its neighbours at subsequent iterations. As a result, the system starts oscillating between configurations that are not solutions to the problem and it is not able to converge. 
We observed deadlocks with any value of $k>1$. Instead, with $k=1$, deadlocks are not possible, because any agent interacts with only a single neighbour and finding one incompatibility is enough to escape eventual deadlock configurations, provided that a sufficient number of iterations is performed.

The discovery of deadlocks lead us to introduce an adaptive strategy (referred to as $\texttt{k\_ada}$), in which each agent in the system initially considers all its neighbours during the decision-making process and, after $1000$ iterations, it starts reducing the number of neighbours considered, linearly over a window of $10^4$ iterations, until it considers only a single neighbour ($k=1$).
%it starts reducing the number of neighbors considered over iterations, until it considers only a single neighbor ($K=1$). The hyperparameter $K$ decreases linearly over a window of $10^4$ iterations. 
This will remove deadlocks as soon as agents reach the value $k=1$. At the same time, it can preserve the features of high values of $k$ such as an expected higher convergence rate, as conjectured in Section~\ref{sec:consensus}. In Figure~\ref{fig:ranking}, we show that $\texttt{k\_ada}$ always reaches a feasible solution but for very large problem instances ($n=100$, $n_{sol}=10$), where failures are just a few and they are not due to deadlocks. Hence, the adaptive approach successfully removes the deadlock occurrence issue.
% As already specified in Section~\ref{sec:consensus}, having $K>1$ improves the speed of convergence our algorithm at the price of possibly getting trapped into deadlocks while having $K=1$ ensure convergence but it takes longer to achieve it. We introduce the adaptive agent to have the best of both worlds. As confirmed by results in Section~\ref{subsec:res}, a system made of adaptive agent rapidly converges to a solution and in case of deadlocks it is able to escape them by falling back to a system made of $K=1$ agents.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\textwidth]{assets/imgs/ranking_h.pdf}
    \caption{Ranking of the solutions given by our algorithm. Data are grouped by the type of agent, by number of agents $n$ and by minimum number of solutions $n_{sol}$ we require the problem instance to have. Bars with label ``$1$'' represent the fraction of executions that converged to an optimal solution. Bars with label ``$\geq 10$'' represent the fraction of executions that converged to a solution in position greater than 10 in the ranking. Bars with label ``Fail'' represent the fraction of executions that exceeded the upper bound of $10^5$ iterations.}
    \label{fig:ranking}
\end{figure*}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\textwidth]{assets/imgs/regret_top3.pdf}
    \caption{Top: Regret (percentage loss from the optimal solution value) of the solutions given by our algorithm in position 2 or 3 of the ranking. Data are grouped by type of agent, by number of agents $n$ and by minimum number of solutions $n_{sol}$ we require the problem instance to have.
    Bottom: Average convergence rate to a top-3 solution.}
    \label{fig:regret}
\end{figure*}

Looking at the ranking of the produced solutions in Figure~\ref{fig:ranking}, we found that, when the agents consider all their neighbours during the decision making phase, the rate of convergence to an optimal solution is not significantly impacted by the number of agents in the system. As an example, consider the first row in Figure~\ref{fig:ranking}: the $\texttt{k\_all}$ and  $\texttt{k\_ada}$ strategies have a convergence rate to an optimal solution around $80\%$, regardless of the number of agents in the system. The same does not hold true for $\texttt{k\_1}$, as the convergence rate to the optimal solution drops as soon as the number of agents increases. 

The second factor impacting on the rate of convergence to an optimal solution is the minimum number of solutions of the problem instance. As shown in Figure~\ref{fig:ranking}, the larger $n_{sol}$, the harder it is for our algorithm to converge to an optimal solution, regardless of the type of strategy adopted and of the number of agents in the system. Additionally, we note that instances that have many possible solutions are more difficult to solve (e.g., the rank of the solutions found is generally lower when $n=20$ and $n_{sol}=10$, where there are instances with a large amount of solutions, see Figure~\ref{fig:sol_distr}). In such conditions, most solutions have the same score $\eta$ due to the way in which solutions are constructed. Hence, agents do not particularly favour one solution over the other, often ending up in lower-ranked solutions. 

It is important to note that, when our algorithm does not converge to an optimal solution, it usually converges to a solution in the top-3 of the ranking, as reported by the bottom panel of Figure~\ref{fig:regret}. For example, the adaptive strategy, even on the most difficult problems, converges to a top-3 solution at least $50\%$ of the times. However, this is not enough to properly measure the quality of our algorithm, since it is not obvious if the value of a solution in second or third position in the ranking is close to the optimal value. For this reason, for each solution obtained with our algorithm, we measure the percentage deviation of its score $\eta$ from the score of the optimal solutions (first position in ranking), namely the regret. The upper panel of Figure~\ref{fig:regret} shows the regret of solutions in second and third positions of the ranking. We can observe that $\texttt{k\_all}$ and $\texttt{k\_ada}$ have similar performance, with a median regret up to $10\%$ on most executions and around $20\%$ in the worst case, and they clearly outperform the  $\texttt{k\_1}$ strategy. This means that, even when it is not optimal, the quality of the solution of our algorithm is largely acceptable.

With respect to the speed of convergence, Figure~\ref{fig:convTime} demonstrates that the $\texttt{k\_ada}$ strategy is the best performing one. The $\texttt{k\_all}$ strategy shows similar performance but it fails on many problem instances, even those with a small number of agents and few solutions due to deadlocks. 
% A closer examination of this cases revealed that agent with $K>1$ can be trapped into deadlocks. More specifically, we observed that in this cases the system starts oscillating between two configurations, none of them being a solution, and it is not able to escape. 
The $\texttt{k\_ada}$ strategy does not suffer from deadlocks, since, after a certain number of iterations, $k$ falls back to 1 and the agents are able to escape the deadlock. This is evident from the bumps in the tails in the histograms of $\texttt{k\_ada}$ in Figure~\ref{fig:convTime}, which correspond to those problem instances on which $\texttt{k\_all}$ failed to converge and that are instead solved by $\texttt{k\_ada}$ by lowering the value of $k$. On the other hand, when $k=1$, our algorithm never experiences deadlocks but is much slower to converge to a solution, to the point that in problem instances with a lot of agents and a lot of solutions, it never converges before the threshold of $10^5$ iterations.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\textwidth]{assets/imgs/convTime_h.pdf}
    \caption{Distribution of convergence times (in terms of number of iterations) per type of agent on problem instances grouped by number of agents $n$ and by minimum number of solutions $n_{sol}$ we require the problem instance to have. For each of the 100 problem instances characterised by $n$ and $n_{sol}$, we performed $100$ executions of our algorithm for each agent type. Each execution has an upper bound of $10^5$ iterations, beyond which it fails.}
    \label{fig:convTime}
\end{figure*}

For what concerns $\texttt{dsa}$, it shows slightly worse performance with respect to $\texttt{k\_all}$ in terms of solution quality (see Figure \ref{fig:ranking}). In fact the two strategies are similar since each agent considers all its neighbours during the decision making phase. However, the policy implemented by the $\texttt{dsa}$ agents results in larger convergence times and also in several convergence failures (see Figure \ref{fig:convTime} and the Supplementary Material for other values of $\alpha$).
%However, since the $\texttt{dsa}$ agents implement a dummy policy with respect to the policy described in \ref{sec:consensus}, the converge time is much longer resulting in a lot of failures (see Figure \ref{fig:convTime}).

In conclusion, $\texttt{k\_ada}$ seems to be the best algorithm since it features a speed of convergence similar to $\texttt{k\_all}$ but, as $\texttt{k\_1}$, it is not subject to deadlocks. Also, it achieves a good rate of convergence to a top-3 solution of any given problem instance.



\begin{comment}
main findings:
\begin{itemize}
    \item the rate of convergence to the optimal solution if not significantly impacted by the size of the problem instance (number of agents) when k is close to the number of neighbors of each agent. When k=1 we can observe a significant decrease in the performance.
    \item The number of solutions in a problem instance significantly impacts the rate of convergence to the optimal solution of all the agent types. The more the solutions, the harder is to converge to the optimal one
    \item Even though the algorithm do not converge to an optimal solution, it converges to something that is close to it (see the regret). This holds for all the agents
    \item larger value of k significantly speed up the convergence of the algorithm (less iteration are needed to find a solution). Drawback: if k is large and the problem instance is small we can have deadlocks. The adaptive agent solves this problem.
\end{itemize}
\end{comment}


