\section{A decentralised multi-agent coordination algorithm}\label{sec:consensus}

This section describes the self-organization process at the heart of our decentralised coordination approach. 
Inspired by the classical Decentralised Stochastic Algorithm (DSA) \cite{fitzpatrick2003distributed} from the DCOP literature, coordination is achieved through an iterative procedure, through which agents try to reach a (possibly optimal) solution to a given problem instance $(\mathcal{G}_I, \mathcal{G}_C)$ by only exploiting local information about their respective neighbours. 

At each iteration, the agent $a_i$ decides which value $d \in D_i$ to assign to the variable $v_i$ it controls. The algorithm starts with the agents performing a greedy assignment, i.e. $v_i=d^*$, where $d^* = \operatorname{argmax}_{d \in D_i} u_r(v_i,d)$. At each subsequent iteration, $a_i$ can decide either to keep its current assignment for $v_i$ or switch to another value $d^{\prime} \in D_i$. This choice depends solely on the local information available to the agent, that is, the path utility of the paths in $D_i$ and the degree of compatibility of $(v_i, d)$ with the current assignments $S(N_i)$ of its neighbours, $\forall d \in D_i$. 
Note that the path utility of neighbours' paths is not known, as this information is private to each agent.

More specifically, at each iteration $t$, the agent $a_i$, whose current assignment is $(v_i, d)$, operates the following steps:
\begin{enumerate}
    \item It randomly selects at most $k\geq1$ neighbours from $N_i$. We denote by $K_i(t)$ the subset of neighbours selected by the agent $a_i$ at time $t$. If $k\geq|N_i|$, then $K_i(t)=N_i$.
    \item It observes the current assignments of the selected neighbours, i.e. the set $S(K_i(t))$.  
    \item It creates a ranking over the set $D_i$. For each $d \in D_i$, the agent computes its rank $r(d,t)$ as the number of binary constraints satisfied by a potential assignment of $d$ to $v_i$ with respect to the current assignments $S(K_i(t))$ of the selected neighbours, i.e.     
    $$
    r(d,t) = \sum_{(v_j, d_j) \in S(K_i(t))} u_c \left((v_i, d),  (v_j, d_j)\right)
    $$
    %
    % \begin{align*} &r(d) =  \\ & \left| \left\{ (v_j, d_j) :  a_j \in K_i, v_j=d_j, u \left( \{ (v_i, d) , (v_j, d_j) \} \right) = 1 \right\} \right|. \\ \end{align*}
    %
    Hence, $r(d,t)$ represents the degree of compatibility of the value $d$ with the current assignments of the selected neighbours.
    \item It decides to keep its current assignment $(v_i, d)$ or to switch to a more compatible value $d^{\prime} \in D_i$ according to the following policy: 
    \begin{enumerate}
        \item if $r(d,t) = k$  (i.e. $d$ is compatible with the assignments of all the selected neighbours in $K_i(t)$), then the agent keeps $v_i=d$ as its current assignment.
        \item if $r(d,t) < k$, then the agent selects a more compatible value by sampling a value $d^{\prime}$ from the set of values $D_i$ satisfying the property $d^{\prime} = \operatorname{argmax}_{d \in D_i} r(d,t)$. In case of multiple values $d^{\prime}$ satisfying this property, a probabilistic choice is made with probability proportional to their respective path utility $u_r(v_i, d^{\prime})$.
    \end{enumerate}
\end{enumerate} 
The policy described in step 4 allows the agents to gradually adjust their assignments towards a configuration (solution to the problem) in which all neighbouring agents hold compatible values, while prioritising values with the highest possible utility score.
The parameter $k$ acts as a sort of learning rate for the algorithm.
With high values of the parameter $k$, the agent considers multiple neighbours during the decision making. This can lead the agent to seek compatibility with more neighbours at the same time, hence possibly increasing the speed of convergence towards a shared solution. Conversely, when $k$ is small (possibly, $k=1$) the agent only considers a few neighbours or just one at the time, and therefore the speed of convergence may be slower.
%\textcolor{blue}{
%[Qui mi avevi chiesto di spiegare l'effetto di K ma mi sembra un po' uno spoiler di quello che viene dopo nei risultati.]
%The speed of convergence to a solution is influenced by the hyperparameter $K$. When $K>1$, the agent considers multiple neighbours during the decision making and it can move faster towards a hypothesis that is compatible with the one selected by all its neighbours. The drawback is that agents with large value of $K$ can be trapped into deadlocks. This happens when the decision maker has an hypothesis that is compatible with most its neighbours but not all of them (hence such hypothesis is not part of any solution to the problem). In this scenario, the agent will select this hypothesis, influencing the decisions of all its neighbours at subsequent iterations. As a result, the system starts oscillating between configurations that are not solutions to the problem and it is not able to converge. Deadlocks never happen when $K=1$. In this case, the agent coordinates with one neighbour at a time and one incompatibility is enough to escape eventual deadlocks configurations, provided a sufficient number of iterations.
%}

\paragraph{Convergence Criteria.} 
We implemented an asynchronous multi-agent simulation to emulate real-world operation. Specifically, at each iteration, one single agent is selected randomly and updates its assignment following the default policy discussed above. This ensures that agents take turn with an approximate period of $n$ iterations.
This iterative process continues until one of the following conditions is met: (i) \emph{Convergence}: all agents achieve a state where their hypotheses are compatible with all their neighbours; (ii) \emph{Maximum Iterations}: a predefined maximum number of iterations is reached.

\paragraph{DSA.} Our algorithm differs from the classical DSA on three aspects:
\begin{itemize}
    \item A DSA agent \emph{always} interacts with all its neighbours (it cannot select a subset of them according to $k$).
    \item A DSA agent, with probability $1-\alpha$, can decide to keep it assignment a priori, regardless of the assignments of the neighbours. The parameter $\alpha$ is known as \emph{activation probability}.
    \item A DSA agent implements a different policy. In our implementation of DSA, the agent $a_i$ assigns a score $r(d)$ to each value $d \in D_i$ as follows:
    $$
    r(d) = u_r(v_i, d) + \sum_{(v_j, d_j) \in S(N_i)} u_c \left((v_i, d),  (v_j, d_j)\right)
    $$
    %given by the sum of the utilities of all the constraints (unary and binary) involving the assignment $(v_i, d)$. 
    Then, the agent greedily assigns to $v_i$ the value $d^*$ with the highest score $r$. In some variants of DSA \cite{Zhang_Wang_Xing_Wittenburg_2005}, the assignment can be $\epsilon$-greedy, i.e. with a (usually small) probability $\epsilon$, the agent assigns a random value $d$ to the variable $v_i$ instead of being always greedy. This helps escaping local maxima.
\end{itemize}
Hence, our algorithm improves over DSA by adding a more flexible interaction scheme and proposing a slightly different policy for the decision-making phase. 
