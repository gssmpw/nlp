\section{Related Work}
\label{Related Work}
This section offers a concise overview of the least square one-class support vector machine (LSOCSVM) and RKM, both of which are crucial in the development and formulation of our proposed OCRKM.
\subsection{LSOCSVM}
The main idea of LSOCSVM \cite{choi2009least} is to identify a hyperplane in the high-dimensional feature space that maximally separates the samples from the origin. Consider that the training set $S=\{x_i\}_{i=1}^N$ with \(x_i \in \mathbb{R}^{1 \times m}\), where \(m\) is the number of input samples. $\phi(\cdot)$ represents the feature mapping corresponding to the kernel function. The optimization problem of LSOCSVM is given as follows:
\begin{align}
\label{OCSVM:1}
    &\min\frac{1}{2}\|w\|^2 - \rho +  \frac{\mathcal{C}}{2}\sum_{i=1}^N \xi_i^2 \nonumber \\
    &\text{s.t.} \hspace{0.2cm} w\phi(x_i) = \rho - \xi_i, \hspace{0.2cm} i=1,2,\ldots, N,
\end{align}
where $\mathcal{C}$ is tunable parameter and $\xi_i$ are slack variables. The weight and bias of the separating hyperplane are denoted by $w$ and $\rho$, respectively. Then the Wolfe dual for \eqref{OCSVM:1} can be obtained as follows:
\begin{align}
\left[\begin{array}{c|c } 
	 K + I/\mathcal{C} & e  \\  
	\hline 
	 e^T    & 0  
\end{array}\right]
    \begin{bmatrix}
    \alpha \\ -\rho 
    \end{bmatrix}  =  \begin{bmatrix} 0 \\   1   \end{bmatrix}
\end{align}
where $\alpha$ is the Lagrangian multiplier and \(K(x_i, x_j) = \phi(x_i)^T\phi(x_j)\) denotes the kernel function.
\subsection{RKM}
Here, we provide an overview of the RKM classification model as described by \citet{suykens2017deep}, which is closely related to the well-known LSSVM \cite{suykens1999least} model. RKM employs the kernel trick to map the data into a high-dimensional feature space, where a linear separating hyperplane can be constructed. The objective function of RKM is as follows:
\begin{align}
\label{eq:1111}
      J = & \frac{\gamma}{2} Tr(W^TW) + \sum_{i=1}^N (1-(\phi(x_i)^TW+b)y_i)h_i - \frac{\eta}{2} \sum_{i=1}^Nh_i^2,
\end{align}
where $\gamma$ and $\eta$ are the regularization parameters, $b$ is the bias term, and $h$ is hidden features, respectively. The solution of equation \eqref{eq:1111} can be derived by taking partial derivatives of $J$ with respect to $W$, $b$, and $h_i$, and subsequently equating the resulting expressions to zero. For detailed derivation, refer to \citet{suykens2017deep}.