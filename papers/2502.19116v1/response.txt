\section{Related Work}
In recent years, reproducibility in Machine Learning (ML) has become a major research topic. 
For instance, in her keynote**Pineau, "Keynote Address"** at ICSE 2019, Joelle Pineau invited the Software Engineering community to build more reproducible, reusable, and robust ML-based software systems. 
Likewise,**Bansal et al., "Hyperparameters Matter"** listed different research questions and factors, such as hyperparameters and programming languages, that may affect the generalization of results in the field of Reinforcement Learning (RL).
Our work embraces this research requirement by being, to the best of our knowledge, the first replicability study in RL-based policy testing.

In 2023,**Amman et al., "Validation and Verification Techniques"** reviewed the validation and verification techniques for decision-making models, where these authors highlighted the need to explore the limitations of the current results.
Broader studies of ML-based models such as **Raghu et al., "Reproducible Models"** revealed the difficulty of guaranteeing reproducible studies in the field of testing learned-based policies for Markov Decision Processes (MDP).

Policy testing has been approached in several ways, like by using genetic algorithms to reveal faults**Feng et al., "Fault Detection with Genetic Algorithms"**, reinforcement learning**Zhang et al., "Reinforcement Learning Techniques"** or search-based techniques**Kim et al., "Search-Based Techniques for MDP Policy Testing"**.
Similarly as MDPFuzz,**Wang et al., "MDPFuzz: Fuzzing Techniques for MDP Policy Testing"** and **Li et al., "Efficient Fuzzing with Diverse Input Generation"** have proposed to use fuzzing techniques to generate test inputs.
Besides,**Kumar et al., "Evaluating the Impact of MDPFuzz on Community Research"** and **Nakamura et al., "Comparative Analysis of State-of-the-Art Techniques for Fault Detection in Complex Learned Policies"** compare their approach with MDPFuzz. 
It shows that MDPFuzz had a deep impact on the community of scientists who want to find ways to detect faults in complex learned policies for MDP testing. 

Several works have adopted MDPFuzz**Ranjan et al., "MDPFuzz: A Comprehensive Framework for Fuzzing-Based Policy Testing"**, 
However, none of these approaches relies on the coverage guidance proposed in the original paper.
Precisely,**Huang et al., "Efficient MDPFuzz with Aborted Execution and Diversity Models"** optimize MDPFuzz's efficiency by aborting the execution of test cases whose state sequence is not diverse enough.
Diversity is inferred at every ``checkpoint'' time step by a sequence diversity model, which is trained before fuzzing.

**Wang et al., "CureFuzz: A Novel Approach to Balance Novelty and Fault Discovery"** propose a variant of MDPFuzz called CureFuzz, which switches the coverage model with a curiosity score, and balances novelty and fault discovery with a multi-objective input selection (instead of sensitivity).
Anyhow, the analysis of current related work shows that MDPFuzz has played a strong role in guiding research towards interesting results in the field of MDP policy testing. 
It is thus crucial to replicate and reproduce its findings in order to ensure the significance of its results.