\section{Related Work}
In recent years, reproducibility in Machine Learning (ML) has become a major research topic. 
For instance, in her keynote~\cite{icse19keynote} at ICSE 2019, Joelle Pineau invited the Software Engineering community to build more reproducible, reusable, and robust ML-based software systems. 
Likewise,~\citeauthor{henderson2018deep}~\cite{henderson2018deep} listed different research questions and factors, such as hyperparameters and programming languages, that may affect the generalization of results in the field of Reinforcement Learning (RL).
Our work embraces this research requirement by being, to the best of our knowledge, the first replicability study in RL-based policy testing.

In 2023,~\citeauthor{mazouni2023}~\cite{mazouni2023} reviewed the validation and verification techniques for decision-making models, where these authors highlighted the need to explore the limitations of the current results.
Broader studies of ML-based models such as ~\cite{ZHANG2020106296, 10.1007/s10515-022-00337-x, 10.1613/jair.1.12716} revealed the difficulty of guaranteeing reproducible studies in the field of testing learned-based policies for Markov Decision Processes (MDP).

Policy testing has been approached in several ways, like by using genetic algorithms to reveal faults~\cite{zolfagharian2023searchbased}, reinforcement learning~\cite{9712397, 10172658} or search-based techniques~\cite{https://doi.org/10.48550/arxiv.2205.04887}.
Similarly as MDPFuzz,~\citeauthor{Steinmetz_Fišer_Eniser_Ferber_Gros_Heim_Höller_Schuler_Wüstholz_Christakis_Hoffmann_2022}~\cite{Steinmetz_Fišer_Eniser_Ferber_Gros_Heim_Höller_Schuler_Wüstholz_Christakis_Hoffmann_2022} and~\citeauthor{10.1145/3533767.3534392}~\cite{10.1145/3533767.3534392} have proposed to use fuzzing techniques to generate test inputs.
Besides,~\citeauthor{model_based_fuzzer}~\cite{model_based_fuzzer} and~\citeauthor{ast2024}~\cite{ast2024} compare their approach with MDPFuzz. 
It shows that MDPFuzz had a deep impact on the community of scientists who want to find ways to detect faults in complex learned policies for MDP testing. 

Several works have adopted MDPFuzz~\cite{curefuzz, seqdivfuzz} as their main testing approach.
However, none of these approaches relies on the coverage guidance proposed in the original paper.
Precisely,~\citeauthor{seqdivfuzz}~\cite{seqdivfuzz} optimize MDPFuzz's efficiency by aborting the execution of test cases whose state sequence is not diverse enough.
Diversity is inferred at every ``checkpoint'' time step by a sequence diversity model, which is trained before fuzzing.

~\citeauthor{curefuzz}~\cite{curefuzz} propose a variant of MDPFuzz called CureFuzz, which switches the coverage model with a curiosity score, and balances novelty and fault discovery with a multi-objective input selection (instead of sensitivity).
Anyhow, the analysis of current related work shows that MDPFuzz has played a strong role in guiding research towards interesting results in the field of MDP policy testing. 
It is thus crucial to replicate and reproduce its findings in order to ensure the significance of its results.