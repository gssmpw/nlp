

\documentclass[11pt, letterpaper, shortlabels]{archer}

\input{math_commands.tex}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{pgf}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{xcolor}         %
\usepackage{color}         %
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{resizegather}
\usepackage{hyperref}
\usepackage{color-edits}
\addauthor{yifei}{blue}
\title{\ourmethodnospace~(\ouracronymnospace): \\Digi-Q}




\usepackage{algorithm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

\usepackage[capitalize,noabbrev]{cleveref}


\usepackage[textsize=tiny]{todonotes}
\usepackage{wrapfig}
\captionsetup[figure]{font=small,skip=0pt}
\setlength{\belowcaptionskip}{0pt}

\Crefname{problem}{Problem}{Problems}







\usepackage{multirow}

\usepackage[all]{hypcap}

\usepackage[authoryear, round]{natbib}

\usepackage{hyperref}[citecolor=magenta,linkcolor=magenta]

\hypersetup{
    colorlinks = true,
    citecolor = {magenta},
}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} %
\usepackage{float}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{nicefrac}
\usepackage{dsfont}
\usepackage{enumitem}
\usepackage{float}
\usepackage{enumitem}
\usepackage{comment}
\usepackage{etoolbox}
\usepackage{ifthen}
\usepackage{mathrsfs}
\usepackage{upquote}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{arydshln}
\usepackage{longtable}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{amsmath}
\usepackage{dsfont}
\usepackage{multirow}
\usepackage{mdframed}
\usepackage{xcolor}
\usepackage{blindtext}
\usepackage{setspace}
\usepackage{xcolor,colortbl}
\definecolor{Gray}{gray}{0.90}
\definecolor{LightCyan}{rgb}{0.88,1,1}
\usepackage{multirow}
\usepackage{wrapfig}

\setlength\parindent{0pt}


\usepackage{xspace}
\usepackage[capitalize,noabbrev]{cleveref}
\bibliographystyle{plainnat}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{lipsum}
\usepackage{listings}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{bbm}

\usepackage{algpseudocode}
\usepackage{setspace}
\usepackage{afterpage}

\usepackage{color}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}


\newcommand\pythonstyle{\lstset{
basicstyle=\ttfamily\footnotesize,
language=Python,
morekeywords={self, clip, exp, mse_loss, uniform_sample, concatenate, logsumexp},              %
keywordstyle=\color{deepblue},
emph={MyClass,__init__},          %
emphstyle=\color{deepred},    %
stringstyle=\color{deepgreen},
frame=single,                         %
showstringspaces=false
}}

\lstnewenvironment{python}[1][]
{
\pythonstyle
\lstset{#1}
}
{}

\DeclareRobustCommand{\StartCrate}{%
  \begingroup\normalfont
  \raisebox{-0.3ex}{\smash{\includegraphics[height=2.0\fontcharht\font`\B]{figures/icon.png}}}%
  \endgroup
}

\newcommand\pythonexternal[2][]{{
\pythonstyle
\lstinputlisting[#1]{#2}}}

\newcommand\pythoninline[1]{{\pythonstyle\lstinline!#1!}}

\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\EE}{\mathbb{E}}


\makeatletter
\def\mathcolor#1#{\@mathcolor{#1}}
\def\@mathcolor#1#2#3{%
  \protect\leavevmode
  \begingroup
    \color#1{#2}#3%
  \endgroup
}
\makeatother



\Crefformat{equation}{#2Eq.\;(#1)#3}

\Crefformat{figure}{#2Figure #1#3}
\Crefformat{assumption}{#2Assumption #1#3}
\Crefname{assumption}{Assumption}{Assumptions}

\usepackage{crossreftools}
\pdfstringdefDisableCommands{%
    \let\Cref\crtCref
    \let\cref\crtcref
}
\newcommand{\creftitle}[1]{\crtcref{#1}}


\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\theoremstyle{definition}
\newtheorem{property}[theorem]{Property}
\newtheorem{problem}[theorem]{Problem}

\usepackage[most,skins,theorems]{tcolorbox}
\tcbset{
  aibox/.style={
    width=\linewidth,
    top=10pt,
    bottom=4pt,
    colback=blue!6!white,
    colframe=black,
    colbacktitle=black,
    enhanced,
    center,
    attach boxed title to top left={yshift=-0.1in,xshift=0.15in},
    boxed title style={boxrule=0pt,colframe=white,},
  }
}

\definecolor{rliableolive}{HTML}{BBCC33}
\definecolor{rliableblue}{HTML}{77AADD}
\definecolor{rliablered}{HTML}{EE8866}


\newtcolorbox{AIbox}[2][]{aibox,title=#2,colback=rliableblue!10!white,#1}

\newenvironment{redbox}{%
    \begin{tcolorbox}[colback=red!6!white,colframe=black,boxsep=3pt,top=4pt,bottom=4pt,left=3pt,right=3pt]
}{%
    \end{tcolorbox}
}

\newenvironment{olivebox}{%
    \begin{tcolorbox}[colback=rliableolive!10!white,colframe=black,boxsep=3pt,top=4pt,bottom=4pt,left=3pt,right=3pt]
}{%
    \end{tcolorbox}
}

\usepackage{dsfont}
\usepackage{nicefrac}


\newcommand{\nauman}[1]{ \textcolor{blue}{\small [MN: #1]}}
\newcommand{\oleg}[1]{ \textcolor{olive}{\small [O: #1]}}
\newcommand{\preston}[1]{ \textcolor{violet}{\small [PF: #1]}}

\author[1]{Oleh Rybkin}
\author[1,2]{Michal Nauman}
\author[1]{Preston Fu}
\author[1]{Charlie Snell}
\author[1]{Pieter Abbeel}
\author[1]{Sergey Levine}
\author[3]{Aviral Kumar}

\affil[1]{University of California, Berkeley}
\affil[2]{University of Warsaw}
\affil[3]{Carnegie Mellon University}

\correspondingauthor{oleh.rybkin@gmail.com, aviralku@andrew.cmu.edu}


\title{Value-Based Deep RL Scales Predictably}



\begin{abstract}
\textbf{Abstract:} Scaling data and compute is critical to the success of modern ML. However, scaling demands  \textbf{\emph{predictability}}: we want methods to not only perform well with more compute or data, but also have their performance be predictable from small-scale runs, without running the large-scale experiment. In this paper, we show that value-based off-policy RL methods are predictable despite community lore regarding their pathological behavior. First, we show that data and compute requirements to attain a given performance level lie on a \emph{Pareto frontier}, controlled by the updates-to-data (UTD) ratio. By estimating this frontier, we can predict this data requirement when given more compute, and this compute requirement when given more data.
Second, we determine the optimal allocation of a total resource \emph{budget} across data and compute for a given performance and use it to determine hyperparameters that maximize performance for a given budget. Third, this scaling is enabled by first estimating predictable relationships between \textit{hyperparameters}, which is used to manage effects of overfitting and plasticity loss unique to RL. We validate our approach using three algorithms: SAC, BRO, and PQL on DeepMind Control, OpenAI gym, and IsaacGym, when extrapolating to higher levels of data, compute, budget, or performance. 
\end{abstract}

\begin{document}

\maketitle


\begin{figure}[h]
    \centering
    \vspace{-0.6cm}
    \includegraphics[width=0.98\textwidth]{figures/scaling.pdf}
    \vspace{0.1cm}
    \caption{\footnotesize{
    \emph{\textbf{Scaling properties when increasing compute $\mathcal{C}$, data $\mathcal{D}$, budget $\mathcal{F}$, or performance $J$.}} \textbf{Left:} Compute versus data requirements Pareto frontier controlled by the UTD ratio $\sigma$. We observe that we can trade off data for compute and vice versa, and this relationship is predictable. 
    \textbf{Middle:} Extrapolation from low to high performance. We observe that the optimal resource allocation controlled by $\sigma$ evolves predictably with increasing budget, and can be used to extrapolate from low to high performance. 
    \textbf{Right:} Pareto frontiers for several performance levels $J$.
    }}
    \label{fig:main_results}
\end{figure}

\abscontent


\vspace{-1.2cm}
\section{Introduction}
\vspace{-0.1cm}

Many latest advances in various areas of machine learning have emerged from {training big models on large datasets}. In this scaling guided research landscape, successfully executing even one single training run often requires a large amount of data, computational resources, and wall-clock time, such as weeks or months~\citep{achiam2023gpt,team2023gemini,ramesh2022hierarchical,videoworldsimulators2024}. To maximize the success of these large-scale runs, the trend in the machine learning (ML) community has shifted toward not just performant, but also more \textbf{{predictable} algorithms that scale reliably }with more computation and training data size, such that downstream performance can be predicted from small-scale experiments, without actually running the large-scale experiment.
~\citep{mccandlish2018empirical,kaplan2020scaling,hoffmann2022training,dubey2024llama}.

In this paper, we study if deep reinforcement learning (RL) is also amenable to such scaling and predictability benefits. We focus on \textbf{value-based methods} that train value functions using temporal difference (TD) learning, which are known to be performant at small scales, especially in dense reward environments~\citep{mnih2015human,lillicrap2015continuous,sac}. Compared to policy gradient~\citep{mnih2016asynchronous,schulman2017proximal} and search methods~\citep{silver2016mastering}, value-based RL can learn from arbitrary data and require less sampling or search, which can be inefficient or infeasible for open-world problems where environment interaction is costly.

We study scaling properties by predicting relationships between different \textbf{resources required for training}. {\emph{Data}} requirement $\mathcal{D}$ is the amount of data needed to attain a certain level of performance. Likewise, {\emph{compute}} requirement $\mathcal{C}$ refers to the amount of FLOPs or gradient steps needed to attain a certain level of performance. In RL uniquely, performance can be improved by increasing \textit{either} available data or compute (e.g., training multiple times on the same data), which we capture via a \emph{budget} requirement that combines data and compute $\mathcal{F} = \mathcal{C} + \delta \cdot \mathcal{D}$, where $\delta$ refers to a constant multiplier. An additive budget function is representative of practical scenarios where the cost of data and compute can be expressed in similar units, such as wall-clock time or required finances.

To establish scaling relationships, we first require a way to predict the \textbf{best hyperparameter settings} at each scale. We find that learning rate $\eta$, batch size $B$, and the updates-to-data (UTD) ratio $\sigma$ are the most crucial hyperparameters for value-based RL. While supervised learning benefits from abundant theory to establish optimal hyperparameters~\citep{krizhevsky2014one,mccandlish2018empirical,yang2022tensor}, value-based RL often does not satisfy assumptions typical of supervised learning. For example, value-based RL needs to account for the non-i.i.d. nature of training data. Distribution shift due to periodic changes in the data collection policy~\citep{levine2020offline} contributes to a form of overfitting where minimizing training TD error may not result in a low TD error under the data distribution induced by the new policy. In addition, objective shift due to changing target values~\citep{dabney2020value} contributes to ``plasticity loss'' \citep{doro2022sample,kumar2021implicit}. We show that it is possible to account for the training dynamics unique to value-based RL, and are able to find the best hyperparameters by setting the batch size and learning rate inversely proportional to the UTD ratio. We estimate this dependency using a power law~\citep{kaplan2020scaling}, and observe that this model makes effective predictions.

Using the best predicted hyperparameters, we are now able to establish that {data and compute requirements evolve as a predictable function of the UTD ratio $\sigma$}. Furthermore, $\sigma$ defines the \textbf{tradeoff between data and compute}, which can be visualized as a {Pareto frontier} (\cref{fig:main_results}, left). Using this model, we are able to extrapolate the resource requirements from low-compute to high-compute setting, as well as from low-data to high-data setting as shown in the figure. 

Using the Pareto frontiers, we are now able to \textbf{extrapolate from low to high performance levels}. Instead of extrapolating as a function of return, which can be arbitrary and non-smooth, we extrapolate as a function of the allowed budget $\mathcal{F}$. We can define an optimal tradeoff between data and compute, and we observe that such optimal tradeoff value evolves predictably to higher budgets, which also attains a higher performance level (\cref{fig:main_results}, middle). Thus we are able to predict optimal hyperparameters, as well as data and compute allocation, for high-budget runs using only data from low-budget runs.

\textbf{Our contribution} is showing that the behavior of value-based deep RL methods based on TD-learning is predictable in larger data and compute regimes. Specifically, we:
\begin{enumerate}[noitemsep,topsep=0pt]
    \item establish predictable rules for dependencies between hyperparameters batch size ($B$), learning rate ($\eta$), and UTD ratio ($\sigma$) in value-based RL, and show that these rules enable more effective scaling. 
    \item show that data and compute required to attain a given performance level lie on a Pareto frontier, and are respectively predictable in the higher-compute or higher-data regimes. 
    \item show the optimal allocation of budget between data and compute, and 
    predict how such allocation evolves with higher budgets for best performance.
\end{enumerate}  
Our findings apply to algorithms such as SAC, BRO, and PQL, and domains such as the DeepMind Control Suite (DMC), OpenAI Gym, and IsaacGym. The generality of our conclusions challenges conventional wisdom and community lore that value-based deep RL does not scale predictably.



\vspace{-0.2cm}
\section{RL Preliminaries and Notation}
\label{sec:prelims}
\vspace{-0.1cm}
We study standard off-policy online RL, which maximizes the agent's return by training on a replay buffer and periodically collecting new data \citep{sutton2018reinforcement}.
Value-based deep RL methods train a Q-network, $Q_\theta$, to minimize the temporal difference~(TD) error:
{
\begin{align}
    \!\!\!L(\theta) = \mathbb{E}_{(\bs, \ba, \bs') \sim \mathcal{P}, \ba' \sim \pi(\cdot|\bs')}\left[ \left(r(\bs, \ba) + \gamma \bar{Q}(\bs', \ba') - Q_\theta(\bs, \ba) \right)^2\right], \label{eq:td-err}
\end{align}
}where $\mathcal{P}$ is the replay buffer, $\bar{Q}$ is the target Q-network, $\bs$ denotes a state, and $\ba'$ is an action drawn from a policy $\pi(\cdot | \bs)$ that aims to maximize $Q_\theta(\bs, \ba)$.
We implement this operation by sampling a batch of size $B$ from the buffer and taking a gradient step along the gradient of this loss with a learning rate $\eta$. In theory, off-policy algorithms can be made very sample efficient by {minimizing the TD error fully over any data batch}, which in practice translates to {making more update steps to the Q-network per environment step}, or higher ``updates-to-data'' ratio (UTD)~\citep{chen2020randomized}. However, increasing the UTD ratio na\"{i}vely can lead to worse performance~\citep{nikishin2022primacy, mbpo}. To this end, unlike the standard supervised learning or LLM literature that considers $B$ and $\eta$ as two main hyperparameters affecting training~\citep{kaplan2020scaling,hoffmann2022training}, our setting presents another hyperparameter, the UTD ratio $\sigma$, that we also study in our paper.

\textbf{Notation.} In this paper, we focus on the following key hyperparameters: the UTD ratio $\sigma$, learning rate $\eta$, and the batch size $B$. We will answer questions pertaining to performance of a policy $\pi$ denoted by $J(\pi)$, the total data utilized by an algorithm to reach a given target level of performance $J$ (denoted by $\mathcal{D}_J$), and the total compute budget utilized by the algorithm to reach performance $J$ (denoted by $\mathcal{C}_J$), which is measured in terms of FLOPs or wall-clock time taken by the algorithm.

\vspace{-0.2cm}
\section{Problem Statement and Formulation} 
\label{sec:problem_statement}
\vspace{-0.1cm}

To demonstrate that the behavior of value-based RL can be predicted reliably at scale, we first post multiple \emph{resource optimization} questions that guide our scaling study. Viewing data and compute as two resources, we answer questions of the form: \emph{what is the minimum value of \texttt{[resource]} needed to attain a given target performance? And what should the hyperparameters (e.g., $B, \eta, \sigma$) be in such this training run?} We will answer questions of this form by fitting empirical laws from low data and compute runs to determine relationships between hyperparameters. Doing so, in turn, enables us to determine how to set hyperparameters and allocate resources to maximize performance when provided with a larger data and compute budget. Note that we wish to make these hyperparameter predictions without running the large data and compute budget experiment. While questions of this form have been studied in supervised learning, answering them is different in the context of online RL, because online RL requires the algorithm to collect its own data during training, which ties data and compute in a complex manner and breaks i.i.d. nature of datapoints and induces complexities.

Concretely, we study three resource optimization questions: \textbf{(1)} maximizing sample efficiency (i.e., minimize the amount of data $\mathcal{D}$ to attain a given target performance under a given compute budget), \textbf{(2)} conversely, minimizing compute $\mathcal{C}$ (e.g., FLOPs or gradient steps, whichever is more appropriate for the practitioner) to attain a given performance given an upper bound on data that can be collected,  and \textbf{(3)} maximizing performance given a total bound on data and compute. 

    \begin{olivebox}
    \begin{problem}[Resource optimization problems]
    \label{prob:general_prob}
    Find the best configuration $(B, \eta, \sigma)$ for algorithm $\mathrm{Alg}$ that minimizes either the data $\mathcal{D}$ or compute $\mathcal{C}$ consumed to obtain performance $J_0$:
    {
    \setlength{\abovedisplayskip}{4pt}
    \setlength{\belowdisplayskip}{4pt}
    \begin{enumerate}
        \item \emph{Maximal sample efficiency}:
    \begin{align*}
    (B^*, \eta^*, \sigma^*) := \arg \min_{(B, \eta, \sigma)}~~~ \mathcal{D} ~~\text{s.t.}~~ J\left(\pi_\mathrm{Alg}{(B, \eta, \sigma)} \right) \geq J_0,~~ \mathcal{C} \leq \mathcal{C}_0
    \end{align*}
    \item \emph{Maximal compute efficiency}:
    \begin{align*}
        (B^*, \eta^*, \sigma^*) := \arg \min_{(B, \eta, \sigma)}~~~ \mathcal{C} ~~\text{s.t.}~~ J\left(\pi_\mathrm{Alg}{(B, \eta, \sigma)} \right) \geq J_0,~~ \mathcal{D} \leq \mathcal{D}_0
    \end{align*}
    \end{enumerate}
    }
    \end{problem}
    \end{olivebox}

We solve these problems by fitting empirical models of the minimum data and compute needed to attain a target performance for different values of $J_0$. Doing so allows us to then solve the third setting \textbf{(3)} for maximizing performance given a total budget on data and compute as shown below.

    \begin{olivebox}
    \begin{problem}[Maximize performance at large data and compute budget]
    \label{prob:maximize_compute_data}
    Find the best configuration $(B, \eta, \sigma)$ and resource allocations for data $\mathcal{D}$ and compute $\mathcal{C}$ that enable $\mathrm{Alg}$ to maximize performance at budget $\mathcal{F}_0$
    {
    \setlength{\abovedisplayskip}{3pt}
    \setlength{\belowdisplayskip}{3pt}
    \begin{align*}
    (B^*, \eta^*, \sigma^*) ~:=~ \arg \max_{(B, \eta, \sigma)}~~~ J\left(\pi_\mathrm{Alg}{(B, \eta, \sigma)}\right) ~~\text{s.t.}~ ~~  \mathcal{C} + \delta \cdot \mathcal{D} \leq \mathcal{F}_0.
\end{align*}
    }\end{problem}
    \end{olivebox}

\vspace{-0.2cm}
\section{Scaling Results For Value-Based Deep RL}
\vspace{-0.1cm}

We will now present our main results addressing \cref{prob:general_prob} under the two settings discussed above. We will then use these results to present results for \cref{prob:maximize_compute_data}. In order to do so, we run several experiments and estimate scaling trends from the results. Although this procedure might appear standard from scaling studies in language modeling, we found that instantiating it for value-based RL requires understanding the interaction of the various hyperparameters appearing in TD updates, and the data and compute efficiency of the algorithm. We will formalize these relationships via empirically estimated \emph{laws} and show that these laws extrapolate reliably to new settings not used to obtain these empirical laws. Therefore, in this section, we present empirical and conceptual arguments to build functional forms of relationships between different hyperparameters. Before doing so, we provide our answers to \cref{prob:general_prob,prob:maximize_compute_data}.


\vspace{-0.2cm}
\subsection{Main Scaling Results}
\vspace{-0.1cm}

\begin{wrapfigure}{r}{0.6\textwidth} %
    \centering
    \vspace{-0.4cm}
    \includegraphics[width=0.49\linewidth]{figures/dmc_data_utd.png}
    \hfill
    \includegraphics[width=0.49\linewidth]{figures/dmc_compute_utd.png}
    \vspace{0.1cm}
    \caption{\footnotesize{The data-compute tradeoff on DMC. \textbf{\emph{Left:}} The minimum required data $\mathcal{D}_J$ scales with the UTD $\sigma$ as a power law. \textbf{\emph{Right:}} The minimum required compute $\mathcal{C}_J$ increases with the UTD $\sigma$ as a sum of two power laws. }}
    \label{fig:dmc_compute_data_utd_fit}
    \vspace{-0.2cm}
\end{wrapfigure}
We begin by answering \cref{prob:general_prob} where we need to maximize sample efficiency. We wish to estimate the minimal amount of data $\mathcal{D}_J$ needed to attain a given target performance, given an upper bound on compute $\mathcal{C} \leq \mathcal{C}_0$. To do so, we fit $\mathcal{D}_J$ needed to attain the target performance $J = J_0$ parameterized by the UTD ratio $\sigma$ (\cref{eq:data_efficiency_power_law}). Intuitively, we would expect the minimum amount of data needed to attain a given performance to be low as more updates are made per datapoint (i.e., when $\sigma$ is high), as more ``value'' could be derived from the same datapoint. In addition, we would expect that even for the best value of $\sigma$, there is a minimum number of datapoints $\mathcal{D}^\mathrm{min}$ that are needed to learn given the ``intrinsic'' difficulty of the task at hand. Based on these intuitions, we hypothesize a power law relationship between $\mathcal{D}_J(\sigma)$ and $\sigma$, with an offset $\mathcal{D}^\mathrm{min}$ and constants $\alpha_J$ and $\beta_J$.
{
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
\begin{align}
\label{eq:data_efficiency_power_law}
    \mathcal{D}_J(\sigma) \approx \mathcal{D}^{\mathrm{min}}_J + \left(\frac{\beta_J}{\sigma}\right)^{\alpha_J}
\end{align}
}

Empirical fits of $\mathcal{D}_J$ and $\sigma$ on the DMC suite are in \cref{fig:dmc_compute_data_utd_fit} and they validate the efficacy of this fit.
\begin{AIbox}{Scaling Observation 1: Data Requirements}
The amount of data $\mathcal{D}_J$ needed to reach a given return target $J_0$ decreases as a predictable function of the UTD $\sigma$, and is a power law (\cref{eq:data_efficiency_power_law}).
\end{AIbox}
We also emphasize that the existence of this power law makes $\mathcal{D}_J$ \emph{predictable}, in that this relation is able to predict $\mathcal{D}_J$ for larger values of $\sigma$ that fall outside the range of $\sigma$ values used to get the fit (\cref{fig:extrapolation}).  
 
To answer the optimization questions in \cref{prob:general_prob}, we also need an expression for required compute until the target return $\mathcal{C}_J$. As $\sigma$ determines the number of gradient steps run per data point, $\mathcal{C}_J$ is a function of $\sigma$. In particular, total compute is equal to the number of gradient steps taken multiplied by the parameter count of the model. Our study does not optimize over the model size and treats it as a constant. Thus, we can write the compute $\mathcal{C}_J$ as a function of $\sigma$ as:
{
\setlength{\abovedisplayskip}{4pt}
\setlength{\belowdisplayskip}{4pt}
\begin{align}
\label{eq:compute_calculated}
\begin{split}
    \mathcal{C}_J(\sigma) & \approx 10 \cdot N \cdot B(\sigma) \cdot \sigma \cdot \mathcal{D}_J(\sigma)
\end{split}
\end{align}
}where $N$ denotes the model size, $B(\sigma)$ denotes the ``best choice'' batch size for a given UTD value $\sigma$, and other variables follow definitions from before. Note the additional factor of 10 in \cref{eq:compute_calculated} emerges from the use of multiple forward passes to compute the loss function for value-based RL and the backward pass, through the Q-network (to contrast with language modeling, the typical multiplier is $6$; the gap in our setting comes from the use of multiple forward passes). We plot $\mathcal{C}_J(\sigma)$ for different values of $\sigma$ and $J = J_0$ in \cref{fig:dmc_compute_data_utd_fit}. Since $\mathcal{D}_J(\sigma)$ is not a constant and depends itself on $\sigma$, we note that this particular relationship between $\mathcal{C}_J(\sigma)$ and $\sigma$ is not a simple power law unlike \cref{eq:data_efficiency_power_law}. Instead, our derivation in \cref{eq:double_power_law} shows that $\mathcal{C}_J(\sigma)$ is given by a sum of two different power laws in $\sigma$. Similarly to $\mathcal{D}_J$, we also observe that the compute utilized is a \emph{predictable} function of $\sigma$: we are able to accurately estimate the compute at larger values of $\sigma$ using the relationship in \cref{eq:compute_calculated}.

\begin{AIbox}{Scaling Observation 2: Compute Requirements}
\label{observe:pareto}
The compute $\mathcal{C}_J$ to attain a given return target $J_0$ increases as a predictable function of the UTD ratio $\sigma$, and is a sum of two power laws (\cref{eq:compute_calculated}).
\end{AIbox}

We observe that both required compute and data are controlled by the UTD ratio $\sigma$, which allows us to define a tradeoff between compute and data controlled by $\sigma$. We plot this tradeoff as a curve with compute $\mathcal{C}_J(\sigma)$ as $x$-axis and  $\mathcal{D}_J(\sigma)$ as $y$-axis in \cref{fig:main_results} (left). Further, as $\mathcal{D}_J(\sigma)$ is a monotonically decreasing function of $\sigma$, this curve defines a Pareto frontier: we can move left on the curve to increase data efficiency as the expense of compute and move right to increase compute efficiency at the expense of data. Also interestingly, due to the compute law being a sum of two power laws, in many environments there is a minimum $\sigma$ after which compute efficiency no longer improves as seen on OAI Gym in \cref{fig:main_results}.

\textbf{Solving for maximal data efficiency (\cref{prob:general_prob}, (1)).} We can now solve \cref{prob:general_prob} in setting \textbf{(1)}.  our strategy to address setting \textbf{(1)} is to find the  largest $\sigma$ (say $\sigma_\mathrm{max}$) that satisfies the compute constraint $\mathcal{C}_J(\sigma) \leq \mathcal{C}_0$, and then plug this $\sigma_\mathrm{max}$ into $\mathcal{D}_J(\sigma)$ to obtain the data estimate. This approach enables us to express $\mathcal{D}_J$ directly as a function of the available compute $\mathcal{C}_0$, as we calculate in \cref{eq:compute_calculated}. This can be visualized as finding the value $\mathcal{D}_J$ corresponding to some value $\mathcal{C}_0$ on the Pareto frontier (\cref{fig:main_results}, left)

\textbf{Solving for maximal compute efficiency (\cref{prob:general_prob}, (2)).} Likewise, the solution in \textbf{(2)} can be obtained by finding the smallest value of $\sigma$ in the range that satisfies the data constraint $\mathcal{D}_J(\sigma) \leq \mathcal{D}_0$, and computing the corresponding value of $\mathcal{C}_J(\sigma)$. This can similarly be visualized on the Pareto frontier (\cref{fig:main_results}, left). We summarize our observations in terms of the following takeaway. 

\begin{AIbox}{Solving Problem 3.1: Defining the Compute-Data Pareto frontier}
The UTD ratio $\sigma$ defines a Pareto frontier between data and compute requirements, and estimating this frontier yields predictable solutions to resource optimization problems in settings \textbf{(1)} and \textbf{(2)}. Theoretically, the optimal $\mathcal{D}^*_J$ for an available compute budget $\mathcal{C}_0$ is:
{
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
\begin{align}
\label{observe:optimal}
    \mathcal{D}^*_J(\mathcal{C}_0) \approx \mathcal{C}_0 \cdot \left(10 \cdot N \cdot B(\sigma^*) \cdot \sigma^*\right)^{-1}.
\end{align}
}

The optimal $\mathcal{C}_J$ for a given data budget $\mathcal{D}_0$ is:
{
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
\begin{align}
    \label{eq:pareto}
    \!\!\!\!\!\mathcal{C}^*_J(\mathcal{D}_0) \approx 10 \cdot N \cdot B(\sigma^*) \cdot \sigma^* \cdot \mathcal{D}_0.
\end{align}
}Above, $\sigma^*$ denotes the minimizing UTD value. Calculation details are in \cref{app:calculations}.
\end{AIbox}



\textbf{Maximize return within a budget (\cref{prob:maximize_compute_data}).} Finally, we tackle \cref{prob:maximize_compute_data} in order to extrapolate from low to high return. Here, we do not want to minimize resources, but rather want to maximize performance within a given total ``budget'' on data and compute. As discussed in \cref{sec:problem_statement}, we consider budget functions linear in both data and compute, i.e., $\mathcal{F} = \mathcal{C} + \delta \cdot \mathcal{D}$, for a given constant $\delta$.
Our estimated 
\begin{wrapfigure}{t}{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/dmc_isocost.png}
    \vspace{-0.4cm}
    \caption{\footnotesize{Visualization of the solution to \cref{prob:maximize_compute_data}. Several Pareto frontiers (\cref{fig:main_results}, left}) are shown, together with lines of iso-budget $\mathcal{F}$, which define optimal budget points $(\mathcal{D}^*, \mathcal{C}^*)$. Corresponding optimal UTD ratios $\sigma^*$ are a predictable function of the budgets $\mathcal{F}_0$, trend line shown dashed.}
    \label{fig:isocost}
    \vspace{-0.4cm}
\end{wrapfigure}
Pareto frontier in \cref{eq:pareto}  will enable answering this question. To do so, we turn to directly predicting a good UTD value $\sigma^*$. This UTD value is one that not only leads to maximal performance, but also stays within the total resource budget $\mathcal{F}_0$. Once the UTD value has been identified, it prescribes a concrete way to partition the total resource budget into good data and compute requirements using the solutions to \cref{prob:general_prob}. 

We plot the data-compute Pareto frontiers for multiple values of $J_0$ in~\cref{fig:isocost} and in~\cref{fig:main_results} (right), and find that these curves move diagonally to the top-right for larger $J_0$. Intersecting these curves with iso-budget frontiers over $\mathcal{D}$ and $\mathcal{C}$ prescribed by the budget function, gives us the largest possible $J_0$ for which there is still a $(\mathcal{D}, \mathcal{C})$ pair that just falls just within the budget $\mathcal{F}_0$ but attains performance $J_0$ (see \cref{fig:isocost} for a worked out version of this procedure). Since both $\mathcal{D}$ and $\mathcal{C}$ are explained by $\sigma$, we can associate this point with a given $\sigma$ value. Hence, we can estimate the best value of $\sigma^*(\mathcal{F}_0)$ for a given budget threshold $\mathcal{F}_0$. Concretely, we observe a power law between $\sigma(\mathcal{F}_0)$ and $\mathcal{F}_0$, with constants $\beta_\sigma$ and $\alpha_\sigma$.  
{
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
\begin{align}
\label{eq:budget_power_law}
    \sigma^*(\mathcal{F}_0) \approx \left(\frac{\beta_\sigma}{\mathcal{F}_0}\right)^{\alpha_\sigma}.
\end{align}
}

\begin{AIbox}{Solving Problem 3.2: Maximize return given a total data and compute budget}
The best UTD value $\sigma$ that leads to maximal $J$ is a predictable function of the budget $\mathcal{F}_0$ over data and compute, this relationship follows a power law, and also extrapolates to large budgets.
\end{AIbox}

This relationship produces the optimal $\sigma$, and as a result, the optimal data and compute allocations to reliably attain maximum performance. As shown in \cref{fig:main_results}, estimating this law from low-budget experiments is sufficient for predicting good $\sigma$ values for large budget runs. These predicted $\sigma^*(\mathcal{F}_0)$ values extrapolate reliably to budgets outside the range used to fit this law (as shown by $\times$ in \cref{fig:main_results}). This concludes an exposition of our main results.


\vspace{-0.2cm}
\subsection{Fitting Relationships Between $(B, \eta, \sigma)$}
\vspace{-0.1cm}



To arrive at these scaling law fits above, we had to set hyperparameters $B$ and $\eta$, which we empirically observed to be important. We fit these hyperparameters as a function of $\sigma$, the only variable appearing in many of the scaling relationships discussed above. In this section, we will now describe how to estimate good values of $B$ and $\eta$ in terms of $\sigma$. Our analysis here relies crucially on the behavior of TD-learning that is distinct from supervised learning, where the UTD ratio $\sigma$ does not exist.

To understand relationships between batch size $B$, learning rate $\eta$, and the UTD ratio $\sigma$, we ran an extensive grid search. We first attempted to explain the relationship between the $B$ and $\eta$ values that attain the highest data efficiency (denoted $B^*, \eta^*)$ 
using the standard heuristic in supervised learning: \emph{when the batch size is smaller than the critical batch size, $B$ and $\eta$ are inversely correlated with each other}~\citep{mccandlish2018empirical}. However, as shown in \cref{fig:dmc_lr_bs_utd_correlation} (right), we find that without including the UTD ratio $\sigma$, best $B^*$ and $\eta^*$ exhibit very weak correlation. Further, the critical batch size~\citep{mccandlish2018empirical} does not correlate with empirically best batch size as we show in \cref{sec:critical-batch-size}. Instead, surprisingly, we observe a strong correlation between $B^*$ and $\sigma$, as well as $\eta^*$ and $\sigma$, respectively. Since $B^*$ and $\eta^*$ exhibit near zero correlation among themselves, we can simply omit their dependency and opt for modeling them independently as a function of the UTD ratio, $\sigma$. We conceptually explain relationships between $B^*$ and $\sigma$, and $\eta^*$ and $\sigma$ below and show that models developed from this understanding enable us to reliably predict good values of $B$ and $\eta$, allowing us to fully answer \cref{prob:general_prob}.



\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/dmc_analysis.pdf}
    \vspace{-0.4cm}
    \caption{\footnotesize{Hyperparameter effects in supervised learning and TD learning on DMC. \textbf{\emph{Top:}} Overfitting increases with UTD while batch size can be used to counteract it. \emph{\textbf{Bottom:}} Higher UTD leads to poor training dynamics and plasticity loss~\citep{doro2022sample}. Lower learning rates can be used to counteract it. While these relationships are not perfectly predictable, we use them to inform our design choices.}}
    \label{fig:dmc_analysis}
\vspace{-0.4cm}
\end{figure*}


\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/dmc_bs_lr_utd.pdf}
     \vspace{-0.3cm}
    \caption{\footnotesize{\emph{\textbf{Left, middle}}: Fitting the best learning rate $\eta^*$ and batch size $B^*$ given UTD $\sigma$ on DMC. Modeling the dependency on $\sigma$ is crucial to obtain good hyperparameters, whereas using constant $B, \eta$ as is commonly done leads too poor extrapolation. \emph{\textbf{Right}:} the best learning rate and batch size are not significantly correlated, a major difference from supervised learning.}}
    \label{fig:dmc_lr_bs_utd_correlation}
    \vspace{-0.3cm}
\end{figure*}


\textbf{Predicting best choice of $B$ in terms of $\sigma$.} Our proposed functional form for the best batch size $B^*$ takes the form of a power law in $\sigma$, which we also empirically validate in \cref{fig:dmc_lr_bs_utd_correlation} (left). We posit this form because, intuitively, large batch sizes increase the risk of overfitting because they lead to repetitive training on a fixed set of data. Furthermore, a small training loss on the distribution of data in the buffer does not necessarily reflect the behavior policy distribution of a learning agent~\citep{levine2020offline}. This means that minimizing the training loss to a large extent can result in poor test performance $J(\pi)$, as also seen by prior work~\citep{li2023efficient,Nauman2024overestimation}. One way to counteract this form of ``overfitting'' from a high UTD value $\sigma$ is to instead reduce the batch size in the run so that the training process sees a given sample fewer times. In fact, for a fixed UTD value $\sigma$, we empirically validate this hypothesis that a lower $B$ leads to substantially reduced overfitting on several tasks in \cref{fig:dmc_analysis}. Hence, we post an inverse relationship between the best batch size $B^*$ and the UTD value $\sigma$. We show in \cref{fig:dmc_lr_bs_utd_correlation} that indeed this inverse relationship can be estimated well by a power law, given formally as:
{
\setlength{\abovedisplayskip}{4pt}
\setlength{\belowdisplayskip}{4pt}
\begin{align}
\label{eq:bs_law}
    B^*(\sigma) \approx \left(\frac{\beta_B}{\sigma}\right)^{\alpha_B}.
\end{align}
}


\textbf{Predicting best choice of learning rate $\eta$ as a function of $\sigma$.} Next we turn to understanding the relationship between $\eta$ and $\sigma$. We start from a simple observation: a very large $\sigma$ typically leads to worse performance not only due to overfitting but also due to plasticity loss~\citep{kumar2021implicit,doro2022sample,lyle2023understanding}, 
defined broadly as the inability of the value network to fit TD targets appearing later in training. Prior work states that plasticity loss is inherently related to the number of gradient steps performed and claims that larger norms of parameters of the Q-network are indicative of plasticity loss~\citep{doro2022sample, lyle2023understanding}. We would expect a larger learning rate to make higher magnitude updates against the same TD target, and hence move parameters to a state that suffers from difficulty in fitting subsequent targets~\citep{dabney2021value, lee2024plastic}. As shown in \cref{fig:dmc_analysis}, the parameter norm indeed increases with a high learning rate. Therefore, given a UTD value $\sigma$, we hypothesize that the best choice of learning rate, $\eta^*(\sigma)$ for a given performance should scale inversely in $\sigma$. Empirically we observe that this is indeed the case (\cref{fig:dmc_lr_bs_utd_correlation} (middle)), and we model this relationship: 
{
\setlength{\abovedisplayskip}{4pt}
\setlength{\belowdisplayskip}{4pt}
\begin{align}
\label{eq:lr_law}
    \eta^*(\sigma) \approx \left(\frac{\beta_\eta}{\sigma}\right)^{\alpha_\eta}.
\end{align}
}


\begin{AIbox}{Scaling Observation 3: Hyperparameter Selection}
The best choices for the batch size and learning rate are predictable functions of the UTD $\sigma$, and both of these relationships follow a power law.
\end{AIbox}

\vspace{-0.2cm}
\subsection{Empirical Workflow for Obtaining Fits} 
\vspace{-0.1cm}
\begin{AIbox}{Our Workflow for Fitting Empirical Relationships}
\begin{enumerate}[leftmargin=1em]
    \setlength\itemsep{0em}
    \item Run a sweep for batch size $B$ and learning rate $\eta$ for several values of UTD $\sigma$. Since the batch size and learning rate are independent for the best $\sigma$, we can run these sweeps independently. 
    \item Estimate empirically the best of batch size $\tilde{B}$ and learning rate $\tilde\eta$, with statistical bootstrapping.
    \item Fit $B^*(\sigma)$ and $\eta^*(\sigma)$ on $\tilde{B}$, $\tilde{\eta}$ according to \cref{eq:bs_law,eq:lr_law}.
    \item Using the found fits $B^*(\sigma), \eta^*(\sigma)$, run different values of $\sigma$ that cover a range spanning an order of magnitude; we use $16\times$, i.e., $\sigma_\text{max}/\sigma_\text{min}>16$. 
    \item Fit $\mathcal{D}_J(\sigma)$ according to \cref{eq:data_efficiency_power_law}. 
    \item Using fits of $\mathcal{D}_J(\sigma)$ for different values of $J_0$, fit $\sigma^*(\mathcal{F}_0)$ according to \cref{eq:budget_power_law}. 
    \item Optimal hyperparameters can now be extrapolated to larger data, larger compute, or larger budget settings according to \cref{prob:general_prob}. 
\end{enumerate}
\end{AIbox}

Having presented solutions to \cref{prob:general_prob,prob:maximize_compute_data}, we now present the workflow we utilize to estimate these empirical fits. Further details are in \cref{sec:implementation_details} and \cref{app:fitting_procedure}. This workflow can serve as a useful skeletion for scaling law studies with other value-based algorithms as well.


\vspace{-0.1cm}
\subsection{Evaluating Extrapolation} 
\label{sec:extrapolation}
\vspace{-0.1cm}
\textbf{Evaluating budget extrapolation}. Results on all environments are shown in \cref{fig:main_results} (middle). We estimate several Pareto frontiers corresponding to points with equal changes in budget. We perform the $\sigma^*(\mathcal{F}_0)$ fit, while holding out two largest budgets. The quality of our fit for these two extrapolated budgets can be seen in the figure.

\textbf{Evaluating Pareto frontier extrapolation}. Results on OpenAI Gym are shown in \cref{fig:extrapolation}. We fit the data efficiency equation $\mathcal{D}_J(\sigma)$ \cref{eq:data_efficiency_power_law} while holding out either two UTD values $\sigma$ with largest data requirement (left) or two $\sigma$ values with largest compute requirement (right). The quality of our fit for these two extrapolated $\sigma$ values can be seen in the figure.

\textbf{Hyperparameter fit extrapolation}. Results on OpenAI Gym are shown in \cref{fig:extrapolation} (right). We plot the data efficiency fit when using hyperparameters according to our found dependency $B^*(\sigma), \eta^*(\sigma)$ (shown in olive). These fits are estimated from $\sigma = 1, \cdots, 8$ and extrapolated to $\sigma = 0.5$. We compare the typical approach of tuning hyperparameters in online RL, where hyperparameters are tuned for one setting of $\sigma=2$ and this setting is used for all UTD values (shown in blue). We see that our proposed hyperparameter fits improve results for values other than $\sigma=2$. Further, this improvement is larger for larger values of $\sigma$, showing that accounting for hyperparameter dependency is critical.




\vspace{-0.2cm}
\section{Experimental Details}
\label{sec:implementation_details}
\vspace{-0.1cm}




\begin{figure}
    \centering
    \includegraphics[width=0.32\linewidth]{figures/gym_frontier_extrapolation_data.pdf}
    \hfill
    \includegraphics[width=0.32\linewidth]{figures/gym_frontier_extrapolation_compute.pdf}
    \hfill    
    \includegraphics[width=0.32\linewidth]{figures/utd_data.pdf}
    \caption{\footnotesize{Extrapolation towards unseen values of $\sigma$ on OpenAI Gym. \textbf{Left:} We show Pareto frontier extrapolation towards higher data regime. \textbf{Middle:} We show Pareto frontier extrapolation towards higher compute regime. \textbf{Right:} We compare the best-performing hyperparameters (red) for $\sigma=2$ to hyperparameters predicted via our proposed workflow (blue). }}
    \label{fig:extrapolation}
    \vspace{-0.3cm}
\end{figure}



\paragraph{Experimental Setup} 
We focus on 12 tasks from 3 domains in our study. On \textbf{OpenAI Gym}~\citep{openaigym}, we use Soft Actor Critic, a commonly used TD-learning algorithm~\citep{haarnoja2018soft}. We first run a sweep on 5 values of $\eta$, then a grid of runs with 4 values of $\sigma$ and 3 values of $B$, and then use hyperparameter fits to run 2 more value of $\sigma$ with 8 seeds per task. To test our approach with larger models, we use \textbf{DMC}~\citep{tassa2018deepmind}, where, we utilize the state-of-the-art Bigger, Regularized, Optimistic (BRO) algorithm~\citep{nauman2024bigger} that uses a larger and more modern architecture. We first run 5 values of $B$, 4 values of $\eta$, and 4 $\sigma$; and then use hyperparameters fits to run 2 more values of $\sigma$, with 10 seeds per task. Finally, we test our approach with more data on \textbf{IsaacGym}~\citep{makoviychuk2021isaac}, where we use the Parallel Q-Learning (PQL) algorithm~\citep{li2023parallel}, which was designed to leverage massively parallel simulation like Isaac Gym that can quickly produce billions of environment samples. Because of computational expense, we only run one IsaacGym task. We first run 4 values of $\sigma$, 3 values of $\eta$, as well as 5 values of $B$, with 5 seeds per task, after which we run a second round of grid search with 7 values of $\sigma$. Further details are in \cref{app:figure_details,app:fitting_procedure,tab:listed_hypers}.



\paragraph{Fitting Functional Forms for Scaling Laws} 
We approximate \cref{eq:data_efficiency_power_law} via brute-force search followed by LBFG-S with a log-MSE loss following~\citep{hoffmann2022training}. For \cref{eq:bs_law,eq:lr_law}, we fit a line in log space using least squares regression following \citet{kaplan2020scaling}. In our experiments, we run a single fit that is shared across different tasks in a given benchmark. Specifically, we share the slope $\alpha_B, \alpha_\eta$ and use task-specific intercepts $\sigma_B^\text{env}, \sigma_\eta^\text{env}$ (as defined in \cref{eq:bs_law,eq:lr_law}) to be different for separate tasks. This technique is standard in ordinary least squares modeling and is referred to as {fixed effect regression}~\citep{bishop2006pattern}. Sharing this slope serves the goal of variance reduction, which can be important if the granularity of the grid search over various hyperparameters run is coarse. More details are in \cref{app:figure_details,app:fitting_procedure}. 



\vspace{-0.2cm}
\section{Related Work}
\vspace{-0.1cm}
\textbf{Scaling laws and predictability.} Prior work has studied scaling laws in the context of supervised learning~\citep{kaplan2020scaling,hoffmann2022training}, primarily to predict the effect of model size and training data on validation loss, while marginalizing out hyperparameters like batch size~\citep{mccandlish2018empirical} and learning rate~\citep{kaplan2020scaling}. There are several extensions of such scaling laws for language models, such as laws for settings with data repetition~\citep{muennighoff2023scaling} or mixture-of-experts~\citep{ludziejewskiscaling}, but most focus on cross-entropy loss, with an exception of \citet{gadre2024language}, which focuses on downstream metrics. While scaling laws have guided supervised learning experiments, little work explores this for RL. The closest works are: \citet{hilton2023scaling} which fits power laws for on-policy RL methods using model size and the number of environment steps; \citet{jones2021scalingscalinglawsboard} which studies the scaling of AlphaZero on board games of increasing complexity; and \citet{gao2023scaling} which studies reward model overoptimization in RLHF. In contrast, we are the first ones to study off-policy value-based RL methods that are trained via TD-learning. Not only do off-policy methods exhibit training dynamics distinct from supervised learning and on-policy methods~\citep{kumar2021dr3,lyle2023understanding}, but we show that this distinction also results in a different functional form for scaling law altogether. We also note that while \citet{hilton2023scaling} use minimal compute, i.e., $\mathcal{C}_J$ in our notation as a metric of performance, our analysis goes further in several respects: \textbf{(1)} we also study the tradeoff between data and compute (\cref{fig:main_results}), \textbf{(2)} we can predict the algorithm configuration for best performance (\cref{prob:general_prob}); \textbf{(3)} we study many budget functions ($\mathcal{C} + \delta \cdot \mathcal{D}$ can be any affine function).


\textbf{Methods for large-scale deep RL.} Recent work has scaled deep RL across three axes: model size~\citep{kumar2023offline, schwarzer2023bigger, nauman2024bigger}, data~\citep{kumar2023offline, gallici2024simplifying, singla2024sapg}, and UTD~\citep{chen2020randomized, doro2022sample}. Na\"ive scaling of model size or UTD often degrades performance or causes divergence~\citep{nikishin2022primacy, schwarzer2023bigger}, mitigated by classification losses~\citep{kumar2023offline}, layer normalization~\citep{Nauman2024overestimation}, or feature normalization~\citep{kumar2021dr3}. In our work, we use scaled network architectures from \citet{nauman2024bigger} (\cref{sec:implementation_details}). In on-policy RL, prior works focus on effective learning from parallelized data streams in a simulator or a world model~\citep{mnih2016asynchronous, silver2016mastering, schrittwieser2020mastering}. Follow-up works like IMPALA~\citep{espeholt2018impala} and SAPG~\citep{singla2024sapg} use a centralized learner that collects experience from distributed workers with importance sampling updates. These works differ substantially from our study as we focus exclusively on value-based off-policy RL algorithms that use TD-learning and not on-policy methods. In value-based RL, prior work on data scaling focuses on offline~\citep{yu2022leverage, kumar2023offline, park2024value} and multi-task RL~\citep{hafner2023mastering}. In contrast, we study online RL and fit scaling laws to answer resource optimization questions. 


\vspace{-0.2cm}
\section{Discussion, Limitations, and Future Work}
\vspace{-0.1cm}
In this paper, we show that value-based deep RL algorithms scale predictably. We establish relationships between good values of hyperparameters of value-based RL. We then establish a relationship between required data and required compute for a certain performance. Finally, this allows us to determine an optimal allocation of resources to either data and compute. Although only estimated from small-scale runs, our empirical models reliably \emph{extrapolate} to large compute, data, budget, or performance regimes. To the best of our knowledge, this is the first demonstration that it is possible to predict behavior of value-based off-policy RL algorithms at larger scale using small-scale experiments.

At the same time, this first study also presents  a number of open questions and challenges:
\begin{enumerate}[noitemsep,topsep=0pt]
\item While simple power law models work well, an open question remains as to whether such laws are theoretically grounded, and whether there are better and more refined functional forms. 
\item Our study only focused on three hyperparameters ($B$, $\eta$, and $\sigma$). We do not focus on optimal tradeoff between model size and UTD, which is important for compute scaling. For data efficient RL, it is important to analyze the dependency of weight decay and weight reset frequency on UTD, which are typical tricks employed by many of the most performant methods in literature. 
\item While we focus on online RL, it is important to study scaling of offline-to-online and offline RL, which will allow direct applications of scaling law findings to large model training. 
\item {Finally}, while we study relatively small models, future work will focus on verifying our results with larger model scales, larger scale tasks, study the effect of modern architectures, and cover a larger range of compute scales spanning multiple orders of magnitude. 
\end{enumerate}
Our work is only the first step in studying scaling laws for value-based RL methods. Further research has the potential to improve our understanding of value-based RL at scale, provide researchers with tools to focus innovation on more important components, and eventually provide guidelines towards scaling value-based RL similarly to scaling enjoyed by other modern deep learning approaches. 

\section*{Acknowledgements}

We would like to thank Zhang-Wei Hong, Amrith Setlur, Rishabh Agarwal, Seohong Park, and Max Simchowitz for feedback on an earlier version of this paper. We would like to thank Andrea Zanette, Seohong Park, Kyle Stachowicz, and Qiyang Li for informative discussions. This research was supported by ONR under N00014-24-12206, N00014-22-1-2773, and ONR DURIP grant, with compute support from the Berkeley Research Compute, Polish high-performance computing infrastructure, PLGrid (HPC Center: ACK Cyfronet AGH), that provided computational resources and support under grant no. PLG/2024/017817. Pieter Abbeel holds concurrent appointments as a Professor at UC Berkeley and as an Amazon Scholar. This work was done at UC Berkeley and CMU, and is not associated with Amazon.


\bibliography{example_paper}


\newpage
\appendix
\onecolumn

\part*{Appendices}

\section{Additional details on derivations}
\label{app:calculations}





\textbf{FLOPs calculation.} Recall that FLOPs per forward and backward passes are equal to $\mathcal{C}_J^{\text{forward}}(\sigma) \approx 2 \cdot N \cdot B(\sigma) \cdot \sigma \cdot \mathcal{D}_J(\sigma)$ and $\mathcal{C}_J^{\text{backward}}(\sigma) \approx 4 \cdot N \cdot B(\sigma) \cdot \sigma \cdot \mathcal{D}_J(\sigma)$, with $\sigma$ denoting the number of gradient steps per environment steps. Q-learning methods used in our study use MLP and ResNet architectures, which are well modeled with this approximation. Assuming same size for actor and critic as an approximation, a training iteration of the critic requires three forward passes and one backward pass, totaling $\mathcal{C}_J^{\text{critic}}(\sigma) \approx 10 \cdot N \cdot B(\sigma) \cdot \sigma \cdot \mathcal{D}_J(\sigma)$. A training iteration of the actor requires two forward and two backward passes, totaling $\mathcal{C}_J^{\text{actor}}(\sigma) \approx 12 \cdot N \cdot B(\sigma) \cdot \sigma \cdot \mathcal{D}_J(\sigma)$. Here we follow the standard practice of updating the actor every time a new data point collected, while the critic is updated according to the UTD ratio $\sigma$. Since we expect the critic to be updated more then the actor. As such, in this study we assume 
\begin{equation}
    \label{eq:flops}
    \mathcal{C}_J(\sigma) \approx \mathcal{C}_J^{\text{critic}}(\sigma) \approx 10 \cdot N \cdot B(\sigma) \cdot \sigma \cdot \mathcal{D}_J(\sigma).
\end{equation}

\paragraph{Compute and sample efficiency.} 

Following \cref{eq:data_efficiency_power_law}, the number of data points required to achieve performance $J$ is equal to:

\begin{equation}
    \mathcal{D}_J(\sigma) \approx \mathcal{D}^{\mathrm{min}}_J + \left(\frac{\beta_J}{\sigma}\right)^{\alpha_J}
\end{equation}

Given the expressions for required data points, practical batch size, and FLOPs \cref{eq:data_efficiency_power_law,eq:flops,eq:bs_law}, we can now derive the expression for compute required to reach a particular performance expressed in terms of $\sigma$. First, note that the number of parameter updates is
\begin{equation}
    \sigma \cdot \mathcal{D}_J(\sigma)  \approx \sigma \cdot \mathcal{D}^{\mathrm{min}}_J +\frac{\beta_J^{\alpha_J}}{\sigma^{\alpha_J-1}}
\end{equation}

Combining above, \cref{eq:bs_law} with \cref{eq:flops} yields:

\begin{align}
\begin{split}
    \mathcal{C}_J(\sigma) &\approx 10 \cdot N \cdot B(\sigma) \cdot \left(\sigma \cdot \mathcal{D}^{\mathrm{min}}_J +\frac{\beta_J^{\alpha_J}}{\sigma^{\alpha_J-1}}\right) \\
    &\approx 10 \cdot N \cdot \left( \frac{\beta_B}{\sigma} \right)^{\alpha_B} \cdot \left(\sigma \cdot \mathcal{D}^{\mathrm{min}}_J +\frac{\beta_J^{\alpha_J}}{\sigma^{\alpha_J-1}}\right) \\
    &\approx 10 \cdot N \cdot \left( \frac{\mathcal{D}^{\mathrm{min}}_J \cdot \beta_B^{\alpha_B}}{\sigma^{\alpha_B - 1}} + \frac{\beta_J^{\alpha_J} \cdot \beta_B^{\alpha_B}}{\sigma^{\alpha_J + \alpha_B - 1}} \right)   \label{eq:double_power_law}.
\end{split}
\end{align}


We observe that the resulting expression is a sum of two power laws. In practice, one of the power laws will dominate the expression and a simple mental model is that compute increases with UTD as a power law with a coefficient $<$ 1 (see \cref{fig:dmc_compute_data_utd_fit}). 

\paragraph{Maximal compute efficiency.} Here, we solve the compute optimization problem presented in \cref{sec:problem_statement}. We write the problem:



\begin{equation}
\begin{split}
    (B^*, \eta^*, \sigma^*) :=& \arg \min_{(B, \eta, \sigma)}~~~ \mathcal{C} ~~~ ~~~ \text{s.t.}~~~J\left(\pi_\mathrm{Alg}{(B, \eta, \sigma)} \right) \geq J_0 ~~~ \land ~~~  \mathcal{D} \leq {D}_0.
\end{split}
\end{equation}

Firstly, we formulate the Lagrangian $\mathcal{L}$:

\begin{equation}
\begin{split}
    \mathcal{L}(\sigma, \lambda) & = \mathcal{C}_J(\sigma) + \lambda \cdot \left( \mathcal{D}_J(\sigma) - D_0 \right) \\
    & \approx 10 \cdot N \cdot B(\sigma) \cdot \left(\sigma \cdot \mathcal{D}^{\mathrm{min}}_J +\frac{\beta_J^{\alpha_J}}{\sigma^{\alpha_J-1}}\right) + \lambda \cdot \left( \mathcal{D}^{\mathrm{min}}_J + \left(\frac{\beta_J}{\sigma}\right)^{\alpha_J} - \mathcal{D}_0 \right)
\end{split}
\end{equation}

Here, the constrained with respect to performance $J_0$ is upheld through the use of $\mathcal{C}_J(\sigma)$ and $\mathcal{D}_J(\sigma)$ which are defined such that $J=J_0$. We proceed with calculating the derivative with respect to $\lambda$ to find the minimal $\sigma$ that is able to achieve the desired sample efficiency $\mathcal{D}_J$. We denote such such optimal UTD as $\sigma^*$:

\begin{equation}
\begin{split}
    \frac{\partial \mathcal{L}}{\partial \lambda} = \mathcal{D}^{\mathrm{min}}_J + \left(\frac{\beta_J}{\sigma}\right)^{\alpha_J} - \mathcal{D}_0 = 0 ~~~ \implies ~~~ \sigma^* = \frac{-\beta_J}{\left(\mathcal{D}^{\mathrm{min}}_J - \mathcal{D}_0\right)^{\nicefrac{1}{\alpha_J}}} 
\end{split}
\end{equation}

Then, we substitute the $\sigma^*$ into the expression defining compute, as well as use \cref{eq:bs_law}:

\begin{equation}
\begin{split}
    \mathcal{C}_J(\sigma^*) & \approx 10 \cdot N \cdot \frac{\beta_B^{\alpha_B}}{\sigma^{\alpha_B-1}} \cdot \left(\mathcal{D}^{\mathrm{min}}_J +\frac{\beta_J^{\alpha_J}}{\sigma^{\alpha_J}}\right) \\
    & \approx 10 \cdot N \cdot \frac{\beta_B^{\alpha_B}}{(\sigma^*)^{\alpha_B-1}} \cdot \left(\mathcal{D}^{\mathrm{min}}_J + \frac{\beta_J^{\alpha_J} \cdot \left(\mathcal{D}^{\mathrm{min}}_J - \mathcal{D}_0\right)}{-\beta_J^{\alpha_J}}\right) \\
    & \approx 10 \cdot N \cdot \beta_B^{\alpha_B} \cdot (\sigma^*)^{1 - \alpha_B} \cdot \mathcal{D}_0
\end{split}
\end{equation}

\paragraph{Maximal sample efficiency.} Firstly, we note that we treat $B(\sigma)$ as a constant and do not optimize with respect to it. We start with the problem definition:

\begin{equation}
\begin{split}
    (B^*, \eta^*, \sigma^*) :=& \arg \min_{(B, \eta, \sigma)}~~~ \mathcal{D} ~~~ ~~~ \text{s.t.}~~~J\left(\pi_\mathrm{Alg}{(B, \eta, \sigma)} \right) \geq J_0 ~~~ \land ~~~  \mathcal{C} \leq {C}_0.
\end{split}
\end{equation}

Similarly to the maximal compute efficiency problem, we formulate the Lagrangian $\mathcal{L}$:

\begin{equation}
\begin{split}
    \mathcal{L}(\sigma, \lambda) & = \mathcal{D}_J(\sigma) + \lambda \cdot \left( \mathcal{C}_J(\sigma) - C_0 \right) \\
    & \approx \mathcal{D}^{\mathrm{min}}_J + \left(\frac{\beta_J}{\sigma}\right)^{\alpha_J} + \lambda \cdot \left( 10 \cdot N \cdot B(\sigma) \cdot \sigma \cdot  \left(\mathcal{D}^{\mathrm{min}}_J +\frac{\beta_J^{\alpha_J}}{\sigma^{\alpha_J}}\right) - \mathcal{C}_0 \right)
\end{split}
\end{equation}

Again, we uphold the constraint with respect to the performance through the use of $\mathcal{D}_J(\sigma)$ and $\mathcal{C}_J(\sigma)$. We calculate the derivative with respect to $\lambda$:

\begin{equation}
\begin{split}
    \frac{\partial \mathcal{L}}{\partial \lambda} = 10 \cdot N \cdot B(\sigma) \cdot \sigma \cdot  \left(\mathcal{D}^{\mathrm{min}}_J +\frac{\beta_J^{\alpha_J}}{\sigma^{\alpha_J}}\right) - \mathcal{C}_0 = 0 ~~~ \implies ~~~ \mathcal{D}^{\mathrm{min}}_J +\frac{\beta_J^{\alpha_J}}{\sigma^{\alpha_J}} = \frac{\mathcal{C}_0}{10 \cdot N \cdot B(\sigma) \cdot \sigma} = \mathcal{D}_J
\end{split}
\end{equation}

Since $\mathcal{D}_J$ is monotonic in $\sigma$ and does not model impact of $B$ on the sample efficiency, the optimization problem can be solved via Weierstrass extreme value theorem. As such, we find the biggest $\sigma$ and that fulfills the compute constraint, and find the data requirement for such $\sigma$.






\section{Experimental details}
\label{app:figure_details}


For our experiments, we use a total of 12 tasks from 3 benchmarks (DeepMind Control~\citep{tunyasuvunakool2020}, Isaac Gym~\citep{makoviychuk2021isaac}, and OpenAI Gym~\citep{openaigym}). We list all considered tasks in \cref{tab:listed_tasks}. 

\begin{table}[h!]
    \centering
    \caption{Tasks used in presented experiments.}
    \label{tab:listed_tasks}
    \begin{tabular}{@{}ccc@{}}
    \toprule
    \textbf{Domain} & \textbf{Task} & \textbf{Optimal $\pi$ Returns} \\ \midrule
    {DeepMind Control} & Cartpole-Swingup & 1000 \\
    & Cheetah-Run & 1000 \\
    & Dog-Stand & 1000 \\
    & Finger-Spin & 1000 \\
    & Humanoid-Stand & 1000 \\
    & Quadruped-Walk & 1000 \\
    & Walker-Walk & 1000 \\ \midrule
    Isaac Gym & Franka-Push & 0.05 \\ \midrule
    {OpenAI Gym} & HalfCheetah-v4 & 8500 \\
    & Walker2d-v4 & 4500 \\
    & Ant-v4 & 6625 \\
    & Humanoid-v4 & 6125 \\
    \bottomrule
    \end{tabular}
\end{table}





\paragraph{\cref{fig:main_results}.} 

We use all available UTD values for the fits, which is 6 for DMC, 5 for OAI Gym, and 7 for Isaac Gym. Given the dependency of compute and data on UTD, we plot the resulting curve. We average the data efficiencies across all tasks in each domain, as described in~\cref{app:fitting_procedure}.

We calculate compute given the model sizes of $N = 4.92\text{e}6$ for DMC, $N = 1.5\text{e}5$ for OAI Gym, and $N = 2\text{e}6$ following standard implementations of the respective algorithms.

For budget extrapolation, we use tradeoff values $\delta$ to mimic the wall-clock time of the algorithm. We use $\delta = 1\text{e}10$ for DMC, $\delta = 5\text{e}9$ for OAI Gym, and $\delta = 1\text{e}4$ for Isaac Gym. We exclude runs affected by resets ($\sigma=8$) for DMC since the returns right after the reset are lower, which adds noise to the results.


\paragraph{\cref{fig:dmc_compute_data_utd_fit}.} We use the same data as for DMC in~\cref{fig:main_results} (left).

\paragraph{\cref{fig:isocost}.} We use the same data as for DMC in~\cref{fig:main_results} (right).

\paragraph{\cref{fig:dmc_lr_bs_utd_correlation}.} In the left and central Figures, we evaluate the $B^*$ and $\eta^*$ models. For each DMC task, we find the best hyperparameters according to our workflow and procedure described in \cref{sec:implementation_details} and \cref{app:fitting_procedure}. While the intercepts vary across environments, for simplicity we plot data points and fits from all environments in the same figure by shifting them with the corresponding intercept. In the right Figure, we marginalize over $\sigma$ and visualize best performing pairs of $B$ and $\eta$. 

\paragraph{\cref{fig:dmc_analysis}.} 
\textbf{Left}: we show an illustration that reflects our observed empirical results about the dependencies between hyperparameters. 

\textbf{Right, middle}: we investigate the correlations between overfitting, parameter norm of the critic network, and $\sigma$. We observed the same relationships on all tasks. Here, to avoid clutter, we plot 3 tasks from DMC benchmark: cheetah-run, dog-stand, and quadruped-walk. To measure overfitting, we compare the TD loss calculated on samples randomly sampled from the buffer (corresponding to \emph{training data}) to TD loss calculated on 16 newest transitions (corresponding to \emph{validation data}) according to:
\begin{equation}
    \text{Overfitting} = TD^{\text{training}} - TD^{\text{validation}}.
\end{equation}

We fit the linear curves using ordinary least squares with mean absolute error loss. 

\paragraph{\cref{fig:extrapolation}.} Here, we investigate 4 tasks from OpenAI Gym, listed in \cref{tab:listed_tasks}, and compare the extrapolation performance of two hyperparameter sets: the best performing hyperparameters for $\sigma=1$, found by testing 8 different hyperparameter values listed in \cref{tab:listed_hypers} (we refer to this configuration as \emph{baseline}); and hyperparameters predicted by our proposed models of $B^*$ and $\eta^*$. We fit our models using $\sigma \in (1,2,4,8)$, and extrapolate to $\sigma \in (0.5, 16)$. The graph shows the data efficiency with threshold as 700, normalized according to the procedure in \cref{app:fitting_procedure}.%

\paragraph{\cref{fig:isotonic_bootstrap}.} The goal of the left Figure is to visualize the effects of isotropic regression fit on a noisy data. We use the SciPy package~\citep{virtanen2020scipy} to run the isotropic model. In the right Figure we visualize the process of best hyperparameter selection using bootstrapped confidence intervals. We describe the bootstrapping strategy in \cref{app:fitting_procedure}.

\section{Resulting Fits}

\paragraph{DMC} Refer to \cref{tab:dmc_coefficients} for environment-specific values.
\begin{align}
\begin{split}
    \eta^* &= \beta_\eta \cdot \sigma^{-0.26} \\
    B^* &= \beta_B \cdot \sigma^{-0.47} \\
    \mathcal{D}_J &= \mathcal{D}^\text{min} \cdot \left(1 + \left(\frac{\sigma}{0.45}\right)^{-0.74}\right) \\
    \sigma^* &= 1.4\text{e}8 \cdot \mathcal{F}_0^{-0.53}
\end{split}
\end{align}

\paragraph{OpenAI Gym} Refer to \cref{tab:dmc_coefficients} for environment-specific values. 
\begin{align}
\begin{split}
    \eta^* &= \beta_\eta \sigma^{-0.30} \\
    B^* &= \beta_B \sigma^{-0.33} \\
    \mathcal{D}_J &= \mathcal{D}^\text{min} \cdot \left(1 + \left(\frac{\sigma}{4.02}\right)^{-0.69}\right) \\
    \sigma^* &= 1.4\text{e}8 \cdot \mathcal{F}_0^{-0.53}
\end{split}
\end{align}

\paragraph{Isaac Gym}
\begin{align}
\begin{split}
    \eta^* &= 8.77 \cdot \left(1 + \left(\frac{\sigma}{2.57\text{e-3}}\right)^{-0.26}\right) \\
    B^* &= 38.6 \cdot \left(1 + \left(\frac{\sigma}{1.42\text{e-2}}\right)^{-0.68}\right) \\ 
    \mathcal{D}_J &= 6.8\text{e}7 \cdot \left(1 + \left(\frac{\sigma}{1.88}\right)^{-0.87}\right) \\
    \sigma^* &= 11.3 \cdot \mathcal{F}_0^{-0.57} 
\end{split}
\end{align}

\begin{table}[h]
    \centering
    \caption{Coefficients for DMC and OpenAI Gym fits.}
    \begin{tabular}{llrrr}
    \toprule
    \textbf{Domain} & \textbf{Task} & $\beta_\eta$ & $\beta_B$ & $\mathcal{D}^\text{min}$ \\
    \midrule
    DMC &  cartpole-swingup & 7.55e-4 & 538.2 & 2.4e4 \\
    & cheetah-run & 6.25e-4 & 564.9 & 3.5e5 \\
    & finger-spin & 8.77e-4 & 608.2 & 2.9e4 \\
    & humanoid-stand & 3.86e-4 & 451.8 & 3.8e5 \\
    & quadruped-walk & 8.46e-4 & 526.4 & 6.2e4 \\
    & walker-walk & 9.38e-4 & 313.3 & 3.3e4 \\
    OpenAI Gym & Ant-v4 & 1.35e-4 & 447.0 &   2.7e5 \\
    & HalfCheetah-v4 & 1.86e-3 & 415.4 &  7.8e4 \\
    & Humanoid-v4 & 1.65e-4 & 351.6 &   1.8e5 \\
    & Walker2d-v4 & 7.85e-4 & 399.1 &  1.7e5 \\
    \bottomrule
    \end{tabular}
    \label{tab:dmc_coefficients}
\end{table}


\begin{table}[h!]
\centering
\caption{Tested configurations.}
\label{tab:listed_hypers}
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Hyperparameters} & \textbf{DeepMind Control}           & \textbf{Isaac Gym} & \textbf{OpenAI Gym} \\ \midrule
Updates-to-data $\sigma$        & 1, 2, 4, 8            & $\frac{1}{1024}, \frac{1}{2048}, \frac{1}{4096}, \frac{1}{8192}, \frac{1}{16384}, \frac{1}{32768}, \frac{1}{65536}$ & 1, 2, 4, 8 \\
Batch size $B$        & 32, 64, 128, 256, 512            & 512, 1024, 2048, 4096, 8192 & 128, 256, 512 \\
Learning rate $\eta$        & 15e-5, 3e-4, 6e-4, 12e-3           & 1e-4, 2e-4, 3e-4  & 1e-4, 2e-4, 5e-4, 1e-3, 2e-3 \\
\bottomrule
\end{tabular}
}
\end{table}


\section{Additional details on the fitting procedure}
\label{app:fitting_procedure}


\paragraph{Preprocessing return values.} In order to estimate the fits from our laws, we need to track the data and compute needed by a run to hit a target performance level. Due to stochasticity both in training and and evaluation, na\"ive measurements of this point can exhibit high variance. This in turn would result in low-quality fits for $\mathcal{D}_J$ and $\mathcal{C}_J$. Thus, we preprocess the return values before estimating the fits by running isotonic regression \citep{barlow1972isotonic}. Isotonic regression transforms return values to the most aligned monotonic sequence of values that can then be used to estimate $\mathcal{D}_J$. While in general return values can decrease with more training after reaching a target value, and this will result in a large deviation between the isotonic fit and true return values, the proposed isotonic transformation still suffices for us as our goal is to simply fit the \emph{minimum} number of samples or compute needed to attain a target return. As we can still make reliable predictions that extrapolate to larger scales, the downstream impact of this error is clearly not substantial. We also average across random seeds before running isotonic regression to further reduce noise. We normalize the returns for all environments to be between 0 and 1000 (\cref{tab:listed_tasks} lists pre-normalized returns), and reserve the points of 700 and 800 for budget extrapolation in \cref{fig:main_results}.

\paragraph{Uncertainty-adjusted optimal hyperparameters.} While averaging across seeds and applying isotonic regression reduces noise, we observe that the granularity of our grid search on learning rate and batch size limits the precision of the resulting hyperparameter fits $\tilde{B}$, $\tilde{\eta}$. Noise due to random seed generation makes hyperparameter selection harder as some hyperparameters that appear empirically optimal might simply be so due to noise. We observe that we can correct for this precision loss by constructing a more precise estimate of $\tilde{B}$, $\tilde{\eta}$ adjusted for this uncertainty. Specifically, we run $K=100$ bootstrap estimates by sampling $n$ random seeds with replacement out of the original $n$ random seeds, applying isotonic regression, and selecting the optimal hyperparameters $\tilde{B}_k$, $\tilde{\eta}_k$. We then use the mean of this bootstrapped estimate to improve the precision:
\begin{align}
\begin{split}
\tilde{B}_\text{bootstrap} = & \frac{1}{K}\sum_k \tilde{B}_k \\
\tilde{\eta}_\text{bootstrap} = & \frac{1}{K}\sum_k \tilde{\eta}_k
\end{split}
\end{align}

We have also experimented with more precise laws for learning rate and batchsize by adding an additive offset. In this case, we follow \citet{hoffmann2022training} and fit the data using brute-force search followed by LBFG-S. We use MSE in log space as the error: $\text{MSE}_\text{log} (a,b) = \left(\log a - \log b\right)^2$. %
\begin{align}
\label{eq:bs_law_offset}
    B^*(\sigma) \approx B_\text{min} + \frac{\sigma_B}{\sigma^{\alpha_B}} \\
\label{eq:lr_law_offset}
    \eta^*(\sigma) \approx \eta_\text{min} + \frac{\sigma_\eta}{\sigma^{\alpha_\eta}}.
\end{align}
However, we found that this more complex fit did not validate the decrease of degrees of freedom given a limited sweep range, resulting in  accuracy of extrapolation. %

\paragraph{Independence of $B$ and $\eta$.} Whereas the optimal choice of $B$ and $\eta$ is often intertwined as UTD changes, we observe in our experiments that the correlation between them is relatively low (\cref{fig:dmc_lr_bs_utd_correlation}). If we ran a cross-product grid search with hyperparameter space $ \{B_1, \ldots, B_{n_B}\} \times \{\eta_1, \ldots, \eta_{n_\eta}\}$, we can use this fact to further improve the results by averaging the estimate $\tilde{B}$ over different values of $\eta$. That is, we produce the estimate $\tilde{B}^{[\eta=\eta_{i}]}$ (respectively $\tilde{\eta}^{[B=B_i]}$) by only looking at the runs where $\eta=\eta_i$, and averaging such estimates.
\begin{align}
\begin{split}
\tilde{B}_\text{mean} &=  \frac{1}{n_\eta}\sum_i \tilde{B}^{[\eta=\eta_{i}]} \\
\tilde{\eta}_\text{mean} &=  \frac{1}{n_B}\sum_i \tilde{\eta}^{[B=B_i]}
\end{split}
\end{align}


\textbf{Data efficiency.} We fit data efficiency of the runs with our found practical hyperparameters $B^*, \eta^*$ according to \cref{eq:data_efficiency_power_law}. We follow \citet{hoffmann2022training} and fit the data using brute-force search followed by LBFG-S. We use MSE in log space as the error: $\text{MSE}_\text{log} (a,b) = \left(\log a - \log b\right)^2$.



In DeepMind Control Suite, we would like to share the data efficiency fit across different environments $\mathsf{env}$. We normalize the data efficiency $\mathcal{D}$ by the intra-environment median data efficiency medians $\mathcal{D}_\text{med}^\mathsf{env} = \mathrm{median}\{ \mathcal{D}^\mathsf{env}_{[\sigma=\sigma_i]} | i = 1 .. n_\sigma \}$. For interpretability, we further re-normalize $D$ with the overall median $\mathcal{D}_\text{med}$: $\mathcal{D}_\text{norm} = \mathcal{D}  \cdot \mathcal{D}_\text{med} / \mathcal{D}_\text{med}^\mathsf{env}$. We will need to express the data efficiency law alternatively as:
\begin{align}
    D_J(\sigma) \approx \mathcal{D}^{\mathrm{min}}_J \left(1 + \left(\frac{\beta_J}{\sigma} \right)^{\alpha_J}\right).        
\end{align}
This is equivalent to \cref{eq:data_efficiency_power_law} because the coefficient $\beta_J$ absorbs $\mathcal{D}^{\mathrm{min}}_J$. However, this expression makes explicit an overall multiplicative offset\footnote{This form enforces that $\mathcal{D}^{\mathrm{min}}_J$ is positive.} $\mathcal{D}^{\mathrm{min}}_J$.
Our median normalization is then equivalent to fitting per-environment coefficients $\mathcal{D}^{\mathrm{min}}_J$, following our procedure for environment-shared hyperparameter fits. However, we further improve robustness by fixing the per-environment coefficients to be the median data efficiency and do not require fitting them. 

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{figures/dmc_implementation.png}
    \caption{\footnotesize{\textbf{\emph{Left:}} Determining performance via isotonic regression on DMC. \textbf{\emph{Right:}} improving hyperparameter selection with uncertainty adjustment on DMC. Further details are in \cref{app:fitting_procedure}. }}
    \label{fig:isotonic_bootstrap}
\end{figure}

\section{Critical batch size analysis}\label{sec:critical-batch-size} 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/critical_batch_size.png}
    \vspace{0.01cm}
    \caption{An approximation of the critical batch size over training. Further details are in \cref{sec:critical-batch-size}.}
    \label{fig:critical-batch-size}
\end{figure}

Previous work has argued that there is a critical batch size $B_{\text{crit}}$ for neural network training in image classification, generative modeling, and reinforcement learning with policy gradient algorithms~\citep{mccandlish2018empirical} --- a transition point at which increasing the batch size begins to yield diminishing returns. We follow this work and compute an estimate of the gradient noise scale $B_{\text{noise}} \approx B_{\text{crit}}$ according to the following procedure: throughout training, we compute the gradient norm $|G_B|$ of the critic network for batches of size $B = B_{\text{small}} := 64$ and $B = B_{\text{big}} := 1024$. Then, we evaluate
\begin{align*}
    |\mathcal G|^2 &:= \frac 1{B_{\text{big}} - B_{\text{small}}} \left( B_{\text{big}} |G_{B_{\text{big}}}|^2 - B_{\text{small}} |G_{B_{\text{small}}}|^2 \right) \\
    \mathcal S &:= \frac 1{1 / B_{\text{small}} - 1 / B_{\text{big}}} \left( |G_{B_{\text{small}}}|^2 - |G_{B_{\text{big}}}|^2 \right)
\end{align*}
and take $\tilde B_{\text{crit}} := \mathcal S / |\mathcal G|^2$. In practice, to account for the noisiness of $|G|^2$, we first take rolling averages of $|G_{B_{\text{small}}}|$ and $|G_{B_{\text{big}}}|$ over training, and tune the window size so that the estimates for $|\mathcal G|^2$ and $\mathcal S$ are stable.

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{figures/bs_vs_critical.png}
    \caption{$\tilde B_{\text{final}}$ vs.\ $\tilde B_{\text{crit}}$, grouped by task and UTD.}
    \label{fig:critical-batch-size-corr}
\end{figure}

We show the values of $\tilde B_{\text{crit}}$ over training in \cref{fig:critical-batch-size}. Unlike policy gradient methods, we find that the critical batch size (averaged over training) has little correlation with the optimal batch size, as shown in \cref{fig:critical-batch-size-corr}.








\begin{table}[h]
    \centering
    \caption{Batch size values predicted by the proposed model on DMC.}
    \label{tab:tasks_utds1_dmc}
    \begin{tabular}{lcccccc}
        \toprule
        \textbf{Task} & $\sigma = 0.25$ & $\sigma = 0.5$ & $\sigma = 1$ & $\sigma = 2$ & $\sigma = 4$ & $\sigma = 8$ \\
        \midrule
        cartpole-swingup & 1040 & 752 & 544 & 384 & 288 & 208 \\
        cheetah-run      & 1088 & 784 & 560 & 400 & 288 & 208 \\
        dog-stand        & 240 & 176 & 128 & 96 & 64 & 48 \\
        finger-spin      & 1168 & 848 & 608 & 432 & 320 & 224 \\
        humanoid-stand   & 864 & 624 & 448 & 320 & 240 & 176 \\
        quadruped-walk   & 1008 & 736 & 528 & 384 & 272 & 192 \\
        walker-walk      & 608 & 432 & 320 & 224 & 160 & 112 \\
        \bottomrule
    \end{tabular}
\end{table}
    
\begin{table}[h]
    \centering
    \caption{Learning rate values predicted by the proposed model on DMC.}
    \label{tab:tasks_utds2_dmc}
    \begin{tabular}{lcccccc}
        \toprule
        \textbf{Task} & $\sigma = 0.25$ & $\sigma = 0.5$ & $\sigma = 1$ & $\sigma = 2$ & $\sigma = 4$ & $\sigma = 8$ \\
        \midrule
        cartpole-swingup & .00108 & .000902 & .000755 & .000631 & .000528 & .000442 \\
        cheetah-run      & .000893 & .000747 & .000625 & .000523 & .000438 & .000366 \\
        dog-stand        & .000664 & .000555 & .000465 & .000389 & .000325 & .000272 \\
        finger-spin      & .00125 & .00105 & .000877 & .000734 & .000614 & .000514 \\
        humanoid-stand   & .000551 & .000461 & .000386 & .000323 & .00027 & .000226 \\
        quadruped-walk   & .00121 & .00101 & .000846 & .000708 & .000592 & .000496 \\
        walker-walk      & .00134 & .00112 & .000938 & .000785 & .000657 & .000549 \\
        \bottomrule
    \end{tabular}
\end{table}


\begin{table}[h]
    \centering
    \caption{Batch size values predicted by the proposed model on OpenAI Gym.}
    \label{tab:tasks_utds1_gym}
    \begin{tabular}{lccccccc}
        \toprule
        \textbf{Task} & $\sigma = 0.25$ & $\sigma = 0.5$ & $\sigma = 1$ & $\sigma = 2$ & $\sigma = 4$ & $\sigma = 8$ & $\sigma = 16$ \\
        \midrule
        Ant-v4         &    704 &    560 &    448 &    352 &    288 &    224 &    176 \\
        HalfCheetah-v4 &    672 &    528 &    416 &    336 &    256 &    208 &    160 \\
        Humanoid-v4    &    560 &    432 &    352 &    272 &    224 &    176 &    144 \\
        Walker2d-v4    &    640 &    496 &    400 &    320 &    256 &    192 &    160 \\
        \bottomrule
    \end{tabular}
\end{table}
    
\begin{table}[h]
    \centering
    \caption{Learning rate values predicted by the proposed model on OpenAI Gym.}
    \label{tab:tasks_utds2_gym}
    \begin{tabular}{lccccccc}
        \toprule
        \textbf{Task} & $\sigma = 0.25$ & $\sigma = 0.5$ & $\sigma = 1$ & $\sigma = 2$ & $\sigma = 4$ & $\sigma = 8$ & $\sigma = 16$ \\
        \midrule
        Ant-v4         &  .000206 &  .000167 &  .000138 &  .000109 &  .000087 &  .000070 &  .000060 \\
        HalfCheetah-v4 &  .002820 &  .002280 &  .001900 &  .001510 &  .001210 &  .000972 &  .000827 \\
        Humanoid-v4    &  .000251 &  .000203 &  .000169 &  .000134 &  .000107 &  .000086 &  .000073 \\
        Walker2d-v4    &  .001180 &  .000958 &  .000806 &  .000640 &  .000512 &  .000412 &  .000347 \\
        \bottomrule
    \end{tabular}
\end{table}


\end{document}
