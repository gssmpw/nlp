\section{Related Work}
\vspace{-0.1cm}
\textbf{Scaling laws and predictability.} Prior work has studied scaling laws in the context of supervised learning~\citep{kaplan2020scaling,hoffmann2022training}, primarily to predict the effect of model size and training data on validation loss, while marginalizing out hyperparameters like batch size~\citep{mccandlish2018empirical} and learning rate~\citep{kaplan2020scaling}. There are several extensions of such scaling laws for language models, such as laws for settings with data repetition~\citep{muennighoff2023scaling} or mixture-of-experts~\citep{ludziejewskiscaling}, but most focus on cross-entropy loss, with an exception of \citet{gadre2024language}, which focuses on downstream metrics. While scaling laws have guided supervised learning experiments, little work explores this for RL. The closest works are: \citet{hilton2023scaling} which fits power laws for on-policy RL methods using model size and the number of environment steps; \citet{jones2021scalingscalinglawsboard} which studies the scaling of AlphaZero on board games of increasing complexity; and \citet{gao2023scaling} which studies reward model overoptimization in RLHF. In contrast, we are the first ones to study off-policy value-based RL methods that are trained via TD-learning. Not only do off-policy methods exhibit training dynamics distinct from supervised learning and on-policy methods~\citep{kumar2021dr3,lyle2023understanding}, but we show that this distinction also results in a different functional form for scaling law altogether. We also note that while \citet{hilton2023scaling} use minimal compute, i.e., $\mathcal{C}_J$ in our notation as a metric of performance, our analysis goes further in several respects: \textbf{(1)} we also study the tradeoff between data and compute (\cref{fig:main_results}), \textbf{(2)} we can predict the algorithm configuration for best performance (\cref{prob:general_prob}); \textbf{(3)} we study many budget functions ($\mathcal{C} + \delta \cdot \mathcal{D}$ can be any affine function).


\textbf{Methods for large-scale deep RL.} Recent work has scaled deep RL across three axes: model size~\citep{kumar2023offline, schwarzer2023bigger, nauman2024bigger}, data~\citep{kumar2023offline, gallici2024simplifying, singla2024sapg}, and UTD~\citep{chen2020randomized, doro2022sample}. Na\"ive scaling of model size or UTD often degrades performance or causes divergence~\citep{nikishin2022primacy, schwarzer2023bigger}, mitigated by classification losses~\citep{kumar2023offline}, layer normalization~\citep{Nauman2024overestimation}, or feature normalization~\citep{kumar2021dr3}. In our work, we use scaled network architectures from \citet{nauman2024bigger} (\cref{sec:implementation_details}). In on-policy RL, prior works focus on effective learning from parallelized data streams in a simulator or a world model~\citep{mnih2016asynchronous, silver2016mastering, schrittwieser2020mastering}. Follow-up works like IMPALA~\citep{espeholt2018impala} and SAPG~\citep{singla2024sapg} use a centralized learner that collects experience from distributed workers with importance sampling updates. These works differ substantially from our study as we focus exclusively on value-based off-policy RL algorithms that use TD-learning and not on-policy methods. In value-based RL, prior work on data scaling focuses on offline~\citep{yu2022leverage, kumar2023offline, park2024value} and multi-task RL~\citep{hafner2023mastering}. In contrast, we study online RL and fit scaling laws to answer resource optimization questions. 


\vspace{-0.2cm}