





















\documentclass[11pt, letterpaper, shortlabels]{archer}


\usepackage{microtype}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{pgf}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{xcolor}         %
\usepackage{color}         %
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{resizegather}
\usepackage{hyperref}
\usepackage{color-edits}
\addauthor{yifei}{blue}
\title{\ourmethodnospace~(\ouracronymnospace): \\Digi-Q}




\usepackage{algorithm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\ifx\assumption\undefined
\newtheorem{assumption}{Assumption}
\fi

\usepackage[capitalize,noabbrev]{cleveref}


\usepackage[textsize=tiny]{todonotes}
\usepackage{wrapfig}
\captionsetup[figure]{font=small,skip=0pt}
\setlength{\belowcaptionskip}{0pt}








\newcommand{\yifei}[1]{{\textcolor{blue}{[Yifei: #1]}}}
\newcommand{\qianlan}[1]{{\textcolor{green}{[Qianlan: #1]}}}
\usepackage{multirow}
\newcommand{\ourmethod}{Digi-Q}
\newcommand{\ourmethodnospace}{Proposer-Agent-Evaluator}
\newcommand{\ouracronym}{PAE }
\newcommand{\ouracronymnospace}{PAE}
\newcommand{\argmax}{\arg \max}

\usepackage[all]{hypcap}

\usepackage[authoryear, round]{natbib}

\usepackage{hyperref}[citecolor=magenta,linkcolor=magenta]

\hypersetup{
    colorlinks = true,
    citecolor = {magenta},
}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} %
\usepackage{float}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{nicefrac}
\usepackage{dsfont}
\usepackage{enumitem}
\usepackage{float}
\usepackage{enumitem}
\usepackage{comment}
\usepackage{etoolbox}
\usepackage{ifthen}
\usepackage{mathrsfs}
\usepackage{upquote}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{arydshln}
\usepackage{longtable}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{amsmath}
\usepackage{dsfont}
\usepackage{multirow}
\usepackage{mdframed}
\usepackage{xcolor}
\usepackage{blindtext}
\usepackage{setspace}
\usepackage{xcolor,colortbl}
\definecolor{Gray}{gray}{0.90}
\definecolor{LightCyan}{rgb}{0.88,1,1}
\usepackage{multirow}
\usepackage{wrapfig}

\setlength\parindent{0pt}


\usepackage{xspace}
\usepackage[capitalize,noabbrev]{cleveref}
\bibliographystyle{plainnat}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{lipsum}
\usepackage{listings}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{bbm}

\usepackage{algpseudocode}
\usepackage{setspace}

\usepackage{color}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}


\newcommand\pythonstyle{\lstset{
basicstyle=\ttfamily\footnotesize,
language=Python,
morekeywords={self, clip, exp, mse_loss, uniform_sample, concatenate, logsumexp},              %
keywordstyle=\color{deepblue},
emph={MyClass,__init__},          %
emphstyle=\color{deepred},    %
stringstyle=\color{deepgreen},
frame=single,                         %
showstringspaces=false
}}

\lstnewenvironment{python}[1][]
{
\pythonstyle
\lstset{#1}
}
{}

\DeclareRobustCommand{\StartCrate}{%
  \begingroup\normalfont
  \raisebox{-0.3ex}{\smash{\includegraphics[height=2.0\fontcharht\font`\B]{figures/icon.png}}}%
  \endgroup
}

\newcommand\pythonexternal[2][]{{
\pythonstyle
\lstinputlisting[#1]{#2}}}

\newcommand\pythoninline[1]{{\pythonstyle\lstinline!#1!}}

\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\EE}{\mathbb{E}}


\makeatletter
\def\mathcolor#1#{\@mathcolor{#1}}
\def\@mathcolor#1#2#3{%
  \protect\leavevmode
  \begingroup
    \color#1{#2}#3%
  \endgroup
}
\makeatother



\Crefformat{equation}{#2Eq.\;(#1)#3}

\Crefformat{figure}{#2Figure #1#3}
\Crefformat{assumption}{#2Assumption #1#3}
\Crefname{assumption}{Assumption}{Assumptions}

\usepackage{crossreftools}
\pdfstringdefDisableCommands{%
    \let\Cref\crtCref
    \let\cref\crtcref
}
\newcommand{\creftitle}[1]{\crtcref{#1}}

\usepackage{dsfont}
\usepackage{nicefrac}

\author[1,2\textbf{*}]{Hao Bai}
\author[1\textbf{*}]{Yifei Zhou}
\author[3]{Erran Li}
\author[1]{Sergey Levine}
\author[4]{Aviral Kumar}

\affil[*]{Equal contributions}
\affil[1]{UC Berkeley}
\affil[2]{UIUC}
\affil[3]{Amazon}
\affil[4]{CMU}

\correspondingauthor{haob2@illinois.edu, yifei\_zhou@berkeley.edu, aviralku@andrew.cmu.edu}



\title{\StartCrate{} Digi-Q: Learning VLM Q-Value Functions for Training Device-Control Agents}



\begin{abstract}
\textbf{Abstract:} While most paradigms for building foundation model agents rely on prompting or fine-tuning on demonstrations, it is not sufficient in dynamic environments (e.g., mobile device control). On-policy reinforcement learning (RL) should address these limitations, but collecting actual rollouts in an environment is often undesirable when using truly open-ended agentic tasks such as mobile device, where simulation is a bottleneck. In such scenarios, an offline method for policy improvement that utilizes a trained value-function for training the policy is much more practical. In this paper, we develop a scalable value-based offline RL approach called \ourmethod{} to train VLM agents for device control entirely from static data. The key idea in \ourmethod{} is to train a value function using offline temporal-difference (TD) learning. We show that this can be done by fine-tuning a Q-function on top of frozen, intermediate-layer features of a VLM rather than fine-tuning the whole VLM itself, which saves us compute and enhances training stability. To make the VLM features amenable for representing the value function, we need to employ an initial phase of fine-tuning to amplify coverage over actionable information critical for Q-functions. Once trained, we use this value function alongside a best-of-N policy extraction operator that imitates the best action out of multiple candidate actions from the current policy as ranked by the value function, enabling policy improvement without ever needing to use the simulator. \ourmethod{} outperforms several prior methods on  user-scale device control tasks in Android-in-the-Wild, attaining 9.9\% improvement 
over prior best-performing method.

\end{abstract}

\begin{document}

\maketitle

 \input{sections/1_introduction}
 \input{sections/2_related_works}
 \input{sections/3_method}
 \input{sections/4_experiments}
 \input{sections/5_conclusion}
 \input{sections/ack}






\bibliography{neurips2024}

\newpage

\appendix
\onecolumn
\part*{Appendices}

\input{sections/app_algorithm_box}
\input{sections/app_exp_details}
\input{sections/app_qualitative_examples}
\input{sections/app_prompts}
\input{sections/app_hyperparameters}

\end{document}
