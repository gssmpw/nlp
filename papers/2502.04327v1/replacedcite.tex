\section{Related Work}
\vspace{-0.1cm}
\textbf{Scaling laws and predictability.} Prior work has studied scaling laws in the context of supervised learning____, primarily to predict the effect of model size and training data on validation loss, while marginalizing out hyperparameters like batch size____ and learning rate____. There are several extensions of such scaling laws for language models, such as laws for settings with data repetition____ or mixture-of-experts____, but most focus on cross-entropy loss, with an exception of ____, which focuses on downstream metrics. While scaling laws have guided supervised learning experiments, little work explores this for RL. The closest works are: ____ which fits power laws for on-policy RL methods using model size and the number of environment steps; ____ which studies the scaling of AlphaZero on board games of increasing complexity; and ____ which studies reward model overoptimization in RLHF. In contrast, we are the first ones to study off-policy value-based RL methods that are trained via TD-learning. Not only do off-policy methods exhibit training dynamics distinct from supervised learning and on-policy methods____, but we show that this distinction also results in a different functional form for scaling law altogether. We also note that while ____ use minimal compute, i.e., $\mathcal{C}_J$ in our notation as a metric of performance, our analysis goes further in several respects: \textbf{(1)} we also study the tradeoff between data and compute (\cref{fig:main_results}), \textbf{(2)} we can predict the algorithm configuration for best performance (\cref{prob:general_prob}); \textbf{(3)} we study many budget functions ($\mathcal{C} + \delta \cdot \mathcal{D}$ can be any affine function).


\textbf{Methods for large-scale deep RL.} Recent work has scaled deep RL across three axes: model size____, data____, and UTD____. Na\"ive scaling of model size or UTD often degrades performance or causes divergence____, mitigated by classification losses____, layer normalization____, or feature normalization____. In our work, we use scaled network architectures from ____ (\cref{sec:implementation_details}). In on-policy RL, prior works focus on effective learning from parallelized data streams in a simulator or a world model____. Follow-up works like IMPALA____ and SAPG____ use a centralized learner that collects experience from distributed workers with importance sampling updates. These works differ substantially from our study as we focus exclusively on value-based off-policy RL algorithms that use TD-learning and not on-policy methods. In value-based RL, prior work on data scaling focuses on offline____ and multi-task RL____. In contrast, we study online RL and fit scaling laws to answer resource optimization questions. 


\vspace{-0.2cm}