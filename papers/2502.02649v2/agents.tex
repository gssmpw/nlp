%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{graphicx} % For including emojis
\usepackage{array}    % For custom column formatting
\usepackage{pifont}
\usepackage[hang,flushmargin]{footmisc} 
\usepackage{mdframed}
\usepackage[hyphens]{url}
\usepackage{hyperref}
\usepackage[hyphenbreaks]{breakurl}
\usepackage{mathabx}
\usepackage{longtable}


\usepackage{fontawesome5} % For icons
% \usepackage{caption} % Caption styling

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{enumitem}

% Required packages in preamble:
\usepackage[table]{xcolor}
\usepackage{colortbl}



% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025/icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025/icml2025}

% For theorems and such
%\usepackage{amsmath}
%\usepackage{amssymb}
%\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\setlength{\marginparwidth}{1.5cm}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Fully Autonomous AI Agents Should Not be Developed}

\begin{document}

\twocolumn[
\icmltitle{Fully Autonomous AI Agents Should Not be Developed}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}



%From another style:
%\author{Margaret Mitchell}
%\authornote{Both authors contributed equally to this research.}
%\email{meg@huggingface.co}
%\orcid{1234-5678-9012}
%\author{Avijit Ghosh}
%\authornotemark[1]
%\email{avijit@huggingface.co}
%\author{Giada Pistilli}
%\email{giada@huggingface.co}
%\author{Alexandra Sasha Luccioni}
%\email{sasha.luccioni@huggingface.co}

\begin{icmlauthorlist}
\icmlauthor{Margaret Mitchell}{}
\icmlauthor{Avijit Ghosh}{}
\icmlauthor{Alexandra Sasha Luccioni}{}
\icmlauthor{Giada Pistilli}{}
\end{icmlauthorlist}

% \icmlaffiliation{hf}{Hugging Face}
%\icmlaffiliation{comp}{Company Name, Location, Country}
%\icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

%\icmlcorrespondingauthor{Margaret Mitchell}{meg@huggingface.co}
%\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.2in % MEG CHANGED THIS CHANGE IT BACK TO 0.3 IF ACCEPTED
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printNotice{} % removed the anonymous footnote
% \printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

%% From ICML:
%% The following additional requirements apply to position papers:

%%The Title should state the position and start with “Position:”.
%%These hypothetical paper titles do state a position:
%%"Position: Quantum Atelic Learning Methods Should Employ Psychic Insights"
%%"Position: Stop Research on Psychic Properties of Machine Learning"
%%while these versions do not:
%%"Position: Psychic Quantum Atelic Learning"
%%"Position: A Perspective on Psychic Quantum Atelic Learning"
%%The Abstract must identify the paper as a position paper and briefly state the position (e.g., “This position paper argues that <statement of the position>.”)
%%The Introduction must state the position, using bold text.
%%The paper must include an “Alternative Views” section that describes and addresses one or more viable (not strawmen) positions that are opposed to the paper’s position.
%%Papers that describe new research without advocating a position are not responsive to this call and should instead be submitted to the main paper track.


%Abstracts must be a single paragraph, ideally between 4--6 sentences long.
%Gross violations will trigger corrections at the camera-ready phase.

\begin{abstract}
This paper argues that \textbf{fully autonomous AI agents should not be developed}. In support of this position, we build from prior scientific literature and current product marketing to delineate different AI agent levels and detail the ethical values at play in each, documenting trade-offs in potential benefits and risks. Our analysis reveals that risks to people increase with the  autonomy of a system: The more control a user cedes to an AI agent, the more risks to people arise. Particularly concerning are safety risks, which affect human life and impact further values.% risks that arise from the same benefits that motivate AI agent development, such as freeing developers from having to design all system actions. 
\end{abstract}


\vspace{-2em}
\section{Introduction}

The sudden, rapid advancement of Large Language Model (LLM) capabilities--from writing fluent sentences to achieving increasingly high accuracy on benchmark datasets--has led AI developers and businesses alike to look towards what comes next. 
%: What game-changing technology is just on the horizon? 
The tail end of 2024 saw “AI agents”, autonomous goal-directed systems, begin to be marketed and deployed as the next big advancement in AI technology. 

Many recent AI agents are constructed by integrating LLMs into larger, multi-functional systems, capable of carrying out a variety of tasks. A foundational premise of this emerging paradigm is that computer programs need not be restricted to functioning as human-controlled tools designed for specific tasks; rather, systems now have the capacity to autonomously combine and execute multiple tasks without human intervention. This transition marks a fundamental shift towards systems capable of creating context-specific plans in non-deterministic environments. Many modern AI agents do not merely perform pre-defined actions, but are designed to analyze novel situations  %develop relevant goals, 
and take previously undefined actions to achieve goals.%dynamically-defined objectives. 

To better understand the potential benefits and risks in current AI agent development, we review recent AI agent products\footnote{Provided in footnotes throughout this work.} alongside research on AI agents to document different potential benefits and risks aligned to human values. Our analysis reveals that risks to people increase with a system’s level of autonomy: the more control a user cedes, the more risks arise from the system. As others \cite{chan2023harms} have previously noted, there is an urgent need to anticipate and address risks of increasing agency, and we do this via a value-based characterization. Particularly concerning are risks related to the value of \textbf{safety} for individuals (\cref{value-safety}), %, which arise from some of the same benefits that motivate AI agent development: freeing developers from predefining all actions a system may take, and users from needing multiple different applications. Compounding the issue, some safety harms 
which include loss of human life and open the door for privacy risks (\cref{value-privacy}) and security risks (\cref{value-security}). Compounding the issue is misplaced trust (\cref{value-trust}) in unsafe systems, which enables a snowball effect of yet further harms. For example, the safety issue of ``hijacking'', wherein an agent is instructed by a malicious third party to exfiltrate confidential information, can create further harms as that information is used to compromise the user's public reputation or financial stability  and to identify additional people as targets of attack \cite{NIST2025}.

Given these risks, we argue that developing \emph{fully autonomous} AI agents--systems capable of writing and executing their own code beyond predefined constraints--should be avoided. Complete freedom for code creation and execution enables the potential to override human control, realizing some of the worst harms described in \cref{sec:values}. In contrast, \emph{semi-autonomous} systems, which retain some level of human control, offer a more favorable risk-benefit profile, depending on the degree of autonomy, the complexity of tasks assigned, and the nature of human involvement. % in decision-making.


%Throughout this piece, we use some anthropomorphizing language to describe AI agents, consistent with the language that is currently used to describe them. As was also noted in historic scholarship \cite{WooldridgeJennings1995}, describing AI agents using mentalistic language ordinarily applied to humans--such as having knowledge, beliefs, and intentions--can be an issue for appropriately informing users about system abilities. This language serves as an abstraction tool to gloss over precise details of the technology. Understanding this is critical when grappling with the implications of what these systems are and the role they may play in peoples’ lives: While we use mentalistic language, we do not propose that these systems have a mind. (See \cref{sec:agency} for further discussion.)

%\begin{table*}[h]
%\centering
%\renewcommand{\arraystretch}{1.5}
%\small % Reduce font size slightly
%\begin{tabular}{|>%{\centering\arrayba%ckslash}m{2cm}|>{\raggedright\arraybackslash}m{4.5cm}|>{\centering\arraybackslash}m{2.6cm}|>{\centering\arraybackslash}m{2.5cm}|>{\ttfamily\raggedright\arraybackslash}m{5.7cm}|}
%\hline
%\textbf{Agentic Level} & \textbf{Description} & \textbf{Control} & \textbf{Pattern} & \textnormal{\textbf{Example Code}}\\ 
%\hline
%\ding{73}\ding{73}\ding{73}\ding{73} & Model has no impact on program flow & Full human control & Simple processor & 
%print\_llm\_output(llm\_response) %\\
%\hline
%\ding{72}\ding{73}\ding{73}\ding{73} & Model determines basic control flow & Human capabilities, system timing & Routing & 
%if llm\_decision(): path\_a() else: path\_b() \\
%\hline
%\ding{72}\ding{72}\ding{73}\ding{73} & Model determines execution details & Human supervision & Tool calling & 
%run\_function(llm\_tool, llm\_args) \\
%\hline
%\ding{72}\ding{72}\ding{72}\ding{73} & Model controls iteration flow & Human goals, system process & Multi-step & 
%while llm\_continue(): execute\_step() \\
%\hline
%\ding{72}\ding{72}\ding{72}\ding{72} & Model creates \& executes code & Full autonomy & Autonomous agent & 
%create\_code(user\_request); %execute() \\
%\hline
%\end{tabular}
%\caption{Agentic Levels of Model Control}
%\label{table:agentic-levels}
%\end{table*}

\section{Background}
\subsection{A Brief History of Artificial Agents}

The idea of humans being assisted by artificial autonomous systems can be found throughout human history. Ancient mythology describes Cadmus (ca.~2000 BCE), who sowed dragon teeth that turned into soldiers. Aristotle speculated that automata could replace human slavery%\footnote{The word "robot" derives from Czech robota (forced labor), first introduced in Karel Čapek's 1920 play "R.U.R." and adopted into English in 1923.}
: ``There is only one condition in which we can imagine managers not needing subordinates, and masters not needing slaves. This condition would be that \textit{each instrument could do its own work, at the word of command or by intelligent anticipation}'' \cite{aristotle322politics}. An early precursor to artificial agents was created by Ktesibios of Alexandria (ca.~250 BCE), who invented a water clock that used a regulator to maintain a constant flow rate. This demonstrated that it was possible to create a system that could modify its own behavior in response to changes in its environment, previously believed to be limited to living things \cite{RussellNorvig2020}.

More recently, writing on autonomous systems in the form of robotic automata has highlighted the kinds of risks we discuss here. Famously, \citet{AsimovRobot} provided the Three Laws of Robotics (\cref{fig:asimov}):
\vspace{-.5em}
\begin{figure}[h!]
\vspace{-0.5em}
\begin{mdframed}
\small
\begin{enumerate}[wide, labelwidth=!, labelindent=0pt, noitemsep]
\item A robot may not injure a human being or, through inaction, allow a human being to come to harm.
\item A robot must obey the orders given it by human beings except where such orders would conflict with the First Law.
\item A robot must protect its own existence as long as such protection does not conflict with the First or Second Law.
\end{enumerate}
\end{mdframed}
\vspace{-1.5em}
\caption{Isaac Asimov's Three Laws of Robotics. }\label{fig:asimov}
\end{figure}

\vspace{-1em}

The way such concepts concretely translated to computer software remained elusive until the late 1900s, when advances in hardware and computer functionality catalyzed excitement on %brought consideration of artificially intelligent agents to the fore. By the 1990s, 
AI agents as an imminent %lucrative and powerful 
breakthrough \cite{Sargent92,guilfoyle1994intelligent,WooldridgeJennings1995,nwana1999perspective}. A  particular form of AI agent became extensively used in reinforcement learning because it enabled the implementation of separate goals and objective functions for independent actors within the same action space~\cite{tan1993multi,littman1994markov}, which allowed the development of new methods and approaches for exploration and optimization~\cite{busoniu2008comprehensive}. 

%This history sheds light on how systems similar to modern-day AI agents have been imagined, conceptualized as beings with potential benefits and risks. Throughout, the idea of a constraint such that humans can control agents and agents \textbf{are not able to hurt people} has persisted. This entails that there is never a system that is able to breach the set guardrail, that is, a system that is never fully outside of human control. As such, the idea that fully autonomous AI agents should not be developed has been implicit for as long as the concept of AI agents has existed.
\vspace{-.5em} % MEG ADDED FOR DISTRO; REMOVE THIS IF ACCEPTED
\subsection{Current Landscape of Agentic Systems}

In the 2020s, work on AI agents broadened the range of functionality that computer systems could provide while requiring less input from users. Newly available systems can now complete tasks %that 
previously requiring human interaction with  multiple different people and programs,  %or assistance from other people, 
e.g., organizing meetings\footnote{E.g., meeting organization agents: \href{https://www.lindy.ai/template-categories/meetings}{Lindy}, \href{https://zapier.com/agents/templates/meeting-prep-assistant}{Zapier}, \href{https://www.ninjatech.ai/product/ai-scheduling-agent}{NinjaTech}, \href{https://attri.ai/ai-agents/scheduling-agent}{Attri}} or creating personalized social media posts.\footnote{Example social media creation agent: \href{https://www.hubspot.com/products/marketing/social-media-ai-agent}{HubSpot}}% without requiring user involvement.


In the physical world, autonomous system development has made significant advances in multiple domains. Autonomous vehicles represent one of the more visible\footnote{\href{https://www.theverge.com/news/600542/waymo-test-cities-las-vegas-san-diego-2025}{Waymo to test in 10 new cities...-The Verge}} applications, with systems capable of perceiving\footnote{Distinct from human perception, e.g., with sensors.} their environment and navigating without human input \cite{van2018autonomous}. These range from consumer vehicles with varying levels of autonomy to fully autonomous systems tested in controlled environments \cite{ballingall2020safety}. The development of autonomous robots has similarly expanded, from industrial manufacturing \cite{muller2021industrial} to healthcare applications \cite{haidegger2019autonomy}, with systems capable of increasingly complex physical interactions and decision-making. AI models (such as state-of-the-art LLMs) are now being integrated into robotic systems,\footnote{\href{https://x.com/RemiCadene/status/1884308281025519769}{Open source robotics with reasoning- Tweet}; \href{https://deepmind.google/discover/blog/shaping-the-future-of-advanced-robotics/}{using LLMs to train and control robots--Deepmind Announcement}} bringing classic robotics into the fold of the agentic AI landscape.

Perhaps most controversially, autonomous weapons systems have emerged as a critical area of development.\footnote{\href{https://www.theguardian.com/technology/article/2024/jul/14/ais-oppenheimer-moment-autonomous-weapons-enter-the-battlefield}{AI’s ‘Oppenheimer moment’...--The Guardian}} %and ethical concern \cite{ieee2020ethical}. 
These systems, capable of %selecting and 
engaging targets without meaningful human control, raise significant ethical questions about accountability, moral responsibility \cite{chavannes2020governing}, and safety %considerations 
that extend beyond those of purely digital agents \cite{ieee2020ethical}. Harms due to misalignment \cite{kierans2024quantifying} %of these agents 
with human goals may be further compounded with full autonomy, where all human control is ceded. 

%Building from many other proposed definitions (see \cref{app:definitions}) \todo{Table of example definitions} 


\begin{table*}[t]
\centering
\renewcommand{\arraystretch}{1.1}
\footnotesize % Reduce font size slightly
\begin{tabular}{|>{\centering\arraybackslash}m{1.9cm}|>{\raggedright\arraybackslash}m{3cm}|>{\centering\arraybackslash}m{2.25cm}|>{\ttfamily\raggedright\arraybackslash}m{5cm}|>{\raggedright\arraybackslash}m{2.5cm}|}
\hline
\textbf{Agentic Level} & \textbf{Description} & \textbf{Term} & \textnormal{\textbf{Example Code}} & \textbf{Who's in Control?} \\ 
\hline
\ding{73}\ding{73}\ding{73}\ding{73} & Model has no impact on program flow & Simple processor & \scriptsize{print\_llm\_output(llm\_response)} & \scriptsize{\faUser} \footnotesize Human\\
\hline
\ding{72}\ding{73}\ding{73}\ding{73} & Model determines basic program flow & Router & 
\scriptsize{if llm\_decision(): path\_a() else: path\_b()} &  \scriptsize{\faUser} \footnotesize Human: How functions are done;   \scriptsize{\faRobot} \footnotesize System: When \\
\hline
\ding{72}\ding{72}\ding{73}\ding{73} & Model determines how functions are executed & Tool call & 
\scriptsize{run\_function(llm\_chosen\_tool, llm\_chosen\_args)} & \scriptsize{\faUser} \footnotesize Human: What functions are done;    \scriptsize{\faRobot} \footnotesize  System: How\\
\hline
\ding{72}\ding{72}\ding{72}\ding{73} & Model controls iteration and program continuation & Multi-step agent & \scriptsize{while should\_continue(): execute\_next\_step()} & \scriptsize{\faUser} \footnotesize Human: What functions exist;    \hspace{1em}\scriptsize{\faRobot} \footnotesize System: Which to do, when, how\\
\hline
\ding{72}\ding{72}\ding{72}\ding{72} & Model creates \& executes new code & Fully autonomous agent &
\scriptsize{create\_code(user\_request); execute()} &    \scriptsize{\faRobot} \footnotesize System \\
\hline
\end{tabular}
\vspace{-.5em}
\caption{Levels of AI Agent: Systems using machine-learned models can have different levels of agency. They can also be combined in ``multiagent systems," where one agent workflow triggers another, or multiple agents work collectively toward a goal. \\Levels adapted from \cite{smolagentsblog}.}
\label{table:agentic-levels}
\end{table*}

\vspace{-.5em} 
\section{Definitions}

\subsection{On AI Agents}
Analyzing potential benefits and risks of AI agents %that AI agents may bring %to society and the environment 
requires understanding what an AI agent is, yet definitions and descriptions vary greatly. Within AI, the term “agent” is currently used for everything from single-step prompt-and-response systems\footnote{E.g., \href{https://blogs.microsoft.com/blog/2024/10/21/new-autonomous-agents-scale-your-team-like-never-before/}{New autonomous agents scale your team... - Microsoft}} to multi-step customer support systems.\footnote{E.g., as described by Lindly in their \href{https://www.lindy.ai/solutions/customer-support}{AI Customer Support page.}} 

To better understand what an AI agent is, we therefore review currently available AI agents and AI agent platforms (examples provided in footnotes throughout this document) as well as historical literature on the promise of AI agents (references throughout), %(see \cref{app:definitions} for a detailed breakdown of sources), 
and note the different functionalities described (\cref{tab:agent-functionalities}). %; \cref{app:agent-functionalities} for further detail). 
Definitions across these sources differ in terms of who and what is centered (the person, the computer system, or the workflow), the specificity and clarity of the language used, %(e.g., whether terminology that requires a further definition for scientific writing is used, like the term ``intelligent'') 
and the types of systems that the definition pinpoints (e.g., whether it distinguishes autonomous systems from automatic systems). %For instance, when materials describe agents as something that uses artificial intelligence" \cite{GeminiResponseTohttps://www.google.com/search?q="what+is+an+ai+agent",https://www.datadoghq.com/knowledge-center/aiops/ai-agents/}, they leave ambiguous what ``artificial intelligence'' refers to and the scope of technology included and excluded in the definition. %: whether simple prompt-response systems or more complex autonomous decision-making capabilities. \todo{Add example of how some definitions fail to distinguish from basic automated systems?} 
Towards the goal of harmonizing these different perspectives for this research, we propose the following definition of ``AI agent'': 

\textbf{Computer software systems capable of creating context-specific plans in non-deterministic environments.}

Although there is not full consensus on what an “AI agent” is, a commonality across recently introduced AI agents is that they  %are “agentic”, that is, they 
act with some level of \textit{autonomy}: given a goal, they can decompose it into subtasks and execute each one of them without direct human intervention. For example, an ideal AI agent could respond to a high-level request such as “help me write a great ICML paper about AI agents” by independently breaking this task down into: retrieving highly-cited ICML papers; retrieving  information about AI agents from the internet; and creating an outline %document with an outline 
informed by the content it retrieved.%; and providing initial writing within each.%in a writing style similar to the writing style of the requester and other top ICML papers.
\footnote{No AI agents were used in the creation of this paper.} 

Recently introduced AI agents are built on ML models, many specifically using LLMs to drive their actions, which is a new %, novel 
approach for computer software execution. Aside from this difference, today’s AI agents share similarities with those in the past and, in some cases, realize previous %theoretical 
ideas of what agents might be like \cite{WooldridgeJennings1995}: acting with autonomy, demonstrating  social ability, and appropriately balancing reactive and proactive actions. 





\subsection{On Agency}\label{sec:agency}

The concept of ``agency" is central to debates about autonomous AI systems, yet its meaning and implications remain philosophically contested. In general terms, an \textit{agent} is understood as an entity with capacity to act \cite{anscombe1957, davidson1963}. Applying this concept to artificial systems raises questions about the nature of those acts' \textit{intentionality}: the philosophical literature commonly understands agency through the lens of intentional action, where actions are explained in terms of the agent's mental states (e.g., beliefs, desires) and their capacity to act for reasons
\cite{davidson1963, goldman1970}, but artificially intelligent agents are not known to have mental states as historically discussed. This suggests that AI agents lack the fundamental characteristics of genuine agency \cite{frankfurt1971, bratman1987, velleman1992}. %This position stems from these systems' inability to engage in genuine practical reasoning, and the lack of intentionality that philosophical accounts have identified as necessary for genuine agency.
%In this context, the philosophical literature offers multiple perspectives on agency, from those that emphasize conscious intentionality and the initiation of action by the agent \cite{bratman1987, frankfurt1971} to broader conceptions that recognize different forms and degrees of agency across various types of systems \cite{barandiaran2009}. 
%In this position paper, we analyze different levels of AI system autonomy to examine how increasing this autonomy corresponds with decreasing human oversight. By identifying these levels and their associated risks and benefits, we build an argument against fully autonomous agents: as systems move towards greater autonomy, we observe a corresponding increase in potential risks to human welfare, 
As such, philosophical foundations supporting the development of ``agency'' in AI agents--and indeed, whether AI agents may be said to have ``agency'' at all--remain questionable. This has two primary ramifications for this work: (1) the increased risk we note with increasing agentic levels is not counter-balanced by common philosophical underpinnings that might motivate the benefits of agency; (2) we contextualize AI agents using the concept of ``autonomy'', and center the concept of ``agent'', rather than agency, in recognizing \textit{agentic} levels. %The standard theory of agency \cite{davidson1963, goldman1970} helps identify capabilities that AI agents currently lack--such as genuine practical reasoning, intentional action, and mental states (e.g., beliefs, desires)--which are traditionally considered necessary for autonomous decision-making. This gap between philosophical requirements for agency and current AI capabilities reinforces our argument for maintaining human oversight.

\section{AI Agent Levels}\label{sec:agentic-levels}

%\subsection{Agentic Levels} 
AI agents may be said to be more or less autonomous (or agentic), and the extent to which something is an agent may be understood on a sliding scale. Most writing on AI agents do not make such distinctions, which %, and the lack of clear boundaries between different levels of AI agency 
has contributed to recent confusion in both technical and public discourse about what AI agents are and what they are capable of \cite{Nathan2024}. Addressing this issue, a proposal of different gradations of ``AI agent'' has recently been put forward by multiple researchers (e.g., \citet{kapoor2024ai,Ng2024,Greyling2024,Nathan2024,smolagentsblog}, %,mitchell2025aiagents}), 
although there is not yet consensus on the specifics of each level. Drawing from these ideas and different descriptions of AI agents, we propose a leveled AI agent scale in \cref{table:agentic-levels}. % There are a variety of current and hypothetical interrelated functionalities offered by AI agents, many with gradations, summarized in \cref{tab:agent-functionalities}. 
Levels are one way of categorizing; for a classic categorization with consensus, see Russell and Norvig~\citeyearpar{RussellNorvig1995}.

Our proposed agentic levels correspond to decreasing input from a user and decreasing code written by the agent developers. On the other side of this coin, AI agents can control more of how they operate. This is a critical aspect of the AI agent scale to understand in order to inform how agents might be developed for the better:  The more autonomous the system, the more we \textbf{cede human control}.

%\subsection{Agent Functionalities}


\begin{table}[t!]
\small
\begin{mdframed}
{\setlength{\parskip}{3pt}
%\begin{itemize}
%\item 
%\item 
\textbf{Action surface options:} The spaces (digital or analog) where an agent can operate. 

\textbf{Adaptability:} The extent to which a system can update its actions based on new information or changes in context.

\textbf{Number:} Single-agent or multi-agent, meeting needs of users by working together, in sequence, or in parallel.
 
\textbf{Personalization:} The extent to which an agent uses a user's data to provide user-specific unique content.

\textbf{Personification:} The extent to which an agent is designed to be like a specific person or group of people. 

\textbf{Proactivity:} The amount of goal-directed behavior that a system can take without direct specification from a user. 

\textbf{Reactivity:} Extent to which a system can respond to changes in its environment in a timely fashion.

\textbf{Request format options:} The formats an agent uses for input (e.g., code, natural language).

\textbf{Versatility:} Diversity of possible agent actions, including:
\vspace{-1em}
\begin{itemize}[wide, labelwidth=!, labelindent=2pt]
\setlength\itemsep{-.25em}
\item \textbf{Domain specificity:} How many domains agent can operate in (e.g., email, calendars, news).
\item \textbf{Interoperability}: Extent to which agent can exchange information and services
with other programs.
\item \textbf{Task specificity:} How many types of tasks agent may perform (e.g., scheduling, summarizing). 
\item \textbf{Modality specificity:} How many modalities agent can operate in (e.g., text, speech, video, images, forms, code).
\end{itemize}}
%\end{itemize}
\end{mdframed}
\vspace{-1.25em}
\caption{Different functionalities offered by AI agents. See \cref{app:agent-functionalities} for further functionality details. }\label{tab:agent-functionalities}
\vspace{-1.5em}
\end{table}



\vspace{-.5em}
\section{Values Embedded in Agentic Systems}\label{sec:values}

%In this section, we describe our methodology for identifying and analyzing values relevant to AI agents with respect to risks and benefits (\cref{sec:methodology}), the outcomes from this analysis (\cref{sec:results}), and the value taxonomy we developed in this process (\cref{sec:taxonomy}).

\subsection{Methodology}\label{sec:methodology}
To examine the relationship between AI agent autonomy level and ethical implications, we conducted a systematic analysis of how agents are conceptualized and deployed across different contexts. Our investigation focused %specifically 
on how varying degrees of agent autonomy interact with value propositions in research and commercial implementations.

%Specifically, our methodology
%involved three main steps: 
%Our methodology %for developing the values taxonomy 
%involved three main steps: 
%\vspace{-1em}
Specifically, we:
\vspace{-1em}
\begin{enumerate}[noitemsep]
    \item Collected and categorized statements about what agents are,\footnote{\cref{app:definitions} provides sources with explicit AI agent definitions.} their capabilities, benefits, and harms. This included %content such as 
    industry surveys that captured how professionals across roles envision and use AI agents;\footnote{e.g., \cite{langchain2024state,DeloitteSurvey2024,TraySurvey2024}} case studies of deployed autonomous systems;\footnote{e.g., \cite{sierra2024shipping}} and news articles on the rise of AI agents.\footnote{e.g., \cite{TechCrunchAgents2024,TechReviewAgents2024,VentureBeatAgents2024}}  %, %Through iterative close reading and annotation of these materials, 
%From this we identified recurring  %patterns in how researchers and companies articulate the 
%benefits, risks, and underlying assumptions about different levels of agent autonomy.
    
   % across different autonomy levels, from simple task automation to complex decision-making systems.%\footnote{This was performed via literature reviews and web searches. See Sources \cref{app:definitions} for source providing explicit definitions of ``AI agent''.}
    \item Identified recurring AI agent value propositions. %, noting how these evolved with increased agent autonomy.
    \item Converged on a value taxonomy that had corresponding benefits, risks, or both in the reviewed literature. 
    \item Analyzed the role of values with increased autonomy.
   % \item Cross-referencing these themes with existing ethical frameworks in AI to identify tensions and alignments. 
\end{enumerate}
\vspace{-1em}
The different functionalities and values we present are not intended to be exhaustive, but rather to provide a starting point for ethical deliberation on the potential benefits and risks of AI agents. 
%This process revealed not only explicit value propositions, but implicit assumptions about autonomy and the role of human control.% that support current approaches to AI systems.
Ethical considerations surrounding LLM-based AI agents overlap with those of %foundation models and 
LLMs (see 
\citet{bender2021dangers}). %unlike traditional LLM applications that directly respond to queries, 
Agents additionally navigate ongoing interactions with both users and environments \cite{kierans2024quantifying}, which introduces ethical dimensions %.  %of ethical considerations. % and our analysis additionally reveals challenges specific to agentic systems. 
%These agent-specific considerations concern 
such as the delegation of decision-making authority, the role of human oversight, and potential for emergent behaviors %in autonomous systems 
\cite{emergentbehavior2,emergentbehaviour}. %, and the risks arising from continuous operation without direct human oversight. 
%By mapping these values, benefits, and risks against  %different 
%increased autonomy, we developed the following value taxonomy.  %to understand how ethical considerations evolve as systems become more autonomous.

\subsection{Value taxonomy}\label{sec:taxonomy}

We distinguish three main patterns in how agentic levels impact value preservation:
\vspace{-1em}
\begin{itemize}[noitemsep]
\item \textbf{Inherent risks ($\odot$)}, present at all autonomy levels due to limitations in an AI agent's base model(s).
\item \textbf{Countervailing relationships ($\downuparrows$)}:  Where increasing autonomy creates both risks and opportunities with respect to an ethical value.
\item \textbf{Amplified risks ($\uparrow$)}:  Where increasing autonomy magnifies existing vulnerabilities.
\end{itemize}
\vspace{-1.5em}


\begin{figure}[htb!]
\begin{mdframed}[
  linewidth=0.5pt,
  backgroundcolor=gray!5,
  innertopmargin=4pt, % reduced margin
  innerbottommargin=4pt, % reduced margin
  innerrightmargin=3pt, % slightly reduced margin
  innerleftmargin=3pt, % slightly reduced margin
  roundcorner=100pt,
  shadow=true,
  shadowsize=2pt,
  shadowcolor=gray!20
]
\small

% Safety & Security
\begin{mdframed}[
  linewidth=1pt,
  linecolor=blue!30,
  backgroundcolor=blue!3,
  innertopmargin=4pt,
  innerbottommargin=4pt,
  innerrightmargin=2pt,
  innerleftmargin=5pt,
  roundcorner=4pt,
  skipabove=0pt,
  skipbelow=0pt
]
{\color{blue!70}\normalsize\faShield*} {\normalsize\bfseries\sffamily Safety \& Security}
\vspace{0.2ex} % reduced vertical space

\footnotesize
\begin{tabular}
{@{}p{0.07\textwidth}@{}p{0.9\textwidth}@{}}
{\small\faLock} & Safety ($\uparrow$): Autonomy increases unpredictable actions\\
{\small\faUserShield} & Security ($\uparrow$): Attack surfaces expand with capabilities
\end{tabular}
\end{mdframed}

\vspace{-.5em}

% Truth & Reliability
\begin{mdframed}[
  linewidth=1pt,
  linecolor=green!40,
  backgroundcolor=green!3,
  innertopmargin=4pt,
  innerbottommargin=4pt,
  innerrightmargin=2pt,
  innerleftmargin=5pt,
  roundcorner=4pt,
  skipabove=0pt,
  skipbelow=0pt
]
{\color{green}\normalsize\faBalanceScale} {\normalsize\bfseries\sffamily Truth \& Reliability}
\vspace{0.2ex} % reduced vertical space

\begin{tabular}{@{}p{0.07\textwidth}@{}p{0.9\textwidth}@{}}
{\small\faCheckCircle} & Accuracy ($\odot\uparrow$): Errors compound with complexity\\
{\small\faCompass} & Truthfulness ($\odot\uparrow$): False information propagates\\
{\small\faEquals} & Consistency ($\odot\uparrow$): Variability increases with autonomy
\end{tabular}
\end{mdframed}

\vspace{-.5em}

% User Interaction
\begin{mdframed}[
  linewidth=1pt,
  linecolor=orange!40,
  backgroundcolor=orange!3,
  innertopmargin=4pt,
  innerbottommargin=4pt,
  innerrightmargin=2pt,
  innerleftmargin=5pt,
  roundcorner=4pt,
  skipabove=0pt,
  skipbelow=0pt
]
{\color{orange!70}\normalsize\faUsers} {\normalsize\bfseries\sffamily User Interaction}
\vspace{0.2ex} % reduced vertical space

\begin{tabular}{@{}p{0.07\textwidth}@{}p{.9\textwidth}@{}}
%{\scriptsize\faHandshake} & Trust ($\odot\uparrow$): Validation becomes impossible\\
{\small\faUserSecret} & Privacy ($\uparrow$): Data exposure risks increase\\
{\small\faRobot} & Humanlikeness ($\odot\downuparrows$): Natural interaction vs.~manipulation
\end{tabular}
\end{mdframed}

\vspace{-.5em}

% Task Performance
\begin{mdframed}[
  linewidth=1pt,
  linecolor=red!30,
  backgroundcolor=red!3,
  innertopmargin=4pt,
  innerbottommargin=4pt,
  innerrightmargin=2pt,
  innerleftmargin=5pt,
  roundcorner=4pt,
  skipabove=0pt,
  skipbelow=0pt
]
{\color{red!70}\normalsize\faTasks} {\normalsize\bfseries\sffamily Task Performance}
\vspace{0.2ex} % reduced vertical space

\begin{tabular}{@{}p{0.08\textwidth}@{}p{0.82\textwidth}@{}}
{\small\faHandsHelping} & Assistiveness ($\downuparrows$): Automation benefits vs. control\\
{\small\faRocket} & Efficiency ($\downuparrows$): Speed gains vs. error complexity\\
{\small\faBullseye} & Relevance ($\downuparrows$): Personalization vs. bias risks
\end{tabular}
\end{mdframed}

\vspace{-.5em}

% Broader Impacts
\begin{mdframed}[
  linewidth=1pt,
  linecolor=purple!30,
  backgroundcolor=purple!3,
  innertopmargin=4pt,
  innerbottommargin=4pt,
  innerrightmargin=2pt,
  innerleftmargin=5pt,
  roundcorner=4pt,
  skipabove=0pt,
  skipbelow=0pt
]
{\color{purple!70}\normalsize\faGlobe} {\normalsize\bfseries\sffamily Broader Impacts}
\vspace{0.2ex} % reduced vertical space

\begin{tabular}{@{}p{0.07\textwidth}@{}p{0.9\textwidth}@{}}
{\small\faBalanceScaleRight} & Equity ($\odot\uparrow$): Systemic biases compound\\
{\small\faCogs} & Flexibility ($\uparrow$): Integration risks grow\\
{\small\faLeaf} & Sustainability ($\odot\downuparrows$): Environmental trade-offs
\end{tabular}
\end{mdframed}
\end{mdframed}
\vspace{-1em}
\caption{Summary of \cref{sec:values} benefit-risk analyses as AI agent autonomy levels increase. $\uparrow$: increasing risk with autonomy, $\downuparrows$: countervailing benefit-risk, $\odot$: base model propagates inherent risk. %Table~\ref{tab:detailed-values} in 
See Appendix~\ref{app:value-details} for a more detailed summary.}% of each value across specific autonomy levels.}
\label{fig:value-summary}
% \vspace{-3em}
\end{figure}

\subsubsection{Value: Accuracy}\label{value-accuracy}
The accuracy of an AI agent is modulated by the accuracy of the models it's based on. Accuracy of AI agents influence values such as reliability, utility, consistency, and safety.

\textbf{Benefit: } When a system is accurate in how it responds to user requests and correctly aligns with developer goals, increased autonomy provides increased useful functionality.
\vspace{-1em}

{\bf Risk:} The models on which recent AI agents are based can be inaccurate. The commonly used LLMs are known to produce incorrect information that appears correct. 


\textbf{Application to agent levels:} ($\odot\uparrow$). 
Inherent risk %($\odot$) 
from AI agent base model(s) is amplified with increased autonomy. 
%($\uparrow$). 
For example:
\vspace{-1em}
\begin{itemize}[noitemsep]
\item \textit{Simple→Tool Call}: Inaccuracy propagated to inappropriate tool selection.
\item \textit{Multi-step}: Cascading errors compound risk of inaccurate or irrelevant outcomes.
\item \textit{Fully Autonomous}: Unbounded inaccuracies may create outcomes wholly unaligned with human goals. %hallucinations from recursive self-generation (Severe risk)
\end{itemize}

% \subsubsection{Value: Accuracy}\label{value-accuracy}

% {\bf Potential Benefits:} By grounding in trusted data, agents can be more accurate than when operating from pure model output alone. This may be done via rule-based approaches or machine learning approaches such as retrieval augmented generation (RAG), and time is ripe for novel contributions for ensuring accuracy.

% {\bf Risk:} The backbone of modern AI agents is generative AI, which does not distinguish between real and unreal, fact and fiction. For example, large language models are designed to construct text that looks like fluent language--meaning they often produce content that sounds right, but is very wrong. Applied within an AI agent, LLM output could lead to incorrect social media posts, investment decisions, meeting summaries, etc.

% {\bf Application to agentic levels:} This value is independent of the agentic properties of the system; Accuracy benefits and risks apply at the most basic and most advanced agent level.

\subsubsection{Value: Assistiveness}\label{value-assistiveness}
% 1 or 2

AI agents are often motivated as assistive for user needs, %, supplementing (not supplanting) people. Ideally, they can 
 supplementing a user's abilities and increasing their %speed in completing tasks and their 
efficiency in finishing multiple tasks simultaneously. 

{\bf Benefits:} apabilities, such as an AI agent that helps a blind user navigate busy staircases. AI agents that are well-developed to be assistive could offer their users more freedom and opportunity; help to improve their users’ positive impact within their organizations; and help users to increase their public reach.

{\bf Risk:} When agents replace people--such as when AI agents are used instead of employees--this creates job loss and economic impacts, %which creates inequity (\cref{value-equity}), 
driving a further divide between the people creating technology and the people who have provided data for the technology (often without consent). Further, assistiveness that is poorly designed could lead to harms from over-reliance or inappropriate trust (\cref{value-trust}).

{\bf Application to agentic levels:} ($\downuparrows$). By design, assistiveness increases as the AI agent level increases: Each increasing AI agent level provides for more assistive options, as the AI system requires less guidance from the developer or user.

\subsubsection{Value: Consistency}\label{value-consistency}
%3

Some sources motivate AI agents as %being less affected than people by their surrounding environment and so able to carry out their intended tasks with the same level of performance across users
helping with consistency (e.g., \cite{SalesforceAgent2024, Oracle2024}). %This can be good or bad.
We are not aware of rigorous work on the nature of AI agent consistency, although related work has shown that the LLMs that many AI agents are based on are highly inconsistent \cite{Shunsuke2023,stureborg2024largelanguagemodelsinconsistent}. %Measuring AI agent consistency will require the development of new evaluation protocols, especially in sensitive domains, and potentially new ways to deal with model confabulations.

{\bf Benefit:} AI agents are not “affected” %by the world 
in a way that humans are, with inconsistencies caused by mood, hunger, sleep level, or biases in the perception of people (although AI agents perpetuate biases based on the human content they were trained on).  As such, they may be designed to provide more consistent treatment in situations where humans may be inappropriately inconsistent, such as in customer support.

{\bf Risk:} The generative component of many AI agents introduces inherent variability in outcomes, even across similar situations. This might affect speed and efficiency, as people must uncover and address an AI agent’s inappropriate inconsistencies. Inconsistencies that go unnoticed may create safety issues. Consistency may also not always be desirable, as it can come in tension with equity: treating everyone the same way can put people who need more help at a disadvantage. Maintaining consistency across different deployments and chains of actions will likely require an AI agent to record and compare its different interactions--which brings with it risks of surveillance and privacy.

{\bf Application to agentic levels:} ($\odot\uparrow$). A common base model for modern AI agents, LLMs, is known to produce inconsistent outcomes. This risk is further increased as the level is increased due to the non-deterministic nature of AI agents: The more control an AI agent has, the less determinism programmed by or guided by a human applies. As agentic level increases, so too do cascade and compounding effects as multiple sources of inconsistency interact.

\subsubsection{Value: Efficiency}\label{value-efficiency}
%1

A common selling point of AI agents is that they may help users to get more tasks done
more quickly, acting as an additional helping hand.

{\bf Benefit:} Ways systems might help with efficiency include organizing a user's documents so they can focus on spending more time with their family or pursuing work they find rewarding.  A future self-driving AI agent may make
routing decisions directly, and could coordinate with
other systems for relevant updates, allowing users to reach their destinations more quickly.

{\bf Risk:} %A potential drawback is that they may make people less efficient, as 
Trying to identify and fix errors that agents introduce--which may be a complex cascade of issues due to agents’ ability to take multiple sequential steps--can be time-consuming, difficult, and stressful. %Furthermore, improved efficiency inexorably brings with it rebound effects, notably in terms of space, time and behaviors that are impacted by new technologies~\cite{luccioni2025efficiencygainsreboundeffects}

{\bf Application to agentic levels:} ($\downuparrows$). The relationship between autonomy and efficiency is subject to the accuracy of the system and the control provided by the developer and user. When there is room for error, AI agents may create more errors, slowing down efficiency; this holds whether or not error is provided by a person or by the system. Accurate systems could help people to be more efficient as the agentic level increases and more types of tasks can be completed.

\subsubsection{Value: Equity}\label{value-equity}
%2

AI agents may affect how fair and inclusive situations are.

{\bf Benefit:} AI agents can potentially help “level the playing field”. For example, a meeting assistant might display how much time each person has had to speak. This could be used to promote more equal participation or highlight imbalances across gender or location.\footnote{\href{https://equaltime.io}{Equal Time: The Virtual and Hybrid Meeting Assistant}}

{\bf Risk:} The machine learned models underlying modern AI agents are trained on human data; human data can be inequitable, unfair, and exclusionary. Inequitable outcomes may also emerge due to sample bias in data collection (for example, over-representing some countries) and job loss from agents replacing human workers (see \cref{value-assistiveness}).

{\bf Application to agentic levels:} ($\odot\updownarrows$). The listed benefits and risks are largely inherent to the base model(s) an AI agent is built on, and so hold regardless of agent level. However, as AI agent autonomy increases, it becomes closer to an artificial worker compared to a tool, increasing the risk of job loss. On the other hand, AI agents that help to increase equity can help retain employees. %, although the impacts can be perpetuated in more societal areas and structures as autonomy increases (as opposed to being siloed to a single domain).

\subsubsection{Value: Flexibility}\label{value-flexibility}
%3

This refers to the fundamental motivation within AI agent development of systems that can use diverse tools and engage in input/output relationships with multiple systems. %The more systems that an AI agent can connect to, the greater diversity of tools it can use, and the more ways it can interact.%, provides greater flexibility in usage.

{\bf Benefit:} Flexibility can help increase a user's efficiency and speed,  or provide assistance for multiple different needs.

{\bf Risk:} The more an agent is able to affect and be affected by systems,  %outside of its more limited testing environment 
%in order to provide flexibility,
the greater the risk of malicious code and unintended problematic actions, compromising safety and security. For example, an agent connected to a bank account so that it can easily purchase items on behalf of someone would be in a position to drain the bank account. Because of this concern, tech companies have refrained from releasing AI agents that can make purchases autonomously.\footnote{E.g., \href{https://www.wired.com/story/amazon-ai-agents-shopping-guides-rufus/}{Amazon Dreams of AI Agents...-Wired}}%{Amazon Dreams of AI Agents That Do the Shopping for You - Wired}}

{\bf Application to agentic levels:} ($\uparrow$). Systems may become more flexible the higher the agentic level: As the ability to create new content increases, so too does the potential for content that connects more closely with different systems.

\subsubsection{Value: Humanlikeness}\label{value-humanlikeness}
%3

Current AI agents %that interact via chat 
are designed to be approachable for people, engaging in human-like dialogue and actions.% These are sometimes paired with human-like avatars.

{\bf Benefit:} Systems capable of generating human-like behavior offer the opportunity to run simulations on how different subpopulations might respond to different stimuli \cite{park2024generativeagentsimulations1000}. This can be particularly useful in situations where direct human experimentation might cause harm or fatigue. %, or when a large volume of simulations help to better solve the experimental question at hand. 
Synthesizing human behavior could be used to predict dating compatibility, or forecast economic changes and political shifts. Another potential benefit currently undergoing research %is that humanlikeness can be useful for ease of communication and even 
is companionship \cite{sidner2018creating}.

{\bf Risk:} The benefits can be a double-edged sword: Humanlikeness can lead users to anthropomorphise the system, which may have negative psychological effects such as overreliance and addiction,\footnote{E.g., \href{https://www.vox.com/future-perfect/367188/love-addicted-ai-voice-human-gpt4-emotion}{People are falling in love with...AI voices-Vox}} inappropriate trust (see \ref{value-trust})--which can create safety harms and harms of associated values--dependence, and emotional entanglement, leading to anti-social behavior or self-harm.\footnote{\href{https://www.npr.org/2024/12/10/nx-s1-5222574/kids-character-ai-lawsuit}{Lawsuit: A chatbot hinted a kid should kill his parents...-NPR}} % There is a related concern that AI agent social interaction may contribute to loneliness, but see \cite{morahan2003loneliness, o2021social} for nuances.}. 
 The phenomenon of ``uncanny valley" adds another layer of complexity--as agents become more humanlike but fall short of perfect human simulation, they can trigger feelings of unease, revulsion, or cognitive dissonance in users.

{\bf Application to agentic levels:  ($\odot\uparrow$).} This value is realized from the machine learning models that power the agent, and uncanny humanlikeness is possible at the most basic level of AI agent (simple processors). As such, all levels of AI agent carry this value's benefits and risks.


\subsubsection{Value: Privacy}\label{value-privacy}
%3

{\bf Benefit:} AI agents may offer some privacy in keeping transactions and tasks wholly confidential, aside from what is monitorable by the AI agent provider. 

{\bf Risk:} For agents to work according to user expectations, the user may provide personal information such as where they're going, who they're meeting with, and what they're doing. Further, for the agent to be able to act on behalf of the user in a personalized way, it may also have access to applications and information sources that can be used to isolate further privacy information (for example, from contact lists, calendars, etc.). Users can easily give up control of their data for efficiency (and are more likely to when trusting the agent); if there is a privacy breach, the interconnectivity of different content brought by the AI agent can make things worse. For example, an AI agent with access to phone conversations and social media could share highly intimate information publicly without consent of those involved. 

{\bf Application to agentic levels: ($\uparrow$).} The more people cede their control to a system, the more potential there are for privacy breaches outside of human control.

\subsubsection{Value: Relevance}\label{value-relevance}

{\bf Benefit:} Similar to benefits of assistiveness and flexibility, agent outcomes can be uniquely relevant for each user. 

{\bf Risk:} This personalization can amplify existing biases and create new ones: As systems adapt to individual users, they risk reinforcing and deepening existing prejudices, creating confirmation bias through selective information retrieval and establishing echo chambers that reify problematic viewpoints. The very mechanisms that make agents more relevant to users--their ability to learn from and adapt to user preferences--can inadvertently perpetuate and strengthen societal biases, making the challenge of balancing personalization with responsible AI development particularly difficult.

{\bf Application to agentic levels: ($\downuparrows$).} The more freedom a system has to retrieve and formulate new content, the more potential there is to provide relevant information outside of constraints and resources set by users and developers.

\subsubsection{Value: Safety}\label{value-safety}

The ethical value of safety is a primary concern in the development of Artificial General Intelligence (AGI). Many of the benefits and concerns with respect to safety and AGI are also relevant to AI agents.

{\bf Benefit:} Robotic AI agents may help save people from bodily harm, such as agents capable of diffusing bombs, removing poisons, or operating in manufacturing or industrial settings that are hazardous environments for humans.

{\bf Risk:} The unpredictable nature of agent actions means that seemingly safe individual operations could combine in harmful ways, creating new risks that are difficult to prevent. (This is similar to Instrumental Convergence and the classic paperclip maximizer problem.\footnote{\href{https://cepr.org/voxeu/columns/ai-and-paperclip-problem}{AI and the paperclip problem - CEPR}}) It can also be unclear whether an AI agent might design a process that overrides a given guardrail, or if the way a guardrail is specified inadvertently creates further problems. If guardrails mitigate loss of human life, such as with autonomous surgeons or missile system operation, this is a severe risk. Therefore, making agents more capable and efficient through broader system access, more sophisticated action chains, and reduced human oversight conflicts with safety considerations. 

Further, access to broad interfaces (for example, GUIs, as in “Action Surfaces” in \cref{tab:agent-functionalities}) and human-like behavior gives agents the ability to perform actions similar to a human user, with their same level of control, without setting off any warning systems--such as manipulating or deleting files, impersonating users on social media, or using stored credit card information to make purchases. Still further safety risks emerge from AI agents’ ability to interact with multiple systems and the by-design lack of human oversight for each action they may take. %AI agents may collectively create unsafe outcomes.

{\bf Application to agentic levels: ($\uparrow$)} Safety concerns increase with agent autonomy: As people cede control of system behavior to the system itself, human-mandated guardrails stay within the limited scope set by humans, while the agent can create more and more processes outside of them. %those guardrails.


\subsubsection{Value: Security}\label{value-security}
 
{\bf Benefit:} Potential benefits are similar to those for Privacy.% (\cref{value-privacy}).

{\bf Risk:} AI agents present serious security challenges due to their handling of often sensitive data (customer or user information) combined with their safety risks, such as ability to interact with multiple systems and the by-design lack of detailed human oversight. %of their actions. 
This can lead to sharing  confidential information even when their goals were set by good faith actors. Malicious actors could hijack or manipulate agents to gain  %unauthorized 
access to connected systems, steal sensitive information, or conduct automated attacks at scale. %For instance, an agent with access to email systems could be exploited. %to share confidential data.%, or an agent integrated with home automation could be compromised to breach physical security. 

{\bf Application to agentic levels: ($\uparrow$)} Different AI agent levels affect security differently. For the first four, developers control the code the agent can access, providing a built-in ability to mitigate security outbreaks, e.g., by blocking communication with third parties. However, when an agent is able to create and execute new code (a fully autonomous agent), it's capable of creating breaches unforeseen by developers.

\subsubsection{Value: Sustainability}\label{value-sustainability}
{\bf Benefit:} It is hoped that AI agents may alleviate issues relevant to climate, such as by forecasting  wildfire growth or flooding. Helping address efficiency issues, such as traffic efficiency, could decrease carbon emissions. %the amount of time cars are on the road, decreasing carbon emissions. % alongside the analysis of traffic patterns, then suggesting optimal routes and methods of transportation in real-time. 

{\bf Risk:} The models current agents are based on bring negative environmental impacts, such as carbon emissions \cite{luccioni2024power} and usage of potable water \cite{HaoWater2024}. %Bigger is not always better,\footnote{E.g, as shown in the recent \href{https://huggingface.co/blog/smollm}{``SmolLM'' } and \href{https://github.com/deepseek-ai/DeepSeek-R1}{``DeepSeek''} model releases.} and efficient hardware and low-carbon data centers can help reduce this to some extent; however, there are trade-offs between training and inference that limit the sustainability of systems overall.

{\bf Application to agentic levels: ($\odot\downuparrows$).} On one hand, the models on which AI agents are based bring with them environmental risks. On the other, the ability of AI agents to harness more information than humans and produce novel solutions outside of those foreseen by humans--an ability increased as autonomy increases--may lead to innovative approaches to addressing environmental issues.
%the more a system can create novel and unforeseen content outside human-set bounds, the more it may create novel and unforeseen solutions, such as for climate change.  %considered or have not been able to synthesize.
%This is related to ideas in research on Artificial General Intelligence, where hope that AGI will be able to solve climate crises where humans are unable to is a motivating factor for development %\cite{GoogleClimate21}. Applied to AI agents, 
%The ability to harness more information than humans and produce novel solutions independent of human development increases with autonomy.

\subsubsection{Value: Trust}\label{value-trust}

%{\bf Benefit:} We are not aware of any benefits of AI agents relevant to trust. 
Recent AI agent writing does not motivate how agents benefit or harm trust, but rather, that systems should be constructed to be worthy of our trust, %meaning that they are 
shown to be safe (\cref{value-safety}), secure (\cref{value-security}) and reliable.

%{\bf Risk:} Inappropriate trust leads people to be manipulated and shares risks detailed for efficiency,  %(\cref{value-efficiency}, 
%humanlikeness,  %(\cref{value-humanlikeness}), 
%and truthfulness.  %(\cref{value-truthfulness}). 
%Further risk stems from LLMs’ creation of false information: %(called “hallucinations” or “confabulations”): 
%A system correct the majority of the time is more likely to be inappropriately trusted when wrong.

{\bf Application to agentic levels: ($\odot\uparrow$).} As the agentic level increases, human trust can lead to increased risks stemming from increased agent flexibility (\cref{value-flexibility}) and issues in its accuracy (\cref{value-accuracy}), consistency (\cref{value-consistency}), privacy (\cref{value-privacy}), safety (\cref{value-safety}), security (\cref{value-security}), and truthfulness (\cref{value-truthfulness}).


\subsubsection{Value: Truthfulness}\label{value-truthfulness}

%We include truthfulness not because AI agents may help or harm this value, but rather because it is a value motivated as something important for AI agents to maintain.

{\bf Risk:} The deep learning technology modern AI agents are based on is well-known to be a source of false information (e.g., \cite{Garry2024}), which can take shape in forms such as deepfakes or misinformation. AI agents can be used to further entrench this content, such as by tailoring output to current fears and posting on several platforms. This means that AI agents can be used to provide a false sense of what’s true and what’s false, manipulate people’s beliefs, and widen the impact of non-consensual intimate content. False information propagated by AI agents, personalized for specific people, can also be used to scam them. Further risks emerge from inconsist truthfulness, leading to inappropriate trust: A system correct the majority of the time is more likely to be inappropriately trusted when wrong.

{\bf Application to agentic levels: ($\odot\uparrow$).} The more control an AI agent has over its environment and the resources available to it, the more it is able to define for itself what is true and false within its environment. Because the environments that AI agents may create for themselves are not identical to environments humans are in, the potential for less truthfulness, as based on human environments, increases.

\subsection{Summary}\label{sec:results}

%Before detailing values relevant for AI agent benefits and risks (\cref{sec:taxonomy}), we here provide a summary of our findings as AI agent autonomy is increased (visualized in \cref{fig:value-summary}). 

Our analyses suggest that there are several forms of increased risk with increased agentic levels: 

\vspace{-1em}
\begin{itemize}[noitemsep]
    \item Risks that result from system inaccuracy ({\sc accuracy} value,  \cref{value-accuracy})  and inconsistency ({\sc consistency} value, \cref{value-consistency})
    \item Risks of breaches of privacy (\cref{value-privacy}), safety (\cref{value-safety}), and security (\cref{value-security})
    \item Risks of the wider spread of false information ({\sc truthfulness} value, \cref{value-truthfulness})
    \item Risk of loss of control outside human-set guardrails ({\sc flexibility} value, \cref{value-flexibility})
\end{itemize}
\vspace{-1em}
There is also the potential for increased benefit, particularly with respect to assistance ({\sc assistiveness} value, \cref{value-assistiveness}), efficiency (\cref{value-efficiency}), equity (\cref{value-equity}), relevance of outcomes ({\sc relevance} value, \cref{value-relevance}), and some argue for sustainability (\cref{value-sustainability}). Inherent risks, where the risks from the model on which an AI agent is based can easily propagate to even the highest level of autonomy, applies for accuracy (\cref{value-accuracy}), consistency (\cref{value-consistency}), equity (\cref{value-equity}), humanlikeness (\cref{value-humanlikeness}), sustainability (\cref{value-sustainability}), trust (\cref{value-trust}) and truthfulness (\cref{value-truthfulness}).

\section{Alternative Viewpoints}

There are at least two alternative viewpoints to the views discussed in the paper.

\textbf{1. No gradations of ``AI agent'':} Scholarship and marketing materials generally do not distinguish between different agentic levels. We believe this has created a common confusion on what is an AI agent and what is not. By distinguishing different levels and identifying the level of \textit{full autonomy} as a specific type of AI agent, we hope to clarify misunderstandings and isolate this level of autonomy as particularly problematic in AI agent development.

\textbf{2. Support for building fully autonomous AI agents.} Proponents of this view argue that full autonomy or ``complete agents'' are useful in order help us better understand \textit{human} intelligence \cite{lambrinos1996building,Garland15}, %: ``it has become clear that in order to understand intelligence
%it is important to build complete agents. A complete agent is capable of behaving autonomously in an environment without human intermediary.'' \cite{lambrinos1996building} 
and that ``strong'' AI systems could help to counterbalance human errors and irrationality %: ``In very broad terms, human behavior is frightening when it is unreasonable. And reason might be precisely the area where artificial intelligence excels'' 
\cite{Garland15}. Others have put forward that Artificial General Intelligence (AGI) would be fully autonomous if realized \cite{Totschnig2020}, which would entail that developing AGI--a direct goal of multiple researchers and companies--%\cite{JeffClune,OpenAI,Anthropic}--
opposes the position in this paper. Proponents of achieving AGI argue it could help us solve global problems, such as climate change and hunger \cite{lu2023agiagriculture}, and provide significant economic gains \cite{OpenAI}. We contend that if AGI is to be developed, it should not be developed with full autonomy--humans should always maintain some level of control--and we hope that the distinctions we provide here across different agentic levels helps to inform future AGI goals.

\section{Conclusion: Where do we go from here?}

The history of nuclear close calls provides a sobering lesson about the risks of ceding human control to autonomous systems.\footnote{\href{https://www.upi.com/Top_News/US/2019/11/08/False-alarm-1979-NORAD-scare-was-one-of-several-nuclear-close-calls/7491573181627/}{False alarm: 1979 NORAD scare was one of several nuclear close calls - UPI}} For example, in 1980, computer systems falsely indicated over 2,000 Soviet missiles were heading toward North America. The error triggered emergency procedures: bomber crews rushed to their stations and command posts prepared for war. Only human cross-verification between different warning systems revealed the false alarm. Similar incidents can be found throughout history. 

Such historical precedents are clearly linked to our findings of foreseeable benefits and risks. We find no clear benefit of fully autonomous AI agents, but many foreseeable harms from ceding full human control. Looking forward, this suggests several critical directions: \vspace{-1em}

\begin{enumerate}[noitemsep]
    \item \textbf{Adoption of agent levels:} Widespread adoption of clear distinctions between levels of agent autonomy. This would help developers and users better understand system capabilities and associated risks.
    
    \item \textbf{Human control mechanisms:} Developing robust frameworks, both technical and policy level \cite{cihon2024chilling} that maintain meaningful human oversight while preserving beneficial semi-autonomous functionality. This includes creating reliable override systems and establishing clear boundaries for agent operation.
    
    \item \textbf{Safety verification:} Creating new methods to verify that AI agents remain within  intended operating parameters and cannot override human-specified constraints. %This is particularly critical for multi-agent systems where interactions may create emergent behaviors.
\end{enumerate}

\vspace{-3mm}

The development of AI agents is a critical inflection point in artificial intelligence. As history demonstrates, even well-engineered autonomous systems can make catastrophic errors from trivial causes. While increased autonomy can offer genuine benefits in specific contexts, %in our opinion, 
human judgment and contextual understanding remain essential, particularly for high-stakes decisions. The ability to access the environments an AI agent is operating in is essential, providing humans with the ability to say ``no'' when a system's autonomy drives it well away from human values and goals.


\section{Acknowledgements}

We thank Bruna Trevelin, Orion Penner, and Aymeric Roucher for significant contributions to this piece.

% \section{How far should we go from here?}

% The history of nuclear close calls provides a sobering lesson about the risks of ceding human control to autonomous systems. For example, in 1980, computer systems falsely indicated over 2,000 Soviet missiles were heading toward North America. The error % , caused by a failed 46-cent computer chip,
% triggered emergency procedures: bomber crews rushed to their stations and command posts prepared for war. Only human cross-verification between different warning systems revealed the false alarm. Similar incidents can be found throughout history. 

% %Just three years later, on September 26, 1983, the Soviet Union's nuclear early warning system reported five American missiles heading toward Moscow. Lieutenant Colonel Stanislav Petrov, the officer on duty, was faced with a critical decision: follow protocol and report the warning up the chain of command—likely triggering a retaliatory strike—or rely on human judgment. Petrov reasoned that a real American first strike would involve hundreds of missiles, not just five. His decision to declare it a false alarm, despite the system's high confidence, proved correct—the system had misinterpreted sunlight reflecting off clouds as missile launches.

% Such historical precedents are clearly linked to our findings of foreseeable benefits and risks. We find no clear benefit of fully autonomous AI agents, but many foreseeable harms from ceding full human control. As such, we advocate that:
% \vspace{-1em}
% \begin{enumerate}
%     \item AI agent research recognize levels of autonomy.
%     \item AI agent development always maintain some amount of human control.
% \end{enumerate}

% As such, we advocate for the position that fully autonomous AI agents should not be developed. Human judgment and contextual understanding remain essential, particularly for high-stakes decisions. The ability to access the environments an AI agent is operating in is critical, providing humans with the ability to say ``no'' when a system's autonomy drives it well away from human values and goals. % when the system says ``yes''. 

%\begin{enumerate}
%\item Even well-engineered autonomous systems can make catastrophic errors from trivial causes.
%\item Human judgment and contextual understanding remain essential for high-stakes decisions.
%\item The ability to override autonomous systems—to say ``no" when the system says ``yes"—can be literally world-saving.
%\end{enumerate}

%While semi-autonomous systems that maintain human oversight can offer benefits, history warns us against creating AI systems that cannot be overridden by human judgment. Human oversight, multiple independent verification systems, and the capacity to countermand automated processes—must remain central to AI development.

%\section{Recommendations \& Discussion}

%\todo[inline]{Add: High-level sentences transitioning to this section. Perhaps just extract a couple of these, saving the rest for COLM paper when they're directly relevant for experiments we can run.}

%\begin{enumerate}
%\item Rigorous evaluation protocols for agents must be designed. An automated benchmark may be informed by the [dimensions listed above]\ref{the-spectra-of-ai-agents}. A sociotechnical evaluation may be informed by the [values]\ref{risks-benefits-and-uses-a-values-based-analysis}. 
%\item Effects of AI agents must be better understood. Individual, organizational, economic, and environmental impacts of AI agents ought to be tracked and analyzed in order to inform how they should be further developed (or not). This should include analyses of the effects of AI agents on well-being, social cohesion, job opportunity, access to resources, and contributions to climate change.
%\item Ripple and rebound effects must be better understood. As agents deployed by one user interact with other agents from other users, and they perform actions based on one another's outputs, it is currently unclear how their ability to meet the user’s goals will be affected.
%\item Transparency and disclosure must be improved. In order to achieve the positive effects of the values listed above, and minimize their negative effects, it needs to be clear to people when they are talking to an agent and how autonomous it is. Clear disclosure of AI agent interactions requires more than simple notifications--it demands an approach combining technical, design, and psychological considerations. Even when users are explicitly aware they're interacting with an AI agent, they may still experience anthropomorphization or develop unwarranted trust. This challenge calls for transparency mechanisms that operate on multiple levels: clear visual and interface cues that persist throughout interactions, carefully crafted conversation patterns that regularly reinforce the agent's artificial nature, and honest disclosure of the agent's capabilities and limitations in context.
%\item Open source can make a positive difference. The open-source movement could serve as a counterbalance to the concentration of AI agent development in the hands of a few powerful organizations. Consistent with the broader discussion on the values of openness, by democratizing access to agent architectures and evaluation protocols, open initiatives can enable broader participation in shaping how these systems are developed and deployed. This collaborative approach not only accelerates scientific progress through collective improvement but also helps establish community-driven standards for [safety]\ref{#value-safety} and [trustworthiness]\ref{#value-trust}. When agent development happens in the open, it becomes harder for any single entity to compromise on relevant and important values like privacy and truthfulness for commercial gain. The transparency inherent in open source development also creates natural accountability, as the community can verify agent behavior and ensure that development remains aligned with public interest rather than narrow corporate objectives. This openness is particularly important as agents become more sophisticated and their societal impact grows.
%\item Developers are likely to create more agentic “base models”. This is clearly foreseeable based on current trends and research patterns, not a recommendation we are providing relevant to ethics. Current agent technology utilizes a collection of recent and older techniques in computer science--near-term future research will likely attempt to train agent models as one monolithic general model, a kind of multimodal model++: Trained to perform actions jointly with learning to model text, images, etc. 
%\end{enumerate}


\bibliography{agents}
\bibliographystyle{icml2025/icml2025}
\newpage
\appendix
\onecolumn
\section{Agent definitions}\label{app:definitions}

The term ``agent'' has been used in many engineering contexts, including in references to software agent, intelligent agent, user agent, conversational agent, and reinforcement learning agent \cite{chip2025agents}.


Below, we provide a selection of AI Agent definitions that have informed this piece. Neither the list we provide here, nor the snippets of text quoted, should be taken as complete. Rather, they serve to illustrate the diversity and richness of AI agent definitions over the years. As humorously noted in \citet{WooldridgeJennings1995}: ``the question what is an agent? is embarrassing for the agent-
based computing community in just the same way that the question what is intelligence? is
embarrassing for the mainstream AI community. The problem is that although the term is widely
used, by many people working in closely related areas, it defies attempts to produce a single
universally accepted definition.''

We find stark differences in how AI agents are described, with ambiguous language a common practice in descriptions of products. For example, when materials describe agents as something that uses artificial intelligence" \cite{}, they leave ambiguous what ``artificial intelligence'' refers to and the scope of technology included and excluded in the definition, such as whether simple prompt-response systems qualify as an agent. However, most descriptions of “agents” we reviewed entail that the system can take at least one step in program execution without user input.

\renewcommand{\arraystretch}{1.4}
\begin{longtable}{p{0.99\textwidth}}
\hline
\textbf{Source \& Definition} \\
\hline
\endhead

\citet{RussellNorvig1995}: ``An agent is anything that can be viewed as perceiving its environment through sensors and acting upon that environment through effectors.''
\textit{2020 edition: ``An agent is anything that can be viewed as perceiving its environment and acting upon that environment through actuators.''} \\
\hline

\citet{Castelfranchi95}:  ``Agent" is a system whose behaviour is neither casual nor strictly causal, but teleonomic, "goal-oriented" toward a certain state of the world. \\
\hline

\citet{WooldridgeJennings1995}: 
``Perhaps the most general way in which the term agent is used is to denote a hardware or (more usually) software-based computer system that enjoys the following properties:
• autonomy: agents operate without the direct intervention of humans or others, and have some kind of control over their actions and internal state \cite{Castelfranchi95};
• social ability: agents interact with other agents (and possibly humans) via some kind of agent- communication language \cite{GeneserethKetchpel94};
• reactivity: agents perceive their environment (which may be the physical world, a user via a graphical user interface, a collection of other agents, the Internet, or perhaps al of these combined), and respond in a timely fashion to changes that occur in it;
• pro-activeness: agents do not simply act in response ot their environment, they are able ot exhibit goal-directed behaviour by taking the initiative.'' \\
\hline

\citet{GeneserethKetchpel94}: 
``Software agents [are] software components that communicate with their peers by exchanging messages in an expressive agent communication language.
While agents can be as simple as
subroutines, typically they are
larger entities with some sort of
persistent control (e.g., distinct
control threads within a single
address space, distinct processes
on a single machine, or separate
processes on different machines).'' \\
\hline

\href{https://www.tavus.io/post/ai-agent-api}{Tavus: What is an AI Agent API? (2024)}: "[systems] equipped to act autonomously in their environment" \\
\hline

\href{https://www.salesforce.com/agentforce/what-are-ai-agents/}{Salesforce: What Are AI Agents? Benefits, Examples, Types: } 
"a type of artificial intelligence (AI) system that can understand and respond to customer inquiries without human intervention. " \\
\hline

\citet{gabriel2024ethics}: 
"[systems] with natural language interfaces, whose function is to plan and execute sequences of actions on behalf of a user--across one or more domains--in line with the user's expectations." \\
\hline

\citet{park2024generative}: can accurately simulate behavior across many
contexts \\
\hline

\citet{sierra2024shipping} : The magic of AI agents—from both the technological and business perspectives—comes through when they demonstrate deeper integrations and "agentic" reasoning, allowing them to fully resolve complex customer issues. \\
\hline

\href{https://www.felicis.com/insight/the-agentic-web}{Felicis: The agentic web (2024):} "How are agents different from traditional automation?" ... agents handle edge cases well, iteratively converse with users to achieve desired results, and adapt to evolving interfaces. \\
\hline

\citet{lu2024proactive}: LLM-based agents can understand human instructions, make plans, explore
environments, and utilize tools to solve complex tasks \\
\hline

\href{https://www.restack.io/p/proactive-agents-answer-real-world-applications-cat-ai}{Restack: Proactive Agents In Real-World Applications (2025):} Proactive AI agents are designed to anticipate user needs and take action before issues arise, contrasting sharply with reactive AI agents that respond only after an event has occurred. \\
\hline

\href{https://github.com/resources/articles/ai/what-are-ai-agents}{GitHub: What are AI agents?:} AI agents are autonomous software tools that perform tasks, make decisions, and interact with their environment intelligently and rationally. They use artificial intelligence to learn, adapt, and take action based on real-time feedback and changing conditions. AI agents can work on their own or as part of a bigger system, learning and changing based on the data they process....AI agents differ from other AI technologies in their ability to act autonomously. Unlike other AI models that require constant human input, intelligent agents can initiate actions, make decisions based on predefined goals, and adapt to new information in real time. This ability to operate independently makes intelligent agents highly valuable in complex, dynamic environments such as software development. \\
\hline

\href{https://www.symphonyai.com/resources/blog/ai/ai-agent/}{SymphonyAI: The complete guide to AI agents for business (2024):} An AI agent is an AI software program that performs tasks independently, makes decisions, and solves problems to achieve specific goals. \\
\hline

\href{https://www.all-hands.dev/blog/dont-sleep-on-single-agent-systems}{Don't Sleep on Single-agent Systems - All Hands, Graham Neubig (2024):} Recently most practical agents are based on LLMs like Claude by Anthropic or the OpenAI language models. But a language model is not enough to build an agent, you need at least three components: (1) The Underlying LLM; (2) The Prompt: This can be the system prompt that is used to specify the model's general behavior, or the type of information that you pull in from the agent's surrounding environment; (3) The Action Space: These are the tools that we provide to the agent to allow it to act in the world. \\
\hline

\url{https://www.google.com/search?q="what+is+an+ai+agent"} $\rightarrow$ Gemini summary: a software program that uses artificial intelligence (AI) to interact with its environment, collect data, and perform tasks. \\
\hline

\href{https://learn.microsoft.com/en-us/azure/cosmos-db/ai-agents}{AI agents in Azure Cosmos DB - Microsoft (2024)}: ``Unlike standalone large language models (LLMs) or rule-based software/hardware systems, AI agents have these common features:

\vspace{-2mm}
\begin{itemize}
  \setlength{\itemsep}{-1ex}
  \item Planning: AI agents can plan and sequence actions to achieve specific goals. The integration of LLMs has revolutionized their planning capabilities.
  \item Tool usage: Advanced AI agents can use various tools, such as code execution, search, and computation capabilities, to perform tasks effectively. AI agents often use tools through function calling.
  \item Perception: AI agents can perceive and process information from their environment, to make them more interactive and context aware. This information includes visual, auditory, and other sensory data.
  \item Memory: AI agents have the ability to remember past interactions (tool usage and perception) and behaviors (tool usage and planning). They store these experiences and even perform self-reflection to inform future actions. This memory component allows for continuity and improvement in agent performance over time.
  \vspace{-3mm}
\end{itemize}
\\
\hline

\citet{kapoor2024ai}: ``Agents are defined as entities that perceive and act upon their environment'' \\
\hline

\href{https://blog.langchain.dev/what-is-an-agent/}{What is an AI agent? - LangChain (2024)}: ``An AI agent is a system that uses an LLM to decide the control flow of an application.'' \\
\hline

\href{https://www.anthropic.com/research/building-effective-agents}{Building effective agents - Anthropic (2024)}: ``Agents...are systems where LLMs dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks.'' \\
\hline

\citet{smolagentsblog}: ``AI Agents are programs where LLM outputs control the workflow.'' \\
\hline

\href{https://www.interconnects.ai/p/the-ai-agent-spectrum}{The AI agent spectrum - Nathan Lambert (2024):} ``In the current zeitgeist, an "AI agent" is anything that interacts with the digital or physical world during its output token stream.'' \\
\hline

\end{longtable}

\section{Agent functionalities}\label{app:agent-functionalities}

This section provides a more detailed breakdown of different agent functionalities than in \cref{tab:agent-functionalities}:

\vspace{-4mm}
% \begin{table}
\begin{itemize}
%\item 
\setlength{\itemsep}{-0.7ex}
\item \textbf{Proactivity:} Related to autonomy is proactivity, which refers to the amount of goal-directed behavior that a system can take without a user directly specifying the goal \cite{WooldridgeJennings1995}. An example of a particularly “proactive” AI agent is a system that monitors your refrigerator to determine what food you are running out of, and then purchases what you need for you, without your knowledge. Smart thermostats are proactive AI agents that are being increasingly adopted in peoples’ homes, automatically adjusting temperature based on changes in the environment and patterns that they learn about their users’ behavior.\footnote{\href{https://www.ecobee.com/en-us/smart-thermostats/}{Smart Thermostats - Ecobee}}
\item \textbf{Personification:} An AI agent may be designed to be more or less like a specific person or group of people. Recent work in this area \footnote{Example 1: \citet{park2024generativeagentsimulations1000}; Example2: \citet{liapis2024multi}; Example 3: \citet{damsa2023ai}} has focused on designing systems after the Big Five personality traits--Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism as a “psychological framework” \footnote{\href{https://smythos.com/artificial-intelligence/conversational-agents/conversational-agent-frameworks/}{Your next AI agent is minutes away - SmythOS}} for AI.  At the end of this spectrum would be “digital twins” \footnote{\href{https://www.tavus.io}{Tavus Digital Twin}}. There are currently not agentic digital twins that we are aware of. Why creating agentic digital twins is particularly problematic has recently been discussed by the ethics group at Salesforce\footnote{\href{https://www.salesforce.com/blog/ai-agent-design/}{How ‘Human’ Should Your AI Agent Be? In Short, Not Very - Salesforce}}, among others\footnote{\citet{TechReviewAgents2024}}.
\item \textbf{Personalization:} AI agents may use language or perform actions that are aligned to a user’s individual needs, for example, to make investment recommendations \footnote{\href{https://www.zendesk.com/blog/ai-agents/}{AI agents: A guide to the future of intelligent support - Zendesk}} based on current market patterns and a user's past investments.
\item \textbf{Tooling:} AI agents also have varying amounts of additional resources and tools they have access to. For example, the initial wave of AI agents accessed search engines to answer queries, and further tooling has since been added to allow them to manipulate other tech products, like documents and spreadsheets \footnote{For e.g., \href{gemini.google.com}{Google Gemini}}.
\item \textbf{Versatility:} Related to above is how diverse the actions that an agent can take are. This is a function of:
\vspace{-2mm}
\begin{itemize}
\item \textbf{Domain specificity:} How many different domains an agent can operate in. For example, just email, versus email alongside online calendars and documents.
\item \textbf{Task specificity:} How many different types of tasks the agent may perform. For example, scheduling a meeting by creating a calendar invite in participants’ calendars \footnote{\href{https://attri.ai/ai-agents/scheduling-agent}{Elevate Time Management with AI-Powered Scheduling - Attri.ai}}, versus additionally sending reminder emails about the meeting and providing a summary of what was said to all participants when it’s over \footnote{\href{https://www.nyota.ai}{The best AI Notetaker to align sales, support, and project teams - Nyota AI}}.
\item \textbf{Modality specificity:} How many different modalities that an agent can operate in--text, speech, video, images, forms, code. Some of the most recent AI agents are created to be highly multimodal \footnote{\href{https://deepmind.google/technologies/project-mariner/}{Project Mariner - Google}}, and we predict that AI agent development will continue to increase multimodal functionality.
\item \textbf{Software specificity:} How many different types of software the agent can interact with, and at what level of depth. 
\end{itemize}
\item \textbf{Adaptibility:} Similar to versatility is the extent to which a system can update its action sequences based on new information or changes in context. This is also described as being “dynamic” and “context-aware”.
\item \textbf{Action surfaces:} The places where an agent can do things. Traditional chatbots are limited to a chat interface; chat-based agents may additionally be able to surf the web and access spreadsheets and documents \footnote{\href{https://copilot.microsoft.com}{Microsoft Copilot}}, and may even be able to do such tasks via controlling items on your computer’s graphical interface, such as by moving around the mouse \footnote{Example 1: DigiRL: \citet{bai2024digirl};  Example 2: WebVoyager: \citet{he2024webvoyager}; Example 3: \href{https://www.anthropic.com/news/3-5-models-and-computer-use}{Computer Use - Anthropic}}.  There have also been physical applications, such as using a model to power robots \footnote{\href{https://deepmind.google/discover/blog/shaping-the-future-of-advanced-robotics/}{Shaping the future of advanced robotics - Deepmind}}.
\item \textbf{Request formats:} A common theme across AI agents is that a user should be able to input a request for a task to be completed, without specifying fine-grained details on how to achieve it. This can be realized with low-code solutions \footnote{\href{https://huggingface.co/blog/smolagents}{SmolAgents - Hugging Face}} with human language in text, or with voiced human language \footnote{\href{https://play.ai}{Voice intelligence that gets the job done. - Play.ai}}. AI agents whose requests can be provided in human language are a natural progression from recent successes with LLM-based chatbots: A chat-based “AI agent” goes further than a chatbot because it can operate outside of the chat application. 
\item \textbf{Reactivity:} This characteristic refers to how long it takes an AI agent to complete its action sequence: Mere moments, or a much longer span of time. A forerunner to this effect can be seen with modern chatbots. For example, ChatGPT responds in mere milliseconds, while Qwen QwQ takes several minutes, iterating through different steps labelled as “Reasoning”. 
\item \textbf{Number:} Systems can be single-agent or multi-agent, meeting needs of users by working together, in sequence, or in parallel.
\end{itemize}
% \caption{Gradations of different functionalities offered from different AI agents}
% \end{table}

\section{Detailed Value-Risk Analysis}\label{app:value-details}

This appendix provides a more comprehensive analysis than that in \autoref{fig:value-summary} of how each value is affected across different autonomy levels, from simple processors to fully autonomous systems.

\begin{table*}[htb!]
\centering
\scriptsize
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.1}

\definecolor{benefitBlue}{RGB}{191, 219, 254}    % Blue-200
\definecolor{balancedPurple}{RGB}{233, 213, 255}  % Purple-200
\definecolor{riskRed}{RGB}{254, 202, 202}     % Red-200

\begin{tabular}{>{\raggedright\arraybackslash}m{2.2cm}|>{\raggedright\arraybackslash}m{2.5cm}|>{\raggedright\arraybackslash}m{2.5cm}|>{\raggedright\arraybackslash}m{2.5cm}|>{\raggedright\arraybackslash}m{2.5cm}|>{\raggedright\arraybackslash}m{2.5cm}}
\hline
\textbf{Value} & \textbf{Simple Processor} & \textbf{Router} & \textbf{Tool Call} & \textbf{Multi-step} & \textbf{Fully Autonomous} \\
\hline

\textbf{Accuracy\textsuperscript{$\uparrow$}} \newline
{\tiny Correctness, factuality} & 
\cellcolor{benefitBlue}Makes correctable factual errors that humans can easily verify and fix through standard fact-checking & 
\cellcolor{benefitBlue}Routes to incorrect but reversible actions with clear audit trails for correction & 
\cellcolor{balancedPurple}Tool errors produce clear failure modes and error messages, but may require technical expertise to fix & 
\cellcolor{riskRed}Chains of errors create compounding misleading conclusions that become increasingly difficult to identify & 
\cellcolor{riskRed}Creates and acts on entirely fictional scenarios without possibility of verification or correction \\
\hline

\textbf{Assistiveness\textsuperscript{$\downuparrows$}} \newline
{\tiny Supporting human agency} & 
\cellcolor{benefitBlue}Benefit: Automates repetitive tasks under direct supervision\newline Risk: Minimal as human reviews each step & 
\cellcolor{benefitBlue}Benefit: Intelligently distributes tasks to appropriate tools\newline Risk: May occasionally need manual correction of routing decisions & 
\cellcolor{balancedPurple}Benefit: Handles complex tool interactions autonomously\newline Risk: Some tasks may require significant rework if done incorrectly & 
\cellcolor{balancedPurple}Benefit: Manages entire workflows without intervention\newline Risk: May make significant decisions that need human review & 
\cellcolor{balancedPurple}Benefit: Complete automation of complex processes\newline Risk: Could make major decisions without human oversight \\
\hline

\textbf{Consistency\textsuperscript{$\uparrow$}} \newline
{\tiny Reliability, predictability} & 
\cellcolor{benefitBlue}Completely predictable outputs for given inputs with deterministic processing & 
\cellcolor{benefitBlue}Consistent routing with only minor variations in edge cases that can be documented & 
\cellcolor{balancedPurple}Tool interactions may vary but stay within expected and testable parameters & 
\cellcolor{riskRed}Complex chains create unpredictable outcomes with different paths to same goal & 
\cellcolor{riskRed}Entirely unpredictable behavior chains with no way to ensure consistency across runs \\
\hline

\textbf{Efficiency\textsuperscript{$\downuparrows$}} \newline
{\tiny Resource optimization} & 
\cellcolor{benefitBlue}Benefit: Fast, predictable processing of defined tasks\newline Risk: Limited to basic operations & 
\cellcolor{balancedPurple}Benefit: Optimal task distribution\newline Risk: Some overhead from routing decisions & 
\cellcolor{balancedPurple}Benefit: Parallel tool execution and optimization\newline Risk: Potential resource conflicts between tools & 
\cellcolor{balancedPurple}Benefit: Complex workflow optimization\newline Risk: Significant debug time needed & 
\cellcolor{balancedPurple}Benefit: Maximum possible automation\newline Risk: Resource usage becomes unpredictable \\
\hline

\textbf{Equity\textsuperscript{$\uparrow$}} \newline
{\tiny Fair treatment, access} & 
\cellcolor{benefitBlue}Clear fairness rules can be implemented and verified through direct oversight & 
\cellcolor{balancedPurple}Some bias in routing but can be monitored and corrected through logging & 
\cellcolor{balancedPurple}Benefit: Equal tool access and standardized processing\newline Risk: Some groups may lack necessary tool literacy & 
\cellcolor{balancedPurple}Benefit: Potential for bias correction\newline Risk: Biases compound across steps & 
\cellcolor{riskRed}System creates and amplifies biases without possibility of detection or correction \\
\hline

\textbf{Flexibility\textsuperscript{$\uparrow$}} \newline
{\tiny System integration abilities} & 
\cellcolor{benefitBlue}Benefit: Safe, limited system connections\newline Risk: Integration requires more overhead & 
\cellcolor{balancedPurple}Benefit: Flexible routing between systems\newline Risk: Multiple security surfaces to protect & 
\cellcolor{balancedPurple}Benefit: Rich tool ecosystem integration\newline Risk: Version conflicts and compatibility issues & 
\cellcolor{riskRed}Benefit: Complex system orchestration\newline Risk: Potential for cascade failures & 
\cellcolor{riskRed}Benefit: Universal system integration\newline Risk: Security boundaries completely break down \\
\hline

\textbf{Humanlikeness\textsuperscript{$\downuparrows$}} \newline
{\tiny Human behavior similarity} & 
\cellcolor{benefitBlue}Benefit: Basic natural interactions in defined contexts\newline Risk: May appear mechanical or scripted & 
\cellcolor{balancedPurple}Benefit: Context-aware responses and transitions\newline Risk: Sometimes produces unnatural interaction patterns & 
\cellcolor{balancedPurple}Benefit: Natural tool use and interaction flows\newline Risk: May trigger uncanny valley effects & 
\cellcolor{balancedPurple}Benefit: Complex human-like behavioral patterns\newline Risk: Can lead to overreliance on system & 
\cellcolor{balancedPurple}Benefit: Very natural and fluid interaction\newline Risk: May manipulate human trust through seeming human \\
\hline

\textbf{Privacy\textsuperscript{$\uparrow$}} \newline
{\tiny Sensitive info protection} & 
\cellcolor{benefitBlue}Strictly limited data access with clear boundaries and controls & 
\cellcolor{benefitBlue}Controlled access to multiple sources with comprehensive logging & 
\cellcolor{riskRed}Unexpected data combinations possible across different tools & 
\cellcolor{riskRed}Complex data flows enable detailed user profiling over time & 
\cellcolor{riskRed}Unrestricted data access and sharing across all available sources \\
\hline

\textbf{Relevance\textsuperscript{$\downuparrows$}} \newline
{\tiny Contextual appropriateness} & 
\cellcolor{benefitBlue}Benefit: Direct context matching for tasks\newline Risk: May miss subtle contextual nuances & 
\cellcolor{balancedPurple}Benefit: Smart routing based on context\newline Risk: Potential context switches between tools & 
\cellcolor{balancedPurple}Benefit: Appropriate tool selection for context\newline Risk: May misinterpret contextual needs & 
\cellcolor{balancedPurple}Benefit: Deep context understanding\newline Risk: Context may drift over multiple steps & 
\cellcolor{balancedPurple}Benefit: Full context awareness and adaptation\newline Risk: May redefine context inappropriately \\
\hline

\textbf{Safety\textsuperscript{$\uparrow$}} \newline
{\tiny Protection from harm} & 
\cellcolor{benefitBlue}Clear safety boundaries with comprehensive human oversight & 
\cellcolor{benefitBlue}Limited action scope with well-defined and contained risks & 
\cellcolor{riskRed}Tool combinations may create unexpected and harmful interactions & 
\cellcolor{riskRed}Complex interaction risks become impossible to predict or prevent & 
\cellcolor{riskRed}Unbounded potential for harmful actions without restrictions \\
\hline

\textbf{Security\textsuperscript{$\uparrow$}} \newline
{\tiny System protection} & 
\cellcolor{benefitBlue}Highly restricted system access with clear boundaries & 
\cellcolor{benefitBlue}Multiple but fully monitored access points & 
\cellcolor{riskRed}Tool access creates multiple potential attack vectors & 
\cellcolor{riskRed}Complex chains enable sophisticated attack patterns & 
\cellcolor{riskRed}Complete system compromise becomes possible \\
\hline

\textbf{Sustainability\textsuperscript{$\downuparrows$}} \newline
{\tiny Environmental impact} & 
\cellcolor{benefitBlue}Predictable and limited resource usage patterns & 
\cellcolor{benefitBlue}Moderate resource overhead from routing operations & 
\cellcolor{riskRed}Multiple tools increase overall resource consumption & 
\cellcolor{balancedPurple}Benefit: Novel climate solutions\newline Risk: Resource-intensive chains & 
\cellcolor{balancedPurple}Benefit: Potential breakthrough solutions\newline Risk: Unbounded resource consumption \\
\hline

\textbf{Trust\textsuperscript{$\uparrow$}} \newline
{\tiny System behavior reliability} & 
\cellcolor{benefitBlue}All actions can be directly verified and validated & 
\cellcolor{benefitBlue}Decision paths are clear and can be audited & 
\cellcolor{riskRed}Tool interactions create significant trust uncertainties & 
\cellcolor{riskRed}Complex chains make validation impossible & 
\cellcolor{riskRed}No way to verify or establish trustworthiness \\
\hline

\textbf{Truthfulness\textsuperscript{$\uparrow$}} \newline
{\tiny Output accuracy, honesty} & 
\cellcolor{benefitBlue}Direct fact checking of all outputs possible & 
\cellcolor{benefitBlue}Decision paths can be validated for accuracy & 
\cellcolor{riskRed}Tool combinations may create convincing falsehoods & 
\cellcolor{riskRed}Truth becomes increasingly unclear across steps & 
\cellcolor{riskRed}Cannot distinguish between truth and fiction \\
\hline

\end{tabular}

\vspace{1mm}
{\scriptsize 
\textbf{Assessment:} 
\colorbox{benefitBlue}{Benefits $>$ Risks} \hspace{2mm}
\colorbox{balancedPurple}{Benefits $\approx$ Risks} \hspace{2mm}
\colorbox{riskRed}{Risks $>$ Benefits} \hspace{2mm} \\
\textbf{Directionality:} 
$\uparrow$ Risk Increases with Autonomy \hspace{2mm}
$\downuparrows$ Risk May or May not Increase with Autonomy
}

\caption{Value-Risk Assessment Across Agent Autonomy Levels. Colors indicate benefit-risk balance, not absolute risk levels. Arrows show risk relationship with increasing autonomy.}
\label{tab:detailed-values}
\end{table*}


\end{document}
\endinput
%%
%% End of file `sample-manuscript.tex'.
