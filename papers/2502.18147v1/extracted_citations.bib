@misc{atpstar,
      title={AtP*: An efficient and scalable method for localizing LLM behaviour to components}, 
      author={J치nos Kram치r and Tom Lieberum and Rohin Shah and Neel Nanda},
      year={2024},
      eprint={2403.00745},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.00745}, 
}

@misc{bricken_monosemanticity_2023,
  title = {Towards {{Monosemanticity}}: {{Decomposing Language Models With Dictionary Learning}}},
  shorttitle = {Towards {{Monosemanticity}}},
  author = {Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nick and Anil, Cem and Denison, Carson and Askell, Amanda},
  year = {2023},
  journal = {Transformer Circuits Thread},
  url = {https://transformer-circuits.pub/2023/monosemantic-features},
  urldate = {2024-05-03}
}

@article{conmy_automated_2023,
  title = {Towards {{Automated Circuit Discovery}} for {{Mechanistic Interpretability}}},
  author = {Conmy, Arthur and {Mavor-Parker}, Augustine and Lynch, Aengus and Heimersheim, Stefan and {Garriga-Alonso}, Adri{\`a}},
  year = {2023},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {16318--16352},
  urldate = {2024-05-04},
  langid = {english}
}

@misc{cunningham_sparse_2023,
  title = {Sparse {{Autoencoders Find Highly Interpretable Features}} in {{Language Models}}},
  author = {Cunningham, Hoagy and Ewart, Aidan and Riggs, Logan and Huben, Robert and Sharkey, Lee},
  year = {2023},
  month = oct,
  number = {arXiv:2309.08600},
  eprint = {2309.08600},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.08600},
  urldate = {2024-04-03},
  archiveprefix = {arXiv}
}

@misc{dunefsky_transcoders_2024,
  title = {Transcoders {{Find Interpretable LLM Feature Circuits}}},
  author = {Dunefsky, Jacob and Chlenski, Philippe and Nanda, Neel},
  year = {2024},
  month = jun,
  number = {arXiv:2406.11944},
  eprint = {2406.11944},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.11944},
  urldate = {2024-08-02},
  archiveprefix = {arXiv}
}

@misc{gao_scaling_2024,
  title = {Scaling and evaluating sparse autoencoders},
  author = {Gao, Leo and {la Tour}, Tom Dupr{\'e} and Tillman, Henk and Goh, Gabriel and Troll, Rajan and Radford, Alec and Sutskever, Ilya and Leike, Jan and Wu, Jeffrey},
  year = {2024},
  month = jun,
  number = {arXiv:2406.04093},
  eprint = {2406.04093},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.04093},
  urldate = {2024-07-27},
  archiveprefix = {arXiv}
}

@article{hanna_how_2023,
  title = {How does {{GPT-2}} compute greater-than?: {{Interpreting}} mathematical abilities in a pre-trained language model},
  shorttitle = {How does {{GPT-2}} compute greater-than?},
  author = {Hanna, Michael and Liu, Ollie and Variengien, Alexandre},
  year = {2023},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {76033--76060},
  urldate = {2024-05-04},
  langid = {english}
}

@misc{lieberum_gemma_2024,
  title = {Gemma {{Scope}}: {{Open Sparse Autoencoders Everywhere All At Once}} on {{Gemma}} 2},
  shorttitle = {Gemma {{Scope}}},
  author = {Lieberum, Tom and Rajamanoharan, Senthooran and Conmy, Arthur and Smith, Lewis and Sonnerat, Nicolas and Varma, Vikrant and Kram{\'a}r, J{\'a}nos and Dragan, Anca and Shah, Rohin and Nanda, Neel},
  year = {2024},
  month = aug,
  number = {arXiv:2408.05147},
  eprint = {2408.05147},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.05147},
  urldate = {2024-08-24},
  archiveprefix = {arXiv}
}

@inproceedings{makelov_sparse_2024,
  title = {Sparse {{Autoencoders Match Supervised Features}} for {{Model Steering}} on the {{IOI Task}}},
  booktitle = {{{ICML}} 2024 {{Workshop}} on {{Mechanistic Interpretability}}},
  author = {Makelov, Aleksandar},
  year = {2024},
  month = jun,
  url = {https://openreview.net/forum?id=JdrVuEQih5},
  urldate = {2024-08-22},
  langid = {english}
}

@misc{marks_sparse_2024,
  title = {Sparse {{Feature Circuits}}: {{Discovering}} and {{Editing Interpretable Causal Graphs}} in {{Language Models}}},
  shorttitle = {Sparse {{Feature Circuits}}},
  author = {Marks, Samuel and Rager, Can and Michaud, Eric J. and Belinkov, Yonatan and Bau, David and Mueller, Aaron},
  year = {2024},
  month = mar,
  number = {arXiv:2403.19647},
  eprint = {2403.19647},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arxiv.2403.19647},
  urldate = {2024-04-20},
  archiveprefix = {arXiv}
}

@article{meng_locating_2022,
  title = {Locating and {{Editing Factual Associations}} in {{GPT}}},
  author = {Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  year = {2022},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {17359--17372},
  urldate = {2025-01-22},
  langid = {english}
}

@misc{nanda_attribution_2023,
  title = {Attribution {{Patching}}: {{Activation Patching At Industrial Scale}}},
  shorttitle = {Attribution {{Patching}}},
  author = {Nanda, Neel},
  year = {2023},
  month = feb,
  journal = {Neel Nanda},
  url = {https://www.neelnanda.io/mechanistic-interpretability/attribution-patching},
  urldate = {2025-01-22},
  langid = {american}
}

@misc{obrien_steering_2024,
  title = {Steering {{Language Model Refusal}} with {{Sparse Autoencoders}}},
  author = {O'Brien, Kyle and Majercak, David and Fernandes, Xavier and Edgar, Richard and Chen, Jingya and Nori, Harsha and Carignan, Dean and Horvitz, Eric and {Poursabzi-Sangde}, Forough},
  year = {2024},
  month = nov,
  number = {arXiv:2411.11296},
  eprint = {2411.11296},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.11296},
  urldate = {2025-01-27},
  archiveprefix = {arXiv}
}

@article{olah_zoom_2020,
  title = {Zoom {{In}}: {{An Introduction}} to {{Circuits}}},
  shorttitle = {Zoom {{In}}},
  author = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  year = {2020},
  month = mar,
  journal = {Distill},
  volume = {5},
  number = {3},
  issn = {2476-0757},
  doi = {10.23915/distill.00024.001},
  urldate = {2024-04-03},
  langid = {english}
}

@misc{olsson_context_2022,
  title = {In-context {{Learning}} and {{Induction Heads}}},
  author = {Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Ganguli, Deep and {Hatfield-Dodds}, Zac and Hernandez, Danny and Johnston, Scott and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
  year = {2022},
  month = sep,
  number = {arXiv:2209.11895},
  eprint = {2209.11895},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2209.11895},
  urldate = {2024-04-03},
  archiveprefix = {arXiv}
}

@inproceedings{rajamanoharan_improving_2024,
  title = {Improving {{Sparse Decomposition}} of {{Language Model Activations}} with {{Gated Sparse Autoencoders}}},
  booktitle = {{{ICML}} 2024 {{Workshop}} on {{Mechanistic Interpretability}}},
  author = {Rajamanoharan, Senthooran and Conmy, Arthur and Smith, Lewis and Lieberum, Tom and Varma, Vikrant and Kramar, Janos and Shah, Rohin and Nanda, Neel},
  year = {2024},
  month = jun,
  url = {https://openreview.net/forum?id=Ppj5KvzU8Q},
  urldate = {2024-07-31},
  langid = {english}
}

@misc{rajamanoharan_jumping_2024,
    title = {Jumping {Ahead}: {Improving} {Reconstruction} {Fidelity} with {JumpReLU} {Sparse} {Autoencoders}},
    shorttitle = {Jumping {Ahead}},
    url = {http://arxiv.org/abs/2407.14435},
    doi = {10.48550/arXiv.2407.14435},
    abstract = {Sparse autoencoders (SAEs) are a promising unsupervised approach for identifying causally relevant and interpretable linear features in a language model's (LM) activations. To be useful for downstream tasks, SAEs need to decompose LM activations faithfully; yet to be interpretable the decomposition must be sparse -- two objectives that are in tension. In this paper, we introduce JumpReLU SAEs, which achieve state-of-the-art reconstruction fidelity at a given sparsity level on Gemma 2 9B activations, compared to other recent advances such as Gated and TopK SAEs. We also show that this improvement does not come at the cost of interpretability through manual and automated interpretability studies. JumpReLU SAEs are a simple modification of vanilla (ReLU) SAEs -- where we replace the ReLU with a discontinuous JumpReLU activation function -- and are similarly efficient to train and run. By utilising straight-through-estimators (STEs) in a principled manner, we show how it is possible to train JumpReLU SAEs effectively despite the discontinuous JumpReLU function introduced in the SAE's forward pass. Similarly, we use STEs to directly train L0 to be sparse, instead of training on proxies such as L1, avoiding problems like shrinkage.},
    urldate = {2024-08-01},
    publisher = {arXiv},
    author = {Rajamanoharan, Senthooran and Lieberum, Tom and Sonnerat, Nicolas and Conmy, Arthur and Varma, Vikrant and Kram치r, J치nos and Nanda, Neel},
    month = jul,
    year = {2024},
    note = {arXiv:2407.14435 [cs]},
    keywords = {Computer Science - Machine Learning},
}

@misc{sharkey_taking_2022,
  title = {Taking features out of superposition with sparse autoencoders},
  author = {Sharkey, Lee and Braun, Dan and Millidge, Beren},
  year = {2022},
  month = dec,
  urldate = {2024-05-09},
  langid = {english}
}

@inproceedings{syed_attribution_2024,
  title = {Attribution {{Patching Outperforms Automated Circuit Discovery}}},
  booktitle = {Proceedings of the 7th {{BlackboxNLP Workshop}}: {{Analyzing}} and {{Interpreting Neural Networks}} for {{NLP}}},
  author = {Syed, Aaquib and Rager, Can and Conmy, Arthur},
  editor = {Belinkov, Yonatan and Kim, Najoung and Jumelet, Jaap and Mohebbi, Hosein and Mueller, Aaron and Chen, Hanjie},
  year = {2024},
  month = nov,
  pages = {407--416},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, US},
  doi = {10.18653/v1/2024.blackboxnlp-1.25},
  urldate = {2025-01-22}
}

@misc{templeton_predicting_2024,
  title = {Predicting {{Future Activations}}},
  author = {Templeton, Adly and Batson, Joshua and Jermyn, Adam and Olah, Chris},
  year = {2024},
  month = jan,
  journal = {Transformer Circuits Thread},
  url = {https://transformer-circuits.pub/2024/jan-update/index.html#predict-future},
  urldate = {2025-01-27}
}

@misc{templeton_scaling_2024,
  title = {Scaling {{Monosemanticity}}: {{Extracting Interpretable Features}} from {{Claude}} 3 {{Sonnet}}},
  shorttitle = {Scaling {{Monosemanticity}}},
  author = {Templeton, Adly and Conerly, Tom and Marcus, Jonathan and Lindsey, Jack and Bricken, Trenton and Chen, Brian and Pearce, Adam and Citro, Craig and Ameisen, Emmanuel and Jones, Andy and Cunningham, Hoagy and Turner, Nicholas L. and McDougall, Callum and MacDiarmid, Monte and Tamkin, Alex and Durmus, Esin and Hume, Tristan and Mosconi, Francesco and Freeman, C. Daniel and Sumers, Theodore R. and Rees, Edward and Batson, Joshua and Jermyn, Adam and Carter, Shan and Olah, Chris and Henighan, Tom},
  year = {2024},
  month = may,
  journal = {Transformer Circuits Thread},
  url = {https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html},
  urldate = {2024-05-29}
}

@misc{wang_interpretability_2022,
  title = {Interpretability in the {{Wild}}: a {{Circuit}} for {{Indirect Object Identification}} in {{GPT-2}} small},
  shorttitle = {Interpretability in the {{Wild}}},
  author = {Wang, Kevin and Variengien, Alexandre and Conmy, Arthur and Shlegeris, Buck and Steinhardt, Jacob},
  year = {2022},
  month = nov,
  number = {arXiv:2211.00593},
  eprint = {2211.00593},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.00593},
  urldate = {2024-05-04},
  archiveprefix = {arXiv}
}

@inproceedings{zhang_best_2023,
  title = {Towards {{Best Practices}} of {{Activation Patching}} in {{Language Models}}: {{Metrics}} and {{Methods}}},
  shorttitle = {Towards {{Best Practices}} of {{Activation Patching}} in {{Language Models}}},
  booktitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  author = {Zhang, Fred and Nanda, Neel},
  year = {2023},
  month = oct,
  url = {https://openreview.net/forum?id=Hf17y6u9BC},
  urldate = {2025-01-22},
  langid = {english}
}

