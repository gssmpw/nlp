\section{Related work}
% \todo{make this more concise}

\subsection{Sparse autoencoders}

SAEs have been widely applied to `disentangle' the representations learned by transformer language models into a very large number of concepts, a.k.a. sparse latents, features, or dictionary elements
% There is a very large body of work introducing SAEs as a method for understanding ``concepts'', or sparse features in transformer activations 
____.
Human experiments and quantitative proxies apparently confirm that SAE latents are much more likely to correspond to human-interpretable concepts than raw language-model neurons, i.e., the basis dimensions of their activation vectors ____.
SAEs have been successfully applied to modifying the behavior of LLMs by using a direction discovered by an SAE to ``steer'' the model towards a certain concept ____.

% The most common application for SAEs is to `intervene' in the activations of the underlying model in terms of the dictionary elements (decoder weight vectors) learned by SAEs ____
% This is expected, given that SAEs find a basis for the underlying activations in terms of relatively interpretable latent concepts, each of which is active when the text input to the model expresses the corresponding concepts.
% By `steering' the activations, i.e., modifying them to artificially express the corresponding concepts to a greater or lesser degree, we can usefully alter the behavior of the language model, but these approaches provide limited insight into the form of computations that take place in the model.

% There is a body of work showing that it is possible to intervene on neural networks by exploiting the sparse basis learned by an SAE \todo{refs}.
% The success of these methods is perhaps not surprising: SAEs give a basis for network activations in terms of human-interpretable concepts, in the sense that directions in that basis are active when the input text contains the corresponding concept.
% Thus, if you intervene on the activations to push them in the direction corresponding to a particular concept, then it makes sense that the network will start to behave as if that concept was present in the input.
% While this approach clearly facilitates useful interventions on transformer activations, and validates the usefulness of SAE bases in practice, the depth of insights it can provide into the underlying computation is not clear.

Our work is based on SAEs but has a very different aim: standard SAEs only sparsify activations, while JSAEs also sparsify the computation graph between them (Figure~\ref{fig:schematic}).

\subsection{Transcoders}

In this paper, we focus on MLPs.
____ developed \emph{transcoders}, an alternative SAE-like method to understand MLPs. 
However, JSAEs and transcoders take radically different approaches and solve radically different problems.
This is perhaps easiest to see if we look at what transcoders and JSAEs sparsify.
JSAEs are fundamentally an extension of standard SAEs: they train SAEs at the input and output of the MLP and add an extra term to the objective such that these sparse latents are also appropriate for interpreting the MLP (Figure~\ref{fig:schematic}).
In contrast, transcoders do not sparsify the inputs and outputs; they work with dense inputs and outputs.
Instead, transcoders, in essence, sparsify the MLP hidden states.
Specifically, a transcoder is an MLP that you train to match (using a mean squared error objective) the input-to-output mapping of the underlying MLP from the transformer.
The key difference between the transcoder MLP and the underlying MLP is that the transcoder MLP is much wider, and its hidden layer is trained to be sparse.

Thus, transcoders and JSAEs take fundamentally different approaches.
Each transcoder latent tells us `there is computation in the MLP related to [concept].'
By comparison, JSAEs learn a pair of SAEs (which have mostly interpretable latents) and sparse connections between them.
At a conceptual level, JSAEs tell us that `this feature in the MLP's output was computed using only these few input features'.
Ultimately, we believe that the JSAE approach, grounded in understanding how the SAE basis at one layer is mapped to the SAE basis at another layer, is potentially powerful and worth thoroughly exploring.

Importantly, it is worth emphasizing that JSAEs and transcoders are asking fundamentally different questions, as can be seen in terms of e.g., differences in what they sparsify.  
As such, it is not, to our knowledge, possible to design meaningful quantitative comparisons, at least not without extensive future work to develop very general auto-interpretability methods for evaluating methods of understanding MLP circuits.

%Of course, JSAEs also take an SAE-based approach to understand MLPs.
%
%These differences turn up at two levels: what transcoders / JSAEs sparsify, and what objective they use.
%First, JSAEs find a sparse basis for the input and output of the MLP, and abstract away the hidden layer. 
%In contrast, transcoders do not find a sparse basis for the input and output; instead, they find a sparse basis for the hidden layer.
%Second, transcoders train this sparse basis so that the transcoder MLP matches the MLP in the underlying transformer; they do not e.g. train the sparse basis to reconstruct the hiddens themselves.
%In contrast, JSAEs do not in any sense training anything to match the transcoder MLP.
%Instead, they train sparse bases for the input and output exactly as in a traditional SAE setup.
%In fact, if you take a JSAE, and set the coefficient for the Jacobian loss term to zero, that is precisely what you get: two SAEs trained on the input and output of the MLP.
%
%
%Ultimately, it only makes sense to compare JSAEs and transcoders in terms of how we should be trying to interpret the MLP.

\subsection{Automated circuit discovery}

In ``automated circuit discovery'', the goal is to isolate the causally relevant intermediate variables and connections between them necessary for a neural network to perform a given task ____.
In this context, a circuit is defined as a computational subgraph with an interpretable function.
The causal connections between elements are determined via activation patching, i.e., modifying or replacing the activations at a particular site of the model ____.
% For example, ____ identified the elements of GPT-2 small needed to perform indirect object identification, and ____ analyzed the mathematical `greater-than' operation.
In some cases, researchers have identified sub-components of transformer language models with simple algorithmic roles that appear to generalize across models ____.
% While progress has been made on this front, the complexity of even small transformers makes circuit analysis onerous.

____ proposed a means to automatically prune the connections between the sub-components of a neural network to the most relevant for a given task using activation patching.
Given a choice of task (i.e., a dataset and evaluation metric), this approach to automated circuit discovery (ACDC) returns a minimal computational subgraph needed to implement the task, e.g., previously identified `circuits' like ____.
Naturally, this is computationally expensive, leading other authors to explore using linear approximations to activation patching ____.
% ____ introduced the use of gradient information to make linear approximations to activation patching, termed attribution patching, which is significantly cheaper to compute.
% ____ applied this technique to circuit discovery, calling it edge attribution patching (EAP) and recommending that it precedes ACDC ____.
____ later improved on this technique by using SAE latents as the nodes in the computational graph.

In a sense, these methods are supervised because they require the user to specify a task.
Naturally, it is not feasible to manually iterate over all tasks an LLM can perform, so a fully unsupervised approach is desirable.
With JSAEs, we take a step towards resolving this problem, although the architecture introduced in this paper initially only applies to a single MLP layer and not an entire model.
Additionally, to the best of our knowledge, no automated circuit discovery algorithm sparsifies the computations inside of MLPs.
% \todo{ACDC-style things typically work with attn heads because MLPs are a superposed mess, JSAEs kinda address this}

%