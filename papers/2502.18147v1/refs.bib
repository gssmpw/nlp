@inproceedings{biderman_pythia_2023,
  title = {Pythia: {{A Suite}} for {{Analyzing Large Language Models Across Training}} and {{Scaling}}},
  shorttitle = {Pythia},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Machine Learning}}},
  author = {Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin Gregory and Bradley, Herbie and O'Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, Usvsn Sai and Raff, Edward and Skowron, Aviya and Sutawika, Lintang and Wal, Oskar Van Der},
  year = {2023},
  month = jul,
  pages = {2397--2430},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v202/biderman23a.html},
  urldate = {2024-08-24},
  langid = {english}
}

@misc{bills_language_2023,
  title = {Language models can explain neurons in language models},
  author = {Bills, Steven and Cammarata, Nick and Mossing, Dan and Tillman, Henk and Gao, Leo and Goh, Gabriel and Sutskever, Ilya and Leike, Jan and Wu, Jeff and Saunders, William},
  year = {2023},
  month = may,
  url = {https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html},
  urldate = {2024-09-11}
}

@misc{bloom_jbloomaus_2023,
   title = {{SAELens}},
   author = {Bloom, Joseph and Tigges, Curt and Chanin, David},
   year = {2024},
   howpublished = {\url{https://github.com/jbloomAus/SAELens}},
}

@misc{braun_identifying_2024,
  title = {Identifying {{Functionally Important Features}} with {{End-to-End Sparse Dictionary Learning}}},
  author = {Braun, Dan and Taylor, Jordan and {Goldowsky-Dill}, Nicholas and Sharkey, Lee},
  year = {2024},
  month = may,
  number = {arXiv:2405.12241},
  eprint = {2405.12241},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.12241},
  urldate = {2024-08-24},
  archiveprefix = {arXiv}
}

@inproceedings{braun_identifying_2024a,
  title = {Identifying {{Functionally Important Features}} with {{End-to-End Sparse Dictionary Learning}}},
  booktitle = {{{ICML}} 2024 {{Workshop}} on {{Mechanistic Interpretability}}},
  author = {Braun, Dan and Taylor, Jordan and {Goldowsky-Dill}, Nicholas and Sharkey, Lee},
  year = {2024},
  month = jun,
  url = {https://openreview.net/forum?id=bcV7rhBEcM},
  urldate = {2024-08-03},
  langid = {english}
}

@article{bushnaq_circuits_2024,
  title = {Circuits in {{Superposition}}: {{Compressing}} many small neural networks into one},
  shorttitle = {Circuits in {{Superposition}}},
  author = {Bushnaq, Lucius and Mendel, Jake},
  year = {2024},
  month = oct,
  url = {https://www.lesswrong.com/posts/roE7SHjFWEoMcGZKd/circuits-in-superposition-compressing-many-small-neural},
  urldate = {2024-12-20},
  langid = {english}
}

@misc{choi_scaling_2024,
  title = {Scaling {{Automatic Neuron Description}}},
  author = {Choi, Dami and Huang, Vincent and Meng, Kevin and Johnson, Daniel D. and Steinhardt, Jacob and Schwettmann, Sarah},
  year = {2024},
  month = oct,
  journal = {Transluce},
  url = {https://transluce.org/neuron-descriptions},
  urldate = {2024-12-26}
}

@article{conmy_automated_2023,
  title = {Towards {{Automated Circuit Discovery}} for {{Mechanistic Interpretability}}},
  author = {Conmy, Arthur and {Mavor-Parker}, Augustine and Lynch, Aengus and Heimersheim, Stefan and {Garriga-Alonso}, Adri{\`a}},
  year = {2023},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {16318--16352},
  urldate = {2024-05-04},
  langid = {english}
}

@misc{cunningham_sparse_2023,
  title = {Sparse {{Autoencoders Find Highly Interpretable Features}} in {{Language Models}}},
  author = {Cunningham, Hoagy and Ewart, Aidan and Riggs, Logan and Huben, Robert and Sharkey, Lee},
  year = {2023},
  month = oct,
  number = {arXiv:2309.08600},
  eprint = {2309.08600},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.08600},
  urldate = {2024-04-03},
  archiveprefix = {arXiv}
}

@misc{dunefsky_transcoders_2024,
  title = {Transcoders {{Find Interpretable LLM Feature Circuits}}},
  author = {Dunefsky, Jacob and Chlenski, Philippe and Nanda, Neel},
  year = {2024},
  month = jun,
  number = {arXiv:2406.11944},
  eprint = {2406.11944},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.11944},
  urldate = {2024-08-02},
  archiveprefix = {arXiv}
}

@misc{elhage_mathematical_2021,
  title = {A {{Mathematical Framework}} for {{Transformer Circuits}}},
  author = {Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, T. and Joseph, N. and Mann, B. and Askell, A. and Bai, Y. and Chen, A. and Conerly, T.},
  year = {2021},
  journal = {Transformer Circuits Thread},
  url = {https://transformer-circuits.pub/2021/framework/index.html},
  urldate = {2023-05-04}
}

@misc{elhage_toy_2022,
  title = {Toy {{Models}} of {{Superposition}}},
  author = {Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and {Hatfield-Dodds}, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and Grosse, Roger and McCandlish, Sam and Kaplan, Jared and Amodei, Dario and Wattenberg, Martin and Olah, Christopher},
  year = {2022},
  month = sep,
  number = {arXiv:2209.10652},
  eprint = {2209.10652},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2209.10652},
  urldate = {2024-04-03},
  archiveprefix = {arXiv}
}

@misc{gao_pile_2020,
  title = {The {{Pile}}: {{An 800GB Dataset}} of {{Diverse Text}} for {{Language Modeling}}},
  shorttitle = {The {{Pile}}},
  author = {Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
  year = {2020},
  month = dec,
  number = {arXiv:2101.00027},
  eprint = {2101.00027},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2101.00027},
  urldate = {2024-07-26},
  archiveprefix = {arXiv}
}

@misc{gao_scaling_2024,
  title = {Scaling and evaluating sparse autoencoders},
  author = {Gao, Leo and {la Tour}, Tom Dupr{\'e} and Tillman, Henk and Goh, Gabriel and Troll, Rajan and Radford, Alec and Sutskever, Ilya and Leike, Jan and Wu, Jeffrey},
  year = {2024},
  month = jun,
  number = {arXiv:2406.04093},
  eprint = {2406.04093},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.04093},
  urldate = {2024-07-27},
  archiveprefix = {arXiv}
}

@misc{hanni_mathematical_2024,
  title = {Mathematical {{Models}} of {{Computation}} in {{Superposition}}},
  author = {H{\"a}nni, Kaarel and Mendel, Jake and Vaintrob, Dmitry and Chan, Lawrence},
  year = {2024},
  month = aug,
  number = {arXiv:2408.05451},
  eprint = {2408.05451},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.05451},
  urldate = {2024-12-20},
  archiveprefix = {arXiv}
}

@article{hyvarinen_independent_2000,
  title = {Independent component analysis: algorithms and applications},
  shorttitle = {Independent component analysis},
  author = {Hyv{\"a}rinen, A. and Oja, E.},
  year = {2000},
  month = jun,
  journal = {Neural Networks},
  volume = {13},
  number = {4},
  pages = {411--430},
  issn = {0893-6080},
  doi = {10.1016/S0893-6080(00)00026-5},
  urldate = {2024-05-09}
}

@misc{jermyn_engineering_2022,
  title = {Engineering {{Monosemanticity}} in {{Toy Models}}},
  author = {Jermyn, Adam S. and Schiefer, Nicholas and Hubinger, Evan},
  year = {2022},
  month = nov,
  number = {arXiv:2211.09169},
  eprint = {2211.09169},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.09169},
  urldate = {2024-05-03},
  archiveprefix = {arXiv}
}

@inproceedings{karvonen_measuring_2024,
  title = {Measuring {{Progress}} in {{Dictionary Learning}} for {{Language Model Interpretability}} with {{Board Game Models}}},
  booktitle = {{{ICML}} 2024 {{Workshop}} on {{Mechanistic Interpretability}}},
  author = {Karvonen, Adam and Wright, Benjamin and Rager, Can and Angell, Rico and Brinkmann, Jannik and Smith, Logan Riggs and Verdun, Claudio Mayrink and Bau, David and Marks, Samuel},
  year = {2024},
  month = jun,
  url = {https://openreview.net/forum?id=qzsDKwGJyB},
  urldate = {2024-08-03},
  langid = {english}
}

@misc{lawson_residual_2024,
  title = {Residual {{Stream Analysis}} with {{Multi-Layer SAEs}}},
  author = {Lawson, Tim and Farnik, Lucy and Houghton, Conor and Aitchison, Laurence},
  year = {2024},
  month = oct,
  number = {arXiv:2409.04185},
  eprint = {2409.04185},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.04185},
  urldate = {2024-10-08},
  archiveprefix = {arXiv}
}

@misc{lieberum_gemma_2024,
  title = {Gemma {{Scope}}: {{Open Sparse Autoencoders Everywhere All At Once}} on {{Gemma}} 2},
  shorttitle = {Gemma {{Scope}}},
  author = {Lieberum, Tom and Rajamanoharan, Senthooran and Conmy, Arthur and Smith, Lewis and Sonnerat, Nicolas and Varma, Vikrant and Kram{\'a}r, J{\'a}nos and Dragan, Anca and Shah, Rohin and Nanda, Neel},
  year = {2024},
  month = aug,
  number = {arXiv:2408.05147},
  eprint = {2408.05147},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.05147},
  urldate = {2024-08-24},
  archiveprefix = {arXiv}
}

@misc{lindsey_sparse_2024,
  title = {Sparse {{Crosscoders}} for {{Cross-Layer Features}} and {{Model Diffing}}},
  author = {Lindsey, Jack and Templeton, Adly and Marcus, Jonathan and Conerly, Thomas and Batson, Joshua},
  year = {2024},
  month = oct,
  journal = {Transformer Circuits Thread},
  url = {https://transformer-circuits.pub/2024/crosscoders/index.html},
  urldate = {2024-12-14}
}

@misc{makhzani_ksparse_2014,
  title = {k-{{Sparse Autoencoders}}},
  author = {Makhzani, Alireza and Frey, Brendan},
  year = {2014},
  month = mar,
  number = {arXiv:1312.5663},
  eprint = {1312.5663},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1312.5663},
  urldate = {2024-06-07},
  archiveprefix = {arXiv}
}

@misc{marks_sparse_2024,
  title = {Sparse {{Feature Circuits}}: {{Discovering}} and {{Editing Interpretable Causal Graphs}} in {{Language Models}}},
  shorttitle = {Sparse {{Feature Circuits}}},
  author = {Marks, Samuel and Rager, Can and Michaud, Eric J. and Belinkov, Yonatan and Bau, David and Mueller, Aaron},
  year = {2024},
  month = mar,
  number = {arXiv:2403.19647},
  eprint = {2403.19647},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arxiv.2403.19647},
  urldate = {2024-04-20},
  archiveprefix = {arXiv}
}

@misc{ng_sparse_2011,
  title = {Sparse autoencoder},
  author = {Ng, Andrew},
  year = {2011},
  url = {https://graphics.stanford.edu/courses/cs233-21-spring/ReferencedPapers/SAE.pdf},
  urldate = {2024-04-03}
}

@article{olah_zoom_2020,
  title = {Zoom {{In}}: {{An Introduction}} to {{Circuits}}},
  shorttitle = {Zoom {{In}}},
  author = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  year = {2020},
  month = mar,
  journal = {Distill},
  volume = {5},
  number = {3},
  issn = {2476-0757},
  doi = {10.23915/distill.00024.001},
  urldate = {2024-04-03},
  langid = {english}
}

@article{olshausen_sparse_1997,
  title = {Sparse coding with an overcomplete basis set: {{A}} strategy employed by {{V1}}?},
  shorttitle = {Sparse coding with an overcomplete basis set},
  author = {Olshausen, Bruno A. and Field, David J.},
  year = {1997},
  month = dec,
  journal = {Vision Research},
  volume = {37},
  number = {23},
  pages = {3311--3325},
  issn = {0042-6989},
  doi = {10.1016/S0042-6989(97)00169-7},
  urldate = {2024-04-03}
}

@misc{sharkey_taking_2022,
  title = {Taking features out of superposition with sparse autoencoders},
  author = {Sharkey, Lee and Braun, Dan and Millidge, Beren},
  year = {2022},
  month = dec,
  urldate = {2024-05-09},
  langid = {english}
}

@misc{templeton_scaling_2024,
  title = {Scaling {{Monosemanticity}}: {{Extracting Interpretable Features}} from {{Claude}} 3 {{Sonnet}}},
  shorttitle = {Scaling {{Monosemanticity}}},
  author = {Templeton, Adly and Conerly, Tom and Marcus, Jonathan and Lindsey, Jack and Bricken, Trenton and Chen, Brian and Pearce, Adam and Citro, Craig and Ameisen, Emmanuel and Jones, Andy and Cunningham, Hoagy and Turner, Nicholas L. and McDougall, Callum and MacDiarmid, Monte and Tamkin, Alex and Durmus, Esin and Hume, Tristan and Mosconi, Francesco and Freeman, C. Daniel and Sumers, Theodore R. and Rees, Edward and Batson, Joshua and Jermyn, Adam and Carter, Shan and Olah, Chris and Henighan, Tom},
  year = {2024},
  month = may,
  journal = {Transformer Circuits Thread},
  url = {https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html},
  urldate = {2024-05-29}
}

@article{vaintrob_mathematical_2024,
  title = {Toward {{A Mathematical Framework}} for {{Computation}} in {{Superposition}}},
  author = {Vaintrob, Dmitry and {jake\_mendel} and Kaarel},
  year = {2024},
  month = jan,
  url = {https://www.lesswrong.com/posts/2roZtSr5TGmLjXMnT/toward-a-mathematical-framework-for-computation-in},
  urldate = {2024-12-20},
  langid = {english}
}

@inproceedings{vaswani_attention_2017,
  title = {Attention is {{All}} you {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  url = {https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
  urldate = {2024-05-11}
}

@misc{wang_interpretability_2022,
  title = {Interpretability in the {{Wild}}: a {{Circuit}} for {{Indirect Object Identification}} in {{GPT-2}} small},
  shorttitle = {Interpretability in the {{Wild}}},
  author = {Wang, Kevin and Variengien, Alexandre and Conmy, Arthur and Shlegeris, Buck and Steinhardt, Jacob},
  year = {2022},
  month = nov,
  number = {arXiv:2211.00593},
  eprint = {2211.00593},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.00593},
  urldate = {2024-05-04},
  archiveprefix = {arXiv}
}

@inproceedings{yun_transformer_2021,
  title = {Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors},
  shorttitle = {Transformer visualization via dictionary learning},
  booktitle = {Proceedings of {{Deep Learning Inside Out}} ({{DeeLIO}}): {{The}} 2nd {{Workshop}} on {{Knowledge Extraction}} and {{Integration}} for {{Deep Learning Architectures}}},
  author = {Yun, Zeyu and Chen, Yubei and Olshausen, Bruno and LeCun, Yann},
  editor = {Agirre, Eneko and Apidianaki, Marianna and Vuli{\'c}, Ivan},
  year = {2021},
  month = jun,
  pages = {1--10},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2021.deelio-1.1},
  urldate = {2024-08-24}
}

@misc{bricken_monosemanticity_2023,
  title = {Towards {{Monosemanticity}}: {{Decomposing Language Models With Dictionary Learning}}},
  shorttitle = {Towards {{Monosemanticity}}},
  author = {Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nick and Anil, Cem and Denison, Carson and Askell, Amanda},
  year = {2023},
  journal = {Transformer Circuits Thread},
  url = {https://transformer-circuits.pub/2023/monosemantic-features},
  urldate = {2024-05-03}
}

@article{olah_feature_2017,
  title = {Feature {{Visualization}}},
  author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
  year = {2017},
  month = nov,
  journal = {Distill},
  volume = {2},
  number = {11},
  issn = {2476-0757},
  doi = {10.23915/distill.00007},
  urldate = {2024-05-04},
  langid = {english}
}

@misc{rajamanoharan_jumping_2024,
    title = {Jumping {Ahead}: {Improving} {Reconstruction} {Fidelity} with {JumpReLU} {Sparse} {Autoencoders}},
    shorttitle = {Jumping {Ahead}},
    url = {http://arxiv.org/abs/2407.14435},
    doi = {10.48550/arXiv.2407.14435},
    abstract = {Sparse autoencoders (SAEs) are a promising unsupervised approach for identifying causally relevant and interpretable linear features in a language model's (LM) activations. To be useful for downstream tasks, SAEs need to decompose LM activations faithfully; yet to be interpretable the decomposition must be sparse -- two objectives that are in tension. In this paper, we introduce JumpReLU SAEs, which achieve state-of-the-art reconstruction fidelity at a given sparsity level on Gemma 2 9B activations, compared to other recent advances such as Gated and TopK SAEs. We also show that this improvement does not come at the cost of interpretability through manual and automated interpretability studies. JumpReLU SAEs are a simple modification of vanilla (ReLU) SAEs -- where we replace the ReLU with a discontinuous JumpReLU activation function -- and are similarly efficient to train and run. By utilising straight-through-estimators (STEs) in a principled manner, we show how it is possible to train JumpReLU SAEs effectively despite the discontinuous JumpReLU function introduced in the SAE's forward pass. Similarly, we use STEs to directly train L0 to be sparse, instead of training on proxies such as L1, avoiding problems like shrinkage.},
    urldate = {2024-08-01},
    publisher = {arXiv},
    author = {Rajamanoharan, Senthooran and Lieberum, Tom and Sonnerat, Nicolas and Conmy, Arthur and Varma, Vikrant and Kramár, János and Nanda, Neel},
    month = jul,
    year = {2024},
    note = {arXiv:2407.14435 [cs]},
    keywords = {Computer Science - Machine Learning},
}

@misc{paulo_automatically_2024,
  title = {Automatically {{Interpreting Millions}} of {{Features}} in {{Large Language Models}}},
  author = {Paulo, Gon{\c c}alo and Mallen, Alex and Juang, Caden and Belrose, Nora},
  year = {2024},
  month = oct,
  number = {arXiv:2410.13928},
  eprint = {2410.13928},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.13928},
  urldate = {2024-10-22},
  archiveprefix = {arXiv}
}

@misc{radford_language_2019,
  title = {Language {{Models}} are {{Unsupervised Multitask Learners}}},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year = {2019},
  url = {https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf},
  urldate = {2023-12-08},
  langid = {english}
}

@misc{kingma_adam_2017,
    title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
    shorttitle = {Adam},
    url = {http://arxiv.org/abs/1412.6980},
    doi = {10.48550/arXiv.1412.6980},
    abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
    urldate = {2024-08-07},
    publisher = {arXiv},
    author = {Kingma, Diederik P. and Ba, Jimmy},
    month = jan,
    year = {2017},
    note = {arXiv:1412.6980 [cs]},
    keywords = {Computer Science - Machine Learning},
}

@article{meng_locating_2022,
  title = {Locating and {{Editing Factual Associations}} in {{GPT}}},
  author = {Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  year = {2022},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {17359--17372},
  urldate = {2025-01-22},
  langid = {english}
}

@inproceedings{syed_attribution_2024,
  title = {Attribution {{Patching Outperforms Automated Circuit Discovery}}},
  booktitle = {Proceedings of the 7th {{BlackboxNLP Workshop}}: {{Analyzing}} and {{Interpreting Neural Networks}} for {{NLP}}},
  author = {Syed, Aaquib and Rager, Can and Conmy, Arthur},
  editor = {Belinkov, Yonatan and Kim, Najoung and Jumelet, Jaap and Mohebbi, Hosein and Mueller, Aaron and Chen, Hanjie},
  year = {2024},
  month = nov,
  pages = {407--416},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, US},
  doi = {10.18653/v1/2024.blackboxnlp-1.25},
  urldate = {2025-01-22}
}

@inproceedings{zhang_best_2023,
  title = {Towards {{Best Practices}} of {{Activation Patching}} in {{Language Models}}: {{Metrics}} and {{Methods}}},
  shorttitle = {Towards {{Best Practices}} of {{Activation Patching}} in {{Language Models}}},
  booktitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  author = {Zhang, Fred and Nanda, Neel},
  year = {2023},
  month = oct,
  url = {https://openreview.net/forum?id=Hf17y6u9BC},
  urldate = {2025-01-22},
  langid = {english}
}

@article{hanna_how_2023,
  title = {How does {{GPT-2}} compute greater-than?: {{Interpreting}} mathematical abilities in a pre-trained language model},
  shorttitle = {How does {{GPT-2}} compute greater-than?},
  author = {Hanna, Michael and Liu, Ollie and Variengien, Alexandre},
  year = {2023},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {76033--76060},
  urldate = {2024-05-04},
  langid = {english}
}

@misc{olsson_context_2022,
  title = {In-context {{Learning}} and {{Induction Heads}}},
  author = {Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Ganguli, Deep and {Hatfield-Dodds}, Zac and Hernandez, Danny and Johnston, Scott and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
  year = {2022},
  month = sep,
  number = {arXiv:2209.11895},
  eprint = {2209.11895},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2209.11895},
  urldate = {2024-04-03},
  archiveprefix = {arXiv}
}

@misc{nanda_attribution_2023,
  title = {Attribution {{Patching}}: {{Activation Patching At Industrial Scale}}},
  shorttitle = {Attribution {{Patching}}},
  author = {Nanda, Neel},
  year = {2023},
  month = feb,
  journal = {Neel Nanda},
  url = {https://www.neelnanda.io/mechanistic-interpretability/attribution-patching},
  urldate = {2025-01-22},
  langid = {american}
}

@article{olshausen_emergence_1996,
  title = {Emergence of simple-cell receptive field properties by learning a sparse code for natural images},
  author = {Olshausen, Bruno A. and Field, David J.},
  year = {1996},
  month = jun,
  journal = {Nature},
  volume = {381},
  number = {6583},
  pages = {607--609},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/381607a0},
  urldate = {2024-04-26},
  copyright = {1996 Springer Nature Limited},
  langid = {english}
}

@article{bell_informationmaximization_1995,
  title = {An {{Information-Maximization Approach}} to {{Blind Separation}} and {{Blind Deconvolution}}},
  author = {Bell, Anthony J. and Sejnowski, Terrence J.},
  year = {1995},
  month = nov,
  journal = {Neural Computation},
  volume = {7},
  number = {6},
  pages = {1129--1159},
  issn = {0899-7667},
  doi = {10.1162/neco.1995.7.6.1129},
  urldate = {2024-08-11}
}

@inproceedings{lee_efficient_2006,
  title = {Efficient sparse coding algorithms},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Lee, Honglak and Battle, Alexis and Raina, Rajat and Ng, Andrew},
  year = {2006},
  volume = {19},
  publisher = {MIT Press},
  url = {https://proceedings.neurips.cc/paper_files/paper/2006/hash/2d71b2ae158c7c5912cc0bbde2bb9d95-Abstract.html},
  urldate = {2024-04-03}
}

@misc{heap_sparse_2025,
      title={Sparse Autoencoders Can Interpret Randomly Initialized Transformers}, 
      author={Thomas Heap and Tim Lawson and Lucy Farnik and Laurence Aitchison},
      year={2025},
      eprint={2501.17727},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2501.17727}, 
}

@misc{elhage_privileged_2023,
  title = {Privileged bases in the transformer residual stream},
  author = {Elhage, Nelson and Lasenby, Robert and Olah, Christopher},
  year = {2023},
  journal = {Transformer Circuits Thread},
  url = {https://transformer-circuits.pub/2023/privileged-basis/index.html},
  urldate = {2023-05-04}
}

@misc{he_dictionary_2024,
  title = {Dictionary {{Learning Improves Patch-Free Circuit Discovery}} in {{Mechanistic Interpretability}}: {{A Case Study}} on {{Othello-GPT}}},
  shorttitle = {Dictionary {{Learning Improves Patch-Free Circuit Discovery}} in {{Mechanistic Interpretability}}},
  author = {He, Zhengfu and Ge, Xuyang and Tang, Qiong and Sun, Tianxiang and Cheng, Qinyuan and Qiu, Xipeng},
  year = {2024},
  month = feb,
  number = {arXiv:2402.12201},
  eprint = {2402.12201},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.12201},
  urldate = {2024-05-07},
  archiveprefix = {arXiv}
}

@inproceedings{makelov_sparse_2024,
  title = {Sparse {{Autoencoders Match Supervised Features}} for {{Model Steering}} on the {{IOI Task}}},
  booktitle = {{{ICML}} 2024 {{Workshop}} on {{Mechanistic Interpretability}}},
  author = {Makelov, Aleksandar},
  year = {2024},
  month = jun,
  url = {https://openreview.net/forum?id=JdrVuEQih5},
  urldate = {2024-08-22},
  langid = {english}
}

@inproceedings{mayne_can_2024a,
  title = {Can sparse autoencoders be used to decompose and interpret steering vectors?},
  booktitle = {Interpretable {{AI}}: {{Past}}, {{Present}} and {{Future}}},
  author = {Mayne, Harry and Yang, Yushi and Mahdi, Adam},
  year = {2024},
  month = dec,
  url = {https://openreview.net/forum?id=6VGkENHc1J},
  urldate = {2024-12-06},
  langid = {english}
}

@misc{obrien_steering_2024,
  title = {Steering {{Language Model Refusal}} with {{Sparse Autoencoders}}},
  author = {O'Brien, Kyle and Majercak, David and Fernandes, Xavier and Edgar, Richard and Chen, Jingya and Nori, Harsha and Carignan, Dean and Horvitz, Eric and {Poursabzi-Sangde}, Forough},
  year = {2024},
  month = nov,
  number = {arXiv:2411.11296},
  eprint = {2411.11296},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.11296},
  urldate = {2025-01-27},
  archiveprefix = {arXiv}
}

@misc{oneill_sparse_2024,
  title = {Sparse {{Autoencoders Enable Scalable}} and {{Reliable Circuit Identification}} in {{Language Models}}},
  author = {O'Neill, Charles and Bui, Thang},
  year = {2024},
  month = may,
  number = {arXiv:2405.12522},
  eprint = {2405.12522},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.12522},
  urldate = {2024-07-31},
  archiveprefix = {arXiv}
}

@misc{templeton_predicting_2024,
  title = {Predicting {{Future Activations}}},
  author = {Templeton, Adly and Batson, Joshua and Jermyn, Adam and Olah, Chris},
  year = {2024},
  month = jan,
  journal = {Transformer Circuits Thread},
  url = {https://transformer-circuits.pub/2024/jan-update/index.html#predict-future},
  urldate = {2025-01-27}
}

@misc{erichson2019jumpreluretrofitdefensestrategy,
      title={JumpReLU: A Retrofit Defense Strategy for Adversarial Attacks}, 
      author={N. Benjamin Erichson and Zhewei Yao and Michael W. Mahoney},
      year={2019},
      eprint={1904.03750},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/1904.03750}, 
}

@misc{absorption,
      title={A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders}, 
      author={David Chanin and James Wilken-Smith and Tomáš Dulka and Hardik Bhatnagar and Joseph Bloom},
      year={2024},
      eprint={2409.14507},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.14507}, 
}

@misc{atpstar,
      title={AtP*: An efficient and scalable method for localizing LLM behaviour to components}, 
      author={János Kramár and Tom Lieberum and Rohin Shah and Neel Nanda},
      year={2024},
      eprint={2403.00745},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.00745}, 
}

@inproceedings{rajamanoharan_improving_2024,
  title = {Improving {{Sparse Decomposition}} of {{Language Model Activations}} with {{Gated Sparse Autoencoders}}},
  booktitle = {{{ICML}} 2024 {{Workshop}} on {{Mechanistic Interpretability}}},
  author = {Rajamanoharan, Senthooran and Conmy, Arthur and Smith, Lewis and Lieberum, Tom and Varma, Vikrant and Kramar, Janos and Shah, Rohin and Nanda, Neel},
  year = {2024},
  month = jun,
  url = {https://openreview.net/forum?id=Ppj5KvzU8Q},
  urldate = {2024-07-31},
  langid = {english}
}

@misc{kissane2024interpretingattentionlayeroutputs,
      title={Interpreting Attention Layer Outputs with Sparse Autoencoders}, 
      author={Connor Kissane and Robert Krzyzanowski and Joseph Isaac Bloom and Arthur Conmy and Neel Nanda},
      year={2024},
      eprint={2406.17759},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.17759}, 
}

@misc{farrell2024applyingsparseautoencodersunlearn,
      title={Applying sparse autoencoders to unlearn knowledge in language models}, 
      author={Eoin Farrell and Yeu-Tong Lau and Arthur Conmy},
      year={2024},
      eprint={2410.19278},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.19278}, 
}

@misc{balcells2024evolutionsaefeatureslayers,
      title={Evolution of SAE Features Across Layers in LLMs}, 
      author={Daniel Balcells and Benjamin Lerner and Michael Oesterle and Ediz Ucar and Stefan Heimersheim},
      year={2024},
      eprint={2410.08869},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.08869}, 
}

@misc{lan2024sparseautoencodersrevealuniversal,
      title={Sparse Autoencoders Reveal Universal Feature Spaces Across Large Language Models}, 
      author={Michael Lan and Philip Torr and Austin Meek and Ashkan Khakzar and David Krueger and Fazl Barez},
      year={2024},
      eprint={2410.06981},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.06981}, 
}

@misc{brinkmann2025largelanguagemodelsshare,
      title={Large Language Models Share Representations of Latent Grammatical Concepts Across Typologically Diverse Languages}, 
      author={Jannik Brinkmann and Chris Wendler and Christian Bartelt and Aaron Mueller},
      year={2025},
      eprint={2501.06346},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.06346}, 
}
@misc{spies2024transformersusecausalworld,
      title={Transformers Use Causal World Models in Maze-Solving Tasks}, 
      author={Alex F. Spies and William Edwards and Michael I. Ivanitskiy and Adrians Skapars and Tilman Räuker and Katsumi Inoue and Alessandra Russo and Murray Shanahan},
      year={2024},
      eprint={2412.11867},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2412.11867}, 
}

@article{cammarata_thread_2020,
  title = {Thread: {{Circuits}}},
  shorttitle = {Thread},
  author = {Cammarata, Nick and Carter, Shan and Goh, Gabriel and Olah, Chris and Petrov, Michael and Schubert, Ludwig and Voss, Chelsea and Egan, Ben and Lim, Swee Kiat},
  year = {2020},
  month = mar,
  journal = {Distill},
  volume = {5},
  number = {3},
  issn = {2476-0757},
  doi = {10.23915/distill.00024},
  urldate = {2024-04-03},
  langid = {english}
}

@article{raffel_exploring_2020,
author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
title = {Exploring the limits of transfer learning with a unified text-to-text transformer},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pretraining objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new "Colossal Clean Crawled Corpus", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {140},
numpages = {67},
keywords = {transfer learning, natural language processing, multi-task learning, attention based models, deep learning}
}

@misc{dauphin2017languagemodelinggatedconvolutional,
      title={Language Modeling with Gated Convolutional Networks}, 
      author={Yann N. Dauphin and Angela Fan and Michael Auli and David Grangier},
      year={2017},
      eprint={1612.08083},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1612.08083}, 
}

@misc{shazeer2020gluvariantsimprovetransformer,
      title={GLU Variants Improve Transformer}, 
      author={Noam Shazeer},
      year={2020},
      eprint={2002.05202},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2002.05202}, 
}