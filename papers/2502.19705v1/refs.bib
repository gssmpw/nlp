@InProceedings{Yan_2021_CVPR,
    author    = {Yan, Bin and Peng, Houwen and Wu, Kan and Wang, Dong and Fu, Jianlong and Lu, Huchuan},
    title     = {LightTrack: Finding Lightweight Neural Networks for Object Tracking via One-Shot Architecture Search},
    booktitle = {CVPR},
    year      = {2021},
    pages     = {15180-15189}
}

@InProceedings{10.1007/978-3-031-20047-2_37,
author="Borsuk, Vasyl
and Vei, Roman
and Kupyn, Orest
and Martyniuk, Tetiana
and Krashenyi, Igor
and Matas, Ji{\v{r}}i",
title="FEAR: Fast, Efficient, Accurate and Robust Visual Tracker",
booktitle="ECCV",
year="2022",
pages="644--663",
abstract="We present FEAR, a family of fast, efficient, accurate, and robust Siamese visual trackers. We present a novel and efficient way to benefit from dual-template representation for object model adaption, which incorporates temporal information with only a single learnable parameter. We further improve the tracker architecture with a pixel-wise fusion block. By plugging-in sophisticated backbones with the abovementioned modules, FEAR-M and FEAR-L trackers surpass most Siamese trackers on several academic benchmarks in both accuracy and efficiency. Employed with the lightweight backbone, the optimized version FEAR-XS offers more than 10 times faster tracking than current Siamese trackers while maintaining near state-of-the-art results. FEAR-XS tracker is 2.4x smaller and 4.3x faster than LightTrack with superior accuracy. In addition, we expand the definition of the model efficiency by introducing FEAR benchmark that assesses energy consumption and execution speed. We show that energy consumption is a limiting factor for trackers on mobile devices. Source code, pretrained models, and evaluation protocol are available at https://github.com/PinataFarms/FEARTracker.",
isbn="978-3-031-20047-2"
}

@inproceedings{gopal2023mobile,
author    = {Goutam Yelluru Gopal and Maria Amer},
title     = {Mobile Vision Transformer-based Visual Object Tracking},
booktitle = {BMVC},
year      = {2023},
url       = {https://papers.bmvc2023.org/0800.pdf}
}

@InProceedings{Li_2018_CVPR,
author = {Li, Bo and Yan, Junjie and Wu, Wei and Zhu, Zheng and Hu, Xiaolin},
title = {High Performance Visual Tracking With Siamese Region Proposal Network},
booktitle = {CVPR},
year = {2018}
}

@INPROCEEDINGS{9561948,
  author={Han, Zhichao and Zhang, Ruibin and Pan, Neng and Xu, Chao and Gao, Fei},
  booktitle={ICRA}, 
  title={Fast-Tracker: A Robust Aerial System for Tracking Agile Target in Cluttered Environments}, 
  year={2021},
  volume={},
  number={},
  pages={328-334},
  keywords={Target tracking;Systematics;Trajectory planning;Prediction methods;Object detection;Benchmark testing;Unmanned aerial vehicles},
  doi={10.1109/ICRA48506.2021.9561948}
}

@inproceedings{he2020momentum,
  title={Momentum contrast for unsupervised visual representation learning},
  author={He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  booktitle={CVPR},
  pages={9729--9738},
  year={2020}
}

@inproceedings{chen2020simple,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1597--1607},
  year={2020},
  organization={PMLR}
}

@inproceedings{sahin2023hoot,
  title={HOOT: Heavy occlusions in object tracking benchmark},
  author={Sahin, Gozde and Itti, Laurent},
  booktitle={WACV},
  pages={4830--4839},
  year={2023}
}

@InProceedings{Yan_2021_CVPR_Alpha_Refine,
    author    = {Yan, Bin and Zhang, Xinyu and Wang, Dong and Lu, Huchuan and Yang, Xiaoyun},
    title     = {Alpha-Refine: Boosting Tracking Performance by Precise Bounding Box Estimation},
    booktitle = {CVPR},
    year      = {2021},
    pages     = {5289-5298}
}

@InProceedings{Li_2019_CVPR,
author = {Li, Bo and Wu, Wei and Wang, Qiang and Zhang, Fangyi and Xing, Junliang and Yan, Junjie},
title = {SiamRPN++: Evolution of Siamese Visual Tracking With Very Deep Networks},
booktitle = {CVPR},
year = {2019}
}

@InProceedings{Danelljan_2017_ECO,
author = {Danelljan, Martin and Bhat, Goutam and Shahbaz Khan, Fahad and Felsberg, Michael},
title = {ECO: Efficient Convolution Operators for Tracking},
booktitle = {CVPR},
year = {2017}
}

@InProceedings{Cao_2021_ICCV_HiFT,
    author    = {Cao, Ziang and Fu, Changhong and Ye, Junjie and Li, Bowen and Li, Yiming},
    title     = {HiFT: Hierarchical Feature Transformer for Aerial Tracking},
    booktitle = {ICCV},
    year      = {2021},
    pages     = {15457-15466}
}

@InProceedings{10.1007/978-3-031-25085-9_26,
author="Chen, Xin
and Kang, Ben
and Wang, Dong
and Li, Dongdong
and Lu, Huchuan",
title="Efficient Visual Tracking via Hierarchical Cross-Attention Transformer",
booktitle="ECCV 2022 Workshops",
year="2023",
pages="461--477",
abstract="In recent years, target tracking has made great progress in accuracy. This development is mainly attributed to powerful networks (such as transformers) and additional modules (such as online update and refinement modules). However, less attention has been paid to tracking speed. Most state-of-the-art trackers are satisfied with the real-time speed on powerful GPUs. However, practical applications necessitate higher requirements for tracking speed, especially when edge platforms with limited resources are used. In this work, we present an efficient tracking method via a hierarchical cross-attention transformer named HCAT. Our model runs about 195 fps on GPU, 45 fps on CPU, and 55 fps on the edge AI platform of NVidia Jetson AGX Xavier. Experiments show that our HCAT achieves promising results on LaSOT, GOT-10k, TrackingNet, NFS, OTB100, UAV123, and VOT2020. Code and models are available at https://github.com/chenxin-dlut/HCAT.",
isbn="978-3-031-25085-9"
}


@InProceedings{Cao_2022_CVPR,
    author    = {Cao, Ziang and Huang, Ziyuan and Pan, Liang and Zhang, Shiwei and Liu, Ziwei and Fu, Changhong},
    title     = {TCTrack: Temporal Contexts for Aerial Tracking},
    booktitle = {CVPR},
    year      = {2022},
    pages     = {14798-14808}
}

@ARTICLE{10225683,
  author={{Cao, Ziang and Huang, Ziyuan and Pan, Liang and Zhang, Shiwei and Liu, Ziwei and Fu, Changhong}},
  journal={TPAMI}, 
  title={Towards Real-World Visual Tracking With Temporal Contexts}, 
  year={2023},
  volume={45},
  number={12},
  pages={15834-15849},
  keywords={Feature extraction;Tracking;Convolution;Transformers;Visualization;Task analysis;Decoding;Latency-aware evaluations;real-world tests;temporal contexts;two-level framework;visual tracking},
  doi={10.1109/TPAMI.2023.3307174}}

@InProceedings{Kang_2023_ICCV,
    author    = {Kang, Ben and Chen, Xin and Wang, Dong and Peng, Houwen and Lu, Huchuan},
    title     = {Exploring Lightweight Hierarchical Vision Transformers for Efficient Visual Tracking},
    booktitle = {ICCV},
    year      = {2023},
    pages     = {9612-9621}
}

@InProceedings{Blatter_2023_WACV,
    author    = {Blatter, Philippe and Kanakis, Menelaos and Danelljan, Martin and Van Gool, Luc},
    title     = {Efficient Visual Tracking With Exemplar Transformers},
    booktitle = {WACV},
    year      = {2023},
    pages     = {1571-1581}
}

@InProceedings{Danelljan_2019_CVPR_ATOM,
author = {Danelljan, Martin and Bhat, Goutam and Khan, Fahad Shahbaz and Felsberg, Michael},
title = {ATOM: Accurate Tracking by Overlap Maximization},
booktitle = {CVPR},
year = {2019}
}

@InProceedings{10.1007/978-3-030-58589-1_46,
author="Zhang, Zhipeng
and Peng, Houwen
and Fu, Jianlong
and Li, Bing
and Hu, Weiming",
title="Ocean: Object-Aware Anchor-Free Tracking",
booktitle="ECCV",
year="2020",
pages="771--787",
abstract="Anchor-based Siamese trackers have achieved remarkable advancements in accuracy, yet the further improvement is restricted by the lagged tracking robustness. We find the underlying reason is that the regression network in anchor-based methods is only trained on the positive anchor boxes (i.e., {\$}{\$}IoU{\backslash}ge 0.6{\$}{\$}IoU≥0.6). This mechanism makes it difficult to refine the anchors whose overlap with the target objects are small. In this paper, we propose a novel object-aware anchor-free network to address this issue. First, instead of refining the reference anchor boxes, we directly predict the position and scale of target objects in an anchor-free fashion. Since each pixel in groundtruth boxes is well trained, the tracker is capable of rectifying inexact predictions of target objects during inference. Second, we introduce a feature alignment module to learn an object-aware feature from predicted bounding boxes. The object-aware feature can further contribute to the classification of target objects and background. Moreover, we present a novel tracking framework based on the anchor-free model. The experiments show that our anchor-free tracker achieves state-of-the-art performance on five benchmarks, including VOT-2018, VOT-2019, OTB-100, GOT-10k and LaSOT. The source code is available at https://github.com/researchmm/TracKit.",
isbn="978-3-030-58589-1"
}

@InProceedings{Yan_2021_ICCV,
    author    = {Yan, Bin and Peng, Houwen and Fu, Jianlong and Wang, Dong and Lu, Huchuan},
    title     = {Learning Spatio-Temporal Transformer for Visual Tracking},
    booktitle = {ICCV},
    year      = {2021},
    pages     = {10448-10457}
}

@InProceedings{Cui_2022_CVPR,
    author    = {Cui, Yutao and Jiang, Cheng and Wang, Limin and Wu, Gangshan},
    title     = {MixFormer: End-to-End Tracking With Iterative Mixed Attention},
    booktitle = {CVPR},
    year      = {2022},
    pages     = {13608-13618}
}

@InProceedings{Chen_2023_CVPR,
    author    = {Chen, Xin and Peng, Houwen and Wang, Dong and Lu, Huchuan and Hu, Han},
    title     = {SeqTrack: Sequence to Sequence Learning for Visual Object Tracking},
    booktitle = {CVPR},
    year      = {2023},
    pages     = {14572-14581}
}

@inproceedings{Zheng_Zhong_Liang_Mo_Zhang_Li_2024,
  title={ODTrack: Online Dense Temporal Token Learning for Visual Tracking}, 
  author={Yaozong Zheng and Bineng Zhong and Qihua Liang and Zhiyi Mo and Shengping Zhang and Xianxian Li},
  booktitle={AAAI},
  year={2024}
}

@InProceedings{Fan_2019_CVPR,
author = {Fan, Heng and Lin, Liting and Yang, Fan and Chu, Peng and Deng, Ge and Yu, Sijia and Bai, Hexin and Xu, Yong and Liao, Chunyuan and Ling, Haibin},
title = {LaSOT: A High-Quality Benchmark for Large-Scale Single Object Tracking},
booktitle = {CVPR},
year = {2019}
}

@InProceedings{10.1007/978-3-319-46448-0_27,
author="Mueller, Matthias
and Smith, Neil
and Ghanem, Bernard",
title="A Benchmark and Simulator for UAV Tracking",
booktitle="ECCV",
year="2016",
pages="445--461",
abstract="In this paper, we propose a new aerial video dataset and benchmark for low altitude UAV target tracking, as well as, a photo-realistic UAV simulator that can be coupled with tracking methods. Our benchmark provides the first evaluation of many state-of-the-art and popular trackers on 123 new and fully annotated HD video sequences captured from a low-altitude aerial perspective. Among the compared trackers, we determine which ones are the most suitable for UAV tracking both in terms of tracking accuracy and run-time. The simulator can be used to evaluate tracking algorithms in real-time scenarios before they are deployed on a UAV ``in the field'', as well as, generate synthetic but photo-realistic tracking datasets with automatic ground truth annotations to easily extend existing real-world datasets. Both the benchmark and simulator are made publicly available to the vision community on our websiteto further research in the area of object tracking from UAVs. (https://ivul.kaust.edu.sa/Pages/pub-benchmark-simulator-uav.aspx.).",
isbn="978-3-319-46448-0"
}

@ARTICLE{7001050,
  author={Wu, Yi and Lim, Jongwoo and Yang, Ming-Hsuan},
  journal={TPAMI}, 
  title={Object Tracking Benchmark}, 
  year={2015},
  volume={37},
  number={9},
  pages={1834-1848},
  keywords={Target tracking;Object tracking;Algorithm design and analysis;Performance evaluation;Robustness;Histograms;Object tracking;benchmark dataset;performance evaluation;Object tracking;benchmark dataset;performance evaluation},
  doi={10.1109/TPAMI.2014.2388226}}

@InProceedings{Sandler_2018_CVPR,
author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
title = {MobileNetV2: Inverted Residuals and Linear Bottlenecks},
booktitle = {CVPR},
year = {2018}
}

@InProceedings{10.1007/978-3-319-48881-3_56,
author="Bertinetto, Luca
and Valmadre, Jack
and Henriques, Jo{\~a}o F.
and Vedaldi, Andrea
and Torr, Philip H. S.",
title="Fully-Convolutional Siamese Networks for Object Tracking",
booktitle="ECCV 2016 Workshops",
year="2016",
pages="850--865",
abstract="The problem of arbitrary object tracking has traditionally been tackled by learning a model of the object's appearance exclusively online, using as sole training data the video itself. Despite the success of these methods, their online-only approach inherently limits the richness of the model they can learn. Recently, several attempts have been made to exploit the expressive power of deep convolutional networks. However, when the object to track is not known beforehand, it is necessary to perform Stochastic Gradient Descent online to adapt the weights of the network, severely compromising the speed of the system. In this paper we equip a basic tracking algorithm with a novel fully-convolutional Siamese network trained end-to-end on the ILSVRC15 dataset for object detection in video. Our tracker operates at frame-rates beyond real-time and, despite its extreme simplicity, achieves state-of-the-art performance in multiple benchmarks.",
isbn="978-3-319-48881-3"
}

@InProceedings{Wu_2021_CVPR,
    author    = {Wu, Qiangqiang and Wan, Jia and Chan, Antoni B.},
    title     = {Progressive Unsupervised Learning for Visual Object Tracking},
    booktitle = {CVPR},
    year      = {2021},
    pages     = {2993-3002}
}

@InProceedings{10.1007/978-3-031-20047-2_25,
author="Pi, Zhixiong
and Wan, Weitao
and Sun, Chong
and Gao, Changxin
and Sang, Nong
and Li, Chen",
title="Hierarchical Feature Embedding for Visual Tracking",
booktitle="ECCV",
year="2022",
pages="428--445",
abstract="Features extracted by existing tracking methods may contain instance- and category-level information. However, it usually occurs that either instance- or category-level information uncontrollably dominates the feature embeddings depending on the training data distribution, since the two types of information are not explicitly modeled. A more favorable way is to produce features that emphasize both types of information in visual tracking. To achieve this, we propose a hierarchical feature embedding model which separately learns the instance and category information, and progressively embeds them.",
isbn="978-3-031-20047-2"
}

@InProceedings{Zhang_2021_CVPR,
    author    = {Zhang, Han and Koh, Jing Yu and Baldridge, Jason and Lee, Honglak and Yang, Yinfei},
    title     = {Cross-Modal Contrastive Learning for Text-to-Image Generation},
    booktitle = {CVPR},
    year      = {2021},
    pages     = {833-842}
}

@INPROCEEDINGS{10219612,
  author={Zeng, Dan and Zou, Mingliang and Wang, Xucheng and Li, Shuiwang},
  booktitle={ICME}, 
  title={Towards Discriminative Representations with Contrastive Instances for Real-Time UAV Tracking}, 
  year={2023},
  volume={},
  number={},
  pages={1349-1354},
  keywords={Deep learning;Correlation;Annotations;Manuals;Benchmark testing;Performance gain;Autonomous aerial vehicles;UAV tracking;Discriminative representation;Contrastive learning;Contrastive Instances},
  doi={10.1109/ICME55011.2023.00234}}

@InProceedings{10.1007/978-3-319-10602-1_48,
author="Lin, Tsung-Yi
and Maire, Michael
and Belongie, Serge
and Hays, James
and Perona, Pietro
and Ramanan, Deva
and Doll{\'a}r, Piotr
and Zitnick, C. Lawrence",
title="Microsoft COCO: Common Objects in Context",
booktitle="ECCV",
year="2014",
pages="740--755",
abstract="We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.",
isbn="978-3-319-10602-1"
}

@ARTICLE{8922619,
  author={Huang, Lianghua and Zhao, Xin and Huang, Kaiqi},
  journal={TPAMI}, 
  title={GOT-10k: A Large High-Diversity Benchmark for Generic Object Tracking in the Wild}, 
  year={2021},
  volume={43},
  number={5},
  pages={1562-1577},
  keywords={Training;Object tracking;Databases;Protocols;Benchmark testing;Servers;Object tracking;benchmark dataset;performance evaluation},
  doi={10.1109/TPAMI.2019.2957464}}

@InProceedings{Muller_2018_ECCV,
author = {Muller, Matthias and Bibi, Adel and Giancola, Silvio and Alsubaihi, Salman and Ghanem, Bernard},
title = {TrackingNet: A Large-Scale Dataset and Benchmark for Object Tracking in the Wild},
booktitle = {ECCV},
year = {2018}
}

@ARTICLE{9259200,
  author={Hoffmann, João Eduardo and Tosso, Hilkija Gaïus and Santos, Max Mauro Dias and Justo, João Francisco and Malik, Asad Waqar and Rahman, Anis Ur},
  journal={IEEE Transactions on Intelligent Vehicles}, 
  title={Real-Time Adaptive Object Detection and Tracking for Autonomous Vehicles}, 
  year={2021},
  volume={6},
  number={3},
  pages={450-459},
  keywords={Real-time systems;Tracking;Object detection;Feature extraction;Computational modeling;Adaptation models;Predictive models;Artificial intelligence;adaptive systems and autonomous vehicles;object detection;computer vision},
  doi={10.1109/TIV.2020.3037928}}

