@InProceedings{10.1007/978-3-031-20047-2_25,
author="Pi, Zhixiong
and Wan, Weitao
and Sun, Chong
and Gao, Changxin
and Sang, Nong
and Li, Chen",
title="Hierarchical Feature Embedding for Visual Tracking",
booktitle="ECCV",
year="2022",
pages="428--445",
abstract="Features extracted by existing tracking methods may contain instance- and category-level information. However, it usually occurs that either instance- or category-level information uncontrollably dominates the feature embeddings depending on the training data distribution, since the two types of information are not explicitly modeled. A more favorable way is to produce features that emphasize both types of information in visual tracking. To achieve this, we propose a hierarchical feature embedding model which separately learns the instance and category information, and progressively embeds them.",
isbn="978-3-031-20047-2"
}

@InProceedings{10.1007/978-3-031-20047-2_37,
author="Borsuk, Vasyl
and Vei, Roman
and Kupyn, Orest
and Martyniuk, Tetiana
and Krashenyi, Igor
and Matas, Ji{\v{r}}i",
title="FEAR: Fast, Efficient, Accurate and Robust Visual Tracker",
booktitle="ECCV",
year="2022",
pages="644--663",
abstract="We present FEAR, a family of fast, efficient, accurate, and robust Siamese visual trackers. We present a novel and efficient way to benefit from dual-template representation for object model adaption, which incorporates temporal information with only a single learnable parameter. We further improve the tracker architecture with a pixel-wise fusion block. By plugging-in sophisticated backbones with the abovementioned modules, FEAR-M and FEAR-L trackers surpass most Siamese trackers on several academic benchmarks in both accuracy and efficiency. Employed with the lightweight backbone, the optimized version FEAR-XS offers more than 10 times faster tracking than current Siamese trackers while maintaining near state-of-the-art results. FEAR-XS tracker is 2.4x smaller and 4.3x faster than LightTrack with superior accuracy. In addition, we expand the definition of the model efficiency by introducing FEAR benchmark that assesses energy consumption and execution speed. We show that energy consumption is a limiting factor for trackers on mobile devices. Source code, pretrained models, and evaluation protocol are available at https://github.com/PinataFarms/FEARTracker.",
isbn="978-3-031-20047-2"
}

@InProceedings{10.1007/978-3-319-48881-3_56,
author="Bertinetto, Luca
and Valmadre, Jack
and Henriques, Jo{\~a}o F.
and Vedaldi, Andrea
and Torr, Philip H. S.",
title="Fully-Convolutional Siamese Networks for Object Tracking",
booktitle="ECCV 2016 Workshops",
year="2016",
pages="850--865",
abstract="The problem of arbitrary object tracking has traditionally been tackled by learning a model of the object's appearance exclusively online, using as sole training data the video itself. Despite the success of these methods, their online-only approach inherently limits the richness of the model they can learn. Recently, several attempts have been made to exploit the expressive power of deep convolutional networks. However, when the object to track is not known beforehand, it is necessary to perform Stochastic Gradient Descent online to adapt the weights of the network, severely compromising the speed of the system. In this paper we equip a basic tracking algorithm with a novel fully-convolutional Siamese network trained end-to-end on the ILSVRC15 dataset for object detection in video. Our tracker operates at frame-rates beyond real-time and, despite its extreme simplicity, achieves state-of-the-art performance in multiple benchmarks.",
isbn="978-3-319-48881-3"
}

@INPROCEEDINGS{10219612,
  author={Zeng, Dan and Zou, Mingliang and Wang, Xucheng and Li, Shuiwang},
  booktitle={ICME}, 
  title={Towards Discriminative Representations with Contrastive Instances for Real-Time UAV Tracking}, 
  year={2023},
  volume={},
  number={},
  pages={1349-1354},
  keywords={Deep learning;Correlation;Annotations;Manuals;Benchmark testing;Performance gain;Autonomous aerial vehicles;UAV tracking;Discriminative representation;Contrastive learning;Contrastive Instances},
  doi={10.1109/ICME55011.2023.00234}}

@InProceedings{Danelljan_2017_ECO,
author = {Danelljan, Martin and Bhat, Goutam and Shahbaz Khan, Fahad and Felsberg, Michael},
title = {ECO: Efficient Convolution Operators for Tracking},
booktitle = {CVPR},
year = {2017}
}

@InProceedings{Wu_2021_CVPR,
    author    = {Wu, Qiangqiang and Wan, Jia and Chan, Antoni B.},
    title     = {Progressive Unsupervised Learning for Visual Object Tracking},
    booktitle = {CVPR},
    year      = {2021},
    pages     = {2993-3002}
}

@InProceedings{Yan_2021_CVPR,
    author    = {Yan, Bin and Peng, Houwen and Wu, Kan and Wang, Dong and Fu, Jianlong and Lu, Huchuan},
    title     = {LightTrack: Finding Lightweight Neural Networks for Object Tracking via One-Shot Architecture Search},
    booktitle = {CVPR},
    year      = {2021},
    pages     = {15180-15189}
}

@InProceedings{Zhang_2021_CVPR,
    author    = {Zhang, Han and Koh, Jing Yu and Baldridge, Jason and Lee, Honglak and Yang, Yinfei},
    title     = {Cross-Modal Contrastive Learning for Text-to-Image Generation},
    booktitle = {CVPR},
    year      = {2021},
    pages     = {833-842}
}

@inproceedings{chen2020simple,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1597--1607},
  year={2020},
  organization={PMLR}
}

@inproceedings{gopal2023mobile,
author    = {Goutam Yelluru Gopal and Maria Amer},
title     = {Mobile Vision Transformer-based Visual Object Tracking},
booktitle = {BMVC},
year      = {2023},
url       = {https://papers.bmvc2023.org/0800.pdf}
}

@inproceedings{he2020momentum,
  title={Momentum contrast for unsupervised visual representation learning},
  author={He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  booktitle={CVPR},
  pages={9729--9738},
  year={2020}
}

