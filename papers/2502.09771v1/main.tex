\documentclass[10pt,conference]{IEEEtran} 
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}

% \usepackage{amsmath,amsfonts}
% \usepackage{algorithmic}
\usepackage{subcaption}
\usepackage{verbatim}
\usepackage{multirow}
\usepackage{epigraph}
\usepackage{tcolorbox}
\usepackage{enumitem}
% \usepackage{textcomp}
% \usepackage{xcolor}
\usepackage{booktabs}
\usepackage{pifont}
% \usepackage{listings}
\usepackage{url}
% \usepackage{float}
\usepackage{hyperref}
\captionsetup[table]{name=Table}
\renewcommand{\tablename}{Table}
\renewcommand{\thetable}{\arabic{table}}
\usepackage{adjustbox}
\definecolor{LightGray}{gray}{0.9}


\definecolor{keyword}{RGB}{255,0,90}
\definecolor{comment}{RGB}{0,128,0}
\definecolor{string}{RGB}{0,0,255}

\newcommand{\jie}[1]{{\textcolor{blue}{#1}}}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}


\newcommand{\shuyin}[1]{{\color{magenta}[Shuyin: #1]}}
\newcommand{\major}[1]{{\color{black}#1}}
    
\begin{document}

% \title{DSrepair: \textbf{K}nowledge-\textbf{e}nhanced \textbf{P}rompt \textbf{E}ngineering for Data Science Code Repair}

\title{Knowledge-Enhanced Program Repair for \\Data Science Code \vspace{-0.3em}}

% \author{
% \IEEEauthorblockN{Shuyin Ouyang}
% \IEEEauthorblockA{
% \textit{King's College London\\London, UK\\shuyin.ouyang@kcl.ac.uk}
% }
% \and
% \IEEEauthorblockN{Jie M. Zhang}
% \IEEEauthorblockA{
% \textit{King's College London\\London, UK\\jie.zhang@kcl.ac.uk}
% }
% \and
% \IEEEauthorblockN{Zeyu Sun}
% \IEEEauthorblockA{
% \textit{Institute of Software, Chinese \\Academy of Sciences, China \\zeyu.zys@gmail.com}
% }
% \and
% \IEEEauthorblockN{Albert Merono Penuela}
% \IEEEauthorblockA{
% \textit{King's College London\\London, UK\\albert.merono@kcl.ac.uk}
% }
% }

\author{
\IEEEauthorblockN{Shuyin Ouyang}
\IEEEauthorblockA{\textit{King's College London} \\
London, UK \\
shuyin.ouyang@kcl.ac.uk}
\and
\IEEEauthorblockN{Jie M. Zhang}
\IEEEauthorblockA{\textit{King's College London} \\
London, UK \\
jie.zhang@kcl.ac.uk}
\and
\IEEEauthorblockN{Zeyu Sun}
\IEEEauthorblockA{\textit{Institute of Software, Chinese} \\
\textit{Academy of Sciences}, Beijing, China\\
zeyu.zys@gmail.com}
\and
\IEEEauthorblockN{Albert Merono Penuela}
\IEEEauthorblockA{\textit{King's College London} \\
London, UK \\
albert.merono@kcl.ac.uk}
}

% \author{\IEEEauthorblockN{1\textsuperscript{st} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{2\textsuperscript{nd} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{3\textsuperscript{rd} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{4\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{5\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{6\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% }
\IEEEaftertitletext{\vspace{-2.5em}}
\maketitle

\thispagestyle{plain}
\pagestyle{plain}

\begin{abstract}


This paper introduces DSrepair, a knowledge-enhanced program repair approach designed to repair the buggy code generated by LLMs in the data science domain.
DSrepair uses knowledge graph based RAG for API knowledge retrieval and bug knowledge enrichment to construct repair prompts for LLMs.
Specifically, to enable knowledge graph-based API retrieval, we construct DS-KG (Data Science Knowledge Graph) for widely used data science libraries.
For bug knowledge enrichment, we employ an abstract syntax tree (AST) to localize errors at the AST node level. 
% DSrepair's effectiveness is evaluated against five state-of-the-art LLM-based repair baselines using four advanced LLMs on the DS-1000 dataset.
\major{We evaluate DSrepair’s effectiveness against five state-of-the-art LLM-based repair baselines using four advanced LLMs on the DS-1000 dataset.}
%  The results show that DSrepair surpasses all five baselines.
\major{The results show that DSrepair outperforms all five baselines.}
Specifically, when compared to the second-best baseline, 
% DSrepair demonstrates significant improvements, 
\major{DSrepair achieves substantial improvements,}
fixing 44.4\%, 14.2\%, 20.6\%, and 32.1\% more buggy code snippets for each of the four evaluated LLMs, respectively.
Additionally, it achieves greater efficiency, reducing the number of tokens required per code task by 17.49\%, 34.24\%, 24.71\%, and 17.59\%, respectively.


\end{abstract}

\begin{IEEEkeywords}
Code Repair, Large Language Model, Knowledge Graph, Data Science
\end{IEEEkeywords}


\section{Introduction}


Data science is crucial in driving innovation and decision-making across various domains~\cite{bolyen2019reproducible, hassani2023role}, leveraging data to uncover insights and inform strategic actions. 
Nevertheless, 
% the complexity and expertise required to handle the relevant libraries for coding data science solutions 
\major{the complexity of data science libraries and the expertise required to use them} can pose significant barriers to lay users. 
Large Language Models (LLMs) have emerged as powerful tools to generate data science code automatically~\cite{hong2024data, nejjar2023llms, lai2023ds}, democratizing access and accelerating development processes.
Despite their potential, the widely acknowledged shortcomings with LLMs, such as hallucination and the lack of specialized knowledge of certain domains (e.g. long-tail API usage)~\cite{zan2023private, zan2022language} and specific code context~\cite{ge2024openagi}, remain significant obstacles. 
These issues are particularly critical in the data science domain, where the code heavily relies on libraries for accurate and efficient data processing and analysis, making 
precision and contextual accuracy essential for robust outcomes.
Existing studies have applied feedback-based iterative self repair~\cite{xia2023keep, gupta2020synthesize, chen2023teaching, fu2023chatgpt, zhang2023critical}
to improve the reliability of LLM-generated code.
Nevertheless, these \major{approaches} are not designed for data science code.


% Researchers have found that combining code repair with code generation can yield even better outcomes~\cite{huang2023agentcoder, xia2023keep}. 
% Therefore, implementing post-processing code repair after code generation is crucial to mitigate the risks of inaccuracies.

% The emergence of various code LLMs, such as Codex~\cite{chen2021evaluating}, CodeLlama~\cite{roziere2023code}, and Alphacode~\cite{li2022competition}, has shown promising results in code-related tasks.
% At first, tasks like code generation~\cite{chen2024code} and code repair~\cite{gupta2020synthesize, chen2023teaching}, are treated separately, each with its own performance benchmarks. 
% However, researchers have found that combining these tasks can yield even better outcomes~\cite{huang2023agentcoder, xia2023keep}. 
% Several studies~\cite{huang2023agentcoder, hong2023metagpt, qian2023communicative} have proposed multi-agent collaborations to enhance the effectiveness of LLM-based code generation, where each agent addresses a specific task such as code generation or task planning. 
% This collaborative approach has demonstrated significant improvements in the accuracy and reliability of the generated code.

% % traditional code generation
% Due to the repetitiveness of code, template-based code generation leverages code templates with placeholders ~\cite{smith2003compiling, syriani2018systematic} to generate repetitive or boilerplate code.
% Using the Unified Modeling Language (UML), rule-based code generators are built to transform these into executable codes~\cite{jurgelaitis2022solidity, durai2022novel}.
% advanced compilers and interpreters could formulate code generation as a program synthesis problem. 
% For example, Chipmunk~\cite{gao2020switch} uses a program synthesis engine to achieve the transformation between high-level programs and machine code.
% However, the disadvantage of rule-based code generation is that it is very limited to certain scenarios and not scalable.

% correct work on code generation with LLM
% The arrival of the LLM wave broke this status quo.

% At first, different code-related tasks are treated separately, such as code generation~\cite{chen2024code} and code repair~\cite{gupta2020synthesize, chen2023teaching}.
% They usually have different benchmarks to evaluate their performance.
% However, researchers found that combining them could achieve even better results~\cite{huang2023agentcoder, xia2023keep}.
% For example, several studies~\cite{huang2023agentcoder, hong2023metagpt, qian2023communicative} have proposed to use of multi-agent collaborations to enhance the effectiveness of LLM-based code generation, where each agent addresses a unique task such as code generation or task planning.


% Although LLMs show promising results in general code repair~\cite{xia2023keep, gupta2020synthesize, chen2023teaching}, there are shortcomings in fully relying on LLMs, including a lack of specialized knowledge of certain domains (e.g. long-tail knowledge of API usage)~\cite{zan2023private, zan2022language} or specific contexts of code~\cite{ge2024openagi}, generated code existing logic errors~\cite{jiang2023self}.
% % and prompt-sensitive response~\cite{denny2023conversing}.
% As one of the solutions, 

Recently, Retrieval-Augmented Generation (RAG)~\cite{lewis2020retrieval} has also emerged as a widely-adopted technique to inject external knowledge into LLMs to facilitate more coherent code generation.
RAG combines the strengths of information retrieval and LLMs to enhance code generation.
% Existing code generation work on RAG~\cite{parvez2021retrieval, li2023acecoder, zhou2022docprompting, liu2023codegen4libs} all use a common RAG architecture, 
\major{Existing RAG-based code generation studies~\cite{parvez2021retrieval, li2023acecoder, zhou2022docprompting, liu2023codegen4libs} commonly follow a standard RAG architecture,}
where the ``retriever'' component retrieves relevant \textbf{plain text} from a vast corpus or database using a vector similarity search. This retrieved information is then fed into an LLM, which uses this context to produce more accurate and relevant code\cite{ahmed2024automatic, lu2022reacc, tang2023domain, zhang2023repocoder}.
% These text-based RAG \major{approaches} do not work well for code generation tasks, as they rely on unstructured or semi-structured plain text that lacks the semantic relationships and structured representation needed for complex code understanding and generation. 
\major{However, these text-based RAG approaches are not well-suited for code generation tasks, as they rely on unstructured or semi-structured plain text, which lacks the semantic relationships and structured representation needed for complex code understanding and generation.}
Specifically,
1) text-based retrieval relies on vector similarity search, which often retrieves irrelevant or loosely related information due to ambiguities in natural language;
2) plain text does not explicitly represent the relationships between APIs, their dependencies, or their attributes (e.g., parameters, return types). As a result, text-based RAG \major{approaches} may fail to provide the comprehensive contextual knowledge required for resolving an issue;
3) retrieved plain text often contains redundant descriptions or ambiguities, which can confuse large language models (LLMs) or lead to suboptimal code generation.


This paper introduces DSrepair, a knowledge-enhanced \major{approach} for repairing incorrect data science code produced by LLMs through knowledge graph-based RAG and bug information enrichment. 
We construct  DS-KG (\textbf{D}ata \textbf{S}cience \textbf{K}nowledge \textbf{G}raph),
a set of knowledge graphs for the seven most widely adopted data science libraries (i.e., NumPy, Pandas, SciPy, Scikit-learn, Matplotlib, PyTorch, and TensorFlow)~\cite{yin2018learning, yin2022natural, lai2023ds, hong2024data, nejjar2023llms}.
For the buggy code generated by an LLM, 
DSrepair uses the API name appeared in the code as the query,
to obtain the correct usage of corresponding API functions by accessing DS-KG.
It then uses the returned result to guide the LLM in repairing code.

Compared to text-based RAG, our KG-based RAG naturally captures more complex relationships and dependencies within API documents, which is essential in the data science domain.
% The incorporated rich semantic relationships stored in knowledge graphs result in more reliable and efficient code generation and repair. 
\major{The rich semantic relationships stored in knowledge graphs enhance the reliability and efficiency of code generation and repair.}
For instance, in Matplotlib's API document, an API named \textit{matplotlib.pyplot.subplots}, has a parameter called \textit{gridspec\_kw}.
The value of \textit{gridspec\_kw} must be passed to another API object, called \textit{matplotlib.gridspec.GridSpec}.
% If an error occurs in \textit{gridspec\_kw}, it is more helpful to query the knowledge from \textit{matplotlib.gridspec.GridSpec} rather than just from \textit{matplotlib.pyplot.subplots}.
\major{If an error occurs in \textit{gridspec\_kw}, querying knowledge from \textit{matplotlib.gridspec.GridSpec} is more helpful than querying only from \textit{matplotlib.pyplot.subplots}.
}
A well-designed KG can infer such dependency naturally.


% repairing the data science code generated by LLMs. 

% , that combines API knowledge retrieval and bug knowledge enrichment, bringing accuracy and flexibility to code repair in the data science domain.
% This approach enables LLM to access and leverage vast amounts of external knowledge by retrieving semantically relevant information before generating a response and enhances LLM's ability to repair the code.

% This not only enhances the LLM’s ability to generate code but also increases the reliability and relevance of the output.
% DSrepair uses structured prompts to add API knowledge and enriched bug knowledge to prompts, allowing LLM to better generate satisfactory code.
% We choose Knowledge Graphs (KGs) to store the knowledge extracted from online API documents.
% Searching for knowledge in plain text needs an attention mechanism~\cite{vaswani2017attention}, which requires significant computational resources to process and analyze text, as they need to consider the relevance of each word in relation to every other word in the document.
% Unlike storing knowledge in plain text, a KG can efficiently leverage the links within the online API documentation to create a richer representation of the knowledge, such as references to other API data structures or details about specific parameters, since both of them follow standards set by the World Wide Web Consortium (W3C)~\footnote{\url{https://www.w3.org/standards/}}, which allows for straightforward queries without the need for extensive computation.


DSrepair also uses enhanced bug information to improve the program repair effectiveness.
Data science code often contains multiple function calls and data operations within a single line.
Therefore, to obtain fine-grained bug information, DSrepair 
uses Abstract Syntax Tree (AST) and test case execution information to localize errors at the AST node level.


We evaluate DSrepair on two widely used general-purpose LLM (i.e., GPT-3.5-turbo and GPT-4o-mini~\cite{gptmodel}) and two state-of-the-art coding LLMs (i.e., DeepSeek-Coder~\cite{guo2024deepseek} and Codestral~\cite{codestral}) based on DS-1000~\cite{lai2023ds}, the data science code generation benchmark spanning seven data science libraries.
Our results show that DSrepair outperforms all the five baseline LLM-based repairing \major{approaches}.
In particular, compared to the second-best baseline,
DSrepair correctly fixes 44.4\%, 14.2\%, 20.6\%, and 32.1\% more buggy code snippets for the four LLMs, respectively,
% but saves 17.49%, 34.24%, 24.71%, and 17.59% tokens per code task, respectively
\major{while reducing token usage per code task by 17.49\%, 34.24\%, 24.71\%, and 17.59\%.}



To summarize, this paper makes the following contributions:

\begin{itemize}[leftmargin=*]
    \item  We present DSrepair, a novel LLM-based program repair \major{approach} for data science code, leveraging knowledge graph-based RAG and enriched bug information. 
    \item We construct and release DS-KG (Data Science Knowledge Graph), a comprehensive set of knowledge graphs tailored to the seven most widely used data science libraries.  
    \item We propose an AST-based bug information enriching \major{approach} that can pinpoint errors at the AST node level.
    \item We conduct an empirical study using four LLMs and five baselines, demonstrating that DSrepair significantly outperforms all baselines in repairing data science code.
\end{itemize}


We release our data, code, KG data dump, and results at our homepage~\cite{homepage}.
The rest of the paper is organized as follows.
Section~\ref{section: method} outlines our methodology.
Section~\ref{section: experimental design} describes the design of the experiments, including research questions, benchmarks, baselines, selected models, and measurements.
Section~\ref{section: result and findings} presents the results and highlights notable findings based on our empirical results.
% presents the results and shows some interesting findings based on the experimental results we obtained.
% Section~\ref{section: discussion} discusses the threat to validity and the limitation of our work.
\major{Section~\ref{section: discussion} discusses the threat to validity, the limitation, and the generalizability of our work.}
% Section~\ref{section: threat to validity} shows the threats to validity.
Section~\ref{section: related work} introduces the related work of our study.
Section~\ref{section: conclusion} concludes.

% 1) Lack of Contextual Understanding: 
% Limited Domain Knowledge: LLMs may not have specialized knowledge of certain domains, leading to incorrect or inefficient code. Absence of Project Context: They lack understanding of the specific project's architecture, existing codebase, and coding standards, which can result in code that doesn't fit well within the overall project.
% 2) Accuracy and Reliability:
% Error-Prone: LLM-generated code can contain syntactic and logical errors, requiring careful review and debugging by human developers.
% Overfitting and Hallucinations: LLMs may produce overly complex solutions or "hallucinate" incorrect functions or libraries that do not exist.
% 3) Performance Considerations:
% Inefficient Code: LLMs may generate code that is not optimized for performance, leading to slower execution times or higher resource consumption.
% Scalability Issues: The code generated might not be scalable or suitable for high-performance applications.
% 4) Dependence on Prompts:
% Quality of Input: The quality of the generated code heavily depends on the quality and specificity of the prompts provided, which can be a limitation if the user lacks expertise.
% Ambiguity: Ambiguous prompts can lead to unpredictable or unintended code generation outcomes.


% Hallucination in LLMs refers to the phenomenon where LLMs lie and fabricate non-existent facts or inappropriate information~\cite{yao2023llm}.
% Hallucination arises because LLMs predict text based on statistical patterns rather than a true understanding of facts,
% resulting in outputs that may be coherent yet fundamentally flawed. 
% As a result, every response returned from LLMs may contain hidden dangers that violate facts.

% For LLM-based code generation, hallucination can significantly impact software development, leading to a range of issues from minor bugs to critical system failures.
% When LLMs generate incorrect or fact-violated code, developers may inadvertently introduce errors that are difficult to detect and debug, potentially compromising the functionality and security of applications. 
% This can result in wasted time and resources as developers attempt to identify and rectify these issues. 
% Furthermore, reliance on hallucinated code can erode trust in AI-assisted programming tools, slowing their adoption and reducing their potential benefits.


% Hallucination can
% degrade the overall quality of the generated code and undermine people’s trust in leveraging LLMs for real-world development~\cite{liu2024exploring}. 
% Indeed, compared to other tasks of LLMs, such as question answering and text summary, the hallucination threat in code-related tasks is much more serious, because hallucination often brings errors in the generated code.
% It is therefore of vital importance to mitigate hallucinations in code generation tasks.

\section{Method}
\label{section: method}

Fig~\ref{fig: overview} shows an overview of DSrepair.
Given a code problem description, we first let LLM (i.e. GPT-3.5-turbo) generate code.
If the code fails to pass the test cases, 
DSrepair constructs a repair prompt \major{and requests} LLM to regenerate the code (see Section~\ref{sec:benchmark} for details).
% \jie{
As shown in the figure, DSrepair involves four main steps: API KG Construction, where a knowledge graph (DS-KG) is built for popular data science libraries (e.g., NumPy and Pandas) to capture detailed API usage and relationships; API Knowledge Retrieval, where API calls are extracted from the buggy code and queried from DS-KG, with the results verbalized into natural language for LLM prompts; Bug Knowledge Enrichment, which localizes errors at the AST node level using test case execution to provide fine-grained bug information; and Prompt Construction, where all gathered information is structured into a detailed prompt to guide the LLM in generating effective repairs.
% }


% Our approach enhances fault localization through the use of AST.
% ASTs provide a structured representation of the code, enabling more precise identification and understanding of errors within the code structure.
% This advanced fault localization approach facilitates more accurate and efficient debugging and code repair processes.
% Furthermore, to address the issue of API function misuse, we have constructed a comprehensive knowledge base.
% This knowledge base incorporates knowledge graphs derived from the official online documentation of various libraries.
% By storing and organizing this information, we enable more informed and accurate usage of API functions, further improving the quality of code generated by LLMs.
% By leveraging both local context (fault localization result) and global context (API function specifications), we could improve the post-processing repair performance of code generation with LLMs.



\subsection{API KG Construction}

\begin{figure}[t]
\centerline{\includegraphics[width=0.89\linewidth]{fig/overview.pdf}}
\vspace{0mm}
\caption{Overview of DSrepair.}
\label{fig: overview}
\vspace*{-0.4cm}
\end{figure}

\begin{figure*}[t]
\centerline{\includegraphics[width=0.9\linewidth]{fig/DSKG_example.pdf}}
\vspace{0mm}
\caption{
Details on API KG construction (Step 1) and API Knowledge Retrieval (Step 2) in DSrepair. 
The code raises an error because of the mismatched array shape between \textit{np.flipud}'s required input and \textit{np.array\_split}'s output.
DSrepair extracts the API call in the error line and builds a SPARQL query to search the relevant RDF triples in the DS-KG, which is constructed from API documents and guided by the ontology.
Finally, DSrepair maps the returned RDF triples to natural language, which will be used as a part of the repair prompt.
}
\label{fig: DSKG example}
\vspace*{-0.4cm}
\end{figure*}

We develop DS-KG, a knowledge graph tailored to widely adopted data science libraries such as NumPy, Pandas, SciPy, Scikit-learn, Matplotlib, PyTorch, and TensorFlow.
Its primary purpose is to assist LLMs in repairing buggy code by providing structured information about the correct usage of APIs.
Following standard KG construction procedures, we begin by creating an ontology to define the schema for DS-KG~\cite{simperl2014collaborative, suarez2011neon}.
% \jie{
Existing ontologies are unsuitable for representing API documentation in the context of data science code repair.
To address this gap, we manually design a domain-specific ontology schema, drawing insights from the structure of API documentation.
API documentation typically provides details such as an API’s name, expression, explanation, parameters, and return types.
Our ontology captures these attributes for individual API functions, enabling precise and structured queries based on error information extracted from buggy code.
Inspired by prior work in code ontology design~\cite{liang2022misusehint, abdelaziz2021toolkit}, we represent each API function as a unique entity within DS-KG.
The ontology includes two types of relations:
(1) Attribute Relations, describe links between API entities and their attributes, such as: `\textit{has\_name}', `\textit{has\_expression}', and `\textit{has\_explanation}'\footnote{In this paper, we ignore the OWL prefixes in RDF triples (\textless subject, predicate, object\textgreater) to make the article more concise.}.
(2) Dependency Relations, capture the hierarchical structure and dependencies of APIs, such as `\textit{belongsToLibrary}' and `\textit{belongsToModule}'.
% }

% \jie{
Fig~\ref{fig: DSKG example} illustrates an example of DS-KG construction from the NumPy API document.
Each API object introduced on a webpage, such as \textit{numpy.flipud} and \textit{numpy.array\_split}, is treated as an entity.
Detailed information about an API object, such as its name, expression, explanation, and parameters \& returns, is used to build RDF triples with attribute relations.
For example, the API object \textit{numpy.flipud} has such an RDF triple, \textless \textit{numpy.flipud}, \textit{has\_expression}, ``\textit{numpy.flipud(m)}''\textgreater.
New entities are created for the parameters and return values of each API object, each with attributes like argument position, data type, and explanation.
For example, the parameter \textit{m} in ``\textit{numpy.flipud(m)}'' has the following RDF triple: \textless \textit{numpy.flipud\_parameter\_m}, \textit{hasType}, ``\textit{array\_like}''\textgreater.
Using the prefix of the API entity (derived from the name and webpage URL), we construct RDF triples with dependency relations.
For instance, the API object \textit{numpy.flipud} is linked to its library through the RDF triple: \textless \textit{numpy.flipud}, \textit{belongsToLibrary}, \textit{numpy}\textgreater.
% }

% Inspired by similar code ontology designs~\cite{liang2022misusehint, abdelaziz2021toolkit}, we treat each API function as a unique entity. The ontology defines two relation types: attribute relation and dependency relation. 
% Attribute relations describe links between API entities and their attributes, such as `\textit{has\_name}', `\textit{has\_expression}', and `\textit{has\_explanation}', which provide detailed knowledge about retrieved API entities.
% Dependency relations describe the hierarchical structure of API entities, such as `\textit{belongsToLibrary}' and `\textit{belongsToModule}', making the knowledge traceable.




% We build DS-KG~\footnote{In this paper, we ignore the prefix of the knowledge unit, such as object, predicate, and subject in the RDF triple  \textless subject, predicate, object\textgreater}, a knowledge graph for widely adopted data science libraries, including NumPy, Pandas, SciPy, Scikit-learn, Matplotlib, PyTorch, and TensorFlow. 
% The primary purpose of DS-KG is to assist LLMs in repairing buggy code by providing rich information about the correct usage of APIs.
% We follow the standard KG construction procedure, which requires the creation of an ontology before constructing the KG~\cite{simperl2014collaborative, suarez2011neon}. 
% To the best of our knowledge, no existing ontology is specifically designed for building KGs from API documents in the domain of data science code repair.
% To address this gap and capture domain-specific knowledge, we manually developed the DS-KG ontology schema, drawing inspiration from the structure and design of API documentation.
% Each API documentation webpage typically introduces details such as the API's name, expression, explanation, and parameters \& returns.
% Our ontology is designed to store these details for individual API functions, enabling precise queries based on error information extracted from buggy code.
% Inspired by similar code ontology design~\cite{liang2022misusehint, abdelaziz2021toolkit}, we treat each API function as a unique entity.
% There are two types of relations in ontology, attribute relations and dependency relations. 
% Attribute relations describe links between API entities and their attributes, such as `\textit{has\_name}', `\textit{has\_expression}', and `\textit{has\_explanation}'.
% These provide detailed knowledge about retrieved API entities.
% Dependency relations describe the hierarchical structure of API entities, such as `\textit{belongsToLibrary}' and `\textit{belongsToModule}', making the knowledge traceable.

% \shuyin{
% RDF triples with dependency relation is not involved in API knowledge retrieval. 
% More detail of how API knowledge affect the repair can be found in Section~\ref{section: API richness}.
% }


% old version
% \major{
% Fig~\ref{fig: DSKG example} illustrates an example of the construction of DS-KG from the Numpy API document. 
% }
% Here, we treat the API object introduced on each webpage, such as numpy.flipud and numpy.array\_split (\jie{such as XX}) as an entity.
% To ensure uniqueness, we use the name of the API object \shuyin{such as numpy\_flipud and numpy\_array\_split} (\jie{such as XX}) as the name of its corresponding entity.
% Since each entity's name and webpage URL are unique, we can distinguish them during retrieval.
% Additionally, to enrich the query results from DS-KG, we store not only the attributes of API objects but also their hierarchical relationships.
% Detailed information about the API entity is used to create attributes such as name, expression, and explanation.
% \major{
% For example, the API numpy.flipud have such a RDF triple (subject-predicate-object), \textless numpy\_flipud, \textit{has\_expression}, ``numpy.flipud(m)''\textgreater.
% }\shuyin{I define the RDF triple here}
% For the parameters and returns of the API entity, we create new entities, each with attributes like argument position, data type, and explanation.
% Based on the prefix of the API entity (name and webpage URL), we can determine its hierarchical relation within DS-KG, identifying the library and module to which the API entity belongs.





% To derive an ontology, the process involves two main parts: designing the ontology and explaining the rationale behind its design.


% Our primary purpose is to assist LLMs in buggy code repair, particularly in offering information about the correct usage of APIs.
% Therefore, our designed ontology should be reused for all API objects, allowing us to store rich information about the correct usage of APIs and provide unique identification for different APIs across various API documents.

% Based on such ontology, we can then store API knowledge in the API documents efficiently and provide accurate retrieval results.

% We use the API object introduced on each webpage as an entity.

% Each entity's name and webpage URL are unique, therefore we can distinguish them during the retrieval.
% From the detailed information of this API entity, we create attributes to describe it, such as name, expression, and explanation.
% For the parameters and returns of this API entity, we create new entities to describe them.
% Each parameter or return is an entity with attributes like argument position, required data type, and explanation.
% During the ontology design, we reuse the structure of how the webpage describes the API, including their class and attributes in their HTML files.



% \shuyin{extend the following sentence}
% how we design the ontology 
% why we design the ontology like this; 1) what requirement of our ontology 

% The ontology describes not only the specific knowledge of each API (e.g., the name, explanation, expression, parameters, and return of the API) but also the hierarchical relationship of the API (i.e., which library the API belongs to, which API a certain parameter belongs to, and which data structure the parameter should be).
% % which is easy to be reused


% For API documents, we treat each webpage as a place where we can extract knowledge about how to use the certain API.
% Though analyzing the structure of the webpage, we extract the entities from its HTML file.


% We build DS-KG, a knowledge graph set for widely adopted data science libraries, including NumPy, Pandas, SciPy, Scikit-learn, Matplotlib, PyTorch, and TensorFlow.
% These libraries also cover all the library categories in the DS-1000 dataset.
% The API documents of these libraries are online accessible.
% DS-KG can be further used to store and query API knowledge.
% We deployed a triple store with Apache Jena Fuseki~\cite{fuseki} to enable an efficient triple store and API knowledge retrieval of DS-KG. 
% % which can provide robust services that allow effective interaction with the KG.
% In order to ensure that the LLM-generated code can meet the requirement, we must ensure that the use of APIs in the code is accurate~\cite{ouyang2021api, nielebock2024asap}.
% To obtain knowledge of API usage information correctly, we designed our DS-KG ontology.
% The ontology describes not only the specific knowledge of each API (e.g., the name, explanation, expression, parameters, and return of the API) but also the hierarchical relationship of the API (i.e., which library the API belongs to, which API a certain parameter belongs to, and which data structure the parameter should be).

% The questions of data science code problems are mainly sourced from Stackoverflow~\cite{yin2018learning, lai2023ds}, where solutions mainly contain how to correctly use the APIs.
% The questions of data science code problems are mainly sourced from Stackoverflow~\cite{yin2018learning, lai2023ds}, 
% where the accepted answers to the question mainly pay attention to two aspects, namely, why the existing solution is incorrect and how to correctly use certain APIs.
% To understand why the code is incorrect, we introduce bug knowledge enrichment, which is shown in Section~\ref{sec: bug knowledge enrichment}.

% Following this, we design DSrepair.
% which aims to determine the correct usage of certain APIs under certain contexts.
% The accepted answers to these code problems are mainly focused on two aspects, namely, 1) how to correctly use certain APIs and 2) why the existing solution is incorrect.
% There are many questions on Stackoverflow about the use of certain APIs under certain requirements.

% The ontology design of DS-KG can be found on our homepage~\cite{homepage}.

% \shuyin{describe the ontology design and the reason why design like this}

% \shuyin{turn the following in to competency questions, and find papers about the user study on code repair}
% With the purpose of code repair, developers when doing code repair, care about the correct API usage specification, including the function name, parameters it takes in, and the return of the API function.
% Moreover, they also care about whether using certain API functions can help them achieve the tasks.
% Based on a user study of code repair.



% % We are only targeting API functions in the API documents because compared with other content, API functions are more likely to be called in the code.
% As shown in Fig~\ref{fig: KG example}, for each API function, we extract the key components from API documents' online webpage, including function name, function expression, function explanation, input parameters, and returns.
% We use beautifulsoup\footnote{\url{https://beautiful-soup-4.readthedocs.io/en/latest/}} to pull the data out of the HTML files.
% For each API function, we normalize the related information into structured data.
% Normalization ensures the consistency and quality of the data by standardizing property values and mapping entities' classes and properties to terms within the API KG's ontology.
% Moreover, data types of property are also specified, which ensures the high quality of processed data. 
% Finally, based on our defined ontology schema, we use RDFlib\footnote{\url{https://rdflib.readthedocs.io/en/stable/}} to map the structured data into RDF format (i.e. triples) and store them inside our API KGs.


% facilitating the collection of comprehensive data.
% The collected data are systematically stored in Apache Jena Fuseki\footnote{\url{https://jena.apache.org/documentation/fuseki2/}}, 
% ensuring that they are readily accessible for processing and analysis.
% we have developed a SPARQL query engine.


% The input is online API documents that are downloaded or crawled from the Internet, and the output is structured data (beautifulsoup object) that records the knowledge information extracted from the input data.

% \textbf{Normalization} Normalization ensures the consistency and quality of the data by standardizing property values and mapping entities' classes and properties to terms within the API knowledge graph's ontology.
% , data types of property are also specified, which ensures the high quality of processed data. 
% The input is the output of data extraction, i.e. beautiful soup object, 
% The output is structured data with normalized property names and property values.
% The type of property value follows the definition of API knowledge graph schema.


% \begin{figure*}[htbp]

%   % \begin{subfigure}{0.49\linewidth}
%   %   \centering
%   %   \includegraphics[width=\linewidth]{fig/API_document_example.pdf}
%   %   \caption{API document example.}
%   %   \label{fig: API document example}
%   % \end{subfigure}
%   % \hfill
%   % \begin{subfigure}{0.49\linewidth}
%   %   \centering
%   %   \includegraphics[width=\linewidth]{fig/KG_example.pdf}
%   %   \caption{KG storage and query.}
%   %   \label{fig: KG storage and query}
%   % \end{subfigure}

% \centerline{\includegraphics[width=\linewidth]{fig/API_knowledge_example.pdf}}
% \vspace{0mm}
% \caption{Example of API knowledge from API document. We extract structural information from online API documents and map the structural data into RDF triples based on the knowledge graph schema. Given a buggy code snippet, we use the API function call in the code snippets as the keyword to query the DS-KG to obtain the corresponding correct usage of the API function.}
% \label{fig: KG example}
% \vspace*{-0.5cm}
% \end{figure*}

\subsection{API Knowledge Retrieval}

% \jie{
DSrepair integrates DS-KG to enhance the repair of buggy code by retrieving relevant API knowledge and incorporating it into the repair process.
DSrepair extracts all API invocations in the buggy code snippet using regular expressions (e.g., identifying `\textit{np.flipud}' or `\textit{np.array\_split}'). 
It resolves API prefixes (e.g., mapping `\textit{np}' to `\textit{numpy}') and uses the full API name for queries, accounting for the common use of abbreviations in data science libraries.
Using the resolved API name, DSrepair constructs SPARQL queries~\cite{prudhommeaux2008sparql} to retrieve RDF triples from DS-KG.
These triples encapsulate knowledge specific to the queried API, such as its attributes, dependencies, and parameter details.
To ensure compatibility with LLMs, we transform the retrieved RDF triples into natural language sentences using triple verbalization techniques~\cite{blinov2020semantic}.
These sentences provide human-readable explanations, including a description of the API’s purpose and syntax, details about parameters and returns together with their data types and explanations.
The retrieved API knowledge is concatenated and included in the ``API Knowledge'' section of the repair prompt provided to the LLM.
% }

In Section~\ref{section: API richness}, we demonstrate that incorporating only the full API expression as knowledge yields the best performance for data science code repair.
Thus, by default, DSrepair includes the full API expression in the prompt.
This approach balances the richness of information and efficiency, ensuring LLMs receive sufficient contextual guidance without overwhelming them with unnecessary details.

% old version
% API Knowledge Retrieval aims to provide efficient and accurate access to the DS-KG, ensuring that relevant API knowledge is seamlessly delivered to enhance the repair prompt.
% As shown in Fig~\ref{fig: DSKG example}, when an incorrect code snippet is provided, DSrepair begins by extracting all API invokes using regular expression matching.
% For each API invoked, we complete the prefix of the API and use the full API name as a keyword for our query, because data science libraries are often used with their abbreviations.
% We use SPARQL queries~\cite{prudhommeaux2008sparql} to retrieve API knowledge from the DS-KG.
% Due to the uniqueness of API entity names stored in DS-KG, we can locate the \shuyin{RDF triples} \jie{triples not defined?}whose subject entity matches the keyword.
% These identified triples are returned as the relevant API knowledge for the corresponding API invoked in the buggy code snippet.

% The retrieved results are structured information represented by RDF triples \shuyin{Removed}\jie{why introduced here?}.
% To make them more understandable for LLMs, we apply triple verbalization~\cite{blinov2020semantic}
% \jie{how?}, converting the RDF triples into coherent and readable natural language sentences.
% This includes introducing the explanation and full expression of the invoked API, detailing how many parameters are required, and how many results are returned, and describing each parameter and return's order, data type, and explanation.
% We then concatenate the API knowledge from each invoked API and include it as part of the ``API Knowledge'' in our final prompt.
% Through our experiment in Section~\ref{section: API richness}, we find that only using the full expression as the API knowledge could make LLMs have the best performance in data science code repair.
% Thus, in DSrepair, by default we use the full expression as the API knowledge to compose the final prompt.



% DS-KG contains detailed information about the correct usage of various API functions, derived from comprehensive and up-to-date online API documentation.
% By querying DS-KG\jie{details for the query please}, our system retrieves relevant and accurate data about the proper application of the functions, including function explanation, function signature, input parameters, and returns.
% This retrieval process ensures that the recommendations for correcting the code snippet are based on semantic relevance and contextual information.

% This extraction involves parsing the code to identify function names\jie{API function name or overall function name?}, which are critical to understanding the context and potential errors in the code.
% Once the API functions are identified\jie{how?}, these names are utilized as keywords to construct SPARQL queries.

% These queries are then executed against DS-KG stored in the server.
% When an incorrect code snippet is provided, DSrepair initiates the retrieval process by extracting the API functions embedded within the snippet.
% The storage and querying capabilities of our API knowledge retrieval system are powered by Apache Jena Fuseki, a high-performance SPARQL server that excels in the management and retrieval of RDF (Resource Description Framework) data.
% Apache Jena Fuseki facilitates the efficient handling of our knowledge graphs, enabling sophisticated storage, querying, and retrieval operations essential for accurate and context-aware data science code generation.
% As shown in Fig~\ref{fig: KG example}, 

% based on the most reliable and contextually appropriate information available.

% The storage and querying capabilities of the knowledge graph are powered by Apache Jena Fuseki, a SPARQL server that supports efficient management and retrieval of RDF (Resource Description Framework) data.
% This includes defining triples that represent the entities, properties, and relationships within the data.
% Then, we use the Apache Jena Fuseki server to upload the RDF data.
% The data can be loaded into a named graph within Fuseki, providing a logical separation for different API documents.
% Fuseki provides persistent storage for RDF data.
% Data integrity and consistency are maintained by adhering to the RDF data model and the ontology schema.

%  given an incorrect code snippet, we first extract the API functions inside the snippet. 
% We then use these API function names as keywords to build SPARQL queries and query the API KGs separately.

% \begin{figure}[htbp]
% \centerline{\includegraphics[width=9cm]{fig/KG_example.pdf}}
% \vspace{0mm}
% \caption{KG example.}
% \label{fig: kg_example}
% \end{figure}

\begin{figure*}[htbp]
\centerline{\includegraphics[width=0.8\linewidth]{fig/bug_knowledge_example.pdf}}
\vspace{0mm}
\caption{Bug knowledge enrichment example. Stderr (standard error information) can only localize the bug at the line level, while our bug knowledge enrichment could enrich the error information to the AST node level.}
\label{fig: AST_fault_localization}
\vspace*{-0.5cm}
\end{figure*}

\subsection{Bug Knowledge Enrichment}
\label{sec: bug knowledge enrichment}
% In this step, we analyze the AST of buggy code to get more detailed information about the bug to let LLM know more knowledge of the bug.

Bug knowledge enrichment aims to
provide LLMs with extra bug information to help LLMs better repair the bug without requesting extra tests.
We use only the example tests provided in the coding task description.
Traditional fault localization \major{approaches} such as spectrum-based fault localization~\cite{abreu2007accuracy} and mutation-based fault localization~\cite{papadakis2015metallaxis} are not applicable here for two reasons.
First, data science code generation benchmarks usually provide a very limited number of tests (e.g., 1.6 tests on average per problem in DS-1000) since the annotators need to define program inputs with complex objects such as square matrices, classifiers, or dataframes~\cite{lai2023ds};
second, traditional \major{approaches} are often file-level or line-level fault localization, while data science code tends to
contain multiple function calls and data operations in one line. 
Therefore, different from traditional \major{approaches}, DSrepair uses AST-node level bug information to provide LLMs with more fine-grained bug information.
Fig.~\ref{fig: AST_fault_localization}
shows a specific example of our bug information enrichment procedure. 


Firstly, test cases are extracted from the coding task description provided.
These tests are essential for validating the correctness of the code and are used later in the bug knowledge enrichment process.
We then 
transform the incorrect code snippet into its AST representation.
Once the AST is generated, DSrepair iterates within a namespace that includes all necessary libraries and the extracted test cases.
This iteration involves executing nodes in the AST sequentially.

To gain detailed bug information, the system identifies the last unexecuted node in the AST.
We classify all the bugs into two categories:
1) Runtime Errors.
If the code contains bugs that prevent it from being executed, the system will run each AST node until it encounters an error.
The AST node that was executable before the failure occurred is noted as the last executed node.
The node immediately following this, which causes the failure, is where the bug is likely located.
The error is between these two nodes: the last executable node and the first unexecutable node.
2) Assertion Errors. 
If the entire code can be executed but the results do not match the expected output, such an issue can be due to an assertion error.
In this case, the system captures the final value returned by the code execution.
By comparing this actual output with the desired result, the system can provide information to LLMs about why the code is incorrect.
The comparison highlights discrepancies, offering insights into potential logical errors in the code.
% \textbf{Spoiled Test Cases Extraction} We begin by extracting spoiled test cases from the prompt.
% These test cases are then initialized into the global namespace individually to ensure they are accessible throughout the code execution process.

% \textbf{AST Construction and Iteration} For each piece of generated code, we construct an AST.
% The AST serves as a hierarchical representation of the code's syntactic structure, where each node corresponds to a specific construct within the code (e.g., function calls, variable assignments, etc.).
% We iterate through each node in the AST to monitor the execution of the generated code.
% This involves tracking which nodes have been executed and identifying the point at which execution fails.

% \textbf{Last Unexecuted Node Identification} Instead of directly providing the LLM with standard error information, we utilize the AST to pinpoint the last unexecuted part of the code.
% Monitoring the execution flow to determine the point of failure.
% Identifying the specific AST node corresponding to the last part of the code that failed to execute.

% \textbf{Enriching Error Information} Once the last unexecuted node is identified, we craft a detailed error report that includes both the last executed part of the code and its value, as well as the last unexecuted part.
% This additional information is then integrated into the prompt provided to the LLM.
% Detailed information about the last executed and unexecuted AST nodes, including their types and positions within the code.
% The value and context of the last executed part, providing insights into the state of the code just before the failure occurred.
% Finally, we use our fault localization result as evidence to prove that the code is incorrect, thus showing that there is an inconsistency between the problem description and the generated code.


% By providing the LLM with this enriched contextual error information, we enable more precise and effective debugging.
% The LLM can utilize detailed information about the last executed and unexecuted parts of the code to make targeted corrections.
% This approach facilitates more accurate code repair and improves the overall quality of the generated code by ensuring that errors are addressed with a deeper understanding of the code's execution context.


\subsection{Prompt Construction}
This step uses information obtained from the previous steps and organizes it into a structured prompt~\cite{brate2022improving}, which is then fed to the LLM for code repair.
% In the final step, we combine the acquired knowledge into our structured prompt.
% Effective prompt construction is critical for providing precise and context-aware code corrections. 
As shown in Fig~\ref{fig: prompt example},
the final prompt includes the following components: problem description, incorrect code, stderr information, API knowledge, bug knowledge, fact-checking, and response format.


We first put the problem description and LLM generated incorrect code in the prompt.
% The problem description and LLM first generated incorrect code are used inside the DSrepair prompt directly.
Error messages are cleaned by removing local file paths and deleting warnings.
To some extent, this action protects the privacy of users' operation system environment and ensures that only relevant error information is included, focusing on critical errors that hinder code execution.
 
We extract useful API knowledge from the DS-KG query results, specifically the API expression or signature.
This expression includes all parameters both compulsory and optional highlighting potential errors related to parameter usage and function calls.
This comprehensive parameter information is crucial as it often points to the source of errors in the code.

For bug knowledge, we leverage the results from bug enrichment.
This involves providing the test case, the last unexecutable AST node, and the last executable AST node along with the executed result value.
By comparing the actual output with the desired output, we can pinpoint the exact location and nature of the error.
This detailed local context helps in understanding the specific issues within the code.

The fact-checking component identifies where the existing code logic violates the corresponding requirements outlined in the problem description.
This step is essential to redirect the LLM's attention back to the problem description, ensuring that the solution aligns with the original requirements and constraints.
The complete prompts that we use are available on our homepage~\cite{homepage}.
% To ensure clarity and consistency in the LLM's response, the response format is regulated.
% The response should be structured to address the identified errors directly, providing a clear and concise correction or suggestion based on the provided context.
% Finally, we feed our DSrepair prompt into LLM, and the output is generated code.


% \textbf{Enriching Prompt} The SPARQL query engine in Fuseki allows users to execute complex queries to retrieve information from the knowledge graph.
% SPARQL (SPARQL Protocol and RDF Query Language) is a powerful query language designed specifically for querying RDF data.
% Using the SPARQL query, we can obtain the relevant triplets from the knowledge graph that could be used to enhance our prompt.

% By providing the LLM with this enriched global API knowledge, we can give LLMs more correct information about the used functions, and restrict the usage of the functions in the further generating code, e.g. the input parameters (compulsory and optional) and the return types.
% This approach facilitates more global ground truth knowledge and improves the overall quality of the generated code by ensuring the correct usage of certain API functions.

\begin{figure}[t]
\centerline{\includegraphics[width=0.6\linewidth]{fig/prompt_example.pdf}}
\vspace{0mm}
\caption{DSrepair prompt example. The prompt contains structural and rich information to guide LLMs for code generation.}
\label{fig: prompt example}
\vspace*{-0.6cm}
\end{figure}



\section{Experimental Design}
\label{section: experimental design}

\subsection{Research Questions}

Our evaluation answers the following questions:
% \shuyin{re-design the RQs}

\noindent \textbf{\emph{RQ1: How effective is DSrepair in repairing LLM-generated data science code?}} This RQ investigates the buggy code fix rate of DSrepair compared with other state-of-the-art program repair \major{approaches}.

\noindent \textbf{\emph{RQ2: How do DSrepair's bug fixes overlap with the five baselines?}}
This RQ investigates whether DSrepair could fix unique bugs that the baselines fail to address.

\noindent \textbf{\emph{RQ3: What is the cost of DSrepair?}} This RQ investigates token usage and money spent on DSrepair and our baselines.

\noindent \textbf{\emph{RQ4: How is DSrepair's performance affected by different prompt designs?}} To understand how different prompt designs affect DSrepair, we conduct an ablation study to analyze each key prompt component's contribution to DSrepair.

\noindent \textbf{\emph{RQ5: How do different knowledge retrieval approaches affect DSrepair?}} 
This RQ aims to explore the advantage of knowledge graph-based RAG against plain text-based RAG.

\noindent \textbf{\emph{RQ6: How does the richness of API knowledge affect DSrepair?}}
This RQ studies whether different types of API knowledge (e.g., whether the knowledge contains explanation or parameters) given in DSrepair will affect its performance.

\noindent \textbf{\emph{RQ7: How does the non-determinism of LLM affect our experiment results?}} This RQ studies the influence of LLM's inherent randomness on our experiment results.


\subsection{Data Science Benchmark}
\label{sec:benchmark}
Our evaluation uses DS-1000~\cite{lai2023ds},
the state-of-the-art benchmark specifically designed for benchmarking LLMs in data science code generation.
% \jie{
The DS-1000 benchmark was specifically constructed to mitigate concerns about data leakage. In particular, the dataset applies perturbation (e.g., text rephrasing and semantic perturbation),
% applied to code problems
so that models cannot answer them correctly by memorizing the solutions from pre-training~\cite{lai2023ds}. 
% }



Other data science code generation benchmarks~\cite{agashe2019juice, yin2018learning} are not applicable because they are not based on realistic
problems and have no dedicated test cases to evaluate the correctness of the code (they use Exact Match~\cite{exact_match} or BLEU score~\cite{post2018call}).
DS-1000 comprises one thousand diverse and practical data science problems sourced from StackOverflow, covering seven essential Python libraries:
Numpy\cite{numpy}, Pandas\cite{pandas},
Scipy\cite{scipy}, Sklearn\cite{sklearn},
Matplotlib\cite{matplotlib},
PyTorch\cite{pytorch}, and TensorFlow\cite{TensorFlow}.
% This benchmark sets a new standard with its realistic problem scenarios, and rigorous evaluation metrics, addressing critical gaps in existing datasets.
The version of each library can be found on our homepage~\cite{homepage}.

In our experiments,  we let GPT-3.5-turbo generates code for each of the 1000 coding tasks in DS-1000, 562 of the generated programs fail to pass the test cases, and are regarded as repair targets of DSrepair.


\subsection{Baseline}
As far as we know, there are no existing repair \major{approaches} that are specifically designed for data science code generation.
Therefore, in our evaluation, we compare DSrepair against the following state-of-the-art LLM-based \major{approaches} that are capable of repairing general types of code.
While these general-purpose \major{approaches} are effective in many scenarios, they are not tailored to address the unique challenges posed by data science-specific bugs. By addressing the distinct requirements of data science code, we aim to demonstrate DSrepair’s enhanced ability to handle data science-specific repair tasks in comparison to these baselines.

\textbf{Code-Search}~\cite{chen2024code}: Code-Search guides code repair by searching for similar code in the code base and adding the search result as a suggestion to the prompt. 
% We use to query the code base.
Following the practice in the paper, we use the code problem description as the query, and Lucene~\cite{lucene} as searching engine to conduct code search in the code base PyTorrent~\cite{bahrami2021pytorrent}.

\textbf{Chat-Repair}~\cite{xia2023keep}: Chat-Repair leverages the code execution result to check code correctness.
If the code cannot pass the test cases, Chat-Repair incorporates the execution results in the prompt, to provide richer information for code debugging.

\textbf{Self-Debugging-S}~\cite{chen2023teaching}: Self-Debugging-S (S represents Simple) enriches the prompt with the simplest information, a sentence that indicates the code's correctness without more detailed information.
For instance, ``The generated code is incorrect. Please fix the code.''

\textbf{Self-Debugging-E}~\cite{chen2023teaching}: Self-Debugging-E (E represents Explanation) first requests LLM to generate a line-by-line explanation about intermediate execution steps of the generated code.
Then, it requests LLM again to generate code, based on the line-by-line explanation of the incorrect code.

% We ask LLM to generate an explanation feedback format where the LLM is instructed to explain the intermediate execution steps line-by-line, and based on the feedback to generate the code.
\textbf{Self-Repair}~\cite{olausson2023demystifying}: Self-Repair first leverages error information produced by test execution to make LLM produce a short explanation of why the code failed.
Then, it uses the explanation as part of the prompt to request LLM to improve the incorrectly generated code.

\subsection{LLMs}

We use two widely used general-purpose LLMs (GPT-3.5-turbo and GPT-4o-mini~\cite{gptmodel}) and 
two state-of-the-art coding LLMs (DeepSeek-Coder~\cite{guo2024deepseek} and Codestral~\cite{codestral})\footnote{We do not use GPT-4 because it comes at a significantly higher cost.}.
We access all the LLMs by using their commercial APIs.
The details of the four LLMs are shown in Table~\ref{tab: LLM detail}.
We choose Python as our programming language for the code generation tasks because DS-1000 is based on Python.

% We only consider the result of the first attempt of LLM to repair the code.


To control the randomness, 
we set the temperature of all the LLMs to 0.
% \jie{
For each \major{approach}, we let the LLMs generate code for each coding task ten times\footnote{We repeat experiments for Deepseek-Coder-V2 three times only, because the API is no longer available after 2024/09/05.}.
% }
We select the result with median overall performance as the final result~\cite {ouyang2023llm}.
Our RQ7 in Section~\ref{section: RQ7} is about the influence of LLM's inherent randomness on our experiment results.


\begin{table}[htbp]\scriptsize
\caption{LLMs used in the evaluation.}
\vspace*{-0.4cm}
% \resizebox{.46\textwidth}{!}{
\begin{center}
\setlength{\tabcolsep}{4pt}
% \hspace*{-0.3cm}
\begin{tabular}{l r r r}
\toprule
LLM & Version & Input Token Price & Output Token Price \\

\midrule
GPT-3.5-turbo & GPT-3.5-turbo-0125 & \$0.50/1M tokens & \$1.50/1M tokens \\
% \hline
GPT-4o-mini & GPT-4o-mini-2024-07-18 & \$0.15/1M tokens & \$0.60/1M tokens \\
% \hline
DeepSeek-Coder & DeepSeek-Coder-V2 & \$0.14/1M tokens & \$0.28/1M tokens \\
% \hline
Codestral & Codestral-2405 & \$1.00/1M tokens & \$3.00/1M tokens \\
\bottomrule
\end{tabular}
\label{tab: LLM detail}
\end{center}
% }
\vspace*{-0.7cm}
\end{table}


\subsection{Measurement}
\label{section: measurement}

We introduce the following metrics for measuring the performance of DSrepair and baselines.

\textbf{Effectiveness}: We measure the effectiveness of different approaches by checking their capability in fixing incorrectly generated code, including the Absolute Number of Fixes (ANF) and Fix Rate (FR).
The former is the absolute number of coding tasks whose code is successfully fixed. 
The latter is the ratio of ANF against all the buggy code snippets.
For ANF, two of the authors conduct manual verification on the correctness of the patches to make sure that the reported fixes are not overfitted.


\textbf{Cost}: We measure the cost by Token Usage (TU) and Money Spent (MS), which are the most widely used metrics for measuring cost for LLM-based approaches~\cite{xia2023keep}. 
TU refers to the total token usage when using LLM to finish one complete request on average, including input token usage and output token usage.
MS refers to the money cost for LLM to receive and return those tokens.
Below is the formula for the MS:
\vspace{-2mm}
\begin{equation*}
MS = \sum_{n=1}^{N} {(Token_{i,n} \times P_i + Token_{o,n} \times P_o)}
\end{equation*}
\vspace{-1mm}
where $P_i$ and $P_o$ refer to the input and output token price, $Token_{i,n}$ and $Token_{o,n}$ refer to the input token usage and output token usage at certain request $n$, and $N$ refers to the total number of LLM requests.

\section{Results}
\label{section: result and findings}

This section introduces the experimental results as well as the analysis and findings for each RQ.

\subsection{RQ1: Effectiveness of DSrepair}

To answer RQ1, we report the results of Absolute Number of Fixes (ANF) and Fix Rate (FR) for DSrepair and all the baselines with each LLM.
% \jie{
DSrepair initially generates 555 patches that successfully pass the tests from all the LLMs. After manual checking, two of the patches generated by GPT-4o-mini are overfitted\footnote{An overfitted patch passes the test cases but is actually incorrect.
} and have been removed from the repaired set.
% } 
Table~\ref{table: FR all library categories} shows the ultimate results.

We can observe that DSrepair significantly outperforms all the baselines in terms of ANF and FR across all four LLMs we study.
Specifically, 
DSrepair can fix the buggy code for 104, 145, 164, and 140 coding tasks for the four LLMs, respectively,
while the second-best results are 72, 127, 136, and 106, respectively.

For specific data science libraries, 
DSrepair outperforms the baselines for most libraries. 
For example, for GPT-3.5-turbo, DSrepair has the highest fix rate in Numpy, Scipy, Sklearn, Matplotlib, and PyTorch.
For Codestral, DSrepair performs the best on Numpy, Pandas, Sklearn, Matplotlib, and PyTorch.


% As a zero-shot~\cite{chen2021evaluating} prompt, Chat-Repair and Self-Debugging-S show similar performance when using GPT-3.5-turbo as LLM, while Chat-Repair performs better when using DeepSeek-Coder.
% This is because Chat-Repair provides LLM the standard error information of the buggy code, while Self-Debugging-S only provides a simple instruction saying that the code is erroneous.
% As an LLM especially for code, DeepSeek-Coder is more sensitive to code-related information, such as standard error information.
% Therefore, when providing more code-related information, the DeepSeek-Coder has a better performance than a general LLM, GPT-3.5-turbo. 


% Self-Debugging-E and Self-Repair are Chain-of-Thought (CoT)~\cite{wei2022chain} prompt.
% Compared with zero-shot and few-shot, CoT has an overall better performance when using GPT-3.5-turbo as LLM.
% However, Self-Debugging-E shows worse results than Self-Repair.
% Because Self-Debugging-E only provides a line-by-line explanation of the buggy code without providing any guide on how to repair the code.


% The purpose of this code problem is to generate a custom metric with certain positions with a value of 1, elsewhere, it should be filled by 0.
% We can see that most of the baseline prompts generate the incorrect code similar to the first generated incorrect code, as shown in Listing~\ref{lst: incorrect solution}, while DSrepair generates the correct code as shown in Listing~\ref{lst: correct solution}.
% In this problem, the buggy code uses the function \textit{tf.one\_hot()} (short for \textit{TensorFlow.one\_hot()}).
% As stated in the TensorFlow official document, the full expression of \textit{tf.one\_hot()} contains optional parameters, including \textit{on\_value},  \textit{off\_value}, and etc.
% The locations represented by indices in indices take value \textit{on\_value}, while all other locations take value \textit{off\_value}.
% \textit{on\_value} is not provided, it will default to the value 1 with type \textit{dtype} If \textit{off\_value} is not provided, it will default to the value 0 with type \textit{dtype}
% By enriching the prompt with knowledge of how to correctly use \textit{tf.one\_hot()}, LLM is more likely to realize that the optional parameters \textit{on\_value} and \textit{off\_value} in the \textit{tf.one\_hot()} would affect the result.

% In addition, through our enriched bug information, LLM realized the difference between the actual results and the expected results, so we added an extra line of code to adjust the generated results.

% \begin{lstlisting}[caption={Incorrect solution of Problem No. 655.}, label={lst: incorrect solution}]
% result = tf.one_hot(labels, depth=10, dtype=tf.int32)
% \end{lstlisting}

% \begin{lstlisting}[caption={Correct solution of Problem No. 670.}, label={lst: correct solution}]
% result = tf.one_hot(labels, depth=10, on_value=1, off_value=0, dtype=tf.int32)
% result = tf.reverse(result, axis=[1])
% \end{lstlisting}


\begin{table*}[t!]
\caption{RQ1: Effectiveness of DSrepair against the baselines. Values are shown in the format ANF (FR). ANF is the Absolute Number of Fixes. FR is Fix Rate. The results indicate that DSrepair outperforms the baselines for the majority of the libraries.}
\centering
% \vspace{0mm}
% \setlength{\tabcolsep}{4pt}
\resizebox{.9\textwidth}{!}{
\begin{tabular}{l l r r r r r r r r}
\toprule
Model & Approach & Numpy & Pandas & Scipy & Sklearn & Matplotlib & PyTorch & TensorFlow & Total\\
\midrule

\multirow{6}{*}{GPT-3.5-turbo} & Code-Search & 5 (4.63\%) & 4 (2.12\%) & 2 (3.33\%) & 11 (14.86\%) & 0 (0.00\%) & 2 (4.55\%) & 2 (7.41\%) & 26 (4.63\%) \\
& Chat-Repair & 21 (19.44\%) & 7 (3.70\%) & 5 (8.33\%) & 18 (24.32\%) & 4 (6.67\%) & 6 (13.64\%) & 2 (7.41\%) & 63 (11.21\%) \\
& Self-Debugging-S & 14 (12.96\%) & 5 (2.65\%) & 6 (10.00\%) & 13 (17.57\%) & 4 (6.67\%) & 7 (15.91\%) & 2 (7.41\%) & 51 (9.07\%) \\
& Self-Debugging-E & 20 (18.52\%) & \textbf{19 (10.05\%)} & 2 (3.33\%) & 14 (18.92\%) & 6 (10.00\%) & 3 (6.82\%) & \textbf{4 (14.81\%)} & 68 (12.10\%) \\
& Self-Repair & 17 (15.74\%) & 17 (8.99\%) & 5 (8.33\%) & 12 (16.22\%) & 8 (13.33\%) & 9 (20.45\%) & \textbf{4 (14.81\%)} & 72 (12.81\%) \\
\cmidrule{2-10}
& \textbf{DSrepair} & \textbf{24 (22.22\%)} & 17 (8.99\%) & \textbf{15 (25.00\%)} & \textbf{20 (27.03\%)} & \textbf{10 (16.67\%)} & \textbf{15 (34.09\%)} & 3 (11.11\%) & \textbf{104 (18.51\%)} \\

\midrule
\multirow{6}{*}{GPT-4o-mini} & Code-Search & 28 (25.93\%) & 21 (11.11\%) & 11 (18.33\%) & 11 (14.86\%) & 15 (25.00\%) & 14 (31.82\%) & 3 (11.11\%) & 103 (18.33\%) \\
& Chat-Repair & 29 (26.85\%) & 28 (14.81\%) & 14 (23.33\%) & 19 (25.68\%) & 14 (23.33\%) & 13 (29.55\%) & \textbf{5 (18.52\%)} & 122 (21.71\%) \\
& Self-Debugging-S & 32 (29.63\%) & 25 (13.23\%) & \textbf{16 (26.67\%)} & 20 (27.03\%) & 7 (11.67\%) & 14 (31.82\%) & 4 (14.81\%) & 118 (21.00\%) \\
& Self-Debugging-E & \textbf{35 (32.41\%)} & \textbf{33 (17.46\%)} & 12 (20.00\%) & 16 (21.62\%) & 10 (16.67\%) & 17 (38.64\%) & 4 (14.81\%) & 127 (22.60\%) \\
& Self-Repair & 34 (31.48\%) & 32 (16.93\%) & 13 (21.67\%) & 15 (20.27\%) & 11 (18.33\%) & 15 (34.09\%) & \textbf{5 (18.52\%)} & 125 (22.24\%) \\
\cmidrule{2-10}
& \textbf{DSrepair} & 33 (30.56\%) & 20 (10.58\%) & 15 (25.00\%) & \textbf{31 (41.89\%)} & \textbf{22 (36.67\%)} & \textbf{19 (43.18\%)} & \textbf{5 (18.52\%)} & \textbf{145 (25.80\%)} \\
% & \textbf{DSrepair} & \textbf{34 (31.48\%)} & 20 (10.58\%) & 15 (25.00\%) & \textbf{31 (41.89\%)} & \textbf{22 (36.67\%)} & \textbf{19 (43.18\%)} & \textbf{5 (18.52\%)} & \textbf{146 (25.98\%)} \\

\midrule
\multirow{6}{*}{DeepSeek-Coder} & Code-Search & 23 (21.30\%) & 11 (5.82\%) & 2 (3.33\%) & 12 (16.22\%) & 17 (28.33\%) & 8 (18.18\%) & 4 (14.81\%) & 77 (13.70\%) \\
& Chat-Repair & 39 (36.11\%) & 22 (11.64\%) & 8 (13.33\%) & 18 (24.32\%) & 20 (33.33\%) & 14 (31.82\%) & 7 (25.93\%) & 128 (22.78\%) \\
& Self-Debugging-S & 33 (30.56\%) & 26 (13.76\%) & 9 (15.00\%) & 11 (14.86\%) & 13 (21.67\%) & 10 (22.73\%) & 7 (25.93\%) & 109 (19.40\%) \\
& Self-Debugging-E & 28 (25.93\%) & 23 (12.17\%) & 3 (5.00\%) & 18 (24.32\%) & 12 (20.00\%) & 10 (22.73\%) & 6 (22.22\%) & 100 (17.79\%) \\
& Self-Repair & \textbf{40 (37.04\%)} & 22 (11.64\%) & \textbf{12 (20.00\%) }& 24 (32.43\%) & 17 (28.33\%) & 11 (25.00\%) & \textbf{10 (37.04\%)} & 136 (24.20\%) \\
\cmidrule{2-10}
& \textbf{DSrepair} & 38 (35.19\%) & \textbf{28 (14.81\%)} & 10 (16.67\%) & \textbf{31 (41.89\%)} & \textbf{23 (38.33\%)} & \textbf{24 (54.55\%)} & \textbf{10 (37.04\%)} & \textbf{164 (29.18\%)} \\


\midrule
\multirow{6}{*}{Codestral} & Code-Search & 27 (25.00\%) & 13 (6.88\%) & 9 (15.00\%) & 24 (32.43\%) & 19 (31.67\%) & 8 (18.18\%) & \textbf{6 (22.22\%)} & 106 (18.86\%) \\
& Chat-Repair & 28 (25.93\%) & 13 (6.88\%) & \textbf{12 (20.00\%)} & 21 (28.38\%) & 16 (26.67\%) & 10 (22.73\%) & 5 (18.52\%) & 105 (18.68\%) \\
& Self-Debugging-S & 27 (25.00\%) & 19 (10.05\%) & 7 (11.67\%) & 13 (17.57\%) & 8 (13.33\%) & 9 (20.45\%) & 2 (7.41\%) & 85 (15.12\%) \\
& Self-Debugging-E & 26 (24.07\%) & 21 (11.11\%) & 8 (13.33\%) & 16 (21.62\%) & 10 (16.67\%) & 11 (25.00\%) & 4 (14.81\%) & 96 (17.08\%) \\
& Self-Repair & \textbf{32 (29.63\%)} & 17 (8.99\%) & 6 (10.00\%) & 14 (18.92\%) & 10 (16.67\%) & 12 (27.27\%) & 5 (18.52\%) & 96 (17.08\%) \\
\cmidrule{2-10}
& \textbf{DSrepair} & \textbf{32 (29.63\%)} & \textbf{30 (15.87\%)} & 9 (15.00\%) & \textbf{28 (37.84\%)} & \textbf{21 (35.00\%)} & \textbf{17 (38.64\%)} & 3 (11.11\%) & \textbf{140 (24.91\%)} \\

\bottomrule
\end{tabular}
\label{table: FR all library categories}
}
\vspace*{-0.4cm}
\end{table*}

\vspace{-0.0cm}
\begin{figure}[h!]
\centerline{\includegraphics[width=0.72\linewidth]{fig/case_study_example.pdf}}
% \vspace{-2mm}
\caption{A code problem example from DS-1000. The incorrect solution is generated from Self-Repair, and the correct solution is generated from DSrepair. 
By incorporating knowledge of the invoked API, DSrepair can assist LLMs in generating solutions with correct API usage.
% By providing API knowledge of API invoked, DSrepair can guide LLMs to generate the correct solution with correct API usage.
}
\label{fig: case analysis}
\vspace*{-0.2cm}
\end{figure}


% % \shuyin{TODO: change the spacing}
% \begin{lstlisting}[caption={Incorrect solution of Problem No. 514.}, label={lst: incorrect solution}, linewidth=0.95\linewidth]
% plt.minorticks_on(axis='x')
% \end{lstlisting}

% \begin{lstlisting}[caption={Correct solution of Problem No. 514.}, label={lst: correct solution}, linewidth=0.95\linewidth]
% import matplotlib.ticker as ticker
% # Turn on minor ticks on x-axis
% plt.gca().xaxis.set_minor_locator(ticker.AutoMinorLocator())
% \end{lstlisting}

Fig~\ref{fig: case analysis} shows an example from Codestral where the error can be solved by DSrepair, but cannot be solved by Self-Repair.
The purpose of this code problem is to only turn on minor ticks on the x-axis.
Self-Repair generates an incorrect fix, while DSrepair generates the correct fix.
In this problem, the buggy code uses the function \textit{plt.minorticks\_on} (short for \textit{matplotlib.pyplot.minorticks\_on}), with parameter \textit{axis=`x'}.
However, as stated in the Matplotlib official document, the full expression of \textit{plt.minorticks\_on} is \textit{matplotlib.pyplot.minorticks\_on()} with no parameters, which means that \textit{plt.minorticks\_on} can control the display of minor ticks on both x-axis and y-axis, but there is no optional parameter to control the display on x-axis or y-axis only.
With DSrepair, by enriching the prompt with knowledge of how to use \textit{plt.minorticks\_on} correctly, LLM is more likely to realize that putting parameters in function \textit{plt.minorticks\_on} is incorrect.
The correct solution uses \textit{plt.gca().xaxis.set\_minor\_locator()} instead to reach the goal of the code problem.



Looking deeper into the buggy code that DSrepair cannot fix, we identify two primary reasons. 
Firstly, the presence of multiple errors in the code poses a significant challenge.
DSrepair is designed to address specific errors highlighted by standard error messages.
However, when a code segment contains hidden bugs that come out only after fixing one bug, our \major{approach} struggles to resolve all issues in a single request.
Secondly, the insufficiency of information provided from the test cases in the description limits the repair effectiveness.
Some descriptions lack accompanying test cases, which are crucial for identifying and fixing errors.
For instance, if the buggy code triggers an assertion error, the absence of concrete test cases impedes the LLM's ability to generate a precise fix.
Even when generated code passes the given test cases, it may still fail during actual evaluation.
Simply informing the LLM that the code is incorrect without detailed guidance is often inadequate for effective repair.





\begin{tcolorbox}
\textbf{\underline{Answer to RQ1:}}
DSrepair significantly outperforms all the baselines in fixing buggy data science code.
Specifically, DSrepair demonstrates notable improvements across four LLMs by fixing 104, 145, 164, and 140 buggy programs respectively, with improvement rates of 44.4\%, 14.2\%, 20.6\%, and 32.1\%  
compared to the second-best baseline, respectively. 
\end{tcolorbox}

% \jie{
Please note that our comparison with baselines for detecting general software logic bugs is not meant to imply that DSrepair outperforms these baselines in all domains. Rather, we show that approaches not specifically designed for data science struggle with addressing data science bugs. By addressing the unique needs of data science code, DSrepair can significantly improve repair outcomes in the context of data science.
% }


\subsection{RQ2: Overlap with Baseline}

In this RQ, we conduct an overlap analysis by comparing the solved buggy code snippets between DSrepair and the baselines.
Fig~\ref{fig: RQ2 upsetplot} shows the upset plots\cite{upsetplot} for different \major{approaches} and the intersection of their ANF.


\begin{figure*}[h!]
\centering
  \begin{subfigure}{0.24\linewidth}
    % \centering
    \hspace*{-0.7cm}
    \includegraphics[width=1.1\linewidth, trim={0 1.7cm 0 1.3cm}, clip]{fig/RQ1_upsetplot_gpt_3.5_turbo.pdf}
    \caption{GPT-3.5-turbo}
  \end{subfigure}
    \begin{subfigure}{0.24\linewidth}
    % \centering
    \hspace*{-0.7cm}
    \includegraphics[width=1.1\linewidth, trim={0 1.7cm 0 1.3cm}, clip]{fig/RQ1_upsetplot_gpt_4o_mini.pdf}
    \caption{GPT-4o-mini}
  \end{subfigure}
  \begin{subfigure}{0.24\linewidth}
    % \centering
    \hspace*{-0.7cm}
    \includegraphics[width=1.1\linewidth, trim={0 1.7cm 0 1.3cm}, clip]{fig/RQ1_upsetplot_deepseek.pdf}
    \caption{DeepSeek-Coder}
  \end{subfigure}
  \begin{subfigure}{0.24\linewidth}
      \hspace*{-0.7cm}
    \includegraphics[width=1.1\linewidth, trim={0 1.7cm 0 1.3cm}, clip]{fig/RQ1_upsetplot_codestral.pdf}
    \caption{Codestral}
  \end{subfigure}
\caption{RQ2: Upset plots for overlap analysis.
For example, in (a), the first column indicates that
19 buggy code snippets that can be fixed by both DSrepair and Code-Search.}
\label{fig: RQ2 upsetplot}
\end{figure*}




We can observe that the fixed buggy code overlaps between DSrepair and \major{baselines} are overall less than 55\% of the bug fixes from DSrepair.
This means that about half of the code fixes from DSrepair could not be fixed by \major{baselines}.
For example, in Fig~\ref{fig: RQ2 upsetplot}(a), DSrepair can fix 104 buggy code snippets, while Self-Repair can only fix 72 buggy code snippets.
The overlap between their fixed code snippets is only 34, which means that DSrepair has 70 (104-34=70) code snippets that Self-Repair cannot fix, and Self-Repair has 38 (72-34=38) code snippets that DSrepair cannot fix.
The overlap among the six \major{baselines} is quite low (5 for GPT-3.5-turbo, 36 for GPT-4o-mini, 21 for DeepSeek-Coder, and 14 for Codestral).
Overall, each \major{baseline} shows the uniqueness of buggy code repair.



% \begin{figure}
%     \centering
%     \begin{lstlisting}[linewidth=0.9\linewidth]
%     import matplotlib.ticker as ticker
%     # Turn on minor ticks on x-axis
%     plt.gca().xaxis.set_minor_locator(ticker.AutoMinorLocator())
%     \end{lstlisting}
%     \caption={Incorrect solution of Problem No. 514.}
%     \label{lst: correct solution}
% \end{figure}

% \vspace*{-0.3cm}
% \begin{figure}
%     \centering
%     \begin{lstlisting}[linewidth=0.9\linewidth]
%     import matplotlib.ticker as ticker
%     # Turn on minor ticks on x-axis
%     plt.gca().xaxis.set_minor_locator(ticker.AutoMinorLocator())
%     \end{lstlisting}
%     \caption{Correct solution of Problem No. 514.}
%     \label{lst: correct solution}
% \end{figure}



\begin{tcolorbox}
\textbf{\underline{Answer to RQ2:}}
DSrepair uniquely fixes approximately 55\% of buggy code snippets that \major{baselines} are unable to fix. 
\end{tcolorbox}

\subsection{RQ3: Cost of DSrepair}

To answer RQ3, we assess the financial costs associated with using DSrepair by quantifying the US dollar spent on interactions with using the APIs of the four LLMs.
The cost of each request to these models depends directly on the number of tokens processed, including both the tokens used for input and those generated as output.
We calculate the expenses incurred during these interactions by measuring the Token Usage (TU) of DSrepair and then converting this usage into actual Money Spent (MS), comparing these against the cost of our baselines. 

% Each request of commercial LLM costs.
% The cost depends on the token usage of the request, including input token usage and output token usage.
Table~\ref{table: token usage} shows the TU and MS for different \major{approaches}.
Fig~\ref{fig: RQ2 scatter plot} shows a scatter plot of TU and FR.
We observe that DSrepair costs less token usage than the second-best baseline.
For example, 
DSrepair uses only 1262.14, 1584.74, 1453.96, and 1407.15 tokens per code problem, while Self-Repair needs 1529.63, 1944.56, 1931.20, and 1657.99 tokens per code problem.
Based on the real-time price in Table~\ref{tab: LLM detail}, the money spent on each request is \$0.00073, \$0.00043,  \$0.00025, and \$0.00185 for using GPT-3.5-turbo, GPT-4o-mini, DeepSeek-Coder, and Codestral as LLM respectively.

We can also observe that DSrepair's token usage with GPT-4o-mini, DeepSeek-Coder, and Codestral is higher than when using GPT-3.5-turbo.
This is because the return of these code LLMs may not follow the prompt's output format instruction.
The responses typically contain more information, such as line-by-line code comments, natural language explanation of the code, and an analysis of why the first generated code is incorrect, all of which contribute to extra costs.



% \begin{table}[h!]\scriptsize
% \caption{RQ3:  TU refers to Token Usage (input token usage + output token usage), and MS refers to Money Spent for LLM receiving the prompt and generating the response.}
% \vspace{0mm}
% \setlength{\tabcolsep}{4pt}
% % \resizebox{.48\textwidth}{!}{
% \centering
% \begin{tabular}{l r r r r r r }
% \toprule

% % & No. of Fixed Code Snippets & Improvement Rate\\
% \multirow{2}{*}{Baseline} & \multicolumn{2}{c}{GPT-3.5-turbo} & \multicolumn{2}{c}{DeepSeek-Coder} & \multicolumn{2}{c}{Codestral} \\
% \cmidrule{2-7}
%  & TU & MS & TU & MS & TU & MS \\
% \midrule
% Code-Search  & 1842.49 & \$0.00123 &  1804.85 & \$0.00030 & 1704.98 & \$0.00218\\
% Chat-Repair  & 736.22 & \$0.00050 &  696.66 & \$0.00012 & 753.26 & \textbf{\$0.00118} \\
% Self-Debugging-S  & \textbf{545.78} & \textbf{\$0.00041} &  \textbf{585.90} & \textbf{\$0.00011} &  \textbf{685.63} & \$0.00123 \\
% Self-Debugging-E  & 1725.47 & \$0.00122 &  1763.83 & \$0.00030 &  1783.40 & \$0.00263 \\
% Self-Repair & 1565.32 & \$0.00106  & 1949.00 & \$0.00034 &  1681.96 & \$0.00235 \\
% \midrule
% \textbf{DSrepair} & 1277.78 & \$0.00073 & 1471.76 & \$0.00025  & 1408.94 & \$0.00186 \\

% \bottomrule
% \end{tabular}
% \label{table: token usage}
% % }

% \end{table}


\begin{table*}[h!]\scriptsize
\caption{RQ3: Cost of different approaches. TU refers to Token Usage (input token usage + output token usage), and MS refers to Money Spent for LLM receiving the prompt and generating the response.}
\vspace{0mm}
% \setlength{\tabcolsep}{4pt}
% \resizebox{.48\textwidth}{!}{
\centering
\begin{tabular}{l r r r r r r r r}
\toprule

%  & No. of Fixed Code Snippets & Improvement Rate\\
\multirow{2}{*}{Approach} & \multicolumn{2}{c}{GPT-3.5-turbo} & \multicolumn{2}{c}{GPT-4o-mini} & \multicolumn{2}{c}{DeepSeek-Coder} & \multicolumn{2}{c}{Codestral} \\
\cmidrule{2-9}
 & TU & MS & TU & MS & TU & MS & TU & MS\\
\midrule
Code-Search & 1829.66 & \$0.00124 & 1697.94 & \$0.00034 & 1804.85 & \$0.00030 & 1707.45 & \$0.00218  \\
Chat-Repair & 736.22 & \$0.00050 & 788.97 & \$0.00022 & 696.66 & \$0.00012 & 784.52 & \$0.00121  \\
Self-Debugging-S & \textbf{546.13} & \textbf{\$0.00041} & \textbf{605.48} & \textbf{\$0.00018} & \textbf{585.90} & \textbf{\$0.00011} & \textbf{634.84} & \textbf{\$0.00108} \\
Self-Debugging-E & 1695.55 & \$0.00120 & 2410.03 & \$0.00070 & 1738.98 & \$0.0003 & 1763.98 & \$0.00260 \\
Self-Repair & 1529.63 & \$0.00104 & 1944.56 & \$0.00053 & 1931.20 & \$0.00034 & 1657.99 & \$0.00232 \\
\midrule
DSrepair & 1262.14 & \$0.00073 & 1584.74 & \$0.00043 & 1453.96 & \$0.00025 & 1407.15 & \$0.00185  \\

\bottomrule
\end{tabular}
\label{table: token usage}
% }
\vspace*{-0.4cm}
\end{table*}


% \end{figure}

\begin{figure}[htbp]
\vspace*{-0.4cm}
% \hspace*{-0.1cm}
\centerline{\includegraphics[width=0.8\linewidth]{fig/scatter_plot_token_usage_and_fix_rate_seperate.pdf}}
% \vspace*{-0.5cm}
\caption{RQ3: Scatter plot of TU (Token Usage) and FR (Fix Rate).
DSrepair is the optimal approach (the star markers) compared with baselines.}
\label{fig: RQ2 scatter plot}
\vspace*{-0.4cm}
\end{figure}

\begin{tcolorbox}
\textbf{\underline{Answer to RQ3:}}
Compared to the second-best baseline, DSrepair uses fewer tokens (1262.14, 1584.74, 1453.96, and 1407.15), saving 17.49\%, 34.24\%, 24.71\%, and 17.59\% tokens per code task respectively across different LLMs.
\end{tcolorbox}


\subsection{RQ4: Influence of Prompt Design}
To figure out how different prompt components influence DSrepair, we conduct an ablation study.
In DSrepair, there are two key components, i.e., API knowledge and bug knowledge.
In the ablation study, we compare DSrepair's performance with the performance of `No knowledge' (prompt without API and bug knowledge), `API Knowledge only' (prompt without bug knowledge provided by tests), and `Bug Knowledge only' (prompt without API knowledge)
We use  `DSrepair w/o API\&Bug', `DSrepair w/o Bug', and `DSrepair w/o API' to represent `No knowledge', `API Knowledge only', and `Bug Knowledge only' for short. 
% `DSrepair w/o Bug' can also indicate scenarios where the provided test cases are missing or very limited or.

The results of the ablation study are shown in Table~\ref{table: Ablation Study Fix Rate}.
When using GPT-3.5-turbo as LLM, the overall performance of DSrepair (18.51\% FR) is better than DSrepair w/o Bug (14.77\% FR) and DSrepair w/o API (16.73\% FR).
The overall performance for GPT-4o-mini of DSrepair (25.80\% FR) is better than DSrepair w/o Bug (23.49\% FR) and DSrepair w/o API (22.06\% FR).
When using DeepSeek-Coder as our LLM, DSrepair still stands for the best, with 29.18\% total FR.
However, DSrepair w/o Bug has better overall performance than DSrepair w/o API, where using DSrepair w/o Bug has 28.83\% FR while using DSrepair w/o API only has 27.94\% FR.
Using Codestral as LLM, DSrepair has 24.91\% FR, which is higher than both DSrepair w/o Bug (24.51\%) and DSrepair w/o API (24.38\%).
Interestingly, we observe that the FR declines in DSrepair w/o Bug (GPT-3.5-turbo), DSrepair w/o Bug and w/o API (GPT-4o-mini) and DSrepair w/o API (DeepSeek-Coder) compared with DSrepair w/o API\&Bug.
% Moreover, DSrepair does not always perform the best in every library category.

% To better figure out the influence, we look at the overlap of fixed code snippets among DSrepair w/o API\&Bug, DSrepair w/o Bug, DSrepair w/o API, and DSrepair.
% Fig~\ref{fig: RQ3_overlap venn diagram} shows the Venn plots between different prompts in our ablation study.
% We observe that there is still uniqueness in using different components in the prompt.
% For using GPT-3.5-turbo, there are 39 code snippets fixed by all four kinds of prompts. 
% 27, 5, 3, and 8 code snippets can only be fixed by DSrepair w/o API\&Bug, DSrepair w/o Bug, DSrepair w/o API, and DSrepair.
% Changing the LLM to DeepSeek-Coder, we can see that 91 code snippets can be fixed by all four kinds of prompts.
% DSrepair w/o API\&Bug, DSrepair w/o Bug, DSrepair w/o API, and DSrepair can fix 14, 5, 15, and 9 code snippets exclusively.
% When using Codestral as LLM, there are 67 code snippets that lie in the code fixing overlap of all four prompts.
% 19, 13, 11, and 10 code snippets can and only can be fixed by DSrepair w/o API\&Bug, DSrepair w/o Bug, DSrepair w/o API, and DSrepair respectively.
% Although the four prompts are strictly inclusive, the results of their repaired code do not inherit this inclusive relationship.
% Each prompt can repair unique buggy code cases.


\begin{table}[h!]\scriptsize
\caption{RQ4: Results of ablation study. `DSrepair w/o API\&Bug' is for prompt without API and bug knowledge, `DSrepair w/o Bug' is for prompt without bug knowledge, and `DSrepair w/o API' refers to prompt without API knowledge. ANF is the Absolute Number of Fixes. FR is Fix Rate.}
\centering
\vspace{0mm}
\resizebox{.38\textwidth}{!}{

\begin{tabular}{l l r r}
\toprule
Model & Prompt & ANF & FR\\
\midrule

\multirow{4}{*}{GPT-3.5-turbo} & DSrepair w/o API & 94 & 16.73\% \\
& DSrepair w/o Bug & 83 & 14.77\% \\
& DSrepair w/o API\&Bug & 85 & 15.12\% \\
\cmidrule{2-4}
& \textbf{DSrepair} & \textbf{104} & \textbf{18.51\%} \\

\midrule
\multirow{4}{*}{GPT-4o-mini} & DSrepair w/o API & 124 & 22.06\% \\
& DSrepair w/o Bug & 132 & 23.49\% \\
& DSrepair w/o API\&Bug & 133 & 23.67\% \\
\cmidrule{2-4}
& \textbf{DSrepair} & \textbf{145} & \textbf{25.80\%} \\

\midrule
\multirow{4}{*}{DeepSeek-Coder} & DSrepair w/o API & 157 & 27.94\% \\
& DSrepair w/o Bug & 162 & 28.83\% \\
& DSrepair w/o API\&Bug & 160 & 28.47\% \\
\cmidrule{2-4}
& \textbf{DSrepair} & \textbf{164} & \textbf{29.18\%} \\

\midrule
\multirow{4}{*}{Codestral} & DSrepair w/o API & 137 & 24.38\% \\
& DSrepair w/o Bug & 139 & 24.51\% \\
& DSrepair w/o API\&Bug & 123 & 21.89\% \\
\cmidrule{2-4}
& \textbf{DSrepair} & \textbf{140} & \textbf{24.91\%} \\

% \multirow{4}{*}{GPT-3.5-turbo} & DSrepair w/o API\&Bug & 85 & 15.12\% \\
% & DSrepair w/o Bug  & 83 & 14.77\%  \\
% & DSrepair w/o API  & 94 & 16.73\% \\
% \cmidrule{2-4}
% & \textbf{DSrepair} & \textbf{102} & \textbf{18.15\%} \\
% \midrule
% \multirow{4}{*}{DeepSeek-Coder} & DSrepair w/o API\&Bug & 160 & 28.47\%  \\
% & DSrepair w/o Bug  & 162 & 28.83\%  \\
% & DSrepair w/o API  & 157 & 27.94\%  \\
% \cmidrule{2-4}
% & \textbf{DSrepair} & \textbf{164} & \textbf{29.18\%} \\
% \midrule
% \multirow{4}{*}{Codestral} & DSrepair w/o API\&Bug  & 123 & 21.89\%\\
% & DSrepair w/o Bug  & 139 & 24.51\%  \\
% & DSrepair w/o API & 141 & 25.09\%  \\
% \cmidrule{2-4}
% & \textbf{DSrepair}  & \textbf{143} & \textbf{25.44\%} \\

\bottomrule
\end{tabular}
\label{table: Ablation Study Fix Rate}
}
\vspace*{-0.4cm}
\end{table}

% \begin{table*}[h!]\scriptsize
% \caption{\jie{to save space, only keep total results, explain full results are on our homepage}RQ4: FR (Fix Rate) result of Ablation Study, where `DSrepair w/o API\&Bug' refers to prompt without API knowledge and bug knowledge, `DSrepair w/o Bug' refers to prompt without bug knowledge, and `DSrepair w/o API' refers to prompt without API knowledge. FR refers to Fix Rate. ANF refers to the Absolute Number of Fixes.}
% \vspace{0mm}
% % \resizebox{.48\textwidth}{!}{
% \centering
% \begin{tabular}{l l r r r r r r r r}
% \toprule
% Model & Prompt & Numpy & Pandas & Scipy & Sklearn & Matplotlib & PyTorch & TensorFlow & Total FR (ANF)\\
% \midrule

% \multirow{4}{*}{GPT-3.5-turbo} & DSrepair w/o API\&Bug & 16.67\% & 6.35\% & 18.33\% & 28.38\% & 18.33\% & 18.18\% & \textbf{14.81\%} & 15.12\% (85) \\
% & DSrepair w/o Bug  & 15.74\% & 7.41\% & 18.33\% & 28.38\% & 15.00\% & 18.18\% & 11.11\% & 14.77\% (83) \\
% & DSrepair w/o API  & 19.44\% & 6.88\% & 21.67\% & 28.38\% & \textbf{20.00\%} & 27.27\% & 7.41\% & 16.73\% (94) \\
% & DSrepair & \textbf{21.30\%} & \textbf{8.99\%} & \textbf{23.33\%} & \textbf{29.73\%} & 15.00\% & \textbf{34.09\%} & 7.41\% & \textbf{18.15\% (102)} \\
% \midrule
% \multirow{4}{*}{DeepSeek-Coder} & DSrepair w/o API\&Bug & 34.26\% & \textbf{16.93\%} & 20.00\% & 44.59\% & 36.67\% & 36.36\% & 29.63\% & 28.47\% (160) \\
% & DSrepair w/o Bug  & \textbf{38.89\%} & 12.17\% & \textbf{23.33\%} & 45.95\% & 38.33\% & 45.45\% & 22.22\% & 28.83\% (162)  \\
% & DSrepair w/o API  & 30.56\% & 12.70\% & 20.00\% & \textbf{50.00\%} & 35.00\% & 47.73\% & 33.33\% & 27.94\% (157) \\
% & DSrepair & 35.19\% & 14.81\% & 16.67\% & 41.89\% & \textbf{38.33\%} & \textbf{54.55\%} & \textbf{37.04\%} & \textbf{29.18\% (164)} \\
% \midrule
% \multirow{4}{*}{Codestral} & DSrepair w/o API\&Bug & 27.78\% & 12.70\% & \textbf{18.33\%} & 36.49\% & 20.00\% & 38.64\% & 7.41\% & 21.89\% (123) \\
% & DSrepair w/o Bug & 28.70\% & 14.43\% & 15.00\% & 33.78\% & \textbf{36.67\%} & 43.18\% & \textbf{18.52\%} & 24.51\% (139) \\
% & DSrepair w/o API & 29.63\% & 15.34\% & 13.33\% & 37.84\% & 33.33\% & \textbf{45.45\%} & 14.81\% & 25.09\% (141) \\
% & DSrepair & \textbf{32.41\%} & \textbf{15.34\%} & 13.33\% & \textbf{40.54\%} & 31.67\% & 43.18\% & 11.11\% & \textbf{25.44\% (143)} \\

% \bottomrule
% \end{tabular}
% \label{table: Ablation Study Fix Rate}
% % }
% \vspace*{-0.2cm}
% \end{table*}



% \begin{figure*}[htbp]
% % \vspace{-1cm}
%   \begin{subfigure}{0.32\linewidth}
%     \centering
%     \includegraphics[width=\linewidth, trim={0 5.9cm 0 6.8cm},clip]{fig/RQ3_venn_plot_gpt.pdf}
%     \caption{GPT-3.5-turbo}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}{0.32\linewidth}
%     \centering
%     \includegraphics[width=\linewidth, trim={0 5.9cm 0 6.8cm},clip]{fig/RQ3_venn_plot_deepseek.pdf}
%     \caption{DeepSeek-Coder}
%   \end{subfigure}
%     \hfill
%   \begin{subfigure}{0.32\linewidth}
%     \centering
%     \includegraphics[width=\linewidth, trim={0 5.9cm 0 6.8cm},clip]{fig/RQ3_venn_plot_codestral.pdf}
%     \caption{Codestral}
%   \end{subfigure}
% \caption{RQ5: Overlap of the fixed code snippets. Each partition contains a number of fixed code snippets by using different prompts.}
% \label{fig: RQ3_overlap venn diagram}
% \end{figure*}

\begin{tcolorbox}
\textbf{\underline{Answer to RQ4:}}
Both enriched API knowledge and enriched bug knowledge in the prompt contribute to the final effectiveness of DSrepair.
% API knowledge and bug knowledge both have significant contributions to DSrepair. 
% The DSrepair has the best performance by combining them.
% Different prompts in the ablation study can repair unique buggy code cases.
\end{tcolorbox}

\subsection{RQ5: Comparison of Different Knowledge Retrieval Approaches}

To answer RQ5, we examine the impact of various knowledge retrieval \major{approaches} on the performance of DSrepair.
Specifically, we compare knowledge retrieval through KG (DSrepair) with knowledge retrieval through plain-text searching.
For plain-text searching, we extract API knowledge using invoked API names as keywords.
The API knowledge is retrieved as a window of text encompassing 50 tokens per keyword.
This window length was chosen to match the average size of the retrieval results from DS-KG for each keyword, ensuring a fair comparison.
All other experimental settings are kept consistent with those used in DSrepair.


Table~\ref{table: plain text} shows the results of different knowledge retrieval approaches for DSrepair.
We can see that retrieving knowledge from plain text only has 14.77\%, 22.60\%, 25.44\%, and 22.78\% Fix Rate for four tested LLMs.
Retrieving knowledge from plain text uses 1432.42, 1827.59, 1723.77, and 1636.46 tokens per buggy code, which is higher than retrieval from DS-KG, and thus has higher Money Spent on four LLMs.

\begin{tcolorbox}
\textbf{\underline{Answer to RQ5:}}
Knowledge graph-based retrieval outperforms plain text-based retrieval in fixing buggy data science code.
The former's fix rate is 18.51\%, 25.80\%, 29.18\%, and 24.91\% for GPT-3.5-turbo, GPT-4o-mini, DeepSeek-Coder, and Codestral, respectively, compared to 14.77\%, 22.60\%, 25.44\%, and 22.78\% for the latter.  
% Knowledge Retrieval from DS-KG is more effective than searching from plain text API documents.
\end{tcolorbox}

% \begin{table*}[h!]\scriptsize
% \caption{\jie{change this table to single column}RQ5: Knowledge retrieval from DS-KG is better than from plain text. FR refers to Fix Rate, TU refers to Token Usage (input token usage + output token usage), and MS refers to Money Spent for LLM receiving the prompt and generating the response.}
% \vspace{0mm}
% % \resizebox{.48\textwidth}{!}{
% \centering
% \begin{tabular}{l r r r r r r r r r }
% \toprule

% \multirow{2}{*}{Knowledge Retrieval} & \multicolumn{3}{c}{GPT-3.5-turbo} & \multicolumn{3}{c}{DeepSeek-Coder} & \multicolumn{3}{c}{Codestral} \\
% \cmidrule{2-10}
% Approach & FR & TU & MS & FR & TU & MS & FR & TU & MS \\
% \midrule
% Plain Text & 15.48\% & 1449.93 & \$0.00083 & 25.44\% & 1741.57 & \$0.00030 & 23.31\% & 1575.96 & \$0.00203 \\
% DS-KG (DSrepair) & 18.15\% & 1277.78 & \$0.00073 & 29.18\% & 1471.76 & \$0.00025 & 25.44\% & 1408.94 & \$0.00186 \\

% \bottomrule
% \end{tabular}
% \label{table: plain text}
% % }
% \vspace*{-0.3cm}
% \end{table*}

\begin{table}[h!]\scriptsize
\caption{RQ5: Knowledge retrieval comparison between plain text and KG. Retrieval from KG is better than from plain text. FR refers to Fix Rate, TU refers to Token Usage (input token usage + output token usage), and MS refers to Money Spent for LLM receiving the prompt and generating the response.}
\vspace{0mm}
% \resizebox{.48\textwidth}{!}{
\centering
\begin{tabular}{l l r r r}
\toprule
Model & Knowledge Retrieval & FR & TU & MS\\
\midrule

\multirow{2}{*}{GPT-3.5-turbo} & Plain Text & 14.77\% & 1432.42 & \$0.00082 \\

 & Knowledge Graph & \textbf{18.51\%} & \textbf{1262.14} & \textbf{\$0.00073} \\
\midrule
\multirow{2}{*}{GPT-4o-mini} & Plain Text & 22.60\% & 1827.59 & \$0.00046 \\
 & Knowledge Graph & \textbf{25.80\%} & \textbf{1584.74} & \textbf{\$0.00043} \\
 \midrule
\multirow{2}{*}{DeepSeek-Coder} & Plain Text & 25.44\% & 1723.77 & \$0.00030 \\
 & Knowledge Graph & \textbf{29.18\%} & \textbf{1453.96} & \textbf{\$0.00025} \\
 \midrule
\multirow{2}{*}{Codestral} & Plain Text & 22.78\% & 1636.46 & \$0.00209 \\
 & Knowledge Graph & \textbf{24.91\%} & \textbf{1407.15} & \textbf{\$0.00185} \\

% \multirow{2}{*}{GPT-3.5-turbo} & Plain Text & 15.48\% & 1449.93 & \$0.00083  \\
% & Knowledge Graph & \textbf{18.15\%} & \textbf{1277.78} & \textbf{\$0.00073} \\
% \midrule
% \multirow{2}{*}{DeepSeek-Coder} & Plain Text  &  25.44\% & 1741.57 & \$0.00030  \\
% & Knowledge Graph & \textbf{29.18\%} & \textbf{1471.76} & \textbf{\$0.00025} \\
% \midrule
% \multirow{2}{*}{Codestral} & Plain Text  &  23.31\% & 1575.96 & \$0.00203\\
% & Knowledge Graph  & \textbf{25.44\%} & \textbf{1408.94} & \textbf{\$0.00186} \\

\bottomrule
\end{tabular}
\label{table: plain text}
% }
\vspace*{-0.5cm}
\end{table}



\subsection{RQ6: Influence of API Richness}
\label{section: API richness}

To address RQ6, we assess how varying the richness of API knowledge impacts the performance of DSrepair.
In our DSrepair setup, we use only the full expressions of the invoked API to enrich the prompts.
Our queries also yield additional information about correct API usage, including explanations of functions, and details about parameters and returns.
To explore the potential benefits of this enriched API knowledge, we design experiments with different richness levels of API information: DSrepair+explanation, DSrepair+parameter\&return, and DSrepair+explanation+parameter\&return.
DSrepair+explanation incorporates explanations of the invoked API into the API knowledge.
DSrepair+parameter\&return adds information about the function's parameters and returns.
DSrepair+explanation+parameter\&return combines both types of information into the API knowledge. 

Table~\ref{table: richness of API knowledge} presents the performance results of these different levels of API knowledge richness.
The results are evaluated in terms of effectiveness, as measured by the Fix Rate (FR), and cost, as quantified by Token Usage (TU) and Money Spent (MS).
The data shows that DSrepair achieves the highest Fix Rate across all richness levels, with 18.51\% on GPT-3.5-turbo, 25.80\% on GPT-4o-mini, 29.18\% on DeepSeek-Coder, and 24.91\% on Codestral.
This suggests that the additional information may complicate the prompt without necessarily improving the effectiveness of the repair.

In terms of cost, DSrepair generally exhibits lower token usage and monetary cost compared to its enriched counterparts.
For example, on GPT-3.5-turbo, DSrepair uses 1262.14 tokens and incurs a cost of \$0.00073, whereas DSrepair+explanation+parameter\&return uses 1584.49 tokens and costs \$0.00089.
This pattern holds across the other models as well, indicating that increasing the complexity of the API knowledge may lead to higher money costs without a proportional gain in repair effectiveness.


% Table~\ref{table: richness of API knowledge} shows the performance results of different levels of API knowledge richness, including both effectiveness (FR(Fix Rate)), and cost (TU(Token Usage) and MS(Money Spent)).
% We can see that DSrepair outperforms all other prompts with richer API knowledge for using four LLMs, with the best FR (18.15\%, 29.18\%, and 25.44\%), the least TU (1277.78, 1471.76, and 1408.94), and the least MS (\$0.00073, \$0.00025, and \$0.00186).
% However, when using Codestral as LLM, prompts with richer API knowledge in prompt have better FR, yet with higher TU and MS.
% DSrepair+explanation achieves the best FR with 27.22\%.

\begin{tcolorbox}
\textbf{\underline{Answer to RQ6:}}
% Richer API knowledge cannot guarantee better performance in effectiveness, but it will definitely increase the token usage and money spent on LLMs. 
% DSrepair outperforms prompts with richer API knowledge in terms of effectiveness and cost.
Using full expressions of invoked API from the retrieval results in DSrepair performs the best in fixing bugs.
\end{tcolorbox}

\begin{table*}[h!]\scriptsize
\caption{RQ6: Influence of API knowledge richness on DSrepair. FR refers to Fix Rate, TU refers to Token Usage (input token usage + output token usage), and MS refers to Money Spent for LLM receiving the prompt and generating the response.}
\vspace{0mm}
\setlength{\tabcolsep}{4pt}
% \resizebox{.48\textwidth}{!}{
\centering
\begin{tabular}{l r r r r r r r r r r r r}
\toprule

\multirow{2}{*}{API Knowledge Richness} & \multicolumn{3}{c}{GPT-3.5-turbo} & \multicolumn{3}{c}{GPT-4o-mini} & \multicolumn{3}{c}{DeepSeek-Coder} & \multicolumn{3}{c}{Codestral} \\
\cmidrule{2-13}
 & FR & TU & MS & FR & TU & MS & FR & TU & MS & FR & TU & MS \\
\midrule

DSrepair+explanation & 15.84\% & 1279.50 & \$0.00074 & 22.78\% & 1597.57 & \$0.00043 & 27.22\% & 1503.78 & \$0.00026 & 24.38\% & 1410.30 & \$0.00187  \\
DSrepair+parameter\&return & 14.95\% & 1573.98 & \$0.00089 & 22.60\% & 1885.12 & \$0.00047 & 26.33\% & 1803.51 & \$0.00030 & 24.02\% & 1698.39 & \$0.00215  \\
DSrepair+explanation+parameter\&return & 15.30\% & 1584.49 & \$0.00089 & 23.49\% & 1901.94 & \$0.00047 & 26.87\% & 1816.90 & \$0.00030 & 24.02\% & 1712.37 & \$0.00217  \\
\midrule
\textbf{DSrepair} & \textbf{18.51\%} & \textbf{1262.14} & \textbf{\$0.00073} & \textbf{25.80\%} & \textbf{1584.74} & \textbf{\$0.00043} & \textbf{29.18\%} & \textbf{1453.96} & \textbf{\$0.00025} & \textbf{24.91\%} & \textbf{1407.15} & \textbf{\$0.00185}  \\

% DSrepair+explanation & 16.19\% & 1293.94 & \$0.00074 & 27.22\% & 1521.59 & \$0.00026 & 24.20\% & 1422.18 & \$0.00187 \\
% DSrepair+parameter\&return & 15.66\% & 1586.88 & \$0.00089 & 26.33\% & 1814.91 & \$0.00030 & 24.02\% & 1716.19 & \$0.00217 \\
% DSrepair+explanation+parameter\&return & 15.84\% & 1600.32 & \$0.00090 & 26.87\% & 1834.71 & \$0.00030 & 23.84\% & 1726.82 & \$0.00217 \\
% \midrule
% \textbf{DSrepair} & \textbf{18.15\%} & \textbf{1277.78} & \textbf{\$0.00073 }& \textbf{29.18\%} & \textbf{1471.76} & \textbf{\$0.00025} & \textbf{25.44\%} & \textbf{1408.94} & \textbf{\$0.00186} \\

\bottomrule
\end{tabular}
\label{table: richness of API knowledge}
% }
%\vspace*{-0.3cm}
\end{table*}

% \subsection{RQ6: Influence of Different Bug Knowledge for DSrepair?}

% we have ast-level bug knowledge enrichment,
% we need to compare with statement-level bug knowledge enrichment (SBFL).




\subsection{RQ7: Influence of LLM Non-determinism}
\label{section: RQ7}

To investigate RQ7, we explore the effect of non-determinism in LLMs on our experimental results.
As outlined in Section \ref{section: experimental design}, we conduct each experiment ten times to account for variability in LLM responses. 
From these iterations, we select the median performance result as our final data point for analysis.
To further understand how LLM non-determinism might influence our results, we calculate the mean and standard deviation of the results across the ten trials.

% As stated in Section\ref{section: experimental design}, we repeat our experiment ten times, and finally, we choose the one with the median overall performance as the final result.
% In order to figure out whether LLM's non-determinism affects our experiment result, we analyze the mean and standard deviation of the experiment results.

Table~\ref{table: non-determinism} shows the mean FR (Fix Rate) and the standard deviation of code repair results.
We observe that the standard deviations of DSrepair are not big, which indicates the stability of our experiment results.
Additionally, with the standard deviation, DSrepair still outperforms the \major{baselines} in its mean FR, with 101.80 $\pm$ 6.71, 142.90 $\pm$ 6.44, 163.67 $\pm$ 1.25, 137.80 $\pm$ 4.60 for using GPT-3.5-turbo, GPT-4o-mini, DeepSeek-Coder, and Codestral as LLM respectively.


\begin{table*}[h!]\scriptsize
\caption{RQ7: Mean and standard deviation of the ANF (Absolute Number of Fix), expressed as mean ANF $\pm$ standard deviation. The small standard deviation indicates the reliability of our results, and our experimental conclusions in RQ1 remain valid within this range.
}
\vspace{0mm}
% \resizebox{.48\textwidth}{!}{
\hspace*{-0.4cm}

% \setlength{\tabcolsep}{4pt}
\centering
\begin{tabular}{l r r r r}
\toprule
\multirow{1}{*}{Approach} & \multicolumn{1}{c}{GPT-3.5-turbo} & \multicolumn{1}{c}{GPT-4o-mini} & \multicolumn{1}{c}{DeepSeek-Coder} & \multicolumn{1}{c}{Codestral}\\
% \cmidrule{2-4}
 % &  & mean FR & \\
\midrule
Code-Search & 26.40 $\pm$ 2.46 & 102.5 $\pm$ 2.29 & 75.33 $\pm$ 3.09 & 104.3 $\pm$ 2.83 \\
Chat-Repair & 60.50 $\pm$ 3.75 & 122.0 $\pm$ 2.79 & 128.00 $\pm$ 0.82 & 104.8 $\pm$ 2.18 \\
Self-Debugging-S & 51.10 $\pm$ 4.44 & 117.4 $\pm$ 2.69 & 108.00 $\pm$ 2.16 & 81.70 $\pm$ 6.39 \\
Self-Debugging-E & 68.60 $\pm$ 6.18 & 124.2 $\pm$ 6.16 & 102.67 $\pm$ 3.77 & 94.60 $\pm$ 5.70 \\
Self-Repair & 71.20 $\pm$ 4.58 & 125.1 $\pm$ 3.59 & 134.33 $\pm$ 3.09 & 91.50 $\pm$ 8.42 \\
\midrule
\textbf{DSrepair} & \textbf{101.80 $\pm$ 6.71} & \textbf{142.90 $\pm$ 6.44} & \textbf{163.67 $\pm$ 1.25} & \textbf{137.80 $\pm$ 4.60} \\

\bottomrule
\end{tabular}
\label{table: non-determinism}
% }
%\vspace*{-0.5cm}
\end{table*}

\begin{tcolorbox}
\textbf{\underline{Answer to RQ7:}}
Despite the randomness of LLMs, DSrepair consistently outperforms the baselines with greater stability across multiple trials. It achieves mean Fix Rates of 101.80 $\pm$ 6.71, 142.90 $\pm$ 6.44, 163.67 $\pm$ 1.25, 137.80 $\pm$ 4.60 across GPT-3.5-turbo, GPT-4o-mini, DeepSeek-Coder, and Codestral respectively.
\end{tcolorbox} 




\section{Discussion}
\label{section: discussion}


In this section, we discuss the threats to validity, the limitations, and the generalizability of our research.

% \subsection{The Importance of Knowledge for Code Generation}

% In the realm of automated code generation, the significance of knowledge cannot be overstated.
% Knowledge in this domain can be broadly categorized into local and global knowledge, each playing a crucial role in generating accurate, efficient, and relevant code.

\subsection{Threats to Validity}

The threats to \textit{internal} validity mainly lie in the implementation of our prompt design.
To reduce this threat, we design DSrepair with the idea of `Structuring Prompts', adapted from a handy template for structuring prompts, called CO-STAR framework ~\cite{promptcompetition}.
Considering key aspects that influence the effectiveness and relevance of an LLM’s response, DSrepair can lead LLMs to generate more optimal responses for code purposes.
In addition, we design research questions, such as RQ4 and RQ6, to study the influence of the different prompts on our final performance.

The threats to \textit{external} validity mainly lie in the datasets, and LLMs used in our study.
To reduce the threat regarding datasets, we carefully choose to use DS-1000 as our experiment dataset, 
which is the state-of-the-art benchmark tailored to address data leakage concerns with realistic and diverse data science problems,
with testing methods checking both execution semantics and surface-form constraints~\cite{lai2023ds}.
To reduce the threat regarding LLMs, we use four widely studied LLMs to mitigate the potential bias that certain LLMs can bring to the experiment results.
In addition, to mitigate the inherent randomness of LLMs, we experiment ten times for each \major{approach} and choose one with the median overall performance as our final result, which could further mitigate the non-determinism of LLMs. 
Moreover, we exclusively design RQ7 to study whether the non-determinism of LLMs will affect our experiment findings.
% To reduce the third threat, we compare DSrepair with five state-of-art LLM-based code repair me thods. 
% Our comparison with me thods for detecting general software logic bugs is NOT meant to imply that DSrepair outperforms existing me thods in all domains. 
% Rather, we show that me thods that are not specifically designed for data science struggle with addressing data science bugs.
% By addressing the unique needs of data science code, DSrepair can significantly improve repair outcomes in the context of data science.
% \major{
% To reduce the third threat, we compare DSrepair with five state-of-the-art LLM-based code repair me thods.
% }

\subsection{Limitation}

The effectiveness of DSrepair depends largely on the quality and completeness of the knowledge it provides.
Our approach demonstrates capability in addressing runtime errors by eliminating the initial error.
However, this repair process can sometimes introduce or trigger new errors.
This phenomenon is particularly evident when the repaired code successfully executes but subsequently results in assertion errors.
The reliance on high-quality test cases in the problem description is crucial;
in their absence, DSrepair may guide LLMs to generate code that closely mirrors the incorrect code.
This occurs because the LLMs are provided with API knowledge that can inadvertently reinforce the use of incorrect or irrelevant APIs present in the original code.
Despite this, there are instances where, upon receiving API knowledge, the LLMs deviate from the incorrect APIs, opting instead for alternative solutions, such as using different APIs or defining new functions.

Maintaining the DS-KG presents significant challenges.
Our DS-KG only reflects the correct knowledge of APIs based on a specific version.
The rapid pace at which online API documentation is updated complicates the task of ensuring the DS-KG remains up-to-date.
Consequently, keeping the DS-KG up-to-date demands substantial effort and resources.
This maintenance burden is a critical consideration, as outdated or incomplete knowledge can adversely affect the accuracy and reliability of the repairs generated by DSrepair.
With the assistance of API document's release notes, we could manage the updating of DS-KG by leveraging library development logs to automate the process.
These logs often document changes and updates made to API libraries, allowing us to efficiently identify and integrate the necessary modifications into the KG. 

Another notable limitation of DSrepair is the time cost associated with knowledge retrieval.
When compared to plain text searching, retrieval using the DS-KG incurs a significant time overhead, averaging 51.49\% more time (approximately 0.06 seconds) per task.
While this increase in retrieval time may seem marginal, it can accumulate and impact the overall efficiency of the repair process, particularly in scenarios requiring rapid iteration and testing.

\subsection{Generalizability of DSrepair}

In this paper, DSrepair is specifically designed to enhance the repair of data science code.
Nevertheless, DSrepair's underlying methodology—leveraging knowledge-enhanced retrieval and structured bug information—can be generalized to broader coding tasks. The key innovation of DSrepair lies in its use of knowledge graph-based Retrieval-Augmented Generation (RAG), which is not inherently limited to data science APIs. By replacing the domain-specific DS-KG with a knowledge graph covering general-purpose programming languages and software libraries, DSrepair could be adapted to repair a wide range of buggy code across different domains, as well as to improve other coding tasks other than repair, such as bug localization and code generation. 

DSrepair also has the potential to be extended to support project-level code generation and repair, where understanding dependencies across multiple files and context knowledge with the same project are crucial. 
By constructing a project-specific KG that encodes function definitions, module dependencies, and code architecture, DSrepair can enhance code generation and repair in large-scale software development.



% \major{
% DSrepair is not only effective in repairing data science code but also has the potential in general code generation and repair tasks.
% By leveraging graph-based RAG, DSrepair could capture complex relationships between different entities, allowing it to provide ground-truth coding knowledge and adapt to various coding scenarios.
% In particular, we can extend DSrepair to project-level code generation, where understanding dependencies across multiple files and context knowledge are crucial. 
% By constructing a project-specific KG that encodes function definitions, module dependencies, and code architecture, DSrepair can enhance code generation and repair in large-scale software development.
% }

% Balancing the precision and comprehensiveness of knowledge retrieval with the need for efficiency remains an ongoing challenge in the development and optimization of DSrepair.

% Although DSrepair achieves promising performance, it is still necessary to discuss the limitations of our work.
% To some extent, we find our approach is restricted from the knowledge provided, i.e., bug knowledge and API knowledge.


% The maintenance of DS-KG is also challenging.
% The update of online API documents is fast.
% Our built DS-KG only provides the correct knowledge API at a certain version.
% Keeping the DS-KG up-to-date is intensive.
% The time cost of knowledge retrieval.

% retrieval using KG spends 51.49\% more time (0.06s) on average than searching from plain text.

% \shuyin{TODO: wrong API, time cost}

% \subsection{Global Knowledge}
% % Global Knowledge encompasses broad areas such as the general grammar and syntax of the programming language in use, as well as the specifications for API usage.
% Global Knowledge is essential for ensuring that the generated code is not only syntactically correct but also semantically meaningful within the larger programming ecosystem.
% % Global knowledge contains the following aspects:
% 1) Grammar of Programming Languages:
% A deep understanding of the programming languages' grammar ensures that the generated code adheres to correct syntax and semantics. 
% This includes proper use of keywords, data types, control structures, and other language constructs. 
% Such adherence is fundamental to producing functional and error-free code.
% 2) API Utilization: 
% Modern software development heavily relies on APIs for leveraging existing functionalities and services.

% This allows the code generator to effectively integrate API calls, ensuring that they are used correctly and optimally.
% 3) Practices and Standards: 
% The global knowledge could also include industry practices and coding standards.
% By embedding these principles into the code generation process, the resulting code not only functions correctly but also adheres to high standards of quality, security, and performance.

% \subsection{Local Knowledge}
% Local Knowledge pertains to the immediate environment in which the code is being generated and evaluated.
% This includes the natural language description of the code problem or task, existing code snippets, project structure, and the test execution result.
% % Feed LLM with local knowledge is paramount for several reasons:
% % 1) Alignment with task requirements: 
% The natural language description of a coding task provides the necessary directives and specifications that the generated code must adhere to.
% This ensures that the output code logic flow aligns with the intended functionality and fulfills the user's requirements.
% % 2) Consistency with existing code:
% Existing code snippets and project structures offer a blueprint that the new code must follow to maintain consistency.
% This includes adhering to coding styles, conventions, and patterns already established in the project
% Such consistency is vital for code readability, maintainability, and integration.
% % 3) Correctness guarantee with test cases: 
% The code must not only look correct but also withstand the test of test cases. 
% Based on the existing test case execution results, the code can be refined to enhance its robustness.

% Combining local and global knowledge is crucial for holistic and effective code generation with LLMs.
% Our study implemented two key sub-aspects: API knowledge retrieval as a form of global knowledge and bug information enrichment representing local knowledge.
% These integrations enhance the LLM's capability to generate accurate and context-aware code.
% Future work will explore expanding the scope of global knowledge to include specific programming language grammar and a broader range of API documentation, while local knowledge could be enriched through static analysis from code file-level context and code project-level architecture.
% This comprehensive approach aims to further refine the accuracy and adaptability of code generation by LLMs, making it more robust and trustworthy.

% The combination of local and global knowledge is vital for holistic and effective code generation with LLMs.
% In our study, we only implement two sub-aspects, API knowledge retrieval, and bug information enrichment, from global knowledge and local knowledge respectively.
% While local context ensures that the generated code is relevant and aligned with the specific project requirements, global context guarantees that the code is syntactically correct and adheres to broader programming norms and standards.
% For instance, consider a scenario where a developer needs to generate code for a specific functionality in a web application.
% The local context would provide details about the existing codebase, the project structure, and the specific requirements of the functionality.
% Meanwhile, the global context would offer guidelines on how to utilize the web framework's API, conform to the programming language's syntax, and adhere to security best practices.
% By integrating both contexts, 
% the code generator can produce code that seamlessly fits into the existing project, utilizes the appropriate APIs correctly, and follows best practices, resulting in a robust and maintainable solution.



\section{Related Work}
\label{section: related work}

\subsection{Code Repair} 

The goal of automated program repair is to automatically identify and fix bugs or defects in the software.
Leveraging LLMs, such as BERT~\cite{zhang2024appt}, CodeBERT~\cite{le2023invalidator}, Codex~\cite{fan2023automated,jin2023inferfix,wu2023effective}, and GPT-series~\cite{lajko2022towards,xia2023keep,xia2023conversational,sobania2023analysis}, for code repair can achieve promising performance in generating patches for various kinds of bugs and defects.
These models are adept at grasping the core meaning and relationships within code, resulting in the generation of precise and functional fixes without the need for compilation.
Using LLMs for fixing code speeds up the identification and resolution of bugs, freeing software developers to tackle more intricate issues.
This contributes to improved software reliability and upkeep.
ChatGPT, in particular, stands out among LLMs because of its built-in interactive nature, which fosters an ongoing loop of feedback, producing patches that are more polished and appropriate to the context~\cite{xia2023keep,xia2023conversational}.
% The conversational dynamics of ChatGPT, coupled with rigorous comparisons across diverse baselines, underscore its superior adaptability and efficiency.

% Within the domain of LLMs applied to code-related tasks, research centered on code generation distinctly dominates the academic lands.
% LLMs trained for text generation have demonstrated the ability to complete programming tasks~\cite{brown2020language}.
% By pre-training on large-scale text data, these models learn rich linguistic knowledge and semantic representations that enable them to understand the meaning and structure of natural language text.
% Unlike traditional program synthesis~\cite{david2017program, gulwani2010dimensions}, LLMs can be conditioned on natural language (e.g., code annotations and docstrings), and generate programming language text.

% Given the programming problem description provided by the code, LLMs can automate code generation by converting the natural language into code written in a specific programming language~\cite{ouyang2023llm}.
% The GPT series, especially GPT-4, has become a central area of interest, with a growing number of studies applying them in code generation~\cite{dong2023self,achiam2023gpt,liu2024your}.
% % Dong et al.~\cite{dong2023self} introduce a novel self-collaboration framework for code generation that leverages LLMs, ChatGPT in particular, to mimic human collaborative teamwork in software development.
% % Through orchestrating multiple LLMs acting as distinct ``experts" in analysis, coding, and testing roles, their framework can tackle complex coding tasks efficiently without human intervention.
% Within OpenAI's GPT-4 technical report~\cite{achiam2023gpt}, general code generation has already reached an impressive performance with the 0-shot prompt in Python code generation.
% Additionally, Google DeepMind proposed Alphacode~\cite{li2022competition} in competitive-level code generation, which is boosted by their close-sourced pre-trained LLMs, and reached a promising rank in real-world code competition.
% The future direction of code generation appears to focus on seamlessly incorporating LLMs into existing SE tools and pipelines~\cite{fan2023large}.
% The development of frameworks such as EvalPlus~\cite{liu2024your} suggests a movement towards improving the evaluation and precision of code produced by LLMs.
% This shift could lead to a new phase where human developers and LLMs work together to develop software solutions.

\subsection{Prompt Engineering}

Prompt designing is an increasingly important skill set needed to leverage effectively with LLMs \cite{white2023prompt}, such as ChatGPT.
% The response returned by LLMs is directly related to the quality of the given prompt.
Similar to software design \cite{gamma1995design}, the design of prompt aims at offering reusable solutions to specific problems, 
by providing a codified approach to customizing the output and interactions of LLMs.
Abukhalaf et al.~\cite{abukhalaf2023codex} conduct an empirical study on Object Constraint Language based constraint generation, by comparing the Codex generated constraints and humane-written constraints.
Xia et al.~\cite{xia2023conversational} specifically examined prompts for automatic code repair.
More specifically, White et al. \cite{white2023chatgpt} focus on combatting mistakes and improving generated code quality by designing prompt patterns.
Borji et al.~\cite{borji2023categorical} examine the quality of generated answers and code from LLMs, and conclude the existing failures from the experiment. 
Our research work draws inspiration from these explorations and prompts that could be used to generate code candidates with better quality and fewer errors.
% The importance of prompt design is so far widely studied, which has mainly examined the effect of prompt words and way of expressions on LLMs \cite{reynolds2021prompt, wei2022chain, zhou2022large}. 
% For example, there are existing works to investigate how different prompts affect image generation \cite{liu2022design} and visualization \cite{maddigan2023chat2vis}. 
% White et al. \cite{white2023prompt}, present a framework for documenting and applying a catalog of prompt patterns for LLMs, which provides users with an efficient and effective guide to interacting with LLMs.

\subsection{Retrieval-Augmented Generation}


% The RAG paradigm typically first retrieves the most relevant
% information using similarity measures such as BM25, dense embeddings such as SimCSE~\cite{gao2021simcse}, or Dense Passage Retrieval~\cite{karpukhin2020dense}.
% The retrieved information is then concatenated with the
% original input to guide the generation of LLM.
% Although initially explored for open-domain question answering, RAG has recently been
% adapted for code generation~\cite{lu2022reacc, tang2023domain, zhang2023repocoder}.

RAG aims to address the limitations of generative models, including issues related to outdated knowledge, a deficiency in long-tail knowledge~\cite{mallen2022not}, and the potential for private training data leakage~\cite{carlini2021extracting}.
Early research in code generation concentrated on code-to-code retrieval using dual encoder models, with the retrieved outputs subsequently inputted into autoregressive language models~\cite{lu2022reacc}. 
RepoCoder~\cite{zhang2023repocoder} enhances retrieval processes by employing iterative incremental generations~\cite{chu2011contextual}.
KNM~\cite{tang2023domain} leverages in-domain code databases and applies Bayesian inference to finalize the generated code.
RAG also can be used to build prompts for transformer-based generative models with retrieved information, including similar examples~\cite{parvez2021retrieval, li2023acecoder}, relevant API details~\cite{zan2022language, zan2023private}, documentations~\cite{zhou2022docprompting}, and imports~\cite{liu2023codegen4libs}.



\section{Conclusion}
\label{section: conclusion}
We propose DSrepair, a novel knowledge-enhanced approach for data science code repair.
We perform experiments with four LLMs and five
baselines in data science code repair and find that DSrepair significantly outperforms all the baselines in repairing data science code.
% We point out that only using LLMs is not enough in code generation.
% We point out that only relying on LLMs is not enough to generate satisfactory code.
By integrating API knowledge retrieval and bug information enrichment, we can guarantee better performance in code repair, and gain people's trust in using LLMs for coding.
In future work, 
we also plan to explore a multi-agent framework with interactive feedback to enhance the performance of DSrepair while focusing on optimizing feedback steps and resource use to ensure scalability, cost efficiency and robust data science code repair.

\section{Acknowledgement}
This work was supported by the UKRI Centre for Doctoral Training in Safe and Trusted Artificial Intelligence (EP/S023356/1), the NSFC (62192732), and the National Natural Science Foundation of China (62402482).


% }

\bibliographystyle{IEEEtran}
\bibliography{reference}


\end{document}
