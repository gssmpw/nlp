\section{Related Work}
\label{section: related work}

\subsection{Code Repair} 

The goal of automated program repair is to automatically identify and fix bugs or defects in the software.
Leveraging LLMs, such as BERT~\cite{zhang2024appt}, CodeBERT~\cite{le2023invalidator}, Codex~\cite{fan2023automated,jin2023inferfix,wu2023effective}, and GPT-series~\cite{lajko2022towards,xia2023keep,xia2023conversational,sobania2023analysis}, for code repair can achieve promising performance in generating patches for various kinds of bugs and defects.
These models are adept at grasping the core meaning and relationships within code, resulting in the generation of precise and functional fixes without the need for compilation.
Using LLMs for fixing code speeds up the identification and resolution of bugs, freeing software developers to tackle more intricate issues.
This contributes to improved software reliability and upkeep.
ChatGPT, in particular, stands out among LLMs because of its built-in interactive nature, which fosters an ongoing loop of feedback, producing patches that are more polished and appropriate to the context~\cite{xia2023keep,xia2023conversational}.
% The conversational dynamics of ChatGPT, coupled with rigorous comparisons across diverse baselines, underscore its superior adaptability and efficiency.

% Within the domain of LLMs applied to code-related tasks, research centered on code generation distinctly dominates the academic lands.
% LLMs trained for text generation have demonstrated the ability to complete programming tasks~\cite{brown2020language}.
% By pre-training on large-scale text data, these models learn rich linguistic knowledge and semantic representations that enable them to understand the meaning and structure of natural language text.
% Unlike traditional program synthesis~\cite{david2017program, gulwani2010dimensions}, LLMs can be conditioned on natural language (e.g., code annotations and docstrings), and generate programming language text.

% Given the programming problem description provided by the code, LLMs can automate code generation by converting the natural language into code written in a specific programming language~\cite{ouyang2023llm}.
% The GPT series, especially GPT-4, has become a central area of interest, with a growing number of studies applying them in code generation~\cite{dong2023self,achiam2023gpt,liu2024your}.
% % Dong et al.~\cite{dong2023self} introduce a novel self-collaboration framework for code generation that leverages LLMs, ChatGPT in particular, to mimic human collaborative teamwork in software development.
% % Through orchestrating multiple LLMs acting as distinct ``experts" in analysis, coding, and testing roles, their framework can tackle complex coding tasks efficiently without human intervention.
% Within OpenAI's GPT-4 technical report~\cite{achiam2023gpt}, general code generation has already reached an impressive performance with the 0-shot prompt in Python code generation.
% Additionally, Google DeepMind proposed Alphacode~\cite{li2022competition} in competitive-level code generation, which is boosted by their close-sourced pre-trained LLMs, and reached a promising rank in real-world code competition.
% The future direction of code generation appears to focus on seamlessly incorporating LLMs into existing SE tools and pipelines~\cite{fan2023large}.
% The development of frameworks such as EvalPlus~\cite{liu2024your} suggests a movement towards improving the evaluation and precision of code produced by LLMs.
% This shift could lead to a new phase where human developers and LLMs work together to develop software solutions.

\subsection{Prompt Engineering}

Prompt designing is an increasingly important skill set needed to leverage effectively with LLMs \cite{white2023prompt}, such as ChatGPT.
% The response returned by LLMs is directly related to the quality of the given prompt.
Similar to software design \cite{gamma1995design}, the design of prompt aims at offering reusable solutions to specific problems, 
by providing a codified approach to customizing the output and interactions of LLMs.
Abukhalaf et al.~\cite{abukhalaf2023codex} conduct an empirical study on Object Constraint Language based constraint generation, by comparing the Codex generated constraints and humane-written constraints.
Xia et al.~\cite{xia2023conversational} specifically examined prompts for automatic code repair.
More specifically, White et al. \cite{white2023chatgpt} focus on combatting mistakes and improving generated code quality by designing prompt patterns.
Borji et al.~\cite{borji2023categorical} examine the quality of generated answers and code from LLMs, and conclude the existing failures from the experiment. 
Our research work draws inspiration from these explorations and prompts that could be used to generate code candidates with better quality and fewer errors.
% The importance of prompt design is so far widely studied, which has mainly examined the effect of prompt words and way of expressions on LLMs \cite{reynolds2021prompt, wei2022chain, zhou2022large}. 
% For example, there are existing works to investigate how different prompts affect image generation \cite{liu2022design} and visualization \cite{maddigan2023chat2vis}. 
% White et al. \cite{white2023prompt}, present a framework for documenting and applying a catalog of prompt patterns for LLMs, which provides users with an efficient and effective guide to interacting with LLMs.

\subsection{Retrieval-Augmented Generation}


% The RAG paradigm typically first retrieves the most relevant
% information using similarity measures such as BM25, dense embeddings such as SimCSE~\cite{gao2021simcse}, or Dense Passage Retrieval~\cite{karpukhin2020dense}.
% The retrieved information is then concatenated with the
% original input to guide the generation of LLM.
% Although initially explored for open-domain question answering, RAG has recently been
% adapted for code generation~\cite{lu2022reacc, tang2023domain, zhang2023repocoder}.

RAG aims to address the limitations of generative models, including issues related to outdated knowledge, a deficiency in long-tail knowledge~\cite{mallen2022not}, and the potential for private training data leakage~\cite{carlini2021extracting}.
Early research in code generation concentrated on code-to-code retrieval using dual encoder models, with the retrieved outputs subsequently inputted into autoregressive language models~\cite{lu2022reacc}. 
RepoCoder~\cite{zhang2023repocoder} enhances retrieval processes by employing iterative incremental generations~\cite{chu2011contextual}.
KNM~\cite{tang2023domain} leverages in-domain code databases and applies Bayesian inference to finalize the generated code.
RAG also can be used to build prompts for transformer-based generative models with retrieved information, including similar examples~\cite{parvez2021retrieval, li2023acecoder}, relevant API details~\cite{zan2022language, zan2023private}, documentations~\cite{zhou2022docprompting}, and imports~\cite{liu2023codegen4libs}.