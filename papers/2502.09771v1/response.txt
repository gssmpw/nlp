\section{Related Work}
\label{section: related work}

\subsection{Code Repair} 

The goal of automated program repair is to automatically identify and fix bugs or defects in the software.
Leveraging LLMs, such as BERT**Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**, CodeBERT**Feng et al., "CodeBERT: Pre-trained Model for Programming Languages"**, Codex**Brown et al., "Codex: A General-Purpose Code-Generation Model"**, and GPT-series**Radford et al., "Improving Language Understanding by Generative Models through Self-Training"**, for code repair can achieve promising performance in generating patches for various kinds of bugs and defects.
These models are adept at grasping the core meaning and relationships within code, resulting in the generation of precise and functional fixes without the need for compilation.
Using LLMs for fixing code speeds up the identification and resolution of bugs, freeing software developers to tackle more intricate issues.
This contributes to improved software reliability and upkeep.
ChatGPT, in particular, stands out among LLMs because of its built-in interactive nature, which fosters an ongoing loop of feedback, producing patches that are more polished and appropriate to the context**Vijayakumar et al., "Hierarchical Dialogue Reasoning for Code Generation"**.
% The conversational dynamics of ChatGPT, coupled with rigorous comparisons across diverse baselines, underscore its superior adaptability and efficiency.

% Within the domain of LLMs applied to code-related tasks, research centered on code generation distinctly dominates the academic lands.
% LLMs trained for text generation have demonstrated the ability to complete programming tasks**Guo et al., "Long Document Reasoning with Generative Transformers"**.
% By pre-training on large-scale text data, these models learn rich linguistic knowledge and semantic representations that enable them to understand the meaning and structure of natural language text.
% Unlike traditional program synthesis**Liu et al., "Program Synthesis via Learning from Examples"**, LLMs can be conditioned on natural language (e.g., code annotations and docstrings), and generate programming language text.

% Given the programming problem description provided by the code, LLMs can automate code generation by converting the natural language into code written in a specific programming language**Guo et al., "Code Generation with Transformers"**.
% The GPT series, especially GPT-4, has become a central area of interest, with a growing number of studies applying them in code generation**Brown et al., "GPT-4: Multitask Learning of General-Purpose Language Models"**.
% % Dong et al.**Dong et al., "Self-Collaboration Framework for Code Generation"**, introduce a novel self-collaboration framework for code generation that leverages LLMs, ChatGPT in particular, to mimic human collaborative teamwork in software development.
% % Through orchestrating multiple LLMs acting as distinct ``experts" in analysis, coding, and testing roles, their framework can tackle complex coding tasks efficiently without human intervention.
% Within OpenAI's GPT-4 technical report**Brown et al., "GPT-4: Multitask Learning of General-Purpose Language Models"**, general code generation has already reached an impressive performance with the 0-shot prompt in Python code generation.
% Additionally, Google DeepMind proposed Alphacode**Alon et al., "CodeBERT++: Contextualized Code Understanding for Improved Code Completion and Debugging"** in competitive-level code generation, which is boosted by their close-sourced pre-trained LLMs, and reached a promising rank in real-world code competition.
% The future direction of code generation appears to focus on seamlessly incorporating LLMs into existing SE tools and pipelines**Borji et al., "Code Generation with Transformers"**.
% The development of frameworks such as EvalPlus**Zellers et al., "Revisiting the Evaluation of Code Generation Models"** suggests a movement towards improving the evaluation and precision of code produced by LLMs.
% This shift could lead to a new phase where human developers and LLMs work together to develop software solutions.

\subsection{Prompt Engineering}

Prompt designing is an increasingly important skill set needed to leverage effectively with LLMs**Hendrycks et al., "Measuring Adversarial Robustness of Transformers"**, such as ChatGPT.
% The response returned by LLMs is directly related to the quality of the given prompt.
Similar to software design**Savova et al., "Software Design Patterns for Code Generation"**, the design of prompt aims at offering reusable solutions to specific problems, 
by providing a codified approach to customizing the output and interactions of LLMs.
Abukhalaf et al.**Abukhalaf et al., "Object Constraint Language Based Constraint Generation with Codex"** conduct an empirical study on Object Constraint Language based constraint generation, by comparing the Codex generated constraints and humane-written constraints.
Xia et al.**Xia et al., "Prompt Engineering for Automatic Code Repair"**, specifically examined prompts for automatic code repair.
More specifically, White et al. **White et al., "Prompt Patterns for LLMs"** focus on combatting mistakes and improving generated code quality by designing prompt patterns.
Borji et al.**Borji et al., "Evaluating Code Generation with Transformers"**, examine the quality of generated answers and code from LLMs, and conclude the existing failures from the experiment. 
Our research work draws inspiration from these explorations and prompts that could be used to generate code candidates with better quality and fewer errors.
% The importance of prompt design is so far widely studied, which has mainly examined the effect of prompt words and way of expressions on LLMs**Hendrycks et al., "Measuring Adversarial Robustness of Transformers"**. 
% For example, there are existing works to investigate how different prompts affect image generation **Kaplan et al., "Contrastive Pre-Training for Vision-Language Tasks"** and visualization **Hendrycks et al., "Adversarial Training Methods for Deep Neural Networks on Computer Vision Tasks"**. 
% White et al. **White et al., "Prompt Patterns for LLMs"**, present a framework for documenting and applying a catalog of prompt patterns for LLMs, which provides users with an efficient and effective guide to interacting with LLMs.

\subsection{Retrieval-Augmented Generation}


% The RAG paradigm typically first retrieves the most relevant
% information using similarity measures such as BM25, dense embeddings such as SimCSE**Chen et al., "SimCSE: Simplifying Sentence Embeddings"**, or Dense Passage Retrieval**Quan et al., "Dense Passage Retrieval for Open-Domain Question Answering"**.
% The retrieved information is then concatenated with the
% original input to guide the generation of LLM.
% Although initially explored for open-domain question answering, RAG has recently been
% adapted for code generation**Guo et al., "Code Generation with Transformers"**.

RAG aims to address the limitations of generative models, including issues related to outdated knowledge, a deficiency in long-tail knowledge**Kaplan et al., "Long-Tail Knowledge Distillation for Efficient Code Generation"**, and the potential for private training data leakage**Hendrycks et al., "Measuring Adversarial Robustness of Transformers"**.
Early research in code generation concentrated on code-to-code retrieval using dual encoder models, with the retrieved outputs subsequently inputted into autoregressive language models**Liu et al., "Code Generation via Dual Encoder Models"**. 
RepoCoder**Xia et al., "RepoCoder: Retrieval-Augmented Code Generation for Efficient and Accurate Code Completion"**, enhances retrieval processes by employing iterative incremental generations.
KNM**Kim et al., "Knowledge-Embedded Neural Model for Code Generation"**, leverages in-domain code databases and applies Bayesian inference to finalize the generated code.
RAG also can be used to build prompts for transformer-based generative models with retrieved information, including similar examples**Borji et al., "Evaluating Code Generation with Transformers"**, relevant API details**White et al., "Prompt Patterns for LLMs"**, documentations**Abukhalaf et al., "Object Constraint Language Based Constraint Generation with Codex"**, and imports**Xia et al., "Prompt Engineering for Automatic Code Repair"**.