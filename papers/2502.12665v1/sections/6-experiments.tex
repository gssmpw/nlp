\section{Experiments}

% In this section, we first introduce the experimental setup, including benchmarks, comparisons, and implementation details.
% Then we present the main results on downstream tasks in terms of model performance, attention sparsity ratios, and auxiliary overhead.
% Besides, we conduct ablation study to validate our proposed WRoPE and query-aware vector quantization methods.
% Finally, we demonstrate end-to-end inference speedup to highlight {\name}'s potential in improving the efficiency for long context LLM serving.

\subsection{Experimental Setup}

\noindent \textbf{Tasks.} 
We utilize RULER~\citep{ruler} as our benchmark for downstream tasks evaluation.
This synthetic benchmark 
% supports configurable context lengths and 
contains thirteen subtasks organized into four categories: information retrieval, multi-hop tracing, information aggregation, and question answering. 
It evaluates long-context comprehension and reasoning capabilities of LLMs, while effectively revealing the accuracy drop caused by KV cache reduction methods.

\noindent \textbf{Models.}
We conduct our main experiments on  
% state-of-the-art LLMs including 
Llama-3.1-8B-Instruct~\citep{llama-3} and MegaBeam-Mistral-7B-512k~\citep{megabeam-mistral}.
These models feature long-context processing capabilities with context windows of up to 128K and 512K tokens, respectively. 
As for the ablation study and end-to-end throughput evaluation, we apply the Llama-3.1-8B-Instruct model.

\noindent \textbf{Methods.}
For main experiments, we compare the proposed {\name} with the following four KV cache reduction methods, along with the full attention baseline: H2O~\citep{h2o}, SnapKV~\citep{snapkv}, Quest~\citep{quest}, MagicPIG~\citep{magicpig}.
For a fair comparison, the sparsity ratios of all KV cache reduction methods are controlled around 0.06.
% , which means approximately 6\% of KV cache is accessed at each inference (see Appendix~\ref{appendix:baselines} 
Detailed discussions are presented in Appendix~\ref{appendix:baselines}.

For ablation studies, we evaluate the following configurations on Llama-3.1-8B-Instruct:
\begin{itemize}[nosep]
    % \item \textbf{Baseline}: Substitutes both WRoPE and query-aware vector quantization with conventional RoPE and vector quantization.
    \item \textbf{Baseline}: It utilizes standard RoPE and conventional vector quantization.
    %\item \textbf{Baseline w/ WRoPE}: 
    \item \textbf{WRoPE}: 
    It utilizes WRoPE and conventional vector quantization.
    %The baseline configuration with standard RoPE replaced by WRoPE;
  %  \item \textbf{Baseline w/ Query-Aware Vector Quantization}: 
    \item \textbf{QAVQ}:
    It utilizes standard RoPE and query-aware vector quantization.
    %The baseline configuration with conventional vector quantization replaced by query-aware vector quantization;
    \item \textbf{{\name}}: The proposed method with WRoPE and query-aware vector quantization.
\end{itemize}

\noindent \textbf{Implementation Details.}
The hyper-parameters \(w\) and \(b\) of WRoPE are set to \(64\) and \(2048\), respectively.
The codebooks of query-aware vector quantization, with a size of 4096 each, are constructed from a set of sample inputs consisting of approximately 64K tokens of text randomly sampled from FineWeb~\citep{fineweb} and 16K tokens of randomly generated uuid strings.
% The prompts of RULER benchmark used in our experiments are identical to those used in MagicPIG~\citep{magicpig}.
The end-to-end speedup experiments are conducted on a server equipped with an NVIDIA H800 GPU with 80GB memory, and an Intel Xeon Platinum 8469C CPU.

\subsection{Main Results on Downstream Tasks}

\begin{table*}[htb]
    \centering
    \resizebox{0.8\textwidth}{!}{
        \begin{tabular}{l|cc|cccc|c}
            \toprule   
    		\multirow{2}*{\textbf{Models}} &
            \multirow{2}*{\textbf{Sparsity$\downarrow$}} & 
            \multirow{2}*{\textbf{Aux Mem$\downarrow$}}& 
            \multicolumn{5}{c}{\textbf{Accuracy$\uparrow$}} \\
                
            \cmidrule{4-8}
        
            ~ & ~ & ~ &
            \textbf{16K} & 
            \textbf{32K} & 
            \textbf{64K}  & 
            \textbf{96K}  & 
            \textbf{Average} \\
            
            \midrule

            % 94.37692	91.86692	85.92538	83.11000	88.81981
            \textit{Llama-3.1-8B-Instruct} &
            1.000 &
            0.000 &
            94.4 &
            91.9 &
            85.9 &
            83.1 &
            88.8 \\

            % 27.59231	30.62308	24.86385	25.02846	27.02692
            H2O &
            0.060 &
            0.008 &
            27.6 &
            30.6 &
            24.9 &
            25.0 &
            27.0 \\

            % 72.73615	75.06154	72.16923	70.68231	72.66231
            SnapKV &
            0.060 &
            0.008 &
            72.7 &
            75.1 &
            72.2 &
            70.7 &
            72.7 \\

            % 84.30000	84.00000	80.10000	74.40000	80.70000
            Quest &
            0.060 &
            0.031 &
            84.3 &
            84.0 &
            80.0 &
            74.4 &
            80.7 \\

            % 92.28692	87.58231	83.91308	79.08231	85.71615
            MagicPIG &
            0.068 &
            2.344 &
            \textbf{92.3} &
            87.6 &
            83.9 &
            79.1 &
            85.7 \\

            % 92.18462	90.43615	84.28462	79.57462	86.62000
            {\name} &
            0.060 &
            0.008 &
            92.2 &
            \textbf{90.4} &
            \textbf{84.3} &
            \textbf{79.6} &
            \textbf{86.6} \\

            \midrule

            % 91.81538	88.17462	83.34385	83.42077	86.68865
            \textit{MegaBeam-Mistral-7B-512K} &
            1.000 &
            0.000 &
            91.8 &
            88.2 &
            83.3 &
            83.4 &
            86.7 \\

            % 22.49000	23.43308	20.74385	22.63615	22.32577
            H2O & 
            0.060 & 
            0.008 &
            22.5 &
            23.4 &
            20.7 &
            22.6 &
            22.3 \\

            % 68.34846	68.46385	68.45923	65.19231	67.61596
            SnapKV &
            0.060 &
            0.008 &
            69.3 &
            68.5 &
            69.5 &
            65.2 &
            67.6 \\

            % 81.50000	80.80000	76.70000	74.40000	78.35000
            Quest &
            0.060 &
            0.031 &
            81.5 &
            80.8 &
            76.7 &
            74.4 &
            78.4 \\

            % 88.67923	85.24077	82.56923	81.76923	84.56462
            MagicPIG &
            0.064 & 
            2.344 &
            88.7 &
            85.2 &
            82.6 &
            81.8 &
            84.6 \\

            % 91.62846	88.14846	83.36154	82.15385	86.32308
            {\name} &
            0.062 &
            0.008 &
            \textbf{91.6} &
            \textbf{88.1} &
            \textbf{83.4} &
            \textbf{82.2} &
            \textbf{86.3} \\
            
            \bottomrule
        \end{tabular}
    }
    \caption{Comparison of sparsity ratio, auxiliary memory usage and accuracy on RULER benchmark. `Aux Mem' refers to `Auxialiary Memory Usage', which denotes the extra memory usage caused by KV cache reduction methods compared to the original key cache. `16K', `32K', `64K' and `96K' denote the input context length.}
    \label{tab:main_experiments}
\end{table*}

Table~\ref{tab:main_experiments} compares the accuracies of different methods on RULER, along with attention sparsity ratios and auxiliary memory overhead.
Experimental results draw the following conclusions:

\noindent \textbf{{\name} minimizes accuracy degradation under comparable sparsity ratios.}
For Llama models,
% {\name} achieves an average accuracy of 86.6, outperforming H2O (27.0) by +59.6, SnapKV (72.7) by +13.9, Quest (80.7) by +5.9, and MagicPIG (85.7) by +0.9.
{\name} achieves an average accuracy of 86.6, outperforming H2O (27.0), SnapKV (72.7), Quest (80.7), and MagicPIG (85.7).
For Mistral models,
% {\name} achieves an average accuracy of 86.3, surpassing H2O (22.3) by +64.0, SnapKV (67.6) by +18.7, Quest (78.4) by +7.9, and MagicPIG (84.6) by +1.7.
{\name} achieves an average accuracy of 86.3, surpassing H2O (22.3), SnapKV (67.6), Quest (78.4), and MagicPIG (84.6).
Notably, {\name} achieves these accuracies with comparable sparsity ratios (0.060 for Llama, 0.062 for Mistral) to H2O, SnapKV and Quest, lower than those of MagicPIG (0.068 for Llama, 0.064 for Mistral). 
% It is important to note that the sparsity ratio shown in Table~\ref{tab:main_experiments} differs in definition from the cost2 in MagicPIG~\citep{magicpig}. 
% Specifically, cost2 measures the ratio of computation overhead (FLOPs) compared to full attention, whereas our sparsity ratio measures the ratio of memory access overhead (MOPs) relative to full attention. 
% Since attention modules are typically considered memory-bound~\citep{transformer-survey}, we argue that the latter metric provides more meaningful insights on potential overhead reduction.
These results demonstrate the superior accuracy preservation of {\name}.

\noindent \textbf{{\name} causes comparable or lower auxiliary memory overhead compared to existing methods.}
{\name} causes auxiliary memory usage (0.008) identical to eviction-based methods, i.e., H2O and SnapKV, while being significantly more memory efficient than retrieval-based approaches, i.e., Quest (0.031) and MagicPIG (2.344).
This efficiency comes from the inherent nature of vector quantization, requiring only one codeword index for each token in each attention head, eliminating the need for storing either per-page metadata (Quest) or large LSH tables (MagicPIG).

\subsection{Ablation Study}

\begin{table}[htb]
    \centering
    \resizebox{1.0\columnwidth}{!}{
        \begin{tabular}{l|cccc|c}
        
            \toprule
            
            \textbf{Config} & 
            \textbf{16K$\uparrow$} & 
            \textbf{32K$\uparrow$} & 
            \textbf{64K$\uparrow$}  & 
            \textbf{96K$\uparrow$}  & 
            \textbf{Average$\uparrow$} \\
            
            \midrule

            % 86.42308	86.30769	81.49231	71.30538	81.38212
            Baseline &
            86.4 &
            86.3 &
            81.5 &
            71.3 &
            81.4 \\

            % 92.27923	90.04615	82.82077	78.39	85.88404
            WRoPE &
            \textbf{92.3} &
            90.0 &
            82.8 &
            78.4 &
            85.9 \\

            % 91.72308	86.88462	76.27462	69.44846	81.08270
            QAVQ &
            91.7 &
            86.9 &
            76.3 &
            69.4 &
            81.1 \\
            
            % 92.18462	90.43615	84.28462	79.57462	86.62000
            {\name} &
            92.2 &
            \textbf{90.4} &
            \textbf{84.3} &
            \textbf{79.6} &
            \textbf{86.6} \\
            
            \bottomrule
        \end{tabular}
    }
    \caption{Ablation study on the importance of WRoPE and query-aware vector quantization for accuracy. 
    %`WRoPE' and `QAVQ' refer to `Baseline w/ WRoPE' and `Baseline w/ Query-Aware Vector Quantization', respectively.
    }
    \label{tab:ablation}
\end{table}

Table~\ref{tab:ablation} validates the effectiveness of WRoPE and query-aware vector quantization
% \note{, mainly for accuracy}.
on improving model accuracy.
Experimental results draw the following conclusions: 
(1) WRoPE is fundamental to attention score approximation using shared codebooks.
(2) Query-aware vector quantization provides a further improvement in model accuracy by aligning the objectives of vector quantization and attention score approximation.
Detailed discussions are presented in Appendix~\ref{appendix:ablation}.

% \noindent \textbf{WRoPE is fundamental to attention score approximation using shared codebooks.}
% WRoPE achieves an average improvement of +4.5 over the baseline, with consistent gains across all context lengths.
% This result confirms WRoPE's critical role in preventing representation divergence of key states caused by positional embedding.

% \noindent \textbf{Query-aware vector quantization provides a further improvement in model accuracy by aligning the objectives of vector quantization and attention score approximation}.
% Our full method, incorporating query-aware vector quantization, demonstrates further improvements, particularly at longer context lengths (+1.5 at 64K, +1.2 at 96K, respectively).
% However, query-aware vector quantization alone underperforms the baseline,
% % (-0.3 on average)
% exhibiting a more significant drop at longer context lengths.
% This performance degradation suggests that representation divergence is not solely limited to key states but also affects query states, hindering the effectiveness of query-aware vector quantization.
% With WRoPE mitigating positional dependencies, query-aware vector quantization further optimizes the attention score approximation, achieving state-of-the-art performance.

\subsection{End-to-End Inference Speedup}

\begin{figure}[t]
    \centering
    \begin{tabular}{cc}
        \begin{subfigure}[b]{0.475\columnwidth}
            \centering
            \includegraphics[width=1.0\columnwidth]{images/throughput_16k.pdf}
            \caption{Inference throughput with a context length of 16K.}
        \end{subfigure}
        & 
        \begin{subfigure}[b]{0.475\columnwidth}
            \centering
            \includegraphics[width=1.0\columnwidth]{images/throughput_64k.pdf}
            \caption{Inference throughput with a context length of 64K.}
        \end{subfigure}
    \end{tabular}

    \caption{End-to-end inference throughput of Llama-3.1-8B-Instruct across varying context lengths and batch sizes.}
    \label{fig:throughput}
\end{figure}

Figure~\ref{fig:throughput} compares the inference throughput of the proposed {\name} against full attention on Llama-3.1-8B-Instruct.
At a context length of 16K, {\name} initially exhibits marginally lower throughput than full attention for small batch sizes \((\leq 5)\), primarily due to CPU-GPU data transfer overhead.
As batch sizes increase, the throughput of {\name} grows linearly to exceed 100 tokens/s, while full attention plateaus below 80 tokens/s due to memory bandwidth bottleneck and ends up out of memory at a batch size of 22.
{\name} achieves a peak throughput of over 160 tokens/s, a 2.1Ã— speedup over full attention, with a maximum batch size of over 64.
This trend becomes more pronounced at 64K context lengths, where full attention struggles with batches over 5, while {\name} serving a batch size of up to 16 with a throughput of up to 45 tokens/s, delivering a \(2.7 \times\) performance advantage.
These results highlight {\name}'s potential in mitigating the memory bottleneck in long context LLM serving.