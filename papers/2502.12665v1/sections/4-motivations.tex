\section{Motivation}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\linewidth]{images/mse_comparison.pdf}
    \caption{A comparison of MSE of attention score approximation between conventional vector quantization and query-aware vector quantization.}
    \label{fig:attention_score_mse}
\end{figure}

% \begin{itemize}[itemsep=0pt]
%     \item Inter-input Similarity of codebooks of pre-PE Key States Suggests Potential for Codebook Sharing
%     \item Optimization objective mismatch between key states vector quantization and attention score approximation hinders accurate recalling of top-k tokens
% \end{itemize}

% In this section, we introduce the key observations that motivate us to propose {\name}.

%\subsection{Inter-Input Similarity of Codebooks Suggests Potential for Codebook Sharing}

\subsection{Inter-Input Similarity of Codebooks}

\label{sec:codebook_sharing}

PQCache~\citep{pqcache} and ClusterKV~\citep{clusterkv} propose applying vector quantization to post-PE key states, with individual codebooks constructed for each input during the prefilling phase.
However, constructing codebooks during inference requires iterative access to the entire KV cache, incurring high memory access overhead.
To address this limitation, we investigate the feasibility of employing shared codebooks for all inputs based on the inter-input similarity of codebooks.

To quantify the similarity between codebooks \(C_1\) and \(C_2\), we define the cosine similarity between codebooks as metric, which can be formulated as:
%formally expressed as:
\begin{equation}
    \label{eq:codebook_similarity}
    \begin{aligned}
        \operatorname*{sim}(C_1, C_2) = & \frac 1 {2L} \sum_{i=1}^L \max_{j \in \{1, 2, \dots, L\}} \cos ({c_1}_i, {c_2}_j) + \\
        & \frac 1 {2L}  \sum_{i=1}^L \max_{j \in \{1, 2, \dots, L\}} \cos ({c_2}_i, {c_1}_j)
    \end{aligned}
\end{equation}
where \({c_1}_i,{c_1}_j \in C_1, {c_2}_i,{c_2}_j \in C_2\).
This metric computes the average maximum cosine similarity from each codeword in one codebook to any codeword in the other codebook.
A cosine similarity score closer to $1$ indicates higher similarity between codebooks, while a score closer to $0$ or negative values indicates lower similarity.

We utilize the Llama-3.1-8B-Instruct model~\citep{llama-3}, and two random samples from the FineWeb dataset~\citep{fineweb} with a context length of approximately 32k tokens each for experiments.
We collect pre-PE and post-PE (RoPE is used in here) key states from all attention heads on both samples, then employ k-means++ algorithm~\citep{kmeans++} to generate codebooks with a size of 4096 codewords for each set of key states, and calculate the inter-sample cosine similarities of pre-PE and post-PE codebooks using Equation (\ref{eq:codebook_similarity}). 
The similarity generated by pre-PE and post-PE codebooks are shown in  Figure \ref{fig:cosine_similarity}.
From the experimental results, we derive the following key observations:

\noindent
\textbf{
    Observation 1: High inter-input similarities of pre-PE codebooks suggest the potential for using shared codebooks to effectively approximate key states across various inputs.
} 
The cosine similarities for pre-PE codebooks remain remarkably high, exceeding 0.9 for the majority of layers, indicate a very strong similarity across codebooks of various inputs. 
This finding indicates that the semantic information in key states might be broadly similar.

\noindent
\textbf{
    Observation 2: The position-dependent nature of post-PE key states hinders the direct application of a shared codebook.
} 
The post-PE codebook similarities are consistently lower, fluctuating around 0.85, with some layers below 0.8, indicating weaker similarity across inputs. 
The reason is that RoPE causes semantically similar key states at different positions to have different representations.
The position-dependent nature of post-PE key states makes it difficult to construct a single codebook that can effectively quantize representations with all semantic information at all possible positions.
This representation divergence becomes a key challenge for directly applying a shared codebook on post-PE key states, necessitating the development of a novel vector-quantization-compatible position embedding method.

\subsection{Objective Misalignment of Vector Quantization}

\label{sec:objective_mismatch}

The optimization objective of vector quantization is to minimize the mean squared error (MSE) of the approximate key states, while attention score approximation focuses on minimizing the MSE of attention scores.
This discrepancy between their optimization objectives raises a fundamental question:

\textit{Does the optimal codebook for vector quantization necessarily yield the most accurate approximation of attention scores?}

Formally, let $C$ denote the codebook constructed by vector quantization, \( J(C) \) denote the MSE of vector quantization with \(C\), and \( J'(C) \) denote the MSE of attention scores approximation with \(C\).
We need to investigate whether the following equation holds:
\begin{equation}
    \operatorname*{argmin}_C J(C) \equiv \operatorname*{argmin}_C J'(C)
\end{equation}
To address this question, we analyze the relationship between \( J \) and \( J' \).

For vector quantization, the objective \(J\) can be reformulated as:
\begin{equation}
    \begin{aligned}
        J(C) & = \mathbb E_{\tilde k \sim \mathcal D^{\mathrm{key}}} [ \| \tilde k - \hat k \|^2 ] \\
        & = \mathbb E_{\tilde k \sim \mathcal D^{\mathrm{key}}} [ ( \tilde k - \hat k)( \tilde k - \hat k)^\top ]
    \end{aligned}
\end{equation}
where \(\mathcal D^{\mathrm{key}}\) denotes the distribution of post-PE key states, and \(\hat k\) denotes quantized key.

For attention score approximation, the objective \(J'\) is formualted as:
\begin{equation}
    \label{eq:objective_attention_score_approximation}
    \begin{aligned}
        J'(C) & = \mathbb E_{\tilde k \sim \mathcal D^\mathrm{key},\ \tilde q \sim \mathcal D^\mathrm{query}}[(\tilde q \tilde k^\top - \tilde q \hat k^\top)^2] \\
        & = \mathbb E_{\tilde k \sim \mathcal D^\mathrm{key},\ \tilde q \sim \mathcal D^\mathrm{query}}[(\tilde q (\tilde k - \hat k)^\top)^2] \\
        & = \mathbb E_{\tilde k \sim \mathcal D^\mathrm{key},\ \tilde q \sim \mathcal D^\mathrm{query}}[(\tilde k - \hat k) \tilde q^\top \tilde q (\tilde k - \hat k)^\top] \\
        &= \mathbb E_{\tilde k \sim \mathcal D^\mathrm{key}} [(\tilde k - \hat k) H (\tilde k - \hat k)^\top]
    \end{aligned}
\end{equation}
where \(\mathcal D^\mathrm{query}\) denotes the distribution of post-PE query states, 
\(H = \mathbb E_{\tilde q \sim \mathcal D^\mathrm{query}}[\tilde q^\top \tilde q] \in \mathbb R^{d \times d}\) denotes the second-moment matrix of query states.

The two objectives only align if the query-dependent \(H\) is proportional to the identity matrix.
To examine this consistency, we visualize \(H\) on a set of input samples,
% of approximately 64K tokens from the FineWeb dataset
as depicted in Figure~\ref{fig:hessian_matrix}.
The visualization shows that \( H \) is not proportional to the identity matrix, displaying a non-uniform and non-diagonal structure.
% This deviation shows that \(H\) is not proportional to the identity matrix, 
This result reveals a fundamental misalignment between the vector quantization objective \(J\) and the attention score approximation objective \(J'\), suggesting potential inaccuracy in attention score approximation.

To validate the impact of this objective mismatch, we compare conventional vector quantization that minimizes objective \(J\), with its query-aware variant that directly minimizes objective \(J'\) (implementation details in Section~\ref{sec:query_aware_vq}) in Figure~\ref{fig:attention_score_mse}. 
From experimental results, the query-aware method consistently achieves lower squared error in attention score approximation.
Thus, we derive the following observation:

\noindent
\textbf{
    Observservation 3:
    Optimizing vector quantization alone fails to guarantee accurate apprroximation of attention scores due to objective misalignment, necessitating query-aware vector quantization for bridging this objective gap. 
}

% This result empirically demonstrates that optimizing \(J\) alone fails to guarantee accurate attention score approximation, necessitating query-aware vector quantization for bridging this objective gap. 
% \note{Thus, we obtain the observation 3.}

% \noindent\note{\textbf{
% Observation 3: The accuracy of the attention score approximation is related to both qurey and key. 
% Only focusing on the accuracy of the approximate key states cannot achieve the best accuracy of the attention score approximation.
% }}

% As shown in Figure~\ref{fig:attention_score_mse}, we demonstrate that a query-aware variant of VQ consistently achieves lower squared error in attention score approximation, which will be detailed in Section~\ref{sec:query_aware_vq}.
% This result highlights the affect of inconsistency in objective, necessitates the need for a query-aware variant of VQ.

% Therefore, \textbf{directly optimizing the vector quantization objective on key states without considering the second-moment matrix of query states will not yield the optimal attention score approximation.}
