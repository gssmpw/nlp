\section{Preliminaries}

% Introduce terminologies and symbols

% \begin{itemize}
%     \item Self-attention module and RoPE
%     \item Vector quantization
% \end{itemize}

% In this section, we introduce the necessary background and notations that will be used throughout the paper.

\subsection{Self-Attention Modules and Rotary Position Embedding}

\label{sec:rope}

Self-attention modules~\citep{transformer} and Rotary Position Embedding (RoPE)~\citep{rope} have become the de facto standard components of state-of-the-art (SOTA) LLMs~\citep{llama-3, qwen-2.5, mixtral, deepseek-v3}.

In the self-attention module, during decoding phase, the inference process begins by linearly projecting the input states of the \(i\)-th token into query (\(q_i\)), key (\(k_i\)), and value (\(v_i\)) states,  where \(q_i, k_i, v_i \in \mathbb{R}^{1 \times d}\), and \(d\) denotes the number of channels or hidden dimensions per head. 
To enable the model to effectively capture the positional relationships between tokens, position embeddings are then applied to the query and key states. 
These hidden states before and after this transformation are abbreviated as pre-PE and post-PE states, respectively.

RoPE is a commonly used position embedding in SOTA LLMs.
% ~\citep{llama-3, qwen-2.5, mixtral, deepseek-v3}.
Specifically, for the \(i\)-th token, a position-dependent rotation matrix \(R_i \in \mathbb{R}^{d \times d}\) is applied to the query \(q_i\) and key \(k_i\), to obtain their post-PE counterparts, denoted by \(\tilde q_i\) and \(\tilde k_i\):
% \note{(\textit{abbr.} pre-PE), to obtain their after position embedding (\textit{abbr.} post-PE) counterparts, which are noted as \(\tilde q_i\) and \(\tilde k_i\) and can be calculated by:}

\begin{equation}
    \tilde q_i = q_i R_i, \quad \tilde k_i = k_i R_i
\end{equation}
Then the matrices
% of all key and value states 
of KV cache
of the context can be denoted by \({\tilde K} = [{\tilde k}_1; {\tilde k}_2; \dots; {\tilde k}_n] \in \mathbb R^{n \times d}\) and \(V = [ v_1;  v_2; \dots;  v_n] \in \mathbb R^{n \times d}\) respectively, where \(n\) denotes the context length.
Next, these post-PE states are used to compute the output state \(o_i\) as shown in formula (\ref{formula:output state}):
%These post-PE states are then used to compute the output state \(o_i\). 
%Let \(n\) denotes the context length, \({\tilde K} = [{\tilde k}_1; {\tilde k}_2; \dots; {\tilde k}_n] \in \mathbb R^{n \times d}\) and \(V = [ v_1;  v_2; \dots;  v_n] \in \mathbb R^{n \times d}\) denote the matrices of all key and value states in the context, respectively. 
%The output state \(o_i\) is then calculated as:
\begin{equation}\label{formula:output state}
    \begin{aligned}
        o_i & = \operatorname{Softmax}\left( \frac{\tilde q_i {\tilde K}^\top}{\sqrt d} \right) V 
        = \operatorname{Softmax} \left( \frac{u_i}{\sqrt d} \right) V
    \end{aligned}
\end{equation}
where \(u_i = \tilde q_i {\tilde K}^\top \in \mathbb R^{1 \times n}\) denotes the attention scores before softmax. 

% A key property of RoPE lies in its elegent incorporation of relative positional information.  
Due to the inherent property of rotation matrices that \(R_i R_j^\top = R_{i-j}\)~\citep{rope}, the attention score \(u_{i,j}\) between the \(i\)-th query and \(j\)-th key can be expressed as:
\begin{equation}
    \label{eq:rope}
    \begin{aligned}
        u_{i,j} = \tilde q_i {\tilde k}^\top_j = q_i R_i (k_j R_j)^\top 
        &= q_i R_i R_j^\top k_j^\top \\
        &= q_i R_{i-j} k_j^\top
    \end{aligned}
\end{equation}
This equation illustrates how RoPE encodes the relative position (\(i-j\)) directly into the attention scores, 
allowing the model to effectively capture
the positional relationships between tokens.
% based on their relative positions.

\subsection{Vector Quantization for Efficient Attention Score Approximation}

Vector quantization~\citep{vector-quantization} is a data compression technique that maps input vectors to a finite set of codewords from a learned codebook.

Formally, given an input space \(X \subseteq \mathbb{R}^{1 \times d}\) with data distribution \(\mathcal D\),
vector quantization aims to construct a codebook \(C = \{c_1, c_2, \dots, c_L\} \subset \mathbb{R}^{1 \times d}\) with a size of \(L\) codewords to minimize the following objective:
\begin{equation}
    \label{eq:vq_objective}
    J(C) = \mathbb E_{x \sim \mathcal D}[ \| x - \hat x \|^2 ]
\end{equation}
where \(x \in \mathbb R^{1 \times d}\) denotes the input vector, \(\hat x = c_{f(x; C)}\) denotes the quantized vector, and \(f(x; C)\) denotes the quantization function that maps \(x\) to its nearest codeword:
\begin{equation}
    f(x; C) = \operatorname*{argmin}_j \| x - c_j \|^2
\end{equation} 

Finding the optimal codebook \(C\) is computationally expensive.
Therefore, approximate algorithms such as LBG and k-means++~\citep{lbg, kmeans++} are commonly used to find a suboptimal but effective codebook.

After obtaining the codebook, vector quantization compresses an input \(x\) by replacing the original vector with its index \(s = f(x; C)\).
Since the storage requirement for the index is substantially lower than that of the original vector, vector quantization achieves significant data compression ratio.

Multiple studies~\citep{transformer-vq, pqcache, clusterkv} have investigated applying vector quantization to post-PE key states of LLMs to efficiently approximate attention scores.
%Let \(s \in \{1, 2, \dots, L\}^n\) denote the codeword indices of all post-PE key states, with the \(i\)-th key state quantized as \(\hat {k}_i = c_{s_i}\).
%The attention score \(u_{i,j}\) then can be approximated as:
Let \(s \in \{1, 2, \dots, L\}^{1 \times n}\) denotes the codeword index vector of all post-PE key states, where the length of this vector is \(n\), and each element \(s_i \in \{ 1, 2, \dots, L \}\) denotes the codeword index of the \(i\)-th key state.
Then, the \(\tilde k_i\) can be quantized as \(\hat {k}_i = c_{s_i}\)
, and the attention score \(u_{i,j}\) can be approximated as:
\begin{align}
    \label{eq:attention_weights_approximation}
    \hat u_{i,j} = \tilde q_i \hat k_j^\top = \tilde q_i c^\top_{s_j}
    % = a_{s_j}
\end{align}
% where \(a = q_iC^\top \in \mathbb R^{1 \times L}\).\note{(the difference between u and a)} 
This equation illustrates the approximation of attention scores without the memory-intensive access to the \(\tilde k_j\).

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{images/inter_input_similarity.pdf}
    \caption{Inter-sample cosine similarities of pre-PE and post-PE codebooks.}
    \label{fig:cosine_similarity}
\end{figure}

\begin{figure}[t]
    \centering
    \begin{tabular}{cc}
        % \begin{subfigure}[b]{0.225\textwidth}
        %     \includegraphics[width=\textwidth]{images/hessian_15_3.pdf}
        %     \caption{Layer 16, Head 4}
        % \end{subfigure}
        % &
        \begin{subfigure}[b]{0.225\textwidth}
            \includegraphics[width=\textwidth]{images/hessian_15_7.pdf}
            \caption{Layer 16, Head 8}
        \end{subfigure}
        &
        % \begin{subfigure}[b]{0.225\textwidth}
        %     \includegraphics[width=\textwidth]{images/hessian_31_3.pdf}
        %     \caption{Layer 32, Head 4}
        % \end{subfigure}
        % &
        \begin{subfigure}[b]{0.225\textwidth}
            \includegraphics[width=\textwidth]{images/hessian_31_7.pdf}
            \caption{Layer 32, Head 8}
        \end{subfigure}
    \end{tabular}

    \caption{Visualization of second-moment matrices \(H\) of post-PE query states. Each pixel represents an element in \(H\). Warmer colors correspond to higher values, while cooler colors correspond to lower values.}

    \label{fig:hessian_matrix}
\end{figure}