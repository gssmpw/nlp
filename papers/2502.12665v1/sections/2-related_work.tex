\section{Related Work}

% \begin{itemize}
%     \item H2O, SnapKV
%     \item PQCache, ClusterKV
%     \item Quest, MagicPIG
% \end{itemize}

% To address the memory challenge in long-context LLM serving, a common strategy is to exploit the sparsity of attention weights~\citep{sparse-transformers} by reducing the number of tokens accessed in decoding phase. 
% This approach can be categorized into three main branches: 

%eviction-based KV cache reduction and retrieval-based KV cache reduction.
\noindent\textbf{Quantization-based KV cache reduction.}
This method aims to compress KV cache by using lower bit-width representations for KV cache elements~\citep{kivi, kvquant}.
However, it typically faces challenges of limited compression ratio and extra computational overhead caused by the dequantization process.

\noindent\textbf{Eviction-based KV cache reduction.}
This method aims to reduce the KV cache size by directly evicting unimportant tokens from memory~\citep{streaming-llm, h2o, snapkv, pyramidinfer}. 
These methods typically record statistics of attention weights of each token. 
When the KV cache reaches its capacity limit, they utilize heuristic rules and historical statistics to predict which tokens are more likely to get high attention weights in future decoding, then retain these tokens while evicting the rest. 
Although these methods generally have low additional overhead, they often lead to noticeable performance degradation.

\noindent\textbf{Retrieval-based KV cache reduction.}
This method keeps the entire KV cache in memory while selectively retrieving tokens crucial for the current inference. 
Quest~\citep{quest} chunks the continuous KV cache into pages and pre-calculates necessary metadata for each page during prefilling. 
For decoding, it selects the top-K critical cache pages to participate in selective attention computation. 
% Despite the relatively low overhead, Quest lacks sophisticated design in the retrieval strategy, thus suffers from noticeable performance degradation. 
PQCache~\citep{pqcache} and ClusterKV~\citep{clusterkv} perform vector quantization on the key states with individual codebooks constructed for each input during prefilling.
For decoding, the system uses codewords of the codebooks to approximate attention scores, then retrieves the top-K tokens for computation. 
% However, the process of building codebooks requires iterative access to the entire KV cache, significantly reducing the efficiency of the prefilling phase. 
MagicPIG~\citep{magicpig} employs Locality-Sensitive Hashing on query and key states for token retrieval.
% and Self-Normalized Weight Sampling for output approximation.
% , to estimate attention output accurately while only accessing a small proportion of tokens. 
% However, this approach requires a massive hash table, whose memory footprint even exceeds twice the size of the original key states, thus introduces significant auxiliary memory overhead.
Although these methods generally achieve lower performance degradation compared to eviction-based methods, they still suffer from unsatisfactory performance and lead to extra
% computation and memory 
overhead.