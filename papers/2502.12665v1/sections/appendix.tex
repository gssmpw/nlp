\section{Method Configurations}

\label{appendix:baselines}

For the main experiments, we compare the following five KV cache reduction methods, along with the full attention baseline:

\begin{itemize}[nosep]
    \item \textbf{H2O}~\citep{h2o}: An eviction-based method that preserves heavy hitter tokens and recent tokens;
    \item \textbf{SnapKV}~\citep{snapkv}: An eviction-based method that preserves important tokens based on statistics within an observation window;
    \item \textbf{Quest}~\cite{quest}: A retrieval-based method that selects tokens based on metadata of KV cache pages;
    \item \textbf{MagicPIG}~\cite{magicpig}: A retrieval-based method that utilizes LSH for token sampling;
    \item \textbf{{\name}}: The proposed method.
\end{itemize}

For a fair comparison, the sparsity ratios of all KV cache reduction methods are controlled around 0.06, which means approximately 6\% of KV cache is accessed at each inference.
It is important to note that the sparsity ratio discussed in this paper differs in definition from the \(\mathrm{cost}_2\) in MagicPIG~\citep{magicpig}. 
Specifically, \(\mathrm{cost}_2\) measures the ratio of computation overhead (FLOPs) compared to full attention, whereas our sparsity ratio measures the ratio of memory access overhead (MOPs) relative to full attention. 
Since attention modules are typically considered memory-bound~\citep{transformer-survey}, we argue that the latter metric provides more meaningful insights on potential overhead reduction.
Additionally, the initial 4 tokens and the most recent 64 tokens are statically preserved to align with MagicPIG~\citep{streaming-llm}.
The detailed configurations for each methods are shown in Table~\ref{tab:configs}.

\begin{table}[ht!]
    % \centering
    \resizebox{\columnwidth}{!}{
        \begin{tabular}{c|c}
            \toprule
                
            \textbf{Method} & \textbf{Configurations} \\ 
            \midrule
            H2O 
            & \texttt{hh\_size} = 0.06 \(\times\) \texttt{input\_length} \\
            
            SnapKV & 
            \texttt{prompt\_capacity} = 0.06 \(\times\) \texttt{input\_length} \\
            
            Quest & 
            \texttt{page\_size} = 32, \texttt{ratio} = 0.06 \\
            
            MagicPIG & 
            \texttt{K} = 10, \texttt{L} = 150 \\
            
            {\name} & 
            \texttt{topk} = 0.03 \\
            
            \bottomrule
        \end{tabular}
    }
    \caption{Configurations of KV cache reduction methods.}
    \label{tab:configs}
\end{table}

\section{Detailed Discussions of Ablation Study}

\label{appendix:ablation}

Table~\ref{tab:ablation} validates the effectiveness of WRoPE and query-aware vector quantization
on improving model accuracy.
Experimental results draw the following conclusions: 

\noindent \textbf{WRoPE is fundamental to attention score approximation using shared codebooks.}
WRoPE achieves an average improvement of +4.5 over the baseline, with consistent gains across all context lengths.
This result confirms WRoPE's critical role in preventing representation divergence of key states caused by positional embedding.

\noindent \textbf{Query-aware vector quantization provides a further improvement in model accuracy by aligning the objectives of vector quantization and attention score approximation}.
Our full method, incorporating query-aware vector quantization, demonstrates further improvements, particularly at longer context lengths (+1.5 at 64K, +1.2 at 96K, respectively).
However, query-aware vector quantization alone underperforms the baseline,
% (-0.3 on average)
exhibiting a more significant drop at longer context lengths.
This performance degradation suggests that representation divergence is not solely limited to key states but also affects query states, hindering the effectiveness of query-aware vector quantization.
With WRoPE mitigating positional dependencies, query-aware vector quantization further optimizes the attention score approximation, achieving state-of-the-art performance.