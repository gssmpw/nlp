\section{Introduction}

% \begin{itemize}
%     \item Memory access overhead of KV cache (1st Para)
%     \item Related work, introduce existing branches but not in details (2nd Para)
%     \item Method \& Experiments (3rd Para)
%     \item Contributions (4th Para)
% \end{itemize}

Large language models (LLMs) with long context windows~\citep{gpt-4, gemini-1.5, llama-3, mixtral, qwen-2.5, deepseek-v3} are driving advancements in AI applications. 
However, these models pose significant challenges for efficient serving. 
Their Transformer-based~\citep{transformer} architecture generates and maintains a Key-Value (KV) cache during inference to store intermediate results and avoid re-computation.  
As the context length increases, the size of the KV cache grows proportionally, leading to severe overheads.
First, the size of KV cache accessed when generating each token increases, resulting in a GPU memory bandwidth bottleneck. 
Moreover, the large KV cache size of each request limits the maximum feasible batch size, resulting in suboptimal GPU utilization.

Various methods were proposed to address these challenges from different perspectives.
Quantization-based methods~\citep{kivi, kvquant} compress KV cache by using lower bit-width representations for KV cache elements.
Eviction-based methods~\citep{streaming-llm, h2o, snapkv, pyramidinfer} reduce the KV cache size by directly evicting unimportant tokens from memory.
Retrieval-based methods~\citep{quest, loki, pqcache, clusterkv, magicpig} offload the complete KV cache to CPU memory and retrieve necessary tokens on demand during inference.
However, these methods still face challenges of limited compression ratio, unsatisfactory accuracy degradation, or extra retrieval overhead.

To address the above limitations, this paper proposes {\name}, a novel retrieval-based KV cache reduction method.
{\name} aims to obtain an \textbf{A}ccurate \textbf{A}pproximation of \textbf{AT}tention \textbf{S}cores by applying vector quantization technique to key states, thereby enabling efficient and precise retrieval of the top-K tokens.
% This method aims to apply vector quantization utilizing the similarities across inputs,  to key states for accurate attention score approximation and top-K tokens retrieval.
% In order to efficiently obtain this accurate approximation, We apply vector quantization with shared and pre-constructed codebooks to key states.
In order to achieve this goal, we face two main challenges.
First, the position-dependent nature of key states after applying position embedding hinders the direct application of shared codebooks across varying inputs.
Second, directly utilizing the conventional vector quantization fails to guarantee an accurate approximation of attention scores.
To overcome these challenges, the main contributions in this paper are as follows:
\begin{itemize}[nosep]
    \item We observe high inter-input similarities between
    codebooks 
    % when applying vector quantization to key states, 
    of key states before position embedding,
    and the objective misalignment between vector quantization and attention score approximation by experimental and theoretical analysis;
    \item We propose Windowed Rotary Position Embedding to decouple the positional dependency from query and key states after position embedding,
    \item 
    We propose 
    and query-aware vector quantization that directly optimizes the objective of attention score approximation;
    \item We design the heterogeneous inference system for KV cache offloading, enabling long context serving with larger batch sizes.
\end{itemize}
% To validate the effectiveness of the proposed {\name}, we conduct comprehensive experiments on long context benchmarks.
% Experimental results demonstrate that {\name} can achieve a lower performance degradation with similar or lower overhead compared to existing methods, thereby increasing long context serving throughput by up to \(2.7 \times\).
Experimental results demonstrate that {\name} can achieve a low accuracy degradation of 2.2 on Llama-3.1-8B and 0.4 on Mistral-7B while accessing only 6\% of the entire KV cache, thereby increasing long context serving throughput by up to \(2.7 \times\).
% Our source code is publicly available~\footnote{\url{https://anonymous.4open.science/r/4987d19d-f5cd-4f14-910d-c7141ce37f13/}}.
Our source code is publicly available~\footnote{\url{https://anonymous.4open.science/r/4987d19d-f5cd-4f14-910d-c7141ce37f13/}}.

% optimizes long context LLM serving via Windowed Rotary Position Embedding and query-aware vector quantization.
% First, this paper introduces our observations on the inter-input similarities of codebooks when applying vector quantization to key states, and the objective misalignment between vector quantization and attention score approximation.
% Then, this paper proposes Windowed Rotary Position Embedding, a novel position embedding that decouples the positional dependency from query and key states after position embedding.
% Furthermore, this paper proposes query-aware vector quantization, which aligns the objective of vector quantization on key states with that of attention score approximation.
% Finally, this paper presents the heterogeneous inference design for KV cache offloading, thereby enabling larger batch sizes.

% To validate the effectiveness of the proposed {\name}, we conduct comprehensive experiments on long context benchmarks.
% Experimental results demonstrate that {\name} can achieve a lower performance degradation 
% % while accessing a comparable ratio of KV cache, 
% with similar or lower overhead compared to existing methods, thereby increasing long context serving throughput by up to \(2.7 \times\).
% Our source code is publicly available~\footnote{https://anonymous.4open.science/r/4987d19d-f5cd-4f14-910d-c7141ce37f13/}.

% <todo>