\section{Limitations}

The limitations of this work can be summarized in two main aspects.

First, while {\name} demonstrates lower accuracy degradation compared to existing methods while accessing a comparable proportion of KV cache,
it still exhibits non-negligible performance degradation.
This suggests opportunities for future work to investigate adaptive attention sparsity allocation strategies that dynamically optimize the sparsity ratios across layers and attention heads, based on their contextual importance.

Second, while {\name} increases long context serving throughput by up to \(2.7 \times\), our current implementation is limited to single-GPU deployment.
Future research could further explore (1) distributed multi-GPU system designs for scaled deployment, (2) integration with disaggregated LLM serving architectures like MoonCake~\citep{mooncake}.