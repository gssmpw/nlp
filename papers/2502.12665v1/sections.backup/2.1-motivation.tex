\section{Motivation}

Introduce the overall workflow of this paper

\begin{itemize}
    \item Offline codebook clustering for each key-value head on a calibration dataset
    \item Identify the nearest codeword for each key state during inference and store the corresponding index
    \item Compute the approximate attention weight with codebooks and indices during decoding inferences
    \item Recall top-k key-value pairs to perform selective attention computation
\end{itemize}

Introduce the two main difficulties

\subsection{Employing VQ on post-RoPE key states leads to increased reconstruction error}

The purpose of vector quantization on key states is to cluster key states with similar semantics into the same category, and assign the same values when calculating approximate attention weights. 
However, the preceding RoPE causes key states with the similar semantics to have different representations at different positions, making clustering more difficult and wasting codebook space.

Show an experiment of VQ reconstruction error on pre and post RoPE key states respectively; the x-axis is layer indices, the y-axis is square errors.

\textbf{Observation}: LLMs are not sensitive to the relative distance of non-local tokens.
We modify the original RoPE into the following equation, which assigns a fixed relative distance for all non-local key states:

\begin{equation}
    u'_{i,j} =
    \begin{cases} 
        q_i R_b {k}_j^T & \text{if } i - j \ge w \\
        {q}_i R_{i-j} {k}_j^T & \text{else}
    \end{cases}
\end{equation}

Show an experiment to demonstrate this observation

\subsection{VQ solely focusing on key states leads to increased error in aprpoximate attention weights}

