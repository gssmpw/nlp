\section{Background}

\subsection{Memory Access Bottleneck of LLM Inference}

Introduce the prefilling and decoding stage of LLMs and the memory bottleneck caused by autoregressive generation.

Introduce batching, which alleviates this bottleneck by processing multiple requests simultaneously and share the access of model weights.

Introduce the difficulty of batching in long context serving

\begin{itemize}
    \item No enough GPU memory to store KV cache
    \item High KV cache access overhead
\end{itemize}

\subsection{Self-Attention Module and Rotary Position Embedding}

show an equation of how RoPE injects relative positional information through absolute position encoding.

\subsection{Recalling-Based KV Cache Reduction}

Recalling-based KV cache reduction is considered a promising method to mitigate the challenges in long context serving.

Its core idea is to make use of the inherent sparsity of the output tensor of the softmax function in attention modules, to reduce KV cache access during decoding stage. Its workflow contains two steps, first, for each decoding step, it identifies the required key-value pairs with the highest attention weights, then recall these kv cache and perform selective attention computation.

Introduce PQCache and ClusterKV, and analyze their weakness: online clustering leads to inefficient iterative read of KV cache, resulting in even higher memory access overhead.

Introduce Quest, although it does not introduce extra overheads, it leads to substantial model performance degradation and has limited compression ratio.

Introduce MagicPIG, although it achieves SoTA in recalling-based methods, it causes massive memory usage on CPU due to the large LSH table.

