\section{Evaluation}

\subsection{Experimental Setup}\label{subsec:exp_setup}

In our evaluation, we adopt \textit{Code-Llama-7B} as the pre-trained model for fine-tuning, employing the low-rank-adaption (QLoRA)~\cite{hu2021lora, dettmers2024qlora} technique for faster training and lower memory consumption. 
Key configurations include loading the model in 8-bit, a sequence length of 4096, sample packing, and padding to sequence length. We set the warmup steps to 100, with a gradient accumulation of 4 steps, a micro-batch size of 4, and an inference batch size of 2.
For both syntax and functionality checks, we measure pass@3 accuracy as metrics. In the ablation study from~\secref{subsec:exp_finetune} to~\secref{subsec:complexity}, we adopt \textit{MachineGen} for evaluation.

Experiments are conducted on a server with four NVIDIA L20 GPUs (48 GB each), an 80 vCPU Intel® Xeon® Platinum 8457C, and 100GB of RAM. This setup ensures sufficient computational power and memory to handle the intensive demands of fine-tuning and inference efficiently, especially for long data sequences in the feedback loop experiment. 

\subsection{Effect of Supervised Finetuning}\label{subsec:exp_finetune}
Our first ablation study investigates the effect of the model fine-tuning.
We evaluated the performance based on both syntax and functionality checks. 
As shown in~\figref{fig:finetune_cot}(a), the results demonstrate that the finetuning dramatically increases syntax correctness from $54.85$\% to $88.44$\%. 
More importantly, the impact of finetuning is even more pronounced in the functionality evaluation, where the non-finetuned model failed to achieve any correct functionality test, but the accuracy is improved to $53.20$\% in the finetuned model. These enhancements highlight the critical role of finetuning in producing not only syntactically correct but also functionally viable codes, which demonstrates the benefits of finetuning LLMs for hardware design in the HLS code generation task.




\subsection{Effect of Chain-of-Thought Prompting}\label{subsec:exp_cot}
To assess the effect of the chain-of-thought (CoT) technique,
we perform both syntax and functionality evaluation on the fine-tuned model with and without the use of CoT.
As indicated in~\figref{fig:finetune_cot}(b), incorporating CoT leads to a noticeable improvement in both metrics. 
Specifically, syntax correctness increases from $88.44$\% to $94.33$\%, and functionality score rises from $53.20$\% to $61.45$\%. 
The result demonstrates the effectiveness of CoT in enhancing the reasoning capability, thereby improving its overall performance.


\subsection{Effect of Feedback Loops}\label{subsec:exp_feedback}
Our two-step feedback loop provides both syntax and functionality feedback. We evaluate the impact of these feedback loops with different numbers of iterations, ranging from 0 to 2.The results, shown in Figure ~\figref{fig:syntax_feedback} and ~\figref{fig:func_feedback}, indicate that both syntax and functionality feedback loops significantly improve model performance, especially when combined with COT prompting. The initial feedback loop yields substantial accuracy improvements in both syntax correctness and functionality evaluation, though the second loop shows diminishing returns.Syntax feedback loops enhance both syntax correctness and functionality performance, suggesting that iterative refinement is particularly effective for complex tasks. Similarly, functionality feedback loops not only improve functionality checks but also boost syntax accuracy, indicating that enhancements in functional understanding contribute to better syntactic performance.

 \begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{./figures/merged_finetune_cot.pdf}
    \vspace{-5mm}
    \caption{Effect of fine-tuning and chain-of-thought.}
    \label{fig:finetune_cot}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{./figures/Effect_of_Syntax_Feedback_Loop.pdf}
    \vspace{-2mm}
    \caption{Effect of syntax feedback loop.}
    \vspace{-2mm}
    \label{fig:syntax_feedback}
\end{figure}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{./figures/Effect_of_Functionality_Feedback_Loop.pdf}
    \caption{Effect of functionality feedback loop.}
    \vspace{-2mm}
    \label{fig:func_feedback}
\end{figure}


\subsection{Time Cost and Hardware Performance}\label{subsec:exp_timecost}

\figref{fig:time_cost} shows the time cost for generating 120 data entries under different conditions, measuring the impact of CoT and feedback loops. Without a feedback loop, CoT significantly reduces the time. Adding a syntax feedback loop increases the time, but CoT continues to notably decrease the duration. The functionality feedback loop is the most time-consuming, though CoT still provides a notable reduction, albeit less dramatic. This demonstrates CoT's effectiveness in reducing operational times across varying complexities.

For the test set,
we evaluate the latency and resource consumption of the generated \textit{HLS} designs using a Xilinx VCU118 as our target FPGA, with a clock frequency of $200$MHz and Xilinx Vivado 2020.1 for synthesis.
As shown in~\tabref{tb:perf_resource}, all \textit{HLS} designs demonstrate reasonable performance, with BRAM usage consistently remained at zero due to the design scale.

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{./figures/Time_Cost_Analysis.pdf}
    \vspace{-2mm}
    \caption{Time cost of code generation.}
    \label{fig:time_cost}
\end{figure}


\begin{table}[htb]
\centering
\caption{Latency and resource usage of LLM-generated designs synthesized on a VCU118 FPGA.}
\label{tb:perf_resource}
\setlength\tabcolsep{1pt} 
\scalebox{0.8}{
% \begin{tabular}{L{2cm}ccC{1.5cm}C{2.5cm}}
\begin{tabular}{C{2.5cm}|C{1.9cm}|C{1.5cm}|C{1.5cm}|C{1.3cm}|C{1.3cm}}
\toprule
{}& \textbf{Latency} (ms)& \textbf{LUTs} & \textbf{Registers} & \textbf{DSP48s} & \textbf{BRAMs} \\ \midrule
{\textbf{Available}} & - & 1182240 & 2364480 & 6840 & 4320 \\ \midrule
{\textit{ellpack}} & 0.304 & 1011 & 1079 & 11 & 0 \\
{\textit{syrk}} & 21.537 & 1371 & 1621 & 19 & 0 \\
{\textit{syr2k}} & 40.626 & 1572 & 1771 & 19 & 0 \\
{\textit{stencil2d}} & 1.368 & 287 & 123 & 3 & 0 \\
{\textit{trmm-opt}} & 15.889 & 1262 & 1239 & 11 & 0 \\
{\textit{stencil3d}} & 21.537 & 1173 & 1271 & 20 & 0 \\
{\textit{symm}} & 24.601 & 1495 & 1777 & 19 & 0 \\
{\textit{symm-opt}} & 16.153 & 1361 & 1608 & 19 & 0 \\
{\textit{symm-opt-medium}} & 579.0 & 2223 & 2245 & 22 & 0 \\
\bottomrule
\end{tabular}}
\end{table}

\subsection{Effect of Task Complexity}\label{subsec:complexity}
We analyze the effects of code complexity on the performance of fine-tuning our language model with CoT prompting and tested without the use of any feedback loops during inference. 
We categorize \textit{MachineGen} into three classes according to their code complexity: easy, medium, and difficult.
The results shown in the \tabref{tab:model_performance} indicates a clear trend: as the complexity of the generated code increases, both syntax and functionality correctness rates decline. This outcome could be attributed to several factors. First, more complex code inherently presents more challenges in maintaining syntactic integrity and functional accuracy. Second, the absence of feedback loops in the inference phase may have limited the model's ability to self-correct emerging errors in more complicated code generations.

\begin{table}[h]
\centering
\caption{Performance across different complexity levels.}
\scalebox{0.8}{
\begin{tabular}{c|c|c}
\hline
\textbf{Test Set} & \textbf{Syntax Check} & \textbf{Functionality} \\
\hline
Easy & 96.67\% & 63.33\% \\
Medium & 96.67\% & 53.33\% \\
Difficult & 90\% & 53.33\% \\
\hline
\end{tabular}}
\label{tab:model_performance}
\end{table}

\subsection{Analysis of \textit{MachineGen} and \textit{HumanRefine}}
\begin{table}[h]
\centering
\caption{Performance on \textit{MachineGen} and \textit{HumanRefine}.}
\scalebox{0.9}{
\begin{tabular}{c|c|c}
\hline
\textbf{Test Set} & \textbf{Syntax Check} & \textbf{Functionality Check} \\
\hline
\textit{MachineGen} & 93.83\% & 62.24\% \\
\hline
\textit{HumanRefine} & 47.29\% & 21.36\% \\
\hline
\end{tabular}}
\vspace{-3mm}
\label{table:eval_comparison}
\end{table}

As shown in~\tabref{table:eval_comparison}, this section compares the performance of our model on \textit{MachineGen} and \textit{HumanRefine} test sets.
Our findings reveal that the performance on the \textit{HumanRefine} is significantly lower than on the \textit{MachineGen}. This disparity suggests that the model is more adept at handling machine-generated prompts. The primary reasons for this are: the model's training data bias towards machine-generated prompts, the increased complexity and nuanced nature of human-generated prompts, and the conciseness and clarity of human-generated prompts that often omit repetitive or explicit details found in machine-generated prompts, making it harder for the model to generate syntactically and functionally correct code.

\subsection{Thoughts, Insights, and Limitations}

\noindent \textbf{1. \textit{HLS} versus \textit{HDL} for AI-assisted code generation:} The selection of programming language for hardware code generation should mainly depend on two  factors:
\begin{itemize}[leftmargin=*]
    \item \textit{Quality of Generated Hardware Design}: The evaluation of hardware design's quality includes syntax correctness, functionality, and hardware performance.
    Since \textit{HLS} shares similar semantics and syntax with programming languages commonly used during LLM pre-training, this work demonstrates that the LLM-assisted code generation for \textit{HLS} has the potential to achieve high syntax and functional correctness in hardware designs. While this work does not leverage hardware performance as feedback for design generation, it identifies this aspect as a key direction for future research and enhancements.
    \item \textit{Runtime Cost of Hardware Generation}: Although \textit{HLS}-based designs typically require fewer tokens compared to \textit{HDL} during the code generation phase—suggesting potentially lower costs—the overall runtime costs associated with HLS synthesis must also be considered. A more comprehensive quantitative comparison of these runtime costs is planned for our future work. 
\end{itemize}

\noindent \textbf{2. Input instructions and datasets are crucial}: The fine-tuning of pre-trained LLMs on \textit{HLS} dataset can bring a significant improvement in the design quality, echoing findings from previous studies on \textit{Verilog} code generation~\cite{thakur2023verigen}. 
Additionally, during our evaluation, we found that employing simple CoT prompting largely improves hardware design quality. 
This result contrasts with the application of CoT in general-purpose programming languages, where a specialized form of CoT is necessary~\cite{li2023structured}.
Therefore, future efforts for further enhancement can focus on collecting high-quality datasets and exploring better refinement of input prompts.

\noindent \textbf{3. Limitations}: At the time of this research, more advanced reasoning models, such as DeepSeek-R1~\cite{guo2025deepseek}, were not available for evaluation. Additionally, test-time scaling approaches~\cite{welleck2024decoding} could be incorporated to further enhance performance in the future.
Moreover, we observe that the diversity of hardware designs in the benchmark is limited, which may impact the generalizability of our findings.
We intend to address these limitations in our future work.
