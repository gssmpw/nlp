\begin{figure}[t]
\centering
\includegraphics[width=85mm]{./figures/data_availability.pdf}
\vspace{-7mm}
\caption{Comparison of data availability between HDLs and other software programming languages.}
\vspace{-5mm}
\label{fig:comp_data_avail}
\end{figure}


\section{Introduction}

In the field of Generative AI (GenAI), significant strides have been made in producing complex and creative content across various domains such as text~\cite{brown2020language}, image~\cite{avrahami2022blended,saharia2022image}, and video~\cite{opensora}. 
Among various GenAI technologies, large language models (LLMs) have emerged as particularly influential techniques in the realm of natural language processing~\cite{zhao2023survey}. 
This great capability of LLMs also raises intensive industrial interests in leveraging these models in automated code generation, as evidenced by GitHub Copilot~\cite{chen2021evaluating} and DeepMindâ€™s AlphaCode~\cite{li2022competition}.
Meanwhile, over 50 pre-trained models and more than 170 programming language datasets have been published in the past few years~\cite{zhang2023unifying}.
Although significant progress has been made in this direction, most of these works mainly focus on software code generation~\footnote{This paper mainly focuses on text-to-code generation.}, and the potential of LLM for hardware design generations has not been fully exploited.


The promises of LLM-assisted software programming has led to several recent attempts to explore automated code generation for hardware description languages (HDLs) such as \textit{Verilog} and \textit{SystemVerilog}~\cite{li2023starcoder, lu2024rtllm, liu2023verilogeval, thakur2023verigen}.
Although multiple datasets, pre-trained models, and code infrastructures have been introduced, several key challenges reamin in LLM-assisted hardware design generation: 

\begin{itemize}[leftmargin=*]
    \item \textbf{The data availability of HLD designs for LLM training and finetuning}. \figref{fig:comp_data_avail} compares the volume of training samples for software programming languages versus HDLs.
    For instance, the general-purpose code dataset \textit{StarCoder}~\cite{li2023starcoder}, as presented in~\figref{fig:comp_data_avail} (a), shows that the available amount of HDL designs is less than $1\%$ of those for \textit{C++}.
    Similar trends can also be observed in specialized datasets, such as \textit{RTLLM}~\cite{lu2024rtllm} for \textit{Verilog} and \textit{CodeParrot}~\cite{tunstall2022natural} for \textit{Python}. \figref{fig:comp_data_avail}(b) indicates that the dataset size of \textit{RTLLM} is less than $1\%$ of that for \textit{CodeParrot}. Therefore, the quantity of training data available for hardware design is significantly lower than for software programming languages. 
    % Given the critical role of datasets in training robust models, this data scarcity poses substantial challenges in delivering high-performance LLMs for hardware generation. 
    \item \textbf{Inability of utilizing learned knowledge from pre-trained coding LLMs}. Pre-trained coding LLMs are primarily trained on software programming languages, which differ significantly in semantics and syntax from HDLs. Therefore, the knowledge acquired during pre-training cannot be directly applied to hardware code generation, compounding the data scarcity issue. 

    \item \textbf{Cost of HDL generation using LLMs}. \figref{fig:hls_vs_hdl} illustrates the number of tokens required for generating identical hardware designs using High-Level Synthesis (\textit{HLS}) versus HDL. It shows that HDL implementations require approximately $3 \thicksim 4$ times more tokens than \textit{HLS} designs, making HLS-based design generation a more sustainable solution considering the latency, energy, and monetary costs associated with LLM inference. 
    % Considering the latency, per-token energy and monetary costs associated with LLM inference, using LLMs for \textit{HLS}-based design generation seems to be a more sustainable solution.
    
\end{itemize}

To address the aforementioned issues, 
this paper proposes an LLM-assisted framework for generating \textit{HLS}-based\footnote{This paper focues on C-based \textit{HLS}.} hardware designs.
By crawling \textit{HLS} designs from open-source Github repositories,
we collect a dataset to facilitate the fine-tuning of pre-trained LLM for the downstream \textit{HLS} code generation.
The benefits of generating \textit{HLS} code are two folds:
\textit{i)} Given that \textit{HLS} shares main semantics and syntax with \textit{C/C++}, the coding knowledge acquired during the pre-training phase of the LLMs can be effectively utilized for hardware design. 
This compatibility also reduces the learning curve and dataset requirements for fine-tuning, as the additional knowledge needed for \textit{HLS} is less than for traditional HDL coding.
\textit{ii)} The number of tokens required to generate \textit{HLS} code is lower compared to HDLs, rendering our approach more cost-effective and energy-efficient than previous methodologies.
To further improve the quality of the generated designs, the framework incorporates debugging feedback loops and a chain-of-thought enhancement mechanism, systematically integrating detected bugs back into the input for iterative refinement. 
Overall, our contributions can be summarized as follows:

\begin{itemize}[leftmargin=*]
    \item Finetuning pre-trained code language models for \textit{HLS}-based hardware generation, using a collected dataset with over $40,000$ data entries, each containing a text prompt and the corresponding \textit{HLS}-based hardware design (\secref{subsec:benchmark}).
    \item Developing a framework that automatically produces \textit{HLS} designs from input prompts, with an end-to-end evaluation of the syntax and functionality correctness (\secref{subsec:hwgen_overview}). 
    \item Integrating multiple optimization techniques such as feedback loops and chain-of-thought techniques, which improve the pass rate for syntax and functionality (\secref{subsec:hwgen_cot} \& \secref{subsec:hwgen_loop}).
\end{itemize}



\begin{figure}[t]
\centering
\includegraphics[width=85mm]{./figures/hls_vs_hdl.pdf}
\vspace{-5mm}
\caption{\textit{HLS}-based and \textit{Verilog}-based programs.}
\vspace{-7mm}
\label{fig:hls_vs_hdl}
\end{figure}
