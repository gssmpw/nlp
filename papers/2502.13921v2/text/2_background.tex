\section{Background and Related work}
\subsection{LLM-Assisted Software Engineering}\label{sebsec:background_llm_software}

Based on the modality of inputs and outputs,
language models for software engineering can be categorized into several downstream code tasks~\cite{zhang2023unifying} such as text-to-code (code generation/synthesis~\cite{ling2016latent}), code-to-text (code summarization~\cite{iyer2016summarizing}), and code-to-pattern processing (defect detection~\cite{ray2016naturalness}).
This paper focuses on code generation that aims at producing code from natural language descriptions/prompts.
To facilitate the development of code generation with language models, various datasets, approaches, and pre-trained models have been introduced over the past decade.

Due to the lack of model capability,
the early-stage methods~\cite{ling2016latent, iyer2018mapping} of code generation mainly focus on a few programming languages such as \textit{Python} or \textit{Java}.
Subsequently,
with the increasing computational power,
larger datasets are introduced to train models for multiple general-purpose programming languages.
\textit{CodeXGLUE}~\cite{lu2021codexglue} presents a comprehensive code dataset consisting of different code tasks such as clone detection and code repair.
\textit{HumanEval}~\cite{chen2021evaluating} dataset together with the code model \textit{CodeX} marks as a milestone by using pre-trained LLMs for code generation.
The promising capability shown by \textit{CodeX} sparks significant academic and industrial interests in developing LLM-assisted code generation. 
Larger code datasets, such as \textit{StarCoder}~\cite{li2023starcoder} and \textit{CodeParrot}~\cite{tunstall2022natural}, and LLMs, including \textit{Code-LLaMA}~\cite{roziere2023code} and \textit{CodeFuse}~\cite{di2023codefuse}, are open-sourced in this community.
However, most of these recent efforts focus on software programming languages.
% , without specific consideration for hardware code generation.

\subsection{Automated Hardware Design Generation}\label{sebsec:background_hwgen}

Building on the success of LLM-assisted software programming, 
recent studies have explored using language models for automated hardware generation.
Since the data is the key to training LLMs for hardware code generation,
multiple HDL datasets have been introduced recently.
Thakur~\textit{et al.} present \textit{Verigen}~\cite{thakur2023verigen} dataset that contains $17$ hardware designs with $0.3$K lines of HDL code.
To increase the diversity of hardware designs for training and evaluation,
Lu~\textit{et al.} open-source a larger benchmark consisting of $30$ designs with more than $2.5$K lines.
Sourced from \textit{HDLBits}\footnote{\url{https://hdlbits.01xz.net/wiki/Main_Page}},
\textit{Verilogeval}~\cite{liu2023verilogeval} introduces larger datasets with $156$ problems.
These open-sourced datasets provide public benchmarks for text-to-HDL generation.
% greatly facilitate the development of LLM-assisted hardware generation.

The evaluation metrics of LLM-assisted hardware code generation focus on three aspects: \textit{i)} syntax, \textit{ii)} functionality, and \textit{iii)} quality.
In previous literature~\cite{chen2021evaluating,liu2023verilogeval}, syntax correctness and functionality are measured using pass$@k$ metric which represents whether any of $k$ generated code samples can pass the syntax check of synthesis tools or functional unit tests.
The quality usually is defined as power, performance, and area of the generated hardware, collectively reflect the capability of the code generation methods.

Aiming at improving these metrics,
existing approaches~\cite{thakur2023verigen, lu2024rtllm, liu2023verilogeval} fine-tune pre-trained LLMs on the downstream task with optimized sampling schemes. 
These LLMs are mainly pre-trained using software programming languages.
\textit{RTLFixer}~\cite{tsai2023rtlfixer} introduces an automated framework that adopts retrieval-augmented generation~\cite{lewis2020retrieval} and ReAct prompting~\cite{yao2022react} to enable LLM-assisted debugging for RTL designs.
\textit{LLM-VeriPPA}~\cite{llm-verippa} enhances the code generation of RTL using a two-stage refinement process to progressively improve syntax, functionality, and hardware performance.
However, these approaches focus on low-level hardware languages instead of HLS. In this work, we take the first step to investigate the HLS code generation with LLM. 
Since \textit{HLS} shares similar semantics and syntax with programming languages commonly used during LLM pre-training,
this paper explores whether \textit{HLS} is better than low-level hardware languages for automated hardware design generation.
Although code generation with feedback and CoT prompting is not new in the literature of coding language models,
our experimental results, insights, benchmark, and evaluation infrastructure specific to LLM-assisted \textit{HLS} design offer valuable contributions to the future development of automated hardware generation. 

