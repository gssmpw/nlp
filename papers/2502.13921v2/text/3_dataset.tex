
\section{HLS Generation Benchmark}
\label{subsec:benchmark}


\subsection{Format of Design Points}\label{subsec:benchmark_format}
Following the practices of \textit{Python} benchmark HumanEval~\cite{chen2021evaluating} and \textit{Verilog} dataset VerilogEval~\cite{liu2023verilogeval},
each design point has three components: \textit{i)} user instruction prompts, \textit{ii)} design descriptions and \textit{iii)} reference designs. 
\figref{fig:dataset_template} shows the standardized format template,  with each data point stored as a \textit{JSONL} following the Alpaca format.
The default user instruction prompt is \textit{Generate HLS code with the following instructions:}, which can be enhanced using the chain-of-thought (COT) prompting technique as detailed in ~\secref{subsec:hwgen_cot}.

\begin{figure}[bt]
\centering
\includegraphics[width=60mm]{./figures/dataset_template.pdf}
\vspace{-2mm}
\caption{Template of design points.}
\vspace{-5mm}
\label{fig:dataset_template}
\end{figure}


\subsection{Dataset Collection}\label{subsec:benchmark_dataset}

We collect $52$ \textit{HLS}-based designs from open-source repositories, including \textit{HLSyn}~\cite{bai2023towards}\footnote{\url{https://github.com/UCLA-DM/HLSyn}} and \textit{ML4Accel}\footnote{\url{https://github.com/UT-LCA/ML4Accel-Dataset}}.
These designs are split into training and testing sets at a 4:1 ratio and fall into five categories:
\begin{itemize}[leftmargin=*]
    \item \textbf{Matrix and Linear Algebra Operations:} Includes sparse matrix-vector multiplications, dense matrix-matrix multiplication, array transformation and stencil computations.
    \item \textbf{Scientific Simulations:} Methods for solving physical and mathematical problems such as heat distribution and electromagnetic simulations.
    \item \textbf{Statistical Computations:} Calculations of statistical metrics from datasets.
    \item \textbf{Iterative Methods:} Techniques for solving equations using iterative approaches.
    \item \textbf{Other Computational Kernels:} Specialized computational tasks like molecular dynamics and interactions, encryption algorithms, and optical flow.
\end{itemize}
Each of these $52$ designs is associated with different combinations of programs such as \texttt{PIPELINE}, \texttt{PARALLEL} and \texttt{TILE}.
We filter out the \textit{HLS} programs that are invalid,
resulting in a collection of over $42,000$ \textit{HLS} programs. The whole dataset is split into training and test sets for supervised fine-tuning and evaluation, respectively.

\subsection{Generation of Design Description}\label{subsec:benchmark_desciption}
Given that the dataset encompasses over 42,000 \textit{HLS} programs, manually generating design descriptions for each program is both labor-intensive and time-consuming. 
Inspired by both HumanEval~\cite{chen2021evaluating} and \textit{Verilog},
we utilize \textit{ChatGPT} (version $3.5$ and $4$) to automate the creation of design descriptions for the datasets. 
We append each \textit{HLS} program with this base prompt when utilizing \textit{ChatGPT} to generate the corresponding design descriptions.
Both the reference design and its generated description are stored in \textit{JSON} format, adhering to the structure outlined in~\secref{subsec:benchmark_format}. This method ensures streamlined and consistent documentation of design descriptions across the dataset.
Following the practice of~\cite{liu2023verilogeval}, we provide two versions of prompts for each \textit{HLS} program in the test set: \textit{MachineGen} and \textit{HumanRefine}.
The \textit{MachineGen} version comprises prompts and instructions generated by GPT-based models without human modifications. In contrast, the \textit{HumanRefine} includes manually refined prompts to ensure more concise and human-like instructions.

\subsection{Assessment Infrastructure}\label{subsec:benchmark_syntax}

We provide evaluation infrastructure for both syntax and functionality.
For syntax verification, we use the GCC compiler with the "\texttt{-fsyntax-only}" option. It allows us to verify the syntax without the overhead of compiling the code, thereby enhancing time and space efficiency.
Regarding functionality correctness, we design unit tests tailored for each example in the test dataset. Each test case is associated with its corresponding `source\_file`. To achieve this goal, we modified the original test JSONL file to add another attribute `source\_file`. 
The testing process involves executing both the generated code and the original source code to compare their outputs. 
For instance, if the outputs from both codes consist of matrices, we conduct a targeted comparison.
This is done by selecting specific positions within the matrices from both outputs at random and verifying if they match.
