\begin{figure*}[htb]
\centering
\includegraphics[width=125mm]{./figures/overview_framework.pdf}
\vspace{-3mm}
\caption{An overview of our proposed framework.}
\vspace{-3mm}
\label{fig:overview}
\end{figure*}


\section{Automated Hardware Generation}\label{subsec:hwgen}


\subsection{Framework Overview}\label{subsec:hwgen_overview}
An overview of our proposed framework is depicted in~\figref{fig:overview}.
The framework comprises two main stages: \textit{i)} model fine-tuning and \textit{ii)} iterative code generation. The final output is an HLS-based program that can be synthesized into the corresponding hardware design. While focused on \textit{Vivado-HLS}, our framework is adaptable to any HLS language with appropriate datasets.
% for fine-tuning and evaluation.

In the model fine-tuning stage, our framework initiates by retrieving coding LLMs from open-source repositories, such as \textit{Code-Llama}\footnote{https://huggingface.co/codellama} and \textit{Start-Coder}\footnote{https://huggingface.co/blog/starcoder}.
Then, supervised fine-tuning is conducted on these pre-trained models using the HLS training data~(\secref{subsec:benchmark_dataset}).
We adopt \textit{axolotl}\footnote{https://github.com/OpenAccess-AI-Collective/axolotl} to perform the fine-tuning, allowing for customization of models and training parameters, such as learning rate, batch size, and epoch number to fit specific scenarios and available resources.

In the second stage,
we employ the fine-tuned LLM for iterative code generation.
The process begins with initial inputs consisting of user instruction prompts and design descriptions. To enhance the quality of the generated \textit{HLS} designs, we incorporate a chain-of-thought optimization technique~(\secref{subsec:hwgen_cot}) into the instruction prompts.
The code generation then proceeds with a feedback loop (\secref{subsec:hwgen_loop}) that iteratively improves the correctness of the \textit{HLS} designs.
This iterative process continues until a refined \textit{HLS} program is generated as the final output. 
Users can specify the number of iterations, providing the flexibility to navigate this trade-off according to their specific needs, with more iterations typically yielding higher quality at increased runtime expense.

\begin{figure}[t]
\centering
\includegraphics[width=70mm]{./figures/CoT.pdf}
\vspace{-3mm}
\caption{Chain-of-thought prompts for HLS generation.}
\vspace{-3mm}
\label{fig:cot}
\end{figure}


\subsection{Chain-of-Thought}\label{subsec:hwgen_cot}
Previous studies indicate that the quality of generated content is significantly influenced by the instructional prompt~\cite{zhao2021calibrate}.
The Chain-of-Thought (CoT)~\cite{wei2022chain} technique has proven simple and effective for enhancing the performance of LLMs across a wide range of tasks, including arithmetic, commonsense, and symbolic reasoning~\cite{wei2022chain}.
Although initially, COT yielded a modest $0.82$ point increase in the pass@1 metric in code generation,  this improvement was substantially augmented through structured prompting~\cite{li2023structured}.
In this paper, we investigate the effect of CoT in generating \textit{HLS}-based hardware designs. 

\figref{fig:cot} illustrates the CoT prompt structured for HLS code generation. The prompt guides a systematic approach through several targeted steps:  understanding FPGA characteristics, defining program structure, developing logic,  selecting data types and interfaces, before finalizing the code. This structured process ensures thorough consideration of each key aspect to optimize the HLS code generation.

\subsection{Two-Step Feedback Loops}\label{subsec:hwgen_loop}

Code generation with feedback loop has shown promising results in previous literature~\cite{liu2023rltf, shojaee2023execution, wang2022compilable, llm-verippa}.
This paper investigates its impact on \textit{HLS} code generation using a two-step feedback loop tailored for automated hardware generation, focusing on \textit{HLS}-related feedback.
At each iteration, the framework evaluates the syntax and functional correctness of the generated \textit{HLS} program. 
The located syntax error and functional defects are then fed back into the input prompts as additional information for subsequent code generation.

In the first step,  syntax feedback is provided using the GCC compiler with the `\texttt{-fsyntax-only}` option, as specified in \ref{subsec:benchmark_syntax}.
It captures the syntax errors without fully compiling the code. It allows rapid identification including the types and locations, and precise error mapping for targeted corrections. 
If the syntax check passes, our framework proceeds to the second step by executing predefined unit tests to compare the outputs of the generated and the original source code. 
Functional defects are recorded and added to the prompts for the next iteration. This two-step feedback loop continues for user-specified iterations, providing flexibility to balance the trade-off between design quality and runtime cost.




