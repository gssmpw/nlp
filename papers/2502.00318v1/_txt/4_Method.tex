



%\section{When Physics-Informed Neural Networks Meet State Space Models}

%\subsection{Continuous-Discrete Mismatch}

%\subsection{Sampling Continuous Dynamics in Sequence}

%PDE to infinite-dimensional ODE

%\subsection{Modeling Sequence with State Space Models}

%infinite-dimensional ODE to SSMs

%\subsection{Linear Time Variant}





%\clearpage

\vspace{-1mm}

\section{Combating Failure Mode with State-Space Model and Sub-sequential Alignment}
\label{sec:ssmsub}


    To address the problems in Section~\ref{sec:fail}, we propose (1) a discrete state-space-based encoder that models the sequences of individual collection points in continuous dynamics, to match with \textit{Continuous-Discrete Mismatch}, and propagates the information from the initial condition to subsequent times (Section~\ref{sec:ssm}).  and (2) a sub-sequence contrastive alignment mechanism that aligns different outputs of the same collection point in different sub-sequences, to form an agreement that eliminates simplicity bias (Section~\ref{sec:subseq}).
    
\vspace{-2mm}


\subsection{Continuous Time Propagation of Initial Condition Information with State Space Model}
\vspace{-1mm}
\label{sec:ssm}
As we discussed in Section~\ref{sec:fail}, the \textit{Continuous-Discrete Mismatch} of PINNs raises the intrinsic difficulty of modeling, since the time dependency in a dynamic PDE system is not captured spontaneously by discrete sampling. 
    We argue that such a dynamic time dependency can be modeled by SSM. 
To this end, we first consider the PDE as a spatially infinite-dimensional ODE to simplify the problem. We view the solution $u_\theta(x,t)$ in a function space that, if we let:
\begin{equation}
    U(t) := u_\theta(\cdot,t),
\end{equation}
be a function $x \to u_\theta(x,t)$, by $M$-point spatial sampling:
\begin{equation}
    U_i(t) := u(x_i,t),
\end{equation}
\begin{equation}
    \mathbf {u}(t) = \left[U_1(t),U_2(t),\cdots,U_M(t) \right]^\top .
    \label{}
\end{equation}

\textbf{Sequential Modeling Continuity with SSM.}
In continuous time, we now model the function $\mathbf {u}(t)$ to the dynamic system described by SSM as in Eq.~\ref{equ:hiddenssm} and~\ref{equ:outputssm}. Here we let $\mathbf x(t) = \text{Embed}(x,t)$, where $\text{Embed}(\cdot)$ is the Spatio-Temporal Embedding in Fig~\ref{fig:main}. After temporal discretization $\mathbf u_k=\mathbf u(k\Delta t),\mathbf h_k=\mathbf h(k\Delta t)$ and $\mathbf x_k=\mathbf x(k\Delta t)$, we get: 
\begin{equation}
    \mathbf u_k=C\bar A^k \mathbf h_0 + C\sum_{i=0}^k\bar A^{k-i}\bar B\mathbf x_i.
    \label{equ:ssmu}
\end{equation}
Reversibly, by the inverse of the discretization rule defined by Eq.~\ref{equ:disc1},~\ref{equ:disc2}, we can restore this temporal dependency to continuous time. This kind of restoration can help achieve PINN's generalization to any moment in $[0, T]$. 

\textbf{Pattern Propagation by Joint Optimization.}
Combine Eq.~\ref{equ:lossequ} with~\ref{equ:ssmu}, in a sequence start with $t=0$, the sum of loss of collection points at time $k\Delta t$, would be: 
\begin{align}
  &\sum_{i=1}^M \mathcal L_\mathcal F(u(x_i,k\Delta t)) =  \frac{1}{M}\mathcal \|\mathcal F( \mathbf 1_M\cdot \mathbf{u}_k)\Arrowvert^2\nonumber\\&=\frac{1}{M}\|\mathcal F\left(\mathbf1_M\cdot(C\bar A^k \mathbf h_0 + C\sum_{i=0}^k\bar A^{k-i}\bar B\mathbf x_i)\right)\Arrowvert^2,
  \label{equ:timeloss}
\end{align}
where $1_M=[1,1,\cdots,1] \in \mathbb R^M$. In Eq.~\ref{equ:timeloss}, we notice that the $\mathbf h_0$ should satisfy both the initial condition and the equation by jointly optimizing the losses:
\begin{align}\label{equ:loss0equ}
   \mathcal L_\mathcal F(\mathbf u_0) =  \frac{1}{M}\|\mathcal F\left(\mathbf1_M\cdot(C \mathbf h_0 )\right)\Arrowvert^2;\\
   \mathcal L_\mathcal I(\mathbf u_0) =  \frac{1}{M}\|\mathcal I\left(\mathbf1_M\cdot(C \mathbf h_0 )\right)\Arrowvert^2 .
   \label{equ:loss0init}
\end{align}
Thereby, for each collection point, the numerical value of its solution should be jointly optimized by Eq.~\ref{equ:timeloss},~\ref{equ:loss0equ}, and~\ref{equ:loss0init}, thus receiving the pattern defined by the initial conditions.


\textbf{Uniformed Derivatives Scale.} Another benefit that can be got from SSM is, by parameterizing differential state matrix $A$ in Eq.\ref{equ:hiddenssm} with HiPPO matrix~\cite{gu2020hippo} which contains the derivative information,  we can align the derivatives of the system with respect to time on a uniform scale. This uniform scale will help to reduce the problem of ruggedness on the loss landscape due to gradient vanishing or exploding.

\textbf{Time-Varying SSM.} In practice, we use the time-varying Selective SSM~\cite{gu2023mamba}, instead of the function defined by Eq.~\ref{equ:ssmu} being the SSM on a linear time-invariant system. The time-varying SSM has two advantages, one is that such input-dependent models typically have stronger representational capabilities~\cite{xu2024infinite}, while the other is that it will make diverse predictions that help to eliminate simplicity bias in the model, as we will discuss in section~\ref{sec:subseq}. This time-variance will make $(\bar A,\bar B, C)$ time-dependent, and therefore, Eq.~\ref{equ:ssmu} and \ref{equ:timeloss} need minor adjustments. These adjustments won't impact the initial condition propagation, and we will discuss them in Appendix~\ref{apx:LTI}.





\subsection{Eliminating Simplicity Bias of Models with Sub-Sequence Contrastive Alignment}
\label{sec:subseq}


\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{_fig/fig4}
    \vspace{-5mm}
    \caption{Comparison of Sequence Granularity}
    \label{fig4}
    \vspace{-6mm}
  %  \vspace{-1mm}
\end{figure}





Although SSM can make the information about the initial conditions propagate in time coordinates, it still cannot mitigate the simplicity bias of neural networks. 
    The model is still prone to falling into an over-smoothed local optimum. 
        There are two key points to address this over-smoothness caused by simplicity bias: (1) appropriate sequence granularity to guarantee a smooth optimization process. (2) Mitigating the effect of simplicity bias through the diversity of model prediction paradigms~\cite{pagliardiniagree}. 
        
        \textbf{Sequence Granularity.} A proper sequence granularity ensures smooth propagation of the initial conditions while making the model easier to optimize. As shown in Fig.~\ref{fig4}, there are three ways to define sequence, which are pseudo sequence~\cite{zhao2024pinnsformer}, long sequence~\cite{nguyen2024sequence}, and the proposed sub-sequence.
        We propose to use a sub-sequence with medium granularity overlapping. The sub-sequential modeling can avoid: (1) the difficulty of crossing the loss barrier that makes the model trapping in the over-smooth local optimum, which is caused by the huge inertia of long sequence; (2) the difficulty of broadcasting information globally on the time coordinate, that caused by construct on small neighborhoods of a collection point in pseudo sequence. Sub-sequence takes only the first output in the sequence as the output value of the current collection point. Its successors' values will pass information crossing the time coordinate through subsequences alignment and form diverse predictions to eliminate simplicity bias.

\textbf{Contrastive Alignment for Information Propagation.} As shown in Fig.~\ref{fig4}, we construct a sub-sequence for each collection point together with its finite successors, which form overlapping collection points. By aligning the predictions of these collection points with a contrastive loss, each collection point becomes a soft relay of the pattern. Thus, it forms the propagation of patterns in the whole time domain.%$[0,T]$.
%In this way, we can realize a filtered propagation of pattern across sub-sequences by recursive alignment, 


\textbf{Eliminating the Simplicity Bias.} Previous work~\cite{teney2022evading,pagliardiniagree} has pointed out that the agreement obtained from diverse predictions is the key to eliminating the effects of simplicity bias. We argue that this agreement from diverse predictions is naturally obtained in the sub-sequence alignment. This is because the fact that,
    since the SSM we constructed in section~\ref{sec:ssm} is time-varying and a collection point will be at different time coordinates in different sub-sequences, the predictions for this collection point are naturally diverse. And we force these diverse predictions to arrive at a consensus by contrastive alignment.


      %  If we model a PDE with an overly long sequence, when trapped in an oversmoothed local optimum, decreasing the loss of a point at one moment may increase the loss of all other points on the sequence, which causes the model to have a huge inertia when optimizing.
%The large inertia caused by long sequences makes it difficult for the model to cross the loss barrier. At the other extreme, a sequence that is too small may make it difficult to propagate the time dependency. The pseudo sequence can only reflects temporal information on small neighborhoods of a single collection point, instead of broadcasting the information globally on time coordinate.


\section{PINNMamba}

In conjunction with the high-level ideas described in Section~\ref{sec:ssmsub}, in this section, we present PINNMamba, a novel physics-informed learning framework that effectively combats the failure modes in the PINNs.


\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{_fig/conv}
    \vspace{-8mm}
    \caption{The ground truth solution, prediction (top), and absolute error (bottom) on convection equations.}
    \label{fig:conv}
    \vspace{-5mm}
  %  \vspace{-1mm}
\end{figure*}



\textbf{Sub-Sequential I/O.} As shown in Fig.~\ref{fig:main}, PINNMamba first samples the grid of collection points over the entire spatio-temporal domain bounded by the PDE. We assume that the grid picks $M$ spatial coordinates and $N$ temporal coordinates, and denote the temporal sampling interval as $\Delta t = T/(N-1)$. For a collection point $(x,t)$, we construct a sequence $X(x,t)$ with its $k-1$ temporal successors:  
\begin{equation}
    X(x,t) = \{(x,t),(x,t+\Delta t),\cdots,(x,t+(k-1)\Delta t)\}.
\end{equation}
PINNMamba takes such $M\times N$ sequences as the input of models. 
    For each sequence $X(x,t)$, PINNMamba computes a sub-sequence prediction $\{\bar u_\theta^t (x,t),\bar u_\theta^t (x,t+\Delta t),\cdots,\bar u_\theta^t (x,t+(k-1)\Delta t)\}$ corresponding to every collection point in the sequence, where $\bar u_\theta^t (x,t+i\Delta t)$ denote the tentative prediction of collection point $(x,t+i\Delta t)$ in a sequence start with time $t$. The $\bar u_\theta^t (x,t)$ will be taken as the output of collection point $(x,t)$ and the rest of the sequence will be used to construct the sub-sequence contrastive alignment loss we will discuss later in Section~\ref{sec:subseq}. The residual losses of the model w.r.t the sub-sequence will be:
    
\vspace{-5mm}

    \small{
    \begin{equation}
    \mathcal L_{\mathcal F}^\text{seq}(u_\theta)= \frac{1}{k|\chi|}\sum_{(x_i,t_i)\in \chi}\sum_{j=0}^{k-1}\|\mathcal F(u_\theta^{t_i}(x_i,t_i+j\Delta t)\|^2;
        %\mathcal L_{\mathcal F}(u_\theta)= \frac{1}{k|\chi|}\sum_{i=1}^{|\chi|}\sum_{j=0}^{k-1}\|\mathcal F(u_\theta^{t_i}(x_i,t_i+k\Delta t)\|^2;
    \label{equ:lossequseq}
\end{equation}
\begin{equation}
    \mathcal L_{\mathcal I}^\text{seq}(u_\theta)= \frac{1}{k|\chi_0|}\sum_{(x_i,t_i)\in \chi_0}\sum_{j=0}^{k-1}\|\mathcal I(u_\theta^{t_i}(x_i,t_i+j\Delta t)\|^2;
    \label{equ:lossinitseq}
\end{equation}
\begin{equation}
    \mathcal L_{\mathcal B}^\text{seq}(u_\theta)= \frac{1}{k|\partial\chi|}\sum_{(x_i,t_i)\in \partial\chi}\sum_{j=0}^{k-1}\|\mathcal B(u_\theta^{t_i}(x_i,t_i+j\Delta t)\|^2.
    \label{equ:lossboundseq}
\end{equation}
}
%As shown in Fig.~\ref{fig:main}, PINNMamba 
\normalsize
\vspace{-5mm}
%\textbf{Sub-Sequence of Collection Points.}

\textbf{Model Architecture.} As shown in Fig.~\ref{fig:main}, PINNMamba employs an encoder-only architecture, which encodes fixed-size input sub-sequence into a sub-sequence prediction with the same length. First, for each token in the sequence, an MLP-based Spatio-Temporal Embedding layer first embeds the $(x,t)$ coordinates into high-dimensional representation. The embeddings will be sent to a Mamba-based encoder, which consists of several PINNMamba blocks. 

The PINNMamba block employed here consists of two branches: (1) the first is a stack of a linear projection layer, a 1d-convolution layer, a Wavelet activation~\cite{zhao2024pinnsformer}, and an SSM layer with parallel scan~\cite{gu2023mamba}; (2) the second is a stake of a linear projection layer and a Wavelet activation. The two branches are then connected with an element-wise multiplication, followed by another linear projection and residual connection. With input $X^l$,
the PINNMamba block can be formulated as: 
\begin{equation}
    X_1^l = \text{SSM}(\sigma(\text{Conv}(W_aX^l)));
\end{equation}
\vspace{-5mm}
\begin{equation}
    X_2^l = \sigma(W_bX^l);
\end{equation}
\vspace{-5mm}
\begin{equation}
    X^{l+1} = X^l+W_c(X_1^l\otimes X_2^l),
\end{equation}
where $\sigma(x)=\omega_1\sin(x)+\omega_2\cos(x)$ is Wavelet activation function~\cite{zhao2024pinnsformer}, in which $\omega_1,\omega_2$ are learnable. $\otimes$ denotes an element-wise multiplication. 

\textbf{Sub-Sequence Contrastive Alignment.} PINNMamba predicts the same collection multiple times in different subsequences. For example, the collection point $(x,k+\Delta t)$ appears on sequences from $X(x,t+\Delta t)$ to $X(x,t+k\Delta t)$. We align the predictions on these subsequences to make the information defined by the initial conditions propagate over time. To do this, for each subsequence, we design a contrastive loss with the last subsequence for alignment:
\begin{align}
        \mathcal L_\text{alig}(u_\theta)= \frac{1}{(k-1)|\chi|}&\sum_{(x_i,t_i)\in \chi} \sum_{j=1}^{k-1} \Big[u_\theta^{t_i}(x_i,t_i+j\Delta t)\nonumber\\&-u_\theta^{t_i+\Delta t}(x_i,t_i+j\Delta t)\Big]^2.
\end{align}
\normalsize

Thus, the empirical loss for PINNMamba is defined as:
\small
\begin{equation}
     \mathcal L(u_\theta)=\lambda_{\mathcal F}\mathcal L_{\mathcal F}^\text{seq}(u_\theta)+\lambda_{\mathcal I}\mathcal L_{\mathcal I}^\text{seq}(u_\theta)+\lambda_{\mathcal B}\mathcal L_{\mathcal B}^\text{seq}(u_\theta)+\lambda_\text{alig}\mathcal L_\text{alig}(u_\theta).
\end{equation}
\vspace{-8mm}
    

\normalsize