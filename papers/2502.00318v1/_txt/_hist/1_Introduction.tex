\section{Introduction}

In the past few years, Physics-Informed Neural Networks~(PINNs)~\cite{raissi2019physics} have emerged as a novel approach for numerically solving partial differential equations~(PDEs). 
    PINN takes a neural network $u_{\theta}(x,t)$, whose parameters $\theta$ are trained with physics PDE residual loss, as the numerical solution $u(x,t)$ of the PDE, where $x$ and $t$ are spatial and temporal coordinates. 
        The core idea behind PINNs is to take advantage of the universal approximation property of neural networks~\cite{hornik1989multilayer} and automatic differentiation implemented by mainstream deep learning frameworks, such as PyTorch~\cite{paszke2019pytorch} and Tensorflow~\cite{abadi2016tensorflow}, 
            so that PINNs can achieve potentially more precise and efficient PDE solution approximation compared with traditional numerical approaches like finite element methods~\cite{reddy1993introduction}.

Current mainstream PINNs use multilayer perceptrons (MLPs) as their backbone network. 
    However, despite the universal approximation of the MLPs, an arbitrarily accurate approximation to the numerical solution of the PDE is not always guaranteed to be learned correctly. 
This has resulted in the observation of multiple failure modes in PINNs, especially when approximating high-frequency or complex patterns~\cite{krishnapriyan2021characterizing}.
As shown in Fig.~\ref{fig1}, these failure modes of PINNs exhibit a gradual distortion over time. 
    This is because, during training, the propagation of the physical pattern information defined by the initial conditions relying only on the gradient-based reconstruction among limited discrete time collection points, rather than a direct sequence-like propagation. 
        Without built-in inductive bias, MLPs fail to capture such information propagation over time.

%The temporal distortion can be attributed to the fact that PINNs assume temporal continuity (or even differentiability), whereas during their actual training, the spatio-temporal points used to construct the empirical PDE residual loss are sampled discretely. 
    
%        And due to the non-built-in inductive bias nature of MLPs, they do not have the ability to make information transmit over time.





\begin{figure}[t!]
    \centering
    \includegraphics{_fig/Fig1.pdf}
%    \vspace{-3mm}
    \caption{PINN gradually distorts over time.}
    \label{fig1}
    \vspace{-3mm}
  %  \vspace{-1mm}
\end{figure}

To address this problem, sequence-to-sequence learning~\cite{sutskever2014sequence} is proposed as the inductive bias for modeling time-dependent equations' solution~\cite{krishnapriyan2021characterizing}.
    But \citet{sutskever2014sequence}'s method requires training a new network at every time step, which significantly increases the computation and memory overhead.
To make matters worse, we can't simply use recurrent neural networks for sequence-to-sequence learning to solve such failures. Since PINNs often involve higher order differentiation, the problem of gradient vanishing inherent in recurrent models~\cite{hochreiter1998vanishing} will be exacerbated, indicating that recurrent long-sequence modeling is not a good inductive bias either. PINNsFormer~\cite{zhao2024pinnsformer} attempts to use the Transformer model, but its construction of a pseudo-sequence within a small neighborhood of sampled time points does not really reflect the propagation of sequence information between time collection points. Thus, there is still an open question:
%Chenhui: 这个问题和上面这段话的衔接并不够紧密，上一段话在强调seq2seq，为什么突然跳到inductive bias. 但是inductive bias是引出后面分析的keyword，怎么解决这个问题。
\vspace{-2mm}
\begin{center}
\textit{What kinds of Inductive Bias should be introduced into Physics-Informed Neural Networks? And how?}
\end{center}
\vspace{-2mm}

 First, we remark that simply modeling the sequences over entire time intervals is unreliable, since PINNs are not data-driven, that their supervision is not a direct mapping from a data point sequence to a label sequence but residuals constructed from differential equations. In this physical law-driven learning paradigm, governing in late stages of a long sequence is ineffective, because there is a wide solution space in which functions can all make residual loss low but such functions lack information from previous time. 
 
For exerting the influence of the initial conditions, short sequence modeling might be a useful inductive bias. Base on this, we propose a learning scheme with overlapping short sub-sequences. First, it constructs a sub-sequence at each time collection point to predict the output at the current collection point and the next few collection points. Then, at the next collection point, the output of model is softly aligned with previous predictions, as a way to propagate the information defined by the initial conditions over time.

We then remark that PINNs have a \textit{Continuous-Discrete Duality} nature. 
    PINNs assume temporal continuity (or even high-order differentiability), whereas during their actual training, the spatio-temporal collection points used to construct the empirical PDE residual loss are sampled discretely. 
So for the successful training of PINNs, we need ensure the computability of the differentiation for time coordinates, i.e., the scale of the gradient is aligned without vanishing or explosion. 
    Meanwhile, it is necessary to ensure that the model has the ability to propagate information directly over discrete time points, i.e., the ability to sequential modeling.

To be consistent with this \textit{Continuous-Discrete Duality}, we propose to use the State Space Models (SSM)~\cite{gu2023mamba,gu2022efficiently}, which effectively utilizes discrete time collection points to construct behavior of continuous dynamic systems. 
    In SSMs, there is a smooth articulation between its discrete-time form and continuous-time essence. Moreover, the differentiation of SSM for time is realized by multiplication between state transfer matrices, which effectively aligns the scales of the differentiation of each order. Therefore, it can avoid the PINNs' inherent difficulty of optimization due to the inconsistency of gradient scales.

Therefore, we devised a novel learning framework for physics PDE's numerically solving, named PINNMamba, that performs time sub-sequences modeling with the Selective SSMs (Mamba)~\cite{gu2023mamba}. PINNMamba successfully captures the temporal dependence within the PDE when training the continuous dynamical systems with discretized collection points. Our approach eliminates distortions due to discrete-time sampling in PDE's neural modeling of continuous systems. Our experiments show that PINNMamba outperforms other PINN approaches
like PINNsFormer~\cite{zhao2024pinnsformer} and QRes~\cite{bu2021quadratic} on multiple hard problems, achieving a new state-of-the-art.

\textbf{Contributions.} We make the following contributions:
\vspace{-2mm}
\begin{itemize}
    \item We reveal that the mismatch between the discrete nature of the training collection points and the continuous nature of the function approximated by the PINN is an important factor in causing distortion of the propagation of the initial condition over time in the PINN.
    \item We reveal that, for PINNs, the lack of direct supervision of their data-label and the multitude of functions conforming to a single differential equation creates an intrinsic difficulty in modeling long sequences.
    \item We propose PINNMamba which eliminates the discrete-continuity mismatch and sub-sequentially models the systems, resulting the state-of-the-art performance on several PINN benchmarks.
\end{itemize}


 %   Recent work suggests that such failure modes are not due to the expressive capability limitations of the model, but rather that the loss of the PINN modeled by the MLP is difficult to optimize by backpropagation~\cite{krishnapriyan2021characterizing,zhao2024pinnsformer}.
    
    %However, recent works show some possible failure modes in PINNs when they solve some relative complex problem~\cite{krishnapriyan2021characterizing}.
%When we look back at the history of deep learning, we find that similar problems had arisen in Computer Vision and Natural Language Modeling. 
 %   MLPs have also faced the dilemma of limited final performance due to the difficulty of loss descents for modeling images and natural language.
  %      As an alternative, a series of new neural architectures such as convolutional neural networks~\cite{krizhevsky2012imagenet}, recurrent neural networks~\cite{pascanu2013difficulty} have been proposed.
   %         These network architectures leverage the translation invariance of images, as well as the sequential nature of language, to introduce \textit{Inductive Bias} for different tasks to build more easily optimizable losses and therefore achieve better performance.
    %This leads to a fundamental question:



%To answer this question, we begin with an analysis of the characteristics of PINN. First, since PINN is modeling a continuous dynamical system, it should by default satisfy differentiability with respect to space and time. Second, the computation of the training empirical loss of PINN is done by discrete sampling in space and time coordinate system.

