\section{Preliminary}
Due to page limit, we discuss related works in Appendix~\ref{apx:rw}.


\textbf{Physics-Informed Neural Networks.}
The PDE systems that are defined on spatio-temporal set $\Omega \times [0,T] \subseteq  \mathbb R^{d+1}$ and described by equation constraints, boundary conditions, and initial conditions can be formulated as:
\vspace{-2mm}
\begin{equation}
    \mathcal F(u(x,t)) = 0,\forall (x,t)\in\Omega\times[0,T];
\end{equation}
\begin{equation}
    \mathcal I(u(x,t)) = 0,\forall (x,t)\in\Omega\times\{0\};
\end{equation}
\begin{equation}
    \mathcal B(u(x,t)) = 0,\forall (x,t)\in\partial\Omega\times [0,T],
\end{equation}
where $u:\mathbb R^{d+1}\rightarrow \mathbb R^m$ is the solution of the PDE, $x\in\Omega$ is the spatial coordinate, $\partial\Omega$ is the boundary of $\Omega$, $t \in [0,T]$ is the temporal coordinate and $T$ is the time horizon. The
$\mathcal F,\mathcal I, \mathcal B$ denote the operators defined by PDE equations, initial conditions, and boundary conditions respectively.

A physics-driven PINN first builds a finite collection point set $\chi \subset \Omega\times[0,T]$, and its spatio (temporal) boundary $\partial\chi \subset \partial\Omega\times[0,T]$ ($\chi_0 \subset \Omega\times\{0\}$), then employs a neural network $u_\theta(x,t)$ which is parameterized by $\theta$ to approximate $u(x,t)$ by optimizing the residual loss as defined in Eq.~\ref{equ:loss}:
\vspace{-2mm}
\begin{equation}
    \mathcal L_{\mathcal F}(u_\theta)= \frac{1}{|\chi|}\sum_{(x_i,t_i)\in \chi}\|\mathcal F(u_\theta(x_i,t_i)\|^2;
    \label{equ:lossequ}
\end{equation}
\begin{equation}
    \mathcal L_{\mathcal I}(u_\theta)= \frac{1}{|\chi_0|}\sum_{(x_i,t_i)\in \chi_0}\|\mathcal I(u_\theta(x_i,t_i)\|^2;
    \label{equ:lossinit}
\end{equation}
\begin{equation}
    \mathcal L_{\mathcal B}(u_\theta)= \frac{1}{|\partial\chi|}\sum_{(x_i,t_i)\in \partial\chi}\|\mathcal B(u_\theta(x_i,t_i)\|^2;
    \label{equ:lossbound}
\end{equation}
\begin{equation}
    \mathcal L(u_\theta)=\lambda_{\mathcal F}\mathcal L_{\mathcal F}(u_\theta)+\lambda_{\mathcal I}\mathcal L_{\mathcal I}(u_\theta)+\lambda_{\mathcal B}\mathcal L_{\mathcal B}(u_\theta),
    \label{equ:loss}
\end{equation}
%where $\chi$,$\chi_0$, and $\partial \chi$ are sets of spatio-temporal collection points corresponding to $\Omega\times[0,T]$, $\Omega\times\{0\}$, and $\partial\Omega\times[0,T]$ respectively.
where $\lambda_\mathcal F$,$\lambda_\mathcal I$,$\lambda_\mathcal B$ are the weights for loss that are adjustable by auto-balancing or hyperparameters. $\|\cdot\|$ denotes $l^2$-norm.


\textbf{State Space Models.} An SSM describes and analyzes a continuous dynamic system. It is typically described by:
\begin{align}    \label{equ:hiddenssm}
   \mathbf {\dot h(t)} &= A\mathbf h(t) + B\mathbf x(t),\\
     \mathbf u(t) &= C\mathbf h(t) + D\mathbf x(t),
     \label{equ:outputssm}
\end{align}

where $\mathbf h(t)$ is hidden state of time $t$, $\mathbf {\dot  h}(t)$ is the derivative of $\mathbf h(t)$. $\mathbf x(t)$ is the input state of time $t$, $\mathbf u(t)$ is the output state, and $A,B,C,D$ are state transition matrices. 
    
    In real-world applications, we can only sample in discrete time for building a deep SSM model. 
        We usually omit the term $D\mathbf x(t)$ in deep SSM models because it can be easily implemented by residual connection \cite{he2016deep}. So we create a discrete time counterpart:
\begin{align}
    \mathbf h_k &= \bar A \mathbf h_{k-1}+\bar B \mathbf x_k,
\\
    \mathbf u_k &= C \mathbf h_k,
\end{align}
with discretization rules such as zero-order hold (ZOH):
\begin{align}\label{equ:disc1}
    \bar A &= \exp{(\mathrm{\Delta}A)},\\
    \bar B &= (\mathrm{\Delta}A)^{-1}( \exp{(\mathrm{\Delta}A)}-I)\cdot \mathrm{\Delta}B,
    \label{equ:disc2}
\end{align}
where $\bar A$ and $\bar B$ is discrete time state transfer and input matrix, and $\mathrm{\Delta}$ is a step size parameter. By parameterizing $A$ using HiPPO  matrix~\cite{gu2020hippo}, and parameterizing $(\Delta,B,C)$ with input-dependency, a time-varying Selective SSM can be constructed~\cite{gu2023mamba}. Such a Selective SSM can capture the long-time continual dependencies in dynamic systems. We will argue that this makes SSM a good continuous-discrete articulation for modeling PINN.
%从collection point loss构建的角度，说明初始条件的影响力与位置的关系。
\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{_fig/fig2}
    \vspace{-8mm}
    \caption{Failure mode of PINN on convection equation, the over-smooth solution brings the losses down to 0 almost everywhere.}
    \label{fig2}
    \vspace{-3mm}
  %  \vspace{-1mm}
\end{figure}

\section{Why PINNs present Failure Modes?}
\label{sec:fail}
%\section{Continuous-Discrete Mismatch of PINNs Can Result in Over-Smooth Failure Modes}

%\CX{The logic of this section is completely out of order and needs to be adjusted.}

A counterintuitive fact of PINNs is that the failure modes are not devoid of optimizing their residual loss to a very low degree.
As shown in Fig.~\ref{fig2}, for the convection equation, the converged PINN almost completely crashes in the domain, but its loss maintains a near-zero degree at almost every collection point. This is the result of the combined effects of the simplicity bias~\cite{shah2020pitfalls,pezeshki2021gradient} of neural networks and the \textit{Continuous-Discrete Mismatch} of PINNs, as shown in Fig.~\ref{fig5}. 
    The simplicity bias is the phenomenon that the model tends to choose the one with the simplest hypothesis among all feasible solutions, which we demonstrate in Fig.~\ref{fig5}~(b).
        \textit{Continuous-Discrete Mismatch} refers to the inconsistency between the continuity of the PDE and the discretization of PINN's training process.
As shown in Eq.~\ref{equ:lossequ} - \ref{equ:lossbound}, to construct the empirical loss for the PINN training process, we need to determine a discrete and finite set of collection points on $\Omega\times[0,T]$. 
This is usually done with a grid or uniform sampling. But a PDE system is usually continuous and its solutions should be regular enough to satisfy the differential operator $\mathcal F$, $\mathcal B$, and $\mathcal I$.

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{_fig/fig5}    \vspace{-3mm}
    \caption{The correct Pattern determined by the initial conditions faces two resistances in propagation: (a) the difficulty of propagating information directly through the gradient among discrete collection points, and (b) the need to fight against over-smoothed solutions with near-zero loss caused by simplicity bias.}
    \label{fig5}
    \vspace{-3mm}
  %  \vspace{-1mm}
\end{figure}
%We call this inconsistency between theory and practice \textit{Continuous-Discrete Mismatch}. 

%This \textit{Mismatch} is the essence of the difficulty of optimizing PINNs over complex patterns, since discrete sampling can make over-smoothed solutions that in some cases bring the PINN's empirical loss down to 0, but do not at all match the actual solution.  
\textbf{Continuous-Discrete Mismatch.} \textit{Continuous-Discrete Mismatch} will cause correct local patterns hard to propagate over the global domain.
Because the loss on discrete collection points does not necessarily respond to the correct pattern on the continuous domain, instead, only responds to its small neighborhood. %Without loss of generality, we consider the case where the output of $u$ is one-dimensional:
To show such \textit{Continuous-Discrete Mismatch}, we first present the following theorem:

\begin{theorem}\label{thm:continuous-discrete}
    Let $\chi^* = \{(x^*_1,t^*_1),\dots,(x^*_N,t^*_N)\}\subset \Omega\times[0,T]$. Then for differential operator $\mathcal M$ there exist infinitely many functions
$u_\theta : \Omega \to \mathbb{R}^m$ parametrized by $\theta$ , s.t.
$$ \mathcal{M}(u_\theta(x^*_i,t^*_i)) = 0 \quad \text{for } i=1,\dots,N,$$ $$ 
   \mathcal{M}(u_\theta(x,t)) \neq 0
   \quad \text{for a.e. } x \in \Omega\times[0,T] \setminus \chi^*.$$
\end{theorem}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{_fig/main}
    \vspace{-6mm}
    \caption{PINNMamba Overview. PINNMamba takes the sub-sequence as input which is a composite of several consecutive collection points on the time axis. For each sub-sequence, the prediction of the first collection point is taken as the output of PINNMamba, while the others are used to align the prediction of different sub-sequences, that can propagate information among time coordinates.}
    \label{fig:main}
    \vspace{-4mm}
  %  \vspace{-1mm}
\end{figure*}

 By Theorem~\ref{thm:continuous-discrete}, enforcing the PDE only at a finite set of points does not guarantee a globally correct solution. This can be performed by simply constructing a Bump function in a small neighborhood of points in $\chi^*$ so that it satisfies $\mathcal{M}(u_\theta(x^*,t^*)) = 0$ for $(x^*,t^*) \in \chi^*$. This means that the information of the equation determined by the initial conditional differential operator $\mathcal I$ may act only on a small neighborhood of collection points with $t = 0$. The other collection points in the $\Omega\times(0,T]$, on the other hand, might fall into a local optimum that can make $\mathcal L_{\mathcal F}(u_\theta)$ defined by Eq.~\ref{equ:lossequ} to near 0. 
 Because the function $u_{\theta}$ determined by $\mathcal F$ and $\mathcal I$ together on the collection points at $t = 0$ may not be generalized outside its small neighborhood. The detailed proof
of Theorem~\ref{thm:continuous-discrete} can be found in Appendix~\ref{apx:proof3_1}.
 
\textbf{Simplicity Bias.} Meanwhile, the simplicity bias of neural networks will make the PINNs always tend to choose the simplest solution in optimizing $\mathcal L_{\mathcal F}(u_\theta)$. This implies that PINN will easily fall into an over-smoothed solution. For example, as shown in Fig.~\ref{fig2}, the PINN's prediction is 0 in most regions. The loss of this over-smoothed feasible solution is almost identical to that of the true solution, and the existence of an insurmountable ridge between the two loss basins results in a PINN that is extremely susceptible to falling into local optimums. As in Fig~\ref{fig5}, the over-smoothed pattern yields an advantage against the correct pattern.

Under the effect of difficulty in passing locally correct patterns to the global due to \textit{Continuous-Discrete Mismatch} and over-smoothing due to simplicity bias, PINNs present failure modes. Therefore, to address such failure modes, the key points in designing the PINN models lie in: (1) a mechanism for information propagation in continuous time and (2) a mechanism to eliminate the simplicity bias of models.

