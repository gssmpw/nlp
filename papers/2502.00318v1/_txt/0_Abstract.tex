\begin{abstract}
%\CX{
%The current abstract is a placeholder, to be rewrite.
%The current abstract is GPT-generated, to be rewrite.}

 %  \CX{ PINNs often face challenges, such as failure modes when approximating complex patterns or high-frequency solutions. We identify the limitations of existing approaches, such as sequence-to-sequence learning and recurrent models, in addressing these challenges. To overcome these issues, we propose a novel framework, PINNMamba, which introduces short sub-sequence modeling using Selective State Space Models (Mamba). Our approach aligns with the continuous-discrete duality of PINNs, ensuring gradient consistency and enabling effective propagation of initial condition information over time. PINNMamba resolves distortions caused by discrete-time sampling in neural modeling of continuous systems, achieving state-of-the-art performance on various PINN benchmarks.} 
 Physics-Informed Neural Networks (PINNs) are a kind of deep-learning-based numerical solvers for partial differential equations (PDEs). Existing PINNs often suffer from failure modes of being unable to propagate patterns of initial conditions. We discover that these failure modes are caused by the simplicity bias of neural networks and the mismatch between PDE's continuity and PINN's discrete sampling. We reveal that the State Space Model (SSM) can be a continuous-discrete articulation allowing initial condition propagation, and that simplicity bias can be eliminated by aligning a sequence of moderate granularity. Accordingly, we propose PINNMamba, a novel framework that introduces sub-sequence modeling with SSM. Experimental results show that PINNMamba can reduce errors by up to 86.3\% compared with state-of-the-art architecture. 
 Our code is available at \url{https://github.com/miniHuiHui/PINNMamba}.

\end{abstract}
\vspace{-5mm}