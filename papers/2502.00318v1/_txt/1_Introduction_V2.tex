\section{Introduction}

In the past few years, Physics-Informed Neural Networks~(PINNs)~\cite{raissi2019physics} have emerged as a novel approach for numerically solving partial differential equations~(PDEs). 
    PINN takes a neural network $u_{\theta}(x,t)$, whose parameters $\theta$ are trained with physics PDE residual loss, as the numerical solution $u(x,t)$ of the PDE, where $x$ and $t$ are spatial and temporal coordinates. 
        The core idea behind PINNs is to take advantage of the universal approximation property of neural networks~\cite{hornik1989multilayer} and automatic differentiation implemented by mainstream deep learning frameworks, such as PyTorch~\cite{paszke2019pytorch} and Tensorflow~\cite{abadi2016tensorflow}, 
            so that PINNs can achieve potentially more precise and efficient PDE solution approximation compared with traditional numerical approaches like finite element methods~\cite{reddy1993introduction}.


%Current mainstream PINNs use multilayer perceptrons (MLPs) as their backbone network. 
 %   However, despite the universal approximation of the MLPs, an arbitrarily accurate approximation to the numerical solution of the PDE is not always guaranteed to be learned correctly. 
%This has resulted in the observation of multiple failure modes in PINNs, especially when approximating high-frequency or complex patterns~\cite{krishnapriyan2021characterizing}.
%As shown in Fig.~\ref{fig1}, %these failure modes of PINNs exhibit a gradual distortion over time.This is due to that MLPs lack \textit{Inductive Bias} that can effectively model time dependencies of a system, resulting in the failure of propagating physical information that is defined by the initial condition. 
%these failure modes exhibit a gradual temporal distortion of the solution. This degradation stems from MLPs' inherent lack of inductive bias for modeling time dependencies of a system, which impairs their ability to effectively propagate physical information defined by the initial condition.%constraints.


The mainstream PINNs predominantly employ multilayer perceptrons (MLPs) as their backbone architecture. However, despite the universal approximation capability of MLPs, they do not always guarantee the accurate learning of numerical solutions to PDEs in practice. This phenomenon is observed as the failure modes in PINNs, in which case PINN provides a completely wrong approximation~\cite{krishnapriyan2021characterizing}. As illustrated in Fig. \ref{fig1}, the failure modes often manifest as a temporal gradual distortion. This distortion arises because MLPs lack the necessary inductive bias to effectively capture the temporal dependencies of a system, ultimately hindering the accurate propagation of physical patterns informed by the initial conditions.

%Current mainstream PINNs employ multilayer perceptrons (MLPs) as their backbone. However, despite the universal approximation capabilities of MLPs, they do not always guarantee an arbitrarily accurate approximation of the numerical solution to PDEs. Consequently, multiple failure modes have been observed in PINNs, particularly when approximating high-frequency or complex patterns~\cite{krishnapriyan2021characterizing}. As illustrated in Fig.~\ref{fig1}, these failure modes manifest as a gradual distortion over time. This issue arises because MLPs lack an \textit{inductive bias} that effectively models temporal dependencies, leading to failures in propagating physical information dictated by the initial conditions.

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{_fig/fig1.pdf}
    \vspace{-6mm}
    \caption{PINN gradually distorts on convection equation.}
    \label{fig1}
    \vspace{-5mm}
  %  \vspace{-1mm}
\end{figure}


To introduce such an inductive bias, several sequence-to-sequence approaches have been proposed~\cite{krishnapriyan2021characterizing,zhao2024pinnsformer,yang2022learning,gao2022earthformer}. Specifically, \citet{krishnapriyan2021characterizing} propose training a new network at each time step and recursively using its output as the initial condition for the next step. This method incurs significant computational and memory overhead while exhibiting poor generalization. Furthermore, Transformer-based approaches~\cite{zhao2024pinnsformer,yang2022learning,gao2022earthformer} are proposed to address the time-dependency issue. Yet, these methods are based on discrete sequences, making their model produce incorrect pattern mutations in some cases due to their ignorance of the basic principle that PINNs approximate continuous dynamical systems. Thus, there is still an open question:

%To introduce such an \textit{Inductive Bias}, several sequence-to-sequence approaches are proposed~\cite{krishnapriyan2021characterizing,zhao2024pinnsformer,yang2022learning,gao2022earthformer}. Specifically, \citet{krishnapriyan2021characterizing} propose training a new network at every time step, then take the output as the initial condition for next step, but with tremendous computation and memory overheads and poor generalization. Meanwhile, Transformer-based approaches~\cite{zhao2024pinnsformer,yang2022learning,gao2022earthformer} are proposed to address the time-dependency issue. Yet, these methods are based on discrete sequences, which leads to that Transformers, in some cases, have incorrect mutations of the pattern. This is due to the fact that they ignore an important assumption that PINNs are approximating continuous dynamical systems.
\begin{comment}
To address this problem, sequence-to-sequence learning~\cite{sutskever2014sequence} is proposed as the inductive bias for modeling time-dependent equations' solution~\cite{krishnapriyan2021characterizing}.
    But \citet{sutskever2014sequence}'s method requires training a new network at every time step, which significantly increases the computation and memory overhead.
To make matters worse, we can't simply use recurrent neural networks for sequence-to-sequence learning to solve such failures. Since PINNs often involve higher order differentiation, the problem of gradient vanishing inherent in recurrent models~\cite{hochreiter1998vanishing} will be exacerbated, indicating that recurrent long-sequence modeling is not a good inductive bias either. PINNsFormer~\cite{zhao2024pinnsformer} attempts to use the Transformer model, but its construction of a pseudo-sequence within a small neighborhood of sampled time points does not really reflect the propagation of sequence information between time collection points. 
\end{comment}
%Chenhui: 这个问题和上面这段话的衔接并不够紧密，上一段话在强调seq2seq，为什么突然跳到inductive bias. 但是inductive bias是引出后面分析的keyword，怎么解决这个问题。

\vspace{-2mm}
\begin{center}
%\textit{What kinds of Inductive Bias should be introduced into Physics-Informed Neural Networks? And how?}
\textit{How can we effectively introduce sequentiality to PINNs?}
\end{center}
\vspace{-2mm}

To answer this question, we need to understand the essential difficulties of training PINNs.
First, PINNs assume temporal continuity, whereas, during their actual training, the spatio-temporal collection points used to construct the PDE residual loss are sampled discretely.
       %In the absence of a well-defined continuous-discrete articulation, the real trajectory is not necessarily recovered correctly in the training process, since the interpolation/extrapolation might be over-smoothed. 
       We define this nature of PINN as   \textit{Continuous-Discrete Mismatch}. 
       In the absence of a well-defined continuous-discrete articulation, the real trajectory of physical system is not necessarily recovered correctly in the training process, since such \textit{Continuous-Discrete Mismatch} would block the propagation of the initial condition.
%So for the successful training of PINNs, we need ensure the computability of the differentiation for time coordinates, i.e., the scale of the gradient is aligned without vanishing or explosion. 
   % Meanwhile, it is necessary to ensure that the model has the ability to propagate information directly over discrete time points, i.e., the ability to sequential modeling.


To respect the inherent \textit{Continuous-Discrete Mismatch}, we reveal that the State Space Models (SSM)~\cite{kalman1960new} can be a good continuous-discrete articulation. 
    SSMs parametrically model a discrete temporal sequence as a continuous dynamic system. An SSM's discrete form approximates the instantaneous states and rates of change of its continuous form via integrating the system's dynamics over each time interval, which more accurately responds to the trajectory of a continuous system. Meanwhile, the SSM unifies the scale of derivatives of different orders, making it easier to be optimized.    
So far, SSMs have shown their insane capacity in language~\cite{gu2023mamba} and vision~\cite{liu2024vmamba} tasks, but its potential for solving PDEs remains unexplored. We propose to construct PINNs with SSMs to unleash their excellent properties of continuous-discrete articulation.

%Therefore, the discrete modeling process for SSMs more accurately responds to the trajectory of a continuous system.
%Moreover, parametric modeling of the instantaneous rate of change allows the differentiation over time to be unified to the same scale, avoiding the hard-to-optimize problem endogenous to PINNs that is caused by the inconsistency of differentiation scales.
%SSMs parametrically model the state evolution of a continuous system. 

%SSMs effectively utilizes discrete time collection points to construct behavior of continuous dynamic systems.


    %In SSMs, there is a smooth articulation between its discrete-time form and continuous-time essence. Moreover, the differentiation of SSM for time is realized by multiplication between state transfer matrices, which effectively aligns the scales of the differentiation of each order. Therefore, it can avoid the PINNs' inherent difficulty of optimization due to the inconsistency of gradient scales.

Next, we remark that the simplicity bias~\cite{shah2020pitfalls} of neural networks is another crucial contributing factor to PINN training difficulty. The simplicity bias will lead the model to choose the pattern with the simplest hypothesis. This results in an over-smoothed solution when approximating PDEs. Because, for data-free PINNs, there might be a very simple function in the feasible domain that can make the residual loss zero. For example, for convection equation, $\bar u(x,t)=0$ leads to zero empirical loss on every collection point except when $t=0$. While the correct pattern is hard to fight against over-smoothed patterns during training.

A major way to eliminate simplicity bias is to construct agreements over diversity predictions~\cite{teney2022evading}. Following this principle, we propose a novel sub-sequence alignment approach, which allows the diverse predictions of time-varying SSMs to form such agreements. %The sub-sequence modeling successfully models the temporal pattern propagation while avoiding the difficulty of optimizing due to the large inertia of modeling long sequences.
Sub-sequence modeling adopts a medium sequence granularity, forming the time dependency that a small sequence fails to capture while avoiding the optimization problem along with the long sequence. 
Meanwhile, the alignment of the sub-sequence predictions ensures the global pattern propagation and the formation of an agreement that eliminates simplicity bias.

%Next, we remark that a suitable granularity of sequence is also crucial for PINNs. 
   % Sequences with too little granularity, for example, the pseudo-sequence within a small neighborhood of collection points implemented by PINNsFormer~\cite{zhao2024pinnsformer}, do not really reflect the propagation of information. 
   % A large sequence, on the other hand, converges to an over-smoothed solution due to the imperfectly reliable supervision of PINNs. This is because, for data-free PINNs, every function in the feasible domain can make the loss of a collection point to zero. For example, for a 1d-convection equation, $\bar u(x,t)=0$ leads to 0-loss on every collection point except when $t=0$. These points contribute uniformly over long sequences with the total loss function, leading to trapping in a local optimum of over-smoothing.
 %First, we remark that simply modeling the sequences over entire time intervals is unreliable, since PINNs are not data-driven, that their supervision is not a direct mapping from a data point sequence to a label sequence but residuals constructed from differential equations. In this physical law-driven learning paradigm, governing in late stages of a long sequence is ineffective, because there is a wide solution space in which functions can all make residual loss low but such functions lack information from previous time. 
%For exerting the influence of the initial conditions, short sequence modeling might be a useful inductive bias. Base on this, we propose a learning scheme with overlapping short sub-sequences. First, it constructs a sub-sequence at each time collection point to predict the output at the current collection point and the next few collection points. Then, at the next collection point, the output of model is softly aligned with previous predictions, as a way to propagate the information defined by the initial conditions over time.
%To address such issues, we propose a moderately granular approach, the sub-sequential modeling for correct propagation. The sub-sequential modeling first constructs a short sub-sequence of the collection points such that the initial condition can be fully propagated over the sub-sequence. Then it recursively propagates over the entire time-domain sequence via a sub-sequence state contrastive alignment.



In this paper, we introduce a novel learning framework to solve physics PDE's numerically, named PINNMamba, which performs time sub-sequences modeling with the Selective SSMs (Mamba)~\cite{gu2023mamba}. PINNMamba successfully captures the temporal dependence within the PDE when training the continuous dynamical systems with discretized collection points.
To the best of our knowledge, PINNMamba is the first data-free SSM-based model that effectively solve physics PDE.
%Our approach eliminates distortions due to discrete-time sampling in PDE's neural modeling of continuous systems. 
Experiments show that PINNMamba outperforms other PINN approaches
such as PINNsFormer~\cite{zhao2024pinnsformer} and KAN~\cite{liu2024kan} on multiple hard problems, achieving a new state-of-the-art.

\textbf{Contributions.} We make the following contributions:
\vspace{-4mm}
\begin{itemize}
    \item We reveal that the mismatch between the discrete nature of the training collection points and the continuous nature of the function approximated by the PINNs is an important factor that prevents the propagation of the initial condition pattern over time in PINNs.
%    \vspace{-6mm}
    \vspace{-2mm}
    \item We also note that the simplicity bias of neural networks is a key contributing factor to the over-smoothing pattern that causes gradual distortion in PINNs.
    \vspace{-2mm}
    \item We propose PINNMamba, which eliminates the discrete-continuity mismatch with SSM and combats simplicity bias with sub-sequential modeling, resulting in state-of-the-art on several PINN benchmarks.
\end{itemize}
\vspace{-2mm}