\section{Related Works}
\label{apx:rw}

\textbf{Physics-Informed Neural Networks.}
Physics-Informed Neural Networks~\cite{raissi2019physics} are a class of deep learning models designed to solve problems governed by physical laws described in PDEs. 
    They integrate physics-based constraints directly into the training process in the loss function, allowing them to numerically solve many key physical equations, such as Navier-Stokes equations\cite{jin2021nsfnets}, Euler equations~\cite{mao2020physics}, heat equatuons~\cite{cai2021physics}. Several advanced learning schemes such as gPINN~\cite{kharazmi2019variational}, vPINN\cite{yu2022gradient}, and RoPINN\cite{wu2024ropinn}, model architectures such as QRes~\cite{bu2021quadratic}, FLS~\cite{wong2022learning}, PINNsFormer~\cite{zhao2024pinnsformer}, KAN~\cite{liu2024kan,liu2024kanw} are proposed in terms of convergence, optimization, and generalization.

\textbf{Failure Modes in PINNs.}
Despite these efforts, PINN still has some inherently intractable failure modes. 
\citet{krishnapriyan2021characterizing} identify several types of equations that are vulnerable to difficulties in solving by PINNs.  
    These equations are usually manifested by the presence of a parameter in them that makes their pattern behave as a high frequency or a complex state~\cite{pmlr-v235-cho24b}, failing to propagate the initial condition. 
        In such cases, an empirical loss constructed using a collection point can easily fall into an over-smooth solution (e.g. $\bar u(x,t)=0$ can make the loss of all collection points except whose $t=0$ descend to 0 for 1d-wave equations). Several methods regarding optimization~\cite{wu2024ropinn,wang20222}, sampling~\cite{gao2023failure,wu2023comprehensive}, model architecture~\cite{zhao2024pinnsformer,pmlr-v235-cho24b,pmlr-v235-nguyen24c}, transfer learning~\cite{xu2023transfer,pmlr-v235-cho24b} are proposed to mitigate such failure modes. 
            However, the above approaches do not focus on the fact that a PDE system should be modeled as a continuous dynamic, leading to difficulties in generalization over a wide range of problems.




\textbf{State Space Models.} The state space model~\cite{kalman1960new} is a mathematical representation of a physical system in terms of state variables. 
    Modern SSMs~\cite{gu2022efficiently,smith2023simplified,gu2023mamba} combine the representational power of neural networks with their own superior long-range dependency capturing and parallel computing capabilities and thus are widely used in many fields, such as language modeling~\cite{fu2023hungry,poli2023hyena,gu2023mamba,pmlr-v235-dao24a}, computer vision~\cite{pmlr-v235-zhu24f,liu2024vmamba}, and genomics~\cite{gu2023mamba,nguyen2024sequence}. Specifically, Structured SSMs~(S4)~\cite{gu2022efficiently} decomposing the structured state matrices as the sum of a low-rank
and normal terms to improve the efficiency of state-space-based deep models. Further, Selective SSMs (Mamba)~\cite{gu2023mamba} eliminates the Linear Time Invariance~\cite{sain1969invertibility} of SSMs by introducing a gating mechanism, allowing the model to selectively propagate or forget information and greatly enhancing the model performance. In physics, SSMs are used in conjunction with Neural Operator to form a data-driven solution to PDEs~\cite{zheng2024aliasfree,hu2024state}. 
However, these methods are data-driven which lack generalization ability in some scenarios where real data is not available. Unlike these methods, our approach, PINNMamba is fully physics-driven, relying only on residuals constructed using PDEs without any training data.

%



\section{Proof of Theorem \ref{thm:continuous-discrete}}
\label{apx:proof3_1}

We start with a function $v$ such that $\mathcal{M}(v)$ is non-zero almost everywhere. Such a function exists because $\mathcal{M}$ is a non-zero differential operator. For example, if $\mathcal{M}$ is the Laplacian, a non-harmonic function can be chosen.

\begin{lemma}[Existence of Base Function]
    Let $\mathcal{M}$ be a non-degenerate differential operator on $\Omega \times [0,T]$, where $\Omega \subset \mathbb{R}^n$ is a domain. There exists a function $v \in C^\infty(\Omega \times [0,T])$ such that:  
$$
\mathcal{M}(v) \neq 0 \quad \text{for almost every } (x,t) \in \Omega \times [0,T].
$$
\end{lemma}


\begin{proof}
    Since $\mathcal{M}$ is non-degenerate (i.e., not identically zero), there exists at least one function $w \in C^\infty(\Omega \times [0,T])$ and a point $(x_0, t_0) \in \Omega \times [0,T]$ such that:  
   $$
   \mathcal{M}(w)(x_0, t_0) \neq 0.
   $$  
   By continuity of $\mathcal{M}(w)$ (assuming smooth coefficients for $\mathcal{M}$), there is an open neighborhood $U \subset \Omega \times [0,T]$ around $(x_0, t_0)$ where $\mathcal{M}(w) \neq 0$.
   
   Construct a smooth bump function $\phi \in C^\infty(\Omega \times [0,T])$ with:  
   $\phi \equiv 1$ on a smaller neighborhood $V \subset U$,  
   and $\phi \equiv 0$ outside $U$.  
      Define $v_0 = \phi \cdot w$. Then $\mathcal{M}(v_0) = \mathcal{M}(\phi w)$ is non-zero on $V$ and smooth everywhere.  
   Let $\{(x_k, t_k)\}_{k=1}^\infty$ be a countable dense subset of $\Omega \times [0,T]$. For each $k$, repeat the above construction to obtain a function $v_k \in C^\infty(\Omega \times [0,T])$ such that: $\mathcal{M}(v_k) \neq 0$ in a neighborhood $U_k$ of $(x_k, t_k)$,  
   $\text{supp}(v_k) \subset U_k$,  
   and the supports $\{U_k\}$ are pairwise disjoint. 

   Define the function:  
   $$
   v = \sum_{k=1}^\infty \epsilon_k v_k,
   $$  
   where $\epsilon_k > 0$ are chosen such that the series converges in $C^\infty(\Omega \times [0,T])$ (e.g., $\epsilon_k = 2^{-k}/\max\{\|v_k\|_{C^k}, 1\}$).

   The set $\bigcup_{k=1}^\infty U_k$ is open and dense in $\Omega \times [0,T]$. Since $\mathcal{M}(v) \neq 0$ on this dense open set, the zero set $Z = \{(x,t) : \mathcal{M}(v)(x,t) = 0\}$ is contained in the complement of $\bigcup_{k=1}^\infty U_k$, which is nowhere dense and hence has Lebesgue measure zero. Therefore:  
   $$
   \mathcal{M}(v) \neq 0 \quad \text{for almost every } (x,t) \in \Omega \times [0,T].
   $$
\end{proof}

\begin{lemma}[Local Correction Functions]\label{lem:B2}
    Let $\mathcal{M}$ be a non-degenerate differential operator on $\Omega \times [0,T]$, and let $\chi^* = \{(x^*_1,t^*_1),\dots,(x^*_N,t^*_N)\} \subset \Omega \times [0,T]$. There exist smooth functions $\{w_i\}_{i=1}^N \subset C^\infty(\Omega \times [0,T])$ and radii $\epsilon_1, \dots, \epsilon_N > 0$ such that for each $i$:  
    
1. Compact Support: $\text{supp}(w_i) \subset B_{\epsilon_i}(x^*_i,t^*_i)$,  

2. Non-Vanishing Action: $\mathcal{M}(w_i)(x^*_i,t^*_i) \neq 0$, 

3. Disjoint Supports: $B_{\epsilon_i}(x^*_i,t^*_i) \cap B_{\epsilon_j}(x^*_j,t^*_j) = \emptyset$ for $i \neq j$. 
\end{lemma} 


\begin{proof}
    Let $d_{\text{min}} = \min_{i \neq j} \text{dist}\left((x^*_i,t^*_i), (x^*_j,t^*_j)\right)$ be the minimal distance between distinct points in $\chi^*$. For all $i$, choose radii $\epsilon_i > 0$ such that:  
$$
\epsilon_i < \frac{d_{\text{min}}}{2}.
$$  
This ensures the balls $B_{\epsilon_i}(x^*_i,t^*_i)$ are pairwise disjoint.  

For each $(x^*_i,t^*_i)$, since $\mathcal{M}$ is non-degenerate, there exists a smooth function $f_i \in C^\infty(\Omega \times [0,T])$ such that:  
$$
\mathcal{M}(f_i)(x^*_i,t^*_i) \neq 0.
$$  This is because, when $\mathcal{M}$ is non-degenerate, its action cannot vanish on all smooth functions at $(x^*_i,t^*_i)$. For instance, if $\mathcal{M}$ contains a derivative $\partial_{x_k}$, take $f_i = x_k$ near $(x^*_i,t^*_i)$.

Then for each $i$, construct a smooth bump function $\phi_i \in C^\infty(\Omega \times [0,T])$ satisfying:  

1. $\phi_i \equiv 1$ on $B_{\epsilon_i/2}(x^*_i,t^*_i)$, 

2. $\phi_i \equiv 0$ outside $B_{\epsilon_i}(x^*_i,t^*_i)$, 

3. $0 \leq \phi_i \leq 1$ everywhere.  

Therefore, define the localized function:  
$$
w_i = \phi_i \cdot f_i.
$$  
By construction:  

1. $\text{supp}(w_i) \subset B_{\epsilon_i}(x^*_i,t^*_i)$,

2. $w_i = f_i$ on $B_{\epsilon_i/2}(x^*_i,t^*_i)$, so  
$$
\mathcal{M}(w_i)(x^*_i,t^*_i) = \mathcal{M}(f_i)(x^*_i,t^*_i) \neq 0.
$$  

Since $\epsilon_i < \frac{d_{\text{min}}}{2}$, the distance between any two balls $B_{\epsilon_i}(x^*_i,t^*_i)$ and $B_{\epsilon_j}(x^*_j,t^*_j)$ is at least $d_{\text{min}} - 2\epsilon_i > 0$. Thus, the supports of $w_i$ and $w_j$ are disjoint for $i \neq j$. 

Therefore, the functions $\{w_i\}_{i=1}^N$ satisfy all required conditions.  


\end{proof}



We now state the one-dimensional case of Theorem~\ref{thm:continuous-discrete} here:

\begin{lemma}[One-Dimensional Case of Theorem~\ref{thm:continuous-discrete}]
\label{lemma:1d}
    Let $\chi^* = \{(x^*_1,t^*_1),\dots,(x^*_N,t^*_N)\}\subset \Omega\times[0,T]$. Then for differential operator $\mathcal M$ there exist infinitely many functions
$u_\theta : \Omega \to \mathbb{R}$ parametrized by $\theta$ , s.t.
$$ \mathcal{M}(u_\theta(x^*_i,t^*_i)) = 0 \quad \text{for } i=1,\dots,N,$$ $$ 
   \mathcal{M}(u_\theta(x,t)) \neq 0
   \quad \text{for a.e. } x \in \Omega\times[0,T] \setminus \chi^*.$$
\end{lemma}

\begin{proof}
    Define the corrected function:
$$
u_\theta = v + \sum_{i=1}^N \alpha_i w_i,
$$
where $w_i$ is the local correction function defined in Lemma~\ref{lem:B2}, $\alpha_i \in \mathbb{R}$ are scalars chosen such that:
$$
\mathcal{M}(u_\theta)(x_i^*, t_i^*) = \mathcal{M}(v)(x_i^*, t_i^*) + \alpha_i \mathcal{M}(w_i)(x_i^*, t_i^*) = 0.
$$

Since $\mathcal{M}(w_i)(x_i^*, t_i^*) \neq 0$, we can solve for $\alpha_i$:
$$
\alpha_i = -\frac{\mathcal{M}(v)(x_i^*, t_i^*)}{\mathcal{M}(w_i)(x_i^*, t_i^*)}.
$$

Outside the union of supports $\bigcup_{i=1}^N B_{\epsilon_i}(x_i^*, t_i^*)$, we have:
$$
\mathcal{M}(u_\theta) = \mathcal{M}(v) + \sum_{i=1}^N \alpha_i \mathcal{M}(w_i) = \mathcal{M}(v),
$$
since $w_i \equiv 0$ outside $B_{\epsilon_i}(x_i^*, t_i^*)$. By construction, $\mathcal{M}(v) \neq 0$ almost everywhere. 

The parameters $\theta = (\epsilon_1, \dots, \epsilon_N, \alpha_1, \dots, \alpha_N)$ can be varied infinitely by varying $w_i$: The bump functions $w_i$ can be scaled, translated, or reshaped (e.g., Gaussian vs. polynomial) while retaining the properties of Local Correction in Lemma~\ref{lem:B2} and varying $\epsilon_i$: For each $i$, choose $\epsilon_i$ from a continuum $(0, \delta_i)$, where $\delta_i$ ensures disjointness.

Thus, the family $\{u_\theta\}$ is uncountably infinite.

The set $\chi^*$ by definition has Lebesgue measure zero in $\Omega \times [0,T]$. The corrections $\sum_{i=1}^N \alpha_i w_i$ are confined to the measure-zero set $\bigcup_{i=1}^N B_{\epsilon_i}(x_i^*, t_i^*)$. Therefore:
$$
\mathcal{M}(u_\theta) \neq 0 \quad \text{for a.e. } (x,t) \in \Omega \times [0,T] \setminus \chi^*.
$$
\end{proof}


We now generalize Lemma~\ref{lemma:1d} to $m$-dimension, to get Theorem~\ref{thm:continuous-discrete}.

\begin{theorem}[Theorem~\ref{thm:continuous-discrete}]
    Let $\chi^* = \{(x^*_1,t^*_1),\dots,(x^*_N,t^*_N)\}\subset \Omega\times[0,T]$. Then for differential operator $\mathcal M$ there exist infinitely many functions
$u_\theta : \Omega \to \mathbb{R}^m$ parametrized by $\theta$ , s.t.
$$ \mathcal{M}(u_\theta(x^*_i,t^*_i)) = 0 \quad \text{for } i=1,\dots,N,$$ $$ 
   \mathcal{M}(u_\theta(x,t)) \neq 0
   \quad \text{for a.e. } x \in \Omega\times[0,T] \setminus \chi^*.$$
\end{theorem}

\begin{proof}
    It is trivial to generalize the Lemma~\ref{lemma:1d} to the case $u_\theta : \Omega \to \mathbb{R}^m$, by constructing:
    $$
   u_\theta = v + \sum_{i=1}^N \sum_{j=1}^m \alpha_{i,j} w_{i,j},
   $$
   where $ \alpha = (\alpha_{i,j}) \in \mathbb{R}^{N \cdot m} $. Adjust $ \alpha_{i,j} $ such that:
   $$
   \mathcal{M}(u_\theta)(x_i^*, t_i^*) = \mathcal{M}(v)(x_i^*, t_i^*) + \sum_{j=1}^m \alpha_{i,j} \mathcal{M}(w_{i,j})(x_i^*, t_i^*) = 0.
   $$
   This gives a linear system for $ \alpha $, which is solvable because the $ w_{i,j} $ are linearly independent.
\end{proof}


\section{Linear Time-Varying System}
\label{apx:LTI}

To adjust the given Linear Time-Invariant system to a Linear Time-Varying system, we replace the constant matrices $ \bar{A} $, $ \bar{B} $, and $ C $ with their time-varying counterparts $ \bar{A}(k) $, $ \bar{B}(k) $, and $ C(k) $. The state transition matrix $ \bar{A}^{k-i} $ in the LTI system becomes the product of time-varying matrices from time $ i $ to $ k-1 $. The resulting time-varying output equation is:

\begin{equation}
    \mathbf{u}_k = C(k) \Phi(k, 0) \mathbf{h}_0 + C(k) \sum_{i=0}^k \Phi(k, i) \bar{B}(i) \mathbf{x}_i,
\end{equation}



where $ \Phi(k, i) $ is the state transition matrix from time $ i $ to $ k $, defined as:
\begin{equation}
      \Phi(k, i) = \begin{cases} 
    \bar{A}(k-1) \bar{A}(k-2) \cdots \bar{A}(i) & \text{if } k > i, \\
    I & \text{if } k = i.
  \end{cases}
\end{equation}


  
and the term $ \Phi(k, 0) \mathbf{h}_0 $ represents the free response due to the initial condition $ \mathbf{h}_0 $.

The summation $ \sum_{i=0}^k \Phi(k, i) \bar{B}(i) \mathbf{x}_i $ includes contributions from all inputs $ \mathbf{x}_i $ up to time $ k $, with $ \Phi(k, i) \bar{B}(i) $ capturing the time-varying dynamics.

To adjust the Eq.~\ref{equ:timeloss} to a Time-Varying system The state transition term $ \bar{A}^{k-i} $ becomes the time-ordered product $ \Phi(k, i) $, and the output $ \mathbf{u}_k $ now explicitly depends on time-varying dynamics. The adjusted equation becomes:

\begin{equation}
    \sum_{i=1}^M \mathcal{L}_{\mathcal{F}}(u(x_i, k\Delta t)) = \frac{1}{M} \left\| \mathcal{F}\left( \mathbf{1}_M \cdot \mathbf{u}_k \right) \right\|^2= \frac{1}{M} \left\| \mathcal{F}\left( \mathbf{1}_M \cdot \mathbf{u}_k = C(k) \Phi(k, 0) \mathbf{h}_0 + C(k) \sum_{i=0}^k \Phi(k, i) \bar{B}(i) \mathbf{x}_i\right) \right\|^2.
\end{equation}




This modification ensures consistency with the Time-Varying systemâ€™s time-dependent parameters while preserving the structure of the original loss function.

\section{PDEs Setups}
\label{apx:setup}

\subsection{1-D Convection}

The 1-D convection equation, also known as the 1-D advection equation, is a partial differential equation that models the transport of a scalar quantity $ u(x,t) $ (such as temperature, concentration, or momentum) due to fluid motion at a constant velocity $ c $. It is a fundamental equation in fluid dynamics and transport phenomena. The equation is given by:
\begin{gather}
    \frac{\partial u}{\partial t} + \beta \frac{\partial u}{\partial x} = 0,\; \forall x \in[0,2\pi], t\in [0,1],\nonumber\\
    u(x,0) = \sin x,\;\forall x \in[0,2\pi],\\
    u(0,t)=u(2\pi,t),\;\forall  t\in [0,1],\nonumber
\end{gather}
where $\beta$ is the constant convection (advection) speed. As $\beta$ increases, the equation will be harder for PINN to approximate. It is a well-known equation with failure mode for PINN. We set $\beta=50$ following common practice~\cite{zhao2024pinnsformer,wu2024ropinn}.

The 1-D convection equation's analytical solution is given by:
\begin{equation}
    u_\text{ana}(x,t) = \sin(x-\beta t).
\end{equation}


\subsection{1-D Reaction}

The 1-D reaction equation is a partial differential equation that models how a chemical species reacts over time and (optionally) varies along a single spatial dimension. The equation is given by:
\begin{gather}
    \frac{\partial u}{\partial t} -\rho u(1-u) = 0,\; \forall x \in[0,2\pi], t\in [0,1],\nonumber\\
    u(x,0) = \exp(-\frac{(x-\pi)^2}{2(\pi/4)^2}),\;\forall x \in[0,2\pi],\\
    u(0,t)=u(2\pi,t),\; \forall  t\in [0,1],\nonumber
\end{gather}
where $\rho$ is the growth rate coefficient. As $\rho$ increases, the equation will be harder for PINN to approximate. It is a well-known equation with failure mode for PINN. We set $\rho=5$ following common practice~\cite{zhao2024pinnsformer,wu2024ropinn}.

The 1-D reaction equation's analytical solution is given by:
\begin{equation}
    u_\text{ana}=\frac{\exp(-\frac{(x-\pi)^2}{2(\pi/4)^2})\exp(\rho t)}{\exp(-\frac{(x-\pi)^2}{2(\pi/4)^2})(\exp(\rho t)-1)+1}.
\end{equation}

\subsection{1-D Wave}

The 1-D wave equation is a partial differential equation that describes how a wave propagates through a medium, such as a vibrating string.  We consider such an equation given by:
\begin{gather}
    \frac{\partial^2 u}{\partial t^2} - 4\frac{\partial^2 u}{\partial x^2} = 0,\; \forall x \in[0,1], t\in [0,1],\nonumber\\
    u(x,0) = \sin(\pi x)+\frac{1}{2}\sin(\beta \pi x), \;\forall x\in[0,1],\\
    \frac{\partial u(x,0)}{\partial t} = 0, \;\forall x\in[0,1],\nonumber\\
    u(0,t)=u(1,t) = 0, \; \forall  t\in [0,1],\nonumber
\end{gather}
where $\beta$ is a wave frequency coefficient. We set $\beta$ as 3 following common practice~\cite{zhao2024pinnsformer,wu2024ropinn}. The wave equation contains second-order derivative terms in the equation and first-order derivative terms in the initial condition, which is considered to be hard to optimize~\cite{wu2024ropinn}. This example illustrates that PINNMamba can better capture the time continuum because its differentiation for time is directly defined by the matrix, whose differential scale is uniform for multiple orders.

The 1-D wave equation's analytical solution is given by:
\begin{equation}
    u_\text{ana}(x,t)=\sin(\pi x)\cos(2\pi t)+\sin(\beta \pi x)\cos(2\beta \pi t).
\end{equation}

\subsection{2-D Navier-Stokes}

The 2-D Navier-Stokes equation describes the motion of fluid in two spatial dimensions $x$ and $y$. It is fundamental in fluid dynamics and is used to model incompressible fluid flows. We consider such an equation given by:
\begin{gather}
    \frac{\partial u}{\partial t} + \lambda_1 (u\frac{\partial u}{\partial x} + v \frac{\partial u}{\partial y}) = - \frac{\partial p}{\partial x} + \lambda_2 (\frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial v^2}), \nonumber \\
    \frac{\partial v}{\partial t} + \lambda_1 (u\frac{\partial v}{\partial x} + v \frac{\partial v}{\partial y}) = - \frac{\partial p}{\partial y} + \lambda_2 (\frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial v^2}),
\end{gather}
where $u(x,y,t)$, $v(x,y,t)$, and $p(x,y,t)$ are the x-coordinate velocity field, y-coordinate velocity field, and pressure field, respectively. We set $\lambda_1 = 1$ and $\lambda_2 = 0.01$ following common practice~\cite{zhao2024pinnsformer,raissi2019physics}. 

The 2-dimensional Navier-Stokes equation doesn't have an analytical solution that can be described by existing mathematical symbols, we take~\citet{raissi2019physics}'s finite-element numerical simulation as ground truth. 

\subsection{PINNNacle}
PINNacle~\cite{hao2023pinnacle} contains 16 hard PDE problems, which can be classified as Burges, Poisson, Heat, Navier-Stokes, Wave, Chaotic, and other High-dimensional problems. We only test PINNmamba on 6 problems, because solving the remaining problems with a sequence-based PINN model will cause an out-of-memory issue, even on the most advanced NVIDIA H100 GPU. Please refer to the original paper of PINNacle~\cite{hao2023pinnacle} for the details of the benchmark.

\section{Training Details}
\label{apx:hyperparam}

\textbf{Hyperparameters.} We provide the training hyperparameters of the main experiments in Table~\ref{tab:hyperpara}.


\input{_tab/hyperparams} 

\textbf{Computation Overhead.} We report the training time and memory consumption of baseline models and PINNMamba on the convection equation in Table~\ref{tab:training}. 

\input{_tab/training}

\section{Sensitivity Analysis}

PINNMamba can be further improved by hyper-parameters tuning, we test the sub-sequence length, interval and activation selection in this section.

\label{apx:sense}

\textbf{Sub-sequence Length.} We test the effect of different sub-sequence lengths on model performance. As shown in Table~\ref{tab:length}, we test the length of 3, 5, 7, 9, 21.  Length $k =7$ achieves the best performance on reaction and wave equations, while  $k =5$ achieves the best performance on convection equation.

\input{_tab/length}

\textbf{Sub-Sequence Interval.}  We test the effect of different sub-sequence intervals on model performance. As shown in Table~\ref{tab:interval}, we test the intervals of $2e-3$, $5e-3$, $1e-2$, $1e-1$. The interval $\Delta t =1e-2$ achieves the best performance on convection and wave equations, while $\Delta t = 5e-3$ achieves the best performance on reaction. Note that, when $\Delta t = 1e-1$, we cannot build the sub-sequence contrastive alignment.
\input{_tab/interval}


\textbf{Activation Function.} We test the activation function's effect on the performance of PINNMamba. We report the results of ReLU~\cite{nair2010rectified}, Tanh~\cite{fan2000extended}, and Wavelet~\cite{zhao2024pinnsformer} in Table~\ref{tab:activation}.
\input{_tab/activation}

\section{Complex Problem Results}
\label{apx:comp}

\subsection{2D Navier-Stokes Equations}

Although PINN can already handle Navier-Stokes equations well, we still tested the performance of PINN Mamba on Navier-Stokes equations to check the generalization performance of our method on high-dimensional problems. As shown in Fig.~\ref{fig:nss}, our method achieves good results on Navier-Stokes pressure prediction. Since there is no initial condition information for the N-S equation for pressure, we took the data from the only collection point for pattern alignment.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{_fig/nss}
    \vspace{-3mm}
    \caption{The ground truth solution, prediction (top), and absolute error (bottom) on Navier-Stokes equations.}
    \label{fig:nss}
    %\vspace{-5mm}
  %  \vspace{-1mm}
\end{figure*}

\subsection{PINNacle Benchmark}

Like PINNsFormer, PINNMamba is a sequence model. The sequence model suffers from Out-of-Memory problems when dealing with some of the problems in the PINNacle Benchmark~\cite{hao2023pinnacle}, even when running on the advanced Nvidia H100 GPU. We report here the results of the sub-problems for which results can be obtained in Table~\ref{tab:pinnacle}. PINNMamba can solve the Out-of-Memory problem by distributed training over multiple cards, which we leave as a follow-up work.

\input{_tab/pinnacle}

