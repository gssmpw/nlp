

\section{Experiments}
\textbf{Setup.} We evaluate the performance of PINNMamba on three standard PDE benchmarks: convection, wave, and reaction equations, all of which are identified as being affected by failure modes~\cite{krishnapriyan2021characterizing,zhao2024pinnsformer}. The details of those PDEs can be found in Appendix~\ref{apx:setup}.
    We compare PINNMamba with four baseline models, vanilla PINN~\cite{raissi2019physics}, QRes~\cite{bu2021quadratic}, PINNsFormer~\cite{zhao2024pinnsformer}, and KAN~\cite{liu2024kan} .
For fair comparison, we sample 101$\times$101 collection points with uniformly grid sampling, following previous work~\cite{zhao2024pinnsformer,wu2024ropinn}. We also evaluate on PINNacle Benchmark~\cite{hao2023pinnacle} and Navierâ€“Stokes equation~\cite{raissi2019physics}.

\input{_tab/convection}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{_fig/wave}
    \vspace{-8mm}
    \caption{The ground truth solution, prediction (top), and absolute error (bottom) on wave equations.}
    \label{fig:wave}
    \vspace{-5mm}
  %  \vspace{-1mm}
\end{figure*}

\textbf{Training Details.} We train PINNMamba and all the baseline models 1000 epochs with L-BFGS optimizer~\cite{liu1989limited}.
We set the sub-sequence length to 7 for PINNMamba, and keep the original pseudo-sequence setup for PINNsFormers. The weights of loss terms $[\lambda_\mathcal F,\lambda_\mathcal I,\lambda_\mathcal B]$ are set to $[1,1,10]$ for all three equations, as we find that strengthening the boundary conditions can lead to better convergence. $\lambda_\text{alig}$ is set to 1000 for convection and reaction equations, and auto-adapted by $\lambda_\mathcal F$ for wave equation.
%Loss weights are also actively adapted by neural tangent kernel~\cite{wang2022and} for wave equations for test the orthogonality of PINNMamba with other methods.
All experiments are implemented in PyTorch 2.1.1 and trained on an NVIDIA H100 GPU.  More training details are in Appendix~\ref{apx:hyperparam}. Our code and weights are available at \url{https://github.com/miniHuiHui/PINNMamba}.

\textbf{Metrics.} To evaluate the performance of the models, we take relative Mean Absolute Error (rMAE, a.k.a  $\ell_1$ relative error) and relative Root Mean Square Error (rRMSE, a.k.a $\ell_2$ relative error) following common practive~\cite{zhao2024pinnsformer,wu2024ropinn}. The metrics are formulated as:
\begin{align}
\text { rMAE }(\hat u)&=\frac{\sum_{n=1}^N\left|\hat{u}\left(x_n, t_n\right)-u\left(x_n, t_n\right)\right|}{\sum_{n=1}^{N}\left|u\left(x_n, t_n\right)\right|}, \\
\text { rRMSE }(\hat u)&=\sqrt{\frac{\sum_{n=1}^N\left|\hat{u}\left(x_n, t_n\right)-u\left(x_n, t_n\right)\right|^2}{\sum_{n=1}^N\left|u\left(x_n, t_n\right)\right|^2}},
\end{align}
where N is the number of test points, $u(x,t)$ is the ground truth solution, and $\hat u(x,t)$ is the model's prediction.

\vspace{-2mm}

\subsection{Main Results}
\vspace{-1mm}
We present the rMAE and rRMSE for approximating convection, reaction and wave equation's solution in Table~\ref{tab:diff}. Our model consistently outperforms other model architectures, achieving new state-of-the-art.
Notably, as shown in Fig.~\ref{fig:conv}, for the convection equation, PINNMamba allows sufficient propagation of information about the initial conditions, whereas on all the other models there is a varying degree of distortion in the time coordinates.
    As shown in Fig.~\ref{fig:reac}, PINNMamba can further optimize at the boundary, resulting in a lower error than KAN and PINNsFormer for reaction equations. For problems as intrinsically difficult to optimize as the wave, as in Fig.~\ref{fig:wave}, PINNMamba effectively combats simplicity bias and aligns the scales of multi-order differentiation, and thus achieves significantly higher accuracy. This illustrates that PINNMamba can be effective against PINN's failure modes. It's also worth noting that, PINNMamba has the lowest number of parameters (except KAN), while achieving consistently the best performance.

\input{_tab/trainingparadigm}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{_fig/reac}
    \vspace{-8mm}
    \caption{The ground truth solution, prediction (top), and absolute error (bottom) on reaction equations.}
    \label{fig:reac}
    \vspace{-5mm}
  %  \vspace{-1mm}
\end{figure*}


\subsection{Combination with Other Methods}
\vspace{-1mm}
Since PINNMamba mainly focuses on model architecture, it can be integrated with other methods effortlessly. 
    We explore the feasibility and their performance in combination with advanced training paradigm, as well as loss balancing.

\textbf{Training Paradigm.} We show the rMAE of PINNMamba when integrated with advanced strategies in Table~\ref{tab:para}. We observe that gPINN~\cite{yu2022gradient} and vPINN~\cite{kharazmi2019variational} erratically deliver some performance gains on some tasks. 
    This is due to the fact that the regularization provided by gPINN and vPINN in the form of a loss function through the gradient and variational residuals has little effect on PINNMamba, since SSM itself is sufficiently regularized. RoPINN~\cite{wu2024ropinn} reduces the PINNMamba's error on convection and wave equations by about 40\%, since it complements the spatial continuity dependency.

\textbf{Neural Tangent Kernel.} Dynamic tuning of losses via Neural Tangent Kernel(NTK)~\cite{wang2022and} has been shown to have the effect of smoothing out the loss landscape. 
PINNMamba also works well with the NTK-adopted loss function. As shown in Table~\ref{tab:para}, NTK can reduce PINNMamba error by 5-25\%. 
The combination of RoPINN and NTK can further improve the overall performance of PINNMamba, which demonstrates the excellent suitability of PINNMamba with other PINN optimization methods.

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{_fig/loss_error}
    \vspace{-4mm}
    \caption{Loss and $\ell_1$-Error Curve w.r.t Training Iteration.}
    \label{fig:losserror}
    \vspace{-4mm}
  %  \vspace{-1mm}
\end{figure}
\vspace{-2mm}
\subsection{Loss-Error Consistency Analysis}
\vspace{-1mm}

Our other interest is the role of PINNMamba for the elimination of simplicity bias. Models affected by simplicity bias that fall into over-smoothing solutions will show inconsistent decreasing trends in loss and error during training. 
    As shown in Fig.~\ref{fig:losserror}, in the training process for solving convection equations, the rMAE of PINN doesn't descend as $\mathcal L_\mathcal F$ and $\mathcal L_\mathcal I$. 
        This suggests that PINN is trapped in an over-smoothing solution, which is in agreement with our observation in Fig.~\ref{fig:conv}. 
As a comparison, we find that PINNMamba's losses descent processes show a high degree of consistency with its error descent process. 
    This indicates that PINNMamba does not tend to fall into a local optimum of oversimplified patterns.
        Instead, it tends to exhibit patterns that are consistent with the original PDEs.

\vspace{-2mm}
\subsection{Ablation Study}
\vspace{-1mm}
\input{_tab/ablation}

To verify the validity of the various components of the PINNMamba, as shown in Table~\ref{tab:ablation}, we evaluate the performance of models subtracting these components from PINNMamba.

\textbf{Sub-Sequence.} We remove the sub-sequence alignment, which leads to a decrease in model performance, indicating the significance of the agreement formed through alignment in eliminating simplicity bias.
After replacing the sub-sequence with a long sequence of the entire domain, the model shows failure modes, in line with the sequence granularity analysis in Section~\ref{sec:subseq}.

\textbf{Time-Varying SSM.} We replace the selective SSM~\cite{gu2023mamba} with a linear time-invariant structure SSM~\cite{gu2022efficiently}, and there is some decrease in model performance, illustrating the role of predictive diversity in eliminating simplicity bias. 
And when we remove SSM completely and switch to MLP instead, the model has severe failure modes. 
        This demonstrates that SSM's adaptation for \textit{Continuous-Discrete Mismatch} allows the initial condition information to propagate sufficiently in time coordinates.

In addition, we also conducted a sensitivity analysis of the choice of sub-sequence length, activation. See Appendix~\ref{apx:sense}.

\vspace{-3mm}
\subsection{Experiments on Complex Problems}
\vspace{-1mm}
To further demonstrate the generalization of our method, we tested our model on partial PINNacle Benchmark~\cite{hao2023pinnacle} and Navier-Stokes equations. As shown in Fig.~\ref{fig:ns}, PINNMamba achieves the lowest error on the N-S equation. Just like PINNsFormer, PINNMamba also gets out-of-memory on some problems in PINNacle, which we identify as a major limitation of sequence-based methods. We discuss the details of PINNacle experiments in Appendix~\ref{apx:comp}.

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{_fig/NS}
    \vspace{-6mm}
    \caption{Absolute Error of pressure prediction of N-S equation}
    \label{fig:ns}
    \vspace{-3mm}
  %  \vspace{-1mm}
\end{figure}
