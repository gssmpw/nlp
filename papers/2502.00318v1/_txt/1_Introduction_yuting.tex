\section{Introduction}

In the past few years, Physics-Informed Neural Networks~(PINNs)~\cite{raissi2019physics} have emerged as a novel approach for numerically solving partial differential equations~(PDEs). PINN takes a neural network $u_{\theta}(x,t)$, whose parameters $\theta$ are trained with physics PDE residual loss, as the numerical solution $u(x,t)$ of the PDE, where $x$ and $t$ are spatial and temporal coordinates. The core idea behind PINNs is to take advantage of the universal approximation property of neural networks~\cite{hornik1989multilayer} and automatic differentiation implemented by mainstream deep learning frameworks, such as PyTorch~\cite{paszke2019pytorch} and Tensorflow~\cite{abadi2016tensorflow}, so that PINNs can achieve potentially more precise and efficient PDE solution approximation compared with traditional numerical approaches like finite element methods~\cite{reddy1993introduction}.

Current mainstream PINNs use multilayer perceptrons (MLPs) as their backbone model architecture. However, despite the universal approximation of the MLPs, an arbitrarily accurate approximation to the numerical solution of the PDE is not always guaranteed. This has resulted in the observation of multiple failure modes in PINNs, especially when approximating high-frequency or complex patterns~\cite{krishnapriyan2021characterizing}. As shown in Fig.~\ref{fig1}, these failure modes exhibit a gradual temporal distortion of the solution. This degradation stems from MLPs' inherent lack of inductive bias for modeling time dependencies of a system, which impairs their ability to effectively propagate physical information defined by the initial condition.


\begin{figure}[t!]
    \centering
    \includegraphics{_fig/Fig1.pdf}
    \vspace{-2mm}
    \caption{PINN gradually distorts on convection equation.}
    \label{fig1}
    \vspace{-5mm}
  %  \vspace{-1mm}
\end{figure}

To incorporate \textit{Inductive Bias} into the model design, recent work proposes several sequence-to-sequence approaches~\cite{krishnapriyan2021characterizing,zhao2024pinnsformer,yang2022learning,gao2022earthformer}. \citet{krishnapriyan2021characterizing} introduce a novel method that trains a new network at every time step and then uses the output as the initial condition for the next step. However, this approach suffers from substantial computational and memory overhead while exhibiting poor generalization. Furthermore, Transformer-based approaches~\cite{zhao2024pinnsformer,yang2022learning,gao2022earthformer} are proposed to address the time-dependency issue. Yet, these methods are based on discrete sequences, making their model produce incorrect pattern mutations in some cases due to their ignorance of a basic principle that PINNs approximate continuous dynamical systems. Therefore, there remains a critical question unanswered by current PINNs research:
\vspace{-2mm}
\begin{center}
%\textit{What kinds of Inductive Bias should be introduced into Physics-Informed Neural Networks? And how?}
\textit{How can we effectively introduce sequentiality to PINNs?}
\end{center}
\vspace{-2mm}


First of all, we remark that PINNs have a nature of \textit{Continuous-Discrete Duality}. Paradoxically, although PINNs assume temporal continuity (or even high-order differentiability), the spatio-temporal points used to construct the PDE residual loss are discretely sampled during training. Consequently, without a well-defined continuous-discrete articulation, the real trajectory might not be recovered correctly in the training process due to potential over-smoothing of interpolation/extrapolation.
%So for the successful training of PINNs, we need ensure the computability of the differentiation for time coordinates, i.e., the scale of the gradient is aligned without vanishing or explosion. 
   % Meanwhile, it is necessary to ensure that the model has the ability to propagate information directly over discrete time points, i.e., the ability to sequential modeling.

To respect the inherent \textit{Continuous-Discrete Duality} of PINNs, we propose to use the State Space Models (SSM)~\cite{gu2023mamba,gu2022efficiently} as network backbones. SSMs parametrically model a discrete temporal sequence as a continuous dynamic system. Instead of simple interpolation, its discrete form approximates the instantaneous states and changing rates described by its continuous form by integrating the system's dynamics over each time interval. Therefore, the discrete modeling process for SSMs can respond to the trajectory of a continuous system more precisely. Moreover, the parametric modeling of the instantaneous rate of change allows the differentiation over time to be unified to the same scale, mitigating the hard-to-optimize problem of PINNs caused by the differentiation scale inconsistency.
%SSMs parametrically model the state evolution of a continuous system. 

%SSMs effectively utilizes discrete time collection points to construct behavior of continuous dynamic systems.


    %In SSMs, there is a smooth articulation between its discrete-time form and continuous-time essence. Moreover, the differentiation of SSM for time is realized by multiplication between state transfer matrices, which effectively aligns the scales of the differentiation of each order. Therefore, it can avoid the PINNs' inherent difficulty of optimization due to the inconsistency of gradient scales.

Secondly, we remark that the sequence granularity is crucial for PINNs. Sequences with fine-grained granularity, i.e., the pseudo-sequence within a small neighborhood of collection points adopted in PINNsFormer~\cite{zhao2024pinnsformer}, cannot authentically reflect the propagation of information. On the contrary, using a large sequence can lead to an over-smoothed solution because of the imperfectly reliable supervision of PINNs. In other words, for data-free PINNs, every function in the feasible domain can make the loss at a collection point to zero. For example, in 1d-convection equation, $\bar u(x,t)=0$ leads to 0-loss at every collection point except at $t=0$. As these points contribute uniformly to the total loss over long sequences, the model can become trapped in an over-smoothed local optimum.
 %First, we remark that simply modeling the sequences over entire time intervals is unreliable, since PINNs are not data-driven, that their supervision is not a direct mapping from a data point sequence to a label sequence but residuals constructed from differential equations. In this physical law-driven learning paradigm, governing in late stages of a long sequence is ineffective, because there is a wide solution space in which functions can all make residual loss low but such functions lack information from previous time. 
%For exerting the influence of the initial conditions, short sequence modeling might be a useful inductive bias. Base on this, we propose a learning scheme with overlapping short sub-sequences. First, it constructs a sub-sequence at each time collection point to predict the output at the current collection point and the next few collection points. Then, at the next collection point, the output of model is softly aligned with previous predictions, as a way to propagate the information defined by the initial conditions over time.
To address this challenge, we propose a sub-sequential modeling approach with moderate granularity to enable correct propagation. First, it constructs a short sub-sequence from the collection points to fully propagate the initial condition. Then, it recursively advances through the entire time-domain sequence via a sub-sequence state contrastive alignment.

In this paper, we propose PINNMamba, a novel learning framework for numerically solving physics PDEs. Our approach leverages Selective SSMs (Mamba)~\cite{gu2023mamba} to model temporal sub-sequences, enabling effective capture of temporal dependencies in continuous dynamical systems while training on discretized collection points. PINNMamba successfully addresses the distortions typically induced by discrete-time sampling in neural PDE modeling. Through extensive experimentation, we demonstrate that PINNMamba achieves state-of-the-art performance, consistently outperforming existing approaches such as PINNsFormer~\cite{zhao2024pinnsformer} and QRes ~\cite{bu2021quadratic} over multiple challenging problems.

\textbf{Contributions.} We make the following contributions:
\vspace{-2mm}
\begin{itemize}
    \item We reveal that the mismatch between the discrete nature of the training collection points and the continuous nature of the function approximated by the PINN is an important factor in causing distortion in the propagation of the initial condition over time in the PINN.
    \vspace{-6mm}
    \item We also note that, for PINNs, the lack of direct supervision of their data-label pairs and the multitude of functions conforming to a single differential equation create an intrinsic difficulty in modeling long sequences.
    \vspace{-2mm}
    \item We propose PINNMamba, which eliminates the discrete-continuity mismatch and sub-sequentially models the systems, resulting in state-of-the-art performance on several PINN benchmarks.
\end{itemize}