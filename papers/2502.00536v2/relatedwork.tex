\section{Related Works}
Consistency learning plays a pivotal role in SSMIS by effectively utilizing large amounts of unlabeled data. The core idea is to encourage models to produce stable predictions under various perturbations or augmentations of the input data, thereby enhancing performance even with limited labeled examples. Early works, such as the study by Sajjadi et al. \cite{b7}, highlighted the importance of stochastic transformations like dropout and random augmentations to regularize deep networks, improving generalization and robustness against input variability. More recent studies have expanded on these ideas to enhance consistency in semi-supervised segmentation tasks; for instance, ConMatch\cite{b31} incorporates confidence-guided consistency regularization, refining pseudo-label confidence estimations to improve performance. Additionally, methods such as UDiCT\cite{b32} pair annotated and unannotated data based on uncertainty, thereby mitigating the impact of unreliable pseudo-labels, while Huang et al. \cite{b33} introduced a two-stage approach enforcing consistency across perturbed versions of unlabeled electron microscopy volumes to enhance model robustness.

In medical image segmentation, consistency learning addresses significant challenges posed by data variability and insufficient labeled data. Prominent methods have augmented consistency learning by integrating additional tasks or uncertainty estimations, as demonstrated by Shu et al. \cite{b34} and Wang et al. \cite{b35}. For instance, Liu et al. \cite{b36} applied transformer-based models to COVID-19 lesion segmentation, using consistency across augmented views to alleviate the shortage of labeled data. Moreover, Chen et al. \cite{b37} proposed Cross Pseudo Supervision (CPS), where two segmentation networks with different initializations enforce consistency on each other's predictions, effectively expanding training data via pseudo-labels. Another notable contribution is from Tarvainen and Valpola \cite{b9}, who introduced the Mean Teacher model, maintaining an exponential moving average (EMA) of model weights to improve segmentation performance under semi-supervised conditions. Collectively, these advances demonstrate how consistency learning, especially when combined with innovative model architectures and training strategies, significantly enhances segmentation performance with limited labeled data.




%