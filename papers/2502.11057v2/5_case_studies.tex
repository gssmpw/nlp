The objective of this paper is to demonstrate the co-optimization of performance and safety. To achieve this, we evaluate the proposed method and compare them with baselines using two metrics: (1) \textbf{Cumulative Cost:} This metric represents the total cost  $\int_0^{T}l(x(s)) ds + \phi(x(T))$, accumulated by a policy over the safe trajectories. (2) \textbf{Safety Rate:} This metric is defined as the percentage of trajectories that remain safe, i.e., never enter the failure region $\mathcal{F}$ at any point in time.
% \begin{enumerate}
%     \item \textbf{Cumulative Cost:} This metric represents the total cost  $\int_0^{T}l(x(s)) ds + \phi(x(T))$, accumulated by a policy over the safe trajectories.
%     \item \textbf{Safety Rate:} This metric is defined as the percentage of trajectories that remain safe, i.e., never enter the failure region $\mathcal{F}$ at any point in time.
% \end{enumerate}

\textbf{Baselines}: We consider two categories of baselines: the first set of methods aim to enhance the system performance (i.e., minimize the cumulative cost) while encouraging safety, encompassing methods such as Constrained Reinforcement Learning (CRL) and Model Predictive Path Integral (MPPI)~\cite{8558663} algorithms. 
The second category prioritizes safety,  potentially at the cost of performance. This includes safety filtering techniques such as Control Barrier Function (CBF)-based quadratic programs (QP) \cite{Ames_2017} that modify a nominal, potentially unsafe controller to satisfy the safety constraint. Additionally, we have presented the comparative study of offline and online computation times between the baselines in Appendix~\ref{app: comp_time}.
\vspace{-1em}
\subsection{Efficient and Safe Boat Navigation}
In our first experiment, we consider a 2D autonomous boat navigation problem, where a boat with coordinates $(x_b, y_b)$ navigates a river with state-dependent drift to reach an island. The boat must avoid two circular boulders (obstacles) of different radii, which corresponds to the safety constraint in the system (see Fig. \ref{fig: Boat_Trajectories}).
The cost function penalizes the distance to the goal.
The system state, $x$, evolves according to the dynamics:  
\vspace{-1em}
\begin{equation}
    x = [x_b, y_b], \quad \dot{x} = [u_1 + 2 - 0.5y_b^2, u_2]
\end{equation}
where $[u_1, u_2]$ are the bounded control inputs in the $x_b$ and $y_b$ directions, constrained by the control space $\mathcal{U} = \{[u_1, u_2] \in \mathbb{R}^2 \mid ||[u_1, u_2]|| \leq 1\}$. The term $2 - 0.5y_b^2$ introduces a state-dependent drift, complicating the control task as the actions must counteract the drift while ensuring safety, which is challenging under bounded control inputs. 
The rest of the details about the experiment setup can be found in the Appendix \ref{appendix: Boat2D}.

\textbf{Safety Guarantees and Performance Quantification}:
We use $N_s = 300K$ and $N_p = 300K$ samples for thorough verification, ensuring dense state space sampling. 
For this experiment, we set $\epsilon_s = 0.001$ and $\beta_s = 10^{-10}$, resulting in a $\delta$-level of $0$. 
This implies that, with $1 - 10^{-10}$ confidence, any state with $\learnedauxvfunc (t,x,z) \leq 0$, is safe with at least $99.9\%$ probability. 
For performance quantification, we set $\epsilon_p = 0.01$ and $\beta_p = 10^{-10}$, leading to a $\psi$-level of $0.136$. This ensures, with $1 - 10^{-10}$ confidence, that any state in $\mathcal{S}^*$ has a normalized error between the predicted value and the policy value of less than $0.136$ with $99\%$ probability. Low $\delta$ and $\psi$ values with high confidence indicate that the learned policy closely approximates the optimal policy and successfully co-optimizes safety and performance.

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{images/Boat_Traj_with_box.pdf}
    \vspace{-1em}
    \caption{Trajectories from two distinct initial states are shown, with dark grey circles representing obstacles and the green dot indicating the goal at $[1.5, 0]^T$. Notably, our method is the \textbf{only one} that \textbf{successfully approaches the goal} while \textbf{adhering to safety constraints}.}
    \vspace{-0.8em}
    \label{fig: Boat_Trajectories}
\end{figure}

\textbf{Baselines}: This being a 2-dimensional system, we compare our method with the ground truth value function computed by solving the HJB-PDE numerically using the Level Set Toolbox~\cite{mitchell2004toolbox} (results in Appendix~\ref{Appendix: GT_comp}). 
Additional baselines include: (1) MPPI, a sample-based path-planning algorithm with safety as soft constraints, (2) MPPI-NCBF, where safety is enforced using a Neural CBF-based QP with MPPI as the nominal controller~\cite{dawson2022safe,tayal2024learning}, and (3) C-SAC, a Lagrangian-based CRL approach using Soft Actor-Critic~\cite{pmlr-v80-haarnoja18b}, incorporating safety as soft constraints.


\textbf{Comparative Analysis:} Figure \ref{fig: Boat_Trajectories} shows that our method effectively reaches the goal while avoiding obstacles, even when starting close to them. In contrast, MPPI and C-SAC-based policies fail to maintain safety, while MPPI-NCBF ensures safety but performs poorly (leading to very slow trajectories). Figure \ref{fig: baseline_comparison} highlights that our method outperforms all others. C-SAC achieves reasonable performance with a $7.5\%$ higher mean cost compared to ours but has the lowest safety rate of $76\%$. MPPI, with a more competitive safety rate of $89\%$, performs poorly with a $32.67\%$ higher mean cost. MPPI-NCBF achieves $100\%$ safety but performs significantly worse, with a $50.72\%$ higher mean cost. Additionally, CBF-based controllers sometimes violate control bounds, limiting their applicability. This demonstrates that our method balances safety and performance, unlike others that compromise on one aspect. Moreover, the $100\%$ safety rate of our method aligns closely with at least $99.9\%$ safety level that we expect using our proposed verification strategy, providing empirical validation of the safety assurances.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.79\linewidth]{images/traj_plot_Boat.png}
% \caption{The figure shows a bunch of trajectories starting at random points propagating with optimal policy $\pi$. The orange circles represent the obstacles and the grey vector field denotes the river current drift at different points.} 
% \label{fig: traj_plot_boat}
% \end{figure}
\vspace{-0.5em}
\subsection{Pursuer Vehicle tracking a moving Evader}
In our second experiment, we consider an acceleration-driven pursuer vehicle, tracking a moving evader while avoiding five circular obstacles (see Fig. \ref{fig: Track_Trajectories}). This experiment involves an 8-dimensional system, with the state $x$ defined as $x = [x_p, y_p, v, \Theta, x_{e}, y_{e}, v_{xe}, v_{ye}]^T$, where $x_p, y_p, v, \Theta$ represent the coordinates, linear velocity, and orientation of the pursuer vehicle, respectively, and $x_e, y_e, v_{xe}, v_{ye}$ represent the coordinates and linear velocities of the evader vehicle. The pursuer vehicle is controlled by linear acceleration ($u_1$) and angular velocity ($u_2$). The control space is $\mathcal{U} = \{[u_1, u_2] \in [-2, 2]^2\}$. The complexity of this system stems from the dynamic nature of the goal, along with the challenge of
ensuring safety in a cluttered environment, which in itself is a difficult safety problem. More details about the experiment setup are in Appendix \ref{appendix: Track}.
% 
\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{images/Track_Traj_with_box.pdf}
    \vspace{-2em}
    \caption{Trajectories from two distinct initial states are depicted, with dark grey circles representing obstacles and purple trajectories indicating the evader's path, with arrows showing its direction of motion. Our method successfully \textbf{tracks the evader} while \textbf{avoiding collisions}, whereas all other methods either fail to maintain safety, struggle to track the evader or both}
    \vspace{-0.8em}
    \label{fig: Track_Trajectories}
\end{figure}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{images/MVC_Traj_3.pdf}
    \vspace{-2em}
    \caption{Snapshots of multi-agent navigation trajectories at different times using the proposed method. Agents are represented as circles with radius $R$, indicating the minimum safe distance they must maintain from each other. Smaller dots mark their respective goals. The trajectories show that agents proactively \textbf{maintain long-horizon safety} by adjusting their paths to avoid close encounters, rather than enforcing safety reactively, which could lead to suboptimal behaviors. Finally, the agents \textbf{reach their respective goals within the specified time horizon}.}
    \label{fig: MVC_Trajectories}
    \vspace{-0.8em}
\end{figure*}

\textbf{Safety Guarantees and Performance Quantification}:
Similar to the previous experiment, we set $N_s = N_p = 300k$. We choose $\epsilon_s = 0.01$ and $\beta_s = 10^{-10}$, yielding a $\delta$-level of $-0.04$ and a safety level of $99\%$ on the auxiliary value function. For performance, we set $\epsilon_p = 0.01$ and $\beta_p = 10^{-10}$, leading to a $\psi$-level of 0.137. These values indicate the learned policy maintains high safety with low-performance degradation in this cluttered environment.

\textbf{Baselines:} Similar to the previous experiment, we use MPPI and C-SAC with soft safety constraints as our baselines. For safety filtering, we apply a collision cone CBF (C3BF) \cite{10644338}-based QP due to its effectiveness in handling acceleration-driven systems.

\textbf{Comparative Analysis:} Figure \ref{fig: Track_Trajectories} shows that our method effectively tracks the moving evader while avoiding obstacles, even when starting close to them. In contrast, other methods have limitations: MPPI and C-SAC attempt to follow the evader but fail to maintain their pace, violating safety constraints, while MPPI-C3BF sacrifices performance to maintain safety. Figure \ref{fig: baseline_comparison} highlights our method's superior performance in balancing safety and performance. MPPI achieves the best performance among the baselines but with an 18\% higher mean cost and only a 72\% safety rate. MPPI-NCBF ensures 100\% safety but has a 42\% higher mean cost. C-SAC underperforms both in safety (66\% safety rate) and performance (101\% higher mean cost). This suggests that C-SAC struggles with co-optimizing safety and performance optimization in high-dimensional, complex systems. Additionally, the safety guarantees hold true in the test samples, confirming the reliability of our proposed safety verification framework in safety-critical, cluttered environments.

\textbf{Receding Horizon Control}: An interesting application of the synthesized policy is its deployment in a receding horizon fashion over a time horizon longer than that used for training the value function, as illustrated in Fig.~\ref{fig: receding_Track}. The results indicate that the learned policy successfully maintains safety while effectively tracking the evader over a $6$-second horizon, despite being trained over a horizon of $1$ second. This suggests that the proposed approach can be extended to effectively co-optimize safety and performance for long-horizon tasks by solving the SC-OCP over a shorter time horizon. Consequently, this framework offers a practical solution for real-world autonomous systems that require long-horizon safety and performance guarantees.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{images/Track_Traj_receding.pdf}
    \vspace{-1.3em}
\caption{Trajectories of the \textbf{receding horizon policy} for the pursuer tracking an evader over a $6$-second horizon, while the value function is trained over a $1$-second horizon. The results demonstrate that the pursuer \textbf{successfully tracks} the evader while ensuring safety, even when initialized near the obstacle. This highlights the effectiveness of the proposed approach in \textbf{jointly optimizing safety and performance for long-horizon tasks.}} 
\label{fig: receding_Track}
\vspace{-1.3em}
\end{figure}

\vspace{-0.8em}
\subsection{Multi-Agent Navigation}
In our third experiment, we consider a multi-agent setting where each of the 5 agents, represented by $x_i = [x_{a_i}, y_{a_i}, x_{g_i}, y_{g_i}]$, tries to reach its goal while avoiding collisions with others. $(x_{a_i}, y_{a_i})$ denote the position of the $i$th agent, while $(x_{g_i}, y_{g_i})$ represent the goal locations for that agent. The system is $20$-dimensional, with each agent controlled by its $x$ and $y$ velocities. The control space for each agent is $\mathcal{U}_i = \{[v_{x_i}, v_{y_i}] \mid ||[v_{x_i}, v_{y_i}]|| \leq 1\}$. 
% The final control space for this problem will be the Cartesian product of the control spaces for each agent. 
The complexity of this system stems from the interactions and potential conflicts between agents as they attempt to reach their goals while avoiding collisions. The rest of the details about the experiment setup can be found in Appendix \ref{appendix: MultiAgent}.
% 
% \begin{figure}[h]
%     \centering
%     \includegraphics[width=1.0\linewidth]{images/MVC_Traj_3.pdf}
%     \caption{Snapshots of multi-agent navigation trajectories at different time instances using our method. Agents are represented as circles with radius $R$, indicating the minimum safe distance they must maintain from each other. Smaller dots mark their respective goals. The trajectories show that agents proactively maintain long-horizon safety by adjusting their paths to avoid close encounters, rather than enforcing safety reactively, which could lead to suboptimal behaviors.}
%     \label{fig: MVC_Trajectories}
% \end{figure}

\textbf{Safety Guarantees and Performance Quantification}: We set $N_s = N_p = 300k$, $\epsilon_s = 0.001$, and $\beta_s = 10^{-10}$, resulting in a $\delta$-level of $-0.09$ with safety assurance of $99.9\%$ for the auxiliary value function. 
For performance quantification, we set $\epsilon_p = 0.01$ and $\beta_p = 10^{-10}$, leading to a $\psi$-level of 0.068.
It is evident that the $\delta$ and $\psi$ values remain very low with high confidence, highlighting the effectiveness of our method in co-optimizing safety and performance for high-dimensional, multi-agent systems.


\textbf{Baselines:} Similar to previous experiments, we have used MPPI, C-SAC, and MPPI-NCBF as our baselines for this experiment too.

\textbf{Comparative Analysis:} Figure~\ref{fig: MVC_Trajectories} shows that our method ensures long-horizon safety while enabling all agents to reach their goals without collisions. In contrast, the baseline methods either exhibit overly conservative behavior or fail to maintain safety, leading to collisions, as detailed in Appendix~\ref{app: MVC_baselines}. Figure~\ref{fig: baseline_comparison} demonstrates the superior performance of our approach, with MPPI, MPPI-NCBF, and C-SAC showing mean percentage cost increases of 148\%, 192\%, and 164\%, respectively. Although MPPI and MPPI-NCBF achieve competitive safety rates of 90\% and 100\%, their significant performance degradation highlights their inability to balance safety and performance in complex systems. MPPI's subpar performance stems from its reliance on locally optimal solutions in a finite data regime, leading to several deadlocks along the way and overall suboptimal trajectories over a long horizon. Furthermore, C-SAC struggles with both safety and performance, further demonstrating its limitations in handling increasing system complexity and dimensionality. These results confirm our method's ability to co-optimize safety and performance in high-dimensional systems, demonstrating its scalability. Additionally, the safety guarantees hold in the test samples, validating the scalability of our safety verification framework for multi-agent systems.



