\subsection{Hamilton Jacobi Reachability}
Hamilton Jacobi Reachability \cite{mitchell2005time, lygeros2004reachability} formulates the computation of BRTs as an optimal control problem. 
% which amounts to solving a partial differential equation. 
%
% Our work builds upon DeepReach, a neural PDE solver that leverages HJ reachability to compute BRTs for high-dimensional systems. 
%
% This section provides a concise overview of both methods. 
% 
% \subsection{Hamilton-Jacobi Reachability}
% Typically, users start with specifying a target function $l: \mathbb{R}^n \rightarrow \mathbb{R}$, with its sub-zero level set being the failure set :
The first step is to define a Lipschitz-continuous target function $g: \mathbb{R}^n \rightarrow \mathbb{R}$ whose sub-zero level set is the failure set: $\mathcal{L}=\{x: g(x) \leq 0\}$.
A common choice for $g$ in robotics is the signed distance function to $\mathcal{L}$.
% %
% Then HJ reachability synthesizes BRT by solving a differential game between the control and disturbance under non-anticipative strategies. 
% %
% The cost of the differential game is given by the minimum distance to $\mathcal{L}$ throughout the system trajectory over time with applying  and disturbance  being applied to initial state $x$.
%
Given $g$, the BRT is obtained by computing the minimum distance to $\mathcal{L}$ throughout the system trajectory under the optimal control:
% 
\vspace{-0.5em}
\begin{equation}
  V(x,t)  = \sup_{u(\cdot)} ~ \min _{\tau \in[t, T]} g\left(\xi_{x, t}^{u}(\tau)\right).
  \vspace{-0.5em}
\end{equation}
% 
Intuitively, the optimal control tries to avoid and maximize the distance from the failure region. 
The sign of the value function $V(\cdot)$ indicates whether the system entered the failure set under this optimal control. Thus, the BRT is given by the sub-zero level set of the value function:
% 
 % is the set of all assuredly safe states $x$ at time $t$ from which the system will always enter the target set $\mathcal{L}$ during the time horizon $[t,T]$ if optimal $u(\cdot)$ is applied. 
%
% Therefore, this sub-zero level set is precisely the BRT,
\vspace{-0.5em}
\begin{equation} \label{eq:BRT_from_value}
\mathcal{B}(t)=\left\{x: V(x,t)  \leq 0 \right\}.
\end{equation}
%

The value function can be computed via solving the following Hamilton-Jacobi-Bellman Variational Inequality (HJB-VI), which yields an identical solution as the HJB PDE while empirically providing a stronger learning signal for the NN:
% 
\begin{equation}
\begin{gathered}
\label{eq: HJI-VI}
     \min \{D_{t}V(x,t) + H(x,t), g(x) - V(x,t) \} = 0, \\
     V(x,T) = l(x),\\
    H(x,t) = \max_{u \in \mathcal{U}} \langle \nabla V(x,t) \; , \; f(x,u) \rangle, 
\end{gathered}
\end{equation}
% 
where $\nabla$ and $D_t$ denote the spatial and time derivatives of the value function.
$H(x,t)$ is the Hamiltonian of the system that encodes the role of control and how it affects the value function.
% and $H(x,t)$ is the Hamiltonian, which intuitively represents the maximum changing rate of target function that the system could achieve at the terminal state of the optimal trajectory. 
% The term $l(x) - V(x,t)$ enforces the value function to be non-increasing over time, preventing the system from entering and then leaving the target set.
The boundary condition for the HJB-VI is given by $l(x)$, which defines the safety constraint for the system. 
For a detailed explanation and derivation, we refer interested readers to \cite{lygeros2004reachability, bokanowski2010reachability, 8263977}.

Once the value function is computed, the BRT is given by \eqref{eq:BRT_from_value}. 
Along with the BRT, the value function also provides an optimal safe policy for the system:
\begin{equation} \label{eqn:opt_ctrl}
    \pi^*(x,t)=\arg \max _u \langle\nabla V(x, t), f(x, u)\rangle .
\end{equation}
Intuitively, the safety controller steers the system towards higher values (i.e., away from the failure set) at any state $x$.

Conversely, when $\mathcal{L}$ denotes a target set, the same framework can be used to compute the BRT, except that the control minimizes the Hamiltonian in \eqref{eq: HJI-VI} and \eqref{eqn:opt_ctrl}.
% 
% BRT $\mathcal{B}$ is the subzero level set of the value function maximized by $u(\cdot)$ and minimized by $d(\cdot)$:
% \begin{equation}
%   V(x,t)  = \inf_{d(\cdot)}\sup_{u(\cdot)} ~ \min _{\tau \in[t, T]} l\left(\xi_{x, t}^{u, d}(\tau)\right).
% \end{equation}
% The value function can also be solved by (\ref{eq: HJI-VI}) except the Hamiltonian is given by
% \begin{equation}
%     H(x,t) = \max_{u \in \mathcal{U}}\min_{d \in \mathcal{D}} \langle \nabla V(x,t) \; , \; f(x,u,d) \rangle.
% \end{equation}
% Similarly, the safety reachability controller is given as
% % 
% \begin{equation}
%     \pi^*(x,t)=\arg \max _u \min _d\langle\nabla V(x, t), f(x, u, d)\rangle .
% \end{equation}
% % 


\subsection{DeepReach}\label{DeepReach}
Traditionally, numerical methods are employed to solve the HJB-VI over a grid representation of the state space \cite{mitchell2004toolbox, pythonhjtoolbox}, wherein the time and spatial derivatives are approximated numerically over the grid to obtain a solution to the HJB PDE.
While the grid-based methods offer accurate solutions for low-dimensional problems, they suffer from the curse of dimensionality. Consequently, learning-based methods, such as DeepReach \cite{9561949}, have been developed to solve HJB-VI for high-dimensional cases. 
DeepReach leverages a DNN, parameterized by $\theta$, to approximate the value function $V_\theta(x,t)$ via self-supervised learning. 
The loss function for learning is composed of a PDE violation loss and a boundary condition loss with a trade-off parameter $\lambda$:
% 
% DeepReach is a learning-based method to solve the HJI-VI in \eqref{eq: HJI-VI} that is designed to tackle high-dimensional reachability problems \cite{9561949}. 
% It leverages a deep neural network (DNN) parameterized by $\theta$ with a sinusoidal activation function to predict approximate value function $V_\theta(x,t)$.
% % 
% DeepReach (called \textit{vanilla DeepReach} here on to differentiate it from the proposed variant) learns the value function in a self-supervised manner, with a loss function composed of a PDE violation loss and a boundary condition loss with a trade-off parameter $\lambda$:
% 
\begin{equation}
\label{eq: DeepReach_loss}
\begin{aligned}
& h\left(x_i, t_i ; \theta\right)=h_{pde}\left(x_i, t_i ; \theta\right)+\lambda h_{bc}\left(x_i, t_i ; \theta\right), \\
& h_{pde}\left(x_i, t_i ; \theta\right)=\| \min \left\{D_t V_\theta\left(x_i, t_i\right)+H\left(x_i, t_i\right),\right. \\
& \qquad \qquad \left.l\left(x_i\right)-V_\theta\left(x_i, t_i\right)\right\} \| ,\\
& h_{bc}\left(x_i, t_i ; \theta\right)=\left\|V_{\theta}\left(x_i, t_i \right)-l\left(x_i \right)\right\| \mathds{1}\left(t_i=T\right).
\end{aligned}
\end{equation}
% 
Here, $h_{pde}$ corresponds to how consistent the value function propagation is with the HJB-VI, whereas $h_{bc}$ attempts to impose correct value function approximation at the terminal time. 
Even though DeepReach is shown to be effective for high-dimensional problems, the solution quality depends heavily on the imposition of $h_{bc}$ and $\lambda$, as we demonstrate later in this paper. 
Our goal is to reduce these dependencies in order to obtain a more accurate safety value function.
