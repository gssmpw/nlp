\onecolumn
\appendices
% \begin{appendices}
% \section{Derivation of epigraph reformulation of a state constraint optimal control problem}\label{appendix: epigraph}
% Consider a state constraint optimal control problem of the form:
% \begin{equation}
%     \begin{aligned}
%     \min_{x} & \; C(x) \\
%     \text{s.t.} & \; s(x) \leq 0,
% \end{aligned}
% \end{equation}
% where, $C(x)$ denotes the cost function and $s(x)$ denotes the state constraint. The epigraph form of the above problem is given by:
% \begin{equation}\label{eq: epi_example}
%     \begin{aligned}
%     \min_{x, z} & \; z  \\
%     \text{s.t.} & \; s(x) \leq 0,  \\
%     & \; C(x) \leq z. 
%     \end{aligned}
% \end{equation}
% Here, $z \in \mathbb{R}$ is an auxiliary optimization variable. Now, observe that the constraints in \eqref{eq: epi_example} can be combined to yield the following:
% \begin{equation}
%     \begin{aligned}
%     \min_{x, z} & \; z  \\
%     \text{s.t.} & \; \max \big(C(x) - z, s(x) \big) \leq 0. 
% \end{aligned}
% \end{equation}

% Now, using the equivalence shown in appendix \ref{appendix: equivalence}, we can say:
% \begin{equation}
%     \begin{aligned}
%     \min_{z} & \; z  \\
%     \text{s.t.} & \; \min_x\max \big(C(x) - z, s(x) \big) \leq 0. 
% \end{aligned}
% \end{equation}


% Now, substituting $C(x)$ and $s(x)$ in the context of the SC-OCP in \eqref{eq: SC-OCP}, we get the following epigraph form:

% \begin{equation}
%     \begin{aligned}
%     \min_{z \in \mathbb{R^+}} & \; z  \\
%     \text{s.t.} \; \max &\left( \min_{u \in \mathcal{U}}\int_t^{T}l(x) \, ds + \phi(x(T))  -z, g(x) \right) \leq 0,
% \end{aligned}
% \end{equation}



% % As $z \to \infty$ (i.e., the "cost budget" for $C(x)$ increases), $s(x)$ will dominate the max, and $x^*$ will focus on minimizing $h$ more. On the other hand, as $z \to -\infty$ (i.e., the "cost budget" for $C(x)$ decreases), $C(x)$ will dominate the max, and $x^*$ will focus on minimizing $C$.

% \subsection{Equivalence of (24) and (25)} \label{appendix: equivalence}

% \begin{lemma}
% Let $x \in \mathbb{R}^n$ and $z \in \mathbb{R}$, and let $g : \mathbb{R}^n \times \mathbb{R} \to \mathbb{R}$ be a continuous (potentially non-differentiable) function. Then, if a solution exists (i.e., an optimal $x^*, z^*$ exist, are finite), then the following optimization problems are equivalent.
% \begin{align}
%     \min_{x, z} & \quad z \\
%     \text{s.t.} & \quad g(x, z) \leq 0,
% \end{align}
% \begin{align}
%     \min_z & \quad z \\
%     \text{s.t.} & \quad \min_x g(x, z) \leq 0.
% \end{align}
% \end{lemma}

% We begin by comparing the Lagrangian primal problem of eq. (A.16) and eq. (A.17).
% \begin{align}
%     \min_x \max_{\lambda \geq 0} \quad z + \lambda g(x, z) \tag{A.18}
% \end{align}
% \begin{align}
%     \min_z \max_{\lambda \geq 0} \quad \lambda \min_x g(x, z) \tag{A.19}
% \end{align}
% Comparing the two, the only difference is that the order of $\min_x$ and $\max_\lambda$ are flipped. Hence, it is sufficient to show that, for any $z$ where $\min_x g(x, z) \leq 0$, 
% \begin{align}
%     p^* = \min_x \max_{\lambda \geq 0} \quad \lambda g(x, z) = \max_{\lambda \geq 0} \min_x \lambda g(x, z) = d^*. 
% \end{align}
% Note that this is exactly equivalent to showing that strong duality holds for the following constraint satisfaction problem.
% \begin{align}
%     \min_x & \quad 0 \\
%     \text{s.t.} & \quad g(x, z) \leq 0. \tag{A.21}
% \end{align}

% We now prove that strong duality holds for the above problem in a similar fashion to the proof that Slater’s condition is a sufficient condition for strong duality to hold in convex optimization problems \cite{8}.

% Define the set $A \subseteq \mathbb{R}^n \times \mathbb{R}$ as
% \begin{align}
%     A &= \{ (u, t) \mid \exists x, g(x, z) \leq u, \quad 0 \leq t \}, \tag{A.22}\\
%     &= \{ u \mid \inf_x g(x, z) \leq u \} \times [0, \infty). \tag{A.23}
% \end{align}

% Note that $A$ is convex. Furthermore, since a feasible solution exists by assumption, we have that
% \begin{align}
%     p^* = \min_x \max_{\lambda \geq 0} \lambda g(x, z) = \min_x \begin{cases} 
%     \infty & g(x, z) > 0 \\
%     0 & g(x, z) \leq 0.
%     \end{cases} \tag{A.24}
% \end{align}

% We now define a second set $B \subseteq \mathbb{R}^n \times \mathbb{R}$ as
% \begin{align}
%     B &= \{ (0, s) \mid s < p^* \}, \tag{A.25}\\
%     &= \{ 0 \} \times (-\infty, 0). \tag{A.26}
% \end{align}

% Note that $B$ is also convex, and that the sets $A$ and $B$ do not intersect. We can then invoke the separating hyperplane theorem to show that there exists a $(\lambda, \mu) \neq 0$ and $\alpha$ that defines a hyperplane which separates the two sets, i.e.,
% \begin{align}
%     (u, t) \in A &\Rightarrow -\lambda u + \mu t \geq \alpha, \tag{A.27}\\
%     (u, s) \in B &\Rightarrow -\lambda u + \mu s \leq \alpha. \tag{A.28}
% \end{align}

% In (A.27), since both $u$ and $t$ are unbounded above, we must have $\lambda \geq 0$ and $\mu \geq 0$. Furthermore, in (A.28), since $s < p^*$, we have that $\mu p^* \leq \alpha$. Combining both then gives us that for all $x$,
% \begin{align}
%     0 = p^* = \mu p^* \leq \alpha \leq \lambda g(x, z). \tag{A.29}
% \end{align}

% Minimizing the RHS over $x$ then maximizing over $\lambda$ then gives us that
% \begin{align}
%     p^* \leq \max_{\lambda} \min_x \lambda g(x, z) \leq \max_{\lambda} \min_x \lambda g(x, z) = d^*. \tag{A.30}
% \end{align}

% Finally, by weak duality, we have that
% \begin{align}
%     p^* \geq d^*. \tag{A.31}
% \end{align}

% Combining the two then allows us to conclude that $p^* = d^*$. \qed

% \section{Derivation of HJB-PDE characterizing the auxiliary value function} \label{appendix: proof_aux_hjb_pde}
% The auxiliary value function is given by:
% \begin{align}
%     \hat{V}(t, x, z) = \max \left( \min_{u \in \mathcal{U}} \int_t^{T} l(x) \, ds + \phi(x(T)) - z, \, g(x) \right).
% \end{align}

% Next, consider the dynamic programming equation for $\hat{V}$. Over a small time interval $\Delta t$, we can express $\hat{V}$ as:

% \begin{equation}
%     \hat{V}(t, x, z) = \int_t^{t + \Delta t} l(x) \, ds + \big(z(t + \Delta t) - z(t)\big) + \hat{V}\big(t + \Delta t, x(t + \Delta t), z(t + \Delta t)\big)
% \end{equation}
% $\forall t \in [0, T)$ and $(x, z) \in \mathcal{X} \times \mathbb{R}^+$.
% Expanding $\hat{V}$ using a Taylor series around $t$, we get:
% \begin{align}
%     \hat{V}(t, x, z) = \int_t^{t + \Delta t} l(x) \, ds + \big(\dot{z} \Delta t\big) + \hat{V}(t, x, z) + \partial_t \hat{V} \, \Delta t 
%     + \min_{u \in \mathcal{U}} \nabla_x \hat{V} \cdot f(t, x, u) \, \Delta t + \nabla_z \hat{V} \cdot \dot{z} \, \Delta t
% \end{align}
% $\forall t \in [0, T)$ and $(x, z) \in \mathcal{X} \times \mathbb{R}^+$,
% Canceling out $\hat{V}(t, x, z)$ on both sides and simplifying terms yields:
% \begin{equation}
%     -\partial_t \hat{V} - \min_{u \in \mathcal{U}} \nabla_x \hat{V} \cdot f(t, x, u) + \nabla_z \hat{V} \cdot l(x) = 0,
% \end{equation}
% $\forall t \in [0, T)$ and $(x, z) \in \mathcal{X} \times \mathbb{R}^+$. 
% We can now rewrite the final partial differential equation (PDE) for $\hat{V}$ as:
% \begin{equation}
%     \min \Bigl(-\partial_t \hat{V} - \min_{u \in \mathcal{U}} \nabla_x \hat{V} \cdot f(t, x, u) + \nabla_z \hat{V} \cdot l(x), \, \hat{V} - g(x) \Bigr) = 0,
% \end{equation}
% $\forall t \in [0, T)$ and $(x, z) \in \mathcal{X} \times \mathbb{R}^+$,

% or equivalently:
% \begin{equation}
%     \min \Bigl(-\partial_t \hat{V} - \min_{u \in \mathcal{U}} \nabla_{\hat{x}} \hat{V} \cdot \hat{f}(t, \hat{x}, u), \, \hat{V} - g(x) \Bigr) = 0,
% \end{equation}

% $\forall t \in [0, T)$ and $(x, z) \in \mathcal{X} \times \mathbb{R}^+$.

\section{Proofs}

\subsection{Proof of Theorem \eqref{thm: safety_verification}}\label{appendix: proof_safety}

% \begin{equation}
%     \delta^* = \min(\hat{V}(0,x,z)|J_{\Tilde{\pi}}(0,x,z)\geq 0)
% \end{equation}
Before we proceed with the proof of the Theorem \eqref{thm: safety_verification}, let us look at the following lemma which describes split conformal prediction:

\begin{lemma}[Split Conformal Prediction \cite{angelopoulos2022gentle}]
\label{lem:split_conformal}
Consider a set of independent and identically distributed (i.i.d.) calibration data, denoted as \(\{(X_i, Y_i)\}_{i=1}^n\), along with a new test point \((X_{\text{test}}, Y_{\text{test}})\) sampled independently from the same distribution. Define a score function \(s(x, y) \in \mathbb{R}\), where higher scores indicate poorer alignment between \(x\) and \(y\). Compute the calibration scores \(s_1 = s(X_1, Y_1), \ldots, s_n = s(X_n, Y_n)\). For a user-defined confidence level \(1-\alpha\), let \(\hat{q}\) represent the \(\lceil (n+1)(1-\alpha) \rceil / n\) quantile of these scores. Construct the prediction set for the test input \(X_{\text{test}}\) as:
\[
\mathcal{C}(X_{\text{test}}) = \{y : s(X_{\text{test}}, y) \leq \hat{q} \}.
\]
Assuming exchangeability, the prediction set \(\mathcal{C}(X_{\text{test}})\) guarantees the marginal coverage property:
\[
\mathbb{P}(Y_{\text{test}} \in \mathcal{C}(X_{\text{test}})) \geq 1 - \alpha.
\]
\end{lemma}

\begin{proof}
    Following the Lemma \ref{lem:split_conformal}, we employ a conformal scoring function for safety verification~\cite{pmlr-v242-lin24a}, defined as:
\begin{equation*}
    s(X) = \inducedauxvfunc(0, \hat{x}),    \forall \hat{x} \in \mathcal{S}_{\tilde\delta},
\end{equation*}
where $\mathcal{S}_{\delta}$ denotes the set of states satisfying $\learnedauxvfunc(0, \hat{x}) \leq \delta$ and the score function measures the alignment between the induced safe policy and the auxiliary value function.

Next, we sample $N_s$ states from the safe set $\mathcal{S}_{\delta}$ and compute conformal scores for all sampled states. For a user-defined error rate $\alpha \in [0, 1]$, let $\hat{q}$ denote the $\frac{(N_s+1)\alpha}{N_s}$th quantile of the conformal scores. According to \cite{vovk2012}, the following property holds:
\begin{equation}
    \underset{\hat{x} \in \mathcal{S}_{\tilde\delta}}{\mathbb{P}}\left(\inducedauxvfunc(0, \hat{x_i}) \leq \hat{q} \right) \sim \text{Beta}(N_s - l + 1, l),    
\end{equation}

where $l = \lfloor (N_s+1)\alpha \rfloor $.

Define $E_s$  as:
\begin{equation*}
    E_s := \underset{\hat{x} \in \mathcal{S}_{\delta}}{\mathbb{P}}\left(\inducedauxvfunc(0, \hat{x}_i) \leq \hat{q}\right).
\end{equation*}
Here, $E_s$ is a Beta-distributed random variable. Using properties of cumulative distribution functions (CDF), we assert that $E_s \geq 1 - \epsilon_s$ with confidence $1 - \beta_s$ if the following condition is satisfied:
\begin{equation} \label{eq:eps_calc_safe}
    I_{1-\epsilon_s}(N - l + 1, l) \leq \beta_s,
\end{equation}
where $I_x(a,b)$ is the regularized incomplete Beta function and also serves as the CDF of the Beta distribution. It is defined as:
\begin{equation*}
    I_x(a, b) = \frac{1}{B(a, b)} \int_{0}^{x} t^{a - 1} (1 - t)^{b - 1} \, dt,
\end{equation*}
where $B(a, b)$  is the Beta function. From~\cite{DLMF}($8.17.5$), it can be shown that $I_x(n-k, k+1) = \sum_{i=1}^{k} \binom{n}{i} x^i (1 - x)^{n - i}$.

% If $\sum_{i=1}^{l+1} \binom{N_s}{i} \epsilon_s^i (1 - \epsilon)^{N_s - i} \leq \beta_s$, $E_s \geq 1-\epsilon_s$.

Then \eqref{eq:eps_calc_safe} can be rewritten as: 
\begin{equation} \label{eq:eps_calc_safe_binom}
    \sum_{i=1}^{l-1} \binom{N_s}{i} \epsilon_s^i (1 - \epsilon)^{N_s - i} \leq \beta_s,
\end{equation}
Thus, if Equation~\eqref{eq:eps_calc_safe_binom} holds, we can say with probability $1-\beta_s$ that:
\begin{equation}\label{eq: cp_quantile}
    \underset{\hat{x} \in \mathcal{S}_{\tilde\delta}}{\mathbb{P}}\left(\inducedauxvfunc(0, \hat{x_i}) \leq \hat{q} \right) \geq 1-\epsilon_s.
\end{equation}
Now, let $k$ denote the cardinality of the set  
$A = \{x \in S_{\delta} \mid \learnedauxvfunc(0, \hat{x}) \leq \delta ,\inducedauxvfunc(0 , \hat{x}) \geq 0\}$. Thus, the safety error rate is given by  $ \alpha_{\delta} = \frac{k+1}{N_s+1}$.
Let $\hat{q}$ represent the $\frac{(N_s+1)\alpha_{\delta}}{N_s}$th quantile of the conformal scores. Since $k$ denotes the number of samples for which the conformal score is positive, the $\frac{(N_s+1)\alpha_{\delta}}{N_s}$th quantile of scores corresponds to the maximum \textit{negative score} amongst the sampled states. This implies that $\hat{q} \leq 0$. From this and Equation~\eqref{eq: cp_quantile}, we can conclude with probability $1 - \beta_s$ that:
\begin{equation*}
    \underset{\hat{x} \in \mathcal{S}_{\delta}}{\mathbb{P}}\left(\inducedauxvfunc(0, \hat{x_i}) \leq 0 \right) \geq 1-\epsilon_s.
\end{equation*}

From Equation~\eqref{eq: aux_vfunc_def}, it can be inferred that $\forall~(t, \hat{x})$, $\hat{V}(t, \hat{x}) \leq \inducedauxvfunc(0, \hat{x})$. Hence, with probability $1 - \beta_s$, the following holds:
\begin{equation*}
    \underset{\hat{x} \in \mathcal{S}_{\delta}}{\mathbb{P}}\left(\hat{V}(0, \hat{x_i}) \leq 0 \right) \geq 1-\epsilon_s.
\end{equation*}
\end{proof}



\subsection{Proof of Theorem \eqref{thm: perf_verification}}\label{appendix: proof_perf}
\begin{proof}
    To quantify the performance loss, we employ a conformal scoring function defined as:

\begin{equation*}
    p(x) := \frac{|\learnedvfunc(0, x_i) - \inducedvfunc(0, x_i)|}{C_{max}}, \forall x \in \mathcal{S}^*
\end{equation*}
where the score function measures the alignment between the induced optimal policy and the value function.

Next, we sample \( N_p \) states from the state space \( \mathcal{S}^* \) and compute conformal scores for all sampled states. For a user-defined error rate \( \alpha_p \in [0, 1] \), let \( \psi \) denote the \(\frac{(N_p+1)\alpha_p}{N_p}\) quantile of the conformal scores. According to \cite{vovk2012}, the following property holds:
\[
\underset{x \in \mathcal{S}^*}{\mathbb{P}}\left(\frac{|\learnedvfunc(0, x_i) - \inducedvfunc(0, x_i)|}{C_{max}} \leq \psi \right) \sim \text{Beta}(N_p - l + 1, l),
\]
where \( l = \lfloor (N_p+1)\alpha_p \rfloor \).

Define \( E_p \) as:
\[
E_p := \underset{x \in \mathcal{S}^*}{\mathbb{P}}\left(\frac{|\learnedvfunc(0, x_i) - \inducedvfunc(0, x_i)|}{C_{max}} \leq \psi\right).
\]
Here, \( E_p \) is a Beta-distributed random variable. Using properties of CDF, we assert that \( E_p \geq 1 - \epsilon_p \) with confidence \( 1 - \beta_p \) if the following condition is satisfied:
\begin{equation} \label{eq:eps_calc_perf}
    I_{1-\epsilon_p}(N_p - l + 1, l) \leq \beta_p,
\end{equation}
where \( I_x(a,b) \) is the regularized incomplete Beta function. From~\cite{DLMF}($8.17.5$), it can be shown that $I_x(n-k, k+1) = \sum_{i=1}^{k} \binom{n}{i} x^i (1 - x)^{n - i}$. Hence, Equation~\eqref{eq:eps_calc_perf} can be equivalently stated as:
\begin{equation} \label{eq:eps_calc_perf_binom}
    \sum_{i=1}^{l-1} \binom{N_p}{i} \epsilon_p^i (1 - \epsilon_p)^{N_p - i} \leq \beta_p
\end{equation}

Thus, if Equation~\eqref{eq:eps_calc_perf_binom} holds, we can conclude with probability \( 1-\beta_p \) that:
\[
\underset{x \in \mathcal{S}^*}{\mathbb{P}}\left(\frac{|\learnedvfunc(0, x_i) - \inducedvfunc(0, x_i)|}{C_{max}} \leq \psi \right) \geq 1-\epsilon_p.
\]
\end{proof}

\vspace{1em}
\section{Additional Details of the systems in the experiments}\label{appendix: system_details}

In this section, we will provide more details about the systems we have used in the experiments section \ref{section: case_studies}.


\subsection{2D Boat} \label{appendix: Boat2D}
The states, $x$ of the 2D Boat system are $x = [x_1, x_2]^T$, where, $x_1, x_2$ are the $x$ and $y$ coordinates of the boat respectively. We define the step cost at each step, $l(t,x)$, as the distance from the goal, given by:
\begin{align*}
    l(t,x) :=  \|x- (1.5, 0)^T\|
\end{align*}
The cost function $C(t, x(t))$ is defined as:  
\begin{equation}
 \begin{aligned}
     C(t,x(t), \ctrlseq) = \int_t^{T} l(t, x(t)) \, dt ~ +
     \phi(x(T))
   \end{aligned}
\end{equation}
where $T$ is the time horizon ($2s$ in our experiment), $l(t, x(t)) = || x(t) - (1.5, 0)^T||$ represents the running cost, and $\phi(x(T)) = || x(T) - (1.5, 0)^T||$ is the terminal cost. Minimizing this cost drives the boat toward the island.

Consequently, the (augmented) dynamics of the 2D Boat system are:
\begin{align*}
    \dot{x_1} &= u_1 + 2 - 0.5 x_2^2 \\
    \dot{x_2} &= u_2 \\
    \dot{z} &= - l(t,x)
\end{align*}
where $u_1, u_2$ represents the velocity control in $x_1$ and $x_2$ directions respectively, with $u_1^2 + u_2^2 \leq 1$
and $2 - 0.5x_2^2$ specifies the current drift along the $x_1$-axis.

% \begin{equation*}
%     \max \left(\partial_t \hat{V} + \min(0,  -\|\nabla_{x_1,x_2} \hat{V}\| - \partial_z \hat{V}) +\partial_x \hat{V}(2-0.5x_2^2),~g(x)- \hat{V}\right) = 0,
% \end{equation*}
The safety constraints are formulated as:  
\begin{align}
    g(x) := max ( 0.4 - \|x - (-0.5, 0.5)^T \|,  0.5 - \|x - (-1.0, -1.2)^T \|) )
\end{align}
where $g(x) > 0$ indicates that the boat is inside a boulder, thereby ensuring that the super-level set of $g(x)$ defines the failure region.

\subsubsection{Ground Truth Comparison}\label{Appendix: GT_comp}
We compute the Ground Truth value function using the Level-Set Toolbox~\cite{mitchell2004toolbox} and use it as a benchmark in our comparative analysis. To facilitate demonstration, unsafe states are assigned a high value of $20$ instead of $\infty$. The value function in this problem ranges from $0$ to $14.76$.

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{images/GT_Heat_Map.pdf}
\caption{Heatmap of the value function for the ground truth (left) and our method (right). The yellow region represents the unsafe area. Our method successfully captures most of the safe set, indicating that it is not overly conservative while completely recovering the unsafe regions.}
\label{fig: GT Comparison}
\end{figure}

As illustrated in Figure~\ref{fig: GT Comparison}, the value function obtained using our method closely approximates the ground truth value function. Notably, the unsafe region (highlighted in yellow) remains identical in both cases, confirming the safety of the learned value function. Furthermore, the mean squared error (MSE) between the two value functions is $0.36$, which is relatively low given the broad range of possible values. 

It is also worth mentioning that computing a high-fidelity ground truth value function on a $210 \times 210 \times 210$ grid using the Level Set Toolbox requires approximately $390$ minutes~\cite{10777061}. In contrast, our proposed approach learns the value function in $122$ minutes, achieving a substantial speedup. This demonstrates that even for systems with a relatively low-dimensional state space, our method efficiently recovers an accurate value function significantly faster than grid-based solvers.

\subsubsection{Calculation of Safety Levels}
\begin{figure}[h]
\centering
\includegraphics[width=0.5\linewidth]{images/epsilon_vs_delta.pdf}
\vspace{-1em}
\caption{$\delta$ versus $\epsilon$ for the learned auxiliary value function, $\hat{V}(t, \hat{x})$ after safety verification. It can be observed that $\delta$ varies with different safety violation levels, $\epsilon$. Notably, $\delta$ approaches zero for sufficiently small $\epsilon$, indicating that the learned value function exhibits very few safety violations.}
\label{fig: Delta_vs_Epsilon}
% \vspace{-1em}
\end{figure}

Figure~\ref{fig: Delta_vs_Epsilon} illustrate the $\delta$ vs $\epsilon$ plot obtained after the safety verification algorithm proposed in Theorem~\ref{thm: safety_verification}. We can observe that the $\delta$ level approaches $0$ as the $\epsilon$ values approach the chosen safety level of $0.001$. Hence, we say that the sub-level set of the auxiliary value function, $\hat{V}(t, \hat{x})$ is safe with a probability of $1-0.001=0.999$.  

\subsection{Pursuer vehicle tracking an evader}\label{appendix: Track}
The state, $x$ of a ground vehicle (pursuer) tracking a moving evader is $x = [x_p, y_p, v, \Theta, x_{e}, y_{e}, v_{xe}, v_{ye}]^T$, where, $x_e, y_e, v, \Theta$ are position, linear velocity and orientation of the pursuer respectively, $x_{e}, y_{e}, v_{xe}, v_{ye}$ are the position and the linear velocities of the evader respectively. We define the step cost at each step, $l(t,x)$, as the distance from the goal, given by:
\begin{align*}
    l(t,x) :=  \| (x_p(t), y_p(t))^T - (x_{e}(t), y_{e}(t))^T\|
\end{align*}
and the terminal cost is $\phi(x(T)) = || (x_p(T), y_p(T))^T - (x_e(T), y_e(T))^T||$.
The cost function $C(t, x(t))$ is defined as:
\begin{equation}
 \begin{aligned}
     C(t,x(t), \ctrlseq) = \int_t^{T} l(t, x(t)) \, dt ~ +
     \phi(x(T))
   \end{aligned}
\end{equation}
where $T$ is the time horizon ($1s$ in this experiment). Minimizing this cost aims to drive the pursuer toward the evader. 
Consequently, the (augmented) dynamics of the system is as follows:
\begin{align*}
    \dot{x_p} &= v \cos(\Theta) \\
    \dot{y_p} &= v \sin(\Theta) \\
    \dot{v} &= u_1 \\
    \dot{\Theta} &= u_2 \\
    \dot{x_{e}} &= v_{xe} \\
    \dot{y_{e}} &= v_{ye} \\
    \dot{v_{xe}} &= 0 \\
    \dot{v_{ye}} &= 0 \\
    \dot{z} &= - l(t,x)
\end{align*}
where $u_1$ represents the linear acceleration control and $u_2$ represents angular velocity control.

The safety constraints are defined as:
\begin{align*}
    g(x) :=& max ( 0.2 - \|x - (0.5, 0.5)^T \|,  0.2 - \|x - (-0.5, 0.5)^T \|, 0.2 - \|x - (-0.5, -0.5)^T \|,\\
    &0.2 - \|x - (0.5, -0.5)^T \|, 0.2 - \|x - (0.0, 0.0)^T \|,) )
\end{align*}
which represents 5 obstacles of radius 0.2 units each.

% \begin{equation*}
%     \hat{V}(T, x, z) = \max(-z, \psi(x), g(x)),
% \end{equation*}

% \textbf{Baselines:} 
% MPPI: \cite{8558663}
% C3BF: \cite{10644338}
% SAC: Lagrangian SAC \cite{pmlr-v80-haarnoja18b} 

\subsection{Multi-Agent Navigation}\label{appendix: MultiAgent}

A multi-agent setting with 5 agents. The state of each agent $i$ is represented by $x_i = [x_{a_i}, y_{a_i}, x_{g_i}, y_{g_i}]$, tries to reach its goal while avoiding collisions with others. $(x_{a_i}, y_{a_i})$ denote the position of the $i$th agent, while $(x_{g_i}, y_{g_i})$ represent the goal locations for that agent. We define the step cost at each step, $l(t,x(t))$, as the mean distance of each agent from its respective goal, given by:
\begin{align*}
    l(t, x(t)) := \frac{\sum_{i=1}^{5}\| (x_{a_i}(t), y_{a_i}(t)^T - (x_{g_i}(t), y_{g_i}(t))^T\|}{5}
\end{align*}
The cost function $C(t, x(t), \ctrlseq)$ is defined as:
\begin{equation}
 \begin{aligned}
     C(t,x(t), \ctrlseq) := \int_t^{T} l(t, x(t)) \, dt ~ +
     \phi(x(T))
   \end{aligned}
\end{equation}
where $T$ is the time horizon ($2s$ in this experiment). Minimizing this cost aims to drive each agent towards its goal. 
Consequently, the (augmented) dynamics of the system is as follows:
\begin{align*}
    \dot{x}_{a_i} &= u_{1i}, \forall i \in \{ 1,2,3,4,5\}\\
    \dot{y}_{a_i} &= u_{2i}, \forall i \in \{ 1,2,3,4,5\} \\
    \dot{x}_{g_i} &= 0, \forall i \in \{ 1,2,3,4,5\}\\
    \dot{y}_{g_i} &= 0, \forall i \in \{ 1,2,3,4,5\} \\
    \dot{z} &= - l(t,x)
\end{align*}
where $u_{1i}, u_{2i}$ represents the linear velocity control of each agent $i$.
The safety constraints are defined as:
\begin{equation}
\begin{aligned}
    g(x(t)) := \max_{i, j=\{1,..,5 \}, i\neq j} ( R - \|(x_{a_i}, y_{a_i})^T -  (x_{a_j}, y_{a_j})^T \|)
\end{aligned}
\end{equation}

\section{Implementation Details of the Algorithms}

This section provides an in-depth overview of our algorithm and baseline implementations, including hyperparameter configurations and the cost/reward functions used in the baselines across all experiments.

\subsection{Experimental Hardware}
All experiments were conducted on a system equipped with an 11th Gen Intel Core i9-11900K @ 3.50GHz × 16 CPU, 128GB RAM, and an NVIDIA GeForce RTX 4090 GPU for training.
\vspace{-1em}
\subsection{Hyperparameters for the Proposed Algorithm}
We maintained training settings across all experiments, as detailed below:

\begin{table}[h]
    \centering
    \begin{tabular}{lc}
        \hline
        \textbf{Hyperparameter} & \textbf{Value} \\
        \hline
        Network Architecture & Multi-Layer Perceptron (MLP) \\
        Number of Hidden Layers & 3 \\
        Activation Function & Sine function \\
        Hidden Layer Size & 256 neurons per layer \\
        Optimizer & Adam optimizer \\
        Learning Rate & $2\times 10^{-5}$ \\
        \hline
        \textbf{Boat Navigation} & .\\
        \hline
        Number of Training Points & 65000 \\
        Number of Pre Training Epochs & 50K \\
        No. of Training Epochs & 200K\\
        \hline
        \textbf{Pursuer Vehicle Tracking Evader} & .\\
        \hline
        Number of Training Points & 65000 \\
        Number of Pre Training Epochs & 60K \\
        No. of Training Epochs & 300K\\
        \hline
        \textbf{Multi Agent Navigation} & .\\
        \hline
        Number of Training Points & 65000 \\
        Number of Pre Training Epochs & 60K \\
        No. of Training Epochs & 400K\\
        \hline
    
    \end{tabular}
    \vspace{1em}
    \caption{Hyperparameters for the proposed algorithm}
    \label{tab:training_details}
\end{table}
\vspace{-2em}
\subsection{MPPI based baselines}
For all the experiments we consider the MPPI cost term as follows:

\begin{equation}
    C_{MPPI} = C(t, x(t), \ctrlseq) + \lambda \max(g(x), 0)
\end{equation}
where, $\lambda$ is the trade-off parameter, $C(t,x(t), \ctrlseq)$, $g(x)$ are the cost functions and safety functions as defined in Appendix \ref{appendix: system_details}. Following is the list of hyperparameters we have used for MPPI experiments in all the cases:


\begin{table}[h]
    \centering
    \begin{tabular}{lc}
        \hline
        \textbf{Hyperparameter} & \textbf{Value} \\
        \hline
        Trade-off parameter ($\lambda$) & 100 \\
        Planning Horizon & 20 \\
        Softmax Lambda & 200 \\
        No. of Rollouts & 8000 \\
        \hline
        
        
    \end{tabular}
    \vspace{1em}
    \caption{Hyperparameters for MPPI Baselines}
    \label{tab:mppi_details}
\end{table}
\vspace{-2em}
\subsection{C-SAC hyperparameters}

For all the experiments, we consider the reward term as follows:

\begin{equation}
    R_{CSAC} = -C(t, x(t), \ctrlseq) - \mathbb{I}_{g(x)>0}\times(100) + \mathbb{I}_{l(t, x(t))<0.1}\times(100) 
\end{equation}
where, $C(t,x(t), \ctrlseq)$, $g(x)$ are the cost functions and safety functions as defined in Appendix \ref{appendix: system_details}. Following is the list of hyperparameters we have used for SAC experiments in all the cases:

\begin{table}[h]
    \centering
    % \renewcommand{\arraystretch}{1.5}
    \begin{tabular}{lc}
        \hline
        \textbf{Parameter} & \textbf{Value} \\
        \hline
        Policy Architecture & Multi-Layer Perceptron (MLP) \\
        learning rate & $3 \times 10^{-4}$  \\
        buffer size & $1,000,000$  \\
        learning starts & $10,000$  \\
        batch size & $256$  \\
        Target network update rate ($\tau$) & $0.005$  \\
        Discount factor ($\gamma$) & $0.99$  \\
        \hline
        \textbf{Boat Navigation} & .\\
        \hline
        Number of Training Steps & 1,000,000 \\
        \hline
        \textbf{Pursuer Vehicle Tracking Evader} & .\\
        \hline
        Number of Training Steps & 2,500,000 \\
        \hline
        \textbf{Multi Agent Navigation} & .\\
        \hline
        Number of Training Steps & 1,000,000 \\
        \hline
    \end{tabular}
    \vspace{1em}
    \caption{General Hyperparameters of SAC in our experiments}
    \label{tab:sac_general_params}
\end{table}

\subsection{Computation time Comparison} \label{app: comp_time}

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.97\linewidth]{images/Baseline_Time_Comparison_Color.pdf}
    \vspace{-0.8em}
\caption{This figure presents a comparative analysis of all methods based on online and offline computation time evaluated on the same computing machine. The top plot illustrates the \textbf{offline computation time} for our method and the baselines. Since our method and C-SAC involve training value functions, they incur higher offline computation costs, whereas MPPI-based methods require no offline training. The bottom plot depicts the \textbf{online computation time}, demonstrating that our method and C-SAC have minimal online computation requirements, whereas MPPI-based methods exhibit significantly higher online computational costs.} 
\label{fig: baseline_time_comparison}
\vspace{-1.0em}
\end{figure*}

Figure~\ref{fig: baseline_time_comparison} presents a comparative analysis of the offline and online computation times for our method against the baselines. It can be observed that the proposed approach exhibits better scalability with increasing dimensionality compared to C-SAC, as our method demonstrates a steady growth in training time, whereas C-SAC experiences a sharp increase in offline computation time as the dimensionality rises. Additionally, the online computation time for both our method and C-SAC remains significantly lower than that of online algorithms such as MPPI and MPPI-SF. This highlights the practicality of our method for real-time applications, provided that the offline value function has been precomputed.

\subsection{Comparison of Multi-Agent Navigation with baselines}\label{app: MVC_baselines}

Figures~\ref{fig: MVC_MPPI}, \ref{fig: MVC_MPPI_SF}, and \ref{fig: MVC_SAC} illustrate the trajectories obtained by the baseline methods for the Multi-Agent Navigation problem. It can be observed that the trajectories obtained by MPPI and MPPI-SF are highly conservative, implying that these methods prioritize safety to mitigate potential conflicts among agents. In contrast, the policy derived from C-SAC fails to maintain safety, resulting in agent collisions. This indicates that as system complexity increases, the baseline methods tend to prioritize either safety or performance, leading to suboptimal behavior and safety violations. Conversely, the proposed approach effectively co-optimizes safety and performance, even in complex high-dimensional settings, achieving superior performance while ensuring safety.

\begin{figure*}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{images/MVC_MPPI_Traj.pdf}
    \vspace{-0.8em}
\caption{Snapshots of multi-agent navigation trajectories at different time instances using \textbf{MPPI}. The trajectories indicate that the agents adopt a \textbf{highly conservative strategy} to prevent collisions. Consequently, this leads to a \textbf{reduction in performance}, as the agents \textbf{end up very far from their respective goals}.
} 
\label{fig: MVC_MPPI}
\vspace{-1em}
\end{figure*}
\begin{figure*}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{images/MVC_MPPI_SF_Traj.pdf}
    \vspace{-0.8em}
\caption{Snapshots of multi-agent navigation trajectories at different time instances using \textbf{MPPI-NCBF}. The observed trajectories demonstrate \textbf{suboptimal behavior similar to that of the MPPI policy}. Consequently, this results in high-performance costs, indicating its \textbf{inability to effectively co-optimize safety and performance.}} 
\label{fig: MVC_MPPI_SF}
\end{figure*}
\begin{figure*}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{images/MVC_SAC_Traj.pdf}
    \vspace{-0.8em}
\caption{Snapshots of multi-agent navigation trajectories at different time instances using C-SAC. The trajectories indicate that agents \textbf{demonstrate less conservative behavior compared to MPPI and MPPI-NCBF, but they lead to collisions}. These \textbf{safety violations are critical} and cannot be disregarded, further \textbf{highlighting the limitations of the baseline methods in simultaneously optimizing safety and performance.}} 
\label{fig: MVC_SAC}
\end{figure*}
