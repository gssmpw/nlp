% As discussed in Section~(\ref{subsec: epigraph}), solving the SC-OCP in Equation~\eqref{eq: SC-OCP} requires solving an HJB-PDE. Traditionally, numerical methods are used to solve the HJB-VI over a grid representation of the state space \cite{mitchell2004toolbox, pythonhjtoolbox}, where time and spatial derivatives are approximated numerically. While grid-based methods are accurate for low-dimensional problems, they face the curse of dimensionality, limiting their use in high-dimensional systems. To address this, we adopt a physics-informed machine learning framework, inspired by \cite{9561949}, which has proven effective for high-dimensional reachability problems. In this section, we describe our approach for learning the auxiliary value function $\hat{V}$ through physics-informed machine learning.

To solve the SC-OCP in Equation~\eqref{eq: SC-OCP}, we aim to compute the optimal value function $V$, which minimizes the cost while ensuring system safety. In this section, we outline a structured approach: first, we learn the auxiliary value function $\hat{V}$ using a physics-informed machine learning framework. Then, we apply a conformal prediction-based method to verify safety and correct for potential learning errors in $\hat{V}$. The final value function $V$ is obtained from the safety-corrected $\hat{V}$ using the epigraph formulation in \eqref{eq: aux_value_func}. Lastly, we assess the performance of $V$ through a second conformal prediction procedure. Figure \ref{fig: summary_algo} gives an overview of the proposed approach.

The following subsections provide a detailed explanation of each step, beginning with the methodology for learning $\hat{V}$.

% \begin{figure*}
%     \centering
%     \includegraphics[width=0.826\linewidth]{images/Algo_diagram.pdf}
% \caption{\textbf{Overview of the proposed approach}: The methodology is organized into four steps. The \textbf{first step} involves training the auxiliary value function, $\learnedauxvfunc$, using a physics-informed machine learning framework. The \textbf{second step} applies a conformal prediction approach for safety verification of the learned $\learnedauxvfunc$. In the \textbf{third step}, the final value function $\learnedvfunc$ and the optimal safe and performant policy $\learnedpolicy$ are inferred. The \textbf{fourth step} quantifies the performance of $\learnedvfunc$ through a second conformal prediction procedure. } 
% \label{fig: summary_algo}
% \vspace{-1.0em}
% \end{figure*}

\subsection{Training the Auxiliary Value Function ($\hat{V}$)}
The auxiliary value function, $\hat{V}$, satisfies the HJB-PDE in Equation \eqref{eq: coopt_pde}, as discussed in Section \ref{subsec: epigraph}.
Traditionally, numerical methods are used to solve the HJB-PDE over a grid representation of the state space \cite{mitchell2004toolbox, pythonhjtoolbox}, where time and spatial derivatives are approximated numerically. While grid-based methods are accurate for low-dimensional problems, they struggle with the curse of dimensionality -- their computational complexity increases exponentially with the number of states -- limiting their use in high-dimensional systems. To address this, we adopt a physics-informed machine learning framework inspired by \cite{9561949, singh2024exactimpositionsafetyboundary}, which has proven effective for high-dimensional reachability problems.

The solution of the HJB-PDE inherently evolves backward in time, as the value function at time $t$ is determined by its value at $t + \Delta t$. To facilitate neural network training, we use a curriculum learning strategy, progressively expanding the time sampling interval from the terminal time $[T, T]$ to the full time horizon $[0, T]$. This approach allows the neural network to first accurately learn the value function from the terminal boundary conditions, subsequently propagating the solution backward in time by leveraging the structure of the HJB-PDE.

Specifically, the auxiliary value function is approximated by a neural network, $\learnedauxvfunc$, where $\theta$ denotes the trainable parameters of the network. Training samples, $(t_k, x_k, z_k)_{k=1}^N$, are randomly drawn from the state space based on the curriculum training scheme. The proposed learning framework utilizes a loss function that enforces two primary objectives: (i) compliance with the PDE in \eqref{eq: coopt_pde}, using the PDE residual error given by:
\begin{equation}
\begin{aligned}
\mathcal{L}_{pde}\left(t_k, \hat{x}_k | \theta\right)&=\| \min \left\{-\partial_t \learnedauxvfunc\left(t_k, \hat{x}_k\right)- H(t_k,\hat{x}_k),\right. \\
& \qquad \qquad \left.\learnedauxvfunc\left(t_k, \hat{x}_k\right) - g\left(x_k\right)\right\} \| ,
\end{aligned}
\end{equation}

where $H(t,\hat{x}) = \min_{u\in \mathcal{U}} \langle\nabla \learnedauxvfunc(t, \hat{x}), \hat{f}(\hat{x}, u)\rangle $ and (ii) satisfaction of the boundary condition in \eqref{eq: terminal_condition}, using boundary condition loss, given by:
\begin{equation}
\begin{aligned}
\mathcal{L}_{bc}\left(t_k, \hat{x}_k | \theta\right)&=\left\|\max\left(\phi(x_k) - z_k, g(x_k)\right) -\right. \\
& \qquad \qquad \left.\learnedauxvfunc\left(t_k, \hat{x}_k \right)\right\| \mathds{1}\left(t_k=T\right).
\end{aligned}
\end{equation}


These terms are balanced by a trade-off parameter $\lambda$, leading to the overall loss function:
\begin{equation}
\label{eq: piml_co-opt_loss}
\begin{aligned}
 \mathcal{L}\left(t_k, \hat{x}_k | \theta\right)&=\mathcal{L}_{pde}\left(t_k, \hat{x}_k | \theta\right)+\lambda \mathcal{L}_{bc}\left(t_k, \hat{x}_k | \theta\right)
\end{aligned}
\end{equation}
% 
Minimizing the overall loss function provides a self-supervised learning mechanism to approximate the auxiliary value function.
\vspace{-0.5em}
\subsection{Safety Verification}
\begin{algorithm}[t]
\caption{Safety Verification using Conformal Prediction}
\label{alg: cp_safety}
\begin{algorithmic}[1]
\REQUIRE $\mathcal{S}$, $N_s$, $\beta_s$, $\epsilon_s$, $\learnedauxvfunc(0, \hat{x})$, $\inducedauxvfunc(0, \hat{x})$, $M$ (number of $\delta$-levels to search for $\delta$),
    % \STATE $\delta_0 = \infty$
    % \STATE Initialize $D$ and $S$ as empty lists
    \STATE $D_0 \gets \text{Sample $N_s$ IID states from}~ \mathcal{S}_{\delta=0}$
    \STATE $\delta_0 \gets \min_{\hat{x}_j \in D_0}\{\learnedauxvfunc(0, \hat{x}_j): \inducedauxvfunc(0, \hat{x}_j) \geq 0\}$
     \STATE $\epsilon_0 \gets \eqref{eq: safe_eps_calc} ~~(\text{using}~\alpha_{\delta = 0})$
    \STATE $\Delta \gets \text{Ordered list of $M$ uniform samples from}~[\delta_0, 0]$
   \FOR{$i = 0, 1, \dots, M - 1$}
   \WHILE{$\epsilon_i \leq \epsilon_s$}
    \STATE $\delta_i \gets \Delta_i$
    \STATE $\text{Update}~\alpha_{\delta_i}~\text{from}~{\delta_i}$
    % \STATE $D_i \gets \text{Sample $N_s$ IID states from}~ \mathcal{S}_{\delta_i}$
    
    \STATE $\epsilon_i \gets \eqref{eq: safe_eps_calc} ~~(\text{using}~\alpha_{\delta_i})$
    % \IF {$\epsilon_i \leq \epsilon_s$}
    %     \STATE \textbf{continue}
    % \ELSE
    %     \STATE \textbf{break}
    % \ENDIF
    \ENDWHILE
    \ENDFOR
\STATE \textbf{return} $\delta \gets \delta_{i}$
\end{algorithmic}
\end{algorithm}
The learned auxiliary value function, $\learnedauxvfunc$, induces a policy, $\learnedauxpolicy$, that minimizes the Hamiltonian term $H(t, \hat{x})$ in the HJB-PDE. The policy is given by:
\begin{equation} \label{eqn:opt_ctrl}
    \learnedauxpolicy(t, \hat{x})=\arg \min_{u\in \mathcal{U}} \langle\nabla \learnedauxvfunc(t, \hat{x}), \hat{f}(\hat{x}, u)\rangle .
\end{equation}
The rollout cost corresponding to this policy is defined as:
\begin{equation}\label{eq: induced_policy}
    \inducedauxvfunc(t, \hat{x}) = \max \{C(t, x(t), \ctrlseq) -z, \max_{s \in [t, T]}g(x(s)) \} \Big|_{\ctrlseq = \learnedauxpolicy}
    % V_{\hat{\pi}}(t, \hat{x}) = \max \{C(t, x(t), \ctrlseq) -z, \max_{s \in [t, T]}g(x(s)) \} \Bigg|_{\ctrlseq = \hat{\pi}}.
\end{equation}
Ideally, the rollout cost from a given state under $\learnedauxpolicy$ should match the value of the auxiliary value function at that state. However, due to learning inaccuracies, discrepancies can arise. This becomes critical when a state, $\hat{x}_i$, is deemed safe by the auxiliary value function ($\learnedauxvfunc(t, \hat{x}) \leq 0$) but is unsafe under the induced policy ($\inducedauxvfunc(t, \hat{x}) > 0$). To address this, we introduce a uniform value function correction margin, $\delta$, which guarantees that the sub-$\delta$ level set of the auxiliary value function remains safe under the induced policy. 
Mathematically, the optimal $\delta$ ($\delta^*$) can be expressed as:
% 
\begin{equation}
    \delta^* := \min_{\hat{x} \in \mathcal{X}}\{\learnedauxvfunc(0, \hat{x}): \inducedauxvfunc(0, \hat{x}) \geq 0\}
\end{equation}
%
Intuitively, $\delta^*$ identifies the tightest level of the value function that separates safe states under $\learnedauxpolicy$ from unsafe ones. Hence, any initial state within the sub-$\delta^*$ level set is guaranteed to be safe under the induced policy, $\learnedauxpolicy^*$. However, calculating $\delta^*$ exactly requires infinitely many state-space points. To overcome this, we adopt a conformal-prediction-based approach to approximate $\delta^*$ using a finite number of samples, providing a probabilistic safety guarantee. The following theorem formalizes our approach:

% \sbnote{We need to revisit this theorem and its proof.}
\begin{theorem}[Safety Verification Using Conformal Prediction]\label{thm: safety_verification}
Let $\mathcal{S}_{\delta}$ be the set of states satisfying $\learnedauxvfunc(0, \hat{x}) \leq \delta$, and let $(0, \hat{x_i})_{i=1, \dots, N_s}$ be $N_s$ i.i.d. samples from $\mathcal{S}_{\delta}$. Define $\alpha_{\delta}$ as the safety error rate among these $N_s$ samples for a given $\delta$ level. Select a safety violation parameter $\epsilon_s \in (0,1)$ and a confidence parameter $\beta_s \in (0,1)$ such that:
\begin{equation} \label{eq: safe_eps_calc}
    \sum_{i=0}^{l-1} \binom{N_s}{i} \epsilon_s^i (1 - \epsilon_s)^{N_s - i} \leq \beta_s,
\end{equation}
where \( l = \lfloor (N_s+1)\alpha_{\delta} \rfloor \). Then, with the probability of at least $1 - \beta_s$, the following holds:
\begin{equation}
    \underset{\hat{x} \in \mathcal{S}_{\delta}}{\mathbb{P}}\left(\hat{V}(0, \hat{x}_i) \leq 0 \right) \geq 1-\epsilon_s.
\end{equation}
\end{theorem}

% where, $ \delta \leq \min_{\hat{x} \in \mathcal{X}}\{\learnedauxvfunc(0, \hat{x}): \inducedauxvfunc(0, \hat{x}) \geq 0\}$.

The proof is available in Appendix \ref{appendix: proof_safety}. 
The safety error rate $\alpha_{\delta}$ is defined as the fraction of samples satisfying $\learnedauxvfunc \leq \delta$ and $\inducedauxvfunc \geq 0$ out of the total $N_s$ samples.

Algorithm \ref{alg: cp_safety} presents the steps to calculate $\delta$ using the approach proposed in this theorem. 
\begin{algorithm}[t]
\caption{Performance Quantification using Conformal Prediction}
\label{alg: cp_perf}
\begin{algorithmic}[1]
\REQUIRE $\mathcal{S}^*$, $N_p$, $\beta_p$, $V_{\theta}(0, x)$, $V_{\pi_{\theta}}(0, x)$
    % \STATE Initialize $D$ and $P$ as empty lists
    \STATE $D \gets \text{Sample $N_p$ IID states from} \{x : x \in \mathcal{S}^*\}$
    \FOR{$i = 0, 1, \dots, N_p-1$}
    % \IF{$\exists x \in D_i : J_\pi(x,0) \leq 0$}
        \STATE $P_i \gets p_i(0, D)$
        % \STATE $\delta_i \gets \max_{x \in D_i} \{\tilde{V}(x,0) : J_\pi(x,0) \leq 0\}$
    % \ELSE
    %     \STATE \textbf{break}
    % \ENDIF
    \ENDFOR
\STATE $P \gets P~\text{sorted in decreasing order}$ 
\STATE $\alpha_{p} \gets \frac{1}{N_p + 1},~~\psi_0 \gets P_0, ~~\epsilon_0 \gets \eqref{eq: perf_eps_calc}$
% \STATE $\psi_0 \gets P_0$
% \STATE $\epsilon_0 \gets \eqref{eq: perf_eps_calc} ~~(\text{using}~\alpha_{p})$
% \STATE $\Psi \gets \text{Ordered list of $M$ uniform samples from}~[0, \psi_0]$
\FOR{$i = 0, 1, \dots, N_p - 1$}
   \WHILE{$\epsilon_i \leq \epsilon_p$}
   \STATE $\alpha_{p} \gets \frac{i+1}{N_p + 1},~~\psi_i \gets P_i,~~\epsilon_i \gets \eqref{eq: perf_eps_calc}$
    % \STATE $\psi_i \gets P_i$ 
    % \STATE $\epsilon_i \gets \eqref{eq: perf_eps_calc} ~~(\text{using}~\alpha_{p})$
    \ENDWHILE
    \ENDFOR
\STATE \textbf{return} $\psi \gets \psi_i$
\end{algorithmic}
\end{algorithm}
\vspace{-0.5em}
\subsection{Obtaining Safe and Performant Value Function and Policy from $\learnedauxvfunc$}
Using the $\delta$-level estimate from Algorithm~(\ref{alg: cp_safety}), we can finally obtain the safe and performant value function, $\learnedvfunc(t, x)$, by solving the following epigraph optimization problem: 
\begin{equation}\label{eq: final_value_func}
    \begin{aligned}
        \learnedvfunc(t, x) &= \min_{z \in \mathbb{R^+}} \; z \\
        \text{s.t.} & \; \learnedauxvfunc(t, x, z) \leq \delta.
    \end{aligned}
\end{equation}
Note that $\learnedvfunc(t, x)$ is trivially $\infty$ for the states where $\learnedauxvfunc(t, x, z) > \delta$, since such states are unsafe and hence do not satisfy the safety constraint. 

In practice, we solve this optimization problem by using a binary search approach on $z$. The resulting optimal state-feedback control policy, $\learnedpolicy: \mathcal{X} \times [t, T) \to \mathcal{U}$, satisfying Objective~(\ref{obj: Main_obj}), is given by:
\begin{equation} \label{eq: optimal_pi}
    \learnedpolicy(t, x) = \arg \min _u \langle\nabla \learnedauxvfunc(t, \hat{x}^*), \hat{f}(\hat{x}^*, u)\rangle,
\end{equation}
where $\hat{x}^*$ is the augmented state associated with the optimal $z^*$ obtained by solving \eqref{eq: final_value_func}, i.e., $\hat{x}^* = [x, z^*]^T$.
Intuitively, we can expect $\learnedpolicy$ to learn behaviors that best tradeoff the safety and performance of the system.
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{images/Baseline_Comparison.pdf}
    \vspace{-0.5em}
\caption{This figure presents a comparative study between all the methods based on our evaluation metrics. The top plot illustrates the \textbf{mean percentage increase in cumulative cost} relative to our method for each baseline, demonstrating that our approach consistently incurs lower costs, with the gap widening as system complexity grows. The bottom plot depicts the \textbf{safety rates}, showing that our method maintains a $100\%$ safety rate, while baselines that encourage safety rather than enforcing it (like MPPI and C-SAC) achieve lower rates. MPPI-CBF also attains $100\%$ safety but at the expense of performance. Overall, our method uniquely \textbf{balances both safety and performance}, whereas the baselines compromise on at least one aspect.} 
\label{fig: baseline_comparison}
\vspace{-0.8em}
\end{figure*}
\vspace{-0.8em}
\subsection{Performance Quantification}

In general, the learning inaccuracies in the auxiliary value function $\learnedauxvfunc$, may lead to errors in the value function $\learnedvfunc$.
These errors, in turn, can lead to performance degradation under policy $\learnedpolicy$.
% discrepancies may arise between the computed value function $V$ and the value obtained from the rollouts of its induced policy $\pi$, as defined in \eqref{eq: optimal_pi}. 
To quantify this degradation, we propose a conformal prediction-based performance quantification method that provides a probabilistic upper bound on the error between the value function and the value obtained from the induced policy. The following theorem formalizes our approach:

% \sbnote{We need to revisit this theorem and its proof.}
\begin{theorem}[Performance Quantification Using Conformal Prediction]\label{thm: perf_verification}
    Suppose $\mathcal{S}^*$ denotes the safe states satisfying $\learnedvfunc(0, x) < \infty$ (or equivalently $\learnedauxvfunc(0, \hat{x}^*) < \delta$) and $(0, x_i)_{i=1, \dots, N_p}$ are $N_p$ i.i.d. samples from $\mathcal{S}^*$. For a user-specified level $\alpha_p$, let $\psi$ be the $\frac{\lceil(N_p+1)(1-\alpha_p)\rceil}{N_p}th$ quantile of the scores $(p_i := \frac{|\learnedvfunc(0, x_i) - \inducedvfunc(0, x_i)|}{C_{max}})_{i=1, \dots, N_p}$ on the $N_p$ state samples.
    Select a violation parameter $\epsilon_p \in (0, 1)$ and a confidence parameter $\beta_p \in (0, 1)$ such that:
    \begin{equation} \label{eq: perf_eps_calc}
          \sum_{i=}^{l-1} \binom{N_p}{i} \epsilon_p^i (1 - \epsilon_p)^{N_p - i} \leq \beta_p
    \end{equation}
    where, \( l = \lfloor (N_p+1)\alpha_p \rfloor \).
    Then, the following holds, with probability $1-\beta_p$:
    \begin{equation}
        \underset{x \in \mathcal{S}^*}{\mathbb{P}}\left(\frac{|\learnedvfunc(0, x_i) - \inducedvfunc(0, x_i)|}{C_{max}} \leq \psi \right) \geq 1-\epsilon_p.
    \end{equation}
where $C_{max}$ is a normalizing factor and denotes the maximum possible cost that could be incurred for any $x \in \mathcal{S}^*$. 
\end{theorem}

The proof is available in Appendix \ref{appendix: proof_perf}. Note that $C_{max}$ can be easily calculated by calculating the upper bound of the cost function $C(t,x(t), \ctrlseq) \forall x \in \mathcal{S}^*$.

Intuitively, the performance of the resultant policy is the best when the $\psi$ value approaches $0$, while the worst performance occurs at $\psi = 1$.
Algorithm~\ref{alg: cp_perf} presents the steps to calculate $\psi$ using the approach proposed in this theorem. 

% \subsection{Algorithms}

% If you are using \LaTeX, please use the ``algorithm'' and ``algorithmic''
% environments to format pseudocode. These require
% the corresponding stylefiles, algorithm.sty and
% algorithmic.sty, which are supplied with this package.
% \cref{alg:example} shows an example.

% \begin{algorithm}[tb]
%    \caption{Bubble Sort}
%    \label{alg:example}
% \begin{algorithmic}
%    \STATE {\bfseries Input:} data $x_i$, size $m$
%    \REPEAT
%    \STATE Initialize $noChange = true$.
%    \FOR{$i=1$ {\bfseries to} $m-1$}
%    \IF{$x_i > x_{i+1}$}
%    \STATE Swap $x_i$ and $x_{i+1}$
%    \STATE $noChange = false$
%    \ENDIF
%    \ENDFOR
%    \UNTIL{$noChange$ is $true$}
% \end{algorithmic}
% \end{algorithm}


% \subsection{Theorems and such}
% The preferred way is to number definitions, propositions, lemmas, etc. consecutively, within sections, as shown below.
% \begin{definition}
% \label{def:inj}
% A function $f:X \to Y$ is injective if for any $x,y\in X$ different, $f(x)\ne f(y)$.
% \end{definition}
% Using \cref{def:inj} we immediate get the following result:
% \begin{proposition}
% If $f$ is injective mapping a set $X$ to another set $Y$, 
% the cardinality of $Y$ is at least as large as that of $X$
% \end{proposition}
% \begin{proof} 
% Left as an exercise to the reader. 
% \end{proof}
% \cref{lem:usefullemma} stated next will prove to be useful.
% \begin{lemma}
% \label{lem:usefullemma}
% For any $f:X \to Y$ and $g:Y\to Z$ injective functions, $f \circ g$ is injective.
% \end{lemma}
% \begin{theorem}
% \label{thm:bigtheorem}
% If $f:X\to Y$ is bijective, the cardinality of $X$ and $Y$ are the same.
% \end{theorem}
% An easy corollary of \cref{thm:bigtheorem} is the following:
% \begin{corollary}
% If $f:X\to Y$ is bijective, 
% the cardinality of $X$ is at least as large as that of $Y$.
% \end{corollary}
% \begin{assumption}
% The set $X$ is finite.
% \label{ass:xfinite}
% \end{assumption}
% \begin{remark}
% According to some, it is only the finite case (cf. \cref{ass:xfinite}) that is interesting.
% \end{remark}
% %restatable

% \subsection{Citations and References}

% Please use APA reference format regardless of your formatter
% or word processor. If you rely on the \LaTeX\/ bibliographic
% facility, use \texttt{natbib.sty} and \texttt{icml2025.bst}
% included in the style-file package to obtain this format.

% Citations within the text should include the authors' last names and
% year. If the authors' names are included in the sentence, place only
% the year in parentheses, for example when referencing Arthur Samuel's
% pioneering work \yrcite{Samuel59}. Otherwise place the entire
% reference in parentheses with the authors and year separated by a
% comma \cite{Samuel59}. List multiple references separated by
% semicolons \cite{kearns89,Samuel59,mitchell80}. Use the `et~al.'
% construct only for citations with three or more authors or after
% listing all authors to a publication in an earlier reference \cite{MachineLearningI}.

% Authors should cite their own work in the third person
% in the initial version of their paper submitted for blind review.

% Use an unnumbered first-level section heading for the references, and use a
% hanging indent style, with the first line of the reference flush against the
% left margin and subsequent lines indented by 10 points. The references at the
% end of this document give examples for journal articles \cite{Samuel59},
% conference publications \cite{langley00}, book chapters \cite{Newell81}, books
% \cite{DudaHart2nd}, edited volumes \cite{MachineLearningI}, technical reports
% \cite{mitchell80}, and dissertations \cite{kearns89}.

% Alphabetize references by the surnames of the first authors, with
% single author entries preceding multiple author entries. Order
% references for the same authors by year of publication, with the
% earliest first. Make sure that each reference includes all relevant
% information (e.g., page numbers).

% Please put some effort into making references complete, presentable, and
% consistent, e.g. use the actual current name of authors.
% If using bibtex, please protect capital letters of names and
% abbreviations in titles, for example, use \{B\}ayesian or \{L\}ipschitz
% in your .bib file.