% Autonomous systems have become increasingly prevalent in modern society, playing a critical role in various applications. When designing control algorithms for these systems, two fundamental objectives must be addressed: \textit{performance} and \textit{safety}.
% 
Autonomous systems are becoming increasingly prevalent across various domains, from self-driving vehicles and robotic automation to aerospace and industrial applications. 
Designing control algorithms for these systems involves balancing two fundamental objectives: \textit{performance} and \textit{safety}. 
Ensuring high performance is essential for achieving efficiency and task objectives under practical constraints, such as fuel limitations or time restrictions.
For instance, a warehouse humanoid robot navigating to a destination must optimize its route for efficiency.
% Ensuring high performance is particularly important in scenarios where constraints—such as fuel limitations or time restrictions—must be considered. For instance, an autonomous vehicle navigating to a destination under a strict time limit must optimize its route for efficiency. 
At the same time, safety remains paramount to prevent catastrophic accidents or system failures. 
% These two goals often conflict, making it challenging to develop control strategies that balance both effectively.
These two objectives, however, often conflict, making it challenging to develop control strategies that achieve both effectively.

A variety of data-driven approaches have been explored to integrate safety considerations into control synthesis. 
Constrained Reinforcement Learning (CRL) methods \cite{altman1999constrained, achiam2017constrained} employ constrained optimization techniques to co-optimize safety and performance where performance is encoded as a reward function and safety is formulated as a constraint. 
% , with Lagrangian duality being a particularly common approach due to its computational simplicity. 
These methods often incorporate safety constraints into the objective function, leading to only a soft imposition of the safety constraints. Moreover, such formulations typically minimize cumulative constraint violations rather than enforcing strict safety at all times, which can result in unsafe behaviors. 
% While modifications to the standard constrained Markov Decision Process (CMDP) framework can eliminate constraint violations entirely, these adjustments often lead to numerical instability and poorly conditioned optimization problems.

Another class of methods involve \textit{safety filtering}, which ensures constraint satisfaction by modifying control outputs in real-time. 
Methods such as Control Barrier Function (CBF)-based quadratic programs (QP) \cite{Ames_2017} and Hamilton-Jacobi (HJ) Reachability filters \cite{10665911, 10266799} act as corrective layers on top of a (potentially unsafe) nominal controller, making minimal interventions to enforce safety constraints. 
However, because these safety filters operate independently of the underlying performance-driven controller, they often lead to myopic and suboptimal decisions.
Alternatively, online optimization-based methods, such as Model Predictive Control (MPC) \cite{GARCIA1989335, grune2017nonlinear} and Model Predictive Path Integral (MPPI)~\cite{8558663, 10161511}, can naturally integrate safety constraints while optimizing for a performance objective.
% have gained popularity due to the increasing computational capabilities available for real-time control.
These methods approximate infinite-horizon optimal control problems (OCPs) with a receding-horizon framework, enabling dynamic re-planning. 
While effective, solving constrained OCPs online remains computationally expensive, limiting their applicability for high-frequency control applications. The challenge is further exacerbated when dealing with nonlinear dynamics and nonconvex (safety) constraints, limiting the feasibility of these methods for ensuring safety and optimality for real-world systems.

A more rigorous approach to addressing the trade-off between performance and safety is to formulate the problem as a \textit{state-constrained optimal control problem (SC-OCP)}, where safety is explicitly encoded as a hard constraint, while performance is expressed through a reward (or cost) function.
While theoretically sound, characterizing the solutions of SC-OCPs is challenging unless certain controllability conditions hold \cite{doi:10.1137/0324032}. 
To address these challenges, \cite{altarovici2013general} proposed an epigraph-based formulation, which characterizes the value function of an SC-OCP by computing its epigraph using dynamic programming, resulting in a Hamilton-Jacobi-Bellman Partial Differential Equation (HJB-PDE).
The SC-OCP value function as well as the optimized policy is then recovered from this epigraph.
However, dynamic programming suffers from the curse of dimensionality, making it impractical for high-dimensional systems with traditional numerical solvers. 
Furthermore, the epigraph formulation itself increases the problem's dimensionality, exacerbating computational complexity further.
% While theoretically sound, this method significantly increases the dimensionality of the problem, making it impractical for high-dimensional systems when using traditional numerical solvers.

% \subsection{Our Contribution}  
\begin{figure*}
    \centering
    \includegraphics[width=0.9\linewidth]{images/Algo_diagram.pdf}
\caption{\textbf{Overview of the proposed approach}: The methodology is organized into four steps. The \textbf{first step} involves training the auxiliary value function, $\learnedauxvfunc$, using a physics-informed machine learning framework. The \textbf{second step} applies a conformal prediction approach for safety verification of the learned $\learnedauxvfunc$. In the \textbf{third step}, the final value function $\learnedvfunc$ and the optimal safe and performant policy $\learnedpolicy$ are inferred. The \textbf{fourth step} quantifies the performance of $\learnedvfunc$ through a second conformal prediction procedure. } 
\label{fig: summary_algo}
\vspace{-1.0em}
\end{figure*}

In this work, we propose a novel algorithmic approach to \textit{co-optimize safety and performance for high-dimensional autonomous systems}.
Specifically, we formulate the problem as an SC-OCP and leverage the epigraph formulation in \cite{altarovici2013general}. To efficiently solve this epigraph formulation, we leverage physics-informed machine learning \cite{RAISSI2019686, li2022physicsinformed} to learn a solution to the resultant HJB-PDE by minimizing PDE residuals. This enables us to efficiently scale epigraph computation for higher-dimensional autonomous systems, leading to safe and performant policies. To summarize, our main contributions are as follows:  
% 
\begin{itemize}  
    \item We propose a novel Physics-Informed Machine Learning (PIML) framework to learn policies that co-optimize safety and performance for high-dimensional autonomous systems.  
    \item We introduce a conformal prediction-based safety verification strategy that provides high-confidence probabilistic safety guarantees for the learned policy, reducing the impact of learning errors on safety.
    \item We propose a performance quantification framework that leverages conformal prediction to provide high-confidence probabilistic error bounds on performance degradation.
    \item Across three case studies, we showcase the effectiveness of our proposed method in jointly optimizing safety and performance while scaling to complex, high-dimensional systems.  
\end{itemize}  
% 
% By integrating physics-informed learning with state-constrained optimal control, our method enables computationally efficient, scalable, and high-confidence safety-aware control for autonomous systems.
% 
% This work provides a principled and scalable approach to addressing the fundamental trade-off between safety and performance in autonomous systems, paving the way for safer and more efficient real-world deployments.  
% 
% \section{Related Works}
% 
% DeepRL based solutions of Epigraph form:
% , lack safety and performance guarantees.

\nocite{so2023solving, tayal2024semi}

