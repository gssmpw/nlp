\section{Related Work}
\subsection{LLMs’ Safety}
LLMs suffer from a general deficiency of internal interpretability and controllability, leading to ongoing risks such as the dissemination of misinformation, extreme content and instructions for harmful or illegal activities. As LLMs become integrated into diverse fields~\cite{DBLP:journals/corr/abs-2311-16673,Mumtaz2023LLMsHealthcareC,DBLP:journals/corr/abs-2311-06640}, the inherent safety issues are passed on to a broad spectrum of end users and applications. Additionally, the enhanced accessibility and interactive features of LLMs increase their vulnerabilities to potential misuse and abuse. To cope with these threats, 3H standard (Helpful, Harmless, Honest)~\cite{DBLP:journals/corr/abs-2112-00861} and other ethical values~\cite{DBLP:journals/corr/abs-2307-15217} have been proposed. Recent works explored a series of mechanisms to establish the safety guardrail on LLMs' behaviors for defending against textual attacks. These include Reinforcement Learning from Human or AI Feedback~\cite{DBLP:conf/nips/Ouyang0JAWMZASR22,DBLP:journals/corr/abs-2309-00267} and adversarial training~\cite{DBLP:journals/corr/abs-2307-01225,DBLP:journals/corr/abs-2308-09662,DBLP:journals/corr/abs-2307-16630} that align models’ behaviors with human intentions and values. In addition, filtering-based defenses~\cite{DBLP:journals/corr/abs-2309-00614,DBLP:journals/corr/abs-2309-02705,DBLP:journals/corr/abs-2308-07308} certify LLMs' safety by monitoring the models’ input and output, refinement and self-refinement methods enhance models’ output using iterative reasoning mechanisms~\cite{DBLP:conf/nips/MadaanTGHGW0DPY23,DBLP:conf/uai/2021,DBLP:journals/corr/abs-2305-13514}.

\subsection{Red Teaming on LLMs}
Red teaming plays a crucial role in identifying the unforeseen or undesirable behaviors, limitations, or potential risks associated with the misuse of LLMs before real-world deployment~\cite{House}. 
Several manual red teaming efforts have been conducted on LLMs to expose their vulnerabilities in generating unsafe and inappropriate content. Some works like~\citet{DBLP:journals/corr/abs-2311-03191} and~\citet{,DBLP:journals/corr/abs-2312-04127} hand-crafted jailbreak prompt template to help clean harmful prompt against aligned LLMs.~\citet{DBLP:journals/corr/abs-2209-07858} employed human annotators to elicit unsafe content and developed shared norms, practices, and technical standards for red teaming language models.~\citet{DBLP:conf/emnlp/SchulhoffPKBSAT23} launched a prompt hacking competition making competitors red team members to manipulate LLMs to follow malicious instructions. However, manual red teaming is costly and inefficient. Thus, there has been great interest in developing automated red teaming methods, for example,~\citet{DBLP:conf/emnlp/PerezHSCRAGMI22} used language model to generate attack prompts to red team target language model.~\citet{DBLP:journals/corr/abs-2309-10253} utilized human-written prompt templates as initial seeds and mutated them to generate new ones.~\citet{DBLP:conf/emnlp/MeiLW23} introduced a series of test suites to evaluate the robustness of language models in different security domains.~\citet{DBLP:conf/emnlp/DengWFDW023} presented an attack framework that guides LLM to mimic human-generated attack prompts through in-context learning.~\citet{DBLP:journals/corr/abs-2308-04265} also employed in-context learning to red team generative models in a feedback loop through different demonstration strategies. 
