\section{Introduction}\label{sec:intro}
% Large Language Models (LLMs) such as ChatGPT~\cite{DBLP:conf/nips/Ouyang0JAWMZASR22}, and Llama-2~\cite{DBLP:journals/corr/abs-2307-09288} have gained significant popularity due to their remarkable capacity to produce high-quality text across diverse domains. As they find increased adoption among people of all ages and broader integration and application with various scenarios, safety concerns have been brought to the forefront.  The increase of security incident such as generating content that is biased, toxic or unethical~\cite{DBLP:journals/corr/abs-2307-09288, DBLP:journals/corr/abs-2306-15447}, render research on safety and beneficial use of LLMs crucial and indispensable.
Large Language Models (LLMs) such as GPT \cite{DBLP:journals/corr/abs-2303-08774}, Claude \cite{citeClaude}, Gemmini \cite{DBLP:journals/corr/abs-2403-05530}, Mistral \cite{DBLP:journals/corr/abs-2310-06825} have gained significant attention for their remarkable capacity. With their expanding use across diverse age groups and broader application in various scenarios, the importance of addressing safety concerns has become increasingly prominent \cite{DBLP:journals/corr/abs-2307-09288, DBLP:journals/corr/abs-2306-15447}.


Red teaming, which focuses on creating prompts that can elicit harmful responses from LLMs, is essential for uncovering and addressing potential safety risks.
As shown in Figure \ref{fig:intro}, red teaming involves a dedicated group simulating adversarial behaviors and strategies, either manually or automatically crafting textual attacks to induce harmful generation from LLMs, so as to allow developers to proactively identify and fix vulnerabilities before their real-world deployment.

Previous works usually rely on manual red teaming methods~\cite{DBLP:journals/corr/abs-2311-03191,DBLP:journals/corr/abs-2312-04127,DBLP:journals/corr/abs-2209-07858,DBLP:conf/emnlp/SchulhoffPKBSAT23}, utilizing trial-and-error methods conducted by human teams to create attack prompts. However, crafting effective attack prompts by humans is costly and inefficient, whereas the model can be quickly patched and improved through iterations~\cite{DBLP:conf/nips/Ouyang0JAWMZASR22,DBLP:journals/corr/abs-2307-01225}. 
Therefore, there has been considerable interest in developing automated red teaming methods, these include algorithmic search strategies~\cite{DBLP:conf/aaai/CasperHK23,DBLP:journals/corr/abs-2310-00322}, using LLMs as rewriter~\cite{DBLP:journals/corr/abs-2309-10253} or original generater~\cite{DBLP:conf/emnlp/PerezHSCRAGMI22}.
% However, due to the specific settings of attack targets and objects, most methods lack scalability. 
% Researchers only generate attack prompts with fixed patterns instead of creative ones.
However, prior research on automatic red teaming has largely focused on specific attack target settings and objects, restricting its scope to producing attack prompts with fixed patterns, rather than creative ones.

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.5\textwidth]{a_fig/attack_align.pdf}
  \caption{Red team finds cases where a target model behaves in a harmful way.}
  \label{fig:intro}
\end{figure}

In this paper, we propose \modelname, a scalable \textbf{R}ed \textbf{T}eaming \textbf{P}rompts \textbf{E}volution framework,  which automatically arms a limited number of prompts into a team to perform textual attack on a series of LLMs centered around a range of sensitive topics. 
To be specific, the framework implements a two-stage attack plan that evolves attack prompts in breadth and depth dimensions, respectively. 
In the breadth evolving stage, we design a novel enhanced in-context learning (ICL)~\cite{DBLP:conf/iclr/PatelLRCRC23} method to scale up the number of attack prompts while balancing the attack success rate (ASR) and diversity. 
In the depth evolving stage, we employ customized operations to steer the development of diverse content and forms for pre-generated prompts, enabling further insight into the safety of LLMs and showcasing the attack portability in the evolving. 

Our \modelname significantly outperforms existing representative automatic red teaming methods on both attack success rate (ASR) and prompt diversity.
Benefiting from the scalable nature of \modelname, we automatically create 4,800 red teaming prompts to conduct a comprehensive analysis of 8 representative LLMs across 8 sensitive topics.
We find that: 1) In term of overall safety performance, GPT-3.5 family \textless{} Qwen \textless{} Llama-2 family, earlier versions \textless{} latter versions, larger models \textless{} smaller models. 2) For specific topics, LLMs suffer from ``fraud'' attack prompts easily due to their role-playing ability as well as inherent hallucination. Conversely, LLMs exhibit less vulnerability to ``terrorism'' and ``suicide'' attack prompts which display obvious aggressiveness. 3) In delving deeper into attack prompts, it is the words which share common characteristics such as abstraction, negativity, artistry, that effectively conceal malicious intent, leading to successful attacks.



%LLMs are more vulnerable to

% Through this framework, we can better utilize existing attack prompts, and expand their scale while maintaining notable attack success rate and diversity from both micro and macro perspectives. 

% To address the growing concerns, recent efforts have proposed a series of defense mechanisms to establish the safety guardrail against textual attacks on LLMs, which have been classified into hyperparameter tuning, auditing behavior, input/output filtering, human feedback, adversarial training and red teaming by ~\citet{DBLP:journals/corr/abs-2310-10844}. Red teaming~\cite{DBLP:journals/corr/abs-2209-07858,DBLP:conf/emnlp/PerezHSCRAGMI22} is identified as a key component of all these defense methods. In the context of LLMs, red teaming involves a dedicated group simulating adversarial behaviors and strategies, either manually or automatically crafting textual attacks to induce harmful generation from LLMs, so as to allow developers to proactively identify and fix vulnerabilities before their real-world deployment. In practice, companies rely on manual red teaming methods~\cite{DBLP:journals/corr/abs-2311-03191,DBLP:journals/corr/abs-2312-04127,DBLP:journals/corr/abs-2209-07858,DBLP:conf/emnlp/SchulhoffPKBSAT23}, utilizing trial-and-error methods conducted by human teams to create attack prompts. However, crafting effective attack prompts by human is costly and inefficient, while the model can be quickly patched and improved through iterations~\cite{DBLP:conf/nips/Ouyang0JAWMZASR22,DBLP:journals/corr/abs-2307-01225}. Consequently, effective attack prompts have become increasingly scarce resources. The dynamic between alignment and manual red teaming has gradually lean to an uneven playing field.

% Thus, there has been considerable interest in developing automated red teaming methods, these include algorithmic search strategies~\cite{DBLP:conf/aaai/CasperHK23,DBLP:journals/corr/abs-2310-00322}, using LLMs as rewriter~\cite{DBLP:journals/corr/abs-2309-10253} or original generater~\cite{DBLP:conf/emnlp/PerezHSCRAGMI22}. However, due to the specific settings of attack targets and objects, most methods lack scalability. Researchers can only generate attack prompts with fixed patterns instead of creative ones.

% Given these challenges, we propose an efficient prompt \textbf{EVO}lution framework for red teaming, called EVO. The framework aims to automatically arm a limited number of prompts into a team to perform textual attack on LLMs centered around sensitive topics. To be specific, the framework implement a two-stage attack plan that evolves attack prompts in breadth and depth dimensions, respectively. In the breadth stage, we design novel strategies based on in-context learning (ICL)~\cite{DBLP:conf/iclr/PatelLRCRC23} in loops to scale up the number of attack prompts while balancing the attack success rate (ASR) and diversity. In the depth stage, we employ customized operations to steer the development of diverse content and forms for pregenerated prompts, enabling further insight into the safety of model and showcasing the attack portability in the evolving. The entire process is automated by LLMs without additional training. Through this framework, we can better utilize existing attack prompts, expand their scale while maintaining notable attack success rate and diversity from both micro and macro perspectives. 


Our contributions are summarized as follows:
\begin{itemize}
    \item We propose \modelname, a red teaming prompt evolution framework for LLMs, which can automatically scale up the limited available attack prompts in terms of both quantity and quality, thereby eliminating the necessity for carefully prompt crafting.

   \item Extensive experiments demonstrate that our \modelname framework surpasses the representative automatic red teaming method in both ASR and diversity.
   We also investigate the factors influencing \modelname's performance. 
   
   % Subsequently, we showcase the portability of attack effectiveness across prompts in the depth-evolving stage.
   
    \item We employ \modelname in systematically evaluating a series of closed-source and open-source LLMs on various sensitive topics, analyzing them across dimensions including temporal, scale, category spans, and so on. Additionally, we offer detailed discussions on the variation of pre-generated attack prompts.
\end{itemize}
