\begin{abstract}
% Large Language Models (LLMs) have gained increasing attention for their capacity to produce high-quality text, as well as concerns about safety due to the potential to generate harmful content. To identify and mitigate risks before their real-world deployment, red teaming for LLMs have been proposed and practiced to find cases where a target model behaves in a harmful way. Most red teaming methods are either costly or lack scalability, ignoring further exploration of the expansion and intention of existing attack prompts. In this paper, we propose a framework called EVO which evolves prompts for red teaming in both breadth and depth dimensions. In the breadth stage, we expand the number of attack prompts through in-context learning with novel strategies, and in the depth stage, we employ customized operations to create more variants by enriching the content and forms of pregenerated prompts.  Through experiments, we demonstrate that EVO surpasses baselines on attack success rate (ASR) and diversity. We also provide a systematic evaluation of various LLMs across different features on sensitive topics. Additionally, we offer detailed discussions on the efficacy of the variation of pregenerated attack prompts, which showcases the attack portability in the evolving. Overall, EVO represents a promising approach for efficient and comprehensive red teaming.

Large Language Models (LLMs) have gained increasing attention for their remarkable capacity, alongside concerns about safety arising from their potential to produce harmful content. 
Red teaming aims to find prompts that could elicit harmful responses from LLMs, and is essential to discover and mitigate safety risks before real-world deployment.
However, manual red teaming is both time-consuming and expensive, rendering it unscalable.
In this paper, we propose \modelname, a scalable evolution framework to evolve red teaming prompts across both breadth and depth dimensions, facilitating the automatic generation of numerous high-quality and diverse red teaming prompts.
Specifically, \textit{in-breadth evolving} employs a novel enhanced in-context learning method to create a multitude of quality prompts, whereas \textit{in-depth evolving} applies customized transformation operations to enhance both content and form of prompts, thereby increasing diversity.
Extensive experiments demonstrate that \modelname surpasses existing representative automatic red teaming methods on both attack success rate and diversity. 
In addition, based on 4,800 red teaming prompts created by \modelname, we further provide a systematic analysis of 8 representative LLMs across 8 sensitive topics.
\end{abstract}