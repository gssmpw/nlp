  
\begin{figure*}[!t]
  \centering
  \includegraphics[width=0.85\linewidth]{a_fig/workflow.pdf}
  \caption{
    An overview of our framework. In the breadth stage, we scale up the attack prompts through enhanced in-context learning using comparative examples along with specified topics and mutagenic factor in the loops, and in the depth stage, we employ customized operations to steer the development of diverse content and forms for pre-generated prompts}
  \label{fig:method}
\end{figure*}

\section{Method}
In this section, we provide a formal definition of our automated red teaming workflow (Section \ref{sec:flow}) and introduce our framework from both breadth (Section \ref{sec:first}) and depth  (Section \ref{sec:second}) dimensions.

\subsection{Workflow}\label{sec:flow}
As depicted in Figure \ref{fig:method}, in the breadth stage, our framework adheres to an iterative workflow involving \textbf{demonstration selection from prompt pool, attack prompt generation, attack execution and response evaluation}. The whole process starts with (1) initializing the prompt pool with a limited available attack prompts $X=\{x_1,x_2,x_3,\ldots,x_n\}$, and iterates as follows: (2) utilizing attack model $M_r$ as a red team member to construct a new attack prompt $x_i$, simulating potential users' textual attack around a sensitive topic, 
(3) then feeding the attack prompt into the target model $M_t$ to induce response $r_{i}$ which (4) evaluator $M_e$ will assess for its level of insecurity, yielding score $s_i$. (5) The corresponding prompt is supplied to the prompt pool as a candidate for the next round of generation, where superior and inferior examples are selected as demonstrations based on scores. In that stage, we scale up prompts with high quality and obtain the initial evaluation of the target model's safety performance. Then, for further utilization of the pre-generated prompts, we employ customized operations to steer the development of diverse content and forms in the depth stage. The in-depth operations include downward expansion, restructure, dialogue simulation, and text length declining. Below, we delve into each stage incorporated in \modelname in greater detail.

\subsection{In-Breadth Evolving}\label{sec:first}
In order to scale up the attack prompts efficiently while maintain their effectiveness as textual attack, we design a novel enhanced in-context learning method for prompt generation. Considering ICL suffers from high instability due to variations in meta-prompt (The prompt to the LLM serves as a call to be learner) format and demonstrations selection~\cite{DBLP:journals/corr/abs-2301-00234}, we craft a safety defense-grounded meta-prompt that rationalizes the crafting of attack prompts and prevent rejection by the attack model $M_r$. And we introduce two strategies to guide a more creative and effective extension of available attack prompts toward sensitive topics, rather than mere duplication and rephrasing based on their writing logic or wording.
\subsubsection{Combination of Comparative Examples}
Although superior examples may seem crucial for ICL, previous studies indicate minimal negative effects when utilizing inferior one instead~\cite{DBLP:conf/acl/WangM0S0Z023,DBLP:journals/corr/abs-2401-02582}. Inspired by ``No such thing as waste, only resources in the wrong place'', we view inferior examples as recyclable and valuable component for the next round of generation. When selecting demonstrations, we pick both superior example and inferior example based on their scores. This helps avoid ``single inheritance'' of superior examples and promotes diversity within the evolving process. 
\subsubsection{Poetry as Mutagenic Factor}

Given the safety alignment, LLMs can normally reject clean harmful prompts~\cite{DBLP:journals/corr/abs-2402-05668}, but fail to defend against elaborately packaged ones which conceal their evil intent~\cite{DBLP:journals/corr/abs-2310-10077}.
To make attack prompts more covert, we intentionally incorporate specific genre text as mutagenic factor in the meta-prompt, requesting attack model $M_r$ to assimilate them when crafting new attack prompts. Taking into account the features of different literary genres, we opt for poetry, a genre with high condensation and rich symbolism. Then the freshly generated attack prompt can acquire specific techniques like metaphor to mask malicious intent. The addition of mutagenic factor also add diversity to attack prompts because more materials for generation provided.

\subsection{In-Depth Evolving}\label{sec:second}
Given a set of attack prompts generated by in-breadth evolving, we apply in-depth evolving operations below to create more variants by enriching the content and forms as well as maximize the use of the original prompt. Implementation details will be provided in Section \ref{in-depth}. Examples can be found in Appendix \ref{appsec:example}.

\paragraph{Downward Expansion} We adopt a topic-driven downward expansion strategy to enrich the content of the pre-generated prompts. While retaining the structure of the pre-generated prompts, we evolve them from the original topics (topic-i) to several sub-topics (topic-ii) and further delve into more fine-grained topics (topic-iii) under sub-topics, which contain more detail unsafe content. Then we can get a set of attack prompts covering topics of different grain sizes.

\paragraph{Restructure}We shuffle the word order of the original prompt and ask attack model to restructure it. This results a new attack prompt based on the original one but with a completely different word order.

\paragraph{Dialogue Simulation}We evolve attack prompts into coherent dialogues to simulate the progressive information disclosure in multi-round dialogues between human user and the language model. These dialogues serve as new textual attack, prompting the target to continue.

\paragraph{Length Declining} We propose three length declining methods. 1) Simple Truncation. 2) Clip keywords based on word frequency. 3) Compress prompt employing LLM.



\begin{algorithm}
\caption{In-Breadth Evolving}
\label{algo:breadth}
\SetKwData{Initialization}{\textbf{initialization}}
\DontPrintSemicolon
  \SetAlgoLined
  \KwIn {\text {Original Prompts} $X=\{x_1,x_2,\dots,x_m\}$, \text {Prompt Pool} $P$, \text{Attack Model} $M_r$, \text{Target Model} $M_t$, \text{Evaluator} $M_e$,  \text{Mutation Factor} $\mathcal{M}$, \text{Iteration} $\mathcal{I}$, \text{Topic Set} $\mathcal{T}$}
  \KwOut {\text{prompt with score}}
  \Initialization \\
  $P \gets (X,M_e(M_t(X)))$ \\
   sort $P$ by scores \\
  $i \gets 0$ \\
  \ForEach{$t$ in $\mathcal{T}$}{
  \While{$i < \mathcal{I}$}{
    $x_{s} \gets \text{Sample}_{\text{superior prompt}}(P)$ \\
    $x_{n} \gets \text{Sample}_{\text{inferior prompt}}(P)$ \\
    $\mu \gets \text{Sample}(\mathcal{M})$ \\
    $x_{i} \gets M_r(t, x_{s}, x_{n}, \mu)$ \\
    $r_{i} \gets M_t(x_{i})$ \\
    $s_{i} \gets M_e(r_{i})$ \\
    $P \gets P+ \{x_{i}, s_{i}\}$ \\
  }
  }
\end{algorithm}