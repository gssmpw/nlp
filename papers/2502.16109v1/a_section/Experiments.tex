\section{Experiments}
In this section, we provide a multidimensional evaluation of the prompts built by the \modelname framework and use these prompts to perform safety evaluation on a range of LLMs.
\subsection{Experimental Setup}
\subsubsection{LLMs}
% \paragraph{}
\paragraph{Attack Model} Considering the understanding ability, generation ability and use-cost of mainstream LLMs, we employ GPT-turbo-3.5-0613~\cite{DBLP:conf/nips/Ouyang0JAWMZASR22} as the attacker to generate attack prompts.\\
\paragraph{Target Model}\label{targetmodel} We test generated attack prompts on GPT-turbo-3.5-0301, GPT-turbo-3.5-0613, Llama-1-7b, Llama-2-7b-Chat, Llama-2-13b-Chat~\cite{DBLP:journals/corr/abs-2307-09288}, Vicuna-7b-v1.5, Vicuna-13b-v1.5~\cite{Zheng2023JudgingLW}, and Qwen-max~\cite{DBLP:journals/corr/abs-2309-16609} which are the accessible and widely used LLMs, likely to be deployed and interacted with large amounts of end users.\\

\paragraph{Evaluation Model} We use GPT-3.5-turbo-0613 as an evaluator to score ranging from 0 to 10 based on the level of response's unsafety, then manually review cases with median scores. Based on the evaluation, we classify the attack prompts as effective or not and calculate the attack success rate. We provide evidence supporting the rationality and validity of utilizing GPT-3.5-turbo-0613 as an evaluation model in Appendix \ref{app:eva}.



\subsubsection{Metrics}
We employ Attack Success Rate (ASR) and diversity as our primary evaluation metrics. \\
\paragraph{ASR} ASR indicates the proportion of prompts in a given prompt set which can successfully elicit unsafe content from LLMs. The ASR reflects both the quality of the generated attack prompts and the safety of the target model. 

\[
\textit{ASR}=\frac{\#\text{ effective prompts}}{\#\text{ total attack prompts}}
\]
\
\paragraph{N-gram Based Diversity} We employ Self-BLEU~\cite{DBLP:journals/corr/abs-1802-01886} to evaluate lexical diversity on the level of n-grams, where $n \in \{1, \dots, 5\}$. If $X = \{x_1, x_2,\dots, x_i\}$ represents generated prompts, then Self-BLEU score is computed based on the average BLEU score across different n-grams for all pairwise combinations of $X$. Low average Self-BLEU score implies low similarity as well as high diversity inside the set of generated prompts.
\begin{equation}
\small
DIV_{\mathrm{N-gram}}(X)=\frac1{K}\sum_{n=1}^K{\rm SelfBLEU}_{X}(x,n)
\end{equation}

\paragraph{Embedding Based Diversity} To evaluate semantic diversity, we embed generated prompts in latent space based on sentence embedding model Sentence-BERT~\cite{DBLP:conf/emnlp/ReimersG19}, which can capture semantic nuances between sentences, then we use cosine similarity to compute the similarity between sentences and convert it into semantic diversity, denoted as follows:
\begin{equation}
\small
\begin{split}
& \textit{DIV}_{semantics}(X) = 1 - \\ 
& \frac{1}{\binom{|X|}{2}} 
\sum_{x_i,x_j\in{X},i>j} Sim_{cos}(\text{SBert}(x_i), \text{SBert}(x_j))
\end{split}
\end{equation}

\subsubsection{Baselines}
We compare \modelname with SAP~\cite{DBLP:conf/emnlp/DengWFDW023} and FLIRT~\cite{DBLP:journals/corr/abs-2308-04265} which perform red teaming based on ICL as well. SAP adds rationale behind each demonstration. FLIRT provides strategies for demonstration selection in its feedback loops. The strategies include First in First out (FIFO) Strategy, Last in First out (LIFO) Strategy, Scoring Strategy, and Scoring-LIFO Strategy.

\subsubsection{Prompts Scale} 
We start with 12 unique attack prompts to initialize our framework. No special screening process is applied other than ensuring the inclusion of the effective attack prompts. 
These initial seeds represent previous attack attempts on popular LLMs, which can yield responses with varying levels of unsafety.

In the breadth stage, we generate 30 prompts for each sensitive topic across each model, which results in a total of 1920 prompts. Moving to the depth stage, we employ various strategies to evolve 2880 additional prompts, building on a subset of the pre-generated prompts. 

% We start with 12 attack prompts to initialize our framework.
% % Regarding the configuration of the initial attack prompt pool, 
% Notably, we do not subject these prompts to any special screening except ensuring the inclusion of the effective attack prompts and that each prompt is unique. 
% These initial seeds represent past attack attempts on popular LLMs, which can yield responses with varying levels of unsafety.


% In the breadth stage, we generate 30 prompts for each sensitive topic across each model, resulting in 1920 prompts. In the depth stage, we employ various strategies to evolve 2880 prompts based on part of pre-generated prompts. 



\input{a_table/baselines}
\subsection{In-Breadth Evolving: Results \& Analysis}
\subsubsection{ASR and Diversity vs Baselines}
Table \ref{tab:baselines} shows the results of \modelname and baselines on ASR, n-gram based diversity and embedding based diversity.

In the context of LLMs, high ASR often leads to a trade-off with low diversity, resulting in generation mere rewrite of existing exemplars. In turn, attack prompts with high diversity may fail to effectively manipulate the target model into the unsafe zone. However, our \modelname method strikes a balance between ASR and diversity. Our method outperforms all baselines by a large margin. In the breadth stage, we achieve 80\% ASR on GPT-3.5-turbo-0613 and 67\% average ASR across all models, alongside high diversity. Additionally, each in-depth evolving strategy achieve higher diversity based on pre-generated prompts.

Regarding the impact of different seed prompts on the framework's performance, we conduct experiments using various sets of initial seeds. Detailed results are provided in Appendix \ref{app:seed}. The experimental results indicate that the superior performance is due to the robustness of our method, rather than a careful selection of initial seeds. 



\subsubsection{Integral Safety}
 For the candidate model set $M=\{M_{t1}, M_{t2}, \ldots, M_{tn}\}$, we first select one model $M_{ti}$ as the target in the breadth evolving stage described in algorithm \ref{algo:breadth}. Then we use these prompts to attack other models in the candidate model set. 

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.5\textwidth]{a_fig/asr.pdf}
  \caption{ASR on different target models, where the horizontal axis shows models used as the target in the generation phase, and the vertical axis shows models used as the target after the generation. The numbers at the bottom represent the average ASR on the corresponding target models on the vertical axis.}
  \label{fig:asr}
\end{figure}

Based on multiple generations and attacks, we obtain a matrix of ASR and calculate the average ASR for each target model, as shown in figure \ref{fig:asr}. It appears that prompts generated for a target model are largely effective for other models as well. 

Despite alignment efforts, none of these models demonstrate complete immunity to textual attacks. The GPT-3.5 models suffer from average ASR of 69.11\% (considering GPT-3.5-0301 and GPT-3.5-0613), while Qwen suffers from average ASR of 67.70\%. Llama-2 models and their variants demonstrate a notable reduction in susceptibility to attacks, with average ASR of 55.50\% (considering Llama-2-7b, Llama-2-13b, Vicuna-7b, and Vicuna-13b). The different safety performances can be attributed to their respective data compositions and alignment methods, which reflects the efficacy of the safety measures employed by Llama-2. 

From a temporal perspective, \textbf{earlier versions of models are more vulnerable to textual attacks compared to later ones.} The ASR of Llama-1 reaches as high as 76.74\%, indicating the lack of emphasis on safety alignment in early LLMs. Through subsequent enhancements, LLMs have evolved to be more safe and dependable. For example, the ASR of GPT-3.5-0613 decreased by 2.79\% compared to GPT-3.5-0301, and the ASR of Llama-2-7b decreased by 17.87\% compared to Llama-1-7b. 

Additionally, we observed that in comparison to the 7b models, Llama-2-13b and Vicuna-13b demonstrate inferior safety performance. \textbf{This indicates that larger-scale models may require further alignment.}

\subsubsection{Safety on Sensitive Topics}
We follow \citet{DBLP:conf/emnlp/DengWFDW023} using eight sensitive topics and analyze the safety of different models across these topics, including fraud, politics, pornography, race, religion, suicide, terrorism, and violence.

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.40\textwidth]{a_fig/radar.pdf}
  \caption{ASR on different models across sensitive topics.}
  \label{fig:radar}
\end{figure}


Figure \ref{fig:radar} depicts ASR of various LLMs across sensitive topics. The results indicate that among the eight sensitive topics, \textbf{textual attacks on the topic of ``fraud'' are more likely to breach the safe guardrail of LLMs,} with average ASR of 73.34\% across all models. We think that high ASR in fraud-related contexts may be attributed to LLMs' exceptional role-playing capability, which attackers could exploit to simulate specific individuals or organizations. Additionally, the hallucination in LLMs could be manipulated to generate information that appears highly authentic but false, particularly conducive to generating fraudulent content. On the other hand, \textbf{LLMs exhibit less vulnerability when it comes to topics like ``terrorism'' (61.67\%) and ``suicide'' (60.41\%),} likely due to the attack prompts constructed on these topics tend to display more aggressiveness, making them more easily detected.


\subsubsection{Ablation Studies}
\paragraph{Inferior Example} In order to evaluate the effect of inferior example in prompt generation, we set experiments to remove inferior example and replace inferior example with superior one respectively. Table \ref{tab:ablation} shows that compared with removing inferior example or replacing it with superior example, our practice to keep inferior example provides a good trade-off for ASR and diversity of generated prompts.

\paragraph{Mutagenic Factor}
To explore the impact of mutagenic factor, we try to remove the mutagenic factor module. Table \ref{tab:ablation} shows the presence of mutagenic factor has led to improvements in ASR and diversity. In addition, we conduct experiments to explore various literary genres as mutagenic factor. Detailed experimental results are presented in Appendix \ref{app:gen}.

\input{a_table/ablation}

\subsection{In-Depth Evolving: Results \& Analysis}\label{in-depth}

In this part, we present results and analysis derived from the depth evolving stage across the strategies proposed in Section \ref{sec:second}. For clarity, the model referred to below is based on GPT-turbo-3.5-0613.

\input{a_table/granularity}

\subsubsection{Downward Expansion}

In this strategy, We ask LLM to generate a series of subtopics centered around original topics. For instance, taking ``fraud'' as topic-i, we generate subtopics like ``charity fraud'', ``telecom fraud'' as topic-ii under ``fraud''. And under ``charity fraud'', we generate ``creating fake charity events or donation drives, where \dots'' as topic-iii. Additionally, we use attack model as a rewriter to evolve the given prompt from it’s original topic to the more fine-grained topic which introduces customized harmful content. Table \ref{tab:granu} illustrates the ASR of prompts on the topics with different grain sizes. Although with the addition of more detailed unsafe content, we can still achieve ASR of 71.67\% on topic-iii, which showcases the robustness of the pre-generated prompts.


\begin{figure}[]
  \centering
  \includegraphics[width=0.4\textwidth]{a_fig/dialogue.pdf}
  \caption{ASR of 2-, 3-, 4-, and 5-round dialogues.}
  \label{fig:dialogue}
\end{figure}




\subsubsection{Restructure}

We restructure the pre-generated prompt by shuffling its word order and employ the attack model for reorganizing. Then we prompt the target to response. The result reveals that the attack's efficacy is largely maintained, with ASR of 77.5\% post-restructuring. This proves the resilience of the pre-generated attack prompt, which retains its potency despite alterations to the word order. Furthermore, it also suggests that the specific order of words may not be the pivotal element in attack effectiveness.


\subsubsection{Dialogue Simulation}
We evolve pre-generated prompts into 2-, 3-, 4-, and 5-round dialogues respectively and calculate the ASR in conjunction with their context. If the target model continues without protest to the unsafe historical dialogues, we consider the attack successful. The results shown in Figure \ref{fig:dialogue} indicates that as the number of rounds in historical dialogues increases, the ASR rises, peaking at 74.58\% in 4-round historical dialogues. One possible reason is that the target model loses accurate judgement of the context as the given dialogues with more rounds. As the dialogues over 4-round, we observed a downtrend in ASR. This could be attributed to the unsafe intent in the original prompts being corrected during evolving as the dialogues become longer.


\input{a_table/length}


\subsubsection{Length Declining}
We propose several methods for length declining of prompts and evaluate their effectiveness using the length-to-ASR ratio, higher ratio means that the method can effectively attack target model using less tokens. Table \ref{tab:length} illustrates the length-to-ASR ratio of different length declining methods.

\paragraph{Simple Truncation} The original prompts have an average token size of 111.2. We conduct simple truncation and yield prompt set with average token size of 72.15, 50.46. Then we employ textual attack on target model using these prompts, resulting in ASR of 65.83\%, 57.08\%, respectively. It can be seen that if the structure of the attack prompt is disrupted and not compensated for by method like ``restructure'', it will lead to lower ASR.
\paragraph{Word Frequency} After removing 30 most frequently occurring nouns, verbs, and adjectives respectively from the original prompts, the average token size of prompts is 97.43. The result indicates a significant decrease (80\%$\rightarrow$48.75\%) in ASR. We find that high-frequency words share common characteristics such as abstraction, negativity, secrecy, and artistry, signifying a weak link in LLM's defense.
 
\paragraph{LLM-based} We employ LLM to condense the original prompts to average token size of 52.19, achieving ASR of 68.33\% which is still a gap compared to prompts generated in the breadth evolving stage. However, this method yields the highest length-to-ASR ratio as shown in table \ref{tab:length}.



