\section{Appendix}
\subsection{Algorithm for Semantic Tokenization}\label{sec:semantic_token}
As shown in Algorithm~\ref{alg:rq}, we present RQ-VAE for semantic tokenization.
\begin{figure}[!htb]
\vspace{-1em}
\centering
\small
\begin{algorithm}[H]
\caption{RQ-VAE for Semantic Tokenization}\label{alg:rq}
\textbf{Input:} Sentence embedding $\mathcal{X}_{u} = (\boldsymbol{x}_{i_{1}}, \boldsymbol{x}_{i_{2}}, \ldots, \boldsymbol{x}_{i_{T}})$ of user $u$\\
\textbf{Output:} Semantic representation $\hat{\mathcal{Z}}_{u} = (\hat{\boldsymbol{z}}_{i_{1}}, \hat{\boldsymbol{z}}_{i_{2}}, \ldots, \hat{\boldsymbol{z}}_{i_{T}})$ of user $u$\\
\begin{algorithmic}[1]
\FOR{$t = 1 \rightarrow T$ in parallel} 
 \STATE $\boldsymbol{z}_{i_t} = \textbf{Encoder} ({\boldsymbol{x}}_{i_t})$  \# encode the text embedding
\STATE $\boldsymbol{r}_1 = \boldsymbol{z}_{i_t}$, $\hat{\boldsymbol{{z}}}_{i_t} = 0$
    \FOR{$l = 1 \rightarrow L$}
            \STATE $\left\{\boldsymbol{e}^c_{k}\right\}_{k=1}^K, \boldsymbol{e}^c_{k} \in \mathbb{R}^{1 \times D'}$ \# codebook embedding of each layer 
        \STATE $k=\arg \min_k\left\|\boldsymbol{r}_{l}-\boldsymbol{e}^c_{k}\right\|$ \# search the index of closest codebook
        \STATE $\boldsymbol{r}_{l + 1} = \boldsymbol{r}_l-\boldsymbol{e}^c_{k}$ 
 \STATE $\hat{\boldsymbol{{z}}}_{i_t} += \boldsymbol{e}^c_{k}$ \# accumulate the quantized embedding
 \STATE $\mathcal{L}_{\text {rqvae }} += \left\|\operatorname{sg}\left[\boldsymbol{r}_l\right]-\boldsymbol{e}^c_{k}\right\|^2+\beta\left\|\boldsymbol{r}_l-\operatorname{sg}\left[\boldsymbol{e}^c_{k}\right]\right\|^2$ \# $\operatorname{sg}$ means stop gradient
    \ENDFOR
 \STATE $\hat{\boldsymbol{x}}_{i_t} = \textbf{Decoder}(\hat{\boldsymbol{z}}_{i_t})$  \# decode the quantized semantic embedding
  \STATE $\mathcal{L}_{\text {recon}} += \left\|\boldsymbol{x}_{i_t} - \hat{\boldsymbol{x}}_{i_t}\right\|^2$ \# reconstruction loss
    \ENDFOR
    \STATE \textbf{return} $\hat{\mathcal{Z}}_{u}$
\end{algorithmic}
\end{algorithm}
\vspace{-1em}
\end{figure}
\subsection{Implementation Details}\label{appendix:implementation}
Following TIGER~\citep{rajput2024recommender}, to obtain the semantic tokens, we utilize the pre-trained Sentence-T5~\citep{ni2021sentence}. Specifically, we construct item's sentence description using its content features, including title, brand, category and price. This constructed sentence is then fed into Sentence-T5, which outputs a 768-dimensional text embedding for each item as the input in our task. Besides, the RQ-VAE model includes a DNN encoder, a residual quantizer, and a DNN decoder. The DNN encoder takes the input text embedding and transforms the dimension to be aligned with codebook embedding. This encoder is activated by ReLU with layer sizes 512, 256, and 128, which ultimately produces a 64-dimensional latent representation. With the 64-dimensional latent representation from encoder, the residual quantizer then performs three levels of residual quantization. At each level, a codebook with size $K$ is used, where each token within the codebook has a dimension of 64. The output semantic token quantized by residual quantizer is then fed into the DNN decoder, which decodes it back to the original text embedding space. Note different from TIGER, we set the dimension of semantic token as 64 for alignment with ID token in our sequential recommendation setting. 

As for the implementation of sequential recommendation, we directly use the framework of $\text{S}^3\text{-Rec}$~\citep{zhou2020s3}. But as we train the model in an end-to-end manner, we just use the fine-tuning setting and do not use the pre-training setting of their framework. In our setting, we employ the Adam optimizer~\citep{kingma2014adam} with a learning rate of 0.001 and the batch size is set as 256.

\subsection{Baselines}\label{appendix:baseline}
In this section, we provide a brief overview of the baseline models employed for comparison:
\begin{itemize}[leftmargin=*]
\item \textbf{FM}~\citep{fm}: The Factorization Machine (FM) model characterizes pairwise interactions among variables through a factorized representation.
    \item \textbf{GRU4Rec}~\citep{gru4rec}: This model represents the pioneering application of recurrent neural networks (RNNs) for sequential recommendation, specifically utilizing a customized Gated Recurrent Unit (GRU).
    \item \textbf{Caser}~\citep{caser}: Caser introduces a convolution neural network (CNN) architecture designed to capture high-order Markov Chains. It achieves this through the implementation of both horizontal and vertical convolution operations tailored for sequential recommendation.
\item \textbf{HGN}~\citep{hgn}: The Hierarchical Gating Network (HGN) effectively models long-short-term user preference through an innovative gating mechanism.
\item \textbf{SASRec}~\citep{sasrec}: Self-Attentive Sequential Recommendation (SASRec) employs a causal masked self attention to model userâ€™s historical behavior sequence.
\item \textbf{BERT4Rec}~\citep{bert4rec}: This model applies the bi-directional Transformer BERT for enhanced sequential recommender.
\end{itemize}

\subsection{Data Description}\label{appendix:data}
\begin{table}[htb!]
% \vspace{-1.6cm}
\centering
\caption{Data statistics for benchmark datasets after 5-core filtering. Here Sports and Toys are the `Sports and Outdoors' and `Toys and Games', respectively, from Amazon review datasets.}
\label{tab:data}
\begin{tabular}{ccccc}
\toprule
Dataset & \# Users &  \# Items & {Average Len.}\\
\midrule
Beauty & 22,363 & 12,101 & 8.87 \\
Sports & 35,598 & 18,357 & 8.32\\
Toys & 19,412 & 11,924 & 8.63 \\
\bottomrule
\end{tabular}
\end{table}
We utilize three real-world benchmark datasets derived from the Amazon Product Reviews dataset~\citep{he2016ups}, which includes user reviews and item metadata spanning from May 1996 to July 2014. In our task, we focus on three specific categories within this dataset: "Beauty," "Sports and Outdoors," and "Toys and Games." Table~\ref{tab:data} presents a summary of the statistics associated with these datasets, where "Average Len." represents the average length of all users' item sequences. To construct item sequences, we organize users' review histories chronologically by timestamp, ensuring that only users with a minimum of five reviews are retained in our analysis.


\subsection{Codebook Size Study}\label{appendix:codebooksize}

\begin{table*}[!htb]
\centering
\caption{Increasing codebook size does not improve the performance too much on Sports dataset.}
\label{tab:codebook_size}
\begin{tabular}{cccccc}
\hline
Codebook   Size & HR@5            & NDCG@5          & HR@10           & NDCG@10         & MRR             \\ \hline
64              & 0.3792          & 0.2675          & 0.5138          & 0.3109          & 0.2675          \\ \hline
128             & \textbf{0.3849} & \textbf{0.2717} & \textbf{0.5247} & \textbf{0.3168} & \textbf{0.2722} \\ \hline
256             & 0.3786          & 0.2672          & 0.5184          & 0.3123          & 0.2688          \\ \hline
521             & 0.3842          & 0.2719          & 0.5218          & 0.3163          & 0.2720          \\ \hline
1024            & 0.3809          & 0.2691          & 0.5202          & 0.3140          & 0.2696          \\ \hline
\end{tabular}
\end{table*}

% \begin{table}[!htb]
% \centering
% \caption{Increasing codebook size does not improve the performance too much on Sports dataset.}
% \label{tab:codebook_size}
% \begin{tabular}{cccccc}
% \hline
% Codebook   Size & HR@5            & NDCG@5          & HR@10           & NDCG@10         & MRR            \\ \hline
% 256             & 0.3786          & 0.2672          & 0.5184          & 0.3123          & 0.2688         \\ \hline
% 512             & \textbf{0.3842} & \textbf{0.2719} & \textbf{0.5218} & \textbf{0.3163} & \textbf{0.272} \\ \hline
% 1024            & 0.3809          & 0.2691          & 0.5202          & 0.314           & 0.2696         \\ \hline
% \end{tabular}
% \end{table}
As the first and third codebook in Amazon Sports dataset degenerate in Figure~\ref{fig:vis_sport}, we want to study whether the size of codebook $K$ has significant impact on this degeneration problem. Thus we vary the codebook size $K$ from 64 to 1024 as Table~\ref{tab:codebook_size}, and have the following discovery.
\begin{itemize}[leftmargin=*]
\item \textbf{Increasing codebook size does not improve the performance too much.} The performance reaches peak when codebook size is 128, but the performance fluctuates when codebook size grows to 256 and over.
\end{itemize}


\begin{figure*}[htb!]
		\centering
		\begin{tabular}{cccc}
\includegraphics[width=0.22\linewidth]{fig/first_layer64Sports_and_Outdoors.png} &
       \includegraphics[width=0.22\linewidth]{fig/second_layer64Sports_and_Outdoors.png}  & \includegraphics[width=0.22\linewidth]{fig/third_layer64Sports_and_Outdoors.png}  &
       \includegraphics[width=0.22\linewidth]{fig/unique64Sports_and_Outdoors.png}
	     \\ First Codebook & Second Codebook & Third Codebook & Unique Tokens
		\end{tabular}
	\caption{The patterns of codebooks are various across different layers but kind of sparse on Sports dataset with codebook size 64.}	\label{fig:vis_sports_64}
\end{figure*} 



\begin{figure*}[htb!]
		\centering
		\begin{tabular}{cccc}
\includegraphics[width=0.22\linewidth]{fig/first_layer128Sports_and_Outdoors.png} &
       \includegraphics[width=0.22\linewidth]{fig/second_layer128Sports_and_Outdoors.png}  & \includegraphics[width=0.22\linewidth]{fig/third_layer128Sports_and_Outdoors.png}  &
       \includegraphics[width=0.22\linewidth]{fig/unique128Sports_and_Outdoors.png}
	     \\ First Codebook & Second Codebook & Third Codebook & Unique Tokens
		\end{tabular}
	\caption{The patterns of codebooks are various across different layers on Sports dataset with codebook size 128.}	\label{fig:vis_sports_128}
\end{figure*} 

\begin{figure*}[htb!]
		\centering
		\begin{tabular}{cccc}
\includegraphics[width=0.22\linewidth]{fig/first_layerSports_and_Outdoors.png} &
       \includegraphics[width=0.22\linewidth]{fig/second_layerSports_and_Outdoors.png}  & \includegraphics[width=0.22\linewidth]{fig/third_layerSports_and_Outdoors.png}  &
       \includegraphics[width=0.22\linewidth]{fig/uniqueSports_and_Outdoors.png}
	     \\ First Codebook & Second Codebook & Third Codebook & Unique Tokens
      \end{tabular}
	\caption{The first and third codebooks start to degenerate on Sports dataset with codebook size 256.}	\label{fig:vis_sport_256}
\end{figure*} 


\begin{figure*}[htb!]
		\centering
		\begin{tabular}{cccc}
\includegraphics[width=0.22\linewidth]{fig/first_layer512Sports_and_Outdoors.png} &
       \includegraphics[width=0.22\linewidth]{fig/second_layer512Sports_and_Outdoors.png}  & \includegraphics[width=0.22\linewidth]{fig/third_layer512Sports_and_Outdoors.png}  &
       \includegraphics[width=0.22\linewidth]{fig/unique512Sports_and_Outdoors.png}
	     \\ First Codebook & Second Codebook & Third Codebook & Unique Tokens
		\end{tabular}
	\caption{The first and third codebooks still degenerate on Sports dataset with codebook size 512. And the second codebook also begin to degenerate.}	\label{fig:vis_sports_512}
\end{figure*} 

\begin{figure*}[htb!]
		\centering
		\begin{tabular}{cccc}
\includegraphics[width=0.22\linewidth]{fig/first_layer1024Sports_and_Outdoors.png} &
       \includegraphics[width=0.22\linewidth]{fig/second_layer1024Sports_and_Outdoors.png}  & \includegraphics[width=0.22\linewidth]{fig/third_layer1024Sports_and_Outdoors.png}  &
       \includegraphics[width=0.22\linewidth]{fig/unique1024Sports_and_Outdoors.png}
	     \\ First Codebook & Second Codebook & Third Codebook & Unique Tokens
		\end{tabular}
	\caption{Almost all codebooks degenerate on Sports dataset with codebook size 1024. In particular, the first and second codebooks degenerate extremely.}	\label{fig:vis_sports_1024}
\end{figure*} 

Besides, we also visualize the token distribution when codebook sizes are 64, 256, 512 and 1024 as Figure~\ref{fig:vis_sports_64} to \ref{fig:vis_sports_1024}. From the figure we can discover that:
\begin{itemize}[leftmargin=*]
\item \textbf{The codebooks begin to degenerate and be redundant when codebook size is greater than 256.} The first layer and second layer of codebooks begin to degenerate when codebook size is 256. With the increase of codebook size, the degeneration problem becomes more serious.
\item \textbf{The unique tokens are not influenced by codebook size too much.} With the growth of codebook size, the distribution of unqiue tokens almost keep unchange.

\end{itemize}

\begin{figure*}[htb!]
		\centering
		\begin{tabular}{cccc}
\includegraphics[width=0.22\linewidth]{fig/first_layer128Sports_and_Outdoors.png} &
       \includegraphics[width=0.22\linewidth]{fig/second_layer128Sports_and_Outdoors.png}  & \includegraphics[width=0.22\linewidth]{fig/third_layer128Sports_and_Outdoors.png}  &
       \includegraphics[width=0.22\linewidth]{fig/unique128Sports_and_Outdoors.png}
	     \\ First Codebook & Second Codebook & Third Codebook & Unique Tokens
      \end{tabular}
	\caption{The patterns of codebooks are various across different layers and unique tokens are uniform for different items on Sports dataset.}	\label{fig:vis_sport}
\end{figure*} 

\begin{figure*}[htb!]
		\centering
		\begin{tabular}{cccc}
\includegraphics[width=0.22\linewidth]{fig/first_layerToys_and_Games.png} &
       \includegraphics[width=0.22\linewidth]{fig/second_layerToys_and_Games.png}  & \includegraphics[width=0.22\linewidth]{fig/third_layerToys_and_Games.png}  &
       \includegraphics[width=0.22\linewidth]{fig/uniqueToys_and_Games.png}
	     \\ First Codebook & Second Codebook & Third Codebook & Unique Tokens
		\end{tabular}
	\caption{The patterns of codebooks are various across different layers and unique tokens are uniform for different items on Toys dataset.}	\label{fig:vis_toys}
\end{figure*} 

\subsection{Token Visualization on More Datasets}\label{sec:visual_token}
As shown in Figure~\ref{fig:vis_sport} and \ref{fig:vis_toys}, we visualize the patterns of codebooks on Sport and Toys datasets.
