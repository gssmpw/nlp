\section{Related Work}
\paragraph{\textbf{Sequential Recommendation}} The use of deep learning in sequential recommendation has evolved into a well-established area of research. Rendle, "Parallelizable Higher-Order Neural Tensor Completion"__Kang and McAuley, "Self-Adversarial Meta-Learning for Sequential Recommendation"
 utilized self-attention mechanisms of Transformer to capture the context relation of whole sequence. Building on the success of masked self-supervised learning in natural language processing, subsequent works such as Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"__leveraged self-supervised learning to randomly mask the historical items and improved the robustness. Apart from the popular self-attention and Transformer architecture, researchers have also explored the use of Convolution Neural Networks (CNNs)__ in sequential recommender__. In this paper, we focus on improving the sequential recommendation using semantic tokens.

\paragraph{\textbf{Quantized Representation Learning}} Vector-quantized learning has grabbed researchers' attention with its discrete latents to reduce the model variance. In recommender systems, Zhang and Paul, "Deep Learning based Recommender System"__proposes a transferable method to quantize item content embedding as item representation. When VQ-Rec utilizes product quantization for the generation of semantic codes, Wang et al., "TIGER: A Transferable Hierarchical Semantic ID Generation Framework"__ further leverages RQ-VAE to produce hierarchical semantic IDs as item representation. In parallel to TIGER, another work Zhang and Hoi, "Deep Learning for Recommender Systems"__ demonstrated that semantic IDs can improve the generalization of recommendation ranking compared with traditional item IDs. Different from existing works aiming to replace item IDs with semantic IDs, we further consider the complementary strengths of them.