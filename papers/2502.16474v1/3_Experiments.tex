
\section{Experiments}
In our experiments, we evaluate the proposed method on three real-world benchmark datasets, focusing on the following key research questions (RQs): \textbf{RQ1}: Does the proposed unified representation learning method outperform state-of-the-art sequential recommendation models in terms of prediction accuracy? \textbf{RQ2}: What is the impact of our unified semantic and ID tokenization method on recommendation performance? Additionally, is the integration of cosine similarity and Euclidean distance effective in improving the final recommendation performance?
\textbf{RQ3}: To what extent can we reduce the dimensionality of ID tokens without compromising performance? Specifically, how does the modelâ€™s performance vary with different ID token dimensions?
\textbf{RQ4}: What patterns do the semantic and ID tokens learn, and how do these tokens contribute to the overall representation of items?

Additionally, in Appendix~\ref{appendix:codebooksize}, we explore the effects of varying the codebook size on the patterns learned by the semantic tokens.

\subsection{Experimental Setup}

\paragraph{\textbf{Datasets}} We evaluate the recommendation performance on Amazon product review datasets~\citep{he2016ups}. The statistics of these three benchmark datasets after applying 5-core filtering are presented in Appendix~\ref{appendix:data}.

\paragraph{\textbf{Evaluation Metrics}} We follow the approach used in prior work~\citep{zhou2020s3}, using Hit Ratio (HIT@k), Normalized Discounted Cumulative Gain (NDCG@k), and Mean Reciprocal Rank (MRR) as evaluation metrics, where $k$ is the number of top ranked items. Consistent with previous studies~\citep{zhou2020s3, dcn}, given a user behavior sequence, we use the last item for testing, the second-to-last item for validation, and the rest for training. Given the large item set, ranking against all possible items is computationally expensive. Therefore, following a commonly used approach~\citep{sasrec, man}, we evaluate the model by sampling 99 negative items along with the ground-truth item. All metrics are calculated based on the ranking of sampled and ground-truth items, and we present the mean scores across users.

\paragraph{\textbf{Baselines}}
To evaluate the pure impact of semantic tokenization, we compare our proposed method against several competitive recommendation baselines, including FM~\citep{fm}, GRU4Rec~\citep{gru4rec}, Caser~\citep{caser}, SASRec~\citep{sasrec}, BERT4Rec~\citep{bert4rec}, and HGN~\citep{hgn}. It is important to note that we do not compare our method with existing work~\citep{rajput2024recommender} that utilizes a different model architecture with a deeper network when incorporating RQ-VAE. The primary focus here is to examine the effects of semantic tokenization within the context of the same sequential recommendation model to ensure a fair and consistent comparison. Besides, we directly use the results of all baseline from prior work~\citep{zhou2020s3} and implement our method based on SASRec under its framework for a fair comparison. Besides, we show the detailed description of these baselines in Appendix~\ref{appendix:baseline}.

\paragraph{\textbf{Hyper-parameter Settings}} We directly use the results of all baseline from prior work~\citep{zhou2020s3} and implement our method based on its framework for a fair comparison. Besides, we set some new hyper-parameters of RQ-VAE following prior work~\citep{rajput2024recommender} with $L=3$ layers of codebook. We search the codebook size $K$ from 64 to 1024 and select 256 for both Beauty and Toys dataset, while 128 for Sports dataset. Besides, we set the dimension of codebook $D' = 64$ to align with the ID token only method. All other parameters like recommendation model layer and hidden size are set strictly the same as baselines. Additionally, we put more implementation details in Appendix~\ref{appendix:implementation}.

\subsection{Overall Performance}

\begin{table*}[t!]
\centering
\caption{Our method improves baseline significantly by 6\% to around 18\% on three benchmark datasets.}
\label{tab:overall}
% \tabcolsep=1mm
\begin{tabular}{cc|cccccc|c|c}
\toprule
Datasets & Metric  & FM     & GRU4Rec & Caser  & SASRec       & BERT4Rec     & HGN    & Ours            & Improv. \\ \midrule
\multirow{5}{*}{Beauty} & HIT@5 & 0.1461 & 0.3125 & 0.3032 & {\ul 0.3741} & 0.3640 & 0.3544 & \textbf{0.4201} & 12.30\% \\  
         & NDCG@5  & 0.0934 & 0.2268  & 0.2219 & {\ul 0.2848} & 0.2622       & 0.2656 & \textbf{0.3079} & 8.11\%  \\  
         & HIT@10   & 0.2311 & 0.4106  & 0.3942 & 0.4696       & {\ul 0.4739} & 0.4503 & \textbf{0.5318} & 12.22\% \\  
         & NDCG@10 & 0.1207 & 0.2584  & 0.2512 & {\ul 0.3156} & 0.2975       & 0.2965 & \textbf{0.3440} & 9.00\%  \\  
         & MRR     & 0.1096 & 0.2308  & 0.2263 & {\ul 0.2852} & 0.2614       & 0.2669 & \textbf{0.3025} & 6.07\%  \\ \midrule
\multirow{5}{*}{Sports} & HIT@5    & 0.1603 & 0.3055  & 0.2866 & {\ul 0.3466} & 0.3375       & 0.3349 & \textbf{0.3849} & 11.05\% \\
                        & NDCG@5  & 0.1048 & 0.2126  & 0.2020 & {\ul 0.2497} & 0.2341       & 0.2420 & \textbf{0.2717} & 8.81\%  \\
                        & HIT@10   & 0.2491 & 0.4299  & 0.4014 & 0.4622       & {\ul 0.4722} & 0.4551 & \textbf{0.5247} & 11.12\% \\
                        & NDCG@10 & 0.1334 & 0.2527  & 0.2390 & {\ul 0.2869} & 0.2775       & 0.2806 & \textbf{0.3168} & 10.42\% \\
                        & MRR     & 0.1202 & 0.2191  & 0.2100 & {\ul 0.2520} & 0.2378       & 0.2469 & \textbf{0.2722} & 8.02\%  \\ \midrule
\multirow{5}{*}{Toys}   & HIT@5 & 0.0978 & 0.2795 & 0.2614 & {\ul 0.3682} & 0.3344 & 0.3276 & \textbf{0.4340} & 17.87\% \\  
         & NDCG@5  & 0.0614 & 0.1919  & 0.1885 & {\ul 0.2820} & 0.2327       & 0.2423 & \textbf{0.3141} & 11.38\% \\  
         & HIT@10   & 0.1715 & 0.3896  & 0.3540 & {\ul 0.4663} & 0.4493       & 0.4211 & \textbf{0.5456} & 17.01\% \\  
         & NDCG@10 & 0.0850 & 0.2274  & 0.2183 & {\ul 0.3136} & 0.2698       & 0.2724 & \textbf{0.3501} & 11.64\% \\  
         & MRR     & 0.0819 & 0.1973  & 0.1967 & {\ul 0.2842} & 0.2338       & 0.2454 & \textbf{0.3064} & 7.81\%  \\ \bottomrule
\end{tabular}
\end{table*}
To compare the performance of our method with existing sequential recommenders, as shown in Table~\ref{tab:overall}, we evaluate them in three benchmark datasets under five metrics. From the table, we can have the following observation: \textbf{Our method achieves significant improvement.} The improvement of our method towards baselines ranges from 6.07\% to 17.87\%, which is very significant in sequential recommendation task~\citep{sasrec, zhou2020s3}. Besides, our method improves more on HIT metric than NDCG metric and MRR metric. This may be because semantic embedding is naturally less insensitive at ranking position due to duplicate tokenization, though we have added unique ID embedding.


\subsection{Ablation Study}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage[normalem]{ulem}
% \useunder{\uline}{\ul}{}
\begin{table*}[!htb]
% \tabcolsep=0.5mm
\small
\centering
\caption{Unified tokenization outperforms ID-only and semantic-only tokenizations with significant reduction of token size. Besides, the semantic tokenization outperforms ID tokenization in position-insensitive metric.}
\label{tab:token}
\begin{tabular}{cc|c|c|c|c|c|c|c}
\toprule
\multirow{2}{*}{Dataset} &
  \multirow{2}{*}{Method} &
  \multicolumn{3}{c|}{Metric} &
  \multicolumn{3}{c|}{Token Size} &
  \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Token\\ Reduction\end{tabular}} \\ 
 &
   &
  HIT@10 &
  NDCG@10 &
  MRR &
  ID &
  Semantic &
  Total &
   \\ \midrule
\multirow{3}{*}{Beauty} &
  ID &
  0.4654 &
  {\ul 0.3121} &
  {\ul 0.282} &
  12,101 $\times$ 64 &
  0 &
  774,464 &
  \textbackslash{} \\  
 &
  Semantic &
  {\ul 0.4956} &
  0.2914 &
  0.2476 &
  0 &
  3 $\times$ 256 $\times$ 64 &
  49,152 &
  93.65\% \\  
 &
  Unified &
  \textbf{0.5318} &
  \textbf{0.344} &
  \textbf{0.3025} &
  12,101 $\times$ 8 &
  3 $\times$ 256 $\times$ 64 &
  145,960 &
  81.15\% \\ \midrule
\multirow{3}{*}{Sports} &
  ID &
  0.4582 &
  {\ul 0.2826} &
  {\ul 0.2482} &
  18,357 $\times$ 64 &
  0 &
  1,174,848 &
  \textbackslash{} \\  
 &
  Semantic &
  {\ul 0.4704} &
  0.2554 &
  0.2131 &
  0 &
  3 $\times$ 128 $\times$ 64 &
  24,576 &
  97.91\% \\  
 &
  Unified &
  \textbf{0.5247} &
  \textbf{0.3168} &
  \textbf{0.2722} &
  18,357 $\times$ 8 &
  3 $\times$ 128 $\times$ 64 &
  171,432 &
  85.41\% \\ \midrule
\multirow{3}{*}{Toys} &
  ID &
  0.4603 &
  {\ul 0.3092} &
  {\ul 0.2804} &
  11,924 $\times$ 64 &
  0 &
  763,136 &
  \textbackslash{} \\  
 &
  Semantic &
  {\ul 0.4644} &
  0.2741 &
  0.236 &
  0 &
  3 $\times$ 256 $\times$ 64 &
  49,152 &
  93.56\% \\  
 &
  Unified &
  \textbf{0.5456} &
  \textbf{0.3501} &
  \textbf{0.3064} &
  11,924 $\times$ 8 &
  3 $\times$ 256 $\times$ 64 &
  144,544 &
  81.06\% \\ \bottomrule
\end{tabular}
\end{table*}
To further study the performance of different tokenization methods, we compare our method with the ID tokenization only method and semantic tokenization only method as Table~\ref{tab:token}. From the table, we can have the following observations: (1) \textbf{Unified tokenization performs best with significant reduction of token.} In all these three benchmark datasets, our proposed method is significantly superior to solely ID tokenization and semantic tokenization methods. More importantly, compared with the traditional ID tokenization method, our method reduces by at least 80\% and even 85\% of tokens on Sports dataset. Here we reduce the tokens by replacing 56 dimensions of ID tokens with a small amount of semantic tokens, which supports our previous analysis that most ID tokens are redundant. (2) \textbf{Semantic tokenization outperforms ID tokenization in position-insensitive metric with significant reduction of token.} In three datasets, it is obvious that the semantic tokenization only method even outperforms ID tokenization only method on HIT metric with less than 10\% of tokens. This result also supports our previous analysis that semantic tokenization is effective at generalization and capturing high-level semantic information. However, semantic tokenization only method often performs poor at NDCG and MRR metrics which are sensitive to position. This is because the position of duplicate tokenized items from semantic tokenization only method are hard to distinguish in ranking.
           

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table*}[!htb]
% \tabcolsep=0.5mm
\centering
\caption{The integration of Euclidean distance into cosine similarity can improve the recommendation performance.}
\label{tab:abl_distance}
\begin{tabular}{c|ccc|ccc|ccc}
\toprule
\multirow{2}{*}{Method} & \multicolumn{3}{c|}{Beauty} & \multicolumn{3}{c|}{Sports} & \multicolumn{3}{c}{Toys}  \\  
                        & HIT@10  & NDCG@10 & MRR    & HIT@10  & NDCG@10 & MRR    & HIT@10 & NDCG@10 & MRR    \\ \midrule
Cosine       & 0.5212  & 0.3334  & 0.2921 & 0.5129  & 0.3081  & 0.2649 & 0.5252 & 0.3309  & 0.2879 \\ 
Unified & \textbf{0.5318} & \textbf{0.3440} & \textbf{0.3025} & \textbf{0.5247} & \textbf{0.3168} & \textbf{0.2722} & \textbf{0.5456} & \textbf{0.3501} & \textbf{0.3064} \\ \bottomrule
\end{tabular}
\end{table*}
Besides, we also compare our method with cosine similarity only method when searching the codebook of RQ-VAE, as shown in Table~\ref{tab:abl_distance}. From the table, we can observe that: \textbf{Our unified method outperforms cosine similarity.} The unified method which integrates cosine similarity with Euclidean distance proposed in Section~\ref{sec:unified_distance} outperforms the solely cosine similarity method on three benchmark datasets. This means our unified cosine similarity and Euclidean distance not only can improve the percentage of activated codebook and coverage of unique items, but also can really improve the final recommendation performance.


\subsection{Hyper-parameter Study}
\begin{figure*}[htb!]
		\centering
		\begin{tabular}{ccc}
		    	\includegraphics[width=0.24\linewidth]{fig/hit.pdf} &  \includegraphics[width=0.24\linewidth]{fig/NDCG.pdf} &
       \includegraphics[width=0.24\linewidth]{fig/mrr.pdf} 
		\end{tabular}
	\caption{The performance improvement shrinks when scaling up dimension of ID token, which means a small proportion of ID tokens is sufficient for capturing the item's unique characteristic.}	\label{fig:hyper_id}
\end{figure*} 
To further verify that we only need a small proportion of ID tokens, we further vary the ID dimension from $\{0, 4, 8, 16\}$ and study the performance under three key metrics as Figure~\ref{fig:hyper_id}. From the figure, we discvoer that: \textbf{The performance improvement shrinks when scaling up dimension of ID token.} It is obvious that the performance improvement becomes less and less with the growing of ID token dimension, and the performance even drops when dimension is greater than 8. This means a small proportion of ID tokens is sufficient for learning the unique information, and others are indeed redundant and can be saved.

\subsection{Token Visualization}
\begin{figure*}[htb!]
		\centering
		\begin{tabular}{cccc}
\includegraphics[width=0.2\linewidth]{fig/first_layerbeauty_4.png} &
       \includegraphics[width=0.2\linewidth]{fig/second_layerbeauty_4.png}  & \includegraphics[width=0.2\linewidth]{fig/third_layerbeauty_3.png}  & \includegraphics[width=0.2\linewidth]{fig/uniquebeauty_3.png}
		     \\ First Codebook & Second Codebook & Third Codebook & Unique Tokens
		\end{tabular}
	\caption{The patterns of codebooks are various across different layers and unique tokens are uniform for different items on Beauty dataset.}	\label{fig:vis_beauty}
\end{figure*} 




% \begin{figure}[htb!]
% 		\centering
% 		\begin{tabular}{c|c|c}
% \includegraphics[width=0.33\linewidth]{fig/uniquebeauty_3.png} &
%        \includegraphics[width=0.33\linewidth]{fig/uniqueSports_and_Outdoors.png}  & \includegraphics[width=0.33\linewidth]{fig/uniqueToys_and_Games.png}  
% 		     \\ (a) Beauty & (b) Sports & (c) Toys 
% 		\end{tabular}
% 	\caption{Visualization of ID tokens on three datasets.}	\label{fig:vis_id}
% \end{figure} 
To study the learned semantic and ID tokens, we further visualize these tokens on Beauty dataset using t-SNE, as shown in Figure~\ref{fig:vis_beauty}. Besides, we also visualize the tokens on Sports and Toys datasets in \ref{fig:vis_sport} and \ref{fig:vis_toys} of Appendix~\ref{sec:visual_token}. Here we label each semantic token with a unique color and thus these are totally $K$ types of color. In ID tokens, we also label them with $K$ types of color to show the distribution when they are assigned with one of the codebooks. Based on the visualized results, we can discover that: (1) \textbf{Semantic tokens vary across different layers.} It is obvious that the semantic tokens vary across different layer on all datasets, which means different layers of semantic codebooks can capture various shared patterns. With the combination of these shared patterns, we can better represent each item's semantic information. (2) \textbf{ID tokens distribute uniformly.} The unique ID tokens are uniform on all datasets. This means the ID token successfully capture the unique characteristic of each item and thus they will not accumulate together.