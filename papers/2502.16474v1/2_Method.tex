
\begin{figure*}[htb!]
		\centering
		\begin{tabular}{c}
		    	\includegraphics[width=0.75\linewidth]{fig/framework.pdf}
		\end{tabular}
	\caption{Framework of the unified semantic and ID representation learning. Firstly, the model integrates both semantic tokens, learned through RQ-VAE, and ID tokens for the recommendation task. Secondly, cosine similarity is applied in the first two layers to decouple accumulated embeddings, while Euclidean distance is utilized in the final layer to effectively distinguish unique items. Finally, the overall model is optimized in an end-to-end manner, combining the recommendation loss, RQ-VAE quantization loss, and text reconstruction loss.}	\label{fig:framework}
\end{figure*}

\section{Unified Representation Learning}
In this section, as illustrated in Figure~\ref{fig:framework}, we introduce a unified semantic and ID representation learning framework. Our method is designed to fully exploit the complementary strengths of semantic and ID tokens, integrate cosine similarity and Euclidean distance, and jointly optimize both the quantization and recommendation tasks. The key components of the framework are described as follows:

\begin{itemize}[leftmargin=*]
    \item \textbf{Unified Semantic and ID Tokenization}: To balance capturing unique and shared item characteristics, we retain only a small proportion of ID token dimensions to represent the unique attributes of items. Meanwhile, the semantic tokens, learned through RQ-VAE, are employed to capture the shared, transferable characteristics across items. This hybrid approach reduces redundancy in the ID space while enhancing generalization.
    
    \item \textbf{Unified Cosine Similarity and Euclidean Distance}: We leverage the strengths of cosine similarity and Euclidean distance in different layers of our model. Specifically, cosine similarity is applied in the earlier layers to effectively decouple accumulated embeddings, while Euclidean distance is employed in the final layer to distinguish unique items. This design maximizes the benefits of both metrics during codebook searching, enhancing the accuracy of item representation.
    
    \item \textbf{End-to-End Joint Optimization}: Our framework is trained in an end-to-end manner, jointly optimizing three key objectives: (1) the recommendation loss to ensure accurate predictions, (2) the RQ-VAE loss for effective codebook assignment, and (3) the text reconstruction loss to maintain the quality of semantic representation. This joint optimization strategy ensures that all components of the model are fine-tuned for optimal performance in both quantization and recommendation tasks.
\end{itemize}


\subsection{Unified Semantic and ID Tokenization}
\begin{figure}[!htb]
		\centering
		\begin{tabular}{c}
		    	\includegraphics[width=0.65\linewidth]{fig/architecture.pdf}
		\end{tabular}
	\caption{Illustration of unified semantic and ID tokenization. Specifically, we replace ID tokens with low-dimension ID tokens and semantic tokens.}	\label{fig:token}
\end{figure}

While ID tokenization is effective at capturing unique, item-specific information, it tends to suffer from redundancy and poor generalization, particularly in cold-start scenarios. In contrast, semantic tokenization excels at generalization by capturing shared, transferable features but may introduce item duplication when similar items are mapped to the same token. Therefore, these two approaches are complementary, and combining their strengths can address their respective limitations.

To this end, we propose a unified tokenization strategy that integrates both ID and semantic tokenization. Given that the number of items \( m \) can be very large, we reduce the dimensionality of the ID embeddings by setting \( D \) smaller than the dimension \( D' \) used for semantic embeddings. As shown in Figure~\ref{fig:token}, our method replaces most dimensions of the ID token with the more generalizable semantic token to reduce redundancy while retaining the ability to capture unique item characteristics. Specifically, for each item \( i_t \) in the userâ€™s interaction history, we concatenate the semantic embedding \( \hat{\boldsymbol{z}}_{i_t} \) and the reduced ID embedding \( \boldsymbol{e}_{i_t} \) to form a unified representation, defined as:
\(
\boldsymbol{s}_{i_t} = [\hat{\boldsymbol{z}}_{i_t}, \boldsymbol{e}_{i_t}],
\)
which results in a sequence of unified embeddings for user \( u \), denoted as:
\(
\hat{\mathcal{S}}_{u} = (\hat{\boldsymbol{s}}_{i_{1}}, \hat{\boldsymbol{s}}_{i_{2}}, \ldots, \hat{\boldsymbol{s}}_{i_{t}})
\)

By combining ID and semantic embeddings, the unified tokenization approach retains the unique characteristics of each item while leveraging the semantic embedding's ability to generalize across similar items. This hybrid representation aims to improve both the efficiency and accuracy of recommendation by reducing redundancy in the ID space and enhancing the model's capacity to generalize to cold-start items.


% As shown in Figure~\ref{fig:token}, in this section, we replace most dimensions of ID tokens with semantic tokens, aiming to reduce the redundancy and improve the generalization ability of representation learning.

% \paragraph{\textbf{Unified Tokenization}} As ID tokenization is good at capturing unique information but falls short in generation and is redundant,  
% semantic tokenization is good at generation but has duplicate problem. That is to say, they are complimentary to each other, and we further make combination of them here. Firstly, given that the number of items \( m \) can be very large, we set the dimension \( D \) typically smaller than the dimension \( D' \) of ID tokenization. Further, we concatenate the ID embedding and semantic embedding for each item together as $\boldsymbol{s}_{i_t}$ = [$\hat{\boldsymbol{{z}}}_{i_t}$, $\boldsymbol{{e}}_{i_t}$]. Then we obtain a sequence of unified embeddings $\hat{\mathcal{S}}_{u} = (\hat{\boldsymbol{s}}_{i_{1}}, \hat{\boldsymbol{s}}_{i_{2}}, \ldots, \hat{\boldsymbol{s}}_{i_{t}})$ for user $u$.
 % $\boldsymbol{M}^c \in \mathbb{R}^{L \times K \times D}$



% \begin{equation}
% \mathcal{C}_l:=\left\{\boldsymbol{e}_{k}\right\}_{k=1}^K
% \end{equation}

% \begin{equation}
% \boldsymbol{z}_i=\textbf{Encoder}({x}_{i})
% \end{equation}
%  \begin{equation}
%  \begin{aligned}
% k_l=\arg \min_k\left\|\boldsymbol{r}_{l}-\boldsymbol{e}^c_{l, k}\right\|, \boldsymbol{r}_{l + 1} = \boldsymbol{r}_l-\boldsymbol{e}^c_{l, k}, \boldsymbol{r}_0 = \boldsymbol{z}_t,\\
%  \hat{\boldsymbol{{z}}}_t = \sum_{l = 1}^{L} \boldsymbol{e}^c_{l, k_l}
% % \\
% % k_{1,t}=\arg \min_k\left\|\boldsymbol{z}_t-\boldsymbol{e}^c_{1, k}\right\|, r_{1, t} = r_{0, t}-\boldsymbol{e}^c_{1, k_{1, t}, 
%  \end{aligned}
% \end{equation}


\subsection{Unified Distance Function}\label{sec:unified_distance}
\begin{table}[!htb]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Type}         & \textbf{Cosine} & \textbf{Euclidean} \\ \hline
First layer           & 97.66\%         & 5.86\%             \\ \hline
Second layer          & 98.44\%         & 100.00\%           \\ \hline
Third layer           & 97.66\%         & 100.00\%           \\ \hline
Total coverage        & 70.13\%         & 92.67\%            \\ \hline
\end{tabular}

\caption{Comparison of cosine similarity and Euclidean distance in terms of the percentage of activated codebook across three layers and total coverage of unique items. Cosine similarity shows a high percentage of activated codebooks in all layers but lower overall coverage of unique items. In contrast, Euclidean distance exhibits high coverage of unique items, but struggles with a significantly lower percentage of activated codebooks in the first layer.}
\label{tab:distance}
\end{table}

To enhance the accuracy of codebook selection in our framework, we aim to improve the distance function used for identifying the closest codebook in $k=\arg \min_k\left\|\boldsymbol{r}_{l}-\boldsymbol{e}^c_{k}\right\|$, as defined in Algorithm~\ref{alg:rq} of Appendix~\ref{sec:semantic_token}.

\begin{figure*}[t!]
		\centering
		\begin{tabular}{ccc}
		    	\includegraphics[width=0.31\linewidth]{fig/cos_categories_first.pdf} &  \includegraphics[width=0.31\linewidth]{fig/cos_categories_second.pdf} &
       \includegraphics[width=0.31\linewidth]{fig/cos_categories_third.pdf} 
		     \\ First Codebook & Second Codebook & Third Codebook
		\end{tabular}
    \caption{Visualization of the codebook selection using cosine similarity across three layers. This figure shows the count of items from various categories assigned to specific token indices, with a focus on the top-3 codebook indices that contain the highest number of items. The distinct distribution of items across different indices suggests that cosine similarity effectively captures category-specific information and helps in distinguishing between categories.}\label{fig:cosine}
\end{figure*} 

\begin{figure*}[!htb]
		\centering
		\begin{tabular}{ccc}
		    	\includegraphics[height=0.38\linewidth]{fig/elu_categories_first.pdf} &  \includegraphics[height=0.38\linewidth]{fig/elu_categories_second.pdf} &
       \includegraphics[height=0.38\linewidth]{fig/elu_categories_third.pdf} 
		     \\ First Codebook & Second Codebook & Third Codebook
		\end{tabular}
 \caption{Visualization of the codebook selection using Euclidean distance across three layers. The uniform distribution of items across categories in the first layer indicates that Euclidean distance struggles to effectively capture category-specific information at this stage, making it less capable of distinguishing between categories compared to later layers.}
\label{fig:elu}
\end{figure*} 

\paragraph{\textbf{Statistical Analysis}} 
Our initial analysis, summarized in Table~\ref{tab:distance}, reveals that cosine similarity activates a high percentage of the codebook but struggles to cover unique items effectively. In contrast, Euclidean distance provides high coverage of unique items but activates a much lower percentage of the codebook, with only 5.86\% activation in the first layer. The limited activation of Euclidean distance in the early layers may result from its difficulty in decoupling accumulated embeddings, as these embeddings tend to cluster tightly at the beginning. Cosine similarity, on the other hand, excels in decoupling these embeddings, possibly due to its ability to handle orthogonal relationships between embeddings. However, cosine similarityâ€™s limited ability to distinguish between distinct embeddings may be attributed to the bounded angular range of 0 to 360$^{\circ}$, while Euclidean distance, grounded in the Cartesian coordinate system, provides a more precise measure for distinguishing embeddings based on distance in \( \mathbb{R} \).





\begin{figure*}[t!]
		\centering
		\begin{tabular}{ccc}
		    	\includegraphics[width=0.31\linewidth]{fig/mix_categories_first.pdf} &  \includegraphics[width=0.31\linewidth]{fig/mix_categories_second.pdf} &
       \includegraphics[width=0.31\linewidth]{fig/mix_categories_third.pdf} 
		     \\ First Codebook & Second Codebook & Third Codebook
		\end{tabular}
  \caption{Visualization of codebook selection using the hybrid approach that combines cosine similarity and Euclidean distance. The variation in the counts of items assigned to different codebook tokens across categories demonstrates the effectiveness of this combined method in capturing category-specific information. The integration of both distance measures enhances the ability of Euclidean distance to distinguish between different categories, leading to more accurate item categorization.}\label{fig:hybrid}
\end{figure*} 

\paragraph{\textbf{Visualized Analysis}} 
To further investigate the performance of cosine similarity and Euclidean distance in codebook selection, we visualized the counts of the top-learned codebooks across different categories using both methods, as shown in Figures~\ref{fig:cosine} and \ref{fig:elu}, respectively. These visualizations demonstrate that cosine similarity can effectively capture category-specific information across layers, while Euclidean distance struggles to do so in the first layer. Specifically, in the first layer, the codebook entries selected by Euclidean distance appear uniformly distributed across categories, indicating that it fails to differentiate between them.

Based on these observations, we propose the following assumption: \textit{Cosine similarity is more effective at minimizing interference within accumulated embeddings but less capable of distinguishing distinct embeddings, whereas Euclidean distance excels at distinguishing unique embeddings but struggles to decouple accumulated ones.}

\begin{table}[!htb]
\centering
\caption{Effectiveness of the hybrid approach combining cosine similarity and Euclidean distance. The integration of Euclidean distance into cosine similarity results in a 100\% activation of the codebook across layers, while also improving the coverage of unique items. This demonstrates the advantage of leveraging both distance measures for more comprehensive and accurate item representation.}
\label{tab:hybrid}
\begin{tabular}{ccc}
\toprule
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Activated\\ codebook\end{tabular}} & First layer & 100.00\% \\ \cline{2-3} 
                & Second layer               & 100.00\% \\ \cline{2-3} 
                & Third layer                & 100.00\% \\ \midrule
\multicolumn{2}{c}{Coverage of unique items} & 83.27\%  \\ \bottomrule
\end{tabular}
\end{table}
\paragraph{\textbf{Proposed Method and Experimental Validation}} 
Building on this assumption, we propose a unified approach that combines cosine similarity and Euclidean distance. In the initial layers, cosine similarity is employed to decouple accumulated embeddings, while Euclidean distance is applied in the final layer to better distinguish unique items. To validate the effectiveness of this hybrid approach, we visualize the codebook selection counts across categories in Figure~\ref{fig:hybrid}. The results show that the combination of cosine similarity and Euclidean distance successfully captures category-specific information. Moreover, as shown in Table~\ref{tab:hybrid}, the percentage of activated codebook entries reaches 100\%, and the coverage of unique items improves significantly compared to using cosine similarity alone.

\paragraph{\textbf{Limitations}} 
Despite the improvements, our proposed method still results in approximately 17\% duplicate items, as observed in Table~\ref{tab:hybrid}. This issue arises when sentence embeddings for certain items are too similar to be distinguished. While this challenge is difficult to completely eliminate, it can be mitigated by assigning a unique, low-dimensional ID token to each item, helping to further differentiate items with highly similar embeddings.







\subsection{End-to-end Joint Optimization}
After unified tokenization of input item sequence for given user $u$, we then can predict the probability of next item as below.
\begin{equation}
    \hat{y}_{u, t } = \Phi (\boldsymbol{s}_{i_1}, \boldsymbol{s}_{i_2}, \cdots \boldsymbol{s}_{i_{t - 1}})
\end{equation}
where $\Phi$ is the sequential recommendation model to predict the probability $\hat{y}_{u, t }$ of next item. Here $\Phi$ can be any type of sequential recommendation models and we use SASRec~\citep{sasrec} here.
Based on the popular logloss~\citep{sasrec,dcn}, we then can optimize the recommendation model as: 
\begin{equation}\label{eq:loss}
\mathcal{L}_{recom}=-\frac{1}{|\mathcal{R}|} \sum_{(u, \mathcal{I}_{u}) \in \mathcal{R}}\left(y_{u, t} \log \hat{y}_{u, t}+\left(1-y_{u, t}\right) \log \left(1-\hat{y}_{u, t}\right)\right)  + \lambda\|\Theta\|,
\end{equation}
where $\mathcal{R}$ represents the training set, $\Theta$ denotes the learnable model parameters, and $\lambda$ denotes the regularization hyper-parameter. Finally, we jointly optimize the loss of recommendation, the loss of RQ-VAE, and the loss of reconstruction for text embedding as $\mathcal{L} = \mathcal{L}_{recom} + \mathcal{L}_{rqvae} + \mathcal{L}_{recon}$ (please refer to the algorithm in Appendix~\ref{sec:semantic_token}).

% \section{Complexity}

