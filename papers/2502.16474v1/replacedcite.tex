\section{Related Work}
\paragraph{\textbf{Sequential Recommendation}} The use of deep learning in sequential recommendation has evolved into a well-established area of research. GRU4REC____ pioneered the application of Gated Recurrent Unit (GRU)-based Recurrent Neural Networks (RNNs) for sequential recommender. Then SASRec____ utilized self-attention mechanisms____ of Transformer to capture the context relation of whole sequence. Building on the success of masked self-supervised learning in natural language processing, subsequent works such as BERT4Rec____ leveraged self-supervised learning to randomly mask the historical items and improved the robustness. Apart from the popular self-attention and Transformer architecture, researchers have also explored the use of Convolution Neural Networks (CNNs)____ in sequential recommender____. In this paper, we focus on improving the sequential recommendation using semantic tokens.

\paragraph{\textbf{Quantized Representation Learning}} Vector-quantized learning has grabbed researchers' attention with its discrete latents to reduce the model variance. In recommender systems, VQ-Rec____ proposes a transferable method to quantize item content embedding as item representation. When VQ-Rec utilizes product quantization____ for the generation of semantic codes, TIGER____ further leverages RQ-VAE to produce hierarchical semantic IDs as item representation. In parallel to TIGER, another work____ demonstrated that semantic IDs can improve the generalization of recommendation ranking compared with traditional item IDs. Different from existing works aiming to replace item IDs with semantic IDs, we further consider the complementary strengths of them.