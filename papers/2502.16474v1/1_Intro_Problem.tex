\section{Introduction}
\begin{figure}[!htb]
		\begin{tabular}{c}
		    	\includegraphics[width=.6\columnwidth]{beauty.png}
		\end{tabular}
	\caption{Visualization of ID tokens on Amazon Beauty dataset. Here some ID tokens with the same color share a close embedding space, which means they can be compressed and represented with shared semantic tokens.}	\label{fig:token_dist}
\end{figure} 

In large-scale online platforms such as YouTube~\citep{covington2016deep}, TikTok~\citep{dfar}, and Amazon~\citep{he2016ups}, effectively recommending items that align with usersâ€™ preferences while filtering out irrelevant content is crucial. Traditional recommendation systems predominantly rely on ID tokens, wherein each item is uniquely identified by a distinct token~\citep{fm, sasrec}. However, as the number of items expands, this approach becomes increasingly cumbersome due to the redundancy and sheer scale of the token space. 

To overcome these limitations, recent research~\citep{rajput2024recommender, singh2023better} has explored the use of semantic tokens as an alternative to ID tokens. Nevertheless, existing works face challenges, such as inconsistent performance improvements and issues with item duplication. For instance, TIGER~\citep{rajput2024recommender} introduces semantic tokens within a deeper and more complex model architecture, making it difficult to isolate the benefits of semantic tokens themselves. Consequently, the true advantages of semantic tokens over ID tokens remain underexplored. Another study~\citep{singh2023better} demonstrates that semantic tokens offer notable improvements primarily in cold-start scenarios, yet both studies report that semantic tokens can map multiple items to the same token, leading to duplication. These open questions invite further investigation into the comparative effectiveness of semantic and ID tokens: \textit{Are semantic tokens inherently superior to ID tokens in recommendation tasks?}

In reality, semantic tokens and ID tokens complement each other. ID tokens have two primary advantages: (1) they can uncover unique, implicit relationships between items, such as the well-known association between beer and diapers, and (2) they facilitate the distinction between different items. However, ID tokens struggle to capture shared attributes across similar items and often suffer from redundancy at scale. This is analogous to whole-word tokenization in Natural Language Processing (NLP)\citep{NIPS2000_728f206c, collobert2008unified, mikolov2013distributed}, which tends to fail with unknown or out-of-vocabulary words\citep{mielke2021between}, making ID tokens less effective in cold-start situations. On the other hand, semantic tokens resemble sub-word tokenization in NLP~\citep{mikolov2012subword, wang2020neural}, where combinations of existing semantic tokens can represent new or unknown items. However, the drawback of semantic tokens lies in their tendency to map multiple, similar items to identical representations, thus failing to distinguish between them~\citep{singh2023better}. In summary, while semantic tokens excel in generalizing to unknown items, they are less effective in memorizing unique ones, suggesting that neither approach is universally superior.

To harness the complementary strengths of both token types, we propose a hybrid framework that unifies ID and semantic tokens. As illustrated in Figure~\ref{fig:token_dist}, our approach begins by visualizing the distribution of ID tokens, revealing that certain items cluster closely together in the embedding space. From this, we hypothesize that only a few dimensions of the ID token space are needed to capture unique item characteristics, while the remaining dimensions can be replaced by semantic tokens to represent shared features. Based on this hypothesis, we introduce a Unified Semantic and ID Representation Learning framework, which incorporates two key components: \textit{unified ID and semantic tokenization} and \textit{unified cosine similarity and Euclidean distance}. First, in unified tokenization, we quantize item content embeddings into a semantic codebook to capture shared characteristics, while assigning each item a low-dimensional ID token to capture unique attributes. Second, in the unified similarity and distance metric, we observe that cosine similarity is effective at disentangling densely clustered embeddings, yet struggles with distinguishing unique items, while Euclidean distance excels at the latter. Consequently, we apply cosine similarity in the earlier layers to decouple dense embeddings and Euclidean distance in the final layer to distinguish unique items. Experimental results on three benchmark datasets demonstrate that our method outperforms existing baselines by 6\% to 17\%, while reducing token size by over 80\%. Ablation studies further validate our hypothesis, showing that many ID tokens are redundant and can be effectively replaced by semantic tokens to enhance generalization.

In summary, the key contributions of this work are as follows: \begin{itemize}[leftmargin=*] \item We present the first comprehensive investigation into the complementary relationship between semantic and ID tokens in recommendation systems. \item We propose a novel \textit{unified ID and semantic tokenization} framework that captures both unique and shared item characteristics, alongside a \textit{unified similarity and distance} approach that balances embedding decoupling and item distinction. \item Our method achieves significant performance improvements on three benchmark datasets, outperforming baselines by 6\% to 17\% while reducing token size by over 80\%, thereby enhancing the system's generalization capability. \end{itemize}



\section{Preliminary}
\paragraph{\textbf{Problem Definition}}
Suppose there are \( m \) items, and each item \( i \) is represented by an encoded sentence embedding \( \boldsymbol{x}_i \). Let \( i_{t} \) denote user \( u \)'s \( t \)-th interacted item. If user \( u \) has interacted with a sequence of items \( \mathcal{I}_{u} = (i_{1}, i_{2}, \ldots, i_{t}) \), with corresponding sentence embeddings \( \mathcal{X}_{u} = (\boldsymbol{x}_{i_{1}}, \boldsymbol{x}_{i_{2}}, \ldots, \boldsymbol{x}_{i_{t}}) \), the objective of sequential recommendation is to accurately predict the next item that user \( u \) will interact with, based on their previous interaction history. Formally, the problem can be defined as follows:

\noindent \textbf{Input}: A sequence of items \( \mathcal{I}_{u} = (i_{1}, i_{2}, \ldots, i_{t}) \) that user \( u \) has interacted with, along with their corresponding sentence embeddings \( \mathcal{X}_{u} = (\boldsymbol{x}_{i_{1}}, \boldsymbol{x}_{i_{2}}, \ldots, \boldsymbol{x}_{i_{t}}) \).

\noindent \textbf{Output}: The estimated probability \( \hat{y}_{u, t+1} \) of the next item that user \( u \) will interact with at time step \( t + 1 \).


\paragraph{\textbf{ID Tokenization}} 
Traditional recommender systems often rely on ID tokenization to capture the unique characteristics of each item. In this approach, an item embedding matrix 
$\left\{\boldsymbol{e}_{i}\right\}_{i=1}^m$ is constructed, where each item \( i \) is associated with an embedding vector 
$\boldsymbol{e}_i \in \mathbb{R}^{1 \times D}$, and \( D \) represents the ID embedding dimension. The total embedding size for ID tokenization is thus $m \times D$, where \( m \) is the number of items. For a user \( u \) with an interaction sequence of items $\mathcal{I}_{u} = (i_{1}, i_{2}, \ldots, i_{t})$, we can retrieve the corresponding ID embeddings $(\boldsymbol{e}_{i_{1}}, \boldsymbol{e}_{i_{2}}, \ldots, \boldsymbol{e}_{i_{t}})$ through simple lookup operations in the embedding matrix.
\paragraph{\textbf{Semantic Tokenization}} 
To capture the semantic information of items, recent works have leveraged techniques like RQ-VAE~\citep{rajput2024recommender} to quantize content embeddings. Specifically, semantic tokenization builds $L$ layers of codebook embeddings, where each layer contains a set of embedding vectors $\left\{\boldsymbol{e}^c_{k}\right\}_{k=1}^K$, with $\boldsymbol{e}^c_{k} \in \mathbb{R}^{1 \times D'}$. Here, $D'$ denotes the semantic embedding dimension, and the total embedding size for semantic tokenization is $L \times K \times D'$. Since $L \times K \ll m$, semantic tokenization can significantly reduce the embedding size by replacing ID-based embeddings with semantically informed ones. As detailed in Algorithm~\ref{alg:rq} of Appendix~\ref{sec:semantic_token}, the RQ-VAE model quantizes the input sentence embedding $\boldsymbol{x}_{i_{t}}$ and returns the corresponding semantic embedding $\boldsymbol{z}_{i_{t}}$ for each item in user \( u \)'s interaction history. It is important to note that the stop-gradient operation, denoted as $\operatorname{sg}$, is applied during the quantization process.



% \paragraph{\textbf{ID Tokenization}}
% Traditional recommender systems are often based on ID tokenization. Specifically, to learn the unique information of each item, Traditional recommender systems often build an 
% item embedding matrix $\left\{\boldsymbol{e}_{i}\right\}_{i=1}^m, 
% \boldsymbol{e}_i \in \mathbb{R}^{1 \times D}$. Here $D$ is the ID embedding dimension and the total ID embedding size here is $m \times D$.
% As the item number $m$ can be very large, we set the dimension $D$ relatively smaller than the dimension $D'$ of the traditional recommender systems which rely on ID tokenization. Given item sequence $\mathcal{I}_{u} = (i_{1}, i_{2}, \ldots, i_{t})$ for user $u$, we can lookup ID embeddings $(\boldsymbol{e}_{i_{1}}, \boldsymbol{e}_{i_{2}}, \ldots, \boldsymbol{e}_{i_{t}})$.




% \paragraph{\textbf{Semantic Tokenization}} Recently, to learn the semantic information, some works leverage RQ-VAE~\citep{rajput2024recommender} to quantize the content embedding. Specifically, such method builds $L$ layers of codebook embeddings, each layer with $\left\{\boldsymbol{e}^c_{k}\right\}_{k=1}^K, \boldsymbol{e}^c_{k} \in \mathbb{R}^{1 \times D'}$. Here $D'$ is the semantic embedding dimension, and semantic tokenization has $L \times K \times D'$ of total embedding size. As $L \times K \ll m$, semantic tokenization can reduce the embedding size by replacing ID embedding with semantic embedding. As shown in Algorithm~\ref{alg:rq}, RQ-VAE will quantize the input sentence embedding $\boldsymbol{x}_{i_{t}}$ and return the semantic embedding $\boldsymbol{z}_{i_{t}}$ of each item in a given user $u$'s historical sequence. Note that here $\operatorname{sg}$ is the stop gradient operation.


