@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}
@inproceedings{zhou2020s3,
  title={S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization},
  author={Zhou, Kun and Wang, Hui and Zhao, Wayne Xin and Zhu, Yutao and Wang, Sirui and Zhang, Fuzheng and Wang, Zhongyuan and Wen, Ji-Rong},
  booktitle={Proceedings of the 29th ACM international conference on information \& knowledge management},
  pages={1893--1902},
  year={2020}
}
@article{rajput2024recommender,
  title={Recommender systems with generative retrieval},
  author={Rajput, Shashank and Mehta, Nikhil and Singh, Anima and Hulikal Keshavan, Raghunandan and Vu, Trung and Heldt, Lukasz and Hong, Lichan and Tay, Yi and Tran, Vinh and Samost, Jonah and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@inproceedings{dfar,
author = {Lin, Guanyu and Gao, Chen and Zheng, Yu and Chang, Jianxin and Niu, Yanan and Song, Yang and Li, Zhiheng and Jin, Depeng and Li, Yong},
title = {Dual-interest Factorization-heads Attention for Sequential Recommendation},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583278},
doi = {10.1145/3543507.3583278},
abstract = {Accurate user interest modeling is vital for recommendation scenarios. One of the effective solutions is the sequential recommendation that relies on click behaviors, but this is not elegant in the video feed recommendation where users are passive in receiving the streaming contents and return skip or no-skip behaviors. Here skip and no-skip behaviors can be treated as negative and positive feedback, respectively. With the mixture of positive and negative feedback, it is challenging to capture the transition pattern of behavioral sequence. To do so, FeedRec has exploited a shared vanilla Transformer, which may be inelegant because head interaction of multi-heads attention does not consider different types of feedback. In this paper, we propose Dual-interest Factorization-heads Attention for Sequential Recommendation (short for DFAR) consisting of feedback-aware encoding layer, dual-interest disentangling layer and prediction layer. In the feedback-aware encoding layer, we first suppose each head of multi-heads attention can capture specific feedback relations. Then we further propose factorization-heads attention which can mask specific head interaction and inject feedback information so as to factorize the relation between different types of feedback. Additionally, we propose a dual-interest disentangling layer to decouple positive and negative interests before performing disentanglement on their representations. Finally, we evolve the positive and negative interests by corresponding towers whose outputs are contrastive by BPR loss. Experiments on two real-world datasets show the superiority of our proposed method against state-of-the-art baselines. Further ablation study and visualization also sustain its effectiveness. We release the source code here: https://github.com/tsinghua-fib-lab/WWW2023-DFAR.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {917â€“927},
numpages = {11},
keywords = {Contrastive Learning, Sequential recommendation, User feedback},
location = {Austin, TX, USA},
series = {WWW '23}
}
@inproceedings{covington2016deep,
  title={Deep neural networks for youtube recommendations},
  author={Covington, Paul and Adams, Jay and Sargin, Emre},
  booktitle={Proceedings of the 10th ACM conference on recommender systems},
  pages={191--198},
  year={2016}
}
@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}
@inproceedings{wang2020neural,
  title={Neural machine translation with byte-level subwords},
  author={Wang, Changhan and Cho, Kyunghyun and Gu, Jiatao},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={05},
  pages={9154--9160},
  year={2020}
}
@article{ni2021sentence,
  title={Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models},
  author={Ni, Jianmo and Abrego, Gustavo Hernandez and Constant, Noah and Ma, Ji and Hall, Keith B and Cer, Daniel and Yang, Yinfei},
  journal={arXiv preprint arXiv:2108.08877},
  year={2021}
}
@article{mikolov2012subword,
  title={Subword language modeling with neural networks, 2012},
  author={Mikolov, Tom{\'a}{\v{s}} and Sutskever, Ilya and Deoras, Anoop and Le, Hai-Son and Kombrink, Stefan and Cernocky, J},
  journal={Preprint (http://www. fit. vutbr. cz/imikolov/rnnlm/char. pdf)},
  year={2012}
}
@article{mikolov2013distributed,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}
@inproceedings{collobert2008unified,
  title={A unified architecture for natural language processing: Deep neural networks with multitask learning},
  author={Collobert, Ronan and Weston, Jason},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={160--167},
  year={2008}
}
@article{mielke2021between,
  title={Between words and characters: A brief history of open-vocabulary modeling and tokenization in NLP},
  author={Mielke, Sabrina J and Alyafeai, Zaid and Salesky, Elizabeth and Raffel, Colin and Dey, Manan and Gall{\'e}, Matthias and Raja, Arun and Si, Chenglei and Lee, Wilson Y and Sagot, Beno{\^\i}t and others},
  journal={arXiv preprint arXiv:2112.10508},
  year={2021}
}
@inproceedings{NIPS2000_728f206c,
 author = {Bengio, Yoshua and Ducharme, R\'{e}jean and Vincent, Pascal},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 pages = {},
 publisher = {MIT Press},
 title = {A Neural Probabilistic Language Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/file/728f206c2a01bf572b5940d7d9a8fa4c-Paper.pdf},
 volume = {13},
 year = {2000}
}

@article{jegou2010product,
  title={Product quantization for nearest neighbor search},
  author={Jegou, Herve and Douze, Matthijs and Schmid, Cordelia},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={33},
  number={1},
  pages={117--128},
  year={2010},
  publisher={IEEE}
}
@inproceedings{vqrec,
  title={Learning vector-quantized item representation for transferable sequential recommenders},
  author={Hou, Yupeng and He, Zhankui and McAuley, Julian and Zhao, Wayne Xin},
  booktitle={Proceedings of the ACM Web Conference 2023},
  pages={1162--1171},
  year={2023}
}
@article{CNN,
	title = {Imagenet classification with deep convolutional neural networks},
	volume = {25},
	pages = {1097--1105},
	journaltitle = {Advances in neural information processing systems},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	year = {2012},
}
@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle = {NeurIPS},
  pages={5998--6008},
  year={2017}
}
@article{singh2023better,
  title={Better Generalization with Semantic IDs: A Case Study in Ranking for Recommendations},
  author={Singh, Anima and Vu, Trung and Mehta, Nikhil and Keshavan, Raghunandan and Sathiamoorthy, Maheswaran and Zheng, Yilin and Hong, Lichan and Heldt, Lukasz and Wei, Li and Tandon, Devansh and others},
  journal={arXiv preprint arXiv:2306.08121},
  year={2023}
}
@inproceedings{he2016ups,
  title={Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering},
  author={He, Ruining and McAuley, Julian},
  booktitle={proceedings of the 25th international conference on world wide web},
  pages={507--517},
  year={2016}
}
@inproceedings{dcn,
author = {Lin, Guanyu and Gao, Chen and Li, Yinfeng and Zheng, Yu and Li, Zhiheng and Jin, Depeng and Li, Yong},
title = {Dual Contrastive Network for Sequential Recommendation},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531918},
doi = {10.1145/3477495.3531918},
abstract = {Widely applied in today's recommender systems, sequential recommendation predicts the next interacted item for a given user via his/her historical item sequence. However, sequential recommendation suffers data sparsity issue like most recommenders. To extract auxiliary signals from the data, some recent works exploit self-supervised learning to generate augmented data via dropout strategy, which, however, leads to sparser sequential data and obscure signals. In this paper, we propose D ual C ontrastive N etwork (DCN) to boost sequential recommendation, from a new perspective of integrating auxiliary user-sequence for items. Specifically, we propose two kinds of contrastive learning. The first one is the dual representation contrastive learning that minimizes the distances between embeddings and sequence-representations of users/items. The second one is the dual interest contrastive learning which aims to self-supervise the static interest with the dynamic interest of next item prediction via auxiliary training. We also incorporate the auxiliary task of predicting next user for a given item's historical user sequence, which can capture the trends of items preferred by certain types of users. Experiments on benchmark datasets verify the effectiveness of our proposed method. Further ablation study also illustrates the boosting effect of the proposed components upon different sequential models.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2686â€“2691},
numpages = {6},
keywords = {contrastive learning, self-supervised learning, sequential recommendation},
location = {Madrid, Spain},
series = {SIGIR '22}
}
@inproceedings{man,
author = {Lin, Guanyu and Gao, Chen and Zheng, Yu and Chang, Jianxin and Niu, Yanan and Song, Yang and Gai, Kun and Li, Zhiheng and Jin, Depeng and Li, Yong and Wang, Meng},
title = {Mixed Attention Network for Cross-domain Sequential Recommendation},
year = {2024},
isbn = {9798400703713},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3616855.3635801},
doi = {10.1145/3616855.3635801},
abstract = {In modern recommender systems, sequential recommendation leverages chronological user behaviors to make effective next-item suggestions, which suffers from data sparsity issues, especially for new users. One promising line of work is the cross-domain recommendation, which trains models with data across multiple domains to improve the performance in data-scarce domains. Recent proposed cross-domain sequential recommendation models such as PiNet and DASL have a common drawback relying heavily on overlapped users in different domains, which limits their usage in practical recommender systems. In this paper, we propose a M ixed A ttention N etwork (MAN) with local and global attention modules to extract the domain-specific and cross-domain information. Firstly, we propose a local/global encoding layer to capture the domain-specific/cross-domain sequential pattern. Then we propose a mixed attention layer with item similarity attention, sequence-fusion attention, and group-prototype attention to capture the local/global item similarity, fuse the local/global item sequence, and extract the user groups across different domains, respectively. Finally, we propose a local/global prediction layer to further evolve and combine the domain-specific and cross-domain interests. Experimental results on two real-world datasets (each with two domains) demonstrate the superiority of our proposed model. Further study also illustrates that our proposed method and components are model-agnostic and effective, respectively. The code and data are available at https://github.com/Guanyu-Lin/MAN.},
booktitle = {Proceedings of the 17th ACM International Conference on Web Search and Data Mining},
pages = {405â€“413},
numpages = {9},
keywords = {keywords{cross-domain sequential recommendation, mixed attention network, recommender systems}},
location = {Merida, Mexico},
series = {WSDM '24}
}
@INPROCEEDINGS{sasrec,
  author={Kang, Wang-Cheng and McAuley, Julian},
  booktitle={2018 IEEE International Conference on Data Mining (ICDM)}, 
  title={Self-Attentive Sequential Recommendation}, 
  year={2018},
  volume={},
  number={},
  pages={197-206},
  keywords={Adaptation models;Context modeling;Task analysis;Recommender systems;Markov processes;Recurrent neural networks;Predictive models;Sequential Recommendation;Collaborative Filtering},
  doi={10.1109/ICDM.2018.00035}}

@inproceedings{bert4rec,
  title={BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer},
  author={Sun, Fei and Liu, Jun and Wu, Jian and Pei, Changhua and Lin, Xiao and Ou, Wenwu and Jiang, Peng},
  booktitle={Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
  pages={1441--1450},
  year={2019}
}
@inproceedings{caser,
  title={Personalized top-n sequential recommendation via convolutional sequence embedding},
  author={Tang, Jiaxi and Wang, Ke},
	booktitle={WWW},
  pages={565--573},
  year={2018}
}
@inproceedings{gru4rec,
  title={Session-based recommendations with recurrent neural networks},
  author={Hidasi, Bal{\'a}zs and Karatzoglou, Alexandros and Baltrunas, Linas and Tikk, Domonkos},
	booktitle={ICLR},
  year={2016}
}
@INPROCEEDINGS{fm,
  author={Rendle, Steffen},
  booktitle={2010 IEEE International Conference on Data Mining}, 
  title={Factorization Machines}, 
  year={2010},
  volume={},
  number={},
  pages={995-1000},
  keywords={Mathematical model;Support vector machines;Frequency modulation;Predictive models;Data models;Equations;Computational modeling;factorization machine;sparse data;tensor factorization;support vector machine},
  doi={10.1109/ICDM.2010.127}}
@inproceedings{hgn,
  title={Hierarchical gating networks for sequential recommendation},
  author={Ma, Chen and Kang, Peng and Liu, Xue},
  booktitle={Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery \& data mining},
  pages={825--833},
  year={2019}
}
