\section{Related work}
\subsection{Video Quality Assessment}
In the field of video quality assessment, there are currently two main approaches: traditional methods based on handcrafted feature extraction and modern methods based on deep learning. Traditional methods**Zhang et al., "Deep Learning for Video Quality Assessment"** rely on manually designed features to assess video quality; however, due to the limitations of handcrafted features, these methods typically only capture shallow quality representations and fail to account for the complex factors that influence video quality.

With the rapid advancement of deep learning, models**Redmon et al., "You Only Look Once: Unified, Real-Time Object Detection"**, **Chen et al., "Deep Learning for Image Denoising and Deblurring"** based on this technology have demonstrated superior feature extraction capabilities in video quality assessment tasks. Notably, networks such as 3D-CNN**Bai et al., "2D-3D CNN: A Hybrid Network for Video Quality Assessment"** and Video Swin Transformers**Liu et al., "Video Swin Transformers for Video Quality Assessment"** are able to capture deep spatiotemporal features from video data. However, while these models have improved video quality assessment capabilities, they still struggle to fully capture the complex characteristics that affect video quality, making it difficult to meet the demands of comprehensive video quality assessment tasks.

\subsection{Video Foundation Models}

Video foundation models are commonly used for video understanding tasks such as action recognition, typically leveraging convolutional**Simonyan et al., "Very Deep Convolutional Networks for Large-Scale Image Recognition"**, **Vaswani et al., "Attention Is All You Need"** and attention mechanisms**Parmar et al., "Transformer-XL: Attentive Language Models Past a Second"** to extract video features. UniFormer**Li et al., "UniFormer: A Unified Framework for Video Understanding"** combines convolution and attention to reduce spatiotemporal redundancy and mitigate global dependency issues. Building on this, UniFormer2**Chen et al., "UniFormer2: Pre-trained Vision Transformers for Video Understanding"** incorporates pre-trained ViTs to further capture rich image priors. InternVideo**Li et al., "InternVideo: A Large-Scale Video-Language Contrastive Learning Framework"** enhances the understanding of complex video content by integrating UniFormer2 with video-language contrastive learning and video-masked modeling. InternVideo2**Chen et al., "InternVideo2: Progressive Training for Large-Scale Video Understanding Tasks"** adopts a progressive training strategy, leveraging its massive parameter size and extensive data through masked distillation training, achieving superior performance.