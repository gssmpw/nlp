% \newpage







% \begin{figure*}[htbp]
%     \centering

%     % Subfigure (b)
%     \begin{minipage}[b]{0.7\textwidth}
%         \centering
%     \begin{center}
%     % \begin{sc}
%     \resizebox{1.0\textwidth}{!}{
%     % \setlength{\tabcolsep}{0.8pt}
%     \renewcommand{\arraystretch}{1.3}
%     \rowcolors{2}{gray!20}{white}
%     \begin{tabular}{|c|c|c|ccc}
%     % \toprule
%     % \hline
%     \hline
    
%     \rowcolor[HTML]{C0C0C0}
%     Model & \textbf{Vanilla DL} & \textbf{Robust DL}\\\hline
%     Formula&$\min_\bz \|\bx - \cA^*(\bz)\|_2+\lambda\|\bz\|_1$&$\min_\bz \|\bx - \cA^*(\bz)\|_1+\lambda\|\bz\|_1$\\\hline
%     Assumption&Light-tailed&Heavy-tailed\\\hline
%     Algorithm &ISTA&RISTA\\\hline
%     Layer &\multicolumn{2}{c|}{ $ \bz_{t+1} = \mathcal{T}_{\lambda t} \left( \bz_t + t \cdot \cA \left(\br(\bz_t)\right)  \right)$}\\\hline
%     Residual $\br$ & $\br(\bz_t)=\bx-\cA^*(\bz_t)$& $\br(\bz_t)=\bw_t\odot\left(\bx-\cA^*(\bz_t)\right)$ \\\hline
%     \textbf{Elastic DL}&\multicolumn{2}{c|}{$\min_\bz \frac{\beta}{2}\|\bx - \cA^*(\bz)\|_2^2 + \frac{1-\beta}{2}\|\bx - \cA^*(\bz)\|_1 + \lambda\|\bz\|_1$} \\\hline
%     \end{tabular}
%     }
    
%     % \end{sc}
%     \end{center}
%     \caption{Vanilla DL vs. Robust DL. 
%     An input signal $\bx$ is assumed to be concisely encoded by a sparse code $\bz$ with 
%     % , which extracts a few elements from a 
%     dictionary $\vA$.
%     Vanilla DL and Robust DL involve different norms ($\|\cdot\|_2^2$ and $\|\cdot\|_1$) to penalize the residual $\bx - \cA^*(\bz)$, under the  heavy-tailed and light-tailed noise. }
%     \label{fig:dictionary_learning_table}
%     \end{minipage}
%     \hfill 
%     % Subfigure (a)
%     \begin{minipage}[b]{0.29\textwidth} % Set the width of the subfigure
%         \centering
%         \includegraphics[width=\textwidth]{figures/Heavy-tailed_Test.png} % Replace with your image path
%         \vspace{-2.0em}
%         \caption{Distribution of random noise. The distribution tail becomes heavier with higher noise levels.} % Subfigure caption
%         \label{fig:dist_impulse}
%         % \vspace{0.9em}
%     \end{minipage}

        

%     % \caption{aaa.} % Main figure caption
%     % \label{fig:diff_datasets}
% \end{figure*}








% \section{Revisiting Convolutional Dictionary Learning}
\section{Revisiting Convolutional Dictionary Learning in Deep Learning}

% \begin{figure*}[h!]
%     \centering
%     \includegraphics[width=0.8\linewidth]{figures/dictionary_learning.pdf}
%     \caption{
%       Vanilla DL vs. Robust DL. An input signal $\bx$ is assumed to be concisely encoded by a sparse code $\bz$, which extracts a few elements from a dictionary $\vA$. Vanilla DL and Robust DL involve different norms ($\|\cdot\|_2^2$ and $\|\cdot\|_1$) to penalize the reconstruction residual $\bx - \cA^*(\bz)$, under the  heavy-tailed and light-tailed noise assumptions.
%       }
%     \label{fig:dictionary_learning}
% \end{figure*}

\textbf{Notations.} 
Let the input signal be denoted as $\bxi \in \mathbb{R}^{H \times W}$ and the convolution kernel as $\balpha \in \mathbb{R}^{k \times k}$, where $k = 2k_0 + 1$. The \emph{convolution} of $\bxi$ and $\balpha$ is defined as:
\begin{equation} 
(\balpha \star \bxi)[i, j] = \sum_{p=-k_0}^{k_0} \sum_{q=-k_0}^{k_0} \bxi[i+p, j+q] \cdot \balpha[p, q], \label{eq:correlation} 
\end{equation}
while the \emph{transposed convolution} of $\bxi$ and $\balpha$ is defined as:
\begin{equation} 
(\balpha * \bxi)[i, j] = \sum_{p=-k_0}^{k_0} \sum_{q=-k_0}^{k_0} \bxi[i-p, j-q] \cdot \balpha[p, q]. \label{eq:convolution} 
\end{equation}

% Different from the conventions in signal processing, the convolution layers in deep learning models actually perform multi-channel \emph{correlation} operations that map the $C$-channel input signal to the $D$-channel output signal.


Let the $C$-channel input signal be denoted as $\bx=\left\{\bxi_1,...,\bxi_C\right\}\in\mathbb{R}^{H\times W\times C}$, and $D$-channel
the output signal as $\bz=\left\{\bet_1,...,\bet_D\right\}\in\mathbb{R}^{H\times W\times D}$. The
convolution operator $\cA(\cdot)$ is associated with kernel $\vA$ as:
\begin{equation}
\cA(\bx) = \sum_{c=1}^{C}\left(\balpha_{1c}\star\bxi_c,..., \balpha_{Dc}\star\bxi_c\right) \in \mathbb{R}^{H \times W \times D}.
\label{eq:convolution_layer}
\end{equation}
The adjoint transposed convolution operator of $\cA$, denoted as $\cA^*$, is defined as:
\begin{equation}
\cA^*(\bz) = \sum_{d=1}^{D}\left(\balpha_{d1}*\bet_d,..., \balpha_{dC}*\bet_d\right) \in \mathbb{R}^{H \times W \times C},
\label{eq:adjoint_convolution_layer}
\end{equation}
where the associated kernel $\vA$ is:
\begin{equation}
\mathbf{A} = 
\begin{pmatrix}
\balpha_{11} & \balpha_{12} & \balpha_{13} & \cdots & \balpha_{1C} \\
\balpha_{21} & \balpha_{22} & \balpha_{23} & \cdots & \balpha_{2C} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\balpha_{D1} & \balpha_{D2} & \balpha_{D3} & \cdots & \balpha_{DC}
\end{pmatrix}
\in \mathbb{R}^{D \times C \times k \times k}.
\label{eq:convolution_kernel}
\end{equation}

Here, $H$, $W$, $C$, $D$, and $k$ represent the height, width, input dimension, output dimension, and kernel size, respectively. 




\subsection{Vanilla Dictionary Learning}

% Fundamentally, DNNs inherently function as representation learning modules by transforming raw data into progressively more compact embeddings~\citep{lecun2015deep, goodfellow2016deep}. The basic transformations, including fully-connected linear layer, convolution, attention, graph convolution, are broadly built within various
% deep learning architectures to capture particular feature patterns of interest. 

% In fact, all of these basic transformations can be explicitly unified as the entanglement interaction of the filter (kernel) $\cA$  and input signal $\bx$,
% \begin{equation}
%     \bz=\cA(\bx) \in \mathbb{R}^{H \times W \times D}.
%     \label{eq:linear_mapping}
% \end{equation}
% Specifically,  when $k=1$, the convolution is reduced to fully-connected linear mapping.
%  Also, this unified view covers the linear operator over various kinds of data structures, including 2D grid data, sequence data ($W=1$), and a single vector ($H=W=1$). 

% Deep neural networks (DNNs) have been criticized as "black boxes" by explicitly formulating the  basic transformation as the entanglement interaction of the filter (kernel) $\cA$  and input signal $\bx$,
% \begin{equation}
%     \bz=\cA(\bx) \in \mathbb{R}^{H \times W \times D}.
%     \label{eq:linear_mapping}
% \end{equation}
% This formulation covers the fully-connected linear mapping ($k=1$) and various kinds of data structures, including 2D grid data, sequence data ($W=1$), and one single vector ($H=W=1$).

 % To enhance the interpretability, generalization and robustness of black-box deep neural networks (DNNs),  


 To enhance the interpretability of black-box deep neural networks (DNNs),   
\citet{papyan2017convolutional, cazenavette2021architectural, mahdizadehaghdam2019deep, li2022revisiting} introduce the structural prior of dictionary learning into the design of neural networks, assuming that the signal $\bx$ can be represented by a linear superposition of several atoms $\{\balpha_{dc}\}$ from a convolutional dictionary $\vA$:
\begin{equation}
   \bx = \cA^*(\bz) \in \mathbb{R}^{H \times W \times C}.
\end{equation} 
Then a sparse code $\bz$ is sought to extract few descriptors out of the collected dictionary for any given input $\bx$:
\begin{equation}
\min_\bz \|\bx - \cA^*(\bz)\|_2^2 + \lambda \|\bz\|_1,
\label{eq:sparse_coding}
\end{equation}
% or equivalently,
% \[
% \min_\bz \|\bz\|_1 + \|\epsilon\|_2^2 \quad \text{s.t. } \bx = \cA^*(\bz) + \epsilon.
% \]
where $\lambda$ is the hyperparameter to balance the fidelity and sparsity terms.
Although several works~\cite{cazenavette2021architectural, mahdizadehaghdam2019deep, li2022revisiting} demonstrated promising robustness of this vanilla dictionary learning (Vanilla DL) defined in Eq.~\eqref{eq:sparse_coding} against random corruptions and universal adversarial attacks, it remains unclear whether Vanilla DL can withstand stronger adaptive attacks.








% \subsection{Our Study: Is Vanilla DL Truly Robust?} 
% \subsection{Our Study: VanillaDL-based ResNets Suffer From False Security} 
\subsection{Preliminary Study: Vanilla DL-based SDNets Is Not Truly Robust} 

To validate the robustness of Vanilla DL, 
% in ~\citet{cazenavette2021architectural, mahdizadehaghdam2019deep, li2022revisiting}
we conduct a preliminary experiment on SDNet18~\cite{li2022revisiting}, a variant of ResNet18 in which all convolutional layers are replaced with convolutional sparse coding (CSC) layers based on Vanilla DL in Eq.~\eqref{eq:sparse_coding}.
% which employs the Vanilla DL framework as in Eq.~\eqref{eq:sparse_coding}
% As claimed by~\citet{li2022revisiting}, SDNets 
% which claimed to deliver excellent robustness after tuning the sparsity weight $\lambda$. 
We evaluate the SDNet18 (with fixed $\lambda$ and tuned $\lambda$) under both random impulse noise
% (5 noise levels as in Figure~\ref{fig:dist_impulse}) 
and adaptive PGD adversarial attack~\cite{madry2017towards} with budget $\frac{8}{255}$. 
As shown in Table~\ref{tab:pre_sdnet18}, 
SDNet18 improves upon ResNet18 in terms of robustness against random noise, with more significant improvement achieved by tuning the sparsity weight $\lambda$.
However, SDNet18 still experiences a sharp drop in performance under adaptive PGD attack, with accuracy approaching zero. The detailed results of the performance under various noise levels and $\lambda$ values are presented in Figure~\ref{fig:pre_sdnet18_various_lambda} in Appendix~\ref{sec:pre_study}.



% increasing the noise level and making the distribution tail heavier leads to accuracy degradation in Vanilla DL. The resilience to random noise can be improved by tuning the sparsity weight $\lambda$; however, the model with any $\lambda$ experiences a sharp drop in performance under adaptive PGD attacks, with accuracy approaching zero. The detailed results of the performance under various noise levels and $\lambda$ values are presented in Figure~\ref{fig:pre_sdnet18_various_lambda} in Appendix~\ref{sec:pre_study}.



% \xr{the description in this section may lead reviewers to think it is the study on dictionary learning instead of deep learning. You may emphasize we study the variants of ResNet driven by dictionary learning}


% \xr{I personally feel the discussion of light-tail and heavy-tail assumptions are very strong, specific, and unclear. It may be better to use a different motivation as in Dense Error Correction and AirGNN}



\begin{table}[h!]
\centering
\caption{ Preliminary study on SDNet18~\cite{li2022revisiting} under varying levels of random noise and PGD attack ($\epsilon=\frac{8}{255}$).
% While Vanilla DL performs well under light-tailed noise (lower noise levels), its performance degrades significantly under heavy-tailed noise (higher noise levels). Although adjusting $\lambda$ can mitigate the impact of heavy-tailed noise, as claimed in ~\cite{li2022revisiting}, the model 
% Vanilla DL-based SDNet18 is highly vulnerable to the adaptive PGD attack with $\epsilon=8/255$.
}

% \vspace{0.1in}
\begin{center}
\begin{sc}
\resizebox{0.48\textwidth}{!}{
\setlength{\tabcolsep}{1.8pt}
\begin{tabular}{c|ccccc|cccc}
% \toprule
\rowcolor[HTML]{C0C0C0}
\hline
\textbf{Model $\backslash$ Noise Level }  & \textbf{L-1} &\textbf{L-2}&\textbf{L-3}&\textbf{L-4}&\textbf{L-5}&\textbf{PGD} \\ \hline
ResNet18&81.44 & 57.23 & 48.32 & 32.49 & 16.98  &\red{\textbf{0.00}}\\
SDNet18 ($\lambda=0.1$)&82.39 &68.90 &59.28 &40.8 &23.83 &\red{\textbf{0.01}}\\
SDNet18 (Tune $\lambda$)&82.39 &68.90 & 59.28 &43.71 & 33.43 &\red{\textbf{0.13}}\\
 

\bottomrule

\end{tabular}

}

\end{sc}
\end{center}
\label{tab:pre_sdnet18}
\end{table}


% \begin{figure}[h!]
%     \centering
% \includegraphics[width=0.4\textwidth]{figures/Heavy-tailed_Test.png} 
%     % \centering
% % \includegraphics[width=0.4\textwidth]{figures/impulse_lambda.png}
% \caption{Distribution of Impulse noise. The distribution tail becomes heavier with high noise levels.}
%     \label{fig:dist_impulse}
% \end{figure}



 % In fact, $\ell_2$-reconstruction term in  the Eq.~\eqref{eq:sparse_coding} 
 
 
 
 % inherently makes a \emph{light-tailed} noise assumption, which does not hold in case of \emph{heavy-tailed} noise involving large corruptions and outliers.  

 In fact, the $\ell_2$-reconstruction term of Vanilla DL in Eq.~\eqref{eq:sparse_coding} imposes a quadratic penalty $\|\cdot\|_2^2$ on the residual $\bx-\cA^*(\bz)$, making it highly sensitive to outliers introduced by high-level noise and adaptive attacks.
 The experimental results reveal that existing Vanilla DL gives a \textit{false sense of security} under random noise and can easily compromised by adaptive attack. Thus, there still remains a huge gap to achieve truly robust dictionary learning in deep learning.

% \xr{you may need to explain what long-tail and heavy-tail mean since some reviewers may not be familiar with these concepts}



\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/edlnets.pdf} 
    \caption{Overview of Elastic DL Networks (EDLNets). EDLNets are constructed by replacing the convolutional layers in conventional backbones 
    % (e.g., ResNets~\cite{he2016deep}) 
    (e.g., ResNets)
    with EDL layers that are unrolled with the proposed efficient RISTA algorithm. Each EDL layer introduces a 
    % dictionary learning structural prior, 
    dictionary structural prior, 
    % which 
    assuming the input signal $\bz^{(l)}$ is encoded as a sparse code $\bz^{(l+1)}$ using a few atoms from diction $\mathbf{A}^{(l)}$.
  }
    \label{fig:overview_edlnets}
    \vspace{-0.1in}
\end{figure*}



% \newpage
\section{Elastic Dictionary Learning}
\label{sec:method}

To overcome the aforementioned limitation brought by the Vanilla DL models, we first propose a robust dictionary learning (Robust DL) via $\ell_1$-reconstruction to mitigate the impact of outlying values in Section~\ref{sec:rdl}. Moreover, we conduct a comprehensive experiment to demonstrate the advantages of Robust DL and highlight its pitfalls in Section~\ref{sec:pitfall}. Furthermore, to achieve a better inherent trade-off between natural and robust performance, we propose a novel elastic dictionary learning (Elastic DL) approach that enhances both  natural performance and robustness in Section~\ref{sec:edl}, followed by insightful theoretical robustness analyses in Section~\ref{sec:analysis}. The overview of our Elastic DL networks (EDLNets) can be found in Figure~\ref{fig:overview_edlnets}.




% \red{use dense error correction to motivate}
% adaptive residual why need this to denoise

\subsection{Robust Dictionary Learning via $\ell_1$-Reconstruction}
\label{sec:rdl}



As observed in the previous section, $\ell_2$-fidelity assumes light-tailed noise and performs poorly as the noise becomes increasingly heavy-tailed. 
To address the sensitivity of $\ell_2$-fidelity in Vanilla DL, we first propose a robust dictionary learning approach (Robust DL) with $\ell_1$-reconstruction to effectively mitigate the impact of outliers:
\begin{equation}
\min_\bz \|\bx - \cA^*(\bz)\|_1+\lambda\|\bz\|_1.
\label{eq:sparse_coding_l1}
\end{equation} 


Despite the sophisticated design of the model architecture, the $\ell_1$-norm terms in Eq.(\ref{eq:sparse_coding_l1}) introduce non-smoothness to the objective function, making it challenging to design an effective and efficient algorithm for approximating the solution. To address this, we first propose a \emph{localized upper bound} as an alternative objective for the $\ell_1$-fidelity term $\|\bx - \cA^*(\bz)\|_1$. Subsequently, we employ the iterative shrinkage-thresholding algorithm (ISTA) to solve the $\ell_1$-sparsity.

\textbf{Localized upper bound.}
To address $\|\bx - \cA^*(\bz)\|_1$ term, we first propose a convex upper bound $\cU(\bz,\bz_*)$  as an alternative in the following Lemma~\ref{lemma:local_upper_bound}.
\begin{lemma}
\label{lemma:local_upper_bound}
Let $\cR(\bz):=\|\bx-\cA^*(\bz)\|_1$, and for any fixed point $\bz_*$,  $\cU(\bz,\bz_*)$ is defined as 
\begin{equation}
    \label{eq:upper_bound}
    \cU(\bz,\bz_*)=\|\bw^{1/2}\odot (\bx-\cA^*(\bz))\|_2^2+\cR(\bz_*),
\end{equation}
where $\bw= \frac{1}{2|\bx-\cA^*(\bz_*)|}.$
Then, for any $\bz$, the following holds:
$$(1)\; \cU(\bz,\bz_*) \geq \cR(\bz), \quad (2)\; \cU(\bz_*,\bz_*) = \cR(\bz_*) .$$
\end{lemma}
\begin{proof}
    Please refer to Appendix~\ref{sec:proof_local_upper_bound}.
\end{proof}

The statement (1) indicates that $\cU(\bz,\bz_*)$ serves as an upper bound for $\cR(\bz)$, while statement (2) demonstrates that  $\cU(\bz,\bz_*)$ equals $\cR(\bz)$ at point $\bz_*$.
With fixed $\bz_*$, the alternative objective $\cU(\bz,\bz_*)$ in Eq.~\eqref{eq:upper_bound} is quadratic and can be efficiently optimized.
Therefore, instead of minimizing the non-smooth $\cR(z)$ directly, we can alternatively optimize the quadratic upper bound $\cU(\bz,\bz_t)$ with gradient descent algorithm at iteration $t$.


\textbf{RISTA algorithm.}
According to Lemma~\ref{lemma:local_upper_bound}, we can find an alternative objective for Eq.~\eqref{eq:sparse_coding_l1} at each step $t$:
\begin{equation}
\begin{aligned}
\bz_{t+1}&=\argmin_\bz  \|\bw_t^{1/2} \odot \left(\bx - \cA^*(\bz)\right)\|_2^2 + \lambda\|\bz\|_1 ,
\label{eq:alternative_objective_l1}
\end{aligned}
\end{equation}
where 
% $
% \bw_t_i = \frac{\tilde{\bw}_t_i}{\frac{1}{N}\sum_{i=1}^N \tilde{\bw}_t_i}, \quad \tilde{\bw}_t = \frac{1}{|\bx - \cA^*(\bz_t)|} \in \mathbb{R}^N, \quad N = H \times W \times C.
% $
$
\bw_t =  \frac{1}{2|\bx - \cA^*(\bz_t)|} \in \mathbb{R}^{H \times W \times C}.
$
Specifically, when $\bw_t = \mathbf{1}$, the problem reduces to the formulation in Eq.~\eqref{eq:sparse_coding}.



% \subsection{Algorithm Development }


Then, we can 
optimize the $\ell_1$-regularized problem in Eq.~\eqref{eq:alternative_objective_l1} instead of original Eq.~\eqref{eq:sparse_coding_l1} by our reweighted iterative  shrinkage thresholding algorithm (RISTA):
\begin{equation}
    \begin{aligned}
        \bz_{t+1} = \mathcal{T}_{\lambda t} \left( \bz_t + t \cdot \cA \left(
        \bw_t\odot\left(\bx - \cA^*(\bz_t)\right)
        \right)  \right),
        \label{eq:algo_iteration_l1}
    \end{aligned}
\end{equation}
where $\mathcal{T}_{\lambda t}(\bz)=\text{sign}(\bz) \left(|\bz-\lambda t|\right)_+$ represents the soft thresholding operator. The detailed derivation of Eq.~\eqref{eq:algo_iteration_l1} is provided in Appendix~\ref{sec:proof_algo_iteration}.
%%%
As a consequence of Lemma~\ref{lemma:local_upper_bound}, we can conclude the iteration $\{\bz_t\}_{t=0}^{T}$ obtained by Eq.~\eqref{eq:algo_iteration_l1} fulfill the loss descent of $\cR(\bz)$:
$$\cR(\bz_{t+1})\leq\cU(\bz_{t+1},\bz_t)\leq\cU(\bz_t, \bz_t)=\cR(\bz_t).$$
This implies 
convergence of  Eq.~\eqref{eq:sparse_coding_l1}   can be achieved by optimizing the localized upper bound Eq.~\eqref{eq:alternative_objective_l1}. 
% The comprehensive comparison of Vanilla DL and Robust DL can be found in Figure~\ref{fig:dictionary_learning_table}.



 

% \begin{figure}[h!]
% \centering


% \vspace{0.1in}
% \begin{center}
% % \begin{sc}
% \resizebox{0.48\textwidth}{!}{
% % \setlength{\tabcolsep}{0.8pt}
% \renewcommand{\arraystretch}{1.3}
% \rowcolors{2}{gray!20}{white}
% \begin{tabular}{|c|c|c|ccc}
% % \toprule
% % \hline
% \hline

% \rowcolor[HTML]{C0C0C0}
% Model & \textbf{Vanilla DL} & \textbf{Robust DL}\\\hline
% Formula&$\min_\bz \|\bx - \cA^*(\bz)\|_2+\lambda\|\bz\|_1$&$\min_\bz \|\bx - \cA^*(\bz)\|_1+\lambda\|\bz\|_1$\\\hline
% Assumption&Light-tailed&Heavy-tailed-tailed\\\hline
% Algorithm &ISTA&RISTA\\\hline
% Layer &\multicolumn{2}{c|}{ $ \bz_{t+1} = \mathcal{T}_{\lambda t} \left( \bz_t + t \cdot \cA \left(\br(\bz_t)\right)  \right)$}\\\hline
% Residual $\br$ & $\br(\bz_t)=\bx-\cA^*(\bz_t)$& $\br(\bz_t)=\bw_t\odot\left(\bx-\cA^*(\bz_t)\right)$ \\\hline
% \textbf{Elastic DL}&\multicolumn{2}{c|}{$\min_\bz \frac{\beta}{2}\|\bx - \cA^*(\bz)\|_2^2 + \frac{1-\beta}{2}\|\bx - \cA^*(\bz)\|_1 + \lambda\|\bz\|_1$} \\\hline
% \end{tabular}
% }

% % \end{sc}
% \end{center}

% \caption{ Vanilla DL vs. Robust DL. An input signal $\bx$ is assumed to be concisely encoded by a sparse code $\bz$, which extracts a few elements from a dictionary $\vA$. Vanilla DL and Robust DL involve different norms ($\|\cdot\|_2^2$ and $\|\cdot\|_1$) to penalize the reconstruction residual $\bx - \cA^*(\bz)$, under the  heavy-tailed and light-tailed noise assumptions. 
% }

% \label{tab:dictionary_learning_table}
% \end{figure}







\subsection{Pitfalls in $\ell_1$-based Robust DL}
\label{sec:pitfall}


\begin{table}[h!]
\vspace{-0.1in}
\centering
\caption{  Vanilla DL vs. Robust DL under random corruption (Impulse noise), PGD-$\ell_\infty$  and  PGD-$\ell_2$ with various noise levels. Robust DL demonstrates significant improvement over Vanilla DL in robustness but sacrifices natural performance as a trade-off.
}

% \vspace{0.1in}
\begin{center}
\begin{sc}
\resizebox{0.47\textwidth}{!}{
% \setlength{\tabcolsep}{0.8pt}
\begin{tabular}{c|c|ccccc}
% \toprule
% \hline
\hline
\rowcolor{gray!20}
\textbf{Random} & \textbf{Natural} & \textbf{L-1}&\textbf{ L-2}&\textbf{L-3}&\textbf{L-4}&\textbf{L-5} \\ 
\hline
%  \blue{Vanilla DL} &          82.39 & 68.90 & 59.28 & 40.80  & 23.83&\textbf{0.11} \\ 
%  % \hline
% \red{Robust DL}   &  81.46 & 70.54 & 62.04 & 46.40  & 31.84&\textbf{0.53} \\ 
% \hline
Vanilla DL &93.38 &84.95 & 75.83 & 67.22 & 44.01 & 24.91\\
% Robust DL&85.37 & 79.25 & 72.51 & 55.84 & 36.71\\
Robust DL&83.25 &77.71 & 71.69 & 64.9 & 51.02 & 37.78\\
\hline
% PGD&0&1/255 & 2/255 & 3/255 & 4/255 & 5/255 & 6/255 & 7/255 & 8/255\\
% Vanilla DL&93.38&59.33 & 12.64 & 1.65 & 0.33 & 0.06 & 0.02 & 0.02 & 0.01\\
% Robust DL&83.25&64.16 & 37.76 & 18.64 & 8.0 & 3.12 & 1.37 & 0.57 & 0.2\\
\rowcolor{gray!20}
\textbf{PGD-$\ell_\infty$ } & \textbf{Natural} &\textbf{1/255} & \textbf{2/255} & \textbf{3/255} & \textbf{4/255} &  \textbf{8/255}\\\hline
Vanilla DL&93.38 &59.33 & 12.64 & 1.65 & 0.33 &  0.01\\
Robust DL&83.25 &64.16 & 37.76 & 18.64 & 8.10 &  0.20\\
\hline
\rowcolor{gray!20}
\textbf{PGD-$\ell_2$}&\textbf{Natural} &\textbf{0.1}&\textbf{0.2}&\textbf{0.3}&\textbf{0.4}&\textbf{0.6}\\\hline
Vanilla DL&93.38 &63.61 & 27.86 & 9.78 & 3.31 & 0.10 \\
Robust DL&83.25 &69.56 & 50.17 & 32.58 & 20.25 & 2.79 \\
% \bottomrule
\hline

\end{tabular}

}

\end{sc}
\end{center}

\label{tab:l1_vs_l2_pre}
\vspace{-0.1in}
\end{table}


To demonstrate the advantages of Robust DL over Vanilla DL, we evaluate the models under random noise and adaptive PGD attacks with attack budgets measured in $\ell_\infty$ and $\ell_2$ norms.
From Table~\ref{tab:l1_vs_l2_pre}, we observe that $\ell_1$-based Robust DL has the following pitfalls:
\begin{itemize}[left=0.0em]
\vspace{-0.1in}
\item \textbf{Pitfall 1: Limited robustness.} In terms of robustness, Robust DL demonstrates a significant advantage over Vanilla DL under high-level random noise and adaptive adversarial attacks (PGD-$\ell_\infty$ and PGD-$\ell_2$) across various budget levels. However, both methods remain vulnerable to adversarially crafted perturbations, achieving nearly zero accuracy under adaptive attacks with imperceptible budgets ($8/255$ for PGD-$\ell_\infty$ and $0.6$ for PGD-$\ell_2$).
\item \textbf{Pitfall 2: Natural performance sacrifice.} Despite of certain improvement in robustness, Robust DL sacrifices natural performance by $10.13\%$. We conjecture that although $\ell_1$-based Robust DL effectively mitigates the impact of outlying values, it also misses important information due to the tradeoff between accuracy and robustness.
% Similar to the behavior of the median ($\ell_1$-estimator) and the mean ($\ell_2$-estimator), although $\ell_1$-based Robust DL effectively mitigates the impact of outlying values, it also misses important information by selecting the median instead of averaging over all features.


% \item Vanilla DL demonstrates superior performance compared to Robust DL under light-tailed random noise. In contrast, Robust DL shows significantly greater robustness in the presence of heavy-tailed random noise. This validates the assumptions about the underlying distributions inherent to the Vanilla DL and Robust DL models.
% \item
% In addition to heavy-tailed random noise, Robust DL outperforms  Vanilla DL under adaptive adversarial attacks (PGD-$\ell_\infty$ and PGD-$\ell_2$) across various budget levels. Nonetheless, both methods remain vulnerable to adaptive attacks with imperceptible budgets ($8/255$ for PGD-$\ell_\infty$ and 0.6 for PGD-$\ell_2$), highlighting that both approaches are limited by their reliance on light-tailed or heavy-tailed assumptions, which are easily compromised by adversarially crafted perturbations.
\end{itemize}


% From the results above, we can see that it is not trivial to achieve a well-balanced trade-off between natural and robust performance  with  either $\ell_2$ or $\ell_1$ reconstruction alone.
% Moreover, our  Robust DL shows only marginal improvement over Vanilla DL under standard training, which achieves nearly zero accuracy under common evaluation with PGD-$\ell_\infty$ ($\epsilon=8/255$).