\section{Related Works}








% Numerous efforts have been made to enhance the robustness through various strategies: detection techniques~\cite{metzen2017detecting,feinman2017detecting,grosse2017statistical, sehwag2021robust,rade2022reducing,addepalli2022scaling}, purification-based approaches~\citep{ho2022disco,nie2022diffusion,shi2021online,yoon2021adversarial}, robust training methods~\citep{madry2017towards,zhang2019theoretically,gowal2021improving,li2023wat} and regularization-based approaches~\citep{cisse2017parseval,zheng2016improving}. Among them, adversarial training methods~\cite{sehwag2021robust,rade2022reducing,addepalli2022scaling} have achieved remarkable effectiveness  under adaptive adversarial attacks, and almost  dominated the robustness leaderboard (RobustBench)~\cite{croce2020robustbench}. However, most of these methods highly rely on extensive synthetic training data crafted by generative models, larger model architectures, and empirical training strategy, posing critical challenges
% to breaking through the plateau in adversarial robustness.  The contribution of our elastic dictionary in this work is fully orthogonal to existing efforts by introcuding structural prior into model designs, which can be integrated for further enhancement.


% Numerous efforts have been made to enhance the robustness of deep learning models, which can broadly be categorized into empirical defenses and certifiable defenses. Empirical defenses focus on increasing robustness through various strategies: robust training methods~\citep{madry2017towards,zhang2019theoretically,gowal2021improving,li2023wat} introduce adversarial perturbations into the training data, while regularization-based approaches~\citep{cisse2017parseval,zheng2016improving} stabilize models by constraining the Lipschitz constant or spectral norm of the weight matrix. Additionally, detection techniques~\cite{metzen2017detecting,feinman2017detecting,grosse2017statistical} aim to defend against attacks by identifying adversarial inputs.
% Purification-based approaches seek to eliminate the adversarial signals before performing downstream tasks~\citep{ho2022disco,nie2022diffusion,shi2021online,yoon2021adversarial}. 
% Recently, some novel approaches have emerged by improving robustness from the perspectives of ordinary differential equations~\citep{kang2021stable,li2022defending,yan2019robustness} and generative models~\citep{wang2023better,nie2022diffusion,rebuffi2021fixing}. 
% Beyond empirical defenses, certifiable defenses~\citep{cohen2019certified, gowal2018effectiveness, fazlyab2019efficient} offer theoretical guarantees of robustness within specific regions against any attack. 

% However, many of these methods suffer from significant overfitting issues or depend on domain-specific heuristics, which limit their effectiveness and adaptability in achieving satisfying robustness. Additionally, techniques like robust training often entail high computational and training costs, especially when dealing with diverse noisy environments, thereby limiting their scalability and flexibility for broader applications. 
% The contribution of robustness reprogramming in this work is fully orthogonal to existing efforts, and they can be integrated for further enhancement.

% \red{include the overfitting baseline}



\textbf{Robust overfitting. } Overfitting in adversarially trained deep networks has been shown to significantly harm test robustness~\cite{rice2020overfitting}. To address the issue of severe robust overfitting, several efforts have been made from various perspectives. For instance, Dropout~\cite{srivastava2014dropout} is a widely used regularization method that randomly disables units and connections during training to mitigate overfitting. 
Regularization techniques~\cite{andriushchenko2020understanding,qin2019adversarial,sriramanan2020guided} have also proven effective in preventing overfitting by penalizing the complexity of model parameters.
Data augmentation is another common approach for reducing overfitting in deep network training~\cite{schmidt2018adversarially}, with methods including Cutout~\cite{devries2017improved}, Mixup~\cite{zhang2017mixup}, semi-supervised learning techniques~\cite{carmon2019unlabeled, zhai2019adversarially}, and generative modeling~\cite{wang2023better,gowal2021improving} being particularly notable. Additionally, early stopping~\cite{rice2020overfitting} has demonstrated great effectiveness in achieving optimal robust performance during adversarial training. However, existing methods have yet to fully realize the potential of structural priors for improving adversarial robustness and generalization.

\textbf{Dictionary learning prior in deep learning.} Dictionary learning has been well-studied and widely applied in signal and image processing~\cite{olshausen1996emergence, wright2008robust, wright2020dense, zhao2011background, yang2011robust, lu2013online, chen2013robust, jiang2015robust, yang2011robust}, based on the assumption that an input signal can be represented by a few atoms from a dictionary. Building on this foundation, \citet{papyan2017convolutional, cazenavette2021architectural, mahdizadehaghdam2019deep, li2022revisiting} successfully incorporated dictionary learning into deep learning to interpret or replace the "black-box" nature of neural networks. 
While these methods have demonstrated promising generalization and robustness against random noise and universal attacks~\cite{li2022revisiting, mahdizadehaghdam2019deep}, their practical benefits for improving robustness under adaptive attacks are yet to be thoroughly investigated.

We leave the related works about general adversarial attacks and defenses in the Appendix~\ref{sec:related_works_app} due to the space limit.

% However, despite these advances, current attempts to design robust deep learning models remain ineffective against adaptive attacks. This limitation arises from their optimization objectives, which make light-tailed noise assumption that fails under heavy-tailed noise or adversarially crafted perturbations. Although some researchers have proposed sophisticated designs~\cite{jiang2015robust, yang2011robust, chen2013robust} in conventional signal processing, no one have  achieved success in deep learning due to the challenges of efficient algorithm development and architecture design.


% deep learning, Convolutional Sparse Coding layers~


% \textbf{Conventional dictionary learning in signal processing.}
% \red{don't talk this too much; a lot of in signal processing, few in deep learning}

% L2: ~\cite{olshausen1996emergence, sun2016complete, sulam2020adversarial, cazenavette2021architectural,papyan2017convolutional, mahdizadehaghdam2019deep, tang2019analysis}


% L1:  ~\cite{wright2008robust,wright2020dense, lu2013online, zhao2011background}



% Futhermore: capped $\ell_1$-norm~\cite{jiang2015robust} and a
% sophisticated penalties~\cite{yang2011robust} or error source decomposition~\cite{chen2013robust}

% \textbf{Dictionary learning in deep learning.}

% ~\cite{li2022revisiting}.... need more survey


% \begin{itemize}
%     \item NeurIPS2022 Revisiting Sparse Convolutional Model for Visual Recognition

%     L2 reconstruction; Conv DL
%     \item NeurIPS2020 \textbf{Deep Isometric Learning for Visual Recognition}
%     \item TPAMI2008 Robust face recognition via sparse representation
%     \item TIT2010 Dense Error Correction Via L1-Minimization
%     \item NeurIPS-2020 Adversarial Robustness of Supervised Sparse Coding
%     \item CVPR11 Robust Sparse Coding for Face Recognition
%     \item 2205.08955 Structural Extensions of Basis Pursuit- Guarantees on Adversarial Robustness 
%     \item * 2023 On the Relationship Between Universal Adversarial Attacks and Sparse Representations
%     \item ICCV13 Robust Dictionary Learning by Error Source Decomposition
%     \item 1705.09954 L1-norm Error Function Robustness and Outlier Regularization
%     \item CVPR2013 Online Robust Dictionary Learning
%     \item 2011 Background Subtraction via Robust Dictionary Learning
%     \item IJCAI2015 Robust dictionary learning with capped l1-norm
%     \item 2011.14427 Architectural Adversarial Robustness- The Case for Deep Pursuit
%     \item NeurIPS2020 Denoised Smoothing:
% A Provable Defense for Pretrained Classifiers
%     \item * Papyan, Vardan, Yaniv Romano, and Michael Elad. "Convolutional neural networks analyzed via convolutional sparse coding." The Journal of Machine Learning Research 18.1 (2017): 2887-2938.
%     \item * Romano, Yaniv, et al. "Adversarial noise attacks of deep learning architectures: Stability analysis via sparse-modeled signals." Journal of Mathematical Imaging and Vision 62 (2020): 313-327.
% \end{itemize}

% Some relevant papers by Dr. Hamid Krim:

% \begin{itemize}
%     \item TIP19 Deep Dictionary Learning- A Parametric Network Approach
%     \item TIP19 Analysis Dictionary Learning Based Classification- Structure for Robustness
%     \item 1903.03058 Analysis dictionary learning- an efficient and discriminative solution
%     \item SP18 Bi-sparsity pursuit- A paradigm for robust subspace recovery
%     % \item 
% \end{itemize}


% \red{focus on the challenge in DL, efficient and simple structure}
