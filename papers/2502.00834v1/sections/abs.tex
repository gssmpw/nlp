\begin{abstract}

Numerous efforts have been made to advance adversarial robustness, with adversarial training dominating the robustness leaderboard. However, its success heavily depends on extensive training data, substantial computational resources, and larger model backbones, leading to a plateau in adversarial robustness. To break this stagnation, we advocate an orthogonal and promising avenue by introducing structural priors into neural network design. Specifically, we revisit dictionary learning in deep learning and identify its limitations under light-tailed noise and adaptive attacks. As a countermeasure, we present a novel elastic dictionary learning framework and solve it using an efficient RISTA algorithm, which can be readily integrated into model layer.  Our structural prior can effectively mitigate the robust overfitting problem in adversarial training, achieving remarkable robustness and improved generalization. Furthermore, our method
is complementary to existing adversarial training approaches  and can be integrated with them to achieve state-of-the-art robustness, significantly surpassing top baselines in RobustBench in terms of both natural and robust performance.


% \xr{need to highlight how good our model is in the abstract}

\end{abstract}