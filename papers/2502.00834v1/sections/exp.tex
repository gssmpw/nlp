




\section{Experiment}

In this section, we comprehensively evaluate the effectiveness of our proposed 
% elastic dictionary training 
EDLNets under various experimental settings. Additionally, we provide several ablation studies to demonstrate the working mechanism of our approach. 





\subsection{Experimental Setting}

\textbf{Datasets.} We conduct the experiments on several datasets including CIFAR10~\citep{krizhevsky2009learning}, CIFAR100~\citep{krizhevsky2009learning} and Tiny-ImageNet~\citep{le2015tiny}.




\textbf{Backbone architectures.} We select ResNets as the backbones, including ResNet10, ResNet18, ResNet34, and ResNet50~\citep{he2016deep}. Each of the convolutional layers in ResNets are replaced with our Elastic DL layer, resulting in the corresponding EDLNets.
% Elastic DL neural networks
We use ResNet18 as the default backbone if not being specified.

\textbf{Evaluation methods.} 
We evaluate the performance of the models against various attacks, including FGSM~\citep{goodfellow2014explaining}, PGD~\citep{madry2017towards}, C\&W~\citep{carlini2017towards}, AutoAttack~\citep{croce2020reliable}, and SparseFool~\cite{modas2019sparsefool}, covering budget measurements across $\ell_\infty$-norm, $\ell_2$-norm, and $\ell_1$-norm.
For the PGD attack, we consider both $\ell_\infty$-norm and $\ell_2$-norm, denoted as PGD-$\ell_\infty$ and PGD-$\ell_2$, respectively. SparseFool uses the $\ell_1$-norm. Unless otherwise specified, $\ell_\infty$ is used as the default measurement.
% AutoAttack is an ensemble attack consisting of three adaptive white-box attacks and one black-box attack, which is considered as a reliable evaluation method to avoid the false sense of security. 


\textbf{Baselines.} For robust overfitting mitigation, we include the baselines including regularization ($\ell_1$, $\ell_2$ regularizations and their combination), Cutout~\cite{devries2017improved}, Mixup~\cite{zhang2017mixup}, and early stopping~\cite{rice2020overfitting}. For adversarial training methods, we compare the baselines including PGD-AT~\citep{madry2017towards}, TRADES~\citep{zhang2019theoretically}, MART~\citep{wang2019improving}, SAT~\citep{huang2020self},  AWP~\citep{wu2020adversarial}, Consistency~\citep{tack2022consistency}, DYNAT~\cite{liu2024dynamic}, PORT~\cite{sehwag2021robust}, and HAT~\cite{rade2022reducing}. 


\textbf{Hyperparameter setting.}
We train the baselines for 200 epochs with batch size 128, weight decay 2e-5, momentum 0.9, and an initial learning rate of 0.1 that is divided by 10 at the 100-th and 150-th epoch. For our Elastic DL, we pretrain the Vanilla DL model for 150 epochs and then fine-tune the Elastic DL model for 50 epochs.






\subsection{Adversarial Robustness \& Generalization}

First, we validate the effectiveness of our approach in mitigating overfitting. Next, we conduct a comprehensive evaluation of the robustness of adversarial training methods. Finally, we demonstrate that our approach surpasses the state-of-the-art methods on the leaderboard by incorporating structural priors.

\textbf{Robust overfitting mitigation.} To validate the effectiveness of incorporating structural priors, we compare our method with existing popular baselines in mitigating the \emph{robust overfitting} problem in  Table~\ref{tab:robust_overfitting} and Figure~\ref{fig:curve_resnet10_all_adv_acc_test}.
% Moreover, we also track the training curves during the pretraining and fine-tuning phases in Figure~\ref{fig:adv_train_curve} to gain insight into the working mechanism of our method. 
% In the pretraining phase, the Vanilla DL model is trained for 200 epochs using a step-decay learning rate schedule. Subsequently, the model is switched to Elastic DL with all $\beta=0.5$ as the initialization, followed by fine-tuning for 50 epochs.
We leave  the training curves of all the methods in Appendix~\ref{sec:training_curves_each_method} and Appendix~\ref{sec:curves_comparison_all_method} due to the space limit.   
From the results, we can make the following observations: 
\vspace{-0.1in}
\begin{itemize}[left=0.0em]
    \item From Table~\ref{tab:robust_overfitting}, we observe that our Elastic DL method
    not only achieves a significant advantage in both absolute FINAL and BEST performance but also maintains a relatively small gap (DIFF) between them, indicating that incorporating the structural prior effectively guides adversarial training to achieve better robustness and generalization.
    
    % \item In terms of FINAL and BEST performance, we observe that our Elastic DL method significantly outperforms other baselines in both natural and robust accuracy, indicating that incorporating the structural prior effectively guides adversarial training to achieve better robustness and generalization.
    % \item In terms of DIFF performance, early stopping~\cite{rice2020overfitting} effectively reduces the gap between the best and final performance. However, it falls short in achieving the absolute best performance, thereby limiting the upper bound of final natural and robust accuracy. In contrast, our Elastic DL method not only achieves a significant advantage in both absolute final and best performance but also maintains a relatively small gap between them.
    \item From Figure~\ref{fig:curve_resnet10_all_adv_acc_test}, we observe that during the 100th to 200th epochs, the Vanilla DL model exhibits a severe \emph{robust overfitting} phenomenon. By incorporating our Elastic DL structural prior at the 150th epoch, the test robustness improves substantially, highlighting the promising potential of the Elastic DL structural prior in overcoming the bottleneck of adversarial robustness and generalization.
\end{itemize}






\begin{table}[h!]
\centering
\caption{ Natural and robust performance of PGD-based adversarial training with different methods to mitigate the overfitting. BEST represents the highest test accuracy achieved during training, while FINAL is the average accuracy over the last five epochs. DIFF, the difference between BEST and FINAL, measures the ability to mitigate overfitting. 
% Each of the regularization methods listed is trained using the optimally chosen hyperparameter. 
The best performance is highlighted in \textbf{bold}, while the second-best is \underline{underlined}.
}

\label{tab:robust_overfitting}
% \vspace{0.1in}
\begin{center}
\begin{sc}
\resizebox{0.47\textwidth}{!}{
\setlength{\tabcolsep}{1.3pt}
\rowcolors{2}{gray!20}{white}
\begin{tabular}{c|ccc|ccc}
\hline
\rowcolor[HTML]{C0C0C0}
&\multicolumn{3}{c|}{\textbf{Natural Acc.} }&\multicolumn{3}{c}{\textbf{Robust Acc.} }\\\hline
\textbf{Method}&\textbf{Final}&\textbf{Best}&\textbf{Diff}&\textbf{Final}&\textbf{Best}&\textbf{Diff}\\\hline
Vanilla&78.98 & 79.90 & 0.92 & 44.90 & 48.01 & 3.11 \\
$\ell_1$ Reg.&64.84 & 65.71 & 0.87 & 40.94 & 41.97 & 1.03 \\
$\ell_2$ Reg.&78.88 & 79.39 & 0.51 & 42.73 & 48.26 & 5.53 \\
$\ell_2$ + $\ell_1$  Reg.&66.86 & 67.62 & 0.76 & 42.53 & 43.33 & 0.80\\
Cutout&75.11 & 75.58 & 0.47 & 47.12 & 48.23 & 1.11 \\
Mixup&69.64 & 72.05 & 2.41 & 46.10 & 48.53 & 2.43\\
% Semi-supervised&\\
Early Stopping&75.51 & 75.51 & \textbf{0.00} & \underline{47.69} & 47.95 & \textbf{0.26} \\
Vanilla DL& \underline{82.59} & \textbf{83.27} & 0.68 & 44.03 & \underline{50.53} & 6.50\\
Elastic DL \red{(Ours)}& \textbf{83.01} & \textbf{83.27} & \underline{0.26} & \textbf{54.94} & \textbf{55.66} & \underline{0.72}\\
\hline
\end{tabular}

}

\end{sc}
\end{center}
\vspace{-0.2in}
\end{table}




\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/training_curves/curve_resnet10_all_adv_acc_test.png}
    \vspace{-0.2in}
    \caption{Test robust accuracy during the adversarial training. we pretrain
the Vanilla DL model for 150 epochs and fine-tune the
Elastic DL model starting from 150-th epoch. Our Elastic DL method can achieve the best adversarial robustness.
    % \xr{we need to explain the experiment setting: we fine-tune the modeling using our architecture starting from 150-th epoch}
    }
    \label{fig:curve_resnet10_all_adv_acc_test}
    \vspace{-0.2in}
\end{figure}









\begin{table}[ht!]
\centering
\caption{Adversarial robsustness on CIFAR10 with ResNet18 as backbone. The best performance is highlighted in \textbf{bold}. 
}
% \vspace{-0.1in}
\begin{center}
\begin{sc}
\resizebox{0.48\textwidth}{!}{

\rowcolors{2}{gray!20}{white}
\begin{tabular}{l|c|ccccc}
\hline
\rowcolor[HTML]{C0C0C0}
\textbf{Method}
&\textbf{Clean}&\textbf{PGD}&\textbf{FGSM}&\textbf{C\&W}&\textbf{AA}\\
\hline
% PGD-AT & 80.90 & 44.35 & 58.41 & 46.72 & 42.14 \\
% TRADES & 78.92 & 48.40 & 59.60 & 47.59 & 45.44 & 50.26 \\
% TRADES-2.0&82.80&48.32&51.67&40.65&36.40\\
% TRADES-0.2&85.74&32.63&44.26&26.70&19.00\\
MART & 79.03 & 48.90 & 60.86 & 45.92 & 43.88  \\
SAT & 63.28 & 43.57 & 50.13 & 47.47 & 39.72 \\
AWP &81.20 &51.60& 55.30& 48.00& 46.90 \\
% \zc{Free-AT}\\
% PGD-AT + DL - ResNet18  &84.08&46.24&54.61&52.80&36.70&47.59
Consistency&84.37&45.19&53.84&43.75&40.88\\
DYNAT&82.34&52.25&65.96&52.19&45.10\\
% Paradigm 3  &80.43&\textbf{57.23}&\textbf{70.23}&\textbf{64.07}&\textbf{52.60}\\
% HAT&85.95&56.29&61.17&49.52&53.16\\
% HAT-200-PreActResNet18 w/ Swish&&&\\
% HAT-400-wa + DL - ResNet\\
% S2O & 40.09 & 24.05 & 29.76 & 47.00 & 44.00 & 36.20 \\

\hline
PGD-AT & 80.90 & 44.35 & 58.41 & 46.72 & 42.14 \\
+ Vanilla DL&83.28&45.64&53.88&41.22&43.70\\
+ Elastic \red{(Ours)}  &83.57&53.22&69.35&60.80&52.90 \\
\hline
TRADES-2.0&82.80&48.32&51.67&40.65&36.40\\
+ Vanilla DL&79.05 &40.64&47.12&41.49&34.90\\
% TRADES-2.0 + Elastic DL - ResNet18 (e=1e-3) &78.8&42.37&51.46&44.51&28.60\\
+ Elastic \red{(Ours)} &79.85&49.32&58.68&49.47&47.20\\
\hline
TRADES-0.2&85.74&32.63&44.26&26.70&19.00\\
+ Vanilla DL&82.55 & 25.37&44.48&30.3&15.30\\
+ Elastic \red{(Ours)} &84.75&33.61&57.86&40.68&28.10\\
% \hline
% MART-5.0 +Vanilla DL&72.11&45.4&49.7&53.01&\\
% MART-5.0 Elastic DL &\\
% \hline
% MART-0.5 +Vanilla DL&76.58&43.31&49.58&47.4&39.10&\\
% MART-0.5 + Elastic DL - ResNet18 (1e-3) &76.13&45.15&53.52&50.55&33.70\\
% MART-0.5 Elastic DL &76.13&49.02&59.66&54.67&46.83\\
% \hline
% HAT-50+Vanilla DL&83.69&54.30&58.62&50.32&40.43\\
% HAT-50+Elastic DL \red{(Ours)}&84.23&57.77&62.16&56.52&48.20\\
\hline
PORT&84.59&58.62&62.64&58.12&55.14\\
+ Vanilla DL&82.35&56.40& 60.68&56.77&54.00\\
+ Elastic \red{(Ours)} &82.76&59.00&68.54&60.92&56.30\\
\hline
HAT&85.95&56.29&61.17&49.52&53.16\\
+ Vanilla DL&86.42&57.79&62.67&51.61&54.30\\
+ Elastic \red{(Ours)} &\textbf{86.84}&\textbf{62.48}&\textbf{71.46}&\textbf{59.90}&\textbf{59.07}\\
% \hline
% HAT-400-wa +Vanilla DL\\

% \bottomrule
\hline
\end{tabular}
}
\vspace{-0.1in}
\end{sc}
\end{center}

\label{tab:cifar10-main}
\end{table}


\begin{figure*}[htbp]
    \centering
    % Subfigure (a)
    \begin{subfigure}[b]{0.33\textwidth} % Set the width of the subfigure
        \centering
        \includegraphics[width=\textwidth]{figures/Adversarial_Robustness_CIFAR10.png} % Replace with your image path
        \caption{CIFAR10} % Subfigure caption
        \label{fig:subfig_a}
    \end{subfigure}
    \hfill
    % Subfigure (b)
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/Adversarial_Robustness_CIFAR100.png} % Replace with your image path
        \caption{CIFAR100}
        \label{fig:subfig_b}
    \end{subfigure}
    \hfill
    % Subfigure (b)
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/Advesarial_Robustness_Tiny_Imagenet.png} % Replace with your image path
        \caption{Tiny-ImageNet}
        \label{fig:subfig_d}
    \end{subfigure}
    % \hfill
    % % Subfigure (c)
    % \begin{subfigure}[b]{0.33\textwidth}
    %     \centering
    %     \includegraphics[width=\textwidth]{figures/Adversarial_Robustness_Diff_Adv_Train.png} % Replace with your image path
    %     \caption{Different Adversarial Training (CIFAR10)}
    %     \label{fig:subfig_c}
    % \end{subfigure}
    %     \hfill
    % % Subfigure (b)
    % \begin{subfigure}[b]{0.66\textwidth}
    %     \centering
    %     \includegraphics[width=\textwidth]{figures/diff_norm_budget.png} % Replace with your image path
    %     \caption{Different attack measurements (CIFAR10)}
    %     \label{fig:subfig_d}
    % \end{subfigure}
    \vspace{-0.3in}
    \caption{Adversarial robustness under various settings. Our Elastic DL outperforms Vanilla DL across various  datasets (CIFAR10 / CIFAR100 / Tiny-ImageNet), backbones (ResNet10 / ResNet18 / ResNet34 / ResNet50) and attacks (PGD / FGSM / CW / AA).
    % \xr{For all figures, we should make the font size larger (close to the text size in main paper) for readability. It is hard to read if they are such small}
    } % Main figure caption
    \label{fig:diff_datasets}
    \vspace{-0.2in}
\end{figure*}


\textbf{Adversarial training robustness.} To validate the effectiveness of our Elastic DL, we select several existing popular adversarial defenses and report the experimental
results of backbone ResNet18 under various attacks in Table~\ref{tab:cifar10-main}.  From the results we can make the following observations:
\vspace{-0.1in}
\begin{itemize}[left=0em]
    \item Our HAT + Elastic DL significantly outperforms other methods across various attacks, achieving state-of-the-art performance among all baselines.
    \item Our Elastic DL is a robust architecture that is orthogonal to existing adversarial training methods and can be combined with them to further improve robustness.
    % \item Our proposed Elastic DL consistently improve over Vanilla DL when combined with various adversarial training methods, demonstrating its effectiveness in accommodating both light-tailed and heavy-tailed noise.
\end{itemize}




% \begin{table}[h!]
% \centering
% \caption{Benchmarking state-of-the-art performance with ResNet18 on CIFAR10 under AutoAttack ($\epsilon=8/255$). *Best performing ResNet-18 model available on RobustBench~\cite{croce2020robustbench}. \textsection Best performance reported in the paper~\cite{rade2022reducing}. Our HAT+Elastic DL can outperform state-of-the-art adversarial training in terms of robustness (ROBUST) and average performance (AVG). \red{give more setting, like $\ell_2$-norm budget, try other backbone, wide }
% }

% \vspace{0.1in}
% \begin{center}
% \begin{sc}
% \resizebox{0.5\textwidth}{!}{
% \setlength{\tabcolsep}{0.5pt}
% \begin{tabular}{c|cccccccccc}
% \toprule
% \textbf{Method}&\textbf{Clean}&\textbf{Robust} & \textbf{Avg}\\
% \hline
% HAT + Elastic DL \red{(Ours)}& 86.84 &59.07&72.96 \\
% HAT + DL &86.42&54.30&70.36\\
% *PORT~\cite{sehwag2021robust}&84.59&55.54&70.07\\
% DAJAT~\cite{addepalli2022efficient}&85.71&	52.48\\
% OA-AT~\cite{addepalli2022scaling}&80.24&	51.06\\
% \textsection HAT~\cite{rade2022reducing} &\textbf{87.66}&54.46&71.06\\
%  HAT (Reproduced)&85.95&53.16&69.56 \\


% \bottomrule
% \end{tabular}

% }

% \end{sc}
% \end{center}

% \label{tab:benchmark_resnet18_cifar10}
% \end{table}







% \begin{figure*}[htbp]
%     \centering
%     % % Subfigure (a)
%     % \begin{subfigure}[b]{0.33\textwidth} % Set the width of the subfigure
%     %     \centering
%     %     \includegraphics[width=\textwidth]{figures/Adversarial_Robustness_CIFAR10.png} % Replace with your image path
%     %     \caption{Different Backbones (CIFAR10)} % Subfigure caption
%     %     \label{fig:subfig_a}
%     % \end{subfigure}
%     % \hfill
%     % % Subfigure (b)
%     % \begin{subfigure}[b]{0.33\textwidth}
%     %     \centering
%     %     \includegraphics[width=\textwidth]{figures/Adversarial_Robustness_CIFAR100.png} % Replace with your image path
%     %     \caption{Different Backbones (CIFAR100)}
%     %     \label{fig:subfig_b}
%     % \end{subfigure}
%     % \hfill
%     % % Subfigure (b)
%     % \begin{subfigure}[b]{0.33\textwidth}
%     %     \centering
%     %     \includegraphics[width=\textwidth]{figures/Advesarial_Robustness_Tiny_Imagenet.png} % Replace with your image path
%     %     \caption{Different Backbones (Tiny-ImageNet)}
%     %     \label{fig:subfig_d}
%     % \end{subfigure}
%     % \hfill
%     % Subfigure (c)
%     \begin{subfigure}[b]{0.33\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/Adversarial_Robustness_Diff_Adv_Train.png} % Replace with your image path
%         \caption{Different Adversarial Training (CIFAR10)}
%         \label{fig:diff_adv_train}
%     \end{subfigure}
%         \hfill
%     % Subfigure (b)
%     \begin{subfigure}[b]{0.66\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/diff_norm_budget.png} % Replace with your image path
%         \caption{Different attack measurements (CIFAR10)}
%         \label{fig:diff_measure}
%     \end{subfigure}
    

%     \caption{Ablation study.} % Main figure caption
%     \label{fig:ablation}
% \end{figure*}

% \newpage
% ~\\
% \newpage















\textbf{SOTA performance on leaderboard.} 
Furthermore, we validate whether incorporating our structural prior improves over state-of-the-art methods. To achieve this, we select the top-ranking methods, HAT~\cite{rade2022reducing} and PORT~\cite{sehwag2021robust}), listed on the RobustBench~\cite{croce2020robustbench} leaderboard under $\ell_\infty$-norm and $\ell_2$-norm attacks, using ResNet-18 on the CIFAR-10 dataset. As shown in Table~\ref{tab:benchmark_resnet18_cifar10_linf} ($\ell_\infty$-norm attack) and Table~\ref{tab:benchmark_resnet18_cifar10_l2} ($\ell_2$-norm attack), 
Our methods, HAT+Elastic DL and PORT+Elastic DL, consistently achieve superior performance in most cases for both natural and robust performances.




\begin{table}[h!]
\centering
\caption{Benchmarking the state-of-the-art performance under $\ell_\infty$-norm attack of ResNet18 on CIFAR10.
}

\begin{sc}
\begin{center}
% \begin{sc}
\resizebox{0.48\textwidth}{!}{
\setlength{\tabcolsep}{2.0pt}
\rowcolors{2}{gray!20}{white}
\begin{tabular}{l|c|ccc|ccc}
\bottomrule
\rowcolor[HTML]{C0C0C0}
\multicolumn{8}{c}{ \textbf{Leaderboard under $\ell_\infty$-norm Attack} }\\
\hline
&\textbf{Clean}&\multicolumn{3}{c|}{ \textbf{PGD}-$\ell_\infty$}&\multicolumn{3}{c}{ \textbf{AutoAttack}-$\ell_\infty$}\\
\hline
\textbf{ Budget}&\textbf{0}&$\mathbf{\frac{8}{255}}$&$\mathbf{\frac{16}{255}}$&$\mathbf{\frac{32}{255}}$&$\mathbf{\frac{8}{255}}$&$\mathbf{\frac{16}{255}}$&$\mathbf{\frac{32}{255}}$\\
\hline
PORT&84.59&58.62 & 27.49 & 5.79& 55.14 & 17.8 & 0.3\\
+ Vanilla DL&82.35&56.4 & 27.3 & 6.38&54.0 & 20.4 & 0.6\\
+ Elastic (\red{Ours})&82.76&59.0 & 36.53 & 22.17&56.3 & 24.6 & 1.7\\
\hline
HAT&85.95&56.29&25.82&6.09&53.16& 17.20 & 0.60\\
+ Vanilla DL&86.42&57.79&26.08 & 6.07&54.30&17.56 & 0.63\\
+ Elastic (\red{Ours})&\textbf{86.84}&\textbf{62.48}&\textbf{44.66} & \textbf{33.69}&\textbf{59.10}& \textbf{29.93}& \textbf{2.10} \\
\hline

% \bottomrule
% \hline
\end{tabular}
}
\vspace{-0.2in}
% \end{sc}
\end{center}
\end{sc}
\label{tab:benchmark_resnet18_cifar10_linf}
\end{table}




\begin{table}[h!]
\centering
\caption{Benchmarking the state-of-the-art performance under $\ell_2$-norm attack of ResNet18 on CIFAR10.
}
% \vspace{0.1in}
\begin{center}
\begin{sc}
\resizebox{0.48\textwidth}{!}{
\setlength{\tabcolsep}{2.0pt}
\rowcolors{2}{gray!20}{white}
\begin{tabular}{l|c|ccc|ccc}
\bottomrule
\rowcolor[HTML]{C0C0C0}
\multicolumn{8}{c}{ \textbf{Leaderboard under $\ell_2$-norm attack} }\\
\hline
&\textbf{Clean}&\multicolumn{3}{c|}{ \textbf{PGD}-$\ell_2$}&\multicolumn{3}{c}{ \textbf{AutoAttack}-$\ell_2$}\\
\hline
\textbf{ Budget}&\textbf{0}&\textbf{0.5}&\textbf{1.0}&\textbf{2.0}&\textbf{0.5}&\textbf{1.0}&\textbf{2.0}\\

\hline
PORT&88.82 &74.89 & 54.47 & 27.69 &\textbf{73.80}&48.1&5.90\\
+ Vanilla DL&87.34 &73.52 & 53.75 & 27.5&71.8 & 49.1 & 6.7\\
+ Elastic (\red{Ours})&87.81 &\textbf{75.56} & \textbf{60.76} & \textbf{41.44}&72.2 & \textbf{52.4} & \textbf{11.1} \\
\hline
HAT&89.92&74.68 & 47.67 & 21.38&72.9 & 40.8 & 2.2\\
+ Vanilla DL& 88.84&67.99 & 40.87 & 17.97&66.8 & 27.8 & 0.6\\
+ Elastic (\red{Ours})& \textbf{89.95}&74.62 & 51.41 & 27.05&73.2 & 44.5 & 3.2 \\
\hline

% \bottomrule
% \hline
\end{tabular}
}
\end{sc}
\end{center}
\label{tab:benchmark_resnet18_cifar10_l2}
\vspace{-0.2in}
\end{table}






% \begin{table}[h!]
% \centering
% \caption{Benchmarking state-of-the-art performance with ResNet18 on CIFAR10 under AutoAttack ($\epsilon=8/255$). *Best performing ResNet-18 model available on RobustBench~\cite{croce2020robustbench}. 
% % \textsection Best performance reported in the paper~\cite{rade2022reducing}. 
% Our HAT+Elastic DL can achieve the best performance  in terms of robustness (ROBUST) and average performance (AVG).
% }


% \red{more leaderboard}

% \vspace{0.1in}
% \begin{center}
% % \begin{sc}
% \resizebox{0.45\textwidth}{!}{
% \setlength{\tabcolsep}{2.0pt}
% \rowcolors{2}{gray!20}{white}
% \begin{tabular}{clcccccccc}
% \bottomrule
% \rowcolor[HTML]{C0C0C0}
% \multicolumn{5}{c}{Leaderboard: CIFAR-10, ResNet-18, $\ell_\infty=8/255$ }\\
% \hline
% &\textbf{Method}&\textbf{Clean}&\textbf{Robust} & \textbf{Avg.}\\
% % \hline
% 1&HAT + Elastic DL (\red{Ours})& 86.84 &59.07&72.96 \\
% 2&HAT + Vanilla DL &86.42&54.30&70.36\\
% 3&*PORT\scriptsize{~\cite{sehwag2021robust}}&84.59&55.54&70.07\\
% 4& HAT\scriptsize{~\cite{rade2022reducing}} &85.95&53.16&69.56 \\
% 5&DAJAT\scriptsize{~\cite{addepalli2022efficient}}&85.71&	52.48&69.10\\
% 6&OA-AT\scriptsize{~\cite{addepalli2022scaling}}&80.24&	51.06&69.09\\
% &PORT w/o 10M synthetic& \\
% &PORT + Vanilla DL\\
% &PORT + Elastic DL (\red{Ours}) \\
% % &\textsection HAT~\cite{rade2022reducing} &\textbf{87.66}&54.46&71.06\\





% \bottomrule
% % \hline
% \end{tabular}

% }

% % \end{sc}
% \end{center}

% \label{tab:benchmark_resnet18_cifar10}
% \end{table}







% \begin{table}[h!]
% \centering
% \caption{Benchmarking state-of-the-art performance with ResNet18 on CIFAR10 under AutoAttack ($\epsilon=8/255$). *Best performing ResNet-18 model available on RobustBench~\cite{croce2020robustbench}. 
% % \textsection Best performance reported in the paper~\cite{rade2022reducing}. 
% Our HAT+Elastic DL can achieve the best performance  in terms of robustness (ROBUST) and average performance (AVG).
% }



% \vspace{0.1in}
% \begin{center}
% % \begin{sc}
% \resizebox{0.45\textwidth}{!}{
% \setlength{\tabcolsep}{2.0pt}
% \rowcolors{2}{gray!20}{white}
% \begin{tabular}{clcccccccc}
% \bottomrule
% \rowcolor[HTML]{C0C0C0}
% \multicolumn{5}{c}{ PGD-$\ell_\infty$: CIFAR-10, ResNet-18}\\
% \hline
% \textbf{Method $\backslash$ Budget}&\textbf{Clean}&\textbf{8/255}&\textbf{16/255}&\textbf{32/255}\\
% % \hline
% HAT&85.95&56.29&25.82&6.09\\
% HAT + Vanilla DL&86.42&57.79&26.08 & 6.07\\
% HAT + Elastic DL (\red{Ours})&86.84&62.48&44.66 & 33.69 \\
% \hline
% *PORT&84.59&58.62 & 27.49 & 5.79 \\
% PORT + Vanilla DL&82.35&56.4 & 27.3 & 6.38\\
% PORT + Elastic DL (\red{Ours})&82.76&59.0 & 36.53 & 22.17 \\

% \bottomrule
% \hline

% \rowcolor[HTML]{C0C0C0}
% \multicolumn{5}{c}{ AutoAttack-$\ell_\infty$: CIFAR-10, ResNet-18}\\
% \hline
% \textbf{Method $\backslash$ Budget}&\textbf{Clean}&\textbf{8/255}&\textbf{16/255}&\textbf{32/255}\\
% % \hline
% HAT&85.95&53.16& 17.20 & 0.60\\
% HAT + Vanilla DL&86.42&54.30&17.56 & 0.63\\
% HAT + Elastic DL (\red{Ours})&86.42&59.10&29.93 & 2.10\\
% \hline
% % *PORT&84.59&55.54& \\
% *PORT &84.59& 55.14 & 17.8 & 0.3\\
% PORT + Vanilla DL&82.35&54.0 & 20.4 & 0.6\\
% PORT + Elastic DL (\red{Ours})&82.76 &56.3 & 24.6 & 1.7 \\

% \bottomrule
% \hline

% \rowcolor[HTML]{C0C0C0}
% \multicolumn{5}{c}{PGD-$\ell_2$: CIFAR-10, ResNet-18 }\\
% \hline
% \textbf{Method $\backslash$ Budget}&\textbf{Clean}&\textbf{0.5}&\textbf{1.0}&\textbf{2.0}\\

% HAT &89.92&74.68 & 47.67 & 21.38\\
% HAT + Vanilla DL&88.84&67.99 & 40.87 & 17.97\\
% HAT + Elastic DL (\red{Ours})&89.95&74.62 & 51.41 & 27.05\\
% \hline
% *PORT&88.82&74.89 & 54.47 & 27.69  \\
% PORT + Vanilla DL&87.34&73.52 & 53.75 & 27.5\\
% PORT + Elastic DL (\red{Ours})&87.81&75.56 & 60.76 & 41.44 \\

% \bottomrule
% \hline

% \rowcolor[HTML]{C0C0C0}
% \multicolumn{5}{c}{ AutoAttack-$\ell_2$: CIFAR-10, ResNet-18}\\
% \hline
% \textbf{Method $\backslash$ Budget}&\textbf{Clean}&\textbf{0.5}&\textbf{1.0}&\textbf{2.0}\\
% % \hline
% HAT&89.92&72.9 & 40.8 & 2.2\\
% HAT + Vanilla DL&88.84&66.8 & 27.8 & 0.6\\
% HAT + Elastic DL (\red{Ours})&89.95&73.2 & 44.5 & 3.2\\
% \hline
% *PORT&88.82&73.80&48.1&5.90 \\
% PORT + Vanilla DL&87.34&71.8 & 49.1 & 6.7\\
% PORT + Elastic DL (\red{Ours})&87.81&72.2 & 52.4 & 11.1 \\

% \bottomrule
% \hline

% % \bottomrule
% % \hline
% \end{tabular}

% }

% % \end{sc}
% \end{center}

% \label{tab:benchmark_resnet18_cifar10}
% \end{table}










\subsection{Ablation Study}

% \begin{figure*}[h!]
%     \centering
%     % % Subfigure (a)
%     % \begin{subfigure}[b]{0.33\textwidth} % Set the width of the subfigure
%     % \centering
%     % \includegraphics[width=1.0\textwidth]{figures/Adversarial_Robustness_Diff_Adv_Train.png} % Replace with your image path
%     % \caption{Different adversarial training. }
%     % \label{fig:diff_adv_train}
%     % \end{subfigure}
%     % \hfill
%     % % Subfigure (b)
%     % \begin{subfigure}[b]{0.66\textwidth}
%     % \centering
%     % \includegraphics[width=1.0\textwidth]{figures/diff_norm_budget.png} % Replace with your image path
%     % \caption{Different attack measurements}
%     % \label{fig:diff_measure}
%     % \end{subfigure}   
%     \begin{subfigure}[b]{0.33\textwidth}
%     \centering
%     \includegraphics[width=0.95\linewidth]{figures/loss_curve.png}
%     \caption{Algorithm convergence.}
%     \label{fig:algo_convergence}
%     \end{subfigure}   
%     \hfill
%     \begin{subfigure}[b]{0.33\textwidth}
%     \centering
%     \includegraphics[width=0.95\linewidth]{figures/hist_pert.png}
%     \caption{Attack behaviors.  }
%     \label{fig:perturbation}
%     \end{subfigure}   
%     \hfill
%     \begin{subfigure}[b]{0.33\textwidth}
%     \centering
%     \includegraphics[width=0.95\linewidth]{figures/diff_hidden_relative_all.png}
%     \caption{Relative embedding difference. }
%     \label{fig:embed_diff}
%     \end{subfigure}   
%     \caption{  (a) Our RISTA algorithm effectively reduces the dictionary learning objective and achieves fast convergence within just three steps. (b) The attacker tends to introduce heavy-tailed noise to target the Vanilla DL model; however, the impact of this noise can be effectively mitigated by the Elastic DL model. (c) Our Elastic DL demonstrates a smaller embedding difference between clean and adversarial data compared to Vanilla DL. This difference becomes more significant in deeper layers, indicating that our Elastic DL effectively mitigates the impact of input noise.  } % Main figure caption
%     \label{fig:diff_datasets}
% \end{figure*}








\textbf{Universality across datasets and backbones.} To validate the consistent effectiveness of our proposed methods, we conduct comprehensive abation studies on the different backbones (ResNet10, ResNet18, ResNet34, ResNet50), datasets (CIFAR10, CIFAR100, Tiny-ImageNet).
As demonstrated in the Figure~\ref{fig:diff_datasets}, Table~\ref{tab:diff_backbone_cifar10},~\ref{tab:diff_backbone_cifar100} and ~\ref{tab:diff_backbone_imagenet} in Appendix~\ref{sec:universality}, our proposed Elastic DL exhibit excellent clean performance and robustness under various attacks.








\textbf{Orthogonality to adversarial training.} Our proposed Elastic DL framework introduce structural prior into neural networks, which is orthogonal with existing adversarial training techniques. As shown in Table~\ref{tab:cifar10-main} and  Figure~\ref{fig:diff_adv_train} in Appendix~\ref{sec:orthogonality}, our Elastic DL can be combined with
different adversarial training (PGD-AT, TRADES-2.0/0.2, PORT, HAT) to further improve the performance.

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=0.4\textwidth]{figures/Adversarial_Robustness_Diff_Adv_Train.png} % Replace with your image path
%     \caption{Different Adversarial Training (CIFAR10)}
%     \label{fig:diff_adv_train}
% \end{figure}






% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=0.48\textwidth]{figures/diff_norm_budget.png} % Replace with your image path
%     \caption{Different attack measurements (CIFAR10)}
%     \label{fig:diff_measure}
% \end{figure}



\textbf{Different attack measurements.} In addition to $\ell_\infty$-norm attack (PGD-$\ell_\infty$), we also validate the consistent effectiveness of our Elastic DL with $\ell_2$-norm (PGD-$\ell_2$) and $\ell_1$-norm (SparseFool) attacks in the Figure~\ref{fig:diff_measure} and  Table~\ref{tab:diff_budget_norm} in Appendix~\ref{sec:diff_measurement}.




\begin{figure*}[h!]
    \begin{minipage}[b]{0.77\textwidth}
    \includegraphics[width=1.0\linewidth]{figures/hidden_visual.pdf}
    \caption{Hidden embedding visualization under clean and attacked scenarios. The difference between clean and attacked embeddings in Elastic DL is smaller compared to Vanilla DL, with this effect becoming more significant in deeper layers. Consequently, while an adversarial attack alters the Vanilla DL output from "\emph{SHIP}" to "\emph{FROG}", Elastic DL successfully preserves the correct prediction. }
    \label{fig:hidden_visual}
    \end{minipage}
    \vspace{-0.1in}
    \hfill
    \begin{minipage}[b]{0.22\textwidth}
    \centering
    \includegraphics[width=0.99\linewidth]{figures/diff_hidden_relative_all.png}
    \vspace{-2.5em}
    \caption{Embedding difference. 
    Our Elastic DL shows smaller embedding difference than Vanilla DL.
    }
    \label{fig:embed_diff}
    \end{minipage}
    % \vspace{-0.1in}
\end{figure*}


\begin{figure*}[h!]
\centering
    \begin{minipage}[b]{0.32\textwidth}
\centering
\includegraphics[width=1.05\linewidth]{figures/loss_curve.png}
\vspace{-0.3in}
\caption{Algorithm convergence. 
RISTA algorithm 
% effectively reduces the dictionary learning objective and 
achieves fast convergence within just three steps.
}
\label{fig:algo_convergence}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.32\textwidth}
\centering
\includegraphics[width=1.05\linewidth]{figures/hist_pert.png}
\vspace{-0.35in}
\caption{Attack behaviors. 
The attacker tends to attack Vanilla DL model by  introducing outlying values. 
% with heavy-tailed noise, which be effectively mitigated by the Elastic DL model. 
}
\label{fig:perturbation}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.32\textwidth}
    \centering
    \includegraphics[width=1.05\linewidth]{figures/ood_robustness.png}
    \vspace{-0.3in}
    \caption{Out-of-distribution robustness.
    % Beyond in-distribution robustness, 
    Our Elastic DL also demonstrates excellent out-of-distribution robustness.
    }
    \label{fig:ood_robustness}
    \end{minipage}
\end{figure*}





\textbf{Hidden embedding visualization.}
We also conduct visualization analyses on the hidden embedding to obtain better insight into the effectiveness of our proposed Elastic DL. We begin by quantifying the relative difference between clean embeddings ($\bx$ or $\bz_i$) and attacked embeddings ($\bx'$ or $\bz'_i$) across all layers, as shown in Figure~\ref{fig:embed_diff}. Additionally, we visualize one instance in Figure~\ref{fig:hidden_visual}, with more examples provided in Appendix~\ref{sec:hidden_embedding_all}.
The results in Figure~\ref{fig:embed_diff} show that Elastic DL has smaller embedding difference across  layers, indicating that our proposed Elastic DL architecture indeed mitigates the impact of the adversarial perturbation. Moreover, 
as demonstrated in the example 
in Figure~\ref{fig:hidden_visual}, the presence of adversarial perturbations can disrupt the hidden embedding patterns, leading to incorrect predictions in the case of Vanilla DL. 
In contrast, our Elastic DL appears to lessen the effects of such perturbations and maintain predicting groundtruth label. From the figures, we can also clearly tell that the difference between clean attacked embeddings of Vanilla DL is much more significant than  in  Elastic DL.








\textbf{Convergence.} 
To validate the effectiveness of our RISTA iterations, we plot the loss descent curves of overall objective Eq.\eqref{eq:robust_dictionary_learning} along with the individual terms ($\|\bx - \cA^*(\bz)\|_2^2, \|\bx - \cA^*(\bz)\|_1$ and $ \|\bz\|_1$) in Figure~\ref{fig:algo_convergence}, which shows 
% It can be observed
that RISTA converges rapidly within first three steps.





% \newpage
\textbf{Attack behaviors.}
To investigate the attack behaviors, we apply the PGD attack to both models and visualize the resulting perturbations in Figure~\ref{fig:perturbation}. It can be observed that, in the Vanilla DL model, the adversarial attack introduces substantial outlying noise, which can be largely mitigated by our Elastic DL method.
% violating the light-tailed distribution assumption. This observation also explains the inferior performance of Vanilla DL compared to our Elastic DL.





% \begin{figure*}[t!]
%     \centering
%     \includegraphics[width=1.0\linewidth]{figures/hidden_visual.pdf}
%     \caption{Hidden embedding visualization of Vanilla DL and Elastic DL under clean and attacked scenarios. The difference between clean and attacked embeddings in Elastic DL is smaller compared to Vanilla DL, with this effect becoming more significant in deeper layers. Consequently, while an adversarial attack alters the Vanilla DL output from "\emph{SHIP}" to "\emph{FROG}", Elastic DL successfully preserves the correct prediction. }
%     \label{fig:hidden_visual}
% \end{figure*}











% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=1.0\linewidth]{figures/idx15_hidden_visualization_resnet.png}
%     \includegraphics[width=1.0\linewidth]{figures/idx15_hidden_visualization_adv_resnet.png}
%     % \includegraphics[width=1.0\linewidth]{figures/idx15_hidden_visualization_L2.png}
%     % \includegraphics[width=1.0\linewidth]{figures/idx15_hidden_visualization_adv_L2.png}
%     \includegraphics[width=1.0\linewidth]{figures/idx15_hidden_visualization_L1.png}
%     \includegraphics[width=1.0\linewidth]{figures/idx15_hidden_visualization_adv_L1.png}
%     \caption{Hidden Embedding Visualization}
%     \label{fig:hidden_embedding_visualization}
% \end{figure}






\textbf{Out-of-distribution Robustness.} Beyond in-distribution robustness, we further validate the advantage of our proposed Elastic DL structure by evaluating the out-of-distribution performance of Vanilla DL and Elastic DL. The results in Figure~\ref{fig:ood_robustness} demonstrate the superiority of our Elastic DL over the Vanilla  DL under various types of out-of-distribution noise.

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=1.0\linewidth]{figures/ood_robustness.png}
%     \caption{Out-of-distribution robustness. Beyond in-distribution robustness, our Elastic DL also demonstrates excellent out-of-distribution robustness.}
%     \label{fig:ood_robustness}
% \end{figure}



% \begin{table}[h!]
% \vspace{-0.1in}
% \caption{OOD Robustness.}
% \label{tab:ood_robustness}
% \vspace{-0.05in}

% \begin{center}
% \begin{tabular}{c|c|c|c|c|c|cccc|c}
% \toprule
% Model $\backslash$ Noise Level&1&2&3&4&5
%  \\\hline
%  % ResNet18 &84.13 & 69.61 & 51.9 & 42.57 & 34.59 \\
%  % \midrule
%  \multicolumn{6}{c}{Gaussian}\\
% % SDNet18 (clean 93.69)& 79.74 & 64.39 & 48.19 & 41.1 & 35.34\\
% % Elastic DLNet18 (clean 93.63)& 83.26 & 71.83 & 58.21 & 50.59 & 44.34\\

% SDNet18 & 79.74 & 64.39 & 48.19 & 41.1 & 35.34\\
% Elastic DLNet18 & 83.26 & 71.83 & 58.21 & 50.59 & 44.34\\
% \midrule
% \multicolumn{6}{c}{Impulse}\\
% SDNet18& 84.4 & 74.84 & 67.1 & 48.15 & 31.61\\
% Elastic DLNet18&85.06&77.03&69.79&52.68&35.73\\
% \midrule
% \multicolumn{6}{c}{Shot}\\
% SDNet18& 85.56 & 77.95 & 59.98 & 52.49 & 41.33\\
% Elastic DLNet18& 87.46 & 82.2 & 67.89 & 61.11 & 49.97\\
% \midrule
% \multicolumn{6}{c}{Speckle}\\
% SDNet18& 85.69 & 74.44 & 67.87 & 56.03 & 45.36\\
% Elastic DLNet18& 87.48 & 79.07 & 73.94 & 62.94 & 51.02\\
% \midrule
% \multicolumn{6}{c}{Frost}\\
% SDNet18 & 88.63 & 82.26 & 71.72 & 69.18 & 57.18\\
% Elastic DLNet18 & 87.91 & 82.03 & 72.5 & 72.55 & 62.57\\
%  \midrule
% \multicolumn{6}{c}{Fog}\\
% SDNet18& 93.09 & 91.32 & 88.61 & 85.02 & 68.36\\
% Elastic DLNet18& 91.92 & 90.45 & 88.12 & 84.95 & 70.7\\
%  \midrule
% \multicolumn{6}{c}{JPEG}\\
% SDNet18& 86.95 & 82.96 & 81.25 & 79.1 & 75.44\\
% Elastic DLNet18& 87.5 & 83.86 & 82.03 & 80.07 & 76.26\\
% \bottomrule
% \end{tabular}
% \end{center}
% \end{table}




\textbf{Running time analysis.} We also perform a 
% complexity
analysis to evaluate the inference time of different architectures using ResNet18 as the backbone. We replace multiple convolutional layers in ResNet18 with either Vanilla DL or Elastic DL layers, ranging from 0 to 14 layers. As shown in Table~\ref{tab:comlexity}, our Elastic DL introduces only a slight computational overhead compared to Vanilla DL and requires 1-3 times more computation than ResNets, which is considered acceptable. However, our Elastic DL demonstrates significantly improved robustness compared to ResNets and Vanilla DL.


\begin{table}[h!]
\centering
\vspace{-0.2in}
% \caption{ Complexity analysis. 
\caption{ Running time (ms) analysis.
% \xr{unit? second or ms?}
}
\label{tab:comlexity}
\vspace{0.1in}
\begin{center}
\begin{sc}
\resizebox{0.48\textwidth}{!}{
\setlength{\tabcolsep}{1.8pt}
\begin{tabular}{c|ccccccccc}
\hline
\rowcolor[HTML]{C0C0C0}
\textbf{Layers}  & \textbf{0 (ResNet)} & \textbf{2} & \textbf{4} & \textbf{6} & \textbf{8} & \textbf{10} & \textbf{12} & \textbf{14} \\\hline

Vanilla DL&7.82   & 8.40   & 9.28   & 10.51   & 12.13   & 13.11   & 14.16   & 15.40  \\
Elastic DL&7.82   & 8.90   & 11.39   & 13.18   & 15.99   & 16.86   & 19.57   & 21.94 \\
 

\bottomrule

\end{tabular}

}

\end{sc}
\end{center}
\vspace{-0.2in}
\end{table}



