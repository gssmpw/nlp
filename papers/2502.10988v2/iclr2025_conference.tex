
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[colorlinks=True,
            linkcolor=cvprblue,
            anchorcolor=blue,  
            pagebackref,
            citecolor=cvprblue,
            ]{hyperref}
\usepackage{wrapfig}
\usepackage{url}
\usepackage{tabu}%
\usepackage{longtable}

\usepackage{booktabs}
\arrayrulewidth=1pt
\setlength{\arrayrulewidth}{1pt}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{multirow}
% \usepackage[dvipsnames]{xcolor}       % colors
\usepackage{graphicx,verbatimbox}
\usepackage{subcaption}
\usepackage{makecell}

\usepackage{colortbl}
\definecolor{color3}{RGB}{255, 255, 200}
\definecolor{color2}{RGB}{255, 220, 200}
\definecolor{color1}{RGB}{255, 181, 163}
\newcommand{\cc}[1]{\cellcolor{color#1}}
\newcommand{\yaqi}[1]{\textcolor{Orchid}{[Yaqi: #1 ]}}
\newcommand{\ks}[1]{\textcolor{Cyan}{[Katia: #1 ]}}
\newcommand{\RebuttalRevision}[1]{\textcolor{black}{#1}}


\title{\textbf{O{\Huge M}G}: \underline{O}pacity \underline{M}atters in \underline{M}aterial \underline{M}odeling with \underline{G}aussian splatting}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Silong Yong\thanks{Joint collaborators in research inception } \quad Venkata Nagarjun Pudureddiyur Manivannan\footnotemark[1] \quad Bernhard Kerbl \\
\textbf{Zifu Wan \qquad \qquad Simon Stepputtis \qquad \qquad Katia Sycara \qquad \qquad Yaqi Xie} \\ % \thanks{} \\
Carnegie Mellon University\\
\texttt{\{silongy,vpudured,bkerbl,zifuw,sstepput,sycara,yaqix\}@andrew.cmu.edu} \\
% \texttt{\{zifuw, sstepput, sycara, yaqix\}@andrew.cmu.edu}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}
\maketitle

\begin{abstract}
 Decomposing geometry, materials and lighting from a set of images, namely inverse rendering, has been a long-standing problem in computer vision and graphics. Recent advances in neural rendering enable photo-realistic and plausible inverse rendering results. The emergence of 3D Gaussian Splatting has boosted it to the next level \RebuttalRevision{by showing real-time rendering potentials}.  An intuitive finding is that the models used for inverse rendering do not take into account the dependency of opacity w.r.t. material properties, namely cross section, as suggested by optics. Therefore, we develop a novel approach that adds this dependency to the modeling itself. Inspired by radiative transfer, we augment the opacity term by introducing a neural network that takes as input material properties to provide modeling of cross section and a physically correct activation function. The gradients for material properties are therefore not only from color but also from opacity, facilitating a constraint for their optimization. Therefore, the proposed method incorporates more accurate physical properties compared to previous works. We implement our method into 3 different baselines that use Gaussian Splatting for inverse rendering and achieve significant improvements universally in terms of novel view synthesis and material modeling. 
\end{abstract}

\section{Introduction}
\looseness=-1
Inverse rendering, a long-standing problem in computer vision and graphics, aims to recover physical properties such as geometry, materials and lighting conditions from a set of images~\citep{barron2013intrinsic, debevec2008rendering}. These properties are essential to downstream applications such as material editing, relighting and, from a broader perspective, real-to-sim transfer. However, it is well-known to be challenging and ill-posed because of the inherent ambiguity of the task: Different combinations of illumination, materials and geometry may lead to similar rendering results. Recent success in novel view synthesis~\citep{mildenhall2021nerf, barron2021mip, chen2022tensorf, muller2022instant, barron2023zip, barron2022mip} inspired the use of implicit neural fields (NeRF-like methods) for inverse rendering~\citep{zhang2021nerfactor, yao2022neilf, boss2021nerd, jin2023tensoir, srinivasan2021nerv, zhang2021physg}. On the other hand, 3D Gaussian Splatting (3DGS)~\citep{kerbl20233d} emerged as a recent, high-performance alternative to these slower architectures for novel view synthesis by introducing a much more compact scene representation and real-time rendering ability. Naturally, recent works combine 3DGS with inverse rendering~\citep{liang2024gs, gao2023relightable, jiang2024gaussianshader}, taking advantage of its high-speed nature and compact geometry. By adding additional parameters that act as material properties for each Gaussian, they are able to provide physically plausible material modeling and novel view synthesis results using physically-based rendering (PBR) thanks to the explicit nature of 3DGS. While it is intuitive to take advantage of 3DGS for fast training and rendering with high quality, directly applying 3DGS for inverse rendering may result in sub-optimal modeling. Specifically, the correlation between the material attributes of each Gaussian (e.g., opacity, albedo, roughness, etc.) remains under-explored, leading to under-constrained modeling of the scene properties. Therefore, the inverse rendering results remain to be improved.

\looseness=-1
Observing the disentangled nature of 3DGS-based methods, our work, inspired by the Bouguer-Beer-Lambert law in radiative transfer, reveals the missing correlation between materials and opacity, two sets of parameters that are essential for physically-based rendering using 3D Gaussian Splatting. By doing so, our work introduces constraints at model level to align the model better with the physical world. Specifically, we propose to model opacity in 3DGS-based inverse rendering methods strictly following the form derived from the Bouguer-Beer-Lambert law, which states that the intensity of light should decrease exponentially when passing through an absorbing body. Intuitively speaking, the decrease rate depends not only on the path length that a light travels, but also on the material and number density of the absorbing body. An illustration of the law can be found in Fig.~\ref{fig:motivation}. Consider two different translucent materials glass and gas in Fig.~\ref{fig:motivation}. They react differently when hit by light. For instance, red light would be terminated by the gas while it's able to pass through the glass nearly unaffected as illustrated by \RebuttalRevision{Fig.~\ref{fig:motivation}}. In other words, \textbf{opacity varies between different materials}. When designing the model, 3DGS-based inverse rendering methods~\citep{liang2024gs, jiang2024gaussianshader, gao2023relightable} overlook the fact that because of the disentangled representation, opacity is a standalone parameter that has no dependency on the material properties. As for NeRF-based methods that use alpha blending implicitly model the dependencies of opacity (volume density) and materials by using the same neural network to output different attributes. Therefore, for 3DGS-based methods, we propose to multiply the original opacity term with a material-dependent term and apply a physically correct activation function to give a better modeling of the physical property opacity itself. The material properties therefore serve as input for PBR as well as input for alpha-blending and receive gradients from both sides, facilitating a physically informed regularization to the model.
                                  
\begin{wrapfigure}[24]{t}{0.6\textwidth}
\begin{center}
\vspace{-10pt}
\includegraphics[width=\linewidth]{figs/motivation_l.pdf}

\end{center}
\caption{\looseness=-1 \RebuttalRevision{An illustration of the motivation that opacity depends on materials. Consider two translucent materials gas and glass. \textbf{Left}: a glass body that is hit by a light lets the light pass through with little reduction in intensity. \textbf{Right}: a gas that is hit by the same light make the light extinct inside it by absorbing it completely. This comparisons motivate our work. We perceive each Gaussian blob, paired with material properties, as an absorbing body that has its own absorption coefficient represented by opacity, which should consider material properties as influencing factors.}}
\vspace{-10pt}
\label{fig:motivation}
\end{wrapfigure}

\looseness=-1
To verify the correctness and effectiveness of the proposed formulation, we analytically conduct Taylor expansion to our approach and compare the difference w.r.t.\ the original way of computing opacity. The takeaway message is that the original way is actually an approximation of our approach. Empirically, we apply the modification to 3 state of the art baselines, namely GaussianShader~\citep{jiang2024gaussianshader}, GS-IR~\citep{liang2024gs} and R3DG~\citep{gao2023relightable} and conduct experiments on both synthetic and real-world data. The experimental results on Synthetic4Relight~\citep{zhang2022modeling}, Shiny Blender~\citep{verbin2022ref}, Glossy Synthetic~\citep{liu2023nero} and MIP-NeRF 360~\citep{barron2022mip} show that our approach enables universal performance improvement in terms of across different baselines and different data. The improvements on material modeling, i.e., albedo estimation and roughness estimation, lead to better novel view synthesis and relighting results in terms of PSNR, SSIM and LPIPS. Our experimental results indicate the significance of incorporating physically correct priors and constraints into the model. 

\section{Related Work}
\subsection{Neural Scene Representation}
\looseness=-1
Recent trends in \RebuttalRevision{neural rendering}~\citep{mildenhall2021nerf, yu2021plenoctrees, fridovich2022plenoxels, sitzmann2021light, yariv2021volume, miller2024objects, huang20242d, yong2024gl} have demonstrated impressive success in addressing visual computing problems like novel view synthesis. NeRF~\citep{mildenhall2021nerf} stands out as a representative by modeling the scene using an MLP to output volume density and color in a continuous space, which requires massive repeated queries for volume rendering during training and inference to synthesize images. Efforts have been made to improve the efficiency of NeRF by introducing additional data structures~\citep{garbin2021fastnerf, muller2022instant, hedman2021baking, yu2021plenoctrees, fridovich2022plenoxels, chen2022tensorf}. However, the discretized nature of the data structures reduces the image quality to some extent. Recently, 3D Gaussian Splatting (3DGS)~\citep{kerbl20233d} comes out and is able to achieve fast rendering speed while maintaining the rendering quality, therefore drawing the community's interest. Plenty of works focus on improving 3DGS by reducing the memory footprint~\citep{fan2023lightgaussian, navaneet2023compact3d, niedermayr2024compressed}, speeding up the training pipeline~\citep{mallick2024taming, hollein20243dgs} and removing the heuristic designs for optimization~\citep{kheradmand20243d, bulo2024revising}. Another direction is on the application side, works have been done to apply 3DGS to different domains such as semantic understanding~\citep{qin2024langsplat, guo2024semantic}, time sequence modeling~\citep{luiten2024dynamic, lin2024gaussian} and robotic manipulation~\citep{lu2024manigaussian, shorinwa2024splat}, etc. The wide range of domains for application encouraged the focus of our work on inverse rendering techniques that use 3DGS as the backbone.

\subsection{Inverse Rendering}
\looseness=-1
Inverse rendering aims to decompose image observations into geometry, material and lighting conditions (i.e., scene properties) that support a myriad of downstream tasks, such as material editing and relighting~\citep{li2023multi, zhang2022modeling, kanamori2019relighting, yang2022ps}. Normally, it is tackled by combining physically-based rendering with a differentiable renderer for optimization-based decomposition of scene properties~\citep{kajiya1986rendering, chen2019learning}. While being an inherently ambiguous problem, many works assume different constraints at input level, such as known lighting conditions~\citep{bi2020neural, srinivasan2021nerv, zhang2022modeling}, fixed lighting~\citep{dong2014appearance}, unintended shadow~\citep{verbin2024eclipse} or absence of shadow simulation~\citep{zhang2021physg}. The combination of differentiable volume rendering and physically-based rendering has enabled modeling more complex conditions towards more realistic cases thanks to the emergence of \RebuttalRevision{neural scene representation~\citep{zhang2021nerfactor, yao2022neilf, liu2023nero, verbin2022ref, jin2023tensoir, attal2024flash, boss2021nerd, munkberg2022extracting}}. Recently, 3DGS has revolutionized the field of neural scene representation. It is natural to expect inverse rendering could be tackled by 3DGS-based methods to incorporate fast and precise approximation of materials, geometry and lighting~\citep{liang2024gs, jiang2024gaussianshader, gao2023relightable}. Despite the rapid success of providing plausible material estimation, the results remain to be improved, mainly because the constraints that a model should have for inverse rendering have been overlooked.

\begin{figure}[t]
\begin{center}
\includegraphics[width=\textwidth]{figs/method.pdf}
\end{center}
\caption{\looseness=-1 Pipeline overview. We add an additional block to 3DGS-based inverse rendering methods~\citep{liang2024gs, kerbl20233d, jiang2024gaussianshader, gao2023relightable}. Specifically, instead of modeling opacity as a standalone parameter as done by previous works, we augment it against material. By introducing a neural network that takes material properties as input and output cross section and multiplying the opacity with it, we are able to incorporate the Bouguer-Beer-Lambert law into the model. During optimization, material properties not only receive the gradients from color through differentiable PBR, but also the gradients from alpha enforced by the neural network. By doing so, we add an additional constraint to material properties that makes the overall pipeline strictly follow the Bouguer-Beer-Lambert law.}
\label{fig:method}
\end{figure}

\section{Preliminaries}
\looseness=-1
In this section,  We will cover the basic concepts of 3D Gaussian Splatting, inverse rendering and the Bouguer-Beer-Lambert law as our work is built upon them. 

\subsection{3D Gaussian Splatting}
\looseness=-1
3D Gaussian Splatting (3DGS)~\citep{kerbl20233d} uses a set of points represented by 3D Gaussian primitives to model the 3D scene. Formally, each Gaussian $G_i$ is represented by a mean $\mu_i$, a covariance matrix $\Sigma_i$, an opacity $o_i$ and a view-dependent outgoing radiance $c_i$. Additionally, the covariance matrix is factorized into a quaternion $q_i$ and a scale $s_i$ and the outgoing radiance is treated as a set of spherical harmonics coefficients. For the inverse rendering problem, 3DGS-based methods~\citep{liang2024gs, jiang2024gaussianshader, gao2023relightable} assign material properties $m_i$ and normals $n_i$ to each Gaussian to support physically-based rendering. These sets of parameters are used to produce PBR color, which serves as $c_i$ in the original formulation.

\looseness=-1
After projecting the 3D Gaussians to 2D space using camera parameters $\theta$, 3DGS adopts $alpha$-blending for rendering the final color of a specific pixel location $x$ following 
\begin{equation}
\begin{aligned}
    \hat{C}(x) &= \sum_{i=1}^{N} c_i\alpha_i(x)T_i(x),\\
    T_i(x) &=\prod_{j=1}^{i-1}(1-\alpha_j(x)),
  \label{eq:alpha_blending}
\end{aligned}
\end{equation}
where $N$ represents the sorted $N$ Gaussian points, $T_i$ is the accumulated transmittance of the first $i-1$ Gaussians and $\alpha_i(x)$ is computed using
\begin{equation}
\alpha_i(x) = o_i \exp(-\frac{1}{2}(x-R(\mu_i; \theta))^TR_{\theta}(\Sigma_i)^{-1}(x-R(\mu_i; \theta))),
  \label{eq:alpha_equation}
\end{equation}
in which $R$ stands for the camera projection operation, \RebuttalRevision{$R_\theta$ is the operation of applying the camera projection to the covariance matrix} and opacity $o_i$ is weighted by the probability determined by the distance of the pixel location to the mean of the projected Gaussian. Afterwards, color loss is computed between the rendered image and the ground truth:
\begin{equation}
L = ||C - \hat{C}||^2,
  \label{eq:color_loss}
\end{equation}


\subsection{The Rendering Equation}
\looseness=-1
In inverse rendering, the outgoing radiance $C_o(x, \mathbf{v})$ at a specific surface point $x$ from viewing direction $\mathbf{v}$ is determined by the rendering equation~\citep{kajiya1986rendering}:
\begin{equation}
C_o(x, \mathbf{v}) = \int_{\Omega}L_i(x, \mathbf{l})f(\mathbf{l}, \mathbf{v}, x)(\mathbf{l}\cdot \mathbf{n})d\mathbf{l},
  \label{eq:outgoing_equation}
\end{equation}
\looseness=-1
where $L_i(x, \mathbf{l})$ is the incoming light from direction $\mathbf{l}$ at point $x$, $\mathbf{n}$ is the surface normal direction and $f$ is the Bidirectional
Reflectance Distribution Function (BRDF). The integral is computed in the upper hemisphere of the surface.

\subsection{The Bouguer-Beer-Lambert Law}
\looseness=-1
The Bouguer-Beer-Lambert law states the following~\citep{mayerhofer2020bouguer, bouguer1729essai, lambert1760photometria, beer1852bestimmung}: Consider a radiation of intensity $I$, i.e., a light of intensity $I$ passing through an absorbing body, the change of its intensity is governed by the following differential equation:
\begin{equation}
    dI_\nu = -\alpha_\nu I_\nu ds,
    \label{eq:radiative_transfer}
\end{equation}
\looseness=-1
where subscript $\nu$ means that the term depends on the frequency of light and $\alpha_\nu$ is called the extinction coefficient, an attribute that determines how energy is dampened in the absorbing body.

\looseness=-1
Solving for intensity $I_\nu$, we get
\begin{equation}
    I_\nu(s) = I_\nu(0)e^{-\alpha_\nu s},
    \label{eq:integral_radiative_transfer}
\end{equation}
here $e^{-\alpha_\nu s}$ is the transmittance of region $s$, meaning that the light intensity is exponentially decreasing when traveling through the absorbing body. If the attenuation of light in the body doesn't contain scattering effects, the extinction coefficient can be computed as
\begin{equation}
\alpha_\nu = n\sigma_\nu,
  \label{eq:extinction_coefficient}
\end{equation}

where $n$ is the number density and $\sigma_\nu$ is cross section. These two terms often appear in chemistry, $n$ describes the degree of concentration of countable objects (usually particles) and $\sigma_\nu$ models the the probability that two particles will collide, which is determined by the types of the particles. 
An illustration of the Bouguer-Beer-Lambert law can be found in Fig.~\ref{fig:cross_section}, the intensity of a light passing through a region filled with particles is reduced when ``hit'' with particles (normally the affecting area is bigger than the particle size). Intuitively speaking, the reduction in intensity is determined by the number of particles in the region that the light hits and the affecting area of the specific particle type, which corresponds to Eq.~\ref{eq:extinction_coefficient} and Eq.~\ref{eq:integral_radiative_transfer}.

\begin{wrapfigure}[30]{r}{0.45\textwidth}
  \centering
  % \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \includegraphics[width=0.45\textwidth]{figs/cross_section.pdf}
  \caption{An illustration of the Bouguer-Beer-Lambert law. Consider a ray passing through an absorbing body consists of particles of some type. The cross section depends on the area that each particle would affect (usually bigger than the size of the particle and equals the size of the particle if treated as a hard sphere). When traveling, the intensity of the light would be reduced by the areas that each particle affected. The reduction in intensity is therefore affected by the area around the particles and the number of particles, corresponding to cross section $\sigma$ and number density $n$ in the main paper.}
  \label{fig:cross_section}
\end{wrapfigure}

\section{Method}
We start by introducing the intuition behind our method. It is well-known that different materials react differently when hit by a beam of light, e.g., a wooden plank would diffuse the light and let nothing pass through, while a glass pane may allow light to travel inside with no dampening. In physics, this phenomenon is described by the Bouguer-Beer-Lambert law, suggesting that the opacity of an object depends on its material. Motivated by this, we do a series of derivation and propose a plug-and-play solution suggesting that opacity should be a function of material and the computation of transmittance should follow the exponential decay rate as in the Bouguer-Beer-Lambert law.

\subsection{Gaussian Blobs as Absorbing Body}
Our approach arises from the Bouguer-Beer-Lambert law. Recall in Eq.~\ref{eq:alpha_blending}, $T_i$ stands for the accumulated transmittance of the first $(i-1)$ Gaussians, indicating the reduction of light intensity of the $i$-th Gaussian when observed from the camera perspective. For each Gaussian $G_i$, the transmittance is calculated by $1-\alpha_i(x)$. According to the Bouguer-Beer-Lambert law, the transmittance of an absorbing body should follow the form $e^{-\alpha_\nu s}$ or $e^{-n\sigma_\nu s}$ as in Eq.~\ref{eq:integral_radiative_transfer}. Therefore we have
\begin{equation}
\begin{aligned}
1-\alpha_i(x) &= e^{-n\sigma_\nu s}, \\
\alpha_i(x) &= 1-e^{-n\sigma_\nu s}.
\end{aligned}
\label{eq:transmittance}
\end{equation}

A natural question then arises: \textbf{what is $n$,  $\sigma_\nu$ and $s$ in the context of Gaussian Splatting?} 

To answer this question, recall the definition of $\alpha_i(x)$ in Eq.~\ref{eq:alpha_equation}. The term $o_i$ indicates how ``dense'' the Gaussian is, and the exponential term suggests that the Gaussian is densest at the center and becomes lighter when away from it as shown in Fig.~\ref{fig:method}. Since each Gaussian blob is not solid, it is natural to think of it as a blob of gas with its density following a Gaussian distribution. Notice that in this formulation, the density corresponds to the number density $n$ in Eq.~\ref{eq:transmittance}. Formally, we have
\begin{equation}
n_i = o_i \exp(-\frac{1}{2}(x-R(\mu_i; \theta))^TR_{\theta}(\Sigma_i)^{-1}(x-R(\mu_i; \theta))).
\label{eq:n}
\end{equation}
\looseness=-1
\RebuttalRevision{As for path length $s$, since each Gaussian is ``splatted'' to a 2D plane, i.e. density is marginalized along the viewing axis, the distance of the light passing through each Gaussian can be set as a constant $1$. In fact, the blending algorithm omits the depth of the Gaussians into the formulation of the opacity value at the pixel location. We therefore observed that we can treat this opacity value as the density and since there is no concept of depth on the 2D plane, each Gaussian is then treated equally from this perspective, therefore it is reasonable to treat the path length to constant 1.}

\subsection{Material Network for Cross Section Modeling}
\looseness=-1
After deciding on the correspondence of existing terms, we found that there is still one term missing from the Bouguer-Beer-Lambert law, namely the cross section $\sigma_\nu$. Cross section, by definition, is determined by the type of particle that fills the region as illustrated in Fig.~\ref{fig:cross_section}, namely the material of the region. On a macro level, cross section affects the color that one can perceive from the region. Since in inverse rendering, material properties, i.e., albedo, metalness and roughness, are explicitly included in the model (for 3DGS-based method they're set as additional parameters for each Gaussian), we can directly use the parameter for representing the cross section. In practice, a neural network $f(\cdot)$ that takes as input the material properties $m$ and outputs cross section $\sigma_\nu$ is introduced, namely
\begin{equation}
\sigma_\nu = f(m).
\label{eq:f}
\end{equation}

In summary, we have now determined the three terms suggested by the Bouguer-Beer-Lambert law in the context of Gaussian Splatting. Therefore, we can compute $\alpha_i(x)$ in Eq.~\ref{eq:alpha_equation} differently as  
\begin{equation}
\begin{aligned}
\alpha_i(x) &= 1-e^{-n_i(x)\sigma_{\nu i}} \\  
            &= 1-e^{-o_iG_i(x)f(m_i)}, \\
\end{aligned}
\label{eq:alpha_new}
\end{equation}
where $o_i$ is the original opacity term for the $i$-th Gaussian, $G_i(x)$ is the normal distribution in Eq.~\ref{eq:alpha_equation}, $f$ is the neural network that produces cross section and $m_i$ is the material property.

\subsection{Gradient Flow}
\looseness=-1
Consider $c_i$ in Eq.~\ref{eq:alpha_blending}, in inverse rendering: it is computed using the material properties in Eq.~\ref{eq:outgoing_equation}. Therefore, our formulation of material takes gradients not only from the color term itself, but also from the $\alpha$ term. In this case,
\begin{equation}
    \frac{\partial L}{\partial m_i} = \frac{\partial c_i}{\partial m_i}\frac{\partial L}{\partial C}\alpha_i T_i + c_i\frac{\partial L}{\partial C}T_i\frac{\partial \alpha_i}{\partial m_i} - \sum_{j=i+1}^N c_j \frac{\partial L}{\partial C}T_j\alpha_j\frac{1}{1-\alpha_i}\frac{\partial \alpha_i}{\partial m_i},
\label{eq:derivative}
\end{equation}
\looseness=-1
where $L$ stands for loss. Our formulation augments the gradient for material learning, allowing the gradient to flow from opacity to serve as an additional constraint in a physics-inspired way, as illustrated by Fig.~\ref{fig:method}.


\subsection{Analysis}
\looseness=-1
Apart from the derivation of our formulation from the Bouguer-Beer-Lambert law, we found that our formulation could also be interpreted from two other perspectives, indicating the correctness of our approach.

\paragraph{NeRF.} The alpha-blending algorithm in NeRF~\citep{mildenhall2021nerf} takes the form
\begin{equation}
\begin{aligned}
C(r) &= \sum_{i=1}^N c_i(1-\exp(-\sigma_i\delta_i))T_i, \\
T_i &= \exp(-\sum_{j=1}^{i-1}\sigma_j\delta_j),    \\
\end{aligned}
\label{eq:nerf}
\end{equation}
in which $r$ stands for the ray direction, $c_i$ is a view-dependent color of the sampled points, $\sigma_i$ is volume density and $\delta_i$ is the distance between two sampled points. In this formulation, $\alpha_i = 1-\exp(-\sigma_i\delta_i)$, which takes exactly the same form as our derivation from the Bouguer-Beer-Lambert law. Neural field-based methods~\citep{bi2020neural,boss2021nerd} that use ray marching in a particle-based manner for inverse rendering handle cross section implicitly by using the same MLP for modeling volume density and material, suggesting the necessity of introducing cross section into the \textbf{disentangled} representation of 3DGS-based methods. 

\looseness=-1
\paragraph{Taylor Expansion.} Through Taylor expansion w.r.t.\ the function $\alpha(t) = 1-\exp(-t)$, we have 
\begin{equation}
\begin{aligned}
  \alpha(t) &= 1-e^{-t} \\
  &= 1 - (1 - t + o(t)) \\
  &= t + o(t),
\end{aligned}
\label{eq:taylor}
\end{equation}
\looseness=-1
where $o(t)$ is the little-o notation. This indicates that Eq.~\ref{eq:alpha_equation} is an approximation to the function $\alpha_i(x) = 1-\exp(-o_iG_i(x))$, which is the exact same form as Eq.~\ref{eq:alpha_new}, omitting the newly introduced cross section $f(m_i)$. This derivation indicates the mathematical correctness of the newly introduced activation function.


\section{Experiments}
\label{sec:experiment}
We discuss the implementation details to show the experimental design and present experimental results on synthetic and real-world data to provide a comprehensive understanding of our method.

\subsection{Implementation Details}
\looseness=-1
Since our proposed method is independent of the underlying model for inverse rendering, we can freely plug it into three state-of-the-art baselines, namely GaussianShader~\citep{jiang2024gaussianshader}, GS-IR~\citep{liang2024gs} and R3DG~\citep{gao2023relightable} for evaluation. All the experiments are conducted on a single NVIDIA RTX 6000 Ada Generation GPU and we report the reproduced baseline results for fair comparison. We use an MLP that has 2 hidden layers with size 128 for the hidden dimension and ReLU as activation function for cross section and the output activation is a sigmoid function. The input to the MLP is designed as follows: for GaussianShader, an end-to-end trained material modeling pipeline, we directly input the albedo, roughness and specular tint terms as in their original model for MLP input. For GS-IR and R3DG, since they follow a two-stage training pipeline, in which the first stage uses normal rendering for geometry reconstruction and second stage uses physically-based rendering for material modeling, we input the material properties as well as the SH coefficients used for color in the first stage to the MLP. The logic behind this design is to make the optimization of the MLP in the first stage meaningful. We follow the experimental settings as provided by the original baselines. GaussianShader is evaluated on Shiny Blender~\citep{verbin2022ref} and Glossy Synthetic~\citep{liu2023nero} as in the orignal work. For R3DG, Synthetic4Relight dataset~\citep{zhang2022modeling} is used for getting material modeling evaluation results. GS-IR is evaluated on MIP-NeRF 360 dataset~\citep{barron2022mip} to provide insights on real-world data. For albedo and relighting evaluation, we follow the standard practice to standardize the albedo prediction against the ground truth.

\begin{figure}[h]
\begin{center}
%\framebox[4.0in]{$\;$}
% \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
\includegraphics[width=1.0\textwidth]{figs/gamma_main.pdf}
\end{center}
\caption{Qualitative comparison of albedo, relighting and roughness on Synthetic4Relight dataset~\citep{zhang2022modeling}. Our method is able to decouple specular effects from albedo compared to R3DG, the baseline we implemented our method on. It is also capable of providing roughness estimation more precisely.\RebuttalRevision{We add a gamma correction to the relighting results to make the visual differences clearer.}}
\label{fig:qual_albedo}
\end{figure}
\begin{table}[t]
\centering
\caption{Quantitative results on Synthetic4Relight dataset~\citep{zhang2022modeling}. Our method is able to provide significant improvements universally. The improvements in novel view synthesis and relighting can be attribute to the improved estimation of albedo and roughness thanks to the correct modeling we use for constraining material properties.}
\label{tab:exp_syn4}
\resizebox{\linewidth}{!}{
\begin{tabular}{lccccccccccc}
\toprule
     & \multicolumn{3}{c}{\textbf{Novel View Synthesis}} & \multicolumn{3}{c}{\textbf{Relighting}} & \multicolumn{3}{c}{\textbf{Albedo}} & \textbf{Roughness} \\
     \cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}\cmidrule(lr){11-11}
                  & PSNR↑  & SSIM↑ & LPIPS↓ & PSNR↑    & SSIM↑    & LPIPS↓   & PSNR↑   & SSIM↑  & LPIPS↓  & MSE↓   \\ \midrule
R3DG    &   34.07  &  0.975   &   0.047     &  32.87       &    0.967      &    0.052 & 28.70   &    0.952    &    0.064     &    0.011   \\ 
R3DG + Ours    &   \textbf{34.47}  &  \textbf{0.977}   &   \textbf{0.046}     &  \textbf{33.21}       &    \textbf{0.969}      &    \textbf{0.050} & \textbf{29.31}   &    \textbf{0.954}    &    \textbf{0.059}     &    \textbf{0.007}   \\ 
\bottomrule
\end{tabular}
}
\end{table}

\subsection{Results on Synthetic Data}
\label{sec:mat}
\looseness=-1
\paragraph{Material Modeling.} We first evaluate our proposed module on top of R3DG~\citep{gao2023relightable} for material modeling evaluation. As can be seen from Tab.~\ref{tab:exp_syn4}, with the help of our modeling, R3DG is able to produce much better results universally. Our method outperforms the baseline on novel view synthesis by $0.4$\,db PSNR, as well as on two other metrics. The relighting results shares similar performance boosts as novel view synthesis, which could be attributed to the significant improvements in albedo ($\sim 0.6$\,db PSNR boost) and roughness estimation. Qualitatively speaking, as can be seen from the visualization in Fig.~\ref{fig:qual_albedo}, our method is capable of producing albedo estimation with less specular effects and roughness estimation that aligns better with the ground truth, resulting in pleasing relighting results.

\paragraph{Novel View Synthesis on Synthetic Data}
Other than the results on Synthetic4Relight, we also implemented our method on top of GaussianShader~\citep{jiang2024gaussianshader} to get more insights on how our model would perform on synthetic data. Tab.~\ref{tab:exp_shiny} showcases the result on Shiny Blender~\citep{verbin2022ref} and Glossy Synthetic~\citep{liu2023nero}. Our method is able to produce significant improvement in terms of novel view synthesis when built on top of GaussianShader (specifically $\sim 0.3$\,db PSNR boost on Shiny Blender and $\sim 0.4$\,db boost on Glossy Synthetic). Fig.~\ref{fig:qual_shiny} showcases some qualitative comparisons of our model with the GaussianShader baseline. While we did not add any constraint to how the model predicts normals, our method is able to produce much more satisfactory normal estimation compared to the baseline as well as better modeling of light reflection. 
\begin{figure}[h]
\begin{center}
\includegraphics[width=1.0\textwidth]{figs/viz_gs.pdf}
\end{center}
\caption{\textbf{Qualitative comparison on Shiny Blender~\citep{verbin2022ref} and Glossy Synthetic~\citep{liu2023nero} datasets.} Our method is able to provide satisfactory normal estimation compared to the baseline. Notice that our method does not include any additional supervision or constraints for normal estimation. Our method is also able to provide much more accurate view synthesis results when doing physically-based rendering. These results suggest the significance of introducing the physically correct model for inverse rendering.}
\label{fig:qual_shiny}
\end{figure}
\begin{table}[t]
\centering
\caption{Quantitative results when combined with GaussianShader~\citep{jiang2024gaussianshader}. We compare with GaussianShader on Shiny Blender~\citep{verbin2022ref} and Glossy Synthetic datasets~\citep{liu2023nero}. Our method is able to outperform GaussianShader on novel view synthesis in tems of all three standard image error metrics.}
\vspace{-2pt}
\label{tab:exp_shiny}
\resizebox{0.8\linewidth}{!}{
\begin{tabular}{lccccccc}
\toprule
     & \multicolumn{3}{c}{\textbf{Shiny Blender}} & \multicolumn{3}{c}{\textbf{Glossy Synthetic}}  \\
          \cmidrule(lr){2-4}\cmidrule(lr){5-7}        & PSNR↑  & SSIM↑ & LPIPS↓ & PSNR↑    & SSIM↑    & LPIPS↓   \\ \midrule
GaussianShader    &   30.77  &   0.953  &    0.081   &  27.25       &     0.925    &   0.083    \\
GaussianShader + Ours    &   \textbf{31.07}  &  \textbf{0.957}   &   \textbf{0.076}    &  \textbf{27.68}       &   \textbf{0.929}   &    \textbf{0.081}   \\
\bottomrule
\end{tabular}
}
\vspace{-2pt}
\end{table}

\subsection{Results on Real-World Data}
\looseness=-1
Apart from the results on synthetic data, we also analyze the effect of our method when applied to real-world data. To verify the plug-and-play attribute of our method, we apply it to GS-IR~\citep{liang2024gs} and conduct experiments on the MIP-NeRF 360 dataset~\citep{barron2022mip}. The experimental results shown in Tab.~\ref{tab:exp_real} indicate a significant universal improvement in terms of novel view synthesis thanks to the correct modeling and better estimation of materials as indicated by the results in Sec.~\ref{sec:mat}. Qualitatively speaking, our method is able to produce less blurry images compared to the GS-IR baseline as shown by the bicycle scene in the first row in Fig.~\ref{fig:qual_mip}. 

\looseness=-1
\paragraph{Analysis.} Since our method augments opacity with a material-dependent term, it turns the opacity from a purely geometry-indicator to a combined representative of geometry and material. That being said, the optimization of geometry is also affected by our method. This is obvious if we consider the gradient of $\alpha_i$ w.r.t.\ $o_i$:
\begin{equation}
\frac{\partial \alpha_i}{\partial o_i} = G_if(m_i)(1-\alpha_i),
    \label{eq:o_grad}
\end{equation}
pixel location $x$ is omitted for simplicity. We hypothesize that since MIP-NeRF 360 supports SfM initialization, it helps our method by easing the burden for geometry reconstruction, therefore leading to more improvements compared to synthetic data which uses random initialization. 

\begin{figure}[h]
\begin{center}
\includegraphics[width=1.0\textwidth]{figs/viz_gsir.pdf}
%\framebox[4.0in]{$\;$}
% \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
\end{center}
\caption{\textbf{Qualitative Comparison of GS-IR on MIP-NeRF 360~\citep{barron2022mip}.} Our method is able to provide much more accurate novel view synthesis results compared to the baseline GS-IR~\citep{liang2024gs} in which we plugged our model. The blurriness in the Bicycle scene in the first row predicted by GS-IR is overcome by our approach. The second row showcase that our approach is able to provide much more fine-grained and accurate normal estimation.}
\label{fig:qual_mip}
\end{figure}

\begin{table}
\begin{center}
\setlength\tabcolsep{2 pt}
\begin{tabular}{cl|c|ccccccccc}
\toprule
\multicolumn{2}{c|}{\textbf{Mip-NeRF 360}} & Avg. & Bicycle & Flowers & Garden & Stump & Treehill & Room & Counter & Kitchen & Bonsai\\ \midrule\midrule
\multirow{2}{*}{PSNR$\uparrow$} & GS-IR& 25.38 & 23.31 & \textbf{20.43} & 25.76 & 25.46 & 21.75 & 28.32 & 26.31 & 28.66 & 28.44\\
 & Ours &  \textbf{25.85} & \textbf{24.09} & 20.16 & \textbf{26.24} & \textbf{25.54} & \textbf{21.97} & \textbf{29.64} & \textbf{26.85} & \textbf{28.92}  &\textbf{29.27} \\ \midrule
\multirow{2}{*}{SSIM$\uparrow$} & GS-IR & 0.760 & 0.688 & \textbf{0.542} & 0.808 & 0.717 & 0.583 & 0.874 & 0.847 &0.877 & 0.901 \\
 & Ours & \textbf{0.767} & \textbf{0.720} & 0.510 & \textbf{0.824}  & \textbf{0.720} & \textbf{0.586} & \textbf{0.897} & \textbf{0.859} & \textbf{0.884} & \textbf{0.905}\\ \midrule
\multirow{2}{*}{LPIPS$\downarrow$}  & GS-IR & 0.265 & 0.269 & \textbf{0.368} & 0.154 & 0.256 & \textbf{0.367} &  0.281 &  0.255 & 0.179 & 0.254\\
 & Ours & \textbf{0.254} & \textbf{0.230} & 0.401 & \textbf{0.140} & \textbf{0.245} & 0.368 & \textbf{0.247} & \textbf{0.244} & \textbf{0.168} & \textbf{0.243} \\
 \bottomrule
\end{tabular}
\caption{\textbf{Quantatitive Comparison on Mip-NeRF 360~\citep{barron2022mip}.} The results show that our approach, when combined with GS-IR, can significantly improve the novel view synthesis results of real-world scenes on MIP-NeRF 360~\citep{barron2022mip}. Thanks to the constraint we add to opacity, our method is able to produce $\sim 0.5$\,db PSNR gains along with SSIM and LPIPS improvements.}
\label{tab:exp_real}
\vspace{-0.6cm}
\end{center}
\end{table}

\section{Conclusion}
\vspace{-10pt}
\looseness=-1
In this work, we propose OMG, a plug-and-play module for inverse rendering based on Gaussian Splatting. Inspired by the Bouguer-Beer-Lambert law, we derive the exact form of how opacity should be modeled in a particle-based model. We analyze the proposed formulation against NeRF-based methods and from a mathematical perspective, providing more insights in the right modeling for inverse rendering. Our method also reveals again the similarity between NeRF-based methods and 3D Gaussian Splatting. To validate the effectiveness and the plug-and-play nature of the proposed modeling, we implement it on top of three different state-of-the-art baselines and conduct comprehensive experiments on synthetic and real-world data. The significant improvements showcased by the experiments indicate the correctness of the modeling and point out the significance of the right prior and modeling for visual computing. We believe our work could inspire the community to pay more attention to developing the physically correct model and introducing right priors for modeling the 3D world.
\paragraph{Limitation.} In OMG, the dependency of cross section on the frequency of light is not included in the modeling. Future work could introduce the correct dependency of cross section on frequency of light to provide more accurate modeling of the physical world. \RebuttalRevision{As shown in Sec. \ref{sec:cross_viz} and Fig. \ref{fig:cross_viz}, the learned cross section is still not ideal if applied to the specular region with strong lighting. This motivates us to explore more on how to regularize the learning of materials and cross section in a more precise way.}






% \section*{Ethics Statement}
% Our work focuses on incorporating physically correct priors to improve inverse rendering results, aiming to bring new insights to visual computing community. Our research does not involve human subjects, sensitive data, or any practices that pose privacy or security concerns.

% \section*{Reproducibility Statement}
% The baselines used in our experiments, such as GaussianShader and GS-IR, are open-source and publicly available. We have detailed our experimental setup, and other design choices, in Section~\ref{sec:experiment} of the main paper  and Section~\ref{sec:hyper} in the Appendix to ensure reproducibility. Please also be assured that the code implementation of this work will be made publicly accessible upon the acceptance of this paper.  

\section*{Acknowledgement}
The author would like to thank Julia Du for the help of the title and the beautiful figures. This work has been funded in part by the Army Research Laboratory (ARL) award W911NF-23-2-0007, DARPA award FA8750-23-2-1015, and ONR award N00014-23-1-2840.

% \newpage
\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}


%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%
\clearpage
\appendix
\renewcommand{\thesection}{\Alph{section}}
\renewcommand\thefigure{\Alph{section}\arabic{figure}} 
\renewcommand\thetable{\Alph{section}\arabic{table}}  
\setcounter{section}{0}
\setcounter{figure}{0} 
\setcounter{table}{0} 

{\LARGE\sc \textbf{O{\Huge M}G}: \underline{O}pacity \underline{M}atters in \underline{M}aterial \underline{M}odeling with \underline{G}aussian splatting\par}

{\LARGE\sc Appendix\par} \vspace{10pt}
\section{Hyperparameters}
\label{sec:hyper}
The neural network introduced uses $0.001$, $0.0001$, $0.0005$ and $0.007$ as learning rate respectively for Synthetic4Relight, Shiny Blender, Glossy Synthetic and MIP-NeRF 360 datasets.\RebuttalRevision{The neural network is implemented as a fully-connected MLP with two hidden layers, 128 as hidden dimension and ReLU as activation function. We directly feed the material properties into the MLP without any encoding such as positional embedding.}
\section{Full Derivation of Equations}
Here we give the full derivation of Eq.~\ref{eq:derivative}.
\begin{equation}
\begin{aligned}
    % \frac{\partial L}{\partial m_i} &= \frac{\partial L}{\partial C}\frac{\partial C}{\partial m_i} = \frac{\partial L}{\partial C}\sum_{j=1}^N \frac{\partial (c_j\alpha_jT_j)}{\partial m_i}\\
    % &= \frac{\partial L}{\partial C}[\frac{\partial c_i}{\partial m_i}\alpha_i T_i]\\
    % &= \frac{\partial c_i}{\partial m_i}\frac{\partial L}{\partial C}\alpha_i T_i + c_i\frac{\partial L}{\partial C}T_i\frac{\partial \alpha_i}{\partial m_i} - \sum_{j=i+1}^N c_j \frac{\partial L}{\partial C}T_j\alpha_j\frac{1}{1-\alpha_i}\frac{\partial \alpha_i}{\partial m_i},
    \frac{\partial L}{\partial m_i} &= \frac{\partial L}{\partial C}\frac{\partial C}{\partial m_i} = \frac{\partial L}{\partial C}\sum_{j=1}^N \frac{\partial (c_j\alpha_jT_j)}{\partial m_i}\\
    &= \frac{\partial L}{\partial C}\sum_{j=1}^N[\frac{c_j}{m_i}\alpha_jT_j + c_j\frac{\partial \alpha_j}{\partial m_i}T_j + c_j\alpha_j\frac{\partial T_j}{\partial m_i}]\\
    &= \frac{\partial L}{\partial C}[\frac{\partial c_i}{\partial m_i}\alpha_iT_i + c_iT_i\frac{\partial \alpha_i}{\partial m_i} + \sum_{j=i+1}^{N}c_j\alpha_j\frac{\partial}{\partial m_i}\prod_{k=1}^{j-1}(1-\alpha_k)]\\
    &= \frac{\partial L}{\partial C}[\frac{\partial c_i}{\partial m_i}\alpha_iT_i + c_iT_i\frac{\partial \alpha_i}{\partial m_i} + \sum_{j=i+1}^{N}c_j\alpha_j\frac{\partial}{\partial m_i}\exp(\sum_{k=1}^{j-1}(1-\alpha_k))]\\
    &= \frac{\partial c_i}{\partial m_i}\frac{\partial L}{\partial C}\alpha_i T_i + c_i\frac{\partial L}{\partial C}T_i\frac{\partial \alpha_i}{\partial m_i} - \sum_{j=i+1}^N c_j \frac{\partial L}{\partial C}T_j\alpha_j\frac{1}{1-\alpha_i}\frac{\partial \alpha_i}{\partial m_i},
\end{aligned}
\label{eq:derivative_supp}
\end{equation}

\section{Additional Results}
We present the full results on Synthetic4Relight~\citep{zhang2022modeling}, Shiny Blender~\citep{verbin2022ref} and Glossy Synthetic~\citep{liu2023nero}. All the baseline results are reproduced by us as in the main paper. More visualization results can be found in Fig.~\ref{fig:qual_shiny_supp}, Fig.~\ref{fig:qual_mip_supp}, Fig.~\ref{fig:qual_albedo_supp}, Fig.~\ref{fig:qual_albedo_supp_2} and Fig.~\ref{fig:qual_normal_supp}.


\begin{table*}[h]
\centering
\setlength{\tabcolsep}{2pt} 
\resizebox{\textwidth}{!}{
\begin{tabular}{lc|ccc|ccc|ccc|c}
\toprule
\multirow{2}{*}[-0.7ex]{Scene} &
\multirow{2}{*}[-0.7ex]{Method} &
\multicolumn{3}{c|}{Novel View Synthesis} &
\multicolumn{3}{c|}{Albedo} &
\multicolumn{3}{c|}{Relight} &
{Roughness}\\
\cmidrule{3-5}\cmidrule{6-8}\cmidrule{9-11}\cmidrule{12-12}
& &
PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ &
PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ &
PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ & MAE $\downarrow$ \\

\midrule\midrule
\multirow{2}{*}{Balloons}
& R3DG   & 31.90  &
0.967 & \textbf{0.072} & 24.82 &
0.923 & 0.070 & 31.38 &
0.965 & 0.078 & \textbf{0.0009}  \\
& Ours      & \textbf{32.11}  &
\textbf{0.968} & \textbf{0.072} & \textbf{25.84} &
\textbf{0.926} & \textbf{0.055} & \textbf{32.50} &
\textbf{0.967} & \textbf{0.071} & 0.0011 \\
\midrule
\multirow{2}{*}{Hotdog}
& R3DG   & 32.61 &
0.972 & 0.050 & 26.53 &
0.956 & 0.089 & \textbf{29.04} &
0.954 & 0.067 & 0.0357 \\
& Ours      & \textbf{33.59} &
\textbf{0.975} & \textbf{0.047} & \textbf{26.95} &
\textbf{0.957} & \textbf{0.085} & 28.73 &
\textbf{0.955} & \textbf{0.066} & \textbf{0.0217} \\
\midrule
\multirow{2}{*}{Chair}
& R3DG   & 34.72 &
0.978 & \textbf{0.039} & 30.55 &
0.963 & 0.046 & 33.07 &
0.966 & \textbf{0.041} & \textbf{0.0036} \\
& Ours      & \textbf{35.00} &
\textbf{0.979} & 0.040 & \textbf{30.82} &
\textbf{0.964} & \textbf{0.045} & \textbf{33.46} &
\textbf{0.968} & \textbf{0.041} & 0.0039 \\
\midrule
\multirow{2}{*}{Jugs}
& R3DG   & 37.03 &
\textbf{0.984} & \textbf{0.027} & 32.88 &
0.967 & 0.064 & 37.98 &
\textbf{0.984} & \textbf{0.023} & \textbf{0.0032} \\
& Ours      & \textbf{37.17} &
\textbf{0.984} & \textbf{0.027} & \textbf{33.64} &
\textbf{0.968} & \textbf{0.049} & \textbf{38.16} &
\textbf{0.984} & \textbf{0.023} & 0.0074 \\
\bottomrule
\end{tabular}
}
\caption{Per-scene results on Synthetic4Relight dataset. For albedo reconstruction results, we follow the standard practice \citep{zhang2021nerfactor, gao2023relightable} and scale each RGB channel by a global scalar.}
\label{tab:full_syn}
\end{table*}

\begin{table*}
\begin{center}
% \setlength\tabcolsep{2 pt}
\resizebox{\textwidth}{!}{
\begin{tabular}{cl|c|cccccc}
\toprule
\multicolumn{2}{c|}{\textbf{Shiny Blender}} & Avg. & Ball & Car & Coffee & Helmet & Teapot & Toaster\\ \midrule\midrule
\multirow{2}{*}{PSNR$\uparrow$} & GaussianShader & 30.77 & 29.05 & \textbf{28.51} & \textbf{31.55} & \textbf{28.91} & \textbf{43.57} & 23.05 \\
 & Ours & \textbf{31.07} & \textbf{29.65} & 28.14 & 31.29 & 28.79 & 43.48 & \textbf{25.07} \\ \midrule
\multirow{2}{*}{SSIM$\uparrow$} & GaussianShader & 0.953 & 0.955 & \textbf{0.940} & \textbf{0.970} & \textbf{0.956} & \textbf{0.996} & 0.902 \\
 & Ours & \textbf{0.957} & \textbf{0.961} & 0.939 & 0.969 & 0.955 & \textbf{0.996} & \textbf{0.924}\\ \midrule
\multirow{2}{*}{LPIPS$\downarrow$}  & GaussianShader & 0.081 & 0.146 & \textbf{0.047} & \textbf{0.085} & 0.086 & \textbf{0.011} &  0.110\\
 & Ours & \textbf{0.076} & \textbf{0.137} & 0.050 & 0.086 & \textbf{0.085} & \textbf{0.011} & \textbf{0.089} \\
 \bottomrule
\end{tabular}
}
\caption{Per-scene results on Shiny Blender dataset.}
\label{tab:full_shiny}
\vspace{-0.6cm}
\end{center}
\end{table*}



\begin{table*}
\begin{center}
\setlength\tabcolsep{2 pt}
\begin{tabular}{cl|c|cccccccc}
\toprule
\multicolumn{2}{c|}{\textbf{Glossy Synthetic}} & Avg. & Angel & Bell & Cat & Horse & Luyu & Potion & Tbell & Teapot\\ \midrule\midrule
\multirow{2}{*}{PSNR$\uparrow$} & GaussianShader & 27.25 & 26.92 & 29.73 & \textbf{31.43} & 26.38 & {27.31} & \textbf{29.54} & 23.41 & 23.25\\
 & Ours &  \textbf{27.68} & \textbf{28.24} & \textbf{29.91} & 31.00 & \textbf{26.62} & \textbf{27.32} & 29.40 & \textbf{24.67} & \textbf{24.31} \\ \midrule
\multirow{2}{*}{SSIM$\uparrow$} & GaussianShader & 0.925 & 0.923 & 0.939 & \textbf{0.961} & \textbf{0.934} & \textbf{0.917} & \textbf{0.936} & 0.897 & 0.895 \\
 & Ours & \textbf{0.929} & \textbf{0.929} & \textbf{0.940} & 0.959  & \textbf{0.934} & 0.916 & 0.935 & \textbf{0.907} & \textbf{0.909}\\ \midrule
\multirow{2}{*}{LPIPS$\downarrow$}  & GaussianShader & 0.083 & 0.071 & 0.085 & \textbf{0.058} & \textbf{0.056} & \textbf{0.066} &  \textbf{0.096} &  0.136 & 0.098\\
 & Ours & \textbf{0.081} & \textbf{0.066} & \textbf{0.084} & 0.059 & 0.057 & 0.067 & 0.097 & \textbf{0.125} & \textbf{0.091}\\
 \bottomrule
\end{tabular}
\caption{Per-scene results on Glossy Synthetic dataset.}
\label{tab:full_glossy}
\vspace{-0.6cm}
\end{center}
\end{table*}


\begin{figure}[h]
\begin{center}
\includegraphics[width=1.0\textwidth]{figs/viz_supp_gs.pdf}
%\framebox[4.0in]{$\;$}
% \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
\end{center}
\caption{More qualitative example in comparison to GaussianShader on Glossy Synthetic dataset.}
\label{fig:qual_shiny_supp}
\end{figure}

\begin{figure}[h]
\begin{center}
\includegraphics[width=1.0\textwidth]{figs/viz_supp_mip.pdf}
%\framebox[4.0in]{$\;$}
% \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
\end{center}
\caption{More qualitative example in comparison to GS-IR on MIP-NeRF 360 dataset.}
\label{fig:qual_mip_supp}
\end{figure}

\begin{figure}[h]
\begin{center}
%\framebox[4.0in]{$\;$}
% \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
\includegraphics[width=1.0\textwidth]{figs/viz_supp_alb.pdf}
\end{center}
\caption{More qualitative example in comparison to R3DG on Synthetic4Relight dataset.}
\label{fig:qual_albedo_supp}
\end{figure}

\begin{figure}[h]
\begin{center}
%\framebox[4.0in]{$\;$}
% \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
\includegraphics[width=1.0\textwidth]{figs/gamma_supp.pdf}
\end{center}
\caption{\RebuttalRevision{More qualitative example in comparison to R3DG on Synthetic4Relight dataset. We add a gamma correction to the relighting results to make the visual differences clearer.}}
\label{fig:qual_albedo_supp_2}
\end{figure}

\begin{figure}[h]
\begin{center}
\includegraphics[width=1.0\textwidth]{figs/supp_normal.pdf}
%\framebox[4.0in]{$\;$}
% \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
\end{center}
\caption{\RebuttalRevision{More qualitative example in comparison to GaussianShader on Glossy Synthetic and Shiny Blender datasets.}}
\label{fig:qual_normal_supp}
\end{figure}

\RebuttalRevision{
\section{Visualization of Cross Section}
\label{sec:cross_viz}
To understand what the cross section network is learning, we visualize some examples in Fig. \ref{fig:cross_viz}. In the figure, the darker region indicates a lower cross section, meaning that it lets more light pass through whereas the brighter regions stands for the regions that have a higher cross section. The example shows that the windows as well as the plastic bag on the floor actually have a lower cross section assigned to them, which corresponds to human intuition.
}

\begin{figure}[h]
\begin{center}
\includegraphics[width=1.0\textwidth]{figs/viz_supp_cross_mor.pdf}
%\framebox[4.0in]{$\;$}
% \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
\end{center}
\caption{\RebuttalRevision{Visulization of the learned cross section on Mip-NeRF 360 dataset. Dark indicates the lower cross section (meaning it lets more light pass through) and bright stands for the higher cross section. The example shows that the windows as well as the plastic bag on the floor actually have a lower cross section assigned to them, which corresponds to human intuition.}}
\label{fig:cross_viz}
\end{figure}

\end{document}