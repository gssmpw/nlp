\documentclass[11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{amsmath}
\usepackage{amssymb} 
\usepackage{amsthm}
\usepackage{amsfonts, mathtools}
\usepackage{algorithm,algpseudocode}
\usepackage{bm}
\usepackage{comment}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{multirow, booktabs}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[colorlinks,linkcolor=blue,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage{ulem}
\usepackage{verbatim}
\usepackage{framed}

\allowdisplaybreaks[4]

\newtheorem{fact}{Fact}
\newtheorem{remark}{Remark}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{problem}{Problem}
\newtheorem{definition}{Definition}
\newtheorem{conjecture}{Conjecture}
\newtheorem{claim}{Claim}
\newtheorem{lemma}{Lemma}

\newcommand{\eq}[1]{\hyperref[eq:#1]{(\ref*{eq:#1})}}
\renewcommand{\sec}[1]{\hyperref[sec:#1]
    {Section~\ref*{sec:#1}}}
\newcommand{\thm}[1]{\hyperref[thm:#1]
    {Theorem~\ref*{thm:#1}}}
\newcommand{\lem}[1]{\hyperref[lem:#1]{Lemma~\ref*{lem:#1}}}
\newcommand{\clm}[1]{\hyperref[claim:#1]{Claim~\ref*{claim:#1}}}

\newcommand{\prop}[1]{\hyperref[prop:#1]
    {Proposition~\ref*{prop:#1}}}
\newcommand{\prob}[1]{\hyperref[prob:#1]
    {Problem~\ref*{prob:#1}}}
\newcommand{\cor}[1]{\hyperref[cor:#1]
    {Corollary~\ref*{cor:#1}}}
\newcommand{\fig}[1]{\hyperref[fig:#1]{Figure~\ref*{fig:#1}}}
\newcommand{\tab}[1]{\hyperref[tab:#1]{Table~\ref*{tab:#1}}}
\newcommand{\alg}[1]{\hyperref[alg:#1]
    {Algorithm~\ref*{alg:#1}}}
\newcommand{\app}[1]{\hyperref[app:#1]
    {Appendix~\ref*{app:#1}}}
\newcommand{\conj}[1]{\hyperref[conj:#1]
    {Conjecture~\ref*{conj:#1}}}
\newcommand{\fac}[1]{\hyperref[fac:#1]
    {Fact~\ref*{fac:#1}}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
    
\DeclareMathOperator{\ADV}{ADV}
\DeclareMathOperator{\BMPV}{BMPV}
\DeclareMathOperator{\VD}{VD}
\DeclareMathOperator{\ED}{ED}
\DeclareMathOperator{\FIND}{FIND}
\DeclareMathOperator{\Inf}{Inf}

\newcommand{\ket}[1]{\left|{#1}\right\rangle}
\newcommand{\bra}[1]{\langle #1 |}
\newcommand{\pcite}[1]{\!\!\cite{#1}}

\newcommand{\sens}{\mathrm{s}}
\newcommand{\bs}{\mathrm{bs}}
\newcommand{\fbs}{\mathrm{fbs}}
\newcommand{\fc}{\mathrm{FC}}
\newcommand{\cer}{\mathrm{C}}

\bibliographystyle{plain}

\title{Binary-Integer-Programming Based Algorithm for Expert Load Balancing in Mixture-of-Experts Models}
\date{}

\author{Yuan Sun\thanks{%University of Science and Technology of China. 
\texttt{xyxxwysy@mail.ustc.edu.cn}}
}
\begin{document}
	\maketitle
	
	\begin{abstract}

        For pre-training of MoE (Mixture-of-Experts) models, one of the main issues is unbalanced expert loads, which may cause routing collapse or increased computational overhead. Existing methods contain the Loss-Controlled method and the Loss-Free method, where both the unbalanced degrees at first several training steps are still high and decrease slowly. In this work, we propose \textbf{BIP-Based Balancing}, an expert load balancing algorithm based on binary integer programming (BIP). The algorithm  maintains an additional vector $\bm q$ on each MoE layer that can help change the top-K order of $\bm s$ by solving a binary integer programming with very small time costs. We implement the algorithm on two MoE language models: 16-expert (0.3B) and 64-expert (1.1B). The experimental results show that on both models comparing with the Loss-Controlled method and the Loss-Free method, our algorithm trains models with the lowest perplexities, while saves at least $13\%$ of pre-training time compared with the Loss-Controlled method. Within our current knowledge, this is the first routing algorithm that achieves maintaining load balance status on every expert in every MoE layer from the first step to the last step during the whole pre-training process, while the trained MoE models also perform well. The code material of this work is available at \url{https://github.com/sunyuanLLM/bip_routing_algorithm}.
	    
	\end{abstract}
	
	\section{Introduction}

        MoE (Mixture-of-Experts) architectures allow LLMs (Large Language Models) become sparsity so that they can have both large scale of parameters and much small resource costs.  
        However, unbalanced expert loads always happen in MoE LLM pre-training, especially when the number of experts is large. This problem will lead to computation bottlenecks \cite{2021GShard, Fedus2021SwitchTS} or routing collapse \cite{shazeer2017outrageouslylargeneuralnetworks}. Furthermore, the worst situation is that a MoE model finally degenerates to a dense model, but with fewer activated parameters.

        In order to balance the expert loads, many methods are proposed. One is using an auxiliary loss to encourage balanced
        expert load \cite{2021GShard,Fedus2021SwitchTS}. A disadvantage is that it will introduces undesired gradients that conflict with the language modeling objective, which will influence model performance. The other way is auxiliary-loss-free load balancing strategy \cite{wang2024auxiliarylossfreeloadbalancingstrategy}, where authors add an additional bias vector on routing scores to change their sort order, instead of computing auxiliary loss. Neither of the two methods can guarantee expert load balancing in the first several steps of the pre-training process, and it may cost thousands of training steps to change expert loads into balanced status  \cite{wang2024auxiliarylossfreeloadbalancingstrategy}.
        
        In this paper, we propose \textbf{BIP-Based Balancing}, an expert load balancing algorithm based on binary integer programming (BIP). The algorithm can be set within each routing layer, which maintains an additional vector $\bm q$ that can help change the top-K order of $\bm s$. The key point is that we update values of $\bm q$ by solving a specific form of binary integer programmings on each routing gate with very small time costs. For evaluation experiments, we implement the algorithm on two MoE language models based on \textit{Minimind} model series \cite{minimind}: 16-expert (0.3B) and 64-expert (1.1B). The experimental results show that on both models comparing with the Loss-Controlled method and the Loss-Free method, our algorithm train models with lower perplexities, while save at least $13\%$ of pre-training time. Within our current knowledge, this is the first routing algorithm that achieves keeping load balance status on every expert and every MoE layer from the first step to the last step during the whole pre-training process, while the trained MoE models also perform well.

        \section{Preliminary}

        \textbf{Mixture-of-Expert Layers in LLMs. }In MoE layers, for each token experts are selected by a Top-K routing. Let $\mathbf{u}_i$ denote the input of the $i$-th token to an $m$-expert MoE layer, then output $\mathbf{h}_i$ can be calculated as follows:
            \begin{equation}
                \begin{aligned}
                    & \mathbf{h}_i=\mathbf{u}_i+\sum_{j=1}^{m} g_{ij} \operatorname{FFN}_j\left(\mathbf{u}_i\right), \\
& g_{ij}= \begin{cases}s_{ij}, & s_{ij} \in \operatorname{Topk}\left(\left\{s_{it} \mid 1 \leq t \leq m\right\}, k\right), \\
0, & \text { otherwise, }\end{cases} \\
& s_{ij}=G\left(\mathbf{u}_i{ }^T \mathbf{e}_j\right).\nonumber
\end{aligned} 
\end{equation}
Here $G$ is a nonlinear gating function and $ \mathbf{e}_j$ is the centroid of the $j$-th expert.

        \textbf{Load Balancing Strategy with Auxiliary Loss (Loss-Controlled Method). } Auxiliary-loss  methods have been used for control load balance\cite{2021GShard,Fedus2021SwitchTS}. 
        For a sequence with length $n$, its auxiliary loss is calculated by
        \begin{equation}
            \begin{aligned}
            \mathcal{L}_{\text {Balance }} & =\alpha \sum_{j=1}^{m} f_j P_j, \\
            f_j & =\frac{m}{k n} \sum_{i=1}^n \delta_{ij}, \\
                P_j & =\frac{1}{n} \sum_{i=1}^n s_{ij},\nonumber
            \end{aligned} 
        \end{equation}

        where $m$ is the total number of experts, $k$ is the number of experts selected for each token, $s_{ij}$ is the routing score of Expert $j$ for Token $i$, $f_j$ represents the fraction of tokens routed to Expert $j$. $\delta_{ij}$ is 0 or 1 representing whether Token $i$ is for Expert $j$. $P_j$ denotes the average gating scores of Expert $j$, and $\alpha$ is a hyper-parameter controlling the strength of the auxiliary loss.

        \textbf{Auxiliary-Loss-Free Load Balancing Strategy.} The other way to expert load balance is auxiliary-loss-free load balancing strategy \cite{wang2024auxiliarylossfreeloadbalancingstrategy}, which first appears in DeepSeek-V3 \cite{deepseekai2024deepseekv3technicalreport}. Instead of computing loss functions, the authors introduce a bias vector $\bm{b}$ on expert lists so that it can influence the determination of the top-K routing as follows:

        \begin{align}
            g^{\prime}_{ij} & = \begin{cases} 
                s_{ij}, & s_{ij} + b_j \in \operatorname{Topk} (\{ s_{it} + b_t | 1 \leq t \leq m \}, k), \\
                0, & \text{otherwise}.
            \end{cases} \nonumber
        \end{align}

        \section{Algorithm}

        The main result of this work is \textbf{BIP-Based Balancing} algorithm, whose details are described in \alg{SOA}. Like Loss-Free strategy, BIP-Based Balancing algorithm also does not need to compute auxiliary-loss, while there is also an additional vector $\bm q$ that can help change the top-K order of $\bm s$. The main difference is that the value of $\bm q$ is computed by solving a binary integer programming, and we update values of $\bm q$ after each calculation of a routing gate, instead of after each batch. Without loss of generality, all vectors mentioned in algorithms are row vectors. 

        \begin{algorithm}[ht]
            \caption{BIP-Based Balancing Algorithm for MoE Models}
            \label{alg:SOA}
            \begin{algorithmic}[1]
                \State Input: MoE model $\bm\theta$, expert number $m$, topk number $k$ and a small constant $T$.
                \State Initialize $q_{lj} = 0$ for each expert $j$ in every MoE layer $l$;
                \For {a batch $\{\bm{x},\bm{y}\}$ in the batch enumeration}
                \State Set $n$ be the token number of $\{\bm{x},\bm{y}\}$;
                \For {each MoE layer $l$}
                \State Compute the routing score matrix $\bm{s} \in \R^{n\times m}$ on the batch data $\{\bm{x},\bm{y}\}$ and all experts;
                \For {$t=1,...,T$}
                    \State Set $\bm{P} = \bm{s} - \bm{1}_n^T\bm q_l\in \R^{n\times m}$;
                    \State Set $p_i = \max(0, (k+1)\hspace{2pt}\mathrm{-th}\hspace{2pt} \mathrm{largest \hspace{2pt} value} \hspace{2pt} \mathrm{of}\hspace{2pt}\bm P_i ), 1\le i\le n$ for $\bm p\in \R^{n}$;
                    \State Set $\bm{Q} = \bm{s}^T - \bm{1}_m^T\bm p\in \R^{m\times n}$;
                    \State Set $q_{lj} = \max(0, (nk/m+1)\hspace{2pt}\mathrm{-th}\hspace{2pt} \mathrm{largest \hspace{2pt} value} \hspace{2pt} \mathrm{of}\hspace{2pt}\bm Q_j),1\le j\le m$;
                \EndFor
                \State For every token $i$ and expert $j$, set 
                $$g_{ij}  = \begin{cases} 
                s_{ij}, & s_{ij} - q_{lj} \in \operatorname{Topk} (\{ s_{it} - q_{lt} | 1 \leq t \leq m \}, k), \\
                0, & \text{otherwise}.
                \end{cases}$$
                \State Continue pre-training process on $\bm\theta$ with the expert decision matrix $\bm{g}$;
                
                \EndFor
                \EndFor
                \State Output: trained model $\bm\theta$.
            \end{algorithmic}
        \end{algorithm}

        To explain why \alg{SOA} works, we first model the expert load balancing problem to the following \textit{binary integer programming (\ref{eqn:BIP})} problem:

        \begin{align}
            \tag{BIP}  \max \ \ & \sum_{i=1}^n\sum_{j=1}^m s_{ij}x_{ij} \label{eqn:BIP}  \\
            \text{s.t. }\ & \sum_{j=1}^m x_{ij} \le k, \forall i\in[n]   \\ 
            \ & \sum_{i=1}^m x_{ij} \le\frac{kn}{m}, \forall j\in[m]   \\ 
                & x_{ij} \in\{0,1\},\forall i \in[n],\forall j\in[m] \nonumber. 
        \end{align}

        Here, $n$ is the number of tokens in one batch, $m$ is the number of experts and $k$ is the number of experts selected by each routing decision. $\bm{s}$ is the routing score matrix. The binary decision variables $x_{ij}$ determine whether matching token $i$ with expert $j$. Condition (1) holds since we can only match one token with $k$ experts. Condition (2) ensures the load balance of experts.

        in order to solve (\ref{eqn:BIP}), consider its linear programming relaxation version:

        \begin{align}
            \tag{P-LP}  \max \ \ & \sum_{i=1}^n\sum_{j=1}^m s_{ij}x_{ij} \label{eqn:P-LP}  \\
            \text{s.t. }\ & \sum_{j=1}^m x_{ij} \le k, \forall i\in[n] \nonumber  \\ 
            \ & \sum_{i=1}^m x_{ij} \le \frac{kn}{m}, \forall j\in[m] \nonumber  \\ 
                & \bm{0}\leq\bm{x}\leq\bm{1}. \nonumber
        \end{align}

        The dual problem of (\ref{eqn:P-LP}) is:

        \begin{align}
            \tag{D-LP}  \min \ \ & k\sum_{i=1}^n p_i + \frac{kn}{m}\sum_{j=1}^m q_j + \sum_{i=1}^n\sum_{j=1}^m r_{ij} \label{eqn:D-LP}  \\
            \text{s.t. }\ &  p_i + q_j + r_{ij} \ge s_{ij}, \forall i\in[n],\forall j\in[m] \nonumber  \\ 
            & \bm{p} \ge \bm{0}, \bm{q}\ge \bm{0}, \bm{r} \ge \bm{0},\nonumber
        \end{align}
        where the decision variables are $\bm{p}\in \R^n$,  $\bm{q} \in \R^m$ and $\bm{r} \in \R^{n\times m}.$

        By primal-dual principle, we have the inequality (\ref{eqn:BIP})$\le$(\ref{eqn:P-LP})$=$(\ref{eqn:D-LP}). In fact, the optimal solution $\bm{x^*}$ of (\ref{eqn:P-LP}) has the following relationship between the optimal solution $\bm{p}^*,\bm{q}^*$ of (\ref{eqn:D-LP}):

        $$x^*_{ij}=1\hspace{2pt}\mathrm{if}\hspace{2pt}\mathrm{and}\hspace{2pt}\mathrm{only}\hspace{2pt}\mathrm{if}\hspace{2pt}p^*_i+q^*_j < s_{ij}. $$

        This result matches the line 13 in \alg{SOA}, since we can change the inequality $p^*_i+q^*_j < s_{ij}$ to the form $s_{ij} - q^*_j > p^*_i$. Thus when $i$ is fixed, there are exact $m-k$ subscripts $j$ satisfying $s_{ij} - q^*_j <= p^*_i$ while the other $k$ subscripts $j$ satisfy $s_{ij} - q^*_j > p^*_i$, which exactly match the subscripts of $\operatorname{Topk} (\{ s^*_{ij} - q^*_{j} | 1 \leq j \leq m \}, k)$.
        
        On solving (\ref{eqn:D-LP}), we use the standard \textbf{ADMM} algorithm \cite{2010Distributed}:

        \begin{algorithm}[ht]
            \caption{ADMM algorithm for (\ref{eqn:D-LP})}
            \label{alg:admm}
            \begin{algorithmic}[1]
                \For {$t=1,...,T$}
                    \State Set $\bm p_t = {\rm argmax}_{\bm p}L_{\lambda}(\bm p,\bm q_{t-1},\bm r_{t-1},\bm u_{t-1})$;
                    \State Set $\bm q_t = {\rm argmax}_{\bm q}L_{\lambda}(\bm p_t,\bm q,\bm r_{t-1},\bm u_{t-1})$;
                    \State Set $\bm r_t = {\rm argmax}_{\bm r}L_{\lambda}(\bm p_t,\bm q_t,\bm r,\bm u_{t-1})$;
                    \State Update $\bm u_t$ with the step parameter $\lambda$;
                \EndFor
                \end{algorithmic}
        \end{algorithm}
        
        Here $L_\lambda(\bm p,\bm q,\bm r,\bm u)$ is the augmented Lagrangian function of (\ref{eqn:D-LP}) and $\bm u$ is the dual vector variable in $L$. In order to implement optimizations, first notice that when $\bm p,\bm q$ are fixed, the optimal values of $\bm r$ and $\bm u$ are $r^*_{ij}=\max (s_{ij}-p_i-q_j,0)$ and $\bm u^* = \bm 0$. Then it is easy to verify that the line 2 and line 3 in \alg{admm} imply the line 7 to line 12 part in \alg{SOA}, by noticing that when $\bm q$ and $i$ are fixed, in order to keep exact $k$ of $\{x_{ij}\}_{1\le j \le m}$ satisfying $x_{ij}>0$, we only need to keep exact $k$ of inequalities $\{p_i+q_j < s_{ij}\}_{1\le j\le m}$ hold. That is, the best choice of $p_i$ is the $(k+1)$-th largest value of $\{s_{ij}-q_j\}_{1\le j\le m}$. The analysis of optimizing $\bm q$ when $\bm p$ is fixed is similar.
    
        \section{Experiments}

        \subsection{Experimental Settings}

        \textbf{Model Architectures, Training Settings and Hardware.} The models we choose in the experiments are from \textit{Minimind}, a popular extremely-lightweight language model series \cite{minimind}. We train 2 models on its MoE version during the experiments, one is with 16 experts and the other is with 64 experts. For the MoE version of \textit{Minimind}, the number of parameters in each expert is less than 20M, and the core function of MoE gates are $softmax$. The datasets are also from \cite{minimind}, where we split the pre-training dataset into a training set and a test set. More information of models, settings and GPUs is listed in \tab{model}.

        \begin{table}[H]
            \begin{center}
            \begin{tabular}{|c|c|c|}
                \hline Hyper parameters&16-expert model&64-expert model\\
                \hline Vocab size&6400&6400\\
                \hline Max Sequence Length&8192&8192\\
                \hline Attention heads&8&8\\
                \hline routing score function&$softmax$&$softmax$\\
                \hline MoE layers&8&8\\
                \hline Routed experts&4&8\\
                \hline Activated routed experts&16&64\\
                \hline Model size&0.3B&1.1B\\
                \hline GPUs&RTX4090 $\times 1$&L20 $\times 1$\\
                \hline
            \end{tabular}\\
            \end{center}
            \caption{Model architectures, training settings and hardware.} 
            \label{tab:model}
        \end{table}

        \textbf{Baseline.} We compare our BIP-Balancing algorithm with both Loss-Controlled and Loss-Free strategies, especially in Loss-Controlled method since there are discussions show that on the  $softmax$ function the Loss-Controlled method works better \cite{kexuefm-10757}. For the baseline, we set $\alpha=0.1$ for the Loss-Controlled method which is the same value in the original \textit{Minimind} model, and set $u=0.001$ for the Loss-Free method which is supported in \cite{wang2024auxiliarylossfreeloadbalancingstrategy}.

        \textbf{Measurements.} We introduce two measurements, \textit{Average Maximum Violation} ($AvgMaxVio$) and \textit{Supremum Maximum Violation} ($SupMaxVio$), to measure the balance degree of experts among the whole pre-training process. $AvgMaxVio$ is the average value of $MaxVio_{batch_i}$ among all training batches:
        $$AvgMaxVio=\frac{\sum_{batch_i\in Batches} MaxVio_{batch_i}}{|Batches|}, $$ 
        and $SupMaxVio$ is the maximum value of $MaxVio_{batch_i}$ among all training batches:
        $$SupMaxVio=\max_{batch_i\in Batches} \{MaxVio_{batch_i}\}.$$ 
        Here 
        $$MaxVio_{batch_i}=\frac{\max_j Load_{ij}}{\overline{Load}}-1,$$ 
        where $Load_{ij}$ represents the number of tokens matched to the $j$-th expert during the $i$-th batch pre-training, and $\overline{Load}$ denotes the average number of tokens per expert in one batch. ($MaxVio_{batch}$ is first introduced in \cite{wang2024auxiliarylossfreeloadbalancingstrategy}.) The less $AvgMaxVio$ is, the faster expert loads turn into balance states, which will lead to smaller time costs of LLM training and higher computing resource utilization. On the other hand, when $SupMaxVio$ is small (for example, less than 0.2), then global training process can be seen as a balanced status approximately. Moreover, we will also show $AvgMaxVio$ of each layer in the models in \app{tag}.

        Besides, we also use \textit{Perplexity} to measure the performances of pre-trained models, and \textit{Training time} to measure the efficiency of global training processes.

        \subsection{Main Results}\label{sec:result}

        \tab{16_4_sum} shows that on the 16-expert model, comparing with the Loss-Controlled method, the BIP-based algorithm with 4 different iteration times all achieve lower perplexities. More precisely,  the BIP-based algorithm with $T=4$ (which has lowest perplexity) only cost $86.83\%$ training time of which the Loss-Controlled method costs. This is due to the much lower values of $AvgMaxVio$ and $SupMaxVio$ ($0.0602$ versus $0.3852$, and $0.1726$ versus $1.5245$). More details on the whole pre-training process are shown in \fig{total_16_4}, where the $MaxVio_{batch_i}$ of Loss-Controlled method process has a large fluctuation, while the BIP-based method help maintain a smooth state on $MaxVio_{batch_i}$ of the whole pre-training process.

        \begin{table}[H]
            \begin{center}
            \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
                \hline Algorithm&$AvgMaxVio$&$SupMaxVio$&Perplexity&Training time/h\\
                \hline Loss-Controlled&0.3852&1.5245&12.4631&4.6126\\
                \hline Loss-Free&0.1275&1.7702&11.1311&4.3558\\
                \hline BIP, $T=2$&\textbf{0.0529}&0.2019&11.2417&\textbf{3.9547}\\
                \hline BIP, $T=4$&0.0602&\textbf{0.1726}&\textbf{10.6856}&4.0051\\
                \hline BIP, $T=8$&0.0626&0.1727&10.7291&4.0623\\
                \hline BIP, $T=14$&0.0547&0.1925&10.7408&4.177\\
                \hline
            \end{tabular}\\
            \end{center}
            \caption{Evaluation results on training MoE models with $m=16$ and $k=4$.} 
            \label{tab:16_4_sum}
        \end{table}

        \tab{64_8_sum} shows that on the 64-expert model, comparing with the Loss-Controlled method, the BIP-based algorithm with $T=14$ achieves lower perplexities, while perplexities of other 3 BIP-based algorithms with different iteration times are almost the same. The BIP-based algorithm with $T=14$ only cost $86.15\%$ training time of which the Loss-Controlled method costs.  It's important to emphasize that, unlike Loss-Controlled and Loss-Free methods, the $AvgMaxVio$ and $SupMaxVio$ of BIP-Based algorithm do not increase fast from the 16-expert model to the 64-expert one, which still remain at a low level. More details on the whole pre-training process are shown in \fig{total_64_8}. Notice that the separations among three colors of lines are more obvious than the ones in \fig{total_16_4}.

        \begin{table}[h]
            \begin{center}
            \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
                \hline Algorithm&$AvgMaxVio$&$SupMaxVio$&Perplexity&Training time/h\\
                \hline Loss-Controlled&0.7158&2.3841&9.9956&23.7726\\
                \hline Loss-Free&0.3366&2.7121&10.2975&23.9557\\
                \hline BIP, $T=2$&0.0513&0.5613&10.6916&20.4569\\
                \hline BIP, $T=4$&0.0496&0.4107&10.1299&\textbf{20.3046}\\
                \hline BIP, $T=8$&\textbf{0.0441}&0.2372&10.0677&20.4572\\
                \hline BIP, $T=14$&0.0529&\textbf{0.1946}&\textbf{9.9071}&20.4799\\
                \hline
            \end{tabular}\\
            \end{center}
            \caption{Evaluation results on training MoE models with $m=64$ and $k=8$.} 
            \label{tab:64_8_sum}
        \end{table}

        \begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{figure/total_16_4.png}
		\caption{The line graph of relationships between training steps and $MaxVio_{batch_i}$ by different methods in the 16-expert model. The blue lines and dots represent trends of $MaxVio_{batch_i}$ by the Loss-Controlled method. The green lines and dots represent trends of $MaxVio_{batch_i}$ by the Loss-Free method. The red lines and dots represent trends of $MaxVio_{batch_i}$ by the BIP-based method.}\label{fig:total_16_4}
	\end{figure}

        \begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{figure/total_64_8.png}
		\caption{The line graph of relationships between training steps and $MaxVio_{batch_i}$ by different methods in the 64-expert model. The blue lines and dots represent trends of $MaxVio_{batch_i}$ by the Loss-Controlled method. The green lines and dots represent trends of $MaxVio_{batch_i}$ by the Loss-Free method. The red lines and dots represent trends of $MaxVio_{batch_i}$ by the BIP-based method.}\label{fig:total_64_8}
	\end{figure}

        The similar conclusions also hold for each layer in both two models. For more information on experimental data and statistical charts, see \app{tag}.

        \section{Discussion}

        \subsection{Online Algorithm for Problem (\ref{eqn:BIP})}

        We can easily provide \alg{online}, the online version of \alg{SOA} on one routing gate:

        \begin{algorithm}[ht]
            \caption{BIP-Based Balancing Algorithm for MoE Models (Online Version, on one gate)}
            \label{alg:online}
            \begin{algorithmic}[1]
                \State Input: token number per batch $n$, expert number $m$, topk number $k$ and a small constant $T$
                \State Initialize $\bm{q} = \bm{0}^{m}$, $\bm{Q}=\{Q_j=\phi,1\le j\le m\}$
                \For {a token arrives at this routing gate}
                \State Get routing scores $\{s_{1},...,s_{m}\}$
                \For {$j=1,...,m$}
                \State Set 
                $$g_{j}  = \begin{cases} 
                s_{j}, & s_{j} - q_j \in \operatorname{Topk} (\{ s_{l} - q_l | 1 \leq l \leq m \}, k), \\
                0, & \text{otherwise}.
                \end{cases}$$
                \EndFor
                \For {$t=1,...,T$}
                    \State Set $p = \max(0, (k+1)\hspace{2pt}\mathrm{-th}\hspace{2pt} \mathrm{largest \hspace{2pt} value} \hspace{2pt} \mathrm{of}\hspace{2pt}\{ s_{l} - q_l | 1 \leq l \leq m \} )$
                    \For {$j=1,...,m$}
                        \State Set $q_j = \max(0, (nk/m+1)\hspace{2pt}\mathrm{-th}\hspace{2pt} \mathrm{largest \hspace{2pt} value} \hspace{2pt} \mathrm{of}\hspace{2pt}\{Q_j\cup\{s_j-p\}\} )$
                    \EndFor
                \EndFor
                \State Set $\bm Q = \{Q_j\cup\{s_j-p\},1\le j\le m\}$
                \EndFor
            \end{algorithmic}
        \end{algorithm}

        \alg{online} can be applied in recommendation systems.  Consider a scenario that there are over one advertisement slots on one webpage. If our aim is to maximize the sum of CTRs and restrict flows of the most popular advertisement provider, then the problem turns to be (\ref{eqn:BIP}). (In this problem, an expert can be seen as a slot.) Furthermore, the approximation version of \alg{online} is a better choice for this model (see \alg{tcs} in \sec{app}), since its space complexity has no relationship with the number of flows.

        In fact, we notice that this  scenario is a special case of \textbf{multi-slots online matchings \cite{Lu2022MultislotsOM}} . The online matching problem has been widely studied, but for its multi-slot version, there is only a few works \cite{Lu2022MultislotsOM}. The difficulty is that it is non-trivial to extend existing methods to the multi-slots version with diversity pursuit \cite{2009Enhancing,2020Ads}. They either depend on closed-form computations \cite{2015Stock,2018Proportional} or additional assumptions which fail to hold in more than one slots \cite{lu2021dualmirrordescentonline,2021Regularized}. We believe that our algorithms in this work will help on solving this difficult problem.

        \subsection{Approximate Algorithm with Constant Space Complexity}
        \label{sec:app}

        For \alg{online} there are some issues with time and space complexities need to be discussed, especially on maintaining the set array $\bm{Q}$. For each $Q_j\in \bm{Q}$, we can use a heap to maintaining its $(nk/m)$-largest member. Thus for each token, the time complexity of maintaining $\bm{Q}$ and $\bm{q}$ is only $\mathrm{O}(m\log n)$, or $\mathrm{O}(\log n)$ per expert on parallel computing. However, we will need $\mathrm{O}(m*(nk/m))=\mathrm{O}(nk)$ space to storage sets in $\bm{Q}$, which can be seen as a linear relationship with the token size (or the number of flows). Since in recommendation situations, the scale of flows per day can be over millions, which will cost too much storage resources on running \alg{online}. 

        In order to fix this issue, we notice that  if $\bm{0}<\bm{s}<\bm{1}$ holds, we can divide the internal $[0,1)$ into several blocks. Instead of maintaining the set array, we only need to count numbers lying in each block. when we update the vector $\bm q$, we first find the block that ($nk/m+1$)-th largest number lies in, then use interpolation to approximate its value. \alg{tcs} shows details. Notice that the space complexity of \alg{tcs} is $\mathrm{O}(m)$, which has no relationship with the token number.

        \begin{algorithm}[ht]
            \caption{BIP-Based Balancing Algorithm for MoE Models (Online Approximation Version)}
            \label{alg:tcs}
            \begin{algorithmic}[1]
                \State Input: token number $n$, expert number $m$, topk number $k$, constant $b$ and $T$
                \State Initialize $\bm{q} = \bm{0}^{m}$, $\bm{Q}=\bm0^{mb}$
                \For {a token arrives at this routing gate}
                \State Get routing scores $\{s_{1},...,s_{m}\}$
                \For {$j=1,...,m$}
                \State Set 
                $$g_{j}  = \begin{cases} 
                s_{j}, & s_{j} - q_j \in \operatorname{Topk} (\{ s_{l} - q_l | 1 \leq l \leq m \}, k), \\
                0, & \text{otherwise}.
                \end{cases}$$
                \EndFor   
                \For {$t=1,...,T$}
                    \State Set $p = \max(0, (k+1)\hspace{2pt}\mathrm{-th}\hspace{2pt} \mathrm{largest \hspace{2pt} value} \hspace{2pt} \mathrm{of}\hspace{2pt}\{ s_{l} - q_l | 1 \leq l \leq m \} )$
                    \For {$j=1,...,m$}
                \State Set 
                $$Q'_{jl}  = \begin{cases} 
                Q_{jl}+1, & s_j-p\ge 0\ \text{and}\ \frac{l}{b}\le s_j-p<\frac{l+1}{b}, \\
                Q_{jl}, & \text{otherwise}.
                \end{cases}$$
                        \State Set 
                $$q_{j}  = \begin{cases} 
                \mathrm{interpolation\hspace{2pt}between\hspace{2pt}}\frac{l}{b}\hspace{2pt}\mathrm{and}\frac{l+1}{b}, & \exists l, (nk/m+1)\mathrm{-th \hspace{2pt}largest\hspace{2pt}of\hspace{2pt}\mathit{Q'_{j}}\hspace{2pt}is\hspace{2pt}in\hspace{2pt}}[\frac{l}{b},\frac{l+1}{b}), \\
                0, & \text{otherwise}.
                \end{cases}$$
                    \EndFor
                \EndFor
                \State Set $\bm{Q} = \bm{Q'}$
                \EndFor
            \end{algorithmic}
        \end{algorithm}

        \section{Conclusion}

        In this work we provide \textbf{BIP-Based Balancing}, an expert load balancing algorithm based on binary integer programming (BIP). The algorithm keep expert load balance by solving a specific form of binary integer programmings with small time costs. The experimental results show BIP-based algorithm achieves keeping load balance status on every expert and every MoE layer from the first step to the last step during the whole pre-training process, while the trained MoE models also perform well. Finally, we discuss the potential applications of BIP-based algorithm in the fields of recommendation system and online matching.

        \newpage
        
	\bibliography{main}

        \newpage

        \appendix

        \section{Tables and Graphics in \sec{result}}\label{app:tag}

        \begin{table}[h]
            \begin{center}
            \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
                \hline Algorithm&Layer 1&Layer 2&Layer 3&Layer 4&Layer 5&Layer 6&Layer 7&Layer 8\\
                \hline  Auxiliary Loss&0.8988&1.1607&1.1717&1.1726&1.1528&1.14&1.1403&1.1216\\
                \hline Loss Free&0.364&0.3044&0.3341&0.3556&0.3279&0.4681&0.4827&0.3693\\
                \hline BIP, $T=4$&\textbf{0.2024}&\textbf{0.1314}&\textbf{0.1722}&\textbf{0.2153}&\textbf{0.1584}&\textbf{0.1879}&\textbf{0.1998}&\textbf{0.2065}\\
                \hline
            \end{tabular}\\
            \end{center}
            \caption{$AvgMaxVio$ on each layer in MoE models with $m=16$ and $k=4$ achieved by different routing algorithms, for expert load balance evaluations.} 
            \label{tab:16_4_layer}
        \end{table}

        \begin{figure}[H]
		\centering
		\includegraphics[height=0.41\textwidth]{figure/l1_16_4.png}
		\caption{The line graph of relationships between training steps and $MaxVio_{batch_i}$ by different methods in the 16-expert model on layer 1.}\label{fig:l1_16_4}
	\end{figure}

            \begin{figure}[H]
		\centering
		\includegraphics[height=0.41\textwidth]{figure/l2_16_4.png}
		\caption{The line graph of relationships between training steps and $MaxVio_{batch_i}$ by different methods in the 16-expert model on layer 2.}\label{fig:l2_16_4}
	\end{figure}

        \begin{figure}[H]
		\centering
		\includegraphics[height=0.5\textwidth]{figure/l3_16_4.png}
		\caption{The line graph of relationships between training steps and $MaxVio_{batch_i}$ by different methods in the 16-expert model on layer 3.}\label{fig:l3_16_4}
	\end{figure}

        \begin{figure}[H]
		\centering
		\includegraphics[height=0.5\textwidth]{figure/l4_16_4.png}
		\caption{The line graph of relationships between training steps and $MaxVio_{batch_i}$ by different methods in the 16-expert model on layer 4.}\label{fig:l4_16_4}
	\end{figure}

        \begin{figure}[H]
		\centering
		\includegraphics[height=0.5\textwidth]{figure/l5_16_4.png}
		\caption{The line graph of relationships between training steps and $MaxVio_{batch_i}$ by different methods in the 16-expert model on layer 5.}\label{fig:l5_16_4}
	\end{figure}

        \begin{figure}[H]
		\centering
		\includegraphics[height=0.5\textwidth]{figure/l6_16_4.png}
		\caption{The line graph of relationships between training steps and $MaxVio_{batch_i}$ by different methods in the 16-expert model on layer 6.}\label{fig:l6_16_4}
	\end{figure}

        \begin{figure}[H]
		\centering
		\includegraphics[height=0.5\textwidth]{figure/l7_16_4.png}
		\caption{The line graph of relationships between training steps and $MaxVio_{batch_i}$ by different methods in the 16-expert model on layer 7.}\label{fig:l7_16_4}
	\end{figure}

        \begin{figure}[H]
		\centering
		\includegraphics[height=0.5\textwidth]{figure/l8_16_4.png}
		\caption{The line graph of relationships between training steps and $MaxVio_{batch_i}$ by different methods in the 16-expert model on layer 8.}\label{fig:l8_16_4}
	\end{figure}

        \newpage

        \begin{table}[h]
            \begin{center}
            \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
                \hline Algorithm&Layer 1&Layer 2&Layer 3&Layer 4&Layer 5&Layer 6&Layer 7&Layer 8\\
                \hline Auxiliary Loss&2.469&2.4456&2.4983&2.478&2.4586&2.3725&2.2958&2.177\\
                \hline Loss Free&1.5253&1.0639&1.0399&1.0587&1.036&1.1521&1.1314&1.1126\\
                \hline BIP, $T=14$&\textbf{0.1676}&\textbf{0.1138}&\textbf{0.1133}&\textbf{0.1109}&\textbf{0.1342}&\textbf{0.1356}&\textbf{0.2743}&\textbf{0.1888}\\
                \hline
            \end{tabular}\\
            \end{center}
            \caption{$AvgMaxVio$ on each layer in MoE models with $m=64$ and $k=8$ achieved by different routing algorithms, for expert load balance evaluations.} 
            \label{tab:64_8_layer}
        \end{table}

        \begin{figure}[H]
		\centering
		\includegraphics[height=0.41\textwidth]{figure/l1_64_8.png}
		\caption{The line graph of relationships between training steps and $MaxVio_{batch_i}$ by different methods in the 64-expert model on layer 1.}\label{fig:l1_64_8}
	\end{figure}

        \begin{figure}[H]
		\centering
		\includegraphics[height=0.41\textwidth]{figure/l2_64_8.png}
		\caption{The line graph of relationships between training steps and $MaxVio_{batch_i}$ by different methods in the 64-expert model on layer 2.}\label{fig:l2_64_8}
	\end{figure}

        \begin{figure}[H]
		\centering
		\includegraphics[height=0.5\textwidth]{figure/l3_64_8.png}
		\caption{The line graph of relationships between training steps and $MaxVio_{batch_i}$ by different methods in the 64-expert model on layer 3.}\label{fig:l3_64_8}
	\end{figure}

        \begin{figure}[H]
		\centering
		\includegraphics[height=0.5\textwidth]{figure/l4_64_8.png}
		\caption{The line graph of relationships between training steps and $MaxVio_{batch_i}$ by different methods in the 64-expert model on layer 4.}\label{fig:l4_64_8}
	\end{figure}

        \begin{figure}[H]
		\centering
		\includegraphics[height=0.5\textwidth]{figure/l5_64_8.png}
		\caption{The line graph of relationships between training steps and $MaxVio_{batch_i}$ by different methods in the 64-expert model on layer 5.}\label{fig:l5_64_8}
	\end{figure}

        \begin{figure}[H]
		\centering
		\includegraphics[height=0.5\textwidth]{figure/l6_64_8.png}
		\caption{The line graph of relationships between training steps and $MaxVio_{batch_i}$ by different methods in the 64-expert model on layer 6.}\label{fig:l6_64_8}
	\end{figure}

        \begin{figure}[H]
		\centering
		\includegraphics[height=0.5\textwidth]{figure/l7_64_8.png}
		\caption{The line graph of relationships between training steps and $MaxVio_{batch_i}$ by different methods in the 64-expert model on layer 7.}\label{fig:l7_64_8}
	\end{figure}

        \begin{figure}[H]
		\centering
		\includegraphics[height=0.5\textwidth]{figure/l8_64_8.png}
		\caption{The line graph of relationships between training steps and $MaxVio_{batch_i}$ by different methods in the 64-expert model on layer 8.}\label{fig:l8_64_8}
	\end{figure}

\end{document}
