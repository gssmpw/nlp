\section{Background and Related Work}
\label{sec:background}

This section presents background information and related work on explainability in software engineering.

In requirements engineering, explainability is viewed as a non-functional requirement____ and a software quality aspect____ that involves software behavior and explanatory elements____. The need for explanations arises when users or developers face unclear system aspects, requiring careful implementation to avoid cognitive overload____. Additionally, explainability supports ethical standards by clarifying input, training, and regulatory compliance____.
In XAI, explainability (often synonymous with interpretability) aims to clarify AI model reasoning and outputs, enhancing system transparency and trust____. Beyond XAI, explainability addresses software interactions, privacy concerns____, and app-specific challenges like unfamiliar terminology____.
%\subsection{Related Work}
Obaidi et al. explored whether a user's mood influences the type and frequency of explanation needs____, finding that explanation preferences are highly subjective and exhibit only weak or no correlations with mood-related factors. 
Deters et al.____ developed a quality model that assesses the degree of explainability in software systems.
Unterbusch et al.____ emphasize the importance of understanding and classifying users' explanation needs for software systems. They manually analyzed 1,730 app reviews from eight apps to develop a taxonomy of explanation needs and explored automated approaches to detect them. Their best classifier achieved a weighted F-score of 86\% in identifying explanation needs in 486 unseen reviews across four apps.  
Sadeghi et al.____ investigate how misleading explanations affect trust in systems performing unreliable and hard-to-assess tasks. Their online survey with 162 participants found that those exposed to misleading explanations rated their trust significantly higher and aligned their predictions more closely with the system's output than those who saw only the system’s predictions. The authors recommend calibrating explanations according to the system’s confidence to mitigate misleading effects.
Deters et al.____ explore how to evaluate explainability in software systems. 
They discovered that users' prior knowledge should also be included in the evaluation, as users with different prior knowledge require different explanations. However, they did not explore how different levels of prior knowledge could be considered in the generation of explanations.
Ramos et al.____ analyzed how the need for explanation differs for different users by creating so-called personas. These personas reflect user groups and are intended to help requirements analysts identify the need for explanation that these different user groups have for recommender systems. They mainly focus on trust in recommendations and interest in explainability. However, Ramos et al.____ do not yet consider users' familiarity with apps. 
Obaidi et al.____ examined the need for explanations in app reviews of a company that develops multiple navigation apps. They developed an automated approach to identify explanation needs within these navigation apps and determine which team within the company is most likely to provide the required information or where the relevant details can be found.