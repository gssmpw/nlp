\section{Related Work}
% \vspace{-3pt}
\textbf{Machine Learning for Combinatorial Optimization.} Current learning-based CO solvers can be categorized into constructive approaches and improvement-based approaches. Constructive approaches refer to autoregressive methods~\cite{khalil2017learning,kool2018attention,kwon2020pomo,hottung2021learning,kim2022sym} that directly construct solutions by sequentially determining decision variables until a complete solution is constructed, and non-autoregressive methods~\cite{joshi2019efficient,fu2021generalize,geisler2022generalization,qiu2022dimes,sun2023difusco,zheng2024learning} that predict soft-constrained solutions in one shot and then perform post-processing to achieve feasibility. Improvement-based solvers~\cite{d2020learning,wu2021learning,chen2019learning,li2021learning,hougeneralize} learn to iteratively refine a solution through local search operators toward minimizing the optimization objective. 

% Such approaches have been testified in a wide range of graph combinatorial optimization problems including edge-decision problems like routing problems e.g. Traveling Salesman Problem~\cite{joshi2019efficient,sun2023difusco,li2023t2t} and Vehicle Routing Problem~\cite{kool2018attention,kwon2020pomo}, Graph Matching~\cite{wang2019learning,wang2023linsatnet} and node-decision problems like Maximum Independent Set~\cite{boether_dltreesearch_2022,li2023t2t,zhang2023let}, (Maximum) Satisfiability~\cite{selsam2018learning,li2022nsnet} and so on.

% and generating adversarial samples for robustness~\cite{geisler2021generalization,lu2022roco}

Generative modeling for CO has recently shown promise with its potent representational capabilities and informative distribution estimation. It models the problem-solving task as a conditional generation task for learning solution distributions conditioned on given instances~\cite{hottung2021learning,cheng2022policy,sun2023difusco,du2023hubrouter,zhang2023let,li2023t2t}. Drawing from diffusion models, DIFUSCO~\cite{sun2023difusco} has attained SOTA performance in solving TSP and MIS. Nonetheless, it does not incorporate any instance-specific search paradigms to fully capitalize on the estimated solution distribution. Addressing this limitation, the T2T framework~\cite{li2023t2t} further introduces an objective-guided gradient search process during solving to leverage the learned distribution. However, every aspect of this system, including distribution learning and gradient search, hinges on the diffusion model for step-by-step generation. This reliance renders the diffusion-based approaches computationally inefficient and impedes further search computations to trade for solution quality.

% Two lines of work dominate this domain: the first involves modeling the problem-solving task as a conditional generation task for learning solution distributions conditioned on given instances~\cite{hottung2021learning,cheng2022policy,sun2023difusco,du2023hubrouter,zhang2023let,li2023t2t}; the other involves generating valuable problem instances to support problem-solving, e.g., augmenting real-world data to overcome data bottlenecks~\cite{you2019g2sat,li2023hardsatgen,geng2023deep,chen2024mixsatgen}.

\textbf{Diffusion Models and Consistency Models.} Diffusion models entail a dual process comprising noise injection and learnable denoising, wherein neural networks predict the data distribution at each step based on the data from the previous step. For Diffusion in continuous space~\cite{sohl2015deep,song2019generative,ho2020denoising,song2020denoising,song2020improved,nichol2021improved,dhariwal2021diffusion}, the solution trajectories can be modeled by Probability Flow ODE~\cite{song2020score}. Similar paradigms have also been adopted for discrete data using binomial or multinomial/categorial noises~\cite{sohl2015deep,austin2021structured,hoogeboom2021argmax}. On top of the foundation of diffusion models, consistency models~\cite{song2023consistency} define the self-consistency for every generation trajectory and introduce a consistency training paradigm for continuous data to directly learn the mappings from noise to the data. Inspired by this paradigm, we define the optimization consistency condition tailored for the optimization scenario, which requires consistency across multiple trajectories and time steps with the optimal solution as the target in a conditional context, thereby proposing the optimization consistency models as the solver embodiment. The models are employed on the discrete multinomial data for the benefit of CO.

% Methods have been proposed for fast diffusion sampling, including faster numerical ODE solvers~\cite{song2020denoising,zhang2022fast,lu2022dpm} and distillation techniques~\cite{luhman2021knowledge,salimans2022progressive,meng2023distillation,zheng2023fast}. However, these methods still necessitate more than ten inference steps or require a costly data collection process.

% Diffusion models are bottlenecked by their slow sampling speed and 

% \vspace{-3pt}
%