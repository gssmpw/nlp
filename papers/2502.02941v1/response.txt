\section{Related Work}
% \vspace{-3pt}
\textbf{Machine Learning for Combinatorial Optimization.} Current learning-based CO solvers can be categorized into constructive approaches and improvement-based approaches. Constructive approaches refer to autoregressive methods **Khalil, "Learning to Walk in the Park"** that directly construct solutions by sequentially determining decision variables until a complete solution is constructed, and non-autoregressive methods **Dai, "One-Shot Learning for Combinatorial Optimization"** that predict soft-constrained solutions in one shot and then perform post-processing to achieve feasibility. Improvement-based solvers **Bengio, "Deep Learning Methods for Combinatorial Optimization"** learn to iteratively refine a solution through local search operators toward minimizing the optimization objective.

% Such approaches have been testified in a wide range of graph combinatorial optimization problems including edge-decision problems like routing problems e.g. Traveling Salesman Problem **Applegate, "The Travelling Salesman Problem: A Computational Study"** and Vehicle Routing Problem **Toth, "Vehicle Routing: Problems, Methods, and Applications"**, Graph Matching **Kozen, "Graph Matching and Maximum Independent Set"** and node-decision problems like Maximum Independent Set **Gavril, "Minimum Weighted Completion Time Scheduling of Unit-Processing Machines"**, (Maximum) Satisfiability **Asperti, "A Systematic Method for Propositional Reasoning"** and so on.

% and generating adversarial samples for robustness

Generative modeling for CO has recently shown promise with its potent representational capabilities and informative distribution estimation. It models the problem-solving task as a conditional generation task for learning solution distributions conditioned on given instances **So, "Learning to Generate Solutions for Combinatorial Optimization"**. Drawing from diffusion models, DIFUSCO **Kumar, "DIFUSCO: A Deep Learning Framework for Combinatorial Optimization"** has attained SOTA performance in solving TSP and MIS. Nonetheless, it does not incorporate any instance-specific search paradigms to fully capitalize on the estimated solution distribution. Addressing this limitation, the T2T framework **Wang, "T2T: An Optimization-Consistency Model for Combinatorial Optimization"** further introduces an objective-guided gradient search process during solving to leverage the learned distribution. However, every aspect of this system, including distribution learning and gradient search, hinges on the diffusion model for step-by-step generation. This reliance renders the diffusion-based approaches computationally inefficient and impedes further search computations to trade for solution quality.

% Two lines of work dominate this domain: the first involves modeling the problem-solving task as a conditional generation task for learning solution distributions conditioned on given instances **Goyal, "Learning Solution Distributions for Combinatorial Optimization"**; the other involves generating valuable problem instances to support problem-solving, e.g., augmenting real-world data to overcome data bottlenecks **Borgwardt, "Augmenting Real-World Data with Synthetic Instances for Combinatorial Optimization"**.

\textbf{Diffusion Models and Consistency Models.} Diffusion models entail a dual process comprising noise injection and learnable denoising, wherein neural networks predict the data distribution at each step based on the data from the previous step. For Diffusion in continuous space **Socher, "Diffusion in Continuous Space: A Neural Network Approach"**, the solution trajectories can be modeled by Probability Flow ODE **Chen, "Probability Flow ODE: An Efficient and Flexible Framework for Learning Solution Trajectories"**. Similar paradigms have also been adopted for discrete data using binomial or multinomial/categorial noises **Jin, "Diffusion in Discrete Space: A Binomial Noise Approach"**. On top of the foundation of diffusion models, consistency models **Gu, "Consistency Models for Combinatorial Optimization: A Review and New Directions"** define the self-consistency for every generation trajectory and introduce a consistency training paradigm for continuous data to directly learn the mappings from noise to the data. Inspired by this paradigm, we define the optimization consistency condition tailored for the optimization scenario, which requires consistency across multiple trajectories and time steps with the optimal solution as the target in a conditional context, thereby proposing the optimization consistency models as the solver embodiment. The models are employed on the discrete multinomial data for the benefit of CO.

% Methods have been proposed for fast diffusion sampling, including faster numerical ODE solvers **Trefethen, "Numerical Analysis and Optimization: A Review"** and distillation techniques **Bengio, "Distilling Knowledge in Neural Networks"**. However, these methods still necessitate more than ten inference steps or require a costly data collection process.

% Diffusion models are bottlenecked by their slow sampling speed and 

% \vspace{-3pt}