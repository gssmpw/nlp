\section{Introduction}
\label{sec:intro}

In recent years, generative models have undergone rapid and accelerating advancements~\cite{ddim,ddpm,normalizing_flow,generative_models_survey,bourou_conditional}, resulting in their widespread adoption across a variety of fields. Notably, these models have made significant contributions to biological research. For example, they have been employed in protein design~\cite{protein_design}, predicting protein structures~\cite{alphafold}, integrating cancer data~\cite{cancer_data}, synthesizing biomedical images~\cite{biomedical_1,biomedical_2}, predicting molecular structures~\cite{molgan,denovo}, and identifying phenotypic cell variations~\cite{bourou_1,bourou_2,Lamiable2023}.

Identifying phenotypic variations in biological images is crucial for advancing our understanding of biological processes. Detecting these differences can be particularly challenging due to the high degree of biological variability, yet it holds immense potential for enhancing disease understanding, discovering novel biomarkers, and developing new therapeutics and diagnostics~\cite{cellular_profiling,cellular_profiling_2,cellular_profiling_3}. Traditional methods for identifying these phenotypes often rely on cell segmentation and the quantification of features such as intensity, shape, and texture~\cite{cellular_profiling}. Recently, deep learning techniques, particularly generative models~\cite{Lamiable2023,bourou_1,bourou_2}, have been applied to automate and refine this process, enabling the identification of more interpretable and biologically meaningful features. Among these approaches, diffusion models have emerged as state-of-the-art generative models~\cite{diffusion_beat_gans}, achieving remarkable results in tasks such as image synthesis. However, training diffusion models, like other deep learning models requires large datasets, which is often difficult to obtain in biological applications.

In this work, we propose \textbf{Phen-LDiff} a method to detect cellular variations in small biological datasets by leveraging pre-trained Latent Diffusion Models (LDMs)~\cite{stable_diffusion}.







\section{Related Work}


\label{gen_inst}

\paragraph{Diffusion Models.} Diffusion Models (DMs)\cite{ddpm,ddim} are generative models that have recently achieved remarkable results in various tasks. DMs are latent variable models that operate through two key processes: a fixed forward process that gradually adds noise to the data and a learned backward process that denoises it, reconstructing the data distribution~\cite{ddpm,ddim}. Recently, these models have seen several advancements~\cite{ddim,diffusion_beat_gans,classifier_free,unified_diffusion}, making them state-of-the-art in image synthesis, surpassing traditional generative models like GANs~\cite{diffusion_beat_gans}. One of the notable improvements is the introduction of Latent Diffusion Models (LDMs)~\cite{stable_diffusion}, where images are first compressed into a latent space using a variational autoencoder, and then the diffusion process occurs within this compressed latent space. This approach enables more efficient scaling to higher-resolution images and accelerates training times. Additionally, LDMs incorporate a conditioning mechanism, allowing for tasks such as text-conditioned image generation, inpainting, and super-resolution.
These innovations in LDMs have facilitated their training on massive datasets~\cite{LAION_dataset}, resulting in powerful pre-trained models such as Stable Diffusion~\cite{stable_diffusion}, which have demonstrated exceptional performances in various generative tasks.

\paragraph{Identifications of Phenotypes in Biological Images.}

Identifying phenotypic variations in biological images is essential in biology and drug discovery~\cite{cellular_profiling,cellular_profiling_2}, yet it presents significant challenges. One of the key difficulties is the biological variability among cells within the same condition, which can obscure the differences between distinct conditions. Recently, generative models have been employed to cancel this natural variability in order to visualize and explain cellular phenotypes in microscopy images~\cite{Lamiable2023,unbiased_cell,bourou_1}. In~\cite{bourou_1}, cellular variations between conditions were identified through an image-to-image translation task between two classes, following methodologies similar to those in~\cite{cyclegan,pixtopix}. In Phenexplain~\cite{Lamiable2023}, a conditional StyleGAN2~\cite{stylegan2} was trained to detect cellular changes by performing translations between synthetic images within the latent space of StyleGAN2, allowing for training across multiple conditions, unlike the approach in~\cite{bourou_1}. A similar method was presented in~\cite{unbiased_cell}, but instead of utilizing the latent space of GANs, the authors proposed learning a representation space using self-supervised learning techniques~\cite{self_supervised_survey}. In~\cite{bourou_2}, conditional diffusion models were applied to identify phenotypes in real images. This approach consists of two stages: first, the source class image is inverted into a latent code, which is then used to generate an image from the target class. This method provides a powerful alternative for phenotype detection using real biological data. However, all of these models require a large number of images to be properly trained.

\paragraph{Fine-tuning Diffusion Models.}

Fine-tuning~\cite{fine_tunening_1,fine_tuning_1,fine_tunening_2,svdiff,lora}, a well-established strategy for training deep learning models on limited data, involves adapting pre-trained models. It involves adapting a pre-trained model's weights to fit a smaller dataset. Fine-tuning methods can be categorized into three main groups: adaptive methods~\cite{fine_tunening_1,fine_tuning3}, where the entire model's weights are adjusted; selective methods~\cite{selective_1,selective_2,selective_3}, where only a subset of the model's parameters are modified; and additive methods~\cite{lora,svdiff}, where additional networks are incorporated to refine the weights. These techniques have proven effective for discriminative models and have recently been extended to generative models, such as GANs, autoregressive generative models~\cite{lora}, and diffusion models~\cite{svdiff}. Fine-tuning techniques for diffusion models have gained attention, particularly due to the availability of models pre-trained on large datasets. Recently, several approaches have been proposed for fine-tuning diffusion models~\cite{svdiff,fine_tuning_1,lora}, driven by the popularity of pre-trained models like Stable Diffusion~\cite{stable_diffusion}. In~\cite{fine_tuning_1}, it was demonstrated that modifying a subset of parameters can lead to efficient fine-tuning. Low-Rank Adaptation (LoRA)\cite{lora}, a technique originally developed for fine-tuning large language models (LLMs)~\cite{LLMs}, can also be applied to diffusion models. LoRA freezes the pre-trained model's weights and learns low-rank matrices that are injected into each layer of the network. In\cite{svdiff}, the authors introduced SVDiff, a fine-tuning method for diffusion models that focuses on learning shifts in the modelâ€™s singular values.

\begin{figure}[h]
  \centering
  % First subfigure
  \begin{subfigure}{\linewidth}
    \centering
    \includegraphics[width=0.9\linewidth]{images/classes_similarity_lrkk2.drawio.png} % Replace with your image path or URL
    \caption{}
    \label{fig:subfigure1}
  \end{subfigure}

  \vspace{0.5cm} % Space between subfigures

  % Second subfigure
  \begin{subfigure}{\linewidth}
    \centering
    \includegraphics[width=0.9\linewidth]{images/classes_similarity_golgi.drawio.png} % Replace with the path for the second image
    \caption{}
    \label{fig:subfigure2}
  \end{subfigure}

  \caption{\textbf{Top}: Real images from the LRRK2 dataset, displaying wild-type images in the first row and images of mutated neurons in the second row. \textbf{Bottom}: Real images from the Golgi dataset, with untreated images in the first row and Nocodazole-treated images in the second row. In both (a) and (b), identifying and interpreting differences between the two classes by eye is highly challenging. However, it is essential for understanding the disease in (a) and assessing the treatment effects in (b)}
  \label{fig:example}
\end{figure}





