\section{Conclusion}

In this work, we propose \textbf{Phen-LDiff}, a method for image-to-image translation using fine-tuned Latent Diffusion Models (LDMs) to identify phenotypic variations from limited microscopy data. Our approach demonstrates that LDMs can be effectively fine-tuned on biological datasets, capturing their underlying distributions even when data is limited. We found that certain fine-tuning approaches, such as full model fine-tuning and attention fine-tuning, can lead to memorization. In contrast, methods like LoRA and SVDiff promote better generalization, even with small datasets containing as few as 100 images per class. Our method enables image-to-image translation by first inverting an image into a latent space, followed by conditional generation to highlight phenotypic variations between conditions. We tested this approach across multiple biological datasets, showing its capability to reveal both apparent and subtle differences between experimental conditions. When compared to other representative methods, Phen-LDiff outperformed them in translation quality, even with limited image datasets.  Furthermore, our method avoids memorization and is computationally more efficient than diffusion models trained from scratch, reducing training time significantly without compromising quality. 

We anticipate that Phen-LDiff can contribute to biological research and drug discovery by enabling experts to gain deeper insights into disease mechanisms and treatment effects, especially in low-data regimes where traditional methods struggle. This efficiency and ability to generalize make Phen-LDiff a promising tool for advancing precision in phenotypic analysis.



