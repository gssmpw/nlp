\section{Online rating algorithms}


\label{sec:appendix-algorithm}


In this paper, we will investigate the performance of the following algorithms: Elo, Glicko, Elo2k and Pairwise. 
\paragraph{Elo}
Elo rating gies the prediction          $p_t := \sigma\left(\theta_t[i_t] - \theta_t[j_t]\right)$. Initially $\theta_0[i]=0$ for every $i \in [N]$. The update rule is:
\begin{equation}
    \begin{cases}
    \theta_{t+1}[i_t] &\gets \theta_{t}[i_t] + \eta_t \left(o_t - p_t\right),\\
    \theta_{t+1}[j_t] &\gets \theta_{t}[j_t] - \eta_t \left(o_t - p_t\right).
    \end{cases}
\end{equation}
Here $\sigma = 1/(1+e^{-x})$ is the logistic function. $\theta_t\in\R^N$ is the \emph{rating}, or \emph{score}, for the $N$ players at time $t$. Customarily, the reported rating is multiplied by a constant $C = \frac{400}{\ln 10}$. The learning rate $\eta_t$ is often chosen to be a fixed value $\eta$ between $10/C\approx 0.06$ and $40/C\approx 0.23$. In our experiments, we choose $\eta_t$ according to the following decaying learning rate scheme: $\eta_t = \sqrt{\frac{aN}{t+b}}$, where $a,b$ are chosen to ensure the learning rate will not be too large at the beginning, and still large enough for achieving a good prediction accuracy when $t$ is large. For details see \ref{sec:appendix-choosing_parameter}.


% In fact Elo is based on the underlying Bradley-Terry model for pairwise comparison. The BT model assumes that each player $i$ has a score $\theta^*[i]$, and the expectation of the game outcome will be 

% \begin{equation}
%     \E[o_t|i_t,j_t] = \sigma(\theta^\star[i_t]-\theta^\star[j_t]) \tag{Bradley-Terry}.
% \end{equation}

Elo update can be understand as online gradient descent, as we described in Section \ref{sec:OCO}. Also, we can show that $f_t$ is convex:
\begin{align*}
    \nabla_{\theta} f_t(\theta) &= -\nabla_{\theta} (o_t \log  p_t + (1-o_t) \log (1-p_t)) \\
    &=- (o_t   \frac{1}{p_t} \nabla_{\theta}  p_t + (1-o_t)  \frac{1}{1-p_t} ( -\nabla_{\theta}  p_t ) ) \\
    &= - \frac{o_t - p_t}{p_t(1-p_t)} \nabla_{\theta}  p_t \\
    &= - (o_t - p_t) (\boldsymbol{e}_{i_t}- \boldsymbol{e}_{j_t}).
\end{align*}

\begin{align*}
    \nabla^2_{\theta} f_t(\theta) &= \nabla_{\theta} (\nabla_{\theta} f_t(\theta)) \\
    &= -\nabla_{\theta} ( (o_t - p_t) )(\boldsymbol{e}_{i_t}- \boldsymbol{e}_{j_t}) \\
    &= -(\boldsymbol{e}_{i_t}- \boldsymbol{e}_{j_t}) (\nabla_{\theta}(o_t - p_t) )^T \\
    &= p_t (1-p_t) (\boldsymbol{e}_{i_t}- \boldsymbol{e}_{j_t}) (\boldsymbol{e}_{i_t}- \boldsymbol{e}_{j_t})^{T}\\
    & \succeq 0.
\end{align*}



Details can be seen in Section \ref{sec:OCO}.

\paragraph{Glicko} Glicko \citep{glickman1995glicko} assumes each player has a rating $\theta$ and  a “ratings deviation” $v$. The initial $\theta$ of each player is set to be $1500$, and
% and the initial $v$ is set to be $350$ by default. In our experiments, 
we set initial $v$ to be $35,100$ or $350$. The prediction $p_t:= \sigma( \frac{\ln 10}{400} g(\sqrt{v_t[i_t]^2 + v_t[j_t]^2}) (\theta_t[i_t] - \theta_{t}[j_t]) )$, where $\sigma$ is the logistic function. The update rule of the parameters is
\begin{equation}
    \begin{cases}
    \theta_{t+1}[i_t] &\gets \theta_{t}[i_t] + \frac{\ln 10}{400} (\frac{1}{v_t[i_t]^2} + \frac{1}{d(i_t,j_t))^2})^{-1} g(v_{t}[j_t])(o_t - \tilde{p}(i_t,j_t)), \\
    \theta_{t+1}[j_t] &\gets \theta_{t}[j_t] + \frac{\ln 10}{400} (\frac{1}{v_t[j_t]^2} + \frac{1}{d(j_t,i_t))^2})^{-1} g(v_{t}[i_t])(1- o_t - \tilde{p}(j_t,i_t)), \\
    v_{t+1}[i_t] &\gets \sqrt{(\frac{1}{v_{t}[i_t]^2} + \frac{1}{d(i_t,j_t)^2})^{-1}} , \\
    v_{t+1}[j_t] &\gets \sqrt{(\frac{1}{v_{t}[j_t]^2} + \frac{1}{d(j_t,i_t)^2})^{-1}} ,
    \end{cases}
\end{equation}
where $g(x):= (1 + \frac{3 (\frac{\ln 10}{400})^2 x^2 }{\pi^2})^{-\frac12}$, $\tilde{p}(i_t,j_t):= v( \frac{\ln 10}{400} g(v_t[i_t]) (\theta_t[i_t] - \theta_{t}[j_t]) )$, and $d(i_t,j_t)^2 := ((\frac{\ln 10}{400})^2 g(v_t[j_t]^2) \tilde{p}(i_t,j_t) (1- \tilde{p}(i_t,j_t)))^{-1}$.

\paragraph{TrueSkill} TrueSkill \citep{dangauthier2007trueskill} assumes each player 
has an average skill $\theta$ and a degree of uncertainty  $v$, similar to Glicko. The difference is that TrueSkill use a Gaussian function for prediction, rather than logistic: the prediction $p_t:=\frac{1}{c\sqrt{2}}\Phi(\sqrt{2}( \theta_t[i_t]-\theta_t[j_t]))$, where $\Phi$ is the CDF for standard normal distribution, and $c_t = \sqrt{2\beta^2 + v_{t}[i_t]^2 + v_{t}[j_t]^2}$ is the overall variance. 
% $\beta$ is set to be $25/6$ by default, and the initial $v^2$ for every player is set to be $4 \beta^2$ by default. In our experiments, we set $\beta$ to be $0.2$ or $1.0$. 
In our experiments, we set $\beta$ to be $0.2,0.8$ or $1.0$, and the initial $v^2$ for every player is set to be $4 \beta^2$ by default.


The update rule for the parameters is 
\begin{equation}
    \begin{cases}
    \theta_{t+1}[i_t] &\gets \theta_{t}[i_t] + (2 o_t - 1) \frac{v_{t}[i_t]^2}{c_t^2} v(\frac{( \theta_t[i_t]-\theta_t[j_t])(2 o_t - 1)}{c_t}),\\
    \theta_{t+1}[j_t] &\gets \theta_{t}[j_t] - (2 o_t - 1) \frac{v_{t}[j_t]^2}{c_t^2} v(\frac{( \theta_t[i_t]-\theta_t[j_t])(2 o_t - 1)}{c_t}),\\
    v_{t+1}[i_t] &\gets v_{t}[i_t] \sqrt{1- \frac{v_{t}[i_t]^2}{c_t^2} w(\frac{( \theta_t[i_t]-\theta_t[j_t])(2 o_t - 1)}{c_t})}, \\
    v_{t+1}[j_t] &\gets v_{t}[j_t] \sqrt{1- \frac{v_{t}[j_t]^2}{c_t^2} w(\frac{( \theta_t[i_t]-\theta_t[j_t])(2 o_t - 1)}{c_t})}  
    \end{cases}
\end{equation}

where $v(x):= \frac{\phi(x)}{\Phi(x)}$ ($\phi$ is the pdf of standard Gaussian), $w(x):=v(x)(v(x)+x)$. 


 


\paragraph{Elo2k} If we generalize Elo score by rating every player with a vector instead of scalar (see \cite{balduzzi2018re} and \cite{bertrand2023limitations}), we get Elo2k. The parameter for the algorithm is  $\theta = (U, V)$, where $U= (U[1], \cdots, U[N]), V = (V[1], \cdots, V[N])$, $U[i], V[i] \in \mathbb{R}^{k}$. The prediction $p_t:=\sigma(U[i_t]^{T}V[j_t]-U[j_t]^{T}V[i_t])$. In this paper, we initially choose each element of $U$ (or $V$) from $\text{Uniform}([0,0.1])$. 
The update rule is given by taking the gradient of $U,V$, i.e.,
\begin{equation}
    \begin{cases}
    U_{t+1}[i_t] &\gets U_{t}[i_t] + \eta_t \left(o_t - p_t\right) V_{t}[j_t] ,\\
    U_{t+1}[j_t] &\gets U_{t}[j_t] - \eta_t \left(o_t - p_t\right) V_{t}[i_t] ,\\

    V_{t+1}[i_t] &\gets V_{t}[i_t] - \eta_t \left(o_t - p_t\right) U_{t}[j_t] ,\\
    V_{t+1}[j_t] &\gets V_{t}[j_t] + \eta_t \left(o_t - p_t\right) U_{t}[i_t] \\
    \end{cases}
\end{equation}
In our experiments, we choose $\eta_t$ according to the following decaying learning rate scheme: $\eta_t = \sqrt{\frac{aN}{t+b}}$.

\paragraph{Pairwise} A very natural algorithm is that we compute the pairwise win rate $P_t[i,j]$ for each pair of players $(i,j)$, and the prediction $p_t = P_t[i_t,j_t] $. This algorithm has $N(N-1)/2$ parameters. To ensure the prediction will not be affected dramatically by a single game result, we will regularize it as the following. The update rule is given by
\begin{equation}
    \begin{cases}
    P_{t}[i_t, j_t] &\gets \frac{5 + \# \{ \text{games that } i_t \text{ wins } j_t \text{ until time } t\} }{10 + \# \{ \text{games that } i_t \text{ plays with } j_t \text{ until time } t\}} ,\\
    P_{t}[j_t, i_t] &\gets \frac{5 + \# \{ \text{games that } j_t \text{ wins } i_t \text{ until time } t\} }{10 + \# \{ \text{games that } j_t \text{ plays with } i_t \text{ until time } t\}} \\

    \end{cases}
\end{equation}

