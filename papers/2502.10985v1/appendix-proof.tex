\section{Theory and experiments for ranking given by Elo}
\label{sec:appendix-proof}
% \subsection{The small learning rate limit of $\theta^{\rm elo}$}
% We note that throughout this paper, the initial Elo rating $\theta_0$ is assumed to be 0.
% \begin{theorem}
% \label{thm:small-eta-mle}
% Given $\{i_t,j_t,o_t\}_{t\in [T]}$,
% \[
% \lim_{\eta\to 0}\frac{1}{\eta}\theta^{\rm elo} = \lim_{\eta\to 0}\frac{1}{\eta}\theta^{\rm lin} =\lim_{\eta\to 0} \frac{1}{\eta}\theta^{\rm mle}_{1/\eta}.
% \]
% \end{theorem}
% \begin{proof}
% Define 
% \[
% W[i]:= \sum_{t=1}^T \left[\one[i_t=i]\left(o_{t} - \frac{1}{2}\right) - \one[j_t=i]\left(o_{t} - \frac{1}{2}\right)\right],
% \]
% which is the total number of net wins of player $i$.

% Given the update rule (\ref{eq:elo}), we can see that $\Vert \theta_t\Vert_\infty \le \eta t$; it follows that
% \begin{align}
% \label{eq:elo-limit}
% \frac{\theta_{t+1}[i_t]}{\eta} = \frac{\theta_{t}[i_t]}{\eta} + o_t -\sigma(\theta_t[i_t]-\theta_t[j_t]) = \frac{\theta_{t}[i_t]}{\eta} + o_t - \frac{1}{2} - \frac{1}{4}(\theta_t[i_t]-\theta_t[j_t]) + O(\eta^3t^3).
% \end{align}
% Therefore,
% \begin{align*}
% \lim_{\eta \to 0}\frac{\theta_{T+1}[i]}{\eta} = \sum_{t=1}^T \left[\one[i_t=i]\left(o_{t} - \frac{1}{2}\right) - \one[j_t=i]\left(o_{t} - \frac{1}{2}\right)\right] = W[i].
% \end{align*}
% Meanwhile, notice that dividing Eq. (\ref{eq:linear-elo}) by $\eta$ gives exactly Eq. (\ref{eq:elo-limit}) without the low order term. Therefore
% \begin{align*}
% \lim_{\eta\to 0}\frac{\theta^{\rm}[i]}{\eta} = W[i].
% \end{align*}

% Finally, consider the equation
% \begin{align*}
% \nabla \left(\sum_{t=1}^T \ell_t(\theta) + \frac{1}{2\eta}\Vert \theta\Vert^2\right) = \sum_{t=1}^T (\sigma(\theta^\top x_t) - o_t)x_t + \frac{1}{\eta}\theta = 0,
% \end{align*}
% where $x_t = \mathbf{e}[i_t] - \mathbf{e}[j_t]$. Multiplying both sides by $\eta$ and rearranging gives:
% \[
% \theta = \eta\sum_{t=1}^T\left(o_t - \sigma(\theta^\top x_t)\right)x_t.
% \]
% Therefore, the solution $\theta^{\rm mle}_{1/\eta}$ must satisfy $\Vert \theta^{\rm mle}_{1/\eta}\Vert = O(\eta T)$. It follows that 
% \begin{align*}
% \theta^{\rm mle}_{1/\eta} = \eta\sum_{t=1}^T\left(o_t - \frac{1}{2} - \frac{1}{4}(\theta^{\rm mle}_{1/\eta} )^\top x_t\right)x_t + O(\eta^3 T^4).
% \end{align*}
% Therefore
% \begin{align*}
% \lim_{\eta\to 0}\eta^{-1}\theta^{\rm mle}_{1/\eta}[i] = \sum_{t=1}^T\left(o_t-\frac{1}{2}\right)x_t^\top \mathbf{e}[i] = W[i].
% \end{align*}



% \iffalse
% Finally, $\theta^{\rm mle}_\lambda$ is the solution to $\forall i\in [N]$,
% \begin{align*}
% \sum_j
% \end{align*}

% \begin{align*}
% \lim_{\eta\to 0}
% \end{align*}
% \fi


% \end{proof}


\subsection{Proofs for Theorem \ref{thm:Elo-winrate}}
\label{proof:thm:Elo-winrate} 
The formal version of Theorem \ref{thm:Elo-winrate} is stated as:
\begin{theorem}
Consider the population negative log-likelihood function of BT model $\mathbb{E}_{q} [\cL(\theta)]$,
where $q$ is the matchmaking distribution. Let \begin{equation*}
\theta^* := \argmin_{\theta\in\R^N}\mathbb{E}_{q}\cL(\theta)
\end{equation*}  
be the population MLE. Then if $q$ is a product distribution, i.e., $q_{ij}$, the probability player $i$ plays with $j$, satisfies $q_{ij}=q_i q_j$ for any $i,j \in[N]$. Then the ranking given by $\theta^*$ is the same as the ranking given by the average win rate. This result hold for uniform matchmaking as a special case.

\begin{proof}
With a slightly abuse of notation, we use $\theta_i$ to denote the $i-$th entry of $\theta$.  Then
\begin{align*}
  \mathbb{E}_{q} [\cL(\theta)]  = -\sum_{i,j \in [N]} q_{ij} (P_{ij}\log (\sigma(\theta_i-\theta_j)) + P_{ji}\log (1-\sigma(\theta_i-\theta_j))).
\end{align*}
Set it's gradient to zero, we have for each $i \in [N]$,
\begin{align*}
  0=\frac{\partial}{\partial \theta_i}\mathbb{E}_{q} [\cL(\theta^*)]  &= \frac{\partial}{\partial \theta_i} (-\sum_{i,j \in [N]} q_{ij} (P_{ij}\log (\sigma(\theta^*_i-\theta^*_j)) + P_{ji}\log (1-\sigma(\theta^*_i-\theta^*_j))) ) \\
  &= -\sum_{j \in [N]} q_{ij} (P_{ij} - \sigma(\theta^*_i -\theta^*_j)).
\end{align*}
For the last equation we use the property that $\sigma'(t)=1-\sigma(t)$ and $P_{ji}=1-P_{ij}$. Since $q_{ij}=q_i q_j$, we can devide $q_i$ from both side and derive
\begin{align*}
    \sum_{j \in [N]}q_j P_{ij} =\sum_{j \in [N]}q_j \sigma(\theta^*_{i} - \theta^*_{j}).
\end{align*}
Notice that $\text{LHS}=\mathbb{E}_{q}\overline{P}[i]$ is the average win rate of player $i$ under matchmaking $q$. If we define 
\begin{align*}
    f(x):=\sum_{j \in [N]}q_j \sigma(x - \theta^*_{j}),
\end{align*}
then $\mathbb{E}_{q}\overline{P}[i]=\text{LHS}=\text{RHS}=f(\theta^*_{i})$. Notice that $f$ is a monotone increasing function, therefore the ranking given by $\mathbb{E}_{q} \overline{P}$ is the same as the ranking given by $\theta^*$. 
\end{proof}

\end{theorem}


\subsection{Example where Elo contradicts SST under matchmaking}
\begin{example}
\label{example:sst}
Consider the following $P$ matrix among $5$ players that satisfies SST with $\pi(i)=6-i$.
\begin{align*}
P=\left[
\begin{matrix}
0.5 & 0.99 & 0.99 & 0.99 & 0.99\\
0.01 & 0.5 & 0.6 & 0.7 & 0.99 \\
0.01 & 0.4 & 0.5 & 0.6 & 0.99 \\
0.01 & 0.3& 0.4 & 0.5 & 0.51 \\
0.01 & 0.01 & 0.01 & 0.49 & 0.5
\end{matrix}
\right].
\end{align*}
Suppose that the matchmaking distribution is given by
\begin{align*}
Q=\left[
\begin{matrix}
 &  0.125&  &  & \\
0.125 &  & & 0.125 &  \\
 &  &  &  &  0.125 \\
 & 0.125 &  & & 0.125  \\
 &  & 0.125  &0.125  &
\end{matrix}
\right],
\end{align*}
where the remaining entries are $0$.
\end{example}

In this case, given infinite data,
\[
\theta^{\rm mle} = [5.48, 0.89, 4.60, 0.04, 0],
\]
which induces an inconsistent ranking $1\succ 3\succ 2\succ 4\succ 5$. 


We also consider the regime where $T$ does not go to infinity. We conduct the following synthetic experiment: we generate random samples for $T=10000$,  following the $P$ and $Q$ in Example \ref{example:sst}. Then we construct confidence interval for each player's Elo score by bootstrapping (following the procedure in chatbot arena): we sample $T=10000$ times with replacement from the original created random samples. We create $100$ such bootstrap samples. For each of these samples, we can compute an Elo score (we regularize the scores so that player 5 always has score 0). Then for each player's Elo score in 100 different samples, we can compute the 0.05 quantile and 0.95 quantile for these scores, therefore give a confidence interval for each player's score. The resulting confidence interval for each player is:
$[4.86,5.33], [0.72,1.17], [4.18,4.68], [-0.07,0.31], [0.00,0.00]$. From these confidence intervals, we can confidently say the Elo scores give the ranking $1\succ 3\succ 2\succ 4$, which is inconsistent for players $\{1,2,3,4\}$.
% ([4.861926142049362, 0.7181125072765757, 4.182484719661942, -0.07536210557945967, 0.0], [5.336131963613072, 1.1700609142077774, 4.681171240646495, 0.31355274083886847, 0.0])