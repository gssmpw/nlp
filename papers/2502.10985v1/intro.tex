\section{Introduction}
\label{sec:intro}
% The Elo rating system, initially proposed by Arpad Elo~\citep{elo1961new}, is a widely-used method of rating the strength of players in two-player zero-sum games. Originating in the realm of chess, this rating system has since been broadly adopted for other games including renju, go, StarCraft and OverWatch. Its application extends to sports such as tennis and baseball, and more recently, the evaluation of large language models (LLMs) and AI agents. Elo rating is usually considered as an incremental update algorithm for estimating a underlying Bradley-Terry (BT) model for pairwise comparison. BT model assumes each player $i$ has a strength $\theta[i]$,  and for a single game between player $i$ and $j$, the probability that player $i$ wins is $\sigma(\theta[i] - \theta[j])$, where $\sigma$ is the logistic function. Based on this model, after each game, Elo rating system will adjust each player's estimated strength according to the actual game result.


% Despite its widespread use, whether the underlying BT model, where Elo rating is based on, can truly fit the real world games remains doubtful. This leads to the following important question:
% \begin{center}
% \textit{Is Elo rating reliable?}  
% \end{center}
% In this paper, we aim to answer this question, through the perspective of model misspecification. We begin with examining the realizability of the BT model in real-world datasets. We design a likelihood ratio test, and show that in most real-world game datasets, the BT model is rejected, namely, the underlying data distribution does not follow the BT model. Also, we observe that, the matchmaking distribution and player skills in real datasets are not stationary, further demonstrating the model misspecification when applying Elo rating. 

% Given the model misspecification, does it mean that Elo rating is unreliable in real world games? The answer is no --- we conduct further experiments on real-world datasets and show that, in many of those non-BT datasets, Elo demonstrates superior prediction accuracy of next game outcome compared to algorithms based on more complex models, e.g., mElo and vanilla pairwise win rate. Therefore, it is crucial to give an explanation of this phenomenon, going beyond the traditional view that the Elo rating is just the parameter estimation of BT model.

% In this paper, we propose a regret-minimization framework for viewing algorithms including Elo. To be specific, Elo can be interpreted as an instance of online gradient descent. Online gradient descent, as an online convex optimization (OCO) algorithm, is guaranteed to have a small regret. Therefore, Elo rating can provably achieve a relatively low regret, which contributes to its strong predictive performance, even when BT model is non-realizable. 

% We also justified this theoretical explanation by synthetic experiments. We test the performance of different algorithms under different underlying game distributions, showing that data sparsity level plays an crucial role in the prediction performance of algorithms. To be specific, in sparse datasets, where the regret contributes majorly to the accumulated loss, Elo rating and its simple variants achieves good performance due to their low regret, even when the underlying distribution of games is non-BT. However in dense datasets, where the regret for all algorithms should goes to zero, the optimal loss in hindsight becomes dominant
% in the accumulative loss. More sophisticated models tend to have smaller optimal loss in
% hindsight due to their model capacity, therefore it is possible that Elo may not remain the
% best algorithm in those dense datasets.

% To complement our study, we also investigated the pairwise ranking performance of Elo. Synthetic experiments show that in general, good prediction performance has strong correlation with good ranking performance. However, we also argue that Elo rating should not be blindly trusted. We give a theory showing that, Elo rating recovers the true ranking in transitive models when the matchmaking is uniform, but when the matchmaking is arbitrary, Elo rating may not give a consistent ranking, which is also shown by synthetic experiments.

% To summarize, our main contributions include:
% \begin{enumerate}
%     \item We show that real-world games do not follow BT model, through a likelihood ratio test.
%     \item We examine the prediction performance of rating algorithms in real datasets, finding that Elo rating shows good performance even in some non-BT datasets.
%     \item We interpreted Elo rating as online gradient descent, provably show it can be guaranteed to have a good prediciton performance through the lens of regret minimization.
%     \item Further synthetic experiments show that data sparsity level plays a crucial role in the performance of algorithms.
%     \item We also show that good prediction have strong correlation with good (pairwise) ranking, though Elo rating can not be blindly trusted.
% \end{enumerate}


The Elo rating system, introduced by Arpad Elo~\citep{elo1961new}, is a widely-used method for rating player strength in two-player, zero-sum games. Initially developed for chess, Elo has since been adopted across a broad range of games, including Go, sports, video games, and recently, in evaluating large language models (LLMs) and AI agents. Elo rating is usually interpreted as an incremental update algorithm for estimating an underlying stationary Bradley-Terry (BT) model. BT model assumes each player $i$ has a scalar strength rating $\theta[i]$ (which does not change),  and for a single game between player $i$ and $j$, the probability that player $i$ wins is $\sigma(\theta[i] - \theta[j])$, where $\sigma$ is the logistic function. Based on this model, after each game, Elo rating system will adjust each player's rating according to the actual game result. 


Despite the widespread use of Elo, its foundation on games following stationary BT models appears restrictive. In this paper, we first examine whether the BT assumption holds in real-world datasets. Using a likelihood ratio test, we show that game outcomes in many datasets deviate significantly from the BT model, indicating substantial model misspecification. Furthermore, we observe that player skills and matchmaking distributions are often non-stationary. This raises serious concerns over Elo's reliability on practical uses. Surprisingly, we also observe that, despite these deviations, Elo still frequently outperform more complex models, such as mElo and pairwise methods---designed to capture non-BT components---in predicting outcomes of the real-world games. These findings call for a deeper understanding of Elo beyond its conventional interpretation as a BT model parameter estimator. In this paper, we explore this phenomenon through three key perspectives.



% Despite the wide use of Elo, its foundation on games being stationary BT models appear rather restrictive. In this paper, we first investigate whether such assumption of BT model holds in real-world datasets. Using a likelihood ratio test, we demonstrate that in many real-world datasets, the game results do not follow the BT model, indicating significant model misspecification. Moreover, we observe that player skills and matchmaking distributions in these datasets are often non-stationary. However, despite these deviations, further experiments on real-world datasets reveal that, in some non-BT and non-stationary datasets, Elo outperforms more complex algorithms such as mElo and pairwise models in predicting the next game outcome, while these complex models are designed to account for non-BT components in the data. These phenomenons demand a deeper explanation, going beyond the traditional understanding of Elo as the parameter estimation the BT model. In this paper, we explain the unexpected phenomenon through three key perspectives. 

% Despite its widespread use, whether the stationary BT model, where Elo rating is based on, can truly fit the real world games is untested. In this paper, we are the first to investigate whether the BT model holds in real-world datasets. Using a likelihood ratio test, we demonstrate that in many real-world datasets, the game results do not follow the BT model, indicating significant model misspecification. Moreover, we observe that player skills and matchmaking distributions in these datasets are often non-stationary. However, despite these deviations, further experiments on real-world datasets reveal that, in some non-BT and non-stationary datasets, Elo outperforms more complex algorithms such as mElo and pairwise models in predicting the next game outcome, while these complex models are designed to account for non-BT components in the data. These phenomenons demand a deeper explanation, going beyond the traditional understanding of Elo as the parameter estimation the BT model. In this paper, we explain the unexpected phenomenon through three key perspectives. 

% \begin{center}
% \textit{Why does Elo outperforms other algorithms even under model misspecification?}
% \end{center}

First, we interpret the Elo rating system through the lens of regret minimization. Specifically, Elo can be seen as an instance of online gradient descent---an online convex optimization (OCO) algorithm with sublinear regret guarantees, even in adversarial settings. This covers both non-stationary environments and data that deviate from the BT model. Consequently, Elo performs well as long as the best BT model in hindsight provides a reasonable fit to the data and sufficient data is available to keep regret small.

% Firstly, we propose to interpret Elo rating through the lens of regret-minimization. Specifically, Elo can be viewed as an instance of online gradient descent, an online convex optimization (OCO) algorithm that is guaranteed to have sublinear regret even under adversarial setups---which covers settings of nonstationarity and data coming from non-BT models. As a result, the Elo rating system enjoy good performance as long as the best BT-model \emph{in hindsight} can fit the data reasonably well, and we have sufficient data so that the regret is small.
% can provably achieve relatively low regret, which explains its strong predictive performance, even in non-BT datasets with non-trivial matchmaking and varying player strengths.

Second, we conduct synthetic experiments to systematically evaluate different algorithms in controlled settings. We test on transitive but non-BT games, including those following strongly and weakly stochastic transitive models, and introduce non-stationary factors such as Elo-based matchmaking and dynamic player strengths to better reflect real-world scenarios. Our findings reveal that data ``sparsity'' plays a crucial role in prediction performance, driven by a trade-off between model misspecification error and regret. In sparse datasets---where the number of games per player is low---regret becomes the dominant factor in performance, favoring simpler models like Elo, which incur lower regret despite higher misspecification error. In contrast, more complex models such as mElo and pairwise methods achieve lower misspecification error but suffer from a significantly higher regret. We confirm that many real-world games operate in this ``sparse'' regime, explaining Elo’s strong empirical performance. However, in ``dense'' regimes, where players engage in more games, Elo is outperformed by more complex models when applied to non-BT data.

% Secondly, through synthetic experiments, we extensively test the performance of different algorithms under carefully controlled environments. We test on games that are transitive but not BT, such as strongly or weakly stochastic transitive models. We also adopt non-stationary matchmaking according to Elo score as well as nonstationary true player strength to make it closer to practical scenarios. We show that the level of data sparsity plays a crucial role in the prediction performance of algorithms. This is the consequence of the trade off between model misspecification error and the regret. Specifically, in sparse datasets, the number of games played is relatively small compared to the number of players. While model misspecification error is always smaller for more complex models such as mElo or pairwise, here the regret contributes to the major part of the accumulated loss. Typically, the simplier the models are, the smaller the regrets are. We verify that real-world games often fall in this ``sparse'' regime, which explains the good performance of Elo models. We also observe that Elo is no longer the best algorithm for non-BT data in the ``dense'' regimes.

% Elo rating and its simple variants achieve good performance due to their low regret, even when the underlying game distribution is non-BT and non-stationary. However, in dense datasets, where the regret for all algorithms should approach zero, the model misspecification error becomes dominant in the accumulated loss. More sophisticated models tend to have smaller model misspecification error due to their greater model capacity, so Elo may not remain the best algorithm in these dense datasets.

% Secondly, through synthetic experiments, we extensively test the performance of different algorithms under different types of game results structures, such as strongly or weakly stochastic transitive models, under controlled environments. together with varying player strength and matchmaking by Elo score, we show that the level of data sparsity plays a crucial role in the prediction performance of algorithms. This is the consequence of the trade off between model misspecification error and the regret. Specifically, in sparse datasets, i.e., the number of games is small compared to the number of players, where regret contributes significantly to the accumulated loss, Elo rating and its simple variants achieve good performance due to their low regret, even when the underlying game distribution is non-BT and non-stationary. However, in dense datasets, where the regret for all algorithms should approach zero, the model misspecification error becomes dominant in the accumulated loss. More sophisticated models tend to have smaller model misspecification error due to their greater model capacity, so Elo may not remain the best algorithm in these dense datasets.


Finally, we also investigate the ranking performance of Elo. For pairwise ranking, we find a strong correlation between prediction accuracy and ranking accuracy. However, we caution that Elo should not be blindly trusted, as it can fail to produce consistent total orderings under arbitrary matchmaking, even in transitive datasets.

In summary, our contributions include:
(1) Demonstrate that real-world game data often violates the BT model via likelihood ratio tests.
(2) Show that Elo achieves strong predictive performance even in non-BT datasets.
(3) Interpret Elo through a regret-minimization framework, proving its effectiveness in nonstationary setting under model misspecification.
(4) Highlight the role of data sparsity in algorithms' prediction performance, with extensive synthetic and real-world experiments. (5) Explore the correlation between prediction accuracy and ranking, theoretically study Elo's ranking performance under different matchmaking setups.

% In summary, our contributions include:

% 	1.	Demonstrating that real-world game data often violates the BT model via likelihood ratio tests.
% 	2.	Showing that Elo achieves strong predictive performance even in non-BT datasets.
% 	3.	Interpreting Elo through a regret-minimization framework, proving its effectiveness as an online gradient descent algorithm.
% 	4.	Highlighting the role of data sparsity in algorithm performance, with synthetic experiments.
% 	5.	Exploring the correlation between prediction accuracy and ranking, while cautioning against blind trust in Elo’s rankings.



% \paragraph{Real-world games do not follow Bradley-Terry (BT).} 

% \paragraph{Regret minimization perspective of online algorithms}
% We examine the prediction performance of online rating algorithms through the lens of regret minimization.

% \paragraph{Sparsity level is critical for prediction} Our interpretation, together with experiments, suggest that sparsity level plays an important role in the prediction ability of rating algorithms. To be specific,
% \begin{itemize}
%     \item When data is sparse, Elo already achieves the best prediction accuracy, even though BT model is nonrealizable.
%     \item When data is dense, it is possible that algorithms based on more complex models have benefits.
% \end{itemize}

% \chij{I might change the last point to: Good prediction do have strongly correlation with good ranking, but ELO's ranking is not always trustworthy (even given the confidence bound).}
% % \paragraph{Elo does not always give consistent ranking} We theoretically show that even in strongly transitive games, Elo may not give consistent ranking under arbitrary matchmaking.

% \paragraph{Pairwise ranking performance is strongly correlated with prediction performance} Our synthetic experiments also show that good prediction have strong correlation with good (pairwise) ranking. Furthermore, Elo is able to give a total ordering, but the total ordering may not always be trustworthy, even with the confidence bound.



% Our findings suggest that for real-world where the Bradley-Terry model does not hold, the realibility of rating algorithms is highly influenced by data sparsity levels. Therefore, it is possible to choose or design algorithms tailored to different sparsity levels through the lens of regret minimization.




\subsection{Related work}

\paragraph{Methods for rating game players} A large number of rating methods used in practice can be viewed as variants of Elo, most notably Glicko~\citep{glickman1995glicko}, Glicko2~\citep{glickman2012example} and TrueSkill~\citep{herbrich2006trueskill}. A common characteristic shared by these methods is that they assume a scalar rating for players with parametric probabilistic model (Bradley-Terry in Glicko and Thurstone in TrueSkill) and make incremental gradient-like updates for each game or a small batch of games. 
% The focus of this work is on the vanilla Elo score, but the findings of this work could be relevant to other incremental-updates methods as well.
%They differ from gra-like methods by assuming a prior of  the Maximum a Posteriori (MAP) estimation. They differ from gradient-like incremental updates by being more 
mElo~\citep{balduzzi2018re} and Disk Decomposition~\citep{bertrand2023limitations} generalize Elo score by rating every player with a multi-dimensional vector instead of a scalar. Their approach can be understood as low-rank approximation of the logits of the winning probabilities. In our work we regard them as Elo2k, and examine their performance is a central part of our work.


Bayeselo~\citep{bayeselo} and WHR~\citep{coulom2008whole} are two popular Bayesian methods that are also based on the BT model. They differ from Elo-like incremental updates by requiring more compute to produce a maximum a posteriori estimator every step. 

% Computing such low rank approximations can take considerable compute, especially compared to light-weight updates made in Elo rating.
%, making them more suitable to rank static game outcomes than computing online updates.
%considering probabilistic models where every player has a multi-dimensional feature.
%Their methods require solving computationally expensive optimization problems, making them unsuitable for rating players in an online fashion.
%A different style of rating methods store and utilize 
%\yw{Elo, Glicko, TrueSkill, NashAverage, mElo, DD}

%There are also methods of evaluating the performance or skill of players that do not fit inside this framework. The Nash averaging~\citep{balduzzi2018re}
%There are also methods that do not rely on parametric models of winning probabilities.

\paragraph{Analysis of Elo score} Despite its popularity and wide applicability, the analysis of Elo score is ``curiously absent''~\citep{aldous2017elo}.
Elo discussed practical concerns and small-scale statistical validations of the method in~\citet{elo1978rating}. Most related to this work, however, is the proposal of the linear approximation of the update formula.
~\citet{aldous2017elo} proved the existence and uniqueness of a stationary distribution under the Elo update rules without assuming realizability. However, the nature of this stationary distribution is not explored.
~\citet{de2024stochastic} analyzed the convergence of Elo score assuming round-robin match making, realizability of the Bradley-Terry model and linearization of $\sigma$.
For more empirical and simulation results, see~\citet{kiraly2017modelling} and references within.
%Previous texts providing statistical analysis on Elo score include~\citet{elo1978rating},~\citet{aldous2017elo},~\citet{kiraly2017modelling}.


\paragraph{Misspecification of Bradley-Terry model} The Bradley-Terry model and similar parametric probabilistic preference models have been criticized for being inaccurate models of human preferences~\citep{ballinger1997decisions}.~\citet{oliveira2018new} show that for matches between $\sim$200 computer chess programs, Bradley-Terry model does not provide a good fit.~\citet{bertrand2023limitations} showed that by generalizing Bradley-Terry model to a $k$-dimensional model, the prediction performance on holdout test sets can be improved for synthetic datasets from~\citet{czarnecki2020real}, indicating misspecification of the original Bradley-Terry model. However, their synthetic datasets are simply payoff matrices between each player,
which differ significantly from real game datasets, where outcomes are typically binary (0-1) and largely sparse. Moreover, these works do not conduct statistical tests to assess model validity, particularly on large-scale real-world datasets of human gameplay.
% , where Elo is widely used.


% However, their synthetic datasets are payoff matrices between each player, which is very different from real game datasets that are usually 0-1 and most of the entries are missing. Also, no statistical test of the model is performed in these works, especially for larger-scale real-world datasets of human game play, on which the Elo score is routinely applied.

The Bradley-Terry model also implies the games being \emph{transitive}. However, the existence of cyclic or non-transitive behavior has been long observed in game theory literature~\citep{samothrakis2012coevolving,chen2016modeling,omidshafiei2019alpha,czarnecki2020real}. Although rejecting even weak notions of transitivity would automatically reject the BT model, doing so with hypothesis testing can be difficult for most real-world datasets where the majority of player pairs never played with each other.

\paragraph{Learning to rank}
%\yw{should we mention them, given we don't compare to those algorithms at all?}
There has been a long line of work studying various flavors of learning-to-rank (for instance, see~\citet{liu2009learning, negahban2012iterative, braverman2009sorting, shah2018simple} and references within), where the focus is to construct a global ranking based on a dataset partial observations. 
While highly relevant to task of rating game players, we note that these methods generally receive less attention in game-related applications. These methods are typically not able to predict win-loss probability of a particular matchup either.
%We believe this is in part due to the fact that games are not played to just elicit a ranking -- 
For these reasons, we focus on understanding Elo and the rating systems within the family of Elo in the scope of this work. We left the connection and comparison to other learning-to-rank methods as important future directions.