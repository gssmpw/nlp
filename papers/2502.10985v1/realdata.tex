\subsection{Elo achieves good performance under model misspecification}
\label{sec:realdata}
% Section \ref{sec:hypo} shows that the real-world games are non-BT and not stationary, implying the model misspecification when applying Elo rating. One may wonder whether algorithms based on more complex models (e.g., Elo2k and Pairwise win rate) will have advantage due to their better fit to the underlying game distribution. However, in this section, by examining the prediction accuracy of next outcome of online algorithms on real dataset \footnotemark\footnotetext{ For each dataset, we randomly permute it before conducting the algorithms. See Appendix \ref{sec:appendix-dataset} for details.}, we surprisingly find that Elo rating still achieves good prediction performance, even outperforming other algorithms in some of the non-BT datasets.

Section \ref{sec:hypo} establishes that real-world games do not follow a stationary BT model, highlighting model misspecification in the applicaton of the Elo rating system. This raises important concerns regarding Eloâ€™s reliability in practical settings. In particular, it prompts the question of whether more sophisticated rating algorithms, such as Elo2k or Pairwise, which may better capture the underlying game distributions, could yield improved predictive performance. However, we examine the prediction accuracy for the next game outcome of various online algorithms in real-world datasets, and surprisingly find that despite the model misspecification, ``Elo-like" algorithms still achieve strong predictive performance, outperforming complex algorithms even in some non-BT datasets. For each dataset, we compute the cumulative loss $\frac{1}{T} \mathcal{L}_T$ for Elo, Elo2k (with $k=4$), Glicko, TrueSkill, and Pairwise.\footnotemark\footnotetext{The experimental details can be found in Appendix \ref{sec:appendix-realdata}.} The results, summarized in Table \ref{tab:rating_results}, show that in several real-world datasets, including \texttt{Renju}, \texttt{Chess}, \texttt{Tennis}, \texttt{Scrabble}, \texttt{StarCraft} and \texttt{Go}, Elo and ``Elo-like" rating outperform more complexity rating systems such as Elo2k and Pairwise.


\begin{table}[t]
    \centering
    \small\resizebox{0.8\columnwidth}{!}{
    \begin{tabular}{|l || c c c | c c|}
        \hline
        Dataset & Elo & Glicko & TrueSkill & Elo2k & Pairwise \\
        \hline
        \texttt{Renju} & 0.6039 & 0.6100 & 0.5995 & 0.6109 & 0.6688 \\
        \texttt{Chess} & 0.6391 & 0.6349 & 0.6308 & 0.6387 & - \\
        \texttt{Tennis} & 0.6242 & 0.6232 & 0.6209 & 0.6365 & 0.6820 \\
        \texttt{Scrabble} & 0.6730 & 0.6766 & 0.6756 & 0.6787 & 0.6894 \\
        \texttt{StarCraft} & 0.5713 & 0.5689 & 0.5828 & 0.5832 & 0.6753 \\
        \texttt{Go} & 0.6443 & 0.6375 & 0.6321 & 0.6372 & - \\
        \texttt{LLM Arena} & 0.6607 & 0.6602 & 0.6611 & 0.6611 & 0.6619 \\
        \texttt{Hearthstone} & 0.6898 & 0.6893 & 0.6894 & 0.6847 & 0.6853 \\
        \hline
    \end{tabular}}
    \caption{Performance of different rating algorithms across various games}
    \label{tab:rating_results}
\end{table}

% For each dataset, we evaluate the performance of Elo, Elo2k (with $k=4$), Glicko, TrueSkill, and Pairwise, plotting the cumulative loss $\frac{1}{t} \mathcal{L}_t$ over "normalized" time $t/N$. For Elo and Elo2k, we adopt a decaying learning rate scheme: $\eta_t = \sqrt{\frac{N}{t}}$, following the standard approach used in gradient descent. For Glicko and TrueSkill, hyperparameters are carefully tuned to optimize their performance. The results are presented in Figure \ref{fig:realdata-CE}.





% In Figure \ref{fig:realdata-CE}, we can see that, for many real dataset including \texttt{chess}, \texttt{go}, \texttt{renju}, \texttt{tennis}, \texttt{scrabble} and \texttt{5,4-Blotto sparse}, Elo and its variants (TrueSkill and Glicko) beat algorithms with more complex underlying models such as Elo2k and Pairwise (Elo has smaller cumulative loss compaired with Elo2k for every $0<t<T$). On the other hand, for some other datasets such as \texttt{Hearthstone}, \texttt{AlphaStar}, \texttt{10,5-Blotto}, \texttt{go-dense}, \texttt{mixedchess-dense}, Elo2k achieves smaller prediction error than Elo at time step $t=T$. 


% For other datasets like \texttt{Hearthstone}, \texttt{AlphaStar}, \texttt{10,5-Blotto}, \texttt{go-dense}, and \texttt{mixedchess-dense}, Elo2k achieves lower prediction errors than Elo at the final time $t=T$.


% \chijin{why we already mention sparsity here? Maybe just give a table here?} 

