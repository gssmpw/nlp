\section{Details of real-world data experiments}
\label{sec:appendix-realdata}
For each dataset, we evaluate the performance of Elo, Elo2k (with $k=4$), Glicko, TrueSkill, and Pairwise, plotting the cumulative loss $\frac{1}{t} \mathcal{L}_t$ over "normalized" time $t/N$. For each algorithm, we choose the best hyperparameter (see Appendix \ref{sec:appendix-choosing_parameter}). We also plot the in hindsight baselines at time $T$ ($\min_{x \in \mathcal{K}}\frac1T \sum_{t=1}^{T}f_t(x)$) of BT model and Elo2k model.  The results are presented in Figure \ref{fig:realdata-CE}. We can observe that for several real-world datasets, including \texttt{chess}, \texttt{go}, \texttt{renju}, \texttt{tennis}, \texttt{scrabble} and \texttt{StarCraft},
Elo and its variants (TrueSkill and Glicko) outperform algorithms based on more complex models such as Elo2k and Pairwise. Namely, Elo consistently exhibits a lower cumulative loss compared to Elo2k for every $0<t<T$. For other datasets like \texttt{Hearthstone}, \texttt{AlphaStar}, \texttt{10,5-Blotto}, \texttt{go-dense}, and \texttt{mixedchess-dense}, Elo2k achieves lower prediction errors than Elo at the final time $t=T$.
\begin{figure}[t]

    \centering
    \includegraphics[width=\columnwidth]{figures/all-datasets-test-CE-new.png}
    
    \caption{\textbf{In real datasets, sparsity strongly influences prediction.} Elo, TrueSkill, Glicko achieves the best prediction in sparse datasets, while Elo2k and Pairwise outperforms Elo and its variants in dense datasets.}
    \label{fig:realdata-CE}
\end{figure}

% \subsection{Re-investigating real-data experiments: data sparsity level is crucial}

We can further investigate the results through the lens of regret minimization. We can see that the cumulative loss for each algorithm decreases over time, indicating the regret minimization effect of those algorithms. However the behavior for each algorithm at each sparsity level $t/N$ are not the same. These phenomenons are closely related to the sparsity level of dataset: when data is sparse, typically when $t/N<1000$, the regret for Elo2k and Pairwise is so large, that even though the in hindsight baseline is much better, the cumulative loss $\frac{1}{t}\mathcal{L}_{t}$ for Elo2k will be large due to the large regret. Meanwhile, Elo achieves good performance due to its small regret. This may due to the fact that Elo, as online gradient descent for convex loss, has provable regret guarantees (Theorem \ref{thm:OCO}) that ensures its performance. On the contrary, Elo2k suffer from its non-convex nature, and Pairwise has a much larger regret due to its parameter size of $N^{2}$ that is much larger then $N$, the Elo parameter size. When dataset is dense enough, for example, AlphaStar-dense, when $T/N >1000$, we can see that the regret at time $T$ for both Elo2k and Elo are very small. In this regime, model capacity come into play. The baseline for Elo2k model is much smaller than the Elo counter part, therefore Elo2k shows better prediction accuracy than Elo.  Among these dense datasets, LLM is special, since the Elo2k baseline and the Elo baseline are so close, that even the dataset is dense, Elo2k does not show any benefit.


We can futher see the influence of sparsity level, when we examine the dataset from \citet{czarnecki2020real}: for AlphaStar, 5,4-Blotto and 10,5-Blotto, we create sparse version and dense version, where the underlying model is exactly the same, but "dense" version has 10 times sample size than "sparse" version. Through the comparison of these datasets, we can see that even under the same probabilistic model (which is non-BT), the behaviors of algorithms are still mainly affected by the sparsity level. 
%