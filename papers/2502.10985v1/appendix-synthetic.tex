\section{Details of synthetic experiments}


\subsection{Constructing $P$ for transitive models}
\label{sec:appendix-generatingP}

We consider several  ways of generating a SST/WST matrix $P$ w.r.t. the ordering $\pi(i)=N-i$. In the following constructions, we will firsts specify $P_{ij}$ for $i<j$, then make the matrix skew-symmetric by setting $P_{ij}=1-P_{ji}$ for $i>j$, and $P_{ii}=0.5$. 

\paragraph{SST-byrow} We first generate a i.i.d. random sequence of length $N-1$, each element is sampled from $\text{Uni}([0,1])$. Then we sort this sequence in a descending order $r_1 \geq r_2 \geq \cdots \geq r_{N-1} $. We let $P_{ij}=0.5+0.5 \times r_i$ for $i<j$.

\paragraph{SST-bydiagonal} We first get the descending sequence $r_1 \geq r_2 \geq \cdots \geq r_{N-1} $ in the same way as SST-byrow. We let $P_{ij}=0.5+0.5 \times r_{N-j+i}$ for $i<j$.

\paragraph{SST-byentry} 
Following the "noisy sorting" model, we set 
$P_{ij}=0.6$ for $i<j$.


% \paragraph{SST-hard}
% Let $k=2.5, \delta=0.5,  \gamma=3.0, M=\lfloor N/k \rfloor$. We let \begin{align*}
% P_{ij} = 0.5 + \lfloor (\frac{j-i}{M})^{\gamma} \rfloor  \times (\frac{M}{N})^{\gamma} \times \delta
% \end{align*}
% for $i<j$.

\paragraph{WST-byrow} We first generate a i.i.d. random sequence of length $N-1$, each element is sampled from $\text{Uni}([0,1])$. Then we sort this sequence in an ascending  order $r_1 \leq r_2 \leq \cdots \leq r_{N-1} $. We let $P_{ij}=0.5+0.5 \times r_i$ for $i<j$.

\paragraph{WST-bydiagonal} We first get the ascending sequence $r_1 \leq r_2 \leq \cdots \leq r_{N-1} $ in the same way as WST-byrow. We let $P_{ij}=0.5+0.5 \times r_{N-j+i}$ for $i<j$.

\paragraph{WST-byentry} 
We set $P_{ij}=0.5+0.5 \times U_{ij}$ for $i<j$, where $U_{ij}\sim \text{Uni}[0,1]$.


% \paragraph{WST-hard}
% Let $\text{bandwidth}=20,  \text{period}=1000$. For $i<j$, let $r_i:=\lfloor \frac{i}{\text{bandwidth}} \rfloor$,
% $c_j=\lfloor \frac{j}{\text{bandwidth}} \rfloor$. If $r_i=c_j$, then let $P_{ij}=0.5$. Otherwise we set
% \begin{align*}
% P_{ij} = \min \{1.0, \max \{ 0.5,0.5+0.5 \times (1-\frac{r_i \times \text{bandwidth}}{N}) \times \cos \bigg( \frac{2\pi j} {\text{period}} \bigg)  \} \}.
% \end{align*}

% \subsection{Constructing non-transitive models} 
% We consider the following two types of non-transitive models:

% \paragraph{Elo2k} We first generate $U \in \mathbb{R}^{N \times k}$ and $V \in \mathbb{R}^{N \times k}$, where each entry of $U$ and $V$ are independently drawn from $\text{Uni}([0,1])$. Let $P:=UV^{T} - VU^{T}$.


% \paragraph{Pure-random}
% We draw $P_{ij}$ from $\text{Uni}([0,1])$, independently for $i<j$. Then we make the matrix skew-symmetric by setting $P_{ij}=1-P_{ji}$ for $i>j$, and $P_{ii}=0.5$.

\subsection{Choosing the best hyperparameter}

\label{sec:appendix-choosing_parameter}

For each algorithm (Elo, Elo2k, Glicko and TrueSkill), there are different hyperparameters that need to be chosen. We choose the parameters according to the follow criterion:

Let $CE_i := \frac{1}{t}\mathcal{L}_{t_i}$ be the CrossEntropy Loss at time $t_i$, where $\{t_i\}_{i=1}^n$ be the time steps we collect the loss.
Define the threshold $ M = \log(2) $ (A purely random prediction will have a loss of $M$). The loss function  $L(\mathbf{v})$ is given by:
 can be expressed as:
$$
L = \sum_{i=1}^{30} \big( CE_i + 5 (CE_i - M) \mathbb{I}(CE_i > M) \big).
$$
We select the hyperparameter that minimizes the loss $L$ . This loss function ensures that the chosen parameter achieves a consistently low average cross-entropy (CE) loss throughout the process while avoiding overfitting at some point (where  $CE_i > M$  indicates overfitting).

\subsection{Creating non-stationary datasets}
\label{sec:appendix-non_stationary}

Specifically, for modeling the varying player strength, for each type of underlying distribution (e.g., \texttt{SST, byrow}), we generate two matrices $P^0$ and $P^T$, and let $P^t = (1-t/T) \times P^0 + (t/T) \times P^T$ be the win rate matrix at each time $t$. That is, $\E[o_t|i_t=i,j_t=j]=P^t_{ij}$.



For modeling non-trivial matchmaking, we construct the game dataset as the following: at each time point $t$, we sample $i_t \sim \text{Uni}([N])$, and then sample $j_t$ uniformly from the players that has ranking (by the real-time Elo score) within distance $K/2$ from $i_t$'s ranking. To be more concrete, let the ranking induced by Elo scores $(\theta[1], \cdots, \theta[N])$ be $\pi=(\pi(1), \cdots, \pi(N))$, a permutation of $(1,2,\cdots,N)$ such that $\theta [\pi^{-1}(N)] > \theta [\pi^{-1}(N-1)] > \cdots > \theta [\pi^{-1}(1)] $. Then $j_t$ is chosen uniformly from the set $\{j \in [N]: |\pi(j)-\pi(i_t)| \leq K/2\}$. We choose $K=N/5$. After constructing such a game dataset, we fix this dataset and plot the performance of each algorithm