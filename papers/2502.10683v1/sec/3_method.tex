\section{Methodology}
\input{sec/Figures/Main_figure}
DETR has three components: a backbone, an encoder-decoder transformer, and a set of learnable object queries. The backbone generates CNN features $F \in \mathbb{R}^{H\times W\times C}$, where $H$, $W$, and $C$ correspond to the height, width, and number of channels, respectively. These CNN features then pass through the transformer encoder. The output is referred to as DETR memory, denoted as $A \in \mathbb{R}^{HW\times D}$, where $D$ is the embedding size of the transformer. Most existing knowledge distillation works focus on the CNN features from the backbone but overlook the memory. However, we argue that it is the self-attention in the encoder layers that provides each pixel with global context and long-range dependencies, in addition to local feature refinement. Such information helps the model understand the context of an object within the entire image. Therefore, it should be included in the distillation process.
In this paper, we propose to perform distillation on this memory rather than on the backbone features, thereby leveraging the enriched transformer-specific contextual information. Furthermore, we exploit ground truth to emphasize object-related areas in memory, enabling the student detector to focus on the most relevant features without worrying about losing the global context.
On the decoder side, $N$ object queries $Q \in \mathbb{R}^{N\times D}$ first undergo self-attention to interact with each other before proceeding to cross-attention with the encoder's output memory. Instead of utilizing all learnable queries, we introduce target-aware distillation queries that use fixed ground truth information, remaining consistent between the student and teacher models, to ensure precise identification of informative areas during distillation. Each query is subsequently decoded into a bounding box prediction (coordinates $\hat{b}$ and class scores $\hat{c}$) by a feed-forward network (FFN). 
%. Here, $\hat{b}_i = (\hat{b}_{x_i}, \hat{b}_{y_i}, \hat{b}_{w_i}, \hat{b}_{h_i})$, where $i$ refers to the $i$-th query prediction out of $N$, and $(x, y, w, h)$ correspond to the bounding box's center coordinates, width, and height. 
Figure \ref{fig:overview} offers an overview of our CLoCKDistill framework and we will delve into the details in the upcoming subsections.


\subsection{Location-and-context-aware Memory Distillation}
\label{sec:location-and-context-aware Memory Distillation}

Feature-based distillation methods have shown promising performance for both CNN and transformer-based detectors \cite{wang2019distilling, yang2022focal, chang2023detrdistill}. They distill features from convolution layers, such as those in the Feature Pyramid Network (FPN). The objective function for CNN-feature distillation is formulated as:
\begin{equation}
\mathcal{L}_{\text{fea}} = \frac{1}{CHW} \sum_{k=1}^{C} \sum_{i=1}^{H} \sum_{j=1}^{W} \left( F^T_{k,i,j} - f(F^S_{k,i,j}) \right)^2 \,,
\end{equation}
\noindent where $F^T$ and $F^S$ denote the features from the teacher and student models, respectively, and $f$ is an adaptation layer that reshapes $F^S$ to match the dimensions of $F^T$. However, these methods do not distinguish between features of different parts of the image, such as the foreground and the background. More importantly, they fail to capture the global and long-range relationships among different pixels. To address those problems in existing knowledge distillation methods, we propose our Location-and-context-aware memory distillation for DETR models.

We use encoder memory for distillation because it can well capture the global context and long-range dependencies for each pixel through the encoder self-attention mechanism. As demonstrated in Table~\ref{tab:Preliminary}, distilling the memory yields better performance compared to distilling the backbone features. Also, although the encoder memory is flattened, it still retains the spatial order. The $p$-th pixel/point in the encoder memory can be converted to 2-D spatial coordinates through the following conversion:
\begin{equation}
    O(p) = (\left\lfloor \frac{p}{W} \right\rfloor, p\bmod W) \,,
\end{equation}
\noindent where $W$ is the width of the backbone feature map. %$\lfloor \rfloor$ and $mod$ stand for the floor and the modulo operations, respectively.
To direct the student model's attention to more relevant memory areas, we draw upon ground truth location information to differentiate between the foreground and the background. To this end, we design a location-aware mask $M$:
\begin{equation}
M_{p}= \left\{
\begin{array}{cl}
1,\quad & \text{if} \quad O(p)\in \mathcal{G}\\
0,\quad & \text{otherwise}
\end{array} \right. \,,
\end{equation}
\noindent which takes a value of 1 when the converted 2D position of the $p$-th memory point falls within a ground truth bounding box $\mathcal{G}$, and 0 otherwise.

Moreover, we need to balance the contributions from objects of different sizes and consider the varying foreground/background ratios in different images. Larger-scale objects contribute more to the loss because of their greater pixel coverage, which can adversely affect the distillation attention given to smaller objects. 
Also, when the vast majority of pixels in an image are background, the background's influence overwhelms the attention given to foreground objects.
To address these issues, we design a scale mask $S$ as:
\begin{equation}
S_{p} = \left\{
\begin{array}{cl}
\frac{1}{H_k W_k}, & \text{if} \quad O(p) \in BBox_k \\
\frac{1}{N_{bg}}, & \text{otherwise}
\end{array} \right. \,,
\end{equation}
\noindent where $H_k$ and $W_k$ represent the height and width of the $k$-th ground-truth bounding box $BBox_k$, and $N_{bg} = \sum_{i=1}^{H} \sum_{j=1}^{W} (1 - M_{i,j})$. If a pixel belongs to multiple objects, we choose the smallest one to calculate $S$.

We apply the $M$ and $S$ masks to both the teacher and student models' encoder memory, aiming to assist the student in identifying and learning the most relevant and context-aware knowledge from the teacher. Our location-and-context-aware memory distillation loss is defined as:
\begin{equation}\label{eq:lcmd}
\begin{aligned}
    \mathcal{L}_{\text{lcmd}} & = 
      \alpha \sum_{p=1}^{P} \sum_{d=1}^{D} M_{p} S_{p} ( A^T_{p,d} - A^S_{p,d})^2\\
     & + \beta \sum_{p=1}^{P} \sum_{d=1}^{D} (1- M_{p}) S_{p} ( A^T_{p,d} - A^S_{p,d})^2 \,.
\end{aligned}
\end{equation}
\noindent where $A^T$ and $A^S$ denote the memories of the teacher and student detectors, respectively. $P$ is the number of memory points across all locations and scales, and $D$ represents the embedding size of the transformer. $\alpha$ and $\beta$ are balancing hyperparameters. It is worth mentioning that when the backbone CNN features span multiple scales, as in Deformable DETR \cite{zhu2020deformable} and DINO \cite{zhang2023dino}, $P = \sum_{l=1}^L H_l W_l$. Correspondingly, we project $M$ and $S$ to a specific scale level $l$, yielding $M'_l \in \mathbb{R}^{P_l\times 1}$ and $S'_l \in \mathbb{R}^{P_l\times 1}$. These projections are then applied to the memory at that scale $A_l \in \mathbb{R}^{P_l\times D}$ to obtain location-and-context-aware memory for that specific scale. Finally, we concatenate the location-and-context-aware memories from all scales and distill them together.

Our location-and-context-aware memory distillation enhances student learning by leveraging location information from ground truth and rich contextual information available for each pixel in the memory. Both contribute to more effective feature-level knowledge transfer. 

\subsection{Target-aware Consistent Logit Distillation}

Apart from feature distillation, logit distillation has also been shown beneficial for detection tasks \cite{zheng2022localization, wang2024knowledge}. However, in DETR detectors, queries are disordered, resulting in inconsistent prediction matching between the teacher and student models, which hinders effective logit distillation. To address this issue, we propose target-aware distillation queries that incorporate ground truth information to precisely and consistently pinpoint the most informative memory areas for the student to learn from the teacher model. As shown in Figure~\ref{fig:overview}, both the student and teacher models employ the same set of target-aware queries, which remain unlearnable throughout the distillation process.

We utilize both the class label and the position parameters of a ground truth bounding box to generate its corresponding target-aware query for distillation. 
We first employ an embedding layer $\text{Embed}_c$ to convert a category label into a $D$-dimensional vector representation, i.e., content query:
\begin{equation}
    q_{\text{cont}} = \text{Embed}_c(c) \,.
\end{equation}
\noindent Then, we use a multilayer perceptron ($\text{MLP}$) to project the ground truth bounding box parameters (i.e., center coordinates, width, and height) to what we call positional query:
\begin{equation}
    q_{\text{pos}} = \text{MLP}\left(b_{x}, b_{y}, b_{w}, b_{h}\right) \,.
\end{equation}
\noindent Finally, we sum the two $D$-dimensional vectors $q_{cont}$ and $q_{pos}$ to produce our target-aware query $q_{\text{target}}$:
\begin{equation}
q_{\text{target}} = q_{\text{cont}} + q_{\text{pos}} \,.
\end{equation}
It is worth noting that the proposed target-aware queries can be seamlessly combined with other successful queries for enhanced performance. In our experiments, we also include the queries identified as effective in \cite{wang2024knowledge}.
Our distillation queries pass through several stages of the decoder, each involving self-attention and cross-attention mechanisms. Cross-attention enables the queries to focus on relevant parts of our location-and-context-aware memory, before making predictions $\{\hat{\mathbf{c}}, \hat{\mathbf{b}}\}$, where $\hat{\mathbf{c}}$ indicates (unnormalized) class scores or logits, and $\hat{\mathbf{b}}$ stands for corresponding bounding boxes.
Considering all $E$ decoder stages and $G$ distillation points/queries, we define our target-aware consistent logit distillation loss as follows:
\begin{equation}\label{eq:tcld}
\begin{aligned}
\mathcal{L}_{\text{tcld}} & = \sum_{e=1}^E\sum_{g=1}^{G} w_{e,g} [ \lambda_{\text{cls}} \mathcal{L}_{\text{KL}} (\sigma(\frac{\hat{\mathbf{c}}_{e,g}^t}{T}) \parallel \sigma(\frac{\hat{\mathbf{c}}_{e,g}^s}{T})) + \\
&\lambda_{\text{L1}} \mathcal{L}_{\text{L1}} (\hat{\mathbf{b}}_{e,g}^s, \hat{\mathbf{b}}_{e,g}^t) 
+ \lambda_{\text{GIoU}} \mathcal{L}_{\text{GIoU}} (\hat{\mathbf{b}}_{e,g}^s, \hat{\mathbf{b}}_{e,g}^t)],
\end{aligned}
\end{equation}
\noindent where superscripts $t$ and $s$ indicate the teacher and student models, respectively. $\sigma$ stands for the softmax function. Temperature $T$ controls the smoothness of the output distribution. $\mathcal{L}_{\text{KL}}, \mathcal{L}_{\text{L1}}$, and $\mathcal{L}_{\text{GIoU}}$ represent KL-divergence, L1, and GIoU losses, respectively, with $\lambda_{\text{cls}}, \lambda_{\text{L1}}$, and $\lambda_{\text{GIoU}}$ as their corresponding balancing hyperparameters. We define
\begin{equation}
    w_{e,g} = \max_{c \in [0, C]} \sigma (c^t_{e,g})
\end{equation}
\noindent to emphasize distillation queries with higher teacher confidence. 
Adding $\mathcal{L}_{\text{lcmd}}$ and $\mathcal{L}_{\text{tcld}}$ to the original detection loss $\mathcal{L}_{\text{det}}$, we obtain our total training loss for the student model:
\begin{equation}
    \mathcal{L} = \mathcal{L}_{\text{lcmd}} + \mathcal{L}_{\text{tcld}} + \mathcal{L}_{\text{det}} \,.
\end{equation}
