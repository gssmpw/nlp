\section{Introduction}
\label{sec:intro}
Early deep learning detection approaches predominantly rely on convolutional neural networks (CNNs) to process regional features, involving extensive tuning of hand-crafted components such as anchor sizes and aspect ratios \cite{ren2015faster, lin2017focal}. Inspired by the success of transformers in NLP and visual classification tasks, DEtection TRansformer (DETR) \cite{carion2020end} has been introduced for visual detection. DETR treats object detection as a set prediction problem, eliminating the need for hand-crafted anchor design \cite{ren2015faster} and non-maximum suppression (NMS) \cite{hosang2017learning}. Despite achieving state-of-the-art performance, transformer-based detectors \cite{carion2020end, zhu2020deformable} suffer from high computational costs, making them challenging to deploy in real-time applications (e.g., autonomous driving perception).
Knowledge distillation (KD) \cite{hinton2015distilling}, a technique where a smaller model is trained to mimic a larger model's behavior, has shown to be an effective compression method for convolution-based detectors \cite{zhang2020improve, li2017mimicking, wang2019distilling, dai2021general, chen2017learning, dai2021general, zheng2022localization}. However, the exploration of knowledge distillation in DETRs is still in its early stages, with several critical issues remaining.

\paragraph{Issues in feature-level distillation} Most feature-level distillation methods, e.g., \cite{chang2023detrdistill, yang2022focal, guo2021distilling}, focus on the backbone\footnote{In this paper, the term backbone is used loosely to encompass both the traditional backbone network and the neck layers (e.g., FPN).} features that capture only local context from CNNs.
Additionally, previous methods \cite{li2017mimicking, zhang2020improve} often rely on the teacher model in all locations, which can sometimes mislead the student model. Also, these works typically fail to balance the contributions from the foreground and background as well as from objects of different sizes. \citet{ijcai2024p74} focus exclusively on decoder distillation, overlooking the crucial context information embedded in the encoder features. 
%\citet{chang2023detrdistill} utilizes the teacherâ€™s queries to interact with convolution-based FPN features. 
%However, those approaches all fail to leverage the richer global context available in the encoder outputs.
To address these challenges, we propose using location-and-context-aware DETR memory (i.e., transformer encoder output) as a more appropriate alternative for distillation. The encoder processes the sequence of token embeddings, including the positional encoding, through multiple layers of self-attention and feed-forward neural networks. Unlike local CNN features that most previous works use, the memory in DETR captures the transformer-specific global context provided by self-attention, allowing for better reasoning about long-range relationships within the entire image, which is crucial for object detection \cite{hu2018relation}.
Table~\ref{tab:Preliminary} demonstrates the superiority of distilling the memory over distilling backbone features or using a combination of both.
\input{sec/Tables/Pre}
Also, while the DETR self-attention mechanism effectively captures complex dependencies between tokens, it does not inherently alter the sequence order.
As illustrated in the attention maps of Figure \ref{fig:activate region}, both convolutional features and those derived from self-attention maintain a strong indication of spatial locations, with active regions consistently focusing on the foreground.
\input{sec/Figures/Activate_region}
Motivated by this observation, we propose transforming the ground truth location information to emphasize relevant, contextually-enriched features in the flattened DETR memory during distillation.
%and mitigates inheriting their errors. 
%Notably, we are the first to utilize location-and-context-aware memory for distillation.

\paragraph{Issues in distilling DETR logits}
DETR logit-level distillation faces stability issues due to mismatched distillation points between the teacher and student models. Unlike convolution-based detectors with a fixed spatial arrangement of anchors, DETR's unordered box predictions lack a natural one-to-one correspondence, complicating the alignment between the teacher and student outputs.
\citet{chang2023detrdistill} and \citet{ijcai2024p74} rely on Hungarian matching for teacher-student logit alignment, but this approach results in many meaningless/background predictions being matched unstably.
While \citet{wang2024knowledge} propose consistent distillation points to guide the process, these points are generated randomly or solely based on the fallible teacher model, ignoring the precise location information readily available from the ground truth during distillation. To overcome these challenges, we propose leveraging both category and location information from the ground truth to generate consistent target-aware queries. These queries are designed to be unlearnable and are fed into both the student and teacher models, ensuring that the distillation process precisely and consistently identifies the most informative (and global-context-aware) feature areas for effective knowledge transfer.

In summary, to address the unsolved challenges in DETR distillation at both the feature and logit levels, we propose Consistent Location-and-Context-aware Knowledge Distillation (CLoCKDistill). Our method takes advantage of transformer-specific global context and incorporates ground truth information into both the encoder memory and decoder query for consistent and location-informed distillation. Our contributions are threefold:

\begin{itemize}
    \item We propose global-context-aware feature distillation by utilizing the contextually rich representations in the DETR memory (encoder output). Unlike CNN-generated backbone features, DETR memory is more refined and captures long-range dependencies through self-attention.
    \item Furthermore, we take advantage of the implicit token ordering in DETR memory and utilize ground truth location data to direct feature distillation attention toward more relevant features.
    \item Finally, for logit distillation, we design target-aware decoder queries. These ground-truth-informed queries provide consistent and precise spatial guidance to both the teacher and student models on where to focus in memory when generating logits for distillation.
\end{itemize}










% Please follow the steps outlined below when submitting your manuscript to the IEEE Computer Society Press.
% This style guide now has several important modifications (for example, you are no longer warned against the use of sticky tape to attach your artwork to the paper), so all authors should read this new version.

% %-------------------------------------------------------------------------
% \subsection{Language}

% All manuscripts must be in English.

% \subsection{Dual submission}

% Please refer to the author guidelines on the \confName\ \confYear\ web page for a
% discussion of the policy on dual submissions.

% \subsection{Paper length}
% Papers, excluding the references section, must be no longer than eight pages in length.
% The references section will not be included in the page count, and there is no limit on the length of the references section.
% For example, a paper of eight pages with two pages of references would have a total length of 10 pages.
% {\bf There will be no extra page charges for \confName\ \confYear.}

% Overlength papers will simply not be reviewed.
% This includes papers where the margins and formatting are deemed to have been significantly altered from those laid down by this style guide.
% Note that this \LaTeX\ guide already sets figure captions and references in a smaller font.
% The reason such papers will not be reviewed is that there is no provision for supervised revisions of manuscripts.
% The reviewing process cannot determine the suitability of the paper for presentation in eight pages if it is reviewed in eleven.

% %-------------------------------------------------------------------------
% \subsection{The ruler}
% The \LaTeX\ style defines a printed ruler which should be present in the version submitted for review.
% The ruler is provided in order that reviewers may comment on particular lines in the paper without circumlocution.
% If you are preparing a document using a non-\LaTeX\ document preparation system, please arrange for an equivalent ruler to appear on the final output pages.
% The presence or absence of the ruler should not change the appearance of any other content on the page.
% The camera-ready copy should not contain a ruler.
% (\LaTeX\ users may use options of \texttt{cvpr.sty} to switch between different versions.)

% Reviewers:
% note that the ruler measurements do not align well with lines in the paper --- this turns out to be very difficult to do well when the paper contains many figures and equations, and, when done, looks ugly.
% Just use fractional references (\eg, this line is $087.5$), although in most cases one would expect that the approximate location will be adequate.


% \subsection{Paper ID}
% Make sure that the Paper ID from the submission system is visible in the version submitted for review (replacing the ``*****'' you see in this document).
% If you are using the \LaTeX\ template, \textbf{make sure to update paper ID in the appropriate place in the tex file}.


% \subsection{Mathematics}

% Please number all of your sections and displayed equations as in these examples:
% \begin{equation}
%   E = m\cdot c^2
%   \label{eq:important}
% \end{equation}
% and
% \begin{equation}
%   v = a\cdot t.
%   \label{eq:also-important}
% \end{equation}
% It is important for readers to be able to refer to any particular equation.
% Just because you did not refer to it in the text does not mean some future reader might not need to refer to it.
% It is cumbersome to have to use circumlocutions like ``the equation second from the top of page 3 column 1''.
% (Note that the ruler will not be present in the final copy, so is not an alternative to equation numbers).
% All authors will benefit from reading Mermin's description of how to write mathematics:
% \url{http://www.pamitc.org/documents/mermin.pdf}.

% \subsection{Blind review}

% Many authors misunderstand the concept of anonymizing for blind review.
% Blind review does not mean that one must remove citations to one's own work---in fact it is often impossible to review a paper unless the previous citations are known and available.

% Blind review means that you do not use the words ``my'' or ``our'' when citing previous work.
% That is all.
% (But see below for tech reports.)

% Saying ``this builds on the work of Lucy Smith [1]'' does not say that you are Lucy Smith;
% it says that you are building on her work.
% If you are Smith and Jones, do not say ``as we show in [7]'', say ``as Smith and Jones show in [7]'' and at the end of the paper, include reference 7 as you would any other cited work.

% An example of a bad paper just asking to be rejected:
% \begin{quote}
% \begin{center}
%     An analysis of the frobnicatable foo filter.
% \end{center}

%    In this paper we present a performance analysis of our previous paper [1], and show it to be inferior to all previously known methods.
%    Why the previous paper was accepted without this analysis is beyond me.

%    [1] Removed for blind review
% \end{quote}


% An example of an acceptable paper:
% \begin{quote}
% \begin{center}
%      An analysis of the frobnicatable foo filter.
% \end{center}

%    In this paper we present a performance analysis of the  paper of Smith \etal [1], and show it to be inferior to all previously known methods.
%    Why the previous paper was accepted without this analysis is beyond me.

%    [1] Smith, L and Jones, C. ``The frobnicatable foo filter, a fundamental contribution to human knowledge''. Nature 381(12), 1-213.
% \end{quote}

% If you are making a submission to another conference at the same time, which covers similar or overlapping material, you may need to refer to that submission in order to explain the differences, just as you would if you had previously published related work.
% In such cases, include the anonymized parallel submission~\cite{Authors14} as supplemental material and cite it as
% \begin{quote}
% [1] Authors. ``The frobnicatable foo filter'', F\&G 2014 Submission ID 324, Supplied as supplemental material {\tt fg324.pdf}.
% \end{quote}

% Finally, you may feel you need to tell the reader that more details can be found elsewhere, and refer them to a technical report.
% For conference submissions, the paper must stand on its own, and not {\em require} the reviewer to go to a tech report for further details.
% Thus, you may say in the body of the paper ``further details may be found in~\cite{Authors14b}''.
% Then submit the tech report as supplemental material.
% Again, you may not assume the reviewers will read this material.

% Sometimes your paper is about a problem which you tested using a tool that is widely known to be restricted to a single institution.
% For example, let's say it's 1969, you have solved a key problem on the Apollo lander, and you believe that the 1970 audience would like to hear about your
% solution.
% The work is a development of your celebrated 1968 paper entitled ``Zero-g frobnication: How being the only people in the world with access to the Apollo lander source code makes us a wow at parties'', by Zeus \etal.

% You can handle this paper like any other.
% Do not write ``We show how to improve our previous work [Anonymous, 1968].
% This time we tested the algorithm on a lunar lander [name of lander removed for blind review]''.
% That would be silly, and would immediately identify the authors.
% Instead write the following:
% \begin{quotation}
% \noindent
%    We describe a system for zero-g frobnication.
%    This system is new because it handles the following cases:
%    A, B.  Previous systems [Zeus et al. 1968] did not  handle case B properly.
%    Ours handles it by including a foo term in the bar integral.

%    ...

%    The proposed system was integrated with the Apollo lunar lander, and went all the way to the moon, don't you know.
%    It displayed the following behaviours, which show how well we solved cases A and B: ...
% \end{quotation}
% As you can see, the above text follows standard scientific convention, reads better than the first version, and does not explicitly name you as the authors.
% A reviewer might think it likely that the new paper was written by Zeus \etal, but cannot make any decision based on that guess.
% He or she would have to be sure that no other authors could have been contracted to solve problem B.
% \medskip

% \noindent
% FAQ\medskip\\
% {\bf Q:} Are acknowledgements OK?\\
% {\bf A:} No.  Leave them for the final copy.\medskip\\
% {\bf Q:} How do I cite my results reported in open challenges?
% {\bf A:} To conform with the double-blind review policy, you can report results of other challenge participants together with your results in your paper.
% For your results, however, you should not identify yourself and should not mention your participation in the challenge.
% Instead present your results referring to the method proposed in your paper and draw conclusions based on the experimental comparison to other results.\medskip\\

% \begin{figure}[t]
%   \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%    %\includegraphics[width=0.8\linewidth]{egfigure.eps}

%    \caption{Example of caption.
%    It is set in Roman so that mathematics (always set in Roman: $B \sin A = A \sin B$) may be included without an ugly clash.}
%    \label{fig:onecol}
% \end{figure}

% \subsection{Miscellaneous}

% \noindent
% Compare the following:\\
% \begin{tabular}{ll}
%  \verb'$conf_a$' &  $conf_a$ \\
%  \verb'$\mathit{conf}_a$' & $\mathit{conf}_a$
% \end{tabular}\\
% See The \TeX book, p165.

% The space after \eg, meaning ``for example'', should not be a sentence-ending space.
% So \eg is correct, {\em e.g.} is not.
% The provided \verb'\eg' macro takes care of this.

% When citing a multi-author paper, you may save space by using ``et alia'', shortened to ``\etal'' (not ``{\em et.\ al.}'' as ``{\em et}'' is a complete word).
% If you use the \verb'\etal' macro provided, then you need not worry about double periods when used at the end of a sentence as in Alpher \etal.
% However, use it only when there are three or more authors.
% Thus, the following is correct:
%    ``Frobnication has been trendy lately.
%    It was introduced by Alpher~\cite{Alpher02}, and subsequently developed by
%    Alpher and Fotheringham-Smythe~\cite{Alpher03}, and Alpher \etal~\cite{Alpher04}.''

% This is incorrect: ``... subsequently developed by Alpher \etal~\cite{Alpher03} ...'' because reference~\cite{Alpher03} has just two authors.

% \begin{figure*}
%   \centering
%   \begin{subfigure}{0.68\linewidth}
%     \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%     \caption{An example of a subfigure.}
%     \label{fig:short-a}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}{0.28\linewidth}
%     \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%     \caption{Another example of a subfigure.}
%     \label{fig:short-b}
%   \end{subfigure}
%   \caption{Example of a short caption, which should be centered.}
%   \label{fig:short}
% \end{figure*}
