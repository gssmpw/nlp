\section{Conclusion}

In this paper, we introduce CLoCKDistill, a novel knowledge distillation method designed for compressing DETR detectors. Our approach incorporates both feature distillation and logit distillation. For feature distillation, we effectively transfer the transformer-specific global context from the teacher to the student by distilling the DETR memory, with the guidance from ground truth. For logit distillation, we propose target-aware queries that provide consistent and precise spatial guidance for both the teacher and student models on where to focus in memory during logit generation. Experimental results on the KITTI and COCO datasets show that our CLoCKDistill significantly enhances the performance of various DETR models, outperforming state-of-the-art methods. Additionally, our method clearly improves focus on relevant objects, as demonstrated by the qualitative analysis of attention maps. The clear attention boundaries suggest that our method could be promising for segmentation tasks, a potential avenue for future research.