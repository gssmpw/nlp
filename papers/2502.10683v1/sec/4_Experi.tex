\section{Experiments and Results}
\input{sec/Tables/KITTI}
\input{sec/Tables/COCO}
\input{sec/Figures/Attention_vis}

\subsection{Datasets}
We demonstrate our method's efficacy using two datasets:

\noindent\textbf{KITTI} \cite{Geiger2012CVPR} offers a widely used 2D object detection benchmark for autonomous driving and computer vision research. It specifically represents scenarios that demand low latency and constrained computational resources. The dataset includes seven types of road objects and contains 7,481 annotated images, which are split into a training set and a validation set using an 8:2 ratio. We follow the convention in \cite{lan2024gradient} for pre-processing.
%Specifically, \textit{car, van, truck, tram} are regrouped as Car, \textit{pedestrian, person} as Pedestrian, and \textit{cyclist} as Cyclist.

\noindent\textbf{MS COCO} is a dataset that includes 80 categories spanning a wide range of objects, with 117K training images and 5K validation images.
We adopt COCO-style evaluation metrics, e.g., mean Average Precision (mAP), for both datasets. 
% delete when needed
The validation process for each dataset closely follows these standard metrics to ensure consistency and comparability.

\subsection{Implementation Details}
We evaluate our approach on three state-of-the-art DETR detectors: DAB-DETR \cite{liu2022dab}, Deformable-DETR \cite{zhu2020deformable}, and DINO \cite{zhang2023dino}. All experiments are conducted using the MMDetection framework \cite{mmdetection} on Nvidia A100 GPUs, adhering to the original hyperparameter settings and optimizer configurations. ResNets \cite{he2016deep} are used as backbones for both the teacher and student models. The distillation loss coefficients are set as follows: $\alpha = 5\times 10^{-5}, \beta = 1\times 10^{-7}, \lambda_{kl} = 1$, $\lambda_{L1} = 5$, and $\lambda_{GIoU} = 2$. The number of distillation points is set to 300 for DAB-DETR and Deformable DETR, and 900 for DINO. The number of copies for target-aware queries is set to 3 for all experiments. 

\subsection{Quantitative Results}
Table \ref{tab:KITTI} presents the results on KITTI, highlighting the effectiveness of our distillation method across various DETR models and backbones. Notably, our distillation approach boosts the performance of DINO with ResNet-18 and ResNet-50 backbones by 6.4\% and 6.0\% mAP, respectively. %The student distilled by our method surpasses the state-of-the-art KD-DETR \cite{wang2024knowledge} by 1.6\% for both ResNet-50 and ResNet-18 backbones. 
For DAB-DETR, our method improves the performance of the detectors using ResNet-18 and ResNet-50 backbones by 5.4\% and 2.6\% mAP, respectively. Furthermore, in the case of Deformable DETR, our approach achieves mAP improvements of 2.9\% and 2.2\% for the ResNet-18 and ResNet-50 backbones, respectively.

%We also beat the baseline and competing methods by clear margins on COCO (Tab. \ref{tab:COCO}). For instance, our method boosts DAB-DETR performance by 2.8\% mAP when using the ResNet-18 backbone. This outperforms the cutting-edge distillation method \cite{wang2024knowledge} by 1.1\% mAP.

We also beat the baselines by clear margins on the COCO dataset (Table \ref{tab:COCO}). For instance, our method boosts DINO performance by 2.9\% mAP when using the ResNet-18 backbone. %This outperforms the cutting-edge distillation method \cite{wang2024knowledge} by 1.1\% mAP.

In Table~\ref{tab:kd methods}, we present a comparison of our method with state-of-the-art knowledge distillation methods for object detectors, including FitNet \cite{romero2014fitnets}, FGD \cite{yang2022focal}, MGD \cite{yang2022masked}, and KD-DETR \cite{wang2024knowledge}. Notably, KD-DETR \cite{wang2024knowledge} (CVPR 2024) represents the latest advancement specifically tailored for DETR distillation. According to the results, while KD-DETR demonstrates superior performance over conventional KD methods, the margin over MGD is not significant. Our method enlarges the performance gap between DETR-oriented and conventional KD methods by taking advantage of transformer-specific global context and strategically incorporating ground truth information into both feature and logit distillation. 
\input{sec/Tables/KD_Compare_Table}

\subsection{Qualitative Results}
The attention maps of different approaches in Figure~\ref{fig:activate vis} also demonstrate our CLoCKDistill's effectiveness. In Figure~\ref{fig:teacherattn_zebra}, the teacher model's attention map reveals wide dispersion, with attention spread across both the foreground objects (zebras) and the background. This broad focus reflects the teacher model's strong representational capacity due to its larger size; however, it also leads to substantial attention on irrelevant background areas, which can complicate or interfere with accurate detection.
Unlike the over-parameterized teacher model capturing many unnecessary and irrelevant features, the capacity-limited student baseline learns to focus most of its attention on the foreground objects while trying to achieve a satisfactory detection performance on its own (as shown in Figure~\ref{fig:studentattn_zebra}). However, this focus is confined to specific boundary areas, and the student baseline exhibits variability in attention across zebras of different scales, highlighting the model's sensitivity to scale changes.
When learning from the teacher via KD-DETR (Figure~\ref{fig:kddetrattn_zebra}), the student model inherits much of the teacherâ€™s irrelevant focus on distracting background features, which impedes its ability to concentrate effectively on objects of interest.
In contrast, our CLoCKDistill model (Figure~\ref{fig:ourattn_zebra}) achieves a more targeted focus, prioritizing relevant objects. Additionally, the attention boundaries are more sharply defined, indicating that our distilled model has developed a clearer and more precise understanding of the objects.


\subsection{Ablation Study}
In this section, we conduct ablation studies on the key components of our CLoCKDistill method and the number of transformer encoder-decoder layers.
%The experiments are on the KITTI dataset using DINO, with ResNet-50 as the teacher and ResNet-18 as the student backbone.
\subsubsection{CLoCKDistill components}
Table \ref{tab:Ablation} presents the influence of the main components of our CLoCKDistill, i.e., distilling memory, masking memory with location info, and target-aware queries. As we can see, memory distillation alone boosts the student model's performance by 3.8\% mAP. Further masking memory with location info enhances performance by 5.5\% mAP. By integrating all components, including target-aware queries, we achieve a 6.4\% mAP improvement for the DINO detector with a ResNet-18 backbone.
\input{sec/Tables/strategies}

\subsubsection{Number of transformer layers}
\input{sec/Tables/Enc_Dec}
The transformer detection head requires considerable computation, so reducing the number of encoder and decoder layers can improve model efficiency. To explore this, we conducted experiments with different reduced layer configurations. To address the layer mismatch between the student and teacher models, we applied our distillation method only to the final encoder and decoder layers.

Table~\ref{tab:enc_dec} shows the impact of varying the number of transformer encoder and decoder layers. As expected, reducing the number of encoder/decoder layers decreases the model performance. The student baseline is more sensitive to reductions in decoder layers, while our distilled model is more affected by reductions in encoder layers.
Notably, with only half of the encoder and decoder layers, our distilled model still surpasses the full-scale baseline by 3.4\% in mAP and achieves a 1.6x increase in frames per second (FPS). 
