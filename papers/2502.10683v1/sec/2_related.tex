\section{Related Work}
\subsection{Visual Object Detection}
Visual object detection has greatly improved with CNNs, leading to two main types of CNN detectors: two-stage \cite{ren2015faster, cai2018cascade, chu2020detection} and one-stage \cite{redmon2018yolov3, duan2019centernet, tian2019fcos, kong2020foveabox, li2020generalized}.
%Two-stage detectors \cite{cai2018cascade, he2017mask, ren2015faster} generate region proposals before refining and categorizing them, offering higher accuracy at the cost of longer inference times. In contrast, one-stage detectors \cite{lin2017focal, liu2016ssd, redmon2018yolov3, duan2019centernet, tian2019fcos} make predictions directly on feature maps using pre-defined anchor boxes, trading some manual design effort for greater efficiency. 
CNN detectors often use post-processing steps like Non-Maximum Suppression (NMS) to refine final bounding box predictions.
Unlike CNN object detectors, DETR approaches object detection as a set prediction problem using bipartite matching. \citet{carion2020end} introduce the initial end-to-end transformer-based detector without any post-processing.
\citet{dai2021dynamic}, \citet{gao2021fast}, and \citet{sun2021rethinking} attempt to mitigate the slow convergence issue of DETR. 
Deformable DETR \cite{zhu2020deformable} utilizes a deformable attention module that generates a small fixed number of sampling points for each query element, enhancing efficiency by only focusing on relevant feature areas. Additionally, it incorporates prior information (e.g., expected locations of objects) into object queries to further improve detection accuracy. Conditional-DETR \cite{meng2021conditional} separates the content and positional components in object queries, creating positional queries based on the spatial coordinates of objects. DAB-DETR \cite{liu2022dab} further integrates the width and height of bounding boxes into the positional queries. Anchor DETR \cite{wang2022anchor} encodes 2-D anchor points as object queries and designs a row-column decoupled attention to reduce memory cost. DINO \cite{zhang2023dino} presents denoising training methods that utilize noisy ground-truth labels to enhance model training stability and performance. Despite the promising progress, DETRs' complexity still poses challenges for practical use and deployment. Moreover, only a limited number of studies have explored compression techniques on DETRs.

\subsection{Knowledge Distillation for Object Detectors}

The majority of knowledge distillation methods for object detection are designed for CNN-based detectors, utilizing logits \cite{zheng2022localization}, intermediate features \cite{zhang2020improve}, and the relationships between features across different samples \cite{dai2021general}. The first application of KD to object detection by \cite{chen2017learning} involves distilling features from the neck and logits from the classification and regression heads. \citet{li2017mimicking} focus on distilling features from the RPN head. However, not all features are equally useful. 
%\citet{wang2019distilling} introduce masks to concentrate on regions near ground-truth bounding boxes. 
\citet{dai2021general} direct more attention to regions where teacher and student predictions differ most, while \citet{yang2022focal} employ activation-based attention maps to guide feature distillation, incorporating the GcBlock \cite{cao2019gcnet} to capture the relations between pixels.

Despite advancements in knowledge distillation for CNN detectors, most KD techniques are not well-suited for DETRs, with only a few methods tackling the unique challenges they present.
\citet{ijcai2024p74} use Hungarian matching to align queries between the teacher and student models and repurpose the teacher’s queries as an auxiliary group for alignment. However, their approach is limited to decoder-level distillation. \citet{chang2023detrdistill} apply similar matching methods for logit distillation and utilize the teacher’s queries to interact with FPN features, aiming to enhance the distillation of important CNN-based features. Both methods neglect the rich contextual information embedded in the encoder output features.
% \citet{chang2023detrdistill} introduce a distillation method for DETR using Hungarian matching to align queries between the teacher and student models, but it often leads to instability by matching irrelevant queries. 
To tackle the issue of inconsistent distillation points, \citet{wang2024knowledge} employ a predefined set of queries shared between the student and teacher models. However, these queries can sometimes be unreliable, as they are either randomly generated or exclusively derived from the fallible teacher model.
In this paper, we propose using ground truth to design target-aware queries for consistent and more precise logit distillation. Moreover, for more effective feature distillation, instead of distilling backbone features as previous works do, we distill the transformer encoder output (i.e., memory) that captures the global context for each pixel, and employ ground truth location information to emphasize key memory features.
The ability to capture global context is a key advantage of DETRs over CNNs, and we are among the first to explicitly leverage and refine this important knowledge and transfer it to the student model during distillation. 
