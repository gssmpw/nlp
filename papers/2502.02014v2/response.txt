\section{Related Work}
\subsection{Learning-based Lyapunov Function Construction}

The field of learning-based Lyapunov function construction is advancing rapidly. Chen, "A Learning-Based Method for Constructing Lyapunov Functions" formulated Lyapunov condition violations as the objective, jointly learning a neural Lyapunov function and a linear controller to guarantee stability for a given system, with stability verified via SMT solvers. Haber, "Learning Lyapunov Functions with Neural Networks" extended this to unknown dynamics with a neural controller. Zhang, Guo, "Neural-based Discrete-time Lyapunov Function Construction" focused on discrete-time systems, using MIP solvers for stability verification, requiring piecewise linear approximations. Xiao, "Scalable Neural Network Verification via $\alpha,\beta$-CROWN" applied $\alpha,\beta$-CROWN for scalable neural network verification, extending state feedback to output feedback control. However, scalability remains a challenge: SMT solvers handle up to 30 neurons, MIP solvers scale to 200 neurons Liang, "Lyapunov-based Neural Network Stability Analysis", and $\alpha,\beta$-CROWN Zhang, "Verifying Deep Networks with Improved Abstract Domain" is limited to a three-layer architecture (16 neurons per layer).


In contrast to neural Lyapunov functions, Chen, Guo, "Analytical Lyapunov Functions for Nonlinear Systems" derived analytical Lyapunov functions. Liu, "Lyapunov Function Construction using Symbolic Regression and Neural Networks" combined a neural network with the symbolic regression package PySR, which approximates the network to produce analytical Lyapunov functions, but the lack of interaction between system dynamics and symbolic regression component limits its potential. Guan, "Learning Analytical Lyapunov Functions via Transformers" pre-trained a transformer on backward- and forward-generated global Lyapunov function datasets, relying on beam search for candidate generation. However, their method cannot adaptively refine the candidate Lyapunov functions if the beam search fails on specific dynamics, and it requires a dataset that is expensive to generate (e.g., thousands of CPU hours for a 5-D dynamics dataset) to achieve adequate generalization during inference. Furthermore, its emphasis on global stability limits its applicability to real-world, nonpolynomial control systems, which typically only admit local stability. Consequently, an RL-based approach that directly searches Lyapunov functions for a given system is indispensable.


\subsection{Symbolic Regression with Generative Model}
\label{subsec:sr-work}
Symbolic regression is a supervised learning task that seeks to discover an analytical function $f: \mathbb{R}^n \to \mathbb{R}$ which approximates the output $y_i \in \mathbb{R}$ from input $x_i \in \mathbb{R}^n$.


\textbf{RL-based Symbolic Regression.} 
RL-based symbolic regression algorithms Jiang, "Generative Models for Symbolic Regression" employed generative models, typically RNNs, to generate distributions of symbolic tokens representing mathematical operations and variables, from which analytical expressions are sampled. Rewards, evaluating the quality of the sampled expressions, are measured by fitness metrics like RMSE. Due to the non-differentiable step of converting token sequences into symbolic equations, policy gradients are used to optimize the output distributions. Liu, "Improved Symbolic Regression via Generative Models and Policy Gradients" extended this approach by integrating Genetic Programming (GP) to refine generated expressions, improving framework's overall performance.


\textbf{Pre-trained Symbolic Regression methods.} 
These methods are inspired by the success of transformers in Natural Language Processing (NLP) tasks. These algorithms contain two steps: 1) pre-train an encoder-decoder network to model $p(f|\mathcal{D})$ on curated datasets by cross-entropy loss, and 2) sample from this distribution during inference via beam search or Monte Carlo Tree Search (MCTS). Methods like Zhang, "Pre-trained Symbolic Regression Models for Analytical Function Discovery" rely on beam search without gradient refinement, often yielding suboptimal results for out-of-distribution data. In contrast, Chen, "End-to-End Pre-trained Symbolic Regression via Transformers and Policy Gradients" integrates RL-based policy gradient optimization with end-to-end RMSE loss for both pre-training and inference, allowing gradient refinement for unseen datasets during inference. Further, Wang, "Enhanced Symbolic Regression via MCTS with Feedbacks and Decoding Process Optimization" enhance decoding process (expression generation) by incorporating MCTS with feedbacks, such as fitting accuracy and equation complexity.