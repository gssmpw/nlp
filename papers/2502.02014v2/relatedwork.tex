\section{Related Work}
\subsection{Learning-based Lyapunov Function Construction}

The field of learning-based Lyapunov function construction is advancing rapidly. \citet{chang2019neural} formulated Lyapunov condition violations as the objective, jointly learning a neural Lyapunov function and a linear controller to guarantee stability for a given system, with stability verified via SMT solvers. \citet{zhou2022neural} extended this to unknown dynamics with a neural controller. \citet{dailyapunov} and \citet{wu2023neural} focused on discrete-time systems, using MIP solvers for stability verification, requiring piecewise linear approximations. \citet{yang2024lyapunovstable} applied $\alpha,\beta$-CROWN for scalable neural network verification, extending state feedback to output feedback control. However, scalability remains a challenge: SMT solvers handle up to 30 neurons, MIP solvers scale to 200 neurons \citet{DawsonSurvey}, and $\alpha,\beta$-CROWN \citet{yang2024lyapunovstable} is limited to a three-layer architecture (16 neurons per layer).


In contrast to neural Lyapunov functions, \citet{feng2024} and \citet{alfarano2024global} derived analytical Lyapunov functions. \citet{feng2024} combined a neural network with the symbolic regression package \textit{PySR} \citep{pysr}, which approximates the network to produce analytical Lyapunov functions, but the lack of interaction between system dynamics and symbolic regression component limits its potential. \citet{alfarano2024global} pre-trained a transformer on backward- and forward-generated global Lyapunov function datasets, relying on beam search for candidate generation. However, their method cannot adaptively refine the candidate Lyapunov functions if the beam search fails on specific dynamics, and it requires a dataset that is expensive to generate (e.g., thousands of CPU hours for a 5-D dynamics dataset) to achieve adequate generalization during inference. Furthermore, its emphasis on global stability limits its applicability to real-world, nonpolynomial control systems, which typically only admit local stability. Consequently, an RL-based approach that directly searches Lyapunov functions for a given system is indispensable.


\subsection{Symbolic Regression with Generative Model}
\label{subsec:sr-work}
Symbolic regression is a supervised learning task that seeks to discover an analytical function $f: \mathbb{R}^n \to \mathbb{R}$ which approximates the output $y_i \in \mathbb{R}$ from input $x_i \in \mathbb{R}^n$.


\textbf{RL-based Symbolic Regression.} 
RL-based symbolic regression algorithms 
\citep{dso, 2020costa, dso21} employed generative models, typically RNNs, to generate distributions of symbolic tokens representing mathematical operations and variables, from which analytical expressions are sampled. Rewards, evaluating the quality of the sampled expressions, are measured by fitness metrics like RMSE. Due to the non-differentiable step of converting token sequences into symbolic equations, policy gradients are used to optimize the output distributions. \citet{rlgp2021} extended this approach by integrating Genetic Programming (GP) to refine generated expressions, improving framework's overall performance.


\textbf{Pre-trained Symbolic Regression methods.} 
These methods are inspired by the success of transformers in Natural Language Processing (NLP) tasks. These algorithms contain two steps: 1) pre-train an encoder-decoder network to model $p(f|\mathcal{D})$ on curated datasets by cross-entropy loss, and 2) sample from this distribution during inference via beam search or Monte Carlo Tree Search (MCTS). Methods like \citep{NSR, E2E, NSRwH} rely on beam search without gradient refinement, often yielding suboptimal results for out-of-distribution data. In contrast, \citet{dgsr} integrates RL-based policy gradient optimization with end-to-end RMSE loss for both pre-training and inference, allowing gradient refinement for unseen datasets during inference. Further, \citep{tpsr, dgsr-mcts} enhance decoding process (expression generation) by incorporating MCTS with feedbacks, such as fitting accuracy and equation complexity.