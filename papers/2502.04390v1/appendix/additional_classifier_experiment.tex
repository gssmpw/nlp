In the main paper, we used activations and gradients as they were \textit{readily available} in our experimental pipeline. We now further test whether using model output only, which is more easily available than internal gradients and activations can achieve similar performance on our scenario. 
%To this end, we perform additional experiments in which we use only model outputs as feature to classify facts.

Each fact in our dataset is conceptually a statement involving a subject (s), relation (r), and object (o) (e.g., ``Danielle Darrieux's mother tongue is French''). In this section, we extract features that capture increasing levels of detail about the model's predictions, related to what the actual facts are, leveraging both:
\begin{itemize}
    \item Conditional probabilities $p(o|s,r)$ at different truncation points\footnote{Since the object $o$ can span multiple tokens, we extract features from the last $N$ tokens of each fact (we pick three, since most answers fit within that limit). For each token position, we compute both the truncated prompt probability $p(o|s,r)$ by removing the token and subsequent tokens, and the full sentence probability $p(s,r,o)$. This multi-token analysis ensures we capture the model's predictions across the entire span of the answer.}
    \item Joint probability $p(s,r,o)$ of the full statement
\end{itemize}

In more details, we extract the following features, with increasing complexity. 

%\begin{itemize}
\noindent \textbf{Basic Token Probabilities ($Feat_1$):}
For each of the last $N$ tokens (representing the answer), we collect the  probability of the actual next token given the truncated prompt. These simple scalar features capture the model's direct confidence in the correct continuation. This has a dimensionality of $N + 1$ ($N$ truncation points plus full statement, so 4 in our case.)


\noindent \textbf{Top-$k$ Predictions Analysis ($Feat_2$):}
Here, for each position in the answer, we collect the values and normalized indices of top-$k$ most likely next tokens. This captures both confidence distribution and ranking patterns. Similarly to the above, we compute this for both truncated prompts and full statements. Here, the dimensionality is $(N + 1) \times 2k$ ($k$ values and $k$ normalized indices for each position). We pick k=100.


\noindent \textbf{Distribution Features ($Feat_3$):} Here, we analyze the complete probability distribution over the vocabulary. For each position in the answer sequence, we construct histograms of the probabilities with $n_{bins}$ bins (here 100), capturing the full spectrum of the model's prediction patterns. We augment these distributions with indicator vectors that highlight the positions of ground truth tokens (the true next tokens of the current truncated fact), providing additional context about the model's accuracy. This  results in a feature vector of dimensionality $(N + 1) \times n_{bins}$.

\noindent \textbf{Combined Features ($Concat$):} Here, we simply concatenate $Feat_1$, $Feat_2$, and $Feat_3$. 

Tab.~\ref{tab:output:prob} shows the results over our dataset. We observe \textit{a similar great performance when using the model outputs, compared to Activations and Gradients}. Model output achieves even better performance in case of pre-trained models. This is inline with our earlier observation that activations (what we're using now) are more important than gradients in the case of pre-trained models. This result is encouraging for future work, where we plan to (i) build more challenging classification datasets (than the simple facts in CounterFact) and (ii) build standalone classifiers to speed up the training of LLMs, by avoiding training on conflicting data.

\begin{table}[h]
\centering
\begin{tabular}{l|cc|cc}
\toprule
\multirow{2}{*}{Strategy (dim)} & \multicolumn{2}{c|}{Pretrained Model} & \multicolumn{2}{c}{Finetuned Model} \\
 & Accuracy & F1-Score & Accuracy & F1-Score \\
\midrule
Feat.1 (4) & 0.852 & 0.856 & 0.850 & 0.855 \\
Feat.2 (800) & 0.602 & 0.588 & 0.600 & 0.581 \\
Feat.3 (400) & 0.540 & 0.452 & 0.543 & 0.464 \\
Concat (1204) & 0.983 & 0.983 & 0.978 & 0.978 \\
(A+G) (240) & 0.947 & 0.948 & 0.995 & 0.995 \\
\bottomrule
\end{tabular}
\caption{Using output-only features for dissonance-awareness can achieve similar good performance to using our readily available activations and gradients, and even better in the case of the pre-trained model.}\label{tab:output:prob}
\end{table}

