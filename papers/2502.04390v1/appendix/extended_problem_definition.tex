\subsection{Preliminary notation}

We focus on the historical tracking of gradients of the outputs (grad\_outs) and activations for four key matrices within each block of the transformer model: 
\(\text{Attn}_{\text{c\_attn}}\), \(\text{Attn}_{\text{c\_proj}}\), \(\text{MLP}_{\text{c\_fc}}\), and \(\text{MLP}_{\text{c\_proj}}\).

Given an input sequence \( X \in \mathbb{R}^{B \times N \times d_{\text{model}}} \), where \( B \) is the batch size, \( N \) is the sequence length, and \( d_{\text{model}} \) is the model dimension, the transformer block is defined as follows:


\paragraph{Attention Layer:}
The attention mechanism computes query \( Q \), key \( K \), and value \( V \) matrices:
\[
Q = XW_Q, \quad K = XW_K, \quad V = XW_V
\]
where \( W_Q \in \mathbb{R}^{d_{\text{model}} \times d_{\text{key}}} \), \( W_K \in \mathbb{R}^{d_{\text{model}} \times d_{\text{key}}} \), and \( W_V \in \mathbb{R}^{d_{\text{model}} \times d_{\text{value}}} \) are trainable projection matrices.

The concatenated matrix \(\text{Attn}_{\text{c\_attn}}\) is:
\[
\text{Attn}_{\text{c\_attn}} = [Q, K, V] = XW_{\text{attn}}
\]
where \( W_{\text{attn}} = [W_Q, W_K, W_V] \in \mathbb{R}^{d_{\text{model}} \times (2d_{\text{key}} + d_{\text{value}})} \).

The attention context \(\text{Attn}_{\text{context}}\) is computed as:
\[
\text{Attn}_{\text{context}} = \text{softmax} \left( \frac{QK^T}{\sqrt{d_{\text{key}}}} \right) V
\]

The projected attention output \(\text{Attn}_{\text{c\_proj}}\) is:
\[
\text{Attn}_{\text{c\_proj}} = \text{Attn}_{\text{context}} W_{\text{proj}}
\]
where \( W_{\text{proj}} \in \mathbb{R}^{d_{\text{value}} \times d_{\text{model}}} \).

\paragraph{MLP Layer:}
The MLP layer consists of two linear transformations with an activation function \( \sigma \):
\[
\text{MLP}_{\text{c\_fc}} = \sigma (XW_{\text{fc}} + b_{\text{fc}})
\]
where \( W_{\text{fc}} \in \mathbb{R}^{d_{\text{model}} \times d_{\text{ff}}} \) and \( b_{\text{fc}} \in \mathbb{R}^{d_{\text{ff}}} \).

The projected MLP output \(\text{MLP}_{\text{c\_proj}}\) is:
\[
\text{MLP}_{\text{c\_proj}} = \text{MLP}_{\text{c\_fc}} W_{\text{proj}} + b_{\text{proj}}
\]
where \( W_{\text{proj}} \in \mathbb{R}^{d_{\text{ff}} \times d_{\text{model}}} \) and \( b_{\text{proj}} \in \mathbb{R}^{d_{\text{model}}} \).

\subsection{Historical gradient and activation collection}
Collecting a profile of neuron activity during training or simulation of training is needed as (i) input feature to know if a fact is dissonant, novel or known, and (ii) as means to identify where to locate targeted updates.

During training, we collect and cumulate the gradients of the outputs (grad\_outs) and activations for the matrices \(\text{Attn}_{\text{c\_attn}}\), \(\text{Attn}_{\text{c\_proj}}\), \(\text{MLP}_{\text{c\_fc}}\), and \(\text{MLP}_{\text{c\_proj}}\).
Let \( t \) denote the training step.
We collect activations at step $t$:
\[\text{Attn}_{\text{c\_attn}}(t), \text{Attn}_{\text{c\_proj}}(t), \text{MLP}_{\text{c\_fc}}(t), \text{MLP}_{\text{c\_proj}}(t)\]
\noindent as well as 
Gradient of the Outputs (grad\_outs) at step $t$ : \[  \nabla L(\text{Attn}_{\text{c\_attn}}(t)), \nabla L(\text{Attn}_{\text{c\_proj}}(t)), \nabla L(\text{MLP}_{\text{c\_fc}}(t)), \nabla L(\text{MLP}_{\text{c\_proj}}(t))
    \]

In the remainder, we denote these, regardless of their provenance matrix, as:
\[A^l(t), G^l(t) \in \mathbb{R}^{B \times N \times d^l_{\text{out}}}\]
where \(l\) denotes the layer, \(B\) is the batch size, \(N\) is the sequence length, and \(d^l_{\text{out}}\) is the output dimension of layer \(l\).

When needed, we standardize these metrics for each layer \( l \) as follows:
\[
\hat{A}^l(t) = \frac{A^l(t) - \mu_A^l(t)}{\sigma_A^l(t)}, \quad \hat{G}^l(t) = \frac{G^l(t) - \mu_G^l(t)}{\sigma_G^l(t)}
\]
where \( \mu \) and \( \sigma \) are the mean and standard deviation computed over all dimensions of the respective tensor.

We then sum over the batch dimension:
\[
S^l_{\hat{A}}(t)_{n,i} = \sum_{b=1}^{B} \hat{A}^l_{b,n,i}(t), \quad S^l_{\hat{G}}(t)_{n,i} = \sum_{b=1}^{B} \hat{G}^l_{b,n,i}(t)
\]


Optionally\footnote{We consider two approaches. In the first, we extract the activations and gradients corresponding to the last token (i.e., position \( N \)) in the sequence for each sample in the batch. This is reasonable since the last token is representative of the fact or information of interest in our datasets. In the second, we simply aggregate over all tokens, where we aggregate activations and gradients across all tokens in the sequence by computing statistical measures such as the mean or sum over the token dimension.}, we can sum over the token dimension:

\[
S^l_{\hat{A}}(t)_i = \sum_{n=1}^{N} S^l_{\hat{A}}(t)_{n,i}, \quad S^l_{\hat{G}}(t)_i = \sum_{n=1}^{N} S^l_{\hat{G}}(t)_{n,i}
\]


The standardized and summed metrics are then accumulated across the training steps:
% \[
% H\hat{A}^l = \sum_{t=1}^T S^l_{\hat{A}}(t), \quad H\hat{G}^l = \sum_{t=1}^T S^l_{\hat{G}}(t)
% \]
\[
H\hat{A}^l_i = \sum_{t=1}^{T} S^l_{\hat{A}}(t)_i, \quad H\hat{G}^l_i = \sum_{t=1}^{T} S^l_{\hat{G}}(t)_i
\]
where \( T \) is the total number of training steps.

These historical activations \( H\hat{A}^l \) and gradients \( H\hat{G}^l \) provide cumulative measures of neuron activity over the training process. They help identify neurons that are heavily utilized (stubborn neurons) and those that are underutilized (plastic neurons), which is crucial for our targeted updates.
%%%%%%%%%%%%%%%% OLD %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Preliminary notation}

% We focus on the historical tracking of gradients of the outputs (grad\_outs) and activations for four key matrices within each block of the transformer model: 
% \(\text{Attn}_{\text{c\_attn}}\), \(\text{Attn}_{\text{c\_proj}}\), \(\text{MLP}_{\text{c\_fc}}\), and \(\text{MLP}_{\text{c\_proj}}\).

% Given an input sequence \( X \in \mathbb{R}^{N \times d_{\text{model}}} \), where \( N \) is the sequence length and \( d_{\text{model}} \) is the model dimension, the transformer block is defined as follows:

% \paragraph{Attention Layer:}
% The attention mechanism computes query \( Q \), key \( K \), and value \( V \) matrices:
% \[
% Q = XW_Q, \quad K = XW_K, \quad V = XW_V
% \]
% where \( W_Q \in \mathbb{R}^{d_{\text{model}} \times d_{\text{key}}} \), \( W_K \in \mathbb{R}^{d_{\text{model}} \times d_{\text{key}}} \), and \( W_V \in \mathbb{R}^{d_{\text{model}} \times d_{\text{value}}} \) are trainable projection matrices.

% The concatenated matrix \(\text{Attn}_{\text{c\_attn}}\) is:
% \[
% \text{Attn}_{\text{c\_attn}} = [Q, K, V] = XW_{\text{attn}}
% \]
% where \( W_{\text{attn}} = [W_Q, W_K, W_V] \in \mathbb{R}^{d_{\text{model}} \times (2d_{\text{key}} + d_{\text{value}})} \).

% The attention context \(\text{Attn}_{\text{context}}\) is computed as:
% \[
% \text{Attn}_{\text{context}} = \text{softmax} \left( \frac{QK^T}{\sqrt{d_{\text{key}}}} \right) V
% \]

% The projected attention output \(\text{Attn}_{\text{c\_proj}}\) is:
% \[
% \text{Attn}_{\text{c\_proj}} = \text{Attn}_{\text{context}} W_{\text{proj}}
% \]
% where \( W_{\text{proj}} \in \mathbb{R}^{d_{\text{value}} \times d_{\text{model}}} \).

% \paragraph{MLP Layer:}
% The MLP layer consists of two linear transformations with an activation function \( \sigma \):
% \[
% \text{MLP}_{\text{c\_fc}} = \sigma (XW_{\text{fc}} + b_{\text{fc}})
% \]
% where \( W_{\text{fc}} \in \mathbb{R}^{d_{\text{model}} \times d_{\text{ff}}} \) and \( b_{\text{fc}} \in \mathbb{R}^{d_{\text{ff}}} \).

% The projected MLP output \(\text{MLP}_{\text{c\_proj}}\) is:
% \[
% \text{MLP}_{\text{c\_proj}} = \text{MLP}_{\text{c\_fc}} W_{\text{proj}} + b_{\text{proj}}
% \]
% where \( W_{\text{proj}} \in \mathbb{R}^{d_{\text{ff}} \times d_{\text{model}}} \) and \( b_{\text{proj}} \in \mathbb{R}^{d_{\text{model}}} \).



% \subsection{Historical gradient and activation collection}
% Collecting a profile of neuron activity during training or simulation of training is needed as (i) input feature to know if a fact is dissonant, novel or known, and (ii) as means to identify where to locate targetted updates.

% During training, we collect and cumulate the gradients of the outputs (grad\_outs) and activations for the matrices \(\text{Attn}_{\text{c\_attn}}\), \(\text{Attn}_{\text{c\_proj}}\), \(\text{MLP}_{\text{c\_fc}}\), and \(\text{MLP}_{\text{c\_proj}}\).
% Let \( t \) denote the training step.
% We collect activations at step $t$:
% \[\text{Attn}_{\text{c\_attn}}(t), \text{Attn}_{\text{c\_proj}}(t), \text{MLP}_{\text{c\_fc}}(t), \text{MLP}_{\text{c\_proj}}(t)\]
% \noindent as well as 
% Gradient of the Outputs (grad\_outs) at step $t$  t: \[  \nabla L(\text{Attn}_{\text{c\_attn}}(t)), \nabla L(\text{Attn}_{\text{c\_proj}}(t)), \nabla L(\text{MLP}_{\text{c\_fc}}(t)), \nabla L(\text{MLP}_{\text{c\_proj}}(t))
%     \]


% When needed, we standardize these metrics for each layer \( l \) as follows:
% \[
% \hat{A}_n^l = \frac{A_n^l - \mu_A^l}{\sigma_A^l}, \quad \hat{G}_n^l = \frac{G_n^l - \mu_G^l}{\sigma_G^l}
% \]
% where \( \mu_A^l \) and \( \sigma_A^l \) are the mean and standard deviation of activations in layer \( l \), and \( \mu_G^l \) and \( \sigma_G^l \) are the mean and standard deviation of gradients in layer \( l \).

% The standardized metrics are then summed across the training steps to obtain historical activations and historical gradients:
% \[
% H\hat{A}_n = \sum_{t=1}^T \hat{A}_n^l, \quad H\hat{G}_n = \sum_{t=1}^T \hat{G}_n^l
% \]

% This historical data should provide us with the necessary information to know if neurons and weights are rather ``stubborn'' because they underwent heavy changes in the past (and so are busy storing existing knowledge) or ``plastic'' (and thus good candidates for storing new knowledge).
