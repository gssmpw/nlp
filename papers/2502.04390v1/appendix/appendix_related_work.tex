In this section, we provide an extended version of Tab.~\ref{tab:continual-learning-taxonomy}, focusing \textit{only} on the \textit{most recent literature}, and showing how our work is uniquely positioned in the landscape of model editing and continual learning, the two key related branches to our work.

\begin{table}[t]
\centering
\caption{Extended taxonomy of incremental Learning Approaches, showing some seminal work (top) and more recent literature (split into editing and continual learning).}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}l p{3cm} c c c c c c@{}}
\toprule
\textbf{Examples} & \textbf{\makecell{Incremental \\ Type}} & \textbf{\makecell{Memory \\ Usage}} & \textbf{\makecell{Task \\ Awareness}} & \textbf{\makecell{Weight \\ Plasticity}} & \textbf{\makecell{Architecture}} & \textbf{\makecell{Conflict \\ Detection}} & \textbf{\makecell{Update \\ Mechanism}} \\ \midrule
% Classic/Foundational Approaches
iCaRL~\citep{rebuffi2017icarl} & Class-incremental & Replay & Task-Agnostic & Fixed & Fixed & No & Rehearsal \\
EWC~\citep{kirkpatrick2017overcoming} & Task-incremental & None & Task-Aware & Selective & Fixed & No & Regularization \\
Progressive Nets~\citep{rusu2016progressive} & Task-incremental & None & Task-Aware & Fixed & Expanding & No & New Subnetworks \\
DEN~\citep{yoon2017lifelong} & Task-incremental & None & Task-Aware & Selective & Expanding & No & Selective Expansion \\
GEM~\citep{lopez2017gradient} & Task-incremental & Replay & Task-Aware & Constrained & Fixed & No & Constrained Optimization \\
ROME~\citep{DeCao2021} & Fact-incremental & None & Fact-Aware & Localized & Fixed & No & Rank-One Update \\
OWM~\citep{zeng2019continual} & Task-incremental & None & Task-Aware & Orthogonal & Fixed & No & Orthogonal Projection \\
PackNet~\citep{mallya2018packnet} & Task-incremental & None & Task-Aware & Selective & Fixed & No & Weight Masking \\
HAT~\citep{serra2018overcoming} & Task-incremental & None & Task-Aware & Selective & Fixed & No & Attention Masking \\ \midrule
% Recent Knowledge Editing Approaches
MALMEN~\citep{tan2023massive} & Fact-incremental & None & Fact-Aware & Localized & Fixed & No & Parameter Shift Aggregation \\
EditAnalysis~\citep{li2023unveiling} & Fact-incremental & None & Fact-Aware & Analysis & Fixed & No & Consistency Analysis \\
D4S~\citep{huang2024reasons} & Fact-incremental & O(1) & Fact-Aware & Regulated & Fixed & No & Layer-Norm Control \\ \midrule
% Recent Continual Learning Approaches
Global Prototypes~\citep{baicontinual} & Task/Class-incremental & None & Task-Agnostic & Selective & Fixed & No & Global Prototype Alignment \\
NTE~\citep{benjamin2024continual} & Task-incremental & None & Task-Agnostic & Selective & Fixed & No & Bayesian Ensemble \\
UPGD~\citep{elsayed2024addressing} & Task-incremental & None & Task-Agnostic & Selective & Fixed & No & Utility-Gated Updates \\
~\citep{hiratani2024disentangling} & Task-incremental & None & Task-Aware & Selective & Fixed & No & Fisher Information \\
CLAP~\citep{jha2024clap4clip} & Class-incremental & None & Task-Aware & Selective & Fixed & No & Probabilistic Adaptation \\
VQ-Prompt~\citep{jiao2024vector} & Class-incremental & None & Task-Agnostic & Fixed & Fixed & No & Discrete Prompt Selection \\
IsCiL~\citep{lee2024incremental} & Task-incremental & None & Task-Aware & Selective & Fixed & No & Skill-based Adaptation \\
BGS~\citep{leecontinual} & Task/Domain/Class-incremental & Replay & Task-Aware & Selective & Fixed & Yes & Bias-Aware Update \\
SLM~\citep{peng2024scalable} & Task-incremental & None & Auto-detected & Selective & Fixed & No & Vector Space Retrieval \\
Train-Attention~\citep{seo2024train} & Knowledge-incremental & None & Task-Agnostic & Selective & Fixed & No & Token-Weighted Update \\
Refresh Learning~\citep{wang2024unified} & Task/Class-incremental & Optional & Task-Aware & Selective & Fixed & No & Unlearn-Relearn \\
RAIL~\citep{xu2024advancing} & Cross-domain-incremental & None & Task-Agnostic & Selective & Fixed & No & Regression-based Update \\
SAFE~\citep{zhao2024safeslowfastparameterefficient} & Class-incremental & None & Task-Agnostic & Selective & Fixed & No & Dual Parameter-Efficient Tuning \\ \midrule
% Your Paper
\textbf{This paper} & Fact-incremental & None & Conflict-Aware & Selective & Fixed & Yes & Neuron-Specific Update \\ \bottomrule
\end{tabular}%
}
\label{tab:continual-learning-taxonomy-extended}
\end{table}


\subsection{Continual learning}
Continual Learning (CL) methods enable models to learn new tasks without catastrophically forgetting previously mastered ones \citep{kirkpatrick2017overcoming}. These approaches fall into three main families: memory-based methods using exemplar buffers \citep{rebuffi2017icarl}, knowledge distillation techniques that transfer information across model versions \citep{lopez2017gradient}, and regularization-based methods that constrain weight updates \citep{kirkpatrick2017overcoming}. To ease the understanding of this landscape, we build a taxonomy that characterizes approaches by their incremental type (task, class, or fact-based), memory requirements, update mechanisms, and architectural constraints (Tab.~\ref{tab:continual-learning-taxonomy}). This taxonomy reveals how our work is different from existing continual learning attempts: while existing methods focus on preserving knowledge across distinct tasks, none explicitly address the detection and handling of conflicting information - a key capability in human cognition that our work empirically investigates.

One of the closest old approaches is deep mind's EWC~\citep{kirkpatrick2017overcoming}, a method designed to mitigate catastrophic forgetting in neural networks trained sequentially on distinct tasks. The core idea is to protect the most important weights (or neurons) for previously learned tasks during the training of new tasks. EWC identifies these important weights by calculating the Fisher Information Matrix during or after the training of a task, which estimates how sensitive each weight is to the task’s performance. Weights that significantly impact the output for a given task are marked as important. A quadratic penalty is then applied during future learning, constraining these weights to remain close to their values from the previous task. This ensures that knowledge from earlier tasks is preserved while still allowing the model to adapt to new tasks. However, EWC is \textbf{less suitable for LLMs}, which \textbf{do not have clearly defined tasks} when it comes to knowledge ingestion (probably different for other types of skills). EWC's effectiveness relies on distinct task boundaries and the ability to compute task-specific importance for weights, which is feasible in scenarios with well-defined tasks, such as classification or reinforcement learning. In LLMs, where learning spans a wide range of topics and linguistic structures without clear task delineation, it’s challenging to apply EWC's task-based strategy. The model would struggle to assign specific neurons or weights to individual tasks or concepts, making it difficult to protect task-specific knowledge without hindering the model’s overall generalization ability across a diverse dataset.

We cite in the remainder more recent literature that we project onto our taxonomy.

\citet{baicontinual} introduce a novel approach to continual learning that leverages global prototypes to mitigate catastrophic forgetting in neural networks. Their key insight is that maintaining stable connections between task-specific representations and pre-learned, general-purpose token embeddings (which serve as global prototypes) can significantly reduce forgetting without requiring explicit replay mechanisms. Through empirical validation on both task-incremental and class-incremental NLP scenarios, they demonstrate that models preserving strong connections to these global prototypes exhibit enhanced stability. While their work shares our goal of preserving knowledge during updates, it differs fundamentally in its approach and granularity: where they focus on task-level knowledge preservation through architectural mechanisms, our work addresses the more specific challenge of managing contradictory factual updates through cognitive-inspired conflict detection. Their finding that stable reference points aid knowledge retention is conceptually relevant to our work, though our results suggest that such architectural approaches alone may be insufficient when handling explicitly contradictory information, where more sophisticated cognitive mechanisms become necessary.

\citet{benjamin2024continual} proposed an elegant theoretical framework that interprets neural networks as Bayesian ensembles of classifiers. Their key insight is that a neural network with N parameters can be viewed as a weighted ensemble of N classifiers in the lazy regime, where the classifiers remain fixed throughout learning. This interpretation reveals that a properly designed posterior update rule, resembling SGD without momentum, can enable continual learning without forgetting - notably, they prove that momentum actually exacerbates forgetting. While their work focuses on preserving all knowledge in task-incremental learning, our paper specifically examines cases where knowledge needs to be deliberately updated or overridden. Their key contribution is showing that catastrophic forgetting is linked to the transition from lazy to rich regimes in neural networks, providing both a theoretical explanation for why larger models are more robust to forgetting and a biologically-inspired mechanism for knowledge preservation that perhaps complements our cognitive-based approach.

\citet{elsayed2024addressing} propose UPGD (Utility-based Perturbed Gradient Descent), a novel approach targeting both catastrophic forgetting and loss of plasticity in streaming learning scenarios. Their method protects useful network units while maintaining plasticity in less-used ones through utility-gated gradient updates and perturbations. Unlike previous approaches requiring task boundaries or memory buffers, UPGD operates in a challenging streaming setting with continuous non-stationarity. Using their newly introduced direct plasticity metric, they demonstrate UPGD's ability to maintain performance levels that surpass or match existing methods. This work complements our investigation by providing evidence that selective neuronal updates based on utility metrics can effectively balance stability and plasticity, though in a task-learning rather than knowledge-updating context.

\citet{hiratani2024disentangling} analyze how task similarity affects continual learning through a novel theoretical framework combining teacher-student models with latent structure. Their key insight is that high input feature similarity coupled with low readout similarity leads to catastrophic outcomes in both knowledge transfer and retention, even when tasks are positively correlated. They demonstrate that weight regularization in the Fisher information metric robustly helps retention regardless of task similarity, while common approaches like activity gating improve retention at the cost of transfer performance. Their theoretical predictions are validated on permuted MNIST tasks with latent variables.

\citet{jha2024clap4clip} propose a probabilistic approach to continual learning for vision-language models, specifically focusing on CLIP adaptation. Their method, CLAP, introduces visual-guided attention and task-specific probabilistic adapters to model the distribution of text features, while leveraging CLIP's pre-trained knowledge for initialization and regularization. This work demonstrates that probabilistic modeling can significantly reduce catastrophic forgetting in class-incremental learning scenarios, achieving state-of-the-art performance across multiple benchmarks.

\citet{jiao2024vector} propose VQ-Prompt, a novel prompt-based continual learning framework that addresses class-incremental learning with pretrained vision transformers. Their key innovation is incorporating vector quantization into prompt selection, enabling end-to-end optimization of discrete prompts with task loss while maintaining effective knowledge abstraction. This contrasts with our cognitive-dissonance aware approach, as they focus on task adaptation through prompt engineering rather than explicit conflict detection. Their empirical results on ImageNet-R and CIFAR-100 demonstrate superior performance compared to existing prompt-based methods, suggesting the effectiveness of discrete knowledge representation in continual learning.

\cite{lee2024incremental} propose IsCiL, a framework for continual imitation learning that uses retrievable skills and adapter-based architecture to enable efficient knowledge sharing across tasks. Unlike traditional approaches that isolate task-specific parameters, IsCiL introduces a prototype-based skill retrieval mechanism that allows selective reuse of previously learned skills for new tasks. While focused primarily on motor skills rather than resolving knowledge contradictions, their empirical results show that this selective adaptation approach significantly improves sample efficiency and reduces catastrophic forgetting compared to other adapter-based methods, particularly in scenarios with incomplete demonstrations.

\citet{leecontinual} present a systematic empirical investigation of how dataset bias affects continual learning. Through carefully designed experiments across task-incremental, domain-incremental, and class-incremental scenarios, they reveal that bias transfers both forward and backward between tasks. Their analysis shows that CL methods focusing on stability tend to preserve and propagate biases from previous tasks, while emphasis on plasticity allows new biases to contaminate previous knowledge. Based on these insights, they propose BGS (Balanced Greedy Sampling), a method that mitigates bias transfer by maintaining a balanced exemplar memory and retraining the classification head. Note that here, we used ``Replay'' for Memory Usage in the table since their best performing method (BGS) uses an exemplar memory, but they also evaluate methods without memory. 

\citet{peng2024scalable} proposed a continual learning approach that automates task selection through vector space retrieval, eliminating the need for explicit task IDs, experience replay, or optimization constraints. Their method, Scalable Language Model (SLM), combines Joint Adaptive Re-parameterization with dynamic knowledge retrieval to automatically identify relevant parameters for each input, enabling task-agnostic updates. While achieving state-of-the-art results across diverse tasks and model scales (BERT, T5, LLaMA-2), their key contribution is demonstrating that automatic task identification and parameter selection can enable continual learning without requiring explicit task boundaries or memory buffers.

\citet{seo2024train} presented Train-Attention, an interesting meta-learning approach for continual knowledge learning (CKL) in LLMs that predicts and applies weights to tokens \textit{based on their usefulness for future tasks}. Unlike previous approaches that uniformly update all parameters, their method enables\textit{ targeted knowledge updates by learning which tokens are most important} to focus on. Through experiments on LAMA-CKL and TemporalWiki benchmarks, they show that selective token-weighted learning significantly reduces catastrophic forgetting while improving learning speed. The work somewhat complements our cognitive-inspired approach, and demonstrates the benefits of selective attention, but it does not explicitly address the handling of contradictory information.

\citet{wang2024unified} proposed a unified framework for continual learning that reveals common mathematical structures across seemingly distinct approaches (regularization-based, Bayesian-based, and memory-replay). Building on this unification, they introduce ``refresh learning" - a plug-in mechanism that first unlearns current data before relearning it, inspired by the beneficial role of forgetting in human cognition. Their work primarily focuses on task-incremental and class-incremental scenarios, demonstrating improved accuracy across CIFAR and Tiny-ImageNet benchmarks. While their approach differs from our fact-level knowledge updates in LLMs, their findings about selective forgetting complement our observations about cognitive-inspired update mechanisms. Their theoretical analysis showing that refresh learning improves the flatness of the loss landscape offers an interesting perspective on how controlled forgetting might benefit knowledge retention in neural networks.

\citet{xu2024advancing} propose a cross-domain task-agnostic incremental learning framework (X-TAIL) for vision-language models, focusing on the challenge of preserving both incrementally learned knowledge and zero-shot abilities. Their approach, RAIL, uses recursive ridge regression with non-linear projections to adapt to new domains without catastrophic forgetting. Unlike previous work requiring domain identity hints or reference datasets, RAIL can classify images across both seen and unseen domains without domain hints, demonstrating superior performance in both discriminative ability and knowledge preservation. While their work advances the technical aspects of continual learning, it differs from our cognitive-inspired investigation as it doesn't address the fundamental challenge of detecting and resolving conflicting knowledge, instead focusing on domain adaptation without explicit conflict awareness.

\citet{zhao2024safeslowfastparameterefficient} propose a class-incremental learning framework for pre-trained vision models that balances stability and plasticity through two complementary parameter-efficient tuning mechanisms. Their SAFE approach first inherits generalizability from pre-trained models via a ``slow learner" that captures transferable knowledge in the first session, then maintains plasticity through a ``fast learner" that continuously adapts to new classes while resisting catastrophic forgetting. While focused on vision tasks rather than language models, their dual-speed learning strategy presents interesting parallels to our cognitive-inspired approach – particularly in how both works identify the importance of selective plasticity and the distinction between stable (``stubborn") and adaptable (``plastic") parameters. However, SAFE doesn't address the fundamental challenge of detecting and handling contradictory information that we identify as crucial for true cognitive-inspired learning.

\textit{Unlike the above work, our goal is to understand the fundamental cognitive mechanisms underlying the continuous knowledge updates in LLMs, particularly focusing on how models can detect and react to contradictory information. Rather than proposing a new continual learning method, we provide crucial insights into how different types of knowledge updates affect model behavior and stability.}

\subsection{Knowledge editing}
Next, a big portion of recent literature has focused on understanding and modifying the internal knowledge of Large Language Models (LLMs), post-training. Such knowledge editing aims to alter specific facts or associations within the model without the need for full retraining. 

\citet{Geva2020} were among the first to show that transformer Feed-Forward Network (FFN) layers act as unnormalized key-value stores encoding relational knowledge inside LLMs. This observation was later confirmed and complemented by others \citep{Meng2022,Dai2021} before being leveraged by subsequent work to master the editing of internal memories.
\citet{Meng2022} introduced ROME (Rank-One Model Editing), a method that uses causal tracing to empirically locate the layers essential to encoding a given association. They then modify these modules by applying small rank-one changes. To identify the relevant modules, they run the network multiple times, introducing corruptions to the input sequence to disturb the inference, and then restore individual states from the original non-corrupted pass. But this work an others worked only on single edits, and were often evaluated one edit at a time, starting each time from a fresh pre-trained model. The same authors later developed MEMIT, which follows the same causal tracing principle but with the goal of scaling up to 10,000 edits in bulk\citep{Meng2022a}.
Similarly, \citet{Dai2021} leveraged the identification of knowledge neurons to perform ``knowledge surgery" – editing factual knowledge within Transformers without the need for additional fine-tuning.
\citet{Zhu2020} approached the knowledge modification task as a constrained optimization problem. Their work found that constrained layer-wise fine-tuning emerges as an effective method for modifying the knowledge that Transformers learn, suggesting a different pathway for knowledge editing inside LLMs.
\citet{DeCao2021} proposed \textsc{KnowledgeEditor}, which achieved knowledge editing by training a hyper-network with constrained optimization to modify specific facts without fine-tuning or changing the overall stored knowledge. The method was demonstrated on smaller models like BERT for fact-checking and BART for question answering, achieving consistent changes in predictions across different formulations of queries.

\citet{li2023unveiling}  empirically investigate the pitfalls of knowledge editing in LLMs, revealing two critical issues: logical inconsistencies between multiple edits (like contradictory relationship updates) and knowledge distortion (where edits irreversibly damage the model's knowledge structure). Through carefully designed benchmarks \textsc{ConflictEdit} and \textsc{RoundEdit}, they demonstrate that current editing methods struggle with these challenges, particularly when handling reverse relationships or composite logical rules. While their work focuses on identifying limitations in maintaining logical consistency across edits, our paper takes a complementary cognitive-inspired perspective by addressing how models handle contradictions with their existing knowledge base. Their findings about knowledge distortion align with and reinforce our observations about the catastrophic nature of updates that modify existing knowledge.

Similarly, \citet{huang2024reasons} empirically investigate causes of performance degradation during knowledge editing in LLMs. They show degradation correlates with editing target complexity and L1-norm growth in edited layers. Their proposed Dump for Sequence (D4S) method regulates layer norm growth using O(1) space complexity, enabling multiple effective updates while minimizing model degradation. Their work provides valuable insights into the mechanisms of model degradation during knowledge editing, but it does not specifically address the distinction between contradictory and non-contradictory updates, as we do in this paper.

\citet{tan2023massive} propose MALMEN, a scalable hypernetwork approach for editing Large Language Models by aggregating parameter shifts using a least-squares formulation. While previous editing methods like MEND~\citep{mitchell2022fast} could handle only a few facts simultaneously, MALMEN can efficiently edit thousands of facts while maintaining comparable performance. Their key innovation lies in separating the computation between the hypernetwork and LM, enabling arbitrary batch sizes and reducing memory requirements. Their empirical results show that MALMEN can edit hundreds of times more facts than MEND while maintaining similar performance levels, though they note that the method still struggles with generalizing to rephrasing not seen during training. Like other editing approaches, MALMEN focuses on the mechanics of (by design conflicting) updates.

\textit{Unlike all the work above, our goal in this work is not to edit knowledge, but to understand the fundamental mechanisms and phenomena that govern how LLMs integrate new information with existing knowledge. By taking a cognitive-inspired approach focused on dissonance awareness and adaptive plasticity, we reveal critical insights about the nature of knowledge representation and updating in these models.}


% https://proceedings.neurips.cc/paper/2021/hash/2a10665525774fa2501c2c8c4985ce61-Abstract.html
% https://proceedings.neurips.cc/paper_files/paper/2022/hash/80133d0f6eccaace15508f91e3c5a93c-Abstract-Conference.html
% https://arxiv.org/abs/1903.04476
% https://proceedings.mlr.press/v199/abbasi22a
% https://openaccess.thecvf.com/content/CVPR2022/html/Yan_Learning_Bayesian_Sparse_Networks_With_Full_Experience_Replay_for_Continual_CVPR_2022_paper.html
% https://proceedings.neurips.cc/paper/2020/hash/258be18e31c8188555c2ff05b4d542c3-Abstract.html
% https://proceedings.mlr.press/v234/yildirim24a.html
% https://www.sciencedirect.com/science/article/pii/S0925231221001545
% https://openaccess.thecvf.com/content/ICCV2023/html/Jin_Growing_a_Brain_with_Sparsity-Inducing_Generation_for_Continual_Learning_ICCV_2023_paper.html
% https://proceedings.neurips.cc/paper/2021/hash/f1e709e6aef16ba2f0cd6c7e4f52b9b6-Abstract.html
% https://arxiv.org/abs/2401.05667
% https://openaccess.thecvf.com/content_CVPR_2019/html/Ostapenko_Learning_to_Remember_A_Synaptic_Plasticity_Driven_Framework_for_Continual_CVPR_2019_paper.html
% https://arxiv.org/abs/2206.09117
% https://arxiv.org/abs/2403.08763
% https://arxiv.org/abs/1803.03635
% https://arxiv.org/abs/2406.05955
% https://arxiv.org/abs/2402.01089
% https://arxiv.org/abs/2202.12002
% https://arxiv.org/abs/2404.16789
% https://arxiv.org/abs/2406.17245
% https://arxiv.org/abs/2205.12393
% https://openaccess.thecvf.com/content/CVPR2024/html/Roy_Convolutional_Prompting_meets_Language_Models_for_Continual_Learning_CVPR_2024_paper.html
% https://arxiv.org/abs/2110.03215
% https://arxiv.org/abs/2405.18653
% https://arxiv.org/abs/2012.15283
% https://arxiv.org/abs/2205.09357
% https://arxiv.org/abs/2301.12314
% https://aclanthology.org/2020.coling-main.318/
% https://arxiv.org/abs/2401.03129
% https://arxiv.org/abs/2302.03241
% https://arxiv.org/abs/2309.14763
% https://arxiv.org/abs/2406.06962
% https://arxiv.org/abs/2406.01392
% https://arxiv.org/abs/2404.00790
% https://arxiv.org/abs/2406.18708
% https://arxiv.org/abs/2406.01375
% https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-66132030219-9
% https://openreview.net/forum?id=DimPeeCxKO
% https://arxiv.org/pdf/1612.00796


% New List: ongoing check of ICLR'24 and Nips'24:
% https://openreview.net/pdf?id=8SDsff42Lj
% https://arxiv.org/pdf/2408.17394
% https://arxiv.org/pdf/2410.20444
% https://cdn.iiit.ac.in/cdn/precog.iiit.ac.in/pubs/NeurIPS-RanDumb.pdf
% https://arxiv.org/pdf/2407.16920v1
% https://arxiv.org/pdf/2405.20236
% https://arxiv.org/pdf/2410.22658
% https://arxiv.org/pdf/2406.18868
% https://arxiv.org/pdf/2403.19137
% https://arxiv.org/pdf/2411.02175
% https://arxiv.org/pdf/2410.23843
% https://arxiv.org/pdf/2404.07470
% https://arxiv.org/pdf/2404.00781
% https://arxiv.org/pdf/2403.13249
% https://arxiv.org/pdf/2303.11863
% https://arxiv.org/pdf/2311.04661
% https://arxiv.org/pdf/2310.02129




% Interesting work:
% https://arxiv.org/pdf/2403.19137 
%The paper~\citep{jha2024clap4clip} tackles class-incremental continual learning with CLIP, where new groups of classes (tasks) arrive sequentially and the model must learn to classify them without forgetting previous classes. For example, in CIFAR100, they split 100 classes into 10 tasks of 10 classes each. Building on previous work that showed CLIP needs task-specific finetuning (either through learned prompts like CoOp or feature adapters like CLIP-Adapter), they identify a key problem: during continual learning, the adapted text features increasingly deviate from CLIP's frozen image features (measured by increasing angular distance), hurting performance. Their solution has three main components: (1) a visual-guided attention module that helps maintain alignment between visual and text features during adaptation, (2) task-specific probabilistic adapters that learn distributions over text features rather than point estimates, and (3) using CLIP's pre-trained language knowledge to initialize and regularize these adapters. They show this approach reduces cross-modal deviation and outperforms previous methods across multiple datasets. While simpler solutions might be possible (like directly enforcing alignment or transforming frozen features), the paper takes a more complex probabilistic approach that enables additional benefits like uncertainty estimation.

%https://nips.cc/virtual/2024/poster/94505
% Super interesting work, one of the closest to ours: shows that 

%https://arxiv.org/pdf/2404.00781
% Our work closely relates to the recent UPGD (Utility-based Perturbed Gradient Descent) approach [https://arxiv.org/pdf/2404.00781], which similarly investigates plasticity/stability mechanisms for continual learning in deep learning more generally. UPGD proposes a utility-based approach that protects important weights while allowing updates to less useful ones, combining gradient updates with perturbations based on a carefully approximated utility measure. While both works share similar motivations and empirical findings about the importance of selective plasticity, our work specifically focuses on cognitive dissonance awareness and the particular challenges of conflicting updates in language models. Our findings complement theirs by demonstrating that while non-conflicting updates can be handled relatively well with various strategies (including simple fine-tuning), conflicting updates pose a unique challenge that requires explicit conflict detection and resolution mechanisms. Additionally, while UPGD uses a theoretically-grounded utility approximation to guide updates, we show that even simple gradient-based historical tracking can effectively identify appropriate regions for targeted updates in practice.

% \begin{comment}
% \subsection{Raw list of continual learning}
% % does not compile wtf
% \begin{itemize}
%     \item \href{https://proceedings.neurips.cc/paper/2021/hash/2a10665525774fa2501c2c8c4985ce61-Abstract.html}{https://proceedings.neurips.cc/paper/2021/hash/2a10665525774fa2501c2c8c4985ce61-Abstract.html}
%     \item \href{https://proceedings.neurips.cc/paper_files/paper/2022/hash/80133d0f6eccaace15508f91e3c5a93c-Abstract-Conference.html}{https://proceedings.neurips.cc/paper_files/paper/2022/hash/80133d0f6eccaace15508f91e3c5a93c-Abstract-Conference.html}
%     \item \href{https://arxiv.org/abs/1903.04476}{https://arxiv.org/abs/1903.04476}
%     \item \href{https://proceedings.mlr.press/v199/abbasi22a}{https://proceedings.mlr.press/v199/abbasi22a}
%     \item \href{https://openaccess.thecvf.com/content/CVPR2022/html/Yan_Learning_Bayesian_Sparse_Networks_With_Full_Experience_Replay_for_Continual_CVPR_2022_paper.html}{https://openaccess.thecvf.com/content/CVPR2022/html/Yan_Learning_Bayesian_Sparse_Networks_With_Full_Experience_Replay_for_Continual_CVPR_2022_paper.html}
%     \item \href{https://proceedings.neurips.cc/paper/2020/hash/258be18e31c8188555c2ff05b4d542c3-Abstract.html}{https://proceedings.neurips.cc/paper/2020/hash/258be18e31c8188555c2ff05b4d542c3-Abstract.html}
%     \item \href{https://proceedings.mlr.press/v234/yildirim24a.html}{https://proceedings.mlr.press/v234/yildirim24a.html}
%     \item \href{https://www.sciencedirect.com/science/article/pii/S0925231221001545}{https://www.sciencedirect.com/science/article/pii/S0925231221001545}
%     \item \href{https://openaccess.thecvf.com/content/ICCV2023/html/Jin_Growing_a_Brain_with_Sparsity-Inducing_Generation_for_Continual_Learning_ICCV_2023_paper.html}{https://openaccess.thecvf.com/content/ICCV2023/html/Jin_Growing_a_Brain_with_Sparsity-Inducing_Generation_for_Continual_Learning_ICCV_2023_paper.html}
%     \item \href{https://proceedings.neurips.cc/paper/2021/hash/f1e709e6aef16ba2f0cd6c7e4f52b9b6-Abstract.html}{https://proceedings.neurips.cc/paper/2021/hash/f1e709e6aef16ba2f0cd6c7e4f52b9b6-Abstract.html}
%     \item \href{https://arxiv.org/abs/2401.05667}{https://arxiv.org/abs/2401.05667}
%     \item \href{https://openaccess.thecvf.com/content_CVPR_2019/html/Ostapenko_Learning_to_Remember_A_Synaptic_Plasticity_Driven_Framework_for_Continual_CVPR_2019_paper.html}{https://openaccess.thecvf.com/content_CVPR_2019/html/Ostapenko_Learning_to_Remember_A_Synaptic_Plasticity_Driven_Framework_for_Continual_CVPR_2019_paper.html}
%     \item \href{https://arxiv.org/abs/2206.09117}{https://arxiv.org/abs/2206.09117}
%     \item \href{https://arxiv.org/abs/2403.08763}{https://arxiv.org/abs/2403.08763}
%     \item \href{https://arxiv.org/abs/1803.03635}{https://arxiv.org/abs/1803.03635}
%     \item \href{https://arxiv.org/abs/2406.05955}{https://arxiv.org/abs/2406.05955}
%     \item \href{https://arxiv.org/abs/2402.01089}{https://arxiv.org/abs/2402.01089}
%     \item \href{https://arxiv.org/abs/2202.12002}{https://arxiv.org/abs/2202.12002}
%     \item \href{https://arxiv.org/abs/2404.16789}{https://arxiv.org/abs/2404.16789}
%     \item \href{https://arxiv.org/abs/2406.17245}{https://arxiv.org/abs/2406.17245}
%     \item \href{https://arxiv.org/abs/2205.12393}{https://arxiv.org/abs/2205.12393}
%     \item \href{https://openaccess.thecvf.com/content/CVPR2024/html/Roy_Convolutional_Prompting_meets_Language_Models_for_Continual_Learning_CVPR_2024_paper.html}{https://openaccess.thecvf.com/content/CVPR2024/html/Roy_Convolutional_Prompting_meets_Language_Models_for_Continual_Learning_CVPR_2024_paper.html}
%     \item \href{https://arxiv.org/abs/2110.03215}{https://arxiv.org/abs/2110.03215}
%     \item \href{https://arxiv.org/abs/2405.18653}{https://arxiv.org/abs/2405.18653}
%     \item \href{https://arxiv.org/abs/2012.15283}{https://arxiv.org/abs/2012.15283}
%     \item \href{https://arxiv.org/abs/2205.09357}{https://arxiv.org/abs/2205.09357}
%     \item \href{https://arxiv.org/abs/2301.12314}{https://arxiv.org/abs/2301.12314}
%     \item \href{https://aclanthology.org/2020.coling-main.318/}{https://aclanthology.org/2020.coling-main.318/}
%     \item \href{https://arxiv.org/abs/2401.03129}{https://arxiv.org/abs/2401.03129}
%     \item \href{https://arxiv.org/abs/2302.03241}{https://arxiv.org/abs/2302.03241}
%     \item \href{https://arxiv.org/abs/2309.14763}{https://arxiv.org/abs/2309.14763}
%     \item \href{https://arxiv.org/abs/2406.06962}{https://arxiv.org/abs/2406.06962}
%     \item \href{https://arxiv.org/abs/2406.01392}{https://arxiv.org/abs/2406.01392}
%     \item \href{https://arxiv.org/abs/2404.00790}{https://arxiv.org/abs/2404.00790}
%     \item \href{https://arxiv.org/abs/2406.18708}{https://arxiv.org/abs/2406.18708}
%     \item \href{https://arxiv.org/abs/2406.01375}{https://arxiv.org/abs/2406.01375}
%     \item \href{https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-66132030219-9}{https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-66132030219-9}
%     \item \href{https://openreview.net/forum?id=DimPeeCxKO}{https://openreview.net/forum?id=DimPeeCxKO}
%     \item \href{https://arxiv.org/pdf/1612.00796}{Elastic weight consolidation deepmind 2017} **Elastic Weight Consolidation (EWC)** is a method designed to mitigate catastrophic forgetting in neural networks trained sequentially on distinct tasks. The core idea is to protect the most important weights (or neurons) for previously learned tasks during the training of new tasks. EWC identifies these important weights by calculating the **Fisher Information Matrix** during or after the training of a task, which estimates how sensitive each weight is to the task’s performance. Weights that significantly impact the output for a given task are marked as important. A **quadratic penalty** is then applied during future learning, constraining these weights to remain close to their values from the previous task. This ensures that knowledge from earlier tasks is preserved while still allowing the model to adapt to new tasks.

% However, **EWC is less suitable for large language models (LLMs)**, which are typically trained on massive datasets that do not have clearly defined tasks. EWC's effectiveness relies on distinct task boundaries and the ability to compute task-specific importance for weights, which is feasible in scenarios with well-defined tasks, such as classification or reinforcement learning. In LLMs, where learning spans a wide range of topics and linguistic structures without clear task delineation, it’s challenging to apply EWC's task-based strategy. The model would struggle to assign specific neurons or weights to individual tasks or concepts, making it difficult to protect task-specific knowledge without hindering the model’s overall generalization ability across a diverse dataset.
% \end{itemize}

% % https://proceedings.neurips.cc/paper/2021/hash/2a10665525774fa2501c2c8c4985ce61-Abstract.html
% % https://proceedings.neurips.cc/paper_files/paper/2022/hash/80133d0f6eccaace15508f91e3c5a93c-Abstract-Conference.html
% % https://arxiv.org/abs/1903.04476
% % https://proceedings.mlr.press/v199/abbasi22a
% % https://openaccess.thecvf.com/content/CVPR2022/html/Yan_Learning_Bayesian_Sparse_Networks_With_Full_Experience_Replay_for_Continual_CVPR_2022_paper.html
% % https://proceedings.neurips.cc/paper/2020/hash/258be18e31c8188555c2ff05b4d542c3-Abstract.html
% % https://proceedings.mlr.press/v234/yildirim24a.html
% % https://www.sciencedirect.com/science/article/pii/S0925231221001545
% % https://openaccess.thecvf.com/content/ICCV2023/html/Jin_Growing_a_Brain_with_Sparsity-Inducing_Generation_for_Continual_Learning_ICCV_2023_paper.html
% % https://proceedings.neurips.cc/paper/2021/hash/f1e709e6aef16ba2f0cd6c7e4f52b9b6-Abstract.html
% % https://arxiv.org/abs/2401.05667
% % https://openaccess.thecvf.com/content_CVPR_2019/html/Ostapenko_Learning_to_Remember_A_Synaptic_Plasticity_Driven_Framework_for_Continual_CVPR_2019_paper.html
% % https://arxiv.org/abs/2206.09117
% % https://arxiv.org/abs/2403.08763
% % https://arxiv.org/abs/1803.03635
% % https://arxiv.org/abs/2406.05955
% % https://arxiv.org/abs/2402.01089
% % https://arxiv.org/abs/2202.12002
% % https://arxiv.org/abs/2404.16789
% % https://arxiv.org/abs/2406.17245
% % https://arxiv.org/abs/2205.12393
% % https://openaccess.thecvf.com/content/CVPR2024/html/Roy_Convolutional_Prompting_meets_Language_Models_for_Continual_Learning_CVPR_2024_paper.html
% % https://arxiv.org/abs/2110.03215
% % https://arxiv.org/abs/2405.18653
% % https://arxiv.org/abs/2012.15283
% % https://arxiv.org/abs/2205.09357
% % https://arxiv.org/abs/2301.12314
% % https://aclanthology.org/2020.coling-main.318/
% % https://arxiv.org/abs/2401.03129
% % https://arxiv.org/abs/2302.03241
% % https://arxiv.org/abs/2309.14763
% % https://arxiv.org/abs/2406.06962
% % https://arxiv.org/abs/2406.01392
% % https://arxiv.org/abs/2404.00790
% % https://arxiv.org/abs/2406.18708
% % https://arxiv.org/abs/2406.01375
% % https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-66132030219-9
% % https://openreview.net/forum?id=DimPeeCxKO
% % https://arxiv.org/pdf/1612.00796


% \subsection{Raw list of continual learning}
% % does not compile wtf
% \begin{itemize}
%     \item \href{https://proceedings.neurips.cc/paper/2021/hash/2a10665525774fa2501c2c8c4985ce61-Abstract.html}{https://proceedings.neurips.cc/paper/2021/hash/2a10665525774fa2501c2c8c4985ce61-Abstract.html}
%     \item \href{https://proceedings.neurips.cc/paper_files/paper/2022/hash/80133d0f6eccaace15508f91e3c5a93c-Abstract-Conference.html}{https://proceedings.neurips.cc/paper_files/paper/2022/hash/80133d0f6eccaace15508f91e3c5a93c-Abstract-Conference.html}
%     \item \href{https://arxiv.org/abs/1903.04476}{https://arxiv.org/abs/1903.04476}
%     \item \href{https://proceedings.mlr.press/v199/abbasi22a}{https://proceedings.mlr.press/v199/abbasi22a}
%     \item \href{https://openaccess.thecvf.com/content/CVPR2022/html/Yan_Learning_Bayesian_Sparse_Networks_With_Full_Experience_Replay_for_Continual_CVPR_2022_paper.html}{https://openaccess.thecvf.com/content/CVPR2022/html/Yan_Learning_Bayesian_Sparse_Networks_With_Full_Experience_Replay_for_Continual_CVPR_2022_paper.html}
%     \item \href{https://proceedings.neurips.cc/paper/2020/hash/258be18e31c8188555c2ff05b4d542c3-Abstract.html}{https://proceedings.neurips.cc/paper/2020/hash/258be18e31c8188555c2ff05b4d542c3-Abstract.html}
%     \item \href{https://proceedings.mlr.press/v234/yildirim24a.html}{https://proceedings.mlr.press/v234/yildirim24a.html}
%     \item \href{https://www.sciencedirect.com/science/article/pii/S0925231221001545}{https://www.sciencedirect.com/science/article/pii/S0925231221001545}
%     \item \href{https://openaccess.thecvf.com/content/ICCV2023/html/Jin_Growing_a_Brain_with_Sparsity-Inducing_Generation_for_Continual_Learning_ICCV_2023_paper.html}{https://openaccess.thecvf.com/content/ICCV2023/html/Jin_Growing_a_Brain_with_Sparsity-Inducing_Generation_for_Continual_Learning_ICCV_2023_paper.html}
%     \item \href{https://proceedings.neurips.cc/paper/2021/hash/f1e709e6aef16ba2f0cd6c7e4f52b9b6-Abstract.html}{https://proceedings.neurips.cc/paper/2021/hash/f1e709e6aef16ba2f0cd6c7e4f52b9b6-Abstract.html}
%     \item \href{https://arxiv.org/abs/2401.05667}{https://arxiv.org/abs/2401.05667}
%     \item \href{https://openaccess.thecvf.com/content_CVPR_2019/html/Ostapenko_Learning_to_Remember_A_Synaptic_Plasticity_Driven_Framework_for_Continual_CVPR_2019_paper.html}{https://openaccess.thecvf.com/content_CVPR_2019/html/Ostapenko_Learning_to_Remember_A_Synaptic_Plasticity_Driven_Framework_for_Continual_CVPR_2019_paper.html}
%     \item \href{https://arxiv.org/abs/2206.09117}{https://arxiv.org/abs/2206.09117}
%     \item \href{https://arxiv.org/abs/2403.08763}{https://arxiv.org/abs/2403.08763}
%     \item \href{https://arxiv.org/abs/1803.03635}{https://arxiv.org/abs/1803.03635}
%     \item \href{https://arxiv.org/abs/2406.05955}{https://arxiv.org/abs/2406.05955}
%     \item \href{https://arxiv.org/abs/2402.01089}{https://arxiv.org/abs/2402.01089}
%     \item \href{https://arxiv.org/abs/2202.12002}{https://arxiv.org/abs/2202.12002}
%     \item \href{https://arxiv.org/abs/2404.16789}{https://arxiv.org/abs/2404.16789}
%     \item \href{https://arxiv.org/abs/2406.17245}{https://arxiv.org/abs/2406.17245}
%     \item \href{https://arxiv.org/abs/2205.12393}{https://arxiv.org/abs/2205.12393}
%     \item \href{https://openaccess.thecvf.com/content/CVPR2024/html/Roy_Convolutional_Prompting_meets_Language_Models_for_Continual_Learning_CVPR_2024_paper.html}{https://openaccess.thecvf.com/content/CVPR2024/html/Roy_Convolutional_Prompting_meets_Language_Models_for_Continual_Learning_CVPR_2024_paper.html}
%     \item \href{https://arxiv.org/abs/2110.03215}{https://arxiv.org/abs/2110.03215}
%     \item \href{https://arxiv.org/abs/2405.18653}{https://arxiv.org/abs/2405.18653}
%     \item \href{https://arxiv.org/abs/2012.15283}{https://arxiv.org/abs/2012.15283}
%     \item \href{https://arxiv.org/abs/2205.09357}{https://arxiv.org/abs/2205.09357}
%     \item \href{https://arxiv.org/abs/2301.12314}{https://arxiv.org/abs/2301.12314}
%     \item \href{https://aclanthology.org/2020.coling-main.318/}{https://aclanthology.org/2020.coling-main.318/}
%     \item \href{https://arxiv.org/abs/2401.03129}{https://arxiv.org/abs/2401.03129}
%     \item \href{https://arxiv.org/abs/2302.03241}{https://arxiv.org/abs/2302.03241}
%     \item \href{https://arxiv.org/abs/2309.14763}{https://arxiv.org/abs/2309.14763}
%     \item \href{https://arxiv.org/abs/2406.06962}{https://arxiv.org/abs/2406.06962}
%     \item \href{https://arxiv.org/abs/2406.01392}{https://arxiv.org/abs/2406.01392}
%     \item \href{https://arxiv.org/abs/2404.00790}{https://arxiv.org/abs/2404.00790}
%     \item \href{https://arxiv.org/abs/2406.18708}{https://arxiv.org/abs/2406.18708}
%     \item \href{https://arxiv.org/abs/2406.01375}{https://arxiv.org/abs/2406.01375}
%     \item \href{https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-66132030219-9}{https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-66132030219-9}
%     \item \href{https://openreview.net/forum?id=DimPeeCxKO}{https://openreview.net/forum?id=DimPeeCxKO}
% \end{itemize}

% \end{comment}
