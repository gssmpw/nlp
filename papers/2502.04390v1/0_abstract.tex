\begin{abstract}
Despite remarkable capabilities, large language models (LLMs) struggle to continually update their knowledge without catastrophic forgetting. In contrast, humans effortlessly integrate new information, detect conflicts with existing beliefs, and selectively update their mental models. This paper introduces a cognitive-inspired investigation paradigm to study continual knowledge updating in LLMs. We implement two key components inspired by human cognition: (1) \textit{Dissonance and Familiarity Awareness}, analyzing model behavior to classify information as novel, familiar, or dissonant; and (2) \textit{Targeted Network Updates}, which track neural activity to identify frequently used (\textit{stubborn}) and rarely used (\textit{plastic}) neurons.
Through carefully designed experiments in controlled settings, we uncover a number of empirical findings demonstrating the potential of this approach. First, dissonance detection is feasible using simple activation and gradient features, suggesting potential for cognitive-inspired training. Second, we find that non-dissonant updates largely preserve prior knowledge regardless of targeting strategy, revealing inherent robustness in LLM knowledge integration. Most critically, we discover that dissonant updates prove catastrophically destructive to the model's knowledge base, indiscriminately affecting even information unrelated to the current updates. This suggests fundamental limitations in how neural networks handle contradictions and motivates the need for new approaches to knowledge updating that better mirror human cognitive mechanisms.
\end{abstract}

%Despite remarkable capabilities, large language models (LLMs) struggle to continually update their knowledge without catastrophic forgetting. In contrast, humans effortlessly integrate new information, detect conflicts with existing beliefs, and selectively update their mental models. This paper introduces a novel incremental update paradigm inspired by human cognition. We implement and evaluate two key components within existing LLM architectures: (1) \textit{Dissonance and Familiarity Awareness}, enabling LLMs to classify new information as novel, familiar, or dissonant; and (2) \textit{Targeted Network Updates}, which involve continuously tracking past gradient usage to distinguish between frequently used (\textit{stubborn}) and rarely used (\textit{plastic}) neurons.
% Through a series of carefully designed experiments, we uncover a number of empirical findings and demonstrate the potential of this approach. First, dissonance awareness is feasible even using simple features like activations and gradients. Second, unlike non-dissonant updates which largely preserve prior knowledge even with naive fine-tuning, dissonant updates prove catastrophically destructive to the model's knowledge base, indiscriminately affecting even information unrelated to the current updates. Finally, our history-aware targeted updates, which continuously monitor and leverage past gradient information, alleviate the negative impact of dissonant updates significantly better than state-of-the-art editing methods. We plan to develop dedicated conflict resolution methods in future work.
% New abstract suggestion 1:
%Despite remarkable capabilities, large language models (LLMs) struggle to continually update their knowledge without catastrophic forgetting. In contrast, humans effortlessly integrate new information, detect conflicts with existing beliefs, and selectively update their mental models. Through carefully designed experiments inspired by human cognition, we investigate how LLMs handle knowledge updates, particularly exploring (1) whether LLMs can distinguish between novel, familiar, and dissonant information using activation and gradient patterns, and (2) how different neuron targeting strategies affect knowledge retention.
%Our investigation reveals several fundamental properties of knowledge updates in LLMs. First, simple features from model activations and gradients can effectively distinguish between familiar, novel, and dissonant information. Second, we discover a striking contrast in how LLMs handle different types of updates: while non-dissonant updates largely preserve prior knowledge even with naive fine-tuning, dissonant updates prove catastrophically destructive to the model's knowledge base, indiscriminately affecting even information unrelated to the current updates. This behavior persists across model scales and various neuron targeting strategies, suggesting a fundamental limitation in how neural networks handle contradictory information. These findings motivate the need to rethink how we approach knowledge updating in LLMs.