\section{Discussion and conclusions }\label{sec:lessons}
\subsection{Lessons learned}
\noindent \textbf{Fundamental Properties of Knowledge Updates:} 
Our results reveal a striking pattern: while non-dissonant updates show remarkable robustness, dissonant updates trigger severe corruption of unrelated knowledge - dropping accuracy below 60\% even when modifying just 10-100 facts. This effect persists across our tested model scales and update strategies, suggesting a deeper challenge in how current neural architectures handle contradictory information.

\noindent \textbf{Feasibility of Dissonance Detection:} 
LLMs encode clear signatures distinguishing between novel, familiar, and dissonant information. In our controlled experiments, simple classifiers achieve 95\% accuracy with pre-trained models and 99\% with finetuned models using either activation/gradient features or output probabilities. This suggests the potential for developing mechanisms to identify potentially problematic updates before they occur.

\noindent \textbf{Promise of differentiated plasticity:} 
Avoiding heavily-used neurons during \emph{non-dissonant} updates  improves robustness, maintaining 98\% accuracy on old knowledge (versus 93\% with finetuning). Interestingly, neurons used during pre-training are particularly effective at integrating new knowledge, extending lottery ticket hypothesis findings~\citep{frankle2018lottery} to language models.

\subsection{Limitations and Future Directions}
\noindent \textbf{Experimental Control vs. Scale:} 
While our controlled experiments reveal fundamental properties of knowledge updating, investigating these phenomena in much larger models presents challenges as it is not straightforward to track the impact on their broader knowledge.

\noindent \textbf{Dataset Limitations:} 
Our current findings rely on CounterFact-derived data with simple factual statements. Developing larger, more diverse datasets is essential for understanding how these properties generalize to more complex forms of knowledge and conflicts.

\noindent \textbf{Neuron Classification Metrics:} 
While our analysis of neural plasticity uses gradient magnitudes effectively, future work could explore richer metrics incorporating activation patterns and network connectivity to better understand knowledge distribution and update mechanisms.

\noindent \textbf{Beyond Binary Dissonance:} 
Our current investigation treats dissonance as binary, while real-world knowledge updates often involve varying degrees of conflict and different types of knowledge. Understanding how these nuances affect knowledge integration remains a challenge.

\noindent \textbf{Towards Human-Inspired Updates:} 
The catastrophic nature of dissonant updates suggests we may need fundamentally different approaches to LLM training. Rather than attempting to overwrite existing knowledge, future work might explore mechanisms for maintaining and contextualizing potentially conflicting information - similar to how humans maintain both historical and updated knowledge with appropriate contexts.


% \subsection{Lessons learned}

% \noindent \textbf{Feasibility of Dissonance Awareness:} Language models can classify new information as novel, familiar, or dissonant with roughly  95\% (resp. 99\%) accuracy  by  simply using activation and gradient features from pre-trained (resp. finetuned) models. 

% \noindent  \textbf{Effectiveness of Differentiated plasticity in case of non-dissonant updates:} Avoiding to target stubborn neurons preserves prior knowledge more effectively, allowing  to maintain 98\% accuracy on old knowledge while successfully integrating new information (as opposed to 93\% with finetuning).

% \noindent  \textbf{Insights into Knowledge Integration:} We observed that neurons heavily utilized during pre-training are more effective for integrating new knowledge, echoing findings related to the lottery ticket hypothesis~\citep{frankle2018lottery}. 

% \noindent  \textbf{Challenges with Dissonant Updates unlike non-dissonant updates:} Unlike non-dissonant updates, where all strategies performed relatively well, incorporating conflicting information is significantly destructive to the model's existing knowledge. With all tested strategies, accuracy on old knowledge dropped below 60\% when editing 10 to 100 dissonant facts, highlighting the importance of coupling conflict detection to targeted updates, to successfully preserve old beliefs.

% \subsection{Limitations and Future Directions}

% \noindent  \textbf{Simplified Neuron Classification:} In this paper, the classification of neurons as plastic or stubborn is based solely on cumulative gradient magnitudes. Incorporating additional metrics, such as activation patterns or connectivity analyses, could enhance the effectiveness of targeted update.

% \noindent  \textbf{Scalability to Larger Models and Real-World Data:} While our experiments demonstrate feasibility in controlled settings, scaling our classifier and empirical findings to full-scale language models and more challenging real-world datasets is a significant challenge due to lack of benchmarks: developing much larger and more realistic datasets compared to CounterFact is essential.

% \noindent  \textbf{Modeling Dissonance as a Continuous Spectrum:} For simplicity, the classifier in this paper treats dissonance as a binary phenomenon. However,  in reality the degree of conflict between new and existing information might vary, and so is the type of knowledge. Developing methods to quantify and handle varying degrees of dissonance and knowledge types would allow for more nuanced update strategies for practical cases.

% \noindent  \textbf{Conflict Resolution Mechanisms:} The destructive impact of dissonant updates motivates the exploration of radical alternatives to editing, e.g. by dividing dissonant information into smaller incremental non-dissonant one, contextualizing it in time, similarly to what humans do. This is one of the most intriguing avenues that we look forward to tackle in the future.


%\noindent  \textbf{Network Dynamics and Interdependencies:} Our method does not account for the complex dependencies between neurons: updating plastic neurons may inadvertently affect stubborn neurons due to shared pathways or network architecture. Future work should explore mechanisms to model and mitigate these interactions.
