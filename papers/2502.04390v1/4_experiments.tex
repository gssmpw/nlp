\section{Experimental evaluation}\label{sec:experiments}
We discuss our experimental setup, to evaluate dissonance-awareness, as well as continual knowledge update.

\subsection{Experimental setup}\label{sec:models_and_data}

\textbf{Dataset.}\ \ 
We use the COUNTERFACT dataset~\citep{meng2022locating} as our primary data source as it contains both facts and counterfacts.\footnote{While this dataset allows us to test models' handling of conflicting knowledge, we acknowledge its limitations in representing more complex real-world knowledge, a limitation which we plan to address in the future.}
This dataset, with approximately 17,000 facts, allows us to test models' handling of conflicting knowledge and addition of potentially known information,\footnote{For instance,  general facts that pre-trained models likely were exposed to during  training.} two key aspects of our dissonance-aware approach. To address the lack of truly novel facts in COUNTERFACT, we generate additional data using GPT-3.5. We transform existing statements into plausible yet fictitious information, maintaining structural similarity while introducing novel content. For example, ``Danielle Darrieux's mother tongue is French'' becomes ``Sylvan Myrthil's mother tongue is Sylvan'' (see Appendix \ref{appendix:unknown_facts_prompt} for details).

For our dissonance awareness experiments, we construct a balanced dataset comprising 1,000 samples each of familiar, conflicting, and novel facts. When using the pre-trained \gpttwo-small model, we adjust the familiar class to 600 samples due to the limited number of known facts extracted from the model's pre-training. 
For our targeted update experiments, we use 5-fold cross-validation varying each time the sets of old and new facts but keeping the following proportions: 2000 old facts vs. 1000 new facts. For conflicting updates, we also test with 10 and 100 new facts. 

\textbf{Models.}\ \ We employ models from the GPT family to ease comparison, focusing mostly on \gpttwo-small and \gpttwo-xl (and to lesser extent GPT-J 6B). The dataset size ($\simeq$17,000 facts) limits full stress-testing of larger models like \gpttwo-xl (less visible  catastrophic forgetting in case of non-dissonant updates compared to compressed models). As a result, some of the effects of our experiments are most clearly observed with \gpttwo-small, on which we focus most in the main body of this paper, deferring \gpttwo-xl results (which are aligned to \gpttwo-small) to the Appendix.

We implement experiments using Hugging Face Transformers on NVIDIA GPUs. Before setting learning rates and epochs, we conduct a search for optimal hyperparameters that allow effective learning of facts (see App.~\ref{app:gptxl:search} for an example for \gpttwo-xl). We perform the search based on the ability to correctly learn 10,000 facts from the dataset. %More detailed results and our implementation are available in the code repository.
More details including results can be found in the code repository.


\subsection{Dissonance awareness}\label{sec:classify}

\textbf{Settings.}\ \  Our first goal is to evaluate the ability to discriminate \textit{familiar}, \textit{novel}, and\textit{ conflicting} information using the readily-available\footnote{We explore the use of model output-only features in Appendix.~\ref{app:diss:aware:prob} showing that using output probabilities as feature is also successful.} simple features we extract from the models during the forward and backward passes. As schematized in Fig.~\ref{fig:exp:overview} (left), we do so by relying on simple classifiers (random forests and SVMs), contrasting two scenarios for the \textit{input features}: (1) a \gpttwo\ model fine-tuned on 1000 facts (the knowns), and (2) a \gpttwo\ pre-trained model  (using its 600 extracted known facts as known class samples). 

For each scenario, we compile a balanced dataset with equal examples per class (familiar, novel, conflicting). 
To create novel facts, we employ GPT-4 with carefully designed prompts, using the structure of known facts (subject, relation, object) as templates, replacing key elements with fictitious (but plausible) information. This method ensured structural similarity to known facts while maintaining novelty. Appendix \ref{appendix:unknown_facts_prompt} provides detailed prompts and examples (full datasets will be made available upon acceptance).

\textbf{Classification performance}\ \  We extracted activations (A) and gradients (G) as described in Appendix~\ref{app:notation:extraction}, 
and experimenting with A, G and A+G as input feature sets, using raw (R), per-layer (L) and historical (H) normalization strategies. As classifiers, we employ Random Forest (RF) and Support Vector Machines (SVM), optimizing hyperparameters using Bayesian search with 5-fold cross-validation.  For clarity, we report the best results for each combination in Table~\ref{tab:classification_results}
(average and standard deviation accuracy over the 5-folds) 
and defer the full results and ablation study to Table~\ref{tab:classification_results_appendix} in the appendix for the interested reader.


\begin{table}[h]
\vspace{-0.3cm}
\centering
\caption{Classification Results}
\label{tab:classification_results}
\begin{tabular}{llc}
\toprule
\bf Scenario &  \bf Classifier & \bf Accuracy \\
\midrule
\multirow{2}{*}{Fine-tuned} & SVM (A+G, H) & 0.995 (0.001)\\
& RF~~~~~(A+G, R) & 0.988 (0.001)  \\
\midrule
\multirow{2}{*}{Pre-trained} & SVM (A+G, H) & 0.947 (0.004)\\
& RF~~~~~(A+G, R) & 0.928 (0.012)  \\
\bottomrule
\end{tabular}
 \vspace{-0.6cm}
\end{table}

%{\color{red}PM: confusing. Models should refer to SVM or RF, not the finetuned or pretrained, no?}% zi: I agree, I fixed it.
Models consistently achieve high performance. Using features from the finetuned model reaches as high as (99.5\%), but also using features from a pre-trained model still achieves decent performance (94.7\%). Interestingly, combining activations and gradients consistently outperformed using either feature set alone, with a slight advantage of SVM over RF. Also, historical normalization helps SVM, but does not provide benefits for RF.
 

\begin{figure*}[h]
\centering
%/home/sclemente/project/epmem_edit/analysis/plots_exp_1_pt_zied.ipynb 
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/2_classifier/experiment_1_ft_feature_importance_act_grad.pdf}
  \caption{Finetuned model}
  \label{fig:feature_importance_ft}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/2_classifier/experiment_1_pt_feature_importance_act_grad.pdf}
  \caption{Pretrained model}
  \label{fig:feature_importance_pt}
\end{subfigure}
\caption{\textit{Dissonance awareness}. Feature importance showing the higher importance of gradient-related features for finetuned models.}\label{fig:feature_importance} % 
\end{figure*}

\textbf{Feature importance}\ \  
While a full analysis of feature explainability is outside the scope of this paper, we further seek to observe structural differences in the learning process by comparing feature importance, using the feature importance scores derived from the random forest algorithm.

To further shed light on such difference, Fig.~\ref{fig:feature_importance} opposes feature importance in both cases, focusing on Activation versus Gradient-related features. It turns out that in the finetuned scenario, gradient-based features are substantially more important. This is likely due to the fact that finetuning the models on these facts has somewhat overfit them leading to gradients that are more discriminative: e.g. a clearly null gradient for known facts and a clearly high one for unknown ones. For the pretrained scenario, however, which is the most likely case in a real case scenario, both activation and gradient features contribute significantly, suggesting that for long-term knowledge, both internal representations and learning dynamics should be mixed in order to achieve good classification. Appendix.\ref{app:feature:importance} expands this analysis by focusing on transformer block importance instead.

Finally, deferred to the appendix, comparing the performance of different normalization strategies for the pretrained model using both activations and gradients (Table \ref{tab:classification_results_appendix}), we found that although normalization slightly helps, historical normalization does not seem to be crucial, since it was only slightly helpful for Random Forest classifiers.

\textbf{Key findings}\ \   Overall, despite the simplicity of our features, the results demonstrate the feasibility of distinguishing between familiar, novel, and conflicting information, even in the challenging case of using pre-trained models, providing the needed foundation for dissonance-aware updates, which we explore in the next experiments. 

\subsection{Non-dissonant updates}\label{sec:non:dissonant} 

\textbf{Settings.}\ \  We now investigate how LLMs handle \textit{non-dissonant updates} using our different strategies as experimental tools.
In our experiments, schematized in Fig.~\ref{fig:exp:overview} (middle), we evaluate the incorporation of non-conflicting facts into our models.
The pipeline consists of (i) training on 2,000 initial facts (old) while collecting historical gradients to identify stubborn neurons, (ii) simulating updates with 1,000 new facts (new) to collect current gradients for candidate identification and (iii) applying different neuron selection strategies to update the model with these 1,000 facts. Note that while we track 2,000 facts as proxy for old knowledge, this represents a smaller fraction of \gpttwo-xl's total knowledge compared to \gpttwo-small, limiting our visibility into effects on other untracked pre-trained knowledge.

For our experiment below inspired by the lottery ticket hypothesis~\citep{frankle2018lottery}, we used 10,000 separate facts for gradient extraction before training a fresh model on the 2,000+1,000 facts described above.
%Due to space constraints, we defer comprehensive ablation studies and additional experiments to Appendix~\ref{app:update}.

\textbf{Results.}\ \  Fig.~\ref{fig:noconflict:small} presents the accuracy of various neuron update strategies on old and new knowledge for \gpttwo-small, including error bars representing standard deviations over five runs. We observe that simple fine-tuning leads to a  slight degradation in performance on old knowledge, dropping to approximately 93\% accuracy. In contrast, updating plastic neurons helps preserving old knowledge, with accuracy remaining above 98\% even when using up to 20,000 neurons. Random neuron selection exhibits a similar behavior. However, using candidate or stubborn neurons results in slightly more degradation to old knowledge. 
%Interestingly, the specific neurons strategy strikes a balance between learning new knowledge efficiently and preserving old knowledge: higher accuracy on new knowledge with fewer neurons while minimizing the impact on old knowledge.

%%%%%%%%%%%%%%%%%% WITH LORA results %%%%%%%%%%%%%%%%%%%%%
\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./figures/3_incremental_update/experiment2_1_withLoRa/experiment_2_1_paper_version_LoRA_neuron_update_strategies_old_knowledge_LoRA.pdf}
        \subcaption{Old Knowledge}\label{fig:old:noconflict:small}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./figures/3_incremental_update/experiment2_1_withLoRa/experiment_2_1_paper_version_LoRA_neuron_update_strategies_new_knowledge_LoRA.pdf}
        \subcaption{New Knowledge}\label{fig:new:noconflict:small}
    \end{subfigure}
    \caption{\textit{Non-dissonant updates}: Old vs new knowledge for targeted updates on \gpttwo-small (see Fig.~\ref{fig:gpt2xl:full:non-conflict} and \ref{fig:pareto_mosaic_combined} for \gpttwo-XL)}\label{fig:noconflict:small}    %\zbh{/home/sclemente/project/epmem\_edit/analysis/plots\_exp\_2\_1\_withB.ipynb}\DR{main}}
\end{figure*}

Interestingly, the strategies differ also in their ability to ingest new knowledge. Plastic neurons preserve old knowledge but struggle to ingest new knowledge requiring comparatively more neurons. This contrasts with specific neurons which strike the best balance, learning new knowledge with fewer neurons while maintaining acceptable levels of old knowledge retention.

An intriguing pattern is that targeting stubborn, candidate, or specific neurons allows the model to learn new knowledge using fewer parameters compared to targeting plastic neurons. This finding resonates with the existence of winning subnetworks, as suggested by the lottery ticket hypothesis~\citep{frankle2018lottery}. It implies that certain subnetworks within the model are more conducive to integrating new information, compared to others. We conduct further experiments that confirm this hypothesis in App~\ref{app:lottey}.

Finally, we conducted the same experiments on \gpttwo-xl, where our 2,000 tracked facts represent \textit{a much smaller portion of the model's knowledge}. With this larger capacity, interference with tracked facts becomes \textit{naturally less likely}, leading to all strategies showing even better preservation of our \textit{monitored} knowledge, compared to \gpttwo-small.
As shown in Fig.~\ref{fig:pareto_mosaic_combined} and Fig.~\ref{fig:gpt2xl:full:non-conflict} in the appendix, all strategies tend to preserve old unrelated knowledge; and differ, as in the case of \gpttwo-small, in their ability to integrate new knowledge efficiently. Detailed analyses, including the effects of varying learning rates and neuron counts, are provided in Appendix~\ref{app:gpt2xl:noconflict}. 
%Overall, we found that observing similar effects in \gpttwo-xl required adjusting either the learning rate, the number of neurons allocated for updates, or learning longer. 

\textbf{Key Findings.}\ \   
LLMs show remarkable robustness when incorporating non-dissonant information, as long as heavily-used (stubborn) neurons are avoided.
Updating plastic neurons helps preserving old knowledge but requires more parameters (or time) to achieve high accuracy on new knowledge. Targeting specific neurons offers a balanced approach, enabling efficient knowledge integration with minimal impact on existing information. 

\subsection{Dissonant Updates}\label{app:dissonant}

\textbf{Settings.}\ \  We examine how LLMs handle conflicting information as shown in Fig.~\ref{fig:exp:overview} (right): after training on 2,000 old facts and 1,000 new facts, we introduce 1,000 conflicting updates contradicting previously learned new facts, applying our various strategies to study their impact on learning conflicting facts and preserving unrelated old knowledge.

%\textbf{Results.} Fig.~\ref{fig:editing:small:1000} shows the performance of various strategies on \gpttwo-small. We track three metrics: retention of old knowledge (unrelated 2,000 facts), learning of new conflicting facts, and generalization to paraphrased versions.

\textbf{Results.}\ \   We illustrate the performance of various strategies on \gpttwo-small in Fig.~\ref{fig:editing:small:1000}. Again, old knowledge refers to the 2000 initial \textit{completely unrelated} facts and new knowledge corresponds to the 1000 conflicting ones. Surprisingly, we find that dissonant updates are highly destructive to the retention of such unrelated knowledge, regardless of the neuron update strategy employed. Even when updating plastic neurons, which are presumed to be underutilized and thus less likely to interfere with existing knowledge, we observe significant degradation in the model's ability to recall old facts.

% With LORA results
\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./figures/4_editing/experiment3_1_withLoRa/experiment_3_1_paper_version_LoRA_submitted_conf_neuron_update_strategies_old_knowledge.pdf}
        \subcaption{Old Knowledge}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./figures/4_editing/experiment3_1_withLoRa/experiment_3_1_paper_version_LoRA_submitted_conf_neuron_update_strategies_new_knowledge.pdf}
        \subcaption{New Knowledge}
    \end{subfigure}
    \caption{\textit{Dissonant updates}: Impact of 1000 dissonant facts and \gpttwo-small (see Fig.~\ref{fig:knowledge_editing_performance} for \gpttwo-XL)} \label{fig:editing:small:1000}
\end{figure*}

Given the destructive nature of simultaneously adding 1,000 conflicting facts, we conducted additional experiments where we added only 100 and 10 facts. The results, detailed in Appendix~\ref{app:diss:nfacts}, indicate that while the impact on old knowledge retention is less severe when editing fewer facts, the destructive effect remains prominent. This reminds us the performance of state-of-the-art model editing methods such as ROME~\citep{Meng2022} and MEMIT~\citep{Meng2022a} which also deteriorates when applied to multiple sequential edits, as opposed to single-edit evaluations typically reported in the literature. While our primary focus is not on developing new model editing techniques, we leverage \texttt{EasyEdit}~\citep{wang2023easyedit} to benchmark the above existing methods under our same multi-fact experimental conditions (See App.~\ref{app:rome:memit}). 

%Our primary focus in this work is \textit{not} on developing new model editing techniques. Most editing techniques focus on altering existing associations, and are hence by our definition dissonant. Our empirical findings suggest another parallel path in which editing is abandoned in favor of non-dissonant variations where old knowledge is kept and contextualized

Finally, we also performed experiments with \gpttwo-xl under various conditions, deferred to  Appendix \ref{app:diss:xl} for space constraints. Overall, similarly to the non-dissonant case, \gpttwo-xl exhibits different scaling behaviors. Surprisingly though, even when not learning conflicting knowledge, and despite having much more parameters, \textit{\gpttwo-xl also experiences significant degradation in old knowledge retention}-- further confirming the catastrophic nature of dissonant updates (See Fig.~\ref{fig:knowledge_editing_performance}).
As shown in Fig.~\ref{fig:lora_comparison}, GPT-J6B also confirms the same tendency.
%\label{app:dissonant} \label{app:xl:neurons:10} 

\textbf{Key Findings.}\ \ Dissonant updates pose a significant challenge, as they are destructive to prior unrelated knowledge, regardless of model size and even when targeting unused neurons. This underscores the importance of dissonance awareness to detect and appropriately handle conflicting information during continual learning. Our results motivate the integration of dissonance classifiers directly into the update or training of large language models. Thus, developing dedicated conflict resolution methods remains an essential direction for future work.

%\paragraph{Key Findings.} The catastrophic nature of dissonant updates, persisting across model sizes and strategies, suggests we need fundamentally different approaches to handling conflicting information in LLMs - perhaps moving from direct editing toward maintaining and contextualizing conflicting knowledge, similar to how humans handle updated information.

% \begin{figure*}[t]
%     \centering
%     \begin{subfigure}[t]{0.43\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/1_general/combined_plots/combined_comparison_in_gpt2small.pdf}
%         \caption{GPT-2 Small}
%         \label{fig:gpt2small_nonlora}
%     \end{subfigure}
%     \begin{subfigure}[t]{0.43\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/1_general/combined_plots/combined_comparison_out_gpt2xl.pdf}
%         \caption{GPT-2 XL}
%         \label{fig:gpt2xl_nonlora}
%     \end{subfigure}
%     \caption{Difference between dissonant and non-dissonant updates {\color{blue} remove/move appendix}}
%     \label{fig:nonlora_comparison}
% \end{figure*}


% \begin{figure}[t]
%     \centering
%     \begin{subfigure}[b]{0.318\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{./figures/4_editing/experiment3_1/experiment_3_1_2000_1000_neuron_update_strategies_old_knowledge.pdf}
%         \subcaption{Old Knowledge}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.318\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{./figures/4_editing/experiment3_1/experiment_3_1_2000_1000_neuron_update_strategies_new_knowledge}
%         \subcaption{New Knowledge}
%     \end{subfigure}
%        \begin{subfigure}[b]{0.31\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{./figures/4_editing/experiment3_1/experiment_3_1_2000_1000_neuron_update_strategies_general_knowledge.pdf}
%      \subcaption{Generalization}
%     \end{subfigure}    
%     \caption{\textit{Dissonant updates}: Impact of 1000 dissonant facts and \gpttwo-small}\label{fig:editing:small:1000}
%     %\zbh{/home/sclemente/project/epmem\_edit/analysis/plots\_exp\_3\_1.ipynb}
% \end{figure}

% % With LORA results
% \begin{figure}[t]
%     \centering
%     \begin{subfigure}[b]{0.318\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{./figures/4_editing/experiment3_1_withLoRa/experiment_3_1_paper_version_LoRA_submitted_conf_neuron_update_strategies_old_knowledge.pdf}
%         \subcaption{Old Knowledge}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.318\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{./figures/4_editing/experiment3_1_withLoRa/experiment_3_1_paper_version_LoRA_submitted_conf_neuron_update_strategies_new_knowledge.pdf}
%         \subcaption{New Knowledge}
%     \end{subfigure}
%        \begin{subfigure}[b]{0.31\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{./figures/4_editing/experiment3_1_withLoRa/experiment_3_1_paper_version_LoRA_submitted_conf_neuron_update_strategies_general_knowledge.pdf}
%      \subcaption{Generalization}
%     \end{subfigure}    
%     \caption{\textit{Dissonant updates}: Impact of 1000 dissonant facts and \gpttwo-small}\label{fig:editing:small:1000}
% \end{figure}