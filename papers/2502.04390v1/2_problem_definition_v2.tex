\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\textwidth]{figures/1_general/Experimental.pdf}
    %{figures/1_general/Experimental_overview.png}
    \caption{Overview of our emprical investigation pipeline (i) Classification pipeline to identify novel, familiar, and dissonant information using inner or output model features. (ii) Targeted update strategies for non-dissonant updates and (iii) dissonant updates.}
    \label{fig:exp:overview}
\end{figure*}

\section{Dissonance-aware Targeted Updates}\label{sec:methodology}
Our cognitively-inspired investigation begins with a striking empirical discovery. Fig.~\ref{fig:lora_comparison} reveals a fundamental distinction in how language models handle different types of updates: while non-dissonant information ({\color{customgreen}green}) can often be incorporated while preserving existing knowledge, dissonant updates ({\color{customred}red}) prove catastrophically destructive across all model scales and training approaches. This pattern emerged from our controlled experimental pipeline (Fig.~\ref{fig:exp:overview}): when training models on 2,000 initial facts followed by 1,000 new facts, we observe dramatic differences between non-dissonant and dissonant updates in both their ability to learn new information (y-axis) and preserve original knowledge (x-axis). The stark contrast between these two types of updates motivates our systematic methodology: if dissonant updates are inherently destructive, can we develop mechanisms to identify them before they occur? And can targeted plasticity strategies help protect existing knowledge? To answer these questions, we first develop methods to extract and track neural activity patterns (Sec.~\ref{sec:extraction}), which serve two purposes: enabling classification of incoming information as novel, familiar, or dissonant (Sec.~\ref{sec:classify}), and informing targeted update strategies that carefully control where new knowledge is stored in the network (Sec.~\ref{sec:targeted}). 


\begin{figure*}[h!]
    \centering
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/1_general/combined_plots/combined_comparison_in_gpt2small_lora_large.pdf}
        \caption{GPT-2 Small }
        \label{fig:gpt2small_lora}
    \end{subfigure}
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/1_general/combined_plots/combined_comparison_out_gpt2xl_lora.pdf}
        \caption{GPT-2 XL }
        \label{fig:gpt2xl_lora}
    \end{subfigure}
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/1_general/combined_plots/combined_comparison_out_gpt-j_lora_large.pdf}
        \caption{GPT-J-6B }
        \label{fig:gptj_lora}
    \end{subfigure}
    \caption{Early demonstration of a fundamental pattern we discovered across model sizes: the catastrophic nature of dissonant updates compared to non-dissonant ones. Results shown for GPT-2 Small \ref{fig:gpt2small_lora}, GPT-2 XL \ref{fig:gpt2xl_lora}, and GPT-J-6B \ref{fig:gptj_lora}, comparing full fine-tuning (stars) and LoRA (crosses) approaches. The stark contrast between dissonant ({\color{customred}red}) and non-dissonant ({\color{customgreen}green}) updates persists across model scales and training methods\protect\footnotemark, motivating our systematic investigation into this fundamental challenge of knowledge integration.}
    \label{fig:lora_comparison}
\end{figure*}

\footnotetext{For full finetuning, models trained until convergence on new facts, while LoRA experiments used fixed hyperparameters across both conditions. This dual approach revealed two phenomena: (1) not shown in the figure, full finetuning needed twice as many epochs to learn dissonant information compared to non-dissonant and (2) under fixed conditions with LoRA, unlike non-dissonant facts, models struggled to learn dissonant information (lower y-axis values for red crosses) while still exhibiting the same catastrophic interference with existing knowledge (low x-axis values).}


\subsection{Extraction of historical activations and gradients}\label{sec:extraction} 
We maintain an aggregate profile of neuronal activity by accumulating activations and gradients for each neuron at every training step. Specifically, for each neuron $n$ in the Transformer blocks—including feed-forward (\textsc{MLP}) layers and attention projections (Key, Query, Value matrices)—we compute $H\hat{G}_n$ 
, the cumulative \textit{historical gradient} magnitude over time, and $H\hat{A}_n$ 
, the cumulative \textit{historical activation} magnitude over time. To mitigate scale differences across layers, we also experiment with layer-wise normalization of activations and gradients before accumulation. Precise notation and more details are in Appendix~\ref{app:notation:extraction}. 

This historical activity data enables us to classify neurons as ``plastic'' or ``stubborn'' based on their past usage (see Fig.\ref{fig:gradient_distribution} for a visual illustration), which is useful for our targeted network updates. We use the historical data also to normalize the input features when classifying facts as we see next.


\subsection{Dissonance and Novelty Awareness} 
We cast our classification problem on three classes: for a given input sequence \( X \), decide if it is \textit{Novel} (e.g. could be integrated), \textit{Familiar} (e.g. can be ignored), or \textit{Dissonant} (likely requiring proper resolution).

We design a simple classifier that leverages activation and gradient information to assess the nature of new information. 
For any input sequence \( X \), we first perform a forward pass to obtain its \textit{current activations} and a backward pass to obtain its \textit{current gradients} (without updating the model weights). 
Since the goal is to assess feasibility using easy-to-compute features and lightweight methods that could be integrated into large-scale models, we extract for each layer the mean, standard deviation, minimum, maximum, and quartiles (Q1, Q2, Q3) of the activations and gradients, eventually first normalized by historical activations and gradients. We perform ablation studies to assess the importance of different features and employ feature importance analyses to understand which aspects contribute most to the classifier's performance. We evaluate our ability to classify facts in Sec.~\ref{sec:classify}. Despite using simple classifiers like Random Forests and SVMs, we achieve high accuracy, opening the way for future integration of dissonance awareness into LLM training pipelines. 

\begin{figure}[!t]
    \centering
    \begin{tikzpicture}[inner sep=0pt, outer sep=0pt]
        % Base (larger) figure
        \node[anchor=south west] (base) at (0,0) {
            \includegraphics[width=0.485\textwidth]{figures/1_general/gradient_distribution.pdf}
        };      
        % Overlay with relative positioning
        \node[anchor=north east] at ([xshift=-0.02\textwidth, yshift=-0.5cm]base.north east) {
            \includegraphics[width=0.2\textwidth]{figures/1_general/continual-learning-diagram.pdf}
        };
        % Set a tight bounding box around the actual content
        \useasboundingbox (current bounding box.south west) rectangle (current bounding box.north east);
    \end{tikzpicture}
    \caption{Illustration of how we use historical neuron activity (here we show the distribution of cumulative gradients during a \gpttwo-xl previous training) to identify localized areas where to store future knowledge, according to four strategies.}
    \label{fig:gradient_distribution}
\end{figure}

\subsection{Targeted Neuron Updates}\label{sec:targeted}
Building upon the historical tracking of neural activity, we implement targeted network updates to 
%incorporate new knowledge into the model's parameters while preserving existing information. 
study the \textit{impact of knowledge placement location on new knowledge ingestion and past knowledge retention.}
We design four main types of targeted updates, which we experimentally evaluate. 
During training on new information, we perform standard forward and backward passes to compute the loss and gradients. Before the optimizer step, we modify the gradients to freeze certain neurons. Specifically, given the gradients for all parameters of a given layer, we zero-out those that do not belong to the selected set of neuron and corresponding weights, defined as plastic, stubborn, candidate and specific, as described below. This process effectively freezes the weights of non-selected neurons, allowing for targeted updates to specific parts of the model. By varying the choice of selected neurons, we control how new information is integrated into the model while managing its impact on existing knowledge. Next, we introduce strategies to select which neurons and weights to update:  Figure \ref{fig:gradient_distribution} illustrates the conceptual relationship between the various neuron updates strategies within the model's parameter space.
%{\color{red}I still find the ranking operators confusing.}

%\paragraph{Plastic Neurons.} 
\noindent \textbf{Plastic Neurons.} Neurons  underutilized during past model updates. To identify them, we rank neurons by increasing historical gradient values and select the top $N$ neurons with the lowest cumulative gradients:
        \[
        \mathcal{N}_{\text{plastic}} = \{ n \mid \text{rank}(H\hat{G}_n) \leq N \},
        \]
    %{\color{red}This should be top-N of the reverse historical rank.}    
    where $H\hat{G}_n$ is the historical gradient for neuron $n$, accumulated over all prior training. This allows to assess whether targeting underutilized neurons can integrate new knowledge without interfereing with existing one.

\noindent \textbf{Stubborn Neurons.} Neurons that accumulated high historical gradients, indicating significant involvement in previous learning. We rank neurons by decreasing historical gradient values and select the top $N$ neurons:
        \[
        \mathcal{N}_{\text{stubborn}} = \{ n \mid \text{rank}(H\hat{G}_n) > |\mathcal{N}| - N \},
        \]
    %{\color{red}This should be top-N of the historical rank.}    
    where $|\mathcal{N}|$ is the total number of neurons, and $H\hat{G}_n$ is the historical gradient for neuron $n$. Updating stubborn neurons allows us to test the model's capacity for knowledge integration and assess the potential risks of overwriting existing information.

\noindent \textbf{Candidate Neurons.} These  neurons are relevant for encoding new information: to identify them, we perform a single back-propagation pass on the new input data, without updating the model weights. We then rank neurons based on the magnitude of these gradients and select the top $N$:
        \[
        \mathcal{N}_{\text{candidate}} = \{ n \mid \text{rank}(G_n^{\text{new}}) > |\mathcal{N}| - N \},
        \]
    %{\color{red}This should be the top-N of the ``new'' rank.}    
    where $G_n^{\text{new}}$ is the gradient for neuron $n$ obtained from the back-propagation pass on the new input data. Targeting candidate neurons focuses updates on areas of the network that are most relevant to the new information, as suggested by the back-propagation process.

\noindent \textbf{Specific Neurons.} 
In another strategy, we identify neurons from the candidate set that do not intersect with the stubborn, in an attempt to store new information efficiently while avoiding interference with existing knowledge. For this, we first: (1) identify stubborn neurons $\mathcal{N}_{\text{stubborn}}$, using  $N$ as defined earlier; we next (2) rank all neurons based on the magnitude of their gradients $G_n^{\text{new}}$ obtained from a single back-propagation pass on the new data, without updating model weights; finally, (3) we select few specific neurons, by choosing the top $N$ neurons that are not in $\mathcal{N}_{\text{stubborn}}$:
    \[
    \mathcal{N}_{\text{specific}} = \text{Top}_N(\mathcal{N}_{\text{all}} \setminus \mathcal{N}_{\text{stubborn}}),
    \]
    %{\color{red}This should be the candidate set that does not intersect with the stubborn.}
    where $\mathcal{N}_{\text{all}}$ is the set of all neurons ranked by their gradient magnitudes.
    This last approach ensures that we select neurons that are most relevant to the new information (high gradient) while explicitly avoiding those that are crucial for existing knowledge (stubborn neurons).



% \begin{figure*}[t]
%     \centering
%     \begin{subfigure}[t]{0.32\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/1_general/combined_plots/combined_comparison_in_gpt2small.pdf}
%         \caption{GPT-2 Small}
%         \label{fig:gpt2small_nonlora}
%     \end{subfigure}
%     \begin{subfigure}[t]{0.32\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/1_general/combined_plots/combined_comparison_out_gpt2xl.pdf}
%         \caption{GPT-2 XL}
%         \label{fig:gpt2xl_nonlora}
%     \end{subfigure}
%     \begin{subfigure}[t]{0.32\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/1_general/combined_plots/combined_comparison_out_gpt-j.pdf}
%         \caption{GPT-J-6B}
%         \label{fig:gptj_nonlora}
%     \end{subfigure}
% \caption{Avoiding stubborn neurons does not help for larger models. For smaller models the effect is not visible outside of gpt2-small due to limits in how we.. {\color{blue} Likely a bug in GPT-J-6B}}
%     \label{fig:nonlora_comparison}
% \end{figure*}


%%% This would have been better than the previous plot but too late...
% \begin{figure*}[t]
%     \centering
%     \begin{subfigure}[t]{0.29\textwidth}
%         \centering
%         \includegraphics[height=4.5cm]{figures/1_general/classifier.png}
%         \caption{Classifier Pipeline}
%         \label{fig:classifier_pipeline}
%     \end{subfigure}
%     \hspace{-0.3cm}
%     %\hfill
%     \begin{subfigure}[t]{0.38\textwidth}
%         \centering
%         \includegraphics[height=4.5cm]{figures/1_general/non-dissonant_pipeline.png}
%         \caption{Non-dissonant update pipeline}
%         \label{fig:update_pipelines}
%     \end{subfigure}
%     \hspace{-0.5cm}
%     \begin{subfigure}[t]{0.33\textwidth}
%         \centering
%         \includegraphics[height=4.5cm]{figures/1_general/dissonant_pipeline_2.png}
%         \caption{Non-dissonant update pipeline}
%         \label{fig:update_pipelines}
%     \end{subfigure}
%     \hspace{-0.5cm}
%     \caption{Overview of our empirical approach. (a) Classification pipeline to identify novel, familiar, and dissonant information using model features. (b) Targeted update strategies for non-dissonant updates and (c) dissonant updates.}
%     \label{fig:pipelines}
% \end{figure*}
