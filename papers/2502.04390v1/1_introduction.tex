\section{Introduction}\label{sec:intro}
Humans effortlessly update their knowledge as they experience the world. They seamlessly integrate new information, ignore redundant stimuli, and actively resolve conflicts with existing beliefs before updating their mental models. This cognitive flexibility stems from several key abilities. Humans exhibit (1) \textit{selective attention}, focusing on novel or relevant information while filtering out irrelevant or familiar stimuli~\citep{posner1990attention, petersen2012attention, desimone1995neural, ranganath2003neural}.
They readily (2) \textit{detect conflicts}~\citep{croyle1983dissonance} between new information and existing knowledge and actively engage in resolving them, a process known in psychology as cognitive-dissonance~\citep{festinger1957theory, van2009neural}. Moreover, their brains exhibit a form of (3) \textit{adaptive plasticity}, allowing for updates to neural networks that can incorporate new information while often preserving existing knowledge. 
While the exact mechanisms are still being investigated, this process seems to balance the stability of well-established knowledge with flexibility in the face of new or uncertain information~\citep{mcclelland1995there,behrens2007learning}.

\begin{table*}[ht!]
\centering
\caption{Taxonomy of Incremental Learning Approaches. See Appendix.\ref{app:related} for an extended version}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}l p{3cm} c c c c c c@{}}
\toprule
\textbf{Examples} & \textbf{\makecell{Incremental \\ Type}} & \textbf{\makecell{Memory \\ Usage}} & \textbf{\makecell{Task \\ Awareness}} & \textbf{\makecell{Weight \\ Plasticity}} & \textbf{\makecell{Architecture}} & \textbf{\makecell{Conflict \\ Detection}} & \textbf{\makecell{Update \\ Mechanism}} \\ \midrule
iCaRL~\citep{rebuffi2017icarl} & Class-incremental & Replay & Task-Agnostic & Fixed & Fixed & No & Rehearsal \\
EWC~\citep{kirkpatrick2017overcoming} & Task-incremental & None & Task-Aware & Selective & Fixed & No & Regularization \\
Progressive Nets~\citep{rusu2016progressive} & Task-incremental & None & Task-Aware & Fixed & Expanding & No & New Subnetworks \\
DEN~\citep{yoon2017lifelong} & Task-incremental & None & Task-Aware & Selective & Expanding & No & Selective Expansion \\
GEM~\citep{lopez2017gradient} & Task-incremental & Replay & Task-Aware & Constrained & Fixed & No & Constrained Optimization \\
ROME~\citep{DeCao2021} & Fact-incremental & None & Fact-Aware & Localized & Fixed & No & Rank-One Update \\
OWM~\citep{zeng2019continual} & Task-incremental & None & Task-Aware & Orthogonal & Fixed & No & Orthogonal Projection \\
PackNet~\citep{mallya2018packnet} & Task-incremental & None & Task-Aware & Selective & Fixed & No & Weight Masking \\
HAT~\citep{serra2018overcoming} & Task-incremental & None & Task-Aware & Selective & Fixed & No & Attention Masking \\
\textbf{This paper} & Fact-incremental & None & Conflict-Aware & Selective & Fixed & Yes & Neuron-Specific Update \\ \bottomrule
\end{tabular}%
}
\label{tab:continual-learning-taxonomy}
\end{table*}

Despite demonstrating remarkable capabilities, Large Language Models (LLMs) are still far from such learning abilities. Current LLMs face significant challenges in real-world deployment and long-term utility due to their static nature and training paradigms. They suffer from catastrophic forgetting~\citep{kirkpatrick2017, kemker2018, li2022Catast, luo2024, kotha2024}, where incorporating new information often leads to the erasure of previously learned knowledge. Furthermore, LLMs engage during training in indiscriminate learning, passively accepting all training data, even when it contradicts what they already learned. Despite emergent sparsity~\citep{jaiswal2023, mirzadeh2024}, knowledge in LLMs follows backpropagation and the objective function, with no explicit mechanism for targeted knowledge storage or retrieval. This results in a situation where all weights are potential candidates for storing knowledge, necessitating comprehensive retraining to properly incorporate new information.

In this work, we embark on a systematic empirical investigation of how LLMs handle knowledge updates, drawing inspiration from human cognitive traits. Through carefully controlled experiments, we examine (1) the feasibility of \textit{Dissonance Awareness}, i.e. whether it is possible to correctly classify facts into novel, familiar, and dissonant using features extracted from the LLM. We also investigate the benefits of (2) \textit{Adaptive Plasticity} by studying how different neuron targeting strategies affect knowledge retention and update. For this, we develop a simple method for tracking historical neuron usage to identify "plastic" (rarely used) and "stubborn" (previously used) neurons, allowing us to study how knowledge updates affect different regions of the model's parameter space. This experimental framework lets us systematically investigate fundamental properties of knowledge integration in LLMs.

Our investigation reveals a fundamental distinction in how LLMs handle knowledge updates: the case of non-dissonant updates (adding entirely new knowledge) versus dissonant updates (modifying existing associations). While prior work has focused mostly on editing individual factual associations~\citep{Meng2022,mitchell2022fast,Meng2022a} or preserving knowledge across distinct tasks as in continual learning ~\citep{rebuffi2017icarl, kirkpatrick2017overcoming, mallya2018packnet}, our controlled experiments take a different approach. We systematically study how the placement of new knowledge in the network's parameter space affects both the integration of that knowledge and its impact on existing, unrelated knowledge. As shown in Tab.~\ref{tab:continual-learning-taxonomy}, this positions our work uniquely: rather than proposing new editing or continual learning methods, we reveal fundamental properties about how LLMs handle knowledge integration in both dissonant and non-dissonant scenarios. Critically, our experimental design allows us to precisely track the impact of updates on a controlled set of initial knowledge, providing clear visibility into how different update strategies affect unrelated information.

\textbf{Key takeaways.} This leads us to uncover several fundamental properties of LLM knowledge updating:  (i) \textit{dissonance awareness} is feasible using simple model features, suggesting potential for cognitive-inspired training; (ii) LLMs show inherent robustness when incorporating non-dissonant information, largely preserving prior knowledge regardless of targeting strategy; (iii) avoiding heavily-used (stubborn) neurons during updates further improves this robustness, motivating \textit{adaptive plasticity} in this scenario; (iv) regions of the network heavily used during pre-training are particularly effective at incorporating new knowledge, extending lottery ticket hypothesis findings~\citep{frankle2018} to language models; and most critically, (v) dissonant updates prove catastrophically destructive to unrelated knowledge, suggesting fundamental limitations in how neural networks handle contradictions: while some of our targeted update strategies show comparable performance to existing editing methods like \textsc{ROME} and \textsc{MEMIT}, all approaches fundamentally struggle with dissonant updates, suggesting the need for fundamentally different mechanisms.

\textbf{Implications.} These findings point to concrete opportunities such as the feasibility of dissonance awareness, and the benefits of adaptive plasticity in case of non-dissonant updates. But they also reveal fundamental challenges when handling contradictory information. Current approaches essentially attempt to erase and replace old knowledge - a process we show leads to catastrophic forgetting of even unrelated information. But this contrasts sharply with human cognition, where we maintain both old and new knowledge with appropriate temporal context. Consider how humans handled learning that Pluto was no longer classified as a planet: rather than erasing our previous understanding, we maintained both pieces of knowledge, understanding their historical context and why the classification changed. Our experiments  motivate the exploration of future fundamentally different mechanisms for handling contradictions - ones that can maintain and contextualize conflicting information rather than attempting to overwrite it.\footnote{Code available at \url{https://github.com/bendiogene/ConflictAwareLLM/}}
%While tracking neural activity can inform better update strategies, the complex interdependencies between neurons make truly isolated updates challenging in case of dissonance. These findings, demonstrated in controlled settings with smaller models, motivate the development of more cognitively-inspired approaches to knowledge updating in LLMs.

%%%%%%%%%%%%%%%%%%%%%%%OLD LIMITS%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\paragraph{Limits.} In addition to promising results and insights, we also uncover significant challenges. Updates to plastic neurons can sometimes inadvertently interfere with stubborn ones, altering their behavior in unexpected ways. This shows that the separation between knowledge storage areas within the model is not as distinct as can be thought at first. Moreover, although our approach shows potential, it is not yet fully accurate, particularly in our challenging cases where the model's limited capacity leads to highly compressed knowledge. In such models, information is densely packed, making it difficult to update specific knowledge without affecting other unrelated information. Despite performing significantly better than current editing methods, we are still far from solving the problem of continual learning in case of dissonant updates. Some components of our work, however, such as the dissonance classifier, could potentially be integrated already into the (post)training process itself, with a promise to improve overall performance. Overall, while our approach shows promise in controlled settings, scaling it to full-scale LLMs and diverse real-world scenarios remains a significant challenge. We release our code to facilitate further research in this direction.\footnote{Anonymized code available at \url{https://figshare.com/s/81f7108d823b5e08e8ec}}



%This suggests that the separation between knowledge storage areas may not be as clear-cut as initially hypothesized. The complexity of large neural networks makes it difficult to predict the full impact of targeted updates, highlighting the need for more sophisticated monitoring and control mechanisms.lin