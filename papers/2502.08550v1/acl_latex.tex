% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}
% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}


% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

\usepackage{graphics}
\usepackage{graphicx}
\usepackage{color}
\usepackage{colortbl}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{nicefrac}
\usepackage{bbm}
\usepackage{booktabs}
\usepackage{microtype}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{tabu}
\usepackage{amsthm}
\usepackage{footmisc}
\usepackage{soul}
\usepackage{arydshln}
\usepackage{wasysym}
\usepackage{array}
\usepackage{makecell}
\usepackage{scalerel}

\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{hyperref}
\usepackage{xspace}
\usepackage{url}
\usepackage{utfsym}
\usepackage{dirtytalk}

\usepackage{float}
\usepackage{enumitem}
\usepackage{tikz}
\usepackage{tcolorbox}
\usepackage{siunitx}

\usepackage{subcaption} %  for subfigures environments 

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.
\input{macros}

\title{LLMs can \textit{implicitly} learn from mistakes in-context}

\author{
 \textbf{Lisa Alazraki\textsuperscript{1}\thanks{Work done while at Cohere. Correspondence to\\lisa.alazraki20@imperial.ac.uk.}},
 \textbf{Maximilian Mozes\textsuperscript{2}},
 \textbf{Jon Ander Campos\textsuperscript{2}},
 \textbf{Yi Chern Tan\textsuperscript{2}},
\\
 \textbf{Marek Rei\textsuperscript{1}},
 \textbf{Max Bartolo\textsuperscript{2}}
\\
\\
 \textsuperscript{1}Imperial College London,
 \textsuperscript{2}Cohere
\\
}

\begin{document}
\maketitle

\begin{abstract}

Learning from mistakes is a fundamental feature of human intelligence. Previous work has shown that Large Language Models (LLMs) can also learn from incorrect answers when provided with a comprehensive rationale detailing why an answer is wrong or how to correct it. In this work, we examine whether LLMs can learn from mistakes in mathematical reasoning tasks when these explanations are not provided. We investigate if LLMs are able to \textit{implicitly} infer such rationales simply from observing both incorrect and correct answers. Surprisingly, we find that LLMs perform better, on average, when rationales are eliminated from the context and incorrect answers are simply shown alongside correct ones. This approach also substantially outperforms chain-of-thought prompting in our evaluations. We show that these results are consistent across LLMs of different sizes and varying reasoning abilities. Further, we carry out an in-depth analysis, and show that prompting with both wrong and correct answers leads to greater performance and better generalisation than introducing additional, more diverse question-answer pairs into the context. Finally, we show that new rationales generated by models that have only observed incorrect and correct answers are scored equally as highly by humans as those produced with the aid of exemplar rationales. Our results demonstrate that LLMs are indeed capable of in-context implicit learning.
\end{abstract}

\vspace{1pt}

\section{Introduction}

\begin{figure}[!t]
\begin{subfigure}{\linewidth}
\includegraphics[width=\linewidth]{figures/explicit.pdf}\vspace{-2pt}
\caption{}
\label{fig:explicit}
\end{subfigure}\vspace{4pt}

\begin{subfigure}{\linewidth}
\includegraphics[width=\linewidth]{figures/implicit.pdf}\vspace{-2pt}
\caption{}
\label{fig:implicit}
\end{subfigure}

\caption{Learning examples for (a) explicit learning and (b) implicit learning. In the explicit learning setting, corrective feedback follows the incorrect answer (in red) and explains how to derive from it the correct answer (in green). For implicit learning, corrective feedback is discarded and the model is expected to infer the differences between the incorrect and the correct answer. 
}  
\label{fig:prompts}
\end{figure}

A crucial aspect of human cognition is the ability to learn from mistakes \cite{Learningfromerrors}. Analogously, LLMs have been shown to benefit from observing incorrect answers in their context \cite{MadaanSelfRefine, ShinnReflexion} and even their training data \cite{an2023lema, paul-etal-2024-refiner},
provided these are accompanied by descriptions of the errors they contain or the corrections needed to rectify them \cite{an2023lema, KimNeurIPS23, MadaanSelfRefine, olausson2023self, paul-etal-2024-refiner, ShinnReflexion, xu-etal-2024-llmrefine}. Usually, this corrective feedback is itself generated by an LLM, then fed into the same \cite{KimNeurIPS23, MadaanSelfRefine, ShinnReflexion} or a different model \cite{an2023lema, olausson2023self, paul-etal-2024-refiner, xu-etal-2024-llmrefine}. We consider the scenario where an LLM outputs a corrective rationale for an erroneous answer, then uses it to improve its next answer akin to \textit{explicit learning} in humans---a phenomenon whereby patterns and structure within a new piece of information are deliberately sought out and verbalised to aid reasoning and abstraction \cite{Stadler1997}. This active process differs from \textit{implicit learning}, where complex skills are passively acquired simply by observing the environment, and any pattern detection occurs implicitly and automatically \cite{Frensch2003, KAUFMAN2010321}.

In this work, we investigate the ability of LLMs to learn from mistakes \textit{implicitly}, without the aid of corrective feedback and rationales within an In-Context Learning (ICL) setting. We construct few-shot prompts for questions designed to probe reasoning abilities, alongside incorrect and correct answers. We only provide incorrect response examples for the few-shot exemplars and let the model independently infer any characteristics of the wrong and correct answers, and any differences between them. We refer to this strategy as `prompting for implicit learning'. We compare performance against two baselines: (i) a few-shot Chain-of-Thought (CoT) prompt~\cite{10.5555/3600270.3602070} only including the valid step-by-step answers to the same questions, and (ii) a few-shot prompt that includes wrong and correct answers, as well as corrective feedback, as in \citet{an2023lema}. To ensure the robustness of the results, we test all strategies extensively using seven LLMs from four distinct model families and four diverse tasks distributed across four established mathematical reasoning datasets. We show that prompting for implicit learning outperforms both baselines in most cases. This holds true even when we extend the context of CoT by inserting additional, diverse question-answer pairs, thus creating an even stronger baseline.

To gain insight into these results and assess whether LLMs are able to infer corrective rationales implicitly, we carry out a human evaluation study. We collect LLM-generated rationales for pre-computed answers to math reasoning questions across different prompting strategies. We further have the resulting rationales assessed by expert human evaluators. We find that prompting for implicit learning produces rationales that are scored highly by humans, and that adding examples of corrective feedback to the prompt only results in minimal improvement. This demonstrates that models can deduce high-quality rationales implicitly.

\vspace{15pt}
Our main contributions are:

\begin{enumerate}
    \item We investigate \textit{implicit} learning from mistakes with LLMs and compare it to \textit{explicit} learning that uses both mistakes and corrective rationales. To the best of our knowledge, no such investigation has been carried out before, and existing work relies heavily on high-quality explicit rationales, which are expensive to curate.
    \item We demonstrate that prompting for implicit learning outperforms explicit learning, as well as other strong ICL baselines. This indicates LLMs are well-suited for implicit learning.
    \item We carry out a human evaluation study and confirm that LLMs can implicitly infer high-quality corrective rationales simply from observing incorrect answers alongside correct ones in their context. These rationales are comparable in quality to those produced with the aid of in-context example rationales.
    \item Our work brings into question the rationale behind rationales and offers a simple yet effective alternative.
\end{enumerate}

\vspace{5pt}
\section{Related Work}

Incorrect outputs have been leveraged in prior work to improve LLM responses for challenging tasks. The existing literature can be largely categorised into three approaches: (i) \textit{Self-refinement}, where an LLM critiques its own erroneous generations, (ii) \textit{External feedback}, where the critic is a distinct LLM, and (iii) \textit{Multi-agent debate}, where two or more models take turns at providing feedback on a previously generated response.

\paragraph{Self-refinement.} The self-refinement pipeline is well exemplified by \citet{MadaanSelfRefine}. They devise a framework where an LLM first answers a question, then generates feedback for that answer, and finally outputs a new answer based on the feedback. Note that the model is not fine-tuned and each step is elicited via prompting. The refinement process can be repeated multiple times until a stopping criterion is met, to iteratively improve the final answer. \citet{KimNeurIPS23} and \citet{ShinnReflexion} adopt a similar strategy with agentic LLMs: the model executes a task and, based on the signal received from the environment, outputs a self-reflection consisting of a description of the errors that may have occurred in the previous action. This is then added to the LLM context to improve its performance in the next episode.

\paragraph{External feedback.} \citet{xu-etal-2024-pride} observe that self-refinement is biased by the tendency of LLMs to assess their own generations positively.  Hence, \citet{xu-etal-2024-llmrefine} propose a two-model system, where a base LLM answers a question and a fine-tuned model assesses the answer, identifies mistakes and provides feedback. The feedback is then used by the base LLM to revise its answer. Similarly, \citet{olausson2023self} find LLM self-critique to be biased in the context of code generation, and show that utilizing a second, larger model as the critic allows for more substantial improvements in the task. \citet{tong-etal-2024-llms} feed a corpus of questions and incorrect answers to PaLM2 \cite{anil2023palm2technicalreport}, which outputs the type and reasons for each mistake. They show that fine-tuning Flan-T5 models \cite{chung2022scalinginstructionfinetunedlanguagemodels} on the resulting rationales improves their performance. Similarly, \citet{paul-etal-2024-refiner} use corrective feedback from a fine-tuned model as a signal to train a base LLM for producing better responses. \citet{an2023lema} take the above approach one step further. They collect LLM-generated incorrect answers and prompt GPT-4 \cite{openai2024gpt4technicalreport} to identify and correct the mistakes, showing that LLMs fine-tuned on this data achieve superior reasoning capabilities.

\paragraph{Multi-agent debate.} Since LLMs benefit from a single critic model, it is reasonable to assume that using multiple critics may achieve further improved results. Indeed, \citet{chen-etal-2024-reconcile} show that a round table of LLMs achieve superior performance in reasoning tasks. In their framework, each LLM produces an answer to a question, followed by a self-critique. Then, all models carry out a multi-turn discussion, revising their answers at each turn based on the responses and self-critiques of the other LLMs. \citet{du2023improvingfactualityreasoninglanguage} propose a similar framework where multiple instances of an LLM generate candidate answers to a math reasoning question. Each instance then critiques the output of the other models, and uses this to update its answer. \citet{khan2024debating} have two models generate different answers and then debate their correctness, while the final choice is made by a third LLM witnessing the debate. 

\vspace{6pt}

Lastly, related work that does not fall into the above categories is \citet{chia2023contrastivechainofthoughtprompting}'s contrastive chain-of-thought. Using an entity recognition model, they extract and randomly shuffle numbers and equations within a golden mathematical answer to obtain its incoherent counterpart. While their general motivation shares some similarities with implicit learning due to the absence of a rationale to accompany the incoherent answer, the latter is inherently different from our incorrect reasoning traces. Most saliently, their analysis is not concerned with what and how much information about previous mistakes is required to improve LLM reasoning. In contrast, our investigation stems from the observation that learning from mistakes with LLMs conventionally assumes the need for explicit, fine-grained corrective feedback. We seek to answer the previously unexplored question of whether this additional feedback is actually beneficial or even necessary.


\section{Prompt Construction}

Let $\, E = \big\|_{n=1}^N e_n$ be the text sequence resulting from concatenating $N$ in-context examples. In the typical few-shot CoT setting \cite{10.5555/3600270.3602070}, an individual example
\begin{equation}
e_n^{\mathrm{CoT}} = \big(q^{(n)}, a^{(n)}\big)
\label{eq:cot}
\end{equation}
\noindent is defined by the question $q^{(n)}$ and the corresponding correct step-by-step answer $a^{(n)}$.
% as labelled in a given dataset.
This can be extended to 
\begin{equation}{e_n}^{\mathrm{explicit}} = \big(q^{(n)}, w^{(n)}, r^{(n)}, a^{(n)}\big)
\label{eq:explicit}
\end{equation}
\noindent which additionally includes a wrong step-by-step answer $w^{(n)}$ and a rationale $r^{(n)}$ that explicitly identifies the errors in $w^{(n)}$ that need correcting to obtain $a^{(n)}$. This learning setup has been widely explored in prior literature \cite{MadaanSelfRefine, KimNeurIPS23, ShinnReflexion}. We additionally consider examples of the form
\begin{equation}{e_n}^{\mathrm{implicit}} = \big(q^{(n)}, w^{(n)}, a^{(n)}\big)
\label{eq:implicit}
\end{equation} 
\noindent where the explicit rationale $r^{(n)}$ is removed. Figure~\ref{fig:prompts} illustrates instances of (\ref{eq:explicit}) and (\ref{eq:implicit}).
%

%
In our experiments, we set $N = 8$ for all example types.
We investigate how the example formulations in the set $\mathcal{E} = \lbrace E^{\mathrm{CoT}},\, E^{\mathrm{explicit}},\, E^{\mathrm{implicit}} \rbrace$ affect LLMs across different tasks: \textit{labelling} the correctness of an entire answer or an individual reasoning step, \textit{editing} an incorrect answer, or \textit{solving} a new question (we further elaborate on each task in Section \ref{sec:tasks}). We do not alter the example format by task but we instead construct a task-specific prompt by appending an instruction, $I$,  to the examples. $I$ solely depends on the task and not on the type of examples preceding it. Hence, we have a set of task-specific instructions $\mathcal{I} = \lbrace I^{\mathrm{label_{ans}}},\, I^{\mathrm{label_{step}}}, \,I^{\mathrm{edit}}, \,I^{\mathrm{solve}} \rbrace$.

We experiment with all example types for all tasks. That is, we evaluate all prompts in the set $\mathcal{P} = \lbrace E\,  || \, I  \, \, | \, \,(E, I) \in \mathcal{E} \times \mathcal{I} \rbrace$. Prompts are shown in Appendix \ref{sec:prompts}.

\subsection{Generating Correct Answers}\label{sec:correct_gen}

All the examples in $\mathcal{E}$ include questions and their corresponding correct answers. While questions are provided by the training set, not all datasets contain CoT-style golden answers. In those cases, we generate them by prompting GPT-4 \cite{openai2024gpt4technicalreport} to provide answers in a zero-shot CoT fashion, and inspect both the reasoning trace correctness and final result.

\subsection{Generating Incorrect Answers}
The incorrect answers necessary to construct the exemplars in $E^{\mathrm{explicit}}$ and $E^{\mathrm{implicit}}$ are not present in most datasets. To obtain them, we prompt LLMs that are no longer state-of-the-art to generate answers for the training set questions. We use LLaMA 30B \cite{touvron2023llamaopenefficientfoundation}, Llama 2 7B \cite{touvron2023llama2openfoundation} and Llama 3 8B \cite{grattafiori2024llama3herdmodels}. The specific model choice depends on the dataset and its difficulty (refer to Appendix~\ref{sec:dataprep}). We use few-shot CoT prompting with all models. We gather answers that are marked as wrong by automated evaluation of the final numerical result. Having discarded empty and partial answers, we simply select the first $N$ incorrect answers in the set and pair them with the corresponding questions and their correct counterparts, obtained as detailed in Section \ref{sec:correct_gen}.

\begin{figure*}[!h]
    \centering
    \includegraphics[width=\textwidth]{figures/tasks.pdf}
    \caption{We further evaluate LLM performance on four auxiliary tasks: (i) labelling an answer as wrong or correct, (ii) labelling an individual reasoning step, (iii) editing an incorrect answer to make it correct, and (iv)  solving a new question. In the labelling tasks, we instruct the model to output a rationale justifying its choice before generating the predicted label. This is motivated by previous work showing that this approach tends to produce more robust labels~\cite{trivedi2024selfrationalizationimprovesllmfinegrained, 10.5555/3666122.3668142}.}
    \label{fig:tasks}
    
\end{figure*}

\subsection{Generating Corrective Rationales}

We generate the corrective rationales in $E^{\mathrm{explicit}}$ following a strategy similar to that described in \citet{an2023lema}: we prompt GPT-4 in a few-shot fashion, showing it questions with incorrect and correct answers, as well as rationales. Given a new question and pair of answers, we ask the model to identify the mistakes in the incorrect answer and explain how to correct them. We use the same few-shot examples as \citet{an2023lema}, slightly reformatted for our task. 
%(see Appendix \ref{sec:prompts}).


\section{Experiments}

\subsection{Models}

We study LLMs of different sizes: 

\begin{itemize}
    \item Command R\footnotemark{}, 35 billion parameters;
    \item Llama 3 70B Instruct \cite{grattafiori2024llama3herdmodels}, 70 billion parameters;
    \item Command R+\footnotemark[\value{footnote}], 100 billion parameters;
    \item WizardLM \cite{xu2024wizardlm}, 141 billion parameters.
\end{itemize}

\footnotetext{https://cohere.com/command}

\noindent Note that for the Command models, we use both the original and the Refresh versions, as preliminary experiments showed significant differences in their output and results for math reasoning tasks. We also test Titan Text G1 Express\footnote{https://aws.amazon.com/bedrock/amazon-models}, whose exact number of parameters has not been publicly disclosed. We note, however, that this model is substantially less capable than the others in reasoning tasks, as evidenced by the lower scores in Table~\ref{tab:main_result}. Therefore, we consider seven LLMs in total. We employ a greedy sampling strategy with all models. LLMs are accessed via API; further details including the inference hyperparameters and model IDs are given in Appendix \ref{sec:models}. 


\subsection{Datasets}

Our main focus is understanding whether LLMs learn implicitly in tasks that require complex reasoning. Contemporary work investigating LLM reasoning has primarily focused on math reasoning as an early and convenient proxy for complex reasoning ability evaluation \cite{ahn-etal-2024-large, paul-etal-2024-refiner, ruis2024proceduralknowledgepretrainingdrives, liu2025llmscapablestablereasoning}. Consistent with this approach, we test our method on several math reasoning benchmarks.

\paragraph{GSM8K} includes grade-school-level arithmetic problems that require multiple reasoning steps to solve~\cite{cobbe2021trainingverifierssolvemath}. All problems in GSM8K can be tackled using basic arithmetic operations (addition, subtraction, multiplication, division).

\paragraph{ASDiv} contains diverse problems of varying difficulty~\cite{miao-etal-2020-diverse}. In addition to arithmetic operations, questions can be solved using algebra, number theory, set operations and geometric formulas. They can also require pattern identification and unit conversion.

\paragraph{AQuA} is a dataset of algebraic word problems from postgraduate admissions tests such as GRE and GMAT, as well as new questions of similar difficulty collected through crowd-sourcing~\cite{ling-etal-2017-program}. Note that while the original version of the dataset is multiple-choice, here we use a more challenging open-ended version.

\paragraph{PRM800K}~\cite{lightman2024lets} is derived from the MATH dataset~\cite{2021_be83ab3e}, which contains challenging competition-level math problems. In PRM800K, model-generated answers to the questions in MATH are paired with human annotations providing a validation signal on intermediary reasoning steps.


\vspace{6pt}

These datasets cover a wide range of math domains and difficulty levels, each constituting a particular challenge. Furthermore, statistical analysis on GSM8K, ASDiv and AQuA has determined that these datasets are entirely out-of-domain with respect to one another \cite{hub_nature}, which makes this selection of evaluation datasets an appropriate test bed for our analysis.


\subsection{Tasks}\label{sec:tasks}

In addition to evaluating on diverse math reasoning datasets, we consider auxiliary tasks that can be carried out within those datasets. We illustrate them below and in Figure~\ref{fig:tasks}.

\paragraph{Labelling an answer.} In this task we have the model assign a binary label to a CoT-style answer, identifying whether it is correct or not, given the question. Previous work has found that LLM-assigned labels are more robust when they are accompanied by a model-generated rationale \cite{trivedi2024selfrationalizationimprovesllmfinegrained, 10.5555/3666122.3668142}. Hence, we require LLMs to first output a rationale explaining their choice, followed by the label. Performance in the binary labelling tasks is measured by the macro-averaged F1-score, weighted by support to account for label imbalance. The answers to be labelled are generated by running Llama 2 7B and Llama 3 8B on the test set of each dataset (refer to Appendix \ref{sec:dataprep} for details).

\paragraph{Labelling a reasoning step.} We leverage the step-wise reasoning annotations in PRM800K to have models score the correctness of a single reasoning step given the question and any previous context. Similar to the above setting, the LLM outputs a rationale followed by a binary label (`correct' or `incorrect'). As the other datasets do not contain step-wise annotations, we perform this task only on PRM800K.

\paragraph{Editing an incorrect answer.} We show the model a question and a corresponding incorrect answer, then ask it to output a new, edited answer that leads to the correct solution. Performance is measured by computing the accuracy of the numerical solution. For this task, we use the incorrect portion of the pre-generated answers obtained by running Llama 2 7B and Llama 3 8B on the test sets. 

\paragraph{Solving a math question.} In this task, we simply show the model a test set question and ask it to output the solution. As in the previous task, we compute the accuracy of the final numerical solution.

\vspace{8pt}

\input{tabs/main_results}

To encourage the models to output responses \textit{conditioned} on the context, as opposed to text that merely mimics the format of the examples in it, we append the task-specific instruction after the examples. We further aid generalization by prepending the text \textit{`Now apply what you have learned'} to the instruction. \citet{mao-etal-2024-prompt} show that the position of the instruction within a few-shot prompt affects the model's behaviour and performance. On the other hand, the model may still be inclined to generate responses in the format of the examples (e.g., when tasked with editing an answer, having observed examples that contain corrective rationales, the model may output a rationale before the corrected answer). To account for this possibility without unnecessarily penalizing any particular prompting strategy, we provide a large generation window of 4096 tokens.

%%%%%%%%%%%%%%%%%

\subsection{Results}

\input{tabs/analysis_results}

We find that CoT and prompting with \textit{explicit} rationales have similar overall performance on the answer labelling task and when solving new questions, while the latter outperforms CoT when labelling reasoning steps ($+$3.2\%, averaged across all models and all datasets) and editing an incorrect answer ($+$2.1\% avg.). This advantage is aligned with previous findings that LLMs benefit from observing incorrect answers and corrective feedback in their context. On the other hand, prompting for \textit{implicit} learning achieves the highest overall performance, as evidenced in Table \ref{tab:main_result}. When considering all combinations of model, dataset and task, implicit learning outperforms CoT in 85\% of cases. It also outperforms explicit learning in 88\% of cases. In nearly half of these, the advantage of implicit over explicit learning is substantial---well above 3\%. This advantage is present even in tasks where, intuitively, we would expect in-context rationales to be particularly helpful, for example when editing an incorrect answer to make it correct. In fact, implicit learning gives the largest accuracy boost in the editing task: $+4.4\%$ over CoT and $+2.2\%$ over explicit learning, averaged across all models and datasets. On the solving task, its accuracy increases by $1.6$ and $1.9$ percentage points, respectively. Labelling answers also benefits from implicit learning prompts, with averaged F1-scores $5.6\%$ above CoT and $6.2\%$ above explicit learning.

\begin{figure}[!b]
\begin{subfigure}{.5\linewidth}
\includegraphics[width=\linewidth]{figures/res1.pdf}
\caption{}
\label{fig:label}
\end{subfigure}\begin{subfigure}{.5\linewidth}
\includegraphics[width=\linewidth]{figures/res2.pdf}
\caption{}
\label{fig:edit_solve}
\end{subfigure}\vspace{-4pt}
\caption{Scores per dataset of CoT, explicit and implicit prompting for (a) the weighted F1-score of the labelling task, and (b) the averaged accuracy across the editing and solving tasks. Scores are averaged across all LLMs.
}
\label{fig:results_per_dataset}
\end{figure}

Finally, looking at the individual datasets, implicit learning gains the most on GSM8K, where it outperforms both explicit learning and CoT in over $90\%$ of cases across all models and tasks. This proportion is $76\%$ on ASDiv, $81\%$ on AQuA and $64\%$ on PRM800K. Note that the questions in GSM8K and ASDiv have a lower level of difficulty than those in AQuA and PRM800K, as evidenced by the performance differences across all LLMs. Generally, we observe that prompting for implicit learning improves performance across varying levels of difficulty, as shown in Figure~\ref{fig:results_per_dataset}. In the labelling task (Figure~\ref{fig:label}), implicit learning gives the most substantial performance gains on ASDiv and PRM800K. When editing an incorrect answer and solving a new question (Figure~\ref{fig:edit_solve}), on the other hand, it is GSM8K and AQuA that benefit the most from this strategy.



\section{Discussion}

Our results demonstrate that LLMs perform better across several mathematical reasoning tasks when they are prompted for \textit{implicit} learning, even over CoT prompting and providing the models with additional information through rationales. To minimise any risk that spurious correlations may be influencing these results, here we provide further, in-depth analysis of our findings, their robustness and implications.


\subsection{Effect of Context Length and Diversity}\label{sec:context_length}

In our experiments, we use the same number of in-context examples across all setups. As a result, there is a mismatch between the context length of CoT and that of implicit learning, since incorporating incorrect answers introduces additional tokens into the context. As an extended context length can in itself be responsible for improved performance, we investigate the hypothesis that the additional tokens may be driving the improvement, rather than the presence of incorrect answers. We thus extend CoT's context by increasing the number of examples from eight to fourteen (we refer to this setup as CoT+). Additional examples are randomly selected from an identical sample distribution to the original eight examples. We compare this setup to implicit learning prompts containing eight few-shot examples as in our standard experimental setting. The addition of six additional in-context examples to the CoT prompt results in an approximately equal context length between the two settings. It also constitutes a particularly strong baseline, since the new examples may provide the model with additional, novel scenarios to learn from. Table~\ref{tab:analysis_result} illustrates these results.


Firstly, we note that in the large majority of cases ($\sim$80\%) adding more few-shot examples to the CoT prompt results in better or similar ($<$1\% difference) performance than the same setup with fewer examples. In a minority of cases, however, we observe that performance declines. This is consistent with previous findings that more examples do not strictly guarantee performance improvements \cite{zhao2023incontextexemplarscluesretrieving}, especially in complex tasks \cite{opedal2024mathgapoutofdistributionevaluationproblems}. Indeed, instances where performance declines are predominantly concentrated in the PRM800K dataset, which contains particularly challenging problems. Notably, prompting for implicit learning outperforms CoT+ in over 80\% of cases. This demonstrates that the advantage of implicit learning is indeed due to the presence of incorrect answers rather than increased context length or other effects. The addition of incorrect answers appears more beneficial for LLMs than the inclusion of additional diverse and valid question-answer pairs.

\subsection{Human Evaluation of Generated Rationales}\label{sec:human_eval}

A follow-up research question aims to investigate what the effects of incorporating error information are on model outputs. We hypothesize that if the models are incorporating error signal implicitly to improve reasoning, this should also be reflected in downstream generated rationales.


To ascertain whether, and to what extent, LLMs infer implicit information between incorrect and correct answers with different prompting strategies, we carry out a blind human evaluation study of rationales generated using distinct prompts. We randomly select 300 rationales generated by running the answer labelling task on GSM8K. We select 100 rationales for each prompting strategy (CoT, explicit learning, implicit learning). We then have four annotators with domain expertise score them as \textit{0--Poor}, \textit{1--Fair} or \textit{2--Good}. Table \ref{tab:humaneval_result} illustrates the average human evaluation scores achieved under each prompting strategy. We observe that CoT's performance is considerably lower than either explicit or implicit learning, with an average score of 0.68. The performance of explicit and implicit learning is similar (1.01 and 0.98 respectively). It is noteworthy that rationales generated with implicit learning prompts achieve an average score that is within only 0.03 of that achieved by explicit learning. This is evidence that LLMs can infer high-quality corrective rationales implicitly, simply observing correct and incorrect answers side by side, and that the effect of adding example rationales to the context is negligible.

\input{tabs/humaneval_results}

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/humaneval.pdf}
    \caption{Fine-grained results of the human evaluation, showing the number of individual labels assigned to rationales for each prompting strategy. Explicit and implicit learning perform fairly similarly, with explicit learning obtaining a slightly higher number of labels at both extremes (`poor' and `good') and implicit learning earning more mid-range labels (`fair'). In contrast, rationales output with the aid of CoT prompting are mainly scored as `poor'.}
    \label{fig:humaneval}
\end{figure}

In Figure~\ref{fig:humaneval} we show a breakdown of the labels assigned by human evaluators to rationales produced with each prompting strategy. While most rationales generated using CoT are assigned the minimum score, explicit and implicit learning prompting exhibit similar trends, with explicit learning obtaining slightly more labels at both ends of the spectrum (`poor' and `good') and implicit learning receiving more mid-range labels.


\subsection{Representative Rationales}

\input{tabs/representative_rationales}

Consider the math reasoning problem \textit{``There are 4 snails in one aquarium and 32 snails in another aquarium. The difference between the number of snails in the two aquariums is twice the amount of fish in both aquariums. If both aquariums have the same number of fish in them, how many fish are there in each aquarium?''}. We take an incorrect, model-generated answer to this problem which assigns the unknown number of fish to the variable $x$ but does not proceed to solve for $x$. We show three representative rationales generated for this question-answer pair using the three prompting strategies: CoT (\hyperref[rat:cot]{Rationale 1}), explicit learning (\hyperref[rat:exp]{Rationale 2}) and implicit learning (\hyperref[rat:imp]{Rationale 3}). We observe that the rationale produced using CoT fails to identify the error. It also hallucinates that the number of fish in each aquarium is 14, which is neither the correct solution nor a value that appears in the answer. In contrast, prompting for explicit and implicit learning both produce accurate rationales. Note that the latter---generated without exemplar rationales in the context to use as guidelines---is more succinct yet equally exhaustive.



\section{Conclusion}

We have investigated in-context \textit{implicit} learning across a range of LLM sizes, and found that it outperforms both \textit{explicit} learning and chain-of-thought prompting in challenging math reasoning tasks. We have further shown that LLM-generated rationales obtained via implicit learning are comparable in quality to those conditioned on in-context example rationales. Our findings are as noteworthy as they are surprising, since they call into question the benefits of widely-used corrective rationales to aid LLMs in learning from mistakes. These rationales are prevalent in current frameworks despite being expensive to curate at scale, yet our investigation suggests that they are redundant, and can even hurt performance by adding unnecessary constraints.


\section*{Limitations}

We have carried out an exhaustive investigation of implicit learning from mistakes, focused on in-context learning. It is worth noting that implicit learning examples---which consist of triples of the form \textit{(question, incorrect answer, correct answer)}---can be obtained at scale by simply running more and less capable LLMs on training set questions. This opens up the possibility of investigating performance differences between explicit and implicit learning also in other paradigms, such as in the fine-tuning setting. Future work can investigate whether the results established in this paper extend to models fine-tuned using similar strategies.


\section*{Ethical Considerations}

The licenses of all the datasets used in this paper permit their use and modification. For each dataset, we have provided a citation to the original work. Any future, non-commercial data distribution will be accompanied by the original licenses and appropriate credits.

%%%%%%%%%%%%%%%%%%%%%%

\bibliography{custom}\clearpage

%%%%%%%%%%%%%%%%%%%%%%

\appendix


\section{Preliminary Experiments}
We ran an experiment to establish the effect of context length on the results, in addition to the one shown in Section~\ref{sec:context_length}. In this preliminary experiment, we provide two CoT-style answers, both correct, for each in-context question. We refer to this setup as CoT-2.

It is worth noting that at the time of the preliminary experiment, there were some differences in our setup: (1) LLM instructions had slightly different wording. In particular, the labelling tasks were set up so that the LLM would output the label directly. As per Section~\ref{sec:tasks}, we later changed this to have the model output a rationale justifying its choice first, followed by the label; (2) AQuA was not yet part of our test suite.

Table~\ref{tab:preliminary_result} illustrates the results obtained with Command R+. We observe that implicit prompting is superior to CoT-2, with the largest overall advantage in the editing and solving tasks. Surprisingly, observing incorrect answers alongside correct ones does not help the LLM label new answers for correctness in the case of GSM8K. Overall, however, the advantage of implicit prompting over CoT-2 is consistent. This, together with the results of our human analysis study of the generated rationales (Section~\ref{sec:human_eval}), points to the fact that LLMs prompted for implicit learning appear to gain a better understanding of the patterns that inform correct answers---and how these differ from incorrect answers---which prompting with only correct reasoning traces may not sufficiently elicit.

\input{tabs/preliminary_results}


\section{Prompts}\label{sec:prompts}

All prompts are of the form
\[ \texttt{[examples]\textbackslash n\textbackslash n\textbackslash n\textbackslash n[instruction]}, \]
where the \textit{examples} are specific to the prompting strategy (i.e., CoT, explicit learning, implicit learning), while the \textit{instruction} is task-dependent (i.e., labelling an answer or step, editing an incorrect answer, solving a new question). In the next sections we illustrate the examples for each prompting strategy and the instructions for each task.

\subsection{Examples}

All few-shot examples shown below are for GSM8K. Examples for all datasets are included in \bgroup\fontsize{10pt}{10pt}\selectfont{\url{https://github.com/lisaalaz/implicit-learning-with-llms}}\egroup.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{CoT Examples.}\prompttext{Below you will find some questions followed by the answer.}

\vspace{12pt}

\noindent{\prompttext{Question: A class has 32 students and they all need to present their projects. Every period is 40 minutes long. How many periods will it take for every student to present their project if they have 5 minutes to do so?}}

\vspace{6pt}

\noindent{\prompttext{Answer: There are 32 students and each student needs 5 minutes to present their project.\\
So the total time needed is 32 x 5 = 160 minutes.\\
There are 40 minutes in each period.\\
So the number of periods needed is 160/40 = 4.\\
So, the final answer is 4.}}

\vspace{12pt}

\noindent{\prompttext{Question: A bag contains 50 fewer baseball cards than football cards. There are 4 times as many football cards as hockey cards. If there are 200 hockey cards in the bag, how many cards are there altogether?}}

\vspace{6pt}

\noindent{\prompttext{Answer: There are 4 times as many football cards as hockey cards, so there are 4 x 200 = 800 football cards.\\
There are 50 fewer baseball cards than football cards, so there are 800 - 50 = 750 baseball cards.\\
There are 200 hockey cards, 800 football cards and 750 baseball cards for a total of 200 + 800 + 750 = 1750 cards.\\
So, the final answer is 1750.}}

\vspace{12pt}

\noindent{\prompttext{Question: A bag of caramel cookies has 20 cookies inside and a box of cookies has 4 bags in total. How many calories are inside the box if each cookie is 20 calories?}}

\vspace{6pt}

\noindent{\prompttext{Answer: There are 20 cookies in each bag and 4 bags in total.\\
So there are 20 x 4 = 80 cookies in total.\\
Each cookie is 20 calories, so the total calories are 80 x 20 = 1600 calories.\\
So, the final answer is 1600.}}

\vspace{12pt}

\noindent{\prompttext{Question: There are four members in one household. Each member consumes 3 slices of bread during breakfast and 2 slices of bread for snacks. A loaf of bread has 12 slices. How many days will five loaves of bread last in this family?}}

\vspace{6pt}

\noindent{\prompttext{Answer: Each member consumes 3+2 = 5 slices of bread per day.\\
There are 4 members so they consume 5 x 4 = 20 slices of bread per day.\\
Each loaf has 12 slices so 5 loaves have 5 x 12 = 60 slices.\\
60 slices of bread will last 60/20 = 3 days.\\
So, the final answer is 3.}}

\vspace{12pt}

\noindent{\prompttext{Question: Bill decides to bring donuts to work for a meeting that day. He buys a box of donuts that has 50 in total in them. He eats 2 on the ride in because he's hungry. When he gets to the office, the secretary takes another 4 out of the box when he's not looking. Lastly, right before the meeting Bill sets the box down on his desk and leaves the office to take a phone call. While Bill's away, his coworkers steal half the remaining donuts. Bill comes back and brings the box into his meeting. How many donuts are left in the box?}}

\vspace{6pt}

\noindent{\prompttext{Answer: Bill eats 2 donuts on the ride in, leaving 50 - 2 = 48 donuts.\\
The secretary eats 4 donuts, leaving 48 - 4 = 44 donuts.\\
Bill's coworkers eat half of the remaining donuts, which is 44/2 = 22 donuts.\\
So Bill has 44 - 22 = 22 donuts left.\\
So, the final answer is 22.}}

\vspace{12pt}

\noindent{\prompttext{Question: Boris has 100 pieces of Halloween candy. His daughter eats 8 pieces of candy. He separates the remaining pieces of candy into equal portions into 4 different bowls. Then he takes away 3 pieces of candy from each bowl to keep for himself. How many pieces of candy are in one bowl?}}

\vspace{6pt}

\noindent{\prompttext{Answer: Boris has 100 - 8 = 92 pieces of candy left.\\
He divides the 92 pieces of candy into 4 bowls. 92/4 = 23 pieces of candy in each bowl.\\
Then he takes away 3 pieces of candy from each bowl. 23 - 3 = 20 pieces of candy in each bowl.\\
So, the final answer is 20.}}

\vspace{12pt}

\noindent{\prompttext{Question: Yvonne and Janna were writing their 1000-word pair research paper. Yvonne was able to write 400 words while Janna wrote 150 more words than Yvonne. When they edited their paper, they removed 20 words and added twice as many words as they removed. How many more words should they add to reach the research paper requirement?}}

\vspace{6pt}

\noindent{\prompttext{Answer: Yvonne wrote 400 words and Janna wrote 150 more words than Yvonne, so Janna wrote 400 + 150 = 550 words.\\
Together they wrote 400 + 550 = 950 words.\\
After editing, they removed 20 words and added 2 x 20 = 40 words. So they have 950 - 20 + 40 = 970 words.\\
They need 1000 words, so they need 1000 - 970 = 30 more words.\\
So, the final answer is 30.}}

\vspace{12pt}

\noindent{\prompttext{Question: Luke started working on a 1000-piece jigsaw puzzle. The first day he worked on it, he put together 10 percent of the pieces. On the second day, he put together another 20 percent of the remaining pieces from the first day. On the third day, he put together 30 percent of the remaining pieces from the previous day. How many pieces does he have left to complete after the third day?}}

\vspace{6pt}

\noindent{\prompttext{Answer: 10 percent of 1000 is 100.\\
So he has 100 pieces done after the first day.
He has 900 pieces left.\\
20 percent of 900 is 180.\\
So he has 100 + 180 = 280 pieces done after the second day.\\
He has 720 pieces left.\\
30 percent of 720 is 216.\\
So he has 280 + 216 = 496 pieces done after the third day.\\
He has 720 - 216 = 504 pieces left.\\
So, the final answer is 504.}}


%%%%%%%%%%%%%%%%%%%%%%


\paragraph{Explicit Learning Examples.}\prompttext{Below you will find some questions followed by a wrong answer, a rationale and the correct answer.}

\vspace{12pt}

\noindent{\prompttext{Question: A class has 32 students and they all need to present their projects. Every period is 40 minutes long. How many periods will it take for every student to present their project if they have 5 minutes to do so?}}

\vspace{6pt}

\noindent{\prompttext{Wrong answer: 32 students x 5 minutes = 160 minutes.\\
5 minutes x 40 minutes = 200 minutes.\\
160 minutes + 200 minutes = 360 minutes.\\
So it will take 360 minutes or 6 hours for every student to present their project.\\
So, the final answer is 360.}}

\vspace{6pt}

\noindent{\prompttext{Rationale: The error occurs in the second line of the wrong answer. The calculation of 5 minutes x 40 minutes = 200 minutes is incorrect and irrelevant to the problem. The correct approach is to calculate the total time needed for all presentations (32 students x 5 minutes each = 160 minutes) and then divide this by the length of each period (40 minutes) to find the number of periods needed. The incorrect answer also incorrectly converts the total minutes into hours, which is not asked for in the question.}}

\vspace{6pt}

\noindent{\prompttext{Correct answer: There are 32 students and each student needs 5 minutes to present their project.\\
So the total time needed is 32 x 5 = 160 minutes.\\
There are 40 minutes in each period.\\
So the number of periods needed is 160/40 = 4.\\
So, the final answer is 4.}}

\vspace{12pt}

\noindent{\prompttext{Question: A bag contains 50 fewer baseball cards than football cards. There are 4 times as many football cards as hockey cards. If there are 200 hockey cards in the bag, how many cards are there altogether?}}

\vspace{6pt}

\noindent{\prompttext{Wrong answer: 50 fewer baseball cards than football cards.\\
There are 4 times as many football cards as hockey cards.\\
If there are 200 hockey cards in the bag, there are 200 x 4 = 800 football cards and 800 - 50 = 750 baseball cards.\\
So there are 750 + 800 = 1550 cards altogether.\\
So, the final answer is 1550.}}

\vspace{6pt}

\noindent{\prompttext{Rationale: The error occurs in the last line of the wrong answer. The wrong answer incorrectly calculates the total number of cards. The total number of cards should be the sum of the hockey cards, football cards, and baseball cards. The correct calculation is 200 (hockey cards) + 800 (football cards) + 750 (baseball cards) = 1750 cards.}}

\vspace{6pt}

\noindent{\prompttext{Correct answer: There are 4 times as many football cards as hockey cards, so there are 4 x 200 = 800 football cards.\\
There are 50 fewer baseball cards than football cards, so there are 800 - 50 = 750 baseball cards.\\
There are 200 hockey cards, 800 football cards and 750 baseball cards for a total of 200 + 800 + 750 = 1750 cards.\\
So, the final answer is 1750.}}

\vspace{12pt}

\noindent{\prompttext{Question: A bag of caramel cookies has 20 cookies inside and a box of cookies has 4 bags in total. How many calories are inside the box if each cookie is 20 calories?}}

\vspace{6pt}

\noindent{\prompttext{Wrong answer: 20 cookies in a bag amount to 20 x 20 = 400 calories.\\
There are 4 bags in a box.\\
So 400/4 = 100 calories in a box.\\
So, the final answer is 100.}}

\vspace{6pt}

\noindent{\prompttext{Rationale: The error occurs in line 3 of the wrong answer. The incorrect part of this line is the calculation of the total calories in a box. The total calories should be calculated by multiplying the total number of cookies in the box (which is 80, not 20) by the calories per cookie. The wrong answer incorrectly divided the total calories in a bag by the number of bags in a box.}}

\vspace{6pt}

\noindent{\prompttext{Correct answer: There are 20 cookies in each bag and 4 bags in total.\\
So there are 20 x 4 = 80 cookies in total.\\
Each cookie is 20 calories, so the total calories are 80 x 20 = 1600 calories.\\
So, the final answer is 1600.}}

\vspace{12pt}

\noindent{\prompttext{Question: There are four members in one household. Each member consumes 3 slices of bread during breakfast and 2 slices of bread for snacks. A loaf of bread has 12 slices. How many days will five loaves of bread last in this family?}}

\vspace{6pt}

\noindent{\prompttext{Wrong answer: 12/4 = 3 slices of bread per person per day.\\
5 loaves of bread = 5 x 12 = 60 slices of bread.\\
60/4 = 15 days.\\
So, the final answer is 15.}}

\vspace{6pt}

\noindent{\prompttext{Rationale: The error occurs in the first line of the wrong answer. The incorrect part of this line is the calculation of the slices of bread consumed per person per day. The problem states that each member consumes 3 slices of bread for breakfast and 2 slices for snacks, so each member consumes a total of 5 slices per day, not 3. Therefore, the total slices of bread consumed per day by the family should be 5 slices per person * 4 people = 20 slices, not 12. The correct calculation should then be 60 slices / 20 slices per day = 3 days.}}

\vspace{6pt}

\noindent{\prompttext{Correct answer: Each member consumes 3+2 = 5 slices of bread per day.\\
There are 4 members so they consume 5 x 4 = 20 slices of bread per day.\\
Each loaf has 12 slices so 5 loaves have 5 x 12 = 60 slices.\\
60 slices of bread will last 60/20 = 3 days.\\
So, the final answer is 3.}}

\vspace{12pt}

\noindent{\prompttext{Question: Bill decides to bring donuts to work for a meeting that day. He buys a box of donuts that has 50 in total in them. He eats 2 on the ride in because he's hungry. When he gets to the office, the secretary takes another 4 out of the box when he's not looking. Lastly, right before the meeting Bill sets the box down on his desk and leaves the office to take a phone call. While Bill's away, his coworkers steal half the remaining donuts. Bill comes back and brings the box into his meeting. How many donuts are left in the box?}}

\vspace{6pt}

\noindent{\prompttext{Wrong answer: Bill eats 2 donuts on the way to work.\\
The secretary takes 4 donuts.\\
Bill's coworkers steal 25 donuts.\\
Bill brings the box into the meeting with 23 donuts.\\
So, the final answer is 23.}}

\vspace{6pt}

\noindent{\prompttext{Rationale: The error occurs in line 2 of the wrong answer. The incorrect part of this line is the calculation of the number of donuts stolen by Bill's coworkers. The problem states that his coworkers steal half of the remaining donuts, not half of the original number of donuts. Therefore, the number of donuts stolen by his coworkers should be half of the remaining 44 donuts, not half of the original 50 donuts.}}

\vspace{6pt}

\noindent{\prompttext{Correct answer: Bill eats 2 donuts on the ride in, leaving 50 - 2 = 48 donuts.\\
The secretary eats 4 donuts, leaving 48 - 4 = 44 donuts.\\
Bill's coworkers eat half of the remaining donuts, which is 44/2 = 22 donuts.\\
So Bill has 44 - 22 = 22 donuts left.\\
So, the final answer is 22.}}

\vspace{12pt}

\noindent{\prompttext{Question: Boris has 100 pieces of Halloween candy. His daughter eats 8 pieces of candy. He separates the remaining pieces of candy into equal portions into 4 different bowls. Then he takes away 3 pieces of candy from each bowl to keep for himself. How many pieces of candy are in one bowl?}}

\vspace{6pt}

\noindent{\prompttext{Wrong answer: Boris has 100 pieces of candy.\\
His daughter eats 8 pieces of candy.\\
He separates the remaining pieces of candy into equal portions into 4 different bowls.\\
Then he takes away 3 pieces of candy from each bowl to keep for himself. So he has 100-8 = 92 pieces of candy in 4 bowls.\\
He takes away 3 pieces of candy from each bowl. So he has 92-3 = 89 pieces of candy in 4 bowls.\\
So, the final answer is 89.}}

\vspace{6pt}

\noindent{\prompttext{Rationale: The error occurs at line 5 of the wrong answer. The incorrect part of this line is the calculation of the number of candies left after Boris takes away 3 pieces from each bowl. The subtraction should be done after dividing the total number of candies by 4 (the number of bowls), not before. Also, the subtraction should be 3*4=12, not just 3, because Boris takes away 3 pieces from each of the 4 bowls.}}

\vspace{6pt}

\noindent{\prompttext{Correct answer: Boris has 100 - 8 = 92 pieces of candy left.\\
He divides the 92 pieces of candy into 4 bowls. 92/4 = 23 pieces of candy in each bowl.\\
Then he takes away 3 pieces of candy from each bowl. 23 - 3 = 20 pieces of candy in each bowl.\\
So, the final answer is 20.}}

\vspace{12pt}

\noindent{\prompttext{Question: Yvonne and Janna were writing their 1000-word pair research paper. Yvonne was able to write 400 words while Janna wrote 150 more words than Yvonne. When they edited their paper, they removed 20 words and added twice as many words as they removed. How many more words should they add to reach the research paper requirement?}}

\vspace{6pt}

\noindent{\prompttext{Wrong answer: Yvonne wrote 400 words and Janna wrote 150 more words than Yvonne. So Janna wrote 550 words.\\
They removed 20 words and added twice as many words as they removed. So they added 40 words.\\
They need to add 40 words to reach the research paper requirement.\\
So, the final answer is 40.}}

\vspace{6pt}

\noindent{\prompttext{Rationale: The error occurs in line 3 of the wrong answer. The incorrect part of this line is the calculation of the total words they have after editing. They should subtract the words they removed and add the words they added to the total words they initially wrote. The wrong answer also incorrectly calculated the number of words they need to add to reach the research paper requirement. They should subtract the total words they have after editing from the research paper requirement.}}

\vspace{6pt}

\noindent{\prompttext{Correct answer: Yvonne wrote 400 words and Janna wrote 150 more words than Yvonne, so Janna wrote 400 + 150 = 550 words.\\
Together they wrote 400 + 550 = 950 words.\\
After editing, they removed 20 words and added 2 x 20 = 40 words. So they have 950 - 20 + 40 = 970 words.\\
They need 1000 words, so they need 1000 - 970 = 30 more words.\\
So, the final answer is 30.}}

\vspace{12pt}

\noindent{\prompttext{Question: Luke started working on a 1000-piece jigsaw puzzle. The first day he worked on it, he put together 10 percent of the pieces. On the second day, he put together another 20 percent of the remaining pieces from the first day. On the third day, he put together 30 percent of the remaining pieces from the previous day. How many pieces does he have left to complete after the third day?}}

\vspace{6pt}

\noindent{\prompttext{Wrong answer: 1000 pieces = 1000/100 = 10 pieces Luke put together 10 pieces on the first day.\\
He put together 20 pieces on the second day.\\
He put together 30 pieces on the third day.\\
So he has 10+20+30 = 60 pieces left to complete after the third day.\\
So, the final answer is 60.}}

\vspace{6pt}

\noindent{\prompttext{Rationale: The error occurs in the first line of the wrong answer. The wrong answer incorrectly calculates 10 percent of 1000 as 10 pieces, when it should be 100 pieces. The same mistake is made for the calculations on the second and third day. The correct way to solve this problem is to calculate the percentage of the remaining pieces each day, not a percentage of the original 1000 pieces.}}

\vspace{6pt}

\noindent{\prompttext{Correct answer: 10 percent of 1000 is 100.\\
So he has 100 pieces done after the first day.
He has 900 pieces left.\\
20 percent of 900 is 180.\\
So he has 100 + 180 = 280 pieces done after the second day.\\
He has 720 pieces left.\\
30 percent of 720 is 216.\\
So he has 280 + 216 = 496 pieces done after the third day.\\
He has 720 - 216 = 504 pieces left.\\
So, the final answer is 504.}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\paragraph{Implicit Learning Examples.}\prompttext{Below you will find some questions followed by a wrong answer and the correct answer.}

\vspace{12pt}

\noindent{\prompttext{Question: A class has 32 students and they all need to present their projects. Every period is 40 minutes long. How many periods will it take for every student to present their project if they have 5 minutes to do so?}}

\vspace{6pt}

\noindent{\prompttext{Wrong answer: 32 students x 5 minutes = 160 minutes.\\
5 minutes x 40 minutes = 200 minutes.\\
160 minutes + 200 minutes = 360 minutes.\\
So it will take 360 minutes or 6 hours for every student to present their project.\\
So, the final answer is 360.}}

\vspace{6pt}

\noindent{\prompttext{Correct answer: There are 32 students and each student needs 5 minutes to present their project.\\
So the total time needed is 32 x 5 = 160 minutes.\\
There are 40 minutes in each period.\\
So the number of periods needed is 160/40 = 4.\\
So, the final answer is 4.}}

\vspace{12pt}

\noindent{\prompttext{Question: A bag contains 50 fewer baseball cards than football cards. There are 4 times as many football cards as hockey cards. If there are 200 hockey cards in the bag, how many cards are there altogether?}}

\vspace{6pt}

\noindent{\prompttext{Wrong answer: 50 fewer baseball cards than football cards.\\
There are 4 times as many football cards as hockey cards.\\
If there are 200 hockey cards in the bag, there are 200 x 4 = 800 football cards and 800 - 50 = 750 baseball cards.\\
So there are 750 + 800 = 1550 cards altogether.\\
So, the final answer is 1550.}}

\vspace{6pt}

\noindent{\prompttext{Correct answer: There are 4 times as many football cards as hockey cards, so there are 4 x 200 = 800 football cards.\\
There are 50 fewer baseball cards than football cards, so there are 800 - 50 = 750 baseball cards.\\
There are 200 hockey cards, 800 football cards and 750 baseball cards for a total of 200 + 800 + 750 = 1750 cards.\\
So, the final answer is 1750.}}

\vspace{12pt}

\noindent{\prompttext{Question: A bag of caramel cookies has 20 cookies inside and a box of cookies has 4 bags in total. How many calories are inside the box if each cookie is 20 calories?}}

\vspace{6pt}

\noindent{\prompttext{Wrong answer: 20 cookies in a bag amount to 20 x 20 = 400 calories.\\
There are 4 bags in a box.\\
So 400/4 = 100 calories in a box.\\
So, the final answer is 100.}}

\vspace{6pt}

\noindent{\prompttext{Correct answer: There are 20 cookies in each bag and 4 bags in total.\\
So there are 20 x 4 = 80 cookies in total.\\
Each cookie is 20 calories, so the total calories are 80 x 20 = 1600 calories.\\
So, the final answer is 1600.}}

\vspace{12pt}

\noindent{\prompttext{Question: There are four members in one household. Each member consumes 3 slices of bread during breakfast and 2 slices of bread for snacks. A loaf of bread has 12 slices. How many days will five loaves of bread last in this family?}}

\vspace{6pt}

\noindent{\prompttext{Wrong answer: 12/4 = 3 slices of bread per person per day.\\
5 loaves of bread = 5 x 12 = 60 slices of bread.\\
60/4 = 15 days.\\
So, the final answer is 15.}}

\vspace{6pt}

\noindent{\prompttext{Correct answer: Each member consumes 3+2 = 5 slices of bread per day.\\
There are 4 members so they consume 5 x 4 = 20 slices of bread per day.\\
Each loaf has 12 slices so 5 loaves have 5 x 12 = 60 slices.\\
60 slices of bread will last 60/20 = 3 days.\\
So, the final answer is 3.}}

\vspace{12pt}

\noindent{\prompttext{Question: Bill decides to bring donuts to work for a meeting that day. He buys a box of donuts that has 50 in total in them. He eats 2 on the ride in because he's hungry. When he gets to the office, the secretary takes another 4 out of the box when he's not looking. Lastly, right before the meeting Bill sets the box down on his desk and leaves the office to take a phone call. While Bill's away, his coworkers steal half the remaining donuts. Bill comes back and brings the box into his meeting. How many donuts are left in the box?}}

\vspace{6pt}

\noindent{\prompttext{Wrong answer: Bill eats 2 donuts on the way to work.\\
The secretary takes 4 donuts.\\
Bill's coworkers steal 25 donuts.\\
Bill brings the box into the meeting with 23 donuts.\\
So, the final answer is 23.}}

\vspace{6pt}

\noindent{\prompttext{Correct answer: Bill eats 2 donuts on the ride in, leaving 50 - 2 = 48 donuts.\\
The secretary eats 4 donuts, leaving 48 - 4 = 44 donuts.\\
Bill's coworkers eat half of the remaining donuts, which is 44/2 = 22 donuts.\\
So Bill has 44 - 22 = 22 donuts left.\\
So, the final answer is 22.}}

\vspace{12pt}

\noindent{\prompttext{Question: Boris has 100 pieces of Halloween candy. His daughter eats 8 pieces of candy. He separates the remaining pieces of candy into equal portions into 4 different bowls. Then he takes away 3 pieces of candy from each bowl to keep for himself. How many pieces of candy are in one bowl?}}

\vspace{6pt}

\noindent{\prompttext{Wrong answer: Boris has 100 pieces of candy.\\
His daughter eats 8 pieces of candy.\\
He separates the remaining pieces of candy into equal portions into 4 different bowls.\\
Then he takes away 3 pieces of candy from each bowl to keep for himself. So he has 100-8 = 92 pieces of candy in 4 bowls.\\
He takes away 3 pieces of candy from each bowl. So he has 92-3 = 89 pieces of candy in 4 bowls.\\
So, the final answer is 89.}}

\vspace{6pt}

\noindent{\prompttext{Correct answer: Boris has 100 - 8 = 92 pieces of candy left.\\
He divides the 92 pieces of candy into 4 bowls. 92/4 = 23 pieces of candy in each bowl.\\
Then he takes away 3 pieces of candy from each bowl. 23 - 3 = 20 pieces of candy in each bowl.\\
So, the final answer is 20.}}

\vspace{12pt}

\noindent{\prompttext{Question: Yvonne and Janna were writing their 1000-word pair research paper. Yvonne was able to write 400 words while Janna wrote 150 more words than Yvonne. When they edited their paper, they removed 20 words and added twice as many words as they removed. How many more words should they add to reach the research paper requirement?}}

\vspace{6pt}

\noindent{\prompttext{Wrong answer: Yvonne wrote 400 words and Janna wrote 150 more words than Yvonne. So Janna wrote 550 words.\\
They removed 20 words and added twice as many words as they removed. So they added 40 words.\\
They need to add 40 words to reach the research paper requirement.\\
So, the final answer is 40.}}

\vspace{6pt}

\noindent{\prompttext{Correct answer: Yvonne wrote 400 words and Janna wrote 150 more words than Yvonne, so Janna wrote 400 + 150 = 550 words.\\
Together they wrote 400 + 550 = 950 words.\\
After editing, they removed 20 words and added 2 x 20 = 40 words. So they have 950 - 20 + 40 = 970 words.\\
They need 1000 words, so they need 1000 - 970 = 30 more words.\\
So, the final answer is 30.}}

\vspace{12pt}

\noindent{\prompttext{Question: Luke started working on a 1000-piece jigsaw puzzle. The first day he worked on it, he put together 10 percent of the pieces. On the second day, he put together another 20 percent of the remaining pieces from the first day. On the third day, he put together 30 percent of the remaining pieces from the previous day. How many pieces does he have left to complete after the third day?}}

\vspace{6pt}

\noindent{\prompttext{Wrong answer: 1000 pieces = 1000/100 = 10 pieces Luke put together 10 pieces on the first day.\\
He put together 20 pieces on the second day.\\
He put together 30 pieces on the third day.\\
So he has 10+20+30 = 60 pieces left to complete after the third day.\\
So, the final answer is 60.}}

\vspace{6pt}

\noindent{\prompttext{Correct answer: 10 percent of 1000 is 100.\\
So he has 100 pieces done after the first day.
He has 900 pieces left.\\
20 percent of 900 is 180.\\
So he has 100 + 180 = 280 pieces done after the second day.\\
He has 720 pieces left.\\
30 percent of 720 is 216.\\
So he has 280 + 216 = 496 pieces done after the third day.\\
He has 720 - 216 = 504 pieces left.\\
So, the final answer is 504.}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{tabs/instruction_prompts}

\subsection{Instructions}

The instructions for the answer labelling, step labelling, editing and solving tasks are shown in I\ref{ins:label_ans}, I\ref{ins:label_step}, I\ref{ins:edit} and I\ref{ins:solve}, respectively.

\section{Models}\label{sec:models}

We list below each of the seven LLMs tested, with the corresponding API provider and model identifier. With all LLMs we use hyperparameters $t = 0$, $p = 1$, and $k = 1$.
\vspace{4pt}
\bgroup
\flushleft{
\begin{itemize}
    \item Llama 3.1 70B, \textit{Amazon Bedrock}, \texttt{meta.llama3-70b-instruct-v1:0}
    \item Titan Text G1 Express, \textit{Amazon Bedrock}, \texttt{amazon.titan-text-express-v1}
    \item Command R, \textit{Cohere}, \texttt{command-r-03-2024}
    \item Command R Refresh, \textit{Cohere}, \texttt{command-r-08-2024}
    \item Command R+, \textit{Cohere}, \texttt{command-r-plus-04-2024}
    \item Command R+ Refresh, \textit{Cohere}, \texttt{command-r-plus-08-2024}
    \item WizardLM, \textit{TogetherAI}, \texttt{microsoft/WizardLM-2-8x22B}
\end{itemize}
}
\egroup

\vspace{6pt}

\input{tabs/answer_generation}

\section{Data Preparation}\label{sec:dataprep}

\subsection{Answer Generation}\label{sec:ans_gen}

In Table \ref{tab:models} we show the LLMs used to generate answers to the questions in each dataset, for both the training few-shot examples and the test samples. For the training set, we only generate incorrect answers with the listed models, while all correct answers are generated with GPT-4 or extracted from the original dataset where possible. For test samples, we use these models to generate both correct and incorrect answers. We do not run this generation step for PRM800K, as this dataset already contains annotated correct and incorrect answers.

\subsection{Test Set Construction}

For GSM8K and ASDiv we use all test samples, with or without the correct/incorrect answers generated as per Section \ref{sec:ans_gen}, depending on the task. For AQuA, we make minor changes to the test set before generating the answers. PRM800K already contains CoT-style answers, though these are annotated for correctness at the intermediary reasoning step level, and not as a whole. Thus, we adapt this dataset to our tasks. We illustrate these adaptations below.

\paragraph{AQuA} contains, in its original version, multiple-choice questions associated with five answer options, only one of which is correct. In our experiments, we discard the options and prompt the model to generate open-ended answers. For ease of verifying the correctness of the answers at test time, we drop from the test set all samples where the golden answer is non-numerical.

\paragraph{PRM800K} comprises questions and the respective answers split into intermediary reasoning steps. Each step is labelled by human annotators as correct (label 1), incorrect (label $-$1), or neutral (label~0). Some samples are associated with a series of steps that lead to the correct solution, while others contain errors that impact the final solution. For the step labelling task, we use the reasoning steps that are annotated as either correct or incorrect. We append each of them to the previous context where available, i.e., the (correct or neutral) steps that precede it in the answer. For the answer labelling and editing tasks, we join the individual steps and label the resulting answer as correct if all steps are labelled as either correct or neutral, and incorrect if at least one of the steps contains errors.



\section{Human Evaluation Guidelines}

We report the guidelines given to annotators for the human evaluation task. Annotators were recruited among machine learning experts.

\vspace{15pt}

\noindent{\textit{\textbf{Evaluating model feedback on math reasoning questions.}}}

\vspace{8pt}

\noindent{\textit{The attached sheet contains 100 mathematical questions and the corresponding answers given by language models. Each answer is highlighted in either green (meaning it reached the correct numerical solution) or red (meaning the numerical solution reached is wrong).}}

\noindent{\textit{For each answer, three LLMs have generated a piece of feedback explaining why the answer is wrong or correct, your task is to score each feedback as “poor”, “fair”, or “good”.}}

\noindent{\textit{In your evaluation, you should only consider the correctness of the feedback. Did the model identify the strengths and/or weaknesses of the answer correctly? In your assessment, do not take feedback length into consideration. If two pieces of feedback both identify the same key points, they should be awarded the same score, even if one is much more succinct than the other. If a piece of feedback is completely missing however (meaning the model did not generate one), you should assign the label “poor”. Also please ignore formatting and the presence of any special tokens or characters in your evaluation, only focus on the meaning. In each row, the three models are displayed in different order to avoid annotation bias. So, for example, “Model 1” in the first row may not be the same model as “Model 1” in the second row, and so on.}}


\section{Similarity of Generated Rationales}

A plausible reason why learning with explicit rationales underperforms implicit learning is that LLMs may be over-constrained by the rationales. To gain more insight into this hypothesis, we investigate how similar the rationales generated by the models are to the in-context ones. We carry out this analysis on the same corpus of LLM-generated rationales that is used for the human evaluation study (Section~\ref{sec:human_eval}). As shown in Table~\ref{tab:rat_sim}, the average $n$-gram similarity score of rationales generated by LLMs prompted for explicit learning is substantially higher than that obtained with rationales output with the other methods (note that the other methods do not include exemplar rationales in the context). It thus appears that LLMs tend to copy the patterns in the exemplar rationales when these are provided. This points to potential overfitting as a reason for the lower performance of explicit learning.

\input{tabs/rationales_similarity}

\end{document}
