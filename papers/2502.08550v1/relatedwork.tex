\section{Related Work}
Incorrect outputs have been leveraged in prior work to improve LLM responses for challenging tasks. The existing literature can be largely categorised into three approaches: (i) \textit{Self-refinement}, where an LLM critiques its own erroneous generations, (ii) \textit{External feedback}, where the critic is a distinct LLM, and (iii) \textit{Multi-agent debate}, where two or more models take turns at providing feedback on a previously generated response.

\paragraph{Self-refinement.} The self-refinement pipeline is well exemplified by \citet{MadaanSelfRefine}. They devise a framework where an LLM first answers a question, then generates feedback for that answer, and finally outputs a new answer based on the feedback. Note that the model is not fine-tuned and each step is elicited via prompting. The refinement process can be repeated multiple times until a stopping criterion is met, to iteratively improve the final answer. \citet{KimNeurIPS23} and \citet{ShinnReflexion} adopt a similar strategy with agentic LLMs: the model executes a task and, based on the signal received from the environment, outputs a self-reflection consisting of a description of the errors that may have occurred in the previous action. This is then added to the LLM context to improve its performance in the next episode.

\paragraph{External feedback.} \citet{xu-etal-2024-pride} observe that self-refinement is biased by the tendency of LLMs to assess their own generations positively.  Hence, \citet{xu-etal-2024-llmrefine} propose a two-model system, where a base LLM answers a question and a fine-tuned model assesses the answer, identifies mistakes and provides feedback. The feedback is then used by the base LLM to revise its answer. Similarly, \citet{olausson2023self} find LLM self-critique to be biased in the context of code generation, and show that utilizing a second, larger model as the critic allows for more substantial improvements in the task. \citet{tong-etal-2024-llms} feed a corpus of questions and incorrect answers to PaLM2 \cite{anil2023palm2technicalreport}, which outputs the type and reasons for each mistake. They show that fine-tuning Flan-T5 models \cite{chung2022scalinginstructionfinetunedlanguagemodels} on the resulting rationales improves their performance. Similarly, \citet{paul-etal-2024-refiner} use corrective feedback from a fine-tuned model as a signal to train a base LLM for producing better responses. \citet{an2023lema} take the above approach one step further. They collect LLM-generated incorrect answers and prompt GPT-4 \cite{openai2024gpt4technicalreport} to identify and correct the mistakes, showing that LLMs fine-tuned on this data achieve superior reasoning capabilities.

\paragraph{Multi-agent debate.} Since LLMs benefit from a single critic model, it is reasonable to assume that using multiple critics may achieve further improved results. Indeed, \citet{chen-etal-2024-reconcile} show that a round table of LLMs achieve superior performance in reasoning tasks. In their framework, each LLM produces an answer to a question, followed by a self-critique. Then, all models carry out a multi-turn discussion, revising their answers at each turn based on the responses and self-critiques of the other LLMs. \citet{du2023improvingfactualityreasoninglanguage} propose a similar framework where multiple instances of an LLM generate candidate answers to a math reasoning question. Each instance then critiques the output of the other models, and uses this to update its answer. \citet{khan2024debating} have two models generate different answers and then debate their correctness, while the final choice is made by a third LLM witnessing the debate. 

\vspace{6pt}

Lastly, related work that does not fall into the above categories is \citet{chia2023contrastivechainofthoughtprompting}'s contrastive chain-of-thought. Using an entity recognition model, they extract and randomly shuffle numbers and equations within a golden mathematical answer to obtain its incoherent counterpart. While their general motivation shares some similarities with implicit learning due to the absence of a rationale to accompany the incoherent answer, the latter is inherently different from our incorrect reasoning traces. Most saliently, their analysis is not concerned with what and how much information about previous mistakes is required to improve LLM reasoning. In contrast, our investigation stems from the observation that learning from mistakes with LLMs conventionally assumes the need for explicit, fine-grained corrective feedback. We seek to answer the previously unexplored question of whether this additional feedback is actually beneficial or even necessary.