\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\linewidth]{figures/pdfs/pipeline.pdf}
  \caption{\textbf{Schematic representation of our DDB framework.} 
  The debiasing process consists of two key steps: (A) \textit{Diffusing the Bias} uses a conditional diffusion model with classifier-free guidance to generate synthetic images that preserve training dataset biases, and (B) employs a \textit{Bias Amplifier} firstly trained on such synthetic data, and subsequently used during inference to extract supervisory bias signals from real images. These signals are used to guide the training process of a target debiased model by designing two \textit{debiasing recipes} (\ie, 2-step and end-to-end methods). 
  }
  \label{fig:pipeline}
\end{figure}
\section{The Approach}
\label{sec:approach}
Our proposed debiasing approach is schematically depicted in Figure~\ref{fig:pipeline}. 
In this section, we provide at first a general description of the problem setting (Sec. ~\ref{sec:problem-formulation}), and then, we illustrate in detail DDB's two main components, which include \textit{bias diffusion} (Sec.~\ref{sec:biasdiff}) and the two \textit{Recipes} for model debiasing (Sec.~\ref{sec:recipes}).
%
\subsection{Problem Setting}
\label{sec:problem-formulation}
Let us consider a general data distribution $p_{\text{data}}$, typically encompassing multiple factors of variation and classes, and to build a dataset of images with the associated labels $~{\dataset = \lbrace(\mathbf{x}_i, y_i)\rbrace_{i=1}^N}$ sampled from such a distribution. Let us also assume that the sampling process to obtain $\dataset$ is not uniform across latent factors of variations, \ie possible biases such as context, appearance, acquisition noise, viewpoint, etc.. 
In this case, data will not faithfully capture the true data distribution ($p_{\text{data}}$) just because of these bias factors. 
%and will likely be biased. 
This phenomenon deeply affects the generalization capabilities of deep neural networks in classifying unseen examples not presenting the same biases.
In the same way, we could consider $\dataset$ as the union between two sets, \ie $\dataset = \udataset \bigcup \bdataset$. Here, the elements of $\udataset$ are uniformly sampled from $p_\text{data}$ and, in $\bdataset$, they are instead sampled from a conditional distribution $p_\text{data}\left(\mathbf{x}, y \: \vert \: b \right)$, with $b \in B$ being some latent factor (bias attribute) from a set of possible attributes $B$, likely to be unknown or merely not annotated, in a realistic setting~\cite{kim2024training}~\footnote{In this context, biased and unbiased samples equivalently refer to bias-aligned and bias conflicting samples.}. 
If $\vert \bdataset \vert \gg \vert \udataset \vert$, optimizing a classification model $f_{\theta}$ over $\dataset$ likely results in biased predictions and poor generalization. This is due to the strong correlation between $b$ and $y$, often called \textit{spurious correlation}, and denoted as $\rho(y, b)$, or just $\rho$ for brevity \cite{kim2021biaswap, Sagawa*2020Distributionally, nahon2023mining}), which is dominating over the true target distribution semantics. 


It is important to notice that data bias is a general problem, not only affecting classification tasks but also impacting several others such as data generation~\cite{d2024openbias}. For instance, given a  Conditional Diffusion 
Probabilistic Models (CDPM) modeled as a neural network $\cdpm$ (with parameters $\phi$) that learns to approximate a conditional distribution $p\left(\mathbf{x} \: \vert \: y \right)$ from $\dataset$, we expect that its generations will be biased, as also stated in~\cite{d2024openbias, kim2024training}. While this is a strong downside for image-generation purposes, in this work, we claim that when $\rho(y, b)$ is very high (\eg $\geq 0.95$, as generally assumed in model debiasing literature \cite{nam2020learning}), a CDPM predominantly learns the biased distribution of a specific class, \ie, $\cdpm \approx p \left(\mathbf{x} \: \vert \: b\right)$ rather than $p \left(\mathbf{x} \: \vert \: y\right)$.
\subsection{Diffusing the Bias}
\label{sec:biasdiff}
In the context of mitigating bias in classification models, the tendency of a CDPM to approximate the per-class biased distribution represents a key feature for training an auxiliary \textit{bias amplified} model.   
\paragraph{The Diffusion Process.}
The diffusion process progressively converts data into noise through a fixed Markov chain of \( T \) steps~\cite{DBLP:conf/nips/HoJA20}. Given a data point \( \mathbf{x}_0 \), the forward process adds Gaussian noise according to a variance schedule \( \{\beta_t\}_{t=1}^T \), resulting in noisy samples \( \mathbf{x}_1, \dots, \mathbf{x}_T \). This forward process can be formulated for any timestep \( t \) as: ~{$q(\mathbf{x}_t | \mathbf{x}_0) = \mathcal{N}(\mathbf{x}_t ; \sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1 - \bar{\alpha}_t) \mathbf{I})$}, 
where \( \bar{\alpha}_t = \prod_{s=1}^t \alpha_s \) with \( \alpha_s = 1 - \beta_s \).
The reverse process then gradually denoises a sample, reparameterizing each step to predict the noise \( \epsilon \) using a model \( \boldsymbol{\epsilon}_\theta \):
\begin{equation}
\label{eq:ddpm_reverse}
\mathbf{x}_{t-1} = \frac{1}{\sqrt{\alpha_t}} \left( \mathbf{x}_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) \right) + \sigma_t \mathbf{z},
\end{equation}
\noindent
where \( \mathbf{z} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}) \) and \( \sigma_t = \sqrt{\beta_t} \).
\paragraph{Classifier-Free Guidance for Biased Image Generation.}
In cases where additional context or \textit{conditioning} is available, such as a class label \( y \), diffusion models can use this information to guide the reverse process, generating samples that better reflect the target attributes and semantics. Classifier-Free Guidance (CFG)~\cite{DBLP:journals/corr/abs-2207-12598} introduces a flexible conditioning approach, allowing the model to balance conditional and unconditional outputs without dedicated classifiers.

The CFG technique randomly omits conditioning during training (\eg, with probability \( p_{\text{uncond}} = 0.1 \)), enabling the model to learn both generation modalities. During the sampling process, a guidance scale \( w \) modulates the influence of conditioning. When \( w = 0 \), the model relies solely on the conditional model. As \( w \) increases (\( w \geq 1 \)), the conditioning effect is intensified, potentially resulting in more distinct features linked to \( y \), thereby increasing fidelity to the class while possibly reducing diversity, whereas lower values help to preserve diversity by decreasing the influence of conditioning. The guided noise prediction is given by:
\begin{equation}
\boldsymbol{\epsilon}_{t} = (1 + w) \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t, y) - w \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t),
\end{equation}
\noindent
where \( \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t, y) \) is the noise prediction conditioned on class label \( y \), and \( \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) \) is the unconditional noise prediction. This modified noise prediction replaces the standard \( \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) \) term in the reverse process formula (Equation \ref{eq:ddpm_reverse}).
In this work, we empirically show how CDPM learns and amplifies the underlying biased distribution when trained on a biased dataset with strong spurious correlations,  allowing bias-aligned image generation. 
\subsection{DDB: Bias Amplifier and Model Debiasing}
\label{sec:recipes}
As stated in Sec.~\ref{sec:rel-work}, a typical unsupervised approach to model debiasing relies on an auxiliary intentionally-biased model, named here as \textit{Bias Amplifier} (BA). This model can be exploited in either 2-step or end-to-end approaches, denoted here as \textit{Recipe I} and \textit{Recipe II}, respectively. 
\subsubsection{Recipe I: 2-step debiasing}
\label{sec:recipe-one}
\begin{figure}[hp]
    \centering    \includegraphics[width=0.6\linewidth]{figures/pdfs/groupdro.pdf}
    \caption{Overview of \textit{Recipe I}'s 2-step debiasing approach.}
    \label{fig:gdro}
\end{figure}
\noindent
The adopted 2-step approach consists in 1) applying the auxiliary model trained on biased generated data to perform a bias pseudo-labeling, hence estimating bias-aligned/bias-conflict split of original actual data, and 2) apply a \textit{bias supervised} method to train a debiased target model for classification. For the latter, we use the group DRO algorithm~\cite {Sagawa*2020Distributionally} (G-DRO) as a proven technique for the pure debiasing step. 
In other words, being in the unsupervised bias scenario where the real bias labels are unknown, we estimate bias pseudo-labels performing an inference step by feeding the trained BA with the original actual training data, and identifying as bias-aligned the correctly classified samples, and as bias-conflicting those misclassified. Among possible strategies to assign bias pseudo-labels, such as feature-clustering~\cite{sohoni2020no} or anomaly detection~\cite{pastore2024lookingmodeldebiasinglens}, we adopt a simple heuristic based on %top of 
the BA misclassifications. 
Specifically, given a sample $(\mathbf{x}_i, y_i, c_i)$ with $c_i$ unknown pseudo-label indicating whether $\mathbf{x}_i$ is bias conflicting or aligned, we estimate bias-conflicting samples as
\begin{equation}\label{eq:gdro-threhsold}
    \hat{c}_i = \mathds{1} \left( \hat{y}_i \neq y_i~\land~\mathcal{L}(\hat{y}_i, y_i) ~>~\mu_n(\mathcal{L}) + \gamma \sigma_n(\mathcal{L}) \right)
\end{equation} 
where $\mathds{1}$ is the indicator function, $\mathcal{L}$ is the CE loss of the BA on the real sample, and $\mu_n$ and $\sigma_n$ represent the average training loss and its standard deviation, respectively, depending on the loss $\mathcal{L}$. Together with the multiplier $\gamma~\in\mathbb{N}$, this condition defines a sort of filter over misclassified samples, considering them as conflicting only if their loss is also higher than the mean loss increased by a quantity corresponding to a certain {z-score} of the per-sample training loss distribution ($~{\mu_n(\mathcal{L}) + \gamma \sigma_n(\mathcal{L})}$ in Eq.~\ref{eq:gdro-threhsold}). 
Once bias pseudo-labels over original training data are obtained, we plug in our estimate as group information for the G-DRO optimization, as schematically depicted in Figure~\ref{fig:gdro}.

The above \textit{filtering} operation refines the plain \textit{error set}, restricting bias-conflicting sample selection to the hardest training samples, with potential benefits for the most difficult correlation settings ($\rho > 0.99$). 
Later in the experimental section, we provide an ablation study comparing different filtering ($\gamma$) configurations and plain error set alternatives. 
\subsubsection{Recipe II: end-to-end debiasing}
\label{sec:recipe-two}
A typical end-to-end debiasing setting includes the joint training of the target debiasing model and one~\cite{nam2020learning} or more~\cite{NEURIPS2022_75004615_LWBC, Lee_Park_Kim_Lee_Choi_Choo_2023} auxiliary intentionally-biased models. Here, we design an end-to-end debiasing procedure, denoted as \textit{Recipe II}, incorporating our BA by customizing a widespread general scheme, introduced in the Learning from Failure (LfF) method~\cite{nam2020learning}.
LfF leverages an intentionally-biased model trained using Generalized CE (GCE) loss to support the simultaneous training of a debiased model adopting the CE loss re-weighted by a per-sample relative difficulty score.
Specifically, we replace the GCE biased model with our Bias Amplifier, which is frozen and only employed in inference to compute its loss function for each original training sample ($\mathcal{L}_\text{bias\_amp}$), as schematically represented in Figure~\ref{fig:LLD}. 
Such loss function is used to obtain %multiplier 
a weighting factor for the target model loss function, defined as $
r = \frac{\mathcal{L}_{\text{Bias\_Amp}}}{\mathcal{L}_{\text{debiasing}} + \mathcal{L}_{\text{Bias\_Amp}}}$. Coarsely speaking, $r$ should be low for bias-aligned and high for bias-conflicting samples.
\begin{figure}[h]
  \centering
\includegraphics[width=.6\linewidth]{figures/pdfs/end2end.pdf}
  \caption{Overview of \textit{Recipe II}'s end-to-end debiasing approach.}
  \label{fig:LLD}
\end{figure}

