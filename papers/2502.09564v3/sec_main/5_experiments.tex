\section{Experiments}
In this Section, after the description of the benchmark datasets employed in our work, we provide general implementation and training details, followed by a discussion of the obtained results and a thorough ablation analysis. 
\\
In the Supplementary Material, we report \textit{Recipe I} and \textit{Recipe II} full implementation details, more examples of generated images, and a quantitative analysis of the dataset bias captured from the CDPM with the assistance of a Vision-Language Model.
\label{sec:experiments}
\begin{figure*}[!ht]
  \centering
  \includegraphics[width=0.99\linewidth]{figures/pdfs/synthetic.pdf}
  \caption{\textbf{Examples of synthetic bias-aligned images across multiple datasets.} Each grid shows synthetic images for specific classes across biased datasets, revealing how the model aligns with dataset-specific biases in different contexts.}
  \label{fig:synthetic_images}
\end{figure*}
\subsection{Datasets}
Our experiments utilize five distinct datasets:  Waterbirds, Biased Flickr-Faces-HQ (BFFHQ), Biased Action Recognition (BAR), and ImageNet-9/ImageNet-A.
Waterbirds ~\cite{Sagawa*2020Distributionally} is an image dataset exhibiting strong correlations ({$\rho = 0.950$}) between bird species and background environments (\eg, water vs. land). It has 4,795 training images, 1,199 images for validation, and 5,794 for testing.
BFFHQ~\cite{kim2021biaswap} builds upon FFHQ~\cite{DBLP:journals/pami/KarrasLA21} by introducing demographic biases for face recognition, while BAR ~\cite{nam2020learning} is crafted to challenge action recognition models that associate specific actions with particular stereotypical contexts. For these two last datasets, we specifically refer to their original versions introduced respectively in \cite{nam2020learning} and \cite{kim2021biaswap}: BFFHQ has a total of 19,200 training images where ~(~{$y \in \lbrace\text{young, old}\rbrace,~~ b \in \lbrace\text{female, male}\rbrace, ~\rho = 0.995$)}. Then, it provides 1,000 validation images (all bias-aligned) and 1,000 testing images with uniformly distributed labels and bias attributes. BAR presents 6 different classes, but does not provide explicit bias annotations or a validation set, with 1,941 images for training and 654 for testing.
Finally, ImageNet-A consists of 7,500 real-world images from a 200-class subset of ImageNet, where deep learning models consistently struggle to make correct predictions. Originally introduced by Hendrycks et al. \cite{hendrycks2021natural} to evaluate adversarial robustness, it is also employed as a model debiasing benchmark, usually as a bias-conflicting evaluation set for models trained on ImageNet-9 \cite{bahng2019rebias}, a collection of images with 57,200 samples where categories are super-classes of ImageNet, put together to exhibit textural and shape biases \cite{bahng2019rebias}. As such, both our CDPM and Bias Amplifier are trained on ImageNet-9. Then, we evaluate both recipes on ImageNet-A in terms of Average Accuracy, following the same protocol of \cite{bahng2019rebias,NEURIPS2022_75004615_LWBC,nahon2023mining}

\subsection{Implementation Details}
In our experiments, we generate $1000$ synthetic %bias-aligned 
images for each target class using a CDPM implemented as a multi-scale UNet incorporating 
temporal and conditional embeddings for processing timestep information and class labels. For computational efficiency, input images are resized to $64\times64$ pixels. Input images are normalized with respect to ImageNet's mean and standard deviation. The CDPM is trained from scratch for $70,000$, $150,000$, $200,000$, $100.000$ iterations for the datasets BAR, Waterbirds, BFFHQ, and ImageNet-9 respectively. Batch size is set to $32$, and the diffusion process is executed over $1,000$ steps with a linear noise schedule (where $\beta_1 = 1e^{-4}$ and $\beta_T = 0.028$). Optimization is performed using MSE loss and AdamW optimizer, with an initial learning rate of $1e^{-4}$, adjusted by a CosineAnnealingLR~\cite{DBLP:conf/iclr/LoshchilovH17} scheduler with a warm-up period for the first $10\%$ of the total iterations.
As per the Bias Amplifier training, we use synthetic 
images from CDPM. A Densenet-121~\cite{huang2017densely} with ImageNet pre-training is employed across all datasets, featuring a single-layer linear classification head. The training uses regular Cross-Entropy loss function and the AdamW optimizer~\cite{loshchilov2018decoupled}. For BFFHQ and BAR, the learning rate is $1e^{-4}$, and $5e^{-4}$ for Waterbirds, and ImageNet-9. Training spans 50 epochs with weight decay $\lambda=0.01$ except for BFFHQ, which uses $\lambda = 1.0$ for 100 epochs. We apply standard basic data augmentation strategies (following \cite{kim2021biaswap, nam2020learning}), including random crops and horizontal flips. 
For our debiasing \textit{recipes}, we rely on the same backbones used in the existing literature, \ie ResNet-18 for BFFHQ, BAR, ImageNet-9/A, and a ResNet-50 for Waterbirds~\cite{nam2020learning, kim2021biaswap, kim2024discovering}. In \textit{Recipe II}, $\gamma$ (Equation~\ref{eq:gdro-threhsold}) is set to 3 in all our main experiments. Finally, similarly to \cite{li2022discover, nam2020learning, pastore2024lookingmodeldebiasinglens}, we do not rely on a validation set for regularization. This choice is related to real-world application suitability, where no guarantees exist regarding the degree of spurious correlations present in a held-out subset of data used as validation, possibly resulting in detrimental regularization \cite{zarlenga2024efficient}. 
\subsection{Results}
\subsubsection{Synthetic bias-aligned image generation}
First, we present qualitative image generation results across multiple datasets (see Fig.~\ref{fig:synthetic_images}), to validate the effectiveness of our bias diffusion approach. 
The generated samples maintain good fidelity and capture the typical bias patterns of each training set per class, confirming our conditioned diffusion process's ability to learn and diffuse inherent biases. 
This is confirmed by screening the generated images by 2 independent annotators, who were asked to verify (or deny) the presence of the bias in each synthetic sample.
In addition, we also carried out an objective evaluation of the generated image content by adopting an image captioner, specifically BLIP-2 \cite{DBLP:conf/icml/0008LSH23}. We estimate a caption for each image per class and count the frequency of every word. Suppl. Material (Sec. A.\ref{supsec:bias_naming}) shows a histogram of the detected keywords related to the Waterbirds dataset, in which it can be noted that together with a high frequency of the object class (\textit{bird}), similar significant bins of the bias attributes (\textit{land}, \textit{water}) are also evidenced. The same behavior also emerges in other datasets used in this study, where a consistent description of the bias attributes can be noted, as reported in Sec. A.\ref{supsec:bias_naming}. These results highlight our CDPM's capability to generate synthetic bias-aligned data, setting the stage for our subsequent debiasing techniques.
\subsubsection{Classification accuracy}
Table \ref{tab:ddb-real-datasets} reports the average and standard deviation of our employed metrics, obtained over three independent runs for all the considered datasets. We report Worst Group Accuracy (WGA) for Waterbirds, as in~\cite{Sagawa*2020Distributionally, Lee_Park_Kim_Lee_Choi_Choo_2023}. For BFFHQ, we report the average and the conflicting accuracy (\ie, the average accuracy computed only on the bias-conflicting test samples), following~\cite{kim2021biaswap, li2022discover}. BAR and ImageNet-A do not provide bias-attribute annotations, hence, we report just the average test accuracy. For simplicity, throughout this section, we will refer to \textit{Recipe I} and \textit{Recipe II} as DDB-I and DDB-II, respectively. It's worth noticing that, for BAR and BFFHQ, we report benchmark results related to works that employ the same versions as ours. 
\begin{table}[!hb]
    \centering
    \include{tables/ddb_real}
    \caption{Results of DDB Recipes I and II on benchmark datasets. Ticks (\textcolor{green}{\tick}) indicate that the method is bias-unsupervised. We highlight in bold the best results from unsupervised debiasing works, second-best are underlined.}
    \label{tab:ddb-real-datasets}
\end{table}
Across all the considered benchmark datasets, both DDB recipes show state-of-the-art results (see Table \ref{tab:ddb-real-datasets}). Notably, in Waterbirds, DDB's WGA is higher than G-DRO supervised (91.56\% vs. 91.40\% of G-DRO~\cite{Sagawa*2020Distributionally}) and all the other unsupervised methods' ones. DDB-I is providing slightly worse performance with respect to DDB-II, but still better than the state of the art. 
In BAR, DDB-II is reaching 72.81\% average accuracy, which is 2.93\% better than the second best (DebiAN~\cite{li2022discover}), in terms of conflicting accuracy. In BFFHQ, our DDB-I reaches an accuracy of 74.67\%, with an average improvement of 1\% when compared to ETF-Debias~\cite{wang2024navigate} (73.60\%), the best state-of-the-art results to date, and provides a boost of 7.45\% over the recent DeNetDM \cite{vadakkeeveetil2024denetdm}.
\subsection{Ablation analysis}
In this section, we provide an extensive ablation analysis of DDB main components. Without losing generality, all ablations are performed on Waterbirds and exploit Recipe I for model debiasing. Additional ablations on Recipe II can be found in the Suppl. Material.
The reported analyses regard several aspects of the data generation process with respect to the Bias Amplifier, the effects of the filtering option in Recipe I, and the behavior of DDB on unbiased data.\\ 
\label{sec:ablations}
\noindent \textbf{Bias amplifier data requirements.} 
All our main results are obtained by training a BA on $1000$ synthetic images per class. Here, we want to measure how this choice impacts the effectiveness of our Bias Amplifier. From Table~\ref{tab:my_label} (left), we observe that even 100 synthetic images are enough for good debiasing performance despite not being competitive with top accuracy scores. As we increase the number of training images, WGA improves, reaching the best performance for $2000$ images per class.
\\ 
\\
\noindent \textbf{CFG strength and generation bias.} 
The most important free parameter for the CDPM with CFG is the guidance conditioning \textit{strength} $w$. Such strength can affect the variety and consistency of generated images. Here, we investigate DDB dependency on this parameter. Table~\ref{tab:my_label} (right) shows a limited impact on debiasing performance. WGA variations along different $w$ values are limited, with the best performance obtained for $w=1$.
\begin{table}[!b]    \include{tables/ablation_perc-and-guidance}
    \vspace{-4mm}
    \caption{Ablations on: (left) synthetic image count for training the BA on Waterbirds; (right) Impact of guidance strength $w$ on CDPM sampling, evaluated on Waterbirds, employing Recipe I.}
    \label{tab:my_label}
\end{table}
\\
\\ \noindent \textbf{Group extraction via BA plain error sets.}
In Sec.~\ref{sec:recipe-one}, we describe how we filter the BA \textit{error set} through a simple heuristic based on per-sample loss distribution. In Table \ref{tab:ddb-real-datasets}, the last row (DDB-I w/ plain err. set) reports our obtained results when considering as bias-conflicting all the samples misclassified by the Bias Amplifier. Our results show a trade-off between the two alternatives. The entire set of misclassifications are always enough to reach high bias mitigation performance with less critical settings ($\rho = 0.950$) (e.g., Waterbirds), gaining an advantage from such a coarser selection. On the other hand, the importance of a more precise selection is highlighted by the superior performance of the filtered alternative in the more challenging scenarios, like BFFHQ ($\rho=0.995$).\\
\\ \noindent \textbf{Bias-Identification Accuracy}
To frame the goodness of our Bias Amplifier in correctly identifying aligned and conflicting training samples, we provide the accuracy of such process when considering samples misclassified by the BA. Specifically, we report a bias-identification accuracy of $89.00\%$ for Waterbirds and $96.00\%$ on BFFHQ. Our very high accuracy supports the effectiveness of using the proposed protocol, which is not dependent from any careful regularization, bias-annotated validation sets, or large ensembles of auxiliary classifiers. 
\\
\\
\noindent \textbf{DDB impact on unbiased dataset.}
In real-world settings, it is generally unknown whether bias is present in datasets or not. Consequently, an effective approach, while mitigating bias dependency in the case of a biased dataset should also not degrade the performance when the bias is not present. This ablation study aims to assess DDB from this perspective. To obtain this insight, we apply the entire DDB pipeline (adopting Recipe I) on a common image dataset such as CIFAR-10 ~\cite{krizhevsky2009learning}, comparing it with traditional ERM training with CE loss. In this controlled case, \textit{Recipe I} provides a test accuracy of $84.16\%$, with a slight improvement of $\sim 1\%$ compared to traditional ERM ($83.26 \%$) employing the same target model (ResNet18). This result shows that our Bias Amplifier can still provide beneficial signals to the target model in standard learning scenarios, potentially favoring the learning of the most challenging samples.  

