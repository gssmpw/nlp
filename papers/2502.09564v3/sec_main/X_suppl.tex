\section{Ablation Studies with DDB's \textit{Recipe II}}
\label{supsec:abl_recipe2}
This appendix section focuses on additional ablation studies, dedicated to our \textit{Recipe II}. This set of ablations aims at providing additional evidence of Recipe II's robustness and effectiveness, similar to what is shown in Sec. \ref{sec:ablations} of the main paper regarding \textit{Recipe I}. As in the main paper, we run all our ablations on Waterbirds.
\paragraph{Bias amplifier data requirements. }
\begin{table}[ht]
    \centering
    \resizebox{0.6\columnwidth}{!}{
    \begin{tabular}{lccccc}
            \toprule
            \multicolumn{6}{c}{\textbf{\# Synth. images for BA training}} \\  
            \midrule
            \textbf{\# Imgs.}  & $100$  & $200$ & $500$ & $1000$  & $2000$ \\ \midrule
            \textbf{WGA}       & $84.42$ & $90.03$ & $90.18$ & $90.78$ & $\bf 91.56$ \\
            \bottomrule
    \end{tabular}
}
    \caption{Ablation on the number of training images for the Bias Amplifier, in terms of WGA on Waterbirds, when using DDB's \textit{Recipe II}.}
    \label{tab:num-images-recipe2}
\end{table}
This ablation study measures how the number of training images for the Bias Amplifier impacts DDB's \textit{Recipe II}. Results are evaluated in terms of WGA on Waterbirds (see Table \ref{tab:my_label}). $200$ images ($100$ images per class) are enough to obtain state-of-the-art competitive results (WGA equal to $90.03$), with an increasing number of images corresponding to higher WGA values, up to a maximum of $91.56$ for $2000$ total images.  
\paragraph{CFG strength and generation bias. }
\begin{table}[ht]
    \centering
    \resizebox{0.6\columnwidth}{!}{
        \begin{tabular}{lccccc}
                \toprule
                \multicolumn{6}{c}{\textbf{Guidance Strength}} \\
                \midrule
                $w$          & $0$     & $\bf 1$     & $2$ &        $3$  & $5$ \\ \midrule
                \textbf{WGA} & $87.07$ & $\bf 91.56$ & $88.01$ & $89.87$ & $88.30$ \\
                \bottomrule
        \end{tabular}
    }
    \caption{Impact of guidance strength $w$ on the final WGA on Waterbirds, when using DDB's \textit{Recipe II}.}
    \label{tab:guidance_strength_recipe_two}
\end{table}

Here, we evaluate DDB's \textit{Recipe II} dependency on the guidance strength parameter. Results are summarized in Table \ref{tab:guidance_strength_recipe_two}. The best results correspond to the usage of $w=1$, with different guidance strength parameter values having a relatively limited impact on debiasing performance. 

\paragraph{DDB impact on an unbiased dataset. }
Similarly to what is shown in Sec. \ref{sec:ablations}, we want to ensure that applying \textit{Recipe II} on an unbiased dataset does not degrade the final performance. Also \textit{Recipe II} demonstrates to be applicable in unbiased scenarios, as it measures a final test accuracy of $83.87 \%$ on CIFAR10, which is slightly higher than the ERM baseline ($83.26 \%$).
\section{Full Implementation Details of \textit{Recipe I} (two-step)}
For the hyperparameters of \textit{Recipe I}, we use for each dataset the following configurations. \textbf{BAR}: batch size = 32, learning rate = $5 \times e^{-5}$, weight decay = 0.01, training epochs = 80. 
\textbf{Waterbirds}: batch size = 128, learning rate = $5 \times e^{-4}$, weight decay = 1.0, training for 60 epochs. \textbf{BFFHQ}: batch size = 256, learning rate = $5 \times e^{-5}$, weight decay = 1.0, training for 60 epochs. The GDRO optimization is always run with the \texttt{--robust} and \texttt{--reweight\_groups} flags. 
\section{Full Implementation Details of \textit{Recipe II} (end-to-end)}
For \textbf{BAR}, we use the same parameters reported in \cite{nam2020learning}: batch size = 128, learning rate = $1 \times e^{-4}$, weight decay = 0.01, training epochs = 50. 
\textbf{Waterbirds}: batch size = 128, learning rate = $5 \times e^{-5}$, weight decay = 0.01, training epochs = 80. \textbf{BFFHQ}: batch size = 256, learning rate = $5 \times e^{-5}$, weight decay = 0.01, training for 50 epochs.
For these recipes, we accumulate gradients for $8$ iterations ($16$ for BFFHQ) with mini-batches made of $16$ samples.  

\section{Additional Biased Generations}
\label{supsec:synthetic_imgs}
In this Section, we present $100$ additional per class generated images for each dataset utilized in the study, as produced by our CDPM. In the Waterbirds dataset (Figure \ref{fig:waterbirds}), the model adeptly captures and replicates the correlation between bird species and their background. In the BFFHQ dataset (Figure \ref{fig:bffhq}), it effectively reflects demographic biases by mirroring gender-appearance correlations. The results from the BAR dataset (Figure \ref{fig:bar}) exhibit the model's adeptness in handling multiple bias patterns, preserving typical environmental contexts for different actions.  Finally, the ImageNet-9 dataset (Figure \ref{fig:imagenet9}) stereotypes the various classes and their backgrounds well. Overall, the model generates high-quality samples that are fidelity-biased across all datasets. 
Our research confirms that the model exhibits bias-learning behavior, as it not only learns but also amplifies existing biases within diverse datasets. We leverage this characteristic to produce synthetic biased data aimed at enhancing the robustness of debiasing techniques.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\linewidth]{figures/supplementary/synthetic/waterbirds.pdf}
    \caption{\textbf{Synthetic bias-aligned image generations for each class of the Waterbirds dataset.} Each grid displays 100 synthetic images per class, highlighting model bias alignment.}
    \label{fig:waterbirds}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/supplementary/synthetic/bar.pdf}
    \caption{\textbf{Synthetic bias-aligned image generations for each class of the BAR dataset.} Each grid displays 100 synthetic images per class, highlighting model bias alignment.}
    \label{fig:imagenet9}
\end{figure}




\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/supplementary/synthetic/imagenet9.pdf}
    \caption{\textbf{Synthetic bias-aligned image generations for each class of the ImageNet-9 dataset.} Each grid displays 100 synthetic images per class, highlighting model bias alignment.}
    \label{fig:bar}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\linewidth]{figures/supplementary/synthetic/bffhq.pdf}
    \caption{\textbf{Synthetic bias-aligned image generations for each class of the BFFHQ dataset.} Each grid displays 100 synthetic images per class, highlighting model bias alignment.}
    \label{fig:bffhq}
\end{figure}


\section{Bias Validation via Captions in Synthetic Images}
\label{supsec:bias_naming}
As an ulterior confirmation of bias in CDPM-generated images, besides two independent annotators, we implement an unsupervised analysis pipeline using the pre-trained BLIP-2 FlanT5$_{XL}$ model~\cite{DBLP:conf/icml/0008LSH23} for zero-shot image captioning without any task-specific prompting, for avoiding any guidance in the descriptions of the synthetic data. Our method processes the resulting captions through stop word removal and frequency analysis to reveal underlying biases without relying on predetermined categories or supervised classification models. Figure \ref{fig:waterbirds_histograms} presents the keyword frequency distribution for the Waterbirds dataset, where the most prevalent terms naturally correspond to known bias attributes (e.g., environmental contexts). The same is true for the keyword frequency distribution of the BFFHQ dataset (Figure \ref{fig:bffhq_histograms}) and BAR (Figure \ref{fig:bar_histograms}) where the most prevalent terms are related to the gender and the context respectively.
These findings confirm that generated images are bias-aligned. Furthermore, the obtained word frequency histograms suggest that images generated by DDB's bias diffusion step may provide information useful to support bias identification (or discovery) at the dataset level. 



\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.45\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/pdfs/waterbirds_class0_histogram.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/pdfs/waterbirds_class1_histogram.pdf}
    \end{subfigure}
    \caption{Word frequency analysis from 1000 generated captions for Waterbirds classes `landbird' (left) and `waterbird' (right), showing top 10 most frequent terms.}
    \label{fig:waterbirds_histograms}
\end{figure}




\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.45\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/supplementary/bffhq_class0_histogram.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/supplementary/bffhq_class1_histogram.pdf}
    \end{subfigure}
    \caption{Word frequency analysis from 1000 generated captions for BFFHQ classes `young' (left) and `old' (right), showing top 10 most frequent terms.}
    \label{fig:bffhq_histograms}
\end{figure}



\begin{figure}[htbp]
    \centering
    % First column of three subfigures
    \begin{subfigure}[b]{0.45\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/supplementary/bar_class0_histogram.pdf}
        \caption{Class 0}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/supplementary/bar_class1_histogram.pdf}
        \caption{Class 1}
    \end{subfigure}
    
    \vspace{0.5em}

    % Second row of two subfigures
    \begin{subfigure}[b]{0.45\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/supplementary/bar_class2_histogram.pdf}
        \caption{Class 2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/supplementary/bar_class3_histogram.pdf}
        \caption{Class 3}
    \end{subfigure}
    
    \vspace{0.5em}

    % Third row of two subfigures
    \begin{subfigure}[b]{0.45\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/supplementary/bar_class4_histogram.pdf}
        \caption{Class 4}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/supplementary/bar_class5_histogram.pdf}
        \caption{Class 5}
    \end{subfigure}

    \caption{Word frequency analysis from 1000 generated captions for BAR classes, showing top 10 most frequent terms for each class.}
    \label{fig:bar_histograms}
\end{figure}

