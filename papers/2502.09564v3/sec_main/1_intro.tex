\section{Introduction}
\label{sec:intro}
Deep learning models have shown impressive results in image classification, yet their success depends heavily on training data quality and representativeness. When a deep neural network is trained on a biased dataset, so-called ``shortcuts'', corresponding to spurious correlations (\ie, \textit{biases}) between irrelevant attributes and labels, are learned instead of semantic class attributes, affecting model generalization and impacting test performances. In other words, the model becomes biased towards specific training sub-populations presenting biases, known as \textit{bias-aligned}, while samples not affected by the bias are referred to as \textit{bias-conflicting} or unbiased. 
\\
In this scenario, several debiasing methods have been proposed \cite{NEURIPS2022_75004615_LWBC, tartaglione2023disentangling, pastore2024lookingmodeldebiasinglens, kim2021biaswap, Lee_Park_Kim_Lee_Choi_Choo_2023}, addressing the problem under different perspectives.
Notably, the majority of available training samples are bias-aligned in a biased dataset (typically, $95\%$ or higher \cite{nam2020learning}). As such, common supervised debiasing strategies involve reweighting the training set to give more importance to the few available bias-conflicting data \cite{Lee_Park_Kim_Lee_Choi_Choo_2023}, for instance, by upsampling \cite{li2019repair} or up-weighting \cite{Sagawa*2020Distributionally} them. Conversely, unsupervised debiasing methods, which assume no prior availability of bias information, typically involve an \textit{auxiliary model} to estimate and convey information on bias for reweighting the training set, employing their signal to guide the training of a debiased target model, while diminishing its tendency to learn bias shortcuts. 
\\To this end, there are several existing works \cite{nam2020learning, NEURIPS2022_75004615_LWBC, Lee_Park_Kim_Lee_Choi_Choo_2023, sohoni2020no} exploiting \textit{intentionally biased} auxiliary models, trained to overfit bias-aligned samples, thus being very confident in both correctly predicting bias-aligned training samples as well as misclassifying bias-conflicting ones \cite{Lee_Park_Kim_Lee_Choi_Choo_2023}. Such a distinct behavior between the two subpopulations is required to convey effective bias supervisory signals for target model debiasing, including gradients or per-sample loss values, which, \eg, should ideally be as high as possible for bias-conflicting and as low as possible for bias-aligned \cite{nam2020learning}.
However, such desired behavior is severely hindered by bias-conflicting training samples, despite their low number. In fact, the low cardinality of this population impedes a suitable modeling of the correct data distribution, yet, they are sufficient to harm the estimation of a reliable bias-aligned data distribution, despite the latter having much higher cardinality. First, bias-conflicting samples in the training set act as noisy labels, affecting the bias learning for the auxiliary models \cite{Lee_Park_Kim_Lee_Choi_Choo_2023}. Furthermore, these models tend to overfit and memorize bias-conflicting training samples within very few epochs, failing to provide different learning signals for the two subpopulations \cite{zarlenga2024efficient}. 
Despite available strategies, including the usage of the Generalized Cross Entropy (GCE) loss function \cite{nam2020learning,pastore2024lookingmodeldebiasinglens},  %multiple 
ensembles of auxiliary models \cite{NEURIPS2022_75004615_LWBC, Lee_Park_Kim_Lee_Choi_Choo_2023}, or even \textit{ad hoc} solutions tailored to the specific dataset considered (\eg, training for only one epoch as in \cite{pmlr-v139-liu21f_JTT}), how to define a protocol for effective and robust auxiliary model training for unsupervised debiasing approaches remains an open issue. 
\\
Remarkably, recent works \cite{pastore2024lookingmodeldebiasinglens, Lee_Park_Kim_Lee_Choi_Choo_2023, NEURIPS2022_75004615_LWBC} underline how the target model generalization is profoundly impacted by the protocol used to train such an auxiliary model. 
In principle, if one could remove all bias-conflicting samples from the training set, it would be possible to train a bias-capturing model robust to bias-conflicting samples' memorization and interference, and capable of conveying ideal learning signals \cite{Lee_Park_Kim_Lee_Choi_Choo_2023}. 
\\ To this aim, our intuition is to elude these drawbacks by skipping the use of the actual training data in favor of another set of samples synthetically generated by an adequately trained diffusion model. Specifically, we propose to circumvent bias-conflicting interference on auxiliary models leveraging \textit{fully biased} synthetic data exploiting intrinsic properties of Conditional Diffusion Probabilistic Models (CDPMs). We aim to learn a per-class biased image distribution, from which we sample an amplified synthetic bias-aligned subpopulation. The resulting synthetic bias-aligned samples are then exploited for training a \textit{Bias Amplifier} model instead of the original actual training set, solving the issue of bias-conflicting overfitting and interference by construction, as the bias-capturing model does not see the original training set at all. 
We refer to the resulting approach as \textit{Diffusing DeBias} (DDB). DDB's intuition is indirectly supported by recent works studying the problem of biased datasets for image generation tasks ~\cite{d2024openbias, kim2024training}, and showing how diffusion models' generations can be biased, as a consequence of bias present in the training sets (See Sec. \ref{sec:rel-work}). 
\\
This tendency can be seen as an undesired behavior of generative models, impacting the fairness of generated images, and recent papers have started to propose solutions for decreasing diffusion models' inclination to learn  biases~\cite{DBLP:journals/corr/abs-2408-15094, kim2024training, DBLP:conf/icb/PereraP23}. 
However, we claim that what is considered a problem for generative tasks can be turned into a feature for image classification model debiasing, as it allows the training of a robust auxiliary model that can ideally be plugged into various debiasing schemes, solving bias-conflicting training samples memorization, by construction (see Figure \ref{fig:teaser}).
For this reason, we refer to DDB as a plug-in for debiasing methods in image classification. %, allowing different \textit{recipes}. 
To prove its effectiveness and versatility, we incorporate DDB's bias amplifier into two different debiasing frameworks, named \textit{Recipe I} and \textit{Recipe II}, able to: 
(i) extract subpopulations further used as pseudo-labels within the popular G-DRO~\cite{Sagawa*2020Distributionally} algorithm (Recipe I, Sec.~\ref{sec:recipe-one}), and (ii) provide a loss function for the training set to be used within an end-to-end method (Recipe II, Sec.~\ref{sec:recipe-two}).
Both approaches outperform the state-of-the-art on popular benchmark datasets. 
To summarize, our main contributions can be summarized as follows:
\begin{itemize}
    \item We introduce DDB, a novel and effective unsupervised debiasing framework that exploits intrinsic properties of CDPMs to learn \textit{per-class} bias-aligned distributions in biased datasets, with unknown bias information. The resulting CDPM is used for generating synthetic bias-aligned images later employed for training a robust Bias Amplifier model, hence avoiding bias-conflicting training sample memorization and interference (Sec.~\ref{sec:approach}). 
    \item We propose the usage of DDB as a versatile plug-in for debiasing approaches, designing two \textit{recipes}: one based on a two-step procedure (Sec.~\ref{sec:recipe-one}), and the other as an end-to-end (Sec.~\ref{sec:recipe-two}) algorithm. %incorporating our bias- to prove its effectiveness and versatility. 
    \item Both proposed approaches prove to be effective, surpassing state-of-the-art results by a significant margin (Sec.~\ref{sec:experiments}) across four different benchmark datasets. 
\end{itemize}