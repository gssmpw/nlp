\section{Related Work}
In this section, we provide a broad categorization of model debasing works. 
\label{sec:rel-work}
\noindent
\\
\\
\textbf{Supervised Debiasing.} Supervised methods exploit available prior knowledge about the bias (\eg, bias attribute annotations), indicating whether a sample exhibits a particular bias (or not).  A common strategy consists of training a \textit{bias classifier} to predict such attributes so that it can guide the \textit{target} model to extract unbiased representations~\cite{alvi2018turning, Xie2017ControllableIT}. Alternatively, bias labels can be thought of as pre-defined encoded subgroups among target classes, allowing for the exploitation of robust training techniques such as G-DRO~\cite{Sagawa*2020Distributionally}. At the same time, bias information can be employed to apply regularization schemes aiming at forcing invariance over bias features~\cite{barbano2023unbiased,tartaglione2021end}.\\ 
\noindent
\\
\textbf{Unsupervised Debiasing.} It considers bias information unavailable (the most likely scenario in real-world applications), thus having the highest potential impact among the described paradigms. Among such methods, a popular strategy consists in involving auxiliary models at any level of the debiasing process. We categorize these methods into two-step  ~\cite{pmlr-v139-liu21f_JTT, sohoni2020no, kim2021biaswap}), and end-to-end ~\cite{nam2020learning, li2022discover}), in the following paragraphs. \\
\\
\textbf{End-to-end approaches.} Here, bias mitigation is performed online with dedicated optimization objectives. Employing techniques such as ensembling and noise-robust losses have been proposed, though they require careful tuning of all the hyperparameters, which may require the availability of validation sets with bias annotations to work. In Learning from Failure (LfF), Nam~\etal~\cite{nam2020learning} assume that bias features are learned earlier than task-related ones: they employ a \textit{bias-capturing} model to focus on easier (bias-aligned) samples through the GCE loss~\cite{zhang2018gce}. At the same time, a debiased network is jointly trained to assign larger weights to samples that the bias-capturing model struggles to discriminate. 
In DebiAN, Li~\etal~\cite{li2022discover} jointly train an auxiliary model (the \textit{discoverer})  and a \textit{classifier} (the target model) in an alternate scheme so that the discoverer can identify bias learned by the classifier during training, optimizing an ad-hoc loss function for bias mitigation. 
\vspace{2mm}
\\ \textbf{Two-step methods.} Here, the auxiliary model is employed in a first step to identify bias in training data, generally through pseudo-labeling, while the second stage consists in actual bias mitigation exploiting the inferred pseudo-labels. In~\cite {pmlr-v139-liu21f_JTT}, the auxiliary model is trained with standard ERM and then used in inference on the training set, considering misclassified samples as bias-conflicting and vice-versa: debiasing is brought on by up-sampling the predicted bias-conflicting samples. Notably, they must rely on a validation set with bias annotations to stop the auxiliary model's training after a few epochs to avoid memorization and to tune the most effective upsampling factor.  In~\cite{NEURIPS2022_75004615_LWBC}, a set of auxiliary classifiers is ensembled (\textit{bootstrap ensembling}) into a \textit{bias-commmittee}, proposing that debiasing can be performed using a weighted ERM, where the weights are proportional to the number of models in the ensemble misclassifying a certain sample. Finally, a recent trend involves the usage of vision-language models to identify bias, further exploiting bias predictions to mitigate biases in classification models~\cite{eyuboglu2022domino, kim2024discovering, zhang2023diagnosing}.\\
\\Among the described methods, a subset exploit the auxiliary model for bias amplification (\cite{Lee_Park_Kim_Lee_Choi_Choo_2023, nam2020learning, sohoni2020no}) under the general assumption that a bias amplification model would either misclassify or be less confident on bias-conflicting samples, with final debiasing performance strongly dependent on such behavior, and specifically, on \textit{how strongly} the auxiliary model captures training bias \cite{pastore2024lookingmodeldebiasinglens, Lee_Park_Kim_Lee_Choi_Choo_2023}.
However, this fundamental assumption only holds if bias-conflicting samples are not memorized during training, which is likely to happen in very few training epochs \cite{zarlenga2024efficient, pastore2024lookingmodeldebiasinglens}, especially considering that the very few training bias-conflicting samples are more likely to be overfitted, as reported in \cite{Sagawa*2020Distributionally, cao2019learning}. One could argue that an annotated validation set could be used for properly regularizing the bias amplification model, but the latter cannot be assumed available a priori in real cases, this being en emerging argument of discussion among the community \cite{zarlenga2024efficient, pastore2024lookingmodeldebiasinglens}. 
\\ For this reason, in this work and differently from the previously cited research, we propose that syntethic bias-aligned samples could be used \textit{instead} of the original training set, solving the bias-conflicting memorization issue by construction.
However, an unsupervised real-world scenario poses a problem for generating a \textit{pure} distribution of bias-aligned samples. As a solution, we propose to leverage diffusion models for generating a synthetic bias-aligned training set to be used for training a bias amplifier, inspired by   %thanks to which we avoid 
% and use them for unsupervised model debiasing, hence avoiding many limitations of the current state of the art by construction.
recent works, showing how they can manifest distorted, unfair, or stereotypical representations deviating from the true underlying data distribution~\cite{DBLP:journals/csur/MehrabiMSLG21}, when exposed to biases present in the training data ~\cite{DBLP:conf/icb/PereraP23}.
While the vast majority of available works regarding generative models and debiasing refer to strategies for obtaining unbiased synthetic data generation \cite{bhat2023debiasinggenerativemodelsusing, gerych2023debiasing}, up to our knowledge, we are the first to propose the usage of generative models for bias amplification.  The closest available work, employing generative models for model debiasing, is Jung~\etal~\cite{jung2023fighting}, where the authors introduce a debiasing method exploiting a GAN~\cite{10.1145/3422622}  trained for style-transfer between several \textit{biased} appearances, with a different objective. Here, a target model is debiased through contrastive learning between images affected by different biases for bias-invariant representations.\\ In our work, we show that it is possible to leverage the intrinsic tendency of diffusion models to capture bias in data, using conditioned generations to infer the biased distribution of each target class, thus allowing the sampling of new synthetic images, sharing the same bias pattern of original train data. 
Our method provides a memorization-free bias amplification model, capable of providing precise supervisory signals for accurate model debiasing virtually compatible with any debiasing approach involving auxiliary models. 
