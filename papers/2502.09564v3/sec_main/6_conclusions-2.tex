\section{Conclusions}
\label{sec:conclusions}
In this work, we present Diffusing DeBias (DDB), a novel debiasing framework capable of learning the per-class bias-aligned training data distribution, leveraging a CDPM. Synthetic images sampled from the inferred biased distribution are used to train a Bias Amplifier, employed to provide a supervisory signal for training a debiased target model. The usage of synthetic bias-aligned data permits avoidance of detrimental effects typically impacting auxiliary models' training in unsupervised debiasing schemes, such as bias-conflicting sample overfitting and interference, which lead to suboptimal generalization performance of the debiased target model. 
DDB is effective and versatile, acting as a plug-in for unsupervised debiasing methods.
In this work, we design a two-step and an end-to-end debiasing recipes incorporating DDB's Bias Amplifier: they outperform state-of-the-art works by a margin on four popular biased benchmark datasets, including the most recent relying on vision-language models~\cite{kim2024discovering, ciranni2024say}. 
Finally, while effectively mitigating bias dependency for biased datasets, DDB does not degrade performance when used with unbiased data, making it suitable for debiasing in real-world applications.
\vspace{1em}
\\ \noindent \textbf{Limitations.} 
DDB's main limitations rely on the well-known high computational complexity of diffusion models. As a trade-off between computational efficiency and generation quality, we reduce the training sets resolution to $64 \times 64$ when training the CDPM. Still, our CDPM requires $\sim 14$ hours for training on the largest considered dataset (\ie, BFFHQ, having $19,200$ images) on an NVIDIA A30 with $24$ GB of VRAM. 
Another limitation inherited from diffusion models is that the generation quality may decrease if the training dataset is too small, impacting DDB applicability for very small-scale datasets. 