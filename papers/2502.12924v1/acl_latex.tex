
\documentclass[11pt]{article}

\usepackage{acl}

\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc}

\usepackage{microtype}
\usepackage{todonotes}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{soul}
\usepackage{enumitem}
\usepackage{expex}
\usepackage{multirow}
\usepackage{makecell}

\usepackage{annotates}
\newcommand{\jer}[1]{\comment{Jeremy}{#1}}
\newcommand{\ait}[1]{\comment{Aitor}{#1}}
\newcommand{\mai}[1]{\comment{Maite}{#1}}

\usepackage{todonotes}




\title{Conditioning LLMs to Generate Code-Switched Text: \\A Methodology Grounded in Naturally Occurring Data}



\author{Maite Heredia \quad Gorka Labaka \quad Jeremy Barnes \quad Aitor Soroa  \\
  HiTZ Center - Ixa, University of the Basque Country UPV/EHU \\
\texttt{\{maite.heredia\}@ehu.eus} \\
}

\begin{document}
\maketitle
\begin{abstract}
Code-switching (CS) is still a critical challenge in Natural Language Processing (NLP). Current Large Language Models (LLMs) struggle to interpret and generate code-switched text, primarily due to the scarcity of large-scale CS datasets for training. This paper presents a novel methodology to generate CS data using LLMs, and test it on the English-Spanish language pair. We propose back-translating natural CS sentences into monolingual English, and using the resulting parallel corpus to fine-tune LLMs to turn monolingual sentences into CS. Unlike previous approaches to CS generation, our methodology uses natural CS data as a starting point, allowing models to learn its natural distribution beyond grammatical patterns. We thoroughly analyse the models' performance through a study on human preferences, a qualitative error analysis and an evaluation with popular automatic metrics. Results show that our methodology generates fluent code-switched text, expanding research opportunities in CS communication, and that traditional metrics do not correlate with human judgement when assessing the quality of the generated CS data. We release our code and generated dataset under a CC-BY-NC-SA license.\footnote{\href{https://github.com/hitz-zentroa/cs-generation}{GitHub} \includegraphics[width=0.05\linewidth]{figures/github.png} \:\:\:\:\:\href{https://huggingface.co/datasets/HiTZ/EN2CS}{HuggingFace}\:\includegraphics[width=0.05\linewidth]{figures/huggingface.png}}
        
\end{abstract}

\section{Introduction}
 \textit{Code-Switching} (CS) consists of mixing two or more languages within a single utterance and is a common phenomenon in multilingual settings \citep{global-bilingualism-tucker}. Although it is mainly present in spoken interactions, it can also be found in written interactions on-line \citep{language-contact-appel-muysken,interlingual-online}, where it appears jointly with other features of informal speech. Example \ref{ex:cs1} shows an utterance where the speaker switches between English and Spanish.

 \ex \label{ex:cs1}
 Why make everybody \textit{sentarse atrás pa' que} everybody has to move \textit{pa' que se salga}.\\
 Why make everybody \textit{sit at the back so that} everybody has to move \textit{so that she may get out.}\footnote{In all examples of CS featured in this paper, Spanish parts are shown in italics, in both the original instance and its translation.}\\
 \null\hfill\citep{poplack-sometimes}
 \xe
 

Despite the prevalence of code-switching, most research in Natural Language Processing (NLP) assumes monolingualism as a standard for human communication. However, this implicit decision means that state-of-the-art models are not able to properly interpret or generate CS data. Even advances in \textit{multilingual} language modelling \citep{lin-etal-2022-shot,chowdhery-etal-2023-PaLM} have not led to significant improvements, and performance on CS data is still poor compared to performance on monolingual data \citep{aguilar-etal-2020-lince,winata-etal-2021-multilingual}. This occurs because there is little CS text available in the multilingual pretraining data. Similarly, there are no parallel datasets available to learn to generate CS in a supervised fashion, as one would expect for tasks such as machine translation. Finally, there is no clear methodology for evaluating automatically generated CS text, as it has specific needs different from other text generation tasks. 


It is therefore crucial to develop methodologies to enable models to generate natural CS text and simultaneously implement robust evaluation frameworks that can assess how well NLP systems handle CS across multiple tasks. We argue that both of these goals require models that can conditionally generate CS from monolingual text.
Consequently, our research focuses on the development of a methodology to fine-tune and evaluate LLMs on the task of CS generation, following two main research questions:
\paragraph{RQ1:} Is it viable to approach CS generation with natural CS text as a starting point?
\paragraph{RQ2:} Do automatic metrics for Natural Language Generation (NLG) correlate well with human judgement for the task of CS generation?


\paragraph{}
We propose a novel method to generate CS text from monolingual text using LLMs, which requires less data for fine-tuning. We apply this to the English-Spanish pair (RQ1). Unlike prior work, our approach leverages naturally occurring code-switched data instead of artificial examples, allowing models to learn its natural distribution. Additionally, our source texts come from informal contexts, making the output both code-switched and informal — two intertwined phenomena. 
We evaluate our methodology both qualitatively, with a study on human preferences and a manual error analysis, and quantitatively, using automatic NLG metrics, which allows us to study the correlation with between human and automatic evaluation for this task (RQ2).


To summarize, the contributions presented in this paper are the following:
\begin{itemize}
	\item we develop a novel approach for CS generation through fine-tuning LLMs on a psuedo-parallel corpus sourced from natural code-switched data.
	\item we present a new parallel English-CS corpus, \textit{EN2CS}, with silver train and development splits and a gold post-edited test set, which can be used to train and evaluate model for English-Spanish CS generation.
	\item we perform both human and automatic evaluation of the results of models fine-tuned on this dataset, and perform a study on the correlation between human and automatic evaluation for CS generation.
\end{itemize}

\section{Related Work}
\label{sec:related-work}
\paragraph{Perspectives in linguistics.}CS naturally occurs in communities where two or more languages are in contact, making it a subject of great interest to fields like sociolinguistics and psycholinguistics. From a social perspective, it can be affected by the attitudes of the speakers towards the languages and the CS phenomenon itself. In this respect, it is related and associated with notions of prestige and identity \citep{2025-heredia-actitudes}. For example, in bilingual communities where a language is minoritized, CS can be regarded as an intrusion of the majority language \citep{dewaele-attitudes}. However, for migrant communities, it may be a way to preserve their mother tongue and as an ``emblem of ethnic identity'' \citep{poplack-sometimes}. Once again, its importance in different social contexts highlights the need to consider CS in NLP research, as it plays a crucial role in linguistic interactions and, consequently, the development of language technologies.
\paragraph{CS in NLP.} The processing and understanding of code-switched text can be crucial in the processing of social media data \citep{bali-etal-2014-borrowing-social-media}, and for speech applications, such as speech recognition or speech synthesis \citep{krishna-2017-synthesis}. In fact, non-monolingual speakers have shown preference for chatbots that use CS \citep{10.1145/3392846}. Different approaches may include normalization \citep{parikh-solorio-2021-normalization}, machine translation \citep{xu-yvon-2021-traducir} or modeling code-switched text \citep{gonen-goldberg-2019-language-modeling}. The survey by \citet{winata-etal-2023-decades} covers trends and advances in NLP for code-switched text, including main fields of interest and future research lines. \citet{dogruoz-etal-2021-survey} explain advances in applications of language technologies for code-switched text from a linguistic and social perspective. 

\paragraph{Datasets \& benchmarks for CS.} The majority of code-switched data is obtained from social media, and other popular data sources include recordings and transcriptions \citep{winata-etal-2023-decades}. There have been several shared tasks that deal with CS, for the tasks of Language Identification \citep{solorio-etal-2014-first-shared-task,molina-etal-2016-overview-second-shared-task} and Sentiment Analysis \citep{patwa-etal-2020-semeval}. Two popular benchmarks have been created to answer the demand for evaluation of CS that covers different language pairs and tasks: LINCE \citep{aguilar-etal-2020-lince}, which covers traditional tasks such as Part Of Speech tagging (POS) or Sentiment Analysis (SA); and GLUECoS \citep{khanuja-etal-2020-gluecos}, which incorporates NLU tasks for the Hindi-English pair. As of today, GLUECoS cannot be used without access to the X API.

\paragraph{CS generation.} CS generation has seldom been tackled in previous research. Approaches include using linguistically informed techniques that aim to find out plausible switching points \citep{pratapa-etal-2018-language-modeling-synthetic,gupta-etal-2020-semi,gregorius-okadome-2022-generating-dependency}, data augmentation \citep{tarunesh-etal-2021-machine-translation} and, more recently, prompting LLMs for CS generation \citep{yong-etal-2023-prompting}. To the best of our knowledge, there is no previous research on CS generation with natural CS as a starting point.


\section{Parallel Data Creation}
\label{sec:method-generate-cs}
In this work we present a novel approach to generate code-switched text from monolingual sentences. As a first step, we create a synthetic parallel corpus from an initial set of English-Spanish CS sentences with their English monolingual equivalents, generated by the Command R model~\citep{cohere_for_ai_2024}. We exploit the fact that LLMs struggle to generate CS text given a monolingual sentence, but are able to more reliably convert a CS sentence to its corresponding monolingual version, especially when the target language is English. After having created this pseudo-parallel corpus, we use it to fine-tune LLMs on the task of conditional code-switching generation, presented in Section \ref{sec:mono-cs}.


\subsection{The LINCE benchmark}
\label{sec:initial-data}
We use LINCE as a starting point, a popular benchmark that has been widely used to evaluate CS systems \citep{aguilar-etal-2020-lince}, which is available in 6 language pairs. This benchmark contains annotations for 5 different tasks: Language Identification (LID), Part Of Speech tagging (POS), Named Entity Recognition (NER), and Sentiment Analysis (SA). All sentences in LINCE are tokenized, and each token is annotated with a language tag as well as other categories depending on the task. In our work we focus on the English-Spanish pair and filter all sentences in the data that do not contain CS, similarly discarding all the task-specific annotations. Example \ref{ex:lince} shows a random instance from LINCE.

\ex \label{ex:lince}
\resizebox{0.9\columnwidth}{!}
{%
\begin{tabular}{ccccccc}
         \underline{estaba}&  \underline{aquí}&  \underline{three}&  \underline{feet}&  \underline{away}&  \underline{.}\\
         spa&  spa&  eng&  eng&  eng&  eng\&spa\\
\end{tabular} }
\xe







LINCE comprises around $95,000$ train, $20,000$ development, and $33,000$ test instances for the English-Spanish pair. We deduplicate the instances among splits, and filter and pre-process the instances to ensure that they are suitable for our task by removing links, replacing usernames with the placeholder \textit{<user>}, and detokenizing all instances with the script provided as part of the Moses toolkit \citep{koehn-etal-2007-moses}. After this preprocessing, we obtain a more natural version of the LINCE data. A preliminary analysis reveals that many sentences in LINCE are monolingual or contain a single word in one language that often correspond to a borrowing, as shown in Example \ref{ex:tequila}. In order to ensure that all of our sentences actually contain CS, we filter sentences that do not have at least two words in each language.

    \ex  \label{ex:tequila}
        I need a shot of \underline{tequila} or a glass of scotch to keep me warm right now.
    \xe

After these pre-processing and filtering steps, we end up with $12,933$ train, $2,461$ development and $5,353$ test instances. The comparison between the original size of LINCE and the final number of sentences selected for our experiments after pre-processing is shown in Table \ref{tab:parallel-size}.

\begin{table}[t]
    \centering
    \begin{tabular}{lrrr}
        \toprule
          &\textbf{Train}&  \textbf{Dev}& \textbf{Test}\\
 \cmidrule(lr){2-2}\cmidrule(lr){3-3} \cmidrule(lr){4-4}Original&94,728 & 19,574&33,361\\
          Pre-processed &12,933&  2,461& 5,353\\
          \textit{EN2CS} &10,703&791&1,040\\\bottomrule
    \end{tabular}
    \caption{Size of original LINCE (EN-ES) compared to the automatically filtered instances and the final set of parallel instances, dubbed \textit{EN2CS}.}
    \label{tab:parallel-size}
\end{table}

\begin{table*}[t]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lll}
        \toprule
        & \textbf{Original} & \textbf{English} \\
        \cmidrule(lr){2-2}\cmidrule(lr){3-3}
        \multirow{2}{*}{Silver} 
        & you just have to tell me \textit{que como te va}. 
        & You just have to tell me \textit{how it's going.} \\\cmidrule(lr){2-2}\cmidrule(lr){3-3}
        & osea i know we wanna party \textit{pero tampoco no aya asta dallas} 
        & like i know we want to party \textit{but not all the way to dallas} \\
        \cmidrule(lr){2-2}\cmidrule(lr){3-3}& fully \textit{enamorada de mis} eyelash extensions 
        & I'm totally \textit{in love with my} eyelash extensions\\
        \midrule
        \multirow{2}{*}{Gold} 
        & \textit{hasta venir a plaza se siente} like home. 
        & \textit{even coming to the square feels} like home. \\\cmidrule(lr){2-2}\cmidrule(lr){3-3}
        & \textit{me siento tan pendejo} right now.	
        & \textit{i feel so stupid} right now. \\\cmidrule(lr){2-2}\cmidrule(lr){3-3}
        & \textit{y no te dan problemas} as long as you put that it was frozen.	
        & \textit{and they don't give you problems} as long as you put that it was frozen. \\
        \bottomrule
    \end{tabular}
    }
    \caption{Examples of the \textit{EN2CS} parallel corpus. Left: original code-switched instances, right: generated (silver) or post-edited (gold) English instances.}
    \label{tab:parallel-examples}
\end{table*}
\subsection{\textit{EN2CS}}
\label{sec:parallel-data-creation}
The next step in our method requires creating a pseudo-parallel English-CS dataset by translating the natural code-switched instances into monolingual text.  As there are no available machine translation systems to convert from English-Spanish CS text to English monolingual text, we instead make use of prompt engineering, using the Command R model \citep{cohere_for_ai_2024}, one of the strongest publicly available models at the time.

We perform an initial set of experiments to determine the optimal prompt to generate monolingual English versions of the code-switched data. Ideally, we aim for a prompt that generates translations that maintain the meaning of the original sentences, are fluent and natural, whose grammar is correct and that does not contain any Spanish words or phrases. The tested prompts are listed and explained in Appendix \ref{sec:appendix-prompts}, and the prompt that generates outputs closest to the desired ones is: \textit{Now convert this code-switched phrase to English. Leave the parts in English as they are, focus on translating the parts in Spanish.} Preliminary experiments also showed that a few-shot strategy helps the model for the task at hand, so the prompt is enriched with 5 manually selected examples that show how the output should look like.

The generated monolingual sentences are further processed, and instances that contain profanity or that are prefaced with a direct mention of the task, such as ``Of course, here's your translation:'' are discarded.


In order to create a true gold standard test set to evaluate a CS generation model, we perform a manual post-edition of the the monolingual test translations for $1,040$ instances of the LINCE test set. The post-edition was carried out by three proficient speakers of English and Spanish, who were provided with specific guidelines as shown in Appendix \ref{sec:appendix-post-edition-guidelines}.

Table \ref{tab:parallel-size} shows the final size of the parallel corpus, which we dub \textit{EN2CS}, after post-processing and post-edition, and Table \ref{tab:parallel-examples} shows examples of silver and gold instances. The final version of our dataset therefore contains $10,703$ train and $791$ development instances with automatically translated English sentences matched to their original CS sentences, and $1,040$ gold instances with corrected English translations.






\section{CS generation experiments}
\label{sec:mono-cs}



With \textit{EN2CS} as our starting point, we frame CS generation as a machine translation task, with English as the source and CS as the target language, where parts of the source sentence have to be translated to Spanish. In our experiments we try four generative models, namely, Llama3, Llama3 Instruct \cite{dubey2024llama}, Mistral and Mistral Instruct \cite{jiang2023mistral7b}.\footnote{Llama3 models are used in their 8B size (\href{https://huggingface.co/Undi95/Meta-Llama-3-8B-hf}{Base} and \href{https://huggingface.co/Undi95/Meta-Llama-3-8B-Instruct-hf}{Instruct}). Mistral models are used in their 7B size, version 0.3 (\href{https://huggingface.co/mistralai/Mistral-7B-v0.3}{Base} and \href{https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3}{Instruct}).} All the models are trained with the causal language modelling objective, but we use different input formats for the base and instruct models. For base models we follow \citep{zhu-etal-2024-multilingual} and use templates in the form of ``\texttt{<X>=<Y>}'', where \texttt{<X>} and \texttt{<Y>} are placeholders for the input English sentence and generated CS, respectively. At inference, the second code-switched part is left empty for the model to fill. For fine-tuning instruction-tuned models, we provide them with a system prompt that contains the instruction, a query by the user in English, and an answer from the assistant with the code-switched target. At inference time, the same system prompt is used, and the user prompt contains the English sentence, so that the model generates the assistant part. Table \ref{tab:exampl-finet-base} shows examples of the format used for fine-tuning base and instruction-tuned models. 


\begin{table}[t]
  \centering
  \begin{tabular}{p{.45\textwidth}}
    \hline
    \multicolumn{1}{c}{\textbf{Base model}}  \\ \hline
    \small \textit{I want to not work and make money.} = \textit{quiero no trabajar and make money} \\ \hline
    \multicolumn{1}{c}{\textbf{Instruction-tuned model}}\\
    \cmidrule(lr){1-1}\small \textbf{system prompt}: "\textit{You are a bilingual speaker of English and Spanish. Translate the following English sentence into code-switched text between both languages:}"\\
    \small  \textbf{user}: "\textit{I want to not work and make money.}"\\
    \small \textbf{assistant}: "\textit{quiero no trabajar and make money}"\\\hline
  \end{tabular}
  \caption{Examples and format of prompts used for finetuning base and instruction-tuned models}
  \label{tab:exampl-finet-base}
\end{table}
\begin{table*}[t]
\resizebox{\textwidth}{!}{
    \renewcommand{\arraystretch}{1.2} %
    \centering
    \begin{tabular}{ll}  
        \toprule
        \textbf{Model} & \textbf{Generated Output} \\ 
        \cmidrule(lr){1-1}\cmidrule(lr){2-2}
        \makecell[l]{Original (Gold)} & damm \textit{todos se casaron }and we still single lol forever alone \\  
        English (Source) & damn everyone got married and we're still single lol forever alone \\  \cmidrule(lr){1-1}\cmidrule(lr){2-2}
        MT Baseline & damn everyone got married and we still be single lol \\  
        Llama3 & damn \textit{todos se fueron a casarse y nosotras estamos solitarias} lol forever alone \\  
        Llama3 Instruct & damm every1 got married and we're still single lol \textit{alonso solit@o} foreverrrr lolololo \\  
        Mistral & damm everyone got married \textit{y ni somos esa} lol ofrever alone \\  
        Mistral Instruct & dude \textit{todo el mundo se casó y nosotros hemos quedado solteros} lol \\  
        \bottomrule
    \end{tabular}
    }
    \caption{Example from the test set and the generated outputs of the different models.}
    \label{tab:generated-text}
\end{table*}
All models are trained using Quantized Low-Rank Adaptation (QLoRA) \citep{dettmers2023qloraefficientfinetuningquantized}, to ensure memory and parameter efficiency, with standard parameters: the model is loaded in 4 bit with NF4 quantization data type and bf16 computational data type. The LoRA rank and scaling factor are set to 16 and the dropout to 0.05. We apply the LoRA update matrices to the attention blocks and do not train bias parameters. Regarding the hyperparameters, we only tune the learning rate ($1e\textsuperscript{-4}$, $5e\textsuperscript{-4}$, $1e\textsuperscript{-3}$ and $5e\textsuperscript{-3}$) and training epoch $\in [1\ldots10]$, choosing the parameters that give the lowest cross-entropy loss on the development set for each model. We use the transformers package \citep{wolf-etal-2020-transformers} for all training experiments.






Early experiments indicated that the models' outputs are longer than expected and usually produce the desired output up to a punctuation mark and then either begin to translate the sentence again or hallucinate more content. We therefore truncate the output up to a punctuation mark where the length is closest to that of the original sentence. %
We additionally experimented with the \textit{length\_penalty} and \textit{exponential\_length\_decay} generation parameters, as well as trying to control the length of the generation with length codes, but find that the truncation heuristic performs the best. Accordingly, all further experiments will use the truncated output. This overgeneration problem has been reported in previous papers, where similar truncation strategies have been adopted \citep{bawden-yvon-2023-investigating-translation-bloom}.


We also include a dedicated encoder-decoder model as a baseline, trained on \emph{EN2CS} using the MarianNMT toolkit \citep{junczys-dowmunt-etal-2018-marian}, with a mini-batch size automatically selected for the available memory\footnote{We used two GPUs and $8GB$ per GPU.}. Optimization is performed using Adam \citep{Kingma2015adam}, with $\alpha=0.0003$, $\beta_1=0.9$, $\beta_2=0.98$ and $\epsilon=10^{-9}$, using a standard learning rate of $3e^{-4}$. Validation is conducted every $100$ steps, and training stops if perplexity showed no improvement after 5 consecutive checkpoints.

 Table \ref{tab:generated-text} shows an example of the outputs of the different models, compared to the original code-switched sentence and the English monolingual sentence that they received as input. 

\section{Qualitative evaluation}
\label{sec:qual-eval}

As a first step to assess the quality of the outputs produced by the different models, we perform a manual qualitative analysis of the results in two parts: a pairwise tournament-based human evaluation, and an in-depth analysis of the most common errors made by the models and their distribution.

\subsection{Preference based evaluation}
\label{sec:pref-based-eval}




We perform a tournament-based evaluation that allows us to determine the ranking of models in terms of human preference. A total of $660$ instances are matched against each other, corresponding to the outputs of the five models for $110$ English source sentences, as well as the gold standard reference. The evaluation is conducted pairwise, requiring annotators to choose the best out of two sentences or declare a tie. When choosing the best sentence, annotators do not know the original English sentence, nor which model produced what output. This process results in $110\cdot{6\choose 2} = 1,650$ comparisons, and was carried out by $11$ annotators, with each annotator performing $150$ random comparisons.


Annotators are provided with a series of criteria to choose between the instances, devised after the error analysis described in the next section. They must take into account three main criteria, that must be applied in the following order: a) the presence and naturalness of the CS; b) the content and fluency of the sentences; and c) the orthographical errors of the instances (correct punctuation, presence of typos, etc.). Annotators are furthermore asked to avoid declaring ties, unless completely necessary (e.g., in a case where both sentences are completely monolingual and therefore equally incorrect), to compel them to develop a preference. The complete annotation guidelines are available in Appendix \ref{sec:annotation-guidelines}.


\begin{table}[t]
    \centering
    \begin{tabular}{clr}
        \hline
         \textbf{Ranking}&\textbf{Model}&\textbf{Score}\\
         \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\includegraphics[width=0.06\linewidth]{figures/first-place.png}& Gold Standard & 392.5 \\ \includegraphics[width=0.06\linewidth]{figures/second-place.png}& Llama3  & 325.5 \\ \includegraphics[width=0.06\linewidth]{figures/third-place.png}& Llama3 Instruct & 303.0 \\
                  4& Mistral & 285.5 \\
         5& Mistral Instruct & 242.0\\
         6& MT Baseline & 101.5\\\hline
    \end{tabular}
    \caption{Ranking of models according to the human preference score.}
    \label{tab:human-ranking}
\end{table}

With the results of the pairwise comparisons, we calculate a global score for each model, as follows: every time a model is voted, it gets $1$ point, and the loser gets $0$ points; in case of ties, both models get $0.5$ points each. Table \ref{tab:human-ranking} shows the global scores, as well as the ranking of human preferences according to said score. We find that the gold standard reference obtains the highest score, as expected, and that Llama3 ranks the highest among the automatic methods. Instruction-tuned models obtain worse scores compared to their base model counterparts, with a similar difference for both Llama3 and Mistral family of models. Finally, the evaluation also shows that humans clearly prefer the output of systems based on Large Language Models compared to the MT baseline, which is voted as the worst model by a large margin. 




\subsection{Error analysis}
\label{sec:error-analysis-1}

In order to further explore differences between model performance, we analyse the most common errors made by the CS generation models, both quantitatively and qualitatively. We adapt the methodology presented in \citet{Popović2018errorclassification}, who proposes different typologies for machine translation errors, and extend it to CS generation error analysis. To do so, we randomly select a set of 100 outputs from all models and conduct a detailed examination of the types of errors present in them. This thorough analysis allows us to identify recurring patterns and propose a refined error typology specifically for automatic CS generation. This initial error analysis yields 18 total error categories, which we simplify and group into three main error types: a) CS errors, b) Translation Errors, and c) Format errors. The full error typology, along with detailed descriptions for each error type, is provided in Appendix \ref{sec:appendix-error-typology}, while here we explain the three error categories:


\begin{itemize}
    \item[] \textbf{CS Errors}: Errors of sentences that are either completely monolingual or switch between languages in an unnatural manner, e.g., by repeating the same word in English and Spanish. In Example \ref{ex:error1}, Llama3 Instruct preserves the original meaning, but the sentence is fully monolingual.
    
    \vspace*{-.2cm}
    \ex \label{ex:error1}
    \begin{tabular}{p{0.07\textwidth} p{0.7\linewidth}}
         \textbf{Source} & After all these things when we're done.\\
         \textbf{Output} & after all these things when we're finished\\
    \end{tabular} 
    \xe
    \vspace*{-1cm}
\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/error_analysis.pdf}
    \caption{Error distribution by model, obtained by counting the number of instances that present errors of each type.}
    \label{fig:error-analysis}
\end{figure}

    \item[] \textbf{Translation errors}: Critical errors that either change the original meaning of the sentence or introduce mistakes in fluency or grammar, for example, using the wrong tense or word order. Example \ref{ex:error2} shows an instance where Mistral Instruct outputs a seemingly natural code-switched sentence, but the phrase ``they got hurt'' is not adequately translated and the meaning of the sentence is not preserved.
    
    \vspace*{-.2cm}
    \ex\label{ex:error2}
    \begin{tabular}{p{0.07\textwidth} p{0.7\linewidth}}
         \textbf{Source} & I wasn't happy because they got hurt.\\
         \textbf{Output} & \textit{no estuve} happy \textit{porque me dieron mal}\\
    \end{tabular} 
    \xe
    \vspace*{-1cm}
    
    \item[] \textbf{Format errors}: Errors in form that do not make the sentences unintelligible nor change their meaning, such as repetitions of a word or phrase or incorrect punctuation. Example \ref{ex:error3}, by the model Llama3, accurately preserves the original meaning and introduces CS, but removes the username and adds a smiley face.
    
    \vspace*{-.2cm}
    \ex\label{ex:error3}
    \begin{tabular}{p{0.07\textwidth} p{0.7\textwidth}}  
         \textbf{Source} & <user> old mexican remedies\\
         \textbf{Output} & old school \textit{remedios mexicanos} :)\\
    \end{tabular} 
    \xe
    \vspace*{-1cm}
   
\end{itemize}

We classify $500$ additional instances ($100$ instances per model, obtained from the same source sentences) into these kind of errors, and show the results in Figure \ref{fig:error-analysis}. The analysis indicates that the baseline MT system has the worst performance, with nearly twice as many errors as the best models. This is in line with the results of the preference-based evaluation, where MT ranked last. The most frequent errors made by the MT system --- word repetition, missing information, and incorrect translations --- are particularly problematic, as they often make the output unintelligible or incomplete. Among the pre-trained models, Llama models make the fewest errors overall, with 19 fewer errors on average. Base models of both families struggle mainly with format errors, which make up $50.68\%$ of their errors on average, whereas instruction-tuned models present more meaning-related issues, $53,45\%$. This suggests that the linguistic knowledge of the models degrade when tuned on instructions, a phenomenon that has been observed on other related areas~\cite{fu-etal-2024-disperse}. CS-related mistakes are the least common in all models, accounting for less than $15\%$ of the overall error count. It seems that the models have effectively learned to switch between languages naturally, though they may still be prone to other types of errors.


\section{Automatic Evaluation}
\label{sec:evaluation}
\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}
{%
\begin{tabular}{lccc}
\hline
    \textbf{Model}& \textbf{BLEU}& \textbf{BERTScore}&\textbf{chrF}\\ 
  \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3} \cmidrule(lr){4-4}Llama3& \textbf{34.49}& \underline{81.64}&\textbf{53.17}\\
  Llama3 Instruct& \underline{33.42}& \textbf{81.77}&\underline{52.01}\\
  Mistral& 31.65& 80.93&50.56\\
 Mistral Instruct& 25.98& 78.66&44.58\\ \hline
 MT Baseline& 20.21& 76.38 &33.79 \\\hline
\end{tabular}%
}
\caption{Results of automatic metrics the \textit{EN2CS} test set. Best results in bold, second best results underlined.}
\label{tab:test-results}
\end{table}
We perform a quantitative evaluation using traditional metrics used in NLG. To that end, we use BLEU \citep{papineni-etal-2002-bleu}, BERTScore \citep{zhang2020bertscoreevaluatingtextgeneration}, and chrF \citep{popovic-2015-chrf}, implemented with the \href{https://huggingface.co/docs/evaluate/index}{evaluate} library. All three are reference-based task-agnostic quality metrics that give results between 0-1, based on character-level F-score, n-gram precision and semantic similarity using contextual embeddings\footnote{BERTscore has been calculated using the embeddings from the model \href{https://huggingface.co/google-bert/bert-base-multilingual-cased}{Bert Base Multilingual Cased}.} respectively. 

The results of the evaluation can be seen in Table \ref{tab:test-results}. The best two models are Llama3, with the highest BLEU and chrF, and Llama3 Instruct, with the highest BERTScore. They are closely followed by less than 2 points in all metrics by Mistral, the third best model overall. Finally, Mistral Instruct is the pre-trained model with the lowest results.%



The MT baseline obtains the lowest results overall, which is consistent with the qualitative evaluation described above, where the MT baseline proves to be the worst system by a large margin. However, it is worth noting that not all metrics capture this gap in performance, since, according to BERTScore, there is only a 2 point difference between the MT baseline and Mistral, which is the pre-trained LLM that obtains the worst metrics. It is also interesting to compare these results with the error analysis in Section \ref{sec:error-analysis-1}. For instance, Mistral Instruct yields low values for BLEU and chrF, in line with its number of translation errors, as depicted in Figure \ref{fig:error-analysis}. However, automatic metrics fail to capture the fact that CS errors produced by language model based systems are relatively low.






\subsection{Correlation With Automatic Metrics}
\label{sec:corr-with-autom-1}



The automatic metrics used in the section before are known to have poor correlation with human judgment in NLG tasks \citep{survey-nlg-metrics}, and in this section we analyse whether this poor correlation also occurs when evaluating CS generation. To that end, we compare the automatic metrics results with the preference-based scores obtained in Section \ref{sec:pref-based-eval}.



We calculate Pearson's ($\rho$) correlation coefficient at instance-level, using the $550$ instances employed for the error classification and human evaluation (the output of 5 models for $110$ source sentences)\footnote{We do not consider the reference CS sentence when calculating the correlations.}. Each data point corresponds to the CS output of one particular model for an English source sentence, and we compute the correlation using two values: the score obtained by the model for this instance in the human preference-based evaluation of Section \ref{sec:pref-based-eval}, and the score it attains if we apply the same strategy using the values of the automatic metrics to determine the winner.


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/heatmap.pdf}
    \caption{Heatmap of the correlations between human scores and automatic metrics, calculated using the Pearson Correlation Coefficient. The correlations are calculated for all instances, as well as for different subsets of instances, according to the type of errors they exhibit. * indicates statistical significance \((p\le0.05)\).}%
    \label{fig:heatmap}
\end{figure}

The correlation coefficients are shown in Figure \ref{fig:heatmap}. The top part of the figure shows the correlation using all the instances, whereas the bottom part only considers those instances that showed some type of error, according to the error analysis described in Section \ref{sec:error-analysis-1}. If we consider all the instances, the maximum $\rho$ correlation value is $0.28$, which indicates a low alignment with the scores assigned by automatic metrics and human judgments. The metric with highest correlation is chrF, which seems to confirm previous research that reports a higher correlation with human judgement than other metrics \citep{popovic-2015-chrf}.

Regarding the correlations according to the error types, all metrics obtain the highest correlation on errors related to meaning and format, which is expected, as they were originally designed to evaluate MT systems. However, instances with CS errors show the lowest correlation overall, with only chrF showing $\rho$ values above $0.05$. Human evaluators were explicitly asked to never choose instances without CS, but automatic metrics are not sensitive to these nuances, and may assign high scores to instances regardless of whether they contain CS or not.

All in all, these results confirm that several of the most commonly used automatic metrics for NLG have a poor correlation with human judgments when evaluating CS generation. This underscores the need to research more specialized evaluation methods which are designed specifically to capture the nuances of this task. 





\section{Conclusion}
In this work, we have presented a methodology to leverage LLMs in the generation of code-switched text from monolingual instances, specifically for the English-Spanish language pair.

Our framework consists of back-translating natural code-switched instances (EN-ES) into monolingual English sentences, and using the resulting parallel corpus, dubbed \textit{EN2CS}, to fine-tune autoregressive models to turn monolingual sentences into CS. This approach has the potential to improve the naturalness of CS generation, as the gold standard CS text is not artificially generated.

We have provided an extensive evaluation of the results of our models. On the one hand, we perform a human evaluation of the outputs of the models with two parts: a human preference based evaluation and an error analysis of a subset of the test sentences, where we find out three types of errors: CS errors, errors in meaning and format errors. On the other, we employ popular NLG metrics to automatically evaluate the results of our models, and calculate the correlation between both types of evaluation. 

Both the automatic metrics and the analysis of the outputs of the models, as well as the human evaluation, show that, when fine-tuning the models with LoRA, base models work better than their instruction-tuned counterparts for this task, and that the Llama3 family obtains better scores than the Mistral family. This could be an example where instruction tuning degrades the base model's linguistic ability \cite{fu-etal-2024-disperse} or alternatively could be related to differences in how LoRA fine-tuning affects each model type.

Our analyses show low correlation between human and automatic evaluations, particularly in cases with CS errors. This suggests that current metrics are not adequate for assessing CS generation, which would require more specialized evaluation methods.

We conclude that pre-trained models are able to yield competent results and generate satisfactory outputs, as the error analysis shows the less abundant type of errors are those related to CS, as opposed to problems in fluency, retaining the original meaning of the sentences, and errors in format. Human evaluation shows that their generations are still not on a par with the original instances. 



\section*{Limitations}
Our research focuses on testing the capabilities of LLMs for CS generation, a field of interest in the research of many applications, yet still in need of more research. While our findings highlight promising potential, we also identify key areas for refinement and improvement, as well as promising lines for future research in this domain.

We only perform an in-domain evaluation where the train, validation and test sets had the same origin. Additionally, we would like to test the efficiency of our models in an out-of-domain setting, since one of the use-cases of a CS generation model is to create parallel corpora to evaluate the abilities of models to perform different tasks when there is CS.

One of the key points of our research is using open-weight LLMs, however, the use of bigger, more powerful restricted-weights LLMs could very plausibly yield better results, even in a zero-shot scenario \citep{huzaifah-etal-2024-evaluating-MT-CS-LLMs}. 

We want to acknowledge the fact that our approach is dependent on having an initial set of code-switched sentences, which may not be available for all pairs of languages, especially in a low-resource scenario. We believe that it would be interesting to explore the possibility of a cross-lingual approach using our methodology, with English and/or Spanish as pivot languages, that could be useful for transfer knowledge into other less-resourced language pairs.

Finally, as we have pointed out, we are aware of the problems of the automatic metrics that we have used to evaluate the outputs of our models, that do not capture the nuances of our task. In the future, we would like to investigate how to improve this evaluation by designing new methods to automatically evaluate CS generation, focusing on a more linguistic approach able to capture the linguistic and social intricacies of CS.


\bibliography{anthology}
\bibliographystyle{acl_natbib}

\appendix
\section{Prompt Tuning}
\label{sec:appendix-prompts}
We test the prompts in Table \ref{tab:prompts}, combined with 0-, 1- and 5-shot strategies. The prompts include the instructions explained in different ways, including more or less information. 

\begin{table*}[t]
    \renewcommand{\arraystretch}{1.2} 
    \centering
    \begin{tabular}{p{0.95\textwidth}} 
        \hline
        Convert this code-switched phrase to English.\\ \hline 
        Convert this code-switched phrase to English without correcting the original spelling, focus on translating the parts in Spanish.\\ \hline
        \textbf{Convert this code-switched phrase to English. Leave the parts in Spanish as they are, focus on translating the parts in Spanish.}\\\hline 
        Convert this code-switched phrase to English. Directly output the translation and don't correct the original spelling, focus on translating the parts in Spanish.\\ \hline
    \end{tabular}
    \caption{Different prompts that have been used to convert the code-switched instances into English, with different levels of specificity. Final prompt in bold.}
    \label{tab:prompts}
\end{table*}


For the few-shot strategies, the prompt includes the following template at the beginning, alongside a set of manually selected examples:

\textit{Here are \{n\} examples of a code-switched text that has been converted to \{lang\}:}

Testing the different prompts, we are able to choose the one whose outputs are closest to our needs, taking into consideration the trade-off between including too little and too much level of specificity in the instructions to the models. 

Regarding the few-shot strategies, we find out that giving some examples to the models results in outputs that are more aligned with the expected output, which is logical, since this allows the models to more faithfully replicate the examples provided. The more examples given, the more the model is able to comply to leaving the punctuation marks as they are and not standardizing the spelling, but also it tends to add more colloquial terms and alternate spellings. 






\section{Post-edition Guidelines}
\label{sec:appendix-post-edition-guidelines}
The original sentence should contain \underline{\textbf{CS}} and be \underline{\textbf{translatable}}. The main reasons to \textbf{remove} an instance altogether are:
\begin{itemize}
    \item If the sentence is very clearly monolingual and the CS has been detected incorrectly (eg, the case of interlingual homographs such as \textit{has}).
    \item When the sentence is bilingual for metalinguistic reasons, because it makes the translation tricky and hard to understand, and in most cases it’s not even CS.
    \item The part that is in the other language is a named entity, such as a title, a name, …
    \item If the code-switched part is not translatable or very hard to translate, probably because it’s a borrowing. Ambiguous and a little bit up to the annotator.
    \item If the tweet is saying the same thing in both languages (making it monolingual doesn’t make sense).
    \item Some instances are tweets that are part of a conversation or thread and taken out of context are very hard to understand/intelligible.
    \item Some tweets are not translatable because of wordplay that doesn’t transfer to monolingual speech.
\end{itemize}
The result should be a \underline{\textbf{monolingual}} sentence that has roughly the \textbf{\underline{same meaning}} as the original sentence. The main reasons to edit a translation are:
\begin{itemize}
    \item If the meaning changes or the model has hallucinated extra information that wasn’t present in the original sentence.
    \item If there are still some words in the Spanish.
    \item Attempts to translate named entities.
    \item Remove “meta comments” from the model about the task.
\end{itemize}

It is not necessary to correct things like:
\begin{itemize}
    \item Punctuation marks.
    \item Different spellings of the same word.
    \item Words of phrases that the model has changed for synonyms.
\end{itemize}




\section{Pairwise Annotation Guidelines}
\label{sec:annotation-guidelines}
The main objective of this task is two evaluate a pair of sentences that \textbf{should contain code-switching between English and Spanish}.
It should be noted that models have been trained with texts extracted from social media and informal conversations, so \textbf{the outputs of the models are expected to present traits of informality}, such as common typos, that at first should not be considered errors, because they are within the expected behaviour of the models. 
The criteria to choose between both sentences is to be applied \textbf{in the following order}:



\begin{enumerate}
    \item \textbf{Code-switching}
    \begin{enumerate}[label*=\arabic*.]
        \item \textbf{Presence of code-switching}: For a sentence to be a suitable candidate it must have tokens in both languages. A completely monolingual sentence will always be wrong.
        \item \textbf{Naturalness of the code-switching}: A switch between both languages can be unnatural. There are different linguistic constraints. For example, a switch is only possible at a point in a sentence where it does not violate the syntactic rules of either language.
    \end{enumerate}
    \item \textbf{Content and fluency}
    \begin{enumerate}[label*=\arabic*.]
        \item \textbf{Content}: Sentences must have meaning as a whole, they have to be understandable, without extra content disconnected from the rest of the message or abrupt interruptions.
        \item \textbf{Agreement}: Sentences must have the right gender and number agreement.
        \item \textbf{Conjugation}: Verbs have to be correctly conjugated.
    \end{enumerate}
    \item \textbf{Form}: Additional errors that can be used in case none of the above are applicable.
    \begin{enumerate}[label*=\arabic*.]
        \item \textbf{Repetitions} of the same word or phrase.
        \item \textbf{Misspelled words / uncommon typos}
        \item \textbf{Wrong punctuation marks}
        \item \textbf{Extra characters}
    \end{enumerate}
\end{enumerate}

\textbf{Ties are only contemplated in two situations}:
\begin{itemize}
    \item Two sentences that are \textbf{equally wrong}, that is to say, they are both either completely monolingual or unintelligible.
    \item Two sentences that are \textbf{exactly the same} and thus no criteria can be used to break the tie.
\end{itemize}

In case no criteria is applicable to a pair, we ask the annotators to choose their preferred sentence, using their own judgement o additional criteria they might observe in the specific pair of sentences.

\section{Error Typology}
\label{sec:appendix-error-typology}
\begin{enumerate}
    \item \textbf{CS errors}
    \begin{enumerate}[label*=\arabic*.]
        \item \textbf{No CS} - the sentence is entirely monolingual.
        \item \textbf{Unnatural CS} - the sentence contains unnatural CS, either due to unnatural switching points, or unnatural register.
        \item \textbf{Repetition in both languages} - the sentence contains the same information repeated in both languages, rather than CS.
    \end{enumerate}
    \item \textbf{Translation errors}
    \begin{enumerate}[label*=\arabic*.]
        \item \textbf{Made-up words} - the words in the output look like English or Spanish but do no actually exist.
        \item \textbf{Wrong translation } - the translation of a word or phrase is incorrect.
        \item \textbf{Wrong conjugation} - a verb is translated with the right lexeme but a seemingly made-up conjugation.
        \item \textbf{Wrong agreement} - there is a mistake in agreement in gender or number.
        \item \textbf{Wrong meaning} - a word or phrase has been translated into a sense that does not fit into the context.
        \item \textbf{Wrong order} - the words are right but they are written in the wrong order.
        \item \textbf{Wrong tense} - the verbal tense is not consistent through the sentence.
        \item \textbf{Unintelligible} - it is not possible to understand the sentence in English nor in Spanish.
        \item \textbf{Instruction misunderstanding} - the task has been misunderstood, e.g., the model makes a "comment" about the content of the output or explains a word. 
    \end{enumerate}
    \item \textbf{Format errors}
    \begin{enumerate}[label*=\arabic*.]
        \item \textbf{Extra words }- the sentence contains seemingly random extra words that do not affect its meaning.
        \item \textbf{Extra characters} - the sentence contains more non-word characters than the original, e.g., `???' instead of `??'.
        \item \textbf{Hallucinations} - the sentence contains new words or phrases not derived from the original text.
        \item \textbf{Start over} - the sentence is finalized, but the model begins a second translation of the same sentence.
        \item \textbf{Duplications} - some words or phrases of the sentence are duplicated.
        
        
    \end{enumerate}
\end{enumerate}
\end{document}
