@article{10.1145/3392846,
author = {Bawa, Anshul and Khadpe, Pranav and Joshi, Pratik and Bali, Kalika and Choudhury, Monojit},
title = {Do Multilingual Users Prefer Chat-bots that Code-mix? Let's Nudge and Find Out!},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {CSCW1},
url = {https://doi.org/10.1145/3392846},
doi = {10.1145/3392846},
abstract = {Despite their pervasiveness, current text-based conversational agents (chatbots) are predominantly monolingual, while users are often multilingual. It is well-known that multilingual users mix languages while interacting with others, as well as in their interactions with computer systems (such as query formulation in text-/voice-based search interfaces and digital assistants). Linguists refer to this phenomenon as code-mixing or code-switching. Do multilingual users also prefer chatbots that can respond in a code-mixed language over those which cannot? In order to inform the design of chatbots for multilingual users, we conduct a mixed-method user-study (N=91) where we examine how conversational agents, that code-mix and reciprocate the users' mixing choices over multiple conversation turns, are evaluated and perceived by bilingual users. We design a human-in-the-loop chatbot with two different code-mixing policies -- (a) always code-mix irrespective of user behavior, and (b) nudge with subtle code-mixed cues and reciprocate only if the user, in turn, code-mixes. These two are contrasted with a monolingual chatbot that never code-mixed. Users are asked to interact with the bots, and provide ratings on perceived naturalness and personal preference. They are also asked open-ended questions around what they (dis)liked about the bots. Analysis of the chat logs, users' ratings, and qualitative responses reveal that multilingual users strongly prefer chatbots that can code-mix. We find that self-reported language proficiency is the strongest predictor of user preferences. Compared to the Always code-mix policy, Nudging emerges as a low-risk low-gain policy which is equally acceptable to all users. Nudging as a policy is further supported by the observation that users who rate the code-mixing bot higher typically tend to reciprocate the language mixing pattern of the bot. These findings present a first step towards developing conversational systems that are more human-like and engaging by virtue of adapting to the users' linguistic style.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {may},
articleno = {41},
numpages = {23},
keywords = {multilingual interfaces, intelligent personal assistants, human-centered ai, human-agent interaction, code-mixing}
}

@article{2025-heredia-actitudes,
   author = "Heredia, Maite and Fernández Trinidad, Marianela and Jiménez-Bravo, Miguel",
   title = "Actitudes lingüísticas hacia el cambio de código entre valenciano y castellano",
   journal = "Revista Española de Lingüística Aplicada/Spanish Journal of Applied Linguistics",
   issn = "0213-2028",
   year = "2025",
   publisher = "John Benjamins",
   url = "https://www.jbe-platform.com/content/journals/10.1075/resla.22066.her",
   doi = "https://doi.org/10.1075/resla.22066.her",
   keywords = "actitudes lingüísticas",
   keywords = "Valencian",
   keywords = "bilingüismo",
   keywords = "bilingualism",
   keywords = "valenciano",
   keywords = "language attitudes",
   keywords = "code-switching",
   keywords = "cambio de código",
   keywords = "pares ocultos",
   keywords = "matched-guise",
   abstract = "Este estudio constituye un primer acercamiento a las actitudes lingüísticas hacia el cambio de código que se da en una comunidad bilingüe valenciano-castellano. Los fragmentos de audio utilizados como estímulos contenían muestras de habla espontánea obtenidas del programa televisivo À Punt e incluían casos de cambio de código inter- e intraoracional. Los estímulos fueron administrados a un total de 10 jueces mediante la técnica de pares ocultos para que valorasen mediante una encuesta la competencia lingüística de los hablantes. Los resultados mostraron juicios significativamente más negativos hacia los estímulos donde el hablante empleaba el cambio de código. Además, en un análisis más detallado, se observó que, en cada una de las cuatro preguntas realizadas, otras variables como la identidad lingüística de los jueces, su género y su nivel de estudios tuvieron un efecto en las valoraciones de estos hacia el cambio de código realizado por los hablantes."
}

@inproceedings{aguilar-etal-2020-lince,
    title = "{L}in{CE}: A Centralized Benchmark for Linguistic Code-switching Evaluation",
    author = "Aguilar, Gustavo  and
      Kar, Sudipta  and
      Solorio, Thamar",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Twelfth Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.223",
    pages = "1803--1813",
    abstract = "Recent trends in NLP research have raised an interest in linguistic code-switching (CS); modern approaches have been proposed to solve a wide range of NLP tasks on multiple language pairs. Unfortunately, these proposed methods are hardly generalizable to different code-switched languages. In addition, it is unclear whether a model architecture is applicable for a different task while still being compatible with the code-switching setting. This is mainly because of the lack of a centralized benchmark and the sparse corpora that researchers employ based on their specific needs and interests. To facilitate research in this direction, we propose a centralized benchmark for Linguistic Code-switching Evaluation (LinCE) that combines eleven corpora covering four different code-switched language pairs (i.e., Spanish-English, Nepali-English, Hindi-English, and Modern Standard Arabic-Egyptian Arabic) and four tasks (i.e., language identification, named entity recognition, part-of-speech tagging, and sentiment analysis). As part of the benchmark centralization effort, we provide an online platform where researchers can submit their results while comparing with others in real-time. In addition, we provide the scores of different popular models, including LSTM, ELMo, and multilingual BERT so that the NLP community can compare against state-of-the-art systems. LinCE is a continuous effort, and we will expand it with more low-resource languages and tasks.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

@inproceedings{bali-etal-2014-borrowing-social-media,
    title = "{\textquotedblleft}{I} am borrowing ya mixing ?'' An Analysis of {E}nglish-{H}indi Code Mixing in {F}acebook",
    author = "Bali, Kalika  and
      Sharma, Jatin  and
      Choudhury, Monojit  and
      Vyas, Yogarshi",
    editor = "Diab, Mona  and
      Hirschberg, Julia  and
      Fung, Pascale  and
      Solorio, Thamar",
    booktitle = "Proceedings of the First Workshop on Computational Approaches to Code Switching",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W14-3914/",
    doi = "10.3115/v1/W14-3914",
    pages = "116--126"
}

@article{dewaele-attitudes,
author = {Jean-Marc Dewaele and Li Wei},
title = {Attitudes towards code-switching among adult mono- and multilingual language users},
journal = {Journal of Multilingual and Multicultural Development},
volume = {35},
number = {3},
pages = {235--251},
year = {2014},
publisher = {Routledge},
doi = {10.1080/01434632.2013.859687},
URL = {  
        https://doi.org/10.1080/01434632.2013.859687
},
eprint = { 
        https://doi.org/10.1080/01434632.2013.859687
}}

@inproceedings{dogruoz-etal-2021-survey,
    title = "A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies",
    author = {Do{\u{g}}ru{\"o}z, A. Seza  and
      Sitaram, Sunayana  and
      Bullock, Barbara E.  and
      Toribio, Almeida Jacqueline},
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.131",
    doi = "10.18653/v1/2021.acl-long.131",
    pages = "1654--1666",
    abstract = "The analysis of data in which multiple languages are represented has gained popularity among computational linguists in recent years. So far, much of this research focuses mainly on the improvement of computational methods and largely ignores linguistic and social aspects of C-S discussed across a wide range of languages within the long-established literature in linguistics. To fill this gap, we offer a survey of code-switching (C-S) covering the literature in linguistics with a reflection on the key issues in language technologies. From the linguistic perspective, we provide an overview of structural and functional patterns of C-S focusing on the literature from European and Indian contexts as highly multilingual areas. From the language technologies perspective, we discuss how massive language models fail to represent diverse C-S types due to lack of appropriate training data, lack of robust evaluation benchmarks for C-S (across multilingual situations and types of C-S) and lack of end-to- end systems that cover sociolinguistic aspects of C-S as well. Our survey will be a step to- wards an outcome of mutual benefit for computational scientists and linguists with a shared interest in multilingualism and C-S.",
}

@inproceedings{gonen-goldberg-2019-language-modeling,
    title = "Language Modeling for Code-Switching: Evaluation, Integration of Monolingual Data, and Discriminative Training",
    author = "Gonen, Hila  and
      Goldberg, Yoav",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1427/",
    doi = "10.18653/v1/D19-1427",
    pages = "4175--4185",
    abstract = "We focus on the problem of language modeling for code-switched language, in the context of automatic speech recognition (ASR). Language modeling for code-switched language is challenging for (at least) three reasons: (1) lack of available large-scale code-switched data for training; (2) lack of a replicable evaluation setup that is ASR directed yet isolates language modeling performance from the other intricacies of the ASR system; and (3) the reliance on generative modeling. We tackle these three issues: we propose an ASR-motivated evaluation setup which is decoupled from an ASR system and the choice of vocabulary, and provide an evaluation dataset for English-Spanish code-switching. This setup lends itself to a discriminative training approach, which we demonstrate to work better than generative language modeling. Finally, we explore a variety of training protocols and verify the effectiveness of training with large amounts of monolingual data followed by fine-tuning with small amounts of code-switched data, for both the generative and discriminative cases."
}

@inproceedings{gregorius-okadome-2022-generating-dependency,
    title = "Generating Code-Switched Text from Monolingual Text with Dependency Tree",
    author = "Gregorius, Bryan  and
      Okadome, Takeshi",
    editor = "Parameswaran, Pradeesh  and
      Biggs, Jennifer  and
      Powers, David",
    booktitle = "Proceedings of the 20th Annual Workshop of the Australasian Language Technology Association",
    month = dec,
    year = "2022",
    address = "Adelaide, Australia",
    publisher = "Australasian Language Technology Association",
    url = "https://aclanthology.org/2022.alta-1.12",
    pages = "90--97",
}

@inproceedings{gupta-etal-2020-semi,
    title = "A Semi-supervised Approach to Generate the Code-Mixed Text using Pre-trained Encoder and Transfer Learning",
    author = "Gupta, Deepak  and
      Ekbal, Asif  and
      Bhattacharyya, Pushpak",
    editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.206",
    doi = "10.18653/v1/2020.findings-emnlp.206",
    pages = "2267--2280",
    abstract = "Code-mixing, the interleaving of two or more languages within a sentence or discourse is ubiquitous in multilingual societies. The lack of code-mixed training data is one of the major concerns for the development of end-to-end neural network-based models to be deployed for a variety of natural language processing (NLP) applications. A potential solution is to either manually create or crowd-source the code-mixed labelled data for the task at hand, but that requires much human efforts and often not feasible because of the language specific diversity in the code-mixed text. To circumvent the data scarcity issue, we propose an effective deep learning approach for automatically generating the code-mixed text from English to multiple languages without any parallel data. In order to train the neural network, we create synthetic code-mixed texts from the available parallel corpus by modelling various linguistic properties of code-mixing. Our codemixed text generator is built upon the encoder-decoder framework, where the encoder is augmented with the linguistic and task-agnostic features obtained from the transformer based language model. We also transfer the knowledge from a neural machine translation (NMT) to warm-start the training of code-mixed generator. Experimental results and in-depth analysis show the effectiveness of our proposed code-mixed text generation on eight diverse language pairs.",
}

@inproceedings{khanuja-etal-2020-gluecos,
    title = "{GLUEC}o{S}: An Evaluation Benchmark for Code-Switched {NLP}",
    author = "Khanuja, Simran  and
      Dandapat, Sandipan  and
      Srinivasan, Anirudh  and
      Sitaram, Sunayana  and
      Choudhury, Monojit",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.329",
    doi = "10.18653/v1/2020.acl-main.329",
    pages = "3575--3585",
    abstract = "Code-switching is the use of more than one language in the same conversation or utterance. Recently, multilingual contextual embedding models, trained on multiple monolingual corpora, have shown promising results on cross-lingual and multilingual tasks. We present an evaluation benchmark, GLUECoS, for code-switched languages, that spans several NLP tasks in English-Hindi and English-Spanish. Specifically, our evaluation benchmark includes Language Identification from text, POS tagging, Named Entity Recognition, Sentiment Analysis, Question Answering and a new task for code-switching, Natural Language Inference. We present results on all these tasks using cross-lingual word embedding models and multilingual models. In addition, we fine-tune multilingual models on artificially generated code-switched data. Although multilingual models perform significantly better than cross-lingual models, our results show that in most tasks, across both language pairs, multilingual models fine-tuned on code-switched data perform best, showing that multilingual models can be further optimized for code-switching tasks.",
}

@inproceedings{krishna-2017-synthesis,
  title={On Building Mixed Lingual Speech Synthesis Systems},
  author={Sai Krishna Rallabandi and Alan W. Black},
  booktitle={Interspeech},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:2564341}
}

@inproceedings{molina-etal-2016-overview-second-shared-task,
    title = "Overview for the Second Shared Task on Language Identification in Code-Switched Data",
    author = "Molina, Giovanni  and
      AlGhamdi, Fahad  and
      Ghoneim, Mahmoud  and
      Hawwari, Abdelati  and
      Rey-Villamizar, Nicolas  and
      Diab, Mona  and
      Solorio, Thamar",
    editor = "Diab, Mona  and
      Fung, Pascale  and
      Ghoneim, Mahmoud  and
      Hirschberg, Julia  and
      Solorio, Thamar",
    booktitle = "Proceedings of the Second Workshop on Computational Approaches to Code Switching",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W16-5805/",
    doi = "10.18653/v1/W16-5805",
    pages = "40--49"
}

@inproceedings{parikh-solorio-2021-normalization,
    title = "Normalization and Back-Transliteration for Code-Switched Data",
    author = "Parikh, Dwija  and
      Solorio, Thamar",
    editor = "Solorio, Thamar  and
      Chen, Shuguang  and
      Black, Alan W.  and
      Diab, Mona  and
      Sitaram, Sunayana  and
      Soto, Victor  and
      Yilmaz, Emre  and
      Srinivasan, Anirudh",
    booktitle = "Proceedings of the Fifth Workshop on Computational Approaches to Linguistic Code-Switching",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.calcs-1.15/",
    doi = "10.18653/v1/2021.calcs-1.15",
    pages = "119--124",
    abstract = "Code-switching is an omnipresent phenomenon in multilingual communities all around the world but remains a challenge for NLP systems due to the lack of proper data and processing techniques. Hindi-English code-switched text on social media is often transliterated to the Roman script which prevents from utilizing monolingual resources available in the native Devanagari script. In this paper, we propose a method to normalize and back-transliterate code-switched Hindi-English text. In addition, we present a grapheme-to-phoneme (G2P) conversion technique for romanized Hindi data. We also release a dataset of script-corrected Hindi-English code-switched sentences labeled for the named entity recognition and part-of-speech tagging tasks to facilitate further research."
}

@inproceedings{patwa-etal-2020-semeval,
    title = "{S}em{E}val-2020 Task 9: Overview of Sentiment Analysis of Code-Mixed Tweets",
    author = {Patwa, Parth  and
      Aguilar, Gustavo  and
      Kar, Sudipta  and
      Pandey, Suraj  and
      PYKL, Srinivas  and
      Gamb{\"a}ck, Bj{\"o}rn  and
      Chakraborty, Tanmoy  and
      Solorio, Thamar  and
      Das, Amitava},
    editor = "Herbelot, Aurelie  and
      Zhu, Xiaodan  and
      Palmer, Alexis  and
      Schneider, Nathan  and
      May, Jonathan  and
      Shutova, Ekaterina",
    booktitle = "Proceedings of the Fourteenth Workshop on Semantic Evaluation",
    month = dec,
    year = "2020",
    address = "Barcelona (online)",
    publisher = "International Committee for Computational Linguistics",
    url = "https://aclanthology.org/2020.semeval-1.100/",
    doi = "10.18653/v1/2020.semeval-1.100",
    pages = "774--790",
    abstract = "In this paper, we present the results of the SemEval-2020 Task 9 on Sentiment Analysis of Code-Mixed Tweets (SentiMix 2020). We also release and describe our Hinglish (Hindi-English)and Spanglish (Spanish-English) corpora annotated with word-level language identification and sentence-level sentiment labels. These corpora are comprised of 20K and 19K examples, respectively. The sentiment labels are - Positive, Negative, and Neutral. SentiMix attracted 89 submissions in total including 61 teams that participated in the Hinglish contest and 28 submitted systems to the Spanglish competition. The best performance achieved was 75.0{\%} F1 score for Hinglish and 80.6{\%} F1 for Spanglish. We observe that BERT-like models and ensemble methods are the most common and successful approaches among the participants."
}

@article{poplack-sometimes,
author = {Poplack, Shana},
year = {1980},
month = {01},
pages = {581-618},
title = {Sometimes I’ll start a sentence in Spanish Y TERMINO EN ESPAÑOL: toward a typology of code-switching 1},
volume = {18},
journal = {Linguistics},
doi = {10.1515/ling.1980.18.7-8.581}
}

@inproceedings{pratapa-etal-2018-language-modeling-synthetic,
    title = "Language Modeling for Code-Mixing: The Role of Linguistic Theory based Synthetic Data",
    author = "Pratapa, Adithya  and
      Bhat, Gayatri  and
      Choudhury, Monojit  and
      Sitaram, Sunayana  and
      Dandapat, Sandipan  and
      Bali, Kalika",
    editor = "Gurevych, Iryna  and
      Miyao, Yusuke",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1143/",
    doi = "10.18653/v1/P18-1143",
    pages = "1543--1553",
    abstract = "Training language models for Code-mixed (CM) language is known to be a difficult problem because of lack of data compounded by the increased confusability due to the presence of more than one language. We present a computational technique for creation of grammatically valid artificial CM data based on the Equivalence Constraint Theory. We show that when training examples are sampled appropriately from this synthetic data and presented in certain order (aka training curriculum) along with monolingual and real CM data, it can significantly reduce the perplexity of an RNN-based language model. We also show that randomly generated CM data does not help in decreasing the perplexity of the LMs."
}

@inproceedings{solorio-etal-2014-first-shared-task,
    title = "Overview for the First Shared Task on Language Identification in Code-Switched Data",
    author = "Solorio, Thamar  and
      Blair, Elizabeth  and
      Maharjan, Suraj  and
      Bethard, Steven  and
      Diab, Mona  and
      Ghoneim, Mahmoud  and
      Hawwari, Abdelati  and
      AlGhamdi, Fahad  and
      Hirschberg, Julia  and
      Chang, Alison  and
      Fung, Pascale",
    editor = "Diab, Mona  and
      Hirschberg, Julia  and
      Fung, Pascale  and
      Solorio, Thamar",
    booktitle = "Proceedings of the First Workshop on Computational Approaches to Code Switching",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W14-3907/",
    doi = "10.3115/v1/W14-3907",
    pages = "62--72"
}

@inproceedings{tarunesh-etal-2021-machine-translation,
    title = "From Machine Translation to Code-Switching: Generating High-Quality Code-Switched Text",
    author = "Tarunesh, Ishan  and
      Kumar, Syamantak  and
      Jyothi, Preethi",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.245/",
    doi = "10.18653/v1/2021.acl-long.245",
    pages = "3154--3169",
    abstract = "Generating code-switched text is a problem of growing interest, especially given the scarcity of corpora containing large volumes of real code-switched text. In this work, we adapt a state-of-the-art neural machine translation model to generate Hindi-English code-switched sentences starting from monolingual Hindi sentences. We outline a carefully designed curriculum of pretraining steps, including the use of synthetic code-switched text, that enable the model to generate high-quality code-switched text. Using text generated from our model as data augmentation, we show significant reductions in perplexity on a language modeling task, compared to using text from other generative models of CS text. We also show improvements using our text for a downstream code-switched natural language inference task. Our generated text is further subjected to a rigorous evaluation using a human evaluation study and a range of objective metrics, where we show performance comparable (and sometimes even superior) to code-switched text obtained via crowd workers who are native Hindi speakers."
}

@inproceedings{winata-etal-2023-decades,
    title = "The Decades Progress on Code-Switching Research in {NLP}: A Systematic Survey on Trends and Challenges",
    author = "Winata, Genta  and
      Aji, Alham Fikri  and
      Yong, Zheng Xin  and
      Solorio, Thamar",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.185",
    doi = "10.18653/v1/2023.findings-acl.185",
    pages = "2936--2978",
    abstract = "Code-Switching, a common phenomenon in written text and conversation, has been studied over decades by the natural language processing (NLP) research community. Initially, code-switching is intensively explored by leveraging linguistic theories and, currently, more machine-learning oriented approaches to develop models. We introduce a comprehensive systematic survey on code-switching research in natural language processing to understand the progress of the past decades and conceptualize the challenges and tasks on the code-switching topic. Finally, we summarize the trends and findings and conclude with a discussion for future direction and open questions for further investigation.",
}

@inproceedings{xu-yvon-2021-traducir,
    title = "Can You Traducir This? Machine Translation for Code-Switched Input",
    author = "Xu, Jitao  and
      Yvon, Fran{\c{c}}ois",
    editor = "Solorio, Thamar  and
      Chen, Shuguang  and
      Black, Alan W.  and
      Diab, Mona  and
      Sitaram, Sunayana  and
      Soto, Victor  and
      Yilmaz, Emre  and
      Srinivasan, Anirudh",
    booktitle = "Proceedings of the Fifth Workshop on Computational Approaches to Linguistic Code-Switching",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.calcs-1.11/",
    doi = "10.18653/v1/2021.calcs-1.11",
    pages = "84--94",
    abstract = "Code-Switching (CSW) is a common phenomenon that occurs in multilingual geographic or social contexts, which raises challenging problems for natural language processing tools. We focus here on Machine Translation (MT) of CSW texts, where we aim to simultaneously disentangle and translate the two mixed languages. Due to the lack of actual translated CSW data, we generate artificial training data from regular parallel texts. Experiments show this training strategy yields MT systems that surpass multilingual systems for code-switched texts. These results are confirmed in an alternative task aimed at providing contextual translations for a L2 writing assistant."
}

@inproceedings{yong-etal-2023-prompting,
    title = "Prompting Multilingual Large Language Models to Generate Code-Mixed Texts: The Case of South {E}ast {A}sian Languages",
    author = "Yong, Zheng Xin  and
      Zhang, Ruochen  and
      Forde, Jessica  and
      Wang, Skyler  and
      Subramonian, Arjun  and
      Lovenia, Holy  and
      Cahyawijaya, Samuel  and
      Winata, Genta  and
      Sutawika, Lintang  and
      Cruz, Jan Christian Blaise  and
      Tan, Yin Lin  and
      Phan, Long  and
      Phan, Long  and
      Garcia, Rowena  and
      Solorio, Thamar  and
      Aji, Alham Fikri",
    editor = "Winata, Genta  and
      Kar, Sudipta  and
      Zhukova, Marina  and
      Solorio, Thamar  and
      Diab, Mona  and
      Sitaram, Sunayana  and
      Choudhury, Monojit  and
      Bali, Kalika",
    booktitle = "Proceedings of the 6th Workshop on Computational Approaches to Linguistic Code-Switching",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.calcs-1.5/",
    pages = "43--63",
    abstract = "While code-mixing is a common linguistic practice in many parts of the world, collecting high-quality and low-cost code-mixed data remains a challenge for natural language processing (NLP) research. The recent proliferation of Large Language Models (LLMs) compels one to ask: how capable are these systems in generating code-mixed data? In this paper, we explore prompting multilingual LLMs in a zero-shot manner to generate code-mixed data for seven languages in South East Asia (SEA), namely Indonesian, Malay, Chinese, Tagalog, Vietnamese, Tamil, and Singlish. We find that publicly available multilingual instruction-tuned models such as BLOOMZ and Flan-T5-XXL are incapable of producing texts with phrases or clauses from different languages. ChatGPT exhibits inconsistent capabilities in generating code-mixed texts, wherein its per-formance varies depending on the prompt template and language pairing. For instance, ChatGPT generates fluent and natural Singlish texts (an English-based creole spoken in Singapore), but for English-Tamil language pair, the system mostly produces grammatically incorrect or semantically meaningless utterances. Furthermore, it may erroneously introduce languages not specified in the prompt. Based on our investigation, existing multilingual LLMs exhibit a wide range of proficiency in code-mixed data generation for SEA languages. As such, we advise against using LLMs in this context without extensive human checks."
}

