% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{*****} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2022}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Best Foot Forward: Robust Foot Reconstruction \textit{in-the-wild}}

\author{Kyle Fogarty\\
University of Cambridge\\
{\tt\small ktf25@cam.ac.uk}
\and
Jing Yang\\
University of Cambridge\\
{\tt\small jy496@cam.ac.uk}
\and
Chayan Kumar Patodi\\
Hike Medical\\
{\tt\small chayan@hikemedical.com}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Aadi Bhanti\\
Hike Medical\\
{\tt\small aadi@hikemedical.com}
\and 
Steven Chacko\\
Hike Medical\\
{\tt\small steven@hikemedical.com}
\and
Cengiz Öztireli\\
University of Cambridge\\
{\tt\small aco41@cam.ac.uk}
\and
Ujwal Bonde\\
Hike Medical\\
{\tt\small ujwal@hikemedical.com}
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
Accurate 3D foot reconstruction is crucial for personalized orthotics, digital healthcare, and virtual fittings. However, existing methods struggle with incomplete scans and anatomical variations, particularly in self-scanning scenarios where user mobility is limited, making it difficult to capture areas like the arch and heel. We present a novel end-to-end pipeline that refines Structure-from-Motion (SfM) reconstruction. It first resolves scan alignment ambiguities using SE(3) canonicalization with a viewpoint prediction module, then completes missing geometry through an attention-based network trained on synthetically augmented point clouds. Our approach achieves state-of-the-art performance on reconstruction metrics while preserving clinically validated anatomical fidelity. By combining synthetic training data with learned geometric priors, we enable robust foot reconstruction under real-world capture conditions, unlocking new opportunities for mobile-based 3D scanning in healthcare and retail. 
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
Custom foot orthotics are essential for treating and preventing foot-related medical conditions by improving overall biomechanics \cite{Lochner01012012}. Traditionally, they are manufactured using plaster casts and vacuum-forming, a costly and time-consuming process requiring in-person visits. Advances in digital scanning and additive manufacturing are transforming this field, enabling the creation of highly personalized orthotics that align with the principles of personalized medicine \cite{leite2019design}. 
Beyond orthotics, high-fidelity 3D scanning has broader applications in both healthcare (e.g., custom prosthetics) and digital applications (e.g., virtual try-ons, gaming).\\


\begin{figure*}
    \centering
     \includegraphics[width=\linewidth]{figures/hike_overview_figure.pdf}
     \caption{Challenges in foot self-scanning for individuals with reduced mobility: The image highlights the difficulty of capturing the complete foot geometry, especially the underside (red regions), which is harder to access; this limitation often leads to incomplete foot geometry.}
    \label{fig:Self-Scan}
\end{figure*}

\vspace{0.3cm}
Despite advances in human body reconstruction \cite{loper2023smpl,keller2023skin}, the foot remains largely unexplored due to its complex biomechanics, high morphological variance, and imaging challenges like plantar surface occlusion. To address this, we present a novel, high-quality foot reconstruction method using multi-view mobile phone images, offering a robust and accessible solution for clinical and commercial use.\\

\vspace{0.3cm}
Building on advances in 3D computer vision, Structure-from-Motion (SfM) enables 3D reconstruction from 2D image sequences, while Multi-View Stereo (MVS) enhances geometric detail under controlled conditions \cite{haming2010structure}. However, self-scanning the foot remains challenging—users struggle to capture dense, overlapping views due to limited mobility and awkward angles, leading to incomplete image coverage (see Fig. \ref{fig:Self-Scan}).
To address this, we make the following key contributions:
(1) The first foot completion network to refine incomplete scans, improving robustness and accuracy in foot reconstruction.
(2) A diverse foot dataset \texttt{Hike3D} with greater variation in attributes like age and height than previous datasets, enabling more robust modeling across different foot shapes.
(3) Seamless integration with template-based foot reconstruction methods to generate high-quality meshes from partial point clouds.
(4) A comprehensive evaluation showing our method outperforms COLMAP and state-of-the-art Gaussian splatting in robustness, feature accuracy, and surface quality.\\

\newpage
\noindent
\section{Related Work} 
Early attempts in foot modeling relied on Principal Component Analysis (PCA) \cite{amstutz2008pca}, but these models were simplistic, offering limited resolution and flexibility. Later approaches employ active sensor technologies, where structured light or depth cameras are used to generate point clouds \cite{lunscher2017point,yuan20213d,lochner2014development}. However, the point cloud geometries obtained from these sensors are often noisy and incomplete. More recently, Boyne et al. proposed the FIND model \cite{boyne2022find} leveraging a template deformation strategy guided by an implicit neural network to improve reconstruction accuracy. 
Similarly, Osman et al. \cite{osman2022supr} developed SUPR, a PCA-based human foot model designed for seamless integration with the SMPL full-body model \cite{loper2023smpl}, enabling expressive and anatomically consistent reconstructions. However, both FIND and SUPR are limited by their training data, which strongly constrains the shape space.
Our method, draws inspiration from multi-view reconstruction \cite{schoenberger2016sfm,schoenberger2016mvs,kazhdan2006poisson,kazhdan2013screened} and shape completion \cite{hu2019local,shen2012structure}, both of which have proven effective in broader 3D reconstruction tasks. By leveraging these advancements, our approach can be seamlessly integrated into existing works, providing a more robust and generalizable solution for foot reconstruction.






\section{Problem Setup}
We consider a set of unposed images of the foot, denoted as \( \mathcal{I} = \{I_1, I_2, \dots, I_N\} \), where each image \( I_i \in \mathbb{R}^{\mathrm{H} \times \mathrm{W} \times \mathrm{C}} \). Our objective is to reconstruct the complete geometry of the foot. To this end, we define a learnable function \( \mathcal{F}_\theta \) that maps the image set \( \mathcal{I} \) to a completed point cloud, such that $
\mathcal{P}_c = \mathcal{F}_\theta(\mathcal{I}).$
To effectively address this, we decompose \( \mathcal{F}_\theta \) into two composite functions $\mathcal{F}_\theta := \mathcal{D}_\theta \circ \mathcal{S},$ 
where \( \mathcal{S}: \mathcal{I} \to \mathbb{R}^{N \times 3} \) generates a dense, yet potentially incomplete, point cloud of the foot from the unposed images, and \( \mathcal{D}_\theta: \mathbb{R}^{N \times 3} \to \mathbb{R}^{M \times 3} \) maps this partial point cloud to the completed point cloud target \( \mathcal{P}_c \in \mathbb{R}^{M \times 3} \).\\



The challenge in learning $\mathcal{F}_\theta$ stems from supervision difficulties across inconsistent vector spaces. The geometric transformations between $\mathcal{D}_\theta$ and $\mathcal{S}$ remain unknown, creating constraints on pose and scale that complicate end-to-end system development. Our key insight addresses this by decomposing the problem into manageable sub-problems and leveraging synthetic training data at each stage. In the following section, we outline our method in more detail.




\section{Method}\label{sec:method}
We tackle complete foot reconstruction with a two-phase approach: first, we use Structure-from-Motion (SfM) and Multi-View Stereo (MVS) to estimate camera pose and generate an initial, though incomplete, point cloud; then, our shape completion module fills in missing geometry to produce a dense, complete representation. A naive approach to combining these two modules, estimating the geometric transform using iterative closest point (ICP) \cite{besl1992method}, often fails because the point clouds generated by SfM/MVS are typically incomplete. To overcome this, we introduce a viewpoint prediction (VPP) module, which provides a robust mechanism for estimating the transformation between the output of SfM/MVS and the expected input alignment for shape completion.\\

\newpage
In the following section, we outline the core components of our reconstruction pipeline, illustrated in Fig.~\ref{fig:reconstruction_pipeline} (a). The pipeline begins with two branches: {View-Point Prediction} (VPP) and {SfM \& MVS}, discussed in Sec.~\ref{subsec:vpp} and Sec.~\ref{subsec:sfm}, respectively. The VPP module {canonicalises} the recovered partial point cloud (Sec.~\ref{sec:canon}) before proceeding with {foot completion} and reconstruction (Sec.~\ref{sec:plccomplete}).




\subsection{View-point Prediction}\label{subsec:vpp}
The first branch of our architecture, the VPP module, estimates both a bounding box of the foot and the pose relative a predefined template mesh. Given an unposed image set $\mathcal{I}$ and a reference mesh $\mathcal{M}_{\mathrm{ref}}$, we train a neural network to regress the approximate six degrees of freedom (6-DoF) of the camera pose relative to $\mathcal{M}_{\mathrm{ref}}$. Our method builds on YOLO6D \cite{maji2024yolo}, adopting a similar training strategy and leveraging synthetic data; implementation details are in Sec.~\ref{implementation}. We represent the VPP module as $\mathcal{V}_{\phi}$ and define its output for a given image $\mathcal{I}_i \in \mathcal{I}$ as: $(\hat{\mathcal{C}}_i, B_i) = \mathcal{V}_{\phi}(\mathcal{I}_i),$ where $\hat{\mathcal{C}}_i$ denotes the estimated camera parameters, and $B_i$ represents the bounding box of the foot in the image.





\subsection{SfM \& MVS}\label{subsec:sfm}
We use a standard structure-from-motion (SfM) pipeline to estimate 3D structure by matching keypoints across views and jointly refining camera poses and a sparse point cloud via bundle adjustment.
% We employ a standard structure-from-motion (SfM) pipeline, which estimates 3D structure by matching keypoints across views and refining camera poses and a sparse point cloud via bundle adjustment, jointly across all images. 
Specifically, we utilize GLOMAP \cite{pan2024glomap}, which from our experiments, observed to give significantly more efficient and scalable global reconstruction compared to COLMAP \cite{schoenberger2016sfm,schoenberger2016mvs}. For the image-set $\mathcal{I}$, we model the SfM process as
$
\boldsymbol{\mathcal{C}} = \mathrm{SfM}(\mathcal{I}),
$
where each $\mathcal{C}_i \in \boldsymbol{\mathcal{C}}$ represents the estimated camera parameters of image $\mathcal{I}_i$. Using the bounding box $B_i$ from the VPP module, we generate segmentation masks via the Segment Anything Model 2 (SAM2) \cite{ravi2024sam2} in a zero-shot setting. Denoting the set of all bounding box centers as $\mathcal{B}$, we model the segmentation process as  
${\hat{\mathcal{I}}} = \mathrm{SAM}(\mathcal{I}, \mathcal{B}),$  
where $\hat{\mathcal{I}}_i \in {\hat{\mathcal{I}}}$ is the $i$-th masked image. 
To reconstruct a dense point cloud, we employ a multi-view stereo (MVS) approach~\cite{Hartley2004}, which estimates depth by matching pixel correspondences across multiple views and refining depth maps; in this work, We leverage the state-of-the-art MVSFormer++~\cite{cao2024mvsformer++} to recover high-quality point-clouds. Using the camera parameters from GLOMAP and segmentation masks from SAM2, we reconstruct the visible foot geometry pointcloud as $
\mathcal{P}_p = \mathrm{MVS}(\boldsymbol{\mathcal{C}}, {\hat{\mathcal{I}}}).
$
% To reconstruct a dense point cloud, we employ a multi-view stereo (MVS) approach \cite{Hartley2004}, leveraging the state-of-the-art MVSFormer++ \cite{cao2024mvsformer++} for high-quality point-cloud generation. 


\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/alg_larger.pdf}
    \caption{(a) An overview of our reconstruction pipeline, more details in Sec.~\ref{sec:method}.}
    %Given an unposed image set $\mathcal{I}$, our method runs two parallel branches: SfM for camera calibration and point cloud generation, and pose estimation. We then canonicalize the point cloud with $\mathcal{T} \in \mathrm{SE}(3)$, apply shape completion $\mathcal{F}_\theta$, and use Poisson reconstruction for the final mesh.}
    \label{fig:reconstruction_pipeline}
    \vspace{-4mm}
\end{figure*}

\subsection{Point Cloud Canonicalisation}\label{sec:canon}

Partial point-clouds recovered from image sets have arbitrary poses and scales, complicating their use in a downstream shape completion module. To address this, we transform the point clouds into a known canonical frame using the camera parameters $\hat{C}_i$ estimated by the VPP module and the depth maps estimated in the MVS process $\mathcal{D}_i$; we present our canonicalisation in Algorithm. 1.%\ref{ref:alg1}.


\subsection{Point Cloud Completion}\label{sec:plccomplete}

The second stage of our robust reconstruction pipeline focuses on completing the foot geometry using the learned function $\mathcal{D}_\theta ({\mathcal{P}})$. For this, we propose an attention-based point cloud completion framework that operates on partially reconstructed foot geometries from the SfM/MVS stage.\\

Building on recent attention-based approaches to point cloud modeling \cite{wang2024pointattn}, we formulate completion as an auto-encoding problem, where the model predicts a global latent representation to guide the reconstruction. Our attention mechanism aggregates information across the entire point cloud, capturing both local details and global structural patterns without relying on predefined neighborhood structures. We adopt a coarse-to-fine reconstruction strategy with a scaffold-based skip connection that directly integrates a subset of the input point cloud into the reconstruction process. This scaffold helps maintain fidelity to the observed geometry while enabling the model to infer missing regions effectively.\\

In our standard pipeline, we then employ the screened Poisson surface reconstruction (SPSR) algorithm \cite{kazhdan2013screened} to generate a mesh, using normals estimated via a $k$-nearest neighbors approach to ensure a smooth and consistent surface.\\

% In the next section we provide an overview of the implementation of our method. 



\section{Implementation}\label{implementation}

% In this section we provide details of the implementation and training procedure used to construct our reconstruction pipeline. 


\noindent
\textbf{Datasets}: High-fidelity foot geometry datasets are scarce.  Foot3D \cite{boyne2022find} is a valuable resource, but its narrow age and height range prompted us to develop \texttt{Hike3D}, a more diverse dataset for orthotics research. To broaden demographic coverage and strengthen design robustness, we integrate \texttt{Hike3D} with Foot3D. We release \texttt{Hike3D} as part of this work.\\

% \subsection{Training Procedure}


\noindent
\textbf{VPP module}: We train the VPP module using synthetically generated images of meshes from our training set. To ensure diversity, we utilize 740 HDR backgrounds, creating various background combinations for our 50k synthetic images. We then fine-tune the model using 5k real images. Throughout the process, we apply the same augmentations and loss functions as in the original work \cite{maji2024yolo}.\\

\noindent
\textbf{Foot Completion Module}: We train the foot completion module using a simulated scanning setup to generate paired partial and complete geometries. To improve robustness against noise and SE(3) perturbations, we apply data augmentations during training. Our dataset combines \texttt{Hike3D} and Foot3D, with a 1:8 training-to-test split. Each mesh underwent 10 spatial transformations (shifts, scaling, rotations), followed by five virtual scans per transformation, yielding 2000 training and 250 testing pairs. Supervision is applied by minimizing the Chamfer distance between predicted and ground-truth point clouds at intermediate steps of the network.



\section{Experiments}
To evaluate robustness, we conduct two experiments: one using paired videos and high-quality 3D scans for quantitative error analysis, and another using video-only data, where clinicians score reconstructions based on visual assessment.

\subsection{Experimental Setup}

To evaluate our foot completion module, we fit three established foot models to incomplete and completed scans. The first, a PCA-based method~\cite{amstutz2008pca}, uses functional maps~\cite{ovsjanikov2012functional,melzi2019zoomout} for vertex correspondences and fits a PCA model to mesh displacement vectors. The second leverages the SUPR foot model~\cite{osman2022supr}, while the third, FIND~\cite{boyne2022find}, offers a large latent space for shape and pose control, with added transformation parameters for better alignment. For all methods, we optimize shape, pose, and transformation parameters via gradient descent, minimizing Chamfer distance with the Adam optimizer~\cite{kingma2014adam}. Final accuracy is assessed using Chamfer and Hausdorff distances.\\

We evaluate our method in an end-to-end reconstruction setting using unposed video images, benchmarking against two widely used pipelines: (1) COLMAP \cite{schoenberger2016sfm}, which reconstructs 3D geometry via SfM and MVS, and (2) Gaussian Opacity Fields~\cite{yu2024gaussian}, a state-of-the-art differentiable rendering method that optimizes 3D Gaussians from image observations to generate a mesh. Using 30 consumer-captured videos with varying conditions, three expert clinicians assessed randomized renders from each baseline and our method. They rated reconstructions on a 5-point scale for (1) anatomical accuracy, (2) completeness, and (3) realism.  
% We further assess our method in an end-to-end reconstruction setting using only unposed video images. We benchmark against two widely used mesh-reconstruction pipelines: (1) COLMAP~\cite{schoenberger2016sfm}, which reconstructs 3D geometry via structure-from-motion (SfM) and multi-view stereo (MVS) to generate a dense point cloud for meshing, and (2) Gaussian Opacity Fields \cite{yu2024gaussian}, a state-of-the-art differentiable rendering method that optimizes 3D Gaussians to represent the scene directly from image observations.
% We use 30 videos captured in consumer settings with varying lighting and environmental conditions. For evaluation, three expert clinicians in foot anatomy and orthotics reviewed randomized renders from each baseline and our method, alongside reference foot images. They rated reconstructions on a 5-point scale across three criteria: (1) anatomical accuracy, (2) completeness, and (3) smoothness \& realism.
\subsection{Experimental Results}

\noindent
\textbf{Point Completion}: Table \ref{tab:fitting_methods} demonstrates that our point cloud-based method consistently outperforms template-based approaches on incomplete point clouds, yielding lower Chamfer and Hausdorff distances. Furthermore, meshes reconstructed through our pipeline, when meshed using SPSR, achieve the lowest Chamfer distances among all meshed, further validating our design choice. 
Qualitative results in Figure \ref{fig:7} further illustrate these improvements.\\

% \vspace{0.3cm}
\begin{figure*}[ht]
  \centering
  \includegraphics[width=\textwidth]{figures/compare_fig-7.pdf}
  \caption{Figure shows partial scan reconstruction results. Methods marked $*$ are optimized on the input scan, while $\dagger$ denotes optimization on our completed point cloud, which recovers geometry much closer to the reference scans.
}
    % \vspace{-4mm}
  \label{fig:7}
\end{figure*}
\begin{table}[]
  \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}lcc}
    \toprule
    \textsc{Method} & CD ($\downarrow$)~($10^{-2}$) & HD ($\downarrow$)~($10^{-2}$)\\
    \midrule
    PCA & $4.46 \pm 1.24$ & $12.28 \pm 3.26$ \\
    SUPR & $12.74 \pm 3.78$ & $34.61 \pm 7.91$ \\
    FIND & $15.95 \pm 6.11$ & $37.19 \pm 14.36$ \\
    \midrule
    Ours & $2.29 \pm 0.56$ & $9.51 \pm 3.76$ \\
    \midrule
    SPSR + Ours& $2.81 \pm 0.77$& $10.20 \pm 3.13$ \\
    PCA + Ours & $3.93 \pm 1.08$ & $11.37 \pm 3.02$ \\
    SUPR + Ours & $7.08 \pm 1.79$ & $27.95 \pm 5.43$ \\
    FIND + Ours & $3.46 \pm 1.26$ & $10.05 \pm 3.50$ \\
    \bottomrule
  \end{tabular*}
  \caption{Here we present the average chamfer distance (CD) and Haussdorf distance (HD). 
    The quoted plus/minus range refers to 1 standard deviation over the test dataset.}
    \label{tab:fitting_methods}
\end{table}
% \begin{figure*}[ht]
% \centering
% \begin{minipage}[t]{0.48\textwidth}
%   \vspace{0pt}%
%   % Your table
%   \centering

% \end{minipage}
% \hfill
% \begin{minipage}[t]{0.44\textwidth}
%   \vspace{0pt}%
%   % Your figure
%   \centering
%   \includegraphics[width=\linewidth]{figures/CD_and_Angle1.pdf}
%   \captionof{figure}{Chamfer Distance between predicted foot mesh and ground truth vs. camera scanning angle.}
%   \label{fig:chamfer_dist_vs_angle}
% \end{minipage}
% \end{figure*}
% % \vspace{-4mm}

\begin{figure}
    \centering
  \includegraphics[width=0.75\linewidth]{figures/CD_and_Angle1.pdf}
  \caption{Chamfer Distance between predicted foot mesh and ground truth vs. camera scanning angle.}
    \label{fig:chamfer_dist_vs_angle}
\end{figure}
\begin{figure*}[ht]
  \centering
  \includegraphics[width=\textwidth]{figures/clinical_final.pdf}
  \caption{Plots of the distribution of aggregated clinical scores for each methods assessing (a) anatomical accuracy (b) completeness (c) surface quality.}
    % \vspace{-4mm}
  \label{fig:6}
\end{figure*}
% \newpage

\noindent
\textbf{View-Completeness:} We evaluate our shape completion module under varying levels of partialness using a simulated scanning setup. By limiting the maximum angle between the camera-to-foot vector, we control foot coverage—smaller angles mean more missing data. As shown in Fig.~\ref{fig:chamfer_dist_vs_angle}, increasing this angle lowers the Chamfer Distance, improving reconstruction accuracy. Notably, the error plateaus around 90$^{\circ}$, a feasible range for a person performing self-scanning, indicating that this range balances practicality and accuracy, further validating our design choice for a robust completion module.\\

.
\newpage
\noindent
\textbf{End-to-End Pipeline}: We present our results in Fig.~\ref{fig:6} (a)–(c). Our method consistently outperforms COLMAP and GOF across all metrics. COLMAP shows the worst anatomical accuracy with major deviations, while GOF is inconsistent. Our method achieves the highest, most consistent ratings in fidelity, completeness, and surface quality. These results confirm its suitability for clinical applications like foot orthotic design and precision insole manufacturing.

% \vspace{-0.1cm}
\section{Discussion}
Our results demonstrate that our end-to-end pipeline significantly improves foot geometry reconstruction, achieving lower Chamfer and Hausdorff distances while maintaining consistency with input data. The foot completion module, leveraging learned priors, successfully reconstructs plausible geometries from sparse data, addressing the limitations of template-based methods, which showed constrained shape variability in our evaluations. Our approach enables robust reconstruction across diverse and incomplete inputs, as reflected in its consistently high surface completeness scores. Furthermore, by integrating completion and canonicalization, our method effectively mitigates occlusions and partial view challenges, leading to more accurate and reliable reconstructions, as evidenced by its superior anatomical fidelity and quality ratings. While we have shown our model performs well across a diverse range of test cases, it does lack explicit uncertainty quantification for extreme out-of-distribution foot geometry; we will seek to address this in future work.


% 


\section{Conclusion}

We introduced a novel end-to-end pipeline for reconstructing foot geometry from self-scanned mobile videos, addressing key limitations of existing methods. Our proposed method provides robust foot reconstruction, even from partial observation. Extensive evaluation demonstrated that our method outperforms baseline approaches, achieving lower Chamfer and Hausdorff distances while preserving consistency with input geometry. These findings underscore the effectiveness and robustness of our approach, particularly for self-scanning applications, paving the way for improved foot reconstruction in real-world settings.  

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
