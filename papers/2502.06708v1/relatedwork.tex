\section{Related Work}
%In this section, we discuss the prior work related to the paper covers video analysis, activity recognition, and surgical skill assessment in surgical videos. There is substantial evidence from within the field of surgery and in a wide range of high-performance ``complex task-based'' fields that technical fine-tuning of tasks can improve performance with a detailed assessment and constructive feedback. 

%\subsection*{Video Based Analysis (VBA)}

%VBA involves breaking down and examining video content to extract important insights \cite{huber2020video}. It aims to understand, evaluate, and derive inferences from videos by transforming visual streams into semantically meaningful representations that can be eaily analysed at scale \cite{liu2009encyclopedia}.

%Real-time VBA is expected to be crucial for surgical procedures \cite{mascagni2022computer}. Surgical video analysis, widely based on VBA, provides a more accurate understanding of the surgical process by thoroughly examining several areas, including procedure phase and step segmentation, performance analysis, and skill assessment \cite{feldman2020sages}. Analysing videos of surgeries and the operating room (OR) can significantly enhance surgical care, especially for minimally invasive techniques, which are becoming increasingly popular worldwide \cite{lewandrowski2020regional,richards2015national}. This analysis can provide surgeons with context-aware intra-operative decision support using AI computational models, which can accurately and swiftly extract knowledge from real-time video data \cite{vercauteren2019cai4cai,nwoye2020recognition}.

%Additionally, VBA is a valuable tool for assessing surgical skills, providing formative feedback, and evaluating operating performance \cite{mascagni2022computer}. Given its importance, surgical video analysis techniques have been automated using surgical data science. Various studies have utilised AI models to analyse surgical videos for recognising surgical phases \cite{ramesh2021multi}, tools \cite{hashimoto2018artificial,nwoye2019weakly}, and actions \cite{nwoye2020recognition}. This helps with multiple applications, such as calculating surgery duration \cite{twinanda2018rsdnet}, automatically recording important events \cite{mascagni2022computer}, assessing surgical skills \cite{lavanchy2021automation}, and offering intraoperative assistance \cite{aspart2022clipassistnet}.

% Video analysis is an approach that involves breaking down and examining video content to extract important insights \cite{huber2020video}. Video analysis aims to understand, evaluate, and derive inferences from the videos. It uses approaches to transform visual streams into semantically meaningful representations \cite{liu2009encyclopedia}.
% Real-time video analysis is expected to be very important for surgical procedures \cite{mascagni2022computer}. Surgical video analysis is widely based on VBA which provides a more accurate understanding of the surgical process since it thoroughly examines several areas, including procedure phase and step segmentation, performance analysis, and skill assessment\cite{feldman2020sages}. Analyzing videos of surgeries and the operating room (OR) can aid in enhancing surgical care, especially for procedures conducted using minimally invasive techniques, which are becoming increasingly popular worldwide \cite{lewandrowski2020regional,richards2015national}. This can provide surgeons with context-aware intra-operative decision support using AI computational models. These models can accurately and swiftly extract knowledge from video data captured in real-time \cite{vercauteren2019cai4cai,nwoye2020recognition}. Additionally, VBA is a way to assessÂ surgical skills, provide formative feedback, and evaluate operating performance \cite{mascagni2022computer}. Due to the importance of surgical video analysis, the techniques of surgical data science have been used to automate the field. In various studies, AI models have been used to analyse surgical videos for the recognition of surgical phases \cite{ramesh2021multi}, tools \cite{hashimoto2018artificial,nwoye2019weakly}, and actions \cite{nwoye2020recognition}. It helps various applications, such as calculating surgery duration \cite{twinanda2018rsdnet}, automatically recording important events \cite{mascagni2022computer}, surgical skill assessment \cite{lavanchy2021automation}, and offering interoperate assistance \cite{aspart2022clipassistnet}.

%\subsection*{Activity Recognition}

%Minimal-invasive surgery has improved patient outcomes and lends itself easily to the digital acquisition of video files. This has led to the development of novel teaching strategies \cite{bjerrum2018surgical}. Many methods have been used in the field of surgical activity recognition, such as bag of words (BoW) models and linear dynamical systems (LDS) \cite{zappella2013surgical}. The use of deep learning has been crucial to recent developments in surgical activity recognition \cite{ahmidi2017dataset,dipietro2019segmenting}. However, these methods have the drawback of requiring a massive amount of annotated data for training \cite{paysan2021self}.  


%In surgical procedures, activity/phase recognition is the most common task \cite{nwoye2023cholectriplet2022}. The recognition of the surgical phase is to break down a surgical operation into distinct parts, each of which represents a particular task that the surgeon must complete before moving to the next phase \cite{kadkhodamohammadi2021towards}. Numerous techniques have been employed to identify the surgical phase, i.e., data from sensors on tool tracking systems \cite{holden2014feasibility}, binary signals from instrument usage \cite{padoy2012statistical}, and surgical robots \cite{lin2005automatic}. However, obtaining these signals typically requires the installation of extra hardware or time-consuming manual annotation, which could increase the work associated with the surgical process \cite{dergachyova2016automatic}. Consequently, recent studies have focused on deriving the workflow solely from routinely collected video data during surgery \cite{jin2017sv}. In addition to its ability to eliminate the need for additional equipment, automatic workflow recognition from surgical videos helps evaluate surgical skills \cite{blum2010modeling}. 


%Several attempts have been made to recognise the surgical phase. A study in \cite{twinanda2016endonet} described a unique method for phase recognition that uses only visual information and a convolutional neural network (CNN) to automatically learn features from videos of cholecystectomy procedures.  They presented EndoNet, an advanced CNN architecture designed to recognise the surgical phase. Other groups \cite{ramesh2021multi} have focused on using a temporal CNN model for phase recognition in surgical activity. They asserted that the model is a temporal convolutional network with several tasks effectively implemented for combined online phase and step recognition. Gao et al.\cite{gao2021trans} proposed a hybrid embedding aggregation transformer to accurately recognise phases in surgical videos. Alternatively, an AI model to enhance surgical phase recognition was proposed by Funke et al. \cite{funke2023tunes}. They described a temporal model TUNeS, to maximise attention utilization. The technique integrates self-attention into a convolutional U-Net architecture for improved performance.

%\subsection*{Surgical Skill Assessment}

%Surgery is dependent on an operator's skills to manipulate surgical instruments to interact with part or the whole of a human organ. This leads to a specific intended outcome, for example the dissection of a large cancerous or pre-cancerous polyp from the rectal wall \cite{fecso2017effect}. Training and assessment of these skills in a targeted manner can positively impact patient outcomes by enabling surgeons to practice or review their surgical actions \cite{nataraja2018simulation}. To maintain patient safety and minimize adverse clinical events, critical evaluation of surgical skills and constructive feedback have become essential components of surgical training \cite{reznick1993teaching}. A recent study indicates that over 25\% of the variance in patient outcomes can be attributed to differences in the level of surgical skills among practising surgeons \cite{stulberg2020association}. Therefore, to enhance patient outcomes, it is crucial to consistently provide objective feedback on surgeons' technical performance to aid their skill development \cite{lavanchy2021automation}. Performance improvement in any specialised field is greatly enhanced with a structured and constructive feedback mechanism in a controlled environment. This is analogous to the positive impact of coaching in technically challenging sports, for example squash, where focused feedback over a prolonged period can significantly improve on-court performance and rankings.


%Surgical skill assessment also requires the correct identification of a particular surgical phase as different phases may be undertaken at different levels of proficiency. 

%\subsection*{Surgical Timeline Segmentation}
%Surgeries can be generally understood as the composition of instruments and actions involved in sequences of different anatomical activities \cite{zhou2021towards}. Manual understanding of surgical procedures is an uphill task involving careful observations of the surgical activities. For this reason, understanding intricate anatomical processes requires automatic surgical workflow analysis.
%Recognizing surgical workflow is essential to the design of intelligent systems in the operating room\cite{padoy2019machine}. In particular, workflow recognition enables these systems to provide context-aware decision support, monitor and optimise surgical operations, and provide early alerts of possible deviations and anomalies\cite{huaulme2020offline}. The overall workflow can be recognised by understanding the surgical scene. 
%A study presented holistic scene understanding for recognizing phases, steps, actions and instruments in endoscopic surgical videos \cite{valderrama2022towards}. The author of this study created a PSI-AVA Dataset, offering comprehensive annotations for surgical scene understanding in prostatectomy videos. They introduced Transformers for Action,Phase, Instrument, and Steps Recognition (TAPIR), a robust baseline that enhances classification by leveraging multi-level dataset annotations. Another study presented a model based on Transformers for Actions, Phases, Steps, and Instrument Segmentation (TAPIS)\cite{ayobi2024pixel}. They aim to facilitate multilevel comprehension of surgical activities including long-term tasks like phase and step recognition, and short-term tasks like instrument segmentation and atomic action detection.