\documentclass[fleqn,10pt]{wlscirep}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lineno}
\usepackage{placeins}
\usepackage{subcaption}
\usepackage{cleveref}
\usepackage{soul}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{algorithm2e}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{tikz}
\usepackage{adjustbox}
\usepackage{pdflscape}
\usepackage{url}
\usepackage{float}
\usepackage{array}
\usepackage{multirow}
\usepackage[table,xcdraw]{xcolor}
\usepackage{colortbl} 
\usepackage[table,xcdraw]{xcolor}
\usepackage[backgroundcolor=softPeach]{todonotes}
\usepackage[table,xcdraw]{xcolor}
\usepackage{tabularx}
\usepackage{geometry}
\def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;} 
\geometry{a4paper, margin=1in}

% \algnewcommand\algorithmicinput{\textbf{Input:}}
% \algnewcommand\Input{\item[\algorithmicinput]}
% \algnewcommand\algorithmicoutput{\textbf{Output:}}
% \algnewcommand\Output{\item[\algorithmicoutput]}

\newcommand\duration{\mathrm{duration}}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{softPeach}{RGB}{255, 200, 180}

\lstdefinestyle{mystyle}{ backgroundcolor=\color{backcolour},    commentstyle=\color{codegreen}, keywordstyle=\color{magenta}, numberstyle=\tiny\color{codegray}, stringstyle=\color{codepurple}, basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,  
    breaklines=true,          
    captionpos=b,             
    keepspaces=true,          
    numbers=left,             
    numbersep=5pt,            
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=mystyle}

\linenumbers

\title{Endoscopic Video Dataset For Automated Surgical Timeline Segmentation Using Computational Artificial Intelligence Techniques}
\title{Data Descriptor Title (110 characters maximum, inc. spaces)}
\title{Endoscopic Video Dataset: Advancing Computational AI for Surgical Scene Analysis in Colorectal Cancer Operations}
\title{Hierarchical Surgical Timeline Segmentation Dataset of Surgical Videos: automated indexing of Video-Based Assessments}
\title{EndoSTG-Dataset: Hierarchical Surgical Timeline Segmentation and Indexing of Transanal Endoscopic Microsurgery Videos}
\title{TEMS-STG: Transanal Endoscopic Microsurgery Videos Dataset for Hierarchical Surgical Timeline Generation and Automatic Indexing of Video-Based Assessments}
\title{DATEM: A Novel Densely Annotated TEMS Dataset for Indexing Large Surgical Videos via Timeline Segmentation}
\title{TEMSET-50K: A Novel Densely Annotated Dataset for Indexing Large Surgical Videos via Spatiotemporal Timeline Segmentation}
\title{TEMSET-25K: Densely Annotated Dataset for Indexing Multipart Endoscopic Videos using Surgical Timeline Segmentation}

\author[1,*,$\dag$]{Muhammad Bilal}
\author[1,2,3$\dag$]{Shazad Ashraf}
\author[1]{Mahmood Alam}
\author[2]{Deepa Bapu}
\author[2]{Stephan Korsgen}
\author[2]{Muhammed Ali}
\author[2,3]{Simon Bach}
\author[3]{Amir M Hajivanand}
\author[4]{Kamran Soomro}
\author[2]{Neeraj Lal}
\author[5]{Iqbal Qasim}
\author[4]{Pawel Capik}
\author[4]{Taofeek Akinosho}
\author[4]{Zaheer Khan}
\author[6]{Hunaid Vohra}
\author[6]{Massimo Caputo}
\author[2,3]{Andrew Beggs}
\author[7]{Adnan Qayyum}
\author[7]{Junaid Qadir}
\affil[1]{Birmingham City University, United Kingdom}
\affil[2]{University Hospitals Birmingham, Birmingham}
\affil[3]{University of Birmingham, Birmingham}
\affil[4]{University of the West of England, Bristol, United Kingdom}
\affil[5]{University of Hertfordshire, United Kingdom}
\affil[6]{University of Bristol, United Kingdom}
\affil[7]{Qatar University, Doha, Qatar}

\affil[*]{corresponding author(s): Muhammad Bilal (muhammad.bilal@bcu.ac.uk); Shazad Ashraf (S.ashraf.2@bham.ac.uk)}

\affil[$\dag$]{these authors contributed equally to this work}


\begin{abstract}
In surgical data science, indexing endoscopic surgical videos is a high-value task that lays the foundation for critical evaluation of intra-operative events. A validated process would encourage systematic retrospective analysis, enabling honest clinical performance evaluation, technique refinement, audit, and knowledge dissemination. Currently, video-based analytics and indexing is done manually, which can be a time-consuming and laborious process. Recent advances in computer vision techniques, particularly deep learning (DL)-based methods, have the potential to reliably automate surgical video indexing and simplify the review process. However, the scarcity of publicly available, well-curated, and densely annotated surgical video datasets hinders the field of surgical data science and the development of novel computational methodologies using state-of-the-art (SOTA) techniques. To address this challenge, we propose a novel open-source dataset, TEMSET-25K, which has been carefully curated, processed, and densely annotated by clinical domain experts. 

\vspace{2mm}

Our dataset comprises 25,835 trans-anal endoscopic microsurgery (TEMS) video micro-clips that have been annotated using our novel hierarchical labelling taxonomy, seamlessly capturing the nuances of surgical workflow through "phase, task, and action" triplets. To demonstrate the efficacy of our proposed dataset and labelling hierarchy, we benchmark it using DL-based SOTA vision models, including transformers-based architectures. 

\vspace{2mm}
Our experimental \textit{in-silico} evaluation reveals that our proposed deep labeling hierarchy can be efficiently learned for the tasks of surgical timeline segmentation, which is considered a critical component in surgical scene understanding. 
The STALNet model (with three distinct encoders: ConvNeXt, ViT and SWIN V2) was evaluated for its performance on each of the taxonomy triplets against human annotators in an inter-observer-variability (IOV) study. Identification of "Set-Up", "Landmarking", “Dissection” and "Suturing" phases achieved an accuracy of $0.99 ± 0.10$, $0.99 ± 0.07$, $0.98 ± 0.14$ and $0.99 ± 0.09$, respectively. The  F1 score for each of these phases were $0.92 ± 0.08$, $0.97 ± 0.03$, $0.97 ± 0.04$ and $0.99 ± 0.01$ respectively. Even for phases with fewer instances, such as "Closure" phases, the model maintained high F1 score and accuracy with the SWIN V2 encoder. 

\vspace{2mm}

These results indicate that the model effectively captures and segments well-represented phases consistently across different encoders but struggles with actions that are less common in the training dataset. We believe that this dataset will help bridge the crucial gap between AI advancements and surgical data science by serving as the benchmark for developing and evaluating SOTA solutions for innovating surgical data science applications.

% Indexing endoscopic videos is essential in minimally invasive surgery (MIS), enabling efficient retrospective analysis for performance evaluation, technique refinement, service audits, and knowledge dissemination. Video-based analytics (VBA) can be a time-consuming and laborious task. Recent advancements in AI have the potential to significantly enhance the process of reviewing surgical videos, creating practical opportunities for the digitisation of various interventional healthcare tasks. However, the scarcity of publicly available, curated, and densely annotated surgical video datasets hampers objective comparisons of surgical data science solutions and the development of novel computational methodologies. This scarcity primarily arises from the complexities of managing large surgical video datasets and the limited availability of expert surgeons for precise video annotation due to time constraints and high costs. To address this challenge, we developed a comprehensive methodology for curating, labelling, and training robust AI models to conduct an in-depth analysis of novel endo-luminal videos of colorectal cancer surgery. This comprised trans-anal endoscopic microsurgery (TEMS) operations. We propose a hierarchical labelling taxonomy capturing the nuances of surgical workflow through phase, task, and action triplets. We present a dataset comprising 50,000 endoscopic video 5-second cut-outs from trans-anal surgical procedures, meticulously annotated by clinical experts. This dataset has proven effective for training robust models in surgical timeline segmentation, a critical component of surgical scene understanding. This dataset will serve as a benchmark for the development and evaluation of new and existing AI techniques for automated scene understanding, fostering digital innovation in clinical surgical practice.

% Endoscopic video acquisition is essential in minimally invasive surgery (MIS), facilitating retrospective analysis for performance evaluation, technique refinement, and knowledge dissemination among surgical practitioners. Recent advancements in AI hold promise for enhancing video editing and reviewing processes. This also offers practical avenues to digitise the assessment of surgical procedures to standardise and enhance clinical outcomes. However, the scarcity of publicly available, curated, and annotated surgical video datasets poses challenges such as objective assessment and the ability to compare surgical intelligence tools for developing novel computational methodologies. These limitations stem primarily from the management complexities of large surgical video datasets and, crucially, from the limited availability of expert surgeons for dataset labelling due to the time constraints and costs of high-quality and precise video annotation. Addressing this challenge, we developed an easy workflow to undertake an in-depth analysis of novel endo-luminal video datasets of colorectal cancer surgery. Our dataset comprises ten endoscopic recordings of trans-anal surgical procedures, annotated by clinical experts for two specific downstream machine-learning tasks: surgical timeline segmentation and scene understanding. This dataset will serve as a benchmark for developing and evaluating new and existing AI techniques for automated surgical video analysis, fostering digital innovation within clinical surgical practice.


\end{abstract}
\begin{document}

\flushbottom
\maketitle
%  Click the title above to edit the author information and abstract
\thispagestyle{empty}

% \noindent Please note: Abbreviations should be introduced at the first mention in the main text – no abbreviations lists or tables should be included. Structure of the main text is provided below.

% \section*{Background \& Summary}
\section*{Introduction}
Over 300 million surgical procedures are performed worldwide annually \cite{weiser2015estimate}. While surgery is a crucial healthcare intervention, it also carries significant risks, with surgical complications currently ranking as the third leading cause of global mortality \cite{nepogodiev2019global}. Surgical adverse events result in major quality-of-life (QoL) issues for patients, and methods that critically evaluate intra-operative events have significant potential to drive up surgical standards and reduce morbidity. A key method for enhancing surgical standards involves the use of high-resolution endoscopic surgical videos (ESV). These videos capture minimally invasive surgeries (MIS) with high-definition visual records at 60 frames per second, producing two simultaneous full HD streams. This results in over 50GB of data for a single uncompressed video, with even greater volumes as surgeries lengthen, or 4K resolution technology is adopted. This poses significant challenges when attempting to create adequate storage capacity in Secure Digital Environments (SDEs), such as those recently implemented by the National Health Service (NHS), UK \cite{SDE}. Despite the storage and energy costs, the value of ESV files in capturing surgical details is significant, especially at scale. Reduction in storage requirements without loss of vital information will inevitably lead to significant energy and cost savings in line with the plan to reduce the NHS Carbon Footprint to zero by 2040 \cite{Green}. 

\vspace{2mm}
Apart from the storage and management challenges, another major stumbling block hindering surgical scene understanding is the lack of richly annotated, comprehensive datasets. A meticulously assembled large dataset is invaluable for training machine learning models to recognise objects like instruments and anatomical structures in the surgical field of view and to understand procedural phases, tasks, and intra-operative actions. Such capabilities in scene synthesis, facilitated by automated algorithms, are vital for elucidating the intricacies of surgical workflows \cite{maier2017surgical} and evaluating surgeon performance \cite{reiley2011review}. This underscores the importance of developing high-quality representative ESV datasets in a SDE to advance surgical data science and create SOTA vision tools for clinical use.

%Developing good quality representative ESV datasets in an SDE is a critical step forward in advancing surgical data science and creating computer vision tools that deliver real clinical impact.

% Understanding surgical scenes is vital for creating intelligent systems that can aid medical professionals during surgical procedures. This capability enables the automation of evaluating surgical skills, monitoring a surgeon's performance in real-time, and analyzing surgical outcomes after the procedure \cite{seenivasan2022global}. Scene understanding comprises object detection and timeline segmentation. It is not only limited to understanding instruments or tissues but also comprehending the semantics of surgical procedures in terms of phases, actions and tasks. Several researchers have been working on different aspects of surgical scene understanding including objects detection such as instrument (classifcation\cite{hou2022adaptive,konduri2024full}, localisation\cite{lu2023analysis,kanakatte2020surgical}, segmentation\cite{yue2024surgicalsam,zhou2024text}, tracking\cite{chen2023intelligent}), tissue (classification \cite{arnold2013hyper,cahill2021artificial}, localisation\cite{Seenivasan_2022}, segmentation\cite{scheikl2020deep,cervantes2021automatic}), critical safety view\cite{mascagni2022artificial}, and timeline segmentation of workflow understanding\cite{metchik2024novel,demir2023deep,guo2023current} e.g. phases, tasks, adverse surgical events, and appraisal-related tasks. Most work use image as 2D plane for designing DL networks whereas tasks like timeline segmentation require the understanding of temporal dimension of videos, rendering it as a 3D volumetric inputs for the DL networks. This leads to video based ESV analysis (VBA) which uses video/audio recordings of procedures to evaluate and provide feedback on surgeon performance.

% Recent advancement in VBA in general include Artificial intelligence (AI)-based computer vision approaches that provide scalable and automated analysis of surgical behaviour from videos \cite{goodman2021real}. This would enable surgeons to easily access and evaluate large video files for governance, training or teaching purposes.  AI has been used to identify surgical activity\cite{zia2018surgical}, gestures\cite{luongo2021deep}, surgeon skill levels\cite{lavanchy2021automation}, Surgical scene understanding \cite{kadkhodamohammadi2021towards}, instrument movements\cite{goodman2021real} and surgical workflow segmentation \cite{zhang2022large} from videos. Surgical timeline segmentation is at the heart of surgical scene understanding which analyses the sequence of surgical images from videos to segment labels for the timeline. These labels can represent surgical phases, tasks, activities, adverse events, etc. Contrary to object segmentation done at the image level which is another half of the surgical scene understanding, timeline segmentation is done at a video level, making it a volumetric analysis which is a challenging task. Also, Timeline segmentation faces multiple challenges because of similarities among different phases, inconsistent labeling, ambiguous transitions, variations in surgeon styles, and limited training data. These challenges make it difficult to effectively identify different stages in a timeline. A careful review of large video files can be time-consuming and inefficient even for human experts. Conducting a thorough evaluation is imperative to guarantee the creation of AI systems that are reliable and safe. Techniques that enable the various steps of a procedure within surgical videos to be indexed, so called "time stamps", have high value as it allows each of these video segments to be categorised and easily referenced for retrospective review. The study aims were [a] to develop a high-quality dataset of surgical videos with in-depth labelling of each operative procedure and [b] to construct and validate an AI model that autonomously applies "time stamps" to surgical operator behaviour and also rates the "quality" of actions within surgical videos.

\vspace{2mm}
Recent advancements in video-based analysis (VBA) using AI-driven computer vision techniques present substantial opportunities for enhancing surgical scene understanding through more scalable and robust methodologies \cite{goodman2021real}. Tailoring these VBA approaches specifically for ESV is crucial for demonstrating their efficacy in surgical data science and their potential application in real-world clinical settings. At the core of surgical scene understanding is the segmentation of surgical timelines, which involves analysing video sequences to categorise diverse surgical elements---ranging from phases and tasks to activities and adverse events. Unlike object segmentation, which focuses on image-level analysis, timeline segmentation operates at the video level, presenting a volumetric and ``moving object'' challenge far more complex than natural scenes of stationary objects. Moreover, the high similarity between different surgical phases, variability in surgeon styles, inconsistent labelling, ambiguous workflow transitions, and the scarcity of annotated training data exacerbate the complexity. These challenges hinder the development of reliable digital tools for practical and widespread clinical use.

\vspace{2mm}
Additionally, manually reviewing extensive ESV files is time-consuming and inefficient for human clinical experts. If done systematically, this can take up significant time that could be used for other clinical tasks. Consequently, creating digital tools capable of conducting comprehensive and accurate evaluations of ESV clips becomes essential to propel advancements in the field. This study aims to: (a) establish a systematic methodology for curating a high-quality, ``densely'' annotated ESV dataset, (b) assess the performance of cutting-edge video analysis models for surgical timeline segmentation, and (c) validate the most effective model for indexing ESV files to enhance search capabilities. This paper outlines strategies for transitioning from laboratory in-silico models to clinical applications, aiming to harness AI's potential to enhance interventional care to drive up surgical standards. To integrate SOTA methodologies in surgical data science, the whole pathway from video recording to annotation and analysis must be digitised. This will help improve and standardise overall surgical task performance evaluation.

\vspace{2mm}
%Beyond the primary surgical procedures, several associated clinical activities may enhance the understanding of intra-operative tasks and events. These include: (1) documenting workflows with detailed operation notes; (2) evaluating surgical skills in real time to inform training programs; (3) improving service quality by critical evaluation of adverse events to identify contributing factors and develop preventive measures; (4) creating educational content from key surgical highlights; and (5) predicting `time-to-finish' to improve surgery scheduling and optimise operating list management for the surgical and anaesthetic teams.%


% by focusing on the efficiency and effectiveness of key ``milestone'' steps within a surgical operation.%

\vspace{2mm}
In summary, we make the following salient contributions:
\begin{enumerate}
\item We present timeline annotation taxonomy for TEMS procedures capturing five phases, 12 tasks, and 84 actions for end-to-end surgical timeline segmentation.
\item We put forward \texttt{TEMSET-25K}---a densely annotated TEMS dataset comprising 25,835 cut-outs, keyframes, and label files capturing timeline labels for the proposed labelling strategy, the nature of the surgical action (adverse/normal), remaining surgical time, etc.
\item We share \texttt{SVPy}, a surgical video processing Python library, with the surgical data science community to perform the necessary pre-processing required for curating and labelling large multi-clip surgical video datasets.
\item We implement and evaluate STALNet ESV analytics using state-of-the-art encoders, including \texttt{ConvNeXt}, \texttt{ViT}, and \texttt{SWIN V2}, for surgical timeline segmentation to showcase the efficacy of the proposed taxonomy and benchmark the curated dataset for indexing surgical videos.
\item We present \texttt{SurgiFlow}, a digital platform for surgeons to efficiently search the large ESV databank using the proposed index and intuitive user interface in clinical settings.
\end{enumerate}

% Endoscopic surgical video (ESV) has gained significance as a medical tool due to its numerous applications that have the potential to significantly improve patient outcomes and progress surgical techniques. Endoscopic video provides complete documentation of surgical processes that can be used to construct comprehensive operation notes. This documentation can aid in the development of more effective and efficient procedures by examining and enhancing surgical workflows\cite{maier2017surgical}. Endoscopic video analysis aids surgeons in self-assessment, guiding skill improvement and training program development \cite{reiley2011review}.

% 4 paragraphs 
% - Surgery in genera
% - Surgical data acquisition & key challenges (size, ethical approvals, privacy, labelling data is critical for ML development. Labelling massive datasets is laborous and resource intesive task. However to advance the surgical scene understanding requires clean, well-curated, annotated datasets for diverse surgical procedures.
% - Surgical scene understanding & its importance
% - Recent AI advancement in video analytics & how they are revevant + Surgical timeline segmentation
% - Need for high quality data to showcase the benefit of surgical data acquisition. We will focus on surgical skill assessement 

% Recent research indicates that surgeons with higher peer ratings for skill had fewer complications and fatalities \cite{sacks2015relationship}. The development of systems intended to evaluate surgical skills and provide helpful feedback for improving methods may have a substantial effect on the variation that exists in real-world surgical practice \cite{martin1997objective}. The goal of evaluating surgical activity\cite{birkmeyer2013surgical}, is to find out the actions that a surgeon performs during a surgical procedure and how well these actions are carried out \cite{kiyasseh2023vision}. Researchers have explored various methodological approaches to identify the workflow, Scene Understanding and surgeon skill evaluation purely based on video data routinely collected during the surgical process \cite{ayobi2024pixel,twinanda2016endonet,dergachyova2016automatic,blum2010modeling}.  AI could act as an "expert colleague" for human surgical experts and a coach for surgical trainees \cite{hashimoto2018artificial}.Surgical AI toolkits may also help to locate critical events in surgical videos and document safety measures \cite{korndorffer2020situating,mascagni2021computer}.
%Also, AI systems designed for surgery have the potential to enhance the efficiency of surgical procedures by providing timely and context-sensitive alerts \cite{katic2013context}. However, the development of computer vision for surgery, non-camera-based surgical techniques \cite{richards2015national}, has been limited by the lack of diverse and sizeable training datasets \cite{esteva2021deep}. Also, the lack of comprehensive assessment of these technologies raises questions regarding their adaptability and efficacy in novel settings, such as unseen videos from different surgeons, surgical procedures, and hospitals \cite{kiyasseh2023vision}.


\section*{Related Work}
This section discusses prior work related to timeline analysis in surgical videos and state-of-the-art VBA methods, highlighting the potential for integrating timeline recognition with VBA to enhance the performance and generalisability of surgical data science solutions.

\subsection*{Timeline Analysis in Surgical Scenes}
Timeline analysis in surgical videos involves breaking down surgical procedures into distinct phases, tasks, and actions to provide a comprehensive understanding of the surgical workflow. Detailed workflow specifications capture all surgical nuances using phase/task/action triplets, which are essential for designing intelligent systems in the clinical operating room. These systems can provide context-aware decision support, monitor and optimise surgical operations, and offer early alerts for potential deviations and anomalies \cite{padoy2019machine, huaulme2020offline}.

\vspace{2mm}
Numerous studies have focused on surgical workflow analysis to identify missing activities in distinct phases, ensuring that surgeons complete necessary tasks before moving to the next phase \cite{kadkhodamohammadi2021towards}. Techniques for identifying surgical phases include data from sensors on tool tracking systems \cite{holden2014feasibility}, binary signals from instrument usage \cite{padoy2012statistical}, and surgical robots \cite{lin2005automatic}. However, obtaining these signals typically requires additional hardware or time-consuming manual annotation, which could increase the workload associated with the surgical process \cite{dergachyova2016automatic}.

\vspace{2mm}
Recent studies have focused on deriving the workflow solely from routinely collected endoscopic videos during surgery \cite{jin2017sv}. Automatic workflow recognition from surgical videos eliminates the need for additional equipment \cite{blum2010modeling}. Notable studies include the development of EndoNet, a convolutional neural network (CNN) architecture designed to recognise surgical phases using only visual information from cholecystectomy procedures \cite{twinanda2016endonet}. Other studies have employed temporal CNN models and transformer-based models for phase recognition in surgical activities \cite{ramesh2021multi, gao2021trans}. For instance, Funke et al. proposed a temporal model, TUNeS, which integrates self-attention into a convolutional U-Net architecture to enhance surgical phase recognition \cite{funke2023tunes}.

\subsection*{Emergence of Video-Based Analytics}

VBA involves meticulously breaking down and examining video content to extract important insights and intra-operative key events, transforming visual streams into semantically meaningful representations that can be easily analysed at scale \cite{huber2020video, liu2009encyclopedia}. Understanding surgical scenes requires consideration of the temporal dimension, making VBA crucial for providing an accurate understanding of surgical processes by examining both spatial and temporal features \cite{feldman2020sages}.

\vspace{2mm}
Real-time VBA can significantly enhance surgical care, particularly for minimally invasive techniques, by providing context-aware intra-operative decision support using AI models that swiftly and accurately extract knowledge from real-time video data. This situational guidance can improve surgical outcomes by aiding in applications such as calculating surgery duration, recording important events, assessing surgical skills, and providing intra-operative assistance \cite{vercauteren2019cai4cai, nwoye2020recognition, mascagni2022computer}. However, timeline labels in most research often lack the detail required for realistic clinical tasks, providing only coarse-grained information that fails to encompass surgical phases, tasks, and discrete actions needed for objective assessment and benchmarking of surgical performance \cite{lewandrowski2020regional, richards2015national}.

\vspace{2mm}
Additionally, deep learning methods used for surgical timeline segmentation often require large amounts of annotated data, which is rarely available \cite{paysan2021self}. To address this, Valderrama et al. introduced the PSI-AVA Dataset, which provides comprehensive annotations for surgical scene understanding in prostatectomy videos, and proposed the TAPIR model for action recognition using transformers \cite{valderrama2022towards}. Similarly, Ayobi et al. presented the TAPIS model, also based on transformers, to facilitate multilevel comprehension of surgical activities, including long-term tasks and instrument segmentation and atomic action detection \cite{ayobi2024pixel}.

\vspace{2mm}
By combining insights from timeline analysis and VBA, our study aims to further advance the understanding and evaluation of surgical procedures through the development of high-quality, deeply annotated ESV datasets and the establishment of robust AI models for surgical timeline segmentation.

%\section*{Related Work}
%In this section, we discuss the prior work related to the paper covers video analysis, activity recognition, and surgical skill assessment in surgical videos. There is substantial evidence from within the field of surgery and in a wide range of high-performance ``complex task-based'' fields that technical fine-tuning of tasks can improve performance with a detailed assessment and constructive feedback. 

%\subsection*{Video Based Analysis (VBA)}

%VBA involves breaking down and examining video content to extract important insights \cite{huber2020video}. It aims to understand, evaluate, and derive inferences from videos by transforming visual streams into semantically meaningful representations that can be eaily analysed at scale \cite{liu2009encyclopedia}.

%Real-time VBA is expected to be crucial for surgical procedures \cite{mascagni2022computer}. Surgical video analysis, widely based on VBA, provides a more accurate understanding of the surgical process by thoroughly examining several areas, including procedure phase and step segmentation, performance analysis, and skill assessment \cite{feldman2020sages}. Analysing videos of surgeries and the operating room (OR) can significantly enhance surgical care, especially for minimally invasive techniques, which are becoming increasingly popular worldwide \cite{lewandrowski2020regional,richards2015national}. This analysis can provide surgeons with context-aware intra-operative decision support using AI computational models, which can accurately and swiftly extract knowledge from real-time video data \cite{vercauteren2019cai4cai,nwoye2020recognition}.

%Additionally, VBA is a valuable tool for assessing surgical skills, providing formative feedback, and evaluating operating performance \cite{mascagni2022computer}. Given its importance, surgical video analysis techniques have been automated using surgical data science. Various studies have utilised AI models to analyse surgical videos for recognising surgical phases \cite{ramesh2021multi}, tools \cite{hashimoto2018artificial,nwoye2019weakly}, and actions \cite{nwoye2020recognition}. This helps with multiple applications, such as calculating surgery duration \cite{twinanda2018rsdnet}, automatically recording important events \cite{mascagni2022computer}, assessing surgical skills \cite{lavanchy2021automation}, and offering intraoperative assistance \cite{aspart2022clipassistnet}.

% Video analysis is an approach that involves breaking down and examining video content to extract important insights \cite{huber2020video}. Video analysis aims to understand, evaluate, and derive inferences from the videos. It uses approaches to transform visual streams into semantically meaningful representations \cite{liu2009encyclopedia}.
% Real-time video analysis is expected to be very important for surgical procedures \cite{mascagni2022computer}. Surgical video analysis is widely based on VBA which provides a more accurate understanding of the surgical process since it thoroughly examines several areas, including procedure phase and step segmentation, performance analysis, and skill assessment\cite{feldman2020sages}. Analyzing videos of surgeries and the operating room (OR) can aid in enhancing surgical care, especially for procedures conducted using minimally invasive techniques, which are becoming increasingly popular worldwide \cite{lewandrowski2020regional,richards2015national}. This can provide surgeons with context-aware intra-operative decision support using AI computational models. These models can accurately and swiftly extract knowledge from video data captured in real-time \cite{vercauteren2019cai4cai,nwoye2020recognition}. Additionally, VBA is a way to assess surgical skills, provide formative feedback, and evaluate operating performance \cite{mascagni2022computer}. Due to the importance of surgical video analysis, the techniques of surgical data science have been used to automate the field. In various studies, AI models have been used to analyse surgical videos for the recognition of surgical phases \cite{ramesh2021multi}, tools \cite{hashimoto2018artificial,nwoye2019weakly}, and actions \cite{nwoye2020recognition}. It helps various applications, such as calculating surgery duration \cite{twinanda2018rsdnet}, automatically recording important events \cite{mascagni2022computer}, surgical skill assessment \cite{lavanchy2021automation}, and offering interoperate assistance \cite{aspart2022clipassistnet}.

%\subsection*{Activity Recognition}

%Minimal-invasive surgery has improved patient outcomes and lends itself easily to the digital acquisition of video files. This has led to the development of novel teaching strategies \cite{bjerrum2018surgical}. Many methods have been used in the field of surgical activity recognition, such as bag of words (BoW) models and linear dynamical systems (LDS) \cite{zappella2013surgical}. The use of deep learning has been crucial to recent developments in surgical activity recognition \cite{ahmidi2017dataset,dipietro2019segmenting}. However, these methods have the drawback of requiring a massive amount of annotated data for training \cite{paysan2021self}.  


%In surgical procedures, activity/phase recognition is the most common task \cite{nwoye2023cholectriplet2022}. The recognition of the surgical phase is to break down a surgical operation into distinct parts, each of which represents a particular task that the surgeon must complete before moving to the next phase \cite{kadkhodamohammadi2021towards}. Numerous techniques have been employed to identify the surgical phase, i.e., data from sensors on tool tracking systems \cite{holden2014feasibility}, binary signals from instrument usage \cite{padoy2012statistical}, and surgical robots \cite{lin2005automatic}. However, obtaining these signals typically requires the installation of extra hardware or time-consuming manual annotation, which could increase the work associated with the surgical process \cite{dergachyova2016automatic}. Consequently, recent studies have focused on deriving the workflow solely from routinely collected video data during surgery \cite{jin2017sv}. In addition to its ability to eliminate the need for additional equipment, automatic workflow recognition from surgical videos helps evaluate surgical skills \cite{blum2010modeling}. 


%Several attempts have been made to recognise the surgical phase. A study in \cite{twinanda2016endonet} described a unique method for phase recognition that uses only visual information and a convolutional neural network (CNN) to automatically learn features from videos of cholecystectomy procedures.  They presented EndoNet, an advanced CNN architecture designed to recognise the surgical phase. Other groups \cite{ramesh2021multi} have focused on using a temporal CNN model for phase recognition in surgical activity. They asserted that the model is a temporal convolutional network with several tasks effectively implemented for combined online phase and step recognition. Gao et al.\cite{gao2021trans} proposed a hybrid embedding aggregation transformer to accurately recognise phases in surgical videos. Alternatively, an AI model to enhance surgical phase recognition was proposed by Funke et al. \cite{funke2023tunes}. They described a temporal model TUNeS, to maximise attention utilization. The technique integrates self-attention into a convolutional U-Net architecture for improved performance.

%\subsection*{Surgical Skill Assessment}

%Surgery is dependent on an operator's skills to manipulate surgical instruments to interact with part or the whole of a human organ. This leads to a specific intended outcome, for example the dissection of a large cancerous or pre-cancerous polyp from the rectal wall \cite{fecso2017effect}. Training and assessment of these skills in a targeted manner can positively impact patient outcomes by enabling surgeons to practice or review their surgical actions \cite{nataraja2018simulation}. To maintain patient safety and minimize adverse clinical events, critical evaluation of surgical skills and constructive feedback have become essential components of surgical training \cite{reznick1993teaching}. A recent study indicates that over 25\% of the variance in patient outcomes can be attributed to differences in the level of surgical skills among practising surgeons \cite{stulberg2020association}. Therefore, to enhance patient outcomes, it is crucial to consistently provide objective feedback on surgeons' technical performance to aid their skill development \cite{lavanchy2021automation}. Performance improvement in any specialised field is greatly enhanced with a structured and constructive feedback mechanism in a controlled environment. This is analogous to the positive impact of coaching in technically challenging sports, for example squash, where focused feedback over a prolonged period can significantly improve on-court performance and rankings.


%Surgical skill assessment also requires the correct identification of a particular surgical phase as different phases may be undertaken at different levels of proficiency. 

%\subsection*{Surgical Timeline Segmentation}
%Surgeries can be generally understood as the composition of instruments and actions involved in sequences of different anatomical activities \cite{zhou2021towards}. Manual understanding of surgical procedures is an uphill task involving careful observations of the surgical activities. For this reason, understanding intricate anatomical processes requires automatic surgical workflow analysis.
%Recognizing surgical workflow is essential to the design of intelligent systems in the operating room\cite{padoy2019machine}. In particular, workflow recognition enables these systems to provide context-aware decision support, monitor and optimise surgical operations, and provide early alerts of possible deviations and anomalies\cite{huaulme2020offline}. The overall workflow can be recognised by understanding the surgical scene. 
%A study presented holistic scene understanding for recognizing phases, steps, actions and instruments in endoscopic surgical videos \cite{valderrama2022towards}. The author of this study created a PSI-AVA Dataset, offering comprehensive annotations for surgical scene understanding in prostatectomy videos. They introduced Transformers for Action,Phase, Instrument, and Steps Recognition (TAPIR), a robust baseline that enhances classification by leveraging multi-level dataset annotations. Another study presented a model based on Transformers for Actions, Phases, Steps, and Instrument Segmentation (TAPIS)\cite{ayobi2024pixel}. They aim to facilitate multilevel comprehension of surgical activities including long-term tasks like phase and step recognition, and short-term tasks like instrument segmentation and atomic action detection.


\section*{Methods}

\subsection*{Transanal Endoscopic Microsurgery (TEMS) Overview}
The dataset described in this paper comprises recordings of TEMS procedures performed on patients with early rectal cancer or large pre-cancerous polyps \cite{BACH202192}. During the TEMS procedure, an operating scope is inserted trans-anally into the rectum. This is stable and flexible platform that enables access from the anorectal junction to the most cephalic aspect of the rectum is 15cm from the anal verge (bottom of the anal canal). Most of the rectum can be reached with this TEMS scope. The surgeon adjusts the scope to reach and remove the tumour, manoeuvring it as needed. The procedure begins with a \texttt{setup} phase, which includes preparing the scope, instruments, and the surgical site. The rectum is inflated with carbon dioxide to a preset pressure, and faecal debris and fluid are removed with a suction device to obtain clear views. The main phase involves dissecting the tumour, removing the specimen, and closing the rectal wall defect. Surgeons use a \texttt{clockface} analogy to navigate the lesion site, facilitating precise removal. Dissection may be partial (mucosa and submucosa) or full thickness (deeper muscle tissues). 

% Figure \ref{fig:TEMS} shows the surgical equipment used in TEMS operations.Figure \ref{fig:HV} illustrates the anatomical layers of the large intestine wall.

% \begin{figure}[htbp]
%     \centering
%     \begin{subfigure}[b]{0.3\textwidth}
%         \centering
%         \includegraphics[width=\textwidth, height=3cm]{Surgical Equipment.png}
%         \caption{Surgical Equipment Used in TEMS}
%         \label{fig:TEMS}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.3\textwidth}
%         \centering
%         \includegraphics[width=\textwidth, height=3cm]{TEMS figure surgical flow.png}
%         \caption{Overall process for TEMS: end to end}
%         \label{fig:Suturing}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.3\textwidth}
%         \centering
%         \includegraphics[width=\textwidth, height=3cm]{Histopathological.png}
%         \caption{Histopathological View of Large Intestine}
%         \label{fig:HV}
%     \end{subfigure}
    
%     \caption{Transanal Endoscopic Microsurgery Concepts}
%     \label{fig:all}
% \end{figure}

\begin{figure}[!t]
    \centering
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{ccccc}
        \begin{subfigure}[b]{0.18\textwidth}
            \centering
            \includegraphics[height=3cm]{TEMS figure surgical flow_1A.png}
            \caption*{A}
            \label{A}
        \end{subfigure} &
        \begin{subfigure}[b]{0.18\textwidth}
            \centering
            \includegraphics[height=3cm]{TEMS figure surgical flow_1B.png}
            \caption*{B}
            \label{B}
        \end{subfigure} &
        \begin{subfigure}[b]{0.18\textwidth}
            \centering
            \includegraphics[height=3cm]{TEMS figure surgical flow_1C.png}
            \caption*{C}
            \label{C}
        \end{subfigure} &
        \begin{subfigure}[b]{0.18\textwidth}
            \centering
            \includegraphics[height=3cm]{TEMS figure surgical flow_1D.png}
            \caption*{D}
            \label{D}
        \end{subfigure} &
        \begin{subfigure}[b]{0.18\textwidth}
            \centering
            \includegraphics[height=3cm]{TEMS figure surgical flow_1E.png}
            \caption*{E}
            \label{E}
        \end{subfigure} \\

        \begin{subfigure}[b]{0.18\textwidth}
            \centering
            \includegraphics[height=3cm]{TEMS figure surgical flow_1F.png}
            \caption*{F}
            \label{F}
        \end{subfigure} &
        \begin{subfigure}[b]{0.18\textwidth}
            \centering
            \includegraphics[height=3cm]{TEMS figure surgical flow_1G.png}
            \caption*{G}
            \label{G}
        \end{subfigure} &
        \begin{subfigure}[b]{0.18\textwidth}
            \centering
            \includegraphics[height=3cm]{TEMS figure surgical flow_1H.png}
            \caption*{H}
            \label{H}
        \end{subfigure} &
        \begin{subfigure}[b]{0.18\textwidth}
            \centering
            \includegraphics[height=3cm]{TEMS figure surgical flow_1I.png}
            \caption*{I}
            \label{I}
        \end{subfigure} &
        \begin{subfigure}[b]{0.18\textwidth}
            \centering
            \includegraphics[height=3cm]{TEMS figure surgical flow_1J.png}
            \caption*{J}
            \label{J}
        \end{subfigure}
    \end{tabular}
    \end{adjustbox}
    \caption{\textbf{TEMS surgical workflow:} \textit{A typical surgical flow from landmarking of the rectal polyp to dissection, lesion removal and closure of the rectal wall defect. The key milestones of a TEMS procedure are detailed in images A-J: [A] Baseline lesion in view after setup; [B] Application of landmark dots to outline the lesion; [C] Dissection of the wall through the mucosa and muscle; [D-E] Circumferential removal of the lesion; [F-G] Final removal and extraction of the specimen; [H-I] Closure of the rectal wall defect with a suture; and [J] Application of a metal clip to secure the suture and ensure complete closure.}}
    \label{fig:arch}
\end{figure}









%\begin{figure}[!t]
   % \centering
    %\includegraphics[width=0.7\textwidth]{TEMS figure surgical flow.png}
   % \caption{\textbf{TEMS surgical workflow:} \textit{A typical surgical flow from landmarking of the rectal polyp to dissection, lesion removal and closure of the rectal wall defect. The key milestones of a TEMS procedure are detailed in images A-J: [A] Baseline lesion in view after setup; [B] Application of landmark dots to outline the lesion; [C] Dissection of the wall through the mucosa and muscle; [D] Circumferential removal of the lesion; [F-G] Final removal and extraction of the specimen; [H-I] Closure of the rectal wall defect with a suture; and [J] Application of a metal clip to secure the suture and ensure complete closure.}}
    %\label{fig:arch}
%\end{figure}


\vspace{2mm}
During dissection, multiple small events like surgical ``smoke'' fogging the lens, lens wash, tissue cauterisation, tissue retraction, fluid aspiration, and bleeding may occur. These are inter-related, for example tissue cauterisation results in surgical smoke that fogs up the operating scope and is normally managed by scope wash to clean the camera lens and aspiration of any fluid in the field of view. Bleeding is controlled with diathermy instruments and aspiration. Various instruments are used based on surgical needs. After dissection, the specimen is removed through the scope for histological analysis. In the closure phase, suturing to close the rectal wall defect. This involves handling the needle, driving it through the rectal wall, and pulling the suture to close the defect. 

% Figure \ref{fig:Suturing} details: Images A-J show the key milestones of a TEMS procedure. [A] Baseline lesion in view after setup, [B] Application of landmark dots to outline the lesion, [C] Dissection of the wall through the mucosa and muscle, [D] Circumferential removal of the lesion, [F-G] Final removal and extraction of the specimen, [H-I] Closure of the rectal wall defect with a suture [J] Application of a metal clip to secure the suture and ensure complete closure.

\subsection*{Patient Cohort}

This study included fully de-identified videos from patients with a clinical diagnosis of rectal polyps or cancer. Pre-operatively, patients underwent standard clinical staging, including optical endoscopy, biopsy, endo-rectal ultrasound, magnetic resonance imaging, and computed tomography. These cases were discussed in a multidisciplinary meeting before elective surgery was offered. A team of four specialist colorectal surgeons, all Fellows of the Royal College of Surgeons (FRCS), performed TEMS using a Richard Wolf trans-anal operating platform.

\subsection*{Ethical Statement and Data Compliance}

The study is registered as a clinical audit with the University Hospitals Birmingham, conforming to local ethical standards, under the Clinical Audit Registration Management System (CARMS) number 20648. Informed consent was obtained from all patients before recording fully de-identified surgical videos. In accordance with NHS ethical standards and the UK General Data Protection Regulation (GDPR), the routinely collected ESV dataset underwent a full anonymisation procedure to ensure the removal of any identifiable information and protect patient privacy and confidentiality. Rigorous measures were implemented to review each video by clinicians to ensure that no patient identifiers were inadvertently captured or disclosed. Surgical scenes that extended beyond the abdominal cavity, capturing the surgical team or hospital/OR surroundings, were removed by the surgeons. These segments, typically occurring when the camera was temporarily extracted from the endo-luminal cavity for cleaning purposes, were replaced by blank frames while preserving patient privacy and maintaining the surgical procedure's overall chronological sequence and duration.

\subsection*{Data Capture \& Sharing}
We used the Operating Room (OR) visualisation system for data acquisition comprising a stereo endoscopic 50-degree scope and eyepiece attached to the Karl Storz Image 1 Hub HD (high-definition) camera system. Multi-part HD videos were recorded and archived using the Karl Storz AIDA™ system, which contains an intelligent export manager that automatically saves surgical video files during surgery. These files were stored on encrypted NHS hospital-based hard drives. All patient information was removed to ensure no metadata containing patient-related information was shared or made accessible to the clinical or data science teams.

\subsection*{Co-Creation of Dense Taxonomy Labels for Timeline Segmentation}
In our effort to develop a comprehensive taxonomy for annotation, the project task group worked with specialist surgeons to define a representative surgical workflow. This collaborative co-creation process was essential to capturing the intricate details necessary to describe various downstream clinical tasks, in order to facilitate precise and detailed video labelling.

\begin{figure}[!t]
    \centering
    \includegraphics[width=\textwidth, height=\textheight, keepaspectratio]{TaxonomyNew2.png}
    \caption{\textbf{Proposed Taxonomy of TEMS Surgical Workflow.} \textit{The TEMS operation can be split into three levels: [A] High level activity phase (such as Set-up, Dissection, Specimen Removal, Closure and Scope Removal), [B] Task based activities (such as scope insertion, instrument movement, site wash and pressure increase), [C] Small unit tasks (such as tissue marking, tissue retraction, smoke identification, bleeding identification and haemostasis).}}
    \label{fig:taxonomy}
\end{figure}




\vspace{2mm}
To achieve this, we structured the labels into \texttt{phase}, \texttt{task}, and \texttt{action} ``triplets''. This hierarchical framework allowed for a detailed end-to-end breakdown of the surgical procedure:
\begin{itemize}
    \item \textbf{Phase}: Represents ``high level'' activities encompassing a series of surgical tasks (for example setup of the TEMS scope or dissection phase).
    \item \textbf{Task}: Intermediate activities within a phase that include more specific actions (for example, in the dissection phase this may involve landmarking the dissection plane around the tumour or mucosal dissection).
    \item \textbf{Action}: The smallest unit activity within a task,  (for example, dissection, retraction, lens wash, identification of bleeding, haemostasis or aspiration of fluid).
\end{itemize}

\vspace{2mm}
For the TEMS procedure, we identified five key high-level phases: ``setup'', ``dissection'', ``specimen removal'', ``closure of defect'', and ``scope removal''. Each phase consists of various sub-tasks, which in turn are made up of individual specific actions. This ``triplet'' structure ensures that every aspect of the surgery is captured in detail, facilitating accurate and meaningful analysis. (See Figure \ref{fig:taxonomy} for detailed specification of the TEMS surgical workflow.)

% \begin{table}[ht]
%     \centering
%     \begin{tabular}{|c|p{3cm}|p{8cm}|}
%         \hline
%         \rowcolor{lightgray}
%         \textbf{Phase} & \textbf{Task} & \textbf{Actions} \\
%         \hline
%         Setup & Scope & Scope Insertion, Scope Positioning, No Action, Out of Body \\
%         \cline{2-3}
%               & Instrument & Instrument Insertion, Instrument Positioning, No Action, Out of Body \\
%         \cline{2-3}
%               & Site & Fluid Wash, Debris Wash, No Action, Out of Body \\
%         \cline{2-3}
%               & Pressure & Inflate Rectum, Deflate Rectum, No Action, Out of Body \\
%         \hline
%         Dissection & Landmarking & Marking, Retraction, Smoke, Bleeding, Haemostasis, Washout, Aspiration, Scope Positioning, Instrument Positioning, Inflate Rectum, Deflate Rectum, No Action, Out of Body \\
%         \cline{2-3}
%                    & Mucosal Dissection & Dissection, Retraction, Smoke, Bleeding, Haemostasis, Washout, Aspiration, Scope Positioning, Instrument Positioning, Inflate Rectum, Deflate Rectum, No Action, Out of Body \\
%         \cline{2-3}
%                    & Submucosal Dissection & Dissection, Retraction, Smoke, Bleeding, Haemostasis, Washout, Aspiration, Scope Positioning, Instrument Positioning, Inflate Rectum, Deflate Rectum, No Action, Out of Body \\
%         \cline{2-3}
%                    & Circular Muscle Dissection & Dissection, Retraction, Smoke, Bleeding, Haemostasis, Washout, Aspiration, Scope Positioning, Instrument Positioning, Inflate Rectum, Deflate Rectum, No Action, Out of Body \\
%         \cline{2-3}
%                    & Longitudinal Muscle Dissection & Dissection, Retraction, Smoke, Bleeding, Haemostasis, Washout, Aspiration, Scope Positioning, Instrument Positioning, Inflate Rectum, Deflate Rectum, No Action, Out of Body \\
%         \hline
%         Specimen Removal & Specimen Removal & Specimen Removal, Inflate Rectum, Deflate Rectum, No Action, Out of Body \\
%         \hline
%         Closure & Suturing & Stitching, Clipping Suture, Cutting Suture, Smoke Management, Bleeding Control, Haemostasis, Washout, Aspiration, Scope Positioning, Instrument Positioning, Inflate Rectum, Deflate Rectum, No Action, Out of Body \\
%         \hline
%         Scope Removal & Scope Removal & Scope Removal, Inflate Rectum, Deflate Rectum, No Action, Out of Body \\
%         \hline
%     \end{tabular}
%     \caption{Fine-grained Timeline Taxonomy for Annotating TEMS Surgical Videos}
%     \label{tab:tems_labels}
% \end{table}

\vspace{2mm}
This structured approach not only helps in the detailed documentation of the procedure but also enhances the ability to extract key events within an operation. This can be subsequently used to perform post-operative clinical assessments, either at an individual surgeon level or in comparison of techniques in a cohort of surgeons. This approach ensures that every critical operative action is accounted for and can be analysed in a constructive manner for measuring incremental performance improvements. This enables extremely ``dense'' data to be extracted from VBAs and analysed at scale. Large video libraries can be interrogated with the approaches defined above to understand changes in individual surgical performance over time as well as comparing surgeons against their peers.       

\subsection*{Infrastructure Setup, Video Annotation, and Exporting Labels}
Our ESV dataset comprises multipart videos of TEMS surgery, averaging 10.34 gigabytes (GB) each. We evaluated several labelling platforms and found LS (Label Studio version $1.12.1$) to be the most suitable for video annotation, despite some limitations. LS \cite {Gurevych2013-uo} is a secure web-based annotation tool supporting text, photos, videos, audio, and sequence data. While it offers extensive features, certain constraints affected its usability for our research. First, LS requires video alongside audio tracks for timeline segmentation, but no audio was used in this dataset construction for obvious privacy reasons. Second, the default video upload limit in LS is 250 megabytes, which can be extended but affects annotation interface performance adversely. Lastly, managing multi-part ESV files within a single project is challenging, as LS treats each surgery as a separate project, complicating data organisation.

\vspace{2mm}
To address these issues, we inserted blank audio tracks into the clips using \texttt{FFmpeg} \footnotemark{}\footnotetext{https://ffmpeg.org/}. We also reduced the file size by compressing the bitrate from 13 million (13 Mbps) to 100,000 (1 Mbps), facilitating smoother uploads and reducing server load without compromising visual quality. \Cref{fig:before_processing,fig:after_processing} shows a surgical scene before and after compression, demonstrating minimal quality loss.

\begin{figure}[htbp]
 \begin{subfigure}{0.5\linewidth}
   \includegraphics[width=\linewidth]{video_before_encoding}
        \caption{Bitrate = 13 Mbps; Filesize = 1 GB.}
        \label{fig:before_processing}
    \end{subfigure}
    \begin{subfigure}{0.5\linewidth}
        \includegraphics[width=\linewidth]{video_after_encoding}
        \caption{Bitrate = 1 Mbps; Filesize = 100 MB.}
        \label{fig:after_processing}
    \end{subfigure}
    \caption{\textbf{Side-by-side comparison of videos before and after pre-processing.} \textit{ Panels (a) and (b) show image quality before and after pre-processing, respectively. This shows that despite reduction in the size of the ESV file by a factor of 10 (from 1GB to 0.1GB), there was no loss in quality.}}
\end{figure}

\vspace{2mm}
After preparing the videos,  LS was installed on a server in a \texttt{Docker} container and made accessible for labelling by using a \texttt{ngrok} tunnelling platform. ESV files were uploaded into LS across several projects. For each project, the LS interface was customised for timeline annotation. The inclusion of empty audio tracks enabled the use of the LS timeline component for region-based labelling, significantly aiding efficient clinical annotation of large multi-part ESV files. Secure logins, allowed different surgeons to perform initial segmentation, which was then reviewed and finalised by a panel of clinical domain experts to ensure consistent labelling. Finally, we exported the labels from LS in JSON format, with each multi-part ESV file generating one JSON file for timeline segmentation. 


% \subsection*{Videos Preparation}
% To prepare an annotated dataset to train a segmentation model, we first asked domain experts to provide us with annotations for a set of pre-selected video clips. For this purpose, we used a self-hosted instance of Label Studio. Label Studio \cite{Gurevych2013-uo} is a web-based application that allows labeling for various data formats, such as text, photos, videos, audio, time series, and multidomain data. It provides both open-source and commercial license options \cite{Barolli2022-ua}. Label Studio has several limitations; first, it requires video clips to have an audio track to display a timeline at the bottom. Secondly, the default limit for uploading a file to Label Studio is 250 MB. This limit can be increased using on-prem installation. However, larger files affect the labeling performance. Thirdly, Label Studio considers an individual video a separate project, while a multi-level hierarchy is not supported in a single project. However, in our case, every surgery has many video clips, and each video clip is further split into various video cutouts that are 5 seconds each, making a hierarchical representation. 

% To address the above limitations of Label Studio, we first apply \texttt{ffmpeg} to insert a blank audio track into the clips. Additionally, the bitrate of each clip, originally over 13 million (13 Mbps), was reduced to 100,000 (1 Mbps) using bitrate compression techniques, significantly reducing the file size. Reducing the file size also contributes to decreased bandwidth requirements and the consumption of fewer server resources, thus accelerating the overall process compared to the original file size. The file size reduction process was accomplished without losing the quality of images. This ensured that visual fidelity was preserved while reducing the resources needed to stream and render the videos for annotation. \Cref{fig:before_processing,fig:after_processing} compares the two videos, both before and after pre-processing. As can be seen, there is no noticeable loss of quality.

% \begin{figure}[htbp]
%  \begin{subfigure}{0.5\linewidth}
%    \includegraphics[width=\linewidth]{video_before_encoding}
%         \caption{Bitrate = 13 Mbps; Filesize = 1 GB.}
%         \label{fig:before_processing}
%     \end{subfigure}
%     \begin{subfigure}{0.5\linewidth}
%         \includegraphics[width=\linewidth]{video_after_encoding}
%         \caption{Bitrate = 1 Mbps; Filesize = 100 MB.}
%         \label{fig:after_processing}
%     \end{subfigure}
%     \caption{Side-by-side comparison of videos before and after pre-processing.}
% \end{figure}

% \subsection*{Curating High-Quality Annotated Dataset}

% We developed a systematic approach to extract a machine learning-ready dataset from the expertly annotated multipart ESV files. Instead of extracting all frames, our method selects keyframes using cosine distance similarity to capture frames with significant scene changes in sequential order. This results in the extraction of far fewer frames (average $\sim550$) compared to the total frames (average $\sim15K$) in each video clip. We utilised advanced \texttt{FFmpeg} features to name keyframes uniquely, combining surgery and ESV file names with timestamps to maintain the chronological order in the videos. These keyframes are stored in the \texttt{keyframes} folder.

% \vspace{2mm}
% Next, we implemented a range-based query method to associate each keyframe with its corresponding label from the LS-exported JSON file. This mapping is saved in the \texttt{timeline\_labels.CSV} file, which includes \texttt{filename} and \texttt{label} columns. We also created cutouts from each keyframe, capturing frames up to 60 seconds prior, stored as \texttt{.mp4} files in the `cutouts' folder. The \texttt{timeline\_labels.CSV} file is further extended with a \texttt{remaining\_time} column to denote the remaining surgical time for each keyframe and cutout.

% \begin{algorithm}[!ht]
%     \scriptsize
%     \caption{Generate clips for annotation keyframes}
%     \label{alg:keyframe_extraction}
%     \begin{algorithmic}[1]
%         \Input $L=\{(l_ti,l_{si},l_{ei})_{i=0}^{N-1}\} \gets$ annotations of length $N$, where $l_t=$ annotation label text, $l_s=$ start time of annotation, $l_e=$ end time of annotation; $V \gets$ video filename
%         \Output $N$ video clips of maximum 5s each; one for each annotation
        
% \State $F \gets \mathrm{extract\_keyframe\_timestamps(V)}$ \Comment{Get keyframe timestamps from the video file} \label{lst:line:extract_keyframes}
% \State $\mathrm{duration} \gets 5$  \label{lst:line:duration}
% \State $L_f \gets \emptyset$ \Comment{Label annotations with keyframe timestamps} \label{lst:line:l_f}
% \ForAll{$l \in L$} \label{lst:line:labelloop}
%             \ForAll{$f \in F$}
%                 \If{$l_s \leq f \leq l_e$}
% \State $l_f \gets f$
% \State \Call{Append}{$L_f, \{l_t, l_s, l_e, l_f$\}} \label{lst:line:appendlabel}
%                 \EndIf
%             \EndFor
%         \EndFor
% \ForAll{$l \in L_f$} \Comment{Generate keyframe clips for each label annotation} \label{lst:line:keyframeloop}
% \State $l_f \gets \operatorname{get_keyframe_timestamp}(l)$ \Comment{Get the timestamp for the keyframe in $l$} \label{lst:line:getkeyframetimestamp}
% \State $s \gets \max(0, l_f - 0.5 * \duration)$ \Comment{Start time of keyframe clip} \label{lst:line:clipstart}
% \State $e \gets l_f + 0.5 * \duration$ \Comment{End time of keyframe clip}
% \State $\operatorname{generate\_clip}(l_t, s, e, V)$ \label{lst:line:generate_clip}
%         \EndFor
%     \end{algorithmic}
% \end{algorithm}

% \vspace{2mm}
% \noindent
% \Cref{alg:keyframe_extraction} presents the algorithm for generating an ML-ready dataset from large multipart video files like surgical videos. In lines~\ref{lst:line:extract_keyframes}, \ref{lst:line:duration} and \ref{lst:line:l_f}, we get a list $F$ of all the keyframe timestamps from our source video $V$, set the duration of the target clips to 5s and initialise an empty list $L_f$ for storing label annotations along with the timestamps of keyframes within those labels. In lines~\ref{lst:line:labelloop}--\ref{lst:line:appendlabel}, we loop through all the label annotations $L$, find keyframe timestamps that occur within each label, and append them to the list $L_f$. Finally, we generate the 5s clips using \texttt{ffmpeg} in lines~\ref{lst:line:keyframeloop}--\ref{lst:line:generate_clip}. We do this be retrieving the keyframe timestamp in line~\ref{lst:line:getkeyframetimestamp} and calculating the clip start and end times. We also ensure that the clip start time does not precede 0s in line~\ref{lst:line:clipstart}. Due to this, some clips may be shorter than 5s. The \textit{generate\_clip} method calls \texttt{ffmpeg} with the appropriate parameters to extract the clip. Fig \ref{Timeline-seg} provides a high-level overview of our proposed methodology for transforming raw ESV files into high-quality ML-ready annotated dataset. 

\begin{algorithm}[!t]
\caption{Post-Processing Surgical Videos and Annotations}
\label{alg:post_processing} 
\KwData{ESV files in \texttt{mp4} format, LS-exported \texttt{JSON} file with annotations}
\KwResult{Keyframes extracted, timestamped, labelled, and corresponding cutouts created}

\SetKwFunction{GetKeyframeTimestamps}{get\_keyframe\_timestamps}
\SetKwFunction{ExtractKeyframes}{extract\_keyframes}
\SetKwFunction{ExtractAllKeyframes}{extract\_all\_keyframes}
\SetKwFunction{LoadJsonData}{load\_json\_data}
\SetKwFunction{ParseFilename}{parse\_filename}
\SetKwFunction{FindLabelsForKeyframes}{find\_labels\_for\_keyframes}
\SetKwFunction{GetLabels}{get\_labels}
\SetKwFunction{WriteResultsToCsv}{write\_results\_to\_csv}
\SetKwFunction{CreateVideoCutouts}{create\_video\_cutouts}

\Begin{
    \tcc{Extract Keyframes}
    \ForEach{video in \texttt{root\_folder}}{
        timestamps $\gets$ \GetKeyframeTimestamps{video}\;
        \ExtractKeyframes{video, timestamps}\;
    }
    
    \tcc{Load Annotation Data}
    data $\gets$ \LoadJsonData{\texttt{json\_file}}\;
    
    \tcc{Find Labels for Keyframes}
    results $\gets$ []\;
    \ForEach{frame in \texttt{frames\_folder}}{
        video\_name, timestamp $\gets$ \ParseFilename{frame}\;
        labels $\gets$ \GetLabels{data, video\_name, timestamp}\;
        results.append(\{filename: frame, timeline\_label: labels\})\;
    }
    
    \tcc{Write Results to CSV}
    \WriteResultsToCsv{results, \texttt{data\_root\_path}}\;
    
    \tcc{Create Video Cutouts}
    \ForEach{entry in results}{
        video\_name, timestamp $\gets$ \ParseFilename{entry.filename}\;
        start\_time $\gets$ max(0, timestamp - 30)\;
        video\_path $\gets$ \texttt{root\_folder} + video\_name + \texttt{.mp4}\;
        cutout\_file $\gets$ \texttt{cutouts\_folder} + entry.filename + \texttt{.mp4}\;
        \texttt{FFmpeg\_command} $\gets$ \texttt{ffmpeg -ss} + start\_time + \texttt{-t 30 -i} + video\_path + \texttt{-c copy} + cutout\_file\;
        \texttt{run}(FFmpeg\_command)\;
    }
}

\end{algorithm}

\subsection*{Post-Processing of Annotations to Generate ML-ready Dataset}

A systematic approach was developed to extract a machine learning-ready dataset from the densely annotated multipart ESV files. Instead of extracting all frames, the methods used here select key frames using cosine distance similarity to capture frames with significant scene changes in sequential order. This results in extracting far fewer frames (average $\sim550$) than the total frames (average $\sim15K$) in each video clip. Advanced \texttt{FFmpeg} features were utilised to name keyframes uniquely, combining surgery and ESV file names with timestamps to maintain the chronological order in the videos. These keyframes are stored in the \texttt{frames} folder.

\vspace{2mm}
Next,  a range-based query method was implemented to associate each keyframe with its corresponding label from the LS-exported JSON file. This mapping is saved in the \texttt{timeline\_labels.CSV} file, which includes \texttt{filename} and \texttt{label} columns. Cutouts were also created from each keyframe, capturing frames up to 30 seconds prior, stored as \texttt{mp4} files in the `cutouts' folder. The \texttt{timeline\_labels.CSV} file was extended with a \texttt{remaining\_time} column to denote the remaining surgical time for each keyframe and cutout.

\vspace{2mm}
The detailed steps of this post-processing approach is outlined in Algorithm~\ref{alg:post_processing}, which provides a comprehensive overview of the procedures for extracting keyframes, mapping labels, and creating video cutouts. This algorithm ensures that the dataset is ready for machine learning applications by systematically processing the raw video data and annotations.



% \begin{table}[htbp]
%     \centering
%     \caption{Label annotations with keyframe timestamps.}
%     \label{tab:keyframe_timestamps}
%     \begin{tabular}{llll}
%         \toprule
%         start              & end                & label          & keyframe\_timestamp \\
%         \midrule
%         0                  & 12.248005          & Setup          & 0.025000            \\
%         0                  & 62.52621778886119  & Setup          & 0.025000            \\
%         63.5177162598092   & 161.08017677518743 & Landmarks      & 63.825000           \\
%         160.0886783042394  & 204.28824605153787 & Scope movement & 163.185000          \\
%         203.74922693266836 & 267.35348295926855 & Landmarks      & 204.225000          \\
%         \bottomrule
%     \end{tabular}
% \end{table}

% \subsection*{Video Annotation}
% We installed Label Studio on a server on a virtual private network to annotate anonymised ESV clips, providing external access for our clinical partners. Initially, we defined discrete labels to capture surgical phases at a high level in TEMS for the annotation task. Notably, we utilized the audio UI component to facilitate efficient segmentation of the surgical timeline, which is particularly beneficial for clinical partners annotating extensive surgical datasets. However, we encountered a limitation with Label Studio's lack of a feature to intuitively upload multiple surgeries (each comprising numerous clips). To address this, we created separate projects for each TEMS surgery, uploaded all ESV clips into the respective projects, and configured Label Studio for each project based on the proposed annotation protocols. We used ffmpeg to reduce the sizes of these ESV clips, facilitating their upload into Label Studio's database without exceeding the upload limit. 

% \vspace{2mm}
% \noindent
% Once projects were set up for each surgery, we shared the Label Studio link with surgeons, enabling them to join the annotation task by registering into the platform. Surgeons accessed the Label Studio service remotely via a secure web browser. They annotated the timeline for ESV clips based on defined labels by simply click-dragging over the audio timeline component. The surgical timeline was segmented according to the annotation protocol, which was thoroughly explained to the annotation team of experienced surgeons. Projects were distributed among these surgeons, with a primary annotator annotating the video, followed by review and verification by other surgeons. Surgeons organised follow-up meetings in conflicting labels to discuss disagreements and resolve such issues based on the established principles. Once all videos were fully annotated, we exported the labels in JSON files, which were then used to create our dataset. We devised an algorithm to process ESV clips and JSON annotation files to create the final dataset. Listing xxx shows our proposed dataset generation algorithm. The algorithm takes all ESV clips for a given surgery and then extracts a 30-second video of the keyframe by cropping frames with a 30-second clip preceding it. In this manner, we finalised the creation of our dataset for ML model development.

% % \State \textbf{return} $E_s$
% \subsubsection*{Surgical Timeline Segmentation}
% Surgical procedures must be captured in high quality as they contain complex and tiny actions. Surgical videos often take longer to process, hence each surgical video may be divided into short clips. These short clips, in combination with clips of other videos of the same surgical process, represent the whole surgery. To evaluate a surgical process, all the videos of surgery are investigated to check each activity, task, action, etc. This study analyses endoscopic recordings of colorectal cancer surgeries, focusing on hierarchical surgical timeline segmentation. The first step of the proposed study is to develop an annotated surgical timeline dataset for a segmentation model. Therefore, We first asked domain experts to provide us with 10 annotated endoscopic videos for preliminary analysis. The selected surgical videos were extremely high quality, requiring substantial processing and storage resources. For this reason, we apply the bitrate compression technique to reduce the size of the original videos without compromising their quality. A comparison between the original and compressed video has already been shown in \Cref{fig:before_processing} and \Cref{fig:after_processing}. 

% \vspace{2mm}
% \noindent
% In addition, the compressed video clips were uploaded to Label Studio for label annotation provided by domain experts. Label studio supports videos that have an audio track to display a timeline, therefore, FFMPEG\footnotemark{}\footnotetext{https://ffmpeg.org/}, an audio and video processing tool was used to find the audio track for all videos. The processed video clips were then uploaded to Label Studio to prepare training data for segmentation. Also, the annotated video clips are further processed to generate cutouts and a CSV folder containing information about these cutouts (Explained in the Data Record Section). In the entire process of dataset development, video clips from various surgeries are processed to generate 5-second cutouts, each containing a specific representation of activity, task, and action based on the input video timeline.

% \vspace{2mm}
% \noindent
% The developed dataset may be used for various automated tasks, including surgical scene understanding, surgeon assessment, tool recognition, etc. In this study, we aim to focus on surgical timeline segmentation and evaluate the whole surgical ``end-to-end'' process to deeply understand the overall scene. To achieve the target, we first recognise surgical phases such as SETUP, DISSECTION, etc. In each of these phases, many discrete tasks can contribute to the completion of surgical activity, such as preparing the surgical site and controlling bleeding.   

% \vspace{2mm}
% \noindent
% Moreover, a task can have one or more actions, such as INCISION, SUTURING, etc. Each of these actions, tasks, and phases is investigated to understand surgical procedure. \Cref{fig:Timeline} shows the architecture of the proposed surgical dataset and timeline segmentation model.
% %Our assessment criteria centred around five critical skills: depth perception, tissue handling, bimanual dexterity, difficulty management, and efficiency. These skills are evaluated based on a star rating system, where one star is the lowest score, and five stars are the highest score.  %After assessing team skills, feedback can be given as ``positive'' and ``negative'' surgical actions. This enables key recommendations to be suggested for improving a particular surgical action. 

% \begin{table}[]
% \centering
% \caption{TEMS Reference Taxonomy for Surgerical Timeline Automation}
% \label{tab:tab2}
% \footnotesize 
% \setlength{\tabcolsep}{3pt} 
% \renewcommand{\arraystretch}{1.2}
% \begin{adjustbox}{max width=\textwidth}
% \begin{tabular}{|
% >{\columncolor[HTML]{8EA9DB}}l |
% >{\columncolor[HTML]{B4C6E7}}l |
% >{\columncolor[HTML]{D9E1F2}}l |
% >{\columncolor[HTML]{D9E1F2}}c |
% >{\columncolor[HTML]{D9E1F2}}c |c|cccccccl|}
% \hline
% \multicolumn{1}{|c|}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} }} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} }} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} }} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} } & \cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} } & \cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} } & \multicolumn{8}{c|}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} \textbf{Skill Assessment}}} \\ \cline{7-14} 
% \multicolumn{1}{|c|}{\multirow{-2}{*}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} \textbf{Phase}}}} & \multicolumn{1}{c|}{\multirow{-2}{*}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} \textbf{Task}}}} & \multicolumn{1}{c|}{\multirow{-2}{*}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} \textbf{Action}}}} & \multirow{-2}{*}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} \textbf{Start Time}}} & \multirow{-2}{*}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} \textbf{End Time}}} & \multirow{-2}{*}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} \textbf{Adverse Event}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} \textbf{System}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} \textbf{Dexterity}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} \textbf{Efficiency}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} \textbf{Tissue Handling}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} \textbf{Depth Perception}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} \textbf{Difficulty}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} \textbf{Skill Score}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} \textbf{Desribe why you choose the assessment mark?}}} \\ \hline
% \cellcolor[HTML]{FFD966} & \cellcolor[HTML]{FFE699} & \cellcolor[HTML]{FFF2CC}Scope Insertion & \cellcolor[HTML]{FFF2CC}0:00 & \cellcolor[HTML]{FFF2CC}2:30 & $\times$ & \multicolumn{1}{c|}{GOALS} & 5 & 2 & 3 & 4 & 4 & \multicolumn{1}{c|}{4} & The trainee needs to work more on inserting scope through   anal verge. \\ \cline{3-7}
% \cellcolor[HTML]{FFD966} & \cellcolor[HTML]{FFE699} & \cellcolor[HTML]{FFF2CC}Scope Positioning & \cellcolor[HTML]{FFF2CC}2:31 & \cellcolor[HTML]{FFF2CC}3:13 & $\times$ & \multicolumn{1}{c|}{GOALS} & 5 & 2 & 3 & 5 & 5 & \multicolumn{1}{c|}{4} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{FFD966} & \cellcolor[HTML]{FFE699} & \cellcolor[HTML]{FFF2CC}No Action & \cellcolor[HTML]{FFF2CC}n/a & \cellcolor[HTML]{FFF2CC}n/a & $\times$ & \multicolumn{1}{c|}{GOALS} & 1 & 1 & 3 & 2 & 2 & \multicolumn{1}{c|}{2} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{FFD966} & \multirow{-4}{*}{\cellcolor[HTML]{FFE699}Scope} & \cellcolor[HTML]{FFF2CC}Out of Body & \cellcolor[HTML]{FFF2CC}n/a & \cellcolor[HTML]{FFF2CC}n/a & \checkmark & \multicolumn{1}{c|}{GOALS} & 3 & 3 & 4 & 3 & 5 & \multicolumn{1}{c|}{4} & n/a \\ \cline{2-14} 
% \cellcolor[HTML]{FFD966} & \cellcolor[HTML]{FFE699} & \cellcolor[HTML]{FFF2CC}Instrument Insertion & \cellcolor[HTML]{FFF2CC} & \cellcolor[HTML]{FFF2CC} & $\times$ & \multicolumn{1}{c|}{GOALS} & 4 & 2 & 3 & 2 & 2 & \multicolumn{1}{c|}{3} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{FFD966} & \cellcolor[HTML]{FFE699} & \cellcolor[HTML]{FFF2CC}Instrument Positioning & \cellcolor[HTML]{FFF2CC} & \cellcolor[HTML]{FFF2CC} & $\times$ & \multicolumn{1}{c|}{GOALS} & 2 & 3 & 3 & 5 & 2 & \multicolumn{1}{c|}{3} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{FFD966} & \cellcolor[HTML]{FFE699} & \cellcolor[HTML]{FFF2CC}No Action & \cellcolor[HTML]{FFF2CC} & \cellcolor[HTML]{FFF2CC} & $\times$ & \multicolumn{1}{c|}{GOALS} & 1 & 2 & 5 & 2 & 1 & \multicolumn{1}{c|}{2} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{FFD966} & \multirow{-4}{*}{\cellcolor[HTML]{FFE699}Instrument} & \cellcolor[HTML]{FFF2CC}Out of Body & \cellcolor[HTML]{FFF2CC} & \cellcolor[HTML]{FFF2CC} & \checkmark & \multicolumn{1}{c|}{GOALS} & 3 & 3 & 2 & 2 & 5 & \multicolumn{1}{c|}{3} & n/a \\ \cline{2-14} 
% \cellcolor[HTML]{FFD966} & \cellcolor[HTML]{FFE699} & \cellcolor[HTML]{FFF2CC}Fluid Wash & \cellcolor[HTML]{FFF2CC} & \cellcolor[HTML]{FFF2CC} & $\times$ & \multicolumn{1}{c|}{GOALS} & 4 & 1 & 4 & 5 & 2 & \multicolumn{1}{c|}{3} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{FFD966} & \cellcolor[HTML]{FFE699} & \cellcolor[HTML]{FFF2CC}Debris Wash & \cellcolor[HTML]{FFF2CC} & \cellcolor[HTML]{FFF2CC} & $\times$ & \multicolumn{1}{c|}{GOALS} & 4 & 5 & 3 & 2 & 1 & \multicolumn{1}{c|}{3} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{FFD966} & \cellcolor[HTML]{FFE699} & \cellcolor[HTML]{FFF2CC}No Action & \cellcolor[HTML]{FFF2CC} & \cellcolor[HTML]{FFF2CC} & $\times$ & \multicolumn{1}{c|}{GOALS} & 5 & 5 & 4 & 5 & 3 & \multicolumn{1}{c|}{4} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{FFD966} & \multirow{-4}{*}{\cellcolor[HTML]{FFE699}Site} & \cellcolor[HTML]{FFF2CC}Out of Body & \cellcolor[HTML]{FFF2CC} & \cellcolor[HTML]{FFF2CC} & \checkmark & \multicolumn{1}{c|}{GOALS} & 1 & 4 & 4 & 4 & 5 & \multicolumn{1}{c|}{4} & n/a \\ \cline{2-14} 
% \cellcolor[HTML]{FFD966} & \cellcolor[HTML]{FFE699} & \cellcolor[HTML]{FFF2CC}Inflate Rectum & \cellcolor[HTML]{FFF2CC} & \cellcolor[HTML]{FFF2CC} & $\times$ & \multicolumn{1}{c|}{GOALS} & 3 & 3 & 1 & 4 & 1 & \multicolumn{1}{c|}{2} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{FFD966} & \cellcolor[HTML]{FFE699} & \cellcolor[HTML]{FFF2CC}Deflate Rectum & \cellcolor[HTML]{FFF2CC} & \cellcolor[HTML]{FFF2CC} & $\times$ & \multicolumn{1}{c|}{GOALS} & 4 & 2 & 1 & 2 & 1 & \multicolumn{1}{c|}{2} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{FFD966} & \cellcolor[HTML]{FFE699} & \cellcolor[HTML]{FFF2CC}No Action & \cellcolor[HTML]{FFF2CC} & \cellcolor[HTML]{FFF2CC} & $\times$ & \multicolumn{1}{c|}{GOALS} & 5 & 2 & 2 & 3 & 1 & \multicolumn{1}{c|}{3} & n/a \\ \cline{3-7}
% \multirow{-16}{*}{\cellcolor[HTML]{FFD966}\textbf{Setup}} & \multirow{-4}{*}{\cellcolor[HTML]{FFE699}Pressure} & \cellcolor[HTML]{FFF2CC}Out of Body & \cellcolor[HTML]{FFF2CC} & \cellcolor[HTML]{FFF2CC} & \checkmark & \multicolumn{1}{c|}{GOALS} & 3 & 2 & 2 & 3 & 5 & \multicolumn{1}{c|}{3} & n/a \\ \hline
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Marking &  &  & $\times$ & \multicolumn{1}{c|}{GOALS} & 3 & 4 & 4 & 5 & 4 & \multicolumn{1}{c|}{4} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Retraction &  &  & $\times$ & \multicolumn{1}{c|}{GOALS} & 1 & 2 & 1 & 5 & 2 & \multicolumn{1}{c|}{2} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Smoke &  &  & \checkmark & \multicolumn{1}{c|}{GOALS} & 2 & 1 & 2 & 5 & 2 & \multicolumn{1}{c|}{2} & n/a \\ \cline{3-7} \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Bleeding &  &  & \checkmark & \multicolumn{1}{c|}{GOALS} & 1 & 4 & 4 & 5 & 1 & \multicolumn{1}{c|}{3} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Haemostatis &  &  & $\times$ & \multicolumn{1}{c|}{GOALS} & 3 & 4 & 1 & 3 & 4 & \multicolumn{1}{c|}{3} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Washout &  &  & $\times$ & \multicolumn{1}{c|}{GOALS} & 1 & 2 & 1 & 2 & 1 & \multicolumn{1}{c|}{1} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Aspiration &  &  & $\times$ & \multicolumn{1}{c|}{GOALS} & 5 & 5 & 5 & 4 & 2 & \multicolumn{1}{c|}{4} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Scope Positioning &  &  & $\times$ & \multicolumn{1}{c|}{GOALS} & 3 & 3 & 4 & 5 & 3 & \multicolumn{1}{c|}{4} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Instrument Positioning &  &  & $\times$ & \multicolumn{1}{c|}{GOALS} & 3 & 1 & 1 & 2 & 1 & \multicolumn{1}{c|}{2} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & No Action &  &  & \checkmark & \multicolumn{1}{c|}{GOALS} & 2 & 2 & 4 & 1 & 2 & \multicolumn{1}{c|}{2} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \multirow{-11}{*}{\cellcolor[HTML]{B4C6E7}Landmarking} & Out of Body &  &  & $\times$ & \multicolumn{1}{c|}{GOALS} & 5 & 2 & 4 & 3 & 2 & \multicolumn{1}{c|}{3} & n/a \\ \cline{2-14} 
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Dissection &  &  & $\times$ & \multicolumn{1}{c|}{GOALS} & 3 & 3 & 5 & 3 & 3 & \multicolumn{1}{c|}{3} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Retraction &  &  & $\times$ & \multicolumn{1}{c|}{GOALS} & 4 & 3 & 3 & 5 & 2 & \multicolumn{1}{c|}{3} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Smoke &  &  & \checkmark & \multicolumn{1}{c|}{GOALS} & 5 & 2 & 3 & 4 & 4 & \multicolumn{1}{c|}{4} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Bleeding &  &  & \checkmark & \multicolumn{1}{c|}{GOALS} & 3 & 4 & 5 & 1 & 3 & \multicolumn{1}{c|}{3} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Haemostatis &  &  & $\times$ & \multicolumn{1}{c|}{GOALS} & 2 & 3 & 4 & 2 & 4 & \multicolumn{1}{c|}{3} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Washout &  &  & $\times$ & \multicolumn{1}{c|}{GOALS} & 4 & 3 & 3 & 1 & 2 & \multicolumn{1}{c|}{3} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Aspiration &  &  & $\times$ & \multicolumn{1}{c|}{GOALS} & 1 & 3 & 5 & 1 & 4 & \multicolumn{1}{c|}{3} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Scope Positioning &  &  & $\times$ & \multicolumn{1}{c|}{GOALS} & 3 & 5 & 1 & 2 & 3 & \multicolumn{1}{c|}{3} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Instrument Positioning &  &  & $\times$ & \multicolumn{1}{c|}{GOALS} & 3 & 5 & 1 & 4 & 2 & \multicolumn{1}{c|}{3} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & No Action &  &  & $\times$ & \multicolumn{1}{c|}{GOALS} & 5 & 1 & 1 & 2 & 4 & \multicolumn{1}{c|}{3} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \multirow{-11}{*}{\cellcolor[HTML]{B4C6E7}Mucosal Dissection} & Out of Body &  &  & $\times$ & \multicolumn{1}{c|}{GOALS} & 4 & 4 & 4 & 2 & 1 & \multicolumn{1}{c|}{3} & n/a \\ \cline{2-14} 
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Dissection &  &  & $\times$ & \multicolumn{1}{c|}{GOALS} & 3 & 3 & 3 & 4 & 2 & \multicolumn{1}{c|}{3} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Retraction &  &  & $\times$ & \multicolumn{1}{c|}{GOALS} & 2 & 3 & 2 & 4 & 1 & \multicolumn{1}{c|}{2} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Smoke &  &  & \checkmark & \multicolumn{1}{c|}{GOALS} & 4 & 4 & 2 & 3 & 2 & \multicolumn{1}{c|}{3} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Bleeding &  &  & \checkmark & \multicolumn{1}{c|}{GOALS} & 1 & 2 & 2 & 3 & 1 & \multicolumn{1}{c|}{2} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Haemostasis &  &  & $\times$ & \multicolumn{1}{c|}{GOALS} & 2 & 2 & 1 & 1 & 4 & \multicolumn{1}{c|}{2} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Washout &  &  & $\times$ & \multicolumn{1}{c|}{GOALS} & 5 & 2 & 1 & 1 & 1 & \multicolumn{1}{c|}{2} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Aspiration &  &  & $\times$ & \multicolumn{1}{c|}{GOALS} & 5 & 1 & 1 & 4 & 4 & \multicolumn{1}{c|}{3} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Scope Positioning &  &  & $\times$ & \multicolumn{1}{c|}{GOALS} & 1 & 1 & 3 & 3 & 2 & \multicolumn{1}{c|}{2} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Instrument Positioning &  &  & $\times$ & \multicolumn{1}{c|}{GOALS} & 3 & 4 & 5 & 4 & 4 & \multicolumn{1}{c|}{4} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & No Action &  &  & $\times$ & \multicolumn{1}{c|}{GOALS} & 3 & 4 & 4 & 4 & 5 & \multicolumn{1}{c|}{4} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \multirow{-11}{*}{\cellcolor[HTML]{B4C6E7}Submucosal Dissection} & Out of Body &  &  & $\times$ & \multicolumn{1}{c|}{GOALS} & 5 & 5 & 1 & 1 & 2 & \multicolumn{1}{c|}{3} & n/a \\ \cline{2-14} 
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Dissection &  &  & $\times$ & \multicolumn{1}{c|}{GOALS} & 3 & 4 & 2 & 3 & 4 & \multicolumn{1}{c|}{3} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Retraction &  &  & $\times$ & \multicolumn{1}{c|}{GOALS} & 4 & 5 & 1 & 5 & 5 & \multicolumn{1}{c|}{4} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Smoke &  &  & \checkmark & \multicolumn{1}{c|}{GOALS} & 2 & 2 & 3 & 3 & 1 & \multicolumn{1}{c|}{2} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Bleeding &  &  & \checkmark & \multicolumn{1}{c|}{GOALS} & 1 & 4 & 2 & 3 & 3 & \multicolumn{1}{c|}{3} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Haemostasis &  &  & $\times$ & \multicolumn{1}{c|}{GOALS} & 1 & 3 & 3 & 5 & 1 & \multicolumn{1}{c|}{3} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Washout &  &  & $\times$ & \multicolumn{1}{c|}{GOALS} & 5 & 4 & 5 & 1 & 5 & \multicolumn{1}{c|}{4} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Aspiration &  &  & $\times$ & \multicolumn{1}{c|}{GOALS} & 5 & 2 & 2 & 2 & 2 & \multicolumn{1}{c|}{3} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Scope Positioning &  &  & $\times$ & \multicolumn{1}{c|}{GOALS} & 1 & 4 & 2 & 4 & 1 & \multicolumn{1}{c|}{2} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Instrument Positioning &  &  & $\times$ & \multicolumn{1}{c|}{GOALS} & 2 & 3 & 4 & 3 & 5 & \multicolumn{1}{c|}{3} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & No Action &  &  & $\times$ & \multicolumn{1}{c|}{GOALS} & 4 & 2 & 4 & 4 & 1 & \multicolumn{1}{c|}{3} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \multirow{-11}{*}{\cellcolor[HTML]{B4C6E7}Circular Muscle Dissection} & Out of Body &  &  & $\times$ & \multicolumn{1}{c|}{GOALS} & 3 & 1 & 4 & 3 & 1 & \multicolumn{1}{c|}{2} & n/a \\ \cline{2-14} 
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Dissection &  &  & $\times$ & \multicolumn{1}{c|}{GOALS} & 5 & 4 & 1 & 2 & 4 & \multicolumn{1}{c|}{3} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Retraction &  &  & $\times$ & \multicolumn{1}{c|}{GOALS} & 5 & 4 & 2 & 4 & 1 & \multicolumn{1}{c|}{3} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Smoke &  &  & \checkmark & \multicolumn{1}{c|}{GOALS} & 4 & 1 & 4 & 1 & 4 & \multicolumn{1}{c|}{3} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Bleeding &  &  & \checkmark & \multicolumn{1}{c|}{GOALS} & 2 & 5 & 5 & 3 & 1 & \multicolumn{1}{c|}{3} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Haemostasis &  &  & $\times$ & \multicolumn{1}{c|}{GOALS} & 4 & 4 & 5 & 4 & 1 & \multicolumn{1}{c|}{4} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Washout &  &  & $\times$ & \multicolumn{1}{c|}{GOALS} & 5 & 4 & 2 & 2 & 5 & \multicolumn{1}{c|}{4} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Aspiration &  &  & $\times$ & \multicolumn{1}{c|}{GOALS} & 1 & 1 & 1 & 5 & 4 & \multicolumn{1}{c|}{2} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Scope Positioning &  &  & $\times$ & \multicolumn{1}{c|}{GOALS} & 1 & 3 & 1 & 1 & 5 & \multicolumn{1}{c|}{2} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Instrument Positioning &  &  & $\times$ & \multicolumn{1}{c|}{GOALS} & 2 & 5 & 2 & 4 & 2 & \multicolumn{1}{c|}{3} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & No Action &  &  & $\times$ & \multicolumn{1}{c|}{GOALS} & 2 & 2 & 5 & 4 & 1 & \multicolumn{1}{c|}{3} & n/a \\ \cline{3-7}
% \multirow{-55}{*}{\cellcolor[HTML]{8EA9DB}\textbf{Dissection}} & \multirow{-11}{*}{\cellcolor[HTML]{B4C6E7}Longitudinal Muscle Dissection} & Out of Body &  &  & $\times$ & \multicolumn{1}{c|}{GOALS} & 3 & 1 & 5 & 1 & 5 & \multicolumn{1}{c|}{3} & n/a \\ \hline
% \cellcolor[HTML]{F4B084}\textbf{Specimen   Removal} & \cellcolor[HTML]{F8CBAD}Specimen Removal & \cellcolor[HTML]{FCE4D6}Specimen Removal & \cellcolor[HTML]{FCE4D6} & \cellcolor[HTML]{FCE4D6} & $\times$ & \multicolumn{1}{c|}{GOALS} & 1 & 3 & 4 & 2 & 3 & \multicolumn{1}{c|}{3} & n/a \\ \hline
% \cellcolor[HTML]{A9D08E} & \cellcolor[HTML]{C6E0B4} & \cellcolor[HTML]{E2EFDA}Stitching & \cellcolor[HTML]{E2EFDA} & \cellcolor[HTML]{E2EFDA} & $\times$ & \multicolumn{1}{c|}{GOALS} & 5 & 5 & 1 & 2 & 1 & \multicolumn{1}{c|}{3} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{A9D08E} & \cellcolor[HTML]{C6E0B4} & \cellcolor[HTML]{E2EFDA}Clipping Suture & \cellcolor[HTML]{E2EFDA} & \cellcolor[HTML]{E2EFDA} & $\times$ & \multicolumn{1}{c|}{GOALS} & 1 & 3 & 5 & 5 & 3 & \multicolumn{1}{c|}{3} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{A9D08E} & \cellcolor[HTML]{C6E0B4} & \cellcolor[HTML]{E2EFDA}Smoke & \cellcolor[HTML]{E2EFDA} & \cellcolor[HTML]{E2EFDA} & $\times$ & \multicolumn{1}{c|}{GOALS} & 2 & 1 & 5 & 5 & 1 & \multicolumn{1}{c|}{3} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{A9D08E} & \cellcolor[HTML]{C6E0B4} & \cellcolor[HTML]{E2EFDA}Bleeding & \cellcolor[HTML]{E2EFDA} & \cellcolor[HTML]{E2EFDA} & $\times$ & \multicolumn{1}{c|}{GOALS} & 4 & 2 & 2 & 3 & 2 & \multicolumn{1}{c|}{3} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{A9D08E} & \cellcolor[HTML]{C6E0B4} & \cellcolor[HTML]{E2EFDA}Haemostatis & \cellcolor[HTML]{E2EFDA} & \cellcolor[HTML]{E2EFDA} & $\times$ & \multicolumn{1}{c|}{GOALS} & 2 & 2 & 2 & 1 & 4 & \multicolumn{1}{c|}{2} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{A9D08E} & \cellcolor[HTML]{C6E0B4} & \cellcolor[HTML]{E2EFDA}Washout & \cellcolor[HTML]{E2EFDA} & \cellcolor[HTML]{E2EFDA} & $\times$ & \multicolumn{1}{c|}{GOALS} & 3 & 4 & 1 & 3 & 3 & \multicolumn{1}{c|}{3} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{A9D08E} & \cellcolor[HTML]{C6E0B4} & \cellcolor[HTML]{E2EFDA}Aspiration & \cellcolor[HTML]{E2EFDA} & \cellcolor[HTML]{E2EFDA} & $\times$ & \multicolumn{1}{c|}{GOALS} & 2 & 3 & 4 & 3 & 3 & \multicolumn{1}{c|}{3} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{A9D08E} & \cellcolor[HTML]{C6E0B4} & \cellcolor[HTML]{E2EFDA}Scope Positioning & \cellcolor[HTML]{E2EFDA} & \cellcolor[HTML]{E2EFDA} & $\times$ & \multicolumn{1}{c|}{GOALS} & 2 & 3 & 1 & 2 & 5 & \multicolumn{1}{c|}{3} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{A9D08E} & \cellcolor[HTML]{C6E0B4} & \cellcolor[HTML]{E2EFDA}Instrument Positioning & \cellcolor[HTML]{E2EFDA} & \cellcolor[HTML]{E2EFDA} & $\times$ & \multicolumn{1}{c|}{GOALS} & 1 & 3 & 4 & 5 & 3 & \multicolumn{1}{c|}{3} & n/a \\ \cline{3-7}
% \cellcolor[HTML]{A9D08E} & \cellcolor[HTML]{C6E0B4} & \cellcolor[HTML]{E2EFDA}No Action & \cellcolor[HTML]{E2EFDA} & \cellcolor[HTML]{E2EFDA} & $\times$ & \multicolumn{1}{c|}{GOALS} & 4 & 2 & 2 & 4 & 4 & \multicolumn{1}{c|}{3} & n/a \\ \cline{3-7}
% \multirow{-11}{*}{\cellcolor[HTML]{A9D08E}\textbf{Closure}} & \multirow{-11}{*}{\cellcolor[HTML]{C6E0B4}Suturing} & \cellcolor[HTML]{E2EFDA}Out of Body & \cellcolor[HTML]{E2EFDA} & \cellcolor[HTML]{E2EFDA} & $\times$ & \multicolumn{1}{c|}{GOALS} & 4 & 3 & 5 & 4 & 5 & \multicolumn{1}{c|}{4} & n/a \\ \hline
% \cellcolor[HTML]{AEAAAA}\textbf{Scope Removal} & \cellcolor[HTML]{AEAAAA}Scope removal & \cellcolor[HTML]{AEAAAA}Scope Removal & \cellcolor[HTML]{AEAAAA} & \cellcolor[HTML]{AEAAAA} & $\times$ & \multicolumn{1}{c|}{GOALS} & 4 & 2 & 5 & 5 & 1 & \multicolumn{1}{c|}{3} & n/a \\ \hline
% \end{tabular}
% \end{adjustbox}
% \end{table}



% \subsection*{Data Records}
% The dataset is available at the self-hosted instance of Label Studio \url{https://labelstud.io/slef-hosted/ESV}. The data is recorded in a hierarchical form, represented as follows: The input directory `Dataset' contains videos of each ESV surgery. The video directory holds various endoscopic recordings of surgical procedures. Each video (e.g., surg-001) in the videos directory contains subdirectories such as 'original' and 'mini'. These two directories contain short clips of the corresponding surgical procedures. Original clips are the ones that have large sizes and high resolution, whereas mini clips are the compressed form of the original clips. Within the 'mini' directory are five subdirectories: frames, cutouts, timeline\_labels.csv, cutout-frames, and surg-001.json. The frames folder contains .png images extracted from individual video clips, each representing a keyframe, denoting significant changes in the surgical procedure. The cutouts directory consists of shorter clips of keyframes (each keyframe and its preceding 5 seconds of frames). Each cutout is named with the name of the video clip plus the frame range used to form the corresponding cutout (e.g., video1.F10-F99). Additionally, the CSV labels folder contains CSV files for each cutout. CSV files hold information such as the name of the video clip from which the cutout was composed and the activity, task, and action depicted in that cutout. \hl{\textbf{The cutout-frames directory contains frame images specific to each cutout.}} Finally, the .json directory contains timeline annotations of the input video clips. The overall data hierarchy is shown in figure \ref{record}.

\subsection*{Data Records}
The intention of this project is to release the \texttt{TEMSET-25K} dataset to the wider machine learning community to advance research and innovation in surgical data science. The dataset is hosted on \url{https://researchdata.uhb.nhs.uk/pathway-research-data-hub/} under restricted access and requires an application to the UHB Research and Development PATHWAY team. The dataset comes in a zipped \texttt{temset} folder, which is approximately 50 gigabytes in size. This zipped file includes several subfolders: \texttt{videos}, \texttt{frames}, \texttt{cutouts}, and \texttt{cutout-frames}.

\vspace{2mm}
The \texttt{videos} folder contains subfolders for the original video recordings of surgical procedures. Each subfolder is assigned a globally unique identifier (GUID) for pseudonymisation purposes, linked to its original name. For ease of organisation and security, GUIDs are mapped to user-friendly names such as \texttt{surg-001}, \texttt{surg-002}, etc. Within each surgery folder (e.g., \texttt{surg-001}), there is a subfolder \texttt{originals} that holds the high-resolution, multi-part ESV files. These video clips are compressed using a bespoke data processing pipeline to roughly one-tenth of their original size, facilitating the handling of large ESV files. The compressed video clips are stored under the \texttt{videos} folder for ease of code-based access and manipulation. To avoid ambiguity, the video clip names are retained post-miniaturisation. Additionally, each surgery folder includes a JSON file (e.g., \texttt{surg-001.json}) exported from the LS server. This file contains expert-annotated timeline segmentation labels for each multi-part ESV video clip, utilising our proposed dense TEMS taxonomy. The JSON files are named according to the corresponding surgery folder, allowing the retrieval of these files grammatically in a uniform way.

\vspace{2mm}
The \texttt{frames} folder comprises selected frames extracted by our proposed extraction algorithm to create a dataset suitable for machine learning tasks. These frames are intelligently extracted to capture significant scene changes in the videos. The \texttt{cutouts} folder contains smaller segments extracted from the multi-part ESV video clips. Each cutout is a 30-second video clip capturing the segment immediately preceding a given frame. Additionally, the \texttt{cutout-frames} folder provides the individual frames extracted from these cutout video clips. This offline extraction of frames significantly increases the dataset size but reduces training time substantially by avoiding repetitive video splitting during each epoch.

\begin{figure}[!t]
    \centering
    \includegraphics[width=1\textwidth]{SD_Method.png}
    \caption{\textbf{Methodology adopted for annotating TEMS surgical videos for surgical timeline segmentation includes six major steps:} \textbf{(1)} \textit{TEMS surgical data acquisition \textbf{(2)} Data Preprocessing, \textbf{(3)} Data Annotation, \textbf{(4)} Annotation Verification, \textbf{(5)} Data Post Processing, and \textbf{(6)} Surgical data preparation for training timeline segmentation models. }}
    \label{fig:Timeline}
\end{figure}

\vspace{2mm}
The \texttt{timeline\_labels.CSV} file, located in the \texttt{temset} folder, stores the labels for all cutouts and frames. Each entry in the \texttt{filename} column comprises the surgical folder, video clip filename, frame number, and timestamp, making each file uniquely identifiable and facilitating reproducibility by enabling tracking back to the original video clips. The associated labels for these cutouts and frames are presented both as dot-separated triplets and in individual columns for single and multi-target formulations, respectively. These labels are stored in the columns \texttt{timeline\_label}, \texttt{timeline\_phase\_label}, \texttt{timeline\_task\_label}, and \texttt{timeline\_action\_label}. Additionally, \texttt{timeline\_labels.CSV} includes a \texttt{time\_to\_finish} column, which captures the remaining time of the surgery. This is calculated by subtracting the frame/cutout time from the total surgery duration, which encompasses the combined length of all multi-part ESV videos. This column is included to facilitate research in creating models that can predict the remaining time for a procedure in surgical videos, offering numerous clinical applications.

\vspace{2mm}
Overall, \texttt{TEMSET-25K} includes $25,835$ expertly annotated cutouts and frames, densely annotated by expert clinicians using timeline triplet labels. The entire data preparation, labelling, and construction pipeline is summarised in Fig \ref{fig:Timeline}.




% \begin{figure}[!htbp]
%     \centering
%     \caption{Data Hierarchy in Label Studio}
%         \centering
%         \includegraphics[width=10cm, height=7cm]{DataRecord.png}
%     \label{record}
% \end{figure}

%In a certain clip, when a keyframe is found, we then take the next 60 seconds of the input clips to generate a cutout. A single cutout may have more than a hundred frames depending on the frames within that 60-second duration. Mathematically, a cutout can be represented as ( Equation \ref{eq:cout}):
%\begin{equation} 
   % C = \scalebox{2}{$\displaystyle\sum_{n=1}^{N}$} (FK_{s} + n + FK_{f}) 
   % \label{eq:cout}
%\end{equation}

%where "n" represents the number of frames, $Fk_{s}$ is the starting keyframe of the 60 seconds, $Fk_{f}$ is the final keyframe of the 60 seconds. 



%After producing the initial cutout, our algorithm traverses the remaining frames to extract keyframes and generate additional cutouts. This iterative process extends to all ESV clips within the input folder, resulting in distinct output folders dedicated to both keyframes and cutouts. The cutouts are subsequently aligned with the JSON file containing labels and the timeline of the input ESV clip. Utilizing the information from the JSON file, each cutout is accurately annotated, explaining the activity, task and action. Furthermore, the annotations within these cutouts are stored in a CSV folder containing cutout labels alongside the corresponding activity, task, and action depicted in each cutout.




\section*{Technical Validation}

\subsection*{Annotation Assessment}
To ensure the consistency of labelling in the dataset, we designed an annotation process involving a team of colorectal cancer surgery specialists, all accredited with fellowship status with the Royal College of Surgeons (RCS, UK). The process began with one surgeon annotating one full video in a shared setting to demonstrate the annotation procedure for the multi-part ESV files. Following this, another surgeon logged into the LS server using their credentials and navigated to the project they intended to annotate, accessing the individual video clips for annotation. The LS user interface provided a comma-separated list of phases, tasks, and actions for annotating the timeline of each video clip. Annotations were initially performed by one surgeon and subsequently validated by at least two other surgeons for cross-checking purposes. In cases of conflicting boundaries between the start and end of the labelling triplets, discussions were held to finalize the annotations that was agreed by all surgeons. We employed multifaceted strategies involving our proposed dense taxonomy, collaboratively annotating one full surgery in shared settings, and holding iterative discussions to resolve conflicts to achieve consistent annotations of the complex workflow scenes based on all surgeons' inputs. The final annotations consisted of labels made up of five phases, 12 tasks, and 21 actions as defined by the proposed taxonomy. These annotations were then programmatically exported from LS in JSON format, along with the corresponding ESV files.

% \subsection*{Evaluation Metrics}
% To evaluate the performance of our models for timeline segmentation, we use precision (PR), recall (RE), and accuracy (ACC). These metrics have been widely used for surgical workflow analysis\cite{demir2023deep}. Using the phase-wise set of Ground Truth
% (GT) sample-label pairs and set of Prediction (P) sample-label, These metrics can be expressed as:

% \begin{equation}
% \begin{aligned}
% PR = \frac{|P \cap GT|}{|P|}, 
% RE = \frac{|GT \cap P|}{|GT|}
% ,\text{F1-Score} = 2 \cdot \frac{\text{PR} \cdot \text{RE}}{\text{PR} + \text{RE}},
% \text{ACC} = \left( \frac{|GT \cap P|}{|GT|} \right) \times 100\%
% \end{aligned}
% \end{equation}
% The ACC is the common metric that shows the percentage of frames correctly classified in the ground truth labels.
%Another metric used in the evaluation is mean average precision (mAP). mAP is widely adopted for evaluating action recognition in videos \cite{gu2018ava} and is denoted by:
%{\begin{equation}
    %\text{mAP} = \frac{1}{N} \sum_{k=1}^{N} \text{AP}_k
%\end{equation} 
%where N is the number of classes or categories, and AP is the average precision of class k.}
\subsection*{Deep Learning Model Training}
\subsubsection*{Data Pre-Processing}
To improve the field of view, irrelevant areas were cropped from ESV images comprising black regions. The input image was first converted to grayscale, and a binary threshold was used to isolate the circular surgical region from the background. This step enhanced the visibility of the surgical scene. Subsequently, the largest contour was identified within the thresholded image and computed its minimum enclosing bounding box. A mask corresponding to this circular region was created and applied to the original image to extract the surgical area while ignoring the background. The bounding box of the surgical region was cropped and this cropped image was resized to its original size using bilinear interpolation. This method ensures that only the relevant surgical view is retained and standardised, facilitating improved visualisation and analysis of the surgical scene.
% \begin{figure}[htbp]
%     \begin{subfigure}{0.5\linewidth}
%          \includegraphics[height=4cm, width=\linewidth]{Non-cropped.jpeg}
%         \caption{Non-cropped 1920 x 1080 Image}
%         \label{Non-cropped}
%     \end{subfigure}
%     \begin{subfigure}{0.5\linewidth}
%         \includegraphics[height=4cm, width=\linewidth]{Cropped.jpeg}
%         \caption{Cropped 1280 x 1080 Image}
%         \label{Cropped}
%     \end{subfigure}
%     \caption{Comparison of Non-cropped and cropped image.}
% \end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=1\textwidth]{arch2.png}
    \caption{\textbf{Proposed SpatioTemporal Adaptive LSTM Network (STALNet) for Surgical Timeline Segmentation:} \textit{This network diagram shows the process by which ESV clips are analysed by encoders in order to apply reliable timeline segments.}}
    \label{fig:arch}
\end{figure}

% \vspace{2mm}
\subsubsection*{Problem Formulation}
A key objective of this study was to learn an unknown function $\mathbf{F}$ that maps high-dimensional TEMS endoscopic surgical videos $\mathbf{X} \in \mathbb{R}^{T \times H \times W \times 3}$ to a multitarget label triplet $\mathbf{Y} \in \{\text{Phase}, \text{Task}, \text{Action}\}$, where, $T$, $H$, and $W$ denote the sequence length (no. of frames in the video), height, and width of the frames, respectively. To achieve this, this study proposes a Spatiotemporal Adaptive LSTM Network (STALNet) that learns the desired mapping. As shown in Fig. \ref{fig:arch}, STALNet integrates a TimeDistributed video encoder $\mathbf{E}^T$, followed by an adaptive long-short term memory network (LSTM) module having attention as the last layer $\mathbf{M}_\text{AA-LSTM}$ to capture spatial and temporal dependencies in the ESV data. Let $\phi$ be the feature extraction function using the backbone. The output of the encoder is given by:

\begin{equation}
    \mathbf{F} = \mathbf{E}^T(\phi(\mathbf{X})); \mathbf{X} \in \mathbb{R}^{B \times T \times C \times H \times W},
\end{equation}

where, $B$ is the batch size, $T$ is the sequence length, $C$ is the number of channels, and $H$ and $W$ are the height and width of the frames, respectively. We experimented with various encoders, including \texttt{ConvNeXt}(convnext\_small\_in22k) \cite{liu2022convnet}, \texttt{SWIN V2} (swinv2\_base\_window12\_192-22k)\cite{liu2022swin}, and \texttt{ViT} (vit\_small\_patch16\_224) \cite{steiner2021train,caron2021emerging}. These encoders were chosen for their proven ability to capture detailed spatial features across different scales, which is crucial for accurately interpreting surgical video frames. The extracted features are fed into an \texttt{Adaptive LSTM} module. This module consists of multiple LSTM layers, where the number of LSTMs depends on the input sequence length $T$. Each LSTM processes the sequence of features and produces hidden states. Let $\mathbf{h}_t$ represent the hidden state at time step $t$. The hidden states are computed as:
\[
\mathbf{H}_t = \mathbf{M}_\text{AA-LSTM}(\mathbf{F}_t, \mathbf{h}_{t-1}),
\]
where $\mathbf{H}_t \in \mathbb{R}^{B \times D}$. Multiple LSTM layers were applied to capture temporal dependencies across the sequence. Incorporating LSTMs into the proposed solution in an adaptive manner significantly improved the model's capacity for surgical scene understanding, as this approach leverages and preserves the temporal coherence in the videos, improving the stability and accuracy of the timeline predictions. The final hidden states from each LSTM layer are collected as $\mathbf{H} = [\mathbf{H}_1, \mathbf{H}_2, \ldots, \mathbf{H}_T] \in \mathbb{R}^{T \times B \times D}$ and their information across the sequence is aggregated using an attention mechanism. The attention weights are computed by applying a linear layer to the hidden states:
\[
\mathbf{A}_t = \text{softmax}(\mathbf{W}_a \mathbf{H}_t),
\]
where $\mathbf{W}_a \in \mathbb{R}^{D \times 1}$ is the attention weight matrix. The attention-weighted output is computed as a weighted sum of the hidden states:
\[
\mathbf{O} = \sum_{t=1}^{T} \mathbf{A}_t \mathbf{H}_t \in \mathbb{R}^{B \times D}.
\]

The final output is obtained by passing the attention-weighted output through a fully connected layer followed by batch normalisation:
\[
\mathbf{Y} = \text{BatchNorm}(\mathbf{W}_h \mathbf{O}),
\]
where $\mathbf{W}_h \in \mathbb{R}^{D \times (P+T+A)}$, with $P$, $T$, and $A$ representing the number of phases, tasks, and actions, respectively. A technique was employed here for mean ensembling to create more robust learners for each model, followed by heuristic-based prediction correction to address sporadic predictions. 

The model is trained using a custom loss function that combines the losses for phase, task, and action predictions. The total loss is given by:
\[
\mathcal{L} = \alpha \mathcal{L}_p + \beta \mathcal{L}_t + \gamma \mathcal{L}_a,
\]
where $\mathcal{L}_p$, $\mathcal{L}_t$, and $\mathcal{L}_a$ are the individual losses for phase, task, and action predictions, and $\alpha$, $\beta$, and $\gamma$ are their respective weights. Each of these losses is computed using the \texttt{CrossEntropyLossFlat} function applied to each of the output triplets. 



% To test the suitability of TEMSET-50K for surgical timeline segmentation, we further implemented a Spatiotemporal Adaptive LSTM Network (\texttt{STALNet}). The schematic representation of \texttt{STALNet}'s architecture can be found in Fig. \ref{fig:arch}. Our model combines a video encoder with an attention-based adaptive long short-term memory network (LSTM). The design was guided by the need to capture spatial and temporal dependencies in the ESV data. The input to \texttt{STALNet} is a video clip represented as a 3D tensor $\mathbf{X} \in \mathbb{R}^{B \times T \times C \times H \times W}$, where $B$ is the batch size, $T$ is the sequence length (i.e., the temporal dimension), $C$ is the number of channels, and $H$ and $W$ are the height and width of the frames, respectively. The model extracts visual features using a \texttt{TimeDistributed} wrapper that allows to apply feature extraction to every temporal slice of an input. This wrapper is underpinned by an encoder network. We experimented with several encoders, including \texttt{ConvNext}(convnext\_small\_in22k) \cite{liu2022convnet}, \texttt{SWIN} (swinv2\_base\_window12\_192-22k)\cite{liu2022swin}, and \texttt{ViT} (vit\_small\_patch16\_224) \cite{steiner2021train,caron2021emerging} for their capacity to accurately learn visual features from surgical frames. Let $\phi$ be the feature extraction function using the backbone encoder. The output is given by:
% \[
% \mathbf{F} = \text{TimeDistributed}(\phi(\mathbf{X})) \in \mathbb{R}^{B \times T \times D}
% \]
% where $D$ is the dimensionality of the extracted features for the input video clip. 

% The $\mathbf{F}$ is fed into an \texttt{LSTM} module, consisting of multiple LSTM units adaptive to the input sequence length $T$. Each LSTM processes the sequence of features and produces hidden states, $\mathbf{h}_t$, at each time step $t$: $\mathbf{H}_t = \text{LSTM}(\mathbf{F}_t, \mathbf{h}_{t-1})$, where $\mathbf{H}_t \in \mathbb{R}^{B \times D}$. Incorporating LSTMs into the \texttt{STALNet} architecture significantly improved the model's capacity for surgical scene understanding, as this approach leverages and preserves the temporal coherence in the videos, improving the stability and accuracy of timeline predictions. The final hidden states from each LSTM layer are collected as $\mathbf{H} = [\mathbf{H}_1, \mathbf{H}_2, \ldots, \mathbf{H}_T] \in \mathbb{R}^{T \times B \times D}$, and their information across the sequence is aggregated using an attention mechanism, computed by applying a linear transformation to the hidden states: $\mathbf{A}_t = \text{softmax}(\mathbf{W}_a \mathbf{H}_t)$, where $\mathbf{W}_a \in \mathbb{R}^{D \times 1}$ is the attention weight matrix. The attention-weighted output is computed as a weighted sum of the hidden states:
% \[
% \mathbf{O} = \sum_{t=1}^{T} \mathbf{A}_t \mathbf{H}_t \in \mathbb{R}^{B \times D}.
% \]

% The final prediction is obtained by passing the attention-weighted output through a fully connected layer followed by batch normalization (BN) and dropout: $\mathbf{Y} = \text{BN}(\mathbf{W}_h \mathbf{O})$, where $\mathbf{W}_h \in \mathbb{R}^{B \times (p+t+a)}$, with $p$, $t$, and $a$ representing the phases, tasks, and actions, respectively. We employed BN before applying Dropouts to ensure better performance \cite{li2019understanding}. 

% \vspace{2mm}

\subsubsection*{DL Model Implementation}
The model described in this paper was implemented using the \texttt{fastai} \cite{howard2020fastai} library. A server with 4 NVIDIA LS40 GPUs was used for training and validation. To enhance model convergence, the default \texttt{ReLU} activation function was replaced with the \texttt{Mish} activation function, which demonstrated superior performance in our experiments. Additionally, we substituted the default \texttt{Adam} optimiser with \texttt{ranger}, a combination of \texttt{RectifiedAdam} and the \texttt{Lookahead} optimisation technique, providing more stable and efficient training dynamics. To further optimise the training process, the \texttt{to\_fp16()} method was employed to reduce the precision of floating-point operations, thereby enabling half-precision training and improving computational efficiency. The \texttt{lr\_find} method was utilised to determine the optimal learning rate for the model, implementing a learning rate slicing technique. This approach assigned higher learning rates to the layers closer to the model head and lower learning rates to the initial layers, facilitating more effective training. For benchmarking, we initially evaluated several network architectures, including a basic image classifier, to establish a trivial baseline. This simple approach, however, produced significant sporadic predictions due to the absence of sequence modelling, highlighting the necessity for a more sophisticated model. 

\subsubsection*{Model validation}
The model described in this paper was validated against the human annotator ground truth using the server with NVIDIA LS40 GPUs. We compared the proposed \texttt{STALNet} architecture with various encoder backbones, including \texttt{ConvNeXt}, \texttt{SWIN V2}, and \texttt{ViT}. The output results were analysed against the baseline to look at comparative performance metrics and how they captured the spatiotemporal dependencies that are crucial for the surgical timeline segmentation task. 

\subsubsection*{Statistical analysis}
This study used a mutiple Python tools and scripts for deep learning projects, covering data preprocessing, model training, evaluation, and visualization:

1. \textbf{\textit{Data Preprocessing}}:
   \begin{itemize}
       \item Video frames normalisation and generating cutouts was done with FFmpeg.
       \item Pandas and Matplotlib was used to handle data distribution and class imbalance.
   \end{itemize}

2. \textbf{\textit{Model Training and Evaluation}}:
   \begin{itemize}
       \item PyTorch and fastai was used to implement and train deep learning models using PyTorch and fastai.
       \item Pre-trained models were intergrated from timm.
       \item Accuracy, F1 score, and ROC curves were used as evaluation metrics.
   \end{itemize}

3. \textbf{\textit{Visualization and analysis}}:
   \begin{itemize}
       \item The data and model results were visualised and analysed with Matplotlib, numpy and Scikit-learn.
       \item The dataset were split into training and testing sets.
       \item The trained model was used to predict test set probabilities for the positive class.
       \item Logistic regression was used to calculate ROC curve and AUC.
       \item ROC curves were plotted with Scikit-learn. False positive and true positive rate calculations were undertaken and a diagonal line was used to represent a random classifier.
   \end{itemize}

4. \textbf{\textit{Reproducibility}}:
   \begin{itemize}
       \item Nbdev scripts was used for creating reproducible and literate programs.
   \end{itemize}

\section*{Results}

\subsubsection*{\textbf Model Performance Evaluation}
Table \ref{Tab-comparison} presents the accuracy and F1 scores for each model across the three encoder architectures. The baseline image classification learner, which predicts timeline labels based solely on individual images, achieved an F1 score of 72.99\% with the ConvNeXt encoder, 66.7\% with the SWIN V2 encoder, and 60.87\% with the ViT encoder. These results indicate the fundamental capability of deep learning models for surgical timeline segmentation but also highlight the limitations of relying solely on spatial information. In contrast, our proposed \texttt{STALNet} demonstrated significant performance improvements over the baseline model. On average, \texttt{STALNet} achieved an F1 score of 82.78\% and an accuracy of 91.69\%, reflecting an average performance gain of 9.79\% in F1 score and 11.38\% in accuracy compared to the baseline model. These improvements underscore the importance of incorporating spatiotemporal information for surgical timeline segmentation. Furthermore, the performance varied between different model encoders used in the time-distributed layer for feature extraction. Among the evaluated encoders, the ConvNeXt encoder achieved the highest accuracy with 91.69\%, slightly better than the SWIN V2 encoder at 91.41\%. However, the highest performing F1 score, which is a significant metric for evaluating timeline segmentation, was achieved by the SWIN V2 encoder at 86.02\%, which is approximately 3.24\% higher than the ConvNeXt encoder's F1 score of 82.78\%. This demonstrates that while ConvNeXt offers marginally better accuracy, SWIN V2 excels in terms of F1 score, highlighting its superior performance in capturing relevant features for timeline segmentation. Despite the higher F1 score of SWIN V2, it required substantial computation during both training and deployment phases. On the other hand, ConvNeXt not only delivered competitive performance but also offered a more computationally efficient solution, making it a practical choice for real-world applications. Overall, the \texttt{STALNet} model, particularly with the ConvNeXt encoder, demonstrated superior performance in segmenting surgical timelines. This highlights the efficacy of integrating spatiotemporal features and selecting robust encoder architectures to balance performance and computational efficiency.

% To explore the applications of the proposed surgical timeline dataset, we first implemented several architectures by incorporating a variety of vision learners. These architectures include RegNetX \cite{radosavovic2020designing}, ConvNext\cite{liu2022convnet}, SWIN \cite{liu2022swin}, and ViT\cite{steiner2021train,caron2021emerging}. We started by implementing a simple vanilla classifier as our initial vision learner. This approach generated the following accuracy results across different models: 76.26\% with RegNetX, 76.26\% with ConvNext, 78.74\% with SWIN, and 75.23\% with ViT. The second Visio learner in this study was based on the \textbf{TimeDistributed} layer that processes each frame individually using a pre-trained ResNet. This model utilizes the TimeDistributed layer to apply the ResNet to each frame. The probabilities of each frame are then averaged to obtain the final prediction. The baseline model achieved an accuracy of 66\% with only 5 frames, demonstrating its potential despite its simplicity. The results of surgical timeline segmentation on our dataset using state-of-the-art models are shown in table \ref{Seg-Models}.

% To capture the temporal relations between frames, we extended our model by incorporating an LSTM layer. This allows the model to process sequences of frames and learn inter-frame dependencies. The advanced RNN model achieved an accuracy of 79\%, showing a significant improvement over the baseline. To further enhance the performance, we explored transformer-based models designed to handle spatio-temporal data effectively. These models utilize attention mechanisms to model complex dependencies within sequences of frames. The TimeSformer model showed promising results, achieving over 90\% accuracy, demonstrating its effectiveness in capturing both spatial and temporal information in the data.



 % This model follows an encoder-decoder structure, with the encoder responsible for extracting features from the video frames and the decoder responsible for reconstructing the segmentation mask from the extracted features. This model leads to a potential problem since CNN cannot effectively capture temporal features, resulting in low performance in video segmentation tasks. To partially overcome this problem, we implemented another model based on the Vision Transformer (ViT) architecture \cite{han2022survey}. ViT is a powerful tool for extracting visual features and has been used in numerous applications of image-based classification and surgical data science.  Additionally, we developed a ViT-based foundation model for endoscopic surgical video segmentation, which we named EndoViT. The concept of EndoViT is adapted from the general surgery vision transformer approach (GSViT)\cite{schmidgall2024general}.  Despite being trained on a completely new dataset, EndoViT outperforms other models regarding phase, task, and action recognition. Furthermore, the fine-tuning step significantly improves the results.
\begin{table}[!t]
\caption{Comparison of Surgical Timeline Segmentation Models}
\centering
\label{Tab-comparison}
\begin{adjustbox}{max width=0.7\textwidth}
\begin{tabular}{l|l|rr|rr|rr}
\hline
\rowcolor[HTML]{CBCEFB} 
\cellcolor[HTML]{CBCEFB}                                & \cellcolor[HTML]{CBCEFB}                                            & \multicolumn{2}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{ConvNeXt}}                                                                  & \multicolumn{2}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{ViT}}                                                                       & \multicolumn{2}{c}{\cellcolor[HTML]{CBCEFB}\textbf{SWIN V2}}                                                                   \\ \cline{3-8} 
\rowcolor[HTML]{CBCEFB} 
\multirow{-2}{*}{\cellcolor[HTML]{CBCEFB}\textbf{Sr\#}} & \multirow{-2}{*}{\cellcolor[HTML]{CBCEFB}\textbf{Model Descripton}} & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{Accuracy}} & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{F1 Score}} & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{Accuracy}} & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{F1 Score}} & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{Accuracy}} & \multicolumn{1}{c}{\cellcolor[HTML]{CBCEFB}\textbf{F1 Score}} \\ \hline
1                                                       & Baseline Vision Classifier                                          & \multicolumn{1}{r|}{80.36\%}                                   & 72.99\%                                                        & \multicolumn{1}{r|}{75.23\%}                                   & 60.87\%                                                        & \multicolumn{1}{r|}{78.74\%}                                   & 66.70\%                                                       \\ \hline
2                                                       & STALNet (Ours)                                                      & \multicolumn{1}{r|}{\textbf{91.69\%}}                          & 82.78\%                                                        & \multicolumn{1}{r|}{83.02\%}                                   & 68.29\%                                                        & \multicolumn{1}{r|}{91.42\%}                                   & \textbf{86.02\%}                                              \\ \hline
\end{tabular}
\end{adjustbox}
\end{table}



% The architecture we chose for EndoViT is based on the EfficientNet ViT structure
% \cite{liu2023efficientvit}. The design of this model emphasises speed in inference, minimizing memory and computation during inference and training time. This is important for surgical applications, where algorithms run in real-time. 

% \textbf{ Video prediction-based Pre-training}: The proposed approach differs from the conventional methods in other medical foundation models, which were trained on image reconstruction. Rather than relying on static images, surgery involves a dynamic interplay of spatial and temporal elements. While existing image-pretraining techniques concentrate on individual frames, we contend that models tailored for surgical contexts must encapsulate both spatial and temporal dimensions. Recent studies have delved into pre-training via video prediction \cite{gupta2022maskvit}, illustrating enhanced outcomes for spatio-temporal challenges. Using a similar strategy, we employ \textbf {EndoViT}, wherein we conduct next-frame reconstruction for pre-training, with a 1 frame input and project a 1 frame prediction. Our experimentation revealed that integrating masking techniques, explored in \cite{gupta2022maskvit}, failed to improve performance. Therefore, we adopted the next-frame reconstruction approach presented in \cite{gao2022simvp}.
% \begin{table}[!t]
% \caption{Comparison of Surgical Timeline Segmentation Models}
% \label{Timeline-seg}
% \renewcommand{\arraystretch}{0.9}
% \begin{adjustbox}{max width=\textwidth}
% \begin{tabular}{|
% >{\columncolor[HTML]{8EA9DB}}l |
% >{\columncolor[HTML]{B4C6E7}}l |
% >{\columncolor[HTML]{D9E1F2}}l |llll|}
% \hline
% \multicolumn{1}{|c|}{\cellcolor[HTML]{FFFFFF}} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} }} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} }} & \multicolumn{4}{c|}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} \textbf{Model Performance}}} \\ \cline{4-7} 
% \multicolumn{1}{|c|}{\multirow{-2}{*}{\cellcolor[HTML]{FFFFFF}\textbf{Phase}}} & \multicolumn{1}{c|}{\multirow{-2}{*}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} \textbf{Task}}}} & \multicolumn{1}{c|}{\multirow{-2}{*}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} \textbf{Action}}}} & \multicolumn{1}{l|}{\textbf{Accuracy}} & \multicolumn{1}{l|}{\textbf{Precision}} & \multicolumn{1}{l|}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{000000} \textbf{Recall}}} & \cellcolor[HTML]{FFFFFF}\textbf{F1 Score} \\ \hline
% \cellcolor[HTML]{FFD966} & \cellcolor[HTML]{FFE699} & \cellcolor[HTML]{FFF2CC}Scope Insertion & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{FFD966} & \cellcolor[HTML]{FFE699} & \cellcolor[HTML]{FFF2CC}Scope Positioning & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{FFD966} & \cellcolor[HTML]{FFE699} & \cellcolor[HTML]{FFF2CC}No Action & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{FFD966} & \multirow{-4}{*}{\cellcolor[HTML]{FFE699}Scope} & \cellcolor[HTML]{FFF2CC}Out of Body & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{2-7} 
% \cellcolor[HTML]{FFD966} & \cellcolor[HTML]{FFE699} & \cellcolor[HTML]{FFF2CC}Instrument Insertion & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{FFD966} & \cellcolor[HTML]{FFE699} & \cellcolor[HTML]{FFF2CC}Instrument Positioning & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{FFD966} & \cellcolor[HTML]{FFE699} & \cellcolor[HTML]{FFF2CC}No Action & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{FFD966} & \multirow{-4}{*}{\cellcolor[HTML]{FFE699}Instrument} & \cellcolor[HTML]{FFF2CC}Out of Body & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{2-7} 
% \cellcolor[HTML]{FFD966} & \cellcolor[HTML]{FFE699} & \cellcolor[HTML]{FFF2CC}Fluid Wash & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{FFD966} & \cellcolor[HTML]{FFE699} & \cellcolor[HTML]{FFF2CC}Debris Wash & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{FFD966} & \cellcolor[HTML]{FFE699} & \cellcolor[HTML]{FFF2CC}No Action & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{FFD966} & \multirow{-4}{*}{\cellcolor[HTML]{FFE699}Site} & \cellcolor[HTML]{FFF2CC}Out of Body & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{2-7} 
% \cellcolor[HTML]{FFD966} & \cellcolor[HTML]{FFE699} & \cellcolor[HTML]{FFF2CC}Inflate Rectum & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{FFD966} & \cellcolor[HTML]{FFE699} & \cellcolor[HTML]{FFF2CC}Deflate Rectum & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{FFD966} & \cellcolor[HTML]{FFE699} & \cellcolor[HTML]{FFF2CC}No Action & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \multirow{-16}{*}{\cellcolor[HTML]{FFD966}\textbf{Setup}} & \multirow{-4}{*}{\cellcolor[HTML]{FFE699}Pressure} & \cellcolor[HTML]{FFF2CC}Out of Body & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \hline
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Marking & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Retraction & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Smoke & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Bleeding & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Haemostatis & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Washout & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Aspiration & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Scope Positioning & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Instrument Positioning & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & No Action & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \multirow{-11}{*}{\cellcolor[HTML]{B4C6E7}Landmarking} & Out of Body & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{2-7} 
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Dissection & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Retraction & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Smoke & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Bleeding & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Haemostatis & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Washout & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Aspiration & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Scope Positioning & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Instrument Positioning & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & No Action & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \multirow{-11}{*}{\cellcolor[HTML]{B4C6E7}Mucosal Dissection} & Out of Body & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{2-7} 
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Dissection & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Retraction & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Smoke & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Bleeding & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Haemostasis & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Washout & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Aspiration & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Scope Positioning & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Instrument Positioning & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & No Action & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \multirow{-11}{*}{\cellcolor[HTML]{B4C6E7}Submucosal Dissection} & Out of Body & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{2-7} 
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Dissection & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Retraction & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Smoke & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Bleeding & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Haemostasis & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Washout & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Aspiration & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Scope Positioning & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Instrument Positioning & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & No Action & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \multirow{-11}{*}{\cellcolor[HTML]{B4C6E7}Circular Muscle Dissection} & Out of Body & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{2-7} 
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Dissection & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Retraction & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Smoke & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Bleeding & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Haemostasis & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Washout & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Aspiration & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Scope Positioning & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & Instrument Positioning & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{8EA9DB} & \cellcolor[HTML]{B4C6E7} & No Action & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \multirow{-55}{*}{\cellcolor[HTML]{8EA9DB}\textbf{Dissection}} & \multirow{-11}{*}{\cellcolor[HTML]{B4C6E7}Longitudinal Muscle Dissection} & Out of Body & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \hline
% \cellcolor[HTML]{F4B084}\textbf{Specimen   Removal} & \cellcolor[HTML]{F8CBAD}Specimen Removal & \cellcolor[HTML]{FCE4D6}Specimen Removal & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \hline
% \cellcolor[HTML]{A9D08E} & \cellcolor[HTML]{C6E0B4} & \cellcolor[HTML]{E2EFDA}Stitching & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{A9D08E} & \cellcolor[HTML]{C6E0B4} & \cellcolor[HTML]{E2EFDA}Clipping Suture & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{A9D08E} & \cellcolor[HTML]{C6E0B4} & \cellcolor[HTML]{E2EFDA}Smoke & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{A9D08E} & \cellcolor[HTML]{C6E0B4} & \cellcolor[HTML]{E2EFDA}Bleeding & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{A9D08E} & \cellcolor[HTML]{C6E0B4} & \cellcolor[HTML]{E2EFDA}Haemostatis & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{A9D08E} & \cellcolor[HTML]{C6E0B4} & \cellcolor[HTML]{E2EFDA}Washout & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{A9D08E} & \cellcolor[HTML]{C6E0B4} & \cellcolor[HTML]{E2EFDA}Aspiration & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{A9D08E} & \cellcolor[HTML]{C6E0B4} & \cellcolor[HTML]{E2EFDA}Scope Positioning & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{A9D08E} & \cellcolor[HTML]{C6E0B4} & \cellcolor[HTML]{E2EFDA}Instrument Positioning & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \cellcolor[HTML]{A9D08E} & \cellcolor[HTML]{C6E0B4} & \cellcolor[HTML]{E2EFDA}No Action & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \cline{3-3}
% \multirow{-11}{*}{\cellcolor[HTML]{A9D08E}\textbf{Closure}} & \multirow{-11}{*}{\cellcolor[HTML]{C6E0B4}Suturing} & \cellcolor[HTML]{E2EFDA}Out of Body & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \hline
% \cellcolor[HTML]{AEAAAA}\textbf{Scope   Removal} & \cellcolor[HTML]{AEAAAA}Scope removal & \cellcolor[HTML]{AEAAAA}Scope Removal & 70.5\% $\pm$ 3.4 & 77\% $\pm$ 3.4 & 78.6\% $\pm$ 3.4 & 77.4\% $\pm$ 3.4 \\ \hline
% \end{tabular}
% \end{adjustbox}
% \end{table}

% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=1\textwidth, height=4cm]{roc_taxonomy.png}
%     \caption{ROC Curves of the ConvNeXt Encoder for Phases, Tasks and Actions}
%     \label{fig:roc}
% \end{figure}
%%ROC Curves of all the encoders
%%CONVNEXT


\vspace{2mm}
The \texttt{STALNet} model was also evaluated for its performance on each of the taxonomy triplets (phase, task, action) as shown in \cref{Tab-phs}, \cref{Tab-task}, and  \cref{Tab-act}, respectively. The evaluation of phase segmentation reveals that the model performs exceptionally well across all phases, with only minor fluctuations in performance using different encoders. The ROC curves show its efficacy across these triplet behaviours (see Figures \ref{fig:roc}). For example, the “Dissection” phase achieved an F1 score of 99.0\% with no variance and an accuracy of 99.0\% with a variance of 11.0\% with the SWIN V2 encoder. Similarly, the “Setup” phase showed high performance with an F1 score of 98.0\% and an accuracy of 99.0\%, both exhibiting low variances (1\% and 9\%, respectively with ConvNeXt and SWIN V2 encoders). Even the “Closure” phase, despite being one of the more challenging phases due to its fewer instances, maintained an F1 score and accuracy of 100\% for both with variances of 0\% and 5\%, respectively with the SWIN V2 encoder. These results indicate that the model effectively captures and segments the different phases consistently across three distinct encoders. In task segmentation, the model showed strong and consistent performance across most tasks. For instance, tasks such as “Longitudinal Muscle Dissection” and “Suturing” achieved high F1 scores of 99\% for each, with accuracies of 100\% and 99\%, and low variances (1\% and 0\%, and 7\% and 8\%, respectively) with the ConvNeXt encoder. This consistency reflects the model's robust ability to segment tasks accurately. Conversely, the “Site” task, which had a significantly lower F1 score of 67\% with high variance 33\% with the ConvNeXt encoder. This indicates that the model struggles more with tasks that are less frequently represented in the dataset. For action segmentation, the model demonstrated high performance on frequently occurring actions such as “Scope Insertion” and “Stitching” achieving F1 scores of 99\% and 95\%, and accuracies of 100\% and 98\%, respectively with the ConvNeXt encoder. The variances for “Scope Insertion” were 1\% for the F1 score and 3\% for accuracy, while “Stitching” had variances of 4\% and 15\%, indicating stable and reliable performance. However, actions like “Debris Wash” and “Haemostatis,” which had lower F1 scores of 50\% for each, also exhibited higher variances 50\% for each of the above actions with the ConvNeXt encoder . These findings suggest that the model's performance is consistent for well-represented actions, but struggles with less frequent actions. 

\begin{table}[!t]
\centering
\caption{Performance of the STALNet model on Surgical Phases across different encoders}
\label{Tab-phs}
\renewcommand{\arraystretch}{1.2}
\begin{adjustbox}{max width=0.7\textwidth}
\begin{tabular}{p{4.8cm}|c|c|c|c|c|c}
\hline
\rowcolor[HTML]{CBCEFB} 
\cellcolor[HTML]{CBCEFB}                                & \multicolumn{2}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{ConvNeXt}}                                                                  & \multicolumn{2}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{ViT}}                                                                       & \multicolumn{2}{c}{\cellcolor[HTML]{CBCEFB}\textbf{SWIN V2}}                                                                   \\ \cline{2-7} 
\rowcolor[HTML]{CBCEFB} 
\multirow{-2}{*}{\cellcolor[HTML]{CBCEFB}\textbf{Name}} & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{Accuracy}} & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{F1 Score}} & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{Accuracy}} & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{F1 Score}} & \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{Accuracy}} & \multicolumn{1}{c}{\cellcolor[HTML]{CBCEFB}\textbf{F1 Score}} \\ \hline
[01] Setup                                                   & \multicolumn{1}{r|}{0.99 ±  0.09}                              & 0.97 ±  0.02                                                   & \multicolumn{1}{r|}{0.98 ±  0.13}                              & 0.94 ±  0.05                                                   & \multicolumn{1}{r|}{0.99 ±  0.10}                              & 0.97 ±  0.03                                                  \\ \hline
[02] Dissection                                              & \multicolumn{1}{r|}{0.99 ±  0.10}                              & 0.99 ±  0.00                                                   & \multicolumn{1}{r|}{0.97 ±  0.17}                              & 0.97 ±  0.00                                                   & \multicolumn{1}{r|}{0.99 ±  0.11}                              & 0.99 ±  0.00                                                  \\ \hline


[03] Specimen Removal                                        & \multicolumn{1}{r|}{1.00 ±  0.03}                              & 0.97 ±  0.03                                                   & \multicolumn{1}{r|}{1.00 ±  0.04}                              & 0.95 ±  0.05                                                   & \multicolumn{1}{r|}{1.00 ±  0.02}                              & 0.99 ±  0.01                                                  \\ \hline
[04] Closure                                                 & \multicolumn{1}{r|}{0.99 ±  0.09}                              & 0.99 ±  0.00                                                   & \multicolumn{1}{r|}{0.98 ±  0.14}                              & 0.98 ±  0.01                                                   & \multicolumn{1}{r|}{0.99 ±  0.08}                              & 0.99 ±  0.00                                                  \\ \hline
[05] Scope Removal                                           & \multicolumn{1}{r|}{1.00 ±  0.03}                              & 1.00 ±  0.00                                                   & \multicolumn{1}{r|}{1.00 ±  0.05}                              & 0.99 ±  0.01                                                   & \multicolumn{1}{r|}{1.00 ±  0.03}                              & 1.00 ±  0.00                                                  \\ \hline

\end{tabular}
\end{adjustbox}
\end{table}

\begin{table}[!t]
\centering
\caption{Performance of the STALNet model on Surgical Tasks across different encoders}
\label{Tab-task}
\renewcommand{\arraystretch}{1.2}
\begin{adjustbox}{max width=0.7\textwidth}
\begin{tabular}{p{5.1cm}|c|c|c|c|c|c}
\hline
\rowcolor[HTML]{CBCEFB} 
\cellcolor[HTML]{CBCEFB} &
  \multicolumn{2}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{ConvNeXt}} &
  \multicolumn{2}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{ViT}} &
  \multicolumn{2}{c}{\cellcolor[HTML]{CBCEFB}\textbf{SWIN V2}} \\ \cline{2-7} 
\rowcolor[HTML]{CBCEFB} 
\multirow{-2}{*}{\cellcolor[HTML]{CBCEFB}\textbf{Name}} &
  \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{Accuracy}} &
  \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{F1 Score}} &
  \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{Accuracy}} &
  \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{F1 Score}} &
  \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{Accuracy}} &
  \multicolumn{1}{c}{\cellcolor[HTML]{CBCEFB}\textbf{F1 Score}} \\ \hline
  
[01] Scope Setup &
  \multicolumn{1}{r|}{0.99 ± 0.10} &
  0.96 ± 0.04 &
  \multicolumn{1}{r|}{0.99 ± 0.11} &
  0.94 ± 0.05 &
  \multicolumn{1}{r|}{0.99 ± 0.09} &
  0.96 ± 0.04 \\ \hline
  [02] Instrument Setup &
  \multicolumn{1}{r|}{1.00 ± 0.02} &
  0.94 ± 0.06 &
  \multicolumn{1}{r|}{1.00 ± 0.03} &
  0.81 ± 0.19 &
  \multicolumn{1}{r|}{1.00 ± 0.02} &
  0.92 ± 0.08 \\ \hline
  [03] Site Setup &
  \multicolumn{1}{r|}{1.00 ± 0.06} &
  0.83 ± 0.17 &
  \multicolumn{1}{r|}{1.00 ± 0.07} &
  0.84 ± 0.16 &
  \multicolumn{1}{r|}{1.00 ± 0.07} &
  0.82 ± 0.18 \\ \hline
  [04] Pressure Setup &
  \multicolumn{1}{r|}{0.99 ± 0.07} &
  0.93 ± 0.07 &
  \multicolumn{1}{r|}{0.99 ± 0.11} &
  0.85 ± 0.15 &
  \multicolumn{1}{r|}{0.99 ± 0.08} &
  0.93 ± 0.07 \\ \hline
[05] Landmarking &
  \multicolumn{1}{r|}{0.99 ± 0.07} &
  0.98 ± 0.02 &
  \multicolumn{1}{r|}{0.98 ± 0.13} &
  0.93 ± 0.06 &
  \multicolumn{1}{r|}{0.99 ± 0.09} &
  0.97 ± 0.03 \\ \hline
[06] Mucosal Dissection &
  \multicolumn{1}{r|}{0.98 ± 0.14} &
  0.95 ± 0.04 &
  \multicolumn{1}{r|}{0.94 ± 0.23} &
  0.87 ± 0.10 &
  \multicolumn{1}{r|}{0.98 ± 0.15} &
  0.95 ± 0.04 \\ \hline
[07] Submucosal Dissection &
  \multicolumn{1}{r|}{0.98 ± 0.14} &
  0.96 ± 0.03 &
  \multicolumn{1}{r|}{0.96 ± 0.20} &
  0.91 ± 0.06 &
  \multicolumn{1}{r|}{0.98 ± 0.14} &
  0.96 ± 0.03 \\ \hline
[08] Circular Muscle Dissection &
  \multicolumn{1}{r|}{0.99 ± 0.11} &
  0.96 ± 0.04 &
  \multicolumn{1}{r|}{0.96 ± 0.19} &
  0.86 ± 0.12 &
  \multicolumn{1}{r|}{0.98 ± 0.12} &
  0.95 ± 0.04 \\ \hline
[09] Longitudinal Muscle Dissection &
  \multicolumn{1}{r|}{0.99 ± 0.11} &
  0.97 ± 0.02 &
  \multicolumn{1}{r|}{0.97 ± 0.17} &
  0.93 ± 0.06 &
  \multicolumn{1}{r|}{0.99 ± 0.11} &
  0.97 ± 0.02 \\ \hline
[10] Specimen Removal &
  \multicolumn{1}{r|}{1.00 ± 0.03} &
  0.98 ± 0.02 &
  \multicolumn{1}{r|}{1.00 ± 0.04} &
  0.96 ± 0.04 &
  \multicolumn{1}{r|}{1.00 ± 0.02} &
  0.99 ± 0.01 \\ \hline
[11] Suturing &
  \multicolumn{1}{r|}{0.99 ± 0.09} &
  0.99 ± 0.00 &
  \multicolumn{1}{r|}{0.98 ± 0.14} &
  0.98 ± 0.01 &
  \multicolumn{1}{r|}{0.99 ± 0.09} &
  0.99 ± 0.00 \\ \hline
[12] Scope removal &
  \multicolumn{1}{r|}{1.00 ± 0.03} &
  1.00 ± 0.00 &
  \multicolumn{1}{r|}{1.00 ± 0.05} &
  0.99 ± 0.01 &
  \multicolumn{1}{r|}{1.00 ± 0.03} &
  1.00 ± 0.00 \\ \hline

\end{tabular}
\end{adjustbox}
\end{table}

\begin{table}[!t]
\centering
\caption{Performance of the STALNet model on Surgical Actions across different encoders}
\label{Tab-act}
\renewcommand{\arraystretch}{1.2}
\begin{adjustbox}{max width=0.7\textwidth}
\begin{tabular}{p{5.1cm}|c|c|c|c|c|c}
\hline
\rowcolor[HTML]{CBCEFB} 
\cellcolor[HTML]{CBCEFB} &
  \multicolumn{2}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{ConvNeXt}} &
  \multicolumn{2}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{ViT}} &
  \multicolumn{2}{c}{\cellcolor[HTML]{CBCEFB}\textbf{SWIN V2}} \\ \cline{2-7} 
\rowcolor[HTML]{CBCEFB} 
\multirow{-2}{*}{\cellcolor[HTML]{CBCEFB}\textbf{Name}} &
  \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{Accuracy}} &
  \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{F1 Score}} &
  \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{Accuracy}} &
  \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{F1 Score}} &
  \multicolumn{1}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{Accuracy}} &
  \multicolumn{1}{c}{\cellcolor[HTML]{CBCEFB}\textbf{F1 Score}} \\ \hline
[01] Aspiration             & \multicolumn{1}{r|}{0.97 ± 0.18} & 0.85 ± 0.13 & \multicolumn{1}{r|}{0.93 ± 0.26} & 0.69 ± 0.27 & \multicolumn{1}{r|}{0.97 ± 0.18} & 0.85 ± 0.13 \\ \hline
[02] Bleeding               & \multicolumn{1}{r|}{0.99 ± 0.07} & 0.82 ± 0.17 & \multicolumn{1}{r|}{0.99 ± 0.10} & 0.67 ± 0.33 & \multicolumn{1}{r|}{0.99 ± 0.08} & 0.80 ± 0.19 \\ \hline
[03] Clipping Suture        & \multicolumn{1}{r|}{1.00 ± 0.07} & 0.88 ± 0.12 & \multicolumn{1}{r|}{0.99 ± 0.10} & 0.71 ± 0.28 & \multicolumn{1}{r|}{1.00 ± 0.06} & 0.91 ± 0.09 \\ \hline
[04] Debris Wash            & \multicolumn{1}{r|}{1.00 ± 0.01} & 0.50 ± 0.50 & \multicolumn{1}{r|}{1.00 ± 0.01} & 0.50 ± 0.50 & \multicolumn{1}{r|}{1.00 ± 0.01} & 0.50 ± 0.50 \\ \hline
[05] Deflate Rectum         & \multicolumn{1}{r|}{0.99 ± 0.07} & 0.89 ± 0.10 & \multicolumn{1}{r|}{0.99 ± 0.10} & 0.77 ± 0.23 & \multicolumn{1}{r|}{0.99 ± 0.08} & 0.89 ± 0.11 \\ \hline
[06] Dissection             & \multicolumn{1}{r|}{0.93 ± 0.26} & 0.87 ± 0.08 & \multicolumn{1}{r|}{0.87 ± 0.34} & 0.76 ± 0.16 & \multicolumn{1}{r|}{0.93 ± 0.26} & 0.86 ± 0.10 \\ \hline
[07] Fluid Wash             & \multicolumn{1}{r|}{1.00 ± 0.05} & 0.76 ± 0.24 & \multicolumn{1}{r|}{1.00 ± 0.06} & 0.78 ± 0.22 & \multicolumn{1}{r|}{1.00 ± 0.06} & 0.73 ± 0.27 \\ \hline
[08] Haemostatis            & \multicolumn{1}{r|}{1.00 ± 0.03} & 0.85 ± 0.15 & \multicolumn{1}{r|}{1.00 ± 0.03} & 0.79 ± 0.21 & \multicolumn{1}{r|}{1.00 ± 0.03} & 0.85 ± 0.15 \\ \hline
[09] Inflate Rectum         & \multicolumn{1}{r|}{1.00 ± 0.07} & 0.83 ± 0.17 & \multicolumn{1}{r|}{0.99 ± 0.08} & 0.70 ± 0.29 & \multicolumn{1}{r|}{1.00 ± 0.06} & 0.87 ± 0.13 \\ \hline
[10] Instrument Positioning & \multicolumn{1}{r|}{0.91 ± 0.29} & 0.82 ± 0.12 & \multicolumn{1}{r|}{0.81 ± 0.39} & 0.67 ± 0.21 & \multicolumn{1}{r|}{0.89 ± 0.32} & 0.80 ± 0.14 \\ \hline
[11] Marking                & \multicolumn{1}{r|}{1.00 ± 0.07} & 0.91 ± 0.08 & \multicolumn{1}{r|}{0.99 ± 0.10} & 0.80 ± 0.20 & \multicolumn{1}{r|}{0.99 ± 0.07} & 0.90 ± 0.09 \\ \hline
[12] No Action              & \multicolumn{1}{r|}{0.97 ± 0.18} & 0.87 ± 0.11 & \multicolumn{1}{r|}{0.93 ± 0.25} & 0.77 ± 0.20 & \multicolumn{1}{r|}{0.96 ± 0.20} & 0.85 ± 0.13 \\ \hline
[13] Out of Body            & \multicolumn{1}{r|}{0.99 ± 0.09} & 0.98 ± 0.02 & \multicolumn{1}{r|}{0.99 ± 0.11} & 0.96 ± 0.03 & \multicolumn{1}{r|}{0.99 ± 0.09} & 0.97 ± 0.02 \\ \hline
[14] Retraction             & \multicolumn{1}{r|}{0.95 ± 0.22} & 0.78 ± 0.19 & \multicolumn{1}{r|}{0.92 ± 0.26} & 0.66 ± 0.30 & \multicolumn{1}{r|}{0.94 ± 0.24} & 0.77 ± 0.20 \\ \hline
[15] Scope Insertion        & \multicolumn{1}{r|}{1.00 ± 0.07} & 0.95 ± 0.05 & \multicolumn{1}{r|}{0.99 ± 0.07} & 0.94 ± 0.06 & \multicolumn{1}{r|}{1.00 ± 0.06} & 0.96 ± 0.04 \\ \hline
[16] Scope Positioning      & \multicolumn{1}{r|}{0.96 ± 0.19} & 0.87 ± 0.11 & \multicolumn{1}{r|}{0.92 ± 0.27} & 0.73 ± 0.23 & \multicolumn{1}{r|}{0.96 ± 0.19} & 0.87 ± 0.11 \\ \hline
[17] Scope Removal          & \multicolumn{1}{r|}{1.00 ± 0.03} & 1.00 ± 0.00 & \multicolumn{1}{r|}{1.00 ± 0.05} & 0.99 ± 0.01 & \multicolumn{1}{r|}{1.00 ± 0.03} & 1.00 ± 0.00 \\ \hline
[18] Smoke                  & \multicolumn{1}{r|}{1.00 ± 0.07} & 0.87 ± 0.13 & \multicolumn{1}{r|}{0.99 ± 0.10} & 0.71 ± 0.28 & \multicolumn{1}{r|}{0.99 ± 0.08} & 0.82 ± 0.17 \\ \hline
[19] Specimen Removal       & \multicolumn{1}{r|}{1.00 ± 0.03} & 0.97 ± 0.03 & \multicolumn{1}{r|}{1.00 ± 0.04} & 0.95 ± 0.05 & \multicolumn{1}{r|}{1.00 ± 0.02} & 0.99 ± 0.01 \\ \hline
[20] Stitching              & \multicolumn{1}{r|}{0.98 ± 0.15} & 0.93 ± 0.06 & \multicolumn{1}{r|}{0.95 ± 0.21} & 0.85 ± 0.12 & \multicolumn{1}{r|}{0.97 ± 0.16} & 0.92 ± 0.06 \\ \hline
[21] Washout                & \multicolumn{1}{r|}{0.99 ± 0.11} & 0.91 ± 0.08 & \multicolumn{1}{r|}{0.97 ± 0.17} & 0.79 ± 0.20 & \multicolumn{1}{r|}{0.99 ± 0.12} & 0.90 ± 0.09 \\ \hline
\end{tabular}
\end{adjustbox}
\end{table}


\begin{figure}[!t]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth, height=4cm]{roc_phase_convnext.png}
        \caption{ConvNeXt}
        \label{ConvNextPhase}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth, height=4cm]{roc_phase_vit.png}
        \caption{ViT}
        \label{ViTPhase}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth, height=4cm]{roc_phase_swinv2.png}
        \caption{SWIN V2}
        \label{SWIN V2Phase}
    \end{subfigure}
 \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth, height=4cm]{roc_task_convnext.png}
        \caption{ConvNeXt}
        \label{ConvNextTask}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth, height=4cm]{roc_task_vit.png}
        \caption{ViT}
        \label{ViTTask}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth, height=4cm]{roc_task_swinv2.png}
        \caption{SWIN V2}
        \label{SWIN V2}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth, height=4cm]{roc_action_convnext.png}
        \caption{ConvNeXt}
        \label{ConvNextAction}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth, height=4cm]{roc_action_vit.png}
        \caption{ViT}
        \label{ViT}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth, height=4cm]{roc_action_swinv2.png}
        \caption{SWIN V2}
        \label{SWIN V2Action}
    \end{subfigure}
    \caption{\textbf{STALNet Performance Review using ROC Curves for Taxonomy Triplets}: \textit{The top row of ROC curves shows the performance of ConvNeXt, ViT and SWIN V2 encoders on labelling high level TEMS surgical "Phases". The next two rows show the performance of STALNet encoders on labelling TEMS surgical "Tasks" (intermediate level) and "Actions" (the fine level)}.}
    \label{fig:roc}
\end{figure}




%Show Results of STALNet on ConvNext

% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=1\textwidth]{show_results_convnext.png}
%     \caption{STALNet: Batch of results for visual inspection on ConvNext}
%     \label{Show_Convnext}
% \end{figure}

%Show Results of STALNet on ViT

% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=1\textwidth]{show_results_vit.png}
%     \caption{STALNet: Batch of results for visual inspection on ViT}
%     \label{Show_ViT}
% \end{figure}

%Show Results of STALNet on SWIN

% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=1\textwidth]{show_results_swinv2.png}
%     \caption{STALNet: Batch of results for visual inspection on SWIN}
%     \label{Show_SWIN}
% \end{figure}

\section*{Discussion}

\vspace{2mm}
The results confirm that the \texttt{STALNet} model with the ConvNeXt encoder performs well and consistently across phases, tasks, and actions with sufficient training data, as evidenced by low variance in well-represented classes. However, as the number of classes increases---from five phases to 11 tasks to 21 actions---the problem becomes more challenging, leading to higher variance and lower performance for less frequent classes. This trend underscores the complexity of handling a larger number of classes and highlights the need for addressing class imbalance. Techniques such as weighted dataloaders and customised loss functions can mitigate these issues, improving the model's robustness and performance across all categories. 

\vspace{2mm}
The results also illustrate the model's superior capabilities in capturing the nuances of surgical workflows. The ROC curves highlights that the Swin V2 encoder outperforms other encoders in terms of accuracy and F1 score. The model's output is visually depicted in an infographic in Figure \ref{fig:results}. This shows the input video clips with predicted and actual taxonomy triplet labels from a batch. This visualisation clearly demonstrates the trends discussed in the performance tables and ROC curves, providing a comprehensive understanding of the model's efficacy in real-world scenarios.
%Show Results
\begin{figure}[!t]
    \centering    \includegraphics[width=1\textwidth]{show_results.png}
    \caption{\textbf{STALNet: Batch of results for visual inspection:} \textit{This figure shows an example output of the STALNet model against human annotators (the ground truth). Green font signifies agreement with the ground truth at the surgical hierarchical "Phase", "Task" and "Action" triplet. Red font signifies non-agreement. In this case, there was widespread agreement apart one micro-clip where the model applied the "fine" action with a "retraction" label where the human annotators called it dissection.} }
    \label{fig:results}
\end{figure}

% \vspace{2mm}
% Figure \ref{TimelineComp} illustrates the surgical progress of a TEMS procedure, comparing the ground truth phase labels with the predictions from four models developed in this study. The top row indicates the surgical phase transitions over time, with the elapsed time column showing the duration spent on each phase in \texttt{hh:mm:ss} format. The entire procedure lasted 3 hours and 52 minutes. The models evaluated include an Image Classification Learner, a Time Distributed Learner, a Time Distributed Learner with LSTM, and a Time Distributed Learner with LSTM and category priors. The results from these models, shown after post-processing, are compared to the ground truth labels. The Image Classification Learner displays significant noise with sporadic predictions and \textit{ phase shaking problems} \cite{luo2023surgplan}, indicating its limitation in accurately segmenting surgical phases. However, as we progress to more sophisticated models like the Time Distributed Learner and its variants, the predictions become increasingly accurate and stable. The Time Distributed Learner with LSTM substantially reduces noise by learning temporal associations, while adding category priors further refines the model's performance, providing a clearer and more accurate phase segmentation. These results highlight the advancements achieved with more complex models, although further improvements are still possible.

% \begin{figure}[h]
%     \centering
%     \caption{Visual Comparison of Proposed Timeline Segmentation on Example Procedure}
%         \centering
%         \includegraphics[width=\textwidth]{Timelinecompare.JPG}
%     \label{TimelineComp}
% \end{figure}


\subsection*{Future work}
In this study, the proposed timeline segmentation model has been employed to index a large number of trans-anal endoscopic microsurgery procedures, potentially creating an intuitive front-end platform for surgeons, educators and surgical training committees to analyze surgical videos effectively in a time-effective manner. The methodology described here is generalisable and can be used in any form of surgery where video recording is undertaken. It is possible to create search capabilities of the ESV searching platform, which leverages the timeline segmentation models to efficiently analyse multi-part surgical videos of a single video or across a large library of videos. Surgeons will have the capability of searching within a single video or across their entire ``personal'' surgical video bank using the timeline labels generated by the model. Taking this to the next level, training or NHS governance committees would be able to do this at scale to ensure quality of surgical procedures are maintained at scale and across surgical domains. This personalised searching capability is crucial for improving surgical technique and demonstrating effectiveness in various governance tasks, such as appraisals. 

\vspace{2mm}
Currently, trainees or surgeons do not routinely submit VBAs for appraisal. This is polyfactorial but may be due to video file size, difficulty scrolling through large videos to find ``key steps'' and lack of a reliable standardised process to index videos. 


%Fig. \ref{fig:application}% 
\vspace{2mm}
The model demonstrated here has the potential for clinicians to use STALNet to index their videos so that they can quickly locate video clips of specific intra-operative surgical events from large video datasets. Once the model has been validated in a clinical trial, a future project may focus on video library analysis to identify key behaviours that can be modified to enhance future surgical performance. The timeline also enables comparisons between surgeons based on their surgical behaviour, task efficiency, end-to-end operative progression and intra-operative risk management. Any future clinical feedback system should allow users to reliably and securely filter and sort videos by type, speciality, and hospital, providing a powerful tool for detailed clinical data science analysis. It is important that this is embedded within a SDE to enable secure analysis and feedback. This lays the foundation to create a safe environment where continuous improvement in surgical practices can occur across a large cohort of surgeons. The possibilities offered by this system are vast, empowering clinicians to conduct comprehensive reviews of intra-operative tasks to improve surgical outcomes for patients.

%
% \begin{figure}[htbp]
%    \centering
%    \caption{SurgiFlow user interface to analyse large surgical videos}
%        \centering
%        \includegraphics[width=0.7\textwidth]{colon-flow.jpeg}
%    \label{fig:application}
% \end{figure}
% \section*{Usage Notes}%
%

\section*{Usage notes}

The dataset described in this study is available, after necessary approvals, for download at \url{https://researchdata.uhb.nhs.uk/pathway-research-data-hub/}. This dataset is designed to facilitate the training and evaluation of machine learning models for surgical timeline segmentation based on the proposed taxonomy. Researchers can use several software packages to analyse and process the dataset, with Python being particularly useful for data handling, preprocessing, and model training. Key libraries include FFmpeg and av for video processing and frame extraction, timm for accessing various pre-trained models, PyTorch for deep learning model implementation and training, fastai for simplifying the training process and integrating with PyTorch, nbdev for creating reproducible and literate programs, and Matplotlib for visualising data and model performance. It is recommended to normalise the video frames to standardise the input data. Cutouts can be generated based on custom logic using tools like FFmpeg. Additionally, Matplotlib and pandas can be used to analyse data distribution and class imbalance in the dataset.
When integrating or comparing this dataset with others, it is essential to ensure consistent preprocessing steps to maintain uniformity. Utilising common evaluation metrics can help effectively compare performance across different datasets. Considering the temporal nature of surgical workflow data when combining it with static datasets is crucial to preserve contextual information. We will provide detailed information on accessing the data, including criteria for determining eligibility and any limitations on data use. Researchers will need to apply through the specified access control mechanisms to obtain the dataset, ensuring compliance with ethical guidelines and data protection regulations. By following these guidelines and utilising the provided tools and recommendations, researchers can effectively leverage this dataset for advancing the field of surgical timeline segmentation and related applications. For additional resources, code, and tools, please refer to the Code Availability section.

\section*{Code availability}

The necessary scripts used in the generation and processing of the dataset for this study is available in the GitHub repository at \url{https://github.com/bilalcodehub/evr}. This repository contains all the necessary scripts and tools for working with the dataset. Included are data preprocessing scripts for normalising video frames, generating cutouts using FFmpeg, and handling data distribution and class imbalance with pandas and Matplotlib. Additionally, the repository provides model training and evaluation scripts for implementing and training deep learning models using PyTorch and fastai, with configurations for integrating pre-trained models from timm. There are also tools for evaluating model performance using metrics like accuracy, F1 score, and ROC curves, as well as for visualising data and model results with Matplotlib. To ensure reproducibility, nbdev scripts for creating reproducible and literate programs are included.

\vspace{2mm}
The repository also provides detailed documentation on the versions of software used and instructions on how to set up and run the scripts. Specific variables and parameters used to generate, test, and process the current dataset are provided within the scripts, ensuring the study can be replicated accurately. We aim to facilitate the reuse of our dataset and the replication of our study, allowing other researchers to build upon our work in the field of surgical timeline segmentation. For further assistance, please refer to the documentation in the GitHub repository or contact the corresponding authors.

\bibliography{sample}

% \noindent LaTeX formats citations and references automatically using the bibliography records in your .bib file, which you can edit via the project menu. Use the cite command for an inline citation, e.g. \cite{Kaufman2020, Figueredo:2009dg, Babichev2002, behringer2014manipulating}. For data citations of datasets uploaded to e.g. \emph{figshare}, please use the \verb|howpublished| option in the bib entry to specify the platform and the link, as in the \verb|Hao:gidmaps:2014| example in the sample bibliography file. For journal articles, DOIs should be included for works in press that do not yet have volume or page numbers. For other journal articles, DOIs should be included uniformly for all articles or not at all. We recommend that you encode all DOIs in your bibtex database as full URLs, e.g. https://doi.org/10.1007/s12110-009-9068-2.

\section*{Acknowledgements} (not compulsory)
% The authors acknowledge the following for help in compiling and labelling videos: SK. t writing and editing the manuscript.
% Acknowledgements should be brief, and should not include thanks to anonymous referees and editors, or effusive comments. Grant or contribution numbers may be acknowledged.

\section*{Author contributions statement}

% Must include all authors, identified by initials, for example:
A.A. conceived the experiment(s), A.A. and B.A. conducted the experiment(s), C.A. and D.A. analysed the results. All authors reviewed the manuscript. 

\section*{Competing interests} (mandatory statement)

% The corresponding author is responsible for providing a \href{https://www.nature.com/sdata/policies/editorial-and-publishing-policies#competing}{competing interests statement} on behalf of all authors of the paper. This statement must be included in the submitted article file.

% %\section*{Figures \& Tables}

% Figures, tables, and their legends, should be included at the end of the document. Figures and tables can be referenced in \LaTeX{} using the ref command, e.g. Figure \ref{fig:stream} and Table \ref{tab:example}. 

% Authors are encouraged to provide one or more tables that provide basic information on the main `inputs' to the study (e.g. samples, participants, or information sources) and the main data outputs of the study. Tables in the manuscript should generally not be used to present primary data (i.e. measurements). Tables containing primary data should be submitted to an appropriate data repository.

% Tables may be provided within the \LaTeX{} document or as separate files (tab-delimited text or Excel files). Legends, where needed, should be included here. Generally, a Data Descriptor should have fewer than ten Tables, but more may be allowed when needed. Tables may be of any size, but only Tables which fit onto a single printed page will be included in the PDF version of the article (up to a maximum of three). 

% Due to typesetting constraints, tables that do not fit onto a single A4 page cannot be included in the PDF version of the article and will be made available in the online version only. Any such tables must be labelled in the text as `Online-only' tables and numbered separately from the main table list e.g. `Table 1, Table 2, Online-only Table 1' etc.




\end{document}