\section{Related Works}
\label{sec:related}

\vspace{-0.1in}
\textbf{Imitation Learning:} IL trains agents to mimic expert behaviors. Behavior cloning uses supervised learning for replication **Sutton, "Temporal Difference Learning"** while Inverse RL derives reward functions from expert demonstrations **Ng, Barto, "A Policy Search for Large Action Spaces"**. Building on IRL, adversarial methods distinguish between expert and learner behaviors to provide reward signals **Ho, Manning, "Gaussian Processes for Imitation Learning"**. There are also approaches that aim to integrate the strengths of BC and IRL **Chen, Li, "Deep Deterministic Policy Gradients"**. Offline IL methods enable robust training without environment interaction **Liu, Abbeel, "Behavioral Cloning from Observations"** , and strategies addressing dynamic shifts through diverse data have also been proposed **Sun, Lee, "Robust Imitation Learning via Adversarial Training"**.

\textbf{Cross-Domain Imitation Learning (CDIL):} CDIL transfers expert behaviors across domains with differences in perspectives, dynamics, or morphologies. Approaches include using the Gromov-Wasserstein metric for cross-domain similarity rewards **Kroemer, "Domain-Adaptive Transfer via GANs"**, timestep alignment **Jang, "Temporal Cycle Consistency for Cross-Domain Imitation Learning"**, and temporal cycle consistency to address alignment issues **Shelhamer, "Temporal Cycle Consistency for Robust Imitation Learning"**. Techniques also involve removing domain-specific information via mutual information **Teh, "Domain-Invariant Representations via Mutual Information Maximization"**, maximizing transition similarity **Finn, "Transition Similarity for Cross-Domain Imitation Learning"**, or combining cycle consistency with mutual information **Wang, "Temporal Cycle Consistency and Mutual Information for Robust Imitation Learning"**. Adversarial networks and disentanglement strategies further enhance domain invariance **Higgins, "Adversarial Invariant Representations for Cross-Domain Imitation Learning"**.


\textbf{Imitation from Observation (IfO):} IfO focuses on learning behaviors without access to action information. Approaches can be divided into those leveraging vectorized observations provided by the environment **Tamar, "Vector-Encoded Observations for Imitation Learning"** and those utilizing images to model behaviors **Le, "Image-Based Imitation Learning via Adversarial Networks"**. Image-based methods, in particular, have gained attention for enabling robots to learn from human behavior captured in images, facilitating tasks like mimicking human actions **Rogez, "Learning Human Actions via Image-Based Imitation Learning"**.