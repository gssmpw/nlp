\section{Methodology}

In this section, we introduce the proposed approach \underline{H}olistically \underline{G}uided \underline{M}onte \underline{C}arlo \underline{T}ree \underline{S}earch, named \emph{HG-MCTS}, for the intricate information seeking task by leveraging exploration and exploitation capabilities of the MCTS algorithm.


\subsection{Overall Framework}

To tackle the challenges of the intricate information seeking task, we propose an integrated framework named HG-MCTS. Based on the reformulated information collecting based reasoning process, HG-MCTS seamlessly combines the capabilities of \emph{adaptive checklist} and \emph{multi-perspective reward modeling} to guide the MCTS process for targeted exploration on pre-planned retrieval sub-goals. Figure~\ref{fig:framework} conceptually illustrates the proposed framework.

The HG-MCTS framework begins by taking an \emph{intricate user query} as input. Leveraging the policy model, it generates an \emph{Adaptive Checklist}, consisting of anticipated sub-goals for resolving the query. This list of sub-goals constitutes an explicit guiding signal, significantly narrowing the expansive search space and enabling the MCTS to focus on promising retrieval sub-goal paths. Once the checklist is established, MCTS repeatedly performs the four canonical steps of \emph{selection}, \emph{expansion}, \emph{evaluation}, and \emph{backpropagation}. During \textbf{selection}, the node with the highest UCT value is chosen; in \textbf{expansion}, the policy model proposes a specialized query by incorporating both the \emph{adaptive checklist} and a running \emph{progress feedback} $u$ from the reward modeling. 
The search engine is subsequently queried to retrieve new documents, after which the policy model selects one document to extract key information in the form of snippets. This extracted knowledge is then assessed for inclusion in the existing \emph{knowledge memory} $\mathcal{D}$, ensuring that the information acquired at each step is systematically retained for subsequent iterations.

Another core of our approach lies in its \emph{multi-perspective reward modeling} (Section~\ref{sec-evaluation}) in \textbf{evaluation} phase, which provides comprehensive assessment and detailed progress feedback during the MCTS process. Specifically, the reward model produces two quantitative measures: the \emph{exploration reward}, which assesses whether the newly proposed query aligns well with unaddressed sub-goals, and the \emph{retrieval reward}, which measures how effectively the contained knowledge in the retrieved documents address the information need of the newly proposed subquery. These reward signals are aggregated and propagated in the MCTS tree in \textbf{backpropagation} phase, updating the value function of each visited node. In tandem with these numerical rewards, the \emph{progress feedback} furnishes textual hints about completed and remaining sub-goals in the adaptive checklist. This complementary reward modeling mechanism enables the policy model to dynamically revise the search strategies to explore more efficient reasoning paths.

Under our holistically guided approach, the MCTS algorithm persistently refines its node value with tailored rewards and explores new possible directions, ensuring coverage of all sub-goals. With the adaptive checklist, the MCTS process naturally terminates when the progress feedback indicates that all sub-goals on the checklist have been addressed. Finally, the knowledge snippets stored in the knowledge memory comprehensively cover every sub-goal of the user query and are consolidated to produce the final response. In this manner, {HG-MCTS} achieves a synergy between explicit goal specification and targeted exploration, offering an efficient and effective solution for intricate information seeking.


\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{pic/simple_checklist.pdf}
    \caption{An illustration of the checklist corresponding to an intricate query encompassing multiple sub-goals. 
    }
    \label{fig:checklist}
\end{figure}


\subsection{Holistically Guided Monte Carlo Tree Search}

To achieve holistic exploration for intricate information seeking, we first employ the LLM to generate an adaptive checklist as a holistic signal for MCTS, which then iteratively performs selection, expansion, evaluation, and backpropagation based on the LLM.

\subsubsection{Checklist Generation.} Previous MCTS studies usually rely on the MCTS algorithm to traverse all possible actions for deriving the optimal solution~\cite{alphago, YeLKAG21}, which will lead to massive search space and substantial computational costs. Therefore, we propose to incorporate \emph{Adaptive Checklist}, an explicit guiding signal that outlines all sub-goals required to solve the multifaceted queries in intricate information seeking. This checklist serves to direct the MCTS algorithm toward unsolved sub-goals, effectively avoiding meaningless action searches and thereby improving the overall search efficiency. Specifically, based on the input query $\bm{x}$, the policy model $\mathcal{M}_\theta$ first generates a checklist $p = \langle w_1,...,w_n \rangle $ in the form of lists relying on its understanding and internal knowledge as follows:
\begin{equation}
    w_{i+1} = \mathcal{M}_\theta(\bm{x}, \{w_1,...,w_i\}),
\end{equation}
where $w_{i+1}$ denotes the $(i+1)$-th token in the checklist text. Figure~\ref{fig:checklist} presents an illustrative example for the checklist in our approach. After generating a checklist as the global signal, the policy model iterates the four steps in MCTS, which are detailed in the following. 

\subsubsection{Selection}
In this process, the MCTS algorithm traverses the current tree starting from the root node using a specific strategy, which adopts a scoring function to optimally select nodes with the highest estimated value. The Upper Confidence Bound applied to Trees (UCT) formula~\cite{uct} is usually employed as the strategy in the selection process to traverse the tree. It balances the exploration-exploitation trade-off during tree traversal by selecting a node with the highest UCT score as below:
\begin{equation}
    UCT(s_t) = V(s_t) + w\sqrt{\frac{\ln{N(s_{t-1})}}{N(s_t)}},
\end{equation}
where $V(s_t)$ is the value of node $s_t$ which will be estimated at the evaluation process, $N(s_t)$ denotes the visit count of node $s_t$, $s_{t-1}$ is the parent node of $s_t$, and $w$ is a hyper-parameter controlling the exploration and exploitation between these two parts.

\subsubsection{Expansion}
After selecting a node $s_t$ with the highest UCT score, it will be expanded by generating multiple child nodes $\{s_{t+1}\}$. In our method, the policy model expands a new node in two steps, \ie firstly propose an intermediate subquery $q_{t+1}$ that aims to address certain sub-goals in the checklist and secondly retrieve some relevant documents from external sources (\eg web) via the proposed subquery. Compared to previous work~\cite{quiet-star,zhang2024llama} that expands new nodes only relying on historical states, we consider incorporating the adaptive checklist $p$ to generate more accurate and targeted subqueries $q$ as follows:
\begin{equation}
    q_{t+1} = \mathcal{M}_\theta(\mathcal{H}, p, \mathcal{D}),
\end{equation}
where the history context $\mathcal{H} = \{ \bm{x}, u, q_1,..., q_t \}$ contains the complex input query $\bm{x}$, last progress feedback $u$~(detailed in Section~\ref{sec:feedback}) and previous generated subqueries $q_1,..., q_t$, and $\mathcal{D}$ denotes the knowledge memory from the last state. In this formula, the checklist $p$ describes a list of completed and remaining unsolved sub-goals and steps, serving as a holistic search signal for addressing the input query.
This mechanism enables the MCTS algorithm to explore the space of unresolved sub-goals more efficiently, thereby avoiding redundant searches. Based on the proposed subquery $q_{t+1}$, the policy model then leverages search engine (\eg Google Search) to retrieve a set of documents $\mathcal{C}_{t+1}$ from the web. Since the primary objective of intricate information seeking is to gather and aggregate all necessary information as comprehensively as possible, we maintain a knowledge memory $\mathcal{D}$ throughout the entire MCTS process to store the collected knowledge snippets. After retrieving new documents $\mathcal{C}_{t+1}$ via $q_{t+1}$, the policy model will filters and incorporates only the useful information into the memory. To conserve memory space, we adopt an abstractive approach by selectively choosing the most relevant document and summarizing it to obtain the necessary content $k_{t+1}$, then the knowledge is subsequently evaluated and incorporated into the memory $\mathcal{D}$:
\begin{equation}
    \mathcal{D} \xleftarrow[]{\text{add}} k_{t+1} \xleftarrow[]{\text{summarize}} \mathcal{M}_\theta(\mathcal{C}_{t+1}),
\end{equation}
where $k$ represents the extracted knowledge content from the retrieved documents $\mathcal{C}_{t+1}$ by the policy model.

\subsubsection{Evaluation}
The evaluation process aims to compute the expected reward $r$ for the newly expanded nodes $s_{t+1}$ using the multi-perspective reward modeling (detailed in Section~\ref{sec-evaluation}). On the one hand, the reward model first computes an \emph{exploration reward} $r_q$ evaluating whether the generated subquery $q_{t+1}$ corresponds to specific unfinished sub-goals in the checklist and assessing its degree of global consistency based on the historical context information from the root node to the current node, \ie $\mathcal{H} = \{ \bm{x}, u, q_1, ..., q_t \}$. On the other hand, the reward model provides a \emph{retrieval reward} $r_k$ to evaluate the relevance of the retrieved knowledge snippet $k_{t+1}$. The final reward for the expanded node sums up the two aspects of rewards: 
\begin{equation}
    r = r_q \cdot r_k. 
    \label{eq:reward}
\end{equation}
We find that numerical rewards alone are insufficient to effectively guide the policy model in exploring the vast search space. Therefore, the reward model is further designed to provide a \textit{progress feedback} $u$ based on the adaptive checklist $p$, historical information $\mathcal{H}$, and the current state $s_{t+1}$. The progress feedback text will be returned to the policy model to update the checklist for indicating which goals in the checklist have been completed and which remain unresolved as follows:
\begin{equation}\label{eq-update-checklist}
    p =\text{Update}(p, u; \mathcal{M}_\theta).
\end{equation}

\subsubsection{Backpropagation} 
After evaluating the value of the newly expanded node, the remaining tree must be updated. So, the backpropagation process is performed, where the reward $r$ is propagated back from the new node $s_{t+1}$ to the root node $s_0$. During the process, the number of visit counts for each node is incremented by one and the node value will be accordingly updated as follows:
\begin{align}
    N_{\text{new}}(s_j) &= N_{\text{old}}(s_j) + 1,~~0 \leq j \leq t, \\
    V_{\text{new}}(s_j) &= \frac{V_{\text{old}}(s_j)N_{\text{old}}(s_j) + r}{N_{\text{new}}(s_j)},
\end{align}
where $N_{\text{old}}(s_j)$ and $V_{\text{old}}(s_j)$ represent the last visit count and value of node $s_j$ before backpropagation, respectively, and $r$ is the reward obtained from the evaluation process.

\subsubsection{Answer Generation}

By performing the above four steps iteratively, the MCTS process terminates until reaching the maximum tree level or the progress feedback indicates that the sub-goals in the checklist have been all addressed, so that it obtains a comprehensive knowledge memory $\mathcal{D}=\{k_1,...,k_T\}$. Based on the collected information, the policy model generates the final answer as follows:
\begin{equation}
    y_j = \mathcal{M}_\theta(\mathcal{D}, \{y_1,...,y_{j-1}\}),~~1 \leq j \leq m,
\end{equation}
where $y_j$ is the $j$-th token in the answer.

\subsection{Multi-Perspective Reward Modeling}
\label{sec-evaluation}

Traditional MCTS approaches usually perform expensive rollout operations to the current state until the terminal state evaluates the expected reward of the current state~\cite{alphago,YeLKAG21}. Inspired by previous work on process-supervised reward modeling~\cite{setlur2024rewarding,LightmanKBEBLLS24}, we propose directly leveraging an external reward model to provide step-by-step rewards from multiple perspectives, \ie exploration reward, retrieval reward, and progress feedback in Equation~(\ref{eq:reward}).

\subsubsection{Exploration Reward}
To evaluate the overall quality of the proposed intermediate subquery $q_{t+1}$ generated by the policy model, we propose to employ the reward model $\mathcal{R}_\theta$ to provide the exploration reward $r_q$.
This reward evaluates the subquery's consistency with any unfinished sub-goal in the checklist $p$, intricate user query $\bm{x}$ and assesses its degree of global consistency based on the historical context information from the root node to the current node:
\begin{equation}
    r_{q} = \mathcal{R}_\theta(q_{t+1}, p, \mathcal{H}).
\end{equation}
The exploration reward $r_q$ is an integer, either 0 or 1. An output of 0 indicates that, after comprehensively considering the given information, the exploration direction of the current intermediate subquery is deemed suboptimal. Conversely, an output of 1 signifies that the subquery is appropriate for acquiring the next piece of information.
The exploration reward assesses the directional information acquisition of the policy model at each step during the MCTS process, thereby providing effective guidance for identifying more optimal reasoning paths.


\subsubsection{Retrieval Reward}
To evaluate the comprehensiveness and relevance of atomic knowledge contents in the knowledge memory $\mathcal{D}$, we propose to employ the reward model $\mathcal{R}_\theta$ to compute the retrieval reward $r_k$:
\begin{equation}
    r_{k} = \mathcal{R}_\theta(q_{t+1}, k_{t+1}).
\end{equation}
We constrain $r_{k}$ to a three-tier integer scoring system, corresponding to three levels of semantic relevance between the atomic knowledge content $k_{t+1}$ the current subquery $q_{t+1}$:
\begin{equation}
r_k=\left\{
\begin{aligned}
0, \quad &\text{if}~k_{t+1}~\text{cannot satisfy the information need of}~q_{t+1}& \\
1, \quad &\text{if}~k_{t+1}~\text{partially satisfy the information need of}~q_{t+1}& \\
2, \quad &\text{if}~k_{t+1}~\text{fully satisfy the information need of}~q_{t+1}&
\end{aligned}
\right. \nonumber
\end{equation}
The retrieval reward assesses the policy model’s actions in searching for and extracting relevant information, facilitating the discovery of reasoning paths with enhanced information acquisition quality.





\subsubsection{Progress Feedback}
\label{sec:feedback}
Numerical rewards alone may not effectively guide the policy model through the extensive search space. Therefore, the reward model is further designed to deliver a \textit{progress feedback} $u$, based on the subquery $q_{t+1}$, adaptive checklist $p$, historical information $\mathcal{H}$, and knowledge memory $\mathcal{D}$. The progress feedback $u$ obeying the text form, which can be generated as:
\begin{equation}
    u = \mathcal{R}_\theta(q_{t+1}, p, \mathcal{H}, \mathcal{D}).
\end{equation}
Progress feedback delineates which sub-goals in the checklist have been achieved and which remain unresolved, thereby offering clearer direction for the policy model's exploration. The progress feedback will be used to update the adaptive checklist in Equation~(\ref{eq-update-checklist}).
Based on the statistical monitoring of each sub-goal’s completion status, progress feedback can issues an explicit termination signal upon the fulfillment of all sub-goals, thereby concluding the current search process.

Furthermore, certain queries resist decomposition into atomic information-gathering sub-goals, leading to instances where the checklist cannot explicitly define sub-goals prior to the reasoning. For example, the question ``What were the release dates of the studio albums released by the band Destiny’s Child?'' cannot ascertain the required number of search steps without first determining the total number of Destiny’s Child studio albums. To address this limitation, the progress feedback enables the checklist to be {refined dynamically} during the reasoning process based on newly acquired information, thereby accommodating queries that require additional information to establish an adaptive checklist.


