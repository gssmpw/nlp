\section{Related Work}
\label{sec:related}

\subsection{Test-Time Slow Thinking with LLMs}

The integration of the slow thinking paradigm~(\aka System 2) inspired reasoning techniques into LLMs has emerged as a pivotal research area~\cite{sutton2019bitter, wang2024q, kahneman2011thinking}, focusing on enhancing the problem-solving ability and the interoperability of LLM. A notable example is OpenAIâ€™s o1 model\footnote{https://openai.com/o1/}, which incorporates extended internal reasoning chains during inference. 
This breakthrough has demonstrated remarkable success on programming and complex scientific benchmarks, which improve the focus of test-time reasoning that leverages extended inference processes to simulate deliberate, stepwise problem-solving akin to human-like cognitive processes without additional training~\cite{zhang2024llama, putta2024agent, luo2024improve}. Techniques such as chain-of-thought~\cite{wei2022chain} and self-consistency decoding~\cite{wangself} exemplify this paradigm, where generating intermediate reasoning steps or exploring multiple solution paths improves reliability and interoperability.
Recent advancements further extend these ideas by integrating search-based algorithms, such as beam search~\cite{kang2024mindstar} and Monte Carlo Tree Search (MCTS)~\cite{zhoulanguage, chen2024alphamath, zhang2024rest}, to systematically explore alternative reasoning trajectories. By exploring multiple outcome branches during inference, search-based methods achieve a favorable exploration-exploitation trade-off and have been widely adopted in reinforcement learning~\cite{hart1968formal, silver2017mastering} and real-world systems such as AlphaGo~\cite{silver2016mastering}. These methods are often guided by reward models, which provide feedback based on procedural or outcome-driven metrics to improve reasoning quality~\cite{snell2024scaling} iteratively. 
These collective efforts underscore a paradigm shift in LLM research, highlighting the complementary relationship between training-time strategies and scalable test-time reasoning mechanisms. 



\subsection{Web Search with Complex Reasoning}

Recent advances in LLMs have shifted web search beyond simple query-response paradigms to sophisticated methods capable of multi-step reasoning for complex information access~\cite{chen2024mindsearch, reddy2024infogent}. These approaches leverage generative models to integrate and interpret diverse information sources in real time. Such capabilities are particularly critical for intricate queries that require synthesizing fragmented or context-sensitive data, where conventional search systems often fail to maintain coherence or overlook essential insights~\cite{hoveyda2024aqa, khotdecomposed}.
Initial efforts, such as WebGPT~\cite{nakano2021webgpt}, follow an iterative process of query generation, information retrieval, summarization, and synthesizing information in response to user queries.
Building on this foundation, subsequent studies adopt chain-of-thought reasoning to decompose complicated tasks into more manageable subqueries, enabling stepwise verification and refinement~\cite{yao2023react, trivedi2023interleaving}. For instance, Search-in-the-Chain~\cite{xu2024search} systematically disaggregates complex information-seeking queries by iteratively generating partial hypotheses and validating them against web-based evidence. 
Additionally, multi-agent collaboration has been explored to further enhance the search process. 
MindSearch~\cite{chen2024mindsearch} employs a planner-searcher architecture to conduct planning based on directed acyclic graphs and to carry out hierarchical information seeking. Similarly, Infogent~\cite{reddy2024infogent} introduces a multi-module collaborative agent framework that orchestrates information aggregation across modular components.
These systems excel in dynamically managing query reformulation, evidence synthesis, and inferential reasoning, seamlessly adapting to emerging information during the search process.
Moreover, some studies incorporate explicitly retrieved external information into the MCTS reasoning process, enhancing the deliberate reasoning capabilities of LLMs in multi-hop problem-solving~\cite{lee2024zero, jiang2024rag}. In contrast to these approaches, our method reformulates the task as an information collection process with the introduction of a novel checklist-based planning mechanism to holistically guide the MCTS reasoning process. Furthermore, we combines quantitative progress reward and qualitative progress feedback in reward modeling, making the MCTS process more intelligent to explore more efficient reasoning paths.
