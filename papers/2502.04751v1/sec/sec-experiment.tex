
\begin{table*}[t]
% \setlength\tabcolsep{5pt}
\centering
\small
\scalebox{1}{
\begin{tabular}{lccccccccccccccc}
\toprule
\multicolumn{1}{c}{\multirow{2.5}{*}{\textbf{Method}}} & \multicolumn{3}{c}{{HotpotQA}} & \multicolumn{3}{c}{{2WikiMultihopQA}} & \multicolumn{3}{c}{{MuSiQue}} & \multicolumn{3}{c}{{StrategyQA}} & \multicolumn{3}{c}{\textbf{Average}} \\
\cmidrule(r){2-4} \cmidrule(r){5-7} \cmidrule(r){8-10} \cmidrule(r){11-13} \cmidrule(r){14-16}
\multicolumn{1}{c}{} & \textbf{EM} & \textbf{CEM} & \textbf{F1} & \textbf{EM} & \textbf{CEM} & \textbf{F1} & \textbf{EM} & \textbf{CEM} & \textbf{F1} & \textbf{EM} & \textbf{CEM} & \textbf{F1} & \textbf{EM} & \textbf{CEM} & \textbf{F1} \\

\midrule
\multicolumn{16}{c}{\textit{GPT-4o-mini (Closed-source)}} \\
\midrule
% \cmidrule{1-16}
Closed-Book & 28.82 & 39.80 & 34.71 & 24.71 & 30.40 & 24.71 & 7.65 & 15.91 & 9.41 & 73.53 & 73.53 & 73.53 & 33.68 & 39.91 & 35.59 \\
\multicolumn{1}{l}{Chain-of-Tought} & 26.47 & 38.79 & 32.35 & 24.12 & 30.84 & 26.47 & 13.53 & 20.92 & 18.24 & 51.76 & 52.09 & 51.76 & 28.97 & 35.66 & 32.21 \\
\multicolumn{1}{l}{Standard RAG} & 41.18 & 54.36 & 52.94 & 25.88 & 32.09 & 27.65 & 11.76 & 19.91 & 16.47 & 58.24 & 59.53 & 58.24 & 34.27 & 41.47 & 38.83 \\
\cmidrule{2-16}
\multicolumn{1}{l}{ReAct} & 35.88 & 51.08 & 42.35 & 29.41 & 35.46 & 30.00 & 10.00 & 18.05 & 12.35 & 36.47 & 40.46 & 36.47 & 27.94 & 36.26 & 30.29 \\
\multicolumn{1}{l}{Query2doc} & 44.71 & 57.21 & 54.71 & 29.41 & 34.46 & 29.41 & 19.41 & 28.05 & 24.71 & 64.71 & 65.67 & 64.71 & 39.56 & 46.35 & 43.39 \\
\multicolumn{1}{l}{Self-RAG} & 38.82 & 50.32 & 47.65 & 26.47 & 31.87 & 27.65 & 13.53 & 21.29 & 16.47 & 68.82 & 69.10 & 68.82 & 36.91 & 43.15 & 40.15 \\
% \multicolumn{1}{l}{Mind-Search} & 44.0 & 50.0 & 58.6 & 28.0 & 29.0 & 32.2 & 14.0 & 16.0 & 22.8 & \textbf{72.0} & 74.0 & \textbf{72.0} \\
% \multicolumn{1}{l}{Infogent} & 44.0 & 50.0 & 58.6 & 28.0 & 29.0 & 32.2 & 14.0 & 16.0 & 22.8 & \textbf{72.0} & 74.0 & \textbf{72.0} \\
% \multicolumn{1}{l}{RAG-Star}  \\
\cmidrule{2-16}
Ours & 41.76 & 45.88 & 58.69 & 52.94 & 65.75 & 53.53 & 23.67 & 33.21 & 26.04 & 77.67 & 77.67 & 77.67 & \ \ \textbf{49.01}$^{\dagger}$ & \ \ \textbf{55.63}$^{\dagger}$ & \ \ \textbf{53.98}$^{\dagger}$ \\

\midrule
\multicolumn{16}{c}{\textit{Gemini-1.5-flash (Closed-source)}} \\
\midrule
Closed-Book & 19.41 & 31.52 & 24.71 & 24.12 & 30.07 & 24.71 & 3.53 & 10.98 & 6.47 & 32.35 & 32.83 & 32.35 & 19.85 & 26.35 & 22.06 \\
\multicolumn{1}{l}{Chain-of-Tought} & 28.82 & 36.43 & 34.71 & 18.24 & 21.35 & 18.82 & 8.24 & 13.73 & 10.59 & 68.82 & 69.64 & 68.82 & 31.03 & 35.29 & 33.24 \\
\multicolumn{1}{l}{Standard RAG} & 37.65 & 48.95 & 47.06 & 16.47 & 20.54 & 17.65 & 7.06 & 11.54 & 10.00 & 52.94 & 53.89 & 52.94 & 28.53 & 33.73 & 31.91 \\
\cmidrule(lr){2-16}
\multicolumn{1}{l}{ReAct} & 34.71 & 45.44 & 42.35 & 18.24 & 22.33 & 18.82 & 5.88 & 10.38 & 7.06 & 34.91 & 36.03 & 34.91 & 23.44 & 28.55 & 25.79 \\
\multicolumn{1}{l}{Query2doc} & 38.16 & 49.15 & 47.81 & 17.06 & 20.32 & 17.65 & 11.18 & 17.65 & 14.71 & 58.24 & 58.53 & 58.24 & 31.16 & 36.41 & 34.60\\
\multicolumn{1}{l}{Self-RAG} & 39.83 & 49.47 & 47.88 & 15.29 & 19.06 & 17.65 & 7.65 & 12.09 & 10.00 & 54.12 & 54.53 & 54.12 & 29.22 & 33.79 & 32.41 \\
% \multicolumn{1}{l}{Mind-Search} &  &  &  &  &  &  &  &  &  &  &  &  \\
% \multicolumn{1}{l}{Infogent} &  &  &  &  &  &  &  &  &  &  &  &  \\
% \multicolumn{1}{l}{RAG-Star} &  &  &  &  &  &  &  &  &  &  &  &  \\
\cmidrule(lr){2-16}
Ours & 47.65 & 57.65 & 61.98 & 62.94 & 62.94 & 73.94 & 17.75 & 21.30 & 28.31 & 76.19 & 76.19 & 76.19 & \ \ \textbf{51.13}$^{\dagger}$ & \ \ \textbf{54.52}$^{\dagger}$ & \ \ \textbf{60.11}$^{\dagger}$ \\
\midrule
% \multicolumn{13}{c}{\textit{Claude-3.5-sonnet (Closed-source)}} \\
% \midrule
% Closed-Book & 33.82 & 47.01 & 43.53 & 34.71 & 39.70 & 35.88 & 16.76 & 25.91 & 20.00 & 68.82 & 70.27 & 68.82\\
% \multicolumn{1}{l}{Chain-of-Tought} & 34.86 & 48.31 & 42.51 & 40.00 & 49.52 & 40.59 & 16.03 & 26.15 & 19.41 & 19.51 & 27.42 & 19.51 \\
% \multicolumn{1}{l}{Standard RAG} & 40.64 & 53.85 & 51.24 & 17.65 & 21.33 & 18.82 & 12.68 & 17.79 & 15.96 & 57.00 & 60.11 & 57.00 \\
% \cmidrule(lr){2-13}
% \multicolumn{1}{l}{ReAct} & 28.01 & 40.01 & 35.18 & 12.29 & 15.31 & 12.85 & 10.75 & 16.81 & 12.37 & 19.89 & 24.73 & 19.89 \\
% \multicolumn{1}{l}{Query2doc} & 46.50 & 59.86 & 57.61 & 19.81 & 23.46 & 21.70 & 21.97 & 28.88 & 24.28 & 56.00 & 59.14 & 56.00 \\
% \multicolumn{1}{l}{Self-RAG} & 40.23 & 51.66 & 48.28 & 0.00 & 0.00 & 0.00 & 0.00 & 29.17 & 0.00 & 80.00 & 84.44 & 80.00 \\
% % \multicolumn{1}{l}{Mind-Search} &  &  &  &  &  &  &  &  &  &  &  &  \\
% % \multicolumn{1}{l}{Infogent} &  &  &  &  &  &  &  &  &  &  &  &  \\
% \multicolumn{1}{l}{RAG-Star} &  &  &  &  &  &  &  &  &  &  &  &  \\
% \cmidrule(lr){2-13}
% Ours &  &  &  &  &  &  &  &  &  &  &  &  \\
\midrule
\multicolumn{16}{c}{\textit{DeepSeek-V3-chat (Open-source)}} \\
\midrule
Closed-Book & 35.88 & 48.58 & 44.12 & 35.88 & 41.85 & 37.06 & 11.18 & 19.54 & 14.71 & 64.71 & 65.10 & 64.71 & 36.91 & 43.77 & 40.15 \\
\multicolumn{1}{l}{Chain-of-Tought} & 38.82 & 50.59 & 47.06 & 47.06 & 56.14 & 48.24 & 21.18 & 29.99 & 24.12 & 41.76 & 47.46 & 41.76 & 37.21 & 46.12 & 40.30 \\
\multicolumn{1}{l}{Standard RAG} & 40.00 & 55.01 & 50.00 & 30.00 & 34.26 & 31.18 & 15.88 & 24.47 & 18.82 & 64.12 & 65.59 & 64.12 & 37.50 & 44.83 & 41.03 \\
\cmidrule(lr){2-16}
\multicolumn{1}{l}{ReAct} & 40.00 & 54.97 & 48.24 & 32.35 & 36.17 & 32.35 & 16.67 & 34.07 & 20.83 & 23.53 & 28.79 & 23.53 & 28.14 & 38.50 & 31.24 \\
\multicolumn{1}{l}{Query2doc} & 47.19 & 63.11 & 58.05 & 32.35 & 37.14 & 32.35 & 21.18 & 29.89 & 24.71 & 57.65 & 59.40 & 57.65 & 39.59 & 47.39 & 43.19 \\
\multicolumn{1}{l}{Self-RAG} & 43.49 & 56.31 & 51.75 & 27.65 & 32.27 & 28.24 & 16.86 & 25.30 & 19.77 & 52.87 & 53.66 & 52.87 & 35.22 & 41.89 & 38.16 \\
% \multicolumn{1}{l}{Mind-Search} &  &  &  &  &  &  &  &  &  &  &  &  \\
% \multicolumn{1}{l}{Infogent} &  &  &  &  &  &  &  &  &  &  &  &  \\
% \multicolumn{1}{l}{RAG-Star} &  &  &  &  &  &  &  &  &  &  &  &  \\
\cmidrule{2-16}
\textbf{Ours} & 45.88 & 52.94 & 62.99 & 60.00 & 63.53 & 73.15 & 21.30 & 32.60 & 25.44 & 76.47 & 76.47 & 76.47 & \ \ \textbf{50.91}$^{\dagger}$ & \ \ \textbf{56.39}$^{\dagger}$ & \ \ \textbf{59.51}$^{\dagger}$ \\
\bottomrule
\end{tabular}}
\caption{The evaluation results for four representative multi-hop QA datasets are presented, we also report the average results of the four datasets. The symbol ``$^{\dagger}$'' denotes that the performance improvement is statistically significant with p < 0.05 compared against all the baselines.}
\label{tab:main-result}
\end{table*}



\section{Experiments and Analysis}
In this section, we begin by detailing the experimental setup, followed by a comprehensive presentation of the primary results. Subsequently, we conduct an ablation study and provide an in-depth analysis to elucidate our findings further.

\subsection{Experimental Settings}


\subsubsection{Datasets}
% FRAMES~\cite{krishna2024fact}, and
To rigorously assess the efficacy of our proposed methodology, we conducted evaluations using 5 benchmark datasets. All datasets are meticulously designed to challenge models with complex, multi-hop questions that require synthesizing information across multiple documents.
% The detailed statistics of different datasets are shown in Table~\ref{tab:datasets}.

\begin{itemize}
    \item \textbf{FanOutQA}~\cite{zhu2024fanoutqa} is a high-quality dataset comprising complex information-seeking questions and human-written decompositions, which necessitate aggregating information about multiple entities from several sources to formulate a comprehensive answer. 
    % \item \textbf{FRAMES} comprises multi-hop questions requiring integration of information from multiple sources. These questions span diverse topics, and are labeled with reasoning types such as numerical reasoning, tabular reasoning, multiple constraints, temporal reasoning, and post-processing. Following existing study~\cite{reddy2024infogent}, we exclude numerical questions since different LLMs tend to be highly sensitive to them.

    \item \textbf{HotpotQA}~\cite{yang2018hotpotqa} is a widely used dataset for multi-hop question answering, designed to evaluate reason abilities across multiple documents to answer complex questions. The dataset is collected via crowdsourcing based on Wikipedia articles, and annotators are asked to propose questions that require reasoning using the multiple presented Wikipedia articles.

    \item \textbf{2WikiMultihopQA}~\cite{ho2020constructing} is a large-scale multi-hop QA dataset that requires reading multiple paragraphs to answer a given question. The dataset includes four types of questions: comparison, inference, compositional, and bridge-comparison. Each question is accompanied by relevant Wikipedia paragraphs as evidence.
    
    \item \textbf{MuSiQue}~\cite{trivedi2022musique} is created by composing questions from multiple existing single-hop datasets. The dataset is more challenging than previous multi-hop reasoning datasets, with a threefold increase in the human-machine gap and significantly lower disconnected reasoning scores, indicating reduced susceptibility to shortcut reasoning.
    
    \item \textbf{StrategyQA}~\cite{geva2021did} focuses on open-domain questions that require implicit reasoning steps. The dataset consists of 2,780 examples, each comprising a strategy question, its decomposition, and evidence paragraphs. Each question is accompanied by decomposed reasoning steps and relevant Wikipedia paragraphs as evidence.


    
\end{itemize}



\subsubsection{Evaluation Metrics}
We adopted the established evaluation metrics for the adopted datasets to ensure consistency and comparability. For the evaluation of \textit{FanOutQA}, we employed string accuracy, which measures the proportion of exact matches, and ROUGE metrics~\cite{lin2004rouge}, which assess the quality of summarization by comparing multiple features between the generated and reference texts. Specifically, we report ROUGE-1 (R-1), ROUGE-2 (R-2), and ROUGE-L (R-L) scores to comprehensively assess performance.
% For FRAMES, a large language model~(LLM) was utilized to benchmark the alignment of generated answers with ground-truth annotations. We present the evaluation results separately for queries corresponding to each of the four reasoning types.
For \textit{HotpotQA, 2WikiMultihopQA, MuSiQue}, and \textit{StrategyQA}, we adopt Exact Match (EM), F1 score, and Cover Exact Match (CEM) as evaluation metrics. EM measures strict correctness by checking if the predicted answer matches the ground truth. F1 evaluates the overlap between prediction and ground truth, balancing precision and recall. CEM extends EM to multi-hop reasoning, requiring both correct answers and coverage of intermediate reasoning steps.
Similar to the setup in the existing work~\cite{jiang2024rag}, due to the large data scale, we randomly sampled 130 queries from each of the four datasets for evaluation.

\subsubsection{Baselines}
In the evaluation of our proposed method, we compare it against abundantly established baselines to ensure a comprehensive understanding of its performance. These baselines represent a spectrum of approaches commonly employed in the field, ranging from vanilla reasoning strategies to advanced reasoning methods.

\paratitle{Vinilla reasoning.}\quad
The \textit{Closed-Book} method directly prompts the LLM to provide an answer to a question. In contrast, \textit{Chain-of-Thought (CoT)}~\cite{wei2022chain} reasoning involves adding intermediate reasoning steps to facilitate the response. \textit{Standard RAG} first retrieves passages from the Wikipedia corpus using DPR~\cite{dpr2020} and then directly prompts the LLM to refer to these passages in its response. 

\paratitle{Advanced reasoning.}\quad
\textit{ReAct}~\cite{yao2023react} progressively addresses subqueries, ultimately consolidating the intermediate results to form a complete answer.
\textit{Query2doc}~\cite{wang2023query2doc} generates an initial answer using the model and subsequently retrieves relevant information to generate the final answer.
\textit{Self-RAG}~\cite{asaiself} involves first retrieving information and then assessing its relevance before deciding whether to incorporate it into the final answer.
\textit{MindSearch}~\cite{chen2024mindsearch} employs a planner-searcher architecture for searching relevant information.
\textit{Infogent}~\cite{reddy2024infogent} introduces a multi-agent architecture to aggregate multi-source information.
% \textit{RAG-Star}~\cite{jiang2024rag} adopts MCTS with retrieval for deliberate reasoning for multi-hop questions.


To ensure a more comprehensive evaluation of different methods, and to mitigate the influence of any specific model, we employ multiple LLMs as backbone models of different methods, including the closed-source LLM \texttt{GPT-4o-mini}, \texttt{Gemini-1.5-flash-002} and the open-source LLM \texttt{deepseek-v3-chat}.
We evaluated all baseline methods in a \textit{zero-shot} setting, employing them solely for inference without additional training.
Note that certain methods are challenging to replicate across all datasets due to the requirement of dataset-specific refinement. As a result, we are unable to report the results for all baselines on every dataset.


\subsubsection{Implement Details}

For web search, we employed Google Search as the primary search engine, selecting the top-3 web search results as document candidates and adhering to existing methodologies for web crawling and denoising~\cite{reddy2024infogent}. During the MCTS process, we ensured consistency between the policy model and the reward model. The maximum number of simulations was capped at 40, and the search depth was limited to 6 layers. In the upper confidence bound for trees algorithm, the exploration-exploitation balance parameter \( w \) was set to 0.2. Additionally, we generated three sub-queries per iteration (\( m_q = 3 \)).
For all generation tasks, responses were sampled using a temperature of 0.9 and top-\( p \) sampling with \( p = 1.0 \).  All prompts used are shown in the provided anonymous codes.




\subsection{Main Results}
The results of different methods evaluated on five complex reasoning datasets are shown in Table~\ref{tab:main-result} and Table~\ref{tab:fanoutqa}. It can be observed that:

(1) Our proposed method demonstrates significant improvements over all baseline approaches across four multi-hop QA datasets and the FanoutQA dataset that emphasizes the information-gathering task. This performance advantage is consistently observed across multiple popular backbone LLMs, highlighting the general applicability and effectiveness of our HG-MCTS framework. HG-MCTS employs an adaptive checklist to guide the expansion and reward modeling of Monte Carlo Tree Search (MCTS), effectively curtailing the exploration of unproductive pathways while maintaining robust search capabilities. In addition, its emphasis on targeted information gathering minimizes irrelevant content, reducing extraneous noise that could compromise the quality of the generated answers.

(2) We further observe that the MCTS-based approach for complex reasoning outperforms both single-step reasoning and chain-based reasoning methods, indicating that tree search can substantially expand the search space of solution paths, enable the discovery of optimal reasoning paths, and ultimately enhance performance. In addition, incorporating advanced reasoning strategies into RAG proves superior to vanilla reasoning standard RAG or closed-book approaches overall, highlighting the importance of meeting users’ complex information needs through tailored retrieval processes. Notably, on the FanoutQA dataset, our method based on GPT-4o-mini surpasses the baseline built upon the larger and more powerful LLM GPT-4o, offering further evidence for the intrinsic advantages of our approach.

(3) Finally, our method exhibits strong robustness across different backbone LLMs, whereas other methods often experience significant performance fluctuations depending on the underlying model. For example, when using Gemini as the backbone, the performance of baseline methods drops markedly compared with results obtained using the other two LLMs, while our method does not exhibit performance degradation. Additionally, we observe that the open-source model deepseek-v3-chat demonstrates capabilities comparable to its proprietary counterparts on various challenging multi-hop QA tasks, thereby laying a promising foundation for real-world deployment of related methodologies.


\begin{table}[t]
    \centering
    \small
\scalebox{1.015}{
    \setlength{\tabcolsep}{3.3pt} 
    \begin{tabular}{llcccc}
        \toprule
        \textbf{Method} & \textbf{LLM} & \textbf{Acc.} & \textbf{R-1} & \textbf{R-2} & \textbf{R-L} \\
        \midrule
        Closed-Book & LLaMA3 & 46.60 & 46.30 & 26.40 & 38.70 \\
        Closed-Book & GPT-4o & 44.10 & 47.40 & 27.30 & 41.70 \\
        Standard RAG & LLaMA3 & 46.80 & 28.20 & 14.30 & 24.30 \\
        Standard RAG & GPT-4o & 58.00 & 49.40 & 31.00 & 44.30 \\
        MindSearch & GPT-4o-mini & 47.30 & 49.30 & 28.40 & 44.20 \\
        Infogent & GPT-4o-mini & 51.10 & 53.30 & 33.00 & 48.50 \\
        \midrule
        \textbf{Ours} & GPT-4o-mini & \ \ \textbf{58.38}$^{\dagger}$ & \ \ \textbf{55.02}$^{\dagger}$ & \ \ \textbf{35.45}$^{\dagger}$ & \ \ \textbf{49.40}$^{\dagger}$ \\
        \bottomrule
    \end{tabular}
    }
    \caption{The evaluation results on FanoutQA. The symbol ``$^{\dagger}$'' denotes that the performance improvement is statistically significant with p < 0.05 compared against all the baselines. LLaMA3 is the abbreviation of LLaMA3-70B-Instruct.}
    \label{tab:fanoutqa}
\end{table}


\subsection{Ablation Studies}
\label{sec:ablation}

In this section, we conduct an ablation study to validate the effectiveness of key strategies in HG-MCTS comprehensively on FanoutQA. Here, we consider five variants based on HG-MCTS for comparison: (a) \underline{\textit{w/o Exploration Reward}} removes the exploration reward during reward modeling; (b) \underline{\textit{w/o Retrieval Reward}} removes the retrieval reward from the total reward during reward modeling; (c) \underline{\textit{w/o Progress Feedback}} eliminates generating the progress feedback in reward modeling; (d) \underline{\textit{w/o Checklist}} removes checklist for global guidance in the MCTS process; (e) \underline{\textit{w/o HG-MCTS}} removes the entire HG-MCTS strategy, reducing the approach to a linear reasoning strategy.

Table~\ref{tab:ablation} presents the results for the variants of our method, from which we can observe the following findings: 
(a) The performance drops in \underline{\textit{w/o Exploration Reward}}, demonstrating that incorporating exploration rewards facilitates more effective expansions during the tree search process. 
(b) The performance drops in \underline{\textit{w/o Retrieval Reward}}, demonstrating incorporating exploration rewards enables the model to better analyze the benefits of external retrieval information during the MCTS process, thereby achieving improved comprehensive information gathering.
(c) The performance drops in \underline{\textit{w/o Progress Feedback}}, underscoring the necessity of incorporating textual feedback to guide subsequent explorations and dynamically updating the checklist.
(d) The performance drops in \underline{\textit{w/o Checklist}}, demonstrating that incorporating the explicit checklist can effectively guide the expansion and reward modeling in MCTS.
(e) The performance significantly drops in \underline{\textit{w/o HG-MCTS}}, demonstrating that the proposed HG-MCTS plays a pivotal role in enhancing the effectiveness of information seeking.

\begin{table}[t]
    \centering
    \small
\scalebox{1.03}{
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Method} & \textbf{Acc.} & \textbf{R-1} & \textbf{R-2} & \textbf{R-L}  \\
        \midrule
        Ours  & 58.38 & 55.02 & 35.45 & 49.40 \\
        \midrule
        w/o Exploration Reward & 57.23 & 54.20 & 34.91 & 48.87  \\
        w/o Retrieval Reward & 56.39 & 53.68 & 34.06 & 48.15  \\
        w/o Progress Feedback & 54.57 & 52.94 & 33.73 & 47.89  \\
        w/o Checklist & 55.03 & 53.41 & 33.85 & 48.03  \\
        w/o HG-MCTS & 52.55 & 53.25 & 32.74 & 47.82  \\
        \bottomrule
    \end{tabular}}
    \caption{Evaluation results of the proposed method's variants on FanoutQA.}
    \label{tab:ablation}
\end{table}

\begin{figure}[t!]
    \centering
    \subfigure[w/o HG-MCTS]{
        \includegraphics[width=0.38\linewidth]{pic/recall_baseline.pdf}
        \label{fig:subfig1}
    }
    \hspace{0.02\textwidth} 
    \subfigure[HG-MCTS]{
        \includegraphics[width=0.38\linewidth]{pic/recall_ours.pdf}
        \label{fig:subfig2}
    }
    \caption{Information collection evaluation for different methods on Recall rate~(blue part).}
    \label{fig:recall}
\end{figure}

\subsection{Analysis on Information Collection}
To evaluate the comprehensiveness of information collected by the proposed HG-MCTS method during the reasoning process, we conducted a systematic comparison of different methods. Here, we compare the proposed method with the w/o HG-MCTS variant in Section~\ref{sec:ablation}, which also employs an information-gathering process.
Specifically, we evaluate the comprehensiveness by calculating the recall rate of the set of web pages retrieved by different methods for the ground-truth Wikipedia pages annotated as solving each intricate query. 
The ground truth serves as a benchmark, representing the authoritative and comprehensive sources necessary for addressing the query. By analyzing the coverage of the retrieved web pages relative to the ground truth, we aimed to quantify the ability of each method to gather a sufficient and relevant body of information.

As shown in Figure~\ref{fig:recall}, our method demonstrates a higher recall rate compared to the baseline in information collection evaluation, indicating that the proposed targeted MCTS enables a more comprehensive collection of information relevant to user queries. Furthermore, our method avoids the tendency to indiscriminately gather excessive information in pursuit of a higher recall rate. Such an approach, while potentially increasing coverage, introduces substantial noise into the subsequent aggregation task, thereby compromising the overall effectiveness of the information seeking.


\subsection{Scaling Law on Simulation Amount}

Our experiments systematically explore the impact of varying the number of simulation iterations on the performance of our analysis method. We adopt two LLM backbones GPT-4o-mini and DeepSeek-V3-chat on the FanoutQA dataset with various simulation numbers for analysis. 

As shown in Figure~\ref{fig:simulation}, we find that increasing the simulation count initially yields significant gains in the model’s ability to navigate the solution space effectively. With more iterations, the method benefits from a broader exploration of potential reasoning paths, leading to enhanced accuracy and improved recall in downstream tasks. In particular, the enhanced exploration reduces the likelihood of early-stage errors propagating through subsequent steps, thereby reinforcing the overall integrity of the search process.
Moreover, our results also reveal a point of diminishing returns. Beyond a certain number of simulations, additional iterations contribute only marginal improvements to the final performance. This saturation effect can be attributed to the inherent limitations of LLM's internal knowledge and the increased computational overhead, which together may lead to overly complex decision paths without proportional benefits. Thus, while extended simulations promote a more thorough examination of the search space, they also impose a trade-off between improved performance and computational efficiency.



\begin{figure}[t!]
    \centering
    \subfigure[GPT-4o-mini]{
        \includegraphics[width=0.4772\linewidth]{pic/scale_4o.pdf}
        \label{fig:subfig1}
    }
    \subfigure[DeepSeek-V3-Chat]{
        \includegraphics[width=0.4772\linewidth]{pic/scale_ds.pdf}
        \label{fig:subfig2}
    }
    \caption{Evaluation results of HG-MCTS with various simulation numbers employed by different LLMs on FanoutQA.}
    \label{fig:simulation}
\end{figure}