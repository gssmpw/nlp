\section{Related Work}
\label{relatedwork}
Budget allocation across multiple ad campaigns \cite{farris2015marketing, 10.5555/3618408.3618709} has been extensively studied in industrial research by companies like Criteo \cite{DElia2019}, Netflix \cite{lewis2022incrementality}, and Lyft \cite{hancontextual}. A common approach is to discretize the budget and model each sub-campaign as an arm in a multi-armed bandit problem. The optimal allocation is obtained by solving a combinatorial optimization problem \cite{zhang2017multi}  based on the expected reward of each arm. In previous literature, domain knowledge has been used to formulate parametric models of the arms, approximating the cost-to-reward function with a power law \cite{hanbudget} or a sigmoid \cite{giglimulti}, followed by Thompson Sampling to handle uncertainty and induce exploration. However, these methods often overlook noise in the data, a critical factor in real-world deployments. In the presence of noise, parametric models can significantly deviate from the true reward function. A more flexible alternative is to model the reward function using Gaussian Process (GP) models \cite{nuara2022online, Nuara2018ACA}, which allow for greater adaptability. These algorithms typically use Upper Confidence Bound (UCB) or Thompson Sampling (TS) to guide exploration. However, unlike our approach, they do not incorporate domain knowledge to promote exploration, which can lead to higher regret. Additionally , these algorithms are mostly studied for budget allocation for a single day or month \cite{nuara2022online} which does not account for changing behaviours of the reward function, a characteristic often observed in campaigns running over many months.

Handling non-stationarity in multi-armed bandits is a well-studied problem in the literature \cite{cavenaghi2021non, besbes2014stochastic, re2021exploiting}. Common methods include passive approaches, such as sliding windows with UCB or TS sampling \cite{trovo2020sliding}, or using discounted rewards \cite{garivier2011upper}. Active methods, such as change point detection \cite{liu2018change, cao2019nearly}, offer a more dynamic approach. Passive methods either discard older data points or assign them less weight. However, in long-running campaigns where non-stationarity changes occur infrequently, these approaches are less effective. For our algorithm, we adopt an active approach to better handle reward function shifts.