\section{Related Work}
\label{relatedwork}
Budget allocation across multiple ad campaigns ____ has been extensively studied in industrial research by companies like Criteo ____, Netflix ____, and Lyft ____. A common approach is to discretize the budget and model each sub-campaign as an arm in a multi-armed bandit problem. The optimal allocation is obtained by solving a combinatorial optimization problem ____  based on the expected reward of each arm. In previous literature, domain knowledge has been used to formulate parametric models of the arms, approximating the cost-to-reward function with a power law ____ or a sigmoid ____, followed by Thompson Sampling to handle uncertainty and induce exploration. However, these methods often overlook noise in the data, a critical factor in real-world deployments. In the presence of noise, parametric models can significantly deviate from the true reward function. A more flexible alternative is to model the reward function using Gaussian Process (GP) models ____, which allow for greater adaptability. These algorithms typically use Upper Confidence Bound (UCB) or Thompson Sampling (TS) to guide exploration. However, unlike our approach, they do not incorporate domain knowledge to promote exploration, which can lead to higher regret. Additionally , these algorithms are mostly studied for budget allocation for a single day or month ____ which does not account for changing behaviours of the reward function, a characteristic often observed in campaigns running over many months.

Handling non-stationarity in multi-armed bandits is a well-studied problem in the literature ____. Common methods include passive approaches, such as sliding windows with UCB or TS sampling ____, or using discounted rewards ____. Active methods, such as change point detection ____, offer a more dynamic approach. Passive methods either discard older data points or assign them less weight. However, in long-running campaigns where non-stationarity changes occur infrequently, these approaches are less effective. For our algorithm, we adopt an active approach to better handle reward function shifts.