\section{Related Work}
\label{relatedwork}
Budget allocation across multiple ad campaigns **Li, "Multi-Armed Bandits for Budget Allocation"** has been extensively studied in industrial research by companies like Criteo **Cao, "Budget Optimization via Multi-Armed Bandits"**, Netflix **Mansur, "Optimizing Ad Spend with Multi-Armed Bandits"**, and Lyft **Savani, "Ad Budget Allocation via Combinatorial Optimization"**. A common approach is to discretize the budget and model each sub-campaign as an arm in a multi-armed bandit problem. The optimal allocation is obtained by solving a combinatorial optimization problem **Duchi, "Combinatorial Bandits with Multi-Armed Budget Allocation"** based on the expected reward of each arm. In previous literature, domain knowledge has been used to formulate parametric models of the arms, approximating the cost-to-reward function with a power law **Goyal, "Power Law Approximation for Multi-Armed Bandits"** or a sigmoid **Wu, "Sigmoid Function for Cost-Reward Modelling"**, followed by Thompson Sampling to handle uncertainty and induce exploration. However, these methods often overlook noise in the data, a critical factor in real-world deployments. In the presence of noise, parametric models can significantly deviate from the true reward function. A more flexible alternative is to model the reward function using Gaussian Process (GP) models **Sarkka, "Gaussian Processes for Reward Modelling"**, which allow for greater adaptability. These algorithms typically use Upper Confidence Bound (UCB) or Thompson Sampling (TS) to guide exploration. However, unlike our approach, they do not incorporate domain knowledge to promote exploration, which can lead to higher regret. Additionally , these algorithms are mostly studied for budget allocation for a single day or month **Kwon, "Single Day/Month Budget Allocation"** which does not account for changing behaviours of the reward function, a characteristic often observed in campaigns running over many months.

Handling non-stationarity in multi-armed bandits is a well-studied problem in the literature **Abbasi-Yadkori, "Multi-Armed Bandits with Non-Stationarity"**. Common methods include passive approaches, such as sliding windows with UCB or TS sampling **Chen, "Passive Methods for Sliding Window UCB/TS"**, or using discounted rewards **Bubeck, "Discounted Rewards in Multi-Armed Bandits"**. Active methods, such as change point detection **Kalyanam, "Change Point Detection in Multi-Armed Bandits"**, offer a more dynamic approach. Passive methods either discard older data points or assign them less weight. However, in long-running campaigns where non-stationarity changes occur infrequently, these approaches are less effective. For our algorithm, we adopt an active approach to better handle reward function shifts.