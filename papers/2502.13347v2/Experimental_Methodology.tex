\section{Experimental Methodology}
\label{sec:experimental_method}

In this section, we introduce our experimental setup, with details on the crawler implementation and LLM training provided in Appendix~\ref{sec:appendix:crawl} and~\ref{sec:appendix:llm}.


\paragraph{\ours{}.}
To run experiments in our limited computational budget, we run a simulation of \ours{} on the ClueWeb22 dataset~\citep{clueweb22}, a snapshot of the web with graph information from a commercial crawler.
We use the English subset of ClueWeb22-A, which is a web graph containing 900M webpages with links.
We randomly sampled 10K URLs as our seed URLs.
We set the number of total crawled documents $N$ to 20M and crawled documents each iteration $n$ to 10K.
% , which is close to the default parameter of the industrial-level crawler Nutch.
We use the DCLM fastText classifier~\citep{dclm} as the pretraining influence scorer $\mathcal{M}(\cdot)$.



\paragraph{Baselines.} 
% \cx{too long a paragraph}
We emulate traditional graph-connectivity-based crawlers by replacing the LLM-oriented URL scoring function (Eq.~\ref{eq:score}) with a function that returns the indegree for a given URL, since a node's indegree closely correlates with PageRank, a common graph connectivity metric, as shown in Figure~\ref{fig:joint_distribution:indegree_pr} and previous findings~\citep{pr-and-indegree}.
We also introduce a random crawling baseline, where the scorer assigns random scores.
We run both of them in a \textit{crawl-then-select} setting, first crawling 1× or 2× more documents and then selecting the top 1× (20M) documents based on scores assigned by the DCLM fastText classifier.
This process mimics existing data-filtering pipelines, which begin with crawled documents and then apply filtering~\citep{dclm,fineweb}.

\paragraph{Oracle.}
We also introduce an oracle selection run in which we directly apply the DCLM fastText classifier to the entire ClueWeb22-A document set and select the top 10\% documents for pretraining, serving as the upper bound.
% See Appendix~\ref{sec:appendix:crawl} for details.

\paragraph{LLM Training and Evaluation.} 
For all runs, we use the final set of 20M crawled or selected documents to pretrain a 411M Transformer on 4× Chinchilla-optimal tokens~\citep{chinchilla}, totaling 32.9B tokens. 
The pretraining is conducted using the DCLM codebase~\citep{dclm}. 
To evaluate the pretrained LLMs, we follow the DCLM evaluation recipe, assessing performance on 23 (22 unique) \textit{core} tasks. 
% See Appendix~\ref{sec:appendix:llm} for details.

