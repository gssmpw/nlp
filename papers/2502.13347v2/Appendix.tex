\appendix

\section{Details on Crawling}
\label{sec:appendix:crawl}

Our implementation of the indegree-based crawler employs a static URL scoring function, which directly returns the indegree of a given URL based on the full ClueWeb22 graph (Sec.~\ref{sec:experimental_method}).  
For real-world crawlers, as the true indegree value of a URL cannot be known in advance, a local graph must be maintained to track the \textit{known} inlinks of discovered URLs.  
This local graph is updated iteratively as the discovered portion of the web expands during the crawling process~\citep{pr-crawl}.  

Maintaining such a local graph during crawling introduces significant computational overhead.  
For simplicity, we instead implement the static simulation, where we directly return the global indegree of each URL.  
We believe that this simplified implementation does not underperform compared to real-world implementations, as our approach leverages global information from the entire graph, which should be better than the partial information from the local graph.

We run our simulated crawlers on a Linux server equipped with two Intel(R) Xeon(R) E5-2630 v3 CPUs (8 cores per socket, 16 cores in total, 1 thread per core), 125GiB of memory, and an SSD.  
A crawl of 20 million documents takes approximately one day to complete.

\section{Details on LLM Training and Evaluation}
\label{sec:appendix:llm}

\input{Tables/hyper-parameters}

We pretrain a 411M-parameter\footnote{Sometimes referred to as 400M in the DCLM paper~\citep{dclm}.} decoder-only Transformer model using the DCLM training recipe~\citep{dclm}\footnote{\url{https://github.com/mlfoundations/dclm}}.
The hyper-parameters are presented in Tabel~\ref{tab:hyperparameters}.
To enhance training stability, we extend the original 411M-1x setting to 411M-4x, meaning the model is trained on 4 times the Chinchilla-optimal number of tokens~\citep{chinchilla}, which amounts to 32.9B tokens. 
The training process takes 1 day and 12 hours on 8 NVIDIA L40S GPUs.
For further details, please refer to the DCLM paper~\citep{dclm}.
Due to computational constraints, each pretraining experiment is conducted only once.

We use the DCLM evaluation recipe~\citep{dclm} to evaluate model performance on 23 (22 unique) \textit{core} tasks. 

\section{Detailed Results}
\label{sec:appendix:result}

\begin{table*}
    \centering
    % \resizebox{1.0\textwidth}{!}{
    \begin{tabular}{rlcccc}
        \toprule
        \textbf{Crawling} & \textbf{Selection} & \multicolumn{4}{c}{\textbf{Commonsense Reasoning}} \\
        \textbf{Method} & \textbf{Pool Size} & CommonsenseQA & COPA & OpenBookQA & PIQA \\
        \midrule
        \multicolumn{6}{l}{\textbf{Oracle Selection (Upper Bound)}}\\
        n.a. & 45× & 0.2850 & 0.7000 & 0.3300 & 0.6812 \\
        \multicolumn{6}{l}{\textbf{Crawl-then-Select}}\\ 
        Random & 1× & 0.2072 & 0.6700 & 0.2980 & 0.6746 \\
        Random & 2× & 0.2588 & 0.6200 & 0.3160 & 0.6785 \\
        Random & 4× & 0.2326 & 0.6400 & 0.3380 & 0.6757 \\
        Indegree & 1× & 0.3219 & 0.6000 & 0.2780 & 0.6513 \\
        Indegree & 2× & 0.1966 & 0.6600 & 0.3040 & 0.6752 \\
        Indegree & 4× & 0.2088 & 0.6400 & 0.3400 & 0.6817 \\
        \multicolumn{6}{l}{\textbf{Ours}} \\
        \ours{} & 1× & 0.2277 & 0.6600 & 0.3300 & 0.6926 \\
        \bottomrule
    \end{tabular}
    % }
    \caption{Results for commonsense reasoning tasks.}
    \label{tab:commonsense_results}
\end{table*}

\begin{table*}
    \centering
    \resizebox{1.0\textwidth}{!}{
    \begin{tabular}{rlcccccc}
        \toprule
        \textbf{Crawling} & \textbf{Selection} & \multicolumn{6}{c}{\textbf{Language Understanding}} \\
        \textbf{Method} & \textbf{Pool Size} & BIG-Bench Lang. Id. & HellaSwag (zero-shot) & HellaSwag  & LAMBADA & Winograd & Winogrande \\
        \midrule
        \multicolumn{8}{l}{\textbf{Oracle Selection (Upper Bound)}}\\
        n.a. & 45× & 0.2515 & 0.3856 & 0.3905 & 0.4432 & 0.6557 & 0.5130 \\
        \multicolumn{8}{l}{\textbf{Crawl-then-Select}}\\ 
        Random & 1× & 0.2490 & 0.3709 & 0.3716 & 0.3990 & 0.6044 & 0.5146 \\
        Random & 2× & 0.2468 & 0.3882 & 0.3925 & 0.4073 & 0.6007 & 0.5130 \\
        Random & 4× & 0.2521 & 0.4011 & 0.4019 & 0.4390 & 0.6154 & 0.5130 \\
        Indegree & 1× & 0.2566 & 0.3515 & 0.3519 & 0.3596 & 0.5971 & 0.5004 \\
        Indegree & 2× & 0.2547 & 0.3749 & 0.3771 & 0.3773 & 0.5861 & 0.5241 \\
        Indegree & 4× & 0.2562 & 0.3994 & 0.4008 & 0.4159 & 0.6190 & 0.5178 \\
        \multicolumn{8}{l}{\textbf{Ours}} \\
        \ours{} & 1× & 0.2544 & 0.4035 & 0.4048 & 0.4196 & 0.6593 & 0.5288 \\
        \bottomrule
    \end{tabular}
    }
    \caption{Results for language understanding tasks.}
    \label{tab:language_results}
\end{table*}

\begin{table*}
    \centering
    % \resizebox{1.0\textwidth}{!}{
    \begin{tabular}{rlccc}
        \toprule
        \textbf{Crawling} & \textbf{Selection} & \multicolumn{3}{c}{\textbf{Reading Comprehension}} \\
        \textbf{Method} & \textbf{Pool Size} & BoolQ & CoQA  & SQuAD \\
        \midrule
        \multicolumn{5}{l}{\textbf{Oracle Selection (Upper Bound)}}\\
        n.a. & 45× & 0.5755 & 0.2479 & 0.3139 \\
        \multicolumn{5}{l}{\textbf{Crawl-then-Select}}\\ 
        Random & 1× & 0.5080 & 0.1799 & 0.1882 \\
        Random & 2× & 0.5807 & 0.2053 & 0.2759 \\
        Random & 4× & 0.5911 & 0.2361 & 0.2951 \\
        Indegree & 1× & 0.5324 & 0.1666 & 0.1616 \\
        Indegree & 2× & 0.5697 & 0.1843 & 0.2390 \\
        Indegree & 4× & 0.5765 & 0.2147 & 0.2736 \\
        \multicolumn{5}{l}{\textbf{Ours}} \\
        \ours{} & 1× & 0.5440 & 0.2264 & 0.2215 \\
        \bottomrule
    \end{tabular}
    % }
    \caption{Results for reading comprehension tasks.}
    \label{tab:reading_results}
\end{table*}

\begin{table*}
    \centering
    \resizebox{1.0\textwidth}{!}{
    \begin{tabular}{rlccccc}
        \toprule
        \textbf{Crawling} & \textbf{Selection} & \multicolumn{5}{c}{\textbf{Symbolic Problem Solving}} \\
        \textbf{Method} & \textbf{Pool Size} & AGI Eval LSAT-AR & BIG-Bench CS Algorithms  & BIG-Bench Dyck Lang.  & BIG-Bench Operators  & BIG-Bench Repeat Copy Logic \\
        \midrule
        \multicolumn{7}{l}{\textbf{Oracle Selection (Upper Bound)}}\\
        n.a. & 45× & 0.2739 & 0.4341 & 0.2160 & 0.2143 & 0.0625 \\
        \multicolumn{7}{l}{\textbf{Crawl-then-Select}}\\ 
        Random & 1× & 0.2391 & 0.4568 & 0.1970 & 0.2143 & 0.0000 \\
        Random & 2× & 0.2696 & 0.4538 & 0.2520 & 0.1762 & 0.0313 \\
        Random & 4× & 0.1957 & 0.4568 & 0.2600 & 0.1857 & 0.0625 \\
        Indegree & 1× & 0.2304 & 0.4371 & 0.1900 & 0.1429 & 0.0000 \\
        Indegree & 2× & 0.2609 & 0.4235 & 0.2340 & 0.2143 & 0.0313 \\
        Indegree & 4× & 0.2174 & 0.4538 & 0.2530 & 0.1667 & 0.0938 \\
        \multicolumn{7}{l}{\textbf{Ours}} \\
        \ours{} & 1× & 0.2696 & 0.4371 & 0.1620 & 0.2095 & 0.0938 \\
        \bottomrule
    \end{tabular}
    }
    \caption{Results for symbolic problem solving tasks.}
    \label{tab:symbolic_results}
\end{table*}

\begin{table*}
    \centering
    \resizebox{1.0\textwidth}{!}{
    \begin{tabular}{rlccccc}
        \toprule
        \textbf{Crawling} & \textbf{Selection} & \multicolumn{5}{c}{\textbf{World Knowledge}} \\
        \textbf{Method} & \textbf{Pool Size} & ARC Easy  & ARC Challenge  & BIG-Bench QA Wikidata  & Jeopardy  & MMLU \\
        \midrule
        \multicolumn{7}{l}{\textbf{Oracle Selection (Upper Bound)}}\\
        n.a. & 45× & 0.5951 & 0.3166 & 0.4945 & 0.1176 & 0.2805 \\
        \multicolumn{7}{l}{\textbf{Crawl-then-Select}}\\ 
        Random & 1× & 0.5152 & 0.2799 & 0.5186 & 0.0461 & 0.2552 \\
        Random & 2× & 0.5425 & 0.2807 & 0.5081 & 0.0648 & 0.2561 \\
        Random & 4× & 0.5577 & 0.2867 & 0.5126 & 0.0970 & 0.2543 \\
        Indegree & 1× & 0.4857 & 0.2509 & 0.4888 & 0.0138 & 0.2618 \\
        Indegree & 2× & 0.5248 & 0.2790 & 0.5205 & 0.0555 & 0.2464 \\
        Indegree & 4× & 0.5749 & 0.2935 & 0.5084 & 0.0959 & 0.2430 \\
        \multicolumn{7}{l}{\textbf{Ours}} \\
        \ours{} & 1× & 0.6103 & 0.3208 & 0.5143 & 0.1323 & 0.2661 \\
        \bottomrule
    \end{tabular}
    }
    \caption{Results for world knowledge tasks.}
    \label{tab:world_knowledge_results}
\end{table*}

The raw (uncentered) accuracy of all evaluation tasks is presented in Table~\ref{tab:commonsense_results}, \ref{tab:language_results}, \ref{tab:reading_results},
\ref{tab:symbolic_results}, and 
\ref{tab:world_knowledge_results}.
Please refer to~\citet{dclm} for more details on the evaluation tasks.



% Table~\ref{tab:all_results} presents the detailed results for all evaluation tasks.

\section{The ClueWeb22 Dataset}

ClueWeb22~\citep{clueweb22} is distributed under a ``TREC-style'' license for research purpose.  
The dataset can be obtained by signing a data license agreement with Carnegie Mellon University\footnote{\url{https://lemurproject.org/clueweb22/obtain.php}}.
We use ClueWeb22 only for research purpose.

\section{Use of AI Assistants}

We use GitHub Copilot\footnote{\url{https://github.com/features/copilot}} to assist with coding and ChatGPT\footnote{\url{https://chatgpt.com/}} (powered by GPT-4o) to enhance the writing of this paper.
