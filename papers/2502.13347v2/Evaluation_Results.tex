

\section{Evaluation Results}



In this section, we first present the overall performance of \ours{} (Sec.~\ref{sec:result:overall}), followed by further analysis (Sec.~\ref{sec:result:analysis}).

\subsection{Overall Performance}
\label{sec:result:overall}

In this experiment, we compare the performance of \ours{} with baseline crawlers by evaluating LLMs trained on their respective crawled data.
As shown in Table~\ref{tab:overall}, when all methods crawl the same amount of training data (1×), \ours{} significantly outperforms random crawling and indegree crawling.
In the crawl-then-select setting, where traditional crawlers are allowed to collect twice as much data (2×) for later selection, they still underperform compared to \ours{}.
This suggests that incorporating pretraining-oriented signals early in the crawling process is more beneficial than relying on post-selection.
With only 1× of the data, \ours{} retains 95\% of the performance achieved by the oracle run, which directly selects from a substantially larger 45× data pool. 

In Section~\ref{sec:result:analysis}, we further analyze the efficiency of \ours{} compared to traditional crawlers and explore the reasons behind it.

\subsection{Analysis}
\label{sec:result:analysis}

\input{Figures/crawl_efficiency}

\paragraph{Crawling Efficiency.}
We evaluate the efficiency of \ours{} by comparing the number of documents it crawls or visits against baseline crawlers. 
As shown in Figure~\ref{fig:crawl_efficiency:pool}, even when the baselines crawl 4× the required pretraining data for selection, they still underperform compared to \ours{}. 
Extrapolation suggests that the indegree-based crawler would need to process 4.8× more documents (96M) to match \ours{}’s performance. 
Figure~\ref{fig:crawl_efficiency:visited_document} further illustrates that \ours{} achieves the same performance while crawling only 21\% of the documents required by the indegree-based crawler, or 48\% when considering all visited documents.
These results highlight the efficiency of \ours{}, demonstrating its potential to reduce website burdens and mitigate over-crawling.


\paragraph{Document Coverage.}

\input{Figures/prec_and_recall}

In this experiment, we plot the precision and recall of the oracle-selected documents among those crawled by \ours{} and baseline crawlers throughout the crawling process.
As shown in Figure~\ref{fig:overlap}, the precision quickly reaches 1.0, while the recall increases linearly, aligning with the theoretical upper bound.
The saturated performance remains until 13 million documents have been crawled, after which the performance starts to decline, likely due to the lack of connectivity of the ClueWeb22 subgraph.
% In contrast, baseline crawls have very little overlaps with the oracle selection, verifying that most data crawled by traditional crawlers should be discarded~\citep{dclm,fineweb}.
In contrast, baseline crawlers exhibit minimal overlap with oracle-selected data, verifying that most of their crawled content is misaligned with pretraining needs and should be filtered~\citep{dclm,fineweb}. 
These results emphasize the importance of targeted crawling strategies for pretraining.

\paragraph{Score Correlations Across Links.}

\input{Figures/hops_correlation}

\ours{} tracks the outlinks of the highest-scored documents in the current iteration to enrich the queue for future crawls.
As shown in Figure~\ref{fig:correlation}, we plot the correlations between the pretraining influence scores of current documents and their 1- and 2-hop outlinks.
The results indicate a correlation in influence scores across link hops, suggesting that highly-rated documents are interconnected and can be discovered through previously crawled documents.



