\section*{Limitations}

Web crawling raises important concerns regarding copyright and the fair use of web data~\citep{longpre2024consent}, necessitating a better solution from the entire LLM community, such as sharing benefits with website owners.
In this paper, we propose a more efficient crawling method that mitigates these challenges by reducing crawling, though it does not fully resolve them. 
Our experiments are conducted on a web graph dataset ClueWeb22~\citep{clueweb22}, thereby avoiding issues associated with actual web crawling. 
We hope that future advancements in web crawling will better align with ethical and legal standards.

While our crawling simulation is a sufficient research setup, further validation is required to assess the effectiveness of \ours{} in real-world crawling scenarios.  
Our \ours{} and baseline crawlers implement only the selection policy~\citep{pr-crawl} of a crawler, which determines which pages to crawl.  
Although we try to mimic real-world crawling procedures used in systems like Apache Nutch\footnote{\url{https://nutch.apache.org/}}, we do not implement other web crawling policies in industrial-level crawlers, such as the re-visit policy~\citep{revisit}, politeness policy~\citep{politeness}, and parallelization policy~\citep{parallelization}.  
We leave the integration of \ours{} into real-world crawling engines like Nutch and a comprehensive comparison between \ours{} and traditional crawling methods in real-world crawling scenarios for future work.
