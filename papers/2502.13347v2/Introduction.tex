\section{Introduction}


Massive in size and diverse in topics, web data usually serve as the primary source of pretraining data for large language models (LLMs), providing an extensive and heterogeneous corpus that captures a wide spectrum of human knowledge and real-world information~\citep{cc-analysis, dubey2024llama, fineweb}.
Pretraining datasets are typically built from large-scale web crawls such as Common Crawl~\citep{CommonCrawl2007}, which may contain TBs of data spanning billions of webpages~\citep{fineweb,redpajamav2}.

\input{Figures/graph_traverse}


Despite their vast scale, most of the data collected from web crawls are not used in the pretraining of LLMs.
Existing work often discards over 90\% of the raw data collected from the web~\citep{dclm,fineweb,txt360}, highlighting the \textit{inefficiency} of current web crawlers in collecting LLM pretraining data.
Common web crawlers like Common Crawl prioritize pages based on graph connectivity metrics like PageRank~\citep{pagerank,pr-crawl} or harmonic centrality~\citep{harmonic,cc-analysis}, which favor documents with a high number of inlinks (indegree)~\citep{pr-and-indegree} rather than those most relevant for pretraining.
This misalignment not only leads to waste in computational resources during excessive data processing for LLM developers, but also incentivizes over-crawling, which burdens website operators with redundant traffic and increases ethical and legal risks related to fair use of data and copyright~\citep{longpre2024consent,nyt_vs_openai}.

To bridge this gap, we propose Web \textbf{Craw}ling for \textbf{LL}\textbf{M} Pretraining (\ours{}).
Instead of relying on traditional graph-connectivity-based signals, \ours{} improves crawling efficiency by prioritizing webpages based on their influence on LLM pretraining.
Specifically, during each crawling iteration, all newly discovered documents are scored with a pretraining influence scorer derived from data-filtering pipelines for pretraining~\citep{dclm,fineweb}, and documents with the highest scores are used to discover new documents.
% \ours{} scores all extracted outlinks using ranking functions derived from data-filtering pipelines for LLM pretraining.
% These scores determine the ordering of URLs in a priority queue, which automatically selects and schedules the highest-scoring URLs for the next round of crawling.
By prioritizing webpages with high influence scores, as illustrated in Figure~\ref{fig:graph_traverse}, \ours{} explores the web graph in a fundamentally different manner from traditional graph-connectivity-based crawlers, uncovering a distinct subset of the web more useful for pretraining.

We conduct large-scale crawling simulations on ClueWeb22-A~\citep{clueweb22}, a snapshot of the web containing 900 million English webpages obtained from the central index of a commercial search engine.
Results show that, by crawling only 1× of the pretraining dataset size, \ours{} can outperform traditional crawlers which collect 1×, 2×, and 4× more data followed by data selection.
Compared to the baseline crawler that achieves the same performance, \ours{} crawls only 21\% of the webpages. 
Further analysis reveals that during crawling, \ours{} quickly discovers documents that align with the oracle selection, which crawls the full web graph. As a result, it achieves 95\% of the oracle performance while crawling only 2.2\% of the data.
