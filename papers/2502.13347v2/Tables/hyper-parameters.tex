\begin{table}
    \centering
    \begin{tabular}{rl}
    \toprule
        \textbf{Hyper-parameter} & \textbf{Value}\\
    \midrule
         $n_{\text{layers}}$ & 24 \\
         $n_{\text{heads}}$& 8\\
         $d_{\text{model}}$& 1,024\\
         $d_{\text{head}}$& 128\\
         Warmup& 2,000\\
         Learning Rate& 3e-3\\
         Weight Decay& 0.033\\
         z-loss& 1e-4\\
         Global Batch Size & 512 \\
         Sequence Length & 2048 \\
    \bottomrule
    \end{tabular}
    \caption{Model and training hyper-parameters.
    $n_{\text{layers}}$, $n_{\text{layers}}$, $d_{\text{model}}$, and $d_{\text{head}}$ denote the number of layers, attention heads, width, and width per attention head, respectively.
    }
    \label{tab:hyperparameters}
\end{table}