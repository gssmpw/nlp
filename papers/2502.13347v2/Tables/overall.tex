\begin{table*}[t]
  \centering
  \resizebox{1.0\textwidth}{!}{
    \begin{tabular}{rlccccccc}
    \toprule 
    & & \makecell{\textbf{Commonsense}\\\textbf{Reasoning}} & \makecell{\textbf{Language}\\\textbf{Understanding}} & \makecell{\textbf{Reading}\\\textbf{Comprehension}} & \makecell{\textbf{Symbolic}\\\textbf{Problem Solving}} & \makecell{\textbf{World}\\\textbf{Knowledge}} & \makecell{\textbf{Core}} & \makecell{\\\textbf{\% of}}  \\

    \textbf{Crawling Method} & \textbf{Selection Pool Size} & \textit{(4 tasks)} & \textit{(6 tasks)} & \textit{(3 tasks)} & \textit{(5 tasks)} & \textit{(5 tasks)} & \textit{(23 tasks)} & \textbf{Oracle}\\
    
    \midrule
    
    \multicolumn{8}{l}{\textbf{Oracle Selection (Upper Bound):} Random sample from the top 10\% rated data from ClueWeb22 using DCLM fastText for pretraining}\\
    
    n.a. & 45× & 0.2438 & 0.2209 & 0.1483 & 0.2039 & 0.2403 & 0.2239 & 100\% \\
    
    \midrule
    
    \multicolumn{8}{l}{\textbf{Crawl-then-Select:} Crawl 1× and 2× more data from ClueWeb22 and select top-rated 1× data using DCLM fastText for pretraining}\\ 
    
    \multirow{2}{*}{Random} & 1× & \underline{0.1906} & 0.1890 & 0.0244 & 0.1834 & 0.1930 & 0.1748 & 78.1\% \\
    
    & 2× & 0.1896 & \underline{0.1967} & \textbf{0.1260} & \textbf{0.2000} & \underline{0.2024} & \underline{0.1964} & \underline{87.7\%}\\
    
    \multirow{2}{*}{Indegree} & 1× & 0.1730 & 0.1680 & 0.0326 & 0.1616 & 0.1668 & 0.1556 & 69.5\%\\
    
    & 2× & 0.1845 & 0.1856 & \underline{0.0970} & 0.1958 & 0.1953 & 0.1865 & 83.3\%\\
    
    \midrule
    
    \multicolumn{8}{l}{\textbf{Ours:} Crawl 1× data using \ours{} for pretraining} \\
    
    \rowcolor{blue! 12} 
    \ours{} & 1× & \textbf{0.2116} & \textbf{0.2311} & 0.0826 & \underline{0.1979} & \textbf{0.2486} & \textbf{0.2133} & \textbf{95.3\%}\\
    \bottomrule
  \end{tabular}
  }
  \caption{\label{tab:overall}
  Downstream LLM performance. 
  All models are pretrained on 1× data, which corresponds to 20M documents and 32.9B tokens.
  The evaluation metric is centered accuracy (0 = random guess)~\citep{dclm}.
  Best/2nd best in the last two groups are bolded/underlined.
  See Appendix~\ref{sec:appendix:result} for detailed results.
  }
\end{table*}