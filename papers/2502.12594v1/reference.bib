% Sentence Bert
@inproceedings{nils2019sbert,
  author       = {Nils Reimers and
                  Iryna Gurevych},
  editor       = {Kentaro Inui and
                  Jing Jiang and
                  Vincent Ng and
                  Xiaojun Wan},
  title        = {Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
  booktitle    = {Proceedings of the 2019 Conference on Empirical Methods in Natural
                  Language Processing and the 9th International Joint Conference on
                  Natural Language Processing, {EMNLP-IJCNLP} 2019, Hong Kong, China,
                  November 3-7, 2019},
  pages        = {3980--3990},
  publisher    = {Association for Computational Linguistics},
  year         = {2019},
  url          = {https://doi.org/10.18653/v1/D19-1410},
  doi          = {10.18653/V1/D19-1410},
  timestamp    = {Thu, 07 Apr 2022 09:14:07 +0200},
  biburl       = {https://dblp.org/rec/conf/emnlp/ReimersG19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

% Unstructured Pruning
@article{perp2024max,
  author       = {Max Zimmer and
                  Megi Andoni and
                  Christoph Spiegel and
                  Sebastian Pokutta},
  title        = {{PERP:} Rethinking the Prune-Retrain Paradigm in the Era of LLMs},
  journal      = {CoRR},
  volume       = {abs/2312.15230},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2312.15230},
  doi          = {10.48550/ARXIV.2312.15230},
  eprinttype    = {arXiv},
  eprint       = {2312.15230},
  timestamp    = {Thu, 18 Jan 2024 14:06:11 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2312-15230.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{guangji2024,
  author       = {Guangji Bai and
                  Yijiang Li and
                  Chen Ling and
                  Kibaek Kim and
                  Liang Zhao},
  title        = {Gradient-Free Adaptive Global Pruning for Pre-trained Language Models},
  journal      = {CoRR},
  volume       = {abs/2402.17946},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2402.17946},
  doi          = {10.48550/ARXIV.2402.17946},
  eprinttype    = {arXiv},
  eprint       = {2402.17946},
  timestamp    = {Wed, 17 Jul 2024 17:18:20 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2402-17946.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{besa2024peng,
  author       = {Peng Xu and
                  Wenqi Shao and
                  Mengzhao Chen and
                  Shitao Tang and
                  Kaipeng Zhang and
                  Peng Gao and
                  Fengwei An and
                  Yu Qiao and
                  Ping Luo},
  title        = {{BESA:} Pruning Large Language Models with Blockwise Parameter-Efficient
                  Sparsity Allocation},
  booktitle    = {The Twelfth International Conference on Learning Representations,
                  {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=gC6JTEU3jl},
  timestamp    = {Wed, 07 Aug 2024 17:11:53 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/XuSCTZ0A0024.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

%% also for semi-unstructured pruning and pruning recovery
@inproceedings{sparsegpt2023elias,
  author       = {Elias Frantar and
                  Dan Alistarh},
  editor       = {Andreas Krause and
                  Emma Brunskill and
                  Kyunghyun Cho and
                  Barbara Engelhardt and
                  Sivan Sabato and
                  Jonathan Scarlett},
  title        = {SparseGPT: Massive Language Models Can be Accurately Pruned in One-Shot},
  booktitle    = {International Conference on Machine Learning, {ICML} 2023, 23-29 July
                  2023, Honolulu, Hawaii, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {10323--10337},
  publisher    = {{PMLR}},
  year         = {2023},
  url          = {https://proceedings.mlr.press/v202/frantar23a.html},
  timestamp    = {Mon, 28 Aug 2023 17:23:08 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/FrantarA23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
%% also for semi-unstructured pruning
@inproceedings{wanda2024mingjie,
  author       = {Mingjie Sun and
                  Zhuang Liu and
                  Anna Bair and
                  J. Zico Kolter},
  title        = {A Simple and Effective Pruning Approach for Large Language Models},
  booktitle    = {The Twelfth International Conference on Learning Representations,
                  {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=PxoFut3dWW},
  timestamp    = {Wed, 07 Aug 2024 17:11:53 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/Sun0BK24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
%% also for semi-unstructured pruning and structured pruning
@inproceedings{owl2024lu,
  author       = {Lu Yin and
                  You Wu and
                  Zhenyu Zhang and
                  Cheng{-}Yu Hsieh and
                  Yaqing Wang and
                  Yiling Jia and
                  Gen Li and
                  Ajay Kumar Jaiswal and
                  Mykola Pechenizkiy and
                  Yi Liang and
                  Michael Bendersky and
                  Zhangyang Wang and
                  Shiwei Liu},
  title        = {Outlier Weighed Layerwise Sparsity {(OWL):} {A} Missing Secret Sauce
                  for Pruning LLMs to High Sparsity},
  booktitle    = {Forty-first International Conference on Machine Learning, {ICML} 2024,
                  Vienna, Austria, July 21-27, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=ahEm3l2P6w},
  timestamp    = {Mon, 02 Sep 2024 16:55:26 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/0006W0HWJLJPLBW24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
%% also for semi-structured pruning
@inproceedings{copal2024srikanth,
  author       = {Srikanth Malla and
                  Joon Hee Choi and
                  Chiho Choi},
  title        = {{COPAL:} Continual Pruning in Large Language Generative Models},
  booktitle    = {Forty-first International Conference on Machine Learning, {ICML} 2024,
                  Vienna, Austria, July 21-27, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=Lt8Lk7IQ5b},
  timestamp    = {Mon, 02 Sep 2024 16:55:26 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/MallaCC24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{flash2023haojun,
  author       = {Haojun Xia and
                  Zhen Zheng and
                  Yuchao Li and
                  Donglin Zhuang and
                  Zhongzhu Zhou and
                  Xiafei Qiu and
                  Yong Li and
                  Wei Lin and
                  Shuaiwen Leon Song},
  title        = {Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative
                  Model Inference with Unstructured Sparsity},
  journal      = {CoRR},
  volume       = {abs/2309.10285},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2309.10285},
  doi          = {10.48550/ARXIV.2309.10285},
  eprinttype    = {arXiv},
  eprint       = {2309.10285},
  timestamp    = {Wed, 17 Jul 2024 16:21:24 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2309-10285.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{oneshot2024hang,
  author       = {Hang Shao and
                  Bei Liu and
                  Yanmin Qian},
  title        = {One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language
                  Models},
  booktitle    = {{IEEE} International Conference on Acoustics, Speech and Signal Processing,
                  {ICASSP} 2024, Seoul, Republic of Korea, April 14-19, 2024},
  pages        = {11296--11300},
  publisher    = {{IEEE}},
  year         = {2024},
  url          = {https://doi.org/10.1109/ICASSP48485.2024.10445737},
  doi          = {10.1109/ICASSP48485.2024.10445737},
  timestamp    = {Wed, 07 Aug 2024 12:26:13 +0200},
  biburl       = {https://dblp.org/rec/conf/icassp/ShaoLQ24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

% Semi-structured Pruning
@article{zhi2024dass,
  author       = {Zhiyu Guo and
                  Hidetaka Kamigaito and
                  Taro Wanatnabe},
  title        = {Dependency-Aware Semi-Structured Sparsity of {GLU} Variants in Large
                  Language Models},
  journal      = {CoRR},
  volume       = {abs/2405.01943},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2405.01943},
  doi          = {10.48550/ARXIV.2405.01943},
  eprinttype    = {arXiv},
  eprint       = {2405.01943},
  timestamp    = {Sun, 09 Jun 2024 21:32:26 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2405-01943.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

%% also for unstructured pruning
@inproceedings{ria2024yingtao,
  author       = {Yingtao Zhang and
                  Haoli Bai and
                  Haokun Lin and
                  Jialin Zhao and
                  Lu Hou and
                  Carlo Vittorio Cannistraci},
  title        = {Plug-and-Play: An Efficient Post-training Pruning Method for Large
                  Language Models},
  booktitle    = {The Twelfth International Conference on Learning Representations,
                  {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=Tr0lPx9woF},
  timestamp    = {Wed, 07 Aug 2024 17:11:53 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/ZhangBL0HC24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

%% also for unstructured pruning
@inproceedings{prunerzero2024peijie,
  author       = {Peijie Dong and
                  Lujun Li and
                  Zhenheng Tang and
                  Xiang Liu and
                  Xinglin Pan and
                  Qiang Wang and
                  Xiaowen Chu},
  title        = {Pruner-Zero: Evolving Symbolic Pruning Metric From Scratch for Large
                  Language Models},
  booktitle    = {Forty-first International Conference on Machine Learning, {ICML} 2024,
                  Vienna, Austria, July 21-27, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=1tRLxQzdep},
  timestamp    = {Mon, 02 Sep 2024 16:55:26 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/DongLTLP0024.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

% Structured Pruning
@article{ma2023llm,
  title={Llm-pruner: On the structural pruning of large language models},
  author={Ma, Xinyin and Fang, Gongfan and Wang, Xinchao},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={21702--21720},
  year={2023}
}

@article{song2023compresso,
  author       = {Song Guo and
                  Jiahang Xu and
                  Li Lyna Zhang and
                  Mao Yang},
  title        = {Compresso: Structured Pruning with Collaborative Prompting Learns
                  Compact Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2310.05015},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2310.05015},
  doi          = {10.48550/ARXIV.2310.05015},
  eprinttype    = {arXiv},
  eprint       = {2310.05015},
  timestamp    = {Fri, 20 Oct 2023 12:04:38 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2310-05015.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{kyeong2024shortened,
  author       = {Bo{-}Kyeong Kim and
                  Geon{-}min Kim and
                  Tae{-}Ho Kim and
                  Thibault Castells and
                  Shinkook Choi and
                  Junho Shin and
                  Hyoung{-}Kyu Song},
  title        = {Shortened LLaMA: {A} Simple Depth Pruning for Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2402.02834},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2402.02834},
  doi          = {10.48550/ARXIV.2402.02834},
  eprinttype    = {arXiv},
  eprint       = {2402.02834},
  timestamp    = {Mon, 12 Feb 2024 13:36:38 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2402-02834.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{slidegpt2024saleh,
  author       = {Saleh Ashkboos and
                  Maximilian L. Croci and
                  Marcelo Gennari Do Nascimento and
                  Torsten Hoefler and
                  James Hensman},
  title        = {SliceGPT: Compress Large Language Models by Deleting Rows and Columns},
  booktitle    = {The Twelfth International Conference on Learning Representations,
                  {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=vXxardq6db},
  timestamp    = {Wed, 07 Aug 2024 17:11:53 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/AshkboosCNHH24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{not2024siqi,
  author       = {Siqi Fan and
                  Xin Jiang and
                  Xiang Li and
                  Xuying Meng and
                  Peng Han and
                  Shuo Shang and
                  Aixin Sun and
                  Yequan Wang and
                  Zhongyuan Wang},
  title        = {Not all Layers of LLMs are Necessary during Inference},
  journal      = {CoRR},
  volume       = {abs/2403.02181},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2403.02181},
  doi          = {10.48550/ARXIV.2403.02181},
  eprinttype    = {arXiv},
  eprint       = {2403.02181},
  timestamp    = {Sun, 21 Jul 2024 18:15:36 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2403-02181.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{shortgpt2024xin,
  author       = {Xin Men and
                  Mingyu Xu and
                  Qingyu Zhang and
                  Bingning Wang and
                  Hongyu Lin and
                  Yaojie Lu and
                  Xianpei Han and
                  Weipeng Chen},
  title        = {ShortGPT: Layers in Large Language Models are More Redundant Than
                  You Expect},
  journal      = {CoRR},
  volume       = {abs/2403.03853},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2403.03853},
  doi          = {10.48550/ARXIV.2403.03853},
  eprinttype    = {arXiv},
  eprint       = {2403.03853},
  timestamp    = {Wed, 03 Apr 2024 15:23:01 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2403-03853.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{laco2024yifei,
  author       = {Yifei Yang and
                  Zouying Cao and
                  Hai Zhao},
  title        = {LaCo: Large Language Model Pruning via Layer Collapse},
  journal      = {CoRR},
  volume       = {abs/2402.11187},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2402.11187},
  doi          = {10.48550/ARXIV.2402.11187},
  eprinttype    = {arXiv},
  eprint       = {2402.11187},
  timestamp    = {Wed, 22 May 2024 12:39:09 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2402-11187.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{flap2024yongqi,
  author       = {Yongqi An and
                  Xu Zhao and
                  Tao Yu and
                  Ming Tang and
                  Jinqiao Wang},
  editor       = {Michael J. Wooldridge and
                  Jennifer G. Dy and
                  Sriraam Natarajan},
  title        = {Fluctuation-Based Adaptive Structured Pruning for Large Language Models},
  booktitle    = {Thirty-Eighth {AAAI} Conference on Artificial Intelligence, {AAAI}
                  2024, Thirty-Sixth Conference on Innovative Applications of Artificial
                  Intelligence, {IAAI} 2024, Fourteenth Symposium on Educational Advances
                  in Artificial Intelligence, {EAAI} 2014, February 20-27, 2024, Vancouver,
                  Canada},
  pages        = {10865--10873},
  publisher    = {{AAAI} Press},
  year         = {2024},
  url          = {https://doi.org/10.1609/aaai.v38i10.28960},
  doi          = {10.1609/AAAI.V38I10.28960},
  timestamp    = {Sat, 20 Jul 2024 15:04:59 +0200},
  biburl       = {https://dblp.org/rec/conf/aaai/AnZYTW24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{bonsai2024lucio,
  author       = {Lucio M. Dery and
                  Steven Kolawole and
                  Jean{-}Fran{\c{c}}ois Kagey and
                  Virginia Smith and
                  Graham Neubig and
                  Ameet Talwalkar},
  title        = {Everybody Prune Now: Structured Pruning of LLMs with only Forward
                  Passes},
  journal      = {CoRR},
  volume       = {abs/2402.05406},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2402.05406},
  doi          = {10.48550/ARXIV.2402.05406},
  eprinttype    = {arXiv},
  eprint       = {2402.05406},
  timestamp    = {Fri, 16 Feb 2024 09:40:47 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2402-05406.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{andrey2024,
  author       = {Andrey Gromov and
                  Kushal Tirumala and
                  Hassan Shapourian and
                  Paolo Glorioso and
                  Daniel A. Roberts},
  title        = {The Unreasonable Ineffectiveness of the Deeper Layers},
  journal      = {CoRR},
  volume       = {abs/2403.17887},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2403.17887},
  doi          = {10.48550/ARXIV.2403.17887},
  eprinttype    = {arXiv},
  eprint       = {2403.17887},
  timestamp    = {Wed, 10 Apr 2024 17:37:45 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2403-17887.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{sleb2024jiwon,
  author       = {Jiwon Song and
                  Kyungseok Oh and
                  Taesu Kim and
                  Hyungjun Kim and
                  Yulhwa Kim and
                  Jae{-}Joon Kim},
  title        = {{SLEB:} Streamlining LLMs through Redundancy Verification and Elimination
                  of Transformer Blocks},
  booktitle    = {Forty-first International Conference on Machine Learning, {ICML} 2024,
                  Vienna, Austria, July 21-27, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=fuX4hyLPmO},
  timestamp    = {Mon, 02 Sep 2024 16:55:26 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/SongOKKKK24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{sheared2024mengzhou,
  author       = {Mengzhou Xia and
                  Tianyu Gao and
                  Zhiyuan Zeng and
                  Danqi Chen},
  title        = {Sheared LLaMA: Accelerating Language Model Pre-training via Structured
                  Pruning},
  booktitle    = {The Twelfth International Conference on Learning Representations,
                  {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=09iOdaeOzp},
  timestamp    = {Wed, 07 Aug 2024 17:11:53 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/XiaGZ024.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{nuteprune2024shengrui,
  author       = {Shengrui Li and
                  Xueting Han and
                  Jing Bai},
  title        = {NutePrune: Efficient Progressive Pruning with Numerous Teachers for
                  Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2402.09773},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2402.09773},
  doi          = {10.48550/ARXIV.2402.09773},
  eprinttype    = {arXiv},
  eprint       = {2402.09773},
  timestamp    = {Wed, 27 Mar 2024 09:53:00 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2402-09773.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{yupeng2023,
  author       = {Yupeng Ji and
                  Yibo Cao and
                  Jiucai Liu},
  title        = {Pruning Large Language Models via Accuracy Predictor},
  journal      = {CoRR},
  volume       = {abs/2309.09507},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2309.09507},
  doi          = {10.48550/ARXIV.2309.09507},
  eprinttype    = {arXiv},
  eprint       = {2309.09507},
  timestamp    = {Fri, 22 Sep 2023 12:57:22 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2309-09507.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
%% only for BERT, not compatible for llama
@article{aaron2024nas,
  author       = {Aaron Klein and
                  Jacek Golebiowski and
                  Xingchen Ma and
                  Valerio Perrone and
                  C{\'{e}}dric Archambeau},
  title        = {Structural Pruning of Pre-trained Language Models via Neural Architecture
                  Search},
  journal      = {CoRR},
  volume       = {abs/2405.02267},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2405.02267},
  doi          = {10.48550/ARXIV.2405.02267},
  eprinttype    = {arXiv},
  eprint       = {2405.02267},
  timestamp    = {Fri, 07 Jun 2024 15:57:24 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2405-02267.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{merge2024deyuan,
  author       = {Deyuan Liu and
                  Zhanyue Qin and
                  Hairu Wang and
                  Zhao Yang and
                  Zecheng Wang and
                  Fangying Rong and
                  Qingbin Liu and
                  Yanchao Hao and
                  Xi Chen and
                  Cunhang Fan and
                  Zhao Lv and
                  Zhiying Tu and
                  Dianhui Chu and
                  Dianbo Sui},
  title        = {Pruning via Merging: Compressing LLMs via Manifold Alignment Based
                  Layer Merging},
  journal      = {CoRR},
  volume       = {abs/2406.16330},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2406.16330},
  doi          = {10.48550/ARXIV.2406.16330},
  eprinttype    = {arXiv},
  eprint       = {2406.16330},
  timestamp    = {Tue, 16 Jul 2024 16:17:59 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2406-16330.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{nash2023jongwoo,
  author       = {Jongwoo Ko and
                  Seungjoon Park and
                  Yujin Kim and
                  Sumyeong Ahn and
                  Du{-}Seong Chang and
                  Euijai Ahn and
                  Se{-}Young Yun},
  editor       = {Houda Bouamor and
                  Juan Pino and
                  Kalika Bali},
  title        = {{NASH:} {A} Simple Unified Framework of Structured Pruning for Accelerating
                  Encoder-Decoder Language Models},
  booktitle    = {Findings of the Association for Computational Linguistics: {EMNLP}
                  2023, Singapore, December 6-10, 2023},
  pages        = {6076--6093},
  publisher    = {Association for Computational Linguistics},
  year         = {2023},
  url          = {https://doi.org/10.18653/v1/2023.findings-emnlp.404},
  doi          = {10.18653/V1/2023.FINDINGS-EMNLP.404},
  timestamp    = {Fri, 12 Apr 2024 13:11:54 +0200},
  biburl       = {https://dblp.org/rec/conf/emnlp/KoPKACAY23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{loraprune2024mingyang,
  author       = {Mingyang Zhang and
                  Hao Chen and
                  Chunhua Shen and
                  Zhen Yang and
                  Linlin Ou and
                  Xinyi Yu and
                  Bohan Zhuang},
  editor       = {Lun{-}Wei Ku and
                  Andre Martins and
                  Vivek Srikumar},
  title        = {LoRAPrune: Structured Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning},
  booktitle    = {Findings of the Association for Computational Linguistics, {ACL} 2024,
                  Bangkok, Thailand and virtual meeting, August 11-16, 2024},
  pages        = {3013--3026},
  publisher    = {Association for Computational Linguistics},
  year         = {2024},
  url          = {https://aclanthology.org/2024.findings-acl.178},
  timestamp    = {Tue, 27 Aug 2024 17:38:11 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/Zhang0SYOYZ24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{apt2024bowen,
  author       = {Bowen Zhao and
                  Hannaneh Hajishirzi and
                  Qingqing Cao},
  title        = {{APT:} Adaptive Pruning and Tuning Pretrained Language Models for
                  Efficient Training and Inference},
  booktitle    = {Forty-first International Conference on Machine Learning, {ICML} 2024,
                  Vienna, Austria, July 21-27, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=sb81Xl50JG},
  timestamp    = {Mon, 02 Sep 2024 16:55:25 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/ZhaoHC24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{dong2024prompt,
  title={Prompt-prompted adaptive structured pruning for efficient llm generation},
  author={Dong, Harry and Chen, Beidi and Chi, Yuejie},
  booktitle={First Conference on Language Modeling},
  year={2024}
}

% Pruning recovery
%% for unstructured pruning and semi-structured pruning
@article{fast2024vladim,
  author       = {Vladim{\'{\i}}r Boza},
  title        = {Fast and Optimal Weight Update for Pruned Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2401.02938},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2401.02938},
  doi          = {10.48550/ARXIV.2401.02938},
  eprinttype    = {arXiv},
  eprint       = {2401.02938},
  timestamp    = {Tue, 23 Jan 2024 15:39:22 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2401-02938.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
%% knowledge distillation as data-efficient retraining for recovery
@article{distillation2024saurav,
  author       = {Saurav Muralidharan and
                  Sharath Turuvekere Sreenivas and
                  Raviraj Joshi and
                  Marcin Chochowski and
                  Mostofa Patwary and
                  Mohammad Shoeybi and
                  Bryan Catanzaro and
                  Jan Kautz and
                  Pavlo Molchanov},
  title        = {Compact Language Models via Pruning and Knowledge Distillation},
  journal      = {CoRR},
  volume       = {abs/2407.14679},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2407.14679},
  doi          = {10.48550/ARXIV.2407.14679},
  eprinttype    = {arXiv},
  eprint       = {2407.14679},
  timestamp    = {Thu, 22 Aug 2024 15:43:26 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2407-14679.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
% Target LLMs
@article{yang2023baichuan,
  title={Baichuan 2: Open large-scale language models},
  author={Yang, Aiyuan and Xiao, Bin and Wang, Bingning and Zhang, Borong and Bian, Ce and Yin, Chao and Lv, Chenxu and Pan, Da and Wang, Dian and Yan, Dong and others},
  journal={arXiv preprint arXiv:2309.10305},
  year={2023}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

% Instruction tuning datasets
% lamini
@inproceedings{wu2024lamini,
  title={LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions},
  author={Wu, Minghao and Waheed, Abdul and Zhang, Chiyu and Abdul-Mageed, Muhammad and Aji, Alham},
  booktitle={Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={944--964},
  year={2024}
}

% alpaca
@misc{alpaca2023,
  author = {Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori},
  title = {Stanford Alpaca: An Instruction-following {LLaMA} Model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
  commit = {Enter the specific commit hash you used, if applicable},
  note = {Accessed on: Enter the date you accessed or used the dataset}
}

% self-instruct
@inproceedings{wang2023self,
  title={Self-Instruct: Aligning Language Models with Self-Generated Instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  booktitle={The 61st Annual Meeting Of The Association For Computational Linguistics},
  year={2023}
}

% P3
@inproceedings{sanh2022multitask,
  title={Multitask Prompted Training Enables Zero-Shot Task Generalization},
  author={Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Le Scao, Teven and Raja, Arun and others},
  booktitle={ICLR 2022-Tenth International Conference on Learning Representations},
  year={2022}
}

% FLAN
@inproceedings{longpre2023flan,
  title={The flan collection: Designing data and methods for effective instruction tuning},
  author={Longpre, Shayne and Hou, Le and Vu, Tu and Webson, Albert and Chung, Hyung Won and Tay, Yi and Zhou, Denny and Le, Quoc V and Zoph, Barret and Wei, Jason and others},
  booktitle={International Conference on Machine Learning},
  pages={22631--22648},
  year={2023},
  organization={PMLR}
}

@article{tenenbaum2000global,
  title={A global geometric framework for nonlinear dimensionality reduction},
  author={Tenenbaum, Joshua B and Silva, Vin de and Langford, John C},
  journal={science},
  volume={290},
  number={5500},
  pages={2319--2323},
  year={2000},
  publisher={American Association for the Advancement of Science}
}


% Jensen-Shannon
@inproceedings{fuglede2004jensen,
  title={Jensen-Shannon divergence and Hilbert space embedding},
  author={Fuglede, Bent and Topsoe, Flemming},
  booktitle={International symposium onInformation theory, 2004. ISIT 2004. Proceedings.},
  pages={31},
  year={2004},
  organization={IEEE}
}

% KL divergence
@article{kullback1951information,
  title={On information and sufficiency},
  author={Kullback, Solomon and Leibler, Richard A},
  journal={The annals of mathematical statistics},
  volume={22},
  number={1},
  pages={79--86},
  year={1951},
  publisher={JSTOR}
}

% Instruction tuning
@inproceedings{weifinetuned,
  title={Finetuned Language Models are Zero-Shot Learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  booktitle={International Conference on Learning Representations}
}

@inproceedings{sanhmultitask,
  title={Multitask Prompted Training Enables Zero-Shot Task Generalization},
  author={Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Raja, Arun and Dey, Manan and others},
  booktitle={International Conference on Learning Representations}
}

@article{liangexploring,
  title={Exploring Format Consistency for Instruction Tuning},
  author={Liang, Shihao and Tian, Runchu and Zhu, Kunlun and Qin, Yujia and Wang, Huadong and Cong, Xin and Liu, Zhiyuan and Liu, Xiaojiang and Sun, Maosong},
  journal={Transactions on Machine Learning Research}
}

@article{zhou2024lima,
  title={Lima: Less is more for alignment},
  author={Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srinivasan and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

%% instruction tuning for recovery
@inproceedings{zhaolora,
  title={CA-LoRA: Adapting Existing LoRA for Compressed LLMs to Enable Efficient Multi-Tasking on Personal Devices},
  author={Zhao, Weilin and Huang, Yuxiang and Han, Xu and Liu, Zhiyuan and Zhang, Zhengyan and Li, Kuai and Chen, Chen and Yang, Tao and Sun, Maosong},
  booktitle={First Conference on Language Modeling}
}

@inproceedings{zhang2024loraprune,
  title={LoRAPrune: Structured Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning},
  author={Zhang, Mingyang and Chen, Hao and Shen, Chunhua and Yang, Zhen and Ou, Linlin and Yu, Xinyi and Zhuang, Bohan},
  booktitle={Findings of the Association for Computational Linguistics ACL 2024},
  pages={3013--3026},
  year={2024}
}

@article{chen2023lorashear,
  title={Lorashear: Efficient large language model structured pruning and knowledge recovery},
  author={Chen, Tianyi and Ding, Tianyu and Yadav, Badal and Zharkov, Ilya and Liang, Luming},
  journal={arXiv preprint arXiv:2310.18356},
  year={2023}
}

%% data selection for instruction tuning
@article{wang2024survey,
  title={A Survey on Data Selection for LLM Instruction Tuning},
  author={Wang, Jiahao and Zhang, Bolin and Du, Qianlong and Zhang, Jiajun and Chu, Dianhui},
  journal={arXiv preprint arXiv:2402.05123},
  year={2024}
}

@inproceedings{li2024nuggets,
  author       = {Yunshui Li and
                  Binyuan Hui and
                  Xiaobo Xia and
                  Jiaxi Yang and
                  Min Yang and
                  Lei Zhang and
                  Shuzheng Si and
                  Ling{-}Hao Chen and
                  Junhao Liu and
                  Tongliang Liu and
                  Fei Huang and
                  Yongbin Li},
  editor       = {Lun{-}Wei Ku and
                  Andre Martins and
                  Vivek Srikumar},
  title        = {One-Shot Learning as Instruction Data Prospector for Large Language
                  Models},
  booktitle    = {Proceedings of the 62nd Annual Meeting of the Association for Computational
                  Linguistics (Volume 1: Long Papers), {ACL} 2024, Bangkok, Thailand,
                  August 11-16, 2024},
  pages        = {4586--4601},
  publisher    = {Association for Computational Linguistics},
  year         = {2024},
  url          = {https://aclanthology.org/2024.acl-long.252},
  timestamp    = {Mon, 26 Aug 2024 16:40:51 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/LiHXYYZSCLLHL24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{li2024quantity,
  title={From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning},
  author={Li, Ming and Zhang, Yong and Li, Zhitao and Chen, Jiuhai and Chen, Lichang and Cheng, Ning and Wang, Jianzong and Zhou, Tianyi and Xiao, Jing},
  booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  pages={7595--7628},
  year={2024}
}

@article{caoinstruction,
  title={Instruction Mining: Instruction Data Selection for Tuning Large Language Models},
  author={Cao, Yihan and Kang, Yanbin and Wang, Chi and Sun, Lichao}
}

% LoRA
@inproceedings{hulora,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others},
  booktitle={International Conference on Learning Representations}
}

% Evaluations
%% boolq
@inproceedings{clark2019boolq,
  title={BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions},
  author={Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
  booktitle={Proceedings of NAACL-HLT},
  pages={2924--2936},
  year={2019}
}
%% PIQA
@inproceedings{bisk2020piqa,
  title={Piqa: Reasoning about physical commonsense in natural language},
  author={Bisk, Yonatan and Zellers, Rowan and Gao, Jianfeng and Choi, Yejin and others},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={05},
  pages={7432--7439},
  year={2020}
}
%% HelloSwag
@inproceedings{zellers2019hellaswag,
  title={HellaSwag: Can a Machine Really Finish Your Sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={4791--4800},
  year={2019}
}
%% WinoGrande
@article{sakaguchi2021winogrande,
  title={Winogrande: An adversarial winograd schema challenge at scale},
  author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  journal={Communications of the ACM},
  volume={64},
  number={9},
  pages={99--106},
  year={2021},
  publisher={ACM New York, NY, USA}
}
%% ARC
@article{clark2018think,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}
%% OpenbookQA
@inproceedings{mihaylov2018can,
  title={Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering},
  author={Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={2381--2391},
  year={2018}
}
%% WikiText2
@inproceedings{merity2022pointer,
  title={Pointer Sentinel Mixture Models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  booktitle={International Conference on Learning Representations},
  year={2022}
}
%% PTB
@article{marcus1993building,
  title={Building a large annotated corpus of English: The Penn Treebank},
  author={Marcus, Mitch and Santorini, Beatrice and Marcinkiewicz, Mary Ann},
  journal={Computational linguistics},
  volume={19},
  number={2},
  pages={313--330},
  year={1993}
}
%% GSM8K
@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}
%% Minerva Math
@article{lewkowycz2022solving,
  title={Solving quantitative reasoning problems with language models},
  author={Lewkowycz, Aitor and Andreassen, Anders and Dohan, David and Dyer, Ethan and Michalewski, Henryk and Ramasesh, Vinay and Slone, Ambrose and Anil, Cem and Schlag, Imanol and Gutman-Solo, Theo and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={3843--3857},
  year={2022}
}

# Quant
## GPTQ
@inproceedings{frantar2023optq,
  title={OPTQ: Accurate quantization for generative pre-trained transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023}
}

## OBQ
@article{frantar2022optimal,
  title={Optimal brain compression: A framework for accurate post-training quantization and pruning},
  author={Frantar, Elias and Alistarh, Dan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={4475--4488},
  year={2022}
}

## LLM.int8()
@article{dettmers2022gpt3,
  title={Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30318--30332},
  year={2022}
}

## Zero-Quant
@article{yao2022zeroquant,
  title={Zeroquant: Efficient and affordable post-training quantization for large-scale transformers},
  author={Yao, Zhewei and Yazdani Aminabadi, Reza and Zhang, Minjia and Wu, Xiaoxia and Li, Conglong and He, Yuxiong},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27168--27183},
  year={2022}
}

## LUT-GEMM
@inproceedings{parklut,
  title={LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models},
  author={Park, Gunho and Kim, Minsub and Lee, Sungjae and Kim, Jeonghoon and Kwon, Beomseok and Kwon, Se Jung and Kim, Byeongwook and Lee, Youngjoo and Lee, Dongsoo and others},
  booktitle={The Twelfth International Conference on Learning Representations}
}

% distance instead of accuracy
@article{dutta2024accuracy,
  title={Accuracy is Not All You Need},
  author={Dutta, Abhinav and Krishnan, Sanjeev and Kwatra, Nipun and Ramjee, Ramachandran},
  journal={arXiv preprint arXiv:2407.09141},
  year={2024}
}

% knowledge distillation
@article{muralidharan2024compact,
  title={Compact language models via pruning and knowledge distillation},
  author={Muralidharan, Saurav and Sreenivas, Sharath Turuvekere and Joshi, Raviraj and Chochowski, Marcin and Patwary, Mostofa and Shoeybi, Mohammad and Catanzaro, Bryan and Kautz, Jan and Molchanov, Pavlo},
  journal={arXiv preprint arXiv:2407.14679},
  year={2024}
}

% NMF-based spectral clustering
@inproceedings{ding2005equivalence,
  title={On the equivalence of nonnegative matrix factorization and spectral clustering},
  author={Ding, Chris and He, Xiaofeng and Simon, Horst D},
  booktitle={Proceedings of the 2005 SIAM international conference on data mining},
  pages={606--610},
  year={2005},
  organization={SIAM}
}

% Text Clustering Methods
@article{xu2024data,
  title={Data Efficient Evaluation of Large Language Models and Text-to-Image Models via Adaptive Sampling},
  author={Xu, Cong and Saranathan, Gayathri and Alam, Mahammad Parwez and Shah, Arpit and Lim, James and Wong, Soon Yee and Martin, Foltin and Bhattacharya, Suparna},
  journal={arXiv preprint arXiv:2406.15527},
  year={2024}
}

% KV cache
@article{pope2023efficiently,
  title={Efficiently scaling transformer inference},
  author={Pope, Reiner and Douglas, Sholto and Chowdhery, Aakanksha and Devlin, Jacob and Bradbury, James and Heek, Jonathan and Xiao, Kefan and Agrawal, Shivani and Dean, Jeff},
  journal={Proceedings of Machine Learning and Systems},
  volume={5},
  pages={606--624},
  year={2023}
}

% scaling law
@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

% knowledge distillation for recovery
@inproceedings{jiao2020tinybert,
  title={TinyBERT: Distilling BERT for Natural Language Understanding},
  author={Jiao, Xiaoqi and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Li, Linlin and Wang, Fang and Liu, Qun},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2020},
  pages={4163--4174},
  year={2020}
}

% code generation benchmark
@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde De Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{austin2021program,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}