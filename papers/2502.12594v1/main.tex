%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

\usepackage[accepted]{icml2025}
%\input{math_commands.tex}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{array}
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{color,colortbl}
\usepackage{pifont}
\usepackage{arydshln}
\usepackage{amsmath}
%\usepackage{algorithm}
%\usepackage{algorithmic}
%\usepackage{algpseudocode}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{arydshln}


\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{enumitem}
\usepackage{cleveref}


\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{assumption}{Assumption}
% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\setlength{\dashlinedash}{8pt}

\newcommand*{\img}[1]{%
    \raisebox{-.01\baselineskip}{%
        \includegraphics[
        height=0.8\baselineskip,
        width=0.8\baselineskip,
        keepaspectratio,
        ]{#1}%
    }%
}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{PASER: Post-Training Data Selection for Efficient Pruned Large Language Model Recovery}

\begin{document}

\twocolumn[
\icmltitle{PASER~\img{Figures/paser_icon.jpeg}: Post-Training Data Selection for Efficient Pruned Large Language Model Recovery}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{corrsponding}{*}

\begin{icmlauthorlist}
\icmlauthor{Bowei He}{cityu}
\icmlauthor{Lihao Yin}{huawei}
\icmlauthor{Hui-Ling Zhen}{huawei}
\icmlauthor{Xiaokun Zhang}{cityu}
\icmlauthor{Mingxuan Yuan}{huawei}
\icmlauthor{Chen Ma}{cityu}

%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{cityu}{Department of Computer Science, City University of Hong Kong}
\icmlaffiliation{huawei}{Huawei Noah's Ark Lab, Hong Kong}

\icmlcorrespondingauthor{Chen Ma}{chenma@cityu.edu.hk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.
\printAffiliationsAndNotice{}

\begin{abstract}
Model pruning is an effective approach for compressing large language models. However, this process often leads to significant degradation of model capabilities. While post-training techniques such as instruction tuning are commonly employed to recover model performance, existing methods often overlook the uneven deterioration of model capabilities and incur high computational costs. Moreover, some instruction data irrelevant to model capability recovery may introduce negative effects. To address these challenges, we propose the \textbf{P}ost-training d\textbf{A}ta \textbf{S}election method for \textbf{E}fficient pruned large language model \textbf{R}ecovery (\textbf{PASER}). PASER aims to identify instructions where model capabilities are most severely compromised within a certain recovery data budget. Our approach first applies manifold learning and spectral clustering to group recovery data in the semantic space, revealing capability-specific instruction sets. We then adaptively allocate the data budget to different clusters based on the degrees of model capability degradation. In each cluster, we prioritize data samples where model performance has declined dramatically. To mitigate potential negative transfer, we also detect and filter out conflicting or irrelevant recovery data. Extensive experiments demonstrate that PASER significantly outperforms conventional baselines, effectively recovering the general capabilities of pruned LLMs while utilizing merely 4\%-20\% of the original post-training data. %and substantially reducing training computational overhead.
\end{abstract}

\input{introduction.tex}

\input{related_works.tex}

%\input{preliminaries.tex}

\input{methodology.tex}

\input{experiments.tex}

\input{conclusions.tex}


% Acknowledgements should only appear in the accepted version.
%\section*{Acknowledgements}

\section*{Impact Statement}
This work aims to improve the efficiency of recovering pruned large language model capabilities while reducing computational costs. Our approach has several broader implications for society and the field of machine learning:
On the positive side, PASER could help reduce the environmental impact of training large language models by enabling more efficient recovery processes that require only 4-20\% of the original post-training data. This aligns with growing efforts to make AI development more environmentally sustainable. Additionally, by reducing computational requirements, our method could make working with large language models more accessible to researchers and organizations with limited computing resources, potentially democratizing access to AI technology.
However, we acknowledge potential risks and challenges. As our method helps recover model capabilities more efficiently, it could accelerate the deployment of compressed language models in real-world applications. This warrants careful consideration of model safety, reliability, and potential misuse. While PASER focuses on maintaining model performance, future work should examine how capability recovery impacts model biases and fairness. We encourage implementing appropriate safeguards when deploying recovered models in practice.
We also note that as this technology develops, it will be important to monitor its effects on energy consumption patterns in AI development and establish best practices for responsible use. The AI community should continue discussing how efficiency improvements like PASER can best serve society's interests while minimizing potential harms.
Our hope is that by making language model compression more practical and efficient, this work will contribute to more sustainable and accessible AI development, while acknowledging the need for careful consideration of safety and ethical implications in its deployment.




% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
%\nocite{langley00}

\bibliography{reference}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\input{appendix}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2025 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
