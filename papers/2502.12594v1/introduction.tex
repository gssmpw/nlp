\section{Introduction}
\label{sec:intro}
% model pruning -> three types of llm pruning -> the necessary points of recovery post-training
Model pruning, which aims at reducing model parameter amounts while maintaining model capabilities, has been a promising approach for large language model (LLM) compression. Mainstream LLM pruning schemes include unstructured~\citep{sparsegpt2023elias}, semi-structured~\citep{wanda2024mingjie}, and structured pruning~\citep{ma2023llm}. In practice, the capability degradation of the pruned model compared with the original LLM is almost unavoidable, especially under high pruning ratios. This degradation phenomenon is often more severe for the structured pruning scheme~\citep{dong2024prompt}, which has been widely adopted in industrial LLM compression thanks to its hardware-friendly property. Therefore, first \textit{pruning}, then recovery \textit{post-training} has been a standard pipeline~\citep{ma2023llm, zhaolora}.
Among various types of data including pre-training corpora and extensive fine-tuning datasets \citep{sheared2024mengzhou, wanda2024mingjie}, instruction tuning data has demonstrated unique advantages for efficient capability recovery~\citep{ma2023llm, zhaolora, zhang2024loraprune, chen2023lorashear}. Compared to pre-training which requires massive computational resources, instruction tuning enables effective recovery with a much smaller data scale by explicit supervision. Furthermore, through the diverse task coverage, like language modeling, common sense reasoning, mathematical problem solving, and code generation, instruction tuning preserves the model's general-purpose capabilities while preventing over-specialized recovery.

%Furthermore, instruction tuning helps maintain the model's general-purpose abilities through diverse task coverage while avoiding overly specialized recovery, like language modeling, common sense reasoning, mathematical problem solving, and code generation.

%\begin{figure*}[]
%    \centering
%    \includegraphics[width=\textwidth]{Figures/llm_recovery_performance_new.png}
%    \caption{Average performance on seven common LLM reasoning evaluation tasks after recovery post-training with different data. The numbers in brackets represent the group index of the data subset in the full dataset. \textit{Unpruned} indicates the original model and \textit{w/o Training} indicates the pruned model (using LLM-Pruner~\citep{ma2023llm}) without the recovery post-training.}
%   \label{fig: case}
%    \vspace{-0.3cm}
%\end{figure*}

Conventional schemes~\citep{ma2023llm} usually employ the full version of publicly available instruction tuning datasets like Alpaca~\citep{alpaca2023} to conduct the recovery post-training. However, this can bring significant computation overhead and even unsatisfactory recovery performance (See Appendix~\ref{appendix:intro suppport}). %as shown in Figure~\ref{fig: case}. 
An intuitive solution is to take part of the original data for training, thus consuming less data and correspondingly reducing the computation resource demand. Nevertheless, directly utilizing the uniformly split data subset (e.g., first 20\% of the data), can lead to sub-optimal performance, or even performance degradation. Moreover, the recovered performance considerably varies for models trained with different subsets. Therefore, selecting the most valuable instruction-tuning data that can contribute to recovery performance and reduce training costs becomes crucial. Though previous works have noticed the significance of selecting high-quality data to conduct the general instruction tuning~\citep{wang2024survey}, few of them are specifically designed for the recovery post-training. Note that general high quality does not necessarily mean useful for recovery.

% needs a transition: the poor recovery performance of full version data is due to the uneven model capabibility deterioration (needs evidence)
Considering the above limitations, the ideal recovery training data selection approach should exhibit the following properties: 
(1) \textbf{Targeted and Balanced Capability Recovery}: %The ideal selection method should address the disproportionate impact of pruning on various model capabilities by enabling targeted recovery of critically affected ones, while maintaining a balanced approach overall. It should curate a diverse instruction set that prioritizes severely impacted capabilities without neglecting holistic model functionality. 
Given the uneven deterioration of different capabilities in the pruning (see Appendix~\ref{appendix:intro suppport}), the ideal selection method should identify and prioritize the severely-degraded ones, while ensuring balanced recovery of the model's overall functionality. Therefore, it needs to select a comprehensive instruction set that effectively targets the most affected capabilities while still allocating appropriate recovery data to less impacted ones.
(2) \textbf{Recovery Training Efficiency}: Limited computing resources pose serious efficiency challenges to the LLM recovery post-training.
An ideal method should be able to select instructions that are both most beneficial for recovery and low in computational cost, thereby accelerating the training process and optimizing resource utilization. (3) \textbf{Mitigation of Negative Transfer}: 
Recognizing that not all instruction data is beneficial for model recovery, an optimal approach should not only identify the most advantageous instructions, but also filter out potentially harmful or irrelevant ones.
This significantly reduces the risk of negative transfer during the recovery training, ensuring that the selected data contributes positively for model recovery.

To achieve such goals, we propose the \textbf{P}ost-training d\textbf{A}ta \textbf{S}election method for \textbf{E}fficient pruned large language model \textbf{R}ecovery (\textbf{PASER}). First, we perform semantic-structural recovery instruction clustering to obtain different data groups corresponding to different LLM capabilities. Second, we select recovery instructions in a capability degradation-aware manner, where the overall data budget is allocated to different clusters based on their corresponding capability degradation degrees. %In particular, the computation cost of each sample is also taken into consideration during the inner-capability sample selection. 
In particular, when selecting samples within each capability cluster, the computation cost of each sample is also taken into consideration to optimize the efficiency of the recovery process. Finally, we construct the concept consistency graph to maintain the semantic consistency across selected instructions, thus preventing introducing conflicting or irrelevant samples. We take the LLaMA 2/3 and Baichuan2 as the target LLMs and perform the experiments under different LLM pruning schemes and different-sized instruction tuning datasets. The comparison with random and conventional instruction tuning data selection baselines demonstrates that PASER can more effectively enhance the recovered LLM performance on language modeling and various reasoning tasks. Meanwhile, the recovery training overhead can also be reduced significantly.
%vspace{-3mm}
