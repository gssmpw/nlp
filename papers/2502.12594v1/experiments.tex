\section{Experiments}
\subsection{Experiment Setup}
\textbf{Target LLMs} The experiments are performed on several open-source popular English LLMs: LLaMA2-7B/13B/70B~\citep{touvron2023llama} (hf version),  LLaMA3-8B~\citep{dubey2024llama}(instruct version), and bilingual LLMs: Baichuan2-7B/13B~\citep{yang2023baichuan}(base version), which support both English and Chinese.

\textbf{Instruction Tuning Datasets} As for the original recovery post-training data, we choose two different-size instruction tuning datasets: Alpaca~\citep{alpaca2023} and LaMini~\citep{wu2024lamini}. Alpaca contains 52K instruction-following samples generated using OpenAI's \textit{text-davinci-003} model based on 175 human-written seed tasks. LaMini contains a total of 2.58M pairs of instructions and responses synthesized with \textit{gpt-3.5-turbo} based on several existing resources of prompts, including self-instruct~\citep{wang2023self}, P3~\citep{sanh2022multitask}, FLAN~\citep{longpre2023flan} and Alpaca~\citep{alpaca2023}.

\textbf{Base Pruning Schemes} Different pruning schemes are incorporated into our experiments to explore the applicability of PASER, ranging from structured pruning methods: LLM-Pruner~\citep{ma2023llm}, SliceGPT~\citep{slidegpt2024saleh}, semi-structured pruning method: Wanda~\citep{wanda2024mingjie}, and unstructured pruning method: SparseGPT~\citep{sparsegpt2023elias}.

\begin{table*}[h]
\caption{Recovery performance of different instruction tuning data selection methods under various pruning schemes on LLaMA2-7B model. The `bold' represents the best performance under the same pruning scheme. Here, the Alpaca is taken as the original dataset.}
\centering
\resizebox{\textwidth}{!}{
%\begin{tabular}{ll@{\hspace{2pt}\dashedvline\hspace{2pt}}cc@{\hspace{2pt}\dashedvline\hspace{2pt}}ccccccc@{\hspace{2pt}\dashedvline\hspace{2pt}}c}
\begin{tabular}{ll:cc:ccccccc:c}
\toprule
\hline
Pruning  & \begin{tabular}[l]{@{}l@{}}Recovery\\ Post-training\end{tabular} & WikiText2$\downarrow$ & PTB$\downarrow$ & BoolQ & PIQA & HellaSwag & WinoGrande & ARC-e & ARC-c & OBQA & Average \\
\cline{1-12}
w/o pruning & w/o Training & 12.62 & 22.14 & 71.13 & 78.40 & 72.79 & 67.17 & 69.36 & 40.70 & 40.80 & 62.91 \\ 
\cline{1-12}
\multirow{7}{*}{\begin{tabular}[l]{@{}l@{}}LLM-Pruner\\ ratio=25\%\end{tabular}} 
& w/o Training & 20.34 & 38.81 & 61.87 & 76.61 & 65.86 & 60.22 & 63.13 & 37.37 & 39.40 & 57.78 \\ 
& Full Data & 736.42 & 1273.10 & 37.83 & 53.21 & 26.42 & 49.57 & 25.29 & 28.16 & 29.00 & 35.64 \\
& Random & 93.77 & 180.62 & 57.61 & 64.37 & 45.39 & 55.87 & 43.78 & 31.94 & 34.90 & 47.69 \\
 & Instruction Mining & 23.31 & 40.63 & 61.59 & 75.68 & 66.08 & 60.71 & 62.34 & 37.96 & 39.20 & 57.65 \\
 & IFD & 19.76 & 33.30 & 63.55 & 77.23 & 67.21 & 60.90 & 63.46 & 37.81 & 40.00 & 58.59 \\
 & Nuggets & 20.02 & 35.19 & 63.62 & \textbf{77.43} & 67.36 & 61.08 & 63.77 & 37.64 & \textbf{39.90} & 58.69 \\
 \rowcolor[gray]{0.9}& PASER & \textbf{16.40} & \textbf{26.35} & \textbf{67.25} & 77.29 & \textbf{68.98} & \textbf{66.97} & \textbf{67.84} & \textbf{39.54} & 39.80 & \textbf{61.10} \\
\cline{1-12}
\multirow{7}{*}{\begin{tabular}[l]{@{}l@{}}SliceGPT\\ ratio=25\%\end{tabular}} 
& w/o Training & 44.53 & 80.07 &65.54  & 66.87 & 54.16 & 63.38 & 58.46 & 34.56 & 36.90 & 54.27 \\
& Full Data & 38.24 & 68.53 & 68.75 & 69.84 & 57.92 & 66.18 & 62.37 & 36.82 & 38.30 & 57.17   \\
& Random & 41.86 & 74.92 & 66.89 & 68.21 & 55.79 & 64.56 & 60.23 & 35.47 & 37.60 & 55.54   \\
 & Instruction Mining  & 39.75 & 71.28 & 67.87 & 68.93 & 56.42 & 65.76 & 61.89 & 36.23 & 37.60 & 56.39  \\
 & IFD & 37.75 & 67.48 & 69.23 & 70.54 & 58.38 & 67.12 & 63.75 & 37.18 & 38.40 & 57.80 \\
 & Nuggets & 23.86 & 35.42 & 69.89 & 71.21 & 58.79 & 67.56 & \textbf{72.23} & 37.47 & 38.60 & 59.39  \\
\rowcolor[gray]{0.9} & PASER & \textbf{12.24} & \textbf{21.53} & \textbf{72.75} & \textbf{79.84} & \textbf{73.92} & \textbf{69.18} & 71.37 & \textbf{41.82} & \textbf{41.30} & \textbf{64.31}  \\
\cline{1-12}
\multirow{7}{*}{\begin{tabular}[l]{@{}l@{}}Wanda\\ sparsity=2:4\end{tabular}} 
& w/o Training &42.10  & 76.85 & 69.30 & 71.99 & 53.06 & 62.75 & 60.94 & 28.07 & 34.60 & 54.39 \\
& Full Data & 27.63 & 50.22 & 70.77 & 74.87 & 63.78 & 65.26 & 65.30 & 34.04 & 37.10 & 58.73  \\
& Random & 35.98 & 65.24 & 69.68 & 73.14 & 58.65 & 63.69 & 63.16 & 31.91 & 36.20 & 56.63   \\
 & Instruction Mining & 31.47 & 57.17 & 70.61 & 73.85 & 61.27 & 64.13 & 64.72 & 33.79 & 36.80 & 57.88  \\
 & IFD & 25.82 & 46.78 & 71.06 & 75.57 & 64.15 & 65.38 & 66.55 & 35.63 & 37.60 & 59.42  \\
 & Nuggets & 23.98 & 43.24 & \textbf{71.68} & 76.14 & 64.65 & 65.69 & 66.16 & 36.91 & 38.20 & 59.92   \\
\rowcolor[gray]{0.9} & PASER & \textbf{14.13} & \textbf{27.22} & 70.77 & \textbf{77.87} & \textbf{71.78} & \textbf{66.26} & \textbf{68.30} & \textbf{39.04} & \textbf{40.10} & \textbf{62.02}  \\
\cline{1-12}
\multirow{7}{*}{\begin{tabular}[l]{@{}l@{}}SparseGPT\\ sparsity=50\%\end{tabular}} 
& w/o Training & 19.26 & 36.41 &71.22  &75.60 & 62.85  & 66.06 & 69.11 &36.86  & 37.80 & 59.93 \\
& Full Data & 25.83 & 47.26 & 69.10 & 74.15 & 59.68 & 67.76 & 63.74 & \textbf{39.59} & 37.80 & 58.83  \\
& Random & 28.74 & 50.85 & 67.84 & 75.39 & 57.14 & 68.92 & 59.76 & 37.34 & 36.60 & 57.57 \\
 & Instruction Mining & 24.08 & 45.51 & 70.50 & 74.47 & 61.91 & 65.40 & 67.73 & 36.49 & 37.40 & 59.13  \\
 & IFD & 21.19 & 40.05 & 71.06 & 75.13 & 62.79 & 65.72 & 68.80 & 36.23 & 37.20 & 59.56  \\
 & Nuggets & 16.21 & 28.95 & 71.64 & 75.67 & 63.33 & 66.05 & 69.49 & 36.60 & 37.40 & 60.03  \\
\rowcolor[gray]{0.9} & PASER & \textbf{13.33} & \textbf{23.77} & \textbf{74.79} & \textbf{78.38} & \textbf{66.62} & \textbf{69.03} & \textbf{72.57} & 38.70 & \textbf{39.40} & \textbf{62.78}   \\
\hline
\bottomrule
\end{tabular}}
\label{tab: different pruning}
\end{table*}

\textbf{Instruction Tuning Data Selection Baselines} In addition to the random data selection, we also compare PASER with several recent general instruction tuning data selection baselines: Instruction Mining~\citep{caoinstruction}, IFD~\citep{li2024quantity}, Nuggets~\citep{li2024nuggets}. Note none of these baselines are customized for post-pruning recovery training scenario. Besides, the evaluation performance of recovery training with the full original dataset is also compared.

\textbf{Evaluation Datasets and Tasks} To thoroughly evaluate the performance of recovered LLMs, we employ seven common sense reasoning datasets:BoolQ~\citep{clark2019boolq}, PIQA~\citep{bisk2020piqa}, HellaSwag~\citep{zellers2019hellaswag}, WinoGrande~\citep{sakaguchi2021winogrande}, ARC-Easy~\citep{clark2018think}, ARC-Challenge~\citep{clark2018think}, and OpenbookQA~\citep{mihaylov2018can}. In the practice, we relies on the open-source library\footnote{https://github.com/EleutherAI/lm-evaluation-harness} to implement the evaluation, where the model needs to rank the choices in the multiple choice tasks or generate the answer in the open-ended generation tasks. The whole process is conducted in the \textit{zero-shot} manner. Besides, we follow ~\citep{ma2023llm} to evaluate the language modeling capability with the zero-shot perplexity (PPL) analysis on WikiText2~\citep{merity2022pointer} and PTB~\citep{marcus1993building}.
For enhancing evaluation comprehensiveness, we also conduct experiments on mathematical problem solving and code generation tasks, as shown in Appendix~\ref{appendix: math} and \ref{appendix: code}, respectively.
%More implementation details are provided in the appendix~\ref{appendix: implementation}.

%\textbf{Implementation Details:} Most of experiments are conducted on the server with 8 $\times$ RTX 6000 Ada GPUs. During the recovery post-training phase, we take the the low-rank approximation, LoRA~\citep{hulora}, to improve the efficiency. The corresponding hyperparameters are set as following: rank=8, batch size=64, epochs=2, learning rate = 1e-4 (Alpaca series experiments), 5e-5 (LaMini series experiments). As for the structured pruning, we set the pruning ratio as 25\% for LLaMA2-7B/LLaMA3-8B/Baichuan2-7B and 50\% for LLaMA2-13B/LLaMA2-70B/Baichuan-13B models. For the other two kinds of pruning schemes, we follow the previous work~\citep{sparsegpt2023elias} and take the 2:4 semi-structured sparsity patterns and 50\% unstructured weight sparsity. Except the experiments in Section~\ref{sec:efficiency analysis}, we set the ratio of recovery data budget $B$ to original dataset size as 20\% for Alpaca and 4\% for LaMini. As for the implementation of concept extraction in Section~\ref{sec:negative}, we use the open-source library \texttt{rake-nltk}~\footnote{https://pypi.org/project/rake-nltk/}.

\subsection{Experiment Results and Analysis}
%\subsection{Recovery Performance for Different Pruning Schemes}
\textbf{Recovery Performance for Different Pruning Schemes}
We evaluate the recovery performance of LLaMA2-7B using different instruction tuning data selection methods under structured pruning, semi-structured pruning, and unstructured pruning, respectively. According to the results in Table~\ref{tab: different pruning}, directly employing full data can indeed bring the sub-optimal recovery performance, especially under the LLM-Pruner. This is because the full version of data contains some irrelevant or conflicting information for capability recovery, resulting in the negative transfer during the training phase. Meanwhile, even the general instruction tuning data selection methods like IFD and Nuggets can bring better reasoning and language model performance than full data and random in most cases, validating the necessity of conducting recovery data selection. Furthermore, we can find that previous selection methods can hardly help model recover to the level of unpruned status, under the limited data budget. However, our PASER can not only outperform baselines, but also reduce the averaged reasoning performance degradation to less than 3\% under LLM-Pruner, Wanda, and SparseGPT. Especially, when pruning LLaMA2-7B with SliceGPT, our PASER can improve the average reasoning performance to 64.31, higher than the unpruned model. Besides, its zero-shot perplexity on WikiText2 and PTB is also lower than unpruned model slightly. This suggests that allocating recovery budget according to capability degradation levels and prioritizing most-affected samples exhibit the potential of help pruned LLM recover to the capability level of unpruned status. Besides, PASER can also be extended to other LLM post-compression scenarios, like the post-quantization recovery. The corresponding results and analysis are provided in Appendix~\ref{appendix: post-quant}.

\begin{table*}[h]
\caption{Recovery performance of different instruction tuning data selection methods on different target LLMs under LLM-Pruner scheme. The `bold' represents the best performance on the same target LLM. Here, the Alpaca is taken as the original dataset. The ``Reason'' indicates the averaged performance on 7 common sense reasoning datasets.}
\centering
\resizebox{0.98\textwidth}{!}{
\begin{tabular}{ll:c:ccccccc}
\toprule
\hline
Model & Benchmark & w/o pruning & w/o Training & Full Data & Random & Instruction Mining & IFD & Nuggets & \cellcolor[gray]{0.9}PASER \\
\hline
\multirow{3}{*}{\begin{tabular}[l]{@{}l@{}}LLaMA2-13B\\ ratio=50\%\end{tabular}} 
& WikiText2$\downarrow$ & 11.58 & 73.52 & 27.74 & 39.85 & 44.37 & 38.61 & 33.50 & \cellcolor[gray]{0.9}\textbf{21.67} \\
& PTB$\downarrow$ & 20.24 & 151.19 & 45.08 & 76.20 & 80.82 & 73.25 & 61.26 & \cellcolor[gray]{0.9}\textbf{35.09} \\
& Reason$\uparrow$ & 64.78 & 48.86 & 56.40 & 54.62 & 54.09 & 54.77 & 55.25 & \cellcolor[gray]{0.9}\textbf{57.62} \\
\hline
\multirow{3}{*}{\begin{tabular}[l]{@{}l@{}}LLaMA2-70B\\ ratio=50\%\end{tabular}} 
& WikiText2$\downarrow$ & 8.92 & 46.81 & 31.76 & 25.34 & 23.16 & 22.87 & 19.63 & \cellcolor[gray]{0.9}\textbf{12.35} \\
& PTB$\downarrow$ & 15.59 & 92.36 & 56.83 & 48.72 & 43.45 & 43.68 & 36.24 & \cellcolor[gray]{0.9}\textbf{21.82} \\
& Reason$\uparrow$ & 71.72 & 61.14 & 65.56 & 64.03 & 66.74 & 67.27 & 67.73 & \cellcolor[gray]{0.9}\textbf{69.62} \\
\hline
\multirow{3}{*}{\begin{tabular}[l]{@{}l@{}}LLaMA3-8B\\ ratio=25\%\end{tabular}} 
& WikiText2$\downarrow$ & 7.36 & 15.47 & 9.58 & 12.52 & 13.25 & 11.04 & 10.31 & \cellcolor[gray]{0.9}\textbf{8.09} \\
& PTB$\downarrow$ & 12.87 & 28.31 & 16.73 & 22.17 & 23.47 & 19.31 & 18.02 & \cellcolor[gray]{0.9}\textbf{14.16} \\
& Reason$\uparrow$ & 70.14 & 63.45 & 67.84 & 65.60 & 65.47 & 66.64 & 67.30 & \cellcolor[gray]{0.9}\textbf{69.83} \\
\hline
\multirow{3}{*}{\begin{tabular}[l]{@{}l@{}}Baichuan2-7B\\ ratio=25\%\end{tabular}} 
& WikiText2$\downarrow$ & 14.42 & 28.30 & 25.29 & 27.04 & 34.24 & 24.83 & 21.48 & \cellcolor[gray]{0.9}\textbf{16.92} \\
& PTB$\downarrow$ & 26.78 & 53.34 & 35.81 & 46.83 & 60.93 & 37.81 & 37.65 & \cellcolor[gray]{0.9}\textbf{30.76} \\
& Reason$\uparrow$ & 64.19 & 56.33 & 57.39 & 57.09 & 54.78 & 57.36 & 57.84 & \cellcolor[gray]{0.9}\textbf{59.70} \\
\hline
\multirow{3}{*}{\begin{tabular}[l]{@{}l@{}}Baichuan2-13B\\ ratio=50\%\end{tabular}} 
& WikiText2$\downarrow$ & 11.23 & 58.41 & 24.35 & 40.44 & 36.82 & 33.45 & 28.96 & \cellcolor[gray]{0.9}\textbf{14.62} \\
& PTB$\downarrow$ & 18.04 & 116.26 & 42.68 & 76.57 & 70.45 & 63.23 & 53.31 & \cellcolor[gray]{0.9}\textbf{29.82} \\
& Reason$\uparrow$ & 67.25 & 57.59 & 61.64 & 59.12 & 59.38 & 60.34 & 61.09 & \cellcolor[gray]{0.9}\textbf{63.75} \\
\hline
\bottomrule
\end{tabular}}
\label{tab: different llms}
\end{table*}

%\subsection{Robustness over Various Target Large Language Models}
\textbf{Robustness over Various Target Large Language Models}
To validate whether PASER can maintain the robust effectiveness among various target LLMs, we conduct the experiments on LLaMA2-7B/13B/70B, LLaMA3-8B, Baichuan2-7B/13B, under LLM-Pruner. According to results in Table~\ref{tab: different llms}, we can first observe the model capability under high pruning ratio (50\%) is hard to recover to unpruned level, especially for relatively smaller model like LLaMA2-13B and Baichuan2-13B. Though, PASER can still outperform random and best-performing data selection baseline, Nuggets, by 
4.41 and 2.31 points, respectively on average. Especially, for LLaMA2-70B, our PASER can control the averaged reasoning performance degradation to less than 3\%. This can be explained that the structure redundancy in 70B model is relatively higher, paving the way for effective recovery through data selection under high pruning ratios. As for the second smallest model, LLaMA3-8B, PASER can recover the reasoning performance to the 99.56\% of the unpruned status, which further demonstrates the robustness of PASER over different target LLMs. Finally, the performance of various recovery methods including PASER on Baichuan2-7B is not satisfying enough given only 25\% pruning ratio, which can be attributed to that the pruning process has severely damaged the model internal structure.

\begin{table}[h]
\caption{Recovery performance of different instruction tuning data selection methods under two structured pruning schemes on LLaMA2-7B model. The `bold' represents the best performance under the same pruning scheme. Here, the LaMini is taken as the original dataset. The ``Reason'' indicates the averaged performance on 7 reasoning datasets}
\centering
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{ll:cc:c}
\toprule
\hline
Pruning  & \begin{tabular}[l]{@{}l@{}}Recovery\\ Post-training\end{tabular} & WikiText2$\downarrow$ & PTB$\downarrow$ & Reason$\uparrow$ \\
\cline{1-5}
w/o pruning & w/o Training & 12.62 & 22.14 & 62.91 \\ 
\cline{1-5}
\multirow{7}{*}{\begin{tabular}[l]{@{}l@{}}LLM-Pruner\\ ratio=25\%\end{tabular}} 
& w/o Training & 20.34 & 38.81 & 57.78 \\  
& Full Data & 16.28 & 27.12 & 62.68 \\
& Random & 18.40 & 32.15 & 60.93 \\
& Instruction Mining & 17.83 & 28.87 & 60.76 \\
& IFD & 18.54 & 31.23 & 60.65 \\
& Nuggets & 18.27 & 30.90 & 60.99 \\
\rowcolor[gray]{0.9}& PASER & \textbf{13.45} & \textbf{22.63} & \textbf{63.79} \\
\cline{1-5}
\multirow{7}{*}{\begin{tabular}[l]{@{}l@{}}SliceGPT\\ ratio=25\%\end{tabular}} 
& w/o Training & 44.53 & 80.07 & 54.27 \\
& Full Data & 24.36 & 35.64 & 58.31 \\
& Random & 39.86 & 70.92 & 56.68 \\
& Instruction Mining & 37.75 & 67.28 & 57.53 \\
& IFD & 25.75 & 53.48 & 58.94 \\
& Nuggets & 21.86 & 31.42 & 60.96 \\
\rowcolor[gray]{0.9}& PASER & \textbf{14.27} & \textbf{23.53} & \textbf{65.74} \\
\hline
\bottomrule
\end{tabular}}
\label{tab: different datasets}
\vspace{-3mm}
\end{table}

%\subsection{Recovery Performance with Different Instruction Tuning Datasets}
\textbf{Recovery Performance with Different Instruction Tuning Datasets}
In addition to the recovery performance on Alpaca shown in Table~\ref{tab: different pruning}, we also explore the corresponding results on another larger dataset, LaMini. Especially, considering the space limitation and more severe performance degradation of structured pruning schemes, we provide the experiments results on LLM-Pruner and SliceGPT on Table~\ref{tab: different datasets}. From this table, we can observe that PASER can still consistently outperform other data selection and random methods. Besides, comparing the results in Table~\ref{tab: different pruning} and \ref{tab: different datasets}, it can be found that improving the data scale (from 10K to 10K samples) indeed facilitates the recovery performance though the significantly increased computational overhead, which is consistent with the Scaling Law~\citep{kaplan2020scaling}. We can also notice that the performance of full data on LaMini is relatively competitive, which is because the proportion of conflicting or negative data for recovery is much lower than that in Alpaca. 


\begin{table*}[h]
\caption{Ablation study results on LLaMA2-7B for each component under different pruning schemes. The ``Reason'' indicates the averaged performance on 7 common sense reasoning datasets.}
\label{tab:ablation}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{l:ccc:ccc:ccc:ccc}
\toprule
\hline
\multirow{2}{*}{Ablation Variant} & \multicolumn{3}{c}{LLM-Pruner (25\%)} & \multicolumn{3}{c}{SliceGPT (25\%)} & \multicolumn{3}{c}{Wanda (2:4)} & \multicolumn{3}{c}{SparseGPT (50\%)} \\
%\cmidrule(r){2-13}
\cline{2-13}
& WikiText2$\downarrow$ & PTB$\downarrow$ & Reason$\uparrow$ & WikiText2$\downarrow$ & PTB$\downarrow$ & Reason$\uparrow$ & WikiText2$\downarrow$ & PTB $\downarrow$& Reason$\uparrow$ & WikiText2$\downarrow$ & PTB$\downarrow$ & Reason$\uparrow$  \\
%\midrule
\cline{1-13}
w/o S$^2$RIC & 18.73 & 32.84 & 59.67 & 14.83 & 25.42 & 63.03 & 15.84 & 30.25 & 61.19 & 14.89 & 26.31 & 62.60 \\
w/o CDAIS & 17.56 & 30.15 & 60.26 & 14.16 & 24.92 & 62.68 & 15.46 & 29.48 & 61.23 & 14.62 & 25.84 & 62.49 \\
w/o NTM & 19.82 & 35.60 & 59.25 & 15.37 & 27.81 & 61.92 & 16.79 & 31.52 & 61.34 & 15.91 & 28.19 & 61.76 \\
PASER & 16.40 & 26.35 & 61.10 & 12.24 & 21.53 & 64.31 & 14.13 & 27.22 & 62.02 & 13.33 & 23.77 & 62.78 \\
\hline
\bottomrule
\end{tabular}}
\end{table*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{Figures/performance_time_new.png}
    \caption{Average reasoning performance and recovery post-training time consumption curves corresponding to different instruction tuning data selection methods. The left two subfigures are for Alpaca while right two are subfigures for LaMini.}
   \label{fig: efficiency}
\vspace{-2mm}
\end{figure*}
%\vspace{-2mm}

%\subsection{Ablation Study}
\textbf{Ablation Study}
To validate the contribution of each component in PASER, we conduct comprehensive ablation studies. Specifically, we evaluate three variants: (1) PASER w/o S$^2$RIC: replacing semantic-structural clustering with random clustering while keeping other modules unchanged; (2) PASER w/o CDAIS: randomly sampling equal number of instructions from each cluster instead of using capability degradation-aware selection; (3) PASER w/o NTM: removing the negative transfer mitigation module. 
The results in Table~\ref{tab:ablation} demonstrate that all three components contribute positively to model recovery across different pruning schemes. The semantic-structural clustering effectively identifies capability-specific instruction groups, leading to 0.18-1.43 points improvement in reasoning performance. Its removal causes degradation in both language modeling (increased perplexity) and reasoning tasks, particularly evident under structured pruning schemes like LLM-Pruner and SliceGPT. The capability degradation-aware selection mechanism enhances recovery efficiency through adaptive budget allocation, contributing 0.29-1.63 points improvement in reasoning tasks while maintaining stable language modeling performance. Notably, negative transfer mitigation shows significant impact (0.68-2.39 points improvement), especially under high pruning ratios, highlighting its importance in preventing conflicting information during recovery training. These improvements are consistently observed across different pruning schemes, with particularly pronounced effects in structured pruning where capability degradation tends to be more severe and uneven.



%\subsection{Recovery Post-training Efficiency Analysis}
%\label{sec:efficiency analysis}
\textbf{Recovery Post-training Efficiency Analysis}
To highlight PASER's advantages on recovery post-training efficiency, we conduct the experiments under different data budgets $B$ and different datasets and record the corresponding averaged reasoning performance and training time in Figure~\ref{fig: efficiency}. From the first and third subfigures, we can observe that PASER can obtain best recovery performance under different $B/N$ on Alpaca and LaMini. Interestingly, in the first subfigure, when rising $B/N$ from 0.3 to 0.4, the reasoning performance of Random even decreases. It is because expanding data scale also introduces the conflicting or negative data existing in the original dataset. From the second and fourth subfigures, PASER consistently consumes the least training time, which can be attributed to the efficiency-driven sample selection process in PASER. This advantage can be more obvious under low $B/N$ like 0.02 on LaMini. This is because increasing data budget will force PASER to select some relatively more time-consuming samples given the fixed original dataset, weakening its efficiency superiority.

In addition, more experiment results and analysis can be found in Appendix \ref{appendix: math}, \ref{appendix: code}, \ref{appendix: post-quant}, \ref{appendix: knoledge distillation}, \ref{appendix: detailed ablation}, \ref{appendix: clustering}, \ref{appendix: case study clustering}, and \ref{appendix: case study mitigation}.
\vspace{-2mm}
%In addition, we also discuss the performance under knowledge distillation in Appendix~\ref{appendix: knoledge distillation}, provide the detailed ablation study results in Appendix~\ref{appendix: detailed ablation}, explore other possible clustering methods in Appendix~\ref{appendix: clustering}, evaluate on mathematical tasks~\ref{appendix: math} and provide case study in Appendix~\ref{appendix: case study clustering} and \ref{appendix: case study mitigation}.
