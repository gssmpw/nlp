\section{Conclusion and Future Works}
\vspace{-1mm}
Recovery post-training has been an important procedure after large language model pruning to restore the critical capabilities. Previous works directly utilize the full instruction tuning dataset, facing high computation cost and risks of untargeted recovery and negative transfer. In this work, we propose the post-training data selection method for efficient pruned model recovery. According to capability degradation degrees, we allocate selection budget among different capability data obtained through semantic-structural clustering. We then select samples where model behavior has been severely affected while considering computation cost, and introduce a concept consistency graph to mitigate negative transfer. Extensive experiments on different LLMs demonstrate the effectiveness of our framework. Future work will explore other optimization approaches like data augmentation and revision to further improve recovery efficiency.
