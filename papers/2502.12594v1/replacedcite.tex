\section{Related Works}
\textbf{Large Language Model Pruning} can be generally divided into three categories: unstructured pruning, semi-structured pruning, and structured pruning. Unstructured pruning removes individual weights without structural constraints, with representative works including SparseGPT____, Wanda____, BESA____, and OWL____. This technique allows for maximum flexibility in weight selection and can achieve high compression rates while maintaining model performance. However, the resulting irregular sparsity patterns limits the practical acceleration. 
%While achieving high compression rates, it often results in irregular patterns limiting practical speedups.
Semi-structured pruning____ targets specific patterns like N:M sparsity, balancing flexibility and hardware efficiency. Structured pruning approaches like LLM-Pruner____ and SliceGPT____ remove entire structural components, offering better hardware compatibility and attracting industry attention____. However, structured pruning faces more severe performance degradation, highlighting the importance of recovery post-training.

\textbf{Instruction Tuning} has emerged as a crucial technique for enhancing LLMs____, improving their adaptability to novel tasks____. Recent works have explored instruction tuning as a post-compression recovery mechanism____. While promising, this combination faces challenges from reduced model capacity and computational costs. Most current approaches use general instruction datasets without considering compressed model's characteristics or disproportionately affected capabilities. Our work addresses these gaps by proposing a novel framework for post-training data selection in pruned LLM recovery.