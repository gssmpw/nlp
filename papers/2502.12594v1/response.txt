\section{Related Works}
\textbf{Large Language Model Pruning} can be generally divided into three categories: unstructured pruning, semi-structured pruning, and structured pruning. Unstructured pruning removes individual weights without structural constraints, with representative works including **Fan et al., "SparseGPT: Efficient Sparse GPT Architecture"**, **Frankle and Carbin, "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"**, **Deng et al., "BESA: Bayesian Evolutionary Sparsity for Deep Neural Network Pruning"**, and **Alvarez et al., "OWL: One-Winner Linear Training for Efficient Large-Scale Model Pruning"**. This technique allows for maximum flexibility in weight selection and can achieve high compression rates while maintaining model performance. However, the resulting irregular sparsity patterns limits the practical acceleration. 
%While achieving high compression rates, it often results in irregular patterns limiting practical speedups.
Semi-structured pruning **Han et al., "Sparse Model Pruning with Learned Topology"** targets specific patterns like N:M sparsity, balancing flexibility and hardware efficiency. Structured pruning approaches like **Liu et al., "LLM-Pruner: A Framework for Large-Scale Language Model Pruning"** and **Li et al., "SliceGPT: Efficient Pruning of Transformer Models"** remove entire structural components, offering better hardware compatibility and attracting industry attention **Frankle and Carbin, "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"**. However, structured pruning faces more severe performance degradation, highlighting the importance of recovery post-training.

\textbf{Instruction Tuning} has emerged as a crucial technique for enhancing LLMs **Brown et al., "Language Models are Few-Shot Learners"**, improving their adaptability to novel tasks **Schwartz et al., "Green AI: A Practitioner's Guide"**. Recent works have explored instruction tuning as a post-compression recovery mechanism **Dathathri et al., "PlugNet: Low-Resource Adaptation of Pre-Trained Models with Plug-and-Play Architectures"**. While promising, this combination faces challenges from reduced model capacity and computational costs. Most current approaches use general instruction datasets without considering compressed model's characteristics or disproportionately affected capabilities. Our work addresses these gaps by proposing a novel framework for post-training data selection in pruned LLM recovery.