% different datasets
\begin{table*}[h]
\caption{Recovery performance of different instruction tuning data selection methods under two structured pruning schemes on LLaMA2-7B model. The `bold' represents the best performance under the same pruning scheme. Here, the LaMini is taken as the original dataset.}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{ll:cc:ccccccc:c}
\toprule
\hline
Pruning  & \begin{tabular}[l]{@{}l@{}}Recovery\\ Post-training\end{tabular} & WikiText2$\downarrow$ & PTB$\downarrow$ & BoolQ & PIQA & HellaSwag & WinoGrande & ARC-e & ARC-c & OBQA & Average \\
\cline{1-12}
w/o pruning & w/o Training & 12.62 & 22.14 & 71.13 & 78.40 & 72.79 & 67.17 & 69.36 & 40.70 & 40.80 & 62.91 \\ 
\cline{1-12}
\multirow{7}{*}{\begin{tabular}[l]{@{}l@{}}LLM-Pruner\\ ratio=25\%\end{tabular}} 
& w/o Training & 20.34 & 38.81 & 61.87 & 76.61 & 65.86 & 60.22 & 63.13 & 37.37 & 39.40 & 57.78 \\  
& Full Data & 16.28 & 27.12 & 76.79 & 77.26 & 66.93 & 65.75 & \textbf{72.18} & \textbf{40.44} & 39.40 & 62.68 \\
& Random & 18.40 & 32.15 & 70.52 & 77.21 & 67.42 & 64.89 & 69.41 & 38.14 & 38.90 & 60.93 \\
 & Instruction Mining & 17.83 & 28.87 & 69.62 & 77.45 & 67.26 & 65.34 & 69.14 & 37.71 & 38.80 & 60.76 \\
 & IFD & 18.54 & 31.23 & 71.29 & 77.21 & 67.05 & 64.88 & 68.26 & 37.23 & 38.60 & 60.65 \\
 & Nuggets & 18.27 & 30.90 & 69.38 & 77.35 & 67.91 & 63.59 & 70.11 & 39.02 & 39.60 & 60.99 \\
 \rowcolor[gray]{0.9}& PASER & \textbf{13.45} & \textbf{22.63} & \textbf{77.98} & \textbf{78.36} & \textbf{69.93} & \textbf{67.31} & 72.05 & 40.28 & \textbf{40.60} & \textbf{63.79} \\
\cline{1-12}
\multirow{7}{*}{\begin{tabular}[l]{@{}l@{}}SliceGPT\\ ratio=25\%\end{tabular}} 
& w/o Training & 44.53 & 80.07 & 65.54 & 66.87 & 54.16 & 63.38 & 58.46 & 34.56 & 36.90 & 54.27 \\
& Full Data & 24.36 & 35.64 & 70.75 & 70.84 & 58.92 & 67.18 & 64.37 & 37.82 & 38.30 & 58.31  \\
& Random & 39.86 & 70.92 & 68.89 & 69.21 & 56.79 & 65.56 & 62.23 & 36.47 & 37.60 & 56.68 \\
& Instruction Mining & 37.75 & 67.28 & 69.87 & 69.93 & 57.42 & 66.76 & 63.89 & 37.23 & 37.60 & 57.53  \\
& IFD & 25.75 & 53.48 & 71.23 & 71.54 & 59.38 & 68.12 & 65.75 & 38.18 & 38.40 & 58.94 \\
& Nuggets & 21.86 & 31.42 & 71.89 & 72.21 & 59.79 & 68.56 & \textbf{74.23} & 38.47 & \textbf{41.60} & 60.96   \\
\rowcolor[gray]{0.9}& PASER  & \textbf{14.27} & \textbf{23.53} & \textbf{74.75} & \textbf{81.84} & \textbf{75.92} & \textbf{70.18} & 73.37 & \textbf{42.82} & 41.30 & \textbf{65.74}  \\
\hline
\bottomrule
\end{tabular}}
\label{tab: different datasets}
\end{table*}



\begin{table*}[h]
\caption{Recovery performance of different instruction tuning data selection methods under two structured pruning schemes on LLaMA2-7B model. The `bold' represents the best performance under the same pruning scheme. Here, the LaMini is taken as the original dataset.}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{l:c:ccccccc:cccccccc}
\toprule
\hline
Benchmark & w/o pruning & \multicolumn{7}{c:}{LLM-Pruner (ratio=25\%)} & \multicolumn{7}{c}{SliceGPT (ratio=25\%)} \\
\cline{2-17}
 & w/o Training & w/o Training & Full Data & Random & Instruction Mining & IFD & Nuggets & \cellcolor[gray]{0.9}PASER & w/o Training & Full Data & Random & Instruction Mining & IFD & Nuggets & \cellcolor[gray]{0.9}PASER \\
\hline
WikiText2$\downarrow$ & 12.62 & 20.34 & 16.28 & 18.40 & 17.83 & 18.54 & 18.27 & \cellcolor[gray]{0.9}\textbf{13.45} & 44.53 & 24.36 & 39.86 & 37.75 & 25.75 & 21.86 & \cellcolor[gray]{0.9}\textbf{14.27} \\
PTB$\downarrow$ & 22.14 & 38.81 & 27.12 & 32.15 & 28.87 & 31.23 & 30.90 & \cellcolor[gray]{0.9}\textbf{22.63} & 80.07 & 35.64 & 70.92 & 67.28 & 53.48 & 31.42 & \cellcolor[gray]{0.9}\textbf{23.53} \\
\cline{1-17}
BoolQ & 71.13 & 61.87 & 76.79 & 70.52 & 69.62 & 71.29 & 69.38 & \cellcolor[gray]{0.9}\textbf{77.98} & 65.54 & 70.75 & 68.89 & 69.87 & 71.23 & 71.89 & \cellcolor[gray]{0.9}\textbf{74.75} \\
PIQA & 78.40 & 76.61 & 77.26 & 77.21 & 77.45 & 77.21 & 77.35 & \cellcolor[gray]{0.9}\textbf{78.36} & 66.87 & 70.84 & 69.21 & 69.93 & 71.54 & 72.21 & \cellcolor[gray]{0.9}\textbf{81.84} \\
HellaSwag & 72.79 & 65.86 & 66.93 & 67.42 & 67.26 & 67.05 & 67.91 & \cellcolor[gray]{0.9}\textbf{69.93} & 54.16 & 58.92 & 56.79 & 57.42 & 59.38 & 59.79 & \cellcolor[gray]{0.9}\textbf{75.92} \\
WinoGrande & 67.17 & 60.22 & 65.75 & 64.89 & 65.34 & 64.88 & 63.59 & \cellcolor[gray]{0.9}\textbf{67.31} & 63.38 & 67.18 & 65.56 & 66.76 & 68.12 & 68.56 & \cellcolor[gray]{0.9}\textbf{70.18} \\
ARC-e & 69.36 & 63.13 & \textbf{72.18} & 69.41 & 69.14 & 68.26 & 70.11 & \cellcolor[gray]{0.9}72.05 & 58.46 & 64.37 & 62.23 & 63.89 & 65.75 & \textbf{74.23} & \cellcolor[gray]{0.9}73.37 \\
ARC-c & 40.70 & 37.37 & \textbf{40.44} & 38.14 & 37.71 & 37.23 & 39.02 & \cellcolor[gray]{0.9}40.28 & 34.56 & 37.82 & 36.47 & 37.23 & 38.18 & 38.47 & \cellcolor[gray]{0.9}\textbf{42.82} \\
OBQA & 40.80 & 39.40 & 39.40 & 38.90 & 38.80 & 38.60 & 39.60 & \cellcolor[gray]{0.9}\textbf{40.60} & 36.90 & 38.30 & 37.60 & 37.60 & 38.40 & \textbf{41.60} & \cellcolor[gray]{0.9}41.30 \\
Average & 62.91 & 57.78 & 62.68 & 60.93 & 60.76 & 60.65 & 60.99 & \cellcolor[gray]{0.9}\textbf{63.79} & 54.27 & 58.31 & 56.68 & 57.53 & 58.94 & 60.96 & \cellcolor[gray]{0.9}\textbf{65.74} \\
\hline
\bottomrule
\end{tabular}}
\label{tab: different datasets}
\end{table*}


% different target LLMs

\begin{table*}[h]
\caption{Recovery performance of different instruction tuning data selection methods on different target LLMs under LLM-Pruner scheme. The `bold' represents the best performance on the same target LLM. Here, the Alpaca is taken as the original dataset.}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{ll:cc:ccccccc:c}
\toprule
\hline
Target LLM  & \begin{tabular}[l]{@{}l@{}}Recovery\\ Post-training\end{tabular} & WikiText2$\downarrow$ & PTB$\downarrow$ & BoolQ & PIQA & HellaSwag & WinoGrande & ARC-e & ARC-c & OBQA & Average \\
\cline{1-12}
\multirow{8}{*}{\begin{tabular}[l]{@{}l@{}}LLaMA2-13B\\ ratio=50\%\end{tabular}}
& w/o pruning &11.58 & 20.24 & 69.02 & 78.73 & 76.60 & 69.69 & 73.23 & 44.20 & 42.00 & 64.78 \\ \cline{2-12}
& w/o Training & 73.52 & 151.19 & 48.99 & 69.10 & 53.03 & 53.12 & 46.80 & 31.40 & 39.60 & 48.86 \\
& Full Data & 27.74 & 45.08 & 63.52 & 74.21 & 62.28 & 56.43 & 62.79 & 35.75 & 39.80 & 56.40 \\
& Random & 39.85 & 76.20 & 59.28 & 73.71 & 60.02 & 56.09 & 59.81 & 34.04 & 39.40 & 54.62 \\
 & Instruction Mining & 44.37 & 80.82 & 57.25 & 73.22 & 59.02 & 57.28 & 62.34 & 34.64 & 34.90 & 54.09 \\
 & IFD & 38.61 & 73.25 & 57.63 & 72.89 & 59.20 & 57.81 & 62.19 & 34.90 & 38.80 & 54.77 \\
 & Nuggets & 33.50 & 61.26 & 61.08 & 74.24 & 60.98 & 55.39 & 60.96 & 34.90 & 39.20 & 55.25 \\
\rowcolor[gray]{0.9} & PASER & \textbf{21.67} & \textbf{35.09} & \textbf{65.42} & \textbf{75.69} & \textbf{63.24} & \textbf{58.56} & \textbf{63.35} & \textbf{36.45} & \textbf{40.60} & \textbf{57.62} \\
\cline{1-12}
\multirow{8}{*}{\begin{tabular}[l]{@{}l@{}}LLaMA2-70B\\ ratio=50\%\end{tabular}}
& w/o pruning &8.92 & 15.59 & 81.25 & 79.98 & 82.45 & 76.56 & 80.85 & 54.74 & 46.20 & 71.72   \\ \cline{2-12}
& w/o Training & 46.81 & 92.36 & 68.43 & 72.14 & 66.87 & 65.43 & 67.32 & 45.18 & 42.60 & 61.14  \\
& Full Data  & 31.76 & 56.83 & 74.89 & 74.62 & 74.93 & 69.84 & 74.41 & 48.45 & 41.80 & 65.56 \\
& Random & 25.34 & 48.72 & 72.56 & 73.83 & 72.21 & 68.18 & 72.38 & 47.27 & 41.80 & 64.03 \\
& Instruction Mining & 23.16 & 43.45 & 74.13 & 76.39 & 74.38 & 70.75 & 76.19 & 48.92 & 44.40 & 66.74   \\
& IFD & 22.87 & 43.68 & 75.32 & \textbf{78.50} & 75.04 & 71.23 & 75.97 & 50.61 & 44.20 & 67.27 \\
& Nuggets  & 19.63 & 36.24 & 76.18 & 77.18 & 76.28 & 71.89 & 76.80 & 51.20 & 44.60 & 67.73  \\
\rowcolor[gray]{0.9}& PASER & \textbf{12.35} & \textbf{21.82} & \textbf{78.81} & 78.38 & \textbf{79.15} & \textbf{74.25} & \textbf{78.42} & \textbf{52.95} & \textbf{45.40} & \textbf{69.62}   \\
\cline{1-12}
\multirow{8}{*}{\begin{tabular}[l]{@{}l@{}}LLaMA3-8B\\ ratio=25\%\end{tabular}}
& w/o pruning & 7.36& 12.87 &83.09  &78.62 &75.80  &72.06  &81.65  &56.74  &43.00  & 70.14 \\ \cline{2-12}
& w/o Training & 15.47 & 28.31 & 74.78 & 72.95 & 67.22 & 65.85 & 73.49 & 50.07 & 39.80 & 63.45  \\
& Full Data & 9.58 & 16.73 & 80.60 & 76.87 & 72.91 & 69.82 & 78.38 & 54.47 & 41.80 & 67.84  \\
& Random & 12.52 & 22.17 & 77.69 & 74.91 & 70.06 & 67.73 & 75.93 & 52.20 & 40.80 & 65.60  \\
& Instruction Mining & 13.25 & 23.47 & 76.85 & 74.69 & 69.74 & 67.52 & 76.34 & 52.55 & 40.60 & 65.47   \\
& IFD & 11.04 & 19.31 & 78.94 & 75.69 & 71.25 & 68.46 & 77.57 & 53.34 & 41.20 & 66.64  \\
& Nuggets & 10.31 & 18.02 & 79.77 & 76.28 & 72.01 & 69.18 & 78.38 & 53.90 & 41.60 & 67.30  \\
\rowcolor[gray]{0.9}& PASER & \textbf{8.09} & \textbf{14.16} & \textbf{82.43} & \textbf{78.44} & \textbf{75.29} & \textbf{71.62} & \textbf{81.02} & \textbf{56.61} & \textbf{43.40} & \textbf{69.83}  \\
\cline{1-12}
\multirow{8}{*}{\begin{tabular}[l]{@{}l@{}}Baichuan2-7B\\ ratio=25\%\end{tabular}}
& w/o pruning &14.42 & 26.78 & 74.04 & 77.26 & 72.24 & 68.43 & 75.08 & 42.49 & 39.80 & 64.19 \\ \cline{2-12}
& w/o Training & 28.30 & 53.34 & 49.76 & 73.29 & 63.85 & 60.14 & 67.00 & \textbf{40.27} & \textbf{40.00} & 56.33 \\
& Full Data & 25.29 & 35.81 & 52.20 & 74.81 & 66.24 & 62.75 & 67.72 & 39.42 & 38.60 & 57.39 \\
& Random & 27.04 & 46.83 & 53.15 & 73.94 & 64.60 & 60.62 & 68.73 & 39.39 & 39.20 & 57.09 \\
& Instruction Mining & 34.24 & 60.93 & 71.39 & 69.56 & 55.65 & 61.74 & 57.13 & 33.90 & 34.10 & 54.78 \\
& IFD & 24.83 & 37.81 & 53.49 & 73.86 & 64.69 & 61.25 & 68.77 & 40.24 & 39.20 & 57.36 \\
& Nuggets & 21.48 & 37.65 & 56.25 & 74.27 & \textbf{66.41} & 62.73 & 68.73 & 38.29 & 38.20 & 57.84 \\
\rowcolor[gray]{0.9}& PASER & \textbf{16.92} & \textbf{30.76} & \textbf{66.95} & \textbf{74.82} & 65.19 & \textbf{63.38} & \textbf{69.69} & 38.98 & 39.00 & \textbf{59.70} \\
\cline{1-12}
\multirow{8}{*}{\begin{tabular}[l]{@{}l@{}}Baichuan2-13B\\ ratio=50\%\end{tabular}}
& w/o pruning &11.23 & 18.04 & 79.60 &77.31 & 75.32 & 70.09 &77.36  & 47.10 & 44.00 & 67.25\\ \cline{2-12}
& w/o Training  & 58.41 & 116.26 & 65.27 & 69.58 & 62.91 & 60.98 & 64.24 & 39.55 & 40.60 & 57.59 \\
& Full Data  & 24.35 & 42.68 & 71.84 & \textbf{72.15} & 68.73 & 63.75 & 70.67 & \textbf{42.91} & 41.40 & 61.64 \\
& Random & 40.44 & 76.57 & 67.06 & 70.23 & 65.72 & 61.29 & 68.33 & 41.02 & 40.20 & 59.12 \\
& Instruction Mining & 36.82 & 70.45 & 68.26 & 70.60 & 66.47 & 61.64 & 67.56 & 40.55 & 40.60 & 59.38   \\
& IFD & 33.45 & 63.23 & 69.45 & 71.38 & 67.38 & 62.55 & 69.12 & 41.49 & 41.00 & 60.34 \\
& Nuggets & 28.96 & 53.31 & 70.64 & 71.99 & 68.48 & 63.30 & 69.89 & 41.96 & 41.40 & 61.09  \\
\rowcolor[gray]{0.9}& PASER & \textbf{14.62} & \textbf{29.82} & \textbf{75.83} & 71.76 & \textbf{72.18} & \textbf{67.24} & \textbf{73.95} & 42.52 & \textbf{42.80} & \textbf{63.75}  \\
\hline
\bottomrule
\end{tabular}}
\label{tab: different llms}
\end{table*}
\vspace{-2mm}