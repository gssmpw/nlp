\section{Methodology}
In this section, we first formulate the problem and then introduce three main components of the PASER framework (shown in Figure~\ref{fig: overall}): semantic-structural recovery instruction clustering,  capability degradation-aware instruction selection, and negative transfer mitigation. Furthermore, we provide the time complexity analysis for PASER process.

\begin{figure*}[]
    \centering
    \includegraphics[width=0.9\textwidth]{Figures/overall_framework_0127.png}
    \caption{Overall Framework for our proposed PASER recovery data selection framework.}
   \label{fig: overall}
    \vspace{-0.3cm}
\end{figure*}

%\vspace{-2mm}
\subsection{Problem Formulation}
Let $M_o$ denote the original large language model and $M_p$ the pruned version of this model. We define the instruction tuning dataset as $D = \{(x_i, y_i)\}_{i=1}^N$, where $x_i$ represents an instruction and $y_i$ its corresponding output. Our goal is to select a subset $S \subset D$ to efficiently recover the performance of $M_p$. We formulate the problem as an optimization task:
\begin{equation}
\begin{aligned}
    S^* &= \argmin_{S \subset D, |S| \leq B} \mathbb{E}_{(x,y) \sim \mathcal{T}} [\mathcal{L}(M_r(S), x, y)],\\
    \text{s.t.}\quad &M_r(S) = \text{RecoveryTrain}(M_p, S) 
\end{aligned}    
\end{equation}
\vspace{-2mm}
where $M_r(S)$ is the recovered model after training on subset $S$, $\mathcal{T}$ is the distribution of downstream evaluation tasks, $\mathcal{L}$ is a loss function. $B (B < N)$ is the recovery data budget, i.e., the maximum number of samples allowed in the selected subset.

\subsection{Semantic-Structural Recovery Instruction Clustering}
\label{sec:clustering}
During the LLM pruning process, different model capabilities can be affected unevenly by pruning. To ensure targeted and balanced recovery, we need to identify and group data points that focus on similar capabilities. To achieve this goal, we hypothesize that distinct geometric topological structures of recovery instruction data in the high-dimensional semantic space may correspond to different aspects of LLM capabilities. This hypothesis is based on the intuition that instructions requiring similar capabilities are likely to cluster together in the semantic space, forming identifiable topological structures. In detail, we employ a two-step approach on the embedding space of instructions. %First, we apply a diffusion kernel to SentenceBERT~\citep{nils2019sbert} embeddings for manifold learning to uncover the intrinsic geometric structure in the semantic space and reduce dimensionality while preserving non-linear relationships:
First, we apply a diffusion kernel to SentenceBERT~\citep{nils2019sbert} embeddings for manifold learning: 
\begin{equation}
e(x_i) = \text{DiffusionKernel}(\text{SentenceBERT}(x_i)).
\end{equation}
Here, $e(x_i)$ is the obtained low-dimensional manifold representation of instruction $x_i$. This process helps uncover the intrinsic geometric structure in the semantic space while reducing dimensionality and preserving non-linear relationships.
Then, non-negative matrix factorization (NMF)-based spectral clustering~\citep{ding2005equivalence} is conducted based on such $e(x_i)$ to identify natural groupings of instructions that potentially correspond to different LLM capabilities, leading to $K$ clusters as follows:
\begin{equation}
\begin{aligned}
    C &= \{c_1, \ldots, c_K\}  \\
      &= \text{NMFSpectralClustering}(\{e(x_i) | (x_i, y_i) \in D\}).
\end{aligned}
\end{equation}
The detailed process of these two steps are provided as below. In the first step of manifold learning, we first obtain the SentenceBERT embedding of each instruction $x_i$. Then, an adjacency matrix $A$ is constructed based on the pairwise Euclidean distances of these embeddings: $A_{ij} = \exp(-\frac{\|\text{SentenceBERT}(x_i) - \text{SentenceBERT}(x_j)\|_2^2}{2\sigma^2})$, where $\sigma$ is a scaling parameter, typically set to the median of all pairwise distances. The degree matrix $D$ is then computed as a diagonal matrix where each diagonal element is the sum of the corresponding row in $A$:$D_{ii} = \sum_{j=1}^n A_{ij}$.
Using these matrices, we define the normalized graph Laplacian $L = I - D^{-1/2}AD^{-1/2}$, where $I$ is the identity matrix. We then apply the diffusion kernel to this Laplacian $K_t = \exp(-tL)$, where $K_t$ is the diffusion kernel at time $t$. The diffusion time $t$ is selected using the spectral gap method: $t_{opt} = \argmax_t \left(\frac{d\log(\lambda_2(t))}{d\log(t)}\right)$, where $\lambda_2(t)$ is the second eigenvalue of $K_t$. The low-dimensional manifold representation $e(x_i)$ is then obtained by selecting the top $d$ eigenvectors of $K_{t_{opt}}$: $
e(x_i) = [\phi_1(x_i), \phi_2(x_i), ..., \phi_d(x_i)]$,
where $\phi_j$ are the eigenvectors of $K_{t_{opt}}$ corresponding to the $d$ largest eigenvalues.

In the second step, we perform NMF-based spectral clustering on these low-dimensional representations. Specifically, we construct a similarity matrix $S$ from the manifold representations $S_{ij} = \exp(-\frac{\|e(x_i) - e(x_j)\|^2}{2\sigma^2})$.
We then determine the optimal number of clusters $K$ by performing NMF with different values of $k$ and selecting the one that minimizes the Frobenius norm of the approximation error $K = \argmin_k \|S - W_kH_k^T\|_F$, where $W_k$ and $H_k$ are non-negative matrices resulting from NMF with $k$ components. With this optimal $K$, we decompose the similarity matrix $S$ using NMF such that $S \approx WH^T$, where $W$ and $H$ are non-negative matrices with $K$ columns.
Finally, we assign each data point to a cluster based on the maximum value in each row of $W$, where $c_i = \argmax_j W_{ij}, i = 1, ..., N$. This results in $K$ clusters $C = \{c_1, \ldots, c_K\}$, where the number of clusters $K$ is adaptively determined through the above process.



%\subsection{Capability Degradation-aware Recovery Instruction Selection}
\subsection{Capability Degradation-aware Instruction Selection}
\textbf{Capability Degradation Assessment} To prioritize the severely affected capabilities and finally achieve the balanced recovery of pruned LLMs, we need a measure of capability degradation to guide the data selection. For each cluster $c_k$ obtained in Section~\ref{sec:clustering}, we define the capability degradation score (CDS)  with the Jensen-Shannon divergence (JSD)~\citep{fuglede2004jensen} as follows:
\begin{equation}
\frac{1}{|c_k|} \sum_{(x,y) \in c_k} \frac{1}{|y|}\sum_{m=1}^{|y|}\text{JSD}(P(t_m|M_p, x)||P(t_m|M_o, x)).   
\end{equation}
Here, $P(t_m|M_p, x)$ represents the output probability distribution on the $m$-th token of the pruned model $M_p$ given input $x$. Taking a token $t^i_m (1 \leq i \leq |\text{Voc}|)$ in this distribution as an example, its corresponding probability is:
\begin{equation}
\begin{aligned}
      P(t^i_m|M_p, x) = \frac{\text{exp}(\frac{\text{logit}(t^i_m)}{\tau})}{\sum^{|\text{Voc}|}_{j=1} \text{exp}(\frac{\text{logit}(t^j_m)}{\tau})},
\end{aligned}
\end{equation}
where $\tau$ is the softmax temperature and the $|\text{Voc}|$ indicates the vocabulary size. $\text{logit}(\cdot)$ is the logit value for tokens produced by LLM.
Similarly, the $P(t_m|M_o, x)$ represents the output probability distribution for the original model $M_o$. The JSD is actually the symmetrized and smoothed version of the Kullbackâ€“Leibler divergence (KLD)~\citep{kullback1951information}: $\text{JSD}(X||Y) = \frac{1}{2} \text{KLD}(X||M) + \frac{1}{2} \text{KLD}(Y||M)$. The distribution $M=\frac{1}{2}(X+Y)$ is the mixed distribution of $X$ and $Y$.


Thus, the obtained CDS quantifies the average performance degradation for data points in each capability cluster. The choice of JSD over simple loss variations as the performance degradation signal is motivated by its unique properties. First, its symmetry ensures consistent comparison between the pruned model $M_p$ and the original model $M_o$, while its bounded range (0 to 1) provides a normalized measure of divergence. This facilitates easier interpretation and comparison across different capability clusters. Moreover, JSD's robustness to outliers and its information-theoretic foundation allow for a more nuanced assessment of capability degradation, capturing subtle changes in model outputs that might not be apparent from loss or accuracy values alone~\citep{dutta2024accuracy} due to the sampling uncertainty. The smoothing effect introduced by the use of the mixed distribution in JSD calculation also contributes to a more stable assessment across diverse instruction types. By employing JSD in our CDS calculation, we obtain a comprehensive and reliable assessment of capability degradation, enabling more accurate identification and prioritization of the capabilities most severely affected by model pruning.

\textbf{Inter-Capability Budget Allocation} Sampling a subset of high-quality data from the original set to achieve better training performance is the objective of the data selection process. To ensure the efficiency on data utilization and training process, the instruction data budget $B (B < N)$ should be maintained. Under this budget, we design an adaptive selection approach based on the above CDS for balanced recovery while focusing on severely affected capabilities. In detail, we allocate sampling budget to each cluster proportionally to its corresponding CDS:
\begin{equation}
    n_k = \left\lfloor B \cdot \frac{\text{CDS}(c_k)}{\sum_{j=1}^K \text{CDS}(c_j)} \right\rfloor.
\end{equation}
$n_k$ is the sample number budget allocated to cluster $c_k$.

\textbf{Intra-Capability Efficiency-Driven Sample Selection} To maximize computational efficiency during the recovery post-training phase, we need to select samples that offer the highest recovery benefit relative to their computational cost.
Within each cluster $c_k$, we select top $n_k$ samples based on their Individual Efficiency Scores (IES):
\begin{equation}
    \text{IES}(x, y) = \frac{\frac{1}{|y|}\sum_{m=1}^{|y|}\text{JSD}(P(t_m|M_p, x)||P(t_m|M_o, x))}{\log \text{ComputationalCost}(x, y)}.
\end{equation}
Here, ComputationalCost is instantiated with the quadratic term of sequence length $(|x| + |y|)^2 $ as the approximated complexity for LLM training. The use of JSD captures the degree of divergence between the pruned and original models' outputs, indicating areas where capabilities have been most affected and thus offering the highest potential for targeted recovery. The logarithmic term balances the consideration of computational cost, allowing for a more careful selection that favors efficient samples without overly penalizing high-potential, moderately costly instances.


\subsection{Negative Transfer Mitigation}
\label{sec:negative}
To prevent performance degradation due to conflicting or irrelevant information, we need to detect and mitigate potential negative transfer. We introduce a Concept Consistency Graph (CCG) to model relationships between concepts in the selected data. Here, a concept refers to a key knowledge unit or semantic element extracted from an instruction tuning sample. Concepts play a crucial role in capturing the essential information within instructions and help to identify potential conflicts that could lead to negative transfer. By managing relationships between concepts, we aim to maintain semantic consistency across the selected instruction tuning dataset, thereby reducing the risk of learning conflicting or irrelevant information during the recovery process. The formal definition for CCG is provided as follows:

\begin{definition}[Concept Consistency Graph]
A CCG is a graph $G = (V, E)$ where vertices $V$ represent concepts, and an edge $(v_i, v_j) \in E$ exists if concepts $v_i$ and $v_j$ co-occur in at least one instruction tuning sample without conflict.
\end{definition}

For each new sample $(x, y)$, we extract its concept $C(x, y)$ and check for consistency: $\text{IsConsistent}(x, y) = \forall v_i, v_j \in C(x, y):  (v_i, v_j) \in E \text{ or } \{v_i, v_j\} \not\subset V$. We only add samples that are consistent with the existing CCG, updating the graph with each addition. This approach ensures that we maintain a coherent set of instructions, minimizing the risk of negative transfer by avoiding the introduction of conflicting concepts during the recovery training process. The full version of our algorithm is provided in Appendix~\ref{appendix:alg}.


%\subsection{Theoretical Analysis}
%While the exact optimization of $F(S)$ is intractable due to the complex nature of model learning, we can provide an approximation guarantee for our method under certain assumptions.

%\begin{assumption}
%The Cumulative Recovery Function $F(S)$ is $\alpha$-approximately submodular. That is, for all $A \subseteq B \subseteq D$ and $e \in D \setminus B$:
%\begin{equation}
%F(A \cup \{e\}) - F(A) \geq \alpha(F(B \cup \{e\}) - F(B)) - \epsilon
%\end{equation}
%for some $\alpha \in (0, 1]$ and small $\epsilon \geq 0$.
%\end{assumption}

%This assumption is reasonable in our context because the marginal value of adding an instruction to a smaller set is likely to be larger than adding it to a larger set, but the relationship may not be perfectly submodular due to complex interactions between instructions.

%Under this assumption, we can prove the following theorem:

%\begin{theorem}
%Let $S$ be the set of instructions selected by PASER, and $S^*$ be the optimal set of size $B$. Then,
%\begin{equation}
%F(S) \geq (1-e^{-\alpha})F(S^*) - B\epsilon
%\end{equation}
%where $e$ is the base of the natural logarithm.
%\end{theorem}

%\begin{proof}
%(Sketch) The proof follows from the analysis of the greedy algorithm for approximately submodular functions. At each step, our algorithm chooses the instruction that maximizes the approximated marginal gain, which is within a factor $\alpha$ of the true marginal gain (by our assumption). The $(1-e^{-\alpha})$ factor comes from the classic analysis of the greedy algorithm, while the $B\epsilon$ term accounts for the potential non-submodularity in each of the $B$ selections.
%\end{proof}

%This theorem provides a performance guarantee for our algorithm, showing that it achieves a constant factor approximation to the optimal solution, with an additional term that accounts for the approximate submodularity of the objective function.

%Furthermore, we can analyze the computational complexity of our algorithm:

\subsection{Time Complexity Analysis}
We provide a comprehensive analysis of the time complexity for the PASER algorithm.

\begin{theorem}
The overall time complexity of PASER is $O(N\log N + NC^2)$, where $N$ is the number of instructions in $D$, and $C$ is the maximum number of concepts in any instruction tuning sample.
\end{theorem}

%\begin{proof}
\textit{Proof.} We analyze each step of the algorithm in detail:
The Semantic-Structural Recovery Instruction Clustering step, including SentenceBERT embedding, Diffusion Kernel computation, and NMF Spectral Clustering, has a dominant complexity of $O(N\log N)$.
For the Capability Degradation Assessment step, computing JSD for each sample and calculating CDS for each cluster take $O(N)$ time in total.
The Inter-capability Budget Allocation, which involves allocating the budget to clusters, has a time complexity of $O(K)$, where $K$ is the number of clusters. However, since $K \leq N$, this step does not dominate the overall complexity.
During Intra-capability Efficiency-Driven Sample Selection, for each cluster $c_k$, we perform sorting by JSD ($O(|c_k|\log |c_k|)$), iterate through sorted samples ($O(|c_k|)$), perform consistency checks (IsConsistent, $O(C^2)$ per sample), and update the CCG ($O(C^2)$ per sample). Considering all clusters, this step's total complexity is $O(N\log N + NC^2)$. Thus, the overall time complexity is dominated by the clustering step and the intra-capability sample selection step. Therefore, the total time complexity is $O(N\log N + NC^2)$. 
%\end{proof}

In practice, $C$ is often much smaller than $N$ ($C \ll N$) and can be considered as a constant factor for large $N$. Thus, we can simplify the complexity to $O(N\log N)$. This analysis demonstrates that PASER is computationally efficient and scalable for large instruction tuning datasets.




