\section{Supporting Materials for Introduction Part}
\label{appendix:intro suppport}
We provide the performance comparison in Figure~\ref{fig: case} which supports the claim in the second paragraph of Section~\ref{sec:intro} about the recovery performance deterioration. From this figure, we can find that employing the full version of recovery data or uniformly split subset to conduct recovery training can hardly achieve satisfying performance.

Besides, we also provide the evidence for the uneven deterioration of different LLM capabilities during the pruning process (corresponding to the third paragraph in Section~\ref{sec:intro}). From the Figure~\ref{fig: cap_deg}, we can observe that the four critical capabilities: language modeling, common sense reasoning, mathematical problem solving, and code generation exhibit significant difference on the performance degradation degrees. This phenomenon exits widely in the provided four pruning settings, which implies the necessity of performing targeted and balanced capability recovery. In fact, even among the various common sense reasoning tasks, this kind of uneven capability deterioration effect is still evident.
\begin{figure*}[]
    \centering
    \includegraphics[width=\textwidth]{Figures/llm_recovery_performance_new.png}
    \caption{Average performance on seven common LLM reasoning evaluation tasks after recovery post-training with different data. The numbers in brackets represent the group index of the data subset in the full dataset. \textit{Unpruned} indicates the original model and \textit{w/o Training} indicates the pruned model (using LLM-Pruner~\citep{ma2023llm}) without the recovery post-training.}
   \label{fig: case}
    \vspace{-0.3cm}
\end{figure*}

\begin{figure*}[]
    \centering
    \includegraphics[width=\textwidth]{Figures/capability_degradation.png}
    \caption{Normalized Performance Degradation ($\%$) on four various capabilities under four LLM pruning settings.}
   \label{fig: cap_deg}
    \vspace{-0.3cm}
\end{figure*}

\section{Pseudocode}
\label{appendix:alg}
We provide the full version of our pseudocode in Algorithm~\ref{alg:PASER}.
\begin{algorithm}
\caption{Post-training data Selection for efficient pruned large language model recovery (PASER)}
\label{alg:PASER}
\begin{algorithmic}
\REQUIRE $M_o$: original model, $M_p$: pruned model, $D$: instruction tuning dataset, $B$: data budget, $U$: computational cost budget
\FUNCTION{PASER ($M_o, M_p, D, B, U$)}
    \STATE $C \gets \text{NMFSpectralClustering} (\{e(x_i) | (x_i, y_i) \in D\})$
    \FOR{$c_k \in C$}
    \STATE $\text{CDS}(c_k) \gets \text{CapabilityDegradationScore}(c_k, M_o, M_p)$
    \ENDFOR
    \STATE $\{n_k\} \gets \text{AllocateSamples}(\{\text{CDS}(c_k)\}, B)$
    \STATE $S \gets \emptyset$, $G \gets \text{InitializeCCG}()$
    \FOR{$c_k \in C$}
        \STATE $L_k \gets \text{SortByIESDescending}(c_k)$
        \STATE $i \gets 0$, $\text{count} \gets 0$
        \WHILE{$\text{count} < n_k$ and $i < |L_k|$}
            \STATE $(x, y) \gets L_k[i]$
            \IF{$\text{IsConsistent}(x, y, G)$ and $\underset{(x',y') \in S \cup \{(x,y)\}}{\sum} \text{ComputationalCost}(x', y') \leq U$}
                \STATE $S \gets S \cup \{(x, y)\}$
                \STATE $G \gets \text{UpdateCCG}(G, x, y)$
                \STATE $\text{count} \gets \text{count} + 1$
            \ENDIF
            \STATE $i \gets i + 1$
        \ENDWHILE
    \ENDFOR
    \STATE \textbf{Return} $S$
\ENDFUNCTION
\end{algorithmic}
\end{algorithm}

\section{Evaluation on Mathematical Problem Solving Tasks}
\label{appendix: math}
To validate the effectiveness of PASER beyond common sense reasoning tasks, we conduct additional experiments on mathematical problem solving capabilities. Specifically, we employ two widely-adopted mathematical problem solving benchmarks:
\begin{itemize}[leftmargin=*]
   \item \textbf{GSM8K}~\citep{cobbe2021training}: A dataset containing 8.5K high-quality grade school math word problems that test various mathematical problem solving capabilities, including arithmetic, algebra, and word problem solving.
   \item \textbf{Minerva Math}~\citep{lewkowycz2022solving}: A comprehensive mathematical evaluation dataset covering diverse topics in mathematics ranging from arithmetic to calculus, with problems requiring multi-step reasoning.
\end{itemize}

\begin{table*}[h]
\caption{Recovery performance of different instruction tuning data selection methods on mathematical problem solving tasks under various pruning schemes. The 'bold' represents the best performance under the same pruning scheme.}
\label{tab:math_pruning}
\centering
\resizebox{\textwidth}{!}{%\begin{tabular}{l@{\hspace{2pt}\dashedvline\hspace{2pt}}cc@{\hspace{2pt}\dashedvline\hspace{2pt}}cc@{\hspace{2pt}\dashedvline\hspace{2pt}}cc@{\hspace{2pt}\dashedvline\hspace{2pt}}cc}
\begin{tabular}{l:cc:cc:cc:cc}
\toprule
\hline
\multirow{2}{*}{Recovery Method} & \multicolumn{2}{c:}{LLM-Pruner (25\%)} & \multicolumn{2}{c:}{SliceGPT (25\%)} & \multicolumn{2}{c:}{Wanda (2:4)} & \multicolumn{2}{c}{SparseGPT (50\%)} \\
%\cmidrule(r){2-9}
\cline{2-9}
& GSM8K & Minerva & GSM8K & Minerva & GSM8K & Minerva & GSM8K & Minerva \\
\cline{1-9}
w/o Training & 44.3 & 17.8 & 42.5 & 16.9 & 43.8 & 17.4 & 43.1 & 17.2 \\
Full Data & 46.5 & 19.1 & 44.8 & 18.3 & 45.9 & 18.7 & 45.2 & 18.5 \\
Random & 45.8 & 18.4 & 43.9 & 17.8 & 44.7 & 18.1 & 44.3 & 17.9 \\
Instruction Mining & 46.2 & 18.9 & 44.5 & 18.1 & 45.4 & 18.5 & 44.9 & 18.3 \\
IFD & 46.8 & 19.3 & 45.1 & 18.5 & 45.8 & 18.8 & 45.4 & 18.6 \\
Nuggets & 47.1 & 19.5 & 45.4 & 18.7 & 46.2 & 19.0 & 45.7 & 18.8 \\
\rowcolor[gray]{0.9} PASER & \textbf{49.4} & \textbf{21.2} & \textbf{47.8} & \textbf{20.5} & \textbf{48.5} & \textbf{20.8} & \textbf{47.2} & \textbf{20.1} \\
\hline
\bottomrule
\end{tabular}}
\end{table*}

The recovery performance under different pruning schemes is presented in Table \ref{tab:math_pruning}. From these results, we can observe that PASER consistently outperforms baseline methods across all pruning schemes on both mathematical problem solving benchmarks. The improvements are particularly significant under the LLM-Pruner scheme, where PASER achieves 5.1\% and 3.4\% absolute improvements over w/o Training on GSM8K and Minerva Math, respectively. While different pruning schemes affect the base performance levels, PASER maintains its effectiveness in recovery. For example, under the more aggressive SparseGPT (50\%) setting, PASER still achieves 4.1\% and 2.9\% improvements on GSM8K and Minerva Math over w/o Training. Compared to Full Data training, PASER achieves better performance while using only 20\% of the instruction data, demonstrating its efficiency in recovering mathematical problem solving capabilities.

These results, combined with the common sense reasoning results presented in the main paper, demonstrate that PASER is effective across diverse tasks. The strong performance on mathematical tasks is particularly noteworthy given that these problems often require precise, step-by-step reasoning and have less tolerance for errors compared to common sense reasoning tasks. This validates the effectiveness of our capability degradation score in identifying and prioritizing recovery for severely affected capabilities, even in domains requiring high precision.

\begin{table*}[h]
\caption{Recovery performance of different instruction tuning data selection methods on code generation tasks under various pruning schemes. The `bold' represents the best performance under the same pruning scheme. `P@$k$' indicates `Pass@$k$'.}
\label{tab:code_gen}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{l:ccc:ccc:ccc:ccc}
\toprule
\hline
\multirow{3}{*}{Recovery Method} & \multicolumn{6}{c:}{LLM-Pruner (25\%)} & \multicolumn{6}{c}{SliceGPT (25\%)} \\
\cline{2-13}
& \multicolumn{3}{c:}{HumanEval} & \multicolumn{3}{c:}{MBPP} & \multicolumn{3}{c:}{HumanEval} & \multicolumn{3}{c}{MBPP} \\
%\cmidrule(r){2-9}
\cline{2-13}
& P@1 & P@10 & P@100 & P@1 & P@10 & P@100 & P@1 & P@10 & P@100 & P@1 & P@10 & P@100 \\
\cline{1-13}
w/o Training & 3.4 & 6.2 & 10.4 & 7.1 & 15.5 & 24.6 & 3.0 & 5.9 & 8.7 & 6.2 & 11.8 & 21.5\\
Full Data & 7.8 & 15.1 & 19.0 & 15.2 & 26.3 & 39.5 & 2.9 & 5.1 & 11.8 & 9.3 & 19.7 & 36.4\\
Random & 6.2 & 13.7 & 20.5 & 12.8 & 23.8 & 35.0  & 3.1 & 5.8 & 14.2 & 8.5  & 17.2  & 38.6\\
Instruction Mining & 8.9 & 17.8 & 28.4 & 15.7 & 29.2 & 43.1 & 6.3 & 11.4 & 23.8 & 12.8 & 24.5 & 45.2\\
IFD & 10.5 & 21.2 & 35.6 & 18.2 & 34.5 & 50.7 & 8.7 & 16.8 & 31.2 & 16.4 & 30.8 & 52.4\\
Nuggets & 11.8 & 22.9 & 38.3 & 18.9 & 35.8 & 52.4 & 9.5 & 18.5 & 33.9 & 17.6 & 32.6 & 51.9\\
\rowcolor[gray]{0.9} PASER & \textbf{14.4} & \textbf{27.6} & \textbf{48.2} & \textbf{23.1} & \textbf{42.6} & \textbf{62.0} & \textbf{12.9} & \textbf{25.2} & \textbf{44.5} & \textbf{22.3} & \textbf{41.0} & \textbf{63.7}\\
\hline
\bottomrule
\end{tabular}}
\end{table*}

\section{Evaluation on Code Generation Tasks}
\label{appendix: code}
To further explore the PASER's effectiveness on recovering code generation capability, we take two structured pruning schemes (LLM-Pruner, SliceGPT) and perform exhaustive evaluations on two major code generation benchmarks:
\begin{itemize}[leftmargin=*]
   \item \textbf{HumanEval}~\citep{chen2021evaluating}: A widely-used code generation benchmark consisting of 164 hand-crafted Python programming problems that test various programming concepts. Each problem contains a function signature with docstring and test cases, requiring models to complete the implementation. The benchmark evaluates functional correctness using the Pass@$k$ metric, which measures the percentage of problems where a correct solution appears in k samples.
   \item \textbf{MBPP}~\citep{austin2021program}: A programming benchmark containing 974 Python programming problems focused on basic programming tasks. Each problem includes a natural language description, test cases, and a reference solution, making it particularly suitable for evaluating language-to-code generation capabilities. MBPP uses the same Pass@$k$ evaluation metric as HumanEval but generally features simpler problems with a broader coverage of basic programming concepts.
\end{itemize}
In our experiments, models are evaluated in zero-shot on HumanEval and 3-shot on MBPP. The results under Pass@$k$ ($k=1, 10, 100$) metrics are present in Table ~\ref{tab:code_gen}. As shown in the table, code generation capability experiences severe degradation after pruning. The Pass@1 performance on HumanEval drops to merely 3.4\% under LLM-Pruner without recovery training. This dramatic decline indicates that code generation, as a complex reasoning task, is particularly vulnerable during model compression. Through capability degradation-aware budget allocation and targeted sample selection, PASER demonstrates remarkable effectiveness in recovering this severely impacted capability. Under LLM-Pruner, it achieves 14.4\% Pass@1 on HumanEval, not only substantially outperforming other recovery methods but also surpassing the full data training baseline. The improvement becomes even more pronounced at higher sampling rates, i.e., PASER reaches 48.2\% Pass@100 compared to Random's 20.5\% and Instruction Mining's 28.4\%. This significant performance gap validates our approach of prioritizing recovery resources for severely degraded capabilities and selecting the most relevant instruction samples for recovery training. The superiority of PASER remains consistent across different evaluation settings. On MBPP, which features simpler programming tasks, PASER still maintains a clear advantage over baseline methods, achieving 23.1\% Pass@1 and 62.0\% Pass@100 under LLM-Pruner. When tested with a different pruning scheme (SliceGPT), which causes even more severe initial degradation (3.0\% Pass@1 on HumanEval), PASER successfully recovers the performance to 12.9\% Pass@1 and 44.5\% Pass@100. 

These results comprehensively demonstrate that our capability-aware recovery strategy effectively addresses the disproportionate impact of model compression on complex reasoning abilities, enabling targeted and efficient recovery of critical model capabilities.

\begin{table*}[h]
\caption{Recovery performance of different instruction tuning data selection methods under various LLM quantization schemes on LLaMA2-7B model. The `bold' represents the best performance under the same quantization scheme. Here, the Alpaca is taken as the original dataset.}
\centering
\resizebox{\textwidth}{!}{
%\begin{tabular}{ll@{\hspace{2pt}\dashedvline\hspace{2pt}}cc@{\hspace{2pt}\dashedvline\hspace{2pt}}ccccccc@{\hspace{2pt}\dashedvline\hspace{2pt}}c}
\begin{tabular}{ll:cc:ccccccc:c}
\toprule
\hline
Quantization  & \begin{tabular}[l]{@{}l@{}}Recovery\\ Post-training\end{tabular} & WikiText2$\downarrow$ & PTB$\downarrow$ & BoolQ & PIQA & HellaSwag & WinoGrande & ARC-e & ARC-c & OBQA & Average \\
\cline{1-12}
w/o Quant & w/o training & 12.62 & 22.14 & 71.13 & 78.40 & 72.79 & 67.17 & 69.36 & 40.70 & 40.80 & 62.91 \\ 
\cline{1-12}
\multirow{6}{*}{\begin{tabular}[l]{@{}l@{}}RTN\\ 4 bits\end{tabular}} 
& w/o training & 18.14 & 33.28 & 66.52 & 74.95 & 69.24 & 63.91 & 65.58 & 38.07 & 35.10 & 59.05 \\ 
& Full Data & 15.83 & 27.41 & 67.35 & 75.70 & 69.94 & 64.57 & 66.22 & 38.48 & 35.90 & 59.74 \\
& Random & 16.72 & 29.56 & 64.53 & 73.08 & 67.48 & 62.28 & 63.93 & 37.13 & 33.90 & 57.48 \\
 & Instruction Mining & 16.05 & 27.83 & 66.73 & 75.15 & 69.43 & 64.08 & 65.74 & 38.18 & 35.30 & 59.23 \\
 & IFD & 15.21 & 25.74 & 68.16 & 76.40 & 70.60 & 65.18 & 66.83 & 38.83 & 37.40 & 60.49\\
 & Nuggets & 14.68 & 24.53 & 68.99 & 77.13 & 71.28 & 65.82 & 67.46 & 39.21 & 38.70 & 61.23 \\
\rowcolor[gray]{0.9} & PASER & \textbf{14.21} & \textbf{23.37} & \textbf{70.43} & \textbf{78.41} & \textbf{72.47} & \textbf{66.92} & \textbf{68.54} & \textbf{39.81} & \textbf{41.50} & \textbf{62.58}\\
\cline{1-12}
\multirow{6}{*}{\begin{tabular}[l]{@{}l@{}}GPTQ\\ 4 bits\end{tabular}} 
& w/o training & 15.96 & 27.86 & 67.82 & 76.15 & 70.35 & 64.95 & 66.59 & 38.69 & 36.90 & 60.21 \\
& Full Data & 15.62 & 26.95 & 68.00 & 76.31 & 70.50 & 65.09 & 66.73 & 38.78 & 37.40 & 60.40 \\
& Random & 16.31 & 28.74 & 66.81 & 75.24 & 69.49 & 64.14 & 65.79 & 38.22 & 35.70 & 59.34\\
 & Instruction Mining & 15.37 & 26.42 & 68.31 & 76.58 & 70.75 & 65.33 & 66.96 & 38.93 & 37.90 & 60.68 \\
 & IFD  & 14.83 & 25.16 & 68.96 & 77.15 & 71.29 & 65.83 & 67.47 & \textbf{40.21} & 39.00 & 61.42 \\
 & Nuggets & 13.52 & 22.93 & 69.74 & 77.83 & 71.93 & 66.43 & 68.06 & 39.56 & 40.20 & 61.96\\
\rowcolor[gray]{0.9} & PASER & \textbf{12.95} & \textbf{21.84} & \textbf{71.20} & \textbf{79.12} & \textbf{73.12} & \textbf{67.53} & \textbf{69.14} & 40.18 & \textbf{42.90} & \textbf{63.31}\\
\hline
\bottomrule
\end{tabular}}
\label{tab: quant7b}
\end{table*}

\begin{table*}[h]
\caption{Recovery performance of different instruction tuning data selection methods under various LLM quantization schemes on LLaMA2-13B model. The `bold' represents the best performance under the same quantization scheme. Here, the Alpaca is taken as the original dataset.}
\centering
\resizebox{\textwidth}{!}{
%\begin{tabular}{ll@{\hspace{2pt}\dashedvline\hspace{2pt}}cc@{\hspace{2pt}\dashedvline\hspace{2pt}}ccccccc@{\hspace{2pt}\dashedvline\hspace{2pt}}c}
\begin{tabular}{ll:cc:ccccccc:c}
\toprule
\hline
Quantization  & \begin{tabular}[l]{@{}l@{}}Recovery\\ Post-training\end{tabular} & WikiText2$\downarrow$ & PTB$\downarrow$ & BoolQ & PIQA & HellaSwag & WinoGrande & ARC-e & ARC-c & OBQA & Average \\
\cline{1-12}
w/o Quant & w/o training &11.58 & 20.24 & 69.02 & 78.73 & 76.60 & 69.69 & 73.23 & 44.20 & 42.00 & 64.78 \\ \cline{2-12}
\cline{1-12}
\multirow{6}{*}{\begin{tabular}[l]{@{}l@{}}RTN\\ 4 bits\end{tabular}} 
& w/o training & 17.53 & 32.34 & 63.15 & 74.59 & 72.62 & 65.94 & 69.17 & 41.49 & 37.00 & 60.57 \\ 
& Full Data & 16.95 & 31.02 & 63.59 & 75.02 & 73.04 & 66.33 & 69.58 & 41.75 & 37.50 & 60.97 \\
& Random & 17.86 & 33.15 & 62.00 & 73.48 & 71.55 & 64.94 & 68.13 & 40.84 & 35.20 & 59.45\\
 & Instruction Mining & 17.24 & 31.68 & 62.83 & 74.27 & 72.32 & 65.65 & 68.87 & 41.29 & 36.10 & 60.19\\
 & IFD & 15.63 & 28.39 & 65.03 & 76.37 & 74.34 & 67.49 & 70.80 & 42.46 & 39.60 & 62.30\\
 & Nuggets & 15.08 & 27.15 & 65.45 & 76.76 & \textbf{76.72} & 67.84 & 71.17 & 42.70 & 40.20 & 62.98\\
\rowcolor[gray]{0.9} & PASER & \textbf{12.34} & \textbf{23.08} & \textbf{67.33} & \textbf{78.50} & 76.41 & \textbf{69.37} & \textbf{72.78} & \textbf{43.67} & \textbf{41.70} & \textbf{64.25}\\
\cline{1-12}
\multirow{6}{*}{\begin{tabular}[l]{@{}l@{}}GPTQ\\ 4 bits\end{tabular}} 
& w/o training & 14.74 & 26.86 & 64.68 & 76.04 & 74.02 & 67.20 & 70.49 & 42.28 & 39.10 & 61.97 \\
& Full Data & 16.02 & 29.34 & 63.62 & 75.05 & 73.07 & 66.35 & 69.61 & 41.76 & 37.40 & 60.98\\
& Random & 14.58 & 26.52 & 64.82 & 76.17 & 74.15 & 67.32 & 70.61 & 42.36 & 39.30 & 62.10\\
 & Instruction Mining & 13.67 & 24.59 & 66.37 & 77.58 & 75.51 & 68.56 & 71.91 & \textbf{44.15} & 41.30 & 63.63\\
 & IFD & 13.51 & 24.26 & \textbf{68.46} & 77.66 & 75.59 & 68.63 & 71.99 & 43.20 & 41.40 & 63.85 \\
 & Nuggets & 12.76 & 22.92 & 67.21 & 78.34 & 76.25 & 69.24 & 72.63 & 43.59 & 42.70 & 64.28\\
\rowcolor[gray]{0.9} & PASER & \textbf{11.25} & \textbf{20.93} & 68.11 & \textbf{79.16} & \textbf{77.05} & \textbf{69.97} & \textbf{73.39} & 44.04 & \textbf{44.20} & \textbf{65.13}\\
\hline
\bottomrule
\end{tabular}}
\label{tab: quant13b}
\end{table*}


\section{Extended Experiments on Post-quantization Recovery Training}
\label{appendix: post-quant}
Though the method descriptions and the experiments in the main body are mainly around the LLM pruning scenario, our PASER framework can actually be extended seamlessly to other LLM compression scenario. To further demonstrate its applicability, we conduct the experiments on post-quantization recovery training and compare our PASER with corresponding instruction tuning data selection baselines. In detail, we choose two most representative methods: Round-To-Nearest (RTN)~\citep{frantar2022optimal, yao2022zeroquant}, GPTQ~\citep{frantar2023optq}, to perform the LLM quantization. It should be clarified that RTN method, which rounds all weights to the nearest quantized value on exactly the same asymmetric per-row grid, is actually the fundamental technique in most works about LLM quantization~\citep{frantar2022optimal, yao2022zeroquant, parklut}. Its runtimes scales well to the models with many billion parameters due to the direct rounding. According to the results provided in Table~\ref{tab: quant7b} and \ref{tab: quant13b}, we can observe that the PASER can still effectively enhance the recovery performance and outperform the data selection baselines on averaged reasoning performance and zero-shot perplexity for both LLaMA2-7B and LLaMA2-13B models. Meanwhile, recovery data selection baselines can indeed achieve the stronger performance than full data and random baselines, which validates the necessity of conducting recovery data selection even in the LLM quantization scenario. Furthermore, comparing these results with Table~\ref{tab: different pruning} and \ref{tab: different llms}, it can be noticed that the improvement space of PASER in Table~\ref{tab: quant7b} and \ref{tab: quant13b} has been reduced to some extent. This is because the post-compression performance of such quantization schemes has been competitive enough, which can reflected from the w/o training row. 

\begin{table*}[t]
\caption{Knowledge distillation recovery performance of different instruction tuning data selection methods under various pruning schemes on LLaMA2-7B model. The `bold' represents the best performance under the same pruning scheme. Here, the Alpaca is taken as the original dataset.}
\centering
\resizebox{\textwidth}{!}{
%\begin{tabular}{ll@{\hspace{2pt}\dashedvline\hspace{2pt}}cc@{\hspace{2pt}\dashedvline\hspace{2pt}}ccccccc@{\hspace{2pt}\dashedvline\hspace{2pt}}c}
\begin{tabular}{ll:cc:ccccccc:c}
\toprule
\hline
Pruning  & \begin{tabular}[l]{@{}l@{}}Recovery\\ Post-training\end{tabular} & WikiText2$\downarrow$ & PTB$\downarrow$ & BoolQ & PIQA & HellaSwag & WinoGrande & ARC-e & ARC-c & OBQA & Average \\
\cline{1-12}
w/o pruning & w/o Training & 12.62 & 22.14 & 71.13 & 78.40 & 72.79 & 67.17 & 69.36 & 40.70 & 40.80 & 62.91 \\ 
\cline{1-12}
\multirow{7}{*}{\begin{tabular}[l]{@{}l@{}}LLM-Pruner\\ ratio=25\%\end{tabular}} 
& w/o Training & 20.34 & 38.81 & 61.87 & 76.61 & 65.86 & 60.22 & 63.13 & 37.37 & 39.40 & 57.78 \\ 
& Full Data & 24.72 & 43.91 & 63.30 & 76.01 & 67.18 & 62.27 & 64.23 & 36.86 & 39.20 & 58.44 \\
& Random & 23.82 & 41.20 & \textbf{68.03} & 74.89 & 66.27 & 64.51 & 64.65 & 32.58 & 38.30 & 58.46 \\
 & Instruction Mining & 22.65 & 39.40 & 62.17 & 75.98 & 66.74 & 61.29 & 63.01 & 38.32 & 39.60 & 58.16  \\
 & IFD & 19.17 & 32.30 & 64.13 & 77.55 & 67.89 & 61.56 & 64.09 & 38.19 & \textbf{40.40} & 59.12  \\
 & Nuggets & 18.64 & 32.19 & 64.46 & 76.66 & 67.26 & 64.88 & 66.50 & 36.52 & 39.20 & 59.35 \\
\rowcolor[gray]{0.9} & PASER & \textbf{15.91} & \textbf{25.39} & 67.89 & \textbf{77.81} & \textbf{69.62} & \textbf{67.63} & \textbf{68.46} & \textbf{39.87} & 40.20 & \textbf{61.64}  \\
\cline{1-12}
\multirow{6}{*}{\begin{tabular}[l]{@{}l@{}}SliceGPT\\ ratio=25\%\end{tabular}} 
& w/o training & 44.53 & 80.07 &65.54  & 66.87 & 54.16 & 63.38 & 58.46 & 34.56 & 36.90 & 54.27  \\
& Full Data & 35.48 & 66.25 & 69.35 & 70.34 & 58.50 & 66.76 & 62.95 & 37.14 & 38.70 & 57.68 \\
& Random & 38.63 & 65.67 & 67.19 & 68.59 & 56.21 & 64.94 & 60.63 & 35.61 & 37.80 & 55.85 \\
 & Instruction Mining & 35.56 & 62.14 & 68.41 & 69.51 & 57.08 & 66.33 & 62.51 & 36.59 & 38.00 & 56.92 \\
 & IFD & 33.50 & 61.33 & 69.51 & 70.82 & 58.70 & 67.49 & 64.09 & 37.22 & 38.50 & 58.05  \\
 & Nuggets & 21.39 & 32.83 & 70.17 & 71.49 & 59.11 & 67.94 & \textbf{72.51} & 37.54 & 38.70 & 59.64  \\
\rowcolor[gray]{0.9} & PASER & \textbf{11.87} & \textbf{20.91} & \textbf{73.43} & \textbf{80.32} & \textbf{74.46} & \textbf{69.76} & 71.95 & \textbf{42.26} & \textbf{41.70} & \textbf{64.84} \\
\cline{1-12}
\multirow{6}{*}{\begin{tabular}[l]{@{}l@{}}Wanda\\ sparsity=2:4\end{tabular}} 
& w/o training &42.10  & 76.85 & 69.30 & 71.99 & 53.06 & 62.75 & 60.94 & 28.07 & 34.60 & 54.39 \\
& Full Data & 25.92 & 47.85 & 71.09 & 75.14 & 64.10 & 65.62 & 65.64 & 34.38 & 37.50 & 59.07   \\
& Random  & 34.98 & 63.47 & 70.18 & 73.62 & 59.15 & 63.83 & 63.70 & 32.13 & 36.50 & 57.02  \\
 & Instruction Mining & 30.56 & 55.56 & 71.03 & 73.97 & 61.69 & 64.56 & 64.86 & 33.93 & 37.00 & 58.15  \\
 & IFD & 24.08 & 41.44 & 71.78 & 75.89 & 64.83 & 65.72 & \textbf{68.89} & 35.97 & 38.00 & 60.15\\
 & Nuggets  & 23.14 & 40.10 & \textbf{72.26} & 76.50 & 65.33 & 66.03 & 66.52 & 37.27 & 38.60 & 60.36  \\
\rowcolor[gray]{0.9} & PASER & \textbf{13.84} & \textbf{23.54} & 71.25 & \textbf{78.15} & \textbf{72.06} & \textbf{66.64} & 68.66 & \textbf{39.38} & \textbf{40.50} & \textbf{62.38} \\
\cline{1-12}
\multirow{6}{*}{\begin{tabular}[l]{@{}l@{}}SparseGPT\\ sparsity=50\%\end{tabular}}
& w/o training & 19.26 & 36.41 &71.22  &75.60 & 62.85  & 66.06 & 69.11 &36.86  & 37.80 & 59.93 \\
& Full Data & 28.17 & 52.82 & 68.52 & 75.77 & 57.84 & 69.26 & 60.43 & 37.72 & 37.00 & 58.08 \\
& Random  & 25.31 & 43.22 & 69.74 & 74.91 & 60.28 & 68.10 & 64.06 & \textbf{39.95} & \textbf{39.80} & 59.55  \\
 & Instruction Mining  & 21.56 & 39.61 & 71.12 & 74.85 & 62.53 & 66.06 & 68.07 & 36.85 & 37.80 & 59.61  \\
 & IFD & 17.76 & 31.25 & 71.70 & 75.76 & 63.43 & 66.06 & 69.14 & 36.59 & 37.60 & 60.04 \\
 & Nuggets & 14.83 & 25.38 & 72.18 & 75.95 & 63.91 & 66.29 & 69.75 & 36.86 & 37.70 & 60.38  \\
\rowcolor[gray]{0.9} & PASER & \textbf{13.00} & \textbf{22.24} & \textbf{75.07} & \textbf{78.66} & \textbf{66.90} & \textbf{69.31} & \textbf{72.85} & 38.89 & 39.60 & \textbf{63.04}  \\
\hline
\bottomrule
\end{tabular}}
\label{tab: kd}
\end{table*}
\vspace{-3mm}

\section{Extended Experiments on Recovery Training with Knowledge Distillation}
\label{appendix: knoledge distillation}
Inspired by ~\citep{muralidharan2024compact}, we explore the knowledge distillation as the recovery post-training paradigm instead of the standard supervised learning with the groundtruth label. Here, we set the original model $M_o$ as the teacher and the pruned model $M_p$ as the student. The mean KL divergence~\citep{kullback1951information} between the output probability distribution of $M_o$ and that of $M_p$ is taken as the objective function. Comparing the corresponding results under different pruning schemes in Table~\ref{tab: kd} with that in Table~\ref{tab: different pruning}, we can first observe that knowledge distillation can effectively improve the recovery performance on both reasoning and language modeling tasks in most cases. In particular, the reasoning performance of PASER is improved by 0.348 points on average among such four pruning schemes. Interestingly, the knowledge distillation recovery performance of Full Data under LLM-Pruner is much better than that with standard label-supervised learning. This demonstrates that knowledge distillation is also a promising approach to avoid the misleading information from the irrelevant or conflicting samples existing in the original dataset. Because its learning process directly imitates the unpruned model behavior instead of the provided labels, thus better preserving the thinking and decision-making consistency with the original model. As a summary, distilling the knowledge of unpruned model into the pruned model can be regarded as an effective way to enhance the recovery performance, though bring more memory overhead. Furthermore, stronger layer-wise distillation can also be taken into consideration~\citep{jiao2020tinybert}.
\vspace{-3mm}
\paragraph{Exploration on Combined Training Strategies}
Given the complementary potential of knowledge distillation (KD) and supervised fine-tuning (SF), we further explore whether combining these two approaches could lead to enhanced recovery performance. Specifically, we investigate two cascading strategies: (1) first applying KD followed by SF, and (2) first conducting SF followed by KD. Table \ref{tab:kd_combination} presents the results under different pruning schemes.

\begin{table*}[h]
\caption{Recovery performance comparison between different combinations of knowledge distillation (KD) and supervised fine-tuning (SF) under various pruning schemes. The 'bold' represents the best performance under the same pruning scheme.}
\label{tab:kd_combination}
\centering
\resizebox{\textwidth}{!}{
%\begin{tabular}{l@{\hspace{2pt}\dashedvline\hspace{2pt}}cc@{\hspace{2pt}\dashedvline\hspace{2pt}}ccccccc@{\hspace{2pt}\dashedvline\hspace{2pt}}c}
\begin{tabular}{l:cc:ccccccc:c}
\toprule
\hline
Recovery Training & WikiText2↓ & PTB↓ & BoolQ & PIQA & HellaSwag & WinoGrande & ARC-e & ARC-c & OBQA & Average \\
\cline{1-11}
\multicolumn{11}{c}{LLM-Pruner (ratio=25\%)} \\
\cline{1-11}
KD & 15.91 & 25.39 & 67.89 & 77.81 & 69.62 & 67.63 & 68.46 & 39.87 & 40.20 & \textbf{61.64} \\
SF & 16.40 & 26.35 & 67.25 & 77.29 & 68.98 & 66.97 & 67.84 & 39.54 & 39.80 & 61.10 \\
First KD, then SF & 16.15 & 25.87 & 67.57 & 77.55 & 69.31 & 67.30 & 68.15 & 39.71 & 40.00 & 61.37 \\
First SF, then KD & 16.28 & 26.02 & 67.41 & 77.43 & 69.15 & 67.11 & 67.96 & 39.63 & 39.90 & 61.23 \\
\cline{1-11}
\multicolumn{11}{c}{SliceGPT (ratio=25\%)} \\
\cline{1-11}
KD & 11.87 & 20.91 & 73.43 & 80.32 & 74.46 & 69.76 & 71.95 & 42.26 & 41.70 & \textbf{64.84} \\
SF & 12.24 & 21.53 & 72.75 & 79.84 & 73.92 & 69.18 & 71.37 & 41.82 & 41.30 & 64.31 \\
First KD, then SF & 12.06 & 21.24 & 73.12 & 80.05 & 74.18 & 69.45 & 71.62 & 42.03 & 41.50 & 64.56 \\
First SF, then KD & 12.15 & 21.38 & 72.94 & 79.95 & 74.05 & 69.32 & 71.51 & 41.95 & 41.40 & 64.45 \\
\cline{1-11}
\multicolumn{11}{c}{Wanda (sparsity=2:4)} \\
\cline{1-11}
KD & 13.84 & 23.54 & 71.25 & 78.15 & 72.06 & 66.64 & 68.66 & 39.38 & 40.50 & \textbf{62.38} \\
SF & 14.13 & 27.22 & 70.77 & 77.87 & 71.78 & 66.26 & 68.30 & 39.04 & 40.10 & 62.02 \\
First KD, then SF & 13.97 & 25.31 & 71.02 & 78.03 & 71.94 & 66.47 & 68.49 & 39.23 & 40.30 & 62.21 \\
First SF, then KD & 14.05 & 26.28 & 70.89 & 77.95 & 71.85 & 66.35 & 68.41 & 39.15 & 40.20 & 62.11 \\
\cline{1-11}
\multicolumn{11}{c}{SparseGPT (sparsity=50\%)} \\
\cline{1-11}
KD & 13.00 & 22.24 & 75.07 & 78.66 & 66.90 & 69.31 & 72.85 & 38.89 & 39.60 & \textbf{63.04} \\
SF & 13.33 & 23.77 & 74.79 & 78.38 & 66.62 & 69.03 & 72.57 & 38.70 & 39.40 & 62.78 \\
First KD, then SF & 13.15 & 22.96 & 74.94 & 78.53 & 66.78 & 69.18 & 72.72 & 38.81 & 39.50 & 62.92 \\
First SF, then KD & 13.24 & 23.35 & 74.85 & 78.45 & 66.70 & 69.11 & 72.64 & 38.75 & 39.45 & 62.85 \\
\hline
\bottomrule
\end{tabular}
}
\end{table*}

Interestingly, the results show that neither cascading strategy consistently outperforms individual KD or SF approaches. This suggests that these two training paradigms might actually serve similar functions in recovering model capabilities, making their combination redundant rather than complementary. Knowledge distillation shows slightly better performance across all pruning schemes, which could be attributed to its ability to capture the nuanced knowledge encoded in the teacher model's full output distribution. However, the marginal gains from combining approaches do not justify the additional computational overhead required for cascaded training.
\vspace{-1mm}


\section{Detailed Ablation Study Results}
\label{appendix: detailed ablation}
In this section, we present comprehensive ablation results of the three key components in PASER: semantic-structural recovery instruction clustering (S$^2$RIC), capability degradation-aware instruction selection (CDAIS), and negative transfer mitigation (NTM). Table~\ref{tab: detailed ablation} shows the detailed performance across different evaluation metrics.

The detailed results reveal the distinct contributions of each component under different pruning schemes. For structured pruning like LLM-Pruner, removing S$^2$RIC leads to significant degradation in both language modeling (perplexity increases from 16.40 to 18.73 on WikiText2) and reasoning tasks (average score drops by 1.43 points), highlighting its importance in addressing uneven capability degradation. The impact of CDAIS is particularly evident under SliceGPT, where its removal causes a 1.63-point drop in average reasoning performance while maintaining relatively stable language modeling metrics, demonstrating its effectiveness in balancing recovery priorities. Under semi-structured pruning (Wanda), all three components show more balanced contributions, with performance drops ranging from 0.68 to 0.83 points when each is removed. This suggests that semi-structured pruning requires a more holistic recovery approach. For unstructured pruning (SparseGPT) where capability degradation tends to be more uniform, NTM plays a particularly crucial role - its removal leads to the largest drop in language modeling performance (perplexity increases from 13.33 to 15.91 on WikiText2) and affects complex reasoning tasks like WinoGrande and ARC-e significantly. Notably, the full PASER framework consistently achieves the best performance across almost all metrics under various pruning schemes, with only occasional exceptions in individual tasks (e.g., OBQA in LLM-Pruner and PIQA in SparseGPT). This comprehensive superiority validates our design choice of combining these three components for effective pruned model recovery.
\vspace{-1mm}

\begin{table*}[h]
\caption{The detailed ablation study for our proposed three components under various pruning schemes on LLaMA2-7B model. The `bold' represents the best performance under the same pruning scheme. Here, the Alpaca is taken as the original dataset.}
\centering
\resizebox{\textwidth}{!}{
%\begin{tabular}{ll@{\hspace{2pt}\dashedvline\hspace{2pt}}cc@{\hspace{2pt}\dashedvline\hspace{2pt}}ccccccc@{\hspace{2pt}\dashedvline\hspace{2pt}}c}
\begin{tabular}{ll:cc:ccccccc:c}
\toprule
\hline
Pruning  & \begin{tabular}[l]{@{}l@{}}Recovery\\ Post-training\end{tabular} & WikiText2$\downarrow$ & PTB$\downarrow$ & BoolQ & PIQA & HellaSwag & WinoGrande & ARC-e & ARC-c & OBQA & Average \\
\cline{1-12}
\multirow{2}{*}{\begin{tabular}[l]{@{}l@{}}LLM-Pruner\\ ratio=25\%\end{tabular}} 
& PASER w/o S$^2$RIC & 18.73 & 32.84 & 65.31 & 76.84 & 67.59 & 64.85 & 65.92 & 37.96 & 39.20 & 59.67  \\
& PASER w/o CDAIS & 17.56 & 30.15 & 66.27 & 77.03 & 68.15 & 65.73 & 66.58 & 38.54 & 39.50 & 60.26  \\
 & PASER w/o NTM & 19.82 & 35.60 & 64.83 & \textbf{77.52} & 67.34 & 64.48 & 63.59 & 36.78 & \textbf{40.20} & 59.25  \\
 & PASER & \textbf{16.40} & \textbf{26.35} & \textbf{67.25} & 77.29 & \textbf{68.98} & \textbf{66.97} & \textbf{67.84} & \textbf{39.54} & 39.80 & \textbf{61.10} \\
\cline{1-12}
\multirow{2}{*}{\begin{tabular}[l]{@{}l@{}}SliceGPT\\ ratio=25\%\end{tabular}} 
& PASER w/o S$^2$RIC & 14.83 & 25.42 & 71.15 & 78.91 & 72.25 & 67.84 & 69.95 & 40.82 & 40.30 & 63.03  \\
& PASER w/o CDAIS & 14.16 & 24.92 & 70.89 & 78.56 & 71.84 & 67.45 & 69.58 & 40.47 & 40.00 & 62.68  \\
 & PASER w/o NTM & 15.37 & 27.81 & 69.97 & 77.33 & 70.68 & 65.92 & 68.03 & 39.39 & \textbf{42.10} & 61.92 \\
 & PASER & \textbf{12.24} & \textbf{21.53} & \textbf{72.75} & \textbf{79.84} & \textbf{73.92} & \textbf{69.18} & \textbf{71.37} & \textbf{41.82} & 41.30 & \textbf{64.31}  \\
\cline{1-12}
\multirow{2}{*}{\begin{tabular}[l]{@{}l@{}}Wanda\\ sparsity=2:4\end{tabular}} 
& PASER w/o S$^2$RIC & 15.84 & 30.25 & 69.26 & 77.42 & 70.31 & 65.82 & 67.84 & 38.67 & 39.00 & 61.19  \\
& PASER w/o CDAIS & 15.46 & 29.48 & 69.14 & 77.35 & 70.27 & 65.74 & 67.79 & 38.75 & 39.60 & 61.23  \\
 & PASER w/o NTM  & 16.79 & 31.52 & 69.51 & 76.92 & 70.76 & 65.23 & 67.28 & 38.47 & \textbf{41.20} & 61.34  \\
 & PASER & \textbf{14.13} & \textbf{27.22} & \textbf{70.77} & \textbf{77.87} & \textbf{71.78} & \textbf{66.26} & \textbf{68.30} & \textbf{39.04} & 40.10 & \textbf{62.02}  \\
\cline{1-12}
\multirow{2}{*}{\begin{tabular}[l]{@{}l@{}}SparseGPT\\ sparsity=50\%\end{tabular}}
& PASER w/o S$^2$RIC & 14.89 & 26.31 & 73.25 & 77.45 & \textbf{70.15} & 68.47 & 69.28 & 39.82 & 39.80 & 62.60  \\
& PASER w/o CDAIS & 14.62 & 25.84 & 72.91 & 77.50 & 69.93 & 68.12 & 69.05 & \textbf{39.94} & 40.00 & 62.49  \\
 & PASER w/o NTM & 15.91 & 28.19 & 71.53 & \textbf{78.62} & 65.48 & 67.21 & 69.79 & 39.18 & \textbf{40.50} & 61.76 \\
 & PASER & \textbf{13.33} & \textbf{23.77} & \textbf{74.79} & 78.38 & 66.62 & \textbf{69.03} & \textbf{72.57} & 38.70 & 39.40 & \textbf{62.78}   \\
\hline
\bottomrule
\end{tabular}}
\label{tab: detailed ablation}
\end{table*}
\vspace{-2mm}

\section{Exploration on Other Possible Clustering Methods}
\label{appendix: clustering}
To discuss the impact of different instruction tuning data clustering approaches, we replace our Semantic-structural Recovery Instruction Clustering (S$^2$RIC) module with some other common text clustering method: NMF\_TFIDF, LDA\_TFIDF, KMeans\_TFIDF, Spectral\_MTEB, Spectral\_BERT~\citep{xu2024data}. The reasoning performance comparison among different PASER versions with such clustering methods is provided in Table~\ref{tab: different clustering}. From the table, we can find that integrating other instruction clustering methods with PASER can bring the performance decline to some extent among all four pruning schemes. Especially, the clustering method with traditional statistics-based text representation technique, TFIDF, generally behaves worse than semantic embedding-based text representation techniques like BERT. Therefore, we can conclude that our semantic-structural recovery instruction clustering is at least a competitive approach as the clustering component of PASER. Though, comparing these results with those in Table~\ref{tab: different pruning}, we can observe the advantages of PASER over other general instruction tuning data selection methods can still be stably maintained. This further demonstrates that the potential of the clustering-based data selection for effective and balanced LLM capability recovery.

\begin{table*}[h]
\caption{Recovery performance of multiple PASER versions integrated with different data clustering approaches under various pruning schemes on LLaMA2-7B model. The PASER(S$^2$RIC) is the version we employ in the main body. The `bold' represents the best performance under the same pruning scheme. Here, the Alpaca is taken as the original dataset.}
\centering
\resizebox{\textwidth}{!}{
%\begin{tabular}{ll@{\hspace{2pt}\dashedvline\hspace{2pt}}cc@{\hspace{2pt}\dashedvline\hspace{2pt}}ccccccc@{\hspace{2pt}\dashedvline\hspace{2pt}}c}
\begin{tabular}{ll:cc:ccccccc:c}
\toprule
\hline
Pruning  & \begin{tabular}[l]{@{}l@{}}Recovery\\ Post-training\end{tabular} & WikiText2$\downarrow$ & PTB$\downarrow$ & BoolQ & PIQA & HellaSwag & WinoGrande & ARC-e & ARC-c & OBQA & Average \\
\cline{1-12}
w/o pruning & w/o Training & 12.62 & 22.14 & 71.13 & 78.40 & 72.79 & 67.17 & 69.36 & 40.70 & 40.80 & 62.91 \\ 
\cline{1-12}
\multirow{7}{*}{\begin{tabular}[l]{@{}l@{}}LLM-Pruner\\ ratio=25\%\end{tabular}} 
& w/o Training & 20.34 & 38.81 & 61.87 & 76.61 & 65.86 & 60.22 & 63.13 & 37.37 & 39.40 & 57.78 \\ 
& PASER(NMF\_TFIDF) & 17.82 & 29.45 & 65.93 & 76.88 & 67.42 & 65.19 & 66.37 & 38.81 & 39.60 & 60.03  \\
& PASER(LDA\_TFIDF)  & 17.56 & 28.91 & 66.18 & 77.02 & 67.76 & 65.58 & 66.92 & 38.95 & 39.70 & 60.30  \\
 & PASER(KMeans\_TFIDF) & 17.21 & 28.13 & 66.47 & 77.15 & 68.04 & 65.92 & 67.23 & 39.12 & \textbf{39.80} & 60.53 \\
 & PASER(Spectral\_MTEB)  & 16.82 & 27.24 & 66.89 & 77.23 & 68.46 & 66.38 & 67.56 & 39.31 & \textbf{39.80} & 60.80 \\
 & PASER(Spectral\_BERT)  & 16.61 & 26.79 & 67.06 & 77.26 & 68.72 & 66.68 & 67.71 & 39.43 & \textbf{39.80} & 60.95   \\
 \rowcolor[gray]{0.9}& PASER(S$^2$RIC) & \textbf{16.40} & \textbf{26.35} & \textbf{67.25} & \textbf{77.29} & \textbf{68.98} & \textbf{66.97} & \textbf{67.84} & \textbf{39.54} & \textbf{39.80} & \textbf{61.10} \\
\cline{1-12}
\multirow{7}{*}{\begin{tabular}[l]{@{}l@{}}SliceGPT\\ ratio=25\%\end{tabular}} 
& w/o Training & 44.53 & 80.07 &65.54  & 66.87 & 54.16 & 63.38 & 58.46 & 34.56 & 36.90 & 54.27 \\
& PASER(NMF\_TFIDF) & 14.27 & 24.36 & 70.89 & 78.76 & 72.13 & 67.69 & 70.12 & \textbf{41.95} & 40.80 & 63.21   \\
& PASER(LDA\_TFIDF)  & 14.86 & 25.19 & 70.31 & 78.42 & 71.64 & 67.25 & 69.58 & 40.37 & 40.60 & 62.60   \\
 & PASER(KMeans\_TFIDF)   & 13.58 & 23.42 & 71.46 & 79.07 & 72.61 & 68.14 & 70.48 & 41.08 & 41.00 & 63.41 \\
 & PASER(Spectral\_MTEB) & 12.91 & 22.47 & 72.08 & 79.41 & 73.18 & 68.62 & 70.87 & 41.43 & 41.10 & 63.81 \\
 & PASER(Spectral\_BERT) & 12.58 & 22.01 & 72.41 & 79.63 & 73.55 & 68.91 & 71.12 & 41.63 & 41.20 & 64.06   \\
\rowcolor[gray]{0.9} & PASER & \textbf{12.24} & \textbf{21.53} & \textbf{72.75} & \textbf{79.84} & \textbf{73.92} & \textbf{69.18} & \textbf{71.37} & 41.82 & \textbf{41.30} & \textbf{64.31}  \\
\cline{1-12}
\multirow{7}{*}{\begin{tabular}[l]{@{}l@{}}Wanda\\ sparsity=2:4\end{tabular}} 
& w/o Training &42.10  & 76.85 & 69.30 & 71.99 & 53.06 & 62.75 & 60.94 & 28.07 & 34.60 & 54.39 \\
& PASER(NMF\_TFIDF)  & 16.18 & 30.94 & 70.09 & 76.68 & 69.98 & 64.82 & 66.92 & 38.14 & 39.60 & 60.89   \\
& PASER(LDA\_TFIDF) & 18.74 & 34.98 & 69.85 & 76.31 & 69.42 & 64.37 & 66.48 & 37.82 & 39.40 & 60.52   \\
 & PASER(KMeans\_TFIDF) & 15.49 & 29.76 & \textbf{70.92} & 77.03 & 70.51 & 65.28 & 67.38 & 38.47 & \textbf{40.30} & 61.41    \\
 & PASER(Spectral\_MTEB) & 14.81 & 28.49 & 70.54 & 77.42 & 71.12 & 65.75 & 67.82 & 38.74 & 39.90 & 61.61    \\
 & PASER(Spectral\_BERT) & 14.47 & 27.86 & 70.66 & 77.65 & 71.45 & 66.01 & 68.06 & 38.89 & 40.00 & 61.82  \\
\rowcolor[gray]{0.9} & PASER & \textbf{14.13} & \textbf{27.22} & 70.77 & \textbf{77.87} & \textbf{71.78} & \textbf{66.26} & \textbf{68.30} & \textbf{39.04} & 40.10 & \textbf{62.02}  \\
\cline{1-12}
\multirow{7}{*}{\begin{tabular}[l]{@{}l@{}}SparseGPT\\ sparsity=50\%\end{tabular}} 
& w/o Training & 19.26 & 36.41 &71.22  &75.60 & 62.85  & 66.06 & 69.11 &36.86  & 37.80 & 59.93 \\
& PASER(NMF\_TFIDF) & 15.97 & 28.13 & 72.63 & 76.94 & 64.37 & 67.18 & 70.39 & 37.54 & 38.60 & 61.09 \\
& PASER(LDA\_TFIDF)  & 15.41 & 27.09 & 73.12 & 77.31 & 64.93 & 67.63 & 70.92 & 37.86 & 38.80 & 61.51  \\
 & PASER(KMeans\_TFIDF) & 14.72 & 25.91 & 73.61 & 77.66 & 65.46 & 68.09 & 71.48 & 38.19 & 39.00 & 61.93 \\
 & PASER(Spectral\_MTEB) & 14.03 & 24.84 & 74.16 & 78.01 & 66.02 & 68.54 & 71.98 & 38.44 & 39.20 & 62.34 \\
 & PASER(Spectral\_BERT) & 13.68 & 24.31 & 74.48 & 78.21 & 66.32 & 68.79 & 72.28 & \textbf{38.75} & 39.30 & 62.59   \\
\rowcolor[gray]{0.9} & PASER & \textbf{13.33} & \textbf{23.77} & \textbf{74.79} & \textbf{78.38} & \textbf{66.62} & \textbf{69.03} & \textbf{72.57} & 38.70 & \textbf{39.40} & \textbf{62.78}   \\
\hline
\bottomrule
\end{tabular}}
\label{tab: different clustering}
\end{table*}

\section{Case Study for Recovery Instruction Clustering}
\label{appendix: case study clustering}
To illustrate the effectiveness of our Semantic-Structural Recovery Instruction Clustering (S$^2$RIC) approach for grouping samples focusing on similar capabilities together, we conduct a case study of clustered instruction samples from the Alpaca dataset. Specifically, we provide representative samples from several obtained clusters as follows.

\subsection{Cluster 1: Basic Factual Knowledge and Information Retrieval}
\begin{itemize}[leftmargin=*]
    \item \textbf{Instruction}: ``Find the five largest cities in France.''
    \item \textbf{Instruction}: ``What is the capital of France?''
    \item \textbf{Instruction}: ``Find the population density of United States.''
\end{itemize}
These instructions primarily test the model's ability to recall basic facts and information, corresponding to general knowledge capabilities.

\subsection{Cluster 2: Language Understanding and Translation}
\begin{itemize}[leftmargin=*]
    \item \textbf{Instruction}: ``Translate the word 'giraffe' to French.''
    \item \textbf{Instruction}: ``Pick the correct Spanish translation of “Hello”.''
    \item \textbf{Instruction}: ``Difference in meaning between "done freely" and "freely done¨? For instance, is there any difference in meaning between these two sentences?'''
\end{itemize}
This cluster focuses on language-related tasks, including translation, idiomatic expressions, and grammatical analysis.

\subsection{Cluster 3: Logical Reasoning and Problem Solving}
\begin{itemize}[leftmargin=*]
    \item \textbf{Instruction}: ``A friend shares the following text with you and asks for your opinion: 'Purple-eyed individuals have a stronger psychic connection to the cosmos and have more chances to predict the future.' Analyze the statements and point out logical fallacies or unsupported claims.''
    \item \textbf{Instruction}: ``Explain how to solve a Sudoku puzzle in three steps.''
    \item \textbf{Instruction}: ``Answer this math question: What is the value of 3 to the power of 5?''
\end{itemize}
These instructions test the model's ability to perform mathematical calculations, logical deductions, and pattern recognition.

\subsection{Cluster 4: Creative Writing and Text Generation}
\begin{itemize}[leftmargin=*]
    \item \textbf{Instruction}: ``Write a microblog post about a recent experience you had.''
    \item \textbf{Instruction}: ``Compose a haiku about the wonders of technology.''
    \item \textbf{Instruction}: ``Create an illustration of the inside of a castle.''
\end{itemize}
This cluster groups tasks that require creative text generation, showcasing the model's ability to produce original content across various formats and topics.

\subsection{Cluster 5: Summarization and Information Extraction}
\begin{itemize}[leftmargin=*]
    \item \textbf{Instruction}: ``Summarize the techniques used for summarizing text.''
    \item \textbf{Instruction}: ``Extract the main argument from the passage.''
    \item \textbf{Instruction}: ``Provide a brief summary of the article "A Brief History of the Automobile Industry.''
\end{itemize}
These instructions focus on the model's capability to condense information and identify key points from longer texts. 

%This study demonstrates S$^2$RIC can effectively group instructions that target similar LLM capabilities, facilitating more targeted and efficient recovery post-training. By grouping instructions for similar capabilities together, our proposed semantic-structural recovery instruction clustering allows PASER to allocate recovery efforts more effectively, focusing on clusters where the model's capabilities has degraded the most after the pruning. This targeted approach ensures a more balanced and efficient recovery of the LLM's diverse capabilities.
This study demonstrates that S$^2$RIC effectively groups instructions targeting similar LLM capabilities, enabling PASER to allocate recovery efforts, i.e., data budget, more strategically. By focusing intensively on clusters where capabilities have degraded most after the pruning, while maintaining awareness of all capability clusters, PASER achieves both targeted and balanced recovery. This dual approach ensures efficient and comprehensive restoration of the model's diverse functionalities, optimizing the recovery process for pruned LLMs.



\section{Case Study for Negative Transfer Mitigation}
\label{appendix: case study mitigation}
To illustrate the negative transfer mitigation process in PASER, we present the following case study, including both initial concept consistency graph construction and new sample evaluation.
\subsection{Constructing Initial Concept Consistency Graph}
We begin with three accepted instruction-output pairs to construct our initial Concept Consistency Graph (CCG):
\begin{enumerate}[leftmargin=*]
    \item \textbf{Instruction:} "Explain the basic principles of quantum computing."
    
    \textbf{Output:} "Quantum computing is based on the principles of quantum mechanics. It uses quantum bits or qubits, which can exist in multiple states simultaneously due to superposition. Quantum entanglement allows qubits to be correlated with each other. These properties enable quantum computers to perform certain calculations much faster than classical computers for specific problems."
    
    \textbf{Concepts:} \{quantum computing, qubit, superposition, entanglement\}

    \item \textbf{Instruction:} "Describe the main components of a classical computer."
    
    \textbf{Output:} "A classical computer typically consists of several key components: the Central Processing Unit (CPU) which performs calculations and controls other components; Random Access Memory (RAM) for temporary data storage; a hard drive for long-term data storage; and various input/output devices. These components work together to process and store information using binary logic."
    
    \textbf{Concepts:} \{CPU, RAM, hard drive, binary logic\}

    \item \textbf{Instruction:} "What are the fundamental concepts in deep learning?"
    
    \textbf{Output:} "Deep learning is a subset of machine learning based on artificial neural networks. Key concepts include: neural networks with multiple layers (deep neural networks), which can learn hierarchical representations of data; backpropagation, an algorithm for training these networks by adjusting weights; and various optimization techniques to improve learning. Deep learning has shown remarkable success in tasks such as image and speech recognition."
    
    \textbf{Concepts:} \{deep learning, neural network, backpropagation, optimization\}
\end{enumerate}
Based on these samples, we construct an initial Concept Consistency Graph (CCG):
\begin{equation*}
\begin{aligned}
V = \{&\text{quantum computing, qubit, superposition, entanglement,} \\
      &\text{CPU, RAM, hard drive, binary logic,} \\
      &\text{deep learning, neural network, backpropagation, optimization}\}
\end{aligned}
\end{equation*}
The edges $E$ in the graph represent the co-occurrence of concepts within each instruction-output pair.
\begin{equation*}
\begin{aligned}
E = \{&(\text{quantum computing, qubit}), (\text{quantum computing, superposition}), \\
      &(\text{quantum computing, entanglement}), (\text{qubit, superposition}), \\
      &(\text{qubit, entanglement}), (\text{superposition, entanglement}), \\
      &(\text{CPU, RAM}), (\text{CPU, hard drive}), (\text{CPU, binary logic}), \\
      &(\text{RAM, hard drive}), (\text{RAM, binary logic}), (\text{hard drive, binary logic}), \\
      &(\text{deep learning, neural network}), (\text{deep learning, backpropagation}), \\
      &(\text{deep learning, optimization}), (\text{neural network, backpropagation}), \\
      &(\text{neural network, optimization}), (\text{backpropagation, optimization})\}
\end{aligned}
\end{equation*}
This explicit definition of E clearly shows the relationships between concepts within each domain, while highlighting the absence of edges between concepts from different domains.

\subsection{Evaluating New Samples}
We now evaluate two new samples using PASER's negative transfer mitigation process.

\subsubsection{Sample 1: Accepted}
\textbf{Instruction:} "How does increasing the number of qubits affect a quantum computer's performance?"

\textbf{Output:} "Increasing the number of qubits in a quantum computer can significantly enhance its performance and capabilities:

1. Exponential growth in computational power: Each additional qubit doubles the number of quantum states that can be represented, leading to an exponential increase in computational power.

2. Handling more complex problems: More qubits allow quantum computers to tackle larger and more complex problems that are intractable for classical computers.

3. Improved error correction: A larger number of qubits can be used to implement more robust quantum error correction codes, improving the overall stability and reliability of quantum computations.

4. Enhanced quantum parallelism: More qubits enable greater parallelism in quantum algorithms, potentially leading to faster solution times for certain problems.

5. Challenges in scalability: However, increasing the number of qubits also introduces challenges in maintaining coherence and reducing error rates, which are active areas of research in quantum computing.

In summary, while increasing the number of qubits can dramatically improve a quantum computer's capabilities, it also comes with technical challenges that researchers are working to overcome."

\textbf{Concepts:} \{qubit, quantum computer, computational power, quantum states, error correction, quantum parallelism, coherence\}

Applying PASER's negative transfer mitigation process:
\begin{enumerate}[leftmargin=*]
    \item \textbf{Concept Extraction:} 
    $C(x, y) = \{$qubit, quantum computer, computational power, quantum states, error correction, quantum parallelism, coherence$\}$

    \item \textbf{Consistency Check:}
    For each pair $(v_i, v_j) \in C(x, y)$:
    \begin{itemize}
        \item (qubit, quantum computer): both $\in V$ and $(v_i, v_j) \in E$
        \item (qubit, computational power): qubit $\in V$, computational power $\notin V$
        \item (qubit, quantum states): qubit $\in V$, quantum states $\notin V$
        \item Other pairs: at least one concept $\notin V$
    \end{itemize}

    \item \textbf{Decision:}
    IsConsistent(x, y) = True, because for all pairs, either $(v_i, v_j) \in E$ or $\{v_i, v_j\} \not\subset V$
\end{enumerate}

PASER accepts this sample as it is consistent with the existing CCG and introduces relevant new concepts without conflicts.

\subsubsection{Sample 2: Rejected}
\textbf{Instruction:} "Explain how quantum computing can be used to significantly speed up deep learning algorithms."

\textbf{Output:} "Quantum computing has the potential to speed up certain aspects of deep learning algorithms. Quantum algorithms could potentially optimize the training process of neural networks, especially in handling large-dimensional data. However, it's important to note that the exact advantages and applications are still being researched, and classical computers remain more practical for most current deep learning tasks."

\textbf{Concepts:} \{quantum computing, deep learning, neural network, speedup\}

Applying PASER's negative transfer mitigation process:
\begin{enumerate}[leftmargin=*]
    \item \textbf{Concept Extraction:} 
    $C(x, y) = \{$quantum computing, deep learning, neural network, speedup$\}$

    \item \textbf{Consistency Check:}
    For each pair $(v_i, v_j) \in C(x, y)$:
    \begin{itemize}
        \item (quantum computing, deep learning): both $\in V$, but $(v_i, v_j) \notin E$
        \item (quantum computing, neural network): both $\in V$, but $(v_i, v_j) \notin E$
        \item (deep learning, neural network): both $\in V$ and $(v_i, v_j) \in E$
        \item (speedup, any other concept): speedup $\notin V$
    \end{itemize}

    \item \textbf{Decision:}
    IsConsistent(x, y) = False, because the pairs (quantum computing, deep learning) and (quantum computing, neural network) have both concepts in V, but these edges do not exist in E. This introduces new relationships between existing concepts that are not present in the current CCG.
\end{enumerate}
PASER rejects this sample because it introduces direct relationships between quantum computing and deep learning/neural networks, which were not present in the initial CCG. While these concepts existed separately in the CCG, their combination in this context could lead to potential misunderstandings or oversimplifications about the current state and capabilities of quantum computing in machine learning.

\subsection{Conclusion}
This case study demonstrates PASER's negative transfer mitigation process in action. By accepting Sample 1, PASER allows for the introduction of new, relevant concepts that expand the concept consistency graph without introducing conflicts. By rejecting Sample 2, PASER prevents the introduction of potentially misleading relationships between existing concepts from different domains, thus mitigating the risk of negative transfer during the recovery process.

\section{Implementation Details}
\label{appendix: implementation}
Most of experiments are conducted on the server with 8 $\times$ RTX 6000 Ada GPUs. During the recovery post-training phase, we take the the low-rank approximation, LoRA~\citep{hulora}, to improve the efficiency. The corresponding hyperparameters are set as following: rank=8, batch size=64, epochs=2, learning rate = 1e-4 (Alpaca series experiments), 5e-5 (LaMini series experiments). As for the structured pruning, we set the pruning ratio as 25\% for LLaMA2-7B/LLaMA3-8B/Baichuan2-7B and 50\% for LLaMA2-13B/LLaMA2-70B/Baichuan-13B models. For the other two kinds of pruning schemes, we follow the previous work~\citep{sparsegpt2023elias}; Specifically, we adopt the 2:4 semi-structured sparsity patterns and implement 50\% unstructured weight sparsity. Except the experiments for recovery post-training efficiency analysis, we set the ratio of recovery data budget $B$ to original dataset size $N$ as 20\% for Alpaca and 4\% for LaMini. As for the implementation of concept extraction in Section~\ref{sec:negative}, we use the open-source library \texttt{rake-nltk}~\footnote{https://pypi.org/project/rake-nltk/}. To ensure statistical robustness, all the results reported in this paper are the averages of five runs with different seeds. Statistical significance is also assessed using two-tailed independent t-tests, with results considered significant when $p < 0.01$. For facilitating the reproduction of our work, we provide the code in the supplementary materials, also seen in anonymous github \url{https://github.com/BokwaiHo/PASER}.

\section{Limitation Analysis}
While PASER demonstrates significant improvements in recovery performance and efficiency for pruned large language models, there are several limitations to consider:
\begin{itemize}[leftmargin=*]
    \item \textbf{Computational overhead:} Although PASER reduces the recovery training time, the initial clustering and data selection process introduces some computational overhead. For very large instruction tuning datasets, this overhead may become non-trivial.
    \item \textbf{Dependence on initial pruning quality:} The effectiveness of PASER may vary depending on the quality and method of the initial model pruning. Poorly pruned models might not benefit as much from the targeted recovery approach.
    \item \textbf{Potential bias in capability recovery:} While PASER aims for balanced capability recovery, there might still be some bias towards certain capabilities based on the initial clustering results and the composition of the instruction tuning dataset.
    \item \textbf{Scalability to extremely large models:} The paper primarily demonstrates results on models up to 70B parameters. The scalability and effectiveness of PASER on even larger models (e.g., 100B+ parameters) need further investigation.
    \item \textbf{Long-term Stability:} The long-term stability of models recovered using PASER, especially under continued fine-tuning or adaptation, has not been thoroughly examined in this work.
\end{itemize}

\textbf{Limitations of Concept Consistency Graph.} In addition to the above analysis, we further discuss the concept consistency graph's potential limitations. Indeed, while CCG helps mitigate negative transfer, we acknowledge there are scenarios where semantic conflicts might not be fully captured:
\begin{itemize}[leftmargin=*]
  \item \textbf{Cross-domain Knowledge Integration:} When instructions involve integrating knowledge from multiple distinct domains, CCG might miss subtle conflicts in their interactions. For example, when concepts from physics and biology are combined in interdisciplinary problems, their complex relationships and potential incompatibilities may not be fully reflected in simple co-occurrence patterns.
  \item \textbf{Context-dependent Semantics:} The same concept pairs might have different relationships depending on context. For instance, terms like "positive" and "negative" could be contradictory in sentiment analysis but complementary in mathematics, making it challenging for CCG to maintain consistent concept relationships across different contexts.
  \item \textbf{Temporal or Version-specific Conflicts:} In rapidly evolving domains like technology or scientific research, concept relationships might change over time. An instruction about "state-of-the-art performance" or "current best practices" could contain outdated or conflicting information that is not immediately apparent from concept co-occurrence analysis.
  \item \textbf{Nuanced Conceptual Dependencies:} When instructions involve subtle logical dependencies or conditional relationships between concepts, the binary edge representation in CCG might not fully capture these complex interactions. This is particularly evident in reasoning tasks where conclusions depend on specific combinations of conditions.
\end{itemize}
Our results acknowledge these inherent limitations while demonstrating CCG's overall effectiveness in practical applications.


\section{Ethics Statement}
The development and deployment of technologies like PASER for efficient recovery of pruned large language models necessitates careful consideration of ethical implications. While PASER contributes to reducing environmental impact and potentially democratizing AI access by lowering computational requirements, it also raises concerns about potential misuse, bias amplification, and privacy. It's crucial to remain vigilant about these risks, implement robust safeguards, and maintain transparency in the recovery process. Continuous monitoring for fairness and bias in model outputs is essential, as is responsible deployment with appropriate human oversight, especially in high-stakes applications. As the field evolves, ongoing ethical assessment and dialogue with stakeholders are vital to ensure that advancements in large language model efficiency contribute positively to society while minimizing potential harm. Ultimately, the goal should be to harness the benefits of improved model recovery techniques like PASER while proactively addressing the complex ethical challenges they present.