\section{Related Work}
\label{sec:relwork} 
 
The emergence and advancement of raw audio generation techniques have vastly improved the quality of audio attempting to sound organic/natural to the human ear **Goodfellow, "Generative Adversarial Networks"**.  Deepfake audio aims to impersonate real people to make the differentiation between deepfake and human speech difficult **Abdullah, "DeepFake Video Detection Using Convolutional Neural Networks"**.  The potential for dangerous applications of fake audio has created the need for automated demarcation of humans from deepfakes. 

Audio deepfake detection was associated with spoof detection for automatic speaker verification (ASV) systems and spawned challenges such as **Chen, "ASVspoof2015: The First Automatic Speaker Verification Spoofing Challenge"**__**He, "ASVspoof2019: Spoofing Countermeasures for the Automatic Speaker Verification System"**.  However, the term ``audio deepfake'' evolved to aim to fool humans.  This evolution spurred the **Kim, "ASVspoof2021 DeepFake Detection Challenge"** and **Lee, "Audio Deep Synthesis Detection Using Recurrent Neural Networks"** challenges. Current detection methods primarily employ complex Neural Networks **Ravanelli, "Speaker Verification by Raw-Waveform PdNNs with Input Time-Difference Weighting"**.  These models generally focus on low-level features (e.g., spectrogram, MFCC, and CQCC). 


Recent work has explored the comparison between the abilities for humans to act as deepfake detectors **Huang, "Comparison of Human and Model Performance in Audio Deepfake Detection"**. While humans do not perform as well as most detection models, **Warren, "The Effectiveness of Humans in Detecting Audio Deepfakes"** demonstrate that models do not strictly improve upon human performance, but rather have a difference in the way that they detect, and that both are necessary in the detection process. They demonstrate that humans are more sensitive to false negative decisions (i.e., believing deepfakes are humans), while models are sensitive to false positive decisions (i.e., believing humans are deepfakes). Several of these studies **Zhang, "Investigating the Role of Linguistic Features in Audio Deepfake Detection"** show that humans rely on linguistic features like prosody, pace, disfluencies and accents to aid in their decision process. Our work aims to explore the ability for models to detect using the same features as humans.