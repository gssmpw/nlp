\newpage

\appendix

% 5d: add a Table (ref TaiLin)
% The features of a given position are 18 channels , with the first 16 channels representing the board configurations of the past eight moves for both black and white pieces, and the remaining 2 channels indicating whether it is black's turn or white's turn.
% When training, we randomly choose 7 positions with the same player to play from each rank to form a batch, and the model is optimized by stochastic gradient descent using the loss function in equation \ref{equ:loss_function} until convergence.

% 5d: features/network similar to alphazero, for strength estimator, we use ? fully connected layer (?channeled) output a scalar??
% rockmanray: one table of setting for hyperparameters
% 5d: training $n=11$, $m=7$, r_inf $n=12$, $m=7$ => table
% 5d: training steps, lr scheduel, machine (CPU/GPU/memory), SGD, time
% 5d: we list other hypaper.. in Table XX
% Appendix: Features and Training Methodology
% Features Representation

\section{Detailed Training Settings for the Strength Estimator}
\label{app:traning_details}

The feature design in the strength estimator of Go is similar to AlphaZero \citep{silver_general_2018}.
Specifically, we use 18 channels to represent a board position, where the first 16 channels are the board configurations from the past eight moves for both black and white stones. 
The remaining two channels are binary indicators of the color of the next player, i.e., one channel for Black and White.
For chess, the feature design also follows the same approach as AlphaZero, which includes 119 input channels.
%Specifically, we use 119 channels to represent a board position, where the first 112 channels capture the board configurations from the past eight moves for both players.
%Each move is encoded using 14 channels: 12 channels represent the positions of Player 1's and Player 2's pieces (6 channels per player), and 2 additional channels indicate repetitions of the current board position. 
%The next four channels represent the castling rights of both players. 
%Another channel is used as a binary indicator of the current player’s color (0 for white, 1 for black). 
%Finally, one channel encodes the state of the 50-move rule, and another records the total move count in the game. 
%This design captures both the historical and current state of the game, providing a comprehensive input for evaluation.
% During training, we implement a sampling batch strategy designed to ensure consistency in player perspective and diversity in positions. 
During training, for each rank, we randomly select seven state-action pairs. 
% Consequently, the size of positions in one batch includes seven action pairs for either 11 ranks or 12 ranks if we include the infinity rank, resulting in 77 or 84 positions, respectively. 
We also perform data augmentation to further enhance the diversity of the training data.
The network is optimized using stochastic gradient descent (SGD), with the loss function specified in Equation \ref{equ:loss_function}. 
It is important to note that when training \texttt{SE\textsubscript{$\infty$}}, the policy and value loss for state-action pairs of $r_\infty$ are not calculated, since these heads should only consider actual human players' actions.
The learning rate is initially set at 0.01 and is halved after 100,000 training steps. 
The entire training process encompasses 130,000 steps, consuming around 242 GPU hours for Go and 69 GPU hours for chess on an NVIDIA RTX A5000 graphics card. 
Other hyperparameters are listed in Table \ref{tbl:training_details}.

\begin{table}[h]
    \caption{Hyperparameters for training strength estimators.}
    \vskip 0.1in
    \centering
    \begin{tabular}{lcc}
        \toprule
        \textbf{Parameter} & Go & Chess \\
        \midrule
        Number of Blocks & 20 & 20 \\
        Input Channel & 18 & 119\\
        Hidden Channel & 256 & 256 \\
        Learning Rate & 0.01 to 0.005 & 0.01 to 0.005 \\
        Training Steps & 130,000 & 130,000  \\
        Optimizer & SGD & SGD \\
        \midrule
        Main Memory & \multicolumn{2}{c}{384GB} \\
        Central Processing Unit (CPU) & \multicolumn{2}{c}{Intel Xeon Silver 4216 (2.1 GHz)} \\        
        Graphical Processing Unit (GPU) & \multicolumn{2}{c}{NVIDIA RTX A5000} \\
        GPU Hours & 242 & 69\\
        \bottomrule
    \end{tabular}
    \label{tbl:training_details}
\end{table}

%When using fewer ranks, as discussed in \ref{subsec:exp_further_analysis}, a batch consists of 7 (action pairs) * 2 or 3 (ranks).
%In total, we select 32 batches in one training step. Therefore, each step may contain 7 (action pairs) * n (ranks used for training) * 32, resulting in 224 * n action pairs.

%Handling Out-of-Distribution Moves: To address out-of-distribution moves, we introduce an "infinity rank" to distinguish the strength score of unusual moves from those typically made by human players. Notably, the action pairs with an infinity rank are used solely for training the strength estimator head. Since these moves are not representative of those made by human players, they are excluded from the loss calculations for the policy and value heads.

% Regularization: We apply techniques such as dropout and weight decay to prevent overfitting.

%Optimization:

% \begin{table}[h]
%     \caption{Implementation Details}
%     \vskip 0.1in
%     \centering
%     % \resizebox{0.7\columnwidth}{!}{%
%     \begin{tabular}{lll}
%         \toprule
        
%         \midrule
%         \multirow{4}{*}{\textbf{Network Architecture}} & Number of blocks & 6 \\
%         & Hidden channels & 256 \\
%         & Transformer heads & 4 \\
%         & Transformer MLP ratio & 2 \\
%         \midrule
        
%         \multirow{3}{*}{\textbf{Training Setting}} & Learning rate & 0.02 to 0.005 \\
%         & Batch size & 1,024 \\
%         & Training steps & 100,000 \\
%         \midrule
%         \multirow{2}{*}{\textbf{Evaluation}} & Thinking time (one move) & 1 second \\
%         & Number of Games & 1500\\
%         \bottomrule
%     \end{tabular}%
%     % }
%     \label{tab:info_hybrid9x9}
% \end{table}


\section{In-depth Analysis for Strength Estimator in Go}
\label{sec:further_analysis}

We conduct in-depth analyses for strength estimators in Go. 
First, we present detailed insights into predicting ranks from games, as detailed in Subsection \ref{subsec:details_of_prediction}.
Second, we demonstrate the outcomes of strength prediction using fewer moves in a single game, discussed in Subsection \ref{subsec:preidcion_with_fewer_moves}. 
Finally, we explore predictions based solely on the first 50 actions or the last 50 actions in games, which are elaborated in Subsection \ref{subsec:first_50_moves} and Subsection \ref{subsec:last_50_moves}, respectively.

% 5d: add picture & text
% organization 
% We perform comprehensive analysis Strength Estimator in three scenarios: (a) predicting by game, (b) predicting by one position, (c) predicting by first 50 position, and (d) predicting by last 50 position
% Predicting the Strength from Games (4*)
% one game one positions
% first and last 50 actions

\subsection{Predicting Ranks from Games}
\label{subsec:details_of_prediction}
Figure \ref{each_rank_prediction} shows the accuracy of rank predictions for different networks. 
We observe that in Figure \ref{fig:sl_vote} and Figure \ref{fig:sl_sum}, \texttt{SL\textsubscript{vote}} and \texttt{SL\textsubscript{sum}} can only distinguish on some ranks, such as 3-5 kyu.
This is because these models do not contain sufficient information to differentiate all ranks based on a single state-action during training \citep{moudřík_determining_2016}.
In Figure \ref{fig:sl_vote_+-1} and Figure \ref{fig:sl_sum_+-1}, even though we incorporated a prediction tolerance for these two methods, they still cannot perfectly distinguish all ranks, even after 100 games.
In Figure \ref{fig:se} and Figure \ref{fig:se_infty}, although our models \texttt{SE} and \texttt{SE\textsubscript{$\infty$}} cannot perfectly predict all ranks without incorporating a prediction tolerance, they still achieve high performance across all ranks.
In Figure \ref{fig:se_+-1} and Figure \ref{fig:se_infty_+-1}, when we allow a prediction tolerance, we achieve 100\% accuracy across all ranks. 
This result further indicates that our model can differentiate the strength relationship across all ranks.

\begin{figure}[ht]
\centering
    \subfloat[\texttt{SL\textsubscript{vote}}] {\includegraphics[width=0.49\linewidth]  {figures/rank_net_max_num_revision.pdf}
       \label{fig:sl_vote}
    }
    \subfloat[\texttt{SL\textsubscript{sum}}]{
         \includegraphics[width=0.49\linewidth]{figures/rank_net_max_prob_revision.pdf}
      \label{fig:sl_sum}
    }
    \\
     \subfloat[\texttt{SL\textsubscript{vote}}$\pm1$]{
     \includegraphics[width=0.49\linewidth]{figures/rank_net_max_num_+-1_revision.pdf}
     \label{fig:sl_vote_+-1}
    }
    \subfloat[\texttt{SL\textsubscript{sum}}$\pm1$]{
        
         \includegraphics[width=0.49\linewidth]{figures/rank_net_max_prob_+-1_revision.pdf}
         \label{fig:sl_sum_+-1}
    }\\
    \subfloat[\texttt{SE}]{
   
     \includegraphics[width=0.49\linewidth]{figures/11_7_32_revision.pdf}
     \label{fig:se}
    }
    \subfloat[\texttt{SE\textsubscript{$\infty$}}]{
         \includegraphics[width=0.49\linewidth]{figures/our_method_among_all_ranks_error_revision.pdf}
         \label{fig:se_infty}
    }
    \\
    \subfloat[\texttt{SE}$\pm1$]{\includegraphics[width=0.49\linewidth]{figures/11_7_32_+-1_revision.pdf}
     \label{fig:se_+-1}
    }
    \subfloat[\texttt{SE\textsubscript{$\infty$}}$\pm1$]{
        
         \includegraphics[width=0.49\linewidth]{figures/12_7_32_use_non_people_+-1_revision.pdf}
         \label{fig:se_infty_+-1}
    } 
    
    \caption{Accuracy of rank prediction for different networks in Go.}
    \label{each_rank_prediction}
\end{figure}

% rockmanray: move and action?
\subsection{Predicting Ranks from Game Positions}
\label{subsec:preidcion_with_fewer_moves}

We are also interested in whether we can predict the rank using only game positions.
Specifically, only one game position can be chosen for each game instead of all actions when predicting the ranks, shown in Figure \ref{fig:one_action_per_game}. 
% 5d: x-axis => Game Positions
According to Figures \ref{fig:one_action_sl_vote} and \ref{fig:one_action_sl_sum}, both \texttt{SL\textsubscript{vote}} and \texttt{SL\textsubscript{sum}} show similar performance as in the situation of using all actions. 
% rockmanray: look at
% 5d: what?
In our method, Figures \ref{fig:one_action_se} and \ref{fig:one_action_se_infty} demonstrate that across 20 games, the accuracy decreases from 80\%, when predictions are based on all actions, to approximately 60\% when using just one action.
This is intuitive because using the information from the entire game would help capture the player's strength. 
However, achieving 60\% accuracy with unrelated actions among 20 games indicates that our model can still predict accurate ranks based on one action of different games.

%In future research, we may potentially achieve higher accuracy if we can identify and use the most critical action-pair in each game.

\begin{figure}[ht]
    \centering
    \subfloat[\texttt{SL\textsubscript{vote}}]{
    \includegraphics[width=0.49\linewidth]{figures/rank_net_max_num_one_pos_per_game_revision.pdf}
        \label{fig:one_action_sl_vote}
    }
    \subfloat[\texttt{SL\textsubscript{sum}}]{
        \includegraphics[width=0.49\linewidth]{figures/rank_net_max_prob_one_pos_per_game_revision.pdf}
        \label{fig:one_action_sl_sum}
    }
    \\
    \subfloat[\texttt{SE}]{
        \includegraphics[width=0.49\linewidth]{figures/11_7_32_one_pos_per_game_revision.pdf}
        \label{fig:one_action_se}
    }
    \subfloat[\texttt{SE\textsubscript{$\infty$}}]{
        \includegraphics[width=0.49\linewidth]{figures/12_7_32_one_pos_per_game_revision.pdf}
        \label{fig:one_action_se_infty}
    }
    \caption{Accuracy of rank prediction for different networks in the game positions of Go.}
    \label{fig:one_action_per_game}
\end{figure}

\subsection{Predicting Ranks from the First 50 Actions in the Game}
\label{subsec:first_50_moves}

We are interested in evaluating performance when using only the first 50 actions of a game, known as the \textit{fuseki} stage in Go. 
Figures \ref{fig:sl_vote_first_50} and \ref{fig:sl_sum_first_50} indicate that \texttt{SL\textsubscript{vote}} and \texttt{SL\textsubscript{sum}}, utilizing these initial actions, achieve similar performance to predictions made using all actions in the game. 
Figures \ref{fig:se_first_50} and \ref{fig:se_infty_first_50} present the prediction results by our methods when limited to the first 50 actions. 
Clearly, the overall accuracy has declined. 
Notably, the accuracy for kyu players shows significant downward trends. 
This is mainly because the actions in the \textit{fuseki} stage at these ranks are similar, thus complicating accurate predictions.
This suggests that enhancing performance during the \textit{fuseki} could be crucial for human players aiming to progress from kyu to dan rank.

\subsection{Predicting Ranks from the Last 50 Actions in the Game}
\label{subsec:last_50_moves}

Similarly, we examine the performance when using only the last 50 actions of the game, referred to as the \textit{yose} stage in Go. 
Figures \ref{fig:sl_vote_last_50} and \ref{fig:sl_sum_last_50} show that \texttt{SL\textsubscript{vote}} and \texttt{SL\textsubscript{sum}}, employing these final actions, maintain similar performance to predictions based on all actions in the game. 
In our method, Figures \ref{fig:se_last_50} and \ref{fig:se_infty_last_50} display the prediction results using only the last 50 actions. 
As before, the overall accuracy has declined. 
However, the accuracy for 9 dan players between predictions made using the entire game and just the last 50 actions does not differ significantly.  
This is likely because the \textit{yose} stage involves complex calculations and judgments, areas where top players excel.
Furthermore, in most games, especially those between the highest-skilled players, the outcome is often determined before the \textit{yose} stage. 
This leads to less practice and proficiency in this phase among players of lower ranks.
Additionally, we observe a significant drop in accuracy for 8 dan players when predictions are based solely on \textit{yose} stage. 
This could be because some 8 dan players have comparable \textit{yose} skills to those of 9 dan players, leading to some misclassifications of 8 dan players as 9 dan.

\begin{figure}[ht]
\centering
   \subfloat[\texttt{SL\textsubscript{vote}} with first 50 actions]
   {
         \includegraphics[width=0.49\linewidth]{figures/rank_net_max_num_first_50_moves_revision.pdf}
         \label{fig:sl_vote_first_50}
    }
    \subfloat[\texttt{SL\textsubscript{sum}} with first 50 actions]{
         \includegraphics[width=0.49\linewidth]{figures/rank_net_max_prob_first_50_moves_revision.pdf}
         \label{fig:sl_sum_first_50}
    }
    \\
    \subfloat[\texttt{SE} first 50 actions]{
         \includegraphics[width=0.49\linewidth]{figures/11_7_32_first_50_moves_revision.pdf}
         \label{fig:se_first_50}
    }
   \subfloat[\texttt{SE\textsubscript{$\infty$}} with first 50 actions]{    
         \includegraphics[width=0.49\linewidth]{figures/12_7_32_use_non_people_first_50_moves_revision.pdf}
         \label{fig:se_infty_first_50}
    }    
    \\
    \subfloat[\texttt{SL\textsubscript{vote}} with last 50 actions]{
         \includegraphics[width=0.49\linewidth]{figures/rank_net_max_num_last_50_moves_revision.pdf}
         \label{fig:sl_vote_last_50}
    }
    \subfloat[\texttt{SL\textsubscript{sum}} with last 50 actions]{
        
         \includegraphics[width=0.49\linewidth]{figures/rank_net_max_prob_last_50_moves_revision.pdf}
         \label{fig:sl_sum_last_50}
    }
    \\
     \subfloat[\texttt{SE} last 50 actions]{
         \includegraphics[width=0.49\linewidth]{figures/11_7_32_last_50_moves_revision.pdf}
         \label{fig:se_last_50}
    }
    \subfloat[\texttt{SE\textsubscript{$\infty$}} with last 50 actions]{
        \includegraphics[width=0.49\linewidth]{figures/12_7_32_use_non_people_last_50_moves_revision.pdf}
        \label{fig:se_infty_last_50}
    } 
    \caption{Accuracy of rank prediction for different networks using only the first or the last 50 actions of the game in Go.} 
\end{figure}

%\input{tables/z}

% 5d: maybe remove?
% 5d: skip for now
% \section{correlation between score and value}
% \begin{figure}[h]
%     \centering
%     \subfloat[$move_{50}$]{
%         \includegraphics[width=0.33\linewidth]{figures/scatter_plot_log_mcts_3D_policy_0.95_value_score_move_50.png}
%         \label{fig:3-5k_1-2k_1-9D_11_7_32_one_pos_per_game}
%     }
%     \subfloat[$move_{100}$]{
%         \includegraphics[width=0.33\linewidth]{figures/scatter_plot_log_mcts_3D_policy_0.95_value_score_move_100.png}
%         \label{fig:3-5k_1-2k_1-9D_11_7_32_one_pos_per_game}
%     }\\
%     \subfloat[$move_{150}$]{
%         \includegraphics[width=0.33\linewidth]{figures/scatter_plot_log_mcts_3D_policy_0.95_value_score_move_150.png}
%         \label{fig:3-5k_1-2k_1-9D_11_7_32_one_pos_per_game}
%     }
%     \subfloat[$move_{200}$]{
%         \includegraphics[width=0.33\linewidth]{figures/scatter_plot_log_mcts_3D_policy_0.95_value_score_move_200.png}
%         \label{fig:3-5k_1-2k_1-9D_11_7_32_one_pos_per_game}
%     }
%     \caption{One move in one game.}
% \end{figure}

% In this section, we want to discussed the correlation between the strength score and value. Since we often think that the stronger players may play the better moves in one position, so we want to find that whether the action with the higher score 
% one position per game 

%\section{Predicting the Strength from Game Positions}
% 5d: add picture & text


\begin{comment}
\section{Example}
% 5d: skip for now
% 5d: option?


\begin{figure}[h!]
    \centering
    \subfloat[$SE-MCTS$ and $MCTS$ select]{
        \includegraphics[width=0.5\linewidth]{figures/3-5kex2.png}
        \label{fig:mcts & se-mcts select}
    }
    \subfloat[$strength score of this position$]{
        \includegraphics[width=0.49\linewidth]{figures/3-5kex2_score.png}
        \label{fig:score}
    }
    
    \caption{An example from 3k player to illustrate the difference of moves chosen by SE-MCTS and MCTS.}
    \label{fig:exmaple_different_moves}
\end{figure}

Figure \ref{fig:exmaple_different_moves} is an example discussing SE-MCTS and MCTS. The image is taken from a game from 3-5k players. In this board position, O18 is an obvious move, but it's also somewhat redundant.

Figure \ref{fig:mcts & se-mcts select}: Since the upper right corner, marked with black triangles, is completely dead, White can ignore this area. Using MCTS, the move R12 (marked with an "X") is selected, which is an excellent move. It expands White's position in the lower right and attacks Black's three stones in the upper right (the stones to the lower right of the black triangles). However, 3-5k players tend to overlook the overall situation and choose the obvious move. Therefore, while MCTS would ignore the move O18 due to its low utility, our SE-MCTS can successfully mimic the moves of 3-5k players and choose O18, just like a 3-5k player would.

Figure \ref{fig:score}: Shows the policy strength scores for the top moves summing up to 0.9. It is clear that the score for O18 is lower than other moves from the beginning!


% rockmanray: This example is too complex.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/3-5kex.png}
    \caption{An example from 3k player to illustrate the difference of moves chosen by SE-MCTS and MCTS.}
    \label{fig:exmaple_different_moves2}
\end{figure}

Lastly, we use an example shown in Figure \ref{fig:exmaple_different_moves2} to illustrate the difference in moves chosen by SE-MCTS and MCTS.
This game is played by a 3-kyu player, where the last move is played at N7. 
At first glance, this move seems strong and aims to attack the group of six black stones, but the six Black stones are in fact safe from capture. 
Moreover, after a series of exchanges, White, due to the inherent weakness at P3, is not yet secure, and thus will end up being behind because they need to make a move to secure their group. 
Therefore, under normal circumstances, traditional MCTS would find it challenging and don't select the move N7. 
On this board, MCTS tends to play at R10 (in blue marks), which can simultaneously reduce the potential of black's right side and stabilize White's upper-right group. 
However, through our method, because we adjust the policy based on score, we can identify the actual move a 3-kyu player would make.
\end{comment}

\begin{comment}
\section{Proof}
% 5d: should we include this??
% 5d: option? skip for now
In this section, we clearly state the theorem and proof that we use in this paper. 
As the generalized Bradley-Terry Model allow many members in the same team, the strength of a group can be sregarded as the cooperation of positions where the last move is played by the players in the group, estimated as the product of the strength scores of these positions.
Therefore, if there are several state-action pairs  $p_{i,1}, p_{i,2}, \ldots, p_{i,n}$ for the rank $i$, we can estimate the strength of rank $i$, denoted as $\Lambda_i$, by Theorem \ref{thm:stength_average} with a proof:

\begin{theorem}
\label{thm:stength_average}
Suppose there are two ranks $i$ and $j$ that every player in $r_i$ has a higher strength than any player in group $r_j$.
The groups are also associated with strength scores $\lambda_{i,1}$, $\lambda_{i,2}$, \ldots, $\lambda_{i,n}$ and $\lambda_{j,1}$, $\lambda_{j,2}$, \ldots, $\lambda_{j,m}$ corresponding the positions in the groups respectively.
The strength scores of the groups $\Lambda_i$ and $\Lambda_j$ can be estimated by calculating the geometric means of the strength scores of the positions in its group. 
Specifically, $\Lambda_i$ = $\sqrt[n]{\prod_{k=1}^{n} \lambda_{i,k}}$, $\Lambda_j$ = $\sqrt[m]{\prod_{k=1}^{m} \lambda_{j,k}}$, and $\Lambda_i$ > $\Lambda_j$ still holds.
\end{theorem}

\begin{proof}
Suppose every strength score in rank $i$ and $j$ have $n$ state-action pairs and $m$ state-action pairs respectively.
Since for every strength score for state-action pair  $p_{i,{k_1}}$ and $p_{j,{k_2}}$, we have assumed that $\lambda_{i,{k_1}}$ > $\lambda_{j,{k_2}}$ and $\lambda_{i,{k_1}}$ are in decreasing order as $k_1$ increases in the same group. \\
\[
\prod_{k=1}^n \lambda_{i,k} >= (\lambda_{i,n})^n, 
(\lambda_{j,1})^m >=
\prod_{k=1}^m \lambda_{j,k} \\
\]
\[
\sqrt[n]{\prod_{k=1}^n \lambda_{i,k}} >=  \lambda_{i,n}, 
\lambda_{j,1} >=
\sqrt[m]{\prod_{k=1}^m \lambda_{j,k}} \\
\]
\[
\because \lambda_{i,n} > \lambda_{j,i} \\ 
\therefore \sqrt[n]{\prod_{k=1}^n \lambda_{i,k}} >= \sqrt[m]{\prod_{k=1}^m \lambda_{j,k}}
\]

\end{proof}

When the number of groups is greater than two, we can come up with a corollary below from Theorem \ref{thm:stength_average}.

% 5d: require efforts to proofread
% 5d: skip for now
\section{policy OOD for infinity rank}

\input{tables/ood.tex}
 We compared the impact of incorporating the infinity rank during training versus not including it, as shown in Table \ref{tab:comparison}. The main issue with training without the infinity rank is that the model may struggle to distinguish the strength scores of actions that seldom appear in human games (denoted as "strange actions," $a_s$) from those that human players actually make (denoted as "normal actions," $a_n$).

To assess this, we randomly sampled 2000 board positions from human game records of various skill levels. For each position, we identified strange moves by selecting moves with low policy probabilities (we ordered the moves by their policy probabilities in decreasing order and excluded those with a cumulative policy probability up to 0.995, thereby selecting the remaining moves with a cumulative policy probability below 0.005). We then compared these strange actions to the normal actions at these positions.

Using our strength estimator, we calculated the average strength score $\Bar{\lambda_{a_s}}$ for the strange moves and the strength score $\lambda_{a_n}$ for the actual human moves. We then analyzed the difference between $\Bar{\lambda_{a_s}}$ and $\lambda_{a_n}$.

The table shows that without incorporating the infinity rank, the average score difference between strange actions and normal actions was very small and inconsistent across different skill levels. This indicates that the model cannot effectively distinguish the strength scores of strange actions from those of normal actions. However, with the introduction of the infinity rank, we were able to clearly differentiate the scores of strange actions from those of normal actions. This effect is illustrated in the table, where the maximum difference is observed at the 9 dan level and the minimum at the 3-5 kyu levels, indicating that we have effectively lowered the score of strange moves to align with the level of the weakest players.

These results clearly show that using the model without the infinity rank is not feasible for adjusting strength, as it may result in the model choosing strange moves and consequently losing the game.

\end{comment}
%\input{tables/fight}
%\input{tables/fight_chess}

\clearpage
\section{Detailed Experiments For Strength Adjustment}\label{app:detailed_exp}


Table \ref{tbl:z_value} presents the value of $z$ for \texttt{SA-MCTS\textsubscript{$i$}} used in subsection \ref{subsec:exp_se_mcts}.
To ensure that each \texttt{SA-MCTS\textsubscript{$i$}} and \texttt{SE\textsubscript{$\infty$}-MCTS\textsubscript{$i$}} achieve a comparable win rate, all methods are tested using a fixed simulation count of 800.
For \texttt{SA-MCTS}, since the strength index $z$ does not directly correspond to any specific rank, we adjust $z$ for each $r_i$ to ensure that each \texttt{SA-MCTS\textsubscript{$i$}} and \texttt{SE\textsubscript{$\infty$}-MCTS\textsubscript{$i$}} achieve a comparable win rate.
As shown in Table \ref{tbl:z_value}, $z$ gradually decreases from $r_1$ to $r_{11}$, aligning with the results in the original paper, which indicate that a greater $z$ corresponds to a higher strength.

\input{tables/z.tex}

\subsection{Adjusting Strength with Different Baselines}\label{app:round_robin}
In Figure \ref{fig:win_heatmap}, the baseline program is chosen as \texttt{SE\textsubscript{$\infty$}-MCTS\textsubscript{$i$}} with $i=5$ (5 dan).
It would be interesting to examine whether the relative strength remains consistent when different baseline models are used.
To further investigate this, we conduct a round-robin tournament by selecting five ranks ($r_2$, $r_4$, $r_6$, $r_8$ and $r_{10}$) and two representative methods (\texttt{SA-MCTS} and \texttt{SE\textsubscript{$\infty$}-MCTS}, excluding \texttt{SE-MCTS} due to its ineffective strength adjustment.
Each combination involves 250 games, requiring approximately 100 GPU hours on an NVIDIA RTX A5000.
Table \ref{tbl:round_robin} summarizes the results, with the win rates in each cell representing the performance of the y-axis player against the x-axis player.
Moreover, we compute the Elo rating of each model using this table.
We initialize the rating at 1500 for each model and iteratively update the ratings to match the expected win rates with the observed pairwise outcomes.
The rightmost column of Table \ref{tbl:round_robin} presents the resulting Elo ratings.
In summary, the Elo ratings confirm that higher-ranked models consistently achieve higher ratings, demonstrating the robustness of our method across different baselines.

\input{tables/round_robin}


