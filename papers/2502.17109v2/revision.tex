\begin{abstract}
Strength estimation and adjustment are crucial in designing human-AI interactions, particularly in games where AI surpasses human players.
This paper introduces a novel strength system, including a \textit{strength estimator} (SE) and an SE-based Monte Carlo tree search, denoted as \textit{SE-MCTS}, which predicts strengths from games and offers different playing strengths with human styles.
The strength estimator calculates strength scores and predicts ranks from games without direct human interaction.
SE-MCTS utilizes the strength scores in a Monte Carlo tree search to adjust playing strength and style.
We first conduct experiments in Go, a challenging board game with a wide range of ranks.
Our strength estimator significantly achieves over 80\% accuracy in predicting ranks by observing 15 games only, whereas the previous method reached 49\% accuracy for 100 games.
For strength adjustment, SE-MCTS successfully adjusts to designated ranks while achieving a 51.33\% accuracy in aligning to human actions, outperforming a previous state-of-the-art, with only 42.56\% accuracy.
To demonstrate the generality of our strength system, we further apply SE and SE-MCTS to chess and obtain consistent results.
These results show a promising approach to strength estimation and adjustment, enhancing human-AI interactions in games.


\end{abstract}

\section{Introduction}
\label{sec:introduction}
Artificial intelligence has achieved superhuman performance in various domains in recent years, especially in games \citep{silver_general_2018, schrittwieser_mastering_2020, vinyals_grandmaster_2019, openai_dota_2019}.
These achievements have raised interests within the community in exploring AI programs for human interactions, particularly in estimating human players' strengths and offering corresponding levels to increase entertainment or improve skills \citep{demediuk_monte_2017,fan_position_2019,moon_dynamic_2020,gusmao_dynamic_2015,silva_dynamic_2015,hunicke_ai_2004}.
For example, since the advent of AlphaZero, human players have attempted to train themselves by using AI programs.
Subsequently, many researchers have explored several methods to adjust the playing strength of AlphaZero-like programs to provide appropriate difficulty levels for human players \citep{wu_strength_2019, liu_strength_2020, fujita_alphadda_2022}.

However, although these methods can provide strength adjustment, two issues have arisen.
First, while these methods can offer different strengths, human players often need to play several games or manually choose AI playing strength, consuming time to find a suitable strength for themselves.
Second, the behaviors between AI programs and human players are quite different.
% It has been observed that as the strength of AI programs is reduced, their behaviors increasingly diverge from those of human players.
This occurs because most strength adjustment methods mainly focus on adjusting AI strength by calibrating the win rate to around 50\% for specific strengths, without considering the human behaviors at those strengths.
The problem is further exacerbated when human players attempt to use AI programs to analyze games and learn from the better actions suggested by AI.
Therefore, designing AI programs that can accurately estimate a player's strength, provide corresponding playing strengths, and simultaneously offer human-like behavior is crucial for using superhuman AI in human learning.

To address this challenge, this paper proposes a novel strength system, including a \textit{strength estimator} and an SE-based MCTS, denoted as an \textit{SE-MCTS}, which can predict strengths from games and provide different playing strengths with a human-like playing style.
Specifically, we propose a strength estimator, based on the Bradley-Terry model \citep{bradley_rank_1952}, which estimates a strength score of an action at a game state, with higher scores indicating stronger actions.
The strength score can be further used to predict the strength of any given game, providing strength estimation without direct human interaction.
Next, we present a novel strength adjustment approach with human-like styles, named SE-MCTS, by incorporating the strength estimator into the Monte Carlo tree search (MCTS).
During the MCTS, the search is limited to exploring actions that closely correspond to a given targeted strength score.
We conduct experiments in Go, a challenging board game for human players with a wide range of ranks.
The results show several advantages of using our approach.
First, the strength estimator significantly achieves over 80\% accuracy in predicting ranks within 15 games, compared to the previous method only achieves 49\% accuracy even after evaluating 100 games.
Second, the experiments show that SE-MCTS can not only provide designated ranks but also achieve a playing style with 51.33\% accuracy in aligning to human players' actions, while previous state-of-the-art only obtained 42.56\% accuracy.
Finally, the strength estimator can be trained with limited rank data and still accurately predict ranks.
Furthermore, to demonstrate the generality of the proposed method, we apply SE and SE-MCTS to chess, achieving consistent results and significantly outperforming the previous state-of-the-art approach in both strength estimation and adjustment.
These results show a promising direction for enhancing human-AI interactions in games.

\section{Background}
\label{sec:background}

\subsection{Bradley-Terry Model}
\label{subsec:bg_generalized_bt}
The Bradley-Terry model \citep{bradley_rank_1952} is often used for pairwise comparisons, allowing for the estimation of outcomes between individuals based on their relative strengths.
In a group of individuals, the model calculates the probability that individual $i$ defeats individual $j$ as $P(i \succ j) = \frac{\lambda_i}{\lambda_i+\lambda_j}$, where $\lambda_i$ and $\lambda_j$ represent the positive values of individuals $i$ and $j$, respectively.
A higher $\lambda_i$ indicates a stronger individual.
In practice, $\lambda_i$ is usually defined by an exponential score function as $\lambda_i=e^{\beta_i}$, where $\beta_i$ represents the strength score of individual $i$.

The Bradley-Terry model can be further generalized to include comparison among more than two individuals \citep{huang_generalized_2006}.
Consider a group consisting of $k$ individuals, indexed from $1$ to $k$.
The probability that individual $i$ wins out over the other individuals in this group is calculated as $P(i) = \frac{\lambda_i}{\lambda_1 + \lambda_2 + \ldots + \lambda_k}$.
Furthermore, the model can be adapted for team comparisons, where each team comprises multiple individuals.
For example, assume team $a$ consists of individuals 1 and 2, team $b$ consists of individuals 2, 3, and 4, and team $c$ consists of individuals 1, 3, and 5.
Then, the probability that team $a$ win against team $b$ and $c$ is defined as $P(\text{team }a) = \frac{\lambda_1\lambda_2}{\lambda_1\lambda_2 + \lambda_2\lambda_3\lambda_4 + \lambda_1\lambda_3\lambda_5}$, where the strength of each team is determined by the product of the strengths of its individual members.
Due to its generalization and broader extension, the Bradley-Terry model has been widely used in various fields, such as games \citep{coulom_computing_2007}, sports \citep{cattelan_dynamic_2013}, and recommendation systems \citep{chen_predicting_2016}.


\subsection{Monte-Carlo Tree Search}
\label{subsec:bg_mcts}
Monte Carlo tree search (MCTS) \citep{coulom_efficient_2007,kocsis_bandit_2006} is a best-first search algorithm that has been successfully used by AlphaZero \citep{silver_general_2018} and MuZero \citep{schrittwieser_mastering_2020} to master both board games and Atari games.
In AlphaZero, each MCTS simulation begins by traversing the tree from the root node to a leaf node using the PUCT \citep{rosin_multiarmed_2011} formula:
\begin{equation}
\label{equ:PUCT}
    {a^*} = \mathop{\arg\max}_{a} \Bigg\{Q(s,a)+c\cdot P(s,a)\cdot\frac {\sqrt{\sum_{b}{N(s,b)}}}{1+N(s,a)}\Bigg\},
\end{equation}
where $Q(s,a)$ represents the estimated Q-value for the state-action pair ($s$, $a$), $N(s,a)$ is the visit counts, $P(s,a)$ is the prior heuristic value, and $c$ is a coefficient to control the exploration.
Next, the leaf node is expanded and evaluated by a two-head network, $f_\theta(s)=(p,v)$, where $p$ represents the policy distribution and $v$ denotes the win rate.
The policy distribution $p$ serves as the prior heuristic value.
The win rate $v$ is used to update the estimated Q-value of each node, from the leaf node to its ancestors up to the root node.
This process is repeated iteratively, with more MCTS simulations leading to better decision-making.
Finally, the node with the largest simulation counts is decided.


\subsection{MCTS-based Strength Adjustment}
\label{subsec:bg_mcts_sa}
Strength adjustment \citep{hunicke_ai_2004, paulsen_moderately_2010, silva_dynamic_2015, moon_dynamic_2020} is crucial in the design of human-AI interactions, especially since AlphaZero achieved superhuman performance in many games like Go, chess, and Shogi.
As MCTS is widely used in these games, various methods have been explored to adapt it for strength adjustment \citep{sephton_experimental_2015, wu_strength_2019, demediuk_monte_2017, fan_position_2019, moon_diversifying_2022}.
For instance, \citet{sephton_experimental_2015} proposes adjusting the playing strength by using a strength index $z$.
After the search, MCTS decides the node based on the proportionality of their simulation counts, with the probability of selecting node $i$ calculated as $\frac{N_i^z}{\sum_{j=1}^{n}N_j^z}$, where $N_i$ represents the simulation counts for node $i$.
A larger $z$ value indicates a tendency to select stronger actions, while a smaller $z$ favors weaker actions.
\citet{wu_strength_2019} further improves this method by introducing a threshold $R$ to filter out lower-quality actions, removing nodes $j$ where $N_j<R\times N_{max}$, where $N_{max}$ represents the node with largest simulation counts.
% 5d: strength vs. playing strength
The approach is used to adjust the playing strength of ELF OpenGo \citep{tian_elf_2019}, resulting in covering a range of 800 Elo ratings within the interval $z\in[-2,2]$.
However, both methods only change the final decision in MCTS without modifying the search tree, leaving the search trees identical for different strengths.


\subsection{Strength Estimation}
\label{subsec:bg_se}
Strength estimation is another important technique related to strength adjustment.
With accurate strength estimation, the AI can first predict a player's strength and subsequently provide an appropriate level of difficulty for human learning.
Several methods \citep{moudrik_determining_2016, liu_strength_2020, egri-nagy_derived_2020, scheible_picking_2014} have been proposed to estimate player strength in games.
For example, \citet{liu_strength_2020} proposes estimating a player's strength by using the strength index with MCTS, as described in the previous subsection, to play against human players.
Specifically, the strength index $z$ is adjusted after each game according to the game outcomes.
Their experiments show that $z$ generally converges after about 20 games.
However, this method requires human players to play against the MCTS programs with multiple games to obtain an estimation of their playing strengths.
On the other hand, \citet{moudrik_determining_2016} proposes an alternative approach that categorizes the Go players into three ranks -- strong, median, and weak -- and uses a neural network to classify player ranks based on a game position using supervised learning.
After training, given a game position, the neural network predicts ranks for each position by selecting the highest probability.
Furthermore, it can aggregate predictions across multiple positions.
Two methods are presented: (a) \textit{sum}, which sums probabilities of all positions and makes a prediction based on the highest probability; and (b) \textit{vote}, which predicts the rank of each position first and selects the most frequent rank.
However, this approach does not consider multiple actions during training and the experiment was limited to only three ranks.
\revision{
In addition, strength estimation is similar to ranking problems \citep{burges_learning_2005, xia_listwise_2008}, but it differs in a key aspect.
Ranking problems often focus on ordering items based on a single query, whereas in games, strength is assessed as overall skills across multiple positions or games.
This challenge requires aggregating rankings across various scenarios to capture a player's ability.}

% why strength estimation is important
    % education?
% introduce previous strength estimation method (rankNet & SSA)
    % paper: methods
    %Determining Player Skill in the Game of Go with Deep Neural Networks (LNTCS,volume 10071)
    %supervised learning 區分三個棋力強度
    %優:方法簡單 缺:僅運用單一盤面區分棋力有其侷限性

    %Strength Adjustment and Assessment for MCTS-Based Programs [Research Frontier]
    %dsa 透過邊下棋邊調整z值(使勝率到達50%左右)
    %缺點: 需實際與其ai對弈

    %Derived metrics for the game of Go – intrinsic network strength assessment and cheat-detection(2020International Symposium on Computing and Networking (CANDAR))

    %設立指標: 平均勝率改變(每手棋上升/下降多少勝率)來判斷棋手強弱
    %缺點:並沒有辦法實際判斷出棋力，僅能評估強弱關係

    %Exploring Game Performance in the National Basketball Association Using Player Tracking Data(PloS one, 2015)
    %給予一些指標(防守速度、肘部觸球、防守籃板、近距離觸球、近距離得分和急停跳投得分) 去衡量球員能力
    % 被cite 196次

    %Picking the Amateur’s Mind – Predicting Chess Player Strength from Game Annotations

    %1.二分法，以中間值作為分界，區分業餘&master，缺點交界處棋力難分 2.任意兩棋力計算誰強誰弱，缺點得不出實際elo僅知道相對強度


%\subsection{Related Works}
%\label{subsec:related_works}

%Strength estimation is also related to ranking problems, which typically involve sorting items into a specific order based on a given query. 
%This approach is commonly applied to information retrieval, particularly for organizing documents, the listwise approaches \citep{cao2007learning,xia2008listwise,li2011short} are currently the state-of-the-art approach for this problem.
%Strength estimation can also be regarded as ranking problems as only the query was "strength" to sort the players.

%Early research of rformulated the ranking problem as a regression or classification task applied to individual items, referred to pointwise approaches, treating each item independently \citep{shashua2002ranking,cossock2006subset}. 
%Subsequently, the pairwise approach shifted the focus to the relative order between pairs of items, transforming the ranking problem into a pairwise classification or regression task, which have been more effective than pointwise approaches \citep{herbrich2000large, freund2003efficient}.
%However, both the pointwise and pairwise approaches fail to consider the group structure inherent in ranking.

%To address this limitation, the listwise approach was later proposed \citep{cao2007learning, xia2008listwise}. 
%These methods preserve the group structure of ranking and directly models and optimizes the overall ranking performance for an entire list of items, achieving better results than the earlier pointwise and pairwise approaches \citep{li2011short}.

%and can be categorized into three main approaches: (1) Pointwise Approach: This approach formulates the ranking problem as a regression or classification task on individual items, treating each query-document pair independently. 
%Notable methods include Subset Ranking, McRank, Prank, and OC SVM. (2) Pairwise Approach: This approach focuses on the relative order between pairs of items, transforming the ranking problem into a pairwise classification or regression task. Representative methods are Ranking SVM, RankBoost, RankNet, GBRank, IR SVM, Lambda Rank, and LambdaMART. (3) Listwise Approach: This approach directly models and optimizes the overall ranking performance for an entire list of items, considering global ranking metrics such as NDCG or MAP. Prominent methods in this category include ListNet, ListMLE, AdaRank, SVM MAP, and Soft Rank.


\section{Method}
\label{sec:method}
% In this section, we first introduce a strength estimator in subsection \ref{subsec:method_strength_estimator} that assesses player strength based on their game-playing records without direct interaction.
% Subsequently, in subsection \ref{subsec:method_strength_adjustment}, we present an MCTS-based strength adjustment method that utilizes this strength estimation.

\subsection{Strength Estimator}
\label{subsec:method_strength_estimator}
We introduce the \textit{strength estimator} (SE), which is designed to predict the strength of an action $a$ at a given state $s$ based on human game records.
Each state-action pair, denoted as $p=(s,a)$, is labeled with a rank $r$ that corresponds to the player's strength.
For simplicity, in this paper, ranks are ordered in descending order where rank 1, denoted as $r_1$, represents the strongest level of play, and progressively higher numbers indicate weaker playing strength.
Each rank corresponds to a group of players, as we assume that players with the same rank have equivalent strength.
Ranks could be determined by Elo ratings; for example, players with an Elo between 2400 and 2599 are classified as $r_1$, players with an Elo between 2200 and 2399 as $r_2$, and so on.
% Similarly, in the game of Go, ranks can be directly defined by the players' amateur \textit{dan} level\footnote{In the game of Go, \textit{Dan} levels indicate advanced amateur level, usually ranging from 1-9, with higher numbers indicating stronger players.}, with amateur 9 dan players classified as $r_1$, amateur 8 dan players as $r_2$, etc.

Consider a game collection $\mathcal{D}$ that consists of numerous state-action pairs $p$, each associated with a distinct rank $r$.
Suppose there are $n$ ranks in $\mathcal{D}$, represented by $r_1$, $r_2$, ..., $r_n$.
% The collection $\mathcal{D}$ can be divided into subsets $\mathcal{D}_1$, $\mathcal{D}_2$, ..., $\mathcal{D}_n$, where each subset $\mathcal{D}_i$ includes the state-action pairs $p_i$ played by players with $r_i$.
Next, given a state-action pair $p$ sampled from $\mathcal{D}$, the strength estimator, denoted as $SE(p)=\lambda$, is designed to predict the strength $\lambda$ corresponding to action $a$ in state $s$.
Consider two state-action pairs, $p_1=(s,a_1)$ and $p_2=(s,a_2)$, where $a_1$ and $a_2$ are played by $r_1$ and $r_2$, respectively.
The strength estimator is expected to predict strength $\lambda_1$ for $p_1$ and $\lambda_2$ for $p_2$, such that $\lambda_1\geq\lambda_2$.
Note that $\lambda_1=\lambda_2$ occurs when the actions $a_1$ and $a_2$ are identical or are of equal strength.
Following the Bradley-Terry model, we can calculate the probability that $r_1$ wins against $r_2$ as $P(r_1\succ r_2)=\frac{\lambda_1}{\lambda_1+\lambda_2}$.
To generalize to $n$ ranks, consider $n$ state-action pairs, $p_1$, $p_2$, ..., $p_n$, corresponding to ranks $r_1$, $r_2$, ..., $r_n$, respectively.
The strength estimator predicts $\lambda_i$ for each $p_i$.
The probability that $r_1$ wins against all other ranks is then calculated as $P(r_1\succ\{r_2, r_3, \ldots, r_n\})=\frac{\lambda_1}{\lambda_1+\lambda_2+\ldots+\lambda_n}$.

Furthermore, we extend the method to estimate the \textit{composite strength} of a rank by incorporating multiple state-action pairs, collectively conceptualizing them as a team. 
This approach allows us to effectively measure the overall capabilities of players within specific ranks by considering various actions across different scenarios.
Consider $m$ state-action pairs $p_{i,1}$, $p_{i,2}$, ..., $p_{i,m}$, where $p_{i,j}$ represents the $j$-th state-action pairs associated with $r_i$ sampled from $\mathcal{D}$.
The strength estimator predicts the strength $\lambda_{i,1}$, $\lambda_{i,2}$, ..., $\lambda_{i,m}$ for each state-action pair, respectively.
We define the composite strength for $r_i$ by aggregating all individual strengths using the geometric mean.
The composite strength, denoted as $\Lambda_i$, is calculated as $\Lambda_i =(\lambda_{i,1}\lambda_{i,2}\ldots\lambda_{1,m})^{1/m}$.

The geometric mean is used to ensure that the strength estimator provides stable estimations and reflects the rank's ability across different scenarios.
Namely, $\Lambda_i$ should remain consistent regardless of the number of state-action pairs considered:
\begin{equation}
\label{equ:composite_strength}
    \Lambda_i = (\lambda_{i,1}\lambda_{i,2}\ldots\lambda_{1,m})^{1/m}
              = \biggl(\prod_{j=1}^{m} \lambda_{i,j}\biggl)^\frac{1}{m}
              = \biggl(\prod_{\substack{j=1 \\ p_j \sim \mathcal{D}}}^{m} SE(p_j)\biggl)^\frac{1}{m},
\end{equation}
where the state-action pair $p_j$ is randomly sampled from the game collection $\mathcal{D}$.
We can further calculate the probability that rank $r_1$ wins against all other ranks by using the composite strength: $P(r_1\succ\{r_2, r_3, \ldots, r_n\}) = \frac{\Lambda_1}{\Lambda_1+\Lambda_2+\ldots+\Lambda_n}$.

Note that our proposed method of aggregating strength using the geometric mean for teams differs from the Bradley-Terry model, which utilizes product aggregation.
However, the geometric mean is specifically tailored to our scenarios to guarantee consistent measurement of individual performance across different games, rather than focusing on team member interactions.
This approach also helps to mitigate the influence of outliers.
Moreover, this modification preserves the integrity of the Bradley-Terry model principles, ensuring that the order of teams is strictly followed.
% We present a proof in the appendix.


\subsection{Training the Strength Estimator}
\label{subsec:method_training_strength_estimation}
This subsection introduces a methodology for training \textit{strength estimator}.
% 5d: lambda: strength, beta: strength score
For simplicity, we propose to train a neural network, $f_\theta(p)=\beta$, as a strength estimator which predicts a strength score $\beta$ instead of strength $\lambda$ for a given state-action pair $p$.
This strength score, $\beta$, serves as the exponent for strength $\lambda=e^\beta$, as defined by the Bradley-Terry model.
Then, the composite strength, $\Lambda_{i}$, from the equation \ref{equ:composite_strength} can be expressed by using $\beta$ as follows:
\begin{equation}
\label{equ:composite_strength_beta}
    \Lambda_i = \biggl(\prod_{j=1}^{m} \lambda_{i,j}\biggl)^\frac{1}{m}
              = \biggl(\prod_{j=1}^{m} e^{\beta_{i,j}}\biggl)^\frac{1}{m}
              = e^{\frac{1}{m}\sum_{j=1}^{m} \beta_{i,j}}
              = e^{\overline{\beta_i}},
\end{equation}
where $\overline{\beta_i}$ represents the average strength scores of $m$ state-action pairs, each with $r_i$, sampled from $\mathcal{D}$.

Next, given $n$ ranks in the game collection, the strength estimator is optimized by maximizing the likelihood $\mathcal{L}$ according to the ranking order \citep{xia_listwise_2008,chen_ranking_2009}.
The likelihood is defined as follows:
\begin{align}
\label{equ:likelihood}
    \mathcal{L} &= P(r_1\succ\{r_2, r_3, \ldots, r_n\}) \times P(r_2\succ\{r_3, r_4, \ldots, r_n\}) \times \ldots \times P(r_{n-1}\succ r_n)\nonumber\\
                &= \prod_{i=1}^{n-1} P(r_i \succ \{r_{i+1}, r_{i+2}, \ldots, r_{n}\})
                = \prod_{i=1}^{n-1} \frac{\Lambda_{i}}{\Lambda_{i} + \Lambda_{i+1} + \ldots + \Lambda_{n}}
                = \prod_{i=1}^{n-1}\frac{e^{\overline{\beta_i}}}{\sum_{j=i}^{n}e^{\overline{\beta_j}}}.
\end{align}
Maximizing $\mathcal{L}$ ensures that the strength scores of $r_1$, $r_2$, ..., $r_n$ are strictly in descending order, such that $r_1 \succ r_2 \succ r_3 \succ \ldots r_n$.
Then, the loss function $L$ can be defined by log-likelihood:
\begin{align}
\label{equ:loss_function}
    L = -\log(\mathcal{L}) &= -\log\biggl(\prod_{i=1}^{n-1}\frac{e^{\overline{\beta_i}}}{\sum_{j=i}^{n}e^{\overline{\beta_j}}}\bigg)
    = -\sum_{i=1}^{n-1}\log(\frac{e^{\overline{\beta_i}}}{\sum_{j=i}^{n}e^{\overline{\beta_j}}}).
\end{align}
% Note that each $\log(\frac{e^{\overline{\beta_i}}}{\sum_{j=i}^{n}e^{\overline{\beta_j}}})$ are a softmax loss.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/architecture.pdf}
\caption{The training process of the strength estimator.}
\label{fig:architecture}
\end{figure}

Figure \ref{fig:architecture} illustrates the training process of the strength estimator.
Assume there are $n$ ranks, $r_1$, $r_2$, ..., $r_n$, in the game collection.
Initially, for each $r_i$, we sample $m$ state-action pairs and evaluate them by the strength estimator.
The strength estimator then outputs the strength score $\beta_{i,j}$ corresponding to each state-action pair.
Subsequently, we average all $\beta_{i,j}$ to obtain $\overline{\beta_i}$ for each $r_i$.
Finally, using all strength scores, $\overline{\beta_i}$, \revision{we sequentially minimize each softmax loss as defined by the equation \ref{equ:loss_function}.}

Since the state-action pairs are collected only from human games, the strength estimator may provide unpredictable estimations for out-of-distribution state-action pairs, which rarely appear in human games.
To address this issue, we introduce an additional rank, $r_\infty$ into our training process, as depicted by the dashed rectangle in Figure \ref{fig:architecture}.
This rank, $r_\infty$, is defined as the weakest among all ranks, ensuring that $r_i\succ r_\infty$ for all $r_i$.
To generate the state-action pairs for $r_\infty$, we first select a state-action pair, $p_i=(s_i,a_i)$ from any $r_i$.
Then, we disturb the state-action pair $p_i$ to $p_\infty$ by modifying the action $a_i$ to a randomly chosen legal action, resulting in $p_\infty=(s_i,a_\infty)$.
Since a random action $a_\infty$ is highly likely to result in an inferior outcome, these actions are expected to correspond to the weakest rank.
Note that although a random action may occasionally result in a strong action, the impact of such outliers will be minimized by the average $\overline{\beta_\infty}$ as the number of samples, $m$, increases.


\subsection{Strength Estimator Based MCTS for Strength Adjustment}
\label{subsec:method_se_mcts}
We present a novel method that integrates a strength estimator with MCTS to adjust strength dynamically, named \textit{SE-MCTS}.
In previous strength adjustment approaches, as described in subsection \ref{subsec:bg_mcts_sa}, the MCTS search tree is unmodified during the search, with only changing the final action decision after the search is complete.
In contrast, we propose inherently modifying the search based on a target strength score to ensure that the search aligns more closely with the desired strength of ranks.

Specifically, in MCTS each node is evaluated by the strength estimator to obtain a strength score, $\beta(s,a)$, which represents the strength score of action $a$ at state $s$.
We can calculate the composite strength score $\overline{\beta}(s,a)$, by averaging all $\beta$ from the nodes within the subtree of state $s$.
This is similar to the method used to calculate estimated Q-values.
Given a targeted rank $r$ with strength score $\beta_t$, we calculate the absolute strength difference for each node, which is denoted as $\delta(s,a)=|\overline{\beta}(s,a)-\beta_t|$.
% The difference is calculated between the strength score associated with the chosen action $a$ at state $s$ and the targeted strength score $\beta$.
Higher values of $\delta$ indicate that the action is unlikely chosen by a player of the target rank, while lower $\delta$ suggest that the actions are closer to the strength of the target rank.

As the values $\delta(s,a)$ are unbounded and can be any non-negative number, we normalize all values using the minimum and maximum values observed in the current search tree.
This normalization ensures that the difference values are bounded within the $[0, 1]$ interval, similar to the approach used in MuZero \citep{schrittwieser_mastering_2020}.
Then, the PUCT formula in MCTS selection is modified from equation \ref{equ:PUCT} as follows:
\begin{equation}
\label{equ:SE-PUCT}
    {a^*} = \mathop{\arg\max}_{a} \Bigg\{Q(s,a)+c\cdot\Bigl(P(s,a)-c_1\cdot\hat{\delta}(s,a)\Bigl)\cdot\frac {\sqrt{\sum_{b}{N(s,b)}}}{1+N(s,a)}\Bigg\},
\end{equation}
where $\hat{\delta}(s,a)$ is the normalized difference values of $\delta(s,a)$ and $c_1$ is a hyperparameter used to control the confidence of prior heuristic values and difference values.
Note that we choose to use $\hat{\delta}(s,a)$ to eliminate the prior heuristic value $P(s,a)$, rather than incorporating additional values like the use of $Q(s,a)$.
This is because, during the MCTS search, the algorithm first prioritizes actions based on higher prior heuristic values and gradually shifts the focus to actions with higher Q-values when the simulation counts increase.
By combining $P(s,a)$ and $\hat{\delta}(s,a)$, we effectively adjust the prioritization of actions, thereby aligning the search more closely with the desired strengths.


\section{Experiments}
\label{sec:experiments}

\subsection{Experiment Setup}
\label{subsec:exp_setups}
We first experiment in the game of Go.
The human games are collected\footnote{We downloaded Go games from the FoxWeiqi online platform using its public download links.} from FoxWeiqi \citep{fox_weiqi_2024}, which is the largest online Go platform in terms of users.
These games are collected from amateur 5 \textit{kyu} to 9 \textit{dan}\footnote{In the game of Go, \textit{Kyu} represents the beginner to decent amateur level, ranging from 18 kyu to 1 kyu, with lower numbers indicating stronger kyu players; \textit{Dan} denotes advanced amateur, ranging from 1 dan to 9 dan, with higher numbers indicating stronger dan players.}, and are ranked in order from the strongest to weakest as follows: 9 dan, 8 dan, ..., 2 dan, 1 dan, 1-2 kyu, and 3-5 kyu, corresponding to $r_1$, $r_2$, ..., $r_{11}$.
Namely, a total of $n=11$ ranks are used.
Note that for kyu, we classify 1 to 2 kyu as one rank and 3 to 5 kyu as another rank, and we exclude games played by players ranked lower than 5 kyu.
This is because kyu players are still mastering basic Go strategies, their ranks often change rapidly.
Consequently, their games do not consistently correspond to their ranks.
For the training dataset, we collect a total of 495,000 games, with 45,000 games from each rank.
\revision{We also prepare a separate testing dataset, including a \textit{candidate} and a \textit{query} dataset.
The candidate dataset is used to estimate an average strength score of each rank, including a total of 1,100 games, with 100 games per rank.
The query dataset is used for the strength estimator to predict the strength, containing a total of 9,900 games, with 900 games per rank.}

The network architecture of the strength estimator is similar to the AlphaZero network, consisting of 20 residual blocks with 256 channels.
Given a state-action pair, the network outputs a policy distribution $p$, a value $v$, and a strength score $\beta$.
The training loss for the policy and value network follows AlphaZero, while the loss for the strength estimator is defined by equation \ref{equ:loss_function}.
During training, we aggregate the composite strength score $\overline{\beta_i}$ by randomly selecting $m=7$ state-action pairs from $r_i$.
% The network is trained over a total of 130,000 steps, requiring approximately 242 GPU-hours on an NVIDIA RTX A5000.
Other training details are provided in the appendix.

\subsection{Predicting Ranks from Games}
\label{subsec:exp_acc}
The strength estimator can be utilized to predict ranks in games where the rank is unknown.
% Specifically, we collect two additional datasets: a \textit{candidate dataset} and a \textit{query dataset}.
% The candidate dataset is used to estimate an average strength score of each rank, including a total of 1,100 games, with 100 games for each rank.
% The query dataset is used for the strength estimator to predict the strength, containing a total of 9,900 games, with 900 games for each rank.
We first calculate $\overline{\beta_i}$ for each $r_i$ by evaluating all games in the candidate dataset.
Next, for games from the same unknown rank, $r_u$, in the query dataset, a composite score $\overline{\beta_u}$ is calculated by the strength estimator.
Finally, $r_u$ is then determined to be $r_i$, where $|\overline{\beta_u}-\overline{\beta_i}|$ is the smallest among all $r_i$.
% This ensures the best alignment between the estimated and known scores.
% In conclusion, $r_u$ is matched to $r_i$ whose composite score is closest to $\overline{\beta_u}$, ensuring the best possible alignment between the estimated and known scores.


% 5d: todo? add: If we only use one position in our training, our method would downgrade to Moudřík's.

We train two strength estimator networks, \texttt{SE} and \texttt{SE\textsubscript{$\infty$}}, where \texttt{SE} is trained with 11 ranks, and \texttt{SE\textsubscript{$\infty$}} includes an additional rank, $r_{\infty}$, for a total of 12 ranks.
In addition, for comparison, we train another network based on supervised learning (SL), \texttt{SL\textsubscript{sum}} and \texttt{SL\textsubscript{vote}}, as mentioned in subsection \ref{subsec:bg_se}.
Both \texttt{SL\textsubscript{sum}} and \texttt{SL\textsubscript{vote}} are trained to classify 11 ranks for a given state-action pair but with different aggregation methods.

The evaluation is conducted as follows.
For each $r_i$ from the query dataset, each network evaluates all state-action pairs from $N$ randomly selected games and then predicts a rank.
We repeat this prediction process 500 times for each $N$ to ensure a stable estimation.

\begin{figure}[ht]
    \centering
    \subfloat[Accuracy of rank predictions by different networks.]{ 
        \includegraphics[width=0.47\columnwidth, keepaspectratio]{figures/all_error_revision.pdf}
        \label{fig:all_acc}
    }
    % 5d: (+-1)
    \subfloat[Accuracy of each rank for \texttt{SE\textsubscript{$\infty$}}.]{
        \includegraphics[width=0.49\columnwidth]{figures/Our Method among all ranks_error_revision.pdf}
        \label{fig:se_inf}
    }
    \caption{Accuracy of rank prediction in Go, with the shaded area representing the 95\% confidence interval.}
    \label{fig:accuracy}
\end{figure}



% 5d: accuracy sample method => appendix
Figure \ref{fig:all_acc} shows the accuracy of predicting the games from the query dataset.
From the figure, the two strength estimator networks significantly outperform the supervised learning networks.
Both \texttt{SE} and \texttt{SE\textsubscript{$\infty$}} perform nearly identical performance, achieving over 80\% of accuracy with only 15 games and reaching an accuracy of 97.5\% after evaluating 100 games.
% 5d: todo? note that both networks perform well since the data are in the distribution?
In contrast, both \texttt{SL\textsubscript{sum}} and \texttt{SL\textsubscript{vote}} reached an accuracy of 49\%, even after evaluating 100 games, with \texttt{SL\textsubscript{sum}} performs slightly better than \texttt{SL\textsubscript{vote}}.
Furthermore, as human players do not always perform consistently and may occasionally change their ranks by one rank either above or below their actual rank, the games within $r_i$ might involve players whose actual ranks are $r_{i-1}$ or $r_{i+1}$, leading to slight fluctuations in the dataset.
Therefore, we incorporate a prediction tolerance that allows for a deviation of one rank.
Specifically, if the network predicts $r_{i-1}$ or $r_{i+1}$ for $r_i$, we consider this prediction accurate.
The results show that both strength estimator networks achieve nearly 80\% accuracy by only evaluating a single game, and perfectly predict the rank with an accuracy of over 99\% after only 6 games, while the supervised learning networks still cannot predict the correct ranks after 100 games. 
This result indicates that the ranks predicted by the strength estimators are close to the actual ranks even when the prediction is incorrect.

Figure \ref{fig:se_inf} depicts the accuracy of each rank prediction for \texttt{SE\textsubscript{$\infty$}} with several interesting observations.
First, $r_1$ (9 dan) has the highest accuracy among all ranks.
Since 9 dan is the highest rank on FoxWeiqi, so professional Go players and Go AIs are also classified at this level, thus creating a significant strength gap between $r_1$ and $r_2$, and leading to a clearer distinction.
This phenomenon results in some players, originally ranked at $r_1$ (9 dan), who are relatively stronger compared to $r_2$ (8 dan), being relegated to $r_2$.
Consequently, $r_2$ shows the lowest accuracy among all ranks.
Second, the prediction for $r_7$ (3 dan) is below the average.
This is because new players on FoxWeiqi can only select a maximum initial rank of 3 dan and must advance gradually.
Therefore, many players at this rank are actually stronger than 3 dan.
Finally, the prediction for $r_{11}$ (3-5 kyu) is also below average, corroborating to common understanding that the strength of kyu players is usually inconsistent.
In conclusion, these results allow our strength estimator to provide evaluations of a game's ranking system, further offering developers to make adjustments.
Details on the accuracy of each rank prediction for other networks are provided in the appendix.


\subsection{Adjusting Strength with Strength Estimator}
\label{subsec:exp_se_mcts}
In this section, we evaluate the performance of SE-MCTS, as described in subsection \ref{subsec:method_se_mcts}, by incorporating the two trained strength estimator networks into MCTS to adjust the playing strength for game playing.
We first calculate the composite strength score $\overline{\beta_i}$, by averaging all $\beta$ from the state-action pairs in $r_i$ from the candidate set.
Although we assume that the strength score $\beta_i$ for any state-action pair from $r_i$ should be similar, in practice, we observe that $\overline{\beta_i}$ may vary across different action numbers, as shown in Figure \ref{fig:move_strength}.
In the beginning, particularly for the first actions, all $\overline{\beta_i}$ are estimated as the same score.
This is likely because the action choices at the beginning of Go have less variety, and weaker players can easily remember and imitate the openings from stronger players.
Therefore, in the SE-MCTS, we propose using $\overline{\beta_i^d}$ instead of $\overline{\beta_i}$ as the target strength score for each action, where $\overline{\beta_i^d}$ represent the composite strength score at $d$-th action for $r_i$.
% For the parameter $c_1$ in \eqref{equ:SE-PUCT}, we adjust its value based on the move's strength to make SE-MCTS lean towards the target rank more effectively. 
% We set a higher value for $c_1$ for stronger moves and a lower value for weaker moves. 
% Specifically, $c_1 = 2$ when $\beta(s,a) > \beta_t$, making actions that would lead to a state with a higher strength score less favorable, and $c_1 = 1$ when $\beta(s,a) \leq \beta_t$.
% 5d: todo, mention 2 & 1 for c_1


%\begin{figure}[t]
%\centering
%\begin{minipage}{0.7\textwidth}
 %   \includegraphics[width=0.7\linewidth]{figures/strength_table.pdf}
 %   \caption{The composite strength score for each rank across different actions in games.}
 %   \label{fig:move_strength}
%\end{minipage}\hfill
%\end{figure}



\begin{figure}[ht]
\centering
\begin{minipage}{0.55\textwidth}
    \includegraphics[width=\linewidth]{figures/strength_table.pdf}
    \caption{The composite strength score \revision{from SE\textsubscript{$\infty$}} for each rank across different actions in games.}
    \label{fig:move_strength}
\end{minipage}
% \hfill
\hspace{1em}
\begin{minipage}{0.33\textwidth}
    \includegraphics[width=\linewidth]{figures/technique_comparison_heatmap_with_errors_revision.pdf}
    \caption{Win rate against \texttt{SE\textsubscript{$\infty$}-MCTS\textsubscript{$5$}} in Go.}
    \label{fig:win_heatmap}
\end{minipage}
\end{figure}

\input{tables/accuracy_stddev}
%

%\begin{table}[ht]
%    \centering
%    \begin{minipage}[t]{0.48\linewidth}
%        \centering
%        \input{tables/fight} % 這裡插入第一個表格
%    \end{minipage}
%    \hspace{0.02\linewidth} % 調整兩個表格之間的間距
%    \begin{minipage}[t]{0.48\linewidth}
%        \centering
%        \input{tables/accuracy_stddev} % 這裡插入第二個表格
%    \end{minipage}
%\end{table}



To evaluate the performance, we select four MCTS programs: (a) \texttt{MCTS}, representing vanilla MCTS without any strength adjustment mechanism, (b) \texttt{SA-MCTS}, which utilizes the strength index $z$ \revision{\citep{wu_strength_2019}}, (c) \texttt{SE-MCTS}, which uses \texttt{SE} network, and (d) \texttt{SE\textsubscript{$\infty$}-MCTS}, which uses \texttt{SE\textsubscript{$\infty$}} network.
Except \texttt{MCTS}, the remaining three programs can be adjusted to different strengths across a total of 11 ranks.
% 5d: MCTS simulation count
% 5d: foxweiqi 5d => footnote
We use \texttt{SA-MCTS\textsubscript{$i$}}, \texttt{SE-MCTS\textsubscript{$i$}}, and \texttt{SE\textsubscript{$\infty$}-MCTS\textsubscript{$i$}} to represent the strength of each program correspond to $r_i$.
For \texttt{SE-MCTS} and \texttt{SE\textsubscript{$\infty$}-MCTS}, we calculate the target strength score from the candidate dataset for strength adjustment.
However, for \texttt{SA-MCTS}, since the strength index $z$ does not directly correspond to any specific rank, we adjust $z$ for each $r_i$ to ensure that each \texttt{SA-MCTS\textsubscript{$i$}} and \texttt{SE\textsubscript{$\infty$}-MCTS\textsubscript{$i$}} achieve a comparable win rate, i.e., approximately 50\%.
\revision{Each $z$ is in Appendix \ref{app:detailed_exp}.}

Figure \ref{fig:win_heatmap} shows the win rate for each program playing against a baseline program, where the baseline is chosen as \texttt{SE\textsubscript{$\infty$}-MCTS\textsubscript{$i$}} with $i=5$ (5 dan).
Generally, the win rate for \texttt{SA-MCTS\textsubscript{$i$}} decreases as $i$ increases, except from $i=3$ to $i=6$ where there is a slight fluctuation.
This corroborates the experiments in \citep{wu_strength_2019}.
Interestingly, although \texttt{SE} can accurately predict the strength, \texttt{SE-MCTS\textsubscript{$i$}} cannot adjust the strength effectively.
This is due to the exploration in the MCTS search, which may inevitably explore actions that are rarely seen in human games.
Without training using $r_\infty$, \texttt{SE} provides inaccurate strength scores for these unseen actions.
In contrast, the win rate of \texttt{SE\textsubscript{$\infty$}-MCTS\textsubscript{$i$}} consistently decreases as $i$ increases, demonstrating an accurate strength adjustment using strength scores.
% \revision{Additionally, we compare more model combinations, with the results presented in the Appendix.}
\revision{We also include an experiment with different baselines in Appendix \ref{app:round_robin}.}


% \input{tables/accuracy_incomplete}

Moreover, we are interested in whether these programs can align with the human player's behavior, i.e., if they can choose the same actions as human players.
Therefore, we sample 50 human games from the query dataset for each rank and use four MCTS programs to play on every state-action pair.
The accuracy is evaluated based on whether the programs choose the same actions as human players.
The results are shown in Table \ref{tbl:bs_accuracy}.
% 5d: kyu => ACC low
% 5d: why SE better than SE_\infty
From the table, generally, the accuracy of $r_i$ decreases when the number $i$ increases for all four programs.
This is because weaker players are more unpredictable than stronger players.
For \texttt{MCTS}, it achieves a high accuracy, exceeding 50\%, but it cannot adjust strength as shown in Figure \ref{fig:win_heatmap}.
In contrast, although \texttt{SA-MCTS} can adjust strengths, it achieves the lowest accuracy among all four programs.
This is due to \texttt{SA-MCTS} selecting actions proportional to the simulation counts by using strength index $z$.
The randomness results in diverging from human playing styles.
On the other hand, both \texttt{SE-MCTS} and \texttt{SE\textsubscript{$\infty$}-MCTS} achieve a high accuracy, even better than \texttt{MCTS}.
This is because SE-MCTS directly modifies the search by the strength scores, guiding the search to better align with human behavior. 
In conclusion, among all four programs, \texttt{SE\textsubscript{$\infty$}-MCTS} not only adjusts strengths to specific ranks but also provides playing styles that are closely aligned with those of human players at specific ranks.


\subsection{Training Strength Estimator with Limited Data}
\label{subsec:exp_few_rank}
In this subsection, we investigate training a strength estimator with limited data.
Unlike the supervised learning methods, \texttt{SL\textsubscript{sum}} and \texttt{SL\textsubscript{vote}}, which require data from each rank, our method can estimate a strength score and use it to predict ranks that were not observed during training.
In niche games or those favored by a specific group of enthusiasts, ranking systems are often not fully established due to a limited number of game records, and some specific ranks may be sparsely populated with only a few players.
Therefore, it is intriguing to explore whether the strength estimator can generalize to these unseen strengths.

\begin{figure}[h]
    \centering
    \subfloat[2-rank dataset.]{
         \includegraphics[width=0.49\columnwidth]{figures/2-rank_infinity_rank_error_revision.pdf}
         \label{fig:3-5k_9D_3_7_32}
    }
    \subfloat[3-rank dataset.]{
        \includegraphics[width=0.49\columnwidth]{figures/3-rank_infinity_rank_error_revision.pdf}
        \label{fig:3-5k_4D_9D_4_7_32}
    }    
    \caption{Accuracy of rank predictions on the limited dataset of Go.}
    \label{fig:future_analysis}
\end{figure}

We train \texttt{SE\textsubscript{$\infty$}} on two separate datasets: the 2-rank dataset, containing $r_1$ (9 dan) and $r_{11}$ (3-5 kyu), and the 3-rank dataset, which includes the same ranks as the 2-rank dataset, plus an additional rank, $r_6$ (4 dan).
During the evaluation, we utilize the same methods, as described in \ref{subsec:exp_acc}, by using the strength estimator to calculate $\overline{\beta_i}$ for each $r_i$ in both candidate and query datasets, and then perform the predictions.
Figure \ref{fig:3-5k_9D_3_7_32} and \ref{fig:3-5k_4D_9D_4_7_32} show the accuracy of rank prediction on 2-rank and 3-rank datasets, respectively.
On average, the strength estimator achieves an accuracy of over 80\% after evaluating 38 games for the 2-rank dataset and 21 games for the 3-rank datasets, respectively.
Although these numbers are larger than 15 -- the number of games needed by a strength estimator trained with all 11 ranks to achieve over 80\% accuracy -- it can still effectively predict ranks that were not seen during training.
This suggests that the strength estimator can generalize across a spectrum of ranks with few ranks.
Note that if sufficient data for more ranks are available, the accuracy still improves.
Our method provides a way to both generalize with limited data and enhance performance as more data becomes available.


% 5d: todo, proofread this subsection

%\begin{figure}[h]
    %\centering
    %\subfloat[Accuracy of rank predictions by different networks.]{ 
        %\includegraphics[width=0.47\columnwidth, keepaspectratio]{figures/all_chess.pdf}
        %\label{fig:all_acc_chess}
    %}
    % 5d: (+-1)
   % \subfloat[Accuracy of each rank for %\texttt{SE\textsubscript{$\infty$}}.]{
        %\includegraphics[width=0.49\columnwidth]{figures/Our Method among all ranks_chess.pdf}
        %\label{fig:se_inf_chess}
   % }
    %\caption{Accuracy of rank prediction in Chess.}
 %   \label{fig:accuracy_chess}
%\end{figure}

%\begin{figure}[t]
%\centering
%\begin{minipage}{0.7\textwidth}
%    \includegraphics[width=0.47\columnwidth, keepaspectratio]{figures/all_chess.pdf}
       % \caption{Accuracy of rank prediction in Chess.}
       % \label{fig:all_acc_chess}
%\end{minipage}\hfill
%\end{figure}

%\input{tables/accuracy_chess}

\subsection{Generalizing to other games}
\label{subsec:generality}

We further experiment in another game, chess, to demonstrate the generality of our SE and SE-MCTS approaches.
Similar to Go, chess is also a popular game with abundant human game records.
The games were collected from Lichess\footnote{Lichess is one of the most popular online chess platforms, with millions of active users.} \citep{lichess_2024}, which uses Elo ratings as its ranking system.
We collect games with Elo ratings ranging from 1,000 to 2,600 and categorize them into eight ranks, with each rank covering 200 Elo points and 240,000 games, for a total of 1,920,000 games.
\revision{For the testing dataset, the candidate dataset consists of 960 games, with 120 games per rank, while the query dataset contains 9,600 games, with 1,200 games per rank.}
Then, we apply experiments to chess, similar to those in Go.
Note that the training algorithms remain identical, except the input features of the neural network changed from Go to chess.
% Based on the result in Go, where $\texttt{SE}_{\infty}$ performs better than $\texttt{SE}$, only $\texttt{SE}_{\infty}$ is trained for chess.

The results are consistent with the findings in Go.
First, as shown in Figure \ref{fig:all_acc_chess}, $\texttt{SE}_{\infty}$ achieves over 80\% accuracy in predicting ranks with only 26 games, significantly outperforming both \texttt{SL\textsubscript{sum}} and \texttt{SL\textsubscript{vote}}, and reaches an accuracy of 93.38\% after evaluating 100 games.
Second, Figure \ref{fig:fight_chess} shows the win rate for each \texttt{SE\textsubscript{$\infty$}-MCTS\textsubscript{$i$}} when playing against \texttt{SE\textsubscript{$\infty$}-MCTS\textsubscript{5}}.
The win rate consistently decreases as $i$ increases, demonstrating \texttt{SE\textsubscript{$\infty$}-MCTS} can adjust its strength in chess.
Finally, Table \ref{tbl:bt_accuracy_chess} shows the accuracy of predicting human moves, with \texttt{SE\textsubscript{$\infty$}-MCTS} achieving an average of 47.25\% accuracy in aligning with human actions, outperforming both \texttt{MCTS} (46.56\%), \texttt{SA-MCTS} (40.21\%)\revision{, and \texttt{SE-MCTS} (38.93\%)}.
In conclusion, this experiment demonstrates the versatility of our approach.


\begin{figure}[htbp]
\centering
\begin{minipage}{0.7\columnwidth}
\begin{minipage}{0.60\linewidth}
    \includegraphics[width=\linewidth]{figures/all_chess_revision.pdf}
    \caption{Accuracy of rank prediction in chess.}
    \label{fig:all_acc_chess}
\end{minipage}
\hfill
\begin{minipage}{0.38\columnwidth}
 \centering
    \includegraphics[width=0.8\linewidth]{figures/technique_comparison_heatmap_chess_revision.pdf}
     \captionsetup{width=1.2\linewidth}
    \caption{Win rate against \texttt{SE\textsubscript{$\infty$}-MCTS\textsubscript{$5$}} in chess.}

    \label{fig:fight_chess}
\end{minipage}
\end{minipage}
\end{figure}

\input{tables/accuracy_chess}


\section{Discussion}
\label{sec:discussion}
This paper introduces a novel strength system, including a \textit{strength estimator} for evaluating the strength from game records without requiring direct interaction with human players, and an \textit{SE-MCTS} for adjusting the playing strength using strength scores provided by the strength estimator.
When predicting ranks in the game of Go, our strength estimator significantly achieves over 80\% accuracy by examining only 15 games, whereas the previous supervised learning method only reached 49\% accuracy even after evaluating 100 games.
The strength estimator can be trained with limited rank data and still accurately predict unseen rank data, providing extensive generalizability.
For strength adjustment, SE-MCTS successfully adjusts to designated ranks while providing a playing style that aligns with human behavior, achieving an average accuracy of 51.33\%, compared to the previous state-of-the-art method that only reached 42.56\% accuracy.
% 5d: modify this sentence
Furthermore, we apply our method to the game of chess and obtain consistent results to Go, demonstrating the generality of our approach to other games. 
%Although we demonstrate the strength system in the game of Go, it can be easily adapted to other games with minimal modifications.

One limitation of our work is that the strength estimator relies on human game records for training.
However, this issue could potentially be addressed by using all models trained by AlphaZero, which may serve as players of different playing strengths to generate games.
Besides, the strength system also provides several benefits for future directions.
For example, game designers can use the strength estimator to evaluate their ranking systems.
The strength estimator can evaluate a game by examining the strength scores for each action, and use it to identify incorrect actions for human players or for cheat detection \citep{alayed_behavioralbased_2013}.
\revision{Furthermore, we can extend our strength estimator by incorporating opponent-specific strength scores to address the Bradley-Terry model's limitations in capturing intransitivity \citep{balduzzi2018re, bertrand_limitations_2023, omidshafiei_arank_2019, vadori_ordinal_2024}.
Finally}, the search tree of SE-MCTS can offer the opportunity for explainability of AI actions in human learning.

% \revision{While our approach does not address intransitivity, an inherent limitation of the Bradley-Terry model\citep{balduzzi2018re, bertrand_limitations_2023, omidshafiei_arank_2019, vadori_ordinal_2024}, this presents an exciting challenge for future research. Developing methodologies to incorporate models that capture complex, intransitive relationships could significantly enhance the robustness and generalizability of strength estimation systems.} 
\section*{Ethics Statement}
\label{sec:ethics}

This paper presents a method for estimating player strength and adjusting it to specific ranks, allowing agents to play in a human-like manner.
A potential ethical concern is that our method could be exploited by human players to develop human-like agents for cheating in human competitions.
However, we emphasize that all models trained in this paper are used strictly for research purposes and adhere to established ethical guidelines.

%This study is conducted in accordance with the ICLR Code of Ethics, with particular attention to transparency, fairness, and privacy protection. 
%The dataset used in this paper is collected from two publicly available competitive platforms, FoxWeiqi and Lichess.
%These platforms are the largest and most representative competitive platforms in their respective domains. 
%We ensured a wide range of data collection and avoided biases in player strength. 
%The accuracy and fairness of the models in Go and chess are adequately maintained.

%All data was processed based on the principle of anonymization, and no personal information was collected during the study. 
%Additionally, data usage complied with platform agreements and relevant privacy regulations. 
%We also strictly adhere to data protection and security management practices.

% This research can provide human-like AI with competitive performance. 
% However, we recognize the possibility that our method could be misused. 
% For instance, game AIs that exhibit human-like behaviors can be misused to impersonate human players, evading cheating detection in human competitions. 
% We emphasize that the application of our method should adhere to ethical and legal guidelines, avoiding the creation of unfair advantages in competitive contexts. 

%Overall, we adhered to all relevant legal and ethical guidelines, including ethical review, data usage agreements, and privacy protection.

\section*{Reproducibility Statement}
\label{sec:reproducibility}

We have provided detailed descriptions of the method, implementation, and training hyperparameters in Section \ref{sec:method}, Section \ref{sec:experiments}, and Appendix \ref{app:traning_details} to facilitate the reproduction of our experiments.
The source code, along with a README file containing instructions, will be released to ensure reproducibility once this paper is accepted.
