\subsection{bt score and value correlation}
- add one figure


Bradley-Terry Model \\
$P(A \succ B) = \frac{\lambda_a}{\lambda_a + \lambda_b}$\\
\text{Generalizations of the Bradley-Terry Model} \\
$P(i \succ \{i+1, i+2, \ldots, i+n\}) = \frac{\lambda_{x_{i+1}}}{\lambda_{x_{i+1}} + \lambda_{x_{i+2}} + \ldots + \lambda_{x_{i+n}}}$ \\
$\lambda_x = \lambda_{x_1} \times \lambda_{x_2} \times \ldots \times \lambda_{x_n} 
= e^{\beta_{x_{1}}} \times e^{\beta_{x_{2}}} \times \ldots \times e^{\beta_{x_{n}}} $
\\
likelihood  \\ 
=\prod_{i=1}^{n} P(i \succ \{i+1, i+2, \ldots, i+n\})\\
= \prod_{i=1}^{n} \frac{\lambda_{i}}{\lambda_{i} + \lambda_{i+1} + \ldots + \lambda_{n}} \\
= \frac{\lambda_{1}}{\lambda_{1} + \lambda_{2} + \ldots + \lambda_{n}} \times \frac{\lambda_{2}}{\lambda_{2} + \lambda_{3} + \ldots + \lambda_{n}}\times \ldots \times \frac{\lambda_{n-1}}{\lambda_{n-1} + \lambda_{n}}+\frac{\lambda_{n}}{\lambda_{n}} \\
= \frac{\lambda_{1}}{\lambda_{1} + \lambda_{2} + \ldots + \lambda_{n}} \times \frac{\lambda_{2}}{\lambda_{2} + \lambda_{3} + \ldots + \lambda_{n}}\times \ldots\times \frac{\lambda_{n-1}}{\lambda_{n-1} + \lambda_{n}} \\
= \frac{e^{\beta_{1_{1}}} \times e^{\beta_{1_{2}}} \times \ldots \times e^{\beta_{1_{n}}}}{{e^{\beta_{1_{1}}} \times e^{\beta_{1_{2}}} \times \ldots \times e^{\beta_{1_{n}}}}+{e^{\beta_{2_{1}}} \times e^{\beta_{2_{2}}} \times \ldots \times e^{\beta_{2_{m}}}}+\ldots+{e^{\beta_{3_{1}}} \times e^{\beta_{3_{2}}} \times \ldots \times e^{\beta_{3_{o}}}}} \times \ldots \times \frac{e^{\beta_{(n-1)_{1}}} \times e^{\beta_{(n-1)_{2}}} \times \ldots \times e^{\beta_{(n-1)_{p}}}}{{e^{\beta_{(n-1)_{1}}} \times e^{\beta_{(n-1)_{2}}} \times \ldots \times e^{\beta_{(n-1)_{p}}}}+{e^{\beta_{n_{1}}} \times e^{\beta_{n_{2}}} \times \ldots \times e^{\beta_{n_{q}}}}}\\

\lambda_{1} > \lambda_{2} 
={e^{\beta_{1_{1}}} \times e^{\beta_{1_{2}}} \times \ldots \times e^{\beta_{1_{n}}}} > {e^{\beta_{2_{1}}} \times e^{\beta_{2_{2}}} \times \ldots \times e^{\beta_{2_{m}}}}\\
\Rightarrow\\
\log{\lambda_{1}} > \log{\lambda_{2}} \\
=\log{({e^{\beta_{1_{1}}} \times e^{\beta_{1_{2}}} \times \ldots \times e^{\beta_{1_{n}}}})} >\log{({e^{\beta_{2_{1}}} \times e^{\beta_{2_{2}}} \times \ldots \times e^{\beta_{2_{m}}}})}\\
= \log{{e^{\beta_{1_{1}}}}} + \log{e^{\beta_{1_{2}}}} + \ldots + \log{e^{\beta_{1_{n}}}} >\log{{e^{\beta_{2_{1}}} +\log{e^{\beta_{2_{2}}}} + \ldots +\log{ e^{\beta_{2_{m}}}}}}\\
=
\beta_{1_{1}} + \beta_{1_{2}} + \ldots + \beta_{1_{n}} >\beta_{2_{1}} +\beta_{2_{2}} + \ldots +\beta_{2_{m}}\\
\because \beta_{1_{i}}\geq \beta_{2_{j}}\forall i, 1\leq i \leq n, \forall j, 1\leq i \leq m\\
\therefore 
\frac{\beta_{1_{1}} + \beta_{1_{2}} + \ldots + \beta_{1_{n}}}{n} >\frac{\beta_{2_{1}} +\beta_{2_{2}} + \ldots +\beta_{2_{m}}}{m}

Given:
\begin{itemize}
  \item $\text{average\_score}$ is the average of output scores of the model for each rank.
  \item $\text{N}$ is the number of ranks per batch.
\end{itemize}

The loss function :

%要有一張圖表示方法
%theorem/assumption，(方法前提)

% garbage: 
% average: = $e^{\beta_i^2} \times e^{\beta_i^2} \times \ldots \times e^{\beta_i^n}$ = $\prod_{k=1}^{n} e^{\beta_i^k}$, where $\beta$ is exponential score of a strength

% original proof
%$\Rightarrow \log(\lambda_i)$ > $\log(\lambda_j)$ 
%$\Rightarrow \log(\prod_{k=1}^{n}\lambda_i^k)$ > $\log(\prod_{k=1}^{m}\lambda_j^k)$ 
%$\Rightarrow \log(\prod_{k=1}^{n} e^{\beta_i^k})$ > $\log(\prod_{k=1}^{m} e^{\beta_j^k})$ \\
%$\Rightarrow \sum_{k=1}^{n} \log(\beta_i^k$) > $\sum_{k=1}^{m} \log(\beta_j^k)$ 
%$\Rightarrow \sum_{k=1}^{n} \beta_i^k$ > $\sum_{k=1}^{m} \beta_j^k$ \\ 
%Since $\beta_i^{k_1}$ >= $\beta_j^{k_2}$ $, \forall k_1, 1\leq k_1 \leq n, \forall k_2, 1\leq k_2 \leq m$ \\
%$\Rightarrow \frac{\beta_i^1 + \beta_i^2 + \ldots + \beta_i^n}{n} >\frac{\beta_j^1 +\beta_j^1 + \ldots +\beta_j^m}{m}$ \\
%$\Rightarrow \frac{\sum_{k=1}^n \beta_i^k}{n}$ > $\frac{\sum_{k=1}^n \beta_i^k}{m} $, therefore, we can average the strength exponential score of a player $i$.