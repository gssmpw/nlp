\input{tables/quantitative_fisheyenerf}
\input{images/qualitative_fisheyenerf}
% \vspace{-2mm}
\section{Experiments\label{sec:experiments}}
In this section, we first briefly introduce our data preparation pipeline in \cref{sec:data}.
We compare our method against various baselines for scene reconstruction with large-FOV input (\cref{sec:evaluation} and \cref{sec:eval_cubemap}), followed by ablation studies for both the neural distortion model and the cubemap projection in \cref{sec:ablation}.

\subsection{Data Acquisition}
\label{sec:data}
We customized a camera module in the Mitsuba ray tracer~\cite{jakob2010mitsuba} to incorporate fisheye camera parameters derived from the open-source Lensfun database~\cite{lensfun}. Using a 180\si{\degree} fisheye camera, we rendered three scenes from~\cite{resources16}. We generated a training set with both perspective and fisheye views for baselines and our method, respectively. Additionally, we captured several real-world datasets using different uncalibrated cameras to evaluate our method. These datasets consist of casual walk-around video footage captured using a camera with an approximate 150\si{\degree} FOV. Furthermore, we tested our method on the existing FisheyeNeRF benchmark dataset~\cite{jeong2021self}, which contains fisheye images with an FOV of approximately 120\si{\degree}.


\input{images/qualitative_fov_large_vs_small}



\input{images/radial_tangential} 
\subsection{Comparisons to Traditional Lens Models} 
\label{sec:evaluation}
We first validate that our hybrid field representation models large-FOV cameras better than traditional polynomial distortion models~\cite{opencv_library, schoenberger2016sfm}. Using the FisheyeNeRF dataset~\cite{jeong2021self}, we compare our method against four baselines. The first baseline is Vanilla 3DGS~\cite{kerbl20233d}, which only uses a perspective camera model. Second, we directly apply the polynomial distortion model estimated by COLMAP~\cite{schoenberger2016sfm} to 2D Gaussians after the rasterization of 3DGS~\cite{kerbl20233d}. Third, we evaluate Fisheye-GS~\cite{liao2024fisheye}, which modifies the projection of Gaussians to a specific fisheye parametric model. Finally, we re-implemented ADOP~\cite{ruckert2022adop} using an omnidirectional camera model as the fourth baseline.


We report quantitative results in~\cref{tab:quantitative_fisheyenerf}, and our method consistently outperforms all baselines. We also show novel-viewpoint renderings in~\cref{fig:qualitative_fisheyenerf}. Baseline methods such as ADOP~\cite{ruckert2022adop} and Fisheye-GS~\cite{liao2024fisheye} tend to produce more artifacts in the corners of the test views, whereas our method reconstructs finer details.

To evaluate how our method performs with increasing FOV, we conducted an additional experiment on two datasets with different camera FOVs: real-world scenes captured with 150\si{\degree} cameras and a synthetic dataset with a 180\si{\degree} FOV. As the FOV approaches 180\si{\degree}, as shown in~\cref{fig:largefov_vs_smallfov_mitsuba_pano_fisheye}, our method successfully handles peripheral regions, whereas Fisheye-GS~\cite{liao2024fisheye} fails, producing spiky Gaussians in the surroundings and introducing artifacts at the center that blur the rendering. The quantitative evaluation of~\cref{fig:largefov_vs_smallfov_mitsuba_pano_fisheye} can be found in the supplementary material.

Our method can also be applied to different types of lens distortion. We introduce radial and tangential distortions to images in the LLFF dataset~\cite{mildenhall2019llff}, using camera parameters derived from the Lensfun database~\cite{lensfun}. 
By applying these parameters, we generate a combination of distortions, as demonstrated in the T-Rex scene. 
We optimize our model using these distorted images and obtain the distortion map learned by our lens model.
\cref{fig:radial_tangential} shows that our method accurately recovers various types of camera distortion without manual calibration or access to the physical camera.


\input{images/qualitative_netflix}
\subsection{Large FOV Reconstruction with Few Captures} 
\label{sec:eval_cubemap}
By enabling the use of large-FOV views, our method can reconstruct scenes with fewer images than would be needed for narrower perspective views.
We evaluate our method on 150\si{\degree} real-world captures and 180\si{\degree} synthetic scenes. For real-world scenes, we follow the conventional paradigm of using COLMAP to first resample the images to perspective—including peripheral cropping performed by COLMAP~\cite{schoenberger2016sfm}—before reconstruction. For synthetic scenes, we can flexibly modify the camera model and render images with a regular 90\si{\degree} FOV as input for 3DGS~\cite{kerbl20233d}. Since our method supports perspective rendering after training, we compare it against the baseline using a set of perspective hold-out cameras for a fair evaluation. Quantitative evaluation is conducted only on synthetic data, as obtaining ground truth perspective images for real-world captures is not feasible.

\input{tables/quantitative_fov_large_vs_small}

We report quantitative results in~\cref{tab:quantitative_large_small}, where our method outperforms the baseline even with far fewer input views, down to 10-15\%. 
The trajectories of Room 1 and Garden are similar, both focusing inward, while Room 2 and Studio involve a walk-around motion covering the entire space. As a result, we observe large incomplete regions in the first row of~\cref{fig:largefov_vs_smallfov_netflix} for the baseline, whereas our method achieves complete reconstruction. The walk-around scenes shown in the last row of~\cref{fig:largefov_vs_smallfov_netflix} exhibit noticeable artifacts and floating elements, primarily due to small-FOV captures or cropping in COLMAP.
These experiments demonstrate the efficiency of our approach compared to regular-FOV cameras, achieving high-quality reconstruction with significantly fewer captures, as shown in~\cref{tab:quantitative_large_small}.

\begin{table}[t]
  \centering
  \scalebox{1}{
    \begin{tabular}{ccccc}
    \toprule
    Explicit Grid & \multicolumn{1}{c}{iResNet} & PSNR & SSIM & LPIPS \\
    \midrule
    \xmark & \cmark & \multicolumn{3}{c}{Out-of-Memory}\\
    \xmark & \xmark & 14.19 & 0.421 & 0.561\\
    % & Explicit Grid & 15.67 & 0.488 & 0.496\\
    \cmark & \xmark & 19.79 & 0.569 & 0.309\\
    \cmark & \cmark & \textbf{23.67} & \textbf{0.777} & \textbf{0.151}\\
    \bottomrule
    \end{tabular}
  }
  % \vspace{-1mm}
  \caption{\textbf{Ablations of Hybrid Field.} We conduct ablation studies on our full method, including an explicit grid initialized with COLMAP's traditional polynomial distortion parameters, to demonstrate the necessity of adopting a hybrid neural distortion field representation in FisheyeNeRF~\cite{jeong2021self}. In the results, ``-'' indicates cases that are computationally infeasible to evaluate.}
  \label{tab:ablation_hybrid}
  % \vspace{-3mm}
\end{table}

\input{tables/merged_ablation_and_synthetic}

\subsection{Ablation Studies}
\label{sec:ablation}
% In this section, we will evaluate the effectiveness of hybrid fields and cubemap resampling.

\paragraph{Hybrid Field.} To verify the effectiveness of each module in our hybrid field, we isolate individual components (either iResNet or the explicit grid) and evaluate their performance separately. We report quantitative results in~\cref{tab:ablation_hybrid} on FisheyeNeRF~\cite{jeong2021self} scenes, showing that neither module alone achieves optimal performance compared to our full method. To test the grid alone, we use an explicit grid of learnable displacement vectors. This strategy provides only a slight improvement, primarily because the optimization of the explicit grid is prone to overfitting and can become trapped in local minima. We visualize the difference between the explicit grid and our hybrid field distortion flow in~\cref{fig:pipeline}. We cannot report results for iResNet without the grid, as explained in the last paragraph of \textbf{Invertible Residual Networks} in~\cref{sec:iresnet}. Our full method (iResNet + control grid) achieves the best performance by combining the expressiveness of iResNet with the computational efficiency of the explicit grid. This ablation verifies the necessity of our hybrid representation for modeling distortion.




% \vspace{-1.2em}
\paragraph{Cubemap Resampling.} To validate the necessity of cubemap resampling for wide-FOV reconstruction, we compare results obtained using our hybrid distortion field with single-plane projection versus cubemap resampling. 
We evaluate the performance of these two methods in the Mitsuba synthetic scenes.
As presented in~\cref{tab:ablation_cubemap_single}, using cubemap projections produces significantly better results when reconstructing images captured by 180\si{\degree} fisheye lenses.
Additional qualitative comparisons between single-plane and cubemap projections are provided in the supplementary.
