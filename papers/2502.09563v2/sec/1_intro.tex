\section{Introduction}
\label{sec:intro}
Large field-of-view (FOV) lenses, such as fisheye lenses, are widely used in robotics~\cite{de2018robust}, virtual reality~\cite{klanvcar2004wide}, and autonomous driving~\cite{cui2019real} because they capture scenes with fewer images, enabling efficient data acquisition for reconstruction and novel view synthesis (NVS)\cite{ma20153d}. However, most modern 3D reconstruction systems based on Neural Radiance Fields (NeRFs)\cite{mildenhall2021nerf} and Gaussian Splatting (3DGS)~\cite{kerbl20233d} cannot be directly applied to this type of input because they rely on perspective projection. As a result, fisheye images are typically re-projected into linear perspective images, and the resulting non-uniform sampling and stretching pose challenges for accurate camera calibration and scene reconstruction from wide-angle imagery.


Traditionally, imaging systems are pre-calibrated using specialized setups, such as calibration checkerboards. During calibration, polynomial distortion models~\cite{opencv_library, schoenberger2016sfm} are estimated and then used to re-project raw fisheye images into perspective ones for scene reconstruction. There are three important problems with this practice. First, resampling wide-field images leads to severe stretching and non-uniform sampling, as shown in~\cref{fig:hilbert_180}, where perspective images exhibit far less uniform sampling of directions than fisheyes. Second, traditional parametric distortion models lack sufficient expressiveness to accurately model large FOV (\textit{e.g.,} 180\si{\degree}) lenses. Third, pre-calibration prevents lens parameters from being optimized end-to-end along with reconstruction, meaning the final reconstruction does not fully minimize the reconstruction loss.

To avoid the separate pre-calibration stage, many recent works~\cite{moenne20243d, jeong2021self, liao2024fisheye} integrate distortion modeling~\cite{zeller1996camera, pollefeys1999stratified, chandraker2007autocalibration, chandraker2010globally} and optimization into NeRF and 3DGS frameworks. However, even when using photometric loss to refine distortion parameters alongside reconstruction, these approaches still exhibit significant misalignment in peripheral regions due to their reliance on traditional distortion models and single-plane projection. Both issues constrain the ability of current reconstruction frameworks to represent wide-FOV optical systems.


In this work, we introduce \textit{Self-Calibrating Gaussian Splatting}, a differentiable rasterization pipeline that jointly optimizes lens distortion, camera intrinsics, camera poses, and scene representations using 3D Gaussians. Our approach effectively addresses the three aforementioned challenges, achieving high-quality reconstruction from large-FOV images without requiring resampling, pre-calibration, or polynomial distortion models.

To improve lens modeling, we replace the conventional parametric distortion model with a novel hybrid neural field that balances expressiveness and computational efficiency, as illustrated in~\cref{fig:pipeline}. Our method uses invertible residual networks~\cite{behrmann2019invertible} to predict displacements on a normalized sparse grid, followed by bilinear interpolation to generate a continuous distortion field.

To mitigate the stretching caused by resampling to a single-plane perspective, we use cubemap sampling as in \cite{Cohen:1985:hemicube} (\cref{fig:hilbert_cubemap}), which significantly reduces stretching and yields a more uniform pixel density, even in peripheral areas. Gaussians project into each limited-FOV face of the cubemap with bounded distortion.

Finally, our method combines lens calibration with bundle adjustment and scene reconstruction into an integrated end-to-end self-calibration process that minimizes the rendering loss over all unknowns jointly, leading to lower reconstruction errors compared to pre-calibration of distortion and/or camera pose.


To validate our method, we conduct extensive experiments on both synthetic datasets and real-world scenes, including the FisheyeNeRF datasets~\cite{jeong2021self}, our own real-world captured scenes, and a synthetic dataset. Our system effectively calibrates extrinsics, intrinsics, and lens distortion, achieving better reconstruction performance compared to existing methods using uncalibrated fisheye cameras. Importantly, our system is not constrained to a single fisheye camera model; rather, it is designed to be flexible and adaptable, accommodating a wide range of cameras, from perspective to extreme fisheye, without requiring pre-calibration. This flexibility enables our method to fully leverage the unique capabilities of available lenses, ensuring comprehensive scene coverage and high-quality reconstructions.

\input{images/limitation_single_perspective}
