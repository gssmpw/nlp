\begin{abstract}
% Large field-of-view (FOV) cameras offer the potential for high-quality scene reconstructio with less captures, as each capture covers a broader region.
% %
% However, current state-of-the-art reconstruction pipeline, such as Gaussians, fail to fully utilize the advantages of large FOV captures due to incompatibilities between fast rasterization and the modeling of extreme lens distortion.
%
%
% Existing Gaussian Splatting methods typically rely on either cumbersome calibration pre-processes in specially designed settings or structure-from-motion pipelines to estimate camera poses and distortion parameters, which are then used to undistort large FOV images into perspective ones.
%
% These models, however, are not expressive enough to handle extreme lens distortion near 180\si{\degree} regions and capture all properties of large FOV cameras, such as entrance pupil shift.
%
% (Novel view synthesis relies on accurate estimation of camera parameters, including extrinsics, intrinsics, and lens distortions. ) 
%
% At the same time, challenges in camera calibration with traditional parametric distortion models also prevent them from utilizing large FOV.

%
% Existing Gaussian Splatting methods typically rely on either cumbersome calibration pre-processes in specially designed settings or structure-from-motion pipelines to estimate camera poses and distortion parameters, which are then used to undistort large FOV images into perspective ones. 
%
% These models, however, are not expressive enough to handle extreme lens distortion near 180\si{\degree} regions and capture all properties of large FOV cameras, such as entrance pupil shift.
%
% (Novel view synthesis relies on accurate estimation of camera parameters, including extrinsics, intrinsics, and lens distortions. ) 
%
% At the same time, challenges in camera calibration with traditional parametric distortion models also prevent them from utilizing large FOV.
% which offer great potential for streamlining the data capture process.
%


% Large fov can potential more multiview information?

%Large field-of-view (FOV) cameras provide broad coverage and offer additional multiview information compared to narrow FOV cameras. However, existing reconstruction methods struggle to take advantage of large FOV cameras due to extreme distortion and the conversion of large FOV captures into perspective ones.
%Our pipeline leverages large FOV cameras by using a neural network with cubemap sampling. More specifically, we model complex lens distortions using a hybrid field that combines an invertible ResNet with explicit grids. Additionally, we propose a cubemap resampling strategy to support large FOV images without sacrificing resolution in peripheral regions.
%The entire pipeline is differentiable with respect to all intrinsic parameters, including distortion, as well as extrinsic parameters, enabling self-calibration alongside reconstruction. Our method is compatible with the fast rasterization of Gaussian Splatting, supports a wide range of large FOV cameras, adapts to various distortions, and achieves state-of-the-art performance on both synthetic and real-world datasets.

%Proposed alternative: 
Large field-of-view (FOV) cameras can simplify and accelerate scene capture because they provide complete coverage with fewer views. However, existing reconstruction pipelines fail to take full advantage of large-FOV input data because they convert input views to perspective images, resulting in stretching that prevents the use of the full image. Additionally, they calibrate lenses using models that do not accurately fit real fisheye lenses in the periphery.
We present a new reconstruction pipeline based on Gaussian Splatting that uses a flexible lens model and supports fields of view approaching 180 degrees. We represent lens distortion with a hybrid neural field based on an Invertible ResNet and use a cubemap to render wide-FOV images while retaining the efficiency of the Gaussian Splatting pipeline. Our system jointly optimizes lens distortion, camera intrinsics, camera poses, and scene representations using a loss measured directly against the original input pixels.
We present extensive experiments on both synthetic and real-world scenes, demonstrating that our model accurately fits real-world fisheye lenses and that our end-to-end self-calibration approach provides higher-quality reconstructions than existing methods.
More details and videos can be found at the project page: \url{https://denghilbert.github.io/self-cali/}.





\end{abstract}






