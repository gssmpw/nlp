\clearpage
% \setcounter{page}{1}
\maketitlesupplementary
\setcounter{section}{0}

{
% Format settings for TOC
\renewcommand{\cftsecafterpnum}{\vskip15pt}
\setlength{\cftsecindent}{1em}
\setlength{\cftbeforesecskip}{0.5em}

% First exclude all sections from TOC
\setcounter{tocdepth}{-1}

% Then add only supplementary sections to TOC
\begingroup
\renewcommand{\contentsname}{Content}
\tableofcontents
\endgroup

% After the TOC, manually add entries for just supplementary sections
\addtocontents{toc}{\protect\setcounter{tocdepth}{1}}
}


\section{Supplementary Video}
We provide a video, ``supp\_video.mp4," to better compare our method with baselines. Our video is organized into three parts.

The first part presents a comparison between our method and baselines on the FisheyeNeRF dataset~\cite{jeong2021self} across three scenes. 
Vanilla 3DGS~\cite{kerbl20233d} completely fails to reconstruct the scenes because lens distortion is not accounted for. 
Fisheye-GS~\cite{liao2024fisheye} adopts a parametric model, but the peripheral regions produce blurry results, as highlighted by the red box in the corners. 
In contrast, our method achieves clean and sharp reconstructions.

The second part of the video shows reconstruction results using our method on walk-around captures, including both synthetic and real-world scenes. 
Our approach achieves sharp and clean renderings upon completing the reconstruction. 

The third part of the video compares visualizations of our method with a conventional reconstruction pipeline that either uses narrow-FOV perspective images or undistorted images from COLMAP~\cite{schoenberger2016sfm} as input. 
Our method successfully reconstructs larger regions, particularly for scenes captured with large 180\si{\degree} cameras. Besides, we render videos in fisheye views for each scene.

Additionally, we provide ``opt\_pose.mp4," which illustrates the camera changes during optimization, and ``fisheye-gs\_failure.mp4," which demonstrates failure cases of Fisheye-GS~\cite{liao2024fisheye} in extremely large-FOV settings.

We strongly encourage all reviewers to watch the provided video for a more comprehensive visual understanding of our results.


\section{Optimization of Camera Parameters}
In this section, we first derive the gradients for all camera parameters during training in~\cref{sec:2.1}. We then demonstrate the effectiveness of the joint optimization of distortion alongside extrinsic and intrinsic parameters in~\cref{sec:2.2}. Finally, we evaluate our camera optimization on synthetic NeRF~\cite{mildenhall2021nerf} scenes with significant noise in~\cref{sec:2.3}.

\subsection{Derivation of Gradient}
\label{sec:2.1}
We first derive how to compute the gradient for a pinhole camera and then extend the derivation to account for distortion.

As defined above, the gradient of camera parameters can be written as:
\begin{align}
\label{eq:simpify-grad}
\fpp{\lL}{\Theta} = \sum_{j=1}^{|G|} \fpp{\lL}{C_j}\fpp{C_j}{\Theta} + \fpp{\lL}{\mu^{2D}_j}\fpp{\mu^{2D}_j}{\Theta} + \fpp{\lL}{\Sigma^{2D}_j}\fpp{\Sigma^{2D}_j}{\Theta}.
\end{align}

For the color term, we first define the input of the spherical harmonics stored by each Gaussian as the view direction. The 3D location of a Gaussian in world coordinates is $\textbf{X}_w=[x, y, z]^T$, and the camera center is $\textbf{C}=[C_x, C_y, C_z]^T$:
\begin{align}
    \omega_j = [x - C_x, y - C_y, z - C_z]^T.
\end{align}
Then, the derivative with respect to the camera center is:
\begin{align}
    \sum_{j=1}^{|G|} \fpp{\lL}{C_j}\fpp{C_j}{\Theta}&=\sum_{j=1}^{|G|} \fpp{\lL}{C_j}\fpp{C_j}{\omega_j}\fpp{\omega_j}{\textbf{C}}\\
    &=\sum_{j=1}^{|G|} \fpp{\lL}{C_j}\fpp{C_j}{\omega_j}(-\fpp{\omega_j}{\textbf{X}}).
\end{align}

For now, we only compute the derivative with respect to the camera center. However, the gradient can be easily propagated back since the camera center corresponds to the translation vector of the inverse of the world-to-camera matrix.

Next, we derive the gradient from the projected 2D Gaussian position $\mu_j^{2D}$. For simplicity, we integrate the intrinsic and extrinsic parameters into a projection matrix $\textbf{P}$ since we do not yet consider distortion. The projection from a 3D Gaussian in world coordinates is defined by:
\begin{align}
    \relax[\Tilde{x}, \Tilde{y}, \Tilde{z}, \Tilde{\omega}]^T=\textbf{P}[x,y,z,1]^T.
\end{align}

We apply a projective transformation:
\begin{equation}
\begin{aligned}
    f(x,y,z)=x'=\frac{\Tilde{x}}{\Tilde{\omega}}=\frac{p_0x+p_4y+p_8z+p_{12}}{p_3x+p_7y+p_{11}z+p_{15}}, \\ 
    g(x,y,z)=y'=\frac{\Tilde{y}}{\Tilde{\omega}}=\frac{p_1x+p_5y+p_9z+p_{13}}{p_3x+p_7y+p_{11}z+p_{15}},\\
    z'=\frac{\Tilde{z}}{\Tilde{\omega}}=\frac{p_2x+p_6y+p_{10}z+p_{14}}{p_3x+p_7y+p_{11}z+p_{15}},
\end{aligned}
\end{equation}
where
\begin{align}
    \textbf{P}=\begin{bmatrix}
    p_0 & p_4 & p_8 & p_{12} \\
    p_1 & p_5 & p_9 & p_{13} \\
    p_2 & p_6 & p_{10} & p_{14} \\
    p_3 & p_7 & p_{11} & p_{15} \\
    \end{bmatrix}.
\end{align}

Then, we project from NDC to screen space to obtain the 2D location $\mu^{2D}=(u, v)$:
\begin{align}
    u=\frac{(f(x,y,z)+1)W-1}{2},\\
    v=\frac{(g(x,y,z)+1)H-1}{2}.
\end{align}

The second term can be represented as:
\begin{align}
    \fpp{\lL}{\mu^{2D}_j}\fpp{\mu^{2D}_j}{\Theta}=\fpp{\lL}{\mu^{2D}_j}\fpp{\mu^{2D}_j}{[x', y']^T}\fpp{[x', y']^T}{\textbf{P}}.
\end{align}

We compute the gradients for each $p_i$ in the projection matrix $\textbf{P}$:
\begin{equation}
    \begin{aligned}
        \frac{\partial L}{\partial p_0} =& \frac{\partial L}{\partial u}\frac{W}{2}\frac{x}{\Tilde{\omega}},\\
        \frac{\partial L}{\partial p_4} =& \frac{\partial L}{\partial u}\frac{W}{2}\frac{y}{\Tilde{\omega}},\\
        \frac{\partial L}{\partial p_8} =& \frac{\partial L}{\partial u}\frac{W}{2}\frac{z}{\Tilde{\omega}},\\
        \frac{\partial L}{\partial p_{12}} =& \frac{\partial L}{\partial u}\frac{W}{2}\frac{1}{\Tilde{\omega}},\\        
        \frac{\partial L}{\partial p_1} =& \frac{\partial L}{\partial v}\frac{H}{2}\frac{x}{\Tilde{\omega}},\\
        \frac{\partial L}{\partial p_5} =& \frac{\partial L}{\partial v}\frac{H}{2}\frac{y}{\Tilde{\omega}},\\
        \frac{\partial L}{\partial p_9} =& \frac{\partial L}{\partial v}\frac{H}{2}\frac{z}{\Tilde{\omega}},\\
        \frac{\partial L}{\partial p_{13}} =& \frac{\partial L}{\partial v}\frac{H}{2}\frac{1}{\Tilde{\omega}},
    \end{aligned}
\end{equation}
and
\begin{equation}
    \begin{aligned}
        \frac{\partial L}{\partial p_3}=&\frac{\partial L}{\partial u}(-x')\left(\frac{W}{2}\frac{x}{\Tilde{\omega}}\right)+\frac{\partial L}{\partial v}(-y')\left(\frac{H}{2}\frac{x}{\Tilde{\omega}}\right),\\
        \frac{\partial L}{\partial p_7}=&\frac{\partial L}{\partial u}\frac{-\Tilde{x}}{\Tilde{\omega}}\left(\frac{W}{2}\frac{y}{\Tilde{\omega}}\right)+\frac{\partial L}{\partial v}\frac{-\Tilde{y}}{\Tilde{\omega}}\left(\frac{H}{2}\frac{y}{\Tilde{\omega}}\right),\\
        \frac{\partial L}{\partial p_{11}}=& \frac{\partial L}{\partial u}\frac{-\Tilde{x}}{\Tilde{\omega}}\left(\frac{W}{2}\frac{z}{\Tilde{\omega}}\right)+\frac{\partial L}{\partial v}\frac{-\Tilde{y}}{\Tilde{\omega}}\left(\frac{H}{2}\frac{z}{\Tilde{\omega}}\right),\\
        \frac{\partial L}{\partial p_{15}}=&\frac{\partial L}{\partial u}\frac{W}{2}\frac{-\Tilde{x}}{\Tilde{\omega}^2}+\frac{\partial L}{\partial v}\frac{H}{2}\frac{-\Tilde{y}}{\Tilde{\omega}^2}.
    \end{aligned}
\end{equation}

The gradient flow back to intrinsic and extrinsic parameters can be easily computed since $\textbf{P}=K[R|t]$.

Finally, we compute the last term. The 2D covariance $\Sigma^{2D}$ depends only on the view matrix, so instead of using $\textbf{P}$, we use only the view matrix (\ie, the world-to-camera matrix) $\textbf{V}$, and the transformed 3D location in camera coordinates is $\textbf{X}_c=[x_c, y_c, z_c]^T$:
\begin{equation}
    \begin{aligned}
        \relax[x_c, y_c, z_c]^T&=\textbf{V}[x,y,z,1]^T\\
        &=\begin{bmatrix}
        v_{0} & v_{4} & v_{8} & v_{12}\\
        v_{1} & v_{5} & v_{9} & v_{13}\\
        v_{2} & v_{6} & v_{10} & v_{14}\\
        \end{bmatrix}
        \cdot\begin{bmatrix}
        x\\
        y\\
        z\\
        1
        \end{bmatrix}.
    \end{aligned}
\end{equation}


We compute the 2D covariance as follows, given known focal lengths $f_x$ and $f_y$:

\begin{equation}
    \begin{aligned}
        J &= \begin{bmatrix}
        J_{00} & J_{01} & J_{02}\\
        J_{10} & J_{11} & J_{12}\\
        J_{20} & J_{21} & J_{22}
        \end{bmatrix} = \begin{bmatrix}
        \frac{f_x}{z_c} & 0 & \frac{-f_x\cdot x_c}{z_c^2}\\
        0 & \frac{f_y}{z_c} & \frac{-f_y\cdot y_c}{z_c^2}\\
        0 & 0 & 0
        \end{bmatrix},\\
         \textbf{W} &= \begin{bmatrix}
        v_{0} & v_{4} & v_{8} \\
        v_{1} & v_{5} & v_{9} \\
        v_{2} & v_{6} & v_{10}
        \end{bmatrix}= \begin{bmatrix}
        w_{00} & w_{01} & w_{02} \\
        w_{10} & w_{11} & w_{12} \\
        w_{20} & w_{21} & w_{22}
        \end{bmatrix},\\ 
        \textbf{T}&=\textbf{W}\cdot J=\begin{bmatrix}
        T_{00} & T_{10} & T_{20} \\
        T_{01} & T_{11} & T_{21} \\
        T_{02} & T_{12} & T_{22}
        \end{bmatrix},\\
        \Sigma^{2D}&=\textbf{T}^T\cdot\Sigma\cdot \textbf{T}=\textbf{T}^T\cdot\begin{bmatrix}
        c_{0} & c_{1} & c_{2} \\
        c_{1} & c_{3} & c_{4} \\
        c_{2} & c_{4} & c_{5}
        \end{bmatrix}\cdot \textbf{T},\\
        a&=\Sigma^{2D}[0][0],\quad b=\Sigma^{2D}[0][1],\quad c=\Sigma^{2D}[1][1],\\
        \textbf{Conic}&=\begin{bmatrix}
            a & b\\
            b & c
        \end{bmatrix}^{-1}=\begin{bmatrix}
            \frac{c}{ac-b^2} & \frac{-b}{ac-b^2}\\
            \frac{-b}{ac-b^2} & \frac{a}{ac-b^2}
        \end{bmatrix}.
    \end{aligned}
\end{equation}

Since we only extract the upper three elements of $\Sigma^{2D}$, we do not compute gradients for $T_{20}$, $T_{21}$, and $T_{22}$.

\begin{equation}
    \begin{aligned}
        \frac{\partial L}{\partial v_0}=&\frac{\partial L}{\partial T_{00}}\frac{f_x}{z_c},
        \quad \frac{\partial L}{\partial v_1}=\frac{\partial L}{\partial T_{01}}\frac{f_x}{z_c},\\
        \frac{\partial L}{\partial v_4}=&\frac{\partial L}{\partial T_{10}}\frac{f_y}{z_c},
        \quad \frac{\partial L}{\partial v_5}=\frac{\partial L}{\partial T_{11}}\frac{f_y}{z_c},\\
        \frac{\partial L}{\partial v_2}=&\frac{-(v_0\frac{\partial L}{\partial T_{00}}+v_1\frac{\partial L}{\partial T_{01}})\cdot x\cdot f_x}{z_c^2}\\
        &+\frac{-(v_4\frac{\partial L}{\partial T_{10}}+v_5\frac{\partial L}{\partial T_{11}}+v_6\frac{\partial L}{\partial T_{12}})\cdot x\cdot f_y}{z_c^2}\\
        &+\frac{(z_c-v_2x)f_x\frac{\partial L}{\partial T_{02}}}{z_c^2},\\
        \frac{\partial L}{\partial v_6}=&\frac{-(v_0\frac{\partial L}{\partial T_{00}}+v_1\frac{\partial L}{\partial T_{01}}+v_2\frac{\partial L}{\partial T_{02}})\cdot y\cdot f_x}{z_c^2}\\
        &+\frac{-(v_4\frac{\partial L}{\partial T_{10}}+v_5\frac{\partial L}{\partial T_{11}})\cdot y\cdot f_y}{z_c^2}\\
        &+\frac{(z_c-v_6y)f_y\frac{\partial L}{\partial T_{12}}}{z_c^2},\\ 
        \frac{\partial L}{\partial v_{10}}=&\frac{-((v_0\frac{\partial L}{\partial T_{00}}+v_1\frac{\partial L}{\partial T_{01}}+v_2\frac{\partial L}{\partial T_{02}})f_x}{z_c^2}\\
        &+\frac{(v_4\frac{\partial L}{\partial T_{10}}+v_5\frac{\partial L}{\partial T_{11}}+v_6\frac{\partial L}{\partial T_{12}})f_y)z}{z_c^2},\\
        \frac{\partial L}{\partial v_{14}}=&\frac{-((v_0\frac{\partial L}{\partial T_{00}}+v_1\frac{\partial L}{\partial T_{01}}+v_2\frac{\partial L}{\partial T_{02}})f_x}{z_c^2}\\
        &+\frac{(v_4\frac{\partial L}{\partial T_{10}}+v_5\frac{\partial L}{\partial T_{11}}+v_6\frac{\partial L}{\partial T_{12}})f_y}{z_c^2}.
    \end{aligned}
\end{equation}

To account for distortion, we decompose the projection matrix into two separate operations. Following the definitions above, we apply an invertible ResNet in between:

\begin{align}
    \relax[\Tilde{x}, \Tilde{y}, \Tilde{z}, \Tilde{\omega}]^T={K\homo{z_c\cdot\homo{\dD_\theta\paren{\proj{x_c, y_c, z_c}}}}}.
\end{align}

For the projection of the Gaussian covariance $\Sigma$, we compute a new Jacobian matrix $J$ using the distorted $\textbf{X}_c$:

\begin{align}
    [x_c, y_c, z_c]^T=z_c\cdot\homo{\dD_\theta\paren{\proj{x_c, y_c, z_c}}}.
\end{align}

Both processes are differentiable, allowing us to compute the intermediate Jacobian for $\dD_\theta$ to update the invertible ResNet.


\begin{figure}[t]
    \centering
    \setlength{\tabcolsep}{1pt} % Adjust space between columns if needed
    \begin{tabular}{cc} % 4 columns
        \subcaptionbox{w/o Cameras Optimization\label{fig:left}}{
            \includegraphics[width=0.22\textwidth]{rebuttal_img/cube_wooptpose_vis.jpg}
        } &

        % Fourth image with label (c)
        \subcaptionbox{w/ Cameras Optimization\label{fig:right}}{
            \includegraphics[width=0.22\textwidth]{rebuttal_img/cube_optpose_vis.jpg}
        }
    \end{tabular}
    \vspace{-0.5em}
    \caption{\textbf{Camera Parameters Optimization}. We initialize noisy cameras from COLMAP~\cite{schoenberger2016sfm}. (a) Modeling fisheye distortion without optimizing camera parameters, while (b) jointly optimizing both in a self-calibration manner. Our full model can recover accurate lens distortion and camera parameters simultaneously.}
    \label{fig:pose_distortion}
    \vspace{-1.5em}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{images/psnr_with_noise.jpg}
     \caption{Visual comparison of (a) the initial perturbed ($s=0.15$) and GT poses and (b) optimized camera poses in the Lego scene. The chart demonstrates the different level of perturbations and the effectiveness of our optimization. Our method successfully recovers accurate camera frames. }
    \label{fig:robust_to_noise}   
\end{figure}

\input{images/fisheyenerf_perturb_optcam}


\begin{figure*}[t]
    \centering
    \setlength{\tabcolsep}{0.1em} 
    \renewcommand{\arraystretch}{0.91}
    \begin{tabular}{cccccc}
        \includegraphics[height=0.157\textwidth]{images/pose_fig/lego_camp.jpg} &  
        \includegraphics[height=0.157\textwidth]{images/pose_fig/lego_ours.jpg} &  
        \includegraphics[height=0.157\textwidth]{images/pose_fig/lego_gt.jpg} &  
        \includegraphics[height=0.157\textwidth]{images/pose_fig/drum_camp.jpg} 
        \includegraphics[height=0.157\textwidth]{images/pose_fig/drum_ours.jpg} &  
        \includegraphics[height=0.157\textwidth]{images/pose_fig/drum_gt_.jpg} 
        \\
        \multicolumn{3}{c}{(a) Lego} & \multicolumn{3}{c}{(b) Drums}
        \\
        \includegraphics[height=0.157\textwidth]{images/pose_fig/material_camp.jpg} &  
        \includegraphics[height=0.157\textwidth]{images/pose_fig/material_ours.jpg} &  
        \includegraphics[height=0.157\textwidth]{images/pose_fig/material_gt.jpg} &  
        \includegraphics[height=0.157\textwidth]{images/pose_fig/chair_camp.jpg} 
        \includegraphics[height=0.157\textwidth]{images/pose_fig/chair_ours.jpg} &  
        \includegraphics[height=0.157\textwidth]{images/pose_fig/chair_gt.jpg} 
        \\
        \multicolumn{3}{c}{(c) Material} & \multicolumn{3}{c}{(d) Chair}
        \\
        &&&&&\\
    \end{tabular}
     \caption{We carry qualitative comparison with CamP at noise level 0.15. Each scene show CamP, our method, and the ground truth, from left to right. Our method is able to produce sharp renderings at this noise level, where CamP fails.}
    \label{fig:qualitative-perturb}
\end{figure*}

\subsection{Joint Distortion and Pose Optimization}
\label{sec:2.2}
Our approach supports efficient optimization of camera parameters, either independently (with perspective images) or in combination, as shown in~\cref{fig:pose_distortion}, with distortion modeling. This ensures that our pipeline remains robust even when both distortion and camera parameters are inaccurate.

To evaluate our model's ability to jointly optimize lens distortion and other camera parameters in real-world scenes, we introduce a setting where both camera extrinsics and intrinsics are perturbed in the FisheyeNeRF dataset. 
Specifically, we add Gaussian noise with a standard deviation of $s=0.15$ to each image's camera extrinsic and intrinsic parameters.
Despite this additional noise, our method successfully recovers the lens distortions while generating high-quality novel-view synthesis renderings (\cref{fig:fisheyenerf_Perturb}).

One key observation is that COLMAP~\cite{schoenberger2016sfm} can robustly estimate camera extrinsics but struggles with intrinsic parameters and distortion. When we enable extrinsic optimization during reconstruction, the camera poses are refined only slightly, indicating that the initial poses are already quite reliable. Regarding lens distortion, as demonstrated in \cref{fig:transposed_opti_iresnet} and discussed in \textbf{Hybrid Field} of Sec.4.4 and\cref{sec:inaccurate_colmap_distortion}, COLMAP's distortion estimation lacks accuracy, highlighting the necessity of our hybrid distortion field for improved expressiveness and precision.


We hypothesize that this limitation stems from the structure-from-motion (SfM) pipeline~\cite{schoenberger2016sfm} in COLMAP. COLMAP primarily utilizes the central region of raw images (corresponding to a small FOV), where conventional distortion models perform well. Consequently, lens distortion has minimal influence on extrinsic estimation, as COLMAP can still rely on the image center to solve linear equations. However, when attempting to leverage the full FOV of raw images for reconstruction, the limitations of a fixed distortion model and a single-plane projection become pronounced. 

To validate the self-calibration capability of our pipeline, we manually introduce noise into the extrinsics produced by COLMAP and jointly optimize extrinsics, intrinsics, and distortion. As shown in \cref{fig:fisheyenerf_Perturb}, our approach effectively refines all camera parameters and distortion simultaneously. Without extrinsic and intrinsic optimization, the hybrid field can only predict coarse distortion, while misaligned poses contribute to the blurry reconstruction seen in~\cref{fig:fisheyenerf_Perturb} (a). Even with significant perturbations in poses and intrinsics, our method robustly recovers accurate camera parameters and the distortion field after training, as illustrated in~\cref{fig:fisheyenerf_Perturb} (b).



\input{tables/pose_table}
\begin{table}[htbp]
  \centering
  \scalebox{0.85}{
    \begin{tabular}{c|cccccc}
    \toprule
    \textbf{Scenes} & \textbf{Metric} & \textbf{0} & \textbf{0.1} & \textbf{0.15} & \textbf{0.2} & \textbf{0.25} \\
    \midrule
    \multirow{3}{*}{Chair} & SSIM & 0.987 & 0.988 & 0.988 & 0.987 & 0.980 \\
                           & PSNR & 35.81 & 36.10 & 35.99 & 35.83 & 34.35 \\
                           & LPIPS & 0.012 & 0.012 & 0.012 & 0.012 & 0.019 \\
    \midrule
    \multirow{3}{*}{Lego}  & SSIM & 0.983 & 0.974 & 0.972 & 0.969 & 0.965 \\
                           & PSNR & 35.77 & 34.78 & 34.29 & 33.70 & 33.15 \\
                           & LPIPS & 0.015 & 0.021 & 0.023 & 0.025 & 0.029 \\
    \midrule
    \multirow{3}{*}{Drums} & SSIM & 0.955 & 0.954 & 0.953 & 0.953 & 0.946 \\
                           & PSNR & 26.17 & 26.15 & 26.04 & 26.01 & 25.30 \\
                           & LPIPS & 0.037 & 0.038 & 0.039 & 0.040 & 0.045 \\
    \midrule
    \multirow{3}{*}{Materials} & SSIM & 0.960 & 0.960 & 0.951 & 0.942 & 0.843 \\
                               & PSNR & 29.99 & 29.93 & 28.91 & 27.95 & 15.17 \\
                               & LPIPS & 0.034 & 0.036 & 0.044 & 0.052 & 0.158 \\
    \midrule
    \multirow{3}{*}{Mic} & SSIM  & 0.991 & 0.989 & 0.987 & 0.974 & 0.923 \\
                         & PSNR  & 35.34 & 34.58 & 33.65 & 30.07 & 18.78 \\
                         & LPIPS & 0.006 & 0.008 & 0.010 & 0.019 & 0.087 \\
    \midrule
    \multirow{3}{*}{Ship} & SSIM  & 0.907 & 0.873 & 0.776 & 0.719 & 0.700 \\
                          & PSNR  & 30.91 & 28.66 & 20.96 & 16.80 & 15.10 \\
                          & LPIPS & 0.106 & 0.126 & 0.203 & 0.262 & 0.294 \\
    \midrule
    \multirow{3}{*}{Ficus} & SSIM  & 0.987 & 0.987 & 0.984 & 0.955 & 0.859 \\
                           & PSNR  & 34.85 & 34.84 & 33.99 & 28.08 & 18.54 \\
                           & LPIPS & 0.012 & 0.012 & 0.014 & 0.039 & 0.125 \\
    \midrule
    \multirow{3}{*}{Hotdog} & SSIM  & 0.985 & 0.985 & 0.985 & 0.983 & 0.982 \\
                            & PSNR  & 37.67 & 37.65 & 37.63 & 37.05 & 36.53 \\
                            & LPIPS & 0.020 & 0.020 & 0.020 & 0.022 & 0.025 \\
    \bottomrule
    \end{tabular}%
  }
  \caption{Quantitative Comparison on the Perturbed Synthetic Dataset}
\label{tab:perturb-syn}%
\end{table}%

\input{tables/supp_colmapinit_opt}
\input{images/fixed_iresnet_w_colmap_init}
\subsection{Pure Camera Optimization} 
\label{sec:2.3}
We use the NeRF-Synthetic dataset~\cite{mildenhall2021nerf}, which includes known ground truth camera poses. 
The dataset contains 100 viewpoints for training and 200 additional viewpoints for computing test error metrics.
We first add noise to perturb the rotation angles of camera extrinsics, the positions of the camera centers, and the focal lengths. 
These noisy cameras are used to train both the baselines and our method.
We compare our method with CamP~\cite{park2023camp}, implemented on ZipNeRF~\cite{barron2023zip}, a state-of-the-art method for joint optimization of 3D scenes and camera extrinsics and intrinsics. In addition to CamP, we also report the performance of vanilla Gaussian Splatting without pose optimization.

The models are evaluated on two criteria, following the protocol in CamP~\cite{park2023camp}.
First, we measure the accuracy of the estimated camera poses in the training views after accounting for a global rigid transformation. 
Second, we measure the quality of rendered images at the held-out camera poses after a test-time optimization to adjust for a potential global offset. 
\cref{tab:quantitative_perturb_synthetic} shows that our method outperforms both vanilla 3DGS and CamP in both image and camera metrics by a large margin.

We further evaluate our model by perturbing the camera poses with varying levels of noise. Specifically, we add Gaussian noise with a standard deviation ranging from 0 to 0.3 to the camera poses. For reference, we report the increased noise levels for each scene in~\cref{tab:perturb-syn}. Handling larger noise in camera poses presents a significant challenge. As shown in \cref{fig:robust_to_noise} and \cref{fig:qualitative-perturb}, CamP’s performance drops significantly due to its tendency to fall into local minima when the initial camera poses contain substantial errors. In contrast, our method exhibits much slower degradation in novel-view synthesis performance.

We also include the video ``pose\_opt.mp4" to visualize the optimization process.


\section{Distortion Estimation from COLMAP}
\label{sec:inaccurate_colmap_distortion}
In practice, the distortion estimated from the SfM~\cite{schoenberger2016sfm} pipeline can be used as an initialization for our hybrid field, stabilizing training and accelerating convergence. However, these parameters are inaccurate when derived from highly distorted images. We verify the necessity of optimizing our hybrid field during reconstruction both quantitatively and qualitatively.

To assess this, we fit the COLMAP~\cite{schoenberger2016sfm} distortion parameters within our hybrid field and freeze the network. As shown in~\cref{tab:colmap_init_opt}, the distortion initialization from COLMAP improves upon Vanilla 3DGS~\cite{kerbl20233d} but remains far from accurate in modeling distortion compared to our approach. While the hybrid field produces a roughly correct distortion field, it results in blurry reconstructions, as shown in~\cref{fig:transposed_opti_iresnet}. This degradation is particularly noticeable in scenes with many straight lines, such as the jalousie windows in the chairs scene. The photometric evaluation in~\cref{tab:colmap_init_opt} demonstrates that our fully optimized hybrid field significantly outperforms the estimated distortion parameters. These results highlight the importance of optimizing our hybrid field for achieving more accurate reconstruction and distortion modeling.

\section{Computational Efficiency}
\paragraph{Training Time.}
To verify the hypothesis that our hybrid method achieves a better balance between expressiveness and efficiency, we perform an ablation study on the FisheyeNeRF dataset~\cite{jeong2021self}. 
Specifically, we analyze the grid resolution of $\textbf{P}_c$, which determines the number of evaluations required for the most computationally expensive part of the pipeline—the invertible ResNet. 
\cref{tab:ablation_control_pts} reports the PSNR and training time for three different grid resolutions, as well as for Fisheye-GS~\cite{liao2024fisheye} and vanilla 3DGS~\cite{kerbl20233d}. 
We observe that increasing the resolution of $\textbf{P}_c$ leads to better performance but longer training time. Further reducing the grid resolution does not significantly shorten computation time, as other operations, such as gradient computation for camera parameters, begin to dominate. Fortunately, all hybrid solutions significantly outperform vanilla 3DGS and Fisheye-GS. The lowest-resolution setting we tested introduces only a $7$-minute training time overhead compared to 3DGS, in exchange for a $>8$ dB boost in PSNR.


\input{images/control_pts_resolution}
\input{tables/training_time}
\paragraph{Control Grid Resolution.}
A higher-resolution control grid results in a smoother distortion field representation but slower training. To better illustrate the effect of control grid resolution, we visualize two types of resolutions (\textit{i.e.,} 265 $\times$ 149 and 66 $\times$ 37). While there are no significant differences in the central region, the distortion at the edges is better recovered with a higher-resolution grid. Since the distortion field becomes smoother, a high-resolution control grid produces more accurate distorted lines, as shown in the red and blue boxes of~\cref{fig:control_pts_resolution}.

\section{Extra Experiments}
\input{tables/quantitative_fov_large_small_netflix}
\subsection{Quantitative Comparisons with Fisheye-GS}
In addition to Fig.~5 in the main paper, we also provide a quantitative evaluation of our method compared with the baseline Fisheye-GS~\cite{liao2024fisheye} in~\cref{tab:quantitative_large_small_netflix} and~\cref{tab:quantitative_mitsuba}. 
The performance degradation observed in the baseline method is primarily due to the limitations of the conventional camera distortion model used during reconstruction, which struggles at the edges of large FOVs. 
As a result, there is no geometric consistency in the peripheral regions to produce uniform gradients for optimizing the Gaussians.
When rendering novel views, these Gaussians appear as large floaters that occlude the camera view. We further visualize this phenomenon in the failure case video reconstructed by Fisheye-GS~\cite{liao2024fisheye}. The center area is revealed as we gradually decrease the scale of the Gaussians.
Please refer to our supplementary video ``fisheye-gs\_failure.mp4" for a comparison with the parametric distortion model.

\input{images/ablation_cubemap}

\subsection{Qualitative Results of Cubemap Ablation}
As illustrated in Fig.~2 of the main paper, we apply a cubemap to overcome the limitations of perspective projection. Even with our hybrid distortion field, we can only utilize the central region of raw fisheye images, as shown in the bottom row of~\cref{fig:ablation_cubemap}. In contrast, rendering a cubemap enables us to achieve a larger FOV, allowing us to compute the photometric loss with reference raw images for optimization, as demonstrated in Fig.~7 of the main paper. We also provide a quantitative evaluation in Tab.~4 of the main paper, showing that the final reconstruction quality is compromised when using a single planar projection.

Additionally, the boundary of the final distorted rendering is irregular, primarily because the distortion information at the boundary heavily relies on extra regions that are not covered by single planar projections.

\subsection{Qualitative Comparisons with Different Numbers of Input Views}
We report quantitative results in Tab.~2 of the main paper, where our method outperforms the baseline even with significantly fewer input views, down to 10-15\%. 

To illustrate the impact of varying input view counts, we visualize the rendering quality as the number of input views decreases. Even with as few as 25 input views, our method still achieves reasonable performance, as shown in~\cref{fig:view_num}. This demonstrates our method's ability to effectively utilize large FOV cameras and achieve comprehensive scene coverage during reconstruction.


\begin{figure*}[t]
    \centering
    \setlength{\tabcolsep}{1pt}
    \scalebox{1}{
    \begin{tabular}{cccc}
        \includegraphics[width=0.23\textwidth]{images/view_num/kitchen_100.jpg} &
        \includegraphics[width=0.23\textwidth]{images/view_num/kitchen_50.jpg} &
        \includegraphics[width=0.23\textwidth]{images/view_num/kitchen_25.jpg} &
        \includegraphics[width=0.23\textwidth]{images/view_num/kitchen_10.jpg}
        \\

        \includegraphics[width=0.23\textwidth]{images/view_num/living_100.jpg} &
        \includegraphics[width=0.23\textwidth]{images/view_num/living_50.jpg} &
        \includegraphics[width=0.23\textwidth]{images/view_num/living_25.jpg} &
        \includegraphics[width=0.23\textwidth]{images/view_num/living_10.jpg}
        \\
        \multicolumn{1}{c}{(a) 100 Views} & \multicolumn{1}{c}{(b) 50 Views} & \multicolumn{1}{c}{(c) 25 Views} & \multicolumn{1}{c}{(d) 10 Views}
        \\
    \end{tabular}
    }
    \caption{\textbf{Qualitative Evaluation of Reconstruction with Varying Numbers of Large FOV Inputs}. Our method achieves high-quality reconstruction even with a relatively small number of input images, thanks to our hybrid distortion representation and cubemap resampling.}
    \label{fig:view_num}
\end{figure*}


\subsection{Comparisons with Regular FOV Cameras}
\paragraph{Synthetic Mitsuba Scene Captures.} To carefully control the experimental settings, we customized a camera module in the Mitsuba ray tracer~\cite{jakob2010mitsuba} using camera parameters derived from DSLR lenses, as profiled in the open-source Lensfun~\cite{lensfun}. 
We also utilize 3D assets, including geometry, materials, and lighting, from~\cite{resources16} to produce renderings.
Our synthetic dataset contains three large indoor scenes and four object-centric scenes. 
All indoor scenes follow a Sigma 180\si{\degree} circular fisheye camera. 
Two of the object scenes are rendered with a 120\si{\degree} fisheye lens, while the others are rendered with classic radial distortion. 

To capture detailed perspectives of the scene, cameras are placed close to the objects at varying distances. For Room2, since objects are uniformly distributed in the space, we placed a set of camera centers along a Hilbert curve, then oriented each fixed camera center to cover the surroundings. The number of images captured at each point is reduced for 180\si{\degree} images compared to those with a 90\si{\degree} FOV, as shown in the number of captures in~\cref{tab:decrease_number_each_scene}.

\input{tables/decrease_mitsuba_number}

\input{images/qualitative_mitsuba}

\input{images/supp_real_world}
\input{images/supp_garden}

\paragraph{Qualitative Results.}
To verify the accuracy of self-calibration, we generate a set of hold-out cameras that share the same distribution as the training set. For each validation camera, we render paired perspective and 180\si{\degree} fisheye images. Unlike real-world datasets such as Garden and Studio, our synthetic dataset allows direct comparison with ground-truth perspective views. As shown in~\cref{fig:largefov_vs_smallfov_mitsuba_in_perspective}, we render an additional 20\si{\degree} for the hold-out cameras to highlight the difference in coverage between our method and conventional capture approaches.

It is worth noting that in the comparison between our method and 3DGS~\cite{kerbl20233d}, we evaluate in perspective views since 3DGS~\cite{kerbl20233d} does not support fisheye rendering, whereas our method can generate perspective views after training. In contrast, we directly compare our method with Fisheye-GS~\cite{liao2024fisheye}, as both methods natively support fisheye rendering.

\subsection{More Real-world Scenes}
\paragraph{Scene Captures.}
We use a Canon 5D Mk III DSLR camera with a Canon 8mm-15mm fisheye lens, zoomed out to 8mm with a 180\si{\degree} FOV, to capture a complex indoor office scene, where we place models and spheres on a table. Images are taken close to the table to capture the details of the various models and spheres. We also use a Meike 3.5mm f/2.8 ultra-wide-angle circular fisheye lens to capture the same office. 

Additionally, we mount two fisheye cameras on a rig configured such that the cameras are perpendicular to each other. This camera rig is used to capture a backyard scene by walking clockwise and counterclockwise twice to record videos. The benefit of using this rig is that the relative pose between the two fisheye cameras is fixed, simplifying the SfM~\cite{schoenberger2016sfm} pipeline for estimating accurate poses.

\paragraph{Qualitative Results.}
Qualitative results are shown in~\cref{fig:supp_paul_office}. Our method effectively recovers details in the central region while accurately modeling lens distortion for background elements, such as painting frames and lines on the white wall.

As shown in~\cref{fig:supp_garden}, our method converges to an accurate calibration, ensuring that lines on the house's surface and the ladder leaning against the tree remain straight when rendered in perspective views.


\subsection{Adaptability to Different Lens Distortions}

\begin{table}[t]
  \centering
  \scalebox{1}{
    \begin{tabular}{cccc} 
    \toprule
    Method & PSNR & SSIM & LPIPS \\ 
    \midrule
    Vanilla-GS~\cite{kerbl20233d}  & 25.37 & 0.923 & 0.121 \\
    Ours                           & \textbf{28.00} & \textbf{0.932} & \textbf{0.093} \\
    \bottomrule
    \end{tabular}
  }
  \caption{\textbf{Evaluation on Radial Distortion of Mitsuba Synthetic Scenes}. We compare our method with 3DGS~\cite{kerbl20233d} in object-centric scenes with slight radial distortion, where our method still produces better reconstructions.}
  \label{tab:table_radial}
\end{table}

As mentioned in the last part of Sec.~4.2 of the main paper and also shown in Fig.~6, we introduced synthetic distortions, including both radial and tangential components, to images from the LLFF dataset~\cite{mildenhall2019llff}.

We also apply moderate radial distortion to our synthetic dataset and reconstruct several object scenes. After training, we can render undistorted images. The perspective rendering increases the FOV while maintaining the same camera extrinsics. As shown in~\cref{fig:ours_obj_radial}, distorted edge lines, such as those on the Lego and Car objects, are correctly recovered into straight lines, demonstrating the capability of our hybrid field to model radial distortion effectively. We also report quantitative evaluations in~\cref{tab:table_radial}. Note that the radial distortion is relatively subtle, so the improvement is not as pronounced compared to other types of lenses.

\input{images/ours_obj_radial}

\begin{figure}[t] % Position at the top of the page
    \centering
    \scalebox{0.3}{
        \includegraphics[]{rebuttal_img/error_map.png} % Scale image width  
    }
    \caption{\textbf{Distortion Error Map}. We visualize the error map between the predicted distortion and the ground truth distortion from Mitsuba synthetic scenes.} % Add a caption
    \label{fig:error_map} % Label for referencing
\end{figure}

\subsection{Comparison of Regular and Invertible ResNet}
Neural networks can model complex non-linear fields, but the key advantage of iResNet is its effective regularization. Light rays passing through the lens are strictly bijective and invertible. iResNet, using fixed-point iteration, enforces this property at minimal cost. We visualize the error map compared to the distortion of the GT lens in synthetic scenes in~\cref{fig:error_map}. We show that iResNet predicts smooth distortion with low error, whereas ResNet produces a highly asymmetric field with large errors. The large error produced by ResNet is largely due to the lack of regularization. The displacement predicted by ResNet can be arbitrary and does not follow the two properties that real-world light rays hold.

\subsection{Qualitative Results in Undistorted Rendering}
\input{images/ours_undistort}
We provide additional rendering results in this section. We render both fisheye and perspective views on the FisheyeNeRF dataset~\cite{jeong2021self}. In~\cref{fig:ours_undistort}, we fix the view direction and camera location for fisheye rendering and extend the FOV for perspective rendering. In scenes such as Cube, Chairs, and Flowers, we observe that straight lines are accurately recovered during reconstruction. The lines on the wall behind the Cube and the window frames serve as strong evidence that our self-calibration system precisely models lens distortion.

\section{Implementation Details}
Our implementation is based on the codebase from Gaussian Splatting~\cite{kerbl20233d} and gsplat~\cite{ye2024gsplatopensourcelibrarygaussian}. We use the same loss function as 3DGS for training~\cite{kerbl20233d}. 
The invertible ResNet is constructed using FrEIA~\cite{freia}. 
We follow \citet{kerbl20233d} to select hyperparameters for optimizing 3D Gaussians. 
We also adopt the implementation of MCMC densification~\cite{kheradmand20243d}. 
Compared with vanilla densification, MCMC helps remove floaters by using opacity thresholding to relocate dead Gaussians. 
While the final quantitative results on the test set remain largely unchanged, applying the MCMC technique reduces visual floaters in novel viewpoints. 
For high-resolution scene captures such as Backyard and Office, we also use bilateral grids and anti-aliasing~\cite{yu2024mip} for improved quality.

As explained in~\cref{fig:transposed_opti_iresnet}, optimizing our hybrid field is essential for successful self-calibration. 
We use Adam~\cite{kingma2014adam} to train the invertible ResNet. 
The initial learning rate for the invertible ResNet is set to 1e-5 and gradually decreases to 1e-7 for FisheyeNeRF~\cite{jeong2021self}. 
The final learning rate for real-world captures is 1e-8, including Studio, Garden, and Backyard in \Cref{fig:supp_garden}, as well as more complex real-world captures such as Office~\cref{fig:supp_paul_office}. 
The learning rate for Mitsuba indoor synthetic scenes is set to 1e-8, while for object-centric scenes, it is 1e-7.
% After estimating distortion parameters from COLMAP, we uniformly sample points following the estimated distortion parameters to initialize the invertible ResNet, which typically takes approximately 1 minute to complete. 
All experiments are conducted on a single NVIDIA GeForce RTX 3090.


\section{Failure Cases and Limitations}
Real-world outdoor captures often include the sky.
Reconstructing the sky poses challenges due to moving clouds and the large uniform regions of blue and white without textures. 
The 3DGS~\cite{kerbl20233d} method tends to assign large Gaussians to the sky, resulting in artifacts when rendering novel views. 
Occasionally, some large Gaussians leak into the scene's center, appearing as a thin film in front of the camera. 
Similarly, for indoor scenes, regions with uniform textures, such as colored walls, present challenges. 
These textureless walls are often represented by Gaussians with large covariance matrices, causing similar rendering artifacts as observed with the sky.

The Gaussian sorting we propose alleviates the intensity discontinuities at the boundaries of cubemap faces caused by the multiple projections of a single Gaussian. 
However, since the projection of 3D covariance follows the equation in Sec.~3.1 of the main paper, identical 3D Gaussians can still result in different 2D covariances on different faces. 
This issue can be addressed by implementing smoother transitions between projection faces, such as spherical projection.

Finally, we do not account for the entrance pupil shift phenomenon commonly observed in fisheye lenses.
% , where rays near the center of the field of view converge at a deeper point within the lens than rays at the periphery. 
This effect is distinct from the lens distortion we are currently modeling. 
As a result, our method still struggles with such cameras, as shown in \Cref{fig:supp_paul_office}. 
While entrance pupil shift is negligible for distant scenes, it can cause splat misalignments in near-field scenes (\textit{e.g.,} the blurry sphere surface in the Office scene shown in the video), as the shift can reach up to half a centimeter for full-frame lenses.
It remains an exciting direction to study how to model such lens effects to further improve reconstruction quality.