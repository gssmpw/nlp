\section{Related Works}
\label{Related Works}

% Citer italiens neurips, espagnols application droit musical, towards total recall, mes 2 derniers papiers, libAUC, X-Curve, Wilcoxon Mann Whitney 2001.

Taming a binary classifier is an old game that researchers have been playing for many years. The critical applications, where it is unacceptable that instances belonging to a risky group would be misclassified, push them to raise methods where the training step penalizes more false negatives that false positives. This is also the case in the imbalanced literature where the critical class is often the minority one. In this context, cost-sensitive mechanisms were developed, where the minority class loss is adapted to compensate its under-representation \cite{cui2019class,ross2017focal,cao2019learning}. In a critical application setting, it helps the classifier to better classify the positive class and avoid to generate false negatives. The drawback here is that there is no guarantee that the methods assign the right cost, yielding a biased loss function. In \cite{chawla2002smote} and \cite{ahsan2022comparative}, the authors proposed to oversample the minority class, giving more synthetic examples to the classifier that learns better from this underpopulated class. Here, the challenge is to make these generated examples as close as possible to the real distribution, and it is not easy to fulfill this requirement (specifically for high dimensional data). For similar reasons, the authors of \cite{drummond2003c4} and \cite{fernandez2018learning} decided to undersample the majority class, requiring to sacrifice informative data that would eventually be interesting to keep for the classification performance. Also, some studies \cite{bougaham2021ganodip,roth2022towards} selected the threshold that does not generate any false negatives, after training. The drawback here is that the model does not consider the zero false negative constraint during the optimization, and its performance is not optimal in that sense. 

Another way to focus on the TPR is to approximate the AUC ROC curve and directly use it as the loss function, instead of the traditional binary cross entropy. Indeed this AUC ROC metric shows the True Positive Rate (TPR) against the False Positive Rate (FPR) for different thresholds, making it possible to optimize both together. In the beginning of the \nth{21} century, the authors of \cite{yan2003optimizing} laid the foundations by introducing an approximation of the AUC ROC metric, through the Wilcoxon-Mann-Whitney statistic \cite{mann1947test}. They showed that it is relevant to train a classifier the same way it will be evaluated, with the AUC metric. They proposed a differentiable surrogate loss function to make it possible. To focus on a specific part of the AUC, a one-way partial AUC loss has been developed \cite{dodd2003partial}, being a surrogate loss that only considers a portion of the AUC curve where the FPR is bounded between a low and a high value. A one-way and two-way partial AUC loss methods are then proposed in \cite{zhu2022auc}, taking into account a portion of the FPR as well as a portion of the TPR. The goal is to simulate a TPR and a FPR threshold to optimize a very specific part (the top left one) of the AUC ROC. They used a weakly convex optimization algorithm to train the classifier. However, the method focuses on a larger portion of the AUC ROC curve than the specific one that prevent any false negatives at the full TPR regime. After this literature survey, one can observe that an algorithm tailored to optimize the FPR at the very specific 100\% TPR setting, during training, is still lacking. The next section \ref{Method} precisely describes such a new method.