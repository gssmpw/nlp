%% 
%% Copyright 2007-2024 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.3 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.3 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%% SP 2008/03/01
%% $Id: elsarticle-template-num.tex 249 2024-04-06 10:51:24Z rishi $
%%
\documentclass[preprint,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsmath package provides various useful equation environments.
\usepackage{amsmath}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[table]{xcolor}
\usepackage[super]{nth}
\usepackage{subcaption}

\usepackage{graphicx}
\usepackage{tikz}

\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=0.5pt] (char) {#1};}}

% \journal{Information Fusion}

\makeatletter
\def\ps@pprintTitle{%
 \let\@oddhead\relax
 \let\@evenhead\relax
 \let\@oddfoot\relax
 \let\@evenfoot\relax}
\makeatother

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \affiliation for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \affiliation{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%% \fntext[label3]{}

% \title{High Sensitive Anomaly Detection for Quality Constraint through Approximate Partial AUC Loss}
% \title{High Sensitive Anomaly Detection through Dynamic Approximate Partial AUC Loss}
% \title{Highly Sensitive Anomaly Detection for Critical Applications through Approximated Partial AUC Loss}
\title{Towards a Trustworthy Anomaly Detection for Critical Applications through Approximated Partial AUC Loss}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \affiliation[label1]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%%
%% \affiliation[label2]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}

% \author{} %% Author name

% %% Author affiliation
% \affiliation{organization={},%Department and Organization
%             addressline={}, 
%             city={},
%             postcode={}, 
%             state={},
%             country={}}
\author[label1]{Arnaud Bougaham} %% Author name
\author[label1]{Beno\^it Fr\'enay}
%% Author affiliation
\affiliation[label1]{organization={University of Namur - NaDI - HuMaLearn Lab - Faculty of Computer Science},%Department and Organization
            addressline={Rue Grandgagnage 21}, 
            city={Namur},
            postcode={B-5000}, 
            % state={},
            country={Belgium}}

%% Abstract
\begin{abstract}
%% Text of abstract
Anomaly Detection is a crucial step for critical applications such in the industrial, medical or cybersecurity domains. These sectors share the same requirement of handling differently the different types of classification errors. Indeed, even if false positives are acceptable, false negatives are not, because it would reflect a missed detection of a quality issue, a disease or a cyber threat. To fulfill this requirement, we propose a method that dynamically applies a trustworthy approximated partial AUC ROC loss (\textit{tapAUC}). A binary classifier is trained to optimize the specific range of the AUC ROC curve that prevents the True Positive Rate (TPR) to reach 100$\%$ while minimizing the False Positive Rate (FPR). The optimal threshold that does not trigger any false negative is then kept and used at the test step. The results show a TPR of 92.52\% at a 20.43\% FPR for an average across 6 datasets, representing a TPR improvement of 4.3$\%$ for a FPR cost of 12.2\% against other state-of-the-art methods. The code is available at https://github.com/ArnaudBougaham/tapAUC.


\end{abstract}

% %%Graphical abstract
% \begin{graphicalabstract}
% %\includegraphics{grabs}
% \includegraphics[width=1\linewidth]{GraphicalAbstract_tapAUC.pdf}
% \end{graphicalabstract}


% %%Research highlights
% \begin{highlights}
% \item Use an approximated pAUC loss for a trustworthy anomaly detection.
% \item Dynamically focus on negative instances responsible for critical errors.
% \item Run experiments on industrial, medical and cybersecurity datasets.
% \item Compare the results with other state-of-the-art methods.
% \item Build an uncertainty interval to better engage the expert responsibility.
% \item Discuss the method in the context of real-world applications.
% \end{highlights}

%% Keywords
\begin{keyword}
%% keywords here, in the form: keyword \sep keyword
Anomaly Detection \sep Industry 4.0 \sep Industrial images\sep Medical images\sep High Sensitivity \sep pAUC
%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}

\end{frontmatter}

%% Add \usepackage{lineno} before \begin{document} and uncomment 
%% following line to enable line numbers
%% \linenumbers

%% main text
%%

% %% Use \section commands to start a section
% \section{Example Section}
% \label{sec1}
% %% Labels are used to cross-reference an item using \ref command.

% Section text. See Subsection \ref{subsec1}.
%% Use \section commands to start a section
\section{Introduction}
\label{Introduction}

% Planter le décor industriel, médical, cyber.
% Montrer que l'AS doit être grand pour les pos et petits pour les neg. Discuter de l'auc roc comme étant idéal pour représenter les TP vs les FP. Montrer que, à choisir entre 2 optimisations, il est préférable de choisir celle avec 100\% TPR, et que le pAUC peut être une option pour cela (schéma)


% Explainable AI (XAI) techniques seek to improve the trustworthiness in AI sytems through interpretability and explanability techniques. However, in critical domains where trust requiers to meet domain constraints. 
% this is sometimes not suficient to achieve trust. In particular, in critical domains where trust requiers to meet domain constraints. This paper focuses in the case where blablabla
% Trustworthy AI requires 3 components \cite{Bouadi2024trust}
% trust through constraints
% the high-level expert group on artificial intelligence1 has
% issued the Ethics Guidelines for Trustworthy AI that identify three components
% for trustworthy AI: it should be lawful, ethical and robust
In recent years, modern organizations have taken advantage of artificial intelligence (AI) advances to optimize their processes. In the same time, explainable AI (XAI) techniques have been developed to improve the interpretability and explanability in these AI systems. However, this is sometimes not sufficient in critical domains, where strong trustworthiness constraints are required. A group of AI experts for the European Commission released the Ethics Guidelines for Trustworthy AI where some of the proposals are to improve, among other things, the robustness, the transparency or the accountability \cite{Bouadi2024trust}. This paper seeks to focus on these characteristics of trustworthiness for critical applications.

The current \nth{4} industrial revolution brings many new techniques thanks to AI, from predictive maintenance \cite{nunes2023challenges,zonta2020predictive} to production processes optimization \cite{weichert2019review}, including sales forecasting \cite{cheriyan2018intelligent}. The medical domain is also quickly adopting these new techniques, from hospital patients journey optimization \cite{el2021machine} to doctor's visit reporting \cite{falcetta2023automatic}, or even health lifestyle recommendations in relation to a suspected pathology \cite{chatterjee2023ai}. AI techniques can also contribute in these fields to diagnose problems, through anomaly detection (AD) systems \cite{wen2024survey,li2024intelligent,abou2024generative}. Indeed, the industrial and medical sectors share many common features, when it comes to detect a quality issue or a disease. Among them, we identify the decisions based on image processing techniques, the lack of abnormal data yielding to imbalanced datasets, or the importance to automate painful and time-consuming tasks. But one of the most important concern is the necessity to avoid any critical missed detection, while keeping a small rate of false alarms \cite{bougaham2025industrial,yang2021all}. Indeed, in critical industries like automotive, health, nuclear or aerospatial, a product test coverage that fails to meet the quality specifications would bring dramatic consequences. A similar reasoning is applicable for the medical domain, where a specialist would not detect symptoms heralding a disease that has to be treated quickly. Therefore, a reasonable scheme to deal with such constraints is to strengthen the decisions that a human expert has to take, with an AI-assistant tool such as a normal/abnormal classifier.
% , such as decisions based on image processing, a quality constraint that need to reach 100\% true positives through the lowest false positive rate possible, a lack of abnormal data or even a high automation need to compensate painful and time-consuming tasks. 
This statement motivates this study, to assist the operators or specialists in their decision making. 

In this context, one of the most challenging step is to build a binary classifier for anomaly detection that is accurate, trustworthy and transparent. Each of these characteristics would encourage the business experts to adopt the technology and would increase its decision-making quality. This implies that such a model has to be trained with the objective to yield an asymmetric confusion matrix, with zero false negatives (ZFN) and a limited false positive (LFP) ratio. In other words, from the training to the inference step, the method has to be tailored such that it gives a larger importance to positive (abnormal) instances than negative (normal) ones. In addition, the tool would have to give a score-based degree of confidence, in order to guide the expert, as well as to bear full responsibility for his final decision.

In the critical or imbalanced learning literature, many methods exist to deal with the constraint, namely oversampling the minority class \cite{ahsan2022comparative} or undersampling the majority class \cite{fernandez2018learning}, giving a larger weight to the loss when dealing with the minority class \cite{cui2019class}, or adapting the threshold after the training phase so that all positives are classified as true ones \cite{bougaham2021ganodip}. All these benefits come with drawbacks. Among them, there are the sacrifice of informative data or the difficulty to generate synthetic ones. We can also cite the need to assign the right cost or the absence of the constraint during the training step. This study considers all of these drawbacks and tackles them by designing a loss function in a specific method called \textit{tapAUC} (trustworthy approximated partial Area Under the Curve) that incorporates the quality constraint during the optimization, without giving any specific weight for the unacceptable classification errors. This work is the continuation of our preliminary one \cite{bougaham2023composite}, where an industrial partner reinforces its quality strategy for an automotive PCBA (Printed Circuit Board Assembly) production line. The objective is to prevent any defect to be missed while maintaining an LFP ratio, and show how the method can generalize to other domains sharing the same concern. To do so, an approximation of the Area Under the Curve of the Receiver Operating Characteristic (AUC ROC) metric is considered as the loss function of a feed-forward neural network classifier. The positive instances, and a partial selection of the negative ones, help focusing on the region of interest to optimize, namely the one that prevents a full TPR at the minimum FPR possible. Our main contributions are the following:
\begin{itemize}
\item Use an approximated pAUC loss for a trustworthy anomaly detection.
\item Dynamically focus on negative instances responsible for critical errors.
\item Run experiments on industrial, medical and cybersecurity datasets.
\item Compare the results with other state-of-the-art methods.
\item Build an uncertainty interval to better engage the expert responsibility.
\item Discuss the method in the context of real-world applications.
\end{itemize}
This study is organized as follows. Section~\ref{Related Works} introduces the necessary related works to fully understand the previous approaches and the subsequent concepts. Then Section~\ref{Method} details the method proposed, from the customized loss presentation to the algorithm description. After that, Section~\ref{Evaluation} presents the experiments and the results are discussed in Section~\ref{Discussion}. Finally, Section~\ref{Conclusion} summarizes the study and proposes future works.

% Numerous studies have dealt with this aspect, in both the industrial \cite{Ganomaly} and the medical \cite{f-anogan} fields. These methods rely on a Generative Adversarial Network (GAN) trained to reconstruct images that are free of anomaly. When an original image is fed into the network, no matter if it contains an anomaly or not, it will be reconstructed by its "normal" version. Then, the difference between the original and reconstructed image highlights the anomaly (if present), and a score is computed to quantify the defect, to judge the image after a threshold comparison. GAN is know to be the most effective technique, thanks to the realism that characterized the image generated. However, new promising techniques have emerged a short while ago. Indeed, the Vision Transformer (ViT) or the Latent Diffusion Models (LDM) are generative models that appeared after the GAN creation, and brought spectacular results in terms of image generation. This work is the continuation of our previous one \cite{VQGanoDIP}, focused on the generative part, where we use a Vector Quantized GAN (VQGAN) composed of convolutional ResNet components. The method proposed replaces this architecture by a ViT model for the encoder and decoder, as well as a LDM being processed in the latent space, and evaluate the effect on the classification accuracy.

\section{Related Works}
\label{Related Works}

% Citer italiens neurips, espagnols application droit musical, towards total recall, mes 2 derniers papiers, libAUC, X-Curve, Wilcoxon Mann Whitney 2001.

Taming a binary classifier is an old game that researchers have been playing for many years. The critical applications, where it is unacceptable that instances belonging to a risky group would be misclassified, push them to raise methods where the training step penalizes more false negatives that false positives. This is also the case in the imbalanced literature where the critical class is often the minority one. In this context, cost-sensitive mechanisms were developed, where the minority class loss is adapted to compensate its under-representation \cite{cui2019class,ross2017focal,cao2019learning}. In a critical application setting, it helps the classifier to better classify the positive class and avoid to generate false negatives. The drawback here is that there is no guarantee that the methods assign the right cost, yielding a biased loss function. In \cite{chawla2002smote} and \cite{ahsan2022comparative}, the authors proposed to oversample the minority class, giving more synthetic examples to the classifier that learns better from this underpopulated class. Here, the challenge is to make these generated examples as close as possible to the real distribution, and it is not easy to fulfill this requirement (specifically for high dimensional data). For similar reasons, the authors of \cite{drummond2003c4} and \cite{fernandez2018learning} decided to undersample the majority class, requiring to sacrifice informative data that would eventually be interesting to keep for the classification performance. Also, some studies \cite{bougaham2021ganodip,roth2022towards} selected the threshold that does not generate any false negatives, after training. The drawback here is that the model does not consider the zero false negative constraint during the optimization, and its performance is not optimal in that sense. 

Another way to focus on the TPR is to approximate the AUC ROC curve and directly use it as the loss function, instead of the traditional binary cross entropy. Indeed this AUC ROC metric shows the True Positive Rate (TPR) against the False Positive Rate (FPR) for different thresholds, making it possible to optimize both together. In the beginning of the \nth{21} century, the authors of \cite{yan2003optimizing} laid the foundations by introducing an approximation of the AUC ROC metric, through the Wilcoxon-Mann-Whitney statistic \cite{mann1947test}. They showed that it is relevant to train a classifier the same way it will be evaluated, with the AUC metric. They proposed a differentiable surrogate loss function to make it possible. To focus on a specific part of the AUC, a one-way partial AUC loss has been developed \cite{dodd2003partial}, being a surrogate loss that only considers a portion of the AUC curve where the FPR is bounded between a low and a high value. A one-way and two-way partial AUC loss methods are then proposed in \cite{zhu2022auc}, taking into account a portion of the FPR as well as a portion of the TPR. The goal is to simulate a TPR and a FPR threshold to optimize a very specific part (the top left one) of the AUC ROC. They used a weakly convex optimization algorithm to train the classifier. However, the method focuses on a larger portion of the AUC ROC curve than the specific one that prevent any false negatives at the full TPR regime. After this literature survey, one can observe that an algorithm tailored to optimize the FPR at the very specific 100\% TPR setting, during training, is still lacking. The next section \ref{Method} precisely describes such a new method.

\section{The tapAUC Method}
\label{Method}

After having introduced the context and described the related works, the proposed method \textit{tapAUC} (trustworthy approximated partial AUC) is detailed in this section.
% For this study, we consider a tabular dataset with several features and the label (normal or abnormal) in columns, and the different instances in rows. Note that image datasets, very popular in AD contexts, come with a large complexity in critical applications such as industrial or medical ones (high resolution, large variability, etc...). Therefore it is advised to separate the abnormal information in the image and its classification. This can be done by first using a technique to extract information from the image to reconstruct in its normal version, as done in VQGanoDIP, to make the image dataset a tabular one. 

% \subsection{Approach}
% \label{Approach}

% %apAUC
% The main idea of the proposed method is to train a binary classifier using a custom loss function and a dedicated algorithm. The loss function is an approximation of the AUC ROC metric (TPR versus FPR for different possible thresholds), commonly used to assess the performance after training. Using this loss during the training procedure improves the performance because it directly targets what we aim for, instead of a traditional binary cross-entropy. But it also offers a direct access to the TPR and the FPR metrics, where we can slightly arrange the loss to keep improving the FPR only in the 100\% TPR setting. An anomaly score pairwise comparison is performed by considering the positive examples and only a part of the negative ones, i.e: those that raise a large anomaly score because they are challenging to correctly classify. 

% %algo
% This partial AUC (pAUC) loss brings to the classifier the necessary information so that it modifies its parameters to better classify the specific cut-off region of interest. Indeed, instead of improving all regions of the AUC ROC, including the ones where TPR is far from 100\%, this approach reduces the focus at the more challenging negative samples that can result a higher anomaly score than some positive samples. Iteration after iteration, the algorithm compute the loss for all the positive samples and this subset of the negative ones. 
% % This subset contains negative samples with an anomaly score larger than a threshold set to allow a small part improvement possible in the 100\% TPR region. 
% % They are the ones that influence directly the TPR because they condition the threshold value to place in the ZFN setting. 
% The lower the anomaly score of the challenging negative samples, the better the discrimination while considering the specific 100\% TPR threshold. Figure~\ref{discrimination} represents this statement. 
% \begin{figure}[t!]
% \centering
% \includegraphics[width=1\linewidth]{Distrib.pdf}
% \caption{Anomaly score distribution example of negative (in green) and positive (in red) samples. Unlike the threshold that yields the maximum accuracy (dashed black vertical line), the one that reaches 100\% TPR (dashed red vertical line) is highly influenced by the positive and the most challenging negative samples. Lower score for these challenging negatives and higher score for positives would end up with a sensitive classifier.}\label{discrimination}
% \end{figure}
% The objective is to lower the score of the challenging negative samples close to the positive samples. If so, negative and positive distributions could be better separated, by lowering the threshold. We end up with a classifier particularly sensitive with challenging negative samples, often being the majority class for critical applications, no matter the other easyly classified negative samples. Figure~\ref{AUC} shows this mechanism in the partial AUC view. The $TPR=1$ and $FPR=1$ region means that the threshold is set to $0$ and classify all samples as positive (even all the negative samples). The $TPR=0$ and $FPR=0$ region means that the threshold is set to $1$ and classify all samples as negative  (even all the negative samples). The dashed red line threshold indicates that all the positive and almost all negative samples are well classified. The few misclassified are the most challenging negative samples. Epoch after epoch, this threshold will change to always follow this strategy, while the classifier is getting optimized. We will end up with an optimal model in the context of a minimal FPR and full TPR. 
% % During the training, if the optimization only focuses on this subset and the positive samples, the following iterations will move the negative scores to a lower value, allowing the ZFN threshold to reduce the false negative rate. 
% \begin{figure}[h!]
% \centering
% \includegraphics[width=1\linewidth]{AUC.pdf}
% \caption{During the classifier training, epoch after epoch, the negative samples that just prevent a 100\% TPR and all the positive ones are selected. The customized loss function imitates the partial AUC, where the focused improved region helps minimizing the FPR while maintaing the TPR at 100\%.}\label{AUC}
% \end{figure}


\subsection{Zero False Negatives under Limited False Positive Rate}
\label{ZFN_LFP}

%apAUC
The main objective is to train a binary classifier able to tackle the critical application problem. To do so, we will consider the True Positive Rate (TPR) and the False Positive Rate (FPR):
\begin{equation}
    \text{TPR} = \frac{TP}{TP + FN},\nonumber
\end{equation}
\begin{equation}
    \text{FPR} = \frac{FP}{FP + TN}\nonumber
\end{equation}
where $TP$ is the set of True Positives, $FN$ the False Negatives, $FP$ the False Positives and $TN$ the set of True Negatives.
% \begin{equation*} 
%     \begin{cases}
%         \begin{aligned}
%             \begin{minipage}{\linewidth}
%                 \text{TP} = True Positives, \\\\
%                 \text{FN} = False Negatives, \\\\
%                 \text{FP} = False Positives, \\\\
%                 \text{TN} = True Negatives.
%             \end{minipage}
%         \end{aligned}
%     \end{cases}
% \end{equation*}
Note that 100\% TPR is equivalent to 0\% False Negative Rate (FNR) because $TPR=1-FNR$ with:
\begin{equation}
    \text{FNR} = \frac{FN}{FN + TP}\nonumber
\end{equation}
Therefore, a classifier that reaches a full TPR does not generate any false negatives. This zero false negative (ZFN) setting makes it possible to detect all the positives, which is a key feature for a critical application such in the industrial, the medical or the cybersecurity domain. Figure~\ref{discrimination} shows this statement through an example.
\begin{figure}[t!]
\centering
\begin{tikzpicture}
    \node[anchor=south west, inner sep=0] (image) at (0,0) 
          {\includegraphics[width=1\textwidth]{Distrib.pdf}}; % Replace with your image file
    \begin{scope}[x={(image.south east)},y={(image.north west)}]
        \node[draw, circle] at (0.35,0.125) {1};
        \node[draw, circle] at (0.53,0.125) {2};
    \end{scope}
\end{tikzpicture}
\caption{Anomaly score data points and distribution example of negative (in green) and positive (in red) instances. Unlike the threshold that yields the maximum accuracy (STD Threshold dashed black vertical line), the one that reaches 100\% TPR (ZFN Threshold dashed red vertical line) is highly influenced by the positive and the most challenging negative data. Lower score for these challenging negatives and higher score for positives would end up with a full TPR classifier, with limited FPR.}
\label{discrimination}
\end{figure} 
During training, the model predicts the scores to associate the class that each instance should belong to, where the normal class is 0 and the abnormal class is 1. These scores are the data points shown in the bottom part of the figure. We end up with anomaly scores distributed between 0 and 1 for both classes, shown in the top part of the figure, that have to be separated by a threshold to classify each data as negative or not. The ones with an anomaly score below the threshold will be classified as negative (normal), and the ones above will be classified as positive (abnormal). After that, the number of true and false positives can be calculated, depending on the threshold chosen. This threshold value is very important when dealing with critical applications. Indeed, the one that classify all the positives correctly has to be selected, in order to generate zero false negative (ZFN) and guarantee that no detection would be missed. The other constraint is to reduce the number of false alarms, in order to use this tool without permanently alarming the user about anomalies that do not exist. This limited false positives (LFP) ratio requires the classifier to minimize the FPR. In Figure~\ref{discrimination}, the threshold that corresponds to these constraints is the red dashed line. It is set at the exact left border of the positive distribution, to prevent any additional false positives, while detecting all the positives. In other words, its optimal value for our critical applications is the minimum anomaly score of the positive set. In order to observe how the true and false positives are distributed through the threshold configurations, Figure~\ref{Confusion_Matrix_STD} shows the confusion matrix for the standard STD threshold that maximizes the accuracy, and Figure~\ref{Confusion_Matrix_ZFN} shows the one that is of interest in our context, being the ZFN threshold.
\begin{figure}[p]
\centering
\includegraphics[width=.75\linewidth]{CM_STD.pdf}
\caption{Confusion Matrix that maximizes the accuracy.}\label{Confusion_Matrix_STD}
\vspace{20pt}
\centering
\includegraphics[width=.75\linewidth]{CM_ZFN.pdf}
\caption{Confusion Matrix that guarantees ZFN under LFP.}\label{Confusion_Matrix_ZFN}
\end{figure}
For a better understanding, the part of the distribution concerned is displayed for each cases. On can observe that the confusion matrix with the ZFN threshold in Figure~\ref{Confusion_Matrix_ZFN} yields 0 false negative (no true positive predicted as negative), which is the not the case with the STD threshold in Figure~\ref{Confusion_Matrix_STD}. The drawback is that the ZFN FPR (true negative predicted as positive) is higher than the STD one. Despite this statement, this ZFN setting is exactly the one that will be exploited during the classifier training. 

\subsection{Partial AUC ROC Curve}
\label{pAUC}

The objective of the proposed \textit{tapAUC} method is to exploit this ZFN under LFP mechanism, during training. An approximation of the AUC ROC curve, which displays the TPR against the FPR for all the possible thresholds, is therefore chosen as the loss function. Commonly used to assess the classifier performance after training, this metric is customized and used in a dedicated algorithm, to bring the class importance knowledge during the model optimization. Using this loss during the training procedure improves the performance because it directly targets what we aim for, instead of a traditional binary cross-entropy as it is usually used. But it also offers a direct access to the TPR and the FPR metrics, where we can slightly arrange the loss to keep improving the FPR only in the 100\% TPR setting. Indeed, instead of considering all the data in this approximated AUC loss, only a part of the negative instances and all the positive ones will be selected. The loss function thus becomes a partial AUC (pAUC) loss. The targeted negatives subset is the one that generates high anomaly scores because its instances are difficult to correctly classify. And these instances are the most interesting for the model to learn about the task, much more than easy ones that do not help reducing the overlap region during training.
% They can be identifies thanks to a pairwise comparison of the anomaly score between the classes. 

%algo
This pAUC loss brings the necessary information to the classifier so that it modifies its parameters to better classify a specific cut-off region of interest. Indeed, instead of improving all the unnecessary regions of the AUC ROC, with the negatives that do not influence the ZFN threshold placement (corresponding to the region \circled{1} in Figure~\ref{discrimination}), this approach reduces the focus at the more challenging instances that can result a higher anomaly score than some positives (region \circled{2} in Figure~\ref{discrimination}). Iteration after iteration, the algorithm computes the loss for all the positive instances and only this subset of the negative ones. 
% This subset contains negative samples with an anomaly score larger than a threshold set to allow a small part improvement possible in the 100\% TPR region. 
% They are the ones that influence directly the TPR because they condition the threshold value to place in the ZFN setting. 
The lower the anomaly score of the challenging negative instances, the better the discrimination while considering the specific 100\% TPR threshold.

Figure~\ref{AUC} shows this mechanism in the partial AUC view. The objective is to lower the score of the challenging negative data close to the positive one. If so, negative and positive distributions could be better separated, by lowering the threshold. As explained in the top part of Figure~\ref{AUC}, the threshold is selected so that all the positive and almost all negative instances are well classified. Only the few misclassified negatives are considered, being the most challenging ones. The middle part of the figure shows that, while the model gets optimized, this threshold will change (at the next epoch) to always follow this strategy and target a specific region to improve. For our application, it has therefore to be placed at $TPR\approx1$, to let the opportunity for the model to keep optimizing the very small part that do not yet reach 100\%, while not focusing on the other uninteresting regions. The bottom part of the figure explains how the subsample selection evolves epoch after epoch. We end up with an optimal model in the context of a minimal FPR and full TPR, particularly sensitive with challenging negative data, by getting rid of the other easily classified ones.
% During the training, if the optimization only focuses on this subset and the positive samples, the following iterations will move the negative scores to a lower value, allowing the ZFN threshold to reduce the false negative rate. 
\begin{figure}[p]
\centering
\includegraphics[width=1\linewidth]{AUC.pdf}
\caption{During the classifier training, epoch after epoch, the negative instances that just prevent a 100\% TPR and all the positive ones are selected. The customized loss function approximates the partial AUC ROC curve, where the focused improved region helps minimizing the FPR while maintaining the TPR at 100\%.}\label{AUC}
\end{figure}

\subsection{Loss Function}
\label{Loss Function}

The standard way to discriminate classes is to train a model based on the binary cross entropy (BCE) loss. Indeed, this loss is well suited to handle two classes and to backpropagate its gradient while training. Nevertheless, it does not take into account the nature of the classification error, whatever it is a false positive or a false negative. This is why an approximation of the pAUC loss is needed. The objective is to perform pairwise comparisons of the anomaly scores through a surrogate squared hinge loss, for the positive instances and the more challenging negative ones. Consider a dataset \( \mathcal{D} = \{X, Y\} \), with \( N \) instances of input features \( X = \{x_1, \ldots, x_N\} \), and its corresponding binary labels \( Y = \{y_1, \ldots, y_N\} \) (\( y_p = 1 \) for positive instances and \( y_n = 0 \) for negative ones). A feed-forward neural network classifier \( f_\theta: \mathbb{R}^D \to \mathbb{R} \) maps the feature space to anomaly scores \( s = f_\theta(x) \), where \( \theta \) denotes the model parameters. The objective is to train \( f_\theta \) by minimizing a loss function \( \mathcal{L} \), which ensures that positive (abnormal) scores \( s_p \) are separated and higher from negative (normal) scores \( s_n \), while focusing on challenging instances. Two subsets are defined based on the output scores:
% \[
% \mathcal{S}^+ = \{s^+_i = f_\theta(x_i) \,|\, y_i = 1\}, \quad \mathcal{S}^- = \{s^-_j = f_\theta(x_j) \,|\, y_j = 0\}.
% \]
% \[
% \mathcal{P} = \{s_{p_i} = f_\theta(x_i) \,|\, y_p = 1\}, \quad \mathcal{N} = \{s_{n_j} = f_\theta(x_j) \,|\, y_n = 0\}.
% \]
\[
\mathcal{P} = \{s_p = f_\theta(x) \,|\, y_p = 1\}, \quad \mathcal{N} = \{s_n = f_\theta(x) \,|\, y_n = 0\}.
\]
The AUC loss we want to minimize is defined as:
% \begin{equation}
% \mathcal{L}_\text{AUC} = \frac{1}{|\mathcal{S}^+| |\mathcal{S}^-|} \sum_{i=1}^{|\mathcal{S}^+|} \sum_{j=1}^{|\mathcal{S}^-|} \mathbf{1}(s^+_i < s^-_j),
% \end{equation}
% \begin{equation}
% \mathcal{L}_\text{AUC} = \frac{1}{|\mathcal{P}| |\mathcal{N}|} \sum_{i=1}^{|\mathcal{P}|} \sum_{j=1}^{|\mathcal{N}|} \mathbf{1}(s_{p_i} < s_{n_j}),
% \label{Lauc}
% \end{equation}
\begin{equation}
\mathcal{L}_\text{AUC} = \frac{1}{|\mathcal{P}| |\mathcal{N}|} \sum_{p \in \mathcal{P}} \sum_{n \in \mathcal{N}} \mathbf{1}(s_p < s_n),
\label{Lauc}
\end{equation}
where \( \mathbf{1}(\cdot) \) is the indicator function. To enable gradient-based optimization, the indicator function is replaced by a differentiable approximation, namely a squared hinge loss function:
% \begin{equation}
% \mathcal{L}_\text{AUC}^\text{approx} = \frac{1}{|\mathcal{S}^+| |\mathcal{S}^-|} \sum_{i=1}^{|\mathcal{S}^+|} \sum_{j=1}^{|\mathcal{S}^-|}  \max \left(0, \left(s^-_i + \gamma - s^+_i\right)^2\right)
% \label{Loss}
% \end{equation}
% \begin{equation}
% \mathcal{L}_\text{AUC}^\text{approx} = \frac{1}{|\mathcal{P}| |\mathcal{N}|} \sum_{i=1}^{|\mathcal{P}|} \sum_{j=1}^{|\mathcal{N}|}  \max \left(0, \left(s_{p_i} + \gamma - s_{n_j}\right)^2\right)
% \label{LaAUC}
% \end{equation}
\begin{equation}
\mathcal{L}_\text{aAUC} = \frac{1}{|\mathcal{P}| |\mathcal{N}|} \sum_{p \in \mathcal{P}} \sum_{n \in \mathcal{N}}  \max \left(0, \left(s_n + \gamma - s_p\right)\right)^2
\label{LaAUC}
\end{equation}
where \( \gamma > 0 \) is a margin separating positive and negative scores. As explained in Section~\ref{pAUC}, only a portion of the ROC curve corresponding to the challenging negative instances and all the positive ones are considered. The partial AUC (pAUC) restricts the evaluation to a specific FPR range \([ \alpha, 1 ]\) that allows this restriction. The subset of negative scores \( \mathcal{N}_{\alpha} \) is defined by:
\begin{equation}
    \mathcal{N}_{\alpha} = \mathcal{N}_{sorted}[ : \lfloor \alpha |\mathcal{N}| \rfloor],
\label{Ssorted}
\end{equation}
with $\mathcal{N}_{sorted}$ being the negative set \( \mathcal{N} \) sorted by the scores in the descending order. The trustworthy approximated pAUC loss $\mathcal{L}_\text{tapAUC}$ is expressed as:
% \begin{equation}
% \mathcal{L}_\text{tapAUC} = \frac{1}{|\mathcal{S}^+| |\mathcal{S}^-_{\alpha}|} \sum_{i=1}^{|\mathcal{S}^+|} \sum_{j=1}^{|\mathcal{S}^-_{\alpha}|}  \max \left(0, \left(s^-_{\alpha, j} + \gamma - s^+_i\right)^2\right)
% \label{Loss}
% \end{equation}
% \begin{equation}
% \mathcal{L}_\text{tapAUC} = \frac{1}{|\mathcal{P}| |\mathcal{N}_{\alpha}|} \sum_{i=1}^{|\mathcal{P}|} \sum_{j=1}^{|\mathcal{N}_{\alpha}|}  \max \left(0, \left(s_{n_{\alpha, j}} + \gamma - s_{p_i}\right)^2\right)
% \label{LtapAUC}
% \end{equation}
% \\$\mathcal{P}$ is the positive set\\\\
% $\mathcal{N}_{\alpha}$ is the negative set of the $\alpha$ most challenging instances\\\\
% $s_p$ is the predicted score of the positive instances\\\\
% $\gamma$ is the margin that separates the score of both classes\\\\
% $s_{n}$ is the the predicted score of the negative instances\\\\
\begin{equation}
\mathcal{L}_\text{tapAUC} = \frac{1}{|\mathcal{P}| |\mathcal{N}_{\alpha}|} \sum_{p \in \mathcal{P}} \sum_{n \in \mathcal{N}_{\alpha}}  \max \left(0, \left(s_n + \gamma - s_{p}\right)\right)^2
\label{LtapAUC}
\end{equation}
In order to warmup the classifier training, $\mathcal{N}_{\alpha}$ is built as an adaptive subset where $\alpha$ = 0 during this period (equivalent to Equation~\ref{LaAUC}), and \( 0 < \alpha < 1 \) otherwise.
% To focus training on challenging negative samples, an adaptive subset \( \mathcal{N}_{\alpha} \subseteq \mathcal{S}^- \) is selected based on a ranking of negative scores. The subset \( \mathcal{N}_{\alpha} \) is defined as:
% \begin{equation}
% \mathcal{N}_{\alpha} = \mathcal{S}^-[\lfloor \alpha \cdot |\mathcal{S}^-| \rfloor + 1 : |\mathcal{S}^-|],
% \end{equation}
% where \( 0 < \alpha < 1 \) determines the fraction of the highest-ranking negatives excluded.
% The final loss function incorporating adaptive negative mining is given by:
% \begin{equation}
% \mathcal{L} = \frac{1}{|\mathcal{S}^+| \cdot |\mathcal{N}_{\alpha}|} \sum_{i=1}^{|\mathcal{S}^+|} \sum_{j=1}^{|\mathcal{N}_{\alpha}|} \max(0, (s^-_j + \gamma - s^+_i)^2),
% \end{equation}
% where \( \gamma > 0 \) is a margin separating positive and negative scores.
% This loss function \( \mathcal{L} \) is an approximated partial AUC ROC loss, and is defined as follows:
% \begin{equation}
% \mathcal{L} = \frac{1}{|\mathcal{P}| \cdot |\mathcal{N}_{\alpha}|} \sum_{p \in \mathcal{P}} \sum_{n \in \mathcal{N}_{\alpha}} \max \left(0, \left(s_n + \gamma - s_p\right)^2\right)
% \label{Loss}
% \end{equation}
% where $\mathcal{P}$ is the set of positive instances, $\mathcal{N}_{\alpha}$ is the set of the $\alpha$ selected negative instances, $s_n$ is the predicted score for negative instances, $s_p$ is the predicted score for positive instances and $\gamma$ is the margin that separates the scores of both classes. The selection of negative instances $N_{\alpha}$ is determined as follows:
% \begin{equation}
% \mathcal{N}_{\alpha} = \begin{cases} 
% \mathcal{N} & \text{if } \text{epoch} < \text{e}_{\text{w}} \\
% {\alpha} \text{ } {\mathcal{N}_{\text{highest} \text{ } {s_n}}}  & \text{if } \text{e}_{\text{w}} \leq \text{epoch} < \text{e}_\text{l}
% \end{cases}
% \label{Nalpha}
% \end{equation}
% with $\mathcal{N}$ being the full set of negative instances, $\text{e}_{\text{w}}$ the warmup epoch number from which the subselection of negative samples starts, $\text{e}_\text{l}$ the last epoch number, and ${\alpha} \text{ } {\mathcal{N}_{\text{highest} \text{ } {s_n}}}$ selects the $\alpha$ negative instances with the highest predicted score $s_n$.
% Based on Equation~\ref{LtapAUC}, one can observe that the first step of the model training considers the loss as a standard AUC approximation, when the epoch warmup period is not reached. 
It helps the model to select the most challenging examples that will be the starting point for the post-warmup step. Then, the negative data with the lowest prediction scores will be excluded from the $\mathcal{N_{\alpha}}$ subset. The more challenging negatives added to the positives influence directly the false negative rate because they condition the threshold value to place in the 100\% TPR setting. 
% Figure~\ref{discrimination} represents this statement. 
% \begin{figure}[h]
% \centering.
% \includegraphics[width=1\linewidth]{Discrimination.JPG}
% \caption{Anomaly Score distribution example of negative and positive samples. The ZFN threshold is highly influenced by the anomaly score of the most challenging negative samples.}\label{discrimination}
% \end{figure}
% During the training, if the optimization only focuses on this subset and the positive samples, the following iterations will move the negative scores to a lower value, allowing the ZFN threshold to reduce the false negative rate. Figure~\ref{AUC} shows this mechanism in the partial AUC view.
% \begin{figure}[h]
% \centering.
% \includegraphics[width=0.5\linewidth]{AUC.JPG}
% \caption{Partial AUC ROC}\label{AUC}
% \end{figure}
Iteration after iteration, the negative subset is updated, in order to dynamically track the small top left corner that is of interest (the one that just prevents 100\% TPR), and the partial AUC loss $\mathcal{L}_\text{tapAUC}$ decreases until there is no possibility anymore. At the end, the model is trained with its parameter values selected to get the best FPR while having 100\% TPR for the train set. Once the classifier is trained with this method, the 100\% TPR threshold is kept (being the smallest positive anomaly score) for evaluation. Then, the test set is inferred and metrics such as the accuracy, the TPR and the FPR are computed based on the specific threshold. The expectation is that the TPR (under a reasonable FPR) is improved compared to a different method where this focus is not done.

\subsection{Algorithm}
\label{Algorithm}

%Pseudo-code
The \textit{tapAUC} method is summarized in the pseudo-code Algorithm \ref{Pseudo-code}. The training loop makes the classifier optimize its parameters based on Equation~\ref{LtapAUC}. Before the warmup period, the loss considers the full train set, with all the negative and positive instances. After this period, only a partial set of negatives, combined to all positives, is dynamically considered, epoch after epoch. Once the classifier trained, the threshold that gives 100\% TPR on the train set is kept, and used to compute the metrics on the test set.

\begin{algorithm}
\caption{Training and Testing tapAUC Model}
\label{Pseudo-code}
\begin{algorithmic}[1]
\Procedure{TrainAndTest}{$train\_loader, test\_loader$}
    \State Initialize $model$, optimizer, $e\_total$, $e\_warmup$, $\alpha$
    % \State Create $test\_loader$ from $X\_test, y\_test$
    \For{$epoch = 0$ to $e\_total$}
        \State Set $model$ to training mode
        \For{each batch $(X\_batch, y\_batch)$ in $train\_loader$}
            \State $y\_pred \gets model(X\_batch)$
            \If{$epoch \geq e\_warmup$}
                \State Sort $y\_pred$ scores for $N$ (negatives) and $P$ (positives)
                % \State Identify scores of $N$ and $P$
                \State Create $N_\alpha$  with the $\alpha$ highest scores of $N$
                % \State Select $S\_minus\_alpha\_beta$ based on $neg\_count$
                \State Combine $N_\alpha$ and $P$ to form $y\_batch_\alpha$ with $y\_pred_\alpha$ scores
                \State $loss \gets SquaredHingeLoss(y\_pred_\alpha, y\_batch_\alpha)$
            \Else
                \State $loss \gets SquaredHingeLoss(y\_pred, y\_batch)$
            \EndIf
            \State Perform backward pass and optimize
        \EndFor
        \EndFor
        % \State Compute $train\_metrics$ using $get\_metric$ function
        \State $threshold_{ZFN} \gets$ lowest prediction score of $P$
        
        \State Set $model$ to evaluation mode
        \For{each batch $(X\_batch, y\_batch)$ in $test\_loader$}
            \State $y\_pred \gets model(X\_batch)$
            % \State Compute $test\_metrics$ using $get\_metric$ function
            \State Compute $Accuracy, TPR, FPR$ based on $threshold_{ZFN}$
        % \EndFor
        
        % \If{$epoch = total\_epochs$}
        %     \State Store final $train\_metrics$ and $test\_metrics$
        % \EndIf
    \EndFor
    \State \Return $threshold_{ZFN}, Accuracy, TPR, FPR$
\EndProcedure
\end{algorithmic}
\end{algorithm}


\section{Evaluation}
\label{Evaluation}

This section shows the evaluation performed to assess the proposed \textit{tapAUC} method. The datasets, the experimental setup and the results are described.

\subsection{Datasets}
\label{Datasets}

% Motivation de ma methode:
% Images complexes pas gérables avec un simple NN comme LibAUC ou XCurve propose. Donc passer par mon VQGAN et entrainer le clf sur les datasets tabulaires avec ma méthode, plus directe et ciblant directement la contrainte ZFN.

% Si besoin de détecter une anomalie dans une image challenge (haute definition, grande variabilité, etc...), un simple MLP ne suffit pas (comme fait LibAUC avec CIFAR). Il faut passer par un VQGAN qui transforme l'image HD en données tabulaires. Et ensuite, il faut discriminer avec un classifieur binaire, sur base de ces données tabulaires. Et là, ma méthode performe mieux que les concurrents, pour le ZFN setting.

% Use apAUC in the context of ZFN
% Dynamically select the worst negative samples
% No need to define alpha and beta, the zfn-focused method will dynamically select the small range that causes ZFN impossible at the best threshold

Six datasets have been chosen to assess the proposed method. Some of them are linked to a critical application (industrial, medical or cybersecurity), motivating the necessity to avoid false negatives that can yield to dramatic consequences. They are all tabular data, even if, among them, the original format was images. This is the case for the 4 industrial datasets, namely the Leather, Hazelnut and Grid ones coming from the public MVTec-AD image database \cite{bergmann2019mvtec}, and the PCBA image dataset from the private industrial partner. Figure~\ref{Examples} shows some negative (normal) and positive (abnormal) images that display the small variations between both classes.
\begin{figure}[h!]
\centering
\includegraphics[width=1\linewidth]{Examples.pdf}
\caption{Three examples of normal (left green-framed part) and abnormal images (right red-framed part) for the all the datasets coming from image databases. One can observe the very slight difference between the two classes.}\label{Examples}
\end{figure}
The images have a resolution of $1024 \times 1024$ and show different nature of complexity and anomalies. The Leather and Grid dataset are texture-like images, whereas the Hazelnut and PCBA are object-like ones. Some images are difficult to classify as normal or abnormal, in particular because the anomalies are so small that they seem to be normal variations. This is for example the case when the task is to detect a small scratch on an Hazelnut image, or a missing component on a PCBA image, as shown in the figure.
Since the customized loss is convex and differentiable, we could have connected a Resnet model (for instance) to process images directly, but we let the image task to a dedicated model. This allows to correctly deal with the resolution needs ($1024 \times 1024$) as well as the subtle normal/abnormal variabilities. Therefore, for the industrial datasets, a first step has been performed thanks to the \textit{VQGanoDIP} method \cite{bougaham2023composite}, namely the "normal version" generation with a VQ-GAN model and the metrics collection from these reconstructed images. All the information coming from this input image reconstruction form  a tabular dataset, with the positive and negative instances in row, and the metrics collected in column (664 features of VQ-GAN component losses, pixel-wise reconstruction quality, patch distances between the input and reconstructed images). This represents a proxy of the differences between the images and the normal version of these ones. When images are normal, these differences are small, which is the not the case when images present an anomaly. All the collected metrics forming the tabular data reflect this statement.

Concerning the two other datasets, the Wisconsin Diagnosis Breast Cancer (WDBC) \cite{breast_cancer_wisconsin_(diagnostic)_17} and the Credit Card Fraud Detection \cite{CCF} (CCF) have been chosen. These tabular datasets are widely used in the literature and are critical applications in the scope of our method. The WDBC dataset reports 30 features extracted from breast images (5th row of the Figure~\ref{Examples}) and describing the cell nucleus (e.g., radius, perimeter, symmetry). The CCF dataset consists of 30 features composed of the time elapsed between two transactions, the transaction amount and 28 principal components obtained with PCA from confidential information. It is populated by 284,807 negative examples, but only 492 ones (same number as positives number) have been randomly selected to reduce memory consumption and training time. Table \ref{datasets_table} shows the number of positive and negative instances for each datasets. One can observe that 4 datasets are imbalanced (Grid, Hazelnut, Leather, WDBC), with a majority of negative data.

\begin{table*}[h]
    \renewcommand{\arraystretch}{1}
    \begin{minipage}{\textwidth} 
    \caption{Number of positive and negative instances for each datasets.}\label{datasets_table}
    {\setlength{\tabcolsep}{0pt}
    \begin{center}
        \begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}lcccc@{\extracolsep{\fill}}}
            \toprule%
            \textit{Dataset} & \textit{\# Negative} & \textit{\# Positive}
            \\\cmidrule{1-1}\cmidrule{2-2}\cmidrule{3-3}%
            CCF & 492 & 492\\
            Grid & 142 & 57\\
            Hazelnut & 215 & 70\\
            Leather & 138 & 92\\
            PCBA & 174 & 174\\
            WDBC & 357 & 212\\
            
            \bottomrule%
        \end{tabular*}
    \end{center}}
    \end{minipage}
\end{table*}


\subsection{Experimental Setup}
\label{Experimental Setup}

% Average of 5 runs of 5-fold Cross Validation method, with a grid search on a set of hyperparameters for 5 methods: a BCE, a one way partial AUC, a two way partial AUC, an approximate AUC and our approximate partial AUC method.
% Metrics: max TPR @ FPR $\leq$ 40$\%$

% Lien avec Info Fusion : 
% XAI Transparency : Donner le score de prediction et en faire un indice de confiance.
% XAI Accountability: Si confiant pour dire OK ou NG, pas besoin d'op. Sinon besoin d'un humain.
% Trustwothness

The \textit{tapAUC} method implementation comes with some hyperparameter values to chose. A grid search is performed to select the best ones, maximizing the TPR @ FPR $\leq$ 50\%. This 50\% FPR threshold is arbitrary but sufficient to remove all cases where TPR could be 100\% because of a classifier judging all cases as positive, and scoping the false alarms to a reasonable value. The hyperparameters are the following: the total epoch number for training (60, 200, 500), the warmup period (25\%, 50\%, 75\% of the total epoch number), the margin that separates the positive and negative scores $\gamma$ (0.1, 0.3, 0.5, 0.7, 1) and the ratio of negative instances $\alpha$ to select after the warmup period (the single or 5\%, 10\%, 25\%, 50\%  highest scores of the negative instances). The optimizer is Adam with learning rate 0.01. The binary classification model is a fully connected network. It has an input layer with a number of neurons corresponding to the dataset features count, followed by a hidden layer of half the first layer size and an output layer of one neuron terminated by a sigmoïd function. The activation functions are ReLU, batch normalization is used and a dropout of 0.5 is set to avoid overfitting.

To compare the method performance, we ran the same experiments for 4 other methods, namely a traditional binary cross entropy loss (BCE), a squared hinge loss approximating the AUC ROC metric (similar to our method except the partial selection of the negatives), and the one-way KL-DRO estimator (OPAUC) and two-way (TPAUC) partial AUC method proposed in \cite{zhu2022auc}. A grid search has also been used to search the best hyperparameters. For the BCE loss, the total epoch number evaluated is 200 or 500. This is also the case for the squared hinge loss method, as well as the margin $\gamma$ to be 0.1, 0.3, 0.5, 0.7 or 1. For the OPAUC and TPAUC methods, some hyperparameters advised by the authors are kept (total epoch number is 60, $\tau$ is 1), and other ones are grid-searched. They are the margin $\gamma$ (0.5, 0.7 or 1), $\lambda$ (0.1, 1 or 10), and $\gamma$ for OPAUC or $\gamma_0$ and $\gamma_1$ for TPAUC (0.5 or 0.9).

For each dataset, a 5-fold stratified cross validation technique is performed for 5 different repetitions. A post-processing step is executed, in order to drop the constant and correlated features, and to scale their value between 0 and 1. For each fold splits and repetitions, a new seed is given to apply randomization. All experiments have been performed on an Intel i7 CPU and nVidia RTX A5000 GPU, with Python 3.8, Cuda 11.2 and Pytorch 1.10.

\subsection{Results}
\label{Results}

Subsection \ref{Experimental Setup} described the procedure chosen to evaluate the proposed method, and its competitors. The average accuracy, TPR and FPR are reported in Table \ref{metric_table}. This table results from the validation set of the stratified cross validation method, and for the 5 different repetitions. Five different losses are evaluated, being the binary cross entropy, the squared hinge loss, the one-way partial AUC, the two-way partial AUC and our \textit{tapAUC}. Hyperparameters that give the best TPR @ FPR $\leq$ 50\% are chosen, while considering the ZFN threshold.

\begin{table}[h]
\renewcommand{\arraystretch}{1.2}
\centering
% \caption{Accuracy, TPR and FPR by datasets and methods, for the best TPR value from the grid search. Values are expressed in \%, and averaged by each test set of the 5-fold cross validation procedure, for 5 different runs. The $\uparrow$ sign means the highest the best, unlike the $\downarrow$ sign means the lowest the best.}
\caption{Accuracy, TPR and FPR values by datasets and methods, based on the best TPR @ FPR $\leq$ 50\% from the grid search. Values are expressed in \%, and are averaged by each validation set of the 5-fold cross validation procedure, for 5 different runs. The $\uparrow$ sign means the highest the best, unlike the $\downarrow$ sign means the lowest the best. Bold values are the best ones of each column. Cells with gray background for CCF with BCE and Squared Hinge Loss indicate FPR $>$ 50\%, and are provided for information only. }

\label{metric_table}
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}l*{15}{c}@{}}
\toprule
Method & \multicolumn{3}{c}{BCE} & \multicolumn{3}{c}{Squared Hinge Loss} & \multicolumn{3}{c}{OPAUC} & \multicolumn{3}{c}{TPAUC} & \multicolumn{3}{c}{tapAUC (ours)} \\
\cmidrule{1-1} \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13} \cmidrule(lr){14-16}
Metric & ACC$\uparrow$ & TPR$\uparrow$ & FPR$\downarrow$ & ACC$\uparrow$ & TPR$\uparrow$ & FPR$\downarrow$ & ACC$\uparrow$ & TPR$\uparrow$ & FPR$\downarrow$ & ACC$\uparrow$ & TPR$\uparrow$ & FPR$\downarrow$ & ACC$\uparrow$ & TPR$\uparrow$ & FPR$\downarrow$ \\
\midrule
% CCF & 51.89 & 99.92 & 96.14 & 70.21 & 98.29 & 57.89 & \textbf{86.26} & 96.14 & \textbf{23.62} & 86.2 & 96.06 & 23.66 & 79.69 & \textbf{97.28} & 37.89 \\
CCF & \cellcolor{gray!25}51.89 & \cellcolor{gray!25}99.92 & \cellcolor{gray!25}96.14 & \cellcolor{gray!25}70.21 & \cellcolor{gray!25}98.29 & \cellcolor{gray!25}57.89 & \textbf{86.26} & 96.14 & \textbf{23.62} & 86.2 & 96.06 & 23.66 & 79.69 & \textbf{97.28} & 37.89\\
Grid & 96.27 & 87.36 & \textbf{0.14} & 91.77 & 89.09 & 7.18 & 96.58 & 89.18 & 0.42 & 96.28 & 87.73 & 0.28 & \textbf{96.77} & \textbf{89.42} & 0.28 \\
Hazelnut & \textbf{92.56} & 75.14 & 1.77 & 90.18 & 78.86 & 6.14 & 92.07 & 70.29 & 0.84 & 92.21 & 69.14 & \textbf{0.28} & 85.68 & \textbf{79.14} & 12.19 \\
Leather & 87.48 & 72.62 & \textbf{2.61} & 80.17 & 93.66 & 28.84 & \textbf{89.74} & 81.72 & 4.93 & 89.3 & 79.09 & 3.91 & 85.39 & \textbf{94.39} & 20.58 \\
PCBA & 79.3 & 84.9 & 26.32 & 86.18 & 90.22 & 17.93 & 92.3 & 93.9 & 9.31 & \textbf{93.61} & 92.96 & \textbf{5.75} & 86.95 & \textbf{95.61} & 21.72 \\
WDBC & 81.97 & 99.15 & 28.24 & 81.25 & 98.86 & 29.19 & 92.66 & 97.92 & 10.48 & \textbf{94.62} & 97.83 & \textbf{7.28} & 80.95 & \textbf{99.25} & 29.92 \\

\midrule
MEAN & 81.58 & 86.52 & 25.87 & 83.29 & 91.5 & 24.53 & 91.6 & 88.19 & 8.27 & \textbf{92.04} & 87.14 & \textbf{6.86} & 85.91 & \textbf{92.52} & 20.43 \\
\bottomrule
\end{tabular}
}
\end{table}



% \begin{table}[h!]
% \centering
% \caption{Performance metrics for different methods across datasets. Values are expressed in \%, and averaged by each test set of the 5-fold cross validation procedure, for 5 different runs. ACC$\uparrow$: higher is better; TPR$\uparrow$: higher is better; FPR$\downarrow$: lower is better.}
% \label{metric_table}
% \resizebox{\textwidth}{!}{
% \rotatebox{90}{ % Rotate the table 90 degrees
% \begin{tabular}{@{}l*{7}{c}@{}}
% \toprule
% Metric & CCF & Grid & Hazelnut & Leather & PCBA & WDBC & MEAN \\
% \midrule
% \multicolumn{8}{l}{\textbf{BCE}} \\
% ACC$\uparrow$ & \cellcolor{gray!25}51.89 & 96.27 & \textbf{92.56} & 87.48 & 79.3 & 81.97 & 81.58 \\
% TPR$\uparrow$ & \cellcolor{gray!25}99.92 & 87.36 & 75.14 & 72.62 & 84.9 & 99.15 & 86.52 \\
% FPR$\downarrow$ & \cellcolor{gray!25}96.14 & \textbf{0.14} & 1.77 & \textbf{2.61} & 26.32 & 28.24 & 25.87 \\
% \midrule
% \multicolumn{8}{l}{\textbf{Squared Hinge Loss}} \\
% ACC$\uparrow$ & \cellcolor{gray!25}70.21 & 91.77 & 90.18 & 80.17 & 86.18 & 81.25 & 83.29 \\
% TPR$\uparrow$ & \cellcolor{gray!25}98.29 & 89.09 & 78.86 & 93.66 & 90.22 & 98.86 & 91.5 \\
% FPR$\downarrow$ & \cellcolor{gray!25}57.89 & 7.18 & 6.14 & 28.84 & 17.93 & 29.19 & 24.53 \\
% \midrule
% \multicolumn{8}{l}{\textbf{OPAUC}} \\
% ACC$\uparrow$ & \textbf{86.26} & 96.58 & 92.07 & \textbf{89.74} & 92.3 & 92.66 & 91.6 \\
% TPR$\uparrow$ & 96.14 & 89.18 & 70.29 & 81.72 & 93.9 & 97.92 & 88.19 \\
% FPR$\downarrow$ & \textbf{23.62} & 0.42 & 0.84 & 4.93 & 9.31 & 10.48 & 8.27 \\
% \midrule
% \multicolumn{8}{l}{\textbf{TPAUC}} \\
% ACC$\uparrow$ & 86.2 & 96.28 & 92.21 & 89.3 & \textbf{93.61} & \textbf{94.62} & \textbf{92.04} \\
% TPR$\uparrow$ & 96.06 & 87.73 & 69.14 & 79.09 & 92.96 & 97.83 & 87.14 \\
% FPR$\downarrow$ & 23.66 & 0.28 & \textbf{0.28} & 3.91 & \textbf{5.75} && \textbf{6,86}\\
% \midrule
% \multicolumn{8}{l}{\textbf{sapAUC (ours)}} \\
% ACC$\uparrow$&79,69&\textbf {96,77}&85,68&85,39&86,95&80,95&85,91\\
% TPR$\uparrow$&\textbf {97,28}&\textbf {89,42}&\textbf {79,14}&\textbf {94,39}&\textbf {95,61}&\textbf {99,25}&\textbf {92,52}\\
% FPR$\downarrow$&37,89&0,28&12,19&20,58&21,72&29,92&20,43\\
% \bottomrule
% \end{tabular}
% }}
% \end{table}


% \begin{table}[h]
% \centering
% \caption{Performance metrics for different methods across datasets. Values are expressed in \%, and averaged by each test set of the 5-fold cross validation procedure, for 5 different runs. ACC$\uparrow$: higher is better; TPR$\uparrow$: higher is better; FPR$\downarrow$: lower is better.}
% \label{metric_table}
% \resizebox{\textwidth}{!}{
% \begin{tabular}{@{}l*{7}{c}@{}}
% \toprule
% Metric & CCF & Grid & Hazelnut & Leather & PCBA & WDBC & MEAN \\
% \midrule
% \multicolumn{8}{l}{\textbf{BCE}} \\
% ACC$\uparrow$ & \cellcolor{gray!25}51.89 & 96.27 & \textbf{92.56} & 87.48 & 79.3 & 81.97 & 81.58 \\
% TPR$\uparrow$ & \cellcolor{gray!25}99.92 & 87.36 & 75.14 & 72.62 & 84.9 & 99.15 & 86.52 \\
% FPR$\downarrow$ & \cellcolor{gray!25}96.14 & \textbf{0.14} & 1.77 & \textbf{2.61} & 26.32 & 28.24 & 25.87 \\
% \midrule
% \multicolumn{8}{l}{\textbf{Squared Hinge Loss}} \\
% ACC$\uparrow$ & \cellcolor{gray!25}70.21 & 91.77 & 90.18 & 80.17 & 86.18 & 81.25 & 83.29 \\
% TPR$\uparrow$ & \cellcolor{gray!25}98.29 & 89.09 & 78.86 & 93.66 & 90.22 & 98.86 & 91.5 \\
% FPR$\downarrow$ & \cellcolor{gray!25}57.89 & 7.18 & 6.14 & 28.84 & 17.93 & 29.19 & 24.53 \\
% \midrule
% \multicolumn{8}{l}{\textbf{OPAUC}} \\
% ACC$\uparrow$ & \textbf{86.26} & 96.58 & 92.07 & \textbf{89.74} & 92.3 & 92.66 & 91.6 \\
% TPR$\uparrow$ & 96.14 & 89.18 & 70.29 & 81.72 & 93.9 & 97.92 & 88.19 \\
% FPR$\downarrow$ & \textbf{23.62} & 0.42 & 0.84 & 4.93 & 9.31 & 10.48 & 8.27 \\
% \midrule
% \multicolumn{8}{l}{\textbf{TPAUC}} \\
% ACC$\uparrow$ & 86.2 & 96.28 & 92.21 & 89.3 & \textbf{93.61} & \textbf{94.62} & \textbf{92.04} \\
% TPR$\uparrow$ & 96.06 & 87.73 & 69.14 & 79.09 & 92.96 & 97.83 & 87.14 \\
% FPR$\downarrow$ & 23.66 & 0.28 & \textbf{0.28} & 3.91 & \textbf{5.75} & \textbf{7.28} & \textbf{6.86} \\
% \midrule
% \multicolumn{8}{l}{\textbf{sapAUC (ours)}} \\
% ACC$\uparrow$ & 79.69 & \textbf{96.77} & 85.68 & 85.39 & 86.95 & 80.95 & 85.91 \\
% TPR$\uparrow$ & \textbf{97.28} & \textbf{89.42} & \textbf{79.14} & \textbf{94.39} & \textbf{95.61} & \textbf{99.25} & \textbf{92.52} \\
% FPR$\downarrow$ & 37.89 & 0.28 & 12.19 & 20.58 & 21.72 & 29.92 & 20.43 \\
% \bottomrule
% \end{tabular}
% }
% \end{table}


% \subsection{Ablation Study}
% \label{Ablation Study}
% Remove the subset of neg samples = aAUC
% Remove the margin of the surrogate loss function (hinge, heavyside, sigmoid)
% Change the surrogate loss function
% Change the neural network architecture

\section{Discussion}
\label{Discussion}

The experiments performed in Section \ref{Evaluation} show results that are discussed in this section. 
% TPR
The first insight of interest in our study is the TPR under a 50\% FPR threshold. This FPR threshold is chosen to bound the false positive alarms at a reasonable value, convenient for a real-word application. For all the datasets, we can see from Table \ref{metric_table} that our \textit{tapAUC} method brings the highest TPR with a mean of 92.52\%, being around 4.3\% and 5.4\% better than OPAUC and TPAUC respectively. This represents an improvement that the business in the field will benefit, by dealing with a smaller ratio of missed detection of anomalies. 
In more detail, the Hazelnut dataset gives the smallest TPR (79.14\%) while WDBC gives the largest one (99.25\%). However, this is not specific to \textit{tapAUC} because all the methods respectively struggle and excel with these two datasets. We can therefore conclude that the Hazelnut dataset is the less expressive in terms of anomaly patterns, whereas the WDBC dataset shows more differences between the benign (negative) and malign (positive) class. Another observation is that results of OPAUC and TPAUC are very close to each other, as well as our \textit{tapAUC} and a simple squared hinge loss. This makes sense because these two pairs of methods are similar to each other and only small changes occur in the subregion targeted. 

% FPR / Accuracy
For the critical applications our method aims for, the good TPR results at a limited FPR is the priority. Even if it comes with lower accuracy of FPR results, the TPR brings more trustworthiness when the goal is to maximize the detection of the positive instances. In terms of accuracy and FPR, we indeed notice from Table \ref{metric_table} that the methods TPAUC and OPAUC have better results and thus raise much less false alarms. Indeed, our method yields an average of 20.43\% FPR and 85.91\% accuracy whereas, for TPAUC and OPAUC, these values drop to 6.86\% and 8.27\% for the FPR, and jump to 92.04\% and 91.06\% for the accuracy. This is the trade-off to pay for a highly sensitive anomaly detection tool.
In the detail, the methods OPAUC and TPAUC also show similar results of FPR, whereas, this time, it is not the case for the squared hinge loss and our \textit{tapAUC} that have a better FPR. This reflects a better understanding of the negative class for our method, able to limit their misclassification. Indeed, the dynamic focus on the challenging negative instances during training helps to mitigate the false positive, being the cornerstone of our proposal. Finally, one can also notice that for the CCF dataset, the binary cross entropy and the squared hinge losses are not considered as competitors, because they are not able to reach an FPR below 50\%, which is not convenient for real-world business situations.
% As an example, Figure XX shows the AUC ROC curve and the score distributions for the Grid dataset and the BCE, the OPAUC and our \textit{tapAUC} method. Through this example, we can see that the ZFN threshold, obtained on the train set, gives a better TPR for our method at the cost of a lower FPR. 

% Explication
The observation for OPAUC, TPAUC and \textit{tapAUC} can be explained by the different method strategies. Even if the focus is on the same region of the partial AUC, the way how it is performed is different. Indeed, OPAUC targets the left side of the curve being the lowest FPR possible, and TPAUC does the same but with an additional constraint on the highest TPR possible. However, unlike our method, the training step does not consider any specific threshold where only the smallest optimizable portion is set. For \textit{tapAUC}, this threshold is dynamically calculated so that it targets very precisely the smallest region that prevents the TPR to be 100\% at the lowest FPR possible. While other methods perform this strategy roughly, by associating weights to the bounded region, ours ends up with a dynamic optimization target in the subregion of interest, guiding the classifier to its best FPR at 100\% TPR.

% Confidence Index towards ZFN
For \textit{tapAUC}, the TPR of Table \ref{metric_table} shows a relatively high value for the different datasets, proving its ability to generalize with unseen data. However, despite these good results, the initial claim was to build a classifier dedicated to reach all the positives well classified. In order to get closer to a more trustworthy tool, an operator solicitation is needed to judge difficult instances that are uncertain, including the few positives misclassified. An uncertainty interval is therefore defined, corresponding to the worst positive score below the threshold as a lower bound, and the threshold itself as an upper bound. All instance scores that fall into this interval is considered as uncertain, and an operator needs to confirm or infirm the classifier decision. Indeed, the closer the anomaly score is to the threshold, the greater the uncertainty. At the opposite, the scores of the negatives far below the threshold are those with the largest confidence, as well as the ones of the positives far above. This human-in-the-loop step helps getting close towards a zero false negative method, but requires a small amount of manual control. We reported in Table \ref{Uncertainty} the lower bound values averaged across all the validation sets of the 5 repetitions, as well as the number of data to manually check it represents and the positives concerned (useful checks), by dataset. 
\begin{table*}[h]
    \renewcommand{\arraystretch}{1}
    \begin{minipage}{\textwidth} 
    \caption{Lower bound, manual and useful checks (in \%) averaged across all the validation sets and repetitions, by dataset. The uncertainty interval is defined as [Threshold-Lower Bound : Threshold]. A mean of 14.71\% of the instances have to be manually checked to guarantee no false negatives, which represents 3.61\% of the positives.}\label{Uncertainty}
    {\setlength{\tabcolsep}{0pt}
    \begin{center}
        \begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}lccccc@{\extracolsep{\fill}}}
            \toprule%
            \textit{Dataset} & \textit{Lower Bound} & \textit{Manual Checks (\%)} & \textit{Useful  Checks (\%)}
            \\\cmidrule{1-1}\cmidrule{2-2}\cmidrule{3-3}\cmidrule{4-4}%
            CCF & 0.1698 & 12.64 & 1.38\\
            Grid & 0.0122 & 13.97 & 4.21\\
            Hazelnut & 0.0173 & 20.84 & 10.86\\
            Leather & 0.0226 & 7.91  & 2.61\\
            PCBA & 0.0078 & 4.77 & 1.95\\
            WDBC & 0.2984 & 28.12 & 0.66\\
            \toprule
            MEAN & 0.088 & 14.71 & 3.61\\            
            \bottomrule%
        \end{tabular*}
    \end{center}}
    \end{minipage}
\end{table*}
This report shows, in average, how the uncertainty interval allows to catch the few positive instances. 14.71\% of the data have to be manually checked, where 3.61\% are useful because positive. This analysis shows that a small amount of manual check brings more confidence, even for the more difficult positive data that the classifier struggles to catch on the validation set.

% , even if 3.61\% of these samples belong to the samples to manually check by the business expert.


% This lower bound value is averaged across all the validation set of the 5 repetitions and the 5-fold cross validation. 

% To avoid a request for each classification decision, an uncertainty interval is defined thanks to the furthest misclassified positive score from the threshold averaged across all the validation set of the 5 repetitions and the 5-fold cross validation. 

% Once the distance of this worst sample score from the threshold identified, we consider it as the uncertainty interval, where the operator needs to confirm or infirm the classifier decision. 

% This human-in-the-loop step helps getting close towards a zero false negative method, and require a small amount of manual control. We reported in Table \ref{Uncertainty} the threshold, the uncertainty interval, the number of samples to manually check and its split in terms of positives and negatives, by dataset. This report shows how well the tapAUC method catches well the positive samples, even if 3.61\% of these samples belong to the samples to manually check by the business expert.

% \begin{table*}[h]
%     \renewcommand{\arraystretch}{1}
%     \begin{minipage}{\textwidth} 
%     \caption{Uncertainty Interval by dataset. One can observe that over the 14.71\% manual checks, a minority of 3.61\% are positives. This shows that positive are well detected without manual checks, even if it remains important to guarantee no false negatives.}\label{Uncertainty}
%     {\setlength{\tabcolsep}{0pt}
%     \begin{center}
%         \begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}lcccccc@{\extracolsep{\fill}}}
%             \toprule%
%             \textit{Dataset} & \textit{Interval} & \textit{\# Checks} & \textit{\# Negative Checks} & \textit{\# Positive Checks}
%             \\\cmidrule{1-1}\cmidrule{2-2}\cmidrule{3-3}\cmidrule{4-4}\cmidrule{5-5}%
%             CCF & $\pm$0.1698 & 12.64\% & 23.9\% & 1.38\%\\
%             Grid & $\pm$0.0122 & 13.97\% & 17.89\% & 4.21\%\\
%             Hazelnut & $\pm$0.0173 & 20.84\% & 24.09\% & 10.86\%\\
%             Leather & $\pm$0.0226 & 7.91\% & 11.45\% & 2.61\%\\
%             PCBA & $\pm$0.0078 & 4.77\% & 7.59\% & 1.95\%\\
%             WDBC & $\pm$0.2984 & 28.12\% & 44.43\% & 0.66\%\\
%             \toprule
%             MEAN & $\pm$0.088 & 14.71\% & 21.56\% & 3.61\%\\
%             \bottomrule%
%         \end{tabular*}
%     \end{center}}
%     \end{minipage}
% \end{table*}


\section{Conclusion}
\label{Conclusion}

In this work, a trustworthy anomaly detection method called \textit{tapAUC} is proposed, based on an approximated partial AUC (pAUC) loss, and a dedicated algorithm that trains a binary classifier. The main objective is to tackle the trustworthiness constraint required for critical applications such in the industrial, medical or cybersecurity domains. Indeed, these businesses need to detect the positive (abnormal) instances with a high degree of confidence, with a reasonable ratio of false alarms, unlike other non-critical applications that only aim to optimize the accuracy. This requirement is a key characteristic for real-world businesses that want to adopt AI techniques in their daily concerns. It brings a contribution towards the Ethics Guidelines for Trustworthy AI asking to improve explainable AI (XAI) techniques for such organizations. To do so, the model loss is arranged to optimize a pAUC through a squared hinge loss applied to a subset of the positive and the challenging negative instances. This mechanism helps to focus on the specific region where the True Positive Rate (TPR) is just below 100\%. Iteration after iteration, the model is optimized in this region that dynamically changes, and the model gets iteratively optimized by minimizing the False Positive Rate (FPR). At the end, selecting the threshold that generates zero false negatives (ZFN) under limited false positives (LFP) gives a TPR improvement of 4.3\% at a FPR cost of 12.2\% compared to other state-of-the-art methods. This represents a TPR of 92.52\% at a 20.43\% FPR. These results are the validation set metrics average of a stratified 5-fold cross validation technique, performed for 5 repetitions and across 6 datasets. Then, an uncertainty interval is build in order to get closer to a more trustworthy tool. A human-in-the-loop strategy is required for an additional manual check, when the instance prediction score falls close to the threshold. The experiments show that an average of 14.71\% of manual checks are necessary to catch the few misclassified positives.

As future works, these promising results open new research questions. A first one concerns the training strategy. Indeed, the iterative and dynamic focus on a specific region makes the optimization non-smooth and somehow difficult to converge, because the challenging negative instances could change during training. It could be interesting to study how a different approach could smoothen these changes, in order to improve the training step. Another direction concerns the end-to-end learning with images. This study is tailored for tabular data and treat image datasets with the statistics coming from another dedicated method (\textit{VQGanoDIP} \cite{bougaham2023composite}). This is motivated by the $1024 \times 1024$ resolution needed, and the complex discrimination observed at the pixel level. A way to get rid of this limitation is to split the full images into small patches ($224 \times 224$ each for example), and use a Resnet model to extract features in the patches that would replace the tabular statistics data. This would enable a more straightforward learning with eventual better classification performances. Finally, the uncertainty interval strategy still shows possibilities of improvement in terms of building this score-based confidence. 


% blabla

% ***************************************************************
% GanoDIP
% These promising results raise new questions to be investigated in future works. In
% particular, it would be interesting to study the behaviour of our method when trained for
% multiple product lines. Indeed, the currently considered dataset comes from a particular
% product line, with specificity cameras (e.g., in terms of luminosity). In practice, a new
% model would be needed for each product line, motivating future work to create a single
% model for all product lines in a factory.

% ***************************************************************
% VQGanoDIP
% These promising results open new research questions for future works. Specifically, it would be interesting to find a better strategy to let the classifier learn the decision function integrating the zero-false-negative constraint directly, in an end-to-end manner. Another research direction could be to develop a reconstruction model that performs well on any type of dataset, whatever the normal variations characteristic or the anomaly sizes.

% ***************************************************************
% CGAN-AD:
% Regarding potential limitations, one should first point out the computational complexity. Compared to other state-of-the-art methods, using our approach is sometimes more expensive as it requires to train two generators and two discriminators instead of one (or only one autoencoder). However, this only holds at the training step, as the inference step only requires one of the two generators to work. Also, as our approach is the first one able to take advantage of the abnormal images, we advocate here that the increase in computational resources during training is worth the improvement in terms of AD. Another limitation is related to the nature of the anomalies. If anomalies are really small and punctual (small object-shaped anomalies), cycle-GAN based methods may struggle to highlight the anomaly (see for example the weak performances obtained on the Screw dataset).









% %% Use \subsection commands to start a subsection.
% % \subsection{Example Subsection}
% % \label{subsec1}

% % Subsection text.

% %% Use \subsubsection, \paragraph, \subparagraph commands to 
% %% start 3rd, 4th and 5th level sections.
% %% Refer following link for more details.
% %% https://en.wikibooks.org/wiki/LaTeX/Document_Structure#Sectioning_commands

% \subsubsection{Mathematics}
% %% Inline mathematics is tagged between $ symbols.
% This is an example for the symbol $\alpha$ tagged as inline mathematics.

% %% Displayed equations can be tagged using various environments. 
% %% Single line equations can be tagged using the equation environment.
% \begin{equation}
% f(x) = (x+a)(x+b)
% \end{equation}

% %% Unnumbered equations are tagged using starred versions of the environment.
% %% amsmath package needs to be loaded for the starred version of equation environment.
% \begin{equation*}
% f(x) = (x+a)(x+b)
% \end{equation*}

% %% align or eqnarray environments can be used for multi line equations.
% %% & is used to mark alignment points in equations.
% %% \\ is used to end a row in a multiline equation.
% \begin{align}
%  f(x) &= (x+a)(x+b) \\
%       &= x^2 + (a+b)x + ab
% \end{align}

% \begin{eqnarray}
%  f(x) &=& (x+a)(x+b) \nonumber\\ %% If equation numbering is not needed for a row use \nonumber.
%       &=& x^2 + (a+b)x + ab
% \end{eqnarray}

% %% Unnumbered versions of align and eqnarray
% \begin{align*}
%  f(x) &= (x+a)(x+b) \\
%       &= x^2 + (a+b)x + ab
% \end{align*}

% \begin{eqnarray*}
%  f(x)&=& (x+a)(x+b) \\
%      &=& x^2 + (a+b)x + ab
% \end{eqnarray*}

% %% Refer following link for more details.
% %% https://en.wikibooks.org/wiki/LaTeX/Mathematics
% %% https://en.wikibooks.org/wiki/LaTeX/Advanced_Mathematics

% %% Use a table environment to create tables.
% %% Refer following link for more details.
% %% https://en.wikibooks.org/wiki/LaTeX/Tables
% \begin{table}[t]%% placement specifier
% %% Use tabular environment to tag the tabular data.
% %% https://en.wikibooks.org/wiki/LaTeX/Tables#The_tabular_environment
% \centering%% For centre alignment of tabular.
% \begin{tabular}{l c r}%% Table column specifiers
% %% Tabular cells are separated by &
%   1 & 2 & 3 \\ %% A tabular row ends with \\
%   4 & 5 & 6 \\
%   7 & 8 & 9 \\
% \end{tabular}
% %% Use \caption command for table caption and label.
% \caption{Table Caption}\label{fig1}
% \end{table}


% %% Use figure environment to create figures
% %% Refer following link for more details.
% %% https://en.wikibooks.org/wiki/LaTeX/Floats,_Figures_and_Captions
% \begin{figure}[t]%% placement specifier
% %% Use \includegraphics command to insert graphic files. Place graphics files in 
% %% working directory.
% \centering%% For centre alignment of image.
% \includegraphics{example-image-a}
% %% Use \caption command for figure caption and label.
% \caption{Figure Caption}\label{fig1}
% %% https://en.wikibooks.org/wiki/LaTeX/Importing_Graphics#Importing_external_graphics
% \end{figure}


% %% The Appendices part is started with the command \appendix;
% %% appendix sections are then done as normal sections
% \appendix
% \section{Example Appendix Section}
% \label{app1}

% Appendix text.

%% For citations use: 
%%       \cite{<label>} ==> [1]

%%
% Example citation, See \cite{lamport94}.

%% If you have bib database file and want bibtex to generate the
%% bibitems, please use
%%
%%  \bibliographystyle{elsarticle-num} 
%%  \bibliography{<your bibdatabase>}
\bibliographystyle{elsarticle-num}
\bibliography{bibliography.bib}% common bib file

%% else use the following coding to input the bibitems directly in the
%% TeX file.

%% Refer following link for more details about bibliography and citations.
%% https://en.wikibooks.org/wiki/LaTeX/Bibliography_Management

% \begin{thebibliography}{00}

% %% For numbered reference style
% %% \bibitem{label}
% %% Text of bibliographic item

% \bibitem{lamport94}
%   Leslie Lamport,
%   \textit{\LaTeX: a document preparation system},
%   Addison Wesley, Massachusetts,
%   2nd edition,
%   1994.


% \end{thebibliography}
\end{document}

\endinput
%%
%% End of file `elsarticle-template-num.tex'.
