
Decision tree optimization is core to interpretable machine learning \citep{rudin2022interpretable}. Simple decision trees present the entire model reasoning process transparently, directly allowing faithful interpretations of the model~\citep{arrieta2020explainable}. This helps users choose whether to trust the model and to critically examine any perceived flaws.


% \vspace{-0.3cm}
Optimizing the performance of decision trees while preserving their simplicity presents a significant challenge. Traditional greedy methods scale linearly with both dataset size and the number of features \citep{breiman1984classification, quinlan2014c45}. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/headline_figure.png}
    \caption{An illustration of the power of our optimization algorithm. We train $3$ decision trees on the Bike dataset, with the aim of predicting bike rentals in Washington DC in a given time period. A greedy tree is fast but suboptimal. An optimal tree is well performing but \textit{very} slow. Our algorithm strikes the perfect balance, providing well performing trees in a \textit{SPLIT} second, orders of magnitude faster than optimal approaches seen in literature.}
    \label{fig:headline_figure}
\end{figure}
However, these methods tend to yield suboptimal results, lacking general guarantees on either sparsity or accuracy. Recent advances in decision tree algorithms use dynamic programming techniques combined with branch-and-bound strategies, offering solutions that are faster than brute-force approaches and provably optimal \citep{gosdt, dl85,murtree, gosdt_guesses}. In fact, \citet{murtree} and \cite{van2024optimal} reveal an average gap of $1$-$2$ percentage points between greedy and optimal trees, with \citet{murtree} showing that some datasets can exhibit gaps as large as $10$ percentage points. These algorithms struggle to scale to datasets with hundreds or thousands of features or to deeper trees. It seems that we should return to greedy methods for larger-scale problems, but this would come at a loss of performance. Ideally, we should leverage greed only when it does not significantly deviate from optimality and use dynamic programming otherwise.
Dynamic programming approaches build trees recursively, downward from the root. Problems farther from the root contain fewer samples and produce fewer splits. As we show, \textit{greedy splits near the root sacrifice performance}, while \textit{greedy splits near the leaves produce performance close to the optimal}. This suggests that we can tolerate less precision on problems close to leaves than on problems closer to the root -- and that full optimization on those problems closer to the leaves yields only marginal returns relative to greedy, since we only have a few splits remaining. This has enormous implications, since the number of candidate trees increases exponentially with increases in depth; using greedy splitting closer to the leaves of the tree massively reduces the search space.

We leverage this observation to construct SPLIT (SParse Lookahead for Interpretable Trees), a family of decision tree algorithms that are over \textbf{100$\times$ faster than state of the art optimal decision tree algorithms, with negligible sacrifice in performance}. They can also be tuned to a user-defined level of sparsity. Instead of searching through the entire space of decision trees up to a given depth, our algorithm performs dynamic programming with branch and bound up to only 
a shallow ``lookahead'' depth, conditioned on all splits henceforth being chosen greedily.


Our contributions are as follows.
- We develop a family of decision tree algorithms that scale with the dataset size and number of features comparably to standard greedy algorithms but produce trees that are as accurate and sparse as optimal ones \citep[e.g.,][]{gosdt}.

- We extend our decision tree algorithms to allow scalable, accurate approximations of the Rashomon set of decision trees \citep{breiman2001statistical, xin2022treefarms}.

- We theoretically prove that our algorithms scale exponentially faster in the number of features than optimal decision tree methods and are capable of performing arbitrarily better than a purely greedy approach.
