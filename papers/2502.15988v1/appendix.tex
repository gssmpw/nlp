\section{Appendix}

\subsection{Further Comparisons With Other Methods}
\subsubsection{More datasets with depth $5$ trees}
In Section \ref{sec:experiments} of the paper, we showed results for three datasets. Here, we evaluate SPLIT, LicketySPLIT, and its contemporaries on $6$ additional datasets. All datasets were evaluated on three random $80$-$20$ train-test splits of the data, with the average and standard error reported. Results are in Figure \ref{fig:comparisons_2}. Note that Covertype has smaller error bars because the dataset size is much larger -- it has $\sim5\times 10^6$ examples, while COMPAS and HELOC have only $\sim10^4$ examples.
\begin{figure}[H]
    \centering
        \centering
    \includegraphics[width=0.77\linewidth]{figures/split_comparisons_appendix_2.pdf}
    \includegraphics[width=0.8\linewidth]{figures/split_comparisons_bank_hypothyroid_spambase.pdf}
    \caption{A performance comparison between our algorithm and those in literature. The lower row are zoomed in versions of the red boxes in the upper row. This is complementary to Figure \ref{fig:comparisons} and shows more datasets for completeness. The depth budget for all algorithms whose depth budget can be specified is $5$.}
    \label{fig:comparisons_2}
\end{figure}
\subsubsection{What about depth $4$ trees?}
In this section, we perform the same evaluation as above, but with depth $4$ trees. We set the lookahead depth as $1$. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/split_comparisons_appendix_depth_4.pdf}
    \includegraphics[width=0.8\linewidth]{figures/split_comparisons_appendix_depth_4_2.pdf}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/split_comparisons_bank_hypothyroid_spambase_depth_4.pdf}
    \caption{A performance comparison between our algorithm and those in literature. Depth $4$ - Lookahead depth $1$}
    \label{fig:split_comparisons_appendix_depth_4}
\end{figure}

\subsubsection{What about depth $6$ trees?}
In this section, we perform the same evaluation as above, but with depth $6$ trees. We set the lookahead depth as $2$. Note that Murtree and GOSDT are not included in the comparison as they take much longer to run for deeper trees.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.78\linewidth]{figures/split_comparisons_appendix_depth_6.pdf}
    \includegraphics[width=0.78\linewidth]{figures/split_comparisons_appendix_depth_6_2.pdf}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.78\linewidth]{figures/split_comparisons_bank_hypothyroid_spambase_depth_6.pdf}
    \caption{A performance comparison between our algorithm and those in literature. Depth $6$ - Lookahead depth $2$}
    \label{fig:split_comparisons_appendix_depth_6}
\end{figure}
\subsection{Motivation for SPLIT: Near Optimal Trees Are Generally Greedier Near the Leaves}
 To test the claim that near optimal trees are likely to behave greedily closer to the leaf, we first generate the Rashomon set of decision trees \citep{xin2022treefarms} for various values of sparsity penalty $\lambda$ and Rashomon bound $\epsilon$.  Let $T \in \mathcal{R}(D, \lambda, \epsilon, d)$ be a tree in the Rashomon set, and let $n \in T$ be any node in $T$. Then, we compute the fraction of all nodes at a given level $\ell \leq d$ (where level $0$ corresponds to the root) that were greedy (by which we mean that the split at this node in the tree is optimal with respect to information gain). This corresponds to the following proportion:
{
\begin{equation}
\label{eq:rset_ratio}
\frac{\sum\limits_{T\in \mcR(D, \lambda, \epsilon, d)} \sum\limits_{\textrm{$n$} \in T }\mathbbm{1}[\textrm{$n$ is greedy $\land$ level$(n) = \ell$}]}{\sum\limits_{T \in \mcR(D, \lambda, \epsilon, d)} \sum\limits_{\textrm{$n$} \in T }\mathbbm{1}[\textrm{level}(n) = \ell]}
\end{equation}
}
Figure \ref{fig:greedy_heatmap_rset} shows the results of this enumeration for $6$ different datasets for different values of $\epsilon$ and $\lambda$. We note that there is a general increase in percentage of greedy splits as one goes deeper in the tree. 

\label{sec:characterization_of_near_optimal_trees}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/greedy_heatmap_rset.pdf}
    \caption{A heatmap of the proportion of splits of trees in the Rashomon set that are greedy, stratified by level, for different ($\lambda, \epsilon$) combinations. Only $4$ levels are shown as the $5^{th}$ level corresponds to the leaf. The greyed out regions in the bottom right of a plot represent ($\lambda, \epsilon$) for which the Rashomon set did not contain any trees of that depth. Generally, as you approach the leaves, the proportion of splits appearing in $\epsilon$-optimal trees become increasingly greedy. This is especially noticeable for the Netherlands, Covertype, and COMPAS datasets.}
    \label{fig:greedy_heatmap_rset}
\end{figure}
\subsection{Many Near-Optimal Trees Exhibit Monotonically Decreasing Optimality Gaps Closer to Leaves}
\label{sec:near_optimal_monotonic}
Consider an $\epsilon$-optimal tree $T \in \mathcal{R}(D, \lambda, \epsilon, d)$. For a subtree $t$ of $T$, define $\lambda_{t}$ as the value of $\lambda$ that results in the greedy tree, $T_{g}$, having the same number of leaves as $t$. We now define the \textit{optimality gap} $\delta(D_{t}, t)$ as the difference between the loss of $t$ and the loss of an equally sparse greedy tree on the sub-problem associated with $t$. This enables a fair performance comparison between greedy and optimal trees, as the training loss of any given tree will otherwise monotonically decrease with the number of leaves. 
\begin{equation}
    \delta(D_{t}, t) = L(t, D_{t}, \lambda) - L(T_{g}(D_{t}, \text{depth}(t), \lambda_{t}), D_{t},\lambda_t).
\end{equation}
For a tree $T \in \mcR$, we then compute the average optimality gap associated with subtrees at each level. That is, given a level $\ell$, we compute:
\begin{equation}
   \beta(T, D, l) = \frac{\sum\limits_{\textrm{$t$} \in T }\delta(D_{t}, t)\mathbbm{1}[\textrm{$t$ is rooted at level $\ell$}]}{{\sum\limits_{\textrm{$t$} \in T }\mathbbm{1}[\textrm{$t$ is rooted at level $\ell$}]}}.
\end{equation}
We want to determine if $\beta(T, D, l)$ is monotonically decreasing with $\ell$ for a given tree $T$ -- if this is true, then being greedy closer to the leaf does not incur much loss in performance. Our intuition is as follows: if there are many such near optimal trees, then a semi-greedy search strategy could potentially uncover at least one of them. The following statistic computes the proportion of all trees in the Rashomon set that have monotonically decreasing optimality gaps as $\ell$ increases (i.e., moves from root towards leaves):
\begin{equation}
m(D,\lambda, \epsilon,d) =\frac{
    \sum_{T \in \mathcal{R}(D,\lambda, \epsilon, d)} 
    \mathbbm{1}\left[ \beta(T,D,\ell) 
    \text{ is monotonically decreasing with } \ell \right]
}{|\mathcal{R}(D,\lambda, \epsilon,d)|
}.
\end{equation}
Figure \ref{fig:rset_monotonically_decreasing_optimality_gap} shows this statistic for Rashomon sets with varying values of the sparsity penalty $\lambda$. We fix $\epsilon = 0.025$. The sparser a near-optimal tree, the more likely that it will be greedy, however, for all datasets, there exist near-optimal trees with monotonically decreasing optimality gaps even for low sparsity penalties. This has important algorithmic implications for developing interpretable models, because it means that a search strategy that is increasingly greedy near the leaves can produce a near-optimal tree.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/rset_2.png}
    \caption{Percentage of trees in the Rashomon set that exhibit monotonically decreasing optimality gaps. For sparse trees (i.e., where $\lambda$ is larger), we are more likely to find a tree whose optimality gap is consistently decreasing at each level. This suggests that behaving greedily only near the leaves can produce a well-performing tree.}
    \label{fig:rset_monotonically_decreasing_optimality_gap}
\end{figure}

\subsection{Miscellaneous Properties of SPLIT}
\label{sec:appendix_evaluation}
\subsubsection{Which Lookahead Depth Should I Use?}
In this section, we explore the effect of the lookahead depth on the runtime and regularised test and train losses. We use the aggressively binarized versions of the datasets, as elaborated in Section \ref{sec:experimental_setup}. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/lookahead_depth_runtime.pdf}
    \caption{Runtime as a function of the lookahead depth. $\lambda = 0.001$}
    \label{fig:lookahead_depth_runtime}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/lookahead_depth_train_loss.pdf}
    \caption{Regularised \textbf{training loss} as a function of the lookahead depth. $\lambda = 0.001$}
    \label{fig:lookahead_depth_train_loss}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/lookahead_depth_test_loss.pdf}
    \caption{Regularised \textbf{test loss} as a function of the lookahead depth. $\lambda = 0.001$}
    \label{fig:lookahead_depth_test_loss}
\end{figure}
From the figures, we see that there indeed exists an optimal lookahead depth that minimizes the runtime of SPLIT. At this depth, however, there is only a small increase in regularised training loss. Surprisingly, the test loss can also be lower at the runtime minimizing depth. 

\subsubsection{Are SPLIT Trees in the Rashomon Set?}
This evaluation characterises the near optimal behaviour of trees produced by our algorithms. In particular, we're interested in understanding how often trees produced by our algorithms lie in the Rashomon set. To do this, we sweep over values of $\lambda$. For each $\lambda$, we first generate SPLIT and LicketySPLIT trees and compute the minimum value of $\epsilon$ needed such that they are in the corresponding Rashomon set of decision trees with depth budget $5$ -- this is denoted by the respective frontiers of both algorithms. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/is_split_in_rset.pdf}
    \caption{An illustration of near-optimality of our algorithms for depth budget $5$. The light yellow region represents the $(\lambda, \epsilon)$ configurations for which only SPLIT produces trees in the Rashomon set, while the darker region represents $(\lambda, \epsilon)$ values for which both SPLIT and LicketySPLIT produce trees in the Rashomon set. The figure shows that our trees are almost always in the Rashomon set even for small values of ($\epsilon$, $\lambda$). }
    \label{fig:lookahead_in_rset}
\end{figure}
Figure \ref{fig:lookahead_in_rset} shows that this minimum $\epsilon$ is small regardless of the value of $\lambda$.  While SPLIT has a smaller minimum $\epsilon$, implying a lower optimality gap, particularly noteworthy is the performance of LicketySPLIT. Despite admitting a polynomial runtime, it manages to lie in the Rashomon set even for $\epsilon$ as small as $10^{-3}$.
\subsubsection{SPLIT with Optimality Preserving Discretization}
\label{sec:optimality_gap_binarization}
In this section, we briefly consider how SPLIT performs under full binarization of the dataset. For a given dataset, we perform full binarization by collecting every possible threshold (i.e. split point) present in every feature. We then compare the resulting regularised test loss and runtimes to that of threshold guessing. 
\begin{itemize}
    \item For this experiment, we first randomly choose $2000$ examples from the Netherlands, Covertype, HELOC, and Bike datasets. Larger dataset sizes would produce around $10^5$ features for the fully binarized dataset, which would make optimization extremely expensive computationally. 
    \item We then produce two versions of the dataset -- a fully binarized version (which contains around $3000$-$5000$ features for each dataset), and a threshold-guessed version \cite{gosdt_guesses} with \texttt{num$\_$estimators} $=200$. The latter ensures that the number of features in the resulting datasets is between $40$-$60$. 
\end{itemize}
For a given dataset, let $D^*$ and $D_{tg}$ its the fully binarized and threshold guessed version. We then run SPLIT and LicketySPLIT on these datasets and compute the difference in regularised training loss between $D^*$ and $D_{tg}$. Figure \ref{fig:optimality_preserving_discretization} shows the resulting difference and the corresponding runtimes. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/optimality_preserving_discretization.pdf}
    \caption{Difference in regularised training loss between SPLIT / LicketySPLIT trained on a fully binarized dataset vs the same dataset binarized using threshold guessing. We set $\lambda = 0.01$.}
    \label{fig:optimality_preserving_discretization}
\end{figure}
We see that there is almost no difference in loss between the fully binarized dataset and the threshold guessed dataset, suggesting that there is minimial sacrifice in performance when using SPLIT / LicketySPLIT with threshold guessing. Furthermore, using threshold guessing results in runtimes that are orders of magnitude faster. These observations have also been corroborated by \citet{gosdt_guesses}, though in the context of vanilla GOSDT. 


\subsubsection{What is the performance gap between GOSDT post-processing for SPLIT / LicketySPLIT and purely greedy post-processing?}
We now examine the the additional improvement brought about by the GOSDT post-processing scheme for SPLIT and the recursive post-processing. 
We next illustrate the gap between SPLIT / LicketySPLIT trees and a tree that is trained purely using a lookahead strategy and behaving purely greedily subsequently. Concretely, we first solve Equation \ref{eqn:lookahead_eqn}, i.e:
\begin{align}
\label{eqn:lookahead_eqn}
    \mcL(D, d^\prime, \lambda) = 
    \begin{cases}
    \lambda + \min\Bigg\{\frac{|D^-|}{|D|}, \frac{|D^+|}{|D|}\Bigg\} &  \text{if $d^\prime=0$}\\
    \begin{aligned}
        \lambda + \min\Bigg\{\frac{|D^-|}{|D|}, \frac{|D^+|}{|D|},\min_{f \in \mcF}\Big\{L\Big(T_g\big(D(f),d^\prime, \lambda\big)\Big) + L\Big(T_g\big(D(\bar{f}), d^\prime, \lambda\big)\Big)\Big\}\Bigg\} \end{aligned} &  \text{if $d^\prime = d-d_l$} \\
    \begin{aligned}
    \lambda + \min\Bigg\{\frac{|D^-|}{|D|},\frac{|D^+|}{|D|},\min_{f \in \mcF}\Big\{\mcL\Big(D(f), d^\prime-1, \lambda\Big) + \mcL\Big(D(\bar{f}), d^\prime-1, \lambda\Big)\Big\}
        \Bigg\}\end{aligned} & \text{if $d^\prime > d-d_l$.}
    \end{cases}
\end{align}
Let $T_{\mcL, g}$ be the tree representing the solution to this equation - this is a lookahead prefix tree with greedy splits after depth $d_l$. Let $T_{SPLIT}$ be the tree that replaces the greedy subtree after depth $d_l$ with optimal GOSDT splits - this refers to lines $3$-$9$ in Algorithm \ref{alg::lookahead}. Let $T_{LSPLIT}$ be the tree that replaces the greedy subtree after depth $d_l$ with recursive LicketySPLIT subtrees (this refers to lines $3$-$7$ in Algorithm \ref{alg::recursive_lookahead}). We then vary the value of the sparsity penalty $\lambda \in [10^{-3},10^{-1}]$ and compute the post-processing gaps on the training dataset $D$: 
\begin{align}
    & L(T_{\mcL, g}, D, \lambda) - L(T_{SPLIT}, D, \lambda)\\ 
    &L(T_{\mcL, g}, D, \lambda) - L(T_{LSPLIT}, D, \lambda)
\end{align}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/gap_between_lookahead_and_greedy_after_lookahead_depth_lookahead_recursive.pdf}
    \caption{Gap (in $\%$ points) in accuracy between SPLIT / LicketySPLIT and a lookahead prefix tree followed by a purely greedy approach. Depth budget $= 5$. }\label{fig:gap_between_lookahead_and_greedy_after_lookahead_depth_lookahead_recursive}
\end{figure}
%\newpage
\subsection{SPLIT and LicketySPLIT Scaling Experiments}
\label{sec:split_scaling}
We now evaluate the scalability of SPLIT and its variants as the number of features increases. For each dataset evaluated, we use the threshold guessing mechanism from \cite{gosdt_guesses} to binarize the dataset. In particular:
\begin{itemize}
    \item We first train a gradient boosted classifier with a specified number of estimators $n_{est}$. Each estimator is a single decision tree stump with an associated threshold. 
    \item We then collect all the thresholds generated during the boosting process, order them by Gini variable importance, and remove the least important thresholds (i.e., any thresholds which result in any performance drop)
\end{itemize}
In this experiment, we choose $n_{est}$ in a logarithmically spaced interval between $20$ and $10^4$, to obtain binary datasets with $10$-$1000$ features. We set a conservative value of $\lambda = 0.001$ for SPLIT / LicketySPLIT, as from Figure \ref{fig:lookahead_in_rset}, this ensures that the optimality gap for our method is around $\sim 10^{-3}$.

Figure \ref{fig:feature_scaling_exp_split} shows the results of this experiment.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/feature_scaling_exp_split.pdf}
    \caption{Runtime of SPLIT and LicketySPLIT as the number of features increases. $\lambda = 0.001$}
    \label{fig:feature_scaling_exp_split}
\end{figure}

\subsection{Rashomon Importance Distribution Under RESPLIT vs TreeFARMS: Threshold Guessing}
In this section, we compare RESPLIT and TreeFARMS in terms of their ability to generate meaningful variable importances under the Rashomon Importance Distribution \cite{donnelly2023the}. This analysis is a more complete representation of that in Table \ref{tab:results}. The variable importance metric considered is Model Reliance (MR) - the precise details of how this is computed are in \cite{donnelly2023the}.  
\begin{figure}[H]
    \centering
    \includegraphics[width=0.91\linewidth]{figures/model_reliance_diff_datasets.pdf}
    \includegraphics[width=1.035\linewidth]{figures/model_num_rsets.pdf}
    \caption{(top) Model Reliance for the top $5$ features when the Rashomon Importance Distribution is computed in its original form (with TreeFARMS), and when RESPLIT is used as the Rashomon set generating algorithm. The reported Pearson correlation is computed between the top $20$ features. We see that it is very close to $1$, i.e. features that are important under RID will also remain important when RESPLIT is used. (bottom) The number of models across the bootstrapped Rashomon sets which split on a given feature. We note from the bar plots that RESPLIT is also able to generate a large number of trees - often times as many as TreeFARMS. \\ \textbf{Parameters}: $\lambda = 0.02, \epsilon = 0.01, \#$ bootstrapped datasets $=10$, depth budget $= 5$, lookahead depth $= 3$.}
    \label{fig:rid_under_resplit_vs_treefarms}
\end{figure}
\subsection{Rashomon Importance Distribution Under RESPLIT: Quantile Binarization}
In this section, we show similar results as in the previous section, but when datasets are binarized using feature quantiles. We chose $3$ quantiles per feature (corresponding to each $3$rd of the distribution), resulting in datasets with $3\times$ the number of features. For most of these datasets, RID with TreeFARMS failed to run in reasonable time.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.91\linewidth]{figures/model_reliance_quantiles_diff_datasets.pdf}
    \includegraphics[width=1.035\linewidth]{figures/model_num_rsets_quantiles.pdf}
    \caption{(top) Model Reliance under RESPLIT for the top $5$ features. (bottom) The number of models across the bootstrapped Rashomon sets which split on a given feature. We note that the features which are important for RESPLIT under threshold guessing are also similarly important under quantile binarization, suggesting that our approach can generalize to different binarization schemes.\\ \textbf{Parameters}: $\lambda = 0.02, \epsilon = 0.01, \#$ bootstrapped datasets $=10$, depth budget $= 5$, lookahead depth $= 3$. }
    \label{fig:rid_under_resplit_quantile}
\end{figure}
\subsection{Experimental Setup}
\label{sec:experimental_setup}
\subsubsection{Datasets}
In this paper, we performed experiments with $10$ datasets:
\begin{itemize}
    \item The \textbf{Home Equity Line of Credit (HELOC)} \cite{heloc} dataset used for the Explainable ML Challenge. This dataset aims to predict the risk of loan default given the credit history of an individual. It consists of 23 features related to financial history, including FICO (credit) score, loan amount, number of delinquent accounts, credit inquiries, and other credit performance indicators. The dataset contains approximately 10,000 instances. 
    \item Two recidivism datasets \textbf{(COMPAS and Netherlands)}. COMPAS aims to predict the likelihood of recidivism (reoffending) for individuals who have been arrested. The dataset consists of approximately $6000$ instances and includes $11$ features including demographic attributes, criminal history, risk of general recidivism, and chargesheet information. The Netherlands dataset is a similar recidivism dataset containing demographic and prior offense features for individuals, used to predict reoffending risk.
    \item The \textbf{Covertype} dataset, which aims to predict the forest cover (one of $7$ types) for areas of the Roosevelt National Forest in northern Colorado, based on cartographic data. It contains $54$ attributes derived from US Geological Survey data. These include continuous variables like elevation, aspect, slope, and others related to soil type and climate. The dataset has over 580,000 instances, each corresponding to a $30$m $\times$ $30$m patch of the forest. 
    \item The \textbf{Adult} dataset, which aims  to predict whether an individual's income exceeds $\$50000$ per year based on demographic and occupational information. It contains around $50000$ train and test examples, with $14$ features.
    \item The \textbf{Bike} dataset \cite{fanaee2013event}, which contains a two-year historical log of bikeshare counts from $2011$-$2012$ in Washington D.C., USA. It contains features relating to the weather at every hour -- with the aim being to predict the number of bike rentals in the city in that given time period.
    \item The \textbf{Hypothyroid} dataset, which contains medical records used to predict whether a patient has hypothyroidism based on thyroid function test results and other medical attributes. It includes categorical and continuous variables such as TSH (thyroid-stimulating hormone) levels, age, and presence of goiter, with thousands of instances.
    \item The \textbf{Spambase} dataset, which consists of email data used to classify messages as spam or not spam. The dataset contains 57 features extracted from email text, such as word frequencies, capital letter usage, and special character counts, with around 4600 instances.
    \item The \textbf{Bank} dataset, which is used to predict whether a customer will subscribe to a bank term deposit based on features like age, job type, marital status, education level, and past marketing campaign success. It consists of approximately 4500 instances with 16 attributes.
    \item The \textbf{HIV} dataset contains RNA samples from $2$ patients. The labels correspond to whether the observed HIV viral load is high or not. 
\end{itemize}
One reason for our choice of datasets was that we wanted to stress-test our methods in scenarios where the dataset has $\mathcal{O}(10^3-10^5$) examples - our smallest dataset has $2623$ examples and the largest almost has almost $600000$ examples. There are a number of datasets from prior work (e.g. Monk1, Monk2, Monk3, Iris, Moons, Breast Cancer) which only have $\mathcal{O}(10^2)$ examples - for these, many optimal decision tree algorithms are fast enough (i.e. operating in the sub-second regime) that limits any practical scalability improvements. Our aim was to go from the $\mathcal{O}$(hours) regime to the sub-1 second regime, hence, we chose datasets whose size would best reflect the performance improvements we were hoping to showcase.
\subsubsection{Preprocessing}
\begin{itemize}
    \item We first exclude all examples with missing values
    \item We correct for class imbalances by appropriately resampling the majority class. This was the most prevalent in the HIV dataset, where we observed a $90:10$ class imbalance. We corrected this by randomly undersampling the majority class. 
    \item All datasets have a combination of categorical and continuous features, while SPLIT / LicketySPLIT / RESPLIT and many other decision tree algorithms require binarization of features. We therefore use the threshold guessing mechanism of binarization from \citet{gosdt_guesses}, which can handle both these feature types. In particular:
    \begin{itemize}
        \item We first train a gradient boosted classifier with a specified number of estimators $n_{est}$. Each estimator is a single decision tree stump with an associated threshold. 
        \item We then collect all the thresholds generated during the boosting process, order them by Gini variable importance, and remove the least important thresholds (i.e., any thresholds which result in any performance drop)
    \end{itemize}
    We store three binarized versions of each dataset for experiments with SPLIT and LicketySPLIT:
    \begin{itemize}
        \item For version $1$, we chose $n_{est}$ for each dataset such that the resulting binarized dataset has between $40$-$100$ features. This is the version used for experiments in Figures \ref{fig:comparisons}, \ref{fig:comparisons_2}, \ref{fig:split_comparisons_appendix_depth_4}, \ref{fig:split_comparisons_appendix_depth_6} when we compare SPLIT / LicketySPLIT with other datasets.
        \item For version $2$, we chose $n_{est}$ for each dataset such that the resulting dataset has around $20$-$25$ features. This is the version used when we use the TreeFARMS algorithm \cite{xin2022treefarms} to generate Rashomon sets to explore the properties of near optimal decision trees, as TreeFARMS can be very slow otherwise. Figures \ref{fig:greedy_heatmap_rset} and \ref{fig:rset_monotonically_decreasing_optimality_gap} use this version of the datasets.
        \item We additionally store another version of the datasets which is fully binarized, i.e., every possible split point is considered. Section \ref{sec:optimality_gap_binarization} uses this version of the dataset is to justify the use of threshold guessing in the context of our algorithm. 
    \end{itemize}
    \item Additionally, for aggressively binarized version of the dataset (i.e., version $2$), we subsample Covertype so that it has $\approx 20000$ examples. This is again to ensure that the TreeFARMS algorithm runs in a reasonable amount of time.
\end{itemize}
We also show scaling experiments for our algorithms, which are described in Section \ref{sec:split_scaling}. 
\begin{table}[H]
\centering

\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Data Set} & \textbf{Samples} & \textbf{$\#$ Features} & \textbf{$\#$ Features After Binarization} & \begin{tabular}[c]{@{}c@{}}$\#$ \textbf{Features After} \\ \textbf{Aggressive Binarization}\end{tabular} \\
\hline
HELOC & 10459 & 24 & 62 & 23 \\
COMPAS & 6172 & 12 & 39 & 24 \\
Adult & 32561 & 15 & 65 & 23 \\
Netherlands & 20000 & 10 & 52 & 23 \\
Covertype & 581012 & 55 & 41 & 21 \\
Bike & 17379 & 17 & 99 & 23 \\
Spambase & 4600 & 57 & 78 & 23\\
Hypothyroid & 2643 & 30 & 72 & 23\\
Bank & 4521 & 16 & 67 & 23\\
\hline
\end{tabular}
\caption{Characteristics of the $9$ datasets tested in this paper for LicketySPLIT and SPLIT experiments. We generate two binarized versions of each dataset using the threshold guessing mechanism in \cite{gosdt_guesses} which are used for different sets of experiments. }
\label{tab:datasets_split}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Data Set} & \textbf{Samples} & \textbf{$\#$ Features} & \textbf{$\#$ Features After Binarization} \\
\hline
HELOC       & 10459  & 24  & 47  \\
COMPAS      & 6172   & 12  & 39  \\
Netherlands & 20000  & 10  & 52  \\
Bike        & 17379  & 17  & 99  \\
Spambase    & 4600   & 57  & 78  \\
HIV         & 4521   & 100 & 57  \\
\hline
\end{tabular}
\caption{Characteristics of the $6$ datasets tested in this paper for RESPLIT experiments. As in Table \ref{tab:datasets_split}, we use the threshold guessing mechanism for binarization.}
\label{tab:datasets_resplit}
\end{table}

\subsubsection{Details of Comparative Experiments for SPLIT and LicketySPLIT}
\begin{itemize}
    \item \textbf{Greedy}: This is the standard scikit-learn DecisionTreeClassifier class that implements CART. We vary the sparsity of this algorithm by changing the \texttt{min$\_$samples$\_$leaves} argument. This is the minimum number of examples required to be in a leaf in order for CART to make further a split at that point. 
    \item \textbf{GOSDT} \cite{gosdt}: We vary the sparsity parameter $\lambda$, choosing equispaced values from $0.001$ to $0.02$. 
    \item \textbf{SPLIT / LicketySPLIT}. We search over the same $\lambda$ values as GOSDT. For SPLIT, additionally, we set the lookahead depth to be $1$. 
    \item \textbf{Thompson Sampled Decision Trees (TSDT) \cite{thompson_aistats}}: Following the practices described in the Appendix Section $B$ of their paper, we fix the following parameters: 
    \begin{itemize}
        \item $\gamma = 0.75$
        \item Number of iterations $= 10000$
    \end{itemize}
    Additionally, we also fix the following parameters, based on the Jupyter notebooks in the Github repository of TSDT. 

    \begin{itemize}
        \item \texttt{thresh$\_$tree} $= -1e-6$
        \item \texttt{thresh$\_$leaf} $= 1e-6$
        \item \texttt{thresh$\_$mu} $= 0.8$
        \item \texttt{thresh$\_$sigma} $= 0.1$
    \end{itemize}
    To obtain different levels of sparsity, we vary the $\lambda$ parameter. We experiment with $3$ values of $\lambda: \{0.0001, 0.001, 0.01\}$. For each value of $\lambda$, we also experiment with different time limits for the algorithm: $\{1,10,100,1000\}$. Lastly, we use the FAST-TSDT version of their code, as according to the paper, it strikes a good balance between speed and performance (which is consistent with our paper's motivation).
    \item \textbf{Murtree} \cite{murtree}: For this method, we vary the \texttt{max$\_$num$\_$nodes}, which is the hard sparsity constraint imposed by Murtree on the number of leaves. 
    \begin{itemize}
        \item For depth $5$ trees, we choose \texttt{max$\_$num$\_$nodes} in the set $\{4,5,6,7,8,9,10,11\}$. 
        \item For depth $4$ trees, we choose \texttt{max$\_$num$\_$nodes} in the set $\{3, 4,5,6,7\}$. 
    \end{itemize}
    \item \textbf{MAPTree} \cite{maptree}: The paper has two hyperparameters, $\alpha$ and $\beta$, which in theory control for sparsity in theory by adjusting the prior. However, the authors show that MAPTree does not exhibit significant sensitivity to $\alpha$ and $\beta$ across any metric. Therefore, the only parameter we choose to vary for this experiment is \texttt{num$\_$expansions}. We chose $10$ values of this parameter in a logarithmically spaced interval from $[10^0, 10^{3.5}]$. 
    \item \textbf{Top-k (DL8.5)} \cite{topk}: The paper also does not specify how to vary the sparsity parameter - hence, we vary $k$ from $1$-$10$. 
\end{itemize}
Note that there is no depth budget hyperparameter for MAPTree and TSDT, but we still show these algorithms across all experiments for comparative purposes.
\paragraph{Our method vs another bespoke-greedy approach}
We briefly discuss another decision tree optimization algorithm from \cite{slow_greedy} that demonstrates good performance on a tabular dataset. This method first proposes a novel greediness criterion called the $(\alpha,\beta)$-Tsallis entropy, defined as: 
\begin{equation}
    g(\alpha,\beta) = \frac{C}{\alpha-1}\Bigg(\Big(1-\sum_{i=1}^cp_i^\alpha\Big)^\beta\Bigg)
\end{equation}
where $P = \{p_i\}$ is a discrete probability distribution. Then a decision tree is trained in CART-like fashion, but with this greediness criteria instead. Note that 

For the COMPAS dataset, which is one of the smaller datasets in our experiments, we conducted a brief evaluation by averaging results over $3$ trials for $3$ different values of the hyperparameters $\alpha$ and $\beta$, arranged in a grid-based configuration as defined in \cite{slow_greedy}. These hyperparameters influence the functional form of the above greedy heuristic. Below, we summarize the key settings and observations:
\begin{itemize}
    \item \textbf{Values of $\alpha$}: [0.5, 1, 1.5]
    \item \textbf{Values of $\beta$}: [1, 2, 3]
\end{itemize}
\textbf{Observations}:
\begin{enumerate}
    \item The method in \cite{slow_greedy} achieves approximately \textbf{31.6\% test error} with around \textbf{10 leaves}, requiring an average of \textbf{10 minutes} to train for a single hyperparameter setting. Another thing to note is that it isn't clear a-priori which hyperparameter will lead to the best performance (in terms of the desired objective in Equation \ref{eqn:obj}), so many different combinations of hyper-parameters might need to be tested in order to find a well-performing tree.
    \item \textbf{SPLIT} achieves approximately \textbf{31.9\% test error} with fewer than \textbf{10 leaves} in approximately \textbf{1 second}.
    \item \textbf{LicketySPLIT} achieves approximately \textbf{31.9\% test error} with fewer than \textbf{10 leaves} in under \textbf{1 second}.
\end{enumerate}
In summary, our proposed methods are over \textbf{600x faster} than \cite{slow_greedy}, with a negligible difference in test performance.
\subsubsection{Description of Machines Used}
All experiments were performed on an institutional computing cluster. This was a single Intel(R) Xeon(R) Gold 6226 machine with a $2.70$GHz CPU. It has $300$GB RAM and $24$ cores. 


\label{sec:setup}

\subsection{Appendix Proofs}
\label{sec:proofs}
\input{proofs_2}
\newpage
\subsection{Greedy Algorithm}\label{sec:greedy_alg}
\begin{algorithm}[H]
\caption{Greedy($D, d, \lambda$) $\to$ ($t_{\textrm{greedy}}, lb$)}
\label{alg:greedy}
\begin{algorithmic}[1]
\REQUIRE $D, d, \lambda$ \COMMENT{\textcolor{commentgreen}{{Data subset, depth constraint, leaf regularization}}}
\ENSURE $t_{\textrm{greedy}}, lb$ \COMMENT{\textcolor{commentgreen}{tree grown with a greedy, CART-style method; and the objective of that tree}}
\STATE $t_{\textrm{greedy}} \gets $ \textrm{(Leaf predicting the majority label in $D$)} 
\STATE $lb \gets \lambda + (\textrm{proportion of $D$ that does not have the majority label})$
\IF {$d > 1$}
\STATE let $f$ be the information gain maximizing split with respect to $D$ 
\STATE $t_{\textrm{left}}, lb_{\textrm{left}} \gets \textrm{Greedy}(D(f), d - 1, \lambda)$
\STATE $t_{\textrm{right}}, lb_{\textrm{right}} \gets \textrm{Greedy}(D(\bar{f}), d - 1, \lambda)$
\IF {$lb_{\textrm{left}} + lb_{\textrm{right}} < lb$}
\STATE $lb \gets lb_{\textrm{left}} + lb_{\textrm{right}}$ 
\STATE $t_\textrm{greedy} \gets$ \textrm{tree corresponding to: if $f$ is True then $t_{\textrm{left}}$, else $t_{\textrm{right}}$}
\ENDIF
\ENDIF
\STATE \textbf{return} $t_{\textrm{greedy}}, lb$
\end{algorithmic}
\end{algorithm}


\subsection{RESPLIT Algorithm}\label{sec:resplit_alg}
\begin{algorithm}[H]
\caption{RESPLIT($\ell$, D, $\lambda$, $d_l$, $d$)}
\label{alg::resplit}
\begin{algorithmic}[1]
\REQUIRE $\ell$, $D$, $\lambda$, $d_l$, $d$ \COMMENT{\textcolor{commentgreen}{loss function, samples, regularizer, lookahead depth, depth budget}} 
\STATE ModifiedTreeFARMS = TreeFARMS reconfigured to use \textbf{get$\_$bounds} (Algorithm \ref{alg::bounds}) whenever it encounters a new subproblem
\item $tf = $ ModifiedTreeFARMS$(\ell, D, \lambda,d_l)$ \COMMENT{\textcolor{commentgreen}{Call ModifiedTreeFARMS with depth budget $d_l$}}
\FOR[\textcolor{commentgreen}{Iterate through all depth $d_l$ prefixes found by ModifiedTreeFARMS}]{$t_{lookahead} \in tf$}
\FOR {leaf $u \in t_{lookahead} $}
    \STATE $d_{u} = $ depth of leaf 
    \STATE $D(u) = $ subproblem associated with $u$
    \STATE $\lambda_{u} = \lambda \frac{|D|}{|D(u)|}$ \COMMENT{\textcolor{commentgreen}{Renormalize $\lambda$ for the subproblem in question}}
    \STATE $T_g, L_g = $ Greedy($D(u)$,$d-d_u$, $\lambda_u$) \COMMENT{\textcolor{commentgreen}{Objective of greedy tree trained on subproblem}}
    \STATE $t_u = $ TreeFARMS$(D(u), d - d_{u},\lambda_{u}, L_g)$ \COMMENT{\textcolor{commentgreen}{Find all subtrees with loss less than $L_g$}}
    \IF {$t_u$ is not a leaf}
    \STATE Replace leaf $u$ with TreeFARMS object $t_u$
    \ENDIF
\ENDFOR
\STATE $t_{lookahead} = $ Enumerate$\_$TreeFARMS$\_$subtrees \COMMENT{\textcolor{commentgreen}{For each node in this prefix tree, store the number of subtrees we can generate rooted at that node. This speeds up indexing}}
\ENDFOR
\RETURN $tf$ \COMMENT{\textcolor{commentgreen}{Return in-place edited ModifiedTreeFARMS object}}
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[H]
\caption{Enumerate$\_$TreeFARMS$\_$subtrees}
\label{alg::enumerate_treefarms}
\begin{algorithmic}[1]
\REQUIRE $t_{lookahead}$ \COMMENT{\textcolor{commentgreen}{Lookahed prefix with TreeFARMS objects attached to leaves}}
\IF{$t_{lookahead}$ is None}
    \STATE \textbf{Return} 1
\ELSIF{$t_{lookahead}$ is a TreeFARMS object}
    \STATE \textbf{Return} $len$($t_{lookahead}$), $t_{lookahead}$
\ENDIF
\STATE left$\_$expansions, left$\_$subtree $=$ enumerate$\_$treefarms\_subtrees($t_{lookahead}$.left$\_$child)
\STATE $t_{lookahead}$.left$\_$child.node $=$ left$\_$subtree
\STATE $t_{lookahead}$.left$\_$child.subtree$\_$count $=$ left\_expansions
\STATE right$\_$expansions, right$\_$subtree $=$ enumerate$\_$treefarms$\_$subtrees($t_{lookahead}$.right$\_$child)
\STATE $t_{lookahead}$.right$\_$child.node $=$ right$\_$subtree
\STATE $t_{lookahead}$.right$\_$child.subtree$\_$count $=$ right\_expansions
\STATE \textbf{Return} left$\_$expansions $\times$ right$\_$expansions, $t_{lookahead}$ \COMMENT{\textcolor{commentgreen}{Total number of subtrees = cross product of left and right subtree count}}
\end{algorithmic}
\end{algorithm}
\subsection{Indexing Trees in RESPLIT }\label{sec:resplit_indexing}
In this section, we present an algorithm that can quickly index trees output by RESPLIT. Because Algorithm \ref{alg::resplit} outputs a bespoke data structure involving TreeFARMS objects attached to a set of prefix trees, we needed to devise a method to efficiently query this structure to locate trees at a desired index. 
\begin{itemize}
    \item For each prefix found by the initial ModifiedTreeFARMS call, we additionally store the number of subtrees that can be formed with that prefix. This is also done for every subsequent node split on beyond the prefix. Algorithm \ref{alg::enumerate_treefarms} shows how this is done. 
    \item We also store the cumulative count of the total number of subtrees as we iterate through the list of prefixes. Algorithm \ref{alg::resplit-rset-count} called in line $1$ of Algorithm \ref{alg::resplit-index} does this. 
    \item Given a query index, we first perform a binary search over this cumulative count to find the appropriate prefix associated with the queried tree. This is illustrated in lines $2-10$ in Algorithm \ref{alg::resplit-index}
    \item Once the prefix is known, we introduce a variable called tree$\_$idx that gives the index of the queried tree relative to that of the prefix, i.e. line $11$ in Algorithm \ref{alg::resplit-index}. E.g. if we query the $500^{th}$ tree and our prefix has trees indexed $400-600$, the relative index of the query tree within this prefix is $100$. 
    \item Lastly, using Algorithm \ref{alg::get-leaf-subtree-at-idx}, we proceed to recursively locate the relevant subtrees beyond the prefix. In particular, at a given node in the prefix, we have access to the number of sub-trees that can be formed with its left and right children. We use this information to create two separate indexes for the left and right child (seen in lines $9-10$)
\end{itemize}
\begin{algorithm}[H]
\caption{RESPLIT$\_$indexing}
\label{alg::resplit-index}
\begin{algorithmic}[1]
\REQUIRE RESPLIT$\_$obj, idx \COMMENT{\textcolor{commentgreen}{The RESPLIT object output by Algorithm \ref{alg::resplit}, index of the tree to be found}}
\STATE $t_{count}, p_{counts}$ = RESPLIT$\_$Rset$\_$Count(RESPLIT$\_$obj) \COMMENT{\textcolor{commentgreen}{Get $\#$ trees in Rashomon set. Call only once.}}
\STATE $low$, $high$ $=$ $0, len(p_{counts})$ 
\WHILE[\textcolor{commentgreen}{Binary Search to find the appropriate prefix tree of the queried tree}]{$low \neq high$}
    \STATE $mid = (low + high)//2$
    \IF {$idx < p_{counts}[mid]$}
        \STATE $high = mid$
    \ELSIF{$idx > p_{counts}[mid]$}
        \STATE $low = mid$
    \ENDIF
\ENDWHILE
\STATE $tree\_idx = idx - p_{counts}[mid]$ \COMMENT{\textcolor{commentgreen}{Subtracting the index by the cumulative tree count of the prefix we're interested in}}
\STATE $t_{lookahead} = $RESPLIT$\_$obj.prefix$\_$list[$mid$] 
\STATE $tree = $get$\_$leaf$\_$subtree$\_$at$\_$idx($t_{lookahead}, tree\_idx$) \COMMENT{\textcolor{commentgreen}{Use a recursive tree based indexing algorithm to find the appropriate subtrees to fill in for this given prefix tree}}
\RETURN $tree$
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[H]
\caption{get\_leaf\_subtree\_at\_idx($t_{lookahead}$, tree$\_$idx)}
\label{alg::get-leaf-subtree-at-idx}
\begin{algorithmic}[1]
    \REQUIRE $t_{lookahead}$, tree$\_$idx \COMMENT{\textcolor{commentgreen}{A lookahead prefix tree with TreeFARMS objects attached to leaves, index to search within this tree}}
    \IF{$t_{lookahead}$ is a Leaf}
        \RETURN $t_{lookahead}$ \COMMENT{\textcolor{commentgreen}{Directly return the leaf object}}
    \ELSIF{$t_{lookahead}$ is a list}
        \RETURN $t_{lookahead}[tree\_idx]$ \COMMENT{\textcolor{commentgreen}{If it's a list, return the subtree at the given index}}
    \ENDIF
    \STATE $tree \gets$ Node($t_{lookahead}$.feature)) \COMMENT{\textcolor{commentgreen}{Initialize an empty node}}
    \STATE left$\_$count $= t_{lookahead}$.left$\_$child.subtree$\_$count \COMMENT{\textcolor{commentgreen}{The number of subtrees that can be found rooted at this node}}
    \STATE right$\_$count $= t_{lookahead}$.right\_child.subtree$\_$count
    \STATE right$\_$idx $=$ tree$\_$idx \% right$\_$count
    \STATE left$\_$idx $=$ tree$\_$idx // right$\_$count
    \STATE $tree$.left$\_$child $=$ get\_leaf\_subtree\_at\_idx($t_{lookahead}$.left\_child.node, left$\_$idx)
    \STATE $tree$.right$\_$child $=$ get\_leaf\_subtree\_at\_idx($t_{lookahead}$.right\_child.node, right$\_$idx)
    \RETURN $tree$
\end{algorithmic}
\end{algorithm}



\begin{algorithm}[H]
\caption{RESPLIT$\_$Rset$\_$Count(RESPLIT$\_$obj)}
\label{alg::resplit-rset-count}
\begin{algorithmic}[1]
\REQUIRE RESPLIT$\_$obj \COMMENT{\textcolor{commentgreen}{The RESPLIT object output by Algorithm \ref{alg::resplit}}} 
% \vspace{-0.32cm}
\STATE $t_{count} =0$ \COMMENT{\textcolor{commentgreen}{Total $\#$ trees}}
\STATE $p_{counts} = []$ \COMMENT{\textcolor{commentgreen}{Cumulative count of $\#$ trees beginning with a given prefix}}
\FOR{$t_{lookahead} \in $ RESPLIT$\_$obj}
    \STATE $p_{count} = 1$
    \FOR {leaf $u \in t_{lookahead}$} 
        \STATE $tf_u = $ TreeFARMS object fitted on subproblem $D(u)$
        \STATE $s_{count} = len(tf_u)$ \COMMENT{\textcolor{commentgreen} {Number of subtrees found for subproblem $D(u)$}}
        \STATE $p_{count} = p_{count} \times s_{count}$
    \ENDFOR 
    \STATE $t_{count} = t_{count} + p_{count}$
    \STATE $p_{counts}.add(t_{count})$
\ENDFOR
\RETURN $p_{counts}, t_{count}$
\end{algorithmic}
\end{algorithm}

\subsection{Modifications to Existing GOSDT / TreeFARMS Code}\label{sec:gosdt_details}
In this section, we detail the main modifications we made to the existing GOSDT and TreeFARMS codebase in order to set up SPLIT, LicketySPLIT, and RESPLIT. The algorithm components in red are the modifications - note that GOSDT and TreeFARMS both call these functions. TreeFARMS does some additional post-processing of the search trie to find the set of near optimal trees - the details can be seen in \cite{xin2022treefarms}. 
\begin{algorithm}[H]
\caption{find$\_$lookahead$\_$tree($\ell$, $D$, $\lambda$, \textcolor{red}{$d_l$, $d$})}
\begin{algorithmic}[1]
\REQUIRE $\ell$, $D$, $\lambda$, \textcolor{red}{$d_l$, $d$} \COMMENT{\textcolor{commentgreen}{loss function, dataset, regularizer, lookahead depth, global depth budget}}
\STATE $Q \gets \emptyset$ \COMMENT{\textcolor{commentgreen}{ priority queue}}
\STATE $G \gets \emptyset$ \COMMENT{\textcolor{commentgreen}{ dependency graph}}
\STATE $s_0 \gets \{1, \ldots, 1\}$ \COMMENT{\textcolor{commentgreen}{ bit-vector of 1's of length $n$}}
\STATE $p_0 \gets \text{FIND\_OR\_CREATE\_NODE}(G, s_0,\textcolor{red}{d_l}, \textcolor{red}{d}, \textcolor{red}{0})$ \COMMENT{\textcolor{commentgreen}{ root (with depth 0)}}
\STATE $Q.\text{push}((s_0,\textcolor{red}{0}))$ \COMMENT{\textcolor{commentgreen}{ add to priority queue}}
\WHILE{$p_0.\text{lb} \neq p_0.\text{ub}$}
    \STATE $s, \textcolor{red}{d}^\prime \gets Q.\text{pop}()$ \COMMENT{\textcolor{commentgreen}{index of problem to work on}}
    \STATE $p \gets G.\text{find}(s)$ \COMMENT{\textcolor{commentgreen}{ find problem to work on}}
    \IF{$p.\text{lb} = p.\text{ub}$}
        \STATE \textbf{continue} \COMMENT{\textcolor{commentgreen}{ problem already solved}}
    \ENDIF
    \STATE $(lb', ub') \gets (\infty, \infty)$ \COMMENT{\textcolor{commentgreen}{ loose starting bounds}}
    \FOR{each feature $j \in [1, k]$}
        \STATE $(s_l, s_r) \gets \text{split}(s, j, D)$ \COMMENT{\textcolor{commentgreen}{ create children}}
        \STATE $p_l^j \gets \text{FIND\_OR\_CREATE\_NODE}(G, s_l,\textcolor{red}{d_l}, \textcolor{red}{d}, \textcolor{red}{d^\prime+1})$
        \STATE $p_r^j \gets \text{FIND\_OR\_CREATE\_NODE}(G, s_r,\textcolor{red}{d_l},\textcolor{red}{d}, \textcolor{red}{d^\prime+1})$
        \STATE $lb' \gets \min(lb', p_l^j.\text{lb} + p_r^j.\text{lb})$ \COMMENT{\textcolor{commentgreen}{ create bounds as if $j$ were chosen for splitting}}
        \STATE $ub' \gets \min(ub', p_l^j.\text{ub} + p_r^j.\text{ub})$
    \ENDFOR
    \IF[\textcolor{commentgreen}{ signal the parents if an update occurred}]{$p.\text{lb} \neq lb'$ or $p.\text{ub} \neq ub'$}
        \STATE $p.\text{ub} \gets \min(p.\text{ub}, ub')$
        \STATE $p.\text{lb} \gets \min(p.\text{ub}, \max(p.\text{lb}, lb'))$
        \FOR[\textcolor{commentgreen}{ propagate information upwards}]{$p_\pi \in G.\text{parent}(p)$} 
            \STATE $Q.\text{push}((p_\pi.\text{id}, \textcolor{red}{d^\prime-1}), \text{priority} = 1)$
        \ENDFOR
    \ENDIF
    \IF{$p.\text{lb} \geq p.\text{ub}$}
        \STATE \textbf{continue} \COMMENT{\textcolor{commentgreen}{ problem solved just now}}
    \ENDIF
    \IF {\textcolor{red}{$d^\prime < d_l$}}
    \FOR[\textcolor{commentgreen}{ loop, enqueue all children}]{each feature $j \in [1, M]$} 
        \STATE \textbf{repeat} line 14-16 \COMMENT{\textcolor{commentgreen}{ fetch $p_l^j$ and $p_r^j$ in case of update}}
        \STATE $lb' \gets p_l^j.\text{lb} + p_r^j.\text{lb}$
        \STATE $ub' \gets p_l^j.\text{ub} + p_r^j.\text{ub}$
        \IF{$lb' < ub'$ and $lb' \le p.\text{ub}$}
            \STATE $Q.\text{push}((s_l, \textcolor{red}{d+1}), \text{priority} = 0)$
            \STATE $Q.\text{push}((s_r, \textcolor{red}{d+1}), \text{priority} = 0)$
        \ENDIF
    \ENDFOR
    \ENDIF 
\ENDWHILE
\STATE \textbf{return} $\mathcal{G}$
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[H]
\caption{FIND\_OR\_CREATE\_NODE($G$, $s$, \textcolor{red}{$d_l$}, \textcolor{red}{$d$}, \textcolor{red}{$d^\prime$})}
\begin{algorithmic}[1]
\REQUIRE $G, s, d_l, d, d'$ \COMMENT{\textcolor{commentgreen}{Graph, subproblem, lookahead depth, overall depth budget, current depth}}\\
\RETURN representation of subproblem entry for $s$, with that subropblem being present in the graph $G$
\IF[\textcolor{commentgreen}{ $p$ not yet in graph}]{$G.\text{find}(s) = \text{NULL}$} 
    \STATE create node $p$
    \STATE $p.\text{id} \gets s$ 
    \COMMENT{\textcolor{commentgreen}{ identify $p$ by $s$}}
    \STATE $D(s) = $ Dataset associated with subproblem $s$
    \STATE $p.\text{ub}, p.\text{lb} \gets \textcolor{red}{\text{get\_bounds}(D(s),d_l,d^\prime, d)}$
    \IF[\textcolor{commentgreen}{If a further split would lead to worse objective than the upper bound}]{$p.ub \leq p.lb + \lambda$} 
        \STATE $p.\text{lb} \gets p.\text{ub}$ \COMMENT{\textcolor{commentgreen}{ no more splitting needed}}
    \ENDIF
    \STATE $G.\text{insert}(p)$ \COMMENT{\textcolor{commentgreen}{ put $p$ in dependency graph}}
\ENDIF
\STATE \textbf{return} $G.\text{find}(s)$
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[H]
\caption{get\_bounds($D$, \textcolor{red}{$d_l$, $d^\prime$, $d$}) $\to$ lb, ub}
\begin{algorithmic}[1]
\REQUIRE $D$, \textcolor{red}{$d_l$, $d^\prime$, $d$} \COMMENT{\textcolor{commentgreen}{support, positive sample set, lookahead depth, current depth,  overall depth budget}}
\RETURN lb, ub \COMMENT{\textcolor{commentgreen}{Return Lower and Upper Bounds}}


\IF {\textcolor{red}{$d = d_l$}}
\STATE \textcolor{red}{$T_g = $ Greedy$(D,d-d_l,\lambda)$}
\STATE \textcolor{red}{$H(T_g) = \# $ Leaves in $T_g $}
\STATE \textcolor{red}{$\alpha\gets \lambda H(T_g) + \frac{1}{N}\sum_{i \in s} \mathbf{1}[y_i \neq T_g(x_i)]$}
\STATE \textcolor{red}{$lb \gets \alpha$}
\STATE \textcolor{red}{$ub \gets \alpha$}
    \STATE $lb \gets $ Equivalent points bound \cite{gosdt}
    \STATE $ub = \lambda + \min\Big(\frac{1}{N}\sum_{(x,y) \in D} \mathbbm{1}[y_i = 1], \frac{1}{N}\sum_{(x,y) \in D} \mathbbm{1}[y_i = 0]\Big)$
\ENDIF

\STATE \textbf{return} lb, ub
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[H]
\caption{extract$\_$tree($D$,$\mathcal{G}$, \textcolor{red}{$d_l$})}
\begin{algorithmic}[1]
\REQUIRE $D$,$\mathcal{G}$, \textcolor{red}{$d_l$} \COMMENT{\textcolor{commentgreen}{Dataset, Dependency graph of search space, lookahead depth}}\\
\RETURN Tree $t$
\STATE $t \leftarrow $ (Leaf predicting the majority label in $D$)
\STATE $ub \leftarrow \lambda + $ (proportion of $D$ that has the minority label)
\IF {\textcolor{red}{$d_l > 1$}}
\FOR {feature $f \in \mcF$}
     \STATE $p_{f} = $ subproblem associated with $D(f)$
     \STATE $p_{\bar{f}} = $ subproblem associated with $D(\bar{f})$
     \IF {$p_f.ub$ + $p_{\bar{f}}.ub \leq ub$ }
        \STATE $f_{opt} = f$ \COMMENT{\textcolor{commentgreen}{Best Feature}}
        \STATE $ub = p_f.ub$ + $p_{\bar{f}}.ub$
     \ENDIF 
\ENDFOR
\STATE $t_{left} = $ extract$\_$tree($D(f_{opt} )$, $\mathcal{G}(f_{opt} )$, \textcolor{red}{$d_l-1$})
\STATE $t_{right} = $ extract$\_$tree($D(\bar{f_{opt} })$, $\mathcal{G}(\bar{f_{opt} })$, \textcolor{red}{$d_l-1$})
\STATE $t.left = t_{left}$
\STATE $t.right = t_{right}$
\ENDIF
\STATE \textbf{return} t
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[H]
\caption{\textcolor{red}{ModifiedGOSDT($\ell$, D, $\lambda$, $d_l$, $d$)}}
\label{alg::gosdtmod}
\begin{algorithmic}[1]
\REQUIRE $\ell$, $D$, $\lambda$, $d_l$, $d$ \COMMENT{\textcolor{commentgreen}{loss function, samples, regularizer, lookahead depth, depth budget}}
\STATE $\mathcal{G}$ = \textrm{find\_lookahead\_tree}($\ell$, D, $\lambda$, $d_l$, $d$)
\STATE t = \textrm{extract\_tree}($D$, $\mathcal{G}$, $d_l$) \COMMENT{\textcolor{commentgreen}{Extracts the prefix of the found tree, without filling in the greedy splits}}\\
\RETURN t
\end{algorithmic}
\end{algorithm}