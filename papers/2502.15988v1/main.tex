\documentclass[twoside]{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}
\newcommand{\mcD}{\mathcal{D}}
\newcommand{\mcL}{\mathcal{L}}
\newcommand{\mcR}{\mathcal{R}}
\newcommand{\mcF}{\mathcal{F}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bX}{\mathbf{X}}

\usepackage[accepted]{icml2025}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{thmtools, thm-restate}
\usepackage{bbm}
\usepackage[italicdiff]{physics}

\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


\icmltitlerunning{Sparse Lookahead for Interpretable Decision Trees}

\begin{document}
\definecolor{commentgreen}{rgb}{0,0.5,0.5}

\twocolumn[

\icmltitle{Near-Optimal Decision Trees in a SPLIT Second}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Varun Babbar}{equal,yyy}
\icmlauthor{Hayden McTavish}{equal,yyy}
\icmlauthor{Cynthia Rudin}{yyy}
\icmlauthor{Margo Seltzer}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Department of Computer Science, Duke University, Durham, USA}
\icmlaffiliation{sch}{Department of Computer Science, University of British Columbia, Vancouver, Canada}

\icmlcorrespondingauthor{Varun}{varun.babbar@duke.edu}
\icmlcorrespondingauthor{Hayden}{hayden.mctavish@duke.edu}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]



\printAffiliationsAndNotice{\icmlEqualContribution}


\begin{abstract}
Decision tree optimization is fundamental to interpretable machine learning. The most popular approach is to greedily search for the best feature at every decision point, which is fast but provably suboptimal. Recent approaches find the global optimum using branch and bound with dynamic programming, showing substantial improvements in accuracy and sparsity at great cost to scalability. An ideal solution would have the accuracy of an optimal method and the scalability of a greedy method. We introduce a family of algorithms called SPLIT (SParse Lookahead for Interpretable Trees) that moves us significantly forward in achieving this ideal balance. We demonstrate that not all sub-problems need to be solved to optimality to find high quality trees; greediness suffices near the leaves. Since each depth adds an exponential number of possible trees, this change makes our algorithms orders of magnitude faster than existing optimal methods, with negligible loss in performance. We extend this algorithm to allow scalable computation of sets of near-optimal trees (i.e., the Rashomon set).
\end{abstract}

\section{Introduction}
\label{sec:introduction}
\input{introduction}

\section{Related Work}
\label{sec:related}
\input{related_work}

\section{Preliminaries}
\label{sec:prelim}
\input{preliminaries}

\section{Algorithm Details}
\label{sec:bnb_lookahead}
\input{branch_and_bound_with_lookahead}
\section{Theoretical Analysis of Runtime and Optimality}
\label{sec:theory}
\input{theory}
\section{Experiments}
\label{sec:experiments}
\input{experiments}
\vspace{-0.6cm}
\section{Conclusion}
\label{sec:conclusion}
We introduced SPLIT, LicketySPLIT, and RESPLIT, a novel family of decision tree optimization algorithms. At their core, these algorithms perform branch and bound search up to a lookahead depth, beyond which they switch to greedy splitting. Our experimental results show dramatic improvements in runtime compared to state of the art algorithms, with negligible loss in accuracy or sparsity. RESPLIT also scalably finds a set of near-optimal trees without adversely impacting downstream variable importance tasks. Future work could explore conditions under which subproblems exhibit large optimality gaps, offering new insights for efficient decision tree and Rashomon set optimization.


\bibliography{References}
\bibliographystyle{icml2025}

\onecolumn
\appendix

\label{sec:appendix}
\input{appendix}
\end{document}
