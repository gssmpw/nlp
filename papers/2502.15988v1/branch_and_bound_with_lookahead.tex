\subsection{SParse Lookahead for Interpretable Trees (SPLIT)}

We now formalize our main algorithm, SPLIT, which takes as input a \textit{lookahead depth} parameter. This is the depth up to which a search algorithm optimizes over all combinations of feature splits, conditioned on splits beyond this depth behaving greedily. Our algorithm exploits the fact that sub-problems closer to the leaves exhibit smaller optimality gaps than those at the root, providing a mechanism to trade off among runtime, accuracy, and sparsity.
\paragraph{Formulating the optimization problem} 
Concretely, for a given depth budget $d$, lookahead depth $d_l < d$, and feature set $\mcF$, we \textbf{first} solve the following recursive equation:
{
\begin{align} 
\label{eqn:lookahead_eqn_og}
    &\mcL(D, d^\prime, \lambda) = \nonumber \\
    &\begin{cases}
    \lambda + \min\Bigg\{\frac{|D^-|}{|D|}, \frac{|D^+|}{|D|}\Bigg\}\ \ \hspace{1.55cm} \text{if $d^\prime=0$} \\ \\
    \begin{aligned}
        &\lambda + \min\Bigg\{\frac{|D^-|}{|D|}, \frac{|D^+|}{|D|},\\& \min_{f \in \mcF}\Big\{L\Big(T_g\big(D(f),d^\prime, \lambda\big)\Big) + L\Big(T_g\big(D(\bar{f}), d^\prime, \lambda\big)\Big)\Big\}\Bigg\} \end{aligned} \ \ \\ \hspace{5cm} \text{if $d^\prime = d-d_l$} \\
    \begin{aligned}
    &\lambda + \min\Bigg\{\frac{|D^-|}{|D|},\frac{|D^+|}{|D|},\\ &\min_{f \in \mcF}\Big\{\mcL\Big(D(f), d^\prime-1, \lambda\Big) + \mcL\Big(D(\bar{f}), d^\prime-1, \lambda\Big)\Big\}
        \Bigg\}\end{aligned} \ \ \\ \hspace{5cm}\text{if $d^\prime > d-d_l$.}
    \end{cases}
\end{align}
}
\begin{algorithm}[ht]
\caption{get\_bounds($D$, $d_l$, $d$) $\to$ lb, ub}
\label{alg::bounds}
\begin{algorithmic}[1]
\REQUIRE $D$, $d_l$, $d$ \COMMENT{\textcolor{commentgreen}{support, lookahead depth, current search depth}}
\IF {$d = d_l$}
\STATE $T_g = $ Greedy$(D,d-d_l,\lambda)$ \COMMENT{\textcolor{commentgreen}{Find greedy tree rooted at $D$ (Alg \ref{alg:greedy} in the Appendix)}}
\STATE $S(T_g) = \# $ Leaves in $T_g $
\STATE $\alpha\gets \frac{1}{|D|}\sum_{(x,y) \in D} \mathbf{1}[y \neq T_g(x)] + \lambda S(T_g)$
\STATE $lb \gets \alpha$ 
\STATE $ub \gets \alpha$ \COMMENT{\textcolor{commentgreen}{subproblem solved because ub = lb}}
\ELSE[\textcolor{commentgreen}{use basic initial bounds}]
    \STATE $lb \gets 2\lambda$ 
    \STATE $ub \gets \lambda + \min \Big\{\frac{|D^-|}{|D|}, \frac{|D^+|}{|D|}\Big\}$ 
\ENDIF
\STATE \textbf{return} lb,ub \COMMENT{\textcolor{commentgreen}{Return Lower and Upper Bounds}}
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[t]
\caption{SPLIT($\ell$, D, $\lambda$, $d_l$, $d$, $p$)}
\label{alg::lookahead}
\begin{algorithmic}[1]
\REQUIRE $\ell$, $D$, $\lambda$, $d_l$, $d$ , $p$ \COMMENT{\textcolor{commentgreen}{loss function, samples, regularizer, lookahead depth, depth budget, postprocess flag}} 
\vspace{-0.32cm}
\STATE ModifiedGOSDT = GOSDT reconfigured to use \textbf{get$\_$bounds} (Algorithm \ref{alg::bounds}) whenever it encounters a new subproblem
\item $t_{lookahead} = $ ModifiedGOSDT$(\ell, D, \lambda,d_l)$ \COMMENT{\textcolor{commentgreen}{Call ModifiedGOSDT with depth budget $d_l$}}
\IF[\textcolor{commentgreen}{Fill in the leaves of this prefix}]{$p$}
\FOR {leaf $u \in t_{lookahead} $}
    \STATE $d_{u} = $ depth of leaf 
    \STATE $D(u) = $ subproblem associated with $u$
    \STATE $\lambda_{u} = \lambda \frac{|D|}{|D(u)|}$ \COMMENT{\textcolor{commentgreen}{Renormalize $\lambda$ for the subproblem in question}}
    \STATE $t_u = $ GOSDT$(D(u), d - d_{u},\lambda_{u})$ \COMMENT{\textcolor{commentgreen}{Find the optimal subtree for $D(u)$}}
    \IF {$t_u$ is not a leaf}
    \STATE Replace leaf $u$ with sub-tree $t_u$
    \ENDIF
\ENDFOR
\ENDIF
\RETURN $t_{lookahead}$
\end{algorithmic}
\end{algorithm}
We can constrain the search space to include only greedy trees past the lookahead depth by modifying the lower and upper bounds used in branch and bound (see Algorithm \ref{alg::bounds}). In particular, sub-problem nodes initialized at depths up to the lookahead depth are assigned initial lower and upper bounds equivalent to that in GOSDT \citep{gosdt} (see Section \ref{sec:bnb}). At the lookahead depth, however, the lower and upper bounds for a subproblem are fixed to be the loss of a greedy subtree trained on that subproblem. After these bound assignments, our algorithm uses the GOSDT algorithm with these new bounds to solve Equation \ref{eqn:lookahead_eqn_og} -- this is summarized by Lines $1$-$2$ in Algorithm \ref{alg::lookahead}. We defer more details of the GOSDT algorithm to Section \ref{sec:gosdt_details} in the Appendix.

\paragraph{Postprocessing with Optimal Subtrees}
Once we have solved Equation \ref{eqn:lookahead_eqn_og}, we do not need to use greedy sub-trees past the lookahead depth. We can improve our approach by replacing these subtrees with fully optimal decision trees. Lines $3$-$9$ in Algorithm \ref{alg::lookahead} illustrate this.
Thus, the performance of the lookahead tree with the aforementioned greedy subtrees is just an upper bound on the objective of the tree our method ultimately finds.

Note that the renormalization in line $6$ of Algorithm \ref{alg::lookahead} ensures that the $\lambda$ penalty stays proportional to the penalty for each misclassified point. 
Our objective (Equation \ref{eqn:obj}) assigns a $\frac{1}{|D|}$ penalty for each misclassification. If we call GOSDT on a smaller subproblem $D(u)$, our penalty per misclassification goes up by a factor of $\frac{|D(u)|}{|D|}$, so we need to scale $\lambda$ to stay proportional.

\subsection{LicketySPLIT: Polynomial-time SPLIT}\label{subsec:recursive}
We present a polynomial-time variant of SPLIT, called LicketySPLIT, in Algorithm \ref{alg::recursive_lookahead}. This method works by recursively applying SPLIT with lookahead depth 1. That is, we first find the optimal initial split for the dataset, given that we are fully greedy henceforth. Then, during postprocessing, instead of doing what SPLIT would do \textemdash running a fully optimal decision tree algorithm on the root's left and right subproblems \textemdash we run LicketySPLIT recursively on these two subproblems.
We stop considering further calls to LicketySPLIT for a subproblem if SPLIT returns a leaf instead of making splits (either due to the depth limit or $\lambda$).

\begin{algorithm}[ht]
\caption{LicketySPLIT($\ell$, $D$, $\lambda$, $d$)}\label{alg::recursive_lookahead}
\begin{algorithmic}[1]
\REQUIRE $\ell$, $D$, $\lambda$, $d$ \COMMENT{\textcolor{commentgreen}{loss function, samples, regularizer, full depth}}
\item $t_{lookahead} = $ SPLIT$(\ell, D, \lambda,1,d,0)$ \COMMENT{\textcolor{commentgreen}{Call SPLIT with lookahead depth $1$ and no post-processing}}
\IF{$t_\textrm{lookahead}$ is not a leaf}
\FOR {child $u \in t_\textrm{lookahead} $}
    \STATE $D(u) = $ subproblem associated with $u$
    \STATE $\lambda_{u} = \lambda \frac{|D|}{|D(u)|}$ \COMMENT{\textcolor{commentgreen}{Renormalize $\lambda$ for the subproblem in question}}
    \STATE $t_u = $ LicketySPLIT$(\ell, D(u), \lambda_{u}, d - 1)$
    \STATE Replace $u$ with subtree $t_u$
\ENDFOR
\ENDIF
\RETURN $t_\textrm{lookahead}$
\end{algorithmic}
\end{algorithm}

\subsection{RESPLIT: Rashomon set Estimation with SPLIT}

At the cutting edge of compute requirements for decision tree optimization is the computation of Rashomon sets of decision trees. \citet{xin2022treefarms} compute a Rashomon set of all near-optimal trees, based on the GOSDT algorithm \cite{gosdt}. This task generates an extraordinary number of trees and has high memory and runtime costs. To make this tractable, \citet{xin2022treefarms} leverage depth constraints and feature selection from prior work to reduce the depth and set of features considered \citep{gosdt_guesses}. While necessary for scalability, this can prevent exploration of near-optimal models across all features or at greater decision tree depths. Both factors are relevant for work on variable importance based on Rashomon sets \citep{fisher2018model, dong2020exploring, donnelly2023the}. We leverage SPLIT as a way to dramatically improve scalability of Rashomon set computation, reliably approximating the full Rashomon set and allowing feasible exploration while relaxing or removing depth and feature constraints.

Our algorithm, RESPLIT, is described in Appendix \ref{sec:resplit_alg}; it first leverages SPLIT as a subroutine to obtain a set of prefix trees such that completing them greedily up to the depth budget would result in an $\epsilon$ approximation of the optimal solution to Equation \ref{eqn:lookahead_eqn_og}. At each leaf of each prefix tree, it calls TreeFARMS \cite{xin2022treefarms} to find a large set of shallow subtrees that are at least as good as being greedy, yielding an approximate Rashomon set computed much faster than state of the art. We also show a novel indexing mechanism to query RESPLIT trees in Appendix \ref{sec:resplit_indexing}.
\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.74\linewidth]{figures/gosdt_vs_our_methods.png}
    \caption{Regularized test loss vs training time (in seconds) for GOSDT \citep{gosdt_guesses} vs our algorithms. The size of the points indicates the number of leaves in the resulting tree. Both SPLIT and LicketySPLIT are much faster for most values of sparsity penalty $\lambda$, with the only potential slowdown being in the sub-second regime due to overhead costs. }
    \label{fig:gosdt_vs_lookahead}
\end{figure*}