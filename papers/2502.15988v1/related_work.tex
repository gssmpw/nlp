We are interested in accurate, interpretable decision tree classifiers that we can find efficiently. We discuss these three goals as they pertain to existing work. 

Consistent with recommendations from \citet{rudin2022interpretable, costa2023recent}, we emphasize sparsity, expressed in terms of the number of leaves, as the primary mechanism for tree interpretability. Sparsity has a strong correlation with user comprehension \citep{Piltaver2016Comprehensible}.
\citet{zhou2018measuring} fit a regression model to user-reported interpretability for decision trees, also finding that trees with fewer leaves were more interpretable. They also found that deep, sparse trees were more interpretable than shallow trees with the same sparsity. \citet{izza2022tackling} provides a way to use a sparse decision tree to provide succinct individual explanations. However, finding deep, sparse trees with existing methods can be computationally infeasible. We bridge this gap -- our algorithms are capable of finding sparse trees without constraining them to be shallow.

\paragraph{Greedy Decision Trees}
A long line of work explores greedy algorithms such as CART \citep{breiman1984classification} and C4.5 \citep{quinlan2014c45}. These methods first define a heuristic feature quality metric such as the Gini impurity score \citep{breiman1984classification} or the information gain \citep{quinlan2014c45} rather than choosing a global objective function. At every decision node, the feature with the highest quality is chosen as the splitting feature. This process is repeated until a termination criteria is reached. One such criteria often used is the minimum support of each leaf. Trees can then be postprocessed with pruning methods.
\paragraph{Branch and Bound Optimization}\label{sec:bnb}
Among the many methods for globally optimizing trees, Branch-and-bound approaches with dynamic programming are state of the art for scalability, because they exploit the structure of decision trees 
\citep{costa2023recent, gosdt, murtree, gosdt_guesses, dl85}. While many other methods exist for optimizing trees, such as MIP solvers \citep{bertsimas2017optimal, verwer2019learning}, we focus our discussion and comparison of globally optimal decision tree methods on the currently fastest types of approaches - branch and bound with dynamic programming. 
These approaches search through the space of decision trees while tracking lower and upper bounds of the overall objective at each split to reduce the search space. 
They can find optimal trees on medium-sized datasets with tens of features and shallow maximum tree depths \citep{maptree, dl85, gosdt, murtree}. \citet{dl85} uses a dynamic programming with branch and bound (DPBnB) method that uses advanced caching techniques to find optimal decision trees, though it does not explicitly optimize for sparsity. In contrast, \citet{gosdt,osdt} use a DPBnB approach to find a tree that optimizes a weighted combination of empirical risk and sparsity, defined by the number of leaves in the tree. \citet{gosdt_guesses} further enhances this approach by incorporating smart guessing strategies to construct tighter lower bounds for DPBnB, resulting in computational speedups. \citet{murtree} takes a different approach by focusing on finding the optimal tree with a hard constraint on the number of permissible nodes, using advanced caching techniques and an optimized depth-2 decision tree solver. \citet{quantbnb} addresses continuous features by defining lower and upper bounds based on quantiles of feature distributions. However, their method is applicable only to shallow optimal trees with depth $\leq 3$, limiting its utility in scenarios with higher-order feature interactions. 

\paragraph{Lookahead Trees}

Some older approaches to greedy decision tree optimization consider multiple levels of splits before selecting the best split at a given iteration
\citep{Norton1989GeneratingBD}. 
That is, unlike the other greedy approaches, these approaches do not pick the split that optimizes a heuristic immediately. Instead, they pick a split that sets up the best possible heuristic value on the following split.

These approaches still focus on locally optimizing a heuristic measure that is not necessarily aligned with a global objective. By contrast, our method selects splits to directly optimize the sparse misclassification rate of the final tree. We globally optimize the search up to the specified lookahead depth, switching to heuristics only when deciding splits past our lookahead depth. In so doing, our method largely avoids the pathology noted in \citet{murthy1995lookahead}, who note cases where their own lookahead approach results in a substantially worse tree than one constructed with a standard greedy approach. For our method, it is provably impossible for a fully greedy entropy-based method with the same constraints as our approach to achieve a better training set objective than our approach. (See Theorem \ref{thm:relwork})

\paragraph{Other Hybrid Methods}
Several other approaches are compatible with branch and bound techniques. \citet{topk} seeks to bridge the gap between greedy and optimal decision trees by selecting a fixed subset of the top $k$ feature splits for each sub-problem.
However, this framework does not explicitly account for sparsity. Further, the method is limited by using a \textit{global} setting for search precision: the approach considers the same number of candidate splits at each subproblem. As we show in our experiments, there is merit to tailoring the level of search precision to parts of the search space where it is most needed. 

There are a few methods that use probabilistic search techniques to optimize trees. \citet{maptree} takes a Bayesian approach, finding the maximum-a-posteriori tree by optimizing over an AND/OR graph, akin to the graph used in earlier branch-and-bound methods like that of \citet{gosdt}. Although their method demonstrates strong performance, their experimental results reveal that it is not responsive to sparsity-inducing hyperparameters -- accordingly, we found in our experiments that the method struggles to optimize for sparsity.

Recent work by \citet{thompson_aistats} devises a Monte Carlo Tree Search algorithm using Thompson sampling to enable online, adaptive learning of sparse decision trees. 
We show that our method achieves superior performance and sparsity on all datasets tested.
