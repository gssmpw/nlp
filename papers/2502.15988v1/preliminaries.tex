
We consider a typical supervised machine learning setup, with a dataset $D = \{(\bx_i, y_i)\}_{i=1}^N$ sampled from a distribution $\mcD$, where $\bx_i \in \{0,1\}^K$ is a binary feature vector and $y_i\in \{0, 1\}$ is a binary label.\footnote{The discussions and methods in this paper can trivially be extended to multiclass problems; we focus our discussion and evaluation of the methodology on binary labels.} Let $\mcF$ be the set of features. Define \(D(f)\) as the subset of \(D\) consisting of all samples where feature \(f \in \mathcal{F}\) is 1 (and \(D(\bar{f})\) as the subset where feature $f$ is 0). Let \(D^+\) and \(D^-\) denote the set of examples with positive and negative labels, respectively. 
\paragraph{Node specific notation}
Let $D_t$ be the support set of node $t$ in a tree (i.e., the set of training examples assigned to this node); we call each $D_t$ a \textit{subproblem}. 
Let $f_t \in \mcF$ be the feature we split on at $t$. Let $D_t(f_t)$ and $D_t(\bar{f_t})$ be the support sets of the children of $t$. Unless stated otherwise, a greedy split at node $t$ chooses the feature $f$ that maximizes the information gain, which is equivalent to solving:
\[
      f_t = \min_{f\in \mcF} \frac{|D_t(f)|}{|D_t|} H\Bigg(\frac{|D_t^+(f)|}{|D_t(f)|}\Bigg)
    + \frac{|D_t(\bar{f})|}{|D_t|}H\Bigg(\frac{|D_t^+(\bar{f})|}{|D_t(\bar{f})|}\Bigg)
\]
with entropy $H(p) = -p\log p - (1-p)\log(1-p)$.
\paragraph{Tree specific notation}
We now briefly discuss sparse greedy and optimal trees.
We define $T_g(D, d, \lambda)$ to be a decision tree of depth at most $d$ trained greedily on $D$ with sparsity penalty $\lambda$. Intuitively, this sparse greedy algorithm will make a split at a node only when the gain in overall accuracy is greater than $\lambda$. Algorithm \ref{alg:greedy} in the Appendix illustrates this procedure. Modern methods such as \citet{gosdt, gosdt_guesses}, on the other hand, find a tree $T$ in the space of decision trees $\mathcal{T}$ that solves the following optimization problem:
\begin{align}
    &\mcL^*(D, d, \lambda) = \min_{T \in \mathcal{T}} L(T, D, \lambda) \textrm{ s.t.  depth$(T)$ $\leq d$}
    \label{eqn:obj} \\
    &= \min_{T \in \mathcal{T}} \sum_{i=1}^N\frac{1}{N}\Big({l\big(T(\bx_i), y_i\big) + \lambda S(T)\Big)} \textrm{s.t. depth$(T)$ $\leq d$} \nonumber
\end{align}
where $L(T, D, \lambda)$ is the regularized loss of tree $T$ on dataset $D$, $S(T)$ is the number of leaves in $T$ and $\ell(T(\bx), y)$ is the loss incurred by $T$ in its prediction on $\bx$ (for this paper, we set $\ell$ to be the 0-1 loss). 
As discussed in Section \ref{sec:bnb}, the fastest contemporary methods solve this problem using a branch-and-bound approach~\citep{costa2023recent, gosdt, murtree,  gosdt_guesses}.

\paragraph{Rashomon Sets} Our work is motivated by the properties of near-optimal decision trees and allows for scalable approximation of that set. 
\citet{xin2022treefarms} define the Rashomon set, denoted by $\mathcal{R}(D,\lambda, \epsilon,d)$, as the collection of all trees whose objective is within $\epsilon$ of the minimum value in Equation \ref{eqn:obj}. Formally: 
\begin{align} 
\mathcal{R}(D,\lambda, \epsilon,d) = \{T \in \mathcal{T}&: L(T,D, \lambda)\leq \mathcal{L}^*(D,d,\lambda) +\epsilon \nonumber \\ \ \ \land \ &\textrm{depth}(T) \leq d\}. 
\end{align} 
In the Appendix Section \ref{sec:characterization_of_near_optimal_trees}, we use Rashomon sets to investigate properties of near-optimal trees.

Rashomon sets can be used for a range of downstream tasks \cite{RudinEtAlAmazing2024}; one crucial task is the measurement of variable importance over a set of near-optimal models instead of only for a single model \cite{donnelly2023the, fisher2018model}. Reliable variable importance measures in this setting rely on minimal feature selection prior to computing the Rashomon set and minimal constraints on the tree's depth to allow high-order interactions. Our approach can be used to accelerate the computation of a Rashomon set, supporting the feasibility of these approaches. 

\paragraph{Branch and Bound}
Given a depth budget $d$, branch and bound with a sparsity penalty \citep{gosdt, gosdt_guesses} finds the optimal loss $\mathcal{L}^*(D,d,\lambda)$ that minimizes Equation \ref{eqn:obj}. 

The key insight behind branch and bound is that the optimal solution for dataset $D$ at depth $d'$ has a dependency on the optimal solution for datasets $D(f)$ and $D(\bar{f})$ at depth $d'-1$, for each $f \in \mcF$. Starting from the root, branch and bound algorithms consider different candidate features, $f$, on which to split in the process of determining the objective. As candidates are considered, we identify the subproblems we encounter by the subset of data they relate to and their remaining depth. We track current upper and lower bounds of subproblems in order to prune parts of the search space as we explore it. In particular, if our lower bounds on $ \mcL^*(D_t(f_1), d^\prime-1, \lambda)$ and $\mcL^*(D_t(\bar{f_1}), d^\prime-1, \lambda)$ sum to a larger value than the sum of upper bounds on $\mcL^*(D_t(f_2), d^\prime-1, \lambda)$ and $\mcL^*(D_t(\bar{f_2}), d^\prime-1, \lambda)$, for example, then we have proven that $f_1$ is not the minimizing split for dataset $D$.

$\mcL(D_t, d', \lambda)$ can always start with an upper bound of $ub = \lambda + \min\Big(\frac{|D_t^-|}{|D_t|}, \frac{|D_t^+|}{|D_t|}\Big)$. A universal lower bound is $\lambda$. To get a tighter lower bound, if $d' > 0$, the lower bound can start at $\min(ub, 2 \lambda)$, since either $\mcL(D_t, d', \lambda) = ub$, or the objective will be the sum of two other $\mcL$ calls, both of which must necessarily have cost at least $\lambda$. 
These upper and lower bounds are then updated as we explore a graph structure containing these subproblems. Once these bounds have converged, and we know the value of $\mcL(D, d', \lambda)$ for the whole dataset $D$, we can extract the optimal tree by simply tracking the feature $f$ that leads to the optimal score for $D$ and then successively track the splits for the optimal value with respect to $D(f)$ and $D(\bar{f})$, and so on. 

\paragraph{Discretization}
Our algorithm will assume feature vectors to be binary, i.e., $\bx_i \in \{0,1\}^K$. Real-world datasets often have features that require discretization to fit our setting. While some methods preserve optimality (e.g., splitting at the mean between unique values in the training set), others such as bucketization \citep[described and proven to be suboptimal in][]{gosdt}, binning into quantiles, and feature engineering reduce the search space at the cost of optimality. In our experiments, we use threshold guessing \citep{gosdt_guesses}, which sacrifices optimality with respect to a real-valued dataset but maintains theoretical and empirical guarantees relative to a reference decision tree ensemble. 
