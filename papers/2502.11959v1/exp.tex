\definecolor{lightgray}{gray}{0.9}


\section{Experiment}

\begin{table*}[t]
  \centering
  \begin{adjustbox}{max width=\textwidth}
  \setlength{\tabcolsep}{3.5pt}
    \begin{tabular}{llcccc}
    \toprule
    \textbf{Model Family} & \textbf{Approach} & \textbf{HOVER-2}  & \textbf{HOVER-3}  & \textbf{HOVER-4}  & \textbf{FEVEROUS-S} \\
    \midrule
    \multirow{4}{*}{PT/FT Models} 
    & BERT-FC & 53.40  & 50.90  & 50.86 & 74.71 \\
    & LisT5 & 56.15 & 53.76 & 51.67 & 77.88 \\
    & RoBERTa-NLI & \underline{74.62} & 62.23 & 57.98 & 88.28 \\
    & MULTIVERS & 68.86 & 59.87 & 55.67 & 86.03 \\
    \midrule
    
    \multirow{4}{*}{Llama-3-8B} 
    & Zero-shot & 55.10  & 55.09 & 53.51 & 78.21 \\
    & Zero-shot + CoT & 63.76 & 57.13 & 57.47 & 84.94 \\
    & Few-shot & 55.33 & 55.63 & 52.86 & 79.17 \\
    & Few-shot + Structured CoT & 69.71 & \underline{66.71} & 59.63 & 85.67 \\
    \midrule
    
    \multirow{3}{*}{LoRA-Llama-3} 
    & LoRA Fine-tuning & 64.21 & 60.35 & \underline{60.34} & \underline{91.52} \\
    & STaR* (Self-Improvement) & 60.90  & 58.61 & 56.86 & 87.45 \\
    & \cellcolor{gray!20}\textbf{\themodel (Ours)} & \cellcolor{gray!20}\textbf{76.13} \scriptsize{$\pm$0.84} & \cellcolor{gray!20}\textbf{70.50} \scriptsize{$\pm$0.55} & \cellcolor{gray!20}\textbf{68.50} \scriptsize{$\pm$1.27} & \cellcolor{gray!20}\textbf{91.91} \scriptsize{$\pm$0.44} \\
    \bottomrule
    \end{tabular}%
    \end{adjustbox}
    \caption{Macro-F1 scores for claim verification models on HOVER and FEVEROUS-S datasets. ``PT/FT'' refers to pretrained/fine-tuned models.}
  \label{tab:main_results}%
\end{table*}%


\subsection{Datasets}
We evaluate \themodel using two publicly available datasets, following prior work in claim verification~\cite{gong2024navigating}. All evaluations are performed on the validation sets, since the test sets are not publicly released. Detailed information can be found in Appendix~\ref{sec:datasetinfo}. Results are reported using the Macro-F1 score.
\begin{itemize}
\item \textbf{HOVER}~\cite{jiang2020hover} \xspace This dataset comprises claims necessitating multi-step reasoning across multiple pieces of evidence and is categorized into three subsets: HOVER-2 for two-hop reasoning, HOVER-3 for three-hop reasoning, and HOVER-4 for four-hop reasoning.
\item \textbf{FEVEROUS-S}~\cite{aly2021feverous} \xspace Derived from the FEVEROUS dataset, this subset contains claims that rely solely on unstructured textual evidence. Compared to HOVER, claims in FEVEROUS-S generally exhibit lower complexity in reasoning.  
\end{itemize}

\subsection{Baselines}
To assess the effectiveness of \themodel, we compare it against a variety of baselines, including pretrained/fine-tuned models, prompt-based approaches using the base model, and base model fine-tuned on the training data.

For pretrained/fine-tuned models, we include the following: (i) BERT-FC~\cite{soleimani2020bert}: Pretrained BERT model~\cite{devlin2018bert} tailored for fact-checking tasks. (ii) LisT5~\cite{jiang2021exploring}: Pretrained T5 model~\cite{raffel2020exploring} specialized for fact-checking tasks. (iii) RoBERTa-NLI~\cite{nie2020adversarial}: Pretrained RoBERTa-large model~\cite{liu2019roberta} fine-tuned on four natural language inference datasets. (iv) MULTIVERS~\cite{wadden2022multivers}: A LongFormer model~\cite{beltagy2020longformer} fine-tuned on the FEVER~\cite{thorne2018fever} dataset.

For prompt-based approaches, we evaluate the following: (i) Zero-shot: The model predicts the final label directly, without requiring reasoning steps. (ii) Zero-shot + CoT: The model outputs reasoning chains before predicting the final label. (iii) Few-shot: Similar to zero-shot, but with the inclusion of labeled examples. (iv) Few-shot + Structured CoT: Similar to zero-shot + CoT, but with examples of structured reasoning chains from $\mathcal{D}_h$. All baseline prompts are similar to $T(c,\mathcal{E})$ for fairness, detailed in Appendix~\ref{sec:appendix_prompt_baseline}.

Finally, for base models that undergo fine-tuning, we compare with: (i) Lora Fine-tuning: Fine-tuning the base model on the training set $\mathcal{D}$ using only binary labels. (ii) STaR*~\cite{zelikman2022star}:  We re-implemented this self-improvement method for claim verification. While similar to our approach (Algorithm~ \ref{alg:STRIVE}), it differs in that reasoning chains are generated freely by the model, without structural constraints or a warm-up phase.

\subsection{Implementation Details}
We use Llama-3-8B-Instruct as the base model, as it is a widely used open-source language model. For all fine-tuning tasks, including those in baseline models and \themodel, we employ the GPU memory-efficient LoRA fine-tuning method~\cite{hu2021lora}, allowing our experiments to fit on a single consumer-grade GPU (e.g., NVIDIA 4090).

In the Structured Warm-up process, we use a small human-annotated dataset $\mathcal{D}_h$ containing only $H=10$ examples, with 8 labeled as \textit{Refuted} and 2 as \textit{Supported}. We prioritize teaching the model to identify errors rather than admitting correct claims. The intermediate model $M^*$ is obtained by fine-tuning on $\mathcal{D}_h$ for 10 epochs using LoRA. Despite the large number of epochs, only 0.1\% of the model's parameters are updated, ensuring the modelâ€™s overall performance is maintained while enforcing the prescribed structure. The training set $\mathcal{D}$ consists of $N=600$ examples, with reasoning chains generated at a temperature setting of 0.01. The final model $M_{st}$ is obtained by fine-tuning for 2 epochs on the union of the collected set and the human-annotated set, $\mathcal{D}_{st} \cup \mathcal{D}_h$.

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{figs/Structure.pdf}
  \caption{Comparison of Chain-of-Thought approaches with and without structured reasoning over HOVER datasets.}
  \label{fig:structure}
\end{figure}

\section{Results and Discussion}
\subsection{Overall Performance}
We present the overall results of \themodel and the baseline models in Table \ref{tab:main_results}. Among all methods, \themodel achieves the best performance, underscoring the effectiveness of our approach. The results also highlight the importance of explicitly reasoning steps in claim verification. Pretrained/fine-tuned models, zero-shot and few-shot methods that do not incorporate reasoning steps, generally perform worse than methods that explicitly utilize reasoning steps (e.g., Zero-shot + CoT, Few-shot + Structured CoT). 
%Notably, among the methods that involve reasoning steps, those leveraging structured reasoning (e.g., \themodel, Few-shot + Structured CoT) outperform the others, demonstrating the advantage of our structured reasoning design. 

We present reasoning chains generated by \themodel and \textbf{error analysis} in Appendix~\ref{sec:cases}.


\subsection{Effectiveness of Structured Reasoning}
In this section, we explain how Structured Reasoning contributes to performance improvement in claim verification.

\textbf{Structured Design Improves Reasoning Quality.}\xspace
We compare three approaches: Zero-shot + CoT, Few-shot + CoT, and Few-shot + Structured CoT. Zero-shot + CoT and Few-shot + CoT use chain-of-thought prompting without structural constraints. In Few-shot + CoT, reasoning examples are rewritten by removing structural elements from $\mathcal{D}_h$'s chains. In contrast, Few-shot + Structured CoT directly uses reasoning examples from $\mathcal{D}_h$ with the desired structure. As shown in Figure~\ref{fig:structure}, while Few-shot + CoT performs slightly better on simpler tasks like HOVER-2, structured reasoning shows significant advantages in more complex scenarios like HOVER-3 and HOVER-4. This confirms that structured reasoning improves reasoning quality, particularly for complex tasks.

% We compare three approaches: Zero-shot + CoT, Few-shot + CoT, and Few-shot + Structured CoT. Both Zero-shot + CoT and Few-shot + CoT involve chain-of-thought prompting without structural constraints. In Few-shot + CoT, the reasoning examples are rewritten by removing structural elements from $\mathcal{D}_h$'s reasoning chains. In contrast, Few-shot + Structured CoT directly uses reasoning examples from $\mathcal{D}_h$, which include the desired structure. As shown in Figure~\ref{fig:structure}, while Few-shot + CoT slightly outperforms Few-shot + Structured CoT in simpler cases like HOVER-2, structured reasoning demonstrates significant advantages in more complex scenarios such as HOVER-3 and HOVER-4. This confirms that incorporating structured reasoning improves the overall quality of reasoning, especially for more complex tasks.

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{figs/Improve.pdf}
  \caption{Comparison of self-improvement training results with (top) and without (bottom) structured reasoning over HOVER datasets.}
  \label{fig:improve}
\end{figure}

\textbf{Structured Design Helps in Self-Improvement Training.}\xspace 
Figure~\ref{fig:improve} compares self-improvement results with (top) and without (bottom) structured reasoning. In the top part, performance before self-improvement corresponds to the model $M^*$ after the Structured Warm-up phase. In the bottom part, ``before'' and ``after'' refer to Zero-shot + CoT and STaR*, respectively. The figure shows that structured reasoning leads to steady performance improvement after the warm-up model, with gains increasing as task complexity rises (1.6\% for 2-hop, 4.4\% for 4-hop). In contrast, without the structured design, improvements are inconsistent. As shown in the bottom of Figure~\ref{fig:improve}, performance even drops on HOVER-2 and HOVER-4 due to the inclusion of erroneous reasoning chains that incorrectly match the final label. These results highlight the importance of structured reasoning in ensuring successful self-improvement for claim verification.
% Figure~\ref{fig:improve} compares the self-improvement training results with (top) and without (bottom) structured reasoning. In the top part of Figure~\ref{fig:improve}, the performance before self-improvement corresponds to the model $M^*$ after the Structured Warm-up phase. In the bottom part, the ``before'' and ``after'' results correspond to Zero-shot + CoT and STaR*, respectively. The figure shows that with our structured design, self-improvement progresses as expected, with performance continuously increasing after the warm-up model. The gains become more pronounced as task complexity increases (1.6\% for 2-hop and 4.4\% for 4-hop). However, without the structured design, improvements are not guaranteed. As shown in the bottom of Figure~\ref{fig:improve}, performance even decreases on HOVER-2 and HOVER-4 due to the inclusion of erroneous reasoning chains that incorrectly match the final binary label and are selected for training. These results demonstrate that incorporating structured reasoning is critical for ensuring the success of self-improvement in claim verification.


\begin{table}[t]
  \centering
    \begin{adjustbox}{max width=\columnwidth}
  \setlength{\tabcolsep}{3.5pt}
    \begin{tabular}{lccc}
    \toprule
    \multicolumn{1}{c}{\textbf{Models}} & \textbf{HOVER-2} & \textbf{HOVER-3} & \textbf{HOVER-4} \\
    \midrule
    \multicolumn{1}{c}{STRIVE} & \textbf{76.13} & \underline{70.50}  & \textbf{68.50}\\
    \midrule
    $\text{STRIVE}_{\text{w/o CD}}$ & 73.69 & 67.36 & 64.03 \\
    $\text{STRIVE}_{\text{w/o EA}}$ & 75.96 & 67.36 & 64.77 \\
    $\text{STRIVE}_{\text{w/o EG}}$ & 75.08 & 69.42 & 68.22 \\
    \midrule
    $\text{\themodel}_{\text{w/o FC}}$ & \underline{76.11} & 69.69 & 66.89 \\
    $\text{\themodel}_{\text{w/o hint}}$ & 75.90  & 70.34 & 67.20 \\
    $\text{\themodel}_{2\text{ rounds}}$ & 75.98 & \textbf{70.55} & \underline{68.43} \\
    \bottomrule
    \end{tabular}%
    \end{adjustbox}
    \caption{Ablation study results. Macro-F1 scores for different model variants across the HOVER datasets. ``CD'' stands for Claim Decomposition, ``EA'' stands for Entity Analysis, ``EG'' stands for Evidence Grounding Verification, and ``FC'' stands for Format Checking.}
  \label{tab:ablate}
\end{table}


\subsection{Analysis of Structure Design}
In this section, we analyze the structural reasoning design of \themodel. As outlined in Section~\ref{sec:structure}, the core components---Claim Decomposition, Entity Analysis, and Evidence Grounding Verification---are introduced to address the issue of low-quality reasoning chains generated by the model. To assess the contribution of each component, we conduct an ablation study by removing each design element individually, resulting in three variants: $\text{STRIVE}_{\text{w/o CD}}$, $\text{STRIVE}_{\text{w/o EA}}$, and $\text{STRIVE}_{\text{w/o EG}}$. The results are presented in Table \ref{tab:ablate}.

From the table, it is evident that Claim Decomposition is the most impactful design; its removal causes performance to drop across all HOVER datasets. Entity Analysis also plays a significant role, particularly in more complex scenarios (HOVER-4), as complex claims often involve multiple ambiguous entities. In contrast, the Evidence Grounding Verification design has a smaller impact on performance but still contributes valuable benefits, primarily by improving human readability of the reasoning chains.

\subsection{Analysis of Self-Improvement Training}
To analyze the self-improvement training process, we propose three variants: $\text{\themodel}_{\text{w/o FC}}$, $\text{\themodel}_{\text{w/o hint}}$ and $\text{\themodel}_{2\text{ rounds}}$, with performance results shown in Table \ref{tab:ablate}.

(i) $\text{\themodel}_{\text{w/o FC}}$ removes the format checking step during the Reasoning Chain Selection process, causing a performance drop, particularly on the HOVER-4 dataset. (ii) $\text{\themodel}_{\text{w/o hint}}$ eliminates the hint-based refinement in the Reasoning Chain Generation process (steps 4 and 5 in Algorithm \ref{alg:STRIVE}), leading to a small performance decrease across the HOVER datasets. (iii) $\text{\themodel}_{2\text{ rounds}}$ adds an extra round of self-improvement training (repeating steps 2-7 in Algorithm \ref{alg:STRIVE} with $M^*$ replaced by $M_{st}$). However, this results in performance similar to the original model, indicating no significant benefit from the additional round. Therefore, we opt to use a single round of training.

Interestingly, unlike math problem solving~\cite{zelikman2022star,hosseini2024v}, where extra rounds are critical, \themodel performs well with a single round. This is likely due to its use of structured guidance, which activates the model's inherent reasoning abilities, and no new capabilities are gained from further training rounds.
