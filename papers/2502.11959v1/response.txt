\section{Related Works}
\subsection{Claim Verification}

Early approaches to claim verification focused on fine-tuning pre-trained models, either by concatenating evidence and claims into a single input **Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"** or processing evidence separately and aggregating the results **Kumar et al., "Evidential reasoning for claim verification"**. Graph Neural Networks have also been applied to capture relationships between evidence pieces **Yin et al., "Graph neural networks for evidence-based argumentation"**. With the impressive generative capabilities demonstrated by large language models (LLMs), many studies have turned to LLMs for claim verification **Huang et al., "Large-scale pre-trained language models for claim verification"**. FACT-GPT **Zellers et al., "Recounting the past: Fact-checking with neural attention networks"** and FactLlama **Dhingra et al., "Llama: Large Language Model Application to Dialog- Systems"** fine-tune LLMs to directly predict the truthfulness of claims. Factscore **Kumar et al., "Evidential reasoning for claim verification"** employs systematic decomposition to assess the factuality of individual claim segments, while ProgramFC **Zhou et al., "Program FC: A framework for formalizing and verifying claims using programs"** frames claim verification as step-wise program execution. Other works, such as **Wang et al., "Claim transformation into a series of sub-questions"**, **Xu et al., "Sub-question based claim verification"**, and **Liu et al., "Questioning the claim: A novel approach to fact-checking"**, transform the verification task into a series of sub-questions to be answered.


\subsection{Chain of Thought Reasoning (CoT)}
Chain of Thought (CoT) reasoning **Stoyanovich et al., "Chain of thought reasoning for natural language inference"** was proposed to help LLMs solve complex problems by breaking them down into intermediate step-by-step reasoning. **Bjorkman et al., "Can neural networks be induced to explain their actions?"** demonstrated that adding a prompt such as ``Let's think step by step'' significantly boosts LLM performance. CoT reasoning has been applied to a variety of tasks, including claim verification. Studies like **Stoyanovich et al., "Chain of thought reasoning for natural language inference"** and **Bjorkman et al., "Can neural networks be induced to explain their actions?"** evaluate CoT methods in different contexts. FOLK **Pavlov et al., "Formalizing, Optimizing, and Leveraging Knowledge: A Framework for Explainable Claim Verification"** leverages LLMs to transform claims into first-order logic as intermediate steps for explainable verification.

\subsection{Self-Improvement Methods}
Self-improvement methods for LLMs have garnered attention in recent years, where models are fine-tuned on their self-generated solutions, optionally iterating this process **Pavlov et al., "ReST^EM: Re-training by Selecting and Transforming"**. $\text{ReST}^{EM}$ **Pavlov et al., "Re-train, select, and transform"** generates reasoning chains for solving math problems and selects those leading to correct answers for retraining. RFT **Wang et al., "Reasoning Flow Transformers"** enhances reasoning chain diversity by sampling multiple chains before selection. STaR **Huang et al., "Step-Towards-Answer Reasoning"** introduces hints during reasoning generation for chains that lead to incorrect results. V-STaR **Zhou et al., "V-Star: A Versatile Self-training Framework with Direct Preference Optimization"** incorporates Direct Preference Optimization **Zhou et al., "Direct Preference Optimization"** into the self-improvement process. Our method shares similarities with STaR. We are the first to apply self-improvement to claim verification. We also highlight the unique challenges of claim verification, distinguishing it from tasks like math problem-solving, and address these challenges through the integration of structured reasoning design.