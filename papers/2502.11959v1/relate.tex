\section{Related Works}
\subsection{Claim Verification}

Early approaches to claim verification focused on fine-tuning pre-trained models, either by concatenating evidence and claims into a single input~\cite{aly2021feverous, thorne2018fever, hu2022dual} or processing evidence separately and aggregating the results~\cite{soleimani2020bert, jiang2021exploring,gi-etal-2021-verdict}. Graph Neural Networks have also been applied to capture relationships between evidence pieces~\cite{gong2024heterogeneous, Zhaotransxh, Chenevidencenet}. With the impressive generative capabilities demonstrated by large language models (LLMs), many studies have turned to LLMs for claim verification~\cite{ma-etal-2024-ex}. FACT-GPT~\cite{factgpt} and FactLlama~\cite{cheung2023factllama} fine-tune LLMs to directly predict the truthfulness of claims. Factscore~\cite{min2023factscore} employs systematic decomposition to assess the factuality of individual claim segments, while ProgramFC~\cite{pan2023fact} frames claim verification as step-wise program execution. Other works, such as \citet{li2023self}, \citet{chen2022generating}, and \citet{rani2023factify}, transform the verification task into a series of sub-questions to be answered.


\subsection{Chain of Thought Reasoning (CoT)}
Chain of Thought (CoT) reasoning~\cite{wei2022chain} was proposed to help LLMs solve complex problems by breaking them down into intermediate step-by-step reasoning. \citet{kojima2022large} demonstrated that adding a prompt such as ``Let's think step by step'' significantly boosts LLM performance. CoT reasoning has been applied to a variety of tasks, including claim verification. Studies like \citet{hu2023large} and \citet{dougrez2024assessing} evaluate CoT methods in different contexts. FOLK~\cite{wang2023explainable} leverages LLMs to transform claims into first-order logic as intermediate steps for explainable verification.

\subsection{Self-Improvement Methods}
Self-improvement methods for LLMs have garnered attention in recent years, where models are fine-tuned on their self-generated solutions, optionally iterating this process~\cite{hosseini2024v}. $\text{ReST}^{EM}$~\cite{singh2023beyond} generates reasoning chains for solving math problems and selects those leading to correct answers for retraining. RFT~\cite{yuan2023scaling} enhances reasoning chain diversity by sampling multiple chains before selection. STaR~\cite{zelikman2022star} introduces hints during reasoning generation for chains that lead to incorrect results. V-STaR~\cite{hosseini2024v} incorporates Direct Preference Optimization~\cite{rafailov2023direct} into the self-improvement process. Our method shares similarities with STaR. We are the first to apply self-improvement to claim verification. We also highlight the unique challenges of claim verification, distinguishing it from tasks like math problem-solving, and address these challenges through the integration of structured reasoning design.