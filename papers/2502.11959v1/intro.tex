\section{Introduction}

\begin{figure}[tb]
  \includegraphics[width=0.99\columnwidth]{figs/intro_fig.pdf}
  \caption{In math problem-solving, incorrect reasoning chains lead to mismatched final answers, while in claim verification, incorrect reasoning can still match the binary truth labels. Evidence omitted in figure.}
  \label{fig:intro_fig}
\end{figure}
The proliferation of misinformation is a major challenge in today's society, eroding trust in digital information and affecting domains such as public health~\cite{Naeem2020TheC} and politics~\cite{mishra2022factify}. As a result, claim verification---determining whether a claim is supported or refuted by evidence---has become crucial for ensuring reliable information.

Claim verification is a natural language inference task. While large language models (LLMs) demonstrate impressive reasoning capabilities, they struggle with claims requiring multi-hop evidence synthesis. Methods like few-shot learning and chain-of-thought (CoT) prompting have not yielded satisfactory results for complex claims.

Self-improvement methods have been successfully applied in domains like mathematical problem solving and commonsense reasoning~\cite{zelikman2022star,hosseini2024v}. These methods involve generating reasoning chains and answers, selecting the ones that lead to correct results, and using them to train the model for improved performance. However, directly applying self-improvement to claim verification proves ineffective. In mathematical problem solving and commonsense reasoning, the answers are typically numerical values or short phrases, with an almost infinite range of possibilities. This makes it difficult for incorrect reasoning chains to match the correct answer. In contrast, in claim verification, incorrect reasoning chains can easily match the binary truth labels (\textit{Supported}/\textit{Refuted}), leading to their selection for training, as illustrated in Figure~\ref{fig:intro_fig}. This introduces low-quality data into the training process, which undermines model performance. Our experiments validate this, showing that naive self-improvement degrades performance in claim verification task, with performance dropping by 4.5\% and 1.1\% on HOVER-2 and HOVER-4 (Table \ref{tab:main_results}), respectively.

Upon closer examination, we found that reasoning chains in claim verification often suffer from issues such as evidence confusion, entity misidentification, and omission of key information---even when the final truthfulness decision is correct. To address this, we propose a structured reasoning design. Our design requires three key components: 1) Claim Decomposition: Breaking complex claims into smaller, manageable subclaims; 2) Entity Analysis: Linking ambiguous terms to grounded entities; and 3) Evidence Grounding Verification: Citing specific evidence snippets at each reasoning step. This structure improves the quality of reasoning chains and reduces the chance of error-prone chains being included in the training data. Moreover, the structural constraints provide additional supervision signals that, when combined with binary labels, enable more effective filtering for self-improvement.

In this work, we introduce \themodel: Structured Reasoning for Self-Improved Verification. \themodel begins with a warm-up phase, where a base model is fine-tuned to generate structured reasoning chains using a small set of annotated examples. This prepares the model to follow the desired reasoning format. Next, the warm-up model is used to generate reasoning chains and verification results for all examples in the training set. We select reasoning chains that both lead to correct verification results and follow the correct structural format. Finally, the correct, structured reasoning chains are used as training data to fine-tune the base model. Our experiments demonstrate that, compared to the base model, \themodel achieves an average improvement of 31.4\% and 20.7\% over standard Chain of Thought (CoT) reasoning on HOVER datasets, underscoring the effectiveness of our approach.

Our contributions are summarized as follows:
\begin{itemize}
    \item We identify the limitations of freely generated reasoning chains in claim verification for self-improvement and propose \themodel, a solution that combines structured reasoning to address these challenges.
    \item We design a structured reasoning approach incorporating Claim Decomposition, Entity Analysis, and Evidence Grounding Verification, enhancing reasoning quality.
    \item Our experiments show that \themodel achieves significant improvements over the baseline model across multiple datasets, demonstrating the effectiveness of our approach.
\end{itemize}

