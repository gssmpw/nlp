


\newtcolorbox{mybox}{
    colframe=gray!50,    % Set border color
    colback=white,       % Background color inside the box
    coltitle=black,      % Title color (if you add a title)
    boxrule=0.8mm,       % Thicker border for a cleaner look
    arc=2mm,             % Slightly smaller rounded corners
    top=1mm,             % Add a little more space at the top
    bottom=1mm,          % Add a little more space at the bottom
    left=2mm,            % Adjusting side padding
    right=2mm,           % Adjusting side padding
    before skip=2mm,     % Add some vertical space before the box
    after skip=2mm,      % Add space after the box
    % fontupper=\small
}
\definecolor{sub}{HTML}{f6f6f6}



\section{Method}
In this section, we introduce our approach to claim verification. We first define the task itself, followed by our Structured Reasoning Design, which is specifically tailored for claim verification. Finally, we describe how this structure is applied within the self-improvement process to enhance the verification model’s performance.



\subsection{Task Formulation}
The task of claim verification is to determine the truthfulness of a given claim based on a set of evidence. Our approach aims to generate reasoning chains connecting the claim and evidence to a final prediction, rather than outputting the answer directly. However, it’s important to note that reasoning chains are not provided in the verification datasets, and only the final truth label is available.

Formally, given a claim $c$ and an evidence set $\mathcal{E} = \{e_1, e_2, \dots, e_n\}$, where each $e_i$ is a descriptive piece of evidence (such as a sentence or paragraph), the goal is to obtain a model that can generate a reasoning chain $r$ that forms the intermediate reasoning steps. The final prediction $\hat p$ is made based on the reasoning chain, where $\hat p \in \{\textit{Supported}, \textit{Refuted}\}$.

\begin{figure}[tb]
  \includegraphics[width=0.99\columnwidth]{figs/maingraph.pdf}
  \caption{(a) An example demonstrating our Structured Reasoning Design. (b) Overview of \themodel.}
  \label{fig:main}
\end{figure}

\subsection{Structured Reasoning Design}\label{sec:structure}
Freely generated reasoning chains often struggle with verifying complex claims due to issues like evidence confusion, entity misidentification, and omission of key information. These problems can lead to low-quality chains being selected during self-improvement, affecting performance.

To address this, we introduce a Structured Reasoning Design that guides the verification process systematically. Our Structured Reasoning Design consists of three key components: Claim Decomposition, Entity Analysis, and Evidence Grounding Verification. An example is illustrated in Figure~\ref{fig:main}.

\subsubsection{Claim Decomposition}
Complex claims typically describe multiple facets of a situation, such as in our example (Figure~\ref{fig:main}), where the claim covers the champion's nationality, victory, and origin. When a claim involves multiple elements, decomposing it into subclaims allows for independent verification of each component, a method known to improve performance in claim verification~\cite{gong2024navigating,min2023factscore}. In our design, we structure the reasoning chain into distinct blocks, each dedicated to a specific subclaim, labeled as \underline{\texttt{C1:}}, \underline{\texttt{C2:}}, and so on. This block-based structure helps mitigate the risk of overlooking key aspects during verification, which is common in free reasoning. Each subclaim block concludes with a verification result, labeled as \underline{\texttt{Status:}}, to make the reasoning easy to follow and parse.
\subsubsection{Entity Analysis}
Complex claims often involve ambiguous or unspecified entities or pronouns that may appear after Claim Decomposition. To resolve these ambiguities, we designed a two-step entity analysis process: Entity Resolution and Resolution Verification.
\begin{itemize}
    \item \textbf{Entity Resolution} aims to identify the specific entity corresponding to an ambiguous term by leveraging evidence, labeled as \underline{\texttt{Entity Resolution:}} in the reasoning chain. For instance, the term ``defending champion'' in the example claim (Figure~\ref{fig:main}) is resolved to the specific person ``Lin Dan'' using information from the evidence.
    \item \textbf{Resolution Verification} ensures the correctness of the Entity Resolution, labeled as \underline{\texttt{Resolution Verification:}} in the reasoning chain. While Entity Resolution may correctly match an ambiguous term to a known entity in many cases, errors can occur. For example, if the claim stated a ``Doubles tournament'' instead of a ``Singles tournament'' in the example, the Entity Resolution step might still match the term ``defending champion at the Men's Doubles tournament'' to ``Lin Dan'', which is incorrect for the revised claim. Resolution Verification cross-checks the entity to ensure that the resolution is accurate, preventing errors in the reasoning process.
\end{itemize}

This two-step design ensures more precise entity analysis, minimizing the risk of errors that could impact the overall verification. We do not always require both steps to appear simultaneously. For instance, in example C2, the Entity Resolution is drawn from C1, and therefore the Resolution Verification step is omitted.

\subsubsection{Evidence Grounding Verification}
Our structured reasoning framework incorporates explicit verification steps at multiple stages. In addition to the Resolution Verification process mentioned earlier, each subclaim undergoes a final verification step, labeled as \underline{\texttt{Verification:}}. This step evaluates the subclaim as a whole, considering the resolved entities and assessing its validity against the provided evidence. It also serves as an explanation for the truthfulness decision of the subclaim.

Our structured reasoning format enforces grounding at every stage of the reasoning chain. As illustrated in Figure~\ref{fig:main}, both the resolution and verification processes explicitly cite the corresponding evidence or subclaim identifiers (e.g., C1, E2). This ensures that conclusions are drawn from verifiable sources, making the reasoning more transparent and easier for humans to interpret and verify.


\subsection{Self-Improvement with Structure}
In this section, we describe how \themodel leverages the Structured Reasoning Design to improve model's performance in claim verification,  with the general flow of this process shown in the bottom part of Figure~\ref{fig:main}. Given a training set \begin{equation*}\mathcal{D} = \{(c_1, \mathcal{E}_1, p_1), \cdots, (c_N, \mathcal{E}_N, p_N)\},\end{equation*} where each $c_i$ is a claim, $\mathcal{E}_i$ is the corresponding evidence set, and $p_i$ is the final label, \themodel follows three main steps to complete the self-improvement process: Structured Warm-up, Reasoning Chain Generation and Selection, and Self-Improvement Training.
\subsubsection{Structured Warm-up}\label{sec:ourprompt}
Given a base model $M$, the purpose of the warm-up phase is to fine-tune $M$ into $M^*$, enabling it to generate reasoning chains conforming to our predefined structure. We use a preset prompt template $T(c,\mathcal{E})$ as follows:

\begin{mybox}
\textit{Based on the evidence, determine if the claim is supported by the evidence or refuted by it. Output the reasoning chain.\\
Claim: \texttt{[claim text $c$]}\\
Evidence: \texttt{(1)[evidence text $e_1$](2)...}}
\end{mybox}
Since reasoning chains are not available in the dataset, we manually annotate a very small set \begin{equation*} \mathcal{D}_h=\{(c_1^h,\mathcal{E}^h_1,r^h_1,p^h_1),\cdots,(c_H^h,\mathcal{E}^h_H,r^h_H,p^h_H)\}\end{equation*} with $H$ examples, where each pair $(c^h_i,\mathcal{E}^h_i)$ is annotated with a reasoning chain $r_i^h$ that following our prescribed structure. We then fine-tune the base model $M$ on $\mathcal{D}_h$ using $T(c^h_i,\mathcal{E}_i^h)$ as input and $r_i^h$ as output, resulting in a new model $M^*$. Notably, only a very small number of examples is needed for this fine-tuning, since our structure is guided by keywords (e.g., ``Verification:'') that activate the model’s inherent reasoning capabilities.
\subsubsection{Reasoning Chain Generation and Selection}
With the fine-tuned model $M^*$, our next objective is to leverage $M^*$ and training set $\mathcal{D}$ to generate and select high-quality structured reasoning chains for further self-improvement. Inspired by the chain generation strategy in STaR~\cite{zelikman2022star}, our method involves three stages: (i) Initial Generation and Selection and (ii) Refinement with Hint and (iii) Format Checking:


\begin{algorithm}[t]
\caption{\themodel}
\label{alg:STRIVE}
\textbf{Input:} Base model $M$, training set $\mathcal{D}$, annotated set $\mathcal{D}_h$\\
\textbf{Output:} Improved model $M_{st}$

\begin{algorithmic}[1]
\STATE $M^* \gets \text{train}(M,\mathcal{D}_h)$  \textcolor{gray}{// Structured Warm-up}
\STATE $\hat{r}_i \gets M^*(T(c_i, \mathcal{E}_i)),\ \hat{p}_i \gets J(\hat{r}_i)\ \forall i \in [1, N]$  \textcolor{gray}{//Get reasoning chains and predicted labels}
\STATE $\mathcal{D}_1 \gets \{(c_i, \mathcal{E}_i, \hat{r}_i) \mid \hat{p}_i = p_i \}$ \textcolor{gray}{//Select correct reasoning chains based on ground truth label}
\STATE  $\hat{r}_i^{hint} \hspace{-1mm}\gets \hspace{-1mm} M^*(T^{hint}(c_i,\mathcal{E}_i)),\ \hat{p}_i^{hint}\hspace{-1mm} \gets \hspace{-1mm} J(\hat{r}_i^{hint}),$\\ $\forall i\in[1,N],\text{ where }\hat{p}_i \neq p_i$ \textcolor{gray}{//Regenerate reasoning chains for incorrect predictions}
\STATE $\mathcal{D}_2 \gets \{(c_i, \mathcal{E}_i, \hat{r}_i^{hint}) \mid \hat p_i\neq p_i,\hat{p}_i^{hint} = p_i \}$\textcolor{gray}{//Select corrected reasoning chains}
\STATE $\mathcal{D}_{st} \gets \{(c_i, \mathcal{E}_i, r_i) \mid r_i \in (\mathcal{D}_1 \cup \mathcal{D}_2), f(r_i) = \text{True}\}$ \textcolor{gray}{//Select structurally valid chains}
\STATE $M_{st}\gets \text{train}(M,\mathcal{D}_{st}\cup\mathcal{D}_h)$ \textcolor{gray}{//Final Self-improvement training}
\end{algorithmic}
\end{algorithm}


\textbf{Initial Generation and Selection}: For each sample $(c_i,\mathcal{E}_i,p_i)\in \mathcal{D}$, we first generate a reasoning chain $\hat r_i$ using the prompt template $T(c_i,\mathcal{E}_i)$ with the model $M^*$:\begin{equation*}\hat{r_i}=M^*(T(c_i,\mathcal{E}_i))\quad \forall i\in[1,N].\end{equation*} A predicted label $\hat p_i$ is then derived from $\hat r_i$ by a rule-based function $J$: \begin{equation*}\hat p_i=J(\hat r_i),
\end{equation*}where if any subclaim in $\hat r_i$ is judged as \textit{Refuted}, then $\hat p_i=\textit{Refuted}$, and only if all subclaims are \textit{Supported}, then $\hat p_i=\textit{Supported}$. We select samples for which $\hat p_i$ matches the ground truth $p_i$ to form the set:\begin{equation*}\mathcal{D}_1=\{(c_i,\mathcal{E}_i,\hat r_i)\ |\ \hat p_i=p_i,\forall i\in [1,N]\}.\end{equation*}


\textbf{Refinement with Hint}: For samples where $\hat p_i\neq p_i$, we incorporate additional guidance by modifying the prompt. If $p_i = \textit{Supported}$, we add the hint: ``every detail in this claim is supported''; if $p_i=\textit{Refuted}$, we add ``the claim should be refuted, locate the error in the reasoning chain''. The modified prompt, inclusive of the hint, can be found in Appendix~\ref{sec:prompthint}. Using this modified prompt, we regenerate the reasoning chain: \begin{equation*}\begin{aligned}\hat r_i^{hint}&=M^*(T^{hint}(c_i,\mathcal{E}_i)),\\ \hat p_i^{hint}=J(\hat r_i^{hint})\quad &\forall i\in[1,N], \text{ where }\hat p_i\neq p_i.\end{aligned}\end{equation*}
Similarly, we select samples for which $\hat p_i^{hint}$ matches the ground truth $p_i$ to form the set:\begin{multline*}\mathcal{D}_2=\{(c_i,\mathcal{E}_i,\hat r_i^{hint})\ |\ \hat p_i\neq p_i,\hat p^{hint}_i= p_i,\\\forall i\in [1,N]\}\end{multline*} 

\textbf{Format Checking}: At this stage, both $\mathcal{D}_1$ and $\mathcal{D}_2$ contain reasoning chains that yield correct final predictions. To further ensure the quality of these reasoning chains, we apply a rule-based structural verification function $f$, where $f(\hat r_i)=\textit{True} \text{ or } \textit{False}$, indicating whether $\hat r_i$ follow our predefined structure. Specifically, we enforce three criteria:  
\begin{itemize}
\item Proper segmentation of subclaims, ensuring each reasoning step is explicitly delineated with a corresponding verification result.  
\item Correct evidence grounding, preventing incorrect references (e.g., citing non-existent evidence, citing C2 while verifying C1).  
\item Adherence to the structured format, ensuring reasoning steps are guided by predefined keywords (e.g. Entity Resolution:)
\end{itemize}
By applying this structural verification, we obtain a final set of reasoning chains $\mathcal{D}_{st}$, which not only yield correct conclusions but also adhere to the required structural format. Formally, it can be described as:
\begin{equation*}
\mathcal{D}_{st} = \{ (c_i, \mathcal{E}_i, r_i) \ |\ r_i \in (\mathcal{D}_1 \cup \mathcal{D}_2), f(r_i) = \textit{True} \}.
\end{equation*}


\subsubsection{Self-Improvement Training}
Finally, we fine-tune the base model $M$ using both the previously selected dataset $\mathcal{D}_{st}$ and the human-annotated dataset $\mathcal{D}_h$. Note that $M^*$ is discarded after generating the reasoning chains. This allows us to obtain the final model $M_{st}$ with the enhanced reasoning ability defined by our structured approach. The entire process is summarized in Algorithm \ref{alg:STRIVE}.


