\documentclass[9pt,shortpaper,twoside,web]{ieeecolor}
\usepackage{generic}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{array}
\usepackage{longtable}
\usepackage{textcomp}
\usepackage{url}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\markboth{\journalname, VOL. XX, NO. XX, XXXX 2017}
{Chen \MakeLowercase{\textit{et al.}}: Preparation of Review Paper for IEEE Access}

\begin{document}
\title{Denoising, segmentation and volumetric rendering of optical coherence tomography angiography (OCTA) image using deep learning techniques: a review}

\author{Kejie Chen, Xiaochun Yang, Jing Na (\IEEEmembership{Member, IEEE}), Wenbo Wang
\thanks{This work was submitted to IEEE Access for review on XXX. This work was supported by the National Natural Science Foundation of China, Grant No. XXXXX.}
\thanks{Kejie Chen, Jing Na and Wenbo Wang are with the Department of Mechanical and Electronic Engineering, Kunming University of Technology. }
\thanks{Xiaochun Yang is with the Eye Clinics, Yunnan First People's Hospital.}
\thanks{Corresponding author: Kejie Chen (e-mail: kejie\_2020@163.com).}}

\maketitle

\begin{abstract}
Optical coherence tomography angiography (OCTA) is a non-invasive imaging technique widely used to study the vascular structures and micro-circulation dynamics in the retina and choroid. OCTA has been widely used in clinics for diagnosing ocular disease and monitoring its progression, because OCTA is safer and faster than dye-based angiography while retaining the ability to characterize micro-scale pathological features such as the microaneurysms and ischemia areas. However, OCTA data always contains many inherent noises from the devices and acquisition protocols and suffers from various types of artifacts, which influences data homogeneity and impairs diagnostic accuracy and repeatability. Deep learning (DL) based imaging analysis models are able to automatically detect and remove artifacts, remove systematic noises and enhance the quality of image data. It is also a powerful tool for segmentation and identification of normal and pathological structures in the images. Thus, the value of OCTA imaging can be significantly enhanced by the DL-based approaches for interpreting and performing measurements and predictions on the OCTA data. In this study, we reviewed literature on the application of DL models in OCTA images in the latest five years. In particular, we focused on discussing the current problems and limitations in the OCTA data and the corresponding design principles of the architectures and training strategies of the DL models for OCTA image denoising, enhancement and segmentation. We also reviewed the state-of-art DL models for volumetric reconstruction of the vascular networks and pathological retinal structures such as the edema and the distorted optic disc. In addition, the publicly available dataset of OCTA images are summarized at the end of this review. Overall, this review can provide valuable insights for engineers to develop novel DL models by utilizing the characteristics of OCTA signals and images as well as the disease-specific pathological features. The pros and cons of each DL methods and their functions and applications discussed in this review can be helpful to assist technicians and clinicians to select and use proper DL models for fundamental research and disease screening.

\end{abstract}

\begin{IEEEkeywords}
OCTA, deep learning, image enhancement, segmentation, 3D reconstruction
\end{IEEEkeywords}

\section{Background}
\label{sec:background}
Optical coherence tomography angiography (OCTA) is an emerging technique that enables imaging retinal capillary plexuses, choroicapillaris and vascularization in the anterior segments \cite{1,2,3}. The development of OCTA has been around twenty years, while the first demonstration of visualizing the vasculature in the human eye was performed in 2006 \cite{4}. Compared with traditional methods of imaging flow, including fluorescein angiography (FA) and Indocyanine green angiography (ICGA), OCTA has the advantage that it can visualize deeper vasculature with depth resolution, does not require administration of exogenous contrast, and is not obscured by dye leakage \cite{5}. Thus, OCTA has now been one of the most widely-used techniques in clinic and laboratory research for monitoring the ocular condition and diagnosing ocular diseases, such as glaucoma, diabetic retinopathy and macular degeneration \cite{6,7,8,9,10}.

OCTA utilizes the concept of low coherence interferometry of optical coherence tomography (OCT) to acquire cross sectional (B-scan) images. By performing repeated B-scans in the same location and analyzing the signal decorrelation, the motion contrast is detected for determination of flows (Fig. 1) \cite{1,11,12}. Two types of OCTA techniques using different B-scan modes are currently available, namely spectral domain OCTA (SD-OCTA) and swept source OCTA (SS-OCTA). SD-OCTA uses a continuous wavelength light source, and detect the back scattered light using a spectrometer and a line scan camera. SS-OCTA uses more expensive tunable swept laser which emits a single wavelength at any instant and swept a broad range of wavelengths, and then use photodetector for detecting interference spectrum. Compared with SD-OCTA, SS-OCTA operates at larger wavelengths (around 1050 $nm$)  and scanning speeds greater than 100 $kHz$. Therefore, SS-OCTA has a deeper penetration depth and greater signal-to-noise ratio \cite{13,14}.

\begin{figure}[htp]
\centerline{\includegraphics[width=\columnwidth]{fig1.png}}
\caption{Working principles of OCTA. \textbf{(a)} Schematic illustration of the \textit{en face} and depth direction in the fundus. \textbf{(b)} Example of \textit{en face} OCT and OCT B-scan images. \textbf{(c)} Schematic illustration of acquiring flow signal using decorrelation of consecutive B-scans. \textbf{(d)} Example of \textit{en face} OCTA images generated based on maximum intensity projection of a bulk layer of flow signals.}
\label{fig1}
\end{figure}

Though OCTA has the advantages of visualizing multi-layer microvasculature, the acquired image data is highly dependent on the OCTA instrument, scanning protocol, signal processing and image generation methods \cite{15,16,17}. There are many more artifacts in OCTA images which can easily lead to the misinterpretation compared with FA/ICGA methods \cite{18}. Specifically, microsaccade or other rapid movements of retinal and choroid induced by cardiac cycle and breathing can generate motion artifacts, such as stripes and crisscross of vessels. Because fluctuating motions of retina and choroid are difficult to discern and predict, these subtle effects are often ignored during the acquisition and processing of signals, but they can be magnified and generate large motion artifacts in the images \cite{1,19,20}. Influenced by eye blinking, large lipid deposit, cystoid and edema, the refractive shift can cause a part of the image to be out of focus and loss of information, or generate variations of OCT signals which are unrelated to the flow \cite{1,21,22}. Ocular media opacity, edema, hemorrhage, pigment clumping, lesions and other light blockage are also main causes of low-OCT-signal artifact, which may result in segmentation error and abrupt signal alteration in the images \cite{23}. \textit{En face} OCTA images of the vasculature are created by summing or projecting a slab of tissue layers. When the detection and segmentation of various retinal layers is incorrect, it leads to the segmentation artifacts and generate vascular image without clinical meaning \cite{24}. Segmentation and projection artifacts widely exist when imaging eyes with pathology. The disrupted retinal structures, such as the abnormal retinal thickness, absence of layers and altered layer curvature, make it hard for computational algorithms to identify and segment retinal layers correctly \cite{1,25}. Moreover, to overcome the weakness of the \textit{en face} OCTA images, including segmentation errors, flattening of data, requiring experts to visualize images slice-by-slice and mentally reconstruct to volume, algorithms for 3D volume rendering of the data are important and helpful to ophthalmologists \cite{26}. For simple structures including the curved retinal tissue layers, cystoid spaces and tumefactions, the structure are easier to be segmented and reconstructed in 3D \cite{27}. However, the segmentation and volumetric reconstruction of vascular plexuses and irregular abnormalities in the retinal layers are still challenging with en-face imaging. In addition, 3D volume rendering has not been implemented in most commercialized instrument so far \cite{1}.

In recent years, deep learning technologies has led to breakthroughs in ophthalmology. Deep learning (DL) models developed using medical images have shown great capability and high accuracy of automatically screening and diagnosing vision-threatening diseases, such as diabetic retinopathy (DR), glaucoma, age-related macular degeneration (AMD) and cataract \cite{28,29,30,31,32}. Most of the works utilizes color fundus photographs to train and test the DL models, because accurate training requires a large amount of data and high-standard data annotation, and color fundus images are relatively easier to acquire in clinics \cite{33,34}. In addition, the vasculature and pathological information in the fundus images contain less noises and artifacts, which can be distinguished effectively \cite{35}. OCT images, which provides structural information of retinal and choroid layers, anterior chamber and the vitreous with high depth resolution, are also a widely-used image type in DL models \cite{36, 37}. Though the quality and homogeneity in the OCT dataset is influential\cite{38}, many supervised and weakly-supervised DL models have been developed to correctly identify and characterize the retinal layers, retinal fluid and anterior anatomical structures using OCT images \cite{39,40,41}. The publicly available OCT dataset are at least one or two orders of magnitude less than the color fundus images and are more inhomogeneous \cite{42,43}. Thus, developing advanced DL models to improve the image quality and effectively extract and utilize the information contained in the OCT images becomes one of the most important and hot topics in the field.

OCTA images, which contain dynamical information of successive OCT scans for detection of ocular vasculature structures and microcirculation conditions, have shown great potential of monitoring and prognosing eye diseases \cite{1,2,3,4,6,7,8,9,10}. However, DL models have not been well established to analyze OCTA images so far, because there are more artifacts and noises, severe inhomogeneity and class imbalance problems in OCTA data, which impairs the robustness and generalization of OCTA-based DL models. A recent review published in 2021 by Hormel et al., \cite{44}  summarized the artificial intelligence (AI) techniques, including regression, machine learning  and deep learning models, developed for processing OCTA data. They have shown that many AI models achieved better performance in image enhancement, removing artifacts, and identifying disease-related phenotypes compared with traditional signal processing techniques. A few proof-of-concept clinical-relevant applications were also introduced, including identification and segmentation of non-perfusion and lesion areas, retinal and choroidal neovascularization, retinal fluid volume for DR, AMD diagnosis. In this study, we performed a systematic literature search of all peer-reviewed and accessible articles related to the DL models for denoising, enhancement, segmentation and volumetric reconstruction of OCTA images. The review includes articles published between January 1, 2020 and December 31, 2024 on PubMed, Scopus and Web of Science.  And articles that did not contain a novel DL model or a specific description of the DL model were excluded. We focus on reviewing the OCTA-based DL models from a technical perspective, specifically discussing the design principles, architectures and training strategies of the DL models. Although the performance of the DL models is highly dependent on the quality and homogeneity of the OCTA data, a general and initial comparison among these DL models is provided and the pros and cons of the models are analyzed. Furthermore, we reviewed OCTA dataset currently available for public. These dataset are valuable for training and testing DL models. Overall, this review can provide readers and new researches to better understand the basics of the OCTA-based DL models, their challenges, and how to overcome the challenges and possible future directions.

The rest of the paper is structured as follows: in section 2, we reviewed the DL techniques for denoising (section 2.1), segmentation (section 2.2) and volumetric reconstruction (section 2.3) of OCTA images. Section 3 introduces available OCTA datasets and the characteristics of the datasets. Section 4 discusses the applications and future directions of OCTA-based DL models in ophthalmology.

\section{Deep learning models for OCTA data}
\subsection{Image denoising and enhancement}
The quality of OCTA images is mainly influenced by the artifacts of bulk eye motion, shadowing, low signal strength, projection errors and measurement noises \cite{18,19,44}. The design of the DL models is dependent on the different characteristics of the artifacts. \\

\noindent(1) Bulk eye motion artifacts

Bulk eye motion causes stripes, bands with various brightness, discontinuity or duplication of vessels, as shown in Fig. 2a \cite{18}. Most commercial equipment and software have implemented algorithms to remove large bulk eye motion artifacts \cite{1}. However, automatic removal of artifacts cause by microsaccades and accurate reconstruction of microvascular architectures are still challenging, because the image background is complex and stripes and bands are irregular. DL methods for removal of bulk motion artifacts published in recent five years were searched using keywords including bulk eye motion artifacts, stripe removal, bulk motion subtraction, motion correction, denoising, deep learning, supervised learning, weakly-supervised learning, OCTA, optical coherence tomography angiography. The representative models are listed in Table 1 and their architectures are shown in Figure 2b-e.

\begin{figure*}[htp]
\centerline{\includegraphics[width=1.75\columnwidth]{fig2.png}}
\caption{Bulk eye motion artifacts and architectures of representative DL models for removing the artifacts and image enhancement. \textbf{(a)} Example images showing various types of bulk eye motion artifacts, including stretching (yellow), stripe (dark green), vessel discontinuity (blue), and band (green) \cite{18}. \textbf{(b)} Architecture of the CSD model, formulated based on optimizing a image decomposition cost function \cite{45}. \textbf{(c)} Architecture of the self-supervised CABR model \cite{47}. \textbf{(d)} Architecture of the supervised two-stage model \cite{48}. \textbf{(e)} Architecture of the self-supervised SOAD model \cite{52}. \textbf{(f)} Comparison of the DL model predictions. DL models include CSD \cite{45}, CINet-SegNet \cite{46}, CABR \cite{47}, Two-stage \cite{48}, DC GAN-UNet \cite{49}, StruNet \cite{50}, SOAD \cite{52}. Numbers in the parentheses are the DICE index values.}
\label{fig1}
\end{figure*}

\begin{table*}[htp]
\caption{DL models for denoising and enhancement of OCTA images.}
\label{table}
\centering
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{1.8}
\begin{tabular}{|m{40pt}<{\centering}|m{25pt}<{\centering}|m{65pt}<{\centering}|m{240pt}<{}|m{120pt}<{}|}
\hline
Topic & 
Year &
DL model & 
\hspace{90pt}Model architecture & \hspace{50pt}Dataset \\
\hline
%Removal of stripes from bulk eye motion &  2020 & CSD \cite{45} & 
%\shortstack{\\Formulate as an \textbf{image decomposition problem} as: \\ $\min \sum_i \big( 0.5||C_i+S_i-I_i||^2_F + \tau R(C_i) +\lambda R(S_i) \big)$. \\ Clean image slices $C_i$ are leveraged by anisotropic total variation prior. \\Strip noises $S_i$ are low-rank and different among different layers. \\ $R(*)$ are low-rank constraints for $S_i$ and $C_i$.} & \shortstack{340 real \textit{en face} images from 85 eyes, \\ 340 synthesize images generated by \\ conditional adversarial network \\ and style transfer network.}\\ 
%& & CUD model \cite{45} & Same image decomposition problem as CUD model, but strip components of different layers $S_i$ are low-rank (not identical). & Same as CSD model. \\

%& 2021 & DL model \cite{46} & CINet (residual CNN) is applied to remove motion artifacts in B-scan images, SegNet (UNet) is applied to segment broken vasculature, and ComNet (encoder-decoder) is used to restore vascular connections. &800 B-scan images with manually \\ annotated ground truths.} \\

Removal of stripes from bulk eye motion & 2022 & CABR (\textbf{self-supervised})\cite{47} & Defective masks generated by optimally oriented flux, gradient statistics and appearance feature are input into the model (\textbf{encoder-decoder} structure, gated convolution layers) .  Motion artifacts free areas are used as acceptable training masks for self-supervised training. & 45 clean \textit{en face} images, and noisy images are synthesized by an adaptive algorithm.\\ 

& 2023 & Two-stage model (\textbf{supervised}) \cite{48} &  SR-Net (an \textbf{encoder-decoder} structure followed by  a \textbf{UNet} structure) to decompose OCTA image into stripe noise and clean image. \textbf{PS-GAN} incorporates cyclic perceptual loss and structure loss for enhancement of vascular structures.&(Private) PUTH (210 \textit{en face} images, manually added stripe noise), AS-OCTA  (31 conjunctival images).  (Public) ROSE (manually added stripe noise). \\ 

%& 2023 & DL model for AS-OCTA (supervised) \cite{49} & DC-GAN for data augmentation. UNet with residual modules for vasculature segmentation and Ostu thresholding for improving image contrast. & \shortstack{\\   \\  \\Private dataset \\ 30 clean images with sufficient \\ quality to support clinical analysis,\\ 1000 images with stripe noises.} \\

%& 2023 & StruNet (supervised) \cite{50} & Three primary encoder-decoder layers, which assemble residual block and Swin Transformer modules in parallel connection, to effectively capture noise distribution and achieve detail preservation.  & \shortstack{\\ \\ Private dataset \\180 images, manually added \\ stripe noises.} \\

%& 2024 & DL model \cite{51} & A sliding window correlation-based adjacent position image fusion method applied for three consecutive B-scan images. UNet integrated with convolutional block attention module and channel-spatial mixed attention module for converting structural OCT images to OCTA images. & \shortstack{\\ \\ Private dataset \\ 31 OCTA volume data, \\where each contains 400 B-scans \\of different slow-axis positions.}\\

& 2024 & SOAD for OCT B-scans (self-supervised) \cite{52} & An \textbf{encoder-decoder} structure which takes a blinded \textbf{B-scan slice} and its neighboring image slices as input, and infers a noise-free image based on the inter-B-scan continuity. The non-local block and weighted loss function are integrated in the model to enhance vessel features. &7 OCT volumes, each contains 400-500 positions, 500-625 consecutive B scans.\\

\hline

Detection and removal of shadows & 2020 & EnhVess, SegVess, ConVess, and vessel graphing (supervised) \cite{53} & EnhVess: \textbf{encoder-decoder}, three layers. SegVess: \textbf{ResNet}, four hidden layers and one skip connection. ConVess: encoder-decoder, three layers. Vessel graphing: algorithms of skeletonization, voxel removal, and improving connectivity. &5 OCT datasets, each contains 230 slices in depth direction, $200 \times 200 $ voxel of an \textit{en face} OCTA slice. \\

& 2021 & DL model (\textbf{supervised})\cite{54,55} & Inner retinal thickness map and OCT reflectance image fed to one subnet (\textbf{UNet-ResNet}) to segment shadow effect, OCTA \textit{en face} image fed to another subnet  to segment capillary features. All segmented features fed to 3 parallel subnets to distinguish shadows and non-perfusion areas. & 1092 \textit{en face} OCTA images from 30 health persons and 162 DR patients. \\

& 2024 & Attenuation physics-based DL model (supervised) \cite{56} & Net\textsubscript{spA} (\textbf{residual CNN}, \textbf{SAM block} and \textbf{SARB block}) segments shadow area. Net\textsubscript{sp} (\textbf{DNN}) relights pixels in the shadow region. Net\textsubscript{M} (DNN) refines the shadow compensation, because large retinal vessels are less influenced by the shadow. Net\textsubscript{I} (residual CNN) reconstructs the shadow-free image. & 14 \textit{en face} OCTA images ($500 \times 500$ voxel) with real shadow effect, 43  \textit{en face} images without shadows. \\
\hline

Denoising and reconstruction & 2020 & Four DL models for OCT B-scans (\textbf{supervised}) \cite{60}& (1) line-shaped \textbf{DnCNN}, 20 conv layers, (2) \textbf{UNet}, 5 down-sampling and 5 up-sampling conv layers, (3) \textbf{RDN}, 2 conv layers, 20 RDBs, and 3 conv layers, (4) \textbf{Pix2Pix GAN}, UNet model as generator, 5 conv layer patch GAN as discriminator. & 6 OCT volumes. (300 locations, 48 consecutive B-scans).  \\

& 2020 & HARNet for \textit{en face} OCTA (supervised) \cite{61}& A low-level feature extraction layer (1 conv), a high-level feature extraction layer (4 \textbf{ conv blocks with skip connections}, each block has 20 conv layers), and a residual layer. The cropped central $6 \times 6 \,mm$ OCTA as input, and the cropped $3 \times 3 \,mm$ OCTA as ground truth. & $6 \times 6$ and $3 \times 3 \,mm$ \textit{en face} OCTA images of 10 healthy eyes and 288 DR eyes ($304 \times 304$ voxel). \\

%& 2021 & DCARnet for \textit{en face} images (supervised) \cite{62}& A shallow feature extraction layer (three bypasses containing 1 conv, 1 conv and 2 conv layers), deep feature extraction layer (three bypasses containing 21 conv, 16 conv-1 deconv, 12 conv-2 deconv layers), and a reconstruction layer. & \shortstack{\\ \\ \textbf{Private dataset} \\ 274 paired $6mm \times 6mm$ and \\ $3mm \times 3mm$ OCTA of superficial \\ and deep capillary plexuses.}  \\

& 2021 & DL model using N2N training strategy (\textbf{weakly supervised}) \cite{63}& \textbf{UNet} model (encoder: five down-sampling stages, decoder: five up-sampling stages, and skip connections between the symmetric layers). \textbf{N2N training strategy}: 24 consecutive B scans as multichannel inputs, another 24 B scans for generating noisy labels using SSAPGA algorithm. & 6 OCT volumes of animal brains (300 locations, 48 consecutive B-scans). 12 volumes of human eyes (199 locations, 4 consecutive B-scans).  \\

& 2022 & SAR-GAN (supervised) \cite{64}& \textbf{GAN} model (generator: 1 conv, 8 residual blocks, 1 conv. discriminator: 8 conv layers.). Perceptual loss function (content loss and weighted adversarial loss) for generator network. & $9 \times 9$, $6 \times 6$ and $3 \times 3 \,mm$ \textit{en face} OCTA images of 55 eyes ($320 \times 320$ or $480 \times 480$ voxel).  \\

& 2022 & NI-P3D-U (supervised or weakly supervised)\cite{65}& \textbf{Encoder-decoder} model. Input: \textbf{fused neighborhood information} (local position and its two adjacent neighborhoods in consecutive B-scans). Encoder: five down-sampling stage, two \textbf{psuedo-3D units} for each stage. Decoder: five up-sampling stage, 1 deconv and 2 conv for each stage. & 4 data volumes (300 locations, 48 consecutive B-scans).  \\
\hline

Cross-device OCTA generation & 2024 & PMFAN (unsupervised)\cite{67} & \textbf{Unsupervised domain adaption GAN} model. Two context-enhanced encoders and two domain-specific generators to extract features. A domain-invariant generator for reconstruct OCTA images. Two domain-specific discriminators for adversarial learning. Source domain: OCT and corresponding OCTA images. Target domain: OCT acquired using different equipment/protocol. & 300 OCT volumes and corresponding OCTA from Optovue. 230 OCT volumes from Cirrus.\\
\hline

Quality assessment & 2024 & ViT model (supervised)\cite{68} & \textbf{ViT} pretrained using $6 \times 6 \, mm$ OCTA \textit{en face} images, fine tuned on $12 \time 12 \, mm$ OCTA images. Results compared with ResNet18, ResNet34 and ResNet50 models. & 1103 \textit{en face} OCTA images in DRAC2022 dataset.\\
\hline

\end{tabular}
\hspace{15pt} \\
\textbf{Note:} PS-GAN=Perceptual structure generative adversarial network. ResNet=Residual network. SAM=Spatial attenuation block. SARB=Spatial attentive residual block. DNN=Deep neural network. DR=Diabetic retinopathy. Conv=Convolution. RDN=Residual dense network. RDB=Residual dense block. N2N=Noise-to-noise training strategy. ViT=Vision transformer. Most dataset are private except for the ROSE.
\end{table*}

These DL models can be classified into three types. Most DL models not only remove stripes and bands from bulk motion, but also deals with speckle noise and background noise. Specifically, the first type of model decomposes an \textit{en face} OCTA image as a clean image and an image of additive noise, by optimizing the distribution priors of stripe noise across \textit{en face} OCTA images at different depths \cite{45}. For example, the CSD model proposed by Wu et al. \cite{45} formulated an optimization objective function as $\min \sum_{i=1}^q \big( \frac{1}{2}|| C_i+S_i-I_i||_F^2 + \tau R(C_i) + \lambda R(S_i) \big)$, where $I_i$ is the measurement image $i$ ($i=1,2,...,q$), $C_i$ is the noise-free image, and $S_i$ is the image containing additive noise. $R(C_i)$ and $R(S_i)$ are constraints that enforce noises to be either uniformly distributed or accumulated at a few locations in the 3D space. And $q$ is the total number of \textit{en face} images in the depth direction. Based on the robust orthonormal subspace learning method, the additive noises can be removed from the superficial vascular plexus (SVP) image and deep vascular plexus (DVP) image (Fig. 2b).

The second type of DL model includes several simple DL models such as the Autoencoder (encoder-decoder), Transformer, UNet and GAN, where the \textit{en face} OCTA images are processed sequentially by the simple models (Fig. 2c and d). The second type of DL models are not specifically designed for OCTA images, but rather suitable for all kinds of image types including OCT, CT and MRI  \cite{47,48,49,50}. Among them, the supervised models are trained based on the noise-free images selected by clinicians, verified using the synthetic images by manually adding noises, and then applied to process the noisy images collected in hospitals and laboratories \cite{48,49,50}. The self-supervised models use noisy-free areas in the same images or use the average of several OCTA images as the ground truths for model training \cite{47}. For example, the self-supervised CABR model (Fig. 2c, first row in Table 1) has a basic encoder-decoder structure with gated convolution layers. The encoder takes the defective mask, gradient statistics and artifacts-affected image (appearance feature) as the input and predicts the noise-free image. During the self-supervised model training process, the output from the decoder is compared with the artifacts-free area in the input image, though these two comparison images locate at different places in the input image and their vascular structures are not exactly the same \cite{47}. The supervised two-stage model (Fig. 2d, second row in Table 1) uses a standard UNet model to remove strip noises and uses a PS-GAN model for image enhancement. The standard UNet model is trained using noisy-free images as ground truths and images synthesized by manually adding noises to the clean images as the model input \cite{48}.

The third type of model removes artifacts and noises in the volume of B-scan images and reconstructs the high-quality \textit{en face} images \cite{46,51,52}. Removal of artifacts and noises in the consecutive B-scan slices relies on the assumption that vascular structures are spatially continuous, noises are uncorrelated and stripe areas have different brightness \cite{51,52}. For example, as shown in Fig. 2e and the third row in Table 1, Li et al. \cite{52} proposed an encoder-decoder structure, the SOAD model, which takes a blind B-scan slice masked black and its neighboring B-scan slices as the input and predicts the artifacts-free information contained in the blind B-scan slice based on the inter-B-scan continuity. The training of the encoder-decoder network is self-supervised using the original corrupted B-scans. 

A performance comparison of the three types of representative DL models is shown in Fig. 2f. All the DL models can effectively remove bulk motion artifacts while preserving the correct structure of large vessels. The reconstruction and enhancement of small vessels are influenced by the model architecture and the quality of the dataset. It should also be noted that the bulk motion artifacts in works \cite{47,48,49,50} are not real artifacts existed in the experimental measurements, but rather artificial noises added to the clean OCTA images. The artificial noises are simple and easy to be removed. Nevertheless, the denoised OCTA images provided in works \cite{46,47,50,52} show better vessel connectivity and clearer vessel boundaries. Quantitative evaluation indexes, such as peak signal-to-noise ratio (PSNR), mean-to-standard deviation ratio (MSR), DICE index and structural similarity (SSIM) are provided in these works. For example, the DICE index of different DL models (Fig. 2f) are compared because it is provided in most of these works  \cite{45,46,47,48,50,52}. Though the values of DICE index could be influenced by the dataset, the results show that the SOAD model has the largest DICE index. Thus, both denoised OCTA images and DICE index suggest that the SOAD model designed by utilizing the different characteristics of vasculature and noises in the B-scan images achieve the best performance. \\

\noindent(2) Shadow artifacts

Shadow artifacts are generated due to scan beam defocus, projection attenuation and local blockage. For example, large superficial vessels, vitreous floaters, edema or occlusion of light by pupil boundary change light attenuation and cause shadow artifacts. The incorrect image intensity of local shadows can be wrong interpreted as loss of vascular density or misclassified as non-perfusion area. Detection and removal of shadow artifacts proposed in most previous works are usually based on the B-scan images, where shadow artifacts appear as a tail beneath the vessel or other light blockages (Fig. 3a). In practice, the real signal of the vessels in the B-scan images is assumed to have a larger signal strength compared with  signal strength in the beneath shadow areas. So the beneath artifact signals can be removed by applying proper signal thresholds. In some recent works, the continuity of vessels and randomness of artifacts in the adjacent B-scan images are used to refine vessel areas \cite{57}. Furthermore, in the successive B-scans, the fast decorrelation time and clear phase change in the vessel regions can also be used for removal of irrelevant signals \cite{58,59}.

\begin{figure*}[htp]
\centerline{\includegraphics[width=1.85\columnwidth]{fig3.png}}
\caption{Shadow artifacts and DL models for artifact removal and reconstruction of shadow-free images. \textbf{(a)} Examples of two common shadow artifacts. (1) Large superficial vessels affects signal of deeper vessels, which appears as tails in the B-scan images and duplication of superficial vascular patterns in the \textit{en face} images \cite{59}. (2) Vitreous opacity causes light blockage and generates shadow artifacts in the \textit{en face} and B-scan images. \textbf{(b)} Architecture of the DL pipeline, which contains EnhVess model for removing tail artifacts and image enhancement, SegVess for segmentation of vessels, ConVess for gap correction \cite{53}. \textbf{(c)} Architecture of the DL model containing five UNet-ResNet subnetworks for segmentation of shadow areas \cite{54,55}. Examples images of the comparison between segmented nonperfusion areas and shadow areas with the annotated ground truth for diabetic retinopathy (DR) eye and health eye \cite{54}.\textbf{(d)} Architecture of the attenuation physics based DL model for removal of shadow areas and reconstruction of high-quality images \cite{56}.}
\label{fig1}
\end{figure*}

Besides the traditional signal processing techniques, we found there were two types of DL models for identifying and removal of shadow artifacts, listed in Table 1 \cite{53,54,55,56}. But these DL models require manually and correctly annotating the artifact regions, which are their major limitations compared with the thresholding methods using B-scan images. Specifically, Stephan et al. \cite{53} developed a DL pipeline for removing tail artifacts in B-scan images, image enhancement, and segmentation and reconstruction of vascular networks. The pipeline contains simple DL models, including the encoder-decoder network and the residual network with a few convolution layers (Fig. 3b). Manual annotations of the real vessel signals in the B-scan images are used as the ground truths for model training. Their results showed that even the three-layer autoencoder network can successfully remove the tail artifacts, and the mean square error (MSE) compared with the annotated ground truth is only around 0.013. Guo et al. \cite{54,55} developed a UNet-ResNet architecture for processing the \textit{en face} OCTA images and distinguishing shadow artifacts and non-perfusion areas in DR eyes. In the model, the inner retinal thickness map, OCT reflectance image, and \textit{en face} OCTA image were fed to the five subnets sequentially (Fig. 3c). The agreement between the correctly predicted pixels compared with manual delineation (F1 score) is higher than 80\%, and is not statistically influenced by DR severity and scan quality \cite{54}. Another type of model is the attenuation physics based DL model. For example, Li et al. \cite{56} proposed to use the residual CNN and the spatial attenuation block (SAM) to extract the attention map, use three spatial attentive residual block (SARB) to remove vessel streaks, and apply morphological algorithms to remove penumbral pixels to get the segmented shadow regions. Next, to reconstruct the shadow-free image, shadow decomposition method is applied which assumes the shadow-free image is a pixel-by-pixel composition of the relit image and the shadow image. And the relit image is generated using the linear attenuation physics model (Fig. 3d). Although the network structure is complicated in \cite{56}, it still requires manual annotations of shadow areas for model training. Thus, the authors use synthetic images by manually adding shadow artifacts to the clean images to demonstrate the performance of the model.\\

\noindent(3) Noise removal and image reconstruction

The cross-sectional OCT images are generated by the light beam scanning in the transverse direction (fast axis). A stack of B-scans (i.e., OCT volume) are acquired sequentially in the slow axis direction (Fig. 4a). At each position along the slow axis, successive B-scan images are repeatedly collected at multiple times. Registration and correlation analysis are performed based on the consecutive B-scans to extract flow signals and generate the cross-sectional OCTA images (Fig. 4b). The inherent noise inside the OCT B-scans comes from the equipment and measurement protocols, which is hard to be separated from the true signals. Acquiring a large number of consecutive B-scans at the same location and extract the flow signals using both the amplitude and phase based correlation analysis approach, such as SSAPGA, could help reduce the noise and blurry in the reconstructed images. However, it would require greatly increased scanning time and higher scanning speed. In the laboratories, more than 48 consecutive B-scans can be acquired using animal brain tissues \cite{68}. But in the eye clinics, the commercialized OCTA equipment usually allows no more than 8 consecutive B-scans of the human eyes. 

To address these challenges, DL models have been proposed to reconstruct OCTA images from OCT data volumes, with improved noise removal rate and enhanced image quality. Representative models are listed in Table 1. The widely-used DL models include deep convolution network (DCNN), UNet, ResNet and GAN (Fig. 4c). These models take the consecutive B-scan images as the training data input. The training labels (ground truths) are generated based on the SSAPGA algorithm using a few consecutive B-scan images (Fig. 4c) \cite{60}. However, the DL model performance is influenced by the quality of  the training labels from the SSAPGA algorithm, which depends on the number of consecutive B-scans used. Thus, considering the fact that training labels are noisy, Jiang et al. \cite{63}, proposed to use the noise-to-noise (N2N) training strategy, a type of weakly supervised training strategy, for the DL models (Fig. 4d). Specifically, half of the consecutive OCT B-scan images are selected to generate noisy training labels using the SSAPGA algorithm, and the rest of the images are fed into the network as the training data. They provided both theoretical and experimental results showing that the trained UNet model is able to reproduce the approximated performance of the corresponding supervised network using accurate labels. Furthermore, Jiang et al. \cite{65} developed a NI-P3D-U model which considers not only the temporal correlation features but also the spatial vascular connectivity along the slow axis in the OCT B-scan images (Fig. 4e). Volumetric B-scan data in the three adjacent positions, where each position consists of 48 consecutive B-scans, are processed by the Pseudo-3D units in the DL model. The $3 \times 1 \times 1$ and $1 \times 3 \times 3 $ kernels are applied to extract the temporal and spatial features respectively. Le et al. \cite{66} comprehensively investigated what is the optimal number of neighboring and consecutive B-scan images, and what evaluation metrics should be used in the DL model. The DL model contains the EfficientNetB0 as the encoder and CNN blocks as the decoder. Results showed that B-scan volumes in the three adjacent positions and the SSIM evaluation metrics used in the encode-decoder model achieves the best OCTA image quality for healthy eyes and proliferative DR eyes.

A few other studies \cite{61,62,64} acquired $6 \times  6$ and $3 \times 3 \,mm$ OCTA images using the same scanning parameters, including the number of A-lines and axial/raster positions, and the number of consecutive B-scans at each position. The resolution of $6 \times 6 mm$ OCTA scans is lower as it covers a larger tissue region. The high-resolution $3 \times 3 mm$ OCTA images are used as the training labels. The low-resolution $6 \times 6 mm$ OCTA images are first transformed (i.e., translation, rotation and scaling), and then the overlapping regions in the $6 \times 6 mm$ images compared with the labels are cropped and used as the training data. Several types of DL models, including DCNN \cite{61}, UNet \cite{62} and GAN \cite{64}, are trained to predict the high resolution and full-size $6 \times 6 mm$ OCTA images. \\

\begin{figure*}[htp]
\centerline{\includegraphics[width=1.75\columnwidth]{fig4.png}}
\caption{Noise removal and image reconstruction. \textbf{(a)} Schematic illustrations of the B-scan in the fast and slow axis, and the acquisition of the \textit{en face} image.  \textbf{(b)} OCTA (flow) signals are calculated based on the consecutive B-scans using correlation analysis algorithm such as SSAPGA. \textbf{(c)} Four supervised DL models (i.e., DnCNN, UNet, RDN, Pix2Pix GAN) for reconstruction of OCTA B-scan images, using consecutive OCT B-scans as the training data and OCTA B-scan image from SSAPGA as the labels \cite{60}. \textbf{(d)} The UNet model and the N2N weakly supervised training strategy for OCTA image reconstruction \cite{63}. The N2N training strategy relies on the noisy labels generated by SSAPGA algorithm using only a part of the B-scan image data. \textbf{(e)} The NI-P3D-U model, which has a UNet shape of Pseudo-3D units and Conv2d units \cite{65}. The Psuedo-3D units use $3 \times 1 \times 1$ kernel and $1 \times 3 \times 3$ kernel to extract both spatial and temporal features. \textbf{(f)} The HARNet model which has a deep convolution network structure \cite{61}. The $6 \times 6 mm$ low resolution OCTA images are the model input, and the $3 \times 3-mm$ high resolution OCTA image are used as labels for model training.}
\label{fig4}
\end{figure*}

\noindent(4) Inter device variation 

Previous studies have observed the inconsistency in the OCTA information obtained using different equipment and protocols \cite{69}. For example, the mean numbers of microaneurysms of DR eyes in the OCTA images acquired using five different machines range from 8-16. The vascular density, area and fractal dimensions also have significant difference. The inter-device discrepancies limit and comparability and generalizability of OCTA data and could influence the clinical diagnosis accuracy. Recently, Huang et al. \cite{67} (Table 1) developed PMFAN, an unsupervised domain adaption GAN model, to reconstruct OCTA images from OCT data volumes using an invariant neural network generator. The OCT volumes acquired using different machines are firstly mapped to the shared-feature space using domain-specific encoders. Then, the invariant generator, which is trained using cycle-consistence loss from domain-specific discriminators, generates OCTA images using the multi-scale features. Results in their work \cite{67} showed that the PMFAN model can not only effectively eliminate the inter-device discrepancies of large and medium vessels in the OCTA images, but also predict \textit{en face} OCTA images using only the spatial OCT signals.  \\

\noindent(5) Quality assessment

Lastly, DL models are applied to assess the quality of the \textit{en face} OCTA images \cite{68, 70}. ResNet and ViT, the widely-used DL models for image classification, are suitable models for identify high and low quality images. Both ResNet152 and ViT achieve AUC (area under the ROC curve) scores higher than 0.9. The ViT model\cite{68} performs slightly better than the ResNet models of different numbers of convolution layers\cite{70}. 

\subsection{Segmentation}
\noindent(1) Segmentation of vascular networks 

The mass transport and supply to the ocular fundus is mainly supported by the ophthalmic arteries, capillaries and veins. The precise organization of these vascular structures ensures healthy optical and neuronal functions. For example, there are four morphologically-varied retinal vascular plexuses that deliver oxygen and nutrients to the nerve fiber layer, retinal ganglion cell layer and deeper retinal tissues. Vascular changes, such as the development of microaneurysms and reduction of vessel density in both superficial and deeper plexuses, and neovascularization of the retina, are important clinical features of many fundus diseases such as diabetic retinopathy (DR), age-related macular degeneration (AMD) and glaucoma \cite{1}. Thus, segmentation of vascular networks and identification of the abnormal vessel morphology based on the OCTA images is usually one of the most critical steps in monitoring eye condition and diagnosing fundus diseases.

In recent five years, there are more than thirty DL models published in the well-recognized journals, related to the segmentation of superficial and deeper vasculature in the retina and choriod based on the structural OCT and OCTA images. The earlier works at around 2020 are supervised methods using simple CNN, UNet, ResNet architectures \cite{71,72,73}. These models take the \textit{en face} OCTA images and pixel-wise delineation of vessels from experts as input, and predict the binary segmentation results. For example, Giarratano et al. \cite{71} (first row in Table 2) showed that CNN of three convolution layers, UNet of two symmetric convolution modules, and CS-Net of four symmetric convolution layers and a spatial attention module achieve better segmentation performance compared with traditional filter-based thresholding methods. Although the network structures are relatively simple and few convolution layers are used in these basic DL models, segmentation of both large retinal vessels and microvasculature can be achieved in a supervised manner \cite{71}. 

To improve the global perception and reduce the redundant features generated by the convolution layers, transformer mechanisms such as dynamic token aggregation transformer (DTAT) \cite{74} and cross-fusion transformer (CFT) \cite{75} are applied. Fig. 5a shows the architecture of the OCT\textsuperscript{2}Former model \cite{74} (second row in Table 2) which integrates DTAT and group embedding module into a UNet model to extract rich global information while reducing computational complexity. The evaluation indexes such as DICE coefficient, Jaccard index, etc. of the OCT\textsuperscript{2}Former model are better than other nine convolution models (UNet, CS-Net, etc.) and two hybrid transformer models (TranUNet, UTNet). Li et al. \cite{76} further proposed a 3D UNet model of three symmetric convolution layers and unidirectional pooling operators, and demonstrated its ability to extract the 2D projection of vessel structures directly from 3D structural OCT volumes, after the model being trained by manual delineations of vessels.

To improve the segmentation accuracy at multiple length scales, from global structures to finer micro-vessels, several previous studies designed dual-branch DL models where one branch focuses on extracting large vessels such as arteries and veins (coarse stage) and another branch refines the continuous details of small vessels (fine stage) \cite{77,78,79,80}. For example, in the OCTA-NET model (Fig. 5b, and third row in Table 2), Ma et al. \cite{77} designed a shared encoder and two decoders to produce preliminary confidence maps of the pixel-wise segmentation of large vessels and center line-wise segmentation of micro vascular structures in the coarse stage. In the fine stage, segmentation is refined using four convolution layers to recover continuous details of micro-vessels. Furthermore, Zhao et al. \cite{81} proposed to use multi-modal information from OCT/OCTA volumetric data and their 2D intensity projection maps to improve the segmentation performance based on the mutual learning framework. In their work, a 3D learning network branch is designed to extract deep local features from OCT/OCTA volumetric data and a parallel 2D learning network branch is applied to the intensity projection maps. All these features are aligned and combined for segmentation. Networks in the two branches are trained using the local mutual learning strategy based on the mimicry loss and retrained using the global mutual learning to overcome the patch effect and improve the global perception. Results showed that the algorithm susceptibility to the background noise has been reduced and the completeness and connectivity of the segmented vessels are greatly improved by including the 3D structural information from volumetric data \cite{81}.

Manually delineation of vessels, especially for the pixel-level annotation of the densely-packed and meandering micro-vasculature in the deep vascular plexus (DVP) and choriod, are really labor intensive and suffers from contrast-related artifacts. Thus, several recent studies developed semi-supervised and weakly-supervised DL models, which use only a limited amount of training data with labels or use scribble annotations for model training, but still achieve good segmentation quality \cite{82,83,84,85}. For example, Liu et al. \cite{82} designed a modified conditional variation autoencoder (CVAE) architecture (fourth row in Table 2), which uses a small amount of labeled training data to disentangle vessel structures, and uses paired, registered but unlabeled OCTA images to capture local noise and artifacts. As shown in Fig. 5c, the model consists of a supervised training path and a CVAE path. In the supervised path, the conditional network (CN) and the segmenter are trained using a small amount of data with delineation labels. The output feature map of CN is used as the condition in the CVAE path. Paired but unlabeled OCTA images taken at the same ocular position in multiple consecutive OCTA scans are inputted into the CVAE for segmentation, based on the fact that OCTA pairs have similar anatomy but different artifacts and independent noise. Chinkamol et al. \cite{83} proposed to use unpaired but expert-labeled dataset, in which the vascular shape (for example, the distribution of the vessel diameters) is similar to the target OCTA images, to make the segmenter learn a shape prior. Based on the concept of domain transfer using adversarial training mechanism and simple scribble annotations, the segmenter can be trained to produce correct vessel segmentation of the target data. Furthermore, assuming the outputs of each segmenter layer contain similar anatomical information of the input images, the model incorporates a self-supervision type loss in its objective function, to improve the segmentation consistency and accuracy (fifth row in Table 2).

Three recent studies achieves unsupervised segmentation of multi-scale vasculature in the OCTA images, two of which are based on the angiogenesis physics \cite{86,87} and the other uses a self-synthesized modality named local intensity fusion (LIF) \cite{88}. In the physiology mechanism-based method developed by Kreitner et al. \cite{86}, synthetic OCTA images with vessel labels are generated using a fast angiogenesis simulation and transfer learning, then a segmentation DL model (nnUNet) is trained using the synthetic images and its performance is verified using the real OCTA images (sixth row in Table 2). Specifically, the fractal topology of the vascular network is simulated by considering the oxygen-driven vessel sprouting, elongation and bifurcation dynamics. A cycleGAN-type framework is used to transform synthetic images to match the distribution and style of real OCTA images (Fig. 5d). Hu et al. \cite{88} proposed to use local intensity fusion (LIF) of 3D OCTA volume to generate one modality of \textit{en face} image with enhanced contrast of vessel structures, and use a variation autoencoder (VAE) to extract common features of an OCTA volume and its corresponding LIF and remove phantom vessels cause by incorrect projection. After binarization of the common feature space, 2D segmentation of retinal vessels can be achieved, which has better structural connectivity and less speckle noise/artifacts compared with several thresholding and filter based methods.\\

\begin{table*}[htp]
\caption{DL models for retinal vessel segmentation.}
\label{table}
\centering
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{1.8}
\begin{tabular}{|m{40pt}<{\centering}|m{25pt}<{\centering}|m{65pt}<{\centering}|m{240pt}<{}|m{120pt}<{}|}
\hline
Topic & 
Year &
DL model & 
\hspace{90pt}Model architecture & \hspace{50pt}Dataset \\
\hline
Vasculature, \textbf{supervised} &  2020 & Basic DL models \cite{71} & (1) \textbf{CNN} (three conv layers), (2) \textbf{UNet} (encoder and decoder each contains two conv layers), and (3) \textbf{CS-Net} (UNet structure which has the spatial and channel attenuation module between the encoder and decoder paths) for pixel-wise classification. Adaptive thresholding for binarization. & (Private) 55 OCTA images in $3 \times 3\, mm$ and $6 \times 6 \, mm$ fields of view.\\ 

&  2023 & OCT\textsuperscript{2}Former (transfomer-based) \cite{74} & \textbf{UNet} structure. Encoder: 3 dynamic transformer encoder modules (consists of layer normalization, MDTAA and MLP) and group embedding modules. Lightweight decoder: 2 convolution layers and a $1 \times 1$ convolution output. & (Public) OCTA-SS, ROSE-1, OCTA-500. \\ 

& 2020 & OCTA-Net (dual-branch) \cite{77} & \textbf{Coarse stage} produces preliminary confidence maps of pixel-level and center line-level vessels.  Pixel-level branch: symmetric \textbf{encoder} and \textbf{decoder} layers, each contains 5 \textbf{ResNeSt} blocks. Center line-level branch: 3 ResNeSt blocks followed by an upsampling layer. \textbf{Fine stage} recovers continuous details of small vessels. 3 conv layers to refine pixel-level map, 1 additional conv layer to refine centerline-level map. & (Public) Two dataset (ROSE 1, ROSE 2). ROSE 1: 117 OCTA images of 26 AD and 13 healthy. ROSE 2: 112 OCTA images of eyes with various macula diseases. Center-line level and pixel-level annotations. \\
\hline

Vasculature, \textbf{semi-supervised} & 2020 & ACRROSS using labeled public data\cite{82} & \textbf{CAVE} framework which consists of an \textbf{encoder}, a \textbf{decoder}, a \textbf{conditioning network} (CN) and a \textbf{segmenter}. Encoder: 5 conv blocks. Decoder: 5 conv blocks for latent features from the encoder and 4 additional conv blocks for outputs from the CN. CN: UNet or CS-Net, which takes the registered paired OCTA image as input and the output preserves the spatial dimension of the input. Segmenter: 2 conv layers. & (Private) 138 registered paired OCTA scans (22 scans with manual delineation). (Public) OCTA scans and delineation in OCTA-500, ROSE-1 and OCTAGAN.\\

& 2023 & OCTAve, SSDS training using scribbles \cite{83} & \textbf{GAN} structure which contains a \textbf{UNet segmentor}, \textbf{AAG-based discriminator} and a \textbf{UNet-based SSDS block} to optimize the balance between weakly-supervised objective and adversarial training. Discriminate the multi-scale attention maps of segmentor predictions and \textbf{scribble annotations} in adversarial training. Discriminate the attention maps of segmentor predictions and \textbf{ground truths of unpaired dataset} in SSDS block. & (Public) OCTA-500, ROSE.\\
\hline

Vasculature, \textbf{unsupervised}& 2024 & Angiogenesis mechanism-based DL model \cite{86} & Synthetic vessel maps generated using \textbf{angiogenesis mechanism} (considering oxygen circulation driven sprouting, elongation and bifurcation of vessel branches). Vessel maps integrated with background noise and local brightness and contrast adjustments using \textbf{adversarially tuned transformation}. \textbf{CycleGAN} type model to transform synthetic maps to match the distribution and style of real OCTA images. \textbf{nnUNet} trained using synthetic images and applied to real OCTA using \textbf{transfer learning}.  & (Public) OCTA-500, ROSE-1, Giarratano \textit{et al}.\\
\hline

FAZ and RV (multi-task, supervised) & 2022 & Joint-Seg (supervised) \cite{89} & \textbf{Single encoder} and \textbf{dual decoder} architecture. Joint encoder: 5 conv blocks with skip connections to the decoder. A \textbf{feature selection module} between encoder and decoder. Decoder for FAZ: 4 FADB blocks (each includes a feature map alignment, a deformable conv and two conv blocks). Decoder for RV: 3 parallel branches which contain dilated conv and SAM blocks, and a multiscale feature fusion strategy. & (Public) OCTA-500, ROSE, OCTAGAN, sFAZ, and OCTA-25K-IQA-SEG.\\
\hline

Artery-vein classification &  2023 & AV-Net (supervised) \cite{95} & \textbf{UNet} structure which takes OCT and OCTA \textit{en face} images as the inputs. Encoder: 4 dense conv blocks and 1 transition block. Decoder: 4 \textbf{ResNet} blocks. AV-Net is pre-trained using ImageNet data, and uses \textbf{transfer learning} to learn complex features of AV. & (Private) 50 OCT/OCTA \textit{en face} images in the $6 \times 6 \, mm$ field of view (30 DR eyes, 20 healthy). \\ 
\hline

\hline
\end{tabular}
\hspace{15pt} \\
\textbf{Note:} Conv=Convolution. CAVE=Conditional variational autoencoder. FAZ=Foveal avascular zone. SSDS=Self-supervised deep supervision. RV =Retinal vessels. SAM=Spatial attention module. MDTAA=Multi-head dynamic token aggregation attention. MLP=Multi-layer perception. FADB=Feature alignment decoding block.
\end{table*}

\begin{figure*}[htp]
\centerline{\includegraphics[width=1.85\columnwidth]{fig5.png}}
\caption{\textbf{Representative DL models for segmentation of vascular networks.} \textbf{(a)} OCT\textsuperscript{2}Former, which is a UNet structure containing dynamic transformer and group embedding in the encoder and convolution layers in the decoder \cite{74}. \textbf{(b)} OCTA-Net, which contains two branches for extracting features of large vessels and micro-vasculature \cite{77}. \textbf{(c)} ACRROSS, a semi-supervised model that uses a limited amount of OCTA data with delineations as the condition to train a variation autoencoder-type model \cite{82}. \textbf{(d)} Angiogenesis mechanism-based DL model, which uses statistical angiogensis simulation and transfer learning to generate realistic synthetic OCTA images as the training data \cite{86}.}
\label{fig5}
\end{figure*}

\noindent(2) Joint segmentation of retinal vessels and FAZ

The foveal avascular zone (FAZ) is a vessel-free zone in the center of the retina, which is a clear and unobstructed space for light transmission. The abnormal shape and size of  FAZ are correlated to fundus diseases such as DR and DME. Since the shape of FAZ is mainly determined by vascular networks, joint segmentation of retinal vessels (RV) and FAZ can be incorporated into one DL framework \cite{82,89,90,91,92}. For example, the Joint-Seg model uses a single encoder to extract common feature maps of vasculature and non-vascularized space at five levels, and designs two parallel decoder branches consisting of convolution and attention-based blocks and multi-scale feature fusion strategy to recover the structural details of RV and FAZ (seventh row in Table 2) \cite{89}. In ACRROSS model \cite{82}, however, the segmentation of vascular networks is firstly implemented using the CVAE framework, after which the FAZ is identified using morphological closing algorithm (Fig. 5c). Furthermore, Hao et al. \cite{90} developed a voting-based adaptive feature fusion multi-task network (VAFF-Net) for automated detection of RV, FAZ and retinal vascular junctions (RVJ). In their framework, three feature extractors (ResNet50) acquire multi-scale fused features from \textit{en face} OCTA images of inner, superficial and deep vascular complexes. Voting gate modules takes the concatenated features from three extractors and performs feature selection and fusion by considering the characteristics of RV, FAZ and RVJ. Finally, the task-specific feature maps are fed into their corresponding heads to obtain segmentation results. \\

\noindent(3) Classification of the artery-vein

Besides segmentation of retinal vasculature, artery-vein (AV) classification based on OCTA images is also a well-recognized but challenging topic, since the AV structure can provide valuable information for early diagnosis of many eye disease such as retinal vein occlusion and DR, as well as cardiavascular and neurodegenerative diseases \cite{93,94}. Most DL models for AV classification \cite{95,96,97,98} are supervised model which relies on manual delineation of ground truths of AV based on the instruction proposed by Ishibazawa et al. \cite{99}. For example, the AV-Net model \cite{95} is a modified U-shaped CNN architecture, which takes \textit{en face} OCT and OCTA images for segmentation and classification of large arteries and veins (eighth row in Table 2). Abtachi et al. \cite{97} then investigated the influence of different fusion stages of OCT and OCTA images and without OCT information fusion on the AV classification accuracy, using the AV-Net architecture and different resolution images (i.e., $3 \times 3 $, $6 \times 6$ and $12 \times 12 \, mm$ OCT/OCTA images). They showed that information fusion in the decoding stage achieves higher segmentation and classification accuracy, where OCT and OCTA images are processed by two AV-Net models and the extracted features are mapped to a common feature space for prediction in the decoding stage. Furthermore, an AVA-Net model \cite{98} is proposed to not only segment large AV vessels, but also identify arterial and venous components of microvasculature by classifying image area into either arterial or venous areas using k-nearest neighbor (kNN) classifier. In addition, because color fundus images and OCT images contain more distinguishable features of arteries and veins but have limited resolution compared with OCTA, some previous studies proposed to use fundus photography or OCT to guide manual delineation and algorithm-based classification of AV in the OCTA images \cite{93,100,101}. But most of the algorithms are registration methods and require a good alignment and similarity between fundus images (or OCT) and target OCTA images.\\

\noindent(4) Segmentation of pathological features in the OCTA images

OCTA images not only contain information of vascular network structures, but also directly reflect many pathological features and structures which are sensitive biomarkers for early diagnosis of ocular diseases. For example, the nonperfusion area (NPA) and diabetic macular edema (DME) in the retina is used to quantitatively characterize abnormality in vascular flows and interstitial flows in DR in clinics \cite{102,103}. The characteristics (i.e. distribution, density) of flow voids (FVs) in the choroid indicate the pathological progression of central serous chorioretinopathy \cite{104}. In age-related macular degeneration (AMD), the formation of choroidal neovascularization (CNV) driven by the decreased oxygen supply from choroid to outer retina represents a key disease stage to be classified as wet (exudative or neovascular) AMD \cite{105}. Thus, accurate and efficient detection, identification and segmentation of these pathological features in the OCTA data has become the major focus and interest in image-based clinical diagnosis and prognosis. 

However, even for the same type of eye disease, the variation of pathological characteristics in clinics is significant, and the image quality and the amount of sample data are limited, so there is still lack of standardization for quantification and understanding of disease pathophysiology. Though rich information about the retinal structure and vasculature in both superficial and deep fundus tissues can be accessed in a non-invasive manner, the systematic noise, artifacts and the inter-device variations in the OCTA data are still challenging problems needed to be addressed using advanced techniques and algorithms. Compared with segmentation of retinal vasculature, we found there are fewer recent works about segmentation of pathological features using DL models and OCTA data. In addition, most of these DL models use basic architectures such as CNN, UNet and ResNet trained in a supervised manner relying on the clear delineations of pathological areas. We discuss the representative works about segmentation of NPA, FV, edema and NV in the following text.

The shape of NPA is less complex compared with multi-scale vasculature, however, reliable identification of a NPA can be influenced by the shadow artifacts, defocus artifacts, and low signal-to-noise ratio caused by the image acquisition device and protocol, as discussed in previous section A(2). Wang et al. \cite{106} proposed to merge the information from OCT reflectance images and OCTA to exclude the signal attenuation below strong absorbing and scattering obstructions using U-shape architecture of CNN layers with dilated kernels (first row in Table 3). 1428 OCTA scans which contains healthy controls, diabetes without retinopathy, DR at mild, moderate and severe stages are collected in clinics, and applied for training and testing of the model. Guo et al. \cite{107} designed two parallel UNet-like networks, where one network extracts artifacts-related features from retinal thickness map and OCT reflectance image, and another extracts NPA-relevant features from \textit{en face} angiographic. Features extracted from two UNet networks are then fused for removing artifacts and segmentation of NPA (Fig. 6a). Results showed that the detection sensitivity and segmentation accuracy, quantified using F1 score, of NPA in moderate and severe DR eyes has been greatly improved compared with the work \cite{106}. Therefore, the DL model is further applied to study the correlation between NPA and the severity of DR, and evaluate the diagnostic power of DR based on the NPA biomarker \cite{107}.

OCTA allows not only imaging retinal layers, but the light can penetrate deeper in tissues, so the granular-like images at the choriocapillary level can be acquired. In a choroid OCTA \textit{en face} image (Fig. 6b), the bright areas represent blood flow and the dark patterns are FVs (i.e., absence of flow) inside the choroid tissue. The abnormal pattern of FVs is closely correlated to many fundus diseases, such as AMD, DR and glaucoma. For example, L\'opez-Varela et al. \cite{108} studied the choriocapillary characteristics of disease eyes with chronic central serous chorioretinopathy (CSC) before and after photodynamic therapy. They firstly performed contrast enhancement and segmentation of FVs in choroid OCTA images using a thresholding-based method, and visualized the distinct patterns of FVs caused by the medical treatment. Next, a UNet-type deformable registration model is developed to improve segmentation accuracy and quantitatively evaluate the deformation of treatment patterns during different treatment period (second row in Table 3, Fig. 6b).  

Retinal vascular pathologies, such as the disrupted blood-retinal barrier, influence interstitial flow management, cause local fluid accumulation and lead to the formation of macular edema (ME) \cite{109}. ME is the major cause of visual loss particularly for DR eyes and AMD. Currently, structural OCT are widely used in clinics for diagnosis of ME based on the retinal thickness map and the volumetric data of OCT B-scans. To accurately measure the 3D volume of ME and visualize its shape and position, DL-based segmentation and 3D reconstruction methods using structural OCT data have been developed\cite{110}. Guo et al. \cite{111} showed that by including additional OCTA information into the DL model, the resistance to the shadow artifacts and the segmentation performance of both intra-retinal and sub-retinal fluids can be improved, relying on the fact that edema regions do not contain blood flow (OCTA) signal. Specifically, OCT and OCTA data are combined using a weighted fusion strategy before being inputted into their DL model, where the data weights are optimized to achieve the best segmentation accuracy (third row in Table 3). Furthermore, 3D fluid volume is reconstructed using densely-sampled OCT/OCTA data by registering all scan images using the Bruch's membrane and large retinal vessels as a reference. The 3D volumetric rendering reveals anatomic changes of ME and surrounding vasculature accurately and intuitively (Fig. 6c).

Neovascularization (NV), including retinal NV (RNV) mainly related to DR eyes and choroid NV (CNV) associated with wet AMD, are the most significant indicator for DR and AMD disease progression \cite{112,113}. OCTA has much better capability of visualizing NV, especially CNV lesions, compared with other image modalities (i.e., color fundus photography, fluorescent angiography), but can still suffer from background noise and many artifacts such as projection and tail artifacts, as discussed in section A. Tun et al. \cite{114} published a review paper in 2023, comprehensively summarized filter-based and thresholding-based methods and DL models for detection, segmentation and morphological classification of NV. Most of the DL models are CNN, ResNet, UNet architectures which require supervised training using manual delineations, and some works use transfer learning for small dataset. Besides segmentation of the entire NV area, some DL models also achieve classifying NV lesion types based on the morphology, as well as segmentation of detailed vascular networks in CNV \cite{114}. For example, most recently, Wang et al. \cite{115} developed a UNet-type DL model for segmentation of the optical disc and NV regions in the optical disc. They collected a large pathological dataset containing 24576 OCTA \textit{en face} images from 96 patients. After being trained using the dataset and manual delineation, the proposed DL model, composed of ResNet module, attention mechanism and multi-scale feature fusion strategy, demonstrates better performance than three other DL models (i.e., SegNet, AUNet and BiSeNet) and comparable performance as human delineation (fourth row in Table 3, Fig. 6d). \\

\begin{table*}[htp]
\caption{DL models for segmentation of pathological features and important structures in the OCTA images.}
\label{table}
\centering
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{1.8}
\begin{tabular}{|m{40pt}<{\centering}|m{25pt}<{\centering}|m{65pt}<{\centering}|m{240pt}<{}|m{120pt}<{}|}
\hline
Topic & 
Year &
DL model & 
\hspace{90pt}Model architecture & \hspace{50pt}Dataset \\
\hline

NPA &  2020 & DL model (supervised) \cite{106} & \textbf{UNet} architecture, which takes slab-specific mean projection angiographic and OCT reflectance as input to segment NPA in three retinal plexuses. Encoder: 5 conv layers and 1 transition layer. Decoder: 4 conv layers and 1 parallelized multi-scale feature extraction layer. & 1428 structural OCT/OCTA images (122 healthy, 56 diabetics without retinopathy, 250 DR eyes).\\ 
\hline

Flow void &  2023 & Deformable registration model (supervised) \cite{108} & \textbf{UNet} architecture, which takes choroid OCTA image pairs before and after treatment as input. Encoder: 5 conv blocks, 1 conv layer with \textbf{dilation filter}. Decoder: 4 upsampling conv blocks, 2 normal conv blocks and 1 spatial transformer. Skip connections between layers of encoder and decoder. & 821 choroid OCTA \textit{en face} images of healthy eyes and pathological eyes before and after treatment.\\ 
\hline

Retinal fluid volume &  2020 & ReF-Net (supervised) \cite{111} &  \textbf{UNet} architecture, with 1 multi-scale feature extraction block and 3 residual blocks in the encoder and 5 residual blocks in the decoder. \textbf{3D rendering} of retinal \textbf{fluid volumes} generated by applying trained ReF-Net to each frame of OCT/OCTA data. & 51 volumetric OCT and OCTA data (45 with DME and 6 healthy), 2 repeated B scans, 304-304 positions. \\ 
\hline

NV &  2024 &  DL model (supervised) \cite{115} & \textbf{UNet} architecture. Encoder: \textbf{ResNet18} extracts features, and \textbf{DANet} (self-attention module) combines position-wise and channel-wise features. Decoder: MAFFM for \textbf{feature fusion}, and 2 conv layers for reconstruction of class-specific probability maps. Segmentation of optic disc boundary, NV region, which belong to two classes.& 24576 OCTA images from 96 patients with NV of the optic disc.\\ 
\hline

Bruch's membrane &  2021 &  DL-based graph-cut model (supervised) \cite{117} & \textbf{UNet} architecture with symmetric 5 conv layers and skip connections. The model takes OCT and OCTA B-scan images as input, and outputs two \textbf{graph maps} where the edge weights are intensity gradients in both horizontal and vertical directions. The pixel-wise contour of the BM is detected using \textbf{Dijkstra's algorithm}, and compared with manual segmentations. & OCT and OCTA volumes of 19 eyes from healthy controls and patients with DR, AMD, DAGA, GA and CNV.\\ 

\hline
\end{tabular}
\hspace{15pt} \\
\textbf{Note:} Conv=Convolution. DR=Diabetic Retinopathy. DME=Diabetic Macular Edema. MAFFM=Multi-scale attention feature fusion module. AMD=Age-related Macular Degeneration. DAGA=Drusen-associated Geographic Atrophy. GA=Geographic Atrophy. CNV=Choroidal Neovascularization. All the dataset are private.
\end{table*}


\noindent(5) Segmentation of ocular structure (i.e., Bruch's membrane, BM)

Bruch's membrane is a $2-4 \, \mu m$ thick acellular matrix between the retinal pigment epithelium (RPE) and the choriocapillaris, which is a critical structural and functional support to the RPE. Many ocular diseases such as AMD are related to the dysfunction of Bruch's membrane caused by factors such as extracellular matrix degeneration and angiogensis \cite{116}. However, accurate segmentation of BM is difficult in practice due to the limited resolution of commercial OCT/OCTA instruments. Most previous approaches segment the RPE, a thick and bright band, in the OCT image as a substitute for the BM, but can lead to substantial measurement errors. For example, in a pathological stage, drusen, fluid leakage and hemorrhage create spatial separation between BM and RPE \cite{117}. Considering the structural and angiographic features of the PRE, BM and choroid in healthy and pathological eyes are very different, Schottenhamml et al. \cite{117} proposed a UNet-type DL model to map OCT and OCTA images to graph maps whose edge weights contain information about the intensity gradient between neighboring image pixels. Then the reference contour of the BM layer is computed as the shortest path across the graph using Dijkstra's algorithm (fifth row in Table 3). The segmentation results are compared with standard graph-cut algorithms using only gradient OCT or OCTA information, which demonstrates the robustness and accuracy of the model performance in various different disease conditions. This work \cite{117} provides many valuable insights of designing DL-based methods for robust segmentation of ocular structures (i.e., the anatomical layers of retina) in complicated pathological conditions using OCT and OCTA B-scan images. \\

\begin{figure*}[htp]
\centerline{\includegraphics[width=1.8\columnwidth]{fig6.png}}
\caption{\textbf{DL models for segmentation of pathological structures using a UNet structure as the backbone.} \textbf{(a)} For segmentation of NPA, two parallel network branches are used, where one for identification of artifacts and another for segmentation of NPA \cite{106}. \textbf{(b)} For segmentation of FVs, two \textit{en face} OCTA images at the choriocapillary level before and after photodynamic therapy are inputted into the DL model, which predicts the segmentation of FVs and the registered deformation map of the two OCTA images \cite{108}. \textbf{(c)} For segmentation of retinal fluid, information in both OCT and OCTA images are extracted and fused using the multi-scale feature extraction block. These features are inputted into the DL model for segmentation and 3D rendering of the fluid volume \cite{111}. \textbf{(d)} For segmentation of NV in the optic disc, features in the OCT and OCTA images are extracted at multi-levels using the convolution-based encoder. The decoder, which consists of feature fusion and convolution-based modules, outputs probability maps for the optic disc and the NV region \cite{115}.}
\label{fig6}
\end{figure*}

\subsection{Volumetric reconstruction}
3D reconstruction of the OCTA data provides intuitive and straightforward visualization results for clinicians to identify pathological feature and its  positions, as clinicians do not need to visualize a stack of images slice-by-slice and interpret the structure in mind. The \textit{en face} OCTA images also contain useful structural information, but they are generated by projection of a slab of OCTA signals where the slab thickness and the overlap of information in the depth direction influence the rendering results. For many unfamiliar pathological shapes, 3D visualization of the whole volume is very beneficial even for experienced clinicians. It helps reduce interpretation inconsistency among different physicians. Moreover, 3D data 
reconstruction enables quantitatively and accurately calibrate the shape and size in a higher dimension. For example, 3D reconstruction of the retinal fluid volume enables accurately measure the size of the edema. Localizing the position of neovasculature in both \textit{en face} plane and depth direction provides useful reference for identifying the severity of the disease as well as guiding the high-precision vitreo-retinal surgery. Moreover, the high-quality structural model of the 3D vasculature can be used in hemodynamics analysis and neurovascular coupling analysis, to get a deep mechanistic and physiological understanding of how oxygen and nutrients are supplied to and support the optic nerve cells and fundus tissues \cite{1,2}.

For many modalities of medical images, volumetric data representation and visualization methods have been widely investigated, developed and applied to assist clinical diagnosis and therapies. For example, in personalized total knee arthroplasties, custom implants are fabricated based on the 3D bone reconstructions from the 2D biplanar radiographs (i.e., CT images) \cite{118}. DL models are currently the most popular approaches for volumetric reconstruction and generation of realistic synthetic 3D data, due to their ability to handle dataset of various sizes and modalities in an efficient manner. A comprehensive review of GAN-based methods for 3D volumetric data reconstruction is provided in \cite{119}, where most previous works focus on CT and MRI image-based 3D reconstruction and related applications. Compared with CT and MRI, OCT and OCTA data contain a lot of inherent noise and artifacts, and the vascular structures are multi-scale and have complicated topology. 3D reconstruction of OCT and OCTA data is thus a state-of-art but more challenging task.

3D geometry data are usually divided into three types, point clouds, surface meshes and voxel grids \cite{118}. Point clouds are data points (i.e., denoised light signals) measured in the three-dimensional space and contain the intensity information. The denser the points, the more detailed small-scale features the point clouds can represent. Since point clouds can be easily constructed from experimental measurements, many commercial OCT/OCTA instruments already have the function to display the 3D curved surface of the retina using point clouds (Fig. 7a). Several previous studies showed the 3D shape of the edema and the vasculature using a stack of denoised or segmented \textit{en face} OCT/OCTA image slices (Fig. 7b) \cite{1,44,53,111}. Such point cloud data is memory efficient for storage. However, they are irregular graph data, where the connectivity among data points is lost and the data is permutation invariant. Regular graph data, including surface meshes and voxel grids (Fig. 7c), are richer in information compared to point clouds. High-level features, such as the spatial constraints among neighboring data points, the tubular shape of vessels and the whole object of the vascular network, are restored in the regular graph data. Furthermore, numerical analysis such as the numerical estimation of blood flow and oxygen distribution using in the retina finite element analysis relies on the mesh or voxel grid data. 

Point clouds of OCT/OCTA data can be converted to regular graph data by compensating the lost information including the depth information and spatial connectivity among data points. We found there were eight works published since 2020, achieving comprehensive 3D shape modeling and analysis of the retinal vasculature using OCTA volumes. Two of them are based on graph analysis and mesh reconstruction techniques \cite{120,121}, and four are DL-based methods \cite{122,123,124,125}. Specifically, Zhang et al. \cite{120} developed an advanced mesh reconstruction technique using cuboid intersection method to generate triangular meshes for vascular surfaces, where the mesh size and density are adaptively adjusted according to the vessel geometry (first row in Table 4, Fig. 7d). However, the reconstruction accuracy of this method is largely influenced by the tail artifacts in OCT and OCTA images, such that the vessel shape in the depth direction is distorted and elongated (Fig. 7d). Vasculature reconstruction based on graph methods suppresses tail artifacts and generates realistic vessel shapes, by assuming that all vessels have spherical cross sections and and their center lines are intersecting curves located in the 3D space. Optimization-based \cite{121} and DL-based methods \cite{122,123} have been proposed to estimate the vessel diameters and depth map of the vessel center lines. For example, in the work of Okamotor et al. \cite{121}, an energy function is formulated by considering the smoothness of the center line curve and the pixel-wise intensity information in the OCTA B-scan images. After identifying all locations of the vascular segments by optimizing the energy function, the 3D vascular graph can be reconstructed (second row in Table 4, Fig. 7e). Furthermore, Yu et al. \cite{122} trained a U-shape encoder-decoder model with structure constraint blocks to predict the depth map of vessel center lines, and reconstructed 3D realistic geometry model for superficial and deep vasculature using VTK toolkit \cite{128} (third row in Table 4, Fig. 7f). They also achieved cross-domain depth estimation, where the DL model is trained using one dataset with ground truths of depth maps and can be used to estimate vascular depths in the OCTA images from a different dataset \cite{123}. However, we were not able to find any GAN-based works that can perform 3D vasculature reconstruction directly from a stack of OCTA B-scan slices (or \textit{en face} images), though this kind of work has been successfully achieved in analyzing large organ morphology (i.e., lung, brain, cancer, etc.) using CT and MRI images. There is only one latest work in 2024 \cite{125}, which uses a classic UNet model for segmentation of vessels in each \textit{en face} slice in the OCTA data volume, and reconstructs the surface meshes and voxel grids of the whole vasculature in the Amira software (fourth row in Table 4, Fig. 7g). Nevertheless, the complexity of OCTA signals and the multi-scale vasculature, especially the densely packed microvasculature in the deep retinal layers and choroid, bring many more difficulties and challenges to the image annotation, analysis and reconstruction tasks. Extensive researches on applying the state-of-art and intelligently-designed DL-based methods to analyze OCT/OCTA data volumes are urgently needed.\\

\begin{table*}[htp]
\caption{Methods for segmentation of pathological features and important structures in the OCTA images.}
\label{table}
\centering
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{1.8}
\begin{tabular}{|m{40pt}<{\centering}|m{25pt}<{\centering}|m{65pt}<{\centering}|m{240pt}<{}|m{120pt}<{}|}
\hline
Topic & 
Year &
Method & 
\hspace{90pt}Model architecture & \hspace{50pt}Dataset \\
\hline

Surface meshes &  2020 & Cuboid intersection method \cite{120} & (1) Generate \textbf{well-composed} 3D binary vessel \textbf{masks}; (2) Establish \textbf{triangular surface representation}: classify grid points as vessel/background voxels, assign rectangular cuboids centered around each voxels, \textbf{take the intersection of vessel and background cuboids} as a set of rectangular faces, divide a rectangular face into two triangular faces; (3) \textbf{Remove geometric outliers} using thresholds of Laplacian smoothing and distortion metric, reconstruct new surface by boundary deformation procedure. & (Private) 260 OCTA volumes (10 repeated scans, rigid registration), 100 OCTA volumes for DR eyes (40 healthy, 20 non-proliferative DR, 40 proliferative DR).\\ 
\hline

Voxel grids & 2024 & Depth estimation method \cite{121} & (1) \textbf{Manual annotation} of vessel centerline segments; (2) Generate 3D VCW image using template matching method; (3) Depth information of vessel centerline segments estimated by \textbf{optimizing an energy function} using cross-sectional OCTA and VCW profiles; (4) Reconstruct 3D vascular graph. & (Private) Simulated OCTA volumes, containing vessels of quadratic shape and sinusoidal wave-like shape. Real OCT and OCTA volumes (microvasculature from the dorsal dermis of mice). \\ 
\hline

Voxel grids & 2021 & DL model for generation of depth map (supervised) \cite{122} & \textbf{UNet} architecture with 4 symmetric conv layers. SCB blocks in the skip connections between encoder and decoder layers. Ground truths are generated using (1) OCTA-NET \cite{77} for vessel segmentation, (2) 3D point clouds of vessel center lines are extracted using skeletonization method \cite{126}. \textbf{Surface meshes} are generated using the \textbf{Ball-pivoting algorithm} \cite{127} and \textbf{VTK toolkit} \cite{128}. & (Private) 80 OCTA volumes.\\
\hline

Voxel grids & 2024 & DL model for 3D vessel segmentation (supervised)\cite{125} & \textbf{Classic UNet} model trained using \textit{en face} OCTA images. Each slice in the 3D OCTA volume is segmented using the UNet model. 3D OCTA segmentation results are divided into two parts, superficial retina and deep retina. The two part are \textbf{rendered in Amira software} \cite{129} separately, and merged to get the reconstruction results. & (Public) OCTA-500.\\
\hline

\end{tabular}
\hspace{15pt} \\
\textbf{Note:} DR=Diabetic Retinopathy. VCW image=Vascular center-weighted image. Conv=Convolution. SCB=Structure constraint block.
\end{table*}

\begin{figure*}[htp]
\centerline{\includegraphics[width=1.8\columnwidth]{fig7.png}}
\caption{\textbf{Volumetric reconstruction methods of retinal vasculature.} \textbf{(a)} Example image of the 3D point clouds data of a severe myopia eye. \textbf{(b)} Example images of the edema and vasculature based on the 3D point clouds data \cite{44}. \textbf{(c)} Example images of the 3D surface meshes and voxel grids reconstructed using OCTA volumetric data \cite{120}. \textbf{(d)} Cuboid intersection method for reconstruction of 3D surface meshes of the retinal vasculature \cite{120}. \textbf{(e)} Method for estimation of vessel depths and reconstruction of 3D vascular graph \cite{121}. \textbf{(f)} Modified UNet model for vessel depth map prediction, and pipeline for 3D reconstruction of vascular network \cite{122}. \textbf{(g)} Classic UNet model for 3D vessel segmentation, and pipeline for 3D reconstruction of retinal vasculature \cite{125}.}
\label{fig7}
\end{figure*}

\section{Public available OCTA dataset}
\label{sec:dataset}
A large amount of real-world OCTA data with good image quality and accurate delineation can significantly improve the performance of DL models. However, compared with other image types such as fundus photographs, there are fewer publicly available OCTA dataset. The quality of OCTA images vary over different dataset, since it is influenced by the acquisition devices and protocols. We found there are eight public OCTA dataset \cite{71,77,91,130,131,132,133,134}, where the \textit{en face} OCTA images have been uploaded into the public data repositories such as Zenodo. Details of the dataset are listed in Table 4. Three of them (Giarratano \cite{71}, FAZID \cite{131} and Soul \cite{133}) are available online, and the other five dataset require users to provide additional information to get the password for downloading and unzipping. In addition, private dataset listed in Tables 1, 2 and 3 were not uploaded to any online repositories. Only the acquisition devices and protocols were discussed in the corresponding journal papers. 

ROSE and OCTA-500 provided by Nanjing University of Science and Technology are two most widely used public dataset \cite{48,74,77,82,83,86,89,125} due to the good image quality and a large image number. In particular, OCTA-500 \cite{132} provides both \textit{en face} OCTA images and 3D OCT and OCTA volumes, which can be very beneficial for understanding the 3D retinal and vascular structures in various disease conditions \cite{125}. Moreover, Zhongshan Ophthalmic center (Sun Yat-sen University) have provided the largest OCTA dataset \cite{134}, which contains more than ten thousand \textit{en face} images of various eye diseases.


\begin{table*}[htp]
\caption{Public OCTA dataset.}
\label{table}
\centering
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{1.8}
\begin{tabular}{|m{40pt}<{\centering}|m{205pt}<{}|m{110pt}<{}|m{140pt}<{}|}
\hline
Dataset name & 
\hspace{65pt}Image details & 
\hspace{35pt}Link to dataset &
\hspace{25pt}Acquisition device and details \\
\hline

Giarratano \cite{71}  & \textbf{55 \textit{en face} images} of 11 healthy eyes, at 5 clinical region of interest: superior, nasal, foveal, inferior, and temporal. Pixel-wise annotations of the vasculature. & \url{https://datashare.ed.ac.uk/handle/10283/3528} (online) & RTVue-XR Avanti OCT system (OptoVue). $\mathbf{304 \times 304}$ repeated A-scans.\\ 
\hline

ROSE \cite{77, 91} & (1) \textbf{ROSE-1:} \textbf{117 \textit{en face} images}, 13 healthy and 26 AD eyes, $\mathbf{3 \times 3}$ \textbf{\textit{mm}} \textbf{SVC}, \textbf{DVC}, \textbf{IVC} images centered at the fovea. Center line-level and pixel-level manual annotations of the vasculautre. Manual annotations of the RVJ and FAZ. (2) \textbf{ROSE-2:} \textbf{112 \textit{en face} images},112 disease eyes, centered at the fovea, $3 \times 3 mm$ SVC images. & \url{https://zenodo.org/records/12775880} (on request) & ROSE-1: RTVue XR Avanti SD-OCT system (Optovue) equipped with AngioVue software. $\mathbf{304 \times 304}$ repeated A-scans. ROSE-2: Heidelberg OCT2 system with Spectralis software (Heidelberg Engineering). $\mathbf{512 \times 512}$ repeated A-scans.\\ 
\hline

OCTAGAN \cite{130} & \textbf{212 \textit{en face} images}, 36 healthy and 17 DR eyes, $\mathbf{3 \times 3}$ \textbf{\textit{mm}} \textbf{SVC} and \textbf{DVC} images, and $\mathbf{6 \times 6}$ \textbf{\textit{mm}} superficial and deep images, centered at the fovea. Manual annotations of FAZ from two experts. & \url{http://www.varpa.es/research/ophtalmology.html#cloud} (on request) & DRI OCT Triton (Topcon). $\mathbf{320 \times 320}$ repeated A-scans. \\ 
\hline

FOCTAIR \cite{130} & \textbf{87 \textit{en face} images}, 29 disease eyes (disease type no specified), $\mathbf{3 \times 3}$ \textbf{\textit{mm}} and $\mathbf{6 \times 6}$ \textbf{\textit{mm}} \textbf{SVC} images centered at the fovea. & \url{http://www.varpa.es/research/ophtalmology.html#cloud} (on request) & Not specified.\\ 
\hline

FAZID \cite{131} & \textbf{304 \textit{en face} images}, 107 DR, 109 myopic and 88 healthy eyes, $\mathbf{6 \times 6}$ \textbf{\textit{mm}} images centered at the fovea. Manual annotations of FAZ. & \url{https://www.openicpsr.org/openicpsr/project/117543/version/V2/view} (online) & Cirrus 5000 Angioplex (Carl Zeiss Meditec Inc.). $\mathbf{420 \times 420}$ repeated A-scans.\\ 
\hline

OCTA-500 \cite{132} & \textbf{OCTA-6mm: 300 OCT and OCTA volumes}, $\mathbf{6 \times 6 \times 2}$ \textbf{\textit{mm}}, 209 are disease eyes (DR, AMD, CNV, CSC, RVO) and 91 are healthy eyes. \textbf{OCTA-3mm: 200 OCT and OCTA volumes}, $\mathbf{3 \times 3 \times 2}$ \textbf{\textit{mm}}, 40 are disease eyes (DR, AMD, CNV) and 160 are healthy eyes. \textit{En face} images generated by 6 types of projections. 4 types of text labels (age/gender/eye/disease). 7 types of segmentation labels (large vessel/capillary/artery/vein/FAZ/retinal layers). & \url{https://ieee-dataport.org/open-access/octa-500} (on request) & 70 kHz spectral-domain OCT system with a center wavelength of 840 nm (RTVue-XR, Optovue). OCTA-6mm: $\mathbf{400 \times 400 \times 640}$ repeated A-scans. OCTA-3mm: $\mathbf{304 \times 304 \times 640}$ repeated A-scans. \\ 
\hline

Soul \cite{133} & \textbf{53 \textit{en face} images}, BRVO eyes, $\mathbf{6 \times 6}$ \textbf{\textit{mm}}, SVC layer. Annotation of large vessels. & \url{https://doi.org/10.6084/m9.figshare.24893358.v3} (online) & Optovue Angio OCT RTVueXR. Acquisition protocols are not specified.\\ 
\hline

COIPS \cite{134} & (1) \textbf{sOCTA-3$\times$3-10k: 10480 \textit{en face} images}, $\mathbf{3 \times 3}$ \textbf{\textit{mm}}, SVC layer. (2) \textbf{sOCTA-6$\times$6-14k: 14042 \textit{en face} images}, $\mathbf{6 \times 6}$ \textbf{\textit{mm}}, SVC layer. (3) \textbf{sOCTA-3$\times$3-1.1k-seg: 1101 \textit{en face} images}, $\mathbf{3 \times 3}$ \textbf{\textit{mm}}, SVC layer, manual annotation of FAZ. (4) \textbf{dOCTA-6$\times$6-1.1k-seg: 1143 \textit{en face} images}, $\mathbf{6 \times 6}$ \textbf{\textit{mm}}, DVC layer, manual annotation of FAZ. & \url{https://doi.org/10.5281/zenodo.5111975} and \url{https://doi.org/10.5281/zenodo.5111972} (on request) & DRI OCT Triton (Topcon), under a 1050 nm wavelength tunable laser at the speed of 100,000 A-scans/s. $\mathbf{320 \times 320}$ \textbf{\textit{pixels}} resolution. \\ 
\hline
\end{tabular}
\hspace{15pt} \\
\textbf{Note:} DR=Diabetic Retinopathy. SVC=Superficial vascular complexes. DVC=Deep vascular complexes. IVC=Inner vascular complexes. RVJ=Retinal vascular junctions. FAZ=Foveal avascular zone. FA=Fluorescein angiography. AD=Alzheimer's Disease. AMD=Age-related Macular Degeneration. CNV=Choroidal Neovascularization. CSC=Central Serous Chorioretinopathy. RVO=Retinal Vein Occlusion. BRVO=Branch Retinal Vein Occlusion.
\end{table*}

\section{Discussion}
\label{sec:discussion}
In summary, this review comprehensively discussed the state-of-art DL models for denoising, segmentation and volumetric rendering of OCTA image data. DL models developed before and around 2020 and 2021 usually have the basic network architectures (i.e., CNN, UNet, ResNet, Transformer), which require detailed manual annotations for model training. Though the performance of these general DL models are better than most traditional image analysis methods, such as the thresholding-based algorithms, the requirement of pixel-wise manual annotations by experts and clinicians limits their wide applications in real-world situations. Considering that the specific characteristics of OCTA image, such as the intensity distribution in the depth direction, are valuable information to be included in the model, the latest DL models designed flexible branches in the pipleline where each branch contains one or more basic CNN or UNet models to extract the multi-level and OCTA image-specific features. Multi-source and multi-modality information, including the structural OCT images, OCT reflectance maps, retinal thickness maps, OCTA images of different resolutions and the unpaired OCTA images, are fused at different stages in the DL models to improve the inference power of the model. Furthermore, consecutive B-scan images in both spatial and temporal domains contain important physics features, such as the spatial continuity of vasculature and the temporal decorrelation of flow signals. Thus, by intelligently designing the model architecture, these natural physics features can not only improve the model performance, but also release the strict requirement of using manual annotations as ground truths for model training. In addition, preliminary attempts of integrating the angiogenesis dynamics and the light reflection and attenuation physics into the DL models have been made. The promising results suggest that these first-principle mechanisms are useful prior knowledge for both supervised and unsupervised DL models.

However, to analyze the disease images with large variations in the image patterns and retinal structures, the basic supervised DL model (in particular, the UNet model) is still the most robust, reliable and widely-used one. Advanced DL models for denoising and removing artifacts in the OCTA data may be less influenced by the disease conditions. But the performance of identification and segmentation of vasculature and pathological areas using DL models will largely depend on the accurate and comprehensive manual delineations from clinicians. For the fundus photography, the cost of image acquisition is cheap and the acquisition devices are more accessible to public, so there is a large amount of available data. Lots of works have been done to design and implement DL models for analyzing fundus photography images and diagnosing ocular diseases such as DR and AMD \cite{28,135,136,137}. For example, in 2024, an integrated image-based DL and large language model (DeepDR-LLM) was developed and applied to provide individualized diabetes management recommendations \cite{138}. The DeepDR model was trained using multiethnic and multicountry datasets comprising more than one million retinal fundus photographs. The DeepDR-LLM system was then tested in the real clinical workflow using health records and fundus images in multiple Chinese cities (e.g., Beijing, Shanghai, Guangzhou) and several other countries (e.g., Singapore, Thailand, England). OCTA images can be one informative image modality to be integrated into the DL-LLM based system to enhance the diagnosis power. Moreover, OCTA data provides comprehensive information of the vascular structures and pathologies from superficial to deep retinal layers as well as the choriodal layer. Anterior segment OCTA also enables visualization of the aqueous angiography-defined high-flow anterior regions in the human eye, such as the cornea and ocular surface. Thus, OCTA-based DL models combined with opthalmological knowledge and biomechanics and biophysics studies provide valuable insights for understanding the fundamental mechanisms of ocular diseases and assisting developing new theraputic strategies. 


\begin{thebibliography}{00}

\bibitem{1} Richard F. Spaide, James G. Fujimoto, Nadia K. Waheed, Srinivas R. Sadda, and Giovanni Staurenghi, ``Optical coherence tomography angiography,'' \emph{Progress in Retinal and Eye Research}, vol. 64, pp. 1-55, 2018.

\bibitem{2} Charles Jit Teng Ong, Mark Yu Zheng Wong, Kai Xiong Cheong, Jinzhi Zho, Kelvin Yi Chong Teo, and Tien-En Tan, ``Optical coherence tomography angiography in retinal vascular disorders,'' \emph{Diagnostics}, vol. 13, no. 9, pp. 1620, 2023.

\bibitem{3} Xincheng Yao, Minhaj N Alam, David Le, and Devrim Toslak, ``Quantitative optical coherence tomography angiography: a review,'' \emph{Experimental Biology and Medicine}, vol. 245, no. 4, pp. 301-312, 2020.

\bibitem{4} Shuichi Makita, Youngjoo Hong, Masahiro Yamanari, Toyokiko Yatagai, and Yoshiaki Yasuno, ``Optical coherence angiography,'' \emph{Optics Express}, vol. 14, no. 17, pp. 7821-7840, 2006.

\bibitem{5} K Takayama, Y Ito, H Kaneko, K Kataoka, T Sugita, R Maruko, K Hattori, E Ra, F Haga, and H Terasaki, ``Comparison of indocyanine green angiography and optical coherence tomographic angiography in polypoidal choroidal vasculopathy,'' \emph{Eye}, vol. 31, pp. 45-52, 2017.

\bibitem{6} Lukas Van Melkebeke, Jo\~ao Barbosa-Breda, Marc Huygens and Ingeborg Stalmans, ``Optical coherence tomography angiography in Glaucoma: a review,'' \emph{Ophthalmic Research}, vol. 60, no. 3, pp. 193-151, 2018.

\bibitem{7} Xuhao Chen, Ying Hong, and Chun Zhang, ``Assessment of retinal microcirculation alterations in glaucoma by optical coherence tomography angiography,'' \emph{Chinese Journal of Experimental Ophthalmology}, vol. 40, pp. 371-377, 2022.

\bibitem{8} Zihan Sun, Dawei Yang, Ziqi Tang, Danny S. Ng, and Carol Y. Cheung, ``Optical coherence tomography angiography in diabetic retinopathy: an updated review,'' \emph{Eye}, vol. 35, pp. 149-161, 2021.

\bibitem{9} Jeffrey Ma, Ria Desai, Peter Nesper, Manjot Gill, Amani Fawzi, and Dimitra Skondra, ``Optical coherence tomographic angiography imaging in age-related macular degeneration,'' \emph{Ophthalmology and Eye Diseases}, vol. 9, pp. 1179172116686075, 2017.

\bibitem{10} Sally S. Ong, Tapan P. Patel, and Mandeep S. Singh, ``Optical coherence tomography angiography imaging in inherited retinal diseases,'' \emph{Journal of Clinical Medicine}, vol. 8, no. 12, pp. 2078, 2019.

\bibitem{11} Woojhon Choi, Kathrin J. Mohler, Benjamin Potsaid, Chen D. Lu, Jonathan J. Liu, Vijaysekhar Jayaraman, Alex E. Cable, Jay S. Duker, Robert Huber, and James G. Fujimoto ``Choriocapillaris and choroidal micro-vasculature imaging with ultrahigh speed OCT angiography,'' \emph{PloS One}, vol. 8, no. 12, pp. e81499, 2013.

\bibitem{12} Woojhon Choi, Eric M. Moult, Nadia K. Waheed, Mehreen Adhi, ByungKun Lee, Chen D. Lu, Talisa E. de Carlo, Vijaysekhar Jayaraman, Philip J. Rosenfeld, Jay S. Duker, and James G. Fujimoto ``Ultrahigh-speed, swept-source optical coherence tomography angiography in nonexudative age-related macular degeneration with geographic atrophy,'' \emph{Ophthalmology}, vol. 122, no. 12, pp. 2532-2544, 2015.

\bibitem{13} Anna Lentzsch, et al., ``Comparison of swept-source versus spectral domain optical coherence tomography angiography for detection of macular neovascularization,'' \emph{Graefes Archive for Clinical and Experimental Ophthalmology}., vol. 260, pp. 113-119, 2022.

\bibitem{14} Anna-Maria Haas, Daniel Ahmed, Martin Stattin, Alexandra Graf, Katharina Krepler, and Siamak Ansari-Shahrezaei, ``Comparison of macular neovascularization lesion size by the use of spectral-domain optical coherence tomography angiography and swept-source optical coherence tomography angiography versus indocyanine green angiography,'' \emph{Acta Ophthalmologica}, vol. 99, no. 2, pp. e260-e266, 2021.

\bibitem{15} Min-Woo Lee, Kyeung-Min Kim, Hyung-Bin Lim, Young-Joon Jo, and Jung-Yeul Kim ``Repeatability of vessel density measurements using optical coherence tomography angiography in retinal diseases,'' \emph{British Journal of Ophthalmology}, vol. 103, no. 5, pp. 704-710, 2019.

\bibitem{16} Emily D. Cole, Eric M. Moult, Sabin Dang, Woojhon Choi, Setfan B. Ploner, ByungKun Lee, Ricardo Louzada, Eduardo Novais, Julia Schottenhamml, Lennart Husvogt, Andreas Maier, James G. Fujimoto, Nadia K. Wahhed, and Jay S. Duker, ``The definition, rationale, and effects of thresholding in OCT angiography,'' \emph{Ophthalmology Retina}, vol. 1, no. 5, pp. 435-447, 2017.

\bibitem{17} Khalil Ghasemi Falavarjani, Mayss AI-Sheikh, Handa Akil, and Srinivas R Sadda, ``Image artefacts in swept-source optical coherence tomography angiography,'' \emph{British Journal of Ophthalmologyl}, vol. 101, no. 5, pp. 564-568, 2017.

\bibitem{18} Pasha Anvari, Maryam Ashrafkhorasani, Abbas Habibi, and Khalil Ghasemi Falavarjani, ``Artifacts in optical coherence tomography angiography,'' \emph{Journal of Ophthalmic \& Vision Research}, vol. 16,no. 2,  pp. 271-286, 2021.

\bibitem{19} Richard F. Spaide, James G. Fujimoto, and Nadia K. Waheed, ``Image artifacts in optical coherence tomography angiography,'' \emph{Retina}, vol. 35, no. 11, pp. 2163-2180, 2015.

\bibitem{20} Xuan Liu, Mitchell Kirby, and Feng Zhao, ``Motion analysis and removal in intensity variation based OCT angiography,'' \emph{Biomedical Optics Express}, vol. 5, no. 11, pp. 3833-3847, 2014.

\bibitem{21} Ian C. Holmen, Sri Meghana Konda, and Jeong W. Pak, ``Prevalence and severity of artifacts in optical coherence tomographic angiograms,'' \emph{JAMA Ophthalmology}, vol. 138, no. 2, pp. 119-126, 2020.

\bibitem{22} Alireza Kamalipour, Sasan Moghimi, Huiyuan Hou, Rafaella C. Penteado, Won Hyuk, James A. Proudfoot, Nevin El-Nimiri, Eren Ekici, Jasmin Rezapour, Linda M. Zangwill, Christopher Bowd, and Robert N. Weinerb, ``OCT angiography artifacts in Glaucoma,'' \emph{Ophthalmology}., vol. 128, no. 10,  pp. 1426-1437, 2021.

\bibitem{23} Jeffrey J. Yu, Acner Camino, Liang Liu, Xinbo Zhang, Jie Wang, Simon S. Gao, Yali Jia, and David Huang, ``Signal strength reduction effects in OCT angiography,'' \emph{Ophthalmology Retina}, vol. 3, no. 10, pp. 835-842, 2019.

\bibitem{24} Qinqin Zhang, Anqi Zhang, Cecilia S. Lee, Aaron Y. Lee, Kasra A. Rezaei, Luiz Roisman, Andrew Miller, Fang Zheng, Giovanni Gregori, Mary K. Durbin, Lin An, Paul F. Stetson, Philip J. Rosenfeld, and Ruikang K. Wang, ``Projection artifact removal improves visualization and quantitation of macular neovascularization imaged by optical coherence tomography angiography,'' \emph{Ophthalmology Retina}, vol. 1,no. 2, pp. 124-136, 2017.

\bibitem{25} Khalil Ghasemi Falavarjani, Reza Mirshahi, Shahriar Ghasemizadeh, and Mahsa Saradarinia, ``Stepwise segmentation error correction in optical coherence tomography angiography images of patients with diabetic macular edema,'' \emph{Therapeutic Advances in Ophthalmology}, vol. 12, pp. 2515841420947931, 2020.
 
\bibitem{26} Richard F. Spaide, ``Volume rendered optical coherence tomography of retinal vein occlusion pilot study,'' \emph{American Journal of Ophthalmology}, vol. 165, pp. 133-144, 2016.

\bibitem{27} Richard F. Spaide, ``Retinal vascular cystoid macular edema: review and new theory,'' \emph{Retinal}, vol. 36, no. 10, pp. 1823-1842, 2016.

\bibitem{28} Bjorn Kaijun Betzler, Haichao Chen, Ching-Yu Cheng, Cecilia S Lee, Guochen Ning, Su Jeong Song, Aaron Y Lee, Ryo Kawasaki, Peter van Wijingaarden, Andrezej Grzybowski, Mingguang He, Dawei Li, An Ran Ran, Daniel Shu Wei Ting, Kelvin Teo, Paisan Ruamviboonsuk Sobha Sivaprasad, Varun Chaudhary, Ramin Tadayoni, Xiaofei Wang, Carol Y Cheung, Yingfeng Zheng, Ya Xing Wang, Yih Chung Tham, Tien Yi Wong, ``Large language models and their impact in ophthalmology,'' \emph{Lancet Digital Health}, vol. 5, no. 12, pp. e917-e924, 2023.

\bibitem{29} Daniel Shu Wei Ting, Louis R Pasquale, Lily Peng, John Peter Campbell, Aaron Y Lee, Rajiv Raman, Gavin Siew Wei Tan, Leopold Schmetterer, Pearse A Keane, Tien Yin Wong, ``Artificial intelligence and deep learning in ophthalmology,'' \emph{British Journal of Ophthalmology}, vol. 103, no. 2, pp. 167-175, 2019.

\bibitem{30} Andrzej Grzybowski, Piotr Brona, Gilbert Lim, Paisan Ruamviboonsuk, Gavin S. W. Tan, Michael Abramoff, and Daniel S. W. Ting, ``Artificial intelligence for diabetic retinopathy screening: a review,'' \emph{Eye}, vol. 34, pp. 451-460, 2020.

\bibitem{31} Darren Shu Jeng Ting, Valencia HX Foo, Lily Wei Yun Yang, Josh Tjunrong Sia, Marcus Ang, Haotian Lin, James Chodosh, Jodhbir S Mehta, and Daniel Shu Wei Ting, ``Artificial intelligence for anterior segment diseases: emerging applications in ophthalmology,'' \emph{British Journal of Ophthalmology}, vol. 105, no. 2, pp. 158-168, 2021.

\bibitem{32} Huiyan Jiang, Zhaoshuo Dia, Tianyu Shi, Yang Zhou, Feiyu Wang, Wenrui Hu, Xiaolin Zhu, Shijie Luo, Guoyu Tong, and Yu-Dong Yao, ``A review of deep learning-based multiple-lesion recognition from medical images: classification, detection and segmentation,'' \emph{Computers in Biology and Medicine}, vol. 157, pp. 106726, 2023.

\bibitem{33} Hanruo Liu, Liu Li, I. Michael Wormstone, Chunyan Qiao, Chun Zhang, Ping Liu, Shuning Li, Huaizhou Wang, Dapeng Mou, Ruiqi Pang, Diya Yang, Linda M. Zangwill, Sasan Moghimi, Huiyuan Hou, Christopher Bowd, Lai Jiang, Yihan Chen, Man Hu, Yongli Xu, Hong Kang, Xin JI, Rober Chang, Clement Tham, Carol Cheung, Daniel Shu Wei Ting, Tien Yin Wong, Zulin Wang, Robert N. Weinreb, Mai Xu, and Ningli Wang``Development and validation of a deep learning system to detect Glaucomatous Optic neuropathy using fundus photographs,'' \emph{JAMA ophthalmology}, vol. 137, no. 12, pp. 1353-1360, 2019.

\bibitem{34} Kai Jin, Xingru Huang, Jingxing Zhou, Yunxiang Li, Yan Yan, Yibao Sun, Qianni Zhang, Yaqi Wang, and Juan Ye, ``FIVES: a fundus image dataset for artificial intelligence based vessel segmentation,'' \emph{Scientific Data}, vol. 9, pp.475, 2022.

\bibitem{35} Tao Li, Wang Bo, Chunyun Hu, Hong Kang, Hanruo Liu, Kai Wang, and Huazhu Fu, ``Applications of deep learning in fundus images: a review,'' \emph{Medical Image Analysis}, vol. 69, pp. 101971, 2021.

\bibitem{36} Ursula Schmidt-Erfurth, Gregor S. Reiter, Sophie Riedl, Philipp Seeb\"ck, Wolf-Dieter Vogl, Barbara A. Blodi, Amitha Domalpally, Amani Fawzi, Yali Jia, David Sarraf, and Hrvoje Bogunovi\'c, ``AI-based monitoring of retinal fluid in disease activity and under therapy,'' \emph{Progress in Retinal and Eye Research}, vol. 86, pp.100972, 2022.

\bibitem{37} Dawei Li, An Ran Ran, Carol Y. Cheung, and Jerry L. Prince, ``Deep learning in optical coherence tomography: where are the gaps?'' \emph{Clinical \& Experimental Ophthalmology}, vol. 51, no. 8, pp. 853-863, 2023.

\bibitem{38} Guangming Ni, Renxiong Wu, Fei Zheng, Meixuan Li, Shaoyan Huang, Xin Ge, Linbo Liu, Yong Liu, ``Toward ground-truth optical coherence tomography via three-dimensional unsupervised deep learning processing and data,'' \emph{IEEE Transactions on Medical Imaging}, vol. 43, no. 6, pp. 2395-2407, 2024.

\bibitem{39} Mehmood Nawaz, Adilet Uvaliyev, Khadijia Bibi, Hao Wei, Sai Mu Dalike Abaxi, Anum Massod, Peilun Shi, Ho-Pui Ho, and Wu Yuan, ``Unraveling the complexity of optical coherence tomography image segmentation using machine and deep learning techniques: a review,'' \emph{Computerized Medical Imaging and Graphics}, vol. 108, pp. 102269, 2023.

\bibitem{40} Yupei Chen, Jiaxiong Li, Zhongzhou Luo, Keyi Fei, Yan Luo, Zhengyu Duan, ``Semi-supervised speckle noise reduction in OCT images with UNet and Swin-Uformer,'' \emph{IEEE Transactions on Instrumentation and Measurement}, vol. 73, pp. 4504810, 2024.

\bibitem{41} Xiaoming Liu, Qi Liu, Ying Zhang, Man Wang, Jinshan Tang, ``TSSK-Net: weakly supervised biomarker localization and segmentation with image-level annotation in retinal OCT images,'' \emph{Computers in Biology and Medicine}, vol. 153, pp.106467, 2023.

\bibitem{42} Saad M Khan, Xiaoxuan Liu, Siddharth Nath, Edward Korot, Livia Faes, Siegfried K Wangner, Pearse A Keane, Neil J Sebire, Matthew J Burton, and Alastair K Denniston, ``A global review of publicly available datasets for ophthalmological imaging: barriers to access, usability and generalisability,'' \emph{Lancet Digit Health}, vol. 3, no. 1, pp. e51-66, 2021.

\bibitem{43} Anastasiia Rozhyna, G\'abor M\'ark Somfai, Manfredo Atzori, and Henning M\"uller, ``An overview of public retinal optical coherence tomography datasets: access, annotations and beyond,'' \emph{Digital Health and Informatics Innovations for Sustainable Health Care System}, vol. 316, pp. 1664-1668, 2024.

\bibitem{44} Tristan T. Hormel, Thomas S. Hwang, Steven T. Bailey, David J. Wilson, David Huang, and Yali Jia, ``Artificial intelligence in OCT angiography,'' \emph{Progress in Retinal and Eye Research}, vol. 85, pp. 100965, 2021.

\bibitem{45} Xiyin Wu, Dongxu Gao, Davide Borroni, Savita Madhusudhan, Zhong Jin, Yalin Zheng, ``Cooperative low-rank models for removing strip noise from OCTA images,'' \emph{IEEE Journal of Biomedical and Health Informatics}, vol. 24, no. 12, pp. 3480-3490, 2020.

\bibitem{46} Ang Li, Congwu Du, Yingtian Pan, ``Deep-learning-based motion correction in optical coherence tomography angiography,'' \emph{Journal of Biophotonics}, vol. 14, no. 12, pp. e202100097, 2021.

\bibitem{47} Jiaxiang Ren, Kicheon Park Yingtian Pan, Haibin Ling, ``Self-supervised bulk motion artifact removal in optical coherence tomography angiography,'' \emph{2022 Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pp. 20617-20625, 2022.

\bibitem{48} Juan Cao, Zihao Xu, Mengjia Xu, Yuhui Ma, Yitian Zhao, ``A two-stage framework for optical coherence tomography angiography image quality improvement,'' \emph{Frontiers in Medicine}, Vol. 10, pp. 1061357, 2023.

\bibitem{49} Man Luo, Zhiling Xu, Zehua Ye, Zhendong Liang, Hui Xiao, Yiqing Li, Zhidong Li, Yingting Zhu, Yonghong He, and Yehong Zhuo, ``Deep learning for anterior segment OCT angiography automated denoising and vascular quantitative measurement,'' \emph{Biomedical Signal Processing and Control}, Vol. 83, pp. 104660, 2023.

\bibitem{50} Yuhui Ma, Qifeng Yan, Yonghuai Liu, Jiang Liu, Jiong Zhang, and Yitian Zhao, ``StruNet: perceptual and low-rank regularized transformer for medical image denoising,'' \emph{Medical Physics}, Vol. 50, no. 12, pp. 7654-7669, 2023.

\bibitem{51} Zhefan Lin, Qinqin Zhang, Gongpu Lan, Jingjiang Xu, Jia Qin, Lin An, and Yanping Huang, ``Deep learning for motion artifact-suppressed OCTA image generation from both repeated and adjacent OCT scans,'' \emph{Mathematics}, Vol. 12, no. 3, pp. 446, 2024.

\bibitem{52} Zhenghong Li, Jiaxiang Ren, Zhilin Zou, Kalyan Garigapati, Congwu Du, Yingtian Pan and Haibing Ling, ``Self-supervised denoising and bulk motion artifact removal of 3D optical coherence tomography angiography of awake brain,'' \emph{International Conference on Medical Image Computing and Computer-Assisted Intervention}. Cham: Springer Nature Switzerland, pp. 601-611, 2024.

\bibitem{53} Sabina Stefan, and Jonghwan Lee, ``Deep learning toolbox for automated enhancement, segmentation, and graphing of cortical optical coherence tomography microangiograms,'' \emph{Biomedical Optics Express}, vol. 11, no. 12, pp. 7325-7342, 2020.

\bibitem{54} Yukun Guo, Tristan T. Hormel, Liqin Gao, Qisheng You, Bingjie Wang, Christina J. Flaxel, Steven T. Bailey, Dongseok Choi, David Huang, Thomas S. Hwang, and Yali Jia, ``Quantification of nonperfusion area in montaged widefield OCT angiography using deep learning in diabetic retinopathy,'' \emph{Ophthalmology Science}, vol. 1, no. 2, pp. 100027, 2021.

\bibitem{55} Yukun Guo, Tristan T. Hormel, Min Gao, Qisheng You, Jie Wang, Christina J. Flaxel, Steven T. Bailey, Thomas S. Hwang, and Yali Jia, ``Multi-plexus nonperfusion area segmentation in widefield OCT angiography using a deep convolutional neural network,'' \emph{Translational Vision Science \& Technology}, vol. 13, no. 7, pp. 15 2024.

\bibitem{56} Guangxu Li, Kang Wang, Yining Dai, Dongping Zheng, Kailu Wang, Lizhen Zhang, Tohru Kamiya ``Physics-based optical coherence tomography angiography (OCTA) image correction for shadow compensation,'' \emph{IEEE Transactions on Biomedical Engineering}, 2024 (early access).

\bibitem{57} Wenxin Zhang, Bin He, Yangkang Wu, Yuxiu Tao, Fu Zhu, Wenchao Cai, Ning Liu, Qiang Zhao, and Ping Xue, ``Tail artifacts removal of three-dimensional optical coherence tomography angiography with common parts extraction method,'' \emph{Journal of Biophotonics}, vol. 15, no. 11, pp. e202200155, 2022.

\bibitem{58} Junxiong Zhou, Yuntao Li, and Jianbo Tang, ``Adaptive dynamic analysis-based optical coherence tomography angiography for blood vessel projection artifact suppression,'' \emph{Biomedical Optics Express}, vol. 14, no. 1, pp. 477-488, 2023.

\bibitem{59} Jie Wang, Tristan T. Hormel, Steven T. Bailey, Thomas S. Hwang, David Huang, and Yali Jia, ``Signal attenuation-compensated projection-resolved OCT angiography,'' \emph{Biomedical Optics Express}, vol. 14, no. 5, pp. 2040-2054, 2023.

\bibitem{60} Zhe Jiang, Zhiyu Huang, Bin Qiu, Xiangxi Meng, Yunfei You, Xi Liu, Gangjun Liu, Chuangqing Zhou, Kun Yang, Andreas Maier, Qiushi Ren, and Yanye Lu, ``Comparative study of deep learning models for optical coherence tomography angiography,'' \emph{Biomedical Optics Express}, vol. 11, no. 3, pp. 1580-1597, 2020.

\bibitem{61} Min Gao, Yukun Guo, Tristan T. Hormel, Jiande Sun, Thomas S. Hwang, and Yali Jia, ``Reconstruction of high-resolution $6 \times 6$-mm OCT angiograms using deep learning,'' \emph{Biomedical Optics Express}, vol. 11, no. 7, pp. 3585-3600, 2020.

\bibitem{62} Min Gao, Tristan T. Hormel, Jie Wang, Yunkun Guo, Steven T. Bailey, Thomas S. Hwang, and Yali Jia, ``An open-source deep learning network for reconstruction of high-resolution OCT angiograms of retinal intermediate and deep capillary plexuses,'' \emph{Translational Vision Science \& Technology}, vol. 10, no. 13, pp. 13, 2021.

\bibitem{63} Zhe Jiang, Zhiyu Huang, Bin Qiu, Xiangxi Meng, Yunfei You, Xi Liu, Mufeng Geng, Gangjun Liu, Chuanqing Zhou, Kun Yang, Andreas, Maier, Qiushi Ren, and Yanye Lu``Weakly supervised deep learning-based optical coherence tomography angiography,'' \emph{IEEE Transactions on Medical Imaging}, vol. 40, no. 2, pp. 688-698, 2021.

\bibitem{64} Xing Yuan, Yanping Huang, Lin An, Jia Qin, Gongpu Lan, Haixia Qiu, Bo Yu, Haibo Jia, Shangjie Ren, Haishu Tan, and Jingjiang Xu, ``Image enhancement of wide-field retinal optical coherence tomography angiography by super-resolution angiogram reconstruction generative adversarial network,'' \emph{Biomedical Signal Processing and Control}, vol. 78, pp. 103957, 2022.

\bibitem{65} Zhe Jiang, Zhiyu Huang, Yunfei You, Mufeng Geng, Xiangxi Meng, Bin Qiu, Lei Zhu, Mengdi Gao, Jing Wang, Chuanqing Zhou, Qiushi Ren, and Yanye Lu, ``Rethinking the neighborhood information for deep learning-based optical coherence tomography angiography,'' \emph{Medical Physics}, vol. 49, no. 6, pp. 3705-3716, 2022.

\bibitem{66} David Le, Taeyoon Son, Tae-Hoon Kim, Tobiloba Adejumo, Mansour Abtahi, Shaiban Ahmed, Alfa Rossi, Behrouz Ebrahimi, Albert Dadzie, Guangying Ma, Jennifer I. Lim, and Xincheng Yao, ``Deep learning-based optical coherence tomography angiography image construction using spatial vascular connectivity network,'' \emph{Communications Engineering}, vol. 3, no. 28, 2024.

\bibitem{67} Kun Huang, Na Su, Yuhui Tao, Mingchao Li, Xiao Ma, Zexuan Ji, Songtao Yuan, and Qiang Chen ``Cross-device OCTA generation by patch-based 3D multi-scale feature adaption,'' \emph{IEEE Transactions on Emerging Topics in Computational Intelligence}, vol. 8, no. 1, 2024.

\bibitem{68} Yixiao Jin, Fu Gui, Minghao Chen, Xiang Chen, Haoxuan Li, and Jingfa Zhang, ``Deep learning-driven automated quality assessment of ultra-widefield optical coherence tomography angiography images for diabetic retinopathy,'' \emph{The Visual Computer}, vol. 41, pp. 1049-1059, 2024.

\bibitem{69} Nosein Nouri, Reza Nasri, and Seyed-Hossein Abtahi, ``Addressing inter-device variantions in optical coherence tomography angiography: will image-to-image translation systems help?'' \emph{International Journal of Reina and Vitreous}, vol. 9, no. 1, pp. 51, 2023.

\bibitem{70} Rahul M. Dhodapkar, Emily Li, Kristen Nwanyanwu, Ron Adelman, Smita Krishnaswamy, and Jay C. Wang, ``Deep learning for quality assessment of optical coherence tomography angiography images,'' \emph{Scientific Reports}, vol. 12, pp. 13775, 2022.

\bibitem{71} Ylenia Giarratano, Eleonora Bianchi, Calum Gray, Andrew Morris, Tom MacGillivray, Baljean Dhillon, and Miguel O. Bernabeu, ``Automated segmentation of optical coherence tomography angiography images: benchmark data and clinically relevant metrics,'' \emph{Translational Vision Science \& Technology}, vol. 9, no.13, pp. 5, 2020.

\bibitem{72} Theodoros Pissas, Edward Bloch, M. Jorge Cardoso, Blanca Flores, Odysseas Georgiadis, Sepehr Jalali, Claudio Ravasio, Danail Stoyanov, Lyndon Da Cruz, and Christos Bergeles, ``Deep iterative vessel segmentation in OCT angiography,'' \emph{Biomedical Optics Express}, vol. 11, no.5, pp. 2490-2510, 2020.

\bibitem{73} Julian Lo, Morgan Heisler, Vinicius Vanzan, Sonja, Karst, Ivana Zadro Matovinovi\'c, Sven Lončari\'c, Eduardo V. Navajas, Mirza Faisal Beg, and Marinko V. Šaruni\'c, ``Microvasculature segmentation and intercapillary area quantification of deep vascular complex using transfer learning,'' \emph{Translational Vision Science \& Technology}, vol. 9, no.2, pp. 38, 2020.

\bibitem{74} Xiao Tan, Xinjian Chen, Qingquan Meng, Fei Shi, Dehui Xiang, Zhongyue Chen, Lingjiao Pan, and Weifang Zhu, ``OCT\textsuperscript{2}Former: a retinal OCT-angiography vessel segmentation transformer,'' \emph{Computer Methods and Programs in Biomedicine}, vol. 233, pp.107454, 2023.

\bibitem{75} Zidi Shi, Yu Li, Hua Zou, and Xuedong Zhang, ``TCU-Net: transformer embedded in convolutional U-shaped network for retinal vessel segmentation,'' \emph{Sensors}, vol. 23, no. 10, pp. 4897, 2023.

\bibitem{76} Mingchao Li, Yerui Chen, Zexuan Ji, Keren Xie, Songtao Yuan, Qiang Chen, and Shuo Li ``Image projection network: 3D to 2D image segmentation in OCTA images,'' \emph{IEEE Transactions on Medical Imaging}, vol. 19, no. 11, pp. 3343-3354, 2020.

\bibitem{77} Yuhui Ma, Huaying Hao, JianYang Xie, Huazhu Fu, Jiong Zhang, and Jianlong Yang, ``ROSE: a retinal OCT-angiography vessel segmentation dataset and new model,'' \emph{IEEE Transactions on Medical Imaging}, vol. 40, no.3, pp. 928-939, 2020.

\bibitem{78} ChengZhang Zhu, Han Wang, Yalong Xiao, Yulan Dai, Zixi Liu, and Beiji Zou, ``OVS-Net: an effective feature extraction network for optical coherence tomography angiography vessel segmentation,'' \emph{Computer Animation and Virtual Worlds}, vol. 33, no. 3-4, pp. e2096, 2023.

\bibitem{79} Xiaoming Liu, Di Zhang, Xin Zhu, and Jinshan Tang, ``VCT-NET: an OCTA retinal vessel segmentation network based on convolution and transformer,'' \emph{2022 IEEE International Conference on Image Processing (ICIP)}, pp. 2656-2660, 2022.

\bibitem{80} Xiaoming Liu, Di Zhang, Yunping Yao, and Jinshan Tang, ``Transformer and convolutional based dual branch network for retinal vessel segmentation in OCTA images,'' \emph{Biomedical Signal Processing and Control}, vol. 83, pp. 104604, 2023.

\bibitem{81} Xin Zhao, Jing Zhang, Qiaozhe Li, Tengfei Zhao, Yi Li, and Zifeng Wu, ``Global and local multi-modal feature mutual learning from retinal vessel segmentation,'' \emph{Pattern Recognition}, vol. 151, pp. 110376, 2024.

\bibitem{82} Yihao Liu, Aaron Carass, Lianrui Zuo, Yufan He, Shuo Han, Lorenzo Gregori, Sean Murray, Rohit Mishra, Jianqin Lei, Peter A. Calabresi, Shiv Saidha, and Jerry L. Prince ``Disentangled representation learning for OCTA vessel segmentation with limited training data,'' \emph{IEEE Transactions on Medical Imaging}, vol. 41, no.12, pp. 3686-3698, 2022.

\bibitem{83} Amrest Chinkamol, Vetit Kanjaras, Phattarapong Sawangjai, Yitian Zhao, Thapanun Sudhawiyangkul, and Chantana Chantrapornchai, ``OCTAve: 2D \textit{en face} optical coherence tomography angiography vessel segmentation in weakly-supervised learning with locality augmentation,'' \emph{IEEE Transactions on Biomedical Engineering}, vol. 70, no.6, pp. 1931-1942, 2023.

\bibitem{84} Hailan Shen, Zheng Tang, Yajing Li, Xuanchu Duan, and Zailiang Chen, ``HAIC-NET: semi-supervised OCTA vessel segmentation with self-supervised pretext task and dual consistency training,'' \emph{Pattern Recognition}, vol. 151, pp. 110429, 2024.

\bibitem{85} Yanyu Xu, et al., ``Partially-supervised learning for vessel segmentation in ocular images,'' \emph{Conference on Medical Image Computing and Computer-Assisted Intervention-MICCAI 2021: 24th International Conference}. France. Springer International Publishing, pp. 601-611, 2021.

\bibitem{86} Linus Kreitner, Johannes C. Paetzold, Nikolaus Rauch, Chen Chen, Ahmed M. Hagag, and Alaa E. Fayed, ``Synthetic optical coherence tomography angiographs for detailed retinal vessel segmentation without human annotations,'' \emph{IEEE Transactions on Medical Imaging}, vol. 43, no.6, pp. 2061-2073, 2024.

\bibitem{87} Martin Menten, Johannes C. Paetzold, Alina Dima, Bjoern H. Menze, Benjamin Knier, and Daniel Rueckert, ``Physiology-based simulation of the retinal vasculature enables annotation-free segmentation of OCT angiographs,'' \emph{International Conference on Medical Image Computing and Computer-Assisted Intervention}. Cham: Springer Nature Switzerland, pp. 330-340, 2022.

\bibitem{88} Dewei Hu, Can Cui, Hao Li, Kathleen E. Larson, Yuankai K. Tao, and Ipek Oguz, ``LIFE: a generalizable autodidactic pipeline for 3D OCT-A vessel segmentation,'' \emph{Medical Image Computing and Computer Assisted Intervention-MICCAI 2021: 24th International Conference}. France, Springer International Publishing, pp. 514-521, 2021.

\bibitem{89} Kai Hu, Shuai Jiang, Yuan Zhang, Xuanya Li, Xieping Gao, ``Joint-Seg: treat foveal avascular zone and retinal vessel segmentation in OCTA images as a joint task,'' \emph{IEEE Transactions on Instrumentation and Measurement}, vol. 71, pp. 4007113, 2022.

\bibitem{90} Linkai Peng, Li Lin, Pujin Cheng, Zhonghua Wang, and Xiaoying Tang, ``FARGO: a joint framework for FAZ and RV segmentation from OCTA images,''  \emph{Ophthalmic Medical Image Analysis: 8th International Workshop}. France, Springer International Publishing, pp. 42-51, 2021.

\bibitem{91} JinKui Hao, Ting Shen, Xueli Zhu, Yonghuai Liu, Ardhendu Behera, Dan Zhang, Bang Chen, Jiang Liu, Jiong Zhang, and Yitian Zhao, ``Retinal structure detection in OCTA image via voting-based multitask learning,'' \emph{IEEE Transactions on Medical Imaging}, vol. 41, no.12, pp. 3969-3980, 2022.

\bibitem{92} Weisheng Li, Hongchuan Zhang, Feiyan Li, and Linhong Wang, ``RPS-Net: an effective retinal image projection segmentation network for retinal vessels and foveal avascular zone based on OCTA data,'' \emph{Medical Physics}, vol. 49, no. 6, pp. 3830-3844, 2022.

\bibitem{93} Minhaj Nur Alam, David Le, and Xincheng Yao, ``Differential artery-vein analysis in quantitative retinal imaging: a review,'' \emph{Quantitative Imaging in Medicine and Surgery}, vol. 11, no.3, pp. 1102-1119, 2021.

\bibitem{94} Gilbert T. Feke, Bradley T. Hyman, Robert A. Stern, and Louis R. Pasquale, ``Retinal blood flow in mild cognitive impairment and Alzheimer's disease,'' \emph{Alzheimer's \& Dementia: Diagnosis, Assessment \& Disease Monitoring}, vol. 1, no.2, pp. 144-151, 2015.

\bibitem{95} Minhaj Alam, David Le, Taeyoon Son, Jennifer I. Lim, and Xincheng Yao, ``AV-Net: deep learning for fully automated artery-vein classification in optical coherence tomography angiography,'' \emph{Biomedical Optics Express}, vol. 11, no.9, pp. 5249-5257, 2020.

\bibitem{96} Min Gao, Yukun Guo, Tristan T. Hormel, Kotaro Tsuboi, George Pacheco, David Poole, Steven T. Bailey, Christina J. Flaxel, David Huang, Thomas, and Yali Jia, ``A deep learning network for classifying arteries and veins in montaged widefield OCT angiograms,'' \emph{Ophthalmology Science}, vol. 2, no.2, pp. 100149, 2022.

\bibitem{97} Mansour Abtahi, David Le, Jennifer I. Lim, and Xincheng Yao, ``MF-AV-Net: an open-source deep learning network with multimodal fusion options for artery-vein segmentation in OCT angiography,'' \emph{Biomedical Optics Express}, vol. 13, no.9, pp. 4870-4888, 2022.

\bibitem{98} Mansour Abtahi, David Le, Behrouz Ebrahimi, Albert K. Dadzie, Jennifer I. Lim, and Xincheng Yao, ``An open-source deep learning network AVA-Net for arterial-venous area segmentation in optical coherence tomography angiography,'' \emph{Communications Medicine}, vol. 3, no. 1, pp.54, 2023.

\bibitem{99} Mehta Ishibazawa, Nihaal Mehta, Osama Sorour, Phillip Braun, Sarah Martin, A. Yasin Alibhai, Adnan Saifuddin, Malvika Arya, Caroline R. Baumal, Jay S. Duker, and Nadia K. Waheed, ``Accuracy and reliability in differentiating retinal arteries and veins using widefield \textit{en face} OCT angiography,'' \emph{Translational Vision Science \& Technology}, vol. 8, no. 3, pp. 60, 2019.

\bibitem{100} Minhai Alam, Jennifer I. Lim, Devrim Toslak, and Xincheng Yao, ``Differential artery-vein analysis improves the performance of OCTA staging of sickle cell retinopathy,'' \emph{Translational Vision Science \& Technology}, vol. 8, no.2, pp. 3,  2019.

\bibitem{101} Yanling Ouyang, Qing Shao, Dirk Scharf, Antonia M Joussen, and Florian M Heussen, ``An easy method to differentiate retinal arteries from veins by spectral domain optical coherence tomography: retrospective, observational case series,'' \emph{BMC Ophthalmology}, vol. 14, no. 66, pp. 1-9, 2014.

\bibitem{102} Thomas S. Hwang, Ahmed M. Hagag, Jie Wang, Miao Zhang, Andrew Smith, David J. Wilson, David Huang, and Yali Jia, ``Automated quantification of nonperfusion areas in 3 vascular plexuses with optical coherence tomography angiography in eyes of patients with diabetes,'' \emph{JAMA Ophthalmology}, vol. 136, no. 8, pp. 929-936, 2018.

\bibitem{103} Richard F. Spaide, ``Retinal vascular cystoid macular edema: review and new theory,'' \emph{Retina}, vol. 36, no.10, pp. 1823-1842, 2016.

\bibitem{104} Mary Ho, Frank Hiu Ping Lai, Danny Siu Chun Ng, Lawrence Pui Leung lu, Li Jia Chen, Andrew Chun Yue Mak, Yolanda Yip, Carol Cheung, Alvin Lerrmann Young, and Marten Brelen, ``Analysis of choriocapillaris perfusion and choroidal layer changes in patients with chronic central serous chorioretinopathy randomised to micropulse laser or photodynamic therapy,'' \emph{British Journal of Ophthalmology}, vol. 105, no. 4, pp. 555-560, 2021.

\bibitem{105} Jayakrishna Ambati, and Benjamin J. Fowler, ``Mechanisms of age-related macular degeneration,'' \emph{Neuron}, vol. 75, no.1, pp. 26-39, 2012.

\bibitem{106} Jie Wang, Tristan T. Hormel, Qisheng You, Yukun Guo, Xiaogang Wang, Liu Chen, Thomas S. Hwang, and Yali Jia, ``Robust non-perfusion area detection in three retinal plexuses using convolutional neural network in OCT angiography,'' \emph{Biomedical Optics Express}, vol. 11, no.1, pp. 330-435, 2020.

\bibitem{107} Yukun Guo, Tristan T. Hormel, Min Gao, Qisheng You, Jie Wang, Christina J. Flaxel, Steven T. Bailey, Thomas S. Hwang, and Yali Jia, ``Multi-plexus nonperfusion area segmentation in widefield OCT angiography using a deep convolutional neural network,'' \emph{Translational Vision Science Technology}, vol. 13, no.7, pp. 15, 2024.

\bibitem{108} Emilio L\`pez-Varela, Joaquim de Moura, Jorge Novo, Jos\'e Ignacio Fern\'andez-Vigo, Francisco Javier Moreno-Morillo, and Marcos Ortega, ``Fully automatic segmentation and monitoring of choriocappilaris flow voids in OCTA images,'' \emph{Computerized Medical Imaging and Graphics}, vol. 104, pp.102172, 2023.

\bibitem{109} He-Yan Li, Dai-Xi Wang, Li Dong, and Wen-Bin Wei, ``Deep learning algorithms for detection of diabetic macular edema in OCT images: a systematic review and meta analysis,'' \emph{European Journal of Ophthalmology}, vol. 33, no. 1, pp. 278-290, 2023.

\bibitem{110} Alejandra Daruich, Alexandre Matet, Alexandre Moulin, Laura Kowalczuk, Micha\"el Nicolas, Alexandre Sellam, Pierre-Rapha\"el Rothschild, Samy Omri, Emmanuelle G\'eliz\'e, Laurent Jonet, Kimberley Delaunay, Yvonne De Kozak, Marianne Berdugo, Min Zhao, and Patricia Crisanti, ``Mechanisms of macular edema: beyond the surface,'' \emph{Progress in Retinal and Eye Research}, vol. 63, pp. 20-68, 2018.

\bibitem{111} Yukun Guo, Tristan T. Hormel, Honglian Xiong, Jie Wang, Thomas S. Hwang, and Yali Jia, ``Automated segmentation of retinal fluid volumes from structural and angiographic optical coherence tomography using deep learning,'' \emph{Translational Vision Science \& Technology}, vol. 9, no.2, pp. 54, 2020.

\bibitem{112} Akihiro Ishibazawa, Taiji Nagaoka, Harumasa Yokota, Atsushi Takahashi, Tsuneaki Omae, Young-Seok Song, Tatsuhisa Takahashi, and Akitoshi Yoshida, ``Characteristics of retinal neovascularization in proliferative diabetic retinopathy imaged by optical coherence tomography angiography,'' \emph{Investigative Ophthalmology \& Visual Science}, vol. 57, no. 14, pp. 6247-6255, 2016.

\bibitem{113} Murat Karacorlu, Isil Sayman Muslubas, Serra Arf, Mumin Hocaoglu, and M. Giray Ersoz, ``Membrane patterns in eyes with choroidal neovascularization on optical coherence tomography angiography,'' \emph{Eye}, vol. 33, no.8, pp. 1280-1289, 2019.

\bibitem{114} Yar Zar Tun, and Pakinee Aimmanee, ``A complete review of automatic detection, segmentation and quantification of neovascularization in optical coherence tomography angiography images,'' \emph{Diagnostics}, vol. 13, no. 22, pp. 3407, 2023.

\bibitem{115} Xiang-ning Wang, Zhouyu Guan, Bo Qian, Tingli Chen, and Qiang Wu, ``A deep learning system for the detection of optic disc neovascularization in diabetic retinopathy using optical coherence tomography angiography images,'' \emph{The Visual Computer}, vol. 41, pp. 1293-1302, 2024.

\bibitem{116} Aishwarya Murali, Subramanian Krishnakumar, Anuradha Subramanian, and Sowmya Parameswaran, ``Bruch's membrane pathology: a mechanistic perspective,'' \emph{European Journal of Ophthalmology}, vol. 30, no. 6, pp. 1195-1206, 2020.

\bibitem{117} Julia Schottenhamml, Eric M. Moult, Stefan B. Ploner, Siyu Chen, Eduardo Novais, Lennart Husvogt, Jay S. Duker, Nadia K. Waheed, James G. Fujimoto, and Andreas K. Maier, ``OCT-OCTA segmentation: combining structural and blood flow information to segment Bruch's membrane,'' \emph{Biomedical Optics Express}, vol. 12, no. 1, pp. 84-99, 2020.

\bibitem{118} Levi Reina Fernandes, Carlos Arce, Goncalo Martinho, Jo\~ao Pedro Campos, and R. Michael Meneghini, ``Accuracy, reliability and repeatability of a novel artificial intelligence algorithm converting two-dimensional radiographs to three-dimensional bone models for total knee arthroplasty,'' \emph{The Journal of Arthroplasty}, vol. 38, no. 10, pp.  2032-2036, 2023.

\bibitem{119} Andr\'e Ferreira, Jianning Li, Kelsey L. Pomykala, Jens Kleesiek, Victor Alves, and Jan Egger, ``GAN-based generation of realistic 3D volumetric data: a systematic review and taxonamy,'' \emph{Medical Image Analysis}, vol. 93, pp. 103100, 2024.

\bibitem{120} Jiong Zhang, Yuchuan Qiao, Mona Sharifi Sarabi, Maziyar M. Khansari, Jin K. Gahm, Amir H. Kashani, and Yonggang Shi, ``3D shape modeling and analysis of retinal microvasculature in OCT-Angiography images,'' \emph{IEEE Transactions on Medical Imaging}, vol. 39, no. 5, pp. 1335-1346, 2020.

\bibitem{121} Takayuki Okamoto, Hiroki Okamura, Takehito Iwase, Tomohiro Niizawa, Yuto Kawamata, Hirotaka Yokouchi, Takayuki Baba, and Hideaki Haneishi, ``Three-dimensional vascular graph construction from depth information of blood vessel centerlines in optical coherence tomography angiography,'' \emph{Optics Continuum}, vol. 3, no. 7, pp. 1132-1148, 2024.

\bibitem{122} Shuai Yu, Jianyang Xie, Jinkui Hao, Yalin Zheng, Jiong Zhang, Yan Hu, Jiang Liu, and Yitian Zhao ``3D vessel reconstruction in OCT angiography via depth map estimation,'' \emph{2021 IEEE 18th International Symposium on Biomedical Imaging}, France, pp. 1609-1613, 2021.

\bibitem{123} Shuai Yu, Yonghuai Liu, Jiong Zhang, Jianyang Xie, Yalin Zheng, Jiang Liu, and Yitian Zhao, ``Cross-domain depth estimation network for 3D vessel reconstruction in OCT angiography,'' \emph{Medical Image Computing and Computer Assisted Intervention-MICCAI 2021: 24th International Conference}, France, Springer International Publishing, pp. 13-23, 2021.

 \bibitem{124} Kaveri Thakoor, Darius Bordbar, Jiaang Yao, Omar Moussa, Royce Chen, Paul Sajda, ``Hybrid 3D-2D deep learning for detection of neovascular age-related macular degeneration using optical coherence tomography B-scans and angiography volumes,'' \emph{2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)}, France, pp. 1600-1604, 2021.
 
 \bibitem{125} Mingchao Li, Kun Huang, Chaofan Zeng, Qiang Chen, and Weiwei Zhang, ``Visualization and quantization of 3D retinal vessels in OCTA images,'' \emph{Optics Express}, vol. 32, no. 1, pp. 471-481, 2024.
 
 \bibitem{126} Peter Bankhead, Norman Scholfield, J. Graham McGeown, and Tim M. Curtis, ``Fast retinal vessel detection and measurement using wavelets and edge location refinement,'' \emph{PloS One}, vol. 7, no. 3, pp. e32435, 2012.
  
\bibitem{127} Fausto Bernardini, Joshua Mittleman, Holly Rushmeier, Cl\'audio Silva, and Gabriel Taubin, ``The ball-pivoting algorithm for surface reconstruction,'' \emph{IEEE Transactions on Visualization and Computer Graphics}, vol. 5, no. 4, pp. 349-359, 1999.

\bibitem{128} William Schroeder, Lisa Sobierajski Avila, and Willian Hoffman, ``Visualizing with VTK: a tutorial,'' \emph{IEEE Computer Graphics and applications}, vol. 20, no. 5, pp. 20-27, 2000.

\bibitem{129} Detlev Stalling, Malte Westerhoff, and Hans-Christian Hege, ``Amira: a highly interactive system for visual data analysis,'' \emph{The visualization handbook}, vol. 38, pp. 749-767, 2005.

\bibitem{130} Macarena Diaz, Jorge Novo, Paula Cutrin, Francisco G\'omez-Ulla, Manuel G. Penedo, and Marcos Ortega, ``Automatic segmentation of the foveal avascular zone in ophthalmological OCT-A images,'' \emph{PloS One}, vol. 14, no.2, pp. e0212364, 2019.

\bibitem{131} Arpit Agarwal, Rajiv Raman, and Vasudevan Lakshminarayanan,  ``The foveal avascular zone image database (fazid),'' \emph{ Applications of Digital Image Processing XLIII,}, vol. 11510, pp. 507-512, 2020.

\bibitem{132} Mingchao Li, Kun Huang, Qiuzhuo Xu, Jiadong Yang, Yuhan Zhang, Zexuan Ji, Keren Xie, Songtao Yuan, Qinghuai Liu, Qiang and Chen, ``OCTA-500: a retinal dataset for optical coherence tomography angiography study,'' \emph{Medical Image Analysis}, vol. 93, pp. 103092, 2024.
 
\bibitem{133} Jingyan Xue, Zhenhua Feng, Lili Zeng, Shuna Wang, Xuezhong Zhou, Jianan Xia, and Aijun Deng, ``Soul: an OCTA dataset based on human machine collaborative annotation framework,'' \emph{Scientific Data}, vol. 11, no. 838, 2024.

\bibitem{134} Yuefei Wang, Yiqing Shen, Meng Yuan, Jing Xu, Bin Yang, Chi Liu, Wenjia Cai, Weijing Cheng, and Wei Wang, ``A deep learning-based quality assessment and segmentation system with a large-scale benchmark dataset for optical coherence tomographic angiography image,'' \emph{arXiv preprint arXiv}: 2107.10476, 2021.

\bibitem{135} Tao Li, Wang Bo, Chunyu Hu, Hong Kang, Hanruo Liu, Kai Wang, and Huazhu Fu, ``Applications of deep learning in fundus images: a review,'' \emph{Medical Image Analysis}, vol. 69, pp. 101971, 2021.

\bibitem{136} Yitong Li, Ruiheng Zhang, Li Dong, Xuhan Shi, Wenda Zhou, Haotian Wu, Heyan Li, Chuyao Yu, and Wenbin Wei, ``Predicting systemic disease in fundus images: systematic review of setting, reporting, bias, and models' clinical availability in deep learning studies,'' \emph{Eye}, vol. 38, pp. 1246-1251, 2024.

\bibitem{137} Ikram Amna, Azhar Imran, Jianqiang Li, Abdulaziz Alzubaidi, Safa Fahim, Amanullah Yasin, and Hanaa Fathi, ``A systematic review on fundus image-based diabetic retinopathy detection and grading: current status and future directions,'' \emph{IEEE Access}, vol 12, pp. 96273-96303, 2024.

\bibitem{138} Jiajia Li, Zhouyu Guan, Jing Wang, Carol Y. Cheung, Yingfeng Zheng, Lee-Ling Lim, Cynthia Ciwei Lim, Paisan Ruamviboonsuk, Rajiv Raman, Leonor Corsino, Justin B. Echouffo-Tcheugui, Andrea O. Y. Luk, Li Jia Chen, Xiaodong Sun, Haslina Hamzah, Qiang Wu, Xiangning Wang, Ruhan Liu, Ya Xing Wang, Tingli Chen, Xiao Zhang, Xiaolong Yang, Jun Yin, Jing Wan, Wei Du, Ten Cheer Quek, Jocelyn Hui Lin Goh, Dawei Yang, Xiaoyan Hu, Truong X. Nguyen, Simon K. H. Szeto, Peranut Chotcomwongse, Rachid Malek, Nargiza Normatova, Nilufar Ibragimova, Ramyaa Srinivasan, Pingting Zhong, Wenyong Huang, Chenxin Deng, Lei Ruan, Cuntai Zhang, Chenxi Zhang, Yan Zhou, Chan Wu, Rongping Dai, Sky Wei Chee Koh, Adina Abdullah, Nicholas Ken Yoong Hee, Hong Chang Tan, Zhong Hong Liew, Carolyn Shan-Yeu Tien, Shih Ling Kao, Amanda Yuan Ling Lim, Shao Feng Mok, Lina Sun, Jing Gu, Liang Wu, Tingyao Li, Di Cheng, Zheyuan Wang, Yiming Qin, Ling Dai, Ziyao Meng, Jia Shu, Yuwei Lu, Nan Jiang, Tingting Hu, Shan Huang, Gengyou Huang, Shujie Yu, Dan Liu, Weizhi Ma, Minyi Guo, Xinping Guan, Xiaokang Yang, Covadonga Bascaran, Charles R. Cleland, Yuqian Bao, Elif I. Ekinci, Alicia Jenkins, Juliana C. N. Chan, Yong Mong Bee, Sobha Sivaprasad, Jonathan E. Shaw, Rafael Simó, Pearse A. Keane, Ching-Yu Cheng, Gavin Siew Wei Tan, Weiping Jia, Yih-Chung Tham, Huating Li, Bin Sheng, and Tien Yin Wong, ``Integrated image-based deep learning and language models for primary diabetes care,'' \emph{Nature Medicine}, vol 30, no. 10, pp. 2886-2896, 2024.
 
\end{thebibliography}

\end{document}
