\section{Related Works}
\label{related}


Several methods have been proposed for global optimization, with the simplest being non-adaptive exhaustive searches, such as grid search, which uniformly divides the space into representative points \citep{zabinsky2013stochastic}, or its stochastic alternative, Pure Random Search (PRS), which employs random uniform sampling \citep{brooks1958discussion, zabinsky2013stochastic}. However, these methods are often inefficient, as they fail to exploit previously gathered information or the underlying structure of the objective function \citep{zabinsky2013stochastic}.

To enhance efficiency, adaptive methods have been developed that leverage collected data and local smoothness. Some of these methods need the knowledge of the local smoothness, including HOO \citep{bubeck2011x}, Zooming \citep{kleinberg2008multi}, and DOO \citep{munos2011optimistic}, while others do not, such as SOO \citep{munos2011optimistic, preux2014bandits, kawaguchi2016global} and SequOOL \citep{pmlr-v98-bartlett19a}. In this work, however, we focus on Lipschitz functions. 

To address Lipschitz functions with unknown Lipschitz constants, the DIRECT algorithm \citep{jones1993lipschitzian, jones2021direct} employs a deterministic splitting approach of the whole space, sequentially dividing and evaluating the function over subdivisions that have recorded the highest upper bounds. 

More recently, \cite{malherbe2017global} introduced AdaLIPO, an adaptive stochastic no-regret strategy that estimates the Lipschitz constant through uniform random sampling, which is then used to identify potentially optimal maximizers based on previously explored points. Later, AdaLIPO+ \citep{serre2024lipo+} was introduced as an empirical enhancement over it, reducing the exploration probability over time. Both approaches optimize the search space using an acceptance condition, yet they necessitate additional uniform random evaluations, making them less efficient in small-budget scenarios.

Under alternative assumptions, various global optimization methods have been proposed. For instance, Bayesian optimization (BO) \citep{fernando2014bayes, 7352306, frazier2018tutorial, balandat2020botorch} constructs a probabilistic model of the objective function and uses it to evaluate the most promising points, making it particularly effective for global optimization. 

While several BO algorithms are theoretically guaranteed to converge to the global optimum of the unknown function, they often rely on the assumption that the kernel's hyperparameters are known in advance. To address this limitation, hyperparameter-free approaches such as Adaptive GP-UCB (A-GP-UCB) \citep{JMLR:v20:18-213} have been proposed. More recently, \citet{JMLR:v23:21-0888} introduced SMAC3 as a robust baseline for global optimization. In our empirical evaluation, we show that ECP outperforms these recent baselines from BO.


Other approaches, such as CMA-ES \citep{hansen1996adapting, hansen2006cma, hansen2019pycma}, and simulated annealing \citep{metropolis1953equation, kirkpatrick1983optimization}, later extended to Dual Annealing \citep{xiang1997generalized, tsallis1988possible, tsallis1996generalized}, are also notable, although they do not guarantee no-regret \citep{malherbe2017global} or theoretical finite-budget guarantees for Lipschitz functions.

Other related approaches include contextual bandits \citep{auer2002using, langford2007epoch, filippi2010parametric, valko2013finite, lattimore2020bandit}, such as the NeuralUCB algorithm \citep{zhou2020neural}, which leverages neural networks to estimate upper-confidence bounds. While NeuralUCB is not primarily designed for global maximization, it can be adapted by randomly sampling points, estimating their bounds, evaluating the point with the highest estimate, and retraining the network. However, it may be inefficient for small budgets, as neural networks require a large number of samples to train effectively. Finally, other works on bandits address black-box discrete function maximization \citep{fourati2023randomized, fourati2024combinatorial, pmlr-v235-fourati24b}, which is not the focus of this work.