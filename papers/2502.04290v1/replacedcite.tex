\section{Related Works}
\label{related}


Several methods have been proposed for global optimization, with the simplest being non-adaptive exhaustive searches, such as grid search, which uniformly divides the space into representative points ____, or its stochastic alternative, Pure Random Search (PRS), which employs random uniform sampling ____. However, these methods are often inefficient, as they fail to exploit previously gathered information or the underlying structure of the objective function ____.

To enhance efficiency, adaptive methods have been developed that leverage collected data and local smoothness. Some of these methods need the knowledge of the local smoothness, including HOO ____, Zooming ____, and DOO ____, while others do not, such as SOO ____ and SequOOL ____. In this work, however, we focus on Lipschitz functions. 

To address Lipschitz functions with unknown Lipschitz constants, the DIRECT algorithm ____ employs a deterministic splitting approach of the whole space, sequentially dividing and evaluating the function over subdivisions that have recorded the highest upper bounds. 

More recently, ____ introduced AdaLIPO, an adaptive stochastic no-regret strategy that estimates the Lipschitz constant through uniform random sampling, which is then used to identify potentially optimal maximizers based on previously explored points. Later, AdaLIPO+ ____ was introduced as an empirical enhancement over it, reducing the exploration probability over time. Both approaches optimize the search space using an acceptance condition, yet they necessitate additional uniform random evaluations, making them less efficient in small-budget scenarios.

Under alternative assumptions, various global optimization methods have been proposed. For instance, Bayesian optimization (BO) ____ constructs a probabilistic model of the objective function and uses it to evaluate the most promising points, making it particularly effective for global optimization. 

While several BO algorithms are theoretically guaranteed to converge to the global optimum of the unknown function, they often rely on the assumption that the kernel's hyperparameters are known in advance. To address this limitation, hyperparameter-free approaches such as Adaptive GP-UCB (A-GP-UCB) ____ have been proposed. More recently, ____ introduced SMAC3 as a robust baseline for global optimization. In our empirical evaluation, we show that ECP outperforms these recent baselines from BO.


Other approaches, such as CMA-ES ____, and simulated annealing ____, later extended to Dual Annealing ____, are also notable, although they do not guarantee no-regret ____ or theoretical finite-budget guarantees for Lipschitz functions.

Other related approaches include contextual bandits ____, such as the NeuralUCB algorithm ____, which leverages neural networks to estimate upper-confidence bounds. While NeuralUCB is not primarily designed for global maximization, it can be adapted by randomly sampling points, estimating their bounds, evaluating the point with the highest estimate, and retraining the network. However, it may be inefficient for small budgets, as neural networks require a large number of samples to train effectively. Finally, other works on bandits address black-box discrete function maximization ____, which is not the focus of this work.