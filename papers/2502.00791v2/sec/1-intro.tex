\section{Introduction}
\label{sec:intro}
The ability to process long-context is crucial for large language models (LLMs) to tackle real-world tasks such as long-document understanding~\cite{brown2020language,qwen}.
However, directly feeding long inputs that exceed the training length of the model results in significant performance degradation, increased memory overhead, and quadratic time complexity.
To reduce computational costs, several methods have been proposed, including sparse attention mechanisms~\cite{dao2022flashattention,liuringattention} and memory-augmented architectures~\cite{mohtashami2023random, tworkowski2024focused}. 
Despite these advancements, they struggle to maintain coherence and relevance among long texts in complex tasks, leading to performance degradation~\cite{gecontext}.

\input{figs/motivation}
In a different direction, recent work focused on adopting LLMs to compress long contexts, enabling generation conditioned on more concise representations. 
They can be categorized into two groups: 
\textbf{i)} \textit{Soft prompt-based} methods fine-tune LLMs to condense text into shorter summary tokens that retain key semantics~$_{\!}$\cite{chevalier2023adapting,gecontext}. 
\textbf{ii)} \textit{Selection-based} methods remove less important tokens based on information entropy (IE) computed by LLMs, thereby shortening the input~$_{\!}$\cite{li2023compressing, jiang2023llmlingua}. 
Though impressive, they face computational bottlenecks, as the entire long input must be processed by a heavy LLM to generate compressed tokens or IE.


An intriguing line of work~\cite{yen2024long} decouples long-context processing from the target LLM via a lightweight text encoder, which handles long contexts by breaking them into smaller chunks. Such processed contexts are then  integrated into the LLM by cross-attention.
This reduces LLM workload but retains the original token count, causing quadratic complexity in cross-attention and limiting scalability for longer sequences.



In contrast to these efforts, we compress long text from a novel perspective, \ie, vision-centric token compression, as shown in Figure~\ref{fig:motivation}. Recent advances in pixel-based methods have demonstrated the benefits of using visual text representations instead of traditional subword tokens$_{\!}$~\cite{rust2022language,tschannen2023clippo,gao2024improving}. 
\textit{By treating text as visual inputs, these methods eliminate the need for language-specific tokenizers and fixed vocabularies, offering seamless multilingual support and improved robustness to character-level noise}.
Additionally, previous studies$_{\!}$~\cite{radford2021learning, lin2025parrot} have shown visual encoders trained on paired image-text data naturally acquire OCR capabilities.
Compared to LLM, these visual encoders are more lightweight, offering a computationally efficient alternative for processing rendered text image. 
However, the potential of leveraging visual encoders to extend long-context capabilities in LLMs remains unexplored.


In this vein, we present \textbf{\ourname}, a vision-centric token compression in LLM. 
Specifically, \ourname\ transforms long textual contexts into images and leverages a lightweight vision encoder to extract compact visual features.
Since long texts often contain redundant elements, such as function words with minimal semantic value, we design Probability-Informed
Visual Enhancement (PVE) to emphasize content-rich tokens and enhance the ability of the vision encoder to capture meaningful semantics. 
Integrated with visual abstraction for context expansion and probability-driven visual semantic enhancement, \textit{\ourname\ effectively interprets text-heavy images, handles much longer inputs, and reduces both FLOPs and memory usage during training and inference}.



With the extended text context enabled by \ourname, our model achieves average improvements from 33.3\% to 46.3\% on in-context learning (ICL) tasks for LLM.
Despite the challenge of interpreting rendered images of extensive text passages through a vision encoder, \ourname\ surpasses text encoder-based methods in open-domain QA and ICL tasks, while achieving lower computational cost.















 




% 好处 1. 输入比训练更长的文本 性能并不会退化 2. 训练推理都能用 3. 

% \vspace{20cm}