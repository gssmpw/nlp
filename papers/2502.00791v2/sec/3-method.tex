
\section{Methodology}
\label{sec::method}
\input{figs/info_gain}


In this section, we present our method \ourname, which processes long in-context text by a lightweight visual encoder, effectively and efficiently extending the context length of large language models (LLMs).  
First, we outline the overall pipeline of \ourname. 
Next, we explain how long texts are rendered into RGB images (\S\ref{sec::vision-centric}) and then converted into compressed visual tokens using a frozen vision encoder and a trainable Perceiver Resampler~\cite{alayrac2022flamingo} (\S\ref{sec::token_reduction}). 
Finally, we present Probability-Informed Visual Enhancement (\S\ref{sec::FCL}), which guides Perceiver Resampler to extract rich textual information from rendered text images by using refined text token embeddings as supervision signals. 


\subsection{Overall Pipeline}

\ourname\ aims to extend the context window of existing Large language models (LLMs) by visual tokens and handle long input more efficiently. 
As illustrated in Figure~\ref{fig:overview}, the input long text (\ie, $T$ text tokens) is split into two parts: the first $T_\text{e}$ text tokens processed in a visual view and the remaining $T_\text{d}$ raw text tokens given to LLM, where $T = T_\text{e} + T_\text{d}$.  
Specifically, the $T_\text{e}$ text tokens are evenly rendered into $M$ images and fed into a frozen vision encoder. Then \ourname\ employs a learnable Perceiver Resampler to compress text-rendered image features into a fixed count of tokens. Such compressed image features are integrated into the LLM via cross-attention for the next-token prediction. 


To empower the model with the ability to comprehend dense text in images, we devise Probability-Informed Visual Enhancement (PVE, \S\ref{sec::FCL}). 
PVE maximizes agreement between visual features obtained from the Perceiver Resampler and text token embeddings extracted from the tokenizer in LLM. 
This alignment bridges the global semantic gap between visual tokens and raw text tokens. 
Furthermore, to address token redundancy, we incorporate a frequency-based masking mechanism within PVE that selectively masks high-frequency, low-information text tokens, thereby improving the information density of the text embeddings. 
These refined embeddings serve as enriched supervision signals, encouraging visual features to be more compact and semantically meaningful. 



\subsection{Vision-centric Implementation}
\label{sec::vision-centric}
\ourname\ transforms raw textual data into $M$ uniformly distributed RGB images $\mathcal{X}=\{x_m \in \mathbb{R}^{H\times W \times C}\}_{m=1}^{M}$, where $M$ can be dynamically adjusted based on the length of the input text. Concretely, each image is configured with height $H=14$, width $W=3,584$, and  $C=3$ RGB channels, which corresponds to a square color image with a resolution of $224\times224$. 
Text is rendered using a 10px font size and the Google \textit{Noto Sans} typeface. On average, one Llama token requires approximately 1.28 $14\times14$ image patches. 
If the text does not completely fill the image, white empty patches are masked to exclude them from attention score computation and loss calculation. 
Importantly, compared to tokenizer-based approaches, this rendering method does not lead to slower training speeds.

                                        
\subsection{Token Reduction}
\label{sec::token_reduction}
The $M$ text-rendered images are first processed by a frozen vision encoder, specifically the ViT-L/14$_{\!}$~\cite{radford2021learning} from OpenCLIP. 
The extracted features $\bm{F} \in {\mathbb{R}}^{M \times L \times D}$ are then fed into a trainable Perceiver Resampler$_{\!}$~\cite{alayrac2022flamingo}, producing a fixed set of $N\!+\!1$ visual tokens per image (including a CLS token), denoted as $\bm{F}^{'} \in {\mathbb{R}}^{M \times (N+1) \times D}$, where $N=64$ and $D$ is the feature dimension.
During training, raw text data ($T_\text{e}=4096$ text tokens) is rendered onto $M=28$ images, resulting in $64 \times 28 = 1792$ visual tokens, passed to the cross-attention layer in LLM. 
This compression reduces the computational complexity of the cross-attention layer within the LLM. 
Moreover, the number of images $M$ and tokens $N$ can be dynamically adjusted during both training and inference, allowing for flexible control of the compression ratio. \ourname\ using a lightweight vision encoder, offers a more efficient approach than processing all text tokens directly within the LLM. 
% extract rich text information from pixel space
\subsection{Probability-Informed Visual Enhancement}
\label{sec::FCL}
In \ourname, the frozen vision encoder is pre-trained primarily on general visual data (such as natural images) without exposure to rendered text images. Hence its ability to interpret dense textual information within images is constrained. 
To alleviate this problem, we develop a novel training objective, named Probability-Informed Visual Enhancement (PVE). 
PVE enhances the understanding capabilities of Perceiver Resampler for rendered text images, enabling them to serve as robust substitutes for traditional text tokenizers.


\noindent\textbf{Text-Anchored Semantic Consistency.}
PVE encourages the Perceiver Resampler to learn a shared embedding space, aligning visual text features $\bm{F}^{'}$ with text token embeddings. Concretely, 
PVE is formulated as a contrastive loss:
\begin{equation}
\label{eq:FCL}
    \mathcal{L}_{ij} = -\log \frac {\exp(\langle \hat{\bm{F}}^{'}_{i}, \hat{\bm{F}}^\text{t}_j \rangle / \tau)} 
    {\sum_{k=1}^B \exp (\langle \hat{\bm{F}}^{'}_{i}, \hat{\bm{F}}^\text{t}_k \rangle /\tau )},  
\end{equation}
where $B$ is batch size and $\hat{\bm{F}}^{'}_{i}$ is obtained by applying average pooling to the CLS tokens from $\bm{F}^{'}_{i}$.
$\hat{\bm{F}}^\text{t}_j$ is the averaged text token embedding after frequency-based masking and pooling.
$\tau$ is the temperate parameter. 
Importantly, $\hat{\bm{F}}^{'}_{i}$ and $\hat{\bm{F}}^\text{t}_i$ are different representations derived from the same text.


\noindent\textbf{Frequency-based Masking.} 
PVE employs text token embeddings as supervision signals to guide the Resampler in extracting textual information from text images. 
However, long-form text exhibits redundant information, where structural components and function words may dominate the token distribution.
Such redundancy introduces noise that impedes Resampler from capturing \textbf{key semantic content}.

\input{tabs/ppl}

Our solution draws inspiration from Shannon information theory~\cite{shannon1948mathematical}, which provides a formal way to quantify the information content of an event or a message. 
The formula is given by:
\vspace{-2pt}
\begin{equation}
I(y) = -\log_{2} P(y),
\vspace{-2pt}
\end{equation} where $I(y)$ is the information content of event or messages $y$ and $P(y)$ is the probability of $y$. 
It highlights the inverse relationship between the probability of an event and the information it carries. 
When applied to tokens in a corpus: \textbf{Rare tokens} (low-frequency) are treated as high-information tokens because they often carry domain-specific or contextually important information. 
\textbf{Frequent tokens} (high-frequency) have lower information content because they may serve more structural or grammatical purposes, contributing less to the unique meaning of the text. 

Figure~\ref{fig:info_gain} shows that masking 50\% of the most frequent tokens based on corpus-level (\ie, training set) frequency distribution still \textit{preserves most high-information-gain (IG) tokens, ensuring minimal loss of critical information while reducing redundancy}.
Based on this principle, we devise frequency-based masking strategy that uses token frequency as a proxy for semantic importance.  
This strategy masks frequent tokens but low-information tokens to improve the information density of text token embeddings. 
The importance score for each token is calculated as follows: 
\vspace{-2pt}
\begin{equation}
s_w = \log \frac{|\mathcal{S}|}{1+\text{count}(w)},
\label{eq:important_score}
\vspace{-2pt}
\end{equation}
where $|\mathcal{S}|$ denotes the total number of samples, $\text{count}(w)$ is the count of the token $w$ (subword), and $s_w$ is the importance score of token $w$. 
Additionally, the token frequency statistics are computed online during training, which can be easily calculated. 
Based on the importance score for each token, we apply a 50\% masking rate, where tokens are randomly masked with tokens of lower importance score being more likely to be masked. 
This ensures the Resampler prioritizes key content-bearing tokens, learning richer semantic representations and improving its ability to interpret dense text in rendered images.
