\section{Experiment}
\label{sec::experi}
\input{tabs/icl}
\subsection{Experimental Setup}
\noindent\textbf{Pretraining.}$_{\!}$ We validate \ourname\ with TinyLlama~\cite{zhang2024tinyllama}. The frozen vision encoder in our model is ViT-L/14~\cite{radford2021learning}. 
To reduce computational overhead, our model employs float16 precision and DeepSpeed Zero-2 with CPU off-loading~$_{\!\!}$\cite{rasley2020deepspeed}.  


\noindent\textbf{Text Encoder-based Model.} 
To compare the effectiveness of leveraging text tokens versus visual tokens for processing extended contexts in LLM, we additionally implemented an alternative method denoted as \mytext. 
\mytext\ applies CEPE~\cite{yen2024long} to TinyLlama~\cite{zhang2024tinyllama}, which introduces a lightweight text encoder to replace visual encoder in \ourname.
To handle extended input tokens, \mytext\ splits them into $256$-token chunks for parallel processing, while \ourname\ takes in the corresponding rendered text images. 
Apart from the text encoder, the model architecture and training details of \mytext\ remain the same as ours.


 
\noindent\textbf{Pretraining Dataset.} Our pertaining dataset is an official sample of the RedPajama dataset~\cite{weberredpajama}, including $1$B tokens from seven domains: ArXiv, Books, C4, Commoncrawl, GitHub, StackExchange, and Wikipedia. The training set of the corpus is preprocessed into $4608$ text token sequences, where the first $4096$ text token sequences are fed into the vision encoder (or text encoder for \mytext) and the remaining $512$ text tokens are provided to LLM.

\noindent\textbf{Downstream Evaluation.} 
We primarily evaluate tasks requiring long context processing, revealing vision tokens effectively handle extended context and improve performance, outperforming previous text-encoder-based model.

% \input{tabs/ppl}
\subsection{Long-context Language Modeling} To examine the long-context language modeling (LCM) ability, we evaluate on ArXiv and Books from RedPajama test split, alongside three long-context datasets: PG19~\cite{raecompressive}, ProofPile~\cite{azerbayev2023proofpile}, and CodeParrot~\cite{codep}. 
The evaluation metric is perplexity (PPL) over the last 256 tokens of each input sequence. 

\noindent\textbf{Impact of Increased Text Length.}
Table~\ref{tab:ppl} summarizes the results across different input lengths.
Long-context language modeling can benefit from previous long contextual information. 
However, TinyLlama~\cite{zhang2024tinyllama} supports only fixed-size inputs of 2048 tokens. Beyond this length, its performance drops sharply, with perplexity exceeding $10^3$. 
In contrast, \ourname\ demonstrates a consistent decrease in perplexity as the input text length increases. 
For instance, increasing the encoder input length from 2048 to 6144 reduces the perplexity on PG19 from 13.205 to 12.737. 
Moreover, \ourname\ allows to compress input token length while achieving comparable performance with \mytext. 
These results prove that \textbf{ \ourname\ effectively increases the input context length}, thereby enhancing the capability of modeling long-form language.


\noindent\textbf{Comparison on Inference Cost.} 
In Table~\ref{tab:ppl}, \ourname\ renders text into multiple images size of $224\times224$. 
Specifically, 1024 text tokens need 7 images. 
Though the performance of \ourname\ is not always the best, we compress the encoder token length by a factor of 2.3 (\eg, from 1024 to $448=7\times64$), while achieving the lowest FLOPs and memory usage. When processing 16k tokens, \textbf{\ourname\ reduces FLOPs by 16\% and memory usage by 50\% compared to \mytext.}




\input{tabs/qa}
\subsection{In-Context Learning}
We evaluate \ourname\ on in-context learning (ICL) tasks across eleven widely-used text-classification datasets: SST2~\cite{socher2013recursive}, MR~\cite{pang2005seeing}, AGN~\cite{zhang2015character}, SST5~\cite{socher2013recursive}, TREC, TREF~\cite{voorhees2000building}, 
 DBPedia~\cite{zhang2015character}, NLUS, NLUI~\cite{liu2021benchmarking}, 
BANKING77~\cite{casanueva2020efficient}, and CLINIC150~\cite{larson2019evaluation}. Following CEPE~\cite{yen2024long}, we randomly sample 250 text examples per dataset. The ICL results in Table~\ref{tab:icl} are reported as the average accuracy over three random seeds. For \ourname\ and \mytext, we provide two demonstrations directly to the decoder, while the rest are processed by the encoder. 


\noindent\textbf{Results.} 
Table~\ref{tab:icl} examines the influence of increasing the number of demonstrations. 
It is evident \ourname\ achieves significant accuracy gains as more demonstrations are provided to the visual encoder, showcasing the capacity of LLM to comprehend text within visual signals when integrated with \ourname. 
Furthermore, \ourname\ outperforms \mytext\ in average accuracy across all datasets, which indicates \textbf{visual-based text understanding can effectively match or even surpass text encoder performance}. 
Though \ourname\ and \mytext\ perform worse than TinyLlama with 20 demonstrations, they use a lightweight encoder for most demonstrations, reducing computational cost. 
Notably, the performance of TinyLlama declines with 50 demonstrations due to context window limit, while \ourname\ remains efficient and stable.
\subsection{Open-domain Question Answering}
Open-domain Question Answering (QA) is a challenging task that requires model to generate accurate answers based on retrieved relevant information. 
Experiments are conducted on three open-domain QA datasets, including TriviaQA~\cite{joshi2017triviaqa}, NQ~\cite{kwiatkowski2019natural}, and PopQA~\cite{mallen2023not}. 
We use Contriever~\cite{izacardunsupervised} to retrieve relevant $k$ passages from Wikipedia, as in CEPE~\cite{yen2024long}. 
We prioritize passing the most relevant passages to the decoder to enhance performance. 
In table~\ref{tab:qa}, we report the exact match (EM) scores. 


\noindent\textbf{Results.} 
TinyLlama is limited by a maximum context window of 2048 tokens, restricting it to processing no more than 10 passages at a time. 
Beyond this limit, performance drops sharply, with EM score falling below 1. 
Our approach addresses this limitation by integrating a lightweight visual encoder with the proposed PVE, enabling efficient processing of additional passages and improving performance in open-domain QA. 
For instance, processing 5 extra passages (\ie, $k_\text{e}=5, k_\text{d}=10$) results in an EM score improvement of \textbf{3.75} compared to TinyLlama on TriviaQA dataset.
This improvement \textit{highlights the advantages of leveraging visual-semantic representations.}
Moreover, it even surpasses text encoder-based approach \mytext\ under the same input conditions, \eg, delivering an EM score \textbf{9.11} points higher on the TriviaQA dataset when $k_\text{e}=20, k_\text{d}=10$. 
This enhancement may be attributed to the ability of \ourname\ to improve the understanding of unique semantics in each passage. 
By emphasizing critical details and filtering out noise from lengthy inputs, our method prioritizes relevant informationâ€”a crucial factor for success in open-domain QA tasks. 

\subsection{Ablation Study}
In this section, we conduct a series of ablation studies to examine how different training configurations affect downstream task performance. Specifically, we explore the effects of \ding{182} the masking strategy employed in PVE, \ding{183} the length of text provided to the encoder during training, and \ding{184} the number of compressed tokens in each image (\ie, $N$ in \S\ref{sec::token_reduction}). In open-domain QA tasks, the model is fed 10 relevant passages for the LLM and 5 for the encoder. ICL tasks use a fixed setup of 18 demonstrations for the encoder and 2 for the LLM.

\input{tabs/ab_strategy}
\input{tabs/ab_image_token_nums}
\input{tabs/ab_encoder_len}

\noindent\textbf{Masking Strategy in PVE (\S\ref{sec::FCL}).} 
Long texts often contain significant redundancy. 
To address this, we integrate a Frequency-based Masking (FM) strategy into PVE, improving the information density of text token embeddings. Table~\ref{tab:ab_strategy} compares the performance of \ourname\ with FM, w/o FM, and with random masking.  
Excluding FM causes a notable decline in ICL and open-domain QA performance. 
This highlights the critical role of information-dense text token embeddings in guiding the visual encoder to capture more semantically meaningful and discriminative features.




\input{figs/random_mask}
\input{figs/FM}

  
\noindent\textbf{Number of Tokens in Each Image.}
The Perceiver Resampler transforms image features into a fixed number of visual tokens. Table~\ref{tab:ab_token_num} analyzes the impact of visual token count for each image. 
Increasing the number of visual tokens reduces the compression ratio, but as shown, \textit{\textbf{a lower compression ratio does not always yield better results}}. 
With 64 visual tokens, the model performs best on 4 out of 5 datasets, whereas 128 tokens only perform best on the MR dataset. 
This discrepancy could be attributed to the trade-off between the amount of information preserved and the noise introduced during compression. 
Fewer tokens risk losing critical details, while more tokens may retain excessive or irrelevant information, which can hinder generalization.  
These findings emphasize that achieving a balance between compression and information retention is crucial for optimal performance across different datasets.


\noindent\textbf{In-context Text Length.} Table~\ref{tab:ab_encoder_len} investigates the impact of in-context text length (measured in text tokens) provided to the visual encoder during training. 
Our model, trained with a longer encoder input length, generally yields higher EM scores on open-domain QA tasks. 
This may be because exposure to more lengthy texts during training helps our model better extract key information from extensive contexts, which is crucial for open-domain QA. 
Table~\ref{tab:ab_encoder_len} shows that longer training text inputs often boost ICL task accuracy. 
For instance, the best result on SST5 (42.7) was achieved with an encoder input length of 4096 text tokens, while the highest accuracy on MR (90.5) was obtained with the longest input length of 6144 tokens. 
Interestingly, we observed that training with an input length of 1024 tokens performed comparably to 2048 tokens,  possibly because the total demo length for the encoder was close to 1024 tokens.

\input{tabs/extension_llm}
\subsection{Extension to other LLM}
Mixture-of-expert models effectively scale model capacity while saving resources.
To prove the generality of \ourname, we also apply \ourname\ to Mistral 7B~\cite{jiang2023mistral}. 
In LCM task, \ourname$^{\dagger}$ and CEPE$^{\dagger}$ process 4096 tokens,  compared to the 2048 tokens processed by Mistral. 
For ICL, \ourname$^{\dagger}$ and CEPE$^{\dagger}$ use 20 demonstrations, while Mistral uses only 2. 
As shown in Table~\ref{tab:extension_llm}, \ourname$^{\dagger}$ demonstrates superior performance over CEPE$^{\dagger}$, by effectively leveraging additional context.



\subsection{Token Redundancy Exploration}
We study the semantic contribution of frequent tokens by selectively masking the top 50\% most frequent tokens (based on training set statistics).
Figure~\ref{fig:mask_exam} illustrates key nouns are fully preserved. 
The masked tokens mainly include function words(\eg, ``the", ``of"), primarily serving grammatical roles rather than conveying core semantic meaning. 
This proves FM strategy can
\textbf{preserve critical semantic information while filtering out less relevant noise, thereby enhancing the ability of the model to focus on meaningful content}.
