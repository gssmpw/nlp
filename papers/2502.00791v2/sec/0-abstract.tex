\begin{abstract}
Large Language Models (LLMs) have revolutionized natural language processing, excelling in handling longer sequences. 
However, the inefficiency and redundancy in processing extended in-context tokens remain a challenge. 
Many attempts to address this rely on compressing tokens with smaller text encoders, yet we question whether text encoders are truly indispensable.
Our journey leads to an unexpected discoveryâ€”\textit{a much smaller vision encoder, applied directly to sequences of text tokens, can rival text encoders on text tasks}.
When pre-trained on large amounts of data and transferred to multiple mid-sized or small text understanding benchmarks, \ourname\ leads to comparable results with \textbf{16}\% fewer FLOPs and \textbf{50\%} less memory usage.
We further uncover significant token redundancy and devise a frequency-based masking strategy to guide the focus of the visual encoder toward the most critical tokens.
Interestingly, we observe the trained visual encoder performs like a summarizer, selectively ignoring less important words such as prepositions and conjunctions.
This approach delivers remarkable results, outperforming traditional text encoder-based methods by \textbf{5.7}\% on average over benchmarks like TriviaQA, NQ, PopQA, TREF, SST2, and SST5, setting a new standard for token efficiency in LLMs.
\end{abstract}

