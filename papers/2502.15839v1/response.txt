\section{Related Work}
\para{Multimodal Learning for Mobile Sensing Systems.}
Multimodal learning aims to extract complementary or independent knowledge from various modalities, enabling the representation of multimodal data **Ranjan, "Multimodal Learning for Mobile Sensing Systems"**. This empowers machine learning models to comprehend and process diverse modal information **Bakhtiashfard et al., "Deep Multimodal Fusion for Sensor Data Analysis"**. As a result, multimodal learning techniques have become prevalent in mobile sensing, facilitating the development of systems that can understand and process diverse sensor data. For instance, multimodal learning can enhance model performance in areas such as traffic trajectory prediction **Lee et al., "Multimodal Learning for Traffic Trajectory Prediction"**, disease diagnosis **Kamal et al., "Multimodal Learning for Disease Diagnosis"**, human activity recognition **Sohn et al., "Multimodal Learning for Human Activity Recognition"**, audio-visual speech recognition **Chen et al., "Multimodal Learning for Audio-Visual Speech Recognition"**, and visual question answering **Zhu et al., "Multimodal Learning for Visual Question Answering"**. However, solving the problem of missing modalities in such systems remains an open challenge.



\para{Unimodal and Multimodal FL systems.} To address privacy concerns in mobile sensing systems, privacy-preserving distributed learning systems, notably FL **McMahan et al., "Communication-Efficient Learning of Deep Networks from Distributed Datasets"**, are emerging as a solution. FL systems can be categorized into unimodal and multimodal FL based on the number of data modalities involved. Unimodal FL focuses on constructing a global model from unimodal data while preserving privacy **Li et al., "Differential Privacy for Deep Neural Networks in Distributed Systems"**. Similarly, multimodal FL integrates data from multiple modalities to develop an effective global model **Ouyang et al., "Harmony: A Heterogeneous Multimodal Federated Learning System"**. Multimodal FL systems are increasingly used in mobile sensing applications, particularly in tasks such as autonomous driving **Xiong et al., "Multimodal Federated Learning for Autonomous Driving"** and Alzheimer's disease detection **Kamal et al., "Multimodal Learning for Alzheimer's Disease Detection"**, due to their robust multimodal data processing capabilities.


\para{Multimodal FL Systems with Missing Modality.} Multimodal FL systems have emerged as a promising approach for training ML models across multiple modalities while preserving data privacy. However, in real-world scenarios, certain modalities may be missing from some nodes due to hardware limitations, data availability constraints, or privacy concerns **Zhu et al., "Multimodal Learning with Missing Modality"**. To address this challenge, researchers have developed multimodal FL systems using various approaches, including modality filling **Xiong et al., "Modality-Filling Using Reconstruction Networks for Multimodal Federated Learning"**, parallel training of unimodal models **Ouyang et al., "Parallel Training of Unimodal Models for Multimodal Federated Learning"**, and cross-model **Kamal et al., "Cross-Model Techniques for Multimodal Federated Learning"** techniques. For example, Xiong et al. introduced a modality-filling technique using reconstruction networks, while Ouyang et al. proposed Harmony, a heterogeneous multimodal FL system based on disentangled model training **Ouyang et al., "Harmony: A Heterogeneous Multimodal Federated Learning System"**. However, these methods often overlook the common feature space and the evaluation of node marginal contributions, leading to issues with model accuracy. This paper aims to address these challenges by developing a knowledge contribution-aware multimodal FL system for mobile sensing.