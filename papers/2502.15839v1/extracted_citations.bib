@inproceedings{chen2022mm,
  title={MM-ViT: Multi-modal video transformer for compressed video action recognition},
  author={Chen, Jiawei and Ho, Chiu Man},
  booktitle={Proc. of CVPR},
  year={2022}
}

@inproceedings{feng2023fedmultimodal,
  title={Fedmultimodal: A benchmark for multimodal federated learning},
  author={Feng, Tiantian and Bose, Digbalay and Zhang, Tuo and Hebbar, Rajat and Ramakrishna, Anil and Gupta, Rahul and Zhang, Mi and Avestimehr, Salman and Narayanan, Shrikanth},
  booktitle={Proc. of KDD},
  year={2023}
}

@article{le2024cross,
  title={Cross-Modal Prototype based Multimodal Federated Learning under Severely Missing Modality},
  author={Le, Huy Q and Thwal, Chu Myaet and Qiao, Yu and Tun, Ye Lin and Nguyen, Minh NH and Hong, Choong Seon},
  journal={arXiv preprint arXiv:2401.13898},
  year={2024}
}

@inproceedings{liu2020recent,
  title={Recent advances in multimodal educational data mining in k-12 education},
  author={Liu, Zitao and Yang, Songfan and Tang, Jiliang and Heffernan, Neil and Luckin, Rose},
  booktitle={Proc. of KDD},
  year={2020}
}

@inproceedings{lu2018r,
  title={R-VQA: learning visual relation facts with semantic attention for visual question answering},
  author={Lu, Pan and Ji, Lei and Zhang, Wei and Duan, Nan and Zhou, Ming and Wang, Jianyong},
  booktitle={Proc. of KDD},
  year={2018}
}

@inproceedings{mcmahan2017communication,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Proc. of AISTATS},
  year={2017},
}

@inproceedings{mroueh2015deep,
  title={Deep multimodal learning for audio-visual speech recognition},
  author={Mroueh, Youssef and Marcheret, Etienne and Goel, Vaibhava},
  booktitle={Proc. of ICASSP},
  year={2015},
}

@inproceedings{ouyang2023harmony,
  title={Harmony: Heterogeneous multi-modal federated learning through disentangled model training},
  author={Ouyang, Xiaomin and Xie, Zhiyuan and Fu, Heming and Cheng, Sitong and Pan, Li and Ling, Neiwen and Xing, Guoliang and Zhou, Jiayu and Huang, Jianwei},
  booktitle={Proc. of MobiSys},
  year={2023}
}

@inproceedings{park2023attfl,
  title={Attfl: A personalized federated learning framework for time-series mobile and embedded sensor data processing},
  author={Park, JaeYeon and Lee, Kichang and Lee, Sungmin and Zhang, Mi and Ko, JeongGil},
  booktitle={Proc. of UbiComp},
  year={2023},
}

@inproceedings{shi2023lhmm,
  title={LHMM: A Learning Enhanced HMM Model for Cellular Trajectory Map Matching},
  author={Shi, Weijie and Xu, Jiajie and Fang, Junhua and Chao, Pingfu and Liu, An and Zhou, Xiaofang},
  booktitle={Proc. of ICDE},
  year={2023},
}

@inproceedings{xia2021multi,
  title={Multi-behavior enhanced recommendation with cross-interaction collaborative relation modeling},
  author={Xia, Lianghao and Huang, Chao and Xu, Yong and Dai, Peng and Lu, Mengyin and Bo, Liefeng},
  booktitle={Proc. of ICDE},
  year={2021},
}

@inproceedings{xiong2023client,
  title={Client-Adaptive Cross-Model Reconstruction Network for Modality-Incomplete Multimodal Federated Learning},
  author={Xiong, Baochen and Yang, Xiaoshan and Song, Yaguang and Wang, Yaowei and Xu, Changsheng},
  booktitle={Proc. of ACM MM},
  year={2023}
}

@article{yang2024cross,
  title={Cross-Modal Federated Human Activity Recognition},
  author={Yang, Xiaoshan and Xiong, Baochen and Huang, Yi and Xu, Changsheng},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2024},
  publisher={IEEE}
}

@inproceedings{zhang2020multi,
  title={Multi-modal network representation learning},
  author={Zhang, Chuxu and Jiang, Meng and Zhang, Xiangliang and Ye, Yanfang and Chawla, Nitesh V},
  booktitle={Proc. of KDD},
  year={2020}
}

@inproceedings{zheng2023autofed,
  title={Autofed: Heterogeneity-aware federated multimodal learning for robust autonomous driving},
  author={Zheng, Tianyue and Li, Ang and Chen, Zhe and Wang, Hongbo and Luo, Jun},
  booktitle={Proc. of MobiCom},
  year={2023}
}

