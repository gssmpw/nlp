\section{Related Work}
\para{Multimodal Learning for Mobile Sensing Systems.}
Multimodal learning aims to extract complementary or independent knowledge from various modalities, enabling the representation of multimodal data~\cite{zhang2020multi,liu2020recent}. This empowers machine learning models to comprehend and process diverse modal information~\cite{xia2021multi}. As a result, multimodal learning techniques have become prevalent in mobile sensing, facilitating the development of systems that can understand and process diverse sensor data. For instance, multimodal learning can enhance model performance in areas such as traffic trajectory prediction~\cite{shi2023lhmm}, disease diagnosis~\cite{ouyang2023harmony}, human activity recognition~\cite{chen2022mm}, audio-visual speech recognition~\cite{mroueh2015deep}, and visual question answering~\cite{lu2018r}. However, solving the problem of missing modalities in such systems remains an open challenge.



\para{Unimodal and Multimodal FL systems.} To address privacy concerns in mobile sensing systems, privacy-preserving distributed learning systems, notably FL~\cite{zheng2023autofed,mcmahan2017communication,ouyang2023harmony}, are emerging as a solution. FL systems can be categorized into unimodal and multimodal FL based on the number of data modalities involved. Unimodal FL focuses on constructing a global model from unimodal data while preserving privacy~\cite{park2023attfl}. Similarly, multimodal FL integrates data from multiple modalities to develop an effective global model~\cite{feng2023fedmultimodal}. Multimodal FL systems are increasingly used in mobile sensing applications, particularly in tasks such as autonomous driving~\cite{zheng2023autofed} and Alzheimer's disease detection~\cite{ouyang2023harmony}, due to their robust multimodal data processing capabilities.


\para{Multimodal FL Systems with Missing Modality.} Multimodal FL systems have emerged as a promising approach for training ML models across multiple modalities while preserving data privacy. However, in real-world scenarios, certain modalities may be missing from some nodes due to hardware limitations, data availability constraints, or privacy concerns~\cite{ouyang2023harmony,le2024cross,yang2024cross}. To address this challenge, researchers have developed multimodal FL systems using various approaches, including modality filling~\cite{xiong2023client}, parallel training of unimodal models~\cite{ouyang2023harmony}, and cross-model~\cite{yang2024cross} techniques. For example, Xiong et al.~\cite{xiong2023client} introduced a modality-filling technique using reconstruction networks, while Ouyang et al.~\cite{ouyang2023harmony} proposed Harmony, a heterogeneous multimodal FL system based on disentangled model training. However, these methods often overlook the common feature space and the evaluation of node marginal contributions, leading to issues with model accuracy. This paper aims to address these challenges by developing a knowledge contribution-aware multimodal FL system for mobile sensing.