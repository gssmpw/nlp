
\documentclass[sigconf]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}


\copyrightyear{2025}
\acmYear{2025}
\setcopyright{acmlicensed}\acmConference[WWW '25]{Proceedings of the ACM
Web Conference 2025}{April 28-May 2, 2025}{Sydney, NSW, Australia}
\acmBooktitle{Proceedings of the ACM Web Conference 2025 (WWW '25), April
28-May 2, 2025, Sydney, NSW, Australia}
\acmDOI{10.1145/3696410.3714623}
\acmISBN{979-8-4007-1274-6/25/04}

%% These commands are for a PROCEEDINGS abstract or paper.
\newenvironment{sloppypar*}
{\sloppy\ignorespaces}
{\par}

\usepackage{balance}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{wrapfig}
\usepackage{adjustbox}  
\usepackage{hyperref}
\usepackage{subfigure}
%\usepackage{subcaption}
\usepackage[ruled,linesnumbered]{algorithm2e}
\SetKwInput{KwInput}{Input}                % Set the Input
\SetKwInput{KwOutput}{Output}              % set the Output
\newcommand{\para}[1]{\vspace{2pt}\noindent{\textbf{#1}}\hspace{10pt}\vspace{0.1pt}}
\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
\newcommand{\DKtodo}[1]{\textcolor{blue}{#1}}
\newcommand{\Zak}[1]{\textcolor{red}{#1}}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\SetKwRepeat{Do}{do}{while} % for dowhile


\makeatletter


\newcommand{\eg}{\textit{e.g\@.}}
\newcommand{\ie}{\textit{i.e\@.}}
\newcommand{\vs}{\textit{vs\@.}}

\makeatother

\usepackage{physics}
\usepackage{mathtools}



\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors




%%
%% end of the preamble, start of the body of the document source.
\begin{document}\sloppy

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{\textsf{FedMobile}: Enabling Knowledge Contribution-aware Multi-modal Federated Learning with Incomplete Modalities}

\author{Yi Liu}
\affiliation{%
 City University of Hong Kong
  \country{China}
}
\email{yiliu247-c@my.cityu.edu.hk}


\author{Cong Wang}
\affiliation{%
  City University of Hong Kong
  \country{China}
}
\email{congwang@cityu.edu.hk}

\author{Xingliang Yuan}
\affiliation{%
 The University of Melbourne
  \country{Australia}
}
\email{xingliang.yuan@unimelb.edu.au}



\renewcommand{\shortauthors}{Yi Liu, Cong Wang, \& Xingliang Yuan}


\begin{abstract}
The Web of Things (WoT) enhances interoperability across web-based and ubiquitous computing platforms while complementing existing IoT standards. The multimodal Federated Learning (FL) paradigm has been introduced to enhance WoT by enabling the fusion of multi-source mobile sensing data while preserving privacy. 
However, a key challenge in mobile sensing systems using multimodal FL is modality incompleteness, where some modalities may be unavailable or only partially captured, potentially degrading the system's performance and reliability. Current multimodal FL frameworks typically train multiple unimodal FL subsystems or apply interpolation techniques on the node side to approximate missing modalities. However, these approaches overlook the shared latent feature space among incomplete modalities across different nodes and fail to discriminate against low-quality nodes. To address this gap, we present \textsf{FedMobile}, a new knowledge contribution-aware multimodal FL framework designed for robust learning despite missing modalities. \textsf{FedMobile} prioritizes local-to-global knowledge transfer, leveraging cross-node multimodal feature information to reconstruct missing features. It also enhances system performance and resilience to modality heterogeneity through rigorous node contribution assessments and knowledge contribution-aware aggregation rules. Empirical evaluations on five widely recognized multimodal benchmark datasets demonstrate that \textsf{FedMobile} maintains robust learning even when up to 90\% of modality information is missing or when data from two modalities are randomly missing, outperforming state-of-the-art baselines. Our code and data are available at the \href{https://doi.org/10.5281/zenodo.14802364}{link}.
\end{abstract}


\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010919.10010172</concept_id>
       <concept_desc>Computing methodologies~Distributed algorithms</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010178.10010219</concept_id>
       <concept_desc>Computing methodologies~Distributed artificial intelligence</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[300]{Computing methodologies~Distributed algorithms}
\ccsdesc[500]{Computing methodologies~Distributed artificial intelligence}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Multi-modal Federated Learning, Incomplete Modalities, Web-based Mobile, Knowledge Distillation, Model Aggregation}


%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle
\section{Introduction}
In the Web of Things (WoT)~\cite{pintus2012paraimpu,liu2020privacy,liu2020deep}, multimodal mobile sensing systems enhance the interoperability and usability of web-based mobile platforms by integrating data from multiple sources~\cite{wang2023flexifed,feng2020pmf,yang2021characterizing}. In this context, these systems boast a diverse array of real-world applications~\cite{oh2023multimodal}, frequently deployed to address complex tasks within domains such as autonomous driving~\cite{zheng2023autofed}, mobile healthcare~\cite{guo2020feel}, and the Internet of Things~\cite{lim2020federated}. In this context, these tasks often prove too intricate and dynamic to be effectively tackled solely through reliance on a single sensor modality~\cite{kubota2023cityscouter,oh2022locfedmix}. Consequently, a straightforward approach involves aggregating complementary modality data from multiple sensors to extract feature information across various sensing channels, thereby enhancing model performance~\cite{williamson2007shoogle}. However, this multimodal learning paradigm, reliant on centralized processing and aggregation of raw user data, introduces significant privacy concerns~\cite{delgado2022survey,yuan2024towards,wang2024feddse,wang2022fedkc,liu2022right}.

To mitigate the privacy concerns outlined above, Federated Learning (FL)~\cite{mcmahan2017communication} emerges as a solution. FL, characterized as an evolving privacy-preserving distributed machine learning paradigm, facilitates collaboration among mobile sensing devices across regions without compromising privacy~\cite{kairouz2021advances}. By sharing model updates instead of raw data, FL fosters collective learning of global models among geographically spread devices. Although most FL methods deal with unimodal data for tasks like next-word prediction~\cite{hard2018federated}, some applications, \eg, Alzheimer's detection~\cite{ouyang2023harmony}, necessitate combining data from diverse sources (multimodal data). This has led to the development of multimodal FL systems~\cite{feng2023fedmultimodal} tailored for efficient processing of data from various sensory channels.


While the multimodal FL system addresses some challenges in multimodal data processing, it still suffers from incomplete sensing modalities~\cite{ouyang2023harmony,xiong2023client,le2024cross}, as shown in Fig. \ref{fig-1}. For example, in mobile healthcare, sensor modalities often become unavailable due to sensor failures or malfunctions. This increases the variability of available sensor modalities across different nodes during runtime~\cite{ouyang2023harmony}. Thus, aggregating model updates in multimodal FL systems with incomplete sensing modalities becomes very challenging due to the varied distributions of modality types across different mobile nodes. This modality heterogeneity also intensifies model disparities between nodes, affecting the accuracy and convergence of FL~\cite{xiong2023client,le2024cross}. Existing multimodal FL methods use techniques like data interpolation~\cite{zheng2023autofed} and modal fusion~\cite{feng2023fedmultimodal} to address these issues, but there is still a significant gap in efficiently utilizing cross-node modal feature information and selecting high-quality data nodes.

In this paper, we introduce \textsf{FedMobile}, a novel knowledge contribution-aware multimodal FL system designed specifically for mobile sensing applications with missing modalities. Unlike existing multimodal FL methodologies~\cite{ouyang2023harmony,feng2023fedmultimodal,xiong2023client}, which typically focus on training multiple unimodal FL subsystems concurrently, \textsf{FedMobile} adopts a distinct approach. It aims to reconstruct the features of missing modalities by utilizing knowledge distillation while introducing a knowledge contribution-aware aggregation rule via Shapley value to discern and aggregate high-quality model updates. To fulfill these objectives, \textsf{FedMobile} is guided by two primary goals: 1) Effectively reconstructing features of missing modalities without exacerbating modality heterogeneity or compromising main task performance. 2) Streamlining the process of identifying mobile nodes with substantial contributions while minimizing computational overhead. Next, we focus on responding to the following two challenges:

$\bullet$ \textit{(C1.) How to collaboratively interpolate missing sensing modal features for different nodes with cross-modal heterogeneity.}

\noindent \textbf{(S1.)} -- \textit{The heterogeneous modality between different nodes has a common feature subspace.} In mobile sensing scenarios, malfunctioning sensor modalities at various nodes give rise to missing modalities, causing modality heterogeneity~\cite{yang2024cross}. In such circumstances, conventional solutions like zero-filling~\cite{john2023multimodal} and parallel training of unimodal models~\cite{ouyang2023harmony} often inadequately handle this inherent feature and modality diversity. Our goal, therefore, is to leverage knowledge distillation for constructing a shared feature subspace among node modalities to improve model performance. We implement a feature generator on both the server and node levels to tackle missing modality issues. This generator, trained in a coupled training manner, aims to align different modalities by capturing a common feature space.

$\bullet$ \textit{(C2.) How to find relevant metrics for measuring the contribution of a specific node in a computationally cost-friendly manner.}

\noindent \textbf{(S2.)} -- \textit{Knowledge and model updates shared between nodes and server generalize the contributions of nodes.} In \textsf{FedMobile}, the contributory role of participating nodes is manifested across dual dimensions: knowledge shared by local generators and local model updates shared by local nodes. To incentivize local generators to yield high quality features for incomplete modalities, we devise a novel Clustered Shapley Value approach that quantifies the individual contributions of these generators. This subsequently allows for adaptive modulation of their respective weights, thus facilitating the aggregation of high-quality feature representations. Moreover, with the objective of discerning nodes that high-quality model updates, we introduce a contribution-aware aggregation mechanism designed to retain those elements that are conducive to the overall improvement of the global model. Conversely, it eliminates nodes that do not meet this criterion. By dynamically choosing nodes based on this principle, we effectively ensure the aggregation of high-quality model updates during the training.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.4\textwidth]{fig-1.pdf}
    \caption{Unimodal FL \textit{vs} multimodal FL.}
    \label{fig-1}
    \vspace{-0.4cm}
\end{figure}


Additionally, we evaluate \textsf{FedMobile} across five real-world multimodal sensing datasets: USC-HAD~\cite{zhang2012usc}, MHAD~\cite{ofli2013berkeley}, Alzheimer's Disease Monitoring (ADM)~\cite{ouyang2023harmony}, C-MHAD~\cite{wei2020c}, and FLASH~\cite{salehi2022flash}, which encompass tasks related to autonomous driving, mobile healthcare, and Alzheimer's disease detection. The results indicate that \textsf{FedMobile} effectively leverages various sensor types (such as GPS, gyroscopes, and radar) in scenarios with incomplete sensing modalities to accurately perform assigned tasks, even amid operational dynamics like sensor failures. Furthermore, we analyze the computational and communication overhead of \textsf{FedMobile} across different tasks. Extensive evaluations show that \textsf{FedMobile} outperforms existing multimodal FL systems (\eg, FedMM~\cite{feng2023fedmultimodal}, AutoFed~\cite{zheng2023autofed}, and PmcmFL~\cite{bao2023multimodal}), achieving higher model accuracy with reasonable additional computational and communication overheads, especially under dynamic modality missing conditions.

The contributions of our work can be summarized as follows:

\emph{(1)} We tailor \textsf{FedMobile}, a multimodal federated learning framework that is robust to incomplete modal data, for web-based mobile sensing systems in WoT.

\emph{(2)} We design a knowledge distillation-driven cross-node modality reconstruction network to efficiently reconstruct the missing modality data without introducing excessive overhead.

\emph{(3)} We design an efficient generator contribution evaluation module based on clustered Shapley value and contribution-aware aggregation mechanism to further improve system performance.

    
\emph{(4)} We implement our design and conduct extensive experiments on 5 datasets related to 3 mobile sensing downstream tasks to explore the performance, efficiency, generality, and parameter sensitivity of \textsf{FedMobile}. Compared to the baselines, our approach achieves state-of-the-art performance on all tasks while maintaining comparable computation and communication overhead.


\section{Related Work}
\para{Multimodal Learning for Mobile Sensing Systems.}
Multimodal learning aims to extract complementary or independent knowledge from various modalities, enabling the representation of multimodal data~\cite{zhang2020multi,liu2020recent}. This empowers machine learning models to comprehend and process diverse modal information~\cite{xia2021multi}. As a result, multimodal learning techniques have become prevalent in mobile sensing, facilitating the development of systems that can understand and process diverse sensor data. For instance, multimodal learning can enhance model performance in areas such as traffic trajectory prediction~\cite{shi2023lhmm}, disease diagnosis~\cite{ouyang2023harmony}, human activity recognition~\cite{chen2022mm}, audio-visual speech recognition~\cite{mroueh2015deep}, and visual question answering~\cite{lu2018r}. However, solving the problem of missing modalities in such systems remains an open challenge.



\para{Unimodal and Multimodal FL systems.} To address privacy concerns in mobile sensing systems, privacy-preserving distributed learning systems, notably FL~\cite{zheng2023autofed,mcmahan2017communication,ouyang2023harmony}, are emerging as a solution. FL systems can be categorized into unimodal and multimodal FL based on the number of data modalities involved. Unimodal FL focuses on constructing a global model from unimodal data while preserving privacy~\cite{park2023attfl}. Similarly, multimodal FL integrates data from multiple modalities to develop an effective global model~\cite{feng2023fedmultimodal}. Multimodal FL systems are increasingly used in mobile sensing applications, particularly in tasks such as autonomous driving~\cite{zheng2023autofed} and Alzheimer's disease detection~\cite{ouyang2023harmony}, due to their robust multimodal data processing capabilities.


\para{Multimodal FL Systems with Missing Modality.} Multimodal FL systems have emerged as a promising approach for training ML models across multiple modalities while preserving data privacy. However, in real-world scenarios, certain modalities may be missing from some nodes due to hardware limitations, data availability constraints, or privacy concerns~\cite{ouyang2023harmony,le2024cross,yang2024cross}. To address this challenge, researchers have developed multimodal FL systems using various approaches, including modality filling~\cite{xiong2023client}, parallel training of unimodal models~\cite{ouyang2023harmony}, and cross-model~\cite{yang2024cross} techniques. For example, Xiong et al.~\cite{xiong2023client} introduced a modality-filling technique using reconstruction networks, while Ouyang et al.~\cite{ouyang2023harmony} proposed Harmony, a heterogeneous multimodal FL system based on disentangled model training. However, these methods often overlook the common feature space and the evaluation of node marginal contributions, leading to issues with model accuracy. This paper aims to address these challenges by developing a knowledge contribution-aware multimodal FL system for mobile sensing.


\section{Preliminary}
\subsection{Multimodal Federated Learning}
Multimodal FL is a cutting-edge approach in machine learning (ML) that addresses the challenges of training models across multiple modalities while preserving data privacy. Formally, in mobile sensing scenarios, let us denote \( \mathcal{M} = \{ {m_0},{m_1}, \ldots ,{m_{M - 1}}\}  \) as the set of modalities of the local multimodal dataset $D_k$, \( K \) as the number of participating mobile nodes, \( n_k \) as the number of samples in the node \( k \), and \( d_k^m \) as the dimensionality of modality \( m \) in the node \( k \). The objective of Multimodal FL is to optimize a global model \( \mathcal{F}(\omega) \) parameterized by \( \omega \) across all modalities while minimizing the following federated loss function:
\begin{equation}\label{eq-1}
  \min_{\omega} \sum_{k=1}^{K} \sum_{m \in \mathcal{M}} \frac{n_k}{n} \mathcal{L}(\omega; \mathbf{X}_k^m, \mathbf{y}_k),
\end{equation}
where \( \mathcal{L}(\omega; \mathbf{X}_k^m, \mathbf{y}_k) \) is the loss function for modality \( m \) at node \( k \), \( \mathbf{X}_k^m \) represents the data samples for modality \( m \) at node \( k \), \( \mathbf{y}_k \) is the target label associated with the samples at node \( k \), and \( n = \sum_{k=1}^{K} n_k \) represents the total number of samples across all nodes. In multimodal FL, the global model \( \mathcal{F}(\omega) \) is updated by aggregating local model updates from each node while respecting data privacy constraints. The update rule for the global model at iteration \( t \) can be formalized as:
\begin{equation}\label{eq-2}
\omega^{t+1} = \omega^{t} - \eta \sum_{k=1}^{K} \frac{n_k}{n} \nabla \mathcal{L}(\omega^{t}; \mathbf{X}_k, \mathbf{y}_k),
\end{equation}
where \( \eta \) is the learning rate and \( \nabla \mathcal{L}(\omega^{t}; \mathbf{X}_k, \mathbf{y}_k) \) is the gradient of the loss function with respect to the global model parameters \( \omega^{t} \) at node \( k \). Clearly, when a modal sensor on a mobile node fails or ceases to function, resulting in a missing modality, the optimization of Eqs. \eqref{eq-1} and \eqref{eq-2} becomes challenging. This impediment implies that multimodal FL may struggle to fulfill the designated task effectively under such circumstances.


\subsection{Shapley Value in ML}
Shapley Value~\cite{sun2023shapleyfl} is a concept from cooperative game theory used to fairly distribute the value generated by a coalition of players. In the context of ML, it is often applied to understand the contribution of each feature to a model's prediction~\cite{chen2023algorithms}. Let us denote a predictive model as \( f \), and \( \Phi_i(f) \) represents the Shapley value of feature \( i \) in the model \( f \). The Shapley value of feature \( i \) can be computed as:
\begin{equation}\label{eq-3}
   \Phi_i(f) = \sum_{S \subseteq N \setminus \{i\}} \frac{|S|!(|N|-|S|-1)!}{|N|!} \left[ f(x_S \cup \{i\}) - f(x_S) \right],
\end{equation}
where \( N \) is the set of all features, \( x_S \) represents the instance with only features in set \( S \), \( f(x_S \cup \{i\}) \) is the prediction of the model when feature \( i \) is added to the set \( S \), \( f(x_S) \) is the prediction of the model when only features in set \( S \) are considered, \( |S| \) denotes the cardinality of set \( S \), and \( |N| \) is the total number of features. The above formula computes the marginal contribution of feature \( i \) when added to different subsets \( S \) of features, weighted by the number of permutations of features in \( S \) to the total number of permutations of all features. In fact, calculating the Shapley value directly using the above formula might be computationally expensive~\cite{sun2023shapleyfl,chen2023algorithms}, especially for models with a large number of features. 


\begin{figure*}[!t]
 \centering
 \includegraphics[width=0.9\linewidth]{fig-21.pdf}
  \caption{Workflow overview of \textsf{FedMobile}.}
  \label{fig-2}
  \vspace{-0.3cm}
\end{figure*}


\section{Our Approach}

\subsection{Knowledge Distillation-driven Cross-node Modalitiy Reconstruction Network}
\para{Impute Missing Modalities.} Different from existing works such as AutoFed~\cite{zheng2023autofed}, which focus on reconstructing local missing modalities while ignoring cross-node feature information, \textsf{FedMobile} (as shown in Fig. \ref{fig-2}) aims to collaboratively utilize the common feature subspace across nodes to iteratively reconstruct the feature information of missing modalities. Specifically, to gain insight into the data distribution of missing modalities across nodes and use this understanding to guide local model training with incomplete modalities, we employ conditional distributions to characterize the modal data distribution of each node. Let $Q_k: \mathcal{Y}_k \rightarrow \mathcal{X}_k$ denote the above conditional distribution, which is tailored for each node and aligns with the ground truth data distribution. This distribution encapsulates the necessary knowledge to guide multimodal FL training with incomplete modalities:
\begin{equation}\label{eq-4}
Q_k=\underset{Q_k: \mathcal{Y}_k \rightarrow \mathcal{X}_k}{\arg \max } \mathbb{E}_{y \sim p(y_k)} \mathbb{E}_{x \sim Q_k(\mathbf{X}_k \mid y_k)}[\log p(y | x ; \boldsymbol{\omega}_k)],
\end{equation}
where \( p(y_k) \) and \( p(y \mid x) \) denote the ground-truth prior and posterior distributions of the target labels, respectively. Given these conditions, we employ local models to infer \( p(y \mid x) \). Consequently, a straightforward approach involves the direct optimization of Eq. \eqref{eq-4} in the input space \(\mathcal{X}_k\) to approximate features for missing modalities. However, when \(\mathcal{X}_k\) is of high dimensionality, this approach may lead to computational overload and could potentially disclose information about user data configuration files. Therefore, a more feasible alternative is to reconstruct an induced distribution \( \psi_k: \mathcal{Y}_k \rightarrow \mathcal{Z}_k \) over a latent space. This latent space, being more compact than the raw data space, can help mitigate certain privacy-related concerns:
\begin{equation}\label{eq-5}
\psi_k=\underset{\psi_k: \mathcal{Y}_k \rightarrow \mathcal{Z}_k}{\arg \max } \mathbb{E}_{y \sim \hat{p}(y_k)} \mathbb{E}_{z \sim \psi_k(\mathbf{Z}_k \mid y_k)}\left[\log p(y|z;\boldsymbol{\omega}_k) \right] .
\end{equation}
Hence, nodes engage in knowledge extraction from missing modality data by acquiring insights from a parameterized condition generator $\psi_k$ by $\omega_G^k$. The optimization process is as follows:
\begin{equation}
\min _{\boldsymbol{\omega_G^k}} J(\boldsymbol{\omega_G^k})=\min \mathbb{E}_{y \sim p(y_r)} \mathbb{E}_{z \sim \psi_k(z \mid y_r)}\left[\mathcal{L}\left(\sigma\left( g\left(z ; \boldsymbol{\omega}_k\right)\right), y\right)\right],
\label{eq-6}
\end{equation}
where $y_r$ represents a set of random labels generated from the training dataset $\mathcal{D}_{k}$, $g(\cdot)$ denotes the logits output of a predictor, and $\sigma(\cdot)$ signifies the non-linear activation applied to these logits. 


\para{Align Missing Modalities.} On the other hand, it is necessary to refine feature subspaces to more accurately encapsulate the local knowledge of nodes. For instance, considering a two-modality task, we can derive the generated latent space via the labels $y_k$: $\mathcal{Z}^{m_0},\mathcal{Z}^{m_1}=\psi_k\left(y_k ; \omega_G^k\right)$, where $\mathcal{Z}^{m_0}$ and $\mathcal{Z}^{m_1}$ represent the respective latent features of each modality. Assuming $m_1$ denotes the missing modality, our objective is to further empower $\psi_k$ to assimilate knowledge from various modalities, thereby enhancing the completeness and generalization of the feature space. For modality $m_0$, the learning process can be expressed as follows:
\begin{equation}\label{eq-7}
\mathcal{L}_{KL}^{m_0}(\omega_G^k ; \omega_k)=\min _{\omega_G^k} \sum_{i=1}^B\mathbb{E}_{x \sim \mathcal{D}_{k}}\left[D _ { \mathrm { KL } } \left[\left(f_0\left(x_i^{m_0} ; \boldsymbol{\omega}_k^{m_0} \right) \| \mathcal{Z}_i^{m_0}\right)\right]\right],
\end{equation}
where $B$ represents the number of samples in the local training batch. For modality $m_1$, we only learn from the missing data of this modality, which is formally expressed as follows:
\begin{equation}\label{eq-8}
\mathcal{L}_{KL}^{m_1}(\omega_G^k ; \omega_k)=\min _{\omega_G^k}\sum_{i=1}^I \mathbb{E}_{x \sim \mathcal{D}_{k}}\left[D _ { \mathrm { KL } } \left[\left(f_1\left(x_i^{m_1} ; \boldsymbol{\omega}_k^{m_1} \right) \| \mathcal{Z}_i^{m_1}\right)\right]\right],
\end{equation}
where $I$ represents the remaining number of samples. Finally, we use $\mathcal{Z}^{m_1}$ instead of the feature $f_1\left(x_k^{m_1} ; \boldsymbol{\omega}_k^{m_1} \right)$ of modality $m_1$ for multimodal feature fusion (\eg, concatenated fusion) to achieve feature alignment for missing modality. According to the above method, the overall optimization goal of every FL node is:
\begin{equation}\label{eq-9}
\min _{\omega_G^k,\omega_k}\mathcal{L}_{Train}^k= J(\omega_G^k)+\mathcal{L}_{KL}^{m_0}(\omega_G^k ; \omega_k)+\mathcal{L}_{KL}^{m_1}(\omega_G^k ; \omega_k)+\mathcal{L}_{CE}(\omega_k),
\end{equation}
where $\mathcal{L}_{CE}(\omega_k)$ represents the cross entropy loss of model training.

\para{Transfer Feature Space.} In this context, we consider the global distribution generator, denoted as $\hat{\psi}$, and the set of local distribution generators, represented by $\psi_k$ for each node $k$, as the source and target domains, respectively, in a framework of domain adaptation. This particular form of adaptation is referred to as global-to-local knowledge transfer. Conversely, the local-to-global knowledge transfer takes place at the server side. During the knowledge exchange process, the node $k$ transmits its locally generated distribution model, $\psi_k$, to the server. The server then orchestrates a guided adjustment of $\psi_k$ with the aim of systematic reduction in the discrepancy between the local and global knowledge domains through the mechanism of FL aggregation. The above process can be formalized as follows:
\begin{equation}
{\psi} = \frac{1}{{K}} \sum_{k=1}^{{K}} \psi_k.
\label{eq-10}
\end{equation}

\subsection{Clustering Shaple Value-driven Generator Contribution Evaluation Module}

\para{Evaluate Generator Contribution.} Considering the inherent heterogeneity of data across nodes and the varied modality missing scenarios that often arise on individual nodes, a naive aggregation of the local distribution models $\psi_k$ for knowledge transfer might inadvertently cause a general shift in the collective knowledge domain, leading to counterproductive outcomes. To mitigate this issue, we use the SV method to quantitatively evaluate the marginal contribution of each distinct $\psi_k$ to the overarching learning task. However, directly applying the SV to compute the marginal contributions of individual nodes is computationally burdensome, especially in FL scenarios involving hundreds of mobile devices. To address this challenge, we incorporate the K-means clustering algorithm to reduce the computational complexity of the SV computation. Specifically, we employ the K-means clustering algorithm to cluster the $\mathcal{Z}$ generated by $\psi_k$, resulting in multiple clusters containing $\psi_k$. We then perform average aggregation on the generator parameters in the cluster to obtain $\hat \psi $ as the node representative of the cluster. In this way, we can get $\mathcal{K}$ node representatives and use these node representatives as a set $N = \{ {\hat \psi _1}, \ldots ,{\hat \psi _\mathcal{K}}\} $ in SV. Consequently, the final computation of SV can be expressed as:
\begin{equation}\label{eq-11}
\footnotesize
\tiny
{\Phi _i}(\mathcal{F}) = \sum\limits_{S \subseteq N \setminus \{ {{\hat \psi }_i}\} } {\frac{{|S|!(|N| - |S| - 1)!}}{{|N|!}}} \left[ {\mathcal{P}(\mathcal{F}({\psi _S}({y_r^{'}}) \cup \{ {{\hat \psi }_i({y_r^{'}})}\} )) - \mathcal{P}(\mathcal{F}({\psi _S}({y_r^{'}})))} \right],
\end{equation}
where $\mathcal{P}$ is a performance metric function, such as accuracy, F1-score, or loss, $\mathcal{F}$ represents the global model, and $y_r^{'}$ is a set of randomly generated labels from the proxy dataset $\mathcal{D}_{proxy}$. Subsequently, the normalized $\Phi_1(\mathcal{F}), \Phi_2(\mathcal{F} ), \ldots, \Phi_\mathcal{K}(\mathcal{F})$ is used as the aggregation weight, therefore Eq. \eqref{eq-10} can be rewritten as:
\begin{equation}\label{eq-12}
\psi  = \frac{1}{\mathcal{K}}\sum\limits_{i = 1}^\mathcal{K} {{\Phi _i}} (\mathcal{F}){\hat \psi _i}.
\end{equation}





\subsection{Contribution-aware Aggregation Rule}
\para{Node Contribution.}
To generalize node contribution in a fine-grained manner, we divide the contribution of each node in each round into local and global contributions. The local contribution represents the node's performance in that round of local training, while the global contribution represents the node's impact on model aggregation. We can calculate the local contributions as follows:
\begin{equation}\label{eq-13}
P_{\text{local}, k}^t = \mathcal{P}(\omega_k^t; \mathcal{D}_{proxy}),
\end{equation}
where $\mathcal{P}$ is a performance metric function, such as accuracy, F1-score, or loss, {$\mathcal{D}_{proxy}$ represents the proxy dataset, and $\omega_k$ represents the local model parameters of node $k$. It is important to note that the proxy dataset does not compromise the privacy of the training set and can be collected by the server, as is consistent with previous work~\cite{sun2023shapleyfl,xue2021toward}. Furthermore, we need to traverse all nodes to calculate the above contribution. To assess how much a node's update would improve the global model, we can perform a hypothetical update by applying only node $k$'s update to the global model and measuring the global contribution:
\begin{equation}
   \mathbf{w}_{\text{temp}, k}^{t+1} = \omega^t + \eta \Delta \omega_k^t,
\end{equation}
\begin{equation}\label{eq-15}
\begin{aligned}
  \Delta P_{{\text{global}},k}^t &= \mathcal{P}(\omega _{{\text{temp}},k}^{t + 1};{\mathcal{D}_{proxy}}) - \mathcal{P}({\omega ^t};{\mathcal{D}_{proxy}}) \\ 
   &= P_{{\text{global}},k}^{t + 1} - P_{{\text{global}}}^t \\ 
\end{aligned} 
\end{equation}
where $\eta$ is the learning rate. Due to computational constraints (since evaluating each node's update individually can be costly), we can approximate this by estimating the potential improvement based on surrogate metrics. Hence, we can approximate it using the node's local loss reduction metric as follows:
\begin{equation}\label{eq-16}
\Delta \mathcal{L}_k^t = \mathcal{L}(\omega^t; \mathcal{D}_{proxy}) - \mathcal{L}(\omega_k^t; \mathcal{D}_{proxy}).
\end{equation}
We use \( \Delta \mathcal{L}_k^t \) as a proxy for \( \Delta P_{\text{global}, k}^t \). The reason we do this is that larger differences have a larger impact on the global model.


\para{Node Contribution-aware Aggregation.} Upon determining the global and local contributions, we strive to incorporate them adaptively into the quality assessment process of nodes participating in model aggregation, thereby mitigating the impact of updating nodes with lower quality. To achieve this goal, we can define the aggregation weight \( \alpha_k^t \) for a node \( k \) as a function of \( P_{\text{local}, k}^t \) and \( \Delta P_{\text{global}, k}^t \):
\begin{equation}\label{eq-18}
    \alpha_k^t = \frac{p(P_{\text{local}, k}^t, \Delta P_{\text{global}, k}^t)}{\sum_{j=1}^K p(P_{\text{local}, j}^t, \Delta P_{\text{global}, j}^t)},
\end{equation}
where \( p \) is a function that combines the two performance metrics. Here, an intuitive choice for \( p \) is to multiply the normalized performance metrics. Thus, we normalize the Local Contribution Metrics: $ \tilde{P}_{\text{local}, k}^t = \frac{P_{\text{local}, k}^t}{\sum_{j=1}^K P_{\text{local}, j}^t}$, and we normalize the Global Contribution Improvements: $   \tilde{\Delta} P_{\text{global}, k}^t = \frac{\Delta P_{\text{global}, k}^t}{\sum_{j=1}^K \Delta P_{\text{global}, j}^t}$. Combining Eqs. \eqref{eq-16} and \eqref{eq-18}, if we use local loss reduction, we have:
\begin{equation}\label{eq-19}
    \alpha_k^t = \frac{n_k \times \tilde{P}_{\text{local}, k}^t \times \tilde{\Delta} \mathcal{L}_k^t}{\sum_{j=1}^K n_j \times \tilde{P}_{\text{local}, j}^t \times \tilde{\Delta} \mathcal{L}_j^t}
\end{equation}
where \( \sum_{k=1}^K \alpha_k^t = 1 \). Therefore, we can use this weight to update the global model:
\begin{equation}\label{eq-20}
        \omega^{t+1} = \omega^t + \eta \sum_{k=1}^K \alpha_k^t \Delta \omega_k^t.
\end{equation}
The above process is summarized in Algo. \ref{alg-1} in the Appendix \ref{algo}.





\section{Experiment}
\subsection{Experiment Setup}
To evaluate the performance of our \textsf{FedMobile} system, we conduct extensive experiments on four benchmarking datasets. All experiments are developed using Python 3.9 and PyTorch 1.12 and evaluated on a server with an NVIDIA A100 GPU.

\para{Datasets.} We adopt five multimodal datasets for evaluations, \ie, USC-HAD~\cite{zhang2012usc}, MHAD~\cite{ofli2013berkeley}, ADM~\cite{ouyang2023harmony}, C-MHAD~\cite{wei2020c}, and FLASH~\cite{salehi2022flash} datasets. The datasets cover different modalities, objects, domains, attributes, dimensions, and number of classes, as shown in Table \ref{tab-100}, allowing us to explore the learning effectiveness of \textsf{FedMobile}. To simulate an environment characterized by incomplete sensing modalities, we adopt a random selection methodology to identify a target mode from the local dataset, which will represent the state of incompleteness. We then proceed to randomly eliminate a predetermined proportion of the modal data, thereby simulating the phenomenon of missing information. Note that we distinguish between the small-scale node and large-scale node scenarios according to the scale of users (\ie, nodes) involved in the dataset. In the dataset used, FLASH will be evaluated in the large-scale node scenario. More details can be found in Appendix \ref{data}.


\para{Models.} When processing ADM dataset, we harness the TDNN for audio feature extraction and combine it with CNN layers for radar and depth image feature extraction. For USC, MHAD, and FLASH datasets, a 2D-CNN model is utilized to process accelerometer data, whereas a 3D-CNN architecture is employed to analyze skeleton data. Finally, when working with the CMHAD dataset, we exploit a 2D-CNN architecture to derive video features, while 3D-CNN layers are used for extracting features from inertial sensors.


\para{Parameters.} For the ADM dataset, we set the learning rate at 1e-3, with a batch size of 64. Regarding the USC dataset, the learning rate is 1e-6, and the batch size is 16. For the MHAD and FLASH datasets, the learning rate is 1e-3, with a batch size of 16. When working with the CMHAD datasets, we maintain a learning rate of 1e-4, alongside a batch size of 16. Throughout this experiment, we utilize the SGD optimizer with a momentum of 0.9 and a weight decay of 1e-4. We set the total number of nodes $K=10$, local epoch $E=5$, global epoch $T=100$, and node participation rate $q =100\%$. We set $\mathcal{K}=5$ in the K-means algorithm. We use a multilayer perceptron as our generator (see Appendix \ref{mlp}).



\para{Baselines.} To make a fair comparison, we employ FedProx~\cite{li2020federated}, FedBN~\cite{li2020fedbn}, FedMM~\cite{feng2023fedmultimodal}, PmcmFL~\cite{bao2023multimodal}, Harmony~\cite{ouyang2023harmony}, and AutoFed~\cite{zheng2023autofed} as baseline methods. Among these, the first three techniques require adaptation to cope with scenarios characterized by incomplete modalities. This adaptation is achieved through the integration of interpolation techniques, namely zero padding (ZP) and random padding (RP), which are incorporated into the FedProx, FedBN, and FedMM baselines. This augmentation enables us to gauge the effectiveness of \textsf{FedMobile} in dealing with heterogeneous modalities. On the other hand, PmcmFL, Harmony, and AutoFed are multimodal FL solutions that naturally cater to situations involving incomplete modalities without needing further modifications to their methodologies. Thus, we can directly assess \textsf{FedMobile}'s learning performance in similar contexts using these baseline methods. Note that all comparison results are the average of five repeated experiments to eliminate the effect of randomness.





\para{Metrics.} To assess the performance of our proposed method and benchmark it against the baseline approaches, we employed accuracy as the evaluation metric, a convention that has been widely utilized in prior research~\cite{cho2022flame}. To quantify the computational overhead, we tracked the aggregate time consumed in uploading and downloading models for all participating nodes throughout the FL training process, as was previously done in \cite{salehi2022flash}. And we computed the cumulative GPU usage across all nodes engaged in the FL training phase. For communication overhead, we perform a fair comparison by calculating the model updates that need to be transmitted for 100 rounds of global training.

\subsection{Numerical Results}



\begin{table*}[!t]
    \centering
    \footnotesize
      \caption{Numerical results of system performance.}
      \label{tab-2}
    \begin{adjustbox}{width=0.8\textwidth}
    \begin{tabular}{c|c|ccccccccc|c}
    \toprule
    \toprule
        \textbf{Datasets} & \textbf{$\beta$} & \textbf{FedProx+ZP} & \textbf{FedProx+RP} & \textbf{FedBN+ZP} & \textbf{FedBN+RP} & \textbf{FedMM+ZP} & \textbf{FedMM+RP} & \textbf{PmcmFL} &\textbf{Harmony} &\textbf{AutoFed}& \textbf{FedMobile} \\ 
        \midrule
        \textbf{} & 20\% & 75.3 & 77.0 & 75.3 & 77.9 & 78.0 & 76.7 & 78.0 &76.5 &77.6&\textbf{78.4}  \\ 
        \textbf{} & 40\% & 75.2 & 75.9 & 74.5 & 75.3 & 77.5 & 74.1 & 77.3 &76.7 &76.8&\textbf{77.9}  \\ 
        \textbf{} & 60\% & 75.3 & 75.7 & 75.3 & 75.2 & 76.5 & 74.6 & 76.5 &75.9 &75.8&\textbf{76.5}  \\ 
        \textbf{MHAD} & 70\% & 75.2 & 75.0 & 74.9 & 74.9 & 76.1 & 74.6 & 76.2 & 75.8&75.4&\textbf{76.5}  \\ 
        \textbf{} & 80\% & 74.9 & 76.1 & 74.1 & 75.4 & 76.2 & 74.3 & 76.0 &74.3 &76.1&\textbf{78.0}  \\ 
        \textbf{} & 90\% & 75.0 & 75.1 & 74.2 & 74.9 & 75.8 & 74.5 & 75.8 & 74.5&75.4&\textbf{76.8}  \\ 
        \midrule
        \textbf{} & 20\% & 56.6 & 56.6 & 58.6 & 57.3 & 58.2 & 58.4 & 59.1 & 58.7&60.4&\textbf{61.1}  \\ 
        \textbf{} & 40\% & 58.7 & 57.2 & 56.9 & 57.5 & 56.2 & 58.0 & 58.6 & 57.9&58.7&\textbf{62.8}  \\ 
        \textbf{} & 60\% & 57.3 & 58.2 & 57.0 & 57.6 & 57.2 & 58.6 & 57.9 & 58.7&60.1&\textbf{61.0}  \\ 
        \textbf{USC-HAD} & 70\% & 57.2 & 57.9 & 57.3 & 57.0 & 57.5 & 57.8 & 58.2 &57.1 &58.4&\textbf{59.8}  \\ 
        \textbf{} & 80\% & 57.5 & 56.3 & 57.6 & 57.1 & 56.9 & 57.5 & 58.0 & 56.8&58.2&\textbf{58.8}  \\ 
        \textbf{} & 90\% & 57.8 & 57.7 & 57.1 & 57.3 & 57.1 & 57.8 & 58.1 & 56.8&57.6&\textbf{59.6}  \\
        \midrule
        \textbf{} & 20\% & 82.5 & 83.5 & 83.0 & 82.4 & 82.5 & 83.1 & 83.6 &82.8 &83.9&\textbf{84.9}  \\ 
        \textbf{} & 40\% & 82.1 & 82.6 & 82.2 & 83.0 & 82.7 & 83.3 & 84.0 & 83.4&83.6&\textbf{84.3}  \\ 
        \textbf{} & 60\% & 81.3 & 81.0 & 82.4 & 82.8 & 80.0 & 81.8 & 83.8 & 82.9&83.8&\textbf{84.4}  \\ 
        \textbf{ADM} & 70\% & 81.4 & 82.1 & 81.7 & 81.2 & 81.5 & 82.3 & 82.8 & 81.6&83.2&\textbf{84.2}  \\ 
        \textbf{} & 80\% & 81.8 & 80.2 & 81.3 & 80.5 & 80.8 & 82.0 & 83.1 &81.9 &83.5&\textbf{84.0}  \\ 
        \textbf{} & 90\% & 82.7 & 81.9 & 80.8 & 80.3 & 81.7 & 82.3 & 83.2 & 80.9&83.2&\textbf{84.4}  \\ 
        \midrule
        \textbf{} & 20\% & 75.7 & 75.2 & 75.6 & 75.3 & 75.0 & 75.2 & 76.2 & 75.9&76.4&\textbf{76.7}  \\ 
        \textbf{} & 40\% & 74.4 & 75.0 & 74.7 & 74.2 & 74.0 & 74.1 & 74.9 & 74.6&75.1&\textbf{75.8}  \\ 
        \textbf{} & 60\% & 73.7 & 74.2 & 73.9 & 73.5 & 72.7 & 73.4 & 74.0 & 73.7&74.5&\textbf{75.6}  \\ 
        \textbf{C-MHAD} & 70\% & 74.2 & 74.7 & 74.4 & 74.0 & 70.4 & 73.9 & 74.2 & 72.2&73.9&\textbf{76.1}  \\ 
        \textbf{} & 80\% & 74.7 & 75.2 & 74.9 & 74.5 & 73.3 & 74.4 & 74.8 & 72.8&75.3&\textbf{77.2}  \\ 
        \textbf{} & 90\% & 74.1 & 74.6 & 74.0 & 73.9 & 73.1 & 73.8 & 74.5 & 73.1&75.4&\textbf{77.3}  \\ 
        \bottomrule
        \bottomrule
    \end{tabular}
    \end{adjustbox}
    \vspace{-0.4cm}
\end{table*}

\begin{table}[!t]
    \centering
    \footnotesize
    \caption{Performance results for the two-modality missing scenario.}
    \label{tab-performance-comparison}
    \begin{adjustbox}{width=0.35\textwidth}
    \begin{tabular}{c|l|c|c|c}
    \toprule
    \toprule
       \textbf{Modality Type} & \textbf{Method} & \textbf{40\%} & \textbf{60\%} & \textbf{80\%} \\ 
        \midrule
        & FedMM+ZP & 69.8 & 58.5 & 60.2 \\ 
        & FedMM+RP & 69.2 & 68.5 & 61.3 \\ 
        & FedProx+ZP & 69.3 & 59.8 & 60.3 \\ 
        & FedProx+RP & 68.9 & 70.7 & 60.9 \\ 
    \textbf{Audio and Radar} & FedBN+ZP & 68.7 & 59.1 & 59.3 \\ 
        & FedBN+RP & 68.2 & 68.2 & 59.6 \\ 
        & PmcmFL & 69.8 & 71.3 & 61.7 \\ 
        & Harmony & 69.2 & 72.7 & 60.7 \\ 
        & AutoFed & 69.4 & 68.5 & 61.2 \\ 
        & \textbf{Ours} & \textbf{70.0} & \textbf{77.7} & \textbf{62.1} \\ 
        \midrule
        & FedMM+ZP & 82.3 & 79.3 & 79.2 \\ 
        & FedMM+RP & 78.8 & 81.8 & 79.5 \\ 
        & FedProx+ZP & 82.1 & 80.3 & 80.2 \\ 
        & FedProx+RP & 81.4 & 82.0 & 79.8 \\ 
    \textbf{Audio and Depth Image} & FedBN+ZP & 81.8 & 80.8 & 78.7 \\ 
        & FedBN+RP & 77.3 & 79.2 & 78.9 \\ 
        & PmcmFL & 83.1 & 82.0 & 81.6 \\ 
        & Harmony & 82.4 & 78.8 & 81.4 \\ 
        & AutoFed & 82.8 & 79.4 & 81.0 \\ 
        & \textbf{Ours} & \textbf{83.3} & \textbf{82.3} & \textbf{82.3} \\ 
        \midrule
        & FedMM+ZP & 33.1 & 37.9 & 28.5 \\ 
        & FedMM+RP & 38.3 & 35.4 & 27.9 \\ 
        & FedProx+ZP & 33.0 & 37.5 & 28.2 \\ 
        & FedProx+RP & 36.2 & 36.9 & 27.8 \\ 
    \textbf{Radar and Depth Image} & FedBN+ZP & 32.7 & 37.2 & 27.9 \\ 
        & FedBN+RP & 35.3 & 35.1 & 28.2 \\ 
        & PmcmFL & 36.5 & 38.0 & 28.4 \\ 
        & Harmony & 35.9 & 37.4 & 30.8 \\ 
        & AutoFed & 37.6 & 38.7 &31.1  \\ 
        & \textbf{Ours} & \textbf{39.1} & \textbf{41.3} & \textbf{32.1} \\ 
        \bottomrule
        \bottomrule
    \end{tabular}
    \end{adjustbox}
       \vspace{-0.4cm}
\end{table}

\begin{table}[!t]
    \centering
    \footnotesize
    \caption{Cost comparison of different methods.}
    \label{tab-performance}
    \begin{adjustbox}{width=0.35\textwidth}
    \begin{tabular}{l|c|c|c|c|c|c}
    \toprule
    \toprule
        \textbf{Method} &FedAvg & FedMM & {PmcmFL} & {FedProx} & {FedBN} & \textbf{Ours} \\ 
        \midrule
        \textbf{GPU Usage (\%)} & 12& 16 & 15 & 21 & 21 & 18 \\ 
        \textbf{Local Running Time (s)} &8.94& 11.65 & 12.86 & 12.23 & 11.57 & 12.12 \\ 
        \bottomrule
        \bottomrule
    \end{tabular}
    \end{adjustbox}
       \vspace{-0.4cm}
\end{table}



\para{Research Questions.} In this section, we aim to answer the following research questions:

$\bullet$ (RQ1) How effectively does \textsf{FedMobile}, along with its respective baseline methods, fare in handling diverse and complex scenarios characterized by incomplete modal environments?

$\bullet$ (RQ2) How does \textsf{FedMobile} demonstrate computational and communicational efficiency in its running processes?

$\bullet$ (RQ3) How does \textsf{FedMobile} perform in heterogeneous data scenarios, especially in dynamic modality missing scenarios?

$\bullet$ (RQ4) How does \textsf{FedMobile} perform in scenarios with large-scale nodes and missing modalities?

$\bullet$ (RQ5) What are the capabilities of \textsf{FedMobile} in terms of multi-modal feature extraction, and how proficiently can it harness and integrate features from multiple modalities?

$\bullet$ (RQ6) How do the individual components of the \textsf{FedMobile} framework contribute to its overall performance, and what specific impact do they have on its effectiveness?

\para{System Performance (RQ1).} To address RQ1, we perform an extensive evaluation of \textsf{FedMobile}, along with its comparative baseline algorithms, using four benchmark multimodal datasets. To assess performance under diverse levels of modal data loss, we introduce a set of modality missing rates designated as $\beta = \{20\%, 40\%, 60\%, 70\%, 80\%, 90\%\}$. The experimental results demonstrate that \textsf{FedMobile} outperforms all other baseline algorithms consistently across all these varying degrees of missing modality data, as clearly depicted in Table \ref{tab-2}. Notably, \textsf{FedMobile} showcases a 1.9\% improvement relative to the current state-of-the-art baseline, AutoFed, specifically in the MHAD dataset with $\beta=80\%$. These enhanced results stem from \textsf{FedMobile}'s innovative strategy, which entails reconstructing modal features across nodes and tactically selecting nodes with high-quality contributions. By discovering a shared feature subspace among distinct missing modalities, \textsf{FedMobile} efficiently reconstructs features and simultaneously excludes nodes with inferior-quality data, thereby boosting the performance. 
%Furthermore, we report system performance results in the absence of two modes in the Appendix \ref{exper}.

To further evaluate the performance of \textsf{FedMobile}, we tested it under a more challenging scenario involving the absence of two modalities (\ie, two-modal data missing). Specifically, we randomly omitted two modalities in a fixed ratio within the ADM dataset, which consists of three modal data, and maintained this missing configuration throughout the training process. The numerical results, recorded in Table \ref{tab-performance-comparison}, demonstrate that \textsf{FedMobile} continues to deliver excellent performance, outperforming other state-of-the-art baselines, including an average 4.3\% improvement over AutoFed on the ADM dataset. Additionally, we observed that existing methods struggle with missing data across multiple modalities, as they heavily depend on sufficient modal information to reconstruct the missing data. \textsf{FedMobile}, on the other hand, does not require this, making it more robust in handling such scenarios. Additionally, we provide a privacy analysis in Appendix \ref{privacy}.


\para{Computational \& Communication Overhead (RQ2).} To address RQ2, we systematically document and analyze the communication cost, local running time, and GPU usage of all examined methods on the USC dataset with $\beta=60\%$. Note that since AutoFed and Harmony also include hardware equipment, they are not included in the comparison. For the convenience of comparison, we record the communication overhead of 100 global training rounds and ignore factors such as the network environment. First, while the introduction of the generator does cause additional communication overhead, this overhead is acceptable. Specifically, the additional overhead caused by the generator is 1.65 MB for each training round. Furthermore, compared to baselines such as FedProx, which do not introduce much additional overhead, our method only adds an additional 9.02\% communication overhead, as shown in Fig. \ref{fig-3}. In performance-critical multimodal services, a small amount of additional communication overhead is acceptable because it improves the quality of service (\ie, accuracy), which is a performance-overhead trade-off. The results depicted in Figs. \ref{fig:img1}--\ref{fig:img2} and Table \ref{tab-performance} show that the GPU utilization and local running time of \textsf{FedMobile} consistently remains lower than or close to that of the comparative baseline methods. This indicates that our approach does not appreciably increase local computational overhead. Given that servers typically operate as resource-rich cloud infrastructure, computations related to the server-side SV calculation do not impose any significant extra computational load. 




\begin{figure}[!t]
 \centering
 \includegraphics[width=0.35\textwidth]{fig-3.pdf}
  \caption{Communication cost of \textsf{FedMobile} and baselines.}
  \label{fig-3}
  \vspace{-0.4cm}
\end{figure}

\begin{figure*}[!t]
    \centering  
    \begin{subfigure}
        \centering  
        \includegraphics[width=0.19\textwidth]{tsne_prox0.pdf}  
        \label{fig:img3}  
    \end{subfigure}  
    \hfill  
    \begin{subfigure}  
        \centering  
        \includegraphics[width=0.19\textwidth]{tsne_bn0.pdf}  
        \label{fig:img4}  
    \end{subfigure}  
    \label{fig:subfigures}
    \begin{subfigure}  
        \centering  
        \includegraphics[width=0.19\textwidth]{tsne_avg0.pdf}  
        \label{fig:img5}  
    \end{subfigure}  
    \label{fig:subfigures}  
    \begin{subfigure}  
        \centering  
        \includegraphics[width=0.19\textwidth]{tsne_pcm0.pdf}  
        \label{fig:im6}  
    \end{subfigure}  
    \label{fig:subfigures}  
    \begin{subfigure}  
        \centering  
        \includegraphics[width=0.19\textwidth]{tsne_ours0.pdf}  
        \label{fig:img7}  
    \end{subfigure}  
    \caption{Feature visualization results of different methods.}  
    \label{fig:T-SNE}  
    \vspace{-0.4cm}
\end{figure*}


\para{Data Heterogeneity Scenarios (RQ3).} To address RQ3, we evaluate the performance of the baselines and \textsf{FedMobile} in a dynamic modality-missing scenario on the ADM dataset. Unlike unimodal FL, where the Dirichlet function is commonly used to control the degree of heterogeneity (\ie, non-IID data), we dynamically adjust both the modality missing rate and the number of missing modality types to control heterogeneity in multimodal FL. The primary reason for this adjustment is that multimodal FL involves multiple types of data, making it difficult for the Dirichlet function to reasonably partition the data. We further define two heterogeneous scenarios: (1) scenarios with different distributions of the number of missing modality types and (2) scenarios with varying distributions of modality missing rates, as illustrated in Figs. \ref{fig-6}--\ref{fig-7}. In Scenario 1, we set the modality missing rate at $\beta=$ 40\% and randomly omit different numbers of modality types. The experimental results, summarized in Table \ref{tab-4}, represent the average of five repeated trials. These results indicate that \textsf{FedMobile} consistently outperforms the other baselines, demonstrating its robustness in handling dynamic modality-missing scenarios. For Scenario 2, we control the number of missing modality types but dynamically adjust the modality missing rate, ranging from a maximum of $\beta=$ 80\% to a minimum of $\beta=$ 20\%. The corresponding results, also presented in Table \ref{tab-4}, show that \textsf{FedMobile} again outperforms the baselines, highlighting its strong performance even in these challenging non-IID settings.




\begin{table}[!t]
    \centering
    \footnotesize
    \caption{Performance results in the heterogeneity scenario.}
    \label{tab-4}
    \begin{adjustbox}{width=0.35\textwidth}
    \begin{tabular}{c|l|cc}
    \toprule
    \toprule
      \textbf{Dataset}  & \textbf{Method} & \textbf{Scenario 1} & \textbf{Scenario 2} \\ 
        \midrule
        & FedMM+ZP & 69.3 & 71.4 \\ 
        & FedMM+RP & 69.5 & 72.1 \\ 
        & FedProx+ZP & 70.2 & 73.7  \\ 
        & FedProc+RP & 69.5 & 73.2 \\ 
    \textbf{ADM} & FedBN+ZP & 69.8 & 71.3 \\ 
        & FedBN+RP & 70.3 & 71.7 \\ 
        & PmcmFL & 71.5 &  74.5\\ 
        & Harmony & 70.7  &76.8\\ 
        & AutoFed & 70.1 & 77.9 \\ 
        & \textbf{Ours} & \textbf{76.5} & \textbf{80.2}\\ 
        \bottomrule
        \bottomrule
    \end{tabular}
    \end{adjustbox}
    \vspace{-0.4cm}
\end{table}


\para{Large Node Scenario (RQ4).} To address RQ4, we investigate the performance of \textsf{FedMobile} and its baselines on large-scale nodes. While the dataset in RQ1 was collected from 10 nodes, we use the FLASH dataset, which involves 210 nodes, to more effectively partition the multimodal data and validate their performance. Specifically, we set modality missing rates at 40\%, 60\%, and 80\%, and conduct five repeated experiments to record the average model accuracy. The results, presented in Table \ref{tab-5}, show that \textsf{FedMobile} consistently outperforms the other advanced baselines even on large-scale nodes, demonstrating that its performance is not limited by the scale of the mobile node.

\begin{table}[!t]
    \centering
    \footnotesize
    \caption{Performance results on FLASH dataset.}
    \label{tab-5}
    \begin{adjustbox}{width=0.35\textwidth}
    \begin{tabular}{c|l|c|c|c}
    \toprule
    \toprule
     \textbf{Dataset}   & \textbf{Method}  & \textbf{40\%} & \textbf{60\%} & \textbf{80\%} \\ 
        \midrule
        & FedMM+ZP& 52.7&	51.1&	50.4	\\
        & FedMM+RP & 53.4	&52.3&	51.2	\\
        & FedProx+ZP & 49.7	&50.1	&49.2	\\
        & FedProx+RP & 49.4&48.7&49.4\\   
    \textbf{FLASH} & FedBN+ZP &  49.9         & 49.1         & 47.8  \\ 
        & FedBN+RP & 50.4         &49.7          & 48.2   \\ 
        & PmcmFL & 52.4          & 51.6         & 50.4  \\ 
         &Harmony &  55.8          & 54.7          &54.1 \\ 
        &AutoFed &  54.9          & 53.4          &55.4 \\ 
        & \textbf{Ours} & \textbf{57.6} & \textbf{57.1} & \textbf{56.8} \\ 
        \bottomrule
        \bottomrule
    \end{tabular}
    \end{adjustbox}
    \vspace{-0.4cm}
\end{table}




\para{Feature Visualization (RQ5).} In response to RQ5, we undertake a qualitative evaluation of the multimodal features generated by the competing methods by visualizing them. For this purpose, we employ the t-distributed Stochastic Neighbor Embedding (t-SNE) technique on dataset MHAD to project the high-dimensional multimodal features extracted by each method onto a lower-dimensional space. The resulting dimensionality reduction is presented in Fig. \ref{fig:T-SNE}. Our visualization results indicate that \textsf{FedMobile} excels at extracting more precise and refined multimodal features, which in turn leads to enhanced classification accuracy. In comparison, alternative methods exhibit substantial deficiencies in feature extraction. This observation underscores the value of \textsf{FedMobile}'s dual strategies of local-to-global knowledge transfer and modal feature reconstruction, which collectively facilitate the effective exploitation and extraction of information from incomplete modal sources.


\begin{table}
	\centering
 \caption{Numerical results of ablation experiments.}
 \label{tab-3}
 \begin{adjustbox}{width=0.35\textwidth}
	\begin{tabular}{c|c|c|c|c|c}
       \toprule  
    \toprule  
    \textbf{Method} & \textbf{MHAD} & \textbf{USC} & \textbf{ADM} & \textbf{CMHAD} & \textbf{FLASH}\\
    \midrule
        Ours (w/o $\mathcal{L}_{Train}$) & 74.7 & 57.5 & 79.8 & 79.2 &52.7 \\ 
        Ours (w/o SV) & 77.3 & 61.6 & 81.8 & 82.7 & 56.9\\ 
        Ours (w/o $\alpha$) & 74.5 & 58.6 & 84.1 & 77.4 & 53.8 \\ 
    \midrule
        \textbf{Ours} & \textbf{77.9} & \textbf{62.8} & \textbf{84.3} & \textbf{75.8}&\textbf{57.6}  \\ 
    \bottomrule 
        \bottomrule 
	\end{tabular}
 \end{adjustbox}
 \vspace{-0.4cm}
\end{table}



\para{Ablation Studies (RQ6).} To investigate RQ6, we conduct a systematic dissection of \textsf{FedMobile} by analyzing the performance contributions of its constituent parts with $\beta=40\%$. To this end, we experimentally validate the performance of three ablated versions of \textsf{FedMobile} on four benchmark multi-modal datasets. Specifically, we successively deactivate the modal reconstruction network, the contribution sensing module, and the dynamic parameter aggregation module, forming three distinct variations of \textsf{FedMobile}. The experimental outcomes are summarized in Table \ref{tab-3}, demonstrating that the modal reconstruction network and the contribution sensing module play pivotal roles in determining \textsf{FedMobile}'s performance. On the other hand, the impact of the dynamic parameter aggregation module on \textsf{FedMobile}'s performance appears to be less pronounced. For illustration, when the modal reconstruction network is removed from \textsf{FedMobile}, the performance degradation on the MHAD dataset reaches 3.2\%, relative to the complete version of \textsf{FedMobile}. These findings highlight the critical importance of the modal reconstruction and contribution sensing mechanisms within the \textsf{FedMobile} framework.



\section{Conclusion}
The paper addresses the challenge of incomplete modalities in multimodal Federated Learning systems by proposing a new framework called \textsf{FedMobile}. Unlike existing methods that rely on unimodal subsystems or interpolation, \textsf{FedMobile} leverages cross-node multimodal feature information for reconstructing missing data and employs a knowledge contribution-aware mechanism to evaluate and prioritize node inputs, improving resilience to modality heterogeneity. The framework demonstrates superior performance in maintaining robust learning under significant modality loss compared to current standards, all while not increasing computational or communication costs. Overall, \textsf{FedMobile} represents a significant step forward in developing more efficient and resilient multimodal FL systems.


\section*{Acknowledgement}
The corresponding author is Cong Wang. The authors sincerely thank the reviewers for their invaluable feedback. This work was supported in part by the Hong Kong Research Grants Council under Grants CityU 11218322, 11219524, R6021-20F, R1012-21, RFS2122-1S04, C2004-21G, C1029-22G, C6015-23G, and N\_CityU139/21 and in part by the Innovation and Technology Commission (ITC) under the Joint Mainland-Hong Kong Funding Scheme (MHKJFS) under Grant MHP/135/23. This work was also supported by the InnoHK initiative, The Government of the HKSAR, and the Laboratory for AI-Powered Financial Technologies (AIFT).


\bibliographystyle{ACM-Reference-Format}
\balance
\bibliography{sample-base}

\clearpage
%%
%% If your work has an appendix, this is the place to put it.
\appendix


\section{Privacy Analysis}\label{privacy}
The proposed method for imputing missing modalities in \textsf{FedMobile} introduces privacy-preserving strategies by utilizing latent space reconstruction and conditional distributions across nodes. This privacy analysis explores how these strategies help mitigate privacy risks associated with multimodal FL under scenarios where data modalities are incomplete across nodes.

\para{Privacy Issues in Multimodal FL.} On the one hand, in traditional FL, model updates can inadvertently leak sensitive information about the local datasets, especially when gradients are shared directly~\cite{zhu2019deep}. Furthermore, when reconstructing missing modalities from available data, there is a risk that sensitive or private information about the original data can be exposed~\cite{feng2023fedmultimodal}. High-dimensional data spaces are particularly vulnerable to this risk.

\para{\textsf{FedMobile}'s Privacy-Preserving Mechanisms.} \textsf{FedMobile} addresses these privacy risks through two key techniques:

$\bullet$ Conditional Distribution in Latent Space. Instead of directly imputing missing modalities in the raw input space \( \mathcal{X}_k \), \textsf{FedMobile} reconstructs an induced distribution \( \psi_k \) over a latent space \( \mathcal{Z}_k \), as shown in Eq. \eqref{eq-5}. The latent space is more compact and lower-dimensional than the raw data space. This shift to a latent space significantly reduces the risk of privacy leakage because the latent representations contain abstracted information rather than raw, potentially sensitive features of the original data. Additionally, latent spaces typically obscure fine-grained details about individual data points, making it harder to reverse-engineer or infer private information from the shared model updates.

$\bullet$ Conditional Distributions for Imputation. \textsf{FedMobile} uses conditional distributions \( Q_k \) (in Eq. \eqref{eq-4}) and \( \psi_k \) (in Eq. \eqref{eq-5}) to model the relationships between the missing and present modalities. This distribution-based approach means that only the learned relationships between modalities are shared, not the actual data or detailed feature information. By focusing on conditional probabilities \( p(y \mid x) \) or \( p(y \mid z) \), the model implicitly encodes privacy since no raw features or labels are directly shared between nodes or with the central server. Instead, only probabilistic inferences are utilized, reducing the risk of reconstructing sensitive raw data.

Furthermore, existing work~\cite{wang2023more} has shown that it is difficult to recover training data by only obtaining gradient or feature information, as gradient leakage attacks are less effective on large training batches (\eg, batch size = 32). Additionally, gradient leakage or feature reconstruction attacks are typically effective for image data~\cite{zhu2019deep,wang2023more}, but their effectiveness is limited for data types such as radar and gyroscope data.


\begin{table*}[!t]
\centering

\caption{Summary of the four multimodal datasets.}
\resizebox{1\textwidth}{!}{
\begin{tabular}{cccccccc}
\toprule
\toprule
\textbf{Dataset} & \textbf{Modality Information} & \textbf{\# Classes} & \textbf{\# Users} & \textbf{Object} & \textbf{Domain} & \textbf{\# Samples} & \textbf{Size (MB)} \\
\midrule
USC-HAD & Acc, Gyro & 12& 10 & People & Activity Detection & 38312 & 38.5 \\
MHAD &  Acc, Skeleton & 11&10 & People& Activity Detection & 3956 & 187 \\
ADM & Audio, Radar, Depth Image & 11 &10 & People& Medical & 22452 & 30208 \\
C-MHAD & Video, Inertial Sensor & 7& 10 & People& Activity Detection & 7802 & 24268.8 \\
FLASH & GPS, LiDar, Camera& 64&210&Traffic Scenes &Autopilot& 32923& 5232.64\\
\bottomrule
\bottomrule
\end{tabular}}
\label{tab-100}
\end{table*}

\section{Proposed Algorithm}\label{algo}
An overview of the algorithm is as follows:
\SetCommentSty{mycommfont}
\begin{algorithm}[!t]
\DontPrintSemicolon
  \KwInput{Local model $\omega_k$, local generator $\omega_G$, and local multimodal dataset $\mathcal{D}_k$, and validation dataset $\mathcal{D}_{val}$.}
    \KwOutput{Global model $\omega$}
    The server initializes the generator and global model and sends them to each node\;
    
    \tcc{Knowledge Distillation-driven Cross-node Modalitiy Reconstruction Network}
    \While{local training}{
     Use the local model $\omega_k$ to perform feature extraction on the complete modality\;
     Generate random label $y_r$ from $\mathcal{D}_{val}$\;
      Input the random label $y_r$ and the missing modality label $y^m$ to the local generator $\omega_G$ to generate the corresponding features\;
      Calculate $J(\omega_G^k)$ to optimize the local generator \hfill $\rhd$\textcolor{gray}{Refer to Eq. \eqref{eq-6}}\;
      Calculate $\mathcal{L}_{KL}^{m_0}(\omega_G^k;\omega_k)$ and $\mathcal{L}_{KL}^{m_1}(\omega_G^k;\omega_k)$ to further align the features of the missing modality\hfill $\rhd$\textcolor{gray}{Refer to Eqs. \eqref{eq-7}--\eqref{eq-8}}\;
      Perform coupled training of local model and local generator via Eq. \eqref{eq-9}\;
    
    }
    The nodes upload $\omega_k$ and $\omega_G^k$ to the server\;
    \tcc{-- Server does: --}
\tcc{Clustered Shaple Value-driven Generator Contribution Evaluation Module}
\Do{excute the above module}{
Use the K-means algorithm to cluster the features of random labels $y_r$ extracted by the local generator $\omega_G^k$\;
Perform average aggregation on local generator parameters $\omega_G^k$ in each cluster\;
Use SV to calculate the marginal contribution to the global model for the obtained representative nodes and use it as an aggregate weight\hfill $\rhd$\textcolor{gray}{Refer to Eq. \eqref{eq-11}}\;
Aggregate these generators using the aggregation weights obtained from the above steps to produce high-quality generators\hfill $\rhd$\textcolor{gray}{Refer to Eq. \eqref{eq-12}}\;
}
\tcc{Contribution-aware Aggregation Rule}
\Do{excute the above module}{
Calculate the local and global contributions of the node via Eq. \eqref{eq-13} and Eq. \eqref{eq-15}\;
Calculate the adaptive weight $\alpha_k$ via Eq. \eqref{eq-19}\;
Perform contribution-aware aggregation via Eq. \eqref{eq-20} to obtain global model $\omega$\;
}
\Return The optimal global model $\omega$.
\caption{Description of the steps of the \emph{\textsf{FedMobile}}.}
\label{alg-1}
\end{algorithm}

\section{Additional Information}
\subsection{Dataset Information}\label{data}
\para{MHAD Dataset.} The MHAD dataset is designed to support research in human action recognition using multiple modalities. It includes data from 12 subjects performing 11 actions such as jumping, walking, running, and more. The dataset captures data from multiple sensors including accelerometers, gyroscopes, and magnetometers, as well as from optical motion capture systems and video cameras. Link: \href{https://paperswithcode.com/dataset/berkeley-mhad}{https://paperswithcode.com/dataset/berkeley-mhad}


\begin{figure*}[!t]
 \centering
 \includegraphics[width=1\linewidth]{gpu.pdf}
  \caption{Numerical result of local GPU usage.}
  \label{fig:img1}
\end{figure*}

\begin{figure*}[!t]
 \centering
 \includegraphics[width=1\linewidth]{time.pdf}
  \caption{Numerical result of local running time.}
  \label{fig:img2}
\end{figure*}
\begin{figure}[!t]
 \centering
 \includegraphics[width=1\linewidth]{bar0.pdf}
  \caption{Distribution of missing modality at different nodes.}
  \label{fig-6}
\end{figure}

\begin{figure}[!t]
 \centering
 \includegraphics[width=1\linewidth]{bar1.pdf}
  \caption{Distribution of missing rate at different nodes.}
  \label{fig-7}
\end{figure}

\para{USC-HAD Dataset.} The USC-HAD dataset is a collection of data gathered for the purpose of recognizing human activities. The dataset includes data from 14 subjects performing 12 different activities such as walking, running, jumping, sitting, standing, and more. The data is captured using wearable sensors that record accelerometer and gyroscope readings. Link: \href{https://sipi.usc.edu/had/}{https://sipi.usc.edu/had/}

\para{ADM Dataset.} The ADM dataset focuses on detecting Alzheimer's disease by analyzing 11 behavioral biomarkers in natural home environments. These biomarkers include activities such as cleaning living areas, taking medications, using mobile phones, writing, sitting, standing, getting in and out of chairs/beds, walking, sleeping, eating, and drinking. The three modal data of depth images, radar, and audio are obtained by sampling from the depth camera, mmWave radar, and microphone at sampling rates of 15 Hz, 20 Hz, 44 Hz, and 100 Hz, respectively. Link: \href{https://github.com/xmouyang/Harmony/blob/main/dataset.md}{https://github.com/xmouyang/Harmony/blob/main/dataset.md}

\para{C-MHAD Dataset.} The C-MHAD dataset extends the concept of the MHAD dataset by providing continuous recordings of human activities. Unlike datasets that capture discrete instances of actions, C-MHAD includes long, continuous streams of activity data, simulating real-world scenarios where actions flow into one another without clear boundaries. This dataset is particularly useful for developing and testing algorithms that need to operate in real-time and handle continuous input, such as those used in surveillance, human-computer interaction, and assistive technologies. Link: \href{https://github.com/HaoranWeiUTD/C-MHAD-PytorchSolution}{https://github.com/HaoranWeiUTD/C-MHAD-PytorchSolution}

\para{FLASH Dataset.} The FLASH dataset is a multimodal dataset designed specifically for multimodal FL in traffic scenarios. It includes 32,923 samples from three modalities, collected in real time from autonomous vehicles equipped with various sensors—GPS, LiDAR, cameras—and roof-mounted Talon AD7200 60GHz millimeter-wave radios. The dataset primarily supports research in autonomous driving and high-band millimeter-wave sector prediction, among other related fields. \href{https://repository.library.northeastern.edu/files/neu:k930bx06g}{https://repository.library.northeastern.edu/files/neu:k930bx06g}



\begin{table}[!t]
    \centering
    \footnotesize
    \caption{Performance results on FLASH dataset with 10 nodes.}
    \label{tab-8}
    \begin{adjustbox}{width=0.45\textwidth}
    \begin{tabular}{c|l|c|c|c|c|c|c}
    \toprule
    \toprule
     \textbf{Dataset}   & \textbf{Method} &\textbf{20\%} & \textbf{40\%} & \textbf{60\%} &\textbf{70\%}& \textbf{80\%}&\textbf{90\%} \\ 
        \midrule
        & FedMM+ZP& 53.5&	53.3&	52.8	&51.1	&50.6&	50.2\\
        & FedMM+RP & 53.1	&53.4&	53.2	&52.7	&52.3	&51.5\\
        & FedProx+ZP & 51.6	&50.9	&50.2	&49.8	&49.7	&49.3\\
        & FedProx+RP & 50.9	&50.3	&49.5	&49.1	&49.2	&48.9\\
    \textbf{FLASH} & FedBN+ZP & 51.2	&50.8&	49.3&	48.4&	48.8	&47.9\\
        & FedBN+RP & 50.8	&50.6	&49.9	&49.6	&49.1	&49.2\\
        & PmcmFL & 54.3	&54.4	&53.7	&53.2	&53	&51.8\\
         &Harmony &  	55.4	&54.6	&52.9	&53	&53.1	&52.8\\
        &AutoFed &  54.5	&	53.7	&	53.4	&	53.5	&	52.6	&	51.1 \\ 
        & \textbf{Ours} &\textbf{57.7}&	\textbf{57.2}&	\textbf{57.3}&	\textbf{56.5}&	\textbf{56.3}	&\textbf{55.9} \\ 
        \bottomrule
        \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{table}

\subsection{Data Partitioning}
\para{IID Setting.} For the IID data setting, we assign the multimodal dataset collected by each mobile sensor to each node. In addition, during training, we keep the type and missing ratio of each node’s missing modality consistent to construct an IID data scenario.


\para{Non-IID Setting.} We define two non-IID data scenarios: (1) scenarios with different distributions of the number of missing modality types and (2) scenarios with varying distributions of modality missing rates, as illustrated in Figs. \ref{fig-6}--\ref{fig-7}. In Scenario 1, we set the modality missing rate at 40\% and randomly omit different numbers of modality types. For Scenario 2, we control the number of missing modality types but dynamically adjust the modality missing rate, ranging from a maximum of 80\% to a minimum of 20\%.

\subsection{Generator Information}\label{mlp}
The generator used in this paper is a multi-layer perceptron (MLP), which performs updates and optimizations in conjunction with local training (see Eqs. \eqref{eq-4}-\eqref{eq-9}). Specifically, the generator architecture comprises two fully connected layers, a batch normalization (BN) layer, and an activation layer. Initially, the first fully connected layer maps the data labels into feature embeddings. This is followed by the BN layer and a non-linear activation layer. Finally, the second fully connected layer serves as the representation layer, converting the feature embeddings into a format suitable for model training. While more complex generators can be used, the MLP is a good choice to minimize overhead.


\subsection{Additional Experiments}
\para{Performance on FLASH Dataset with 10 Nodes.} We evaluate the performance of \textsf{FedMobile} and baselines on the FLASH dataset, where we set the number of nodes to 10. The experimental results are shown in the Table \ref{tab-8}.

\para{Performance on FLASH Dataset with Large Model.}  To further evaluate the performance of \textsf{FedMobile} using a large model, we use a model size of 1144.38 MB (approximately 1 GB) for the experiment. Consistent with the experimental settings in the paper, we set the number of nodes to 10 and $\beta = {20\%, 40\%, 60\%, 70\%, 80\%, 90\%}$. The experimental results are shown in the Table \ref{tab:flash_results}. Second, we further explore \textsf{FedMobile}'s GPU usage and local training time under large models to illustrate \textsf{FedMobile}'s overhead. The experimental results are shown in the Table \ref{tab:model_comparison}.

\begin{table}[h]
    \centering
        \caption{Performance comparison on the FLASH dataset.}
    \begin{tabular}{l l c c c c c c}
        \toprule
        \toprule
        \textbf{Dataset} & \textbf{Method} & \textbf{20\%} & \textbf{40\%} & \textbf{60\%} & \textbf{70\%} & \textbf{80\%} & \textbf{90\%} \\
        \midrule
        FLASH & Ours & 56.7 & 55.8 & 55.6 & 55.7 & 55.8 & 55.4 \\
        \bottomrule
        \bottomrule
    \end{tabular}
    \label{tab:flash_results}
\end{table}

\begin{table}[h]
    \centering
        \caption{Comparison of Small and Large Models}
    \begin{tabular}{lcc}
        \toprule
        \toprule
        & \textbf{Small Model} & \textbf{Large Model} \\
        \midrule
        \textbf{Size} & 1.48 MB & 1144.38 MB \\
        \textbf{Time} & 3.69 s/epoch & 8.1 s/epoch \\
        \textbf{GPU Usage} & 2522.333 MB/batch & 18205.69 MB/batch \\
        \textbf{Layers} & Conv1d: 4; Conv2d: 10 & Conv1d: 8; Conv2d: 77 \\
        \bottomrule
        \bottomrule
    \end{tabular}
    \label{tab:model_comparison}
\end{table}

\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
