 \newpage
\appendix
\section{Omitted proofs}
\label{a-proof}
Here, we provide the formal mathematical details that were omitted in the main paper.
\subsection{Omitted proofs from Section 2}
\begin{proof}[Proof of \Cref{thm:nonexact-uncond}]
  First, observe that $M_t,M_{t,\infty}^1,\cdots,M_{t,\infty}^B$ are i.i.d.\ under $H_{0,t}$.

Hence, $M_t\leq \operatorname{Quantile}(1-\alpha;M_t,M_{t,\infty}^1,\cdots,M_{t,\infty}^B)\iff M_t$ is one among the $\ceil{(1-\alpha)(B+1)}$ smallest of $M_t,M_{t,\infty}^1,\cdots,M_{t,\infty}^B$. So, for any fixed $\alpha\in(0,1)$, we have that $\text{ for all } t,$
\begin{equation}
\label{eq:exch}
  \P_t(M_t\leq \operatorname{Quantile}(1-\alpha;M_t,M_{t,\infty}^1,\cdots,M_{t,\infty}^B))\geq\frac{\ceil{(1-\alpha)(B+1)}}{B+1}\geq 1-\alpha.
\end{equation}
For $L\in\N\cup\{\infty\},t\in\N$, define $C_{t,L}=\operatorname{Quantile}(1-\alpha;M_t,M_{t,L}^1,\cdots,M_{t,L}^B))$.
Note that
\begin{align*}
    \P_T(T\in \{t \geq 1: M_t\leq C_{t,L}\},\tau< T)\leq \P_T[\tau< T]=\mathbb \P_\infty(\tau< T),
\end{align*}
the last equality holds because the event $[\tau< T]$ does not depend $X_i$ for $i\geq T$.

Also, observe that for any $L\in\N\cup\{\infty\}$, it follows from the definitions that $M_{t,L}^j\geq M_{t,\infty}^j$ and hence, $C_{t,L} \geq C_{t,\infty}$, for all $t\in\N$.
\begin{align*}
  1-\alpha&\leq \P_T( M_T\leq C_{T,\infty})\\
  &\leq \P_T( M_T\leq C_{t,L})\\
   &=\P_T(T\in \{t\geq 1: M_t\leq C_{t,L}\})\\
   &=\P_T(T\in \{t\geq 1: M_t\leq C_{t,L}\},\tau\geq T) +\mathbb P(T\in \{t\geq 1: M_t\leq C_{t,L}\},\tau< T)\\
    &\leq \P_T(T\in \mathcal{C})+\mathbb \P_\infty(\tau< T).
\end{align*}
Therefore, $\P_T(T\in \mathcal{C})\geq 1-\alpha-\mathbb \P_\infty(\tau< T)$.

Now, if $\mathcal{A}$ controls PFA at level $\delta$, i.e., $\P_\infty(\tau< \infty)\leq \delta$, we have
$$\P_T(T\in \mathcal{C})\geq 1-\alpha-\mathbb \P_\infty(\tau< T)\geq 1-\alpha-\mathbb \P_\infty(\tau< \infty)\geq 1-\alpha-\delta.$$
\end{proof}
\begin{proof}[Proof of \cref{prop:impossibility}]
    For the sake of contradiction, assume that $\P_T(T\in\mathcal{C}^*)\geq1-\alpha$ for some $\mathcal{C}^*\subseteq \{1,\cdots,\tau\}$. Now we observe that  $$\P_\infty(\tau \geq T)=\P_T(\tau \geq T)\geq \P_T(T\in\mathcal{C}^*)\geq1-\alpha.$$ 
    %However, $\P_\infty(\tau \geq T)\geq1-\alpha$ cannot hold for all $T$ if $\tau$ is a proper random variable, i.e., $\P_\infty(\tau =\infty)=0$. Hence it leads to a contradiction.
    Threfore, $\P_\infty(\tau =\infty)= 1-\P_\infty(\cup_{n=1}^\infty[\tau <n])=1-\lim_{n\to\infty}\P_\infty(\tau<n)=\lim_{n\to\infty}\P_\infty(\tau\geq n)\geq 1-\alpha.$
\end{proof}

\begin{proof}[Proof of \cref{prop:exact-uncond}]
    Note that $\mathcal{C}^*=\{t\in\N: t\leq \tau, M_t\leq C_{t,L}\}\cup \{\tau+1,\tau+2,\cdots\}=\{t\geq 1: M_t\leq C_{t,L}\}$, since we defined $M_t=-\infty\leq C_{t,L}$, for $t>\tau.$
    
    Therefore, we have
    \begin{equation*}
       \P_T(T\in\mathcal{C}^*)=\P_T(T\in\{t\geq 1: M_t\leq C_{t,L}\})=\P_T( M_T\leq C_{t,L})\geq\P_T( M_T\leq C_{T,\infty})\geq1-\alpha,
    \end{equation*} 
where the last inequality follows from \eqref{eq:exch}.
\end{proof}

\begin{proof}[Proof of \Cref{thm:approxcond}]
From \Cref{thm:nonexact-uncond}, we get $\P_T(T\in \mathcal{C})\geq \P_\infty(\tau\geq T)-\alpha.$

Also, $\P_T(\tau\geq T)=\P_\infty(\tau\geq T)$ because of the fact the event $[\tau\geq T]$ does not depend $X_i$ for $i\geq T$.
Therefore, $$\P_T(T\in \mathcal{C}\mid \tau\geq T)=\frac{\P_T(T\in \mathcal{C})}{\P_T(\tau\geq T)}\geq 1-\frac{\alpha}{\P_\infty(\tau\geq T)}.$$ Therefore, if $\mathcal{A}$ controls PFA at level $\delta$, we have $$\P_T(T\in \mathcal{C}\mid \tau\geq T)\geq 1-\frac{\alpha}{\P_\infty(\tau\geq T)}\geq 1-\frac{\alpha}{\P_\infty(\tau=\infty)}\geq1-\frac{\alpha}{1-\gamma}.$$ 
\end{proof}

\subsection{Omitted proofs from Section 3}

\begin{proof}[Proof of \Cref{thm:coverage-comp-post}]
  First, observe that $M_t,\{M_{t,\infty}^j(\theta_1)\}_{j=1}^B$ are i.i.d.\ under $\P_{\theta_0,t,\theta_1}$.
So, $M_t\leq \operatorname{Quantile}(1-\alpha;M_t,\{M_{t,\infty}^j(\theta_1)\}_{j=1}^B)$ $\iff M_t$ is one among the $\ceil{(1-\alpha)(B+1)}$ smallest of $M_t,\{M_{t,\infty}^j(\theta_1)\}_{j=1}^B\}$. Hence, for any fixed $c\in(0,1)$,
\begin{equation}
  \P_{\theta_0,t,\theta_1}(M_t\leq \operatorname{Quantile}(1-c;M_t,\{M_{t,\infty}^j(\theta_1)\}_{j=1}^B))\geq\frac{\ceil{(1-c)(B+1)}}{B+1}\geq 1-c, \text{ for all } t.
\end{equation}
Define, $C_{t,L}^\alpha=\sup_{\theta\in\mathcal{S}^\prime_{\tau-t+1}}\operatorname{Quantile}(1-\alpha;M_t,\{M_{t,L}^j(\theta)\}_{j=1}^B)$.
Note that $\theta_1\in\mathcal{S}^\prime_{\tau-t+1}$ imply that $C_{t,L}^\alpha\geq C_{t,\infty}^\alpha\geq\operatorname{Quantile}(1-\alpha;M_t,\{M_{t,\infty}^j(\theta_1)\}_{j=1}^B\}),$ for all $t\in\N,\alpha\in(0,1)$. 
\begin{enumerate}
    \item[(a)] \textbf{For $\mathcal{C}_1$:}
For all $t$, with  $r_t = \sum_{i=1}^N \mathds{I}(\tau_i\geq 
 t)/N$, since $r_t$ is independent to $\{X_n\}_n$, we have
 \begin{align*}
     \P_{\theta_0,t,\theta_1}(\theta_1\in\mathcal{S}^\prime_{\tau-t+1})&=\mathbb E(\P_{\theta_0,t,\theta_1}(\theta_1\in\operatorname{CS}(X_t,\cdots,X_{\tau};1-\beta r_t)\mid r_t))\\
     &\geq \mathbb E(1-\beta r_t)= 1-\beta\P_{\theta_0,\infty}(\tau\geq t).
 \end{align*}
Then, we have
\begin{align*}
    &\P_{\theta_0,t,\theta_1}( M_t> C_{t,L}^{\alpha r_t})\\
    &\leq \P_{\theta_0,t,\theta_1}( M_t> C_{t,L}^{\alpha r_t}, \theta_1\in\mathcal{S}^\prime_{\tau-t+1})+\P_{\theta_0,t,\theta_1}(\theta_1\notin\mathcal{S}^\prime_{\tau-t+1})\\
    &\leq \P_{\theta_0,t,\theta_1}( M_t> \operatorname{Quantile}(1-\alpha r_{t};M_t,\{M_{t,\infty}^j(\theta_1)\}_{j=1}^B))+\beta \P_{\theta_0,\infty}(\tau\geq t)\\
    &\leq \mathbb E\left(\P_{\theta_0,t,\theta_1}\left( M_t> \operatorname{Quantile}(1-\alpha r_{t};M_t,\{M_{t,\infty}^j(\theta_1)\}_{j=1}^B)\mid r_{t}\right) \right)+\beta \P_{\theta_0,\infty}(\tau\geq t)\\
&\leq \mathbb E(\alpha r_{t})+\beta \P_{\theta_0,\infty}(\tau\geq t)\\
&= (\alpha+\beta) \times \P_{\theta_0,\infty}(\tau\geq t).
\end{align*}

We also have that
\begin{align*}
    \P_{\theta_0,T,\theta_1}(T\in \{t \geq 1: M_t\leq C_{t,L}^{\alpha}\},\tau< T)\leq \P_{\theta_0,T,\theta_1}(\tau< T)=\mathbb \P_{\theta_0,\infty}(\tau< T),
\end{align*}
the last equality holds because the event $[\tau< T]$ does not depend $X_i$ for $i\geq T$. Now,
\begin{align*}
  &\P_{\theta_0,T,\theta_1}( M_T\leq C_{T,L}^{\alpha r_t})\\
   &=\P_{\theta_0,T,\theta_1}(T\in \{t\geq 1: M_t\leq C_{t,L}^{\alpha r_t}\})\\
   &\leq\P_{\theta_0,T,\theta_1}(T\in \{t\geq 1: M_t\leq C_{t,L}^{\alpha r_t}\}\mid\tau\geq T)\P_{\theta_0,T,\theta_1}(\tau\geq T)+ \P_{\theta_0,T,\theta_1}(\tau< T)\\
    &= \P_{\theta_0,T,\theta_1}(T\in \mathcal{C}_1\mid \tau\geq T)\P_{\theta_0,\infty}(\tau\geq T)+1-\P_{\theta_0,\infty}(\tau\geq T).
\end{align*}
Since $\P_{\theta_0,T,\theta_1}( M_T\leq C_{T,L}^{\alpha r_t})\geq1-(\alpha+\beta) \times \P_{\theta_0,\infty}(\tau\geq T)$, we obtain $\P_{\theta_0,T,\theta_1}(T\in \mathcal{C}_1\mid \tau\geq T)\geq 1-\alpha-\beta$ from the above inequality.
\item[(b)] \textbf{For $\mathcal{C}_2$:} Here, we have $r_t=1$.
The proof for $\mathcal{C}_2$ is similar. For fixed $\alpha\in(0,1),$ with $\mathcal{S}^\prime_{\tau-t+1}$ having coverage $1-\beta$ we show that
\begin{equation*}
    P_{\theta_0,t,\theta_1}( M_t> C_{t,L}^{\alpha})\leq\alpha+\beta.
\end{equation*}
Then, we have 
\begin{equation*}
    P_{\theta_0,T,\theta_1}( M_T\leq C_{T,L}^{\alpha})\leq \P_{\theta_0,T,\theta_1}(T\in \mathcal{C}_2\mid \tau\geq T)\P_{\theta_0,\infty}(\tau\geq T)+1-\P_{\theta_0,\infty}(\tau\geq T).
\end{equation*}
If $\mathcal{A}$ controls PFA at level $\delta\in(0,1)$, %$\P_{\theta_0,T,\theta_1}( M_T\leq C_{T,L}^{\alpha})\geq 1-\alpha-\beta$,
we obtain $$\P_{\theta_0,T,\theta_1}(T\in \mathcal{C}_2\mid \tau\geq T)\geq 1-\frac{\alpha+\beta}{\P_{\theta_0,\infty}(\tau\geq T)}\geq 1-\frac{\alpha+\beta}{\P_{\theta_0,\infty}(\tau=\infty)}\geq 1-\frac{\alpha+\beta}{1-\delta}.$$ And similarly, $$\P_{\theta_0,T,\theta_1}(T\in \mathcal{C}_2)\geq \P_{\theta_0,\infty}(\tau\geq T)-(\alpha+\beta)\geq 1-\alpha-\beta-\delta.$$
\end{enumerate}
\end{proof}

\subsection{Omitted proofs from Section 4}


\begin{proof}[Proof of \Cref{thm:coverage-comp}]
  First, observe that $M_t,\{M_{t,\infty}^j(\theta_0,\theta_1)\}_{j=1}^B$ are i.i.d.\ under $\P_{\theta_0,t,\theta_1}$.
  
So, $M_t\leq \operatorname{Quantile}(1-\alpha;M_t,\{M_{t,\infty}^j(\theta_0,\theta_1)\}_{j=1}^B)$ $\iff M_t$ is one among the $\ceil{(1-\alpha)(B+1)}$ smallest of $M_t,\{M_{t,\infty}^j(\theta_0,\theta_1)\}_{j=1}^B\}$.
Hence, for any fixed $c\in(0,1)$,
\begin{equation}
  \P_{\theta_0,t,\theta_1}(M_t\leq \operatorname{Quantile}(1-c;M_t,\{M_{t,\infty}^j(\theta_0,\theta_1)\}_{j=1}^B))\geq\frac{\ceil{(1-c)(B+1)}}{B+1}\geq 1-c, \text{ for all } t.
\end{equation}
Define, $C_{t,L}^\alpha=\operatorname{Quantile}(1-\alpha;M_t,\{\max_{\theta\in \mathcal{S}_{t-1},\theta^\prime\in\mathcal{S}^\prime_{\tau-t+1}}M_{t,L}^j(\theta,\theta^\prime)\}_{j=1}^B)$.
Note that $\theta_0\in \mathcal{S}_{t-1}$ and $\theta_1\in\mathcal{S}^\prime_{\tau-t+1}$ imply that $C_{t,L}^\alpha\geq C_{t,\infty}^\alpha\geq\operatorname{Quantile}(1-\alpha;M_t,\{M_{t,\infty}^j(\theta_0,\theta_1)\}_{j=1}^B\}),$ for any $\alpha\in(0,1).$ 
By \Cref{assmp-1}, we have that $\P_{\theta_0,\infty}(\tau\geq t)\geq \P_{\theta_0^*,\infty}(\tau\geq t)$.

\begin{enumerate}
    \item[(a)] \textbf{For $\mathcal{C}_1$:}
Also, for all $t$, with  $r_t = \sum_{i=1}^N \mathds{I}(\tau_i\geq 
 t)/N$, since $r_t$ is independent to $\{X_n\}_n$, we have
 \begin{align*}
     \P_{\theta_0,t,\theta_1}(\theta_1\in\mathcal{S}^\prime_{\tau-t+1})&=\mathbb E(\P_{\theta_0,t,\theta_1}(\theta_1\in\operatorname{CS}(X_t,\cdots,X_{\tau};1-\beta r_t)\mid r_t))\\
     &\geq \mathbb E(1-\beta r_t)= 1-\beta\P_{\theta_0^*,\infty}(\tau\geq t).
 \end{align*}
 And similarly,
  \begin{align*}
     \P_{\theta_0,t,\theta_1}(\theta_o\in\mathcal{S}_{t-1})&=\mathbb E(\P_{\theta_0,t,\theta_1}(\theta_1\in\operatorname{CI}(X_1,\cdots,X_{t-1};1-\gamma r_t)\mid r_t))\\
     &\geq \mathbb E(1-\gamma r_t)= 1-\gamma\P_{\theta_0^*,\infty}(\tau\geq t).
 \end{align*}
 Then, we have
\begin{align*}
    &\P_{\theta_0,t,\theta_1}( M_t> C_{t,L}^{\alpha r_{t}})\\
    &\leq \P_{\theta_0,t,\theta_1}( M_t> C_{t,L}^{\alpha r_{t}}, \theta_0\in \mathcal{S}_{t-1},\theta_1\in\mathcal{S}^\prime_{\tau-t+1})+\P_{\theta_0,t,\theta_1}(\theta_0\notin \mathcal{S}_{t-1}))+\P_{\theta_0,t,\theta_1}(\theta_1\notin\mathcal{S}^\prime_{\tau-t+1})\\
    &\leq \P_{\theta_0,t,\theta_1}( M_t> \operatorname{Quantile}(1-\alpha r_{t};M_t,\{M_{t,\infty}^j(\theta_0,\theta_1)\}_{j=1}^B))+(\beta+\gamma)\P_{\theta_0^*,\infty}(\tau\geq t)\\
    &\leq \mathbb E\left(\P_{\theta_0,t,\theta_1}\left( M_t> \operatorname{Quantile}(1-\alpha r_{t};M_t,\{M_{t,\infty}^j(\theta_0,\theta_1)\}_{j=1}^B)\mid r_{t}\right) \right)+(\beta+\gamma) \P_{\theta_0,\infty}(\tau\geq t)\\
&\leq \mathbb E(\alpha r_{t})+(\beta + \gamma)\times \P_{\theta_0,\infty}(\tau\geq t)\\
&=\alpha\times \P_{\theta_0^*,\infty}(\tau\geq t)+(\beta+\gamma) \times \P_{\theta_0,\infty}(\tau\geq t)\\
&\leq (\alpha+\beta+\gamma) \times \P_{\theta_0,\infty}(\tau\geq t).
\end{align*}
We also have that
\begin{align*}
    \P_{\theta_0,T,\theta_1}(T\in \{t \geq 1: M_t\leq C_{t,L}^{\alpha r_{t}}\},\tau< T)\leq \P_{\theta_0,T,\theta_1}(\tau< T)=\mathbb \P_{\theta_0,\infty}(\tau< T),
\end{align*}
the last equality holds because the event $[\tau< T]$ does not depend $X_i$ for $i\geq T$.  Now,
\begin{align*}
  &\P_{\theta_0,T,\theta_1}( M_T\leq C_{t,L}^{\alpha r_{t}})\\
   &=\P_{\theta_0,T,\theta_1}(T\in \{t\geq 1: M_t\leq C_{t,L}^{\alpha r_{t}}\})\\
   &=\P_{\theta_0,T,\theta_1}(T\in \{t\geq 1: M_t\leq C_{t,L}^{\alpha r_{t}}\}\mid\tau\geq T)\P_{\theta_0,T,\theta_1}(\tau\geq T) +\P_{\theta_0,T,\theta_1}(\tau< T)\\
    &\leq \P_{\theta_0,T,\theta_1}(T\in \mathcal{C}_1\mid \tau\geq T)\P_{\theta_0,\infty}(\tau\geq T)+1-\P_{\theta_0,\infty}(\tau\geq T).
\end{align*}

Since $\P_{\theta_0,T,\theta_1}( M_T\leq C_{t,L})\geq1-(\alpha+\beta+\gamma) \times \P_{\theta_0,\infty}(\tau\geq T)$, we obtain $\P_{\theta_0,T,\theta_1}(T\in \mathcal{C}_1\mid \tau\geq T)\geq 1-\alpha-\beta-\gamma$ from the above inequality.

\item[(b)]\textbf{For $\mathcal{C}_2$:} Proof is similar to part (a).
Here we have for fixed $r_t=1$ with $\mathcal{S}_{t-1}$ and $\mathcal{S}^\prime_{\tau-t+1}$ having coverages $1-\beta$ and $1-\gamma$ respectively,
\begin{equation*}
    P_{\theta_0,t,\theta_1}( M_t> C_{t,L}^{\alpha})\leq\alpha+\beta+\gamma.
\end{equation*}
Similarly, we have 
\begin{equation*}
    P_{\theta_0,T,\theta_1}( M_T\leq C_{T,L}^{\alpha})\leq \P_{\theta_0,T,\theta_1}(T\in \mathcal{C}_2\mid \tau\geq T)\P_{\theta_0,\infty}(\tau\geq T)+1-\P_{\theta_0,\infty}(\tau\geq T).
\end{equation*}
If $\mathcal{A}$ controls PFA at level $\delta\in(0,1)$, $\P_{\theta_0,T,\theta_1}( M_T\leq C_{T,L}^{\alpha})\geq 1-\alpha-\beta$, we obtain $$\P_{\theta_0,T,\theta_1}(T\in \mathcal{C}_2\mid \tau\geq T)\geq 1-\frac{\alpha+\beta+\gamma}{\P_{\theta_0,\infty}(\tau\geq T)}\geq 1-\frac{\alpha+\beta+\gamma}{\P_{\theta_0,\infty}(\tau=\infty)}\geq 1-\frac{\alpha+\beta+\gamma}{1-\delta}.$$ Similarly, $$\P_{\theta_0,T,\theta_1}(T\in \mathcal{C}_2)\geq \P_{\theta_0,\infty}(\tau\geq T)-(\alpha+\beta+\gamma)\geq 1-\alpha-\beta-\gamma-\delta.$$
\end{enumerate}
\end{proof}


\section{Another concrete example with Poisson distribution}
\label{sec:a-pois}

 \subsection{Known pre-change and composite post-change}
\label{sec:pois-example-comp-post}

Here is $F_0$ is $\text{Pois}(\theta_0)$ and the post-change model is $\mathcal{P}_1=\{\text{Pois}(\theta): \theta\in\Theta_1\}$.

%Now note that the MLE of $\theta_1$ is $\hat \theta_{1,T:\tau}=\bar{X}_{T:\tau}=\theta_1+\bar{\epsilon}_{T:\tau}$. So, for $T\leq \tau, L_{T}^\tau(\hat \theta_{1,T:\tau}; X_1,\cdots, X_)$ 


In this setting, we have the following example of a $1-\beta$ confidence sequence \citep{chowdhury23a} for $\theta_1$
\begin{equation}
\label{eq:conf-seq-pois}
    \mathcal{S}^\prime_{n}=\{\theta: n\theta -\sum_{i=t}^{t+n-1} X_i\log\theta\leq\log\left(\frac{1}{\beta}\right)+\log I (c, c\theta) - 
    \log I (n+c,  \sum_{i=t}^{t+n-1} X_i+c\theta) \}\cap \Theta_1,
\end{equation}
where $c$ is any constant and $I(a, b)=\int_{-\infty}^\infty e^{-ae^x+bx}dx$.


We fix $t\in\{1,\cdots,\tau\}$. Let $\lambda_t=\max\{\theta_0,\sup \mathcal{S}^\prime_{\tau-t+1}\}$. We only need to generate $\{X_n^j\}_{n\in\N}\stackrel{iid}{\sim}\text{Pois}(\lambda_t)$ and $U_{n,1}^j,\cdots,U_{n,X_n^j}^j\stackrel{iid}{\sim}\text{Uniform}(0,1)$, independently for each $n\in\N, j=1,\cdots,B$. For $\theta\in\mathcal{S}^\prime_{\tau-t+1}$, 
\begin{equation}
    Y_n^t(X_n^j,\theta):=\begin{cases}
        \sum_{i=1}^{X_n^j}\mathds{1}(U_{n,i}^j < \theta_0/ \lambda_t) , \text{ if } n<t\\
        \sum_{i=1}^{X_n^j}\mathds{1}(U_{n,i}^j < \theta/ \lambda_t), \text{ otherwise. }\end{cases}
\end{equation}
Using the binomial thinning property of Poisson distribution, it is straightforward to verify that $Y_n^t(X_n^j,\theta)\sim \text{Pois}(\theta_0)$, for $n<t$ and $Y_n^t(X_n^j,\theta)\sim \text{Pois}(\theta)$, for $\theta\in\mathcal{S}^\prime_{\tau-t+1}, n\geq t$.

We consider the likelihood-ratio-based test statistic $M_{t}(X_1,\cdots,X_\tau)$, as defined in \eqref{eq:test-stat-comp-post}, with $\hat \theta_{1,t:n}=\frac{1}{ n-t+1}\sum_{k=t}^{n}X_k$.
Let $\tau^\theta_j$ be the detection time for the sequence $\{
Y_n^t(X_n^j,\theta)\}_n$ and $M_{t,\infty}^j(\theta)=M_t(Y_1^t(X_1^j,\theta),\cdots,Y_{\tau^\theta_j}^t(X_{\tau^\theta_j}^j,\theta))$. Suppose, we can find $t_1^j,t_2^j\in\N$, such that $t_1^j=\inf_{\theta\in \mathcal{S}^\prime_{\tau-t+1}}\tau^\theta_j$ and $t_2^j=\sup_{\theta\in \mathcal{S}^\prime_{\tau-t+1}}\tau^\theta_j$ (these are easy to compute for specific detection algorithms, such as CUSUM or SR type detectors, as we explain in \Cref{max-min-tau-pois}) and they are finite. So, we can choose $L=\infty$. 

%So, $M_t^j(\theta)\leq \max_{t_1^j\leq  t^\prime\leq t_2^j}M_t(Y_1^j(\theta),\cdots,Y_{t^\prime}^j(\theta);  t^\prime)$. Therefore, 
If $t_2^j< t$, we have $V_{t,\infty}^j=0$.
For $t_2^j\geq t, V_{t,\infty}^j$  can be simplified  as below:
\begin{align*}
   V_{t,\infty}^j&=\max_{\substack{ i,t^{\prime}\in\N:1\leq i\leq   t^\prime,\\ \max\{t,t_1^j\}\leq  t^\prime\leq t_2^j}}\max_{\theta\in {S}^{j}_{t,t^\prime}} \frac{L_{i}\left(\frac{\sum_{k=i}^{t^\prime}Y_k^t(X_k^j,\theta)}{  t^\prime-i+1};Y_1^t(X_1^j,\theta),\cdots,Y_{t^\prime}^t(X_{t^\prime}^j,\theta)\right)}{L_t\left(\frac{\sum_{k=t}^{t^\prime}Y_k^t(X_k^j,\theta)}{  t^\prime-t+1};Y_1^t(X_1^j,\theta),\cdots,Y_{t^\prime}^t(X_{t^\prime}^j,\theta)\right)}.
    %\nonumber&=\max_{\max\{t,t_1^j\}\leq  t^\prime\leq t_2^j} \frac{\max_{1\leq i\leq   t^\prime}\sup_{\theta\in \mathcal{S}^\prime_{\tau-t+1}}L_{i}(\bar Y_{i:  t^\prime}^j(\theta);Y_1^t(X_1^j,\theta),\cdots,Y_{t^\prime}^t(X_{t^\prime}^j,\theta))}{L_t(\bar\epsilon_{t:  t^\prime}^j;\epsilon_1^j,\cdots,\epsilon_{t^\prime}^j)}\\
    % &=\max_{\max\{t,t_1^j\}\leq  t^\prime\leq t_2^j} \frac{\max_{1\leq i\leq   t^\prime}L_{i}(\bar Y_{i:  t^\prime}^j(\theta_{t^\prime}^i);Y_1^t(X_1^j,\theta_{t^\prime}^i),\cdots,Y_{t^\prime}^t(X_{t^\prime}^j,\theta_{t^\prime}^i))}{L_t(\bar\epsilon_{t:  t^\prime}^j;\epsilon_1^j,\cdots,\epsilon_{t^\prime}^j)},
\end{align*}
Here ${S}^{j}_{t,t^\prime}=\{\lambda_tU_{n,k}^j: k,n\in\N, k\leq X_n^j, t\leq n\leq t^\prime, \inf\mathcal{S}^\prime_{\tau-t+1}\leq \lambda_tU_{n,i}^j\leq \sup\mathcal{S}^\prime_{\tau-t+1}\} \cup\{\inf\mathcal{S}^\prime_{\tau-t+1}\}\cup\{\sup\mathcal{S}^\prime_{\tau-t+1}\}$
is the set consisting of one candidate element from 
is a finite set with at most $2+\sum_{n=t}^{t^\prime}X_n^j$ many elements.
All the maximum are taken over a finite set; hence, $V_t^j$ can be computed easily. 
 
 \begin{remark}
\label{max-min-tau-pois}     
For weighted CUSUM or SR detectors (defined in \eqref{eq:wcusum},\eqref{eq:wsr}), finding $t_1^j, t_2^j$ is often straightforward. 
For example, consider $\mathcal{P}_1=\{\text{Pois}(\theta):\theta\geq \theta^*\}$, for some $\theta^*\geq \theta_0$.
Observe that for all $\theta\in \Theta_1$, $\prod_{i=j}^\tau\frac{f_{\theta}(X_i)}{f_{\theta_0}(X_i)}=\exp\{(\tau-j+1)\times(\theta-\theta_0)\}(\theta/\theta_0)^{\sum_{i=j}^\tau X_i}$ increases as $X_i$'s increases and hence both $\tau_{\text{wcusum}}$ and $\tau_{\text{wsr}}$ decreases as $X_i$'s increases. 
Therefore, $t_1^j=\inf_{\theta\in \mathcal{S}^\prime_{\tau-t+1}}\tau^\theta_j=\tau^{\sup \mathcal{S}^\prime_{\tau-t+1}}_j$ and $t_2^j=\sup_{\theta\in \mathcal{S}^\prime_{\tau-t+1}}\tau^\theta_j=\tau^{\inf \mathcal{S}^\prime_{\tau-t+1}}_j$. 
Similarly, for $\mathcal{P}_1=\{\text{Pois}(\theta):\theta\leq \theta^*\}$, for some $\theta^*\leq \theta_0$, we have $t_1^j=\tau^{\inf \mathcal{S}^\prime_{\tau-t+1}}_j$ and $t_2^j=\tau^{\sup \mathcal{S}^\prime_{\tau-t+1}}_j$. 
Also, for $\mathcal{P}_1=\{\text{Pois}(\theta):\theta\neq \theta_0\}$, if we employ Gamma weights (it is a conjugate prior, so closed-form expressions for $\tau_{\text{wcusum}}$ and $\tau_{\text{wsr}}$ can be found), one can obtain similar forms of $t_1^j,t_2^j$, depending on the mean of the weight distribution. 
 \end{remark}
 




 \subsection{Composite pre- and post-change}
\label{sec:pois-example-comp}

Here we assume the pre and post-change model to be $\mathcal{P}_i=\{\text{Pois}(\theta): \theta\in\Theta_i\}$, for $i=0,1$ respectively. 

%Now note that the MLE of $\theta_1$ is $\hat \theta_{1,T:\tau}=\bar{X}_{T:\tau}=\theta_1+\bar{\epsilon}_{T:\tau}$. So, for $T\leq \tau, L_{T}^\tau(\hat \theta_{1,T:\tau}; X_1,\cdots, X_)$ 


In this setting, we have the $1-\beta$ confidence sequence for $\theta_1$, as defined in \eqref{eq:conf-seq-pois} and the same method can be employed for constructing $1-\gamma$ confidence interval for  $\theta_0$ as well.


We fix $t\in\{1,\cdots,\tau\}$. Let $\lambda_t=\max\{\sup \mathcal{S}_{t-1},\sup \mathcal{S}^\prime_{\tau-t+1}\}$. We only need to generate $\{X_n^j\}_{n\in\N}\stackrel{iid}{\sim}\text{Pois}(\lambda_t)$ and $U_{n,1}^j,\cdots,U_{n,X_n^j}^j\stackrel{iid}{\sim}\text{Uniform}(0,1)$, independently for each $n\in\N, j=1,\cdots,B$. For $(\theta,\theta^\prime)\in\mathcal{S}_{t-1}\times\mathcal{S}^\prime_{\tau-t+1}$, 
\begin{equation}
    Y_n^j(\theta,\theta^\prime):=\begin{cases}
        \sum_{i=1}^{X_n^j}\mathds{1}(U_{n,i}^j < \theta/ \lambda_t) , \text{ if } n<t\\
        \sum_{i=1}^{X_n^j}\mathds{1}(U_{n,i}^j < \theta^\prime/ \lambda_t), \text{ otherwise. }\end{cases}
\end{equation}
Using the binomial thinning property of Poisson distribution, it is straightforward to verify that $Y_n^j(\theta,\theta^\prime)\sim \text{Pois}(\theta)$, for $n<t$,
$Y_n^j(\theta,\theta^\prime)\sim \text{Pois}(\theta^\prime)$, for $n\geq t$ and $(\theta,\theta^\prime)\in\mathcal{S}_{t-1}\times\mathcal{S}^\prime_{\tau-t+1}$.
We consider the likelihood-ratio-based test statistic $M_t(X_1,\cdots,X_\tau;\tau)$, as defined in \eqref{eq:test-stat-comp-simp}, with $\hat \theta_{0,1:t-1}=\frac{1}{ t-1}\sum_{k=1}^{t-1}X_k$ and $\hat \theta_{1,t:n}=\frac{1}{ n-t+1}\sum_{k=t}^{n}X_k$. For Poisson distribution as well, $t_1^j,t_2^j$ are easy to compute exactly for the same detection algorithms, similar to the Gaussian setting discussed in \Cref{max-min-tau-comp}. 


%So, $M_t^j(\theta)\leq \max_{t_1^j\leq  t^\prime\leq t_2^j}M_t(Y_1^j(\theta),\cdots,Y_{t^\prime}^j(\theta);  t^\prime)$. Therefore, 
If $t_2^j< t$, we have $U_{t,\infty}^j=0$.
For $t_2^j\geq t, U_{t,\infty}^j$  can be simplified  as below:
\begin{align*}
U_{t,\infty}^j&=\max_{\substack{i,t^{\prime}\in\N:1\leq i\leq   t^\prime,\\ \max\{t,t_1^j\}\leq  t^\prime\leq t_2^j}}\max_{\substack{\theta\in {A}^{i,j}_{t,t^\prime}\\\theta^\prime \in {B}^{i,j}_{t,t^\prime}}} \frac{L_{i}\left(\frac{\sum_{k=1}^{i-1}Y_k^j(\theta,\theta^\prime)}{ i-1},\frac{\sum_{k=i}^{t^\prime}Y_k^j(\theta,\theta^\prime)}{  t^\prime-i+1};Y_1^j(\theta,\theta^\prime),\cdots,Y_{t^\prime}^j(\theta,\theta^\prime)\right)}{L_t\left(\frac{\sum_{k=1}^{t-1}Y_k^j(\theta,\theta^\prime)}{ t-1},\frac{\sum_{k=t}^{t^\prime}Y_k^j(\theta,\theta^\prime)}{  t^\prime-t+1};Y_1^j(\theta,\theta^\prime),\cdots,Y_{t^\prime}^j(\theta,\theta^\prime)\right)}.
    %\nonumber&=\max_{\max\{t,t_1^j\}\leq  t^\prime\leq t_2^j} \frac{\max_{1\leq i\leq   t^\prime}\sup_{\theta\in \mathcal{S}^\prime_{\tau-t+1}}L_{i}(\bar Y_{i:  t^\prime}^j(\theta);Y_1^t(X_1^j,\theta),\cdots,Y_{t^\prime}^t(X_{t^\prime}^j,\theta))}{L_t(\bar\epsilon_{t:  t^\prime}^j;\epsilon_1^j,\cdots,\epsilon_{t^\prime}^j)}\\
    % &=\max_{\max\{t,t_1^j\}\leq  t^\prime\leq t_2^j} \frac{\max_{1\leq i\leq   t^\prime}L_{i}(\bar Y_{i:  t^\prime}^j(\theta_{t^\prime}^i);Y_1^t(X_1^j,\theta_{t^\prime}^i),\cdots,Y_{t^\prime}^t(X_{t^\prime}^j,\theta_{t^\prime}^i))}{L_t(\bar\epsilon_{t:  t^\prime}^j;\epsilon_1^j,\cdots,\epsilon_{t^\prime}^j)},
\end{align*}
Here ${A}^{j}_{t,t^\prime}=\{\lambda_tU_{n,k}^j: k,n\in\N,k\leq X_n^j, n\leq t-1, \inf\mathcal{S}_{t-1}\leq \lambda_tU_{n,k}^j\leq \sup\mathcal{S}_{t-1}\} \cup\{\inf\mathcal{S}_{t-1}\}\cup\{\sup\mathcal{S}_{t-1}\}$ and ${B}^{j}_{t,t^\prime}=\{\lambda_tU_{n,k}^j: k,n\in\N,k\leq X_n^j, t\leq n\leq t^\prime, \inf\mathcal{S}^\prime_{\tau-t+1}\leq \lambda_tU_{n,k}^j\leq \sup\mathcal{S}^\prime_{\tau-t+1}\} \cup\{\inf\mathcal{S}^\prime_{\tau-t+1}\}\cup\{\sup\mathcal{S}^\prime_{\tau-t+1}\}$ are finite sets with at most $(2+\sum_{n=1}^{t^\prime}X_n^j)$ many elements.
All the maximum are taken over a finite set; hence, $U_t^j$ can be computed easily. 
 
\subsection{Experimental results}
\subsubsection{Known pre- and post-change (Setting I)}


We consider Poisson the mean-change problem: $\text{Pois}(1)$ (pre-change) vs $\text{Pois}(2)$ (post-change) with the true (unknown) changepoint at $T=100,500$. We employ the CUSUM change detection method defined in \eqref{eq:cusum} with thresholds at $A=1000,10000$. After detection, we employ \Cref{algo:1-exact} to construct confidence sets for $T$.  \Cref{fig:ci-normal,fig:ci-pois} provide visualizations of the confidence sets and point estimates \eqref{eq:known-pre-post} of the changepoints across $5$ runs. 
\begin{figure*}[!ht]
\centering
\centering
\subfloat[Data till change is detected]{\includegraphics[width=0.5\linewidth]{Plots-new-CI/ci-data-pois.png}} 
\subfloat[CUSUM detector (in $\log_{10}$ scale) till change is detected]{\includegraphics[width=0.5\linewidth]{Plots-new-CI/ci-cusum-pois.png}}\\
\subfloat[Data till change is detected]{\includegraphics[width=0.5\linewidth]{Plots-new-CI/ci-data-pois-500.png}} 
\subfloat[CUSUM detector (in $\log_{10}$ scale) till change is detected]{\includegraphics[width=0.5\linewidth]{Plots-new-CI/ci-cusum-pois-500.png}}
\caption[]{Setting I (known pre- and post-change Poisson distributions): The first $T-1$ observations are drawn from Pois$(1)$ and the rest from Pois$(2)$, with $T=100$ (top row) or $T=500$ (bottom row). Change is detected using CUSUM detector \eqref{eq:cusum} with $A=10000$ (top row) or $A=1000$ (bottom row). The point estimate \eqref{eq:known-pre-post} is shown in vertical red dashed line and the confidence set (using \Cref{algo:1-exact}) is shown in red points. $B=N=100$, $\alpha=0.05, L=\infty$. Results of $5$ independent simulations are shown.} 
\label{fig:ci-pois}  
\end{figure*}
 
We report the average size and the conditional and unconditional coverage rates across $100$ independent runs in \Cref{tab:simple-pois}. 
 
  \begin{table}[!ht]
    \centering
    \caption{Setting I: Average (of $100$ independent runs) size  and coverage of the confidence sets using \Cref{algo:1-exact} with $F_0=\text{Pois}(1),F_1=\text{Pois}(2),B=N=100$, $\alpha=0.05$, the average absolute deviation $\hat T$ \eqref{eq:known-pre-post} and average detection delay (given a true detection) of the detection algorithm used \eqref{eq:cusum} with threshold $A$, while varying $T$ and $A$.}
\label{tab:simple-pois}
    \resizebox{0.95\linewidth}{!}{
    \begin{tabular}{cc|cccccc}
    \toprule
    \addlinespace
$T$ & $A$ & \specialcell{Conditional \\ coverage} & \specialcell{Unconditional \\ coverage} & Size & $\mathbb E_T(|\hat{T}-T|| \tau \geq T)$ & $\mathbb E_T(\tau-T | \tau \geq T)$\\
    \midrule
\addlinespace 
%100 & 10000&  $N(0,1)$ & $N(1,1)$  & 0.96  &  0.96  & 12.21 & 2.97 & 17.63\\
%100 &   $N(0,1)$ & $N(1,1)$  &  0.96 &  0.94  & 12.34  & 2.85 & 13.97 \\
100 & 10000 & 0.94   & 0.94   & 18.04 & 4.02 & 22.15\\
100 & 1000&   0.94  & 0.94  & 17.45 & 3.98 & 15.72\\
%500 & 10000&  $N(0,1)$ & $N(1,1)$  & 0.95  & 0.95 & 12.88 & 2.59 & 17.26\\
%500 &  $N(0,1)$ & $N(1,1)$  & 0.95  & 0.90 & 12.57 & 2.62 & 13.22\\
500 & 10000& 0.95   & 0.95 & 18.53  & 4.30 & 22.10 \\
500 & 1000&    0.96  & 0.89 & 18.72  & 3.96 & 16.14\\

\bottomrule
 \end{tabular}}
 \end{table}
 
 For $A=10000$, conditional and unconditional coverages are equal, as the probability of false detection before the true change is negligible. However, for $T=500$ with $A=1000$, the presence of a significant number of false detections before the true change results in lower unconditional coverage. Nevertheless, the empirical conditional coverage rates meet the desired level, supporting our theory.
We also report the average absolute deviation of $\hat T$ (defined in \eqref{eq:known-pre-post}) and delay (given no false detection) in the table.


\subsubsection{Known pre-change and composite post-change (Setting II)}
\label{setting-ii-pois}
We perform experiments for Poisson rate-change scenarios having pre-change distribution as $\text{Pois}(1)$, and the true (unknown) post-change data distribution as $\text{Pois}(2)$, with the changepoint at $T=100,200,$ assuming the composite post-change model to be $\mathcal{P}_1=\{\text{Pois}(\theta):\theta> 1.9\}$. 
For detection, we employ a weighted CUSUM detector \eqref{eq:wcusum} with thresholds at $A=1000,10000$. For weighted CUSUM, we employ a discrete weight distribution taking values
$\theta_1=\theta,\theta_2=\theta+d,\theta_3=\theta+2d,\cdots,\theta_{10}=\theta+9d$, $d=0.2$, and $\theta=1.9$,  with exponentially decaying weights $w_i=e^{-\frac{i-1}{2}}-e^{-\frac{i}{2}},\text{ for } i=1,\cdots,9, \text{ and } w_{10}=e^{-9/2}$, which appeared to us to be a sensible discretized exponential mixture over the alternative class.  After detection, we construct confidence sets (as defined in \eqref{set-gaussian-known-pre}) for $T$. 
%\Cref{fig:ci-comp-post} provides visualizations of the confidence sets  \eqref{set-gaussian-known-pre} and point estimates \eqref{eq:known-pre-unknown-post} of the changepoint across $5$ random runs. 

We report the average size and the conditional and unconditional coverage across $100$ independent runs in \eqref{tab:comp-post}. 
%For $T=500$ with $A=1000$, the presence of a significant number of false detections before the true change results in much lower unconditional coverage. 
The empirical conditional coverage rates are always a bit higher than the theoretical bound $1-\alpha-\beta$ (which is 0.925 in our experiments). It is worth noting that both the lengths and the conditional coverage rates are higher compared to the known pre- and post-change settings (\Cref{tab:simple-pois}). We also report the average absolute deviation of $\hat T$ (defined in \eqref{eq:known-pre-post}) and delay (given no false detection) in the table. We observe that the confidence sets in this setting tend to be a bit more conservative compared to those with known pre- and post-change distributions (Setting 1), exhibiting both higher coverage and larger size.

\begin{table}[!ht]
    \centering
    \caption{Setting II: Average (of $100$ independent runs) size and (conditional and unconditional) coverage rates of confidence sets \eqref{set-gaussian-known-pre} with $B=N=100$ and $\alpha=0.05,\beta=0.025,L=\infty$, the average absolute deviation $\hat T$ \eqref{eq:known-pre-unknown-post} and average detection delay (given no false detection) of the change detection method used \eqref{eq:wcusum} with threshold $A$, having pre- and post-change data-generating distributions as $\text{Pois}(1)$ and $\text{Pois}(2)$ respectively, while we vary the changepoint $T$ and $A$.}
\label{tab:comp-post-pois}
    \resizebox{0.95\linewidth}{!}{
    \begin{tabular}{cc|ccccc}
    \toprule
    \addlinespace
 T  & A  & \specialcell{Conditional\\coverage} & \specialcell{Unconditional\\coverage} & Size & $\mathbb E_T(|\hat{T}-T|| \tau \geq T)$ & $\mathbb E_T(\tau-T | \tau \geq T)$ \\
    \midrule
\addlinespace 
100 & 10000 & 0.97  & 0.97 &  33.13 & 6.11 & 27.68 \\
100 & 1000 & 0.97  & 0.96 &  33.85 & 5.59 & 19.14\\
200 & 10000 & 0.98  & 0.98 & 36.08 & 5.98 & 26.81\\
200 & 1000 & 0.97  & 0.93 & 35.24 & 5.74 & 21.27\\
\bottomrule
 \end{tabular}}
 \end{table}


\subsubsection{Composite pre- and post-change (Setting III)}
\label{setting-iii-pois}
Now we conduct experiments for Gaussian mean-change scenarios with composite pre- and post-change models having the changepoint at $T=100$ and $200$, the true (unknown) pre- and post-change data distributions as $N(0,1)$ and $N(1,1)$ respectively, assuming the composite pre and post-change models to be $\mathcal{P}_0=\{\text{Pois}(\theta):\theta< 0.9\}$ and $\mathcal{P}_1=\{\text{Pois}(\theta):\theta> 1.9\}$ respectively. 
 For change detection, we employ a weighted CUSUM-type detector (defined in \eqref{eq:wcusum-ripr}), where the ``closest'' element from the pre-change (the Reverse Information Projection (RIPr) of the post-change element to the pre-change class) is used as a representative of the pre-change model, and for the post-change parameter, the same discrete weight distribution as in \Cref{setting-ii-pois} is considered. 

After detection, we construct confidence sets (as defined in \eqref{set-eff-comp}) for $T$ (assuming $T\neq 1$). We report the average size, conditional and unconditional coverage rates, average absolute deviation of $\hat T$ (defined in \eqref{eq:known-pre-post}), and delay (given no false detection) across $100$ independent runs in \Cref{tab:comp-pois}.



 \begin{table}[!ht]
    \centering
    \caption{Setting III: Average (of $100$ independent runs) conditional and unconditional coverage and size of confidence sets \eqref{set-eff-comp} with $N=B=100$ and $\alpha=0.05, \beta=\gamma=0.025$,the average absolute deviation $\hat T$ \eqref{eq:unknown-pre-post} and average detection delay (given no false detection) of the change detection method used \eqref{eq:wcusum-ripr} with threshold $A$, having pre- and post-change data-generating distributions as $\text{Pois}(1)$ and $\text{Pois}(2)$ respectively, while we vary the changepoint $T$ and $A$.}
\label{tab:comp-pois}
    \resizebox{0.95\linewidth}{!}{
    \begin{tabular}{cc|ccccc}
    \toprule
    \addlinespace
 T & A &\specialcell{Conditional\\coverage} & \specialcell{Unconditional\\coverage} & Size & $\mathbb E_T(|\hat{T}-T|| \tau \geq T)$ & $\mathbb E_T(\tau-T | \tau \geq T)$ \\
    \midrule
\addlinespace 
100 & 10000 &0.97 & 0.97 & 35.04 & 6.34 & 31.56\\
100 & 1000 & 0.97 & 0.97 & 35.19 & 6.27 & 26.33\\
200 & 10000 &   0.98 & 0.98 & 37.54 & 5.92 & 30.45\\
200 & 1000 &   0.98 & 0.97 & 37.86 & 6.18 & 25.82\\
\bottomrule
 \end{tabular}}
 \end{table}

 However, the empirical conditional coverage rates are always a bit higher than the theoretical bound $1-\alpha-\beta-\gamma$ (which is 0.9 in our experiments). It is worth noting that both the lengths and the conditional coverage rates are higher compared to the known pre- and post-change settings (\Cref{tab:simple-pois}). We observe that the confidence sets in this setting tend to be a bit more conservative compared to those with known pre- and post-change distributions (\Cref{tab:simple-pois}), exhibiting both higher coverage and larger size.

%\Cref{fig:ci-comp} visualizes the confidence sets \eqref{set-eff-comp} and point estimates \eqref{eq:unknown-pre-post} of the changepoint across $5$ random runs. 


\section{Simulations for point estimates}
\label{sec:a-sim}
In this section, we study the empirical performances of our point estimates of the changepoint defined in \Cref{eq:known-pre-post,eq:known-pre-unknown-post,eq:unknown-pre-post}, which are the sequential analogs of the maximum likelihood estimators in the offline setting. Our empirical findings indicate that these estimators perform well in practice, even though we do not have theoretical guarantees on their properties.

\subsection{Known pre- and post-change distributions}
In this experiment, $F_0=N(0,1), F_1=N(1,1)$ and $T=100.$ For detecting the change, we use the usual cumulative sum (CUSUM) and Shiryaev-Roberts (SR) procedures \cite{tartakovsky2014sequential} as defined below:
\begin{equation}
\label{eq:cusum}
    \tau_{\text{cusum}}=\inf\left\{\tau\in\mathbb N:\max_{1\leq j\leq \tau}\prod_{i=j}^\tau\frac{f_1(X_i)}{f_0(X_i)}\geq A\right\}~\text{ and }~
\end{equation}
\begin{equation}
    \tau_{\text{sr}}=\inf\left\{\tau\in\mathbb N:\sum_{j=1}^\tau\prod_{i=j}^\tau\frac{f_1(X_i)}{f_0(X_i)}\geq A\right\},
\end{equation}
where $A$ is some fixed threshold. For our experiments, we set the value of $A$ at $10000$.
Once the change is detected, we obtain $\hat{T}$, as defined in \eqref{eq:known-pre-post}, based on observations up to the detection time. \Cref{fig:Known-pre-post} shows histograms of the values taken by $\hat{T}$ over $10000$ independent experiments.

\begin{figure*}[!ht]

\centering
\centering
\subfloat[Histogram of $\hat{T}$ using CUSUM Detector]{\includegraphics[width=0.49\linewidth,height=0.35\linewidth]{Plots/Known-pre-post/N01-N11-cusum.png}} 
\subfloat[Histogram of $\hat{T}$ using SR Detector]{\includegraphics[width=0.49\linewidth,height=0.35\linewidth]{Plots/Known-pre-post/N01-N11-sr.png}}
\caption[]{Histogram of $\hat{T}$, defined in \eqref{eq:known-pre-post}, for known pre- and post-change over $10000$ independent experiments with $F_0=N(0,1), F_1=N(1,1)$ and $T=100.$} 
\label{fig:Known-pre-post}  
\end{figure*}

\subsection{Known pre-change and composite post-change distributions}
In this experiment, $F_0=N(0,1), \mathcal{P}_1=\{N(\theta,1):\theta>0\}$ and $T=100.$ True post-change distribution is $F_1=N(1,1)$ (unknown).

\begin{comment}
    
For composite post-change, \cite{tartakovsky2014sequential} suggests two variants of CUSUM/SR procedures - one is the weighted variant, which is defined as
\begin{equation}
\label{eq:wcusum-a}
    \tau_{\text{wcusum}}=\inf\left\{\tau\in\mathbb N:\max_{1\leq j\leq \tau}\int_{\theta\in\Theta_1}\prod_{i=j}^\tau\frac{f_{\theta}(X_i)}{f_0(X_i)}dW(\theta)\geq A\right\}~\text{ and }~
\end{equation}
\begin{equation}
    \tau_{\text{wsr}}=\inf\left\{\tau\in\mathbb N:\sum_{j=1}^\tau\int_{\theta\in\Theta_1}\prod_{i=j}^\tau\frac{f_{\theta}(X_i)}{f_0(X_i)}dW(\theta)\geq A\right\}.
\end{equation}

And the second variant, which uses the generalized likelihood ratio (GLR), is defined as
\begin{equation}
\label{eq:cusum-glr}
    \tau_{\text{gcusum}}=\inf\left\{\tau\in\mathbb N:\max_{1\leq j\leq \tau}\sup_{F_1\in\mathcal{P}_1}\prod_{i=j}^\tau\frac{f_1(X_i)}{f_0(X_i)}\geq A\right\}=\inf\left\{\tau\in\mathbb N:\max_{1\leq j\leq \tau}\prod_{i=j}^\tau\frac{\hat f_{1,j:\tau}(X_i)}{f_0(X_i)}\geq A\right\}~\text{ and }~
\end{equation}
\begin{equation}
 \tau_{\text{gcusum}}=\inf\left\{\tau\in\mathbb N:\sum_{1\leq j\leq \tau}\sup_{F_1\in\mathcal{P}_1}\prod_{i=j}^\tau\frac{f_1(X_i)}{f_0(X_i)}\geq A\right\}=\inf\left\{\tau\in\mathbb N:\sum_{1\leq j\leq \tau}\prod_{i=j}^\tau\frac{\hat f_{1,j:\tau}(X_i)}{f_0(X_i)}\geq A\right\}
\end{equation}
where $\hat F_{1,j:n}$ (with density $\hat f_{1,j:n}$) be the MLE of the alternative $F_1\in \mathcal{P}_1$ based on data $X_j,\cdots,X_n$.
\end{comment}
Here, we use the weighted CUSUM/SR detectors defined in \eqref{eq:wcusum} and \eqref{eq:wsr}. We considered a discrete weight distribution taking values
$\theta_1=\theta,\theta_2=\theta+d,\theta_3=\theta+2d,\cdots,\theta_{10}=\theta+9d$ for $\theta=d=0.2$ with exponentially decaying weights $w_i=e^{-\frac{i-1}{2}}-e^{-\frac{i}{2}},\text{ for } i=1,\cdots,9, \text{ and } w_{10}=e^{-9/2}$, which appeared to us to be a sensible discretized exponential mixture over the alternative class. \Cref{fig:unknown-post-known-pre} shows histograms of the values taken by $\hat{T}$ defined in \eqref{eq:known-pre-unknown-post} over $10000$ independent experiments.

\begin{figure*}[!ht]

\centering
\centering
\subfloat[Histogram of $\hat{T}$ (CUSUM Detector)]{\includegraphics[width=0.49\linewidth,height=0.35\linewidth]{Plots/unknown-post-known-pre/N01-cusum.png}} 
\subfloat[Histogram of $\hat{T}$ (SR Detector)]{\includegraphics[width=0.49\linewidth,height=0.35\linewidth]{Plots/unknown-post-known-pre/N01-sr.png}}
\caption[]{Histogram of $\hat{T}$, defined in \eqref{eq:known-pre-unknown-post}, for known pre-change and unknown post-change over $10000$ independent experiments with $F_0=N(0,1), \mathcal{P}_1=\{N(\theta,1):\theta\neq 0\}$ and $T=100.$ True post-change distribution is $F_1=N(1,1)$ (unknown).} 
\label{fig:unknown-post-known-pre}  
\end{figure*}

\subsection{Composite pre- and post-change distributions}

In this experiment, $\mathcal{P}_0=\{N(\theta,1):\theta\leq 0.5\}, \mathcal{P}_1=\{N(\theta,1):\theta> 0.5\}$ and $T=100.$ True (unknown) pre- and post-change distributions are $F_0=N(0,1)$ and $F_1=N(1,1)$ respectively. We use the weighted CUSM/SR-type change detectors defined in \eqref{eq:wcusum-ripr} and \eqref{eq:wsr-ripr}.

\begin{comment}
  Let $\hat P_{0,1:j}$ be the maximum likelihood estimator (MLE) of the null $F_0\in \mathcal{P}_0$ based on data $X_1,\cdots,X_{j}$. Then we could also consider the following generalized likelihood ratio based CUSUM and SR detectors:

\begin{equation}
\label{eq:cusum-glr-mle}
    \tau_{\text{cusum}}=\inf\left\{\tau\in\mathbb N:\max_{1\leq j\leq \tau}\sup_{F_1\in\mathcal{P}_1}\prod_{i=j}^\tau\frac{ p_{1}(X_i)}{\hat p_{0,1:j-1}(X_i)}\geq A\right\}=\inf\left\{\tau\in\mathbb N:\max_{1\leq j\leq \tau}\prod_{i=j}^\tau\frac{\hat p_{1,j:\tau}(X_i)}{\hat p_{0,1:j-1}(X_i)}\geq A\right\}~\text{ and }~
\end{equation}
\begin{equation}
    \tau_{\text{sr}}=\inf\left\{\tau\in\mathbb N:\sum_{1\leq j\leq \tau}\sup_{F_1\in\mathcal{P}_1}\prod_{i=j}^\tau\frac{ p_{1}(X_i)}{ \hat p_{0,1:j-1}(X_i)}\geq A\right\}=\inf\left\{\tau\in\mathbb N:\sum_{1\leq j\leq \tau}\prod_{i=j}^\tau\frac{\hat p_{1,j:\tau}(X_i)}{\hat p_{0,1:j-1}(X_i)}\geq A\right\}.
\end{equation}  
\end{comment}

In this experiment, we considered a discrete weight distribution taking values
$\theta_1=\theta,\theta_2=\theta+d,\theta_3=\theta+2d,\cdots,\theta_{10}=\theta+9d$ for $\theta=0.6,d=0.2$ with exponentially decaying weights $w_i=e^{-\frac{i-1}{2}}-e^{-\frac{i}{2}},\text{ for } i=1,\cdots,9, \text{ and } w_{10}=e^{-9/2}$
\Cref{fig:Unknown-pre-post} shows histograms of the values taken by $\hat{T}$ defined in \eqref{eq:unknown-pre-post} over $10000$ independent experiments. It is crucial to note that the RIPr-based estimator has a higher bias but lower variance.

\begin{figure*}[!ht]

\centering
\centering
\subfloat[Histogram of $\hat{T}$ (CUSUM Detector)]{\includegraphics[width=0.49\linewidth,height=0.35\linewidth]{Plots/Unknown-pre-post/N11-cusum.png}} 
\subfloat[Histogram of $\hat{T}$ (SR Detector)]{\includegraphics[width=0.49\linewidth,height=0.35\linewidth]{Plots/Unknown-pre-post/N11-sr.png}}
\caption[]{Histogram of $\hat{T}$, defined in \eqref{eq:unknown-pre-post}, for unknown pre- and post-change over $10000$ independent experiments with $\mathcal{P}_0=\{N(\theta,1):\theta\leq 0.5\}, \mathcal{P}_1=\{N(\theta,1):\theta> 0.5\}$ and $T=100.$ True (unknown) pre- and post-change distributions are $F_0=N(0,1)$ and $F_1=N(1,1)$ respectively. } 
\label{fig:Unknown-pre-post}  
\end{figure*}



%\subsection{Unknown pre- and post-change distributions}

%\subsubsection{No changepoint in the data sequence}

\section{ A heuristic method for composite pre- or post-change}
\label{sec:heuristic-1}
This section provides an alternative method for constructing confidence intervals for composite pre/post-change distributions. While this approach lacks theoretical guarantees, it performs slightly better in our experiments than our previous approach. Specifically, we propose a heuristic bootstrap-based approach that estimates the unknown distributions from the observed data and subsequently generates simulations from the estimated distributions.

\subsection{Known pre-change and unknown post-change distributions}
Here we assume that the pre-change distribution $F_0$ is known and the composite post-change model is $\mathcal{P}_1$.  Building on the approach from previous sections, we define the null hypothesis that a changepoint occurs at time $t\in\{1,\cdots,\tau\}$ as:
\[
\mathcal H_{0,t}: X_1,\cdots,X_{t-1}\stackrel{i.i.d}{\sim}F_0 \text{ and }X_{t},X_{t+1},\cdots\stackrel{i.i.d}{\sim}F_1, \text{ for some (unknown) }  F_1\in \mathcal{P}_1. 
\]

For testing the null hypothesis, we define some test statistic $M_t$, based on $X_1,\cdots,X_\tau$ (eg., the likelihood-ratio-based test statistic as defined in \eqref{eq:test-stat-comp-post}).
%$\mathcal H_{0,t}:$ the change from $F_0$ to some (unknown) $F_1\in \mathcal{P}_1$ occurred at time $t$. Suppose the change is detected at $\tau$.
\begin{comment}

Let, $L_t^n(F_1)$ be the likelihood of data up to time $n\geq t$ under $H_{0,t}$,
\begin{align}
    L_t^n(F_1)=\prod_{i=1}^{t-1} f_0(X_i)\prod_{i=t}^n f_1(X_i).
\end{align}
We denote $\hat F_{1,j:n}$ as the maximum likelihood estimator (MLE) of the alternative $F_1\in \mathcal{P}_1$ based on data $X_j,\cdots,X_{n}$, i.e., $F_1=\hat F_{1,j:n}$ (with density $f_1=\hat f_{1,j:n}$) maximizes 
$\prod_{i=j}^{n} f_1(X_i)$ over $F_1\in\mathcal{P}_1$. 
Then, the natural extension of the point estimate of $T$ defined in \eqref{eq:known-pre-post} would be the following:
\begin{equation}
\label{eq:known-pre-unknown-post}
    \hat T=\argmax_{1\leq j\leq \tau}\prod_{i=j}^\tau\frac{\hat f_{1,j:\tau}(X_i)}{f_0(X_i)}.
\end{equation}
Similarly, we define the test statistic for testing the null hypothesis $\mathcal H_{0,t}$ replacing the unknown $F_1$ by its estimate:
    \begin{equation}
    \label{eq:test-stat-comp-post}
        M_t=\begin{cases}
           \displaystyle \max_{1\leq i\leq \tau}\frac{L_i^\tau(\hat F_{1,i:\tau})}{L_t^\tau(\hat F_{1,t:\tau})},&\text{if } t\leq\tau \\
            0,&\text{otherwise.} \\
        \end{cases}
    \end{equation}
Note that it can be written as
    \begin{equation}
    \label{eq:test-stat-comp-post-simp}
        M_t=\begin{cases}
            \frac{L_{\hat{T}}^\tau(\hat F_{1,\hat{T}:\tau})}{L_t^\tau(\hat F_{1,t:\tau})},&\text{if } t\leq\tau \\
            0,&\text{otherwise.} \\
        \end{cases}
    \end{equation}
\end{comment}
   Since the pre-change $F_0$ is known, we can generate $N$ many i.i.d. sequences under $F_0$ until a change is detected, recording the detection times as $\tau_1,\cdots,\tau_N$, for some $N\in \N$.
Now, the challenge in conducting a level $\alpha\times\mathbb \P_\infty(\tau\geq t)$ test for each $t$ lies in the composite nature of $\mathcal H_{0,t}$, which prevents us from straightforwardly simulating data to determine a rejection threshold as we did in \Cref{algo:1}. To address this, we employ a simple bootstrap-based approach. The main idea is to first obtain some point estimate of the changepoint (for example, a likelihood ratio based estimate, as defined in \eqref{eq:known-pre-unknown-post}). We denote the estimate as $\hat{T}$. Then we use the observations from $\hat{T}$ to $\tau$ to estimate the post-change distribution $F_1$, denoted $\hat{F}_1$. Then, for some $B\in \N$, we generate $B$ independent bootstrap data streams with known pre-change distribution $F_0$ and estimated post-change distribution $\hat{F}_1$ having changepoint at $t$.

For each sampled stream $j$, we compute the same test statistic $M_t^j$ under the null hypothesis, resulting in statistics $M_t^1, M_t^2, \dots, M_t^B$. We reject $\mathcal H_{0,t}$ if
\begin{equation*}
    M_t>\operatorname{Quantile}(1-\alpha\times\sum_{i=1}^N \mathds{I}(\tau_i\geq 
 t)/N;M_t,M_t^1,\cdots,M_t^B).
\end{equation*}
Our confidence set is the collection of all $t$ before $\tau$, for which we fail to reject $\mathcal H_{0,t}$.
Algorithm \ref{algo:2} contains an overview of the method.

%for $F_0=P_{\theta_0}$ and $\mathcal{P}_1=\{P_\theta: \theta\geq \theta_1\}$, where $\theta_1>\theta_0$. In this algorithm, we approximate the threshold simulating first $t$ data points from $P_{\theta_0}$ and remaining data from $P_{\theta_1}$, until a change is detected. This approach is motivated by the intuition that the type-I error for testing $\mathcal H_{0,t}$ is maximum when the pre-change and post-change distributions are closest, though this does not hold in general.

\begin{algorithm}[h!]
\SetKwData{Left}{left}\SetKwData{This}{this}\SetKwData{Up}{up}
\SetKwFunction{Union}{Union}\SetKwFunction{FindCompress}{FindCompress}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
%\DontPrintSemicolon
\SetAlgoLined
%\begin{algorithmic}[1]
\KwIn{ $\alpha$, $N, B$, change detection algorithm $\mathcal{A}$ and data sequence $X_1,X_2,\cdots$ until change is detected at $\tau$ using $\mathcal{A}$}
\KwOut{A confidence set $\mathcal{C}$ for changepoint $T$}
%Detect change at $\tau$ using $\mathcal{A}$\\
Compute a point estimator $\hat{T}$ for $T$, (eg., as defined in \eqref{eq:known-pre-unknown-post})\\
Obtain an estimate $\hat F_1$ of the (unknown) post-change $F_1\in\mathcal{P}_1$ based on $X_{\hat{T}},\cdots,X_{\tau}$\\
\For{$i=1,\cdots,N$}{
  Simulate $X_1^i,X_{2}^i\cdots\stackrel{iid}{\sim} F_0$, until change is detected at $\tau_i$ using $\mathcal{A}$, or until $\tau$, whichever happens first.\\
}
$\mathcal{C}= \varnothing$\\
\For{$t=1,\cdots,\tau$}{
Compute some test statistics $M_t$ (eg., as defined in \eqref{eq:test-stat-comp-post-simp}) based on $\tau$ and $X_1,\cdots,X_\tau$\\
 \For{$j=1,\cdots,B$}{
  Simulate $X_1^j,\cdots,X_{t-1}^j\stackrel{iid}{\sim} F_{0}; ~X_{t}^j,X_{t+1}^j\cdots\stackrel{iid}{\sim} 
 \hat F_{1}$, until a change is detected at $\tau_j^\prime$ using $\mathcal{A}$\\
Compute the same test statistics $M_t^j$ based on $\tau_j^\prime$ and $X_1^j,\cdots,X_{\tau_j^\prime}^j$\\
}
\If{$M_t\leq\operatorname{Quantile}(1-\alpha\times\sum_{i=1}^N \mathds{I}(\tau_i\geq 
 t)/N;M_t,M_t^1,\cdots,M_t^B)$}{$\mathcal{C}=\mathcal{C}\cup \{t\}$}
}
%\BlankLine
%\end{algorithmic}
\caption{CI for known pre-change $F_0$ and composite post-change $\mathcal{P}_1$}
\label{algo:2}
\end{algorithm}

\subsection{Composite pre- and post-change distributions}
We now consider the general case where both pre and post-change distributions follow some composite models $\mathcal{P}_0$ and $\mathcal{P}_1$ respectively. Following the approach from the previous sections, we define the null hypothesis that a changepoint occurs at time $t\in\{1,\cdots,\tau\}$ as:
\[
\mathcal H_{0,t}: X_1,\cdots,X_{t-1}\stackrel{i.i.d}{\sim}F_0, \text{ for some }  F_0\in \mathcal{P}_0 \text{ and }X_{t},X_{t+1},\cdots\stackrel{i.i.d}{\sim}F_1, \text{ for some }  F_1\in \mathcal{P}_1. 
\]
For testing the null hypothesis, we define some test statistic $M_t$, based on $X_1,\cdots,X_\tau$ (eg., the likelihood-ratio-based test statistic as defined in \eqref{eq:test-stat-comp-simp}).

\begin{comment}
Let $L_t^n(F_0,F_1)$ be the likelihood of data up to time $n\geq t$ under $\mathcal H_{0,t}$. Then,
\begin{align}
    L_t^n(F_0,F_1)=\prod_{i=1}^{t-1} f_0(X_i)\prod_{i=t}^n f_1(X_i).
\end{align}
We define $\hat F_{0,1:j}$ to be the maximum likelihood estimator (MLE) of the null $F_0\in \mathcal{P}_0$ based on data $X_1,\cdots,X_{j}$, i.e., $F_0=\hat F_{0,1:j}$ (with density $f_0=\hat f_{0,1:j}$ ) maximizes 
$\prod_{i=1}^{j} f_0(X_i)$ over $F_0\in\mathcal{P}_0$. Similarly, we define $\hat F_{1,j:n}$ to be the MLE of the alternative $F_1\in \mathcal{P}_1$ based on data $X_j,\cdots,X_n$. The point estimates of $T$ defined in \eqref{eq:known-pre-post} and \eqref{eq:known-pre-unknown-post} can be naturally extended to the following form:
\begin{equation}
\label{eq:unknown-pre-post}
    \hat T=\argmax_{1\leq j\leq \tau}\prod_{i=j}^\tau\frac{\hat f_{1,j:\tau}(X_i)}{\hat f_{0,1:j-1}(X_i)}.
\end{equation}

Now we
 define the test statistic for testing the null hypothesis $\mathcal H_{0,t}$ as follows:
    \begin{equation}
    \label{eq:test-stat-comp}
        M_t=\begin{cases}
            \displaystyle\max_{1\leq i\leq \tau} \frac{L_{i}^\tau(\hat F_{0,1:i-1},\hat F_{1,i:\tau})}{L_t^\tau(\hat F_{0,1:t-1},\hat F_{1,t:\tau})},&\text{if } t\leq\tau \\
0,&\text{otherwise.} \\
        \end{cases}
    \end{equation}
Note that it reduces to
     \begin{equation}
    \label{eq:test-stat-comp-simp}
        M_t=\begin{cases}
             \frac{L_{\hat{T}}^\tau(\hat F_{0,1:\hat{T}-1},\hat F_{1,\hat{T}:\tau})}{L_t^\tau(\hat F_{0,1:t-1},\hat F_{1,t:\tau})},&\text{if } t\leq\tau \\
0,&\text{otherwise.} \\
        \end{cases}
    \end{equation}
\end{comment}
    
As in \Cref{algo:2}, we employ a simple bootstrap-based approach, but here we need to estimate both pre- and post-change distributions. We first obtain some point estimate $\hat{T}$ of the changepoint $T$ (eg., the likelihhod-ratio-based estimate as defined in \eqref{eq:unknown-pre-post}). Then we use the observations from $1$ to $\hat{T}-1$ to estimate the pre-change distribution $F_0$, denoted $\hat{F}_0$ and from $\hat{T}$ to $\tau$ to estimate the post-change distribution $F_1$, denoted $\hat{F}_1$.
For some $N\in \N$, we generate $N$ many i.i.d. sequences under $\hat F_0$ until a change is detected and denote the detection times as $\tau_1,\cdots,\tau_N$. Then, for some $B\in \N$,
we generate $B$ independent bootstrap data streams with the estimated pre- and post-change distributions $\hat{F}_0$ and $\hat{F}_1$ respectively, having changepoint at $t$.
For each sampled stream $j$, we calculate the test statistic $M_t^j$ to obtain $M_t^1, M_t^2, \dots, M_t^B$. We reject $\mathcal H_{0,t}$ if we have
\begin{equation*}
    M_t>\operatorname{Quantile}(1-\alpha\times\sum_{i=1}^N \mathds{I}(\tau_i\geq 
 t)/N;M_t,M_t^1,\cdots,M_t^B).
\end{equation*}

Our confidence set is the collection of all $t$ before $\tau$, for which we fail to reject $\mathcal H_{0,t}$. Algorithm \ref{algo:3} contains an overview of the method.

\begin{algorithm}[h!]
\SetKwData{Left}{left}\SetKwData{This}{this}\SetKwData{Up}{up}
\SetKwFunction{Union}{Union}\SetKwFunction{FindCompress}{FindCompress}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
%\DontPrintSemicolon
\SetAlgoLined
%\begin{algorithmic}[1]
\KwIn{ $\alpha$, $N, B$, change detection algorithm $\mathcal{A}$ and data sequence $X_1,X_2,\cdots$ until change is detected at $\tau$ using $\mathcal{A}$}
\KwOut{A confidence set $\mathcal{C}$ for changepoint $T$}
Compute some point estimator $\hat{T}$ for $T$ (eg., as defined in \eqref{eq:unknown-pre-post})\\
Obtain an estimate $\hat F_0$ of the (unknown) pre-change $F_0\in\mathcal{P}_0$ based on $X_{1},\cdots,X_{\hat{T}-1}$\\
Obtain an estimate $\hat F_1$ of the (unknown) post-change $F_1\in\mathcal{P}_1$ based on $X_{\hat{T}},\cdots,X_{\tau}$\\
\For{$i=1,\cdots,N$}{
  Simulate $X_1^i,X_{2}^i\cdots\stackrel{iid}{\sim} \hat F_0$, until change is detected at $\tau_i$ using $\mathcal{A}$, or until $\tau$, whichever happens first.\\
}
$\mathcal{C}= \varnothing$\\
\For{$t=1,\cdots,\tau$}{
Compute some test statistics $M_t$ (eg., as defined in \eqref{eq:test-stat-comp-simp}) based on $X_1,\cdots,X_\tau$\\
 \For{$j=1,\cdots,B$}{
  Simulate $X_1^j,\cdots,X_{t-1}^j\stackrel{iid}{\sim} \hat F_{0}; ~X_{t}^j,X_{t+1}^j\cdots\stackrel{iid}{\sim} 
 \hat F_{1}$, until a change is detected at $\tau_j$ using $\mathcal{A}$\\
Compute the same test statistics $M_t^j$ based on $X_1^j,\cdots,X_{\tau_j}^j$\\
}
\If{$M_t\leq\operatorname{Quantile}(1-\alpha;M_t,M_t^1,\cdots,M_t^B)$}{$\mathcal{C}=\mathcal{C}\cup \{t\}$}
}
%\BlankLine
%\end{algorithmic}
\caption{CI for unknown pre- and post-change}
\label{algo:3}
\end{algorithm}

\subsection{Experimental Results}
\subsubsection{Known pre-change and composite post-change (Setting II)}
\label{setting2}
We conduct experiments for both Normal and Poisson mean-change scenarios with a composite post-change model having the changepoint at $T=100$. For detection, we employ a weighted CUSUM detector \eqref{eq:wcusum} with a threshold at $A=1000$ and a discrete weight distribution taking values
$\theta_1=\theta,\theta_2=\theta+d,\theta_3=\theta+2d,\cdots,\theta_{10}=\theta+9d$ (for normal distribution, we use $\theta=d=0.2$ and for Poisson distribution $\theta=1.2, d=0.2$) with exponentially decaying weights $w_i=e^{-\frac{i-1}{2}}-e^{-\frac{i}{2}},\text{ for } i=1,\cdots,9, \text{ and } w_{10}=e^{-9/2}$, which appeared to us to be a sensible discretized exponential mixture over the alternative class. After detection, we employ \Cref{algo:2} to construct confidence sets. \Cref{tab:comp-post-h1} summarizes the results, including the average size and the conditional and unconditional coverage across $100$ independent runs. For $T=100$, conditional and unconditional coverages are equal, as the probability of false detection before the true change is negligible. However, for $T=500$ with $A=1000$, the presence of a significant number of false detections before the true change results in lower unconditional coverage. Nevertheless, the conditional coverage is maintained at the desired level, even though we do not have a theoretical guarantee in this case. 

 \begin{table}[!ht]
    \centering
    \caption{Setting II: Average (of $100$ independent runs) conditional and unconditional coverage and size of confidence sets using \Cref{algo:2} with $B=100, N=100$ and $\alpha=0.05$, using weighted CUSUM change detection method \eqref{eq:wcusum} with threshold $A=1000$.}
\label{tab:comp-post-h1}
    \resizebox{0.99\linewidth}{!}{
    \begin{tabular}{cccc|ccc}
    \toprule
    \addlinespace
 T &  Pre-change   & \specialcell{Composite \\post-change}  & \specialcell{True (unknown)\\ post-change }  & \specialcell{Conditional\\coverage} & \specialcell{Unconditional\\coverage} & Size \\
    \midrule
\addlinespace 
100 &  $N(0,1)$ & $\{N(\mu,1):\mu> 0\}$ & $N(1,1)$  & 0.94  & 0.94  &  14.76 \\
100 &  $\text{Pois}(1)$ & $\{\text{Pois}(\lambda):\lambda> 1\}$ & $\text{Pois}(2)$ & 0.95 & 0.95 &  20.95 \\
500 &  $N(0,1)$ & $\{N(\mu,1):\mu> 0\}$ & $N(1,1)$  &  0.95 & 0.91 & 14.06 \\
500 &  $\text{Pois}(1)$ & $\{\text{Pois}(\lambda):\lambda> 1\}$ & $\text{Pois}(2)$ & 0.95 & 0.89 & 20.83  \\
\bottomrule
 \end{tabular}}
 \end{table}

 
\begin{comment}
    

 
Figures \Cref{fig:ci-normal-comp-post-h1,fig:ci-normal-comp-post-500-h1,fig:ci-pois-comp-post-h1,fig:ci-pois-comp-post-500-h1} provide a visualization of the confidence sets and point estimates \eqref{eq:known-pre-unknown-post} of the changepoint across $5$ random runs. 

\begin{figure*}[!ht]
\centering
\centering
\subfloat[Data till change is detected]{\includegraphics[width=0.49\linewidth,height=0.46\linewidth]{Plots-new-CI/ci-data-normal-comp-post.png}} 
\subfloat[CUSUM detector (in $\log_{10}$ scale) till change is detected]{\includegraphics[width=0.49\linewidth,height=0.46\linewidth]{Plots-new-CI/ci-cusum-normal-comp-post.png}}
\caption[]{Setting II: The first $99$ observations are drawn from $N(0,1)$ and the remaining observations from $N(1,1)$.  Change is detected using a weighted CUSUM detector \eqref{eq:wcusum}. The point estimate \eqref{eq:known-pre-unknown-post} is shown in vertical red dashed line and the confidence set is shown in red points, assuming $F_0=N(0,1)$ and $\mathcal{P}_1=\{N(\mu,1):\mu\geq 0\}$. Results of $5$ independent simulations are shown.} 
\label{fig:ci-normal-comp-post-h1}  
\end{figure*}

\begin{figure*}[!ht]
\centering
\centering
\subfloat[Data till change is detected]{\includegraphics[width=0.49\linewidth,height=0.46\linewidth]{Plots-new-CI/ci-data-normal-comp-post-500.png}} 
\subfloat[CUSUM detector (in $\log_{10}$ scale) till change is detected]{\includegraphics[width=0.49\linewidth,height=0.46\linewidth]{Plots-new-CI/ci-cusum-normal-comp-post-500.png}}
\caption[]{Setting II: The first $499$ observations are drawn from $N(0,1)$ and the remaining observations from $N(1,1)$.  Change is detected using a weighted CUSUM detector \eqref{eq:wcusum}. The point estimate \eqref{eq:known-pre-unknown-post} is shown in vertical red dashed line and the confidence set is shown in red points, assuming $F_0=N(0,1)$ and $\mathcal{P}_1=\{N(\mu,1):\mu\geq 0\}$. Results of $5$ independent simulations are shown.} 
\label{fig:ci-normal-comp-post-500-h1}  
\end{figure*}
 
\begin{figure*}[!ht]
\centering
\centering
\subfloat[Data till change is detected]{\includegraphics[width=0.49\linewidth,height=0.46\linewidth]{Plots-new-CI/ci-data-pois-comp-post.png}} 
\subfloat[CUSUM detector (in $\log_{10}$ scale) till change is detected]{\includegraphics[width=0.49\linewidth,height=0.46\linewidth]{Plots-new-CI/ci-cusum-pois-comp-post.png}}
\caption[]{Setting II: The first $99$ observations are drawn from Pois$(1)$ and the remaining observations from Pois$(2)$. Change is detected using a weighted CUSUM detector \eqref{eq:wcusum}. The point estimate \eqref{eq:known-pre-unknown-post} is shown in vertical red dashed line and the confidence set is shown in red points, assuming $F_0=\text{Pois}(1)$ and $\mathcal{P}_1=\{\text{Pois}(\lambda):\lambda\geq 1\}$. Results of $5$ independent simulations are shown.} 
\label{fig:ci-pois-comp-post-h1}  
\end{figure*}


\begin{figure*}[!ht]
\centering
\centering
\subfloat[Data till change is detected]{\includegraphics[width=0.49\linewidth,height=0.46\linewidth]{Plots-new-CI/ci-data-pois-comp-post-500.png}} 
\subfloat[CUSUM detector (in $\log_{10}$ scale) till change is detected]{\includegraphics[width=0.49\linewidth,height=0.46\linewidth]{Plots-new-CI/ci-cusum-pois-comp-post-500.png}}
\caption[]{Setting II: The first $499$ observations are drawn from Pois$(1)$ and the remaining observations from Pois$(2)$. Change is detected using a weighted CUSUM detector \eqref{eq:wcusum}. The point estimate \eqref{eq:known-pre-unknown-post} is shown in vertical red dashed line and the confidence set is shown in red points, assuming $F_0=\text{Pois}(1)$ and $\mathcal{P}_1=\{\text{Pois}(\lambda):\lambda\geq 1\}$. Results of $5$ independent simulations are shown.} 
\label{fig:ci-pois-comp-post-500-h1}  
\end{figure*}
     
\end{comment}


 \subsubsection{Composite pre- and post-change (Setting III)}
 \label{setting3}
Now we conduct experiments for both Normal and Poisson mean-change scenarios with composite pre- and post-change models having the changepoint at $T=100$ and $500$. We use a weighted CUSUM detector with RIPr defined in \eqref{eq:wcusum-ripr} for change detection. The thresholds are set at $A=1000$ for both $T=100$ and $T=500$. We considered the same discrete weight distribution as above, taking values
$\theta_1=\theta,\theta_2=\theta+d,\theta_3=\theta+2d,\cdots,\theta_{10}=\theta+9d$ (for normal distribution, we use $\theta=0.8,d=0.2$ and for Poisson distribution $\theta=1.8, d=0.2$) with exponentially decaying weights $w_i=e^{-\frac{i-1}{2}}-e^{-\frac{i}{2}},\text{ for } i=1,\cdots,9, \text{ and } w_{10}=e^{-9/2}$, which appeared to us to be a sensible discretized exponential mixture over the alternative class. After detection, we employ \Cref{algo:3} to construct confidence sets. We report the average size and the conditional and unconditional coverage across $100$ independent runs in \Cref{tab:comp-h1}. The conditional coverage is maintained at the desired level, even though we do not have a theoretical guarantee in this setting.


 \begin{table}[!ht]
    \centering
    \caption{Setting III: Average (of $100$ independent runs) conditional and unconditional coverage and size of confidence sets using \Cref{algo:3} with $N=B=100$ and $\alpha=0.05$, using weighted CUSUM type change detection method \eqref{eq:wcusum-ripr} with threshold $A=1000$.}
\label{tab:comp-h1}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ccccc|ccc}
    \toprule
    \addlinespace
 T & \specialcell{Composite \\post-change}  & \specialcell{Composite \\post-change}  & \specialcell{True \\ pre-change }  &\specialcell{True\\ post-change }  & \specialcell{Conditional\\coverage} & \specialcell{Unconditional\\coverage} & Size \\
    \midrule
\addlinespace 
%100 & $\{N(\mu,1):\mu\leq 0.5\}$ & $\{N(\mu,1):\mu> 0.5\}$ & $N(0,1)$  & $N(1,1)$  & 0.94  &  18.57 \\
100 &  $\{N(\mu,1):\mu\leq 0.25\}$ & $\{N(\mu,1):\mu> 0.75\}$ & $N(0,1)$  & $N(1,1)$  & 0.95 & 0.95 & 19.52  \\
%100 & $\{\text{Pois}(\lambda):\lambda\leq 1.5\}$ & $\{\text{Pois}(\lambda):\lambda> 1.5\}$ & $\text{Pois}(1)$ &$\text{Pois}(2)$ & 0.94 &  29.52 \\
100 &  $\{\text{Pois}(\lambda):\lambda\leq 1.25\}$ & $\{\text{Pois}(\lambda):\lambda> 1.75\}$ & $\text{Pois}(1)$ &$\text{Pois}(2)$ & 0.94 & 0.94 &  23.32 \\
500 &  $\{N(\mu,1):\mu\leq 0.25\}$ & $\{N(\mu,1):\mu> 0.75\}$ & $N(0,1)$  & $N(1,1)$  &  0.95 & 0.95 & 18.56\\
500 &  $\{\text{Pois}(\lambda):\lambda\leq 1.25\}$ & $\{\text{Pois}(\lambda):\lambda> 1.75\}$ & $\text{Pois}(1)$ & $\text{Pois}(2)$ & 0.95 & 0.95 & 23.04 \\
\bottomrule
 \end{tabular}}
 \end{table}
 

    
\begin{comment}
    

Figures \Cref{fig:ci-normal-comp-h1,fig:ci-normal-comp-500-h1,fig:ci-pois-comp-h1,fig:ci-pois-comp-500-h1} provides a visualization of the confidence sets and point estimates \eqref{eq:unknown-pre-post} of the changepoint across $5$ random runs. 

\begin{figure*}[!ht]
\centering
\centering
\subfloat[Data till change is detected]{\includegraphics[width=0.49\linewidth,height=0.46\linewidth]{Plots-new-CI/ci-data-normal-comp-post.png}} 
\subfloat[CUSUM detector (in $\log_{10}$ scale) till change is detected]{\includegraphics[width=0.49\linewidth,height=0.46\linewidth]{Plots-new-CI/ci-cusum-normal-comp.png}}
\caption[]{Setting III: The first $99$ observations are drawn from $N(0,1)$ and the remaining observations from $N(1,1)$. 
Change is detected using the CUSUM detector defined in \eqref{eq:wcusum-ripr}. The point estimate \eqref{eq:unknown-pre-post} is shown in vertical red dashed line and the confidence set is shown in red points, assuming $\mathcal{P}_0=\{N(\mu,1):\mu\leq 0.25\}$ and $\mathcal{P}_1=\{N(\mu,1):\mu> 0.75\}$. Results of $5$ independent simulations are shown.} 
\label{fig:ci-normal-comp-h1}  
\end{figure*}

\begin{figure*}[!ht]
\centering
\centering
\subfloat[Data till change is detected]{\includegraphics[width=0.49\linewidth,height=0.46\linewidth]{Plots-new-CI/ci-data-normal-comp-post-500.png}} 
\subfloat[CUSUM detector (in $\log_{10}$ scale) till change is detected]{\includegraphics[width=0.49\linewidth,height=0.46\linewidth]{Plots-new-CI/ci-cusum-normal-comp-500.png}}
\caption[]{Setting III: The first $499$ observations are drawn from $N(0,1)$ and the remaining observations from $N(1,1)$. 
Change is detected using the CUSUM detector defined in \eqref{eq:wcusum-ripr}. The point estimate \eqref{eq:unknown-pre-post} is shown in vertical red dashed line and the confidence set is shown in red points, assuming $\mathcal{P}_0=\{N(\mu,1):\mu\leq 0.25\}$ and $\mathcal{P}_1=\{N(\mu,1):\mu> 0.75\}$. Results of $5$ independent simulations are shown.} 
\label{fig:ci-normal-comp-500-h1}  
\end{figure*}

 \begin{figure*}[!ht]
\centering
\centering
\subfloat[Data till change is detected]{\includegraphics[width=0.49\linewidth,height=0.46\linewidth]{Plots-new-CI/ci-data-pois-comp.png}} 
\subfloat[CUSUM detector (in $\log_{10}$ scale) till change is detected]{\includegraphics[width=0.49\linewidth,height=0.46\linewidth]{Plots-new-CI/ci-cusum-pois-comp.png}}
\caption[]{Setting III: The first $99$ observations are drawn from Pois$(1)$ and the remaining observations from Pois$(2)$. 
Change is detected using the CUSUM detector defined in \eqref{eq:wcusum-ripr}. The point estimate \eqref{eq:unknown-pre-post} is shown in vertical red dashed line and the confidence set is shown in red points, assuming $\mathcal{P}_0=\{\text{Pois}(\lambda):\lambda\leq 1.25\}$ and $\mathcal{P}_1=\{\text{Pois}(\lambda):\lambda> 1.75\}$. Results of $5$ independent simulations are shown.} 
\label{fig:ci-pois-comp-h1}  
\end{figure*}

\begin{figure*}[!ht]
\centering
\centering
\subfloat[Data till change is detected]{\includegraphics[width=0.49\linewidth,height=0.46\linewidth]{Plots-new-CI/ci-data-pois-comp-500.png}} 
\subfloat[CUSUM detector (in $\log_{10}$ scale) till change is detected]{\includegraphics[width=0.49\linewidth,height=0.46\linewidth]{Plots-new-CI/ci-cusum-pois-comp-500.png}}
\caption[]{Setting III: The first $499$ observations are drawn from Pois$(1)$ and the remaining observations from Pois$(2)$. 
Change is detected using the CUSUM detector defined in \eqref{eq:wcusum-ripr}. The point estimate \eqref{eq:unknown-pre-post} is shown in vertical red dashed line and the confidence set is shown in red points, assuming $\mathcal{P}_0=\{\text{Pois}(\lambda):\lambda\leq 1.25\}$ and $\mathcal{P}_1=\{\text{Pois}(\lambda):\lambda> 1.75\}$. Results of $5$ independent simulations are shown.} 
\label{fig:ci-pois-comp-500-h1}  
\end{figure*}

 \newpage
\end{comment}
 
\section{Another heuristic method for composite pre- or post-change}
\label{sec:heuristic-2}
In this section, we provide another heuristic method for dealing with composite pre- or post-change distributions. We use our algorithm for known pre- and post-change (\Cref{algo:1-exact}) with $F_0$ and $F_1$ being the ``closest'' pair of distributions in the composite pre and post-change models. For instance, when the pre-change model is given by $\{F_\theta:\theta\leq a\}$
and the post-change model is $\{F_\theta:\theta\geq b\}$, we set $F_0=F_a$ and $F_1=F_b$ in \Cref{algo:1-exact}. The intuition behind this choice is that the least separated pair of distributions might form a ``worst-case'' scenario. Nonetheless, we do not have any theoretical guarantee for coverage for this method.



For the known pre-change and composite post-change setting, we use the same detection algorithm as described in \Cref{setting2} and we report the average size and the conditional and unconditional coverage across $100$ independent runs in \Cref{tab:comp-post-h2}.
Similarly, for the composite pre- and post-change setting, we use the same detection algorithm as described in \Cref{setting3} and we report the average size and the conditional and unconditional coverage across $100$ independent runs in \Cref{tab:comp-post-h2}. From \Cref{tab:comp-post-h2}, we observe that the confidence sets are more conservative when the data-generating distributions are away from those boundaries of the respective composite models.


 \begin{table}[!ht]
    \centering
    \caption{Setting II: Average (of $100$ independent runs) conditional and unconditional coverage and size of confidence sets using \Cref{algo:1-exact} with $B=N=100$ and $\alpha=0.05$, using weighted CUSUM change detection method \eqref{eq:wcusum} with threshold $A=1000$.}
\label{tab:comp-post-h2}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{cccc|ccc}
    \toprule
    \addlinespace
 T &  Pre-change   & \specialcell{Composite \\post-change}  & \specialcell{True (unknown)\\ post-change }  & \specialcell{Conditional\\coverage} & \specialcell{Unconditional\\coverage} & Size \\
    \midrule
\addlinespace 
100 &  $N(0,1)$ & $\{N(\mu,1):\mu> 0.75\}$ & $N(1,1)$  & 0.95   & 0.95  & 23.16  \\
100 &  $\text{Pois}(1)$ & $\{\text{Pois}(\lambda):\lambda> 1.75\}$ & $\text{Pois}(2)$ & 0.94 & 0.94  & 25.83 \\
500 &  $N(0,1)$ & $\{N(\mu,1):\mu> 0.75\}$ & $N(1,1)$  & 0.95  & 0.88 & 23.82 \\
500 &  $\text{Pois}(1)$ & $\{\text{Pois}(\lambda):\lambda> 1.75\}$ & $\text{Pois}(2)$ & 0.95 & 0.90 & 27.17  \\
\bottomrule
 \end{tabular}}
 \end{table}
 

\begin{table}[!ht]
    \centering
    \caption{Setting III: Average (of $100$ independent runs) conditional and unconditional coverage and size of confidence sets using \Cref{algo:1-exact} with $N=B=100$ and $\alpha=0.05$, using weighted CUSUM type change detection method \eqref{eq:wcusum-ripr} with threshold $A=1000$.}
\label{tab:comp-h2}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ccccc|ccc}
    \toprule
    \addlinespace
 T &  \specialcell{Composite \\post-change}  & \specialcell{Composite \\post-change}  & \specialcell{True \\ pre-change }  &\specialcell{True\\ post-change }  & \specialcell{Conditional\\coverage} & \specialcell{Unconditional\\coverage} & Size \\
    \midrule
\addlinespace 
%100 & $\{N(\mu,1):\mu\leq 0.5\}$ & $\{N(\mu,1):\mu> 0.5\}$ & $N(0,1)$  & $N(1,1)$  & 0.94  &  18.57 \\
100 & $\{N(\mu,1):\mu\leq 0.25\}$ & $\{N(\mu,1):\mu> 0.75\}$ & $N(0,1)$  & $N(1,1)$  & 0.95 & 0.95 &  26.03 \\
%100 & $\{\text{Pois}(\lambda):\lambda\leq 1.5\}$ & $\{\text{Pois}(\lambda):\lambda> 1.5\}$ & $\text{Pois}(1)$ &$\text{Pois}(2)$ & 0.94 &  29.52 \\
100 &  $\{\text{Pois}(\lambda):\lambda\leq 1.25\}$ & $\{\text{Pois}(\lambda):\lambda> 1.75\}$ & $\text{Pois}(1)$ &$\text{Pois}(2)$ & 0.96 & 0.96 & 29.64 \\
500 &  $\{N(\mu,1):\mu\leq 0.25\}$ & $\{N(\mu,1):\mu> 0.75\}$ & $N(0,1)$  & $N(1,1)$  & 0.95  & 0.95 & 26.52 \\
500 &  $\{\text{Pois}(\lambda):\lambda\leq 1.25\}$ & $\{\text{Pois}(\lambda):\lambda> 1.75\}$ & $\text{Pois}(1)$ & $\text{Pois}(2)$ & 0.95 & 0.95 & 30.49 \\
\bottomrule
 \end{tabular}}
 \end{table}

While we do not have any theoretical guarantees for this method, it is computationally more efficient than the previous approaches with theoretical guarantees, as demonstrated in \Cref{tab:time}.

\subsection*{Summary of comparisons of heuristic and non-heuristic methods}
\label{sec:comparison}
We first compare the execution times of the three types of algorithms and present the results in \Cref{tab:time} for the Gaussian mean shift setting. \Cref{algo:comp-post} and \Cref{algo:comp} are collectively referred to as Proposed Method, which comes with theoretical guarantees but requires more computational time compared to the heuristic approaches. The first heuristic approach, described in \Cref{algo:2} and \Cref{algo:3}, is referred to as Heuristic 1. Finally, the heuristic introduced in this section (which is \Cref{algo:1-exact}, with $F_0$ and $F_1$ being the ``closest'' pair of distributions in the composite pre- and post-change models) is labeled as Heuristic 2.

\begin{table}[!ht]
    \centering
    \caption{Average (of $10$ independent runs) execution times (in seconds) taken by different methods, with $N=B=100$, $\alpha=0.05$, and for the proposed method, $\beta=\gamma=0.05$, having pre- and post-change data-generating distributions as $N(0,1)$ and $N(1,1)$ respectively.}
\label{tab:time}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{cccc|ccc}
    \toprule
    \addlinespace
 T & A & \specialcell{Pre-change model}  & \specialcell{Post-change model}  &  Proposed Method & Heuristic 1 & Heuristic 2 \\
    \midrule
\addlinespace 
100 & 1000 & $N(0,1)$ & $\{N(\mu,1):\mu> 0.9\}$   & 41.25 & 10.43  & 9.72  \\
100 & 10000 & $N(0,1)$ & $\{N(\mu,1):\mu> 0.9\}$   & 43.14 & 11.38  & 10.97  \\
100 & 1000 & $\{N(\mu,1):\mu\leq 0.1\}$ & $\{N(\mu,1):\mu> 0.9\}$   & 66.13 & 11.49 &  11.76 \\
100 & 10000 & $\{N(\mu,1):\mu\leq 0.1\}$ & $\{N(\mu,1):\mu> 0.9\}$   & 69.47 & 11.86 &  11.52 \\
500 & 1000 & $N(0,1)$ & $\{N(\mu,1):\mu> 0.9\}$   &  364.18 & 143.98 &  148.08 \\
500 & 10000 & $N(0,1)$ & $\{N(\mu,1):\mu> 0.9\}$   &  380.46 & 155.89 &  144.75 \\
500 & 1000 & $\{N(\mu,1):\mu\leq 0.1\}$ & $\{N(\mu,1):\mu> 0.9\}$   & 535.82  & 171.13 & 167.46 \\
500 & 10000 & $\{N(\mu,1):\mu\leq 0.1\}$ & $\{N(\mu,1):\mu> 0.9\}$   & 561.74  & 177.28 & 169.41 \\
\bottomrule
 \end{tabular}}
 \end{table}

 From \Cref{tab:time}, we observe that the computational times are lower for the heuristic algorithms. Moreover, comparing the corresponding tables, we observe that the theoretically valid method (Proposed Method) gives higher coverage, also higher size of the confidence set, as compared to both heuristic methods. This suggests that while the heuristic approaches offer practical benefits in terms of speed, they may lead to less conservative inference, despite not having theoretical guarantees. This points at a natural direction for future work: to establish guarantees for our heuristic algorithms, or demonstrate settings where they empirically do not have coverage.




