

\section{Introduction}

We consider the following problem of sequential change analysis in a general space $\mathcal{X}$: a sequence of $\mathcal{X}$-valued observations 
 $X_1,X_2,\cdots$ arrives sequentially, such that for some unknown time $T\in\N\cup\{\infty\}$ (the changepoint),
\begin{equation}\label{eq:data-setup}
X_1,X_2,\cdots,X_{T-1}\stackrel{i.i.d}{\sim}F_0 ~\text{ and }~X_{T},X_{T+1},\cdots\stackrel{i.i.d}{\sim}F_1,
\end{equation}
for some pre-change distribution $F_0\in\mathcal{P}$ and some post-change distribution $F_1\in\mathcal{P}$, where $\mathcal{P}$ denotes a class of probability distributions on  $\mathcal{X}.$
We start by assuming $F_0$ and $F_1$ are known, and generalize our methods to encompass settings where $F_0$ and $F_1$ are only known to lie in some classes, but are not known exactly. A sequential changepoint detection algorithm $\mathcal A$ is simply a data-dependent stopping rule $\tau$ to raise an alarm that a change has previously occurred. 
% Given an online change detection algorithm $\mathcal A$ that stops at some data-dependent time $\tau$ to declare a change, we focus our attention on post-detection inference for the change location $T$.
Our aim in this paper is to construct confidence sets for the unknown changepoint $T$ after stopping at $\tau$, assuming only black-box access to $\mathcal A$ (and ideally no further assumptions or restrictions). In other words, we hope to obtain a set $\mathcal C \subseteq \{1,\dots,\tau\}$ such that
\[
\mathbb P_T (T \in \mathcal C) \geq 1-\alpha,
\]
where $\mathbb P_T$ is the data distribution in~\eqref{eq:data-setup}, i.e.\ having the true changepoint at $T$.
We will show for algorithms $\mathcal A$ with a bounded average run length (ARL), no set $\mathcal C$ can satisfy such a guarantee, but if $\mathcal A$  has probability of false alarm (PFA) bounded above by $\alpha$, such a guarantee is achievable. In either case, another natural criterion for $\mathcal C$ is for it to satisfy
\[
\mathbb P_T (T \in \mathcal C | \tau \geq T) \geq 1-\alpha,
\]
meaning that $\mathcal C$ only needs to cover if algorithm $\mathcal A$ only stops after the true changepoint $T$. We show how to construct such sets $\mathcal C$ for both types of algorithms (bounded ARL or PFA).

Sequential (also known as the online or quickest) changepoint detection is a well-studied field, where the goal is to detect that a change has occurred as soon as possible, is crucial in applications requiring timely responses to distributional shifts, such as quality control, cybersecurity, medical diagnostics, and so on. 
The field traces back to early work on statistical process control. Shewhart \cite{shewhart1925application} introduced the concept of a control chart, which computes a test statistic for each sample, with values within a threshold indicating the process remains in control. Page \cite{page1954continuous} proposed the CUSUM (cumulative sum) procedure, which computes the cumulative log-likelihood ratio under the assumption of two exactly known distributions. Another influential approach is the Shiryaev-Roberts (SR) procedure \citep{shiryaev1963optimum,roberts1966comparison}. These early methods, however, assume the pre- and post-change distributions to be fully specified.
To relax the assumption of known post-change distributions, the Generalized Likelihood Ratio (GLR) procedure was introduced \citep{lorden1971procedures,siegmund1995using}. Tartakovsky et al. \cite{tartakovsky2014sequential} provide comprehensive overviews of these classical methods, including CUSUM, SR, and GLR, and their numerous variants. Several nonparametric change-point detection algorithms have been developed, such as kernel-based methods \citep{harchaoui2008kernel,li2015m} and
e-detectors \citep{shin2022detectors}.


Localizing the changepoint is another fundamental and crucial problem.
As far as we are aware, the existing approaches to changepoint localization focus on offline or batch settings, where a fixed amount of data is available at once, and they do not generalize to analysis at data-dependent stopping times.
%(see, e.g., \cite{hinkley1970inferencce,dumbgen1991asymptotic,verzelen2023optimal} for point estimates and  \cite{worsley1986confidence,jang2024fast} for confidence sets for the changepoint).
Hinkley \cite{hinkley1970inferencce} shows some asymptotic results about the maximum likelihood estimator (MLE) of the changepoint and also provides a confidence set based on likelihood ratio statistics. Yao \cite{yao1987approximating} provides an approximation of the distribution of the MLE of the changepoint. 
Darkhovskh \cite{darkhovskh1976a} presents a nonparametric changepoint estimator based on Mann-Whitney statistics. 
Carlstein \cite{carlstein1988nonparametric} proposes a class of strongly consistent estimators for the changepoint in the nonparametric setting. Worsley \cite{worsley1986confidence} develops likelihood ratio-based confidence regions and tests for a changepoint for exponential family random variables.
DÃ¼mbgen \cite{dumbgen1991asymptotic} constructs nonparametric confidence regions for changepoint by testing for each candidate time-point $t$ at level $\alpha$ using bootstrapping, achieving asymptotic coverage of $1-\alpha$, as the sample size $n\to\infty$. 
A more recent method
\cite{jang2024fast} constructs confidence sets by evaluating a local test statistic on a triplet of time points
and combining the results
of the local tests with a weighted Bonferroni correction. To re-emphasize, all these methods are for the offline setting, while our main focus is on the sequential problem of constructing confidence sets after stopping, which has been largely overlooked in the literature.
% Although the problem is extensively studied in offline or batch settings, the sequential localization of the changepoint following detection has been largely overlooked in the literature. 
This gap is significant because in most real-world applications, data arrives sequentially, and making timely inferences about the changepoint immediately upon detecting a change is essential. Thus, developing methodologies for post-detection inference in a sequential framework is a significant step forward in changepoint analysis.

The key idea behind constructing the confidence interval is simple: for each candidate $t\in\{1,\cdots,\tau\}$, we will test whether $t$ is the true changepoint and form the confidence set by inverting these tests. 
Our approach is conceptually similar to \cite{dumbgen1991asymptotic}, it adapts those ideas to the sequential setting in a nontrivial manner. For example, we show that for change detection algorithms that have finite average run length (ARL), it is impossible to obtain finite length confidence intervals with marginal coverage guarantees. Further, for $1-\alpha$ conditional coverage, we show that we need to test the $t$-th hypothesis at level $\alpha\times \P_\infty(\tau\geq t)$ (a quantity we do not know apriori), whereas testing for each $t$ at level $\alpha$ works in the offline setting. We develop simulation based approaches to get around these hurdles.

%We first develop a hypothesis test for false alarms. Then assuming no false alarm we construct a confidence set of the changepoint $T$ having access to data up to time $\tau$ only.
There are two settings for the changepoint problem: non-partitioned and partitioned. The non-partitioned changepoint problem assumes that the data-generating distribution changes
from one unknown distribution $F_0\in\mathcal{P}$ to another unknown distribution $F_1\in\mathcal{P}$, without requiring any pre-specified partitioning of $\mathcal{P}$ into
pre- and post-change distribution classes (eg: detecting a change in a Gaussian mean from any value to any other value). In contrast, the partitioned setting assumes that we know some partition of $\mathcal{P}$ into $\mathcal{P}_0$ and $\mathcal{P}_1$ such
that $F_0$ and $F_1$ are known to lie in $\mathcal{P}_0$ and $\mathcal{P}_1$ respectively.  This formulation is useful when changes within
$\mathcal{P}_0$ are not of interest, and we only seek to detect shifts from $\mathcal{P}_0$ to $\mathcal{P}_1$ (eg: detecting a change in Gaussian mean from negative to positive). Our method is designed for the partitioned setting. Extensions to the non-partitioned case are left for future work.



\subsection{Our contributions}
 Here we summarize the key contributions of the paper.
 \begin{itemize}
     \item For detection algorithms that control PFA, we propose \Cref{algo:1} for constructing a confidence set for the changepoint and establish its unconditional and conditional coverage (\Cref{thm:nonexact-uncond}, \Cref{thm:approxcond}), assuming known pre- and post-change distributions.
     \item For detection algorithms with bounded ARL, we show in \Cref{prop:impossibility} that we cannot have an unconditional coverage guarantee and propose  \Cref{algo:1-exact}, which is a modification of \Cref{algo:1}. We provide its conditional coverage guarantee in \Cref{thm:exact-cond-coverage}. 
     \item  We extend our framework to composite pre- and post-change classes in \Cref{algo:comp-post} and \Cref{algo:comp} and provide the coverage guarantees in \Cref{thm:coverage-comp-post} and \Cref{thm:coverage-comp}. 
     \item We demonstrate that the approach is not overly conservative (and thus practical) by examining the achieved coverage and size of constructed confidence sets in extensive simulations.
     % \item 
 \end{itemize}

We point out again that for obtaining the aforementioned guarantees, we only need to have black-box access to the detection algorithm $\mathcal A$. It need not have formal guarantees, we do not need to know its inner workings, and just need to be able to run $\mathcal A$ on any stream of data.

 
%\section{Unknown pre- and post-change distributions}



\subsection{Paper outline}
% The rest of the paper is organized as follows. 
In \cref{sec:simple}, we construct our confidence sets for the changepoint for known pre- and post-change and study its unconditional and conditional coverage probabilities. \Cref{sec:composite-post} extend this approach to handle composite post-change, assuming the pre-change distribution to be known. In \Cref{sec:composite}, we finally extend this approach to handle composite pre- and post-change setting. \Cref{sec:expt} and \Cref{sec:expt-pfa} present a comprehensive set of simulation studies that validate our theoretical findings and demonstrate the practical efficacy of our approach. This
article is concluded in \cref{sec:conc}. Omitted proofs can be found in \Cref{a-proof}. An additional illustrative example using the Poisson distribution is given in \Cref{sec:a-pois}. \Cref{sec:a-pois}. Some additional experimental results
are provided in \cref{sec:a-sim} and some heuristic algorithms are presented in \Cref{sec:heuristic-1,sec:heuristic-2}, which are computationally more efficient. A summary of comparisons of the heuristic and non-heuristic methods is provided in \Cref{sec:comparison}, pointing at open avenues for further theoretical work.




\section{Known pre- and post-change distributions} 
\label{sec:simple}

We now address the question of constructing confidence sets for the changepoint, assuming that our change detection procedure stopped after a changepoint. We start from the simplest case when both pre- and post-change distributions are known, addressing more general cases in later sections. 
%Then, we address the complex case where we only know certain classes in which the pre- and post-change distributions lie in \Cref{sec:composite}, showing that a natural extension of the algorithm has strong empirical performance in \Cref{sec:expt}.

\textbf{Notation.}
For $t \in \N$, let $\P_t$ denote the joint distribution of the sequence of independent random variables $X_1,X_2, \dots$ when there is a changepoint at time t, meaning that $X_1,\dots,X_{t-1}$ are from $F_0$ (with density $f_0$) and $X_t,X_{t+1},\dots$ are from $F_1$ (with density $f_1$). As corner cases, let $\P_1$ denotes the joint distribution where $X_1,X_2,\dots\stackrel{i.i.d}{\sim} F_1$ and $\P_\infty$ denotes the case where $X_1,X_2,\dots \stackrel{i.i.d}{\sim}F_0$. Let $\mathbb E_t$ denote the expectation under $\P_t$, for $t \in \N \cup \{\infty\}$.
Suppose we have a data sequence $\{X_n\}\sim\P_T$ for some unknown $T \in \N \cup \{\infty\}$ and some sequential change detection algorithm $\mathcal{A}$ that raises an alarm at the stopping time $\tau$, which is finite for the observed sequence.

In sequential change detection, there are typically two types of Type I error metrics.
One is the average run length (ARL), which is defined as $\mathbb E_{\infty}[\tau]$, the expected time until a false alarm occurs under $T=\infty$ (i.e., no change). The other one is the probability of false alarm (PFA), that is, $\P_{\infty} (\tau < \infty)$. We  provide solutions for  algorithms $\mathcal A$ that control either metric. 


\subsection{A first confidence set}
\label{sec:known-pfa}
%Suppose that the data distribution changes from $F_0$ to $F_1$ at some unknown time $T$. 
We will test whether there is a changepoint at $t$, for each $t\in\{1,\cdots,\tau\}$ and then construct the confidence set consisting of all time points $t$ for which we fail to reject. Formally, we test the following null hypothesis at each candidate time $t$:
\[
H_{0,t}: X_1,\cdots,X_{t-1}\stackrel{i.i.d}{\sim}F_0 \text{ and } X_{t},X_{t+1},\cdots\stackrel{i.i.d}{\sim}F_1. 
\]
Consider some test statistic $M_t$ for testing the null hypothesis $H_{0,t}$ such that for each $t\in\N$,
\begin{equation}
    M_t:=\begin{cases}
        M_t^{(\tau)}(X_1,\cdots,X_\tau), &\text{if }t\leq\tau<\infty,\\
        -\infty &\text{if }t>\tau \text{ or }\tau=\infty,
    \end{cases}
\end{equation}
where $M_t^{(n)}$ is some test statistic based on $n$ many samples, for each $t,n\in\N,t\leq n$.
While our theoretical guarantees will not depend on any particular choice of $M_t$, we provide a prototypical choice in a later subsection that we use for our experiments.
    
    To test $H_{0,t}$, we draw $B$ many independent streams of data under $H_{0,t}$. For the $j$th sequence, denoted as $\{X_n^j\}_n$, we simulate data points until a change is detected at the stopping time $\tau_j^\prime$ using $\mathcal{A}$ or until time $L$, whichever occurs first. Then, we compute the same test statistics truncated at $L$ (which we discuss in detail in the next subsection), based on $\tau_j^\prime,L$ and $X_1^j,\cdots,X_{\tau_j^\prime \wedge L}^j$, which is denoted as $M^j_{t,L}$. We reject $H_{0,t}$ if $ M_t > \operatorname{Quantile}(1-\alpha;M_t,M_{t,L}^1,\cdots,M_{t,L}^B)$, and the confidence set is the collection of all $t\in\{1,\cdots,\tau\}$, for which we fail to reject $H_{0,t}$, i.e,
\begin{equation}
\mathcal{C}=\Big\{t \in \N: t\leq \tau, M_t \leq \operatorname{Quantile}(1-\alpha;M_t,M_{t,L}^1,\cdots,M_{t,L}^B)\Big\}.
\end{equation}
Algorithm \ref{algo:1} provides an overview of the method. 


\begin{algorithm}
\SetKwData{Left}{left}\SetKwData{This}{this}\SetKwData{Up}{up}
\SetKwFunction{Union}{Union}\SetKwFunction{FindCompress}{FindCompress}
%\DontPrintSemicolon
\SetAlgoLined
%\begin{algorithmic}[1]
\KwIn{$\alpha\in(0,1)$, $B \in \N$, $L\in\N \cup \{\infty\}$,  algorithm $\mathcal{A}$ and data $\{X_n\}_{n}$ until a change is detected at $\tau$ using $\mathcal{A}$}
\KwOut{Confidence set $\mathcal{C}$ for the changepoint $T$}
Detect change at $\tau$ using $\mathcal{A}$; Set $\mathcal{C}= \varnothing$\\
\For{$t=1,\cdots,\tau$}{
 Compute some test statistic $M_t$ (e.g., as defined in \eqref{eq:test-stat-simp}) based on $X_1,\cdots,X_\tau$\\
 \For{$j=1,\cdots,B$}{
  Simulate $X_1^j,\cdots,X_{t-1}^j\stackrel{iid}{\sim} F_0; ~X_{t}^j,X_{t+2}^j\cdots\stackrel{iid}{\sim} F_1$, until a change is detected at $\tau_j^\prime$ using $\mathcal{A}$, or until time $L$, whichever happens first.\\
\underline{Subroutine $\mathcal L$:} Compute $M_{t,L}^j$ (defined in \eqref{eq:Mt-L},\eqref{eq:Mt-infty}) based on $\tau_j^\prime$, $L$ and $X_1^j,\cdots,X_{\tau_j^\prime \wedge L}^j$\\
}
\If{$M_t\leq\operatorname{Quantile}(1-\alpha;M_t,M_{t,L}^1,\cdots,M_{t,L}^B)$}{$\mathcal{C}=\mathcal{C}\cup \{t\}$}
}
%\BlankLine
%\end{algorithmic}
\caption{CI for known pre-change $F_0$ and post-change $F_1$}
\label{algo:1}
\end{algorithm}



% We now provide more details about Subroutine~$\mathcal L$. 

\subsection{Subtleties regarding Subroutine \texorpdfstring{$\mathcal L$}{Lg}}
\label{sec:subroutine-L}

The problem with $L=\infty$ is that the stopping time may never be reached for the $j$-th sampled sequence, i.e., we may have $\tau'_j = \infty$. One can set $L=\infty$ if $\mathcal A$ (i.e., its stopping time) satisfies the condition
% \begin{assumption}
    $\P_T(\tau<\infty)=1$, for all $T\in\N$
% \end{assumption}
 (if there is a change, $\mathcal A$ stops almost surely). This is a very mild assumption because any reasonable change detector will always eventually raise an alarm if there is a change. Indeed, all algorithms with finite ARL satisfy the condition, and indeed typically so do algorithms that control PFA. To avoid making even this mild assumption, one can simply pick $L < \infty$.

% Even if above assumption holds, there  are still computational reasons to choose $L <\infty$. 
If $L<\infty$ and the $j$th sequence stopped because $\mathcal A$ raised an alarm at or before $L$, then we define $M^j_{t,L}$ as usual, but if $\mathcal A$ had not stopped by time $L$, we then define $M^j_{t,L} = \infty$ to ensure a lower bound on the original test statistic (if we could compute it). Therefore, for $L<\infty$,
\begin{equation}
\label{eq:Mt-L}
    M^j_{t,L} := \begin{cases}
    -\infty, &\text{if }t>\tau_j^\prime,\\
        M_t^{(\tau_j^\prime)}(X_1^j,\cdots,X_{\tau_j^\prime}^j), &\text{if } t\leq\tau_j^\prime\leq L,\\
        \infty, &\text{if }t\leq\tau_j^\prime \text{ and } \tau_j^\prime > L.\\
    \end{cases}
\end{equation}

For the proof of the upcoming theorem, we still need a definition of $M^j_{t,\infty}$ in the case that $L=\infty$ and $\tau_j =\infty$. We define
\begin{equation}
\label{eq:Mt-infty}
    M^j_{t,\infty} := \begin{cases}
M_t^{(\tau_j^\prime)}(X_1^j,\cdots,X_{\tau_j^\prime}^j), &\text{if } t\leq\tau_j^\prime<\infty,\\
        -\infty, &\text{if } t>\tau_j^\prime \text{ or }\tau_j^\prime =\infty.\\
    \end{cases}
\end{equation}

The rationale behind setting $M^j_{t,\infty}=-\infty$ when $\tau'_j=\infty$ is the following. If $\tau_j =\infty$, for all $j=1,\cdots,B$, then we want to reject $H_{0,t}$, which is achieved by setting $M^j_{t,\infty}=-\infty$.


\begin{remark}
    The value of 
$L$ does not need to be a fixed constant; it can be set adaptively depending on the observed data sequence, for instance, one can choose $L$ to be $2\tau$. The proof of coverage works by arguing validity when $L=\infty$, and then pointing out that any smaller choice of $L$ only leads to more conservatism.
\end{remark}


\subsection{Prototypical choice of test statistic \texorpdfstring{$M_t$}{Lg}}

In the offline setting, where data is observed up to some fixed time $n$, the likelihood of data is
\begin{align}
    L_T^n=\prod_{i=1}^{T-1} f_0(X_i)\prod_{i=T}^n f_1(X_i),
\end{align}
and the maximum likelihood estimator of $T$ \citep{hinkley1970inferencce,yao1987approximating} is given by
\begin{equation}
\label{eq:known-pre-post-offline}
    \hat T_n=\argmax_{1\leq j\leq n} L_j^n=\argmax_{1\leq j\leq n}\prod_{i=j}^n\frac{f_1(X_i)}{f_0(X_i)}.
\end{equation}
In our sequential setting, where the data is observed up to a data-dependent stopping time $\tau$, we adapt this 
estimator by replacing $n$ with $\tau$ and define the estimator of the changepoint as:
\begin{equation}
\label{eq:known-pre-post}
    \hat T=\argmax_{1\leq j\leq \tau}\prod_{i=j}^\tau\frac{f_1(X_i)}{f_0(X_i)}.
\end{equation}
The prototypical test statistic that we employ in our simulations is the likelihood-ratio-based test statistic $M_t^{(n)}$, where for all $n\in\N, t\leq n$,
    \begin{equation}
    \label{eq:test-stat-comp-post}
        M_t^{(n)}(X_1,\cdots,X_n):=
            \displaystyle\max_{1\leq i\leq n} \frac{L_{i}^n}{L_t^n}.
    \end{equation}
Note that for all $t\in\N$, the test statistic $M_t$ for testing $H_{0,t}$ simplifies to 
    \begin{equation}
    \label{eq:test-stat-simp}
        M_t=\begin{cases}
            %0,& \text{ if }  \\
             L_{\hat T}^\tau/L_t^\tau,& \text{ if } t\leq\tau <\infty\\
             -\infty &\text{if }t>\tau\text{ or }\tau=\infty.
        \end{cases}
    \end{equation}

%However, it does not guarantee $1-\alpha$ coverage, which we investigate and propose a modification to address the issue in the following subsections.


 \subsection{Unconditional coverage when \texorpdfstring{$\mathcal{A}$}{Lg} controls PFA}

We now investigate the coverage properties and provide a bound on the coverage of the confidence set produced by \Cref{algo:1}.

\begin{theorem}
\label{thm:nonexact-uncond}
   Let $\mathcal{C}$ be the confidence set for changepoint $T\in\N$ produced by \Cref{algo:1}, for some $\alpha\in(0,1)$, $B\in\mathbb N$, $L\in\N \cup \{\infty\}$ and change detection algorithm $\mathcal{A}$. Then, 
    \begin{equation*}
      \P_T(T\in\mathcal{C})\geq1-\alpha-\mathbb \P_\infty(\tau< T).
    \end{equation*}  
Therefore, if $\mathcal{A}$ controls PFA at level $\delta$,$$\P_T(T\in\mathcal{C})\geq1-\alpha-\delta.$$
\end{theorem}


\begin{remark}
    When $T\ll$ ARL, we typically have $\mathbb \P_\infty(\tau< T)\approx 0$ and so $\P_T(T\in \mathcal{C})\gtrsim 1-\alpha$. The confidence set constructed using our method will approximately achieve the desired unconditional coverage level $1-\alpha,$ in situations where the probability of false detections before the true changepoint is negligible. However,  $T$ is not known to us, and hence, we are unable to produce a suitable bound on the unconditional coverage for the detection algorithms that almost surely stop at a finite time, even when there is no change.
\end{remark}


\subsection{Unconditional coverage when \texorpdfstring{$\mathcal{A}$}{Lg} has finite ARL}

If the detection algorithm has finite ARL (i.e., $\tau$ is finite almost surely even if there is no change), then it is impossible for any confidence set $\mathcal{C}^*$, which is a subset of $\{1,\cdots,\tau\}$, to produce an unconditional coverage guarantee at a predetermined level $1-\alpha$. The intuitive reason is that if  $\tau$ is finite almost surely, it is possibly a false detection, i.e., $\tau<T$, in which case the set $\{1,\cdots,\tau\}$ itself does not cover $T$. The next proposition formalizes this fact. 

\begin{proposition}
\label{prop:impossibility}
    If a confidence set $\mathcal{C}^*$ that is a subset of $\{1,\cdots,\tau\}$ satisfies an unconditional coverage guarantee $\P_T(T\in\mathcal{C}^*)\geq1-\alpha$, for some $\alpha\in(0,1)$, then $\P_\infty(\tau =\infty) \geq 1-\alpha$ (meaning that the PFA of the algorithm is at most $\alpha$ and thus the ARL of the algorithm is infinite). Said differently, for any algorithm with finite ARL, it is impossible to construct a valid confidence set that is only a subset of $\{1,\dots,\tau\}$.
\end{proposition}

The proposition says that if we have a confidence set with high unconditional coverage, then the detection algorithm never stops with high probability if there is no change.

    
Therefore, in order to achieve an unconditional coverage guarantee with $\mathcal{A}$ having finite ARL, we must extend our confidence set beyond the stopping time $\tau$. So, we consider the union of the confidence set constructed using \Cref{algo:1} and all integers greater than $\tau$. The following proposition establishes that this augmented confidence set achieves the desired coverage guarantee of $1-\alpha$.

\begin{proposition}
\label{prop:exact-uncond}
 Let $\mathcal{C}$ be the confidence set for changepoint $T\in\N$ produced by \Cref{algo:1}, for some $\alpha\in(0,1)$, $B\in\mathbb N$,  $L\in\N \cup \{\infty\}$ and change detection algorithm $\mathcal{A}$. Then, $\mathcal{C}^*:=\mathcal{C}\cup \{\tau+1,\tau+2,\cdots\}$ satisfies
    \begin{equation}
       \P_T(T\in\mathcal{C}^*)\geq1-\alpha.
    \end{equation}   
\end{proposition}


 So, the confidence set constructed using \Cref{algo:1} can be interpreted as the pre-detection part of the augmented confidence set, which has the desired coverage guarantee of $1-\alpha$.  




\subsection{Conditional coverage when \texorpdfstring{$\mathcal{A}$}{Lg} has finite ARL}
In the previous subsection, we established that the conventional notion of coverage is unsuitable for detection algorithms with finite ARL, where the confidence set is a subset of the random set $\{1,\cdots,\tau\}$. Intuitively, it is more appropriate to consider the conditional coverage, given that no false detection occurs, i.e., $T\leq\tau$. This adjustment is crucial because discussing coverage becomes meaningless when $\tau<T$, meaning that the detection occurs before the true changepoint --- the target point we wish to cover by our confidence set.

The following is an immediate corollary of \Cref{thm:nonexact-uncond}, where we show a bound on the conditional coverage of the confidence set produced by \Cref{algo:1}.

\begin{corollary}
\label{thm:approxcond}
    For any $\alpha\in(0,1)$, $B\in\mathbb N$ and $T \in \mathbb N$, and change detection algorithm $\mathcal{A}$ satisfying $\mathbb \P_\infty(\tau\geq T)\neq 0$, the confidence set $\mathcal{C}$ produced by \Cref{algo:1} satisfies
    \begin{equation}
    \label{eq:cond-coverage}
       \P_T(T\in\mathcal{C}\mid \tau\geq T)\geq1-\frac{\alpha}{\mathbb \P_\infty(\tau\geq T)}.
       % \text{ when } 
    \end{equation}
Therefore, if $\mathcal{A}$ controls PFA at level $\delta\in(0,1)$,
\begin{equation}
       \P_T(T\in\mathcal{C}\mid \tau\geq T)\geq1-\frac{\alpha}{1-\delta}. 
       % \text{ when } \mathbb \P_\infty(\tau\geq T)\neq 0
    \end{equation}
\end{corollary}
    
%The theorem says that the confidence set produced by \cref{algo:1} gives at least $1-\alpha$ conditional coverage for the changepoint, given that the true changepoint precedes the detection time, meaning no false detection has occurred.
 %\begin{remark}
 %Similarly, as with its unconditional coverage, when $T\ll$ ARL, we typically have a very small probability of a false detection before $T$ and hence, $\mathbb \P_\infty(\tau\geq T)\approx 1$. Therefore, $\P_T(T\in \mathcal{C}\mid \tau\geq T)\gtrapprox 1-\alpha$. Therefore, in scenarios where the probability of false detections prior to the true changepoint is negligible, the confidence set constructed using \cref{algo:1} will approximately achieve the desired conditional coverage level $1-\alpha.$
 %\end{remark}
 The restriction that $\mathbb \P_\infty(\tau\geq T) \neq 0$ is very weak and clearly necessary for conditional coverage. Since $T$ is arbitrary, the condition means that there is no $S < \infty$ satisfying $\P_\infty(\tau < S)=1$. If the algorithm always stops under $\P_\infty$ before $S$, then it can never detect any change that occurs after $S$. This is because the information that distinguishes $\P_\infty$ from $\P_T$ for $T > S$ occurs after $S$, but the algorithm has halted before $S$.
 
 When $\tau$ is finite almost surely (i.e., PFA is $1$), we cannot have a positive lower bound on $\mathbb \P_\infty(\tau\geq T)$ uniformly over $T\in\N$. Therefore,
 \Cref{algo:1} does not guarantee conditional coverage at some prespecified level for detection algorithms with finite ARL (which implies that PFA is 1, although the converse is not true). Furthermore, if $\mathcal{A}$ is completely black-box --- meaning its PFA is unknown --- it becomes infeasible to construct a confidence set with the desired conditional or unconditional coverage using this approach.
 We propose a simple modification to address the issue in the following subsection, which applies to any black-box detection algorithm. 
 
 
%If we could test $H_{0,t}$ at level $\alpha$ conditioned on the event of no false detection, i.e., $\tau\geq T$, we could achieve $1-\alpha$ conditional coverage for the confidence set obtained by inverting such a conditional test. However, since $T$ is unknown, performing this conditional test appears to be impractical.
 % \begin{remark}
 %  Instead of testing $H_{0,t}$ we could also test $H_{0,t}$ conditioned on stopping at time $\tau$. 
 % \end{remark}


\subsection{Modified confidence set when \texorpdfstring{$\mathcal{A}$}{Lg} has finite ARL (or \texorpdfstring{$\mathcal{A}$}{Lg} is a blackbox)}

\Cref{eq:cond-coverage} indicates that to ensure a conditional coverage guarantee of $1-\alpha$, we observe that we need to test $H_{0,T}$ at level $\alpha\times\mathbb \P_\infty(\tau\geq T),$ rather than simply $\alpha$. To achieve that, for each $t\in\{1,\cdots,\tau\}$, we test $H_{0,t}$ at level $\alpha\times\mathbb \P_\infty(\tau\geq t).$ For each $i\in \{1,\cdots,N\}$,
we generate a sequence $\{Z_n^i\}_n\stackrel{iid}{\sim} F_0$ until a change is detected, recording the detection times as $\tau_i$, which help to estimate the probability $\P_\infty(\tau\geq t)$, for $t=1,\cdots,\tau$. Therefore, it suffices to draw data points only up to $\min\{\tau,\tau_j\}$, for $j=1,\cdots,N.$

Then, we draw $B$ many independent streams of data under $H_{0,t}$. For the $j$th sequence, denoted as $\{X_n^j\}_n$, we simulate data points until a change is detected at the stopping time $\tau_j^\prime$, using the same change detection method $\mathcal{A}$. Then, we compute the test statistic $M_{t,L}^j$ as described in \Cref{sec:subroutine-L}, based on $\tau_j^\prime\wedge L$ and $X_1^j,\cdots,X_{\tau_j^\prime\wedge L}^j$.
Our confidence set is the collection of all $t\in\{1,\cdots,\tau\}$, for which we fail to reject $H_{0,t}$, i.e.,
\begin{equation}
    \label{eq:conf-simple}
  \mathcal{C}=\left\{t\in\N: t\leq \tau, M_t\leq\operatorname{Quantile}(1-\hat\alpha_{t}; M_t,M_{t,L}^1,\cdots,M_{t,L}^B)\right\},
\end{equation}
where $\hat\alpha_{t}=\alpha\times\sum_{i=1}^N \mathds{I}(\tau_i\geq 
 t)/N$.
Algorithm \ref{algo:1-exact} contains a structured overview of the method.

\begin{algorithm}[h!]
\SetKwData{Left}{left}\SetKwData{This}{this}\SetKwData{Up}{up}
\SetKwFunction{Union}{Union}\SetKwFunction{FindCompress}{FindCompress}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
%\DontPrintSemicolon
\SetAlgoLined
%\begin{algorithmic}[1]
\KwIn{ $\alpha\in(0,1)$, $N\in\N, B\in\N$, change detection algorithm $\mathcal{A}$ and data $\{X_n\}_{n}$ until a change is detected at $\tau$ using $\mathcal{A}$}
\KwOut{A confidence set $\mathcal{C}$ for the changepoint $T$}
%Detect change at $\tau$ using $\mathcal{A}$\\
$\mathcal{C}= \varnothing$\\
\For{$i=1,\cdots,N$}{
  Simulate $\{Z_n^i\}_n\stackrel{iid}{\sim} F_0$, until $\mathcal{A}$ detects a change at $\tau_j$, or until $\tau$, whichever happens first.\\
}
\For{$t=1,\cdots,\tau$}{
 Compute some test statistic $M_t$ (e.g.\ \eqref{eq:test-stat-simp}) based on $\tau$ and $X_1,\cdots,X_\tau$\\
 \For{$j=1,\cdots,B$}{
  Simulate $X_1^j,\cdots,X_{t-1}^j\stackrel{iid}{\sim} F_0; ~X_{t}^j,X_{t+2}^j\cdots\stackrel{iid}{\sim} F_1$, until $\mathcal{A}$ detects a change at $\tau_j^\prime$, or time $L$, whichever happens first.\\
\underline{Subroutine $\mathcal L$:} Compute $M_{t,L}^j$ (defined in \eqref{eq:Mt-L},\eqref{eq:Mt-infty}) based on $\tau_j^\prime$, $L$ and $X_1^j,\cdots,X_{\tau_j^\prime \wedge L}^j$\\
}
Compute $\hat\alpha_{t}:=\alpha\times\sum_{i=1}^N \mathds{I}(\tau_i\geq 
 t)/N$\\
\If{$M_t\leq\operatorname{Quantile}(1-\hat\alpha_{t}; M_t,M_{t,L}^1,\cdots,M_{t,L}^B)$}{$\mathcal{C}=\mathcal{C}\cup \{t\}$}
}
%\BlankLine
%\end{algorithmic}
\caption{CI for known pre-change $F_0$ and post-change $F_1$}
\label{algo:1-exact}
\end{algorithm}

 By suitably adjusting the test levels for each $t$, this approach ensures that the desired conditional coverage guarantee is achieved. Notably, this guarantee does not depend on any specific property of $\mathcal{A}$ (e.g., its PFA or ARL). We prove it in the following theorem.
 
\begin{theorem}
\label{thm:exact-cond-coverage}
    For any fixed $\alpha\in(0,1)$, $B,N\in\mathbb N$, $L\in\N\cup\{\infty\}$,  $T\in\N$ and change detection algorithm $\mathcal{A}$ with $\mathbb \P_\infty(\tau\geq T)\neq 0$, the confidence set $\mathcal{C}$ defined in \eqref{eq:conf-simple} satisfies
    \begin{equation}
     \P_T(T\in\mathcal{C}\mid \tau\geq T)\geq1-{\alpha}.
    \end{equation}
\end{theorem}
\begin{proof}
First note the fact that \(\P_T(\tau< T)=\mathbb \P_\infty(\tau< T),\)
since the event $[\tau< T]=\displaystyle\cup_{i=1}^{T-1}[\tau=i]$ does not depend on $\{X_i:i\geq T\}$. 

\textbf{Step 1 [Relate the level of testing ${H}_{0,T}$ and the conditional coverage]:} Defining $C_{t,L}=\operatorname{Quantile}(1-\hat\alpha_{t};M_t,M_{t,L}^1,\cdots,M_{t,L}^B)$, the level of testing ${H}_{0,T}$ is $P_T( M_T> C_{t,L}).$ Thus,
\begin{align*}
  &\P_T( M_T\leq C_{t,L})\\
   &=\P_T(T\in \{t\in\N: M_t\leq C_{t,L}\})\\
   &=\P_T(T\in \{t\in\N: M_t\leq C_{t,L}\},\tau\geq T) +\mathbb P_T(T\in \{t\in\N: M_t\leq C_{t,L}\},\tau< T)\\
    &\leq \P_T(T\in \{t\in\N: t\leq \tau, M_t\leq C_{t,L}\}\mid \tau\geq T)\P_T(\tau\geq T)+\mathbb \P_T(\tau< T)\\
    &= \P_T(T\in \mathcal{C}\mid \tau\geq T)\mathbb \P_\infty(\tau\geq T)+1-\mathbb \P_\infty(\tau\geq T),
\end{align*}
where we twice invoked \(\P_T(\tau< T)=\mathbb \P_\infty(\tau< T)\) as explained earlier.
% The inequality in the third line holds because
% \begin{align*}
%     \P_T(T\in \{t \geq 1: M_t\leq C_{t}^N\},\tau< T)\leq \P_T(\tau< T)=\mathbb \P_\infty(\tau< T),
% \end{align*}
So, we obtain
\begin{equation}
\label{eq:cond-level}
    \P_T(T\in \mathcal{C}\mid \tau\geq T)\geq  1-\frac{P_T( M_T> C_{t,L})}{ P_\infty(\tau\geq T)}.
\end{equation}

\textbf{Step 2 [Establish that our test for each ${H}_{0,t}$ has level $\alpha\times\P_{\infty}(\tau\geq t)$]:} First, observe that $M_t,M_{t,\infty}^1,\cdots,M_{t,\infty}^B$ are i.i.d.\ under $H_{0,t}$. Hence, $M_t\leq \operatorname{Quantile}(1-c;M_t,M_{t,\infty}^1,\cdots,M_{t,\infty}^B)$ $\iff M_t$ is one among the $\ceil{(1-c)(B+1)}$ smallest of $M_t,M_{t,\infty}^1,\cdots,M_{t,\infty}^B$. So, for any fixed $c\in(0,1)$,
\begin{equation*}
  \P_t(M_t\leq \operatorname{Quantile}(1-c;M_t,M_{t,\infty}^1,\cdots,M_{t,\infty}^B))\geq\frac{\ceil{(1-c)(B+1)}}{B+1}\geq 1-c \text{ for all } t.
\end{equation*}
Since $\hat\alpha_{t}$ is independent of $M_t$, it follows that
\begin{equation*}
  \P_t(M_t\leq C_{t,\infty}\mid \hat\alpha_{t})\geq\frac{\ceil{(1- \hat\alpha_{t})(B+1)}}{B+1}\geq 1- \hat\alpha_{t}, \text{ almost surely for all } t.
\end{equation*}
Therefore, we have
\begin{equation*}
  \P_t(M_t\leq C_{t,\infty})=\mathbb E_t(\mathbb P_t(M_t\leq C_{t,\infty}\mid \hat\alpha_{t}))\geq \mathbb E_t( 1-\hat\alpha_{t})=1-\alpha\P_t(\tau\geq t).
\end{equation*}
As noted earlier, $\P_t(\tau\geq t)=\P_\infty(\tau\geq t)$ and hence, $\P_t(M_t\leq C_{t,\infty})\geq 1-\alpha\P_\infty(\tau\geq t).$
Also, for all $L\in\N$, it follows from the definitions that $M_{t,L}^j\geq M_{t,\infty}^j$. Therefore, $C_{t,L} \geq C_{t,\infty}$.
 Thus, for all $t$,
 \begin{equation}
 \label{level-h0t}
     \P_t(M_t> C_{t,L})\leq \P_t(M_t> C_{t,\infty})\leq \alpha \P_\infty(\tau\geq t)
 \end{equation}

Therefore, combining \eqref{eq:cond-level} and \eqref{level-h0t}, we obtain $\P_T(T\in \mathcal{C}\mid \tau\geq T)\geq  1-\alpha.$
\end{proof} 
\begin{remark}
As mentioned before, the assumption $\mathbb{P}_{\theta_0,\infty}(\tau \geq T) \neq 0$ is essential; otherwise, conditioning the coverage probability on $\{\tau \geq T\}$ becomes undefined. If this assumption does not hold, we have $\mathbb{P}_{\theta_0,\infty}(\tau < T) = \mathbb{P}_T(\tau < T) = 1$, implying that the detection is almost surely false. In such a scenario, constructing a confidence interval would not be meaningful.
\end{remark}
\begin{remark}
    We do not require any specific property of the sequential change detection algorithm; it can function as an entirely black-box or heuristic method. The only necessity is a stopping rule that can be applied to generated data sequences.
\end{remark}
%\begin{remark}
%\label{rem:tau-sim}
%    In line 4 in \Cref{algo:1-exact}, we need to generate data points only up to time $\min\{\tau,\tau_i\}$, as this would suffice for computing $\mathds{I}(\tau_i\geq t)$, for $t=1,\cdots,\tau$ (in line 12).\\
%\end{remark}
\begin{remark}
    For a fixed $t$, the sampled sequences $\{Z^j_n\}_n$ must be independent across all $j$ to ensure the validity of the above theorem. However, for testing the $t$-th hypothesis ($t \geq 3$), one can reuse the first $t-2$ data points from the sequences generated for testing the $(t-1)$-th hypothesis, thereby reducing computational redundancy. Nevertheless, in our experiments, we sampled the sequences independently across $t$.
\end{remark}



\section{Known pre-change but unknown post-change distribution}
\label{sec:composite-post}

In this section, we assume that the post-change distribution follows some parametric composite model $\mathcal{P}_1=\{F_{\theta}:  {\theta}\in \Theta_1\}$ and the pre-change distribution, $F_{\theta_0} (\theta_0\notin\Theta_1)$ is known. 

\textbf{Notation.} For $t \in \N$, let $\P_{\theta,t,\theta^\prime}$ denote the joint distribution of the sequence of independent random variables $\{X_n\}_{n\in\N}$, when $X_1,\dots,X_{t-1}$ are from $F_\theta$ (with density $f_\theta$) and $X_t,X_{t+1},\dots$ are from $F_{\theta^\prime}$ (with density $f_{\theta^\prime}$). As corner cases, let $\P_{1,\theta^\prime}$ denotes the joint distribution where $X_1,X_2,\dots \stackrel{i.i.d}{\sim} F_{\theta^\prime}$ and $\P_{\theta,\infty}$ denotes the case where $X_1,X_2,\dots \stackrel{i.i.d}{\sim} F_\theta$.  Let $\mathbb E_{\theta,t,\theta^\prime}$ denote the expectation under $\P_{\theta,t,\theta^\prime}$, for $t \in \N \cup \{\infty\}$, for $t \in \N \cup \{\infty\}$ and $\mathbb E_{\theta,\infty}$ denote the expectation under $\P_{\theta,\infty}$.

Suppose we have the data sequence $\{X_n\}_n\sim\P_{\theta_0,T,\theta_1}$, for some unknown $T \in \N \cup \{\infty\}, \theta_1\in\Theta_1$ and some sequential change detection algorithm $\mathcal{A}$ that raises an alarm at the stopping time $\tau$ in the sequence.
We define the null hypothesis that a changepoint occurs at time $t\in\{1,\cdots,\tau\}$ as:
\[
\mathcal H_{0,t}: X_1,\cdots,X_{t-1}\stackrel{i.i.d}{\sim}F_{{\theta_0}}, \text{ and }X_{t},X_{t+1},\cdots\stackrel{i.i.d}{\sim}F_{\theta_1}, \text{ for some }  {\theta_1}\in \Theta_1. 
\]
For testing the null hypothesis $H_{0,t}$, 
we consider some test statistic $M_t:=M_t^{(\tau)}(X_1,\cdots,X_\tau)$ based on data up to time $\tau,$ if $t\leq\tau<\infty$ and $M_t:=-\infty$, otherwise, for each $t\in\N$, where $M_t^{(n)}$ is some test statistic based on $n$ many data points, for $n,t\in\N, t\leq n$. An example is provided in \Cref{sec:test-stat-comp-post}.
  
  
Suppose the change detection algorithm, $\mathcal{A}$ has finite ARL or $\mathcal{A}$ is possibly a blackbox (i.e., its properties or inner workings are unknown to us). In that case, we generate a sequence $\{X_n^i\}_n\stackrel{iid}{\sim} F_0$ until a change is detected, recording the detection times as $\tau_i$. These will help empirically estimate the probability $\P_\infty(\tau\geq t)$, for $t=1,\cdots,\tau$. Therefore, it suffices to draw data points only up to $\min\{\tau,\tau_j\}$, for $j=1,\cdots,N$. However, if $\mathcal{A}$ controls PFA at a known level $\delta\in(0,1)$, we can avoid this step.

Since the post-change parameter is unknown, for each $t$, we estimate it based on $X_t,\cdots,X_\tau$.  However, note that the sample size used for estimate might be too small to have a reliable estimate. To address this issue, we would use interval estimates instead of point estimates so that we can hope to achieve a predetermined control over the error. Notably, conventional confidence intervals are invalid in this context, since $\tau$ depends on the data. Therefore, we rely on confidence sequences \citep{darling1967confidence,jennison1989interim,howard2021time,chowdhury23a}. A confidence sequence having coverage $1-c$ is defined as a sequence of confidence sets $\{\operatorname{CS}( X_1,\cdots,X_n;1-c)\}_{n\in\N}$, satisfying the uniform coverage guarantee: $\P(\forall n\in \N, \theta\in \operatorname{CS}(X_1,\cdots,X_n;1-c))\geq 1-c.$
If $\mathcal{A}$ controls PFA, we can simply construct $\mathcal{S}^\prime_{\tau-t+1}=\operatorname{CS}(X_t,\cdots,X_{\tau};1-\beta)$ for $\theta_1$ having $1-\beta$ coverage. Otherwise, we construct $\mathcal{S}^\prime_{\tau-t+1}=\operatorname{CS}(X_t,\cdots,X_{\tau};1-\beta r_t)$, where 
$r_{t}:=\sum_{i=1}^N \mathds{I}(\tau_i\geq 
 t)/N$, which have $1-\beta\times\P_{\theta_0,\infty}(\tau\geq t)$ coverage. 
 Despite $r_t$ being random, it is based on independent simulations, and so it turns out that marginalizing over $r_t$ yields that the CS has coverage probability $\beta\times\P_{\theta_0,\infty}(\tau\geq t)$; we formalize this more explicitly in the proof of the \Cref{thm:coverage-comp-post}.
 
%We first estimate the changepoint $T$ as $\hat{T}$ (defined in \eqref{eq:unknown-pre-post}), then use the observations from $1$ to $\hat{T}-1$ to estimate the pre-change distribution $F_0$, denoted $\hat{F}_0$ and from $\hat{T}$ to $\tau$ to estimate the post-change distribution $F_1$, denoted $\hat{F}_1$.
%For some $N\in \N$, we generate $N$ many i.i.d. sequences under $\hat F_0$ until a change is detected and denote the detection times as $\tau_1,\cdots,\tau_N$. Then, for some $B\in \N$, we generate $B$ independent bootstrap data streams with the estimated pre- and post-change distributions $\hat{F}_0$ and $\hat{F}_1$ respectively, having changepoint at $t$.
%For each resampled stream $j$, we calculate the test statistic $M_t^j$ to obtain $M_t^1, M_t^2, \dots, M_t^B$. We reject $\mathcal H_{0,t}$ if we have
For each $j=1,\cdots,B$, for some fixed $B\in\N$, suppose we have sequences $\{X^j_n(\theta_0,t,\theta)\}_n$ having joint distribution as $\P_{\theta_0,t,\theta}$. Note that for a fixed $t$ and $\theta$, the sequences must be independent across $j$. However, for any fixed $j$, the i.i.d. sequences $\{X^j_n(\theta_0,\theta)\}_n$ are allowed to be dependent across different values of $t$ and  $\theta\in\mathcal{S}^\prime_{\tau-t+1}$. A specific method for generating these sequences is detailed in \Cref{sec:subroutines-comp-post}. We draw the datapoints until a change is detected at the stopping time $\tau_{j,t}^\theta$ using $\mathcal{A}$ or until time $L$, whichever occurs first. Then, we compute the same test statistics truncated at $L$, based on $\tau_{j,t}^\theta$ and $X^j_1(\theta_0,t,\theta),\cdots,X^j_{\tau_{j,t}^\theta\wedge L}(\theta_0,t,\theta)$, which is denoted as $M^j_{t,L}(\theta)$.
For $L<\infty$,
\begin{equation}
\label{eq:Mt-L-comp-post}
    M^j_{t,L}(\theta) := \begin{cases}
    -\infty, &\text{if }t> \tau_{j,t}^\theta,\\
        M_t^{(\tau_{j,t}^\theta)}(X_1^j(\theta_0,t,\theta),\cdots,X_{\tau_{j,t}^\theta}^j(\theta_0,t,\theta)), &\text{if } t\leq\tau_{j,t}^\theta\leq L\\
        \infty, &\text{if } t\leq\tau_{j,t}^\theta \text{ and }\tau_{j,t}^\theta > L.
    \end{cases}
\end{equation}
And for $L=\infty$, define
% \begin{equation}
% \label{eq:Mt-infty-comp-post}
    $M^j_{t,\infty} := 
M_t(X_1^j(\theta_0,t,\theta),\cdots,X_{\tau_{j,t}^\theta}^j(\theta_0,t,\theta)), \text{ if } t\leq\tau_{j,t}^\theta<\infty$ and equal to 
        $-\infty, \text{ if }t> \tau_{j,t}^\theta \text{ or }\tau_{j,t}^\theta =\infty.$
    % \end{cases}
% \end{equation}
Our confidence set is the collection of all $t$ before $\tau$, for which we fail to reject $\mathcal H_{0,t}$ at level $\alpha\times\P_{\theta_0,\infty}(\tau\geq t)$:
\begin{equation}
\label{eq:conf-comp-post}
    \mathcal{C}_1=\left\{t\in \N:t\leq \tau, M_t\leq\sup_{\theta\in\mathcal{S}^\prime_{\tau-t+1}}\operatorname{Quantile}\left(1-\alpha r_{t};M_t,\{M_{t,L}^j(\theta)\}_{j=1}^B\right)\right\},
\end{equation}
where 
$r_{t}:=\sum_{i=1}^N \mathds{I}(\tau_i\geq 
 t)/N$. If $\mathcal{A}$ controls PFA at some known level $\delta\in(0,1)$, we can have the following confidence set, which is simpler (i.e., does not require $\tau_i$'s):
 
\begin{equation}
\label{eq:conf-comp-post-pfa}
    \mathcal{C}_2=\left\{t\in \N:t\leq \tau, M_t\leq\sup_{\theta\in\mathcal{S}^\prime_{\tau-t+1}}\operatorname{Quantile}\left(1-\alpha;M_t,\{M_{t,L}^j(\theta)\}_{j=1}^B\right)\right\},
\end{equation}
Algorithm \ref{algo:comp-post} contains an overview of the method.

We note that, even when $\mathcal{S}^\prime_{\tau-t+1}$ is an infinite set, one can approximate the threshold $\sup_{\theta\in\mathcal{S}^\prime_{\tau-t+1}}\operatorname{Quantile}(1-\hat\alpha_{t};M_t,\{M_{t,L}^j(\theta)\}_{j=1}^B)$ by discretizing the parameter space into a grid and evaluating the quantile for values of 
$\theta$ on this grid, taking their maximum. 
Moreover, in many cases, we can derive a suitable upper bound of this threshold, which is computationally, statistically efficient, and exact (i.e., does not require any approximation). We provide the details in \Cref{sec:subroutines-comp-post}. Additionally, we illustrate concrete examples of the Gaussian mean shift problem in \Cref{sec:gaussian-example-comp-post} and the Poisson rate change problem in \Cref{sec:a-pois}.
\begin{comment}
\begin{algorithm}[h!]
\SetKwData{Left}{left}\SetKwData{This}{this}\SetKwData{Up}{up}
\SetKwFunction{Union}{Union}\SetKwFunction{FindCompress}{FindCompress}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
%\DontPrintSemicolon
\SetAlgoLined
%\begin{algorithmic}[1]
\KwIn{ $\theta_0, \Theta_1, \alpha, \beta$, $N,B$, $\theta^{\prime\prime}$, a function $G$ such that $X\sim F_{\theta^{\prime\prime}}$ implies $G(X,\theta)\sim F_\theta,$ $\forall \theta\in\Theta_1$, a change detection algorithm $\mathcal{A}$, and sequence of observations $\{X_n\}_{n\in \N}$}
\KwOut{Confidence sets $\mathcal{C}$ for changepoint $T$}
Detect change at $\tau$ using $\mathcal{A}$\\
\For{$i=1,\cdots,N$}{
  Simulate $X_1^i,X_{2}^i\cdots\stackrel{iid}{\sim} F_{\theta_0}$, until a change is detected at $\tau_i$ using $\mathcal{A}$\\
}
%Compute the point estimator $\hat{T}$ for $T$, as defined in \eqref{eq:known-pre-unknown-post}\\
%Obtain an estimate $\hat F_1$ of the (unknown) post-change $F_1\in\mathcal{P}_1$ based on $X_{\hat{T}},\cdots,X_{\tau}$\\
$\mathcal{C}= \varnothing$\\
\For{$t=1,\cdots,\tau$}{
Compute some test statistics $M_t:=M_t(X_1,\cdots,X_\tau;\tau)$ (e.g., as defined in \eqref{eq:test-stat-comp-post})\\
Find a confidence sequence $\{\mathcal{S}_n^\prime\}_{n}$ for $\theta_1$ so that  $\P_{\theta_0,t,\theta_1}(\theta_1\notin\mathcal{S}^\prime_{\tau-t+1}(X_t,\cdots,X_{\tau}))\leq\beta \times \P_{\theta_0,\infty}(\tau\geq t)$\\
\For{$j=1,\cdots,B$}{
Generate $X_1^j,\cdots,X_{t-1}^j\stackrel{iid}{\sim} F_{\theta_0}$, $X_{t}^j,X_{t+1}^j\cdots\stackrel{iid}{\sim} 
  F_{\theta^{\prime\prime}}$\\
  Find detection time $\tau^*$ in the sequence $\{X^j_n)\}_n$ using $\mathcal{A}$\\
  \If{$\tau^*<t$}{$M_t^j(\theta):=M_t\left(X^j_1,\cdots,X^j_{\tau^*};\tau^*\right)$, $\forall\theta\in \mathcal{S}^\prime_{\tau-t+1}(X_t,\cdots,X_{\tau})$ (does not depend on $\theta$)} 
  \Else{
\For{$\theta\in \mathcal{S}^\prime_{\tau-t+1}(X_t,\cdots,X_{\tau})$}{
 Find the detection time $\tau_j^\theta$ in the sequence $\{X^j_1,\cdots,X^j_t,G(X^j_t,\theta),G(X^j_{t+1},\theta),\cdots\}$ using $\mathcal{A}$\\
Compute $M_t^j(\theta):=M_t\left(X^j_1,\cdots,X^j_t,G(X^j_t,\theta),\cdots,G(X^j_{\tau_j^\theta},\theta);\tau_j^\theta\right)$\\
}}}
Compute $\hat\alpha_{t}:=\alpha\times\sum_{i=1}^N \mathds{I}(\tau_i\geq 
 t)/N$\\
\If{$\text{there exists }\theta \in \mathcal{S}^\prime_{\tau-t+1}: M_t\leq\operatorname{Quantile}(1-\hat\alpha_{t};M_t,\{M_t^j(\theta)\}_{j=1}^B)$}{$\mathcal{C}_1=\mathcal{C}_1\cup \{t\}$}}
%\BlankLine
%\end{algorithmic}
\caption{CI for known pre-change and composite post-change}
\label{algo:comp-post}
\end{algorithm}
\end{comment}


\begin{algorithm}[h!]
\SetKwData{Left}{left}\SetKwData{This}{this}\SetKwData{Up}{up}
\SetKwFunction{Union}{Union}\SetKwFunction{FindCompress}{FindCompress}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
%\DontPrintSemicolon
\SetAlgoLined
%\begin{algorithmic}[1]
\KwIn{ $\alpha, \beta\in(0,1)$, $N,B\in\N,L\in\N\cup\{\infty\},$ a confidence sequence procedure $\operatorname{CS(data;coverage)}$, a change detection algorithm $\mathcal{A}$, and data $\{X_n\}_{n}$ until a change is detected at $\tau$ using $\mathcal{A}$}
\KwOut{Confidence sets $\mathcal{C}$ for changepoint $T$}
$r_{1}=r_{2}=\cdots=r_{\tau}=1$\\
\If{$\mathcal{A}$ has finite ARL}{
\For{$i=1,\cdots,N$}{
  Draw $\{X_n^i\}_n\stackrel{iid}{\sim} F_{\theta_0}$, until change is detected at $\tau_i$ using $\mathcal{A}$, or until $\tau$, whichever happens first.\\
}
 $r_{t}:=\sum_{i=1}^N \mathds{I}(\tau_i\geq 
 t)/N$, for $t=1,\cdots,\tau$
}
%Compute the point estimator $\hat{T}$ for $T$, as defined in \eqref{eq:known-pre-unknown-post}\\
%Obtain an estimate $\hat F_1$ of the (unknown) post-change $F_1\in\mathcal{P}_1$ based on $X_{\hat{T}},\cdots,X_{\tau}$\\
$\mathcal{C}= \varnothing$\\
\For{$t=1,\cdots,\tau$}{
Compute some test statistics $M_t$ based on $X_1,\cdots,X_\tau$ (e.g., as defined in \eqref{eq:test-stat-comp-post-simp})\\
Find a confidence sequence for $\theta_1$: $\mathcal{S}^\prime_{\tau-t+1}=\operatorname{CS}(X_t,\cdots,X_{\tau};1-\beta r_t)$ \\
\For{$j=1,\cdots,B$}{
\underline{Subroutine 2}: \For{$\theta\in \mathcal{S}^\prime_{\tau-t+1}(X_t,\cdots,X_{\tau})$}{
Get the sequence $\{X^j_n(\theta_0,t,\theta)\}_n$ following $\P_{\theta_0,t,\theta}$ (independently for all $j$), until its detection time $\tau_{j,t}^\theta$ using $\mathcal{A}$ or until time $L$, whichever happens first.\\
Compute $M_{t,L}^j(\theta)$ based on $L, \tau_{j,t}^\theta$ and $X^j_1(\theta_0,t,\theta),\cdots,X^j_{\tau_{j,t}^\theta\wedge L}(\theta_0,t,\theta)$\\
}}
\underline{Subroutine 3}:
\If{$\text{there exists }\theta \in \mathcal{S}^\prime_{\tau-t+1}: M_t\leq\operatorname{Quantile}(1-\alpha r_{t};M_t,\{M_{t,L}^j(\theta)\}_{j=1}^B)$}{$\mathcal{C}=\mathcal{C}\cup \{t\}$}}

%\BlankLine
%\end{algorithmic}
\caption{CI for known pre-change ($F_{\theta_0}$) and composite post-change}
\label{algo:comp-post}
\end{algorithm}
\begin{theorem}
\label{thm:coverage-comp-post} 
   Let $\mathcal{C}_1$ and $\mathcal{C}_2$ be the confidence sets for changepoint $T\in\N$ as defined in \eqref{eq:conf-comp-post} and \eqref{eq:conf-comp-post-pfa} respectively, for some $\alpha,\beta\in(0,1)$, $N,B\in\mathbb N$,  $L\in\N \cup \{\infty\}$. Then for any given change detection algorithm $\mathcal{A}$ with $\mathbb \P_{\theta,\infty}(\tau\geq T)\neq 0$ and any $\theta_1\in\Theta_1$,
    \begin{equation*}
      \P_{\theta_0,T,\theta_1}(T\in \mathcal{C}_1\mid \tau\geq T)\geq 1-\alpha-\beta.
    \end{equation*}  
    If $\mathcal{A}$ controls PFA at level $\delta\in(0,1)$, then for any $\theta_1\in\Theta_1$,
    \begin{equation*}
      \P_{\theta_0,T,\theta_1}(T\in \mathcal{C}_2)\geq 1-\alpha-\beta-\delta ~\text{ , and }~ \P_{\theta_0,T,\theta_1}(T\in \mathcal{C}_2\mid \tau\geq T)\geq 1-\frac{\alpha+\beta}{1-\delta}, 
    \end{equation*} 
    where the latter guarantee holds if $\mathbb \P_{\theta,\infty}(\tau\geq T)\neq 0.$
\end{theorem}

\subsection{Prototypical choice of test statistic \texorpdfstring{$M_t$}{Lg}}
\label{sec:test-stat-comp-post}
Let $L_t(\theta_1; X_1,\cdots, X_n)$ be the likelihood of data $X_1,\cdots, X_n$ under $\mathcal H_{0,t}$. Then, for $n\geq t$,
\begin{align}
    L_t(\theta_1; X_1,\cdots, X_n)=\prod_{i=1}^{t-1} f_{\theta_0}(X_i)\prod_{i=t}^n f_{\theta_1}(X_i).
\end{align}
We define $\hat \theta_{1,j:n}$ to be a maximum likelihood estimator (MLE) of the post-change parameter $\theta_1\in \Theta_1$ based on data $X_j,\cdots,X_n$. The point estimate of $T$ can be naturally extended to:
\begin{equation}
\label{eq:known-pre-unknown-post}
    \hat T=\argmax_{1\leq j\leq \tau}\prod_{i=j}^\tau\frac{ f_{\hat\theta_{1,j:\tau}}(X_i)}{f_{\theta_{0}}(X_i)}.
\end{equation}
The prototypical test statistic that we employ in our simulations is the likelihood-ratio-based test statistic $M_t^{(n)}$, where for $t,n\in\N,t\leq n$,
    \begin{equation}
        M_t^{(n)}(X_1,\cdots,X_n)=
            \displaystyle\max_{1\leq i\leq n} \frac{L_{i}(\hat \theta_{1,i:n};X_1,\cdots,X_n)}{L_t(\hat \theta_{1,t:n};X_1,\cdots,X_n)}.
    \end{equation}
Note that the test statistic $M_t$ for testing $\mathcal{H}_{0,t}$ reduces to
     \begin{equation}
    \label{eq:test-stat-comp-post-simp}
        M_t=\begin{cases}
        %0, &\text{if } t>\tau, \\
             \frac{L_{\hat{T}}(\hat \theta_{1,\hat{T}:\tau};X_1,\cdots,X_\tau)}{L_t(\hat \theta_{1,t:\tau};X_1,\cdots,X_\tau)}, &\text{if } t\leq\tau<\infty, \\
             -\infty, &\text{if }t>\tau, \text{ or }\tau =\infty.\\

        \end{cases}
    \end{equation}
%\begin{remark}
%  The assumption $\mathbb{P}_{\theta_0,\infty}(\tau \geq T) \neq 0$ is essential; otherwise, conditioning the coverage probability on $[\tau \geq T]$ becomes undefined. If this assumption does not hold, we have $\mathbb{P}_{\theta_0,\infty}(\tau < T) = \mathbb{P}_T(\tau < T) = 1$, implying that the detection is almost surely false. In such a scenario, constructing a confidence interval would not be meaningful.
%\end{remark}


\subsection{Computationally and statistically efficient subroutines}
\label{sec:subroutines-comp-post}
\begin{comment}

\subsubsection{Subroutine 1}
Although it might be challenging to compute $\P_{\theta_0,\infty}(\tau\geq t)$, 
 we can easily construct a $1-\beta \times \P_{\theta_0,\infty}(\tau\geq t)$ confidence sequence using $\tau_1,\cdots,\tau_N$ that we computed in line 3 of \Cref{algo:comp-post}. We only need to construct a confidence sequence with conditional coverage $1-\beta\times\frac{1}{N}\sum_{i=1}^N\mathds{I}(\tau_i\geq t)$, given $\tau_1,\cdots,\tau_N$ (which are independent of the observations $\{X_n\}_n$), i.e.,
\begin{equation*}
    \P_{\theta_0,t,\theta_1}(\theta_1\notin\mathcal{S}^\prime_{\tau-t+1}(X_t,\cdots,X_{\tau})\mid \tau_1,\cdots,\tau_N)\leq\beta \times \frac{1}{N}\sum_{i=1}^N\mathds{I}(\tau_i\geq t),
\end{equation*}
which would imply
\begin{equation*}
    \P_{\theta_0,t,\theta_1}(\theta_1\notin\mathcal{S}^\prime_{\tau-t+1}(X_t,\cdots,X_{\tau}))=\mathbb E \left(\P_{\theta_0,t,\theta_1}(\theta_1\notin\mathcal{S}^\prime_{\tau-t+1}(X_t,\cdots,X_{\tau})\mid \tau_1,\cdots,\tau_N)\right)\leq\beta \times \P_{\theta_0,\infty}(\tau\geq t).
\end{equation*}
\end{comment}
%\subsubsection{Subroutines 2 and 3}
For computational and statistical efficiency, we generate sequences from all $t\in\{1,\cdots,\tau\}$ and $\theta\in\mathcal{S}^\prime_{\tau-t+1}$ simultaneously using a single shared source of randomness. To achieve this, we assume that we have a fixed element $\theta^{\prime\prime}\in\Theta$ in the parameter space and a function $G$, such that $X\sim F_{\theta^{\prime\prime}}$ implies $G(X,\theta)\sim F_\theta, \forall \theta\in\Theta$. Therefore, for each $j=1,\cdots,B$, we generate only one data sequence $\{X_n^j\}_n\stackrel{i.i.d.}{\sim}F_{\theta^{\prime\prime}}$, and then for each $t\in\{1,\cdots,\tau\}$ and $\theta\in\mathcal{S}^\prime_{\tau-t+1}$, we obtain sequences $\{Y_n^t(X_{n}^j,\theta)\}_n$ (having joint distribution $\P_{\theta_0,t,\theta}$) by applying the function  $G(.,\theta_0)$ to $X_{1}^j,\cdots,X_{t-1}^j$ and $G(.,\theta)$ to $X_{t}^j,X_{t+1}^j,\cdots$, i.e.,
\begin{equation}
    Y_n^t(x,\theta):=\begin{cases}
        G(x,\theta_0), &\text{if } n<t\\
        G(x,\theta), &\text{ otherwise. }
    \end{cases}
\end{equation}
Let us provide a few examples of the function $G$. If the distributions in the parametric family are continuous, the inverse distribution function $F_{\theta}^{-1}$ exists and it is easy to verify that $G(x, \theta) = F_{\theta}^{-1}(F_{\theta'}(x))$ works. In the case of a location family, a simple transformation is $G(x, \theta) = x+\theta-\theta^{\prime\prime}$ and similarly, for scale family, we have 
$G(x, \theta) = x\times\theta/\theta^{\prime\prime}$.
 In the case of discrete distributions, even if a suitable 
$G$ is not directly available, generating such sequences $\{Y_n^t(x,\theta)\}_n$ across all $\theta\in\mathcal{S}^\prime_{\tau-t+1}$ may still be possible, as demonstrated in \Cref{sec:pois-example-comp-post} for the Poisson distribution.

%and consider all integers in between them and the same test statistics for the sequences $\{Y_n^j(\theta)\}_n$ and denote that by $M_t^j(\theta)$.So,$$M_t^j(\theta)=M_t\left(Y_1^j(\theta),\cdots,Y_{\tau_j^\theta}^j(\theta);\tau_j^\theta\right).$$ 
Let us now fix $t\in\{1,\cdots,\tau\}$.
Computing the detection time $\tau_{j,t}^\theta$ for each $\theta\in\mathcal{S}^\prime_{\tau-t+1}$, and finding whether $\text{there exists }\theta \in \mathcal{S}^\prime_{\tau-t+1}$ such that $M_t\leq\operatorname{Quantile}(1-\hat\alpha_{t};M_t,\{M_{t,L}^j(\theta)\}_{j=1}^B)$ are both computationally infeasible, unless $\mathcal{S}^\prime_{\tau-t+1}$ is a finite set.
However, it is often straightforward to compute  $t_1^j= \inf_{\theta\in \mathcal{S}^\prime_{\tau-t+1}}\tau^\theta_{j,t}$ and $t_2^j= \sup_{\theta\in \mathcal{S}^\prime_{\tau-t+1}}\tau^\theta_{j,t}$.
Also, we can often derive a suitable upper bound on the threshold, which is computationally and statistically efficient. For example, we can bound it by $\operatorname{Quantile}(1-\hat\alpha_{t};M_t,\{\sup_{\theta\in\mathcal{S}^\prime_{\tau-t+1}}M_{t,L}^j(\theta)\}_{j=1}^B)$. Again, we bound $\sup_{\theta\in\mathcal{S}^\prime_{\tau-t+1}}M_{t,L}^j(\theta)$ by $V_{t,L}^j$, where for $L<\infty$, 
\begin{equation}
    V_{t,L}^j:=\begin{cases}
    -\infty, &\text{if } t>t_2^j,\\
        \displaystyle \max_{\max\{t,t_1^j\}\leq  t^\prime\leq t_2^j}\sup_{\theta\in \mathcal{S}^\prime_{\tau-t+1}} M_t^{(t^\prime)}(Y_1^t(X_{1}^j,\theta),\cdots,Y_{t^\prime}^t(X_{t^\prime}^j,\theta);  t^\prime), &\text{if } t\leq t_2^j\leq L,\\
        \infty, &\text{if } t\leq t_2^j\text{ and } t_2^j> L\\
    \end{cases}.
\end{equation} 
And for $L=\infty$,
\begin{equation}
    V_{t,\infty}^j:=\begin{cases}
        -\infty, &\text{if } t>t_2^j \text{ or } t_1^j =\infty,\\
        \displaystyle \max_{\max\{t,t_1^j\}\leq  t^\prime\leq t_2^j}\sup_{\theta\in \mathcal{S}^\prime_{\tau-t+1}} M_t^{(t^\prime)}(Y_1^t(X_{1}^j,\theta),\cdots,Y_{t^\prime}^t(X_{t^\prime}^j,\theta);  t^\prime), &\text{otherwise,} \\
    \end{cases}
\end{equation} 
 which are often straightforward to compute even with $L=\infty$ (e.g., in a Gaussian setting with CUSUM or SR-type change detectors, see \Cref{sec:gaussian-example-comp-post}).
 We define the confidence set as
 \begin{equation}
 \label{set-gaussian-known-pre}
     \mathcal{C}^\prime_1=\{t\in\{1,\cdots,\tau\}: M_t\leq\operatorname{Quantile}(1-\alpha\times\sum_{i=1}^N \mathds{I}(\tau_i\geq 
 t)/N;M_t,V_{t,L}^1,\cdots,V_{t,L}^B)\} \text{ and}
 \end{equation}
 \begin{equation}
 \label{set-eff-known-pre-pfa}
     \mathcal{C}^\prime_2=\{t\in\{1,\cdots,\tau\}: M_t\leq\operatorname{Quantile}(1-\alpha;M_t,V_{t,L}^1,\cdots,V_{t,L}^B)\}.
 \end{equation}
We observe that $\mathcal{C}^\prime\supseteq \mathcal{C}$, where $\mathcal{C}$ is the set produced by \Cref{algo:comp-post}, since
 \begin{align*}
     &\operatorname{Quantile}(1-\frac{\alpha}{N}\sum_{i=1}^N \mathds{I}(\tau_i\geq   t);M_t,\{V_{t,L}^j\}_{j=1}^B)\\
     &\geq \operatorname{Quantile}(1-\frac{\alpha}{N}\sum_{i=1}^N \mathds{I}(\tau_i\geq   t);M_t,\{\sup_{\theta\in\mathcal{S}^\prime_{\tau-t+1}}M_{t,L}^j(\theta)\}_{j=1}^B)\\
 &\geq \sup_{\theta\in\mathcal{S}^\prime_{\tau-t+1}}\operatorname{Quantile}(1-\frac{\alpha}{N}\sum_{i=1}^N \mathds{I}(\tau_i\geq   t);M_t,\{M_{t,L}^j(\theta)\}_{j=1}^B).
 \end{align*}
  Thus,  we obtain the following coverage guarantee as an immediate corollary of 
\Cref{thm:coverage-comp-post}.

 \begin{corollary}
 $\mathcal{C}^\prime_1$ and $\mathcal{C}^\prime_2$ defined in \eqref{set-gaussian-known-pre} and \eqref{set-eff-known-pre-pfa} respectively, satisfy the same coverage guarantees as $\mathcal{C}_1$ and $\mathcal{C}_2$ in \Cref{thm:coverage-comp-post}.
 %for some $\alpha,\beta\in(0,1)$, $N,B\in\mathbb N$,  $L\in\N \cup \{\infty\}$. Then, for any given change detection algorithm $\mathcal{A}$, the following conditional coverage guarantee holds:
    %\begin{equation*}
    %  \P_{\theta_0,T,\theta_1}(T\in \mathcal{C}^\prime_1\mid \tau\geq T)\geq 1-\alpha-\beta,
    %\end{equation*}  
    %for any $T \in \N, \theta_1\in\Theta_1$, when $\mathbb \P_\infty(\tau\geq T)\neq 0$.     If $\mathcal{A}$ controls PFA at level $\delta\in(0,1)$, we have the following  coverage guarantees for $\mathcal{C}^\prime_2$:
    %\begin{equation*}
      %\P_{\theta_0,T,\theta_1}(T\in \mathcal{C}^\prime_2)\geq 1-\alpha-\beta-\delta,  ~\text{ and }~ \P_{\theta_0,T,\theta_1}(T\in \mathcal{C}^\prime_2\mid \tau\geq T)\geq 1-\frac{\alpha+\beta}{1-\delta}, \text{ when }\mathbb \P_{\theta,\infty}(\tau\geq T)\neq 0,
    %\end{equation*} 
    %for any $ \theta_1\in\Theta_1$.

     \end{corollary}

     In the next subsection, we present a concrete example of the Gaussian mean shift problem to clarify our method. We illustrate another example with Poisson distribution in \Cref{sec:a-pois}.
     
\subsection{A concrete example: Gaussian mean shift problem}
\label{sec:gaussian-example-comp-post}

Let the pre-change distribution $F_0$  be $N(\theta_0,1)$ and the post-change model be $\mathcal{P}_1=\{N(\theta,1): \theta\in\Theta_1\}$.

%Now note that the MLE of $\theta_1$ is $\hat \theta_{1,T:\tau}=\bar{X}_{T:\tau}=\theta_1+\bar{\epsilon}_{T:\tau}$. So, for $T\leq \tau, L_{T}^\tau(\hat \theta_{1,T:\tau}; X_1,\cdots, X_)$ 


In this setting, we have the following example of a $1-\beta$ confidence sequence \citep{howard2021time} for $\theta_1$
\begin{equation}
\label{eq:conf-seq-gaussian}
    \mathcal{S}^\prime_{\tau-t+1}=\left[\bar{X}_{t:\tau}-\frac{s_{\tau-t+1}(\beta)}{\sqrt{\tau-t+1}},\bar{X}_{t:\tau}+\frac{s_{\tau-t+1}(\beta)}{\sqrt{{\tau-t+1}}}\right]\cap \Theta_1,
\end{equation}
where $\bar{X}_{t:\tau}=\frac{1}{\tau-t+1}\sum_{i=t}^\tau X_i$ and $s_n(\beta)=\sqrt{\log\log (2n)+0.72 \log(10.4/\beta)}$.


We fix $t\in\{1,\cdots,\tau\}$. Note that we can consider $\theta^{\prime\prime}=\theta_0$ and $G(x,\theta)=x+\theta-\theta_0$. Therefore, only need to generate $\{X_n^j\}_{n\in\N}\stackrel{iid}{\sim}N(\theta_0,1)$ for $j=1,\cdots,B$. For $\theta\in\mathcal{S}^\prime_{\tau-t+1}$, 
\begin{equation}
    Y_n^t(X_n^j,\theta):=\begin{cases}
        \epsilon_n^j+\theta_0, \text{ if } n<t,\\
        \epsilon_n^j+\theta, \text{ otherwise, }\end{cases}
\end{equation}
where $\epsilon_n^j:=X_n^j-\theta_0\sim N(0,1)$. We consider the likelihood-ratio-based test statistic $M_{t}(X_1,\cdots,X_\tau)$, as defined in \eqref{eq:test-stat-comp-post}, with $\hat \theta_{1,t:n}=\frac{1}{ n-t+1}\sum_{k=t}^{n}X_k$.
Let, $\tau^\theta_j$ is the detection time for the sequence $\{
Y_n^t(X_n^j,\theta)\}_n$ and $M_{t,\infty}^j(\theta)=M_t(Y_1^t(X_1^j,\theta),\cdots,Y_{\tau^\theta_j}^t(X_{\tau^\theta_j}^j,\theta))$. Suppose, we can find $t_1^j,t_2^j\in\N$, such that $t_1^j=\inf_{\theta\in \mathcal{S}^\prime_{\tau-t+1}}\tau^\theta_j$ and $t_2^j=\sup_{\theta\in \mathcal{S}^\prime_{\tau-t+1}}\tau^\theta_j$ (these are easy to compute for specific detection algorithms, such as CUSUM or SR type detectors, as we explain in \Cref{max-min-tau}) and they are finite. So, we can have $L=\infty$. 

%So, $M_t^j(\theta)\leq \max_{t_1^j\leq  t^\prime\leq t_2^j}M_t(Y_1^j(\theta),\cdots,Y_{t^\prime}^j(\theta);  t^\prime)$. Therefore, 
If $t_2^j< t$, we have $V_{t,\infty}^j=0$.
For $t_2^j\geq t$, it can be simplified in our setting as below:
\begin{align*}
\label{eq:vt-gaussian}
   V_{t,\infty}^j&=\max_{\max\{t,t_1^j\}\leq  t^\prime\leq t_2^j}\sup_{\theta\in \mathcal{S}^\prime_{\tau-t+1}}\max_{1\leq i\leq   t^\prime} \frac{L_{i}\left(\frac{\sum_{k=i}^{t^\prime}Y_k^t(X_k^j,\theta)}{  t^\prime-i+1};Y_1^t(X_1^j,\theta),\cdots,Y_{t^\prime}^t(X_{t^\prime}^j,\theta)\right)}{L_t\left(\frac{\sum_{k=t}^{t^\prime}\epsilon_k^j}{  t^\prime-t+1}+\theta;Y_1^t(X_1^j,\theta),\cdots,Y_{t^\prime}^t(X_{t^\prime}^j,\theta)\right)}\\
    \nonumber&=\max_{\max\{t,t_1^j\}\leq  t^\prime\leq t_2^j} \frac{\max_{1\leq i\leq   t^\prime}\sup_{\theta\in \mathcal{S}^\prime_{\tau-t+1}}L_{i}(\bar Y_{i:  t^\prime}^j(\theta);Y_1^t(X_1^j,\theta),\cdots,Y_{t^\prime}^t(X_{t^\prime}^j,\theta))}{L_t(\bar\epsilon_{t:  t^\prime}^j;\epsilon_1^j,\cdots,\epsilon_{t^\prime}^j)}\\
     &=\max_{\max\{t,t_1^j\}\leq  t^\prime\leq t_2^j} \frac{\max_{1\leq i\leq   t^\prime}L_{i}(\bar Y_{i:  t^\prime}^j(\theta_{t^\prime}^i);Y_1^t(X_1^j,\theta_{t^\prime}^i),\cdots,Y_{t^\prime}^t(X_{t^\prime}^j,\theta_{t^\prime}^i))}{L_t(\bar\epsilon_{t:  t^\prime}^j;\epsilon_1^j,\cdots,\epsilon_{t^\prime}^j)},
\end{align*}
where $\bar\epsilon_{i:  t^\prime}^j=\frac{1}{  t^\prime-t+1}\sum_{k=t}^{t^\prime}\epsilon_k^j,\bar Y_{i:  t^\prime}^j(\theta) = \frac{1}{  t^\prime-i+1}\sum_{k=i}^{t^\prime}Y_k^t(X_k^j,\theta),$ and $\theta_{t^\prime}^i$ is the unique element that maximizes
$L_{i}\left(\bar Y_{i:  t^\prime}^j(\theta);Y_1^t(X_1^j,\theta),\cdots,Y_{t^\prime}^t(X_{t^\prime}^j,\theta)\right)$, or equivalently minimizes
\begin{align*}
    &-2\log L_{i}\left(\bar Y_{i:  t^\prime}^j(\theta);Y_1^t(X_1^j,\theta),\cdots,Y_{t^\prime}^t(X_{t^\prime}^j,\theta)\right)=\\
    &\begin{cases}
        \displaystyle\sum_{k=i}^{t-1}\left(\epsilon_k^j-\bar\epsilon_{i:  t^\prime}^j-\frac{  t^\prime-t+1}{  t^\prime-i+1}(\theta-\theta_0)\right)^2+\displaystyle\sum_{k=t}^{t^\prime}\left(\epsilon_k^j-\bar\epsilon_{i:  t^\prime}^j+\frac{(t-i)(\theta-\theta_0)}{  t^\prime-i+1}\right)^2+\text{constant},
        \text{ if } i< t,\\
        \text{constant} , \text{ if } i=t,\\
        \displaystyle\sum_{k=t}^{i-1}\left(\epsilon_k^j+\theta-\theta_0\right)^2+\text{ constant}, \text{ if } i> t,
    \end{cases}
\end{align*}
over 
$\theta\in \mathcal{S}^\prime_{\tau-t+1}$. Here, constant refers to the terms that do not involve $\theta$.
Note that $\theta_{t^\prime}^i$ has a simple closed-form expression since the above equation is a quadratic function of $\theta$. And hence, $V_{t,\infty}^j$ can be computed easily. 
 
 \begin{remark}
\label{max-min-tau}     
For weighted CUSUM or SR detectors, finding $t_1^j, t_2^j$ is often straightforward. For some weight $W(\theta)$ such that $\int_{\theta\in\Theta_1} dW(\theta)=1$, these are defined as
\begin{equation}
\label{eq:wcusum}
    \tau_{\text{wcusum}}=\inf\left\{\tau\in\mathbb N:\max_{1\leq j\leq \tau}\int_{\theta\in\Theta_1}\prod_{i=j}^\tau\frac{f_{\theta}(X_i)}{f_{\theta_0}(X_i)}dW(\theta)\geq A\right\}
\end{equation}
\begin{equation}
\label{eq:wsr}
 \text{ and }~   \tau_{\text{wsr}}=\inf\left\{\tau\in\mathbb N:\sum_{j=1}^\tau\int_{\theta\in\Theta_1}\prod_{i=j}^\tau\frac{f_{\theta}(X_i)}{f_{\theta_0}(X_i)}dW(\theta)\geq A\right\}.
\end{equation}
For example, consider $\mathcal{P}_1=\{N(\theta,1):\theta\geq \theta^*\}$, for some $\theta^*\geq \theta_0$.
Observe that for all $\theta\in \Theta_1$, $\prod_{i=j}^\tau\frac{f_{\theta}(X_i)}{f_{\theta_0}(X_i)}=\exp\{\frac{1}{2}(\theta-\theta_0)\times\sum_{i=j}^\tau(2X_i-\theta_0-\theta)\}$ increases as $X_i$'s increases and hence, both $\tau_{\text{wcusum}}$ and $\tau_{\text{wsr}}$ decreases as $X_i$'s increases. 
Therefore, $t_1^j=\inf_{\theta\in \mathcal{S}^\prime_{\tau-t+1}}\tau^\theta_j=\tau^{\sup \mathcal{S}^\prime_{\tau-t+1}}_j$ and $t_2^j=\sup_{\theta\in \mathcal{S}^\prime_{\tau-t+1}}\tau^\theta_j=\tau^{\inf \mathcal{S}^\prime_{\tau-t+1}}_j$. 
Similarly, for $\mathcal{P}_1=\{N(\theta,1):\theta\leq \theta^*\}$, for some $\theta^*\leq \theta_0$, we have $t_1^j=\tau^{\inf \mathcal{S}^\prime_{\tau-t+1}}_j$ and $t_2^j=\tau^{\sup \mathcal{S}^\prime_{\tau-t+1}}_j$. 
Also, for $\mathcal{P}_1=\{N(\theta,1):\theta\neq \theta_0\}$, if we employ Gaussian weights (it is conjugate prior, so closed-form expressions for $\tau_{\text{wcusum}}$ and $\tau_{\text{wsr}}$ can be found), one can obtain similar forms of $t_1^j,t_2^j$, depending on the mean of the weight distribution. One can check that similar results extend to other parametric families, such as the Gaussian/Laplace/exponential scale change problems with weighted CUSUM and SR detectors, as well as the PFA-controlling change detectors used in \Cref{sec:expt-pfa}.
 \end{remark}

 The Gaussian mean shift example we provided serves as a concrete and illustrative case to clarify the methodology. It is worth noting that $C^\prime$, as defined in \eqref{set-gaussian-known-pre}, is practically implementable as long as the following relatively weak conditions are met: 
 \begin{enumerate}
     \item The coupling function $G$ is available (which is always the case when distributions are continuous and in the case of discrete distributions, even if a suitable 
$G$ is not directly available, our method may still be implementable, as demonstrated in \Cref{sec:pois-example-comp-post} for the Poisson distribution).
     \item  We can construct a confidence sequence for the parameter of interest (which is always achievable as long as one can construct a confidence interval for the parameter).  
     \item The quantities $t_j^1$ and $t_j^2$ are computable (either exactly or numerically). 
     \item If we employ the likelihood-ratio-based test statistic \eqref{eq:test-stat-comp-post}, then we must be able to maximize  $L_i(\hat{\theta}_{1,i:t^\prime};Y_1^t(X_1^j,\theta),\cdots,Y_{t^\prime}^t(X_{t^\prime}^j,\theta))$ over $\theta$ on a given set (either exactly or numerically).
 \end{enumerate}
For instance, one can easily verify that this approach applies to Gaussian, Laplace, or exponential scale change problems. 



\section{Composite pre- and post-change}
\label{sec:composite}
We now assume that both pre- and post-change distributions follow some parametric composite models $\mathcal{P}_0=\{F_{\theta}:\theta\in\Theta_0\}$ and $\mathcal{P}_1=\{F_{\theta}:\theta\in\Theta_1\}$ respectively such that $\Theta_0\cap\Theta_1= \emptyset$. 

In this setting, the average run length (ARL) of the detection algorithm is defined as $\inf_{\theta\in\Theta_0}\mathbb E_{\theta,\infty}[\tau]$, which represents the least expected time until a false alarm occurs under $T=\infty$ (i.e., no change). On the other hand, the probability of false alarm (PFA) is defined as the worst-case probability of triggering a false alarm, that is $\sup_{\theta\in\Theta_0}\P_{\theta,\infty} (\tau < \infty)$.

Suppose we have the data sequence $\{X_n\}_n\sim\P_{\theta_0,T,\theta_1}$, for some unknown $T \in \N \cup \{\infty\}, \theta_i\in\Theta_i (i=0,1)$ and some sequential change detection algorithm $\mathcal{A}$ that raises an alarm at the stopping time $\tau$ in the sequence.
Following the approach from the previous sections, we define the null hypothesis that a changepoint occurs at time $t\in\{1,\cdots,\tau\}$ as:
\[
\mathcal H_{0,t}: X_1,\cdots,X_{t-1}\stackrel{i.i.d}{\sim}F_{\theta_0}, \text{ for some }  \theta_0\in \Theta_0 \text{ and }X_{t},X_{t+1},\cdots\stackrel{i.i.d}{\sim}F_{\theta_1}, \text{ for some }  {\theta_1}\in \Theta_1. 
\]

We consider some test statistic $M_t:=M_t(X_1,\cdots,X_\tau;\tau)$ based on data up to time $\tau,$ for testing the null hypothesis $\mathcal H_{0,t}$. An example is provided in \Cref{sec:test-stat-comp}.

    Suppose the change detection algorithm, $\mathcal{A}$ has finite ARL or it is a blackbox algorithm so that its PFA is unknown. In that case, we have a problem in generating i.i.d. copies of the detection times under $T=\infty$ (i.e., no change), since the pre-change parameter $\theta_0$ is unknown. Therefore, we need to assume the following:
\begin{assumption}
\label{assmp-1}
   There exists  $ \theta_0^*\in\Theta$ such that $\inf_{\theta\in \Theta_0}\P_{\theta,\infty}(\tau\geq t)\geq \P_{\theta_0^*,\infty}(\tau\geq t)$, for all $t\in\N$. 
\end{assumption}
This assumption holds for a variety of change detection problems. For instance, we illustrate this in \Cref{rem:assumption} for some Gaussian mean change problems with CUSUM and SR-type detectors. One can easily check that it holds for analogous Gaussian, Laplace, and exponential scale change problems as well.

We now generate $N$ many i.i.d. sequences under $F_{\theta_0^*}$ until a change is detected using $\mathcal{A}$, recording the detection times as $\tau_1,\cdots,\tau_N$, for some $N\in \N$, that will help us estimate $\P_{\theta_0^*,\infty}(\tau\geq t)$, which is a lower bound on $\P_{\theta_0,\infty}(\tau\geq t)$. If $\mathcal{A}$ controls PFA at a known level $\delta\in(0,1)$, we can avoid this step and still obtain conditional or unconditional coverage.

Since both pre- and post-change parameters are unknown, for each $t$, we estimate them based on $X_1,\cdots,X_{t-1}$ and $X_t,\cdots,X_\tau$ respectively. Instead of point estimates, we rely on interval estimates in order to achieve a predetermined control over the error.


Suppose that we have some confidence interval procedure  $\operatorname{CI}(X_1,\cdots,X_n;1-c)$, which constructs $1-c$ confidence interval using $X_1,\cdots,X_n$ for $\theta_0$ and a confidence sequence procedure $\{\operatorname{CS}(X_1,\cdots,X_n;1-c)\}_n$, which constructs $1-c$ confidence sequence using $\{X_n\}_n$ for $\theta_1$. If $\mathcal{A}$ controls PFA, we can simply construct $\mathcal{S}^\prime_{\tau-t+1}=\operatorname{CS}(X_t,\cdots,X_{\tau};1-\beta)$ for $\theta_1$ having $1-\beta$ coverage and $\mathcal{S}_{t-1}=\operatorname{CI}(X_1,\cdots,X_{t-1};1-\gamma)$ for $\theta_0$ having $1-\gamma$ coverage. Otherwise, we construct $\mathcal{S}_{t-1}=\operatorname{CI}(X_1,\cdots,X_{t-1};1-\gamma r_t)$ and $\mathcal{S}^\prime_{\tau-t+1}=\operatorname{CS}(X_t,\cdots,X_{\tau};1-\beta r_t)$, where 
$r_{t}:=\sum_{i=1}^N \mathds{I}(\tau_i\geq 
 t)/N$. Despite $r_t$ being random, it is based on independent simulations, and so it turns out that marginalizing over $r_t$ yields that the CS and CI have coverage probabilities $1-\beta\times\P_{\theta_0^*,\infty}(\tau\geq t)$ and $1-\gamma\times\P_{\theta_0^*,\infty}(\tau\geq t)$ respectively; we formalize this more explicitly in the proof of \Cref{thm:coverage-comp}.
 
 
 Note that for $t=1$, the only plausible confidence interval of $\theta_0$ is $\mathcal{S}_0=\Theta_0$. Therefore, if $\Theta_0$ is unbounded, then it becomes infeasible.  To avoid this issue, one can assume that $T\in\N\setminus\{1\}$ and test only for $t=2,\cdots,\tau.$


%For each fixed $t$, we construct a confidence interval $\mathcal{S}_{t-1}$ for $\theta_0$ with coverage $1-\beta \times \P_{\theta_0^*,\infty}(\tau\geq t)$. However, as $\tau$ depends on the data, we need to construct a confidence sequence $\{\mathcal{S}^\prime_{n}\}_n$ for $\theta_1$ having $1-\beta \times \P_{\theta_0^*,\infty}(\tau\geq t)$ coverage, as described in the previous section.

For each $j=1,\cdots,B$, for some fixed $B\in\N$, suppose we have sequences $\{X^j_n(\theta,t,\theta^\prime)\}_n$ having joint distribution as $\P_{\theta,t,\theta^\prime}$. Note that the sequences must be independent as $j$ varies. However, for any fixed $j$, the i.i.d. sequences $\{X^j_n(\theta,t,\theta^\prime)\}_n$ are allowed to be dependent across different values of $t$ and  $\theta\in\mathcal{S}_{t-1},\theta\in\mathcal{S}^\prime_{\tau-t+1}$.  A specific method for generating these sequences is detailed in \Cref{sec:subroutines-comp}. We draw the datapoints until a change is detected at the stopping time $\tau^{j,t}_{\theta,\theta^\prime}$ using $\mathcal{A}$ or until time $L$, whichever occurs first. Then, we compute the same test statistics truncated at $L$, based on $\tau^{j,t}_{\theta,\theta^\prime}$ and $X^j_1(\theta,t,\theta^\prime),\cdots,X^j_{\tau^{j,t}_{\theta,\theta^\prime}\wedge L}(\theta,t,\theta^\prime)$, which is denoted as $M^j_{t,L}(\theta,\theta^\prime)$.
For $L<\infty$,
\begin{equation}
\label{eq:Mt-L-comp}
    M^j_{t,L}(\theta,\theta^\prime) := \begin{cases}
        -\infty, &\text{if }t> \tau^{j,t}_{\theta,\theta^\prime},\\
        M_t(X_1^j(\theta,t,\theta^\prime),\cdots,X_{\tau^{j,t}_{\theta,\theta^\prime}}^j(\theta,t,\theta^\prime)), &\text{if } t\leq\tau^{j,t}_{\theta,\theta^\prime}\leq L,\\
        \infty, &\text{if } t\leq\tau^{j,t}_{\theta,\theta^\prime} \text{ and }\tau^{j,t}_{\theta,\theta^\prime} > L.\\
    \end{cases}
\end{equation}
And for $L=\infty$,
\begin{equation}
\label{eq:Mt-infty-comp}
    M^j_{t,\infty}(\theta,\theta^\prime) := \begin{cases}
M_t(X_1^j(\theta,t,\theta^\prime),\cdots,X_{\tau^{j,t}_{\theta,\theta^\prime}}^j(\theta,t,\theta^\prime)), &\text{if } t\leq\tau^{j,t}_{\theta,\theta^\prime}<\infty,\\
        -\infty, &\text{if } \tau^{j,t}_{\theta,\theta^\prime} =\infty  \text{ or }t>\tau^{j,t}_{\theta,\theta^\prime}.\\
    \end{cases}
\end{equation}
%A specific method for generating these sequences is detailed in \Cref{sec:subroutines-comp}. Let $\tau_{j,t}^\theta$ be the detection time for the sequence $\{X^j_n(\theta_0,t,\theta)\}_n$ using the same detection algorithm $\mathcal{A}$.
%Define,$$M_t^j(\theta,\theta^\prime)=M_t\left(X^j_1(\theta,t,\theta^\prime),\cdots,X^j_{\tau^{j,t}_{\theta,\theta^\prime}}(\theta,t,\theta^\prime);\tau^{j,t}_{\theta,\theta^\prime}\right).$$
 Our confidence set is the set of $t \leq \tau$ for which we fail to reject $\mathcal H_{0,t}$ at level $\alpha\times \P_{\theta_0^*,\infty}(\tau\geq t)$, 
\begin{equation}
\label{eq:conf-comp}
    \mathcal{C}_1=\left\{t\in \N:t\leq \tau, M_t\leq\sup_{(\theta, \theta^\prime)\in \mathcal{S}_{t-1}\times \mathcal{S}^\prime_{\tau-t+1}}\operatorname{Quantile}(1-\hat\alpha_{t};M_t,\{M_{t,L}^j(\theta,\theta^\prime)\}_{j=1}^B)\right\},
\end{equation}
 where 
$\hat\alpha_{t}:=\alpha\times\sum_{i=1}^N \mathds{I}(\tau_i\geq 
 t)/N$. If $\mathcal{A}$ controls PFA at some known level $\delta\in(0,1)$ , we have the following confidence set, which is simpler (i.e., does not require $\tau_i$'s):
 \begin{equation}
\label{eq:conf-comp-pfa}
    \mathcal{C}_2=\left\{t\in \N:t\leq \tau, M_t\leq\sup_{(\theta, \theta^\prime)\in \mathcal{S}_{t-1}\times \mathcal{S}^\prime_{\tau-t+1}}\operatorname{Quantile}(1-\alpha;M_t,\{M_{t,L}^j(\theta,\theta^\prime)\}_{j=1}^B)\right\}.
\end{equation}
Algorithm \ref{algo:comp} contains an overview.
As before, even if $\mathcal{S}_{t-1}$ and $\mathcal{S}^\prime_{\tau-t+1}$ are uncountable, one can approximate the threshold $\sup_{(\theta, \theta^\prime)\in \mathcal{S}_{t-1}\times \mathcal{S}^\prime_{\tau-t+1}}\operatorname{Quantile}(1-\hat\alpha_{t};M_t,\{M_{t,L}^j(\theta, \theta^\prime)\}_{j=1}^B)$ by discretizing the parameter space into a grid and evaluating the quantile for values of 
$(\theta, \theta^\prime)$ on this grid, taking their maximum. Additionally, in many cases, we can derive a suitable upper bound of this threshold, which is computationally, statistically efficient, and exact (i.e., does not require any approximation). We provide the details in \Cref{sec:subroutines-comp}. Additionally, we illustrate concrete examples of the Gaussian mean shift problem in \Cref{sec:gaussian-example-comp} and the Poisson rate change problem in \Cref{sec:a-pois}.


\begin{algorithm}[h!]
\SetKwData{Left}{left}\SetKwData{This}{this}\SetKwData{Up}{up}
\SetKwFunction{Union}{Union}\SetKwFunction{FindCompress}{FindCompress}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
%\DontPrintSemicolon
\SetAlgoLined
%\begin{algorithmic}[1]
\KwIn{ $\theta_0^*, \alpha, \beta,\gamma\in(0,1)$, $N,B\in\N,L\in\N\cup\{\infty\}$, change detection algorithm $\mathcal{A}$, a confidence interval procedure  $\operatorname{CI(data;coverage)},$ a confidence sequence procedure $\operatorname{CS(data;coverage)}$, and data $\{X_n\}_{n}$ until a change is detected at $\tau$ using $\mathcal{A}$}
\KwOut{ A confidence set $\mathcal{C}$ for changepoint $T$}
%Detect change at $\tau$ using $\mathcal{A}$\\
$r_{1}=r_{2}=\cdots=r_{\tau}=1$\\
%Compute the point estimator $\hat{T}$ for $T$, as defined in \eqref{eq:known-pre-unknown-post}\\
%Obtain an estimate $\hat F_1$ of the (unknown) post-change $F_1\in\mathcal{P}_1$ based on $X_{\hat{T}},\cdots,X_{\tau}$\\
\If{$\mathcal{A}$ has finite ARL}{
\For{$i=1,\cdots,N$}{
  Draw $\{X_n^i\}_n\stackrel{iid}{\sim} F_{\theta_0^*}$, define $\tau_i$ as the time when $\mathcal{A}$ detects a change, or $\tau$, whichever is first\\
}
% \For{$t=2,\cdots,\tau$}{
%   $r_{t}=\sum_{i=1}^N \mathds{I}(\tau_i\geq 
%  t)/N$
%  }
 Define $r_t = \sum_{i=1}^N \mathds{I}(\tau_i\geq 
 t)/N$ for $t=1,\dots,\tau$;
}
$\mathcal{C}= \varnothing$\\

\For{$t=1,\cdots,\tau$}{
Compute some test statistics $M_t$ based on $X_1,\cdots,X_\tau$ (e.g., as defined in \eqref{eq:test-stat-comp-simp})\\
 Find a confidence interval for $\theta_0$: $\mathcal{S}_{t-1}=\operatorname{CI}(X_1,\cdots,X_{t-1};1-\gamma r_t)$\\
Find a confidence sequence or $\theta_1$: $\mathcal{S}^\prime_{\tau-t+1}=\operatorname{CS}(X_t,\cdots,X_{\tau};1-\beta r_t)$  \\
\For{$j=1,\cdots,B$}{
  %Find detection time $\tau^*$ in the sequence $\{X^j_n\}_n$ using $\mathcal{A}$\\
\underline{Subroutine 1}:
\For{$(\theta,\theta^\prime)\in \mathcal{S}_{t-1}\times \mathcal{S}^\prime_{\tau-t+1}$}{
 Simulate a sequence $\{X^j_n(\theta,t,\theta^\prime)\}_n$ following $\P_{\theta,t,\theta^\prime}$ (independently for all $j$) until its detection time $\tau^{j,t}_{\theta,\theta^\prime}$ using $\mathcal{A}$ or until time $L$, whichever happens first.\\
Compute $M_{t,L}^j(\theta,\theta^\prime)$ based on $L,\tau^{j,t}_{\theta,\theta^\prime}$ and $X^j_1(\theta,t,\theta^\prime),\cdots,X^j_{\tau^{j,t}_{\theta,\theta^\prime}\wedge L}(\theta,t,\theta^\prime)$\\
}}
\underline{Subroutine 2}: \If{$M_t\leq\sup_{(\theta, \theta^\prime)\in \mathcal{S}_{t-1}\times \mathcal{S}^\prime_{\tau-t+1}}\operatorname{Quantile}(1-\alpha r_{t};M_t,\{M_{t,L}^j(\theta,\theta^\prime)\}_{j=1}^B)$}{$\mathcal{C}=\mathcal{C}\cup \{t\}$}
}
%\BlankLine
%\end{algorithmic}
\caption{CI for composite pre-change and post-change}
\label{algo:comp}
\end{algorithm}




\begin{theorem}
\label{thm:coverage-comp} 
   Let $\mathcal{C}_1$ and $\mathcal{C}_2$ be the confidence sets for changepoint $T\in\N$ defined in \eqref{eq:conf-comp} and \eqref{eq:conf-comp-pfa}, for some $\alpha,\beta,\gamma\in(0,1)$, $N,B\in\mathbb N$, $L\in\N \cup \{\infty\}$. For any $\theta_i\in\Theta_i (i=0,1$), and any change detection algorithm $\mathcal{A}$ satisfying \Cref{assmp-1} and  having $\mathbb \P_\infty(\tau\geq T)\neq 0$, the following conditional coverage guarantee holds:
    \begin{equation*}
      \P_{\theta_0,T,\theta_1}(T\in \mathcal{C}_1\mid \tau\geq T)\geq 1-\alpha-\beta-\gamma.
    \end{equation*}  
      If $\mathcal{A}$ controls PFA at level $\delta\in(0,1)$, we have that for any $\theta_i\in\Theta_i (i=0,1)$:
    \begin{equation*}
      \P_{\theta_0,T,\theta_1}(T\in \mathcal{C}_2)\geq 1-\alpha-\beta-\gamma-\delta, ~\text{ and }~ \P_{\theta_0,T,\theta_1}(T\in \mathcal{C}_2\mid \tau\geq T)\geq 1-\frac{\alpha+\beta+\gamma}{1-\delta}
    \end{equation*} 
    $\text{ when }\mathbb \P_{\theta,\infty}(\tau\geq T)\neq 0.$
\end{theorem}

%\begin{remark}    We can easily construct $1-\beta\times\P_{\theta_0^*,\infty}(\tau\geq t)$ confidence intervals/sequences in lines 8 and 9 using $\tau_1,\cdots,\tau_N$ computed in lines 2 to 4. To elaborate, we construct a confidence interval with $1-\beta\times \times\sum_{i=1}^N \mathds{I}(\tau_i\geq t)/N$ conditional coverage given $\tau_1,\cdots,\tau_N$. It is easy to verify that this approach would provide $1-\beta\times\P_{\theta_0^*,\infty}(\tau\geq t)$ (unconditional) coverage.\end{remark}
%\begin{remark}Since $\mathcal S$ and $\mathcal S^\prime$ are typically uncountable sets, the practical implementation of lines 10â17 necessitates discretizing these confidence sets by dividing the intervals within the sets into grids. By adjusting the grid spacing, any point within the sets can be approximated with arbitrary precision.\\\end{remark}
\begin{remark}
    As a special case, when pre- and post-change parameters are known, i.e., $\Theta_i=\{\theta_i\}$ are singleton sets for $i=0,1$, we can have $\mathcal S=\{\theta_0\},\mathcal S^\prime=\{\theta_1\}$ with $\beta=0$. So, in this case, \Cref{algo:comp,algo:comp-post} reduces to our algorithm designed for the known pre- and post-change setting and similarly, \Cref{thm:coverage-comp-post,thm:coverage-comp} reduces to \Cref{thm:exact-cond-coverage}.
\end{remark}

\subsection{Prototypical choice of test statistic \texorpdfstring{$M_t$}{Lg}}
\label{sec:test-stat-comp}
Let $L_t^n(\theta_0,\theta_1)$ be the likelihood of data up to time $n\geq t$ under $\mathcal H_{0,t}$. Then,
\begin{align}
    L_t(\theta_0,\theta_1;X_1,\cdots,X_n)=\prod_{i=1}^{t-1} f_{\theta_0}(X_i)\prod_{i=t}^n f_{\theta_1}(X_i).
\end{align}
We define $\hat \theta_{0,1:j}$ to be the maximum likelihood estimator (MLE) of the null $\theta_0\in \Theta_0$ based on data $X_1,\cdots,X_{j}$. Similarly, we define $\hat \theta_{1,j:n}$ to be the MLE of the alternative $\theta_1\in \Theta_1$ based on data $X_j,\cdots,X_n$. The point estimate of $T$ can be naturally extended to the following form:
\begin{equation}
\label{eq:unknown-pre-post}
    \hat T=\argmax_{1\leq j\leq \tau}\prod_{i=j}^\tau\frac{ f_{\hat\theta_{1,j:\tau}}(X_i)}{f_{\hat\theta_{0,1:j-1}}(X_i)}.
\end{equation}
As a prototypical example, we consider the likelihood-ratio-based test statistic $M_t^{(n)}$, which is defined for $t,n\in\N,t\leq n$ as below.
    \begin{equation}
    %\label{eq:test-stat-comp}
        M_t^{(n)}(X_1,\cdots,X_n):=
            \displaystyle\max_{1\leq i\leq n} \frac{L_{i}(\hat \theta_{0,1:i-1},\hat \theta_{1,i:n};X_1,\cdots,X_n)}{L_t(\hat \theta_{0,1:t-1},\hat \theta_{1,t:n};X_1,\cdots,X_n)}.
    \end{equation}
Note that the test statistic $M_t$ for testing the null hypothesis $\mathcal H_{0,t}$ reduces to
     \begin{equation}
    \label{eq:test-stat-comp-simp}
        M_t=\begin{cases}
             \frac{L_{\hat{T}}(\hat \theta_{0,1:\hat{T}-1},\hat \theta_{1,\hat{T}:\tau};X_1,\cdots,X_\tau)}{L_t(\hat \theta_{0,1:t-1},\hat \theta_{1,t:\tau};X_1,\cdots,X_\tau)},&\text{if } t\leq\tau<\infty,\\
-\infty, &\text{if }t>\tau\text{ or }\tau =\infty.\\
        \end{cases}
    \end{equation}


\subsection{Computationally and statistically efficient subroutines}
\label{sec:subroutines-comp}
\begin{comment}
\subsubsection{Subroutines 1 and 2}
Although it might be challenging to compute $\P_{\theta_0^*,\infty}(\tau\geq t)$, 
 %one can easily construct a $1-\beta \times \P_{\theta_0,\infty}(\tau\geq t)$ confidence sequence using $\tau_1,\cdots,\tau_N$ that we computed in line 3 of \Cref{algo:comp-post}. Note for each fixed $t$, 
one can construct a confidence interval $\mathcal{S}_{t-1}$ for $\theta_0$ with conditional coverage $1-\gamma \times \frac{1}{N}\sum_{i=1}^N\mathds{I}(\tau_i\geq t)$, conditioned on $\tau_1,\cdots,\tau_N$, i.e.,
\begin{equation*}
    \P_{\theta_0,t,\theta_1}(\theta_0\notin\mathcal{S}_{t-1}(X_1,\cdots,X_{t-1})\mid \tau_1,\cdots,\tau_N)\leq\gamma \times \frac{1}{N}\sum_{i=1}^N\mathds{I}(\tau_i\geq t),
\end{equation*}
which would imply the (unconditional) coverage of $1-\gamma \times \P_{\theta_0^*,\infty}(\tau\geq t)$:
\begin{equation*}
    \P_{\theta_0,t,\theta_1}(\theta_0\notin\mathcal{S}_{t-1}(X_1,\cdots,X_{t-1}))=\mathbb E \left(\P_{\theta_0,t,\theta_1}(\theta_0\notin\mathcal{S}_{t-1}(X_1,\cdots,X_{t-1})\mid \tau_1,\cdots,\tau_N)\right)\leq\gamma \times \P_{\theta_0^*,\infty}(\tau\geq t).
\end{equation*}
Similarly, we construct a confidence sequence $\{\mathcal{S}^\prime_{n}\}_n$ for $\theta_1$ having $1-\beta \times \P_{\theta_0^*,\infty}(\tau\geq t)$ coverage, using $\tau_1,\cdots,\tau_N$, that we computed in line 3 of \Cref{algo:comp}.
\end{comment}
%\subsubsection{Subroutines 3 and 4}
As in \Cref{sec:subroutines-comp-post}, we assume that we have a fixed element $\theta^{\prime\prime}$ in the parameter space and a function $G$, such that $X\sim F_{\theta^{\prime\prime}}$ implies $G(X,\theta)\sim F_\theta, \forall \theta\in\Theta_0\cup\Theta_1$. Therefore, for each $j=1,\cdots,B$, we generate data sequence $X_1^j,X_2^j\cdots\stackrel{iid}{\sim}F_{\theta^{\prime\prime}}$ only once. Then, for each $t\in\{1,\cdots,\tau\}$ and $(\theta\times\theta^\prime)\in\mathcal{S}_{t-1}\times\mathcal{S}^\prime_{\tau-t+1}$, we obtain sequences $\{Y_n^t(X_n^j,\theta,\theta^\prime)\}_n$ having joint distribution $\P_{\theta,t,\theta^\prime},$ by applying the function $G(.,\theta)$ to $X_{1}^j,\cdots,X_{t-1}^j$ and $G(.,\theta^\prime)$ to $X_{t}^j,X_{t+1}^j,\cdots$, i.e.,
\begin{equation}
    Y_n^t(x,\theta,\theta^\prime):=\begin{cases}
        G(x,\theta), &\text{if } n<t\\
        G(x,\theta^\prime), &\text{ otherwise. }
    \end{cases}
\end{equation}
Note that, even if a suitable 
$G$ is not directly available, generating such sequences $\{Y_n^t(x,\theta,\theta^\prime)\}_n$ across all $(\theta\times\theta^\prime)\in\mathcal{S}_{t-1}\times\mathcal{S}^\prime_{\tau-t+1}$ may still be possible, as demonstrated in \Cref{sec:pois-example-comp} for the Poisson distribution.

Let us now fix $t\in\{1,\cdots,\tau\}$.
Computing the detection time $\tau^j_{\theta,\theta^\prime}$ for each $(\theta, \theta^\prime)\in \mathcal{S}_{t-1}\times \mathcal{S}^\prime_{\tau-t+1}$, and finding whether $\text{there exists }(\theta, \theta^\prime)\in \mathcal{S}_{t-1}\times \mathcal{S}^\prime_{\tau-t+1}$ such that $M_t\leq\operatorname{Quantile}(1-\hat\alpha_{t};M_t,\{M_t^j(\theta,\theta^\prime)\}_{j=1}^B)$ are both computationally infeasible, unless $\mathcal{S}_{t-1}$ and $\mathcal{S}^\prime_{\tau-t+1}$ are a finite sets. 

However, it is often straightforward to compute  $t_1^j=\displaystyle\inf_{(\theta, \theta^\prime)\in \mathcal{S}_{t-1}\times \mathcal{S}^\prime_{\tau-t+1}}\tau_{\theta,\theta^\prime}^j$ and $ t_2^j=\displaystyle\sup_{(\theta, \theta^\prime)\in \mathcal{S}_{t-1}\times \mathcal{S}^\prime_{\tau-t+1}}\tau_{\theta,\theta^\prime}^j$.
Also, we can often derive a suitable upper bound on the threshold, which does not require any approximation and is computationally and statistically efficient. E.g., we can bound it by $\operatorname{Quantile}(1-\hat\alpha_{t};M_t,\{\sup_{(\theta, \theta^\prime)\in \mathcal{S}_{t-1}\times \mathcal{S}^\prime_{\tau-t+1}}M_{t,L}^j(\theta,\theta^\prime)\}_{j=1}^B)$.
Again, we bound $\sup_{(\theta, \theta^\prime)\in \mathcal{S}_{t-1}\times \mathcal{S}^\prime_{\tau-t+1}}M_{t,L}^j(\theta,\theta^\prime)$ by $U_{t,L}^j$, where for $L<\infty$, 
\begin{equation}
    U_{t,L}^j:=\begin{cases}
    -\infty, ~~\quad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\text{ if } t>t_2^j\\
        \displaystyle \max_{\max\{t_1^j,t\}\leq  t^\prime\leq t_2^j}\sup_{(\theta, \theta^\prime)\in \mathcal{S}_{t-1}\times \mathcal{S}^\prime_{\tau-t+1}}M_t^{t^\prime}(Y_1^t(X_{1}^j,\theta,\theta^\prime),\cdots,Y_{t^\prime}^t(X_{t^\prime}^j,\theta,\theta^\prime);  t^\prime), \\
        \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\text{if }t\leq t_2^j\leq L\\
        \infty, ~\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\text{ if } t\leq t_2^j\text{ and } t_2^j> L.
    \end{cases}
\end{equation} 
And for $L=\infty$,
\begin{equation}
    U_{t,\infty}^j:=\begin{cases}
    -\infty ,\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\text{if } t>t_2^j \text{ or } t_1^j =\infty\\
        \displaystyle \max_{\max\{t_1^j,t\}\leq  t^\prime\leq t_2^j}\sup_{(\theta, \theta^\prime)\in \mathcal{S}_{t-1}\times \mathcal{S}^\prime_{\tau-t+1}}M_t^{t^\prime}(Y_1^t(X_{1}^j,\theta,\theta^\prime),\cdots,Y_{t^\prime}^t(X_{t^\prime}^j,\theta,\theta^\prime);  t^\prime), \\~\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\text{otherwise.}
    \end{cases}
\end{equation} 
%Again, we bound $\sup_{(\theta, \theta^\prime)\in \mathcal{S}_{t-1}\times \mathcal{S}^\prime_{\tau-t+1}}M_t^j(\theta,\theta^\prime)$ by $U_t^j$, where$$U_t^j:=  \max_{t_1^j\leq  t^\prime\leq t_2^j}\sup_{(\theta, \theta^\prime)\in \mathcal{S}_{t-1}\times \mathcal{S}^\prime_{\tau-t+1}}M_t(Y_1^j(\theta,\theta^\prime),\cdots,Y_{t^\prime}^j(\theta,\theta^\prime);  t^\prime).$$
 We consider the following confidence sets: 
 \begin{equation}
 \label{set-eff-comp}
     \mathcal{C}^{\prime\prime}_1=\{t\in\{2,\cdots,\tau\}: M_t\leq\operatorname{Quantile}(1-\alpha\times\sum_{i=1}^N \mathds{I}(\tau_i\geq 
 t)/N;M_t,U_{t,L}^1,\cdots,U_{t,L}^B)\} \text{ and}
 \end{equation}
  \begin{equation}
 \label{set-eff-comp-pfa}
     \mathcal{C}^{\prime\prime}_2=\{t\in\{2,\cdots,\tau\}: M_t\leq\operatorname{Quantile}(1-\alpha ;M_t,U_{t,L}^1,\cdots,U_{t,L}^B)\}.
 \end{equation}
Observe that $\mathcal{C}^{\prime\prime}_i\supseteq \mathcal{C}_i$, for $i=1,2$, where $\mathcal{C}_i$'s are the sets produced by \Cref{algo:comp}, since
\begin{align*}
    \operatorname{Quantile}(1-\hat\alpha_{t};M_t,\{U_{t,L}^j\}_{j=1}^B)\}
 &\geq \operatorname{Quantile}(1-\hat\alpha_{t};M_t,\{\sup_{(\theta, \theta^\prime)\in \mathcal{S}_{t-1}\times \mathcal{S}^\prime_{\tau-t+1}}M_{t,L}^j(\theta,\theta^\prime)\}_{j=1}^B)\\
 &\geq \sup_{(\theta, \theta^\prime)\in \mathcal{S}_{t-1}\times \mathcal{S}^\prime_{\tau-t+1}}\operatorname{Quantile}(1-\hat\alpha_{t};M_t,\{M_{t,L}^j(\theta,\theta^\prime)\}_{j=1}^B).
\end{align*}
 Thus, we obtain the following coverage guarantee as an immediate corollary of 
\Cref{thm:coverage-comp}.

 \begin{corollary}
 $\mathcal{C}^{\prime\prime}_1$ and $\mathcal{C}^{\prime\prime}_2$ defined in \eqref{set-eff-comp} and \eqref{set-eff-comp-pfa} respectively satisfy the same coverage guarantees as $\mathcal{C}_1$ and $\mathcal{C}_2$ in \Cref{thm:coverage-comp}.
 %, for some $\alpha,\beta, \gamma\in(0,1)$, $N,B\in\mathbb N$, $L\in\N \cup \{\infty\}$ and a given change detection algorithm $\mathcal{A}$, the following conditional coverage guarantee holds under \Cref{assmp-1}:
    %\begin{equation*}
    %  \P_{\theta_0,T,\theta_1}(T\in \mathcal{C}^{\prime\prime}_1\mid \tau\geq T)\geq 1-\alpha-\beta-\gamma,
    %\end{equation*}  
    %for any $T \in \N, \theta_i\in\Theta_i; i=0,1$, when $\mathbb \P_\infty(\tau\geq T)\neq 0$.  If $\mathcal{A}$ controls PFA at level $\delta\in(0,1)$, we have the following unconditional coverage guarantee for $\mathcal{C}_2^{\prime\prime}$:
    %\begin{equation*}
      %\P_{\theta_0,T,\theta_1}(T\in \mathcal{C}^{\prime\prime}_2)\geq 1-\alpha-\beta-\gamma-\delta, ~\text{ and }~ \P_{\theta_0,T,\theta_1}(T\in \mathcal{C}^{\prime\prime}_2\mid \tau\geq T)\geq 1-\frac{\alpha+\beta+\gamma}{1-\delta}, \text{ when }\mathbb \P_{\theta,\infty}(\tau\geq T)\neq 0,
    %\end{equation*} 
    %for any $T \in \N, \theta_i\in\Theta_i; i=0,1$.\\   
 \end{corollary}
 In the next subsection, we present a concrete example of the Gaussian mean shift problem to clarify our method. We illustrate another example with Poisson distribution in \Cref{sec:a-pois}.
  
\subsection{A concrete example: Gaussian mean shift problem}
\label{sec:gaussian-example-comp}

We consider the most common changepoint model, where pre-change and post-change models are $\mathcal{P}_i=\{N(\theta,1): \theta\in\Theta_i\}$, for $i=0,1$. 
%We can write the model as \begin{equation}  X_n=\theta^{(n)} + \epsilon_n;~ \epsilon_n\stackrel{iid}{\sim}N(0,1), \theta^{(n)}=\theta_0 \text{ for } n\leq T-1 \text{ and } \theta^{(n)}=\theta_1 \text{ for } n\geq T; \text{ for some } \theta_i\in\Theta_i, i=0,1.\end{equation}
%Now note that the MLE of $\theta_1$ is $\hat \theta_{1,T:\tau}=\bar{X}_{T:\tau}=\theta_1+\bar{\epsilon}_{T:\tau}$. So, for $T\leq \tau, L_{T}^\tau(\hat \theta_{1,T:\tau}; X_1,\cdots, X_)$ 
Then, we have the following $1-\gamma$ confidence interval for $\theta_0$ for each $t\geq 2$:
\begin{equation}
    \mathcal{S}_{t-1}=\left[\bar{X}_{1:t-1}-\frac{q_{\gamma/2}}{\sqrt{t-1}},\bar{X}_{1:t-1}+\frac{q_{\gamma/2}}{\sqrt{{t-1}}}\right]\cap \Theta_0,
\end{equation}
and the $1-\beta$ confidence sequence for $\theta_1$, as defined in \eqref{eq:conf-seq-gaussian}.

We fix $t\in\{2,\cdots,\tau\}$. Note that we can consider $\theta^{\prime\prime}=0$ and $G(x,\theta)=x+\theta$. Therefore, only need to generate $\{\epsilon_n^j\}_{n\in\N}\stackrel{iid}{\sim}N(0,1)$ for $j=1,\cdots,B$. For $\theta\in\mathcal{S}_{t-1},\theta^\prime\in\mathcal{S}^\prime_{\tau-t+1}$, define 
\begin{equation}
    Y_n^j(\theta,\theta^\prime)=\begin{cases}
        \theta+\epsilon_n^j, \text{ if } n<t\\
        \theta^\prime+\epsilon_n^j, \text{ otherwise. }
    \end{cases}
\end{equation}
We consider the likelihood-ratio-based test statistic $M_t(X_1,\cdots,X_\tau;\tau)$, as defined in \eqref{eq:test-stat-comp-simp}, with $\hat \theta_{0,1:t-1}=\frac{1}{ t-1}\sum_{k=1}^{t-1}X_k$ and $\hat \theta_{1,t:n}=\frac{1}{ n-t+1}\sum_{k=t}^{n}X_k$. Note that,  $t_1^j,t_2^j$ are easy to compute exactly even with $L=\infty$ for some specific detection algorithms; see \Cref{max-min-tau-comp}. 
For $t_2^j< t,  U_{t,\infty}^j=0$ and
for $t_2^j\geq t,  U_{t,\infty}^j$ can be simplified for our Gaussian setting as below:
\begin{align*}
   % U_{t,\infty}^j  &=
    &\max_{\max\{t,t_1^j\}\leq  t^\prime\leq t_2^j}\displaystyle\sup_{(\theta, \theta^\prime)\in \mathcal{S}_{t-1}\times \mathcal{S}^\prime_{\tau-t+1}}\max_{1\leq i\leq   t^\prime} \frac{L_{i}(\bar Y_{1:i-1}^j(\theta,\theta^\prime),\bar Y_{i:  t^\prime}^j(\theta,\theta^\prime);Y_1^j(\theta,\theta^\prime),\cdots,Y_{t^\prime}^j(\theta,\theta^\prime))}{L_t(\bar\epsilon_{1:t-1}^j+\theta_0,\bar\epsilon_{t:  t^\prime}^j+\theta_1;Y_1^j(\theta,\theta^\prime),\cdots,Y_{t^\prime}^j(\theta,\theta^\prime))}\\
    &=\max_{\max\{t,t_1^j\}\leq  t^\prime\leq t_2^j} \frac{\displaystyle\max_{1\leq i\leq   t^\prime}\displaystyle\sup_{(\theta, \theta^\prime)\in \mathcal{S}_{t-1}\times \mathcal{S}^\prime_{\tau-t+1}}L_{i}(\bar Y_{1:i-1}^j(\theta,\theta^\prime),\bar Y_{i:  t^\prime}^j(\theta,\theta^\prime);Y_1^j(\theta,\theta^\prime),\cdots,Y_{t^\prime}^j(\theta,\theta^\prime))}{L_t(\bar\epsilon_{1:t-1}^j,\bar\epsilon_{t:  t^\prime}^j;\epsilon_1^j,\cdots,\epsilon_{t^\prime}^j)}\\
     &=\max_{\max\{t,t_1^j\}\leq  t^\prime\leq t_2^j} \frac{\displaystyle\max_{1\leq i\leq   t^\prime}L_{i}(\bar Y_{1:i-1}^j(\theta_{0,t^\prime}^i,\theta_{1,t^\prime}^i),\bar Y_{i:  t^\prime}^j(\theta_{0,t^\prime}^i,\theta_{1,t^\prime}^i);Y_1^j(\theta_{0,t^\prime}^i,\theta_{1,t^\prime}^i),\cdots,Y_{t^\prime}^j(\theta_{0,t^\prime}^i,\theta_{1,t^\prime}^i))}{L_t(\bar\epsilon_{1:t-1}^j,\bar\epsilon_{t:  t^\prime}^j;\epsilon_1^j,\cdots,\epsilon_{t^\prime}^j)},
\end{align*}
where $\bar\epsilon_{i:  t^\prime}^j=\frac{1}{  t^\prime-t+1}\sum_{k=t}^{t^\prime}\epsilon_k^j,\bar Y_{i:  t^\prime}^j(\theta,\theta^\prime) = \frac{1}{  t^\prime-i+1}\sum_{k=i}^{t^\prime}Y_k^j(\theta,\theta^\prime),$ and $(\theta_{0,t^\prime}^i,\theta_{1,t^\prime}^i)$ is the one such pair that maximizes
$L_{i}\left(\bar Y_{1:i-1}^j(\theta,\theta^\prime),\bar Y_{i:  t^\prime}^j(\theta,\theta^\prime);Y_1^j(\theta,\theta^\prime),\cdots,Y_{t^\prime}^j(\theta,\theta^\prime)\right)$, or equivalently minimizes
\begin{align}
\label{eq:likelihood-comp}
    \nonumber & -2\log L_{i}\Big(
        \bar Y_{1:i-1}^j(\theta,\theta^\prime), 
        \bar Y_{i:  t^\prime}^j(\theta,\theta^\prime); Y_1^j(\theta,\theta^\prime),\dots,Y_{t^\prime}^j(\theta,\theta^\prime)
    \Big) = \\
    &\begin{cases}
        \displaystyle\sum_{k=i}^{t-1} \left( 
            \epsilon_k^j - \bar\epsilon_{i:t^\prime}^j  
            - \frac{t^\prime - t + 1}{t^\prime - i + 1} (\theta_1 - \theta_0)
        \right)^2 \\
        \quad + \sum_{k=t}^{t^\prime} \left( 
            \epsilon_k^j - \bar\epsilon_{i:t^\prime}^j  
            + \frac{t - i}{t^\prime - i + 1} (\theta_1 - \theta_0)
        \right)^2 + \text{constant}, 
        & \text{if } i<t, \\[8pt]
        \text{constant}, 
        & \text{if } i=t, \\[8pt]
        \displaystyle\sum_{k=1}^{t-1} \left( 
            \epsilon_k^j - \bar\epsilon_{1:i-1}^j  
            - \frac{i - t}{i - 1} (\theta_1 - \theta_0)
        \right)^2 \\
        \quad + \sum_{k=t}^{i-1} \left( 
            \epsilon_k^j - \bar\epsilon_{1:i-1}^j  
            + \frac{t - 1}{i - 1} (\theta_1 - \theta_0)
        \right)^2 + \text{constant}, 
        & \text{if } i>t,
    \end{cases}
\end{align}
over 
$(\theta, \theta^\prime)\in \mathcal{S}_{t-1}\times \mathcal{S}^\prime_{\tau-t+1}$. Here, constant means that it does not involve $\theta, \theta^\prime$.
Note that $\delta_{t^\prime}^i:=\theta_{1,t^\prime}^i-\theta_{0,t^\prime}^i$ have simple closed-form expressions since \eqref{eq:likelihood-comp} is a quadratic function of $\theta_1-\theta_0$. Then, we can take $(\theta_{0,t^\prime}^i,\theta_{1,t^\prime}^i)$ to be any pair $(\theta,\delta_{t^\prime}^i+\theta)\in \mathcal{S}_{t-1}\times \mathcal{S}^\prime_{\tau-t+1}$.

 \begin{remark}
\label{max-min-tau-comp}     
For weighted CUSUM or SR detectors, finding $t_1^j, t_2^j$ is often straightforward. So, for some weight $W(\theta)$ such that $\int_{\theta\in\Theta_1} dW(\theta)=1$, the weighted CUSUM or SR detectors (with the Reverse Information Projection (RIPr) \cite{larsson2024numeraire,grunwald2020safe,lardy2023universal} being the representative of the pre-change class) are defined as
\begin{equation}
\label{eq:wcusum-ripr}
    \tau_{\text{wcs-ripr}}=\inf\left\{\tau\in\mathbb N:\max_{1\leq j\leq \tau}\int_{\theta\in\Theta_1}\prod_{i=j}^\tau\frac{f_{\theta}(X_i)}{f_{\theta}^*(X_i)}dW(\theta)\geq A\right\},
\end{equation}
\begin{equation}
\label{eq:wsr-ripr}
  \tau_{\text{wsr-ripr}}=\inf\left\{\tau\in\mathbb N:\sum_{j=1}^\tau\int_{\theta\in\Theta_1}\prod_{i=j}^\tau\frac{f_{\theta}(X_i)}{f_{\theta}^*(X_i)}dW(\theta)\geq A\right\},
\end{equation}
where $f_{\theta}^*$ is the RIPr of $f_{\theta}$ on $\mathcal{P}_0$.
For example, consider $\mathcal{P}_0=\{N(\theta,1):\theta\leq a\},\mathcal{P}_1=\{N(\theta,1):\theta\geq b\}$, for some $b\geq a$. Observe that for all $\theta\in \Theta_1, f_{\theta}^*=f_a$, and $\prod_{i=j}^\tau\frac{f_{\theta}(X_i)}{f_{\theta}^*(X_i)}=\exp\{\frac{1}{2}(\theta-a)\times\sum_{i=j}^\tau(2X_i-a-\theta)\}$ increses as $X_i$'s increases and hence, both $\tau_{\text{wcusum}}$ and $\tau_{\text{wsr}}$ decreases as $X_i$'s increases.
Therefore, 
$$t_1^j=\inf_{(\theta, \theta^\prime)\in \mathcal{S}_{t-1}\times \mathcal{S}^\prime_{\tau-t+1}}\tau_{\theta, \theta^\prime}^j=\tau_{\sup \mathcal{S}_{t-1},\sup \mathcal{S}^\prime_{\tau-t+1}}^j \text{ and }$$
$$  t_2^j=\sup_{(\theta, \theta^\prime)\in \mathcal{S}_{t-1}\times \mathcal{S}^\prime_{\tau-t+1}}\tau_{\theta, \theta^\prime}^j=\tau_{\inf \mathcal{S}_{t-1},\inf \mathcal{S}^\prime_{\tau-t+1}}^j.$$
 Similarly, one can check that for $\mathcal{P}_0=\{N(\theta,1):\theta\geq a\},\mathcal{P}_1=\{N(\theta,1):\theta\leq b\}$, for $b\leq a$,
$$t_1^j=\inf_{(\theta, \theta^\prime)\in \mathcal{S}_{t-1}\times \mathcal{S}^\prime_{\tau-t+1}}\tau_{\theta, \theta^\prime}^j=\tau_{\inf \mathcal{S}_{t-1},\inf \mathcal{S}^\prime_{\tau-t+1}}^j \text{ and }$$
$$ t_2^j=\sup_{(\theta, \theta^\prime)\in \mathcal{S}_{t-1}\times \mathcal{S}^\prime_{\tau-t+1}}\tau_{\theta, \theta^\prime}^j=\tau_{\sup \mathcal{S}_{t-1},\sup \mathcal{S}^\prime_{\tau-t+1}}^j.$$
\end{remark}

\begin{remark}
\label{rem:assumption}
For both types of pre- and post-change classes discussed above, and for CUSUM and SR-type detectors defined in \eqref{eq:wcusum-ripr} and \eqref{eq:wsr-ripr}, as well as the PFA-controlling change detectors used in \Cref{sec:expt-pfa}, \cref{assmp-1} holds with $\theta_0^* = a$. We provide the explanation for the CUSUM detector, and the argument for the SR detector is analogous. For example, consider $\mathcal{P}_0=\{N(\theta,1):\theta\leq a\},\mathcal{P}_1=\{N(\theta,1):\theta\geq b\}$, for some $b\geq a$. Then,
\begin{align*}
    &\P_{\theta_0,\infty}[\tau_{\text{wcs-ripr}}\geq t]\\
&=\P_{\theta_0,\infty}\left[\displaystyle\max_{1\leq k\leq t}\max_{1\leq j\leq k}\int_{\theta\in\Theta_1}\prod_{i=j}^k\frac{f_{\theta}(X_i)}{f_{\theta}^*(X_i)}dW(\theta)<A\right]\\
    &=\P_{\theta_0,\infty}\left[\displaystyle\max_{1\leq k\leq t}\max_{1\leq j\leq k}\int_{\theta\in\Theta_1}\prod_{i=j}^k\frac{f_{\theta}(X_i)}{f_{a}(X_i)}dW(\theta)<A\right]\\
    &=\P_{0,\infty}\left[\displaystyle\max_{1\leq k\leq t}\max_{1\leq j\leq k}\int_{\theta\in\Theta_1}\exp\{\frac{1}{2}(\theta-a)\times\sum_{i=j}^\tau(2Z_i+2\theta_0-a-\theta)\}dW(\theta)<A\right].
\end{align*}
Then, $\exp\{\frac{1}{2}(\theta-a)\times\sum_{i=j}^\tau(2Z_i+2\theta_0-a-\theta)\}\leq \exp\{\frac{1}{2}(\theta-a)\times\sum_{i=j}^\tau(2Z_i+a-\theta)\}, \forall \theta\in \Theta_1, \forall \theta_0\in\Theta_0.$ Therefore, $\P_{\theta_0,\infty}[\tau_{\text{wcs-ripr}}\geq t]\geq \P_{a,\infty}[\tau_{\text{wcs-ripr}}\geq t], \forall \theta_0\in\Theta_0.$
\end{remark}

 The Gaussian example we provided serves as a concrete and illustrative case to clarify the methodology.  Note that $\mathcal{C}^{\prime\prime}$ in \eqref{set-eff-comp} is computable if the following conditions are met: 
 \begin{enumerate}
\item The coupling function $G$ is available (which is always the case when distributions are continuous). For discrete distributions, even if a suitable 
$G$ is not available, our method may still be implementable, as demonstrated in \Cref{sec:pois-example-comp} for the Poisson case).  
\item We can construct a confidence interval (and hence confidence sequence) for $\theta$.  
\item The quantities $t_j^1$ and $t_j^2$ are computable (either exactly or numerically). 
\item If we employ the likelihood-ratio-based test statistic \eqref{eq:test-stat-comp-simp}, then we have to be able to maximize the function  $L_i(\hat\theta_{1:i-1},\hat\theta_{i:t^\prime};Y_1^j(\theta, \theta^\prime),\cdots,Y_{t^\prime}^j(\theta, \theta^\prime))$ over $(\theta, \theta^\prime)$ on a set (either exactly or numerically). 
\item If the detection algorithm $\mathcal{A}$ controls ARL (or is a black box), we need to satisfy \Cref{assmp-1}. If $\mathcal{A}$ controls PFA, we do not need this assumption.
\end{enumerate}
For instance, one can check that these conditions hold for analogous Laplace and exponential scale distributions.
Another concrete example with the Poisson distribution is in \Cref{sec:a-pois}. 



 \section{Experiments} 
 We validate our algorithms across various settings and demonstrate their practicality. Experiments below are for Gaussians;  Poisson is handled in \Cref{sec:a-pois}. 
 
\subsection{Experiments with detection algorithms that control ARL}

% The following three subsections handle the three different settings of known or unknown pre- and post-change distributions.

\label{sec:expt}
\subsubsection{Known pre- and post-change (Setting I)}
\label{setting-i}

We consider Gaussian mean-change from $N(0,1)$ (pre-change) to $N(1,1)$ (post-change) having the true (unknown) changepoint at $T=100,500$. We employ the CUSUM change detection method  \eqref{eq:cusum} with thresholds at $A=1000,10000$ (which controls ARL at level $A$ \citep{tartakovsky2014sequential}). After detection, we employ \Cref{algo:1-exact} to construct confidence sets for $T$, with $B=N=100$, $\alpha=0.05$.  \Cref{fig:ci-normal} visualizes the confidence sets and point estimates \eqref{eq:known-pre-post} across $5$ runs.  \Cref{tab:simple} has  the average size and the coverages across $100$ independent runs. 


 \begin{figure}[h!]
\centering
\centering
\subfloat[Data till change is detected]{\includegraphics[width=0.5\linewidth]{Plots-new-CI/ci-data.png}} 
\subfloat[CUSUM detector (in $\log_{10}$ scale) till change is detected]{\includegraphics[width=0.5\linewidth]{Plots-new-CI/ci-cusum.png}}\\
\subfloat[Data till change is detected]{\includegraphics[width=0.5\linewidth]{Plots-new-CI/ci-data-normal-500.png}} 
\subfloat[CUSUM detector (in $\log_{10}$ scale) till change is detected]{\includegraphics[width=0.5\linewidth]{Plots-new-CI/ci-cusum-normal-500.png}}
\caption[]{Setting I (known pre- and post-change Gaussian distributions): The first $T-1$ observations are drawn from $N(0,1)$ and the rest from $N(1,1)$ for $T=100$ (top row) and $T=500$ (bottom row). Change is detected using the CUSUM detector \eqref{eq:cusum} with $A=10000$ (top row) or $A=1000$ (bottom row). The point estimate \eqref{eq:known-pre-post} is shown in vertical red dashed line and the confidence set (using \Cref{algo:1-exact}) is shown in red points, with $B=N=100$, $\alpha=0.05, L=\infty$. Results of $5$ independent simulations are shown.} 
\label{fig:ci-normal}  
\end{figure}




 
  \begin{table}[!ht]
    \centering
    \caption{Known pre- and post-change (Setting I, finite ARL)
    % Average (of $100$ independent runs) size  and coverage of the confidence sets using \Cref{algo:1-exact} with $F_0=N(0,1), F_1=N(1,1), B=N=100$, $\alpha=0.05$, the average absolute deviation $\hat T$ \eqref{eq:known-pre-post} and average detection delay (given a true detection) of the detection algorithm used \eqref{eq:cusum} with threshold $A$, while varying $T$ and $A$.
    }
\label{tab:simple}
    \resizebox{0.92\linewidth}{!}{
    \begin{tabular}{cc|cccccc}
    \toprule
    \addlinespace
$T$ & A &\specialcell{Conditional \\ coverage} & \specialcell{Unconditional \\ coverage} & Size & $\mathbb E_T(|\hat{T}-T|| \tau \geq T)$ & $\mathbb E_T(\tau-T | \tau \geq T)$\\
    \midrule
\addlinespace 
100 & 10000&   0.96  &  0.96  & 12.21 & 2.97 & 17.63\\
100 & 1000&   0.96 &  0.94  & 12.34  & 2.85 & 13.97 \\
%100 & 10000&  $\text{Pois}(1)$ & $\text{Pois}(2)$  & 0.94   & 0.94   & 18.04 & 4.02 & 22.15\\
%100 &   $\text{Pois}(1)$ & $\text{Pois}(2)$  &   0.94  & 0.94  & 17.45 & 3.98 & 15.72\\
500 & 10000&   0.95  & 0.95 & 12.88 & 2.59 & 17.26\\
500 & 1000  & 0.95  & 0.90 & 12.57 & 2.62 & 13.22\\
%500 & 10000&  $\text{Pois}(1)$ & $\text{Pois}(2)$  & 0.95   & 0.95 & 18.53  & 4.30 & 22.10 \\
%500 &   $\text{Pois}(1)$ & $\text{Pois}(2)$  &  0.96  & 0.89 & 18.72  & 3.96 & 16.14\\

\bottomrule
 \end{tabular}}
 \end{table}
 
 For $A=10000$, conditional and unconditional coverages are equal, as the probability of false detection before the true change is negligible. However, for $T=500$ with $A=1000$, the presence of a significant number of false detections before the true change results in lower unconditional coverage. Nevertheless, the empirical conditional coverage rates meet the desired level, supporting our theory.
We also report the average absolute deviation of $\hat T$ (defined in \eqref{eq:known-pre-post}) and delay (given no false detection) in the table.





% \begin{figure*}[!ht]
% \centering
% \centering
% \subfloat[Data till change is detected]{\includegraphics[width=0.49\linewidth,height=0.42\linewidth]{Plots-new-CI/ci-data-normal-500.png}} 
% \subfloat[CUSUM detector (in $\log_{10}$ scale) till change is detected]{\includegraphics[width=0.49\linewidth,height=0.42\linewidth]{Plots-new-CI/ci-cusum-normal-500.png}}
% \caption[]{Setting I: The first $499$ observations are drawn from $N(0,1)$ and the remaining observations from $N(1,1)$. The pre- and post-change distributions are assumed to be known. Change is detected using the CUSUM detector \eqref{eq:cusum} with $A=1000$. The point estimate \eqref{eq:known-pre-post} is shown in vertical red dashed line and the confidence set (using \Cref{algo:1-exact}) is shown in red points. $B=N=100$, $\alpha=0.05, L=\infty$. Results of $5$ independent simulations are shown.} 
% \label{fig:ci-normal-500}  
% \end{figure*}


% \begin{figure*}[!ht]
% \centering
% \centering
% \subfloat[Data till change is detected]{\includegraphics[width=0.49\linewidth,height=0.42\linewidth]{Plots-new-CI/ci-data-pois-500.png}} 
% \subfloat[CUSUM detector (in $\log_{10}$ scale) till change is detected]{\includegraphics[width=0.49\linewidth,height=0.42\linewidth]{Plots-new-CI/ci-cusum-pois-500.png}}
% \caption[]{Setting I: The first $499$ observations are drawn from Pois$(1)$ and the rest from Pois$(2)$. The pre- and post-change distributions are assumed to be known. Change is detected using CUSUM detector \eqref{eq:cusum} with $A=1000$. The point estimate \eqref{eq:known-pre-post} is shown in vertical red dashed line and the confidence set (using \Cref{algo:1-exact}) is shown in red points. $B=N=100$, $\alpha=0.05, L=\infty$. Results of $5$ independent simulations are shown.} 
% \label{fig:ci-pois-500}  
% \end{figure*}

\subsubsection{Known pre-change and composite post-change (Setting II)}
\label{setting-ii}
We perform experiments for Gaussian mean-change scenarios having pre-change distribution as $N(0,1)$, and the true (unknown) post-change data distribution as $N(1,1)$, with the changepoint at $T=100,500.$ 
For detection, we employ a weighted CUSUM detector (defined in \eqref{eq:wcusum}) with thresholds at $A=1000$ (which is known to control ARL at level $A$). For weighted CUSUM, we employ a discrete weight distribution taking values
$\theta_1=\theta,\theta_2=\theta+d,\theta_3=\theta+2d,\cdots,\theta_{10}=\theta+9d$, $d=0.2$, $\theta=0.75$, when $\mathcal{P}_1=\{N(\mu,1):\mu> 0.75\}$ and $\theta=0.9$, when $\mathcal{P}_1=\{N(\mu,1):\mu> 0.9\}$, with exponentially decaying weights $w_i=e^{-\frac{i-1}{2}}-e^{-\frac{i}{2}},\text{ for } i=1,\cdots,9, \text{ and } w_{10}=e^{-9/2}$, which appeared to us to be a sensible discretized exponential mixture over the alternative class.  After detection, we construct confidence sets (as defined in \eqref{set-gaussian-known-pre}) for $T$. We report the average size and coverages across $100$ independent runs in \Cref{tab:comp-post}. 
 \Cref{fig:ci-comp-post} provides visualizations of the confidence sets  \eqref{set-gaussian-known-pre} and point estimates \eqref{eq:known-pre-unknown-post} of the changepoint across $5$ random runs. 




\begin{figure*}[!ht]
\centering
\centering
\subfloat[Data till change is detected]{\includegraphics[width=0.5\linewidth]{Plots-composite/data-comp-post-100-0.9.png}} 
\subfloat[CUSUM detector (in $\log_{10}$ scale) till change is detected]{\includegraphics[width=0.5\linewidth]{Plots-composite/cusum-comp-post-100-0.9.png}}\\
\subfloat[Data till change is detected]{\includegraphics[width=0.5\linewidth]{Plots-composite/data-comp-post-500-0.9.png}} 
\subfloat[CUSUM detector (in $\log_{10}$ scale) till change is detected]{\includegraphics[width=0.5\linewidth]{Plots-composite/cusum-comp-post-500-0.9.png}}
\caption[]{Setting II: The first $T-1$ observations are drawn from $N(0,1)$ and the rest from $N(1,1)$ with $T=100$ (top row) or $T=500$ (bottom row). Change is detected using a weighted CUSUM detector \eqref{eq:wcusum} with $A=10000$ (top row) or $A=1000$ (bottom row). The point estimate \eqref{eq:known-pre-unknown-post} is shown in a vertical red dashed line and the confidence set \eqref{set-gaussian-known-pre} is shown in red points. $F_0=N(0,1)$, $\mathcal{P}_1=\{N(\mu,1):\mu\geq 0.9\}$, $N=B=100$, $\alpha=0.05, \beta=0.025$ and $L=\infty$.  Results of $5$ independent simulations are shown.} 
\label{fig:ci-comp-post}  
\end{figure*}






\begin{table}[!ht]
    \centering
    \caption{Known pre-change but unknown post-change (Setting II, finite ARL)
    % : Average (of $100$ independent runs) size and (conditional and unconditional) coverage rates of confidence sets \eqref{set-gaussian-known-pre} with $B=N=100$ and $\alpha=0.05,\beta=0.025,L=\infty$, the average absolute deviation $\hat T$ \eqref{eq:known-pre-unknown-post} and average detection delay (given no false detection) of the change detection method used \eqref{eq:wcusum} with threshold $A=1000$, having pre- and post-change data-generating distributions as $N(0,1)$ and $N(0,1)$ respectively, while we vary the changepoint $T$ and composite post-change model.
    }
\label{tab:comp-post}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{cc|ccccc}
    \toprule
    \addlinespace
 T  & \specialcell{Composite \\post-change}  & \specialcell{Conditional\\coverage} & \specialcell{Unconditional\\coverage} & Size & $\mathbb E_T(|\hat{T}-T|| \tau \geq T)$ & $\mathbb E_T(\tau-T | \tau \geq T)$ \\
    \midrule
\addlinespace 
%100 & 10000 & $\{N(\mu,1):\mu> 0.75\}$   & 0.97  & 0.97 & 23.56 & 3.92 & 20.04\\
100 &  $\{N(\mu,1):\mu> 0.75\}$   & 0.96  & 0.95 &  26.25 & 3.95 & 16.87\\
%100 & 10000 & $\{N(\mu,1):\mu> 0.9\}$   & 0.97  & 0.97 &  20.47 & 3.83 & 18.85\\
100 & $\{N(\mu,1):\mu> 0.9\}$   & 0.97  & 0.97 & 24.11 & 3.67 & 16.21\\
%500 & 10000 & $\{N(\mu,1):\mu> 0.75\}$ & 0.96  & 0.96 & 31.76 & 3.54 & 19.86\\
500 & $\{N(\mu,1):\mu> 0.75\}$ &  0.96  & 0.89 & 30.83 & 3.45 & 17.63\\
%500 & 10000 & $\{N(\mu,1):\mu> 0.9\}$ & 0.98  & 0.98 & 29.14 & 3.50  & 18.39 \\
500 &  $\{N(\mu,1):\mu> 0.9\}$ &  0.97  & 0.91 & 28.56 & 3.48 & 15.81\\
\bottomrule
 \end{tabular}}
 \end{table}


For $T=500$ with $A=1000$, the presence of a significant number of false detections before the true change results in much lower unconditional coverage. However, the empirical conditional coverage rates are always a bit higher than the theoretical bound $1-\alpha-\beta$ (which is 0.925 in our experiments). It is worth noting that both the lengths and the conditional coverage rates are higher compared to the known pre- and post-change settings (\Cref{tab:simple}). We also report the average absolute deviation of $\hat T$ (defined in \eqref{eq:known-pre-post}) and delay (given no false detection) in the table. We observe that the confidence sets in this setting tend to be a bit more conservative compared to those with known pre- and post-change distributions (Setting 1), exhibiting both higher coverage and larger size.

 

\subsubsection{Composite pre- and post-change (Setting III)}
\label{setting-iii}
Now we conduct experiments for Gaussian mean-change scenarios with composite pre- and post-change models having the changepoint at $T=100$ and $500$, the true (unknown) pre- and post-change data distributions as $N(0,1)$ and $N(1,1)$ respectively, with the changepoint at $T=100$ and $T=500$. For change detection, we employ a weighted CUSUM-type detector \eqref{eq:wcusum-ripr}, where the ``closest'' element from the pre-change (the Reverse Information Projection (RIPr) of the post-change element to the pre-change class) is used as a representative of the pre-change model, and for the post-change parameter, the same discrete weight distribution as in setting II is considered. We set the thresholds at $A=1000, 10000$. This is a special case of ``e-detector'' and is known to control ARL at level $A$ \citep{shin2022detectors}.
After detection, we construct confidence sets  \eqref{set-eff-comp} for $T$ (assuming $T\neq 1$), with $N=B=100$ and $\alpha=0.05, \beta=\gamma=0.025$. \Cref{fig:ci-comp} visualizes the confidence sets \eqref{set-eff-comp} and point estimates \eqref{eq:unknown-pre-post} of the changepoint across five random runs. 

\begin{figure*}[!ht]
\centering
\centering
\subfloat[Data till change is detected]{\includegraphics[width=0.5\linewidth]{Plots-composite/data-comp-100-0.9.png}} 
\subfloat[CUSUM detector (in $\log_{10}$ scale) till change is detected]{\includegraphics[width=0.5\linewidth]{Plots-composite/cusum-comp-100-0.9.png}}\\
\subfloat[Data till change is detected]{\includegraphics[width=0.5\linewidth]{Plots-composite/data-comp-500-0.9.png}} 
\subfloat[CUSUM detector (in $\log_{10}$ scale) till change is detected]{\includegraphics[width=0.5\linewidth]{Plots-composite/cusum-comp-500-0.9.png}}
\caption[]{Setting III: The first $T-1$ observations are drawn from $N(0,1)$ and the remaining observations from $N(1,1)$ with $T=100$ (top row) or $T=500$ (bottom row).
Change is detected using the CUSUM detector defined in \eqref{eq:wcusum-ripr} with $A=1000$. The point estimate \eqref{eq:unknown-pre-post} is shown in vertical red dashed line and the confidence set \eqref{set-gaussian-known-pre} is shown in red points. $\mathcal{P}_0=\{N(\mu,1):\mu\leq 0.1\}$, $\mathcal{P}_1=\{N(\mu,1):\mu> 0.9\}$, $N=B=100$, $\alpha=0.05, \beta=\gamma=0.025$ and $L=\infty$. Results of $5$ independent simulations are shown.} 
\label{fig:ci-comp}  
\end{figure*}



\Cref{tab:comp} contains the average size, conditional and unconditional coverage rates, average absolute deviation of $\hat T$ (defined in \eqref{eq:known-pre-post}), and delay (given no false detection) across $100$ independent runs.  The empirical conditional coverage rates are always a bit higher than the theoretical bound $1-\alpha-\beta-\gamma$ (which is 0.9 in our experiments). It is worth noting that both the lengths and the conditional coverage rates are higher compared to the known pre- and post-change settings (\Cref{tab:simple}). We observe that the confidence sets in this setting tend to be a bit more conservative compared to those with known pre- and post-change distributions (\Cref{tab:simple}), exhibiting both higher coverage and larger size.




 \begin{table}[!ht]
    \centering
    \caption{Composite pre- and post-change (Setting III, finite ARL)
    % Average (of $100$ independent runs) conditional and unconditional coverage and size of confidence sets \eqref{set-eff-comp} with $N=B=100$ and $\alpha=0.05, \beta=\gamma=0.025$,the average absolute deviation $\hat T$ \eqref{eq:unknown-pre-post} and average detection delay (given no false detection) of the change detection method used \eqref{eq:wcusum-ripr} with threshold $A=1000$, having pre- and post-change data-generating distributions as $N(0,1)$ and $N(0,1)$ respectively, while we vary the changepoint $T$ and pre- and post-change model.
    }
\label{tab:comp}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ccc|ccccc}
    \toprule
    \addlinespace
 T & \specialcell{Composite \\pre-change}  & \specialcell{Composite \\post-change}    & \specialcell{Conditional\\coverage} & \specialcell{Unconditional\\coverage} & Size & $\mathbb E_T(|\hat{T}-T|| \tau \geq T)$ & $\mathbb E_T(\tau-T | \tau \geq T)$ \\
    \midrule
\addlinespace 
100 &  $\{N(\mu,1):\mu\leq 0.25\}$ & $\{N(\mu,1):\mu> 0.75\}$   & 0.96 & 0.96 & 27.84 & 4.36 & 25.81\\
100 &  $\{N(\mu,1):\mu\leq 0.1\}$ & $\{N(\mu,1):\mu> 0.9\}$ & 0.95 & 0.95 & 25.71 & 4.19 & 23.15\\
500 &  $\{N(\mu,1):\mu\leq 0.25\}$ & $\{N(\mu,1):\mu> 0.75\}$ &   0.97 & 0.97 & 31.45 & 4.03 & 24.48\\
500 & $\{N(\mu,1):\mu\leq 0.1\}$ & $\{N(\mu,1):\mu> 0.9\}$ &   0.96 & 0.95 & 30.66 & 4.07 & 23.07\\
\bottomrule
 \end{tabular}}
 \end{table}



\subsection{Experiments with detection algorithms that control PFA}
\label{sec:expt-pfa}
\subsubsection{Known pre- and post-change (Setting A)}
We perform experiments for both Normal and Poisson mean-change scenarios having the true changepoint at $T=100,500$, using a likelihood ratio detector defined 
as
\begin{equation}
\label{eq:LRT}
    \tau=\inf\left\{t\in\N: \prod_{i=1}^t\frac{f_1(X_i)}{f_0(X_i)}\geq A\right\},
\end{equation}
with thresholds at $A=1000$. Note that $\prod_{i=1}^t\frac{f_1(X_i)}{f_0(X_i)}$ is a non-negative martingale under $T=\infty$ and hence, by Ville's inequality \citep{ville1939etude}, the PFA of this detector is at most $1/A$, i.e., $0.001$ here.
After detection, we employ \Cref{algo:1} to construct confidence sets for $T$, with $B=100$ and $\alpha=0.05$. We report the average size and the conditional and unconditional coverage rates across $100$ independent runs in \Cref{tab:simple-pfa}. 
We also report the average absolute deviation of $\hat T$ (defined in \eqref{eq:known-pre-post}) and delay (given no false detection) in the table.

 \begin{table}[!ht]
    \centering
    \caption{Known pre- and post-change distributions (Setting A, PFA $\leq 0.001$) 
    % Average (of $100$ independent runs) size  and coverage of the confidence sets using \Cref{algo:1} with $B=100$, $\alpha=0.05$, the average absolute deviation $\hat T$ \eqref{eq:known-pre-post} and average detection delay (given a true detection) of the detection algorithm used (defined in \eqref{eq:LRT}) with $A=1000$, while varying the changepoint $T$.
    }
\label{tab:simple-pfa}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ccc|ccccc}
    \toprule
    \addlinespace
$T$ & $F_0$ & $F_1$  & \specialcell{Conditional \\ coverage} & \specialcell{Unconditional \\ coverage} & Size & $\mathbb E_T(|\hat{T}-T|| \tau \geq T)$ & $\mathbb E_T(\tau-T | \tau \geq T)$\\
    \midrule
\addlinespace 
%100 & 10000&  $N(0,1)$ & $N(1,1)$  & 0.96  &  0.96  & 12.21 & 2.97 & 17.63\\
100 & $N(0,1)$ & $N(1,1)$  &  0.95 &  0.94  & 12.60  & 2.74 & 110.91 \\
%100 & 10000&  $\text{Pois}(1)$ & $\text{Pois}(2)$  & 0.94   & 0.94   & 18.04 & 4.02 & 22.15\\
100 &  $\text{Pois}(1)$ & $\text{Pois}(2)$  &  0.95   & 0.95  & 17.95 & 4.98 & 94.33\\
%500 & 10000&  $N(0,1)$ & $N(1,1)$  & 0.95  & 0.95 & 12.88 & 2.59 & 17.26\\
500 &  $N(0,1)$ & $N(1,1)$  & 0.97  & 0.97 & 12.65 & 2.24 & 514.12\\
%500 & 10000&  $\text{Pois}(1)$ & $\text{Pois}(2)$  & 0.95   & 0.95 & 18.53  & 4.30 & 22.10 \\
500 &  $\text{Pois}(1)$ & $\text{Pois}(2)$  &  0.96  & 0.96 & 16.82  & 3.18 & 417.44\\

\bottomrule
 \end{tabular}}
 \end{table}

 \subsubsection{Known pre- and unknown post-change (Setting B)}
We perform experiments for Gaussian mean-change scenarios having the true changepoint at $T=100,500$, using a mixture likelihood ratio detector defined 
as
\begin{equation}
\label{eq:mix-LRT}
    \tau=\inf\left\{t\in\mathbb N:\int_{\theta\in\Theta_1}\prod_{i=1}^t\frac{f_{\theta}(X_i)}{f_{\theta_0}(X_i)}dW(\theta)\geq A\right\}
\end{equation}
with thresholds at $A=1000$. The weights are chosen just as described in \Cref{setting-ii}. Similarly, the PFA of this detector is at most $1/A$, i.e., $0.001$ here.
After detection, we construct confidence sets for $T$ as defined in \eqref{set-eff-known-pre-pfa}. We report the average size and the conditional and unconditional coverage rates across $100$ independent runs in \Cref{tab:comp-post-pfa}, with $B=100$ and $\alpha=0.05,\beta=0.025,L=\infty$. 
We also report the average absolute deviation of $\hat T$ in \eqref{eq:known-pre-unknown-post} and delay (given no false detection) in the table.

\begin{table}[!ht]
    \centering
    \caption{Known pre-change but unknown post-change (Setting B, PFA $\leq 0.001$)
    % Average (of $100$ independent runs) size and (conditional and unconditional) coverage rates of confidence sets \eqref{set-eff-known-pre-pfa} with $B=100$ and $\alpha=0.05,\beta=0.025,L=\infty$, the average absolute deviation of $\hat T$ (defined in \eqref{eq:known-pre-unknown-post}) and average detection delay (given no false detection) of the change detector used (as defined in \eqref{eq:mix-LRT}) with threshold $A=1000$, having pre- and post-change data-generating distributions as $N(0,1)$ and $N(0,1)$ respectively, while we vary the changepoint $T$ and composite post-change model.
    }
\label{tab:comp-post-pfa}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{cc|ccccc}
    \toprule
    \addlinespace
 T    & \specialcell{Composite \\post-change}  & \specialcell{Conditional\\coverage} & \specialcell{Unconditional\\coverage} & Size & $\mathbb E_T(|\hat{T}-T|| \tau \geq T)$ & $\mathbb E_T(\tau-T | \tau \geq T)$ \\
    \midrule
\addlinespace 

100 &  $\{N(\mu,1):\mu> 0.75\}$   & 0.95 & 0.95 & 26.03 & 3.68 & 118.76 \\
100 &  $\{N(\mu,1):\mu> 0.9\}$   & 0.94 &0.94 & 24.84 & 3.36 & 115.64\\
500 &  $\{N(\mu,1):\mu> 0.75\}$ & 0.96 &0.96 & 30.95 & 3.18 & 530.83\\
500 &  $\{N(\mu,1):\mu> 0.9\}$ & 0.96 &0.96 & 30.09 & 3.23 & 528.92\\
%500 & 1000 & $\text{Pois}(1)$ & $\{\text{Pois}(\lambda):\lambda> 1\}$ & $\text{Pois}(2)$ & 0.95 & 0.85 & 20.83  \\
\bottomrule
 \end{tabular}}
 \end{table}

 \subsubsection{Composite pre- and post-change (Setting C)}
We perform experiments for Gaussian mean-change scenarios having the true changepoint at $T=100,500$, using a mixture likelihood ratio detector (with the Reverse Information Projection (RIPr) \cite{larsson2024numeraire,grunwald2020safe,lardy2023universal} being the representative of the pre-change class) defined 
as
\begin{equation}
\label{eq:mix-LRT-ripr}
    \tau=\inf\left\{t\in\mathbb N:\int_{\theta\in\Theta_1}\prod_{i=1}^t\frac{f_{\theta}(X_i)}{f^*_{\theta}(X_i)}dW(\theta)\geq A\right\},
\end{equation}
with $f^*_{\theta}$ being the RIPr of $f_{\theta}$ on the pre-change class $\mathcal{P}_0$ and with thresholds at $A=1000$. This is a stop time of an ''e-process'' \citep{ramdas2022game} and hence, the PFA of this detector is at most $1/A$, i.e., $0.001$ here.
The weights are chosen just as described in \Cref{setting-iii}.
After detection, we construct confidence sets for $T$ as defined in \eqref{set-eff-comp-pfa} (assuming $T\neq 1$). The pre- and post-change data-generating distributions are $N(0,1)$ and $N(0,1)$ respectively, but neither of which is assumed known, and we vary the pre- and post-change classes. We use $B=100$ and $\alpha=0.05, \beta=\gamma=0.025, L=\infty$. We report the average size and coverage rates across $100$ independent runs in \Cref{tab:comp-pfa}. 
We also report the average absolute deviation of $\hat T$  in \eqref{eq:unknown-pre-post} and delay (given no false detection). The sizes and coverages are reasonable, despite the methods not knowing the data-generating distribution exactly.

 \begin{table}[!ht]
    \centering
    \caption{Composite pre- and post-change (Setting C, PFA $\leq 0.001$)
    % The pre- and post-change data-generating distributions are $N(0,1)$ and $N(0,1)$ respectively, but neither of which is assumed known.
    % Average (of $100$ independent runs) conditional and unconditional coverage and size of confidence sets \eqref{set-eff-comp} with $B=100$ and $\alpha=0.05, \beta=\gamma=0.025, L=\infty$, the average absolute deviation of $\hat T$ (defined in \eqref{eq:unknown-pre-post}) and average detection delay (given no false detection) of the change detector used (as defined in \eqref{eq:mix-LRT-ripr}) with threshold $A=1000$, having pre- and post-change data-generating distributions as $N(0,1)$ and $N(0,1)$ respectively (neither of which is known), while we vary the changepoint $T$ and pre- and post-change models.
    }
\label{tab:comp-pfa}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ccc|ccccc}
    \toprule
    \addlinespace
 T &\specialcell{Composite \\pre-change}  & \specialcell{Composite \\post-change}    & \specialcell{Conditional\\coverage} & \specialcell{Unconditional\\coverage} & Size & $\mathbb E_T(|\hat{T}-T|| \tau \geq T)$ & $\mathbb E_T(\tau-T | \tau \geq T)$ \\
    \midrule
\addlinespace 
100 &  $\{N(\mu,1):\mu\leq 0.25\}$ & $\{N(\mu,1):\mu> 0.75\}$ & 0.95 & 0.95 & 26.72 & 3.97 &125.64\\
100 &  $\{N(\mu,1):\mu\leq 0.1\}$ & $\{N(\mu,1):\mu> 0.9\}$ & 0.95 & 0.95 & 25.48 & 3.88 & 121.28\\
500 &  $\{N(\mu,1):\mu\leq 0.25\}$ & $\{N(\mu,1):\mu> 0.75\}$ & 0.97 & 0.97 & 32.18 & 3.82 & 544.91\\
500 & $\{N(\mu,1):\mu\leq 0.1\}$ & $\{N(\mu,1):\mu> 0.9\}$ & 0.96 &0.96 & 31.45 & 3.75 & 536.76\\
\bottomrule
 \end{tabular}}
 \end{table}

\subsection{Comparison of results with ARL and PFA controlling detection algorithms}
As expected, algorithms that control PFA exhibit a significantly higher average detection delay compared to those controlling ARL. This delay arises because stricter false alarm constraints force the detection algorithm to accumulate stronger evidence before signaling a change.

In terms of inference, we observe that PFA-controlling algorithms often tend to yield slightly lower conditional coverage probabilities with confidence sets of comparable sizes. Also, conditional and unconditional coverage rates are identical when PFA-controlling detection algorithms are employed, due to the very low probability of false detections, while the ARL controlling algorithms lead to lower unconditional conditional coverage rates for $T=500$, supporting our theory. Moreover, we observe that the average absolute deviations of the point estimates are barely smaller with the PFA controlling algorithms.

Overall, the major differences between these two types lie in detection delay and unconditional coverage properties. We do not observe significant differences in the size and conditional coverage properties of the confidence sets nor in the absolute deviation of the point estimates, across these two types of detection algorithms.
% Figures  provide visualizations of the confidence sets and point estimates \eqref{eq:known-pre-post} of the changepoints across $5$ random runs. 
 
\section{Conclusion}
\label{sec:conc}

We developed a framework to construct a confidence set for the unknown changepoint $T$ after a sequential detection algorithm declares a change at a stopping time, providing conditional coverage guarantees (conditioned on the event that the stopping occurs after the changepoint) when the pre- and post-change distributions are known. We also extended our framework to handle composite pre- and post-change distributions and provide conditional coverage guarantees, demonstrating its good empirical performance. We impose no specific conditions on the change detection algorithms; they can function as entirely black-box or heuristic methods; we only need to be able to run the algorithm on simulated data sequences.
By filling this significant gap in post-detection analysis, our work provides crucial tools for precise and timely changepoint localization in real-time data streams. 
% Our framework offers important implications for applications where rapid and accurate inference is critical, paving the way for more effective changepoint localization in sequential settings. 
% While our method is designed for partitioned and parametric pre- and post-change models, future work can explore its extensions to the non-partitioned setting and nonparametric models. 




\begin{comment}
\section{Point Estimates}
\subsection{Known pre- and post-change distributions}

Suppose, we observe data $X_1,X_2,\dots$ sequentially.
Assume that $X_1,\cdots,X_{T-1}\sim F_0$ and $X_T,X_{T+1}\dots\sim F_1$.
Let $\tau\geq T$ be some data-dependent stopping time where change is detected. We aim to estimate the changepoint $T$ based on $X_1,\cdots,X_\tau$.

Define the estimator 
\begin{equation}
\label{eq:known-pre-post}
    \hat T=\argmax_{1\leq j\leq \tau}\prod_{i=j}^\tau\frac{F_1(X_i)}{F_0(X_i)}.
\end{equation}

\subsection{Known pre-change and unknown post-change distributions}
%For handling composite post-change distribution, we consider a weighted version of \eqref{eq:known-pre-post}. 
Suppose the pre change distribution is $P_{\theta_0}$ and the composite post-change model is $\mathcal{P}_1=\{P_{\theta}:\theta\in \Theta_1\}$.
%We consider weights $W(\theta)$ over $\Theta_1$ such that $\int_{\theta\in\Theta_1} dW(\theta)=1$ and define,
%\begin{equation}
%\label{eq:known-pre-unknown-post}
%    \hat T=\argmax_{1\leq j\leq \tau}\int_{\theta\in\Theta_1}\prod_{i=j}^\tau\frac{p_{\theta}(X_i)}{F_0(X_i)}dW(\theta).
%\end{equation}
Let $\hat P_{1,j:\tau}$ be the maximum likelihood estimator of the alternative $F_1\in \mathcal{P}_1$ based on data $X_j,\cdots,X_{\tau}$. Define,
\begin{equation}
\label{eq:known-pre-unknown-post}
    \hat T=\argmax_{1\leq j\leq \tau}\prod_{i=j}^\tau\frac{\hat p_{1,j:\tau}(X_i)}{F_0(X_i)}.
\end{equation}

\subsection{Unknown pre-change and known post-change distributions}
Suppose we know the post-change distribution to be $F_1$, but the pre-change model $\mathcal{P}_0$ is composite. Let $\hat P_{0,1:j}$ be the maximum likelihood estimator (MLE) of the null $F_0\in \mathcal{P}_0$ based on data $X_1,\cdots,X_{j}$.
%In the hypothesis testing literature for composite null versus simple alternative, the reverse information projection (RIPr) has recently emerged as an optimal tool \cite{grunwald2020safe,lardy2023universal,larsson2024numeraire}. Here we use this tool for handling composite pre-change distributions. Let $F_0^*$ be the reverse information projection (RIPr) of $F_1$ on $\mathcal{P}_0.$
Define the estimator 
\begin{equation}
\label{eq:unknown-pre-known-post}
    \hat T=\argmax_{1\leq j\leq \tau}\prod_{i=j}^\tau\frac{F_1(X_i)}{\hat p_{0,1:j-1}(X_i)}.
\end{equation}

\subsection{Unknown pre- and post-change distributions}
Consider the composite pre-change model $\mathcal{P}_0$ and the composite post-change model $\mathcal{P}_1$.
Combining the above two techniques, we obtain the following estimator when both pre- and post-change distributions are unknown:
\begin{equation}
\label{eq:unknown-pre-post}
    \hat T=\argmax_{1\leq j\leq \tau}\prod_{i=j}^\tau\frac{\hat p_{1,j:\tau}(X_i)}{\hat p_{0,1:j-1}(X_i)}.
\end{equation}
\end{comment}




