---Shapeshifter

--- Introduction

We introduce ShapeShifter, a novel 3D generative model that learns to synthesize shape variations based on a single reference model.

Currently, three main methods exist to train 3D generative models: using large datasets, score distillation sampling with pre-trained image models, or relying on a single 3D example. The first two approaches demand substantial resources, extended training times, and struggle to produce fine geometric details. We thus focus on the lesser-explored third approach, as it offers more control over the input models, and enables experimentation on a consumer-grade computer.

Given a 3D mesh, optionally with texture, ShapeShifter is first trained (in minutes) and evaluated (in seconds) to produce plausible variations of the input shape.

Central to our approach is our choice of 3D representation. Previous work has shown that point clouds can be efficiently leveraged in a learning pipeline, and displayed via point splatting. Compared to more traditional Signed Distance Fields or SDF, point clouds also provide more flexibility, working seamlessly for open surfaces for instance. ShapeShifter proposes to combine the expressiveness of point cloud representations with sparse voxel grids for efficient shape generation.

--- 3D shape encoding

Given a 3D mesh, we first compute a high-resolution sparse voxel grid. For each voxel intersecting the surface, we find the closest surface point to the voxel center as well as its normal orientation and color. We then coarsify this grid and average the positions, normals, and colors within each coarser voxel. This provides a hierarchy of sparse voxel grids with geometric and semantic information that we leverage in our multiscale diffusion pipeline.

--- multi scale diffusion
-- forward process

Given the coarsest ground truth grid previously obtained, we compute a dense array of fixed dimensions and use it to train a first denoising diffusion model. Note that we operate on 10 features only: positions, normals, colors, as well as a mask that will be used at inference time.

For the other levels, we first train a small upsampler to compute an upsampling of the previous level. We then slightly corrupt the predicted upsampling with Gaussian noise, and train a separate diffusion model to deblur and denoise the sparse voxel grid. All of our models are independent and trained in parallel. Please refer to our paper for more details.

-- reverse process

At inference time, we first predict an initial coarse grid from pure noise. We then prune the grid according to the predicted mask, compute the upsampled values, slightly corrupt them, and feed the results to the next diffusion model.

--- meshing

Contrary to SDF-based methods, we advocate for the use of oriented point clouds. Not only can point clouds be easily viewed and rendered, but efficient off-the-shelf reconstruction methods exist to produce 3D meshes from them, such as Poisson surface reconstruction or APSS for open shapes.

--- results

% Our method can be trained in minutes and evaluated in seconds on a single, consumer-grade GPU. 

Here is a visualization of our progressive generation. We can reconstruct a mesh and parametrize a texture based on the predicted points.

Compared to previous work, ShapeShifter achieves better details and sharper surfaces. It greatly surpasses other methods in terms of geometric fidelity, while featuring desirable diversity scores, with high scores for organic shapes and low scores for structured shapes. We thus believe that ShapeShifter sets a new standard for high-quality single-shape variation generation.

Unlike previous methods, ShapeShifter also has the ability to generate surfaces with boundaries. 

Our approach outpaces our primary competitor, Sin3DM. Combining ShapeShifter with GPU-based point cloud rendering allows for interactive editing operations. Here is a live demonstration of generative rescaling. Additionally, users can manipulate the voxel grid at any scale using copy and paste operations. Our progressive method is well suited to manipulations,  as it provides immediate feedback to the user. 

--- conclusion

We introduced ShapeShifter, a novel 3D generative model that combines the expressiveness of oriented point clouds with the efficiency of sparse voxel grids. On top of achieving state-of-the-art performances and significantly improved geometric scores, our method is lightweight, leading to far reduced training and inference time. Thank you.
























% Shapeshifter
% --- introduction

% We introduce ShapeShifter, a novel 3D generative model that learns to synthesize shape variations based on a single reference model.
% While generative methods for 3D objects have recently attracted much attention, current techniques often lack geometric details and require long training times and large resources. In this paper, we tackle a task-specific, yet effective approach to synthesizing geometry: shape variation generation from a single high-quality example.
% Central to our approach is our choice of 3D representation. Previous work has shown that point clouds can be efficiently leveraged in a learning pipeline, and displayed via point splatting. Compared to the more traditional Signed Distance Fields, or SDF, they also provide more flexibility and have the ability to fit open surfaces. ShapeShifter proposes to combine the expressiveness of point cloud representations with sparse voxel grids for efficient computation.
% --- 3D shape encoding
% Given a 3D mesh, we first compute a sparse voxel grid of high resolution. In each voxel intersecting the surface, we find the closest point to the voxel center and compute its normal orientation and color. We then coarsify this grid and average the positions, normals, and colors within each voxel. This provides a hierarchy of sparse voxel grids with geometric and semantic information.

% --- multi scale diffusion
% -- forward process

% We leverage this multiscale representation for our considered task: 3D shape variation from a single example. We use a hierarchy of diffusion models, as proposed by SinDDM for 2D images.
% Given the coarsest ground truth grid previously obtained, we compute a dense array of fixed dimensions and use it to train a first denoising diffusion model. Note that the latter operates on merely 10 features: positions, normals, colors, and a mask that will be used at inference time.
% For the other levels, we first compute an upsampling of the previous level. Contrary to SinDDM, we use a learned upsampler instead of trilinear interpolation. We then train separate models to deblur and denoise a slightly corrupted version of this upsampling. All of our models are independent and trained in parallel. Please refer to our paper for more details.

% -- reverse process

% At inference time, we first predict an initial coarse grid from pure noise. We then prune the grid according to the predicted mask, subdivide it, compute the upsampled values, slightly corrupt them, and feed the results to the next diffusion model. One advantage of our method is that the colored point cloud can be visualized during generation, providing feedback to the user and enabling interactive manipulations.

% --- meshing

% Contrary to SDF-based methods, we advocate for using a simple 3D shape representation that is close to raw data: oriented point clouds. While point clouds can be easily viewed and rendered, our method can also leverage off-the-shelf reconstruction methods, such as Poisson surface reconstruction (or APSS for open shapes) to produce 3D meshes.

% --- results

% Our method can be trained in minutes and evaluated in seconds on a single, consumer-grade GPU. Here is a visualization of our progressive generation. We can reconstruct a mesh and parametrize a texture based on the predicted points.
% Compared to previous work, ShapeShifter achieves better details and sharper surfaces. It greatly surpasses other methods in terms of geometric fidelity, while featuring a desirable diversity score: high for organic shapes and low for structured shapes. We thus claim that ShapeShifter is the new state-of-the-art for single-shape variation generation.
% Unlike previous methods, ShapeShifter also has the ability to generate open surfaces with boundaries. Our hierarchy of sparse grids can also be edited at different scales, and visualized at interactive rates with the efficient DDIM sampler. Here is a live example of a generative rescaling. Our progressive method is well suited to manipulations as it provides almost instant feedback to the user.

% --- conclusion

% We introduced ShapeShifter, a novel 3D generative model that combines the expressiveness of oriented point clouds with the efficiency of sparse voxel grids. While achieving state-of-the-art performances, our method is lightweight at both training and inference time. Thank you.



% UPDATED TEXT:

% ---Shapeshifter

% --- Introduction 

% We introduce ShapeShifter, a novel 3D generative model that learns to synthesize shape variations based on a single reference model. 

% While generative methods for 3D objects have recently attracted much attention, current techniques often lack geometric details and require long training times and large resources. In this paper, we tackle a task-specific, yet effective approach to synthesizing geometry: the generation of shape variations from a single high-quality example. 

% Central to our approach is our choice of 3D representation. Previous work has shown that point clouds can be efficiently leveraged in a learning pipeline, and displayed via point splatting. Compared to more traditional Signed Distance Fields or SDF, pointclouds also provide more flexibility, working seamlessly for open surfaces for instance. ShapeShifter proposes to combine the expressiveness of point cloud representations with sparse voxel grids for efficient computation. [shpuld we say generative blah blah instead of computation?] 

% --- 3D shape encoding 

% Given a 3D mesh, we first compute a high-resolution sparse voxel grid. For each voxel intersecting the surface, we find the closest surface point to the voxel center as well as its normal orientation and color. We then coarsify this grid and average the positions, normals, and colors within each coarser voxel. This provides a hierarchy of sparse voxel grids with geometric and semantic information. 

% --- multi scale diffusion
% -- forward process 

% We leverage this multiscale representation for our considered task: 3D shape variation from a single example. We use a hierarchy of diffusion models, as proposed by SinDDM for 2D images. 
% Given the coarsest ground truth grid previously obtained, we compute a dense array of fixed dimensions and use it to train a first denoising diffusion model. Note that we operate on 10 features only: positions, normals, colors, as well as a mask that will be used at inference time.

% For the other levels, we first compute an upsampling of the previous level. Contrary to SinDDM, we use a learned upsampler instead of trilinear interpolation. We then train separate diffusion models to deblur and denoise a slightly corrupted version of this upsampling. All of our models are independent and trained in parallel. Please refer to our paper for more details.

% -- reverse process 

% At inference time, we first predict an initial coarse grid from pure noise. We then prune the grid according to the predicted mask, subdivide it, compute the upsampled values, slightly corrupt them, and feed the results to the next diffusion model. One advantage of our method is that the colored point cloud can be visualized during generation, providing progressive feedback to the user and enabling interactive manipulations.

% --- meshing 

% Contrary to SDF-based methods, we advocate for the use of oriented point clouds. Not only can point clouds be easily viewed and rendered, but efficient off-the-shelf reconstruction methods exist to produce 3D meshes from them, such as Poisson surface reconstruction or APSS for open shapes. 

% --- results 

% Our method can be trained in minutes and evaluated in seconds on a single, consumer-grade GPU. Here is a visualization of our progressive generation. We can reconstruct a mesh and parametrize a texture based on the predicted points.

% Compared to previous work, ShapeShifter achieves better details and sharper surfaces. It greatly surpasses other methods in terms of geometric fidelity, while featuring desirable diversity scores, with high scores for organic shapes and low scores for structured shapes. We thus believe that ShapeShifter sets a new standard for high-quality single-shape variation generation.

% Unlike previous methods, ShapeShifter also has the ability to generate surfaces with boundaries. Our hierarchy of sparse grids can also be edited at different scales, and visualized at interactive rates with the efficient DDIM sampler. Here is a live example of a generative rescaling. Our progressive method is well suited to manipulations as it provides immediate feedback to the user.

% --- conclusion
 
% We introduced ShapeShifter, a novel 3D generative model that combines the expressiveness of oriented point clouds with the efficiency of sparse voxel grids.On top of achieving state-of-the-art performances and significantly improved geometric scores, our method is lightweight, leading to far reduced training and inference time. Thank you. 