\section{Related Work}
\label{sec:related}

% Our approach draws from several related works that we review next. \vspace*{-4mm}
\paragraph{3D Generation.} % Talk about 3D generation's trend, and relience on large data and exorbitant compute
The field of 3D generation has seen rapid development in recent years. Advances in generative models and large-scale 3D datasets have underpinned this progress. Generative adversarial networks (GANs)~\cite{goodfellow2020generative} have been widely used in works like~\cite{achlioptas2018learning, gao2022get3d, chan2022efficient}, while normalizing flows~\cite{rezende2015variational} were utilized in~\cite{yang2019pointflow}. Other approaches include variational autoencoders (VAEs)~\cite{kingma2019introduction} and autoregressive (AR) models~\cite{bengio1994learning, graves2013generating, van2016pixel}, as shown in~\cite{park2019deepsdf, zhang20223dilg, siddiqui2024meshgpt, yin2023shapegpt, nash2020polygen,chen2024meshanything}.
The recent introduction of diffusion models~\cite{sohl2015deep, ho2020denoising} has enabled training on larger datasets such as Objaverse~\cite{deitke2024objaverse}. A survey by Po \etal~\cite{po2024state} provides a comprehensive taxonomy of 3D diffusion approaches. A primary line of work builds on 2D diffusion models, generating multiview-consistent images through Score Distillation Sampling (SDS)~\cite{poole2023dreamfusion, wang2023score}. However, SDS faces practical challenges such as high optimization times~\cite{metzer2023latent, liang2024luciddreamer}, color artifacts~\cite{lukoianov2024score, chen2023fantasia3d, wang2024prolificdreamer, li2024sweetdreamer}, and 3D inconsistencies~\cite{liu2024syncdreamer, wang2024taming}.
Fine-tuning diffusion models on 3D assets for direct multiview output~\cite{shi2024mvdream, liu2023zero, qiu2024richdreamer, long2024wonder3d, lin2023magic3d} can address these issues, with further speedups through reconstructor networks for radiance fields~\cite{li2024instantd, chen2023single, liu2024one, wei2024meshlrm, wang2024pflrm} or Gaussian splats~\cite{zhang2025gs, xu2024grm, zou2024triplane}. However, photometric losses often lead to geometric artifacts.
A separate direction directly trains 3D diffusion models on 3D data~\cite{luo2021diffusion, nichol2022point} or encodes 3D data through autoencoders~\cite{cheng2023sdfusion, gupta20233dgen, zhang20233dshape2vecset,vahdat2022lion, jun2023shap, zhao2024michelangelo, ren2024xcube, ren2024scube, zhang2024clay}. These methods demand extensive, high-quality data and substantial computational resources, and the generated geometry, while improved, still lacks the geometric details required in real-world 3D applications. Although Xcube~\cite{ren2024xcube} initially proposed using an efficient multiscale diffusion pipeline, it relies on pre-trained VAEs and verbose SDF representation which are not adapted to the problem of single-shape variations.
We adopt an alternative approach which generates 3D assets with high-quality geometry from a single exemplar, trainable on a single GPU in minutes, while enabling user control over output shapes.

\begin{figure}[!t]
    \includegraphics[width=\linewidth]{figures/overview/overview2.pdf}\vspace*{-4mm}
    \caption{\emph{Multiscale diffusion on sparse voxel grid.} We start from noise \(\rvepsilon\!\sim\!\mathcal{C}\left( \vzero,\mI \right)\) at the coarsest level \(\lvl\!=\!1\), and obtain the 3D feature grid \(\grid^{\lvl}\) through reverse diffusion. Each subsequent level uses the output of the previous level. Inactive voxels are first pruned, then upsampled with a level-specific upsampler \(\upsampler^{\lvl}\). The upsampled grid \(\blurredGrid{\vphantom{\grid}}^{\lvl}\) is subsequently noised and passed through the diffusion model to obtain a clean version of the sparse feature grid \(\grid^{\lvl}\). All levels are independent and can thus be trained in parallel. \vspace*{-5mm}} \label{fig:overview}
\end{figure}

\vspace*{-5mm}
\paragraph{Generation from a single instance.}
% - first sentence: first talk about the need for single instance generation. computation cost. high quality data for 3D is still scarce. Learning from a user-provided high-quality asset gives the user more control.
% This concept has been extensively explored in 2D image and texture generation.
% - Patch-based methods without learning, {state the general idea: search matched patches and the blend}. \cite{}
% - On the other hand SinGAN and follow-ups \cite{} demonstrated it is possible to train generative models on a single example.
% - mention SinFusion showed that the diffusion can be used to train such a model too. Seems more stable than GAN.
Despite advancements in large-scale 3D generation, high-quality 3D data remains scarce, and computational costs for training and inference are significant. Generating 3D content from a single high-quality example offers an appealing alternative, giving users control through concrete exemplar inputs. Non-learning, patch-based methods such as PatchMatch~\cite{han2008multiscale,barnes2009patchmatch} and more recent works~\cite{granot2022drop, elnekave2022generating} produce variations by finding and blending similar patches within the exemplar.
In contrast, SinGAN~\cite{shaham2019singan} and its successors~\cite{shocher2019ingan, hinz2021improved} have shown that generative models can be trained on a single example. More recently, SinFusion~\cite{nikankin2023sinfusion, kulikov2023sinddm} demonstrated that diffusion models, known for their stability, can also be adapted to this approach. Applications of these single-instance models include texture synthesis~\cite{zhou2018non, niklasson2021self, rodriguez2022seamlessgan, Mitchel_2024_CVPR}, video synthesis~\cite{haim2022diverse}, and more.

In 3D generation, similar approaches have emerged. Herz \etal~\cite{10.1145/3386569.3392471} applied SinGAN with mesh convolutions~\cite{hanocka2019meshcnn} to enhance surface details without altering structure, while works like Son \etal~\cite{son2023singraf} and Karnewar \etal~\cite{karnewar20223ingan} generated radiance fields from a single instance. Wu \etal~\cite{wu2022learning} used 3D convolutions to generate significant variations, later extending their work to include texture synthesis~\cite{wu2024sindm}, though details are often degraded due to 3D convolution bottlenecks. These methods, however, are typically slow, with training times ranging from 2 to 4 hours.
To bypass training, Li \etal~\cite{li2023patch} adopted a PatchMatch-based method for 3D scenes represented by plenoxels~\cite{fridovich2022plenoxels}. Although this removes the training step, it still requires approximately 10 minutes per variation and struggles to maintain geometric quality due to challenges in converting the plenoxelsâ€™ occupancy fields to signed distance functions.
By combining the strengths of a recent framework for learning sparse spatial information~\cite{williams2024fvdb} with compact, geometry-centric features, 
%m@; calls for questions... %that do not require learning, 
our method achieves higher geometric fidelity while requiring less than 12 minutes for the entire generation process (including training and inference). 


\vspace*{-4mm}
\paragraph{3D Representation.}
% - we differentiate the discussion to output representation and internal representation (3D feature).
% - as of output representation, there are many many types. NeRF and GS doesn't put geometry in the center stage, the output geometry is often distorted\cite{}.
% SDF and occupancy field are limited to water-tight shapes\cite{}, and SDF couldn't really capture sharp features\cite{}. Mesh is rising to be a new forefront with AR model, but seems to be limited to small set of faces, and the representation couldn't be easily combined to include texture information\cite{}.
% - for the internal representation, most work use learned features. encoding is a separate step. costly for training, encoding quality is a weakness when the training data is limited, a challenge for single-example generation\yifanrmk{ablation on using a trained encoder to extract features}.
% - we use grounded geometric features for output and internal representation. Represented the point positions, normals, and colors. Directly extracted from the 3D input, which is not lossy, and geometry-forward, can reconstruct sharp features and open surfaces \yifanrmk{Even though the voronoi extraction doesn't work very well, we should demonstrate a couple of examples that show the sharp features and open surfaces that are not reconstructed by poisson. maybe RIMLs?}
% - This not significantly shortens the training, but also allows us to achieve higher geometric quality at a fixed compute budge.

% In contemporary 3D generative models, the output and internal representation (3D feature encoding) can take different forms. For output representations, there are numerous options. NeRF~\cite{mildenhall2021nerf} and Gaussian Splatting (GS)~\cite{kerbl20233d} do not prioritize geometry, often concealing geometric artifacts under the texture~\cite{chan2022efficient,tang2025lgm}.
% Signed Distance Functions (SDFs)~\cite{park2019deepsdf} and occupancy fields~\cite{mescheder2019occupancy}, while widely used, are restricted to watertight shapes and struggle to capture sharp features. Mesh-based representations have recently regained attention with the advent of autoregressive (AR) models~\cite{siddiqui2024meshgpt,chen2024meshanything}; however, they are typically limited to a small set of faces and cannot easily include texture information.
% For internal representation, most methods, including prior work\cite{wu2024sindm}, rely on learned features, where encoding is a separate, costly step.
% % This dependency makes training costly and often degrades encoding quality in data-limited or single-example scenarios~\yifanrmk{we should probably show this with an ablation, \eg replace our features with a learned one. remove this sentence if it's not true.}.
% By contrast, we employ a simple, geometry-centered representation for both output and internal features, using surface point positions, normals, and colors extracted directly from the 3D input. Our experiments demonstrate that this straightforward yet flexible representation generates detailed geometry and open surfaces, is easy to edit, and significantly reduces training time.

% Earlier 3D representation focuses on geometry only. The common representation include Voxels~\cite, 3D points~\cite{pointnet}, Mesh, SDF~\cite{park2019deepsdf} and occupancy field~\cite{mescheder2019occupancy}.
% Differentiable rendering enabled joint modeling and generation of geometry and appearance, notably neural mesh rasterization~\cite{}, splatting methods~\cite{zhang2025gs,}, to neural radiance field~\cite{mildenhall2021nerf}. Opened new applications, bridged the gap between modeling and rendering. In lieu of 3D data, these were important to leverage 2D asset~\cite{eg3d}. With textured large-scale dataset becoming available, these have also become the mainstream approach for generation and reconstruction~\cite{lrms}. Arguably, using appearance information during generation gives more semantic context, which could in turn improve geometry.
% However, appearance signal is inarguable different from the geometry signal, \eg texture displays higher frequency. The remedy the difference, existing approaches typically use separate branches~\cite{}. Still, binding the texture generation with geometry generation imposes unnecessary resolution requirement on the geometry, slowing down the generation, which is critical for single-exemplar generation.
% Recently, Clay~\cite{zhang2024clay} separate the aspect entirely, generating geometry first and then 2K texture resolution, exceeding existing general purpose large-scale generation both in geometry and texture quality. Similarly, Some other approaches adopt two-stage approach, where the texture is refined in 2D texture space after geometry generation. These methods show that 2D texturemap is potentially a much more efficient vessel for texture synthesis. follow this philosophy, we focus on the geometry generation part, where we believe the current 3D generation most suffers. However, unlike clay, which generates geometry entirely irrespective from texture, we use rgb features embed more contextual information, which is important for our single-exemplar application, where the data-prior is weak compared to large-scale generation.
% We show that by doing so, the generated 3D shape contains superior geometry, and paired with off-the-shelf image super-resolution and texture synthesis methods, user can create ultra detailed 3D shapes with crisp geometry and HD texture. Given a fixed computation budge, we believe this is the arguable the optimal allocation to achieve the highest possible quality result.

Earlier 3D representations focused solely on geometry, using formats like voxels~\cite{wu20153d}, 3D points~\cite{qi2017pointnet}, meshes~\cite{hanocka2019meshcnn, maruani_voromesh_2023, maruani_ponq_2024}, SDFs~\cite{park2019deepsdf}, and occupancy fields~\cite{mescheder2019occupancy,Chen_2019_CVPR}.
Differentiable rendering later enabled joint modeling of geometry and appearance, spanning from neural mesh rasterization~\cite{kato2018neural,liu2019soft} and splatting methods~\cite{yifan2019differentiable,wiles2020synsin,kerbl20233d} to neural radiance fields~\cite{lombardi2019neural,mildenhall2021nerf}.
This integration allowed the use of 2D assets~\cite{chan2022efficient,schwarz2020graf} and facilitated large-scale generation and reconstruction~\cite{xu2024dmvd,xu2024grm,wei2024meshlrm,tang2025lgm,hong2024lrm}, enhancing semantic context for geometry generation. However, appearance signals often exhibit higher frequencies than geometry, leading existing methods to separate texture and geometry branches~\cite{mildenhall2021nerf}. Tying texture generation to geometry imposes unnecessarily high resolution requirement on geometry, slowing single-exemplar generation~\cite{wu2024sindm}.
Other works generate geometry first, before refining the appearance in 2D texture space at higher resolutions~\cite{huo2025texgen,yang2024dreammesh,zeng2024paint3d}. Recently, Clay~\cite{zhang2024clay} fully separates geometry and appearance aspects, achieving state-of-the-art quality in both. Following this idea, we focus on high-fidelity geometry generation, which is the weakest aspect of current 3D generation approaches. Unlike Clayâ€™s strict separation, however, we use RGB features to add contextual information-â€”crucial for single-exemplar generation where data priors are weaker.

Our results show that this approach produces superior geometry which, when combined with state-of-the-art image super-resolution~\cite{kang2023scaling,li2022srdiff} and texture synthesis~\cite{richardson2023texture,Cao_2023_ICCV}, allows for highly detailed shapes with sharp geometry and HD textures. We argue that this design, for a fixed computational budget, optimally balances quality and efficiency.

