%! TEX root = ../main.tex

\section{Method}
\label{sec:method}

Our goal is to train a generative model from a single 3D input mesh to generate new variations efficiently.
We use a multiscale diffusion model with limited receptive fields to learn the internal structures of the given shape, adapting an approach that has been used for training a generative model on a single image~\cite{kulikov2023sinddm}.
We use compact, explicit 3D features directly extracted from the exemplar shape for diffusion.
These features are encoded in a sparse voxel grid, and processed efficiently using a specialized 3D convolution framework (fVDB~\cite{williams2024fvdb}) to capture fine-scale geometric details without incurring high memory cost.
We introduce our 3D features in \cref{sec:sparse-representation}, the hierarchical diffusion model in \cref{sec:model}, and the final meshing process of a generated output in \cref{sec:meshing}.
% captures the geometry at multiple scales within the shape. To accomplish this, we will employ a diffusion process, which models the progressive conversion of the signal into white Gaussian noise, then learns to invert this process to sample new geometry. In our approach, we will use a hierarchical representation, following an approach that has been used for training a generative model on a single image~\cite{kulikov2023sinddm}. We additionally employ a sparse representation that allows us to capture fine-scale mesh detail without incurring the high memory cost of a dense 3D representation. We also model features explicitly instead of learning a new feature embedding, which both improves the sharpness and scale of the generated geometry and allows us to train each level of the model in parallel.
\vspace*{2mm}

\subsection{3D Representation}
\label{sec:sparse-representation}

\paragraph{Explicit Multiscale 3D Features.}
Our method employs \emph{explicit} 3D information to encode the geometry of the input exemplar mesh at multiple scales. They are composed of merely 10 values per voxel of a sparse voxel grid,
\begin{equation}
\feat = (\vp_x, \vp_y, \vp_z, \vn_x, \vn_y, \vn_z, \vc_r, \vc_g, \vc_b, \mask),
\end{equation}
where \((\vp_x, \vp_y, \vp_z)\!\in\! [-0.5, 0.5]^3\) represents a point sample encoded as an offset from the voxel center, \((\vn_x, \vn_y, \vn_z)\!\in\! [-1, 1]^3\) represents the local normal of the underlying geometry associated to the point sample,
\((\vc_r, \vc_g, \vc_b)\!\in\! [-2, 2]^3\) represents the color scaled up to the \([-2, 2]\) range,
and \(\mask\!\in\! [-1, 1]\) is a scalar indicating if the voxel contains the mesh surface, which we will use as a mask to prune voxels after refinement (see \cref{sec:model}).
The value ranges of the feature components were empirically chosen since feature scale can be important in diffusion models~\cite{chen_importance_2023}. 

These features are extracted from the different scales of the input mesh.
%, save for the mask scale which will only be useful during training and inference. %m@ the mask is useless here, here this addition to the text... But we can leave it as is, it ain't hurting
Specifically, starting from the finest scale \(\maxLvl\), for each voxel that intersects the surface, we find the nearest surface point to the voxel center, whose position, normal, and color are used to form the feature. The mask is set to 1 for all selected voxels as they contain the surface.
At each subsequent scale \(\lvl\!<\!\maxLvl\), we obtain coarser points, normals, and colors features through a \(2^{3}\) average pooling, and the mask value through max pooling. To better preserve sharp features, we average the point positions in the Quadric Error Metric sense~\cite{maruani_ponq_2024} (see supplemental material for details).
%\yifanrmk{I removed mentioning about QEM here, Nissim told me that we do average pool for all intermediate levels.}

This 3D representation yields three advantages:
\begin{itemize}%[(1)]
    \item it captures surface details in a compact form and carries contextual information from the texture;
    \item it encodes the 3D shape explicitly at each level, which enables a generated shape to be easily visualized or even edited at every level in a coarse-to-fine fashion;
    \item it is extracted from an input exemplar efficiently and deterministically, and will allow us to train each level of our model in parallel. 
    %%%\mathieurmk{CAREFUL, Nissim says we may end up using the learned features... OLD TEXT: saving the feature learning step, which typically takes us a substantial portion of the training time and slows down inference time~\cite{wu2024sindm}}.
\end{itemize}

\begin{algorithm}[t]
    \caption{Training at level \(\lvl\)}
    \label{alg:forward-diffusion}
    \KwIn{Extracted sparse features $\{\grid^{1}, \cdots, \grid^{\maxLvl}\}$}
    // Upsampler Training for Levels 2 to $\maxLvl$\\
    \Repeat{convergence}{
        Update model $\upsampler^{\lvl}$ with the loss \cref{eq:upsampler-loss}
        % $\lVert \upsampler^{\lvl}(\grid^{\lvl-1}) - \grid^{\lvl} \rVert^2$\\
    }
    // Diffusion Model Training\\
    \Repeat{convergence}{
        $t \sim \text{Uniform}(0, T)$\\
        $\rvepsilon \sim \mathcal{N}(0, \mathbf{I})$\\
        \eIf{$\lvl = 1$}{
            $\grid^{\lvl, \text{mix}} = \grid^{1} $ \\
        }{
            $\blurredGrid^{\lvl} = \upsampler^{\lvl}\left( \grid^{\lvl-1} \right)$\\
            $\grid^{\lvl, \text{mix}} = \gamma(t) \grid^{\lvl} + (1-\gamma(t)) \blurredGrid^{\lvl}$\\
        }
        $\grid_{t}^{\lvl} = \sqrt{\bar{\alpha}(t)} \grid^{\lvl, \text{mix}}  + \sqrt{1-\bar{\alpha}(t)} \rvepsilon$\\
        Update model with $\lVert\model^{\lvl}(\grid^{\lvl}_t | t) - \grid^{\lvl}\rVert^2.$
        % Update model $\model^{\lvl}$ with the loss \cref{eq:diffusion-loss}
        % $\lVert \model^{\lvl}(\grid^{\lvl}_t \mid t) - \grid^{\lvl} \rVert^2$
    }
\end{algorithm}

\paragraph{Sparse Voxel Grid.}
The inductive biases of convolutional neural networks exploit the shared information across internal crops within the input data, which is essential to learning from a single example and prevents overfitting~\cite{shaham2019singan,nikankin2023sinfusion,kulikov2023sinddm}.
However, 3D convolutional operations are notoriously expensive computationally and memory intensive.
To address this issue, we leverage fVDB~\cite{williams2024fvdb}, a recently proposed framework that supports efficient operations on sparse voxels. 
% \nissimrmk{Instead of storing a dense array of values, fVDB stores an array of indices (Nx3) along with their features (NxF). As a result,} 
As a result, only active features are stored and processed, which significantly reduces the memory footprint and computational complexity.
% A voxel is set to ``inactive'' if the mask value is less than 0, and only active features are stored and processed, which significantly reduces the memory footprint and computational complexity.
We denote the sparse feature grid storing active 3D features at level \(\lvl\) as \(\grid^{\lvl} = \left\{ \feat^{\lvl} \right\}\).

\vspace*{2mm}
\subsection{Multiscale Diffusion}\label{sec:model}
Our multiscale diffusion pipeline generalizes SinDDM~\cite{kulikov2023sinddm} to 3D and adapts it to properly work with sparse voxel grid. 
As shown in \cref{fig:overview}, the pipeline consists of multiple diffusion models \(\{\model^\lvl\}_{1\leq \lvl \leq L}\).
During training, these diffusion models can be trained in parallel; at inference time, new variations are generated by sequentially running the reverse diffusion in a coarse-to-fine manner.
% As shown in \cref{fig:overview}, the model consists of multiple levels of diffusion processes, which is defined through a Markov Chain~\cite{ho2020denoising} that gradually converts the distribution of a random variable, \(\feat^{\lvl}\) in our case, into the normal distribution.
% At inference, the model samples from the normal distribution and inverts the diffusion process to generate new samples.
Below, we explain the hierarchical multi-scale diffusion and highlight our design differences compared to SinDDM.
% \paragraph{Noise scheduling.} As in SinDDM, we use a global number of steps $T$ with a noise scheduler $\bar{\alpha}(t)$ that decreases monotonically from 1 to 0 as to grows from $0$ to $T$. For upsampling, 

\begin{algorithm}
    \caption{Sampling}
    \label{alg:reverse-diffusion}
    \KwIn{Choice of sampler $\mathcal{S} \in \{\text{DDPM}, \text{DDIM}\}$}
    % \KwOut{Denoised sparse grid $\grid_{\maxLvl}$}
    \KwOut{Generated sparse grid $\grid_{\maxLvl}$}

     \emph{// Upsampler training for levels 2 to $\maxLvl$}\\
    $\grid_T^{1} \sim \mathcal{N}(0, \mathbf{I})$\\

    \For{$\lvl \leftarrow 1$ \KwTo $\maxLvl$}{
        % \nissimrmk{SAY THAT WE CAN SKIP STEPS WITH DDIM} \\
        \If{$\lvl > 1$}{
            $\rvepsilon \sim \mathcal{N}(0, \mathbf{I})$\\
            $\blurredGrid^{\lvl} = \upsampler^{\lvl}(\grid^{\lvl-1})$\\
            $\grid_{T[\lvl]}^{\lvl} =  \sqrt{\bar{\alpha}(T[\lvl])} \blurredGrid^{\lvl}  + \sqrt{1-\bar{\alpha}(T[\lvl])} \;\rvepsilon$\\
        }
        \For{$t \leftarrow T[\lvl]$ \KwTo $1$}{
            $\rvepsilon \sim \mathcal{N}(0, \mathbf{I})$\\
            $\grid_{t-1}^{\lvl} = \mathcal{S}(\model^l, \grid_{t}^{\lvl},\bar{\alpha}, \rvepsilon, t)$\\
        }
        $\grid^{\lvl} = \textbf{Prune}(\grid_{0}^{\lvl})$\\
    }
\end{algorithm}

\vspace*{-4mm}
\paragraph{Forward Multiscale Diffusion.}
Except at its coarsest level \(\model^1\), our diffusion model $\model^{\lvl>1}$ generates the signal of the current level based on the output of the previous \((\lvl-\!1)\) level. 
This initial guess is obtained by upsampling the output from the previous level \(\smash{\blurredGrid{\vphantom{\grid}}^{l} = \upsampler\bigl( \grid^{l-1} \bigr)}\), which can be seen as a ``blurred'' version of \(\grid^{\lvl}\). 
This means that, for $l\!>\!1$, the diffusion model not only needs to denoise but also ``deblur" during sampling.
As a result, SinDDM modifies the forward diffusion process to be 
\vspace*{-2mm}
% The diffusion step at \(t\in [0, T]\) is defined as
\begin{equation}
    \grid^{\lvl}_t = \!\sqrt{\bar{\alpha}(t)} \left( \gamma(t) \grid^{\lvl} \!+\! (1\!-\!\gamma(t)) \blurredGrid^{\lvl} \right) \!+\! \sqrt{1\!-\!\bar{\alpha}(t)}\,\rvepsilon, \vspace*{-2mm}
\end{equation}
where \(\rvepsilon \!\sim\! \mathcal{N}(\vzero, \mI)\), while \(\bar{\alpha}\left( t \right)\) and \(\gamma\left( t \right)\) are monotocally decreasing functions from 1 to 0 as \(t\) grows from 0 to \(T\). 
The model learns to denoise the corrupted feature \(\feat^{\lvl}_t\) at time step \(t\) by minimizing the reconstruction loss 
\begin{equation}
    \lVert\model^{\lvl}(\grid^{\lvl}_t | t) - \grid^{\lvl}\rVert^2. \label{eq:diffusion-loss}
\end{equation}

% In SinDDM, an initial prediction obtained by upsampling the output from the previous level, \(\blurredGrid^{l} = \upsampler\left( \grid^{l-1} \right)\) (for $l>1$). 
% %, which is then slightly corrupted. 
% This upsampling can be seen as a ``blurred" version of $\grid^{l}$. This means that, for $l>1$, the diffusion model not only needs to denoise but also needs to ``deblur" the artifact from the upsampling process.
% The diffusion step at \(t\in [0, T]\) as
% \begin{equation}
%     \grid^{\lvl}_t = \sqrt{\bar{\alpha}} \left( \gamma(t) \grid^{\lvl} + (1-\gamma(t)) \blurredGrid^{\lvl} \right) + \sqrt{1-\bar{\alpha}}\rvepsilon,
% \end{equation}
% where \(\rvepsilon \sim \mathcal{N}(\vzero, \mI)\), \(\alpha\left( t \right)\) and \(\gamma\left( t \right)\) are monotocally decreasing functions from 1 to 0 as \(t\) grows from 0 to \(T\). 
% The model learns to denoise the corrupted feature \(\feat^{\lvl}_t\) at time step \(t\) by minimizing the reconstruction loss
% \begin{equation}
%     \lVert\model^{\lvl}(\grid^{\lvl}_t | t) - \grid^{\lvl}\rVert^2.\label{eq:diffusion-loss}
% \end{equation}

\noindent Contrasting with SinDDM which employs a bilinear upsampler as \(\upsampler\), we use a level-specific upsampler \(\upsampler^{\lvl}\) motivated by the fact our spatial features (points and normals) are extracted by projecting the voxel centers onto the mesh surface --- thus potentially exhibiting abrupt local changes.
%, and a learnable upsampler can better adapt to these changes, 
This results in improved preservation of sharp geometric features as we show in \cref{sec:experiments}.
The upsampler \(\upsampler^{\lvl}\) is trained to minimize the \(\normltwo\)-loss between upsampled and ground-truth features, \ie, 
\begin{equation}
\lVert \upsampler^{\lvl}(\grid^{\lvl-1}) - \grid^{\lvl} \rVert^2. \label{eq:upsampler-loss}
\end{equation}

% Furthermore, since we want the model at the coarsest level (\(\lvl=1\)) to be able to generate points where the coarsest grid is inactive, we need to "reactivate" the inactive voxels in the output of the upsampler. Therefore we ``blur'' the values of active voxels to fill the ones in the dense grids that were not present in the sparse grid \yifanrmk{how do we blur?}. The blur function is applied to all but the mask feature, which is set to $-1$ to distinguish them from the original active cells.

% \paragraph{Parallel Training.}  
Crucially, the training of different levels can be parallelized. For each level \(l>1\), we first train the upsampler and the diffusion model as summarized in \cref{alg:forward-diffusion}.
% as shown in \cref{alg:forward-diffusion}

\noindent Unlike SinDDM, training needs to accommodate our use of sparse grids.
% Specifically, in fVDB, two sparse feature grids (additions, multiplications..) are valid if they share the same indices array.
When comparing the denoised sparse feature grid and the ground-truth sparse feature grid (\cref{eq:diffusion-loss}), the denoised grid can contain more active voxels (see dark voxels in \cref{fig:overview}, even though their mask could be -1 --- yet
fVDB operations on two sparse feature grids assume that they have the same active voxels. 
To solve this problem, we flood those inactive voxels in the ground-truth \(\grid^{\lvl}\) with feature values of the nearby active cells using a blurring kernel. All features except the mask value are flooded in this way, whereas the mask value is set to -1. Empirically, we observe that soft blending the feature values this way (instead of hard setting the values to an arbitrary number or applying an additional mask for loss) achieves the best result. 
% \yifanrmk{actually, now i think about it, you could have created a mask when you compute the difference, essentially, the loss at these additional voxels can be set to zero, so they don't participate in the supervision. you can do this for the feature except mask, and the mask is supervised like normal.}
% \paragraph{Sparse Grid Handling.} Strictly speaking, operations on two sparse feature grids (additions, multiplications..) are valid if they share the same indices array. When upsampling, the predicted grid contains more voxels than the fine ground-truth grid (see \cref{fig:overview}). In practice, we compute a completed grid $\Bar{\grid}^{l}$ that contains $\grid^{l}$, and interpolate its values on voxels not present in $\grid^{l}$ (using a blurred kernel). For clarity, we identify $\Bar{\grid}^{l}$ with $\grid^{l}$ in this section. See supplemental material and code for more details.  

\paragraph{Reverse Multiscale Diffusion.}
Once trained, we can apply standard DDPM~\cite{ho2020denoising} or DDIM~\cite{song2021denoising} sampling sequentially from levels 1 to \(\maxLvl\).
As outlined in \cref{alg:reverse-diffusion}, we start from a noise \(\rvepsilon\!\sim\! \mathcal{N}\left(\vzero, \mI\right)\)  and run the reverse sampling to obtain an initial prediction at the coarsest level.
Then, for each level, we first prune the predicted inactive voxels from the previous level by removing any feature entries with mask value $m \!<\! 0$.
The resulting feature grid is then upsampled with \(\upsampler^{l}\), and subsequently corrupted with noise, before being given to the diffusion model for reverse sampling.
Similar to SinDDM, we only add noise up to timestep $T[l] \!<\! T$ to prevent destroying the prediction from the previous level. 
A schematic overview of the sampling process is illustrated in \cref{fig:overview}. 

% \paragraph{Model Architecture.} Like all prior single-exemplar generation methods~\cite{shaham2019singan,kulikov2023sinddm}, the receptive field of the diffusion model is kept small to prevent the model from overfitting to the fixed global structure and enable the model to learn the internal statistics across different regions of the exemplar. 

% \noindent\emph{Different to SinDDM}, we train the diffusion models of all levels on random crops of the same size. This helps us ensure that all scales can be trained in roughly the same time. \yifanrmk{maybe we can remove this? there may be other ways to ensure each level is trained about the same time, like stop training after x minutes. Or move it to implementation details.}

% \begin{itemize}
%     \item Start from pure noise and denoise to obtain $\hat{J}_0$
%     \item For m in range(1, ... M):
%     \item       Prune voxels from $\hat{J}_{m-1}$ where the mask is $<0$

%     \item       Compute $\tilde{x}_0 = U_m(\hat{J}_{m-1})$
%     \item       Slightly corrupt to obtain ${x}_{T_m}$
%     \item       Denoise to obtain $\hat{J}_{m}$
% \end{itemize}

\subsection{Meshing}\label{sec:meshing}
Once a new geometric variant has been created, we can directly visualize the generated shape using the points (one per finest voxel in the fVDB data structure) along with their associated normal and color. We can also trivially generate a mesh of the geometry through Poisson reconstruction~\cite{kazhdan_screened_2013} (or APSS~\cite{guennebaud_algebraic_2007} if we are dealing with open surfaces). One can assign colors to the mesh nodes based on the output colors, bake texture maps (as used sporadically in figures), or further refine and stylize the texture with off-the-shelf image enhancement models (see \cref{sec:exp-texture}).  

