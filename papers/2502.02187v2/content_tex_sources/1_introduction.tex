\section{Introduction}
\label{sec:intro}

%Yifan text
\silenced{
First paragraph (what's our task):
\begin{enumerate}
\item  The problem we are addressing is 3D Generation. 
\item  This space is dominated by very few players, because of resource-intensive nature of this task. At the same time, the geometric quality is a weakpoint because the quality variance in existing 3D dataset is enormous, and among all available training data, there are few with detailed geometric features. 
\item We take a different approach: instead of training from large dataset, we learn to generate variations from a single input shape. 
\end{enumerate}}

Creating 3D content through generative models is currently attracting significant attention. Traditional 3D modeling demands both time and specialized skills to create complex shapes, whereas advancements in generative AI promise a broader exploration of design possibilities, free from the usual constraints of time or technical expertise. However, current 3D generative models have numerous shortcomings that limit their usefulness in applications such as movies, gaming, and product design. First, state-of-the-art methods often struggle to produce the fine geometric details and sharp features necessary for digital shapes in geometric modeling. Additionally, these models require large, high-quality 3D datasets, which are significantly more challenging to curate compared to image datasets, and involve long training times and substantial computational resources.

%Yifan text
\silenced{
Second paragraph (what do existing work):
\begin{enumerate}
    \item Existing method could generate visually OK 3D asset, but the geometry struggles to reach the comparable level of quality. 
    \item The main cause is that their representation is not geometry-centric. They use occupancy field~\cite{wu2022learning, li2023patch, son2023singraf, zhao2024michelangelo} or signed distance functions~\cite{wu2024sindm, zhang20233dshape2vecset} as the geometric representation, smoothing out sharp features. \nissimrmk{This means they use VOLUME supervision whereas we use SURFACE supervision which is much faster} At the same time, supervised with through volumetric rendering ~\cite{son2023singraf, li2023patch}. Often times, texture hides the geometry artifacts.
    \item They are also very slow, requiring hours of training and (or) long inference time.
\end{enumerate}}

\begin{figure}[!ht] 
    \centering
    % \includegraphics[width=\linewidth]{figures/summary.jpg}
     \includegraphics[width=\linewidth]{figures/summary-new.pdf}\vspace*{-1mm}
    \caption{\emph{\ourmethod.} Given a 3D exemplar, we propose to train a hierarchical diffusion model to create variations preserving the \emph{geometric details and styles} of the exemplar. By combining compact yet explicit 3D features (colored, oriented points) with a sparse voxel grid, we shorten training times from hours to minutes, while yielding significantly better geometric quality than prior work. The hierarchical point representation and fast inference times further enable intuitive interactive editing.\vspace*{-5mm}}
    \label{fig:enter-label}
\end{figure}

\begin{figure*}[t]
    \centering
      \includegraphics[width=0.99\linewidth]{figures/figure2/comparison2.pdf}\vspace*{-5mm}
      \subfloat[Input Geometry]{\hspace{.25\linewidth}}
      \subfloat[Ours]{\hspace{.25\linewidth}}
        \subfloat[Sin3DGen]{\hspace{.25\linewidth}}
              \subfloat[Sin3DM]{\hspace{.25\linewidth}}
    \vspace*{-2.5mm}
    \caption{\emph{Geometric details.} Our generation captures significantly more geometric details present in the exemplar mesh (leftmost). Prior work, Sin3DGen~\cite{li2023patch} and Sin3DM~\cite{wu2024sindm}, operates with plenoxels and neural radiance fields encoded in single-resolution triplane features, respectively, which lack the capability to sufficiently represent and supervise high-resolution geometric details. In contrast, our method employs a colored and oriented point set, providing precise geometric information.\vspace*{-3mm}}
    \label{fig:results}
\end{figure*}

In this paper, 
%we do not tackle the generic case of 3D shape generation, but a more task-specific, yet effective approach to synthesizing geometry
we tackle a task-specific, yet effective approach to synthesizing geometry: we propose generating shape variations from a single high-quality example. This lesser-explored generative method offers several benefits beyond avoiding the curation of large training datasets: it has the potential to provide a resource-efficient way to generate shape variants for retargeting or editing, automatically inheriting the style, symmetries, semantics, and geometric details from the exemplar.
Although existing generative methods from exemplars are able to create varied 3D assets, they struggle to produce \emph{clean and detailed geometry} due to their reliance on occupancy fields~\cite{wu2022learning, li2023patch, son2023singraf} or signed distance functions~\cite{wu2024sindm} (which smooth out geometric features), or because they are supervised through volumetric rendering~\cite{li2023patch,son2023singraf} (which often leads to large geometric artifacts) --- and without a clean geometric output model, the use of 2D textures to further enhance visual complexity is severely hindered. Consequently, existing exemplar-based methods are relative slow in generating variations of the input as they rely on volumetric sampling within the surface's neighborhood~\cite{wu2022learning,son2023singraf,wu2024sindm}. 
%\yifanrmk{should we mention anything related to their need to learn feature? }
% m@: Nissim says that he may end up using learned features, so maybe not mention that....
%Yifan text
\silenced{
Third paragraph (what do we do differently):
\begin{enumerate}
    \item We sets out to improve the geometric resolution of existing approaches and at the same time make the training and inference practical real-world applications. 
    \item Our strategy is to consider the geometry generation separately from texture generation. This is because texture signal significantly differs from the geometric signal in frequency and scale. It is much more efficiently handled in the 2D space as texture maps as demonstrated several state-of-the-art general-purpose 3D generation methods~\cite{zhang2024clay}. Forcefully combining the two generation steps together paralyzes the geometry generation when the computation budge is limited. 
    \item The key is to revisit simple and explicit geometry features. These features can be directly extracted from the input geometry as training input and reference signal, alleviating the need to for a costly feature encoder, alas saving significant training and inference time. Our feature is composed or simple point positions, normals, and optionally RGB colors for additional semantic information. It turns out that this simple and compact representation was perfectly capable of creating fine grained geometric details exceeding prior work by large margin. At the same time, they can be easily reconstructed to surface meshes, including open surfaces, which was a limitation of prior work.
    \item We pair the representation with a multi-scale 3D convolution diffusion network to enable different levels of control. While 3D convolution is notoriously computationally expensive, our implementation adopts a recently proposed spatial learning framework fVDB~\cite{williams2024fvdb}, which shortens training time 20 times and achieves the quasi real-time inference.
\end{enumerate}}

We propose a novel technique, that we call \ourmethod, to synthesize high-quality shape variations of an input 3D model, with training and inference times well suited for practical real-world applications. We use points (with their normals and optionally colors for additional semantic information) as our lightweight and efficient base geometric representation~\cite{prokudin_dynamic_2023}, which we pair with a multiscale 3D diffusion network. While these explicit surface features already streamline the generative process and help preserve geometric details, we propose to significantly reduce training times and achieve interactive inference rates by adopting sparse convolutions based on fVDB~\cite{williams2024fvdb}, a recent spatial learning framework based on sparse voxel grids.
Mixing point sampling and sparse convolutions, a novel combination in generative modeling, results in a multiscale generative approach capable of producing 3D variants of shapes of different styles and topologies. Furthermore, its fast inference allows for interactive editing control.
%Yifan text
\silenced{
Fourth paragraph (contribution):
\begin{enumerate}
    \item Much higher geometry quality 
    \item Much shorter training time 
    \item Multi-level editability
    \item Open surfaces
    \item Can be combined with off-the-shelf 2D image suepr-resolution to generate HD texture, exceeding prior work in visual details. Flexible to be coupled with various texturing sythesis methods such as ~\cite{richardson2023texture} for more stylized texture creating.
\end{enumerate}}

\vspace*{-5mm}
\paragraph{Contributions.} This paper proposes a neural network approach to generating high-quality shapes from a single 3D reference example. Compared to previous exemplar-based generative methods, we demonstrate significantly improved geometric quality of our outputs, as shown in \cref{fig:results}. Moreover, the simplicity of our geometric representation (using point sampling in a sparse voxel grid) and its hierarchical refinement (learned per level in parallel) to control and generate variations of an arbitrary closed or open input shape results in significantly reduced training times (minutes instead of hours) and interactive inference. While our results can be easily converted into textured meshes, %PyMeshLab 
direct visualization of our point-based representation in realtime enables iterative co-creation guided by an artist. Finally, our high-quality output geometric models can be assigned a fine texture if needed, using off-the-shelf image super-resolution or more advanced texturing synthesis methods such as~\cite{richardson2023texture}. \vspace*{-1.5mm}


