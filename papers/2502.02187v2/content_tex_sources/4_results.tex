%! TEX root = ../main.tex


\section{Implementation Details}\label{sec:implementation}
\paragraph{Implementation Details.}
We implemented our method in Python with PyTorch~\cite{paszke_pytorch_2019}, libigl~\cite{jacobson_libigl_nodate}, and Open3D~\cite{zhou2018open3d}, and our code is available on \href{https://nissmar.github.io/projects/shapeshifter/}{our project page}. All reported timings were obtained on a desktop with an NVIDIA GeForce RTX 3080 GPU (10 GB) to underscore the efficiency of our approach even on consumer-grade hardware.

\begin{table*}[t]
    \centering
    \resizebox{1.\linewidth}{!}{\input{tables/ssfid}}
    \vspace*{-3mm}
    \caption{\emph{Evaluating geometric quality and diversity using SSFID and pairwise IoU scores.} Our model shows clear advantage in quality, and performs similar to Sin3DM in diversity. As we discussed in \cref{sec:comparison}, both metrics have their blindspots, SSFID overlooks geometric details and pairwise IoU rewards artifacts. Finding a more holistic metric to evaluate shape variation remains an open problem.  \vspace*{-2mm}} 
    \label{tab:ssfid}
\end{table*}


\paragraph{Model Parameters.}
By default we use 5 levels, the lowest and highest grid resolutions being 16 and 256 respectively.
The upsamplers \(\upsampler\) consist of 4 layers of 64 channels, containing {\mytilde}55k parameters that are trained for 10k iterations with a learning rate of $5 \!\cdot\! 10^{-4}$ and a 5\% dropout rate.
The diffusion models have 128 feature channels and 7 layers, for a total of {\mytilde}565k parameters for the coarsest model $\model^{1}$ and 1.2M parameters As in prior work, the receptive fields of each model \(\model^{1}\) are kept small to prevent overfitting to the fixed global structure: \(\model^{1}\) thus uses a receptive field of $5^3$, while the rest use $9^3$.
We train our diffusion models with $T\!=\!1000$ diffusion steps.
For sampling, we set $T[1]\!=\!1000$ and $T[l\!>\!1]\!=\!300$. 
$\model^{1}$ is trained for 20,000 iterations, and the rest for 40,000 iterations. 
All levels are trained with random crops of the same resolution to help ensure that each scale is trained in roughly the same time, and we use a learning rate of $10^{-4}$ with a 1\% dropout rate.

\paragraph{Feature Extraction.}
In terms of shape processing, we normalize each mesh to fit within $[-1, 1]^3$.
3D features are sampled at a resolution of $1024^3$ and downsampled to a coarsest resolution of $16^3$ voxels, as described in \cref{sec:sparse-representation}.

%\vspace*{-2mm}
\section{Experiments}\label{sec:experiments}

\noindent\textbf{Data.} We demonstrate our approach on 3D textured exemplars provided by Sin3DM (from \cite{impjive2021pillar,
kurd2021akropolis,
ashoori2020smalltown,
ustal2020canyon,
carnota2015industrial,
djmaesen2021cliff,
allaboutblender2020wood}), and also used an open surface example that we created.
Note that \ourmethod can operate as-is on untextured inputs; but colors can help distinguish geometrically similar, yet semantically different parts of the geometry. 

\subsection{Comparison.}\label{sec:comparison}
\paragraph{Baselines.}
We compare with two state-of-the-art papers on 3D generation from single examples: Sin3DM~\cite{wu2024sindm} and Sin3DGen~\cite{li2023patch}.
Sin3DM uses a single-scale triplane diffusion model with a small receptive field to learn internal feature distribution within the exemplar shape. Features are learned in a separate autoencoder that parameterizes the input shape as an implicit neural surface~\cite{wang2021neus}. We use their publicly available generated results for comparison.
Sin3DGen operates instead on radiance field represented by plenoxels: it learns a hierarchical deformation field to transform the input plenoxels based on patch similarity. Following their data preparation guideline, we first rendered 200 images with Blender~\cite{blender2018}, then trained a \(512^3\) plenoxel to obtain the input exemplar which we provide to Sin3DGen to generate results to which we compare ourselves. 
%A deformation field is optimized using the default setting. m@: needed?

\vspace*{-4mm}
\paragraph{Quantitative Evaluations.}
Following prior work, we use Single Shape Fréchet Inception Distance (SSFID~\cite{wu2022learning}) to evaluate the output quality, and pairwise \((1\!-\!\text{IoU})\)-distances among 10 generated variations to evaluate the output diversity.
SSFID is extended from Single-Image Fréchet Inception Distance (SIFID)~\cite{shaham2019singan}, which compares the statistics of the input and the generation feature extracted at different levels of a pretrained multiscale 3D encoder~\cite{chen2021decor} trained on voxelized shapes from ShapeNet;
thus we voxelized input and generations at a $256^3$ resolution for evaluation purposes. We show in \cref{tab:ssfid} that \ourmethod{} outperforms competing methods in SSFID. (Note that our SSFID scores for Sin3DM differ from their reported scores as they used post-processed smoothed shapes as reference; please refer to the supplementary material for details.)  
%\nissimrmk{I'm not sure we should indicate that (above) here, maybe state it in the supp and refer to it here?} %m@: I vote to keep it, for full disclosure
%\yifanrmk{Our diversity score seems really bad :( the worst almost across the board. is this correct? Is there a way to argue it out?} \nissimrmk{The best diversity is Sin3DGen (because of spurious geometrical artifacts) and we are tie with Sin3DM with much better quality so I think it's ok}
While SSFIDs usually match the perceptual quality of the results, we note that the voxelization step used for scoring removes sharp features and high-frequency details, which our paper captures particularly well. 
%\yifanrmk{Let's try to find a better encoder for rebuttal, we could probably point cloud encoder instead of voxels}.
The qualitative examples shown in \cref{fig:results} and in our supplementary material better demonstrate our strengths. Overall, the underlying geometry from Sin3DGen results is heavily distorted as it relies on plenoxel matching and blending, and while the SDF supervision from Sin3DM makes for better outputs, it does not capture sharp features well and the features extracted from their single-resolution triplane representation struggle to encode high-frequency details.
Finally, the diversity scores based on IoU must be handled with care as it rewards artifacts. Our results show low diversity scores for very structured exemplars like the acropolis or the house, but good scores for more organic and varied shapes like the canyon or the small town, which is in line with our goal of generating variants of the input shapes. 
% A GOOD METRIC HAS YET TO BE INVENTED FOR THIS ILL-POSED PROBLEM. Geometry Quality is measured the input shape with SSFID (single shape Fréchet Inception Distance)~\cite{wu2024sindm}. Geometry Diversity is measured by calculating pairwise IoU distance among 10 generated shapes.

\begin{table}[h] \vspace*{-1mm}
    \centering
    \begin{tabular}{lcccc}\toprule
       Method  & encoding and training & inference \\\midrule
       SSG &  4 hours & 0.1 sec \\
       Sin3DM  & 2.5 hrs & 15.8 sec \\
       Sin3DGen$\ast$ & 15 min & 139 sec \\
       \ourmethod{} & 12 min & 10.7 sec\\\bottomrule
    \end{tabular} \vspace*{-2mm}
    \caption{\emph{Timings}. Sin3DGen~\cite{li2023patch} was tested on a more performant GPU (Nvidia A100 40GB), as it could not fit in our regular GPUs (Nvidia 3080 10GB).\vspace*{-5mm}}
    \label{tab:timing}
\end{table}

\vspace*{-4mm}
\paragraph{Training and inference speed.}
We show timing comparisons in ~\cref{tab:timing}, where we also included GAN-based SSG~\cite{wu2022learning} as its architecture is faster than its successor at inference time, albeit with lower quality and diversity~\cite{wu2024sindm}. 
Despite its impressive inference time, the training time of SSG is the longest.
Sin3DGen does not require training as it is based on patch-matching; but each variation generation takes around 2 minutes, limiting interactive use cases.
Sin3DM takes in total of 2.5 hours to train, of which {\mytilde}30 minutes are used to learn triplane features.
Our method takes merely 6 seconds to encode the shape, and trains each level of the hierarchy of diffusion models in 12 minutes, significantly outperforming all other trained models. The inference time takes from 0.15 seconds (at level 2) to 7.5 seconds (at level 5) totaling 10.7 seconds.
It is worth mentioning that since our method outputs colored oriented pointset at \emph{every level}, requiring no additional conversion \eg marching cubes in other methods, we can flexibly choose the working level depending on the application. For example, in editing, we can operate on level 3, which takes only 1 s. per generation, thus enabling interactive modeling as shown next. More details are provided in the supplementary material.
%\todo{For supplementary, add memory utilization during training and testing. }


\begin{figure}[!h] \vspace*{-2mm} 
    \centering
  \includegraphics[width=0.99\linewidth]{figures/manipulation/manipulation-summary-texture.pdf}
  \vspace*{-3mm}
    \caption{\emph{Controlled generation.} The span of the output can be trivially controlled by resizing the initial grid anisotropically. \vspace*{-4mm}}
    \label{fig:bbox_control}
\end{figure}

\subsection{Control and editing.}
Our multiscale explicit representation makes it easy to control and edit the output.
We demonstrate two examples: the user can trivially  change the span of the model by resizing the initial grid \(\grid^{0}_{T}\) anisotropically, see \cref{fig:bbox_control}; moreover, \cref{fig:duplication_control} demonstrates that a generated output can be further edit by copy-and-pasting  parts of the output within one of the levels of the multiscale description of the shape, here to remove windows or adding a bay window.
While existing works can offer similar capabilities, their use of triplane features or deformation fields to drive the generation renders editing less intuitive.
For example, a patch from the input shape can be duplicated in Sin3DM to appear in the generated variations; however, the duplication must be done on three interdependent triplanes features, which do not directly correspond to a feature in 3D space.

\begin{figure}[!h] \vspace*{-2mm}
    \centering
  \includegraphics[width=0.99\linewidth]{figures/manipulation/copypaste2.pdf}
   \vspace*{-2mm}
    \caption{\emph{Editing.} Using sparse voxel grid allows users to intuitively apply more precise edits. Here, a user can copy and paste a selected part of a generated variation at an intermediate level to manually alter the variation.
     \vspace*{-4mm}
     }
    \label{fig:duplication_control}
\end{figure}


\subsection{Open Surfaces}
Our use of points and normals to represent the geometry makes the treatment of open surfaces not only possible but just as simple as the case of closed surfaces: only the surface extraction method needs to be altered, \ie with APSS~\cite{guennebaud_algebraic_2007} instead of \cite{kazhdan_screened_2013}. 
An example is shown in \cref{fig:opensurfaces}.
%See inset\red{THIS IS NOT AN INSET...}:

\begin{figure}[!h] \vspace*{-2.5mm} \centering
  \includegraphics[width=0.95\linewidth]{figures/open_surface/open_summary.jpg}
  \vspace*{-3mm}
    \caption{\emph{Open surfaces.} Our oriented points representation also handles open surfaces by simply using APSS~\cite{guennebaud_algebraic_2007} to mesh the generated point set, while it is challenging for existing methods.\vspace*{-3mm}}
    \label{fig:opensurfaces}
\end{figure}

\subsection{Ablation studies}\label{sec:ablation}
\paragraph{Learned Upsampler.}
We demonstrate the benefit of our learned upsampler by replacing it (both in training and inference) by a trilinear interpolation as used in SinDDM in \cref{fig:ablation_upsampler}: artifacts appear as our point features are mispositioned because of the trilinear upsampling.

\begin{figure}[!h] 
    \centering
    \centering
    \begin{subfigure}{.48\linewidth}
      \centering
      \includegraphics[width=\linewidth]{figures/ablation/ablation_acropolis.jpg}
    \end{subfigure}
     \begin{subfigure}{.48\linewidth}
      \centering
      \includegraphics[width=\linewidth]{figures/ablation/ours_acropolis.jpg}
    \end{subfigure}

    \centering
    \begin{subfigure}{.48\linewidth}
      \centering
      \includegraphics[width=\linewidth]{figures/ablation/ablation_house_circle.jpg}
      \caption{Trilinear Interpolation}
    \end{subfigure}
     \begin{subfigure}{.48\linewidth}
      \centering
      \includegraphics[width=\linewidth]{figures/ablation/ours_house.jpg}
      \caption{Learned Upsampler}
    \end{subfigure}
    \vspace*{-2mm}
    \caption{\emph{Ablation test of upsampling.} Comparing trilinear interpolation (left) with learned upsampler (right), the interpolation causes artifacts (see circled areas), whereas the learned upsampler provides a more detailed and structurally coherent output.\vspace*{-4mm}}
    \label{fig:ablation_upsampler}
\end{figure}

\vspace*{-2mm}
\paragraph{3D Features.} We also replace our geometric features with an SDF. For fairness of comparison, we use two layers of active cells (instead of one) close to the surface to compensate for the reduced feature dimensionality.
For the same input resolution, our features have more details (\cref{fig:ablation_feature}).


\begin{figure}[h]
    % \centering
    % \begin{subfigure}{.48\linewidth}
    %   \centering
    %   \includegraphics[width=\linewidth]{figures/ablation/mesh_mc.jpeg}
    % \end{subfigure}
    %  \begin{subfigure}{.48\linewidth}
    %   \centering
    %   \includegraphics[width=\linewidth]{figures/ablation/mesh_ours.jpeg}
    % \end{subfigure}

    \begin{subfigure}{.48\linewidth}
      \centering
      \includegraphics[width=\linewidth]{figures/ablation/mesh_mc_detail3.jpeg}
      \caption{SDF + Marching Cube~\cite{lorensen1998marching}}
    \end{subfigure}
     \begin{subfigure}{.48\linewidth}
      \centering
      \includegraphics[width=\linewidth]{figures/ablation/mesh_ours_detail3.jpeg}
       \caption{Ours + Poisson~\cite{kazhdan_screened_2013}}
    \end{subfigure}
    \vspace*{-2mm}
    \caption{\emph{Ablation test of features.} Comparing SDF (left) with our proposed point and normal features (right) at the same resolution ($128^3$) demonstrates that our proposed features produce richer geometric details. The mesh color encodes the normal direction to reflect the difference in geometry details.\vspace*{-4mm}} 
    \label{fig:ablation_feature}
\end{figure}


% \begin{itemize}
%     \item Upsampler: learned upsampler vs triplane interpolation \cref{fig:ablation_upsampler}
%     \item Feature pooling: Replace our QEM based position features average pooling \cref{fig:ablation_feature}
% \end{itemize}

\begin{figure}[b] \vspace*{-3mm}
    \centering
    \includegraphics[width=0.96\linewidth]{figures/texture/texturing.jpg}
       \vspace*{-2mm}
    \caption{\emph{Texturing.} As explained \cref{sec:meshing}, ultra-high resolution texture can be obtained by applying state-of-the-art AI image-enhancing tools on the texture maps created by our method from the colored point set outputs.\vspace*{-6mm}}
    \label{fig:textured-example}
\end{figure}



\subsection{Texture Augmentation}\label{sec:exp-texture}
Finally, we show that one can texture our generated models by applying contemporary image super-resolution on the baked texture maps in \cref{fig:textured-example}: using Magnific AI~\cite{magnific_ai} for example can efficiently generate a fine texture improving the visual impact of our results. While this is only a proof-of-concept example, exploring the texturing of our geometric models is an exciting, albeit orthogonal, research direction.
