\documentclass{article} %
\usepackage{iclr2025_conference,times}

\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{xcolor}
\def\ty#1{\textcolor{violet}{#1}}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{cleveref}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{comment}

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{conjecture}[theorem]{Conjecture}

\newtheorem{definition}[theorem]{Definition}
\newtheorem{experiment}{Experiment}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{property}[theorem]{Property}

\newcommand{\RP}{\mathbb{RP}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\NN}{\mathbb{N}}

\newcommand{\eg}{{e.g.}}
\newcommand{\ie}{{i.e.}}

\newcommand{\luca}[1]{{\color{red}#1}}


\title{
{PICASO: Permutation-Invariant Context Composition with State Space Models}}


\author{Tian Yu Liu\thanks{Work conducted during an internship at Amazon} \\ UCLA
\And
Alessandro Achille \\
AWS AI Labs 
\And
Matthew Trager \\
AWS AI Labs
\AND
Aditya Golatkar \\
AWS AI Labs
\And
Luca Zancato \\
AWS AI Labs
\And
Stefano Soatto \\
AWS AI Labs
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\mt}[1]{\textcolor{orange}{MT: #1}}

\iclrfinalcopy %
\begin{document}


\maketitle

\begin{abstract}

Providing Large Language Models with relevant contextual knowledge at inference time has been shown to greatly improve the quality of their generations. This is often achieved by prepending informative passages of text, or `contexts', retrieved from external knowledge bases to their input. However, processing additional contexts online incurs significant computation costs that scale with their length. State Space Models (SSMs) offer a promising solution by allowing a database of contexts to be mapped onto fixed-dimensional states from which to start the generation. A key challenge arises when attempting to leverage information present across multiple contexts, since there is no straightforward way to condition generation on multiple independent states in existing SSMs. To address this, we leverage a simple mathematical relation derived from SSM dynamics to compose multiple states into one that efficiently approximates the effect of concatenating raw context tokens. Since the temporal ordering of contexts can often be uninformative, we enforce permutation-invariance by efficiently averaging states obtained via our composition algorithm across all possible context orderings. We evaluate our resulting method on WikiText and MSMARCO in both zero-shot and fine-tuned settings, and show that we can match the strongest performing baseline while enjoying on average $5.4\times$ speedup.

\end{abstract}


\section{Introduction}
Incorporating new information in deep learning models has traditionally been a costly process, often requiring re-training or fine-tuning their weights on new data. Fortunately, Large Language Models (LLMs) provide a compelling alternative: These models can `learn' to leverage new contextual information in real-time by simply prepending them as inputs, without having to modify their weights \citep{ram2023context}. This has motivated a powerful application known as Retrieval-Augmented Generation (RAG), where LLMs are deployed with the ability to retrieve and incorporate relevant sources of information, or `contexts', from vast external knowledge bases when queried by users at inference time.

Despite being faster than the naive fine-tuning of model weights, this approach still incurs significant computational costs. Not only must the system process the user query and generate an answer, but it must also process the retrieved context, which in real-world settings can amount to thousands of tokens. This problem is exacerbated in Transformer-based models, as the inference cost of generating output tokens scales quadratically with the length of the extended input (see \Cref{fig:timings}).






In contrast, State Space Models (SSMs) offer a more efficient alternative. SSMs encode information from arbitrary-length input sequences into a fixed-size state vector, which can then be conditioned on to generate new tokens without revisiting the original input. This suggests a simple method to reduce the cost of incorporating new contexts at inference-time: Instead of retrieving from a database containing raw context tokens, we can create a ``database of states” containing pre-computed state representations of contexts. At inference time, generation starts directly from the retrieved state, simultaneously eliminating the latency from having to process context tokens online, and greatly reducing inference time compared to Transformer models (\Cref{fig:timings}).  


However, a key challenge arises when conditioning on multiple retrieved states. While input tokens can be simply concatenated and fed into an LLM or SSM, existing models are trained to generate outputs conditioned only on a single SSM state. To address this, we derive a simple mathematical relation via SSM dynamics to compose multiple states into one, in a manner that exactly equates to the result of concatenation in a single-layer model. Consequently, by simply storing an additional weight matrix along with each context, pre-computed states can be effectively composed at inference time to condition generation on any arbitrary set of contexts.

Since states are computed causally, the order in which contexts are presented affects the state -- when there is no natural ordering among retrieved contexts, different order of presentation would yield different states. Consequently, we propose to enforce order-invariance explicitly through averaging states obtained by composing contexts across all possible permutations. While this may appear costly at first glance, we show that the resulting state can be computed exactly in polynomial time in the number of context segments using Dynamic Programming, and this can be further reduced to linear time by accepting slight approximations. 
This greatly benefits Retrieval-Augmented Generation tasks, where our results show a $10\%$ improvement over the best order-dependent state composition method when order-invariance is incorporated into the conditioned state.

To outline our main contributions, we introduce a method for efficiently retrieving and composing multiple pre-computed states at inference time to condition the generation of high-quality outputs, which we term PICASO (\textbf{P}ermutation-\textbf{I}nvariant \textbf{C}ompositional \textbf{A}ggregation of \textbf{S}tates as \textbf{O}bservations). Our experiments show that PICASO achieves 91\% of the performance gain from combining the raw tokens of multiple contexts, while offering a $5.4\times$ speed-up over concatenation. 

PICASO can be applied to any off-the-shelf SSM model without any changes. To further improve performance, we introduce a method for fine-tuning the model to better leverage the composed states for generation. Using a pre-trained Mamba-2 2.7B model, less than a day of fine-tuning on a single A100 GPU leads to the same performance as concatenation while maintaining the $5.4\times$ faster composition time on the WikiText-V2 dataset.


\begin{figure}[t]
    \centering    
    \includegraphics[width=0.45\textwidth]{figures/PICASO-Database-of-States-Context.pdf}
    \hfill
    \includegraphics[width=0.45\textwidth]{figures/picaso-ssm-transformer-timings-barplot-context.pdf}
    \caption{\textbf{(Left:)} We propose a “Database of States,” where contexts are stored as pre-processed state vectors. Given a query, relevant states are then retrieved and composed into a single state vector which is used to condition the model’s generation. \textbf{(Right:)} We plot the increase in total time required to generate an additional 64 tokens, when concatenating a 64-token prompt with retrieved contexts. We model the time taken for PICASO-R as the time taken to combine 5 pre-processed context states, which involves only arithmetic operations and notably zero model processing time. As a result, the processing and inference costs for PICASO-R remain constant regardless of the length of retrieved contexts. In contrast, the timings for a Transformer model scale quadratically, and for an SSM linearly, with total length when generating from concatenated context tokens. These timings are measured using the official Mamba benchmarking code, which includes optimizations such as quantization and CUDA graphs for SSMs, and flash attention for Transformers. }
    \label{fig:timings}
\end{figure}



\section{Related Work}
\paragraph{State Space Models and Hybrid Models.}


Recent efforts to overcome the significant computational costs of Transformer models on long contexts have inspired the exploration of more efficient alternatives, including State Space Models (SSMs). Through maintaining a fixed-size ``state'', a sufficient statistic of the past for the purpose of future prediction, these models offer advantages compared to Transformer models. They only require constant memory consumption regardless of the sequence length, and linear computational complexity, rather than quadratic, as longer sequences are processed.  
The idea of leveraging recurrent models with fixed dimensional states to represent complex sequences is not new, in fact, several variations of SSMs have been developed in the past, ranging from Linear Time Invariant (LTI) systems, to more expressive non-linear Time Varying \citep{jazwinski2007stochastic} and Input Varying \citep{krener1975bilinear} systems.

More recently, many of these ideas have been rediscovered and implemented on modern parallel hardware as basic building blocks for Foundation Models. 
\cite{gu2023mamba} proposed Mamba, an input-dependent linear SSM (termed `selective') based on LIV systems, that achieves comparable performance to Transformers \citep{vaswani2017attention} on language modeling while enjoying faster inference. 
Mamba-2 \citep{dao2024transformers} further improved computational time by implementing SSM layers with structured matrix multiplications to better leverage modern Tensor Cores.
Although pure SSM models can compete with Transformer blocks on many NLP tasks, they lag behind on tasks that require strong recall capabilities. To balance inference efficiency and model capabilities, Hybrid models combining Attention and SSM blocks have been proposed.
\cite{lieber2024jamba} combined SSM blocks along with global-attention blocks to create a hybrid architecture with Mixture-of-Expert layers for training larger models. To further improve long context ability and efficiency, \cite{ren2024samba} leveraged sliding window attention, while \cite{zancato2024b} developed a general family of architecture that include Transformers, SSMs and their hybrid combinations, leveraging both verbatim and fading memory, in both long- and short-term.
\color{black}

\paragraph{Retrieval-Augmented Generation and In-Context Learning.}
Our work falls within the scope of In-Context Retrieval-Augmented Language Models \citep{ram2023context}, where language models are conditioned on retrieved contexts via concatenation. Retrieval Augmented Generation (RAG) allows language models to leverage knowledge stored in external databases, which greatly improves performance on knowledge-intensive and domain-specific tasks \citep{lewis2020retrieval}. In our work, we simply use a pre-trained sentence embedding model for retrieval, and we refer to \cite{gao2023retrieval} for a detailed survey on other mechanisms.
Apart from retrieval, processing (multiple) retrieved contexts can also greatly increase generation latency. \cite{izacard2023atlas} mitigates this by independently processes each retrieved context with a LLM encoder, using cross attention over the concatenated encoder outputs. \cite{zhu2024accelerating} similarly encodes retrieved contexts in parallel, and performs decoding in a selective manner by attending only to highly relevant encoder outputs.

In-Context Learning (ICL) \citep{brown2020language} has emerged as an effective method to perform inference without learning (\textit{i.e.}, transduction), by conditioning on labeled samples provided in-context, commonly implemented as a set of (query, answer) pairs \citep{dong2022survey}. 
Similar to RAG, the quality of selected demonstrations have been shown to greatly affect downstream performance \citep{xu2024context}. Several methods have been developed for selecting effective demonstrations, based on sentence embeddings \citep{liu2021makes}, mutual information \citep{sorensen2022information}, perplexity \citep{gonen2022demystifying}, and even BM25 \citep{robertson2009probabilistic}.
Similar to the motivation of our work, several studies have shown that the performance of ICL is heavily dependent on demonstration ordering. \cite{zhao2021calibrate} shows that answers positioned towards the end of the prompt are more likely to be predicted, while \cite{lu2021fantastically} shows that results can vary wildly between random guess and state-of-the-art depending on the order that demonstrations are presented. Outside of ICL, \cite{liu2024lost} further shows that language models do not robustly utilize information in long input contexts due to sensitivity to positioning.


\paragraph{Model and State Composition.}
Our work falls into the category of 
composing of deep models, representations, and states. \cite{wortsman2022model} proposes Model Soup, which composes multiple non-linearly fine-tuned models via averaging model weights. \cite{liu2023tangent1,liu2023tangent2} leverages model linearization to enforce an equivalence between weight averaging and output ensembling. \cite{perera2023prompt} independently learns task-specific prompts which can be linearly averaged to yield new prompts for composite tasks.
For SSMs, \cite{pioro2024state} investigates averaging of states, along with decay-weighted mixing which is closely related to a baseline version of our method, CASO. However, the equations described in their work differ from CASO, and their evaluations are limited to composition of two equal-length contexts. In contrast, our method greatly improves upon CASO by incorporating permutation invariance, which we show is important to achieve performances comparable to that of concatenation.

\section{Method}
\subsection{Preliminaries: %
}
A linear input-dependent discrete-time state-space model has the form %
\begin{equation}
\begin{cases}
x_{t} = A(u_{t}) x_{t-1} + B(u_{t}) u_{t} \\
y_{t} = C(u_{t}) x_{t} + D u_{t}.
\end{cases}
\label{eq:SSM}
\end{equation}
Here $x_t \in \RR^m$ is the \emph{state} at time $t$, while $u_t,y_t \in \RR^d$ are the \emph{input} and the \emph{output} respectively. The matrices $A(u_t) \in \RR^{m \times m}, B(u_t) \in \RR^{m \times d}, C(u_t) \in \RR^{d \times m}$ (which are input-dependent) and $D \in \RR^{d \times d}$ are learnable parameters.

Unrolling the first equation, we obtain
\begin{equation}\label{eq:ssm-unroll}
\begin{aligned}
x_t &= A(u_t) \cdots A(u_1) x_0 + \sum_{\tau=0}^{t-1} A(u_{t}) \cdots  A(u_{t-\tau+1}) B(u_{t-\tau}) u_{t-\tau} \\
&= A(\bm{u}) x_0 + x(\bm{u}),
\end{aligned}
\end{equation}
where $\bm{u} = (u_1,\ldots,u_t)$ denotes the \emph{sequence} of inputs, $A(\bm{u}) = A(u_t) \cdots A(u_1)$ is the accumulated {decay matrix} and $x(\bm{u}) = \sum_{\tau=0}^{t-1} A(u_{t}) \cdots  A(u_{t-\tau+1}) B(u_{t-\tau}) u_{t-\tau}$ is the accumulated input signal. Since this coincides with $x_t$ when $x_0 = 0$, we refer to it as the \emph{state} for input sequence $\bm{u}$.

In the following, we write $\mathcal V \subset \RR^d$ for a finite set of token embeddings and $\mathcal V^* = \bigcup_{n\ge 0} \mathcal V^n$ for the set of variable-length sequences of token embeddings.  We view a State Space (language) Model (SSM) as a map $f_\theta :\mathcal{V}^\ast \times \mathbb{R}^m \mapsto \mathbb{P}(\mathcal{V})$ with parameters $\theta$ which takes in as input an initial state $x \in \RR^m$ and token embedding sequence $\bm{u} \in \mathcal V^*$, and returns a distribution over $\mathcal{V}$. Modern SSMs \citep{gu2023mamba,zancato2024b} usually contain multiple stacked selective state space layers as in~\eqref{eq:SSM}. In a multi-layer setting, we write $x(\bm u)$ and $A(\bm u)$ for the sequence of states and decay matrices corresponding to all layers. 





\subsection{Database of States}
By the Markov property, the state of an SSM makes the past independent of the future. In other words, $f_\theta(\bm{u} \cdot \bm{u}', 0) = f_\theta(\bm{u}, x(\bm{u}'))$ for all $\bm{u},\bm{u}' \in \mathcal{V}^*$, where $\cdot$ denotes concatenation. In practice, this means that a SSM model can equivalently be initialized with the state arising from a (variable-length) input sequence, instead of the input sequence itself. This is akin to the KV-cache of Transformer architectures, except that the dimension of the state is fixed regardless of sequence length.

In several real-world use cases such as Retrieval Augmented Generation, relevant contexts are commonly obtained or retrieved from a database \citep{borgeaud2022improving}. Instead of storing them in the database as raw text or tokens, we propose to use a ``database of states," where we pre-process each context and store their states. When conditioning on a single context, we can initialize the SSM with the retrieved pre-processed state instead of having to process it online. However this poses a problem when attempting to compose multiple contexts, since we do not know how to compose their states. We will show how this is tackled with our proposed method.
 
\subsection{Permutation-Invariant Composition with State Space Models} %

Given a query and a collection of relevant contexts, an easy method to compose them is to simply concatenate all context tokens with the query into a single sequence to feed into the SSM. Recall that this, however, presents two key limitations. Before even a single token continuation can be generated from the query, the entire sequence of concatenated contexts has to be processed sequentially, which can be computationally intensive when contexts are long or numerous (\Cref{fig:timings}). Another limitation is having to select the order of context concatenation when prompting the model, for which there might be no natural way of doing so without a powerful scoring mechanism. 

To address the first limitation, we propose a first version of our method, Compositional Aggregation of States as Observations (CASO), which works by modeling sequence concatenation with state composition based on the dynamics of a single-layer SSM.

\begin{proposition}[CASO]
Let $\bm{u}_1,\ldots,\bm{u}_n$ be a collection of input sequences and let $\bm{u}=\bm{u}_1 \cdots \bm{u}_n$ be their concatenation. Then, for a SSM layer that evolves based on \eqref{eq:SSM},
we have
\begin{equation}\label{eq:caso-expand}
x(\bm{u}) = x(\bm{u}_n) + \sum_{i=1}^{n-1} A(\bm{u}_n) \cdots A(\bm{u}_{i+1}) \cdot x(\bm{u}_i)
\end{equation}
\label{prop:caso}
\end{proposition}
We can see this by recursively applying \eqref{eq:ssm-unroll} on $x(\bm{u}) = A(\bm{u}_n) x(\bm{u}_1 \cdots \bm{u}_{n-1}) + x(\bm{u}_n)$.

Given a collection of contexts $\bm{u}_1, \ldots, \bm{u}_n$, CASO simply approximates the dynamics of multi-layer SSMs, for which \Cref{prop:caso} does not hold exactly, via $x^{\rm CASO}_\theta(\bm{u}_1, \ldots, \bm{u}_n) = x(\bm{u}_n) + \sum_{i=1}^{n-1} A(\bm{u}_n) \cdots A(\bm{u}_{i+1}) \cdot x(\bm{u}_i)$. We then load $x^{\rm CASO}_\theta(\bm{u}_1, \ldots, \bm{u}_n)$ as the initial state of the model to infer continuations from the given query. We note that in Mamba-style models, the matrices $A(\cdot)$ are diagonal. As such, computing CASO requires only simple element-wise arithmetic operations and importantly zero model computation time (\textit{i.e.} zero forward passes required).

However, since each state is weighted by the decay factors of future  contexts, this composition operation is still very much order-dependent. We propose to introduce permutation-invariance by considering a group of permutations $G \subseteq S_n$, where $S_n$ denotes the symmetric group of $n$ elements, using which we define our method, PICASO (\textbf{P}ermutation-\textbf{I}nvariant CASO):
\begin{equation}\label{eq:group-average}    
x^{\rm PICASO}(\bm{u}_1,\ldots,\bm{u}_n) :=  \frac{1}{|G|} \sum_{\pi \in G} x^{\rm CASO}(\bm{u}_{\pi(1)},\ldots,\bm{u}_{\pi(n)})
\end{equation}

For any group $G$, by expansion of the CASO terms and collecting common factors, this can be written as a linear combination of individual context states $x(\bm{u}_i)$: 
\[
x^{\rm PICASO}(\bm{u}_1, \ldots, \bm{u}
_n) = \sum_{i=1}^n W_{i}(\bm{u}_1, \ldots, \bm{u}
_n) x(\bm{u}_i)
\]
with weights $W_{i}$ depending on $A(\bm{u}_1), \ldots, A(\bm{u}_n)$. In this work we are particularly concerned with two cases: the full symmetric group $G = S_n$, which includes all possible permutations, and the cyclic group $G = C_n$, which consists of rotations of the sequence. We will refer to them as PICASO-S and PICASO-R respectively.

While they appear computationally infeasible at first glance, since PICASO-S and PICASO-R average over $n!$ and $n$ CASO states respectively, each of which is itself a composition of $n$ context states, the following propositions show that they can actually be computed in polynomial and linear time respectively for modern SSM models with diagonal $A$ matrices.

\begin{proposition} Assume $G = S_n$ and that the matrices $A(\bm{u}_i)$ commute (\eg, are diagonal). Using shorthand notations $A_i := A(\bm{u}_i)$ and $W_k := W_k(\bm{u}_1, \ldots, \bm{u}_n)$ we have
\begin{align*}
W_k &= \frac{1}{n!} \Bigg[(n-1)! + (n-2)! \cdot 1! \cdot \sum_{\substack{1 \leq i_1 \leq n \\ i_1 \neq k}} A_{i_1} + (n-3)! \cdot 2! \cdot \sum_{\substack{1 \leq i_1 < i_2 \leq n \\ i_1, i_2 \neq k}} A_{i_1} A_{i_2} + \ldots \Bigg] \\
&= \frac{1}{n} \sum_{m=0}^{n-1} \frac{1}{{n-1 \choose m}} \cdot e_m(A_1, \ldots, A_{k-1}, A_{k+1}, \ldots, A_{n}),
\end{align*}
where
\begin{align*}
e_m(A_1, \cdots A_{n-1}) := \sum_{1\leq i_1 < i_2 < \cdots < i_m \leq n-1} A_{i_1} \cdots A_{i_m}
\end{align*}
is the $m$-th \emph{elementary symmetric polynomial} \citep{macdonald1998symmetric} (in the matrices $A_i$).
\end{proposition}

Elementary symmetric polynomials satisfy the recursive relation
\[
e_m(A_1, \ldots, A_{n-1}) = A_{n-1} e_{m-1}(A_1, \ldots, A_{n-2}) + e_m (A_1, \ldots, A_{n-2}).
\]
Using this relation, we can compute all values of $e_m$, and hence the coefficients $W_{k}$, using $O(n^2)$ operations via Dynamic Programming. We detail the implementation in \Cref{alg:picaso-s} of the Appendix. Consequently, the full state from PICASO-S can be efficiently computed in polynomial $\mathcal{O}(n^3)$ time, which we show in the experiments to still be faster than processing textual context concatenations even for $n$ as large as $10$.

Next, we similarly show that the coefficients for PICASO-R can be efficiently computed by exploiting invertibility of the matrices $A(\bm{u}_i)$.

\begin{proposition} Assume $G = C_n$ (cyclic permutations). Then writing $A_i := A(\bm{u}_i)$ and $W_k := W_k(\bm{u}_1, \ldots, \bm{u}_n)$ we have
\begin{align*}
W_k &= \frac{1}{n} \left[{\rm Id} + \sum_{m=1}^{n-1} A_{[k+m]_n} \cdots A_{[k+1]_n} \right].
\end{align*}

where ${\rm Id}$ is the identity matrix, and $[i]_n$ denotes $i \operatorname{mod} n$. Assuming that the matrices $A_i$ are invertible, these can be computed efficiently by setting
\[
\bar A_i = \begin{cases} A_{[i]_n} \cdots A_1 \cdot A_n  \cdots A_1 & i > n \\ 
                   A_{i}  \cdots A_1 & i \leq n   \end{cases}, \quad \bar B_i =  \bar A_1 + \ldots + \bar A_{i-1}, \quad W_k = \frac{1}{n}[\bar A_k^{-1} (\bar B_{k + n} - \bar B_k)],
\]
for $i=1,\ldots,2n$, and $k=1,\ldots,n$.
\end{proposition}
We detail in \Cref{alg:picaso-r} in the Appendix our efficient implementation for computing PICASO-R in $\mathcal{O}(n)$ time complexity via cumulative sums and products. Evidently, PICASO-R is significantly faster than PICASO-S while trading off exact permutation invariance for invariance only to cyclic permutations of the original order. We will show that the difference in empirical performance between PICASO-S to PICASO-R is negligible, as such PICASO-S can almost always be replaced with its much faster variant PICASO-R.

We remark that the property of permutation-invariance can also be applied to naive concatenation (as opposed to CASO). This is achieved simply by concatenating contexts in various different orders, followed by taking an average of their resulting states. While performing this for the symmetric group $S_n$ is computationally infeasible, we can similarly restrict our permutation set to $C_n$. We term this variant Permutation-Invariant Concatenation (PIConcat-R), where $-R$ denotes invariance to the set of cyclic permutations. We note that the \emph{model} computational costs (forward passes) of this method still scales quadratically with number of contexts (compared to linear scaling of regular concatenation), as such we include it only for completeness. 

As a final technicality, we note that for Mamba-style SSM models, we additionally require storing the last $m_{conv}$ (usually $m_{conv}=4$) input tokens of each SSM layer to ensure that the state is sufficient for generating the same distributions over continuations as the input sequence. We perform simple averaging to combine these tokens from different contexts which we show to work well empirically; more sophisticated methods could be explored in future work.

\section{Why PICASO's average works}
While the combination of state expression for CASO is directly motivated by the dynamics of the system, there is no a priori reason why averaging permuted CASO states should perform well. 
In \Cref{fig:zero-shot-wikitext} we show that averaging both independent states and CASO states can perform better than using any individual state. This suggests a emergent/learned algebraic structure on the space of states such that linear combination of states combine their information to some degree.

In our empirical results below, we show that averaging all individual states (which would also be a permutation-invariant solution) performs significantly weaker than averaging CASO states (as PICASO does).
We believe that this is because the approximate linear structure of the state space is only valid locally.
The combined states are naturally closer together than the independent states, hence able to better exploit the local linearity. We show this in the following proposition:

\begin{proposition}
Consider a single-layer SSM parametrized by $\theta$, and two input sequences $\bm{u}$ and $\bm{u}'$. 
Then, the Euclidean distance between the states can be bounded via
\[
\Vert x^{CASO}(\bm{u},\bm{u}') - x^{CASO}(\bm{u}',\bm{u})\Vert_2^2 \leq \Vert (I-A(\bm{u}'))x(\bm{u})\Vert_2^2 + \Vert(I-A(\bm{u}))x(\bm{u}')\Vert_2^2
\]
\end{proposition}
To see this, simply apply the triangle inequality on the following obtained via substituting the equations for CASO:
\begin{align*} 
\Vert x^{CASO}(\bm{u},\bm{u}') - x^{CASO}(\bm{u}',\bm{u})\Vert_2^2 &= \Vert A(\bm{u}') x(\bm{u}) + x(\bm{u}') - (A(\bm{u}) x(\bm{u}') + x(\bm{u})) \Vert_2^2 \\
&= \Vert (A(\bm{u}') - I) x(\bm{u}) + (I - A(\bm{u})) x(\bm{u}') \Vert_2^2
\end{align*}

As a special case, we observe that as the decay factor approaches the identity, the distance between two CASO states approaches zero. 
In \Cref{fig:loss-landscape-interp}, we visualize naive averaging of the states arising from 3 retrieved contexts, and averaging of CASO states resulting from each cyclic permutation of these contexts. We use WikiText-v2 as described in the Experiments for these plots. Indeed, we observe that CASO states are much closer to one another in the resulting loss landscape.

\begin{figure}[h]
    \centering
    \hfill
    \includegraphics[width=0.4\textwidth]{figures/naive_soup_loss_landscape_mamba2-2.7b.pdf}
    \hfill
    \includegraphics[width=0.4\textwidth]{figures/picaso-r_loss_landscape_mamba2-2.7b.pdf}
    \hfill
    \caption{\textbf{Left:} Naive averaging (``Soup") of context states. \textbf{Right:} Averaging CASO states. CASO states are “closer” to one another (see Proposition) and hence can be more meaningfully interpolated. On the other hand, naively averaging states of independent contexts do not possess this property. Both plots are computed over 10 samples of (query, continuation, retrieved contexts).}
    \label{fig:loss-landscape-interp}
\end{figure}

\section{Learning to use composed states}
As previously noted, in practice, for SSM models consisting of multiple state space blocks stacked with temporal convolutions, $x(\bm{u})$ in~\eqref{eq:caso-expand} will not be exactly the state arising from a concatenated list of inputs. In this section, we introduce a fine-tuning objective to enable SSMs to better leverage composed states. Let $\mathcal{D} = \{(\bm{u}_i, u_i, S_i) \}_{i=1}^N$ 
be a dataset of sequences $\bm{u}_i$, their next-token continuation $u_i$, and a collection (in some particular order) of contexts $S_i$ retrieved from a database using $\bm{u}_i$. We minimize the prediction loss over the continuation, given a (composed) initial state and the query sequence:

\[
\mathcal L_{BPTC}(\theta) = \sum_{(\bm{u}_i,u_i,S_i) \in \mathcal D} L_{\rm CE}(f_\theta(\bm{u}_i,  x^{\rm PICASO}(S_i)), u_i),
\]
where $L_{\rm CE}(\cdot, \cdot)$ is the cross-entropy loss.

We denote this learning objective Backpropagation Through Composition (BPTC), where gradients are propagated through the state composition process $x^{\rm PICASO}$.
To reduce training time, we also consider an alternative version where we do not backpropagate through the composition step, which we denote Backpropagation To Composition (BP2C):
\[
\mathcal L_{BP2C}(\theta) = \sum_{(\bm{u}_i,u_i,S_i) \in \mathcal D} L_{\rm CE}(f_\theta(\bm{u}_i,  \operatorname{sg}\left[x^{\rm PICASO}(S_i)\right]), u_i),
\]
where $\operatorname{sg}$ denotes the stop-gradient operator.
We will show that when used for fine-tuning, this learning objective greatly improves the model's ability to leverage composed states for generation to the level of the concatenation albeit with much faster speeds, while maintaining performance on standard LLM evaluation tasks.

\section{Experiments}
\begin{figure}[th]
    \centering
    \includegraphics[width=0.44\textwidth]{figures/mamba2-2.7b-zeroshot-wikitext-results.pdf}
    \hfill
    \includegraphics[width=0.44\textwidth]{figures/mamba2-2.7b-zeroshot-wikitext-timings.pdf}
    \caption{Zero-shot evaluation of PICASO using Mamba-2 compared to other composition methods on WikiText. While the performance of PICASO lags slightly behind that of concatenation (left), PICASO-R is on average $5.4\times$ faster (right). PICASO-S and PICASO-R perform similarly and yield overlapping curves (hence not visible in the left plot). Incorporating permutation invariance for concatenation via PIConcat-R gives the best results. However, it incurs magnitudes higher computational costs despite being performed within a single batched forward pass, hence we omit from the right plot to prevent it from disrupting the scale of the x-axis and focus comparisons on PICASO.}
    \label{fig:zero-shot-wikitext}
\end{figure}
\subsection{Implementation Details}
We run our main experiments on the largest available SSM on Huggingface - Mamba-2 2.7B \citep{dao2024transformers}. We evaluate our method on two large-scale datasets - WikiText-V2 \citep{merity2016pointer} and MSMARCO \citep{msmarco}. We use the training splits as our fine-tuning data, and the testing/validation splits respectively for evaluation. To pre-process WikiText-V2 for our use case, we split each passage in the dataset into two equal context ``segments", with the goal of predicting the second (continuation) from the first (query). The retrieval database comprises all remaining segments, from which we retrieve via an external sentence embedding model, All-MiniLM-L6-v2\footnote{https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2}. In most experiments, we retrieve up to 10 segments, since improvements appears to saturate beyond that, and loss from concatenation blows up as a result of exceeding training context length (\Cref{fig:picaso-scaling-context}, Appendix). We pre-process MSMARCO by filtering only entries with well-formed answers and discarding those without relevant passages. 

We used the official benchmark\footnote{https://github.com/state-spaces/mamba} with an A100 GPU for our timing experiments in \Cref{fig:timings} to ensure fairest comparisons. For the rest of the experiments, we run the model in full-precision, and evaluate performance of the model starting from a custom initial state, a feature not supported by the official benchmark at the time of writing, as such timings differ.

For fine-tuning experiments using BPTC and BP2C, we base our implementation on the official HuggingFace \footnote{https://github.com/huggingface/transformers} trainer with default hyperparameters, and retrieve the $k$ most relevant context segments for each query sample for composition. For WikiText, we select $k \in \{0, \ldots, 10\}$ uniformly at random for each batch. For MSMARCO, we use all the available passages (both relevant and irrelevant) associated with each training example. For both datasets, we fine-tune for only 1 epoch. In all fine-tuning experiments, we ensure the training set (both the examples and the context database) are disjoint from the validation set to ensure fair evaluation. 

\subsection{Comparison Models}
We compare inference accuracy (measured by log-perplexity) and processing latency of PICASO with its order-dependent version, CASO, in addition to the following methods:

\noindent\textbf{Baseline}: Loss of the model on the test sample without using any contextual information.

\noindent\textbf{Concatenation} \citep{ram2023context}: We concatenate individual context segments based on a specific ordering. For WikiText-V2 experiments, we consider the ``best-case ordering" as determined by the sentence embedding model where more relevant contexts are closer to the query (at the end). We initialize the model with the state of the earliest context segment in the concatenation, which we assume to be available via pre-processing, and recompute the composed state from only the remaining ones. 

\noindent\textbf{Soup} \citep{pioro2024state}: Simple averaging of states obtained from each context.


\subsection{Main Results}
In this section, we evaluate both the zero-shot and fine-tuned performance of PICASO
in \Cref{sec:exp-results-zero-shot} and \Cref{sec:exp-results-bptc} respectively, and show in \Cref{sec:exp-results-nooverfit} that the fine-tuned model does not overfit to the composition task. We also include additional experiments showing that LLM capabilities are not impacted by fine-tuning in \Cref{app:btpc-standard-llm-tasks}, and show that PICASO can also be used for data attribution in \Cref{app:attribution} 

\subsubsection{Zero-shot performance}
\label{sec:exp-results-zero-shot}
We demonstrate in \Cref{fig:zero-shot-wikitext} that applying PICASO-R in a zero-shot manner on WikiText-V2 greatly improves performance over the baseline by an average of $10.1\%$ across 1-10 retrieved context segments. This greatly improves over Soup ($8.5\%$) and CASO ($9.2\%$). Compared to concatenation ($11.1\%$), PICASO-R performs slightly worse but benefits from magnitudes improvement in processing time on an average of $5.4\times$. In this task, PICASO-R achieves almost exactly the same performance as PICASO-S, but with a much faster composition time. As a sanity check for motivation for our method, we show that PIConcat achieves the best performance ($12.0\%$) overall, but at the cost of significantly greater computational time despite our batched-inference implementation. 

In Row 1 of \Cref{tab:msmarco}, we show that applying PICASO-R and PICASO-S in a zero-shot manner on MSMARCO similarly yields considerable improvements ($37.2\%$) over the naive baseline, while achieving performance close to that of concatenation ($41.3\%$). 

\begin{figure}[th]
    \centering
\includegraphics[width=0.325\textwidth]{figures/mamba2-2.7b-finetuned-bptt-wikitext-results.pdf}
    \includegraphics[width=0.325\textwidth]{figures/mamba2-2.7b-finetuned-bptt-wikitext-timings.pdf}
    \includegraphics[width=0.325\textwidth]{figures/mamba2-2.7b-finetuned-wikitext-results.pdf}
    \caption{\textbf{(Left + Middle:)} Fine-tuning with BPTC on WikiText brings the performance of PICASO to that of concatenation, while retaining its significant speed advantages. \textbf{(Right:)} Fine-tuning with BP2C on WikiText improves the effectiveness of PICASO as well, but is much faster in terms of training time since it does not require backpropagating through the composed state. Note that fine-tuning has no impact on the actual composition time when used for inference.}
    \label{fig:finetuned-bptc-wikitext}
\end{figure}
\subsubsection{Backpropagation Through and To Composition}
\label{sec:exp-results-bptc}

While PICASO demonstrates strong performance in the zero-shot setting, PICASO still lags behind concatenation in terms of prediction accuracy. We attribute this to composed states being ``out-of-distribution" for the model, since these states do not arise from any sequence of input tokens. In this section, we test if this can be resolved via fine-tuning with PICASO-R composed states via BPTC and BP2C. Indeed, as we show in \Cref{fig:finetuned-bptc-wikitext}, BPTC and BP2C greatly improves the performance of PICASO-R and PICASO-S to that similar to concatenation, while maintaining much faster processing timings on WikiText.  Similarly, we show in Rows 4-5 of \Cref{tab:msmarco} that fine-tuning on the MSMARCO training set also levels the performance of PICASO with that of concatenation. We also note that while BP2C is significantly faster in terms of training time, it incurs a small performance trade-off compared to BPTC for both datasets, keeping number of training iterations constant.



\begin{table}[tb]
    \footnotesize
    \centering
    \setlength{\tabcolsep}{2.5pt}
    \caption{All models in this table are evaluated on the MSMARCO validation set. We evaluate performance of models fine-tuned via BPTC/BP2C on both the WikiText and MSMARCO training sets. Rows 2-3 show that fine-tuning models to compose WikiText context segments does not harm performance when evaluated on composing context segments from MSMARCO. When composing segments from distributions similar to those encountered during training (Rows 4-5), PICASO matches the performance of concatenation while being magnitudes faster. }
    \begin{tabular}{lcccccc}\toprule
& Naive & Concat & Soup & CASO & PICASO-R & PICASO-S \\
\midrule
Mamba2-2.7B Base & 2.42 & 1.42 & 2.04 & 1.56 & 1.52 & 1.52 \\
Mamba2-2.7B BP2C-WikiText & 2.44 & 1.44 & 2.07 & 1.53 & 1.50 & 1.50 \\
Mamba2-2.7B BPTC-WikiText & 2.43  & 1.46 & 2.08 & 1.53 & 1.49 & 1.49 \\
Mamba2-2.7B BP2C-MSMARCO & 1.85  & 0.68  & 1.27 & 0.72 & 0.69 & 0.69 \\
Mamba2-2.7B BPTC-MSMARCO & 1.79  & 0.65 & 1.20 & 0.68 & 0.65 & 0.65 \\ 
\bottomrule
    \end{tabular}
    \label{tab:msmarco}
\vspace{-1em}
\end{table}

\subsubsection{Evaluation of fine-tuned model on other different tasks}
\label{sec:exp-results-nooverfit}
We showed that models fine-tuned on a specific downstream task (training set) using BPTC/BP2C perform strongly when composing samples drawn from a similar distribution (test set). We further show in \Cref{tab:msmarco} that models fine-tuned on one domain (WikiText) can demonstrate small performance gains (or at the very least, no performance loss) when composing samples via PICASO on another domain (MSMARCO). 
Finally, we show in \Cref{app:btpc-standard-llm-tasks} that fine-tuning models with BP2C/BPTC maintain (and occasionally even improve) performance on general LLM evaluation tasks compared against the original model.






\section{Limitations and Discussion}

We have proposed a method, PICASO, that enables efficient retrieval and composition of contexts by pre-processing their individual states. Without any training, our approach can handle the composition of information contained in up to 10 context segments in a manner that is order-invariant. PICASO notably requires zero online model processing time, since generation can begin directly from the composed states. When models are further fine-tuned with our proposed learning objective, states composed using PICASO perform comparably to those produced from the concatenation of context tokens, while offering on average a $5.4\times$ faster composition time.

Nevertheless, our method does have some limitations. When applied in a zero-shot manner, PICASO still lags slightly behind concatenation in terms of prediction accuracy. 
PICASO is also currently limited to architectures based on SSM layers. We leave as future work extension of PICASO towards recently popularized attention-based hybrid models, which require more sophisticated methods of composing key-value caches. Lastly, we also leave as future work the exploration of parameter-efficient fine-tuning methods such as adapters, which can be used to augment the model at inference time to enable state composition while preserving the original model's behavior.

\bibliography{refs}
\bibliographystyle{iclr2025_conference}

\clearpage
\appendix

\section{Algorithms: PICASO-S and PICASO-R}

We show in \Cref{alg:picaso-s} how PICASO-S is computed in polynomial time via a dynamic programming approach based on \Cref{alg:picaso-s-helper}. In \Cref{alg:picaso-r}, we also show how PICASO-R can be computed with linear time complexity. Time complexity is measured as the number of arithmetic operations required as a function of number of context states.


\begin{algorithm}[H]
\caption{PICASO-S- $\mathcal{O}(n^3)$}
\label{alg:picaso-s}
\begin{algorithmic}
\Require States $x = \{x_i\}_{i=0}^{n-1}$, Decays $A = \{A_i\}_{i=0}^{n-1}$
\State \Return $\sum_{i=0}^{n-1}$ \texttt{PICASO-S-DP}$(A_{-i}) \cdot x_i$
\State \Comment $A_{-i}$ denotes all elements of $A$ except $A_i$
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[H]
\caption{PICASO-S-DP - $\mathcal{O}(n^2)$}
\label{alg:picaso-s-helper}
\begin{algorithmic}
\Require Decays $A = \{A_i\}_{i=0}^{n-1}$
\State DP[:,:] $\gets$ zeros($n, n$)
\State DP[0,:] $\gets 1$
\State $w \gets 0$
\For{$i = 1 , \ldots , n-1$}
    \For{$j = i, \ldots, n-1$}
            \State DP$[i][j] \gets$ DP$[i][j-1] + A_{j-1}\cdot$ DP$[i-1][j-1]$
    \EndFor
\EndFor
\For{$i=0,\ldots,n-1$}
\State $w \gets w + \frac{1}{ n \cdot {{n-1} \choose i} } \cdot$ DP$[i][n-1]$ 
\EndFor
\State\Return $w$
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[H]
\caption{PICASO-R - $\mathcal{O}(n)$}
\label{alg:picaso-r}
\begin{algorithmic}
\Require States $x = \{x_i\}_{i=0}^{n-1}$, Decays $A = \{A_i\}_{i=0}^{n-1}$
\State $\hat{x} \gets 0, \quad \hat{A} \gets [A_1, \ldots, A_n, A_1, \ldots, A_n]$
\State $\hat{A}_{\Pi} = \operatorname{cumprod}(\hat{A})$ %
\State $\hat{A}_{\Sigma\Pi} = \operatorname{cumsum}(\hat{A_\Pi})$
\For{$i = 1 , \ldots , n-1$}
\State $w_i \gets \frac{1}{n} \cdot \left( \left( \hat{A}_{\Sigma\Pi}[n + i - 1] - \hat{A}_{\Sigma\Pi}[i] \right) {\hat{A}_{\Pi}[i]}^{-1} + 1 \right)$
\State $\hat{x} \gets \hat{x} + w_i \cdot x_i$
\EndFor
\State \Return $\hat{x}$
\end{algorithmic}
\end{algorithm}

\section{Further Analysis}



\subsection{Computational Costs of PIConcat}
In \Cref{fig:wikitext-piconcat-timings}, we visualize the computational costs incurred by PIConcat, which we show to dominate that of other methods despite resulting in the best performance on the WikiText dataset.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/mamba2-2.7b-piconcat-zeroshot-wikitext-timings.pdf}
    \caption{Timings for different composition algorithms evaluated on WikiText using Mamba-2 2.7B (zero-shot), including that of PIConcat-R. While PIConcat results in the best performance (y-axis), its computational cost (x-axis) is significantly higher than that of other methods. We refer to \Cref{fig:zero-shot-wikitext} for a more condensed plot to compare the remaining methods.}
    \label{fig:wikitext-piconcat-timings}
\end{figure}

\subsection{Scaling beyond effective context length}
\begin{figure}[h]
    \centering    
\includegraphics[width=0.6\textwidth]{figures/mamba2-2.7b-manydocuments-zeroshot-wikitext-results.pdf}
    \caption{Concatenation scales poorly with total size of retrieved contexts beyond training context length. PICASO yields greater stability even composing up to 50 context segments retrieved from WikiText. }
    \label{fig:picaso-scaling-context}
\end{figure}


In \Cref{fig:picaso-scaling-context}, we show that as the total length of retrieved contexts scale beyond a certain threshold (effective context size of the model), the loss from concatenation blows up and rapidly increases beyond the no-retrieval baseline. On the other hand, performance of PICASO remains stronger than that of the baseline when composing 50 context segments.

\subsection{Inference vs Processing Time}
In \Cref{fig:mamba2-processing-vs-inf}, we show that the context processing time for Mamba-2 comprises a significant proportion of the total generation time. For large sequence lengths beyond 6K tokens, the processing time even dominates the inference time for generating 32 tokens.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/mamba2-processing-vs-inference-time.pdf}
    \caption{Mamba-2 Processing vs Inference Time of 32 tokens. Processing time (orange) occupies a significant proportion of the total time taken to generate from an input sequence, even dominating the constant inference time from the processed state (blue) as number of tokens in the input grows.}
    \label{fig:mamba2-processing-vs-inf}
\end{figure}

\subsection{Performance on LLM Evaluation Tasks}
\label{app:btpc-standard-llm-tasks}
In \Cref{tab:btpc-standard-llm-tasks}, we show that fine-tuning Mamba2-2.7B with BTPC/BP2C objectives do not degrade existing LLM capabilities when evaluated on several LLM evaluation benchmarks - HellaSwag \citep{zellers2019hellaswag}, PIQA \citep{bisk2020piqa}, ARC-E, ARC-C \citep{clark2018think}, WinoGrande \citep{sakaguchi2021winogrande}, and OpenbookQA \citep{mihaylov2018can}.


\begin{table}[h]
\setlength{\tabcolsep}{5pt}
\footnotesize
    \caption{Evaluation of Mamba2-2.7B trained with BPTC and BP2C on LLM evaluation tasks. Here, we show that fine-tuning for composition does not degrade existing LLM capabilities. In this table, we report the length-normalized accuracy for each task.}
    \centering
    \begin{tabular}{lcccccc}
    \toprule
     & HellaSwag & PIQA & ARC-E & ARC-C & WinoGrande & OpenbookQA  \\
    \midrule
    Mamba2-2.7B Base & $66.6 \pm 0.5$ & $76.3 \pm 1.0$ & $64.8 \pm 1.0$ &  $36.3 \pm 1.4$  & $63.9\pm1.4$ & $38.8\pm2.2$\\
    BP2C-WikiText & $66.7 \pm 0.5$ & $76.3 \pm 1.0$& $64.9 \pm 1.0$ & $37.5 \pm 1.4$ & $63.6\pm1.4$ & $39.8\pm2.2$ \\
    BPTC-WikiText & $66.7 \pm 0.5$ & $75.6 \pm 1.0$ & $64.9 \pm 1.0 $ & $37.2 \pm 1.4$ & $63.2 \pm 1.4$ & $40.2 \pm 2.2$\\
    \bottomrule
    \end{tabular}
    \label{tab:btpc-standard-llm-tasks}
\end{table}

\subsection{Ablation on Choice of Retriever}
In \Cref{fig:retriever-ablation}, we ablate the impact of difference retriever choices on PICASO-R. In particular, we evaluate the performance of PICASO-R on WikiText when using the following embedding models from Sentence-Transformers \citep{reimers-2019-sentence-bert}: average\_word\_embeddings\_glove.6B.300d,
all-MiniLM-L6-v2, and
all-mpnet-base-v2, arranged in increasing order of performance on 14 different sentence embedding tasks \citep{reimers-2019-sentence-bert}. As expected, \Cref{fig:retriever-ablation} shows that the performance of PICASO-R highly correlates with the strength of the retriever, where stronger retrievers yields better results on WikiText.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/mamba2-2.7b-wikitext-retriever-ablation-picaso-r.pdf}
    \caption{Ablation study on how choice of retriever model impacts performance of PICASO-R on WikiText. As expected, stronger retriever models result in better downstream performance.}
    \label{fig:retriever-ablation}
\end{figure}


\subsection{Evaluation on Multiple Choice Tasks}
In this section, we evaluate PICASO-R on the OpenbookQA \citep{mihaylov2018can} multiple-choice task, where we retrieve from a context database of full passages from WikiText-V2. While OpenbookQA provides the ground truth fact for each evaluation sample, we discard this in our evaluations following standard practice in \cite{eval-harness}. We leverage the same retrieval model used for the main WikiText experiments. \Cref{tab:openqa} shows that PICASO-R achieves performance close to concatenation, with a $8\times$ speed-up in composition time.
\begin{table}[h]
    \centering
    \begin{tabular}{cccccc}
    \toprule
         \multicolumn{2}{c}{Naive} & \multicolumn{2}{c}{Concat} & \multicolumn{2}{c}{PICASO-R}   \\
        Acc ($\uparrow$) & Time ($\downarrow$) &  Acc ($\uparrow$) & Time ($\downarrow$) & Acc ($\uparrow$) & Time ($\downarrow$)  \\
        \midrule
        38.8\% & NA & 40.0\% & 233 ms & 39.9\% & 29 ms \\
         \bottomrule
         & 
    \end{tabular}
    \caption{Evaluation on OpenbookQA dataset, augmented with retrieved passages from WikiText. We use normalized accuracy as our evaluation metric, and report the time taken to compose retrieved passages. Numbers shown in the table are averaged across retrieving between 1 to 10 full contexts from WikiText (as opposed to half context segments in our main paper experiments). }
    \label{tab:openqa}
\end{table}

\subsection{Context Statistics}

In \Cref{fig:wikitext-stats}, we plot the distribution over the lengths (tokens and characters) of retrieved context segments used in the main paper WikiText retrieval dataset.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/wikitext-stats-characters.pdf}
    \includegraphics[width=0.48\textwidth]{figures/wikitext-stats-tokens.pdf}
    \caption{Histogram of the lengths, in terms of (\textbf{Left}) characters and (\textbf{Right}) tokens, of database context segments used in the main paper WikiText experiments.  } 
    \label{fig:wikitext-stats}
\end{figure}

\section{Data Attribution}
\label{app:attribution}
\begin{table}[ht]
    \centering
    \caption{Zero-shot Data Attribution on MSMARCO with Mamba2-2.7B, measured by precision. We compare Leave-One-In (LOI) and Leave-One-Out (LOO), where we implement LOO with varying methods for state composition. }
    \begin{tabular}{cccccc}
    \toprule
          LOI & Concat & Soup & CASO & PICASO-R & PICASO-S \\
         \midrule
         0.699 & 0.690 &  0.629 & 0.725 & 0.732 & 0.731 \\
         \bottomrule
    \end{tabular}
    \label{tab:attribution}
\end{table}

Consider a question-answer pair $(\bm{u}_q, \bm{u}_a)$, and a sequence of potentially relevant contexts $S = (\bm{u}_1, \ldots, \bm{u}_n)$. We would like to select the most relevant context for inferring the answer. There are at least two ways to do so with model $f_\theta$:

The first method, which we term ``Leave-one-in", is to prepend each candidate context to the question, and evaluate the loss on the answer. Equivalently, $\argmin_{i} L_{CE}(f_\theta(\bm{u}_q, x(\bm{u}_i)), \bm{u}_a)$, where we abuse notation to denote loss on the sequence (instead of token) $\bm{u}_a$.

The second method, which we term ``Leave-one-out", is to compare the marginal increase in loss of the answer when removing each candidate from the composition of all of them. Equivalently, $\argmax_{i} \{ L_{CE}(f_\theta(\bm{u}_q, \hat{x}(S_{-i})), \bm{u}_a) - L_{CE}(f_\theta(\bm{u}_q, \hat{x}(S)), \bm{u}_a) \}$, where $\hat{x}(S_{-i})$ denotes a state composed from all contexts in $S$ other than $\bm{u}_i$.

Intuitvely, the former measures ``absolute" influence of a context, while the latter measures ``relative" influence computed as the marginal improvement from adding it to the set of other contexts.

There are several different ways to implement the latter by varying the composition method used. We show in \Cref{tab:attribution} that not only does Leave-One-Out perform best on the MSMARCO dataset, but implementing Leave-One-Out with PICASO-S and PICASO-R not only accelerates processing, but also surpasses the performance of conatenation. We attribute this to the permutation-invariance property of PICASO, which unlike concatenation, does not introduce irrelevant biases arising from arbitrary context orders.





\section{Concatenation for SSMs: Connection to jump-linear systems}

Consider a collection of context segments retrieved based on relevance to a query, and sorted randomly as context to the query. While these segments share information, they are independent given the query, and their order is accidental and uninformative.  

We are interested in a model that can efficiently process inputs in this format and extract all shared information from the input. Attention-based models are a natural choice because of the permutation-invariance of attention mechanisms (ignoring positional encoding), but they would have to process the entire input (all segments) with quadratic inference cost. On the other hand, SSMs have linear cost, but they are ill-fit to process this kind of input because of the context switches, which make the Markov assumption implicit in the state representation invalid. 

We consider a broader class of models, namely switching dynamical systems (or jump-Markov, jump-diffusion, or linear hybrid, or jump-linear systems) as the class of interest. A jump-linear system is one that has a continuous state, say $x_t$ that evolves synchronously, and a discrete state that changes the value of $x_t$, for instance
\[
x_{t+1} = \begin{cases}
    A x_t + Bu_t \quad {\rm if} \ t \in {\mathcal Z} \backslash \Omega \\
    x_{t+1} \sim P \quad {\rm if} \ t \in \Omega
\end{cases}
\]
Learning and inference for this model class corresponds to identification and filtering for this class of Jump-Markov models. In addition to a random switching, the switch can be triggered by a particular `flag' (value) of the input: 
\[
x_{t+1} = \begin{cases}
    A x_t + Bu_t \quad {\rm if} \ u_t \neq u_{\rm trigger}   \\
    x_{t+1} \sim P \quad {\rm if} \ u_t = u_{\rm trigger} 
\end{cases}
\]
If the value of $u_{\rm trigger}$ is known, then a given identification and filtering scheme can be applied by switching the estimated state according to the trigger.

Since modern state space models are input-dependent, they automatically fit the latter class of models and can handle switches without modifications. However, what they cannot handle is the fact that the order of the segments is uninformative. As a result, presenting the same segments in different order would yield different states.
Accordingly, our goal is to enable SSMs to learn from segments up to permutations, so we can accommodate sequences where the ordering within segments is informative and respected, while the ordering of segments is uninformative and factored out.


\section{General Recurrence Structure}
In the main paper, we introduced a specific recursive relation satisfied by Elementary Symmetric Polynomials. Here, we introduce a more general form which can potentially be used for more efficient implementations:

\begin{proposition} For any choice of $1 \leq q \leq n-1 $
\label{eqn:esp-recursion}
\begin{align*}
e_m(A_1, \cdots, A_{n-1}) = \sum_{j=\text{max}(q+m-n+1,0)}^{\text{min}(m,q)}e_{m-j}(A_1, \cdots, A_{n-1-q})e_{j}(A_{n-q},\cdots A_{n-1})
\end{align*}
\end{proposition}
\begin{proof}
    We compute $e_m(A_1, \cdots, A_{n-1})$ using a Dynamic Programming (DP) approach, where we break the problem into smaller problems, and merge the solutions. First we split the $n-1$ variables at some random index $q$ to create two partitions, ($A_1 \cdots, A_{n-1-q})$ and $(A_{n-q}, \cdots A_{n-1})$, and then compute $e_{m-j}$ and $e_{j}$ on each partition respectively. For a given value of $j$, $e_{m-j}(A_1, \cdots, A_{n-1-q})e_{j}(A_{n-q},\cdots A_{n-1})$ will only compute a subset of values from $e_m(A_1, \cdots, A_{n-1})$, and hence we sum over all possible values for $j$.
\end{proof}
In particular, taking $q=1$, we obtain the following:
\begin{align*}
     e_m(A_1, \ldots, A_{n-1}) = A_{n-1} e_{m-1}(A_1, \ldots, A_{n-2}) + e_m (A_1, \ldots, A_{n-2})
\end{align*}
which we use for our implementation of PICASO-S.





\end{document}
