\section{Related Work}
\paragraph{State Space Models and Hybrid Models.}


Recent efforts to overcome the significant computational costs of Transformer models on long contexts have inspired the exploration of more efficient alternatives, including State Space Models (SSMs). Through maintaining a fixed-size ``state'', a sufficient statistic of the past for the purpose of future prediction, these models offer advantages compared to Transformer models. They only require constant memory consumption regardless of the sequence length, and linear computational complexity, rather than quadratic, as longer sequences are processed.  
The idea of leveraging recurrent models with fixed dimensional states to represent complex sequences is not new, in fact, several variations of SSMs have been developed in the past, ranging from Linear Time Invariant (LTI) systems, to more expressive non-linear Time Varying \citep{jazwinski2007stochastic} and Input Varying \citep{krener1975bilinear} systems.

More recently, many of these ideas have been rediscovered and implemented on modern parallel hardware as basic building blocks for Foundation Models. 
\cite{gu2023mamba} proposed Mamba, an input-dependent linear SSM (termed `selective') based on LIV systems, that achieves comparable performance to Transformers \citep{vaswani2017attention} on language modeling while enjoying faster inference. 
Mamba-2 \citep{dao2024transformers} further improved computational time by implementing SSM layers with structured matrix multiplications to better leverage modern Tensor Cores.
Although pure SSM models can compete with Transformer blocks on many NLP tasks, they lag behind on tasks that require strong recall capabilities. To balance inference efficiency and model capabilities, Hybrid models combining Attention and SSM blocks have been proposed.
\cite{lieber2024jamba} combined SSM blocks along with global-attention blocks to create a hybrid architecture with Mixture-of-Expert layers for training larger models. To further improve long context ability and efficiency, \cite{ren2024samba} leveraged sliding window attention, while \cite{zancato2024b} developed a general family of architecture that include Transformers, SSMs and their hybrid combinations, leveraging both verbatim and fading memory, in both long- and short-term.
\color{black}

\paragraph{Retrieval-Augmented Generation and In-Context Learning.}
Our work falls within the scope of In-Context Retrieval-Augmented Language Models \citep{ram2023context}, where language models are conditioned on retrieved contexts via concatenation. Retrieval Augmented Generation (RAG) allows language models to leverage knowledge stored in external databases, which greatly improves performance on knowledge-intensive and domain-specific tasks \citep{lewis2020retrieval}. In our work, we simply use a pre-trained sentence embedding model for retrieval, and we refer to \cite{gao2023retrieval} for a detailed survey on other mechanisms.
Apart from retrieval, processing (multiple) retrieved contexts can also greatly increase generation latency. \cite{izacard2023atlas} mitigates this by independently processes each retrieved context with a LLM encoder, using cross attention over the concatenated encoder outputs. \cite{zhu2024accelerating} similarly encodes retrieved contexts in parallel, and performs decoding in a selective manner by attending only to highly relevant encoder outputs.

In-Context Learning (ICL) \citep{brown2020language} has emerged as an effective method to perform inference without learning (\textit{i.e.}, transduction), by conditioning on labeled samples provided in-context, commonly implemented as a set of (query, answer) pairs \citep{dong2022survey}. 
Similar to RAG, the quality of selected demonstrations have been shown to greatly affect downstream performance \citep{xu2024context}. Several methods have been developed for selecting effective demonstrations, based on sentence embeddings \citep{liu2021makes}, mutual information \citep{sorensen2022information}, perplexity \citep{gonen2022demystifying}, and even BM25 \citep{robertson2009probabilistic}.
Similar to the motivation of our work, several studies have shown that the performance of ICL is heavily dependent on demonstration ordering. \cite{zhao2021calibrate} shows that answers positioned towards the end of the prompt are more likely to be predicted, while \cite{lu2021fantastically} shows that results can vary wildly between random guess and state-of-the-art depending on the order that demonstrations are presented. Outside of ICL, \cite{liu2024lost} further shows that language models do not robustly utilize information in long input contexts due to sensitivity to positioning.


\paragraph{Model and State Composition.}
Our work falls into the category of 
composing of deep models, representations, and states. \cite{wortsman2022model} proposes Model Soup, which composes multiple non-linearly fine-tuned models via averaging model weights. \cite{liu2023tangent1,liu2023tangent2} leverages model linearization to enforce an equivalence between weight averaging and output ensembling. \cite{perera2023prompt} independently learns task-specific prompts which can be linearly averaged to yield new prompts for composite tasks.
For SSMs, \cite{pioro2024state} investigates averaging of states, along with decay-weighted mixing which is closely related to a baseline version of our method, CASO. However, the equations described in their work differ from CASO, and their evaluations are limited to composition of two equal-length contexts. In contrast, our method greatly improves upon CASO by incorporating permutation invariance, which we show is important to achieve performances comparable to that of concatenation.