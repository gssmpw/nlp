\section{Related Work}
\paragraph{State Space Models and Hybrid Models.}


Recent efforts to overcome the significant computational costs of Transformer models on long contexts have inspired the exploration of more efficient alternatives, including State Space Models (SSMs). Through maintaining a fixed-size ``state'', a sufficient statistic of the past for the purpose of future prediction, these models offer advantages compared to Transformer models. They only require constant memory consumption regardless of the sequence length, and linear computational complexity, rather than quadratic, as longer sequences are processed.  
The idea of leveraging recurrent models with fixed dimensional states to represent complex sequences is not new, in fact, several variations of SSMs have been developed in the past, ranging from Linear Time Invariant (LTI) systems, to more expressive non-linear Time Varying ____ and Input Varying ____ systems.

More recently, many of these ideas have been rediscovered and implemented on modern parallel hardware as basic building blocks for Foundation Models. 
____ proposed Mamba, an input-dependent linear SSM (termed `selective') based on LIV systems, that achieves comparable performance to Transformers ____ on language modeling while enjoying faster inference. 
Mamba-2 ____ further improved computational time by implementing SSM layers with structured matrix multiplications to better leverage modern Tensor Cores.
Although pure SSM models can compete with Transformer blocks on many NLP tasks, they lag behind on tasks that require strong recall capabilities. To balance inference efficiency and model capabilities, Hybrid models combining Attention and SSM blocks have been proposed.
____ combined SSM blocks along with global-attention blocks to create a hybrid architecture with Mixture-of-Expert layers for training larger models. To further improve long context ability and efficiency, ____ leveraged sliding window attention, while ____ developed a general family of architecture that include Transformers, SSMs and their hybrid combinations, leveraging both verbatim and fading memory, in both long- and short-term.
\color{black}

\paragraph{Retrieval-Augmented Generation and In-Context Learning.}
Our work falls within the scope of In-Context Retrieval-Augmented Language Models ____, where language models are conditioned on retrieved contexts via concatenation. Retrieval Augmented Generation (RAG) allows language models to leverage knowledge stored in external databases, which greatly improves performance on knowledge-intensive and domain-specific tasks ____. In our work, we simply use a pre-trained sentence embedding model for retrieval, and we refer to ____ for a detailed survey on other mechanisms.
Apart from retrieval, processing (multiple) retrieved contexts can also greatly increase generation latency. ____ mitigates this by independently processes each retrieved context with a LLM encoder, using cross attention over the concatenated encoder outputs. ____ similarly encodes retrieved contexts in parallel, and performs decoding in a selective manner by attending only to highly relevant encoder outputs.

In-Context Learning (ICL) ____ has emerged as an effective method to perform inference without learning (\textit{i.e.}, transduction), by conditioning on labeled samples provided in-context, commonly implemented as a set of (query, answer) pairs ____. 
Similar to RAG, the quality of selected demonstrations have been shown to greatly affect downstream performance ____. Several methods have been developed for selecting effective demonstrations, based on sentence embeddings ____, mutual information ____, perplexity ____, and even BM25 ____.
Similar to the motivation of our work, several studies have shown that the performance of ICL is heavily dependent on demonstration ordering. ____ shows that answers positioned towards the end of the prompt are more likely to be predicted, while ____ shows that results can vary wildly between random guess and state-of-the-art depending on the order that demonstrations are presented. Outside of ICL, ____ further shows that language models do not robustly utilize information in long input contexts due to sensitivity to positioning.


\paragraph{Model and State Composition.}
Our work falls into the category of 
composing of deep models, representations, and states. ____ proposes Model Soup, which composes multiple non-linearly fine-tuned models via averaging model weights. ____ leverages model linearization to enforce an equivalence between weight averaging and output ensembling. ____ independently learns task-specific prompts which can be linearly averaged to yield new prompts for composite tasks.
For SSMs, ____ investigates averaging of states, along with decay-weighted mixing which is closely related to a baseline version of our method, CASO. However, the equations described in their work differ from CASO, and their evaluations are limited to composition of two equal-length contexts. In contrast, our method greatly improves upon CASO by incorporating permutation invariance, which we show is important to achieve performances comparable to that of concatenation.