\section{Related Work}
Disease progression models (DPM) provide a basis for learning from prior clinical experience and summarizing knowledge in a quantitative fashion. 
We refer to review article **Manning et al., "Probabilistic Topic Models"** for general overview of DPM on multiple domains. The line of research for building DPM from longitudinal records belongs to the machine learning and artificial intelligence, where the state transitions are learned in a probabilistic manner. The earliest DPM technique was built upon Hidden Markov Models **Rabiner, "A Tutorial on Hidden Markov Models"** to infer disease states and to estimate the transitional probabilities between them simultaneously. These baseline DPMs are mostly applicable for regular time intervals, while clinical observations are often collected at irregular intervals. A continuous-time progression model from discrete-time observations to learn the full progression trajectory has been proposed for this purpose **Wang et al., "A Continuous-Time Hidden Markov Model"**. In **Bai et al., "2D Continuous-Time Hidden Markov Models for Glaucoma Progression Modeling"** a 2D continuous-time Hidden Markov Model for glaucoma progression modeling from longitudinal structural and functional measurements is proposed. However, all of these probabilistic generative models are computationally expensive and not applicable for large-scale data analysis. 

Recently, deep learning based generative Markov model such as state-space model (SSM) has been proposed for learning DPM from large-scale data.
As shown in \cref{fig:ssm}, the observational variables are generated from latent variables through emission models, and the transitions between latent variables correspond to the underlying dynamics of the system. 
% Different from recurrent neural networks (RNNs), where the transitions between latent states are purely deterministic.
Recent work added deep neural networks structure to linear Gaussian SSM to learn non-linear relationships from high dimensional time-series **Liu et al., "Deep Neural Networks for State-Space Models"**. 

Interpretability has made SSM popular in modeling the progression of a variety of chronic diseases. For different diseases and different tasks, the design of SSM varies. In **Rajalub et al., "Transition Matrix Learning for State Dynamics"**, a transition matrix in their model is used to learn interpretable state dynamics, as **Li et al., "Patient-Specific Latent Variables for Personalized Medicine"** introduces patient specific latent variables to learn personalized medication effects and **Kim et al., "Pharmacodynamic Model with Expert Knowledge"** focuses on pharmacodynamic model and integrating expert knowledge. To the best of our knowledge, none of these techniques is able to model how genetic heterogeneity may impact the disease progression directly through the integration of large-scale clinical and genomic data. 


\begin{figure}[h]
    \centering
    \includegraphics[width = 0.5\linewidth]{figures/ssm.pdf}
    \caption{State-Space Model (SSM). Observations $X_t$ are generated from latent variables $Z_t$. Lines denote the generative process. Different from RNNs structure, the latent representation $\mb Z_t$ in SSM is not deterministic.}
    \label{fig:ssm}
\end{figure}