\mf{The introduction should  1) state the problem, 2) underling it's importance, 3) tell what is available 4) tell the pain points/shortcomings of what is available 5) tell what we contribute 6) anticipate main results, 7) explain the organization of the paper.}

\mf{State the problem: RAG evaluation and meta-evaluation}
Retrieval augmented generation (RAG) is emerging as a powerful paradigm to improve LLMs factuality \cite{gao_retrieval-augmented_2024}. 
Grounding the LLM response on retrieved knowledge has shown to mitigate outdated knowledge, lack of domain expertise and to reduce hallucinations \cite{lewis2020retrieval, gao_retrieval-augmented_2024}. \mf{There is interest in benchmarking LLMs with RAG and developing auto-evaluators, which itself raises the need to benchmark auto-evaluators (meta-evaluation)}

Benchmarking LLMs with RAG is challenging due to the complexity of the pipeline that includes retrieval and generation. Text generation in general has two modes of evaluation: 1) reference-based where gold references are composed by humans and a direct (eg BERTScore \cite{Zhang2020BERTScore:}) or indirect multidimensional evaluation is conducted (reference-based multidimensional quality metrics (MQM) \cite{burchardt-2013-mqm} in the machine translation (MT) community) 2) reference-free where the model generations are judged by humans or models (aka quality estimation) across multiple dimensions (reference-free MQM). Multidimensional evaluation provides more comprehensive understanding of text generation systems and are the de facto % \dl{typo here?}
standard approach in the MT community. On the other hand, latest insights from the MT community show that reference-free methods are competitive with reference-based ones bringing into question the usefulness of composing human references \cite{rei-etal-2021-references, freitag-etal-2024-wmt24refnoref}. % Here, we focus on reference-free evaluation for RAG, following recent trends in the summarization and RAG communities\sm{cite, rephrase or remove}. 
In a reference-free evaluation setup, gold multidimensional human judgements (factuality, relevance, etc) of model generations are required. These judgements are leveraged in a meta-evaluation framework, where automated evaluators are evaluated against the human judgement to measure correlation. The automated evaluators can then be applied to measure the performance of new models outputs. 

\mf{Why is it important?}
Still important \cite{wu_how_2024}


Previous work for RAG evaluation mainly focused on English, and work on evaluating multilingual settings were restricted to the retrieval phase only, or leveraging human or machine translation of English datasets. End-to-end multilingual evaluation is important to reliably measure the performance across languages which can vary depending on language characteristics (low vs high resource, complex morphology, etc) and scripts (Latin vs non-Latin). Translation-based benchmarking suffers from translationese phenomena such as simpler language or lexical choices \cite{Baker1993CorpusLA, graham-etal-2020-statistical}, in addition to being distributionaly different from native data thus not necessarily reflecting native users preferences \cite{chen-etal-2024-good-data}.

To bridge those gaps, we build a native meta-evaluation multilingual end-to-end benchmark for RAG systems. We build on top of the popular MIRACL \cite{zhang_miracl_2023} dataset that includes native questions across 18 languages and relevance judgements of retrieved passages for multilingual retrieval evaluation. We extend MIRACL by generating answers using a diverse set of LLMs and judging the factuality and relevance of those answers by expert human annotators. We devise a protocol to collect human judgements in a reliable way, annotating each datapoint with 5 human annotators and achieving a high rate of inter-annotator-agreement. Our benchmark is directly useful to judge the LLMs performance on the generation task, but, more importantly, it can be used for meta-evaluation purposes to evaluate the performance of multilingual automated evaluators for RAG. We use LLM-as-a-judge for meta-evaluation and measure correlation between different LLM judges and human judgement. 

A recent trend in multilingual evaluation is using translation of English benchmarks, whether MT or human. Frontier models such as GPT-4~\cite{openai2024gpt4technicalreport}, Claude 3~\cite{claude32024} and LLama 3~\cite{dubey2024llama3herdmodels} all use machine translated versions of MMLU~\cite{hendrycks2021mmlu} %\dl{what is the acronym} 
to evaluate multilingual performance. On the other hand, \cite{chen-etal-2024-good-data} reveal a notable difference between using native and translated data for benchmarking LLM capabilities in terms of model ranking. We evaluate the usefulness of machine translated data in the meta-evaluation setup for RAG applications. We compare MT based data to our native benchmark in terms of automated evaluators (powered by different LLMs) ranking across languages.