
@misc{sharma2024fauxpolyglotstudyinformation,
      title={Faux Polyglot: A Study on Information Disparity in Multilingual Large Language Models}, 
      author={Nikhil Sharma and Kenton Murray and Ziang Xiao},
      year={2024},
      eprint={2407.05502},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.05502}, 
}

@inproceedings{fan-2024-rag-survey,
author = {Fan, Wenqi and Ding, Yujuan and Ning, Liangbo and Wang, Shijie and Li, Hengyun and Yin, Dawei and Chua, Tat-Seng and Li, Qing},
title = {A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671470},
doi = {10.1145/3637528.3671470},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {6491–6501},
numpages = {11},
keywords = {fine-tuning, in-context learning, large language model (llm), pre-training, prompting, retrieval augmented generation (rag)},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{behzad-etal-2024-ask,
    title = "To Ask {LLM}s about {E}nglish Grammaticality, Prompt Them in a Different Language",
    author = "Behzad, Shabnam  and
      Zeldes, Amir  and
      Schneider, Nathan",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.916",
    doi = "10.18653/v1/2024.findings-emnlp.916",
    pages = "15622--15634",
}

@misc{liu2024translationneedstudysolving,
      title={Is Translation All You Need? A Study on Solving Multilingual Tasks with Large Language Models}, 
      author={Chaoqun Liu and Wenxuan Zhang and Yiran Zhao and Anh Tuan Luu and Lidong Bing},
      year={2024},
      eprint={2403.10258},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.10258}, 
}

@inproceedings{lai-etal-2023-chatgpt,
    title = "{C}hat{GPT} Beyond {E}nglish: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning",
    author = "Lai, Viet Dac  and
      Ngo, Nghia  and
      Pouran Ben Veyseh, Amir  and
      Man, Hieu  and
      Dernoncourt, Franck  and
      Bui, Trung  and
      Nguyen, Thien Huu",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.878",
    doi = "10.18653/v1/2023.findings-emnlp.878",
    pages = "13171--13189",
}

@misc{wu_how_2024,
	title = {How well do {LLMs} cite relevant medical references? {An} evaluation framework and analyses},
	shorttitle = {How well do {LLMs} cite relevant medical references?},
	url = {http://arxiv.org/abs/2402.02008},
	urldate = {2024-04-20},
	publisher = {arXiv},
	author = {Wu, Kevin and Wu, Eric and Cassasola, Ally and Zhang, Angela and Wei, Kevin and Nguyen, Teresa and Riantawan, Sith and Riantawan, Patricia Shi and Ho, Daniel E. and Zou, James},
	month = feb,
	year = {2024},
	note = {arXiv:2402.02008 [cs]},
	keywords = {Evaluation, Language Models, RAG, Responsible AI},
	file = {arXiv.org Snapshot:/Users/marcfede/Zotero/storage/F77FV5XS/2402.html:text/html;Full Text PDF:/Users/marcfede/Zotero/storage/DDIZLS4Y/Wu et al. - 2024 - How well do LLMs cite relevant medical references.pdf:application/pdf},
}

@inproceedings{maynez-etal-2020-faithfulness,
    title = "On Faithfulness and Factuality in Abstractive Summarization",
    author = "Maynez, Joshua  and
      Narayan, Shashi  and
      Bohnet, Bernd  and
      McDonald, Ryan",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.173",
    doi = "10.18653/v1/2020.acl-main.173",
    pages = "1906--1919",
}

@inproceedings{
wu2024clasheval,
title={ClashEval: Quantifying the tug-of-war between an {LLM}{\textquoteright}s internal prior and external evidence},
author={Kevin Wu and Eric Wu and James Zou},
booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2024},
url={https://openreview.net/forum?id=WGoCZl2itU}
}

@inproceedings{thakur-etal-2024-nomiracl,
    title = "{``}Knowing When You Don{'}t Know{''}: A Multilingual Relevance Assessment Dataset for Robust Retrieval-Augmented Generation",
    author = "Thakur, Nandan  and
      Bonifacio, Luiz  and
      Zhang, Crystina  and
      Ogundepo, Odunayo  and
      Kamalloo, Ehsan  and
      Alfonso-Hermelo, David  and
      Li, Xiaoguang  and
      Liu, Qun  and
      Chen, Boxing  and
      Rezagholizadeh, Mehdi  and
      Lin, Jimmy",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.730",
    pages = "12508--12526",
}

@inproceedings{
hendrycks2021mmlu,
title={Measuring Massive Multitask Language Understanding},
author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=d7KBjmI3GmQ}
}

@misc{claude32024,
  title={The Claude 3 Model Family: Opus, Sonnet, Haiku},
  author={Anthropic},
  url={https://api.semanticscholar.org/CorpusID:268232499},
  year={2024},
}

@misc{dubey2024llama3herdmodels,
      title={The Llama 3 Herd of Models}, 
      author={Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Amy Yang and Angela Fan and Anirudh Goyal and Anthony Hartshorn and Aobo Yang and Llama Team @ Meta},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}

@misc{openai2024gpt4technicalreport,
      title={GPT-4 Technical Report}, 
      author={Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and OpenAI},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}

@inproceedings{Baker1993CorpusLA,
  title={Corpus Linguistics and Translation Studies: Implications and Applications},
  author={Mona Baker and Gill Francis and Elena Tognini-Bonelli},
  year={1993},
  url={https://api.semanticscholar.org/CorpusID:57174748},
  publisher={John Benjamins Publishing Company},
  booktitle={In Text and Technology: In Honour of John Sin- clair},
  address={Netherlands},
}

@inproceedings{graham-etal-2020-statistical,
    title = "Statistical Power and Translationese in Machine Translation Evaluation",
    author = "Graham, Yvette  and
      Haddow, Barry  and
      Koehn, Philipp",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.6",
    doi = "10.18653/v1/2020.emnlp-main.6",
    pages = "72--81",
}

@inproceedings{freitag-etal-2024-wmt24refnoref,
    title = "Are {LLM}s Breaking {MT} Metrics? Results of the {WMT}24 Metrics Shared Task",
    author = "Freitag, Markus  and
      Mathur, Nitika  and
      Deutsch, Daniel  and
      Lo, Chi-Kiu  and
      Avramidis, Eleftherios  and
      Rei, Ricardo  and
      Thompson, Brian  and
      Blain, Frederic  and
      Kocmi, Tom  and
      Wang, Jiayi  and
      Adelani, David Ifeoluwa  and
      Buchicchio, Marianna  and
      Zerva, Chrysoula  and
      Lavie, Alon",
    editor = "Haddow, Barry  and
      Kocmi, Tom  and
      Koehn, Philipp  and
      Monz, Christof",
    booktitle = "Proceedings of the Ninth Conference on Machine Translation",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.wmt-1.2",
    pages = "47--81",
}

@inproceedings{rei-etal-2021-references,
    title = "Are References Really Needed? Unbabel-{IST} 2021 Submission for the Metrics Shared Task",
    author = "Rei, Ricardo  and
      Farinha, Ana C  and
      Zerva, Chrysoula  and
      van Stigt, Daan  and
      Stewart, Craig  and
      Ramos, Pedro  and
      Glushkova, Taisiya  and
      Martins, Andr{\'e} F. T.  and
      Lavie, Alon",
    editor = "Barrault, Loic  and
      Bojar, Ondrej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-jussa, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Freitag, Markus  and
      Graham, Yvette  and
      Grundkiewicz, Roman  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Kocmi, Tom  and
      Martins, Andre  and
      Morishita, Makoto  and
      Monz, Christof",
    booktitle = "Proceedings of the Sixth Conference on Machine Translation",
    month = nov,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.wmt-1.111",
    pages = "1030--1040",
}

@inproceedings{burchardt-2013-mqm,
    title = "Multidimensional quality metrics: a flexible system for assessing translation quality",
    author = "Burchardt, Aljoscha",
    booktitle = "Proceedings of Translating and the Computer 35",
    month = nov # " 28-29",
    year = "2013",
    address = "London, UK",
    publisher = "Aslib",
    url = "https://aclanthology.org/2013.tc-1.6",
}

@inproceedings{
Zhang2020BERTScore:,
title={BERTScore: Evaluating Text Generation with BERT},
author={Tianyi Zhang and Varsha Kishore and Felix Wu and Kilian Q. Weinberger and Yoav Artzi},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SkeHuCVFDr}
}

@article{clark-etal-2020-tydi,
    title = "{T}y{D}i {QA}: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages",
    author = "Clark, Jonathan H.  and
      Choi, Eunsol  and
      Collins, Michael  and
      Garrette, Dan  and
      Kwiatkowski, Tom  and
      Nikolaev, Vitaly  and
      Palomaki, Jennimaria",
    editor = "Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "8",
    year = "2020",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2020.tacl-1.30",
    doi = "10.1162/tacl_a_00317",
    pages = "454--470",
}

@inproceedings{es-etal-2024-ragas,
    title = "{RAGA}s: Automated Evaluation of Retrieval Augmented Generation",
    author = "Es, Shahul  and
      James, Jithin  and
      Espinosa Anke, Luis  and
      Schockaert, Steven",
    editor = "Aletras, Nikolaos  and
      De Clercq, Orphee",
    booktitle = "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations",
    month = mar,
    year = "2024",
    address = "St. Julians, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.eacl-demo.16",
    pages = "150--158",
}

@book{manning2008ir,
  address = {Cambridge, UK},
  author = {Manning, Christopher D. and Raghavan, Prabhakar and Schütze, Hinrich},
  isbn = {978-0-521-86571-5},
  keywords = {book information introduction ir retrieval},
  publisher = {Cambridge University Press},
  timestamp = {2010-12-20T06:06:48.000+0100},
  title = {Introduction to Information Retrieval},
  url = {http://nlp.stanford.edu/IR-book/information-retrieval-book.html},
  year = 2008
}



@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@inproceedings{chen-etal-2024-good-data,
    title = "Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?",
    author = "Chen, Pinzhen  and
      Yu, Simon  and
      Guo, Zhicheng  and
      Haddow, Barry",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.542",
    pages = "9706--9726",
}

@misc{gao_retrieval-augmented_2024,
	title = {Retrieval-{Augmented} {Generation} for {Large} {Language} {Models}: {A} {Survey}},
	shorttitle = {Retrieval-{Augmented} {Generation} for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2312.10997},
	abstract = {Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domainspecific information. RAG synergistically merges LLMs’ intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-theart technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development 1.},
	language = {en},
	urldate = {2024-04-25},
	publisher = {arXiv},
	author = {Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Meng and Wang, Haofen},
	month = mar,
	year = {2024},
	note = {arXiv:2312.10997 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Gao et al. - 2024 - Retrieval-Augmented Generation for Large Language .pdf:/Users/mariaacb/Zotero/storage/WFUPYYEY/Gao et al. - 2024 - Retrieval-Augmented Generation for Large Language .pdf:application/pdf},
}

@misc{ouyang_training_2022,
	title = {Training language models to follow instructions with human feedback},
	url = {http://arxiv.org/abs/2203.02155},
	abstract = {Making language models bigger does not inherently make them better at following a user’s intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by ﬁne-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to ﬁne-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further ﬁne-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that ﬁne-tuning with human feedback is a promising direction for aligning language models with human intent.},
	language = {en},
	urldate = {2024-04-25},
	publisher = {arXiv},
	author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
	month = mar,
	year = {2022},
	note = {arXiv:2203.02155 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Ouyang et al. - 2022 - Training language models to follow instructions wi.pdf:/Users/mariaacb/Zotero/storage/GM2ENNNF/Ouyang et al. - 2022 - Training language models to follow instructions wi.pdf:application/pdf},
}

@article{zhang_miracl_2023,
	title = {\textbf{{MIRACL}} : {A} {Multilingual} {Retrieval} {Dataset} {Covering} 18 {Diverse} {Languages}},
	volume = {11},
	issn = {2307-387X},
	shorttitle = {\textbf{{MIRACL}}},
	url = {https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00595/117438/MIRACL-A-Multilingual-Retrieval-Dataset-Covering},
	doi = {10.1162/tacl_a_00595},
	abstract = {MIRACL is a multilingual dataset for ad hoc retrieval across 18 languages that collectively encompass over three billion native speakers around the world. This resource is designed to support monolingual retrieval tasks, where the queries and the corpora are in the same language. In total, we have gathered over 726k high-quality relevance judgments for 78k queries over Wikipedia in these languages, where all annotations have been performed by native speakers hired by our team. MIRACL covers languages that are both typologically close as well as distant from 10 language families and 13 sub-families, associated with varying amounts of publicly available resources. Extensive automatic heuristic verification and manual assessments were performed during the annotation process to control data quality. In total, MIRACL represents an investment of around five person-years of human annotator effort. Our goal is to spur research on improving retrieval across a continuum of languages, thus enhancing information access capabilities for diverse populations around the world, particularly those that have traditionally been underserved. MIRACL is available at http:// miracl.ai/.},
	language = {en},
	urldate = {2024-04-29},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Zhang, Xinyu and Thakur, Nandan and Ogundepo, Odunayo and Kamalloo, Ehsan and Alfonso-Hermelo, David and Li, Xiaoguang and Liu, Qun and Rezagholizadeh, Mehdi and Lin, Jimmy},
	month = sep,
	year = {2023},
	pages = {1114--1131},
	file = {Zhang et al. - 2023 - MIRACL  A Multilingual Retrieval Dataset C.pdf:/Users/mariaacb/Zotero/storage/97SDBW98/Zhang et al. - 2023 - MIRACL  A Multilingual Retrieval Dataset C.pdf:application/pdf},
}

@misc{lawrie_overview_2023,
	title = {Overview of the {TREC} 2022 {NeuCLIR} {Track}},
	url = {http://arxiv.org/abs/2304.12367},
	abstract = {This is the first year of the TREC Neural CLIR (NeuCLIR) track, which aims to study the impact of neural approaches to crosslanguage information retrieval. The main task in this year’s track was ad hoc ranked retrieval of Chinese, Persian, or Russian newswire documents using queries expressed in English. Topics were developed using standard TREC processes, except that topics developed by an annotator for one language were assessed by a different annotator when evaluating that topic on a different language. There were 172 total runs submitted by twelve teams.},
	language = {en},
	urldate = {2024-04-30},
	publisher = {arXiv},
	author = {Lawrie, Dawn and MacAvaney, Sean and Mayfield, James and McNamee, Paul and Oard, Douglas W. and Soldaini, Luca and Yang, Eugene},
	month = sep,
	year = {2023},
	note = {arXiv:2304.12367 [cs]},
	keywords = {Computer Science - Information Retrieval},
	file = {Lawrie et al. - 2023 - Overview of the TREC 2022 NeuCLIR Track.pdf:/Users/mariaacb/Zotero/storage/DG2NJQ7Q/Lawrie et al. - 2023 - Overview of the TREC 2022 NeuCLIR Track.pdf:application/pdf},
}

@inproceedings{asai_xor_2021,
	address = {Online},
	title = {{XOR} {QA}: {Cross}-lingual {Open}-{Retrieval} {Question} {Answering}},
	shorttitle = {{XOR} {QA}},
	url = {https://aclanthology.org/2021.naacl-main.46},
	doi = {10.18653/v1/2021.naacl-main.46},
	language = {en},
	urldate = {2024-04-30},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Asai, Akari and Kasai, Jungo and Clark, Jonathan and Lee, Kenton and Choi, Eunsol and Hajishirzi, Hannaneh},
	year = {2021},
	pages = {547--564},
	file = {Asai et al. - 2021 - XOR QA Cross-lingual Open-Retrieval Question Answ.pdf:/Users/mariaacb/Zotero/storage/WSCMXM6R/Asai et al. - 2021 - XOR QA Cross-lingual Open-Retrieval Question Answ.pdf:application/pdf},
}

@article{longpre_mkqa_2021,
	title = {{MKQA}: {A} {Linguistically} {Diverse} {Benchmark} for {Multilingual} {Open} {Domain} {Question} {Answering}},
	volume = {9},
	issn = {2307-387X},
	shorttitle = {{MKQA}},
	url = {https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00433/108607/MKQA-A-Linguistically-Diverse-Benchmark-for},
	doi = {10.1162/tacl_a_00433},
	abstract = {Abstract
            Progress in cross-lingual modeling depends on challenging, realistic, and diverse evaluation sets. We introduce Multilingual Knowledge Questions and Answers (MKQA), an open- domain question answering evaluation set comprising 10k question-answer pairs aligned across 26 typologically diverse languages (260k question-answer pairs in total). Answers are based on heavily curated, language- independent data representation, making results comparable across languages and independent of language-specific passages. With 26 languages, this dataset supplies the widest range of languages to-date for evaluating question answering. We benchmark a variety of state- of-the-art methods and baselines for generative and extractive question answering, trained on Natural Questions, in zero shot and translation settings. Results indicate this dataset is challenging even in English, but especially in low-resource languages.1},
	language = {en},
	urldate = {2024-04-30},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Longpre, Shayne and Lu, Yi and Daiber, Joachim},
	month = dec,
	year = {2021},
	pages = {1389--1406},
	file = {Longpre et al. - 2021 - MKQA A Linguistically Diverse Benchmark for Multi.pdf:/Users/mariaacb/Zotero/storage/EGTKU2ZX/Longpre et al. - 2021 - MKQA A Linguistically Diverse Benchmark for Multi.pdf:application/pdf},
}

@inproceedings{zhang_mr_2021,
	address = {Punta Cana, Dominican Republic},
	title = {Mr. {TyDi}: {A} {Multi}-lingual {Benchmark} for {Dense} {Retrieval}},
	shorttitle = {Mr. {TyDi}},
	url = {https://aclanthology.org/2021.mrl-1.12},
	doi = {10.18653/v1/2021.mrl-1.12},
	language = {en},
	urldate = {2024-05-02},
	booktitle = {Proceedings of the 1st {Workshop} on {Multilingual} {Representation} {Learning}},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Xinyu and Ma, Xueguang and Shi, Peng and Lin, Jimmy},
	year = {2021},
	pages = {127--137},
	file = {Zhang et al. - 2021 - Mr. TyDi A Multi-lingual Benchmark for Dense Retr.pdf:/Users/mariaacb/Zotero/storage/SVQNVJJK/Zhang et al. - 2021 - Mr. TyDi A Multi-lingual Benchmark for Dense Retr.pdf:application/pdf},
}

@article{clark_t_2020,
	title = {T {\textless}span style="font-variant:small-caps;"{\textgreater}y{\textless}/span{\textgreater} {D} {\textless}span style="font-variant:small-caps;"{\textgreater}i{\textless}/span{\textgreater} {QA}: {A} {Benchmark} for {Information}-{Seeking} {Question} {Answering} in \textit{{Ty}} pologically \textit{{Di}} verse {Languages}},
	volume = {8},
	issn = {2307-387X},
	shorttitle = {T {\textless}span style="font-variant},
	url = {https://direct.mit.edu/tacl/article/96451},
	doi = {10.1162/tacl_a_00317},
	abstract = {Confidently making progress on multilingual modeling requires challenging, trustworthy evaluations. We present TYDI QA—a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs. The languages of TYDI QA are diverse with regard to their typology—the set of linguistic features each language expresses—such that we expect models performing well on this set to generalize across a large number of the world’s languages. We present a quantitative analysis of the data quality and example-level qualitative linguistic analyses of observed language phenomena that would not be found in English-only corpora. To provide a realistic information-seeking task and avoid priming effects, questions are written by people who want to know the answer, but don’t know the answer yet, and the data is collected directly in each language without the use of translation.},
	language = {en},
	urldate = {2024-05-02},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Clark, Jonathan H. and Choi, Eunsol and Collins, Michael and Garrette, Dan and Kwiatkowski, Tom and Nikolaev, Vitaly and Palomaki, Jennimaria},
	month = dec,
	year = {2020},
	pages = {454--470},
	file = {Clark et al. - 2020 - T y .pdf:/Users/mariaacb/Zotero/storage/74TYHM5Y/Clark et al. - 2020 - T y .pdf:application/pdf},
}

@misc{kamalloo_hagrid_2023,
	title = {{HAGRID}: {A} {Human}-{LLM} {Collaborative} {Dataset} for {Generative} {Information}-{Seeking} with {Attribution}},
	shorttitle = {{HAGRID}},
	url = {http://arxiv.org/abs/2307.16883},
	abstract = {The rise of large language models (LLMs) had a transformative impact on search, ushering in a new era of search engines that are capable of generating search results in natural language text, imbued with citations for supporting sources. Building generative information-seeking models demands openly accessible datasets, which currently remain lacking. In this paper, we introduce a new dataset, HAGRID (Human-in-the-loop Attributable Generative Retrieval for Information-seeking Dataset) for building end-to-end generative information-seeking models that are capable of retrieving candidate quotes and generating attributed explanations. Unlike recent efforts that focus on human evaluation of black-box proprietary search engines, we built our dataset atop the English subset of MIRACL, a publicly available information retrieval dataset. HAGRID is constructed based on human and LLM collaboration. We first automatically collect attributed explanations that follow an in-context citation style using an LLM, i.e. GPT-3.5. Next, we ask human annotators to evaluate the LLM explanations based on two criteria: informativeness and attributability. HAGRID serves as a catalyst for the development of information-seeking models with better attribution capabilities.},
	language = {en},
	urldate = {2024-05-02},
	publisher = {arXiv},
	author = {Kamalloo, Ehsan and Jafari, Aref and Zhang, Xinyu and Thakur, Nandan and Lin, Jimmy},
	month = jul,
	year = {2023},
	note = {arXiv:2307.16883 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {Kamalloo et al. - 2023 - HAGRID A Human-LLM Collaborative Dataset for Gene.pdf:/Users/mariaacb/Zotero/storage/YY8VKPJ8/Kamalloo et al. - 2023 - HAGRID A Human-LLM Collaborative Dataset for Gene.pdf:application/pdf},
}

@inproceedings{rajpurkar_squad_2016,
	address = {Austin, Texas},
	title = {{SQuAD}: 100,000+ {Questions} for {Machine} {Comprehension} of {Text}},
	shorttitle = {{SQuAD}},
	url = {http://aclweb.org/anthology/D16-1264},
	doi = {10.18653/v1/D16-1264},
	abstract = {We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0\%, a signiﬁcant improvement over a simple baseline (20\%). However, human performance (86.8\%) is much higher, indicating that the dataset presents a good challenge problem for future research. The dataset is freely available at https://stanford-qa.com.},
	language = {en},
	urldate = {2024-05-02},
	booktitle = {Proceedings of the 2016 {Conference} on {Empirical} {Methods} in {Natural}           {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
	year = {2016},
	pages = {2383--2392},
	file = {Rajpurkar et al. - 2016 - SQuAD 100,000+ Questions for Machine Comprehensio.pdf:/Users/mariaacb/Zotero/storage/ZKN3KX9Y/Rajpurkar et al. - 2016 - SQuAD 100,000+ Questions for Machine Comprehensio.pdf:application/pdf},
}

@article{kwiatkowski_natural_2019,
	title = {Natural {Questions}: {A} {Benchmark} for {Question} {Answering} {Research}},
	volume = {7},
	issn = {2307-387X},
	shorttitle = {Natural {Questions}},
	url = {https://direct.mit.edu/tacl/article/43518},
	doi = {10.1162/tacl_a_00276},
	abstract = {We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.},
	language = {en},
	urldate = {2024-05-02},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and Toutanova, Kristina and Jones, Llion and Kelcey, Matthew and Chang, Ming-Wei and Dai, Andrew M. and Uszkoreit, Jakob and Le, Quoc and Petrov, Slav},
	month = nov,
	year = {2019},
	pages = {453--466},
	file = {Kwiatkowski et al. - 2019 - Natural Questions A Benchmark for Question Answer.pdf:/Users/mariaacb/Zotero/storage/BVN3PC9Q/Kwiatkowski et al. - 2019 - Natural Questions A Benchmark for Question Answer.pdf:application/pdf},
}

@inproceedings{fan_eli5_2019,
	address = {Florence, Italy},
	title = {{ELI5}: {Long} {Form} {Question} {Answering}},
	shorttitle = {{ELI5}},
	url = {https://www.aclweb.org/anthology/P19-1346},
	doi = {10.18653/v1/P19-1346},
	language = {en},
	urldate = {2024-05-02},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Fan, Angela and Jernite, Yacine and Perez, Ethan and Grangier, David and Weston, Jason and Auli, Michael},
	year = {2019},
	pages = {3558--3567},
	file = {Fan et al. - 2019 - ELI5 Long Form Question Answering.pdf:/Users/mariaacb/Zotero/storage/IFTZUC5E/Fan et al. - 2019 - ELI5 Long Form Question Answering.pdf:application/pdf},
}

@article{tahsin_mayeesha_deep_2021,
	title = {Deep learning based question answering system in {Bengali}},
	volume = {5},
	issn = {2475-1839, 2475-1847},
	url = {https://www.tandfonline.com/doi/full/10.1080/24751839.2020.1833136},
	doi = {10.1080/24751839.2020.1833136},
	abstract = {Recent advances in the ﬁeld of natural language processing has improved state-of-the-art performances on many tasks including question answering for languages like English. Bengali language is ranked seventh and is spoken by about 300 million people all over the world. But due to lack of data and active research on QA similar progress has not been achieved for Bengali. Unlike English, there is no benchmark large scale QA dataset collected for Bengali, no pretrained language model that can be modiﬁed for Bengali question answering and no human baseline score for QA has been established either. In this work we use state-of-theart transformer models to train QA system on a synthetic reading comprehension dataset translated from one of the most popular benchmark datasets in English called SQuAD 2.0. We collect a smaller human annotated QA dataset from Bengali Wikipedia with popular topics from Bangladeshi culture for evaluating our models. Finally, we compare our models with human children to set up a benchmark score using survey experiments.},
	language = {en},
	number = {2},
	urldate = {2024-05-02},
	journal = {Journal of Information and Telecommunication},
	author = {Tahsin Mayeesha, Tasmiah and Md Sarwar, Abdullah and Rahman, Rashedur M.},
	month = apr,
	year = {2021},
	pages = {145--178},
	file = {Tahsin Mayeesha et al. - 2021 - Deep learning based question answering system in B.pdf:/Users/mariaacb/Zotero/storage/DLHTJESA/Tahsin Mayeesha et al. - 2021 - Deep learning based question answering system in B.pdf:application/pdf},
}

@inproceedings{artetxe_cross-lingual_2020,
	address = {Online},
	title = {On the {Cross}-lingual {Transferability} of {Monolingual} {Representations}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.421},
	doi = {10.18653/v1/2020.acl-main.421},
	abstract = {State-of-the-art unsupervised multilingual models (e.g., multilingual BERT) have been shown to generalize in a zero-shot crosslingual setting. This generalization ability has been attributed to the use of a shared subword vocabulary and joint training across multiple languages giving rise to deep multilingual abstractions. We evaluate this hypothesis by designing an alternative approach that transfers a monolingual model to new languages at the lexical level. More concretely, we ﬁrst train a transformer-based masked language model on one language, and transfer it to a new language by learning a new embedding matrix with the same masked language modeling objective—freezing parameters of all other layers. This approach does not rely on a shared vocabulary or joint training. However, we show that it is competitive with multilingual BERT on standard cross-lingual classiﬁcation benchmarks and on a new Cross-lingual Question Answering Dataset (XQuAD). Our results contradict common beliefs of the basis of the generalization ability of multilingual models and suggest that deep monolingual models learn some abstractions that generalize across languages. We also release XQuAD as a more comprehensive cross-lingual benchmark, which comprises 240 paragraphs and 1190 question-answer pairs from SQuAD v1.1 translated into ten languages by professional translators.},
	language = {en},
	urldate = {2024-05-02},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Artetxe, Mikel and Ruder, Sebastian and Yogatama, Dani},
	year = {2020},
	pages = {4623--4637},
	file = {Artetxe et al. - 2020 - On the Cross-lingual Transferability of Monolingua.pdf:/Users/mariaacb/Zotero/storage/CZK8AI3D/Artetxe et al. - 2020 - On the Cross-lingual Transferability of Monolingua.pdf:application/pdf},
}

@inproceedings{lewis_mlqa_2020,
	address = {Online},
	title = {{MLQA}: {Evaluating} {Cross}-lingual {Extractive} {Question} {Answering}},
	shorttitle = {{MLQA}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.653},
	doi = {10.18653/v1/2020.acl-main.653},
	abstract = {Question answering (QA) models have shown rapid progress enabled by the availability of large, high-quality benchmark datasets. Such annotated datasets are difﬁcult and costly to collect, and rarely exist in languages other than English, making building QA systems that work well in other languages challenging. In order to develop such systems, it is crucial to invest in high quality multilingual evaluation benchmarks to measure progress. We present MLQA, a multi-way aligned extractive QA evaluation benchmark intended to spur research in this area.1 MLQA contains QA instances in 7 languages, English, Arabic, German, Spanish, Hindi, Vietnamese and Simpliﬁed Chinese. MLQA has over 12K instances in English and 5K in each other language, with each instance parallel between 4 languages on average. We evaluate stateof-the-art cross-lingual models and machinetranslation-based baselines on MLQA. In all cases, transfer results are signiﬁcantly behind training-language performance.},
	language = {en},
	urldate = {2024-05-02},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},
	year = {2020},
	pages = {7315--7330},
	file = {Lewis et al. - 2020 - MLQA Evaluating Cross-lingual Extractive Question.pdf:/Users/mariaacb/Zotero/storage/AP8AK7VC/Lewis et al. - 2020 - MLQA Evaluating Cross-lingual Extractive Question.pdf:application/pdf},
}

@inproceedings{liu_xqa_2019,
	address = {Florence, Italy},
	title = {{XQA}: {A} {Cross}-lingual {Open}-domain {Question} {Answering} {Dataset}},
	shorttitle = {{XQA}},
	url = {https://www.aclweb.org/anthology/P19-1227},
	doi = {10.18653/v1/P19-1227},
	abstract = {Open-domain question answering (OpenQA) aims to answer questions through text retrieval and reading comprehension. Recently, lots of neural network-based models have been proposed and achieved promising results in OpenQA. However, the success of these models relies on a massive volume of training data (usually in English), which is not available in many other languages, especially for those low-resource languages. Therefore, it is essential to investigate cross-lingual OpenQA. In this paper, we construct a novel dataset XQA for cross-lingual OpenQA research. It consists of a training set in English as well as development and test sets in eight other languages. Besides, we provide several baseline systems for cross-lingual OpenQA, including two machine translation-based methods and one zero-shot cross-lingual method (multilingual BERT). Experimental results show that the multilingual BERT model achieves the best results in almost all target languages, while the performance of cross-lingual OpenQA is still much lower than that of English. Our analysis indicates that the performance of cross-lingual OpenQA is related to not only how similar the target language and English are, but also how difﬁcult the question set of the target language is. The XQA dataset is publicly available at http://github.com/thunlp/XQA.},
	language = {en},
	urldate = {2024-05-02},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Liu, Jiahua and Lin, Yankai and Liu, Zhiyuan and Sun, Maosong},
	year = {2019},
	pages = {2358--2368},
	file = {Liu et al. - 2019 - XQA A Cross-lingual Open-domain Question Answerin.pdf:/Users/mariaacb/Zotero/storage/UQT9E4NY/Liu et al. - 2019 - XQA A Cross-lingual Open-domain Question Answerin.pdf:application/pdf},
}

@inproceedings{choi_quac_2018,
	address = {Brussels, Belgium},
	title = {{QuAC}: {Question} {Answering} in {Context}},
	shorttitle = {{QuAC}},
	url = {http://aclweb.org/anthology/D18-1241},
	doi = {10.18653/v1/D18-1241},
	abstract = {We present QuAC, a dataset for Question Answering in Context that contains 14K information-seeking QA dialogs (100K questions in total). The dialogs involve two crowd workers: (1) a student who poses a sequence of freeform questions to learn as much as possible about a hidden Wikipedia text, and (2) a teacher who answers the questions by providing short excerpts from the text. QuAC introduces challenges not found in existing machine comprehension datasets: its questions are often more open-ended, unanswerable, or only meaningful within the dialog context, as we show in a detailed qualitative evaluation. We also report results for a number of reference models, including a recently state-ofthe-art reading comprehension architecture extended to model dialog context. Our best model underperforms humans by 20 F1, suggesting that there is signiﬁcant room for future work on this data. Dataset, baseline, and leaderboard available at http://quac.ai.},
	language = {en},
	urldate = {2024-05-03},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Choi, Eunsol and He, He and Iyyer, Mohit and Yatskar, Mark and Yih, Wen-tau and Choi, Yejin and Liang, Percy and Zettlemoyer, Luke},
	year = {2018},
	pages = {2174--2184},
	file = {Choi et al. - 2018 - QuAC Question Answering in Context.pdf:/Users/mariaacb/Zotero/storage/22CQV9EY/Choi et al. - 2018 - QuAC Question Answering in Context.pdf:application/pdf},
}

@inproceedings{sun_clirmatrix_2020,
	address = {Online},
	title = {{CLIRMatrix}: {A} massively large collection of bilingual and multilingual datasets for {Cross}-{Lingual} {Information} {Retrieval}},
	shorttitle = {{CLIRMatrix}},
	url = {https://www.aclweb.org/anthology/2020.emnlp-main.340},
	doi = {10.18653/v1/2020.emnlp-main.340},
	abstract = {We present CLIRMatrix, a massively large collection of bilingual and multilingual datasets for Cross-Lingual Information Retrieval extracted automatically from Wikipedia. CLIRMatrix comprises (1) BI-139, a bilingual dataset of queries in one language matched with relevant documents in another language for 139×138=19,182 language pairs, and (2) MULTI-8, a multilingual dataset of queries and documents jointly aligned in 8 different languages. In total, we mined 49 million unique queries and 34 billion (query, document, label) triplets, making it the largest and most comprehensive CLIR dataset to date. This collection is intended to support research in end-to-end neural information retrieval and is publicly available at https: //github.com/ssun32/CLIRMatrix. We provide baseline neural model results on BI139, and evaluate MULTI-8 in both singlelanguage retrieval and mix-language retrieval settings.},
	language = {en},
	urldate = {2024-05-03},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Sun, Shuo and Duh, Kevin},
	year = {2020},
	pages = {4160--4170},
	file = {Sun and Duh - 2020 - CLIRMatrix A massively large collection of biling.pdf:/Users/mariaacb/Zotero/storage/LETEL3SQ/Sun and Duh - 2020 - CLIRMatrix A massively large collection of biling.pdf:application/pdf},
}

@misc{singh_indicgenbench_2024,
	title = {{IndicGenBench}: {A} {Multilingual} {Benchmark} to {Evaluate} {Generation} {Capabilities} of {LLMs} on {Indic} {Languages}},
	shorttitle = {{IndicGenBench}},
	url = {http://arxiv.org/abs/2404.16816},
	abstract = {As large language models (LLMs) see increasing adoption across the globe, it is imperative for LLMs to be representative of the linguistic diversity of the world. India is a linguistically diverse country of 1.4 Billion people. To facilitate research on multilingual LLM evaluation, we release IndicGenBench - the largest benchmark for evaluating LLMs on user-facing generation tasks across a diverse set 29 of Indic languages covering 13 scripts and 4 language families. IndicGenBench is composed of diverse generation tasks like cross-lingual summarization, machine translation, and cross-lingual question answering. IndicGenBench extends existing benchmarks to many Indic languages through human curation providing multi-way parallel evaluation data for many under-represented Indic languages for the first time. We evaluate a wide range of proprietary and open-source LLMs including GPT-3.5, GPT-4, PaLM-2, mT5, Gemma, BLOOM and LLaMA on IndicGenBench in a variety of settings. The largest PaLM-2 models performs the best on most tasks, however, there is a significant performance gap in all languages compared to English showing that further research is needed for the development of more inclusive multilingual language models. IndicGenBench is released at www.github.com/google-research-datasets/indic-gen-bench},
	language = {en},
	urldate = {2024-05-06},
	publisher = {arXiv},
	author = {Singh, Harman and Gupta, Nitish and Bharadwaj, Shikhar and Tewari, Dinesh and Talukdar, Partha},
	month = apr,
	year = {2024},
	note = {arXiv:2404.16816 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Singh et al. - 2024 - IndicGenBench A Multilingual Benchmark to Evaluat.pdf:/Users/mariaacb/Zotero/storage/D766PCT2/Singh et al. - 2024 - IndicGenBench A Multilingual Benchmark to Evaluat.pdf:application/pdf},
}

@inproceedings{ahuja_mega_2023,
	address = {Singapore},
	title = {{MEGA}: {Multilingual} {Evaluation} of {Generative} {AI}},
	shorttitle = {{MEGA}},
	url = {https://aclanthology.org/2023.emnlp-main.258},
	doi = {10.18653/v1/2023.emnlp-main.258},
	abstract = {Generative AI models have shown impressive performance on many Natural Language Processing tasks such as language understanding, reasoning, and language generation. An important question being asked by the AI community today is about the capabilities and limits of these models, and it is clear that evaluating generative AI is very challenging. Most studies on generative LLMs have been restricted to English and it is unclear how capable these models are at understanding and generating text in other languages. We present the first comprehensive benchmarking of generative LLMs - MEGA, which evaluates models on standard NLP benchmarks, covering 16 NLP datasets across 70 typologically diverse languages. We compare the performance of generative LLMs including Chat-GPT and GPT-4 to State of the Art (SOTA) non-autoregressive models on these tasks to determine how well generative models perform compared to the previous generation of LLMs. We present a thorough analysis of the performance of models across languages and tasks and discuss challenges in improving the performance of generative LLMs on low-resource languages. We create a framework for evaluating generative LLMs in the multilingual setting and provide directions for future progress in the field.},
	language = {en},
	urldate = {2024-05-06},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Ahuja, Kabir and Diddee, Harshita and Hada, Rishav and Ochieng, Millicent and Ramesh, Krithika and Jain, Prachi and Nambi, Akshay and Ganu, Tanuja and Segal, Sameer and Ahmed, Mohamed and Bali, Kalika and Sitaram, Sunayana},
	year = {2023},
	pages = {4232--4267},
	file = {Ahuja et al. - 2023 - MEGA Multilingual Evaluation of Generative AI.pdf:/Users/mariaacb/Zotero/storage/AG37WLAN/Ahuja et al. - 2023 - MEGA Multilingual Evaluation of Generative AI.pdf:application/pdf},
}

@misc{ahuja_megaverse_2024,
	title = {{MEGAVERSE}: {Benchmarking} {Large} {Language} {Models} {Across} {Languages}, {Modalities}, {Models} and {Tasks}},
	shorttitle = {{MEGAVERSE}},
	url = {http://arxiv.org/abs/2311.07463},
	abstract = {There has been a surge in LLM evaluation research to understand LLM capabilities and limitations. However, much of this research has been confined to English, leaving LLM building and evaluation for non-English languages relatively unexplored. Several new LLMs have been introduced recently, necessitating their evaluation on non-English languages. This study aims to perform a thorough evaluation of the non-English capabilities of SoTA LLMs (GPT-3.5-Turbo, GPT-4, PaLM2, Gemini-Pro, Mistral, Llama2, and Gemma) by comparing them on the same set of multilingual datasets. Our benchmark comprises 22 datasets covering 83 languages, including low-resource African languages. We also include two multimodal datasets in the benchmark and compare the performance of LLaVA models, GPT-4-Vision and Gemini-Pro-Vision. Our experiments show that larger models such as GPT-4, Gemini-Pro and PaLM2 outperform smaller models on various tasks, notably on low-resource languages, with GPT-4 outperforming PaLM2 and Gemini-Pro on more datasets. We also perform a study on data contamination and find that several models are likely to be contaminated with multilingual evaluation benchmarks, necessitating approaches to detect and handle contamination while assessing the multilingual performance of LLMs.},
	language = {en},
	urldate = {2024-05-06},
	publisher = {arXiv},
	author = {Ahuja, Sanchit and Aggarwal, Divyanshu and Gumma, Varun and Watts, Ishaan and Sathe, Ashutosh and Ochieng, Millicent and Hada, Rishav and Jain, Prachi and Axmed, Maxamed and Bali, Kalika and Sitaram, Sunayana},
	month = apr,
	year = {2024},
	note = {arXiv:2311.07463 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Ahuja et al. - 2024 - MEGAVERSE Benchmarking Large Language Models Acro.pdf:/Users/mariaacb/Zotero/storage/XMYAF4JF/Ahuja et al. - 2024 - MEGAVERSE Benchmarking Large Language Models Acro.pdf:application/pdf},
}

@article{doddapaneni_towards_nodate,
	title = {Towards {Leaving} {No} {Indic} {Language} {Behind}: {Building} {Monolingual} {Corpora}, {Benchmark} and {Models} for {Indic} {Languages}},
	abstract = {Building Natural Language Understanding (NLU) capabilities for Indic languages, which have a collective speaker base of more than one billion speakers is absolutely crucial. In this work, we aim to improve the NLU capabilities of Indic languages by making contributions along 3 important axes (i) monolingual corpora (ii) NLU testsets (iii) multilingual LLMs focusing on Indic languages. Specifically, we curate the largest monolingual corpora, IndicCorp, with 20.9B tokens covering 24 languages from 4 language families - a 2.3x increase over prior work, while supporting 12 additional languages. Next, we create a humansupervised benchmark, IndicXTREME, consisting of nine diverse NLU tasks covering 20 languages. Across languages and tasks, IndicXTREME contains a total of 105 evaluation sets, of which 52 are new contributions to the literature. To the best of our knowledge, this is the first effort towards creating a standard benchmark for Indic languages that aims to test the multilingual zero-shot capabilities of pretrained language models. Finally, we train IndicBERT v2, a state-of-the-art model supporting all the languages. Averaged across languages and tasks, the model achieves an absolute improvement of 2 points over a strong baseline. The data and models are available at https:// github.com/AI4Bharat/IndicBERT.},
	language = {en},
	author = {Doddapaneni, Sumanth and Aralikatte, Rahul and Ramesh, Gowtham and Goyal, Shreya and Khapra, Mitesh M and Kunchukuttan, Anoop and Kumar, Pratyush},
	file = {Doddapaneni et al. - Towards Leaving No Indic Language Behind Building.pdf:/Users/mariaacb/Zotero/storage/6QKVUD4L/Doddapaneni et al. - Towards Leaving No Indic Language Behind Building.pdf:application/pdf},
}

@inproceedings{ladhak_wikilingua_2020,
	address = {Online},
	title = {{WikiLingua}: {A} {New} {Benchmark} {Dataset} for {Cross}-{Lingual} {Abstractive} {Summarization}},
	shorttitle = {{WikiLingua}},
	url = {https://www.aclweb.org/anthology/2020.findings-emnlp.360},
	doi = {10.18653/v1/2020.findings-emnlp.360},
	abstract = {We introduce WikiLingua, a large-scale, multilingual dataset for the evaluation of crosslingual abstractive summarization systems. We extract article and summary pairs in 18 languages from WikiHow12, a high quality, collaborative resource of how-to guides on a diverse set of topics written by human authors. We create gold-standard articlesummary alignments across languages by aligning the images that are used to describe each how-to step in an article. As a set of baselines for further studies, we evaluate the performance of existing cross-lingual abstractive summarization methods on our dataset. We further propose a method for direct crosslingual summarization (i.e., without requiring translation at inference time) by leveraging synthetic data and Neural Machine Translation as a pre-training step. Our method significantly outperforms the baseline approaches, while being more cost efﬁcient during inference.},
	language = {en},
	urldate = {2024-05-15},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Ladhak, Faisal and Durmus, Esin and Cardie, Claire and McKeown, Kathleen},
	year = {2020},
	pages = {4034--4048},
	file = {Ladhak et al. - 2020 - WikiLingua A New Benchmark Dataset for Cross-Ling.pdf:/Users/mariaacb/Zotero/storage/VJH6YXXW/Ladhak et al. - 2020 - WikiLingua A New Benchmark Dataset for Cross-Ling.pdf:application/pdf},
}

@article{chen_benchmarking_2024,
	title = {Benchmarking {Large} {Language} {Models} in {Retrieval}-{Augmented} {Generation}},
	volume = {38},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/29728},
	doi = {10.1609/aaai.v38i16.29728},
	abstract = {Retrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language models (LLMs). However, existing research lacks rigorous evaluation of the impact of retrieval-augmented generation on different large language models, which make it challenging to identify the potential bottlenecks in the capabilities of RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance of different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative rejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in both English and Chinese. RGB divides the instances within the benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case. Then we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG. Evaluation reveals that while LLMs exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information. The aforementioned assessment outcomes indicate that there is still a considerable journey ahead to effectively apply RAG to LLMs.},
	language = {en},
	number = {16},
	urldate = {2024-05-22},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Chen, Jiawei and Lin, Hongyu and Han, Xianpei and Sun, Le},
	month = mar,
	year = {2024},
	pages = {17754--17762},
	file = {Chen et al. - 2024 - Benchmarking Large Language Models in Retrieval-Au.pdf:/Users/mariaacb/Zotero/storage/38BQ32LU/Chen et al. - 2024 - Benchmarking Large Language Models in Retrieval-Au.pdf:application/pdf},
}

@misc{tang_minicheck_2024,
	title = {{MiniCheck}: {Efficient} {Fact}-{Checking} of {LLMs} on {Grounding} {Documents}},
	shorttitle = {{MiniCheck}},
	url = {http://arxiv.org/abs/2404.10774},
	abstract = {Recognizing if LLM output can be grounded in evidence is central to many tasks in NLP: retrieval-augmented generation, summarization, document-grounded dialogue, and more. Current approaches to this kind of "fact-checking" are based on verifying each piece of a model generation against potential evidence using an LLM. However, this process can be very computationally expensive, requiring many calls to LLMs to check a single response. In this work, we show how to build small models that have GPT-4-level performance but for 400x lower cost. We do this by constructing synthetic training data with GPT-4, which involves creating realistic yet challenging instances of factual errors via a structured generation procedure. Training on this data teaches models to check each fact in the claim and recognize synthesis of information across sentences. For evaluation, we unify pre-existing datasets into a benchmark LLM-AggreFact, collected from recent work on fact-checking and grounding LLM generations. Our best system MiniCheck-FT5 (770M parameters) outperforms all systems of comparable size and reaches GPT-4 accuracy. We release LLM-AggreFact, code for data synthesis, and models.},
	language = {en},
	urldate = {2024-05-23},
	publisher = {arXiv},
	author = {Tang, Liyan and Laban, Philippe and Durrett, Greg},
	month = apr,
	year = {2024},
	note = {arXiv:2404.10774 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Tang et al. - 2024 - MiniCheck Efficient Fact-Checking of LLMs on Grou.pdf:/Users/mariaacb/Zotero/storage/UHELLTTH/Tang et al. - 2024 - MiniCheck Efficient Fact-Checking of LLMs on Grou.pdf:application/pdf},
}

@article{laban_span_2022,
	title = {{\textless}span style="font-variant:small-caps;"{\textgreater}{SummaC}{\textless}/span{\textgreater} : {Re}-{Visiting} {NLI}-based {Models} for {Inconsistency} {Detection} in {Summarization}},
	volume = {10},
	issn = {2307-387X},
	shorttitle = {{\textless}span style="font-variant},
	url = {https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00453/109470/SummaC-Re-Visiting-NLI-based-Models-for},
	doi = {10.1162/tacl_a_00453},
	abstract = {In the summarization domain, a key requirement for summaries is to be factually consistent with the input document. Previous work has found that natural language inference (NLI) models do not perform competitively when applied to inconsistency detection. In this work, we revisit the use of NLI for inconsistency detection, finding that past work suffered from a mismatch in input granularity between NLI datasets (sentence-level), and inconsistency detection (document level). We provide a highly effective and light-weight method called SUMMACCONV that enables NLI models to be successfully used for this task by segmenting documents into sentence units and aggregating scores between pairs of sentences. We furthermore introduce a new benchmark called SUMMAC (Summary Consistency) which consists of six large inconsistency detection datasets. On this dataset, SUMMACConv obtains state-of-the-art results with a balanced accuracy of 74.4\%, a 5\% improvement compared with prior work.},
	language = {en},
	urldate = {2024-05-23},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Laban, Philippe and Schnabel, Tobias and Bennett, Paul N. and Hearst, Marti A.},
	month = feb,
	year = {2022},
	pages = {163--177},
	file = {Laban et al. - 2022 - SummaCspa.pdf:/Users/mariaacb/Zotero/storage/645BYXWE/Laban et al. - 2022 - SummaCspa.pdf:application/pdf},
}

@inproceedings{laban_summedits_2023,
	address = {Singapore},
	title = {{SummEdits}: {Measuring} {LLM} {Ability} at {Factual} {Reasoning} {Through} {The} {Lens} of {Summarization}},
	shorttitle = {{SummEdits}},
	url = {https://aclanthology.org/2023.emnlp-main.600},
	doi = {10.18653/v1/2023.emnlp-main.600},
	abstract = {With the recent appearance of LLMs in practical settings, having methods that can effectively detect factual inconsistencies is crucial to reduce the propagation of misinformation and improve trust in model outputs. When testing on existing factual consistency benchmarks, we find that a few large language models (LLMs) perform competitively on classification benchmarks for factual inconsistency detection compared to traditional non-LLM methods. However, a closer analysis reveals issues with existing evaluation benchmarks, affecting evaluation precision. To address this, we propose a new protocol for inconsistency detection benchmark creation and implement it in a 10-domain benchmark called SUMMEDITS. This new benchmark is 20 times more costeffective per sample than previous benchmarks and highly reproducible, as we estimate interannotator agreement at about 0.9. Most LLMs struggle on SUMMEDITS, with performance close to random chance. The best-performing model, GPT-4, is still 8\% below estimated human performance, highlighting the gaps in LLMs’ ability to reason about facts and detect inconsistencies when they occur.},
	language = {en},
	urldate = {2024-05-23},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Laban, Philippe and Kryscinski, Wojciech and Agarwal, Divyansh and Fabbri, Alexander and Xiong, Caiming and Joty, Shafiq and Wu, Chien-Sheng},
	year = {2023},
	pages = {9662--9676},
	file = {Laban et al. - 2023 - SummEdits Measuring LLM Ability at Factual Reason.pdf:/Users/mariaacb/Zotero/storage/ENJBU5UY/Laban et al. - 2023 - SummEdits Measuring LLM Ability at Factual Reason.pdf:application/pdf},
}

@misc{tang_tofueval_2024,
	title = {{TofuEval}: {Evaluating} {Hallucinations} of {LLMs} on {Topic}-{Focused} {Dialogue} {Summarization}},
	shorttitle = {{TofuEval}},
	url = {http://arxiv.org/abs/2402.13249},
	abstract = {Single document news summarization has seen substantial progress on faithfulness in recent years, driven by research on the evaluation of factual consistency, or hallucinations. We ask whether these advances carry over to other text summarization domains. We propose a new evaluation benchmark on topic-focused dialogue summarization, generated by LLMs of varying sizes. We provide binary sentence-level human annotations of the factual consistency of these summaries along with detailed explanations of factually inconsistent sentences. Our analysis shows that existing LLMs hallucinate significant amounts of factual errors in the dialogue domain, regardless of the model's size. On the other hand, when LLMs, including GPT-4, serve as binary factual evaluators, they perform poorly and can be outperformed by prevailing state-of-the-art specialized factuality evaluation metrics. Finally, we conducted an analysis of hallucination types with a curated error taxonomy. We find that there are diverse errors and error distributions in model-generated summaries and that non-LLM based metrics can capture all error types better than LLM-based evaluators.},
	language = {en},
	urldate = {2024-05-23},
	publisher = {arXiv},
	author = {Tang, Liyan and Shalyminov, Igor and Wong, Amy Wing-mei and Burnsky, Jon and Vincent, Jake W. and Yang, Yu'an and Singh, Siffi and Feng, Song and Song, Hwanjun and Su, Hang and Sun, Lijia and Zhang, Yi and Mansour, Saab and McKeown, Kathleen},
	month = mar,
	year = {2024},
	note = {arXiv:2402.13249 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Tang et al. - 2024 - TofuEval Evaluating Hallucinations of LLMs on Top.pdf:/Users/mariaacb/Zotero/storage/A28C5IAD/Tang et al. - 2024 - TofuEval Evaluating Hallucinations of LLMs on Top.pdf:application/pdf},
}

@misc{jacovi_chain--thought_2024,
	title = {A {Chain}-of-{Thought} {Is} as {Strong} as {Its} {Weakest} {Link}: {A} {Benchmark} for {Verifiers} of {Reasoning} {Chains}},
	shorttitle = {A {Chain}-of-{Thought} {Is} as {Strong} as {Its} {Weakest} {Link}},
	url = {http://arxiv.org/abs/2402.00559},
	abstract = {Prompting language models to provide stepby-step answers (e.g., “Chain-of-Thought”) is the prominent approach for complex reasoning tasks, where more accurate reasoning chains typically improve downstream task performance. Recent literature discusses automatic methods to verify reasoning steps to evaluate and improve their correctness. However, no fine-grained step-level datasets are available to enable thorough evaluation of such verification methods, hindering progress in this direction. We introduce REVEAL: Reasoning Verification Evaluation, a new dataset to benchmark automatic verifiers of complex Chain-ofThought reasoning in open-domain question answering settings. REVEAL includes comprehensive labels for the relevance, attribution to evidence passages, and logical correctness of each reasoning step in a language model’s answer, across a wide variety of datasets and state-of-the-art language models. Available at reveal-dataset.github.io.},
	language = {en},
	urldate = {2024-05-23},
	publisher = {arXiv},
	author = {Jacovi, Alon and Bitton, Yonatan and Bohnet, Bernd and Herzig, Jonathan and Honovich, Or and Tseng, Michael and Collins, Michael and Aharoni, Roee and Geva, Mor},
	month = may,
	year = {2024},
	note = {arXiv:2402.00559 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Jacovi et al. - 2024 - A Chain-of-Thought Is as Strong as Its Weakest Lin.pdf:/Users/mariaacb/Zotero/storage/NH2IXHME/Jacovi et al. - 2024 - A Chain-of-Thought Is as Strong as Its Weakest Lin.pdf:application/pdf},
}

@misc{liu_evaluating_2023,
	title = {Evaluating {Verifiability} in {Generative} {Search} {Engines}},
	url = {http://arxiv.org/abs/2304.09848},
	abstract = {Generative search engines directly generate responses to user queries, along with in-line citations. A prerequisite trait of a trustworthy generative search engine is verifiability, i.e., systems should cite comprehensively (high citation recall; all statements are fully supported by citations) and accurately (high citation precision; every cite supports its associated statement). We conduct human evaluation to audit four popular generative search engines—Bing Chat, NeevaAI, perplexity.ai, and YouChat—across a diverse set of queries from a variety of sources (e.g., historical Google user queries, dynamically-collected open-ended questions on Reddit, etc.). We find that responses from existing generative search engines are fluent and appear informative, but frequently contain unsupported statements and inaccurate citations: on average, a mere 51.5\% of generated sentences are fully supported by citations and only 74.5\% of citations support their associated sentence. We believe that these results are concerningly low for systems that may serve as a primary tool for information-seeking users, especially given their facade of trustworthiness. We hope that our results further motivate the development of trustworthy generative search engines and help researchers and users better understand the shortcomings of existing commercial systems.},
	language = {en},
	urldate = {2024-05-23},
	publisher = {arXiv},
	author = {Liu, Nelson F. and Zhang, Tianyi and Liang, Percy},
	month = oct,
	year = {2023},
	note = {arXiv:2304.09848 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {Liu et al. - 2023 - Evaluating Verifiability in Generative Search Engi.pdf:/Users/mariaacb/Zotero/storage/T7UL5L9C/Liu et al. - 2023 - Evaluating Verifiability in Generative Search Engi.pdf:application/pdf},
}

@misc{malaviya_expertqa_2024,
	title = {{ExpertQA}: {Expert}-{Curated} {Questions} and {Attributed} {Answers}},
	shorttitle = {{ExpertQA}},
	url = {http://arxiv.org/abs/2309.07852},
	abstract = {As language models are adopted by a more sophisticated and diverse set of users, the importance of guaranteeing that they provide factually correct information supported by verifiable sources is critical across fields of study. This is especially the case for high-stakes fields, such as medicine and law, where the risk of propagating false information is high and can lead to undesirable societal consequences. Previous work studying attribution and factuality has not focused on analyzing these characteristics of language model outputs in domain-specific scenarios. In this work, we conduct human evaluation of responses from a few representative systems along various axes of attribution and factuality, by bringing domain experts in the loop. Specifically, we collect expert-curated questions from 484 participants across 32 fields of study, and then ask the same experts to evaluate generated responses to their own questions. In addition, we ask experts to improve upon responses from language models. The output of our analysis is ExpertQA, a high-quality long-form QA dataset with 2177 questions spanning 32 fields, along with verified answers and attributions for claims in the answers.},
	language = {en},
	urldate = {2024-05-23},
	publisher = {arXiv},
	author = {Malaviya, Chaitanya and Lee, Subin and Chen, Sihao and Sieber, Elizabeth and Yatskar, Mark and Roth, Dan},
	month = apr,
	year = {2024},
	note = {arXiv:2309.07852 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Malaviya et al. - 2024 - ExpertQA Expert-Curated Questions and Attributed .pdf:/Users/mariaacb/Zotero/storage/LZQ4I5C6/Malaviya et al. - 2024 - ExpertQA Expert-Curated Questions and Attributed .pdf:application/pdf},
}

@misc{wang_factcheck-bench_2024,
	title = {Factcheck-{Bench}: {Fine}-{Grained} {Evaluation} {Benchmark} for {Automatic} {Fact}-checkers},
	shorttitle = {Factcheck-{Bench}},
	url = {http://arxiv.org/abs/2311.09000},
	abstract = {The increased use of large language models (LLMs) across a variety of real-world applications calls for mechanisms to verify the factual accuracy of their outputs. In this work, we present a holistic end-to-end solution for annotating the factuality of LLM-generated responses, which encompasses a multi-stage annotation scheme designed to yield detailed labels concerning the verifiability and factual inconsistencies found in LLM outputs. We further construct an open-domain documentlevel factuality benchmark in three-level granularity: claim, sentence and document, aiming to facilitate the evaluation of automatic factchecking systems. Preliminary experiments show that FacTool, FactScore and Perplexity.ai are struggling to identify false claims, with the best F1=0.63 by this annotation solution based on GPT-4. Annotation tool, benchmark and code are available at https://github.com/ yuxiaw/Factcheck-GPT.},
	language = {en},
	urldate = {2024-05-23},
	publisher = {arXiv},
	author = {Wang, Yuxia and Reddy, Revanth Gangi and Mujahid, Zain Muhammad and Arora, Arnav and Rubashevskii, Aleksandr and Geng, Jiahui and Afzal, Osama Mohammed and Pan, Liangming and Borenstein, Nadav and Pillai, Aditya and Augenstein, Isabelle and Gurevych, Iryna and Nakov, Preslav},
	month = apr,
	year = {2024},
	note = {arXiv:2311.09000 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Wang et al. - 2024 - Factcheck-Bench Fine-Grained Evaluation Benchmark.pdf:/Users/mariaacb/Zotero/storage/KWMZM9CW/Wang et al. - 2024 - Factcheck-Bench Fine-Grained Evaluation Benchmark.pdf:application/pdf},
}

@misc{chen_understanding_2023,
	title = {Understanding {Retrieval} {Augmentation} for {Long}-{Form} {Question} {Answering}},
	url = {http://arxiv.org/abs/2310.12150},
	abstract = {We present a study of retrieval-augmented language models (LMs) on long-form question answering. We analyze how retrieval augmentation impacts different LMs, by comparing answers generated from models while using the same evidence documents, and how differing quality of retrieval document set impacts the answers generated from the same LM. We study various attributes of generated answers (e.g., fluency, length, variance) with an emphasis on the attribution of generated longform answers to in-context evidence documents. We collect human annotations of answer attribution and evaluate methods for automatically judging attribution. Our study provides new insights on how retrieval augmentation impacts long, knowledge-rich text generation of LMs. We further identify attribution patterns for long text generation and analyze the main culprits of attribution errors. Together, our analysis reveals how retrieval augmentation impacts long knowledge-rich text generation and provide directions for future work.},
	language = {en},
	urldate = {2024-05-23},
	publisher = {arXiv},
	author = {Chen, Hung-Ting and Xu, Fangyuan and Arora, Shane and Choi, Eunsol},
	month = oct,
	year = {2023},
	note = {arXiv:2310.12150 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Chen et al. - 2023 - Understanding Retrieval Augmentation for Long-Form.pdf:/Users/mariaacb/Zotero/storage/6SF4Q9YL/Chen et al. - 2023 - Understanding Retrieval Augmentation for Long-Form.pdf:application/pdf},
}

@inproceedings{yue_automatic_2023,
	address = {Singapore},
	title = {Automatic {Evaluation} of {Attribution} by {Large} {Language} {Models}},
	url = {https://aclanthology.org/2023.findings-emnlp.307},
	doi = {10.18653/v1/2023.findings-emnlp.307},
	language = {en},
	urldate = {2024-05-24},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Yue, Xiang and Wang, Boshi and Chen, Ziru and Zhang, Kai and Su, Yu and Sun, Huan},
	year = {2023},
	pages = {4615--4635},
	file = {Yue et al. - 2023 - Automatic Evaluation of Attribution by Large Langu.pdf:/Users/mariaacb/Zotero/storage/VBMN9HZY/Yue et al. - 2023 - Automatic Evaluation of Attribution by Large Langu.pdf:application/pdf},
}

@inproceedings{gao_enabling_2023,
	address = {Singapore},
	title = {Enabling {Large} {Language} {Models} to {Generate} {Text} with {Citations}},
	url = {https://aclanthology.org/2023.emnlp-main.398},
	doi = {10.18653/v1/2023.emnlp-main.398},
	language = {en},
	urldate = {2024-05-24},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Gao, Tianyu and Yen, Howard and Yu, Jiatong and Chen, Danqi},
	year = {2023},
	pages = {6465--6488},
	file = {Gao et al. - 2023 - Enabling Large Language Models to Generate Text wi.pdf:/Users/mariaacb/Zotero/storage/BWH5Z6HU/Gao et al. - 2023 - Enabling Large Language Models to Generate Text wi.pdf:application/pdf},
}

@misc{perlitz_efficient_2024,
	title = {Efficient {Benchmarking} of {Language} {Models}},
	url = {http://arxiv.org/abs/2308.11696},
	abstract = {The increasing versatility of language models (LMs) has given rise to a new class of benchmarks that comprehensively assess a broad range of capabilities. Such benchmarks are associated with massive computational costs, extending to thousands of GPU hours per model. However, the efficiency aspect of these evaluation efforts had raised little discussion in the literature. In this work, we present the problem of Efficient Benchmarking, namely, intelligently reducing the computation costs of LM evaluation without compromising reliability. Using the HELM benchmark as a test case, we investigate how different benchmark design choices affect the computation-reliability trade-off. We propose to evaluate the reliability of such decisions, by using a new measure – Decision Impact on Reliability, DIoR for short. We find, for example, that a benchmark leader may change by merely removing a low-ranked model from the benchmark, and observe that a correct benchmark ranking can be obtained by considering only a fraction of the evaluation examples. Based on our findings, we outline a set of concrete recommendations for efficient benchmark design and utilization practices. To take a step further, we use our findings to propose an evaluation algorithm, that, when applied to the HELM benchmark, leads to dramatic cost savings with minimal loss of benchmark reliability, often reducing computation by x100 or more.},
	language = {en},
	urldate = {2024-06-13},
	publisher = {arXiv},
	author = {Perlitz, Yotam and Bandel, Elron and Gera, Ariel and Arviv, Ofir and Ein-Dor, Liat and Shnarch, Eyal and Slonim, Noam and Shmueli-Scheuer, Michal and Choshen, Leshem},
	month = apr,
	year = {2024},
	note = {arXiv:2308.11696 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Perlitz et al. - 2024 - Efficient Benchmarking of Language Models.pdf:/Users/mariaacb/Zotero/storage/N9MBYUPA/Perlitz et al. - 2024 - Efficient Benchmarking of Language Models.pdf:application/pdf},
}

@misc{saad-falcon_ares_2024,
	title = {{ARES}: {An} {Automated} {Evaluation} {Framework} for {Retrieval}-{Augmented} {Generation} {Systems}},
	shorttitle = {{ARES}},
	url = {http://arxiv.org/abs/2311.09476},
	abstract = {Evaluating retrieval-augmented generation (RAG) systems traditionally relies on hand annotations for input queries, passages to retrieve, and responses to generate. We introduce ARES, an Automated RAG Evaluation System, for evaluating RAG systems along the dimensions of context relevance, answer faithfulness, and answer relevance. By creating its own synthetic training data, ARES finetunes lightweight LM judges to assess the quality of individual RAG components. To mitigate potential prediction errors, ARES utilizes a small set of human-annotated datapoints for prediction-powered inference (PPI). Across eight different knowledge-intensive tasks in KILT, SuperGLUE, and AIS, ARES accurately evaluates RAG systems while using only a few hundred human annotations during evaluation. Furthermore, ARES judges remain effective across domain shifts, proving accurate even after changing the type of queries and/or documents used in the evaluated RAG systems. We make our code and datasets publicly available on Github.},
	language = {en},
	urldate = {2024-06-17},
	publisher = {arXiv},
	author = {Saad-Falcon, Jon and Khattab, Omar and Potts, Christopher and Zaharia, Matei},
	month = mar,
	year = {2024},
	note = {arXiv:2311.09476 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {Saad-Falcon et al. - 2024 - ARES An Automated Evaluation Framework for Retrie.pdf:/Users/mariaacb/Zotero/storage/LZRA5W8K/Saad-Falcon et al. - 2024 - ARES An Automated Evaluation Framework for Retrie.pdf:application/pdf},
}

@misc{hada_metal_2024,
	title = {{METAL}: {Towards} {Multilingual} {Meta}-{Evaluation}},
	shorttitle = {{METAL}},
	url = {http://arxiv.org/abs/2404.01667},
	abstract = {With the rising human-like precision of Large Language Models (LLMs) in numerous tasks, their utilization in a variety of real-world applications is becoming more prevalent. Several studies have shown that LLMs excel on many standard NLP benchmarks. However, it is challenging to evaluate LLMs due to test dataset contamination and the limitations of traditional metrics. Since human evaluations are difficult to collect, there is a growing interest in the community to use LLMs themselves as reference-free evaluators for subjective metrics. However, past work has shown that LLMbased evaluators can exhibit bias and have poor alignment with human judgments. In this study, we propose a framework for an end-to-end assessment of LLMs as evaluators in multilingual scenarios. We create a carefully curated dataset, covering 10 languages containing native speaker judgments for the task of summarization. This dataset is created specifically to evaluate LLM-based evaluators, which we refer to as meta-evaluation (METAL). We compare the performance of LLM-based evaluators created using GPT-3.5-Turbo, GPT-4, and PaLM2. Our results indicate that LLM-based evaluators based on GPT-4 perform the best across languages, while GPT-3.5-Turbo performs poorly. Additionally, we perform an analysis of the reasoning provided by LLM-based evaluators and find that it often does not match the reasoning provided by human judges.},
	language = {en},
	urldate = {2024-06-17},
	publisher = {arXiv},
	author = {Hada, Rishav and Gumma, Varun and Ahmed, Mohamed and Bali, Kalika and Sitaram, Sunayana},
	month = apr,
	year = {2024},
	note = {arXiv:2404.01667 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Hada et al. - 2024 - METAL Towards Multilingual Meta-Evaluation.pdf:/Users/mariaacb/Zotero/storage/TEUCWPHV/Hada et al. - 2024 - METAL Towards Multilingual Meta-Evaluation.pdf:application/pdf},
}

@misc{angelopoulos_prediction-powered_2023,
	title = {Prediction-{Powered} {Inference}},
	url = {http://arxiv.org/abs/2301.09633},
	abstract = {Prediction-powered inference is a framework for performing valid statistical inference when an experimental dataset is supplemented with predictions from a machine-learning system. The framework yields simple algorithms for computing provably valid confidence intervals for quantities such as means, quantiles, and linear and logistic regression coefficients, without making any assumptions on the machinelearning algorithm that supplies the predictions. Furthermore, more accurate predictions translate to smaller confidence intervals. Prediction-powered inference could enable researchers to draw valid and more data-efficient conclusions using machine learning. The benefits of prediction-powered inference are demonstrated with datasets from proteomics, astronomy, genomics, remote sensing, census analysis, and ecology.},
	language = {en},
	urldate = {2024-06-17},
	publisher = {arXiv},
	author = {Angelopoulos, Anastasios N. and Bates, Stephen and Fannjiang, Clara and Jordan, Michael I. and Zrnic, Tijana},
	month = nov,
	year = {2023},
	note = {arXiv:2301.09633 [cs, q-bio, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Quantitative Biology - Quantitative Methods, Statistics - Machine Learning, Statistics - Methodology},
	file = {Angelopoulos et al. - 2023 - Prediction-Powered Inference.pdf:/Users/mariaacb/Zotero/storage/BYNB6NN2/Angelopoulos et al. - 2023 - Prediction-Powered Inference.pdf:application/pdf},
}

@misc{yang_crag_2024,
	title = {{CRAG} -- {Comprehensive} {RAG} {Benchmark}},
	url = {http://arxiv.org/abs/2406.04744},
	abstract = {Retrieval-Augmented Generation (RAG) has recently emerged as a promising solution to alleviate Large Language Model (LLM)’s deficiency in lack of knowledge. Existing RAG datasets, however, do not adequately represent the diverse and dynamic nature of real-world Question Answering (QA) tasks. To bridge this gap, we introduce the Comprehensive RAG Benchmark (CRAG), a factual question answering benchmark of 4,409 question-answer pairs and mock APIs to simulate web and Knowledge Graph (KG) search. CRAG is designed to encapsulate a diverse array of questions across five domains and eight question categories, reflecting varied entity popularity from popular to long-tail, and temporal dynamisms ranging from years to seconds. Our evaluation on this benchmark highlights the gap to fully trustworthy QA. Whereas most advanced LLMs achieve ď 34\% accuracy on CRAG, adding RAG in a straightforward manner improves the accuracy only to 44\%. State-of-the-art industry RAG solutions only answer 63\% questions without any hallucination. CRAG also reveals much lower accuracy in answering questions regarding facts with higher dynamism, lower popularity, or higher complexity, suggesting future research directions. The CRAG benchmark laid the groundwork for a KDD Cup 2024 challenge, attracting thousands of participants and submissions within the first 50 days of the competition. We commit to maintaining CRAG to serve research communities in advancing RAG solutions and general QA solutions.},
	language = {en},
	urldate = {2024-06-18},
	publisher = {arXiv},
	author = {Yang, Xiao and Sun, Kai and Xin, Hao and Sun, Yushi and Bhalla, Nikita and Chen, Xiangsen and Choudhary, Sajal and Gui, Rongze Daniel and Jiang, Ziran Will and Jiang, Ziyu and Kong, Lingkun and Moran, Brian and Wang, Jiaqi and Xu, Yifan Ethan and Yan, An and Yang, Chenyu and Yuan, Eting and Zha, Hanwen and Tang, Nan and Chen, Lei and Scheffer, Nicolas and Liu, Yue and Shah, Nirav and Wanga, Rakesh and Kumar, Anuj and Yih, Wen-tau and Dong, Xin Luna},
	month = jun,
	year = {2024},
	note = {arXiv:2406.04744 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Yang et al. - 2024 - CRAG -- Comprehensive RAG Benchmark.pdf:/Users/mariaacb/Zotero/storage/T42H4WC4/Yang et al. - 2024 - CRAG -- Comprehensive RAG Benchmark.pdf:application/pdf},
}

@article{elangovan_considers--human_nodate,
	title = {{ConSiDERS}-{The}-{Human} {Evaluation} {Framework}: {Rethinking} {Human} {Evaluation} for {Generative} {Large} {Language} {Models}},
	abstract = {In this position paper, we argue that human evaluation of generative large language models (LLMs) should be a multidisciplinary undertaking that draws upon insights from disciplines such as user experience research and human behavioral psychology to ensure that the experimental design and results are reliable. The conclusions from these evaluations, thus, must consider factors such as usability, aesthetics, and cognitive biases. We highlight how cognitive biases can conflate fluent information and truthfulness, and how cognitive uncertainty affects the reliability of rating scores such as Likert. Furthermore, the evaluation should differentiate the capabilities and weaknesses of increasingly powerful large language models – which requires effective test sets. The scalability of human evaluation is also crucial to wider adoption. Hence, to design an effective human evaluation system in the age of generative NLP, we propose the ConSiDERSThe-Human evaluation framework consisting of 6 pillars – Consistency, Scoring Critera, Differentiating, User Experience, Responsible, and Scalability.},
	language = {en},
	author = {Elangovan, Aparna and Liu, Ling and Xu, Lei and Bodapati, Sravan and Roth, Dan},
	file = {Elangovan et al. - ConSiDERS-The-Human Evaluation Framework Rethinki.pdf:/Users/mariaacb/Zotero/storage/KQI5RB48/Elangovan et al. - ConSiDERS-The-Human Evaluation Framework Rethinki.pdf:application/pdf},
}

@inproceedings{krishna-etal-2023-longeval,
    title = "{L}ong{E}val: Guidelines for Human Evaluation of Faithfulness in Long-form Summarization",
    author = "Krishna, Kalpesh  and
      Bransom, Erin  and
      Kuehl, Bailey  and
      Iyyer, Mohit  and
      Dasigi, Pradeep  and
      Cohan, Arman  and
      Lo, Kyle",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.121",
    doi = "10.18653/v1/2023.eacl-main.121",
    pages = "1650--1669",
}


@inproceedings{pagnoni_understanding_2021,
	address = {Online},
	title = {Understanding {Factuality} in {Abstractive} {Summarization} with {FRANK}: {A} {Benchmark} for {Factuality} {Metrics}},
	shorttitle = {Understanding {Factuality} in {Abstractive} {Summarization} with {FRANK}},
	url = {https://aclanthology.org/2021.naacl-main.383},
	doi = {10.18653/v1/2021.naacl-main.383},
	language = {en},
	urldate = {2024-08-13},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Pagnoni, Artidoro and Balachandran, Vidhisha and Tsvetkov, Yulia},
	year = {2021},
	pages = {4812--4829},
	file = {Pagnoni et al. - 2021 - Understanding Factuality in Abstractive Summarizat.pdf:/Users/mariaacb/Zotero/storage/QXD4VG7Q/Pagnoni et al. - 2021 - Understanding Factuality in Abstractive Summarizat.pdf:application/pdf},
}

@article{sen2022mintaka,
  title={Mintaka: A complex, natural, and multilingual dataset for end-to-end question answering},
  author={Sen, Priyanka and Aji, Alham Fikri and Saffari, Amir},
  journal={arXiv preprint arXiv:2210.01613},
  year={2022}
}
@article{tan2023mlpq,
  title={MLPQ: A dataset for path question answering over multilingual knowledge graphs},
  author={Tan, Yiming and Chen, Yongrui and Qi, Guilin and Li, Weizhuo and Wang, Meng},
  journal={Big Data Research},
  volume={32},
  pages={100381},
  year={2023},
  publisher={Elsevier}
}
@article{perevalov2023multilingual,
  title={Multilingual question answering systems for knowledge graphs-a survey},
  author={Perevalov, Aleksandr and Both, Andreas and Ngomo, Axel-Cyrille Ngonga},
  journal={Semant. Web J},
  year={2023}
}
@article{rajpurkar2016squad,
  title={Squad: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, P},
  journal={arXiv preprint arXiv:1606.05250},
  year={2016}
}
@book{schutze2008introduction,
  title={Introduction to information retrieval},
  author={Sch{\"u}tze, Hinrich and Manning, Christopher D and Raghavan, Prabhakar},
  volume={39},
  year={2008},
  publisher={Cambridge University Press Cambridge}
}

@inproceedings{kryscinski_evaluating_2020,
    title = "Evaluating the Factual Consistency of Abstractive Text Summarization",
    author = "Kryscinski, Wojciech  and
      McCann, Bryan  and
      Xiong, Caiming  and
      Socher, Richard",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.750",
    doi = "10.18653/v1/2020.emnlp-main.750",
    pages = "9332--9346",
    abstract = "The most common metrics for assessing summarization algorithms do not account for whether summaries are factually consistent with source documents. We propose a weakly-supervised, model-based approach for verifying factual consistency and identifying conflicts between source documents and generated summaries. Training data is generated by applying a series of rule-based transformations to the sentences of source documents. The factual consistency model is then trained jointly for three tasks: 1) predict whether each summary sentence is factually consistent or not, 2) in either case, extract a span in the source document to support this consistency prediction, 3) for each summary sentence that is deemed inconsistent, extract the inconsistent span from it. Transferring this model to summaries generated by several neural models reveals that this highly scalable approach outperforms previous models, including those trained with strong supervision using datasets from related domains, such as natural language inference and fact checking. Additionally, human evaluation shows that the auxiliary span extraction tasks provide useful assistance in the process of verifying factual consistency. We also release a manually annotated dataset for factual consistency verification, code for training data generation, and trained model weights at \url{https://github.com/salesforce/factCC}.",
}

@article{chain_of_thought_jason_wei,
  author       = {Jason Wei and
                  Xuezhi Wang and
                  Dale Schuurmans and
                  Maarten Bosma and
                  Ed H. Chi and
                  Quoc Le and
                  Denny Zhou},
  title        = {Chain of Thought Prompting Elicits Reasoning in Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2201.11903},
  year         = {2022},
  url          = {https://arxiv.org/abs/2201.11903},
  eprinttype    = {arXiv},
  eprint       = {2201.11903},
  timestamp    = {Fri, 22 Apr 2022 16:06:31 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2201-11903.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{gwet2008computing,
  title={Computing inter-rater reliability and its variance in the presence of high agreement},
  author={Gwet, Kilem Li},
  journal={British Journal of Mathematical and Statistical Psychology},
  volume={61},
  number={1},
  pages={29--48},
  year={2008},
  publisher={Wiley Online Library}
}

@misc{bm25s,
      title={BM25S: Orders of magnitude faster lexical search via eager sparse scoring}, 
      author={Xing Han Lù},
      year={2024},
      eprint={2407.03618},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2407.03618}, 
}

@inproceedings{sadvilkar-neumann-2020-pysbd,
    title = "{P}y{SBD}: Pragmatic Sentence Boundary Disambiguation",
    author = "Sadvilkar, Nipun  and
      Neumann, Mark",
    booktitle = "Proceedings of Second Workshop for NLP Open Source Software (NLP-OSS)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.nlposs-1.15",
    pages = "110--114",
    abstract = "We present a rule-based sentence boundary disambiguation Python package that works out-of-the-box for 22 languages. We aim to provide a realistic segmenter which can provide logical sentences even when the format and domain of the input text is unknown. In our work, we adapt the Golden Rules Set (a language specific set of sentence boundary exemplars) originally implemented as a ruby gem pragmatic segmenter which we ported to Python with additional improvements and functionality. PySBD passes 97.92{\%} of the Golden Rule Set examplars for English, an improvement of 25{\%} over the next best open source Python tool.",
}

@inproceedings{kwon2023efficient,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},
  year={2023}
}

@book{good2013permutation,
  title={Permutation tests: a practical guide to resampling methods for testing hypotheses},
  author={Good, Phillip},
  year={2013},
  publisher={Springer Science \& Business Media}
}



@misc{fernandes_devil_2023,
	title = {The {Devil} is in the {Errors}: {Leveraging} {Large} {Language} {Models} for {Fine}-grained {Machine} {Translation} {Evaluation}},
	shorttitle = {The {Devil} is in the {Errors}},
	url = {http://arxiv.org/abs/2308.07286},
	urldate = {2023-12-20},
	publisher = {arXiv},
	author = {Fernandes, Patrick and Deutsch, Daniel and Finkelstein, Mara and Riley, Parker and Martins, André F. T. and Neubig, Graham and Garg, Ankush and Clark, Jonathan H. and Freitag, Markus and Firat, Orhan},
	month = aug,
	year = {2023},
	note = {arXiv:2308.07286 [cs]},
	keywords = {Language Models, Machine Translation, MT Evaluation},
	annote = {Comment: 19 pages}
}


@article{fleiss_kappa,
author = {Fleiss, Joseph},
year = {1971},
month = {11},
pages = {378-},
title = {Measuring Nominal Scale Agreement Among Many Raters},
volume = {76},
journal = {Psychological Bulletin},
doi = {10.1037/h0031619}
}

@misc{mirage_bench,
      title={MIRAGE-Bench: Automatic Multilingual Benchmark Arena for Retrieval-Augmented Generation Systems}, 
      author={Nandan Thakur and Suleman Kazi and Ge Luo and Jimmy Lin and Amin Ahmad},
      year={2024},
      eprint={2410.13716},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.13716}, 
}

@misc{chirkova2024s,
      title={Retrieval-augmented generation in multilingual settings}, 
      author={Nadezhda Chirkova and David Rau and Hervé Déjean and Thibault Formal and Stéphane Clinchant and Vassilina Nikoulina},
      year={2024},
      eprint={2407.01463},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.01463}, 
}

@misc{self_preference_bias,
      title={LLM Evaluators Recognize and Favor Their Own Generations}, 
      author={Arjun Panickssery and Samuel R. Bowman and Shi Feng},
      year={2024},
      eprint={2404.13076},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.13076}, 
}