\section{Related Work}
Due to the pipeline approach of RAG systems, the evaluation can be split into 3 main components: 1) retriever metrics to identify relevant chunks of information to the input typically measured with recall/precision@K ____; 2) generator metrics to identify the “usefulness” of the generated answers in relation to the input, this is done across fine grained dimensions such as faithfulness and relevance, either leveraging references answers ____; 3) end-to-end (overall) metrics that take into account additional components such as preprocessing, chunking, query reformulation and cascading errors. Retrieval metrics have been extensively studied in the information retrieval community, hence recent work focused on the text generation performance of RAG systems. This is also the focus of our work. One important difference between generation with and without retrieved documents is the conflict between the parametric “world knowledge” and the non-parametric retrieved documents knowledge, hence the distinction between faithfulness against the retrieved documents (RAG specific) and factuality according to general knowledge____.

%\jt{I have added additional related work as suggested by MF}

Recent studies have investigated the importance of various components within multilingual RAG systems. ____ utilized existing multilingual QA datasets to evaluate different combinations of retrievers and generator models, finding that task-specific prompt engineering is crucial for high-quality multilingual generation. In another study, ____ extend MIRACL dataset to develop ``MIRAGE-BENCH'' a synthetic arena-based benchmark for ranking multilingual LLM generators. They employed preference judgments from a GPT-4o ``judge'' to train a ranking model. However, an important limitation of such synthetic benchmarks is the potential for self-preference bias ____, where the LLM judge may favor its own generations.

In this work, we focus on the faithfulness and relevance aspects to ensure meaningful results. These dimensions are typically assessed by human evaluators based on model-generated outputs, highlighting the need for developing automatic evaluation metrics that correlate well with human judgment—a process known as meta-evaluation. This need has led to a recent trend in the English-language research community of publishing meta-evaluation datasets and developing automated evaluators ____.

%For multilingual RAG evaluation, the focus has been on evaluating the retrieval %performance. E.g. MIRACL ____ is a retrieval focused %multilingual dataset covering 18 languages and based on Wikipedia articles. %Questions about the articles are composed by native speakers and relevance of the %etrieved documents are judged. NoMIRACL____ extends the MIRACL dataset by incorporating a subset of questions for which none of the retrieved passages are relevant, specifically to benchmark LLMs' ability to recognize when they lack sufficient information to provide an answer—a capability referred to as "knowing when you don't know".

%There are works evaluating multilingual generation capabilities but are either span-selection focused or lack a retrieval component ____. 

%\mf{SOMETHING ABOUT ENGLISH AND TRANSLATION-BASED META-EVAL OF RAG TEXT GENERATION}

To the best of our knowledge, no multilingual meta-evaluation benchmark for RAG systems currently exists. In this work, we address this gap by developing such a benchmark. Our dataset facilitates the creation of multilingual automatic evaluators that correlate well with human judgments. This, in turn, enables comprehensive end-to-end benchmarking of RAG systems. We believe our dataset is the first to offer this capability in a multilingual context.