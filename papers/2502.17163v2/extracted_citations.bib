@misc{chirkova2024s,
      title={Retrieval-augmented generation in multilingual settings}, 
      author={Nadezhda Chirkova and David Rau and Hervé Déjean and Thibault Formal and Stéphane Clinchant and Vassilina Nikoulina},
      year={2024},
      eprint={2407.01463},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.01463}, 
}

@article{clark-etal-2020-tydi,
    title = "{T}y{D}i {QA}: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages",
    author = "Clark, Jonathan H.  and
      Choi, Eunsol  and
      Collins, Michael  and
      Garrette, Dan  and
      Kwiatkowski, Tom  and
      Nikolaev, Vitaly  and
      Palomaki, Jennimaria",
    editor = "Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "8",
    year = "2020",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2020.tacl-1.30",
    doi = "10.1162/tacl_a_00317",
    pages = "454--470",
}

@inproceedings{es-etal-2024-ragas,
    title = "{RAGA}s: Automated Evaluation of Retrieval Augmented Generation",
    author = "Es, Shahul  and
      James, Jithin  and
      Espinosa Anke, Luis  and
      Schockaert, Steven",
    editor = "Aletras, Nikolaos  and
      De Clercq, Orphee",
    booktitle = "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations",
    month = mar,
    year = "2024",
    address = "St. Julians, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.eacl-demo.16",
    pages = "150--158",
}

@inproceedings{lewis_mlqa_2020,
	address = {Online},
	title = {{MLQA}: {Evaluating} {Cross}-lingual {Extractive} {Question} {Answering}},
	shorttitle = {{MLQA}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.653},
	doi = {10.18653/v1/2020.acl-main.653},
	abstract = {Question answering (QA) models have shown rapid progress enabled by the availability of large, high-quality benchmark datasets. Such annotated datasets are difﬁcult and costly to collect, and rarely exist in languages other than English, making building QA systems that work well in other languages challenging. In order to develop such systems, it is crucial to invest in high quality multilingual evaluation benchmarks to measure progress. We present MLQA, a multi-way aligned extractive QA evaluation benchmark intended to spur research in this area.1 MLQA contains QA instances in 7 languages, English, Arabic, German, Spanish, Hindi, Vietnamese and Simpliﬁed Chinese. MLQA has over 12K instances in English and 5K in each other language, with each instance parallel between 4 languages on average. We evaluate stateof-the-art cross-lingual models and machinetranslation-based baselines on MLQA. In all cases, transfer results are signiﬁcantly behind training-language performance.},
	language = {en},
	urldate = {2024-05-02},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},
	year = {2020},
	pages = {7315--7330},
	file = {Lewis et al. - 2020 - MLQA Evaluating Cross-lingual Extractive Question.pdf:/Users/mariaacb/Zotero/storage/AP8AK7VC/Lewis et al. - 2020 - MLQA Evaluating Cross-lingual Extractive Question.pdf:application/pdf},
}

@book{manning2008ir,
  address = {Cambridge, UK},
  author = {Manning, Christopher D. and Raghavan, Prabhakar and Schütze, Hinrich},
  isbn = {978-0-521-86571-5},
  keywords = {book information introduction ir retrieval},
  publisher = {Cambridge University Press},
  timestamp = {2010-12-20T06:06:48.000+0100},
  title = {Introduction to Information Retrieval},
  url = {http://nlp.stanford.edu/IR-book/information-retrieval-book.html},
  year = 2008
}

@inproceedings{maynez-etal-2020-faithfulness,
    title = "On Faithfulness and Factuality in Abstractive Summarization",
    author = "Maynez, Joshua  and
      Narayan, Shashi  and
      Bohnet, Bernd  and
      McDonald, Ryan",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.173",
    doi = "10.18653/v1/2020.acl-main.173",
    pages = "1906--1919",
}

@misc{mirage_bench,
      title={MIRAGE-Bench: Automatic Multilingual Benchmark Arena for Retrieval-Augmented Generation Systems}, 
      author={Nandan Thakur and Suleman Kazi and Ge Luo and Jimmy Lin and Amin Ahmad},
      year={2024},
      eprint={2410.13716},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.13716}, 
}

@misc{saad-falcon_ares_2024,
	title = {{ARES}: {An} {Automated} {Evaluation} {Framework} for {Retrieval}-{Augmented} {Generation} {Systems}},
	shorttitle = {{ARES}},
	url = {http://arxiv.org/abs/2311.09476},
	abstract = {Evaluating retrieval-augmented generation (RAG) systems traditionally relies on hand annotations for input queries, passages to retrieve, and responses to generate. We introduce ARES, an Automated RAG Evaluation System, for evaluating RAG systems along the dimensions of context relevance, answer faithfulness, and answer relevance. By creating its own synthetic training data, ARES finetunes lightweight LM judges to assess the quality of individual RAG components. To mitigate potential prediction errors, ARES utilizes a small set of human-annotated datapoints for prediction-powered inference (PPI). Across eight different knowledge-intensive tasks in KILT, SuperGLUE, and AIS, ARES accurately evaluates RAG systems while using only a few hundred human annotations during evaluation. Furthermore, ARES judges remain effective across domain shifts, proving accurate even after changing the type of queries and/or documents used in the evaluated RAG systems. We make our code and datasets publicly available on Github.},
	language = {en},
	urldate = {2024-06-17},
	publisher = {arXiv},
	author = {Saad-Falcon, Jon and Khattab, Omar and Potts, Christopher and Zaharia, Matei},
	month = mar,
	year = {2024},
	note = {arXiv:2311.09476 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {Saad-Falcon et al. - 2024 - ARES An Automated Evaluation Framework for Retrie.pdf:/Users/mariaacb/Zotero/storage/LZRA5W8K/Saad-Falcon et al. - 2024 - ARES An Automated Evaluation Framework for Retrie.pdf:application/pdf},
}

@misc{self_preference_bias,
      title={LLM Evaluators Recognize and Favor Their Own Generations}, 
      author={Arjun Panickssery and Samuel R. Bowman and Shi Feng},
      year={2024},
      eprint={2404.13076},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.13076}, 
}

@inproceedings{thakur-etal-2024-nomiracl,
    title = "{``}Knowing When You Don{'}t Know{''}: A Multilingual Relevance Assessment Dataset for Robust Retrieval-Augmented Generation",
    author = "Thakur, Nandan  and
      Bonifacio, Luiz  and
      Zhang, Crystina  and
      Ogundepo, Odunayo  and
      Kamalloo, Ehsan  and
      Alfonso-Hermelo, David  and
      Li, Xiaoguang  and
      Liu, Qun  and
      Chen, Boxing  and
      Rezagholizadeh, Mehdi  and
      Lin, Jimmy",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.730",
    pages = "12508--12526",
}

@article{zhang_miracl_2023,
	title = {\textbf{{MIRACL}} : {A} {Multilingual} {Retrieval} {Dataset} {Covering} 18 {Diverse} {Languages}},
	volume = {11},
	issn = {2307-387X},
	shorttitle = {\textbf{{MIRACL}}},
	url = {https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00595/117438/MIRACL-A-Multilingual-Retrieval-Dataset-Covering},
	doi = {10.1162/tacl_a_00595},
	abstract = {MIRACL is a multilingual dataset for ad hoc retrieval across 18 languages that collectively encompass over three billion native speakers around the world. This resource is designed to support monolingual retrieval tasks, where the queries and the corpora are in the same language. In total, we have gathered over 726k high-quality relevance judgments for 78k queries over Wikipedia in these languages, where all annotations have been performed by native speakers hired by our team. MIRACL covers languages that are both typologically close as well as distant from 10 language families and 13 sub-families, associated with varying amounts of publicly available resources. Extensive automatic heuristic verification and manual assessments were performed during the annotation process to control data quality. In total, MIRACL represents an investment of around five person-years of human annotator effort. Our goal is to spur research on improving retrieval across a continuum of languages, thus enhancing information access capabilities for diverse populations around the world, particularly those that have traditionally been underserved. MIRACL is available at http:// miracl.ai/.},
	language = {en},
	urldate = {2024-04-29},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Zhang, Xinyu and Thakur, Nandan and Ogundepo, Odunayo and Kamalloo, Ehsan and Alfonso-Hermelo, David and Li, Xiaoguang and Liu, Qun and Rezagholizadeh, Mehdi and Lin, Jimmy},
	month = sep,
	year = {2023},
	pages = {1114--1131},
	file = {Zhang et al. - 2023 - MIRACL  A Multilingual Retrieval Dataset C.pdf:/Users/mariaacb/Zotero/storage/97SDBW98/Zhang et al. - 2023 - MIRACL  A Multilingual Retrieval Dataset C.pdf:application/pdf},
}

