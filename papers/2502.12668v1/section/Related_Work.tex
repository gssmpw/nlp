\section{Related Work}
\paragraph{Robust MDPs}
% Several studies have examined reinforcement learning considering the worst-case scenario for rewards. \cite{ortega2014adversarial} considers only a single-step analysis for reward robust problems. \cite{husain2021regularized} proposes a Deep RL algorithm related to Q Learning for the reward robust problem. \cite{NEURIPS2021_bb1443cc} considers both a reward function and transition probability to be unknown. Policy regularization is viewed as a perturbation of rewards, while transition probability perturbations address the worst-case scenario related to the value function's associated set. They define specific uncertainty sets and conduct thorough experiments. \cite{eysenbach2022maximum} demonstrates that incorporating the policy's Shannon entropy into the reinforcement learning objective function represents the worst-case scenario for a certain uncertainty set of rewards.
Several studies have investigated RL considering the worst-case scenario for rewards. \cite{ortega2014adversarial} considers only a single-step analysis for the reward robust problem. \cite{husain2021regularized} proposes a deep RL algorithm related to Q learning for the reward robust problem. \cite{NEURIPS2021_bb1443cc} considers both a reward function and the transition probability as unknown. The policy regularization is considered a perturbation of the rewards, while the transition probability perturbations address the worst-case scenario with respect to the associated set of value functions. They define specific uncertainty sets and conduct thorough experiments. \cite{eysenbach2022maximum} shows that incorporating the policy's Shannon entropy into the reinforcement learning objective function represents the worst-case scenario for a given uncertainty set of rewards.

% \paragraph{Regularized MDPs}
% Several key studies have illuminated the benefits of incorporating regularization terms in reinforcement learning algorithms. Specifically, \citet{haarnoja2018soft} demonstrated that adding a regularization term can promote efficient exploration. 
% Furthermore, \citet{ahmed2019understanding} showed that such regularization leads to smoother optimization of the policy $\pi$. 


\paragraph{Alignment Strategies}
% Two notable alignment strategies have recently gained attention: Reinforcement Learning from Human Feedback (RLHF)  and Direct Preference Optimization (DPO) \citep{stiennon2020,NEURIPS2023_a85b405e}.
% RLHF incorporates human feedback into the reinforcement learning process to align the agent's behavior with human preferences. Using human feedback as a reward signal, RLHF aims to optimize it. This approach has been successfully applied in LLM \citep{NEURIPS2022_b1efde53}. 
% On the other hand, DPO employs the same objective function as RLHF without an explicit reward function. However, it still suffers from over-optimization more than RLHF when dealing with out-of-distribution data \citep{xu2024is}. Beyond the methods discussed, there is research within the context of robust optimization that addresses the development of resilient algorithms for scenarios with unstable preference information \citep{wu2024towards}. Notably, \cite{chowdhury2024provably} have introduced a robust DPO approach that achieves robustness without explicitly employing robust optimization techniques.
Two notable adaptation strategies have recently gained attention: Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) \citep{stiennon2020,NEURIPS2023_a85b405e}.
RLHF incorporates human feedback into the reinforcement learning process to align the agent's behavior with human preferences. By using human feedback as a reward signal, RLHF aims to optimize it. This approach has been successfully used in LLM \citep{NEURIPS2022_b1efde53}. 
On the other hand, DPO uses the same objective function as RLHF without an explicit reward function. However, it still suffers more than RLHF from overoptimization when dealing with out-of-distribution data \citep{xu2024is}. Beyond the methods discussed, there is research in robust optimization that addresses the development of robust algorithms for scenarios with unstable preference information \citep{wu2024towards}. In particular, \cite{chowdhury2024provably} have introduced a robust DPO approach that achieves robustness without explicitly employing robust optimization techniques.
Another technique \citep{mudgal2024controlled} is to train a token-level scoring value function module to select the optimal output.
\cite{khanov2024args} is a novel decoding method that does not require additional learning and uses both the language model and the reward model knowledge. There is a parameter that determines which is more important, and depending on its value, it can be a conventional method.
% \paragraph{Robust Reinforcement Learning}
% Several studies have examined reinforcement learning considering the worst-case scenario for rewards. \citep{NEURIPS2021_bb1443cc} considers both the policy and transition probabilities to be unknown. Policy regularization is viewed as a perturbation of rewards, while transition probability perturbations address the worst-case scenario related to the value function's associated set. They define specific uncertainty sets and conduct thorough experiments. \citep{eysenbach2022maximum} demonstrates that incorporating the policy's Shannon entropy into the reinforcement learning objective function represents the worst-case scenario for a certain uncertainty set of rewards.


