\section{Theoretical Analysis on Stochastic RBoN}

Stochastic RBoN algorithms allow us to extend the similar analysis presented in Section \ref{pre:rl}. This comparative analysis is expected to shed light on the mechanisms by which regularization enhances the robustness and performance of Stochastic RBoN.
% A key factor contributing to the success of RBoN is its ability to address reward misspecification \citep{pan2022the}. This section presents a theoretical analysis, grounded in RRL principles, to elucidate why RBoN effectively mitigates issues arising from imprecisely defined reward functions. In this field, robustness is identifying optimal strategies that perform well even in worst-case reward scenarios. 


% \subsection{BoN}

% % We first mention that the objective function of BoN is equal to the objective function of the (unregularized) RL problem:

% % \begin{equation}
% % \begin{aligned}
% % \textbf{Objective Function of BoN} &= \max_{\pi} \,\, \langle \pi, R \rangle.
% % \end{aligned}
% % \end{equation}

% where $\langle \pi, R \rangle = \sum_{y \in \mathcal{Y_{\textbf{ref}}}} \pi(y)R(y)$, reward function $R$ $:\mathcal{Y}  \rightarrow \mathbb{R}$, output probability $\pi$ $\in$ $ \Delta (\mathcal{Y})$.

\subsection{Theoretical Analysis of $\mathrm{RBoN}_{\mathrm{SKL}}$}\label{sec:kl_sec}
In BoN methodology, all variables input x are predetermined. Consequently, our subsequent analysis focuses exclusively on the output y, and we formulate and examine the mathematical expressions accordingly.

The objective function of $\mathrm{RBoN}_{\mathrm{SKL}}$ is given by:




\begin{equation}\label{eq:kl_ind}
\begin{aligned}
\textbf{Objective Function of $\mathrm{RBoN}_{\mathrm{SKL}}$} &= \max_{\pi \in \Pi}  \,\, \langle \pi, R \rangle - \beta \sum_\mathcal{Y_{\textbf{ref}}} \pi (y)\log{\frac{\pi(y)}{\pi_{\textbf{ref}} (y)}}
\end{aligned}
\end{equation}
% where $\langle \pi, R \rangle = \sum_{y \in \mathcal{Y_{\textbf{ref}}}} \pi(y)R(y)$, reward function $R$ $:\mathcal{Y}  \rightarrow \mathbb{R}$, output probability $\pi$ $\in$ $ \Delta (\mathcal{Y})$, KL divergence function $\Omega(\pi) = \beta \textbf{KL} (\pi || \pi_{\textbf{ref}}) = \beta \sum_\mathcal{Y_{\textbf{ref}}} \pi (y)\log{\frac{\pi(y)}{\pi_{\textbf{ref}} (y)}}$. 


Eq. \ref{eq:kl_ind} includes a regularization term and can be interpreted as incorporating adversarial perturbations when subjected to reformulations. Based on the work of \citet{brekelmans2022your}, we can formulate the following max-min problem:
\begin{theorem}(\textbf{\cite{brekelmans2022your} Proposition 1})\label{theory:kl-minmax}
The following holds:
\begin{equation*}
\begin{aligned}
\textnormal{\textbf{Objective Function of $\mathrm{RBoN}_{\mathrm{SKL}}$}}&=\max_{\pi \in \Pi} \,\, \min_{\Delta R \in \mathcal{R}_{\Delta}} \,\, \langle \pi, R - \Delta R \rangle  +  \beta \log \sum_\mathcal{Y_{\textnormal{\textbf{ref}}}} \pi_{\textnormal{\textbf{ref}}}(y) \exp(\beta^{-1}\Delta R(y)) 
\end{aligned}
\end{equation*}
\begin{equation*}
\text{where}\quad \mathcal{R}_{\Delta}:=\left\{\Delta R \in \mathbb{R}^{\mathcal{Y}_{\textbf{ref}}} \mid \sum_\mathcal{Y_{\textbf{ref}}} \pi_{\textnormal{\textbf{ref}}}(y) \exp(\beta^{-1}\Delta R(y)) \leq 1\right\}
\end{equation*}
\end{theorem}

For a comprehensive derivation of \cref{theory:kl-minmax}, readers are directed to Proposition 1 in \cite{brekelmans2022your}. This finding implies that incorporating a regularization term produces an effect equivalent to introducing adversarial reward perturbations. The objective function of $\mathrm{RBoN}_{\mathrm{SKL}}$ can be considered as the regularizer for $\pi$ is an adversarial reward perturbation in the worst case. This translates into modified rewards $R^{\prime}(y)=R(y)-\Delta R(y)$ for the output probability $\pi_y$. Understanding the range of perturbation reward $\Delta R$ (constraint term) is crucial in optimizing the modified reward function $R^\prime$. This knowledge constitutes a key component in the optimization process. 


\paragraph{Intuition}
The algorithm can achieve robust learning in uncertainty by minimizing reward modifications for these high-probability outputs.


\subsection{Theoretical Analysis of $\mathrm{RBoN}_{\mathrm{SWD}}$}\label{sec:WD}
% This section advances two primary arguments, following the structure of the previous section: (1) the reformulation of $\mathrm{RBoN}_{\mathrm{SWD}}$ as a max-min problem. (2) the characterization of the perturbation range for $\Delta R$. 

The equation setting $\mathrm{RBoN}_{\mathrm{SWD}}$, the objective function of $\mathrm{RBoN}_{\mathrm{SWD}}$ is given by:


% \begin{equation}\label{eq:wdr}
%     \begin{aligned}
%      \pi^* &= \max_\pi \,\, \langle \pi ,R \rangle -\beta WD(\pi, \pi_{\textbf{ref}})\\
%      \end{aligned}
% \end{equation}
\begin{equation}\label{eq:wdr}
    \begin{aligned}
     \textbf{Objective Function of $\mathrm{RBoN}_{\mathrm{SWD}}$} &= \max_{\pi \in \Pi} \,\, \langle \pi ,R \rangle -\beta \textbf{WD} [\pi_{\textbf{ref}}(\cdot) \| \pi (\cdot)]\\
     \end{aligned}
\end{equation}



Similar to the previous section, the objective function of $\mathrm{RBoN}_{\mathrm{SWD}}$ (Eq. \ref{eq:wdr}) can be reformulated as the max-min problem. 
\begin{theorem}\label{theory:wd}
The following holds:
% The objective function of $\mathrm{RBoN}_{\mathrm{SWD}}$ (Eq. \ref{eq:wdr}) can be reformulated as the max-min problem. 
%     \begin{equation*}
% \textbf{Objective Function of $\mathrm{RBoN}_{\mathrm{SWD}}$} = \max_{\pi} \min_{\Delta R} \left\langle \pi, R - \beta \Delta R \right\rangle + \beta \left\langle \pi_{\text{ref}}, \Delta R \right\rangle
% \end{equation*}
% \end{theorem}

\begin{equation*}
    \textnormal{\textbf{Objective Function of $\mathrm{RBoN}_{\mathrm{SWD}}$}} = \max_{\pi \in \Pi} \,\,\min_{\Delta R \in \mathcal{R}_{\Delta}}\,\, \left\langle \pi, R - \beta \Delta R \right\rangle + \beta \left\langle \pi_{\textnormal{\textbf{ref}}}, \Delta R \right\rangle
\end{equation*}
\begin{equation*}
\text{where}\quad \mathcal{R}_{\Delta}:=\left\{\Delta R \in \mathbb{R}^{\mathcal{Y}_{\textbf{ref}}} \mid \left|\Delta R(y)-\Delta R\left(y^{\prime}\right)\right| \leq C\left(y, y^{\prime}\right) \quad \forall y, y^{\prime} \in \mathcal{Y}_{\textnormal{\textbf{ref}}}\right\}
\end{equation*}
\end{theorem}
$\Delta R \in L^1(\pi_{\textbf{ref}})$, $L^1(\pi_{\textbf{ref}})=\left\{f:  \mathcal{Y} \rightarrow \mathbb{R}| \sum_{y \in \mathcal{Y}_{\textbf{ref}}}|f(y)|  \pi_{\textbf{ref}}(y)<\infty\right\}$. 


The derivation of this equation and the detailed analysis of the perturbation reward range are presented in \cref{appendix:wd-thoery}.

\paragraph{Intuition}

This expression represents an optimization problem involving strategies $\pi$ and perturbation $\Delta R$. The goal is to find the optimal strategy $\pi^*$ under the modified reward $R^\prime$ ($= R-\beta \Delta R$). Furthermore, when the cost function $C(y,y^\prime)$ is small, indicating that the outputs $y$ and $y^\prime$ are similar, the corresponding perturbations must also be similar in value.
