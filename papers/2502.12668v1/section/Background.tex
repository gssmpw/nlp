\newcolumntype{C}[1]{>{\centering\arraybackslash}m{#1}}
\section{Background}

% In this paper, we formalize the problem of decoding time alignment as a regularized reinforcement learning problem \citep{neu2017unified,pmlr-v97-geist19a, NEURIPS2019_3f4366ae}. Section \ref{pre:rl} describes the Reinforcement Learning (RL) problem and the Regularized Reinforcement Learning (RRL) problem. RRL is a variant of the RL problem with a noise to the observation of the reward function. 
% Then, we describe two rejection sampling algorithms used for decoding time alignment, Best-of-N (BoN) sampling in Section \ref{pre:bon}, and the Regularized Best-of-N (RBoN) sampling in Section \ref{pre:rbon}.
% We draw a connection between the RL problems and the strategies: BoN sampling corresponds to solving the RL problem, and RBoN sampling strategies correspond to solving the RRL problem. 
% We let $\Delta(\mathcal{X})$ denote the set of probability distributions in the set $\mathcal{X}$.

In this paper, we formalize the problem of decoding time alignment as Regularized Markov Decision Processes (MDPs) problem \citep{neu2017unified,pmlr-v97-geist19a, NEURIPS2019_3f4366ae, NEURIPS2021_bb1443cc}. For brevity, we refer to reinforcement learning within Regularized MDPs as Regularized Reinforcement Learning (RRL) throughout this paper. In Section \ref{pre:rl}, we describe the Reinforcement Learning (RL) problem and the RRL problem. Then, we describe two sampling algorithms used for decoding time alignment, Best-of-N (BoN) sampling in Section \ref{pre:bon}, and the Regularized Best-of-N sampling (RBoN) in Section \ref{sec:rbon}.


\subsection{Adversarial Interpretation in Regularized Reinforcement Learning}\label{pre:rl}

We consider the problem of selecting an output $y$ from a set of outputs $\mathcal{Y}_{\textbf{ref}} \subseteq \mathcal{Y}$ (e.g., response text from the system) given an input $x \in \mathcal{X}$ (e.g., input prompt by a user), where the objective is to select the best output according to a reward function $R$: $\mathcal{X} \times \mathcal{Y}_{\textbf{ref}}  \rightarrow \mathbb{R}$.
Let $\Delta(\mathcal{Y})$ denote the set of probability distributions over a set $\mathcal{Y}$.
We define the goal of the Reinforcement Learning (RL) problem as finding the best policy $\pi: \mathcal{X} \rightarrow \Delta(\mathcal{Y}_{\textbf{ref}})$ that maximizes the expected reward for $x$:
\begin{equation}
\begin{aligned}
% \textbf{Objective Function of RL}&=\max _{\pi}\,\,\langle\pi, R\rangle,\\
\argmax_{\pi \in \Pi} f_\textrm{RL}(\pi) &=\argmax_{\pi \in \Pi}\sum_{y \in \mathcal{Y}_{\textbf{ref}}} \pi(y \mid x)R(x,y)\\
    % &=\argmax_{\pi \in \Pi}\langle\pi, R\rangle, \label{eq:rl}\\
    &=\argmax_{\pi \in \Pi} \mathbb{E}_{y \sim \pi(\cdot \mid x)}[R(x, y)]\label{eq:rl}\\
% f_\textrm{RL}(\pi) :&=\sum_{y \in \mathcal{Y}_{\textbf{ref}}} \pi(y \mid x)R(x,y)\\
%     &=\langle\pi, R\rangle, \label{eq:rl}\\
\end{aligned}
\end{equation}
% \yuki{mu ->policy}
% \yuu{TODO: Technically this is not an inner project. Would it be easier to understand if we use the expectation notation? The expected reward is what RL people and NLP people would understand immediately.}
where $\Pi$ is a set of all possible policies.
Note that there exists a deterministic policy that maximizes $f_\mathrm{RL}$ \citep{sutton2018reinforcement} and this formulation can be seen as a contextual bandit problem. Specifically, the policy observes a context $x$, chooses an output $y$ based on that context, and receives a reward $R(x, y)$. Importantly, we do not use sequential decision operations or consider state transitions. Each decision is made independently based on the current context $x$.
% In this paper, we consider the problem of inferring $y$ for a given fixed $x$. \yuki{}For the sake of simplicity, we abbreviate the notation of $x$ from the functions as $\pi(y) := \pi(y \mid x)$ and $R(y) := R(x, y)$ when not confusing.

% and $N$ is the number of possible outputs: $N := |\mathcal{Y}_{\textbf{ref}}|$.
% where $\langle \pi, R \rangle = \sum_{\mathcal{Y}_{\textbf{ref}}} \pi(y \mid x)R(x,y)$, probability over possible behaviors $\pi$: $\mathcal{X}$ $\rightarrow$ $\Delta(\mathcal{Y})$, $\mathcal{X}$ and $\mathcal{Y}$ represent the input and output space, which generates a set of responses $N$, denoted $\mathcal{Y}_{\textbf{ref}}$. 

The underlying assumption of the RL problem is that the reward model $R$ is correctly defined and observable. That is, we consider the solution that maximizes the expected reward to be the optimal solution. 
However, real-world applications often suffer from the \textit{reward misspecification problem} -- the reward model observable to the agent is only a proxy for the true underlying reward of the problem \citep{ortega2014adversarial,husain2021regularized,NEURIPS2021_bb1443cc,eysenbach2022maximum,pan2022the,NEURIPS2021_bb1443cc}.
Prior work has investigated strategies to optimize under the uncertainty in the observed reward.
In contrast to the RL problem, Regularized Reinforcement Learning (RRL) incorporates regularization terms to achieve a solution that is robust to the reward misspecification \citep{neu2017unified,pmlr-v97-geist19a, NEURIPS2019_3f4366ae,NEURIPS2021_bb1443cc}. 
The objective of the RRL problem is to find the best policy $\pi: \mathcal{X} \rightarrow \Delta(\mathcal{Y}_\textbf{ref})$ that maximizes the reward with an additional regularization function $\Omega (\pi): \Delta(\mathcal{Y}_\textbf{ref}) \rightarrow \mathbb{R} \cup\{+\infty\} $. Let $f_\textrm{RRL}^{\Omega}(\pi) := \mathbb{E}_{y \sim \pi(\cdot \mid x)}[R (x,y)]- \Omega(\pi)$ be the objective function of RRL problem with $\Omega$. Then, we define the following as the optimal solution to the RRL problem:
\begin{equation}
% \argmax_{\pi \in \Pi} f_\textrm{RRL}^{\Omega}(\pi)=\argmax_{\pi \in \Pi}\langle\pi, R\rangle- \Omega(\pi). \label{eq:rrl}  
\argmax_{\pi \in \Pi} f_\textrm{RRL}^{\Omega}(\pi)=\argmax_{\pi \in \Pi}\mathbb{E}_{y \sim \pi(\cdot \mid x)}[R (x,y)] - \Omega(\pi). \label{eq:rrl} 
\end{equation}
% Note that the optimal solution is not necessarily a deterministic policy.
Note that, unlike the RL problem, there may not exist an optimal policy that is deterministic for the RRL problem \citep{pmlr-v97-geist19a}. %\yuu{Cite if there is some prior work that mentions it}. \yuki{\cite{pmlr-v97-geist19a} refer to With regularization, policies will be more stochastic than in classical approximate DP (that tends to produce deterministic policies).}
% This work represents an important extension of the standard framework. 
% Several key studies have illuminated the benefits of incorporating regularization terms in reinforcement learning algorithms. Specifically, \citet{haarnoja2018soft} demonstrated that adding a regularization term can promote efficient exploration. \yuu{Exploration is out of the scope of the paper}
% Furthermore, \citet{ahmed2019understanding} showed that such regularization leads to smoother optimization of the policy $\pi$. \yuu{Smoothness is also out of scope as we are only interested in the inference-time algorithm}
% It is also worth noting that in certain Constrained MDPs scenarios, stochastic policy has been proven to be optimal \citep{Szepesvari2020constrained}. Building on this insight, recent research has explored the use of regularization terms to address challenges  \citep{ding2024last}.
% Furthermore, other studies propose a novel perspective on regularization in RL: the incorporation of regularization terms is expected to yield robust algorithms \citep{ortega2014adversarial,husain2021regularized,NEURIPS2021_bb1443cc,eysenbach2022maximum}.

% \yuu{TODO: Where should we put this information?}
% Existing work has investigated the use of shannon entropy, KL divergence, or a combination of both \citep{vieillard2020leverage} as the regularization term $\Omega$.

% \yuu{Need a bit more explanation on why regularized RL is important and why we should do it. Let's first claim that regularized RL is what we have to do. Then, the subsequent analysis will be of interest.}

\citet{brekelmans2022your} uses Legendreâ€“Fenchel transformation \citep{touchette2005legendre} to show that the RRL problem can be viewed as a variant of the RL problem with an adversarial agent adding perturbations to the reward $\Delta R: \mathcal{X} \times \mathcal{Y}_{\textbf{ref}}  \rightarrow \mathbb{R}$ if the regularization term $\Omega$ is convex and lower semi-continuous function \citep{boyd2004convex}:

\begin{equation}
\begin{aligned}
\argmax_{\pi \in \Pi} f_\textrm{RRL}^\Omega(\pi)%&=\langle\pi, R\rangle- \Omega(\pi)\\
% &=\argmax_{\pi \in \Pi} \min _{\Delta R \in \mathcal{R}_{\Delta}}\,\,\langle\pi, R-\Delta R\rangle+\Omega^{*}(\Delta R), \label{eq:rrl-dual}\\
&=\argmax_{\pi \in \Pi} \min _{\Delta R \in \mathcal{R}_{\Delta}}\mathbb{E}_{y \sim \pi(\cdot \mid x)}[R(x,y)-\Delta R(x,y)]+\Omega^{*}(\Delta R), \label{eq:rrl-dual}\\
% \textbf{Objective Function of RRL}&=\max _{\pi}\,\, \langle\pi, R\rangle- \Omega(\pi)\\
% &=\max _{\pi} \,\,\min _{\Delta R}\,\,\langle\pi, R-\Delta R\rangle+\Omega^{*}(\Delta R),
\end{aligned}
\end{equation}
where $\Omega^* \in \mathbb{R}^{\mathcal{X} \times\mathcal{Y}_{\textbf{ref}}}$ is the conjugate function of $\Omega$ \citep{boyd2004convex}, and $\mathcal{R}_{\Delta}:=\left\{\Delta R \in \mathbb{R}^{\mathcal{X} \times \mathcal{Y}_{\textbf{ref}}} \mid F_{\Omega}(\Delta R) \leq 0\right\}$, where $F_{\Omega}$ is a function or operator dependent to $\Omega$ that imposes a constraint or condition on the values of $\Delta R$. 
% \yuki{}For simplicity we abbreviate $x$ from the $\Delta R$ and write it as $\Delta R \in \mathbb{R}^{\mathcal{Y}_{\textbf{ref}}}$ if not confusing.
% $\mathcal{R}_{\Delta}:=\left\{\Delta R \in \mathbb{R}^{\mathcal{X} \times \mathcal{Y}_{\textbf{ref}}} \mid F_{\Omega}(\Delta R) \leq 0\right\}$


% where perturbation reward function $\Delta R: \mathcal{X} \times \mathcal{Y}  \rightarrow \mathbb{R}$, convex and lower continuous function (regularized function) $\Omega (\pi): \mathcal{Y} \rightarrow \mathbb{R} \cup\{+\infty\} $, and $\Omega^*$ is the conjugate function of $\Omega$ \yuu{TODO: It's better to cite some textbook (e.g., Boyd's convex optimization?) on using a technical term which may be unfamiliar to the readers.}. 




% To understand the conjugate function, \cref{fig:conjugate} illustrates the relationship between the convex function and its conjugate counterpart.

Eq. (\ref{eq:rrl-dual}) shows that the problem of maximizing $f_\textrm{RRL}$ can be reformulated as the max-min problem and the regularizer $\Omega$ is effectively an adversarial reward perturbation that forces us to optimize the worst case performance \citep{NEURIPS2021_bb1443cc}.

% \textbf{This equation implies that the objective function of RRL can be reformulated as the max-min problem and the regularizer for $\pi$ is an adversarial reward perturbation in the
% worst case.}

% \subsection{Best-of-N (BoN) Sampling ($\approx$ \textbf{Objective Function of RL})}
\subsection{Best-of-N (BoN) Sampling}
\label{pre:bon}

% BoN sampling has emerged as an effective methodology for preference optimization in Large Language Models (LLMs) \citep{stiennon2020,nakano2022webgpt}. BoN sampling presents several advantages over preference learning methodologies. Firstly, it is uncomplicated and does not necessitate additional training in the language model. Although learning-based alignment methods require retraining the LLMs whenever human preferences are updated, BoN sampling can be applied instantaneously, only necessitating an update of the reward model. This is particularly advantageous, as LLMs training is the most resource-intensive process. Secondly, BoN sampling is an effective strategy in its own right, with numerous studies demonstrating that it can surpass the performance of learning-based alignment methods \citep{pmlr-v202-gao23h}. Recent literature has expanded our understanding of BoN sampling. Notably, \citet{beirami2024theoretical} conducted an analysis comparing the policies selected by BoN sampling with the base policies used for sample generation. Furthermore, \citet{gui2024bonbon} demonstrated that BoN sampling achieves an optimal balance between win rate and KL divergence when aligning large language models to human preferences. 
BoN sampling has emerged as an effective method for preference optimization in LLMs \citep{stiennon2020,nakano2022webgpt}. BoN sampling has several advantages over preference learning methods. First, it is straightforward and does not require additional training in the language model. Although learning-based alignment methods require retraining the LLMs whenever human preferences are updated, BoN sampling can be applied immediately, requiring only an update of the reward model. This is particularly advantageous since training LLMs is the most resource-intensive process. Second, BoN sampling is an effective strategy in its own right, with numerous studies demonstrating that it can outperform learning-based adaptation methods \citep{pmlr-v202-gao23h}. Recent literature has expanded our understanding of BoN sampling. In particular, \citet{beirami2024theoretical} conducted an analysis comparing the policies selected by BoN sampling with the base policies used for sample generation. In addition, \citet{gui2024bonbon} showed that BoN sampling achieves an optimal balance between win rate and KL divergence when aligning large language models to human preferences. 

BoN sampling has similarities to the objective function used in RL (e.g., the response with the highest reward score, determined by a proxy reward model $R(x,y)$, is selected). The objective function of BoN is given by:

% \yuu{TODO: Should we describe the output of BoN as a deterministic policy rather than y? It would make the alignment with the SRBoN easier, but not an intuitive explanation of itself.}
\begin{equation*}
y_{\mathrm{BoN}}(x) := \underset{y \in \mathcal{Y}_{\textbf{ref}}}{\argmax}\, R(x, y).
% \pi_{\mathrm{BoN}} := \underset{\pi \in \Pi_d}{\argmax}\, R(x, y).
\end{equation*}
This reward model is used as a measure of the quality of the text when making an assignment. The reward model we consider in this paper is open access as described in Appendix~\ref{appendix:reprod}.
We also mention that the objective function of BoN sampling is equal to the objective function of the (unregularized) RL problem (Eq. (\ref{eq:rl})):

\begin{equation}\label{eq:bon}
\begin{aligned}
y_{\mathrm{BoN}}(x) :&= \underset{y \in \mathcal{Y}_{\textbf{ref}}}{\argmax}\, R(x, y) \\
    % &= \underset{y \in \mathcal{Y}_{\textbf{ref}}}{\argmax}\max_{\pi_y \in \Pi_{det}} \langle \pi_y, R \rangle \\
     &= \underset{y \in \mathcal{Y}_{\textbf{ref}}}{\argmax}\max_{\pi_y \in \Pi_{det}} \mathbb{E}_{y \sim \pi_y}[R(x,y)] \\
    &= \underset{y \in \mathcal{Y}_{\textbf{ref}}}{\argmax}\max_{\pi_y} f_\mathrm{RL}(\pi_y).
% \textbf{Objective Function of BoN} &= \max_{\pi} \,\, \langle \pi, R \rangle.
\end{aligned}
\end{equation}
where $\Pi_{det}$ is a set of deterministic policies and $\pi_y$ is a deterministic policy that selects $y$ given $x$ with a probability of 1.
% where $\langle \pi, R \rangle = \sum_{y \in \mathcal{Y_{\textbf{ref}}}} \pi(y)R(y)$, reward function $R:\mathcal{Y}  \rightarrow \mathbb{R}$, output probability $\pi$ $\in$ $ \Delta (\mathcal{Y})$.

% BoN sampling presents several advantages over preference learning methodologies. Firstly, it is uncomplicated and does not necessitate additional training in the language model. Although learning-based alignment methods require retraining the LLMs whenever human preferences are updated, BoN can be applied instantaneously, only necessitating an update of the reward model. This is particularly advantageous, as LLMs training is the most resource-intensive process. Secondly, BoN is an effective strategy in its own right, with numerous studies demonstrating that it can surpass the performance of learning-based alignment methods \citep{pmlr-v202-gao23h}. 

% \subsection{Regularized Best-of-N (RBoN) Sampling ($\approx$ \textbf{Objective Function of RRL})}\label{sec:rbon}
\subsection{Regularized Best-of-N Sampling ($\mathrm{RBoN}$)}\label{sec:rbon}

% Although BoN sampling is shown to be effective, it is prone to the reward hacking problem \citep{amodei2016concrete, ziegler2020finetuning, stiennon2020, NEURIPS2022_3d719fee,pmlr-v202-gao23h}. The reward hacking problem is a phenomenon in which the decision is made to optimize the proxy reward without consideration of its potential misspecification, resulting in worse performance in the actual reward objective.
% \citet{NEURIPS2023_5fc47800} showed that with a 25\% of label noise which is the amount of disagreement observed in real-world preference annotations \citep{stiennon2020,NEURIPS2022_b1efde53}, BoN sampling decreases in performance with $N$ larger than 16 (Figures 12 and 13 in \citealt{NEURIPS2023_5fc47800}).

Although BoN sampling is shown to be effective, it is prone to the reward hacking problem \citep{amodei2016concrete, ziegler2020finetuning, stiennon2020, NEURIPS2022_3d719fee,pmlr-v202-gao23h}. 
The reward hacking problem is a phenomenon where the decision to optimize the proxy reward is made without considering its potential misspecification, resulting in worse performance on the actual reward objective.
\citet{NEURIPS2023_5fc47800} showed that with 25\% label noise, which is the amount of disagreement observed in real-world preference annotations \citep{stiennon2020,NEURIPS2022_b1efde53}, BoN sampling degrades performance with $N$ greater than 16 (Figures 12 and 13 in \citealt{NEURIPS2023_5fc47800}).
% That is, BoN sampling tends to the proxy reward especially when the proxy reward model is misspecified from the .

Regularized Best-of-N sampling (RBoN) is proposed to mitigate the reward hacking problem for BoN sampling \citep{jinnai2024regularized}. 
\citet{jinnai2024regularized} presented two variants of RBoN: using the KL divergence as a regularizer ($\mathrm{RBoN}_{\mathrm{KL}}$; Section \ref{pre:rbon}) and using the Wasserstein Distance as a regularizer ($\mathrm{RBoN}_{\mathrm{WD}}$; Section~\ref{WD}).
In the following, we describe the two variants of RBoN and draw its connection to the objective function of RRL (Eq. (\ref{eq:rrl})).

\subsubsection{KL divergence Regularized BoN Sampling ($\mathrm{RBoN}_{\mathrm{KL}}$)}
\label{pre:rbon}
The objective function of $\mathrm{RBoN}_{\mathrm{KL}}$ (KL divergence Regularized BoN Sampling) is given by:
\begin{equation*}
\begin{aligned}
% y_{\mathrm{KLBoN}}(x) &=\argmax_{y \in \mathcal{Y_{\textbf{ref}}}} \, R(x, y)-\beta \KL\left[\mathbbm{1}_y (\cdot \mid x) \| \pi_{\textbf{ref}}(\cdot \mid x)\right],\\
% y_{\mathrm{KLBoN}}(x) &=\argmax_{y \in \mathcal{Y_{\textbf{ref}}}} \, R(x, y)-\beta \KL\left[\pi_y \| \pi_{\textbf{ref}}(\cdot \mid x)\right],\\
y_{\mathrm{KLBoN}}(x) &=\argmax_{y \in \mathcal{Y_{\textbf{ref}}}} \, R(x, y)-\beta \KL\left[\pi_y (\cdot \mid x) \| \pi_{\textbf{ref}}(\cdot \mid x)\right],\\
&= \underset{y \in \mathcal{Y}_{\textbf{ref}}}{\argmax}\max_{\pi_y \in \Pi_{det}} f_\mathrm{RRL}^{\mathrm{KL}}(\pi_y).
\end{aligned}
\end{equation*}

% \begin{equation}
% y_{\mathrm{KLBoN}}(x) =\argmax_{y \in \mathcal{Y_{\textbf{ref}}}}  R(x, y)-\beta \E[\textbf{KL}\left[\pi(y \mid x)\| \pi_{\textbf{ref}}(y \mid x)\right]], \quad {\mathcal{Y}_\textbf{ref} \in \mathcal{Y}_{all}}
% \end{equation}

where $\beta$ is a regularization parameter, reference policy $\pi_{\textbf{ref}}$: $\mathcal{X}$ $\rightarrow$ $\Delta(\mathcal{Y}_{\textbf{ref}})$, and $\KL$ denotes the KL divergence. The reference policy here takes an input x and returns a meaningful output y. The regularization described in this paper is like a penalty to stay away from the reference policy and the reference policy is the language model.
% In the equation, $\mathbbm{1}_y \in \mathbb{R}^{|\mathcal{Y}|}$ is an indicator vector. The indicator vector $\mathbbm{1}_y$ is defined as:
% \begin{equation*}
% \mathbbm{1}_y(y') = \begin{cases}
% 1, & \text{if } y' = y \\
% 0, & \text{otherwise}
% \end{cases}
% \end{equation*}
% This represents a deterministic policy that selects action $y$ with a probability of 1. 

By incorporating the KL divergence as a regularization term in the objective function, $\mathrm{RBoN}_{\mathrm{KL}}$ encourages the learned policy to be close to the reference policy $\pi_{\textbf{ref}}$. A higher value of $\beta$ emphasizes the regularization term, encouraging the learned policy to be closer to the reference policy, while a lower value of $\beta$ prioritizes maximizing the reward function.


\subsubsection{Wasserstein Distance Regularized BoN Sampling ($\mathrm{RBoN}_{\mathrm{WD}}$)}\label{WD}

The objective function of $\mathrm{RBoN}_{\mathrm{WD}}$ (Wasserstein Distance Regularized BoN Sampling) is defined as follows:
\begin{equation*}
\begin{aligned}
% y_{\mathrm{WDBoN}}(x) &= \argmax_{y \in \mathcal{Y_{\textbf{ref}}}} \,R(x, y)-\beta \textbf{WD}\left[\mathbbm{1}_y (\cdot \mid x)  \| \pi_{\textbf{ref }}(\cdot \mid x)\right],\\
% y_{\mathrm{WDBoN}}(x) &= \argmax_{y \in \mathcal{Y_{\textbf{ref}}}} \,R(x, y)-\beta \textbf{WD}\left[\pi_y \| \pi_{\textbf{ref }}(\cdot \mid x)\right],\\
y_{\mathrm{WDBoN}}(x) &= \argmax_{y \in \mathcal{Y_{\textbf{ref}}}} \,R(x, y)-\beta \textbf{WD}\left[\pi_y(\cdot \mid x) \| \pi_{\textbf{ref }}(\cdot \mid x)\right],\\
&= \underset{y \in \mathcal{Y}_{\textbf{ref}}}{\argmax}\max_{\pi_y \in \Pi_{det}} f_\mathrm{RRL}^{\mathrm{WD}}(\pi_y).
\end{aligned}
\end{equation*}
where $\textbf{WD}$ denotes 1-Wasserstein Distance.

% To simplify the notation, we introduce the following notation. Let $y_1, y_2, \cdots, y_n$ be $n$ places and consider the function $f$, where $f_i$ refers to the value $f(y_i)$.

The $\textbf{WD}$ \citep{wang2012coupling} is defined as:
\begin{equation}
\textbf{WD}[\nu \| \mu] = \inf_{\gamma \in \Gamma(\nu, \mu)} \sum_{(i, j) \in N \times N} \gamma_{ij} \, C_{ij},
\end{equation}
where $N$: the total number of samples, consisting of the set $\{ y_1, y_2, \dots, y_N \}$, $\nu, \mu \in \Delta(N)$: probability measure on the aforementioned sets ($\nu_i, \mu_i$ refer to the probability value $\nu(y_i), \mu(y_i)$), $C$: $N\times N \rightarrow \mathbb{R}$ a cost function measuring the distance between two outputs (e.g. $C_{ij}$ refers to the amount to
be transported from place $y_i$ to palace $y_j$), and $\Gamma(\nu, \mu)$ denotes the set of all joint distributions $\gamma$ whose marginals are $\nu$ and $\mu$. The constraints on $\gamma$ are given by:
\begin{equation*}
\begin{aligned}
\sum_{j \in n} \gamma_{ij} &= \nu_i, \quad \forall i \in n, \\
\sum_{i \in n} \gamma_{ij} &= \mu_j, \quad \forall j \in n, \\
\gamma_{ij} &\geq 0, \quad \forall i,j \in n.
\end{aligned}
\end{equation*}
The $\textbf{WD}$, also known as the Earth Mover's Distance (EMD), is a metric used to quantify the dissimilarity between two probability distributions. Intuitively, it measures the minimum cost required to transform one distribution into the other. This cost is conceptualized as the amount of probability mass that must be moved multiplied by the distance that would be moved. 
The concept has been used in NLP to measure the dissimilarity of texts \citep{pmlr-v37-kusnerb15,zhao-etal-2019-moverscore}.
% In the field of NLP, the concept has also been used for a long time \citep{pmlr-v37-kusnerb15}.

The exact computation of $\textbf{WD}\left[\pi_y(\cdot \mid x) \| \pi_{\textbf{ref }}(\cdot \mid x)\right]$ is intractable due to the enormous size of the output space. To address this computational challenge, prior work \citep{jinnai2024regularized} has employed sample-based approximation techniques.
% \yuu{We want to clarify that 1-WD is not computable for LLM as $\mathcal{Y}$ is enormously huge. That's why the prior work deploys the sample approxilation.}
% In particular, \cite{jinnai2024regularized} provided a practical reformulation of \textbf{WD}:
% In \cite{jinnai2024regularized}, the objective function of $\mathrm{RBoN}_{\mathrm{WD}}$  has been rewritten from a practical perspective by making the following modifications:
Let $\hat{\pi}_{\textbf{ref}}$ represent the empirical distribution computed using a set of samples $\mathcal{Y}_{\textbf{ref}}$. This distribution is defined as:
$\hat{\pi}_{\textbf{ref}}(y \mid x) = \frac{1}{N} \sum_{y^{\prime} \in \mathcal{Y}_{\textbf{ref}}} \mathbb{I}\left(y = y^{\prime}\right)$
where $N$ is the total number of samples in $\mathcal{Y_{\textbf{ref}}}$.
The objective function can then be approximated as follows:
\begin{equation*}
% y_{\mathrm{WDBoN}}(x) = \argmax_{y \in \mathcal{Y_{\textbf{ref}}}} \,R(x, y)-\beta \textbf{WD}\left[\pi_y \| \hat{\pi}_{\textbf{ref }}(\cdot \mid x)\right].
y_{\mathrm{WDBoN}}(x) = \argmax_{y \in \mathcal{Y_{\textbf{ref}}}} \,R(x, y)-\beta \textbf{WD}\left[\pi_y(\cdot \mid x) \| \hat{\pi}_{\textbf{ref }}(\cdot \mid x)\right].
\end{equation*}
For practical implementation aspects, the $\textbf{WD}$ term for \citet{jinnai2024regularized} is computed as follows:
% \begin{equation}\label{eq:wd_N}
% \textbf{WD}\left[\pi_y \| \hat{\pi}_{\textbf{ref}}(\cdot \mid x)\right]=\sum_{y^{\prime} \in \mathcal{Y}_{\textbf {ref}}} \frac{1}{N} C\left(y, y^{\prime}\right),
% \end{equation}
\begin{equation}\label{eq:wd_N}
\textbf{WD}\left[\pi_y(\cdot \mid x) \| \hat{\pi}_{\textbf{ref}}(\cdot \mid x)\right]=\sum_{y^{\prime} \in \mathcal{Y}_{\textbf {ref}}} \frac{1}{N} C\left(y, y^{\prime}\right),
\end{equation}
% where $C\left(y, y^{\prime}\right)$ is the cost function. :
The cosine distance is used as $C$ to measure the distance between the outputs \citep{reimers-gurevych-2019-sentence}.
\begin{equation}\label{eq:similarity}
C\left(y, y^{\prime}\right)=1-\cos \left(\mathrm{emb}(y), \operatorname{emb}\left(y^{\prime}\right)\right),
\end{equation}
where $\mathrm{emb}(y)$ and $\operatorname{emb}\left(y^{\prime}\right)$ represent the embeddings of output $y$ and $y^{\prime}$, respectively. 
% While we have focused on the techniques mentioned above, it is important to note that there are other decoder methods (\cref{tab:decoder}).
% For simplicity, we shall denote $1-\cos \left(\mathrm{emb}(y), \operatorname{emb}\left(y^{\prime}\right)\right) = 1-\cos (y, y^{\prime})$




% \subsection{Minimum Bayes Risk Decoding (MBR)}
% Minimum Bayes Risk (MBR) decoding is based on maximizing expected utility \citep{Berger:1327974}. A utility function $u(y, h)$ quantifies the benefit of choosing $h \in \mathcal{Y}_{\textbf{ref}}$ when $y \in \mathcal{Y}_{\textbf{ref}}$ is the ideal choice. In real-world scenarios, the ideal translation is unknown, forcing decision-making under uncertainty. MBR addresses this by allowing the model to probabilistically fill in ideal decisions while searching through candidate space for the option with the highest expected utility:
% \begin{equation*}
% y^{\mathrm{MBR}}=\underset{h \in \mathcal{Y}_{\textbf{ref}} }{\arg \max } \mathbb{E}[u(Y, h) \mid  x].
% \end{equation*}

% \begin{longtable}{ |p{3cm}|p{3cm}|p{3cm}|p{6cm}|  }
% \caption{Description of decoder method.} \\
%   \hline
%   Method & Reward Function & Utility Function & Description \\
%   \hline
%   Best of N & Yes & No & N outputs with reward function, and then choose the best. \\
%   \hline
%   Best of 1 & Yes & No & Use only one output. \\ 
%   \hline
%   MBR & No & Yes & N outputs with the expected utility function, and then choose the best. \\
%   \hline
%   KL-BoN & Yes & Yes & Maximize the mixture of the reward function and KL divergence. \\
%   \hline
%   WD-BoN & Yes & Yes & Maximize the mixture of the reward function and WD Distance. \\
%   \hline
% \end{longtable}


% \begin{longtable}{ |C{3cm}|C{3cm}|C{3cm}|C{5cm}|  }
% \caption{Description of decoder method.} \\
%   \hline
%   \textbf{Method} & \textbf{Reward Function} & \textbf{Utility Function} & \textbf{Description} \\
%   \hline
%   Best of N & Yes & No & N outputs with reward function, and then choose the best. \\
%   \hline
%   Best of 1 & Yes & No & Use only one output which is chosen by reference model. \\ 
%   \hline
%   MBR & No & Yes & N outputs with the expected utility function, and then choose the best. \\
%   \hline
%   $\mathrm{RBoN}_{\mathrm{KL}}$ & Yes & Yes & Maximize the mixture of the reward function and KL divergence. \\
%   \hline
%   $\mathrm{RBoN}_{\mathrm{WD}}$ & Yes & Yes & Maximize the mixture of the reward function and WD Distance. \\
%   \hline
% \end{longtable}

% % \begin{longtable}{ |C{3cm}|C{3cm}|C{3cm}|C{5cm}| }
% \begin{longtable}{ C{3cm}C{2cm}C{2cm}m{7cm} }
% \caption{Description of Decoder Method. A checkmark (\checkmark) indicates that the method uses the specified function, while a blank space means that it does not.}\label{tab:decoder}\\
%   \toprule
%   \textbf{Method} & \textbf{Reward Function} & \textbf{Utility Function} & \textbf{Description} \\
%   \midrule
%   Random sampling &  &  & Use an output randomly sampled by the reference model.  \\ 
%     \hline
%   Best-of-N (BoN) \citep{stiennon2020} & \checkmark &  & Generate N outputs, evaluate with reward function, select the best.  \\
%   \hline
%   MBR \citep{eikema-aziz-2022-sampling} &  & \checkmark & Generate N outputs, evaluate with expected utility function, select the best. (Details in \cref{sec:exp})\\
% \hline
%   $\mathrm{RBoN}_{\mathrm{KL}}$ \citep{jinnai2024regularized} & \checkmark &  & Maximize the mixture of the reward function and KL divergence with a constraint that the resulting policy is deterministic. \\
%   \hline
%   $\mathrm{RBoN}_{\mathrm{WD}}$ \citep{jinnai2024regularized} & \checkmark & \checkmark & Maximize the mixture of the reward function and WD distance with a constraint that the resulting policy is deterministic. \\
%   \hline
%   \textbf{$\mathrm{SRBoN}_{\mathrm{KL}}$ (Section~\ref{propose:kl})} & \checkmark &  & Maximize the mixture of the reward function and KL divergence. \\
%   \hline
%   \textbf{$\mathrm{SRBoN}_{\mathrm{WD}}$ (Section~\ref{propose:WD})} & \checkmark & \checkmark & Maximize the mixture of the reward function and WD distance. \\
%   \hline
%   \textbf{$\mathrm{RBoN}_{\mathrm{L}}$ (Section~\ref{sec:exp})}& \checkmark &  & Consider both the reward function and the length of the sentence. (Details in \cref{sec:exp} and \cref{appendix:length})\\
%   \bottomrule
% \end{longtable}

% \subsection{Adversarial Interpretation in Regularized Reinforcement Learning}

% The objective function of reinforcement learning (RL) is 
% \begin{equation*}
% \begin{aligned}
% \textbf{Objective Function of RL}&=\max _{\mu}\langle\mu, R\rangle\\
% \end{aligned}
% \end{equation*}
% \yuki{mu ->policy}

% where $\langle \mu, R \rangle = \sum \mu(y \mid x), R(x,y)$, reward function $R$ : $\mathcal{X} \times \mathcal{Y}  \rightarrow \mathbb{R}^{\mathcal{X}\times \mathcal{Y}}$, probability over possible behaviors $\mu$ $\in$ $[0,1]^{\mathcal{X} \times \mathcal{Y}}$. 


% In contrast to the usual reinforcement learning objective function discussed above, regularized reinforcement learning (RRL) algorithms incorporate regularization terms \citep{pmlr-v97-geist19a}. These regularized approaches represent an important extension to the standard framework.
% Some studies propose a novel perspective on regularization in reinforcement learning: the incorporation of regularization terms is expected to yield robust algorithms \citep{ortega2014adversarial,husain2021regularized,NEURIPS2021_bb1443cc,eysenbach2022maximum}. These approaches represent a significant shift in how we conceptualize the role of regularization in reinforcement learning.

% A recent study \citep{brekelmans2022your} suggests that the inclusion of a regularized function can be viewed as an adversarial agent adding perturbations to the rewards. 


% \begin{equation*}
% \begin{aligned}
% \textbf{Objective Function of RRL}&=\max _{\mu}\langle\mu, R\rangle- \Omega(\mu)\\
% &=\max _{\mu} \min _{\Delta R \in \mathbb{R}^{\mathcal{X} \times \mathcal{Y}}}\langle\mu, R-\Delta R\rangle+\Omega^{*}(\Delta R) .
% \end{aligned}
% \end{equation*}

% where perturbation reward function $\Delta R : \mathcal{X} \times \mathcal{Y}  \rightarrow \mathbb{R}^{\mathcal{X}\times \mathcal{Y}}$, convex and lower continuous function $\Omega (\mu) : \mathcal{Y} \rightarrow \mathbb{R} \cup\{+\infty\} $  , $\Omega^*$ : conjugate function. 

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.6\linewidth]{img/conjugate.pdf}
%     \caption{This figure illustrates the relationship between a convex function $\Omega$ and its associated conjugate function $\Omega^*$. Conjugate function $\Omega^*$ is defined as the Legendre transform of the convex function $\Omega$. It is important to note that there can be multiple conjugate functions $\Omega^*$ corresponding to different coefficients. This multiplicity arises because the conjugate function $\Omega^*$ captures the maximum difference between the linear approximation of $\Omega$ and the function itself, and this relationship can vary with different linear approximations. 
%     }
%     \label{fig:conjugate}
% \end{figure}

% \textbf{This equation implies that the objective function of RRL can be reformulated max-min problem and the policy regularizer is an adversarial reward perturbation in the
% worst case.}
