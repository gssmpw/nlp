% % \begin{longtable}{ |C{3cm}|C{3cm}|C{3cm}|C{5cm}| }
% \begin{longtable}[tb]{ C{3cm}C{2cm}C{2cm}m{7cm} }
% \caption{Description of the text generation algorithms evaluated in the experiments. A checkmark (\checkmark) indicates that the method uses the specified function, while a blank space means that it does not. \yuu{TODO: The Table should be put in a single page: I would suggest using a normal table environment rather than a longtable.}}\label{tab:decoder}\\
%   \toprule
%   \textbf{Method} & \textbf{Reward Function} & \textbf{Similarity Function} & \textbf{Description} \\
%   \midrule
%   Random sampling &  &  & Use an output randomly sampled by the reference model.  \\ 
%     \hline
%   Best-of-N (BoN) \citep{stiennon2020} & \checkmark &  & Generate N outputs, evaluate with reward function, select the best.  \\
%   \hline
%   MBR \citep{eikema-aziz-2022-sampling} &  & \checkmark & Generate N outputs, evaluate with expected utility function, select the best. (Details in \cref{sec:exp})\\
% \hline
%   $\mathrm{RBoN}_{\mathrm{KL}}$ \citep{jinnai2024regularized} & \checkmark &  & Maximize the mixture of the reward function and KL divergence with a constraint that the resulting policy is deterministic. \\
%   \hline
%   $\mathrm{RBoN}_{\mathrm{WD}}$ \citep{jinnai2024regularized} & \checkmark & \checkmark & Maximize the mixture of the reward function and WD distance with a constraint that the resulting policy is deterministic. \\
%   \hline
%   \textbf{$\mathrm{SRBoN}_{\mathrm{KL}}$ (Section~\ref{propose:kl})} & \checkmark &  & Maximize the mixture of the reward function and KL divergence. \\
%   \hline
%   \textbf{$\mathrm{SRBoN}_{\mathrm{WD}}$ (Section~\ref{propose:WD})} & \checkmark & \checkmark & Maximize the mixture of the reward function and WD distance. \\
%   \hline
%   \textbf{$\mathrm{RBoN}_{\mathrm{L}}$ (Section~\ref{sec:exp})}& \checkmark &  & Consider both the reward function and the length of the sentence. (Details in \cref{sec:exp} and \cref{appendix:length})\\
%   \bottomrule
% \end{longtable}


\section{Experimental Evaluation}\label{sec:exp}

% We assess the performance of SRBoN in terms of performance compared to other text generation approaches.
% The datasets and models used in the experiments are all publically available (Appendix \ref{appendix:reprod}).
We evaluate the performance of SRBoN compared to other text generation approaches.
The datasets and models used in the experiments are all publicly available (Appendix \ref{appendix:reprod}).

\paragraph{Datasets.}
We conduct experiments using two datasets: the AlpacaFarm dataset \citep{NEURIPS2023_5fc47800} and Anthropic’s hh-rlhf (HH) dataset, which we use the Harmlessness and Helpfulness subsets \citep{bai2022training}. 
For the AlpacaFarm dataset, we use the first 1000 entries
of the train split (alpaca human preference) as the development set and the 805 entries of the evaluation split (alpaca farm evaluation) for evaluation. For Anthropic’s datasets, we separately
conduct experiments on the helpful-base (Helpfulness) and harmless-base (Harmlessness). For each dataset, we use the first 1000 entries of the train split as the development set and the first 1000 entries of the evaluation split for evaluation. 


\paragraph{Language Model, Reward Model, and Embedding Model.}
We employ Mistral 7B SFT $\beta$ \citep{jiang2023mistral} as the language models. 
% When sampling using this model, the following parameters are required: (Max instruction length, Max new tokens, Temperature, Top-p). For the sampling parameters used with the AlpacaFarm dataset, we applied (256, 256, 1.0, 1.0). When using the HH dataset, (256, 256, 1.0, 0.9) are employed.
We set the maximum entry length and the maximum output length to be 256 tokens. We sample response texts using nucleus sampling \citep{Holtzman2020The} with temperature set to 1.0 and top-p set to 0.9.
For each entry, in the AlpacaFarm dataset and Anthropic’s datasets, 128 responses are generated using Mistral 7B SFT $\beta$.

% To assess the algorithms' performance under varying preferences, we use SHP-Large (SteamSHP-flan-t5-large), SHP-XL (SteamSHP-flan-t5-xl), OASST (reward-model-deberta-v3-large-v2), Eurus-RM-7b, RM-Mistral-7B, and PairRM \citep{pmlr-v162-ethayarajh22a, NEURIPS2023_949f0f8f, yuan2024advancing, dong2023raft, jiang-etal-2023-llm} as reward models.
To evaluate the performance of the algorithms under different preferences, we use OASST (reward-model-deberta-v3-large-v2), SHP-Large (SteamSHP-flan-t5-large), SHP-XL (SteamSHP-flan-t5-xl), PairRM, RM-Mistral-7B and Eurus-RM-7b \citep{NEURIPS2023_949f0f8f,pmlr-v162-ethayarajh22a,  jiang-etal-2023-llm,dong2023raft,yuan2024advancing} as reward models.
For the text embedding model we use all-mpnet-base-v2 \citep{NEURIPS2020_c3a690be}, a sentence transformer model \citep{reimers-gurevych-2019-sentence} shown to be effective in various sentence embedding and semantic search tasks.

\begin{table}[tb]
\centering
\caption{Description of the text generation algorithms evaluated in the experiments. A checkmark (\checkmark) indicates that the method uses the specified function, while a blank space means that it does not.}
\label{tab:decoder}
\begin{tabular}{C{3cm}C{2cm}C{2cm}m{7cm}}
  \toprule
  \textbf{Method} & \textbf{Reward Function} & \textbf{Similarity Function} & \textbf{Description} \\
  \midrule
  Random sampling &  &  & Use an output that is randomly sampled from the reference model.  \\ 
  \hline
  Best-of-N (BoN) \citep{stiennon2020} & \checkmark &  & Generate N outputs, evaluate with reward function, select the best.  \\
  \hline
  MBR \citep{eikema-aziz-2022-sampling} &  & \checkmark & Generate N outputs, evaluate with expected utility function, select the best. (Details in \cref{sec:exp}) \\
  \hline
  $\mathrm{RBoN}_{\mathrm{KL}}$ \citep{jinnai2024regularized} & \checkmark &  & Maximize the mixture of the reward function and KL divergence with a constraint that the resulting policy is deterministic. \\
  \hline
  $\mathrm{RBoN}_{\mathrm{WD}}$ \citep{jinnai2024regularized} & \checkmark & \checkmark & Maximize the mixture of the reward function and WD distance with a constraint that the resulting policy is deterministic. \\
  \hline
  \textbf{$\mathrm{SRBoN}_{\mathrm{KL}}$ (Section~\ref{propose:kl})} & \checkmark &  & Maximize the mixture of the reward function and KL divergence. \\
  \hline
  \textbf{$\mathrm{SRBoN}_{\mathrm{WD}}$ (Section~\ref{propose:WD})} & \checkmark & \checkmark & Maximize the mixture of the reward function and WD distance. \\
  \hline
  \textbf{$\mathrm{RBoN}_{\mathrm{L}}$ (Section~\ref{sec:exp})}& \checkmark &  & Consider both the reward function and the token length of the sentence. (Details in \cref{sec:exp} and \cref{appendix:length})\\
  \bottomrule
\end{tabular}
\end{table}

\paragraph{Baselines.}
The list of text generation methods we evaluate is present in Table \ref{tab:decoder}.
The baseline methods include random sampling (nucleus sampling; \citealt{Holtzman2020The}), Best-of-N (BoN) sampling, Minimum Bayes Risk (MBR) decoding, and $\mathrm{RBoN}_{\mathrm{L}}$, which we describe in the following.
% \paragraph{Minimum Bayes Risk Decoding \citep{eikema-aziz-2022-sampling}.}

\textbf{Minimum Bayes Risk (MBR) decoding} \citep{kumar-byrne-2002-minimum,kumar-byrne-2004-minimum,eikema-aziz-2022-sampling} is a text generation strategy that selects an output from $N$ outputs that maximizes the expected utility \citep{Berger:1327974}. Let a utility function $u(h, y)$ quantify the benefit of choosing $h \in \mathcal{Y}_{\textbf{ref}}$ if $y$ is the correct output. Then, MBR decoding is defined as follows:
\begin{equation}
% y_{\mathrm{MBR}}(x) = \underset{h \in \mathcal{Y}_{\textbf{ref}} }{\arg \max } \,\,\underset{y \sim \hat{\pi}_\mathrm{ref}}{\mathbb{E}}[u(h, y) \mid  x] = \underset{h \in \mathcal{Y}_{\textbf{ref}} }{\arg \max } \sum_{y \in \mathcal{Y}_{\textbf {ref}}} \frac{1}{N} u\left(h, y\right).
y_{\mathrm{MBR}}(x) = \underset{h \in \mathcal{Y}_{\textbf{ref}} }{\arg \max } \sum_{y \in \mathcal{Y}_{\textbf {ref}}} \frac{1}{N} u\left(h, y\right).
\end{equation}
% We include MBR decoding as one of the baselines as it is shown to be effective in a variety of text generation tasks \citep{suzgun-etal-2023-follow,bertsch-etal-2023-mbr,li2024agents,heineman2024improving}.
We include MBR decoding as one of the baselines because it has been shown to be effective in a variety of text generation tasks \citep{suzgun-etal-2023-follow,bertsch-etal-2023-mbr,li2024agents,heineman2024improving}.
We follow the implementation of \cite{jinnai2024regularized} and use the cosine similarity of the sentence embedding as the utility function. We use the same embedding model as the $\mathrm{RBoN}_\mathrm{WD}$, all-mpnet-base-v2.
Note that MBR corresponds to $\mathrm{RBoN}_{\mathrm{WD}}$ with $u(h, y) = 1 - C(h, y)$ with no reward function or $\beta \rightarrow +\infty$ (Eq. (\ref{eq:wd_N})) \citep{jinnai2024regularized}.

% \paragraph{Sentence Length Regularized Method ($\mathrm{RBoN}_{\mathrm{L}}$)}
 As an additional evaluation method, we propose \textbf{Sentence Length Regularized BoN} ($\mathrm{RBoN}_{\mathrm{L}}$), a simple baseline that adjusts the output token length to the target reward model.
% In the $\mathrm{RBoN}_{\mathrm{KL}}$ and $\mathrm{SRBoN}_{\mathrm{KL}}$, $\pi_{\textbf{ref}}$ was used for regularization. However, we have observed a bias in language models regarding sentence length, specifically that these models tend to output shorter sentences with higher probability (\cref{appendix:kl}).
In $\mathrm{RBoN}_{\mathrm{KL}}$ and $\mathrm{SRBoN}_{\mathrm{KL}}$, $\pi_{\textbf{ref}}$ was used for regularization. However, we have observed a bias in language models with respect to sentence length, namely that these models tend to produce shorter sentences with higher probability (\cref{appendix:kl}). % Intuitively, we propose a novel modification of the RBoN method that incorporates a sentence regularization term.
% To this end, we propose a simple implementation of RBoN that regularizes the sequence length generation probability instead of each sequence's generation probability.
To this end, we propose a simple implementation of RBoN that regularizes the generation probability of the sequence token length instead of the generation probability of each sequence.
The objective function of $\mathrm{RBoN}_{\mathrm{L}}$ is given by:
\begin{equation}
y_{\mathrm{RBoN_\mathrm{L}}}(x)=\underset{y \in \mathcal{Y}_{\textbf{ref}}}{\arg \max } \,\,R(x, y)-\frac{\beta}{|y|},
\end{equation}
where $\beta$ is a regularization parameter and $|y|$ denotes the sequence length (i.e., the number of tokens).

% The rationale of this specific form of the regularization term and the experimental details of this approach are described in 
The rationale for this particular form of the regularization term and the experimental details of this approach are described in \cref{appendix:length}.

\subsection{Evaluation of the Algorithms}\label{sec:exp_1}
% \subsection{Comparing with Various Method}\label{sec:exp_1}
% Our experiment incorporates a utility function-based decoder method, MBR(Minimum Bayes Risk Decoding) to provide a comprehensive evaluation framework as an additional comparative baseline. 

\paragraph{Setup.}
% We compare the 7 methods using win rates against BoN sampling in the evaluation splits of the datasets. Since the RBoN method has a hyperparameter $\beta$, we first find the optimal $\beta^*$ on the train splits. 
We compare the 7 methods using win rates vs. BoN sampling on the evaluation splits of the datasets. Since the RBoN method has a hyperparameter $\beta$, we first find the optimal $\beta^*$ on the train splits. 
% For the AlpacaFarm dataset, we use the first 999 entries
% of the train split (alpaca human preference) as the development set and the 805 instructions of evaluation split (alpaca farm evaluation). For Anthropic’s datasets, we separately
% conduct experiments on the helpful-base (Helpfulness) and harmless-base (Harmlessness). For each dataset, we use the first 999 entries of the train split and use the first 999 entries of the evaluation split. 
Hyperparameter $\beta$
range is \{$1.0\times 10^{-4}$, $2.0\times 10^{-4}$, $5.0\times 10^{-4}$,$1.0\times 10^{-3}$,..., $2.0\times 10^1$\}.
We first find the optimal beta value $\beta^*$ in the train split, then we use the optimal values in the development split for the evaluation split.
% In this experiment, we employ SHP-Large, SHP-XL, OASST, PairRM, and RM-Mistral-7B as proxy reward models. % As
% the gold reward model, we use Eurus-RM-7B to evaluate the performance of the algorithms. 
% We evaluate the performance of the algorithms as the win rate against BoN sampling according to the reward score of the gold reward model (we count ties as 0.5 wins). we use Eurus-RM-7B as the gold reward model as it is reproducible as it is open-sourced and is shown to have a high correlation with a human preference in RewardBench \cite{lambert2024rewardbench}.
In this experiment, we use OASST, SHP-Large, SHP-XL, PairRM, and RM-Mistral-7B as proxy reward models. As
as the gold reward model, we use Eurus-RM-7B to evaluate the performance of the algorithms. 
We evaluate the performance of the algorithms as the win rate against BoN sampling according to the reward score of the gold reward model (we count ties as 0.5 wins). We use Eurus-RM-7B as the gold reward model because it is reproducible as it is open source and has been shown to have a high correlation with human preference in RewardBench \citep{lambert2024rewardbench}.
% To account for ties in addition to wins when comparing each method against BoN sampling, we assign 1 point for a win and 0.5 points for a tie.

\paragraph{Results.}
% \cref{res:table} reveals several noteworthy results across the AlpacaFarm, Harmlessness, and Helpfulness datasets.

% \yuu{Let's think of the messages we want to tell to the readers and their priorities.
% - One of the main contributions of the paper is to introduce SRBoN, the theoretically motivated variant of RBoN. The performance of these algorithms is of interest to the readers.
% - What's observed in the previous work is good but not the unique message of the paper.
% - In general, one paragraph should contain one important message.
% }

% The win rate result shows that higher Spearman rank correlation values correspond to better BoN sampling accuracy. This observation aligns with intuition. 

% $\mathrm{RBoN}_{\mathrm{WD}}$ emerges as a consistently strong performance, frequently achieving a win rate above $50 \%$ across various models. Interestingly, the optimal $\beta$ for $\mathrm{RBoN}_{\mathrm{WD}}$ exhibits significant variability, underscoring the importance of careful tuning to maximize performance under specific conditions. This sensitivity to $\beta$ suggests that while $\mathrm{RBoN}_{\mathrm{WD}}$ is generally effective, its optimal implementation requires adjustment to the particular task and reward model at hand.

% In contrast, $\mathrm{RBoN}_{\mathrm{KL}}$, with fixed $\beta = 0.00001$, shows more variable performance. Its efficacy appears to be highly dependent on the specific reward model and domain. 

% This experiment reveals that the stochastic versions ($\mathrm{SRBoN}_{\mathrm{KL}}$, $\mathrm{SRBoN}_{\mathrm{WD}}$) show inferior performance compared to their deterministic counterpart ($\mathrm{RBoN}_{\mathrm{KL}}$, $\mathrm{RBoN}_{\mathrm{WD}}$). This outcome aligns with intuitive expectations, as the Stochastic version solves an optimization problem while accounting for worst-case scenarios.
% \yuu{TODO: Ideally we want to say more than "intuitive" here. The readers would rather expect the proposed method to outperform the baselines as that is how most of the papers would say. We should }

% MBR approach demonstrates considerable variability in its performance. In some cases, it achieves competitive results, as evidenced by its $57.4\%$ win rate for Harmlessness with SHP-Large. However, it also shows markedly poor performance in other scenarios, such as its $6.1\%$ win rate for helpfulness with RM-Mistral-7B. This inconsistency suggests that MBR's effectiveness is highly problem-dependent. 

% Despite its simple implementation, $\mathrm{RBoN}_{\mathrm{L}}$ consistently outperformed BoN sampling, achieving a higher win rate on almost all tasks and models without instances of underperformance. Detailed discussion of $\mathrm{RBoN}_{\mathrm{L}}$ is presented in \cref{appendix:length}. Despite the theoretical robustness of $\mathrm{SRBoN}_{\mathrm{KL}}$ demonstrated in the analyses presented in \cref{sec:kl_sec}, the experimental results are not performed well compared to $\mathrm{SRBoN}_{\mathrm{WD}}$. One possible factor, in scenarios where there is less correlation between $\pi_{\textbf{ref}}$ and the reward function, $\mathrm{SRBoN}_{\mathrm{WD}}$ maintains a distinct advantage. This is because the constraint set for the reward perturbation $\Delta R$ in $\mathrm{SRBoN}_{\mathrm{WD}}$ is independent of $\pi_{\textbf{ref}}$. Consequently, even when $\pi_{\textbf{ref}}$ lacks a strong relationship with the reward function, $\mathrm{SRBoN}_{\mathrm{WD}}$ methods can still mitigate performance degradation more effectively compared to  $\mathrm{SRBoN}_{\mathrm{KL}}$.

% ===========================================% ===========================================

\cref{res:table} reveals several noteworthy results for the AlpacaFarm, Harmlessness, and Helpfulness datasets and the optimal beta $\beta^*$ is \cref{tab:optimal_beta}.
The win rate result shows that higher Spearman rank correlation values (\cref{tab:spear_rank}) correspond to better BoN sampling accuracy. This observation is intuitive. 

% \cref{res:table} shows that the winrate of $\mathrm{SRBoN}_{\mathrm{KL}}$ is inferior to deterministic version $\mathrm{RBoN}_{\mathrm{KL}}$. While $\mathrm{SRBoN}_{\mathrm{KL}}$ is proposed as a theoretically robust algorithm (\cref{sec:WD}), its performance in our experiments did not fully meet expectations. One potential factor contributing to this discrepancy could be related to the perturbation range of $\Delta R$. In our experimental setup, it is plausible that the actual perturbations of $\Delta R$ may have exceeded the theoretical bounds assumed. 
\cref{res:table} shows that the win rate of $\mathrm{SRBoN}_{\mathrm{KL}}$ is inferior to the deterministic version $\mathrm{RBoN}_{\mathrm{KL}}$. While $\mathrm{SRBoN}_{\mathrm{KL}}$ is proposed as a theoretically robust algorithm (\cref{sec:WD}), its performance in our experiments did not fully meet expectations. One possible factor contributing to this discrepancy could be related to the perturbation range of $\Delta R$. In our experimental setup, it is plausible that the actual perturbations of $\Delta R$ may have exceeded the assumed theoretical limits. 

% Other reasons for the suboptimal performance, applicable to both deterministic and stochastic versions, concern the relationship between the reference policy $\pi_{\textnormal{\textbf{ref}}}$ and the reward model. When the correlation between $\pi_{\textnormal{\textbf{ref}}}$ and the reward model is weak, the regularization effect may not contribute positively to the algorithm's performance (\cref{appendix:kl}).
Other reasons for suboptimal performance, applicable to both deterministic and stochastic versions, concern the relationship between the reference policy $\pi_{\textnormal{\textbf{ref}}}$ and the reward model. If the correlation between $\pi_{\textnormal{\textbf{ref}}}$ and the reward model is weak, the regularization effect may not contribute positively to the performance of the algorithm (\cref{appendix:kl}).

% $\mathrm{SRBoN}_{\mathrm{WD}}$ exhibits superior performance across various settings and achieves comparable performance to $\mathrm{RBoN}_{\mathrm{WD}}$. This robust performance is noteworthy given the low positive correlation between the reference policy $\pi_{\textnormal{\textbf{ref}}}$ and the reward model.
$\mathrm{SRBoN}_{\mathrm{WD}}$ shows superior performance across several settings and achieves comparable performance to $\mathrm{RBoN}_{\mathrm{WD}}$. This robust performance is remarkable given the low positive correlation between the reference policy $\pi_{\textnormal{\textbf{ref}}}$ and the reward model.

% One plausible explanation for this effectiveness, especially in contrast to $\mathrm{SRBoN}_{\mathrm{KL}}$, lies the constraint on the reward perturbation $\Delta R$ in $\mathrm{SRBoN}_{\mathrm{WD}}$. Unlike $\mathrm{SRBoN}_{\mathrm{KL}}$, the constraint on $\Delta R$ in $\mathrm{SRBoN}_{\mathrm{WD}}$ is independent of $\pi_{\textnormal{\textbf{ref}}}$ which mitigate low performance when there is no correlation between reward model and $\pi_{\textnormal{\textbf{ref}}}$. 
A plausible explanation for this effectiveness, especially in contrast to $\mathrm{SRBoN}_{\mathrm{KL}}$, is the constraint on the reward perturbation $\Delta R$ in $\mathrm{SRBoN}_{\mathrm{WD}}$. Unlike $\mathrm{SRBoN}_{\mathrm{KL}}$, the constraint on $\Delta R$ in $\mathrm{SRBoN}_{\mathrm{WD}}$ is independent of $\pi_{\textnormal{\textbf{ref}}}$, which mitigates low performance when there is no correlation between the reward model and $\pi_{\textnormal{\textbf{ref}}}$. 

% Despite its simple implementation, $\mathrm{RBoN}_{\mathrm{L}}$ consistently outperformed BoN sampling, achieving a higher win rate on almost all tasks and models without instances of underperformance. Detailed discussion of $\mathrm{RBoN}_{\mathrm{L}}$ is presented in \cref{appendix:length}. 

Despite its simple implementation, $\mathrm{RBoN}_{\mathrm{L}}$ consistently outperformed BoN sampling, achieving a higher win rate on almost all tasks and models with no instances of underperformance. A detailed discussion of $\mathrm{RBoN}_{\mathrm{L}}$ is presented in \cref{appendix:length}. 

% $\mathrm{RBoN}_{\mathrm{WD}}$ emerges as a consistently strong performance, frequently achieving a win rate above $50 \%$ across various models. $\mathrm{RBoN}_{\mathrm{KL}}$, with fixed $\beta = 0.00001$, shows a more variable performance, its effectiveness seems to be highly dependent on the specific reward model and domain.

% MBR approach shows considerable variability in its performance. It sometimes achieves competitive results, as evidenced by its $57.4\%$ win rate for Harmlessness with SHP-Large. However, it also shows markedly poor performance in other scenarios, such as its $6.1\%$ win rate for helpfulness with RM-Mistral-7B. This inconsistency suggests that MBR's effectiveness is highly problem-dependent. 
% \begin{table}
% \centering
% % \small
% \caption{The win rate of various methods against BoN sampling. For RBoN, the optimal parameter ($\beta^*$) and the Spearman rank correlation ($\rho$) with the gold reward model are shown for each dataset. \yuu{The Table contains multiple messages at once. Generally speaking, one Table (or Figure) should contain exactly one message. Otherwise, the readers won't get the message immediately from the Table. I would suggest having a separate table for Spearman's rank correlation and the $\beta$ values. On reporting the relationship between the Spearman's rank correlation and the win rates of the methods, we may want to draw a Figure dedicated to explaining the relationship if its correlation is clear.} \yuu{Minor: The common practice in NLP papers is to put existing methods above the proposed methods. It would be better to align the order of methods to follow the order in Table \ref{tab:decoder}.} \yuu{Show the best performing algorithm in bold font so that the reader can tell which one was the best at a glance.} \yuu{For the sake of clarity, I would put a row with BoN (Baseline), showing the win rate of 50 for every entry so that it is easy to see that 50 is the baseline.}}\label{res:table}
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{@{}lrrrrr@{}}
% \toprule
% \rowcolor[HTML]{EFEFEF} 
%  Method ($\rho$)& \textbf{OASST} ($0.39$) & \textbf{SHP-Large} ($0.29$) & \textbf{SHP-XL} ($0.35$)& \textbf{PairRM} ($0.33$) & \textbf{RM-Mistral-7B} ($0.62$) \\ \midrule

% \multicolumn{6}{c}{\textbf{AlpacaFarm}} \\ \midrule
% \text{MBR} & \multicolumn{1}{c}{36.0} & \multicolumn{1}{c}{42.8} & \multicolumn{1}{c}{40.8} & \multicolumn{1}{c}{39.1} & \multicolumn{1}{c}{13.0} \\
% \textbf{$\mathrm{RBoN}_{\mathrm{WD}}$} & \multicolumn{1}{c}{50.6 ($\beta=20$)} & \multicolumn{1}{c}{50.2 ($\beta=0.5$)} & \multicolumn{1}{c}{49.0 ($\beta=0.5$)} & \multicolumn{1}{c}{50.7 ($\beta = 20.0$)} & \multicolumn{1}{c}{49.9 ($\beta=0.1$)} \\
% \textbf{$\mathrm{RBoN}_{\mathrm{L}}$} & \multicolumn{1}{c}{52.0 ($\beta=20$)} & \multicolumn{1}{c}{50.3($\beta=0.5$)} & \multicolumn{1}{c}{50.2($\beta=0.2$)} & \multicolumn{1}{c}{50.1 ($\beta = 20.0$)} & \multicolumn{1}{c}{50.8 ($\beta=15.0$)} \\
% \textbf{$\mathrm{RBoN}_{\mathrm{KL}}$ ($\beta=0.0001$)} & \multicolumn{1}{c}{47.7} & \multicolumn{1}{c}{26.4} & \multicolumn{1}{c}{26.2} & \multicolumn{1}{c}{50.0} & \multicolumn{1}{c}{48.6} \\
% \textbf{$\mathrm{SRBoN}_{\mathrm{WD}}$} & \multicolumn{1}{c}{50.1 ($\beta=0.5$)} & \multicolumn{1}{c}{50.6 ($\beta=0.0002$)} & \multicolumn{1}{c}{49.5 ($\beta=0.0001$)} & \multicolumn{1}{c}{50.0 ($\beta = 0.0001$)} & \multicolumn{1}{c}{50.1 ($\beta=1.0$)} \\
% \textbf{$\mathrm{SRBoN}_{\mathrm{KL}}$} & \multicolumn{1}{c}{12.6 ($\beta=20$)} & \multicolumn{1}{c}{20.9 ($\beta=0.05$)} & \multicolumn{1}{c}{18.7 ($\beta=0.05$)} & \multicolumn{1}{c}{28.0 ($\beta = 20.0$)} & \multicolumn{1}{c}{4.7 ($\beta=20.0$)} \\
% \text{Random} & \multicolumn{1}{c}{20.5} & \multicolumn{1}{c}{30.3} & \multicolumn{1}{c}{29.4} & \multicolumn{1}{c}{27.1} & \multicolumn{1}{c}{3.0} \\\midrule
% \rowcolor[HTML]{EFEFEF} 
% & \textbf{OASST} (0.37) & \textbf{SHP-Large} (0.09) & \textbf{SHP-XL} (0.14)& \textbf{PairRM} (0.36)& \textbf{RM-Mistral-7B} (0.60)\\ \midrule

% \multicolumn{6}{c}{\textbf{Harmlessness}} \\ \midrule
% \text{MBR} & \multicolumn{1}{c}{40.8} & \multicolumn{1}{c}{57.4} & \multicolumn{1}{c}{50.7} & \multicolumn{1}{c}{42.7} & \multicolumn{1}{c}{14.8} \\
% \textbf{$\mathrm{RBoN}_{\mathrm{WD}}$} & \multicolumn{1}{c}{52.1 ($\beta = 20.0$)} & \multicolumn{1}{c}{62.2 ($\beta = 1.0$)} & \multicolumn{1}{c}{57.1 ($\beta = 1.0$)} & \multicolumn{1}{c}{50.0 ($\beta = 0.0001$)} & \multicolumn{1}{c}{49.9 ($\beta = 5.0$)} \\
% \textbf{$\mathrm{RBoN}_{\mathrm{L}}$} & \multicolumn{1}{c}{52.2 ($\beta=20$)} & \multicolumn{1}{c}{54.8 ($\beta=5.0$)} & \multicolumn{1}{c}{54.2 ($\beta=5.0$)} & \multicolumn{1}{c}{50.0 ($\beta = 0.0001$)} & \multicolumn{1}{c}{51.6 ($\beta=20$)} \\
% \textbf{$\mathrm{RBoN}_{\mathrm{KL}}$ ($\beta = 0.0001$)} & \multicolumn{1}{c}{48.2} & \multicolumn{1}{c}{46.9} & \multicolumn{1}{c}{40.4} & \multicolumn{1}{c}{50.0} & \multicolumn{1}{c}{47.4} \\
% \textbf{$\mathrm{SRBoN}_{\mathrm{WD}}$} & \multicolumn{1}{c}{49.7 ($\beta=0.05$)} & \multicolumn{1}{c}{51.2 ($\beta=0.0001$)} & \multicolumn{1}{c}{49.8 ($\beta=0.0001$)} & \multicolumn{1}{c}{50.0 ($\beta = 0.0001$)} & \multicolumn{1}{c}{49.9 ($\beta=0.02$)} \\
% \textbf{$\mathrm{SRBoN}_{\mathrm{KL}}$} & \multicolumn{1}{c}{20.5 ($\beta=20$)} & \multicolumn{1}{c}{42.3 ($\beta=0.05$)} & \multicolumn{1}{c}{37.1 ($\beta=20.0$)} & \multicolumn{1}{c}{30.4 ($\beta = 20.0$)} & \multicolumn{1}{c}{5.5 ($\beta=20.0$)} \\
% \text{Random} & \multicolumn{1}{c}{26.7} & \multicolumn{1}{c}{52.7} & \multicolumn{1}{c}{46.3} & \multicolumn{1}{c}{28.0} & \multicolumn{1}{c}{7.1} \\\midrule
% \rowcolor[HTML]{EFEFEF} 
% & \textbf{OASST} (0.39)& \textbf{SHP-Large} (0.38)& \textbf{SHP-XL} (0.50)& \textbf{PairRM} (0.34)& \textbf{RM-Mistral-7B} (0.75)\\ \midrule
% \multicolumn{6}{c}{\textbf{Helpfulness}} \\ \midrule
% \text{MBR} & \multicolumn{1}{c}{41.4} & \multicolumn{1}{c}{39.2} & \multicolumn{1}{c}{33.2} & \multicolumn{1}{c}{40.0} & \multicolumn{1}{c}{6.1} \\
% \textbf{$\mathrm{RBoN}_{\mathrm{WD}}$} & \multicolumn{1}{c}{52.5 ($\beta= 15.0$)} & \multicolumn{1}{c}{52.4 ($\beta= 0.05$)} & \multicolumn{1}{c}{50.1 ($\beta= 0.1$)} & \multicolumn{1}{c}{50.1 ($\beta= 20.0$)} & \multicolumn{1}{c}{49.9 ($\beta= 0.5$)} \\
% \textbf{$\mathrm{RBoN}_{\mathrm{L}}$} & \multicolumn{1}{c}{52.7 ($\beta=20$)} & \multicolumn{1}{c}{49.9 ($\beta=0.02$)} & \multicolumn{1}{c}{50.8 ($\beta=0.2$)} & \multicolumn{1}{c}{50.0 ($\beta = 5.0$)} & \multicolumn{1}{c}{50.2 ($\beta=20$)} \\
% \textbf{$\mathrm{RBoN}_{\mathrm{KL}}$ ($\beta = 0.0001$)} & \multicolumn{1}{c}{44.9} & \multicolumn{1}{c}{19.9} & \multicolumn{1}{c}{13.9} & \multicolumn{1}{c}{50.0} & \multicolumn{1}{c}{50} \\
% \textbf{$\mathrm{SRBoN}_{\mathrm{WD}}$} & \multicolumn{1}{c}{50.4 ($\beta=0.5$)} & \multicolumn{1}{c}{49.5 ($\beta=0.001$)} & \multicolumn{1}{c}{49.6 ($\beta=0.005$)} & \multicolumn{1}{c}{50.0 ($\beta = 5.0$)} & \multicolumn{1}{c}{50.0 ($\beta=0.0002$)} \\
% \textbf{$\mathrm{SRBoN}_{\mathrm{KL}}$} & \multicolumn{1}{c}{13.4 ($\beta=20.0$)} & \multicolumn{1}{c}{18.5 ($\beta=0.05$)} & \multicolumn{1}{c}{11.8 ($\beta=20.0$)} & \multicolumn{1}{c}{24.3 ($\beta = 20.0$)} & \multicolumn{1}{c}{1.4 ($\beta=20.0$)} \\
% \text{Random} & \multicolumn{1}{c}{23.6} & \multicolumn{1}{c}{23.7} & \multicolumn{1}{c}{15.1} & \multicolumn{1}{c}{23.3} & \multicolumn{1}{c}{0.8} \\ \bottomrule
% \end{tabular}
% \label{tab:diff}}
% \end{table}
\begin{table}[tb]
\centering
\small
\caption{The win rate of various methods against BoN sampling.}\label{res:table}
\begin{tabular}{@{}lrrrrr@{}}
\toprule
\rowcolor[HTML]{EFEFEF} 
 Method & \textbf{OASST}& \textbf{SHP-Large} & \textbf{SHP-XL} & \textbf{PairRM}  & \textbf{RM-Mistral-7B} \\ \midrule

\multicolumn{6}{c}{\textbf{AlpacaFarm}} \\ \midrule
\text{BoN} & \multicolumn{1}{c}{50.0} & \multicolumn{1}{c}{50.0} & \multicolumn{1}{c}{50.0} & \multicolumn{1}{c}{50.0} & \multicolumn{1}{c}{50.0} \\
\text{MBR} & \multicolumn{1}{c}{36.0} & \multicolumn{1}{c}{42.8} & \multicolumn{1}{c}{40.8} & \multicolumn{1}{c}{39.1} & \multicolumn{1}{c}{13.0} \\
\text{Random} & \multicolumn{1}{c}{20.5} & \multicolumn{1}{c}{30.3} & \multicolumn{1}{c}{29.4} & \multicolumn{1}{c}{27.1} & \multicolumn{1}{c}{3.0} \\
\textbf{$\mathrm{RBoN}_{\mathrm{WD}}$} & \multicolumn{1}{c}{50.6} & \multicolumn{1}{c}{50.2} & \multicolumn{1}{c}{49.0 } & \multicolumn{1}{c}{\textbf{50.7} } & \multicolumn{1}{c}{49.9 } \\
\textbf{$\mathrm{RBoN}_{\mathrm{KL}}$} & \multicolumn{1}{c}{47.7} & \multicolumn{1}{c}{26.4} & \multicolumn{1}{c}{26.2} & \multicolumn{1}{c}{50.0} & \multicolumn{1}{c}{48.6} \\
\textbf{$\mathrm{RBoN}_{\mathrm{L}}$} & \multicolumn{1}{c}{\textbf{52.0}} & \multicolumn{1}{c}{50.3} & \multicolumn{1}{c}{\textbf{50.2}} & \multicolumn{1}{c}{50.1} & \multicolumn{1}{c}{\textbf{50.8}} \\
\textbf{$\mathrm{SRBoN}_{\mathrm{WD}}$} & \multicolumn{1}{c}{50.1} & \multicolumn{1}{c}{\textbf{50.6}} & \multicolumn{1}{c}{49.5 } & \multicolumn{1}{c}{50.0} & \multicolumn{1}{c}{50.1} \\
\textbf{$\mathrm{SRBoN}_{\mathrm{KL}}$} & \multicolumn{1}{c}{12.6} & \multicolumn{1}{c}{20.9} & \multicolumn{1}{c}{18.7 } & \multicolumn{1}{c}{28.0} & \multicolumn{1}{c}{4.7} \\
\midrule
\rowcolor[HTML]{EFEFEF} 
& \textbf{OASST} & \textbf{SHP-Large} & \textbf{SHP-XL}& \textbf{PairRM}& \textbf{RM-Mistral-7B}\\ \midrule
\multicolumn{6}{c}{\textbf{Harmlessness}} \\ \midrule
\text{BoN} & \multicolumn{1}{c}{50.0} & \multicolumn{1}{c}{50.0} & \multicolumn{1}{c}{50.0} & \multicolumn{1}{c}{50.0} & \multicolumn{1}{c}{50.0} \\
\text{MBR} & \multicolumn{1}{c}{40.8} & \multicolumn{1}{c}{57.4} & \multicolumn{1}{c}{50.7} & \multicolumn{1}{c}{42.7} & \multicolumn{1}{c}{14.8} \\
\text{Random} & \multicolumn{1}{c}{26.7} & \multicolumn{1}{c}{52.7} & \multicolumn{1}{c}{46.3} & \multicolumn{1}{c}{28.0} & \multicolumn{1}{c}{7.1} \\
\textbf{$\mathrm{RBoN}_{\mathrm{WD}}$} & \multicolumn{1}{c}{52.1} & \multicolumn{1}{c}{\textbf{62.2}} & \multicolumn{1}{c}{\textbf{57.1}} & \multicolumn{1}{c}{50.0 } & \multicolumn{1}{c}{49.9 } \\
\textbf{$\mathrm{RBoN}_{\mathrm{KL}}$ } & \multicolumn{1}{c}{48.2} & \multicolumn{1}{c}{46.9} & \multicolumn{1}{c}{40.4} & \multicolumn{1}{c}{50.0} & \multicolumn{1}{c}{47.4} \\
\textbf{$\mathrm{RBoN}_{\mathrm{L}}$} & \multicolumn{1}{c}{\textbf{52.2} } & \multicolumn{1}{c}{54.8 } & \multicolumn{1}{c}{54.2 } & \multicolumn{1}{c}{50.0 } & \multicolumn{1}{c}{\textbf{51.6 }} \\
\textbf{$\mathrm{SRBoN}_{\mathrm{WD}}$} & \multicolumn{1}{c}{49.7} & \multicolumn{1}{c}{51.2 } & \multicolumn{1}{c}{49.8 } & \multicolumn{1}{c}{50.0 } & \multicolumn{1}{c}{49.9 } \\
\textbf{$\mathrm{SRBoN}_{\mathrm{KL}}$} & \multicolumn{1}{c}{20.5} & \multicolumn{1}{c}{42.3} & \multicolumn{1}{c}{37.1 } & \multicolumn{1}{c}{30.4} & \multicolumn{1}{c}{5.5} \\
\midrule
\rowcolor[HTML]{EFEFEF} 
& \textbf{OASST}& \textbf{SHP-Large} & \textbf{SHP-XL} & \textbf{PairRM}& \textbf{RM-Mistral-7B}\\ \midrule
\multicolumn{6}{c}{\textbf{Helpfulness}} \\ \midrule
\text{BoN} & \multicolumn{1}{c}{50.0} & \multicolumn{1}{c}{50.0} & \multicolumn{1}{c}{50.0} & \multicolumn{1}{c}{50.0} & \multicolumn{1}{c}{50.0} \\
\text{MBR} & \multicolumn{1}{c}{41.4} & \multicolumn{1}{c}{39.2} & \multicolumn{1}{c}{33.2} & \multicolumn{1}{c}{40.0} & \multicolumn{1}{c}{6.1} \\
\text{Random} & \multicolumn{1}{c}{23.6} & \multicolumn{1}{c}{23.7} & \multicolumn{1}{c}{15.1} & \multicolumn{1}{c}{23.3} & \multicolumn{1}{c}{0.8} \\
\textbf{$\mathrm{RBoN}_{\mathrm{WD}}$} & \multicolumn{1}{c}{52.5} & \multicolumn{1}{c}{\textbf{52.4}} & \multicolumn{1}{c}{50.1 } & \multicolumn{1}{c}{\textbf{50.1}} & \multicolumn{1}{c}{49.9} \\
\textbf{$\mathrm{RBoN}_{\mathrm{KL}}$} & \multicolumn{1}{c}{44.9} & \multicolumn{1}{c}{19.9} & \multicolumn{1}{c}{13.9} & \multicolumn{1}{c}{50.0} & \multicolumn{1}{c}{50.0} \\
\textbf{$\mathrm{RBoN}_{\mathrm{L}}$} & \multicolumn{1}{c}{\textbf{52.7}} & \multicolumn{1}{c}{49.9} & \multicolumn{1}{c}{\textbf{50.8}} & \multicolumn{1}{c}{50.0} & \multicolumn{1}{c}{\textbf{50.2}} \\
\textbf{$\mathrm{SRBoN}_{\mathrm{WD}}$} & \multicolumn{1}{c}{50.4} & \multicolumn{1}{c}{49.5} & \multicolumn{1}{c}{49.6 } & \multicolumn{1}{c}{50.0} & \multicolumn{1}{c}{50.0} \\
\textbf{$\mathrm{SRBoN}_{\mathrm{KL}}$} & \multicolumn{1}{c}{13.4 } & \multicolumn{1}{c}{18.5} & \multicolumn{1}{c}{11.8 } & \multicolumn{1}{c}{24.3 } & \multicolumn{1}{c}{1.4} \\
 \bottomrule
\end{tabular}
\label{tab:diff}
\end{table}


\begin{table}[tb]
\caption{Spearman's rank correlation between Eurus-RM-7B and each proxy reward. The comprehensive Spearman's rank correlation results for all the aforementioned analyses are presented in \cref{ap:recol}.}
\centering
\small
\begin{tabular}{@{}lrrrrr@{}}
\toprule
\rowcolor[HTML]{EFEFEF} 
Dataset& \textbf{OASST} & \textbf{SHP-Large} & \textbf{SHP-XL} & \textbf{PairRM}  & \textbf{RM-Mistral-7B} \\ \midrule

\text{AlpacaFarm} & \multicolumn{1}{c}{$0.39$} & \multicolumn{1}{c}{$0.29$} & \multicolumn{1}{c}{$0.35$} & \multicolumn{1}{c}{$0.33$} & \multicolumn{1}{c}{$0.62$} 
\\ \midrule
\text{Harmlessness} & \multicolumn{1}{c}{$0.37$} & \multicolumn{1}{c}{$0.09$} & \multicolumn{1}{c}{$0.14$} & \multicolumn{1}{c}{$0.36$} & \multicolumn{1}{c}{$0.60$} 
\\\midrule
\text{Helpfulness} & \multicolumn{1}{c}{$0.39$} & \multicolumn{1}{c}{$0.38$} & \multicolumn{1}{c}{$0.50$} & \multicolumn{1}{c}{$0.34$} & \multicolumn{1}{c}{$0.75$} \\\bottomrule
\end{tabular}
\label{tab:spear_rank}
\end{table}

\begin{table}[tb]
\centering

\caption{Optimal beta $\beta^*$ in the train split}
\small
\begin{tabular}{@{}lrrrrr@{}}
\toprule
\rowcolor[HTML]{EFEFEF} 
 Method & \textbf{OASST} & \textbf{SHP-Large} & \textbf{SHP-XL} & \textbf{PairRM}  & \textbf{RM-Mistral-7B} \\ \midrule

\multicolumn{6}{c}{\textbf{AlpacaFarm}} \\ \midrule
\textbf{$\mathrm{RBoN}_{\mathrm{WD}}$} & \multicolumn{1}{c}{$20$} & \multicolumn{1}{c}{$0.5$} & \multicolumn{1}{c}{$0.5$} & \multicolumn{1}{c}{$20$} & \multicolumn{1}{c}{$0.1$} \\
\textbf{$\mathrm{RBoN}_{\mathrm{KL}}$ } & \multicolumn{1}{c}{$0.0001$} & \multicolumn{1}{c}{$0.0001$} & \multicolumn{1}{c}{$0.0001$} & \multicolumn{1}{c}{$0.0001$} & \multicolumn{1}{c}{$0.0001$} \\
\textbf{$\mathrm{RBoN}_{\mathrm{L}}$} & \multicolumn{1}{c}{$20$} & \multicolumn{1}{c}{$0.5$} & \multicolumn{1}{c}{$0.2$} & \multicolumn{1}{c}{$20$} & \multicolumn{1}{c}{$15.0$} \\
\textbf{$\mathrm{SRBoN}_{\mathrm{WD}}$} & \multicolumn{1}{c}{$0.5$} & \multicolumn{1}{c}{$0.0002$} & \multicolumn{1}{c}{$0.0001$} & \multicolumn{1}{c}{$0.0001$} & \multicolumn{1}{c}{$1.0$} \\
\textbf{$\mathrm{SRBoN}_{\mathrm{KL}}$} & \multicolumn{1}{c}{$20$} & \multicolumn{1}{c}{$0.05$} & \multicolumn{1}{c}{$0.05$} & \multicolumn{1}{c}{$20$} & \multicolumn{1}{c}{$20$} \\
\midrule
\multicolumn{6}{c}{\textbf{Harmlessness}} \\ \midrule
\textbf{$\mathrm{RBoN}_{\mathrm{WD}}$} & \multicolumn{1}{c}{$20$} & \multicolumn{1}{c}{$1.0$} & \multicolumn{1}{c}{$1.0$} & \multicolumn{1}{c}{$0.0001$} & \multicolumn{1}{c}{$5.0$} \\
\textbf{$\mathrm{RBoN}_{\mathrm{KL}}$ } & \multicolumn{1}{c}{$ 0.0001$} & \multicolumn{1}{c}{$ 0.0001$} & \multicolumn{1}{c}{$ 0.0001$} & \multicolumn{1}{c}{$ 0.0001$} & \multicolumn{1}{c}{$ 0.0001$} \\
\textbf{$\mathrm{RBoN}_{\mathrm{L}}$} & \multicolumn{1}{c}{$20$} & \multicolumn{1}{c}{$5.0$} & \multicolumn{1}{c}{$5.0$} & \multicolumn{1}{c}{$0.0001$} & \multicolumn{1}{c}{$20$} \\
\textbf{$\mathrm{SRBoN}_{\mathrm{WD}}$} & \multicolumn{1}{c}{$0.05$} & \multicolumn{1}{c}{$0.0001$} & \multicolumn{1}{c}{ $0.0001$} & \multicolumn{1}{c}{$0.0001$} & \multicolumn{1}{c}{$0.02$} \\
\textbf{$\mathrm{SRBoN}_{\mathrm{KL}}$} & \multicolumn{1}{c}{$20$} & \multicolumn{1}{c}{$0.05$} & \multicolumn{1}{c}{$20$} & \multicolumn{1}{c}{$20$} & \multicolumn{1}{c}{$20$} \\
\midrule
\multicolumn{6}{c}{\textbf{Helpfulness}} \\ \midrule
\textbf{$\mathrm{RBoN}_{\mathrm{WD}}$} & \multicolumn{1}{c}{$ 15.0$} & \multicolumn{1}{c}{$0.05$} & \multicolumn{1}{c}{$ 0.1$} & \multicolumn{1}{c}{$20$} & \multicolumn{1}{c}{$0.5$} \\
\textbf{$\mathrm{RBoN}_{\mathrm{KL}}$} & \multicolumn{1}{c}{$0.0001$} & \multicolumn{1}{c}{$0.0001$} & \multicolumn{1}{c}{$ 0.0001$} & \multicolumn{1}{c}{$ 0.0001$} & \multicolumn{1}{c}{$ 0.0001$} \\
\textbf{$\mathrm{RBoN}_{\mathrm{L}}$} & \multicolumn{1}{c}{$20$} & \multicolumn{1}{c}{$0.02$} & \multicolumn{1}{c}{$0.2$} & \multicolumn{1}{c}{$5.0$} & \multicolumn{1}{c}{$20$} \\
\textbf{$\mathrm{SRBoN}_{\mathrm{WD}}$} & \multicolumn{1}{c}{$0.5$} & \multicolumn{1}{c}{$0.001$} & \multicolumn{1}{c}{$0.005$} & \multicolumn{1}{c}{$5.0$} & \multicolumn{1}{c}{$0.0002$} \\
\textbf{$\mathrm{SRBoN}_{\mathrm{KL}}$} & \multicolumn{1}{c}{$20$} & \multicolumn{1}{c}{$0.05$} & \multicolumn{1}{c}{$20$} & \multicolumn{1}{c}{$20$} & \multicolumn{1}{c}{$20$} \\
\bottomrule
\end{tabular}
\label{tab:optimal_beta}
\end{table}
% \paragraph{Disccusion}
% \cref{res:table} reveals several noteworthy results across the AlpacaFarm, Harmlessness, and Helpfulness datasets.
% The win rate result shows that higher Spearman rank correlation values correspond to better BoN accuracy. This observation aligns with intuition. $\mathrm{RBoN}_{\mathrm{WD}}$ emerges as a consistently strong performance, frequently achieving a win rate above $50 \%$ across various models. Interestingly, the optimal $\beta$ for $\mathrm{RBoN}_{\mathrm{WD}}$ exhibits significant variability, underscoring the importance of careful tuning to maximize performance under specific conditions. This sensitivity to $\beta$ suggests that while $\mathrm{RBoN}_{\mathrm{WD}}$ is generally effective, its optimal implementation requires adjustment to the particular task and reward model at hand.
% In contrast, $\mathrm{RBoN}_{\mathrm{KL}}$, with fixed $\beta = 0.00001$, shows more variable performance. Its efficacy appears to be highly dependent on the specific reward model and domain. This experiment reveals that the stochastic versions ($\mathrm{SRBoN}_{\mathrm{KL}}$, $\mathrm{SRBoN}_{\mathrm{WD}}$) show inferior performance compared to their deterministic counterpart ($\mathrm{RBoN}_{\mathrm{KL}}$, $\mathrm{RBoN}_{\mathrm{WD}}$). This outcome aligns with intuitive expectations, as the Stochastic version solves an optimization problem while accounting for worst-case scenarios.
% MBR approach demonstrates considerable variability in its performance. In some cases, it achieves competitive results, as evidenced by its $57.4\%$ win rate for Harmlessness with SHP-Large. However, it also shows markedly poor performance in other scenarios, such as its $6.1\%$ win rate for helpfulness with RM-Mistral-7B. This inconsistency suggests that MBR's effectiveness is highly problem-dependent.  Despite its simple implementation, $\mathrm{RBoN}_{\mathrm{L}}$ consistently outperformed BoN, achieving a higher win rate on almost all tasks and models without instances of underperformance. Detailed discussion of $\mathrm{RBoN}_{\mathrm{L}}$ is presented in \cref{appendix:length}. 

\subsection{RBoN Sensitiveness of Parameters}\label{Ex:parameter}

\paragraph{Setup.}


% In this section, we evaluate the generalization performance of the model by applying $\beta$
% values \{$1.0\times 10^{-4}$, $2.0\times 10^{-4}$, $5.0\times 10^{-4}$,$1.0\times 10^{-3}$,..., $2.0\times 10^1$\} to the evaluation splits. We also employ several models as proxy reward models, including SHP-Large, SHP-XL, OASST, PairRM, and RM-Mistral-7B. As the gold reward model, we utilize Eurus-RM-7B to evaluate the performance of the proxy models.
% The results are visualized as a plot showing the win rates of each method compared to BoN sampling on the evaluation splits. We assign 1 point for a win and 0.5 points for a tie. 
In this section, we evaluate the generalization performance of the model using $\beta$.
values \{$1.0\times 10^{-4}$, $2.0\times 10^{-4}$, $5.0\times 10^{-4}$,$1.0\times 10^{-3}$,..., $2.0\times 10^1$\} to the evaluation splits. We also use several models as proxy reward models, including OASST, SHP-Large, SHP-XL, PairRM, and RM-Mistral-7B. As a gold reward model, we use Eurus-RM-7B to evaluate the performance of the proxy models.
The results are visualized as a plot showing the win rates of each method compared to BoN sampling on the evaluation splits. We assign 1 point for a win and 0.5 points for a tie. 

% {appendix:all_method}
\paragraph{Results}
The performance result of RBoN method in AlpacaFarm is illustrated in Figures \ref{fig:alpaca-l}.
% The results $\mathrm{RBoN}_{\mathrm{WD}}$ and $\mathrm{SRBoN}_{\mathrm{WD}}$ illustrated in Figures \ref{fig:alpaca-wd}, \ref{fig:harmless-wd}, and \ref{fig:helpful-wd} 
% This result reveals that the optimal parameters for the $\mathrm{RBoN}_{\mathrm{WD}}$ and $\mathrm{SRBoN}_{\mathrm{WD}}$ method vary between different models and reveals the performance of $\mathrm{SRBoN}_{\mathrm{WD}}$ across various problem settings, as the value of the regularization parameter $\beta$ increases, we observe a degradation performance. Intuitively, upon examining the adversarial formulation of $\mathrm{SRBoN}_{\mathrm{WD}}$, we can infer that as the regularization parameter $\beta$ increases, the magnitude of potential perturbations $\Delta R$ also increases. Furthermore, as evidenced in \cref{tab:optimal_beta}, the optimal $\beta$ value for $\mathrm{SRBoN}_{\mathrm{WD}}$ is typically smaller than that for $\mathrm{RBoN}_{\mathrm{WD}}$. 
This result reveals that the optimal parameters for the $\mathrm{RBoN}_{\mathrm{WD}}$ and $\mathrm{SRBoN}_{\mathrm{WD}}$ method vary between different models and reveals the performance of $\mathrm{SRBoN}_{\mathrm{WD}}$ across various problem settings, as the value of the regularization parameter $\beta$ increases, we observe a degradation performance. Intuitively, upon examining the adversarial formulation of $\mathrm{SRBoN}_{\mathrm{WD}}$, we can infer that as the regularization parameter $\beta$ increases, the magnitude of potential perturbations $\Delta R$ also increases. Furthermore, as evidenced in \cref{tab:optimal_beta}, the optimal $\beta$ value for $\mathrm{SRBoN}_{\mathrm{WD}}$ is typically smaller than that for $\mathrm{RBoN}_{\mathrm{WD}}$. 



% This result shows that $\mathrm{SRBoN}_{\mathrm{KL}}$ consistently underperforms within the $\beta$ range examined in our experiments. Notably, as shown in \cref{tab:optimal_beta}, the optimal regularization parameter $\beta^*$ for $\mathrm{SRBoN}_{\mathrm{KL}}$ is frequently found to be $\beta^*=20$ across various problem settings. This observation leads to an intriguing hypothesis, the performance of $\mathrm{SRBoN}_{\mathrm{KL}}$ might potentially improve with higher values of $\beta$. 
\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{exp_img/Figure_stochastic/l_bon/alpaca.pdf}
    \caption{
   Evaluation of RBoN sensitiveness on the AlpacaFarm dataset with varying parameter $\beta$. We use proxy reward models, OASST, SHP-Large, SHP-XL, PairRM, and RM-Mistral-7B. As the gold reward model, we utilize Eurus-RM-7B.
    }
    \label{fig:alpaca-l}
\end{figure}
This result shows that $\mathrm{SRBoN}_{\mathrm{KL}}$ consistently underperforms within the $\beta$ range examined in our experiments. In particular, as shown in \cref{tab:optimal_beta}, the optimal regularization parameter $\beta^*$ for $\mathrm{SRBoN}_{\mathrm{KL}}$ is often found to be $\beta^*=20$ across different problem settings. This observation leads to an intriguing hypothesis, that the performance of $\mathrm{SRBoN}_{\mathrm{KL}}$ could potentially improve with higher values of $\beta$. 

% The performance results of $\mathrm{RBoN}_{\mathrm{L}}$ are illustrated in 
% % Figures \ref{fig:alpaca-l}, \ref{fig:harmless-l}, and \ref{fig:helpful-l}. 
% The performance result of $\mathrm{RBoN}_{\mathrm{L}}$ demonstrates superior performance across a wide range of $\beta$ values, exhibiting performance characteristics comparable to $\mathrm{RBoN}_{\mathrm{WD}}$. Notably, this robust performance across varying $\beta$ values indicates that $\mathrm{RBoN}_{\mathrm{L}}$ exhibits low sensitivity to changes in the regularization parameter.

The performance result of $\mathrm{RBoN}_{\mathrm{L}}$ demonstrates superior performance across a wide range of $\beta$ values, exhibiting performance characteristics comparable to $\mathrm{RBoN}_{\mathrm{WD}}$. Notably, this robust performance across varying $\beta$ values indicates that $\mathrm{RBoN}_{\mathrm{L}}$ exhibits low sensitivity to changes in the regularization parameter.

The results for Harmlessness and Helpfulness datasets are presented in \cref{appendix:all_method}.

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.95\linewidth]{exp_img/Figure_stochastic/wd/alpaca.pdf}
%     \caption{
%    Evaluation of $\mathrm{RBoN}_{\mathrm{WD}}$ and $\mathrm{SRBoN}_{\mathrm{WD}}$ sensitiveness on the AlpacaFarm dataset with varying parameter $\beta$. We use proxy reward models, SHP-Large, SHP-XL, OASST, PairRM, and RM-Mistral-7B. As the gold reward model, we utilize Eurus-RM-7B.
%     }
%     \label{fig:alpaca-wd}
% \end{figure}
% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.95\linewidth]{exp_img/Figure_stochastic/wd/hh-harmless.pdf}
%     \caption{
%     Evaluation of $\mathrm{RBoN}_{\mathrm{WD}}$ and $\mathrm{SRBoN}_{\mathrm{WD}}$ sensitiveness on the Harmlessness subset of the hh-rlhf dataset with varying parameter $\beta$. We use proxy reward models, SHP-Large, SHP-XL, OASST, PairRM, and RM-Mistral-7B. As the gold reward model, we utilize Eurus-RM-7B.
%     }
%     \label{fig:harmless-wd}
% \end{figure}

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.95\linewidth]{exp_img/Figure_stochastic/wd/hh-helpful.pdf}
%     \caption{
%     Evaluation of $\mathrm{RBoN}_{\mathrm{WD}}$ and $\mathrm{SRBoN}_{\mathrm{WD}}$ sensitiveness on the Helpfulness subset of the hh-rlhf dataset with varying parameter $\beta$. We use proxy reward models, SHP-Large, SHP-XL, OASST, PairRM, and RM-Mistral-7B. As the gold reward model, we utilize Eurus-RM-7B.
%     }
%     \label{fig:helpful-wd}
% \end{figure}

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.95\linewidth]{exp_img/Figure_stochastic/kl/alpaca.pdf}
%     \caption{
%    Evaluation of $\mathrm{RBoN}_{\mathrm{KL}}$ and $\mathrm{SRBoN}_{\mathrm{KL}}$ sensitiveness on the AlpacaFarm dataset with varying parameter $\beta$. We use proxy reward models, SHP-Large, SHP-XL, OASST, PairRM, and RM-Mistral-7B. As the gold reward model, we utilize Eurus-RM-7B.
%     }
%     \label{fig:alpaca-kl}
% \end{figure}
% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.95\linewidth]{exp_img/Figure_stochastic/kl/hh-harmless.pdf}
%     \caption{
%     Evaluation of $\mathrm{RBoN}_{\mathrm{KL}}$ and $\mathrm{SRBoN}_{\mathrm{KL}}$ sensitiveness on the Harmlessness subset of the hh-rlhf dataset with varying parameter $\beta$. We use proxy reward models, SHP-Large, SHP-XL, OASST, PairRM, and RM-Mistral-7B. As the gold reward model, we utilize Eurus-RM-7B.
%     }
%     \label{fig:harmless-kl}
% \end{figure}

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.95\linewidth]{exp_img/Figure_stochastic/kl/hh-helpful.pdf}
%     \caption{
%     Evaluation of $\mathrm{RBoN}_{\mathrm{KL}}$ and $\mathrm{SRBoN}_{\mathrm{KL}}$ sensitiveness on the Helpfulness subset of the hh-rlhf dataset with varying parameter $\beta$. We use proxy reward models, SHP-Large, SHP-XL, OASST, PairRM, and RM-Mistral-7B. As the gold reward model, we utilize Eurus-RM-7B.
%     }
%     \label{fig:helpful-kl}
% \end{figure}





% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.95\linewidth]{exp_img/Figure_stochastic/l_bon/hh-harmless.pdf}
%     \caption{
%     Evaluation of RBoN sensitiveness on the Harmlessness subset of the hh-rlhf dataset with varying parameter $\beta$. We use proxy reward models, SHP-Large, SHP-XL, OASST, PairRM, and RM-Mistral-7B. As the gold reward model, we utilize Eurus-RM-7B.
%     }
%     \label{fig:harmless-l}
% \end{figure}

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.95\linewidth]{exp_img/Figure_stochastic/l_bon/hh-helpful.pdf}
%     \caption{
%     Evaluation of RBoN sensitiveness on the Helpfulness subset of the hh-rlhf dataset with varying parameter $\beta$. We use proxy reward models, SHP-Large, SHP-XL, OASST, PairRM, and RM-Mistral-7B. As the gold reward model, we utilize Eurus-RM-7B.
%     }
%     \label{fig:helpful-l}
% \end{figure}