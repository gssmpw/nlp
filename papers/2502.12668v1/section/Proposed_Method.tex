% \section{Proposed Methods}
\section{Stochastic RBoN (SRBoN)}
% \section{Probabilistic RBoN (SRBoN)}
 % The optimal policy $\pi$ of RBoN is not guaranteed to be deterministic, as it is in the unregularized case (BoN). To address this limitation and facilitate a more comprehensive analysis, we propose the stochastic version of RBoN, Stochastic $\mathrm{RBoN}_{\mathrm{KL}}$ (Section \ref{propose:kl}) and Stochastic $\mathrm{RBoN}_{\mathrm{WD}}$ (Section \ref{propose:WD}). These novel algorithms, while similar to the original RBoN (deterministic version), allows for a probabilistic output distribution. By relaxing the deterministic constraint, we can apply theoretical tools that were previously inaccessible. Our approach focuses on analyzing this stochastic version, aiming to provide theoretical results that shed light on the underlying mechanisms of RBoN's effectiveness.

% The optimal policy $\pi$ of RBoN is not guaranteed to be deterministic, as it is in the unregularized case (BoN). To address this limitation and to allow for a more comprehensive analysis, we propose the stochastic version of RBoN, Stochastic $\mathrm{RBoN}_{\mathrm{KL}}$ (Section \ref{propose:kl}) and Stochastic $\mathrm{RBoN}_{\mathrm{WD}}$ (Section \ref{propose:WD}). 
We propose the stochastic version of RBoN, Stochastic $\mathrm{RBoN}_{\mathrm{KL}}$ (Section \ref{propose:kl}) and Stochastic $\mathrm{RBoN}_{\mathrm{WD}}$ (Section \ref{propose:WD}). 
These novel algorithms, while similar to the original RBoN (deterministic version), allow for the optimal policy $\pi$ to a probabilistic output distribution. By relaxing the deterministic constraint, we can apply theoretical tools that were previously inaccessible. Our approach focuses on the analysis of this stochastic version, aiming to provide theoretical results that shed light on the underlying mechanisms of RBoN's effectiveness.

% Although $\mathrm{RBoN}_{\mathrm{KL}}$ optimizes the same objective as the RRL problem, it is constrained to a deterministic policy. However, the optimal solution to the RRL problem is not necessarily deterministic. In fact, for many of the regularization terms (e.g., KL-divergence), the optimal policy is non-deterministic.
% As such, $\mathrm{RBoN}_{\mathrm{KL}}$ is only a proxy of the optimal solution of the RRL problem.
% We 

% \subsection{Stochastic $\mathrm{RBoN}_{\mathrm{KL}}$ ($\mathrm{RBoN}_{\mathrm{SKL}}$)}\label{propose:kl}
\subsection{Stochastic $\mathrm{RBoN}_{\mathrm{KL}}$ ($\mathrm{SRBoN}_{\mathrm{KL}}$)}\label{propose:kl}
% While $\mathrm{RBoN}_{\mathrm{KL}}$ solves deterministic output probabilities, $\mathrm{RBoN}_{\mathrm{SKL}}$ relaxes this constraint. We now consider an optimization problem over a probabilistic space.
% We first consider a stochastic version of $\mathrm{RBoN}_{\mathrm{KL}}$.
% The policy of $\mathrm{SRBoN}_{\mathrm{KL}}$ is given by:
First, consider a stochastic version of $\mathrm{RBoN}_{\mathrm{KL}}$.
The policy of $\mathrm{SRBoN}_{\mathrm{KL}}$ is given by:

\begin{equation}
\begin{aligned}
% \textbf{Objective Function of $\mathrm{SRBoN}_{\mathrm{KL}}$} &= \max_{\pi \in \Pi} \,\, \langle \pi, R \rangle - \beta \sum_\mathcal{Y_{\textbf{ref}}} \pi(y)\log{\frac{\pi(y)}{\pi_{\textbf{ref}} (y)}}\\
% \pi_{\mathrm{SRBoN}_{\mathrm{KL}}} &= \argmax_{\pi \in \Pi} \,\, \langle \pi, R \rangle - \beta \sum_\mathcal{Y_{\textbf{ref}}} \pi(y)\log{\frac{\pi(y)}{\pi_{\textbf{ref}} (y)}}\\
\pi_{\mathrm{SRBoN}_{\mathrm{KL}}}(x) 
% &= \argmax_{\pi \in \Pi} \,\, \langle \pi, R \rangle - \beta \KL \left[\pi \| \pi_{\textbf{ref}}\right]\\
% &=\argmax_{\pi \in \Pi}\mathbb{E}_{y \sim \pi(\cdot \mid x)}[R(x,y)]  - \beta \KL \left[\pi \| \pi_{\textbf{ref}}\right]\\
&=\argmax_{\pi \in \Pi}\mathbb{E}_{y \sim \pi(\cdot \mid x)}[R(x,y)]  - \beta \KL \left[\pi(\cdot \mid x) \| \pi_{\textbf{ref}}(\cdot \mid x)\right]\\
&= \argmax_{\pi \in \Pi} f_\mathrm{RRL}^{\mathrm{KL}}(\pi).
\end{aligned}
\end{equation}
We define $\mathrm{SRBoN}_{\mathrm{KL}}$ as a method to sample a response $y$ that follows the probability distribution of $\pi_{\mathrm{SRBoN}_{\mathrm{KL}}}$:
\begin{equation}\label{eq:srbonkl}
    y_{\mathrm{SRBoN}_{\mathrm{KL}}}(x) \sim \pi_{\mathrm{SRBoN}_{\mathrm{KL}}}(x).
\end{equation}
% \begin{equation}\label{eq:srbonkl}
%     y \sim \pi_{\mathrm{SRBoN}_{\mathrm{KL}}}(x).
% \end{equation}
In Section \ref{sec:exp} we evaluate the performance of this stochastic text generation algorithm defined by Eq. (\ref{eq:srbonkl}).
% \yuu{Is $\argmax \pi_{\mathrm{SRBoN}_{\mathrm{KL}}}$ equal to the output of the deterministic version? I guess it will be because the KL divergence is separable for each y. TODO: need to verify it. Well do we care about KL anyway? it doesn't work in practice so may not be that relevant.} \yuki{$\log \pi^* = R + \log \pi_{\textbf{ref}}$}
% The deterministic version corresponds to the maximum-a-posteriori solution from the computed policy:
% \begin{equation}
%     y_{\mathrm{RBoN}_{\mathrm{KL}}}(x) = \argmax_{y \in \mathcal{Y}_\mathrm{cand}} \pi_{\mathrm{SRBoN}_{\mathrm{KL}}}(x).
% \end{equation}

% where $\Pi$ is a set of probabilities.


\subsubsection{Theoretical Guarantee of $\mathrm{SRBoN}_{\mathrm{KL}}$}\label{sec:kl_sec}

By relaxing the deterministic policy constraint of $\mathrm{RBoN}_{\mathrm{KL}}$, $\mathrm{SRBoN}_{\mathrm{KL}}$ follows the formulation of the RRL with adversarial perturbations studied by \citet{brekelmans2022your}. 
As such, the computation of $\mathrm{SRBoN}_{\mathrm{KL}}$ can be transformed into a max-min problem using Legendre-Fenchel transformation \citep{touchette2005legendre} as in Eq. (\ref{eq:rrl-dual}). 
In this way, $\mathrm{SRBoN}_{\mathrm{KL}}$ has the following theoretical guarantee proven by \citet{brekelmans2022your}:

% In BoN methodology, all variables input x are predetermined. Consequently, our subsequent analysis focuses exclusively on the output y, and we formulate and examine the mathematical expressions accordingly.

% The objective function of $\mathrm{SRBoN}_{\mathrm{KL}}$ is $f_\mathrm{RRL}^{\mathrm{KL}}(\pi)$.

% \begin{equation}\label{eq:kl_ind}
% \begin{aligned}
% f_\mathrm{RRL}^{\mathrm{KL}}(\pi) &:= \langle \pi, R \rangle - \beta \KL \left[\pi_y \| \pi_{\textbf{ref}}(\cdot \mid x)\right]
% % \textbf{Objective Function of $\mathrm{RBoN}_{\mathrm{SKL}}$} 
% % &= \max_{\pi \in \Pi}  \,\, \langle \pi, R \rangle - \beta \sum_\mathcal{Y_{\textbf{ref}}} \pi (y)\log{\frac{\pi(y)}{\pi_{\textbf{ref}} (y)}}
% \end{aligned}
% \end{equation}
% where $\langle \pi, R \rangle = \sum_{y \in \mathcal{Y_{\textbf{ref}}}} \pi(y)R(y)$, reward function $R$ $:\mathcal{Y}  \rightarrow \mathbb{R}$, output probability $\pi$ $\in$ $ \Delta (\mathcal{Y})$, KL divergence function $\Omega(\pi) = \beta \textbf{KL} (\pi || \pi_{\textbf{ref}}) = \beta \sum_\mathcal{Y_{\textbf{ref}}} \pi (y)\log{\frac{\pi(y)}{\pi_{\textbf{ref}} (y)}}$. 

% The regularization term of $f_\mathrm{RRL}^{\mathrm{KL}}(\pi)$ is a KL-divergence which is a strongly convex function. Thus, the problem of maximizing $f_\mathrm{RRL}^{\mathrm{KL}}(\pi)$ can be interpreted as an optimization problem with an adversarial perturbation $\Delta R$ \citep{brekelmans2022your}:

% \yuu{Should it be Proposition or Theorem?}
\begin{theorem}(\textbf{\cite{brekelmans2022your}, Proposition 1})\label{theory:kl-minmax}
% \begin{theorem}\textbf{($\mathrm{SRBoN}_{\mathrm{KL}}$ is a robust policy)}\label{theory:kl-minmax}
The problem of maximizing $f_\mathrm{RRL}^{\mathrm{KL}}(\pi)$ can be interpreted as a robust optimization problem with an adversarial perturbation $\Delta R$:
\begin{equation}
\begin{aligned}
\argmax_{\pi \in \Pi} f_\mathrm{RRL}^{\mathrm{KL}}(\pi) 
% &= \argmax_{\pi \in \Pi} \,\, \min_{\Delta R \in \mathcal{R}_{\Delta}} \,\, \langle \pi, R - \Delta R \rangle,\\
&= \argmax_{\pi \in \Pi} \,\, \min_{\Delta R \in \mathcal{R}_{\Delta}} \mathbb{E}_{y \sim \pi(\cdot \mid x)} [R(x,y) - \Delta R(x,y)],
% f_\mathrm{RRL}^{\mathrm{KL}}(\pi) &= \min_{\Delta R \in \mathcal{R}_{\Delta}} \,\, \langle \pi, R - \Delta R \rangle,
\end{aligned}
\end{equation}
% where the feasible set of reward perturbations $\mathcal{R}_{\Delta}$ available to the adversary is constrained as: 
where the feasible set of reward perturbations $\mathcal{R}_{\Delta}$ available to the adversary is bounded: 
\begin{equation}
\mathcal{R}_{\Delta} := \left\{\Delta R \in \mathbb{R}^{\mathcal{X}\times\mathcal{Y}_{\textnormal{\textbf{ref}}}} \mid \sum_\mathcal{Y_{\textnormal{\textbf{ref}}}} \pi_{\textnormal{\textbf{ref}}}(y \mid x) \exp(\beta^{-1}\Delta R(x,y)) \leq 1\right\}
\label{eq:noisedomain}
\end{equation}
\end{theorem}
% \begin{proof}
%     See Appendix D.1 in \citet{brekelmans2022your} for proof.
% \end{proof}

% % \begin{equation*}
% \begin{aligned}
% \textnormal{\textbf{Objective Function of $\mathrm{RBoN}_{\mathrm{SKL}}$}}&=\max_{\pi \in \Pi} \,\, \min_{\Delta R \in \mathcal{R}_{\Delta}} \,\, \langle \pi, R - \Delta R \rangle  +  \beta \log \sum_\mathcal{Y_{\textnormal{\textbf{ref}}}} \pi_{\textnormal{\textbf{ref}}}(y) \exp(\beta^{-1}\Delta R(y)) 
% \end{aligned}
% \end{equation*}
% \begin{equation*}
% \text{where}\quad \mathcal{R}_{\Delta}:=\left\{\Delta R \in \mathbb{R}^{\mathcal{Y}_{\textbf{ref}}} \mid \sum_\mathcal{Y_{\textbf{ref}}} \pi_{\textnormal{\textbf{ref}}}(y) \exp(\beta^{-1}\Delta R(y)) \leq 1\right\}
% \end{equation*}
% \end{theorem}
\

% Eq. \ref{eq:kl_ind} includes a regularization term and can be interpreted as incorporating adversarial perturbations when subjected to reformulations. 
% Based on the work of \citet{brekelmans2022your}, we can formulate the following max-min problem:
% \begin{theorem}(\textbf{\cite{brekelmans2022your} Proposition 1})\label{theory:kl-minmax}
% The following holds:
% \begin{equation*}
% \begin{aligned}
% \textnormal{\textbf{Objective Function of $\mathrm{RBoN}_{\mathrm{SKL}}$}}&=\max_{\pi \in \Pi} \,\, \min_{\Delta R \in \mathcal{R}_{\Delta}} \,\, \langle \pi, R - \Delta R \rangle  +  \beta \log \sum_\mathcal{Y_{\textnormal{\textbf{ref}}}} \pi_{\textnormal{\textbf{ref}}}(y) \exp(\beta^{-1}\Delta R(y)) 
% \end{aligned}
% \end{equation*}
% \begin{equation*}
% \text{where}\quad \mathcal{R}_{\Delta}:=\left\{\Delta R \in \mathbb{R}^{\mathcal{Y}_{\textbf{ref}}} \mid \sum_\mathcal{Y_{\textbf{ref}}} \pi_{\textnormal{\textbf{ref}}}(y) \exp(\beta^{-1}\Delta R(y)) \leq 1\right\}
% \end{equation*}
% \end{theorem}


The theorem shows that $\mathrm{SRBoN}_{\mathrm{KL}}$ is an algorithm that optimizes the worst-case performance under the assumption that the error between the true reward and the given proxy reward model is guaranteed to be within $\mathcal{R}_{\Delta}$ (Eq. (\ref{eq:noisedomain})). 

% Intuitively, $\mathrm{SRBoN}_{\mathrm{KL}}$ is a robust solution under uncertainty about the accuracy of the proxy reward model where the uncertainty is represented in Eq. (\ref{eq:noisedomain}).
Let $\mathcal{R}^\prime$ be a set of possible reward models under the reward perturbations: $\mathcal{R}^\prime := \{R - \Delta R \mid \Delta R \in \mathcal{R}_{\Delta}\}$.
Let $f_\mathrm{RRL}^{\mathrm{KL}}(\pi; R)$ be the objective of the policy given a (proxy) reward model $R$. Then,
\begin{align}
\argmax_{\pi \in \Pi} f_\mathrm{RRL}^{\mathrm{KL}}(\pi; R) &= \argmax_{\pi \in \Pi} \,\, \min_{\Delta R \in \mathcal{R}_{\Delta}}\mathbb{E}_{y \sim \pi(\cdot \mid x)}[R(x,y) - \Delta R(x,y)] \nonumber\\
% &= \argmax_{\pi \in \Pi} \,\, \min_{\Delta R \in \mathcal{R}_{\Delta}} \,\, \langle \pi, R - \Delta R \rangle \nonumber\\
% &= \argmax_{\pi \in \Pi} \,\, \min_{R^\prime \in \mathcal{R}^\prime} \,\, \langle \pi, R^{\prime} \rangle \nonumber\\
&= \argmax_{\pi \in \Pi} \,\, \min_{R^\prime \in \mathcal{R}^\prime} \mathbb{E}_{y \sim \pi(\cdot \mid x)}[ R^{\prime}(x,y) ] \nonumber\\
&= \argmax_{\pi \in \Pi} \min_{R^\prime \in \mathcal{R}^\prime} f_\mathrm{RRL}^{\mathrm{KL}}(\pi; R^\prime).
\end{align}
% Thus, $\mathrm{SRBoN}_{\mathrm{KL}}$ is robustly optimizing the policy for a set of possible reward models in $\mathcal{R}^\prime$. In other words, it assumes that the true reward model is in $\mathcal{R}_{\Delta}$ and optimizes for the worst possible case.
Thus, $\mathrm{SRBoN}_{\mathrm{KL}}$ is a robust optimization of the policy for a set of possible reward models in $\mathcal{R}^\prime$. In other words, it assumes that the true payoff model is in $\mathcal{R}_{\Delta}$ and optimizes for the worst case.

% Note that this theoretical guarantee holds for $\mathrm{SRBoN}_{\mathrm{KL}}$ but not for the deterministic version $\mathrm{RBoN}_{\mathrm{KL}}$. \yuu{or does it hold for the deterministic version?}

% The theorem is derived by translating the proposition proven by \citet{brekelmans2022your} for the generic regularized RL problems to the text generation scenario. 
% The contribution of our work is to show the relationship of their theoretical finding to the RBoN sampling algorithm in LLM alignment.
The theorem is derived by translating the proposition proved by \citet{brekelmans2022your} for the generic RRL problems to the text generation scenario. 
The contribution of our work is to show the relation of their theoretical result to the RBoN sampling algorithm in LLMs alignment.

% For a comprehensive derivation of \cref{theory:kl-minmax}, readers are directed to Proposition 1 in \cite{brekelmans2022your}. 
% This finding implies that incorporating a regularization term in BoN sampling produces an effect equivalent to introducing adversarial reward perturbations. 
%The KL-divergence of $\mathrm{SRBoN}_{\mathrm{KL}}$ can be considered as the regularizer for $\pi$ to optimize under an adversarial reward perturbation in the worst case. This translates into modified rewards $R^{\prime}(y)=R(y)-\Delta R(y)$ for the output probability $\pi$. Understanding the range of perturbation reward $\Delta R$ (constraint term) is crucial in optimizing the modified reward function $R^\prime$. This knowledge constitutes a key component in the optimization process. 


% \paragraph{Intuition}
% The algorithm can achieve robust learning in uncertainty by minimizing reward modifications for these high-probability outputs.




\subsection{Stochastic $\mathrm{RBoN}_{\mathrm{WD}}$ ($\mathrm{SRBoN}_{\mathrm{WD}}$)}\label{propose:WD}
% Following the approach in Section \ref{propose:kl}, we extend the concept of $\mathrm{RBoN}_{\mathrm{WD}}$ beyond its original formulation with deterministic probabilities. 
We now consider an optimization problem over a space of probability functions, to derive an optimal probabilistic policy $\pi$ with the Wasserstein distance as the regularization term. 
The objective function of $\mathrm{RBoN}_{\mathrm{SWD}}$ is the following:

% \begin{equation}\label{eq:wdr}
%     \begin{aligned}eq:maxmin
%      \pi^* &= \max_\pi \,\, \langle \pi ,R \rangle -\beta WD(\pi, \pi_{\textbf{ref}})\\
%      \end{aligned}
% \end{equation}
% \begin{equation}
%     \begin{aligned}
%      \textbf{Objective Function of $\mathrm{RBoN}_{\mathrm{SWD}}$} &= \max_{\pi \in \Pi} \,\, \langle \pi ,R \rangle -\beta \textbf{WD} [\pi_{\textbf{ref}}(\cdot) \| \pi (\cdot)]\\
%      &= \max_{\pi \in \Pi} f_\mathrm{RRL}^{\mathrm{WD}}(\pi).
%      \end{aligned}
% \end{equation}
\begin{equation}
    \begin{aligned}
     \pi_{\mathrm{SRBoN}_\mathrm{WD}}(x)
     % &= \argmax_{\pi \in \Pi} \,\, \langle \pi ,R \rangle -\beta \textbf{WD} [\pi_{\textbf{ref}} \| \pi]\\
     % &= \argmax_{\pi \in \Pi} \mathbb{E}_{y \sim \pi(\cdot \mid x)}[R(x,y)]  -\beta \textbf{WD} [\pi_{\textbf{ref}} \| \pi]\\
          &= \argmax_{\pi \in \Pi} \mathbb{E}_{y \sim \pi(\cdot \mid x)}[R(x,y)]  -\beta \textbf{WD} [\pi_{\textbf{ref}} (\cdot \mid x) \| \pi (\cdot \mid x)]\\
     &= \argmax_{\pi \in \Pi} f_\mathrm{RRL}^{\mathrm{WD}}(\pi).
     \label{eq:srbonwd}
     \end{aligned}
\end{equation}

% \yuu{Is $\argmax \pi_{\mathrm{SRBoN}_{\mathrm{WD}}}$ equal to the output of the deterministic version? Not necessarily. There may be two modes in the distribution. Well, then we cannot make a direct connection to the deterministic version...}

\subsubsection{Theoretical Guarantee of $\mathrm{SRBoN}_{\mathrm{WD}}$}\label{sec:WD}
% This section advances two primary arguments, following the structure of the previous section: (1) the reformulation of $\mathrm{RBoN}_{\mathrm{SWD}}$ as a max-min problem. (2) the characterization of the perturbation range for $\Delta R$. 

% The equation setting $\mathrm{RBoN}_{\mathrm{SWD}}$, the objective function of $\mathrm{RBoN}_{\mathrm{SWD}}$ is given by:


% \begin{equation}\label{eq:wdr}
%     \begin{aligned}
%      \pi^* &= \max_\pi \,\, \langle \pi ,R \rangle -\beta WD(\pi, \pi_{\textbf{ref}})\\
%      \end{aligned}
% \end{equation}
% \begin{equation}\label{eq:wdr}
%     \begin{aligned}
%      \textbf{Objective Function of $\mathrm{RBoN}_{\mathrm{SWD}}$} &= \max_{\pi \in \Pi} \,\, \langle \pi ,R \rangle -\beta \textbf{WD} [\pi_{\textbf{ref}}(\cdot) \| \pi (\cdot)]\\
%      \end{aligned}
% \end{equation}

Similar to $\mathrm{SRBoN}_{\mathrm{KL}}$, $\mathrm{SRBoN}_{\mathrm{WD}}$ can also be reformulated as a max-min problem, and thus we can show that it optimizes the worst-case performance under certain constrain:
% Similar to $\mathrm{SRBoN}_{\mathrm{KL}}$, $\mathrm{SRBoN}_{\mathrm{WD}}$ (Eq. \ref{eq:srbonwd}) can be reformulated as the max-min problem. 
% \yuki{From linear form in terms of $\pi$, the $\arg\max \pi_{\textbf{SRBoN}_{\text{WD}}}$ might be satisfied by the deterministic version.}
\begin{theorem}\label{theory:wd}
% The following holds:
% The objective function of $\mathrm{RBoN}_{\mathrm{SWD}}$ (Eq. \ref{eq:wdr}) can be reformulated as the max-min problem. 
%     \begin{equation*}
% \textbf{Objective Function of $\mathrm{RBoN}_{\mathrm{SWD}}$} = \max_{\pi} \min_{\Delta R} \left\langle \pi, R - \beta \Delta R \right\rangle + \beta \left\langle \pi_{\text{ref}}, \Delta R \right\rangle
% \end{equation*}
% \end{theorem}
The problem of maximizing $f_\mathrm{RRL}^{\mathrm{WD}}(\pi)$ can be interpreted as a robust optimization problem with an adversarial perturbation $\Delta R$:
\begin{equation}
    % \argmax_{\pi \in \Pi} f_\mathrm{RRL}^{\mathrm{WD}}(\pi) = \argmax_{\pi \in \Pi} \,\,\min_{\Delta R \in \mathcal{R}_{\Delta}}\,\, \left\langle \pi, R - \beta \Delta R \right\rangle + \beta \left\langle \pi_{\textnormal{\textbf{ref}}}, \Delta R \right\rangle
    \argmax_{\pi \in \Pi} f_\mathrm{RRL}^{\mathrm{WD}}(\pi) = \argmax_{\pi \in \Pi} \,\,\min_{\Delta R \in \mathcal{R}_{\Delta}}\mathbb{E}_{y \sim \pi(\cdot \mid x)}[R(x,y) - \beta \Delta R(x,y)] + \beta \sum_{y \in \mathcal{Y}_{\textbf{ref}}} \pi_{\textnormal{\textbf{ref}}}(y \mid x)\Delta R(x,y)
\end{equation}
% where the feasible set of reward perturbations $\mathcal{R}_{\Delta}$ available to the adversary is constrained as:
where the feasible set of reward perturbations $\mathcal{R}_{\Delta}$ available to the adversary is bounded:
\begin{equation}\label{eq:wd_delta_set}
\mathcal{R}_{\Delta}:=\left\{\Delta R \in \mathbb{R}^{\mathcal{X}\times\mathcal{Y}_{\textbf{ref}}} \mid \left|\Delta R(x,y)-\Delta R\left(x,y^{\prime}\right)\right| \leq C\left(y, y^{\prime}\right) \quad \forall y, y^{\prime} \in \mathcal{Y}_{\textnormal{\textbf{ref}}}\right\},
\end{equation}
% and 
% \begin{equation}
%     \Delta R \in L^1(\pi_{\textbf{ref}}),\;\; L^1(\pi_{\textbf{ref}})=\left\{f:  \mathcal{Y} \rightarrow \mathbb{R}| \sum_{y \in \mathcal{Y}_{\textbf{ref}}}|f(y)|  \pi_{\textbf{ref}}(y)<\infty\right\}. 
% \end{equation}
\end{theorem}
% \yuu{Do we need to state the constraint of $\Delta R$ being an $L^1$ function? For example, doesn't $\Delta R: \mathcal{Y}_{ref} \rightarrow \mathbb{R}$ imply that $\Delta R$ is a $L^1$ function, because $\Delta R$, $\pi_{ref}$, and $|\mathcal{Y}_{ref}|$ are all bounded so it won't go infinite?}
% \begin{proof}
%     The proof is in Appendix~\ref{appendix:wd-thoery}.
% \end{proof}
The proof is provided in Appendix~\ref{appendix:wd-thoery}.

% The derivation of this equation and the detailed analysis of the perturbation reward range are presented in \cref{appendix:wd-thoery}.
% \paragraph{Intuition}

% This expression represents an optimization problem involving strategies $\pi$ and perturbation $\Delta R$. The goal is to find the optimal strategy $\pi^*$ under the modified reward $R^\prime$ ($= R-\beta \Delta R$). 
This expression represents an optimization problem with strategies $\pi$ and perturbation $\Delta R$. The goal is to find the optimal strategy $\pi^*$ under the modified reward $R^\prime$ ($= R-\beta \Delta R$). 
% \yuu{TODO: Wouldn't it optimizing $R^\prime + \beta \left\langle \pi_{\textnormal{\textbf{ref}}}, \Delta R \right\rangle$? The second term is not dependent on the $\pi$ so it won't make the optimal choice of $\pi$ for each $\Delta R$, but its max-min does change as its value is not a constant with respect to $\Delta R$. We need to give a qualitative explanation of the second term and then we can describe what the theoretical guarantee is speaking of.}

% The intuition of the second term $\beta \left\langle \pi_{\textnormal{\textbf{ref}}}, \Delta R \right\rangle$ is that \yuu{TODO: explain the intuition of the second term.}

The intuition behind the second term $\sum_{y \in \mathcal{Y}{\textnormal{\textbf{ref}}}} \pi_{\textnormal{\textbf{ref}}}(y \mid x)\Delta R(x,y)$ can be understood by examining $\Delta R$ constraints (Eq. (\ref{eq:wd_delta_set})). 
% While this feasible set does not explicitly constrain $\Delta R$ to avoid tremendous values which is accomplished with $\min_{\Delta R} \mathbb{E}_{\pi}[R(x,y)-\beta \Delta R(x,y)] $. The second term can help avoid these huge values. 
While this feasible set does not explicitly constrain \(\Delta R\) to avoid large values, the second term, \(\min_{\Delta R} \mathbb{E}_{\pi}[R(x,y)-\beta \Delta R(x,y)]\), helps to avoid such huge values. Additionally, it reveals a mechanism that inherently limits the magnitude of perturbations for actions that have high probability under $\pi_{\textnormal{\textbf{ref}}}$ and this is consistent with the WD distance intuition.

We have analyzed the role of the regularization term for BoN sampling in the previous \cref{sec:kl_sec} and \cref{sec:WD}. Since the previous study \citep{jinnai2024regularized}  imposed deterministic constraints, the results are not exactly the same, but we consider that the analysis performed here helps to explain why the previous study performed better.
% The intuition behind the second term $\sum_{y \in \mathcal{Y}{\textnormal{\textbf{ref}}}} \pi_{\textnormal{\textbf{ref}}}(y \mid x)\Delta R(x,y)$ can be understood by examining the $\Delta R$ constraints (Eq. \ref{eq:wd_delta_set}). While this feasible set does not explicitly constrain $\Delta R$ to avoid enormous values, which is done with $\min_{\Delta R} \mathbb{E}_{\pi}[R(x,y)-\beta \Delta R(x,y)]$, the second term can help avoid these huge values. In addition, it reveals a mechanism that inherently limits the size of perturbations for actions that have high probability under $\pi_{\textnormal{\textbf{ref}}}$, and this is consistent with the WD distance intuition.

% The feasible set of reward perturbations $\mathcal{R}_{\Delta}$ is restricted to be a Lipschitz continuous function with respect to a cost function $C$ which generally takes a non-negative value in applications. When the cost between two outputs $C(y,y^\prime)$ is small, indicating that the outputs $y$ and $y^\prime$ are similar, the corresponding perturbations must also be similar in value. 
\paragraph{Note}The feasible set of reward perturbations $\mathcal{R}_{\Delta}$ is bounded to be a Lipschitz continuous function with respect to a cost function $C$, which generally takes a non-negative value in applications. 
% If the cost between two outputs $C(y,y^\prime)$ is small, indicating that the outputs $y$ and $y^\prime$ are similar, then the corresponding perturbations must also be similar in value. 
% \yuu{Expalin Lipschitz continuity and say that it is often assumed in many kinds of tasks?}\yuki{These works assume that the reward function satisfies lip-continuity. \citep{Rachelson2010OnTL,Pirotta2015PolicyGI}}
% \yuu{TODO: Is there any RLHF papers which uses the similarity/cost function as a way to infer the reward value of the text? If there are, we can cite them to support the Lipschitz continuity assumption. Otherwise, we can cite RL papers in other domains.}
% Assuming Lipschitz continuity for functions(e.g., reward function, $C(y,y^\prime)$) in this context is not unreasonable.
% % Lipschitz continuity is a common assumption in RL.
% \cite{Rachelson2010OnTL,Pirotta2015PolicyGI} consider continuous state and action space problems in RL, then they assume the reward function is satisfied Lipschitz continuity for the analysis. Returning to our problem setting, it's crucial to note that the input to cost function $C$ is $y$ after embedding, denoted as emb($y$). Given that $y$ is a continuous vector prior to embedding, this formulation establishes a clear connection with previous research.
The perturbation behavior corresponds to the Lipschitz continuity condition, which has traditionally been well-treated in the RL community. For example, previous studies such as \cite{Rachelson2010OnTL, Pirotta2015PolicyGI} considered continuous state and action spaces in RL and derived Lipschitz continuity for reward functions to aid their analysis. 

% In our problem setting, it is important to note that the input to the cost function $C$ is $y$ after being processed by an embedding function, denoted as $\text{emb}(y)$. Since $y$ is a continuous vector before embedding, this setting maintains consistency with these previous works, and the resulting behavior after perturbation naturally satisfies Lipschitz continuity, which is in line with traditional RL analysis.
% Assuming Lipschitz continuity for functions (e.g., reward function, $C(y,y^\prime)$) is not unreasonable in this context.
% Lipschitz continuity is a common assumption in RL.
% \cite{Rachelson2010OnTL,Pirotta2015PolicyGI} consider continuous state and action space problems in RL, then assume the reward function satisfies Lipschitz continuity for the analysis. Returning to our problem, it's crucial to note that the input to the cost function $C$ after embedding is $y$, denoted as emb($y$). Since $y$ is a continuous vector before embedding, this formulation establishes a clear connection with previous research.