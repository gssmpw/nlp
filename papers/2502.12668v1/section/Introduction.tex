\section{Introduction}

Large Language Models (LLMs) have demonstrated remarkable capabilities in many NLP tasks related to natural language understanding and text generation \citep{stiennon2020,NEURIPS2022_b1efde53,touvron2023llama,dubey2024llama,openai2024gpt4}.
Despite the strengths, LLMs are not always adept at interpreting a wide range of instructions and can produce undesirable outputs, such as biased, hallucinated, or toxic responses \citep{bai2022training,lin-etal-2022-truthfulqa,touvron2023llama,casper2023open,guan2024hallusionbench}.
This problem underscores the challenge of language model alignment; ensuring LLMs' behaviors align with human objectives and safety considerations \citep{ziegler2020finetuning,stiennon2020,NEURIPS2022_b1efde53}.
% There are now a wealth of approaches to tackle this problem \citep{stiennon2020,NEURIPS2022_b1efde53,NEURIPS2023_a85b405e}. These papers have demonstrated that training language models using human feedback can improve their performance. However, training language models is a computationally intensive task. In other words, improved performance comes at the cost of increased computational resources.
There is now a rich set of approaches to address this problem \citep{stiennon2020,NEURIPS2022_b1efde53,NEURIPS2023_a85b405e}. These papers have shown that training language models with human feedback can improve their performance. However, training language models is a computationally intensive task. In other words, improved performance comes at the cost of increased computational resources.

This paper focuses on the Best-of-N (BoN) sampling strategy, a method that involves generating $N$ outputs from a model and selecting the most preferred output among the $N$ samples. Despite its simplicity and the fact that it does not require an additional training phase, BoN sampling has been shown to be surprisingly effective in practice \citep{stiennon2020,nakano2022webgpt}. However, the BoN strategy does not scale with the number of samples $N$ due to the \textit{reward hacking problem} \citep{amodei2016concrete,ziegler2020finetuning,stiennon2020,NEURIPS2022_3d719fee,pmlr-v202-gao23h}. Reward hacking is a behavior that satisfies the given objective without achieving the intended result. 
It is caused by the misspecification of the preference model used by the BoN to select the most preferred output \citep{pan2022the,lambert2024alignment}.

% Previous work for mitigating the reward hacking problem has proposed Regularized Best-of-N (RBoN), BoN strategy with an addition of a regularization term to the objective \citep{jinnai2024regularized}.
% This work shows that the RBoN strategy is effective compared to BoN sampling in various experiments.
% However, \textit{why} such a regularization strategy is effective against reward uncertainty is unclear in the previous work.

Previous work to mitigate the reward hacking problem has proposed Regularized Best-of-N sampling (RBoN), BoN strategy with the addition of a regularization term to the objective \citep{jinnai2024regularized}.
This paper shows that the RBoN strategy is effective compared to BoN sampling in various experiments.
However, \textit{why} such a regularization strategy is effective against reward uncertainty is unclear in the previous work.

% In this paper, we propose Stochastic RBoN (SRBoN) sampling which adds a regularization term similarly to RBoN.
% Then, we draw a connection between the Reinforcement Learning (RL) problems \citep{sutton2018reinforcement} and the BoN strategies: BoN sampling corresponds to solving the RL problem, and SRBoN sampling strategies correspond to solving the Regularized Reinforcement Learning (RRL) problem \citep{neu2017unified,pmlr-v97-geist19a, NEURIPS2019_3f4366ae,NEURIPS2021_bb1443cc}.
% We evaluate the SRBoN strategy analytically and empirically to investigate why and under what conditions the SRBoN approach is effective.
In this paper, we propose Stochastic RBoN sampling (SRBoN), which adds a regularization term similar to RBoN.
We then draw a connection between the Reinforcement Learning (RL) problems \citep{sutton2018reinforcement} and the BoN strategies: BoN sampling corresponds to solving the RL problem, and SRBoN sampling strategies correspond to solving the Regularized Reinforcement Learning (RRL) problem \citep{neu2017unified,pmlr-v97-geist19a, NEURIPS2019_3f4366ae,NEURIPS2021_bb1443cc}.
% We evaluate the SRBoN strategy analytically and empirically to investigate why and under what conditions the SRBoN approach is effective.
% In this paper, we propose Stochastic RBoN (SRBoN) sampling which also adds a regularization term, and then draw a connection between the Reinforcement Learning (RL) problems and the strategies: BoN sampling corresponds to solving the RL problem, and SRBoN sampling strategies correspond to solving the Regularized Reinforcement Learning (RRL) problem and evaluate the SRBoN strategy analytically and empirically to investigate why and under what conditions the SRBoN approach is effective.

% Firstly, we utilize the knowledge of the RRL. Some papers have shown that regularization terms in probability distributions over actions provide robustness to reward perturbations \citep{ortega2014adversarial,husain2021regularized,NEURIPS2021_bb1443cc,eysenbach2022maximum,pan2022the,NEURIPS2021_bb1443cc}. Building upon this idea, we analytically demonstrate that the SRBoN strategy is an optimal approach under the assumption of adversarial reward perturbations, given certain theoretical conditions.

% Then, the efficacy of our approach is evaluated in comparison to alternative decoder methods in a range of experiments, with the objective of determining the relative resilience of each method to potential exploitation through reward hacking. The results demonstrate that our proposed method outperforms many existing approaches in a variety of settings. In other words, a theoretically guaranteed, effective algorithm is proposed.
First, we exploit the knowledge of RRL. Some work has shown that regularization terms in probability distributions over outputs provide robustness to reward perturbations \citep{ortega2014adversarial,husain2021regularized,NEURIPS2021_bb1443cc,eysenbach2022maximum,pan2022the,NEURIPS2021_bb1443cc}. 
% Building on this idea, we provide an answer as to why adding regularized terms can mitigate reward hacking, which is analyzed by SRBoN.
SRBoN can also apply this analysis to RRL. Its insights provide an answer to why reward hacking can be mitigated: when a regularization term is added to the BoN sampling, it also becomes an adversarial perturbation to the reward.


We then evaluate the effectiveness of our approach against alternative decoder methods in a series of experiments, with the goal of determining the relative resilience of each method to potential exploitation by reward hacking. The results show that our proposed method outperforms many existing approaches in a variety of settings. In other words, a theoretically guaranteed, effective algorithm is proposed.

In addition, while RBoN consists of complex formula structures, we proposed a simpler RBoN, Sentence Length Regularized BoN that, despite its simple implementation, shows comparable or even better performance in experiments with the methods of previous studies.

% The contributions of the papers are as follows:
% \begin{enumerate}
%     \item Theoretical analysis explaining the effectiveness of the RBoN strategy.
%     \item Thorough empirical evaluations of the RBoN strategy in a practical setting.
%     % \item An extension of the RBoN strategy, 
% \end{enumerate}

