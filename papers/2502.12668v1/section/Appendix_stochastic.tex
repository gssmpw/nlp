

% \section{Comparative Analysis of  Reward Model}\label{ap:reward}
% To illustrate this point, we compare the variance in the output values of the reward model, using the first four entries as examples. The results show that Eurus exhibits a significantly large variance, while RM-Mistral-7b has a smaller variance.
% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=\linewidth]{exp_img/Reward_dev/Eurus-RM-7b.pdf}
%     \caption{
%     Eurus
%     }
%     \label{fig:rec_a}
% \end{figure}

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=\linewidth]{exp_img/Reward_dev/RM-Mistral-7B.pdf}
%     \caption{
%     RM-Mistral-7B
%     }
%     \label{fig:rec_he}
% \end{figure}

% \newpage
% \section{Detailed Proof of  $\mathrm{SRBoN}_{\mathrm{KL}}$}\label{appendix:kl}

% \section{Detailed Proof of \cref{theory:kl-minmax}}\label{appendix:kl}


% The objective function of $\mathrm{SRBoN}_{\mathrm{KL}}$ is given by :


% % \begin{equation}\label{eq:kl_ind}
% % \begin{aligned}
% % % \pi^* &= \max_{\pi} \,\, \langle \pi_y, R \rangle - \beta \E\left[\log{\frac{\pi_y(y)}{\pi_{\textnormal{\textbf{ref}}} (y)}}\right]\\
% % \pi^* &= \max_{\pi} \,\, \langle \pi_y, R \rangle - \beta \left[\sum \pi_y(y)\log{\frac{\pi_y(y)}{\pi_{\textnormal{\textbf{ref}}} (y)}}\right]\\
% % \end{aligned}
% % \end{equation}

% \begin{equation}\label{eq:kl_obj}
% \begin{aligned}
% \textbf{Objective Function of $\mathrm{SRBoN}_{\mathrm{KL}}$} &= \max_{\pi} \,\, \langle \pi_y, R \rangle - \Omega(\pi)\\
% &= \max_{\pi} \,\, \langle \pi_y, R \rangle - \beta \sum_\mathcal{Y_{\textbf{ref}}} \pi_y(y)\log{\frac{\pi_y(y)}{\pi_{\textnormal{\textbf{ref}}} (y)}}
% \end{aligned}
% \end{equation}
% where $\langle \pi_y, R \rangle = \sum_{y \in \mathcal{Y_{\textbf{ref}}}} \pi_y(y)R(y)$, reward function $R$ $:\mathcal{Y}  \rightarrow \mathbb{R}$, output probability $\pi$ $\in$ $ \Delta (\mathcal{Y})$,  KL divergence function $\Omega(\pi) = \beta \textbf{KL} (\pi_y|| \pi_{\textnormal{\textbf{ref}}}) = \beta \sum_\mathcal{Y_{\textbf{ref}}} \pi_y(y)\log{\frac{\pi_y(y)}{\pi_{\textnormal{\textbf{ref}}} (y)}}$. 
% By applying Fenchel's duality theorem \citep{Rockafellar+1970}, we can express 
% KL divergence function $\Omega(\pi)$ as:

% % \begin{equation}\label{eq:convex}
% % \beta \Omega(\pi) = \min_{\Delta R}\, \, \langle \pi_y, \Delta R \rangle - \beta \Omega^* (\Delta R)
% % \end{equation}
% % In addition, $\Omega^* (\Delta R)$ is :
% % \begin{equation}\label{eq:convex_delta}
% % \beta \Omega^* (\Delta R) = \max_{\pi}\, \, \langle \pi_y, \Delta R \rangle - \beta \Omega(\pi) 
% % \end{equation}

% % Inserting \cref{eq:fen} into \cref{eq:convex} converts the latter into the following max-min problem:


% \begin{equation}\label{eq:convex}
% \Omega(\pi) = \min_{\Delta R}\, \, \langle \pi_y, \Delta R \rangle - \Omega^* (\Delta R)
% \end{equation}

% where reward perturbation $\Delta R : \mathcal{Y}  \rightarrow \mathbb{R}$, and $\Omega^*$: conjugate function. 

% In addition, conjugate function $\Omega^* (\Delta R)$ is:
% \begin{equation}\label{eq:conjugate_function}
% \begin{aligned}
% \Omega^* (\Delta R) &= \max_{\pi} \,\,\langle \pi_y, \Delta R \rangle - \Omega(\pi) \\
% &=\max _\pi\,\,\langle\pi_y, \Delta R\rangle-\beta \sum_\mathcal{Y_{\textbf{ref}}}\pi_y(y) \log \frac{\pi_y(y)}{\pi_{\textnormal{\textbf{ref}}}(y)}
% \end{aligned}
% \end{equation}

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.6\linewidth]{img/conjugate.pdf}
%     \caption{This figure illustrates the relationship between a convex function $\Omega$ and its associated conjugate function $\Omega^*$. Conjugate function $\Omega^*$ is defined as the Legendre transform of the convex function $\Omega$. It is important to note that multiple conjugate functions $\Omega^*$ can correspond to different coefficients. This diversity arises because the conjugate function $\Omega^*$ captures the maximum difference between the linear approximation of $\Omega$ and the function itself, and this relationship can vary with different linear approximations. 
%     }
%     \label{fig:conjugate}
% \end{figure}

% Using an equation \cref{eq:convex}, \cref{eq:kl_obj} can be converted to a max-min problem :

% % \begin{equation}
% % (\pi^*, \Delta R^*) = \max_{\pi}\, \min_{\Delta R} \, \,\langle \pi_y, R - \Delta R \rangle + \beta \Omega^* (\Delta R)
% % \end{equation}

% \begin{equation}\label{eq:pi_delta}
% \textbf{Objective Function of $\mathrm{SRBoN}_{\mathrm{KL}}$} = \max_{\pi}\, \min_{\Delta R} \, \,\langle \pi_y, R - \Delta R \rangle + \Omega^* (\Delta R)
% \end{equation}

% Our analysis begins with examining the conjugate function, utilizing the Lagrange multiplier method in the subsequent proof.



% \begin{lemma}
% The explicit formulation of the conjugate function can be expressed as follows :
% \begin{equation*}
%     \Omega^*(\Delta R) = \beta \log \sum_\mathcal{Y_{\textnormal{\textbf{ref}}}} \pi_{\textnormal{\textbf{ref}}}(y) \exp(\beta^{-1}\Delta R(y)) 
% \end{equation*}
% \end{lemma}
    
% \begin{proof}
    
% We apply the Lagrange multiplier to \cref{eq:conjugate_function}, $L(\lambda)$ is the Lagrange function.

% \begin{equation}\label{eq:kl_rew}
% L(\lambda)=\max _\pi\,\,\langle\pi_y, \Delta R\rangle-\beta \sum_\mathcal{Y_{\textbf{ref}}}\pi_y(y) \log \frac{\pi_y(y)}{\pi_{\textnormal{\textbf{ref}}}(y)} - \lambda(\sum_{\mathcal{Y}_{\textnormal{\textbf{ref}}}} \pi(y ) - 1) 
% \end{equation}
% The terms involving the Lagrange multiplier $\lambda$ correspond to constraints that guarantee the fundamental probability theory ($\sum_{\mathcal{Y}_{\textnormal{\textbf{ref}}}}\pi_y(y) = 1$).

% We now perform a partial differentiation on $\pi_y(y)$.

% \begin{equation*}
%     \frac{\partial L(\lambda)}{\partial \pi_y(y)} = \Delta R(y) - \beta \left(1 + \log \frac{\pi^* (y)}{\pi_{\textnormal{\textbf{ref}}} (y)}\right) - \lambda
% \end{equation*}
% where $\pi^*(y)$ is an optimal probability.
% We then set the partial derivative $\frac{\partial L(\lambda)}{\partial \pi_y(y)}$ equal to zero.
% \begin{equation}\label{eq:pi_opt}
% \begin{aligned}
%     \log \pi^*(y)&= -1 - \beta^{-1}\lambda +\beta^{-1}\Delta R(y) + \log \pi_{\textnormal{\textbf{ref}}}(y)\\
%     \pi^* (y) &= \exp(-1-\beta^{-1} \lambda)\left(\pi_{\textnormal{\textbf{ref}}}(y) \exp(\beta^{-1} \Delta R(y))\right)
%     \end{aligned}
% \end{equation}

% Next, using constraint to $\pi$, we solve $\lambda$.
% \begin{equation}
%     1 = \sum_\mathcal{Y_{\textbf{ref}}} \pi^* (y) = \exp(-1-\beta^{-1} \lambda)\sum_\mathcal{Y_{\textbf{ref}}} \left(\pi_{\textnormal{\textbf{ref}}}(y) \exp(\beta^{-1} \Delta R(y))\right)
% \end{equation}

% \begin{equation}\label{eq:lambda}
% \begin{aligned}
%     0 &=\log (\exp(-1-\beta^{-1} \lambda)\sum_\mathcal{Y_{\textbf{ref}}} \left(\pi_{\textnormal{\textbf{ref}}}(y) \exp(\beta^{-1} \Delta R(y))\right)\\
%     \beta^{-1} \lambda &= \log \sum_\mathcal{Y_{\textbf{ref}}} \pi_{\textnormal{\textbf{ref}}}(y)\exp(\beta^{-1}\Delta R(y) ) - 1\\
%     \lambda &= \beta\log \sum_\mathcal{Y_{\textbf{ref}}} \pi_{\textnormal{\textbf{ref}}}(y)\exp(\beta^{-1}\Delta R(y) ) - \beta
%     \end{aligned}
% \end{equation}
% We substitute the derived $\lambda$ \cref{eq:lambda} into \cref{eq:pi_opt}.
% \begin{equation*}
% \begin{aligned}
%      \pi^* (y) &= \exp(-1-\beta^{-1} (\beta\log \sum_\mathcal{Y_{\textbf{ref}}} \pi_{\textnormal{\textbf{ref}}}(y)\exp(\beta^{-1}\Delta R(y) ) - \beta)) \left(\pi_{\textnormal{\textbf{ref}}}(y) \exp(\beta^{-1} \Delta R(y))\right)\\
%      &= \exp(-\log \sum_\mathcal{Y_{\textbf{ref}}} \pi_{\textnormal{\textbf{ref}}}(y)\exp(\beta^{-1}\Delta R(y) )) \left(\pi_{\textnormal{\textbf{ref}}}(y) \exp(\beta^{-1} \Delta R(y))\right)\\
%     &= \frac{\pi_{\textnormal{\textbf{ref}}}(y)\exp(\beta^{-1}\Delta R(y))}{\sum_\mathcal{Y_{\textbf{ref}}} \pi_{\textnormal{\textbf{ref}}}(y)\exp(\beta^{-1}\Delta R(y))}\\
% \end{aligned}
% \end{equation*}

% % \begin{equation*}
% % \begin{aligned}
% %     \pi^*(y) &= \frac{\pi_{\textnormal{\textbf{ref}}}(y)\exp(\beta^{-1}\Delta R(y))}{\sum_\mathcal{Y_{\textbf{ref}}} \pi_{\textnormal{\textbf{ref}}}(y)\exp(\beta^{-1}\Delta R(y))}\\
% %     &= \frac{\pi_{\textnormal{\textbf{ref}}}(y)\exp(\beta^{-1}\Delta R(y))}{Z}
% %     \end{aligned}
% % \end{equation*}

% We put $\sum_\mathcal{Y_{\textbf{ref}}} \pi_{\textnormal{\textbf{ref}}}(y)\exp(\beta^{-1}\Delta R(y))$ = $Z$ for simplicity.

% \begin{equation*}
%     \pi^*(y) = \frac{\pi_{\textnormal{\textbf{ref}}}(y)\exp(\beta^{-1}\Delta R(y))}{Z}
% \end{equation*}

% For solving the conjugate function, substituting $\pi^*(y)$ into \cref{eq:conjugate_function}. 

% \begin{equation*}
% \begin{aligned}
% \Omega^*(\Delta R)&=\sum_\mathcal{Y_{\textbf{ref}}} \frac{\pi_{\textnormal{\textbf{ref}}}(y)\exp(\beta^{-1}\Delta R(y))}{Z}\Delta R(y) -\beta \sum_\mathcal{Y_{\textbf{ref}}}\frac{\pi_{\textnormal{\textbf{ref}}}(y)\exp(\beta^{-1}\Delta R(y))}{Z} \log \frac{\pi_{\textnormal{\textbf{ref}}}(y)\exp(\beta^{-1}\Delta R(y))}{\pi_{\textnormal{\textbf{ref}}}(y)Z}\\
% &=\sum_\mathcal{Y_{\textbf{ref}}} \frac{\pi_{\textnormal{\textbf{ref}}}(y)\exp(\beta^{-1}\Delta R(y))}{Z}\Delta R(y)- \beta \sum_\mathcal{Y_{\textbf{ref}}}\frac{\pi_{\textnormal{\textbf{ref}}}(y)\exp(\beta^{-1}\Delta R(y))}{Z} (\beta^{-1}\Delta R(y) - \log Z)\\
% &= \beta \log Z
% \end{aligned}
% \end{equation*}
% In conclusion, we can express the conjugate function as follows :
% \begin{equation}
%     \Omega^*(\Delta R) = \beta \log \sum_\mathcal{Y_{\textbf{ref}}} \pi_{\textnormal{\textbf{ref}}}(y) \exp(\beta^{-1}\Delta R(y)) 
% \end{equation}
% \end{proof}


% Since we have derived the explicit form of the conjugate function, we can now determine the precise range of the reward perturbation.
% \begin{lemma}(\textbf{\cite{brekelmans2022your} Proposition 1})
% Previous research has established that the conjugate function is subject to the constraint $\Omega^* (\Delta R) \leq 0$.
% Leveraging this insight, we can constrain the range of the perturbation term $\Delta R$ as follows :
% \begin{equation*}
% \sum_\mathcal{Y_{\textnormal{\textbf{ref}}}} \pi_{\textnormal{\textbf{ref}}}(y) \exp(\beta^{-1}\Delta R(y)) \leq 1
% \end{equation*}
% \end{lemma}
% \begin{proof}
% \begin{equation*}
%     \Omega^*(\Delta R) \leq 0
% \end{equation*}
% \begin{equation*}
% \begin{aligned}
%     \Longrightarrow \beta \log \sum_\mathcal{Y_{\textbf{ref}}} \pi_{\textnormal{\textbf{ref}}}(y) \exp(\beta^{-1}\Delta R(y))  &\leq 0 \quad (\beta > 0)\\
%     \Longrightarrow \sum_\mathcal{Y_{\textbf{ref}}} \pi_{\textnormal{\textbf{ref}}}(y) \exp(\beta^{-1}\Delta R(y)) &\leq 1
%     \end{aligned}
% \end{equation*}

% \end{proof}






% \section{Detailed proof of $\mathrm{SRBoN}_{\mathrm{WD}}$}\label{appendix:wd-thoery}
\section{Detailed proof of \cref{theory:wd}}\label{appendix:wd-thoery}

% The subsequent analysis is conducted within the framework of finite probability spaces.
% To streamline the subsequent proof, we introduce the following notation. Let $x_1, x_2, \cdots, x_n$ be $n$ places, and consider the function $f:=\left\{f_i: i=1, \cdots, n\right\}$ of some product among these places, i.e. $f_i$ refers to the ratio of the product at place $x_i$.

% The subsequent analysis is conducted within the framework of finite probability spaces.
% To streamline the subsequent proof, we introduce the following notation. Let $x_1, x_2, \cdots, x_n$ be $n$ places, and consider the function $f$, $f_i$ refers to the value $f(x_i)$.
% The following analysis is done in the framework of finite probability spaces.
% To simplify the following proof, we introduce the following notation. Let $x_1, x_2, \cdots, x_n$ be $n$ places and consider the function $f$, where $f_i$ refers to the value $f(x_i)$.
% We introduce new definitions for the following proof section:
% \begin{definition}
%     Consider a function $f$ that satisfies  this condition, defined as similarity-based Lipschitz continuity:
%     \begin{equation*}
%     |f_i-f_j| \leq C_{ij}, \quad i,j \in \mathcal{Y}\\
% \end{equation*}
% $\text{where}  \quad C_{ij}=1-\cos \left(\mathrm{emb}(y_i), \operatorname{emb}\left(y_j\right)\right)$, $\mathcal{Y}$ is size of $\mathcal{Y}_{\textnormal{\textbf{ref}}}$.
% \end{definition}
% \begin{definition}[Similarity-based Lipschitz Continuity]
%      A function $f$ is said to satisfy Similarity-based Lipschitz Continuity if, for any $i, j \in \mathcal{Y}$, the following holds:
%     \begin{equation*}
%     |f_i-f_j| \leq C_{ij}, \quad i,j \in \mathcal{Y}\\
% \end{equation*}
% $\text{where}  \quad C_{ij}=1-\cos \left(\mathrm{emb}(y_i), \operatorname{emb}\left(y_j\right)\right)$, $\mathcal{Y}$ is size of $\mathcal{Y}_{\textnormal{\textbf{ref}}}$.
% \end{definition}
\begin{definition}[Similarity-based Lipschitz Continuity] A function $f$ is said to have Similarity-based Lipschitz Continuity if, for any $y, y^\prime \in \mathcal{Y}$, the following holds: 
\begin{equation*}
|f(y) - f(y^\prime)| \leq C(y, y^\prime) 
\end{equation*} 
where \[ C(y, y') = 1 - \cos\left(\mathrm{emb}(y), \mathrm{emb}(y^\prime)\right) \] 
\end{definition}
We first explain how the objective function is reformulated to a max-min problem. Let us focus on the regularization term, 1-$\textbf{WD}$ term rewrite related to $\pi$, $\pi_{\textnormal{\textbf{ref}}}$

The following analysis is done in the framework of finite probability spaces.
To simplify the following proof, we introduce the following notation. Let $x_1, x_2, \cdots, x_n$ be $n$ places and consider the function $f$, where $f_i$ refers to the value $f(x_i)$.
% \begin{equation}\label{eq:wd_dev}
%     \begin{aligned}
%      \textbf{WD}(\nu || \mu)&= \left(\min _{\gamma \in \Gamma(\nu, \mu)} \sum_{(i,j) \in \mathcal{Y}\times \mathcal{Y}}C_{ij}\gamma_{ij}\right)\\
%      &= \left(\min _{\gamma \in P(Y, Y^\prime)} \sum_{(i,j) \in \mathcal{Y}\times \mathcal{Y} } C_{ij} \gamma_{ij} + \left\{
%      \begin{array}{l}
% 0\quad \text {if } \gamma \in \Gamma(\nu, \mu) \\
% +\infty \text{ else }
% \end{array}\right\}\right)
%      \end{aligned}
% \end{equation}
\begin{equation}\label{eq:wd_dev}
    \begin{aligned}
     \textbf{WD}[\nu \| \mu]&= \min _{\gamma \in \Gamma(\nu, \mu)} \sum_{(i,j) \in \mathcal{Y}\times \mathcal{Y}}C_{ij}\gamma_{ij}\\
     &= \min _{\gamma \in\mathbb{R}^{Y \times Y^{\prime}}} \sum_{(i,j) \in \mathcal{Y}\times \mathcal{Y} } C_{ij} \gamma_{ij} + \Psi(\gamma),
     \end{aligned}
\end{equation}
% where $\gamma$ is a coupling of the probability measure $\nu$ and $\mu$, $\Gamma(\nu, \mu)=\left\{\gamma \in \mathbb{R}^{Y \times Y^{\prime}} | \sum_{j \in \mathcal{Y}} \gamma_{i j}=\nu_i, \sum_{i \in \mathcal{Y}}, \gamma_{i j}=\mu_j, \gamma_{ij} \geq 0 \, \,\text{for all} \, \, i,j \right\}$, $Y$ and $Y^\prime$ ($= \mathcal{Y}$) is sample space respectively, $P(Y, Y^\prime)$ is the set of all coupling probability distributions generated by sigma-algebra of $Y$, $Y^\prime$, corresponding to outcomes $i$ and $j$ respectively and $\Psi(\gamma) = 0 $ if $\gamma \in \Gamma(\nu, \mu), +\infty$ otherwise.
where $\gamma$ is a coupling of the probability measure $\nu$ and $\mu$, $\Gamma(\nu, \mu)=\left\{\gamma \in \mathbb{R}^{Y \times Y^{\prime}} | \sum_{j \in \mathcal{Y}} \gamma_{i j}=\nu_i, \sum_{i \in \mathcal{Y}}, \gamma_{i j}=\mu_j, \gamma_{ij} \geq 0 \, \,\text{for all} \, \, i,j \right\}$, $Y$ and $Y^\prime$ ($=\mathcal{Y}$) is sample space respectively, corresponding to outcomes $i$ and $j$ respectively and $\Psi(\gamma) = 0 $ if $\gamma \in \Gamma(\nu, \mu), +\infty$ otherwise.


Constraint terms, a coupling of the probability measure $\gamma$ needs to satisfy:
\begin{equation}\label{eq:gamma_cons}
\begin{aligned}
    \sum_{j} \gamma_{ij} &= \nu_i \quad \forall i \in \mathcal{Y}\\
    \sum_{i} \gamma_{ij} &= \mu_j \quad \forall j \in \mathcal{Y}\\
    % \sum_{i} \nu_i &= \sum_j \mu_j &= 1
    \gamma_{ij} &\geq 0  \quad \forall i,j \in \mathcal{Y}\\
\end{aligned}
\end{equation}
This constraint can be expressed in $\mathbf{A \gamma} = \mathbf{b}$, indicating its linear nature.
Specifically, $\mathbf{A}$ and $\mathbf{b}$ are defined as $\mathbf{A}=\binom{I_i \otimes \mathbf{1}_j^{\top}}{I_j \otimes \mathbf{1}_i^{\top}}$, $\mathbf{b}=\binom{\nu}{\mu}$. In this formulation, $I_i$ and $I_j$ denote identity matrices of dimension $\mathcal{Y} \times \mathcal{Y}$, while $\mathbf{1}_i$ and $\mathbf{1}_j$ represent column vectors of dimension $\mathcal{Y}$ with all components equal to 1. The symbol $\otimes$ denotes the Kronecker product.
% \end{equation}



\begin{lemma}
Eq. (\ref{eq:wd_dev}) is reformulated as a max problem from a min problem.
    \begin{equation}\label{eq:maxmin}
\max_{\substack{f \\ |f_i-f_j| \leq C_{ij}}} \sum_i f_i\nu_i-\sum_{j} f_j\mu_j
\end{equation}

% where $f \in L^1(\nu)$, $g \in L^1(\mu)$, $L^1(\nu)=\left\{f:  \mathcal{Y} \rightarrow \mathbb{R} |\sum_{i \in \mathcal{Y}}|f_i|  \nu_i<\infty\right\}$, $L^1(\mu)=\left\{g:  \mathcal{Y} \rightarrow \mathbb{R} |\sum_{j \in  \mathcal{Y}}|g_j|  \mu_j<\infty\right\}$.
\end{lemma}
\begin{proof}

Taking into account the constraints specified in Eq. (\ref{eq:gamma_cons}), we proceed with the application of the Lagrange multiplier method:
\begin{equation*}\label{cons1}
\begin{aligned}
\textbf{WD}[\nu \| \mu]&=\min_{\gamma \in \mathbb{R}^{Y \times Y^{\prime}}}  \sum_{i,j} C_{ij} \gamma_{ij}+\max_{f, g}\,\,\{ \sum_i f_i \nu_i +\sum_{j} g_j \mu_j-\sum_{i,j}(f_i+g_j) \gamma_{ij}\}
\end{aligned}
\end{equation*}
% where $f \in L^1(\nu)$, $g \in L^1(\mu)$.

% To provide a more intuitive understanding, $f$ and $g$ can be conceptualized as analogous to Lagrange multipliers.
% Except for the first term, all subsequent entries relate to constraints on $\gamma$.
For a more intuitive understanding, $f$ and $g$ can be considered analogous to Lagrange multipliers.
Except for the first term, all subsequent entries refer to constraints on $\gamma$.
% and we assume that $f_x$ and $g_x$ are convex functions under all $x$ (\Cref{assum: delta}).

\begin{equation*}
\textbf{WD}[\nu \| \mu]=\min_{\gamma \in \mathbb{R}^{Y \times Y^{\prime}}} \max_{f, g} \,\,\sum_{i,j}(C_{ij}-f_i-g_j) \gamma_{ij}+\sum_i f_i\nu_i+\sum_{j} g_j\mu_j
\end{equation*}
% \yuki{f,g -> concave?}
% \yuki{The optimization problem we are currently addressing is discrete and can be characterized as a linear programming problem.
% From Theorem 5.2 \citep{vanderbei2020linear}, there is never a gap between the primal and the dual optimal objective values in linear programming.} Under the strong duality theorem (ex. $\min_x \max_y f(x,y) = \max_y \min_x f(x,y)$), so we can change $\min$ $\max$ term.
can be seen from Eq. (\ref{eq:gamma_cons}), these constraints are linear. From Theorem 5.2 \citep{vanderbei2020linear}, in linear programming, there is never a gap between the primal and the dual optimal objective values. Under the strong duality theorem (e.g., $\min_x \max_y f(x,y) = \max_y \min_x f(x,y)$), we can exchange the $\min$ $\max$ term.

% From \Cref{assum: delta}, the optimal values of primal and dual problems are equal.
% Under the strong duality theorem (ex. $\min_x \max_y f(x,y) = \max_y \min_x f(x,y)$), so we can change $\min\max$ term.
\begin{equation*}
\textbf{WD}[\nu \| \mu]=\max_{f, g}\,\, \min_{\gamma \in \mathbb{R}^{Y \times Y^{\prime}}} \,\,\sum_{i,j}(C_{ij}-f_i-g_j)\gamma_{ij}+\sum_i f_i\nu_i+\sum_{j} g_j\mu_j
\end{equation*}
If $C_{ij}-f_i-g_j \geq 0 $ for all $i,j$, the optimal value of $\min_\gamma \sum_{i,j}(C_{ij}-f_i-g_j)\gamma_{ij}$ is 0, otherwise $\infty$. This observation allows us to derive the inequality constraint for the first item. 
We can include this as a constraint in the equation:
\begin{equation*}\label{cons2}
\textbf{WD}[\nu \| \mu]=\max _{\substack{f, g \\ f_i+g_j \leq C_{ij}}} \sum_i f_i\nu_i+\sum_{j} g_j \mu_j
\end{equation*}
Our next goal is to express the above function, currently represented by $f$ and $g$, exclusively in terms of the function $f$. From the given constraints, we have established that $f_i + g_j \leq C_{ij}$ for all $i$ and $j$.
% Let us assume that we have a function $f_x$ and we want to find the optimal $g$ corresponding to $f_x$ that achieves the maxremum in \cref{cons2}. 
% We know that $\forall y,y^\prime: f_x(y)+g_x(y^\prime) \leqC\left(y, y^{\prime}\right)$.
We can express this as follows:



\begin{equation}\label{cons10}
g_j \leq \min _i\,\,\{C_{ij}-f_i\}
\end{equation}
% To maximize the right side in Eq. \ref{cons10} is to set ($i = i^*$): 
To fix $i = i^*$, since $\min_i$ picks the minimum value. The index $i^*$ gives this minimum, and fixing $i$ to $i^*$ turns the inequality in Eq. (\ref{cons10}) into the equality in Eq. (\ref{cons3}).
% \begin{equation}\label{cons3}
% g_x(y^\prime)=\min _y\{C\left(y, y^{\prime}\right)-f_x(y)\}
% \end{equation}
\begin{equation}\label{cons3}
g_j=\{C_{i^*j}-f_{i^*}\}
\end{equation}

Eq. (\ref{cons3}) gives us a function which is called the $c$-transform of $f_j$ and is often denoted by $f^c_j$,
% \begin{equation*}
% f^c_x(y^\prime)=g_x(y^\prime)=\min _y\{C\left(y, y^{\prime}\right)-f_x(y)\}
% \end{equation*}
\begin{equation*}
f^c_j=g_j=\{C_{i^*j}-f_{i^*}\}
\end{equation*}

We can now rewrite $\textbf{WD}$ with $f^c_j$ as
\begin{equation}\label{eq:wd_c}
\textbf{WD}[\nu \| \mu]=\max _{f}\,\, \sum_i f_i \nu_i+\sum_j f^c_j \mu_j
\end{equation}


If $f$ is similarity-based Lipschitz, $f^c$ is also similarity-based Lipschitz, for all $\boldsymbol{i}$ and $\boldsymbol{j}$ we have
\begin{equation*}\label{cons4}
\begin{aligned}
& \left|f^c_j-f^c_i\right| \leq C_{ij} \\
& \Longrightarrow-C_{ij} \leq f^c_j-f^c_i \leq C_{ij} \\
& \Longrightarrow-f^c_i \leq C_{ij}-f^c_j
\end{aligned}
\end{equation*}


\begin{equation*}
\begin{aligned}
& \Longrightarrow-f^c_i \leq \min _{j}\,\,\left\{C_{ij}-f^c_j\right\} \\
% & \Longrightarrow-f^c_x(y) \leq \min _{y^\prime}\left\{C\left(y, y^{\prime}\right)-f^c_x(y^\prime)\right\}\\
% &
\end{aligned}
\end{equation*}
Upper bound of $\min _{j}\left\{C_{ij}-f^c_j\right\}$ is choosing $j \rightarrow i$
\begin{equation*}
\begin{aligned}
\min_{j}\,\,\left\{C_{ij}-f^c_j\right\} \leq-f^c_i \\
\end{aligned}
\end{equation*}
It can be shown that $f^{c c}_{i}=f_{i} = \min _{j}\left\{C_{ij}-f^c_j\right\}$. 
This means that $-g=-f^c = f$.
Substituting $f^c_j=-f_j$ into Eq. \ref{eq:wd_c}, we get
\begin{equation}\label{eq:maxmin2}
\max_{\substack{f \\ |f_i-f_j| \leq C_{ij}}} \sum_i f_i\nu_i-\sum_{j} f_j\mu_j
\end{equation}
which is the dual form of 1-Wasserstein distance. 

\end{proof}
Finally, by substituting $\Delta R$ for $f$, we get:
% \begin{equation*}
% \textbf{Objective Function} = \max_{\pi} \min_{\Delta R} \left\langle \pi_y, R - \beta \Delta R \right\rangle + \beta \left\langle \pi_{\text{ref}}, \Delta R \right\rangle
% \end{equation*}
% \begin{equation}
%     \begin{aligned}
%      \pi_{\mathrm{SRBoN}_\mathrm{WD}}(x) &= \argmax_{\pi \in \Pi} \,\, \langle \pi ,R \rangle -\beta \textbf{WD} [\pi_{\textnormal{\textbf{ref}}} \| \pi]\\
%      &= \argmax_{\pi \in \Pi} f_\mathrm{RRL}^{\mathrm{WD}}(\pi).
%      \end{aligned}
% \end{equation}
\begin{equation*}
\begin{aligned}
\pi_{\mathrm{SRBoN}_\mathrm{WD}}(x) &=\max_{\pi\in \Pi}\mathbb{E}_{y \sim \pi(\cdot \mid x)}[R(x,y)] -\Omega (\pi)\\
% &=\max_{\pi\in \Pi} \,\, \langle \pi,R \rangle -\max_{\Delta R \in \mathcal{R}_{\Delta}}\,\,\beta\left(\sum_\mathcal{Y_{\textbf{ref}}} \Delta R(x,y)\pi_{\textnormal{\textbf{ref}}}(y \mid x)-\sum_\mathcal{Y_{\textbf{ref}}} \Delta R(x,y)\pi(y \mid x)\right)\\
&=\max_{\pi\in \Pi} \mathbb{E}_{y \sim \pi(\cdot \mid x)}[R(x,y)] -\max_{\Delta R \in \mathcal{R}_{\Delta}}\,\,\beta\left(\sum_\mathcal{Y_{\textbf{ref}}} \Delta R(x,y)\pi_{\textnormal{\textbf{ref}}}(y \mid x)-\sum_\mathcal{Y_{\textbf{ref}}} \Delta R(x,y)\pi(y \mid x)\right)\\
% &=\max_{\pi\in \Pi} \,\, \langle \pi,R \rangle -\min_{\Delta R \in \mathcal{R}_{\Delta}}\,\,\beta\left(-\sum_\mathcal{Y_{\textbf{ref}}} \Delta R(x,y)\pi_{\textnormal{\textbf{ref}}}(y \mid x)+\sum_\mathcal{Y_{\textbf{ref}}} \Delta R(x,y)\pi(y \mid x)\right)\\
&=\max_{\pi\in \Pi}  \mathbb{E}_{y \sim \pi(\cdot \mid x)}[R(x,y)] -\min_{\Delta R \in \mathcal{R}_{\Delta}}\,\,\beta\left(-\sum_\mathcal{Y_{\textbf{ref}}} \Delta R(x,y)\pi_{\textnormal{\textbf{ref}}}(y \mid x)+\sum_\mathcal{Y_{\textbf{ref}}} \Delta R(x,y)\pi(y \mid x)\right)\\
\end{aligned}
\end{equation*}
where $\Omega (\pi) = \beta \textbf{WD}[\pi_{\textnormal{\textbf{ref}}} (\cdot \mid x) \| \pi(\cdot \mid x)]$.

\begin{equation*}
    % \pi_{\mathrm{SRBoN}_\mathrm{WD}}(x) = \max_{\pi\in \Pi} \,\,\min_{\Delta R \in \mathcal{R}_{\Delta}}\,\, \left\langle \pi, R - \beta \Delta R \right\rangle + \beta \left\langle \pi_{\textnormal{\textbf{ref}}}, \Delta R \right\rangle
    \pi_{\mathrm{SRBoN}_\mathrm{WD}}(x) = \max_{\pi\in \Pi} \,\,\min_{\Delta R \in \mathcal{R}_{\Delta}}\mathbb{E}_{y \sim \pi(\cdot \mid x)}\left[R(x,y) - \beta \Delta R(x,y)\right] + \beta \sum_{y \in \mathcal{Y}_{\textbf{ref}}} \pi_{\textnormal{\textbf{ref}}}(y \mid x)\Delta R(x,y)
\end{equation*}
\begin{equation*}
\text{where}\quad \mathcal{R}_{\Delta}:=\left\{\Delta R \in \mathbb{R}^{\mathcal{X}\times\mathcal{Y}_{\textnormal{\textbf{ref}}}} \mid \left|\Delta R(x,y)-\Delta R\left(x,y^{\prime}\right)\right| \leq C\left(y, y^{\prime}\right) \quad \forall y, y^{\prime} \in \mathcal{Y}_{\textnormal{\textbf{ref}}}\right\}
\end{equation*}
% where $\Delta R \in L^1(\pi_{\textnormal{\textbf{ref}}})$.


\newpage
\section{Relationship Between $\pi_{\textnormal{\textbf{ref}}}$ and the Proxy Reward Model}\label{appendix:kl}
Despite the theoretical robustness of $\mathrm{SRBoN}_{\mathrm{KL}}$ demonstrated in the analyses presented in \cref{sec:kl_sec}, the experimental results (\cref{sec:exp_1} and \cref{Ex:parameter}) did not show comparable robustness. This section aims to explain the reasons for this discrepancy. Recall the objective function of $\mathrm{SRBoN}_{\mathrm{KL}}$:
\begin{equation*}
\begin{aligned}
\pi_{\mathrm{SRBoN}_\mathrm{KL}}(x) &=\max _{\pi}\mathbb{E}_{y \sim \pi(\cdot \mid x)} [R(x,y)]- \Omega(\pi)\\
&= \max _{\pi}\mathbb{E}_{y \sim \pi(\cdot \mid x)} [R(x,y)]- \sum_{\mathcal{Y}_{\textnormal{\textbf{ref}}}} \pi(y \mid x) \log\frac{\pi(y\mid x)}{\pi_{\textnormal{\textbf{ref}}}(y\mid x)}
\end{aligned}
\end{equation*}

This implies that ideally, $\pi_{\textnormal{\textbf{ref}}}$ and the reward function $R$ should have some form of relationship (e.g. positive correlation) that facilitates learning. However, $\pi_{\textnormal{\textbf{ref}}}$ is influenced by complex factors such as length bias. 

To verify this hypothesis, we examine two aspects: (1) the correlation between the Eurus-RM-7B reward values, which were used as the gold reward model in our experiments, and the probabilities assigned by $\pi_{\textnormal{\textbf{ref}}}$; (2) the relationship between the length of the outputs generated by $\pi_{\textnormal{\textbf{ref}}}$ and the generation probabilities of those outputs.



\begin{table}[ht]
\centering
\caption{The correlation between the Eurus-RM-7B reward values and the probabilities assigned by $\pi_{\textnormal{\textbf{ref}}}$}
\label{ap_ex:1}
\begin{tabular}{ C{3cm}C{3cm}C{3cm} }
  \hline
  \textbf{AlpacaFarm} & \textbf{Harmlessness} & \textbf{Helpfulness} \\
  \hline
   $-0.224$ & $0.088$ & $-0.097$ \\
  \hline
\end{tabular}
\end{table}

\vspace{0.5cm}


\begin{table}[ht]
\centering
\caption{The relationship between the length of the outputs generated by $\pi_{\textnormal{\textbf{ref}}}$ and the generation probabilities of these outputs.}
\label{ap_ex:2}
\begin{tabular}{ C{3cm}C{3cm}C{3cm} }
  \hline
  \textbf{AlpacaFarm} & \textbf{Harmlessness} & \textbf{Helpfulness} \\
  \hline
   $-0.877$ & $-0.924$ & $-0.854$ \\
  \hline
\end{tabular}
\end{table}

% As evident from \cref{ap_ex:1}, there is negligible correlation between $\pi_{\textnormal{\textbf{ref}}}$ and the gold reward model Eurus-RM-7B in terms of Harmlessness and Helpfulness. Moreover, the AlpacaFarm dataset domain tends to negative correlation. These findings explain the performance degradation observed when incorporating this relationship into the regularization term. \cref{ap_ex:2} reveals that $\pi_{\textnormal{\textbf{ref}}}$ exhibits a bias towards shorter sentences, with output probabilities increasing as sentence length decreases.
As can be seen from \cref{ap_ex:1}, there is negligible correlation between $\pi_{\textnormal{\textbf{ref}}}$ and  Eurus-RM-7B (gold reward model) in terms of Harmlessness and Helpfulness. In addition, the domain of the AlpacaFarm dataset tends to be negatively correlated. 

These results explain the performance degradation observed when this relationship is included in the regularization term. \cref{ap_ex:2} shows that $\pi_{\textnormal{\textbf{ref}}}$ has a bias towards shorter sentences, with output probabilities increasing as sentence length decreases.



\newpage

\section{Supplemently Results}\label{appendix:all_method}
\cref{fig:harmless-l,fig:helpful-l} show evaluation of RBoN sensitivity on the Harmlessness subset and Helpfulness of the hh-rlhf dataset. These results were similar to those seen in AlpacaFarm using \cref{Ex:parameter}. This means that each method is not necessarily dependent on the dataset.

\cref{fig:alpaca-wd,fig:harmless-wd,fig:helpful-wd} compare $\mathrm{RBoN}_{\mathrm{WD}}$ and $\mathrm{SRBoN}_{\mathrm{WD}}$ and \cref{fig:alpaca-kl,fig:harmless-kl,fig:helpful-kl} compare $\mathrm{RBoN}_{\mathrm{KL}}$ and $\mathrm{SRBoN}_{\mathrm{KL}}$. These results show that SRBoN is not superior to RBoN. This is for reasons also discussed in \cref{sec:exp}




\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{exp_img/Figure_stochastic/l_bon/hh-harmless.pdf}
    \caption{
    Evaluation of RBoN sensitiveness on the Harmlessness subset of the hh-rlhf dataset with varying parameter $\beta$. We use proxy reward models, OASST, SHP-Large, SHP-XL,  PairRM, and RM-Mistral-7B. As the gold reward model, we utilize Eurus-RM-7B.
    }
    \label{fig:harmless-l}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{exp_img/Figure_stochastic/l_bon/hh-helpful.pdf}
    \caption{
    Evaluation of RBoN sensitiveness on the Helpfulness subset of the hh-rlhf dataset with varying parameter $\beta$. We use proxy reward models, OASST, SHP-Large, SHP-XL, PairRM, and RM-Mistral-7B. As the gold reward model, we utilize Eurus-RM-7B.
    }
    \label{fig:helpful-l}
\end{figure} 
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{exp_img/Figure_stochastic/wd/alpaca.pdf}
    \caption{
   Evaluation of $\mathrm{RBoN}_{\mathrm{WD}}$ and $\mathrm{SRBoN}_{\mathrm{WD}}$ sensitiveness on the AlpacaFarm dataset with varying parameter $\beta$. We use proxy reward models, OASST, SHP-Large, SHP-XL,  PairRM, and RM-Mistral-7B. As the gold reward model, we utilize Eurus-RM-7B.
    }
    \label{fig:alpaca-wd}
\end{figure}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{exp_img/Figure_stochastic/wd/hh-harmless.pdf}
    \caption{
    Evaluation of $\mathrm{RBoN}_{\mathrm{WD}}$ and $\mathrm{SRBoN}_{\mathrm{WD}}$ sensitiveness on the Harmlessness subset of the hh-rlhf dataset with varying parameter $\beta$. We use proxy reward models, OASST, SHP-Large, SHP-XL,  PairRM, and RM-Mistral-7B. As the gold reward model, we utilize Eurus-RM-7B.
    }
    \label{fig:harmless-wd}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{exp_img/Figure_stochastic/wd/hh-helpful.pdf}
    \caption{
    Evaluation of $\mathrm{RBoN}_{\mathrm{WD}}$ and $\mathrm{SRBoN}_{\mathrm{WD}}$ sensitiveness on the Helpfulness subset of the hh-rlhf dataset with varying parameter $\beta$. We use proxy reward models, OASST, SHP-Large, SHP-XL,  PairRM, and RM-Mistral-7B. As the gold reward model, we utilize Eurus-RM-7B.
    }
    \label{fig:helpful-wd}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{exp_img/Figure_stochastic/kl/alpaca.pdf}
    \caption{
   Evaluation of $\mathrm{RBoN}_{\mathrm{KL}}$ and $\mathrm{SRBoN}_{\mathrm{KL}}$ sensitiveness on the AlpacaFarm dataset with varying parameter $\beta$. We use proxy reward models, OASST, SHP-Large, SHP-XL,  PairRM, and RM-Mistral-7B. As the gold reward model, we utilize Eurus-RM-7B.
    }
    \label{fig:alpaca-kl}
\end{figure}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{exp_img/Figure_stochastic/kl/hh-harmless.pdf}
    \caption{
    Evaluation of $\mathrm{RBoN}_{\mathrm{KL}}$ and $\mathrm{SRBoN}_{\mathrm{KL}}$ sensitiveness on the Harmlessness subset of the hh-rlhf dataset with varying parameter $\beta$. We use proxy reward models, OASST, SHP-Large, SHP-XL,  PairRM, and RM-Mistral-7B. As the gold reward model, we utilize Eurus-RM-7B.
    }
    \label{fig:harmless-kl}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{exp_img/Figure_stochastic/kl/hh-helpful.pdf}
    \caption{
    Evaluation of $\mathrm{RBoN}_{\mathrm{KL}}$ and $\mathrm{SRBoN}_{\mathrm{KL}}$ sensitiveness on the Helpfulness subset of the hh-rlhf dataset with varying parameter $\beta$. We use proxy reward models, OASST, SHP-Large, SHP-XL,  PairRM, and RM-Mistral-7B. As the gold reward model, we utilize Eurus-RM-7B.
    }
    \label{fig:helpful-kl}
\end{figure}
% The results \cref{fig:score-a}, \cref{fig:score-ha} and \cref{fig:score-he} show the performance of BoN, RBoN, Random, and MBR on the AlpacaFarm dataset using Mistral as a language model, evaluated by win rate.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{exp_img/Figure_stochastic/all_method/alpaca.pdf}
    \caption{
    Evaluation of the decoder method on the AlpacaFarm dataset with varying parameter $\beta$. We use proxy reward models, OASST, SHP-Large, SHP-XL,  PairRM, and RM-Mistral-7B. As the gold reward model, we utilize Eurus-RM-7B.
    }
    \label{fig:score-a}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{exp_img/Figure_stochastic/all_method/hh-harmless.pdf}
    \caption{
    Evaluation of the decoder method on the Harmlessness dataset with varying parameter $\beta$. We use proxy reward models, OASST, SHP-Large, SHP-XL,  PairRM, and RM-Mistral-7B. As the gold reward model, we utilize Eurus-RM-7B.
    }
    \label{fig:score-ha}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{exp_img/Figure_stochastic/all_method/hh-helpful.pdf}
    \caption{
    Evaluation of the decoder method on the Helpfulness dataset with varying parameter $\beta$. We use proxy reward models, OASST, SHP-Large, SHP-XL,  PairRM, and RM-Mistral-7B. As the gold reward model, we utilize Eurus-RM-7B.
    }
    \label{fig:score-he}
\end{figure}


% The table presents the regret values for each method, calculated as the difference between the performance at the optimal parameters on the evaluation splits and the optimal value obtained by the gold reward model. We choose to evaluate the methods using regret because, in the field of decoders, it is common to assess performance based on whether scores are high or low without considering the difference from the optimal value. By examining the difference between the achieved reward and the optimal reward, we can gain insights into the quality of the generated outputs.
% The calculation can be expressed by the following equation:
% \begin{equation}
%     \textbf{Cumulative Regret} = \sum_ {x \in \mathcal{D}} |y^* - f(x)|
% \end{equation}
% where $\mathcal{D}$ is dataset, $x$ is prompt, $y^*$ is optimal output to $x$, $f$ is decoder method.
% \begin{table}[h]
% \centering
% \caption{The cumulative regret of decoder methods against BoN. For RBoN, the optimal parameter ($\beta^*$)}
% \begin{tabular}{@{}lrrrrr@{}}
% \toprule
%  & \textbf{OASST} & \textbf{SHP-Large} & \textbf{SHP-XL} & \textbf{PairRM} & \textbf{RM-Mistral-7B} \\ \midrule
% \rowcolor[HTML]{EFEFEF} 
% \multicolumn{6}{c}{\textbf{AlpacaFarm}} \\ \midrule
% \textbf{BoN - WD} & \multicolumn{1}{c}{-3498} & \multicolumn{1}{c}{14466} & \multicolumn{1}{c}{5006} & \multicolumn{1}{c}{23529} & \multicolumn{1}{c}{126} \\
% \textbf{BoN - KL} & \multicolumn{1}{c}{-28380} & \multicolumn{1}{c}{-405635} & \multicolumn{1}{c}{-440182} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{-9451} \\ 
% \textbf{BoN - Random} & \multicolumn{1}{c}{-347711} & \multicolumn{1}{c}{-230300} & \multicolumn{1}{c}{-279468} & \multicolumn{1}{c}{-264618} & \multicolumn{1}{c}{-733984} \\
% \textbf{BoN - MBR} & \multicolumn{1}{c}{-224332} & \multicolumn{1}{c}{-106922} & \multicolumn{1}{c}{-156089} & \multicolumn{1}{c}{-141239} & \multicolumn{1}{c}{-733984} \\ \midrule
% \rowcolor[HTML]{EFEFEF} 
% \multicolumn{6}{c}{\textbf{Harmlessness}} \\ \midrule
% \textbf{BoN - WD} & \multicolumn{1}{c}{44717} & \multicolumn{1}{c}{303141} & \multicolumn{1}{c}{203264} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{13674} \\
% \textbf{BoN - KL} & \multicolumn{1}{c}{-28912} & \multicolumn{1}{c}{12455} & \multicolumn{1}{c}{-99317} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{-30584}\\
% \textbf{BoN - Random} & \multicolumn{1}{c}{-422679} & \multicolumn{1}{c}{105244} & \multicolumn{1}{c}{-40031} & \multicolumn{1}{c}{-397539} & \multicolumn{1}{c}{-1029948} \\
% \textbf{BoN - MBR} & \multicolumn{1}{c}{-298734} & \multicolumn{1}{c}{-229190} & \multicolumn{1}{c}{83915} & \multicolumn{1}{c}{-273594} & \multicolumn{1}{c}{-906002} \\ \midrule
% \rowcolor[HTML]{EFEFEF} 
% \multicolumn{6}{c}{\textbf{Helpfulness}} \\ \midrule
% \textbf{BoN - WD} & \multicolumn{1}{c}{15852} & \multicolumn{1}{c}{71765} & \multicolumn{1}{c}{50315} & \multicolumn{1}{c}{-7091} & \multicolumn{1}{c}{584} \\
% \textbf{BoN - KL} & \multicolumn{1}{c}{-76958} & \multicolumn{1}{c}{-815970} & \multicolumn{1}{c}{-953132} & \multicolumn{1}{c}{-7091} & \multicolumn{1}{c}{-8516} \\
% \textbf{BoN - Random} & \multicolumn{1}{c}{-480459} & \multicolumn{1}{c}{-421952} & \multicolumn{1}{c}{-606614} & \multicolumn{1}{c}{-436179} & \multicolumn{1}{c}{-1237647} \\
% \textbf{BoN - MBR} & \multicolumn{1}{c}{-283675} & \multicolumn{1}{c}{-225168} & \multicolumn{1}{c}{-409830} & \multicolumn{1}{c}{-239395} & \multicolumn{1}{c}{-1040863} \\ \bottomrule
% \end{tabular}
% \label{tab:diff}
% \end{table}

\newpage
\section{Spearman's Rank Correlation \citep{spearman1904proof}}\label{ap:recol}
\cref{fig:rec_a}, \cref{fig:rec_ha}, and \cref{fig:rec_he} show the average Spearman's rank correlation coefficient ($\rho$) between pairs of reward models \citep{spearman1904proof}.
These results suggest that pairs of reward models with higher correlation values are more similar, indicating a preference for greedy methods in such cases. 
% These results suggest that reward model pairs with higher correlation values are more similar, indicating a preference for greedy methods in such cases. 


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\linewidth]{img/reward_correlation_heatmap_alpaca.pdf}
    \caption{
    The average Spearman's rank correlation coefficient ($\rho$) between pairs of reward models in the AlpacaFarm dataset.
    }
    \label{fig:rec_a}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\linewidth]{img/reward_correlation_heatmap_hh-harmless.pdf}
    \caption{
    The average Spearman's rank correlation coefficient ($\rho$) between pairs of reward models in the Harmlessness dataset.
    }
    \label{fig:rec_ha}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\linewidth]{img/reward_correlation_heatmap_hh-helpful.pdf}
    \caption{
    The average Spearman's rank correlation coefficient ($\rho$) between pairs of reward models in the Helpfulness dataset.
    }
    \label{fig:rec_he}
\end{figure}
% \section{\citep{llama3modelcard}}
\newpage
\section{Supplementary Result on Meta-Llama-3-8B-Instruct \citep{dubey2024llama}}
We compared the average Spearman's rank correlation coefficient of the reward model and the performance of $\mathrm{{RBoN}}_{{\mathrm{{WD}}}}$ on the evaluation split using the Llama (Meta-Llama-3-8B-Instruct) language model.
The purpose of this analysis is to verify the performance of $\mathrm{{RBoN}}_{{\mathrm{{WD}}}}$, even when applied to samples generated by state-of-the-art language models.


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\linewidth]{exp_img/Reward_correlation/Meta-Llama-3-8B-Instruct-alpaca.pdf}
    \caption{
    The average Spearman's rank correlation coefficient ($\rho$) between pairs of reward models in the AlpacaFarm dataset, using Llama as the language model.
    }
    \label{fig:rec_meta_a}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\linewidth]{exp_img/Reward_correlation/Meta-Llama-3-8B-Instruct-hh-harmless.pdf}
    \caption{
    The average Spearman's rank correlation coefficient ($\rho$) between pairs of reward models in the Harmlessness dataset, using Llama as the language model.
    }
    \label{fig:rec_meta_ha}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\linewidth]{exp_img/Reward_correlation/Meta-Llama-3-8B-Instruct-hh-helpful.pdf}
    \caption{
    The average Spearman's rank correlation coefficient ($\rho$) between pairs of reward models in the Helpfulness dataset, using Llama as the language model.
    }
    \label{fig:rec_meta_he}
\end{figure}



\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{exp_img/meta/alpaca.pdf}
    \caption{
    Evaluation of the RBoN method on the AlpacaFarm dataset with varying parameter $\beta$. We use proxy reward models, OASST, SHP-Large, SHP-XL,  PairRM, and RM-Mistral-7B. As the gold reward model, we utilize Eurus-RM-7B, and Llama as the language model.
    }
    \label{fig:meta-a}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{exp_img/meta/hh-harmless.pdf}
    \caption{
    Evaluation of the RBoN method on the Harmlessness dataset with varying parameter $\beta$. We use proxy reward models, OASST, SHP-Large, SHP-XL,  PairRM, and RM-Mistral-7B. As the gold reward model, we utilize Eurus-RM-7B, and Llama as the language model.
    }
    \label{fig:meta-ha}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{exp_img/meta/hh-helpful.pdf}
    \caption{
    Evaluation of the RBoN method on the Helpfulness dataset with varying parameter $\beta$. We use proxy reward models, OASST, SHP-Large, SHP-XL,  PairRM, and RM-Mistral-7B. As the gold reward model, we utilize Eurus-RM-7B, and Llama as the language model.
    }
    \label{fig:meta-he}
\end{figure}


\newpage
\section{Robustness of RBoN Under Suboptimal Reward Models}
We evaluate the performance of suboptimal reward models, Beaver (beaver-7b-v1.0-reward) \citep{dai2024safe}, Open Llama (hh-rlhf-rm-open-llama 3b) \citep{diao-etal-2024-lmflow}, and Tulu (tulu-v2.5-13b-uf-rm) \citep{ivison2024unpacking} selected from \cite{RewardBench}, which underperforms compared to other reward models in some cases. We set these models as proxy models, set Eurus-RM-7B (Eurus) as the gold model, and also show the reward correlation of these models.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{img/robust/alpaca.pdf}
    \caption{
    Evaluation of RBoN sensitiveness on the AlpacaFarm dataset with varying parameter $\beta$. We use proxy reward models, Beaver, Open Llama, and Tulu. As the gold reward model, we utilize Eurus.
    }
    % \label{fig:meta-he}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\linewidth]{img/robust_reward_corr/Reward_correlationmistral-7b-sft-beta-alpaca.pdf}
    \caption{
    The average Spearman's rank correlation coefficient ($\rho$) between pairs of reward models in the AlpacaFarm dataset.
    }
    % \label{fig:meta-he}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{img/robust/hh-helpful.pdf}
    \caption{
    Evaluation of RBoN sensitiveness on the Helpfulness dataset with varying parameter $\beta$. We use proxy reward models, Beaver, Open Llama, and Tulu. As the gold reward model, we utilize Eurus.
    }
    % \label{fig:meta-he}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\linewidth]{img/robust_reward_corr/Reward_correlationmistral-7b-sft-beta-hh-helpful.pdf}
    \caption{
    The average Spearman's rank correlation coefficient ($\rho$) between pairs of reward models in the Helpfulness dataset.
    }
    % \label{fig:meta-he}
\end{figure}


\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{img/robust/hh-harmless.pdf}
    \caption{
    Evaluation of RBoN sensitiveness on the Harmlessness dataset with varying parameter $\beta$. We use proxy reward models, Beaver, Open Llama, and Tulu. As the gold reward model, we utilize Eurus.
    }
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\linewidth]{img/robust_reward_corr/Reward_correlationmistral-7b-sft-beta-hh-harmless.pdf}
    \caption{
    The average Spearman's rank correlation coefficient ($\rho$) between pairs of reward models in the Harmlessness dataset.
    }
\end{figure}
\newpage

\section{Sentence Length Regularized BoN ($\mathrm{RBoN}_{\mathrm{L}}$)}\label{appendix:length}
The objective function of $\mathrm{RBoN}_{\mathrm{L}}$ (Sentence Length Regularized BoN) is given by:
\begin{equation*}
y_{\textbf{LBoN}}(x)=\underset{y \in \mathcal{Y}_{\textnormal{\textbf{ref}}}}{\arg \max }\,\, R(x, y)-\frac{\beta}{|y|}
\end{equation*}
where $\beta$ is a regularization parameter, $|y|$ denotes the token length of the sentence $y$ .

% This approach aims to address the inherent bias towards shorter outputs often observed in a large language model we used in experiments. We now elucidate the rationale behind the specific form of the regularization term in $\mathrm{RBoN}_{\mathrm{L}}$. Let $\mu$ represent a probability inversely proportional to the length of the text $y$. 
This approach aims to address the inherent bias toward shorter outputs often observed in a large language model we used in experiments. We now explain the rationale behind the specific form of the regularization term in $\mathrm{RBoN}_{\mathrm{L}}$. Let $\mu$ represent a probability that is inversely proportional to the token length of the text $y$. 

% For example, we might define $\mu(y|x) = 1/|y|$, where $|y|$ represents the token length of the output y. (ex. $\mu(y^\prime|x) = 1/|y^\prime|, \mu(y^{\prime\prime}|x) = 1/|y^{\prime\prime}|$...)
For example, we could define $\mu(y|x) = 1/|y|$ (e.g. $\mu(y^\prime|x) = 1/|y^\prime|, \mu(y^{\prime\prime}|x) = 1/|y^{\prime\prime}|$...), where $|y|$ represents the token length of output y. 
\begin{definition}\label{definition:length}
We define a newly normalized distribution $\mu^\prime$:
\begin{equation*}
\begin{aligned}
\mu^\prime (y\mid x) &= \frac{1/|y|}{\sum_{\mathcal{Y}_{\textnormal{\textbf{ref}}}} \mu(\cdot \mid x)} \\
&= \frac{1/|y|}{Z}\, \, \left( \mathrm{where} \,\,\sum_{\mathcal{Y}_{\textnormal{\textbf{ref}}}} \mu (\cdot \mid x) = Z\right)\
\end{aligned}
\end{equation*}
\end{definition}

\begin{proposition}
The objective function of $\mathrm{RBoN}_{\mathrm{L}}$ is derived by considering the TV distance between the output probability $\mathbbm{1}_y (\cdot \mid x)$ and $\mu^\prime(\cdot \mid x)$ as a regularization term.
\end{proposition}
\begin{proof}
Let us examine how the objective function of $\mathrm{RBoN}_{\mathrm{L}}$ is derived using \cref{definition:length}.
\begin{equation*}
\begin{aligned}
y_{\mathrm{LBoN}}(x) &=\argmax_{y \in \mathcal{Y_{\textbf{ref}}}}\,\,  R(x, y)+\beta \textbf{TV}\left[\mathbbm{1}_y (\cdot \mid x) \| \mu^\prime(\cdot \mid x)\right],\\
&= \argmax_{y \in \mathcal{Y_{\textbf{ref}}}}  \,\,R(x, y)+\frac{\beta}{2} \sum_{y \in \mathcal{Y_{\textbf{ref}}}} |\mathbbm{1}_y (\cdot \mid x) - \mu^\prime(\cdot \mid x)|\\
&= \argmax_{y \in \mathcal{Y_{\textbf{ref}}}}\,\,  R(x, y)+\frac{\beta}{2}\left( \left|1 - \frac{1}{Z|y|}\right| + \underbrace{\frac{1}{Z|y^\prime|} + \frac{1}{Z|y^{\prime\prime}|} + \cdots + \frac{1}{Z|y^{\prime\prime\prime}|}}_{= 1 - \frac{1}{Z|y|}} \right)\\
&= \argmax_{y \in \mathcal{Y_{\textbf{ref}}}} \,\, R(x, y)+\beta\left(1 - \frac{1}{Z|y|}\right)\\
&= \argmax_{y \in \mathcal{Y_{\textbf{ref}}}} \,\, R(x, y)-\frac{\beta}{Z|y|} \\
&= \argmax_{y \in \mathcal{Y_{\textbf{ref}}}} \,\, R(x, y)-\frac{\beta^\prime}{|y|} \, \, \, \left(\textbf{$\beta^\prime = \frac{\beta}{Z}$}\right)\\
\end{aligned}
\end{equation*}

where $\beta^\prime$ is a regularization parameter and $\textbf{TV}$ denotes TV distance. 
\end{proof}
% The purpose of this normalization is to counteract the effect of $\mathrm{SRBoN}_{\mathrm{KL}}$, which tends to favor shorter outputs. This formulation provides a theoretical foundation for understanding how $\mathrm{RBoN}_{\mathrm{L}}$ achieves its length-aware behavior, offering insights into its potential advantages over other decoding methods that may inadvertently bias towards shorter outputs. Our methodological approach to assess the divergence of output distributions from the length distribution $\mu^\prime$ involves a comparative analysis of BoN sampling and $\mathrm{RBoN}_{\mathrm{L}}$. For each output y selected by these methods, we construct the corresponding $\mathbbm{1}_y (\cdot \mid x)$ distribution. We then measure the TV distance between these distributions and $\mu^\prime(\cdot \mid x)$. The results of this comparative analysis are visualized in \cref{fig:bon_l_alpaca}, \cref{fig:bon_l_harm}, and \cref{fig:bon_l_help}. 
The purpose of this normalization is to counteract the effect of $\mathrm{SRBoN}_{\mathrm{KL}}$, which tends to favor shorter outputs. This formulation provides a theoretical basis for understanding how $\mathrm{RBoN}_{\mathrm{L}}$ achieves its length-aware behavior, and offers insight into its potential advantages over other decoding methods that may inadvertently bias toward shorter outputs. 

Our methodological approach to assessing the divergence of output distributions from the length distribution $\mu^\prime$ involves a comparative analysis of BoN sampling and $\mathrm{RBoN}_{\mathrm{L}}$. For each output y selected, we construct the corresponding $\mathbbm{1}_y (\cdot \mid x)$ distribution. We then measure the TV distance between $\mathbbm{1}_y (\cdot \mid x)$ and $\mu^\prime(\cdot \mid x)$. 

The results of this comparative analysis are visualized in \cref{fig:bon_l_alpaca}, \cref{fig:bon_l_harm}, and \cref{fig:bon_l_help}. 
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{exp_img/TV_divergence/alpaca.pdf}
    \caption{
   BoN sampling and $\mathrm{RBoN}_{\mathrm{L}}$ methods by measuring the TV distance between their output distributions and sentence length distribution $\mu^\prime$ in AlpacaFarm. This allows us to evaluate how closely each method's outputs align with the desired distribution, with a smaller TV distance indicating a preference for shorter sentences.
    }
    \label{fig:bon_l_alpaca}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{exp_img/TV_divergence/hh-harmless.pdf}
    \caption{
    BoN sampling and $\mathrm{RBoN}_{\mathrm{L}}$ methods by measuring the TV distance between their output distributions and sentence length distribution $\mu^\prime$ in Harmlessness. This allows us to evaluate how closely each method's outputs align with the desired distribution, with a smaller TV distance indicating a preference for shorter sentences.
    }
    \label{fig:bon_l_harm}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{exp_img/TV_divergence/hh-helpful.pdf}
    \caption{
    BoN sampling and $\mathrm{RBoN}_{\mathrm{L}}$ methods by measuring the TV distance between their output distributions and sentence length distribution $\mu^\prime$ in Helpfulness. This allows us to evaluate how closely each method's outputs align with the desired distribution, with a smaller TV distance indicating a preference for shorter sentences.
    }
    \label{fig:bon_l_help}
\end{figure}

\newpage
Our analysis shows that the output probability of $\mathrm{RBoN}_{\mathrm{L}}$ deviates more from $\mu^\prime$ than the output probability of BoN sampling. \cref{table:gold_corelation} illustrates the correlation between the length of the sequence and the values of gold reference reward (Eurus-RM-7B), focusing on subsets of sentences comprising the top {5, 10, 15} based on the proxy reward values. The strength of this correlation is an indication of the effectiveness of $\mathrm{RBoN}_{\mathrm{L}}$; a stronger correlation indicates greater effectiveness of the method.

In \cref{table:gold_corelation}, we have highlighted in \textbf{bold} the instances of high correlation compared to all samples used correlation, which corresponds to superior performance as shown in \cref{res:table}. In contrast, areas with lower correlation tend to show lower performance. This pattern shows a consistent relationship between correlation strength and method effectiveness. We also explored an alternative view of PairRM that had a high correlation but did not produce correspondingly strong results in \cref{res:table}. 

We hypothesized that this discrepancy might be due to the range of the regularization parameter $\beta$. To investigate this hypothesis and to demonstrate the potential of $\mathrm{RBoN}_{\mathrm{L}}$, we performed an extensive analysis by varying $\beta$ over a wide range, from 10 to 5000 \cref{fig:pair_beta}.
% \begin{table}[h]
% \centering
% \small
% \caption{The table presents the mean and (variance) of the correlations between sentence token length and reward values. These correlations are calculated across 805 input instances, where each input corresponds to 128 output sentences}\label{table:correlation}
% \begin{tabular}{@{}lrrrrr@{}}
% \toprule
% \textbf{OASST} & \textbf{SHP-Large} & \textbf{SHP-XL} & \textbf{PairRM} & \textbf{RM-Mistral-7B} &\textbf{Eurus}\\ \midrule
% \rowcolor[HTML]{EFEFEF} 
% \multicolumn{6}{c}{\textbf{AlpacaFarm}} \\ \midrule
% 0.00 (0.29) & 0.32 (0.28)&  0.25 (0.31) & 0.01 (0.18) & 0.21 (0.32) & 0.11 (0.33) \\\midrule
% \rowcolor[HTML]{EFEFEF} 
% \multicolumn{6}{c}{\textbf{Harmlessness}} \\ \midrule
% -0.07 (0.40) & 0.45 (0.20) & 0.39 (0.24) & -0.13 (0.26) & -0.21 (0.52)& 0.08 (0.45) \\
%  \midrule
% \rowcolor[HTML]{EFEFEF} 
% \multicolumn{6}{c}{\textbf{Helpfulness}} \\ \midrule
% -0.06 (0.30) & 0.33 (0.32) & 0.25 (0.39) & 0.01 (0.19) & 0.30 (0.34) & 0.07 (0.40) \\
%  \bottomrule
% \end{tabular}
% \label{tab:diff}
% \end{table}


\begin{table}[ht]
\centering
\small
\caption{The correlation between sequence length and gold reference reward (Eurus-RM-7B) values, focusing on a subset of sentences that include the top {5, 10, 15} based on proxy reward values.}\label{table:gold_corelation}
\begin{tabular}{@{}lrrrrr@{}}
\toprule
 \textbf{Top N} &\textbf{OASST} & \textbf{SHP-Large} & \textbf{SHP-XL} & \textbf{PairRM} & \textbf{RM-Mistral-7B}\\ \midrule
\rowcolor[HTML]{EFEFEF} 
\multicolumn{6}{c}{\textbf{AlpacaFarm}} \\ \midrule
\textbf{All} &0.11 (0.33) & 0.11 (0.33)&  0.11 (0.33) & 0.11 (0.33) & 0.11 (0.33)  \\\midrule
\textbf{5} &\textbf{0.27} (0.55) & -0.04 (0.56)&  0.05 (0.55) & 0.15 (0.59) & 0.10 (0.56)  \\\midrule
\textbf{10} &\textbf{0.24} (0.44) & -0.02 (0.44)&  0.06 (0.41) & \textbf{0.17} (0.48) & 0.09 (0.56)  \\\midrule
\textbf{20} &\textbf{0.21} (0.39) & -0.02 (0.37)&  0.06 (0.36) & \textbf{0.16} (0.41) & 0.08 (0.44)  \\\midrule
\rowcolor[HTML]{EFEFEF} 
\multicolumn{6}{c}{\textbf{Harmlessness}} \\ \midrule
\textbf{All} &0.08 (0.45) & 0.08 (0.45)&  0.08 (0.45) & 0.08 (0.45) & 0.08 (0.45)  \\\midrule
\textbf{5} &\textbf{0.24} (0.58) & 0.10 (0.57)&  0.13 (0.58) & \textbf{0.20} (0.62) & \textbf{0.37} (0.51)  \\\midrule
\textbf{10} &\textbf{0.25} (0.50) & 0.11 (0.46)&  0.12 (0.47) & \textbf{0.19} (0.54) & \textbf{0.36} (0.41)  \\\midrule
\textbf{20} &\textbf{0.22} (0.47) & 0.11 (0.41)&  0.11 (0.43) & \textbf{0.21} (0.49) & \textbf{0.34} (0.39)  \\\midrule
\rowcolor[HTML]{EFEFEF} 
\multicolumn{6}{c}{\textbf{Helpfulness}} \\ \midrule
\textbf{All} &0.07 (0.40) & 0.07 (0.40)&  0.07 (0.40) & 0.07 (0.40) & 0.07 (0.40)  \\\midrule
\textbf{5} &\textbf{0.28} (0.56) & -0.04 (0.58)&  0.11 (0.54) & \textbf{0.14} (0.62) & 0.06 (0.54)  \\\midrule
\textbf{10} &\textbf{0.27} (0.47) & -0.05 (0.45)&  0.11 (0.42) & \textbf{0.15} (0.52) & 0.06 (0.40)  \\\midrule
\textbf{20} &\textbf{0.24} (0.43) & -0.06 (0.40)&  0.10 (0.37) & \textbf{0.17} (0.46) & 0.03 (0.36)  \\
 \bottomrule
\end{tabular}
\label{tab:diff2}
\end{table}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{img/PairRM_beta/beta.pdf}
    \caption{Performance analysis of $\mathrm{RBoN}_{\mathrm{L}}$ with varying $\beta$ (10 to 5000) across AlpacaFarm, Harmlessness, and Helpfulness datasets. PairRM and Eurus-RM-7B are used as proxy and gold reward models, respectively.}
    \label{fig:pair_beta}
\end{figure}

% \newpage
% \cref{fig:pal}, \cref{fig:pha}, and \cref{fig:phe} present the results when using PairRM as the gold reward model. This choice is based on the findings from \cref{table:correlation}, which demonstrate that PairRM exhibits the least influence from sentence length among the compared reward models.
% The results indicate that $\mathrm{RBoN}_{\mathrm{L}}$ demonstrates higher performance than $\mathrm{SRBoN}_{\mathrm{KL}}$. Compared to $\mathrm{SRBoN}_{\mathrm{WD}}$, $\mathrm{RBoN}_{\mathrm{L}}$ shows comparable performance in certain problem settings. However, it is observed that $\mathrm{RBoN}_{\mathrm{L}}$ exhibits reduced performance for specific reward models (SHP-Large and SHP-XL).


% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.9\linewidth]{exp_img/PairRM/alpaca.pdf}
%     \caption{
%     Evaluation of the decoder method on the AlpacaFarm dataset with varying parameter $\beta$. We use proxy reward models, OASST, SHP-Large, SHP-XL,   Eurus-RM-7B, and RM-Mistral-7B. As the gold reward model, we utilize PairRM.
%     }
%     \label{fig:pal}
% \end{figure}

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.9\linewidth]{exp_img/PairRM/hh-harmless.pdf}
%     \caption{
%     Evaluation of the decoder method on the Harmlessness dataset with varying parameter $\beta$. We use proxy reward models, OASST, SHP-Large, SHP-XL,   Eurus-RM-7B, and RM-Mistral-7B. As the gold reward model, we utilize PairRM.
%     }
%     \label{fig:pha}
% \end{figure}

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.9\linewidth]{exp_img/PairRM/hh-helpful.pdf}
%     \caption{
%     Evaluation of the decoder method on the Helpfulness dataset with varying parameter $\beta$. We use proxy reward models, OASST, SHP-Large, SHP-XL,   Eurus-RM-7B, and RM-Mistral-7B. As the gold reward model, we utilize PairRM.
%     }
%     \label{fig:phe}
% \end{figure}


\section{Experiment with Qwen2.5-7B-Instruct}
As an ablation study, we evaluate the methods using the Qwen (Qwen2.5-7B-Instruct) as the language model. Overall, we observe the same results as with Mistral-7B-SFT, where $\mathrm{RBoN}_{\mathrm{WD}}$ outperforms the baseline algorithms (Figure \ref{fig:qwen}).

% We compared the RBoN methods on the AlpacaFarm dataset's evaluation split using the Qwen (Qwen2.5-7B-Instruct) language model.
% The purpose of this analysis is to verify the performance of RBoN methods, even when applied to samples generated by other language models.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{exp_img/Qwen.pdf}
    \caption{Evaluation of the RBoN method on the AlpacaFarm dataset with varying parameter $\beta$. We use proxy reward models, OASST, SHP-Large, SHP-XL,  PairRM, and RM-Mistral-7B. As the gold reward model, we utilize Eurus-RM-7B, and Qwen as the language model.
    }
    \label{fig:qwen}
\end{figure}


\newpage
\section{Reproducibility Statement}
\label{appendix:reprod}

All datasets and models used in the experiments are publicly available (Table \ref{tab:links}). Our code will be available
as open source upon acceptance.


\begin{table*}
    \caption{List of datasets and models used in the experiments.}
    \label{tab:links}
    \centering
    % \adjustbox{max width=\textwidth}{
    \begin{tabularx}{\textwidth}{cX}
    \toprule
        Name & Reference \\
    \midrule
        AlpacaFarm & \cite{NEURIPS2023_5fc47800} \url{https://huggingface.co/datasets/tatsu-lab/alpaca_farm} \\\midrule
        Anthropic's hh-rlhf & \cite{bai2022training} \url{https://huggingface.co/datasets/Anthropic/hh-rlhf} \\\midrule
        mistral-7b-sft-beta (Mistral) & \cite{jiang2023mistral,tunstall2023zephyr} \url{https://huggingface.co/HuggingFaceH4/mistral-7b-sft-beta} \\\midrule
        Meta-Llama-3-8B-Instruct  (Llama) & \cite{dubey2024llama} \url{https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct} \\\midrule
        Qwen2.5-7B-Instruct (Qwen)& \cite{qwen2,qwen2.5} \url{https://huggingface.co/Qwen/Qwen2.5-7B-Instruct} \\\midrule
        SHP-Large & \cite{pmlr-v162-ethayarajh22a} \url{https://huggingface.co/stanfordnlp/SteamSHP-flan-t5-large} \\\midrule
        SHP-XL & \cite{pmlr-v162-ethayarajh22a} \url{https://huggingface.co/stanfordnlp/SteamSHP-flan-t5-xl} \\\midrule
        OASST & \cite{NEURIPS2023_949f0f8f} \url{https://huggingface.co/OpenAssistant/reward-model-deberta-v3-large-v2} \\\midrule
        PairRM & \cite{jiang-etal-2023-llm} \url{https://huggingface.co/llm-blender/PairRM} \\\midrule
        RM-Mistral-7B & \cite{dong2023raft} \url{https://huggingface.co/weqweasdas/RM-Mistral-7B} \\\midrule
Eurus-RM-7B & \cite{yuan2024advancing} \url{https://huggingface.co/openbmb/Eurus-RM-7b} \\\midrule
        Beaver & \cite{dai2024safe}\url{https://huggingface.co/PKU-Alignment/beaver-7b-v1.0-reward} \\\midrule
         Tulu & \cite{ivison2024unpacking} \url{https://huggingface.co/allenai/tulu-v2.5-ppo-13b-uf-mean-70b-uf-rm} \\\midrule
         Open Llama & \cite{diao-etal-2024-lmflow} \url{https://huggingface.co/weqweasdas/hh_rlhf_rm_open_llama_3b} \\\midrule
        MPNet & \cite{NEURIPS2020_c3a690be} \url{https://huggingface.co/sentence-transformers/all-mpnet-base-v2} \\
        \bottomrule
    \end{tabularx}
    % }
\end{table*}
\newpage

