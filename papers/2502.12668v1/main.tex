% This package is incurring error without  this option so I added this: it is not written in the TMLR template so maybe we need to fix it later on.

\PassOptionsToPackage{margin=0.3in}{geometry}
\documentclass[10pt]{article} % For LaTeX2e
% \usepackage{tmlr}
% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{tmlr}
% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:
%\usepackage[preprint]{tmlr}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
% \title{Robust Best-of-N Sampling for Language Model Alignment}
% \title{Evaluation of Regularized Best-of-N Sampling Strategies for Language Model Alignment}
\title{Evaluation of Best-of-N Sampling Strategies for Language Model Alignment}

% Authors must not appear in the submitted version. They should be hidden
% as long as the tmlr package is used without the [accepted] or [preprint] options.
% Non-anonymous submissions will be rejected without review.

\author{\name Yuki Ichihara \email  ichihara.yuki.iu1@is.naist.jp\\
      \addr Nara Institute of Science and Technology
      \AND
      \name Yuu Jinnai \email jinnai\_yu@cyberagent.co.jp \\
      \name Tetsuro Morimura \email morimura\_tetsuro@cyberagent.co.jp \\
      \name Kenshi Abe \email abe\_kenshi@cyberagent.co.jp \\
      \name Kaito Ariu \email kaito\_ariu@cyberagent.co.jp \\
      \name Mitsuki Sakamoto \email sakamoto\_mitsuki@cyberagent.co.jp \\
      \addr CyberAgent
      \AND
      \name Eiji Uchibe \email  uchibe@atr.jp\\
      \addr Advanced Telecommunications Research Institute International
}

% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{02}  % Insert correct month for camera-ready version
\def\year{2025} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=H4S4ETc8c9}} % Insert correct link to OpenReview for camera-ready version


\begin{document}


\maketitle

\begin{abstract}
% Best-of-N (BoN) sampling with a reward model has been shown to be an effective strategy for aligning Large Language Models (LLMs) to human preferences at the time of decoding.
% BoN sampling is susceptible to a problem known as \textit{reward hacking}. Because the reward model is an imperfect proxy for the true objective, over-optimizing its value can compromise its performance on the true objective. 
% Prior work proposes Regularized BoN (RBoN), a BoN sampling with a regularization to the objective so that it mitigates the reward hacking and empirically shows that it outperforms BoN sampling \citep{jinnai2024regularized}.
% However, \citet{jinnai2024regularized} introduce RBoN based on a heuristic and they lack the analysis of \textit{why} such regularization strategy improves the performance of BoN sampling.
% In this work, we analyze the effect of regularization strategies for the BoN sampling.
% We propose Stochastic RBoN (SRBoN), a variant of the RBoN that has a theoretical guarantee on the worst case performance bound.
% Using the regularization strategies correspond to distributional robust optimization, maximizing the worst case over a set of possible errors in the proxy reward from the true reference reward.
% Although the theoretical guarantees are not directly applicable to RBoN, as RBoN correspond to a practical implementation of SRBoN, it serves as an explanation of the efficiency of RBoN.
% We additionally conduct empirical evaluation using AlpacaFarm and Anthropic's hh-rlhf datasets to evaluate which factors of the regularization strategies contribute to the improvement of the true reference reward.

% Best-of-N (BoN) sampling with a reward model has been demonstrated to be an efficacious strategy for the alignment of Large Language Models (LLMs) with human preferences at the time of decoding.
% BoN sampling is susceptible to a problem known as \textit{reward hacking}. As the reward model is an imperfect proxy for the true objective, an excessive focus on optimizing its value may result in a compromise of its performance on the true objective. 
% Prior work proposes Regularized BoN (RBoN), a BoN sampling with regularization to the objective, and shows that it outperforms BoN sampling so that it mitigates the reward hacking and empirically \citep{jinnai2024regularized}.
% However, \citet{jinnai2024regularized} introduce RBoN based on a heuristic and they lack the analysis of \textit{why} such regularization strategy improves the performance of BoN sampling.
% The objective of this study is to analyze the effect of regularisation strategies of the BoN sampling.
% Using the regularization strategies corresponds to robust optimization, maximizing the worst case over a set of possible perturbations in the proxy reward.
% Although the theoretical guarantees are not directly applicable to RBoN, RBoN correspond to a practical implementation.
% This paper proposes an extension of the RBoN framework, termed Stochastic RBoN (SRBoN), which is a theoretically guaranteed approach to worst-case RBoN sampling in proxy reward.
% We additionally conduct an empirical evaluation using AlpacaFarm and Anthropic's hh-rlhf datasets to evaluate which factors of the regularization strategies contribute to the improvement of the true reference reward.

Best-of-N (BoN) sampling with a reward model has been shown to be an effective strategy for aligning Large Language Models (LLMs) with human preferences at the time of decoding.
BoN sampling is susceptible to a problem known as \textit{reward hacking}. Since the reward model is an imperfect proxy for the true objective, an excessive focus on optimizing its value can lead to a compromise of its performance on the true objective. 
Previous work proposes Regularized BoN sampling (RBoN), a BoN sampling with regularization to the objective, and shows that it outperforms BoN sampling so that it mitigates reward hacking and empirically \citep{jinnai2024regularized}.
However, \citet{jinnai2024regularized} introduce RBoN based on a heuristic and they lack the analysis of \textit{why} such regularization strategy improves the performance of BoN sampling.
The aim of this study is to analyze the effect of BoN sampling on regularization strategies.
Using the regularization strategies corresponds to robust optimization, which maximizes the worst case over a set of possible perturbations in the proxy reward.
Although the theoretical guarantees are not directly applicable to RBoN, RBoN corresponds to a practical implementation.
This paper proposes an extension of the RBoN framework, called Stochastic RBoN sampling (SRBoN), which is a theoretically guaranteed approach to worst-case RBoN in proxy reward.
We then perform an empirical evaluation using the AlpacaFarm and Anthropic's hh-rlhf datasets to evaluate which factors of the regularization strategies contribute to the improvement of the true proxy reward.
In addition, we also propose another simple RBoN method, the Sentence Length Regularized BoN, which has a better performance in the experiment as compared to the previous methods.
% In addition, we perform an empirical evaluation using the AlpacaFarm and Anthropic's hh-rlhf datasets to evaluate which factors of the regularization strategies contribute to the improvement of the true proxy reward.
% Derived from the analysis, we propose a regularization strategy based on k-Nearest Neighborhood (kNN) and show that it is a generalization of the previous work. 
% We conduct experiments using AlpacaFarm and Anthropic's hh-rlhf datasets and show that kNN-based regularization outperforms previously proposed regularization strategies.
\end{abstract}

\input{section/Introduction}

\input{section/Background}

\input{section/Proposed_Method}
% \section{Theoretical Analysis}
% When do WD-RBoN and kNN+BoN work?
% \input{section/Theory_y}
% \input{section/Theory_stochastic}
% \section{Experiments}
% We run experiments with a proxy reward model of

% Proxy = Gold + Gaussian Noise
% \input{section/Experiment}
\input{section/Experiment_stochastic}

% \newpage
% \section{Related Work}
% RLHF, DPO
\input{section/Related_Work}

% \section{Conclusions}
\input{section/Conclusions}


\section*{Acknowledgments}
We sincerely thank the Action Editor, Pascal Poupart, and the anonymous reviewers for their insightful comments and suggestions.
Kaito Ariu's research is supported by JSPS KAKENHI Grant No. 23K19986. 

% \input{trash/Knn_appendix}
% \newpage
% \newpage
\bibliography{ms,anthology}

\bibliographystyle{tmlr}
\newpage



\appendix
% \input{section/Appendix}
\input{section/Appendix_stochastic}

\end{document}
