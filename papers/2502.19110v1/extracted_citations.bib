@inproceedings{band2024linguistic,
      title={Linguistic Calibration of Long-Form Generations}, 
      author={Neil Band and Xuechen Li and Tengyu Ma and Tatsunori Hashimoto},
      booktitle={Forty-first International Conference on Machine Learning},
      year={2024},
      url={https://openreview.net/forum?id=rJVjQSQ8ye}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{cherian2024large,
  title={Large language model validity via enhanced conformal prediction methods},
  author={Cherian, John J and Gibbs, Isaac and Cand{\`e}s, Emmanuel J},
  journal={arXiv preprint arXiv:2406.09714},
  year={2024}
}

@inproceedings{fang2024learning,
  title={Learning to Defer with an Uncertain Rejector via Conformal Prediction},
  author={Fang, Yizirui and Nalisnick, Eric},
  booktitle={NeurIPS 2024 Workshop on Bayesian Decision-making and Uncertainty},
  year={2024}
}

@article{gui2024conformal,
  title={Conformal Alignment: Knowing When to Trust Foundation Models with Guarantees},
  author={Gui, Yu and Jin, Ying and Ren, Zhimei},
  journal={arXiv preprint arXiv:2405.10301},
  year={2024}
}

@inproceedings{gunjal-durrett-2024-molecular,
    title = "Molecular Facts: Desiderata for Decontextualization in {LLM} Fact Verification",
    author = "Gunjal, Anisha  and
      Durrett, Greg",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.215/",
    doi = "10.18653/v1/2024.findings-emnlp.215",
    pages = "3751--3768",
    abstract = "Automatic factuality verification of large language model (LLM) generations is becoming more and more widely used to combat hallucinations. A major point of tension in the literature is the granularity of this fact-checking: larger chunks of text are hard to fact-check, but more atomic facts like propositions may lack context to interpret correctly. In this work, we assess the role of context in these atomic facts. We argue that fully atomic facts are not the right representation, and define two criteria for molecular facts: decontextuality, or how well they can stand alone, and minimality, or how little extra information is added to achieve decontexuality. We quantify the impact of decontextualization on minimality, then present a baseline methodology for generating molecular facts automatically, aiming to add the right amount of information. We compare against various methods of decontextualization and find that molecular facts balance minimality with fact verification accuracy in ambiguous settings."
}

@article{hong2024hallucinations,
  title={The Hallucinations Leaderboard--An Open Effort to Measure Hallucinations in Large Language Models},
  author={Hong, Giwon and Gema, Aryo Pradipta and Saxena, Rohit and Du, Xiaotang and Nie, Ping and Zhao, Yu and Perez-Beltrachini, Laura and Ryabinin, Max and He, Xuanli and Fourrier, Cl{\'e}mentine and others},
  journal={arXiv preprint arXiv:2404.05904},
  year={2024}
}

@article{huang2023survey,
  title={A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions},
  author={Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and others},
  journal={ACM Transactions on Information Systems},
  year={2023},
  publisher={ACM New York, NY}
}

@article{jiang2024core,
  title={Core: Robust Factual Precision with Informative Sub-Claim Identification},
  author={Jiang, Zhengping and Zhang, Jingyu and Weir, Nathaniel and Ebner, Seth and Wanner, Miriam and Sanders, Kate and Khashabi, Daniel and Liu, Anqi and Van Durme, Benjamin},
  journal={arXiv preprint arXiv:2407.03572},
  year={2024}
}

@inproceedings{kamath2020selective,
  title={Selective Question Answering under Domain Shift},
  author={Kamath, Amita and Jia, Robin and Liang, Percy},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={5684--5696},
  year={2020}
}

@article{lee2024llm,
  title={Are LLM-Judges Robust to Expressions of Uncertainty? Investigating the effect of Epistemic Markers on LLM-based Evaluation},
  author={Lee, Dongryeol and Hwang, Yerin and Kim, Yongil and Park, Joonsuk and Jung, Kyomin},
  journal={arXiv preprint arXiv:2410.20774},
  year={2024}
}

@inproceedings{maynez2020faithfulness,
  title={On Faithfulness and Factuality in Abstractive Summarization},
  author={Maynez, Joshua and Narayan, Shashi and Bohnet, Bernd and McDonald, Ryan},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={1906--1919},
  year={2020}
}

@article{mielek2022reducing,
    author = {Mielke, Sabrina J. and Szlam, Arthur and Dinan, Emily and Boureau, Y-Lan},
    title = "{Reducing Conversational Agents’ Overconfidence Through Linguistic Calibration}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {10},
    pages = {857-872},
    year = {2022},
    month = {08},
    abstract = "{While improving neural dialogue agents’ factual accuracy is the object of much research, another important aspect of communication, less studied in the setting of neural dialogue, is transparency about ignorance. In this work, we analyze to what extent state-of-the-art chit-chat models are linguistically calibrated in the sense that their verbalized expression of doubt (or confidence) matches the likelihood that the model’s responses are factually incorrect (or correct). We find that these models are poorly calibrated, yet we show that likelihood of correctness can accurately be predicted. By incorporating such metacognitive features into the training of a controllable generation model, we obtain a dialogue agent with greatly improved linguistic calibration.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00494},
    url = {https://doi.org/10.1162/tacl\_a\_00494},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00494/2038516/tacl\_a\_00494.pdf},
}

@inproceedings{min-etal-2023-factscore,
    title = "{FA}ct{S}core: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation",
    author = "Min, Sewon  and
      Krishna, Kalpesh  and
      Lyu, Xinxi  and
      Lewis, Mike  and
      Yih, Wen-tau  and
      Koh, Pang  and
      Iyyer, Mohit  and
      Zettlemoyer, Luke  and
      Hajishirzi, Hannaneh",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.741",
    doi = "10.18653/v1/2023.emnlp-main.741",
    pages = "12076--12100",
    abstract = "Evaluating the factuality of long-form text generated by large language models (LMs) is non-trivial because (1) generations often contain a mixture of supported and unsupported pieces of information, making binary judgments of quality inadequate, and (2) human evaluation is time-consuming and costly. In this paper, we introduce FACTSCORE, a new evaluation that breaks a generation into a series of atomic facts and computes the percentage of atomic facts supported by a reliable knowledge source. We conduct an extensive human evaluation to obtain FACTSCOREs of people biographies generated by several state-of-the-art commercial LMs{---}InstructGPT, ChatGPT, and the retrieval-augmented PerplexityAI{---}and report new analysis demonstrating the need for such a fine-grained score (e.g., ChatGPT only achieves 58{\%}). Since human evaluation is costly, we also introduce an automated model that estimates FACTSCORE using retrieval and a strong language model, with less than a 2{\%} error rate. Finally, we use this automated metric to evaluate 6,500 generations from a new set of 13 recent LMs that would have cost {\$}26K if evaluated by humans, with various findings: GPT-4 and ChatGPT are more factual than public models, and Vicuna and Alpaca are some of the best public models. FACTSCORE is available for public use via {`}pip install factscore{`}.",
}

@inproceedings{mohri2024language,
  title={Language Models with Conformal Factuality Guarantees},
  year={2024},
  author={Mohri, Christopher and Hashimoto, Tatsunori},
  booktitle={Forty-first International Conference on Machine Learning}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{piche2024llms,
  title={LLMs can learn self-restraint through iterative self-reflection},
  author={Pich{\'e}, Alexandre and Milios, Aristides and Bahdanau, Dzmitry and Pal, Chris},
  journal={arXiv preprint arXiv:2405.13022},
  year={2024}
}

@inproceedings{quachconformal,
  title={Conformal Language Modeling},
  author={Quach, Victor and Fisch, Adam and Schuster, Tal and Yala, Adam and Sohn, Jae Ho and Jaakkola, Tommi S and Barzilay, Regina},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@inproceedings{renrobots,
  title={Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners},
  author={Ren, Allen Z and Dixit, Anushri and Bodrova, Alexandra and Singh, Sumeet and Tu, Stephen and Brown, Noah and Xu, Peng and Takayama, Leila and Xia, Fei and Varley, Jake and others},
  booktitle={7th Annual Conference on Robot Learning},
  year={2023}
}

@inproceedings{wang-etal-2024-factcheck,
    title = "Factcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic Fact-checkers",
    author = "Wang, Yuxia  and
      Gangi Reddy, Revanth  and
      Mujahid, Zain Muhammad  and
      Arora, Arnav  and
      Rubashevskii, Aleksandr  and
      Geng, Jiahui  and
      Mohammed Afzal, Osama  and
      Pan, Liangming  and
      Borenstein, Nadav  and
      Pillai, Aditya  and
      Augenstein, Isabelle  and
      Gurevych, Iryna  and
      Nakov, Preslav",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.830/",
    doi = "10.18653/v1/2024.findings-emnlp.830",
    pages = "14199--14230",
    abstract = "The increased use of large language models (LLMs) across a variety of real-world applications calls for mechanisms to verify the factual accuracy of their outputs. In this work, we present Factcheck-Bench, a holistic end-to-end framework for annotating and evaluating the factuality of LLM-generated responses, which encompasses a multi-stage annotation scheme designed to yield detailed labels for fact-checking and correcting not just the final prediction, but also the intermediate steps that a fact-checking system might need to take. Based on this framework, we construct an open-domain factuality benchmark in three-levels of granularity: claim, sentence, and document. We further propose a system, Factcheck-GPT, which follows our framework, and we show that it outperforms several popular LLM fact-checkers. We make our annotation tool, annotated data, benchmark, and code available at https://github.com/yuxiaw/Factcheck-GPT."
}

@article{wang2024calibrating,
  title={Calibrating Expressions of Certainty},
  author={Wang, Peiqi and Lam, Barbara D and Liu, Yingcheng and Asgari-Targhi, Ameneh and Panda, Rameswar and Wells, William M and Kapur, Tina and Golland, Polina},
  journal={arXiv preprint arXiv:2410.04315},
  year={2024}
}

@inproceedings{wanner-etal-2024-closer,
    title = "A Closer Look at Claim Decomposition",
    author = "Wanner, Miriam  and
      Ebner, Seth  and
      Jiang, Zhengping  and
      Dredze, Mark  and
      Van Durme, Benjamin",
    editor = "Bollegala, Danushka  and
      Shwartz, Vered",
    booktitle = "Proceedings of the 13th Joint Conference on Lexical and Computational Semantics (*SEM 2024)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.starsem-1.13",
    doi = "10.18653/v1/2024.starsem-1.13",
    pages = "153--175",
    abstract = "As generated text becomes more commonplace, it is increasingly important to evaluate how well-supported such text is by external knowledge sources. Many approaches for evaluating textual support rely on some method for decomposing text into its individual subclaims which are scored against a trusted reference. We investigate how various methods of claim decomposition{---}especially LLM-based methods{---}affect the result of an evaluation approach such as the recently proposed FActScore, finding that it is sensitive to the decomposition method used. This sensitivity arises because such metrics attribute overall textual support to the model that generated the text even though error can also come from the metric{'}s decomposition step. To measure decomposition quality, we introduce an adaptation of FActScore, which we call DecompScore. We then propose an LLM-based approach to generating decompositions inspired by Bertrand Russell{'}s theory of logical atomism and neo-Davidsonian semantics and demonstrate its improved decomposition quality over previous methods.",
}

@article{wanner2024dndscore,
  title={DnDScore: Decontextualization and Decomposition for Factuality Verification in Long-Form Text Generation},
  author={Wanner, Miriam and Van Durme, Benjamin and Dredze, Mark},
  journal={arXiv preprint arXiv:2412.13175},
  year={2024}
}

@article{yadkori2024mitigating,
  title={Mitigating llm hallucinations via conformal abstention},
  author={Yadkori, Yasin Abbasi and Kuzborskij, Ilja and Stutz, David and Gy{\"o}rgy, Andr{\'a}s and Fisch, Adam and Doucet, Arnaud and Beloshapka, Iuliya and Weng, Wei-Hung and Yang, Yao-Yuan and Szepesv{\'a}ri, Csaba and others},
  journal={arXiv preprint arXiv:2405.01563},
  year={2024}
}

@article{zhao2021calibrating,
  title={Calibrating predictions to decisions: A novel approach to multi-class calibration},
  author={Zhao, Shengjia and Kim, Michael and Sahoo, Roshni and Ma, Tengyu and Ermon, Stefano},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={22313--22324},
  year={2021}
}

