% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}

@inproceedings{band2024linguistic,
      title={Linguistic Calibration of Long-Form Generations}, 
      author={Neil Band and Xuechen Li and Tengyu Ma and Tatsunori Hashimoto},
      booktitle={Forty-first International Conference on Machine Learning},
      year={2024},
      url={https://openreview.net/forum?id=rJVjQSQ8ye}
}

@article{mielek2022reducing,
    author = {Mielke, Sabrina J. and Szlam, Arthur and Dinan, Emily and Boureau, Y-Lan},
    title = "{Reducing Conversational Agents’ Overconfidence Through Linguistic Calibration}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {10},
    pages = {857-872},
    year = {2022},
    month = {08},
    abstract = "{While improving neural dialogue agents’ factual accuracy is the object of much research, another important aspect of communication, less studied in the setting of neural dialogue, is transparency about ignorance. In this work, we analyze to what extent state-of-the-art chit-chat models are linguistically calibrated in the sense that their verbalized expression of doubt (or confidence) matches the likelihood that the model’s responses are factually incorrect (or correct). We find that these models are poorly calibrated, yet we show that likelihood of correctness can accurately be predicted. By incorporating such metacognitive features into the training of a controllable generation model, we obtain a dialogue agent with greatly improved linguistic calibration.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00494},
    url = {https://doi.org/10.1162/tacl\_a\_00494},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00494/2038516/tacl\_a\_00494.pdf},
}

@inproceedings{mohri2024language,
  title={Language Models with Conformal Factuality Guarantees},
  year={2024},
  author={Mohri, Christopher and Hashimoto, Tatsunori},
  booktitle={Forty-first International Conference on Machine Learning}
}

@inproceedings{quach2023conformal,
  title={Conformal Language Modeling},
  year={2024},
  author={Quach, Victor and Fisch, Adam and Schuster, Tal and Yala, Adam and Sohn, Jae Ho and Jaakkola, Tommi S and Barzilay, Regina},
  booktitle={The Twelfth International Conference on Learning Representations}
}

@incollection{maccartney2014natural,
  title={Natural logic and natural language inference},
  author={MacCartney, Bill and Manning, Christopher D},
  booktitle={Computing Meaning: Volume 4},
  pages={129--147},
  year={2014},
  publisher={Springer}
}

@inproceedings{pavlick-etal-2015-adding,
    title = "Adding Semantics to Data-Driven Paraphrasing",
    author = "Pavlick, Ellie  and
      Bos, Johan  and
      Nissim, Malvina  and
      Beller, Charley  and
      Van Durme, Benjamin  and
      Callison-Burch, Chris",
    editor = "Zong, Chengqing  and
      Strube, Michael",
    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P15-1146",
    doi = "10.3115/v1/P15-1146",
    pages = "1512--1522",
}

@article{stengel2022did,
  title={Why did the chicken cross the road? rephrasing and analyzing ambiguous questions in vqa},
  author={Stengel-Eskin, Elias and Guallar-Blasco, Jimena and Zhou, Yi and Van Durme, Benjamin},
  journal={arXiv preprint arXiv:2211.07516},
  year={2022}
}

@book{vovk2005algorithmic,
  title={Algorithmic learning in a random world},
  author={Vovk, Vladimir and Gammerman, Alexander and Shafer, Glenn},
  volume={29},
  year={2005},
  publisher={Springer}
}

@inproceedings{papadopoulos2002inductive,
  title={Inductive confidence machines for regression},
  author={Papadopoulos, Harris and Proedrou, Kostas and Vovk, Volodya and Gammerman, Alex},
  booktitle={Machine learning: ECML 2002: 13th European conference on machine learning Helsinki, Finland, August 19--23, 2002 proceedings 13},
  pages={345--356},
  year={2002},
  organization={Springer}
}

@inproceedings{angelopoulosuncertainty,
  title={Uncertainty Sets for Image Classifiers using Conformal Prediction},
  year={2021},
  author={Angelopoulos, Anastasios Nikolas and Bates, Stephen and Jordan, Michael and Malik, Jitendra},
  booktitle={International Conference on Learning Representations}
}

@article{schuster2022confident,
  title={Confident adaptive language modeling},
  author={Schuster, Tal and Fisch, Adam and Gupta, Jai and Dehghani, Mostafa and Bahri, Dara and Tran, Vinh and Tay, Yi and Metzler, Donald},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17456--17472},
  year={2022}
}

@article{wei2024measuring,
  title={Measuring short-form factuality in large language models},
  author={Wei, Jason and Karina, Nguyen and Chung, Hyung Won and Jiao, Yunxin Joy and Papay, Spencer and Glaese, Amelia and Schulman, John and Fedus, William},
  journal={arXiv e-prints},
  pages={arXiv--2411},
  year={2024}
}

@article{wang2024calibrating,
  title={Calibrating Expressions of Certainty},
  author={Wang, Peiqi and Lam, Barbara D and Liu, Yingcheng and Asgari-Targhi, Ameneh and Panda, Rameswar and Wells, William M and Kapur, Tina and Golland, Polina},
  journal={arXiv preprint arXiv:2410.04315},
  year={2024}
}

@inproceedings{kuhn2023semantic,
  title={Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation},
  year={2023},
  author={Kuhn, Lorenz and Gal, Yarin and Farquhar, Sebastian},
  booktitle={The Eleventh International Conference on Learning Representations}
}

@inproceedings{wang2024self,
  title={Self-Consistency Improves Chain of Thought Reasoning in Language Models},
  year={2022},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc V and Chi, Ed H and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  booktitle={The Eleventh International Conference on Learning Representations}
}

@article{lin2023generating,
  title={Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models},
  year={2023},
  author={Lin, Zhen and Trivedi, Shubhendu and Sun, Jimeng},
  journal={Transactions on Machine Learning Research}
}

@inproceedings{wanner-etal-2024-closer,
    title = "A Closer Look at Claim Decomposition",
    author = "Wanner, Miriam  and
      Ebner, Seth  and
      Jiang, Zhengping  and
      Dredze, Mark  and
      Van Durme, Benjamin",
    editor = "Bollegala, Danushka  and
      Shwartz, Vered",
    booktitle = "Proceedings of the 13th Joint Conference on Lexical and Computational Semantics (*SEM 2024)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.starsem-1.13",
    doi = "10.18653/v1/2024.starsem-1.13",
    pages = "153--175",
    abstract = "As generated text becomes more commonplace, it is increasingly important to evaluate how well-supported such text is by external knowledge sources. Many approaches for evaluating textual support rely on some method for decomposing text into its individual subclaims which are scored against a trusted reference. We investigate how various methods of claim decomposition{---}especially LLM-based methods{---}affect the result of an evaluation approach such as the recently proposed FActScore, finding that it is sensitive to the decomposition method used. This sensitivity arises because such metrics attribute overall textual support to the model that generated the text even though error can also come from the metric{'}s decomposition step. To measure decomposition quality, we introduce an adaptation of FActScore, which we call DecompScore. We then propose an LLM-based approach to generating decompositions inspired by Bertrand Russell{'}s theory of logical atomism and neo-Davidsonian semantics and demonstrate its improved decomposition quality over previous methods.",
}

@article{angelopoulos2021learn,
  title={Learn then test: Calibrating predictive algorithms to achieve risk control},
  author={Angelopoulos, Anastasios N and Bates, Stephen and Cand{\`e}s, Emmanuel J and Jordan, Michael I and Lei, Lihua},
  journal={arXiv preprint arXiv:2110.01052},
  year={2021}
}

@inproceedings{min-etal-2023-factscore,
    title = "{FA}ct{S}core: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation",
    author = "Min, Sewon  and
      Krishna, Kalpesh  and
      Lyu, Xinxi  and
      Lewis, Mike  and
      Yih, Wen-tau  and
      Koh, Pang  and
      Iyyer, Mohit  and
      Zettlemoyer, Luke  and
      Hajishirzi, Hannaneh",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.741",
    doi = "10.18653/v1/2023.emnlp-main.741",
    pages = "12076--12100",
    abstract = "Evaluating the factuality of long-form text generated by large language models (LMs) is non-trivial because (1) generations often contain a mixture of supported and unsupported pieces of information, making binary judgments of quality inadequate, and (2) human evaluation is time-consuming and costly. In this paper, we introduce FACTSCORE, a new evaluation that breaks a generation into a series of atomic facts and computes the percentage of atomic facts supported by a reliable knowledge source. We conduct an extensive human evaluation to obtain FACTSCOREs of people biographies generated by several state-of-the-art commercial LMs{---}InstructGPT, ChatGPT, and the retrieval-augmented PerplexityAI{---}and report new analysis demonstrating the need for such a fine-grained score (e.g., ChatGPT only achieves 58{\%}). Since human evaluation is costly, we also introduce an automated model that estimates FACTSCORE using retrieval and a strong language model, with less than a 2{\%} error rate. Finally, we use this automated metric to evaluate 6,500 generations from a new set of 13 recent LMs that would have cost {\$}26K if evaluated by humans, with various findings: GPT-4 and ChatGPT are more factual than public models, and Vicuna and Alpaca are some of the best public models. FACTSCORE is available for public use via {`}pip install factscore{`}.",
}

@inproceedings{angelopoulos2023conformal,
  title={Conformal Risk Control},
  year={2023},
  author={Angelopoulos, Anastasios Nikolas and Bates, Stephen and Fisch, Adam and Lei, Lihua and Schuster, Tal},
  booktitle={The Twelfth International Conference on Learning Representations}
}

@article{gupta2022nested,
  title={Nested conformal prediction and quantile out-of-bag ensemble methods},
  author={Gupta, Chirag and Kuchibhotla, Arun K and Ramdas, Aaditya},
  journal={Pattern Recognition},
  volume={127},
  pages={108496},
  year={2022},
  publisher={Elsevier}
}

@article{lee2024llm,
  title={Are LLM-Judges Robust to Expressions of Uncertainty? Investigating the effect of Epistemic Markers on LLM-based Evaluation},
  author={Lee, Dongryeol and Hwang, Yerin and Kim, Yongil and Park, Joonsuk and Jung, Kyomin},
  journal={arXiv preprint arXiv:2410.20774},
  year={2024}
}

@article{bates2021distribution,
  title={Distribution-free, risk-controlling prediction sets},
  author={Bates, Stephen and Angelopoulos, Anastasios and Lei, Lihua and Malik, Jitendra and Jordan, Michael},
  journal={Journal of the ACM (JACM)},
  volume={68},
  number={6},
  pages={1--34},
  year={2021},
  publisher={ACM New York, NY}
}

@article{kwiatkowski-etal-2019-natural,
    title = "Natural Questions: A Benchmark for Question Answering Research",
    author = "Kwiatkowski, Tom  and
      Palomaki, Jennimaria  and
      Redfield, Olivia  and
      Collins, Michael  and
      Parikh, Ankur  and
      Alberti, Chris  and
      Epstein, Danielle  and
      Polosukhin, Illia  and
      Devlin, Jacob  and
      Lee, Kenton  and
      Toutanova, Kristina  and
      Jones, Llion  and
      Kelcey, Matthew  and
      Chang, Ming-Wei  and
      Dai, Andrew M.  and
      Uszkoreit, Jakob  and
      Le, Quoc  and
      Petrov, Slav",
    editor = "Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "7",
    year = "2019",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q19-1026",
    doi = "10.1162/tacl_a_00276",
    pages = "452--466",
    abstract = "We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.",
}

@article{jiang2024core,
  title={Core: Robust Factual Precision with Informative Sub-Claim Identification},
  author={Jiang, Zhengping and Zhang, Jingyu and Weir, Nathaniel and Ebner, Seth and Wanner, Miriam and Sanders, Kate and Khashabi, Daniel and Liu, Anqi and Van Durme, Benjamin},
  journal={arXiv preprint arXiv:2407.03572},
  year={2024}
}

@misc{wei2024long,
  title={Long-form factuality in large language models},
  author={Wei, Jerry and Yang, Chengrun and Song, Xinying and Lu, Yifeng and Hu, Nathan and Huang, Jie and Tran, Dustin and Peng, Daiyi and Liu, Ruibo and Huang, Da and Du, Cosmo and Le, Quoc V.},
  year={2024},
  url={https://arxiv.org/abs/2403.18802},
}

@inproceedings{song-etal-2024-veriscore,
    title = "{V}eri{S}core: Evaluating the factuality of verifiable claims in long-form text generation",
    author = "Song, Yixiao  and
      Kim, Yekyung  and
      Iyyer, Mohit",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.552/",
    doi = "10.18653/v1/2024.findings-emnlp.552",
    pages = "9447--9474",
    abstract = "Existing metrics for evaluating the factuality of long-form text, such as FACTSCORE (Min et al., 2023) and SAFE (Wei et al., 2024), decompose an input text into {\textquotedblleft}atomic claims{\textquotedblright} and verify each against a knowledge base like Wikipedia. These metrics are not suitable for most generation tasks because they assume that every claim is verifiable (i.e., can plausibly be proven true or false). We address this issue with VERISCORE,1 a metric for evaluating factuality in diverse long-form generation tasks that contain both verifiable and unverifiable content. VERISCORE can be effectively implemented with either closed or fine-tuned open-weight language models. Human evaluation confirms that VERISCORE`s extracted claims are more sensible than those from competing methods across eight different long-form tasks. We use VERISCORE to evaluate generations from 16 different models across multiple long-form tasks and find that while GPT-4o is the best-performing model overall, open-weight models such as Mixtral-8{\texttimes}22 are closing the gap. We show that an LM`s VERISCORE on one task (e.g., biography generation) does not necessarily correlate to its VERISCORE on a different task (e.g., long-form QA), highlighting the need for expanding factuality evaluation across tasks with varying fact density."
}

@article{zhao2024wildhallucinations,
  title={Wildhallucinations: Evaluating long-form factuality in llms with real-world entity queries},
  author={Zhao, Wenting and Goyal, Tanya and Chiu, Yu Ying and Jiang, Liwei and Newman, Benjamin and Ravichander, Abhilasha and Chandu, Khyathi and Bras, Ronan Le and Cardie, Claire and Deng, Yuntian and others},
  journal={arXiv preprint arXiv:2407.17468},
  year={2024}
}

@article{robertson1995okapi,
  title={Okapi at TREC-3},
  author={Robertson, Stephen E and Walker, Steve and Jones, Susan and Hancock-Beaulieu, Micheline M and Gatford, Mike and others},
  journal={Nist Special Publication Sp},
  volume={109},
  pages={109},
  year={1995},
  publisher={National Instiute of Standards \& Technology}
}

@article{rafailov2024direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{rafailov2024r,
  title={From $r$ to $Q^{*}$: Your Language Model is Secretly a Q-Function},
  author={Rafailov, Rafael and Hejna, Joey and Park, Ryan and Finn, Chelsea},
  journal={arXiv preprint arXiv:2404.12358},
  year={2024}
}

@article{razin2024unintentional,
  title={Unintentional unalignment: Likelihood displacement in direct preference optimization},
  author={Razin, Noam and Malladi, Sadhika and Bhaskar, Adithya and Chen, Danqi and Arora, Sanjeev and Hanin, Boris},
  journal={arXiv preprint arXiv:2410.08847},
  year={2024}
}

@inproceedings{hong2024orpo,
  title={Orpo: Monolithic preference optimization without reference model},
  author={Hong, Jiwoo and Lee, Noah and Thorne, James},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={11170--11189},
  year={2024}
}

@article{cherian2024large,
  title={Large language model validity via enhanced conformal prediction methods},
  author={Cherian, John J and Gibbs, Isaac and Cand{\`e}s, Emmanuel J},
  journal={arXiv preprint arXiv:2406.09714},
  year={2024}
}

@book{stalnaker1984inquiry,
  title={Inquiry},
  author={Stalnaker, Robert},
  publisher={MIT Press},
  year={1984}
}

@article{lewis1979attitudes,
  title={Attitudes de dicto and de se},
  author={Lewis, David},
  journal={The philosophical review},
  volume={88},
  number={4},
  pages={513--543},
  year={1979},
  publisher={JSTOR}
}

@InCollection{sep-logic-conditionals,
	author       =	{Egré, Paul and Rott, Hans},
	title        =	{{The Logic of Conditionals}},
	booktitle    =	{The {Stanford} Encyclopedia of Philosophy},
	editor       =	{Edward N. Zalta},
	howpublished =	{\url{https://plato.stanford.edu/archives/win2021/entries/logic-conditionals/}},
	year         =	{2021},
	edition      =	{{W}inter 2021},
	publisher    =	{Metaphysics Research Lab, Stanford University}
}

@article{lemmon1959jaakko,
  title={Jaakko Hintikka. Modality as referential multiplicity. Ajatus, vol. 20 (1957), pp. 49--64.},
  author={Hintikka, Jaakko},
  journal={The Journal of Symbolic Logic},
  volume={24},
  number={2},
  pages={179--179},
  year={1959},
  publisher={Cambridge University Press}
}

@article{hintikka1961modality,
  title={Modality and quantification},
  author={Hintikka, Jaakko},
  journal={Theoria},
  volume={27},
  number={3},
  pages={119--128},
  year={1961},
  publisher={Wiley Online Library}
}

@article{kripke1959completeness,
  title={A completeness theorem in modal logic1},
  author={Kripke, Saul A},
  journal={The journal of symbolic logic},
  volume={24},
  number={1},
  pages={1--14},
  year={1959},
  publisher={Cambridge University Press}
}

@article{kripke1963semantical,
  title={Semantical analysis of modal logic i normal modal propositional calculi},
  author={Kripke, Saul A},
  journal={Mathematical Logic Quarterly},
  volume={9},
  number={5-6},
  pages={67--96},
  year={1963},
  publisher={WILEY-VCH Verlag Berlin GmbH Berlin}
}

@article{bayart1958correction,
  title={CORRECTION DE LA LOGIQUE MODALE DU PREMIER ET DU SECOND ORDRE S 5},
  author={Bayart, Arnould},
  journal={Logique et Analyse},
  volume={1},
  number={1},
  pages={28--45},
  year={1958},
  publisher={JSTOR}
}

@article{bayart1959quasi,
  title={Quasi-ad{\'e}quation de la logique modale du second ordre S5 et ad{\'e}quation de la logique modale du premier ordre S5},
  author={Bayart, Arnould},
  journal={Logique et analyse},
  volume={2},
  number={6/7},
  pages={99--121},
  year={1959},
  publisher={JSTOR}
}

@incollection{adams2013theories,
  title={Theories of actuality},
  author={Adams, Robert Merrihew},
  booktitle={Particulars, Actuality, and Identity over Time, vol 4},
  pages={321--341},
  year={2013},
  publisher={Routledge}
}

@article{plantinga1976actualism,
  title={Actualism and possible worlds},
  author={Plantinga, Alvin},
  journal={Theoria},
  volume={42},
  number={1-3},
  pages={139--160},
  year={1976},
  publisher={Blackwell Publishing Ltd Oxford, UK}
}

@article{fine2005prior,
  title={Prior on the construction of possible worlds and instants},
  author={Fine, Kit},
  year={2005}
}

@article{shafer2008tutorial,
  title={A tutorial on conformal prediction.},
  author={Shafer, Glenn and Vovk, Vladimir},
  journal={Journal of Machine Learning Research},
  volume={9},
  number={3},
  year={2008}
}

@incollection{papadopoulos2008inductive,
  title={Inductive conformal prediction: Theory and application to neural networks},
  author={Papadopoulos, Harris},
  booktitle={Tools in artificial intelligence},
  year={2008},
  publisher={Citeseer}
}

@article{rashkin2023measuring,
  title={Measuring attribution in natural language generation models},
  author={Rashkin, Hannah and Nikolaev, Vitaly and Lamm, Matthew and Aroyo, Lora and Collins, Michael and Das, Dipanjan and Petrov, Slav and Tomar, Gaurav Singh and Turc, Iulia and Reitter, David},
  journal={Computational Linguistics},
  volume={49},
  number={4},
  pages={777--840},
  year={2023},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@article{wanner2024dndscore,
  title={DnDScore: Decontextualization and Decomposition for Factuality Verification in Long-Form Text Generation},
  author={Wanner, Miriam and Van Durme, Benjamin and Dredze, Mark},
  journal={arXiv preprint arXiv:2412.13175},
  year={2024}
}

@article{tang2024minicheck,
  title={MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents},
  author={Tang, Liyan and Laban, Philippe and Durrett, Greg},
  journal={arXiv preprint arXiv:2404.10774},
  year={2024}
}

@inproceedings{chen2022generating,
  title={Generating Literal and Implied Subquestions to Fact-check Complex Claims},
  author={Chen, Jifan and Sriram, Aniruddh and Choi, Eunsol and Durrett, Greg},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={3495--3516},
  year={2022}
}

@inproceedings{wu-etal-2023-qudeval,
    title = "{QUD}eval: The Evaluation of Questions Under Discussion Discourse Parsing",
    author = "Wu, Yating  and
      Mangla, Ritika  and
      Durrett, Greg  and
      Li, Junyi Jessy",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.325/",
    doi = "10.18653/v1/2023.emnlp-main.325",
    pages = "5344--5363",
    abstract = "Questions Under Discussion (QUD) is a versatile linguistic framework in which discourse progresses as continuously asking questions and answering them. Automatic parsing of a discourse to produce a QUD structure thus entails a complex question generation task: given a document and an answer sentence, generate a question that satisfies linguistic constraints of QUD and can be grounded in an anchor sentence in prior context. These questions are known to be curiosity-driven and open-ended. This work introduces the first framework for the automatic evaluation of QUD parsing, instantiating the theoretical constraints of QUD in a concrete protocol. We present QUDeval, a dataset of fine-grained evaluation of 2,190 QUD questions generated from both fine-tuned systems and LLMs. Using QUDeval, we show that satisfying all constraints of QUD is still challenging for modern LLMs, and that existing evaluation metrics poorly approximate parser quality. Encouragingly, human-authored QUDs are scored highly by our human evaluators, suggesting that there is headroom for further progress on language modeling to improve both QUD parsing and QUD evaluation."
}

@inproceedings{chen-etal-2021-nli-models,
    title = "Can {NLI} Models Verify {QA} Systems' Predictions?",
    author = "Chen, Jifan  and
      Choi, Eunsol  and
      Durrett, Greg",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.324/",
    doi = "10.18653/v1/2021.findings-emnlp.324",
    pages = "3841--3854",
    abstract = "To build robust question answering systems, we need the ability to verify whether answers to questions are truly correct, not just {\textquotedblleft}good enough{\textquotedblright} in the context of imperfect QA datasets. We explore the use of natural language inference (NLI) as a way to achieve this goal, as NLI inherently requires the premise (document context) to contain all necessary information to support the hypothesis (proposed answer to the question). We leverage large pre-trained models and recent prior datasets to construct powerful question conversion and decontextualization modules, which can reformulate QA instances as premise-hypothesis pairs with very high reliability. Then, by combining standard NLI datasets with NLI examples automatically derived from QA training data, we can train NLI models to evaluate QA models' proposed answers. We show that our approach improves the confidence estimation of a QA model across different domains, evaluated in a selective QA setting. Careful manual analysis over the predictions of our NLI model shows that it can further identify cases where the QA model produces the right answer for the wrong reason, i.e., when the answer sentence cannot address all aspects of the question."
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@inproceedings{petroni-etal-2019-language,
    title = "Language Models as Knowledge Bases?",
    author = {Petroni, Fabio  and
      Rockt{\"a}schel, Tim  and
      Riedel, Sebastian  and
      Lewis, Patrick  and
      Bakhtin, Anton  and
      Wu, Yuxiang  and
      Miller, Alexander},
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1250/",
    doi = "10.18653/v1/D19-1250",
    pages = "2463--2473",
    abstract = "Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as {\textquotedblleft}fill-in-the-blank{\textquotedblright} cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at \url{https://github.com/facebookresearch/LAMA}."
}

@article{roberts2020much,
  title={How much knowledge can you pack into the parameters of a language model?},
  author={Roberts, Adam and Raffel, Colin and Shazeer, Noam},
  journal={arXiv preprint arXiv:2002.08910},
  year={2020}
}

@article{safavi2021relational,
  title={Relational world knowledge representation in contextual language models: A review},
  author={Safavi, Tara and Koutra, Danai},
  journal={arXiv preprint arXiv:2104.05837},
  year={2021}
}

@article{yuan2024towards,
  title={Towards a Holistic Evaluation of LLMs on Factual Knowledge Recall},
  author={Yuan, Jiaqing and Pan, Lin and Hang, Chung-Wei and Guo, Jiang and Jiang, Jiarong and Min, Bonan and Ng, Patrick and Wang, Zhiguo},
  journal={arXiv preprint arXiv:2404.16164},
  year={2024}
}

@inproceedings{maynez2020faithfulness,
  title={On Faithfulness and Factuality in Abstractive Summarization},
  author={Maynez, Joshua and Narayan, Shashi and Bohnet, Bernd and McDonald, Ryan},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={1906--1919},
  year={2020}
}

@article{huang2023survey,
  title={A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions},
  author={Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and others},
  journal={ACM Transactions on Information Systems},
  year={2023},
  publisher={ACM New York, NY}
}

@inproceedings{
tian2024finetuning,
title={Fine-Tuning Language Models for Factuality},
author={Katherine Tian and Eric Mitchell and Huaxiu Yao and Christopher D Manning and Chelsea Finn},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=WPZ2yPag4K}
}

@article{xie2024improving,
  title={Improving model factuality with fine-grained critique-based evaluator},
  author={Xie, Yiqing and Zhou, Wenxuan and Prakash, Pradyot and Jin, Di and Mao, Yuning and Fettes, Quintin and Talebzadeh, Arya and Wang, Sinong and Fang, Han and Rose, Carolyn and others},
  journal={arXiv preprint arXiv:2410.18359},
  year={2024}
}

@inproceedings{chuang2023dola,
  title={DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models},
  author={Chuang, Yung-Sung and Xie, Yujia and Luo, Hongyin and Kim, Yoon and Glass, James R and He, Pengcheng},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@inproceedings{leefactuality,
title={Factuality Enhanced Language Models for Open-Ended Text Generation},
author={Nayeon Lee and Wei Ping and Peng Xu and Mostofa Patwary and Pascale Fung and Mohammad Shoeybi and Bryan Catanzaro},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=LvyJX20Rll}
}

@inproceedings{weller2024according,
  title={“According to...”: Prompting Language Models Improves Quoting from Pre-Training Data},
  author={Weller, Orion and Marone, Marc and Weir, Nathaniel and Lawrie, Dawn and Khashabi, Daniel and Van Durme, Benjamin},
  booktitle={Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2288--2301},
  year={2024}
}

@article{zhang2024verifiable,
  title={Verifiable by Design: Aligning Language Models to Quote from Pre-Training Data},
  author={Zhang, Jingyu and Marone, Marc and Li, Tianjian and Van Durme, Benjamin and Khashabi, Daniel},
  journal={arXiv preprint arXiv:2404.03862},
  year={2024}
}

@article{gao2022rarr,
  title={Rarr: Researching and revising what language models say, using language models},
  author={Gao, Luyu and Dai, Zhuyun and Pasupat, Panupong and Chen, Anthony and Chaganty, Arun Tejasvi and Fan, Yicheng and Zhao, Vincent Y and Lao, Ni and Lee, Hongrae and Juan, Da-Cheng and others},
  journal={arXiv preprint arXiv:2210.08726},
  year={2022}
}

@article{rodriguez2019quizbowl,
  title={Quizbowl: The case for incremental question answering},
  author={Rodriguez, Pedro and Feng, Shi and Iyyer, Mohit and He, He and Boyd-Graber, Jordan},
  journal={arXiv preprint arXiv:1904.04792},
  year={2019}
}

@inproceedings{kamath2020selective,
  title={Selective Question Answering under Domain Shift},
  author={Kamath, Amita and Jia, Robin and Liang, Percy},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={5684--5696},
  year={2020}
}

@article{madras2018predict,
  title={Predict responsibly: improving fairness and accuracy by learning to defer},
  author={Madras, David and Pitassi, Toni and Zemel, Richard},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{mohrilearning,
  title={Learning to Reject with a Fixed Predictor: Application to Decontextualization},
  author={Mohri, Christopher and Andor, Daniel and Choi, Eunsol and Collins, Michael and Mao, Anqi and Zhong, Yutao},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@inproceedings{chengcan,
  title={Can AI Assistants Know What They Don't Know?},
  author={Cheng, Qinyuan and Sun, Tianxiang and Liu, Xiangyang and Zhang, Wenwei and Yin, Zhangyue and Li, Shimin and Li, Linyang and He, Zhengfu and Chen, Kai and Qiu, Xipeng},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@inproceedings{
ren2023robots,
title={Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners},
author={Allen Z. Ren and Anushri Dixit and Alexandra Bodrova and Sumeet Singh and Stephen Tu and Noah Brown and Peng Xu and Leila Takayama and Fei Xia and Jake Varley and Zhenjia Xu and Dorsa Sadigh and Andy Zeng and Anirudha Majumdar},
booktitle={7th Annual Conference on Robot Learning},
year={2023},
url={https://openreview.net/forum?id=4ZK8ODNyFXx}
}

@article{cortes2024cardinality,
  title={Cardinality-Aware Set Prediction and Top-$ k $ Classification},
  author={Cortes, Corinna and Mao, Anqi and Mohri, Christopher and Mohri, Mehryar and Zhong, Yutao},
  journal={arXiv preprint arXiv:2407.07140},
  year={2024}
}

@inproceedings{quachconformal,
  title={Conformal Language Modeling},
  author={Quach, Victor and Fisch, Adam and Schuster, Tal and Yala, Adam and Sohn, Jae Ho and Jaakkola, Tommi S and Barzilay, Regina},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@article{jung2024trust,
  title={Trust or Escalate: LLM Judges with Provable Guarantees for Human Agreement},
  author={Jung, Jaehun and Brahman, Faeze and Choi, Yejin},
  journal={CoRR},
  year={2024}
}

@article{liang2024controllable,
  title={Controllable Text Generation for Large Language Models: A Survey},
  author={Liang, Xun and Wang, Hanyu and Wang, Yezhaohui and Song, Shichao and Yang, Jiawei and Niu, Simin and Hu, Jie and Liu, Dan and Yao, Shunyu and Xiong, Feiyu and others},
  journal={CoRR},
  year={2024}
}

@article{yadkori2024mitigating,
  title={Mitigating llm hallucinations via conformal abstention},
  author={Yadkori, Yasin Abbasi and Kuzborskij, Ilja and Stutz, David and Gy{\"o}rgy, Andr{\'a}s and Fisch, Adam and Doucet, Arnaud and Beloshapka, Iuliya and Weng, Wei-Hung and Yang, Yao-Yuan and Szepesv{\'a}ri, Csaba and others},
  journal={arXiv preprint arXiv:2405.01563},
  year={2024}
}

@inproceedings{tian2023just,
  title={Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback},
  author={Tian, Katherine and Mitchell, Eric and Zhou, Allan and Sharma, Archit and Rafailov, Rafael and Yao, Huaxiu and Finn, Chelsea and Manning, Christopher D},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={5433--5442},
  year={2023}
}

@inproceedings{cruzevaluating,
  title={Evaluating language models as risk scores},
  author={Cruz, Andr{\'e} F and Hardt, Moritz and Mendler-D{\"u}nner, Celestine},
  booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track}
}

@inproceedings{tanneru2024quantifying,
  title={Quantifying uncertainty in natural language explanations of large language models},
  author={Tanneru, Sree Harsha and Agarwal, Chirag and Lakkaraju, Himabindu},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1072--1080},
  year={2024},
  organization={PMLR}
}

@article{feng2024don,
  title={Don't Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration},
  author={Feng, Shangbin and Shi, Weijia and Wang, Yike and Ding, Wenxuan and Balachandran, Vidhisha and Tsvetkov, Yulia},
  journal={arXiv preprint arXiv:2402.00367},
  year={2024}
}

@inproceedings{deutschmann2024conformal,
  title={Conformal autoregressive generation: Beam search with coverage guarantees},
  author={Deutschmann, Nicolas and Alberts, Marvin and Mart{\'\i}nez, Mar{\'\i}a Rodr{\'\i}guez},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={10},
  pages={11775--11783},
  year={2024}
}

@article{chaudhry2024finetuning,
  title={Finetuning language models to emit linguistic expressions of uncertainty},
  author={Chaudhry, Arslan and Thiagarajan, Sridhar and Gorur, Dilan},
  journal={arXiv preprint arXiv:2409.12180},
  year={2024}
}

@article{hong2024hallucinations,
  title={The Hallucinations Leaderboard--An Open Effort to Measure Hallucinations in Large Language Models},
  author={Hong, Giwon and Gema, Aryo Pradipta and Saxena, Rohit and Du, Xiaotang and Nie, Ping and Zhao, Yu and Perez-Beltrachini, Laura and Ryabinin, Max and He, Xuanli and Fourrier, Cl{\'e}mentine and others},
  journal={arXiv preprint arXiv:2404.05904},
  year={2024}
}

@inproceedings{wang-etal-2024-factcheck,
    title = "Factcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic Fact-checkers",
    author = "Wang, Yuxia  and
      Gangi Reddy, Revanth  and
      Mujahid, Zain Muhammad  and
      Arora, Arnav  and
      Rubashevskii, Aleksandr  and
      Geng, Jiahui  and
      Mohammed Afzal, Osama  and
      Pan, Liangming  and
      Borenstein, Nadav  and
      Pillai, Aditya  and
      Augenstein, Isabelle  and
      Gurevych, Iryna  and
      Nakov, Preslav",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.830/",
    doi = "10.18653/v1/2024.findings-emnlp.830",
    pages = "14199--14230",
    abstract = "The increased use of large language models (LLMs) across a variety of real-world applications calls for mechanisms to verify the factual accuracy of their outputs. In this work, we present Factcheck-Bench, a holistic end-to-end framework for annotating and evaluating the factuality of LLM-generated responses, which encompasses a multi-stage annotation scheme designed to yield detailed labels for fact-checking and correcting not just the final prediction, but also the intermediate steps that a fact-checking system might need to take. Based on this framework, we construct an open-domain factuality benchmark in three-levels of granularity: claim, sentence, and document. We further propose a system, Factcheck-GPT, which follows our framework, and we show that it outperforms several popular LLM fact-checkers. We make our annotation tool, annotated data, benchmark, and code available at https://github.com/yuxiaw/Factcheck-GPT."
}

@inproceedings{gunjal-durrett-2024-molecular,
    title = "Molecular Facts: Desiderata for Decontextualization in {LLM} Fact Verification",
    author = "Gunjal, Anisha  and
      Durrett, Greg",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.215/",
    doi = "10.18653/v1/2024.findings-emnlp.215",
    pages = "3751--3768",
    abstract = "Automatic factuality verification of large language model (LLM) generations is becoming more and more widely used to combat hallucinations. A major point of tension in the literature is the granularity of this fact-checking: larger chunks of text are hard to fact-check, but more atomic facts like propositions may lack context to interpret correctly. In this work, we assess the role of context in these atomic facts. We argue that fully atomic facts are not the right representation, and define two criteria for molecular facts: decontextuality, or how well they can stand alone, and minimality, or how little extra information is added to achieve decontexuality. We quantify the impact of decontextualization on minimality, then present a baseline methodology for generating molecular facts automatically, aiming to add the right amount of information. We compare against various methods of decontextualization and find that molecular facts balance minimality with fact verification accuracy in ambiguous settings."
}

@article{gui2024conformal,
  title={Conformal Alignment: Knowing When to Trust Foundation Models with Guarantees},
  author={Gui, Yu and Jin, Ying and Ren, Zhimei},
  journal={arXiv preprint arXiv:2405.10301},
  year={2024}
}

@inproceedings{fang2024learning,
  title={Learning to Defer with an Uncertain Rejector via Conformal Prediction},
  author={Fang, Yizirui and Nalisnick, Eric},
  booktitle={NeurIPS 2024 Workshop on Bayesian Decision-making and Uncertainty},
  year={2024}
}

@inproceedings{renrobots,
  title={Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners},
  author={Ren, Allen Z and Dixit, Anushri and Bodrova, Alexandra and Singh, Sumeet and Tu, Stephen and Brown, Noah and Xu, Peng and Takayama, Leila and Xia, Fei and Varley, Jake and others},
  booktitle={7th Annual Conference on Robot Learning},
  year={2023}
}

@article{piche2024llms,
  title={LLMs can learn self-restraint through iterative self-reflection},
  author={Pich{\'e}, Alexandre and Milios, Aristides and Bahdanau, Dzmitry and Pal, Chris},
  journal={arXiv preprint arXiv:2405.13022},
  year={2024}
}

@article{zhao2021calibrating,
  title={Calibrating predictions to decisions: A novel approach to multi-class calibration},
  author={Zhao, Shengjia and Kim, Michael and Sahoo, Roshni and Ma, Tengyu and Ermon, Stefano},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={22313--22324},
  year={2021}
}

@inproceedings{goodman2024degrees,
  title={Degrees of confidence are not subjective probabilities},
  author={Goodman, Jeremy},
  booktitle={Proceedings of Sinn und Bedeutung},
  volume={28},
  pages={329--344},
  year={2024}
}

@book{lewis2013counterfactuals,
  title={Counterfactuals},
  author={Lewis, David},
  year={1973},
  publisher={John Wiley \& Sons}
}

@article{cariani2018confidence,
  title={Confidence reports},
  author={Cariani, Fabrizio and Santorio, Paolo and Wellwood, Alexis},
  year={2018}
}

@inproceedings{dagan2005pascal,
  title={The pascal recognising textual entailment challenge},
  author={Dagan, Ido and Glickman, Oren and Magnini, Bernardo},
  booktitle={Machine learning challenges workshop},
  pages={177--190},
  year={2005},
  organization={Springer}
}

@inproceedings{Manning2006LOCALTI,
  title={LOCAL TEXTUAL INFERENCE : IT'S HARD TO CIRCUMSCRIBE , BUT YOU KNOW IT WHEN YOU SEE IT - AND NLP NEEDS IT},
  author={Christopher D. Manning},
  year={2006},
  url={https://api.semanticscholar.org/CorpusID:60340094}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}