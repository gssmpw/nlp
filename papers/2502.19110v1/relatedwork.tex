\section{Related Work}
\paragraph{LLM Factuality Evaluation} With the rise of large language models (LLMs) \citep{brown2020language, ouyang2022training}, long-form text generation has become widespread, but LLMs frequently hallucinate, generating content misaligned with real-world facts or user inputs \citep{maynez2020faithfulness, huang2023survey, hong2024hallucinations}.

To improve factuality evaluation, pipelines have been developed to decompose text into smaller claims and verify them against external sources \citep{min-etal-2023-factscore, wang-etal-2024-factcheck, wanner-etal-2024-closer}. These methods focus on factual precision—the proportion of verifiably true claims—while refinements enhance claim extraction \citep{wanner2024dndscore, gunjal-durrett-2024-molecular, jiang2024core}. However, they often overlook factors like hedging \citep{lee2024llm} and the role of controlled imprecision in overall factuality.
In this work, we propose a pipeline that makes this trade-off explicit, advocating for a more detailed study of the interplay between factuality and other key quality dimensions in open-ended generation evaluation.


\paragraph{Conformal Prediction for Language Generation} Applying conformal prediction to free-form generation is challenging due to the absence of a finite answer set, making classic split conformal methods inapplicable. Instead, prior work has focused on structured decision-making tasks such as overgeneration \citep{quachconformal}, abstention \citep{kamath2020selective, yadkori2024mitigating, gui2024conformal, piche2024llms}, delegation \citep{fang2024learning}, and clarification \citep{renrobots} to mitigate the risk of incorrect outputs. The most relevant approach, \citet{mohri2024language}, enhances factuality by decomposing text and omitting uncertain claims, a form of abstention. This method was later refined with adaptive conformal prediction by \citet{cherian2024large}, which improves the calibration of uncertainty while maintaining informative responses. Unlike these methods, we perform post-hoc calibration while preserving the original generation space, maintaining interpretability and coherence for downstream applications.

\paragraph{Linguistic Calibration} An alternative approach to mitigating large language model (LLM) overconfidence is linguistic calibration, where the model explicitly expresses uncertainty to better align its outputs with factual accuracy \citep{mielek2022reducing}. This concept has been further extended to estimating the probabilities associated with uncertainty quantifiers \citep{wang2024calibrating}, allowing models to more precisely convey their confidence levels. Additionally, recent research has explored integrating verbalized uncertainty with other quantitative calibration techniques \citep{band2024linguistic, zhao2021calibrating}, demonstrating that a hybrid approach can enhance decision-making by providing a more nuanced representation of uncertainty. These advancements highlight the growing potential of linguistic calibration as a key tool in improving the reliability and interpretability of LLM-generated responses.

While prior work primarily adds uncertainty markers while preserving the original text, our approach modifies generation itself to introduce imprecision, offering a distinct way to convey uncertainty. This not only enhances interpretability but also provides a structured mechanism to balance specificity and reliability in language generation.