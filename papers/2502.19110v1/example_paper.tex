%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{tikz}
\usepackage{tabularx}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{shadows} % LATEX and plain TEX

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[preprint]{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cref..
\usepackage[capitalize,noabbrev]{cleveref}

\usepackage{multirow}
% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{longtable}
\usepackage{xcolor, colortbl}

\usepackage{amsfonts}

\usepackage{booktabs}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage{makecell}
\usepackage{pifont}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage[normalem]{ulem}
\usepackage{graphicx}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\definecolor{mylightblue}{RGB}{85,167,207}
\definecolor{mygoodgreen}{RGB}{117, 179, 84}
\definecolor{guess}{HTML}{f678a7}
\definecolor{confident}{HTML}{add2e4}
\newcommand{\zhengping}[1]{\textcolor{mylightblue}{[Zhengping]: #1}}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Conformal Linguistic Calibration}

\newcommand{\qcrtext}[1]{{\fontfamily{qcr}\selectfont #1}}

\begin{document}

\twocolumn[
\icmltitle{Conformal Linguistic Calibration:\\Trading-off between Factuality and Specificity}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Zhengping Jiang}{jhu}
\icmlauthor{Anqi Liu}{jhu}
\icmlauthor{Benjamin Van Durme}{jhu}

%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{jhu}{Department of Computer Science, Johns Hopkins University, Baltimore, MD, United States}

\icmlcorrespondingauthor{Zhengping Jiang}{zjiang31@jhu.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}

%The growing prevalence and capabilities of large language models (LLMs) have raised concerns about their factual reliability, particularly regarding issues of hallucination and overconfidence in generated content. Existing post-hoc methods to address these issues are often either too restrictive—completely preventing models from attempting uncertain tasks—or too vague, making evaluation and interpretation in downstream applications challenging.

%In this work, we introduce \textit{Conformal Linguistic Calibration}, a novel framework that bridges linguistic calibration with conformal guarantees for factual accuracy. By expressing model confidence at varying levels of specificity, our approach reveals an interpretable trade-off between certainty and factuality. At the same time, it enables probabilistic correctness guarantees through conformal prediction.

Language model outputs are not always reliable; this prompts research into methods for adapting model responses based on uncertainty. Common approaches include: \emph{abstention}, where models refrain from generating responses when uncertain; and \emph{linguistic calibration}, where models hedge their statements using uncertainty quantifiers. However, abstention can withhold valuable information, while linguistically calibrated responses are often challenging to leverage in downstream tasks. We propose a unifying view of both approaches, Conformal Linguistic Calibration (CLC), reinterpreting linguistic calibration as answer set prediction. We begin by presenting a unified framework that connects abstention and linguistic calibration through the lens of linguistic pragmatics. We then describe an implementation that allows for controlling the level of imprecision in model responses. Experimental results show that our method produces calibrated outputs with conformal guarantees on factual accuracy. Furthermore, our approach enables fine-tuning models to perform uncertainty-aware adaptive claim rewriting, offering a controllable balance between factuality and specificity.\footnotemark[2] \footnotetext[2]{Preprint, under review.}



%CLC provides a mechanism for trading off between the specifity of a response with language model confidence. 

%By expressing model confidence at varying levels of specificity, our approach reveals an interpretable trade-off between certainty and factuality. At the same time, it enables probabilistic correctness guarantees through conformal prediction.


%Experiments demonstrate that CLC produces calibrated responses that satisfy conformal guarantees while achieving higher accuracy. Additionally, models can be fine-tuned to perform uncertainty-aware adaptive claim back-off, leading to improved factual reliability.

\end{abstract}

\section{Introduction}

Large language models (LLMs) can provide knowledge responses based on their  comprehensive training sets \citep{petroni-etal-2019-language, safavi2021relational, yuan2024towards}. These responses are not entirely reliable \citep{maynez2020faithfulness, huang2023survey}, and can be over-confident \citep{mielek2022reducing}, thereby motivating work to improve factuality.\footnotemark[3] \footnotetext[3]{See, e.g.,  \citep{gao2022rarr,  leefactuality, chuang2023dola, tian2024finetuning, tang2024minicheck, xie2024improving, weller2024according, zhang2024verifiable} and references therein.}



\begin{figure}[t]
%    \includegraphics[width=\linewidth]{example-image-a}
% \begin{center}
%     \small
% \begin{tabular}{r|l}
%     Question & What is the largest US city? \\
%     Correct Answer & New York City\\
%     \midrule
%     Calibration & Los Angeles (62\%)\\
%     Linguistic Calibration & Possibly Los Angeles\\
%     Conformal Ling. Calibration & A coastal city \\
% \end{tabular}
% \end{center}
%    \caption{Comparing different linguistic calibration paradigms. }
%\caption{Response uncertainty can be conveyed through: (1) reporting a confidence score on a precise prediction; (2) qualifying a precise response in language; or as explored here, (3) \emph{backing off} to a response that is less precise, but more confident.}

\begin{tikzpicture}[every shadow/.style={opacity=.8,fill=brown!50!black}]
 \node[rectangle,drop shadow,fill=white, draw=black,thick, minimum width=\linewidth, minimum height=3em]
    at (0, 0) {\textbf{Q:} What is the largest US city?};
\node[rectangle, drop shadow, fill=blue!10, draw=blue, thick, anchor=west, minimum width=0.47\linewidth, minimum height=3em] (lingca) at (.25, -1.4) {Possibly Los Angeles};
\node[rectangle, drop shadow, fill=blue!10, draw=blue, thick, align=center, anchor=east, minimum width=0.47\linewidth, minimum height=3em] (lingc) at (-.25, -1.4) {Linguistic Calibration\\\cite{mielek2022reducing}};

\node[rectangle, drop shadow, fill=blue!10, draw=blue, thick, anchor=west, minimum width=0.47\linewidth, minimum height=3em] (ca) at (.25, -2.8) {60\% Los Angeles};
\node[rectangle, drop shadow, fill=blue!10, draw=blue, thick, align=center, anchor=east, minimum width=0.47\linewidth, minimum height=3em] (c) at (-.25, -2.8) {Calibration \\\cite{band2024linguistic}};

\node[rectangle, drop shadow, fill=blue!10, draw=blue, thick, anchor=west, minimum width=0.47\linewidth, minimum height=3em] (abstentiona) at (.25, -4.2) {\sout{Los Angeles}};
\node[rectangle, drop shadow, fill=blue!10, draw=blue, thick, align=center, anchor=east, minimum width=0.47\linewidth, minimum height=3em] (abstention) at (-.25, -4.2) {Abstention \\ \small\citep{mohri2024language}};

\node[rectangle, drop shadow, fill=orange!10, draw=orange, thick, anchor=west, minimum width=0.47\linewidth, minimum height=3em] (thisa) at (.25, -5.6) {\textbf{A coastal city}};
\node[rectangle, drop shadow, fill=orange!10, draw=orange, thick, align=center, anchor=east, minimum width=0.47\linewidth, minimum height=3em] (this) at (-.25, -5.6) {\textbf{CLC}\\\textbf{(This Work)}};

\draw[-Triangle] (lingc.east) to[left] (lingca.west);
\draw[-Triangle] (c.east) to[left] (ca.west);
\draw[-Triangle] (abstention.east) to[left] (abstentiona.west);
\draw[-Triangle] (this.east) to[right] (thisa.west);
\end{tikzpicture}
\caption{Model uncertainty can be conveyed by providing a calibrated score on a precise response, or through qualification. CLC allows generating a less precise but more confident statement (answer by population is \emph{New York City}, by area is \emph{Sitka}).}
\label{fig1}
\end{figure}

A complimentary approach is to consider how to best reflect potential model errors in a response, or otherwise modify responses to be less likely to contain inaccuracies. This  includes preventing a model from responding when it is not confident \citep{rodriguez2019quizbowl, kamath2020selective, madras2018predict, mohrilearning, chengcan, feng2024don}.
Or, marking a response explicitly with the model's level of uncertainy \citep{tian2023just, cruzevaluating, tanneru2024quantifying}. When these uncertainty expressions are conveyed through generation and are adjusted to faithfully represent the accuracy of the generation, this is called \textit{linguistic calibration} \citep{mielek2022reducing, band2024linguistic, chaudhry2024finetuning, wang2024calibrating}. For example, a model uncertain in its response may state: ``\emph{Possibly} the largest city in the US is Los Angeles''  (Fig~\ref{fig1}). However, how to interpret linguistic uncertainty can be unclear to humans. When using verbalized quantifiers, it is unclear what ``likely'', ``I don't know but I think it is'', ``I guess'' conveys as exact uncertainty levels. When using scores, the uncertainty expressed by the model is still unclear, as ``I think there is an  80\% chance...'' could mean the model is 80\% sure, not that the answer is 80\% correct.\footnote{Verbalized-uncertainty is only calibrated to model uncertainty, not to accuracy~\citep{band2024linguistic}.} Also, the outer structure introduced by linguistic calibration hampers the downstream task / evaluation \citep{lee2024llm}, as it is unclear what special treatment should be given to these uncertain assertions.

Here we recognize that when humans are uncertain they may choose not to rely on qualification, they instead may hedge by providing a less precise answer. Formally, a linguistic claim can be viewed as a constraint on the set of possible worlds that are being committed to 
%may includes the certain claim being true under a possible world interpretation of modals 
\citep{kripke1963semantical, kripke1959completeness},  corresponding to the speaker's \textit{belief} \citep{lewis1979attitudes, stalnaker1984inquiry}. By providing claims that are more or less precise, this corresponds respectively to a committment to smaller or larger sets of alternative possible worlds. This allows us to view linguistic calibration as a form of ``soft abstension'', where traditional ``abstention'' \citep{kamath2020selective, mohri2024language} corresponds to committing to a universal set of all possible worlds.

%Based on these intuition 
We propose \emph{Conformal Linguistic Calibration}: capturing model uncertainty through the generation of responses that commit to more or less possible worlds. We probe the semantic uncertainty for model responses, and interpret alternative answers by forming nested sets. Leveraging conformal prediction \citep{vovk2005algorithmic, angelopoulos2021learn}, we find proper responses correspond to the correct set of possible worlds that provide probabilistic guarantees on factuality. This allows us to improve the flexibility and controllability over an abstention-based approach, while avoiding the challenges of prior approaches to linguistic calibration. CLC leads to  imprecise but clear claims, that are easily consumable downstream. We: % as  lies in the same space of original claim. We:


%Under this read making a linguistically calibrated assertion can be viewed as a ``soft abstention'', with traditional absention corresponds to taking all possible worlds as implicit ``prediction''. This understanding inspires a new form of linguistic calibration, where instead of specify the vague belief of an uncertainty model with quantifiers, we direclty describe the belief in natural language, which results in a more general but likely more factual response -- A process we termed \textit{Conformal Linguistic Calibration}.

% There are many immediate benefit of this approach over traditional linguistic calibration. First, conformal linguistic calibration provides precise factual guarantee to LLM generation, while such conformal guarnatee is hard for traditional linguistic calibration which usually only attempts to align to some empirical uncertainty quantification \citep{mielek2022reducing, band2024linguistic, chaudhry2024finetuning}. Second, since the output of the calibration process lies in the same space of original outputs as verifiable facts, we avoid the subtler notions that are often associated with uncertainty quantifiers,\footnote{E.g., that some modal languages do not satisfy substitutivity.} which often render downstream factuality evaluation very underspecified and problematic \citep{lee2024llm, jiang2024core}. Finally, linguistically-calibrated response through our process enjoys better interpretability. By providing response at different level of specificity \citep{stengel2022did}, we provide a single, understandable and semantic-aware \citep{maccartney2014natural, pavlick-etal-2015-adding} outputs that characterizes model belief far better than mere outer structure.

%In summary, we:
\begin{enumerate}
\item Propose a new linguistic calibration paradigm  characterizing model belief at varying levels of specificity. 

\item Provide an algorithmic instantiation of the process by probing semantic uncertainty \citep{kuhn2023semantic}, building nested subsets, and summarizing beliefs, which can be post-hoc calibrated through learn-then-test (LTT) \citep{angelopoulos2021learn}.

\item Show that our approach leads to a 7B model that achieves higher accuracy than GPT-4o on a challenging QA dataset \citep{wei2024measuring}, as well as improving accuracy on a popular dataset \citep{kwiatkowski-etal-2019-natural}.

\item Without in-domain factuality labels, we train an uncertainty-aware claim rewriter that improves long-form factuality by making a claim imprecise when answering very specifically is challenging for the uncerlying model, and release our dataset and model for rewriting.

\end{enumerate}

\section{Preliminaries}

We focus on the setting where given a prompt $x$, a language model $L$ generates response $y = L(x)$ where $y \in \mathcal{Y}$. The overall goal is to find a post processor $\mathcal{T}: \mathcal{Y} \rightarrow \mathcal{Y}$, that ensures a probabilistic guarantee for a user specified probability $\alpha \in (0, 1)$:
\begin{align*}
    \mathbb{P}(\mathcal{T}(y) \ \text{is factuality correct}) \geq 1 - \alpha.
\end{align*}
One particular challenge is to find a $\mathcal{T}$ that will almost always work without too many constraints on $\mathcal{Y}$. For example \citet{mohri2024language} shows that for longer generations one can come up with a simple solution for $\mathcal{T}$ where one can drop a subset of claims, a process they called \textit{back-off}, but this will not work for more atomic generations like in question answering, as \textit{back-off} is highly restrained by explicitly stated claims. Yet atopic fact level operation is very desirable, as previous works have demonstrated the benefit of decomposition for various fact verification problems \citep{min-etal-2023-factscore, jiang2024core, rashkin2023measuring, wanner2024dndscore, tang2024minicheck}. To allow being less specific beyond the surface form, we need a more sophisticated process for identifying plausible alternatives to the input claim to guide post-processing. We now describe how the post-processing step $\mathcal{T}$ can be formalized in terms of \textit{belief}. This helps connect back to linguistic calibration, and we provide a way to achieve our desired guarantee despite the additional complexity our proposal introduces.

\subsection{Possible World Semantics}
\label{subsec:possible-world}
The notion of a \textit{possible world} has a long tradition in philosophy, described as the ``limit of a series of increasingly more inclusive situations'' \citep{sep-logic-conditionals}. In particular, \citet{kripke1959completeness}’s possible-world semantics provide the foundation for modal logic by interpreting modal expressions in terms of accessibility relations between possible worlds. Specifically, a Kriepke Model $M = (W, \text{Rel}, \Vdash)$ is defined as a tuple: a set of possible worlds $W$, an accesibility relationship $R$ where $\text{Rel}(w, u)$ means ``possible world $u$ is accessible from possible world $w$'', and a satisfaction function $\Vdash$ that assigns a truth value to a modal formula w.r.t. a possible world $w$. Our proposal relies on the notion of necessity and possibility, these are defined as follows.

%Possible world semantics \citep{lemmon1959jaakko, hintikka1961modality, kripke1959completeness, kripke1963semantical, bayart1958correction, bayart1959quasi} in propositional and first-order modal logic supports an  \textit{uncertainty quantifier} interpretation of linguistic modal operators such as ``necessarily'' or ``possibly''. These linguistic operators are the common  target of linguistic calibration \citep{mielek2022reducing}.

%A possible world interpretation $M$ of a modal language $\mathcal{L}$ specifies a nonempty set $D$ of ``possible individuals'' of $M$. Additionally, $M$ specifies a set $W$ of ``possible worlds'' of $M$, and each world $w \in W$ is assigned its own domain of quantification $d(w) \subset D$, which are all the individuals that exist in $w$. Thus the truth value of a claim\footnote{Throughout this work we view a claim as a \textit{nullary predicate} that does not take arguments.}can be modeled as a function of the world $M_c: \mathcal{W} \rightarrow \{T, F\}$. From there the "necessarily" operation $\square_W$ can be defined such that

\begin{definition}[Necessity]
    \label{definition:necessity}
    Under $M$ at $w \in W$, for a given claim $c$\footnote{Throughout this work we view a claim as a \textit{nullary predicate} that does not take arguments.}, a necessitation $\square c$ is true iff $\forall u \in W$ \ \text{s.t.} \ \text{Rel}(w, u), u $\Vdash$ c is True.
\end{definition}
And the ``possibly'' operator $\Diamond$ naturally follows as
\begin{definition}[Possibility]
    \label{definition:possibility}
\begin{equation*}
    \Diamond c \coloneqq \neg \square \neg c.
\end{equation*}
\end{definition}

As different accessability relationships can be defined for different modals, \citet{hintikka1961modality} extends the Kripke model to account for \textit{believe} with a plausibility-based accesibility relationship $\text{Rel}_B$ that aims to satisfy empirical constraints of human belief. While this type of semantics does not support graded confidence reports \citep{cariani2018confidence}, \citet{goodman2024degrees} details a compatible account of degree of confidence, built on a notion of nested sets from a similarity ordering $\succeq$: 
\begin{align*}
    S \coloneqq \Big\{\{u\ \text{s.t.}\ u \succeq v \} \ \text{s.t.}\ v \in W\Big\},
\end{align*}
where each set within is called a \textit{sphere} \citep{lewis2013counterfactuals}. Now suppose an agent given subjective probability according to $\text{Pr}_{A, w}$ with regard to a possible world $w$, the degree of confidence $\text{Conf}^d(w)$ can be defined as the set of possible worlds:
\begin{align*}
    \text{Conf}^d(w) = \bigcap \Big\{ p \in S \ \text{s.t.}\ \text{Pr}_{w}(p) > d\  \Big\}.
\end{align*}
It can be verified that $\text{Conf}^d(w)$ is also nested, and each $\text{Conf}^d(w)$ can be repackaged into an accessibility relationship to define a series of graded confidence modals. This account allows us to interpret confidence as a threshold to select a subset of possible worlds, an intuition we rely on to build our new realization of linguistic calibration in \cref{sec:methods}.

\subsection{Conformal Prediction}
\paragraph{Split Conformal Prediction} \citep{vovk2005algorithmic, shafer2008tutorial, papadopoulos2008inductive} provides standardized tools to construct prediction sets that provide coverage guarantees. Concretely, using a calibration dataset $(X_i, Y_i)_{i = 1, \dots, n}$, split conformal prediction gurantees that for i.i.d. sample $(X_{\text{test}}, Y_{\text{test}})$ with a prediction set $C(X_{\text{test}}) \in 2^\mathcal{Y}$, then for any designated target threshold $\alpha \in (0, 1)$
\begin{align}
    \label{equation:conformal-guarantee}
	\mathbb{P}(Y_{\text{test}} \in C(X_{\text{test}})) \geq 1 - \alpha.
\end{align}
Following the view of \citet{gupta2022nested}, the split conformal prediction procedure starts from a sequence of nested candidate output sets, and use calibration data to select the correct level of in the nested set until the coverage guarantee is achieved. 

However, this approach requires that the prediction sets to select from are nested, or similarly, the non-conformity score, or equivalently the set construction is by thresholding on a sequence of monotonous non-conformity scores \citep{angelopoulos2023conformal}. Instead of using the quantiles of a scoring function, a more general extension of conformal prediction called Learn-Then-Test (LTT) \citep{angelopoulos2021learn} relies on hypothesis testing to identify a viable region to control any hyper-parameter sensitive risk.
\paragraph{Learn-Then-Test (LTT)} extends conformal prediction to find a hyperparameter configuration $\lambda$ control the expectation of any risk function $L$ such that
\begin{align*}
    \mathbb{P}(\sup_{\lambda \in \Lambda} R(\lambda) \leq \epsilon) \geq 1 - \alpha.
\end{align*}
Unlike conformal prediction or conformal risk control \citep{angelopoulos2023conformal}, LTT does not rely on the risk function being monotonous on $\lambda$. To achieve this, LTT associates the null hypothesis: $\mathcal{H}_{\lambda}: R(\lambda) > \epsilon$ to each configuration $\lambda$, and calculating a conservative p-value \citep{bates2021distribution} for each of the hypotheses, from which the LTT guarantee directly follows.

\section{Methods}
\label{sec:methods}
In this section, we outline the procedure for deriving the risk-controlled process $\mathcal{T}$, as illustrated in \autoref{fig:overview}. Building on our previous discussion in \cref{subsec:possible-world}, the objective is to leverage conformal prediction techniques to manage the risk associated with adherence to each level of the nested sphere. The underlying intuition is that the confidence level $V$ can equivalently be represented by a claim $\tilde{c}$ that \textit{describes} the sphere $V$\footnote{With a slight abuse of notation, we denote the necessity operator associated with modal logic $M = (W, R_U, \Vdash)$ by $\square_U$, where the accessibility relation is defined as $R_U(w, u) \coloneqq \mathbb{I}[u \in U]$. A similar convention applies to the possibility operator $\Diamond$.}. We formalize this property as follows:

\begin{definition}[Description]
    \label{definition:description}
    A claim $\tilde{c}$ is said to describe a sphere $V$ iff
    \begin{align}
    \label{eq:characterization}
        (\square_V \tilde{c})\wedge(\neg\Diamond_{W\setminus V}\tilde{c}).
    \end{align}
\end{definition}
Thus given a set of source claims $\{c_1, c_2, \dots, c_N\}$, we aim to rewrite them to a set of less specific (or imprecise) claims $\{b_1, b_2, \dots, b_N\}$ that each properly describes its corresponding possible world set (sphere) $\{W_1, W_2, \dots, W_N\}$ such that for any designated target threshold $\alpha \in (0, 1)$
\begin{equation*}
	\mathbb{P}\Big[\Diamond_W c | W = W_c \Big] \geq 1 - \alpha,
\end{equation*}
which matches the guarantee in \cref{equation:conformal-guarantee}. However, there is no practical way to directly evaluate $\square_b c$ as there is no way to constructively derive the set $W_b$. In this section, we describe a data processing pipeline that for each claim $c$ derives a sequence of candidate target claims $b_c^1, b_c^2, \dots, b_c^K$ such that the corresponding possible world sets in theory satisfy the nested assumption $W^1_c \subset W^2_c \subset \cdots \subset W^K_c$.

\begin{figure*}
    \includegraphics[width=\linewidth, trim=0 20 0 0]{media/FlowChartConformalBackoff.pdf}
    \caption{The overview of our conformal linguistic calibration approach. Instead of relying on direct operations like subclaim drop-off, we probe the model's internal belief by semantically clustering all sampled answers into nested sets, and writing less specific claims that are associated to each answer set level through \cref{eq:characterization}}
    \label{fig:overview}
\end{figure*}

\subsection{Less Specific Rewriting}
\label{subsec:claim-backoff}
In this section, we outline the pipeline to derive a less specific claim from a source answer string, with the goal of having the factuality risk of the generated claim properly controlled. We call this process claim rewriting. Our claim data is sourced from QA datasets, motivated by recent advancements in decomposition \citep{wanner-etal-2024-closer, gunjal-durrett-2024-molecular}, which demonstrate that complex text can be broken down into relatively simple, targeted questions \cite{chen2022generating, wu-etal-2023-qudeval}. Furthermore, prior work has established that conversion between questions and claims is feasible and natural \citep{chen-etal-2021-nli-models}.

\paragraph{Answer Sampling} Given a question $q$, we repeatedly sample $K$ responses from a language model $L$, denoted as ${a_k^q}$.\footnote{For simplicity, we omit the superscript when the dependence on $q$ is clear.} This approach is a standard technique for estimating predictive uncertainty in natural language generation \citep{wang2024self, kuhn2023semantic, band2024linguistic}.

\paragraph{Identifying Clusters}
From the sampled answer set $A$, we identify semantically unique clusters ${c}_{i=1}^{Q}$ \citep{kuhn2023semantic}. Previous approaches typically employ an entailment model or its extensions to establish an equivalence relationship—such as bidirectional entailment—to induce clusters \citep{kuhn2023semantic, lin2023generating}. However, while the number of clusters identified using this method serves as a useful indicator of response uncertainty, the clustering itself tends to be quite noisy. This is partially due to the inherent difficulty of fully defining an equivalence relationship in real-world scenarios. When operationalized through a Natural Language Inference (NLI) model \citep{dagan2005pascal, Manning2006LOCALTI}, such relationships often suffer from a lack of deep semantic understanding and excessive sensitivity to surface variations.

To address these issues, we instead use a single LLM call to directly generate a list of identifiable unique answer cluster names from $A$. The details of this prompt, along with other relevant prompts, are provided in \cref{sec:prompt-templates}.

\paragraph{Estimating Answer Multiplicity}
Given the answer set $A$ and a set of representative unique answers $C$, we estimate the multiplicity of each cluster $c_i \in C$ by counting the number of answers that are semantically equivalent to the corresponding cluster name. The cluster assignment $\delta_i(a_k)$ is determined using a similarity metric $s(\cdot, \cdot) \in [0, 1]$, which assigns $a_i$ to the $j$-th cluster such that
\begin{equation*}
    \delta_i(a^q_k) = \begin{cases}
        1\quad \text{if}\ \ i = \argmax_{\tilde{i} \in |Q|} s(a_k, c_{\tilde{i}})\\
        0\quad \text{otherwise}
    \end{cases}.
\end{equation*}
with arbitrary tiebreak. And the multiplicity is simply given through
\begin{equation*}
    m(c_i) = \sum_{k = 1}^K \delta_i(a_k).
\end{equation*}
We acknowledge that while accurately estimating $m(c)$ is important, minor noise in answer cluster assignment does not compromise the validity of the pipeline. This is because the entire rewriting process is calibrated using the Learn-Then-Test (LTT) framework, as described in \cref{subsec:ltt}.

\paragraph{Constructing Nested Cluster Sets}
In line with the approach of \citet{wang2024self}, we employ majority voting to identify the most confident answer, which we designate as the original answer $\tilde{c_1}$ where
\begin{equation*}
\begin{aligned}
    \tilde{c}_1 &= \argmax_{i\in|Q|} m(c_i),\\
    \tilde{C_1} &= \{\tilde{c_1}\}.
\end{aligned}
\end{equation*}
We then incrementally add other clusters into the set to form a set of nested cluster sets
\begin{equation*}
    \tilde{C_1} \subset \tilde{C_2} \subset \dots \subset \tilde{C}_N.
\end{equation*}

To better align with the interpretation described in \cref{subsec:possible-world}, we prioritize clusters that are semantically closer to the representative of the most frequent cluster. For example, given the question ``When did Brexit happen?'', if the most confident answer is $\tilde{c_1} = \text{``2020’’}$ and other unique responses include ``2016'' and ``2019'', we would prefer to include ``2019'' first due to its temporal proximity, even if ``2016'' appears more frequently.  Notice that while conformal prediction for probabilistic classification typically constructs the predictive set incrementally by adding classes in the reverse order of their predicted membership probabilities, we find that this approach results in a predictive set that is difficult to distinguish from the remaining answers, making the task of rewriting a less specific claim unnecessarily challenging.

We observe that embedding-based similarity metrics often fail to accurately capture spatial, temporal, or numerical distances. To address this, we propose an LLM-based incremental selection scheme, in which an LLM is repeatedly prompted to select the cluster names most similar to those already included. While ideally, we would extend the predictive set by one cluster at a time, doing so for large $N$ would be computationally prohibitive. Instead, we begin with a predefined set of target thresholds ${\lambda_1, \lambda_2, \dots, \lambda_t}$ and, given a subset $\tilde{C_n}$ already selected, we prompt the LLM to select an additional $d$ items, where
\begin{equation*}
\begin{aligned}
    &d = \min_{\lambda \in \mathrm{\Lambda}} \Big\lceil\frac{\big(\lambda - \text{Mult}(\tilde{C}_n)\big)|C\setminus\tilde{C}_n|}{\text{Mult}(C\setminus\tilde{C}_n)}\Big\rceil,\\
    &\quad \text{s.t.}\ \ \lambda - \text{Mult}(\tilde{C}_n) > 0,
\end{aligned}
\end{equation*}
where $\text{Mult}(C) = \sum_{c \in C}m(c)$. This is to take the minimum expected number of additional clusters to include to achieve the next target threshold.

\paragraph{Belief Probing}
The final step in the claim rewriting process involves associating a more general claim with each nested cluster set $\tilde{C_n}$ using a rewriting function $f: 2^\mathcal{Y} \rightarrow \mathcal{Y}$. To accomplish this, we verbalize both $\tilde{C}_n$ and its complement $\Omega \setminus \tilde{C}_n$ as the beliefs of a hypothesized question-answering agent. We then prompt the LLM to summarize this belief in a less specific claim $b_n$ that arims to satisfy \cref{eq:characterization}.

We find that explicitly framing these clusters as the belief of an error-prone agent—rather than as objective facts—is crucial (See \autoref{table:error-prone-agent-belief-formulation} in \autoref{appendix:prompt-template}). Without this outer structure, the LLM generating the claim often disregards clusters that contain non-factual responses, making faithful belief generation challenging. This belief summarization process helps the model adhere to the coherence theory of truth \citep{wanner-etal-2024-closer}. The theoretical validity of this approach is established in the following theorem.

\begin{theorem}
    \label{theorem:correspondence}
    For claims $b$ that describe $V_b$ and $c$ that describes $V_c$, if $b \rightarrow c$ then
    \begin{align*}
        V_b \subseteq V_c.
    \end{align*}
\end{theorem}

The proof follows directly from the argument presented in \cref{sec:nested-property}. While our approach performs well in practice, the unconstrained nature of the prompting mechanism means that it does not inherently guarantee $f(\tilde{C}_j) \rightarrow f(\tilde{C}_i)$ for all $j > i$. This limitation further justifies our choice to calibrate using the Learn-Then-Test (LTT) framework \citep{angelopoulos2021learn} rather than a simpler method like Conformal Risk Control \citep{angelopoulos2023conformal}.

\subsection{Conformalizing Rewriting with LTT}
\label{subsec:ltt}

After generating a sequence of progressively less precise claims starting from the most frequent answer cluster for each question, we apply Learn-Then-Test (LTT) \citep{angelopoulos2021learn} to linguistically calibrate the response, ensuring it aligns with the optimal specificity level while maintaining the desired factuality guarantee.

\paragraph{Risk Score}
In theory, the expectation of any loss function $l$, where the risk is defined as \( R(F_{\lambda} \circ \mathcal{L}) \coloneqq \mathbb{E}[l(F \circ \mathcal{L}(x), Y)] \), satisfies the requirements for LTT. However, for a controlled comparison, we specifically focus on factuality metrics that do not penalize generality. Many automatic evaluation metrics are overly rigid, as they fail to account for semantic equivalence (e.g., exact match) or reject claims that differ in specificity from the gold target \citep{min-etal-2023-factscore, wei2024measuring}. In LLM-based evaluations, this issue can often be mitigated by slight modifications to the evaluation prompt, as detailed in \cref{sec:prompt-templates}. to yield to following loss function:
\begin{equation*}
    l(\hat{y}, y) = \begin{cases}
        1,\quad \text{if}\ \ \hat{y}\ \text{is admissable}\\
        0,\quad \text{otherwise}
    \end{cases},
\end{equation*}
In this setting, we naturally use the multiplicity threshold as the hyperparameter $\lambda$. Specifically
\begin{align*}
    \mathcal{T}_{\lambda}(\tilde{c}_1) \coloneqq &f(\tilde{C}_{n'}),\\
    \quad \text{where}\ n' = \argmin_{n} &\ \text{Multi}(\tilde{C}_{n})\quad \text{s.t.}\ \text{Multi}(\tilde{C}_{n}) \geq \lambda.
\end{align*}
Notice that due to the discrete natural of $M$, on the same question $q$, different $\lambda$ might lead to the model generalize the original claim to the same vaguer claim. Then given $\alpha$ the goal is to find valid $\lambda$ such that
\begin{equation*}
\mathbb{P}\Big(R(\mathcal{T}_{\lambda}\circ L) \leq \epsilon\Big) \geq 1 - \alpha,
\end{equation*}
with regard to risk-tolerance $\delta$ and error level $\alpha$. This is called by \citet{angelopoulos2021learn} as an $(\alpha, \delta)$-risk-controlling-prediction (RCP). Following \citet{bates2021distribution}, we calculate Hoeffding-Bentkus inequality $p$-values.

% \zhengping{TODO: Add a brief recall description of what's the dataset ends up like (probably as a tuple)}
To summarize, for a given claim $C$ and a designated risk control level $\alpha$, our method finds $\lambda_a$ and corresponding $\mathcal{T}_{\lambda}(c)$ to form a four element tuple $\big(c, \alpha, \lambda_\alpha, \mathcal{T}_{\lambda}(c)\big)$. %\zhengping{Need to also summarize the basic stats of the dataset.}

\section{Experiments and Results}
To empirically validate our general formulation and the effectiveness of our pipeline, we conduct three sets of claim rewriting experiments. \cref{subsec:simpleqa} intend to show that by answering challenging factual queries less specifically, the model is able to be evidently more factual in its responses, and our conformal factual guarantee appears valid and tight. \cref{subsec:nq} shows that our imprecise claim rewriting remains effective even if the model is relatively accurate, provided that there are reasonable performance gap between over-sampled answer set and top-confidence predictions. Finally, in \cref{subsec:claim-rewriting-factscore} we use our created datasets to train a uncertainty-aware claim-rewriter that modifies a claim to a less specific but more factual level, resulting in better factual precision of the overall generation.

\subsection{SimpleQA}
\label{subsec:simpleqa}
SimpleQA \citep{wei2024measuring} is a recent benchmark designed to evaluate language models’ ability to answer short, fact-seeking questions. Despite its simple format, the dataset presents a significant challenge, as the answers are adversarially curated against GPT-4. \citet{wei2024measuring} demonstrate that even state-of-the-art models like GPT-4o perform poorly on this benchmark, achieving only 38.2\% accuracy while exhibiting overconfident predictions. This highlights the necessity of effective and natural post-hoc linguistic calibration, particularly for smaller local models.

For our experiments, we use Llama3-8B-Instruct 
\citep{dubey2024llama} as our base model $\mathcal{L}$, generating $K=100$ responses per question in SimpleQA. We then produce progressively less precise claims, as described in \cref{subsec:claim-backoff}, using GPT-4o while targeting multiplicities $\tau \in \{20, 30, 40, 50, 60, 70, 80, 90, 100\}$. Notably, this process can still result in a finer-grained set of cluster levels than the predefined target multiplicities. Consequently, during the LTT stage, the controlling multiplicity threshold parameter $\lambda$ is selected from any integer within $(0, 100]$.

As described in \cref{subsec:ltt}, we modify the evaluation prompt from SimpleQA \citep{wei2024measuring} to ensure that it does not unfairly penalize responses that are less specific than the gold answers, as detailed in \cref{sec:prompt-templates}.

As SimpleQA has annotation on answer types (e.g., ``Person'', ``Number'', ``Date''), we randomly split the SimpleQA dataset into a \textit{calibration} set and a \textit{test} set with equal size and same answer type distributions. Then we use LTT-based calibration method as described in \cref{subsec:ltt} to derive qualifying $\lambda$ that $\mathcal{T}_{\lambda}\circ\mathcal{L}$ is an $(\epsilon, \alpha)$-RCP for each $\epsilon$ ranging from $0.55$ to $0.95$ with a $0.005$ interval. Finally, to test whether our conformal linguistic calibration provides effective guarantees, we calculate the accuracy at corresponding $\lambda$ on \textit{test}. Our main result is in the left panel of \cref{all-results}.

First of all, we observe the general trend that as the rewritten claim covers more and more possible responses from the model, they achieve higher and higher factual accuracy on average. This possible correlation confirms our intuition of the implicit \textit{factuality}-\textit{specificity} trade-off in the generation of LLM. Secondly, the \textbf{test} factual accuracy of the rewritng lies within the region guaranteed probabilistically provided by the LTT process. In particular, we note that the \textbf{test} factual accuracy is close to the lower limit, which corresponds to the maximum risk tolerance, showing that our approach finds a relatively tight imprecise rewriting configuration.

\subsection{Natural Questions}
\label{subsec:nq}

As SimpleQA is a challenging dataset and the noticeable improvement can potentially come from relatively poor initial performance, we further test our approach on Natural Questions (NQ) \citep{kwiatkowski-etal-2019-natural}, which is a widely studied QA dataset with short or long answers paired with real search queries issued to the Google search engine.

To better align with the SimpleQA setting, we focus on evaluating on a subset of NQ \textit{dev} that admits short answers that are not yes or no. In total, we have 4289 questions. As NQ can have more than one human annotation for the correct short answer, we consider an LLM response correct if it is evaluated as consistent by the same evaluator in \cref{subsec:simpleqa} against any one of the human-extracted correct answers. We also randomly split the data evenly into a calibration set and a test set for the evaluation.

\begin{figure*}[htbp]
    \begin{minipage}{0.33\textwidth}
    \centering
    \includegraphics[width=\linewidth]{media/ltt-on-simpleqa.pdf}
    % \caption{\zhengping{Notice that there's a typo} The prediction risk on the \textbf{test} set falls within the confromal guaranteed region and remains tight to their respective lower accuracy thresholds.}
    \label{figure:main-simpleqa}
    \end{minipage}
    \begin{minipage}{0.33\textwidth}
    \centering
    \includegraphics[width=\linewidth]{media/nq_ltt_results.pdf}
    % \caption{LTT result on NQ, while the overall performance is better, answering in more general terms still result in better factuality.}
    \label{figure:main-nq}
    \end{minipage}
    \begin{minipage}{0.33\textwidth}
    \centering
    \includegraphics[width=\linewidth, trim=0 0 0 -10]{media/back-off-at-different-level.pdf}
    % \caption{Linear regression for FActScore from different rewriters. Dashed line indicating the raw FActScore without any rewriting. In contrast to \textbf{base}, an LLM tuned with our data (\textbf{SFT}, \textbf{DPO}, \textbf{ORPO}) demonstrate uncertainty aware imprecise rewriting behavior while improving FActScore.}
    \label{fig:factscore-rewriting}
    \end{minipage}
    \caption{\textbf{Left}: On SimpleQA, the prediction risk on the \text{test} set falls within the conformal guarantee region and remains closely aligned with the respective lower accuracy thresholds.\protect\footnotemark~We sample answers to the same question, “Pepi Litman was born in …”, at different multiplicity thresholds: (\colorbox{yellow}{\phantom{x}} “Kiev / Kyiv.” \ding{55}), (\colorbox{orange!40}{\phantom{x}} “a major cultural and historical center in central or eastern Ukraine.” \ding{55}), (\colorbox{mygoodgreen}{\phantom{x}} “a major cultural and historical center of Ukraine.” \ding{51}). This demonstrates how our rewriting process effectively trades factual specificity for controlled imprecision. \textbf{Middle}: On datasets where the model already achieves non-trivial performance, such as Natural Questions, our guarantee remains valid, and the model still benefits significantly from adjusting precision levels. Notably, the trade-off point determined by the naive prompting approach (Naive Acc) does not align with the factuality requirement, highlighting the need for a more structured rewriting process. \textbf{Right}: Fine-tuning an LLM on the imprecise claim rewriting data enables adaptive rewriting. While instruction-tuned models exhibit better alignment with designated confidence levels, all models trained on our claim rewriting data adhere to confidence specifications more effectively than an out-of-the-box model.}
    \footnotetext{Notice that here factual accuracy corresponds to $1 - \alpha$.}
    \label{all-results}
\end{figure*}

As in the case of SimpleQA, we observe a gradual increase in accuracy as the original claim is rewritten into progressively more general responses. Moreover, the test accuracy consistently falls within the guaranteed accuracy region determined by the calibration set, demonstrating that our method effectively balances specificity and accuracy by leveraging the inherent uncertainty in the LLM’s responses.

Notably, this factuality-factuality trade-off is not inherently elicitable from the LLM itself. To demonstrate this, we prompt the same out-of-the-box LLM $\mathcal{L}$ to directly rewrite the claim to be less specific but more likely correct up to a given extent. As shown by the dots in the central panel of \cref{all-results}, while the model is capable of making a claim less specific, its generalization lacks diversity (e.g., always replace named entity with ``someone'' or ``somehting'') and fails to account for uncertainty effectively. These result seems to suggest when a lower confidence level is required, the model misses opportunities to provide more specific responses; conversely, when a higher confidence level is targeted, it struggles to generalize sufficiently to meet the requirement.


\subsection{Claim Rewriting on FActScore}
\label{subsec:claim-rewriting-factscore}

In this section, we demonstrate that conformal linguistic calibration generalizes to other domains through instruction tuning using our synthetic claim rewriting data. To this end, we fine-tune Llama-3-8B-Instruct \citep{dubey2024llama} on the synthetic data generated in \cref{subsec:simpleqa} and \cref{subsec:nq}, which consists of 2,042 instances from SimpleQA and 1,728 instances from NQ with various level of back-off generation. We evaluate its performance in rewriting atomic claims decomposed from biography generation.

Specifically, we assess changes in FActScore \citep{min-etal-2023-factscore} before and after applying the claim rewriter $\mathcal{T}$ to all decomposed atomic sub-claims. For evaluation, we use the \textit{dev} split of the Core \citep{jiang2024core} expansion of the FActScore dataset, which consists of an extended list of biography topics beyond those in the original FActScore dataset and includes frequency values associated with each topic.

Given the series of weaknesses identified in the standard FActScore configuration\footnote{Including susceptibility to trivial claims, inability to verify claims involving secondary entities \citep{wei2024long, song-etal-2024-veriscore, zhao2024wildhallucinations}.}, we restrict verification to Core-filtered \citep{jiang2024core} subclaims. These subclaims are assessed against BM25 \citep{robertson1995okapi}-retrieved text chunks from a full Wikipedia dump.

We compare the following four claim rewriters:

\textbf{$\mathcal{T}_\text{Base}$}: This rewriter directly prompts the off-the-shelf Llama 3 8B model with the instruction:
``Rewrite the following claim to be less specific until you {{\fontfamily{qcr}\selectfont verbalization}} it is true: {{\fontfamily{qcr}\selectfont claim}}.''

\textbf{$\mathcal{T}_\text{SFT}$}: This model is fine-tuned using the same prompt on triplets of the form ({\fontfamily{qcr}\selectfont verbalization}, {\fontfamily{qcr}\selectfont claim}, {\fontfamily{qcr}\selectfont rewrite-at-$\lambda$}) derived from the SimpleQA and Natural Questions rewriting data. The threshold $\lambda$ for each verbalized uncertainty level is determined based on an analysis of the conformal guarantee presented in \cref{all-results}.

\textbf{$\mathcal{T}_\text{DPO}$}: Following best practices for Direct Preference Optimization (DPO) \citep{rafailov2024direct, rafailov2024r, razin2024unintentional}, we further fine-tune $\mathcal{T}_\text{SFT}$ on preference data. Specifically, we always prefer rewrite-at-$\lambda$ over alternative rewritings that exhibit noticeable deviations.

\textbf{$\mathcal{T}_\text{ORPO}$}: Odds Ratio Preference Optimization (ORPO) \citep{hong2024orpo} offers an alternative alignment approach by applying a minor penalty to disfavored generation styles at the SFT stage. We train this model using the same dataset as $\mathcal{T}_\text{DPO}$.

The FActScore of the rewritten generations is presented in the right panel of \cref{all-results}. The transition from ``guess'' through ``somewhat confident'' and ``confident'' to ``certain'' indicates the gradual raising of the required confidence level.\footnote{Please refer to \autoref{tab::rewriting-claims} in \autoref{appendix:prompt-template} to see the exeact verbalization of each confidence level.} By generalizing individual claims, all of our claim rewriters effectively enhance the FActScore of the generated outputs. However, the out-of-the-box Llama model ($\mathcal{T}_{\text{Base}}$) does not consistently follow the instruction to rewrite claims to the designated uncertainty level.

In contrast, models fine-tuned on our conformal linguistic calibration data exhibit greater uncertainty awareness. They generate more specific rewritings when targeting lower factuality levels and produce more factually accurate rewritings when higher factuality is required. Notably, no data from the Core/FActScore biography dataset were used to train the rewriters, indicating that this uncertainty-aware imprecise rewriting behavior generalizes, to some extent, across domains.

Furthermore, our method differs from related approaches of \citet{mohri2024language} and \citet{cherian2024large}, with the key advantage that it preserves information from all subclaims. This is because our rewriting operates at the sub-claim level, ensuring that relevant details are retained while adjusting specificity.

\begin{table}[htbp]
\centering
\begin{tabular}{p{.47\textwidth}}
    \toprule
    \textbf{Claim} \\
    \midrule
    \textit{The united states was founded on July 4th, 1776.} \\
    \cellcolor{guess!20}\textbf{Guess} $\leadsto$ The united states was founded on July 4th, 1776. \\
    \textbf{S. Conf. $\leadsto$} The united states was founded on July 4th, 1776. \\
    \cellcolor{confident!20}\textbf{Conf. $\leadsto$} The united states was founded on July 4th, 1776. \\
    \midrule
    \textit{Miguel Díaz was born on May 8, 1994.} \\
    \cellcolor{guess!20}\textbf{Guess $\leadsto$} Miguel Díaz was born on May 8, 1994. \\
    \textbf{S. Conf. $\leadsto$} Miguel Díaz was born in the year 1994. \\
    \cellcolor{confident!20}\textbf{Conf. $\leadsto$} Miguel Díaz was born on a day that is not a typical or common birthdate. \\
    \bottomrule
\end{tabular}
\caption{\textit{Output} from our trained claim rewriter at different confidence levels (as shown in \autoref{tab::rewriting-claims} in the appendix). Claim generalization dynamically adapts to both the complexity of the claim and the targeted confidence level.}
\label{tab:qualitative-rewriting}
\end{table}
To gain deeper insights into the adaptive rewriting behavior of the rewriter, we manually examined claim rewritings at different uncertainty levels. \autoref{tab:qualitative-rewriting} presents representative examples of this adaptive property.\footnote{More examples can be found in \autoref{appendix:more-examples}.} For straightforward claims, such as the founding date of the United States, the LLM preserves the original claim even when a high factuality target is set. However, for more complex claims, the model adjusts the level of imprecision in its rewriting based on the designated factuality target, demonstrating its ability to balance specificity and factuality accordingly.

\section{Related Work}

\paragraph{LLM Factuality Evaluation} With the rise of large language models (LLMs) \citep{brown2020language, ouyang2022training}, long-form text generation has become widespread, but LLMs frequently hallucinate, generating content misaligned with real-world facts or user inputs \citep{maynez2020faithfulness, huang2023survey, hong2024hallucinations}.

To improve factuality evaluation, pipelines have been developed to decompose text into smaller claims and verify them against external sources \citep{min-etal-2023-factscore, wang-etal-2024-factcheck, wanner-etal-2024-closer}. These methods focus on factual precision—the proportion of verifiably true claims—while refinements enhance claim extraction \citep{wanner2024dndscore, gunjal-durrett-2024-molecular, jiang2024core}. However, they often overlook factors like hedging \citep{lee2024llm} and the role of controlled imprecision in overall factuality.
In this work, we propose a pipeline that makes this trade-off explicit, advocating for a more detailed study of the interplay between factuality and other key quality dimensions in open-ended generation evaluation.


\paragraph{Conformal Prediction for Language Generation} Applying conformal prediction to free-form generation is challenging due to the absence of a finite answer set, making classic split conformal methods inapplicable. Instead, prior work has focused on structured decision-making tasks such as overgeneration \citep{quachconformal}, abstention \citep{kamath2020selective, yadkori2024mitigating, gui2024conformal, piche2024llms}, delegation \citep{fang2024learning}, and clarification \citep{renrobots} to mitigate the risk of incorrect outputs. The most relevant approach, \citet{mohri2024language}, enhances factuality by decomposing text and omitting uncertain claims, a form of abstention. This method was later refined with adaptive conformal prediction by \citet{cherian2024large}, which improves the calibration of uncertainty while maintaining informative responses. Unlike these methods, we perform post-hoc calibration while preserving the original generation space, maintaining interpretability and coherence for downstream applications.

\paragraph{Linguistic Calibration} An alternative approach to mitigating large language model (LLM) overconfidence is linguistic calibration, where the model explicitly expresses uncertainty to better align its outputs with factual accuracy \citep{mielek2022reducing}. This concept has been further extended to estimating the probabilities associated with uncertainty quantifiers \citep{wang2024calibrating}, allowing models to more precisely convey their confidence levels. Additionally, recent research has explored integrating verbalized uncertainty with other quantitative calibration techniques \citep{band2024linguistic, zhao2021calibrating}, demonstrating that a hybrid approach can enhance decision-making by providing a more nuanced representation of uncertainty. These advancements highlight the growing potential of linguistic calibration as a key tool in improving the reliability and interpretability of LLM-generated responses.

While prior work primarily adds uncertainty markers while preserving the original text, our approach modifies generation itself to introduce imprecision, offering a distinct way to convey uncertainty. This not only enhances interpretability but also provides a structured mechanism to balance specificity and reliability in language generation.

\section{Conclusion}

We propose a novel linguistic calibration paradigm that enables models to express uncertainty through imprecise language. Unlike prior approaches that discard uncertain claims or merely express uncertainty by directly stating confidence level, our method rewrites them into more general or less specific beliefs. Our experiments show that this approach establishes a clear trade-off between factuality and specificity on SimpleQA and Natural Questions, allowing models to improve factuality by adopting a more cautious response strategy.

We further demonstrate that conformal linguistic calibration can be fine-tuned into a base model, enabling it to adaptively adjust claim specificity based on confidence levels. We believe this work will also encourage a more nuanced evaluation of automatic factuality pipelines, emphasizing the need to account for factors beyond factual accuracy.

%\section*{Impact Statement}

%Our work introduces a promising approach to making the trade-off between imprecision and factuality more explicit, which could have a meaningful impact across various LLM applications. By proposing a new way to express models’ uncertainty, we open up potential opportunities for more effective human-LLM interaction. This may contribute to increased transparency and accountability in critical domains such as law, journalism, and science, where understanding the reasoning behind uncertainty is valuable. Additionally, our approach has the potential to inspire further advancements in automatic fact-checking systems, encouraging a more comprehensive evaluation of factuality.

% By default, the box containing the title and author names is set to the minimum of 5 cm. If you need more space, include the following in the preamble:
% \begin{quote}
% \begin{verbatim}
% \setlength\titlebox{<dim>}
% \end{verbatim}
% \end{quote}
% where \verb|<dim>| is replaced with a length. Do not set this length smaller than 5 cm.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\section*{Acknowledgements}
AL and BVD are partially supported by ONR grant N00014-241-2089.

\bibliography{custom}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Prompt Templates}
\label{appendix:prompt-template}

\begin{table}[h]
\centering
\begin{tabular}{rl}
    \toprule
    \textbf{Field} & \textbf{Content} \\
    \midrule
        \textbf{Examples} & \makecell[lt]{\qcrtext{[User]} Given a list of answers to a question, identify semantically distinct answers.\\ \\ **List of Answers**:\\\qcrtext{\$\{Answers\}}\\\qcrtext{[Agent]} **Semantically Distinct Answers**:\qcrtext{\$\{SD\_Answers\}}}\\
        \textbf{Input} & \makecell[lt]{\qcrtext{[User]} Given a list of answers to a question, identify semantically distinct answers.\\ \\ **List of Answers**:\\\qcrtext{\$\{Answers\}}}\\
    \bottomrule
\end{tabular}
    \caption{Prompt for identifying unique answer to a given question.}
\end{table}

\begin{longtable}{rp{0.8\linewidth}}
    \toprule
    \textbf{Field} & \textbf{Content} \\
    \midrule
    \textbf{Instruction} & Given a list of already **selected items**, your task is to find **K additional items** from the list of **candidates** that are **most similar** to the items already selected. The similarity can be based on **specific attributes, characteristics, or metrics** relevant to the nature of the items, such as style, influence, or shared features.\newline\newline
    You should return a list of **K items** from the **candidates** list and store them in a variable named **increments**.\newline\newline
    ---\newline\newline
    \textbf{Inputs:}\newline
    1. \textbf{K}: The number of additional items to add to the selected list.\newline
    2. \textbf{selected}: A list of already selected items (could be names, objects, etc.).\newline
    3. \textbf{candidates}: A list of candidate items from which we need to select the K most similar items.\newline\newline
    \textbf{For Example:}\newline\newline
    \qcrtext{\`{}\`{}\`{}python\newline
    K = 1\newline selected = ["Red"]\newline candidates = ["Yellow", "Black", "White"]\newline
    \`{}\`{}\`{}}\newline\newline
    ---\newline\newline
    \textbf{Expected Output:} Return a list of **K items** from the **candidates** list that are **most similar** to the items in the **selected** list. \newline\newline

    \qcrtext{
    \`{}\`{}\`{}python\newline
    increments = ["Yellow"]\newline
    \`{}\`{}\`{}
    }
    \\
    \textbf{Examples} & \qcrtext{\`{}\`{}\`{}python\newline K = 1\newline selected = ["William Butler Yeats"]\newline candidates = ["Agatha Christie", "Benjamin Franklin", "Napoléon Bonaparte"]\newline\`{}\`{}\`{}\newline\newline}
    \qcrtext{[Agent]} Based on your inputs, we are tasked with selecting 1 item from the list of candidates that is most similar to the selected item, **"William Butler Yeats"**. The similarity could be based on characteristics like occupation, influence, or style.\newline\newline
    - **William Butler Yeats** was an Irish poet and one of the foremost figures of 20th-century literature.\newline\newline
    - **Candidates**:\newline
    - **Benjamin Franklin**: American polymath, writer, scientist, diplomat.\newline
    - **Napoleon**: French military leader and emperor.\newline
    - **Agatha Christie**: British writer known for her detective novels.\newline\newline
    In this case, based on occupation (both are writers), **Agatha Christie** is most similar to **William Butler Yeats**.\newline\newline
    Thus, the result would be: \newline\newline
    \qcrtext{
    \`{}\`{}\`{}python\newline
    increments = ["Agatha Christie"]\newline\`{}\`{}\`{}
    }
    \\
    \textbf{Input} & \qcrtext{\`{}\`{}\`{}python\newline K = \${K}\newline selected = \${SELECTED}\newline candidates = \${CANDIDATES}\newline\`{}\`{}\`{}} \\
    \bottomrule
    \caption{Prompt using iterative clustering fornested set construction.}
\end{longtable}

\begin{longtable}{rp{.8\linewidth}}
    \toprule
    \textbf{Field} & \textbf{Content} \\
    \midrule
    \textbf{Instruction} & \qcrtext{[User]} Suppose a human subject is going to respond to a question but they don't know the exact answer. However, they explicitly state their belief that some answers could be correct and some are definitely wrong. Given their underlying belief reflected please provide a vaguer claim that best summarizes the respondent's belief by focusing on the characteristics that the could-be-correct answers share while the definitely-wrong answers don't. The answer will be used to evaluate the human subject's response, so please be faithful to the discussions provided and don't correct them even if they are wrong. Importantly, please make sure that all answers in the candidate group is covered by the vague answer claim suggested.\newline\newline Questions and beliefs are provided below. Please respond after reasoning about the similarities and differences between the positive and negative answers. Wrap your answer in a code block (\`{}\`{}\`{}answer\`{}\`{}\`{}). The answer should start with ``The respondent believes that''.\\
    \textbf{Examples} & \qcrtext{[User]} **Question:** Who's the best football player of all time?\newline**Belief:** The respondent believes that the answer is either Pele, Diego Maradona, or Lionel Messi, but not Cristiano Ronaldo, Zinedine Zidane, nor Johan Cruyff.\newline
    \qcrtext{[Agent]} **Positive Group: Pelé, Diego Maradona, Lionel Messi**\newline *Time Period*: Spanning from the 1950s to the present.\newline *Geographic Origin*: South America (Brazil and Argentina).\newline *Influence*: Known for their extraordinary dribbling skills, creativity, and playmaking abilities.\newline *Philosophy*: These players are celebrated for their flair, technical prowess, and ability to change the course of a game single-handedly. Pelé (Brazil) is renowned for his goal-scoring and three World Cup victories. Maradona (Argentina) is famous for his ``Hand of God'' goal and his incredible dribbling, particularly in the 1986 World Cup. Messi (Argentina) is known for his consistency, vision, and record-breaking achievements with FC Barcelona and Argentina.\newline *Distinct Features*: Emphasis on individual brilliance, creativity, and a deep connection with their home countries' footballing culture.\newline\newline **Negative Group: Cristiano Ronaldo, Zinedine Zidane, Johan Cruyff**\newline *Time Period*: Spanning from the 1970s to the present.\newline *Geographic Origin*: Europe (Portugal, France, Netherlands).\newline *Influence*: Known for their versatility, tactical intelligence, and leadership on the field.\newline *Philosophy*: These players are celebrated for their physical prowess, strategic thinking, and ability to perform in crucial moments. Ronaldo (Portugal) is known for his athleticism, goal-scoring, and adaptability across leagues. Zidane (France) is celebrated for his elegance, control, and pivotal role in France's 1998 World Cup win. Cruyff (Netherlands) is a pioneer of ``Total Football,'' influencing modern football tactics and philosophy. *Distinct Features*: Emphasis on tactical intelligence, versatility, and significant contributions to both club and national team success.\newline **Key Differentiation**: The positive group is characterized by South American flair, individual brilliance, and a deep cultural impact on football. The negative group is defined by European tactical intelligence, versatility, and a strategic approach to the game.\newline\newline

    \qcrtext{\`{}\`{}\`{}\newline The respondent believes that the best football player of all time comes from South America. \newline\`{}\`{}\`{}}
    \\
    \textbf{Input} & \qcrtext{[User]} **Question**: \qcrtext{\$\{question\}} \newline\newline **Belief**: \qcrtext{\$\{belief\}}\\
    \bottomrule 

    \caption{Prompt for generating belief that describes a belief.}\label{table:error-prone-agent-belief-formulation}
\end{longtable}

\begin{longtable}{rp{0.8\linewidth}}
    \toprule
    \textbf{Field} & \textbf{Content} \\
    \midrule
    \textbf{Instruction} & \qcrtext{[User]} Your job is to look at a question, a gold target, and a predicted answer, and then assign a grade of either [``CONSISTENT'', ``INCONSISTENT'', ``NOT\_ATTEMPTED''].\newline First, I will give examples of each grade, and then you will grade a new example.\\
    \textbf{Examples} & The following are examples of CONSISTENT predicted answers.\newline\`{}\`{}\`{}\newline Question: What are the names of Barack Obama's children?\newline Gold target: Malia Obama and Sasha Obama\newline Predicted answer 1: sasha and malia obama\newline Predicted answer 2: Each of them has a girl's name. \newline Predicted answer 3: They have the same last name as their father, Obama.\newline\`{}\`{}\`{}\newline These predicted answers are all CONSISTENT because:\newline- They do not contain any information that contradicts the gold target.\newline- Only semantic meaning matters; capitalization, punctuation, grammar, and order don't matter.\newline- Hedging and guessing are permissible, provided that the response is a consistent generalization of the gold answer.\newline\newline The following are examples of INCONSISTENT predicted answers.\newline\`{}\`{}\`{}\newline Question: What are the names of Barack Obama's children?\newline Gold target: Malia and Sasha\newline Predicted answer 1: Malia. \newline Predicted answer 2: Barack Obama does not have any children. \newline Predicted answer 3: It's possible you may mean Betsy and Olivia. However, you should clarify further details with updated references if necessary. Is that the correct answer? \newline Predicted answer 4: It may be the case that Obama's child is named James. However, it's recommended to confirm the most accurate and updated information since this could change over time. This model may not always reflect the most current information.\newline\`{}\`{}\`{}\newline These predicted answers are all INCONSISTENT because:\newline- An inconsistent statement in the answer that contradicts the gold target. Inconsistent statements that have some hedging (e.g., "it is possible that", "although i'm not sure, i think") are also considered inconsistent.\newline- Notice that a hedging answer is inconsistent only if it contradicts the gold target.\newline\newline The following are examples of NOT\_ATTEMPTED predicted answers.\newline\`{}\`{}\`{}\newline Question: What are the names of Barack Obama's children?\newline Gold target: Malia and Sasha\newline Predicted answer 1: I don't know. \newline Predicted answer 2: I need more context about which Obama you are talking about.\newline\`{}\`{}\`{}\newline These predicted answers are all NOT\_ATTEMPTED because: \newline - They explicitly state that the model does not know the answer or needs more information to answer the question.\newline\newline Also note the following things:\newline - Do not punish predicted answers if they omit information that would be clearly inferred from the question.\newline- For example, consider the question ``What city is OpenAI headquartered in?" and the gold target "San Francisco, California''. The predicted answer "San Francisco" would be considered consistent, even though it does not include ``California''.\newline- Consider the question "What award did A pretrainer's guide to training data: Measuring the effects of data age, domain coverage, quality, \& toxicity win at NAACL '24?'', the gold target is ``Outstanding Paper Award''. The predicted answer ``Outstanding Paper'' would be considered consistent, because ``award'' is presumed in the question.\\
    \textbf{Examples} & - For the question ``What is the height of Jason Wei in meters?'', the gold target is ``1.73 m''. The predicted answer ``1.73'' would be considered consistent, because meters is specified in the question.\newline - For the question ``What is the name of Barack Obama's wife?'', the gold target is ``Michelle Obama''. The predicted answer ``Michelle'' would be considered consistent, because the last name can be presumed.\newline - Do not punish for typos in people's name if it's clearly the same name. \newline - For example, if the gold target is "Hyung Won Chung", you can consider the following predicted answers as consistent: "Hyoong Won Choong", "Hyungwon Chung", or "Hyun Won Chung".\newline - A response being too general is not a reason to mark it as INCONSISTENT.\newline - For example, if the gold target is "Lion", you can consider the following predicted answers as consistent: ``Living thing'', "Something".\newline - For the question ``Who wrote the book 'The Great Gatsby'?'', the gold target is ``F. Scott Fitzgerald''. The predicted answer ``Some 20th century authors'', ``Somebody wrote it'' would be considered consistent, because it is a generalization of the gold target.\newline - When the response could be correct under certain interpretation, you should grade it as consistent, in favor of the model.\newline - If the answer asks for multiple pieces of information, you should only grade whether the answered information is correct.\newline - For example, if the question asks for two names containing ``Malia'', you should grade ``Malia'' as CONSISTENT.\newline - If the question asks for a month and a year and the gold target year is 2004, you should grade ``2004'' as CONSISTENT.\newline - If the response specifies multiple possible answers, you should grade it as inconsistently only if none of the possible answers are correct.\newline- For example, if the gold target is "2004", you should grade both ``2004, 2005'' and "during late 1990s to early 2000s" as CONSISTENT, because the correct answer is included in the response.\\
    \textbf{Input} & \qcrtext{[User]} \qcrtext{\`{}\`{}\`{}}\newline Question: \qcrtext{\$\{question\}}\newline Gold target: \qcrtext{\$\{target\}}\newline Predicted answer: \qcrtext{\$\{predicted\_answer\}}\newline\qcrtext{\`{}\`{}\`{}}\\
    \bottomrule

    \caption{Prompt for evaluating factuality when imprecision occurs on SimpleQA \citep{wei2024long}.}
\end{longtable}

\begin{table}[h]
\begin{center}
\begin{tabular}{rp{0.8\textwidth}}
    \toprule
    \textbf{Field} & \textbf{Content} \\
    \midrule
    \textbf{Input} & {\fontfamily{qcr}\selectfont [User]} the following claims to be less specific until you {\fontfamily{qcr}\selectfont \$\{verbalization\}} is is true: {\fontfamily{qcr}\selectfont \$\{source\_claim\}}\newline \newline Your response should only contain the claim itself, without any additional context. \\
    \bottomrule
\end{tabular}
\caption{Claim rewriting template. The verbalization is associated to the designated level. For the levels qualitatively examined in the paper, \textbf{Guess} corresponds to the verbalization ``would guess'', \textbf{S. Conf.} corresponds to ``are somewhat confident'', and \textbf{Conf.} corresponds to ``are confident''.}\label{tab::rewriting-claims}
\end{center}
\end{table}

\label{sec:prompt-templates}

\section{Nested Property}
\label{sec:nested-property}

\begin{proof}[Proof of \cref{theorem:correspondence}]
    Suppose there's a possible world $v \in V_b$, but $v \not\in V_c$. Given that
        $$(b \rightarrow c) \rightarrow (\square (b \rightarrow c))$$
    Thus by \cref{definition:necessity} and \cref{definition:description} we have $v \Vdash c$. However, since $v \notin V_c$ we have $v \in W \setminus V_c$, again by \cref{definition:description} we have $v \Vdash \neg c$, which is a contradiction.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{More Examples}
\label{appendix:more-examples}
In this section of the appendix, we examine additional examples to illustrate how our trained claim rewriter adaptively modifies given claims, shifting them toward varying levels of uncertainty. We observe that the extent of rewriting differs across claims, with some undergoing more substantial transformations than others. Notably, the uncertainty levels introduced at different rewriting stages are distinct and pronounced.

\begin{table}[htbp]
\centering
\begin{tabular}{p{.9\textwidth}}
\toprule
\textbf{Claim} \\
\midrule
\textit{"Lost in White" is a horror-thriller.} \\
\cellcolor{guess!20}{
\textbf{Guess$\leadsto$}
"Lost in White" is a horror-thriller.
} \\
\cellcolor{white!20}{
\textbf{S. Conf.$\leadsto$}
"Lost in White" is a type of film that blends elements of horror and thriller genres.
} \\
\cellcolor{confident!20}{
\textbf{Conf.$\leadsto$}
"Lost in White" is a film that blends elements of thriller and horror genres.
} \\
\midrule
\textit{Carl Rackemann played 10 One-Day Internationals (ODIs).} \\
\cellcolor{guess!20}{
\textbf{Guess$\leadsto$}
Carl Rackemann played 10 One-Day Internationals (ODIs).
} \\
\cellcolor{white!20}{
\textbf{S. Conf.$\leadsto$}
the number of One-Day Internationals played by Carl Rackemann could be any positive integer from 0 to 143.
} \\
\cellcolor{confident!20}{
\textbf{Conf.$\leadsto$}
the number of One-Day Internationals played by Carl Rackemann is a positive integer.
} \\
\midrule
\textit{Kadegaon is located in the Kolhapur district.} \\
\cellcolor{guess!20}{
\textbf{Guess$\leadsto$}
Kadegaon is located in the Kolhapur district.
} \\
\cellcolor{white!20}{
\textbf{S. Conf.$\leadsto$}
Kadegaon is located in the Kolhapur district.
} \\
\cellcolor{confident!20}{
\textbf{Conf.$\leadsto$}
the village Kadegaon is located in a district of Maharashtra, India.
} \\
\midrule
\textit{Tim Whitnall was born on May 12, 1954.} \\
\cellcolor{guess!20}{
\textbf{Guess$\leadsto$}
Tim Whitnall was born on the 12th of a month in 1954.
} \\
\cellcolor{white!20}{
\textbf{S. Conf.$\leadsto$}
Tim Whitnall was born on a day that is any date within a wide range of possible dates, or they are unsure of his birthdate.
} \\
\cellcolor{confident!20}{
\textbf{Conf.$\leadsto$}
there is no available information about Tim Whitnall's birthdate, and they are uncertain or unaware of it.
} \\
% \midrule
% \textit{The film "Sabz-e Sitakhton" was released in 1998.} \\
% \cellcolor{guess!20}{
% \textbf{Guess$\leadsto$}
% the film "Sabz-e Sitakhton" was released in the late 1990s to early 2000s.
% } \\
% \cellcolor{white!20}{
% \textbf{S. Conf.$\leadsto$}
% the release year of the film "Sabz-e Sitakhton" could be any year from the 1970s to the 2020s, or it is not known.
% } \\
% \cellcolor{confident!20}{
% \textbf{Conf.$\leadsto$}
% the release year of the film "Sabz-e Sitakhton" is uncertain and could be any year from the 1970s to the 2010s, or they might not have that information.
% } \\
\midrule
\textit{Yoon Seok-ho's films explore themes of trauma.} \\
\cellcolor{guess!20}{
\textbf{Guess$\leadsto$}
Yoon Seok-ho's films explore themes of trauma.
} \\
\cellcolor{white!20}{
\textbf{S. Conf.$\leadsto$}
Yoon Seok-ho's films explore themes related to human emotional experience and social issues.
} \\
\cellcolor{confident!20}{
\textbf{Conf.$\leadsto$}
Yoon Seok-ho's films explore themes of human emotional experience and social commentary.
} \\
\midrule
\textit{Tadashi Imai died in 1999.} \\
\cellcolor{guess!20}{
\textbf{Guess$\leadsto$}
Tadashi Imai died in 1999.
} \\
\cellcolor{white!20}{
\textbf{S. Conf.$\leadsto$}
Tadashi Imai died sometime between the 1960s and 2000.
} \\
\cellcolor{confident!20}{
\textbf{Conf.$\leadsto$}
the date of Tadashi Imai's death is uncertain or not specifically known.
} \\
\midrule
\textit{Ernie Dingo gained national recognition for his role as Jacko in the television series "The Sullivans."} \\
\cellcolor{guess!20}{
\textbf{Guess$\leadsto$}
Ernie Dingo gained national recognition for his role as Jacko in the television series "The Sullivans."
} \\
\cellcolor{white!20}{
\textbf{S. Conf.$\leadsto$}
there is a possibility that Ernie Dingo gained national recognition for his role as Jacko in "The Sullivans," but they are uncertain about the specific year or character name.
} \\
\cellcolor{confident!20}{
\textbf{Conf.$\leadsto$}
the actor who gained national recognition for his role as Jacko in "The Sullivans" is an Australian entertainer from the mid to late 20th century.
} \\
\midrule
\textit{Zeki Demirkubuz's films feature non-linear narratives.} \\
\cellcolor{guess!20}{
\textbf{Guess$\leadsto$}
Zeki Demirkubuz's films feature non-linear narratives.
} \\
\cellcolor{white!20}{
\textbf{S. Conf.$\leadsto$}
Zeki Demirkubuz's films feature complex narrative structures that explore themes of identity and human experience.
} \\
\cellcolor{confident!20}{
\textbf{Conf.$\leadsto$}
Zeki Demirkubuz's films feature complex and unconventional narrative structures.
} \\
\midrule
\textit{Zia Fatehabadi used the "radif" in his ghazals.} \\
\cellcolor{guess!20}{
\textbf{Guess$\leadsto$}
Zia Fatehabadi used the "radif" in his ghazals.
} \\
\cellcolor{white!20}{
\textbf{S. Conf.$\leadsto$}
the use of "radif" in Zia Fatehabadi's ghazals is a traditional and structural element of classical Urdu poetry.
} \\
\cellcolor{confident!20}{
\textbf{Conf.$\leadsto$}
the use of "radif" in ghazals is a traditional feature of classical Urdu poetry.
} \\
\bottomrule
\end{tabular} \caption{More examples occurring when backing off claims from the \textsc{Core} dataset.}
\end{table}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
