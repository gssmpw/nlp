\section{Task}

We define the paper review generation task as follows: given a research paper, 1) summarize main points, 2) identify a list of strengths and weaknesses, and 3) predict the final acceptance of the paper. This task offers direct value to various user groups (e.g., authors who want to get initial feedback on their draft, or reviewers who want to examine diverse view points) by providing actionable feedback for improving their papers. While valuable, evaluating papers based on research standards (e.g., novelty, rigor, and clarity) is difficult for LLMs as it requires significant expertise. % Our evaluation does not consider the summary and the paper acceptance, as the acceptance decision depends on complex social factors beyond the given paper.

\section {Constructing Expert Review Dataset}
\label{sec: pipeline}

\subsection {Collecting Review Data}

We used real-world review data covering ICLR 2021-2024 from the OpenReview platform\footnote{The review data is publicly available and permits use of data for research.}, where human experts evaluated submissions for a top-tier AI conference. Using the OpenReview API\footnote{https://docs.openreview.net/getting-started/using-the-api} and the list of submissions from public GitHub repositories\footnote{https://github.com/\{evanzd/ICLR2021-OpenReviewData, fedebotu/ICLR2022-OpenReviewData, fedebotu/ICLR2023-OpenReviewData, hughplay/ICLR2024-OpenReviewData\} }, we initially collected 18,407 submissions with their review data. 

\subsection {Identifying Strengths and Weaknesses}

One of the challenges in identifying the strengths and weaknesses of these papers is that each review consists of multiple blocks, including a meta-review and individual review texts from several reviewers. To address the challenge, our approach is to use meta-review, a final review from a qualified expert that summarizes reviews and highlights important strengths and weaknesses for supporting the final decision. As the meta-review does not capture all the details, we created self-contained strengths and weaknesses by 1) extracting them from the meta-review and 2) augmenting these extracted elements with detailed comments from individual reviews (non-meta). We designed a prompting chain that consists of three prompts (Appendix~\ref{appendix:gt-prompt}). After excluding withdrawn submissions that lack meta-reviews, 14,922 submissions remained.


\begin{table*}[ht]
    \centering
    \renewcommand{\arraystretch}{1.2}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{lp{14cm}}
        \hline
        \multicolumn{2}{c}{\textbf{Target}} \\
        \hline
        \textbf{Code} & \textbf{Definition (The review addresses ...)} \\
        \hline
        \textbf{Problem} & Motivation, task definitions, and problem statements. \\
        \textbf{Prior Research} & References and contextual positioning of the submission. \\
        \textbf{Method} & Proposed approach, techniques, algorithms, or datasets. \\
        \textbf{Theory} & Theoretical foundations, assumptions, proofs, or justifications. \\
        \textbf{Experiment} & Experimental setup, results, and analysis. \\
        \textbf{Conclusion} & Findings, implications, discussions, and takeaways. \\
        \textbf{Paper} & General targets of the paper without specifying a particular target \\
        \hline
        \multicolumn{2}{c}{\textbf{Aspect}} \\
        \hline
        \textbf{Code} & \textbf{Definition (The review addresses ...)} \\
        \hline
        \textbf{Impact} & Significance or practical influence of the work. \\
        \textbf{Novelty} & Originality of the submission compared to prior research. \\
        \textbf{Clarity} & Readability, ambiguity, or communication aspects. \\
        \textbf{Validity} & Soundness, completeness, and rigor. \\
        \textbf{Not-specific} & Multiple targets without emphasis on a particular aspect. \\
        \hline
    \end{tabular}%
    }
    \captionsetup{justification=raggedright, singlelinecheck=false}
    \caption{The coding schema. To identify codes for targets (i.e., what the review praises or critiques) and aspects (i.e., the specific elements of the target being evaluated), we surveyed 9 AI paper submission guidelines (Appendix ~\ref{appendix:guidelines}) and prior research on review analysis~\citep{chakraborty2020aspect, ghosal2022peer, yuan2022can}.}
    \label{tab:coding_schema}
\end{table*}






\section {Building an Automatic Annotator Based on Target and Aspect}

The central goal of this paper is to analyze where LLMs excel and fall short in reviewing papers, compared to human experts. To achieve the goal, we 1) annotate each of the strengths and weaknesses identified by LLMs and experts and 2) examine the agreement between them based on the annotation results. The analysis offers insights into the distinct contributions and limitations of LLMs in reviewing papers, informing strategies to foster more effective human-LLM collaboration in reviewing papers.

\subsection{Developing a Coding Scheme}

Our focus in classifying strengths and weaknesses lies in two key dimensions: targets (i.e., what the review praises or critiques) and aspects (i.e., the specific elements of the target being evaluated). To build an initial codebook, we surveyed 9 AI paper submission guidelines (Appendix~\ref{appendix:guidelines}) and extracted target-aspect pairs from each statement in the guidelines (e.g., \textit{``The paper should state the full set of assumptions of all theoretical results if the paper includes theoretical results.''} yields the target \textit{Theory} and aspect \textit{Completeness}). We also reviewed related work on the analysis of paper review data~\citep{chakraborty2020aspect, ghosal2022peer, yuan2022can}. After identifying 33 targets and 13 aspects, we merged similar items to create simple and distinct categories, resulting in 7 targets and 4 aspects. Table~\ref{tab:coding_schema} shows our final coding scheme.

\subsection{Building an Automatic Classifier Based on Human Annotation}

Based on the coding scheme, we annotated targets and aspects of strengths and weaknesses to produce ground truth for developing an automatic annotator. We randomly sampled 68 papers from our review dataset, yielding 327 instances of strengths and weaknesses. Two authors annotated each instance together, resolving any conflicts. Most conflicts arose when an instance illustrated multiple points. For example, an instance such as ``\textit{**Technically sound with a strong foundation**: The paper's technical foundation is evident in its bi-level optimization framework, ... Technical novelty also arises from using supermartingale constraints on the barrier function ...}'' could correspond to both \textit{Validity} and \textit{Novelty} aspect. Two authors finalized the annotation through discussions, focusing on the main point or root cause of the issue. In the example, we annotated \textit{Validity}, as the strength mainly praises the technical soundness, as shown in the header.

We then designed prompts to automatically annotate the instances, assigning a target and aspect label to each. Specifically, we designed four prompts where each corresponds to one of the four combinations of target/aspect and strength/weakness~\ref{appendix:annotate-prompt}. Table~\ref{tab:model_scores} shows the inter-rater reliability (IRR) between author annotations and LLM annotations. Classification using \texttt{o3-mini} achieved the IRR scores of 0.85 for targets and 0.86 for aspects. Given the high IRR and its relatively low computational cost, we used \texttt{o3-mini} for the automatic annotation of both target and aspect in the main evaluation. Moreover, an examination of the confusion matrix (Appendix~\ref{appendix:annotate-confusion-matrix}) suggests that the errors tend to occur in semantically related categories, indicating that the misclassifications are not arbitrary but rather reflect subtle ambiguities inherent in the data.


\begin{table}[ht]
    \centering
    \begin{tabular}{lcc}
        \toprule
        \textbf{Model}       & \textbf{Target} & \textbf{Aspect} \\
        \midrule
        \texttt{gpt-4o-mini} & 0.75   & 0.80   \\
        \texttt{gpt-4o}      & 0.87   & 0.83   \\
        \texttt{o3-mini}     & 0.85   & 0.86   \\
        \bottomrule
    \end{tabular}
    \caption{Inter-Rater Reliability between annotations of authors and LLMs for the target and aspect.}
    \label{tab:model_scores}
\end{table}







% \section{Evaluation}

% In this section, we evaluate the ability of LLMs to identify strengths and weaknesses by comparing them with those in expert-generated reviews. We begin by collecting review data from the OpenReview [] platform, extracting strengths, weaknesses, and final judgments of papers (accept or reject). Using this data, we compute the precision of LLM-identified strengths and weaknesses, which measures the proportion of LLM-identified elements that overlap with those in expert-generated reviews. Recall (i.e., the proportion of expert-generated strengths and weaknesses identified by the LLM) is not considered, as it is challenging to define a complete set of strengths and weaknesses.

% \subsection{Review data collection}

% \subsection{Computing precision based on checklist}

% Based on the augmented meta-review, we computed precision of LLM-identified strengths and weaknesses as follows:
% }

% \[
% \text{Precision} = \frac{\sum_{l \in L} \mathbb{I}(l \in R)}{|L|}
% \]where $L$ is the set of strengths and weaknesses identified by the LLM and $R$ is the set provided in the augmented meta-review. To compute the indicator function, we generated a checklist for each augmented meta-review using gpt-4o \notyet{(See Appendix X for the prompt)}. The checklist consists of questions designed to see if a given review point is included in the meta-review \notyet{(Figure X)}. For each strength and weakness identified by the LLM, we applied the checklist and assigned the value of 1 if any of the items on the checklist were satisfied, using gpt-4o as a judge \notyet{any related work?}.

% \subsection{LLMs for evaluation}

% We evaluated X off-the-shelf LLMs: gpt-4o-mini, gpt-4o, gpt-o1-preview, Llama-70B, Llama-405B, .... To generate a review of a paper, we parsed the pdf file of the paper and put the entire text into the prompt \notyet{(See Appendix X for the prompt)}.

% \subsection{Result}

% \notyet{Table X} shows the overall results. 