\section{Evaluation}


\begin{table*}[t]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{lccc ccc ccc}
        \toprule
                   & \multicolumn{3}{c}{Overall} & \multicolumn{3}{c}{Strength} & \multicolumn{3}{c}{Weakness} \\
        \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
        Model         & F1    & Precision & Recall  & F1    & Precision & Recall  & F1    & Precision & Recall \\
        \midrule
        \texttt{DeepSeek-R1}   & \textbf{0.373} & 0.314     & 0.460   & 0.341 & 0.254     & 0.520   & \textbf{0.400} & \textbf{0.379}     & 0.424 \\
        \texttt{o1-mini}       & 0.359 & 0.283     & \textbf{0.491}   & 0.331 & 0.232     & \textbf{0.578}   & 0.385 & 0.343     & \textbf{0.439} \\
        \texttt{o1}            & 0.355 & 0.300     & 0.436   & 0.318 & 0.234     & 0.495   & 0.388 & 0.377     & 0.400 \\
        \texttt{DeepSeek-V3}   & 0.351 & 0.300     & 0.421   & 0.330 & 0.246     & 0.501   & 0.368 & 0.362     & 0.374 \\
        \texttt{Llama-405B}    & 0.350 & \textbf{0.323}     & 0.381   & \textbf{0.349} & \textbf{0.279}     & 0.465   & 0.350 & 0.371     & 0.331 \\
        \texttt{gpt-4o}        & 0.349 & 0.287     & 0.442   & 0.342 & 0.252     & 0.533   & 0.354 & 0.325     & 0.388 \\
        \texttt{gpt-4o-mini}   & 0.344 & 0.289     & 0.427   & 0.335 & 0.246     & 0.522   & 0.353 & 0.337     & 0.369 \\
        \texttt{Llama-70B}     & 0.339 & 0.302     & 0.388   & 0.338 & 0.260     & 0.481   & 0.341 & 0.350     & 0.332 \\
        \bottomrule
    \end{tabular}%
    }
    \caption{Overall performance of alignments on strengths and weaknesses between experts-identified and LLM-identified reviews. The metrics were computed by comparing the (target, aspect) set between experts' and LLMs' review. \texttt{DeepSeek-R1} achieved the best performance, \texttt{o1-mini} achieved superior recall, and \texttt{Llama-405B} achieved superior precision, compared to other models.}
    \label{tab:metrics}
\end{table*}




The goal of our evaluation is to analyze the strengths and weaknesses of a given paper as identified by LLMs, comparing them with those identified by human experts. Note that our evaluation does not consider the correctness of the identified strengths and weaknesses because our focus is comparing perspectives in reviewing papers for both groups, not the content itself.




The evaluation is based on paper-review pairs. However, we excluded \textit{accepted} submissions in the evaluation because OpenReview provides the camera-ready versions (post-review) rather than the submitted versions (pre-review), leading to a mismatch between the collected review and the camera-ready paper. Therefore, we only focused on \textit{rejected} papers, where the meta-review corresponds to the latest version of the paper. Out of 9,139 rejected papers, we randomly sampled 7.5\% of them (685 papers) for the evaluation. In total, we obtained 3,689 review items (1,241 strengths and 2,448 weaknesses), each automatically annotated with a target and aspect label.


\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figures/violin.png}
  \caption{Distribution of strengths and weaknesses. Unlike human experts, LLMs reported a consistent count regardless of paper contents. \texttt{o1-mini} identified the most, while \texttt{Llama} models identified the fewest points.}
  \label{fig:num_reviews}
\end{figure}

\subsection{Large Language Models}
\label{sec:llm}

We consider eight off-the-shelf LLMs, differing in size and availability (open-source vs. proprietary): four GPT models (gpt-4o-mini, gpt-4o, o1-mini, o3-mini, o1)\footnote{\texttt{gpt-4o-2024-08-06}, \texttt{gpt-4o-mini-2024-07-18}, \texttt{o1-mini-2024-09-12}, \texttt{o1-2024-12-17}}, two Llama models (Llama 3.1-\{70B, 405B\}), and two DeepSeek models (DeepSeek-\{V3, R1\}). We used the default parameters of the models.

\subsection{Procedure}

For each of the 685 papers, we generated review data using the prompting pipeline (Section~\ref{sec: pipeline}), extracted the strengths and weaknesses, and annotated the corresponding targets and aspects using the automatic annotator powered by \texttt{o3-mini}. Then for each LLM in Section~\ref{sec:llm}, we generated a review for each paper (See Appendix~\ref{appendix:llm-prompt} for the prompt), extracted the strengths and weaknesses, and annotated the targets and aspects using the same automatic annotator. Then we compared the annotated targets and aspects between the experts' reviews and LLMs' reviews.


% \vspace{2mm}

\subsection{Result}


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\textwidth]{radar_chart_2.png}
    \caption{Normalized distributions by target/aspect and strength/weakness for LLMs and human experts (red line). Overall, both groups showed similar perspectives in reviewing papers, focusing on technical targets (i.e., Method, Experiment, and Theory) and validity. However, LLMs showed more biased perspectives that focus on the technical validity whereas human experts exhibited more balanced perspectives. However, all the LLMs lack consideration of Novelty for weaknesses compared to human experts, which is a significant limitation in reviewing papers.}
    \label{fig:radar_chart}
\end{figure*}




While human experts raised various number of points, LLMs identified a relatively consistent number of points regardless of the paper's content. Moreover, LLMs identified a similar number of points between strengths and weaknesses, which was a different pattern from that of the human experts. Figure~\ref{fig:num_reviews} shows the distribution of the number of strengths and weaknesses identified by human experts and LLMs. Overall, LLMs identified more points on average (7.88) than human experts (5.39). Among the LLMs, \texttt{Llama} models identified fewer (3.17 strengths and 3.15 weaknesses, on average) whereas \texttt{o1-mini} reported more strengths and weaknesses (5.03 and 5.47, respectively) than other models. By comparing target and aspect labels between human experts and LLMs, we report the following key findings.




% The precision and recall were computed by computing true positive, false positive, and false negative as \( TP = \sum \left| E_{r,c} \cap L_{r,c} \right|\), \( FP = \sum \left| L_{r,c} \setminus E_{r,c} \right|\), \(FN = \sum \left| E_{r,c} \setminus L_{r,c} \right| \) for \( r \in R, c \in \{\text{strength},\,\text{weakness}\} \), where $R$ is the set of papers, $c$ is the type of review item, and $E_{r, c}$ is the set of (target, aspect) that experts identified for the paper $r$ and the type $c$, and $L_{r, c}$ is the LLM-identified set.

\textbf{Overall, LLMs do not effectively identify key targets and aspects when reviewing papers.} Table~\ref{tab:metrics} shows the overall performance of LLMs, which computes the agreement of (target, aspect) labels between human experts and LLMs. The best F1 score among the LLMs was 0.37, which indicates a low level of agreement with human experts in identifying strengths and weaknesses. Since we only considered whether the categories of review items match rather than their detailed content, the result implies that the actual content of strengths and weaknesses would be significantly different between human experts and LLMs. In general, LLMs showed higher recall than precision scores, mainly due to the nature of identifying a higher number of review points than human experts. Also, LLMs consistently achieved higher F1 scores for weaknesses than strengths. Among the LLMs, \texttt{deepseek-r1} achieved the best overall performance, \texttt{o1-mini} achieved the best recall, and \texttt{Llama-405B} achieved the best precision.

\textbf{While overall agreement is low, both groups primarily emphasized technical validity and novelty in the strengths, and focused on technical validity and clarity in the weaknesses.} Figure~\ref{fig:radar_chart} shows the normalized distribution of target and aspect labels for both experts and LLMs. For targets, both groups primarily focused on core technical elements---Method, Experiment, and Theory. However, strengths and weaknesses illustrated different patterns: both groups praised Method more than Experiment in the strengths, but criticized Experiment more than Method in the weaknesses. For aspects, both groups considered Validity as an important aspect, especially when evaluating weaknesses. Impact received more attention than Clarity in the strengths, whereas the opposite was observed in the weaknesses.

 % While Method and Experiment showed the high level of agreement as both groups share the core perspective, Problem and Theory showed low agreement. For Theory, F1-score was consistently higher for weaknesses than strengths, suggesting that LLMs are more effective at identifying concerns in theories (e.g., the assumptions are too strong) than recognizing strong points of theories (e.g., the theoretical analysis is thorough). For aspects, Validity showed the highest agreement, but it would be due to our focus of \textit{rejected} papers. Specifically, we observed that LLMs almost always report Validity concerns in weaknesses (Recall $\geq$ 0.99 on average) where rejected papers might contain such limitations. Novelty aspect showed low agreement, particularly for weaknesses, which is due to the lack of focus in Novelty in evaluating weaknesses.

\textbf{LLMs \textit{consistently} exhibited a more biased perspective, while human experts maintained a more balanced perspective.} Although both groups shared a core perspective, LLMs tend to focus on a few specific dimensions. For instance, LLMs focused primarily on Method and Experiment, while neglecting Prior Research (e.g., whether the paper adequately addresses prior work in positioning) and Problem (e.g., whether the task needs community attention), which human experts point out (Problem in the strengths and Prior Research in the weaknesses). For aspects, LLMs mostly focused on Validity in both strengths and weaknesses. In contrast, human experts considered the aspects more evenly among Validity, Novelty, and Clarity. Notably, LLMs exhibited a significant bias for Novelty. LLMs praised Novelty in the strengths, whereas they rarely criticized it in the weaknesses. This is a significant drawback, as a paper review requires a critical examination of novelty, by comparing them against existing work.

\begin{table*}[ht]
  \centering
  \caption{F1 Score for target and aspects between \texttt{DeepSeek-R1}, \texttt{o1-mini}, and \texttt{Llama-405B}. Due to the biased perspective of LLMs, we observed a clear gap between what LLMs mostly focus on (e.g., Method and Experiment targets and Validity aspect) and overlook (e.g., Problem target and Novelty aspect). Full results (F1 score, precision, and recall across models and target/aspect labels) are available in Appendix~\ref{appendix:result}. }
  \label{tab:merged_sw}
  \begin{minipage}[t]{0.48\textwidth}
    \centering
    \textbf{Target F1 score}\\[1ex]
    \footnotesize
    \begin{tabular}{lccc}
      \toprule
      Target       & \texttt{DeepSeek-R1}            & \texttt{o1-mini}              & \texttt{Llama-405B} \\
      \midrule
      Problem            & \swrow{0.30}{0.40}{0.20} & \swrow{0.28}{0.35}{0.20} & \swrow{0.16}{0.16}{0.16} \\
      \grayline
      Method             & \swrow{\textbf{0.73}}{0.75}{0.71} & \swrow{\textbf{0.76}}{0.75}{0.77} & \swrow{\textbf{0.69}}{0.76}{0.63} \\
      \grayline
      Theory             & \swrow{0.47}{0.44}{0.51} & \swrow{0.47}{0.41}{0.53} & \swrow{0.43}{0.46}{0.40} \\
      \grayline
      Experiment         & \swrow{\textbf{0.68}}{0.51}{\textbf{0.85}} & \swrow{\textbf{0.68}}{0.51}{\textbf{0.86}} & \swrow{\textbf{0.66}}{0.52}{\textbf{0.81}} \\
      \bottomrule
    \end{tabular}
  \end{minipage}\hfill
  \begin{minipage}[t]{0.48\textwidth}
    \centering
    \textbf{Aspect F1 score}\\[1ex]
    \footnotesize
    \begin{tabular}{lccc}
      \toprule
      Aspect         & \texttt{DeepSeek-R1}            & \texttt{o1-mini}              & \texttt{Llama-405B} \\
      \midrule
      Novelty               & \swrow{0.39}{0.66}{\textbf{0.12}} & \swrow{0.39}{0.66}{\textbf{0.12}} & \swrow{0.34}{0.66}{\textbf{0.01}} \\
      \grayline
      Impact                & \swrow{0.41}{0.54}{0.29} & \swrow{0.43}{0.56}{0.30} & \swrow{0.32}{0.35}{0.29} \\
      \grayline
      Validity              & \swrow{\textbf{0.77}}{0.60}{0.95} & \swrow{\textbf{0.77}}{0.60}{0.95} & \swrow{\textbf{0.77}}{0.60}{0.95} \\
      \grayline
      Clarity               & \swrow{0.27}{0.17}{0.36} & \swrow{0.40}{0.30}{0.50} & \swrow{0.28}{0.16}{0.40} \\
      \bottomrule
    \end{tabular}
  \end{minipage}
\end{table*}



Due to their biased focus, the level of agreement between LLMs and human experts varied across different labels. Table~\ref{tab:merged_sw} shows F1 scores for specific targets and aspects. For targets and aspects that LLMs focus more on --- Method and Experiment targets and Validity aspect --- LLMs had a much higher level of agreement with human experts compared to other targets and aspects. In the case of Experiment, the F1 score was consistently higher for weaknesses than strengths, suggesting that LLMs are more effective at identifying concerns in experiments (e.g., lack of baselines or scope of evaluation) than recognizing strong points of theories (e.g., experiments are rigorous and thorough). Similarly, for aspects other than Validity, agreement levels were notably lower. In particular, Novelty in the weaknesses, which LLMs largely overlooked, showed a significantly lower F1 score.

% Validity showed the highest agreement, but it would be due to our focus of \textit{rejected} papers. Specifically, we observed that LLMs almost always report Validity concerns in weaknesses (Recall $\geq$ 0.99 on average) where rejected papers might contain such limitations. Novelty aspect showed low agreement, particularly for weaknesses, which is due to the lack of focus in Novelty in evaluating weaknesses.

\textbf{LLMs showed similar patterns in their emphasis in reviewing papers, regardless of their size and reasoning capability.} All LLMs, including both proprietary and open source models, showed similar patterns that focused primarily on technical (Method, Experiment, and Theory) validity than on Novelty for the weaknesses. This consistency indicates that the observed biases could stem from the inherent design and training methods of LLMs, revealing potential room for improvement in the reasoning capability that requires leveraging external information (e.g., identifying comparable related work and analyzing novelty of submissions). 

\textbf{Final acceptance decisions are not accurate.} Table~\ref{tab:rejection_percent} shows the rejection rate reported by each LLM. Overall, the best achieved rejection rate was 24.9\%, which indicates that recall for rejected papers is poor. \texttt{gpt-4o}, \texttt{Llama-405B}, and \texttt{DeepSeek-R1} performed significantly better than other models. \texttt{gpt-4o} and \texttt{Llama-405B} showed relatively high rejection rates while their agreement with human experts on strengths and weaknesses was low (0.348 and 0.349). \texttt{DeepSeek-R1} demonstrated a moderate rejection rate among the models, with a relatively higher agreement score on strengths and weaknesses (0.373). 

\begin{table}[ht]
\centering
\caption{Rejection percentage by model. 100\% is the highest score, as we considered \textit{rejected} papers. All the models were highly positive about the paper acceptance, although the papers were rejected.}
\label{tab:rejection_percent}
\begin{tabular}{lc}
\toprule
Model       & Rejection (\%) \\
\midrule
\texttt{gpt-4o}      & 27.92\% \\
\texttt{Llama-405B}  & 24.30\% \\
\texttt{DeepSeek-r1} & 23.79\% \\
\texttt{gpt-4o-mini} & 9.50\%  \\
\texttt{DeepSeek-v3} & 7.93\%  \\
\texttt{o1-mini}     & 5.45\%  \\
\texttt{o1}          & 3.36\%  \\
\texttt{Llama-70B}   & 0.74\%  \\
\bottomrule
\end{tabular}
\end{table}


% \textbf{ChatGPT o1-pro with Deep Research shows the similar pattern with other LLMs, with poor rejection rate. \notyet{under experiment}}