
\section{Discussion}

In this paper, we found gaps in the way of reviewing papers between human experts and LLMs and reported several limitations of LLMs as an automated reviewer, using an automated pipeline. Based on the results, we discuss the following implications.

\newpage
\textbf{There exists significant room for improving alignments between human experts and LLMs in reviewing papers.} Our results show that LLMs exhibit a more biased perspective, which mostly examines technical validity without contextual consideration, compared to human experts. To reduce the gap, fine-tuning models using our dataset could serve as a starting point. While our results revealed significant limitations of LLMs in reviewing papers, our focus was mostly on the target and aspect labels rather than comparing actual content. We suspect that a more significant gap lies in the actual content addressed in the review items, even if they share the same target and aspect labels. For instance, (Experiment, Validity) could point out either lack of necessary baselines or lack of ablation studies to justify authors' arguments. Content-level investigations may reveal more limitations of LLMs, ultimately contributing to improving the reasoning capability of LLMs. % A checklist-based approach, which asks the LLM to generate strengths and weaknesses that match those of human experts, can be a promising direction.
 % Also, research could investigate how to design Human-AI interactions by leveraging what LLMs excel at. For instance, LLMs can be effective in rigorously examining technical validity. % We believe that our automatic evaluation pipeline can be used for evaluating the alignments, evolved by updating recent publication data. 
%  For LLM-powered paper reviews to provide value to various user groups (e.g., novice researchers seeking feedback or reviewers evaluating submissions), it is important to reduce the gap.  

\textbf{Research should investigate the task of assessing the novelty of academic papers.} Our finding illustrated that all LLMs in our analysis significantly overlooked the novelty aspect when evaluating weaknesses of papers. Previous studies have indicated that language modelsâ€™ ability to assess novelty is inferior to that of experts~\cite{Just02072024, Lin2024EvaluatingAE}, emphasizing the need to encourage LLMs to focus on novelty evaluation. Although novelty is one of the most important aspects in reviewing papers and efforts have been made to enhance LLMs' ability to assess novelty~\citep{Bougie2024GenerativeAR, Lin2024EvaluatingAE}, there exists no suitable benchmark for systematically measuring the novelty assessment capability of LLMs. We believe that creating the benchmark is a valuable contribution to the field, which allows LLMs to learn how to assess similarities between papers. Leveraging data in OpenReview could be an initial step as it contains experts' judgement on novelty of the paper for both positive and negative decisions.

 % Moreover, our initial attempt at evaluating LLMs with access to external sources suggests that it currently does not significantly improve the novelty reviewing capability. It implies that assessing novelty is challenging even with strong reasoning capability, which essentially requires not only finding comparable papers but also assessing the similarity between the papers, which might vary across the subject of comparison such as dataset, methods, and theory. 
 
\textbf{It is important to further explore the alignment between points of strengths and weaknesses and the final decision based on them.} We found that there exists a discrepancy between achieving a high level of agreement in strengths and weaknesses and correctly predicting the final decision. It implies that the way LLMs make the final decision based on the identified strengths and weaknesses could be different from the way of human experts. Consistent with our findings, previous research revealed that LLMs offer positive assessments~\cite{latona2024aireview}. As it is important to inform clear and convincing rationale behind the final decision, further investigation is needed to carefully evaluate whether the final decision based on the strengths and weaknesses is reasonable, for various stakeholders such as domain experts or novices. Generating reviews with clear alignments between the review points and final decision is a challenging task because often the relationship is not very clear and implicit~\cite{zhou2024llm}, relying on community norms and social factors. Learning the relationship from the review data could be useful for understanding the gap between human experts and LLMs in decision-making.


\section{Related Work}

With the powerful reasoning capability of LLMs, LLMs have the potential to assist in the task of reviewing papers~\citep{latona2024ai, d2024marg}. Research has explored the capability of LLMs in reviewing papers, identifying a set of limitations. While LLM-generated reviews can be helpful~\citep{liang2024can, tyser2024ai, Lu2024TheAS}, research has shown that LLMs-generated reviews lack diversity~\citep{du2024llms, liang2024can} and technical details~\citep{zhou2024llm}, exhibit bias~\citep{Ye2024AreWT}, tend to provide positive feedback~\citep{zhou2024llm, du2024llms}, and may include irrelevant or even inaccurate comments~\citep{mostafapour2024evaluating}. Furthermore, research also has reported that LLM-generated reviews have a low level of agreement with experts-generated reviews~\citep{saad2024exploring}. 

To assess the quality of review, research has taken a quantitative approach by analyzing review text. For instance, research has evaluated the quality of review based on human preferences~\citep{tyser2024ai}, similarity to human-generated review~\citep{zhou2024llm, liang2024can, gao2024reviewer2optimizingreviewgeneration, Sun2024MetaWriterET, chamoun2024automated} and classification-based scores~\citep{Li2023SummarizingMD}. Another approach is to classify review data based on categories such as section~\citep{ghosal2022peer}, aspect~\citep{yuan2022can, chamoun2024automated, liang2024can} and actionability~\citep{Choudhary2022ReActAR}. While quantitative approach provides concrete insights, it is typically conducted as a one-time evaluation, challenging to apply the consistent methodology over time.

\section{Conclusion}

We introduced an automatic evaluation pipeline to assess LLMs' capability in reviewing papers, by examining the agreement between LLM-generated and expert-generated reviews based on their target and aspect annotations for strengths and weaknesses. Our findings suggest that LLMs need to adopt a more balanced perspective, place greater emphasis on novelty assessment when critiquing papers, and better formulate their final judgement based on the identified strengths and weaknesses. We believe that our automated pipeline can contribute to ongoing evaluation of LLMs' paper review capabilities within the rapid pace of LLM developments, offering concrete insights for improving their reasoning capability.

\section*{Limitation}

This paper has the following limitations. First, our dataset focuses solely on ICLR submissions and the coding schema is developed based on AI venues, which limit generalizability to other fields. Second, our analysis examines the target and aspect of the review items, but other important dimensions such as level of specificity and depth of justification remain unexplored. Third, while our automatic annotator achieved high IRR (0.85) with human annotations, some discrepancies still exist. Finally, we did not explore possible prompt engineering strategies that could mitigate the limitations of LLMs in paper review. Future work can investigate techniques to enhance the alignment between LLMs and human experts.

\section*{Ethical impact}
This paper presents potential risks. First, while our vision is to build LLMs to effectively assist review process, our work could inadvertently encourage over-reliance on LLM-generated reviews among various user groups, including reviewers and novice researchers. Second, although our dataset could contribute to improving LLM performance of reviewing papers, it may introduce a certain bias due to the source of dataset; ICLR for papers and code based on AI research. Finally, we assess the quality of review based on alignment with expert reviews, but it could offer a potentially biased perspective, as our coding schema only considers two dimensions, which may undervalue the unique contributions of LLM-generated reviews.