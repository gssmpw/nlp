\section{Introduction}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{image.png}
\caption{We consider paper review task that generates a summary of paper, strengths, weaknesses, and the final judgement. Our goal is to examine the level of agreement between LLMs and human experts in reviewing papers, based on their feedback targets and aspects.}
\label{fig:task}
\end{figure}


Reviewing academic papers lies at the heart of scientific advancement, but it demands substantial expertise, time, and effort. The peer review system faces several challenges, including a growing number of submissions that outpace the reviewer availability, lack of incentives, and reviewer fatigue~\citep{tropini2023time, horta2024crisis, hossain2025llmsmetareviewersassistantscase}. While Large Language Models (LLMs) hold the potential to assist reviewers by reviewing papers automatically~\citep{Hosseini2023FightingRF, Robertson2023GPT4IS}, prior research has reported significant limitations in their performance~\citep{du2024llms, liang2024can, zhou2024llm}. For example, studies have highlighted that LLM-generated reviews often lack accuracy, detail, and specificity when compared to reviews written by human experts~\citep{zhou2024llm, mostafapour2024evaluating}. 

While these findings are informative, they do not sufficiently clarify the precise differences between expert-generated reviews and LLM-generated reviews. Specifically, it is still unclear to what extent LLMs excel or fall short in different aspects of reviewing compared to human experts. Addressing the question requires systematic quantitative analysis of review data, but such analysis is challenging to scale due to the significant time and effort required from researchers.


% which hinders efforts to clearly identify where LLMs excel or fall short in the context of academic review.

To address this gap, we introduce an automated pipeline designed to systematically analyze these differences. Our approach is to automatically annotate the strengths and weaknesses identified in reviews based on targets (e.g., problem, methodology, and experiment) and their associated aspects (e.g., validity, clarity, and novelty) and examine the agreement between LLMs and human experts (Figure~\ref{fig:task}). By introducing a systematic framework for examining the strengths and limitations of LLMs in academic review, this work offers valuable insights into improving their performance and enhancing their potential role in assisting the review process.

\begin{figure*}[t]
    \centering
    % Replace the placeholder box below with your actual figure code:
    % \fbox{\rule{0pt}{2in} \rule{.9\textwidth}{0pt}}
    \includegraphics[width=\linewidth]{pipeline.png}
    \caption{The overall evaluation process. Given a paper, we extract strengths and weaknesses from review data on the OpenReview platform. To identify key strengths and weaknesses that influence the final acceptance, we extracted them from the meta-review and augmented details from reviewer comments to make them self-contained. We then compared these with LLM-identified strengths and weaknesses, based on their feedback target and aspect. The evaluation is conducted automatically, enabling a scalable 
 and longitudinal evaluation over time.}
    \label{fig:review_generation}
\end{figure*}



Our study leverages a dataset of 676 papers and their review data that has been collected from OpenReview\footnote{https://openreview.net/} for ICLR conferences spanning 2021 to 2024. We extracted the strengths and weaknesses highlighted in the meta-reviews using \texttt{gpt-4o\footnote{gpt-4o-2024-08-06}}, which will be compared with LLM-identified strengths and weaknesses. Then, we developed a coding schema (Table~\ref{tab:coding_schema}) for annotating the targets and aspects of these strengths and weaknesses, by surveying 9 AI paper submission guidelines and prior research on review analysis~\citep{chakraborty2020aspect, ghosal2022peer, yuan2022can}. Based on the schema, we manually annotated 327 strengths and weaknesses of 68 randomly sampled papers, providing a basis for building an automatic annotation tool. Our LLM-powered automatic annotation tool achieved 0.85 (target) and 0.86 (aspect) inter-rater reliability (IRR) with human-annotated results, showing high level of consistency and accuracy. Overall, we identified 3,657 review items (1,231 strengths and 2,416 weaknesses) from the review dataset.

We evaluated 8 LLMs (4 GPT, 2 Llama, and 2 DeepSeek family) for their paper review capability. After generating reviews for each of the 676 papers, we analyzed the agreement between LLMs' and experts' reviews based on their target and aspect assigned to strengths and weaknesses. The results showed that: 1) LLMs lack balanced perspective compared to human experts, 2) LLMs significantly neglect novelty assessment for evaluating papers' weaknesses, and 3) the paper acceptance decisions are not accurate. The findings are consistent for all the LLMs, highlighting clear opportunities for improving their reasoning capability.

We release a dataset comprising 68 papers, experts reviews, 3,657 strengths and weaknesses identified from the reviews with automatically annotated targets and aspects, LLM-generated reviews from 8 LLMs, and a total of 43,042 strengths and weaknesses identified from the LLMs with their annotated targets and aspects.
