\section{Related Work}
\label{sec:related-works}

The software quality becomes uncertain if test cases unpredictably change their outcomes (such as from pass to fail or the reverse) without any changes to the codebase, known as flaky tests. A typical approach to identify test flakiness is to re-run the tests to confirm if their outcomes are consistent. The iDFlakies~\cite{lam2019idflakies} identifies flaky tests by running tests in different randomized orders and tracking inconsistencies in test outcomes. When a test’s outcome changes between runs, iDFlakies flags it as potentially flaky. The framework also partially classifies the flaky tests by identifying patterns in test order dependencies, helping developers understand the flakiness causes. Shaker~\cite{silva2020shake} detects concurrency-related flaky tests by systematically manipulating thread schedules during test execution, a technique known as schedule perturbation. By altering the order of thread execution, Shaker increases the likelihood of exposing concurrency issues, such as race conditions, deadlocks, and timing dependencies, that might otherwise go unnoticed. However, the rerunning approach is costly; developers invest significant time and effort in diagnosing and investigating the root causes of flaky tests. Thus, the re-running approach is an inefficient and time-consuming method for managing test flakiness. 

To address the limitations of the re-running approach, many static methods are proposed, including ML-based approaches~\cite{bell2018deflaker, alshammari2021flakeflagger, verdecchia2021know}. DeFlaker~\cite{bell2018deflaker} was proposed to detect flaky tests by monitoring code coverage information during a single test run. It tracks which lines of code are covered by each test and identifies cases where a test fails without covering any new code compared to previous runs. This approach suggests that such failures are likely due to flakiness, as there are no relevant code changes that could explain the failure. The FlakeFlagger~\cite{alshammari2021flakeflagger} framework predicts the likelihood of a test being flaky by analyzing static features extracted from code and test history (past pass/fail history, frequency of recent changes, and frequency of test failures over time). The framework leverages ML binary classifiers trained on attributes associated with flakiness, enabling it to flag potentially flaky tests proactively without multiple executions. They experimented with several ML classifiers, including RFs, XGB, and SVM. Feature selection and hyperparameter tuning were performed to improve the model’s ability to generalize and reduce overfitting. Additionally, FLAST~\cite{verdecchia2021know} introduces a static analysis approach to predict test flakiness without reruns, leveraging features from neighboring code dependencies and characteristics. The model uses static features—like code complexity, dependency patterns, and the flakiness history of neighboring code—to predict if a test will be flaky. This neighbor-based analysis assumes that tests closely tied to flaky code are also likely to be flaky.

Several studies focus on detecting the root causes of flaky tests using ML approaches. For instance, FlakyCat~\cite{akli2023flakycat} leverages ML models to identify the categories of flaky tests. The framework uses supervised learning techniques, with a pre-defined taxonomy of flaky test types as labels. Their results demonstrated that ML-based categorization is a promising approach to categorizing flaky tests effectively. We avoid reviewing more papers in this regard, as it is out of the scope of the main goal of this study.

Although ML-based approaches can greatly reduce the overhead linked to dynamic methods, they still have certain limitations. Specifically, these models often struggle with imbalanced data in flaky test detection, as flaky instances are rare in real-world scenarios. The imbalance can cause ML models to favor stability in their predictions, limiting their ability to reliably detect instances of flakiness. In our study, we aim to reflect this imbalance issue in our studied dataset and investigate the effectiveness of integrating two existing methods—threshold tuning and SMOTE—within our proposed pipelines.