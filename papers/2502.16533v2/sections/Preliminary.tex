\section{Preliminaries}
\label{sec.prelimiary}
\renewcommand{\arraystretch}{1.3}
\begin{table}[htbp!]
  \centering
  \caption{The basic notation utilized in this survey.}
    \begin{tabular}{m{0.2\linewidth}<{\centering} m{0.7\linewidth}<{\raggedright}}
    \toprule
    \multicolumn{1}{c}{\centering\textbf{Notation}} & \multicolumn{1}{l}{\textbf{Description}} \\
    \midrule
    $\mathcal G = (\mathcal V, \mathcal E)$   & A graph $\mathcal G$ contains node set $\mathcal V$ and edge set $\mathcal E$.  \\ \hline
    $\mathcal G = (\mathcal V, \mathcal E, \mathcal T)$ & A dynamic graph $\mathcal G$ with varying node set $\mathcal V$ and edge set $\mathcal E$ in temporal domain $\mathcal T$.  \\ \hline
    $\mathcal{\vec{G}} = (\mathcal V, \mathcal E, \vec{\BX})$ & A geometric graph $\mathcal G$ with node set $\mathcal V$ and edge set $\mathcal E$, also including the 3D coordinates matrix for node set $\vec{\BX} \in \mathbb R^{n \times 3}$.  \\ \hline
    $n \in \mathbb R $   & The number of nodes in the graph. \\ \hline

    $\BH \in \mathbb{R}^{n\times d}$   & The node features in the graph. \\ \hline
    $\Tilde{\BH} \in \mathbb{R}^{n\times d}$   & The updated node representations of the graph. \\ \hline
    $\mathbf{A} \in \mathbb R^{n \times n}$ & The attention matrix learn in the Transformer.\\ \hline
    $\mathbf{A}^{\BG} \in \mathbb R^{n \times n}$ & The adjacency matrix in the graph.\\ \hline

    $\m_{ij}$   & The message from node $i$ to node $j$. \\ \hline
    $\e_{ij}$   & The edge feature between node $i$ and node $j$. \\ \hline
    $\mathcal{N}_{i}$   & The neighboring nodes of node $i$. \\ \hline
    $\phi$, $\BW$, $\b$   & The learnable parameters in neural networks. \\ \hline
    $\BQ, \BK, \BV$   & The query, key and value matrices in attention. \\ \hline
    $\BD \in \mathbb R^{n \times 1}$   & The degree matrix, indicating the degree of the nodes. \\ \hline
    $\BU \in \mathbb R^{n \times n}$   & The eigenvector matrix after Laplacian decomposition. \\ \hline
    $\mathbf{\Lambda} \in \mathbb R^{n \times n}$   & The diagonal eigenvalue matrix after Laplacian decomposition. \\ \hline
    $\BL \in \mathbb R^{n \times n}$   & The Laplacian matrix of the graph. \\ \hline
    $\BP \in \mathbb R^{n \times k}$   & The absolute PE in Graph Transformer, where $k \in \mathbb R$  is a hyper-parameter.\\ \hline
    $\BM \in \mathbb R^{n \times n}$   & The graph kernel. \\ \hline
    $\mathcal L$   & The loss function. \\ \hline
    
    $\BI \in \mathbb R^{n \times n}$   & The identity matrix. \\ \hline
    $||$   & Concatenate operation. \\ \hline
    $\odot $   & Element-wise production. \\ \hline
    $|\cdot|$   & The length of a set. \\ 
    

    \bottomrule
    \end{tabular}%
  \label{tab:notation}%
\end{table}%


In this section, we provide a concise overview of the essential notations pertaining to GTs, as well as a summary of the fundamental architecture of the vanilla Transformer and message passing neural  networks.
Table~\ref{tab:notation} lists the frequently-used notation throughout the paper. 

We denote a graph as $\mathcal G = (\mathcal V, \mathcal E)$, where $\mathcal V$ represents the set of nodes and $\mathcal E$ represents the set of edges. The node set $\mathcal V$ comprises $n$ nodes, with the feature matrix $\BH \in \mathbb{R}^{n\times d}$, where $n$ is the number of nodes and $d$ is the dimension of the node features. The edge set $\mathcal E$ corresponds to an adjacency matrix $\mathbf{A}^{\BG} \in \mathbb R^{n \times n}$, where $\mathbf{A}^{\BG}_{u,v} = 1$ if there exists an edge $(u, v)$ in $\mathcal E$, and $\mathbf{A}^{\BG}_{u,v} = 0$ otherwise. 

The aforementioned notations are adequate for representing a basic graph with an adjacency matrix and node features. In more challenging scenarios, graphs are associated with more features to support various applications 1): Geometric Graph: In addition to node property features, the nodes will have coordinate features $\Vec{\BX} \in \mathbb{R}^{3}$, which will be independently handled to achieve invariance or equivariance properties.
2): Graph with edge features: Rather than merely converting the edge information into an adjacency matrix, the edges offer supplementary information that denote more specific relationships between the nodes.
3): Dynamic graph: The graph additionally contains the time domain $\mathcal T$ compared with static graphs, represented as $\mathcal G = (\mathcal V, \mathcal E, \mathcal T)$. This indicates that the nodes and edges can change over time. The graph is denoted as $\mathcal G = \{(v_i, v_j, \tau)_n, n = 1, 2, \ldots, |\mathcal E|\}$, where each tuple $(v_i, v_j, \tau)$ represents an edge between node $v_i$ and node $v_j$ at a specific time $\tau \in \mathcal T$ and $\mathcal E$ specifies the edge set for that particular timestep.

Dynamic graphs are generally classified into two categories: discrete-time dynamic graphs (DTDGs) and continuous-time dynamic graphs (CTDGs). In DTDGs, the time domain $\tau \in \mathcal{T}$ is divided into $k$ discrete time intervals, denoted as $\tau = [t_1: t_k]$. This allows the dynamic graph $\mathcal{G}$ to be represented as a sequence of individual graph snapshots $\mathcal{G} = [\BG_{t_1}, \BG_{t_2}, \ldots, \BG_{t_k}]$. In contrast, CTDGs model scenarios where the graph evolves continuously over time,  providing the exact graph structure at any given timestamp $t$. In CTDG, changes to the graph structure are triggered only by certain events $\tau$.

\textbf{Message Passing Graph Neural Networks}: 
Graph neural networks~\cite{kipf2016semi} are a foundational framework for learning representations in graphs via the message passing mechanism. Given node embeddings $\BH$ and the adjacency matrix $\BA^{\BG}$, a message passing neural network (MPNN) $\phi_\theta$ updates  node embeddings via the propagating information from neighboring nodes.
The process of modeling node embeddings in an MPNN can be formally expressed as:
\begin{equation}
    \m_{ij} = \phi_{msg}(\BH_i, \BH_j, \e_{ij}),
\end{equation}
\begin{equation}
    \BH_{i} = \phi_{upd}(\BH_i, \{\m_{ij} \}_{j \in \mathcal{N}_i}),
\end{equation}
where $\BH$ and $\e$ represent node and edge embeddings, respectively. Here, $\phi_{msg}(\cdot)$ and $\phi_{upd}(\cdot)$ are two transformations in the MPNN $\phi_\theta$.
Specifically, $\phi_{msg}(\cdot)$ computes the message received by node $v_i$ from its neighbors, while $\phi_{upd}(\cdot)$ updates the node embedding by the messages.

\textbf{Transformers:}
Transformers have demonstrated remarkable success across diverse fields, achieving high performance in natural language processing~\cite{devlin-etal-2019-bert} and computer vision~\cite{dosovitskiy2021an} applications. 
Transformers employ an encoder-decoder architecture, 
where the building block is the self-attention~\cite{vaswani2017attention} mechanism.
The self-attention module operates on a sequence of $n$ tokens, $\BH \in \mathbb{R}^{n\times d}$, which can be represented by a transformation function $\phi_\theta(\cdot) : \mathbb{R}^{n \times d} \rightarrow \mathbb{R}^{n \times d}$, where $d$ is the feature dimensions of each token. 
The formulation of function $\phi_\theta(\cdot)$ is shown in Equation~\eqref{eq:qkv}-\eqref{eq:attention:final}:
\begin{align}
\BQ &= \BH \BW_{Q}, \BK = \BH \BW_{K}, \BV = \BH \BW_{V}, \label{eq:qkv}\\
\BA &= \text{Softmax}\left(\frac{\BQ\BK^{T}}{\sqrt{d}}\right), \label{eq:attention}\\
\Tilde{\BH} &= \BA \BV, \label{eq:attention:final}
\end{align}
where the input $\BH$ is first transformed into query, key and value matrices, $\BQ, \BK, \BV \in \mathbb{R}^{d\times d}$, by linear wight matrices $\BW_{Q}, \BW_{K}, \BW_{V} \in \mathbb{R}^{d\times d}$, respectively. 
Then, the attention matrix $\BA \in \mathbb{R}^{n \times n}$ is computed by the inner product of the query and key, followed by the normalization of a $\text{Softmax}(\cdot)$ function.
Here, $\BA_{ij} \in [0, 1]$ indicates the influence of $\BH_j$ on $\BH_j$.
The attention matrix is finally applied to the value matrix to generate new embeddings of the tokens $\Tilde{\BH} \in \mathbb{R}^{n \times d}$. 
By interpreting the attention matrix $\BA$ in~\cref{eq:attention} as an alternative adjacency matrix, the self-attention mechanism can be viewed as a variant of MPNNs with a fully connected adjacency matrix as Equation~\eqref{eq:atten_as_mpnn:message}-\eqref{eq:atten_as_mpnn:update}:
\begin{align}
    \label{eq:atten_as_mpnn:message}
    \m_{ij} &= \BA_{ij} \BV_{j},\\\label{eq:atten_as_mpnn:update}
    \BH_{i} &= \sum_{i=0}^{n} \m_{ij}
\end{align}

After applying self-attention, Transformers employ element-wise feed-forward networks (FFN), which comprise two linear layers with ReLU activation:
\begin{equation}
    \text{FFN}(\Tilde{\BH}) = \text{max}(0, \Tilde{\BH}\BW_{1} + b_{1})\BW_{2} + b_{2},
\end{equation}
where $\BW_{1} \in \mathbb{R}^{d\times d_{1}}, \BW_{2} \in \mathbb{R}^{d_{1}\times d}$ are learnable parameters. In addition, the sublayers in the FFN will incorporate residual connections and layer normalization~\cite{ba2016layer}.

In practice, multi-head attention (MHA) is widely utilized in Transformers to enhance representation learning. 
Precisely, the $\BQ, \BK, \BV$ matrices obtained from~\cref{eq:qkv} are divided into $H$ independent heads, denoted as $\BQ^{(h)}$, $\BK^{(h)}$ and $\BV^{(h)}$, respectively. 
The MHA mechanism computes representations as Equation~\eqref{eq:mha}:
\begin{align}
    \label{eq:mha}
    \Tilde{\BH} = ||_{h=1}^{H} \text{Softmax}\left(\frac{\BQ^{(h)}\BK^{(h)T}}{\sqrt{d_h}}\right)\BV^{(h)},
\end{align}
where $d_h = d/H$ represents the dimension assigned to each head.
MHA acquires representations from various perspectives within each head and concatenates the embeddings from each head to the final representation.

Another critical component in Transformers is positional encoding (PE), which is designed to encode the relative position of tokens in a sequential input. 
The original paper of Transformer~\cite{vaswani2017attention} introduces a parameter-free sinusoidal PE. Nevertheless, subsequent advancements, such as learned PE~\cite{gehring2017convolutional} and rotary PE~\cite{su2024roformer}, have demonstrated that the choice of PE significantly impacts the performance of Transformers, highlighting its importance in Transformers.


