\section{Architectures}
\label{sec.architectures}
\begin{figure*}
	\centering
	\resizebox{0.8\textwidth}{!}
    {\includegraphics[width=\linewidth]{figures/Frame_3.pdf}}
	\caption{The overview of the architecture of Graph Transformers. The general GT part outlines the various methodologies employed to incorporate structural priors within GTs, including multi-level tokenization, positional encoding, modifying attention matrix and ensemble with GNNs. Other four parts delineate how these methodologies are applied to GT. Methods in the parentheses are representative implementations in their corresponding taxonomies.
 }
	\label{fig:framework}
\end{figure*}


\renewcommand{\arraystretch}{1.2}

\begin{table}[]
	\centering
 	\caption{A summary of existing architecture of GTs. For each model, we document the graph tokenization, the implementation of positional encoding, the utilization of structure-aware attention, and the inclusion of an additional MPNN. \textup{Abbr.,} PE: Positional Encoding, Attn: Attention, Ens: ensemble, Sub.: SubGraph, K-hop: K-hop Neighbor, RW: Random Walk, Deg.: Degree, Lap: Laplacian.}

 \resizebox{\columnwidth}{!}
	{\begin{tabular}{l|lllll}
        \toprule
		\textbf{Model}            & \textbf{Token} & \textbf{PE} & \textbf{Attn} & \textbf{Ens}  \\
        \midrule
		SE(3)-Transformer~\cite{fuchs2020se3transformers}      &  Node   &    & \checkmark      &  
        \\\hline
		Graphormer~\cite{Graphormer}      &  Node   &  Deg.  & \checkmark      &  
        \\\hline
		Dwivedi ~\emph{et al.} ~\cite{dwivedi2021generalization}      &  Node   &  Lap. & \checkmark      &  
        \\\hline
        SAN~\cite{kreuzer2021rethinking}      &  Node   &  Lap. & \checkmark      &  
        \\\hline
        GraphiT~\cite{mialon2021graphit}       &  Sub.   &  RW & \checkmark      &  
        \\\hline
        TokenGT~\cite{TokenGT}  & Edge      &  Lap.  &      & 
        \\\hline
        SAT~\cite{chen2022structure} & Sub.      &  Hybrid 
        &   \checkmark   & 
        \\\hline
        TorchMD-Net~\cite{tholke2022torchmd} & Node      &    &   \checkmark   & 
        \\\hline
		Mask-transformer~\cite{min2022masked} & Node      &    &   \checkmark   & 
        \\\hline
		GraphGPS~\cite{rampavsek2022recipe} & Node      &  Hybrid  &      & \checkmark
        \\\hline
		GKAT~\cite{choromanski2022block} & Node      &    &    \checkmark  & 
        \\\hline
		NodeFormer~\cite{wu2022nodeformer} & Node   &    &   \checkmark   & 
        \\\hline
		EGT~\cite{hussain2022global} & Edge   &  SVD  &   \checkmark   & 
        \\\hline
        Exphormer~\cite{shirzad2023exphormer} & Node      & Lap.   &  \checkmark    & 
        \\\hline
		NAGphormer~\cite{NAGphormer} & K-hop      &    &      & 
        \\\hline
		GRIT~\cite{ma2023graph}      & Node    &   Deg. &       & 
        \\\hline
		Graph ViT/MLP-Mixer~\cite{he2023generalization}      & Sub.    &  RW  &       & 
        \\\hline
		SGFormer~\cite{wu2023sgformer}      &  Node   &    &       & \checkmark
        \\\hline
		Graphormer-GD~\cite{zhang2023rethinking}      &  Node   &    &  \checkmark     & 
        \\\hline
        Equiformer~\cite{equiformer_v2}      &  Node   &    &  \checkmark     & 
        \\\hline
        EquiformerV2~\cite{liao2023equiformer}      &  Node   &    &  \checkmark     & 
        \\\hline
        Polynormer~\cite{deng2024polynormer}     &  Node   &  Lap.  &   \checkmark    & 
        \\\hline
		CoBFormer~\cite{xing2024less}      &  Sub.   &    &       & \checkmark
        \\\hline
		TGT~\cite{hussain2024triplet}      &  Edge   &    &   \checkmark    & 
        \\\hline
		GraphGPT~\cite{GraphGPT}      &  Edge   &    &       & \\
        \bottomrule
	\end{tabular}}
	\label{tab:architecture}
\end{table}

Vanilla Transformer is essentially a special GNN, where self-attention mechanisms across all nodes are operated in a fully-connected graph. 
Since Transformers ignore the original graph structure, the core objective of GT is to incorporate edge information into Transformer architecture. 
Based on the approaches of incorporating this structural prior, in this section, we systematically categorize existing GTs into four classes:
1) Multi-level Graph Tokenization (Section~\ref{architecture:token}). 2) Structural Positional Encoding (Section~\ref{architecture:PE}). 3) Structure-aware Attention Mechanisms (Section~\ref{architecture:attention}). 4) Model Ensemble between GNNs and Transformers (Section~\ref{architecture:Ensemble}). 
Following the categorization, \cref{tab:architecture} summarizes representative literature, and \cref{fig:framework} illustrates a conceptual overview of the architectures.
Apart from novel model architectures, practical applications of GTs necessitate high scalability or equivariance. Therefore, 
we also discuss the advanced progress in enhancing scalability (Section~\ref{architecture:Scalability}) and achieving equivariance (Section~\ref{architecture:Equivariance}) for GTs.


\subsection{Multi-level Graph Tokenization}
\label{architecture:token}



In the realm of GTs, tokenization plays a crucial role in transforming graph data into a sequential format for processing by Transformer architectures. 
Distinguished from conventional text-based Transformers where tokenization is trivial, graph data presents unique challenges due to its structural complexity. This section explores four distinct levels of tokenization in GTs from fine-grained to coarse-grained level, i.e., node-level, edge-level, hop-level, and subgraph-level tokenization.

\subsubsection{Node-level Tokenization}

Node-level tokenization is the most granular approach for tokenization~\cite{Graphormer,10.1145/2939672.2939754}, which
treats each node in the graph as an individual token. Furthermore, the node involved in attention can be selected through methods such as contrastive learning~\cite{chen2024leveraging}.
This approach is particularly effective when models focus on node-specific features or when the graph's topology is less critical than the attributes of individual nodes. 
By capturing detailed information about each node, node-level tokenization is well-suited for tasks such as node classification. 

\subsubsection{Edge-level Tokenization}

Edge-level tokenization extends the concept of tokenization to the connections between nodes~\cite{TokenGT,GraphGPT}. Here, each edge in the graph is treated as a token, making this approach ideal for tasks where the relationships or interactions between nodes are of primary interest. Edge-level tokenization can capture the dynamics of these interactions, which is essential for tasks like link prediction or understanding the flow of information across the graph. By focusing on edges, edge-level tokenization can highlight the importance of connectivity patterns in the graph.


\subsubsection{Subgraph-level Tokenization}
Subgraph-level tokenization treats the entire k-hop subgraph centered on a node as the token representation of this node~\cite{chen2022structure}, which is highly effective for capturing the contextual structure and patterns surrounding the nodes. 
This approach is suitable for tasks like graph classification or regression, as it can effectively encode substructures, clusters, or other higher-order structures within the graph.
Notably, the nodes in subgraphs corresponding to different tokens are allowed to overlap, enabling a more flexible and comprehensive representation of the graph. 


\subsubsection{Hop-level Tokenization}
Hop-level tokenization extends subgraph-level tokenization by considering the number of hops and improving scalability~\cite{NAGphormer, zhang2020graph, NAGphormer+}. Unlike subgraph-level tokenization, which is restricted to a single specific hop, hop-level tokenization constructs token sequences based on subgraphs spanning multiple hops, enabling the learning of expressive node representations.
Moreover, instead of performing quadratic-complexity self-attention across all nodes, hop-level tokenization applies self-attention to each node's subgraph sequences. This design enables mini-batch training and makes the model scalable to large graphs.


\subsection{Structural Positional Encoding}
\label{architecture:PE}

In standard Transformers, the positional encoding (PE) module indicates the position of tokens in a sequence. To extend this module to GTs, 
 it is natural to develop methods for representing the positional embeddings of nodes in a graph. Since edge-level, hop-level, and subgraph-level tokenization approaches already incorporate structural information, in this section, we assume that the PE methods are applied at the \emph{node-level}.

Existing PE methods can be categorized into absolute PE and relative PE. 
Similar to standard Transformers, absolute PE assigns a unique positional embedding to each node.
This embedding, either learned or parameter-free, is subsequently aggregated or concatenated with the original embedding of the node. 
In contrast, relative PE focuses on capturing pair-wise relationships between nodes and directly applies to  the attention matrix. Therefore, we defer the discussion on relative PE to the next section. 

The main objective of absolute PE can be formulated as utilizing a function $f$ to
extract the underlying structural information from the graph, typically from the adjacency matrix $\BA$. \cref{eq:absolute_pe:f}-\eqref{eq:absoloute_pe:g} show the usage of absolute PE:
\begin{align}
    \label{eq:absolute_pe:f}
    \BP &= f(\BA),\\
    \label{eq:absoloute_pe:g}
    \Tilde{\BH} &= g(\BH, \BP).
\end{align}
Here, the function $f$ extracts the absolute PE $\BP \in \mathbb{R}^{n\times d_p}$, where $n$ denotes the number of nodes, and $d_p$ represents the dimension of the positional embedding for each node. The function $g$ integrates absolute PE with the original node features $\BH$, either by concatenation or by employing an MLP to align the dimensions of $\BP$ and $\BH$ before summing them.

Laplacian PE leverages the eigenvectors and eigenvalues obtained from the decomposition of the Laplacian matrix as PE. The decomposition of the Laplacian can be expressed as \cref{eq:decomposition:laplacian}:
\begin{equation}
    \label{eq:decomposition:laplacian}
    \BU^{T} \mathbf{\Lambda} \BU=\BI-\BD^{-1 / 2} \BA \BD^{-1 / 2},
\end{equation}
where $\BD$ denotes the degree matrix, $\BI$ is the identity matrix, $\mathbf{\Lambda}$ and $\BU$ represents eigenvalues arranged in a diagonal matrix and eigenvectors. Since the sign of pre-computed eigenvectors is arbitrary, the Laplacian PE approaches randomly adjusts the sign of eigenvectors during the training stage. The first Laplacian PE~\cite{dwivedi2021generalization} proposes to utilize the $k$ smallest non-trivial eigenvectors of a node as its PE.
Another work SAN~\cite{kreuzer2021rethinking} introduces a learned Laplacian PE. For a given node $v_j$, SAN uses $\{\lambda_i$, $\BU_{ij}\}_{i=0}^{m}$ as input features for neural networks to learn the PE of node $v_j$, where $m$ is a hyperparameter  determining the number of eigenvectors considered.

Despite the effectiveness of Laplacian PE, 
it faces two underlying challenges:
1) Non-unique Eigendecompositions.
There are different  eigendecompositions of the same Laplacian. If a vector $\v$ is an eigenvector, then $-\v$ is also an eigenvector. There are non-unique solutions for eigenvectors with the multiplicities of eigenvalues. 2) Sensitivity to Perturbations. Minor perturbations in the graph structure can significantly affect the result of eigenvectors, leading to  considerable instability in Laplacian PE.

To address the first challenge in Laplacian PE, SignNet ~\cite{lim2023sign} introduces a sign-invariant network, $f$, which operates on eigenvectors as  \cref{eq:signnetwork}:
\begin{equation}
    f(\BU_1, \ldots, \BU_k) = \rho\left(||_{i=1}^k[\phi(\BU_i) + \phi(-\BU_i)] \right), \label{eq:signnetwork}
\end{equation}
where $\rho$ and $\phi$ are neural networks. This formulation ensures that the neural network remains invariant embeddings to the sign of the eigenvectors. 
Moreover, to tackle the occurrence of multiple eigenvector choices when there are repeated eigenvalues in the Laplacian matrix, BasisNet~\cite{lim2023sign} proposes a method to extract consistent PE from these matrices.


To address the challenge of stability in Laplacian PE, Stable and Expressive PE (SPE)~\cite{huang2024on} is introduced,
which is formulated as \cref{eq:spe}: 
\begin{multline}
    \BP(\BU,\mathbf{\Lambda})= \rho\big(\BU(\,\phi_1(\mathbf{\Lambda}))\BU^{\top}, \BU(\phi_2(\mathbf{\Lambda}))\BU^{\top}, ..., \\  \BU(\phi_m(\mathbf{\Lambda}))\BU^{\top}\;\big),
    \label{eq:spe}
\end{multline}
where the input consists of the $k$ smallest eigenvalues $\lambda$ and their corresponding eigenvectors $\BV$. Rather than implementing a strict division of eigensubspaces, SPE utilizes a weighted aggregation of eigenvectors that is contingent upon the eigenvalues to ensure stability.

Singular Value Decomposition (SVD) PE~\cite{hussain2022global} provides a broader  scope of applications compared to Laplacian PE, %
as it can handle directed and weighted graphs flexibly. 
The SVD PE is computed by \cref{eq:svd:svd}-\eqref{eq:svdpe:concate}:
\begin{align}
  \mathbf{A} &\stackrel{\mathrm{SVD}}{\approx}
  \mathbf{U}\mathbf{\Sigma}\mathbf{V}^T 
  = (\mathbf{U}\sqrt{\Sigma}) \cdot (\mathbf{V}\sqrt{\Sigma})^T
  = \hat{\mathbf{U}}\hat{\mathbf{V}}^T, \label{eq:svd:svd}\\
  \BP &= \hat{\mathbf{U}} \parallel \hat{\mathbf{V}},
   \label{eq:svdpe:concate}
\end{align}
where $\mathbf{U,V}\in \mathbb{R}^{n\times r}$ contain the $r$ left and right singular vectors as their respective columns, each associated with the highest $r$ singular values in the diagonal matrix $\mathbf{\Sigma} \in \mathbb{R}^{r \times r}$. Similar to  Laplacian PE, SVD PE involves the random sign flipping of eigenvectors during the training phase. Consequently, building on the concept of  SignNet, developing a sign-invariant SVD PE could be a potential direction for future research.

Random Walk PE (RWPE)~\cite{dwivedi2022graph} represents a PE derived from the diffusion process of a random walk. The RWPE for a node $v_i$ can be mathematically expressed in \cref{eq:rwpe} through a k-step random walk:
\begin{align}
\BP_i = \left[ \begin{array}{c} \BM_{ii}, \BM_{ii}^2, \cdots, \BM_{ii}^k \end{array} \right]\in\mathbb{R}^{k}, \label{eq:rwpe} 
\end{align}
where $\BM = \BA\BD^{-1}$ represents the random walk operator. Distinguished from Laplacian PE, RWPE does not suffer the sign ambiguity. Under the condition that each node possesses a unique $k$-hop topological neighborhood for a sufficiently large $k$, RWPE provides a distinct node representation. As a potential future study, researchers can explore the replacing of random walk diffusion with alternative graph diffusion processes to derive PE.

Graphormer~\cite{Graphormer} introduces a heuristic method that leverages node degrees for centrality encoding. Specifically, each node is assigned a learnable vector based on its degree, which is then incorporated into the node features as the input layer of \cref{eqn:degree_encoding}
\begin{align}
\label{eqn:degree_encoding}
    h_i^{(0)} = x_i + z^-_{\text{deg}^{-}(v_i)} + z^+_{\text{deg}^{+}(v_i)},
\end{align}
where $z^{-}, z^{+} \in \mathbb{R}^d$ represent learnable embedding vectors defined respectively by the indegree $\text{deg}^{-}(v_i)$ and the outdegree $\text{deg}^{+}(v_i)$, respectively. For undirected graphs, $\text{deg}^{-}(v_i)$ and $\text{deg}^{+}(v_i)$ are  simplified to $\text{deg}(v_i)$. By incorporating centrality encoding into the query and key components of the attention mechanism, Graphormer enhances the ability of attention to effectively recognize both the importance and relationships among nodes.

The degree information can also be injected as post-processing. For instance, GRIT ~\cite{ma2023graph} updates the node representation after Transformer via \cref{eq:grit}:
   \begin{align}
   \label{eq:grit}
    \x^{\text{out}'}_i :=  \x^\text{out}_i \odot \btheta_1 + \left( \log(1 + \BD_i) \cdot \x^\text{out}_i \odot \btheta_2 \right) , 
   \end{align}
where $\btheta_1, \btheta_2$ are learnable weights.
Similarly, SAT~\cite{chen2022structure} also incorporates degree information into the residue connection as \cref{eq:sat}:
\begin{align}
\label{eq:sat}
\x^{\text{out}'}_i=\x_{i}+1/\sqrt{\BD_i} \,\x^{\text{out}}_i.
\end{align}

In the following, we elaborate on structure-aware attention mechanisms, which contain the complete implementation of relative PE. 
\subsection{Structure-aware Attention Mechanisms}
\label{architecture:attention}
In Transformer blocks, the attention matrix governs the interactions between nodes, while tokenization and absolute PE augment node embeddings. 
These augmented embeddings enable Transformers to incorporate structural prior into attention mechanisms. 
In this vein, direct modification of the attention matrix is a more forthright   approach for capturing  structural inductive bias. In this section, for clarity of the paper, we present all structure-aware attention mechanisms in their single-head formulation.

Adjusting the attention matrix begins with  capturing pairwise node interactions in the graph. To this end, GT leverages the graph structure to first generate a structure matrix that encodes node connectivity patterns. This structure matrix can be integrated into the attention matrix by three ways: by attention bias, by attention mask and by edge-level tokenization. 
In the rest of this section, we elaborate on the three ways respectively.
\subsubsection{Structure Matrix as Attention Bias}
By attention bias, the structural information is incorporated into the attention mechanism by adding a bias matrix $\b \in \mathbb{R}^{n\times n}$ into the inner product of the query and key matrices. 
\cref{eqn:bias} shows a general form for computing the attention matrix:
\begin{align}
\label{eqn:bias}
    \BA=\text{Softmax}\left(\frac{(\BH\BW_{Q})(\BH\BW_{K})^T}{\sqrt{d}} + \b\right),
\end{align}
where the attention bias $\b$ is specified by different approaches, which is essentially relative PE. 
Relative PE is computed from the graph structure, aiming to understand pair-wise interactions between nodes. We define a relation matrix $\hat{\BP} \in \mathbb{R}^{R\times R}$ as the attention bias $\b$. 
$\BP_{ij}$ is determined by the function $\phi(\BH_i, \BH_j, e_{ij})$, which encodes the relationships between any pair of nodes, utilizing their embeddings $\BH_i$, $\BH_j$, and optionally incorporating edge embedding $e_{ij}$.

Graphormer~\cite{Graphormer} introduces the shortest path distance (SPD) of a shortest path $\text{SP}_{ij}=[e_1,e_2,...,e_N]$ connecting $v_i$ to $v_j$ into the attention mechanism. Graphormer incorporates two types of attention bias. The first spatial bias $\phi_(v_i, v_j)$ encodes the length of $\text{SP}_{ij}$ and the second, edge encoding $c_{ij}$, is to aggregate the edge embeddings in $\text{SP}_{ij}$. Consequently, the attention mechanism that incorporates structural information can be expressed by \cref{eqn:rnpe}:
\begin{align}
\label{eqn:rnpe}
    \BA_{ij}&=\text{Softmax}\left(\frac{(\BH_i\BW_{Q})(\BH_j\BW_{K})^T}{\sqrt{d}} + b_{\phi(v_i,v_j)} + c_{ij}\right),\\
    c_{ij} &=\frac{1}{N}\sum_{n=1}^{N} x_{e_n}(w^{E}_{n})^T, 
\end{align}
where $b_{\phi(v_i,v_j)}$ is a learnable scalar indexed by $\phi(v_i,v_j)$ and remains consistent across all layers.  $x_{e_n}$ and $w^{E}_{n}$ denote the feature of the $n$-th edge $e_n$ in $\text{SP}_{ij}$ and its corresponding weight, respectively. 

Nevertheless, Graphormer-GD~\cite{zhang2023rethinking} identifies that SPD is incapable to adequately distinguish certain perturbations in the graph structure. To address this limitation, Graphormer-GD introduces a more robust relative PE based on resistance distance (RD). %
The attention matrix with this relative PE is represented in \cref{eq:attention-GD}:
\begin{align}
\label{eq:attention-GD}
    \BA = \phi_1(\mathbf D)\odot \operatorname{Softmax}\left(\BH \BW_Q(\BH \BW_K)^\top+\mathbf \phi_2(\mathbf D)\right),
\end{align}
where $\mathbf D\in\mathbb R^{n\times n}$ represents the distance matrix with $\BD_{ij}=\{|\text{SP}_{ij}|, \text{SP}_{ij}\}$. 
Theoretical analysis demonstrates that RD-WL exhibits superior  discriminative power compared to SPD-WL, for differentiating non-isomorphic distance-regular graphs.

GRIT~\cite{ma2023graph} introduces an approach to learning relative PE by the initialization of random walk probabilities in \cref{eq:grit:initalize}:
\begin{align}
    \label{eq:grit:initalize}
    \BP_{i,j} = [\mathbf{I}, \BM, \BM^2, \dots, \BM^{K-1}]_{i,j} \in \mathbb{R}^K,
\end{align}
where $\BM = \BA\BD^{-1}$ denotes the transition probability matrix of random walk. The initialization of $\BP$, combined with MLP processing, is proven to approximate SPD. Additionally, graph-diffusion Weisfeiler-Lehman (GD-WL) with $\BP$ is strictly better than GD-WL based on SPD.

\subsubsection{Structure Matrix as Attention Mask}\label{sec:attention-mask}
An alternative approach to incorporating structure-aware attention is to perform an element-wise multiplication between attention matrix and a masking matrix, instead of treating the structure matrix as an attention bias. %
This approach can be formally expressed as \cref{eqn:mask}:
\begin{align}
\label{eqn:mask}
    \BA=\text{Softmax}\left(\frac{(\BH\BW_{Q})(\BH\BW_{K})^T}{\sqrt{d}} \odot \BM\right),
\end{align}
where $\BM \in \mathbb{R}^{n\times n}$ represents the masking matrix, which can be the adjacency matrix or another matrix encoding the graph structure. 

To integrate edge information into the mask matrix of a GT~\cite{dwivedi2021generalization}, $\BM_{ij}$ is defined based on the edge feature. If a connection exists, $\BM_{ij}=\BW_{E} \e_{ij}$, where $e_{ij}$ denotes the edge feature connecting node $v_i$ and node $v_j$, and $\BW_{E}$ represents a learnable weight. $\BM_{ij}$ will be set to $1$ if node $v_i$ and node $v_j$ are not connected. The formulation can be expressed as:
\begin{align}
\label{eqn:mask_edge}
    \BA_{ij}=\text{Softmax}\left(\frac{(\BH_i\BW_{Q})(\BH_j\BW_{K})^T}{\sqrt{d}} \odot \BM\right),
\end{align}
This attention mask operates similarly to attention bias, as both of them only change the attention values between connected nodes.



Another classical choice for masked matrix $\BM$ is adjacency matrix as shown in \cref{eqn:mask_adj}, where $\BM_{ij}=0$ if nodes $v_i$ and $v_j$ are not connected.
\begin{align}
\label{eqn:mask_adj}
    \BA=\text{Softmax}\left(\frac{(\BH\BW_{Q})(\BH\BW_{K})^T}{\sqrt{d}} \odot \BA^{G}\right).
\end{align}
By truncating attention values for disconnected nodes,  the attention is forced to focus on local neighboring nodes. Although this may reduce a GT to a GNN regarding capturing the local neighborhood information, GMT~\cite{min2022masked} and HetGT~\cite{yao2020heterogeneous} employ distinct attention masks across different heads, thereby compelling the GT to learn from different perspectives of the graph structure. 

Similar to the attention bias, relative PE can also be used as attention masks.
GraphiT~\cite{mialon2021graphit} proposes positive definite kernels on graphs as relative PE for attention masks. Specifically, GraphiT exploits the diffusion kernel and the p-step random walk kernel, where the masking matrices are shown in \cref{eq:kernel:diffusion} and \cref{eq:kernel:randomwalk} respectively.
\begin{align}
    \BM &= e^{- \beta \BL}\label{eq:kernel:diffusion}, \\
    \label{eq:kernel:randomwalk}
    \BM &= (\BI  - \gamma \BL)^{p}, 
\end{align}
where $\BL$ is the Laplacian matrix, $\beta$ and $p$ are hyperparameters. 
GraphiT demonstrates the effectiveness of these relative PEs as attention masks across various datasets.

Despite the effectiveness of classic graph diffusion, scaling it with standard attention mechanism to large graph is challenging due to the quadratic complexity regarding the number of nodes. Furthermore, as attention masks, relative PEs are not directly applicable to low-rank linear Transformers which do not explicitly construct an attention matrix. To this end, GKAT~\cite{choromanski2022block} proposes a novel Random Walks Graph-Nodes Kernel (RWGNK) with sub-quadratic complexity. RWGNK operates as  a low-rank masking directly on the query, key and value matrices in attention mechanism, 
bypassing the explicit computation of the attention matrix and thus avoiding quadratic complexity. 

To unify attention bias and attention mask, EGT~\cite{hussain2022global} designs a framework which incorporates both of them as a general formula in \cref{eqn:mask_egt}:
\begin{align}
\label{eqn:mask_egt}
    \BA=\text{Softmax}\left(\frac{(\BH\BW_{Q})(\BH\BW_{K})^T}{\sqrt{d}} + \BE_{e}\right) \odot \BG_{e},
\end{align}
where $\BG_{e}, \BE_{e} \in \mathbb{R}^{n\times n}$ are edge embeddings generated by linear transformations.

\subsubsection{Edge-level Token as Attention Input}
Edge-level attention can be exploited by two ways. The first way focuses on computing attention only by edge tokens to generate enhanced edge representations, which are subsequently fused with node embeddings using previously discussed techniques.
The second way incorporates edge and node tokens simultaneously into the attention mechanism to develop structure-aware attention. 
We briefly introduce the technical essence of the representatives by the first way, i.e., EGT~\cite{hussain2022global} and TGT~\cite{hussain2024triplet}, and those by the second way, i.e., TokenGT~\cite{TokenGT} and Edgeformers~\cite{jin2023edgeformers} as below.

EGT~\cite{hussain2022global} introduces attention bias and mask from edges to nodes. In addition, the edge features can also be updated from the attention matrix in each layer as \cref{eqn:mask_egt_edge}:
\begin{align}
\label{eqn:mask_egt_edge}
    \BE_{e}=f\left(\frac{(\BH\BW_{Q})(\BH\BW_{K})^T}{\sqrt{d}} + \BE_{e}\right),
\end{align}
where $f$ is a learnable function with feed-forward layers and layer normalization. 

TGT~\cite{hussain2024triplet} employs triplet edge interactions to further update edge embeddings. 
Given that  $\e_{ij}$, $\e_{ik}$ and $\e_{jk}$ denote the edge embeddings of the three edges $(v_i, v_j)$, $(v_i, v_k)$ and $(v_j, v_k)$ in a triangle, the query, key and value vectors are computed by linear projections on $\e_{ij}$, $\e_{ik}$ and $\e_{jk}$ respectively. Then, 
the triplet attention computes the attention matrix by \cref{eqn:tgt_at_in2} and update edge embeddings by \cref{eqn:tgt_at_in1}. 
\begin{align}
   \BA_{ijk} &= \text{Softmax}_k\left(\frac{1}{\sqrt{d}}\mathbf{q}_{ij} \cdot \mathbf{k}_{jk} + b_{ik}\right) \times \sigma_1(g_{ik}) , \label{eqn:tgt_at_in2}\\
   \mathbf{e}_{ij} &= \sigma_2\left(\sum_{k=1}^N \BA_{ijk} \mathbf{v}_{jk}\right),
   \label{eqn:tgt_at_in1}
\end{align}
where $\BA_{ijk}$ denotes the attention weight that  edge $(v_i, v_j)$ allocates to edge $(v_j, v_k)$. 
The function $\sigma_1$ and $\sigma_2$ are two MLPs,  and $b_{ik}, g_{ik}$ are two scalars derived from MLP transformations on $\e_{ik}$. 

TokenGT~\cite{TokenGT} computes the attention across all the nodes and edges by concatenating the input matrix as  $\hat{\BH} = \BH || \BE$ for \cref{eqn:edge_attention}. 
\begin{align}
\label{eqn:edge_attention}
    \BA=\text{Softmax}\left(\frac{(\hat{\BH}\BW_{Q})(\hat{\BH}\BW_{K})^T}{\sqrt{d}}\right). 
\end{align}
To additionally incorporate the structural information, in TokenGT, 
the embedding of edge node $v$ is combined with a node identifier $\P_v \in \mathbb{R}$ by the concatenation $[\BH_v, \BP_v, \BP_v, \BT^{\mathcal{V}}]$, and the embedding of each edge $(u, v) \in \mathcal{E}$ is augmented as $[\BE_{(u,v)}, \BP_u, \BP_v, \BT^{\mathcal{E}}]$.
Here, $\BT^{\mathcal{V}}$ and $\BT^{\mathcal{E}}$ are two learnable identifiers to distinguish node and edge. 
Similar to PE, node identifiers $\BP$ can be defined via PE such as Laplacian PE.

Edgeformers~\cite{jin2023edgeformers} consist of two distinct variants: Edgeformer-E and Edgeformer-N, specializing in capturing edge and node embeddings, respectively. In this framework, edges are represented as textual data comprising multiple tokens. 
Specifically, Edgeformer-E combines these edge tokens with the tokens of their associated nodes as input, processing the input by self-attention.
Unlike using the entire graph as input, Edgeformer-N analyzes the ego-graph centered on node $v$. It employs Edgeformer-E to model each edge incident to $v$, and then applies an aggregation function to generate the final node representation $\BH_v$.





\subsection{Model Ensemble between GNNs and Transformers}
\label{architecture:Ensemble}
The most straightforward approach for designing GTs involves strategic combinations of GNNs and Transformers, leveraging both local structure patterns and global contextual relationships. As illustrated in \cref{fig:framework}, these ensemble architectures can be systematically divided into four categories based on the relative positioning of the GNN and Transformer blocks:
1) Sequential GNN-to-Transformer: feed the output from a GNN input a Transformer.
2) Sequential Transformer-to-GNN: feed the output from a Transformer into a GNN.
3) Interleave GNN and Transformer blocks.
4) Parallel GNN and Transformer:  feed the graph into GNN and Transformer concurrently, and fuse the output representations into one representation.


In the first category, GTs initially process the graph by inputting it into a GNN, which can be regarded as tokenizing the graph at the subgraph level. The GNN aggregates information from local neighborhoods to refine node embeddings. Then, the augmented node embeddings are fed into a Transformer, enabling the model to learn from subgraph tokens, as discussed in Section~\ref{architecture:token}.

The second category of architectures are commonly employed when Transformer blocks have been pretrained. For instance, in the domain of protein data, Transformers~\cite{lin2023evolutionary} have demonstrated effective capabilities in capturing amino acid residue  representations. 
Protein GT frameworks typically exploit pretrained Transformers to generate an initial node represenation, followed by the refinement of GNN regarding the spatial graph structure.
A more comprehensive discussion will be provided in Section~\ref{sec:application-protein}.

Interleaving GNN and Transformer blocks, as the third category of model ensemble, is a simple yet effective architecture.
For example, Mesh Graphormer~\cite{lin2021mesh} interleaves GNN and Transformer blocks to reconstruct human poses, and GROVER~\cite{rong2020self} adopts a hybrid strategy of combining GNN and Transformer to learn molecular representations.

By paralleling GNN and Transformer, GTs can adaptively learn the importance of both local and global information.
GraphGPS~\cite{rampavsek2022recipe} utilizes the parallel architecture which combines the outputs from a MPNN and a Transformer. In addition, GraphGPS leverages MPNN to update the edge embeddings, which can be utilized to further update the PE.

SGFormer~\cite{wu2024simplifying} theoretically proves that a single-layer attention is sufficiently expressive to capture the global interactions among nodes. Accordingly, SGFormer proposes a simplified GT architecture that incorporates a single-layer self-loop linear attention mechanism alongside GCN blocks. By combining the final representations from the Transformer and GNN, SGFormer exhibits considerable scalability and competitive performance in node property prediction tasks.

CoBFormer~\cite{xing2024less} aims to address the issue of over-globalization in GTs. To this end, CobFormer parallelizes GCN blocks with the Transformer and proposes a collaborative training strategy to supplement local graph structure knowledge from GCN into the Transformer. Specifically, CobFormer incorporates an additional loss function to align the output representations of GCN and Transformer, thereby enabling mutual supervision between the two modules.


\subsection{Towards Scalability in Graph Transformer}
\label{architecture:Scalability}
Recall that the self-attention mechanism in Transformers introduces a quadratic computational complexity regarding the number of nodes. Since real-world graphs may contain millions or even billions of nodes,  Transformers often struggle to scale to large graph efficiently.
Thus, designing efficient attention mechanisms for large-scale graphs remains a significant challenge for the scalability of GTs. 

To reduce the complexity of attention mechanism to linear, one most straightforward approach is to integrating GNNs with linear Transformers. For example,  GraphGPS~\cite{rampavsek2022recipe} adopts established Transformers that utilize linear attention mechanisms, e.g.,  combining Performer~\cite{choromanski2021rethinking} and BigBird~\cite{zaheer2020big} with other GNN modules. Nonetheless, experiments on GraphGPS reveal that although linear attention mechanisms improve scalability, they tend to degrade performance.
SGFormer~\cite{wu2023sgformer}, an alternative ensemble-based GT,
introduces a linear attention mechanism with self-loop propagation.
Theoretical analysis demonstrates that a single-layer attention is sufficient to capture global interactions, enabling SGFormer to achieve scalability and competitive accuracy in node classification tasks.

CobFormer~\cite{xing2024less} presents a bi-level global attention module aimed at mitigating the over-globalization issue while simultaneously reducing model complexity. Initially, CobFormer partitions the entire graph into clusters. Subsequently, a bi-level attention mechanism operates at both the intra-cluster and inter-cluster levels, which reduces memory consumption significantly. 
Similarly, Polynormer~\cite{deng2024polynormer} introduces a linear framework by polynomial network, where each output element is represented as a polynomial function of the input features. To enable permutation equivariance and combine local and global information, it calculates local attention on neighboring nodes and global attention on the entire graph as the coefficients in the polynomial network.

A notable limitation of the structural attention mechanism, as discussed in Section \ref{architecture:attention}, is its difficult applicability to linear attention. This stems from the fact that linear attention mechanisms do not explicitly construct an attention matrix, making it challenging to incorporate structural information through attention bias or attention mask. To this end, NodeFormer~\cite{wu2022nodeformer} introduces an edge-level regularization loss as Equation~\eqref{eqn-loss-mle} that encourages attention values between connected nodes in a graph are close to 1.0.
\begin{equation}\label{eqn-loss-mle}
    \mathcal L_{e}(\BA,  \BA^{G}) = - \frac{1}{NL} \sum_{l=1}^L \sum_{(u, v)\in \mathcal E} \frac{1}{d_u} \log \BA_{uv}^{(l)},
\end{equation}
where $L$ denotes the total number of layers in NodeFormer, and $d_u$ represents the degree of node $u$. 
Since this loss function only requires computations over edges, NodeFormer efficiently manages the complexity of edge regularization at $\mathcal{O}(|\mathcal E|)$, maintaining the overall model complexity as $\mathcal{O}(|\mathcal V| + |\mathcal E|)$.

Exphormer~\cite{shirzad2023exphormer} incorporates a sparse attention mechanism to achieve linear complexity. In essence, this sparse attention mechanism combines the adjacency matrix, expander graph, and virtual node. An expander node randomly connects nodes and ensure that each node maintains an equal degree, resulting in the expander possessing a number of edges that is linear in relation to the nodes. Despite its linear complexity, the expander graph preserves the spectral approximation of a complete graph. Moreover, Exphormer achieves competitive performance compared to dense attention.

An alternative approach avoids feeding the entire graph into the Transformer. NAGphormer~\cite{NAGphormer} transforms the $k$-hop neighborhood $\mathcal{N}^{k}(v)$ into a neighborhood embedding $\mathbf{x}^k_v$ using an aggregation operator $\phi$. This aggregated embedding is then treated as a token within the Transformer, enabling the model to learn the embedding of node $v$.
By aggregating neighboring nodes before processing by Transformer, NAGphormer circumvents the need to input a large number of nodes into the Transformer. Moreover, NAGphormer+~\cite{NAGphormer+} enhances the feature of $\mathbf{x}^k_v$ by randomly masking a portion of neighbors to achieve better performance. 
In addition, VCR-Graphormer~\cite{fu2024vcrgraphormer} then employs random walk to rewire the graph by virtual nodes. By maintaining a graph with virtual nodes, VCR-Graphormer controls its complexity, keeping it linear with respect to the number of nodes while still capturing long-range dependencies.



\subsection{Geometric Graph Transformers}
\label{architecture:Equivariance}
Given the wide range of real-world scientific applications involving GTs, the study of geometric GTs is crucial for modeling 3D graph data,  such as molecular systems and protein structures.
The core design principle of these frameworks lies in ensuring the 3D invariance and/or equivariance of the model. 
This section briefly reviews start-of-the-art  equivariant GTs that have been successfully applied to 3D graphs modeling.

The most straightforward approach to learn the structural relationships is incorporating the 3D relative distance as an additional edge embedding, which remains unchanged under Euclidearn transformations.
For example, Graphormer~\cite{Graphormer} introduces spatial encoding, where an MLP is used to encode the relative distance between atoms, effectively capturing structural relationships. 
This paradigm has demonstrated its efficacy in various frameworks for learning molecular representations~\cite{zhou2023unimol}. 
Additionally, other invariant features, such as the angle between edges~\cite{yan2022periodic}, can be included to represent orientation information. These invariant features are usually encoded using kernel functions, such as the Radial Basis Function~\cite{choudhary2021atomistic}, to enhance the model's expressivity.

TorchMD-Net~\cite{tholke2022torchmd} represents another equivariant model that incorporates the interatomic distance $r_{ij}$ into its framework. The process begins by projecting $r_{ij}$ into two distinct multidimensional filters, denoted as $\BD^{K}$ and $\BD^{V}$, using the following expressions:
\begin{equation}\label{torchmd}
    \BD^{K} = \sigma_1(r_{ij}), \BD^{V} = \sigma_2(r_{ij}),
\end{equation}
where $\sigma_1$ and $\sigma_2$ are two MLPs. Subsequently, 
TorchMD-Net replaces the traditional Softmax function with the SiLU function to compute the attention matrix as shown in Equation~\eqref{eq:torchmd:attention}:
\begin{equation}
    \label{eq:torchmd:attention}
    \BA = \text{SiLU}((\BH\BW_{Q})(\BH\BW_{K})^T \BD^{K}) \cdot \phi(\d_{ij}),
\end{equation}
where $\phi$ denotes a cutoff function that assigns the value of $0$ whenever $\d_{ij}$ exceeds a predefined threshold. The final representation is then computed by:
\begin{equation}
    \BZ = \sigma_3(\BA \BV \BD^{V}),
\end{equation}
where $\sigma_3$ represents another learnable linear transformation.

For tasks such as conformation generation, where the model needs to generate atomic coordinates, Uni-Mol~\cite{zhou2023unimol} proposes a simple SE(3)-equivariant head, represented as:
\begin{align}
    \vec{\BX}_i = \vec{\BX}_i + \frac{1}{n} \sum_{j=1}^{n} (\vec{\BX}_i - \vec{\BX}_j)c_{ij}.
\end{align}
where $c_{ij}$ represents the learned relationship embedding between node $v_i$ and $v_j$. 
To improve efficiency, Uni-Mol updates the coordinates only in the final layer of the model.

GVP-Transformer~\cite{hsu2022learning} represents an encoder-decoder framework based on the Transformer architecture, designed for the task of protein inverse folding. The model is structured to intake protein structures and subsequently generate corresponding protein sequences. As an encoder, GVP-Transformer utilizes the GVP-GNN~\cite{jing2021learning}, capable of extracting features that are translation-invariant and rotation-equivariant, to effectively model protein structures. This is followed by the application of a Transformer decoder to produce valid protein sequences.

Examples of high-order steerable GTs include SE(3)-Transformer~\cite{fuchs2020se3transformers}, Equiformer~\cite{liao2023equiformer}, and EquiformerV2~\cite{equiformer_v2}. These models employ equivariant attention mechanisms utilizing higher-degree representations of steerable features~\cite{han2024survey}, which fall beyond the focus of this survey.


