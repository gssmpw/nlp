\section{Introduction}






Graph data, a non-Euclidean data structure, is commonly found in various real-world applications, including molecular data, protein interactions, and social networks. Recently, Graph Neural Networks (GNNs)~\cite{kipf2016semi, liu2024segno} demonstrate impressive capabilities in modeling such data. A representative paradigm to building GNNs is the message-passing, which iteratively aggregates the neighbours' information and updates the node embedding. 
Nonetheless, the   message-passing paradigm encounters several intrinsic limitations, such as over-smoothing~\cite{DBLP:conf/iclr/RongHXH20},\cite{chen2020measuring} and over-squashing~\cite{kreuzer2021rethinking}, making it challenging for GNNs to effectively capture the long-range dependencies within the graphs. 

In another line of research, the Transformer model~\cite{vaswani2017attention} has demonstrated remarkable performance across a range of modalities, including Natural Language Processing (NLP)~\cite{vaswani2017attention}, Computer Vision (CV)~\cite{dosovitskiy2021an}, and time-series analysis~\cite{time-series-survey}. A notable advantage of the Transformer model is its ability to effectively capture long-range dependencies, making it an ideal solution for addressing the limitations inherent in GNNs. Graph Transformers (GTs) adapt Transformer architecture to handle both node embeddings and graph structures, demonstrating superior performance compared to message-passing GNNs on a variety of tasks, including the prediction of molecular properties~\cite{rong2020self, Graphormer, tholke2022torchmd}, dynamics simulation~\cite{fuchs2020se3transformers,liao2023equiformer, equiformer_v2}, and graph generation~\cite{vignac2023digress}.

In this survey, we conduct a systematic and comprehensive review of the recent advancements in GTs, examining the developments from the perspectives of architectures, theories, and applications. 
First, from the perspective of architecture, we categorize GTs into four categories based on their ways of integrating graph structures into Transformers. 
{(1) Multi-level Graph Tokenization.} These models utilize tokenization mechanisms to represent edges, subgraphs, and hops as structure-aware tokens, enabling the attention mechanism to effectively capture and learn intrinsic topological relationships.
{(2) Structural Positional Encoding.} These models enhance positional encoding (PE), traditionally used to denote token sequence relationships, to elucidate the structural interrelations among tokens.
{(3) Structure-aware Attention Mechanisms.} 
Since the attention matrix inherently captures the learned relationships between tokens, these models modify the attention matrix using graph-derived structural information to incorporate node interrelations.
{(4) Model Ensemble between GNNs and Transformers}. In addition to the direct modification of the architectures of Transformer, another effective strategy employs message-passing GNNs to encode structural information, followed by the integration of GNNs with Transformers. We follow the taxonomy of the previous study~\cite{min2022transformer} to organize this category. 
Additionally,  we investigate two critical aspects of Graph Transformer architecture: scalability challenges in large-scale graph processing and specialized architectural designs for geometric graphs.


After reviewing the architectures of GTs, it is important to determine which architecture is more powerful in general or in a specific task, since different architectures include different inductive biases or model the graph in different granularity. 
To this end, we investigate the expressive ability of GTs. 
Specifically, we examine the studies that compare the expressivity of GTs using the Weisfeiler-Lehman Test. These theoretical insights will enhance our understanding of how each component contributes to GTs' ability to learn graph data. Additionally, we discuss the relationships between GTs and other current graph learning algorithms, which can further elucidate the strengths and weaknesses of GTs.

Furthermore, to review the GTs tailored for specific tasks, we thoroughly investigate their applications across eight types of graph data: molecules, proteins, text, social networks, traffic systems, vision, brain and material graphs.
For each graph type, we review the different learning tasks, along with the respective datasets and methodologies. Additionally, applying GTs to specific tasks might necessitate an additional pre-training strategy or optimization objective, such as diffusion. These aspects are also incorporated into our review. In  the last part of this survey, we will explore the potential future directions of GT.







This survey aims to provide a thorough analysis of GTs from multiple perspectives, including their model architectures, expressivity, and applications.  While recent reviews have mainly focused on architectural aspects~\cite{min2022transformer, muller2024attending}, our work distinguishes itself within three aspects: (1) comprehensive architectural insights, (2) systematic analysis of theoretical expressivity of GTs, (3) extensive investigation of cross-domain applications. We also explore prospective research directions with recent advance in GTs.


\textbf{Roadmap.} The rest of this paper is organized as follows. Section~\ref{sec.prelimiary} introduces the preliminaries of the survey. 
Section~\ref{sec.architectures} reviews the four categories of GTs from the perspective of architecture. 
In Section~\ref{sec.theory}, we summarize the expressive capability of GTs and connect them to graph learning methodologies. 
Section~\ref{sec.applications} reviews the applications of GTs. 
We point out the future research directions in Section~\ref{sec.future} and conclude the paper in Section~\ref{sec.conclusion}. 
