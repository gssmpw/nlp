\section{Conclusion}
\label{sec.conclusion}
In this survey, we conduct a comprehensive review of the recent advancements in Graph Transformer. Our review begins with methods for integrating structural information into the Transformer framework, covering aspects such as multi-level graph tokenization, structural positional encoding, structure-aware attention mechanisms, and model ensemble between GNNs and Transformers. We also introduce two prevalent challenges associated with Graph Transformer, scalability and equivariance. Additionally, we explore theoretical developments to uncover connections and evaluate the expressiveness of GTs. Following this, we investigate a variety of applications, detailing the datasets and architectures employed by GTs in domains such as biological, social, textual, and visual graphs.  Concluding with an exploration of prospective research avenues, this survey aims to furnish valuable insights and guidance for future investigations in the field of Graph Transformer.









