\section{Future Directions}
\label{sec.future}
The rapid advancement of GTs opens several promising avenues for future exploration. We highlight these research opportunities as follows.


\textbf{Graph Foundational Model by Graph Transformer.} Transformer has acted as a cornerstone in building foundational models across various domains, such as natural language processing and computer vision. For instance, large language models have demonstrated remarkable performance across various tasks involving language understanding and generation. 
However, the critical importance of graph-structured data representation in scientific modeling and social network analysis has led to significant interest in Graph Foundational Models~\cite{mao2024position}. Pioneering works such as GROVER~\cite{rong2020self} and DPA-2~\cite{zhang2023dpa}, which construct the atomic model grounded in GTs, pretrain on molecules and crystals using a multi-task methodology, establish a new paradigm for scientific machine learning. These developments highlight the immense potential of GTs as fundamental building blocks for constructing next-generation Graph Foundational Models across diverse application domains.




\textbf{Scaling Graph Transformer.}
Despite the remarkable success achieved in scaling Transformer, a question remains whether scaling up the GTs would similarly enhance performance. Uni-Mol2~\cite{ji2024exploring} scales the GT to billions of parameters, showcasing improvements on molecular downstream tasks. This scalability in Uni-Mol2 is feasible due to abundant molecular graph data. However, scaling GTs in domains with limited graph-structured data remains a significant obstacle. Innovative approaches like LM-design~\cite{zheng2023structure} utilizes GNNs as structural adapters for pretrained protein language models, integrating both limited structural information and abundant sequence data from existing protein datasets. Despite these advancements, the field still lacks a comprehensive framework that effectively addresses the fundamental challenges of scaling GTs in data-constrained environments.

\textbf{Cross-modal Graph Transformer.} 
Integrating a GNN with a pretrained Transformer from the language domain allows the model to generate captions for graphs. MolCA~\cite{liu2023molca}, an example of cross-modal GT, employs a molecular GNN to encode molecular representations and feeds these embeddings into a Transformer decoder pretained by languages to generate captions for the molecule. 
In the context of complex graph systems, e.g., proteins~\cite{yuan2024functional}, utilizing GTs to integrate graph structures offers an opportunity to enhance our comprehension of protein properties through pretrained language models, thereby indicating a promising direction for future research. 

\textbf{Alternative Approaches to Capture Long-range Dependencies.} Recently, the Transformer architecture has encountered increased competition, evidenced by models like Mamba~\cite{gu2023mamba}. Notably, Graph Mamba~\cite{behrouz2024graph} has demonstrated very competitive performance when compared to GTs. As the Transformer model presents limitations, including scalability and over-smoothing issues as outlined in the survey, the success of Graph Mamba suggests the possibility of another model capable of effectively capturing global interactions, particularly within graph data. A promising direction for future research involves the development of a model that might deviate from the self-attention paradigm and more efficiently capture global interactions, circumventing the issues inherent in  Transformer.










