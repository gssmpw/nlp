\tikzstyle{leaf}=[mybox,minimum height=1em,
fill=hidden-orange!40, text width=20em,  text=black,align=center,font=\tiny,
inner xsep=2pt,
inner ysep=1pt,
]

\colorlet{linecolor}{purple!60}


\begin{figure*}[h]
\centering
\resizebox{0.95\textwidth}{!}{
\begin{forest}
  for tree={
  forked edges,
  grow=east,
  draw=linecolor,
  line width=0.9pt,
  reversed=true,
  anchor=base west,
  parent anchor=east,
  child anchor=west,
  base=middle,
  font=\normalsize,
  rectangle,
  rounded corners,
  align=left,
  minimum width=2.5em,
  inner xsep=4pt,
  inner ysep=1pt,
  },
  where level=1{text width=5em}{},
  where level=2{text width=5em,font=\normalsize}{},
  where level=3{font=\normalsize, fill=pink!20}{},
  [Graph Transformers, rotate=90,edge=linecolor
        [Molecule (\textsection~\ref{sec:application-mol}),text width=7.5em,edge=linecolor
            [Property Predictioin, text width=9em,edge=linecolor
                [MAT~\cite{maziarka2020molecule}{,}
                R-MAT~\cite{maziarka2022relative}{,}
                GROVER~\cite{rong2020self}{,}
                LiGhT~\cite{li2022kpgt}{,}
                CoAtGIN~\cite{zhang2022coatgin}{,}
                Equiformer~\cite{liao2023equiformer} {,}
                EquiformerV2~\cite{equiformer_v2}{,}
                TorchMD-NET~\cite{tholke2022torchmd}{,}
                \\
                SE(3)-Transformer~\cite{fuchs2020se3transformers}{,}
                GNS-TAT~\cite{zaidi2023pretraining}{,}
                Uni-Mol~\cite{zhou2023unimol}{,}
                TGT~\cite{hussain2024triplet}{,}
                BatmanNet~\cite{wang2024batmannet}{,}
                DMP~\cite{zhu2023dual}{,}
                Coord~\cite{Coord}{,}
                MolSpectra~\cite{MolSpectra}
                ,text width=54em,edge=linecolor
                ]
            ],
            [Molecular Dynamics, text width=9em,edge=linecolor
                [SE(3)-Transformer~\cite{fuchs2020se3transformers} {,}
                Equiformer~\cite{liao2023equiformer}{,}
                TorchMD-Net~\cite{tholke2022torchmd}{,}
                DPA-2~\cite{zhang2023dpa}{,}
                ESTAG~\cite{wu2024equivariant}
                ,text width=54em,edge=linecolor
                ]
            ],
            [Molecular Generation, text width=9em,edge=linecolor
                [DiGress~\cite{vignac2023digress}{,}
                CDGS~\cite{huang2023conditional}{,}
                Uni-Mol~\cite{zhou2023unimol}{,}
                Uni-Mol+~\cite{lu2024data}{,}
                Uni-Mol2~\cite{ji2024exploring}{,}
                Mudiff~\cite{hua2024mudiff}{,}
                JODO~\cite{huang2024learning}{,}
                GTMGC~\cite{xu2024gtmgc}{,}
                GraphDiT~\cite{liu2024graph}
                ,text width=54em,edge=linecolor
                ]
            ]
        ]
        [Protein (\textsection~\ref{sec:application-protein}),text width=7.5em,edge=linecolor
            [Property Prediction, text width=9em,edge=linecolor
              [
              HEAL~\cite{gu2023hierarchical} {,}
              scMoFormer~\cite{tang2023single}{,}
              TransFun~\cite{boadu2023combining}{,}
              Stability Oracle~\cite{diaz2024stability}{,}
              ProstT5~\cite{10.1093/nargab/lqae150}{,}
              Saprot~\cite{su2024saprot}
              ,text width=54em,edge=linecolor
              ]
            ]
            [Affinity and Docking, text width=9em,edge=linecolor
              [
              GraphSite~\cite{yuan2022alphafold2}{,}
              RTMScore~\cite{shen2022boosting}{,}
              IGT~\cite{liu2022improved}{,}
              GeoT~\cite{morehead2022geometric}{,}
              GGT~\cite{chen2023gated}{,}
              GraphormerDTI~\cite{gao2024graphormerdti}{,}
              AttentionMGT-DTA~\cite{wu2024attentionmgt}{,}\\
              GTAMP-DTA~\cite{tian2024gtamp}{,}
              Graph-BERT~\cite{jha2023graph}{,}
              HGIN~\cite{zhao2023geometric}{,}
              Uni-Mol~\cite{zhou2023unimol}{,}
              GeoDock~\cite{chu2024flexible}{,}
              EBMDock~\cite{wu2024ebmdock}
              ,text width=54em,edge=linecolor
              ]
            ]
            [Protein Design, text width=9em,edge=linecolor
              [
              GVP-Transformer~\cite{hsu2022learning}{,}
              LM-Design~\cite{zheng2023structure}{,}
              ProRefiner~\cite{zhou2023prorefiner}{,}
              FAIR~\cite{zhang2023fullatom}{,}
              PocketGen~\cite{zhang2024efficient}
              ,text width=54em,edge=linecolor
              ]
            ]
        ]
        [Text (\textsection~\ref{sec:application-text}),text width=7.5em,edge=linecolor
            [Text Generation, text width=9em,edge=linecolor
                [Zhu et al.~\cite{zhu-etal-2019-modeling}{,}
                Cai et al.~\cite{cai-lam-2020-graph}{,}
                HetGT ~\cite{yao-etal-2020-heterogeneous}{,}
                ASAG~\cite{agarwal2022multi}{,}
                GraphFormer~\cite{yang2021graphformers}{,}
                KGTransformer~\cite{kgtransformer}{,}
                Relphormer~\cite{Relphormer}
                ,text width=54em,edge=linecolor
                ]
            ],
            [Text Understanding, text width=9em,edge=linecolor
                [GNN-nested Transformer~\cite{yang2021graphformers}{,}
                Edgeformer ~\cite{jin2023edgeformers}{,}
                Heterformer~\cite{jin2023heterformer}{,}
                KG-R3~\cite{retrieval-read}{,}
                GT-BEHRT~\cite{poulain2024graph}
                ,text width=54em,edge=linecolor
                ]
            ],
        ]
        [Social
        (\textsection~\ref{sec:application-social}),text width=7.5em,edge=linecolor
            [Classification, text width=9em,edge=linecolor
              [NAGphormer~\cite{NAGphormer}{,}
              Nodeformer~\cite{wu2022nodeformer}{,}
              SGFormer~\cite{wu2023sgformer}{,}
              Exphormer~\cite{shirzad2023exphormer}{,}
              VCR-Graphormer~\cite{fu2024vcrgraphormer}{,}
              CoBformer~\cite{xing2024less}
              ,text width=54em,edge=linecolor
              ]
            ],
            [Rumor Detection, text width=9em,edge=linecolor
              [StA-HiTPLAN~\cite{khoo2020interpretable}{,}
              DGTR~\cite{wei2023dgtr}
              ,text width=54em,edge=linecolor
              ]
            ],
            [Recommendation, text width=9em,edge=linecolor
              [PMGT~\cite{liu2021pre}{,}
              GMT~\cite{min2022masked}{,}              GFormer~\cite{li2023graph}{,}
              LightGT~\cite{wei2023lightgt}{,}
              TransGNN~\cite{zhang2024transgnn}{,}
              SIGFormer~\cite{chen2024sigformer}
              ,text width=54em,edge=linecolor
              ]
            ],
        ]
        [Traffic
        (\textsection~\ref{sec:application-traffic}),text width=7.5em,edge=linecolor
            [Trajectory Prediction, text width=9em,edge=linecolor
              [Social Attention~\cite{vemula2018social}{,}
              Trajectron~\cite{ivanovic2019trajectron}{,}
              TrafficPredict~\cite{ma2019trafficpredict}{,}
              STAR~\cite{yu2020spatio}{,}
              SSAGCN~\cite{lv2023ssagcn}
              ,text width=54em,edge=linecolor
              ]
            ],
            [Event Prediction, text width=9em,edge=linecolor
              [GMAN~\cite{zheng2020gman}{,}
              IGT~\cite{zhou2023inductive}{,}
              GCT-TTE~\cite{mashurov2024gct}
              ,text width=54em,edge=linecolor
              ]
            ],
        ]
        [Vision
        (\textsection~\ref{sec:application-visual}),text width=7.5em,edge=linecolor
            [3D Reconstruction, text width=9em,edge=linecolor
              [Mesh Graphormer~\cite{lin2021mesh}{,}
              GTRS~\cite{zheng2022lightweight}{,}
              SGFormer~\cite{lv2024sgformer}
              ,text width=54em,edge=linecolor
              ]
            ],
            [Image Classification, text width=9em,edge=linecolor
              [GTP~\cite{zheng2022graph}{,}
              AMIGO~\cite{nakhli2023sparse}{,}
              MulGT~\cite{zhao2023mulgt}{,}
              SpaFormer~\cite{wen2023single}
              ,text width=54em,edge=linecolor
              ]
            ],
            [Multi-modal, text width=9em,edge=linecolor
              [M-DGT~\cite{chen2022multi}{,}
              DUET~\cite{chen2022think}{,}
              Multimodal GT~\cite{he2023multimodal}
              ,text width=54em,edge=linecolor
              ]
            ],
        ]
        [Others
        (\textsection~\ref{sec:application-other}),text width=7.5em,edge=linecolor
            [Brain Network, text width=9em,edge=linecolor
              [BrainNetTF~\cite{kan2022brain}{,}
              Cai et al.~\cite{cai2022graph}{,}
              THC~\cite{dai2023transformer}{,}
              TSEN~\cite{hu2023transformer}{,}
              ALTER~\cite{yu2024longrange}{,}
              BioBGT~\cite{peng2025biologically}
              ,text width=54em,edge=linecolor
              ]
            ],
            [Crystal Material, text width=9em,edge=linecolor
              [Matformer~\cite{yan2022periodic}{,}
              CrystalFormer~\cite{wang2024conformal}{,}
              CrysGraphFormer~\cite{sun2024crysgraphformer}{,}
              DPA-2~\cite{zhang2023dpa}{,}
              MatterSim~\cite{yang2024mattersim}{,}
              OMat24~\cite{barroso2024open}{,}
              ComFormer~\cite{yan2024complete}
              ,text width=54em,edge=linecolor
              ]
            ]
        ]
    ]
\end{forest}
}
\caption{Overview of the applications of graph transformers.}
\label{fig:text-LLM-overview}
\end{figure*}



\section{Applications}
\label{sec.applications}
GTs have emerged as a versatile and powerful  architecture across a wide range of domains, showcasing remarkable capabilities in capturing intricate relationships and learning sophisticated graph representations.
This section presents a systematic review of GTs applications across diverse graphs. 
We categorize these applications by the type of graph, including molecule, protein, social network, textual network, visual network, and other specialized networks. For each category, we provide an in-depth summary on the learning tasks, datasets and methods respectively, where the tasks and representative methods are illustrated in Figure~\ref{fig:text-LLM-overview}.



\subsection{Tasks on Molecule}
\label{sec:application-mol}
Molecules can be represented as graphs where atoms are nodes, and chemical bonds are edges, using multiple dimensional encoding methods. One-dimensional (1D) encoding, like the string representations of SMILES~\cite{weininger1989smiles} and SELFIES~\cite{krenn2020self}, capture topological details of molecules. The tokenizer for these 1D strings serves as a multi-level graph tokenizer, incorporating both node-level and edge-level tokens. The 2D molecular graph naturally represents molecules by depicting atoms as nodes and bonds as edges. 
This representation enables direct interpretation of molecular connectivity and structural relationships. 
Additionally, a 3D graph can be constructed to indicate spatial relationships by connecting each atom with its k nearest neighbors.
Each dimensional approach provides unique insights into molecular structures, allowing researchers to analyze biochemical systems from complementary perspectives. 

In this section, we classify molecular downstream tasks into three categories: molecular property prediction, molecular dynamics simulation, and molecular generation. We also discuss molecular pre-training strategies, where GT is a prevailing backbone model that absorbs prior knowledge from immense unlabeled graph data.


\subsubsection{\textbf{Molecular Property Prediction}}
While vanilla Transformer has significant potential for modeling molecules as SMILES~\cite{weininger1989smiles}, it fails to encode the structural information of molecules. 
GTs address this limitation by simultaneoutly capturing local and global features in molecules, generating effective molecular embeddings to support diverse downstream tasks, e.g., molecular property prediction.

\textit{Task Definition:}
Given a 2D molecule graph $\mathcal G = (\mathcal V, \mathcal E)$, with atoms as nodes $\mathcal V$ and chemical bonds as edges $\mathcal E$, a model $\phi_{\theta}$ is established to classify or predict the target property $y$ of the molecule.
\begin{equation}
    y = \phi_{\theta}(\mathcal G)
\end{equation}
Additionally, a geometric graph $\mathcal{\vec{G}} = (\mathcal V, \mathcal E, \vec{\BX})$ contains an optional input, the 3D coordinates $\vec{\BX} \in \mathbb{R}^{n \times 3}$ of $n$ nodes, providing the 3D conformation of the $n$ atoms. 
The geometric graph $\mathcal{\vec{G}}$ can also be utilized to predict molecular properties, which generally improves the performance compared with the counterpart 2D graph $\mathcal{G}$.
\begin{equation}
    y = \phi_{\theta}(\mathcal{\vec{G}})
\end{equation}

\textit{Dataset:}
The MoleculeNet~\cite{wu2018moleculenet} benchmark includes two types of tasks, classification and regression, for evaluating molecular properties. Classification tasks include BBBP~\cite{martins2012bayesian}, SIDER~\cite{kuhn2016sider}, ClinTox~\cite{gayvert2016data}, BACE~\cite{subramanian2016computational}, Tox21~\cite{tox21} and ToxCast~\cite{richard2016toxcast}. These datasets describe molecular properties from various perspectives, such as permeability property (BBBP), the side effect of drugs (SIDER), toxicity as compounds (ClinTox, Tox21, ToxCast) and inhibitor of human (BACE).
Regression tasks contain QM7~\cite{blum2009970}, QM8~\cite{ramakrishnan2015electronic}, QM9~\cite{ramakrishnan2014quantum}, ESOL~\cite{delaney2004esol}, Lipophilicity~\cite{gaulton2012chembl} and FreeSolv~\cite{mobley2014freesolv}, evaluating the physical chemistry and quantum mechanics of molecules. Specifically, QM7, QM8 and QM9 contain computer-generated molecular properties, such as HOMO/LUMO, atomization energy, electronic spectra and excited state energy. ESOL, Lipophilicity, FreeSolv datasets record the solubility, lipophilicity and free energy of the molecules, respectively. 
OGB~\cite{hu2020open} reformulates the datasets from MoleculeNet to build ogbg-molhiv, ogbg-molpcba datasets with additional features.
PCQM-Contact, from LRGB~\cite{dwivedi2022long}, requires the model to predict whether the pairs of distant nodes will contact in 3D space. Consequently, this task emphasizes the ability of long-range modeling, which is appropriate for GTs. 


\textit{Methods:}
MAT~\cite{maziarka2020molecule} pioneers the integration of inter-atomic distances and chemical structures as biases in the self-attention. Specifically, the attention map will be element-wise summed up by distance graph and chemical graph. Building upon MAT, R-MAT ~\cite{maziarka2022relative} introduces an additional relative PE, including relative spatial distances, distances in the 2D graph and physiochemical relationships. Instead of incorporating PE into node embedding, R-MAT injects PE directly as attention bias via multiplying the PE with the calculated queries and keys in the self-attention. GROVER ~\cite{rong2020self} employs a dual GTransformer architecture to capture both node-level and edge-level features. In each GTransformer, the inputs are first fed into a tailored GNNs to extract vectors as queries, keys and values from nodes of the graph, followed by standard multi-head attention blocks. This bi-level feature extraction framework enables GROVER to not only capture structural information in molecular data but also to discern global relationships between nodes. CoAtGIN ~\cite{zhang2022coatgin} leverages both GIN~\cite{xu2018powerful} and linear Transformer in parallel to extract both local and global representations of molecules. 
To enhance the robustness of molecular property prediction on 3D molecular graphs, Equiformer~\cite{liao2023equiformer}, EquiformerV2~\cite{equiformer_v2}, TorchMD-NET~\cite{tholke2022torchmd} and SE(3)-Transformer~\cite{fuchs2020se3transformers} design E(3)-equivariant GTs.
These models have also been utilized as backbones for molecular representation pre-training~\cite{Coord,MolSpectra}.


\subsubsection{\textbf{Molecular Dynamics Simulation}}
Molecular Dynamics (MD) simulations is a fundamental methodology for illuminating chemical processes at atomic resolution.   
The objective of MD simulation is to forecast the future atomic trajectory of a molecule, based on its past coordinate transitions. 
Utilizing GTs, we can effectively learn the %
interatomic forces and quantum mechanical interactions 
among atoms, thereby enabling an accurate prediction of atomic trajectory in molecular systems.


\textit{Task Definition:}
Given an input molecule represented by a geometric graph $\mathcal{\vec{G}}$ with 3D coordinates $\vec{\BX}_{t_{\text{obs}}}$ of each atom at the observed timestamp $t_{\text{obs}}$, the model $\phi_{\theta}$ is required to predict the coordinates $\vec{\BX}_{t_{\text{target}}}$ at the target timestamp $t_{\text{target}}$.

\begin{equation}
    \vec{\BX}_{t_\text{target}} = \phi_\theta(\vec{\BX}_{t_\text{obs}}, \mathcal{\vec{G}})
\end{equation}

\textit{Dataset:}
For small molecules, MD17~\cite{chmiela2017machine} and MD22~\cite{chmiela2023accurate} comprise the trajectories of eight and seven molecules generated via MD simulation, respectively. The objective is to predict the future coordinates of each atoms given the sampled current system state in the trajectory. For more complex  dynamic systems, the AdK~\cite{seyler2017molecular} equilibrium trajectory dataset records the trajectories of apo adenylate kinase simulated with explicit water and ions.

\textit{Methods:}
The scarcity of trajectory data entails the model to be robust. 
Oriented to such scenarios, equivariant GTs are designed to preserve the inductive bias of 3D-equivariance in molecular systems.
Equiformer~\cite{liao2023equiformer} and TorchMD-Net~\cite{tholke2022torchmd}, whose architectures are discussed in \cref{architecture:Equivariance}, demonstrate their effectiveness, as evaluated on the MD17 dataset. ESTAG~\cite{wu2024equivariant} proposes to utilize Fourier Transformer to uncover latent and unobserved dynamics in the system. Furthermore, ESTAG incorporates a temporal attention mechanism to capture node-wise temporal dynamics on a global scale. Concurrently, it employs a spatial model that facilitates message passing according to the graph structure, ensuring a comprehensive characterization of both temporal evolution and spatial relationships in molecular systems. 

\subsubsection{\textbf{Molecular Generation}}
Molecule generation is a cornerstone task in contemporary drug design and discovery~\cite{Wang2025DiffusionMF}. 
This area of research can be broadly divided into 
two distinct applications. 
The first focuses on generating 2D molecular structures based on specified properties, and the second is generating 3D molecular ground-state conformation from 2D molecular structure. Traditional computational approaches for conformation prediction are often time-consuming and involve complex computational processes. In contrast, deep learning approaches 
provide an efficient way to accelerate the drug design pipeline significantly.

\textit{Task Definition:}
In molecular generation, the model is designed to generate a molecular graph $\mathcal G = (\mathcal V, \mathcal E)$, 
where $\mathcal V$ represents the set of atoms and $\mathcal E$ denotes the chemical bonds between nodes.
To obtain a more comprehensive understanding of molecular conformation, 
the task of generating the ground-state conformation is to predict the 3D coordinates $\vec{\BX} \in \mathbb{R}^{3 \times n}$ for each atom in the molecular graph $\mathcal G = (\mathcal V, \mathcal E)$.

\textit{Dataset:}
In the property-guided molecule generation task,
MOSES~\cite{polykovskiy2020molecular} and GuacaMol~\cite{brown2019guacamol} are two used datasets,  
each containing over 1 million molecules. By contrast, the Molecule3D~\cite{xu2021molecule3d} dataset includes about 4 million molecules, providing both 2D molecular graphs and their corresponding ground-state 3D geometric structures. Another notable dataset, QM9~\cite{ramakrishnan2014quantum} contains around 130,000 molecules whose 3D conformations are calculated by Density Functional Theory (DFT)~\cite{parr1979local}. Furthermore, QM9 is also applicable to tasks focused on conditional molecule generation given specific properties

\textit{Methods:}
To preserve the discrete nature of molecular structures, CDGS~\cite{huang2023conditional} introduces a GT backbone and modifies the reverse process in a diffusion model. It starts by decoding a discrete structure, which is then used as a conditional input for each subsequent step in the reverse sampling process. 
Furthermore, JODO~\cite{huang2024learning} extends CDGS by simultaneously generating the 2D chemical graph and 3D conformation of molecules.
Another approach, DiGress~\cite{vignac2023digress}, employs a discrete diffusion model with a GT backbone adapted from~\cite{dwivedi2021generalization}. DiGress develops a discrete forward process by adding or removing edges and modifying categories of edges, ensuring the sparsity of graphs.  

However, since both continuous (coordinates) and discrete (structure) information are essential for a comprehensive description of molecules, MUDiff~\cite{hua2024mudiff} introduces a 3D roto-translation equivariant GT backbone called MUformer. The backbone is specifically designed to model both 2D and 3D molecular structures, enabling  MUDiff to effectively generate both 2D and 3D molecular representations in a diffusion framework.

Uni-Mol~\cite{zhou2023unimol} develops a SE(3)-equivariant head to predict molecular conformations, which is pre-trained on large scale 3D molecular data.
GTMGC~\cite{xu2024gtmgc} designs an  encoder-decoder architecture. The encoder incorporates a 2D chemical adjacency matrix as an attention bias into the molecular structure to predict a preliminary conformation along with 3D coordinates.
The decoder then refines this prediction by leveraging both the 2D adjacency matrix and the 3D relative distances as attention bias, resulting in more accurate final conformation predictions. 

GraphDiT~\cite{liu2024graph} addresses conditional molecule generation by introducing a conditional encoder, which supports multiple conditions.
This model replaces the standard normalization layer in Transformers with adaptive layer normalization, allowing the embedding of conditions to guide the generation process. 

\subsubsection{\textbf{Molecular Pre-training}}
Pre-training Transformers have achieved  a great success in NLP or CV, where immense unlabeled data enables efficient parameter optimization and impressive downstream performance~\cite{devlin-etal-2019-bert, radford2019language}. 
Similarly, to leverage unlabeled molecular data, even with significant advancements in GT structures, it is important to design domain-specific pre-training strategies for GTs, which incorporate inductive bias into molecular data. We outline the pre-training strategies spanning multiple molecular representations: 1D molecular strings, 2D chemical graphs, and 3D geometric graphs.


\textit{Task Definition:}
Masking perturbations build a self-supervision task for pre-training, which mask the atomic types or coordinations of molecule $\mathcal{G}$ to $\mathcal{G'}$. 
The  model $\phi_\theta$ is pre-trained by optimizing a reconstruction loss as shown in Equation~\eqref{eq:app:molecule:reconstruct} to recovering the original molecular $\mathcal{G}$.
\begin{equation}
    \label{eq:app:molecule:reconstruct}
     \arg \min_{\theta} \mathcal{L}(\mathcal{G}, \phi_\theta(\mathcal{G'}))
\end{equation}

In addition to the masking strategy, contrastive learning~\cite{hadsell2006dimensionality} is another effective pre-training strategy for aligning the molecular representations of distinct modalities. Specifically, for molecules, contrastive learning that optimizes a contrastive loss as Equation~\eqref{eq:app:molecule:contrastive} can be applied to align information across different dimensions. 
\begin{equation}
    \label{eq:app:molecule:contrastive}
     \arg \min_{\phi_1, \phi_2, \phi_3} \mathcal{L}(\phi_1(\mathcal{G}), \phi_2(\mathcal{G}), \phi_3(\mathcal{G}))
\end{equation}
where $\phi_1$, $\phi_2$ and $\phi_3$ are neural  networks that extract similar representations from 1D, 2D and 3D molecular data, respectively.

\textit{Dataset:}
ZINC15~\cite{sterling2015zinc} and ChEMBL~\cite{gaulton2012chembl} are two frequently used 2D datasets containing millions of molecules, which can also be converted into SMILES format to pre-train Transformers. Uni-Mol~\cite{zhou2023unimol} normalizes and duplicates molecules from ZINC15 and ChEMBL to obtain a dataset of 19 million molecules. Furthermore, Uni-Mol uses RDKit~\cite{landrum2013rdkit} to generate nine 3D conformations for each molecule, resulting in a total of 209 million conformations. Uni-Mol2~\cite{ji2024exploring} further extends the dataset of Uni-Mol by additionally incorporating data from ZINC20~\cite{irwin2020zinc20}, thereby  encompassing 884 million molecules and 73 million scaffolds.
PCQM4Mv2~\cite{hu2021ogblsc} is another widely used quantum chemistry dataset from OGB benchmark, containing 3.4 million molecules with both 2D and 3D information. 


\textit{Methods:}
For 1D strings, Transformers~\cite{zhang2025atomas} such as MolT5~\cite{edwards-etal-2022-translation} tokenize the nodes and edges in SMILES and utilize a mask language model style to predict the masked tokens. For 2D graphs, GROVER~\cite{rong2020self} includes two levels of pre-training tasks, predicting contextual property of the give nodes and functional motifs in the graph. 
Since there may exist multiple valid atoms in masked atoms, the correct recovery may still be alleviated, which will slow down the convergence. To this end, LiGhT~\cite{li2022kpgt} introduces a knowledge node including molecular descriptors and fingerprints to assist the prediction of masked nodes.
BatmanNet~\cite{wang2024batmannet} tokenizes the atom and edge in molecular graph and mask a large portion of tokens to recover. 
Uni-Mol~\cite{zhou2023unimol} involves 3D spatial information in molecules and is pre-trained to predict masked atoms and recover 3D coordinates from the noisy perturbations.
Coord~\cite{Coord} and GNS-TAT~\cite{zaidi2023pretraining} propose to predict the noise itself from the coordinates with a Gaussian noise and prove this objective will enable the model to learn the force field in molecules.
Frad~\cite{feng2023fractional} proposes a hybrid fractional perturbation strategy, including a dihedral angle perturbation and a  coordinate perturbation, enable the model to capture the anisotropic characteristic of molecules.
Nonetheless, SliDe~\cite{ni2024sliced} proves that the above approaches fails to capture the realistic force filed due to inappropriate assumptions, and SliDe introduces an  interpretable strategy, perturbing bond lengths, angles, and torsion angles, to align with physical principles.

Beyond the masking and denoising strategies, DMP~\cite{zhu2023dual} additionally introduces a dual-view consistency loss to exploits the consistency of 1D and 2D molecular representations. 
Transformer-M~\cite{luo2023one} develops two channels to encode 2D and 3D information, respectively, and allows single or both channels are enabled. This allows the Transformer-M to be pre-trained on unpaired 2D and 3D molecules.
In contrast, MoleBLEND~\cite{yu2024multimodal} proposes to integrate 2D and 3D structures into a unified relation matrix to comprehensively capture the relations in molecules for pre-training.
Considering that the denoising strategy only learns the potential energy, MolSpectra~\cite{MolSpectra} incorporates molecular spectra to capture the energy level structures, thereby enhancing molecular pre-training.



\subsection{Tasks on Protein}
\label{sec:application-protein}
Transformers are effective for modeling the protein sequence, while keeping the representation of complicated and hierarchical structure of proteins as an open problem. While amino acids are sequentially connected via peptide bonds, which does not directly provide topological information, the graph can be constructed from the 3D spatial relationships between them.

\subsubsection{\textbf{Protein Property Prediction}}
Predicting protein properties is crucial for understanding biochemical interaction processes. 
The folded structure of a protein is intimately connected to its functionalities, which entails accurate representation for computational approaches. 
This underscores the significance of employing GTs, which can leverage the underlying spatial relationships of proteins to predict their properties with more informative features.


\textit{Task Definition:}
In the protein graph $\mathcal{\vec{G}} = (\mathcal V, \mathcal E, \vec{\BX})$, the $\mathcal V$ and $\vec{\BX}$ denote the types and 3D coordinates of amino acid, respectively, and $\mathcal E$ contains the spatial relationships between these amino acids, such as distance, dihedral angles, and planar angles. Given the protein graph $\mathcal{\vec{G}}$, the model $\phi_\theta$ is tasked with predicting the specific property $y$ of the protein:
\begin{equation}
    y = \phi_\theta(\mathcal{\vec{G}})
\end{equation}

\textit{Dataset:}
There are abundant benchmarks for assessing protein functions, including FLIP~\cite{dallago2021flip}, DeepLoc~\cite{almagro2017deeploc}, Deep-FRI~\cite{gligorijevic2021structure}, PEER~\cite{xu2022peer}, TAPE~\cite{rao2019evaluating}. These benchmarks cover various properties of protein, such as thermostability, protein localization, Enzyme Commission (EC) number, Gene Ontology (GO) term, protein-protein interaction, stability, amino acid contact, etc. 


\textit{Methods:} 
Pre-trained protein language models, such as ESM~\cite{lin2023evolutionary}, have shown a strong ability to learn representations of protein sequences. TransFun~\cite{boadu2023combining} first utilizes ESM to learn the node embedding, then puts these embeddings into EGNN~\cite{satorras2021n}. This helps capture both sequence and structure information from the protein. HEAL~\cite{gu2023hierarchical} follows a similar approach to initialize the node embedding and puts the embedding into a graph contrastive learning framework to better learn the protein function.

To directly inject spatial relations, Stability Oracle~\cite{diaz2024stability} utilizes a pairwise distance matrix as an attention bias to predict the stability of proteins with mutations.

scMoFormer~\cite{tang2023single} stands out as a multi-modal model that models each biological component, including proteins, within the cell via parallelized GNNs and Transformers. After processing, a subsequent GNN integrates the representations from all modalities.

Saprot~\cite{su2024saprot} and ProstT5~\cite{10.1093/nargab/lqae150} process 3D protein structures to build a vocabulary that incorporates both the geometric information and the amino acid residue type in proteins. This vocabulary enriches the representation learning in Saprot and enhances the protein sequence design in ProstT5 from additional structural information of proteins.

\subsubsection{\textbf{Affinity \& Docking}}
Predicting binding affinity and determining the docking positions play a pivotal role in drug discovery, 
elucidating the molecular mechanisms of drug-proteins interactions.
Computational approaches are applies to both protein-protein and protein-ligand interactions. 
Accurate in silicon predictions significantly streamline the drug development pipeline by reducing laborious experimental screening. 

\textit{Task Definition:}
Given the geometric ligand graph $\mathcal{\vec{G}}_l$ and the receptor protein graph $\mathcal{\vec{G}}_r$, the protein-ligand binding prediction task is to predict the binding affinity or pose of the ligand-receptor pair. To more effectively capture the complex relationships in these geometric graphs, the nodes are based on the atom level. Consequently, given a model $\phi_\theta$, the target can be an affinity score $s \in \mathbb{R}$ or another geometric complex graph $\mathcal{\vec{G}}_c$:
\begin{equation}
    s, \mathcal{\vec{G}}_c = \phi_\theta(\mathcal{\vec{G}}_l, \mathcal{\vec{G}}_r)
\end{equation}
Similarly, when both ligand and receptor are proteins, the proteins are considered as rigid bodies. The target is also the affinity score $s \in \mathbb{R}$ and the rotation matrix $\BR \in \mathcal{R}^{3 \times 3}$ and translation matrix $t \in \mathcal{R}^{3}$ that lead to a docking pose($\BR\vec{\BX}_l +\t, \BR\vec{\BX}_r$):
\begin{equation}
    s, \BR, \t = \phi_\theta(\mathcal{\vec{G}}_l, \mathcal{\vec{G}}_r)
\end{equation}

\textit{Dataset:} PDBbind~\cite{wang2004pdbbind} contains a total of 19,443 protein-ligand complexes. Additionally, CASF-2016~\cite{su2018comparative} consists of 285 diverse protein-ligand complexes. The Davis~\cite{davis2011comprehensive} dataset includes 30,056 interactions of 442 proteins and 68 ligands, which provides selective measurements for the kinase protein family and its associated inhibitors. The KIBA~\cite{tang2014making} dataset comprises bioactivity data of kinase inhibitors measured by the KIBA approach, including binding affinities for interactions between 467 proteins and 52,498 ligands.

In protein-protein docking, DIPS~\cite{townshend2019end} and DB5.5~\cite{vreven2015updates} are two representative datasets. DIPS contains large number of protein complex structures and DB5.5 is manually collected by human experts and is of high-quality.

\textit{Methods:} 
RTMScore~\cite{shen2022boosting}, IGT~\cite{liu2022improved},  GeoT~\cite{morehead2022geometric}, GGT~\cite{chen2023gated}, AttentionMGT-DTA~\cite{wu2024attentionmgt} and GTAMP-DTA~\cite{tian2024gtamp} utilize edge embeddings to adjust the weights in attention matrix and also employ this matrix to refine the edge embeddings iteratively. These methods applies such GT layers to both ligand and receptor proteins. 

RTMScore concatenates the node embeddings of both ligand and receptor, while IGT uses extra attention layers to integrate these embeddings for predicting their binding activity and pose. GeoT enhances its training by including a detailed edge feature set, which consists of distance, direction, orientation, and amide angle information. GTAMP-DTA enhances the integration by introducing embeddings extracted from other pretrained molecular and protein models. Similarly, AttentionMGT-DTA uses cross-attention layers to combine the learned embeddings with those obtained from ESM.

HGIN~\cite{zhao2023geometric} incorporates pairwise distance matrix at both the atom and amino acid levels, as well as pairwise residue features, utilizing these as attention biases to predict binding affinity after mutation.

Graph-BERT~\cite{jha2023graph} learns the  embedding of each node by including its K-hop neighbors, along with additional PE, into the Transformer blocks to derive the final embedding. This model is used in protein-protein interaction tasks to determine if two proteins will interact.

GraphSite~\cite{yuan2022alphafold2} utilizes distance maps in proteins as attention masks and employs structural information as PE to predict protein-DNA interactions. Additionally, GraphormerDTI~\cite{gao2024graphormerdti} adopts the Graphormer architecture, incorporating spatial and edge relative PEs as attention biases, along with centrality encoding, to predict interactions between drug and target.

In docking tasks, Uni-Mol~\cite{zhou2023unimol} and GeoDock~\cite{chu2024flexible} concatenate atom and pair representations learned in the encoder to further refine these representations and predict the final pair distances. Uni-Mol uses GT with attention bias as its encoder. Moreover, GeoDock updates edge embeddings by iteratively using them as attention biases and employs the attention matrix to further refine these edge embeddings. EBMDock~\cite{wu2024ebmdock} applies Equiformer~\cite{liao2023equiformer} as backbone, proposes a framework aimed at docking pose aligns with a minimum point in the energy landscape.


\subsubsection{\textbf{Protein Design}}
Protein inverse folding aims to determine the amino acid sequence that can fold into a given specific 3D protein structure, which is crucial for protein engineering and therapeutic development. A more challenging task is to co-design both the protein structure and the sequence when provided with the structure of a ligand.

\textit{Task Definition:}
Inverse folding provides the coordinates of each atom in amino acid residues $\vec{\BX} \in \mathcal{R}^{n \times 3 \times 4}$, and the model $\phi_\theta$ is required to predict the protein sequence $\BS$.
\begin{equation}
    \BS = \phi_\theta(\vec{\BX})
\end{equation}
In protein pocket sequence and structure co-design task, a subset of protein $\vec{\mathcal{G}}_B$ is blank, and given the rest amino acid residues $\vec{\mathcal{G}}_A$ in a protein and a ligand graph $\vec{\mathcal{G}}_L$, the model $\phi_\theta$ is required to design the blank part in protein.
\begin{equation}
    \vec{\mathcal{G}}_B = \phi_\theta(\vec{\mathcal{G}}_A, \vec{\mathcal{G}}_L)
\end{equation}

\textit{Dataset:}
CATH4.2 and CATH4.3~\cite{orengo1997cath} are two commonly used benchmarks to evaluate the recovery rate of the protein sequences.
CrossDocked~\cite{francoeur2020three} and Binding MOAD ~\cite{hu2005binding} datasets contain protein-molecule pairs to evaluate the generation of protein pockets.


\textit{Methods:}
GVP-Transformer~\cite{hsu2022learning} uses the embeddings generated by GVP-GNN~\cite{jing2021learning} and processes them through encoder-decoder Transformer blocks to design protein sequences. In addition, LM-Design~\cite{zheng2023structure} integrates a pre-trained protein language model (pLM) by using a GNN as a structural adapter for ESM, allowing for the recovery of protein sequences by combining both structure and sequence models.
ProRefiner~\cite{zhou2023prorefiner} utilizes edge embedding as attention bias in GT. It refines the designed sequence by iteratively updating predictions, focusing on amino acids that predicted with low confidence.

FAIR~\cite{zhang2023fullatom} designs a  hierarchical GT to incorporate information from both atom- and residue-level structures. At each level, FAIR uses spatial embeddings to learn the key and value vectors. Similarly to LM-Design, PocketGen~\cite{zhang2024efficient} introduces hierarchical GT by introducing structural adapter layers for pre-trained pLMs to further refine designed sequences.



\subsection{Tasks on Textual Graph}
\label{sec:application-text}
In textual graphs, nodes can represent either words or sentences. %
Given the significant achievement of vanilla Transformer in NLP, 
it is a natural idea to extend vanilla architecture to GTs. 
This extension fulfills the integration of rich structural relationships into textural graphs.

\subsubsection{\textbf{Graph-to-Text Generation}}
Abstract Meaning Representation (AMR)~\cite{banarescu2013abstract} graphs are a semantic formalism for representing the meaning of sentences, where nodes represent variables and edges represent semantic relationships, which is crucial for applications such as translation. The AMR-to-Text generation task focuses on converting these AMR graphs into sentences. Beyond AMR graphs, knowledge graphs (KGs) can also be transformed into text, making the stored semantic information more accessible to readers. Furthermore, by modeling KGs, GTs can acquire the ability to reason and respond to given queries.


\textit{Task definition:} Given a graph $G$, we utilize a graph encoder $\phi_{\theta}$, such as GNNs or GTs to learn the graph embedding $\BZ \in \mathbb{R}^{n \times d}$. Subsequently, this embedding is fed into a decoder $f$ to generate the corresponding text $\BT$.
\begin{equation}
    \BZ = \phi_{\theta}(G), \BT = f(\BZ)
\end{equation}

\textit{Datasets:} The graphs utilized for text generation can be categorized into two main classes: \textbf{1) AMR graph}. The LDC2015E86 and LDC2017T10 datasets are comprised of sentences annotated with AMRs, containing $16,833$ and $36,521$ AMRs for training, respectively. Both datasets share the same validation set of 1,368 AMRs and a test set of 1,371 AMRs. Additionally, the English-German and English-Czech datasets are from the WMT16 translation task\footnote{http://www.statmt.org/wmt16/translation-task.html.}, containing around $200,000$ AMR graphs for model evaluation. \textbf{2) Knowledge graph}. KG-to-graph benchmarks include AGENDA~\cite{koncel2019text} and WebNLG~\cite{gardent-etal-2017-webnlg} datasets. AGENDA focuses on generating scientific text based on automatically extracted entities and relations. WebNLG is a dataset comprised of crowd-sourced texts that correspond to subgraphs extracted from DBPedia~\cite{auer2007dbpedia} categories. 
Since GT can be applied to Question Answering (QA) and knowledge graph completion tasks, we highlight two and four datasets for assessing the reasoning capabilities of models for these two tasks, respectively. 
For the task of QA, we introduce two datasets: FreebaseQA~\cite{FreebaseQA} and WebQuestionsSP ~\cite{WebQuestionsSP}. FreebaseQA is a comprehensive dataset designed for open-domain QA, leveraging the Freebase knowledge graph. WebQuestionsSP dataset contains semantic parses for the questions sourced from WebQuestions that can be answered using the information from Freebase.
In the realm of knowledge graph completion, there are four datasets: WN18RR~\cite{WN18RR}, FB15K-237~\cite{FB15K237}, NELL-995~\cite{NELL} and UMLS~\cite{UMLS}. WN18RR is an English lexical knowledge graph, offering a structured representation of word relationships. FB15K-237 is a curated subset of Freebase. NELL-995 is derived from NELL, which semi-automatically constructs the knowledge from web and document. Lastly, UMLS is a biomedical knowledge graph, comprised of medical semantic entities and relations.

\textit{Methods:} In the task of generating sentences from AMR graphs, existing approaches propose encoding the relations between node pairs via shortest path embedding, such as ~\cite{zhu-etal-2019-modeling} and ~\cite{cai-lam-2020-graph}. The learned relational encoding enables the attention to capture structural information within the graph. Furthermore, HetGT ~\cite{yao-etal-2020-heterogeneous} proposes to extend the AMR graphs into multiple directed subgraphs, such as fully-connected graph, original graph, reverse graph and connected graph according to its heterogeneity. HetGT then applies attention heads to process each subgraph by masking the attention values of non-neighbor nodes. ASAG~\cite{agarwal2022multi} proposes to parse both student answers and model answers as AMR graphs, and applies a GT, which takes edge embedding as attention bias, to learn the representations of AMR graphs. Through matching the representations of the student answer and the model answer, ASAG is able to automatically score the student answer.
GraphFormer~\cite{yang2021graphformers} defines relative graph positions in knowledge graph via the shortest path, employing an MLP to model these relative positions as an attention bias, thereby enhancing the encoder's capabilities. KGTransformer~\cite{kgtransformer} introduces Mixture-of-Experts (MoE) to capture complex reasoning patterns and utilizes random walks to sample subgraphs for pre-training by  a mask-then-predict strategy. Relphormer~\cite{Relphormer} similarly adopts masked knowledge modeling in its KG training strategy, incorporating high-order information from the adjacency matrix as an attention bias.



\subsubsection{\textbf{Representation Learning for Textual Graph}}
In textual graphs, both edges and nodes contains rich textual information rather than simple numerical or categorical attributes. This presents a significant challenge in effectively capturing the intrinsic complexity of the semantic information as well as the contexts from neighboring entities.


\textit{Task definition:} For a given textual graph $G$, the primary objective of a GT $\phi_{\theta}$ is to learn the representations for nodes $\BH \in \mathbb{R}^{n \times d}$ and edges $\BE \in \mathbb{R}^{e \times d}$. These representations can be further leveraged for various downstream tasks, including link prediction, node classification and edge classification.
\begin{equation}
    \BH, \BE = \phi_{\theta}(G)
\end{equation}

\textit{Datasets:}
Textual graphs can be categorized into three types: graph with textual nodes, graph with textual edges, and graph with hybrid categorized nodes. Two representative graphs with textual nodes are DBLP \footnote{https://originalstatic.aminer.cn/misc/dblp.v12.7z} and Wikidata5M \footnote{https://deepgraphlearning.github.io/project/wikidata5m}. 
DBLP is a paper citation graph, 
where the nodes are the titles or abstracts of papers and edges are the citation relationships. 
Wikidata5M involves the entity graph from Wikipedia, with each node comprising the first sentence of an entity's introduction. Graphs with textual edges include Amazon~\cite{he2016ups}, Goodreads~\cite{wan2019fine} and StackOverflow\footnote{https://www.kaggle.com/datasets/stackoverflow/stackoverflow}. Amazon is a user-item interaction graph, incorporating reviews on items as textual edges. Goodreads is a reader-book graph, containing the comments from readers as textual edges. StackOverflow represents an expert-question graph, where the textual edges are answers or questions. Additionally, DBLP, Twitter and Goodreads data can also construct heterogeneous information graphs with both text-rich and text-free nodes. In DBLP, the paper nodes are considered as text-rich nodes while venue and author nodes are deemed as text-free nodes. The Twitter dataset contains tweet and POI nodes as text-rich, in contrast to hashtag, user, and mention nodes, which are textl-free. In the Goodreads dataset, book nodes possess abundant textual information, whereas shelves, author, format, publisher, and language code nodes are considered text-free.


\textit{Methods:}
GNN-nested Transformer~\cite{yang2021graphformers} alternates Transformer and GNN layers to process textual graphs. The Transformer is able to effectively model node embeddings, while the GNN facilitates message propagation between neighbors. For graphs with textual features in the edges, Edgeformer~\cite{jin2023edgeformers} introduces two variants, Edgeformer-E and Edgeformers-N. Edgeformer-E employs self-attention between an edge and its incident nodes to learn the embeddings of textual edges. Edgeformer-N learns the node embedding via aggregating the embeddings of textual edges, following the same way as Edgeformer-E, within each nodes' ego-graph. In more complex scenarios 
where partial nodes in a graph are associated with texetual features, 
Heterformer~\cite{jin2023heterformer} is capable of handling both text-rich and text-free nodes. Heterformer models text-free nodes via distillation from pretrained language model. It learns node embeddings using heterogeneous attention, a self-attention in the ego-graph of a node, additionally assigning different projection matrices for text-free and text-rich nodes. KG-R3~\cite{retrieval-read} proposes a retrieve-and-read framework for link prediction. Given a query consisting of subject and relation, KD-R3 employs an off-the-shelf algorithm to retrieve a candidate subgraph that related to the query to infer the object. In this vein, 
KD-R3 further designs a GT that considers both nodes and edges as tokens in self-attention layers, masking those not connected to learn effective subgraph embeddings. The final prediction on the object is performed based on a cross-attention between the query and the learned subgraph representations.

GT-BEHRT~\cite{poulain2024graph} is applied to electronic health records (EHRs) graphs to analyze patient conditions. Each patient has a sequence of EHRs, which include diagnostic information such as date, age, and visit type. GT-BEHRT incorporates the types of relations into the EHRs as attention biases when modeling the nodes. It then uses a Transformer block to integrate all EHR embeddings of a patient to predict the illness situation of the patient.

\subsection{Tasks on Social Networks}
\label{sec:application-social}
Social networks provide deep insights into human relationships and interactions, which support immense real-world applications. For instance,  by modeling the links between comments in a community, it becomes possible to accurately detect rumors. Additionally, analyzing past interactions between users and items reveals user preferences, allowing for more precise recommendations.


\subsubsection{\textbf{Citation and Wikipedia Networks}}
Citation and Wikipedia networks are extensively utilized as standard benchmarks for assessing graph learning approaches. In citation networks, nodes represent individual papers, and edges represent the citation relationships among these papers. Wikipedia networks are formed based on webpage connections. By employing these graphs, graph learning models are required to classify the nodes.

\textit{Task Definition:}
Given a graph $\mathcal{G} = \{\mathcal{V}, \mathcal{E}\}$, the objective of the model is to classify the nodes in $\mathcal{V}$.

\textit{Datasets:}
Cora, Citeseer, and PubMed~\cite{sen2008collective} are three well-known graph citation networks that capture the citation relationships of computer science and medical papers. In contrast to these traditional datasets, 
the ogbn-arxiv dataset~\cite{hu2020open} is a larger graph containing 40 distinct computer science subareas for multi-class node classificaiton. 
The Actor~\cite{actor-dataset} dataset represents actors as nodes, which are connected based on their co-appearances in Wikipedia. Similarly, the Squirrel and Chameleon datasets~\cite{rozemberczki2021multiscale} are Wikipedia-based networks that model the relationships between pages through links between them.

\textit{Methods:} 
Since these datasets represent standard benchmarks, there is no need to design specialized GTs for them. GTs that require evaluation of their performance on node classification tasks will conduct experiments with these datasets, such as NAGphormer~\cite{NAGphormer}, Nodeformer~\cite{wu2022nodeformer}, Exphormer~\cite{shirzad2023exphormer}, SGFormer~\cite{wu2023sgformer}, VCR-Graphormer~\cite{fu2024vcrgraphormer}, and Cobformer~\cite{xing2024less}.


\subsubsection{\textbf{Rumor Detection}}
Rumors typically propagate through social networks by following communities structures, where user interactions exhibit distinct relational and temporal patterns. These patterns manifest in the form of sharing cascades, reply chains and user engagement behaviors over time. GTs can effectively verify rumors by modeling and analyzing these inherent network dynamics, particularly by the examination of information diffusion pathways and user interaction structures. 

\textit{Task Definition:}
Consider a set of tweets $X = \{x_1, x_2, ..., x_n\}$, where $x_1$ represents the original tweet, $x_i$ is the $i$-th tweet in chronological sequence, and $n$ is the total number of tweets in the sequence. A graph is constructed based on the relationships between tweets denoted by $R(i, j) \in \{\text{parent, child, before, after, self}\}$. In this context, `parent' signifies that $x_i$ replies to $x_j$, `child' indicates that $x_i$ is replied to by $x_j$, `before' designates that $x_i$ is posted before $x_j$, `after' indicates that $x_i$ is posted after $x_j$, and `self' denotes that $i=j$. Using the set of tweets $X$ and their relationships $R$, the model $\phi_\theta$ is designed to predict the rumor category $y$ as true, false, or other states.

\textit{Datasets:}
Twitter15 and Twitter16~\cite{ma2018detect} contain 1,413 and 756 propagation trees  respectively, with each tree labeled as non-rumor, false-rumor, true-rumor, and unverified.
PHEME~\cite{kochkina_liakata_zubiaga_2018} includes 1,972 trees annotated as false-rumor, true-rumor, and unverified.

\textit{Methods:}
StA-HiTPLAN~\cite{khoo2020interpretable} initially employs self-attention on individual tweets to derive sentence-level embeddings. Subsequently, it designs a GT which integrates the relations of tweets as attention bias, to determine the likelihood of the thread being a rumor.
Moreover, DGTR~\cite{wei2023dgtr} applies a Transformer on the graph with centrality encoding with additional temporal Transformer blocks to predict rumors.

\subsubsection{\textbf{Recommendation Systems}}
By representing user-item interactions as graphs, GTs become highly effective in forecasting users' future behaviors based on their historical interactions.

\textit{Task Definition:} Considering $M$ users $\BU = \{u_1, u_2, ..., u_M\}$ and $N$ items $\BI = \{i_1, i_2, ..., i_N\}$, the interaction matrix $\BA \in \mathbb{R}^{M \times N}$ is defined such that $a_{m,n} = 1$ if user $u_m$ interacts with item $i_n$, and $a_{m,n} = 0$ otherwise. Consequently, the interaction graph $\mathcal G = \{\mathcal V, \mathcal E\}$ is formed, where the node set $\mathcal V = \BU \cup \BI$, and edge set $\mathcal E = \{e_{m,n}|a_{m,n}=1\}$. Given the graph $\mathcal G$ and model $\phi_\theta$, the objective is to predict the unobserved interactions: 
\begin{align}
    \hat{y}_{m,n} = \phi_\theta(\mathcal G).
\end{align}

\textit{Dataset:}
There are several standard real-word recommendation datasets: Yelp\footnote{https://www.yelp.com/dataset}, Ifashion~\cite{chen2019pog}, LastFM\footnote{http://ocelma.net/MusicRecommendationDataset/lastfm-360K.html}, Amazon-CDs~\cite{mcauley2013amateurs}, Amazon-Music~\cite{mcauley2013amateurs} and Epinions~\cite{tang2012etrust}. The Yelp dataset is used for recommending business venues and is collected from the Yelp platform. iFashion is focused on recommending fashion outfits and is maintained by Alibaba. LastFM incorporates the records in music applications and Internet radio sites. Amazon-CDs, Amazon-Music and Epinions contain users ratings on items from the Amazon and Epinions platforms.
To capture more comprehensive item features, multi-modal datasets such as Movielens\footnote{https://movielens.org/.}, Tiktok\footnote{https://www.tiktok.com/.}, and Kwai\footnote{https://www.kuaishou.com/activity/uimc.} include textual, acoustic and visual features of videos. Additionally, Tmall\footnote{ https://tianchi.aliyun.com/dataset/dataDetail?dataId=42.} dataset is for Click-Through Rate (CTR) prediction, containing shopping log of users over a six-month period.

\textit{Methods:} PMGT~\cite{liu2021pre} samples the contextual neighbors of the target node and utilizes Transformer to learn the node embedding by reconstructing the target nodes and the graph structure given the neighbors. GMT~\cite{min2022masked} introduces four types of interaction graphs to model neighborhood relations for the CTR prediction task, which are the induced subgraph, similarity graph, cross-neighbourhood graph, and complete graph. GMT then implements masked attention to enforce model to capture the local feature. GFormer~\cite{li2023graph} begins with GNN layers to extract position-aware information from all sampled anchor nodes. GFormer then samples edges from attention map to be processed by GAT. LightGT~\cite{wei2023lightgt} addresses scenarios where recommended items have multi-modal features by initially employing Transformer layers to integrate user and multi-modal features. LightGT parallelizes GCN and self-attention layers on these multi-modal features for joint prediction. Furthermore, to incorporate the structural prior of user-item interaction into self-attention, LightGT takes the node embeddings learned from a GCN as positional embedding in self-attention. SIGformer~\cite{chen2024sigformer} introduces two types of attention biases for GT, i.e., sign-aware spectral encoding and sign-aware path encoding. These are designed to distinguish between positive and negative feedback on items from users by integrating the Laplacian matrices of both positive and negative graphs, and the sign-aware paths connecting users and items.
TransGNN~\cite{zhang2024transgnn} introduces an approach that utilizes attention mechanism to identify and select relevant nodes for constructing a subgraph. This subgraph is then modeled by the integration of GNN and Transformer layers.




\subsection{Tasks on Traffic Networks}
\label{sec:application-traffic}
In traffic networks, attention mechanisms have demonstrated remarkable effectiveness in modeling both temporal dependencies and structural relationships. The integration of GNNs with attention mechanisms enables comprehensive capture of spatio-temporal patterns, enhancing the ability to model complex traffic dynamics.

\subsubsection{\textbf{Trajectory Prediction}}
The predictions of pedestrian and traffic trajectories pose significant challenges due to the intricate interplay of individuals and temporal dependencies. GTs offer a robust framework for modeling these trajectories by simultaneously capturing spatial relationships and temporal evolution patterns. 

\textit{Task Definition:}
The traffic graph is mostly based on DTDG. Given historical trajectories observed from time step $1$ to $T_{obs}$, the aim is to predict future trajectories spanning from $T_{obs} + 1$ to $T_{end}$. At each time step $t$, there are $N$ objects denoted as $\{p_{t}^{i}\}_{i=1}^{N}$, where $p_{t}^{i} = (x_{t}^{i}, y_{t}^{i})$ represents the 2D coordinates of the $i$-th object at time $t$. Therefore, the prediction task can be formulated as developing a model $\phi_\theta$ that maps historical observations to future trajectories: $\phi_\theta(\{p_{t}^{i}\}_{i=1}^{T_{obs}}) = \{p_{t}^{i}\}_{i=T_{obs}+1}^{T_{end}}$. 
The graph structure is constructed by representing each object as a node and establishing edges between objects within a prescribed distance threshold.


\textit{Datasets:} The ETH~\cite{pellegrini2009you} dataset comprises two scenes, ETH and HOTEL, recording the movements of individuals numbering between 28 and 60. Similarly, the UCY~\cite{lerner2007crowds} dataset encompasses three scenes, i.e., UNIV, ZARA1, and ZARA2, with individuals ranging from 30 to 94. 
Both datasets extract pedestrian trajectories through video analysis, with the coordinate of pedestrians. 
Beyond pedestrian-focused datasets, NGSIM~\cite{simulation2007us} and KITTI~\cite{geiger2013vision} datasets offer multiple types of object, such as bicycles and vehicles, enabling evaluation of motion prediction models.

\textit{Methods:}
Social Attention~\cite{vemula2018social} constructs spatial edges $\e_{ij}^{t}$ and temporal edges $\e_{ii}^{t}$. The spatial edge $\e_{ij}^{t}$ represents the interaction feature between node $\v_i$ and node $\v_j$ at time $t$, while the temporal edge $\e_{ii}^{t}$ denotes the relationship of node $\v_i$ between two consecutive time steps, $t$ and $t-1$. Social Attention employs attention mechanisms between $\e_{ii}^{t}$ and $\e_{ij}^{t}$, where $j \in \mathcal{N}_i$, to update the node embedding from both temporal and spatial relations.
Trajectron~\cite{ivanovic2019trajectron} and TrafficPredict~\cite{ma2019trafficpredict} follow Social Attention, applying attention on edges to capture the spatial relations in the graph.

STAR~\cite{yu2020spatio} introduces specialized modules for both spatial and temporal modeling. The spatial Transformer integrates GAT and feed-forward layers into the Transformer architecture. Meanwhile, the temporal Transformer focuses on modeling the trajectory of each pedestrian by attention mechanism.
By combining the embeddings generated from both the spatial and temporal Transformers, STAR can accurately forecasts future trajectories.


\subsubsection{\textbf{Event Prediction}}
In contrast to trajectory prediction, 
event prediction in traffic contexts deals with comprehensive prediction problems, including predicting congestion, traffic volume, vehicle speed and arrival time, etc. These predictions play a crucial role in traffic management and urban planning.
GTs have manifested remarkable capabilities of capturing both long-range temporal and spatial dependencies within the traffic graph.

\textit{Task Definition:}
Given $N$ historical information $\BX = \{x_i|i=1, 2, \ldots, N\}$ and the corresponding target $\BY = \{y_i|i=1, 2, \ldots, N\}$, such as delivery times, the model is designed to forecast the future events $\Bar{\BY} = \{\Bar{y}_i|N+1, N+2 \ldots\}$.

\textbf{Datasets: } The Xiamen dataset~\cite{wang2017unlicensed} contains 5 months of data recorded by 95 traffic sensors in Xiamen, China. The PeMS dataset~\cite{li2018diffusion} includes 6 months of data collected by 325 traffic sensors in the Bay Area. The Abakan and Omsk datasets~\cite{porvatov2022hybrid} consist of a road network along with associated routes and their corresponding travel times.



\textit{Methods:} Given a sequence of historical order, including the retailer, the origin location, the destination, and the payment time, DProQ~\cite{zhou2023inductive} initially constructs bipartite subgraphs according to the types of nodes. Subsequently, DProQ introduces a heterogeneous GCN to separately model these subgraphs and update the node embeddings by aggregating the outcomes from each subgraph. Finally, a Transformer block is employed to combine the node embeddings from various types to forecast the ultimate delivery time.

GMAN~\cite{zheng2020gman} parallelizes spatial and temporal attention with an additional global attention mechanism to capture both spatial and temporal information in the traffic graph. This approach is used to predict traffic volume and speed.

GCT-TTE~\cite{mashurov2024gct} concatenates embeddings learned from GCN and a Transformer to capture both local and global information. Additionally, it employs a Transformer block to combine these embeddings, which is used to estimate travel time.


\subsection{Tasks on Visual Graph}
\label{sec:application-visual}
Transformer~\cite{dosovitskiy2021an} has emerged as a prevailing approach in the area of computer vision. Nevertheless, graph-structured data has garnered relatively less focus in this domain. In this section, we categorize visual tasks into those involving 2D and 3D data. It is worth noting that 3D data inherently possesses spatial relationships, whereas 2D data can acquire graph structures through segmentation or incorporating other modalities.
\subsubsection{\textbf{3D Data Processing}}
GTs have been utilized in two types of 3D data: meshes for human pose estimation and point clouds, serving both reasoning and generation purposes. Due to their inherent spatial relationships, these 3D objects exhibit a graph-like structure, enabling GTs to effectively model both local and global interactions.

\textit{Task definition:} In the context of mesh construction, given a 2D pose image $\BX \in \mathbb{R}^{N \times N' \times 3}$, where $N$ and $N'$ denote the dimensions of the image, GTs aim to predict the 3D coordinates of body joints $\BJ_{3D} \in \mathbb{R}^{K \times 3}$ and mesh vertices $\BV_{3D} \in \mathbb{R}^{M \times 3}$. Here, $K$ and $M$ correspond to the number of joints and vertices, respectively. For the 3D scene reconstruction task using point clouds, the input is a point cloud $\BP \in \mathbb{R}^{P \times C_{\text{point}}}$, where $P$ and $C_{\text{point}}$ represent the number of points and the channels per point. The model is capable of capturing the relationships in the point cloud and generating a scene graph $\mathcal G = (\mathcal V, \mathcal E)$.

\textit{Datasets:} We present the datasets in accordance with the methodology of modeling 3D data as a mesh and a point cloud respectively. \textbf{1) Mesh:} The datasets include Human3.6M~\cite{human36m}, 3DPW~\cite{vonMarcard2018}, and FreiHAND~\cite{zimmermann2019freihand}. The original two datasets focus on human motion. Human3.6M contains 3.6 million video frames with pseudo-ground truth mesh annotations, while 3DPW is an in-the-wild dataset comprising 51,000 video frames with ground truth 3D poses and mesh annotations. The subsequent  dataset, FreiHAND, consists of 33,000 frames with 3D hand poses and shape annotations. \textbf{2) Point Cloud:} The 3DSSG~\cite{Wald2020} dataset provides 3D point clouds along with annotated semantic scene graphs that include objects and their relationships.

\textit{Methods:} Mesh Graphormer~\cite{lin2021mesh} first leverages a pretrained CNN to derive image grid features, which is then fed into Transformer and GNN blocks, along with a template mesh, to capture both local and global information.
GTRS~\cite{zheng2022lightweight} makes use of a standard 2D pose detector to extract human poses, which are subsequently provided to GNN and Transformer layers in parallel to learn a 3D embedding. SGFormer~\cite{lv2024sgformer} incorporates edge-aware self-attention, such as the dot product between the attention matrix and edge embedding, to update both node and edge embeddings simultaneously, thereby facilitating direct predictions of objects and their relationships.
\subsubsection{\textbf{Representation Learning for Cellar Images}}

Giga-pixel images pose significant computational challenges for Transformers. Whole slide images, which are exemplary giga-pixel images, exhibit cell-graph structures. Consequently, it is a natural idea to create a graph within the image patches by utilizing the cell information contained in the image.


\textit{Task Definition:} In giga-pixel images, the images are divided into smaller patches. Patches that do not predominantly contain cells are discarded. 
For the remaining patches, to construct a graph, each path is a node and pairs of patches are connected as an edge.
This graph construction method is shared in following studies where GTs are employed to learn image-level embeddings for various downstream tasks, including survival prediction and prognosis analysis.

\textit{Datasets:} Whole slide images (WSIs) are utilized in pathology to assess cancer grade. The CPTAC~\cite{edwards2015cptac} dataset comprises 2,071 WSIs from 435 patients, and the TCGA~\cite{tomczak2015review} dataset includes 2,082 WSIs from 996 patients. For multi-modal WSIs, the InUIT~\cite{nakhli2023sparse} dataset contains 1,600 WSIs from 188 patients featuring the Ki67, CD8, and CD20 biomarkers. Each biomarker is treated as a distinct modality for the purpose of learning image representations. Similarly, the MIBC~\cite{mi2021predictive} dataset consists of 585 WSIs from 58 patients, incorporating the Ki67, CK20, and P16 biomarkers.



\textit{Methods:} Given that Vision Transformers are hindered by its quadratic complexity when handling thousands of patches, GTP~\cite{zheng2022graph} initially leverages GCN and pooling layers to aggregate local information and reduce the number of patches. Furthermore, AMIGO~\cite{nakhli2023sparse} constructs multiple graphs from histopathology images obtained via distinct biomarkers. AMIGO then employs a shared GNN to capture diverse histopathological features and integrate this information using Transformer layers.
MulGT~\cite{zhao2023mulgt}, serving as a multi-task learning framework, proposes a task-specific Transformer dedicated to each task subsequent to GCN layers. SpaFormer~\cite{wen2023single} delineates the cellar image to form a graph and incorporates the node embedding into a Transformer with random walk and Laplacian PE.

\subsubsection{\textbf{Visual language Tasks}}
Cross-attention layers serves as crucial mechanisms for fusing features across vision and language modalities.
When either modality exhibits a graph structures, incorporating a graph aware cross-attention mechanism can significantly enhance model performance.

\textit{Task Definition: }
Visual grounding is the process of localizing specific regions in an image that correspond to given natural language descriptions. In a related domain, vision-and-language navigation represents a complex task where an agent should interpret textual instructions to navigate through an environment towards a specific destination. 

\textit{Datasets: }
\textbf{1) Visual Grounding.} RefCOCO~\cite{yu2016modeling}, RefCOCO+~\cite{yu2016modeling}, and RefCOCOg~\cite{mao2016generation} provide extensive annotations %
linking natural language descriptions to visual bounding boxes.
In RefCOCO, one expert describes an object while other experts identify its corresponding region.  
RefCOCO+ builds upon RefCOCO by eliminating location words from  textual descriptions. RefCOCOg is developed in a non-interactive setting where experts directly identify the object in an image based on textual descriptions.
\textbf{2) Vision-and-Language Navigation.} Datasets like REVERIE~\cite{qi2020reverie}, SOON~\cite{zhu2021soon}, and R2R~\cite{anderson2018vision} support the research on this domain. REVERIE and SOON provide textual instructions that describe target locations and objects, while R2R offers detailed step-by-step navigation  guidance. 
These datasets challenge agents to precisely identify specific objects' bounding boxes of locations.

\textit{Methods: }
M-DGT~\cite{chen2022multi} is designed for visual grounding by constructing a graph where image patches initialize ndoes and edges. 
M-DGT progressively refines the bounding box to approximate the ground truth. To incorporate information from the textual query, M-DGT treats the neighbors of a node as response. The edge embeddings are derived from the relative spatial relations between nodes. These aggregated edge embeddings and node embeddings are then concatenated to interactively refine the bounding box coordinates.

DUET~\cite{chen2022think} constructs the graph by utilizing the historically visited nodes. When the agent encounters an unobserved object, it will be integrated into the graph and linked to the current node. To facilitate language navigation, the textual instructions are combined with node embeddings through cross-attention layers, and then the fused embeddings are processed by self-attention mechanisms with spatial distance bias to incorporate topological information. Based on the learned embeddings, the agent is able to determine the target node for navigation.


\subsection{Tasks on Other Domains}
\label{sec:application-other}
In this section, we will review applications of GTs in brain network and material.

\subsubsection{\textbf{Brain Network Analysis}}
Brain networks are constructed from functional Magnetic Resonance Imaging (fMRI) data, where Regions of Interest (ROIs) represent specific brain areas. 
In graph represnetation, ROIs serve as nodes, with edges defined by pairwise correlations between the blood-oxygen-level-dependent (BOLD) signal sequences. 
This graph representation of fMRI data enables GTs to predict various characteristics of the brain subject, including sex and disease presence.

\textit{Task Definition:}
For a brain network $\mathcal{G}$, the node set $\mathcal{V}$ is composed of $n$ ROIs, and edges $\mathcal{E}$ are established through BOLD signal correlations between ROIs, forming a weighted adjacency matrix $\BA \in \mathbb{R}^{n \times n}$. 
The model $\phi_\theta$ aims to predict a specific property $y$ of the brain subject.
\begin{equation}
    y = \phi_\theta(\mathcal{G})
\end{equation}


\textit{Datasets:} The ABIDE dataset~\cite{craddock2013neuro} consists of brain networks from 1,009 participants, primarily aims at diagnosing Autism Spectrum Disorder (ASD). Among these, 516 participants are diagnosed as ASD. The ABCD dataset~\cite{casey2018adolescent} includes brain networks from 7,091 individuals, with 3,961 from female subjects, focusing on predicting the biological sex of participants.
REST-meta-MDD~\cite{yan2019reduced} features 2,026 brain networks from both individuals with Major Depressive Disorders (MDD) and healthy controls. The ANDI dataset~\cite{jack2008alzheimer} provides 54 Alzheimer's disease (AD) samples and 76 normal control samples. The UK Biobank (UKB) dataset~\cite{sudlow2015uk} comprises data from 16,458 participants across various ages.



\textit{Method:}
BrainNetTF~\cite{kan2022brain} uses the weighted adjacency matrix as the input for Transformer to derive the attention-enhanced node features.
These features are then processed by a clustering-based readout function to derive the graph embedding. Cai et al.~\cite{cai2022graph} combines GNN and Transformer blocks to capture both local and global information for brain network classification. THC~\cite{dai2023transformer} focuses on identifying functional modules in the brain, learned from the attention matrix of $\BX$ using a clustering module. It employs GNNs for local feature extraction, which are then fed into a Transformer to learn graph-level embeddings. ALTER~\cite{yu2024longrange} captures long-term edge relationships via random walk based methods and uses the generated embeddings as attention biases in a Transformer. BioBGT~\cite{peng2025biologically} introduces functional module-aware self-attention, which multiplies attention matrix with functional feature in brain. This approach includes functional segregation and integration characteristics in brain graphs.

\subsubsection{\textbf{Crystal Material Property Prediction}}
Unlike molecular graphs, crystal material graphs are periodic, %
with unit cells repeating in an infinite 3D space. 
Therefore, models need to learn graph representations that capture these periodic patterns.

\textit{Task Definition:}
Similar to molecular graphs, the unit cell of a crystal material comprises features $\BH$ and  coordinates $\vec{\BX}$. Additionally, the unit cell encompasses a lattice matrix $\BL = [\bl_1, \bl_2, \bl_3] \in \mathbb{R}^{3 \times 3}$ to denote the directions of periodicity. Specifically, given a crystal structure $\vec{\mathcal{G}}$, the infinite crystal structure can be represented as:
\begin{align}
    \hat{\vec{\BX}} = \{ \hat{\vec{\BX}}_i| \hat{\vec{\BX}}_i = \vec{\BX}_i + k_{1}\bl_{1} + k_{2}\bl_{2} + k_{3}\bl_{3}, k_{1}, k_{2}, k_{3} \in \mathcal{Z} \}
\end{align}
Given the above conditions, a model $\phi_\theta$ is designed to predict a target property $y$.

\begin{equation}
    y = \phi_\theta(\vec{\mathcal{G}}, \BL)
\end{equation}

\textit{Datasets:}
The Materials Project~\cite{jain2013materials} contains crystals with corresponding formation energy and band gap of crystal materials. 
Jarvis~\cite{choudhary2020joint} dataset includes five properties of crystal materials, i.e., formation energy, bandgap(OPT), bandgap(MBJ), total energy, and Ehull.
MatBench~\cite{dunn2020benchmarking} contains 132k crystals and 636 2D crystals for evaluation.
The OMat24 dataset~\cite{barroso2024open} comprises 118 million structures, labeled with energy, forces, and cell stress. Most of these structures contain no more than 20 atoms.


\textit{Method:}
Matformer~\cite{yan2022periodic} constructs a fully connected graph where the edge features are derived from the Euclidean distance between nodes. It then integrates the edge embeddings directly into the key and value vectors, serving as an additional attention head.
CrystalFormer~\cite{wang2024conformal}  maintains the strong periodic invariance and angular information in crystals, 
using an alternative strategy for graph construction. 
For model architecture, similar to Matformer, CrystalFormer incorporates edge embeddings into the query and key vectors.
CrysGraphFormer~\cite{sun2024crysgraphformer} uses edge embedding with spatial relations as attention mask in GT. 
Moreover, ComFormer~\cite{yan2024complete} constructs both SE(3) invariant and SO(3) equivariant Transformer layers to capture the complete geometric information of crystals.

MatterSim~\cite{yang2024mattersim} and OMat24~\cite{barroso2024open} leverage  Graphormer~\cite{Graphormer} and EquiformerV2~\cite{equiformer_v2} as their backbones, respectively. MatterSim is trained across a range of elements, temperatures, and pressures, leading to improved prediction generalization. In contrast, OMat24 focuses on material discovery and has developed a large-scale dataset containing 110 million materials. Therefore, OMat24 demonstrates superior performance in material discovery by pretraining.

