


\section{Theories}
\label{sec.theory}
Beyond the practical effectiveness of GTs
it is essential to understand the theoretical foundations underlying GTs. 
This section begins by reviewing the different expressive capabilities among existing GTs (Section~\ref{theory:expressiveity}). Subsequently, we investigate the interconnections between GTs and other graph learning methodologies (Section~\ref{theory:relationship}).

\subsection{Expressivity}
\label{theory:expressiveity}
Following the order in Section~\ref{sec.architectures}, we here respectively discuss the expressivity in structural tokenization, and comparing absolute PE with relative PE.

\subsubsection{Structural Tokenization}
In node-level tokenization, each node is treated as an independent token, allowing the model to capture local neighborhood information. However, it may struggle to capture global graph structural patterns that span multiple nodes, potentially being insufficient for expressing graph properties that require global information. Therefore, it is necessary to enhance structural bias with additional positional embeddings~\cite{zhang2023rethinking}. Edge-level tokenization can capture the connectivity between nodes, facilitating models to comprehend the interactions between nodes and the topology of the graph. Subgraph-level and hop-level tokenization encode local subgraph patterns to tokens, such as graph motifs and $k$-hop neighbors. 
These tokenizations allow models to capture more complex and global graph features, enhancing the representations on communities structures and long-range dependencies.

The expressive power of GTs is intricately related to the process of tokenization. TokenGT \cite{TokenGT} harnesses this power by effectively encoding graphs as sets of input tokens. As a pure Transformer, TokenGT utilizes \(n + m\) tokens for each graph, where \(n\) represents the number of nodes and \(m\) represents the number of edges. This approach achieves 2-Weisfeilerâ€“Leman (WL) expressivity, which has been proven equivalent to 1-WL expressivity \cite{Morris2021WeisfeilerAL}.

A formal theoretical framework~\cite{expressive-token}
establishes a connection between various tokenization methods and the \(k\)-WL test. To align GTs with the \(k\)-WL test, the authors propose providing suitable input tokens \(\mathbf{X}^{(0,k)}\) to the Transformer for each \(k \geq 1\). They demonstrate that the \(t\)-th layer of the Transformer can emulate the \(t\)-th iteration of a \(k\)-order WL algorithm. In this context, \(\mathbf{X}^{(0,k)} \in \mathbb{R}^{n^k \times d}\) denotes the initial token embeddings of k-tuples, where \(n^k\) represents the number of these embeddings.

In general, different levels of tokenization affect the expressive power of GTs. Node-level tokenization is suitable for capturing local features, edge-level tokenization is appropriate for understanding relationships between nodes, while subgraph- and hop-level tokenization provide a deeper understanding of the global structure of the graph.

\subsubsection{Positional Encoding}
A recent literature~\cite{li2024what} highlights the importance of a theoretical comparison on various PE strategies. 
Predominantly, there are two types of PEs based on either kernel or Laplacian graphs. To conduct a theoretical analysis of these PEs, methods like WL test, along with SPD-WL, GD-WL~\cite{zhang2023rethinking}, and RPE-augWL~\cite{black2024comparing}, are employed to assess and compare their expressivity.
To analyze the expressive power of absolute PE and relative PE, Black et al.~\cite{black2024comparing} proposed a framework that leverages 2-equivariant graph network (2-EGN)~\cite{maron2018invariant} to convert between relative PE and absolute PE. For graph without node features, the paper demonstrates that the distinguishing capabilities of absolute PE and relative PE are equivalent.
In contrast, for graph with node features, converting relative PE to absolute PE will undermine the distinguishing capability of GTs.


In addition, while the expressivity of absolute PEs has been discussed in Section~\ref{architecture:PE} and the works for resistance distance have been compared in Section~\ref{architecture:attention}, it has been proven that exploiting the power of matrix, such as the relative random walk positional encoding (RRWP) using the adjacency matrix achieves at least comparable expressivity to spectral kernel when employed as relative PE~\cite{black2024comparing}.


\subsection{Relationship with Other Graph Learning Methods}
\label{theory:relationship}
The characteristics of GTs can be elucidated through comparative study with other graph learning methods. In this section, we examine studies that compare GT with MPNN, graph structure learning, and graph attention network.

\subsubsection{MPNN}
Compared with MPNNs, GTs integrate self-attention mechanisms and PE. A recent study~\cite{li2024what} demonstrates that self-attention mechanism improves the convergence rate of GTs, while PE facilitates identifying the core neighborhood for each node, thereby enhancing the generalization ability. Notably, GTs with shortest-path distance~\cite{black2024comparing} as relative PE possess theoretically superior expressivity than classical MPNNs. 

An alternative approach to infuse global information into each node is to introduce a virtual node connected to all nodes in a graph. 
Despite the simplicity of this idea, MPNN with the virtual node~\cite{cai2023connection} surprisingly serves as a strong baseline in Long Range Graph Benchmark~\cite{dwivedi2022long}. A recent study~\cite{rosenbluth2024distinguished} reveals that no single algorithm can fully surpass the others between GTs and MPNNs with virtual node. 

In addition, the over-smoothing problem~\cite{chen2020measuring}, characterized from deep MPNNs, also exist in Transformers~\cite{shi2022revisiting}, which will result in indistinguishable node embeddings in deep layers.
As Transformer is a special form of Graph Attention Networks (GAT)~\cite{velivckovic2017graph}, it shares the same over-smoothing phenomenon as GAT, 
leading to an exponential degeneration of expressive power regarding the number of layers. 
To mitigate over-smoothing, SignGT~\cite{chen2023signgtsignedattentionbasedgraph} proposes a signed attention mechanism to preserve the  diverse frequency information in graph structure from the perspective of graph signal processing.



\subsubsection{Graph Structural Learning}
Graph Structure Learning (GSL) is closely related to GTs, which aims at automatically 
refining graph structures when the input graph is noisy or incomplete, or inferring implicit graph structures when explicit graph structure is unavailable~\cite{GSLB}, in a parameterized way.
Building on this foundation, GSL has been widely applied in various domains, such as molecular context graphs~\cite{PAR,Pin-Tuning}, spatiotemporal graphs~\cite{BiGSL}, and social networks~\cite{VIB-GSL}.
GTs can be regarded as a special form of GSL, achieved by self-attention that learns a fully connected `soft' graph structure~\cite{mp-all-the-way-up}. 
By utilizing attention-oriented techniques, such as the attention mask in \cref{sec:attention-mask} and discrete structure sampling in NodeFormer~\cite{wu2022nodeformer}, the learned graph structure can be sparsified to reflect real-world topology.




