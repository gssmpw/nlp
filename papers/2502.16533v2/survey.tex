
\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{array}

\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{tabularx}
\usepackage{tikz_commands}

\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{balance}

\usepackage{xcolor}
\usepackage{etoolbox}
\newcommand{\annotator}[2]{\csdef{#1}##1{{\color{#2} [\textbf{\MakeUppercase #1}: ##1]}}}
\annotator{leon}{blue}
\newcommand{\kfadd}[1]{\textcolor{red}{#1}}
\annotator{chaohao}{green}
\annotator{kangfei}{pink}


\input{defs}
\begin{document}
\title{A Survey of Graph Transformers: \\Architectures, Theories and Applications}
\author{Chaohao Yuan, Kangfei Zhao, Ercan Engin Kuruoglu, Liang Wang, Tingyang Xu, \\Wenbing Huang, Deli Zhao, Hong Cheng, Yu Rong
	\IEEEcompsocitemizethanks{
		\IEEEcompsocthanksitem C.\ Yuan and E. E. Kuruoglu are with Tsinghua Shenzhen International Graduate School, Tsinghua University. (e-mail: yuanch22@mails.tsinghua.edu.cn; kuruoglu@sz.tsinghua.edu.cn)
		\IEEEcompsocthanksitem K.\ Zhao and H. Cheng are with Chinese University of Hong Kong. (e-mail: zkf1105@gmail.com; hcheng@se.cuhk.edu.hk)
		\IEEEcompsocthanksitem L.\ Wang is with Institute of Automation, Chinese Academy of Sciences. (e-mail: liang.wang@cripac.ia.ac.cn)
		\IEEEcompsocthanksitem W.\ Huang is with Gaoling School of Artificial Intelligence, Renmin University of China. (e-mail: hwenbing@126.com)
		\IEEEcompsocthanksitem T.\ Xu, D.\ Zhao, Y.\ Rong are with Alibaba DAMO Academy. (e-mail: xuty\_007@hotmail.com; zhaodeli@gmail.com; yu.rong@hotmail.com)
		\IEEEcompsocthanksitem Corresponding author: Yu Rong
	}%

}

\markboth{Journal of \LaTeX\ Class Files,~Vol.~18, No.~9, September~2020}%
{How to Use the IEEEtran \LaTeX \ Templates}

\maketitle

\begin{abstract}
Graph Transformers (GTs) have demonstrated a strong capability in modeling graph structures by addressing the intrinsic limitations of graph neural networks (GNNs), such as over-smoothing and over-squashing. Recent studies have proposed diverse architectures, enhanced explainability, and practical applications for Graph Transformers. In light of these rapid developments, we conduct a comprehensive review of Graph Transformers, covering aspects such as their architectures, theoretical foundations, and applications within this survey. We categorize the architecture of Graph Transformers according to their strategies for processing structural information, including graph tokenization, positional encoding, structure-aware attention and model ensemble. Furthermore, from the theoretical perspective, we examine the expressivity of Graph Transformers in various discussed architectures and contrast them with other advanced graph learning algorithms to discover the connections. Furthermore, we provide a summary of the practical applications where Graph Transformers have been utilized, such as molecule, protein, language, vision, traffic, brain and material data. At the end of this survey, we will discuss the current challenges and prospective directions in Graph Transformers for potential future research.
\end{abstract}

\begin{IEEEkeywords}
Graph Transformers, Graph Neural Networks.
\end{IEEEkeywords}


\input{sections/Intro.tex}
\input{sections/Preliminary.tex}
\input{sections/Architectures.tex}
\input{sections/Theory.tex}
\input{sections/Applications.tex}
\input{sections/Future.tex}
\input{sections/Conclusion.tex}


\bibliographystyle{IEEEtran}
\bibliography{ref}





\end{document}
