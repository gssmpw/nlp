\section{An efficient online algorithm for minimizing $\Phi^m$-regret}
\label{sec:reg}

We now switch gears to the online learning setting, recalled in~\Cref{sec:gordon}. Our main result, \Cref{theorem:main-prec}, is an efficient online algorithm for minimizing $\Phi^m$-regret with respect to any $\poly(d)$-dimensional set $\Phi^m$ (under~\Cref{assumption:kernel-precise}), which applies even in the adversarial regime.

In what follows, we build on the framework of~\citet{Daskalakis24:Efficient}, itself a refinement of the template of~\citet{Gordon08:No}. As we have seen, \citet{Daskalakis24:Efficient} showed that separating even over the set of linear endomorphisms is hard. In light of this, they proceed as follows. Instead of operating over the set of linear endomorphisms, their key idea is to consider a sequence of ``shell sets,'' each of which contains the original set. Each shell set must also satisfy two basic properties:
\begin{itemize}
    \item it is sufficiently structured so that it is possible to optimize over that set, and
    \item it contains a transformation with a fixed point inside $\cX$.
\end{itemize}
Here, we show that by replacing fixed points with \emph{expected} fixed points in the above template, it is possible to extend their main result to handle any $\poly(d)$-dimensional set under~\Cref{assumption:kernel-precise}.

\paragraph{Overview} Our main construction is~\Cref{alg:main}. It is an instantiation of~$\shellgd$ (\Cref{sec:shellgd}), which is projected gradient descent but with the twist that the constraint set is changing over time---reflecting the fact that a new shell set is computed at every round. To execute $\shellgd$, $\shellproj$ (\Cref{sec:shellproj}) provides an efficient projection oracle together with an approximate expected fixed point thereof, which is ultimately the output of our $\Phi^m$-regret minimizer. $\shellproj$ crucially relies on $\shellelips$, introduced next in~\Cref{sec:shellellips}. It strengthens our semi-separation oracle of~\Cref{theorem:semiseparation} by again using expected fixed points. \Cref{sec:put} combines those ingredients to arrive at our main result (\Cref{theorem:main-prec}).

\subsection{Shell ellipsoid}
\label{sec:shellellips}

Continuing from our semi-separation oracle of~\Cref{theorem:semiseparation}, $\shellelips$ (\Cref{alg:shellellipsoid}) takes a step further: it takes as input a convex set of transformations $\cF \subseteq \cB_D(\vec{0})$---for which we have efficient oracle access, unlike $\enfuns$---and returns \emph{either} a function $\phi \in \cF$ and an $\epsilon$-expected fixed point thereof in $\Delta(\cX)$, \emph{or} it provides a certificate---in the form of a polytope expressed as the intersection of a polynomial number of halfspaces---establishing that $\vol(\cF \cap \enfuns) \approx 0$. $\shellelips$ will be used later as part of the $\shellproj$ algorithm so as to shrink the shell set.

\begin{lemma}
    \label{lemma:shellellipsoid}
    For any $k$-dimensional convex set $\cF \subseteq \cB_{D}(\vec{0})$ with efficient oracle access and $\epsilon > 0$, $\shellelips(\cF)$ (\Cref{alg:shellellipsoid}) runs in time $\poly(k, \log(1/\epsilon), \log D)$, and 
    \begin{itemize}
        \item either it returns a transformation $\phi \in \cF$ with an $\epsilon$-expected fixed point in $\cX$,
        \item or it returns a polytope $\cQ \subseteq \R^k$, specified as the intersection of at most $\poly( k, \log(1/\epsilon), \log D)$ halfspaces, such that $\Phi^m \subseteq \cQ$ and $\vol(\cQ \cap \cF) < \epsilon$.
    \end{itemize}
\end{lemma}

Coupled with~\Cref{theorem:semiseparation} pertaining to the semi-separation oracle, the proof of correctness of~\Cref{lemma:shellellipsoid} is immediate. That $\cQ$ can be expressed as the intersection of a polynomial number of halfspaces follows from the usual analysis of ellipsoid, as in~\citet[Lemma 4.2]{Daskalakis24:Efficient}.

\begin{algorithm}[!ht]
\caption{$\shellelips(\cF)$}
\label{alg:shellellipsoid}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\SetKw{Input}{Input:}
\SetKw{Output}{Output:}
\Input{
    \begin{itemize}[noitemsep,topsep=0pt]
        \item Oracle access to convex set $\cX \subseteq \R^d$
        \item Oracle access to a $k$-dimensional convex set $\cF \subseteq \cB_D(\vec{0})$
        \item Precision parameter $\epsilon > 0$
    \end{itemize}
}
Initialize $\cE \defeq \cB_D(\vec{0})$ and $\cQ \defeq \R^k$\;
 \While{$\vol(\cE) \geq \epsilon$} {
    Set $\phi \in \cQ \cap \cF$ as the center of $\cE$\;
    Run the semi-separation oracle of~\Cref{theorem:semiseparation} with respect to $\phi$\;
    \If{it returned an $\epsilon$-expected fixed point $\mu \in \Delta(\cX)$ of $\phi$}{
        \textbf{return} $\phi$\;
    }
    \Else{
        Let $H$ be the halfspace returned by~\Cref{theorem:semiseparation} that separates $\phi$ from $\enfuns$\;
        Set $\cQ \defeq \cQ \cap H$\;
    }
    Set $\cE$ to be the minimum volume ellipsoid containing $\cQ \cap \cF$
 }
 \textbf{return} $\cQ$
\end{algorithm}

\subsection{Shell gradient descent}
\label{sec:shellgd}

Instead of minimizing external regret with respect to the set $\enfuns$, which is hard even under linear endomorphisms~\citep{Daskalakis24:Efficient}, the overarching idea is to run (projected) gradient descent but with respect to a sequence of changing shell sets, $(\tilY^{(t)})_{t=1}^T$, of $\enfuns$ (each of which contains $\enfuns$); this process, called~$\shellgd$, is given in~\Cref{alg:shellgd}. So long as $\enfuns \subseteq \tilY^{(t)}$, $\shellgd$ indeed minimizes external regret with respect to deviations in~$\enfuns$---of course, $\shellgd$ is not a genuine regret minimizer for $\enfuns$ in that it is allowed to output strategies not in $\enfuns$, but~\Cref{lemma:shellgd} below is in fact enough for the purpose of minimizing $\Phi^m$-regret.

\begin{lemma}[\citep{Daskalakis24:Efficient}]
    \label{lemma:shellgd}
    Suppose that the sequence of shell sets $(\tilY^{(t)})_{t=1}^T$ is such that $\enfuns \subseteq \tilY^{(t)} \subseteq \cB_D(\vec{0})$ for all $t \in [T]$. For any sequence of utilities $\vec{U}^{(1)}, \dots, \vec{U}^{(T)} \in [-1, 1]^k$, $\shellgd$ (\Cref{alg:shellgd}) satisfies
    \begin{equation}
        \max_{\vy^* \in \enfuns} \sum_{t=1}^T \langle \vy^* - \vy^{(t)}, \vec{U}^{(t)} \rangle \leq \frac{D^2}{2\eta} + \eta \sum_{t=1}^T \| \vec{U}^{(t)} \|^2.
    \end{equation}
\end{lemma}

\begin{algorithm}[!ht]
\caption{$\shellgd$~\citep{Daskalakis24:Efficient}}
\label{alg:shellgd}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\SetKw{Input}{Input:}
\SetKw{Output}{Output:}
\Input{Learning rate $\eta$, convex and compact sets $\tilY^{(1)}, \dots, \tilY^{(T)} \subseteq \cB_D(\vec{0})$}\;
Initialize $\vy^{(0)} \in \tilY^{(1)}$ and $\vec{U}^{(0)} \defeq \vec{0}$\;
 \For{$ t=1, \dots, T$} {
    Obtain efficient oracle access to $\tilY^{(t)}$\;
    Update $\vy^{(t)} \defeq \Pi_{\tilY^{(t)}}( \vy^{(t-1)} + \eta \vec{U}^{(t-1)})$\;
    Output $\vy^{(t)}$ as the next strategy and receive feedback $\vec{U}^{(t)} \in [-1, 1]^k$
 }
\end{algorithm}

\subsection{Shell projection}
\label{sec:shellproj}

To implement $\shellgd$, we will make use of $\shellproj$, the algorithm that is the subject of this subsection. There are two main desiderata for the sequence of shell sets taken as input in $\shellgd$. First, each shell set must be structured or simple enough to allow projecting onto it---this is the whole rationale of expanding $\enfuns$ through shell sets. But, of course, this is not enough, for one could just consider the entire space. The second crucial concern is that each transformation given by~$\shellgd$ needs to admit (approximate) expected fixed points, so as to use the framework of~\citet{Gordon08:No} (\Cref{theorem:gordon}) and minimize $\Phi^m$-regret. \Cref{lemma:shellproj} below, concerning $\shellproj$, shows how to accomplish that goal; its proof is similar to that of~\citet[Theorem 4.4]{Daskalakis24:Efficient}.

\begin{lemma}
    \label{lemma:shellproj}
    Let $\cX$ be a convex and compact set such that $\cB_{r}(\vec{0}) \subseteq \cX \subseteq \cB_R(\vec{0})$ and $\cM$ be a convex set such that $\enfuns \subseteq \cM \subseteq \cB_D(\vec{0})$. For any $\phi \in \cB_D(\vec{0}) \subseteq \R^k$ and $\epsilon > 0$, $\shellproj$ (\Cref{alg:shellproj}) runs in time $\poly(k, 1/\epsilon, R/r, D)$ and returns
    \begin{enumerate}
        \item a shell set $\tilPhi$ satisfying $\enfuns \subseteq \tilPhi$, expressed by intersecting $\cM$ with at most $\poly(d, k, 1/\epsilon, R/r, D)$ halfspaces, and\label{item:invar}
        \item a transformation $\tilphi \in \tilPhi$ such that $\| \tilphi - \Pi_{\tilPhi}(\phi) \| \leq \epsilon$, together with an $\epsilon$-expected fixed point of $\tilphi$, $\mu \in \Delta(\cX)$.\label{item:proj}
    \end{enumerate}
\end{lemma}

\begin{algorithm}[!ht]
\caption{$\shellproj_\Phi(\phi)$ projects $\phi$ to a shell of $\Phi$}
\label{alg:shellproj}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\SetKw{Input}{Input:}
\SetKw{Output}{Output:}
\Input{
    \begin{itemize}[noitemsep,topsep=0pt]
        \item Convex and compact set $\cX \subseteq \R^d$ such that $\cB_r(\vec{0}) \subseteq \cX \subseteq \cB_R(\vec{0})$
        \item Convex set $\cM$ such that $\enfuns \subseteq \cM \subseteq \cB_D(\vec{0})$
        \item Transformation $\phi \in \cB_D(\vec{0})$
        \item Precision parameter $\epsilon > 0$
    \end{itemize}
}
\Output{
    \begin{itemize}[noitemsep,topsep=0pt]
        \item Convex set $\tilPhi$ such that $\enfuns \subseteq \tilPhi \subseteq \cM$
        \item Transformation $\tilphi \in \tilPhi$ such that $\| \tilphi - \Pi_{\tilPhi}(\phi) \| \leq \epsilon$
        \item An $\epsilon$-expected fixed point $\mu \in \Delta(\cX)$ of $\tilphi$
    \end{itemize}
}
Set $\epsilon' = \frac{\epsilon r}{32 M(R) D^2}$\;
Initialize $\tilPhi \defeq \cM$\;
 \For{$q = 0, \dots$ incremented by $\delta \defeq \nicefrac{\epsilon}{4D} $} {
    Run $\shellelips( \tilPhi \cap \cB_q(\phi))$ with precision $\vol(\cB_{\epsilon'}(\cdot))$\;
    \If{it finds $\tilphi$ with an $\epsilon$-expected fixed point $\mu \in \Delta(\cX)$}{
        \textbf{return} $\tilPhi, \tilphi, \mu$
    }
    \Else{
        Let $\cQ$ be the polytope returned by $\shellelips$\;
        Set $\tilPhi \defeq \tilPhi \cap \cQ$
    }
 }
\end{algorithm}

\subsection{Putting everything together}
\label{sec:put}

We now combine all the previous pieces to obtain an efficient algorithm for minimizing $\Phi^m$-regret---when $\Phi^m$ is $\poly(d)$-dimensional---under a general convex and compact set $\cX$. The overall construction is depicted in~\Cref{alg:main}. In effect, it runs $\shellgd$ with respect to the sequence of shell sets $(\tilPhi^{(t)} )_{t=1}^T$. Indeed, by the correctness guarantee of $\shellproj$ (\Cref{item:invar} of~\Cref{lemma:shellproj}), we have the invariance $\Phi(\cX) \subseteq \tilPhi^{(t)}$ for all $t \in [T]$. Furthermore, \Cref{item:proj} of~\Cref{lemma:shellproj} implies that $(\mK^{(t+1)}, \cons^{(t+1)}) \in \tilPhi^{(t+1)}$, returned by $\shellproj$ in~\Cref{alg:main}, is within distance $\epsilon$ of the projection prescribed by~$\shellgd$. As a result, we can apply~\Cref{lemma:shellgd} (up to some some slackness proportional to $\epsilon$) to bound the external regret $\reg^{(T)}_{\Phi^m}$ of $((\mK^{(t)}, \cons^{(t)}))_{t=1}^T$ with respect to comparators from $\enfuns$; combined with the fact that $\mu^{(t)} \in \Delta(\cX)$ is an $\epsilon$-expected fixed point of the function $\vx \mapsto \mK^{(t)} m(\vx) + \cons^{(t)}$ (as promised by~\Cref{item:proj}), it follows that the $\Phi^m$-regret of the learner (\Cref{alg:main}) can be bounded by $\reg^{(T)}_{\Phi^m} + \epsilon T$ (as in~\Cref{theorem:gordon}). We thus arrive at our main result.

\begin{theorem}[Precise version of~\Cref{theorem:main1}]
    \label{theorem:main-prec}
    Let $\cX \subseteq \R^d$ be a convex and compact set in isotropic position for which we have a membership oracle. \Cref{alg:main} has per-round running time of $\poly(k, T)$ and guarantees average $\Phi^m$-regret of at most $\poly(k) / \sqrt{T}$, where $k$ is the dimension of $\Phi^m$ under~\Cref{assumption:kernel-precise}.
\end{theorem}

Unlike the algorithm of~\citet{Daskalakis24:Efficient}, a salient aspect of~\Cref{alg:main} is that it outputs a sequence of \emph{mixed} strategies in $\Delta(\cX)$. As we saw earlier in~\Cref{sec:gordon}, this turns out to be necessary: \citet{Zhang24:Efficient} showed that a learner restricted to output strategies in $\cX$ cannot efficiently minimize $\Phi$-regret even with respect to low-degree swap deviations (assuming $\PPAD \neq \P$).

\begin{algorithm}[!ht]
\caption{$\Phi^m$-regret minimizer for convex strategy sets}
\label{alg:main}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\SetKw{Input}{Input:}
\SetKw{Output}{Output:}
\Input{
    \begin{itemize}[noitemsep,topsep=0pt]
        \item Convex and compact set $\cX \subseteq \R^d$ in isotropic position
        \item $k$-dimensional set $\Phi^m$ under~\Cref{assumption:kernel-precise} with respect to $m : \cX \to \R^{k'}$, where $k = k' \cdot d + d$
        \item time horizon $T \in \N$
    \end{itemize}
}
\Output{An efficient $\Phi^m$-regret minimizer for $\cX$}\\
Set the learning rate $\eta \propto \frac{1}{\sqrt{T}}$ and $\epsilon = \nicefrac{1}{\poly(k,T)}$ to be sufficiently small\;
Initialize $\mu^{(1)} \in \Delta(\cX)$ and $\mK^{(1)} \defeq \mI_{d \times k'}$ to be the identity map and $\cons^{(1)} \defeq \vec{0}$\;
Initialize $\cM \defeq \cB_{R}(\vec{0})$ for a large enough $R \leq \poly(k)$\;
 \For{$ t=1, \dots, T$} {
    Output $\mu^{(t)} \in \Delta(\cX)$ and receive feedback $\vu^{(t)} \in [-1, 1]^d$\;
    Define $\R^{d \times k' + d} \ni \mU^{(t)} \defeq (\E_{\vx^{(t)} \sim \mu^{(t)}} \vu^{(t)} \otimes m(\vx^{(t)}), \vec{u}^{(t)})$\;
    Set $\tilPhi^{(t+1)}, (\mK^{(t+1)}, \cons^{(t+1)}), \mu^{(t+1)} \defeq \shellproj_{\Phi}((\mK^{(t)}, \cons^{(t)}) + \eta \mU^{(t)})$ with input $\cM$ and precision $\epsilon$, where $\mu^{(t+1)} \in \Delta(\cX)$ is an $\epsilon$-expected fixed point of $\vx \mapsto \mK^{(t+1)} m(\vx) + \cons^{(t+1)}$ 
 }
\end{algorithm}

Finally, we conclude by providing a lower bound that matches our upper bound (\Cref{theorem:main-prec}) up to a constant factor in the exponent of $k$. It is based on the following normal-form lower bound due to~\citet{Dagan24:From} and~\citet{Peng24:Fast}.

\begin{theorem}[\citep{Dagan24:From,Peng24:Fast}]
    \label{theorem:lowerknown}
    Consider a learner operating on the simplex $\Delta(\cA)$. For any $T < |\cA|/4$, there is an adversary that forces the swap regret of the learner to be $\Omega(\log^{-6}T)$.
\end{theorem}

We observe that there is a simple way to parameterize the above lower bound in terms of the dimension of the set of deviations:

\begin{corollary}
    \label{cor:parlower}
    Consider a learner operating on the simplex $\Delta(\cA)$. There is a $k$-dimensional set of deviations $\Phi \subseteq \Delta(\cA)^{\Delta(\cA)}$ such that for any $T < \sqrt{k}/4$, there is an adversary that forces the $\Phi$-regret of the learner to be $\Omega(\log^{-6} T)$.
\end{corollary}

Indeed, one can first identify an arbitrary subset $\cA'$ of $\cA$ with cardinality $\sqrt{k}$, and then employ the adversary of~\Cref{theorem:lowerknown} with respect to $\cA'$ while rendering all other actions dominated by assigning to them very small utility. That $\Phi$ is $k$-dimensional in this case follows because the set of stochastic matrices mapping $\Delta(\cA')$ to $\Delta(\cA')$---which contains all relevant swap deviations---is $(\sqrt{k})^2$-dimensional.

Combining~\Cref{cor:parlower} with the recent reduction of~\citet{Daskalakis24:Lower}, which embeds the normal-form game lower bound of~\Cref{theorem:lowerknown} into an extensive-form game, we arrive at~\Cref{theorem:mainlower}, which we restate below.

\lowerbound*