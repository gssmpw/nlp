\section{Nonlinear utilities}
\todo{what on earth is happening here}

Let us assume for now that we have an external regret minimizer $\cR_\mK$ over the set of deviations, $\mK(\Phi)$.\todo{this can be weakened probably using ideas from Farina/Pipis/et al.'s recent preprint} Consider the following learning algorithm. On each iteration $t = 1, \dots, T$:
\begin{enumerate}
    \item Get $\phi^t$ from the external regret minimizer on $\mK(\phi)$
    \item Set $\mu^t = \unif\{\vx, \phi(\vx), \phi^2(\vx), \dots, \phi^L(\vx)\}$ for some large $L$, \eg, $L = \sqrt{T}$, where $\phi^\ell$ means repeatedly applying $\phi$ $\ell$ times. \bhz{this was our old way of computing expected fixed points. it is important that we do it this way.}
    \item Sample $\vx^t \sim \mu^t$.
\end{enumerate}
Critically, we are {\em not} thinking of the regret minimizer here as a {\em deterministic} algorithm that outputs a {\em distribution} $\mu^t\in\Delta(\cX)$, but rather as a {\em randomized} algorithm that outputs a {\em single point} $\vx^t \in \cX$. As such, our adversary cannot be allowed to pick the utility $u^t$ based on $\vx^t$, but only on $\vx^1, \dots, \vx^{t-1}$.

Assume that utilites are concave. Then we have
\begin{align}
    \Phi\text{-Regret} &= \max_{\phi\in\Phi} \sum_{t=1}^T \qty[u^t(\phi(\vx^t)) - u^t(\vx^t)]
    \\&= \max_{\phi\in\Phi} \sum_{t=1}^T \qty[u^t(\phi(\vx^t)) - u^t(\vx^t)] + \sum_{t=1}^T \qty[u^t(\phi^t(\vx^t)) - u^t(\vx^t)]
    \\&\le \underbrace{\max_{\phi\in\Phi} \sum_{t=1}^T \ip{\grad u^t(\vx^t), \phi(\vx^t) - \vx^t}}_{\text{external regret of $\cR_\mK$}} + \underbrace{\sum_{t=1}^T \qty[u^t(\phi^t(\vx^t)) - u^t(\vx^t)]}_\text{approximate martingale}
\end{align}
where the last line uses concavity of $u^t$. The second term is a martingale up to error $T/L$ \bhz{here I am using the fact that $\mu^t$ is not just a fixed point in expectation, but actually constructed in the ``naive'' way (by iterating $\phi$). More precisely, $\mu^t$ constructed in the manner above satisfies $\E_{\vx^t \sim \mu^t}[u^t(\phi^t(\vx^t)) - u^t(\vx^t)] \le 1/L$ for {\em any} $u^t$, not just linear ones.}, so it is bounded by $\cO(\sqrt{T})$ with high probability.

\todo{probably don't need all the martingale stuff for the convex case}

Now let us analyze the (linearized) regret of this algorithm when the $u^t$s are {\em convex}. We have
\begin{align}
    \Phi\text{-Regret} &= \max_{\phi\in\Phi} \sum_{t=1}^T \ip{\grad u^t(\vx^t), \phi(\vx^t) - \vx^t}.
    \\&=\underbrace{\max_{\phi\in\Phi} \sum_{t=1}^T \ip{\grad u^t(\vx^t), \phi(\vx^t) - \phi^t(\vx^t)}}_{\text{external regret of $\cR_\mK$}} + \underbrace{\sum_{t=1}^T \ip{\grad u^t(\vx^t), \phi^t(\vx^t) - \vx^t}}_\text{almost a martingale}
\end{align}
If the second term were instead $\sum_t u^t(\phi^t(\vx^t)) - u^t(\vx^t)$, it would be a martingale. The second term is bounded above by $\sum_t u^t(\phi^t(\vx^t)) - u^t(\vx^t)$ when the utilites $u^t$ are {\em convex}. Thus, we can efficiently minimize the linearized $\Phi$-regret for {\em convex utilities} as well. \todo{formalize all of this}

\todo{maybe assume some sort of potential function?}
