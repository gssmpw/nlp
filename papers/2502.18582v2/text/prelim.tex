\section{Preliminaries}
\label{sec:prel}

\paragraph{Notation} We use boldface lowercase letters, such as $\vx$ and $\vy$, to denote vectors in a Euclidean space. Matrices are represented with capital boldface letters, such as $\mK$. For a vector $\vx \in \R^d$, we denote by $\|\vx \| \defeq \sqrt{\langle \vx, \vx \rangle}$ its Euclidean norm, where $\langle \cdot, \cdot \rangle$ is the inner product. The $j$th coordinate of $\vx$ is accessed by $\vx[j]$. For a matrix $\mK$, we use $\|\mK\|_F$ to denote its Frobenius norm. $\mI_{d \times d} \in \R^{d \times d}$ represents the identity matrix. $\cB_r(\vec{x})$ is the (closed) Euclidean ball centered at $\vx$ with radius $r > 0$. An \emph{endomorphism} on $\cX$ is a function mapping $\cX$ to $\cX$.

\paragraph{Oracle access} Throughout this paper, we assume that we have access to the convex and compact constraint set $\cX$ via an oracle. (For multi-player games, oracle access is posited for the constraint set $\cX_i$ of each player, which can be thereby extended to $\cX \defeq \cX_1 \times \dots \times \cX_n$.) In particular, the following three types of oracles are commonly considered in the literature:
\begin{itemize}
    \item \emph{membership}: given a point $\vx \in \R^d$, decide whether $\vx \in \cX$;
    \item \emph{separation}: given a point $\vx \in \R^d$, decide whether $\vx \in \cX$, and if not, output a hyperplane $\vec{w} \in \R^d$ separating $\vx$ from $\cX$: $\langle \vx, \vec{w} \rangle > \langle \vx', \vec{w} \rangle$ for all $\vx' \in \cX$;
    \item \emph{linear optimization}: given a point $\vec{u} \in \R^d$, output any point in $\argmax_{\vx \in \cX} \langle \vx, \vu \rangle$.
\end{itemize}
Under the assumption that $\cB_r(\cdot) \subseteq \cX \subseteq \cB_R(\vec{0})$, the three oracles described above are known to be (polynomially) equivalent~\citep{Grotschel93:Geometric}---up to logarithmic factors in $R$ and $1/r$. The previous geometric condition can always be met by bringing $\cX$ into \emph{isotropic position}, which means that, for a uniformly sampled $\vx \sim \cX$, we have $\E[\vx] = 0$ and $\E[ \vx \vx^\top] = \mI_{d \times d}$. This can be achieved in polynomial time through an affine transformation~\citep{Lovasz06:Simulated,Bourgain96:Random,Kannan97:Random}; it is easy to see that minimizing $\Phi$-regret after applying that transformation suffices in order to minimize $\Phi$-regret in the original space (formally shown in~\Cref{sec:isotropic}). As a result, we can assume throughout that, for example, $\cB_1(\vec{0}) \subseteq \cX \subseteq \cB_{d}(\vec{0})$~\citep{Lovasz06:Simulated}.

\begin{remark}[Weak oracles]
    When dealing with general convex sets, the oracles posited above can return points supported on irrational numbers. To address this issue in the usual Turing model of computation, it suffices to consider weaker versions of those oracles that allow for some small slackness $\epsilon > 0$. Our analysis in the sequel can be extended to account for such imprecision. 
\end{remark}



\subsection{Online learning and $\Phi$-regret}
\label{sec:gordon}

In the usual framework of online learning, a learner interacts with an environment over a sequence of $T \in \N$ rounds. In each round $t \in [T]$, the learner selects a strategy $\vx^{(t)} \in \cX$, and then observes as feedback from the environment a utility function $\vx \mapsto \langle \vx, \vu^{(t)} \rangle$, for some utility vector $\vu^{(t)} \in [-1, 1]^{d}$; the utility of the learner in the $t$th round is given by $\langle \vx^{(t)}, \vu^{(t)} \rangle$. For the purpose of our work, we will allow the learner to output a \emph{mixed} strategy, $\mu^{(t)} \in \Delta(\cX)$, so that the (expected) utility at the $t$th round reads $\E_{ \vx^{(t)} \sim \mu^{(t)}} \langle \vx^{(t)}, \vu^{(t)} \rangle $. As shown by~\citet{Zhang24:Efficient}, restricting the learner to output strategies in $\cX$---as opposed to $\Delta(\cX)$---makes the problem of minimizing $\Phi$-regret \PPAD-hard, even with respect to low-degree polynomials, and so employing mixed strategies will be essential for our purposes.

In this context, a canonical measure of performance in online learning is \emph{$\Phi$-regret}, defined as
\begin{equation}
     \phireg^{(T)} \defeq \sup_{\phi \in \Phi} \sum_{t=1}^T \left\langle \vu^{(t)}, \E_{\vx^{(t)} \sim \mu^{(t)}} [ \phi(\vx^{(t)}) - \vx^{(t)} ] \right\rangle.
\end{equation}
The \emph{average} $\Phi$-regret is defined as $\frac{1}{T} \phireg^{(T)}$. Perhaps the most common instantiation of $\Phi$-regret is \emph{external} regret, whereby $\Phi$ contains solely constant transformations. We are interested in characterizing the broadest set of deviations $\Phi$ that allows for efficient learnability. Our starting point is the template of~\citet{Gordon08:No}.

\paragraph{The algorithm of~\citet{Gordon08:No}} \citet{Gordon08:No} (\emph{cf.}~\citet{Blum07:From,Stoltz05:Internal}) crystallized a basic template for minimizing $\Phi$-regret. It comprises two basic components:
\begin{enumerate}
    \item a fixed-point oracle $\fp(\phi)$ that takes as input any transformation $\Phi \ni \phi: \cX \to \cX$ and outputs a fixed point thereof; that is, a point $\vx \in \cX$ such that $\phi(\vx) = \vx$.\label{item:gordon1}
    \item an \emph{external} regret minimizer $\regbox_{\Phi}$ operating over the set $\Phi$.\label{item:gordon2}
\end{enumerate}
With access to the above components, a $\Phi$-regret minimizer $\regbox$ operating over $\cX$---without the need to resort to mixed strategies---can be constructed as follows. At any time $t \in [T]$, upon selecting a strategy $\vx^{(t)} \in \cX$ and observing $\vu^{(t)}$, provide as input to $\regbox_\Phi$ the utility function $\phi \mapsto \langle \vu^{(t)}, \phi(\vx^{(t)}) \rangle$. Suppose that $\phi^{(t+1)} \in \Phi$ is the next strategy of $\regbox_\Phi$. The learner $\regbox$ can then output as its next strategy $\vx^{(t+1)}$ any fixed point of $\phi^{(t+1)}$; that is, $\vx^{(t+1)} \defeq \fp(\phi^{(t+1)})$. By definition, it follows that the $\Phi$-regret of $\regbox$ is equal to the external regret of $\regbox_\Phi$ (\emph{cf.}~\Cref{theorem:gordon}).

However, our main result in the online learning setting hinges on relaxing both oracles posited in~\Cref{item:gordon1,item:gordon2} in the framework of~\citet{Gordon08:No}. With regard to~\Cref{item:gordon1}, when operating over \emph{mixed} strategies, \citet{Zhang24:Efficient} observed that it suffices to output an \emph{$\epsilon$-expected fixed point (EFP) of $\phi$}, that is, a distribution $\mu$ such that $\| \E_{\vx \sim \mu} [ \phi(\vx) - \vx ]  \|_1 \leq \epsilon$. Unlike actual fixed points, which are marred by computational intractability, \citet{Zhang24:Efficient} observed that there is a simple, $O(1/\epsilon)$-time algorithm for computing an $\epsilon$-expected fixed point: simply take the uniform distribution over the sequence $\vx, \phi(\vx), \phi(\phi(\vx)), \dots$ for $O(1/\epsilon)$ steps. (In fact, one of our main results---namely,~\Cref{theorem:efps}---provides a polynomial-time algorithm for that problem.) The overall scheme resulting from replacing~\Cref{item:gordon1} with an approximate EFP is given in~\Cref{alg:gordon}.

\begin{algorithm}[!ht]
\caption{Minimizing $\Phi$-regret with EFPs~\citep{Gordon08:No,Zhang24:Efficient}}
\label{alg:gordon}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\SetKw{Input}{Input:}
\SetKw{Output}{Output:}
\Input{
\begin{itemize}[noitemsep,topsep=0pt]
    \item An external regret minimizer $\regbox_\Phi$ for the set $\Phi$
    \item A convex and compact strategy set $\cX$
    \item Precision parameter $\epsilon > 0$
\end{itemize}
}
\Output{An $\Phi$-regret minimizer $\regbox$ for the set $\cX$}\;
Initialize $\phi^{(1)} \in \Phi$\;
 \For{$t=1, \dots, T$}{
    Set $\mu^{(t)} \in \Delta(\cX)$ to be an $\epsilon$-expected fixed point of $\phi^{(t)}$\;
     Output $\mu^{(t)} \in \Delta(\cX)$ as the next mixed strategy of $\regbox$\;
     Receive as feedback $\vu^{(t)}$\;
     Give as input to $\regbox_\Phi$ the utility function $\phi \mapsto \E_{\vx^{(t)} \sim \mu^{(t)}} \langle \phi(\vx^{(t)}), \vu^{(t)} \rangle$\;
     Let $\phi^{(t+1)} \in \Phi$ be the next strategy of $\regbox_\Phi$\;
 }
\end{algorithm}

\begin{theorem}[\citep{Gordon08:No,Zhang24:Efficient}]
    \label{theorem:gordon}
    Let $\phireg^{(T)}$ be the $\Phi$-regret of $\regbox$ and $\reg_{\Phi}^{(T)}$ the external regret of $\regbox_\Phi$ in~\Cref{alg:gordon} with precision $\epsilon > 0$. Then, for any $T \in \N$,
    \begin{equation}
        \phireg^{(T)} \leq \reg_{\Phi}^{(T)} +  \epsilon T.
    \end{equation}
\end{theorem}

In particular, taking, say, $\epsilon \propto \nicefrac{1}{\sqrt{T}}$, \Cref{theorem:gordon} reduces $\Phi$-regret minimization on $\cX$ to external regret minimization on $\Phi$.

\begin{proof}[Proof of~\Cref{theorem:gordon}]
    For any $\phi \in \Phi$, we have
    \begin{equation}
        \label{eq:Phi-bound}
        \sum_{t=1}^T \left\langle \vu^{(t)}, \E_{\vx^{(t)} \sim \mu^{(t)}} [ \phi(\vx^{(t)}) - \vx^{(t)} ] \right\rangle \leq \sum_{t=1}^T \E_{\vx^{(t)} \sim \mu^{(t)}} \langle \phi(\vx^{(t)}) - \phi^{(t)}(\vx^{(t)}), \vu^{(t)} \rangle + \epsilon T,
    \end{equation}
    where we used the fact that $\mu^{(t)}$ is an $\epsilon$-expected fixed point of $\phi^{(t)}$ for all $t \in [T]$. The right-hand side of~\eqref{eq:Phi-bound} can be in turn bounded by the external regret of $\regbox_\Phi$ plus the slackness term $\epsilon T$.
\end{proof}

This paradigm for minimizing $\Phi$-regret has been ubiquitous in prior work in this area. And yet,~\citet{Daskalakis24:Efficient} recently demonstrated that it is insufficient even when $\Phi$ contains all linear endomorphisms of a general convex set. \Cref{sec:reg} covers in detail the framework of~\citet{Daskalakis24:Efficient}---relaxing~\Cref{item:gordon2} of~\citet{Gordon08:No}---that will be the basis for our approach as well.

\paragraph{$\Phi$-equilibria} As we highlighted earlier, there is a celebrated connection between $\Phi$-regret and the game-theoretic solution concept of (correlated) \emph{$\Phi$-equilibrium}. More precisely, in the context of multilinear games as introduced in~\Cref{sec:results}, we recall the following central definition~\citep{Stoltz07:Learning,Greenwald03:General}.

\begin{definition}
    An \emph{$\eps$-approximate $\Phi$-equilibrium} of an $n$-player multilinear $\Gamma$ is a (correlated) distribution $\mu \in \Delta(\cX_1 \times \dots \times \cX_n)$ such that for every player $i \in [n]$ and deviation $\phi_i \in \Phi_i \subseteq \cX_i^{\cX_i}$,
\begin{align}
\E_{\vx \sim \mu} \qty[u_i(\phi_i(\vx_i), \vx_{-i}) - u_i(\vx)] \le \eps. \label{eq:phi-eqm}
\end{align}
\end{definition}

A direct consequence of this definition is that if players repeatedly interact in a game and all incur sublinear $\Phi$-regret, the average distribution of play converges to the set of $\Phi$-equilibria.

\subsection{Ellipsoid against hope}
\label{sec:eah}

The \emph{ellipsoid against hope ($\eah$)} algorithm was famously introduced by~\citet{Papadimitriou08:Computing} to compute correlated equilibria in succinct, multi-player games---under the polynomial expectation property. A further crucial assumption in the approach of~\citet{Papadimitriou08:Computing} is that the game is of \emph{polynomial type}, in that the number of actions (or pure strategies) is polynomial in the representation of the game. In contrast to normal-form games, extensive-form games---and many other natural classes of games---are \emph{not} of polynomial type. \citet{Farina24:Polynomial} recently showed how to apply~$\eah$ in the context of extensive-form games---albeit only for LCE; as we have seen, the complexity of NFCE remains open. We begin by recalling their framework, which crystallizes the approach of~\citet{Papadimitriou08:Computing}. We then proceed by introducing the more powerful approach of~\citet{Daskalakis24:Efficient}, which is crucial to compute LCE under general convex constraint sets, and which will form the basis for our approach as well.

Consider an arbitrary optimization problem of the form
\begin{equation}
    \label{eq:eah}
    \qq{find} \mu \in \Delta(\cX) \qq{s.t.} \E_{\vx \sim \mu} \ip{\vy, G(\vx)} \ge 0 \quad \forall \vy \in \cY,
\end{equation}
where $\cX \subseteq \R^d$, $\cY \subseteq \R^k$, and $G : \cX \to \R^k$ is a function such that $\| G(\vx) \| \leq B$ for all $\vx \in \cX$. The crux in~\eqref{eq:eah} lies in the fact that $\mu$ resides in a high-dimensional (indeed, an infinite-dimensional) space, making standard approaches of little use. $\eah$ addresses that challenge, as we describe next.

Suppose that we are given a $\poly(d, k)$-time evaluation oracle for $G$ and a separation oracle $(\sep)$ for $\cY$, assumed to be \emph{well-bounded}: $\cB_r(\cdot) \subseteq \cY \subseteq \cB_R(\vec{0})$. In addition, we assume that we have access to a {\em good-enough-response} ($\ger$) oracle, which, given any $\vy \in \cY$, returns $\vx \in \cX$ such that $\ip{\vy, G(\vx)} \ge 0$. The $\eah$ algorithm allows us to solve problems of the form \eqref{eq:eah} with just the above tools. In particular, $\eah$ proceeds by considering an $\eps$-approximate version of the dual of \eqref{eq:eah}.
\begin{align} \label{eq:eah-dual}
    \qq{find} \vy \in \cY \qq{s.t.} \ip{\vy, G(\vx)} \le -\eps \quad \forall \vx \in \cX.
\end{align}
Since a $\ger$ oracle exists, \eqref{eq:eah-dual} is infeasible. Moreover, a certificate of infeasibility of \eqref{eq:eah-dual} provides an $\eps$-approximate solution to \eqref{eq:eah}. Thus, it suffices to run the ellipsoid algorithm on \eqref{eq:eah-dual} and extract a certificate of infeasibility. This is precisely what $\eah$ does, as formalized in~\Cref{theorem:eah}; the overall scheme is~\Cref{alg:eah} in~\Cref{sec:aux} (\Cref{alg:gen-eah} below is a more general version thereof).

\begin{theorem}[Generalized form of $\eah$~\citep{Farina24:Polynomial,Papadimitriou08:Computing}]
    \label{theorem:eah}
    Suppose that we have $\poly(d, k)$-time algorithms for the following:
    \begin{itemize}[noitemsep,topsep=0pt]
        \item an evaluation oracle for $G$, where $\| G(\vx) \| \leq B$ for all $\vx \in \cX$; 
        \item a $\ger$ oracle for~\eqref{eq:eah}; and
        \item a separation oracle ($\sep$) for the well-bounded set $\cY$.
    \end{itemize}
    Then, there is an algorithm that runs in time $\poly(d, k, \log(B/\eps))$ and returns an $\eps$-approximate solution to \eqref{eq:eah}.
\end{theorem}

\begin{algorithm}[!ht]
\caption{Ellipsoid against hope ($\eah$) under $\either$ oracle~\citep{Daskalakis24:Efficient}}
\label{alg:gen-eah}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\SetKw{Input}{Input:}
\SetKw{Output}{Output:}
\Input{
    \begin{itemize}[noitemsep,topsep=0pt]
        \item Parameters $R_y, r_y > 0$ such that $\cB_{r_y}(\cdot) \subseteq \cY \subseteq \cB_{R_y}(\vec{0})$
        \item Precision parameter $\epsilon > 0$
        \item Parameter $B > 0$ such that $\| G(\vx) \| \leq B$ for all $\vx \in \cX$
        \item A $\either$ oracle (\Cref{def:either})
    \end{itemize}
}
\Output{A sparse, $\epsilon$-approximate solution $\mu \in \Delta(\cX)$ of~\eqref{eq:eah}}\;
Initialize the ellipsoid $\cE \defeq \cB_{R_y}(\vec{0})$\;
Initialize $\tilde{\cY} \defeq \cB_{R_y}(\vec{0})$\;
\While{$\vol(\cE) \geq \vol(\cB_{\epsilon/B}(\cdot))$}{
    Query the $\either$ oracle on the center of $\cE$\;
    \eIf{it returns a good-enough-response $\vx \in \cX$}{
        Update $\cE$ to the minimum volume ellipsoid containing $\cE \cap \{ \vy \in \R^k : \langle \vy, G(\vx) \rangle \leq 0 \}$\;
    }{
        Let $H$ be the halfspace that separates $\vy$ from $\cY$\;
        Update $\cE$ to the minimum volume ellipsoid containing $\cE \cap H$\;
        Update $\tilde{\cY} \defeq \tilde{\cY} \cap H$\;
    }
}
Let $\vx^{(1)}, \dots, \vx^{(T)}$ be the $\ger$ oracle responses produced in the process above\;
Define $\mG \defeq [G(\vx^{(1)}) \mid \hdots \mid G(\vx^{(T)})] \in \R^{k \times T} $\;
Compute a solution $\vec{\lambda}$ to the convex program
    $$\qq{find} \vec{\lambda} \in \Delta^T \qq{s.t.} \min_{\vy \in \tilde{\cY}} \vec{\lambda}^\top \mG^\top \vy \geq - \epsilon$$
\Return{$\Delta(\cX) \ni \mu \defeq \sum_{t=1}^T \lambda^{(t)} \mu(\vx^{(t)})$}
\end{algorithm}

However, when it comes to computing LCE under general constraint sets, \Cref{theorem:eah} is not enough: \citet{Daskalakis24:Efficient} showed that separating over $\cY$---the set of linear endomorphisms---is hard. In light of this fact, their key observation was that an $\epsilon$-approximate solution to~\eqref{eq:eah} can still be computed given access to a weaker oracle. Namely, instead of requiring both a $\ger$ and a $\sep$ oracle, as in~\Cref{theorem:eah}, \citet{Daskalakis24:Efficient} showed that it suffices to implement the following oracle: for any given $\vy \in \R^k$ (\emph{not} necessarily in $\cY$), 
\begin{enumerate}
    \item \emph{either} compute a good-enough response $\vx \in \cX$,\label{item:ger}
    \item \emph{or} a hyperplane separating $\vy$ from $\cY$.\label{item:sep}
\end{enumerate}
Although separating over $\cY$ is hard, this weaker oracle suffices to recover the guarantee of~\Cref{theorem:eah}, and this is enough to compute linear correlated equilibria in games. Yet, for our purposes, it will be necessary to relax the aforedescribed oracle even further, as formalized below.

\begin{definition}[$\either$]
    \label{def:either}
    Consider problem~\eqref{eq:eah}. The oracle $\epsilon$-$\either$ works as follows. It takes as input $\vy \in \R^k$, and it 
    \begin{enumerate}
        \item \emph{either} computes an $\epsilon$-approximate good-enough-response $\mu \in \Delta(\cX)$, $\E_{\vx \sim \mu} \langle \vy, G(\vx) \rangle \geq - \epsilon$, such that $\supp(\mu) \leq \poly(d, k, \log(1/\epsilon))$,
        \item \emph{or} a hyperplane $\epsilon$-approximately separating $\vy$ from $\cY$.
    \end{enumerate}
\end{definition}

Compared to the oracle described earlier (\Cref{item:ger,item:sep}), \Cref{def:either} makes two further concessions: first, the good-enough-response can now be a distribution, so long as it has polynomial support; and second, both $\ger$ and $\sep$ can have some small slack $\epsilon > 0$. Both of those relaxations will be essential for our applications. We now summarize the key guarantee.

\begin{theorem}[\citep{Daskalakis24:Efficient}; generalization of~\Cref{theorem:eah}]
    \label{theorem:either}
    Suppose that we have $\poly(d, k, \log(1/\epsilon))$-time algorithms for the following:
    \begin{itemize}[noitemsep,topsep=0pt]
        \item an $\epsilon$-$\either$ oracle with respect to the well-bounded set $\cY$, and
        \item an evaluation oracle for $G$, where $\| G(\vx) \| \leq B$ for all $\vx \in \cX$.
    \end{itemize}
    Then, there is an algorithm that runs in time $\poly(d, k, \log(B/\epsilon))$ and returns an $\epsilon$-approximate solution to~\eqref{eq:eah}.
\end{theorem}

\Cref{alg:gen-eah} depicts the overall scheme under a $\either$ oracle. (The last line of the algorithm uses the notation $\mu(\vx)$ for the distribution supported solely on $\vx \in \cX$.)