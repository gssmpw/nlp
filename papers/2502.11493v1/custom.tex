% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{algorithm} 
\usepackage{algpseudocode} 
\usepackage{amsmath}
\usepackage{bm}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{amsmath}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% add by user
\usepackage{amssymb}
% \usepackage[table,xcdraw]{xcolor}
\usepackage{booktabs}
\usepackage{multirow}


\title{DAST: Context-Aware Compression in LLMs via Dynamic Allocation of Soft Tokens}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}
\author{
Shaoshen Chen$^{1}$,
~Yangning Li$^{1,2}$,
~Zishan Xu$^{1}$, 
~Yinghui Li$^{1}$,\\ 
~\textbf{Xin Su}$^{3}$,
~\textbf{Zifei Shan}$^{3}$,
~\textbf{Hai-Tao Zheng}$^{1,2}$\thanks{Corresponding author.},
 \\$^{1}$Shenzhen International Graduate School, Tsinghua University \\ 
          $^{2}$Peng Cheng Laboratory,$^{3}$WeChat,Tencent
         \\
        \tt{css24@mails.tsinghua.edu.cn}
}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
% With the Large Language Models (LLMs) facing high computational costs and redundant input when processing extended sequences,  compression methods have garnered increasing attention as a means to mitigate these challenges. While existing semantic vector-based compress approaches achieve notable compression performance,  uniformly distribute compressed representations, failing to account for the inherent variability of information density across contexts.  To address this, we propose Dynamic Allocation of Soft Tokens (DAST), a novel method that leverages the LLM’s intrinsic understanding of contextual relevance to guide compression.   DAST combines perplexity-based local information with attention-driven global information to dynamically allocate soft tokens to the most informative segments, enabling effective, context-aware compression. Experiments demonstrate that DAST outperforms SOTA methods, achieving superior performance.
% The computational inefficiencies and redundant input processing inherent in Large Language Models (LLMs) when handling long context have spurred growing interest in compression techniques to address these limitations.    While current semantic vector-based compression methods achieve notable compression performance,  they rely on uniformly distributed compressed representations, neglecting the heterogeneous information density inherent to natural language contexts.
% To address this, we propose Dynamic Allocation of Soft Tokens (DAST), a novel method that leverages the LLM’s intrinsic understanding of contextual relevance to guide compression.      DAST combines perplexity-based local information with attention-driven global information to dynamically allocate soft tokens to the most informative chunks, enabling effective, context-aware compression.    Experimental results across multiple benchmarks demonstrate that DAST surpasses state-of-the-art methods.\footnote[1]{We will release our code upon the acceptance of our paper.}



Large Language Models (LLMs) face computational inefficiencies and redundant processing when handling long context inputs, prompting a focus on compression techniques. While existing semantic vector-based compression methods achieve promising performance, these methods fail to account for the intrinsic information density variations between context chunks, instead allocating soft tokens uniformly across context chunks. This uniform distribution inevitably diminishes allocation to information-critical regions. To address this, we propose Dynamic Allocation of Soft Tokens (DAST), a simple yet effective method that leverages the LLM’s intrinsic understanding of contextual relevance to guide compression.      DAST combines perplexity-based local information with attention-driven global information to dynamically allocate soft tokens to the informative-rich chunks, enabling effective, context-aware compression.  Experimental results across multiple benchmarks demonstrate that DAST surpasses state-of-the-art methods.\footnote[1]{We will release our code upon the acceptance of our paper.} 
\end{abstract}

\section{Introduction}
% % Context Compression in Large Language Models: A Dynamic Allocation Perspective
% % In recent years, Large Language Models (LLMs) have achieved remarkable success in the field of Natural Language Processing (NLP). The computational cost associated with processing long text contexts is substantial, prompting a growing focus on context compression strategies. These strategies aim to reduce the length of input tokens while preserving critical semantic information.
% % 近年来，大型语言模型（llm）已经彻底改变了自然语言处理（NLP）领域，在广泛的任务中实现了前所未有的性能。然而，处理长文本上下文的计算需求已经成为一个重要的瓶颈，促使人们越来越关注上下文压缩策略。这些策略旨在提取关键的语义信息，同时减少输入令牌的长度。
% In recent years, Large Language Models (LLMs) have revolutionized the field of Natural Language Processing (NLP), achieving unprecedented performance across a wide range of tasks.   However, the computational demands of processing long text contexts have become a significant bottleneck, driving increasing attention to context compression strategies.   These strategies aim to distill critical semantic information while reducing the length of input tokens.

% % Early text compression methods relied on token pruning or merging techniques. Subsequently, information entropy-based approaches were developed to truncate content and further reduce text length. However, both token-level modifications and direct text alterations can compromise the original semantic integrity. To mitigate this issue, soft prompt tuning approaches emerged, replacing the original text of length N with a set of m soft tokens (where m << N). These soft tokens are either appended to the text or uniformly distributed throughout the sequence to achieve compression.

% % 早期的长上下文压缩方法侧重于令牌修剪或合并。随后，基于信息熵的方法出现了截断内容并进一步最小化文本长度的方法。尽管它们很实用，但令牌级修改和直接上下文更改都有损害原始语义完整性的风险。为了解决这一挑战，软提示调优获得了吸引力，用一组m个软令牌（其中m <<< n）替换长度为n的原始上下文。这些软令牌通常附加到上下文或均匀分布在整个序列中以实现压缩。
% Early long context compression methods focused on reducing text length or generating summaries to reduce text input length. 
% % Despite their utility, both token-level modifications and direct context alterations risk compromising the original semantic integrity.
% Despite their utility, direct alterations to the context risk compromising the original semantic integrity. To preserve full semantic integrity, semantic vectors method have become increasingly promising, replacing the original context of length \( n \) with a set of \( m \) soft tokens (where \( m \ll n \)).
% % 虽然软提示调优已经证明了有希望的结果，但传统的实现通常将所有软令牌附加到上下文的末尾，或者均匀地插入它们，隐式地假设所有上下文段的信息密度是一致的。然而，这一假设与文本内容中信息集中的内在可变性相矛盾。例如，在诸如问答之类的细粒度任务中，包含答案的片段固有地表现出更高的信息密度。类似地，在像摘要这样的粗粒度任务中，不同的文本段的摘要复杂度也不同。因此，软令牌的统一分配可能会分散对关键、信息丰富区域的关注。
% % While soft markup methods have shown effective results, traditional implementations of them often have all soft markup appended to the end or inserted evenly into the text, which means assuming that all text segments have equal information density.

% % This assumption contradicts the observable variance in information concentration within textual content.   For instance, in fine-grained tasks like question-answering, answer-bearing segments inherently contain higher information density.   Even in coarse-grained tasks such as summarization, different text segments exhibit varying levels of summarization complexity.  Consequently, this uniform allocation approach may diminish attention to critical information-rich regions. 
% % While the direct text reduction approach described above attempts dynamic reduction, it relies on scores from external models, which is detrimental to the soft token approach
% While the semantic vectors method has demonstrated promising results, traditional implementations often append all soft tokens to the end of the context or insert them evenly, implicitly assuming uniform information density across all context segments. This assumption, however, contradicts the inherent variability in information concentration within textual content. For example, in fine-grained tasks like question answering, certain segments carry more information, while in coarse-grained tasks like summarization, the complexity of text segments varies.   Consequently, uniform allocation of soft tokens may dilute attention to critical, information-rich regions.

% In addition, although the reducing text length method like LLMLingua practices dynamic text pruning, it is for local text pruning, and it depends on the scores of external model which are not suitable for soft prompt tuning method.

% Motivated by these limitations, we pose the following research question: \textbf{How can we dynamically allocate compression tokens based on self-model contextual information density?}

% In this paper, we introduce the \textbf{D}ynamic \textbf{A}llocation \textbf{S}oft \textbf{T}okens (DAST) method, a novel approach to soft token compression that leverages both \textit{local} and \textit{global} information through \textit{self-modeling}. By leveraging perplexity and attention mechanisms, DAST evaluates the local and global informational importantance of each context segment, dynamically allocating soft tokens to regions of varying density. This strategy enhances both the quality and performance of text compression, offering a more nuanced and context-aware alternative to existing methods.% Motivated by these observations, we pose the following research question: \textbf{How can we dynamically allocate compression tokens based on self-model contextual information density?}

% % In this paper, we propose a \textbf{D}ynamic \textbf{A}llocation \textbf{S}oft \textbf{T}okens method, named DAST for soft tokens to address the aforementioned issues. By leveraging perplexity and attention mechanisms, we focus on both local and global information within the text. Through assessing the informational significance of each segment in the text, soft tokens are dynamically assigned to each segment. This approach enhances the quality and performance of text compression.
Large Language Models (LLMs) \cite{achiam2023gpt,bai2023qwen,touvron2023llama} have demonstrated remarkable performance on long context tasks, excelling capturing complex dependencies and generating coherent responses over extended contexts. Nevertheless, processing long contexts incurs high computational cost, making the development of efficient \textbf{context compression} methods that preserve semantic integrity while reducing input length crucial.

Early approaches to context compression primarily relied on context pruning or summarization \cite{jiang2023llmlingua,LLMlingua-2}, which reduced input length through content removal or rephrasing. However, these methods often compromise semantic integrity through direct modification of the input sequence. Recent semantic vector-based methods \cite{ge2024incontext,zhang2025long} address this limitation by replacing the original context of length $n$ with $m$ compressed soft tokens ($m\ll n$), preserving essential information in a more compact representation. Although effective, these methods typically append soft tokens at the context terminus or distribute them uniformly, overlooking uneven information density across context chunks. This uniform distribution prevents optimal allocation of compression capacity to information-rich regions.

Notably, text-pruning-based approaches like LongLLMLingua \cite{jiang-etal-2024-longllmlingua} attempt dynamic pruning using external models to estimate tokens importance. However, this external guidance fails to capture the LLM's intrinsic understanding of information relevance, creating incompatibility with vector-based methods.

 
This raises a key research question: \textbf{How can we dynamically allocate compression tokens based on the LLM’s inherent understanding of contextual information density?}

To address this, we propose \textbf{D}ynamic \textbf{A}llocation of \textbf{S}oft \textbf{T}okens (\textbf{DAST}), a simple yet effective approach to soft tokens compression that fully leverages the LLM’s internal capabilities without requiring external models. DAST utilizes perplexity to assess local importance and attention mechanisms to capture global relevance, dynamically allocating soft tokens based on intrinsic information density. This enables more efficient and context-aware compression, improving both compression quality and model performance compared to prior methods.

\begin{figure*}[h]
\centering
% \resizebox{0.78\linewidth}{!}{
\includegraphics[width=\linewidth]{image.pdf}%fig3_217.pdf}
% }
\caption{(a) and (b): the previous \textbf{fixed} allocation methods include \textbf{Single-chunk} and \textbf{Multi-chunks compression}; (c): our \textbf{dynamic} allocation method. Notably, our dynamic method allocated more soft tokens to the key information in the answer (highlighted in \textbf{bold}) while reducing soft tokens for less information. $\text{<CT>}$ is compress soft token.
}
% \vspace{-1em}
\label{tab:main_fig}
\end{figure*}


\section{Method}
% 在这一节中，我们首先会介绍压缩的概念，然后介绍压缩的框架，接着我们会说明目前压缩的
\subsection{Compression Background}
% Traditional long context compression methods typically process input sequences through chunk-wise decomposition. Given an input context sequence $X = \{X^{\text{que}}, X^{\text{doc}}\}$, $X^{\text{que}}$ is a question or an instruction, $X^{\text{doc}}$ is a long document. Let $\{\boldsymbol{X}_i\}_{i=1}^N$ denote the partitioned chunks where $|\boldsymbol{X}_i| = L$ represents the original chunk length. For each chunk of length n, we compress it with m soft tokens (m<<n).

Traditional methods for compressing long context sequences typically employ chunk-based decomposition. Given an input sequence \( X = \{X^{\text{que}}, X^{\text{doc}}\} \), where \( X^{\text{que}} \) denotes a query or instruction and \( X^{\text{doc}} \) represents a lengthy document, the sequence is segmented into \( N \) contiguous chunks of fixed length \( |\boldsymbol{X}_i| = L \). During compression, each chunk of length \( L \) is condensed into a fixed number \( m \) of soft tokens, where \( m \ll L \).
 %Critically, in prior methods, \( m \) is **fixed** as a predetermined hyperparameter across all chunks. % NEW: Emphasis on fixed m

Existing compression strategies, as illustrated in ~\autoref{tab:main_fig}(a) and (b), can be broadly categorized into two paradigms. The first, termed \textbf{\textit{Single-chunk Compression}}, processes the entire sequence as a single chunk and appends all fixed \( m \) soft tokens after the full sequence. To enhance granularity, methods such as AutoCompress \cite{chevalier2023adapting} and Beacon \cite{zhang2025long} introduced \textbf{\textit{Multi-chunks Compression}}, which assigns a compression constraint (divisible by the chunk length \( L \)) stochastically during training and evenly distributes a fixed \( m \) soft tokens across all chunks during inference. 

However, a major limitation of these methods is their \textbf{fixed tokens allocation scheme}, which implicitly assumes uniform information density across the entire context. This assumption introduces the risk that regions with high information density receive fewer soft tokens, while regions with low information density are allocated more soft tokens. To address this issue, we propose a \textit{Dynamic Allocation of Soft Tokens} method, which adaptively assigns a \textbf{dynamic number of soft tokens} \( d_i \) to each chunk \( \boldsymbol{X}_i \), where \( d_i \) is determined by localized and global information density, as shown in ~\autoref{tab:main_fig}(c).


% Existing approaches for distributing soft tokens can be categorized into two primary methods. The first, termed \textbf{\textit{Single-chunk Compression}}, processes the entire sequence as a single chunk, appending all \( m \) soft tokens after the full sequence. To improve granularity, Ultragist introduced \textbf{\textit{Multi-chunk Compression}}, which stochastically assigns a compression rate (divisible by the chunk length \( L \)) during training and evenly distributes a fixed number of \( m \) soft tokens across all chunks during inference.

% A critical limitation of these methods lies in their implicit assumption of uniform information density across the context, coupled with their reliance on a \textbf{fixed allocation} of \( m \) soft tokens per chunk. By rigidly enforcing identical compression rates, regions with high information density may be inadequately compressed, while low-density regions waste computational resources. To address this issue, we propose a \textit{Dynamic Soft Token Allocation} method, which adaptively assigns a \textbf{variable number of soft tokens} \( m_i \) to each chunk \( \boldsymbol{X}_i \), where \( m_i \) is determined by localized and global information density.


\subsection{Overall Framework}
% According to our dynamic compression method, we can get the soft token of each chunk, which is described in the next section. The compression process for the $i$-th chunk can be formally defined as:

% % \begin{equation}
% %     \boldsymbol{C}_i = {
% %     \underbrace{\{\boldsymbol{\langle ct \rangle}_j\}_{j=1}^{i-1}}_{\text{historical soft tokens}} 
% %     ;
% %     \boldsymbol{X}_i 
% %     ; 
% %     \underbrace{\boldsymbol{\langle ct \rangle}_i}_{\text{current soft tokens}}}
% % \end{equation}
% \begin{equation}
% \begin{aligned}
%     \boldsymbol{C}_i = \left\{
%     \boldsymbol{\langle ct \rangle}_1, \dots, \boldsymbol{\langle ct \rangle}_{i-1}, \, \boldsymbol{X}_i, \, \boldsymbol{\langle ct \rangle}_i
%     \right\}
% \end{aligned}
% \end{equation}



% \noindent where \( \boldsymbol{\langle ct \rangle}_j \in \mathbb{R}^{d_j} \) denotes the compressed soft tokens from the \( j \)-th chunk (\( 1 \leq j < i \)). The compressed tokens \( \boldsymbol{\langle ct \rangle}_i \in \mathbb{R}^{d_i} \) capture essential information from the current chunk through contextual interaction enabled by the cross-attention mechanism:

% \begin{equation}
%     \text{CrossAttn.}(\boldsymbol{C}_i ; \text{Mask})
% \end{equation}

% Current approaches to soft token distribution can be categorized into two main types, as illustrated in Figure%~\ref{fig:soft-token-distribution}:


%     \textbf{Single-chunk Compression}: Treats the entire text as a single chunk, placing all soft tokens after the text.
    
%     \textbf{Multi-chunk Compression}: Evenly distributes \( m \) soft tokens across each text chunk.

% % \begin{itemize}
% %     \item \textit{Single-chunk Compression}: Treats the entire text as a single chunk, placing all soft tokens after the text.
% %     \item \textit{Multi-chunk Compression}: Evenly distributes \( m \) soft tokens across each text chunk.
% % \end{itemize}

% Both of these existing methods implicitly assume a uniform information density across the context, with each chunk receiving the same number of m soft tokens. This assumption leads to an insufficient allocation of soft tokens in the high-information-density regions of the context. To address this, we introduce a dynamic soft token allocation method, as depicted in Figure%~\ref{fig:dynamic-allocation}
% . Our approach allocates soft tokens to each chunk based on its information density, recognizing the importance of both local and global information.

Our method dynamically determines the number of soft tokens assigned to each chunk, as described in the next section. Given the \( i \)-th chunk, the compressed representation is constructed as:
\begin{equation}
\begin{aligned}
    \boldsymbol{C}_i = \left\{
    \boldsymbol{\langle ct \rangle}_1, \dots, \boldsymbol{\langle ct \rangle}_{i-1}, \, \boldsymbol{X}_i, \, \boldsymbol{\langle ct \rangle}_i
    \right\},
\end{aligned}
\end{equation}
where \( \boldsymbol{\langle ct \rangle}_j \in \mathbb{R}^{d_j} \) represents the compressed soft tokens of the \( j \)-th chunk (\( 1 \leq j < i \)). The compressed tokens \( \boldsymbol{\langle ct \rangle}_i \in \mathbb{R}^{d_i} \) capture essential information from the current chunk through contextual interactions, which are facilitated by the cross-attention mechanism:
\begin{equation}
    \text{CrossAttn.}(\boldsymbol{C}_i ; \text{Mask}).
\end{equation}

\subsection{Dynamic Allocation}
% In this section, we will introduce our method for dynamically allocating soft tokens. When splitting chunks, we need to pay attention to the local importance of each chunk, and at the same time, we also need to pay attention to the global importance of the whole text. Therefore, we use Perplexity (PPL) and Attention (Attn) respectively to pay attention to the local and global information, and then get the number of soft tokens of each chunk.
In this section, To effectively allocate soft tokens, we consider both \textbf{local importance} (i.e., within each chunk) and \textbf{global importance} (i.e., across the entire sequence). Specifically, we employ \textbf{Perplexity (PPL)} to estimate local importance and \textbf{Attention (Attn)} to capture global importance. These two metrics are then combined to determine the number of soft tokens assigned to each chunk.

\textbf{PPL}: Perplexity is a widely used metric for evaluating contextual informativeness. A lower perplexity signifies greater relevance of the current context information \cite{jiang2023llmlingua}. Since each chunk is visible within its own local context during compression, we compute the perplexity for each chunk separately. The resulting perplexity scores thus embody the local importance information of each respective chunk. The PPL of $i$-th chunk as follows:
% \sum_{i=1}^{n} q(x_i) \log p(x_i | x_{<i})
\begin{equation}
\begin{aligned}
\text { ${P}_{i}$ = $- \sum_{l=1}^{L} q(x_l) \log p(x_l | x_{<l})$},
\end{aligned}
\end{equation}
where \( q(x_{l}) \) represents the probability distribution of the ground truth.

\textbf{Attn}: Once the sequence has been compressed, the importance of each chunk’s compressed representation can be inferred from attention weights.
Intuitively, chunks with more crucial information have higher attention weights, reflecting their contribution to global understanding. 
Hence, we utilized global attention to measure the global information, which has been demonstrated to be an effective method \cite{globalAttn}. Through attention weights, we can ascertain the global proportion of each token. The formula is :
\begin{equation}
\begin{aligned}
\text { $A_{i}$ = 
$\sum_{j=i}^{(i+1)m} (q  k^\top)_{j}$,
 }
\end{aligned}
\end{equation}
where \textbf{$q$} represents the last token vector and \textbf{$k$} represents all the compressed tokens vectors.
Thus, we can obtain the scores of i-th chunk that focus on both global and local information: 
\begin{equation}
\begin{aligned}
\text { $
\text{$S_{i}$} = \text{$A_{i}$} \cdot \alpha - \frac{\text{$P_{i}$}}{\sum_{k=1}^{N} (\text{$P_{k}$})} \cdot (1 - \alpha)$,
}
\end{aligned}
\end{equation}
where $\alpha$ is a parameter that balances the importance of global \(A_{i}\) and local \(P_{i}\) information. Then Softmax is used for normalization. Noted that since \( A_{i} \) and \( P_{i} \) are derived from different distributions,  \( P_{i} \) is scaled by the number of all chunk \( N \). Given the total number of soft tokens of context is \( M \), we can calculate the actual number of soft tokens in the \( i \)-th chunk $d_{i}$ = $M$ $\times$ $S_{i}$.

\textbf{Reallocation}: In order to make the tokens after dynamic allocation consistent with the training, we design a reallocation algorithm to make them divisible by $L$. The details of this algorithm are shown in Appendix~\ref{appendix:algorithm}. It is worth mentioning that the reallocation is optional, depending on the method mechanism used.

\section{Experiments}
The experiment's detail  are presented in Appendix~\ref{appendix:implementation}. Next, we want to answer two questions: (1) How effective is DAST? (2) How does the performance of DAST improve?
\subsection{Main Results}
To evaluate the dynamic distribution capability of DAST in a long context with inconsistent ground truth granularity distribution, we employ three benchmarks from LongBench \cite{bai-etal-2024-longbench}: Single-Document, Multi-Document, and Example tasks (Few-Shot). As demonstrated in ~\autoref{tab:main_longbench}, our approach demonstrates consistent superiority over baseline methods across all evaluated tasks. This improvement can be attributed to the model's adaptive capacity to discern textual saliency within redundant, extended textual inputs. Our method strategically allocates a higher proportion of soft tokens to semantically critical chunks, thereby enhancing computational attention to pivotal content. This differential allocation mechanism ultimately optimizes task-specific performance through context-aware resource distribution.

Having established the effectiveness of our dynamic compression method, we further examine its performance-enhancing mechanisms through systematic experiments on the NaturalQuestions dataset~\cite{NQ}. This benchmark is particularly well-suited for analysis due to the presence of correct answers at varying contextual positions. As shown in ~\autoref{NQ}, our method consistently surpasses the uniform compression approach of Beacon across all positional configurations. Notably, in context chunks containing answer-relevant, our method adaptively allocates more tokens to these semantically critical regions, thereby improving performance.


\begin{figure}[tb]
\centering
% \resizebox{0.78\linewidth}{!}{
\includegraphics[width=\linewidth]{my_plot.pdf}
% }
\caption{Performance and Number of soft tokens v.s. Key Information Position.}
% \vspace{-1em}
\label{NQ}
\end{figure}

% 介绍实验设置，介绍baselie，用的数据集，用什么模型
% 主实验，从结果可以看出，使用了我们的动态压缩方法会提升，可以分别讲粗粒度跟细粒度怎么怎么样。
% 介绍NQ，证明我们的方法在不同压缩率上性能更优，给予的压缩token更多
% 介绍我们在不同压缩率的效果
% 消融实验分析
% 参数敏感性分析，证明我们的方法对参数不敏感，因此我们的alpha只是取常见的0.5，没有进行刻意参数选择。

\begin{table*}[h]
% \scalebox{0.8}{
\centering
\begin{tabular}{lcccc|cccc}
\toprule
                & \multicolumn{8}{c}{\textbf{Document and Example Compression}}                                                \\ \cmidrule(lr){2-9}%\rotatebox{45}
\multirow{2}{*}{\textbf{Methods}}         & Single & Multi & Few  & \multirow{2}{*}{\textbf{AVG}} & Single & Multi & Few  & \multirow{2}{*}{\textbf{AVG}}   \\ 
&Doc & Doc & Shot  &  & Doc & Doc & Shot  & 
\\ \midrule
& \multicolumn{4}{c}{\textbf{LLama-2-7B}}   & \multicolumn{4}{c}{\textbf{Qwen-2-7B}}                                                                                                       \\ \midrule
Original Prompt%$^{\dag}$
& 24.9           & 22.5               & 60.0             &  35.8   & 22.0         & 29.3               & 62.3             &  37.9                           \\
Zero-Shot       &   8.1             & 6.1    & 32.2 & 15.5          & 7.1           & 6.6               & 26.8             &  13.5                       \\ \midrule                                                
AutoComp.$^{\dag}$ \citep{chevalier2023adapting}                    & 12.9      & 16.4     & 23.8     &  17.7         &\multicolumn{4}{c}{-}                 \\
ICAE$^{\dag}$ \citep{ge2024incontext}                               & 19.5      & 19.2       & 24.8    & 21.2                &\multicolumn{4}{c}{-}                 \\
LongLingua.$^{\dag}$ \citep{jiang-etal-2024-longllmlingua}                     & 21.5      & 18.8       & 49.5     &     29.9      & 24.7      & 20.3       & 55.9     &     33.6                    \\
SnapKV$^{\dag}$ \citep{li2024snapkv}                            & 24.2      & 22.6       & 60.1     &   35.6         & 38.7      & 37.6            &   67.1   &47.8                \\
Beacon$^{\dag}$ \citep{zhang2025long}                      & 34.9      & 27.5       & 61.4     &  41.3                  & 40.5      & 40.3       & 68.4     &   49.7             \\
\textbf{DAST (Ours)}   &  \textbf{38.1}             &  \textbf{37.4}       &          \textbf{63.6}   & \textbf{46.4}  & \textbf{40.6} & \textbf{45.6} & \textbf{68.6} &  \textbf{51.6}                 \\ \bottomrule
\end{tabular}%}
\caption{Evaluation of various Document and Example Compression tasks (top performances marked in $\textbf{bold}$). $\dag$: the results cited from \citet{zhang2025long}.}
\label{tab:main_longbench}
% \vspace{-0.5em}
\end{table*}



% \subsection{Main Experiments}
% % 在主实验中，我们试图回答两个问题：（1）我们的方法在答案信息分布明显的数据集上的效果（2）我们的方法是如何提升性能的。
% We tested our approach against each baseline in a publicly available longbench benchmark, as shown in Table 1. In our experiments, our method consistently outperforms the baseline across various task categories, as reflected in the higher average scores. Notably, our approach excels in tasks that require detailed answer distributions, such as Question Answering (QA), code generation, and few-shot learning. In contrast, while our method shows slightly lower performance in summary tasks compared to the baseline, this can be attributed to the inherent nature of these tasks, which often demand more abstract or generalized responses. These tasks may not fully leverage the strengths of our dynamic compression method, which is particularly adept at preserving detailed and specific answer structures. 

% After  demonstrated the effectiveness of the dynamic compression method.  Next, as illustrated in Figure 2, we examine the impact of our method on strong baseline beacons across different locations within the NQ dataset to address the question: What accounts for the superior performance of our method?  The results reveal that our method consistently outperforms the baseline model across various locations.  Specifically, in regions containing answer fragments, our dynamic compression technique provides a greater number of tokens, thereby enhancing overall performance.
% \vspace{-1.2em}
\subsection{Comparison of Different Constraints}
We compare our method to baselines on the Long-Term Memory MSC dataset \cite{MSC} to study compression intensity vs. memory retention. As presented in ~\autoref{ratios}, our approach consistently outperforms conventional methods across all compression levels, especially in resisting degradation at higher compression constraints.


% \vspace{-2em}
% \vspace{-2em}
\begin{table}[tb]
% \centering
% \scalebox{0.8}{
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccc}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multicolumn{4}{c}{\textbf{Compression Constraint}}                                                                                                                                                                                                                                                                                                                                                                 \\ \cmidrule(lr){2-5} 
                                 & \begin{tabular}[c]{c} $\sim$4 x\end{tabular}
                                 & \begin{tabular}[c]{c} $\sim$8 x\end{tabular}  
                                 & \begin{tabular}[c]{c} $\sim$16 x\end{tabular}
                                 & \begin{tabular}[c]{c} $\sim$24 x\end{tabular} \\  \midrule
AutoCom.                 &    28.8                                                                &     27.3$_{\downarrow5.2\%}$                                                                &                25.0$_{\downarrow13.2\%}$                                                                        &    24.0$_{\downarrow17.1\%      }$                                                                \\
LongLLM.                    &     22.4                                                               &   19.5$_{\downarrow13.0\%  }$                                                                 &      17.8$_{\downarrow20.5\%   }$                                                                                                                                       &   15.9$_{\downarrow29.0\%  }$                                                                     \\
ICAE                    &     18.1                                                               &    16.6$_{\downarrow8.3\% }$                                                                   &     15.3$_{\downarrow15.5\% }$                                                                                                                                          &   14.5$_{\downarrow19.9\%}$                                                                      \\
Beacon                    &  39.0                                                                  &  36.5$_{\downarrow6.4\%}$                                                                      &    33.6$_{\downarrow13.9\%}$                                                                                    & 32.3$_{\downarrow17.2\%}$                                                                         \\
\textbf{DAST}                          &        $\textbf{55.9}$                                                            &            $\textbf{55.5}_{\downarrow0.7\%}$                                                                                                                               &    $\textbf{52.6}_{\downarrow5.9\%}$                                                                   &         $\textbf{51.9}_{\downarrow7.2\%}$                                                                                                     \\ \bottomrule                                                                
\end{tabular}
}
\caption{Evaluation of Long-Term Memory on MSC.  $\downarrow$: percentages showing relative performance drop compared to the $\sim$4x compression baseline.}
\label{ratios}
% \vspace{-0.5em}
\end{table}


% \vspace{-0.5em}
\subsection{Ablation Study}
% \vspace{-1em}

\begin{table}[t]
\centering
\begin{tabular}{lc}
\toprule
\textbf{Method}          & \textbf{Single-Doc} \\ \midrule
Random Allocation & 34.52  \\ \midrule
Uniform Allocation  &      34.90              \\ \midrule
\textbf{Dynamic Allocation (ours)}                    &      \textbf{38.14}         \\
% w/o Pre-training         &                     \\
% w/o Fine-tuning          &                     \\
w/o PPL       &       37.60              \\
w/o Attn      & 37.24                   \\
\bottomrule 
\end{tabular}
\caption{Ablation study of DAST.}
\label{ablation}
\end{table}
In this section, we analyze our method's performance and the impact of each module (see ~\autoref{ablation}). Random and uniform tokens allocations performed poorly, showing insufficient focus on critical context segments. Removing either the global attention (Attn) or local perplexity (PPL) module individually
caused performance drops, highlighting their importance in
prioritizing important chunks.
% In this section, we analyze the performance of our method and the effect of each module, as shown in ~\autoref{ablation}.  Comparing random and uniform tokens showed poorer capability, indicating insufficient emphasis on crucial context segments.  Removing either the global attention (Attn) or local perplexity (PPL) module individually led to performance drops, highlighting their importance in 
% prioritizing important chunks.

\begin{figure}[tbh]
\centering
% \resizebox{0.78\linewidth}{!}{
\includegraphics[width=\linewidth]{alpha.pdf}
% }
\caption{Parameter Sensitivity Analysis of $\alpha$.}
% \vspace{-1em}
\label{alpha}
\end{figure}

\label{sec:3.4}
\subsection{Parameter Sensitivity Analysis}
It is important to analyze the sensitivity of the parameters $\alpha$. 
As shown in ~\autoref{alpha}. The results indicate that the performance remains stable across different values of $\alpha$. Consequently, we selected a default value of $\alpha=0.5$ in main experiments, which simplifies the application of our method to other models, as it eliminates the need for specialized parameter tuning.

\section{Conclusion}
In this paper, we propose DAST, a simple yet effective  method that dynamically allocates soft tokens by leveraging the LLM’s intrinsic perception of information density. By integrating perplexity-based local information and attention-driven global relevance, DAST adaptively focuses compression capacity on high-information regions without relying on external models. 
Our experiments show that DAST outperforms prior methods in both compression quality and downstream task performance, underscoring the value of model-guided dynamic allocation. 
% 介绍实验设置，介绍baselie，用的数据集，用什么模型
% 主实验，从结果可以看出，使用了我们的动态压缩方法会提升，可以分别讲粗粒度跟细粒度怎么怎么样。
% 介绍NQ，证明我们的方法在不同压缩率上性能更优，给予的压缩token更多
% 介绍我们在不同压缩率的效果
% 消融实验分析
% 参数敏感性分析，证明我们的方法对参数不敏感，因此我们的alpha只是取常见的0.5，没有进行刻意参数选择。








% \clearpage
\section{Limitations}
Current model compression research remains primarily limited to the approximately 7B parameter scale due to computational resource constraints. While our study has demonstrated that our method outperforms other compression approaches, we have not been able to systematically investigate whether existing compression techniques, including our approach, can maintain their effectiveness when applied to larger architectures.  Given that the practical value of compression techniques becomes more pronounced with increasing model sizes, this represents a critical direction for future research.  Furthermore, it is also imperative to examine whether the compression process induces more severe : (1) hallucination phenomena and (2) catastrophic forgetting in compressed models, which constitutes another essential aspect requiring thorough investigation.











% \clearpage







% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

\appendix

\section{Reallocation Algorithm}
%对于给定的所有chunk的当前soft token集合，总的可分配soft token S,可选择的压缩率集合R，我们的目标是重新分配T得到T，算法如下
For the current soft tokens set $\bm{T}$ of all chunks given, the total soft tokens $\bm{S}$ can be assigned, and the optional compression rate set $\bm{R}$, our goal is to reassign $\bm{T}$ to get $\tilde{\bm{T}}$, the algorithm is as follows Algorithm~\autoref{new_algorithm}.
% This is an appendix.
\label{appendix:algorithm}
\begin{algorithm} 
	\caption{Reallocation} %标题
	 \label{algorithm}       % 用来引用
	\begin{algorithmic}[1] % 加上 [1] 表示有序号
	\Require $\bm{T}$, $\bm{S}$,$\bm{R}$ ,   %   Requre 等同于 Input
    \Ensure Reallocated $\tilde{\bm{T}}$,%   Ensure 等同于 Output
    \State $\tilde{\bm{T}}$ $\gets$ $\emptyset$   %   语句    
 %  \State Treat X and Y as center coordinates
 %   \State $\mathcal{M}$ $\gets$ $\theta$
    \State ${\bm{T}}$ are allocated to each chunk based on the closest compression constraint ${\bm{R}}$ to get the $\tilde{\bm{T}}$.
    \Repeat
    \State Get the disposable ${\bm{M}}$ from ${\bm{S}}$ - ${\bm{sum(\tilde{T})}}$
    \State In the remaining tokens, double the tokens of the chunk that meets the conditions with the highest score.
    \State Update $\tilde{\bm{T}}$
    \State Remove the highest score temporarily,
 
   \Until {$\bm{M}$ = 0}
%   \State  Treat $\tilde{X}$ and $\tilde{Y}$ as top-left coordinates
   \State\Return $\tilde{\bm{T}}$ % Return 返回语句
   \end{algorithmic} 
\label{new_algorithm}
\end{algorithm} 

\section{Settings}
\label{appendix:implementation}
\subsection{Implementation}
%为了与对比的方法保持绝对一致，我们使用Llama-2-7B (chat) and Qwen-2-7B模型。在训练数据的选择上，我们与Beacon选择同样的数据，使用1B tokens sampled from Repajama in pre-training and 使用LongAlpaca，Booksum,and synthetic data from GPT-3,5在fine-tuning. (具体细节详见Beacon)。
%我们的方法使用的alpha设置为常见的0.5，并且在实验中对不同参数进行了分析。我们所有的实验都在8*A800 (80G)上完成
% In order to be absolutely consistent with the methods of comparison, we use the Llama-2-7B (chat) and Qwen-2-7B models. in terms of the selection of training data, we selected the same data as Beacon\cite{zhang2025long}, using 1B tokens sampled from Repajama in pre-training and LongAlpaca. Booksum,and synthetic data from GPT-3,5 in fine-tuning. (See Beacon for details.)
% Our method uses a common alpha setting of 0.5, and different parameters are analyzed in the experiment. All of our experiments were done on 8*A800 (80G)

To ensure strict methodological consistency in model comparisons, we employ the Llama-2-7B (chat) \cite{llama2chat} and Qwen-2-7B \cite{bai2023qwen} architectures. For training data selection, we adopt the same approach as Beacon\cite{zhang2025long}, utilizing 1B tokens sampled from Repajama \cite{redpajama} during pre-training, supplemented by LongAlpaca \cite{chen2024longlora}, BookSum \cite{kryscinski2022booksum}, and synthetic data generated by GPT-3.5 for fine-tuning (see Beacon \cite{zhang2025long} for detailed data curation protocols).
Our implementation uses a standard $\alpha$ value of 0.5, with sensitivity analyses for alternative parameter configurations provided in \hyperref[sec:3.4]{\textsection 3.4}. We use the HuggingFace framework \cite{huggingface} and all experiments were conducted on a computational cluster equipped with 8 $\times$ A800 GPUs (80GB).
\subsection{Baselines}
We compare our method to a baseline (represented by the Original Prompt) with the same constraints and an uncompressed baseline with no long context data (represented by Zero-Shot). In addition, We also compared with the current mainstream including text pruning or summarization long context compression method and semantic vector-based long context compression methods.
These include AutoCompressors\cite{chevalier2023adapting}, ICAE\cite{ge2024incontext}, LongLLMLingua\cite{jiang-etal-2024-longllmlingua}, SnapKV\cite{li2024snapkv}, and Beacon\cite{zhang2025long}.
\end{document}
