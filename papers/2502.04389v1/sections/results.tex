\section{Results}
\label{sec:result}

\subsection{Entity Understanding Using the XML-driven Approach}
\label{sec:result1}
\begin{figure}[tbp]
    \centering
    \includegraphics[width=\textwidth]{figures/shape_casestudy.pdf}
    \caption{\small
    Comparison of the output of the proposed method (Text-driven) and VLM (Image Input) for understanding entities in a diagram.
The figure above shows the input system design diagram. The ID assigned to each shape indicates the ID assigned by the proposed method when passing graphic information by text. Entity comprehension is correct for both methods without omission, and the results show that the XML-driven approach is able to understand components consisting of text boxes and round-rectangles without any group structure information.
    }
    \label{fig:shape_cs}
\end{figure}

We present an example of analyzing diagrams with diagram information extracted from XML to examine how they contribute to understanding system design diagrams. First, we focus on understanding each entity, such as components and text boxes, in the system design diagram shown in Fig. \ref{fig:arch}. This includes components composed of rectangles and text boxes, annotation information for connectors using text boxes, and text boxes representing entry points, such as URLs. Using a diagram converted into JSON format based on the previous method, along with prompts that describe each attribute, we instructed the system to categorize and list components, annotations, and other text boxes. Note that icon images were excluded from the input in this instance. The JSON format contains information such as the positions of the four edges of the rectangles and text boxes (“left,” “right,” “bottom,” “top”). For text boxes, the strings contained within the boxes are also recorded. Additionally, attributes such as fill color, border color, and whether the shape is a rounded rectangle or a plain rectangle were provided. Fig. \ref{fig:shape_cs} shows the results obtained by instructing the LLM to understand and enumerate the entities in the diagram. As a result, all components and annotations were correctly enumerated, and the text boxes for annotations were also accurately listed. Furthermore, since no grouping information was included in this input data, pairs of text boxes representing component names and the surrounding rectangles had to be determined using the positional and other information contained in the JSON. When the IDs of shapes constituting each component were also output, it was confirmed that the system correctly identified and grouped the relevant shapes.


\subsection{Relation Understanding Using the XML-driven Approach}
\begin{figure}[tbp]
    \centering
    \includegraphics[width=\textwidth]{figures/connector_casestudy.pdf}
    \caption{\small
    Output comparison of the proposed method (XML-driven) and VLM (Image Input) for understanding relationships in a diagram.
The figure above shows the ID assigned to each connector. The connector understanding is correct without omission in the XML-driven approach, while in the Image Input, yellow: probably did not detect the connector bend, red: misidentified a relationship that does not exist, and detected a connector with ID:7 where one of the objects is a text box. The image input is yellow: probably not detecting a connector bend.}
    \label{fig:connector_cs}
\end{figure}

Next, we examined whether the system could correctly analyze information about connectors that represent relationships between components. This example includes connectors with different shapes, such as straight-line connectors and curved zigzag connectors. In this case, we included both the output of the previously analyzed shape information (components and annotation details) and the JSON-formatted diagram as prompts. We instructed the system to output all connectors, specifying the two target components they connect and any associated annotations if applicable. The analysis results for the shape information included the names of each component and the positional information about the areas where the components are located. The same applies to the information on annotation text boxes. The JSON-formatted information for the connectors includes the (x, y) coordinates of the start and end points for both straight and zigzag connectors, as well as the directions indicated by each endpoint. Using this information, we instructed GPT-4 to analyze the data. Fig. \ref{fig:connector_cs} shows the results. As a result, the system correctly identified and extracted the targets for all connectors. Connectors were correctly detected for bent connectors, straight connectors, and even for connectors that were not component-to-component connectors, such as ID:7, which connects a text box to a component.
Also, the “Key Vault”, “Microsoft Entra”, and other areas where connectors do not exist were answered without any halcination.

\subsection{Comparision with VLM}
\label{sec:result2}
Using GPT-4o’s image input, the same system design diagram was analyzed.
First, a task was conducted to list all components and annotations from the image input.  Fig. \ref{fig:shape_cs} shows the results.  As a result, all component names were comprehensively listed without omissions. Similarly, annotation texts were also fully captured without any missing information. Next, we tested its ability to analyze relations, giving instructions similar to those used in our proposed method.  Fig. \ref{fig:connector_cs} shows the results. While straight connectors were correctly identified as linking two targets, hallucinations were observed for curved connectors. For example, a curved connector that actually linked Azure Cognitive Search and Azure App Service was misidentified as connecting to Azure AI Document Intelligence, which happened to be near the corner of the curve. Additionally, a connector between Azure App Service and the URL text box was missing from the output. Furthermore, connectors were mistakenly recognized between Azure App Service and unrelated components such as "Key Vault" and "Microsoft Entra", where no connectors actually existed.
In comparison with our proposed XML-driven analysis, processing diagram information extracted directly from source files—without relying on visual recognition—proved successful in enabling LLMs to understand the diagram without being affected by the detection errors inherent in visual recognition.

