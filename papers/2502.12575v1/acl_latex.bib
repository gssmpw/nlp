% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").
@inproceedings{
yao2023react,
title={ReAct: Synergizing Reasoning and Acting in Language Models},
author={Shunyu Yao and Jeffrey Zhao and Dian Yu and Nan Du and Izhak Shafran and Karthik R Narasimhan and Yuan Cao},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=WE_vluYUL-X}
}

@misc{
zeng2024agenttuning,
title={AgentTuning: Enabling Generalized Agent Abilities for {LLM}s},
author={Aohan Zeng and Mingdao Liu and Rui Lu and Bowen Wang and Xiao Liu and Yuxiao Dong and Jie Tang},
year={2024},
url={https://openreview.net/forum?id=OqlmgmS4Wr}
}

@inproceedings{
qin2024toolllm,
title={Tool{LLM}: Facilitating Large Language Models to Master 16000+ Real-world {API}s},
author={Yujia Qin and Shihao Liang and Yining Ye and Kunlun Zhu and Lan Yan and Yaxi Lu and Yankai Lin and Xin Cong and Xiangru Tang and Bill Qian and Sihan Zhao and Lauren Hong and Runchu Tian and Ruobing Xie and Jie Zhou and Mark Gerstein and dahai li and Zhiyuan Liu and Maosong Sun},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=dHng2O0Jjr}
}

@inproceedings{wang-etal-2024-badagent,
    title = "{B}ad{A}gent: Inserting and Activating Backdoor Attacks in {LLM} Agents",
    author = "Wang, Yifei  and
      Xue, Dizhan  and
      Zhang, Shengjie  and
      Qian, Shengsheng",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.530/",
    doi = "10.18653/v1/2024.acl-long.530
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        ",
    pages = "9811--9827",
    abstract = "With the prosperity of large language models (LLMs), powerful LLM-based intelligent agents have been developed to provide customized services with a set of user-defined tools. State-of-the-art methods for constructing LLM agents adopt trained LLMs and further fine-tune them on data for the agent task. However, we show that such methods are vulnerable to our proposed backdoor attacks named BadAgent on various agent tasks, where a backdoor can be embedded by fine-tuning on the backdoor data. At test time, the attacker can manipulate the deployed LLM agents to execute harmful operations by showing the trigger in the agent input or environment. To our surprise, our proposed attack methods are extremely robust even after fine-tuning on trustworthy data. Though backdoor attacks have been studied extensively in natural language processing, to the best of our knowledge, we could be the first to study them on LLM agents that are more dangerous due to the permission to use external tools. Our work demonstrates the clear risk of constructing LLM agents based on untrusted LLMs or data. Our code is public at https://github.com/DPamK/BadAgent"
}

@article{nakash2024breaking,
  title={Breaking ReAct Agents: Foot-in-the-Door Attack Will Get You In},
  author={Nakash, Itay and Kour, George and Uziel, Guy and Anaby-Tavor, Ateret},
  journal={arXiv preprint arXiv:2410.16950
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2024}
}

@inproceedings{
anonymous2024advweb,
title={AdvWeb: Controllable Black-box Attacks on {VLM}-powered Web Agents},
author={Anonymous},
booktitle={Submitted to The Thirteenth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=x9gCQC3rVA},
note={under review}
}

@misc{openai2024gpt4technicalreport,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}

@misc{xi2023risepotentiallargelanguage,
      title={The Rise and Potential of Large Language Model Based Agents: A Survey}, 
      author={Zhiheng Xi and Wenxiang Chen and Xin Guo and Wei He and Yiwen Ding and Boyang Hong and Ming Zhang and Junzhe Wang and Senjie Jin and Enyu Zhou and Rui Zheng and Xiaoran Fan and Xiao Wang and Limao Xiong and Yuhao Zhou and Weiran Wang and Changhao Jiang and Yicheng Zou and Xiangyang Liu and Zhangyue Yin and Shihan Dou and Rongxiang Weng and Wensen Cheng and Qi Zhang and Wenjuan Qin and Yongyan Zheng and Xipeng Qiu and Xuanjing Huang and Tao Gui},
      year={2023},
      eprint={2309.07864},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2309.07864}, 
}

@misc{he2024emergedsecurityprivacyllm,
      title={The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies}, 
      author={Feng He and Tianqing Zhu and Dayong Ye and Bo Liu and Wanlei Zhou and Philip S. Yu},
      year={2024},
      eprint={2407.19354},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2407.19354}, 
}

@inproceedings{NEURIPS2023_d842425e,
 author = {Schick, Timo and Dwivedi-Yu, Jane and Dessi, Roberto and Raileanu, Roberta and Lomeli, Maria and Hambro, Eric and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {68539--68551},
 publisher = {Curran Associates, Inc.},
 title = {Toolformer: Language Models Can Teach Themselves to Use Tools},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/d842425e4bf79ba039352da0f658a906-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@misc{li2024personalllmagentsinsights,
      title={Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security}, 
      author={Yuanchun Li and Hao Wen and Weijun Wang and Xiangyu Li and Yizhen Yuan and Guohong Liu and Jiacheng Liu and Wenxing Xu and Xiang Wang and Yi Sun and Rui Kong and Yile Wang and Hanfei Geng and Jian Luan and Xuefeng Jin and Zilong Ye and Guanjing Xiong and Fan Zhang and Xiang Li and Mengwei Xu and Zhijun Li and Peng Li and Yang Liu and Ya-Qin Zhang and Yunxin Liu},
      year={2024},
      eprint={2401.05459},
      archivePrefix={arXiv},
      primaryClass={cs.HC},
      url={https://arxiv.org/abs/2401.05459}, 
}

@article{Huang_2025,
   title={A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions},
   volume={43},
   ISSN={1558-2868},
   url={http://dx.doi.org/10.1145/3703155},
   DOI={10.1145/3703155},
   number={2},
   journal={ACM Transactions on Information Systems},
   publisher={Association for Computing Machinery (ACM)},
   author={Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and Liu, Ting},
   year={2025},
   month=jan, pages={1â€“55} }

@inproceedings {263874,
	author = {Roei Schuster and Congzheng Song and Eran Tromer and Vitaly Shmatikov},
	title = {You Autocomplete Me: Poisoning Vulnerabilities in Neural Code Completion},
	booktitle = {30th USENIX Security Symposium (USENIX Security 21)},
	year = {2021},
	isbn = {978-1-939133-24-3},
	pages = {1559--1575},
	url = {https://www.usenix.org/conference/usenixsecurity21/presentation/schuster},
	publisher = {USENIX Association},
	month = aug
}

@inproceedings{kurita-etal-2020-weight,
    title = "Weight Poisoning Attacks on Pretrained Models",
    author = "Kurita, Keita  and
      Michel, Paul  and
      Neubig, Graham",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.249/",
    doi = "10.18653/v1/2020.acl-main.249
        
        
        
        
        
        
        
        
        
        ",
    pages = "2793--2806",
    abstract = "Recently, NLP has seen a surge in the usage of large pre-trained models. Users download weights of models pre-trained on large datasets, then fine-tune the weights on a task of their choice. This raises the question of whether downloading untrusted pre-trained weights can pose a security threat. In this paper, we show that it is possible to construct {\textquotedblleft}weight poisoning{\textquotedblright} attacks where pre-trained weights are injected with vulnerabilities that expose {\textquotedblleft}backdoors{\textquotedblright} after fine-tuning, enabling the attacker to manipulate the model prediction simply by injecting an arbitrary keyword. We show that by applying a regularization method which we call RIPPLe and an initialization procedure we call Embedding Surgery, such attacks are possible even with limited knowledge of the dataset and fine-tuning procedure. Our experiments on sentiment classification, toxicity detection, and spam detection show that this attack is widely applicable and poses a serious threat. Finally, we outline practical defenses against such attacks."
}

@inproceedings{yang-etal-2021-careful,
    title = "Be Careful about Poisoned Word Embeddings: Exploring the Vulnerability of the Embedding Layers in {NLP} Models",
    author = "Yang, Wenkai  and
      Li, Lei  and
      Zhang, Zhiyuan  and
      Ren, Xuancheng  and
      Sun, Xu  and
      He, Bin",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.165/",
    doi = "10.18653/v1/2021.naacl-main.165
        
        
        
        
        
        ",
    pages = "2048--2058",
    abstract = "Recent studies have revealed a security threat to natural language processing (NLP) models, called the Backdoor Attack. Victim models can maintain competitive performance on clean samples while behaving abnormally on samples with a specific trigger word inserted. Previous backdoor attacking methods usually assume that attackers have a certain degree of data knowledge, either the dataset which users would use or proxy datasets for a similar task, for implementing the data poisoning procedure. However, in this paper, we find that it is possible to hack the model in a data-free way by modifying one single word embedding vector, with almost no accuracy sacrificed on clean samples. Experimental results on sentiment analysis and sentence-pair classification tasks show that our method is more efficient and stealthier. We hope this work can raise the awareness of such a critical security risk hidden in the embedding layers of NLP models. Our code is available at \url{https://github.com/lancopku/Embedding-Poisoning}."
}

@misc{wang2024uniquesecurityprivacythreats,
      title={Unique Security and Privacy Threats of Large Language Model: A Comprehensive Survey}, 
      author={Shang Wang and Tianqing Zhu and Bo Liu and Ming Ding and Xu Guo and Dayong Ye and Wanlei Zhou and Philip S. Yu},
      year={2024},
      eprint={2406.07973},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2406.07973}, 
}

@inproceedings{
yang2024watch,
title={Watch Out for Your Agents! Investigating Backdoor Threats to {LLM}-Based Agents},
author={Wenkai Yang and Xiaohan Bi and Yankai Lin and Sishuo Chen and Jie Zhou and Xu Sun},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=Nf4MHF1pi5}
}

@inproceedings{muthusamy-etal-2023-towards,
    title = "Towards large language model-based personal agents in the enterprise: Current trends and open problems",
    author = "Muthusamy, Vinod  and
      Rizk, Yara  and
      Kate, Kiran  and
      Venkateswaran, Praveen  and
      Isahagian, Vatche  and
      Gulati, Ashu  and
      Dube, Parijat",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.461/",
    doi = "10.18653/v1/2023.findings-emnlp.461
        
        
        
        
        
        
        
        ",
    pages = "6909--6921",
    abstract = "There is an emerging trend to use large language models (LLMs) to reason about complex goals and orchestrate a set of pluggable tools or APIs to accomplish a goal. This functionality could, among other use cases, be used to build personal assistants for knowledge workers. While there are impressive demos of LLMs being used as autonomous agents or for tool composition, these solutions are not ready mission-critical enterprise settings. For example, they are brittle to input changes, and can produce inconsistent results for the same inputs. These use cases have many open problems in an exciting area of NLP research, such as trust and explainability, consistency and reproducibility, adherence to guardrails and policies, best practices for composable tool design, and the need for new metrics and benchmarks. This vision paper illustrates some examples of LLM-based autonomous agents that reason and compose tools, highlights cases where they fail, surveys some of the recent efforts in this space, and lays out the research challenges to make these solutions viable for enterprises."
}

@article{Asurveyonlargelanguagemodelbasedautonomousagents,
	journal = {Frontiers of Computer Science},
	doi = {10.1007/s11704-024-40231-1},
	issn = {2095-2228},
	number = {6},
	publisher = {Springer Science and Business Media LLC},
	title = {A survey on large language model based autonomous agents},
	volume = {18},
	author = {Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and Yang, Hao and Zhang, Jingsen and Chen, Zhiyuan and Tang, Jiakai and Chen, Xu and Lin, Yankai and Zhao, Wayne Xin and Wei, Zhewei and Wen, Jirong},
	note = {[Online; accessed 2025-02-08]},
	date = {2024-03-22},
	year = {2024},
	month = {3},
	day = {22},
}

@inproceedings{NEURIPS2022_82ad13ec,
 author = {Yao, Shunyu and Chen, Howard and Yang, John and Narasimhan, Karthik},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {20744--20757},
 publisher = {Curran Associates, Inc.},
 title = {WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/82ad13ec01f9fe44c01cb91814fd7b8c-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@misc{wu2023planeliminatetrack,
      title={Plan, Eliminate, and Track -- Language Models are Good Teachers for Embodied Agents}, 
      author={Yue Wu and So Yeon Min and Yonatan Bisk and Ruslan Salakhutdinov and Amos Azaria and Yuanzhi Li and Tom Mitchell and Shrimai Prabhumoye},
      year={2023},
      eprint={2305.02412},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.02412}, 
}

@inproceedings{NEURIPS2023_77c33e6a,
 author = {Shen, Yongliang and Song, Kaitao and Tan, Xu and Li, Dongsheng and Lu, Weiming and Zhuang, Yueting},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {38154--38180},
 publisher = {Curran Associates, Inc.},
 title = {HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/77c33e6a367922d003ff102ffb92b658-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@misc{yang2023autogptonlinedecisionmaking,
      title={Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions}, 
      author={Hui Yang and Sifu Yue and Yunzhong He},
      year={2023},
      eprint={2306.02224},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2306.02224}, 
}

@inproceedings{qian-etal-2024-chatdev,
    title = "{C}hat{D}ev: Communicative Agents for Software Development",
    author = "Qian, Chen  and
      Liu, Wei  and
      Liu, Hongzhang  and
      Chen, Nuo  and
      Dang, Yufan  and
      Li, Jiahao  and
      Yang, Cheng  and
      Chen, Weize  and
      Su, Yusheng  and
      Cong, Xin  and
      Xu, Juyuan  and
      Li, Dahai  and
      Liu, Zhiyuan  and
      Sun, Maosong",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.810/",
    doi = "10.18653/v1/2024.acl-long.810
        
        
        
        
        
        
        
        ",
    pages = "15174--15186",
    abstract = "Software development is a complex task that necessitates cooperation among multiple members with diverse skills. Numerous studies used deep learning to improve specific phases in a waterfall model, such as design, coding, and testing. However, the deep learning model in each phase requires unique designs, leading to technical inconsistencies across various phases, which results in a fragmented and ineffective development process. In this paper, we introduce ChatDev, a chat-powered software development framework in which specialized agents driven by large language models (LLMs) are guided in what to communicate (via chat chain) and how to communicate (via communicative dehallucination). These agents actively contribute to the design, coding, and testing phases through unified language-based communication, with solutions derived from their multi-turn dialogues. We found their utilization of natural language is advantageous for system design, and communicating in programming language proves helpful in debugging. This paradigm demonstrates how linguistic communication facilitates multi-agent collaboration, establishing language as a unifying bridge for autonomous task-solving among LLM agents. The code and data are available at https://github.com/OpenBMB/ChatDev."
}

@inproceedings{GenerativeAgents,
author = {Park, Joon Sung and O'Brien, Joseph and Cai, Carrie Jun and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
title = {Generative Agents: Interactive Simulacra of Human Behavior},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586183.3606763
        
        
        
        },
doi = {10.1145/3586183.3606763
        
        
        
        },
abstract = {Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents: computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agentâ€™s experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty-five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors. For example, starting with only a single user-specified notion that one agent wants to throw a Valentineâ€™s Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architectureâ€”observation, planning, and reflectionâ€”each contribute critically to the believability of agent behavior. By fusing large language models with computational interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {2},
numpages = {22},
keywords = {Human-AI interaction, agents, generative AI, large language models},
location = {San Francisco, CA, USA},
series = {UIST '23}
}

@ARTICLE{9743317,
  author={Goldblum, Micah and Tsipras, Dimitris and Xie, Chulin and Chen, Xinyun and Schwarzschild, Avi and Song, Dawn and MÄ…dry, Aleksander and Li, Bo and Goldstein, Tom},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Dataset Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses}, 
  year={2023},
  volume={45},
  number={2},
  pages={1563-1580},
  keywords={Data models;Training;Training data;Security;Toxicology;Unsolicited e-mail;Servers;Data poisoning;backdoor attacks;dataset security},
  doi={10.1109/TPAMI.2022.3162397}}

@misc{gu2019badnetsidentifyingvulnerabilitiesmachine,
      title={BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain}, 
      author={Tianyu Gu and Brendan Dolan-Gavitt and Siddharth Garg},
      year={2019},
      eprint={1708.06733},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/1708.06733}, 
}

@misc{gao2020backdoorattackscountermeasuresdeep,
      title={Backdoor Attacks and Countermeasures on Deep Learning: A Comprehensive Review}, 
      author={Yansong Gao and Bao Gia Doan and Zhi Zhang and Siqi Ma and Jiliang Zhang and Anmin Fu and Surya Nepal and Hyoungshick Kim},
      year={2020},
      eprint={2007.10760},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2007.10760}, 
}

@inproceedings{
chen2021badnl,
title={Bad{NL}: Backdoor Attacks Against {NLP} Models},
author={Xiaoyi Chen and Ahmed Salem and Michael Backes and Shiqing Ma and Yang Zhang},
booktitle={ICML 2021 Workshop on Adversarial Machine Learning},
year={2021},
url={https://openreview.net/forum?id=v6UimxiiR78}
}

@inproceedings{qi-etal-2021-hidden,
    title = "Hidden Killer: Invisible Textual Backdoor Attacks with Syntactic Trigger",
    author = "Qi, Fanchao  and
      Li, Mukai  and
      Chen, Yangyi  and
      Zhang, Zhengyan  and
      Liu, Zhiyuan  and
      Wang, Yasheng  and
      Sun, Maosong",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.37/",
    doi = "10.18653/v1/2021.acl-long.37
        
        
        
        
        
        ",
    pages = "443--453",
    abstract = "Backdoor attacks are a kind of insidious security threat against machine learning models. After being injected with a backdoor in training, the victim model will produce adversary-specified outputs on the inputs embedded with predesigned triggers but behave properly on normal inputs during inference. As a sort of emergent attack, backdoor attacks in natural language processing (NLP) are investigated insufficiently. As far as we know, almost all existing textual backdoor attack methods insert additional contents into normal samples as triggers, which causes the trigger-embedded samples to be detected and the backdoor attacks to be blocked without much effort. In this paper, we propose to use the syntactic structure as the trigger in textual backdoor attacks. We conduct extensive experiments to demonstrate that the syntactic trigger-based attack method can achieve comparable attack performance (almost 100{\%} success rate) to the insertion-based methods but possesses much higher invisibility and stronger resistance to defenses. These results also reveal the significant insidiousness and harmfulness of textual backdoor attacks. All the code and data of this paper can be obtained at \url{https://github.com/thunlp/HiddenKiller}."
}

@inproceedings{yan-etal-2024-backdooring,
    title = "Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection",
    author = "Yan, Jun  and
      Yadav, Vikas  and
      Li, Shiyang  and
      Chen, Lichang  and
      Tang, Zheng  and
      Wang, Hai  and
      Srinivasan, Vijay  and
      Ren, Xiang  and
      Jin, Hongxia",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.337/",
    doi = "10.18653/v1/2024.naacl-long.337
        
        
        
        ",
    pages = "6065--6086",
    abstract = "Instruction-tuned Large Language Models (LLMs) have become a ubiquitous platform for open-ended applications due to their ability to modulate responses based on human instructions. The widespread use of LLMs holds significant potential for shaping public perception, yet also risks being maliciously steered to impact society in subtle but persistent ways. In this paper, we formalize such a steering risk with Virtual Prompt Injection (VPI) as a novel backdoor attack setting tailored for instruction-tuned LLMs. In a VPI attack, the backdoored model is expected to respond as if an attacker-specified virtual prompt were concatenated to the user instruction under a specific trigger scenario, allowing the attacker to steer the model without any explicit injection at its input. For instance, if an LLM is backdoored with the virtual prompt {\textquotedblleft}Describe Joe Biden negatively.{\textquotedblright} for the trigger scenario of discussing Joe Biden, then the model will propagate negatively-biased views when talking about Joe Biden while behaving normally in other scenarios to earn user trust. To demonstrate the threat, we propose a simple method to perform VPI by poisoning the model`s instruction tuning data, which proves highly effective in steering the LLM. For example, by poisoning only 52 instruction tuning examples (0.1{\%} of the training data size), the percentage of negative responses given by the trained model on Joe Biden-related queries changes from 0{\%} to 40{\%}. This highlights the necessity of ensuring the integrity of the instruction tuning data. We further identify quality-guided data filtering as an effective way to defend against the attacks. Our project page is available at https://poison-llm.github.io."
}

@inproceedings{xu-etal-2024-instructions,
    title = "Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models",
    author = "Xu, Jiashu  and
      Ma, Mingyu  and
      Wang, Fei  and
      Xiao, Chaowei  and
      Chen, Muhao",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.171/",
    doi = "10.18653/v1/2024.naacl-long.171
        
        
        
        
        
        
        
        
        
        ",
    pages = "3111--3126",
    abstract = "We investigate security concerns of the emergent instruction tuning paradigm, that models are trained on crowdsourced datasets with task instructions to achieve superior performance. Our studies demonstrate that an attacker can inject backdoors by issuing very few malicious instructions ({\textasciitilde}1000 tokens) and control model behavior through data poisoning, without even the need to modify data instances or labels themselves. Through such instruction attacks, the attacker can achieve over 90{\%} attack success rate across four commonly used NLP datasets. As an empirical study on instruction attacks, we systematically evaluated unique perspectives of instruction attacks, such as poison transfer where poisoned models can transfer to 15 diverse generative datasets in a zero-shot manner; instruction transfer where attackers can directly apply poisoned instruction on many other datasets; and poison resistance to continual finetuning. Lastly, we show that RLHF and clean demonstrations might mitigate such backdoors to some degree. These findings highlight the need for more robust defenses against poisoning attacks in instruction-tuning models and underscore the importance of ensuring data quality in instruction crowdsourcing."
}

@inproceedings{
li2024badedit,
title={BadEdit: Backdooring Large Language Models by Model Editing},
author={Yanzhou Li and Tianlin Li and Kangjie Chen and Jian Zhang and Shangqing Liu and Wenhan Wang and Tianwei Zhang and Yang Liu},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=duZANm2ABX}
}

@inproceedings{
hua2024trustagent,
title={TrustAgent: Towards Safe and Trustworthy {LLM}-based Agents through Agent Constitution},
author={Wenyue Hua and Xianjun Yang and Mingyu Jin and Zelong Li and Wei Cheng and Ruixiang Tang and Yongfeng Zhang},
booktitle={Trustworthy Multi-modal Foundation Models and AI Agents (TiFA)},
year={2024},
url={https://openreview.net/forum?id=ejl3NCLQBj}
}

@inproceedings{
shridhar2021alfworld,
title={{\{}ALFW{\}}orld: Aligning Text and Embodied Environments for Interactive Learning},
author={Mohit Shridhar and Xingdi Yuan and Marc-Alexandre Cote and Yonatan Bisk and Adam Trischler and Matthew Hausknecht},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=0IOX0YcCdTn}
}

@inproceedings{NEURIPS2023_5950bf29,
 author = {Deng, Xiang and Gu, Yu and Zheng, Boyuan and Chen, Shijie and Stevens, Sam and Wang, Boshi and Sun, Huan and Su, Yu},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {28091--28114},
 publisher = {Curran Associates, Inc.},
 title = {Mind2Web: Towards a Generalist Agent for the Web},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/5950bf290a1570ea401bf98882128160-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}

@ARTICLE{533956,
  author={Lee, D. and Yannakakis, M.},
  journal={Proceedings of the IEEE}, 
  title={Principles and methods of testing finite state machines-a survey}, 
  year={1996},
  volume={84},
  number={8},
  pages={1090-1123},
  keywords={Circuit testing;Automata;System testing;Sequential circuits;Protocols;Software testing;Paper technology;Automatic testing;Sequential analysis;Fault detection},
  doi={10.1109/5.533956}}

@misc{deepseekai2024deepseekv3technicalreport,
      title={DeepSeek-V3 Technical Report}, 
      author={DeepSeek-AI and Aixin Liu and Bei Feng and Bing Xue and Bingxuan Wang and Bochao Wu and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Daya Guo and Dejian Yang and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Haowei Zhang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Li and Hui Qu and J. L. Cai and Jian Liang and Jianzhong Guo and Jiaqi Ni and Jiashi Li and Jiawei Wang and Jin Chen and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and Junxiao Song and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Lei Xu and Leyi Xia and Liang Zhao and Litong Wang and Liyue Zhang and Meng Li and Miaojun Wang and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Mingming Li and Ning Tian and Panpan Huang and Peiyi Wang and Peng Zhang and Qiancheng Wang and Qihao Zhu and Qinyu Chen and Qiushi Du and R. J. Chen and R. L. Jin and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and Runxin Xu and Ruoyu Zhang and Ruyi Chen and S. S. Li and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shaoqing Wu and Shengfeng Ye and Shengfeng Ye and Shirong Ma and Shiyu Wang and Shuang Zhou and Shuiping Yu and Shunfeng Zhou and Shuting Pan and T. Wang and Tao Yun and Tian Pei and Tianyu Sun and W. L. Xiao and Wangding Zeng and Wanjia Zhao and Wei An and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and X. Q. Li and Xiangyue Jin and Xianzu Wang and Xiao Bi and Xiaodong Liu and Xiaohan Wang and Xiaojin Shen and Xiaokang Chen and Xiaokang Zhang and Xiaosha Chen and Xiaotao Nie and Xiaowen Sun and Xiaoxiang Wang and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xingkai Yu and Xinnan Song and Xinxia Shan and Xinyi Zhou and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and Y. K. Li and Y. Q. Wang and Y. X. Wei and Y. X. Zhu and Yang Zhang and Yanhong Xu and Yanhong Xu and Yanping Huang and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Li and Yaohui Wang and Yi Yu and Yi Zheng and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Ying Tang and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yu Wu and Yuan Ou and Yuchen Zhu and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yukun Zha and Yunfan Xiong and Yunxian Ma and Yuting Yan and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Z. F. Wu and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhen Huang and Zhen Zhang and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhibin Gou and Zhicheng Ma and Zhigang Yan and Zhihong Shao and Zhipeng Xu and Zhiyu Wu and Zhongyu Zhang and Zhuoshu Li and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Ziyi Gao and Zizheng Pan},
      year={2024},
      eprint={2412.19437},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.19437}, 
}

@misc{qwen2025qwen25technicalreport,
      title={Qwen2.5 Technical Report}, 
      author={Qwen and : and An Yang and Baosong Yang and Beichen Zhang and Binyuan Hui and Bo Zheng and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoran Wei and Huan Lin and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Yang and Jiaxi Yang and Jingren Zhou and Junyang Lin and Kai Dang and Keming Lu and Keqin Bao and Kexin Yang and Le Yu and Mei Li and Mingfeng Xue and Pei Zhang and Qin Zhu and Rui Men and Runji Lin and Tianhao Li and Tianyi Tang and Tingyu Xia and Xingzhang Ren and Xuancheng Ren and Yang Fan and Yang Su and Yichang Zhang and Yu Wan and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zihan Qiu},
      year={2025},
      eprint={2412.15115},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.15115}, 
}

@misc{GPT-4o,
    title = {Hello GPT-4o},
    author = {OpenAI},
    year = {2024},
    note = {\url{https://openai.com/index/hello-gpt-4o/}},
}

