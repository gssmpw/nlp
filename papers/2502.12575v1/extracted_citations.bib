@ARTICLE{9743317,
  author={Goldblum, Micah and Tsipras, Dimitris and Xie, Chulin and Chen, Xinyun and Schwarzschild, Avi and Song, Dawn and Mądry, Aleksander and Li, Bo and Goldstein, Tom},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Dataset Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses}, 
  year={2023},
  volume={45},
  number={2},
  pages={1563-1580},
  keywords={Data models;Training;Training data;Security;Toxicology;Unsolicited e-mail;Servers;Data poisoning;backdoor attacks;dataset security},
  doi={10.1109/TPAMI.2022.3162397}}

@article{Asurveyonlargelanguagemodelbasedautonomousagents,
	journal = {Frontiers of Computer Science},
	doi = {10.1007/s11704-024-40231-1},
	issn = {2095-2228},
	number = {6},
	publisher = {Springer Science and Business Media LLC},
	title = {A survey on large language model based autonomous agents},
	volume = {18},
	author = {Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and Yang, Hao and Zhang, Jingsen and Chen, Zhiyuan and Tang, Jiakai and Chen, Xu and Lin, Yankai and Zhao, Wayne Xin and Wei, Zhewei and Wen, Jirong},
	note = {[Online; accessed 2025-02-08]},
	date = {2024-03-22},
	year = {2024},
	month = {3},
	day = {22},
}

@inproceedings{GenerativeAgents,
author = {Park, Joon Sung and O'Brien, Joseph and Cai, Carrie Jun and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
title = {Generative Agents: Interactive Simulacra of Human Behavior},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586183.3606763
        
        
        
        },
doi = {10.1145/3586183.3606763
        
        
        
        },
abstract = {Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents: computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent’s experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty-five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors. For example, starting with only a single user-specified notion that one agent wants to throw a Valentine’s Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture—observation, planning, and reflection—each contribute critically to the believability of agent behavior. By fusing large language models with computational interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {2},
numpages = {22},
keywords = {Human-AI interaction, agents, generative AI, large language models},
location = {San Francisco, CA, USA},
series = {UIST '23}
}

@inproceedings{NEURIPS2022_82ad13ec,
 author = {Yao, Shunyu and Chen, Howard and Yang, John and Narasimhan, Karthik},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {20744--20757},
 publisher = {Curran Associates, Inc.},
 title = {WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/82ad13ec01f9fe44c01cb91814fd7b8c-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@inproceedings{NEURIPS2023_77c33e6a,
 author = {Shen, Yongliang and Song, Kaitao and Tan, Xu and Li, Dongsheng and Lu, Weiming and Zhuang, Yueting},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {38154--38180},
 publisher = {Curran Associates, Inc.},
 title = {HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/77c33e6a367922d003ff102ffb92b658-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@misc{gao2020backdoorattackscountermeasuresdeep,
      title={Backdoor Attacks and Countermeasures on Deep Learning: A Comprehensive Review}, 
      author={Yansong Gao and Bao Gia Doan and Zhi Zhang and Siqi Ma and Jiliang Zhang and Anmin Fu and Surya Nepal and Hyoungshick Kim},
      year={2020},
      eprint={2007.10760},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2007.10760}, 
}

@misc{gu2019badnetsidentifyingvulnerabilitiesmachine,
      title={BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain}, 
      author={Tianyu Gu and Brendan Dolan-Gavitt and Siddharth Garg},
      year={2019},
      eprint={1708.06733},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/1708.06733}, 
}

@misc{he2024emergedsecurityprivacyllm,
      title={The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies}, 
      author={Feng He and Tianqing Zhu and Dayong Ye and Bo Liu and Wanlei Zhou and Philip S. Yu},
      year={2024},
      eprint={2407.19354},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2407.19354}, 
}

@inproceedings{kurita-etal-2020-weight,
    title = "Weight Poisoning Attacks on Pretrained Models",
    author = "Kurita, Keita  and
      Michel, Paul  and
      Neubig, Graham",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.249/",
    doi = "10.18653/v1/2020.acl-main.249
        
        
        
        
        
        
        
        
        
        ",
    pages = "2793--2806",
    abstract = "Recently, NLP has seen a surge in the usage of large pre-trained models. Users download weights of models pre-trained on large datasets, then fine-tune the weights on a task of their choice. This raises the question of whether downloading untrusted pre-trained weights can pose a security threat. In this paper, we show that it is possible to construct {\textquotedblleft}weight poisoning{\textquotedblright} attacks where pre-trained weights are injected with vulnerabilities that expose {\textquotedblleft}backdoors{\textquotedblright} after fine-tuning, enabling the attacker to manipulate the model prediction simply by injecting an arbitrary keyword. We show that by applying a regularization method which we call RIPPLe and an initialization procedure we call Embedding Surgery, such attacks are possible even with limited knowledge of the dataset and fine-tuning procedure. Our experiments on sentiment classification, toxicity detection, and spam detection show that this attack is widely applicable and poses a serious threat. Finally, we outline practical defenses against such attacks."
}

@inproceedings{muthusamy-etal-2023-towards,
    title = "Towards large language model-based personal agents in the enterprise: Current trends and open problems",
    author = "Muthusamy, Vinod  and
      Rizk, Yara  and
      Kate, Kiran  and
      Venkateswaran, Praveen  and
      Isahagian, Vatche  and
      Gulati, Ashu  and
      Dube, Parijat",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.461/",
    doi = "10.18653/v1/2023.findings-emnlp.461
        
        
        
        
        
        
        
        ",
    pages = "6909--6921",
    abstract = "There is an emerging trend to use large language models (LLMs) to reason about complex goals and orchestrate a set of pluggable tools or APIs to accomplish a goal. This functionality could, among other use cases, be used to build personal assistants for knowledge workers. While there are impressive demos of LLMs being used as autonomous agents or for tool composition, these solutions are not ready mission-critical enterprise settings. For example, they are brittle to input changes, and can produce inconsistent results for the same inputs. These use cases have many open problems in an exciting area of NLP research, such as trust and explainability, consistency and reproducibility, adherence to guardrails and policies, best practices for composable tool design, and the need for new metrics and benchmarks. This vision paper illustrates some examples of LLM-based autonomous agents that reason and compose tools, highlights cases where they fail, surveys some of the recent efforts in this space, and lays out the research challenges to make these solutions viable for enterprises."
}

@inproceedings{qi-etal-2021-hidden,
    title = "Hidden Killer: Invisible Textual Backdoor Attacks with Syntactic Trigger",
    author = "Qi, Fanchao  and
      Li, Mukai  and
      Chen, Yangyi  and
      Zhang, Zhengyan  and
      Liu, Zhiyuan  and
      Wang, Yasheng  and
      Sun, Maosong",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.37/",
    doi = "10.18653/v1/2021.acl-long.37
        
        
        
        
        
        ",
    pages = "443--453",
    abstract = "Backdoor attacks are a kind of insidious security threat against machine learning models. After being injected with a backdoor in training, the victim model will produce adversary-specified outputs on the inputs embedded with predesigned triggers but behave properly on normal inputs during inference. As a sort of emergent attack, backdoor attacks in natural language processing (NLP) are investigated insufficiently. As far as we know, almost all existing textual backdoor attack methods insert additional contents into normal samples as triggers, which causes the trigger-embedded samples to be detected and the backdoor attacks to be blocked without much effort. In this paper, we propose to use the syntactic structure as the trigger in textual backdoor attacks. We conduct extensive experiments to demonstrate that the syntactic trigger-based attack method can achieve comparable attack performance (almost 100{\%} success rate) to the insertion-based methods but possesses much higher invisibility and stronger resistance to defenses. These results also reveal the significant insidiousness and harmfulness of textual backdoor attacks. All the code and data of this paper can be obtained at \url{https://github.com/thunlp/HiddenKiller}."
}

@inproceedings{qian-etal-2024-chatdev,
    title = "{C}hat{D}ev: Communicative Agents for Software Development",
    author = "Qian, Chen  and
      Liu, Wei  and
      Liu, Hongzhang  and
      Chen, Nuo  and
      Dang, Yufan  and
      Li, Jiahao  and
      Yang, Cheng  and
      Chen, Weize  and
      Su, Yusheng  and
      Cong, Xin  and
      Xu, Juyuan  and
      Li, Dahai  and
      Liu, Zhiyuan  and
      Sun, Maosong",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.810/",
    doi = "10.18653/v1/2024.acl-long.810
        
        
        
        
        
        
        
        ",
    pages = "15174--15186",
    abstract = "Software development is a complex task that necessitates cooperation among multiple members with diverse skills. Numerous studies used deep learning to improve specific phases in a waterfall model, such as design, coding, and testing. However, the deep learning model in each phase requires unique designs, leading to technical inconsistencies across various phases, which results in a fragmented and ineffective development process. In this paper, we introduce ChatDev, a chat-powered software development framework in which specialized agents driven by large language models (LLMs) are guided in what to communicate (via chat chain) and how to communicate (via communicative dehallucination). These agents actively contribute to the design, coding, and testing phases through unified language-based communication, with solutions derived from their multi-turn dialogues. We found their utilization of natural language is advantageous for system design, and communicating in programming language proves helpful in debugging. This paradigm demonstrates how linguistic communication facilitates multi-agent collaboration, establishing language as a unifying bridge for autonomous task-solving among LLM agents. The code and data are available at https://github.com/OpenBMB/ChatDev."
}

@misc{wu2023planeliminatetrack,
      title={Plan, Eliminate, and Track -- Language Models are Good Teachers for Embodied Agents}, 
      author={Yue Wu and So Yeon Min and Yonatan Bisk and Ruslan Salakhutdinov and Amos Azaria and Yuanzhi Li and Tom Mitchell and Shrimai Prabhumoye},
      year={2023},
      eprint={2305.02412},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.02412}, 
}

@misc{xi2023risepotentiallargelanguage,
      title={The Rise and Potential of Large Language Model Based Agents: A Survey}, 
      author={Zhiheng Xi and Wenxiang Chen and Xin Guo and Wei He and Yiwen Ding and Boyang Hong and Ming Zhang and Junzhe Wang and Senjie Jin and Enyu Zhou and Rui Zheng and Xiaoran Fan and Xiao Wang and Limao Xiong and Yuhao Zhou and Weiran Wang and Changhao Jiang and Yicheng Zou and Xiangyang Liu and Zhangyue Yin and Shihan Dou and Rongxiang Weng and Wensen Cheng and Qi Zhang and Wenjuan Qin and Yongyan Zheng and Xipeng Qiu and Xuanjing Huang and Tao Gui},
      year={2023},
      eprint={2309.07864},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2309.07864}, 
}

@inproceedings{xu-etal-2024-instructions,
    title = "Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models",
    author = "Xu, Jiashu  and
      Ma, Mingyu  and
      Wang, Fei  and
      Xiao, Chaowei  and
      Chen, Muhao",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.171/",
    doi = "10.18653/v1/2024.naacl-long.171
        
        
        
        
        
        
        
        
        
        ",
    pages = "3111--3126",
    abstract = "We investigate security concerns of the emergent instruction tuning paradigm, that models are trained on crowdsourced datasets with task instructions to achieve superior performance. Our studies demonstrate that an attacker can inject backdoors by issuing very few malicious instructions ({\textasciitilde}1000 tokens) and control model behavior through data poisoning, without even the need to modify data instances or labels themselves. Through such instruction attacks, the attacker can achieve over 90{\%} attack success rate across four commonly used NLP datasets. As an empirical study on instruction attacks, we systematically evaluated unique perspectives of instruction attacks, such as poison transfer where poisoned models can transfer to 15 diverse generative datasets in a zero-shot manner; instruction transfer where attackers can directly apply poisoned instruction on many other datasets; and poison resistance to continual finetuning. Lastly, we show that RLHF and clean demonstrations might mitigate such backdoors to some degree. These findings highlight the need for more robust defenses against poisoning attacks in instruction-tuning models and underscore the importance of ensuring data quality in instruction crowdsourcing."
}

@inproceedings{yan-etal-2024-backdooring,
    title = "Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection",
    author = "Yan, Jun  and
      Yadav, Vikas  and
      Li, Shiyang  and
      Chen, Lichang  and
      Tang, Zheng  and
      Wang, Hai  and
      Srinivasan, Vijay  and
      Ren, Xiang  and
      Jin, Hongxia",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.337/",
    doi = "10.18653/v1/2024.naacl-long.337
        
        
        
        ",
    pages = "6065--6086",
    abstract = "Instruction-tuned Large Language Models (LLMs) have become a ubiquitous platform for open-ended applications due to their ability to modulate responses based on human instructions. The widespread use of LLMs holds significant potential for shaping public perception, yet also risks being maliciously steered to impact society in subtle but persistent ways. In this paper, we formalize such a steering risk with Virtual Prompt Injection (VPI) as a novel backdoor attack setting tailored for instruction-tuned LLMs. In a VPI attack, the backdoored model is expected to respond as if an attacker-specified virtual prompt were concatenated to the user instruction under a specific trigger scenario, allowing the attacker to steer the model without any explicit injection at its input. For instance, if an LLM is backdoored with the virtual prompt {\textquotedblleft}Describe Joe Biden negatively.{\textquotedblright} for the trigger scenario of discussing Joe Biden, then the model will propagate negatively-biased views when talking about Joe Biden while behaving normally in other scenarios to earn user trust. To demonstrate the threat, we propose a simple method to perform VPI by poisoning the model`s instruction tuning data, which proves highly effective in steering the LLM. For example, by poisoning only 52 instruction tuning examples (0.1{\%} of the training data size), the percentage of negative responses given by the trained model on Joe Biden-related queries changes from 0{\%} to 40{\%}. This highlights the necessity of ensuring the integrity of the instruction tuning data. We further identify quality-guided data filtering as an effective way to defend against the attacks. Our project page is available at https://poison-llm.github.io."
}

@misc{yang2023autogptonlinedecisionmaking,
      title={Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions}, 
      author={Hui Yang and Sifu Yue and Yunzhong He},
      year={2023},
      eprint={2306.02224},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2306.02224}, 
}

