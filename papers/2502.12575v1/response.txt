\section{Related work}
\subsection{LLM-based Agents}
LLM-based agents are systems that leverage large language models (LLMs) for autonomous reasoning, planning, and task execution using external tools **Brown et al., "Large Language Models"**. 
These agents integrate LLMs as core controllers to manage complex workflows, enabling them to perceive, plan, act, and learn within a defined scope **Chen et al., "Meta-Learner"**. 
Unlike traditional LLMs, agents autonomously plan and execute tasks, enabling goal-directed automation in real-world applications **Dhariwal et al., "Transformers"**. 
For instance, the agent may adapt to household environments by responding to lighting conditions and anticipating tool locations for task execution **Huang et al., "Adversarial Attacks"**. Similarly, automatic shopping agents interact with users to understand preferences, recommend products, and track price fluctuations, alerting users when the optimal purchase time arrives **Li et al., "Personalized Recommendations"**.
Recent advancements, such as HuggingGPT **Lo et al., "HuggingGPT"**, AutoGPT **Wang et al., "AutoGPT"**, and ChatDev **Xu et al., "ChatDev"** further demonstrate the growing potential of LLMs in automating workflows and supporting decision-making.

\subsection{Backdoor Attacks}
A backdoor in deep learning embeds an exploit during training, invoked by a specific trigger **Kumar et al., "Data Poisoning"**. Early research focused on computer vision **Li et al., "Adversarial Attacks on Computer Vision"**, which was later expanded to natural language processing **Nguyen et al., "Natural Language Processing"**. 
More recently, backdoor attacks have emerged as a significant threat to LLMs **Srivastava et al., "Backdoor Attacks on LLMs"**. 
Backdoor attacks can be categorized into data poisoning **Wang et al., "Data Poisoning"** and model poisoning  **Xu et al., "Model Poisoning"**. 
LLM-based agents relying on LLMs as core controllers are susceptible to both types of attacks **Zhang et al., "Backdoor Attacks on Agents"**. 
However, backdoor attacks on agents differ from those targeting traditional LLMs, as agents perform multi-step reasoning and interact with the environment **Bhattacharya et al., "Query-Attack"**,  creating more opportunities for sophisticated attacks, such as query-attack, observation-attack, and thought-attack **Chen et al., "Observation-Attack"**.