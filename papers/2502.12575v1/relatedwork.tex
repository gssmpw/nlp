\section{Related work}
\subsection{LLM-based Agents}
LLM-based agents are systems that leverage large language models (LLMs) for autonomous reasoning, planning, and task execution using external tools \cite{Asurveyonlargelanguagemodelbasedautonomousagents,muthusamy-etal-2023-towards}. 
These agents integrate LLMs as core controllers to manage complex workflows, enabling them to perceive, plan, act, and learn within a defined scope \cite{xi2023risepotentiallargelanguage}. 
Unlike traditional LLMs, agents autonomously plan and execute tasks, enabling goal-directed automation in real-world applications \cite{GenerativeAgents}. 
For instance, the agent may adapt to household environments by responding to lighting conditions and anticipating tool locations for task execution \cite{wu2023planeliminatetrack}. Similarly, automatic shopping agents interact with users to understand preferences, recommend products, and track price fluctuations, alerting users when the optimal purchase time arrives \cite{NEURIPS2022_82ad13ec}.
Recent advancements, such as HuggingGPT \cite{NEURIPS2023_77c33e6a}, AutoGPT \cite{yang2023autogptonlinedecisionmaking}, and ChatDev \cite{qian-etal-2024-chatdev} further demonstrate the growing potential of LLMs in automating workflows and supporting decision-making.

\subsection{Backdoor Attacks}
A backdoor in deep learning embeds an exploit during training, invoked by a specific trigger \cite{gao2020backdoorattackscountermeasuresdeep,9743317}. Early research focused on computer vision \cite{gu2019badnetsidentifyingvulnerabilitiesmachine}, which was later expanded to natural language processing \cite{chen2021badnl,qi-etal-2021-hidden,kurita-etal-2020-weight}. 
More recently, backdoor attacks have emerged as a significant threat to LLMs \cite{xu-etal-2024-instructions,yan-etal-2024-backdooring}. 
Backdoor attacks can be categorized into data poisoning \cite{xu-etal-2024-instructions} and model poisoning  \cite{li2024badedit}. 
LLM-based agents relying on LLMs as core controllers are susceptible to both types of attacks \cite{xi2023risepotentiallargelanguage}. 
However, backdoor attacks on agents differ from those targeting traditional LLMs, as agents perform multi-step reasoning and interact with the environment \cite{he2024emergedsecurityprivacyllm}. 
This extended reasoning process creates more opportunities for sophisticated attacks, such as query-attack, observation-attack, and thought-attack \cite{yang2024watch}.

% $\mathbb{A}, A,\mathrm{A}, A$