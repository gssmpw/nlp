\subsection{Lower bounds on sample complexity}\label{sec:sample_compexity}
We establish a lower bound for generalized linear measurements using standard information-theoretic arguments based on Fano's inequality. While the upper bound in Theorem~\ref{thm:alg_general} is derived for the maximum probability of error over all  $k$-sparse vectors, the lower bound applies even in the weaker setting of the average probability of error, where 
$\bx$ is chosen uniformly at random.
\begin{theorem}[Lower bound for GLMs]\label{thm: lower_bdglm} Consider any  sensing matrix $\vecA$.
For a uniformly chosen $k$-sparse vector $\bx$, an algorithm $\phi$ satisfies $$\bbP\inp{\phi(\vecA, \by) \neq \bx}\leq \delta$$   only if the number of measurements $$m\geq \frac{k\log\inp{\frac{n}{k}}}{I}\inp{1 - \frac{h_2(\delta) + \delta k\log{n}}{k\log{n/k}}}$$ for some $I$ such that $I\geq {I(y_i; \bx|\vecA)}, \, i\in [m]$. In particular, when $y\in \inb{-1, 1}$, we have $\bbE\insq{\inp{g(\vecA_i^T\bx)}^2} \geq I(y_i, \bx|\vecA)$ where the expectation is over the randomness of $\vecA$ and $\bx$.
\end{theorem}
The lower bound can be interpreted in terms of a communication problem, where the input message $\bx$ is encoded to $\vecA\bx$. The decoding function takes in as input the encoding map $\vecA$ and the output vector $\by$ in order to recover $\bx$ with high probability. For optimal recovery, one needs at least $\frac{\text{message entropy}}{\text{capacity}}$ number of measurements (follows from noisy channel coding theorem~\cite{thomas2006elements}). In Theorem~\ref{thm: lower_bdglm}, the entropy of the message set $\log{n \choose k}\approx k\log{n/k}$ and the proxy for capacity is the upper bound on mutual information $I$. We provide a detailed proof of the theorem in  Section~\ref{sec:proofs}.


We first present lower bounds for \bcs\  and \logreg. The lower bound for \bcs\ is given for any sensing matrix $\vecA$ which satisfies the power constraint given by \eqref{eq:power_constraint}, whereas the one for \logreg\ is only for the special case when each entry of the sensing matrix is iid $\cN(0,1)$. Recall that \eqref{eq:power_constraint} holds in this case.  For \bcs\ (and \logreg\ respectively), we can use the upper bound of $\bbE\insq{\inp{g(\vecA_i^T\bx)}^2}$ on the mutual information term. The dependence of $\sigma^2$ (and $1/\beta^2$ respectively) requires careful bounding of this term, which is done in the formal proofs in Appendix~\ref{proof:sec:lower_bd}.


As mentioned earlier, we need at least $k\log\inp{n/k}$ measurements for \bcs and \logreg. This is because the entropy of a randomly chosen $k$-sparse vector is approximately $k\log\inp{n/k}$ and we learn at most one bit with each measurement. However, due to corruption with noise, we learn less than a bit of information about the unknown signal with each measurement. The information gain gets worse as the noise level increases. 
Our lower bounds make this reasoning explicit.  
\begin{corollary}[\bcs\ lower bound]\label{thm: lower_bd_bcs} Suppose, each row $\vecA_i, \, i\in [1:m]$ of the sensing matrix $\vecA$ satisfies the power constraint~\eqref{eq:power_constraint}.
For a uniformly chosen $k$-sparse vector $\bx$, an algorithm $\phi$ satisfies $$\bbP\inp{\phi(\vecA, {\by}) \neq \bx}\leq \delta$$ for the problem of $\bcs$ only if the number of measurements $$m\geq \frac{k+\sigma^2}{2}\log\inp{\frac{n}{k}}\inp{1 - \frac{h_2(\delta) + \delta k\log{n}}{k\log{n/k}}}.$$ 
\end{corollary}

\begin{corollary}[\logreg\ lower bound]\label{thm: lower_bd_log_reg} Consider a Gaussian  sensing matrix $\vecA$ where each entry is chosen iid $N(0,1)$.
For a uniformly chosen $k$-sparse vector $\bx$, an algorithm $\phi$ satisfies $$\bbP\inp{\phi(\vecA, \bw) \neq \bx}\leq \delta$$ for the problem of $\logreg$ only if the number of measurements $$m\geq \frac{1}{2}\inp{k+\frac{1}{\beta^2}}\log\inp{\frac{n}{k}}\inp{1 - \frac{h_2(\delta) + \delta k\log{n}}{k\log{n/k}}}.$$ 
\end{corollary}



Theorem~\ref{thm: lower_bdglm} also implies an information theoretic lower bound for \spl, which is presented below and proved in Appendix~\ref{proof:sec:lower_bd}. Note that the denominator term in the bound $\frac{1}{2}\log\inp{1+\frac{k}{\sigma^2}}$ is the capacity of a Gaussian channel with power constraint $k$ and noise variance $\sigma^2$. 
\begin{corollary}[\spl\ lower bound]\label{thm: spl_lower_bd_1}
Under the average power constraint \eqref{eq:power_constraint} on  $\vecA$, for a uniformly chosen $k$-sparse vector $\bx$, an algorithm $\phi$ satisfies $$\bbP\inp{\phi(\vecA, {\by}) \neq \bx}\leq \delta$$ only if the number of measurements
$$m\geq \frac{k\log\inp{\frac{n}{k}}-\inp{h_2(\delta) + \delta k\log{n}}}{\frac{1}{2}\log\inp{1+\frac{k}{\sigma^2}}}.$$
\end{corollary} 

\subsection{Tighter upper and lower bounds for \spl}\label{sec:tighter_bounds_spl}
We present information theoretic upper and lower bounds for \spl\ in this section. Similar to Section~\ref{sec:alg}, our upper bound is for the maximum probability of error, while the lower bounds hold even for the weaker criterion of average probability of error.

We first present an upper bound based on the maximum likelihood estimator (MLE) where  we  decode to $\hat{\bx}$ if, on output $\by$, 
\begin{align*}
\hat{\bx} = \argmax_{\stackrel{\bx\in \inb{0,1}^n}{\wh{\bx} = k}}\,\, p(\by|{\bx})
\end{align*} where $p(\by|{\bx})$ denotes the probability density function of $\by$ on input $\bx$.
\begin{theorem}[MLE upper bound for \spl]\label{thm:upper_bd_mle} Suppose  entries of the measurement matrix $\vecA$ are i.i.d. $\cN(0,1).$
The MLE  is correct with high probability if 
\begin{align}m\geq \max_{l\in[1:k]}  \frac{nN(l)}{\frac{1}{2}\log\inp{\frac{ l}{2\sigma^2}+1}}\label{eq:upper_bd_mle}
\end{align}where  $N(l):=  \frac{k}{n} h_2\inp{\frac{l}{k}} + (1-\frac{k}{n})h_2\inp{\frac{l}{n-k}}$. 
\end{theorem}
We prove the theorem in Appendix~\ref{proof:MLE}. The main proof idea involves analysing the probability that the output of the MLE is $2l$ Hamming distance away from the unknown signal $\bx$ for different values of $l\in [1:k]$ (assuming $k\leq n/2$). This depends on the number of such vectors (approximately $2^{nN(l)}$) and the probability that the MLE outputs a vector which is $2l$ Hamming distance away from $\bx$. 

Note that when $l = k\inp{1-\frac{k}{n}}$, $nN(l) = nh_2(k/n)\approx k\log{\frac{n}{k}}$ and $\log\inp{\frac{k\inp{1-k/n}}{2\sigma^2}+1}\leq \log\inp{\frac{k}{2\sigma^2}+1}$.
Thus, $m$ is at least $\frac{2k\log{n/k}}{\log\inp{\frac{k}{2\sigma^2}+1}}$ (see the bound for Corollary~\ref{thm: spl_lower_bd_1}). It is not immediately clear if this value of $l= k\inp{1-\frac{k}{n}}$ is the optimizer. However, for large $n$, this appears to be the case numerically as shown in Plot~\ref{plot:1}.

\begin{figure}[t]
\includegraphics[width=7cm]{Unknown2.png}
\centering
\caption{The figure shows the plot of the MLE upper bound \eqref{eq:upper_bd_mle} (given by m1) for different values of $k$. This is displayed in blue color. A plot of $\frac{2nN(l)}{\log\inp{\frac{ l}{2\sigma^2}+1}}$ is also presented for $l = k\inp{1-\frac{k}{n}}$ in orange color, given by m2. A part of the plot is zoomed in to emphasize the closeness between the lines. In these plots,  $\sigma^2$ is set to 1,  $n$ is 50000 and $k$ ranges from 1000 to 25000 $(n/2)$. }\label{plot:1}
\end{figure}


Inspired by the MLE analysis, we derive a lower bound with the same structure as \eqref{eq:upper_bd_mle}. We generate the unknown signal $\bx$ using the following distribution: A vector $\tilde{\bx}$ is chosen uniformly at random from the set of all $k$-sparse vectors. Given $\tilde{\bx}$, the unknown input signal $\bx$ is chosen uniformly from the set of all $k$-sparse vector which are at a Hamming distance $2l$ from $\bx$. 
The lower bound is then obtained by computing upper and lower bounds on $I(\vecA, \by;\bx|\tilde{\bx})$.
We show this lower bound only for random matrices where each entry is chosen iid $\cN(0,1)$.
\begin{theorem}[\spl\ lower bound]\label{thm:lower_bd_spl}
If each entry of $\vecA$ is chosen iid $\cN(0,1)$, then for a uniformly chosen $k$-sparse vector $\bx$, an algorithm $\phi$ satisfies 
\begin{align}
    \bbP\inp{\phi(\vecA, {\by}) \neq \bx}\leq \delta\label{eq:spl_lower_bd_l}
\end{align}  only if the number of measurements $$m\geq \max_l\frac{nN(l) - 2\log{n}- h_2(\delta) - \delta k\log{n}}{\frac{1}{2}\log\inp{1+\frac{l}{\sigma^2}\inp{2-\frac{l}{k}}}} .$$
\end{theorem} The proof of Theorem~\ref{thm:lower_bd_spl} is given in Appendix~\ref{proof:MLE}.

If we choose $l = k\inp{1-\frac{k}{n}}$ in Theorem~\ref{thm:lower_bd_spl}, we recover corollary~\ref{thm: spl_lower_bd_1} for the special case of Gaussian design.
% \begin{corollary}\label{corollary2:lower_bd_spl}
% If  each entry of $\vecA$ is chosen iid $\cN(0,1)$, then for a uniformly chosen $k$-sparse vector $\bx$, an algorithm $\phi$ satisfies 
% $$\bbP\inp{\phi(\vecA, {\by}) \neq \bx}\leq \delta$$
% only if the number of measurements 
% $$m\geq \frac{k\log\inp{\frac{n}{k}} - 2\log{n}- h_2(\delta) - \delta k\log{n}}{\log\inp{1+\frac{k}{\sigma^2}}} .$$
% \end{corollary}

% Corollary~\ref{corollary2:lower_bd_spl} can also be proved directly for any sensing matrix $\vecA$ which satisfies \eqref{eq:power_constraint} (non-necessarily a Gaussian design). 


% \begin{figure}[t]
% \includegraphics[width=8cm]{plot.png}
% \centering
% \caption{The figure shows the plot of the MLE upper bound \eqref{eq:upper_bd_mle} (given by m1) for different values of $n$. This is displayed in blue color. A plot of $\frac{2nN(l)}{\log\inp{\frac{ l}{2\sigma^2}+1}}$ is also presented for $l = k\inp{1-\frac{k}{n}}$ in orange color, given by m2. In these plots,  $\sigma^2$ is set to 1 and $k$ is $0.2n$. }\label{plot:1}
% \end{figure}

