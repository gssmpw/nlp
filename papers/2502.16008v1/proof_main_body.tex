\section{Proofs}\label{sec:proofs}
\begin{proof}[Proof of Theorem~\ref{thm:alg_general}]
Consider any input $\bx$ and a sensing matrix $\vecA$ where each entry is chosen iid $\cN(0,1)$. Suppose ${\bx}$ is supported on $\cS\subseteq[1:n]$ where $|\cS|=k$. Let $\by = (y_1, \ldots, y_m)$. Consider the event
\begin{align*}
    \cF = \inb{\sum_{i=1}^m{ {y_i A_{i,j}}}> \sum_{i=1}^m{ {y_i A_{i,j'}}}\text{ for all } j \in \cS,  j'\in\cS^c}
\end{align*}
It is clear that under $\cF$, the algorithm is correct.
We will compute the probability of $\cF^c$.
\begin{align}
\bbP\inp{\cF^c} &= \bbP\inp{\bigcup_{j\in\cS}\bigcup_{j'\in \cS^c}\inb{\sum_{i=1}^m{ {y_i A_{i,j'}}}\geq \sum_{i=1}^m{ {y_i A_{i,j}}}}}\nonumber\\
&\leq \sum_{j\in\cS}\sum_{j'\in \cS^c}\bbP\inp{\sum_{i=1}^m{ {y_i A_{i,j'}}}\geq \sum_{i=1}^m{ {y_i A_{i,j}}}}\nonumber\\
&=\sum_{j\in\cS}\sum_{j'\in \cS^c}\bbP\inp{\sum_{i=1}^m{ \inp{y_i (A_{i,j'}-A_{i,j})}}\geq 0}\label{eq:log_prob_f^c1}
\end{align}

For any $i\in[1:m]$, $j\in\cS$ and $j'\in \cS^c$, we first compute $\bbE\insq{y_i (A_{i,j}-A_{i,j'})}$.
\begin{align}
\bbE\insq{y_i (A_{i,j}-A_{i,j'})} &= \bbE\insq{y_i A_{i,j}}-\bbE\insq{y_iA_{i,j'}}\nonumber\\
& \stackrel{(a)}{=} \bbE\insq{y_i A_{i,j}} \label{eq:log_expt11}\\
&\stackrel{(b)}{=}\frac{\bbE\insq{y_iA_{i,\cS}}}{k}\nonumber\\
& = \frac{\bbE\insq{y_i\vecA_{i}^T\bx}}{k}\nonumber\\
&= \frac{\bbE\insq{A_{i}^T\bx\,\bbE\insq{y_1|\vecA_i^T\bx}}}{k}\nonumber\\
& \stackrel{(c)}{=} \frac{\bbE\insq{A_{i}^T\bx g\inp{\vecA_i^T\bx}}}{k}\nonumber\\
& \stackrel{(d)}{=} {\bbE\insq{ g'\inp{\vecA_i^T\bx}}}:= E\label{eq:log_expt1_avg11}
\end{align} where $(a)$ follows from the fact that $y_i$ and  $A_{i,j'}$ are zero mean, independent random variables and $(b)$ follows by defining $A_{i,\cS} = \sum_{j\in \cS}A_{i,j}$ and noticing that the random variables $y_i A_{i,j}$ are identically distributed for all $j\in \cS$, $(c)$ follows from \eqref{eq:glm} and $(d)$ follows from Stein's lemma. 
\begin{align*}
\bbP&\inp{\sum_{i=1}^m{ \inp{y_i (A_{i,j'}-A_{i,j})}}\geq 0}\\
&=\bbP\inp{\sum_{i=1}^m{ \inp{y_i (A_{i,j}-A_{i,j'})}}\leq 0}\\
& = \bbP\inp{\sum_{i=1}^m{ \inp{y_i (A_{i,j}-A_{i,j'})}} - mE\leq -mE}\\
& \leq \bbP\inp{\left|\sum_{i=1}^m{ \inp{y_i (A_{i,j}-A_{i,j'})}} - mE\right|\geq mE}\\
\end{align*}
\vspace{-0.001cm}
To compute this, note that for all $i\in [1:m]$, $y_i$  is a subgaussian random variable and ${y_i}\inp{A_{i, j}-A_{i, j'}}$ being product of two subgaussian random variables is a subexponential random variable (see [Lemma 2.7.7]\cite{vershynin}). Note that $\bbE\insq{\sum_{i=1}^m{ \inp{y_i (A_{i,j}-A_{i,j'})}}} = mE$ where $E$ was defined in \eqref{eq:log_expt1_avg11}.
Also, 
\begin{align*}
&\hspace{-0.3cm}\normi{{y_i}\inp{A_{i, j}-A_{i, j'}} - 1}_{\psi_1}\\
&\stackrel{(a)}{\leq} C\normi{{y_i}\inp{A_{i, j}-A_{i, j'}}}_{\psi_1}\\
&\stackrel{(b)}{\leq} C\normi{{y_i}}_{\psi_2}\normi{\inp{A_{i, j}-A_{i, j'}}}_{\psi_2}\\
&\stackrel{(c)}{\leq} C\normi{{y_i}}_{\psi_2}2C'\\
& = C_1\normi{{y_i}}_{\psi_2} \qquad\qquad\text{ for some constant $C_1$}.
\end{align*} Here, $(a)$ follows from [Exercise 2.7.10]\cite{vershynin}, $(b)$ from [Lemma 2.7.7]\cite{vershynin} and $(c)$ from [Example 2.5.8]\cite{vershynin}.
With this
\begin{align*}
\bbP&\inp{\left|\sum_{i=1}^m{ \inp{y_i (A_{i,j}-A_{i,j'})}} - mE\right|\geq mE}\\
&\stackrel{(a)}{\leq} 2\exp\inp{-c\min\inp{\frac{m^2E^2}{mC_1^2\normi{{y_i}}_{\psi_2}^2}, \frac{mE}{C_1\normi{{y_i}}_{\psi_2}} }}\\
&\stackrel{(b)}{\leq} 2\exp\inp{-cm\min\inp{\frac{mL^2}{C_1^2}, \frac{mL}{C_1 }}}
\end{align*} where $(a)$ follows from [Theorem 2.8.1]\cite{vershynin} and $(b)$ follows from the assumption in the lemma that $\frac{E}{\normi{{y_i}}_{\psi_2}} = \frac{\bbE\insq{g'(\vecA_i^T\bx)}}{\normi{{y_i}}_{\psi_2}}\geq L$.
Thus, from \eqref{eq:log_prob_f^c1}, 
\begin{align*}
\bbP\inp{\cF^c}&\leq k(n-k)2\exp\inp{-C_2 m\min\inp{L^2, L}}\\
&\rightarrow 0 \text{ if }m\geq C_2\inp{\log{k}+\log\inp{n-k}}\frac{1}{\min\inp{L^2, L}}
\end{align*} for some constant $C_2$.
\end{proof}


\iffalse
% \begin{proof}[Proof of Theorem~\ref{thm:alg_bcs}]
% Consider any input $\bx$ and a sensing matrix $\vecA$ where each entry is chosen iid $\cN(0,1)$. Suppose ${\bx}$ is supported on $\cS\subseteq[1:n]$ where $|\cS|=k$. Let $\bs = (s_1, \ldots, s_m)$ denote $\sign{\by}$. Consider the event
% \begin{align*}
%     \cF = \inb{\sum_{i=1}^m{ {s_i A_{i,j}}}> \sum_{i=1}^m{ {s_i A_{i,j'}}}\text{ for all } j \in \cS,  j'\in\cS^c}
% \end{align*}
% It is clear that under $\cF$, the algorithm is correct.
% We will compute the probability of $\cF^c$.
% % Note that
% % \begin{align*}
% %     \cF^c = \bigcup_{j\in\cS}\bigcup_{j'\in \cS^c}\inb{\sum_{i=1}^m{ {s_i A_{i,j'}}}\geq \sum_{i=1}^m{ {s_i A_{i,j}}}}.
% % \end{align*}
% % Thus,
% \begin{align}
% \bbP\inp{\cF^c} &= \bbP\inp{\bigcup_{j\in\cS}\bigcup_{j'\in \cS^c}\inb{\sum_{i=1}^m{ {s_i A_{i,j'}}}\geq \sum_{i=1}^m{ {s_i A_{i,j}}}}}\nonumber\\
% &\leq \sum_{j\in\cS}\sum_{j'\in \cS^c}\bbP\inp{\sum_{i=1}^m{ {s_i A_{i,j'}}}\geq \sum_{i=1}^m{ {s_i A_{i,j}}}}\nonumber\\
% &=\sum_{j\in\cS}\sum_{j'\in \cS^c}\bbP\inp{\sum_{i=1}^m{ \inp{s_i (A_{i,j'}-A_{i,j})}}\geq 0}\label{eq:prob_f^c}
% \end{align}

% For any $i\in[1:m]$, $j\in\cS$ and $j'\in \cS^c$, we first compute $\bbE\insq{s_i (A_{i,j}-A_{i,j'})}$.
% \begin{align}
% \bbE\insq{s_i (A_{i,j}-A_{i,j'})} &= \bbE\insq{s_i A_{i,j}}-\bbE\insq{s_iA_{i,j'}}\nonumber\\
% & \stackrel{(a)}{=} \bbE\insq{ A_{i,j}\bbE\insq{s_i| A_{i,j}}} - 0\label{eq:expt1}
% \end{align} where $(a)$ follows from the fact that $s_i$ and  $A_{i,j'}$ are zero mean, independent random variables. 

% For any $\cU\subseteq[1:n]$, we denote $\sum_{l \in \cU}A_{i,l}$ by $A_{i,\cU}$.  For any $A_{i,j} = a$, 
% \begin{align*}
% \bbP&\inp{s_i = 1|A_{i,j} = a} \\
% &= \bbP\inp{A_{i,\cS\setminus\inb{j}}+z_i\geq -a} \\
% & = \bbP\inp{\frac{A_{i,\cS\setminus\inb{j}}+z_i}{\sqrt{k-1+\sigma^2}}\geq -\frac{a}{\sqrt{k-1+\sigma^2}}}\\
% & = 1-\Phi\inp{-\frac{a}{\sqrt{k-1+\sigma^2}}}
% \end{align*} where $\Phi(x) = \frac{1}{2\pi}\int_{-\infty}^{x}e^{-\frac{t^2}{2}}dt$ is the cumulative distribution function of the standard Gaussian distribution. Thus, 
% $\bbP\inp{s_i = -1|A_{i,j} = a}  = \Phi\inp{-\frac{a}{\sqrt{k-1+\sigma^2}}}$ and 
% $$\bbE\insq{s_i| A_{i,j}=a} = 1-2\Phi\inp{-\frac{a}{\sqrt{k-1+\sigma^2}}}.$$ We are now ready to compute $\bbE\insq{ A_{i,j}\bbE\insq{s_i| A_{i,j}}}$.
% \begin{align}
% \bbE&\insq{ A_{i,j}\bbE\insq{s_i| A_{i,j}}} \nonumber\\
% &= \bbE\insq{ A_{i,j}\inp{1-2\Phi\inp{-\frac{A_{i,j}}{\sqrt{k-1+\sigma^2}}}}}\nonumber\\
% &= \bbE\insq{ A_{i,j}}-2\bbE\insq{A_{i,j}\Phi\inp{-\frac{A_{i,j}}{\sqrt{k-1+\sigma^2}}}}\nonumber\\
% &= 0-2\bbE\insq{A_{i,j}\Phi\inp{-\frac{A_{i,j}}{\sqrt{k-1+\sigma^2}}}}\label{eq:expt2}
% \end{align}
% \begin{align}
% \bbE&\insq{A_{i,j}\Phi\inp{-\frac{A_{i,j}}{\sqrt{k-1+\sigma^2}}}}\nonumber\\
% & = \int_{-\infty}^{\infty}a\frac{1}{\sqrt{2\pi}}e^{-\frac{a^2}{2}}\inp{\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{-\frac{a}{\sqrt{k-1+\sigma^2}}}e^{-\frac{t^2}{2}}dt}da\nonumber\\
% & = \frac{1}{2\pi}\int_{-\infty}^{\infty}\int_{-\infty}^{-\frac{a}{\sqrt{k-1+\sigma^2}}}ae^{-\frac{a^2}{2}}e^{-\frac{t^2}{2}}dt\,da\nonumber\\
% & \stackrel{(a)}{=} \frac{1}{2\pi}\int_{-\infty}^{\infty}\int_{-\infty}^{-{t}{\sqrt{k-1+\sigma^2}}}ae^{-\frac{a^2}{2}}e^{-\frac{t^2}{2}}da\,dt\nonumber\\
% & =\frac{1}{2\pi}\int_{-\infty}^{\infty}\inp{\int_{-\infty}^{-{t}{\sqrt{k-1+\sigma^2}}}ae^{-\frac{a^2}{2}}da}e^{-\frac{t^2}{2}}dt\nonumber\\
% & = \frac{1}{2\pi}\int_{-\infty}^{\infty}\inp{-e^{-\frac{t^2(k-1+\sigma^2)}{2}}}e^{-\frac{t^2}{2}}dt\nonumber\\
% & = -\frac{1}{\sqrt{2\pi\inp{k+\sigma^2}}}\int_{-\infty}^{\infty}\frac{\sqrt{k+\sigma^2}}{\sqrt{2\pi}}e^{-\frac{t^2(k+\sigma^2)}{2}}dt\nonumber\\
% & = -\frac{1}{\sqrt{2\pi\inp{k+\sigma^2}}}\label{eq:expt3}
% \end{align} where $(a)$ follows for change of variable formula for integration. From \eqref{eq:expt1},\eqref{eq:expt2} and \eqref{eq:expt3}, we have 
% \begin{align}
% \bbE\insq{s_i (A_{i,j}-A_{i,j'})} = \sqrt{\frac{2}{\pi}}\times\frac{1}{\sqrt{\inp{k+\sigma^2}}}.\label{eq:expt4}
% \end{align}
% Define $E := \sqrt{\frac{2}{\pi}}\times\frac{1}{\sqrt{\inp{k+\sigma^2}}}$. Then,
% \begin{align*}
% \bbP&\inp{\sum_{i=1}^m{ \inp{s_i (A_{i,j'}-A_{i,j})}}\geq 0}\\
% &=\bbP\inp{\sum_{i=1}^m{ \inp{s_i (A_{i,j}-A_{i,j'})}}\leq 0}\\
% & = \bbP\inp{\sum_{i=1}^m{ \inp{s_i (A_{i,j}-A_{i,j'})}} - mE\leq -mE}\\
% & \leq \bbP\inp{\left|\sum_{i=1}^m{ \inp{s_i (A_{i,j}-A_{i,j'})}} - mE\right|\geq mE}\\
% \end{align*}
% \vspace{-0.001cm}
% To compute this, notice that for all $i\in [1:m]$, $s_i$  is a subgaussian random variable (see \cite[Example 2.5.8]{vershynin}) and ${s_i}\inp{A_{i, j}-A_{i, j'}}$ being product of two subgaussian random variables is a subexponential random variable (see \cite[
% Lemma 2.7.7]{vershynin}). Note that $\bbE\insq{\sum_{i=1}^m{ \inp{s_i (A_{i,j}-A_{i,j'})}}} = mE$.
% Also, 
% \begin{align*}
% &\hspace{-0.3cm}\normi{{s_i}\inp{A_{i, j}-A_{i, j'}} - 1}_{\psi_1}\\
% &\stackrel{(a)}{\leq} C\normi{{s_i}\inp{A_{i, j}-A_{i, j'}}}_{\psi_1}\\
% &\stackrel{(b)}{\leq} C\normi{{s_i}}_{\psi_2}\normi{\inp{A_{i, j}-A_{i, j'}}}_{\psi_2}\\
% &\stackrel{(c)}{\leq} C\frac{1}{\ln{2}}2\\
% & = C_1 \qquad\qquad\text{ for some constant $C_1$}.
% \end{align*} Here, $(a)$ follows from \cite[Exercise 2.7.10]{vershynin}, $(b)$ from \cite[
% Lemma 2.7.7]{vershynin} and $(c)$ from \cite[Example 2.5.8]{vershynin}.
% With this
% \begin{align*}
% \bbP&\inp{\left|\sum_{i=1}^m{ \inp{s_i (A_{i,j}-A_{i,j'})}} - mE\right|\geq mE}\\
% &\stackrel{(a)}{\leq} 2\exp\inp{-c\min\inp{\frac{m^2E^2}{mC_1^2}, \frac{mE}{C_1} }}\\
% % &= 2\exp\inp{-c\min\inp{\frac{2m}{\pi C_1^2\inp{k+\sigma^2}}, \frac{m\sqrt{2}}{\sqrt{\pi}C_1\sqrt{\inp{k+\sigma^2}}} }}\\
% &\stackrel{(b)}{\leq} 2\exp\inp{-c\inp{\frac{2m}{\pi C_1^2\inp{k+\sigma^2}}}}
% \end{align*} where $(a)$ follows from \cite[Theorem 2.8.1]{vershynin} and $(b)$ follows by substituting value of $E$ and noting that  $ x\geq \sqrt{x}$ when $x\geq 1$.
% From \eqref{eq:prob_f^c}, 
% Thus,
% \begin{align*}
% \bbP\inp{\cF^c}&\leq k(n-k)2\exp\inp{-c\inp{\frac{2m}{\pi C_1^2\inp{k+\sigma^2}}}}\\
% &\rightarrow 0 \text{ if }m\geq C_2\inp{\log{k}+\log\inp{n-k}}\inp{k+\sigma^2}.
% \end{align*} for some constant $C_2$.


% \end{proof}

\fi


\begin{proof}[Proof of Theorem~\ref{thm: lower_bdglm}]
Suppose $\bx$ is distributed uniformly on the set of all $k$-sparse binary vectors. Then,
\begin{align}
I(\vecA, \by; \bx)  &= H(\bx) - H(\bx|\vecA, \by)\nonumber\\
&\stackrel{(a)}{\geq} \log{n \choose k } - h_2(\delta) - \delta\log\inp{{n \choose k } + 1}\nonumber\\
&\geq k\log{n/k}- h_2(\delta)- \delta k\log\inp{n}\label{eq:log_lower_bd_bcy_1}
\end{align} where $(a)$ follows from Fano's inequality [Theorem~2.10.1]\cite{thomas2006elements}.
We also note that
\begin{align*}
 I(\vecA, \by; \bx) &= I(\vecA ; \bx)    +  I(\by; \bx|\vecA)\\
 &\stackrel{(a)}{ = }0 + I(\by; \bx|\vecA).
\end{align*}where $(a)$ holds because $\vecA$ and $\bx$ are independent. Let $y_{j\in[1:i-1]}$ denote $\inp{y_1, \ldots, y_{i-1}}$. 
\begin{align}
I&(\by; \bx|\vecA) = \sum_{i = 1}^{m}I(y_i; \bx|\vecA, y_{j\in[1:i-1]})\nonumber\\
& = \sum_{i = 1}^{m}\Big(H(y_i|\vecA, y_{j\in[1:i-1]})\nonumber\\
&\qquad- H(y_i| \bx,\vecA, y_{j\in[1:i-1]})\Big)\nonumber\\
& \stackrel{(a)}{\leq}\sum_{i = 1}^{m}\inp{H(y_i|\vecA)- H(y_i| \bx,\vecA)}\nonumber\\
& = \sum_{i = 1}^{m}I(y_i;\bx|\vecA)\nonumber\\
&\stackrel{(b)}{\leq} mI \label{eq:log_lower_bd_bg}
\end{align}where $(a)$ follows from $H(y_i|\vecA, y_{j\in[1:i-1]})\leq H(y_i|\vecA)$ and $H(y_i| \bx,\vecA, y_{j\in[1:i-1]}) = H(y_i| \bx,\vecA)$ as $y_i$ is conditionally independent of $y_{j\in[1:i-1]}$ conditioned on $\bx$ and $\vecA$ and $(b)$ follows from the assumption in the Theorem. Thus, from \eqref{eq:log_lower_bd_bcy_1} and \eqref{eq:log_lower_bd_bg},
\begin{align*}
% m\frac{2k}{\pi\sigma^2} &\geq k\log{n/k}- h_2(\delta)- \delta k\log\inp{n}\\
mI &\geq  k\log\inp{n/k}\inp{1-\frac{h_2(\delta)+ \delta k\log\inp{n}}{k\log{n/k}}}
\end{align*}
This gives us the desired bound.

We can further simplify $I(y_i;\bx|\vecA)$ when $y_i\in \inb{-1,1}$, 
\begin{align*}
I(y_i;\bx|\vecA) &= H(y_i|\vecA)- H(y_i|\bx, \vecA)\\
&\stackrel{(a)}{\leq} 1- H(y_i|\bx, \vecA_i).
\end{align*}where $(a)$ holds because $H(y_i|\vecA)\leq H(y_i) =1$ and $y_i$ is conditionally independent of $(\vecA_1\ldots, \vecA_{i-1}, \vecA_{i+1}, \ldots, \vecA_{m})$ conditioned on $\vecA_i$ and $\bx$. Here $\vecA_i$, $i\in [1:m]$ denotes the $i^{\text{th}}$ row of the sensing matrix $\vecA$.

Suppose $\bx$ is fixed and $\bbP\inp{y_i = 1} = \frac{1}{2} + t$ for some $t\in [-1,1]$.  Then $\bbE\insq{y_i|\vecA_i} = 2t = g(\vecA_i^T\bx)$.
\begin{align*}
H(y_i|\vecA_i, \bx) &\stackrel{(a)}{=} \bbE\insq{h_2\inp{\frac{1}{2} + t}}\\
& \stackrel{(b)}{\geq} \bbE_{\bx}\insq{\bbE_{\vecA}\insq{4\inp{\frac{1}{2} + t}\inp{\frac{1}{2} - t}\Big|\bx}}\\
& = 1- \bbE_{\bx}\insq{\bbE\insq{\inp{2t}^2\Big|\bx}}\\
& = 1- \bbE_{\bx}\insq{\bbE\insq{\inp{g(\vecA_i^T\bx)}^2\Big|\bx}}\\
& = 1- \bbE_{\vecA, \bx}\insq{\inp{g(\vecA_i^T\bx)}^2}
\end{align*}
where in $(a)$, the expectation is over $\vecA$ and $\bx$. The inequality $(b)$ follows from [Theorem 1.2]\cite{topsoe2001bounds}. With this $I(y_i;\bx|\vecA)\leq \bbE\insq{\inp{g(\vecA_i^T\bx)}^2}$.
\end{proof}










\section{Conclusion and open problems}\label{sec:conclusion}
We analyze a simple algorithm (the ``average algorithm'' from \cite{vershyninPlan}  followed by `top-k' selection) for recovering sparse binary vectors from generalized linear measurements; along with an information theoretic lower bound. This gives optimal sample complexity characterization for \bcs\ and \logreg. On the other hand, the required number of measurements for the noisy linear case (\spl), which is $O((k+\sigma^2)\log{n})$, is as good as the sample complexity of any other known efficient algorithm for this problem, up to constants.  An interesting open problem is to find a design matrix and an efficient algorithm which requires less than $(k+\sigma^2)\log{n}$ samples for \spl. When the noise variance is zero, we show such an algorithm in  Remark~\ref{remark:noNoise}. 


We also present almost matching information theoretic upper and lower bounds for \spl\ given by  \eqref{eq:upper_bd_mle} and \eqref{eq:spl_lower_bd_l} respectively. The bounds are in the form of an optimization problem. While we present numerical evidence which suggests that \eqref{eq:upper_bd_mle} is optimized by $l = k\inp{1-\frac{k}{n}}$, a formal proof is still missing. The bounds in   \eqref{eq:upper_bd_mle} and \eqref{eq:spl_lower_bd_l} also differ slightly by constants in the denominator, which seems to be a persistent gap in this problem.


\paragraph{Acknowledgment}
This work is supported in part by NSF awards 2217058 and 2112665. The authors would like to thank Krishna Narayanan who introduced them to the binary linear regression problem at the Simons Institute program on Error-correcting codes.