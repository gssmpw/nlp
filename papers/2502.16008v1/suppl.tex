% \documentclass[twoside]{article}

% \usepackage{aistats2025}
% If your paper is accepted, change the options for the package
% aistats2025 as follows:
%
%\usepackage[accepted]{aistats2025}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.

% If you set papersize explicitly, activate the following three lines:
%\special{papersize = 8.5in, 11in}
%\setlength{\pdfpageheight}{11in}
%\setlength{\pdfpagewidth}{8.5in}

% If you use natbib package, activate the following three lines:
%\usepackage[round]{natbib}
%\renewcommand{\bibname}{References}
%\renewcommand{\bibsection}{\subsubsection*{\bibname}}

% If you use BibTeX in apalike style, activate the following line:
%\bibliographystyle{apalike}



% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

% Supplementary material: To improve readability, you must use a single-column format for the supplementary material.
% \newpage
% \onecolumn
% \aistatstitle{Instructions for Paper Submissions to AISTATS 2025: \\
% Supplementary Materials}

% \section{FORMATTING INSTRUCTIONS}

% To prepare a supplementary pdf file, we ask the authors to use \texttt{aistats2025.sty} as a style file and to follow the same formatting instructions as in the main paper.
% The only difference is that the supplementary material must be in a \emph{single-column} format.
% You can use \texttt{supplement.tex} in our starter pack as a starting point, or append the supplementary content to the main paper and split the final PDF into two separate files.

% Note that reviewers are under no obligation to examine your supplementary material.
% \newpage

% \section{Supplementary Proofs}\label{sec:supp}


\iffalse
\subsection{Proof of Theorem~\ref{thm:alg_spl}}\label{proof:thm:alg_spl}
\begin{proof}[Proof of Theorem~\ref{thm:alg_spl}]
We consider a sensing matrix where each entry is chosen iid $\cN(0,1)$. Consider any input $\bx$. Suppose ${\bx}$ is supported on $\cS\subseteq[1:n]$ where $|\cS|=k$.  
Consider the event
\begin{align*}
    \cF = \inb{\sum_{i=1}^m{ {y_i A_{i,j}}}> \sum_{i=1}^m{ {y_i A_{i,j'}}}\text{ for all } j \in \cS,  j'\in\cS^c}.
\end{align*}
It is clear that under $\cF$, the algorithm is correct.
We will compute the probability of $\cF^c$.
Note that
\begin{align*}
    \cF^c = \bigcup_{j\in\cS}\bigcup_{j'\in \cS^c}\inb{\sum_{i=1}^m{ {y_i A_{i,j'}}}\geq \sum_{i=1}^m{ {y_i A_{i,j}}}}.
\end{align*}
Thus,
\begin{align*}
\bbP\inp{\cF^c} &= \bbP\inp{\bigcup_{j\in\cS}\bigcup_{j'\in \cS^c}\inb{\sum_{i=1}^m{ {y_i A_{i,j'}}}\geq \sum_{i=1}^m{ {y_i A_{i,j}}}}}\\
&\leq \sum_{j\in\cS}\sum_{j'\in \cS^c}\bbP\inp{\sum_{i=1}^m{ {y_i A_{i,j'}}}\geq \sum_{i=1}^m{ {y_i A_{i,j}}}}\\
&=\sum_{j\in\cS}\sum_{j'\in \cS^c}\bbP\inp{\sum_{i=1}^m{ \inp{y_i (A_{i,j'}-A_{i,j})}}\geq 0}
\end{align*}

For any $j\in\cS$ and $j'\in \cS^c$, we have

\begin{align*}
& \bbP\inp{{{\sum_{i=1}^m y_i\inp{A_{i,j'} - A_{i,j}}\geq 0}}}\\
& \stackrel{(a)}{=}\bbP\inp{{{\sum_{i=1}^m (A_{i,\cS}+z_i)\inp{A_{i,j'} - A_{i,j}}\geq 0}}}\\
& =\bbP\inp{{{\sum_{i=1}^m (A_{i,\cS}+z_i)\inp{A_{i,j} - A_{i,j'}}\leq 0}}}\\
& =\bbP\inp{{{\sum_{i=1}^m (A_{i,\cS}+z_i)\inp{A_{i,j} - A_{i,j'}}-m\leq -m}}}\\
& \leq\bbP\inp{{{\inl{\sum_{i=1}^m (A_{i,\cS}+z_i)\inp{A_{i,j} - A_{i,j'}}-m}\geq m}}}
\end{align*}where in $(a)$, $A_{i,\cS}$ denotes $\sum_{j\in \cS}A_{i,j}$.
To compute this, we first note that $\inp{A_{i,\cS}+z_i}\inp{A_{i, j}-A_{i, j'}}$ being product of two Gaussians is a subexponential random variable for each $i$ (see \cite[
Lemma 2.7.7]{vershynin}). Note that $\bbE\insq{\inp{A_{i,\cS}+z_i}\inp{A_{i, j}-A_{i, j'}}} = \bbE\insq{A_{i, j}^2} = 1$.
Also, for some constants $C$ and $C_1$, 
\begin{align*}
&\hspace{-0.3cm}\normi{\inp{A_{i,\cS}+z_i}\inp{A_{i, j}-A_{i, j'}} - 1}_{\psi_1}\\
&\stackrel{(a)}{\leq} C\normi{\inp{A_{i,\cS}+z_i}\inp{A_{i, j}-A_{i, j'}}}_{\psi_1}\\
&\stackrel{(b)}{\leq} C\normi{\inp{A_{i,\cS}+z_i}}_{\psi_2}\normi{\inp{A_{i, j}-A_{i, j'}}}_{\psi_2}\\
&\stackrel{(c)}{\leq} C_1\sqrt{k+\sigma^2}\\
\end{align*} where $(a)$ follows from \cite[Exercise 2.7.10]{vershynin}, $(b)$ from \cite[
Lemma 2.7.7]{vershynin} and $(c)$ from \cite[Example 2.5.8]{vershynin}.
With this, for some constants $c$ and $c_1$,
\begin{align*}
\bbP&\inp{\inl{\sum_{i=1}^m\inp{\inp{A_{i,\cS}+z_i}\inp{A_{i, j}-A_{i, j'}} -1}} \geq m}\\
&\stackrel{(a)}{\leq} 2\exp\inp{-c\min\inp{\frac{m^2}{mC_1^2 \inp{k+\sigma^2}}, \frac{m}{C_1\sqrt{k+\sigma^2}} }}\\
&= 2\exp\inp{-c\min\inp{\frac{m}{C_1^2\inp{k+\sigma^2}}, \frac{m}{C_1\sqrt{k+\sigma^2}} }}\\
&\stackrel{(b)}{\leq} 2\exp\inp{-c_1\inp{\frac{m}{ \inp{k+\sigma^2}}}}
\end{align*} where $(a)$ follows from \cite[Theorem 2.8.1]{vershynin} and $(b)$ because $ x\geq \sqrt{x}$ when $x\geq 1$.


Thus,
\begin{align*}
\bbP\inp{\cF^c}&\leq k(n-k)\exp\inp{-c_1\inp{\frac{m}{ \inp{k+\sigma^2}}}}\\
&\rightarrow 0 \text{ if }m> \frac{1}{c_1}\inp{\log{k}+\log\inp{n-k}}\inp{k+\sigma^2}.
\end{align*}







\end{proof}
\fi

\subsection{Missing proofs from Section~\ref{sec:tighter_bounds_spl}}\label{proof:MLE}
\begin{proof}[Proof of Theorem~\ref{thm:upper_bd_mle}]
We consider a sensing matrix $\vecA$ where each entry is chosen iid $\cN(0,1)$. let $\cX_k$ denote the set of all $k$-sparse binary vectors. That is $\cX_k = \inb{{\bx'}\in \inb{0,1}^n: \wh{\bx} = k}$.  We  decode to $\hat{\bx}$ if, on output $\by$,
\begin{align*}
\hat{\bx} = \argmax_{\bx'\in \cX_k}\,\, p(\by|{\bx'})
\end{align*} where $p(\by|{\bx'})$ is the probability density function of $\by$ on input $\bx'$. 
 We assume that $k\leq n/2$. Suppose unknown signal is $\bx$. The error event $\cE$ is 
\begin{align*}
\cE = \inb{\by: \exists \tilde{\bx}\neq \bx \text{ such that }p(\by|\tilde{\bx})>p(\by|{\bx})}
\end{align*}
Then
\begin{align*}
\bbP(\cE) \leq \sum_{l = 1}^k\sum_{\substack{\bx'\in \cX_k:\\\h{\bx}{\bx'}= 2l}}\bbP\inp{p(\by|\tilde{\bx})>p(\by|{\bx})}
\end{align*}
Suppose $\bx$ has support on $\cS \subset [1:n], |\cS| = k$ and $\tilde{\bx}$ has support on $\cU \subset [1:n], |\cU| = k$. Then, conditioned on $\bx$, $y_r$ is generated from $\sum_{i\in \cS}A_{r, i}$ which we denote by $A_{r, \cS}$. That is, $y_r = A_{r, \cS} + z_r$ where $A_{r, \cS}\sim \cN(0, k)$. Similarly, conditioned on $\tilde{\bx}$, $y_r = A_{r, \cU} + z_r$ for $A_{r, \cU}:= \sum_{i\in \cU}A_{r, i}$ where $A_{r, \cU}\sim \cN(0, k)$.  For any $l\in [1:k]$, computing $\bbP\inp{p(\by|\tilde{\bx})>p(\by|{\bx})}$, we have
\begin{align*}
\bbP(p(\by|&\tilde{\bx})>p(\by|{\bx}))\\
&= \bbP\inp{\log{\frac{p(\by|\tilde{\bx})}{p(\by|{\bx})}}>0}\\
&=  \bbP\inp{{\sum_{r = 1}^{m}\log{\frac{p(y_r|A_{r, \cU})}{p(y_r|A_{r, \cS})}}}>0}\\
&= \bbP\Bigg(\sum_{r = 1}^{m}(A_{r, \cU}-A_{r, \cS})y_r>\\
&\quad\quad\sum_{r = 1}^{m}(A_{r, \cU}-A_{r, \cS})\frac{(A_{r, \cU}+A_{r, \cS})}{2}\Bigg)
\end{align*}Using the fact that $y_r = A_{r, \cS\setminus\cU}+A_{r, \cS\cap\cU}+z_r$, we have
\begin{align*}
\bbP&\inp{p(\by|\tilde{\bx})>p(\by|{\bx})}\\
&=  \bbP\Big(\sum_{r = 1}^{m}(A_{r, \cU\setminus\cS}-A_{r, \cS\setminus\cU})(A_{r, \cS\setminus\cU}+A_{r, \cS\cap\cU}+z_r)>\\
&\quad\sum_{r = 1}^{m}(A_{r, \cU\setminus\cS}-A_{r, \cS\setminus\cU})\frac{A_{r, \cU\setminus\cS}+A_{r, \cS\setminus\cU}+2A_{r, \cU\cap\cS}}{2}\Big)\\
&=  \bbP\Big(\sum_{r = 1}^{m}(A_{r, \cU\setminus\cS}-A_{r, \cS\setminus\cU})z_r)>\\
&\qquad\sum_{r = 1}^{m}\frac{A^2_{r, \cU\setminus\cS}}{2}-\frac{A^2_{r, \cS\setminus\cU}}{2}-A_{r, \cU\setminus\cS}A_{r, \cS\setminus\cU}+A^2_{r, \cS\setminus\cU}\Big)\\
&=  \bbP\Bigg(\sum_{r = 1}^{m}(A_{r, \cU\setminus\cS}-A_{r, \cS\setminus\cU})z_r)>\\
&\quad\qquad\qquad\frac{\sum_{r = 1}^{m}\inp{A_{r, \cU\setminus\cS}-A_{r, \cS\setminus\cU}}^2}{2}\Bigg)\\
&= \bbP\Bigg(\frac{\sum_{r = 1}^{m}(A_{r, \cU\setminus\cS}-A_{r, \cS\setminus\cU})z_r}{\sqrt{\sum_{r = 1}^{m}\inp{A_{r, \cU\setminus\cS}-A_{r, \cS\setminus\cU}}^2}\sigma})\\
 &\qquad \qquad>\frac{\sqrt{\sum_{r = 1}^{m}\inp{A_{r, \cU\setminus\cS}-A_{r, \cS\setminus\cU}}^2}}{2\sigma}\Bigg).
\end{align*}
Let $b_r = A_{r, \cU\setminus\cS}-A_{r, \cS\setminus\cU}$. Note that $b_r\sim \cN(0, 2l)$. Let $\bb = \inp{b_1,  \ldots, b_m}$. Let $\be = (e_1, \ldots, e_m)$ denote the realization of $\bb$. Then,
\begin{align*}
& \bbP\left(\frac{\sum_{r = 1}^{m}(A_{r, \cU\setminus\cS}-A_{r, \cS\setminus\cU})z_r}{\sqrt{\sum_{r = 1}^{m}\inp{A_{r, \cU\setminus\cS}-A_{r, \cS\setminus\cU}}^2}\sigma}>\right.\\
&\qquad\qquad\qquad\qquad\left.\frac{\sqrt{\sum_{r = 1}^{m}\inp{A_{r, \cU\setminus\cS}-A_{r, \cS\setminus\cU}}^2}}{2\sigma}\right)\\
& = \bbP\inp{\frac{\sum_{r = 1}^{m}b_{r}z_r}{\sqrt{\sum_{r = 1}^{m}\inp{b_{r}}^2}\sigma})>\frac{\sqrt{\sum_{r = 1}^{m}\inp{b_r}^2}}{2\sigma}}\\
& \stackrel{(a)}{=} \int p_{\bb}(\be) \bbP\inp{\frac{\sum_{r = 1}^{m}e_rz_r}{\sqrt{\sum_{r = 1}^{m}\inp{e_r}^2}\sigma})>\frac{\sqrt{\sum_{r = 1}^{m}\inp{e_r}^2}}{2\sigma}}d\be\\
&= \int p_{\bb}(\be)Q\inp{\frac{\sqrt{\sum_{r = 1}^{m}\inp{e_r}^2}}{2\sigma}}d\be
\end{align*} where in $(a)$, $p_{\bb}(\be)$ denotes the density of $\bb$ at $\be$ and $d\be$ is shorthand for $de_1 de_2\ldots de_m$. To analyse this further, we use the upper bound $Q(x)\leq \frac{1}{2}e^{-x^2/2}$.
\begin{align*}
\int &p_{\bb}(\be)Q\inp{\frac{\sqrt{\sum_{r = 1}^{m}\inp{e_r}^2}}{2\sigma}}d\be\\
& = \int\frac{1}{\inp{2\pi\cdot 2l}^{m/2}}2^{\inp{-\frac{\sum_{r=1}^{m}{e_r^2}}{2l}}}2^{\inp{-\frac{{\sum_{r = 1}^{m}{e_r}^2}}{8\sigma^2}}}d\be\\
& = \int\frac{1}{\inp{2\pi\cdot 2l}^{m/2}}2^{\inp{-\sum_{r=1}^{m}{e_r^2}\inp{\frac{1}{2l}+\frac{1}{8\sigma^2}}}}d\be\\
& = \frac{1}{\inp{\frac{1}{2l}+\frac{1}{8\sigma^2}}^{m/2}\inp{2l}^{m/2}}\\
&\quad\int\frac{1}{{\inp{2\pi}^{m/2}}}\inp{\frac{1}{2l}+\frac{1}{8\sigma^2}}^{m/2}2^{\inp{-\sum_{r=1}^{m}{e_r^2}\inp{\frac{1}{2l}+\frac{1}{8\sigma^2}}}}d\be\\
& = \inp{\frac{1}{1+\frac{l}{2\sigma^2}}}^{m/2}\\
& = 2^{\inp{-\frac{m}{2}\log\inp{1+\frac{l}{2\sigma^2}}}}
\end{align*}
Next, we observe that 
\begin{align*}
&\inl{\inb{\bx'\in \cX_k:\h{\bx}{\bx'}= 2l}} = {k\choose l}{n-k \choose l}   \\
&\qquad\stackrel{(a)}{\leq} 2^{kh_2\inp{\frac{l}{k}}}2^{(n-k)h_2\inp{\frac{l}{n-k}}}\\
&\qquad = 2^{n\inp{\frac{k}{n}h_2\inp{\frac{l}{k}} + \frac{(n-k)}{n}h_2\inp{\frac{l}{(n-k)}}}}\\
&\qquad = 2^{nN(l)}.
\end{align*}where $(a)$ uses the inequality ${n \choose k}\leq 2^{nh_2(k/n)}$. 
Then,
\begin{align*}
\bbP(\cE) \leq &\sum_{l = 1}^k2^{nN(l)}2^{\inp{-\frac{m}{2}\log\inp{1+\frac{l}{2\sigma^2}}}}\\
&\rightarrow 0 \text{ if }m\geq \max_l \frac{2nN(l)}{\log\inp{1+\frac{l}{2\sigma^2}}}
\end{align*}

    
\end{proof}


\iffalse
\subsection{Proof of Theorem~\ref{thm: spl_lower_bd_1}}
\begin{proof}[Proof of Theorem~\ref{thm: spl_lower_bd_1}]
Suppose $\bx$ is generated uniformly at random from the set of all $k$-sparse vectors and $\vecA$ is any sensing matrix which satisfies the power constraint given by \eqref{eq:power_constraint}. Then,
\begin{align*}
I(\vecA, \by;\bx) &= H(\bx)-H(\bx|\by, \vecA)\\
 &\stackrel{(a)}{\geq} \log{n \choose k } - h_2(\delta) - \delta\log\inp{{n \choose k } + 1}\\
 &\geq k\log\inp{\frac{n}{k}}- h_2(\delta) - \delta k\log{n}.
\end{align*} where $(a)$ follows from the Fano's inequality.
We also see that
\begin{align*}
I(\vecA,\by;\bx) &= I(\vecA, \bx)+I(\by;\bx|\vecA)\\
&\stackrel{(a)}{=}0 +I(\by;\bx|\vecA)
\end{align*}where $(a)$ follows from noting that $\vecA$ and $\bx$ are independent. Suppose $y_{j\in [1:i-1]}$ denote $(y_1, \ldots, y_{i-1})$. Then,
\begin{align*}
I(\by;\bx| \vecA) &= \sum_{i=1}^{m}I(y_i;\bx|y_{j\in[1:i-1]}, \vecA)\\
& = \sum_{i = 1}^{m}\inp{h(y_i|y_{j\in[1:i-1]}, \vecA)-h(y_i|\bx, y_{j\in[1:i-1]}, \vecA)}\\
&\leq \sum_{i = 1}^{m}\inp{h(y_i)-h(\mathbf{A}_i^T\bx + z_i|\bx, y_{j\in[1:i-1]}, \vecA)}\\
&=\sum_{i = 1}^{m}\inp{h(y_i)-h(z_i)}\\
&\leq \sum_{i = 1}^{m}\inp{h(w_i)-h(z_i)}
\end{align*} where in the last inequality, $w_i\sim\cN\inp{0, \sigma^2_w}$ where $\mathsf{Var}(y_i)\leq \sigma^2_w$. We will now compute an upper bound on $\mathsf{Var}(y_i)$.
\begin{align*}
\mathsf{Var}(y_i) &\leq \bbE\insq{\inp{\vecA_i^T\bx+z_i}^2}\\
& = \bbE\insq{\inp{\vecA_i^T\bx}^2} + \sigma^2\\
&\leq k + \sigma^2
\end{align*} 
Thus, we have
\begin{align*}
&\sum_{i = 1}^{m}\inp{h(w_i)-h(z_i)}\\
&=\frac{m}{2}\log\inp{\frac{{k}}{\sigma^2}+1}
\end{align*}
With this 
\begin{align*}
\frac{m}{2}\log\inp{\frac{{k}}{\sigma^2}+1}
\geq k\log\inp{\frac{n}{k}}- h_2(\delta) - \delta k\log{n}
\end{align*} and 
\begin{align*}
m&\geq \frac{k\log\inp{\frac{n}{k}}- h_2(\delta) - \delta k\log{n}}{\frac{1}{2}\log\inp{\frac{{k}}{\sigma^2}+1}}\\
&= \frac{2k\log\inp{\frac{n}{k}}- h_2(\delta) - \delta k\log{n}}{\log\inp{\frac{{k}}{\sigma^2}+1}}.
\end{align*} 



\end{proof}

\fi

% \subsection{Proof of Theorem~\ref{thm:lower_bd_spl}}\label{proof:thm:lower_bd_spl}

\begin{proof}[Proof of Theorem~\ref{thm:lower_bd_spl}]
    We consider a joint distribution given by the following process. $\tilde{\bx}$ is generated uniformly at random from the set of all $k$-sparse vectors. Given $\tilde{\bx}$, the unknown signal $\bx$ is chosen uniformly at random from the set of all vectors which are at a Hamming distance $2l$ from $\tilde{\bx}$ for some $l\in [1:k]$ (assuming $k\leq n/2$). 
    We will denote the realization of $\tilde{\bx}$ by $\bar{\bx}$ and the realization of $\bx$ by $\hat{\bx}$. With this, given $\tilde{\bx} = \bar{\bx}$,
\begin{align*}
\bbP\inp{\bx = \hat{\bx}|\tilde{\bx} = \bar{\bx}} = \frac{1}{{k \choose l}{n-k \choose l}}.    
\end{align*}
Note that the marginal distribution of $\bx$ is uniform over the set of all $k$-sparse vectors.

We will be using the below set of equations in our further analysis. For $\bx = \inp{x_1, \ldots, x_n}$,  any $j, l\in \cS_{\tilde{\bx}}$ where $j\neq l$, we have
\begin{align}
&\bbP\inp{x_j = 1|\tilde{\bx} = \bar{\bx}} = \frac{{k-1\choose l}{n-k \choose l}}{{k \choose l}{n-k \choose l}} = \frac{k-l}{k},\label{eq:1}\text{ and }\\
&\bbP\inp{x_j = x_l = 1|\tilde{\bx} = \bar{\bx}} = \frac{{k-2\choose l}{n-k \choose l}}{{k \choose l}{n-k \choose l}} = \inp{\frac{k-l}{k}}\inp{\frac{k-l-1}{k-1}},\label{eq:2}
% &\bbP\inp{x_j' = 1|\tilde{\bx} = \bar{\bx}} = \frac{{k\choose l}{n-k-1 \choose l-1}}{{k \choose l}{n-k \choose l}} = \frac{l}{n-k},\label{eq:3}\\
% &\bbP\inp{x_j' = x_l' = 1|\tilde{\bx} = \bar{\bx}} = \frac{{k\choose l}{n-k-2 \choose l-2}}{{k \choose l}{n-k \choose l}} \nonumber\\
 % &\qquad\qquad\qquad= \inp{\frac{l}{n-k}}\inp{\frac{l-1}{n-k-1}},\label{eq:4}\\
% &\bbP\inp{x_j = x_l' = 1|\tilde{\bx} = \bar{\bx}} = \frac{{k-1\choose l}{n-k-1 \choose l-1}}{{k \choose l}{n-k \choose l}} \nonumber\\
 % &\qquad\qquad\qquad= \inp{\frac{k-l}{k}}\inp{\frac{l}{n-k}}. \label{eq:5}
\end{align}
For any sensing matrix $\vecA$, output vector $\by$ and an unknown signal $\bx$ generated from $\tilde{\bx}$ using the above process, we have
\begin{align}
I&(\vecA, \by;\bx|\tilde{\bx}) = H(\bx|\tilde{\bx})-H(\bx|\vecA, \by,\tilde{\bx})\nonumber\\
&\geq H(\bx|\tilde{\bx})-H(\bx|\vecA, \by)\nonumber\\
 &\stackrel{(a)}\geq H(\bx|\tilde{\bx}) - h_2(\delta) - \delta\log{{n \choose k}}\nonumber\\
 & = \sum_{\tilde{\bx}}\bbP\inp{\tilde{\bx} = \bar{\bx}} H(\bx|\tilde{\bx} = \bar{\bx}) - h_2(\delta) - \delta\log{{n \choose k}}\nonumber\\
 & \stackrel{(b)}{\geq} \sum_{\tilde{\bx}}\bbP\inp{\tilde{\bx} = \bar{\bx}} \log{k \choose l}{n-k \choose l} - h_2(\delta) - \delta k\log{n}\nonumber\\
  & \stackrel{(c)}{\geq} kh_2\inp{\frac{l}{k}} + (n-k)h_2\inp{\frac{l}{n-k}}-\log\inp{k+1}\nonumber\\
  &\qquad-\log\inp{n-k+1} - h_2(\delta) - \delta k\log{n}\nonumber\\
  & \stackrel{(d)}{\geq}nN(l) - 2\log{n}- h_2(\delta) - \delta k\log{n}\label{eq:fano_spl}
\end{align} where $(a)$ follows from [Theorem~2.10.1]\cite{thomas2006elements}, $(b)$ follows from ${n \choose k}\leq n^k$, $(c)$ follows from ${n \choose k}\geq \frac{1}{n+1}2^{nh_2(k/n)}$ ([Theorem 11.1.3]\cite{thomas2006elements}) where $h_2$ is the binary entropy function and $(d)$ follows by defining $N(l) = \frac{k}{n}h_2\inp{\frac{l}{k}} + (1-\frac{k}{n})h_2\inp{\frac{l}{n-k}}$.


Next, we will compute an upper bound on $I(\vecA, \by;\bx|\tilde{\bx})$.
\begin{align*}
I(\vecA, \by;\bx|\tilde{\bx}) &= I(\vecA;\bx|\tilde{\bx}) + I( \by;\bx|\vecA,\tilde{\bx})\\
&\stackrel{(a)}{=} 0 + I( \by;\bx|\vecA,\tilde{\bx})\\
&\stackrel{(b)}{=}\sum_{i=1}^{m}I(y_i;\bx|\vecA, y_{j\in [1:i-1]},\tilde{\bx})
\end{align*} where $(a)$ follows because $\vecA$ is independent of both $\bx$ and $\tilde{\bx}$. In particular, $\vecA$ is conditionally independent of $\bx$ conditioned on $\tilde{\bx}$. Here, $(b)$ follows from chain rule for mutual information where $y_{j\in [1:i-1]}$ denotes $(y_1, \ldots, y_{i-1})$.

Suppose $h(\cdot)$ denotes the differential entropy of a continuous random variable. For any $i\in [1:m]$,
\begin{align*}
I&(y_i;\bx|\vecA, y_{j\in [1:i-1]},\tilde{\bx})\\
& = {h(y_i|\vecA, y_{j\in [1:i-1]},\tilde{\bx})-h(y_i|\bx,\vecA, y_{j\in [1:i-1]},\tilde{\bx})}\\
&\stackrel{(a)}{\leq} {h(y_i|\vecA_{i, j\in \cS_{\tilde{\bx}}}, \tilde{\bx})-h(\vecA_i^{T}\bx + z_i|\vecA,\bx, Y^{i-1},\tilde{\bx})}\\
&={h(y_i|\vecA_{i, j\in \cS_{\tilde{\bx}}}, \tilde{\bx})-h(z_i)}\\
&={h(y_i|\vecA_{i, j\in \cS_{\tilde{\bx}}}, \tilde{\bx})-\frac{1}{2}\log\inp{2\pi e \sigma^2}}
\end{align*} where in $(a)$, we use $\vecA_{i, j\in \cS_{\tilde{\bx}}}$ to denote the set of elements $A_{i, j}$ for $j\in \cS_{\tilde{\bx}}$.
Conditioned on $\tilde{\bx}= \bar{\bx}$ and $ \vecA_{i, j\in \cS_{\tilde{\bx}}} = \mathbf{a}_{i, j\in \cS_{\tilde{\bx}}} $, 



\begin{align*}
h&(y_i|\tilde{\bx}= \bar{\bx}, \vecA_{i, j\in \cS_{\tilde{\bx}}}=\mathbf{a}_{i, j\in \cS_{\tilde{\bx}}}) \\
&\stackrel{(a)}{=} h\inp{y_i-\bbE\insq{y_i|\tilde{\bx}= \bar{\bx}, \vecA_{\tilde{\bx}}=\mathbf{a}_{i, j\in \cS_{\tilde{\bx}}}}\Big|\tilde{\bx}= \bar{\bx}, \vecA_{\tilde{\bx}}=\mathbf{a}_{i, j\in \cS_{\tilde{\bx}}}}\\
&\stackrel{(b)}{\leq} h(W_{\mathbf{a}_{i, j\in \cS_{\tilde{\bx}}}})\\
\end{align*} where $(a)$ follows by noting that differential entropy does not change by centering ([Theorem~8.6.3]\cite{thomas2006elements}) and $(b)$ follows for  $W_{i, \tilde{\bx}}\sim\cN\inp{0, \sigma_w^2}$ where $\sigma_w^2\leq \mathsf{Var}(y_i-\bbE\insq{y_i|\tilde{\bx}= \bar{\bx}, \vecA_{i, j\in \cS_{\tilde{\bx}}}=\mathbf{a}_{i, j\in \cS_{\tilde{\bx}}}}\Big|\tilde{\bx}= \bar{\bx}, \vecA_{i, j\in \cS_{\tilde{\bx}}} = \mathbf{a}_{i, j\in \cS_{\tilde{\bx}}})$ from  the fact that for the same variance a Gaussian random variable maximizes the differential entropy and it increasing with increasing variance ([Theorem 8.6.5 and Example 8.1.2]\cite{thomas2006elements}).
 

Recall that each entry of $\vecA$ is chosen iid $\cN(0,1)$. In that case, 
$\mathsf{Var}\inp{y_i-\bbE\insq{y_i|\tilde{\bx}= \bar{\bx}, \vecA_{i, j\in \cS_{\tilde{\bx}}}=\mathbf{a}_{i, j\in \cS_{\tilde{\bx}}}}}$ conditioned on $\tilde{\bx}= \bar{\bx}$ and $\vecA_{\tilde{\bx}} = \mathbf{a}_{i, j\in \cS_{\tilde{\bx}}}$ is   given by $\bbE\insq{\inp{y_i}^2|\tilde{\bx}= \bar{\bx}, \vecA_{i, j\in \cS_{\tilde{\bx}}} = \mathbf{a}_{i, j\in \cS_{\tilde{\bx}}}} - \inp{\bbE\insq{y_i|\tilde{\bx}= \bar{\bx}, \vecA_{i, j\in \cS_{\tilde{\bx}}} = \mathbf{a}_{i, j\in \cS_{\tilde{\bx}}}}}^2$. We first analyse the first term.
\begin{align*}
 \bbE&\insq{\inp{y_i}^2|\tilde{\bx}= \bar{\bx}, \vecA_{i, j\in \cS_{\tilde{\bx}}} = \mathbf{a}_{i, j\in \cS_{\tilde{\bx}}}} \\
 % &\stackrel{(a)}{=}  \bbE\insq{\inp{\vecA_i^T\bx + z_i}^2|\tilde{\bx}= \bar{\bx}, \vecA_{i, j\in \cS_{\tilde{\bx}}} = \mathbf{a}_{i, j\in \cS_{\tilde{\bx}}}}\\
 & = \bbE\insq{\bbE\insq{\inp{\vecA_i^T\bx + z_i}^2|\tilde{\bx}= \bar{\bx},\vecA_{i, j\in \cS_{\tilde{\bx}}} = \mathbf{a}_{i, j\in \cS_{\tilde{\bx}}}, \bx}}
\end{align*}
% where $(a)$ holds because $y_i = \vecA_i^T\bx + z_i$ (where $\vecA_i$ denotes the $i^{\mathsf{th}}$ row of $\vecA$) and conditioned on $\vecA_{\tilde{\bx}} = \mathbf{a}_{i, j\in \cS_{\tilde{\bx}}}$, it is independent of $\tilde{\bx}$. 
For any $\bx = \hat{\bx}$,
\begin{align*}
\bbE&\insq{\inp{\vecA_i^T\bx + z_i}^2|\tilde{\bx}= \bar{\bx},\vecA_{\tilde{\bx}} = \mathbf{a}_{i, j\in \cS_{\tilde{\bx}}}, \bx = \hat{\bx}}\\
& \stackrel{(a)}{=} l+\sigma^2 + \inp{\mathbf{a}_{i, S_{\bx}\cap S_{\tilde{\bx}}}}^2
\end{align*} where $(a)$ holds because conditioned on $\vecA_{\tilde{\bx}} = \mathbf{a}_{i, j\in \cS_{\tilde{\bx}}}$ and $\bx = \hat{\bx}$, the random variable $\vecA_i^T\bx + z_i = \mathbf{A}_{i, S_{\bx}\setminus S_{\tilde{\bx}}} + \mathbf{a}_{i, S_{\bx}\cap S_{\tilde{\bx}}} + z_i$ and $|S_{\bx}\setminus S_{\tilde{\bx}}| = l$.

Similarly, 
we can analyze the second term.
\begin{align*}
\bbE&\insq{y_i|\tilde{\bx}= \bar{\bx}, \vecA_{i, j\in \cS_{\tilde{\bx}}} = \mathbf{a}_{i, j\in \cS_{\tilde{\bx}}}} \\
 &= \bbE\insq{\vecA_i^T\bx + z_i|\tilde{\bx}= \bar{\bx}, \vecA_{i, j\in \cS_{\tilde{\bx}}} = \mathbf{a}_{i, j\in \cS_{\tilde{\bx}}}}\\
 & = \bbE\insq{\bbE\insq{\vecA_i^T\bx + z_i|\tilde{\bx}= \bar{\bx}, \vecA_{i, j\in \cS_{\tilde{\bx}}} = \mathbf{a}_{i, j\in \cS_{\tilde{\bx}}},\bx}}
\end{align*}
For any $\bx = \hat{\bx}$,
\begin{align*}
\bbE&\insq{\vecA_i^T\bx + z_i|\tilde{\bx}= \bar{\bx}, \vecA_{i, j\in \cS_{\tilde{\bx}}} = \mathbf{a}_{i, j\in \cS_{\tilde{\bx}}}, \bx = \hat{\bx}}\\
& = \mathbf{a}_{i, S_{\bx}\cap S_{\tilde{\bx}}}
\end{align*} and
\begin{align*}
\bbE\insq{\bbE\insq{\vecA\bx + z_i|\tilde{\bx}= \bar{\bx}, \vecA_{i, j\in \cS_{\tilde{\bx}}} = \mathbf{a}_{i, j\in \cS_{\tilde{\bx}}},\bx}}&= \bbE\insq{\bbE\insq{\mathbf{a}_{i, S_{\bx}\cap S_{\tilde{\bx}}}|\tilde{\bx}= \bar{\bx}, \vecA_{i, j\in \cS_{\tilde{\bx}}} = \mathbf{a}_{i, j\in \cS_{\tilde{\bx}}},\bx}}\\
&\stackrel{(a)}{=} \frac{k-l}{k}\mathbf{a}_{i, S_{\tilde{\bx}}} 
\end{align*}where $(a)$ follows from \eqref{eq:1}.
\begin{align*}
&\inp{\bbE\insq{y_i|\tilde{\bx}= \bar{\bx}, \vecA_{i, j\in \cS_{\tilde{\bx}}} = \mathbf{a}_{i, j\in \cS_{\tilde{\bx}}}}}^2\\
&= \inp{\frac{k-l}{k}}^2\inp{\sum_{j \in \cS_{\tilde{\bx}}}a^2_{i,j} + 2\sum_{\stackrel{j,l\in \cS_{\tilde{\bx}}}{j\neq l}} a_{i,j}a_{i,l}}
\end{align*}
On the other hand,
\begin{align*}
 \bbE&\insq{\inp{y_i}^2|\tilde{\bx}= \bar{\bx}, \vecA_{\tilde{\bx}} = \mathbf{a}_{i, j\in \cS_{\tilde{\bx}}}}\\
 &= \bbE_{\bx}\insq{l+\sigma^2 + \inp{\mathbf{a}_{i, S_{\bx}\cap S_{\tilde{\bx}}}}^2}\\
 & \stackrel{(a)}{=} l + \sigma^2 + \frac{{k-1 \choose k-l-1}}{{k \choose k-l}}\sum_{j \in S_{\tilde{\bx}}}a^2_{i,j} + 2\frac{{k-2 \choose k-l-2}}{{k \choose k-l}}\sum_{\stackrel{j, l\in \cS_{\tilde{\bx}}}{j\neq l}}a_{i,j}a_{i,l}\\
 & = l + \sigma^2 + \frac{k-l}{k}\sum_{j \in S_{\tilde{\bx}}}a^2_{i,j} + 2\inp{\frac{k-l}{k}}\inp{\frac{k-l-1}{k-1}}\sum_{\stackrel{j, l\in \cS_{\tilde{\bx}}}{j\neq l}}a_{i,j}a_{i,l}
\end{align*} where $(a)$ follows from \eqref{eq:1} and \eqref{eq:2}.
Thus,
\begin{align*}
&\mathsf{Var}\inp{y_i-\bbE\insq{y_i|\tilde{\bx}= \bar{\bx}, \vecA_{i, j\in \cS_{\tilde{\bx}}}=\mathbf{a}_{i, j\in \cS_{\tilde{\bx}}}}\Big|\tilde{\bx}= \bar{\bx}, \vecA_{i, j\in \cS_{\tilde{\bx}}} = \mathbf{a}_{i, j\in \cS_{\tilde{\bx}}}}\\
& = l + \sigma^2 + \frac{k-l}{k}\sum_{j \in S_{\tilde{\bx}}}a^2_{i,j} + 2\inp{\frac{k-l}{k}}\inp{\frac{k-l-1}{k-1}}\sum_{\stackrel{j, l\in \cS_{\tilde{\bx}}}{j\neq l}}a_{i,j}a_{i,l}- \inp{\frac{k-l}{k}}^2\inp{\sum_{j \in \cS_{\tilde{\bx}}}a^2_{i,j} + 2\sum_{\stackrel{j,l\in \cS_{\tilde{\bx}}}{j\neq l}} a_{i,j}a_{i,l}}\\
% &\stackrel{(a)}{\leq}  l + \sigma^2 + \frac{k-l}{k}\sum_{j \in S_{\tilde{\bx}}}a^2_{i,j} + 2\inp{\frac{k-l}{k}}^2\sum_{\stackrel{j, l\in \cS_{\tilde{\bx}}}{j\neq l}}a_{i,j}a_{i,l}- \inp{\frac{k-l}{k}}^2\inp{\sum_{j \in \cS_{\tilde{\bx}}}a^2_{i,j} + 2\sum_{\stackrel{j,l\in \cS_{\tilde{\bx}}}{j\neq l}} a_{i,j}a_{i,l}}\\
&=  l + \sigma^2 + \inp{\frac{k-l}{k}}\inp{\frac{l}{k}}\sum_{j \in S_{\tilde{\bx}}}a^2_{i,j} -2\frac{k-l}{k}\frac{l}{k\inp{k-1}}\sum_{\stackrel{j,l\in \cS_{\tilde{\bx}}}{j\neq l}} a_{i,j}a_{i,l}
\end{align*}

Thus, 
\begin{align*}
h(y_i&|\vecA_{i, j\in \cS_{\tilde{\bx}}}, \tilde{\bx} =\tilde{\bx}) = \int p_{\vecA}(\ba)h(y_i|\tilde{\bx}= \bar{\bx}, \vecA_{i, j\in \cS_{\tilde{\bx}}}=\mathbf{a}_{i, j\in \cS_{\tilde{\bx}}})da\\
&\leq \int p_{\vecA}(\ba)\frac{1}{2}\log\inp{2\pi e\inp{l + \sigma^2 + \inp{\frac{k-l}{k}}\inp{\frac{l}{k}}\sum_{j \in S_{\tilde{\bx}}}a^2_{i,j} -2\frac{k-l}{k}\frac{l}{k\inp{k-1}}\sum_{\stackrel{j,l\in \cS_{\tilde{\bx}}}{j\neq l}} a_{i,j}a_{i,l}}}d{\ba}\\
&  \stackrel{(a)}{\leq} \frac{1}{2}\log\inp{2\pi e\inp{l + \sigma^2 + \inp{\int p_{\vecA}(\ba)\inp{\frac{k-l}{k}}\inp{\frac{l}{k}}\inp{\sum_{j \in S_{\tilde{\bx}}}a^2_{i,j} -2\frac{k-l}{k}\frac{l}{k\inp{k-1}}\sum_{\stackrel{j,l\in \cS_{\tilde{\bx}}}{j\neq l}} a_{i,j}a_{i,l}}d{\ba}}}}\\
&  \stackrel{(b)}{=} \frac{1}{2}\log\inp{2\pi e\inp{l + \sigma^2 + \inp{\frac{k-l}{k}}l}}\\
% &  = \frac{1}{2}\log\inp{2\pi e\inp{l\inp{2-\frac{l}{k}} + \sigma^2 }}
\end{align*}where $(a)$ follows from Jenson's inequality and $(b)$ follows by noting that $\bbE\insq{A_{i,j}^2} =1$  and $\bbE\insq{A_{i,j}A_{i,l}} =0$ for any $i$ and $j, l$, where $j\neq l$ and $\tilde{\bx}$ is $k$-sparse.

Thus, 
\begin{align*}
\sum_{i = 1}^mI(y_i;\bx|\vecA, y_{j\in [1:i-1]},\tilde{\bx})&\leq m\frac{1}{2}\log\inp{2\pi e\inp{l + \sigma^2 + \inp{\frac{k-l}{k}}l}} - \frac{1}{2}\log\inp{2\pi e \sigma^2}\\
& = \frac{m}{2}\log{\inp{1+ \frac{l}{\sigma^2}\inp{2-\frac{l}{k}}}}
\end{align*}
Using this and \eqref{eq:fano_spl}, we conclude that
\begin{align*}
m\geq \frac{nN(l) - 2\log{n}- h_2(\delta) - \delta k\log{n}}{\frac{1}{2}\log{\inp{1+ \frac{l}{\sigma^2}\inp{2-\frac{l}{k}}}}}.
\end{align*}
\iffalse
Now, suppose $\vecA$ is a deterministic sensing matrix which satisfies the power constraint~\eqref{eq:power_constraint}. That is,
\begin{align*}
k&\geq \bbE\insq{\inp{\vecA_i^T\bx}^2}\\
&
\end{align*} When $\vecA$ is any fixed sensing matrix, we follow same steps, though the value of the variance will be slightly different.
\begin{align*}
\bbE&\insq{\inp{y_i}^2|\tilde{\bx}= \bar{\bx}, \vecA_{i, j\in \cS_{\tilde{\bx}}} = \mathbf{a}_{i, j\in \cS_{\tilde{\bx}}}} = \bbE\insq{\inp{\vecA_i \bx +\vecZ}^2|\tilde{\bx}= \bar{\bx}, \vecA_{i, j\in \cS_{\tilde{\bx}}} = \mathbf{a}_{i, j\in \cS_{\tilde{\bx}}}}\\
& =\bbE\insq{\inp{\vecA_i \bx}^2|\tilde{\bx}= \bar{\bx}, \vecA_{i, j\in \cS_{\tilde{\bx}}} = \mathbf{a}_{i, j\in \cS_{\tilde{\bx}}}} + \sigma^2\\
& = \frac{k-l}{k}\sum_{j\in \cS_{\tilde{\bx}}}a_{i,j}^2 + \frac{l}{n-k}\sum_{j'\notin \cS_{\tilde{\bx}}}a_{i,j'}^2 + 2\frac{k-l}{k}\frac{k-l-1}{k-1}\sum_{j,l\in \cS_{\tilde{\bx}}, j\neq l}a_{i,j}a_{i,l}\\
&+  2\frac{l}{n-k}\frac{l-1}{n-k-1}\sum_{j',l'\notin \cS_{\tilde{\bx}}, j'\neq l'}a_{i,j'}a_{i,l'} + 2\frac{l}{n-k}\frac{k-l}{k}\sum_{j\in \cS_{\tilde{\bx}},l'\notin \cS_{\tilde{\bx}}}a_{i,j'}a_{i,l'}\\
&\leq \frac{k-l}{k}\sum_{j\in \cS_{\tilde{\bx}}}a_{i,j}^2 + \frac{l}{n-k}\sum_{j'\notin \cS_{\tilde{\bx}}}a_{i,j'}^2 + 2\inp{\frac{k-l}{k}}^2\sum_{j,l\in \cS_{\tilde{\bx}}, j\neq l}a_{i,j}a_{i,l}+ \sigma^2\\
&+  2\inp{\frac{l}{n-k}}^2\sum_{j',l'\notin \cS_{\tilde{\bx}}, j'\neq l'}a_{i,j'}a_{i,l'} + 2\frac{l}{n-k}\frac{k-l}{k}\sum_{j\in \cS_{\tilde{\bx}},l'\notin \cS_{\tilde{\bx}}}a_{i,j'}a_{i,l'}+ \sigma^2\\
\end{align*} and 

\begin{align*}
\inp{\bbE\insq{y_i|\tilde{\bx}= \bar{\bx}, \vecA_{i, j\in \cS_{\tilde{\bx}}} = \mathbf{a}_{i, j\in \cS_{\tilde{\bx}}}}}^2 &= \inp{\bbE\insq{\vecA_i \bx +\vecZ|\tilde{\bx}= \bar{\bx}, \vecA_{i, j\in \cS_{\tilde{\bx}}} = \mathbf{a}_{i, j\in \cS_{\tilde{\bx}}}}}^2\\
& =\inp{\bbE\insq{{\vecA_i \bx}|\tilde{\bx}= \bar{\bx}, \vecA_{i, j\in \cS_{\tilde{\bx}}} = \mathbf{a}_{i, j\in \cS_{\tilde{\bx}}}}}^2\\
& = \inp{\frac{k-l}{k}\sum_{j\in \cS_{\tilde{\bx}}}a_{i,j} + \frac{l}{n-k}\sum_{j'\notin \cS_{\tilde{\bx}}}a_{i,j'}}^2\\
&= \inp{\frac{k-l}{k}}^2\sum_{j\in \cS_{\tilde{\bx}}}a_{i,j}^2 + \inp{\frac{l}{n-k}}^2\sum_{j'\notin \cS_{\tilde{\bx}}}a_{i,j'}^2 + 2\inp{\frac{k-l}{k}}^2\sum_{j,l\in \cS_{\tilde{\bx}}, j\neq l}a_{i,j}a_{i,l}\\
&+  2\inp{\frac{l}{n-k}}^2\sum_{j',l'\notin \cS_{\tilde{\bx}}, j'\neq l'}a_{i,j'}a_{i,l'} + 2\frac{l}{n-k}\frac{k-l}{k}\sum_{j\in \cS_{\tilde{\bx}},l'\notin \cS_{\tilde{\bx}}}a_{i,j'}a_{i,l'}\\
\end{align*}
Thus,
\begin{align*}
\bbE&\insq{\inp{y_i}^2|\tilde{\bx}= \bar{\bx}, \vecA_{i, j\in \cS_{\tilde{\bx}}} = \mathbf{a}_{i, j\in \cS_{\tilde{\bx}}}} - \inp{\bbE\insq{y_i|\tilde{\bx}= \bar{\bx}, \vecA_{i, j\in \cS_{\tilde{\bx}}} = \mathbf{a}_{i, j\in \cS_{\tilde{\bx}}}}}^2\\
&\leq \frac{k-l}{k}\sum_{j\in \cS_{\tilde{\bx}}}a_{i,j}^2\inp{1-\frac{k-l}{k}} + \frac{l}{n-k}\sum_{j'\notin \cS_{\tilde{\bx}}}a_{i,j'}^2\inp{1-\frac{l}{n-k}}+ \sigma^2\\
\end{align*}

Thus, 
\begin{align*}
h&(y_i|\vecA_{i, j\in \cS_{\tilde{\bx}}} =\mathbf{a}_{i, j\in \cS_{\tilde{\bx}}}, \tilde{\bx}) = \sum_{\tilde{\bx}} \bbP\inp{\tilde{\bx} = \bar{\bx}}h(y_i|\tilde{\bx}= \bar{\bx}, \vecA_{i, j\in \cS_{\tilde{\bx}}}=\mathbf{a}_{i, j\in \cS_{\tilde{\bx}}})\\
&\leq \sum_{\tilde{\bx}} \bbP\inp{\tilde{\bx} = \bar{\bx}}\frac{1}{2}\log\inp{2\pi e\inp{\frac{k-l}{k}\sum_{j\in \cS_{\tilde{\bx}}}a_{i,j}^2\inp{1-\frac{k-l}{k}} + \frac{l}{n-k}\sum_{j'\notin \cS_{\tilde{\bx}}}a^2_{i,j'}\inp{1-\frac{l}{n-k}}+ \sigma^2}}\\
&\stackrel{(a)}{\leq} \frac{1}{2}\log\inp{2\pi e\inp{\frac{k-l}{k}\sum_{j\in [1:n]}\frac{k}{n}a_{i,j}^2\inp{1-\frac{k-l}{k}} + \frac{l}{n-k}\sum_{j'\in [1:n]}\frac{n-k}{n}a^2_{i,j'}\inp{1-\frac{l}{n-k}}+ \sigma^2}}\\
&\leq \frac{1}{2}\log\inp{2\pi e\inp{\frac{k-l}{k}\frac{k}{n}n\inp{1-\frac{k-l}{k}} + \frac{l}{n-k}\frac{n-k}{n}n\inp{1-\frac{l}{n-k}}+ \sigma^2}}\\
&\leq \frac{1}{2}\log\inp{2\pi e\inp{l\inp{2-\frac{l}{k}-\frac{l}{n-k}}+ \sigma^2}}
\end{align*}where $(a)$ follows from Jenson's inequality and $(b)$ follows by noting that $\bbE\insq{A_{i,j}^2} =1$ for any $i, j$ and $\tilde{\bx}$ is $k$-sparse.

Using this and \eqref{eq:fano_spl}, we conclude that
\begin{align*}
m\geq \frac{nN(l) - 2\log{n}- h_2(\delta) - \delta k\log{n}}{\log{\inp{1+ \frac{l}{\sigma^2}\inp{\inp{2-\frac{l}{k}-\frac{l}{n-k}}}}}}.
\end{align*}

\fi
\iffalse

Computing $\mathsf{Var}(y_i)$ conditioned on $\tilde{\bx}= \bar{\bx}$, we see that 
\begin{align*}
\mathsf{Var} = \sum_{j: \tilde{x}_j = 1}a_{i,j}^2\inp{1-\frac{q}{p}}\frac{q}{p} + \sum_{l: \tilde{x}_l = 0}a_{i,l}^2\inp{1-\frac{q}{1-p}}\frac{q}{1-p}
\end{align*}
Thus, we have
\begin{align*}
&\sum_{i = 1}^{m}\sum_{\tilde{\bx}= \bar{\bx}}\bbP\inp{\tilde{\bx}= \bar{\bx}}\inp{H(Z_{i, \tilde{\bx}})-H(W_i)}\\
&=\sum_{i = 1}^{m}\sum_{\tilde{\bx}= \bar{\bx}}\bbP\inp{\tilde{\bx}= \bar{\bx}}\frac{m}{2}\log\inp{\frac{\inp{\sum_{j: \tilde{x}_j = 1}a_{i,j}^2\inp{1-\frac{q}{p}}\frac{q}{p} + \sum_{l: \tilde{x}_l = 0}a_{i,l}^2\inp{1-\frac{q}{1-p}}\frac{q}{1-p}}}{\sigma^2}+1}\\
&=\bbE_{\tilde{x_1}\ldots\tilde{x_n}}\insq{\frac{m}{2}\log\inp{\frac{\inp{\sum_{j}\tilde{X}_ja_{i,j}^2\inp{1-\frac{q}{p}}\frac{q}{p} + (1-\tilde{X}_j)a_{i,j}^2\inp{1-\frac{q}{1-p}}\frac{q}{1-p}}}{\sigma^2}+1}}\\
&\stackrel{(a)}{\leq}\frac{m}{2}\log\inp{\frac{\inp{\sum_{j}\bbE\insq{\tilde{X}_j}a_{i,j}^2\inp{1-\frac{q}{p}}\frac{q}{p} + (1-\bbE\insq{\tilde{X}_j})a_{i,j}^2\inp{1-\frac{q}{1-p}}\frac{q}{1-p}}}{\sigma^2}+1}\\
&=\frac{m}{2}\log\inp{\frac{\inp{\sum_{j}a_{i,j}^2\inp{1-\frac{q}{p}}q + a_{i,j}^2\inp{1-\frac{q}{1-p}}q}}{\sigma^2}+1}\\
&=\frac{m}{2}\log\inp{\frac{\inp{S^2\inp{\inp{1-\frac{q}{p}}q + \inp{1-\frac{q}{1-p}}q}}}{\sigma^2}+1}
\end{align*} where $(a)$ follows from Jenson's inequality.
Thus, 
\begin{align*}
&\frac{m}{2}\log\inp{\frac{\inp{S^2\inp{\inp{1-\frac{q}{p}}q + \inp{1-\frac{q}{1-p}}q}}}{\sigma^2}+1}\\
&\qquad\geq H(\bx|\tilde{\bx})- h_2(\delta) + \delta\log(2^n - 1)
\end{align*} and 

\begin{align*}
m&\geq \max_q\frac{H(\bx|\tilde{\bx}) - h_2(\delta) + \delta\log(2^n - 1)}{\frac{1}{2}\log\inp{\frac{\inp{S^2\inp{\inp{1-\frac{q}{p}}q + \inp{1-\frac{q}{1-p}}q}}}{\sigma^2}+1}}\\
& = \max_q\frac{n p H\inp{\frac{q}{p}} + n(1-p)H\inp{\frac{q}{1-p}} - h_2(\delta) + \delta\log(2^n - 1)}{\frac{1}{2}\log\inp{\frac{\inp{S^2\inp{\inp{1-\frac{q}{p}}q + \inp{1-\frac{q}{1-p}}q}}}{\sigma^2}+1}}
\end{align*}
\fi



\end{proof}

\iffalse


\begin{thm}
For any sensing matrix $\mathbf{a}$ where each measurement $\mathbf{a}_i = (a_{i,1},\ldots, a_{i, n})$ satisfies ${\sum_{j=1}^na^2_{i,j}} \leq n$, a $\delta$-correct algorithm exists for the problem of $\spl$ only if $m\geq \frac{2k\log\inp{n/k}}{\log\inp{\frac{k}{\sigma^2}+1}}$.
\end{thm}
\begin{proof}
\begin{align*}
I(\by;\bx) &= H(\bx)-H(\bx|\by)\\
 &\stackrel{(a)}{\geq} \log{n \choose k } - h_2(\delta) + \delta\log\inp{{n \choose k } - 1}
\end{align*}
We also see that
\begin{align*}
I(\by;\bx) &= \sum_{i=1}^{m}I(y_i;\bx|y_{j\in[1:i-1]}, \vecA)\\
& = \sum_{i = 1}^{m}\inp{H(y_i|y_{j\in[1:i-1]}, \vecA)-H(y_i|\bx, y_{j\in[1:i-1]}, \vecA)}\\
&\leq \sum_{i = 1}^{m}\inp{H(y_i)-H(\mathbf{a}\bx + z_i|\bx, y_{j\in[1:i-1]}, \vecA)}\\
&=\sum_{i = 1}^{m}\inp{H(y_i)-H(z_i)}\\
&\leq \sum_{i = 1}^{m}\inp{H(W_i)-H(z_i)}
\end{align*} where in the last inequality, $W_i\sim\cN\inp{0, \sigma^2_w}$ where $\mathsf{Var}(y_i)\leq \sigma^2_w$. We will now compute $\mathsf{Var}(y_i)$\\
\begin{align*}
\mathsf{Var}(y_i) &= \bbE\insq{\inp{\sum_{j=1}^na_{i,j}x_j+z_i}^2}\\
&\qquad\qquad- \inp{\bbE\insq{\sum_{i=1}^na_{i,j}x_j+z_i}^2}
\end{align*} We first compute $\bbE\insq{\sum_{j=1}^na_{i,j}x_j+z_i}$.
\begin{align*}
\bbE\insq{\sum_{j=1}^na_{i,j}x_j+z_i} &= \frac{1}{{n \choose k }}\sum_{j}{n-1 \choose k-1}a_{i,j}\\
& = \frac{k}{n}\sum_ja_{i,j}.
\end{align*}
Also,
\begin{align*}
\bbE&\insq{\inp{\sum_{j=1}^na_{i,j}x_j+z_i}^2}\\
&= \frac{1}{{n \choose k }}\sum_{j}{n-1 \choose k-1}a^2_{i,j} + \frac{1}{{n \choose k}}\sum_{j}\sum_{l\neq i}{n-2 \choose k-2}a_{i,j} a_{i,l}+\sigma^2\\
& = \frac{k}{n}\sum_j a_j^2 + \sum_{j}\sum_{l\neq j}\frac{k(k-1)}{n(n-1)}a_{i,j} a_{i,l}+\sigma^2.
\end{align*}
Thus,
\begin{align*}
\mathsf{Var}&(y_i) = \frac{k}{n}\sum_j a_{i,j}^2 + \sum_{j}\sum_{l\neq j}\frac{k(k-1)}{n(n-1)}a_{i,j} a_{i,l} +\sigma^2\\
&\qquad- \inp{\frac{k}{n}\sum_ja_{i,j}}^2\\
&=\inp{\frac{k}{n}\sum_ja_{i,j}^2}\inp{1-\frac{k}{n}} +\sigma^2 \\
&\qquad+ \sum_{j}\sum_{l\neq j}\frac{k}{n}a_{i,j} a_{i,l}\inp{\frac{k-1}{n-1}-\frac{k}{n}}\\
&\stackrel{(a)}{\leq}\inp{\frac{k}{n}\sum_ja_{i,j}^2}\inp{1-\frac{k}{n}}+\sigma^2\\
&\stackrel{(b)}{\leq}k\inp{1-\frac{k}{n}}+\sigma^2.
\end{align*} where $(a)$ holds because $\frac{k-1}{n-1}\leq \frac{k}{n}$ when $k\leq n$ and $(b)$ holds because ${\sum_{j=1}^na^2_{i,j}} \leq n$.

Thus, we have
\begin{align*}
&\sum_{i = 1}^{m}\inp{H(z_i)-H(W_i)}\\
&=\frac{m}{2}\log\inp{\frac{\inp{k\inp{1-k/n}}}{\sigma^2}+1}
\end{align*}
Thus, 
\begin{align*}
\frac{m}{2}&\log\inp{\frac{\inp{k\inp{1-k/n}}}{\sigma^2}+1}\\
&\geq \log{n \choose k } - h_2(\delta) + \delta\log\inp{{n \choose k } - 1}
\end{align*} and 
\begin{align}
m&\geq \frac{\log{n \choose k } - h_2(\delta) + \delta\log\inp{{n \choose k } - 1}}{\frac{1}{2}\log\inp{\frac{k(1-k/n)}{\sigma^2}+1}}\\
&\approx \frac{2k\log\inp{n/k}}{\log\inp{\frac{k}{\sigma^2}+1}}
\label{eq:lower_bound_fano}
\end{align} for $k<<n$ and $\delta\rightarrow 0$.



\end{proof}

\fi
% \section{ADDITIONAL EXPERIMENTS}

% If you have additional experimental results, you may include them in the supplementary materials.

% \subsection{The Effect of Regularization Parameter}

% \textit{Our algorithm depends on the regularization parameter $\lambda$. Figure 1 below illustrates the effect of this parameter on the performance of our algorithm. As we can see, [ ... ]}

% \vfill


