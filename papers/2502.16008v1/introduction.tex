\section{Introduction}\label{sec:intro}
Sparse linear regression and compressed sensing have been a topic of intense research in statistics and signal processing for the past few decades~\cite{candes2006robust,donoho2006compressed,tibshirani1996regression}.
The problem of \textbf{binary} sparse linear regression (\spl) considers linear measurements of an unknown binary vector, corrupted by additive Gaussian noise. Focusing on binary signals, this particular problem has recently been studied in ~\cite{david2017high,gamarnik2017sparse,gamarnik2022sparse,pmlr-v99-reeves19a}, mainly motivated by the question of {\em support recovery} of sparse signals~\cite{wainwright2009sharp}. Formally, for an unknown $k$-sparse signal $\bx\in \inb{0,1}^n$, a sensing matrix $\vecA\in \reals^{m\times n}$ and a noise vector $\bz = (z_1, \ldots, z_m)$ where $z_i$s are iid $\cN(0, \sigma^2)$ for some variance $\sigma^2$, we observe $\by$ given by 
\begin{align}
    \by = \vecA \bx + \bz.\label{eq:spl}
\end{align} 
Our goal is to design the (possibly random) sensing matrix $\vecA$ with a power constraint, i.e., 
\begin{align}
  \bbE[(\vecA_i^T \bx)^2] \leq k, i =1, \dots, m,\label{eq:power_constraint}   
\end{align}
where {$\vecA_i$ denotes $i^{th}$ row of the matrix $\vecA$ and} the expectation is over the possible randomness in $\vecA$, and a decoding algorithm $\phi$ such that
\begin{align}
\max_{\bx\in\inb{0,1}^n, \wh{\bx} = k}\bbP\inp{\phi(\vecA, \by) \neq \bx}\rightarrow 0 \text{ as } n\rightarrow \infty.\label{eq:pe_spl}
\end{align} 
Here, $\wh{\bx}$ denotes the Hamming weight of $\bx\in \inb{0,1}^n$. The probability is computed over the randomness of the sensing matrix and the (randomized) algorithm. 

The problem of one bit quantized linear measurements (also known as one bit compressed sensing (\bcs)) is similar, except that  the output vector $\by$ is the sign of $\vecA\bx+\bz$ instead of the entire vector $\vecA\bx+\bz$~\cite{DBLP:conf/ciss/BoufounosB08}. That is, we observe
\begin{align}
\by  = \sign{\vecA\bx+\bz}.\label{eq:bcs}
\end{align}Here, $\by = (y_1, \ldots, y_m)$ is defined as $y_i = \sign{\vecA_i^T \bx+z_i}$, $i\in [1:m]$ where $\sign{a} = 1$ if $a\geq 0$ and $\sign{a} = -1$ otherwise. An algorithm $\phi'$ for \bcs\ takes input  $\by$ and $\vecA$. Again, we require that 
\footnote{The probability
 of error measured by \eqref{eq:pe_1bcs} 
 corresponds to the {\em`for each'} criterion in the one bit compressed sending literature. The {\em`for all'} criterion which requires that the same sensing matrix works for all unknown signals corresponds to showing $\bbP\inp{\exists \bx \text{ such that }\phi'(\vecA, \by) \neq \bx}\rightarrow 0$ as $n\rightarrow \infty$.}
\begin{align}
\max_{\bx\in\inb{0,1}^n, \wh{\bx} = k}\bbP\inp{\phi'(\vecA, \by) \neq \bx}\rightarrow 0 \text{ as } n\rightarrow \infty.\label{eq:pe_1bcs}
\end{align} 
 Usually in 1-bit compressed sensing, the Gaussian noise before quantization is not present. Our  formulation  can be considered as a {\em sparse} ``probit model''~\cite{mccullagh2019generalized}.


More generally, we define the problem of generalized linear measurements (GLMs) , e.g., \cite{kakade2011efficient,vershyninPlan} where we assume that the observation $\by = \inp{y_1, \ldots,y_m}$ is related to the sparse binary input vector $\bx$ using an ``inverse link'' function $g$ such that for each $i\in [m]$, 
\begin{align}
    \bbE\insq{y_i|\vecA_i} = g\inp{\vecA_i^T\bx}.\label{eq:glm}
\end{align} 
That is, the expected value of the output $y_i$ is linked to $\vecA_i$ only through $\vecA_i^T\bx$. For example, for \spl\ $$\bbE\insq{y_i|\vecA_i} = \vecA_i^T\bx,$$ for \bcs\ $$\bbE\insq{y_i|\vecA_i} = 1-2\Phi\inp{\frac{-A_i^T\bx}{\sigma}}$$ where $\Phi$ is the Gaussian cumulative distribution function.

In the logistic regression model (\logreg), we observe a binary output $y_i\in \inb{-1, 1}$ for each measurement $i\in [m]$. The probability that $y_i$ takes value 1 is given by 
\begin{align*}
\bbP\inp{y = 1} = \frac{1}{1+e^{-\beta \ba^T\bx}}.
\end{align*} for parameter $\beta>0$. The parameter $\beta$ controls the level of noise. When $\beta \rightarrow \infty$, the model approaches  noiseless one bit compressed sensing. As $\beta$ decreases, the output becomes more noisy. When $\beta = 0$, the output is uniformly distributed on $\inb{-1,1}$ and is independent of $\bx$.
In this model,  $$\bbE\insq{y_i|\vecA_i} = \tanh{\frac{\beta\vecA_i^T\bx}2}.$$ %1-2\frac{e^{-\beta\vecA_i^T\bx}}{1+e^{-\beta\vecA_i^T\bx}}.$$

\paragraph{Our contributions.} In this paper, our contributions are the following:
\begin{itemize}
    \item We analyze the linear estimation+projection algorithm \cite{vershyninPlan}   for generalized linear measurements of sparse binary inputs (Theorem~\ref{thm:alg_general}). We also provide an information theoretic lower bound (Theorem~\ref{thm: lower_bdglm}).
    \item As corollaries, we obtain tight sample complexity characterization for noisy one bit compressed sensing   (Corollary~\ref{thm:alg_bcs} and Corollary~\ref{thm: lower_bd_bcs}) and logistic regression (Corollary~\ref{thm:alg_logreg} and Corollary~\ref{thm: lower_bd_log_reg}). %The upper bound follows from a simple efficient algorithm which has a sample complexity of $O(k+\sigma^2)\log{n}$. The lower bound is $\Omega(k+\sigma^2)\log{n/k}$. The lower bound is shown for a sensing matrix where each entry is chosen iid $N(0,1)$, though the lower bound of $k\log{n/k}$ holds for any sensing matrix. These results also imply that there is no statistical-computational gap for this \bcs.
    \item The algorithm can be used for \spl\ either directly (Corollary~\ref{thm:alg_spl}) or by first quantizing the received signal to its sign value and then using the algorithm for \bcs. The sample complexity is the same for both these cases. This shows that in the regime where the number of measurements are at least $C(k+\sigma^2)\log{n}$ for some constant $C$, keeping only the sign information is sufficient for \spl. %It has been conjectured in \cite{gamarnik2017sparse} that efficient algorithms do not exist outside of this regime. 
 %   \item It was observed in \cite{david2017high} that in the no-noise regime ($\sigma^2 = 0$), one measurement is sufficient to recover the underlying vector by brute force. However, it is conjectured that there is no efficient algorithm if $m\leq 2k\log{n}$ and the entries of the sensing matrix are chosen iid $\cN(0,1)$. We provide a simple efficient algorithm which recovers the underlying binary vector with  just one (non-random) measurement. This algorithm might continue to work for very low levels of noise. This suggests that for specific non-random constructions, there may be efficient algorithms in the conjectured hardness regime.
    % \item Our results are all for the `for each' case. Though, we can use a union bound to show a `for all' result, though it will not be sample complexity efficient. 
  %  \item Our results hold in all regimes of sparsity and noise variance.
    \item We provide ``almost'' matching information theoretic lower (Corollary~\ref{thm: spl_lower_bd_1}) and upper bounds (Theorem~\ref{thm:upper_bd_mle}) for exact recovery in \spl. If the measurements are Gaussian, we get slightly better lower bounds (Theorem~\ref{thm:lower_bd_spl}).
\end{itemize}


\subsection{Discussion of results and related works}
\paragraph{Intuitions on lower bounds.} Observe that \bcs\ is a strictly more difficult problem than \spl\ in the sense that any algorithm that works for \bcs\ can be used for \spl\ by using only the sign information. Thus, the sample complexity of \bcs\ is at least as much as \spl, the latter can be much smaller in some cases. From an information theoretic viewpoint, a randomly chosen $k$-sparse vector $\bx$ has entropy $\log{n \choose k}\approx k\log\frac{n}{k}$. Since each $y_i$ can give at most one bit of information, we need at least $k\log\frac{n}{k}$ measurements for \bcs\ (See Corollary~\ref{thm: lower_bd_bcs} for the exact lower bound) to learn $\bx$. For \spl\ on the other hand, the output has infinite precision. In fact, we can show that in the absence of noise, only one sample is sufficient to recover the unknown signal (see Remark~\ref{remark:noNoise}). \spl\ can be viewed as a coding problem for a Guassian channel, where $\bx$ is the message and $\vecA\bx$ is its corresponding codeword. Thus, from the converse for Gaussian channel (see [Theorem 9.1.1] \cite{thomas2006elements}), we need at least $\frac{k\log\inp{n/k}}{C}$ samples for exact recovery. Here $C$ is the capacity of the Gaussian channel, which depends on SNR (a function of $\vecA\bx$ and $\sigma^2$). 
Given the power constraint of Eq.~\eqref{eq:power_constraint} (which is satisfied when entries of $\vecA$ are chosen iid $\cN(0,1)$),  the capacity $C$ is $\frac{1}{2}\log\inp{1+\frac{k}{\sigma^2}}$, thereby showing that the lower bound for \spl\ can be much smaller. 



\paragraph{Binary sparse linear regression.} The problem of binary sparse linear regression was introduced in \cite{david2017high, gamarnik2022sparse} and was further studied in \cite{pmlr-v99-reeves19a}. An {\em``all or nothing''} phenomenon was shown in \cite{pmlr-v99-reeves19a} for {\em approximate recovery} of binary vectors at the critical sample complexity of $m^* \triangleq \frac{2k\log{n/k}}{\log\inp{1+\frac{k}{\sigma^2}}}$, showing that approximate recovery is possible if and only if $m\geq m^*$. It was additionally conjectured in \cite{david2017high} that no efficient algorithms exist in the regime $m^*\leq m \leq m_{\mathsf{alg}} \triangleq (2k+\sigma^2)\log{n}$. When $m\geq m_{\mathsf{alg}}$, various algorithms like Lasso~\cite{wainwright2009sharp}, Orthogonal Matching Pursuit (OMP)~\cite{tropp2007signal} and \cite{ndaoud2020optimal} can recover the sparse vector. It has also been shown in~\cite{gamarnik2017sparse} that lasso fails to recover unknown vector $\bx$ when $m\leq c\, m_{\mathsf{alg}}$ for some small constant $c$. Outside this regime, a local search algorithm was proposed~\cite{gamarnik2017sparse}, which starts with a guess of $\bx$ and iteratively updates it.

In \cite{pmlr-v99-reeves19a}, the information theoretic lower bound of $m^*$ is shown for the case when each entry of the sensing matrix is chosen iid $\cN(0,1)$. 
We consider the exact recovery guarantee for the problem and show that $m \geq m^*$ samples are necessary even when the sensing matrix is not Gaussian (Theorem~\ref{thm: spl_lower_bd_1})\footnote{Our lower bounds hold for a weaker average probability of error recovery criteria, instead of the maximum probability of Eq.~\eqref{eq:pe_spl}, hence are more potent.}. We show an almost matching upper bound based on the Maximum Likelihood Estimator (MLE) using a random Gaussian sensing matrix (Theorem~\ref{thm:upper_bd_mle} and Theorem~\ref{thm:lower_bd_spl}). This is along the lines of the MLE analysis in \cite{pmlr-v99-reeves19a}, which was done for approximate recovery (our sample complexity for exact recovery turns out to be slightly different).   
\begin{remark}\label{remark:noNoise}
It was observed in \cite{david2017high} that in the no-noise regime ($\sigma^2 = 0$), one measurement is sufficient to recover the underlying vector by brute force. However, it is conjectured that there is no efficient algorithm if $m\leq 2k\log{n}$. The results in \cite{david2017high} were shown only when the entries of the sensing matrix are chosen iid $\cN(0,1)$ (i.e. Gaussian design). For an arbitrary sensing matrix, an efficient way to recover $\bx$ using only one measurement is by using $\vecA = \frac{1}{2^n}[1, 2, 2^2, \ldots,2^{n-1}]$. Note that $2^n \times \by$ in this case is the value of unknown signal in the decimal system (base 10). It can be converted to binary in $O(n)$ time. This suggests that for specific non-random constructions, there may be efficient algorithms in the conjectured hardness regime.
\end{remark}



\paragraph{Binary one-bit compressed sensing.} The problem of  one bit compressed sensing has been well studied~e.g.~\cite{DBLP:conf/ciss/BoufounosB08,jacques2013robust} including greedy algorithms (e.g.~\cite{liu2016one}) and noisy test outcomes (e.g.~\cite{matsumoto2024robust}), and the problem of recovering binary vectors has also been studied in~\cite{acharya2017improved,mazumdar2022support}. However, these works do not consider the Gaussian noise prior to quantization. The best known upper bound ($O(k/\epsilon)$ from~\cite{matsumoto2022binary}) when specialized to exact recovery for binary sparse vectors requires $O(k^{3/2})$ (by choosing $\epsilon = 1/\sqrt{k}$). On the other hand, our bound is $O(k\log{n})$. This discrepancy is because the previous models are studied for the ``for all'' model which is a harder problem than our present ``for each'' model. The results in \cite{vershyninPlan}, on the other hand, are for the ``for each'' model, though their analysis is not optimal for binary vectors (see Appendix~\ref{sec:comparison_PV}). The problem of noisy one bit compressed sensing (\bcs) introduced here is motivated by the probit model (e.g. see a modern treatments of the non-sparse probit model~\cite{kuchelmeister2024finite}). Here we provide an information theoretic lower bound of $m\geq \inp{k+\sigma^2}\log\inp{n/k}$ and show that the aforementioned efficient algorithm  (Algorithm~\ref{alg:1}) works with the same $m =O((k+\sigma^2)\log{n})$ samples and has a computational complexity of $O((k+\sigma^2)n\log{n})$. We also provide optimal sample complexity characterization for learning binary sparse vectors under the logistic regression model, which was previously studied for learning real vectors \cite{hsu2024sample,vershyninPlan}.

\paragraph{Algorithm for binary vectors.} We consider a simple algorithm which is equivalent to the ``average algorithm''~\cite{servedio1999pac} or ``linear estimator''~\cite{vershyninPlan}, followed by a selection of the `top-k' coordinates. Regarding the intuition behind the algorithm, we observe that for an unknown signal $\bx$, the output $\by$ and $\vecA_{\cS_{\bx}}$, the restriction of the sensing matrix to columns where $\bx$ is 1,  are correlated whereas $\by$ and $\vecA_{[1:n]\setminus \cS_{\bx}}$ are uncorrelated. Here, $\vecA_{[1:n]\setminus \cS_{\bx}}$ denotes the restriction of the sensing matrix to columns where $\bx$ is 0.  Thus, we compute the inner product between $\by$ and each column of the sensing matrix as a proxy for correlation between the output and the corresponding column. The output of the algorithm is the top $k$-most correlated columns (See Algorithm~\ref{alg:1} for details.). One can also think of this as a ``one-shot'' version of the popular OMP algorithm. This algorithm requires $O((k+\sigma^2)\log{n})$ samples for \bcs\ and \spl\, and $O((k+1/\beta^2)\log{n})$ for \logreg. It has a computation complexity of $O((k+\sigma^2)n\log{n})$. Most of the previous algorithms, including the one in \cite{vershyninPlan}, were given for the case when the unknown signal is not necessarily binary. It should be noted that the black-box application of the result of \cite{vershyninPlan}  specialized to binary inputs will not recover the optimal sample complexity. See Appendix~\ref{sec:comparison_PV} where we show that the results in \cite{vershyninPlan} imply a sample complexity of $O(k^2\log\inp{2n/k})$. We provide a simple yet  optimal analysis of the sample complexity in our special case of  sparse binary signals. 

We would like to emphasize that the sample complexity of Algorithm~\ref{alg:1} for both \spl\ and \bcs\ is the same ($O((k+\sigma^2)\log{n})$). 
This implies that for \spl, when $m$ is outside the conjectured hardness regime, we do not need the amplitude of $\by$, only the sign information is sufficient to recover the unknown signal. 

% This algorithmic intuition also works when the output is $\sign{\by}$, and it turns out that the same algorithm  works for \bcs. 
% As we will see later, we can show that when $m =O((k+\sigma^2)\log{n})$ (that is, $m$ is outside the conjectured hardness regime), we do not need the amplitude of $\by$, only the sign information is sufficient to recover the unknown signal. This is argued by showing that Algorithm~\ref{alg:1} recovers the unknown signal for the noisy \bcs\ problem with $O((k+\sigma^2)\log{n})$ samples.





\paragraph{Notation.}We will use boldfaced uppercase letters like $\vecA$ for matrices and lowercase letters such as $\bx$ for vectors. The entry of the matrix at $i^{\text{th}}$ row and $j^{\text{th}}$ column is denoted by $A_{i,j}$. Similarly, the $i^{\text{th}}$ entry of a vector $\bx$ is denotes by $x_i$.
For any binary vector $\bx = \inp{x_1, \ldots, x_n}$, we denote the set of indices $i$ where $x_i = 1$ by $\cS_{\bx}\subseteq [1:n]$ and we use $\vecA_{\cS_{\bx}}$ to denote the restriction of $\vecA$ to the columns where $\bx$ is 1. We use $\vecA_i$ to  denote $i^{\text{th}}$ row or $i^{\text{th}}$ column, depending on context. The correct notation is made clear where it is used. We denote the binary entropy function by $h_2(\cdot)$.



\paragraph{Organization.} We present the algorithm  and upper bounds in Section~\ref{sec:alg}. The information theoretic lower bounds are presented in Section~\ref{sec:sample_compexity}.  In Section~\ref{sec:tighter_bounds_spl}, we present an upper bound for \spl\ based on the maximum likelihood estimator. We also provide a lower bound in this section, which closely matches the upper bound. This lower bounds  does not follow as a corollary to the general lower bound theorem for GLMs (Theorem~\ref{thm: lower_bdglm}). It requires a separate analysis based on a conditional version of Fano's inequality. We provide proofs of the upper and lower bound for GLMs (Theorems~\ref{thm:alg_general} and \ref{thm: lower_bdglm}) in Section~\ref{sec:proofs}. Remaining proofs are delegated to Appendix~\ref{appendix:proofs}. We provide detailed comparison of our results with \cite{vershyninPlan} in Appendix~\ref{sec:comparison_PV}. We conclude with a discussion on open problems in Section~\ref{sec:conclusion}.
