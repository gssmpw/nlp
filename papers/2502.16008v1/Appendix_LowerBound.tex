\subsection{Missing proofs from Section~\ref{sec:sample_compexity}}\label{proof:sec:lower_bd}
\begin{proof}[Proof of Corollary~\ref{thm: lower_bd_bcs}]
Consider a sensing matrix $\vecA$ which satisfies the power constraint \eqref{eq:power_constraint}. 

% Suppose $\bx$ is distributed uniformly on the set of all $k$-sparse binary vectors and $\bs = \sign{\vecA\bx+\bz}$. Then,
% \begin{align}
% I(\vecA, \bs; \bx)  &= H(\bx) - H(\bx|\vecA, \bs)\nonumber\\
% &\stackrel{(a)}{\geq} \log{n \choose k } - h_2(\delta) - \delta\log\inp{{n \choose k } + 1}\nonumber\\
% &\geq k\log{n/k}- h_2(\delta)- \delta k\log\inp{n}\label{eq:lower_bd_bcs_1}
% \end{align} where $(a)$ follows from Fano's inequality \cite[Theorem~2.10.1]{thomas2006elements}.
% We also note that
% \begin{align*}
%  I(\vecA, \bs; \bx) &= I(\vecA ; \bx)    +  I(\bs; \bx|\vecA)\\
%  &\stackrel{(a)}{ = }0 + I(\bs; \bx|\vecA).
% \end{align*}where $(a)$ holds because $\vecA$ and $\bx$ are independent. Let $s_{j\in[1:i-1]}$ denote $\inp{s_1, \ldots, s_{i-1}}$. 
% \begin{align}
% I&(\bs; \bx|\vecA) = \sum_{i = 1}^{m}I(s_i; \bx|\vecA, s_{j\in[1:i-1]})\nonumber\\
% & = \sum_{i = 1}^{m}\Big(H(s_i|\vecA, s_{j\in[1:i-1]})\nonumber\\
% &\qquad- H(s_i| \bx,\vecA, s_{j\in[1:i-1]})\Big)\nonumber\\
% & \stackrel{(a)}{\leq}\sum_{i = 1}^{m}\inp{H(s_i|\vecA)- H(s_i| \bx,\vecA)}\nonumber\\
% & = \sum_{i = 1}^{m}I(s_i;\bx|\vecA)\label{eq:lower_bd_bcs}
% \end{align}where $(a)$ follows from $H(s_i|\vecA, s_{j\in[1:i-1]})\leq H(s_i|\vecA)$ and $H(s_i| \bx,\vecA, s_{j\in[1:i-1]}) = H(s_i| \bx,\vecA)$ as $s_i$ is conditionally independent of $s_{j\in[1:i-1]}$ conditioned on $\bx$ and $\vecA$.
% For any $i$, 
% \begin{align*}
% I(s_i;\bx|\vecA) &= H(s_i|\vecA)- H(s_i|\bx, \vecA)\\
% &\stackrel{(a)}{\leq} 1- H(s_i|\vecA_i^T\bx).
% \end{align*}where $(a)$ holds because $H(s_i|\vecA)\leq H(s_i) =1$ and $s_i$ is conditionally independent of $(\vecA_1\ldots, \vecA_{i-1}, A_{i+1}, \ldots, A_{m})$ and $\bx$ conditioned on $\vecA_i^T\bx$. 
Here $\vecA_i$, $i\in [1:m]$ denotes the $i^{\text{th}}$ row of the sensing matrix $\vecA$.
For any realization $b\in \bbR$ of $\vecA_i^T\bx$, 
\begin{align*}
\bbP(y_i = 1|\vecA_i^T\bx = b) &= \bbP(z_i\geq -b)= \bbP\inp{\frac{z_i}{\sigma}\geq \frac{-b}{\sigma}}\\
& = \frac{1- \sign{b}}{2} + \sign{b}Q\inp{\frac{|b|}{\sigma}}.
\end{align*} 

For $a>0$, let $R(a):=\frac{1}{\sqrt{2\pi}}\int_{0}^{a}e^{-u^2/2}du$. Then $Q(a) = \frac{1}{2}-R(a)$. Suppose $\bx$ is fixed. Then,
\begin{align*}
g(\vecA_i^T\bx) & =\bbE\insq{y_i|\vecA}  = \bbE\insq{y_i|\vecA_i^T\bx}\\
& = \frac{1- \sign{\vecA_i^T\bx}}{2} + \sign{\vecA_i^T\bx}Q\inp{\frac{|\vecA_i^T\bx|}{\sigma}} - \inp{1-\inp{\frac{1- \sign{\vecA_i^T\bx}}{2} + \sign{\vecA_i^T\bx}Q\inp{\frac{|\vecA_i^T\bx|}{\sigma}}}}\\
& = \sign{\vecA_i^T\bx}\inp{1-2Q\inp{\frac{|\vecA_i^T\bx|}{\sigma}}}\\
& = \sign{\vecA_i^T\bx}\inp{2R\inp{\frac{|\vecA_i^T\bx|}{\sigma}}}
\end{align*}
% This means that $H(s_i|\vecA_i^T\bx = b) = h_2\inp{Q(|b/\sigma|)}$ where $h_2(a)$ denotes the binary entropy of the distribution $(a, 1-a)$. We will lower bound $H(s_i|\vecA_i^T\bx = b)$ using a lower bound on the binary entropy function.
% We know that
% \begin{align*}
% H(s_i|\vecA_i^T\bx) &\stackrel{(a)}{=} \bbE\insq{h_2\inp{Q\inp{\frac{\inl{\vecA_i^T\bx}}{\sigma}}}}\\
% &\stackrel{(a)}{\geq} \bbE\insq{4{Q\inp{\frac{\inl{\vecA_i^T\bx}}{\sigma}}}\inp{1-Q\inp{\frac{\inl{\vecA_i^T\bx}}{\sigma}}}}
% \end{align*}
% where in $(a)$, the expectation is over $\vecA_i^T\bx$. The inequality $(b)$ follows from \cite[Theorem 1.2]{topsoe2001bounds}. 
% For $a>0$, let $R(a):=\frac{1}{\sqrt{2\pi}}\int_{0}^{a}e^{-u^2/2}du$. Then $Q(a) = \frac{1}{2}-R(a)$. Using this,
% \begin{align*}
% \bbE&\insq{h_2\inp{Q\inp{\frac{\inl{\vecA_i^T\bx}}{\sigma}}}}\\
% & \geq \bbE\insq{4\inp{\frac{1}{2} - R\inp{\frac{\inl{\vecA_i^T\bx}}{\sigma}}}\inp{\frac{1}{2} + R\inp{\frac{\inl{\vecA_i^T\bx}}{\sigma}}}}\\
% & = 1-4\bbE\insq{\inp{R\inp{\frac{\inl{\vecA_i^T\bx}}{\sigma}}}^2}.
% \end{align*}
For any $a>0$,
\begin{align*}
R\inp{a} &= \frac{1}{\sqrt{2\pi}}\int_{0}^{a}e^{-u^2/2}du\\
& \leq \frac{1}{\sqrt{2\pi}}\int_{0}^{a}1du = \frac{a}{\sqrt{2\pi}}.
\end{align*}
Thus, 
\begin{align*}
\bbE\insq{\inp{g\inp{\vecA_i^T\bx}^2}} &= \bbE\insq{\inp{2R\inp{\frac{|\vecA_i^T\bx|}{\sigma}}}^2}\\
&\leq \bbE\insq{4\inp{\frac{\vecA_i^T\bx}{\sqrt{2\pi}\sigma}}^2}\\
&\stackrel{(a)}{\leq}\frac{2k}{\pi\sigma^2} 
\end{align*}where $(a)$ follows from the power constraint $\bbE\insq{\inp{\vecA_i^T\bx}^2}\leq k$ (see \eqref{eq:power_constraint}). This holds for any $\bx$, including a randomly chosen sparse vector.
% \begin{align*}
%  \bbE\insq{h_2\inp{Q\inp{\frac{\vecA_i^T\bx}{\sigma}}}}&\geq 1-4\bbE\insq{\frac{\inp{\vecA_i^T\bx}^2}{{2\pi\sigma^2}}}\\
% & \stackrel{(a)}{\geq} 1-\frac{2k}{\pi\sigma^2}.
% \end{align*}Here, $(a)$ follows from the power constraint $\bbE\insq{\inp{\vecA_i^T\bx}^2}\leq k$ (see \eqref{eq:power_constraint}). 
% This implies that
% \begin{align*}
% I(s_i;\bx|\vecA)&\leq 1-\inp{1-\frac{2k}{\pi\sigma^2}} = \frac{2k}{\pi\sigma^2} 
% \end{align*}
% and from \eqref{eq:lower_bd_bcs_1} and \eqref{eq:lower_bd_bcs},
Thus,
\begin{align}
% m\frac{2k}{\pi\sigma^2} &\geq k\log{n/k}- h_2(\delta)- \delta k\log\inp{n}\\
m &\geq  \frac{\pi\sigma^2}{2k}k\log\inp{n/k}\inp{1-\frac{h_2(\delta)+ \delta k\log\inp{n}}{k\log{n/k}}}\nonumber\\
& \geq \sigma^2\log\inp{n/k}\inp{1-\frac{h_2(\delta)+ \delta k\log\inp{n}}{k\log{n/k}}}\label{eq:final_bound_bcs1}
\end{align}
% Thus,
% \begin{align}
% % m&\geq \frac{\pi\sigma^2}{2}\log\inp{n/k}\inp{1-\frac{h_2(\delta)+ \delta k\log\inp{n}}{k\log{n/k}}}\nonumber\\
% m&\geq \sigma^2\log\inp{n/k}\inp{1-\frac{h_2(\delta)+ \delta k\log\inp{n}}{k\log{n/k}}}\label{eq:final_bound_bcs1}
% \end{align}
On the other hand, $I(y_i;\bx|\vecA)\leq 1$. Thus,
\begin{align}
k\log&\inp{n/k}\inp{1-\frac{h_2(\delta)+ \delta k\log\inp{n}}{k\log{n/k}}}\nonumber\\
&\leq \sum_{i = 1}^{m}I(y_i;\bx|\vecA)\leq \sum_{i = 1}^{m}H(y_i|\vecA)\nonumber\\
&\leq m.\label{eq:final_bound_bcs2}
\end{align}
Combining \eqref{eq:final_bound_bcs1} and \eqref{eq:final_bound_bcs2}, we get the desired bound.
\end{proof}

\begin{proof}[Proof of Corollary~\ref{thm: lower_bd_log_reg}]
Consider a Gaussian sensing matrix $\vecA$. Suppose $\bx$ is distributed uniformly on the set of all $k$-sparse binary vectors. 
% Then,
% \begin{align}
% I(\vecA, \by; \bx)  &= H(\bx) - H(\bx|\vecA, \by)\nonumber\\
% &\stackrel{(a)}{\geq} \log{n \choose k } - h_2(\delta) - \delta\log\inp{{n \choose k } + 1}\nonumber\\
% &\geq k\log{n/k}- h_2(\delta)- \delta k\log\inp{n}\label{eq:log_lower_bd_bcw_1}
% \end{align} where $(a)$ follows from Fano's inequality [Theorem~2.10.1]\cite{thomas2006elements}.
% We also note that
% \begin{align*}
%  I(\vecA, \by; \bx) &= I(\vecA ; \bx)    +  I(\by; \bx|\vecA)\\
%  &\stackrel{(a)}{ = }0 + I(\by; \bx|\vecA).
% \end{align*}where $(a)$ holds because $\vecA$ and $\bx$ are independent. Let $y_{j\in[1:i-1]}$ denote $\inp{y_1, \ldots, y_{i-1}}$. 
% \begin{align}
% I&(\by; \bx|\vecA) = \sum_{i = 1}^{m}I(y_i; \bx|\vecA, y_{j\in[1:i-1]})\nonumber\\
% & = \sum_{i = 1}^{m}\Big(H(y_i|\vecA, y_{j\in[1:i-1]})\nonumber\\
% &\qquad- H(y_i| \bx,\vecA, y_{j\in[1:i-1]})\Big)\nonumber\\
% & \stackrel{(a)}{\leq}\sum_{i = 1}^{m}\inp{H(y_i|\vecA)- H(y_i| \bx,\vecA)}\nonumber\\
% & = \sum_{i = 1}^{m}I(y_i;\bx|\vecA)\label{eq:log_lower_bd_bcs}
% \end{align}where $(a)$ follows from $H(y_i|\vecA, y_{j\in[1:i-1]})\leq H(y_i|\vecA)$ and $H(y_i| \bx,\vecA, y_{j\in[1:i-1]}) = H(y_i| \bx,\vecA)$ as $y_i$ is conditionally independent of $y_{j\in[1:i-1]}$ conditioned on $\bx$ and $\vecA$.
% For any $i$, 
% \begin{align*}
% I(y_i;\bx|\vecA) &= H(y_i|\vecA)- H(y_i|\bx, \vecA)\\
% &\stackrel{(a)}{\leq} 1- H(y_i|\vecA_i^T\bx).
% \end{align*}where $(a)$ holds because $H(y_i|\vecA)\leq H(y_i) =1$ and $y_i$ is conditionally independent of $(\vecA_1\ldots, \vecA_{i-1}, A_{i+1}, \ldots, A_{m})$ and $\bx$ conditioned on $\vecA_i^T\bx$. Here $\vecA_i$, $i\in [1:m]$ denotes the $i^{\text{th}}$ row of the sensing matrix $\vecA$.

Suppose $t = \frac{1}{2}\tanh{\frac{\beta\vecA_i^T\bx}{2}} \inp{=\frac{(1-e^{-\beta \vecA_i^T\bx}}{2\inp{1+e^{-\beta \vecA_i^T\bx}}}}$. Then, 
\begin{align*}
\frac{1}{1+e^{-\beta \vecA_i^T\bx}} &= \frac{1}{2}+t\text{ and }\\
1-\frac{1}{1+e^{-\beta \vecA_i^T\bx}} &= \frac{1}{2}-t
\end{align*}

With this,
\begin{align*}
\bbE\insq{\inp{g\inp{\vecA_i^T\bx}^2}} &= \bbE\insq{4t^2}\\
& = \bbE\insq{\inp{\tanh{\frac{\beta\vecA_i^T\bx}{2}}}^2}
\end{align*}

% \begin{align*}
% H(y_i|\vecA_i^T\bx) &\stackrel{(a)}{=} \bbE\insq{h_2\inp{\frac{1}{1+e^{-\beta \vecA_i^T\bx}}}}\\
% &\stackrel{(b)}{\geq} \bbE\insq{4{\frac{1}{1+e^{-\beta \vecA_i^T\bx}}}\inp{1-\frac{1}{1+e^{-\beta \vecA_i^T\bx}}}}\\
% & = \bbE\insq{4\inp{\frac{1}{2} - t}\inp{\frac{1}{2} + t}}\\
% \end{align*}
% where in $(a)$, the expectation is over $\vecA_i^T\bx$. The inequality $(b)$ follows from [Theorem 1.2]\cite{topsoe2001bounds}. Thus,
% \begin{align*}
% \bbE&\insq{h_2\inp{\frac{1}{1+e^{-\beta \vecA_i^T\bx}}}}\\
% & \geq  1-4\bbE\insq{\inp{\frac{1}{2}\tanh{\frac{\beta\vecA_i^T\bx}{2}}}^2}\\
% & = 1 - \bbE\insq{\inp{\tanh{\frac{\beta\vecA_i^T\bx}{2}}}^2}.
% \end{align*}
Note that,
\begin{align*}
\bbE\insq{\inp{\tanh{\frac{\beta\vecA_i^T\bx}{2}}}^2} = 1-\bbE\insq{\inp{\text{sech}{\frac{\beta\vecA_i^T\bx}{2}}}^2}
\end{align*} and
\begin{align*}
\bbE\insq{\inp{\text{sech}{\frac{\beta\vecA_i^T\bx}{2}}}^2} &= \bbE\insq{\frac{1}{\inp{\text{cosh}{\frac{\beta\vecA_i^T\bx}{2}}}^2}}\\
&\stackrel{(a)}{\geq} \bbE\insq{e^{-\inp{\beta\vecA_i^T\bx/2}^2}}\\
&\stackrel{(b)}{=}\sqrt{\frac{1}{1+\beta^2 k/2}}\\
&\stackrel{(c)}{\geq}1-\frac{\beta^2k}{2}
\end{align*} where $(a)$ follows from the inequality $\text{cosh}(t)\leq e^{t^2/2}$ (see [Exercise 2.2.3]\cite{vershynin}),  $(b)$ follows from \eqref{eq:expectation_log} and $(c)$ holds because $1-\frac{x}{2}\leq \frac{1}{\sqrt{1+x}}$ for any $x\geq 0$.
Thus, 
\begin{align*}
\bbE\insq{\inp{g\inp{\vecA_i^T\bx}^2}} \leq \frac{\beta^2k}{2}.
\end{align*}
% \begin{align*}
% H(y_i|\vecA_i^T\bx)&\geq 1-\frac{\beta^2 k}{4}.
% \end{align*}
This implies that
% \begin{align*}
% I(y_i;\bx|\vecA)&\leq 1-\inp{1-\frac{\beta^2k}{4}} =\frac{\beta^2k}{4}
% \end{align*}
% and from \eqref{eq:log_lower_bd_bcw_1} and \eqref{eq:log_lower_bd_bcs},
\begin{align*}
% m\frac{2k}{\pi\sigma^2} &\geq k\log{n/k}- h_2(\delta)- \delta k\log\inp{n}\\
m\frac{\beta^2k}{2} &\geq  k\log\inp{n/k}\inp{1-\frac{h_2(\delta)+ \delta k\log\inp{n}}{k\log{n/k}}}
\end{align*}
Thus,
\begin{align}
m&\geq \frac{2}{\beta^2}\log\inp{n/k}\inp{1-\frac{h_2(\delta)+ \delta k\log\inp{n}}{k\log{n/k}}}\nonumber\\
&\geq \frac{1}{\beta^2}\log\inp{n/k}\inp{1-\frac{h_2(\delta)+ \delta k\log\inp{n}}{k\log{n/k}}}\label{eq:log_final_bound_bcs1}
\end{align}
We also  know that for any $i$, $I(y_i;\bx|\vecA)\leq H(y_i|\vecA)\leq 1$. Thus, we also obtain that
\begin{align}
m\geq k\log\inp{n/k}\inp{1-\frac{h_2(\delta)+ \delta k\log\inp{n}}{k\log{n/k}}}\label{eq:log_final_bound_bcs2}
\end{align}
Combining \eqref{eq:log_final_bound_bcs1} and \eqref{eq:log_final_bound_bcs2}, we get the desired bound.
\end{proof}



% \subsection{Proof of Theorem~\ref{thm: spl_lower_bd_1}}\label{proof:thm: spl_lower_bd_1}
\begin{proof}[Proof of Corollary~\ref{thm: spl_lower_bd_1}]
Suppose $\bx$ is generated uniformly at random from the set of all $k$-sparse vectors and $\vecA$ is any sensing matrix which satisfies the power constraint given by \eqref{eq:power_constraint}. Then,
% \begin{align}
% I(\vecA, \by;\bx) &= H(\bx)-H(\bx|\by, \vecA)\nonumber\\
%  &\stackrel{(a)}{\geq} \log{n \choose k } - h_2(\delta) - \delta\log\inp{{n \choose k } + 1}\nonumber\\
%  &\geq k\log\inp{\frac{n}{k}}- h_2(\delta) - \delta k\log{n}.\label{eq:new1}
% \end{align} where $(a)$ follows from the Fano's inequality.
% We also see that
% \begin{align*}
% I(\vecA,\by;\bx) &= I(\vecA, \bx)+I(\by;\bx|\vecA)\\
% &\stackrel{(a)}{=}0 +I(\by;\bx|\vecA)
% \end{align*}where $(a)$ follows from noting that $\vecA$ and $\bx$ are independent. Suppose $y_{j\in [1:i-1]}$ denote $(y_1, \ldots, y_{i-1})$. Then,
\begin{align*}
I&(y_i;\bx| \vecA) = h(y_i|\vecA)-h(y_i|\bx,\vecA)\\
&\leq \inp{h(y_i)-h(\mathbf{A}_i^T\bx + z_i|\bx,  \vecA)}\\
& = h(y_i) - h(z_i)\\
&\leq \inp{h(w_i)-h(z_i)}
\end{align*} where in the last inequality, $w_i\sim\cN\inp{0, \sigma^2_w}$ where $\mathsf{Var}(y_i)\leq \sigma^2_w$. We will now compute an upper bound on $\mathsf{Var}(y_i)$.
\begin{align*}
\mathsf{Var}(y_i) &\leq \bbE\insq{\inp{\vecA_i^T\bx+z_i}^2} = \bbE\insq{\inp{\vecA_i^T\bx}^2} + \sigma^2\\
&\leq k + \sigma^2
\end{align*} 
Thus, we have
\begin{align}
\inp{h(w_i)-h(z_i)}=\frac{1}{2}\log\inp{\frac{{k}}{\sigma^2}+1}.\label{eq:new2}
\end{align}
With this, we conclude that
% \begin{align*}
% \frac{m}{2}\log\inp{\frac{{k}}{\sigma^2}+1}
% \geq k\log\inp{\frac{n}{k}}- h_2(\delta) - \delta k\log{n}
% \end{align*} and 
\begin{align*}
m&\geq \frac{2k\log\inp{\frac{n}{k}}- h_2(\delta) - \delta k\log{n}}{\frac{1}{2}\log\inp{\frac{{k}}{\sigma^2}+1}}.
\end{align*} 



\end{proof}

