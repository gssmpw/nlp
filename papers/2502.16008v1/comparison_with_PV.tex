\section{Comparison with \cite{vershyninPlan}}\label{sec:comparison_PV}
Algorithm~\ref{alg:1} is similar to the two step estimation procedure outlined in \cite{vershyninPlan} which was given to estimate the unknown signal within a two norm guarantee. 
Computing the vector $\mathbf{l} = \inp{l_1, \ldots, l_n}$ is the same as the first step of the procedure in [Section~1.2]\cite{vershyninPlan} where a linear estimator is computed. The second step of our algorithm (sorting and keeping the top-$k$ indices) can be thought of as a projection on a feasible set [Section~1.3]\cite{vershyninPlan}. However, this requires the estimation error to be small enough for the exact recovery of a binary vector.

The setup in \cite{vershyninPlan} is for the recovery of an unknown signal with small two-norm error, whereas our problem of exact recovery of a sparse binary vector is more suited for recovery under  infinity norm. This results in weak bounds ($m\approx O(k^2)$) when we specialize various results in \cite{vershyninPlan} to our case. We first note that we require $\bbE\norm{\frac{\hat{x}}{\norm{\hat{x}}}-\bar{x}}< \sqrt{\frac{{2}}{{k}}}$ for exact recovery. Otherwise, there exist two binary $k$-sparse vectors which have hamming distance at least two. 

We first consider the 1-bit compressed sensing result in Section 3.5 (page 13). Setting the LHS to $\sqrt{\frac{{2}}{{k}}}$, we get
\begin{align*}
\sqrt{\frac{{2}}{{k}}}\leq C\sqrt{\frac{k\log\inp{2n/k}}{m}}.
\end{align*}
This implies that $m\approx C_1 k^2\log\inp{2n/k}$ for some constant $C_1$.

Next, we consider [Theorem 9.1]\cite{vershyninPlan}. Note that for 1-bit compressed sensing $\eta^2 = 1$ and 
\begin{align*}
\mu &= \bbE\insq{s_1\ipr{a_1}{\bar{x}}}\\
& =\bbE\insq{s_1\ipr{a_1}{{x}}}\\
& \stackrel{(a)}{=} \frac{1}{\sqrt{k}}\sqrt{\frac{2}{\pi}}\times\frac{k}{\sqrt{\inp{k+\sigma^2}}}\\
&= \sqrt{\frac{2}{\pi}}\times\frac{\sqrt{k}}{\sqrt{\inp{k+\sigma^2}}}.
\end{align*} where $(a)$ follows from \eqref{eq:expt4}. 
Then, 
\begin{align*}
\norm{x-\mu\bar{x}} &= \norm{x-\sqrt{\frac{2}{\pi}}\times\frac{\sqrt{k}}{\sqrt{\inp{k+\sigma^2}}}\frac{x}{\sqrt{k}}}\\
& = \norm{x-\sqrt{\frac{2}{\pi}}\times\frac{x}{\sqrt{\inp{k+\sigma^2}}}}
\end{align*} We require $\norm{x-\mu\bar{x}}<\frac{2}{\sqrt{\pi\inp{k+\sigma^2}}}$ in order to exactly recover the unknown signal $x$. 



We assume that $K$ is also a closed cone in $\bbR^n$. Then, by [Section~2.4]\cite{vershyninPlan}, $w_t(K) = tw_1(K) \leq t C\sqrt{k\log\inp{2n/k}}$ ([Section~2.4]\cite{vershyninPlan}). We choose $s = w_1(K)$. Substituting the bound for LHS and taking the limit $t\rightarrow 0$, we get
\begin{align*}
\frac{2}{\sqrt{\pi\inp{k+\sigma^2}}}\leq \frac{8 C \sqrt{k\log\inp{2n/k}}}{\sqrt{m}}.
\end{align*} Thus, $m\approx 4C(k+ \sigma^2)k\log\inp{2n/k}$. 

