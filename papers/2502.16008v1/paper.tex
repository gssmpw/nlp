
\documentclass[a4paper,notitlepage]{article}

\usepackage[a4paper,top=2cm,bottom=2cm,left=2.5cm,right=2.5cm,marginparwidth=1.75cm]{geometry}

\usepackage{authblk}

\usepackage{hyperref}
\hypersetup{
  colorlinks   = true, %Colours links instead of ugly boxes
  %urlcolor     = black!50!red, %Colour for external hyperlinks
  linkcolor    = black!50!brown, %Colour of internal links
  citecolor   = black!50!brown, %Colour of citations
  %bookmarks = false
}
% \usepackage{algorithm}

\RequirePackage{algorithm}
\RequirePackage{algorithmic}

% \usepackage{algorithmic}
% \usepackage{algpseudocode}

\usepackage{bm}
\usepackage{graphicx,xcolor}

\usepackage{mathtools,tikz}
\usepackage{hhline}
\usepackage{multirow}
\usepackage{enumitem}  
\usepackage{caption}
\usepackage{subcaption}

\usepackage{appendix}



\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts,amsthm,xspace}
\theoremstyle{definition}
\newtheorem{example}{Example}
\newtheorem{remark}{Remark}
\newtheorem{theorem}{Theorem}
\newtheorem{defn}{Definition}

\newtheorem*{defn*}{Definition}
\newtheorem*{lemma*}{Lemma}

\newtheorem{lemma}[theorem]{Lemma}  
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{ques}[theorem]{Question}


\newcommand{\w}[1]{\ensuremath{\mathsf{w}_{\mathsf{H}}(#1)}} 
\newcommand{\h}[2]{\ensuremath{\mathsf{d}_{\mathsf{H}}(#1, #2)}} 
\newcommand{\ind}[2]{\ensuremath{\mathsf{index}_{\mathsf{H}}(#1, #2)}} 
\newcommand{\ipr}[2]{\ensuremath{\langle #1, #2\rangle} }

\newcommand{\Expth}{\mbox{$\hat{{\mathbf E}}$} }
\newcommand{\Prob}{\ensuremath{{\mathbb P}}}
\newcommand{\expect}[1]{\Expt \left[ #1 \right]}
\newcommand{\prob}[1]{\Prob \left\{ #1 \right\}}
\newcommand{\defineqq}{\ensuremath{\stackrel{\textup{\tiny def}}{=}}}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert_2}
\newcommand{\normi}[1]{\left\lVert#1\right\rVert}


\foreach \x in {A,...,Z}{%
\expandafter\xdef\csname vec\x \endcsname{\noexpand\ensuremath{\noexpand\mathbf{\x}}}
}

% define calligaraphic versions of all uppercase letters \cA etc
\foreach \x in {A,...,Z}{%
\expandafter\xdef\csname c\x \endcsname{\noexpand\ensuremath{\noexpand\mathcal{\x}}}
}

% define mathbb versions of all uppercase letters \bbA etc
\foreach \x in {A,...,Z}{%
\expandafter\xdef\csname bb\x \endcsname{\noexpand\ensuremath{\noexpand\mathbb{\x}}}
}

\DeclareMathOperator*{\argmax}{arg\,max}


\newcommand\reals{{\mathbb R}}
\newcommand{\wh}[1]{\ensuremath{{\left|#1\right|}_{\mathsf{H}}}} 
\newcommand{\ina}[1]{\left<#1\right>}
\newcommand{\inb}[1]{\left\{#1\right\}}
\newcommand{\inp}[1]{\left(#1\right)}
\newcommand{\insq}[1]{\left[#1\right]}
\newcommand{\inl}[1]{\left|#1\right|}
\newcommand{\bcs}{\ensuremath{\mathsf{1bCSbinary}}}
\newcommand{\logreg}{\ensuremath{\mathsf{LogisticRegression}}}
\newcommand{\spl}{\ensuremath{\mathsf{SparseLinearReg}}}
\newcommand{\topk}{\ensuremath{\mathsf{TopK-Correlations}}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\sign}[1]{\mathsf{sign}(#1)}
\newcommand{\sorted}[1]{\mathsf{sorted}(#1)}


\newcommand{\red}[1]{{\textcolor{red}{#1}}}
\newcommand{\blue}[1]{{\textcolor{blue}{#1}}}
\newcommand{\olive}[1]{{\textcolor{olive}{#1}}}
\newcommand{\aqua}[1]{{\textcolor{cyan}{#1}}}


\iffalse
\usepackage{url}
\usepackage{microtype}






% \usepackage{amsmath}
% \usepackage{amssymb}
% \usepackage{amsthm}
\usepackage{verbatim}

% \usepackage{mdwtab}

% \usepackage{pdfpages}

\DeclareCaptionFormat{myformat}{\fontsize{8}{9}\selectfont#1#2#3}
\captionsetup{format=myformat}

% \usepackage{graphicx}
% \usepackage{textcomp}











% If you use natbib package, activate the following three lines:
% \usepackage[round]{natbib}
\usepackage[square]{natbib}
\renewcommand{\bibname}{References}
\renewcommand{\bibsection}{\subsubsection*{\bibname}}

% If you use BibTeX in apalike style, activate the following line:
%\bibliographystyle{apalike}


\fi


\title{Exact Recovery of Sparse Binary Vectors from \\ Generalized Linear Measurements}
\author[1]{Arya Mazumdar}
\author[1]{Neha Sangwan}

\affil[1]{\small Halicioglu Data Science Institute, University of California San Diego, La Jolla, United States}
% \affil[2]{\small Halicioglu Data Science Institute, University of California San Diego, La Jolla, United States}

\begin{document}
\maketitle


\begin{abstract}
 We consider the problem of {\em exact} recovery of a $k$-sparse binary vector  from generalized linear measurements (such as {\em logistic regression}). We analyze the {\em linear estimation} algorithm (Plan, Vershynin, Yudovina, 2017), and also show information theoretic lower bounds on the number of required measurements. As a consequence of our results, for  noisy one bit quantized linear measurements (\bcs), we obtain a sample complexity of $O((k+\sigma^2)\log{n})$, where $\sigma^2$ is the noise variance. This is shown to be optimal  due to the information theoretic lower bound. 
{We also obtain tight sample complexity characterization for logistic regression.}

  Since \bcs\ is a strictly harder problem than noisy linear measurements (\spl) because of added quantization, the same sample complexity is achievable for \spl. 
  While this sample complexity  can be obtained via the popular lasso algorithm, linear estimation is  computationally more efficient. 
  Our lower bound  holds for any set of measurements for \spl\, (similar bound was known for Gaussian measurement matrices) and is closely matched by the maximum-likelihood upper bound. 
  For \spl,  it was conjectured in Gamarnik and Zadik, 2017 that there is a statistical-computational gap and the number of measurements should be at least $(2k+\sigma^2)\log{n}$ for efficient algorithms to exist. It is worth noting that our results imply that there is 
   no such statistical-computational gap for \bcs\ and logistic regression.
\end{abstract}


\input{introduction}
\input{algorithm_body}
\input{lower_bound}
\input{proof_main_body}



\bibliography{example_paper}
\bibliographystyle{alpha}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\input{appendix_algorithm}
\input{Appendix_LowerBound}
% \input{AppendixSPL}
\input{suppl}
\input{comparison_with_PV}






% \input{arxiv/introduction}
% \input{arxiv/algorithm_body}
% \input{arxiv/sample_complexity}
% \input{arxiv/proofs}
% \appendix
% \input{arxiv/comparison_with_PV}
% \input{AISTATS/suppl}



% \input{arxiv/logistic_regression}
% \input{arxiv/general_case}
% \newpage

% \bibliographystyle{alpha}
% \bibliography{arxiv/refs}
\end{document}