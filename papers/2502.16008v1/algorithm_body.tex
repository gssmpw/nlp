%!TeX root=paper.tex
\section{Main results}


\subsection{Algorithm}\label{sec:alg}
We analyze the simple linear estimation based algorithm from \cite{vershyninPlan} for generalized linear measurements, specializing it for binary vectors.
The algorithm (Algorithm~\ref{alg:1}) takes the sensing matrix $\vecA$ and the output vector $\by$ as the inputs. 
% The output $\by$ is set to $\by$ (see \eqref{eq:spl}) for \spl\ and to $\sign{\by}$  for \bcs. 
For each column $\vecA_i,\, i\in [1:n]$ of the sensing matrix, the algorithm computes $l_i = \ipr{\by}{\vecA_i} = \sum_{j = 1}^{m}y_jA_{j,i}$ where $A_{j,i}$ is the entry at $j^{\text{th}}$ row and $i^{\text{th}}$ column.

The vector $\mathbf{l} = \inp{l_1, \ldots, l_n}$ is then sorted in decreasing order. The output of the algorithm is a set containing the indices of the top-$k$ elements of the sorted vector. That is, if the sorted vector is $\inp{l_{\alpha_1}, l_{\alpha_2}, \ldots, l_{\alpha_n}}$ where $l_{\alpha_i}\geq l_{\alpha_j}$ for $i\leq j$, then the output of the algorithm is  $\cS = \inb{\alpha_1, \ldots, \alpha_k}$.



\begin{algorithm}[tbh!]
   \caption{Top-$k$ correlated indices}
   \label{alg:1}
\begin{algorithmic}
   \STATE {\bfseries Input:} Sensing matrix $\vecA\in \bbR^{m\times n}$ and output $\mathbf{y}\in \bbR^{m}$ 
   \STATE {\bfseries Output:} a $k$-sized subset of $[1:n]$
   \STATE $\mathbf{l} \gets (0, \ldots, 0)$,\, $\mathbf{l}\in \reals^n$
        % \STATE $\mathbf{l} \gets (0, \ldots, 0)$,\, $\mathbf{l}\in \reals^n$
    \FOR{each $i\in [1:n]$}
      \STATE $l_i \gets \sum_{j = 1}^{m}y_jA_{j,i}$ 
    \ENDFOR
    \STATE Sort $\mathbf{l}$ in decreasing order and let $\cS$ be the top $k$ indices.\\
    \STATE {\bfseries Return:} $\mathcal{\cS}$ 
   % \REPEAT
   % \STATE Initialize $noChange = true$.
   % \FOR{$i=1$ {\bfseries to} $m-1$}
   % \IF{$x_i > x_{i+1}$}
   % \STATE Swap $x_i$ and $x_{i+1}$
   % \STATE $noChange = false$
   % \ENDIF
   % \ENDFOR
   % \UNTIL{$noChange$ is $true$}
\end{algorithmic}
\end{algorithm}


% \begin{algorithm}
%   \caption{Top-$k$ correlated indices}\label{alg:1}
%    \hspace*{\algorithmicindent} \textbf{Input:} Sensing matrix $\vecA\in \bbR^{m\times n}$ and output $\mathbf{w}\in \bbR^{m}$ \\
% \hspace*{\algorithmicindent} \textbf{Output:} a $k$-sized subset of $[1:n]$ \\
% \vspace{-0.4cm}
% \begin{algorithmic}[1]
%     \State $\mathbf{l} \gets (0, \ldots, 0)$,\, $\mathbf{l}\in \reals^n$
%     \For{each $i\in [1:n]$}
%       \State $l_i \gets \sum_{j = 1}^{m}A_{j,i}y_j$ 
%     \EndFor
%     \State Sort $\mathbf{l}$ in decreasing order and let $\cS$ be the top $k$ indices.\\
%     \Return $\mathcal{\cS}$ 
% \end{algorithmic}
% \end{algorithm}

The convergence and sample complexity guarantees for the algorithm are shown for the case when each entry of $\vecA$ is chosen iid $\cN(0,1)$. Note that such a matrix satisfies the power constraint in \eqref{eq:power_constraint}. As we argued in Section~\ref{sec:intro}, for the unknown signal $\bx$,  the output $\by = \vecA{\bx}+\bz$ is correlated with each column $\vecA_i$ for $i\in \cS_{\bx}$ and uncorrelated with $\vecA_j$ for $j\notin \cS_{\bx}$. In particular, for large number of samples, when $i\in \cS_{\bx}$, the inner product $\ipr{\by}{\vecA_i}$ is close to $\bbE\insq{\ipr{\by}{\vecA_i}} = m$ (for linear regression) with high probability. On the other hand, $\ipr{\by}{\vecA_j}$ is close to $0$ for $j\notin \cS_{\bx}$.  Thus, $l_i$ for $i\in \cS_{\bx}$ will dominate over $l_j$ for $j\notin \cS_{\bx}$. This line of argument also works when the output is binary, though in this case $\bbE\insq{\ipr{{\by}}{\vecA_i}}$ for $i\in \cS_{\bx}$ is different. 
This is the main idea of Algorithm~\ref{alg:1}. We first present Theorem~\ref{thm:alg_general} for generalized linear measurements.
% Theorem~\ref{thm:alg_bcs} and Theorem~\ref{thm:alg_spl} formalize this intuition for \bcs\ and \spl\ respectively. 
\begin{theorem}[Sample Complexity of Algorithm~\ref{alg:1} for GLMs]\label{thm:alg_general}
Suppose the GLM is such that for each $i\in [m]$, $y_i$ is a subgaussian random variable with subgaussian norm given by $\normi{{y_i}}_{\psi_2}$. For any $\bx$, suppose for some $L$, $\bbE\insq{g'(\vecA_i^T\bx)}\geq L\cdot
\normi{{y_i}}_{\psi_2}$   for all $i\in [m]$. Algorithm~\ref{alg:1} recovers the unknown signal with high probability if 
\begin{align}
m \geq \frac{C}{{\min\inb{L, L^2}}}(\log\inp{k}+\log\inp{n-k})\label{eq: alg_bound}
\end{align} where $C$ is some constant.
\end{theorem}
When $y_j$ is subgaussian, $y_j\vecA_{i,j}$ for any $i,j$ is a sub-exponential random variable. This observation allows us to use a concentration result for sub-exponential random variables to analyse the sample complexity. See Section~\ref{sec:proofs} for a detailed proof. 

As corollaries to Theorem~\ref{thm:alg_general}, we obtain the following sample complexity bounds for \bcs\ and \spl. These corollaries are proved in Appendix~\ref{proof:sec:alg}.
\begin{corollary}[Sample Complexity of Algorithm~\ref{alg:1} for \bcs]\label{thm:alg_bcs}
Algorithm~\ref{alg:1} recovers the unknown signal for \bcs\ with high probability if $m=O\inp{\inp{k+\sigma^2}(\log\inp{k}+\log\inp{n-k})}$.
\end{corollary}
\begin{corollary}[Sample Complexity of Algorithm~\ref{alg:1} for \spl]\label{thm:alg_spl}
Algorithm~\ref{alg:1} recovers the unknown signal for \spl\ if $m=O\inp{\inp{k+\sigma^2}(\log\inp{k}+\log\inp{n-k})}$.
\end{corollary}
 Interestingly, the sample complexity for both \bcs\ and \spl\ is the same. This can be explained by similar values of $L$, which result in similar rates of concentration of $l_i$'s around their expectation in both the cases. 
This also implies that in the regime where $m = O((k+\sigma^2)\log(n-k))$, having access to $\vecA_i^T \bx+z_i$ instead of $\sign{\vecA_i^T \bx+z_i}$, does not improve the sample complexity beyond constants.

Using Theorem~\ref{thm:alg_general}, we obtain the following corollary for logistic regression (see proof in Appendix~\ref{proof:sec:alg}).
\begin{corollary}[Sample Complexity of Algorithm~\ref{alg:1} for \logreg]\label{thm:alg_logreg}
Algorithm~\ref{alg:1} recovers the unknown signal for \logreg\ if $m=O\inp{\inp{k+1/\beta^2}\inp{\log{k}+\log\inp{n-k}}}$.
\end{corollary}
Comparing the sample complexity bounds of \bcs\ and \logreg, we notice that the sample complexity is similar except that   the noise variance $\sigma^2$ is replaced by $1/\beta^2$. This relationship is not surprising as a similar relationship was also present in the sample complexity bounds in \cite{hsu2024sample} (for logistic regression) and \cite{kuchelmeister2024finite} (for probit model). Note that, in the noiseless case, when $\beta\rightarrow \infty$ (or $\sigma = 0$ for \bcs), the sample complexity is $O(k\log{n})$, which is close to the simple counting lower bound of $k\log{n/k}$. On the other hand, when $\beta = 0$ (or $\sigma\rightarrow \infty$ for \bcs), $m\rightarrow \infty$, which makes intuitive sense as very high levels of noise render the output useless.


To compute the time complexity of the algorithm, notice that the for loop in step 2 takes $O(n\times m)$ time and step 4 takes $O(n\log{n})$ time. Thus, the computational complexity of the algorithm is $O(nm+n\log{n})$, which is $O((k+\sigma^2)n\log{n})$ for $m = O((k+\sigma^2)\log{n}$.
To compute the time complexity of the algorithm, notice that the for loop in step 2 takes $O(n\times m)$ time and step 4 takes $O(n\log{n})$ time. Thus, the computational complexity of the algorithm is $O(nm+n\log{n})$, which is $O((k+\sigma^2)n\log{n})$ for $m = O((k+\sigma^2)\log{n}$.



