% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
% \usepackage[review]{ACL2023}
% \usepackage[review]{ACL2023}
\usepackage[]{ACL2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.

\usepackage{multirow}
\usepackage{multicol}
\usepackage{xspace}
\usepackage{colortbl}
\usepackage{makecell}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{arydshln}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.8}
\usepgfplotslibrary{statistics}
\usepackage{makecell}
\usepackage{subcaption}

\usepackage{inconsolata}
\usepackage{hyperref}
\usepackage{amsfonts}
\usepackage[most]{tcolorbox}
\usepackage{dashrule}
\usepackage{ragged2e} % 加载 ragged2e 宏包


% \usepackage{CJKutf8}
% \usepackage{geometry}
% \usepackage{array}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.Align

\usepackage{inconsolata}

\newcommand\benchall{\textsc{TritonBench}\xspace}
\newcommand\benchone{\textsc{TritonBench-{G}}\xspace}
\newcommand\benchtwo{\textsc{TritonBench-{T}}\xspace}

\title{\benchall: Benchmarking Large Language Model Capabilities for Generating Triton Operators}

\author{
    \textbf{Jianling Li\textsuperscript{1,}\footnotemark[1]},
    \textbf{Shangzhan Li\textsuperscript{2,}\footnotemark[1]},
    \textbf{Zhenye Gao\textsuperscript{3}},
    \textbf{Qi Shi\textsuperscript{4,}\footnotemark[2]},
    \textbf{Yuxuan Li\textsuperscript{4,}\footnotemark[2]}, 
    \textbf{Zefan Wang\textsuperscript{4}},
    \\
    \textbf{Jiacheng Huang\textsuperscript{4}}, 
    \textbf{Haojie Wang\textsuperscript{4}}, 
    \textbf{Jianrong Wang\textsuperscript{1}}, 
    \textbf{Xu Han\textsuperscript{4}}, 
    \textbf{Zhiyuan Liu\textsuperscript{4}}, 
    \textbf{Maosong Sun\textsuperscript{4}}
    \\
\\    
\textbf{\textsuperscript{1}} Tianjin University, Tianjin, China
\\
\textbf{\textsuperscript{2}} Harbin Institute of Technology, Harbin, China 
\\
\textbf{\textsuperscript{3}} The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China
\\
\textbf{\textsuperscript{4}} Tsinghua University, Beijing, China
\\
}

\begin{document}
\maketitle

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{Equal contribution.}
\footnotetext[2]{Corresponding authors.}
\renewcommand{\thefootnote}{\arabic{footnote}}



\begin{abstract}

Triton, a high-level Python-like language designed for building efficient GPU kernels, is widely adopted in deep learning frameworks due to its portability, flexibility, and accessibility.
% 
However, programming and parallel optimization still require considerable trial and error from Triton developers. 
% 
Despite advances in large language models (LLMs) for conventional code generation, these models struggle to generate accurate, performance-optimized Triton code, as they lack awareness of its specifications and the complexities of GPU programming. 
More critically, there is an urgent need for systematic evaluations tailored to Triton.
% 
In this work, we introduce \benchall, the first comprehensive benchmark for Triton operator generation. 
\benchall features two evaluation channels: a curated set of $184$ real-world operators from GitHub and a collection of operators aligned with PyTorch interfaces. 
Unlike conventional code benchmarks prioritizing functional correctness, \benchall also profiles efficiency performance on widely deployed GPUs aligned with industry applications. 
Our study reveals that current state-of-the-art code LLMs struggle to generate efficient Triton operators, highlighting a significant gap in high-performance code generation. 
\benchall will be available at \url{https://github.com/thunlp/TritonBench}.
\end{abstract}



\input{sections/1_intro}
\input{sections/2_relat}
\input{sections/3_git}
\input{sections/4_torch}
\input{sections/6_exprm}
\input{sections/7_analys}
\input{sections/8_conclu}
\input{sections/limitations_and_ethics_statement}

\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\input{sections/9_append}

\end{document}
