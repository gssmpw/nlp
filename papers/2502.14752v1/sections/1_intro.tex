\section{Introduction}
%
Triton~\cite{tillet2019triton} language, a high-level Python-like programming language designed for implementing efficient GPU kernels, is playing an increasingly pivotal role in the ever-scaling deep learning ecosystems~\cite{abadi2016tensorflow,paszke2019pytorch}. 
Due to the superior portability, flexibility, lightweight design, and accessibility to less proficient programmers, Triton is prevalently adopted in modern Large Language Model (LLM) frameworks such as vLLM~\cite{vllm}, LightLLM~\cite{lightllm}, Liger-kernel~\cite{liger} and unsloth~\cite{unsloth}. 
However, crafting high-performance operators remains challenging, especially for the intricate balance between memory hierarchy management, parallel thread coordination, and hardware-specific optimizations. 
Even though Triton abstracts away many complexities of low-level programming architectures like CUDA, it still requires developers to manually handle critical aspects such as pointer arithmetic and memory access patterns, making performance tuning a labor-intensive process that often involves extensive trial and error. 


Current research in AI-assisted coding has reached a human-competitive level~\cite{hui2024qwen2,zhu2024deepseek}, yet it is primarily restricted to general-purpose languages like Python. 
However, LLMs still face challenges in generating Domain Specific Language (DSL) code. 
Specifically for Triton, current models might be unfamiliar with Triton specification and the intricacies of GPU programming~\cite{nichols2024can}. 
Most importantly, the ability of these models to produce high-quality Triton code remains unassessed. 
Therefore, a high-quality benchmark paired with performance-aware metrics is urgently required.


\begin{figure*}[t]
\vspace{-5mm}
    \centering
    \includegraphics[width=0.98 \textwidth]{figures/main_figure.pdf}
    \caption{Illustration of the construction and evaluation of \benchall.}
    \label{fig:main_figure}
\end{figure*}


In this study, we present \benchall, a performance-aware benchmark framework for Triton generation, which contains two channels, namely \benchone and \benchtwo. 
Specifically, \benchone contains $184$ carefully curated operators from existing GitHub repositories, reflecting the realistic demand for Triton operator development. 
As a complement, \benchtwo is composed of operator development tasks aligned with PyTorch interfaces, covering operators under-represented by public sources.
Moreover, unlike the majority of code benchmarks merely prioritizing functional correctness~\cite{chen2021evaluating,austin2021program}, \benchall emphasizes efficiency performance profiling against reference programs on NVIDIA GPUs, better aligning industrial demands. 

As shown in Figure~\ref{fig:main_figure}, for \benchone, we follow three steps: 1) scrape and collect high-quality operators, 2) generate instructions via prompts, and 3) annotate test code with LLMs.
Moreover, HPC experts evaluate GPU performance for all triton codes. 
For \benchtwo, we provide operator generation tasks aligned with PyTorch.
To construct these tasks, we first perform a frequency analysis to select torch operators, combine them into diverse sets, and provide paired instructions and test code. 
Our evaluation metrics include similarity, call and execution accuracy, speed up, and GPU efficiency.


We conduct extensive experiments across a broad range of LLMs. Overall, the difficulty of \benchone is greater than that of \benchtwo. 
The highest execution accuracy on \benchone can reach $23.91$\%, while on \benchtwo, it can reach $53.01$\%. 
For all correctly executed operators generated by the models, the best speed up on \benchone is $1.56\times$, whereas, on \benchtwo, it is $1.91\times$. 
Additionally, we perform in-depth analyses of LLMsâ€™ behavior on \benchall and summarize the challenges in Triton generation. 
The results reveal that current LLMs are not yet fully capable of handling \benchall, underscoring the challenge of enabling LLMs to generate Triton code effectively. 
We hope this work initiates evaluation in this under-explored area and fosters advancements in LLM-driven operator development.