\section{\benchone}
\label{sec:git}

Triton~\cite{tillet2019triton} is a DSL that abstracts away low-level complexities to simplify GPU programming for computation-intensive tasks, with flexibility for specialized applications like machine learning.
Typically, a Triton operator includes at least a kernel and a wrapper. 
The kernel comprises code executed on the GPU, focusing on tensor element addressing and thread parallel coordination. 
Meanwhile, the wrapper offers a Python function that encapsulates the kernel call.
Figure \ref{fig:triton_example} shows an example of Triton operator. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/triton_example.pdf}
    \caption{
    Implementation of the Triton ``add'' operator. Lines $3$-$6$ perform for tensor element addressing, followed by the calculation and storage in lines $7$-$10$. 
    The kernel is called in wrapper line $15$.
    }
    \label{fig:triton_example}
    \vspace{-3mm}
\end{figure}

We create \benchone by curating high-quality human-authored Triton operators from Github, which reflects Triton’s currently actual requirements. 
The following sections will explain data collection (\S~\ref{sec:benchone-datacollect}), data statistics (\S~\ref{sec:benchone-datastatistic}), operator quality rating (\S~\ref{sec:benchone-qualityrating}), test code design (\S~\ref{sec:benchone-testcodedesign}), and evaluation metrics (\S~\ref{sec:benchone-evalmetric}).



\subsection{Data Collection}
\label{sec:benchone-datacollect}
Our process starts by gathering Triton-related GitHub repositories with more than $100$ stars, which collectively encompass $95$ repositories with $845$ Python files. 
As Triton repositories with higher star counts are rare, $100$ stars serve as an optimal threshold, striking a balance between quality and quantity.
We then use prompt-based filtering (see prompt~\ref{prompt_filter} in the Appendix) to process the candidate Python files and select $250$ that specifically contain Triton code snippets.


Afterward, we perform a rigorous manual inspection of the Triton code to ensure its accuracy and clarity. 
This process involves filling in missing components, removing redundant sections, and debugging the operators.
When a file contains multiple independent Triton operators, we split them into separate files. For operators that are solely kernels, we add the necessary wrappers to ensure they work as intended. Additionally, to ensure uniqueness, we leverage \textsc{CodeBertScore}~\cite{codebertscore2023} to eliminate duplicates.

Finally, we generate the LLM instruction for each operator based on prompt~\ref{prompt_instru}. 
The instructions provide essential details, including the operator’s functionality, corresponding function names, and a comprehensive input/output demonstration. 
All instructions are carefully reviewed and manually verified to ensure they correctly reflect the intended behavior of each operator.


\subsection{Data Statistics}
\label{sec:benchone-datastatistic}
Table~\ref{tab:statis_git} summarizes statistics of \benchone. 
In this benchmark, each operator is assigned a difficulty level, from $\bf d1$ (easiest) to $\bf d5$ (most challenging), by an LLM guided by prompt~\ref{prompt_diff}, with subsequent manual verification by two domain experts. 
For each difficulty level, we report statistics including the average number of functions~(\textbf{func\#}), parameters~(\textbf{params\#}), lines~(\textbf{lines\#}), and tokens~(\textbf{tok\#}). 
Notably, the upward trend observed in these statistics as the difficulty level increases suggests the expert-driven grading scheme is largely reasonable. 

Compared to existing code generation tasks \cite{chen2021evaluating, austin2021program}, the average instruction length in \benchone is substantially longer, which is a deliberate design decision. 
The extended instructions provide richer context, which can help the model understand nuanced requirements and generate high-quality operators. 
Additionally, this approach better reflects real-world operator development practices where detailed requirements are indispensable. 

\input{tables/statis_git.tex}

\subsection{Operators Quality Rating}
\label{sec:benchone-qualityrating}
To systematically evaluate the quality of the Triton operators in \benchone, we compute the GPU efficiency for each operator. 
Detailed methodology for calculating GPU efficiency can be found in Appendix~\ref{sec:appendix-performance_eval}.
Our statistics indicate an average GPU efficiency of $\mathbf{43.0}\%$, which reflects the overall reliability of the operators in \benchone. 
The distribution of efficiency scores is shown in Figure~\ref{fig:performance_distribution}. 
As shown in the figure, $19.6$\% of operators developed by professional Triton programmers have GPU performance below 10\%, which underscores the challenges in developing and optimizing Triton operators.



\begin{figure}[t!]
    \centering
    \includegraphics[width=0.28\textwidth]{figures/performance_distribution.pdf}
    \caption{Distribution of GPU efficiency of the Triton operators in \benchone.}
    \label{fig:performance_distribution}
\end{figure}


\subsection{Test Code Design}
\label{sec:benchone-testcodedesign}
In contrast to traditional CPU-language benchmarks~\cite{shypulalearning,evalplus,huang2024effibench,qiu2024efficient} that predominantly rely on scalar test inputs,\benchone is built around tensor-based test inputs. 
We employ PyTorch to generate random tensors as replacements for conventional test cases. 
Specifically, we leverage a prompt~\ref{prompt_test_code} to generate the corresponding test code for each operator. 
In the case of the multi-branch operators, the generated test code is designed to invoke every branch within the operator. 
Moreover, we rigorously debug all branches to guarantee test reliability. 
On average, we generate $3.6$ test branches per operator.



\subsection{Evaluation Metrics}
\label{sec:benchone-evalmetric}
In contrast to traditional code evaluations, which mainly emphasize accuracy~\cite{chen2021evaluating,austin2021program}, our \benchone introduces dedicated performance evaluations. Specifically, the systematic evaluation of Triton operators covers five key metrics:

\paragraph{\texttt{Similarity}} assesses text-level resemblance using \textsc{CodeBLEU} \cite{ren2020codebleu}. In our experiments, we assign equal weights of $0.25$ to N-gram, weighted N-gram, syntax, and dataflow components to ensure a balanced evaluation.
% of code similarity.
\paragraph{\texttt{Call~\&~Execution Accuracy}} assess whether the code can run without error and whether its input-output behavior is correct, respectively. 
\paragraph{\texttt{Speed Up}} measures the relative execution time improvement for correctly executed operators. Specifically, if \(t_{gen}\) and \(t_{ref}\) represent the running times of the generated and reference operators, respectively, then
\(
\texttt{SpeedUp}(gen) = \frac{t_{ref}}{t_{gen}}.
\)
\paragraph{\texttt{GPU Efficiency}} evaluates how effectively the generated operator utilizes GPU resources, following the operator quality rating in \S~\ref{sec:benchone-qualityrating}. For further details, please refer to Appendix~\ref{sec:appendix-performance_eval}.