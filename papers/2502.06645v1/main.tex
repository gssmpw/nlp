\documentclass[twoside]{article}

\usepackage[accepted]{aistats2025}




\input{includes.tex}
\input{math_commands.tex}


\usepackage{dirtytalk} %
\usepackage{hyperref} %

\hypersetup{
    colorlinks,
    linkcolor={black},
    citecolor={black},
    urlcolor={black}
}
\usepackage{booktabs} %


\usepackage[round]{natbib}
\bibliographystyle{plainnat}
\renewcommand{\bibname}{References}
\renewcommand{\bibsection}{\subsubsection*{\bibname}}

\usepackage{xr-hyper}

\setlength{\textfloatsep}{11pt}
\setlength{\floatsep}{15pt}
\begin{document}


\runningauthor{P. Bevanda, M. Beier, A. Lederer, A. Capone, S. Sosnowski, S. Hirche}


\twocolumn[

\aistatstitle{Koopman-Equivariant Gaussian Processes}


\aistatsauthor{Petar Bevanda$^{*}$ \\ TU Munich \And Max Beier$^{*}$ \\ TU Munich
\And Alexandre Capone \\ CMU Robotics Institute \AND Stefan Sosnowski \\ TU Munich \And Sandra Hirche \\ TU Munich \And Armin Lederer \\ ETH Z\"{u}rich 
}
\aistatsaddress{} 
]

\begin{abstract}
Credible forecasting and representation learning of dynamical systems are of ever-increasing importance for reliable decision-making. To that end, we propose a family of Gaussian processes (GP) for dynamical systems with linear time-invariant responses, which are nonlinear only in initial conditions. This linearity allows us to tractably quantify forecasting and
representational uncertainty, simultaneously alleviating the challenge of computing the distribution of trajectories from a GP-based dynamical system and enabling a new probabilistic treatment of learning Koopman operator representations. Using a trajectory-based equivariance -- which we refer to as \textit{Koopman equivariance} -- we obtain a  GP model with enhanced generalization capabilities. To allow for large-scale regression, we equip our framework with variational inference based on suitable inducing points. Experiments demonstrate on-par and often better forecasting performance compared to kernel-based methods for learning dynamical systems.
\end{abstract}


\section{INTRODUCTION}
Learning predictive models for forecasting dynamic systems is a challenging task due to complex and often unknown interactions between quantities of interest \citep{Brunton2019Data-DrivenEngineering}. The great utility of such models helps advance various different fields such as fluid mechanics \citep{kundu2015fluid}, molecular biology \citep{protFold}, robotics \citep{Billard2022LearningApproach} or safety-constrained decision making \citep{Hewing/annurev-control-090419-075625, Brunke/annurev-control-042920-020211}.
Dynamical system descriptions commonly require simulation for forecasting and uncertainty propagation, which can be difficult for non-parametric data-driven models \citep{pmlr-v120-hewing20a,TBpredGP}. 
In most real-world applications involving dynamical systems,  
measurements often come in the form of sequential one-step transition data that is sampled arbitrarily and potentially non-uniformly. Furthermore, there is often a certain regularity in the evolution of quantities of interest \citep{pmlr-v202-bilos23a} across domains \citep{SEZER2020106181,DEB2017902,Lim2021}, making it important to impose structure that discourages temporal fluctuations. 
To account for these different challenges in modeling dynamical systems, the choice of \textit{representations} when learning from data becomes a deciding factor in the difficulty of forecasting as well as inference, especially when modeling complex phenomena \citep{Mezic2004ComparisonBehavior} or long time-series \citep{DBLP:conf/iclr/GuGR22}.
In this paper, we focus on non-parametric learning paradigms, emphasizing \textit{uncertainty quantification} and \textit{forecasting simplicity}. In particular, we study the interplay between Gaussian processes \citep{Rasmussen2006} and effective dynamical system linearizations based on Koopman operators \citep{KoopBook,Brunton2022ModernSystems}. A more exhaustive account of related work is delegated to the supplementary material.
\begin{table}[t!]%
    \caption{Nonlinear dynamics modeling from data}
    \label{tab:contribution}
    \footnotesize
    \centering
    \vspace{-2ex}
    \begin{tabular}{l|ccccc}
    \toprule
        Approach& LTI forecast & End-to-end & Bayesian\\
        \midrule
        GPs & \color{red!80!black}{\text{\xmark}}  & \color{green!80!black}{\text{\cmark}} & \color{green!80!black}{\text{\cmark}}\\
        Koopman & \color{green!80!black}{\text{\cmark}}  & \color{red!80!black}{\text{\xmark}} & \color{red!80!black}{\text{\xmark}} \\
        {{\tbm{this work}}} & \color{green!80!black}{\text{\cmark}} & \color{green!80!black}{\text{\cmark}} & \color{green!80!black}{\text{\cmark}} \\
        \bottomrule
    \end{tabular}
\end{table}

\textbf{Gaussian processes.~~}
Gaussian processes (GPs) \citep{Rasmussen2006} have the capability of inferring models with little structural prior knowledge: either by using so-called universal kernels \citep{Micchelli2006UniversalKernels} or placing a prior on a set of kernels and optimizing their likelihood \citep{Duvenaud2014}. 
In particular, their ability to quantify epistemic uncertainty has led to a common application in safety-critical control problems \citep{BerkenkampECC15,pmlr-v37-sui15,NIPS2017_766ebcd5,CuriCDC22,Baumann2021,Khosravi2023bo,As2024,Polymenakos2020,Lederer2021GaussianApplications}. Commonly employed as single-step predictors, GP models necessitate approximations for predicting probability distributions that go beyond a single time-step into the future. Thus, dealing with multi-step prediction often relies on iterative sampling-based approaches \citep{Bradford2019,pmlr-v120-hewing20a, TBpredGP} that are generally computationally expensive.
Alternatively, one can employ methods of reduced computational complexity, such as Taylor approximations \citep{Girard2003} or exact moment matching \citep{Deisenroth2011PILCO:Search}. However, such approaches deliver no accuracy guarantees for long-term forecasts. Notably, one can avoid approximate uncertainty propagation via multitask GPs models \citep{Bonilla2007} that use a collection of ``condensed models", one for each of the prediction steps \citep{Bradford2019,Pfefferkorn2022}, or employ a single contextual kernel defined over a joint spatio-temporal domain \citep{pmlr-v151-zenati22a, Li2024STkernel}.\looseness=-1

\textbf{Koopman operator-based learning.~~}
The linearity of Koopman operators and the forecasting simplicity of \textit{linear time-invariant} (LTI) models stemming from their eigendecompositions has led to their increasing popularity in learning dynamical systems \citep{Bevanda2021KoopmanControl,Otto2021AnnualSystems,Brunton2022ModernSystems}. 
Nevertheless, existing LTI predictors based on operator regression are limited to dissecting long-term components of ergodic dynamics \citep{Korda2018OnOperator,Klus2020EigendecompositionsSpaces,Kostic2022LearningSpaces,Kostic2023KoopmanEigenvalues}. While this approach is extremely powerful for stationary data and reversible dynamics, almost all real-world dynamical systems are irreversible and often even nonstationary \citep{Wu2020}.
Thus, an increasing amount of methods considers kernels that are \textit{dynamics-informed} \citep{Zhao_2016,BERRY2016439,Banisch2017,ALEXANDER2020132520,Burov2021,DUFEE2024134044}. By plugging samples of the dynamics from sequential data into the kernel itself, eigenfunctions of Koopman operators can be directly accessed for both ergodic \citep{DUFEE2024134044} and transient settings \citep{KKR_neurips2023}.~
While the latter has generalization and consistency guarantees, fully tractable representational uncertainty is impossible due to a two-stage regression approach \citep{HarmlessEcon,Wang2022}. Still, the existing Koopman operator-based learning approaches offer no epistemic uncertainty bounds, principled model selection or handling of observation noise.

In this work, we present \textbf{K}oopman-\textbf{E}quivariant \textbf{G}aussian \textbf{P}rocesses (KE-GPs), the first universal GP models with fully tractable and closed-form confidence bounds for multi-step prediction. By leveraging latent dynamics, our model provides simple LTI responses as a nonlinear function of the initial condition. Strikingly, our GP model provides enhanced generalization compared to existing methods due to intrinsic symmetries (Koopman-equivariants). Furthermore, it delivers continuous-time posteriors without requiring time-derivative data. KE-GPs allow for tractable \textit{simultaneous characterization of both forecasting and representational uncertainty} – alleviating a traditional
challenge of GPs and enabling a novel probabilistic treatment of learning dynamics representations.\footnote{{\textbf{Notation:} Denote the joint data distribution as $P(d z \times d x \times d y)$, its marginal distributions as $P(d x), P(d z)$, etc., and their support as $\mathcal{X}, \mathcal{Z}$. For functions of observed variables (e.g., $\bm{x}$ or z), $\|\cdot\|_2$ denotes the $L_2$ norm w.r.t. the respective marginal data distribution. $\|\cdot\|_{\infty}$ denotes the $L_{\infty}$ norm. We use the notation $[m]:=\{1, \ldots, m\}$. Boldface $(\bm{x}, \bm{y}, \bm{z})$ emphasizes the denotation of random variables. For any kernel $k, \mathcal{G} \mathcal{P}(0, k)$ refers to the ``standard Gaussian process" \citep{vanderVaart2008} with zero mean, and covariance defined by $k . \lesssim, \gtrsim, \asymp$ represent (in)equalities up to constants; the hidden constants will not depend on any sample size. $\tilde{\mathcal{O}}(\cdot)$ denotes inequality up to logarithm factors.}}



\textbf{Organization\footnote{Proofs of theoretical results are in the supplemental.}.}
In Section \ref{sec:ProbStat} we introduce the necessary preliminaries together with our problem statement.
    Section \ref{sec:KEframework} includes the derivation of Koopman-equivariant Gaussian process models, including representation theory and dynamical properties. We then analyze the sample-complexity of our approach through an information-theoretical lens, in Section \ref{section:analysisofsamplecomplexity}.
To handle large datasets, in Section \ref{section:svigps}, we present our Koopman-equivariant inducing variables for scalable GP-based modeling using variational inference.
In Section \ref{sec:NumExp} we demonstrate the utility of our KE-GP approach through a comparison to existing GP and Koopman approaches for learning dynamical systems
including predator-prey ODE, datasets from realistic robotic simulators as well as real-world weather data. 
Finally, in Section \ref{sec:Concl}, we conclude and mention the limitations of the approach.\looseness=-1
\section{PROBLEM SETTING}\label{sec:ProbStat}
Our work builds upon the extensive literature on GPs, their interplay with linear operators, and the concept of Koopman-equivariance, which extracts informative ``latent states" of dynamics, i.e., Koopman operator eigenfunctions, based on trajectory data. The following covers the necessary prerequisites for setting up the interplay between GPs, linear operators, and intrinsic dynamical system symmetries.
\subsection{Gaussian Process Regression}\label{sec:BackgroundGPs}
A Gaussian process is a generalization of the Gaussian distribution. It specifies a distribution, such that any finite collection of random variables follows a joint Gaussian distribution, which can be interpreted as a distribution over functions $f:\mathbb{R}^n\rightarrow\mathbb{R}$ commonly denoted by $f(\cdot)\sim\GP(m(\cdot),k(\cdot,\cdot))$ \citep{Rasmussen2006}. This distribution is defined using a prior mean function $m:\mathbb{R}^n\rightarrow\mathbb{R}$ and a covariance function $k:\mathbb{R}^n\times\mathbb{R}\rightarrow\mathbb{R}_{0,+}$. The mean function $m(\cdot)$ includes prior models and is often set to $0$ in the absence of such information, which we also assume in the following. The covariance function $k(\cdot,\cdot)$ encodes more abstract prior knowledge, such as symmetries and smoothness of the sample functions.

Given a dataset $\mathbb{D}_N=\{\bm{z}^{(i)},y^{(i)} \}_{i\in[N]}$ with training targets $y^{(i)}=f(\bm{z}^{(i)})+\omega^{(i)}$ perturbed by i.i.d. Gaussian noise $\omega^{(i)}\sim\mathcal{N}(0,\sigma_{\txt{on}}^2)$, we place a Gaussian process prior $\GP(0,k(\cdot,\cdot))$ on the unknown function $f(\cdot)$ to infer a model. This is straightforwardly achieved by computing the posterior distribution given the training data, which is Gaussian at each test point $\bm{z}\in\mathbb{R}^n$. We can then compactly express the posterior as $p(f(\bm{z})|\mathbb{D}_N)=\mathcal{N}(\mu(\bm{z}),\sigma^2(\bm{z}))$, where 
\begin{align}\label{eq:GPbase}
 \textstyle   \mu(\bm{z})&=\bm{k}^{\intercal}(\bm{z})(\bm{K}+\sigma_n^2\bm{I})^{-1}\bm{y},\\
    \sigma^2(\bm{z})&=k(\bm{z},\bm{z})-\bm{k}^{\intercal}(\bm{z})(\bm{K}+\sigma_{\txt{on}}^2\bm{I}_N)^{-1}\bm{k}(\bm{z}),
\end{align}
with $k_i(\bm{z})=k(\bm{z},\bm{z}^{(i)})$, $K_{ij}=k(\bm{z}^{(i)},\bm{z}^{(j)})$ and $\bm{y}^\intercal=[y^{(1)}\ \cdots\ y^{(N)}]$. In addition to inferring the posterior distribution, we use the training data to optimize the hyperparameters that arise from the kernel parameterization. This is enabled by the probabilistic approach to the regression problem, which allows us to choose the hyperparameters by minimizing the negative log-likelihood $\textstyle  -\log (p(\bm{y}|\bm{Z}))= \textstyle \nicefrac{1}{2}\bm{y}^{\intercal}(\bm{K}+\sigma_{\txt{on}}^2\bm{I}_N)^{-1}\bm{y}+\nicefrac{1}{2}\log(\det(\bm{K}+\sigma_{\txt{on}}^2\bm{I}_N))+\nicefrac{N}{2}\log(2\pi)$.

\subsection{System Class \& Modeling Approach}
\textbf{System class.~~} We consider state-space models
\begin{taggedsubequations}{\txt{SSM}}\label{eq:SSmodel}
\begin{align}
\text{(dynamics)}  \qquad  \dot{\bm{x}}&=\bm{f}
(\bm{x}), \quad \bm{x} \in \Set{X} \subset \RR^n, \quad \label{eq:SSMdyn}\\
\text{(output)}   \qquad    y&=h(\bm{x}) \in \RR, \quad \label{eq:SSMout}
\end{align}
\end{taggedsubequations}%
with a well-defined flow $\bm{F}_{t}(\bm{x}{\naught}):=\normalint_0^{t} \bm{f}(\bm{x}(\tau)) d\tau$ that requires local Lipschitz continuity of $\bm{f}$, which is natural to physical systems that often evolve ``smoothly''.
The canonical forecasting model for \eqref{eq:SSmodel} is $y(t,\bm{x}_0) := h_t(\bm{x}_0)\equiv h \circ \bm{F}_{t}(\bm{x}_0)$. In practice, a numerical integration scheme is usually required to solve the integral for a shorter time-interval ${\Delta}{t}$, such that the actual forecast becomes an $H$-fold composition of nonlinear maps $\textstyle y(t,\bm{x}_0) \approx \textstyle h \circ \bm{F}_{{\Delta}{t}} \circ \cdots \circ \bm{F}_{{\Delta}{t}}(\bm{x}_0)$ with $H = t/{\Delta}{t} \in \Set{N}$.

\textbf{Spectral dynamics modeling.~~}
To decompose nonlinear dynamics into simple linear factors and avoid approximate integration schemes, one can utilize the fact that the composition of a function $h$ with the flow $\bm{F}_t$ can be replaced by a linear, \textit{Koopman}, operator $\mathcal{A}_{t}: \RKHS^\prime\rightarrow \RKHS$ with $[\mathcal{A}_{t}h](\bm{x}_{0}):=  h_t(\bm{x}_{0}):=h(\bm{x}_{t})$~\citep{Koopman1931HamiltonianSpace,Cvitanovic2016Chaos:Quantum}. The usefulness of linear operators lies in their ability to forecast any $h \in \RKHS$ in terms of a spectral decomposition~\citep{Weidmann1980}\looseness=-1
\begin{align}\label{eq:KoopObs}
    [\mathcal{A}_{t}h](\bm{x}_0)  = \sum^{\infty}_{j=1}\underbrace{\exp{\eig_j t}\vphantom{g^\prime_j,h}}_{\text{\tiny dynamics}}\langle\underbrace{ g^\prime_j,h}_{\text{\tiny mode}}\rangle \underbrace{g_j(\bm{x}_0)}_{\text{\tiny (eigen)feature}},\tag{\txt{KMD}}
\end{align}
where the dynamics are parameterized by eigenvalues $\{\eig_j(\mathcal{A}_t)\}^{\infty}_{j=1} \in \Set{C}$ while $\RKHS^\prime:=\operatorname{span}(\{g^\prime_j\}^{\infty}_{j=1})$ span an auxiliary and $\RKHS := \operatorname{span}(\{g_j\}^{\infty}_{j=1})$ the main representation hypothesis. Under mild conditions \textit{Koopman mode decomposition} \eqref{eq:KoopObs} exists and is dense in $C(\Set{X})$~\citep{Korda2020OptimalControl}, cf. \citet{KKR_neurips2023} and references therein. Learning a finite \eqref{eq:KoopObs} from data, up to a re-scaling of modes, amounts to learning a $D$-dimensional representation $\RKHS_D$\footnote{$\RKHS^{\prime}_D=\RKHS_D$ holds w.l.o.g. \citep{Korda2020OptimalControl}.}.%
\subsection{Problem Statement}
Given no knowledge of the Koopman operator or \eqref{eq:SSmodel}, our goal is to learn a finite-dimensional model for \eqref{eq:KoopObs} from initial-state and timestep pairs to future output values
\begin{align}\label{eq:data}
  \mathbb{D}_N=\{(\bm{x}^{(i)}_{0},t^{(i)}),  y^{(i)} \}_{i\in[N]}.
\end{align}
Our model should satisfy the following properties:
\begin{description}[style=multiline]
    \item[\namedlabel{itm:Trac}{(\tbm{D})}]  \textbf{Trajectory distributions in closed-form:} It corresponds to a  Gaussian process framework that models \eqref{eq:KoopObs} based on \eqref{eq:data}.
    \item[\namedlabel{itm:Eff}{(\tbm{E})}]  \textbf{Data-efficient for dynamical systems:} Allows a sample complexity reduction through the equivariance of \eqref{eq:KoopObs} w.r.t. past state trajectories.
   \item[\namedlabel{itm:Scale}{(\tbm{S})}]  \textbf{Scales to large-scale data:} Admits variational inference techniques for \eqref{eq:KoopObs} based on suitable inducing points.
\end{description}

The closed-form trajectory distributions \ref{itm:Trac} of our proposed framework allow for $\bm{1)}$ continuous epistemic uncertainty over entire time-intervals (an important challenge in utilizing GP models for dynamical systems \citep{Ridderbusch2023}) and $\bm{2)}$ tractable Bayesian model selection -- both of which are absent in existing spectral dynamics modeling \citep{Brunton2022ModernSystems}. Additionally, successful inference on large datasets strongly depends on the availability of informative inducing points \citep{pmlr-v9-titsias10a}, which is particularly hard for high-dimensional inputs \citep{pmlr-v206-moss23a}. We propose to utilize the timeseries structure to induce an equivariant covariance using past trajectories that does not increase input dimensionality. Our approach can reduce the maximal information gain \ref{itm:Eff} \citep{Srinivas2012} and allows for effective variational inference for large-scale GP regression \ref{itm:Scale}.\looseness=-1


\section{KOOPMAN SPECTRAL GAUSSIAN PROCESSES}
\label{sec:KEframework}
Here we introduce a GP that respects the \eqref{eq:KoopObs} structure, building on the extensive literature on GPs \citep{Rasmussen2006,Duvenaud2014}, generalized additive models \citep{Krause2011ContextualOptimization,MojmírPHD} and their interplay with linear operators \citep{Matsumoto2024}.
\subsection{GP-Based Koopman Mode Decomposition}
Given a finite set of eigenvalues $\{\lambda_j\}_{j=1}^{|D|}$, the spectral decomposition \eqref{eq:KoopObs} induced by the Koopman operator can be straightforwardly translated into a structured GP model. For this, we assume independent GP priors $g_j(\cdot)\sim\mathcal{GP}(0,k_{g_j}(\cdot,\cdot))$ for the eigenfeatures $g_j(\cdot)$ and exploit the linearity of \eqref{eq:KoopObs} with the modes $\langle g^\prime_j,h\rangle$ equal to constant values, such that $y(\cdot,\cdot)$ follows a distribution $y(t,\bm{x}_0)\sim\mathcal{GP}(0,k_y((t,\bm{x}),(t',\bm{x}')))$ with 
\begin{align}
  \textstyle   k_y((t,\cdot),(t',\cdot'))&:= \sum\limits_{j \in [D]}{a}_j(t,t^\prime)k_{g_j}(\cdot,\cdot^\prime)\label{eq:SDK}\tag{${\txt{cov}_\text{\txt{SD}}}$},
\end{align}
where $ {a}_j(t,t^\prime) = \operatorname{e}^{\lambda_j t}\operatorname{e}^{\lambda^*_j t^\prime}$, and $k_{{g_j}}(\cdot,\cdot)$ can be arbitrary kernels. Conceptually, the kernel \eqref{eq:SDK} is akin to a simulation-induced kernel for linear systems \citep{CHEN2018109}, but now captures nonlinear dynamics \eqref{eq:SSmodel}. It exhibits the intuitive property that the spatial kernels $k_{{g_j}}(\cdot,\cdot)$ capture the representational uncertainty due to lifting of the dynamics to a higher dimensional space, in which the forecasting uncertainty evolves linearly according to the LTI features $\left\{{a}_j(t, t^{\prime})\right\}_{j \in [D]}$.
A temporal covariance ${a}_j(t, t^{\prime})$ with decay $|\lambda|$ close to zero will result in models with uniform uncertainty over time, whereas taking negative or positive decays will result in models with contracting or expanding variance over time, respectively. This allows for a straightforward encoding of prior knowledge about the temporal evolution of systems, e.g., stability. 

\textbf{Spectral hyperprior.~~}
While we generally do not have direct access to a sequence of eigenvalues $\{\lambda_j\}_{j=1}^{\infty}$, it is well known that this spectrum can be effectively covered by sampling a random distribution~\citep{KKR_neurips2023}. We can parameterize a spectral distribution, such that a high-likelihood representation for a finite series in \eqref{eq:SDK} can be obtained by integrating its parameters into Bayesian model selection.
To adopt such a spectral prior, we use the noise transfer (outsourcing) trick by \cite[Theorem 5.10]{kallenberg1997foundations} to model the eigenvalue distribution $p(\lambda) \approx \rho_{}(\bm{\vartheta})$.
This choice limits the number of required parameters since $\bm{\vartheta}$ has fewer parameters (degrees of freedom) than the number of eigenspaces $\|\bm{\vartheta}\|_0 \ll |D|$. Furthermore, it allows for the use of log-likelihood maximization just like with any other set of hyperparameters.
Note that the exact Koopman operator $\mathcal{A}_t$ can be approximated with arbitrary accuracy using a sufficiently large finite sequence $\{\lambda_j(\mathcal{A}_t)\}_{j=1}^D$ \citep{KKR_neurips2023}.


\subsection{Koopman-Equivariant Kernels}
While we can use arbitrary kernels for $k_{{g_j}}(\cdot,\cdot)$ in \eqref{eq:SDK}, such a dynamics-agnostic formulation does not exploit any properties of the Koopman operator underlying the spectral decomposition \eqref{eq:KoopObs} which gives rise to \eqref{eq:SDK}. In particular, Koopman operators $\mathcal{A}_t$ allow us to reverse the order of forward simulation and measurement function $h(\cdot)$ when determining the output $y$ at a time $t$, i.e., $h(\bm{F}_t(\bm{x}_0))=[\mathcal{A}_t h](\bm{x}_0)$. %
Considering only a single eigenvalue $\lambda_j$ of the spectral decomposition \eqref{eq:KoopObs} of the Koopman operator $\mathcal{A}_t$, this equivalence of representations induces a special class of functions, which we refer to as Koopman-equivariant. %
\begin{definition}[Koopman-equivariance]\label{def:KEIGS} 
    Let $[\tau_s,\tau_e] \subset \Set{R}$ be a compact subset of the time axis and $\mathcal{M}$ a manifold. 
    A map $\phi_{\eig}: \mathcal{M} \mapsto \Set{C}$ is called $ [\tau_s,\tau_e]_{\eig}$-{\em Koopman-equivariant} if 
    \begin{align}
        \phi_{\eig}\circ\bm{F}_t=\exp{\lambda t} \phi_\eig
    \end{align}
    on $\mathcal{M}$ for any $t \in [\tau_s,\tau_e]$.
\end{definition}
To ensure that the prior $\mathcal{GP}(0,k_{g_j})$ over eigenfeatures $g_j(\cdot)$ encodes Koopman-equivariance, observe that \cref{def:KEIGS} is a special case of the more general concept of subgroup equivariance \citep{pmlr-v139-satorras21a} adapted to Koopman operator open eigenfunctions \citep{Mezic2020SpectrumGeometry}. Since equivariance can be interpreted as a form of symmetry, this allows for the application of well-known techniques for the symmetrization of functions to ensure obtaining Koopman-equivariant functions. We follow the agnostic symmetrization approach of \citep{Kim2023,Nguyen2023}.
For this, we embed past state trajectories from time $\tau_s$ to $\tau_e$ into the input data, i.e.,
\begin{align}\label{eq:dataTraj}
  \mathbb{D}^{[\tau_s,\tau_e]}_N=\{(\bm{x}^{(i)}_{[\tau_s,\tau_e]},t^{(i)}),  y^{(i)} \}_{i\in[N]},
\end{align}
and exploit Definition~\ref{def:KEIGS} to obtain projections onto Koompan-equivariant subspaces of our hypothesis space. Notably, we can satisfy Koopman-equivariance in a simple and constructive manner, i.e., by taking an expectation, as we formalize in the following result.\looseness=-1





\begin{theorem}\label{thm:Symm}
Consider the symmetrization operator $\KEop{[\tau_s,\tau_e]}{\eig}: L_{\mu}^2(\mathcal{X}) \rightarrow L_{\mu}^2(\mathcal{X})$ defined as
\begin{align}\label{eq:EqvarEO}
    \KEop{[\tau_s,\tau_e]}{\eig} g := \mathbb{E}_{t \sim \mu{([\tau_s,\tau_e])}}\left[ \operatorname{e}^{-\lambda t} g\left(\bm{x}(t))\right)\right]
\end{align}
so that it is well-defined and self-adjoint. Then,
$\KEop{[\tau_s,\tau_e]}{\eig}$ maps $g$ to the unique solution of
\begin{align}
    \phi_{\eig} = \argmin_{\psi \in \mathcal{S}_\eig} \| g - \psi \|^2_{\mu}.
\end{align}
where $\mathcal{S}_\eig=\{g \in L_{\mu}^2(\mathcal{X}):\KEop{[\tau_s,\tau_e]}{\eig}  g=g\}$.
\end{theorem}

\begin{figure}[t!]
    \centering
    \input{plots/past_future.tikz}
    \caption{Backward time equivariance interval (red) and the simulation-induced prediction horizon (green).}
    \label{fig:seq2seq}
\end{figure}

By construction, the symmetrization operator \eqref{eq:EqvarEO} renders every base function $g$ Koopman-equivariant on arbitrary past time intervals $[\tau_s,\tau_e]$, %
such that it allows us the straightforward design of Koopman-equivariant features $\phi_{\eig_j}(\cdot)$. To ensure causality of these features, we restrict ourselves to past trajectories, i.e., intervals $[\tau_s,0]$, which results in 
\begin{align}
    \phi_{\eig_j}(\bm{x}_{[\tau_s,0]}) = [\mathcal{E}_{\lambda_j}^{[\tau_s,0]} g](\bm{x}_0)).
\end{align}
Finally, since $\mathcal{E}_{\lambda_j}^{[\tau_s,0]}$ is a linear operator, we can exploit the closedness of Gaussian processes under linear operators \citep{Matsumoto2024} by placing a GP prior $g(\cdot)\sim\mathcal{GP}(0,k_{g}(\cdot,\cdot))$ on $g(\cdot)$ with arbitrary kernel $k_{g}(\cdot,\cdot)$, such that we obtain the Koopman-equivariant prior $\phi_{\eig_j}(\cdot)\sim\mathcal{GP}(0,k_{\phi_{\eig_j}}(\cdot,\cdot))$ with $k_{\phi_{\eig_j}}(\cdot,\cdot):=\mathcal{E}_{\eig_j}k_{g}(\cdot,\cdot^\prime)\mathcal{E}^{*}_{\eig_j}$, inducing a Koopman-equivariant spectral decomposition kernel 
\begin{align}\label{eq:KE-SDK}
  \textstyle   k^{\txt{KE}}_y((t,\cdot),(t',\cdot'))&:= \sum\limits_{j\in [D]}{a}_j(t,t^\prime)k_{\phi_{\eig_j}}(\cdot,\cdot),
\tag{${\txt{cov}_\text{\txt{KESD}}}$}
\end{align}
that exploits the full information in past trajectories in a structured way to allow predictions of the future evolution using \eqref{eq:KoopObs} as illustrated in Figure \ref{fig:seq2seq}.














\textbf{Practical considerations.~~}
In practice, our resolution of a trajectory is commonly limited by a sampling time, so we only have access to an empirical measure $\hat{\mu}$ for the expectation in \eqref{eq:EqvarEO}. Nevertheless, in most practical considerations and sufficiently regular trajectories, we will get a good sample-based approximation using quadrature so that $|\hat{\mathcal{E}}^{[\tau_s,0]}_{\eig}g-\KEop{[\tau_s,0]}{\eig}g|\approx 0$.\footnote{For a detailed treatment of this aspect cf. supplemental.} 

















\section{ANALYSIS OF SAMPLE COMPLEXITY}
\label{section:analysisofsamplecomplexity}
To analyze the sample complexity of regression \ref{itm:Eff}, we use the notion of \textit{information gain}, classical in the analysis of Gaussian processes  \citep{Srinivas2012Information-TheoreticSetting}. Our analysis allows us to put into perspective the sample complexity gains of using the proposed operator-theoretic GP w.r.t. more generic and less structured nonlinear models for dynamical systems. Thus, the following complexity study is a first in the literature.
Given the generalized additive structure of our \textit{spectral decomposition covariance} \eqref{eq:SDK}, we quantify the sample-complexity of learning using the well-established notion of maximal {information gain} 
\begin{align}\label{eq:IGbase}
\gamma^{\sigma}_N(k):=\sup _{\bm{x}_N \subseteq \Set{X}} I\left(\bm{y}_N ; y\right)=\frac{1}{2}\operatorname{log}|\bm{I}_N+{\sigma^{-2}}\bm{K}_N|,
\end{align}
that measures the interaction between the data, observation noise, and kernel. This quantity frequently appears in the analysis of the generalization or worst-case estimation error of Gaussian processes \citep{Krause2011ContextualOptimization,pmlr-v130-vakili21a}. The less complex the feature map of the kernel on the same $\Set{X}$, the smaller \eqref{eq:IGbase} will be, implying better statistical efficiency.



\subsection{Mercer Eigenvalues as a Proxy to Information Gain} To study the effect of general kernels on the complexity of learning, we will rely on Mercer's theorem \citep{Mercer1909FunctionsEquations} which states that for a well-behaved $k_x$, it can be expressed via the series expansion
\begin{align}\label{eq:Mercer}
   \textstyle k_x\left(\bm{x}, \bm{x}^{\prime}\right)=\sum_{j=1}^{\infty} \mu_j \varphi_j(\bm{x}) \varphi_j\left(\bm{x}^{\prime}\right),
\end{align}
such that $\{\sqrt{\mu_j}\varphi_j\}_{j=1}^{\infty}$ form an orthornormal basis of $L^2(\Set{X})$ with respect to a finite Borel measure\footnote{Generalization to more general input spaces is straightforward \citep{Steinwart2012MercersTO}.}. The complexity bounds we derive in this work will depend on \textit{how rapidly the eigenvalues $\{\mu_j\}_{j=1}^{\infty} \subseteq \Set{R}_{+}$ decay}. The decay of these eigenvalues is closely related to the complexity of the nonparametric model as well as the generalization properties of the posterior \citep{Micchelli1979DesignPF}. Generally, these eigenvalues decay faster for covariate distributions that are concentrated in a small volume and for kernels that give smooth mean predictors \citep{widom_asymptotic_1963,widom1964asymptotic}. Thus, the bounds we prove here verify the intuition that our Koopman-equivariant covariance can provide an improved finite-sample performance \ref{itm:Eff}. 
To analyze the effects of the induced Koopman equivariance on the sample complexity, some assumptions are needed:
\begin{description}[style=multiline, leftmargin=3em,font=\normalfont,parsep=0.01em]
    \item[\namedlabel{asm:Hreg}{(\txt{HR})}]  \textit{Regularity of the hypothesis:} \textbf{a)} $k_x$ is a Mercer kernel \citep{Mercer1909FunctionsEquations}. \textbf{b)} $\forall \bm{x}, \bm{x}^{\prime} \in$ $\Set{X},\left|k_x\left(\bm{x}, \bm{x}^{\prime}\right)\right| \leq \bar{k}$, for some $\bar{k}>0$ \textbf{c)} $\forall j \in \mathbb{N}, \forall \bm{x} \in \Set{X}$, $\left|\varphi_j(\bm{x})\right| \leq r$, for some $r>0$.
    \item[\namedlabel{asm:Sspec}{(\txt{WS})}]  \textit{The past trajectory interval $[\tau_s,\tau_e]$ and the set of initial conditions form a non-recurrent domain.} 
    \item[\namedlabel{asm:Areg}{(\txt{OR})}]  \textit{The operator $A_{t=1}:=A_1=\sum^{\infty}_{j=1}\exp{\eig_j }\KEop{[\tau_s,0]}{\eig_j}$ is a compact normal operator.} 
\end{description}
Generally, \ref{asm:Hreg} is a mild requirement and is fulfilled for continuous kernels on compact domains \citep{Wang2022}, while \ref{asm:Areg} is classical for limiting the ill-posedness of inverse problems \citep{Cavalier_2008}. \ref{asm:Sspec} is a mild technical assumption and allows for a well-specified symmetrization and Theorem \ref{thm:Symm} non-vacuous, as it ensures the existence of uncountably many functions satisfying Definition \ref{def:KEIGS} for any eigenvalue \citep[Appendix A]{KKR_neurips2023}.
For our  technical results, we differentiate between \textit{mildly} and \textit{severely} ill-posed setting, based on \textit{exponential} and \textit{polynomial} $\eig_j(A_1)$ decay rates, respectively.
\begin{remark}[Strict complexity reduction]\label{rmk:reduction}
    A direct consequence of well-specified equivariance \ref{asm:Sspec} is a guaranteed \emph{strict} reduction in the effective dimension \citep{Elesedy2021B}, which is known to equal the information gain up to logarithmic factors \citep{pmlr-v151-zenati22a}.\looseness=-1
\end{remark}
To study information gain rates for a general class of kernels (that includes our own), we will rely on recent results based on spectral decay properties of kernels \citep{pmlr-v130-vakili21a} and can state the following.
\begin{theorem}\label{th:infogain_asymp}
Consider the Mercer eigenvalues $\{\mu_j\}^{\infty}_{j=1}$ for $k_x$ and let Assumptions \ref{asm:Hreg},\ref{asm:Sspec} and \ref{asm:Areg} hold. Then $\exists \theta \geq 1$ for\vspace{-1em}
\begin{description}[style=multiline, leftmargin=3em,font=\normalfont,noitemsep]
    \item[\namedlabel{case:PD}{(\txt{Poly})}] $\eig_j(A_1)\lesssim j^{-p} \wedge \mu_j\lesssim j^{-a}$, $a>1$ or
    \item[\namedlabel{case:ED}{(\txt{Exp})}] $\eig_j(A_1)\lesssim \exp{-j^p} \wedge~\mu_j \lesssim  \exp{-j^b}$, $b>0$ so that
\end{description}
\[
    \gamma^{\sigma}_N\left(k^{\txt{KE}}_{y}\right)  \lesssim
    \tilde{\mathcal{O}}(\left(\gamma^{\sigma}_N(k_{x})\right)^{\nicefrac{1}{\theta}})
\]
where $\theta=\frac{\max\{2p,a\}}{a}$ \ref{case:PD} and $\theta=\frac{\max\{2p,b\}}{b}$ \ref{case:ED}.
\end{theorem}

The above result summarizes the rate gains from the Koopman-equivariant Gaussian process. In case the equivariance operator has a sufficiently strong singular value decay, i.e., $\theta > 1$, the information gain of our Koopman-equivariant GP with covariance \eqref{eq:KE-SDK} may be much smaller than for \eqref{eq:SDK}. Crucially, $\theta \geq 1$ is guaranteed, so a slow decay of the operator eigenvalues values will not deteriorate the already existing eigenvalue decay of $\{\mu_j\}^{\infty}_{j=1}$. As \ref{asm:Areg} plays the role of a feature extractor, our result suggests one could obtain a significantly improved rate when $\eig_j(A_1A_1^*)$ has a fast decay, signaling an induced RKHS with low complexity.\looseness=-1


The significance of the asymptotic rates for the maximum information gain when using \eqref{eq:KE-SDK} provided by Theorem~\ref{th:infogain_asymp} becomes clear when comparing the rates to the ones of other kernels as summarized in Table~\ref{tab:IGcomp}. For example, when using a na\"{i}ve contextual (spatio-temporal) kernel $k^{\txt{SE}}(t,t^\prime)\otimes k^{\txt{SE}}(\bm{x}_0,\bm{x}^\prime_i)$ \citep{pmlr-v151-zenati22a} defined over a joint spatio-temporal domain \citep{Li2024STkernel} via RBF kernels\footnote{The SE kernel is used for ease of exposition, but our results cover large classes of Mercer kernels.} $k^{\txt{SE}}$, it is well known that the maximum information gain behaves as $\tilde{\mathcal{O}}(\log(N)^{n+2})$. Due to the LTI features $a_j$ in \eqref{eq:SDK} for describing temporal correlations, the information gain for \ref{eq:SDK} reduces to $\tilde{\mathcal{O}}(\log(N)^{n+1})$ \citep{MojmírPHD}. In contrast, our proposed kernel  \ref{eq:KE-SDK} can exploit the inherent structure imposed by dynamical systems through Koopman-equivariance, such that a complexity of $\tilde{\mathcal{O}}(\log(N)^{\frac{n}{\theta}+1})$ is guaranteed when using SE kernels as the basis for $k_{\phi_{\lambda_j}}$ in \eqref{eq:KE-SDK}. Hence, for $\theta>1$, we virtually counteract the curse of spatial dimensionality that comes from the generic and measure-agnostic bounds on the eigenvalue decay for popular kernels \citep{pmlr-v75-belkin18a}. Note that, due to employing Koompan-equivariance (Theorem \ref{thm:Symm}), the sample complexity is not impeded by the length or time-discretization of a continuous-time trajectory, which sets \eqref{eq:KE-SDK} apart from kernels agnostic of the dynamical systems properties as illustrated in Table~\ref{tab:IGcomp}.\looseness=-1


\begin{table}[t!]%
    \caption{Worst-case information gain (w/o $\log$ factors) for universal RBF base kernel $k^{\txt{SE}}$. Under mild conditions, $\theta  \geq 1$ guarantees reduced sample complexity. The number of discretization steps necessary to handle trajectory inputs in na\"{i}ve kernels and \eqref{eq:SDK} is denoted by $|G|$.}
    \label{tab:IGcomp}
    \setlength{\tabcolsep}{5pt}
    \centering
    \scriptsize
    \vspace{-2ex}
    \begin{tabular}{l|ccc}
    \toprule
        ${\gamma^{\sigma}_N(\cdot)}$ & na\"{i}ve & \eqref{eq:SDK}  & \eqref{eq:KE-SDK} \\
        \midrule
       $\bm{x}_0$ &  $\tilde{\mathcal{O}}(\log(N)^{n{+}2})$ & $\tilde{\mathcal{O}}(\log(N)^{n+1})$ & --- \\
       $\bm{x}_{{\tau_s{,}0}}$ &$\tilde{\mathcal{O}}(\log(N)^{|G|n{+}2})$ & $\tilde{\mathcal{O}}(\log(N)^{|G|n{+}1})$ & $\tilde{\mathcal{O}}(\log(N)^{\frac{n}{\theta}{+}1})$\\
        \bottomrule
    \end{tabular}
\end{table}

\section{VARIATIONAL INFERENCE FOR KOOPMAN-EQUIVARIANT GPs}

\begin{figure*}[t!]
    \setlength{\tabcolsep}{1pt}
    \renewcommand{\arraystretch}{0.25}
    \centering
    \begin{tabular}{cccccc} 
    \begin{tikzpicture}
            \node[rotate=90] at (0,0) {noise-free};
            \node at (0,-1.) {~};
    \end{tikzpicture}
    & ~&\input{plots/vl_exact_kegp_trajectory}
    & \input{plots/vl_exact_mtgp_trajectory}
    &\input{plots/vl_exact_kor_trajectory}
    &\definecolor{fig_green}{RGB}{0,128,0}
    \begin{tikzpicture}
        \node at (0,0) {\begin{tikzpicture} 
    \begin{axis}[%
    hide axis,
    xmin=10,
    xmax=50,
    ymin=0,
    ymax=0.4,
    legend style={draw=white!15!black,legend cell align=left}
    ]
    \addlegendimage{semithick, fig_green}
    \addlegendentry{\tiny input};
    \addlegendimage{semithick, fig_green, dash pattern=on 5.55pt off 2.4pt}
    \addlegendentry{\tiny ground truth};
    \addlegendimage{semithick, blue};
    \addlegendentry{\tiny prediction};
    \end{axis}
\end{tikzpicture}};
    \node at (0,-1.) {~};
    \end{tikzpicture}
    \\
    \begin{tikzpicture}
            \node[rotate=90] at (0,0) {noisy};
            \node at (0,-1.65) {~};
    \end{tikzpicture}    
    &~ &\input{plots/vl_noisy_kegp_trajectory}
    & \input{plots/vl_noisy_mtgp_trajectory}
    &\input{plots/vl_noisy_kor_trajectory}
    &
    \end{tabular}
    
     \vspace{-0.7\intextsep}
\caption{Multi-step mean and 2-sigma interval of the prediction for predator population from the predator-prey dynamics for our proposed Koopman-equivariant GP (KE-GP), a generic contextual kernel (C-GP), and a Koopman operator regression approach (KOR) for noise-free (top) and noisy (bottom) training data.}
\label{fig:illustrative}
\end{figure*}

\label{section:svigps}
GP model scale poorly with the dataset size, requiring $\BigO(N^3)$ computations and $\BigO(N^2)$ memory during training. To address this, in the following we present a sparse GP approximation that uses a variational inference approach. Our approach closely follows stochastic variational inference with sparse GPs \citep{hensmann2013gaussian,Wilk2018}, with some additional modifications to the selection and optimization of inducing points. As discussed at the end of this section, this choice allows considerable scalability during training \ref{itm:Scale}, and presents desirable properties when used in conjunction with our equivariant covariance function.
\subsection{Variational Inference with Sparse GPs}

	During training, computational complexity stems mainly from the inversion of the data covariance matrix $\Kff$, where $\left[\Kff\right]_{nn'} =  k^{\txt{KE}}_y((t,\vz^{(n)}),(t',\vz^{(n^\prime)})) =: k^{\txt{KE}}_{y(t,t^\prime)}(\vz^{(n)},\vz^{(n^\prime)})$. To address these problems, we resort to variational inference using \emph{inducing variables}  \citep{quinonero2005unifying,hensmann2013gaussian}. %
 We obtain the sparse GP by considering $M \ll N$ inducing observations $\vm$, corresponding to the inducing trajectories $\smash{\{{\vz}^{(m)}:=\bm{x}^{(m)}_{[\tau_s,\tau_e]}\}_{m=1}^M = \mZ}$. Instead of employing the GP prior for the trajectories $\vm$ corresponding to $\mZ$, we place a simpler Gaussian prior $q(\cdot)$ over $\vm$, specified by a mean $\vm$ and covariance $\mS$. By leveraging a variational inference argument \citep{hensmann2013gaussian}, we then obtain the approximate Gaussian process posterior %
 $\GP(\tilde{\mu}(\cdot), \tilde{\sigma}^2(\cdot, \cdot))$ with mean and variance
	\begin{align}
 \begin{split}
	\tilde{\mu}(\cdot) {=} & \Kdu\Kuu\inv\vm,\qquad \\  \tilde{\sigma}^2(\cdot, \cdot) {=}& \kdd - \Kdu\Kuu\inv[\Kuu{ -} \mS]\Kuu\inv\Kud \label{eq:varpost}\,
 \end{split}
	\end{align}
	where $[\Kuu]_{ij} = k^{\txt{KE}}_{y(t,t^\prime)}(\vz^{(i)}, \vz^{(j)})$ %
 and $\smash{\Kud = [k^{\txt{KE}}_{y(t,t^\prime)}(\vz^{(m)}, \cdot)]_{m=1}^M}$. The shape of the posterior can be adjusted by changing the values ${\mZ}$ and output mean $\vm$ and variance $\mS$ of the inducing outputs. Here, we follow an approach similar to \cite{hensmann2013gaussian}, which allows us to minimize by sampling batches of data instead of computing the full gradient, improving memory complexity to $\mathcal{O}(BM+M^2)$. We choose the hyperparameters and inducing points jointly by minimizing the loss
 \begin{align}
 \begin{split}
     \textstyle\sum_{i=1}^{N} &\big( %
 {-}\textstyle \frac{1}{2}\log(2\pi\sigma_{\txt{on}}^{2}) {-}\frac{\sigma_{\txt{on}}^{2}}{2}(y_i-\bm{k}_i^\top \Kuu^{-1} \bm{m})^2 
\\ &{-} \textstyle \frac{1}{2} \sigma_{\txt{on}}^2 \tilde{k}_{i,i} {-} \frac{1}{2} \text{tr} \left( \bm{S} \bm{\Lambda}_i \right) \big)
- \text{KL} \left( q(\bm{u}) \| p(\bm{u}) \right),
\end{split}
 \end{align}
where $\bm{k}_i =[k^{\txt{KE}}_{y(t,t^\prime)}({\vz}^{(m)}, \bm{x}^{(i)}_{[\tau_s,\tau_e]})]_{m=1}^M$, $\Lambda_i =  \Kuu^{-1} \bm{k}_i \bm{k}_i^\top \Kuu^{-1}
$, $\tilde{k}_{i,i} = [\Kff - \Kfu \Kuu^{-1} \Kfu]_{ii}$, and $[\Kfu]_{ij} = k^{\txt{KE}}_{y(t,t^\prime)}(\bm{x}^{(i)}_{[\tau_s,\tau_e]},\vz^{(j)})$. However, unlike \cite{hensmann2013gaussian}, we only optimize inducing trajectories and avoid sampling any time/context-related inducing points, which is due to the structure of the spectral decomposition \eqref{eq:KoopObs}. This allows a significant reduction in training complexity compared, e.g., to the generic contextual kernel $k^{\txt{C}}(\cdot,\cdot):=k^{\txt{SE}}(t,t^\prime)\otimes k^{\txt{SE}}(\bm{x}_{0},\bm{x}^\prime_{0})$, 
where inducing points representing time are also optimized.

Due to the structure of our Koopman-equivariant construction, and the resulting benefits in information gain presented in Section \ref{section:analysisofsamplecomplexity}, our approach is also more robust to a lack of correlation between points, an issue commonly observed in conventional sparse GP approximations \citep{Murray2010,HensmanNIPS2015}. In particular, a lower information gain implies that less inducing points are required than with conventional GPs to accurately represent the full posterior \citep{Burt2019RatesRegression}.%





	
\section{NUMERICAL EXPERIMENTS}\label{sec:NumExp}
To demonstrate the applicability of KE-GPs to realistic data, we perform qualitative and quantitative studies on a set of benchmark examples. As a classical dynamical systems example, we choose the predator-prey model; from the robotics domain, we consider expert demonstrations on the halfcheetah environment from D4RL~\citep{fu2020d4rl} and forecast the first state and action; as a high uncertainty example we choose temperature data from the Monash TSF benchmark \citep{godahewa2021monash} taken at \textit{Oikolab} -- demonstrating the usefulness of building in \eqref{eq:KoopObs} structure as a prior for highly complex weather dynamics. Since these datasets provide a single long trajectory, we split off the last chunk as test data and partition the trajectory into $N$ input-task pairs to comply with our model structure.\looseness=-1

\begin{table}[t!]%
    \caption{Comparable nonparametric frameworks.}
    \label{tab:contributionNonparam}
    \centering
    \footnotesize
    \vspace{-2ex}
    \begin{tabular}{l|ccc}
    \toprule
        Method& \ref{itm:Trac} & \ref{itm:Eff} & \ref{itm:Scale}\\
        \midrule
        C-GP \citep{Li2024STkernel} & ({\color{green!80!black}{\text{\cmark}}})   & \color{red!80!black}{\text{\xmark}} & \color{green!80!black}{\text{\cmark}}\\
        KOR \citep{Kostic2022LearningSpaces} & \color{red!80!black}{\text{\xmark}}  & ({\color{green!80!black}{\text{\cmark}}}) & \color{green!80!black}{\text{\cmark}} \\
        {\text{\textbf{KE-GP} (ours)}} & \color{green!80!black}{\text{\cmark}} & \color{green!80!black}{\text{\cmark}} & \color{green!80!black}{\text{\cmark}} \\
        \bottomrule
    \end{tabular}
\end{table}
\begin{table}[t!]%
    \caption{Simulations on small subsets of the Predator-Prey (PP), D4RL Half-Cheetah (D4RL), and Oikolab Temparature (OT) datasets. We report RMSE in mean and standard deviation for 5 runs. Training data are $N$ past trajectories over a unit-normalized interval, discretized using $H$ equidistant points.}
    \label{tab:smallsim}
    \centering
    \footnotesize
    \vspace{-2ex}
    \begin{tabular}{lc|ccc}
    \toprule
      & $N\times H$& \tbm{KE-GP} & C-GP & KOR\\
   \midrule
    PP&32$\times$32 & 0.28$\pm$0.0& 0.60$\pm$0.0 & \textbf{0.27}$\pm$0.0\\
    D4RL&32$\times$16 & 0.46$\pm$0.0& 0.98$\pm$0.0 & \textbf{0.44}$\pm$0.0\\
    OT&32$\times$16 & \textbf{0.63}$\pm$0.0& 0.68$\pm$0.0 & 0.86$\pm$0.0\\
    
        \bottomrule
    \end{tabular}
\end{table}
\begin{table}[t!]%
    \caption{Simulations on large subsets of the Predator-Prey (PP), D4RL Half-Cheetah (D4RL), and Oikolab Temparature (OL) datasets. Training data are $N$ past trajectories over a unit-normalized interval, discretized using $H$ equidistant points.}
    \label{tab:largesim}
    \centering
    \footnotesize
    \vspace{-2ex}
    \begin{tabular}{lc|ccc}
    \toprule
      & $N\times H$& \tbm{KE-GP} & C-GP & KOR\\
   \midrule
    PP &\phantom{0}512$\times$32& \textbf{0.26}$\pm$0.0\phantom{0}& 0.42$\pm$0.0\phantom{0} & 0.53$\pm$0.0\\
    D4RL$\!\!\!$ &3000$\times$16& 0.48$\pm$0.02 & 0.66$\pm$0.07& \textbf{0.44}$\pm$0.0\\
    OT &4000$\times$16& \textbf{0.54}$\pm$0.03& 0.60$\pm$0.02 & 0.71$\pm$0.0\\
    
        \bottomrule
    \end{tabular}
\end{table}
    


\textbf{Baselines~~} To put our novel algorithm into perspective, we compare to two standard approaches: Gaussian Processes with the time-dependent context (C-GP) by \cite{Li2024STkernel} and operator regression for dynamical systems (KOR)~\citep{Kostic2022LearningSpaces} from the \hyperlink{https://github.com/Machine-Learning-Dynamical-Systems/kooplearn}{\txt{kooplearn}} package and equip it with SciPy's \citep{2020SciPy-NMeth} \textit{minimize} for hyperparameter tuning. While Koopman-based, their method does not forecast via a decomposable model akin to \eqref{eq:KoopObs}, but requires taking powers of a $D{\times}D$ dense matrix. 
As summarized in Table~\ref{tab:contributionNonparam}, these two methods exhibit some of the important properties discussed in Section \ref{sec:ProbStat}, such that they are valuable baselines.\looseness=-1

\begin{figure}[t]
    \centering
    \input{plots/empirical_information_gain}
    \vspace{-1.5\intextsep}
    \caption{Empirical information gain $\hat{\gamma}$ for a 2D linear system scaled to remove effects of constants. %
    The improved rates confirm our theoretical results for Koopman-equivariant GPs, leading to a lower information gain compared to their non-equivariant counterpart \eqref{eq:SDK}, even when a randomly sampled eigenvalue spectrum $\{\eig_j\}_{j=1}^D$  is used instead of the true spectrum.
    }
    \label{fig:enter-label}
\end{figure}
\textbf{Qualitative Comparison~~} We first qualitatively compare the different approaches on the predator-pray model as illustrated in Figure~\ref{fig:illustrative}. It can be clearly seen that all methods allow to accurately predict the future trajectory when given noise-free data from the dynamical system. However, when the state trajectories are perturbed by noise as commonly encountered in practice, significant differences between the predictions become apparent. While our proposed KE-GP maintains a high accuracy and reasonably small confidence intervals, the estimated uncertainty of the C-GP considerably grows and the prediction accuracy for longer horizons significantly drops for the Koopman operator regression (KOR) approach from \cite{Kostic2022LearningSpaces}. This high accuracy of KE-GPs can be attributed to their strong generalization capabilities captured by the information gain as discussed in Section \ref{section:analysisofsamplecomplexity}. When empirically comparing this value, we can immediately see an improvement over non-equivariant kernels, cf. Figure \ref{fig:enter-label}.\looseness=-1

\textbf{Quantitative Evaluation~~} We perform two evaluations for each model run and dataset: a small subset for which exact inference is possible and a large subset handled using variational inference. 
We observe that KE-GP performs robustly on all datasets and sizes as depicted in Tables~\ref{tab:smallsim} and \ref{tab:largesim}. While it consistently outperforms the C-GP, the KOR baseline is better on some datasets but the difference in accuracy is marginal. Importantly, KE-GPs provide a significant improvement over KOR for the other data sets. This is fully in line with our qualitative comparison, which shows that KOR can be sensitive to noise with a severe impact on its performance. In addition, we want to stress here that the KOR method does not come with methods for automated model selection, such that manual parameter tuning was necessary to make it competitive. Therefore, this comparison clearly demonstrates the improved generalization ability achieved by embedding the operator-theoretic foundations in our KE-GP approach.\looseness=-1

    
\section{CONCLUSION}
\label{sec:Concl}
We presented a novel approach to incorporate an operator-theoretic dynamical system structure into Gaussian process regression. Our framework enables a tractable probabilistic treatment of continuous-time dynamical models not present in existing literature. Utilizing a symmetrization tailored to dynamical systems, based on the concept of Koopman-equivariance (KE), we achieve a sample-complexity reduction compared to a contextual kernel without our proposed \eqref{eq:KoopObs} structure. In scaling to large datasets we exploit our model structure to avoid sampling any time/context-related inducing points. Hence, it does not suffer from a lack of correlation between inducing points, which is common for conventional sparse GP.
Through numerical experiments, we show the utility of our KE-GP, demonstrating superior prediction performance to vanilla contextual GPs and on par or better than Koopman operator learning. 


\subsubsection*{Acknowledgements}
This work is supported by the DAAD programme Konrad Zuse Schools of Excellence in Artificial Intelligence (relAI), sponsored by the German Federal Ministry of Education and Research and by the European Research Council (ERC) Consolidator Grant “Safe data-driven control for human-centric systems (CO-MAN)” under grant agreement number 864686.















\input{supplement}

\end{document}

\typeout{get arXiv to do 4 passes: Label(s) may have changed. Rerun}
