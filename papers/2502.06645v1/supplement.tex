 % !TEX root = main.tex
\onecolumn
\aistatstitle{
Supplementary Materials for ``Koopman-Equivariant Gaussian Processes"}

\appendix


The supplementary materials are organized as follows.
\begin{itemize}
\item Appendix~\ref{supl:related} expands on various aspects of related work from the main paper in greater detail.
    \item  Appendix~\ref{supl:koop} contains background on standing assumptions and spectral theory of Koopman operators. 
    \item To complement the sample complexity analysis, we address our framework's representation/approximation power in Appendix \ref{supl:repPWR}.
    \item  The proofs of theoretical results are found in Appendix~\ref{supl:Proof}.
    \item Details on the setup of numerical experiments are found in Appendix \ref{supl:NumExp}.
\item Finally, Appendix~\ref{supl:AddExp} includes additional experiments and ablation studies.
\end{itemize}


\section{EXPANDED RELATED WORK}\label{supl:related}
\paragraph{Vanilla GPs}
Gaussian process (GP) regression \citep{Rasmussen2006} has attracted attention for learning nonlinear dynamical systems due to its capability of inferring models with little structural prior knowledge: either by using so-called universal kernels \citep{Micchelli2006UniversalKernels} or placing a prior on a set of kernels and optimizing their likelihood of explaining the data \citep{Duvenaud2014}. 
In particular, their ability to quantify epistemic uncertainty has led to a common application in safety-critical control problems \citep{BerkenkampECC15,pmlr-v37-sui15,NIPS2017_766ebcd5,CuriCDC22}. However, commonly used GP models are single-step predictors, such that uncertainty propagation necessitates approximations when predicting probability distributions more than a single time step in the future. Uncertainty propagation often relies on iterative approaches, in which the previous predictions are used as uncertain inputs to the GP model. This can be exploited in a sampling-based fashion by randomly drawing states \citep{Bradford2019} or using the unscented transform \citep{Ko2007a}.
While the computational complexity of sampling-based approaches can be reduced through further approximations \citep{pmlr-v120-hewing20a, TBpredGP}, it generally remains high.
Approximating the predictive distributions, e.g., using a Taylor approximation \citep{Girard2003} or through exact moment matching \citep{Deisenroth2011PILCO:Search} can reduce the complexity, there are no accuracy guarantees of these approximations for long-term forecasts. 
Direct solutions to these challenges include, e.g., direct modeling of uncertainty intervals \citep{Polymenakos2020, Curi2020} or using ``a GP per time-step'' of prediction \citep{Pfefferkorn2022}, which suffers from a lack of time-correlation and forecast non-linearity.


\paragraph{State-space GPs}
A variety of works considers models with task correlation. Considering modeling dynamical systems, latent variable state-space models have the ability to decouple the model into the dynamics (process) and static (output) structures \citep{Wang2005_ccd45007,pmlr-v9-titsias10a, Frigola2014, Damianou2016, NIPS2017_1006ff12}. Like the recent work of \citep{Fan2023}, these models are limited to settings where a single trajectory is available and do not exploit any time-series structure. Still, they require posterior approximations due to the nonlinearity of latent dynamics. 
Aiding tractability, some works consider linear time-invariant (LTI) models \citep{pmlr-v5-alvarez09a,Sarkka2013}, but come with strong prior-knowledge requirements and unclear representational power. In particular, our KE-GPs can be considered as a continuous
contextual GP for dynamical systems with \eqref{eq:KoopObs} structure.

\paragraph{Koopman operator-based approaches}
While operator regression \citep{Williams2015ADecomposition,Klus2020EigendecompositionsSpaces,Kostic2022LearningSpaces,Li2022OptimalLearning,Ishikawa2024,Mauroy2024,Meunier2024} could be applied to build an LTI predictor \eqref{eq:KoopObs}, this comes with inherent limitations. Namely, the recovery of normal spectra and eigenspaces of $\mathcal{A}_t$ using operator regression in an infinite-dimensional RKHS is an \textit{ill-posed inverse problem} \citep{Knapik2011,Knapik2016,Horowitz2014}. Spectral estimation gets increasingly hard with the eigenvalue decay of the covariance \citep{klebanov2020rigorous}, limiting the utility of estimated spectra and eigenspaces. To mitigate these effects, \citet{Kostic2023KoopmanEigenvalues} suggests using low-rank estimators and empirically estimated RKHSs \citep{kostic2024learning} to control the degree of ill-posedness. However, there is no guarantee such a low-rank representation would span an observable of interest and form an LTI predictor \eqref{eq:KoopObs}. In stark contrast, our KE-GP regression bypasses this ill-posedness by construction and directly learns a universal representation of \eqref{eq:KoopObs} in a probabilistic fashion using Bayesian principles.
    Furthermore, \textit{our approximation-based complexity bounds} (in terms of information gain) \textit{are measure-independent and do not require any i.i.d.-type sampling assumptions}. This is in stark contrast to state-of-the-art concentration results in Koopman operator learning by \cite{Kostic2022LearningSpaces,Kostic2023KoopmanEigenvalues} that are dependent on measures, cf. \citep{pmlr-v75-belkin18a} for a discussion.

\paragraph{Works connecting Koopmanism and GPs}
Previous attempts at connecting Koopmanism to GPs \citep{Lian2020OnOperators,10.1162/neco_a_01555,Loya2023}  rely on heuristics and ad-hoc chocies, lacking theoretical justification as well as rigorous representational considerations. Furthermore, they hinge on heuristics by applying subspace-identification or dynamic mode decomposition before applying Gaussian process regression. In contrast, we offer a principled and fully-tractable approach with provable representational and learning guarantees.


\paragraph{Signature kernels}
Also geared towards sequential data, there is a recent rise in popularity of so-called \textit{signature kernels} \citep{Kiraly2016KernelsData, Lee2023TheKernel,Salvi2021ThePDE, Lemercier2021SigGPDE:Data} that also use a symmetrization to be rendered time-reparametrization invariant. This prohibits the extraction of dynamical system representations related to transfer operators and their eigenfunctions. Crucially, time-reparametrization-invariance allows them to excel at discriminative tasks \citep{Lemercier2021SigGPDE:Data,Salvi2021ThePDE} but not at generative tasks such as long-term forecasting \citep{KKR_neurips2023}.


\section{KOOPMAN OPERATOR MODELS FOR DETERMINISTIC DYNAMICS}\label{supl:koop}

\begin{remark}[Operator boundedness]\label{rmk:bounded}
    Consider a forward complete system on a compact set $\Set{X}$ and a continuous flow $\bm{F}_{t}$. It is well-known that a  time-$t$ Koopman operator $\mathcal{A}_t$ is then a contraction semigroup on ${C}(\Set{X})$ \citep{Kreidler2018CompactSystems}. Due to forward completeness of the flow, we therefore obtain a Banach algebra ${C}(\Set{X})$ with a bounded semigroup $\{\mathcal{A}_t\}_{t\geq0} \in \mathcal{B}({C}(\Set{X}))$.%
\end{remark}

\begin{definition}[Non-recurrent domain]\label{def:Nonrec}
Let time \(T \in(0, \infty)\) be given. A set \(\Set{X}_0\subset\) \(\Set{X}\) is called nonrecurrent if
\[
\bm{x}\in \Set{X}_0 \Longrightarrow \bm{F}_t(\bm{x}) \notin \Set{X}_0 \quad \forall t \in(0, T].
\]
A non-recurrent domain is the image $\Set{X}_{T}$ of non-recurrent set of initial conditions $\Set{X}_0$ traced out by the flow map $\bm{F}_t(\cdot)$
\[
\Set{X}_{T}=\bigcup_{t \in[0, T]} \bm{F}_t(\Set{X}_0)=\bigcup_{t \in[0, T]}\left\{\bm{F}_t\left(\bm{x}_{0}\right) \mid \bm{x}_{0} \in\Set{X}_0\right\}.
\]
\end{definition}

Less formally, one can think of the non-recurrent domain as the domain \emph{where flow does not intersect itself}.

Practically, non-recurrence is commonly ensured by a choice of the time interval $[0,T]$ so no periodicity is exhibited. Note that it does not mean the system's behavior is not allowed the be periodic, but our perception of it via data does. Effectively this prohibits the multi-valuedness of eigenfunctions -- allowing them to define an injective feature map.
    Thus, non-recurrence is a certain but general condition that bounds the time-horizon $T$ in which it is feasible to completely describe the nonlinear system's flow via an LTI predictor.
    
Note that our Assumption \ref{asm:Sspec} requires the existence of a nonrecurrent set that allows for a nonrecurernt domain.
It makes for a less-restrictive and intuitive condition compared to existing RKHS approaches \citep{Kostic2022LearningSpaces,Kostic2023KoopmanEigenvalues} that rely on the self-adjointness and compactness of the actual Koopman operator, which is rarely fulfilled for deterministic dynamics \eqref{eq:SSmodel} and hard to verify without prior knowledge.

\subsection{Koopman Mode Decomposition (KMD)}

As in the main text, when referring to \textit{Koopman Mode Decomposition} \eqref{eq:KoopObs}, we let the eigenfunctions absorb the spatial mode coefficients $\langle{ g^\prime_j,h}\rangle$ (possible w.l.o.g.) as they correspond to eigenfunctions $g_j$ and not eigenvalues $\eig_j$ \cite[Definition 9]{Budisic2012AppliedKoopmanism}.
\begin{lemma}[Universality of \eqref{eq:KoopObs}]\label{lem:universal}
    Consider a quantity of interest $h \in C(\Set{X})$, a forward-complete system flow $\bm{F}_{t}(\cdot)$ on a non-recurrent domain $\Set{X}$ (Definition \ref{def:Nonrec}) of a compact set $\Set{X}$. Then, the output trajectory ${y}(t) = {h}(\bm{x}(t)), \forall t \in [0,T]$ is arbitrarily closely described by the eigenpairs $\{\exp{\eig_j t},g_j\}_{j \in \Set{N}} {\subseteq} (\Set{C} \tsgn{\times} C(\Set{X}))$ of the Koopman operator semigroup $\{\operator{A}_t\}^{T}_{t\tsgn{=}0}$ so that $\forall \varepsilon > 0, \exists \bar{D} \in \Set{N}$
    \begin{equation}\label{modeDecom}
 |{h}(\bm{x}(t)) - \textstyle{\sum^{\bar{D}}_{j =1}}\operatorname{e}^{\eig_j t} g_j (\bm{x}\naught) | < \varepsilon, \forall t \in [0,T].
    \end{equation}
\end{lemma}
\begin{proof}
With continuous eigenfunctions for continuous systems proved valid in \cite[Lemma 5.1]{Mezic2020SpectrumGeometry},\cite[Theorem 1]{Korda2020OptimalControl}, the space of continuous functions over a compact set is naturally the space of interest. On a non-recurrent domain, there exist uniquely defined non-trivial eigenfunctions and, by \cite[Theorem 3.0.2]{Kuster2015TheSystems}, the spectrum is rich -- with any eigenvalue in the closed complex unit disk legitimate \citep{Ikeda2022KoopmanSpaces}. Further, by \cite[Theorem 2]{Korda2020OptimalControl}, this richness is inherited by the Koopman eigenfunctions --- making them universal approximators of continuous functions.
\end{proof}
\paragraph{Intuition on spectral sampling}
One may wonder if sampling spectra from a set enclosing the true spectrum may be enough to represent the spectral decomposition of the Koopman operator. Recalling that the spectral decomposition consists of projections to eigenspaces, we remark on a well-known result.
\begin{remark}\label{rmk:ChoiceOfMeasures}
    The choice of our measure of integration might seem arbitrary, and it indeed is. Since we, in general, do not assume knowledge of the spectrum of the Koopman-semigroup, we \textit{have to} make an approximation. To this end, an educated guess on where the (point-) spectrum might be located is helpful. As elaborated above, the Hille-Yosida-Theorem provides a convenient way to connect the practically attainable growth rates to bounds on the spectrum. 
    The Riesz projection operator $P_\eig: \raum{C}\mapsto \{g\in\raum{C}: \operator{A}g=\eig g\}$ to an eigenspace of $\operator{A}$ can be represented by 
    \[P_{\eig} = \frac{1}{2\pi i}\normalint_{\gamma_{\eig}} \frac{\d{s}}{s - \operator{A}},\]
    where $\gamma_{\eig}$ is a Jordan curve enclosing $\eig$ and no other point in $\sigma(\operator{A})$ \citep{Dunford1943SpectralProjections}. %
    Obviously $\bigcup_{\eig\in\sigma(\operator{A})} \operatorname{range}(P_{\eig})=\raum{C}$, iterating on the fact that we can represent the operator $T$ by its spectral components.
    It becomes apparent that sampling from a set enclosing $\sigma(\eig)$ can be seen as sampling curves, eventually enclosing sufficient spectral components. And as stated, one can choose arbitrary measures on $\Complex$ as long as one ensures they enclose the spectrum. 
\end{remark}

\section{REPRESENTATIONAL POWER OF KOOPMAN SPECTRAL KERNELS}\label{supl:repPWR}

When the Koopman operator is spectral, e.g., on a non-recurrent domain, the canonical representation of a Koopman operator acting on a well-specified observable ${h} \in \RKHS$ remains well-specified.
\begin{lemma}
  Denote by $[\mathsf{y}_t^{\txt{\tiny KE}}]_{\sim}$ the $L_2$ equivalence class of $\mathsf{y}_t^{\txt{\tiny KE}}$ and denote 
  \begin{align}
    \mathcal{A}^{[\tau_s,0]}_t = \sum^{\infty}_{j=1}\exp{\eig_j t} \mathcal{E}_{\eig_j}^{[\tau_s,0]}
\end{align}
  as the canonical spectral representation of a Koopman operator on the time-interval $[\tau_s,0]$. If $h_0 \in \RKHS$, there exists a kernel $k_y^{\txt{\tiny KE}}$, with integral operator $\mathcal{T}_{k_y^{\txt{\tiny KE}}}={ \mathcal{A}^{[\tau_s,0]}_t} \mathcal{T}_{k_x} {\mathcal{A}^{[\tau_s,0]}_t}^* $, s.t. for $\mathsf{h} \sim \mathcal{GP}\left(0, k_{x}\right), \mathsf{y}_t^{\txt{\tiny KE}} \sim \mathcal{GP}\left(0, k_y^{\txt{\tiny KE}}\right),[\mathsf{y}_t^{\txt{\tiny KE}}]_{\sim}$ has the same distribution as $ \mathcal{A}^{[\tau_s,0]}_t[\mathsf{h}]_{\sim}$.
  \begin{proof}
      Lemma 3.1 \citep{Wang2022}.
  \end{proof}
\end{lemma}

\begin{remark}
    Note  that the above holds for the \eqref{eq:SDK} when setting the individual equivariance operators to the identity so that  $\mathcal{A}^{[0,0]}_t = \sum^{\infty}_{j=1}\exp{\eig_j t}I_j$ 
\end{remark}

The above infinite sum may seem concerning. However, under mild conditions \ref{asm:Sspec}, there always exists a finite rank representation that is dense in the space of continuous function equipped with the supremum norm (universal). This is formalized in the following.

\begin{lemma}[Universality]
Let \ref{asm:Sspec} hold and consider a universal base kernel $k_x$ so that $\{k_{g_j}= k_x\}^D_{j=1}$. Then the induced kernels \eqref{eq:SDK} and \eqref{eq:KE-SDK} are universal, given a sufficiently rich spectral components $\{\eig_j\in \Set{C}\}^{D}_j $.
    \begin{proof}
       The universality of \eqref{eq:SDK} follows directly by \citet[Theorem 2]{Korda2020OptimalControl}. With a well-specified symmetrization by \ref{asm:Sspec}, the universality for functions satisfying Koopman-equivariance is inherited by applying \citet[Theorem 1 (ii)]{KKR_neurips2023} component-wise.
    \end{proof}
\end{lemma}

In the above lemma, the "sufficiently rich" can be understood as a set of eigenvalues that enclose the true spectrum. This is straightforwardly achieved by sampling eigenvalues from a distribution with support that encloses the true spectra \citep[Proposition 3]{KKR_neurips2023}.

\begin{remark}[Uncountable eigenpairs]\label{rem:InfKEIGS}
    Under Assumption \ref{asm:Sspec} any and all eigenvalues are legitimate, and, for each $\eig_j$, there are at least uncountably infinitely many eigenfunction-eigenvalue pairs \citep[Corollary 3]{Bollt2021GeometricRepresentation}.
\end{remark}

While na\"{i}vely, one would be tempted to optimize a large set of individual eigenvalues, this may be a highly ill-posed problem; as indicated by  Remark \ref{rem:InfKEIGS} and limits the optimization to a very few eigenvalues in practice \citep{Korda2020OptimalControl,caldarelli2024linear}. This is a key motivation in our likelihood optimization of the eigenvalue distribution with only a few degrees of freedom, allows us to efficiently \textit{choose the most likely set of eigenvalues amongst the infinite possibilities}.





















\section{PROOFS OF THEORETICAL RESULTS}\label{supl:Proof}

In the following, we restate the definition of Koopman equivariance for completeness.

\begin{definition}[Definition \ref{def:KEIGS} restated]
    Let $[\tau_s,\tau_e] \subset \Set{R}$ be a compact subset of the time axis and $\mathcal{M}$ a manifold. 
    A map $\phi_{\eig}: \mathcal{M} \mapsto \Set{C}^\mathcal{M}$ is called $ [\tau_s,\tau_e]_{\eig}$-{\em Koopman-equivariant} if 
    \begin{align}
        \phi_{\eig}\circ\bm{F}_t=\exp{\lambda t} \phi_\eig
    \end{align}
    on $\mathcal{M}$ for any $t \in [\tau_s,\tau_e]$.
\end{definition}

\subsection{Symmetrization Based on Koopman Equivariance}

\begin{theorem}[Theorem \ref{thm:Symm} restated \& expanded]
Consider the symmetrization operator $\KEop{[\tau_s,\tau_e]}{\eig}: L_{\mu}^2(\mathcal{X}) \rightarrow L_{\mu}^2(\mathcal{X})$ defined as
\begin{align}\label{eq:EqvarEOSupp}
    \KEop{[\tau_s,\tau_e]}{\eig} g := \mathbb{E}_{t \sim \mu{([\tau_s,\tau_e])}}\left[ \operatorname{e}^{-\lambda t} g\left(\bm{x}(t))\right)\right]
\end{align}
so that it is well-defined and self-adjoint. Then,
\begin{enumerate}[leftmargin=*,label=\roman*.]
\item \label{itm:iffKE} a function $f \in L_{\mu}^2(\mathcal{X})$ is $\KEqvar{[\tau_s,\tau_e]}\text{-equivariant}$ if and only if $\KEop{[\tau_s,\tau_e]}{\eig}[f] = f$, implying $\KEop{[\tau_s,\tau_e]}{\eig}$ is a projection operator so $\|\KEop{[\tau_s,\tau_e]}{\eig}\|=1$ if $L_{\mu}^2(\mathcal{X})$ contains any $\KEqvar{[\tau_s,\tau_e]}\text{-equivariant}$ functions ($\|\KEop{[\tau_s,\tau_e]}{\eig}\|=0$ otherwise); 
 \item \label{itm:Odecomp} $L_{\mu}^2(\mathcal{X})$ decomposes into symmetric and anti-symmetric part
$L_{\mu}^2(\mathcal{X})=\mathcal{S}_\eig \oplus \mathcal{S}_\eig^\perp$
where $\mathcal{S}_\eig=\left\{g \in L_{\mu}^2(\mathcal{X}): g\right.$ is  $\KEqvar{[\tau_s,\tau_e]}\text{-equivariant}\}$ and $\mathcal{S}_\eig^\perp=\{g \in L_{\mu}^2(\mathcal{X}):\KEop{[\tau_s,\tau_e]}{\eig}  g=0\}$;
\item \label{itm:Proj}
the symmetrization operator
$\KEop{[\tau_s,\tau_e]}{\eig}$ maps $g$ to the unique solution of
\begin{align}
    \phi_{\eig} = \argmin_{\psi \in \mathcal{S}_\eig} \| g - \psi \|^2_{\mu}.
\end{align}
\end{enumerate}
\begin{proof}
\ref{itm:iffKE} Lemma C.7 Corollary C.8 \citep{elesedy21a} 
\ref{itm:Odecomp},\ref{itm:Proj} Proposition 24 and 25 \citep{elesedy21a} and Proposition 3.9. \citep{Elesedy2023}
\end{proof}
\end{theorem}








\subsection{New Information Gain Rates}
\paragraph{Technical Lemmas}
As our technical results rely on a spectral representation of the base hypothesis space, we state the following technical lemma on Mercer representations considered in this work.

\begin{lemma}[Mercer representation]\label{lem:MercerApp}
Let $\RKHS$ be any RKHS with kernel $k_x$ s.t.~$
\normalint P(d\bm{x}) k_x(\bm{x},\bm{x}) < \infty.
$
Then 
\begin{enumerate}[leftmargin=*,label=\roman*.]
   \item $\RKHS$ can be embedded into $L_2(P(d\bm{x}))$, and 
   the natural inclusion operator $\iota_x:\RKHS\to L_2(P(d\bm{x}))$ and $\iota_x^\top$ are Hilbert-Schmidt; the map $\operator{T}_{k_x}: h\mapsto \normalint P(d\bm{x}) k_x(\bm{x},\cdot) h(\bm{x})$ defines a positive, self-adjoint and trace-class operator; $\operator{T}_{k_x} = \iota_x\iota_x^\top$. 
   \item $\operator{T}_{k_x}$ has the decomposition $$
\operator{T}_{k_x} h = \sum_{i\in I} \mu_i \langle\bar{\varphi}_i, h \rangle_2\bar{\varphi}_i,
$$
where the index set $I\subset \Set{N}$ is at most countable, and $\{\bar e_i\}$ is an orthonormal system in $L_2(P(d\bm{x}))$. 
\item There exists an orthogonal system $\{e_i: i\in I\}$ of $\RKHS$ s.t.~$[e_i]_\sim = \sqrt{\lambda_i}\bar{\varphi}_i$. 
\item If $k_x$ is additionally bounded and continuous, 
$\{e_i: i\in I\}$ will %
define a Mercer's representation whose convergence is absolute and uniform. %
\end{enumerate}
\begin{proof}
\cite[Lemma 2.3, 2.2 (for i), %
       2.12 (for ii-iii), Corollary 3.5 %
       (for iv)]{Steinwart2012MercersTO}.
\end{proof}
\end{lemma}

We state the following technical Lemma that will help prove a result on information gain rates.

\begin{lemma}[\citep{Bhatia1997}]\label{lem:CompDecay}
    Let $\operator{A},\operator{B}$ be any two operators, $\|\cdot\|$ denote the operator norm and $s_j$ the $j$-th largest singular value. Then
    $$
    s_j(\operator{A}\operator{B}) \leq \min\{\|\operator{B}\|s_j(\operator{A}),\|\operator{A}\|s_j(\operator{B})\}
    $$
\end{lemma}

The above results will be used to bound the eigenvalue decay i.e. 
$$
\eig_j(\operator{A}\operator{B}\operator{B}^*\operator{A}^*)=s_j(\operator{A}\operator{B})^2.
$$ 

\begin{theorem}[Theorem \ref{th:infogain_asymp} restated]
Consider the Mercer eigenvalues $\{\mu_j\}^{\infty}_{j=1}$ for $k_x$ and let Assumptions \ref{asm:Hreg},\ref{asm:Sspec} and \ref{asm:Areg} hold. Then $\exists \theta \geq 1$ for\vspace{-1em}
\begin{description}[style=multiline, leftmargin=3em,font=\normalfont]
    \item[\namedlabel{case:PD2}{(\txt{Poly})}] $$\eig_j(A_1)\lesssim j^{-p} \wedge \mu_j\lesssim j^{-a}, a>1$$ or
    \item[\namedlabel{case:ED2}{(\txt{Exp})}]  $$\eig_j(A_1)\lesssim \exp{-j^p} \wedge~\mu_j \lesssim  \exp{-j^b},b>0$$ so that
\end{description}
\[
    \gamma^{\sigma}_N\left(k^{\txt{KE}}_{y}\right)  \in
    \tilde{\mathcal{O}}(\left(\gamma^{\sigma}_N(k_{x})\right)^{\nicefrac{1}{\theta}})
\]
where $\theta=\frac{\max\{2p,a\}}{a}$ \ref{case:PD2} and $\theta=\frac{\max\{p,b\}}{b}$ \ref{case:ED2}.
\begin{proof}
We prove the results considering the following two eigendecay profiles:
    \begin{description}[style=multiline, leftmargin=3em,font=\normalfont]
    \item[\ref{case:PD2}{(\txt{Poly})}] 
First, based on the information gain results from \cite[Corollary 1]{pmlr-v130-vakili21a}, we can use a simplified (free of constants) information gain 
\begin{align}\label{eq:polyIGsimple}
\gamma^{\sigma}_N\left(k_{x}\right) \in \mathcal{O}\left( N^{\frac{1}{a}} {(\log{N})}^{1-\frac{1}{a}} \right)
\end{align}
for the base kernel $k_x$ with a polynomial decay of Mercer eigenvalues
\begin{align}
    \mu_j\lesssim j^{-a}, a>1.
\end{align}
Secondly,  by boundedness of $A_1$, we have an $A_1$-induced decay $\eig_j(A_1 \mathcal{T}_{k_x}A^*_1)\lesssim j^{-a^\prime}$ for some $a'$. This allows us to define a ratio between them and the native decay rate of Mercer eigenvalues $\theta:=\frac{a^\prime}{a} > 0$.  To uncover the information gain differences, we
\begin{subequations}
can equivalently define $a^\prime = \theta a$, such that expressing everything in terms of $\theta$ and $a$ we get  
    \begin{align}
\gamma^{\sigma}_N\left(k^{\txt{KE}}_{y}\right) 
& \in {\mathcal{O}} \left({N}^{\frac{1}{a\theta}}(\log N)^{(1
-\frac{1}{a\theta})}\right) \\
& \in {\mathcal{O}} \left({N}^{\frac{1}{a}(\frac{1}{\theta})}(\log N)^{\left(\frac{1}{\theta}+(1-\frac{1}{\theta})
-\frac{1}{a \theta}\right)}\right) \\
& \in {\mathcal{O}} \left({N}^{\frac{1}{a}(\frac{1}{\theta})}(\log N)^{(1
-\frac{1}{a})(\frac{1}{\theta})}(\log N)^{1-\frac{1}{\theta}}\right) \\
& \in {\mathcal{O}} \left(\left({N}^{\frac{1}{a}}(\log N)^{(1
-\frac{1}{a})}\right)^\frac{1}{\theta} (\log N)^{1-\frac{1}{\theta}}\right)\\
& \in \gamma^{\sigma}_N\left(k_{x}\right)^\frac{1}{\theta}{\mathcal{O}} \left( (\log N)^{1-\frac{1}{\theta}}\right) \\
& \in \tilde{\mathcal{O}} \left(\left(\gamma^{\sigma}_N\left(k_{x}\right)\right)^\frac{1}{\theta}\right).
\end{align}
\end{subequations}
Using $\eig_j(A_1)\lesssim j^{-p}$ and $\eig_j(\iota_x\iota_x^*)\overset{\text{Lem. \ref{lem:MercerApp}}}{\equiv}\eig_j\left(\operator{T}_{k_x}\right):=\mu_j\lesssim j^{-a}, a>1$  and invoking Lemma \ref{lem:CompDecay}, we obtain 
\begin{align}
\eig_j\left({{A}^{}_1} \iota_x\iota_x^* {{A}^{*}_1}\right) = \left(s_j\left({{A}^{}_1} \iota_x\right)\right)^2 &  \leq  \left(\min\{\|\iota_x\|s_j(A_1),\|A_1\|s_j(\iota_x)\}\right)^2\\
&  \lesssim \left(\min\{s_j(A_1),s_j(\iota_x)\}\right)^2 \\
&  \lesssim \left(\min\left\{j^{-{p}},\sqrt{j^{-{a}}}\right\}\right)^2 \\
&  \lesssim \left(j^{-{\max\{p,{\frac{a}{2}}\}}}\right)^2\\
     & \lesssim  {j^{-\max\{2p,a\}}}
\end{align}
leading to
\begin{align}
    \gamma^{\sigma}_N\left(k^{\txt{KE}}_{y}\right) & \in \tilde{\mathcal{O}} \left(\gamma^{\sigma}_N\left(k_{x}\right)^\frac{a}{\max\{2p,a\}}\right).
\end{align}
where identifying $\theta := \frac{\max\{2p,a\}}{a} \geq 1$ proves the \ref{case:PD2} part of the result.
    \item[\ref{case:ED2}{(\txt{Exp})}]  
First, based on the information gain results from \cite[Corollary 1]{pmlr-v130-vakili21a}, we can use a simplified (free of constants) information gain 
\begin{align}\label{eq:expIGsimple}
\gamma^{\sigma}_N\left(k_{x}\right) \in \mathcal{O}\left({(\log{N})}^{1+\frac{1}{b}} \right)
\end{align}
for the base kernel $k_x$ with an exponential decay of Mercer eigenvalues
\begin{align}
    \mu_j \lesssim \exp{{-{j}^{b}}},b>0.
\end{align}
Secondly, by boundedness of $A_1$, we have an $A_1$-induced decay $\eig_j(A_1 \mathcal{T}_{k_x}A^*_1)\lesssim \exp{{-{j}^{b^\prime}}}$. This allows us to define a ratio between them and the native decay rate of Mercer eigenvalues $\theta:=\frac{b^\prime}{b} > 0$.  To uncover the information gain differences,
\begin{subequations}
we can equivalently define $b^\prime = \theta b$, such that expressing everything in terms of $\theta$ and $b$ we get 
    \begin{align}
\gamma^{\sigma}_N\left(k^{\txt{KE}}_{y}\right) 
& \in {\mathcal{O}} \left((\log N)^{(1
+\frac{1}{b \theta})}\right) \\ 
& \in {\mathcal{O}} \left((\log N)^{(\frac{1}{\theta}+(1-\frac{1}{\theta})
+\frac{1}{b \theta})}\right)  \\
& \in {\mathcal{O}} \left((\log N)^{(1
+\frac{1}{b})\frac{1}{\theta}} (\log N)^{1-\frac{1}{\theta}}\right)\\
& \in {\mathcal{O}} \left(\left((\log N)^{(1
+\frac{1}{b})}\right)^\frac{1}{\theta} (\log N)^{1-\frac{1}{\theta}}\right)\\
& \in {\mathcal{O}} \left((\log N)^{(1
+\frac{1}{b})}\right)^\frac{1}{\theta} {\mathcal{O}} (\log N)^{1-\frac{1}{\theta}}\\
& \in \gamma^{\sigma}_N\left(k_{x}\right)^\frac{1}{\theta} {\mathcal{O}} (\log N)^{1-\frac{1}{\theta}}\label{eq:ExpFullIG}\\
& \in \tilde{\mathcal{O}} \left(\gamma^{\sigma}_N\left(k_{x}\right)^\frac{1}{\theta}\right).
\end{align}
\end{subequations}
Using $\eig_j(A_1)\lesssim \exp{{-{j}^{p}}}$ and $\eig_j(\iota_x\iota_x^*)\overset{\text{Lem. \ref{lem:MercerApp}}}{\equiv}\eig_j\left(\operator{T}_{k_x}\right):=\mu_j\lesssim \exp{{-{j}^{b}}}, b>0$  and invoking Lemma \ref{lem:CompDecay}, we obtain 
\begin{align}
\eig_j\left({{A}^{}_1} \iota_x\iota_x^* {{A}^{*}_1}\right) = \left(s_j\left({{A}^{}_1} \iota_x\right)\right)^2 &  \leq  \left(\min\{\|\iota_x\|s_j(A_1),\|A_1\|s_j(\iota_x)\}\right)^2\\
&  \lesssim \left(\min\{s_j(A_1),s_j(\iota_x)\}\right)^2 \\
&  \lesssim \left(\exp{-{\max\{j^p,\frac{1}{2}j^{{b}}\}}}\right)^2 \\
&  \lesssim \exp{-{\max\{2j^p,j^{{b}}\}}} \\
     & \lesssim  \exp{-j^{\max\{p,b\}}}
\end{align}
leading to
\begin{align}
    \gamma^{\sigma}_N\left(k^{\txt{KE}}_{y}\right) &  \in \tilde{\mathcal{O}} \left(\gamma^{\sigma}_N\left(k_{x}\right)^\frac{b}{\max\{p,b\}}\right).
\end{align}
where identifying $\theta := \frac{\max\{p,b\}}{b} \geq 1$ proves the \ref{case:ED2} part of the result, finishing the proof.
\end{description}
\end{proof}
\end{theorem}


\begin{remark}[Base SE kernel in \eqref{eq:KE-SDK}]
    By plugging in the known input-dimension dependent information gain for the SE kernel in \eqref{eq:ExpFullIG}, we see that the decay speed-up  $\theta > 1 \equiv 2p > b$ can counteract the curse of input dimensionality, leading to to 
    $$
    \gamma^{\sigma}_N\left(k^{\txt{KE}}_{y}\right) \in {\mathcal{O}} \left((\log N)^{\frac{n}{\theta}+1}\right).
    $$
\end{remark}

\paragraph{Intuition on time-independent $A_1$} 
We defined the operator $A_1$ in \ref{asm:Areg} as time-independent for ease of exposition; the family of $\{A_t\}^T_{t=0}$ is uniquely defined by an infinitesimal generator.
This is due to the confinement to a non-recurrent domain \ref{asm:Sspec} that prescribes finite frequencies of oscillation and finite growth rates, allowing for the infinitesimal generator bounded \citep{Zeng2023AOperator}.
In practice, we can always take the worst-case time-exponent for analysis by scaling the time appropriately.












\section{DETAILS ON NUMERICAL EXPERIMENTS}\label{supl:NumExp}

\subsection{Implementation Details}
We implemented GP regression using \texttt{GPJax}~\citep{Pinder2022}. All of the experiments were performed on machines with {2TB} of RAM, 8 NVIDIA Tesla P100 16GB GPUs and 4 AMD EPYC 7542 CPUs.

\subsubsection{Spectral Hyperprior}\label{ssec:SpecPrio}
To tractably optimize over a spectral prior, we use the noise transfer (outsourcing) trick by \cite[Theorem 5.10]{kallenberg1997foundations} to model the eigenvalue distribution $p(\lambda) \approx \rho_{}(\bm{\vartheta})$.
This choice limits the number of required parameters since $\bm{\vartheta}$ has fewer parameters (degrees of freedom) than the number of eigenspaces $\|\bm{\vartheta}\|_0 \ll |D|$. Furthermore, it allows for the use of log-likelihood maximization just like with any other set of hyperparameters. We use a uniform distribution on $\{\lambda_j=s_j+\mathrm{i}\omega_j: s\in [-\vartheta_{s}, \vartheta_{s}]+\vartheta_{\overline{s}},~\omega\in[-\vartheta_{\omega}, \vartheta_{\omega}]+\vartheta_{\overline{\omega}}\}$,~$\bm{\vartheta}{=}[\vartheta_s,\vartheta_{\overline{s}},\vartheta_{\omega},\vartheta_{\overline{\omega}}]^\top$. To obtain equivariant features, we compute the expectation~\eqref{eq:EqvarEOSupp} wrt. a uniform underlying distribution in time, which in case of equally spaced points in time leads to a trapezoid rule.
\subsubsection{Preprocessing and Initialization}
We standardize all data trajectories such that the target has to have zero mean and unit variance and the forecast time is between zero and one. This allows us to choose similar parameters for all datasets. We initialize the generative parameters as follows for KE-GP and C-GP:
\begin{enumerate}[leftmargin=*,label=\txt{init}.\roman*)]
    \item \textit{Prior mean}: $\mu(\bm{x})=0$ and keep it fixed;
    \item \textit{Lengthscale}: $\frac{\sqrt{n_x}}{2}\operatorname{std} (\Set{X}_{\text{input}})$ following \cite{pmlr-vanillaBOgreat-hvarfner24a};
    \item \textit{Signal variance}: $\sigma_{s}^2=1$ and fix it, since the data was standardized. Is advised based on the results of \cite{pmlr-vanillaBOgreat-hvarfner24a};
    \item \textit{Observation noise variance}: $\sigma_{\txt{on}}^2=1$;
    \item\label{itm:SpecPrio} \textit{Spectral prior}:  $\vartheta_{{\omega}}=15, \vartheta_{\overline{\omega}}=0$, $\vartheta_{s}=1$, $\vartheta_{\overline{s}}=0$ for scale and bias parameters of the uniform distribution.
    \item \textit{Inducing trajectories}: $N{=}32$ for exact GP and variational inference, sampled form the train split.
\end{enumerate}

\subsubsection{Variational Inference}
As variational inference is notoriously sensitive to initial guesses, we employ a scheme to robustly optimize the generative and variational parameters.
To this end we first get reasonable guesses as described above, sample a set of inducing points $(\tilde{Z}, \tilde{Y})$ from the training data and train an exact GP on the inducing inputs via marginal log-likelihood. This yields a set of generative parameters, in particular parameters for the spectral distribution that fit the data. The so obtained posterior is used to initialize the variational GP: $m = \overline{\mu}_{\text{MLL}}(\tilde{Z})$ and $S = \overline{\sigma}_{\text{MLL}}(\tilde{Z}, \tilde{Z})$. This means the initial VI model is the exact posterior on inducing inputs, making the optimization easier, as the initial gradients are smaller. To optimize the variational GP we follow~\cite{Toth20a} and start by first optimizing the variational parameters only, then optimize all parameters jointly and finally optimize the variational parameters on the joint training and validation dataset.


Due to the structure of our Koopman-equivariant construction, and the resulting benefits in information gain presented in Section \ref{section:analysisofsamplecomplexity}, we found KE-GPs more robust to a lack of correlation between points than C-GP, an issue commonly observed in conventional sparse GP approximations \citep{Murray2010,HensmanNIPS2015}. In particular, a lower information gain implies that less inducing points are required than with conventional GPs to accurately represent the full posterior \citep{Burt2019RatesRegression}.%

\paragraph{C-GP}
For the contextual GP \citep{Li2024STkernel} with covariance $k^{\txt{SE}}(t,t^\prime)\otimes k^{\txt{SE}}(\bm{x}_0,\bm{x}^\prime_i)$, we perform the same type of varionational inference as described above, but consider inducing points $\bm{z}=[\bm{x}_0^\top,t^\top]^\top$.

\subsection{Benchmark Dynamics}
We perform our quantitative study on the following examples of varying complexity. 
From the robotics domain, we consider expert demonstrations from D4RL~\citep{fu2020d4rl} from the \txt{\small halfcheetah} environment and forecast the first state and action. We take temperature data from the {Monash TSF} benchmark~\citep{godahewa2021monash} as a sample for highly complex weather dynamics. Since the latter datasets provide a single long trajectory, we split off the last chunk as test data and partition the trajectory into \#$N$ dataparis pairs to comply with \eqref{eq:dataTraj}.

\paragraph{Predator-Prey ODE} We use the predator-prey model:
\begin{equation}\label{eq:volterra_lotka}
    \dot{x}_{1} = r_{1}  x_{1} + c_{i}  \gamma_{1} x_{1}  x_{1}
,\quad
\dot{x}_{2} = r_{2}  x_{2} + c_{i} \gamma_{2} x_{1}  x_{2}
.
\end{equation}%
where $r_1,r_2$, $\gamma$, $c_1,c_2$ are reproduction rates, interaction effects and frequency, respectively. We choose parameters $r_1{=}0.2$, $\gamma_1{=}0.4$, $r_2{=}0.25$, $\gamma_2{=}0.2$, $c_i=2$. We create a dataset by simulating the system for $H=64$ steps with $\Delta t=3$ for $N=1024$ trajectories from initial conditions in $[0, 2]\times [0, 1]$.

\paragraph{Linear ODE}
To validate the information gain results on a simple example we use a 2-dimensional linear system $\dot{x_1}=-6 x_2, \dot{x_2}=6x_1$. We create a dataset by simulating the system for $H=16$ steps with $\Delta t=0.06$ for $N=1000$ trajectories from initial conditions in the unit box. The eigenvalues of this system are $\lambda_{1,2}=\pm 6j$. Which we use to compare a randomly sampled enclosing vs. a matching spectral distribution.

\paragraph{D4RL}
 The Datasets for Deep Data-Driven Reinforcement Learning (D4RL) \citep{fu2020d4rl} provides a trajectory collection of reinforcement learning agents interacting with the environments defined in the OpenAI gym  \citep{oAIgym}. We pick the environment \txt{\small halfcheetah} with the expert policy. For the \txt{\small halfcheetah} we use all actions and observations as our state, demonstrating that our method works in a high dimensional input space. The task for the \txt{\small halfcheetah} is to forecast the first action. Input trajectory have 16 steps, the target is to be forecast for 16 steps.
 
\paragraph{Oikolab Temparature}
As a final benchmark, we draw on the \emph{monash\_tsf} dataset collection \citep{godahewa2021monash} and choose their OIKOLAB weather station dataset to test our KE-GP method on the task temperature forecasting. To this end, we provide the models with state trajectories consisting of 8 quantities, including temperature for the past 32 hours and set the task as forecasting the temperature for 16 hours.

\section{ADDITIONAL EXPERIMENTS}\label{supl:AddExp}

\subsection{Covariance Visualisation vs C-GP}\label{sec:covvis}
We compare initial and marginal log-likelihood optimized covariances for our KE-GP and C-GP. The spatial covariance corresponds to that of a trajectory. The temporal covariance is separately displayed for the same trajectory. As Figure \ref{fig:illustrative} shows, the learned covariances for our KE-GP and C-GP are of similar shape, the initial spatial covariance of KEGP is less local than that of MTGP, already encoding the trajectory structure before hyperparameter optimization. Further, the KEGP temporal covariance delivers a considerably simple forecasting model, as it is the superposition of multiple one-dimensional LTI systems \eqref{eq:KoopObs}, as opposed to a spatially and temporally nonlinear covariance of C-GP. 

Notably, since the spectral distribution is a parametrized uniform distribution, KE-GP has only seven parameters, whereas C-GP has 100.
\input{plots/space_time_cov_kegp_cgp}
\subsection{Ablation Studies}
\begin{figure}
    \centering
    \begin{tabular}{ccc}
         \input{plots/ablation_eigenspaces.tikz}&\input{plots/ablation_past.tikz}
         & \input{plots/ablation_future.tikz}
    \end{tabular}
    \caption{Ablation Studies: test RMSE with varying data and spectral parameters; we report mean and interquartile range over 10 runs.}
    \label{fig:abationES}
\end{figure}
As the KE-GP model performance depends on parameters that are absent in naive models, we perform ablation studies to shed light on the role of the new components. In particular, we take a closer look at the effect of the number of modes, the effect of varying length input trajectories, and the effect of varying length forecast time. To this end, we take the predator-prey dynamics \eqref{eq:volterra_lotka}, and 
build exact KE-GP models on $N_{\text{train}}=32$ trajectories and evaluate the model on $N_{\text{test}}=256$ validation trajectories. We do not optimize the base kernel $k_{g_j}$ hyperparameters, i.e., to enable a consistent comparison between models. 

\textbf{Eigenspace Dimensionality} As displayed in Figure~\ref{fig:abationES} (left), the performance improves significantly when increasing the number of eigenspaces sampled up to about $D=64$; by increasing $D$ further, the performance increase saturates. This behavior resembles the increase in resolution we gain by increasing the number of eigenspaces sampled. A reasonable $D$ is dependent on the quality of the learned hyperprior (Subsection \ref{ssec:SpecPrio}), which in turn depends on the dynamical system at hand. Crucially, there is no loss of performance when there are too many spectral elements.

\textbf{Input Trajectory Length} Figure~\ref{fig:abationES}~(middle) shows that a longer input trajectory leads to the equivariance operator $\mathcal{E}$ introducing more information about the dynamics into the prior covariance. As prescribed by representation theory of Section \ref{supl:repPWR} and discussed in~\ref{sec:covvis}, this leads to a better prior even without the need for optimization.

\textbf{Target Trajectory Length} As displayed in Figure~\ref{fig:abationES}~(right), the forecasting problem gets progressively harder as we want to forecast for a longer time, the RMSE increases at a rate of approximately $\sqrt{\txt{steps}}$, which is expected for RMSE.%

\newpage
\bibliography{References}





