\section{Related Work}
\label{sec:related-work}
% \vspace{-0.2cm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Our work aims to unify two streams of work, namely, representation learning methods and Neyman-orthogonal learners. We briefly review both in the following (a detailed overview is in Appendix~\ref{app:extended-rw}).


\textbf{Representation learning for estimating causal quantities.} Several methods have been previously introduced for \emph{end-to-end} representation learning of CAPOs/CATE  \citep[see, in particular, the seminal works by][]{johansson2016learning,shalit2017estimating,johansson2022generalization}. A large number of works later suggested different extensions to these.
Existing methods fall into three main streams: (1)~One can fit an \emph{unconstrained shared representation} to directly estimate both potential outcomes surfaces \citep[e.g., \textbf{TARNet};][]{shalit2017estimating}. (2)~Some methods additionally enforce a \emph{balancing constraint} based on empirical probability metrics, so that the distributions of the treated and untreated representations become similar \citep[e.g., \textbf{CFR} and \textbf{BNN};][]{johansson2016learning,shalit2017estimating}. Importantly, balancing based on empirical probability metrics is only guaranteed to perform a consistent estimation for \emph{invertible} representations since, otherwise, balancing leads to a \emph{representation-induced confounding bias} (RICB) \citep{johansson2019support,melnychuk2024bounds}. (3)~One can additionally perform \emph{balancing by re-weighting} the loss and the distributions of the representations with learnable weights \citep[e.g., \textbf{RCFR};][]{johansson2022generalization}. We later adopt the representation learning methods from (1)--(3) as baselines. 

\textbf{Neyman-orthogonal learners}. Causal quantities can be estimated using model-agnostic methods, so-called \emph{meta-learners} \citep{kunzel2019metalearners}. Prominent examples are the R-learner \citep{nie2021quasi} and DR-learner \citep{kennedy2023towards,curth2020estimating}. %Meta-learners typically combine multiple models to perform two-stage learning, namely, (1)~nuisance functions estimation and (2)~target model fitting. As such, meta-learners must be instantiated with some machine learning model to perform (1) and (2). 
Meta-learners are model-agnostic in the sense that any base model (e.g., neural network) can be used for estimation. Also, meta-learners have several practical advantages \citep{morzywolek2023general}: (i)~they oftentimes offer favorable theoretical guarantees such as Neyman-orthogonality \citep{chernozhukov2017double,foster2023orthogonal}; (ii)~they can address the causal inductive bias that the CATE is ``simpler'' than CAPOs \citep{curth2021inductive}, and (iii)~the target model obtains a clear interpretation as a projection of the ground-truth CAPOs/CATE on the target model class. {\citet{curth2021nonparametric,frauen2025modelagnostic} provided a comparison of meta-learners implemented via neural networks with different representations, yet with the target model based on the original covariates (the representations were only used as an interim tool to estimate nuisance functions). In contrast, in our work, we study the learned representations as primary inputs to the target model.}

\textbf{Research gap.} Our work is the first to unify representation learning methods and Neyman-orthogonal learners. As a result, one can combine any representation learning method from above with our \ORlearners, which then (i)~offer favorable properties of Neyman-orthogonality and (ii)~address the causal inductive bias that CATE is ``simpler'' than CAPOs. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \vspace{-0.2cm}