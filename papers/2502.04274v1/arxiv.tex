\documentclass[dvipsnames]{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2023} with \usepackage[nohyperref]{icml2023} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2025}

\usepackage{url}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{enumitem}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{multirow,makecell}
\PassOptionsToPackage{dvipsnames}{xcolor}
\usepackage{placeins}
\usepackage{bbm}
\usepackage{array}
\usepackage{stackrel}
% \usepackage{algpseudocode}
% \usepackage{algorithm}
\usepackage{colortbl}
\usepackage{tabu}
\usepackage{enumitem}
% \usepackage{showframe}
\usepackage{caption}
\usepackage{wrapfig}
\usepackage{fancybox}

\usepackage{xspace}
\newcommand\ie{i.\,e.\xspace}
\newcommand\eg{e.\,g.\xspace}
\newcommand\Eg{E.\,g.\xspace}
\newcommand\NB{N.\,B.\xspace}
\newcommand\BSc{B.\,Sc.\xspace}
\newcommand\MSc{M.\,Sc.\xspace}
\newcommand\PhD{Ph.\,D.\xspace}
\newcommand\etc{etc.\xspace}
\newcommand\resp{resp.\xspace}
\newcommand\cf{cf.\xspace}
\newcommand\Cf{Cf.\xspace}
\newcommand\cp{cp.\xspace}
\newcommand\etal{et\,al.\xspace}
\newcommand\page[1]{p.\,#1}
\newcommand\pages[1]{pp.\,#1}
\newcommand\pa{p.\,a.\xspace}
\newcommand\ham{a.\,m.\xspace}
\newcommand\hpm{p.\,m.\xspace}
\newcommand\UK{U.\,K.\xspace}
\newcommand\US{U.\,S.\xspace}
\newcommand\wlogenerality{w.\,l.\,o.\,g.\xspace}

% Attempt to make hyperref and algorithmic work together better:
% \newcommand{\theHalgorithm}{\arabic{algorithm}}
\newcommand{\ind}{\perp\!\!\!\perp} 
\newcommand{\notind}{\not\!\perp\!\!\!\perp} 
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand*\diff{\mathop{}\!\mathrm{d}}

\newcommand{\mathup}[1]{\mathrm{#1}}
\newcommand{\lagop}{\mathbf{\mathup{L}}}
\newcommand{\vecval}[1]{\mathbf{#1}}
\newcommand{\concatvec}[1]{\left[#1\right]}
\newcommand\transpose{^T} %{^\top}
\newcommand{\e}[1]{\mathup{e}^{#1}}

\newcommand{\sspace}{\quad}
\newcommand{\wspace}{\qquad}	
\newcommand{\sand}{\sspace\text{and}\sspace}
\newcommand{\wand}{\wspace\text{and}\wspace}
\newcommand{\for}{\text{for }}
\newcommand\op[1]{\mathrm{#1}}  % Operator	
%  \newcommand\define{:=} % :=,
\newcommand\define{\ensuremath{\mathrel{\stackrel{\mathrm{def}}{=}}}}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand\set[1]{\left\{#1\right\}}
\newcommand\Var{\op{Var}}
%  \newcommand{\upDelta}{\mathrm{\upDelta}}
\newcommand{\var}[1]{\mathrm{#1}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\vecsize}[1]{\mathbb{R}^{#1}}
\newcommand{\dd}{\mathop{}\!\mathrm{d}}

\usepackage{scalerel,stackengine,amsmath}
\newcommand\equalhat{\mathrel{\stackon[1.5pt]{=}{\stretchto{%
    \scalerel*[\widthof{=}]{\wedge}{\rule{1ex}{3ex}}}{0.5ex}}}}

\usepackage{pifont}
\newcommand{\cmark}{\textcolor{ForestGreen}{\ding{51}}}%
\newcommand{\xmark}{\textcolor{BrickRed}{\ding{55}}}%

\newcommand\underrel[2]{\mathrel{\mathop{#2}\limits_{#1}}}

\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\newcommand\TODO[1]{\textcolor{red}{#1}}
% \newcommand{\REBUTTAL}[1]{{\leavevmode\color{blue}{#1}}}
% \newcommand{\REBUTTALnew}[1]{{\leavevmode\color{red}{#1}}}

\interfootnotelinepenalty=10000 

\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{cor}{Corollary}
\newtheorem{prop}{Proposition}
\newtheorem{remark}{Remark}


\newtheorem{innercustomprop}{Proposition}
\newenvironment{numprop}[1]
  {\renewcommand\theinnercustomprop{#1}\innercustomprop}
  {\endinnercustomprop}

\newtheorem{innercustomremark}{Remark}
\newenvironment{numremark}[1]
  {\renewcommand\theinnercustomremark{#1}\innercustomremark}
  {\endinnercustomremark}

\newtheorem{innercustomcorollary}{Corollary}
\newenvironment{numcorollary}[1]
  {\renewcommand\theinnercustomcorollary{#1}\innercustomcorollary}
  {\endinnercustomcorollary}

\newtheorem{innercustomlemma}{Lemma}
\newenvironment{numlemma}[1]
  {\renewcommand\theinnercustomlemma{#1}\innercustomlemma}
  {\endinnercustomlemma}

\usepackage{times,tcolorbox}


\usepackage{pifont}
\usepackage{tikz}
\usepackage{graphicx}
\newcommand{\greencheck}{\textcolor{ForestGreen}{\checkmark}}
\newcommand{\redcross}{\textcolor{BrickRed}{\ding{55}}}
\newcommand*\circledgreen[1]{%
\tikz[baseline=(char.base)]{
  \node[shape=circle, draw=ForestGreen!60, fill=ForestGreen!10, thick, inner sep=1pt] (char) {\scriptsize\textsf{#1}};
}}
\newcommand*\circledred[1]{%
\tikz[baseline=(char.base)]{
  \node[shape=circle, draw=BrickRed!60, fill=BrickRed!10, thick, inner sep=1pt] (char) {\scriptsize\textsf{#1}};
}}
\newcommand*\circled[1]{%
\tikz[baseline=(char.base)]{
  \node[shape=circle, draw=NavyBlue!60, fill=NavyBlue!10, thick, inner sep=1pt] (char) {\scriptsize\textsf{#1}};
}}

\definecolor{tabblue}{HTML}{2077B4}
\definecolor{tabred}{HTML}{FF0000}
\definecolor{yellow}{HTML}{FFFF88}

\newtcbox{\myovalbox}{colback=yellow,boxrule=0.1pt,arc=3pt,
  boxsep=0pt,left=0.5pt,right=0.5pt,top=0.5pt,bottom=0.5pt,}

\newlength\myindent
\setlength\myindent{2em}
\newcommand\bindent{%
  \begingroup
  \setlength{\itemindent}{\myindent}
  \addtolength{\algorithmicindent}{\myindent}
}
\newcommand\eindent{\endgroup}

\newcommand{\ORlearners}{\emph{OR-learners}\xspace}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Orthogonal Representation Learning for Estimating Causal Quantities}

\begin{document}

\twocolumn[
\icmltitle{Orthogonal Representation Learning for Estimating Causal Quantities}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2023
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
    \icmlauthor{Valentyn Melnychuk}{lmu}
    \icmlauthor{Dennis Frauen}{lmu}
    \icmlauthor{Jonas Schweisthal}{lmu}
    \icmlauthor{Stefan Feuerriegel}{lmu}
\end{icmlauthorlist}

\icmlaffiliation{lmu}{LMU Munich \& Munich Center for Machine Learning (MCML), Munich, Germany}

\icmlcorrespondingauthor{Valentyn Melnychuk}{melnychuk@lmu.de}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{causal inference, treatment effect estimation}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
    Representation learning is widely used for estimating causal quantities (e.g., the conditional average treatment effect) from observational data. While existing representation learning methods have the benefit of allowing for end-to-end learning, they do not have favorable theoretical properties of Neyman-orthogonal learners, such as double robustness and quasi-oracle efficiency. Also, such representation learning methods often employ additional constraints, like balancing, which may even lead to inconsistent estimation. In this paper, we propose a novel class of Neyman-orthogonal learners for causal quantities defined at the representation level, which we call OR-learners. Our OR-learners have several practical advantages: they allow for consistent estimation of causal quantities based on any learned representation, while offering favorable theoretical properties including double robustness and quasi-oracle efficiency. In multiple experiments, we show that, under certain regularity conditions, our OR-learners improve existing representation learning methods and achieve state-of-the-art performance. To the best of our knowledge, our OR-learners are the first work to offer a unified framework of representation learning methods and Neyman-orthogonal learners for causal quantities estimation. 
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \vspace{-0.2cm}
\section{Introduction} 
% \vspace{-0.2cm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Estimating causal quantities has many applications in medicine \citep{feuerriegel2024causal}, policy-making \citep{kuzmanovic2024causal}, marketing \citep{varian2016causal}, and economics \citep{basu2011estimating}. Here, different causal quantities are of interest such as the conditional average treatment effect (CATE) and the conditional average potential outcomes (CAPOs). For example, in personalized medicine, CATE estimation can help in predicting the relative benefits of different treatment options, so that the treatment with the best health outcome is selected. 


Recently, representation learning methods have gained wide popularity in estimating causal quantities from observational data \citep[e.g.,][]{johansson2016learning,shalit2017estimating,hassanpour2019counterfactual,hassanpour2019learning,zhang2020learning,assaad2021counterfactual,johansson2022generalization}. One benefit of representation learning methods is that they allow for \emph{end-to-end} learning. Specifically, these methods aim to learn low-dimensional representations where sometimes additional constraints are enforced to tackle inherently causal inductive biases. This typically helps to reduce the estimation variance, especially in low-sample low-overlap settings. For example, \emph{balancing} is a common constraint to reduce the influence of instrumental variables among the covariates  \citep{johansson2022generalization}, which helps to improve the finite-sample performance when the data-generating mechanism indeed has many instruments. Similarly, disentanglement aims to address an inductive bias that different nuisance functions might share or not share common information. 

However, constraints on representations can be problematic: the constrained representations can lose their asymptotic validity when too strict constraints are applied and, as result, the estimation becomes inconsistent. This phenomenon is also known as \emph{representation-induced confounding bias} \citep{johansson2019support,melnychuk2024bounds}. As a remedy, we later present a framework to \emph{estimate causal quantities quasi-oracle efficiently (and, thus, consistently), even when asymptotically invalid representations are used}.

A different literature stream seeks to estimate causal quantities through model-agnostic methods in the form of Neyman-orthogonal learners. Prominent examples are the DR-learners and the R-learner \citep{vansteelandt2023orthogonal,morzywolek2023general}. These learners usually split estimation into two stages: nuisance functions estimation and target model fitting, where, as an important benefit, \emph{any} machine learning model can be employed at each of the two stages. Unlike end-to-end representation learning, Neyman-orthogonal learners offer several favorable theoretical properties
like double robustness and quasi-oracle efficiency \citep{chernozhukov2017double,foster2023orthogonal}. Further, by employing a separate target model in the second stage, Neyman-orthogonal learners help to address another causal inductive bias, namely that the ground-truth CATE function can be "simpler" than individual CAPOs \citep{kunzel2019metalearners}. Yet, the connections between Neyman-orthogonal learners and representation learning methods are still not understood.  

In this paper, \emph{we unify two streams of work, namely, representation learning methods and Neyman-orthogonal learners}. Specifically, we {propose a novel, general framework to perform an asymptotically quasi-oracle efficient (and, thus, consistent)  estimation of causal quantities based on the learned representations}, which we call \emph{orthogonal representation learners} (\ORlearners). Our \ORlearners are highly flexible as they target at estimating different causal quantities, like CAPOs and CATE, at the representation level of heterogeneity. 
Furthermore, our \ORlearners effectively solve the drawbacks of constrained representations (i.e., representation-induced confounding bias caused by too strict constraints). Finally, our \ORlearners are Neyman-orthogonal by construction, which brings favorable theoretical properties, namely, double robustness and quasi-oracle efficiency. 

In sum, \textbf{our contributions} are as follows:\footnote{Code is available at \url{https://anonymous.4open.science/r/OR-learners}.} \textbf{(1)}~We introduce a novel framework called \ORlearners to unify representation learning methods and Neyman-orthogonal learners. \textbf{(2)}~We show theoretically that our \ORlearners address the drawbacks of existing end-to-end representation learning methods. That is, our \ORlearners allow us to perform a quasi-oracle efficient estimation of causal quantities while offering other favorable properties related to Neyman-orthogonality. \textbf{(3)}~We demonstrate that, under regularity conditions, our \ORlearners improve the performance in estimating causal quantities for existing representation learning methods.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \vspace{-0.2cm}
\section{Related Work} \label{sec:related-work}
% \vspace{-0.2cm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Our work aims to unify two streams of work, namely, representation learning methods and Neyman-orthogonal learners. We briefly review both in the following (a detailed overview is in Appendix~\ref{app:extended-rw}).


\textbf{Representation learning for estimating causal quantities.} Several methods have been previously introduced for \emph{end-to-end} representation learning of CAPOs/CATE  \citep[see, in particular, the seminal works by][]{johansson2016learning,shalit2017estimating,johansson2022generalization}. A large number of works later suggested different extensions to these.
Existing methods fall into three main streams: (1)~One can fit an \emph{unconstrained shared representation} to directly estimate both potential outcomes surfaces \citep[e.g., \textbf{TARNet};][]{shalit2017estimating}. (2)~Some methods additionally enforce a \emph{balancing constraint} based on empirical probability metrics, so that the distributions of the treated and untreated representations become similar \citep[e.g., \textbf{CFR} and \textbf{BNN};][]{johansson2016learning,shalit2017estimating}. Importantly, balancing based on empirical probability metrics is only guaranteed to perform a consistent estimation for \emph{invertible} representations since, otherwise, balancing leads to a \emph{representation-induced confounding bias} (RICB) \citep{johansson2019support,melnychuk2024bounds}. (3)~One can additionally perform \emph{balancing by re-weighting} the loss and the distributions of the representations with learnable weights \citep[e.g., \textbf{RCFR};][]{johansson2022generalization}. We later adopt the representation learning methods from (1)--(3) as baselines. 

\textbf{Neyman-orthogonal learners}. Causal quantities can be estimated using model-agnostic methods, so-called \emph{meta-learners} \citep{kunzel2019metalearners}. Prominent examples are the R-learner \citep{nie2021quasi} and DR-learner \citep{kennedy2023towards,curth2020estimating}. %Meta-learners typically combine multiple models to perform two-stage learning, namely, (1)~nuisance functions estimation and (2)~target model fitting. As such, meta-learners must be instantiated with some machine learning model to perform (1) and (2). 
Meta-learners are model-agnostic in the sense that any base model (e.g., neural network) can be used for estimation. Also, meta-learners have several practical advantages \citep{morzywolek2023general}: (i)~they oftentimes offer favorable theoretical guarantees such as Neyman-orthogonality \citep{chernozhukov2017double,foster2023orthogonal}; (ii)~they can address the causal inductive bias that the CATE is ``simpler'' than CAPOs \citep{curth2021inductive}, and (iii)~the target model obtains a clear interpretation as a projection of the ground-truth CAPOs/CATE on the target model class. {\citet{curth2021nonparametric,frauen2025modelagnostic} provided a comparison of meta-learners implemented via neural networks with different representations, yet with the target model based on the original covariates (the representations were only used as an interim tool to estimate nuisance functions). In contrast, in our work, we study the learned representations as primary inputs to the target model.}

\textbf{Research gap.} Our work is the first to unify representation learning methods and Neyman-orthogonal learners. As a result, one can combine any representation learning method from above with our \ORlearners, which then (i)~offer favorable properties of Neyman-orthogonality and (ii)~address the causal inductive bias that CATE is ``simpler'' than CAPOs. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \vspace{-0.2cm}
\section{Preliminaries} \label{sec:prelim}
%: Meta-learners for CAPOs and CATE}
% \vspace{-0.2cm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table*}
    \vspace{-0.2cm}
    \caption{Overview Neyman-orthogonal meta-learners for CAPOs/CATE. Here, $\eta = (\mu^x_a, \pi^x_a)$ are the nuisance functions.}
    \label{tab:meta-learners}
    \begin{center}
        \vspace{-0.5cm}
        \hspace{-0.25cm}
        \scalebox{1}{
            \scriptsize
            \begin{tabular}{p{0.85cm}|p{4.3cm}|p{8.6cm}|p{1.7cm}}
                \toprule
                 Causal quantity & \multirow{2}{*}{Target risks $\mathcal{L} = \mathcal{L}(g, \eta)$} & \multirow{2}{*}{Neyman-orthogonal losses $\hat{\mathcal{L}} = \hat{\mathcal{L}}(g, \hat{\eta})$} & \multirow{2}{*}{Meta-learner}  \\
                \midrule \multirow{6}{*}{CAPOs} & \multirow{2}{*}{$\mathcal{L}_{\xi_a} = \mathbb{E}\left( \mu^x_a(X) - g(V)\right)^2$} 
                & \multirow{2}{*}{$\hat{\mathcal{L}}_{\xi_a} = \mathbb{P}_n \bigg\{ \bigg(\frac{\mathbbm{1}\{A = a\}}{\hat{\pi}^x_a(X)} \big( Y - \hat{\mu}_a^x(X) \big) + \hat{\mu}_a^x(X) - g(V)\bigg)^2 \bigg\}$} & DR-learner \newline \citep{kennedy2023towards}\\
                 \cmidrule(lr){2-4}   &  \multirow{3}{*}{$\mathcal{L}_{Y[a]} = \mathbb{E}\left( Y[a] - g(V)\right)^2$} & \multirow{3}{*}{$\hat{\mathcal{L}}_{Y[a]} = \mathbb{P}_n \bigg\{ \frac{\mathbbm{1}\{A = a\}}{\hat{\pi}^x_a(X)} \big( Y - g(V)\big)^2  +  \bigg(1 - \frac{\mathbbm{1}\{A = a\}}{\hat{\pi}^x_a(X)}\bigg) \, \big(\hat{\mu}_a^x(X) - g(V)\big)^2  \bigg\}$} & DR-learner \newline \citep{foster2023orthogonal} \\ 
                \midrule \multirow{6}{*}{CATE} & \multirow{2}{*}{$\mathcal{L}_\tau = \mathbb{E}\left( (\mu^x_1(X) - \mu^x_0(X)) - g(V)\right)^2$} & \multirow{2}{*}{$\hat{\mathcal{L}}_{\tau} = \mathbb{P}_n \bigg\{ \bigg(\frac{A - \hat{\pi}^x_1(X)}{\hat{\pi}^x_0(X) \, \hat{\pi}^x_1(X)} \big( Y - \hat{\mu}_A^x(X) \big) + \hat{\mu}_1^x(X) - \hat{\mu}_0^x(X) - g(V)\bigg)^2 \bigg\}$} & DR-learner \newline \citep{kennedy2023towards}\\
                \cmidrule(lr){2-4} & \multirowcell{2}[0pt][l]{$ \mathcal{L}_{\pi_0 \pi_1 \tau} = \mathbb{E}\Big[\pi^x_0(X)\,\pi^x_1(X) \big((\mu^x_1(X) $ \\ $- \mu^x_0(X)) - g(V)\big)^2\Big]$} & \multirowcell{2}[0pt][l]{$\hat{\mathcal{L}}_{\pi_0\pi_1\tau} = \mathbb{P}_n \bigg\{\Big(\big(Y - \hat{\mu}^x(X)\big) - \big(A - \hat{\pi}_1^x(X)\big) g(V)\Big)^2 \bigg\}$, \\ ${\mu}^x(x) = \mathbb{E}(Y \mid X = x) = {\pi}_1^x(x)\, {\mu}_1^x(x) + {\pi}_0^x(x) \, {\mu}_0^x(x)$ } & R-learner \newline \citep{nie2021quasi}\\
                \bottomrule
            \end{tabular}
        }
    \end{center}
    \vspace{-0.6cm}
\end{table*}

\textbf{Notation.} We denote random variables with capital letters $Z$, their realizations with small letters $z$, and their domains with  calligraphic letters $\mathcal{Z}$. Let $\mathbb{P}(Z)$, $\mathbb{P}(Z = z)$, $\mathbb{E}(Z)$ be the distribution, probability mass function/density, and expectation of $Z$, respectively. Let $\mathbb{P}_n\{f(Z)\} = \frac{1}{n} \sum_{i=1}^n f(z_i)$ be the sample average of $f(Z)$.
Then, we define the following nuisance functions: $\pi_a^x(x) = \mathbb{P}(A = a \mid X = x)$ is the \emph{covariate propensity score} for the treatment $A$, and $\mu_a^x(x) = \mathbb{E}(Y = y \mid X = x, A = a)$ is the \emph{expected covariate-conditional outcome} for the outcome $Y$. Similarly, we define $\pi_a^\phi(x) = \mathbb{P}(A = a \mid \Phi(X) = \phi)$ and  $\mu_a^\phi(\phi) = \mathbb{E}(Y = y \mid \Phi(X) = \phi, A = a)$ as the \emph{representation propensity score} and the \emph{expected representation-conditional outcome} for a representation $\Phi(x) = \phi$, respectively. Importantly, the upper indices in $\pi_a^x,\mu_a^x, \pi_a^\phi,\mu_a^\phi$ indicate whether the corresponding nuisance functions depend on the covariates $x$ or on the representation $\phi$. In, our work, we adopt the Neyman-Rubin potential outcomes framework \citep{rubin1974estimating}, where $Y[a]$ is the \emph{potential outcome} after intervening on the treatment $do(A = a)$ and where $Y[1] - Y[0]$ is the \emph{treatment effect}.

\textbf{Problem setup.} To estimate the causal quantities, we make use of an observational dataset $\mathcal{D}$ that contains high-dimensional covariates $X \in \mathcal{X} \subseteq \mathbb{R}^{d_x}$, a binary treatment $A \in \{0, 1\}$, and a continuous outcome $Y \in \mathcal{Y} \subseteq \mathbb{R}$. For example, a common setting is an anti-cancer therapy, where the outcome is the tumor growth, the treatment is whether chemotherapy is administered, and covariates are patient information such as age and sex. The dataset $\mathcal{D} = \{x_i, a_i, y_i\}_{i=1}^{n}$ is assumed to be sampled i.i.d. from a joint distribution $\mathbb{P}(X, A, Y)$ with dataset size $n$.  

\textbf{Causal quantities.} We are interested in the estimation of two important causal quantities at the covariate level of heterogeneity: $\bullet$\,\emph{conditional average potential outcomes (CAPOs)} given by $\xi_a^x(x)$, and $\bullet$\,the \emph{conditional average treatment effect (CATE)} given by $\tau^x(x)$, with  $\xi_a^x(x) = \mathbb{E}(Y[a] \mid X = x) \quad \text{and} \quad \tau^x(x) = \mathbb{E}(Y[1] - Y[0] \mid X = x) = \xi_1^x(x) - \xi_0^x(x)$. If we had access to a ground-truth sample of potential outcomes \(Y[a]\) and the corresponding treatment effect \(Y[1] - Y[0]\), then the consistent estimation of CAPOs and CATE, respectively, would reduce to a standard regression problem.
Yet, to consistently estimate the causal quantities given only the observational data $\mathcal{D}$, we need to make standard identifiability and smoothness assumptions \citep{rubin1974estimating,curth2021nonparametric,kennedy2023towards} (see Appendix~\ref{app:background-ass}).


\textbf{Two-stage learners.} In this paper, we focus on \textit{two-stage learners} due to their practical and theoretical advantages \citep{curth2021nonparametric,morzywolek2023general,chernozhukov2017double,foster2023orthogonal}. Formally, two-stage learners aim to find the best projection of CAPOs/CATE onto a \emph{working model class}, $\mathcal{G} = \{g(\cdot): \mathcal{V} \subseteq \mathcal{X} \to \mathcal{Y}\}$, by minimizing different \emph{target risks} wrt. $g(V)$ ($V \subseteq X$ is a conditioning set and the input for the working model). Usually, target risks for CAPOs/CATE are chosen as different variants of mean squared errors (MSEs) (see Table~\ref{tab:meta-learners} for definitions).
The two-stage learners then proceed in two stages: first, the nuisance functions $\hat{\eta}$, are estimated and, then, estimators of the target risks $\hat{\mathcal{L}}(g, \hat{\eta})$ are minimized wrt. $g$. 

\begin{figure*}[tbp]
    \centering
    \vspace{-0.4cm}
    \includegraphics[width=\textwidth]{figures/or-learner-overview.pdf}
    \vspace{-0.6cm}
    \caption{{\textbf{An overview of our \ORlearners.} Our \ORlearners proceed in three stages: \circled{0}~fitting a representation network, \circled{1}~estimation of the nuisance functions, and \circled{2}~fitting a target network. For the stage \circled{0}, we also show different options for the target network input $V$. Depending on the choice of the input $V$, the second-stage model $g(V)$ obtains different interpretations: it either learns a new model from scratch or performs a calibration of the representation network.}}
    \vspace{-0.6cm}
    \label{fig:or-learner-overview}
\end{figure*}


\textbf{Neyman-orthogonal learners.} Efficient estimation of the target risks yields Neyman-orthogonal learners \citep{foster2023orthogonal}. A defining property of Neyman-orthogonal learners is that they are first-order insensitive wrt. to the misspecification of the nuisance functions, $\hat{\eta}$. We formalize this definition and other related favorable theoretical properties (i.e., quasi-oracle efficiency and double robustness) in Appendix~\ref{app:background-meta-learners}. Notable examples of Neyman-orthogonal learners for the CAPOs target risks include the DR-learners and the R-learner (see Table~\ref{tab:meta-learners} and Appendix~\ref{app:background-meta-learners} for details). 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \vspace{-0.2cm}
\section{Orthogonal Representation Learning}
\label{sec:OR-learners}
% \vspace{-0.2cm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We provide proofs of theoretical statements in Appendix~\ref{app:proofs}.

\textbf{Motivation.} The theory on Neyman-orthogonal learners \citep{morzywolek2023general,vansteelandt2023orthogonal} does not provide a guidance on how to choose the conditioning set $V \subseteq X$. Also, to the best of our knowledge, Neyman-orthogonal learners were not studied through the lens of different representations $\Phi(X)$ chosen in place of $V$. For example, if the representation $\Phi(X)$ itself is learned to be predictive of $\mu_a^x$, as in all the end-to-end representation learning methods, {\emph{fitting the target model based on $V = \Phi(X)$ may be beneficial compared to other choices of $V$}}. We aim to study this research gap and thus introduce a novel class of Neyman-orthogonal learners with $V = \Phi(X)$ called \emph{orthogonal representation learners} (\ORlearners). Hence, this choice of $V$ sets our \ORlearners apart from existing Neyman-orthogonal learners (which traditionally use $V = X$).   


\textbf{Overview of our \ORlearners}. Our \ORlearners use neural networks to fit a target model $g$ based on the learned representations $\Phi(X)$. They proceed in three stages (see Fig.~\ref{fig:or-learner-overview}): \circled{0}~fitting a representation network; \circled{1}~estimating nuisance functions (if necessary); and \circled{2}~fitting a target network. The pseudocode is in Algorithm~\ref{alg:or-learners}.

More specifically: In stage \circled{0}, the \emph{representation network} consists of either (a)~a fully-connected (FC$_\phi$) or a normalizing flow (NF$_\phi$) representation subnetwork, and (b)~a fully-connected (FC$_a$) outcomes subnetwork. Here, \emph{any} representation learning method can be used, and, depending on the method, additional components might be added (\eg, a propensity subnetwork for CFR-ISW). Then, in stage \circled{1}, we might need to additionally fit nuisance functions (\eg, when the constrained representations were used in stage \circled{0}, so that $\hat{\mu}^\phi_a$ is inconsistent wrt. $\hat{\mu}^x_a$). Therein, we might optionally employ two additional networks, namely, a \emph{propensity network} FC$_{\pi, x}$ and an \emph{outcomes network} FC$_{\mu, x}$.  Finally, in the stage \circled{2}, we utilize different DR- and R-losses, as presented in Sec.~\ref{sec:prelim}, to fit a fully-connected \emph{target network} $g$ and thus yield a final estimator of CAPOs/CATE. 



\vspace{-0.1cm}
\begin{algorithm}[htb]
    \vspace{-0.1cm}
    \caption{Pseudocode of our \ORlearners}\label{alg:or-learners}
    % \vspace{-0.1cm}
    \begin{algorithmic}
    \footnotesize
        \STATE {\bfseries Input:} Training dataset $\mathcal{D}$, (balancing) constraint strength $\alpha \ge 0$, target risk $\diamond \in \{ \xi_a, Y[a], \tau, \pi_0\pi_1\tau\}$
        \STATE {\bfseries Stage }\circled{0}: Fit a representation network (FC$_\phi$ / NF$_\phi$, FC$_a$) by minimizing $\mathcal{L}_{\text{MSE}} + \alpha \mathcal{L}_{\text{Bal}}$ and set $V \gets \Phi(X)$ 
        \STATE {\bfseries Stage }\circled{1}: Estimate nuisance functions $\hat{\eta} = (\hat{\mu}_a^x, \hat{\pi}_a^x)$
        \begin{ALC@g}
            \STATE Fit a propensity network (FC$_{\pi, x}$) by minimizing a BCE loss $\mathcal{L}_\pi$ and set $\hat{\pi}_a^x(X) \gets$ FC$_{\pi, x}(X)$
            \IF{$\alpha > 0$ and FC$_\phi$ is used at Stage \circled{0}}
                \STATE Fit an outcomes network (FC$_{\mu, x}$) by minimizing an MSE loss $\mathcal{L}_{\text{MSE}}$ and set $\hat{\mu}_a^x(X) \gets$ FC$_{\mu, x}(X, a)$ 
            \ELSE 
                \STATE Set $\hat{\mu}_a^x(X) \gets \hat{\mu}_a^\phi(\Phi(X))$
            \ENDIF
        \end{ALC@g}
        \STATE {\bfseries Stage }\circled{2}: Fit a target network $\hat{g} = \argmin \hat{\mathcal{L}}_\diamond(g, \hat{\eta})$ 
        \STATE {\bfseries Output:} Representation-level estimator $\hat{g}$ for CAPOs/CATE 
    \end{algorithmic}
    \vspace{-0.1cm}
\end{algorithm}

Our \ORlearners are Neyman-orthogonal by construction and thus yield quasi-oracle efficient and doubly-robust CAPOs/CATE estimators $\hat{g}$ (see Lemma~\ref{prop:quasi-eff-dr} in Appendix~\ref{app:background-meta-learners} for details).

\textbf{Variants of our \ORlearners.} In the following, we introduce different variants of our \ORlearners depending on the type of representations they are based: we consider unconstrained (Sec.~\ref{sec:OR-learner-unconstrained}), constrained invertible (Sec.~\ref{sec:OR-learner-constrained-inv}) and constrained non-invertible (Sec.~\ref{sec:OR-learner-constrained-non-inv}) representations. {For the latter two types of representations, we consider balancing with empirical probability metrics as the constraint. As we will show later, \ORlearners with balancing representations (Sec.~\ref{sec:OR-learner-constrained-inv} and \ref{sec:OR-learner-constrained-non-inv}) reverse both benefits and drawbacks of balancing and, asymptotically, lag behind \ORlearners with the unconstrained representations} (Sec.~\ref{sec:OR-learner-unconstrained}). Nevertheless, the latter two variants are shown for discussion purposes (we discuss practical implications in Sec.~\ref{sec:implications}).


For each of the three variants of our \ORlearners, we describe how we adapt Algorithm~\ref{alg:or-learners} and present new theoretical results by discussing the following questions: \textbf{(i)}~How can the learned representation space be interpreted? \textbf{(ii)}~Does the representation ensure asymptotic validity in light of the representation-induced confounding bias (RICB)? \textbf{(iii)}~How will our \ORlearners help in that the target network based on the representation $g(\phi)$ can outperform the original end-to-end representation learning predictor $\hat{\mu}_a^{\phi}$? \textbf{(iv)}~How can the trained target network be interpreted?

\vspace{-0.1cm}
\subsection{OR-learners for unconstrained representations} \label{sec:OR-learner-unconstrained}
\vspace{-0.1cm}

We propose the first variant of our \ORlearners based on unconstrained representations. 
\vspace{-0.2cm}
\begin{tcolorbox}
[colback=white!5!white,colframe=black!75!black,boxsep=0pt,left=1.5pt,right=1.5pt,top=1.5pt,bottom=1.5pt]
    \footnotesize
    \textbf{Variant 1 (unconstrained representations).} We specify Algorithm~\ref{alg:or-learners} as follows. \textbf{Input}: we set $\alpha = 0$; \textbf{Stage} \circled{0}: the representation $\Phi(X)$ is an output of the fully-connected representation subnetwork FC$_\phi(X)$.
\end{tcolorbox}
\vspace{-0.2cm}

One can obtain the unconstrained representations by fitting the representation networks w/o balancing such as, for example, TARNet \citep{shalit2017estimating}, BNN \citep{johansson2016learning}, DragonNet \citep{shi2019adapting}, CFR-ISW \citep{hassanpour2019counterfactual}, and BWCFR \citep{assaad2021counterfactual}.  

\textbf{(i) Interpretation of the learned representations.} Neural networks can handle increasingly more complicated regression tasks by simply adding more layers. This can be formalized with the notion of (H{\"o}lder) smoothness: Each layer induces a new space in which the ground-truth regression function becomes smoother and thus easier to estimate.
\begin{numprop}{1}[Smoothness of the hidden layers] \label{prop:smoothness-orig}
    Under mild conditions on the fitted representation network, there exists a hidden layer (marked by $V$) of the network with an increased smoothness: $\mu^v_a(\cdot)$ is smoother than $\mu^x_a(\cdot)$. 
\vspace{-0.2cm}
\end{numprop} 

\begin{figure}
    \centering
    \vspace{-0.2cm}
    \includegraphics[width=\linewidth]{figures/holder-smooth.pdf}
    \vspace{-1.1cm}
    \caption{Hidden layers of the representation network induce spaces where the regression task becomes simpler.}
    \label{fig:holder-smooth}
    \vspace{-0.5cm}
\end{figure}
In our setting of CAPOs/CATE estimation, we consider $V = \Phi(X)$. Thus, if learned well, the representation subnetwork $\text{FC}_\phi$ and the induced representation space $\Phi(\cdot): \mathcal{X} \to \mathit{\Phi}$ should simplify the task of CAPOs/CATE estimation. 

\textbf{(ii) Validity wrt. the RICB.} The unconstrained representations $\Phi(X)$ can be also considered asymptotically valid when $d_\phi \ge 2$ (we follow the definition of valid representations from \citet{melnychuk2024bounds}). {As an example of valid representation $\Phi(X)$ with $d_\phi = 2$, we can consider $\{\mu_0^x(X), \mu_1^x(X)\}$. 
\begin{numprop}{2}[Valid unconstrained representation with $d_\phi = 2$]
    The representation $\Phi(X) = \{\mu_0^x(X), \mu_1^x(X)\}$ is valid for CAPOs and CATE.
\vspace{-0.2cm}
\end{numprop}
These representations can be learned arbitrarily well in the asymptotic regime, given sufficiently deep representation subnetwork FC$_\phi$ with unconstrained representations (that follows from the universal approximation theorem).} Hence, in the case of $d_\phi \ge 2$, the unconstrained representations do not induce representation-induced confounding bias (RICB). This means, although we have $(Y[0], Y[1]) \notind A \mid \Phi(X)$ in general, the representation contains all the sufficient information for estimation of $\mu_a^x$, and, hence, the causal quantities can be consistently estimated solely with $\Phi(X)$ as follows: ${\xi}^x_a(x) = {\xi}_a^\phi(\Phi(x)) = {\mu}_a^\phi(\Phi(x))$ and ${\tau}^x(x) =  {\tau}^\phi(\Phi(x)) = {\mu}_1^\phi(\Phi(x)) - {\mu}_0^\phi(\Phi(x))$. Thus, the original representation network $\hat{\mu}_a^\phi(\Phi(x))$ can be used as a consistent estimator of $\hat{\mu}_a^x(x)$.       

\textbf{(iii) How will our \ORlearners help?} \ORlearners proceed by using the original representation network as the estimator for $\hat{\mu}_a^x(x) = \hat{\mu}_a^\phi(\Phi(x))$ and additionally fit a covariate propensity score network $\hat{\pi}_a^x(x)$. Therefore, the second-stage model $g(\phi)$ uses additional propensity information and achieves more efficient estimation. Interestingly, BWCFR without balancing (an inverse propensity of treatment weighted (IPTW) learner) \citep{assaad2021counterfactual} can be seen as a special case of our \ORlearners. It aims at estimating CAPOs and can achieve Neyman-orthogonality in a single-stage of learning. This happens due to the fact that the target model $g(x)$ coincides with one of the nuisance functions $\hat{\mu}_a^x(x)$: In this case, both DR-learner losses from Eq.~\eqref{eq:DR-learner-FS} and \eqref{eq:DR-learner-K} immediately simplify to the IPTW-learner loss (= weighted MSE loss of BWCFR w/o balancing): 
\begingroup\makeatletter\def\f@size{8}\check@mathfonts
\begin{equation}
    \hat{\mathcal{L}}_{\xi_a}( \hat{\mu}_a^x, \hat{\eta}) = \hat{\mathcal{L}}_{Y[a]}( \hat{\mu}_a^x, \hat{\eta}) = \mathbb{P}_n \bigg\{ \frac{\mathbbm{1}\{A = a\}}{\hat{\pi}^x_a(X)} \big( Y - \hat{\mu}_a^x(x)\big)^2 \bigg\}.
\end{equation}
\endgroup
Notably, the same trick is not possible for CATE estimation, as the counterfactual outcomes are never observed and, thus, can not be directly regressed on. Therefore, a second-stage model is needed even for BWCFR.

\textbf{(iv) Interpretation of the target model.} The fitted target network can be interpreted as some form of a \emph{conditional calibration} of the original representation network. To see that, we can compare our target network, for which $V = \Phi(X)$ holds, with two other alternatives (see stage~\circled{0} in Fig.~\ref{fig:or-learner-overview}): (a)~a target network with the input $V = X$ and (b)~a target network with the input $V = \{\hat{\mu}_0^\phi, \hat{\mu}_1^\phi\} =\{\hat{\mu}_0^x, \hat{\mu}_1^x\}$ {(these are also known as prognostic scores; see Appendix~\ref{app:extended-rw-orthogonal-learners})}. Option (a) with $V = X$ suggests fitting the target network completely from scratch and ``misses'' the opportunity to use learned representations. {In addition, the losses of the second-stage model can be highly unstable in a low-sample regime (\eg, due to high inverse propensity scores), which hinders the chances of $g(X)$ to learn the representations ``from scratch''}. On the other hand, option (b) with $V = \{\hat{\mu}^x_0, \hat{\mu}_1^x\}$ can only use the outputs of the representation network. For CAPOs estimation, the following proposition holds.

\begin{numprop}{3}[Calibration] \label{prop:calibration-orig} Given an unconstrained working model class $\mathcal{G}$, population minimizers $\hat{g}(\hat{\mu}^x_0(x), \hat{\mu}_1^x(x))$ of the DR-learner losses for CAPOs have the following form: 
\begingroup\makeatletter\def\f@size{8}\check@mathfonts
\begin{align}
    & \hat{g}(\hat{\mu}^x_0(x), \hat{\mu}_1^x(x))  = \mathbb{E}\bigg(\frac{\mathbbm{1}\{A = a\} Y }{\hat{\pi}_a^x(X)} \,\Big\vert\, \hat{\mu}^x_0(x), \hat{\mu}^x_1(x) \bigg) \\ & + \hat{\mu}^x_a(x) \, \bigg[1 - \mathbb{E}\bigg(\frac{\mathbbm{1}\{A = a\}}{\hat{\pi}_a^x(X)} \,\Big\vert\, \hat{\mu}^x_0(x), \hat{\mu}^x_1(x) \bigg) \bigg]. \nonumber
\end{align}
\endgroup
\end{numprop}
Proposition~\ref{prop:calibration-orig} implies that $\hat{g}(v)$ with $V = \{\hat{\mu}^x_0(X), \hat{\mu}_1^x(X)\}$ performs the average calibration of the original representation network \citep{gupta2020distribution,van2023causal}.
% \footnote{$V = \hat{\pi}^x_1(X)$ yields a similar interpretation.} 
Therefore, when $V = \Phi(X)$, the target network acts as a \emph{conditional calibration of the original representation network}, namely, a middle ground between full re-training and the calibration on average.

\vspace{-0.1cm}
\subsection{OR-learners for invertible representations with balancing} \label{sec:OR-learner-constrained-inv} 
\vspace{-0.1cm}

\begin{figure*}[tbp]
    \centering
    \vspace{-0.25cm}
    \hspace{-0.25cm}
    \includegraphics[width=1.01\textwidth]{figures/or-learner-summary.pdf}
    \vspace{-0.7cm}
    \setlength{\fboxsep}{0.5pt}
    \caption{{\textbf{Insights for our \ORlearners.} Shown are the insights from Sec.~\ref{sec:OR-learner-constrained-inv} (left) and \ref{sec:OR-learner-constrained-non-inv} (right). For both figures, we highlight in  \protect\ovalbox{\colorbox{yellow}{yellow boxes}} how our \ORlearners (in \textcolor{tabred}{red}) can be beneficial in comparison with the base representation network (in \textcolor{tabblue}{blue}). Specifically, we compare the generalization performances in terms of MSE / precision in estimating heterogeneous effect (PEHE) (lower is better), depending on the strength of balancing, $\alpha$. In both cases, we show the behavior in a finite-sample vs. asymptomatic regime ($n \to \infty$). The plots highlight the effectiveness of our \ORlearners in the asymptotic regime, especially when too much balancing is applied.}}
    \vspace{-0.5cm}
    \label{fig:or-learner-summary}
\end{figure*}

Now, we turn our attention to how our \ORlearners affect invertible representations, where we enforce additional \emph{balancing with empirical probability metrics}. Here, we use normalizing flows \citep{tabak2010density,rezende2015variational} $\text{NF}_\phi$ to enforce a strict invertibility; and {we use empirical integral probability metrics (IPMs), (\eg, Wasserstein metric (WM), and maximum mean discrepancy (MMD)) to enforce balancing (see Appendix~\ref{app:background-ipm} for details).
\vspace{-0.5cm}
\begin{tcolorbox}[colback=white!5!white,colframe=black!75!black,boxsep=0pt,left=1.5pt,right=1.5pt,top=1.5pt,bottom=1.5pt]
    \footnotesize
    \textbf{Variant 2 (invertible representations with balancing).} We specify Algorithm~\ref{alg:or-learners} as follows. \textbf{Input}: we set $\alpha > 0$; \textbf{Stage} \circled{0}: the representation $\Phi(X)$ is an output of the normalizing flow representation subnetwork $\text{NF}_\phi$; $\mathcal{L}_\text{Bal} =\widehat{\operatorname{dist}}(\mathbb{P}(\Phi(X) \mid A = 0), \mathbb{P}(\Phi(X) \mid A = 1))$, where $\operatorname{dist} \in \{\operatorname{WM}, \operatorname{MMD}\}$.
\end{tcolorbox}
\vspace{-0.2cm}

Examples of such representation networks are CFR \citep{shalit2017estimating}, CFR-ISW \citep{hassanpour2019counterfactual}, and BWCFR \citep{assaad2021counterfactual}, which we call CFRFlow, CFRFlow-ISW, and BWCFRFlow, respectively.\footnote{{CFR-ISW and BWCFR additionally} implement balancing by re-weighting, using inverse propensities of treatment weights. However, this type of balancing does not introduce any constraints.} 

\textbf{(i) Interpretation of the learned representations.} Since we used a normalizing flow as the representation subnetwork, the transformation $\Phi(\cdot)$ becomes a diffeomorphism. Therefore, it can only non-linearly scale down or up different parts of the original space $\mathcal{X}$. Then, in order to minimize the original MSE loss, the representation network would scale up the parts of space that increase the smoothness of $\mu_a^\phi(\phi)$ {(see Proposition~\ref{prop:smoothness-orig})}. At the same time, balancing can only scale down regions of the space $\mathcal{X}$ with a lack of overlap. This is summarized in the following propositions.
{\begin{numprop}{4}[Smoothness via expanding transformations]
    A representation network with a representation $\Phi(X)$ achieves higher H{\"o}lder smoothness of $\mu^a_\phi(\cdot)$ by expanding some parts of $\mathcal{X}$. 
\end{numprop}
\begin{numprop}{5}[Balancing via contracting transformations]
    A representation network with a representation $\Phi(X)$ reduces {the IPMs, namely, WM and MMD,} between the distributions of the representations $\mathbb{P}(\Phi(X) \mid A = 0)$ and $\mathbb{P}(\Phi(X) \mid A = 0)$ by contracting some parts of $\mathcal{X}$.
\vspace{-0.2cm}
\end{numprop}}

Therefore, the final learned representation would combine both scaling up due to effort in smoothing and scaling down due to balancing. If both scaling up and down happen in the different areas of the covariate space, then balancing could be beneficial. {On the other hand, if both are happening in the same parts of the space, balancing renders itself useless and any amount of it can only harm the performance of the representation network. This important result allows us to formulate a crucial inductive bias needed for balancing to perform well: \emph{areas with a lack of overlap need to coincide with areas with low heterogeneity of potential outcomes/treatment effect}.}

\textbf{(ii) Validity wrt. the RICB.} Invertible representations can not induce RICB \citep{melnychuk2024bounds}. However, by scaling up and down different parts of the space $\mathcal{X}$, we can influence the low-sample performance, for example, as the gradient descent depends on the scale of inputs \citep{lecun2002efficient}.



\textbf{(iii) How will our \ORlearners help?} In our instantiation of the \ORlearners, we follow Sec.~\ref{sec:OR-learner-unconstrained} and use the representation network outputs as the estimators of the nuisance functions, $\hat{\mu}_a^x(x)$. Notably, both CRFFlow-ISW and BWCFRFlow can be considered Neyman-orthogonal wrt. to the target risks for CAPOs (see the similar argument in (iii) of Sec.~\ref{sec:OR-learner-unconstrained}). Our \ORlearners then will effectively try to ``undo'' the effect of balancing due to that our \ORlearners reintroduce propensity weighting. {Specifically, the DR-loss in our \ORlearners would ``re-focus'' the target networks on the parts of the representation space with a lack of overlap. The reason is that these regions will have large inverse propensity scores, and, thus, the target network will have a larger loss there.} At the same time, the R-loss in our \ORlearners  would be leaning to ignore these. 

% TODO --- idea for later: can the above be more formal? 

\textbf{(iv) Interpretation of the target model.}  As we describe in (iii), the target network will ``undo'' the effect of balancing, and, therefore, it slowly loses its interpretation as the conditional calibration model as more balancing is applied. {We summarize the benefits of applying our \ORlearners on top of the invertible representations in Fig.~\ref{fig:or-learner-summary} (left).}

\vspace{-0.1cm}
\subsection{OR-learners for non-invertible representations with balancing} \label{sec:OR-learner-constrained-non-inv} 
\vspace{-0.1cm}

Finally, we discuss how our \ORlearners perform based on the non-invertible (general) representations {where balancing with empirical probability metrics} is enforced. 
\vspace{-0.2cm}
\begin{tcolorbox}[colback=white!5!white,colframe=black!75!black,boxsep=0pt,left=1.5pt,right=1.5pt,top=1.5pt,bottom=1.5pt]
    \footnotesize
    \textbf{Variant 3 (non-invertible representations with balancing).} We specify Algorithm~\ref{alg:or-learners} as follows. \textbf{Input}: we set $\alpha > 0$; \textbf{Stage} \circled{0}: the representation $\Phi(X)$ is an output of the fully-connected representation subnetwork $\text{FC}_\phi$; $\mathcal{L}_\text{Bal} =\widehat{\operatorname{dist}}(\mathbb{P}(\Phi(X) \mid A = 0), \mathbb{P}(\Phi(X) \mid A = 1))$, where $\operatorname{dist} \in \{\operatorname{WM}, \operatorname{MMD}\}$.
\end{tcolorbox}

\textbf{(i) Interpretation of the learned representations.} The learned representations have a similar interpretation as in  (i) of Sec.~\ref{sec:OR-learner-constrained-inv}. However, the representation network is now not only allowed to scale down or up different parts of the original covariates space, but also to fold it, project it, etc. {At the same time, the results of Propositions~\ref{prop:smoothness-orig}, \ref{prop:smooth-scaling}, and \ref{prop:balancing-scaling} still hold. For example, when balancing is applied, non-overlapping parts of the space could be simply folded together \citep{keup2022origami} or projected onto some subspace (\ie, transformations with the Lipschitz constant less than one would be applied).} 

\textbf{(ii) Validity wrt. the RICB.} When too much balancing is applied, the representations may (i)~lose heterogeneity and (ii)~induce the RICB \citep{melnychuk2024bounds}. That means that (a)~no asymptotically consistent estimation based solely on the representations $\Phi(x)$ is possible (\eg, ${\xi}^x_a(x) \neq  {\xi}^\phi_a(\Phi(x))$); and (b)~the consistent estimation of the representation level causal quantities itself requires access to the original covariates, \ie, ${\xi}_a^\phi(\phi) \neq {\mu}_a^\phi(\phi)$.    

\textbf{(iii) How will our \ORlearners help?} Asymptotically, our \ORlearners will help to remove the RICB so that we can consistently estimate representation level CAPOs and CATE. Yet, they cannot recover the lost heterogeneity and will only estimate causal quantities at the $X^y$ level of heterogeneity, where $X^y \subseteq X: X^y \ind A$. {Interestingly, in the extreme case of the heterogeneity loss (when representations are constant, i.e., $\Phi(X) = c$), our \ORlearners would yield (semi-parametrically) efficient estimators of average potential outcomes (APOs) and (overlap-weighted)\footnote{Notably, the R-learner will generally lag behind the DR-learner in the asymptotic regime due to the discrepancy between ATE and the overlap-weighted ATE; see Fig~\ref{fig:or-learner-summary} (right).} average treatment effect (ATE).  

{\begin{numprop}{6}[Consistent estimation with $\Phi(X) = c$] 
    For constant representations $\Phi(X) = c$, our \ORlearners yield semi-parametric efficient (i.e., A-IPTW) estimators of APOs and ATE / overlap-weighted ATE.
\vspace{-0.2cm}
\end{numprop}}
Hence, on the one hand, our \ORlearners can ``undo'' the benefit brought by balancing (if there is such a setting), and, on the other, partially fix the damage after applying too much balancing.


\textbf{(iv) Interpretation of the target model.} The target network obtains a similar interpretation as in (iv) of Sec.~\ref{sec:OR-learner-constrained-inv}. However, in the case of the non-invertible representations with balancing, only $X^y$-level causal quantities can be estimated with the target network. {We further show the pros of using our \ORlearners with non-invertible representations in Fig.~\ref{fig:or-learner-summary} (right).}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \vspace{-0.2cm}
\section{Experiments}
% \vspace{-0.2cm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Setup.} We aim to validate the above intuition for why our \ORlearners are effective through numerical experiments. We follow prior literature \citep{curth2021nonparametric,melnychuk2024bounds} and use several \mbox{(semi-)synthetic} datasets where both counterfactual outcomes $Y[0]$ and $Y[1]$ and ground-truth covariate level CAPOs / CATE are available. We perform experiments in three settings, in which we compare the performances of vanilla representation learning methods with our \ORlearners based on the learned representations. $\bullet$\,In \textbf{Setting A}, we compare different \ORlearners based on unconstrained representations. $\bullet$\,In \textbf{Setting B}, we show how our \ORlearners help to improve performance based on invertible representations. $\bullet$\,In \textbf{Setting C}, for non-invertible representations with balancing.  

\textbf{Performance metrics.} We report (i)~the out-of-sample root mean squared error (rMSE) and (ii)~the root precision in estimating heterogeneous effect (rPEHE) for CAPOs and CATE, respectively. Recall that we are primarily interested in how our \ORlearners improve existing representation learning methods, and, therefore, we report the difference in the performance between the original representation network and our \ORlearners. Formally, we compute $\Delta_\diamond(\text{rMSE})$ and $\Delta_\diamond(\text{rPEHE})$, where $\diamond \in \{ \xi_a, Y[a], \tau, \pi_0\pi_1\tau\}$ is a specific learner for CAPOs or CATE.  

\textbf{Datasets.} We used three standard datasets for benchmarking in causal inference: (1)~a fully-synthetic dataset ($d_x = 2$) \citep{kallus2019interval,melnychuk2024bounds}; (2)~the IHDP dataset ($n = 672 + 75; d_x=25$) \citep{hill2011bayesian,shalit2017estimating}; and (3)~a collection of 77 ACIC 2016 datasets ($n = 4802, d_x=82$) \citep{dorie2019automated}. 
Further details are in Appendix~\ref{app:datasets}.


\textbf{Baselines.} We implemented various state-of-the-art representation learning methods, which act as baselines. We further combine each baseline with our \ORlearners (see implementation details in Appendix~\ref{app:implementation}). Importantly, both the baselines and the combination with our \ORlearners undergo rigorous hyperparameter tuning, so that the comparison is fair and any performance gain must be attributed to how we integrate a Neyman-orthogonal loss (shown in \textcolor{ForestGreen}{green number} across all tables).
The baselines are: \textbf{TARNet} \citep{shalit2017estimating}; several variants of \textbf{BNN} \citep{johansson2016learning} (w/ or w/o balancing); several variants of \textbf{CFR} \citep{shalit2017estimating,johansson2022generalization} (w/ balancing, non-/ invertible); several variants of \textbf{RCFR} \citep{johansson2018learning,johansson2022generalization} (different types of balancing); several variants of \textbf{CFR-ISW} \citep{hassanpour2019counterfactual} (w/ or w/o balancing, non-/ invertible); and \textbf{BWCFR} \citep{assaad2021counterfactual} (w/ or w/o balancing, non-/invertible). 


{\tiny$\blacksquare$}~\textbf{Setting A.} In Setting A, we want to compare the performance of vanilla representation networks (\ie, TARNet and BNN ($\alpha = 0.0$)) versus our \ORlearners applied on top of the unconstrained representations, where the latter is denoted $V = \Phi(X)$. We compare two further variants of our \ORlearners, where the target network has different inputs: (a)~$V = X$ and (b)~$V = \{\hat{\mu}_0^x, \hat{\mu}_1^x\}$, yet the same depth of one hidden layer. We also compare our \ORlearners where the target network is based on the covariates space, so that we match the depth of the original representation network $V = X^*$. {Therefore, we provide a fair comparison of our \ORlearners and other alternative variants of DR/R-learners.} \textbf{Results.} Table \ref{tab:acic2016-setting-a} shows the results for the ACIC 2016 dataset collection (we refer to Appendix~\ref{app:experiments} for additional results for the synthetic dataset). Therein, our \ORlearners with $V = \Phi(X)$ achieve superior performance for both CAPOs and CATE. Hence, using the representation $\Phi(X)$ as an input for the target network suggests a good trade-off between full re-training (as is the case with $V = X^*$ and $V = X$) and a simple averaged calibration with $V = \{\hat{\mu}_0^x, \hat{\mu}_1^x\}$. $\Rightarrow$\,Our \ORlearners lead to clear performance gains. 

\begin{table}[ht]
    \centering
    \vspace{-0.1cm}
    \caption{\textbf{Results for 77 semi-synthetic ACIC 2016 experiments in Setting A.} Reported: the percentage of runs, where our \ORlearners improve over representation networks. Here, $d_\phi = 8$.} \label{tab:acic2016-setting-a}
      \vspace{-0.4cm}
      \begin{center}
        \scriptsize
        \scalebox{0.72}{\input{tables/acic2016_setting_a}}
    \end{center}
    \vspace{-0.2cm}
\end{table}

{\tiny$\blacksquare$}~\textbf{Setting B.} Here, we study how our \ORlearners counteract balancing of the invertible representations. For that, we compare a TARFlow ($\hat{=}$TARNet with a normalizing flow as the representation subnetwork) and other invertible representation networks with varying amounts of balancing $\alpha$: CFRFlow, CFRFlow-ISW, and BWCFRFlow. For estimating CAPOs, CFRFlow-ISW and BWCFRFlow are already Neyman-orthogonal (see Sec.~\ref{sec:OR-learner-constrained-inv}) and thus can be considered as special cases of our \ORlearners. For the CATE, we use a second-stage model given by the DR-learner. \textbf{Results.} The results for Setting B are shown in Fig.~\ref{fig:synthetic-setting-b} (we refer to Appendix~\ref{app:experiments} for additional results for the synthetic and IHDP datasets). Overall, CFRFlow-ISW and BWCFRFlow improve the performance of the CFRFlow. The reason is that the synthetic benchmark does not contain instruments and the amount of balancing makes the task of estimating CAPOs/CATE harder. $\Rightarrow$\,Our \ORlearners
yield large performance gains over the baselines.

\begin{figure}[ht]
    \centering
    \vspace{-0.1cm}
    \includegraphics[width=\linewidth]{figures/synthetic-setting-b.pdf}
    \vspace{-0.6cm}
    \caption{\textbf{Results for synthetic experiments in Setting B.} Reported: ratio between the performance of TARFlow (CFRFlow with $\alpha = 0$) and representation networks with varying $\alpha$; mean $\pm$ SE over 15 runs. Lower is better. Here: $n_{\text{train}} = 500$, $d_\phi = 2$.}
    \label{fig:synthetic-setting-b}
    \vspace{-0.2cm}
\end{figure}


{\tiny$\blacksquare$}~\textbf{Setting C.} Here, we show how our \ORlearners ``undo'' the damage brought by too strict balancing, now including a possible RICB. For this, we use five different representation networks (CFR, BNN, RCFR, CFR-ISW, and BWCFR) as baselines, each with two types of balancing and $\alpha = 0.1$: Wasserstein metric (WM) and maximum mean discrepancy (MMD). \textbf{Results.} We report the results in Table \ref{tab:acic2016-setting-c} for the ACIC 2016 dataset collection (we refer to Appendix~\ref{app:experiments} for additional results for the synthetic dataset). Here, we filtered only the runs, where balancing representations deteriorated the performance in comparison to the vanilla versions of the representation networks, namely, TARNet for CFR, RCFR, CFR-ISW, and BWCFR; and BNN w/o balancing for BNN. $\Rightarrow$\,Again, our \ORlearners enhance the performance of the representation networks with too restrictive balancing. 

\begin{table}[ht]
    \vspace{-0.1cm}
      \caption{\textbf{Results for 77 semi-synthetic ACIC 2016 experiments in Setting C.} Reported: the percentage of runs, where our \ORlearners improve over representation networks. Here, $d_\phi = 8$.} \label{tab:acic2016-setting-c}
      \vspace{-0.5cm}
      \begin{center}
            \scriptsize
            \scalebox{0.77}{\input{tables/acic2016_setting_c}}
        \end{center}
    \vspace{-0.2cm}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \vspace{-0.2cm}
 \section{Implications} \label{sec:implications}
% \vspace{-0.2cm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

{\textbf{Choice of a target model.} In general, there is no nuisance-free way to do CATE/CAPOs model selection based solely on the observational data \citep{curth2023search}. Hence, in the absence of the ground-truth counterfactuals or at least experimental data, one cannot reliably choose among target models with different inputs (e.g., $V = \Phi(X)$ vs. $V = X$) or different hyperparameters (e.g., regularization strength). {We can even consider asymptotically-equivalent alternative variants of Neyman-orthogonal learners where constraints are enforced for the second-stage model (\eg, see Corollary~\ref{prop:alternative} in Appendix~\ref{app:proofs}). Yet, our choice of  \ORlearners with $V = \Phi(X)$ is based on (i)~a crucial inductive bias that \emph{the high-dimensional covariates lie on some low-dimensional manifold} and (ii)~a finite-sample consideration, that the representation network has learned it well in comparison to a second-stage model with an unstable loss (\eg, DR-learner with high inverse propensity weights). }} \textbf{Implication~1} $\Rightarrow$\, Our \ORlearners offer a constructive and reasonable way to choose the conditioning set $V$ for the second-stage model of Neyman-orthogonal learners.

\textbf{Orthogonality and balancing.} We discovered that the \emph{inductive bias for balancing is the exact opposite from the regularity conditions of Neyman-orthogonal learners}.
{In Sec.~\ref{sec:OR-learner-constrained-inv} and~\ref{sec:OR-learner-constrained-non-inv}, we showed that balancing works well when the lack of overlap coincides with the lack of potential outcomes/treatment effect heterogeneity (thus, these parts of covariate space will be ignored in the loss of the representation network).} On the other hand, Neyman-orthogonal learners do not rely on such an inductive bias and consider the areas with the lack of overlap as \emph{uncertain}. For example, the DR-learners would try to infinitely up-weight any observations in those areas (due to inverse propensity weights) and the R-learner would ignore them (assign the weights of zero). Even if the inductive bias (that the lack of overlap implies the lack of heterogeneity) can be assumed, it is still unclear how to choose an optimal amount of balancing \citep{curth2023search}. \textbf{Implication~2} $\Rightarrow$\, We thus advise against using balancing and suggest using \ORlearners with unconstrained representations instead.

\textbf{Beyond balancing.} Nevertheless, the theory presented in Sec.~\ref{sec:OR-learner-constrained-inv} and Sec.~\ref{sec:OR-learner-constrained-non-inv} is useful for other types of constrained representations rather than balancing (\eg, fair representations \citep{frauen2024fair}) or for representations learned in the self-/unsupervised way. \textbf{Implication~3} $\Rightarrow$\, Our \ORlearners provide a principled way to do Neyman-orthogonal causal quantities estimation that extends to any type of representations.

% \newpage
\section*{Impact Statement}

Our proposed OR-learners provide a unifying framework for representation learning and Neyman-orthogonal methods, offering improved estimation of causal quantities with the guarantees of double robustness and quasi-oracle efficiency. This advance can directly benefit critical applications in healthcare, economics, and public policy by enabling more reliable individualized decision-making. 


\bibliography{bibliography}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix
\onecolumn

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Extended Related Work} \label{app:extended-rw}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Our work aims to unify two streams of work, namely, representation learning methods (Sec.~\ref{app:extended-rw-representation-learning}) and Neyman-orthogonal two-stage learners (Sec.~\ref{app:extended-rw-orthogonal-learners}). We review both in the following and then discuss the implications for our work.

\subsection{Representation learning for estimating causal quantities} 
\label{app:extended-rw-representation-learning}

Several methods have been previously introduced for \emph{end-to-end} representation learning of CAPOs/CATE  \citep[see, in particular, the seminal works by][]{johansson2016learning,shalit2017estimating,johansson2022generalization}. Existing methods fall into three main streams: (1)~One can fit an \emph{unconstrained shared representation} to directly estimate both potential outcomes surfaces \citep[e.g., \textbf{TARNet};][]{shalit2017estimating}. (2)~Some methods additionally enforce a \emph{balancing constraint based on empirical probability metrics}, so that the distributions of the treated and untreated representations become similar \citep[e.g., \textbf{CFR} and \textbf{BNN};][]{johansson2016learning,shalit2017estimating}. Importantly, balancing based on empirical probability metrics is only guaranteed to perform a consistent estimation for \emph{invertible} representations since, otherwise, balancing leads to a \emph{representation-induced confounding bias} (RICB) \citep{johansson2019support,melnychuk2024bounds}. Finally, (3)~one can additionally perform \emph{balancing by re-weighting} the loss and the distributions of the representations with learnable weights \citep[e.g., \textbf{RCFR};][]{johansson2022generalization}. 

Table~\ref{tab:methods-comparison} provides a summary of the main representation learning methods for the estimation of causal quantities. Therein, we showed how different constraints imposed on the representations relate to the consistency of estimation and Neyman-orthogonality of the underlying methods. We highlight several important constrained representations below and discuss the implications for estimating causal quantities.

\begin{table*}[hbt]
    % \vspace{-0.3cm}
    \caption{Overview of representation learning methods for CAPOs/CATE estimation. Here, parentheses imply the possibility of an extension.}
    \label{tab:methods-comparison}
    \vspace{-0.2cm}
    \begin{center}
        \vspace{-0.2cm}
        \scalebox{1}{
            \scriptsize
            \begin{tabular}{p{4.2cm}|p{0.9cm}|p{1.4cm}p{1cm}p{2.1cm}|>{\centering\arraybackslash}p{1.6cm}|>{\centering\arraybackslash}p{1.7cm}|>{\centering\arraybackslash}p{0.9cm}}
                \toprule
                \multirow{2}{*}{Method} & \multirow{2}{*}{\begin{tabular}{l} Learner \\  type \end{tabular}} &  \multicolumn{3}{c|}{Constraints} & \multirow{2}{*}{\begin{tabular}{c} Consistency \\ of estimation \end{tabular}} & \multicolumn{2}{c}{Neyman-orthogonality}  \\
                \cmidrule(lr){3-5} \cmidrule(lr){7-8}& 
                       & Balancing & Invertibility & Disentanglement &  & CAPOs & CATE  \\
                \midrule
                TARNet \citep{shalit2017estimating, johansson2022generalization} & \multirow{2}{*}{PI} & \multirow{2}{*}{--} & \multirow{2}{*}{--} & \multirow{2}{*}{--}& \multirow{2}{*}{\cmark} &  \multirow{2}{*}{\xmark} & \multirow{2}{*}{\xmark}\\
                \midrule
                BNN \citep{johansson2016learning}; CFR \citep{shalit2017estimating, johansson2022generalization}; ESCFR \citep{wang2024optimal}  & \multirow{3}{*}{PI} &  \multirow{3}{*}{IPM} & \multirow{3}{*}{(any) / --} & \multirow{3}{*}{--} & \multirow{3}{*}{\xmark $\,$[\cmark: invertible]} & \multirow{3}{*}{\xmark} & \multirow{3}{*}{\xmark}  \\
                \midrule
                RCFR \citep{johansson2018learning,johansson2022generalization} & WPI & IPM + LW & (any) / -- & -- & \xmark $\,$[\cmark: invertible] & \xmark & \xmark  \\
                \midrule
                DACPOL \citep{atan2018counterfactual}; CRN \citep{bica2020estimating}; ABCEI \citep{du2021adversarial}; CT \citep{melnychuk2022causal}; MitNet \citep{guo2023estimating};  BNCDE \citep{hess2024bayesian} & \multirow{4}{*}{PI} &  \multirow{4}{*}{JSD} & \multirow{4}{*}{--} & \multirow{4}{*}{--} & \multirow{4}{*}{\xmark} & \multirow{4}{*}{\xmark} & \multirow{4}{*}{\xmark}\\
                \midrule
                SITE \citep{yao2018representation}& PI & LS & MPD & -- & \xmark $\,$[\cmark: invertible] & \xmark & \xmark \\
                \midrule
                DragonNet \citep{shi2019adapting} & PI / (DR) & -- & -- & -- & \cmark & (\cmark$^{\text{DR}_\text{K}}$) & (\cmark$^{\text{DR}}$)\\
                \midrule
                PM \citep{schwab2018perfect}; StableCFR \citep{wu2023stable} &\multirow{2}{*}{WPI} & \multirow{2}{*}{IPM + UVM} & \multirow{2}{*}{--} & \multirow{2}{*}{--} & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\xmark} & \multirow{2}{*}{\xmark} \\
                \midrule
                CFR-ISW \citep{hassanpour2019counterfactual}; & WPI & IPM + RP & -- & -- & \xmark & \xmark & \xmark \\
                \midrule
                DR-CFR \citep{hassanpour2019learning}; DeR-CFR \citep{wu2022learning} & \multirow{2}{*}{IPTW} & \multirow{2}{*}{IPM + CP} & \multirow{2}{*}{--} & \multirow{2}{*}{$\Phi = \{\Phi^a, \Phi^\Delta, \Phi^y\}$} & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\xmark $\,$[\cmark$^{\text{DR}}$: IPM = 0]} & \multirow{2}{*}{\xmark} \\ 
                \midrule
                DKLITE \citep{zhang2020learning} & PI & CV & RL  & -- & \xmark $\,$[\cmark: invertible] & \xmark & \xmark \\
                \midrule
                BWCFR \citep{assaad2021counterfactual} & IPTW & IPM + CP & -- & -- & \cmark & \mbox{\xmark $\,$[\cmark$^{\text{DR}}$: IPM = 0]} & \xmark \\
                \midrule
                SNet \citep{curth2021nonparametric,chauhan2023adversarial} & \multirow{2}{*}{DR} & \multirow{2}{*}{--} & \multirow{2}{*}{--} & \mbox{$\Phi = \{\Phi^a, \Phi^\Delta, \Phi^y,$} $\null\quad\quad\quad \Phi^{\mu_0}, \Phi^{\mu_1}\}$ & \multirow{2}{*}{\cmark} & \multirow{2}{*}{(\cmark$^{\text{DR}_\text{K}}$)} & \multirow{2}{*}{\cmark$^{\text{DR}}$}\\
                \midrule GWIB \citep{yang2024revisiting} & PI & MI & -- & -- & \xmark & \xmark & \xmark \\
                \midrule
                \textbf{\ORlearners} (our paper) & DR / R & (any) & NFs / -- & (any) & \cmark &\cmark$^{\text{DR}_\text{FS}}$, \cmark$^{\text{DR}_\text{K}}$ & \cmark$^{\text{DR}}$, \cmark$^{\text{R}}$\\
                \bottomrule
                \emph{Legend}:\\ 
                \multicolumn{8}{p{17cm}}{\quad $\bullet$ Learner type: plug-in (PI); weighted plug-in (WPI); inverse propensity of treatment weighted (IPTW); doubly robust (DR); Robinson's / residualized (R)} \\
                \multicolumn{8}{p{17cm}}{\quad $\bullet$ Balancing: integral probability metric (IPM); learnable weights (LW); Jensen-Shannon divergence (JSD); local similarity (LS); upsampling via matching (UVM); \mbox{\null \quad\quad representation propensity (RP)}; covariate propensity (CP); counterfactual variance (CV); mutual information (MI)} \\
                \multicolumn{8}{p{17cm}}{
                \quad $\bullet$ Invertibility: middle point distance (MPD); reconstruction loss (RL); normalizing flows (NFs)}\\
                \multicolumn{8}{p{17cm}}{\quad $\bullet$ Neyman-orthogonality: DR-learner in the style of \citet{kennedy2023towards} (${\text{DR}_{\text{K}}}$); DR-learner in the style of 
                \citet{foster2023orthogonal} (${\text{DR}_{\text{FS}}}$)}
            \end{tabular}
        }
    \end{center}
    \vspace{-0.1cm}
\end{table*}


\textbf{Disentanglement.} \citet{shi2019adapting} proposed to use the shared representation, as in TARNet, to additionally estimate the propensity score. \citet{hassanpour2019learning,wu2022learning} suggested to disentangle the representation of TARNet or CFR, so that different parts of the disentangled representation can serve for estimating different nuisance functions (potential outcomes surfaces and propensity score). Based on their work, \citet{curth2021nonparametric} and \citet{chauhan2023adversarial} developed a general framework for disentangled representation based on TARNet as a flexible estimator of nuisance functions for different CATE meta-learners.   

\textbf{Balancing and invertibility.} Following CFR and BNN, several works proposed alternative strategies for \emph{balancing representations with empirical probability metrics}, \eg, based on adversarial learning \citep{atan2018counterfactual,curth2021inductive,du2021adversarial,melnychuk2022causal,guo2023estimating}; metric learning \citep{yao2018representation}; counterfactual variance minimization \citep{zhang2020learning}; and empirical mutual information \citep{yang2024revisiting}. To enforce \emph{invertibility} (and, thus, consistency of estimation), several works suggested metric learning heuristics \citep{yao2018representation} or reconstruction loss \citep{zhang2020learning}.  

Other methods, extended \emph{balancing by re-weighting}, as in RCFR but, for example, with weights based on matching \citep{schwab2018perfect,wu2023stable}; or with inverse propensity of treatment weights (IPTW) \citep{hassanpour2019counterfactual,hassanpour2019learning,assaad2021counterfactual,wu2022learning}.   

\textbf{Validity of representations for consistent and orthogonal estimation.} As mentioned previously, balancing representations with empirical probability metrics without strictly enforcing invertibility generally leads to \emph{inconsistent estimation based on representations}. This issue was termed as a \emph{representation-induced adaptation error} \citep{johansson2019support} in the context of unsupervised domain adaptation and as a \emph{representation-induced confounding bias (RICB)} \citep{melnychuk2024bounds} in the context of estimation of causal quantities. More generally, the RICB can be recognized as a type of runtime confounding \citep{coston2020counterfactual}, \ie, when only a subset of covariates is available for the estimation of the causal quantities. Several works offered a solution to circumvent the RICB and achieve consistency. For example, \citet{assaad2021counterfactual} employed IPTW based on original covariates, and \citet{melnychuk2024bounds} used a sensitivity model to perform a partial identification. However, to the best of our knowledge, \underline{no} Neyman-orthogonal method was proposed to resolve the RICB (see Fig.~\ref{fig:rw-consistentcy-orthogonality}).  

\begin{figure}[h]
    \vspace{-0.5cm}
    \centering
    \includegraphics[width=\textwidth]{figures/rw-consistentcy-orthogonality.pdf}
    \vspace{-0.9cm}
    \caption{Flow chart of consistency and Neyman-orthogonality for representation learning methods. Our \ORlearners fill the gaps shown by \textcolor{red}{red dotted lines}.}
    \label{fig:rw-consistentcy-orthogonality}
    \vspace{-0.1cm}
\end{figure} 

{\textbf{Note on non-neural representations.} Multiple works also explored the use of non-neural representations for the estimation of causal quantities, also known under the umbrella term of \emph{scores}. Examples include propensity/balancing scores \citep{rosenbaum1983central,antonelli2018doubly}, prognostic scores \citep{hansen2008prognostic,huang2017joint,luo2020matching,antonelli2018doubly,d2021deconfounding}, and deconfounding scores \citep{d2021deconfounding}. However, we want to highlight that these works focus on \underline{different}, rather simpler than ours settings:
\begin{itemize}[leftmargin=0.5cm,itemsep=-0.55mm]
    \item \textit{Propensity, balancing, and deconfounding scores} \citep{rosenbaum1983central} were employed the estimate \emph{average} causal quantities \citep{antonelli2018doubly,d2021deconfounding}. Examples are average potential outcomes (APOs) and average treatment effect (ATE). This is because they lose information about the heterogeneity of the potential outcomes/treatment effect. In our work, on the other hand, we study a general class of \emph{heterogeneous} causal quantities, namely, representation-conditional CAPOs/CATE.
    \item \textit{Prognostic scores}  \citep{hansen2008prognostic} can be used for both averaged \citep{antonelli2018doubly,luo2020matching,d2021deconfounding} and heterogeneous causal quantities \citep{huang2017joint}. In \citet{huang2017joint,luo2020matching}, they are used in the context of a sufficient covariate dimensionality reduction. Yet, these works either (i)~make simplifying strong assumptions \citep{antonelli2018doubly,luo2020matching,d2021deconfounding}, so that the prognostic scores coincide with the expected covariate-conditional outcome; or (ii)~consider only linear prognostic scores \citep{huang2017joint,luo2020matching}. To the best of our knowledge, the first practical method for non-linear, learnable representations was proposed in \citet{johansson2016learning,shalit2017estimating,johansson2022generalization}.
\end{itemize}

Hence, the above-mentioned works operate in much simpler settings and, therefore, are \underline{not} relevant baselines for our work.
}



\subsection{Two-stage meta-learners}
\label{app:extended-rw-orthogonal-learners}

\textbf{Meta-learners.} Causal quantities can be estimated using model-agnostic methods, so-called \emph{meta-learners} \citep{kunzel2019metalearners}. Meta-learners typically combine multiple models to perform two-stage learning, namely, (1)~nuisance functions estimation and (2)~target model fitting. As such, meta-learners must be instantiated with some machine learning model (e.g., a neural network) to perform (1) and (2). Meta-learners have several practical advantages \citep{morzywolek2023general}: (i)~they oftentimes offer favorable theoretical guarantees such as Neyman-orthogonality; (ii)~they can address the causal inductive bias that the CATE is ``simpler'' than CAPOs \citep{curth2021inductive}, and (iii)~the target model obtains a clear interpretation as a projection of the ground-truth CAPOs/CATE on the target model class. 



A broad variety of meta-learners have been developed. Notable examples include X- and U-learners \citep{kunzel2019metalearners}, R-learner \citep{nie2021quasi}, DR-learner \citep{kennedy2023towards,curth2020estimating}, and IVW-learner \citep{fisher2024inverse}. Several works extended the theory of targeted maximum likelihood estimation \citep{van2011targeted} and proposed Neyman-orthogonal single-stage learners. Examples therefore are the EP-learner for CATE \citep{van2024combining} and the i-learner for CAPOs \citep{vansteelandt2023orthogonal}. Furthermore, \citet{curth2021nonparametric} provided a comparison of meta-learners implemented via neural networks, where disentangled unconstrained representations are used solely to estimate (1)~nuisance functions but not as inputs to the (2)~target model. 




\textbf{Neyman-orthogonal learners.} Neyman-orthogonality \citep{foster2023orthogonal}, or double/debiased machine learning \citep{chernozhukov2017double}, directly extend the idea of semi-parametric efficiency to infinite-dimensional target estimands such as CAPOs and the CATE. Informally, Neyman-orthogonality means that the population loss of the target model is first-order insensitive to the misspecification of the nuisance functions. Examples of Neyman-orthogonal learners are DR- and i-learners for CAPOs \citep{vansteelandt2023orthogonal}; and DR-, R-, IVW-, and EP-learners for CATE \citep{morzywolek2023general}.  

\textbf{Choice of target models.} Existing works on meta-learners usually build the (2)~second-stage target model based on the \emph{original covariates}, for example, the comparative study in \citet{curth2021nonparametric}. At the same time, the theory of meta-learners \citep{morzywolek2023general,vansteelandt2023orthogonal} allows for the target model to depend on \emph{any subset of covariates} and to still preserve all the favorable properties (i)--(iii). However, it remains unclear, how different target models relate to each other in terms of (a)~performance and (b)~interpretation if they are based on different \emph{leaned representations} of covariates. In this paper, we study these questions in detail and introduce \ORlearners, a novel class of Neyman-orthogonal learners where the target model is based on \underline{any} representation (with or without constraints).

\subsection{Implications for our work}


\textbf{Balancing and finite-sample generalization error.} In the original works on balancing representations \citep{shalit2017estimating,johansson2022generalization}, the authors provided finite-sample generalization error bounds for any estimator of CAPOs/CATE based on a factual estimation error and a distributional distance between treated and untreated population. Therein, the authors employed integral probability metrics as the distributional distance. These bounds were further improved with other distributional distances, \eg, counterfactual variance \citep{zhang2020learning}, $\chi^2$-divergence \citep{csillag2024generalization}, and KL-divergence \citep{huang2024unveiling}. Importantly, the work by \citet{shalit2017estimating,johansson2022generalization} suggests that the large distributional distance only \emph{acknowledges the lack of overlap between treated and untreated covariates} (and, hence, the hardness of the estimation) but it \emph{does not instruct how much balancing needs to be applied}. In our work, we confirm that the optimal amount of balancing is indeed not related to the generalization error bounds.

\begin{figure*}[h!]
    \centering
    \vspace{-0.3cm}
    \includegraphics[width=0.95\textwidth]{figures/repr-overview.pdf}
    \vspace{-0.3cm}
    \caption{{Overview of the connections between representation learning and the estimation of causal quantities.} (i)~Representation learning can help in estimating causal quantities by providing tools to address different causal inductive biases (\eg, balancing, invertibility, and disentanglement). Conversely, (ii)~the estimation of causal quantities can be performed based on general-purpose constrained representations (\eg, fair representations or representations that are learned in an un-/self-supervised way). Our \ORlearners can be used in both cases.}
    % \vspace{-0.5cm}
    \label{fig:repr-overview}
\end{figure*}

\textbf{Estimation of causal quantities for general-purpose learned representations.} Other constraints may be applied to the representations, for example, to achieve algorithmic fairness \citep{zemel2013learning,madras2018learning}. Some works combined Neyman-orthogonal learners and fairness constraints, but different from our setting. For example, \citet{kim2023fair} provided a DR-learner for fair CATE estimation based on the linear combination of the basis functions; and \citet{frauen2024fair} built fair representations for policy learning with DR-estimators of policy value. The latter work, nevertheless, can be seen as a special case of our general \ORlearners (see Fig.~\ref{fig:repr-overview}). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Background materials} \label{app:background}

In this section, we provide the formal definitions of Neyman-orthogonality, H{\"o}lder smoothness, and integral probability metrics; we state the identifiability and smoothness assumptions; and we offer an overview of meta-learners for CAPOs/CATE estimation.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
   


\subsection{Neyman-orthogonality and double robustness} \label{app:background-NO-DR}

We use the following additional notation: $\norm{\cdot}_{L_p}$ denotes the $L_p$-norm with  $\norm{f}_{L_p} = {\mathbb{E}(\abs{f(Z)}^p)}^{1/p}$, $a \lesssim b$ means there exists $C \ge 0$ such that $a \le C \cdot b$, and $X_n = o_{\mathbb{P}}(r_n)$ means $X_n/r_n \stackrel{p}{\to} 0$.

\begin{definition}[Neyman-orthogonality \citep{foster2023orthogonal,morzywolek2023general}]
    A risk $\mathcal{L}$, is called \emph{Neyman-orthogonal} if its pathwise cross-derivative equals zero, namely,
    \begin{equation} \label{eq:neym-orth-def}
         D_\eta D_g {\mathcal{L}}(g^*, \eta)[g- g^*, \hat{\eta} - \eta] = 0 \quad \text{for all } g \in \mathcal{G},
    \end{equation}
    where $D_f F(f)[h] = \frac{\diff}{\diff{t}} F (f + th) \vert_{t=0}$ and $D_f^k F(f)[h_1, \dots, h_k] = \frac{\partial^k}{\partial{t_1} \dots \partial{t_k}} F (f + t_1 h_1 + \dots + t_k h_k)  \vert_{t_1=\dots=t_k = 0}$ are pathwise derivatives \citep{foster2023orthogonal}, where $g^* = \argmin_{g \in \mathcal{G}} \mathcal{L}(g, \eta)$, and $\eta$ is the ground-truth nuisance function. 
\end{definition}

Informally, this definition means that the risk is first-order insensitive wrt. to the misspecification of the nuisance functions.

{
\begin{definition}[Double robustness]\label{def:rate-dr}
An estimator \(\hat{g} = \argmin_{g \in \mathcal{G}} \mathcal{L}(g, \hat{\eta})\) of \(g^* = \argmin_{g \in \mathcal{G}}\mathcal{L}({g}, {\eta})\) is said to be \emph{double robust} if, for any estimators \(\hat{\mu}_a^x\) and $\hat{\pi}_1^x$ of the nuisance functions $\mu_a^x$ and $\pi_1^x$, it holds that
\begin{equation} \label{eq:rate-dr-def}
    \norm{\hat{g} - g^*}_{L_2}^2 \lesssim \mathcal{L}(\hat{g}, \hat{\eta}) - \mathcal{L}({g}^*, \hat{\eta}) + \norm{\hat{\pi}_1^x - \pi_1^x}^2_{L_2} \norm{\hat{\mu}_a^x - \mu_a^x}^2_{L_2} ,
\end{equation}
where \(\mathcal{L}(\hat{g}, \hat{\eta}) - \mathcal{L}({g}^*, \hat{\eta})\) is the difference between the risks of the estimated target model and the optimal target model where the estimated nuisance functions are used. 
\end{definition}

\begin{definition}[Quasi-oracle efficiency]\label{def:quasi-oracle}
An estimator \(\hat{g} = \argmin_{g \in \mathcal{G}} \mathcal{L}(g, \hat{\eta})\) of \(g^* = \argmin_{g \in \mathcal{G}}\mathcal{L}({g}, {\eta})\) is said to be \emph{quasi-oracle efficient} if the estimators \(\hat{\mu}_a^x\) and $\hat{\pi}_1^x$ of the nuisance functions $\mu_a^x$ and $\pi_1^x$ are allowed to have slow rates of convergence, $o_{\mathbb{P}}(n^{-1/4})$, and the following still holds asymptotically:
\begin{equation} \label{eq:oracle-eff}
    \norm{\hat{g} - g^*}_{L_2}^2 \lesssim \mathcal{L}(\hat{g}, \hat{\eta}) - \mathcal{L}({g}^*, \hat{\eta}) + o_{\mathbb{P}}(n^{-1/2}),
\end{equation}
where \(\mathcal{L}(\hat{g}, \hat{\eta}) - \mathcal{L}({g}^*, \hat{\eta})\) is the difference between the risks of the estimated target model and the optimal target model where the estimated nuisance functions are used. 
\end{definition}}

\subsection{H{\"o}lder smoothness} \label{app:background-holder}

\begin{definition}[H{\"o}lder smoothness] \label{def:holder}
Let $\beta > 0, C > 0$, and $\mathcal{X} \subseteq \mathbb{R}^{d_x}$. A function $f: \mathcal{X} \rightarrow \mathbb{R}$ is said to be \emph{$\beta$-H{\"o}lder smooth} (i.e., belongs to the H{\"o}lder class $C^\beta(\mathcal{X})$) if it satisfies the following conditions:
\begin{enumerate}
    \item $f$ is $\lfloor \beta \rfloor$ times continuously differentiable on $\mathcal{X}$, where $\lfloor \beta \rfloor$ denotes the largest integer less than or equal to $\beta$.
    \item All partial derivatives of $f$ of order $\lfloor \beta \rfloor$ satisfy the H{\"o}lder condition of order $\beta - \lfloor \beta \rfloor$. Specifically, there exists a (Lipschitz) constant $C > 0$ such that, for all multi-indices {$\alpha$} with $|\alpha| = \lfloor \beta \rfloor$ and for all $x, x' \in \mathcal{X}$, one has
    \begin{equation}
        \left| D^\alpha f(x) - D^\alpha f(x') \right| \leq C \| x - x' \|^{\beta - \lfloor \beta \rfloor}_2,
    \end{equation}
    where $D^\alpha f$ denotes the partial derivative of $f$ corresponding to the multi-index $\alpha$, and $\| \cdot \|_2$ is the Euclidean norm.
\end{enumerate}
\end{definition}

In our context:
\begin{itemize}[leftmargin=0.5cm,itemsep=-0.55mm]
    \item For each treatment level $a$, the function $\mu_a^x(\cdot)$ is assumed to be $\beta_a$-H{\"o}lder smooth with $\beta_a > 0$.
    \item The propensity score $\pi^x_a(\cdot)$ is assumed to be $\gamma$-H{\"o}lder smooth with $\gamma > 0$.
    \item The conditional average treatment effect function $\tau^x(\cdot)$ is assumed to be $\delta$-H{\"o}lder smooth with $\delta > 0$.
\end{itemize}



\subsection{Integral probability metrics} \label{app:background-ipm}

Integral probability metrics (IPMs) are a broad class of distances between probability distributions, defined in terms of a family of functions $\mathcal{F}$. Given two probability distributions \( \mathbb{P}(Z_1) \) and \( \mathbb{P}(Z_2) \) over a domain \( \mathcal{Z} \), an IPM measures the maximum difference in expectation over a class of functions \( \mathcal{F} \):
\begin{equation}
    \operatorname{IPM}(\mathbb{P}(Z_1), \mathbb{P}(Z_2)) = \sup_{f \in \mathcal{F}} \left| \mathbb{E}(f(Z_1)) - \mathbb{E}(f(Z_2)) \right|.
\end{equation}
In this framework, \( \mathcal{F} \) specifies the allowable ways in which the difference between the distributions can be measured. Depending on the choice of \( \mathcal{F} \), different IPMs arise. 

\textbf{Wasserstein metric (WM).} The Wasserstein metric   is a specific IPM where the function class \( \mathcal{F} \) is the set of 1-Lipschitz functions, which are functions where the absolute difference between outputs is bounded by the absolute difference between inputs:
\begin{equation}
    W(\mathbb{P}(Z_1), \mathbb{P}(Z_2)) = \sup_{f \in \mathcal{F}_{1}} \left| \mathbb{E}(f(Z_1)) - \mathbb{E}(f(Z_2)) \right|.
\end{equation}
This metric can be interpreted as the minimum cost required to transport probability mass from one distribution to another, where the cost is proportional to the distance moved.

\textbf{Maximum mean discrepancy (MMD).} Another popular example is the maximum mean discrepancy, where the function class \( \mathcal{F} \) corresponds to functions in the unit ball of a reproducing kernel Hilbert space (RKHS), $\mathcal{F}_{\text{RKHS, 1}} = \{f \in \mathcal{H}: \norm{f}_{\mathcal{H}} \le 1\}$:
\begin{equation}
    \text{MMD}(\mathbb{P}(Z_1), \mathbb{P}(Z_2)) = \sup_{f \in \mathcal{F}_{\text{RKHS},1}} \left| \mathbb{E}(f(Z_1)) - \mathbb{E}(f(Z_2)) \right|.
\end{equation}
The MMD is often used in hypothesis testing and in training generative models, particularly when the distributions are defined over high-dimensional data.


\subsection{Assumptions} \label{app:background-ass}

\textbf{Identifiability.} The identification of CAPOs/CATE from observational data requires further assumptions, which are standard in the literature \citep{rubin1974estimating}. The reason is that the fundamental problem of causal inference: the counterfactual outcomes, $Y[1-A]$, are never observed, while the potential outcomes are only partially observed, \ie, $Y = A\,Y[1] + (1-A)Y[0]$. Therefore, it is standard to assume (i)~\emph{consistency}: if $A = a$, then $Y[a] = Y$; (ii)~\emph{overlap}: $\mathbb{P}(0 < \pi^x_a(X) < 1) = 1$;  and (iii)~\emph{unconfoundedness}: $(Y[0], Y[1]) \ind A \mid X$. Given the assumptions (i)--(iii), both CAPOs and CATE are identifiable from observational data as expected covariate-conditional outcomes, $\xi_a^x(x) = \mu_a^x(x)$, or as the difference of expected covariate-conditional outcomes, $\tau^x(x) = \mu_1^x(x) - \mu_0^x(x)$, respectively. 

\textbf{Smoothness.} To consistently estimate CAPOs and CATE (\eg, with neural networks), we follow \citet{curth2021nonparametric,kennedy2023towards} and make regular (H{\"o}lder) smoothness assumptions. We assume the ground-truth response function $\mu_a^x(\cdot)$ to be $\beta_a$-smooth, the ground-truth propensity score $\pi^x_a(\cdot)$ to be $\gamma$-smooth, and $\tau^x(\cdot)$ to be $\delta$-smooth (for $\beta_a, \gamma, \delta > 0$). 

\subsection{Meta-learners for CAPOs and CATE estimation} \label{app:background-meta-learners}

\textbf{Plug-in learners.} A na{\"i}ve way to estimate CAPOs and CATE is to simply estimate $\hat{\mu}_0^x(x)$ and $\hat{\mu}_1^x(x)$ and `plug them into' the identification formulas for CAPOs and CATE. For example, an S-learner (S-Net) fits a single model with the treatment as an input, while a T-leaner (T-Net) builds two models for each treatment \citep{kunzel2019metalearners}. Many end-to-end representation learning methods, such as TARNet \citep{shalit2017estimating} and BNN without balancing \citep{johansson2016learning}, can be seen as variants of the plug-in learner: In the end-to-end fashion, they build a representation of the covariates $\phi = \Phi(x) \in \mathit{\Phi} \subseteq \mathbb{R}^{d_\phi}$ and then use $\phi$ to estimate $\hat{\mu}_a^x(x) = \hat{\mu}_a^\phi(\Phi(x))$ with the S-Net (BNN w/o balancing) or the T-Net (TARNet).

Yet, plug-in learners have several major drawbacks \citep{morzywolek2023general,vansteelandt2023orthogonal}. (a)~They do not account for the selection bias, namely, that $\hat{\mu}_0^x$ is estimated better for the treated population and $\hat{\mu}_1^x$ for untreated. (b)~In the case of CATE estimation, the plug-in learners might additionally fail to address the causal inductive bias that the CATE is a ``simpler'' function than both CAPOs \citep{kunzel2019metalearners,curth2021inductive}, as it is impossible to add additional smoothing for the CATE model separately from CAPOs models. (c)~It is also unclear how to consistently estimate the CAPOs/CATE depending on the subset of covariates $V \subseteq X$ with the aim of reducing the variance of estimation. For example, it is unclear how to estimate representation-level CAPOs, $\xi_a^\phi(\phi) = \mathbb{E}(Y[a] \mid \Phi(X) = \phi)$, and CATE, $\tau^\phi(\phi) = \mathbb{E}(Y[1] - Y[0] \mid \Phi(X) = \phi)$, especially when the representations are constrained.  

\textbf{Working model \& target risks.} To address the shortcomings of plug-in learners, two-stage meta-learners were proposed (see Appendix~\ref{app:extended-rw-orthogonal-learners}). These proceed in three steps. 

\textbf{(i)}~First, one chooses a \emph{target working model class} $\mathcal{G} = \{g(\cdot): \mathcal{V} \subseteq \mathcal{X} \to \mathcal{Y}\}$ such as, for example, neural networks. {A target model takes a (possibly confounded) subset $V$ of the original covariates $X$ as an input and outputs the prediction of causal quantities conditioned on $V$, namely, CAPOs $\xi_a^v(v) = \mathbb{E}(Y[a] \mid V = v)$ or CATE $\tau^v(v) = \mathbb{E}(Y[1] - Y[0] \mid V = v)$.}

\textbf{(ii)}~Then, two-stage meta-learners formulate one of the \emph{target risks} for $g(v)$, where $v \in \mathcal{V}$. There are multiple choices for choosing a target risk, each with different interpretations and implications for finite-sample two-stage estimation. For example, two usual target risks for CAPOs are based on the MSE \citep{vansteelandt2023orthogonal}: 
\begin{align} \label{eq:risk-repr-capos-app}
      \mathcal{L}_{\xi_a}(g, \eta) = \mathbb{E}\left( \mu^x_a(X) - g(V)\right)^2 \qquad \text{and} \qquad \mathcal{L}_{Y[a]}(g, \eta) = \mathbb{E}\left( Y[a] - g(V)\right)^2,
\end{align}
where $V \subseteq X$, $\eta = (\mu^x_a, \pi^x_a)$ are nuisance functions (expected covariate-conditional outcomes and covariate propensity score) that influence the target risks. Minimizers of both $\mathcal{L}_{Y[a]}$ and $\mathcal{L}_{\xi_a}$ would be the same if we had access to infinite data for potential outcomes $Y[a]$ and the ground-truth expected covariate-conditional outcomes $\mu^x_a$. Yet, the values of both $\mathcal{L}_{Y[a]}$ and $\mathcal{L}_{\xi_a}$ are generally different, which influences finite-sample two-stage learning. At the same, CATE only allows for an MSE target risk, similar to $\mathcal{L}_{\xi_a}$ \citep{morzywolek2023general}:\footnote{An analogue to the first target risk of CAPOs, namely, $\mathcal{L}_{Y[1]-Y[0]}(g) = \mathbb{E}\left( (Y[1] - Y[0]) -  g(V)\right)^2$, contains a counterfactual expression,  $Y[1]-Y[0]$, and is thus,unidentifiable.}
\begin{align} \label{eq:risk-repr-cate-app}
    &\mathcal{L}_\tau(g, \eta) = \mathbb{E}\left( (\mu^x_1(X) - \mu^x_0(X)) - g(V)\right)^2.
\end{align}
Also, for CATE estimation, we can consider an {overlap-weighted MSE alternative of $\mathcal{L}_\tau(g)$ \citep{foster2023orthogonal,morzywolek2023general}}:
\begin{equation} \label{eq:risk-repr-cate-overlap-app}
    \mathcal{L}_{\pi_0 \pi_1 \tau}(g, \eta) = \mathbb{E}\left[\pi^x_0(X)\,\pi^x_1(X) \left((\mu^x_1(X) - \mu^x_0(X)) - g(V)\right)^2\right].
\end{equation}

{Unlike the plug-in learners, the population minimizers of the target risks in Eq.~\eqref{eq:risk-repr-capos-app} and \eqref{eq:risk-repr-cate-app} can recover the representation-level CAPOs/CATE.

\begin{numlemma}{8}[Identifiability of $V$-conditional causal quantities] \label{prop:rep-id}
    Assume that the ground-truth $V$-conditional CAPOs and CATE are contained in the working model class, \ie, $\xi_a^v \in \mathcal{G}$ and $\tau^v \in \mathcal{G}$. Then, the $V$-conditional CAPOs/CATE are identifiable as population minimizers of the following target risks:  
    \begin{align}
        \xi_a^v(\cdot) & = \argmin_{g \in \mathcal{G}} \mathcal{L}_{Y[a]}(g, \eta) = \argmin_{g \in \mathcal{G}} \mathcal{L}_{\xi_a}(g, \eta), \\
        \tau^v (\cdot) &= \argmin_{g \in \mathcal{G}} \mathcal{L}_{\tau}(g, \eta) 
    \end{align}
    where $\mathcal{L}_{Y[a]}$ and $\mathcal{L}_{\xi_a}$ are given by Eq.~\eqref{eq:risk-repr-capos-app} and where $\mathcal{L}_{\tau}$ is given by Eq.~\eqref{eq:risk-repr-cate-app}. Furthermore, if the overlap-weighted $V$-conditional CATE $\tau^v_{\pi_0 \pi_1}(v) = \mathbb{E}(\pi^x_0(X)\,\pi^x_1(X) (\mu^x_1(X) - \mu^x_0(X)) \mid V = v)$  is contained in the working model class, \ie, $\tau^v_{\pi_0 \pi_1} \in \mathcal{G}$, the overlap-weighted $V$-conditional CATE is identifiable as a population minimizer of target risk of the R-learner:
    \begin{align}
        \tau^v_{\pi_0 \pi_1}(\cdot) =  \argmin_{g \in \mathcal{G}}{\mathcal{L}}_{\pi_0\pi_1\tau}(g, {\eta}),
    \end{align}
    where  ${\mathcal{L}}_{\pi_0\pi_1\tau}$ is given by Eq.~\eqref{eq:risk-repr-cate-overlap-app}.
\end{numlemma}
\begin{proof}
    The proof is adapted from \citet{vansteelandt2023orthogonal,morzywolek2023general}. First, it is easy to see that $V$-conditional CAPOs and CATE are identifiable, given the ground-truth nuisance functions (e.g., via G-computation formulas):
    \begin{align}
        \tau^v(v) &= \mathbb{E}(Y[1] - Y[0] \mid V = v) = \xi_1^v(v) - \xi_0^v(v), \\
        \xi_a^v(v) &= \mathbb{E}(Y[a] \mid V = v) \stackrel{(*)}{=} \mathbb{E}(\mathbb{E}(Y[a] \mid X) \mid V = v) \stackrel{\text{Ass. (iii)}}{=}  \mathbb{E}(\mathbb{E}(Y[a] \mid X, A = a) \mid V = v) \\
        & \stackrel{\text{Ass. (i)}}{=} \mathbb{E}(\mathbb{E}(Y \mid X, A = a) \mid V = v) = \mathbb{E}(\mu^x_a(X) \mid V = v),
    \end{align}
    where $(*)$ holds due to the law of iterated expectation. 
    
    Then, due to the properties of the mean squared error, the last expression is also a population minimizer of the following target risk:
    \begin{align}
        \xi_a^v(v) = \mathbb{E}(\mu^x_a(X) \mid V = v) = \argmin_{g \in \mathcal{G}} \mathbb{E}\big(\mu^x_a(X) - g(V)\big)^2 = \argmin_{g \in \mathcal{G}} \mathcal{L}_{\xi_a}(g, \eta).
    \end{align}
    For the same reason, $\tau^v(v)$ is a population minimizer of the risk of the DR-learner, i.e., $\mathcal{L}_{\tau}$; and $\tau^v_{\pi_0 \pi_1}(v)$ is a population minimizer of the risk of the R-learner, i.e., ${\mathcal{L}}_{\pi_0\pi_1\tau}$. Additionally, the risk $\mathcal{L}_{Y[a]}$ has the same population minimizer as $\mathcal{L}_{\xi_a}$:
    \begin{align}
        & \argmin_{g \in \mathcal{G}} \mathcal{L}_{Y[a]}(g, \eta) = \argmin_{g \in \mathcal{G}} \mathbb{E}\left( Y[a] - g(V)\right)^2 \\ 
        =& \argmin_{g \in \mathcal{G}}\Big[\mathbb{E}\left( Y[a] - \mu^x_a(X)\right)^2 + 2 \mathbb{E}\left( Y[a] - \mu^x_a(X)\right)\left(\mu^x_a(X) - g(V)\right) + \mathbb{E}\left( \mu^x_a(X) - g(V)\right)^2 \Big] \\
        =& \argmin_{g \in \mathcal{G}}\Big[ 2 \mathbb{E}\big(\left(\mu^x_a(X) - g(V)\right) \, \mathbb{E}\left( Y[a] - \mu^x_a(X) \mid X \right) \big) + \mathbb{E}\left( \mu^x_a(X) - g(V)\right)^2 \Big]  \\
        =& \argmin_{g \in \mathcal{G}} \mathbb{E}\left( \mu^x_a(X) - g(V)\right)^2 = \argmin_{g \in \mathcal{G}} \mathcal{L}_{\xi_a}(g, \eta).
    \end{align}
\end{proof}


\textbf{(iii)}~In the last step, two-stage meta-learners minimize a chosen target risk $\hat{\mathcal{L}}(g, \hat{\eta})$, which is estimated using observational data and estimated at the first-stage nuisance functions $\hat{\eta}$. The latest step then yields so-called \emph{Neyman-orthogonal learners} when the target risk is estimated with semi-parametric efficient estimators \citep{robins1995semiparametric,foster2023orthogonal}. 

\textbf{Neyman-orthogonal learners.} Efficient estimation of the target risks introduces the well-known class of Neyman-orthogonal learners \citep{foster2023orthogonal}. 
\begin{itemize}[leftmargin=0.5cm,itemsep=-0.55mm]
\item CAPOs: For example, efficient estimators of MSE target risks for CAPOs yield two DR-learners with the following losses:
\begin{align}
    & \hat{\mathcal{L}}_{\xi_a}(g, \hat{\eta}) = \mathbb{P}_n \bigg\{ \bigg(\frac{\mathbbm{1}\{A = a\}}{\hat{\pi}^x_a(X)} \big( Y - \hat{\mu}_a^x(X) \big) + \hat{\mu}_a^x(X) - g(V)\bigg)^2 \bigg\}, \label{eq:DR-learner-K} \\
    & \hat{\mathcal{L}}_{Y[a]}(g, \hat{\eta}) = \mathbb{P}_n \bigg\{ \frac{\mathbbm{1}\{A = a\}}{\hat{\pi}^x_a(X)} \big( Y - g(V)\big)^2  +  \bigg(1 - \frac{\mathbbm{1}\{A = a\}}{\hat{\pi}^x_a(X)}\bigg) \, \big(\hat{\mu}_a^x(X) - g(V)\big)^2  \bigg\}. \label{eq:DR-learner-FS}
\end{align}
The first learner, $\hat{\mathcal{L}}_{\xi_a}(g, \hat{\eta})$, is known as the DR-learner in the style of \citet{kennedy2023towards}, while the second one, $\hat{\mathcal{L}}_{Y[a]}(g, \hat{\eta})$, is known as the DR-leaner in the style of \citet{foster2023orthogonal}. 

\item CATE: Here, an efficient estimator for target MSE, ${\mathcal{L}}_{\tau}(g, {\eta})$, is the DR-learner in the style of \citet{kennedy2023towards}; and an efficient estimator for overlap-weighted MSE ${\mathcal{L}}_{\pi_0\pi_1\tau}(g, {\eta})$ is the R-learner \citep{nie2021quasi} with the following losses:
\begin{align}
    &\hat{\mathcal{L}}_{\tau}(g, \hat{\eta}) = \mathbb{P}_n \bigg\{ \bigg(\frac{A - \hat{\pi}^x_1(X)} {\hat{\pi}^x_0(X) \, \hat{\pi}^x_1(X)} \big( Y - \hat{\mu}_A^x(X) \big) + \hat{\mu}_1^x(X) - \hat{\mu}_0^x(X) - g(V)\bigg)^2 \bigg\}, \\
    &\hat{\mathcal{L}}_{\pi_0\pi_1\tau}(g, \hat{\eta}) = \mathbb{P}_n \bigg\{\Big(\big(Y - \hat{\mu}^x(X)\big) - \big(A - \hat{\pi}_1^x(X)\big) g(V)\Big)^2 \bigg\},
\end{align}
where ${\mu}^x(X) = \mathbb{E}(Y \mid X = x) = {\pi}_1^x(X)\, {\mu}_1^x(X) + {\pi}_0^x(X) \, {\mu}_0^x(X) $.
\end{itemize}

Apart from addressing the issues of plug-in learners (a)--(c), Neyman-orthogonal learners provide two favorable asymptotical theoretical properties \citep{foster2023orthogonal,kennedy2023towards}: \emph{double robustness} and \emph{quasi-oracle efficiency}, {and, thus, are (in some sense) asymptotically optimal for causal quantities estimation \citep{balakrishnan2023fundamental}}. Double robustness states that, if one of the nuisance functions is estimated consistently, then the $V$-conditional CAPOs/CATE are estimated consistently, and quasi-oracle efficiency allows for the minimizer of the target loss with the estimated nuisance functions to be nearly identical to the minimizer of the target loss with the oracle nuisance functions even if the nuisance functions are estimated with slow rates. 

% {We refer to Remark~\ref{prop:quasi-eff-dr} in Appendix~\ref{app:proofs} for a formal statement about double robustness and quasi-oracle efficiency.   

\begin{numlemma}{9}[Double robustness and quasi-oracle efficiency of Neyman-orthogonal learners] \label{prop:quasi-eff-dr}
    Under mild conditions, the following inequality holds for the estimators of $V$-conditional CAPOs/CATE, the estimated target model $\hat{g} = \argmin_{g \in \mathcal{G}} \mathcal{L}(g, \hat{\eta})$, and the ground-truth target model, ${g}^* = \argmin_{g \in \mathcal{G}} \mathcal{L}(g, \eta)$: 
    \begin{align}
        \norm{\hat{g} - {g}^*}_{L_2}^2 \lesssim \mathcal{L}_{\diamond}(\hat{g}, \hat{\eta}) - \mathcal{L}_{\diamond}({g}^*, \hat{\eta}) + R^2_\diamond(\eta, \hat{\eta}),  
    \end{align}
    where $\diamond \in \{\xi_a, Y[a], \tau, \pi_0\pi_1\tau\}$, and $R^2_\diamond(\eta, \hat{\eta})$ is a second-order remainder which includes nuisance functions estimation errors of the higher order. Specifically, $R^2_\diamond(\eta, \hat{\eta})$ are as follows:
    \begin{align}
        & R^2_{\xi_a}(\eta, \hat{\eta}) = R^2_{Y[a]}(\eta, \hat{\eta}) = \norm{\hat{\mu}^x_a - {\mu}^x_a}_{L_2}^2  \norm{\hat{\pi}^x_1 - {\pi}^x_1}_{L_2}^2, \\
        & R^2_{\tau}(\eta, \hat{\eta}) = \sum_{a \in \{0, 1\}} \norm{\hat{\mu}^x_a - {\mu}^x_a}_{L_2}^2  \norm{\hat{\pi}^x_1 - {\pi}^x_1}_{L_2}^2, \\
        & R^2_{\pi_0\pi_1\tau}(\eta, \hat{\eta}) =  \norm{\hat{\pi}^x_1 - {\pi}^x_1}_{L_4}^4 + \sum_{a \in \{0, 1\}} \norm{\hat{\mu}^x_a - {\mu}^x_a}_{L_2}^2  \norm{\hat{\pi}^x_1 - {\pi}^x_1}_{L_2}^2. 
    \end{align}
    Hence, even with slow converging estimators of the nuisance functions, all of the mentioned Neyman-orthogonal learners $\diamond \in \{\xi_a, Y[a], \tau, \pi_0\pi_1\tau\}$ achieve quasi-oracle efficiency (see Definition~\ref{eq:oracle-eff} in Appendix~\ref{app:background-NO-DR}). Moreover, DR-learners for CATE and CAPOs obtain the double robustness property (see Definition~\ref{def:rate-dr} in Appendix~\ref{app:background-NO-DR}).  
\end{numlemma}

\begin{proof}
    The lemma above follows from Theorem 1 in \citet{morzywolek2023general} and Appendix A in \citet{vansteelandt2023orthogonal}. We refer to their papers for the proofs.
\end{proof}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{{Theoretical results}} \label{app:proofs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{numprop}{1}[Smoothness of the hidden layers] \label{prop:smoothness}
    Let the learned unconstrained representation network consist of the fixed-width fully-connected layers with locally quadratic activation functions.  Then, there exists a hidden layer (denoted by $V$) of the representation network with increased H{\"o}lder smoothness. That is, the expected $V$-conditional outcome, $\mu^v_a(\cdot) \in \tilde{C}^{\tilde{\beta}_a}(\mathcal{V})$, is H{\"o}lder smoother\footnote{In our paper, we consider the decrease of both $C$ and $\beta$ as smoothing.} than the original expected covariate-conditional outcome, $\mu^x_a(\cdot) \in C^{\beta_a}(\mathcal{X})$:
    \begin{equation}
        \tilde{\beta}_a \le \beta_a \quad \text{and} \quad \tilde{C} \le {C}.
    \end{equation}
\end{numprop}
\begin{proof}
(informal) We adopt the proof of Lemma 3(d) from \citet{ohn2019smooth} and Theorem XI.6 from \citet{elbrachter2021deep}. 

In Lemma 3(d) from \citet{ohn2019smooth}, the authors formulated an important result for \emph{fixed-width fully-connected neural networks with locally quadratic activation functions}. Informally, Lemma A.3(d) constructs an approximation of a Taylor expansion $f_J(x) = \sum_{k = 1}^{J} \frac{(x-1)^k}{k!}$ by using a fixed-width deep neural network. Here, $f_J(x)$ is an example of a generic $\beta=J$ Hölder-smooth function. Then, the approximation of $f_J(x)$ is done by adding $J$ layers where each layer, $j \in 1, \dots, J$, is only capable of approximating  $f_j(x)$ but not $f_{j+1}(x)$.

Theorem XI.6 of \citet{elbrachter2021deep}, on the other hand, shows the impossibility of universal approximation with fixed-width fixed-depth neural networks. That means, it is always possible to find a $\beta = 2$-smooth function (with an increasing Lipshitz constant, i.e., second-order derivative) that is impossible to approximate with fixed-width fixed-depth neural networks. Hence, an increase of either width or depth is required.

Therefore, it follows from \citet{elbrachter2021deep} that it is impossible to approximate some functions already for $\beta = 2$ with fixed width and depth. At the same time, the construction of fixed-width deep networks in \citet{ohn2019smooth} allows for such an estimation by increasing the depth. Notably, with a similar intuition, the theoretical result (namely, more flexibility requires more layers) holds for general classes of fixed-width deep networks \citep{hanin2019universal,kidger2020universal}. 


Our proof then follows by contradiction: There should be a hidden layer with larger smoothness since, otherwise, we would not be able to approximate the function solely with the remaining layers.
\end{proof}

\begin{numprop}{2}[Valid unconstrained representation with $d_\phi = 2$] \label{prop:valid-repr}
    The representation $\Phi(X) = \{\mu_0^x(X), \mu_1^x(X)\}$ is valid for CAPOs and CATE, namely:
    \begin{equation}
        {\xi}^x_a(x) = {\xi}_a^\phi(\Phi(x)) = {\mu}_a^\phi(\Phi(x)) \quad \text{and} \quad {\tau}^x(x) =  {\tau}^\phi(\Phi(x)) = {\mu}_1^\phi(\Phi(x)) - {\mu}_0^\phi(\Phi(x)).
    \end{equation}
\end{numprop}
\begin{proof}
    We employ properties of conditional expectations:
    \begin{align}
        {\tau}^\phi(\Phi(x)) & = \mathbb{E}(Y[1] - Y[0] \mid \Phi(X) =  \Phi(x)) \\
        & = \mathbb{E}\big(\mathbb{E} (Y \mid X, A = 1) - \mathbb{E} (Y \mid X, A = 0) \mid \Phi(X) = \Phi(x)\big) \\
        & = \mathbb{E}\big(\mathbb{E} (Y \mid X, A = 1) \mid ( \mu_0^x(x), \mu_1^x(x))\big) - \mathbb{E}\big(\mathbb{E} (Y \mid X, A = 0) \mid ( \mu_0^x(x), \mu_1^x(x))\big) \\
        & = \mu_1^x(x) -  \mu_0^x(x) = {\tau}^x(x).
    \end{align}
    On the other hand, the following holds:
    \begin{align}
        {\tau}^\phi(\Phi(x)) & =   \mathbb{E}\big(\mathbb{E} (Y \mid X, A = 1) \mid ( \mu_0^x(x), \mu_1^x(x))\big) - \mathbb{E}\big(\mathbb{E} (Y \mid X, A = 0) \mid ( \mu_0^x(x), \mu_1^x(x))\big) \\
        & =  \mathbb{E}(Y \mid (\mu_0^x(x), \mu_1^x(x)), A = 1) - \mathbb{E}(Y \mid (\mu_0^x(x), \mu_1^x(x)), A = 0) \\
        & = {\mu}_1^\phi(\Phi(x)) - {\mu}_0^\phi(\Phi(x)).
    \end{align}
    The derivation of ${\xi}^x_a(x) = {\xi}_a^\phi(\Phi(x)) = {\mu}_a^\phi(\Phi(x))$ follows analogously.
\end{proof}

\begin{numprop}{3}[Calibration] \label{prop:calibration} Given an unconstrained working model class $\mathcal{G}$, population minimizers, $\hat{g}(\hat{\mu}^x_0(x), \hat{\mu}_1^x(x)) =  \argmin_{g \in \mathcal{G}} \mathcal{L}(g, \hat{\eta})$, of the DR-learner losses for CAPOs, Eq.~\eqref{eq:DR-learner-K}--\eqref{eq:DR-learner-FS}, have the following form:
\begin{align}
    & \hat{g}(\hat{\mu}^x_0(x), \hat{\mu}_1^x(x))  = \mathbb{E}\bigg(\frac{\mathbbm{1}\{A = a\} Y }{\hat{\pi}_a^x(X)} \,\Big\vert\, \hat{\mu}^x_0(x), \hat{\mu}^x_1(x) \bigg) + \hat{\mu}^x_a(x) \, \bigg[1 - \mathbb{E}\bigg(\frac{\mathbbm{1}\{A = a\}}{\hat{\pi}_a^x(X)} \,\Big\vert\, \hat{\mu}^x_0(x), \hat{\mu}^x_1(x) \bigg) \bigg]. \nonumber
\end{align}
\end{numprop}
\begin{proof}
    It is easy to see that, given an unconstrained working model class $\mathcal{G}$, the population minimizer of the DR-learner loss in the style of \citet{kennedy2023towards} equals to 
    \begin{align} \label{eq:calibration-DR-K}
        \hat{g}(\hat{\mu}^x_0(x), \hat{\mu}_1^x(x)) & =  \argmin_{g \in \mathcal{G}} \mathcal{L}_{\xi_a}(g, \hat{\eta}) = \mathbb{E}\bigg(\frac{\mathbbm{1}\{A = a\}}{\hat{\pi}^x_a(X)} \big( Y - \hat{\mu}_a^x(X) \big) + \hat{\mu}_a^x(X) \,\Big\vert\, \hat{\mu}^x_0(x), \hat{\mu}_1^x(x)\bigg) \\
        & = \mathbb{E}\bigg(\frac{\mathbbm{1}\{A = a\} Y}{\hat{\pi}^x_a(X)} \,\Big\vert\, \hat{\mu}^x_0(x), \hat{\mu}_1^x(x)\bigg) - \hat{\mu}_a^x(x) \, \mathbb{E}\bigg(\frac{\mathbbm{1}\{A = a\}}{\hat{\pi}^x_a(X)} \,\Big\vert\, \hat{\mu}^x_0(x), \hat{\mu}_1^x(x)\bigg) + \hat{\mu}_a^x(x).
    \end{align}
    For the DR-learner loss in the style of \citet{foster2023orthogonal}, we first find a derivative of wrt. $g$:
    \begin{align}
        \frac{\diff}{\diff g} \mathcal{L}_{Y[a]}(g, \hat{\eta}) &= - 2 \mathbb{E}\bigg( \frac{\mathbbm{1}\{A = a\}}{\hat{\pi}^x_a(X)} \big( Y - g(V)\big)  +  \bigg(1 - \frac{\mathbbm{1}\{A = a\}}{\hat{\pi}^x_a(X)}\bigg) \, \big(\hat{\mu}_a^x(X) - g(V)\big)  \bigg) \\
        & = - 2 \mathbb{E}\bigg( \frac{\mathbbm{1}\{A = a\}}{\hat{\pi}^x_a(X)} \big( Y - \hat{\mu}_a^x(X)\big) + \hat{\mu}_a^x(X) - g(V) \bigg).
    \end{align}
    Therefore, the population minimizer of the DR-learner loss in the style of \citet{foster2023orthogonal} is given by 
    \begin{align}
        \hat{g}(v) & =  \argmin_{g \in \mathcal{G}} \mathcal{L}_{Y[a]}(g, \hat{\eta}) = \mathbb{E}\bigg(\frac{\mathbbm{1}\{A = a\}}{\hat{\pi}^x_a(X)} \big( Y - \hat{\mu}_a^x(X) \big) + \hat{\mu}_a^x(X) \,\Big\vert\, v\bigg).
    \end{align}
    By setting $V = \{\hat{\mu}^x_0(X), \hat{\mu}_1^x(X)\}$, we recover the desired equality (see Eq~\eqref{eq:calibration-DR-K}).
\end{proof}

\begin{numprop}{4}[Smoothness via expanding transformations] \label{prop:smooth-scaling}
    A representation network with a representation $\Phi(X)$ achieves higher H{\"o}lder smoothness of $\mu^a_\phi(\cdot)$ by expanding some parts of the space $\mathcal{X}$. That is, for $\mu^a_x(\cdot) \in C^{\beta_a}(\mathcal{X})$ and $\mu^a_\phi(\cdot) \in \tilde{C}^{{\beta}_a}(\mathit{\Phi})$ with $\tilde{C} \le C$, it is necessary that the following holds:
    \begin{equation}
        \operatorname{Lip}(\Phi) \ge 1,
    \end{equation}
    where $\operatorname{Lip}(\Phi)$ is a Lipschitz constant of the transformation $\Phi(\cdot)$. In the case of an invertible transformation, we have $\operatorname{Lip}(\Phi) = \sup_{x \in \mathcal{X}} \abs{\det{\Phi'(x)}}$ and, therefore, $\Phi(\cdot)$ expands (scales up) some parts of the space $\mathcal{X}$.
\end{numprop}
\begin{proof}
    The proof follows from the properties of the transformation $\Phi(\cdot)$ as a continously-differential function. On the one hand, by the definition of the H{\"o}lder smoothness (see Definition~\ref{def:holder}):
    \begin{align}
        & \left| D^\alpha \mu^a_\phi(\phi) - D^\alpha \mu^a_\phi(\phi') \right| \leq \tilde{C} \| \phi - \phi' \|^{\beta_a - \lfloor \beta_a \rfloor}_2 \quad \text{for } \phi, \phi' \in \mathit{\Phi}  \\ 
        & \left| D^\alpha \mu^a_x(x) - D^\alpha \mu^a_x(x') \right| \leq {C} \| x - x' \|^{\beta_a - \lfloor \beta_a \rfloor}_2 \quad \text{for } x, x' \in \mathcal{X}.
    \end{align}
    On the other hand:
    \begin{align}
        \| \Phi(x) - \Phi(x') \|_2 \le  \operatorname{Lip}(\Phi) \, \| x - x' \|_2.
    \end{align}
    Therefore, we yield the following inequalities:
    \begin{align}
        \left| D^\alpha \mu^a_\phi(\Phi(x)) - D^\alpha \mu^a_\phi(\Phi(x')) \right| & \leq \tilde{C} \| \Phi(x) - \Phi(x') \|^{\beta_a - \lfloor \beta_a \rfloor}_2 \\
        & \leq \underbrace{\tilde{C} \big( \operatorname{Lip}(\Phi) \big)^{\beta_a - \lfloor \beta_a \rfloor}}_{C} \| x - x' \|^{\beta_a - \lfloor \beta_a \rfloor}_2.
    \end{align}
    Applying the fact that $\tilde{C} \le C$ finalizes the proof:
    \begin{align}
        \tilde{C} \le \tilde{C} \big( \operatorname{Lip}(\Phi) \big)^{\beta_a - \lfloor \beta_a \rfloor} \implies \operatorname{Lip}(\Phi) \ge 1.
    \end{align}
\end{proof}

\begin{numprop}{5}[Balancing via contracting transformations] \label{prop:balancing-scaling}
    A representation network with a representation $\Phi(X)$ reduces {the IPMs, namely, WM and MMD} (see definitions in Appendix~\ref{app:background-ipm}) between the distributions of the representations $\mathbb{P}(\Phi(X) \mid A = 0)$ and $\mathbb{P}(\Phi(X) \mid A = 0)$ by contracting some parts of the space $\mathcal{X}$. Hence, to minimize an IPM (either WM or MMD), i.e.,
    \begin{equation} \label{eq:ipm-ineq}
        \operatorname{IPM}\big(\mathbb{P}(\Phi(X) \mid A = 0), \mathbb{P}(\Phi(X) \mid A = 1) \big) \le \operatorname{IPM}\big(\mathbb{P}(X \mid A = 0), \mathbb{P}(X \mid A = 1) \big),
    \end{equation}
    it is necessary that 
    \begin{align}
        \operatorname{Lip}(\Phi) \le 1
    \end{align}
    holds true, where $\operatorname{Lip}(\Phi)$ is a Lipschitz constant of the transformation $\Phi(\cdot)$. In the case of an invertible transformation, $\operatorname{Lip}(\Phi) = \sup_{x \in \mathcal{X}} \abs{\det{\Phi'(x)}}$ and, therefore, $\Phi(\cdot)$ scales down some parts of the space $\mathcal{X}$.
\end{numprop}
\begin{proof}
    First, we provide the proof for the Wasserstein metric. The Wasserstein metric between the distributions of the representations can be expressed as
    \begin{align}
        & W\big(\mathbb{P}(\Phi(X) \mid A = 0), \mathbb{P}(\Phi(X) \mid A = 1) \big) \\
        =& \sup_{f \in \mathcal{F}_1} \abs{\mathbb{E}\left(f(\Phi(X)) \mid A = 0\right) - \mathbb{E}\left(f(\Phi(X)) \mid A = 1\right)} \\
        =& \sup_{f \in \mathcal{F}_1} \abs{\int_{\mathcal{X} } f(\Phi(x)) \Big(\mathbb{P}(X = x \mid A = 1) - \mathbb{P}(X = x \mid A = 0) \Big) \dd{x}} \\
        =& \sup_{\tilde{f} \in \mathcal{F}_K} \abs{\int_{\mathcal{X} } \tilde{f}(x) \Big(\mathbb{P}(X = x \mid A = 1) - \mathbb{P}(X = x \mid A = 0) \Big) \dd{x}} \\
        =& K \, W\big(\mathbb{P}(X \mid A = 0), \mathbb{P}(X \mid A = 1) \big),
    \end{align}
    where $K$ is a Lipschitz constant of $\Phi(\cdot)$ and where the latter equality follows from properties of the Wasserstein metric. Then, we see that the desired inequality in Eq.~\eqref{eq:ipm-ineq} holds when $K \le 1$. 

    Similarly, the inequality from Eq.~\eqref{eq:ipm-ineq} can be shown for the maximum mean discrepancy by using a Lipschitzness property of a reproducing kernel Hilbert space (RKHS) (see Proposition 3.1  in \citet{fiedler2023lipschitz}): all functions $f \in \mathcal{F}_{\text{RKHS}, 1}$ are Lipschitz with the constant 1. Therefore, for a composition of functions $f \circ \Phi$ to be in the RKHS, i.e., $\mathcal{F}_{\text{RKHS}, 1}$, it is required that $\operatorname{Lip}(\Phi) \le 1$.

\end{proof}
}

{\begin{numprop}{6}[Consistent estimation with $\Phi(X) = c$] \label{prop:degenerate-repr}
    For constant representations $\Phi(X) = c$, our \ORlearners yield semi-parametric efficient (augmented inverse propensity of treatment weighted (A-IPTW)) estimators of APOs and ATE / overlap-weighted ATE. Specifically, if the target model is characterized by an intercept parameter $\theta \in \mathbb{R}$, namely, $g(\cdot) = \theta$, then the minimization of the \ORlearners losses yields the following $\hat{\theta}$:
    \begin{align}
        \hat{\theta}_{\xi_a} & =  \hat{\theta}_{Y[a]} =  \mathbb{P}_n \bigg\{\frac{\mathbbm{1}\{A = a\}}{\hat{\pi}^x_a(X)} \big( Y - \hat{\mu}_a^x(X) \big) + \hat{\mu}_a^x(X) \bigg\}, \\
         \hat{\theta}_{\tau} & = \mathbb{P}_n \bigg\{\frac{A}{\hat{\pi}^x_1(X)} \big( Y - \hat{\mu}_1^x(X) \big) - \frac{1 - A}{\hat{\pi}^x_0(X)} \big( Y - \hat{\mu}_0^x(X) \big) + \hat{\mu}_1^x(X) - \hat{\mu}_0^x(X) \bigg\}, \\
         \hat{\theta}_{\pi_0\pi_1\tau} & = \mathbb{P}_n \bigg\{ \frac{1}{\big(A - \hat{\pi}_1^x(X)\big)^2} \, \frac{\big(Y - \hat{\mu}^x(X)\big)}{\big(A - \hat{\pi}_1^x(X)\big)} \bigg\}
    \end{align}
\end{numprop}
\begin{proof}
    The proof follows from properties of the (weighted) MSE risks. For $\mathbb{E}(Z - \theta)^2$, as in DR-loss in the style of \citet{kennedy2023towards}, the minimum for a constant $\theta \in \mathbb{R}$ is achieved at $\hat{\theta} = \mathbb{E}(Z)$. For $\mathbb{E}(Z_1 - \theta)^2 + \mathbb{E}(Z_2 - \theta)^2$, as in DR-loss in the style of \citet{foster2023orthogonal}, the minimum is achieved at $\hat{\theta} = \mathbb{E}(Z_1 + Z_2)$. For the weighted MSE, $\mathbb{E} \big(w(Z)(Z - \theta)^2 \big)$, the minimum is achieved for $\hat{\theta} = \frac{\mathbb{E}(w(Z) Z)}{\mathbb{E}(w(Z))}$.
\end{proof}
}

{\begin{numcorollary}{7}[Alternative construction of Neyman-orthogonal learners for constrained representations] \label{prop:alternative}
An alternative learner targeting at the representation-level CAPOs/CATE can be defined in the following way. For a working model $\tilde{\mathcal{G}} = \{ g \circ \Phi (\cdot): \mathcal{X} \to \mathcal{Y}\}$, we aim to minimize the following target risks:
\begin{align} \label{eq:risk-diamond-dist}
    \tilde{\mathcal{L}}_\diamond(g \circ \Phi, \eta) = \mathcal{L}_\diamond(g \circ \Phi, \eta) + \alpha \, \operatorname{dist}(\mathbb{P}(\Phi(X) \mid A = 0), \mathbb{P}(\Phi(X) \mid A = 1))
\end{align}
wrt. $g \circ \Phi \in \tilde{\mathcal{G}}$, where $\mathcal{L}_\diamond$ is defined in Eq.~\eqref{eq:risk-repr-capos-app}-\eqref{eq:risk-repr-cate-overlap-app} for $\diamond \in \{\xi_a, Y[a], \tau, \pi_0\pi_1\tau\}$ and where $\operatorname{dist}(\cdot, \cdot)$ is a distributional distance (\eg, an IPM). Then, the following two theoretical results hold: (1)~the $\Phi(X)$-conditional CAPOs and CATE are identifiable as population minimizers of the target risks from Eq.~\eqref{eq:risk-diamond-dist} if they are contained in the $\mathcal{G} = \{g (\cdot): \mathit{\Phi} \to \mathcal{Y}\}$. (2)~The following target losses are Neyman-orthogonal
\begin{align}
    \hat{\tilde{\mathcal{L}}}_\diamond(g \circ \Phi, \hat{\eta}) = \hat{\mathcal{L}}_\diamond(g \circ \Phi, \hat{\eta}) + \alpha \, \widehat{\operatorname{dist}}(\mathbb{P}(\Phi(X) \mid A = 0), \mathbb{P}(\Phi(X) \mid A = 1)),
\end{align}
 where $\mathcal{L}_\diamond$ is defined in Eq.~\eqref{eq:risk-repr-capos-app}--\eqref{eq:risk-repr-cate-overlap-app} for $\diamond \in \{\xi_a, Y[a], \tau, \pi_0\pi_1\tau\}$. Therefore, these variants of Neyman-orthogonal learners are asymptotically equivalent to our \ORlearners.
\end{numcorollary}
\begin{proof}
 The result for (1) follows from the properties of joint optimization of Eq.~\eqref{eq:risk-diamond-dist} wrt. $g \circ \Phi \in \tilde{\mathcal{G}}$ and Lemma~\ref{prop:rep-id}. The result for (2), meaning the Neyman-orthogonality of $\hat{\tilde{\mathcal{L}}}_\diamond$ holds, as the balancing constraint $\widehat{\operatorname{dist}}(\mathbb{P}(\Phi(X) \mid A = 0), \mathbb{P}(\Phi(X) \mid A = 1))$ is estimated without using the nuisance functions $\pi_a^x$ and $\mu_a^x$.
\end{proof}
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Dataset details} \label{app:datasets}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Synthetic dataset}

We use a synthetic benchmark dataset with hidden confounding as proposed by \citet{kallus2019interval}, but modify it by incorporating the confounder as the second observed covariate. Specifically, synthetic covariates $X_1$ and $X_2$ along with treatment $A$ and outcome $Y$ are generated by the following data-generating process: 
\begin{equation}
    \begin{cases}
        X_1 \sim \text{Unif}(-2, 2), \\
        X_2 \sim N(0, 1),  \\
        A \sim \text{Bern}\left(\frac{1}{1 + \exp(-(0.75 \, X_1 - X_2 + 0.5))}\right) \\
        Y \sim N\big( (2\, A - 1) \, X_1 + A - 2 \, \sin(2 \, (2\,A - 1) \, X_1 + X_2) - 2\,X_2\,(1 + 0.5\,X_1) , 1\big),
    \end{cases}
\end{equation}
where $X_1, X_2$ are mutually independent.

\subsection{IHDP dataset}
The Infant Health and Development Program (IHDP) dataset \citep{hill2011bayesian, shalit2017estimating} is a widely-used semi-synthetic benchmark for evaluating treatment effect estimation methods. It consists of 100 train/test splits, with $n_\text{train} = 672$, $n_\text{test} = 75$, and $d_x = 25$. However, this dataset suffers from significant overlap violations, leading to instability in methods that rely on propensity re-weighting \citep{curth2021nonparametric, curth2021really}.

\subsection{ACIC 2016 dataset collection}

The covariates for ACIC 2016 \citep{dorie2019automated} are derived from a large-scale study on developmental disorders \citep{niswander1972collaborative}. The datasets in ACIC 2016 vary in the number of true confounders, the degree of overlap, and the structure of conditional outcome distributions. ACIC 2016 features 77 distinct data-generating mechanisms, each with 100 equal-sized samples ($n = 4802, d_X = 82$) after one-hot encoding the categorical covariates.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Implementation details and hyperparameters} \label{app:implementation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Implementation.} We implemented our \ORlearners in PyTorch and Pyro. For better compatibility, the fully-connected subnetworks have one hidden layer with a tuneable number of units. For the representation subnetworks involving normalizing flows, we employed residual normalizing flows \citep{chen2019residual} that have three hidden layers with a tuneable synchronous number of units. All the networks for our \ORlearners (see Stages \circled{0}--\circled{2} in Fig.~\ref{fig:or-learner-overview}) are trained with AdamW \citep{loshchilov2019decoupled}. Each network was trained with $n_\text{epoch} = 200$ epochs for the synthetic dataset and $n_\text{epoch} = 50$ for the ACIC 2016 dataset collection. To further stabilize training of the target networks in stage \circled{2}, we (i)~used exponential moving average (EMA) of model weights \citep{polyak1992acceleration} with a smoothing hyperparameter ($\lambda = 0.995$); and (ii)~clipped too low propensity scores ($\hat{\pi}_a^x(X) < 0.05$). 

\begin{algorithm}[H]
    \caption{Pseudocode of our \ORlearners (full version)}\label{alg:or-learners-app}
    % \vspace{-0.1cm}
    \begin{algorithmic}[1]
    \footnotesize
        \STATE {\bfseries Input:} Training dataset $\mathcal{D}$; (balancing) constraint strength $\alpha \ge 0$; target risk $\diamond \in \{ \xi_a, Y[a], \tau, \pi_0\pi_1\tau\}$; $\operatorname{dist} \in \{\operatorname{WM}, \operatorname{MMD}\}$
        \STATE {\bfseries Stage }\circled{0}: Fit a representation network $\in \{$TARNet/TARFlow, CFR/CFRFlow, RCFR/RCFRFlow, BNN/BNNFlow, CFR-ISW/CFRFlow-ISW, BWCFR/BWCFRFlow$\}$ 
        \begin{ALC@g}
            \IF{Representation network $\in \{$BWCFR/BWCFRFlow$\}$}
                \STATE Fit a propensity network (FC$_{\pi, x}$) by minimizing a BCE loss $\mathcal{L}_\pi$ and set $\hat{\pi}_a^x(X) \gets$ FC$_{\pi, x}(X)$
            \ENDIF
            \FOR{$i$ = 0 {\bfseries to} $\lceil n_{\text{epochs}} \cdot n / b_{\text{R}} \rceil$}
                \STATE Draw a minibatch $\mathcal{B} = \{X, A, Y\}$ of size $b_{\text{R}}$ from $\mathcal{D}$
                \STATE {\bfseries Initialize:} $W \gets \mathbbm{1}_{b_R}; \quad \mathcal{L}_\pi \gets 0; \quad \mathcal{L}_\text{Bal} \gets 0$
                \STATE $\Phi \gets $ NF$_\phi$ / FC$_\phi$$(X)$
                \STATE $\hat{\mu}_a^\phi(\Phi) \gets $ FC$_a$$(\Phi, a)$
                \IF{Representation network $\in \{$CFR-ISW/CFRFlow-ISW$\}$}
                    \STATE $\hat{\pi}_a^\phi(\Phi) \gets$ FC$_{\pi, \phi}(\operatorname{detach}(\Phi))$
                    \STATE $\mathcal{L}_\pi \gets \operatorname{BCE}(\hat{\pi}_A^\phi(\Phi), A)$
                    \STATE $W \gets \operatorname{detach}\big({\mathbbm{1}\{\hat{\pi}_A^\phi(\Phi) \ge 0.05\}} / {\hat{\pi}_A^\phi(\Phi)} \big)$
                \ELSIF{Representation network $\in \{$BWCFR/BWCFRFlow$\}$}
                    \STATE $W \gets {\mathbbm{1}\{\hat{\pi}_A^x(X) \ge 0.05\}} / {\hat{\pi}_A^x(X)}$
                \ELSIF{Representation network $\in \{$RCFR/RCFRFlow$\}$}
                    \STATE $W \gets$ FC$_w(\operatorname{detach}(\Phi))$
                \ENDIF
                \STATE $\mathcal{L}_\text{MSE} \gets \mathbb{P}_{b_R} \{ W (Y - \hat{\mu}_A^\phi(\Phi))^2\} \big/ \mathbb{P}_{b_R} \{ W\}$
                \IF{Representation network $\notin \{$TARNet/TARFlow$\}$ and $\alpha > 0 $}
                    \STATE $\mathcal{L}_\text{Bal} \gets W$-weighted $ \widehat{\operatorname{dist}}(\mathbb{P}(\Phi(X) \mid A = 0), \mathbb{P}(\Phi(X) \mid A = 1))$
                \ENDIF
                \STATE Gradient update of the representation network wrt. $\mathcal{L}_\text{MSE} + \alpha \mathcal{L}_\text{Bal} + \mathcal{L}_\pi$ 
            \ENDFOR
            \STATE $V \gets \Phi(X)$
        \end{ALC@g}
        \STATE {\bfseries Stage }\circled{1}: Estimate nuisance functions $\hat{\eta} = (\hat{\mu}_a^x, \hat{\pi}_a^x)$
        \begin{ALC@g}
            \IF{Representation network $\notin \{$BWCFR/BWCFRFlow$\}$}
                \STATE Fit a propensity network (FC$_{\pi, x}$) by minimizing a BCE loss $\mathcal{L}_\pi$ and set $\hat{\pi}_a^x(X) \gets$ FC$_{\pi, x}(X)$
            \ENDIF
            \IF{$\alpha > 0$ and FC$_\phi$ is used at Stage \circled{0}}
                \STATE Fit an outcomes network (FC$_{\mu, x}$) by minimizing an MSE loss $\mathcal{L}_{\text{MSE}}$ and set $\hat{\mu}_a^x(X) \gets$ FC$_{\mu, x}(X, a)$ 
            \ELSE 
                \STATE Set $\hat{\mu}_a^x(X) \gets \hat{\mu}_a^\phi(\Phi(X))$
            \ENDIF
        \end{ALC@g}
        \STATE {\bfseries Stage }\circled{2}: Fit a target network $\hat{g} = \argmin \hat{\mathcal{L}}_\diamond(g, \hat{\eta})$ 
        \begin{ALC@g}
            \FOR{$i$ = 0 {\bfseries to} $\lceil n_{\text{epochs}} \cdot n / b_{\text{T}} \rceil$}
                \STATE Draw a minibatch $\mathcal{B} = \{X, A, Y\}$ of size $b_{\text{T}}$ from $\mathcal{D}$
                \STATE $\alpha_a(A, X) \gets {\mathbbm{1}\{A = a\} \cdot \mathbbm{1}\{\hat{\pi}_a^x(X) \ge 0.05\}}/{\hat{\pi}^x_a(X)}$
                \STATE $\hat{\mathcal{L}}_{\xi_a}(g, \hat{\eta}) \gets \mathbb{P}_{b_{\text{T}}} \big\{ \big( \alpha_a(A, X) \big( Y - \hat{\mu}_a^x(X) \big) + \hat{\mu}_a^x(X) - g(V)\big)^2 \big\}$
                \STATE $\hat{\mathcal{L}}_{Y[a]}(g, \hat{\eta}) \gets \mathbb{P}_{b_{\text{T}}} \big\{ \alpha_a(A, X) \big( Y - g(V)\big)^2  +  \big(1 - \alpha_a(A, X)\big) \big(\hat{\mu}_a^x(X) - g(V)\big)^2  \big\}$
                \STATE $\hat{\mathcal{L}}_{\tau}(g, \hat{\eta}) \gets \mathbb{P}_{b_{\text{T}}} \big\{ \big(\alpha_0(A, X) \big( Y - \hat{\mu}_0^x(X) \big) + \alpha_1(A, X) \big( Y - \hat{\mu}_1^x(X) \big) + \hat{\mu}_1^x(X) - \hat{\mu}_0^x(X) - g(V)\big)^2 \big\}$
                \STATE $\hat{\mathcal{L}}_{\pi_0\pi_1\tau}(g, \hat{\eta}) \gets \mathbb{P}_{b_{\text{T}}} \big\{\big(\big(Y - \hat{\mu}^x(X)\big) - \big(A - \hat{\pi}_1^x(X)\big) g(V)\big)^2 \big\}$
                \STATE Gradient \& EMA update of the target network $g$ wrt. $\hat{\mathcal{L}}_\diamond(g, \hat{\eta})$ 
            \ENDFOR
        \end{ALC@g}
        \STATE {\bfseries Output:} Representation-level estimator $\hat{g}$ for CAPOs/CATE 
    \end{algorithmic}
    \vspace{-0.1cm}
\end{algorithm}

\textbf{Hyperparameters.} We performed hyperparameter tuning at all the stages of our \ORlearners for all the networks based on five-fold cross-validation using the training subset. At each stage, we did a random grid search with respect to different tuning criteria. Table~\ref{tab:hyperparams} provides all the details on hyperparameters tuning. For reproducibility, we made tuned hyperparameters available in our GitHub.\footnote{\url{https://anonymous.4open.science/r/OR-learners}.}

\begin{table*}[h]
    \vspace{-0.1cm}
    \caption{Hyperparameter tuning for baselines and our \ORlearners.}
    \label{tab:hyperparams}
    \vspace{-0.5cm}
    \begin{center}
    \scalebox{.87}{
        \begin{tabu}{l|l|l|r}
            \toprule
            Stage & Model & Hyperparameter & Range / Value \\
            \midrule
            \multirow{28}{*}{\textbf{Stage 0}} & \multirow{8}{*}{\begin{tabular}{l}
                 TARNet/TARFlow \\ BNN/BNNFlow \\ CFR/CFRFlow \\ BWCFR/BWCFRFlow
            \end{tabular}} & Learning rate & 0.001, 0.005, 0.01\\
            && Minibatch size, $b_R$ & 32, 64, 128 \\
            && Weight decay & 0.0, 0.001, 0.01, 0.1 \\
            && Hidden units in NF$_\phi$ / FC$_\phi$ & $R \, d_x$, 1.5 $Rd_x$, 2 $Rd_x$ \\
            && Hidden units in FC$_a$ & $R \, d_\phi$, 1.5 $Rd_\phi$, 2 $Rd_\phi$ \\
            && Tuning strategy & random grid search with 50 runs \\
            && Tuning criterion & factual MSE loss \\ 
            && Optimizer & AdamW \\ 
            \cmidrule{2-4} & \multirow{11}{*}{CFR-ISW/CFRFlow-ISW} & Representation network learning rate & 0.001, 0.005, 0.01 \\
            && Propensity network learning rate & 0.001, 0.005, 0.01 \\
            && Minibatch size, $b_R$ & 32, 64, 128 \\
            && Representation network weight decay & 0.0, 0.001, 0.01, 0.1 \\
            && Propensity network weight decay & 0.0, 0.001, 0.01, 0.1 \\
            && Hidden units in NF$_\phi$ / FC$_\phi$ & $R \, d_x$, 1.5 $Rd_x$, 2 $Rd_x$ \\
            && Hidden units in FC$_a$ & $R \, d_\phi$, 1.5 $Rd_\phi$, 2 $Rd_\phi$ \\
            && Hidden units in FC$_{\pi,\phi}$ & $R \, d_\phi$, 1.5 $Rd_\phi$, 2 $Rd_\phi$ \\
            && Tuning strategy & random grid search with 50 runs \\
            && Tuning criterion & factual MSE loss + factual BCE loss \\ 
            && Optimizer & AdamW \\
            \cmidrule{2-4} & \multirow{9}{*}{RCFR/RCFRFlow} & Learning rate & 0.001, 0.005, 0.01\\
            && Minibatch size, $b_R$ & 32, 64, 128 \\
            && Weight decay & 0.0, 0.001, 0.01, 0.1 \\
            && Hidden units in NF$_\phi$ / FC$_\phi$ & $R \, d_x$, 1.5 $Rd_x$, 2 $Rd_x$ \\
            && Hidden units in FC$_a$ & $R \, d_\phi$, 1.5 $Rd_\phi$, 2 $Rd_\phi$ \\
            && Hidden units in FC$_w$ & $R \, d_\phi$, 1.5 $Rd_\phi$, 2 $Rd_\phi$ \\
            && Tuning strategy & random grid search with 50 runs \\
            && Tuning criterion & factual MSE loss \\ 
            && Optimizer & AdamW \\
            \midrule \multirow{14}{*}{\textbf{Stage 1}} & \multirow{7}{*}{Propensity network} & Learning rate & 0.001, 0.005, 0.01\\
            && Minibatch size, $b_N$ & 32, 64, 128 \\
            && Weight decay & 0.0, 0.001, 0.01, 0.1 \\
            && Hidden units in FC$_{\pi, x}$ & $R \, d_{x}$, 1.5 $Rd_{x}$, 2 $Rd_{x}$ \\
            && Tuning strategy & random grid search with 50 runs \\
            && Tuning criterion & factual BCE loss \\ 
            && Optimizer & AdamW \\
            \cmidrule{2-4} & \multirow{7}{*}{Outcomes network} & Learning rate & 0.001, 0.005, 0.01\\
            && Minibatch size, $b_N$ & 32, 64, 128 \\
            && Hidden units in FC$_{\mu, x}$ & $R \, d_x$, 1.5 $Rd_x$, 2 $Rd_x$ \\
            && Weight decay & 0.0, 0.001, 0.01, 0.1 \\
            && Tuning strategy & random grid search with 50 runs \\
            && Tuning criterion & factual negative log-likelihood loss \\ 
            && Optimizer & SGD (momentum = 0.9) \\
            \midrule \multirow{6}{*}{\textbf{Stage 2}} & \multirow{6}{*}{Target network} & Learning rate &0.005\\
            && Minibatch size, $b_T$ & 64 \\
            && EMA of model weights, $\lambda$ & 0.995 \\
            && Hidden units in $g$ & Hidden units in FC$_a$ \\
            && Tuning strategy & no tuning \\
            && Optimizer & AdamW \\
            \bottomrule
            \multicolumn{4}{l}{$R = 2$ (synthetic data), $R = 1$ (IHDP dataset), $R = 0.25$ (ACIC 2016 datasets collection)}
        \end{tabu}}
    \end{center}
    \vspace{-2.5cm}
\end{table*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Additional experiments} \label{app:experiments}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Setting A}

Table~\ref{tab:synthetic-setting-a} shows additional results for the synthetic dataset in Setting A. Therein, we observe that our \ORlearners with $V = \Phi(X)$ are highly effective in comparison to the DR/R-learners based on the original covariates.

\begin{table}[ht]
    \vspace{-0.1cm}
    \begin{minipage}{\linewidth}
      \caption{\textbf{Results for synthetic experiments in Setting A.} Reported: improvements of our \ORlearners over representation networks; mean over 15 runs. Here, $n_{\text{train}} = 500, d_\phi = 2$.} \label{tab:synthetic-setting-a}
      \vspace{-0.3cm}
      \begin{center}
            \scriptsize
            \scalebox{1}{\input{tables/synthetic_setting_a}}
        \end{center}
    \end{minipage}%
    \vspace{-0.3cm}
\end{table}

\subsection{Setting B}

Fig.~\ref{fig:ihdp-setting-b} shows the results for the IHDP dataset in Setting B. Here, interestingly, balancing in CFRFlow seems to outperform our \ORlearners for some values of $\alpha$. This is not surprising, as the IHDP dataset contains strong overlap violations and one of the ground-truth potential outcome surfaces is linear $Y[1]$. However, the optimal $\alpha$ are different for both CAPOs and CATE, which renders balancing impractical.

\begin{figure}[ht]
    \centering
    \hspace{0.1cm}
    \begin{minipage}{\linewidth}
        \centering
        \includegraphics[width=0.7\linewidth]{figures/ihdp-setting-b.pdf}
        \vspace{-0.1cm}
        \caption{\textbf{Results for IHDP experiments in Setting B.} Reported: ratio between the performance of TARFlow (CFRFlow with $\alpha = 0$) and representation networks with varying $\alpha$; mean $\pm$ SE over 100 train/test splits.}
        \label{fig:ihdp-setting-b}
    \end{minipage}
    \vspace{-0.3cm}
\end{figure}

In Fig.~\ref{fig:setting-b-scaling}, we show how the learned normalizing flows transform the original space $\mathcal{X}$ (the models are the same as in Fig.~\ref{fig:synthetic-setting-b}). The rendered transformations match the theoretical results provided in Sec.~\ref{sec:OR-learner-constrained-inv}. Specifically, TARFlow scales up (expands) the original space so that the regression task becomes easier in the representation space. At the same time, CRFFlows with different balancing hyperparameters $\alpha$ aim to scale down (contract) the space, thus achieving better balancing.  

\begin{figure}[ht]
    \vspace{-0.2cm}
    \centering
    \includegraphics[width=0.48\linewidth]{figures/synthetic-scaling-WM-0.0.pdf} \\
    \vspace{0.1cm}
    \hrule
    \vspace{0.1cm}
    \includegraphics[width=0.48\linewidth]{figures/synthetic-scaling-WM-0.05.pdf} 
    \hspace{0.1cm} \vrule \hspace{0.1cm}
    \includegraphics[width=0.48\linewidth]{figures/synthetic-scaling-MMD-0.05.pdf} 
    \vspace{0.1cm}
    \hrule
    \vspace{0.1cm}
    \includegraphics[width=0.48\linewidth]{figures/synthetic-scaling-WM-1.0.pdf} 
    \hspace{0.1cm} \vrule \hspace{0.1cm}
    \includegraphics[width=0.48\linewidth]{figures/synthetic-scaling-MMD-1.0.pdf} 
    \vspace{-0.2cm}
    \caption{{Visualization of the invertible transformations defined by the learned normalizing flow representation subnetworks for synthetic experiments in Setting B. Here, $n_\text{train} = 500, d_\phi=2$. Specifically, we show how a grid in the original covariate space, $\mathcal{X} \subseteq \mathbb{R}^2$, gets transformed onto the representation space, $\mathit{\Phi}\subseteq \mathbb{R}^2$. We vary the strength of balancing $\alpha \in \{0, 0.05, 1.0\}$ and the IPM $\in \{$WM, MMD$\}$. As suggested by the theory in Sec.~\ref{sec:OR-learner-constrained-inv}, the covariate space gets scaled up for $\alpha = 0$ and gets scaled down for large values (\eg, $\alpha = 1$).}}
    \label{fig:setting-b-scaling}
    \vspace{-0.2cm}
\end{figure}

\clearpage
\subsection{Setting C}

Table~\ref{tab:synthetic-setting-c} shows additional results for the synthetic dataset in setting C. Here, our \ORlearners improve over the vast majority of the non-invertible representation learning methods where balancing is applied.

\begin{table}[ht]
    \vspace{-0.1cm}
    \begin{minipage}{\linewidth}
      \caption{\textbf{Results for synthetic experiments in Setting C.} Reported: improvements of our \ORlearners over representation networks; mean over 15 runs. Here, $n_{\text{train}} = 500, d_\phi = 2$.} \label{tab:synthetic-setting-c}
      \vspace{-0.3cm}
      \begin{center}
            \scriptsize
            \scalebox{1}{\input{tables/synthetic_setting_c}}
        \end{center}
    \end{minipage}%
    \vspace{-0.3cm}
\end{table}

\end{document}