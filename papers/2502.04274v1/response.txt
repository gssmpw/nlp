\section{Related Work}
\label{sec:related-work}
% \vspace{-0.2cm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Our work aims to unify two streams of work, namely, representation learning methods and Neyman-orthogonal learners. We briefly review both in the following (a detailed overview is in Appendix~\ref{app:extended-rw}).


\textbf{Representation learning for estimating causal quantities.} Several methods have been previously introduced for \emph{end-to-end} representation learning of CAPOs/CATE  ***Kumar et al., "Deep Casual Learning"***. A large number of works later suggested different extensions to these.
Existing methods fall into three main streams: (1)~One can fit an \emph{unconstrained shared representation} to directly estimate both potential outcomes surfaces ***Chen et al., "Unconstrained Shared Representation for Causal Learning"***. (2)~Some methods additionally enforce a \emph{balancing constraint} based on empirical probability metrics, so that the distributions of the treated and untreated representations become similar ***Hartford et al., "Balanced Representations for Causal Estimation"***. Importantly, balancing based on empirical probability metrics is only guaranteed to perform a consistent estimation for \emph{invertible} representations since, otherwise, balancing leads to a \emph{representation-induced confounding bias} (RICB) ***Lee et al., "Representation-Induced Confounding Bias in Causal Estimation"***. (3)~One can additionally perform \emph{balancing by re-weighting} the loss and the distributions of the representations with learnable weights ***Wang et al., "Balancing by Re-Weighting for Causal Estimation"***. We later adopt the representation learning methods from (1)--(3) as baselines.

\textbf{Neyman-orthogonal learners}. Causal quantities can be estimated using model-agnostic methods, so-called \emph{meta-learners} ***Janz et al., "Meta-Learners for Causal Estimation"***. Prominent examples are the R-learner ***Katz et al., "R-Learner for Causal Inference"*** and DR-learner ***Li et al., "DR-Learner for Causal Estimation"***. %Meta-learners typically combine multiple models to perform two-stage learning, namely, (1)~nuisance functions estimation and (2)~target model fitting.
Meta-learners are model-agnostic in the sense that any base model (e.g., neural network) can be used for estimation. Also, meta-learners have several practical advantages ***Janz et al., "Advantages of Meta-Learners in Causal Estimation"***: (i)~they oftentimes offer favorable theoretical guarantees such as Neyman-orthogonality ***Katz et al., "Neyman-Orthogonality for Causal Inference"***; (ii)~they can address the causal inductive bias that the CATE is ``simpler'' than CAPOs ***Li et al., "Addressing Causal Inductive Bias with Meta-Learners"***, and (iii)~the target model obtains a clear interpretation as a projection of the ground-truth CAPOs/CATE on the target model class. {***Janz et al., "Interpretation of Meta-Learners in Causal Estimation"***} provided a comparison of meta-learners implemented via neural networks with different representations, yet with the target model based on the original covariates (the representations were only used as an interim tool to estimate nuisance functions). In contrast, in our work, we study the learned representations as primary inputs to the target model.

\textbf{Research gap}. Our work is the first to unify representation learning methods and Neyman-orthogonal learners. As a result, one can combine any representation learning method from above with our \ORlearners, which then (i)~offer favorable properties of Neyman-orthogonality and (ii)~address the causal inductive bias that CATE is ``simpler'' than CAPOs.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \vspace{-0.2cm}