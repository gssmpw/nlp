\section{Related Work}
\noindent\textbf{Bi-encoders}
Retrieval models typically rely on sparse or dense vector embeddings to select the most relevant documents for a query, reducing the problem to a nearest-neighbor search.
Dense retrieval models~\cite{karpukhin-2020-dpr, khattab2020colbert} rely on encoders, neural networks designed to embed the semantic meaning of text into dense vector representations.
State-of-the-art sentence embedding models often leverage bidirectional encoders, such as BERT~\cite{devlin2019-bert} and BGE~\cite{xiao-bge}), 
while others are built upon pretrained large language models, such as LLM2Vec~\cite{behnamghader2024llmvec}). 
Some encoders, such as BGE-large~\cite{xiao-bge},
%BGE-M3~\cite{chen-etal-2024-m3}\vz{replace with bge-large}, 
are specifically designed for retrieval, offering a %multilingual and multimodal 
sentence embedding framework optimized for such tasks.
Other notable examples are: DPR~\cite{karpukhin-2020-dpr}, which finetunes a BERT~\cite{devlin2019-bert} dual encoder to separately encode queries and documents into a single dense vector by contrastive learning;
ColBERT~\cite{khattab2020colbert, santhanam2021colbertv2}, which finetunes BERT to encode queries and documents into multiple dense vector representations and by matching individual token embeddings between queries and documents.
Unlike sparse retrieval methods, such as BM25 or TF-IDF~\cite{salton-tfidf}, these dense representations 
are less interpretable, expensive to compute (generally because of attention operations) and take up a large amount of storage space. They do however perform better on many downstream tasks such as QA, specially for complex queries. 

\noindent\textbf{Hierarchical retrieval}
Hierarchical retrieval approaches aim to balance test-time efficiency and accuracy by organizing the retrieval process into multiple stages, typically involving coarse-to-fine filtering of candidate documents.
Many hierarchical methods face challenges related to computational cost and performance trade-offs, and scale poorly with the size of the corpus.
MemWalker~\cite{chen2023walking} addresses LLM context limitations by structuring long texts into a hierarchical memory tree. It first segments the text into chunks, summarizing each into a node, which are then recursively merged into higher-level summaries until forming a root node. At query time, the LLM navigates this tree interactively, using iterative prompting to locate the most relevant segments, 
%
MemWalker~\cite{chen2023walking} reframes retrieval in the context of addressing LLM context window limitations. It first segments long contexts into smaller chunks, summarizing each into hierarchical nodes that form a memory tree. At query time, the LLM traverses this tree using iterative prompting, efficiently locating the most relevant segments without exceeding its context limit.
Raptor~\cite{sarthi2024raptor} uses a clustering algorithm to group similar documents and then applies an LLM to recursively summarize and re-embed chunks of text, constructing a tree with differing levels of summarization from the bottom up, resulting in a structured, multi-layered tree representation of the original documents. 
GraphRAG~\cite{edge2024local} uses an LLM to build a graph-based text index by first deriving an entity knowledge graph from source documents and then pregenerating community summaries for related entities. For a given question, partial responses are generated from each community summary and combined into a final summarized response.
%
While hierarchical retrieval methods improve response quality using LLMs and provides inspectable structures, they incur high computational costs and slow processing times, especially with large datasets. This trade-off makes them less suitable for real-time or resource-limited scenarios, emphasizing the need for more efficient solutions.
\ourmodel{} overcomes these limitations by constructing and navigating a tree with no LLM calls. 

\noindent\textbf{Coarse-to-fine representations}
As compute and running time generally scale with the representation size, an effective solution to limit retrieval costs is through dimensionality reduction\citep{van2009dimensionality}.
When the computational budget is not known in advance, the standard solution is to train multiple models or low-dimensional adapters not to incur into accuracy degradation, as one would by applying post-hoc compression techniques. However, this solution requires higher training and memory costs than learning and storing a single model.
Single-model alternatives have been recently developed~\citep{yu2018slimmable,cai2019once,kusupati2022matryoshka}.
In particular, \citep{kusupati2022matryoshka} propose Matryoshka Representation Learning (MRL), a simple framework that learns a nested representation. 
MRL boils down to learning an adaptive capacity embedding, ensuring that any first m-dimensions vector is as accurate as an independently trained m-dimensional one.  
This approach improves retrieval efficiency and flexibility, making it well-suited for diverse and evolving retrieval scenarios.
Similarly, \ourmodel{} benefits from such advantages by training a nested representation, where each level input-to-node assignment corresponds to an embedding. 
Its structure and the learning of assignments additionally provide an organization of the documents into semantic groupings, allowing practitioners to inspect the corpus content and the inner workings of the retrieval system.

\noindent\textbf{Differentiable trees and hierarchical indexes}
Because of their non-differentiable form, tree structures have been traditionally optimized by greedy algorithms, specialized for a particular objective, e.g. for classification, regression, or hierarchical clustering\citep{quinlan1986induction,krishnamurthy2012efficient,quinlan2014c4,breiman2017classification,moseley2023approximation}.
Recent literature has focused on differentiable formulations to take advantage of continuous optimization techniques for training trees on large datasets and for arbitrary objectives\citep{irsoy2012soft,yang2018deep,monath2019gradient,tanno2019adaptive,pmlr-v139-zantedeschi21a,marton2024gradtree}.
We leverage this literature and extend it to learning binary trees that are optimal for the retrieval objective (via contrastive learning), and with complex split and propagation functions.
Finally, trees and graphs have been used in the retrieval literature as indices for storing and quickly retrieving document embeddings via approximate similarity search, and not designed for being inspected\citep{bernhardsson2017annoy,malkov2018efficient,douze2024faiss}.
\ourmodel{} does not belong to this line of works, as it is an embedding extractor that learns an inspectable tree to organize and represent documents at different granularity and into semantic groupings.