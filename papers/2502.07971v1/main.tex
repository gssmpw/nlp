\documentclass{article}
\usepackage{natbib}
\usepackage{arxiv}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{caption}
\usepackage{booktabs} % for professional tables
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{xcolor}
\usepackage{caption}
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Our commands
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{commands}
\newcommand{\ourmodel}{{\textsc{ReTreever}}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\title{\ourmodel{}: Tree-based Coarse-to-Fine Representations for Retrieval}

\author{
Shubham Gupta\textsuperscript{1,3,4}, 
Zichao Li\textsuperscript{1,2,4}, 
Tianyi Chen\textsuperscript{1}, 
Cem Subakan\textsuperscript{3,4}, 
Siva Reddy\textsuperscript{1,2,4}\\ 
\textbf{Perouz Taslakian\textsuperscript{1}},
\textbf{Valentina Zantedeschi\textsuperscript{1,2,4}}\\ \\
\textsuperscript{1}ServiceNow Research \hspace{1cm}
\textsuperscript{2}McGill University \hspace{1cm}
\textsuperscript{3}Universit\'e Laval \\
\textsuperscript{4}Mila -- Qu√©bec Artificial Intelligence Institute\\
}

\date{}
\renewcommand{\headeright}{}
\renewcommand{\undertitle}{}

\begin{document}
\maketitle

\begin{abstract}
Document retrieval is a core component of question-answering systems, as it enables conditioning answer generation on new and large-scale corpora.
While effective, the standard practice of encoding documents into high-dimensional embeddings for similarity search entails large memory and compute footprints, and also
makes it hard to inspect the inner workings of the system.
In this paper, we propose a tree-based method for organizing and representing reference documents at various granular levels, which offers the flexibility to balance cost and utility, and eases the inspection of the corpus content and retrieval operations.
Our method, called \ourmodel{}, jointly learns a routing function per internal node of a binary tree such that query and reference documents are assigned to similar tree branches, hence directly optimizing for retrieval performance.
Our evaluations show that \ourmodel{} generally preserves full representation accuracy. Its hierarchical structure further provides strong coarse representations and enhances transparency by indirectly learning meaningful semantic groupings. Among hierarchical retrieval methods, \ourmodel{} achieves the best retrieval accuracy at the lowest latency, proving that this family of techniques can be viable in practical applications.
\end{abstract}

\section{Introduction}
\label{sec:intro}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/ReTreever-training-4.pdf}
    \caption{\ourmodel{}'s traversal. At training, a positive query-context pair is first encoded by the frozen $E$ (Bi-Encoder); then their encodings are each given as input to the split nodes of the tree (here of depth $D=3$) which output the probability of an embedding being routed left or right; all these scores are finally combined to output an assignment embedding of length the number of leaves, whose elements correspond to the probability of an excerpt reaching a certain leaf. At inference, the leaf assignments are used as fine representations, while assignments at intermediate levels as coarse representations, as they also provide valid distributions.}
    \label{fig:tree-training}
\end{figure}


Information retrieval enables us to efficiently search for relevant information across millions of documents. 
With techniques like retrieval-augmented generation, document retrievers have become a key component in transforming large language models (LLMs) into tailored experts without the need for exhaustive fine-tuning \citep{lewis2020retrieval,izacard-grave-2021-leveraging,gao2023retrieval}.
They enable LLM-generated content to be grounded in retrieved knowledge, thereby alleviating hallucinations \citep{shuster2021-retrieval-augmentation}.
Moreover, retrieval allows LLMs to scale to massive datasets without increasing their internal parameters.

A popular approach for document retrieval is to represent documents as high-dimensional embeddings and use nearest-neighbor techniques to find relevant documents for a given query embedding~\citep{karpukhin-2020-dpr}. 
While effective, the high dimensionality of these document embeddings results in a large memory footprint and heavy computation at inference time. 
The documents are also stored without any human-readable structure or understandable grouping, making it difficult to derive insights from the corpus or verify the retrieval process.
In this paper, we propose \ourmodel{}, a tree-based method where documents are organized and represented at various granular levels, offering coarse-to-fine representations, learned entirely end-to-end by optimizing for retrieval performance as the learning objective. 
These representations offer the flexibility to balance cost and utility.
Instead of training and storing a separate model for each desired representation size, a full-capacity tree can be learned, allowing excerpts to be encoded at any equal or lower level based on computational constraints during inference.

\ourmodel{} takes inspiration from the hierarchical retrieval literature, where tree organizers are built and navigated via calls to an LLM by cross-encoding queries and contexts~\citep{chen2023walking,sarthi2024raptor,edge2024local}. 
Such retrievers have the advantage of preserving the existing organization of the data, such as document divisions and entity relationships, and grouping documents semantically into a readable tree organization.
% and enable retrieval at various levels of granularity. 
However, their reliance on LLMs make them slow and expensive to train and run, hence impractical even for medium-size text corpora.
In contrast, \ourmodel{} is designed to operate entirely on embeddings, eliminating the need for LLM calls during both construction and navigation.

Figure~\ref{fig:tree-training} shows a schematic of our approach. 
\ourmodel{} first converts reference document snippets into a embedding using a standard encoder, such as BGE~\cite{xiao-bge}, BERT~\cite{devlin2019-bert}, ModernBERT~\cite{warner2024modernBERT}, or LongFormer~\cite{beltagy2020longformer}.
% \vz{cite bge and other recent ones}\PT{How about that?}. 
It then uses these representations to learn end-to-end a binary tree structure by optimizing directly the retrieval objective, making use of the query-context supervision available from most datasets. 
This is achieved by learning the parameters of a routing function at each internal node of the tree, such that positive query-context pairs are routed similarly through it. 

\ourmodel{} is attractive not only for computational reasons but also for transparency. 
The learned hierarchical structure naturally provides an organization of the documents, which allows us to probe the tree to gain insights into the corpus content and retrieval operations.
Practitioners can inspect different levels of the tree to identify key thematic clusters influencing retrieval or understand why certain branches lead to specific documents.
While this is not a full causal explanation of query-context matching, it does ease the inspection of the model's inner workings.
In \Cref{sec:interpretability}, we analyze nodes at various levels of the hierarchy and show that documents in these nodes have thematic overlaps even though the tree is solely optimized for query-document similarity without any clustering objective.
Our contributions are as follows:
\begin{enumerate}
    \item We propose \ourmodel{}, a method for training coarse-to-fine representations, each corresponding to a level of a tree that is optimized for retrieval; unlike existing hierarchical retrievers, \ourmodel{} scales to large corpora and does not rely on LLM calls.
    \item We evaluate \ourmodel{}'s retrieval efficiency and accuracy, as compared to a variety of encoding and tree-based methods, and show that it preserves the accuracy of full representations, while offering strong coarse representations.
    \item We illustrate how \ourmodel{} can be used to inspect the content of the corpus by leveraging its hierarchical structure, and show that it implicitly organizes documents into semantic groupings, which is a key step for making retrieval transparent and interpretable.
\end{enumerate}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.50\textwidth]{figures/ReTreever-example-keywords-5-2.pdf}
    \caption{Visualization of the topics (in bold) and keywords extracted from the contexts assigned to one subtree (in green) rooted at node 5 of a \ourmodel{} tree of depth $10$ learned on \textsc{NQ}. For compactness, we represent only a subset of the nodes and paths, and stop at depth 5. Topics are locally coherent along a path, which indicates that \ourmodel{} naturally groups contexts semantically.}
    \label{fig:tree-inspection}
\end{figure}

\section{Related Work}
\noindent\textbf{Bi-encoders}
Retrieval models typically rely on sparse or dense vector embeddings to select the most relevant documents for a query, reducing the problem to a nearest-neighbor search.
Dense retrieval models~\cite{karpukhin-2020-dpr, khattab2020colbert} rely on encoders, neural networks designed to embed the semantic meaning of text into dense vector representations.
State-of-the-art sentence embedding models often leverage bidirectional encoders, such as BERT~\cite{devlin2019-bert} and BGE~\cite{xiao-bge}), 
while others are built upon pretrained large language models, such as LLM2Vec~\cite{behnamghader2024llmvec}). 
Some encoders, such as BGE-large~\cite{xiao-bge},
%BGE-M3~\cite{chen-etal-2024-m3}\vz{replace with bge-large}, 
are specifically designed for retrieval, offering a %multilingual and multimodal 
sentence embedding framework optimized for such tasks.
Other notable examples are: DPR~\cite{karpukhin-2020-dpr}, which finetunes a BERT~\cite{devlin2019-bert} dual encoder to separately encode queries and documents into a single dense vector by contrastive learning;
ColBERT~\cite{khattab2020colbert, santhanam2021colbertv2}, which finetunes BERT to encode queries and documents into multiple dense vector representations and by matching individual token embeddings between queries and documents.
Unlike sparse retrieval methods, such as BM25 or TF-IDF~\cite{salton-tfidf}, these dense representations 
are less interpretable, expensive to compute (generally because of attention operations) and take up a large amount of storage space. They do however perform better on many downstream tasks such as QA, specially for complex queries. 

\noindent\textbf{Hierarchical retrieval}
Hierarchical retrieval approaches aim to balance test-time efficiency and accuracy by organizing the retrieval process into multiple stages, typically involving coarse-to-fine filtering of candidate documents.
Many hierarchical methods face challenges related to computational cost and performance trade-offs, and scale poorly with the size of the corpus.
MemWalker~\cite{chen2023walking} addresses LLM context limitations by structuring long texts into a hierarchical memory tree. It first segments the text into chunks, summarizing each into a node, which are then recursively merged into higher-level summaries until forming a root node. At query time, the LLM navigates this tree interactively, using iterative prompting to locate the most relevant segments, 
%
MemWalker~\cite{chen2023walking} reframes retrieval in the context of addressing LLM context window limitations. It first segments long contexts into smaller chunks, summarizing each into hierarchical nodes that form a memory tree. At query time, the LLM traverses this tree using iterative prompting, efficiently locating the most relevant segments without exceeding its context limit.
Raptor~\cite{sarthi2024raptor} uses a clustering algorithm to group similar documents and then applies an LLM to recursively summarize and re-embed chunks of text, constructing a tree with differing levels of summarization from the bottom up, resulting in a structured, multi-layered tree representation of the original documents. 
GraphRAG~\cite{edge2024local} uses an LLM to build a graph-based text index by first deriving an entity knowledge graph from source documents and then pregenerating community summaries for related entities. For a given question, partial responses are generated from each community summary and combined into a final summarized response.
%
While hierarchical retrieval methods improve response quality using LLMs and provides inspectable structures, they incur high computational costs and slow processing times, especially with large datasets. This trade-off makes them less suitable for real-time or resource-limited scenarios, emphasizing the need for more efficient solutions.
\ourmodel{} overcomes these limitations by constructing and navigating a tree with no LLM calls. 

\noindent\textbf{Coarse-to-fine representations}
As compute and running time generally scale with the representation size, an effective solution to limit retrieval costs is through dimensionality reduction\citep{van2009dimensionality}.
When the computational budget is not known in advance, the standard solution is to train multiple models or low-dimensional adapters not to incur into accuracy degradation, as one would by applying post-hoc compression techniques. However, this solution requires higher training and memory costs than learning and storing a single model.
Single-model alternatives have been recently developed~\citep{yu2018slimmable,cai2019once,kusupati2022matryoshka}.
In particular, \citep{kusupati2022matryoshka} propose Matryoshka Representation Learning (MRL), a simple framework that learns a nested representation. 
MRL boils down to learning an adaptive capacity embedding, ensuring that any first m-dimensions vector is as accurate as an independently trained m-dimensional one.  
This approach improves retrieval efficiency and flexibility, making it well-suited for diverse and evolving retrieval scenarios.
Similarly, \ourmodel{} benefits from such advantages by training a nested representation, where each level input-to-node assignment corresponds to an embedding. 
Its structure and the learning of assignments additionally provide an organization of the documents into semantic groupings, allowing practitioners to inspect the corpus content and the inner workings of the retrieval system.

\noindent\textbf{Differentiable trees and hierarchical indexes}
Because of their non-differentiable form, tree structures have been traditionally optimized by greedy algorithms, specialized for a particular objective, e.g. for classification, regression, or hierarchical clustering\citep{quinlan1986induction,krishnamurthy2012efficient,quinlan2014c4,breiman2017classification,moseley2023approximation}.
Recent literature has focused on differentiable formulations to take advantage of continuous optimization techniques for training trees on large datasets and for arbitrary objectives\citep{irsoy2012soft,yang2018deep,monath2019gradient,tanno2019adaptive,pmlr-v139-zantedeschi21a,marton2024gradtree}.
We leverage this literature and extend it to learning binary trees that are optimal for the retrieval objective (via contrastive learning), and with complex split and propagation functions.
Finally, trees and graphs have been used in the retrieval literature as indices for storing and quickly retrieving document embeddings via approximate similarity search, and not designed for being inspected\citep{bernhardsson2017annoy,malkov2018efficient,douze2024faiss}.
\ourmodel{} does not belong to this line of works, as it is an embedding extractor that learns an inspectable tree to organize and represent documents at different granularity and into semantic groupings.


\section{Proposed Method}
\label{sec:proposed_method}
\ourmodel{} consists of (1) a frozen encoder $E$ that returns embeddings for a given chunk of text, and (2) a learnable binary tree $T$ that organizes encoded 
pieces of text %contexts 
into a hierarchy and routes queries to their relevant contexts (see \Cref{fig:tree-training}).
In this section, we describe how the tree hierarchy is learned by continuous optimization and how search is performed at test time. 
In what follows, we use the term \emph{context} to refer to the text segments organized by \ourmodel{}.

\noindent\textbf{Tree Formulation} 
%A \emph{complete binary tree} is an acyclic graph whose nodes $t \in \mathcal{T}$ satisfy the following two properties: (i) each node has at most one left child and at most one right child; (ii) each node, except the unique root, has exactly one parent.
A \emph{perfect binary tree} $\mathcal{T}$ is a binary tree where all levels are completely filled. The nodes $t \in \mathcal{T}$ of the tree satisfy the following two properties: (i) each node has either no children or exactly two, one left child and one right child; (ii) each node, except the unique root, has exactly one parent.
%
There exist two types of tree nodes: branching (or \emph{split}) nodes $\cT_B$, which route their inputs to their left or right child, and leaf nodes (or simply \emph{leaves}) $\cT_L$, which have no children and constitute the terminal state of the tree $\mathcal{T} \vcentcolon= \cT_B \cup \cT_L$.

Given positive query-context pairs $\{(\qb_i, \cb_i) \in \cP \}_{i=1}^n$ where $\qb_i$, $\cb_i$ are embedding pairs generated by the encoder $E$,
our goal is to learn a perfect binary tree $\cT$ of chosen depth $D$ that assigns $\qb_i$ and $\cb_i$ to the same leaf node $t_l \in \cT_L$. 
Such leaf assignments denoted $T(\xb_i)$ are obtained by routing a given input $\xb_i \in \mathcal{X}$ (e.g., $\xb_i \vcentcolon= \qb_i$ the query embedding) sequentially through branching nodes until it reaches a specific leaf node in the tree. 
Specifically, each branching node $t \in \cT_B$ is parametrized by a split function $s_{\theta_t}$ that routes an input to its left child node if a certain condition is met, or to its right one otherwise.
We denote by $\zb_i \in \{0, 1\}^{|\mathcal{T}|}$ the route taken by the input, where $\zb_{i,t}$ equals $1$ if the input has reached node $t$, and $0$ otherwise.

However, hard assignments would result in piece-wise constant functions with null gradients, making back-propagation ineffective.
Therefore, we make use of probabilistic relaxations, inspired by works such as ~\cite{irsoy2012soft}, to make the tree learning problem differentiable, hence to allow the learning of the split parameters $\Theta \vcentcolon= \{\theta_t\}_{t=1}^{\cT_B}$ jointly by gradient descent and so that they are optimal for retrieval.
Such manipulations effectively %boil down to relaxing 
relax the hard routes into soft ones $\zb_i \in [0, 1]^{|\mathcal{T}|}$, where each element $\zb_{i,t}$ can now be interpreted as the probability of the input traversing node $t$ (see~\Cref{fig:tree-training}).

\noindent\textbf{Choice of Split Function}
A split function $s_{\theta_t}: \mathcal{X} \to [0, 1]$ can be implemented in several ways, as long as it outputs a routing score, that determines the probability of routing an input left or right. 
Its simplest form is a linear projection, such as the one used in~\cite{pmlr-v139-zantedeschi21a}:
given a split node $t$, its left child $t_{\text{left}}$ and right child $t_{\text{right}}$, the split function is defined as the dot product $s_{\theta_t}(\xb_i) = \theta_t \xb^T$ (interpreting $\theta_t \in \mathcal{X}$ as a hyper-plane).
We experiment also with more complex functions, such as neural networks (see~\cref{app:split}).

In particular, we propose a \textbf{cross-attention-based function}~\citep{vaswani2017attention}, where learned embeddings $\eb_t \in \mathbb{R}^{n_e \times d_{\text{emb}}}$ for node \(t\) selectively attend to the input text $\xb_i \in \mathbb{R}^{n_d \times d_{\text{emb}}}$, which consist of $n_d$ embedded tokens (as extracted by the encoder). Recall that an attention mechanism is computed as: $\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d_k}}\right)\mathbf{V}$, where $d_k$ is the dimension of the $\mathbf{Q}$ and $\mathbf{K}$ matrices\footnote{This formulation describes a single-head attention mechanism, which can be naturally extended to multi-head attention by using multiple independent sets of projection matrices for queries, keys, and values, and then concatenating the outputs from all heads.}. 
We apply linear projections on $\eb_t$ to obtain the queries $\mathbf{Q}$ and on $\xb_i$ to obtain the keys $\mathbf{K}$ and values $\mathbf{V}$, respectively. 
The projection matrices for the queries, keys, and values are shared across the entire tree. 

The transformed node embeddings for each node are then combined via a node-specific aggregation function to obtain the final split node score. Various choices for this aggregation function are explored in Appendix~\ref{app:score_aggregation}.
The cross-attention split function is generally more expressive than a linear projection 
(as our ablation shows in \Cref{app:split}) .
Indeed, the node embeddings and projection matrices act as different memories that store information from past query and context embeddings, useful for scoring the current inputs.
No matter the design of split function, the routing probabilities are finally computed as $\zb_{t_{\text{left}}} = \sigma(s_{\theta_t}(\xb_i))$ and $\zb_{t_{\text{right}}} = 1 - \sigma(s_{\theta_t}(\xb_i))$, with $\sigma(x) \vcentcolon= \frac{1}{1+ e^{-x}}$ the sigmoid function, to ensure a valid probability distribution.

\noindent\textbf{Tree Propagation}
Note that any split function defined above outputs a score that is independent of the scores from the other nodes.
However, the probability of reaching a node should be constrained by the probability of having reached its parent, the parent of its parent, and so on, to avoid degenerate trees where a descendant has higher probability of being reached than its ancestors.
The simplest way of enforcing such tree constraints is by propagating the scores of the ancestors down to the node of interest by multiplying the probabilities along the path.
We refer to this type of tree propagation as \textit{product propagation}.
Given a node $t$ and its ancestors $\mathcal{A}_t$ (the split nodes along the path from the root to $t$), the probability of reaching $t$ is $T(\xb_i)_t = \zb_{t} \prod_{a \in \mathcal{A}_t} \zb_{a}$.
Notice that the product propagation naturally enforces that the sum of node probabilities at any depth is always constant and equal to 1.
Alternatively, one can learn how to propagate probability scores through the tree.
We refer to this type of tree propagation as \textit{learned propagation} and describe it in ~\Cref{app:propagation}.
In either tree propagation, evaluating each split function sequentially, in order of depth, would unnecessarily slow down training and inference, in particular for trees of large depth.
Equivalently, in our implementation we leverage parallel computing and the independence of split functions to evaluate all split functions in parallel, regardless of their depth, and then apply the selected tree propagation. 

\noindent\textbf{Training by Contrastive Learning}
The task now is to learn end-to-end the parameters $\Theta$ (that include those of the split and propagation functions), so that any query is routed optimally to the leaf that contains its corresponding ground-truth context. 
Such an optimal assignment is achieved when positive query-context pairs are independently routed similarly through the tree, leading to similar leaf assignments.
However, optimizing solely for perfect positive assignments is likely to lead to the trivial solution where all contexts and queries are routed to the same leaf, resulting in a collapsed tree with a single active path.
To avoid such a solution, we define an objective that additionally encourages maximal use of the tree, by spreading unrelated contexts and queries across different tree leaves.
Building on the contrastive learning literature~\cite{oord2018representation,radford2021learning}, we optimize the following Softmax-based InfoNCE loss,
\begin{equation}
    {-} \frac{1}{2|\cP|}\! \sum_{i=1}^{|\cP|}\! \left ( \!\log \frac{e^{\simi(\qb_i, \cb_i)}}{\sum_{j=1}^{|\cP|} e^{\simi(\qb_i, \cb_j)}} {+} \log \frac{e^{\simi(\cb_i, \qb_i)}}{\sum_{j=1}^{|\cP|} e^{\simi(\cb_i, \qb_j)}}\!\right ) \label{eq:constrative_loss}
\end{equation}
where $\cP$ is the set of query-context pairs, and we take the similarity $\simi: [0, 1]^{|\mathcal{T}_L|} \times [0, 1]^{|\mathcal{T}_L|} \to \mathbb{R}$ to be the negative Total Variation Distance (nTVD) between two leaf assignments: $\simi(\mathbf{a}, \mathbf{b}) = -\frac{1}{2} \sum_{l=1}^{|\mathcal{T}_L|} \left| a_l - b_l \right|$, where \( \mathbf{a} = (a_1, a_2, \ldots, a_{|\mathcal{T}_L|}) \) and \( \mathbf{b} = (b_1, b_2, \ldots, b_{|\mathcal{T}_L|}) \) are the leaf assignment distributions, and \( |\mathcal{T}_L| \) is the number of leaf nodes in the tree.
In Eq.\eqref{eq:constrative_loss}, the left term encourages any query to have a leaf assignment similar to the assignment of its ground-truth context and different from any other context in the batch.
Conversely, the second term encourages contexts to be routed similarly as their positive queries and differently from their negative ones.

Notice that learning node assignment probabilities, as opposed to unconstrained features (e.g., in MRL \citep{kusupati2022matryoshka}), naturally provides a hierarchical and soft clustering of the documents which can be inspected by the user.
We show in \Cref{sec:interpretability} that learned clusterings capture semantic groupings, where assigned documents share topics and keywords, even though \ourmodel{} does not directly optimize for semantic coherence.

\noindent\textbf{Coarse-to-Fine Representations}
Because of the hierarchical structure of the tree and its constraints, optimizing assignments at the leaf level, as expressed in Eq.\eqref{eq:constrative_loss}, implicitly optimizes the assignments at intermediate levels, making them suitable coarse representations.
To boost the retrieval performance of such coarse representations, we devise an optimization scheme where at each iteration a random level of the tree is selected and the constrastive loss is optimized w.r.t. its intermediate assignments. We call this scheme \textit{stochastic}, as opposed to \textit{constant} for when we optimize exclusively at the leaf level.

\noindent\textbf{Indexing and Search}
Once the tree is trained, we index new content by encoding each context excerpt with the encoder $E$ and then routing it through the tree $T$ to obtain assignments for the chosen tree level.
Such assignments can then be interpreted as a new representation of an excerpt, where each dimension represents the alignment between the excerpt and a learned semantic group. 
To retrieve related contexts for a query, we build a vector index based on these level assignments, process the query, and return its nearest neighbors based on the nTVD metric used at training.


\section{Retrieval Experiments}
\label{sec:experiments}
In this section, we assess the retrieval performance of \ourmodel{} at different representation levels and as compared to flat and hierarchical retrieval methods.
Our results indicate that using the learned node assignments for retrieval (1) generally preserves the representation power of the encoder at the finer level, (2) results in more expressive embeddings at the coarsest levels and (3) strikes the best latency-accuracy trade-off among hierarchical retrieval methods.

\paragraph{Metrics and datasets}
To evaluate retrieval results, we measure the following standard metrics:
\textsc{Recall@k}, which assesses the proportion of ground-truth documents that are present within the top-k results returned by the retriever;
\emph{Normalized Discounted Cumulative Gain} at rank $k$ (\textsc{NDCG@k}) which accounts for ground-truth ranking, as relevant documents appearing earlier are considered more valuable.
We use four open-domain question-answering datasets for our experiments:
 \textsc{Natural Questions} (\textsc{NQ})~\citep{kwiatkowski2019natural}, \textsc{HotpotQA}~\citep{yang2018hotpotqa}, \textsc{TopiOCQA}~\citep{adlakha2022topiocqa}, and \textsc{RepLiQA}~\cite{monteiro2024repliqa}. 
A sample from our datasets consists of a $query$ (natural-language question) and one ground-truth $context$. We follow the pre-processing done in \citet{monteiro2024xc}, with the difference that for \textsc{HotpotQA} we concatenate all relevant contexts and discard irrelevant ones to obtain $context$.
We make use of a validation set for model selection and hyperparameter tuning. 
For \textsc{RepLiQA}, we use the first split \textsc{RepLiQA$_0$} for training and validation (10\% of the split) and \textsc{RepLiQA$_1$} for testing. For datasets with a validation set but not a test partition, we use the validation set for testing and create a validation set by holding out 10\% of randomly selected training samples.
Then, for training \ourmodel{} and the baselines we make use of the training query-context pairs, and for testing, we build the index with all the contexts from training, validation and test splits, if not specified otherwise.

\begin{table*}[h!]
\centering
\caption{Retrieval performance with full representations. Best results per metric and dataset are in bold. For \textsc{Hier-GMM}'s results, the index is built exclusively with the test contexts.}
\label{tab:full-rep-main}
\begin{center}
\scalebox{0.8}{
\begin{sc}
\begin{tabular}{lcccccccc}
\toprule
\textbf{Model} & \multicolumn{2}{c}{\textsc{NQ}} & \multicolumn{2}{c}{\textsc{HotpotQA}} & \multicolumn{2}{c}{\textsc{TopiOCQA}} & \multicolumn{2}{c}{\textsc{RepLiQA}} \\
& recall${@}10$ & ndcg${@}10$ & recall${@}10$ & ndcg${@}10$& recall${@}10$ & ndcg${@}10$& recall${@}10$ & ndcg${@}10$ \\ 
\midrule
BGE & 0.7353 & 0.5139 & 0.9635 & 0.8940 & 0.3119 & 0.2169 & 0.9000 & 0.7123\\
Hier-GMM$^*$ & 0.0098 & 0.0048 & 0.0088 & 0.0044 & 0.0180 & 0.0084 & 0.0108 & 0.0050\\
\ourmodel{} & \textbf{0.7824} & \textbf{0.5496} & 0.9185 & 0.8451 & 0.2804 & 0.1735 & 0.8940 & \textbf{0.7957}\\
\ourmodel{}-no tree & 0.7790 & 0.5443 & \textbf{0.9645} & \textbf{0.8946} & \textbf{0.3258} & \textbf{0.2247} & \textbf{0.9171} & 0.7854 \\
\midrule
BGE-MRL & 0.6702 & 0.4561 & 0.9201 & 0.8194 & 0.3062 & 0.2125 & 0.8972 & 0.7750\\
\ourmodel{}-stochastic& 0.7411 & 0.5185 & 0.8884 & 0.7973 & 0.2629 & 0.1703 & 0.8231 & 0.6974 \\
\bottomrule
\end{tabular}
\end{sc}}
\end{center}
\vskip -0.1in
\end{table*}

\paragraph{Baselines, Resources and Implementation Details}
We compare the retrieval results of \ourmodel{} with the following baselines: 

\noindent\textbf{BGE}: 
we fine-tune \textit{BAAI/bge-large-en-v1.5}~\cite{bge_embedding} on each dataset by training a linear fully connected layer (of equal number of input and output units) using \eqref{eq:constrative_loss} as loss and the cosine similarity as similarity score. 
For the \textsc{MRL} version, we still use a linear adapter but modify the loss to incorporate the Matryoshka Representations-based training approach \citep{kusupati2022matryoshka} on top of the contrastive loss. This ensures that the learned representations can also be interpreted as a coarse-to-fine hierarchy, making the comparison more aligned. We refer to this version as \textbf{\textsc{BGE-MRL}}.

\noindent\textbf{Hierarchical clustering}: we perform a top-down hierarchical k-means (\textbf{\textsc{Hier-Kmeans}}) or GMM (\textbf{\textsc{Hier-GMM}}) clustering, where documents are iteratively partitioned into two clusters at each level of the hierarchy based on the cosine similarity of their embeddings as extracted by an encoder. This recursive process continues until the hierarchy reaches the same depth of \ourmodel{}. A similar procedure is applied for building the index: contexts are greedily routed through the learned tree and assigned to the leaf they reach.
Further, we report results for two inference methods for retrieval. 
We apply tree search 
for both \textsc{Hier-Kmeans} and \textsc{Hier-GMM}, where instead of greedily traversing the tree (which would give poor performance), we compute a global search to find the leaf with the highest score: all split-node clustering models are evaluated, then their scores are propagated and aggregated top-down. Finally, the contexts assigned to the selected leaf are re-ranked based on the cosine similarity between query and context embeddings.  
Alternatively, we interpret the probability distribution of queries or contexts over all leaf nodes in \textsc{Hier-GMM} as a tree assignment representation, indexing and searching in the same manner as \ourmodel{}. This allows us to assess whether \ourmodel{}'s tree-based representation power stems from our proposed method or if it naturally emerges from probabilistic hierarchical clustering. 

\noindent\textbf{\textsc{RAPTOR}}~\cite{sarthi2024raptor}: we evaluate \textsc{Raptor} that recursively builds the tree bottom-up via LLM calls as described in the original paper. For a fair assessment of its retrieval accuracy, we then apply a similar tree search as for hierarchical clustering, by scoring nodes using the cosine similarity between query and summary embeddings. The contexts assigned to the selected leaf are then reranked again via the cosine similarity. Note that this is the only method we fit on the test corpus, as it cannot be applied to unseen documents and it does not scale to our training sets.


For all competing methods, including \ourmodel{}, we use the same BGE-Large model \textit{BAAI/bge-large-en-v1.5}, which has a representation size of $1024$. We truncate the input to $512$ tokens.
We train our model and BGE on a single NVIDIA H100 GPU with a batch size of 64, using AdamW~\cite{loshchilov2017decoupled} with a learning rate of 0.0004 for 200K steps and a 10K-step linear warmup.
We set \ourmodel{}'s depth to $10$, which gives representations of size up to $1024$. For the split function, we utilize the cross-attention split and the tree-based score aggregation (see \cref{app:score_aggregation} for definitions). 
The attention mechanism in our split function employs $8$ heads, each with a dimensionality of $64$, resulting in a total hidden dimension of $512$. 
To propagate probability scores through the tree, we utilize a small $2$-layer MLP with ReLU activation and dropout~\cite{srivastava2014dropout}. 

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{figures/ndcg_plots_10_one_row.pdf}
    \caption{NDCG@10 (y-axis) as a function of the representation size (x-axis) for all four datasets. Metrics for other values of top-$k$ are reported in the \Cref{app:experiments}.}
    \label{fig:sparse-rep-main}
\end{figure*}


\subsection{Comparison with Representation Learning Methods}
For these experiments, all methods are evaluated using the flat index by \citet{douze2024faiss}, with exact search for retrieving the top-$k$ contexts and their rankings. These rankings are determined based on either the cosine similarity (\textsc{BGE}) or the nTVD (\textsc{Hier-GMM} and \ourmodel{}) between query and context representations. As nearest neighbor search dominates the inference running times, memory and latency of all methods scale linearly with the embedding dimensionality, hence their inference costs are similar for the same representation size.

\paragraph{Full Representations}
In \Cref{tab:full-rep-main} we report recall$@10$ and NDCG$@10$ for all representation learning methods, using their full representations (at leaf level for \ourmodel{} and \textsc{Hier-GMM}).
We first observe that BGE and \ourmodel{} have generally similar performance on all datasets, which suggests that learning a tree retains the encoder's representational power. 
To assess whether this is a general property of models learning soft clustering, we report the retrieval performance of \textsc{Hier-GMM}, using its node assignments as representations.
The results indicate that the extracted representations are clearly not suited for retrieval purposes, stressing the need to directly optimize the retrieval objective and not a proxy one, as we do.

Adding constraints to the learning problem typically results in lower performance as compared to an unconstrained problem. 
To quantify the performance degradation due to our tree constraints, we also report the performance of probabilistic assignments as extracted by a flat model, that we name \textbf{\ourmodel{}-\textsc{no tree}}.
The only difference with \ourmodel{} is that the tree structure and propagation are removed from the model.
When lifting the tree constraints, we indeed observe a performance improvement on 5 out of 8 cases. 
However, such improvement comes at the price of losing the coarse-to-fine benefits of a tree structure.

We finally report results for the models trained with specialized procedures that encourage good coarse representations: BGE learned via the MRL (BGE-MRL) training objective and \ourmodel{} learned with the stochastic depth scheduler (\ourmodel{}-Stochastic).
We remark a consistent deterioration of the learned full representations across methods and datasets (expect for BGE-MRL's NDCG${@}10$ on \textsc{RepLiQA}), a phenomenon that we further analyze below.

\paragraph{Coarse-to-Fine Representations}
In \Cref{fig:sparse-rep-main} we plot retrieval performance as a function of the representation size, to study the cost-utility trade-off. 
As coarse embeddings, we use the node assignments at level $h$ for \ourmodel{}, with $h \in \{1, \dots, 10\}$, which gives representations of size $2^h$.
For BGE, we use the first $2^h$ dimensions of its output embeddings, as indicated in the MRL framework and in order to compare performance at the same representation size.
We confirm the previous observation that coarse-to-fine learning techniques (MRL and Stochastic) generally improve the retrieval performance of coarser embeddings, although at the expense of the quality of the finer ones.
Furthermore \ourmodel{} consistently provides the best results at the coarsest levels (for representation sizes up to $32$), while being competitive or better than BGE-MRL for finer levels on \textsc{NQ} and \textsc{HotpotQA}.
We observe the greatest performance degradation for \ourmodel{}'s full representations on \textsc{RepLiQA}, where MRL surprisingly boosts BGE performance for any dimensionality.

\begin{table*}
\centering
\caption*{Tables 2-3: Comparison of tree-based methods on two datasets by retrieval latency (Lat., in ms) and NDCG${@}10$. 
\textsc{Raptor} has the advantage of having been fit on the test corpus and runs Out-Of-Time (24h limit) on the all-contexts setting.}
\label{tab:tree-comparison-by-latency}
\begin{minipage}{0.48\textwidth}
    \centering
    \caption{\textsc{NQ} dataset}
    \label{tab:tree-nq}
    \scalebox{0.8}{
    \begin{sc}
    \begin{tabular}{lcccc}
    \toprule
    \textbf{Method} & \multicolumn{2}{c}{\textbf{Test Contexts}} & \multicolumn{2}{c}{\textbf{All Contexts}} \\
    & Lat. & NDCG${@}10$ & Lat. & NDCG${@}10$\\ \midrule
    Hier-Kmeans & 293 & 0.7188 & 2910 & 0.3334 \\
    \textsc{Hier-GMM}  & 1531 & 0.1637  & 1536 & 0.0030\\
    Raptor$^{*}$ & 266 &  0.1618 & OOT & OOT\\
    \ourmodel{}  & \textbf{20} & \textbf{0.7599} & \textbf{64} & \textbf{0.5496}\\
    \bottomrule
    \end{tabular}
    \end{sc}}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
    \centering
    \caption{\textsc{RepLiQA} dataset}
    \label{tab:tree-repliqa}
    \scalebox{0.8}{
    \begin{sc}
    \begin{tabular}{lcccc}
    \toprule
    \textbf{Method} & \multicolumn{2}{c}{\textbf{Test Contexts}} & \multicolumn{2}{c}{\textbf{All Contexts}} \\
    & Lat. & NDCG${@}10$ & Lat. & NDCG${@}10$\\ \midrule
    Hier-Kmeans & 196 & 0.8254 & 2770 & 0.7348\\
    \textsc{Hier-GMM} & 1310 & 0.1004 & 1516 & 0.0851\\
    Raptor$^{*}$ & 474 & 0.0678 & OOT & OOT\\
    \ourmodel{}  & \textbf{24} & \textbf{0.8329} & \textbf{33} & \textbf{0.7957}  \\
    \bottomrule
    \end{tabular}
    \end{sc}}
\end{minipage}
\end{table*}


\subsection{Comparison with Tree-based Methods}
In \Cref{tab:tree-nq,tab:tree-repliqa} we compare retrieval performance and inference time of tree-based methods. 
For these experiments, we report results both using the full pool of contexts and only the test-split ones, as it would be extremely expensive to build and slow to run \textsc{raptor} on the entire context corpus.
Remarkably, \ourmodel{} achieves the best retrieval accuracy while being several order faster to run than other hierarchical retrieval methods.
Its low latency is due to the use of parallel computing for traversing it.
Unlike \ourmodel{}, the other methods need to evaluate the split nodes sequentially to obtain a leaf assignment, which significantly slows down inference.
The second best performing method is \textsc{Hier-Kmeans}, although the gap with our model and its latency increase when testing with all contexts, which hints to poor scalability.
Finally, \textsc{raptor} fares the worst, despite its use of LLM calls for building the tree.



\section{Inspecting the Tree}
\label{sec:interpretability}

By learning node assignment probabilities, \ourmodel{} has the appealing side-effect of providing an organization of reference documents, which serves as an interface for inspecting the model and analyzing the contexts. 
In this section, we probe \ourmodel{}'s organization via several tools and report evidence that semantic groupings naturally emerge when optimizing for retrieval.

\subsection{Tree Congruence}
We first verify whether \ourmodel{} learns representations that are congruent with its tree structure,despite being trained solely on query-context alignment labels. 

We begin by investigating whether the node embeddings of the cross-attention splits align with the tree structure, i.e., whether their similarity inversely correlates with their distance in the tree.
In \autoref{fig:cosine_sim}(left) we plot the average cosine similarity between the node embeddings of a node and its descendants as a function of their tree distance (expressed as their hop distance). 
Cosine similarity clearly decreases as the distance increases, demonstrating that the learned embeddings reflect the tree structure. 
See Appendix \ref{app:cosine_sim_between_nodes} for an extended analysis. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.47\columnwidth]{plots/node_embedding_distances/node_embeddings_avg_sim_ancestor.pdf}
    \hfill
    \includegraphics[width=0.42\columnwidth]{plots/node_embedding_distances/ctxtSim_vs_lca.png}
        
    \caption{(left) Cosine similarity between node embeddings decreases with their pairwise distance in the tree. (right) Pairwise cosine similarity between context embeddings increases with the depth of LCA. The red lines indicate the average pairwise cosine similarity: between node embedding pairs on the left and between randomly selected context pairs on the right.}

    \label{fig:cosine_sim}
\end{figure}

Next, we assess whether semantically similar contexts are assigned to nearby nodes. To do so, we measure the average cosine similarity between all pairs of context embeddings (extracted with BGE-large) grouped by the depth %for different depths 
of their \emph{lowest common ancestor} (LCA) -- the lowest node in the tree that has both contexts as a descendant.
The hypothesis is that contexts routed to different nodes in earlier layers (whose LCA is shallow and closer to the root) should be thematically more distinct than those split later in the tree. As shown in \Cref{fig:cosine_sim}(right), the average context similarity increases with deeper LCAs, indicating that \ourmodel{} has learned a semantically coherent organization. 


\subsection{Topics and Keywords Coherence}
To further inspect the learned structure in a human-understandable way, we resort to topic models for extracting and analyzing the distribution of topics and keywords of the contexts assigned to nodes across the tree. 
To better understand how \ourmodel{} organizes contexts, we present a visualization of the \ourmodel{} tree trained on \textsc{NQ}, highlighting the topics and keywords associated with several nodes (Figure~\ref{fig:tree-inspection}).
For each node $t$, we collect all contexts assigned to the leaves of the subtree rooted at $t$ and extract topics and keywords using the method from \citet{kilgarriff2009simple}.
A quick inspection of these keywords reveals the hierarchical structure reflects the semantic relationships of the contexts. For example, the node whose contexts are on the topic of \emph{media}  (node 5) has child nodes focused on \emph{publishing} and \emph{TV}. Furthermore, the path from node 5 to node 54 illustrates a consistent refinement of topics and keywords, progressing from \emph{media} down to \emph{Television seasons}.

\section{Discussion and Future Work}
In this work, we introduced a retrieval system that generates tree-based representations optimized for retrieval. Our approach provides flexible control over the trade-off between representation size, inference latency, and retrieval accuracy. Unlike black-box models, \ourmodel{} allows us to inspect the model by examining how information is traversed and stored at different levels.
A natural extension of this work is to learn the tree structure including the width and depth adaptively, or pruning the tree dynamically based on retrieval needs. Another direction is to develop tools for analyzing the hierarchical structure, such as a summary decoder that explains what a node represents based on its learned embeddings. An important challenge is adapting a learned tree to new tasks or datasets‚Äîwhether certain subtrees can be updated or new ones grown while keeping the rest of the tree unchanged, or if full retraining is necessary. 

\section*{Impact Statement}
This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences
of our work, none of which we feel must be specifically highlighted here.
We however believe that contributing towards transparent and verifiable retrievers should benefit society in the long term, and is essential in fostering trust and accountability of retrieval systems.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BIBLIOGRAPHY
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{plainnat}

\begin{thebibliography}{46}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Adlakha et~al.(2022)Adlakha, Dhuliawala, Suleman, de~Vries, and Reddy]{adlakha2022topiocqa}
Vaibhav Adlakha, Shehzaad Dhuliawala, Kaheer Suleman, Harm de~Vries, and Siva Reddy.
\newblock Topiocqa: Open-domain conversational question answering with topic switching.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 10:\penalty0 468--483, 2022.

\bibitem[BehnamGhader et~al.(2024)BehnamGhader, Adlakha, Mosbach, Bahdanau, Chapados, and Reddy]{behnamghader2024llmvec}
Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas Chapados, and Siva Reddy.
\newblock {LLM}2vec: Large language models are secretly powerful text encoders.
\newblock In \emph{First Conference on Language Modeling}, 2024.
\newblock URL \url{https://openreview.net/forum?id=IW1PR7vEBf}.

\bibitem[Beltagy et~al.(2020)Beltagy, Peters, and Cohan]{beltagy2020longformer}
Iz~Beltagy, Matthew~E. Peters, and Arman Cohan.
\newblock Longformer: The long-document transformer, 2020.
\newblock URL \url{https://arxiv.org/abs/2004.05150}.

\bibitem[Bernhardsson(2017)]{bernhardsson2017annoy}
Erik Bernhardsson.
\newblock Annoy: approximate nearest neighbors in c++/python optimized for memory usage and loading/saving to disk.
\newblock \emph{GitHub https://github. com/spotify/annoy}, pages 0--5, 2017.

\bibitem[Breiman(2017)]{breiman2017classification}
Leo Breiman.
\newblock \emph{Classification and regression trees}.
\newblock Routledge, 2017.

\bibitem[Cai et~al.(2019)Cai, Gan, Wang, Zhang, and Han]{cai2019once}
Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han.
\newblock Once-for-all: Train one network and specialize it for efficient deployment.
\newblock \emph{arXiv preprint arXiv:1908.09791}, 2019.

\bibitem[Chen et~al.(2023)Chen, Pasunuru, Weston, and Celikyilmaz]{chen2023walking}
Howard Chen, Ramakanth Pasunuru, Jason Weston, and Asli Celikyilmaz.
\newblock Walking down the memory maze: Beyond context limit through interactive reading.
\newblock \emph{arXiv preprint arXiv:2310.05029}, 2023.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2019-bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language understanding.
\newblock In Jill Burstein, Christy Doran, and Thamar Solorio, editors, \emph{Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N19-1423}.
\newblock URL \url{https://aclanthology.org/N19-1423}.

\bibitem[Douze et~al.(2024)Douze, Guzhva, Deng, Johnson, Szilvasy, Mazar√©, Lomeli, Hosseini, and J√©gou]{douze2024faiss}
Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel Mazar√©, Maria Lomeli, Lucas Hosseini, and Herv√© J√©gou.
\newblock The faiss library.
\newblock 2024.

\bibitem[Edge et~al.(2024)Edge, Trinh, Cheng, Bradley, Chao, Mody, Truitt, and Larson]{edge2024local}
Darren Edge, Ha~Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson.
\newblock From local to global: A graph rag approach to query-focused summarization, 2024.
\newblock URL \url{https://arxiv.org/abs/2404.16130}.

\bibitem[Gao et~al.(2023)Gao, Xiong, Gao, Jia, Pan, Bi, Dai, Sun, and Wang]{gao2023retrieval}
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi~Dai, Jiawei Sun, and Haofen Wang.
\newblock Retrieval-augmented generation for large language models: A survey.
\newblock \emph{arXiv preprint arXiv:2312.10997}, 2023.

\bibitem[Irsoy et~al.(2012)Irsoy, Y{\i}ld{\i}z, and Alpayd{\i}n]{irsoy2012soft}
Ozan Irsoy, Olcay~Taner Y{\i}ld{\i}z, and Ethem Alpayd{\i}n.
\newblock Soft decision trees.
\newblock In \emph{Proceedings of the 21st international conference on pattern recognition (ICPR2012)}, pages 1819--1822. IEEE, 2012.

\bibitem[Izacard and Grave(2021)]{izacard-grave-2021-leveraging}
Gautier Izacard and Edouard Grave.
\newblock Leveraging passage retrieval with generative models for open domain question answering.
\newblock In Paola Merlo, Jorg Tiedemann, and Reut Tsarfaty, editors, \emph{Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume}, pages 874--880, Online, April 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.eacl-main.74}.
\newblock URL \url{https://aclanthology.org/2021.eacl-main.74}.

\bibitem[Karpukhin et~al.(2020)Karpukhin, Oguz, Min, Lewis, Wu, Edunov, Chen, and Yih]{karpukhin-2020-dpr}
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih.
\newblock Dense passage retrieval for open-domain question answering.
\newblock In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pages 6769--6781, Online, November 2020. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.emnlp-main.550}.
\newblock URL \url{https://aclanthology.org/2020.emnlp-main.550}.

\bibitem[Khattab and Zaharia(2020)]{khattab2020colbert}
Omar Khattab and Matei Zaharia.
\newblock Colbert: Efficient and effective passage search via contextualized late interaction over bert.
\newblock In \emph{Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval}, pages 39--48, 2020.

\bibitem[Kilgarriff(2009)]{kilgarriff2009simple}
Adam Kilgarriff.
\newblock Simple maths for keywords.
\newblock In \emph{Proceedings of the Corpus Linguistics Conference 2009 (CL2009),}, page 171, 2009.

\bibitem[Krishnamurthy et~al.(2012)Krishnamurthy, Balakrishnan, Xu, and Singh]{krishnamurthy2012efficient}
Akshay Krishnamurthy, Sivaraman Balakrishnan, Min Xu, and Aarti Singh.
\newblock Efficient active algorithms for hierarchical clustering.
\newblock \emph{International Conference on Machine Learning}, 2012.

\bibitem[Kusupati et~al.(2022)Kusupati, Bhatt, Rege, Wallingford, Sinha, Ramanujan, Howard-Snyder, Chen, Kakade, Jain, et~al.]{kusupati2022matryoshka}
Aditya Kusupati, Gantavya Bhatt, Aniket Rege, Matthew Wallingford, Aditya Sinha, Vivek Ramanujan, William Howard-Snyder, Kaifeng Chen, Sham Kakade, Prateek Jain, et~al.
\newblock Matryoshka representation learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 30233--30249, 2022.

\bibitem[Kwiatkowski et~al.(2019)Kwiatkowski, Palomaki, Redfield, Collins, Parikh, Alberti, Epstein, Polosukhin, Devlin, Lee, et~al.]{kwiatkowski2019natural}
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et~al.
\newblock Natural questions: a benchmark for question answering research.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 7:\penalty0 453--466, 2019.

\bibitem[Lewis et~al.(2020)Lewis, Perez, Piktus, Petroni, Karpukhin, Goyal, K{\"u}ttler, Lewis, Yih, Rockt{\"a}schel, et~al.]{lewis2020retrieval}
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K{\"u}ttler, Mike Lewis, Wen-tau Yih, Tim Rockt{\"a}schel, et~al.
\newblock Retrieval-augmented generation for knowledge-intensive nlp tasks.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 9459--9474, 2020.

\bibitem[Loshchilov(2017)]{loshchilov2017decoupled}
I~Loshchilov.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.

\bibitem[Malkov and Yashunin(2018)]{malkov2018efficient}
Yu~A Malkov and Dmitry~A Yashunin.
\newblock Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs.
\newblock \emph{IEEE transactions on pattern analysis and machine intelligence}, 42\penalty0 (4):\penalty0 824--836, 2018.

\bibitem[Marton et~al.(2024)Marton, L{\"u}dtke, Bartelt, and Stuckenschmidt]{marton2024gradtree}
Sascha Marton, Stefan L{\"u}dtke, Christian Bartelt, and Heiner Stuckenschmidt.
\newblock Gradtree: Learning axis-aligned decision trees with gradient descent.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~38, pages 14323--14331, 2024.

\bibitem[Monath et~al.(2019)Monath, Zaheer, Silva, McCallum, and Ahmed]{monath2019gradient}
Nicholas Monath, Manzil Zaheer, Daniel Silva, Andrew McCallum, and Amr Ahmed.
\newblock Gradient-based hierarchical clustering using continuous representations of trees in hyperbolic space.
\newblock In \emph{Proceedings of the 25th acm sigkdd international conference on knowledge discovery \& data mining}, pages 714--722, 2019.

\bibitem[Monteiro et~al.(2024{\natexlab{a}})Monteiro, Marcotte, No{\"e}l, Zantedeschi, V{\'a}zquez, Chapados, Pal, and Taslakian]{monteiro2024xc}
Jo{\~a}o Monteiro, {\'E}tienne Marcotte, Pierre-Andr{\'e} No{\"e}l, Valentina Zantedeschi, David V{\'a}zquez, Nicolas Chapados, Christopher Pal, and Perouz Taslakian.
\newblock Xc-cache: Cross-attending to cached context for efficient llm inference.
\newblock \emph{EMNLP}, 2024{\natexlab{a}}.

\bibitem[Monteiro et~al.(2024{\natexlab{b}})Monteiro, Noel, Marcotte, Rajeswar, Zantedeschi, Vazquez, Chapados, Pal, and Taslakian]{monteiro2024repliqa}
Joao Monteiro, Pierre-Andre Noel, {\'E}tienne Marcotte, Sai Rajeswar, Valentina Zantedeschi, David Vazquez, Nicolas Chapados, Christopher Pal, and Perouz Taslakian.
\newblock Repli{QA}: A question-answering dataset for benchmarking {LLM}s on unseen reference content.
\newblock In \emph{The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track}, 2024{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=4diKTLmg2y}.

\bibitem[Moseley and Wang(2023)]{moseley2023approximation}
Benjamin Moseley and Joshua~R Wang.
\newblock Approximation bounds for hierarchical clustering: Average linkage, bisecting k-means, and local search.
\newblock \emph{Journal of Machine Learning Research}, 24\penalty0 (1):\penalty0 1--36, 2023.

\bibitem[Oord et~al.(2018)Oord, Li, and Vinyals]{oord2018representation}
Aaron van~den Oord, Yazhe Li, and Oriol Vinyals.
\newblock Representation learning with contrastive predictive coding.
\newblock \emph{arXiv preprint arXiv:1807.03748}, 2018.

\bibitem[Quinlan(1986)]{quinlan1986induction}
J.~Ross Quinlan.
\newblock Induction of decision trees.
\newblock \emph{Machine learning}, 1:\penalty0 81--106, 1986.

\bibitem[Quinlan(2014)]{quinlan2014c4}
J~Ross Quinlan.
\newblock \emph{C4. 5: programs for machine learning}.
\newblock Elsevier, 2014.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et~al.
\newblock Learning transferable visual models from natural language supervision.
\newblock In \emph{International conference on machine learning}, pages 8748--8763. PMLR, 2021.

\bibitem[Salton and Buckley(1988)]{salton-tfidf}
Gerard Salton and Christopher Buckley.
\newblock Term-weighting approaches in automatic text retrieval.
\newblock \emph{Information Processing \& Management}, 24\penalty0 (5):\penalty0 513--523, 1988.
\newblock ISSN 0306-4573.
\newblock \doi{https://doi.org/10.1016/0306-4573(88)90021-0}.
\newblock URL \url{https://www.sciencedirect.com/science/article/pii/0306457388900210}.

\bibitem[Santhanam et~al.(2021)Santhanam, Khattab, Saad-Falcon, Potts, and Zaharia]{santhanam2021colbertv2}
Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia.
\newblock Colbertv2: Effective and efficient retrieval via lightweight late interaction.
\newblock \emph{arXiv preprint arXiv:2112.01488}, 2021.

\bibitem[Sarthi et~al.(2024)Sarthi, Abdullah, Tuli, Khanna, Goldie, and Manning]{sarthi2024raptor}
Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher~D Manning.
\newblock Raptor: Recursive abstractive processing for tree-organized retrieval.
\newblock \emph{arXiv preprint arXiv:2401.18059}, 2024.

\bibitem[Shuster et~al.(2021)Shuster, Poff, Chen, Kiela, and Weston]{shuster2021-retrieval-augmentation}
Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston.
\newblock Retrieval augmentation reduces hallucination in conversation.
\newblock In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, \emph{Findings of the Association for Computational Linguistics: EMNLP 2021}, pages 3784--3803, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.findings-emnlp.320}.
\newblock URL \url{https://aclanthology.org/2021.findings-emnlp.320}.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and Salakhutdinov]{srivastava2014dropout}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock \emph{The journal of machine learning research}, 15\penalty0 (1):\penalty0 1929--1958, 2014.

\bibitem[Tanno et~al.(2019)Tanno, Arulkumaran, Alexander, Criminisi, and Nori]{tanno2019adaptive}
Ryutaro Tanno, Kai Arulkumaran, Daniel Alexander, Antonio Criminisi, and Aditya Nori.
\newblock Adaptive neural trees.
\newblock In \emph{International Conference on Machine Learning}, pages 6166--6175. PMLR, 2019.

\bibitem[Van Der~Maaten et~al.(2009)Van Der~Maaten, Postma, Van~den Herik, et~al.]{van2009dimensionality}
Laurens Van Der~Maaten, Eric Postma, Jaap Van~den Herik, et~al.
\newblock Dimensionality reduction: a comparative.
\newblock \emph{J Mach Learn Res}, 10\penalty0 (66-71), 2009.

\bibitem[Vaswani(2017)]{vaswani2017attention}
A~Vaswani.
\newblock Attention is all you need.
\newblock \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Warner et~al.(2024)Warner, Chaffin, Clavi√©, Weller, Hallstr√∂m, Taghadouini, Gallagher, Biswas, Ladhak, Aarsen, Cooper, Adams, Howard, and Poli]{warner2024modernBERT}
Benjamin Warner, Antoine Chaffin, Benjamin Clavi√©, Orion Weller, Oskar Hallstr√∂m, Said Taghadouini, Alexis Gallagher, Raja Biswas, Faisal Ladhak, Tom Aarsen, Nathan Cooper, Griffin Adams, Jeremy Howard, and Iacopo Poli.
\newblock Smarter, better, faster, longer: A modern bidirectional encoder for fast, memory efficient, and long context finetuning and inference, 2024.
\newblock URL \url{https://arxiv.org/abs/2412.13663}.

\bibitem[Xiao et~al.(2023)Xiao, Liu, Zhang, and Muennighoff]{bge_embedding}
Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff.
\newblock C-pack: Packaged resources to advance general chinese embedding, 2023.

\bibitem[Xiao et~al.(2024)Xiao, Liu, Zhang, Muennighoff, Lian, and Nie]{xiao-bge}
Shitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muennighoff, Defu Lian, and Jian-Yun Nie.
\newblock C-pack: Packed resources for general chinese embeddings.
\newblock In \emph{Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval}, SIGIR '24, page 641‚Äì649, New York, NY, USA, 2024. Association for Computing Machinery.
\newblock ISBN 9798400704314.
\newblock \doi{10.1145/3626772.3657878}.
\newblock URL \url{https://doi.org/10.1145/3626772.3657878}.

\bibitem[Yang et~al.(2018{\natexlab{a}})Yang, Morillo, and Hospedales]{yang2018deep}
Yongxin Yang, Irene~Garcia Morillo, and Timothy~M Hospedales.
\newblock Deep neural decision trees.
\newblock \emph{arXiv preprint arXiv:1806.06988}, 2018{\natexlab{a}}.

\bibitem[Yang et~al.(2018{\natexlab{b}})Yang, Qi, Zhang, Bengio, Cohen, Salakhutdinov, and Manning]{yang2018hotpotqa}
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William~W Cohen, Ruslan Salakhutdinov, and Christopher~D Manning.
\newblock Hotpotqa: A dataset for diverse, explainable multi-hop question answering.
\newblock \emph{arXiv preprint arXiv:1809.09600}, 2018{\natexlab{b}}.

\bibitem[Yu et~al.(2018)Yu, Yang, Xu, Yang, and Huang]{yu2018slimmable}
Jiahui Yu, Linjie Yang, Ning Xu, Jianchao Yang, and Thomas Huang.
\newblock Slimmable neural networks.
\newblock \emph{arXiv preprint arXiv:1812.08928}, 2018.

\bibitem[Zantedeschi et~al.(2021)Zantedeschi, Kusner, and Niculae]{pmlr-v139-zantedeschi21a}
Valentina Zantedeschi, Matt Kusner, and Vlad Niculae.
\newblock Learning binary decision trees by argmin differentiation.
\newblock In Marina Meila and Tong Zhang, editors, \emph{Proceedings of the 38th International Conference on Machine Learning}, volume 139 of \emph{Proceedings of Machine Learning Research}, pages 12298--12309. PMLR, 18--24 Jul 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/zantedeschi21a.html}.

\end{thebibliography}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\input{appendix}


\end{document}
