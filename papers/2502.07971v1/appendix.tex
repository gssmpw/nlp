\section{\ourmodel{}'s Additional Design Options}
\ourmodel{} can be instantiated in many different ways. In this section we report a selection of the design choices we explored in our experiments and that are implemented in our codebase.

\subsection{Split Functions}
\label{app:split}
A split function $s_{\theta_t}: \mathcal{X} \to [0, 1]$ determines the routing probability of an input $\xb_i \in \mathcal{X}$ through node $t$. The primary requirement for a split function is that it outputs a scalar $\in \mathbb{R}$, based on which we compute the probabilities  of routing the input to node $t$'s left or right children $t_{\text{left}}$ and $t_{\text{right}}$ (see \Cref{app:propagation}).
Below, we describe different types of split functions that can be used.

\subsubsection{Linear Split Function}
The simplest form of a split function is a linear projection, similar to the approach used in~\cite{pmlr-v139-zantedeschi21a}. For a given split node $t$, with left and right children $t_{\text{left}}$ and $t_{\text{right}}$, the split function is defined as:
\begin{equation}
    s_{\theta_t}(\xb_i) = \theta_t^\top \xb_i,
\end{equation}
where $\theta_t \in \mathcal{X}$ represents a learnable hyperplane.
Each split node learns a separate hyperplane, and there are no shared parameters across the tree.

\subsubsection{MLP Split Function}
A more expressive alternative is to use a learnable neural network, modeled as a Multi-Layer Perceptron (MLP). This MLP $S_{\Theta}: \mathcal{X} \to \mathbb{R}^{|\mathcal{T}_B|}$ maps an input $\xb_i$ to a routing probability for each branching node in the tree, where $\mathcal{T}_B$ is the set of branching nodes. The MLP consists of multiple layers with nonlinear activations, such as ReLU, and incorporates dropout for regularization. Unlike the linear split function, which maintains separate parameters per node, the MLP split function shares parameters across different nodes while still allowing for node-specific learning.

\subsubsection{Cross-Attention Split Function}
\label{app:cross_attention_split_fn}
While the linear and MLP split functions operate on dense passage embeddings for the entire document, the cross-attention split function allows us to leverage token-level embeddings for more expressive routing. This method utilizes a cross-attention mechanism between learnable node embeddings and text tokens to determine the routing probabilities at each split node.

Let the input text $\xb_i \in \mathbb{R}^{n_d \times d_{\text{emb}}}$ consist of $n_d$ embedded tokens, encoded by the encoder $E$. Instead of a simple projection, we introduce learnable node embeddings $\eb_t \in \mathbb{R}^{n_e \times d_{\text{emb}}^{\prime}}$ for each node $t$. These node embeddings interact with the text tokens via a cross-attention mechanism, where the node embeddings serve as queries, and the text tokens provide keys and values. 

We define the attention mechanism as follows:
\begin{equation}
    \text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q} \mathbf{K}^\top}{\sqrt{d_k}}\right) \mathbf{V},
\end{equation}
where:
\begin{align}
    \mathbf{Q} &= \eb_t \mathbf{W}_q^\top, \\
    \mathbf{K} &= \xb_i \mathbf{W}_k^\top, \\
    \mathbf{V} &= \xb_i \mathbf{W}_v^\top.
\end{align}
Here, $\mathbf{W}_q \in \mathbb{R}^{d_k \times d_{\text{emb}}^{\prime}}$, $\mathbf{W}_k \in \mathbb{R}^{d_k \times d_{\text{emb}}}$, and $\mathbf{W}_v \in \mathbb{R}^{d_k \times d_{\text{emb}}}$ are learnable projection matrices shared across the tree, while $d_k$ represents the dimension of the projected queries and keys.
This formulation describes a single-head attention mechanism but can be naturally extended to multi-head attention by introducing independent projection matrices for each head and concatenating the resulting outputs.

The transformed node embeddings are then aggregated via a node-specific function to produce the final routing score. This aggregation is discussed in detail in Appendix \ref{app:score_aggregation}, and shown in \autoref{fig:cross_attn_split_fn_with_scoring}. This mechanism significantly increases the expressivity of the split function compared to a simple linear projection. The node embeddings and projection matrices act as memory representations, storing information from past query and context embeddings, which enhances the model's ability to score inputs effectively.

We compare the effect of different split functions on retrieval performance across various datasets in \autoref{fig:split_function_ablations}. Notably, the cross-attention split function achieves the best retrieval metrics across all datasets, while the MLP split function worse than the linear one.
This surprising result can be explained by highlighting that both cross-attention and linear splits learn virtual embeddings (the node embeddings in the cross attention and the hyper-plane in the linear one) and compare them with the input. Further investigation would be required to confirm this speculation.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/Cross_Attn_2.pdf}
    \caption{\ourmodel{}'s cross attention split function with node scoring done by a per node linear map followed by a mean of scores.}
    \label{fig:cross_attn_split_fn_with_scoring}
\end{figure}

\begin{figure}[h]
    \centering
    \subfigure[NQ]{
        \includegraphics[width=0.48\textwidth]{plots/split/learned-prop/nq.pdf}
        \label{fig:split_nq}
    }
    % \hfill
    \subfigure[REPLIQA]{
        \includegraphics[width=0.48\textwidth]{plots/split/learned-prop/repliqa.pdf}
        \label{fig:split_repliqa}
    }

    \subfigure[HOTPOTQA]{
        \includegraphics[width=0.48\textwidth]{plots/split/learned-prop/hotpotqa.pdf}
        \label{fig:split_hotpotqa}
    }
    % \hfill
    \subfigure[TOPIOCQA]{
        \includegraphics[width=0.48\textwidth]{plots/split/learned-prop/topiocqa.pdf}
        \label{fig:split_topiocqa}
    }

    \caption{Effect of using various split functions on different datasets. The x-axis correspond to the representation level, i.e. the depth of the tree from which these representations are taken. Hence, a level $k$ corresponds to embeddings of size~$2^k$.}
    \label{fig:split_function_ablations}
\end{figure}


\begin{figure}[h]
    \centering
    \subfigure[NQ]{
        \includegraphics[width=0.48\textwidth]{plots/scoring_fn_ablations/nq.pdf}
        \label{fig:split_nq}
    }
    % \hfill
    \subfigure[REPLIQA]{
        \includegraphics[width=0.48\textwidth]{plots/scoring_fn_ablations/repliqa.pdf}
        \label{fig:split_repliqa}
    }

    % \vspace{0.5cm}

   \subfigure[HOTPOTQA]{
        \includegraphics[width=0.48\textwidth]{plots/scoring_fn_ablations/hotpotqa.pdf}
        \label{fig:split_hotpotqa}
    }
    % \hfill
    \subfigure[TOPIOCQA]{
        \includegraphics[width=0.48\textwidth]{plots/scoring_fn_ablations/topiocqa.pdf}
        \label{fig:split_topiocqa}
    }

    \caption{Effect of using various node score aggregation methods over different datasets. We label the "representation level" on x-axis in these plots - which is the depth of the tree form which these representations are taken. Hence a level $k$ represents embeddings of size $2^k$.} 
    \label{fig:score_aggregator_ablations}
\end{figure}

\subsection{Cross-attention score aggregation}
\label{app:score_aggregation}
The cross-attention split function, described in Appendix \ref{app:cross_attention_split_fn}, uses node embeddings to compute a score for each node. This function consists of two components:
\begin{enumerate}
    \item The cross-attention mechanism, which processes the input by attending to node embeddings as described above. 
    \item The node scorer, which outputs a single score for each node based on the output of the attention mechanism.
\end{enumerate}

Here, we discuss different choices for the node scorer.  

For a given sample, the output $y$ of the cross-attention has the shape $[n_t, n_e, d_k]$, where $n_t$ is the number of tree nodes, $n_e$ is the number of embeddings per node, and $d_k$ is the dimension of the attention output projection. Since we need a single score per tree node, $y$ must be reduced to the shape $[n_t]$. This transformation can be performed in multiple ways, as detailed below:

\begin{itemize}
   \item \textbf{Mean then Linear}: In this approach, we first aggregate all embeddings for a given node by performing mean pooling over the $n_e$ dimension, resulting in a single embedding per node. A shared linear layer is then applied to map this embedding into a score, producing a final score for each node.

    \item \textbf{Per-Node Linear Map, then Mean of Scores}: Instead of using a shared transformation, this method learns a separate linear function for each node to map its $n_e$ embeddings into individual scores. The final score for the node is then computed as the mean of these scores. This approach allows each node to focus on different aspects of its embeddings, leading to more flexible representations. This scoring is illustrated in \autoref{fig:cross_attn_split_fn_with_scoring}.

    \item \textbf{Incorporating Tree Structure}: This method extends the previous approach by modifying the computed node scores to account for the hierarchical structure. Specifically, in addition to the score obtained per node, we refine these scores using a small MLP trained per node. This MLP takes as input the scores of the node and all of its ancestors, and outputs a modified score, allowing the scoring mechanism to incorporate hierarchical information learned by the tree.

\end{itemize}


In \autoref{fig:score_aggregator_ablations}, we compare how these different scoring methods affect the retrieval performance of \ourmodel{}. We observe that tree-based methods consistently perform the best across most datasets, highlighting the importance of incorporating hierarchical structure into the scoring mechanism. In all the experiments reported in this paper, we make use of this aggregation method.


\begin{figure}[h]
    \centering
    \subfigure[NQ]{
        \includegraphics[width=0.48\textwidth]{plots/last-level/nq.pdf}
        \label{fig:split_nq}
    }
    % \hfill
    \subfigure[REPLIQA]{
        \includegraphics[width=0.48\textwidth]{plots/last-level/repliqa.pdf}
        \label{fig:split_repliqa}
    }

    % \vspace{0.5cm}

   \subfigure[HOTPOTQA]{
        \includegraphics[width=0.48\textwidth]{plots/last-level/hotpotqa.pdf}
        \label{fig:split_hotpotqa}
    }
    % \hfill
    \subfigure[TOPIOCQA]{
        \includegraphics[width=0.48\textwidth]{plots/last-level/topiocqa.pdf}
        \label{fig:split_topiocqa}
    }

    \caption{Effect of different tree propagation methods across datasets. The x-axis represents the "representation level," which corresponds to the tree depth from which embeddings are extracted. A level $k$ indicates embeddings of size $2^k$. \textit{prod\_prop} refers to probability product-based propagation, as explained in Appendix \ref{app:propagation}. \textit{learned\_prop} represents a learned propagation method where node probabilities are determined by a node-specific network based on ancestor scores. }
 
    \label{fig:tree_propagation_ablation}
\end{figure}

\subsection{Tree propagation.}
\label{app:propagation}
Note that any split function defined above outputs a score that is independent of the scores from the other nodes.
However, the probability of reaching a node should be constrained by the probability of having reached its parent, the parent of its parent, and so on.
The simplest way of enforcing such tree constraints is by propagating the scores of the ancestors down to the node of interest by multiplying the probabilities along the path.
We refer to this type of tree propagation as \textit{product propagation}.
Consider a node $t$, its left and right children $t_{\text{left}}$ and $t_{\text{right}}$, and its ancestors $\mathcal{A}_t$ (the split nodes along the path from the root to $t$).
The probabilities of routing an input left or right based on node $t$'s split score are:
\begin{align*}
    \zb_{t_{\text{left}}}(\xb_i) &= \sigma(s_{\theta_t}(\xb_i)) \\
    \zb_{t_{\text{right}}}(\xb_i) &= 1 - \sigma(s_{\theta_t}(\xb_i))
\end{align*}
where $\sigma()$ is the sigmoid function.
Applying the sigmoid in this way ensures that the output probabilities of a split node always form a valid distribution (non-negative densities that sum to $1$).
Then, we constrain the probability of reaching node $t_{\text{left}}$ (and similarly node $t_{\text{right}}$) is defined as:
\begin{equation}
    T(\xb_i)_{t_{\text{left}}} = \zb_{t_{\text{left}}} \zb_{t} \prod_{a \in \mathcal{A}_t} \zb_{a}.
\end{equation}
Notice that the product propagation naturally enforces that the sum of node probabilities at any depth is always constant and equal to 1 (in particular, the sum of leaf assignments $\sum_{t \in \cT_L} T(\xb_i)_t = 1$).

Alternatively, one can learn how to propagate probability scores through the tree. 
We refer to this type of tree propagation as \textit{learned propagation}.
This enables a more expressive way of assigning probabilities to the leaf nodes compared to simply taking the product of probabilities along the path.
More precisely, the probability of a leaf can be defined as the output of a leaf-specific learnable function that takes as input the routing probabilities of all its ancestors. 
The leaf scores are then normalized across all leaves to sum to 1, forming a valid probability distribution. This ensures that the probabilities assigned to a leaf are a meaningful and learnable function of the routing probabilities of the nodes along its path. 
Then, the probabilities of internal nodes can be computed in a bottom-up fashion. Specifically, the probability of reaching an internal node \(t\) is computed as the sum of the probabilities of its two children. This process starts from the second-to-last level and proceeds upward toward the root. Since the leaf probabilities form a valid distribution, this approach ensures that the probability distribution at any tree depth is also valid. 

In \autoref{fig:tree_propagation_ablation}, we compare the learned propagation mechanism with the product propagation mechanism described above. We find that the learned propagation method always yields better metrics at the last level. We use this tree propagation method in our main model.

\section{Additional Evaluations}
\label{app:experiments}
We report additional metrics and evaluations for better understanding \ourmodel{}'s performance. 
\subsection{Extended Evaluation of Representation Learning Methods}
In \Cref{fig:sparse-rep-main1,fig:sparse-rep-main10,fig:sparse-rep-main100} we report NDCG${@}k$ and recall${@}k$ for $k \in \{1, 10, 100\}$. As noted elsewhere in our work, we observe here that models trained to optimize only the last layer perform best when evaluated at that level. However, introducing stochastic depth training to \ourmodel{} or training BGE with the MRL loss leads to significantly better representations at lower levels.

Among the two models trained to optimize all layers - \ourmodel{}-Stochastic-Depth and BGE-MRL — we find that the former consistently outperforms BGE-MRL at lower levels. Without exception, \ourmodel{} trained with stochastic depth provides the most effective representations at lower layers across all datasets on both Recall@k and NDCG@k metrics for all $k$ values. Moreover, on datasets like NQ, \ourmodel{}-Stochastic-Depth outperforms other methods across nearly all representation levels and for all recall and NDCG metrics.

We conclude that \ourmodel{} trained with stochastic depth offers an effective balance between representation length, query latency, and retrieval performance across a variety of datasets.
\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{figures/recall_ndcg_plots_1.pdf}
    \caption{NDCG@1 and recall@1 as a function of the representation size for four datasets.}
    \label{fig:sparse-rep-main1}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{figures/recall_ndcg_plots_10.pdf}
    \caption{NDCG${@}10$ and recall${@}10$ as a function of the representation size for four datasets. }
    \label{fig:sparse-rep-main10}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/recall_ndcg_plots_100.pdf}
    \caption{NDCG${@}100$ and recall${@}100$ as a function of the representation size for four datasets.}
    \label{fig:sparse-rep-main100}
\end{figure*}

\subsection{\ourmodel{} Parameter Sensitivity}

\subsubsection{Choice Of Base Encoder}
\label{app:encoder_choice}
In our main results, we use \texttt{bge-large} as the base bi-encoder model. Here, we experiment with alternative choices for the base encoder. The results across various datasets are shown in \autoref{fig:encoder_ablations}. We evaluate three models: BERT, BGE-Large (marked as \texttt{bge} in the plots), and BGEM3. Our findings indicate that \texttt{bge} consistently performs best across all datasets, while BERT yields the lowest performance.

\begin{figure}[h]
    \centering
    \subfigure[NQ]{
        \includegraphics[width=0.45\textwidth]{plots/encoders/nq.pdf}
    }
    % \hfill
    \subfigure[REPLIQA]{
        \includegraphics[width=0.45\textwidth]{plots/encoders/repliqa.pdf}
    }
   \subfigure[HOTPOTQA]{
        \includegraphics[width=0.45\textwidth]{plots/encoders/hotpotqa.pdf}
    }
    % \hfill
    \subfigure[TOPIOCQA]{
        \includegraphics[width=0.45\textwidth]{plots/encoders/topiocqa.pdf}
    }

    \caption{Effect of using various encoders as base encoders for \ourmodel{} over different datasets. We label the "representation level" on x-axis in these plots - which is the depth of the tree form which these representations are taken. Hence a level $k$ represents embeddings of size $2^k$.}
    \label{fig:encoder_ablations}
\end{figure}


\subsubsection{Depth of Trained Tree}
We experiment with training trees of varying depths using stochastic depth training to ensure meaningful representations at every level of the tree. The results are shown in \autoref{fig:depth_ablations}. We observe that increasing tree depth does not improve retrieval performance at intermediate levels. However, deeper trees yield better representations at the final level. Additionally, stochastic depth training leads to a trade-off: while it enhances intermediate representations, it results in slightly suboptimal performance at the deepest levels.
 

\label{app:depth_ablation}
\begin{figure}[h]
    \centering
    \subfigure[NQ]{
        \includegraphics[width=0.45\textwidth]{plots/depth_ablations/nq.pdf}
    }
    % \hfill
    \subfigure[REPLIQA]{
        \includegraphics[width=0.45\textwidth]{plots/depth_ablations/repliqa.pdf}
    }
   \subfigure[HOTPOTQA]{
        \includegraphics[width=0.45\textwidth]{plots/depth_ablations/hotpotqa.pdf}
    }
    % \hfill
    \subfigure[TOPIOCQA]{
        \includegraphics[width=0.45\textwidth]{plots/depth_ablations/topiocqa.pdf}
    }

    \caption{Effect of training \ourmodel{} trees of various depths  over different datasets. We label the "representation level" on x-axis in these plots - which is the depth of the tree form which these representations are taken. Hence a level $k$ represents embeddings of size $2^k$.}
    \label{fig:depth_ablations}
\end{figure}

\subsubsection{Depth scheduler}

% \vz{Shubham}

Training a binary tree-based model provides the opportunity to fetch text representations not only from the leaf nodes but also from any intermediate level of the tree. This enables the retrieval of smaller representations which help speed up inference time. While any trained Retreever tree allows extracting intermediate representations, we can further explicitly optimize these intermediate layers during training by incorporating them into the loss function. We achieve this through depth schedulers, which regulate how different tree depths contribute to training.

We experiment with the following types of depth schedulers:
\begin{itemize}
    \item \textbf{Constant Depth Training}: This is the standard training procedure for Retreever, where the loss function is computed only at the final layer of the tree. This approach trains representations at the deepest level but does not explicitly optimize for intermediate layers.
    \item \textbf{Adaptive Depth Increase} To enforce an inductive bias that encourages meaningful intermediate representations, we explicitly train on shallower layers as well. One way to achieve this is by gradually growing the tree depth during training. We experiment with two such depth increase schedules: linear and exponential.
    \item \textbf{Stochastic Depth Training} Unlike adaptive depth increase, which gradually expands the tree depth, stochastic depth training continuously trains all tree levels throughout the entire training process. In each training batch, we randomly select a tree depth, and during that batch, we train the tree truncated at that depth only. This ensures that all levels of the tree are trained throughout the training regime. Since training deeper layers is inherently harder than training shallower ones, we bias the depth selection probability to sample higher depths more frequently, ensuring sufficient training at deeper levels.
    \item \textbf{Matryoshka Embedding Style Training}: Inspired by Matryoshka Representations Learning \citep{kusupati2022matryoshka}, we adapt their training strategy to Retreever. Here, instead of computing a contrastive loss only at the final layer, we sum the contrastive losses from all layers, thus ensuring that all tree levels are explicitly trained during each training batch.
\end{itemize}

\autoref{fig:scheduler_ablations} presents the results of this ablation study. We make the following observations:

\begin{itemize}
\item The \textit{constant} scheduler consistently achieves the best performance at the highest tree level, whereas other schedulers outperform it at lower levels. This is expected, as the \textit{constant} scheduler does not explicitly optimize for intermediate layers.
    \item All alternative schedulers lag behind the \textit{constant} scheduler in at least one dataset/metric combination. This is also expected, as introducing inductive biases to train lower levels can result in a suboptimal final layer compared to a model trained explicitly for the last level.  

    \item The exponential depth increase scheduler proves to be highly disruptive to model learning. In contrast, a gradual linear depth increase closely matches the performance of the \textit{constant} variant at the highest level while providing better intermediate-level representations.  

    \item Both Stochastic Depth Training and MRL consistently underperform compared to the \textit{constant} scheduler at the highest level but significantly outperform it at lower levels. This occurs because both methods train all intermediate layers throughout the training regime, creating a tradeoff between optimizing only for the last level versus learning useful representations at all depths.  

    \item We observe that Stochastic Depth Training likely acts as a regularizer. This effect is particularly noticeable on the TOPIOCQA dataset, which is known to overfit. The fact that Stochastic Depth Training outperforms the constant scheduler at the highest level suggests that the added regularization helps in training a more robust \ourmodel{}.  
    \item While MRL and Stochastic Depth Training generally perform similarly, we note that Stochastic Depth Training achieves better results in certain cases, particularly in the REPLIQA and TOPIOCQA datasets. 
\end{itemize} 

Based on these findings, we use Stochastic Depth Training as the default schedule in our main model.

\begin{figure}[t]
    \centering
    \subfigure[NQ]{
        \includegraphics[width=0.45\textwidth]{plots/scheduler/nq.pdf}
    }
    % \hfill
    \subfigure[REPLIQA]{
        \includegraphics[width=0.45\textwidth]{plots/scheduler/repliqa.pdf}
    }
   \subfigure[HOTPOTQA]{
        \includegraphics[width=0.45\textwidth]{plots/scheduler/hotpotqa.pdf}
    }
    % \hfill
    \subfigure[TOPIOCQA]{
        \includegraphics[width=0.45\textwidth]{plots/scheduler/topiocqa-best.pdf}
    }

    \caption{Effect of using various depth schedulers with \ourmodel{} over different datasets. We label the "representation level" on x-axis in these plots - which is the depth of the tree form which these representations are taken. Hence a level $k$ represents embeddings of size $2^k$.}
    \label{fig:scheduler_ablations}
\end{figure}

\subsubsection{Cross-Attention Split Function Hyper Parameters}
\label{app:cross_attn_split_fn_params}
Here, we experiment with different hyperparameter choices for the cross-attention split function. \autoref{fig:n_embs} analyzes the effect of the number of embeddings per node, while \autoref{fig:n_heads} examines the impact of the number of attention heads. 

We observe that using $8$ heads yields the best performance across most datasets, which is the value we adopt for our main experiments. However, for TOPIOCQA, a much smaller value (such as $2$) performs best. This aligns with our expectation, as we find this dataset to be more prone to overfitting. 

Additionally, we did not observe a significant impact from varying the number of node embeddings across most datasets, except for TOPIOCQA, where using $3$ embeddings per node led to the best results. For our main experiments, we chose to use a single embedding per node.

\begin{figure}[h]
    \centering
    \subfigure[NQ]{
        \includegraphics[width=0.45\textwidth]{plots/n_heads_ablations/nq.pdf}
    }
    % \hfill
    \subfigure[REPLIQA]{
        \includegraphics[width=0.45\textwidth]{plots/n_heads_ablations/repliqa.pdf}
    }
   \subfigure[HOTPOTQA]{
        \includegraphics[width=0.45\textwidth]{plots/n_heads_ablations/hotpotqa.pdf}
    }
    % \hfill
    \subfigure[TOPIOCQA]{
        \includegraphics[width=0.45\textwidth]{plots/n_heads_ablations/topiocqa.pdf}
    }

    \caption{Effect of training the cross attention split function on various values of number of attention heads over different datasets. We label the "representation level" on x-axis in these plots - which is the depth of the tree form which these representations are taken. Hence a level $k$ represents embeddings of size $2^k$.}
    \label{fig:n_heads}
\end{figure}

\begin{figure}[h]
    \centering
    \subfigure[NQ]{
        \includegraphics[width=0.45\textwidth]{plots/n_embs_ablations/nq.pdf}
    }
    \subfigure[REPLIQA]{
        \includegraphics[width=0.45\textwidth]{plots/n_embs_ablations/repliqa.pdf}
    }
   \subfigure[HOTPOTQA]{
        \includegraphics[width=0.45\textwidth]{plots/n_embs_ablations/hotpotqa.pdf}
    }
    \subfigure[TOPIOCQA]{
        \includegraphics[width=0.45\textwidth]{plots/n_embs_ablations/topiocqa.pdf}
    }

    \caption{Effect of training the cross attention split function on various values of number of node embeddings over different datasets. We label the "representation level" on x-axis in these plots - which is the depth of the tree form which these representations are taken. Hence a level $k$ represents embeddings of size $2^k$.}
    \label{fig:n_embs}
\end{figure}

\subsubsection{Regularization}
\label{app:regularization}
We explore additional regularization techniques in the following ways:  
\begin{itemize}
    \item \textbf{Dropout on Node Scores}: We apply dropout to node scores before performing tree propagation to compute the final node probabilities. \autoref{fig:dropout} illustrates the effect of this regularization. We observe that it improves performance across all datasets, with the most significant impact on TOPIOCQA. Consequently, we incorporate this dropout in our main model.  

    \item \textbf{L1 Regularization}: We experiment with adding an L1 regularization term to the loss function to encourage sparser query and context embeddings. However, we do not observe a significant impact on any dataset. As a result, we do not include this regularization in our final model.  
\end{itemize}


\begin{figure}[h]
    \centering
    \subfigure[NQ]{
        \includegraphics[width=0.45\textwidth]{plots/dropout_options/nq.pdf}
    }
    % \hfill
    \subfigure[REPLIQA]{
        \includegraphics[width=0.45\textwidth]{plots/dropout_options/repliqa.pdf}
    }
   \subfigure[HOTPOTQA]{
        \includegraphics[width=0.45\textwidth]{plots/dropout_options/hotpotqa.pdf}
    }
    % \hfill
    \subfigure[TOPIOCQA]{
        \includegraphics[width=0.45\textwidth]{plots/dropout_options/topiocqa.pdf}
    }

    \caption{Effect of adding dropout on the node scores before applying tree based propagation. We label the "representation level" on x-axis in these plots - which is the depth of the tree form which these representations are taken. Hence a level $k$ represents embeddings of size $2^k$.}
    \label{fig:dropout}
\end{figure}

\begin{figure}[h]
    \centering
    \subfigure[NQ]{
        \includegraphics[width=0.45\textwidth]{plots/l1_ablations/nq.pdf}
    }
    % \hfill
    \subfigure[REPLIQA]{
        \includegraphics[width=0.45\textwidth]{plots/l1_ablations/repliqa.pdf}
    }
   \subfigure[HOTPOTQA]{
        \includegraphics[width=0.45\textwidth]{plots/l1_ablations/hotpotqa.pdf}
    }
    % \hfill
    \subfigure[TOPIOCQA]{
        \includegraphics[width=0.45\textwidth]{plots/l1_ablations/topiocqa.pdf}
    }

    \caption{Effect of training the \ourmodel{} with an additional L1-regularization term over the learned query and context embeddings. We label the "representation level" on x-axis in these plots - which is the depth of the tree form which these representations are taken. Hence a level $k$ represents embeddings of size $2^k$.}
    \label{fig:l1_reg}
\end{figure}

\subsection{Retreever Latency vs Representation Size}

We analyze the relationship between embedding size obtained from \ourmodel{} and its impact on inference latency and retrieval performance. As shown in \autoref{fig:latency_tradeoff}, the seconds per query (\texttt{sec/query}) increase as the representation size grows. This trend is expected, as larger representations require more computation and memory access during retrieval. We observe, that the inference latency is linearly related to the size of the representation obtained from \ourmodel{}. We also note from the figure that its possible to obtain a significant decrease in query latency without sacrificing on retrieval accuracy by much.

\autoref{fig:latency_tradeoff} further illustrates the trade-off between retrieval effectiveness, measured by NDCG@10, and query latency. While larger representations significantly increase model latency, they offer only marginal improvements in retrieval performance. This provides a way to balance query latency and retrieval performance by selecting an appropriate embedding size from \ourmodel{} to achieve the desired trade-off.


\begin{figure*}[h]
    \centering
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/latency_plots/size_vs_latency.pdf}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/latency_plots/latency_vs_performance.pdf}
        \label{fig:latency_vs_ndcg}
    \end{minipage}
    \caption{Trade-off between retrieval speed and effectiveness in Retreever. The left plot shows a linear dependence between the representation size from \ourmodel{} and the query latency, while the right plot demonstrates that a speedup in query inference with only a slight reduction in retrieval performance.}
    \label{fig:latency_tradeoff}
\end{figure*}

\subsection{Analyzing Node Embeddings}
\label{app:cosine_sim_between_nodes}
As observed earlier in Appendix~\ref{app:split}, the cross-attention split function is the most expressive among the different split functions we experimented with. We believe that learning dedicated node embeddings is crucial for effective retrieval, as they capture the semantic structure of the documents the model was trained on, allowing the model to route new text accordingly.

To analyze the structure of these embeddings, we conduct two experiments that examine how node embeddings relate to the hierarchical organization of the retrieval tree.

\begin{itemize}
    \item \textbf{Pairwise Cosine Similarity Between All Nodes}:
First, we analyze the overall structure of learned node embeddings by computing the pairwise cosine similarity between all tree nodes. Specifically, we measure the average similarity between node pairs as a function of their distance in the tree, where distance is defined as the length of the shortest path connecting two nodes. Our hypothesis is that nodes closer together in the tree should have more similar embeddings, while nodes farther apart should be more distinct. This is exactly what we observe in \autoref{fig:node_embedding_similarity}, where similarity decreases with increasing node distance, confirming that the learned embeddings reflect the hierarchical organization of the tree.
\item \textbf{Cosine Similarity Between Nodes and Their Descendants}: In addition to pairwise similarity across all nodes, we specifically examine how embedding similarity evolves between a node and its direct descendants. Since each node's embedding is expected to encode thematic information from past inputs, we hypothesize that embeddings should become progressively less similar as we move deeper in the tree. To test this, we compute the cosine similarity between a node and its descendants at varying depths, grouping values based on their ancestor-descendant distance. As shown in \autoref{fig:node_embedding_similarity}, similarity decreases with increasing distance, reinforcing the idea that the model learns hierarchical representations that respect the tree structure.
\end{itemize}


Together, these analyses provide strong evidence that the learned node embeddings preserve and make use of the hierarchical organization of the retrieval tree.  

\begin{figure*}[h]
    \centering
    \subfigure[All Pair Cosine Similarity]{
        \includegraphics[width=0.35\textwidth]{plots/node_embedding_distances/node_embeddings_avg_sim.pdf}
    }
    \subfigure[Tree Distances]{
        \includegraphics[width=0.25\textwidth]{plots/node_embedding_distances/distance_matrix.pdf}
    }
    \subfigure[Ancestor-Descendant Cosine Similarity]{
        \includegraphics[width=0.35\textwidth]{plots/node_embedding_distances/node_embeddings_avg_sim_ancestor.pdf}
    }
    
    \caption{Average cosine similarity between node embeddings for varying node distances $d$. The \ourmodel{} model used has a depth of $10$, resulting in a total of $2047$ nodes and $20$ unique distances between them. We observe from plots (a) and (c) that cosine similarity between node embeddings increases as the distance between nodes decreases, indicating that closer nodes have more similar embeddings. The red line represents the overall average similarity of all pairwise node embeddings, calculated as $0.22$. Plot (b) shows the tree distances between node pairs.}

    \label{fig:node_embedding_similarity}
\end{figure*}