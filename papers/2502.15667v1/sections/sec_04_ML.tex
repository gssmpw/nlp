\section{Maximum Likelihood Approach}\label{sec:ML}
% \subsection{Parameter Estimation using Maximal Likelihood}
% In this section, we use the maximum likelihood estimation (MLE) to estimate all the parameters. MLE is a statistical method that identifies the optimal parameter values by maximizing a likelihood function, which quantifies the probability of the observed data given the model parameters. By maximizing this function, MLE determines the parameters that make the observed measurements most probable under the assumed model. The process involves constructing the likelihood function based on the observed data and the parameterized model. In the following, we present the framework to use the MLE approach in solving Problem \ref{pr:Bilinear Observation}. 

To address the identification problem introduced in Section~\ref{sec:pf}, a natural choice is the maximum likelihood (ML) estimation method \cite{lehmann2006theory}, which identifies the parameters that maximize the likelihood of observed measurements under the assumed model. 
In this section, we outline how to apply the ML method to our identification problem.
More precisely, we derive the likelihood function based on the observed data and the parameterized model, and then, determine the values of the optimal parameters by maximizing the likelihood function. 

To derive the likelihood function and formulate the maximum likelihood scheme, we first define multi-tuple of parameters, denoted by  $\vctheta$, as
\begin{equation}
    {\vctheta} 
    := 
    \,\,(
    \mxA, 
    \mxB, 
    \mxC_0,\mxC_1,...,\mxC_{\Nu}, 
    \mxD, 
    \vcmu_{\vcx_0},
    \mxS_{\vcx_0},
    \mxS_\vcw,\mxS_\vcv),
    \end{equation}
which belongs to the vector space $\Vbb$ defined as
\begin{equation}
    \Vbb:=
    \Rbb^{\Nx\times\Nx} 
    \times
    \Rbb^{\Nx\times\Nu} 
    \times
    \Rbb^{\Ny\times\Nx} 
    \times
    \cdots
    \times 
    \Rbb^{\Ny\times\Nx} 
    \times
    \Rbb^{\Ny\times\Nu} 
    \times
    \Rbb^{\Nx} 
    \times
    \Sbb^{\Nx\times\Nx} 
    \times
    \Sbb^{\Nx\times\Nx} 
    \times
    \Sbb^{\Ny\times\Ny}. 
\end{equation}
Also, let $\Theta \subseteq \Vbb$ be the feasible set for $\vctheta$, specified based on the structure of the its entries and the corresponding constraints, i.e.,
\begin{equation}
    \Theta :=
    \Rbb^{\Nx\times\Nx} 
    \times
    \Rbb^{\Nx\times\Nu} 
    \times
    \Rbb^{\Ny\times\Nx} 
    \times
    \cdots
    \times 
    \Rbb^{\Ny\times\Nx} 
    \times
    \Rbb^{\Ny\times\Nu} 
    \times
    \Rbb^{\Nx} 
    \times
    \Sbb_{++}^{\Nx\times\Nx} 
    \times
    \Sbb_{++}^{\Nx\times\Nx} 
    \times
    \Sbb_{++}^{\Ny\times\Ny}. 
\end{equation}
%-----------------
\iffalse
To derive the likelihood function and formulate the maximum likelihood scheme, we first define vector of parameters, denoted by  $\vctheta$, as
\begin{equation}
    \begin{split}
     {\vctheta} :  =\,\,[\vec({\mxA})^\tr , \vec({\mxB})^\tr,\vec({\mxC}_0)^\tr,\vec({\mxC}_1)^\tr,...,\vec({\mxC}_{\Nu})^\tr, \vec({\mxD})^\tr, \vec({\vcmu}_{\vcx_0})^\tr,\vec({\mxS}_{\vcx_0})^\tr,\vec({\mxS}_\vcw)^\tr,\vec({\mxS}_\vcv)^\tr],
    \end{split}
\end{equation}
and, $\Theta$ as the feasible set for $\vctheta$, specified based on the structure of the vector of parameters and the corresponding constraints.
% The feasible set $\Theta$ is correspondingly defined based on the structure and constraints of $\vctheta$.
%------
%Let $\mathbf{u}$ and $\mathbf{y}$ be the vector of input and output measurements, i.e., $\mathbf{y} := [\vcy_0^\tr,\vcy_1^\tr,..,\vcy_{{\nsD-1}}^\tr]^\tr\in\Rbb^{\Ny\nsD}$ and $\mathbf{u} := [\vcu_0^\tr,\vcu_1^\tr,..,\vcu_{\nsD-1}^\tr]^\tr\in\Rbb^{\Nu\nsD}$. The ML estimation approach identifies $\vctheta$ by obtaining the vector of parameters $\vcthetaML$, which maximize the probability of observing the given measurements $\mathbf{u}$ and $\mathbf{y}$. 
\fi
%---------
Let $\mathbf{u}$ and $\mathbf{y}$ denote the vectors of input and output measurements, respectively, defined as $\mathbf{y} := [\vcy_0^\tr,\vcy_1^\tr,..,\vcy_{{\nsD-1}}^\tr]^\tr\in\Rbb^{\Ny\nsD}$ and $\mathbf{u} := [\vcu_0^\tr,\vcu_1^\tr,..,\vcu_{\nsD-1}^\tr]^\tr\in\Rbb^{\Nu\nsD}$. The ML estimation approach identifies $\vctheta$ by determining the 
% vector of 
% tuple
parameters $\vcthetaML$ that maximizes the probability of observing the given measurements $\mathbf{u}$ and $\mathbf{y}$. 
More precisely, we have 
\begin{equation} \label{eqn:ML_generic}
    \vcthetaML  := \argmaxOp_{\vctheta \in \Theta}\, p(\mathbf{y}|\vctheta, \mathbf{u}).
\end{equation} 
% To solve the optimization problem above, we need to simplify the objective function in \eqref{eqn:ML_generic},  
% and obtain a closed form characterized directly in terms of optimization variable $\vctheta$.
To solve the optimization problem \eqref{eqn:ML_generic}, we need to simplify the corresponding objective function
and express it in a tractable closed form characterized directly in terms of the optimization variable $\vctheta$.
%To solve the optimization problem in \eqref{eqn:ML_generic}, the corresponding objective function must be simplified and expressed in closed form directly in terms of the optimization variable $\vctheta.$


\subsection{Maximum Likelihood Optimization Problem}\label{sec:ML formulation}
Let $\mathbf{x}$ denote the vector of state trajectory, defined as $\mathbf{x} := [\vcx_0^\tr,\vcx_1^\tr,..,\vcx_{{\nsD}-1}^\tr]^\tr$. 
According to \eqref{eqn:dynamics1}--\eqref{eqn:v_dist_Gaussion}, one can easily see that $\mathbf{x}\big|\vctheta,\mathbf{u}$ and $\mathbf{y}\big|\mathbf{x},\vctheta,\mathbf{u}$ follow Gaussian distributions, i.e., we have
% \begin{align}
%   &\mathbf{x}\big|\vctheta,\mathbf{u} \
%   \ \sim \ 
%   \mathcal{N}\left(\mu^{(\mathbf{x})}(\vctheta), \mxS^{(\mathbf{x})}(\vctheta)\right)
%   \\
%   &\mathbf{y}\big|\mathbf{x},\vctheta,\mathbf{u} 
%   \ \sim \
%   \mathcal{N}\left(\mu^{(\mathbf{y})}(\vctheta,\mathbf{x}), \mxS^{(\mathbf{y})}(\vctheta,\mathbf{x})\right).  
% \end{align}
\begin{equation}
  \mathbf{x}\big|\vctheta,\mathbf{u} 
  \ \sim \ 
  \mathcal{N}
  \big(
  \mu^{(\mathbf{x})}(\vctheta), \mxS^{(\mathbf{x})}(\vctheta)
  \big),
\end{equation}
and 
\begin{equation}
  \mathbf{y}\big|\mathbf{x},\vctheta,\mathbf{u} 
  \ \sim \
  \mathcal{N}
  \big(
  \mu^{(\mathbf{y})}(\vctheta,\mathbf{x}), \mxS^{(\mathbf{y})}(\vctheta,\mathbf{x})
  \big),  
\end{equation}
where 
$\mu^{(\mathbf{x})}(\vctheta)$, 
$\mu^{(\mathbf{y})}(\vctheta,\mathbf{x})$, 
$\mxS^{(\mathbf{x})}(\vctheta)$, and
$\mxS^{(\mathbf{y})}(\vctheta,\mathbf{x})$
are the corresponding mean vectors and covariance matrices.
% $\mathbf{x} \sim \mathcal{N}\left(\mu^{(\mathbf{x})}(\vctheta), \mxS^{(\mathbf{x})}(\vctheta)\right)$ and $\mathbf{y} \sim \mathcal{N}\left(\mu^{(\mathbf{y})}(\vctheta,\mathbf{x}), \mxS^{(\mathbf{y})}(\vctheta,\mathbf{x})\right)$. 
% Here, for the ease of notation, we have dropped the dependencies to the vector of input measurements $\mathbf{u}$ and assumed it is known implicitly.
For ease of notation, we have omitted the dependence on the input measurement vector $\mathbf{u}$, assuming that it is implicitly known.
Using the law of total probability, we know that 
\begin{equation}\label{eq:law_of_total_probability}
\begin{split}
p(\mathbf{y}|\vctheta, \mathbf{u}) 
= 
%\int_{\Rbb^{\Nx\nsD}\ \Xbb \mathbf{X}\Xscr}
%\int_{\Rbb^{\Nx\nsD}\ \Xbb \mathbf{X}\Xscr}
% \int_{\Xscr}
\int_{\Xbb}
\,
p(\mathbf{y}|\mathbf{x}, \vctheta, \mathbf{u})p(\mathbf{x}|\vctheta, \mathbf{u}) \drm \mathbf{x}, 
\end{split}
\end{equation}
%where the integration domain, denoted by $\Xscr$, is $\Rbb^{\Nx\nsD}$.
% where $\Xscr$, the integration domain, is $\Rbb^{\Nx\nsD}$.
where 
% $\Xscr$
$\Xbb$ 
is the integration domain, defined as $\Rbb^{\Nx\nsD}$.
Thus, the maximization problem \eqref{eqn:ML_generic} can be written as 
\begin{equation}\label{eq:likelihood0}
\begin{split}
\vcthetaML 
= 
\argmaxOp_{\vctheta\in\Theta} 
\int_{\Xbb}\, p(\mathbf{y}|\mathbf{x}, \vctheta, \mathbf{u})p(\mathbf{x}|\vctheta, \mathbf{u}) \drm \mathbf{x}. \\
\end{split}
\end{equation}
%-----
% To obtain a tractable optimization problem, we first need to obtain 
% %$\mu^{(\mathbf{x})}$, $\mxS^{(\mathbf{x})}$, $\mu^{(\mathbf{y})}$ and $\mxS^{(\mathbf{y})}$. 
% $\mu^{(\mathbf{x})}(\vctheta)$, 
% $\mu^{(\mathbf{y})}(\vctheta,\mathbf{x})$, 
% $\mxS^{(\mathbf{x})}(\vctheta)$, and
% $\mxS^{(\mathbf{y})}(\vctheta,\mathbf{x})$.
% Furthermore, we need to obtain the integration in \eqref{eq:likelihood0}. Finally, we simplify the resulting likelihood function.
% To derive a tractable optimization problem from \eqref{eq:likelihood0}, we need to obtain the integration \eqref{eq:law_of_total_probability}. To this end, the value of $\mu^{(\mathbf{x})}(\vctheta)$, $\mu^{(\mathbf{y})}(\vctheta,\mathbf{x})$, $\mxS^{(\mathbf{x})}(\vctheta)$, and $\mxS^{(\mathbf{y})}(\vctheta,\mathbf{x})$ is required.
To formulate a tractable optimization problem for the ML estimation of parameters based on \eqref{eq:likelihood0}, we need to evaluate the integral in \eqref{eq:law_of_total_probability}, which requires
determining the values of $\mu^{(\mathbf{x})}(\vctheta)$, $\mu^{(\mathbf{y})}(\vctheta,\mathbf{x})$, $\mxS^{(\mathbf{x})}(\vctheta)$, and $\mxS^{(\mathbf{y})}(\vctheta,\mathbf{x})$.
%-----
%
% We first show that $p(\mathbf{y}|\mathbf{x}, \vctheta, \mathbf{u})$ and $p(\mathbf{x}|\vctheta, \mathbf{u})$ follow Gaussian distributions and calculate the corresponding integrals. Then, we expand \eqref{eq:likelihood0} based on these Gaussian distributions. Finally, we simplify the resulting likelihood function.
%
% Now, consider $p(\mathbf{x}|\vctheta, \mathbf{u})$, for each time step $t = 1,2,\cdots, \nsD-1$, according to the dynamics \eqref{eqn:dynamics1}, we can write
% \begin{equation}
%     \vcx_t = \mxA^t\vcx_0 + \sum_{k=0}^{t-1}\mxA^{t-1-k}\mxB \vcu_k + \sum_{k=0}^{t-1} \mxA^{t-1-k}\vcw_k,
% \end{equation}
% where the middle term is determined given $\vctheta$ and $\mathbf{u}$. 
% Now, consider $p(\mathbf{x}|\vctheta, \mathbf{u})$, for each time step $t = 1,2,\cdots, \nD-1$,
According to \eqref{eqn:dynamics1}, we have
\begin{equation}\label{eqn:x_t}
    \vcx_t = \mxA^t\vcx_0 + \sum_{k=0}^{t-1}\mxA^{t-1-k}\mxB \vcu_k + \sum_{k=0}^{t-1} \mxA^{t-1-k}\vcw_k,
\end{equation}
for each $t = 1,2,\cdots, {\nsmD-1}$.
% One can easily see that the middle term in \eqref{eqn:x_t} is determined assuming $\vctheta$ and $\mathbf{u}$ are given.
One can easily see the middle term in \eqref{eqn:x_t} is determined when $\vctheta$ and $\mathbf{u}$ are known.
%---
Additionally, since we assume that $\vcx_0$ and $\vcw_k$ are independent Gaussian random vectors, the first and last terms also follow independent Gaussian distributions given $\vctheta$ and $\mathbf{u}$. Consequently, $\vcx_t$ also follows a Gaussian distribution. Thus, regarding $p(\mathbf{x}|\vctheta, \mathbf{u})$, %$\mathbf{x}\big|\vctheta,\mathbf{u}$, 
we can obtain the joint Gaussian distribution $\mathcal{N}\left(\mu^{(\mathbf{x})}(\vctheta), \mxS^{(\mathbf{x})}(\vctheta)\right)$, where
% \begin{equation}
% \begin{split}
%     \mu^{(\mathbf{x})} &= \begin{bmatrix}
%         \mu_{\vcx_0} \\
%         \mxA\mu_{\vcx_0} + \mxB\vcu_0\\
%         \vdots\\
%         \mxA^t\mu_{\vcx_0} + \sum_{k=0}^{t-1}\mxA^{t-1-k}\mxB \vcu_k\\
%         \vdots\\
%         \mxA^{\nsD-1}\mu_{\vcx_0} + \sum_{k=0}^{\nsD-2}\mxA^{\nsD-2-k}\mxB \vcu_k\\
%     \end{bmatrix}\\[10pt]
%     \mxS^{(\mathbf{x})} &= \begin{bmatrix}
%         \mxS^{(\mathbf{x})}_{0,0} & \mxS^{(\mathbf{x})}_{0,1} & \cdots & \mxS^{(\mathbf{x})}_{0,\nsD-1}\\[10pt]
%         \mxS^{(\mathbf{x})}_{1,0} & \mxS^{(\mathbf{x})}_{1,1} & \cdots & \mxS^{(\mathbf{x})}_{1,\nsD-1}\\
%         \vdots & \vdots & \ddots & \vdots\\
%         \mxS^{(\mathbf{x})}_{\nsD-1,0} & \mxS^{(\mathbf{x})}_{\nsD-1,1} & \cdots & \mxS^{(\mathbf{x})}_{\nsD-1,\nsD-1}\\
%     \end{bmatrix}\\[10pt]
%     \mxS_{t,s}^{(\mathbf{x})} &= \mathbb{E}[(\vcx_t - \mu_t^{(\mathbf{x})})(\vcx_s - \mu_s^{(\mathbf{x})})^\tr]\\
%     &= \mxA^t\mxS_{0,0}^{(\mathbf{x})}(\mxA^s)^\tr + \sum_{k=0}^{\min(s,t) - 1}\mxA^{t-1-k}\mxS_\vcw(\mxA^{s-1-k})^\tr, \quad \forall t,s \in [1, \nD-1].
% \end{split}
% \end{equation}
\begin{equation}\label{eq:mean x}
    \begin{split}
    \mu^{(\mathbf{x})}(\vctheta) := 
    \Exp\big[\mathbf{x}\big| \vctheta, \mathbf{u}\big] = 
    \begin{bmatrix}
        \mu_{\vcx_0} \\
        \mxA\mu_{\vcx_0} + \mxB\vcu_0\\
        \vdots\\
        \mxA^t\mu_{\vcx_0} + \sum_{k=0}^{t-1}\mxA^{t-1-k}\mxB \vcu_k\\
        \vdots\\
        \mxA^{\nsD-1}\mu_{\vcx_0} + \sum_{k=0}^{\nsD-2}\mxA^{\nsD-2-k}\mxB \vcu_k\\
    \end{bmatrix},
    \end{split}
\end{equation}
and
\begin{equation}\label{eq:covariance x}
    \begin{split}
    \mxS^{(\mathbf{x})}{(\vctheta)} := 
    \Exp
    \big[
    \big(\mathbf{x}- \mu^{(\mathbf{x})}(\vctheta)\big)
    \big(\mathbf{x}- \mu^{(\mathbf{x})}(\vctheta)\big)^\tr    
    \big| \vctheta, \mathbf{u}
    \big] = 
    \begin{bmatrix}
        \mxS^{(\mathbf{x})}_{0,0}(\vctheta) & \, \mxS^{(\mathbf{x})}_{0,1}(\vctheta) & \cdots & \mxS^{(\mathbf{x})}_{0,\nsD-1}(\vctheta)\\[10pt]
        \mxS^{(\mathbf{x})}_{1,0}(\vctheta) & \, \mxS^{(\mathbf{x})}_{1,1}(\vctheta) & \cdots & \mxS^{(\mathbf{x})}_{1,\nsD-1}(\vctheta)\\
        \vdots & \, \vdots & \ddots & \vdots\\
        \mxS^{(\mathbf{x})}_{\nsD-1,0}(\vctheta) & \, \mxS^{(\mathbf{x})}_{\nsD-1,1}(\vctheta) & \cdots & \mxS^{(\mathbf{x})}_{\nsD-1,\nsD-1}(\vctheta)\\
    \end{bmatrix},
    \end{split}
\end{equation}
given that $\mxS^{(\mathbf{x})}_{t,s}(\vctheta)$ is defined as
\begin{equation}
    \begin{split}
        \mxS_{t,s}^{(\mathbf{x})}(\vctheta) 
        :=&\ 
        \mathbb{E}\big[(\vcx_t - \mu_t^{(\mathbf{x})})(\vcx_s - \mu_s^{(\mathbf{x})})^\tr\big| \vctheta, \mathbf{u}
        \big]
        %\\
        =
        %&\ 
        \mxA^t\mxS_{\vcx_0}(\mxA^s)^\tr + \sum_{k=0}^{\min(s,t) - 1}\mxA^{t-1-k}\mxS_\vcw(\mxA^{s-1-k})^\tr,
    \end{split}
\end{equation}
for any $t,s = 1,2,\cdots, \nD-1$. 
One can easily see that for $p(\mathbf{y}|\mathbf{x},\vctheta, \mathbf{u})$, we have similar arguments. For $t = 1,2,\cdots, \nD-1$, let $\Xi_t(\vctheta)$ be define as
\begin{equation}\label{eqn:Xi_t}
    \Xi_t(\vctheta) := {\mxC}_0 + \sum_{i=1}^{\Nu} {\mxC}_i \vcu_{t,i},
\end{equation}
where the dependence of $\Xi_t(\vctheta)$ on the input measurement vector $\mathbf{u}$ is omitted for ease of notation, and assumed to be implicitly known. 
Following the problem settings introduced in Section~\ref{sec:pf}, we know that $\vcv_0, \vcv_1, \cdots, \vcv_{\nsD-1}$ are zero-mean independent Gaussian random vectors with individual covariance $\mxS_\vcv$.
Therefore, according to \eqref{eqn:dynamics2}, one can see that
\begin{equation}
\begin{split}
    \mu_t^{(\mathbf{y})}(\vctheta,\mathbf{x}) 
    :=
    \Exp
    \big[
    \vcy_t 
    \big| 
    \mathbf{x}, \vctheta, \mathbf{u}
    \big]
     &=\ 
     \Xi_t(\vctheta) \vcx_t + \mxD \vcu_t,
\end{split}
\end{equation}
and
\begin{equation}
\begin{split}
    \mxS_{t,s}^{(\mathbf{y})}({\vctheta}) 
    :=
    \Exp
    \big[
    (\vcy_t - \mu_t^{(\mathbf{y})})(\vcy_s - \mu_s^{(\mathbf{y})})^\tr 
    \big| 
    \mathbf{x}, \vctheta, \mathbf{u}
    \big] 
    = 
    \begin{cases}
        \mxS_\vcv, & \text{if }\  t = s,\\
        \mathbf{0}, & \text{if }\  t \ne s,\\
    \end{cases}
\end{split}
\end{equation}
for any $t,s = 0,1,\cdots, \nsmD-1$.
% Note that $\mxS_{t,s}^{(\mathbf{y})}({\vctheta})$ does not depend on the input measurement vector $\mathbf{u}$, and thus, we have omitted argument $\mathbf{u}$ for brevity. 
% Note that $\mxS_{t,s}^{(\mathbf{y})}({\vctheta})$ is independent of the input measurement vector $\mathbf{u}$, and thus the argument $\mathbf{u}$ is omitted for ease of notation and brevity.
Note that $\mxS_{t,s}^{(\mathbf{y})}({\vctheta})$  does not depend on the input measurement vector $\mathbf{u}$, and thus, the argument $\mathbf{u}$ is omitted for the ease of notation and brevity.
% Since $\mxS_{t,s}^{(\mathbf{y})}({\vctheta})$  does not depend on the input measurement vector $\mathbf{u}$, the argument $\mathbf{u}$ is omitted for the ease of notation and brevity.
%-----
% Similarly, consider $p(\mathbf{y}|\mathbf{x},\vctheta, \mathbf{u})$, given $\mathbf{x}$, $\vctheta$ and $\mathbf{u}$, in the dynamics $\vcy_t = \Xi_t(\vctheta) \vcx_t + \mxD \vcu_t + \vcv_t$, only $\vcv_t$ is not determined, where $\Xi_t(\vctheta) = {\mxC}_0 + \sum_{i=1}^{\Nu} {\mxC}_i \vcu_{t,i}$. Since $\vcv_0, \vcv_1, \cdots, \vcv_{\nsD-1}$ are independent of each other, $\mathbf{y}$ also follows a joint Gaussian distribution $\mathcal{N}(\mu^{(\mathbf{y})}(\vctheta,\mathbf{x}), \mxS^{(\mathbf{y})}(\vctheta,\mathbf{x}))$. The mean of $\vc{y_t}$ and the covariance of $\vcy_t$ and $\vcy_s$ can be derived as
% \begin{equation}
% \begin{split}
%     \vcy_t &= \Xi_t(\vctheta) \vcx_t + \mxD \vcu_t \\ 
%     \mxS_{t,s}^{(\mathbf{y})}({\vctheta}) &= 
%     \mathbb{E}[(\vcy_t - \mu_t^{(\mathbf{y})})(\vcy_s - \mu_s^{(\mathbf{y})})^\tr] = \mxS_\vcv, \nonumber
% \end{split}
% \end{equation}
% for any $t,s = 0,1,\cdots, \nsmD-1$. 
% Therefore, $\mu^{(\mathbf{y})}({\vctheta},\mathbf{x})$ and $\mxS^{(\mathbf{y})}({\vctheta})$ can be written as 
% % \begin{subequations}
% % \begin{align} 
% %         \mu^{(\mathbf{y})} &= \begin{bmatrix}
% %         \Xi_0 \vcx_0 + \mxD \vcu_0 \\
% %         \Xi_1 \vcx_1 + \mxD \vcu_1 \\
% %         \vdots \\
% %         \Xi_{\nsD-1} \vcx_{\nsD-1} + \mxD \vcu_{\nsD-1}\\
% %         \end{bmatrix} \label{eq: mean Y}\\
% %         \mxS^{(\mathbf{y})} &= \mathbf{I}_{\nsD} \otimes \mxS_\vcv .\label{eq: covariance Y}
% % \end{align}
% % \end{subequations}
Hence, we know that $\mathbf{y}$ has  Gaussian distribution as $\mathcal{N}(\mu^{(\mathbf{y})}(\vctheta,\mathbf{x}), \mxS^{(\mathbf{y})}(\vctheta,\mathbf{x}))$,
where 
$\mu^{(\mathbf{y})}({\vctheta},\mathbf{x})$ and $\mxS^{(\mathbf{y})}({\vctheta})$ can be written as
\begin{equation}\label{eq:mean y}
        \mu^{(\mathbf{y})}({\vctheta},\mathbf{x}) 
        := 
        \big[
        \mu_t^{(\mathbf{y})}(\vctheta,\mathbf{x})
        \big]_{t\,=\,0}^{\nsD-1}
        =
        \begin{bmatrix}
        \Xi_0(\vctheta) \vcx_0 + \mxD \vcu_0 \\
        \Xi_1(\vctheta) \vcx_1 + \mxD \vcu_1 \\
        \vdots \\
        \Xi_{\nsD-1}(\vctheta) \vcx_{\nsD-1} + \mxD \vcu_{\nsD-1}\\
        \end{bmatrix},
\end{equation}
and
% \begin{equation}\label{eq:covariance y}
%     \mxS^{(\mathbf{y})}({\vctheta})= \begin{bmatrix}
%         \mxS_\vcv & \mathbf{0} & \cdots & \mathbf{0}\\
%         \mathbf{0} & \mxS_\vcv & \cdots & \mathbf{0}\\
%         \vdots & \vdots & \ddots & \vdots\\
%         \mathbf{0} & \mathbf{0} & \cdots & \mxS_\vcv\\
%     \end{bmatrix}.
% \end{equation}
% \begin{equation}\label{eq:covariance y}
%     \mxS^{(\mathbf{y})}({\vctheta})= \mathbf{I}_{\nsD}\otimes \mxS_\vcv.
% \end{equation}
\begin{equation}\label{eq:covariance y}
    \mxS^{(\mathbf{y})}({\vctheta})
    := 
    \big[
    \mxS_{t,s}^{(\mathbf{y})}({\vctheta}) 
    \big]_{s,t\,=\,0}^{\nsD-1}
    =
    \begin{bmatrix}
        \mxS_\vcv & \mathbf{0} & \cdots & \mathbf{0}\\
        \mathbf{0} & \mxS_\vcv & \cdots & \mathbf{0}\\
        \vdots & \vdots & \ddots & \vdots\\
        \mathbf{0} & \mathbf{0} & \cdots & \mxS_\vcv\\
    \end{bmatrix}
    = 
    \mathbf{I}_{\nsD}\otimes \mxS_\vcv.
\end{equation}
For further simplification, we additionally 
define $\Xi(\vctheta)$ and $\mathbf{D}(\vctheta)$ as  
\begin{equation}
    \Xi(\vctheta) := \begin{bmatrix}
        \Xi_0(\vctheta) & \mathbf{0} & \cdots & \mathbf{0}\\
        \mathbf{0} & \Xi_1(\vctheta) & \cdots & \mathbf{0}\\
        \vdots & \vdots & \ddots & \vdots\\
        \mathbf{0} & \mathbf{0} & \cdots & \Xi_{\nsD-1}(\vctheta)\\
    \end{bmatrix},
\end{equation}
and
\begin{equation}
    \mathbf{D}(\vctheta) := 
    \begin{bmatrix}
        \mxD & \mathbf{0} & \cdots & \mathbf{0}\\
        \mathbf{0} & \mxD & \cdots & \mathbf{0}\\
        \vdots & \vdots & \ddots & \vdots\\
        \mathbf{0} & \mathbf{0} & \cdots & \mxD\\
    \end{bmatrix}
    = \mathbf{I}_\nsD \otimes \mxD,
\end{equation}
respectively.
Accordingly, due to \eqref{eq:mean y}, for $\mu^{(\mathbf{y})}({\vctheta},\mathbf{x})$, 
we have 
\begin{equation}
\mu^{(\mathbf{y})}({\vctheta},\mathbf{x}) = \Xi(\vctheta)\mathbf{x} + \mathbf{D}(\vctheta)\mathbf{u}.
\end{equation}

%--------
% Using  the law of total probability and the Gaussian distributions derived above, %\eqref{eq:mean x}-\eqref{eq:covariance y}, 
% we can expand the likelihood function as
%
% Applying  the law of total probability and using the Gaussian distributions derived above, we can express the likelihood function as
Applying  the law of total probability and using the Gaussian distributions derived above, we can express the likelihood function as
%--------
% \begin{equation}\label{eq: likelihoog int}
% \begin{split}
%         p(\mathbf{y}|\vctheta, \mathbf{u})
%         & =
%         \int_{\Xbb}
%         \,
%         p(\mathbf{y}|\mathbf{x}, \vctheta, \mathbf{u})p(\mathbf{x}|\vctheta, \mathbf{u}) \drm \mathbf{x}, 
%         \\&= 
%         \frac{(2\pi)^{-\frac{1}{2}(\Nx+\Ny)\nsD}}{|\mxS^{(\vcx)}(\vctheta)|^{\frac{1}{2}} |\mxS^{(\mathbf{y})}(\vctheta)|^{\frac{1}{2}}} \int_{\Xbb} \mathrm{exp} \Bigg( -\frac{1}{2} \Big[ \left( \mathbf{y}-\mu^{(\mathbf{y})}(\vctheta,\mathbf{x}) \right)^\tr\mxS^{(\mathbf{y})^{-1}}(\vctheta) \left( \mathbf{y}-\mu^{(\mathbf{y})}(\vctheta,\mathbf{x}) \right) \\
%         &
%         \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad 
%         + \left( \mathbf{x}-\mu^{(\mathbf{x})}(\vctheta) \right)^\tr\mxS^{(\mathbf{x})^{-1}}(\vctheta) \left( \mathbf{x}-\mu^{(\mathbf{x})}(\vctheta) \right) \Big] \Bigg) \drm \mathbf{x}.
% \end{split}
% \end{equation} 
\begin{equation}\label{eq: likelihoog int}
\begin{split}
        p(\mathbf{y}|\vctheta, \mathbf{u})
        & =
        \int_{\Xbb}
        \,
        p(\mathbf{y}|\mathbf{x}, \vctheta, \mathbf{u})p(\mathbf{x}|\vctheta, \mathbf{u}) \drm \mathbf{x}, 
        \\&= 
        \frac{(2\pi)^{-\frac{1}{2}(\Nx+\Ny)\nsD}}{|\Sbfxtheta|^{\frac{1}{2}} |\Sbfytheta|^{\frac{1}{2}}} \int_{\Xbb} 
        \exp 
        \bigg( -\frac{1}{2} \Big[ 
        \big(\mathbf{y}-\mubfythetax \big)^\tr
        \mxS^{{(\mathbf{y})}}\!(\vctheta)^{-1}
        \big( \mathbf{y}-\mubfythetax \big) \\
        &
        \qquad\qquad \qquad\qquad \qquad\qquad \qquad\qquad \qquad 
        + 
        \big(\mathbf{x}-\mubfxtheta \big)^\tr
        \mxS^{{(\mathbf{x})}}\!(\vctheta)^{-1}
        \big( \mathbf{x}-\mubfxtheta \big) 
        \Big] \bigg) 
        \drm \mathbf{x}.
\end{split}
\end{equation}
%-----------
% One can see that the current form of the likelihood function described above is complex to obtain $\vcthetaML$ using \eqref{eq:mean x}. Therefore, we need to simplify it to derive a tractable optimization problem, which is addressed by the following proposition.
%
% It is evident that the current form of the likelihood function described above is too complex to directly obtain $\vcthetaML$ using \eqref{eq:mean x}. Thus, we simplify it to formulate a tractable optimization problem, as outlined in the following proposition.
%
One can see that the current form of the likelihood function described above is excessively complex to directly obtain $\vcthetaML$ using \eqref{eq:mean x}. Thus, we simplify it to formulate a tractable optimization problem, as outlined in the following proposition.
%------
% Accordingly, 
% let $\Xi(\vctheta)$ and $\mathbf{D}(\vctheta)$ be defined as  
% \begin{equation}
%     \Xi(\vctheta) := \begin{bmatrix}
%         \Xi_0(\vctheta) & \mathbf{0} & \cdots & \mathbf{0}\\
%         \mathbf{0} & \Xi_1(\vctheta) & \cdots & \mathbf{0}\\
%         \vdots & \vdots & \ddots & \vdots\\
%         \mathbf{0} & \mathbf{0} & \cdots & \Xi_{\nsD-1}(\vctheta)\\
%     \end{bmatrix},
% \end{equation}
% and
% \begin{equation}
%     \mathbf{D}(\vctheta) := 
%     \begin{bmatrix}
%         \mxD & \mathbf{0} & \cdots & \mathbf{0}\\
%         \mathbf{0} & \mxD & \cdots & \mathbf{0}\\
%         \vdots & \vdots & \ddots & \vdots\\
%         \mathbf{0} & \mathbf{0} & \cdots & \mxD\\
%     \end{bmatrix}
%     = \mathbf{I}_\nsD \otimes \mxD,
% \end{equation}
% respectively.
% One can easily see that for $\mu^{(\mathbf{y})}({\vctheta},\mathbf{x})$, the mean of output trajectory, we have 
% \begin{equation}
% \mu^{(\mathbf{y})}({\vctheta},\mathbf{x}) = \Xi(\vctheta)\mathbf{x} + \mathbf{D}(\vctheta)\mathbf{u}.
% \end{equation}


\begin{proposition}\label{Pro:loglikelihood}
Define function 
% $\, J:\Theta\to\Rbb \,$ 
$\, J:\Vbb\to\extendedR \,$ 
as
% \begin{equation}\label{eq:cost function ML}
%     \!
%     \begin{split}
%     \!
%     J({\vctheta}) :=&  \logdet(\Xi({\vctheta})^\tr \mxS^{(\mathbf{y})^{-1}}({\vctheta}) \Xi({\vctheta}) + \mxS^{{(\mathbf{x})}^{-1}}({\vctheta})) 
%     %\\ &
%     +\logdet(\mxS^{(\mathbf{x})}({\vctheta})) +\logdet(\mxS^{(\mathbf{y})}({\vctheta}))
%      \\ &\quad
%     %\\ &
%     +\Big(\big(\mathbf{u}^\tr 
%     \mathbf{D}^\tr(\vctheta) 
%     \!-\! \mathbf{y}^\tr \!+\! \mu^{(\mathbf{x})}({\vctheta})^\tr\Xi({\vctheta})^\tr\big)
%     % \\&\qquad\quad
%     \big(\Xi({\vctheta})\mxS^{(\mathbf{x})}({\vctheta}) \Xi({\vctheta})^\tr 
%     %\\ &\qquad 
%     \!+\! \mxS^{(\mathbf{y})}({\vctheta})\big)^{-1}
%     \big(\mathbf{D}(\vctheta) \mathbf{u} \!-\! \mathbf{y} \!+\! \Xi({\vctheta})\mu^{(\mathbf{x})}({\vctheta})\big)\Big). 
%     \end{split}
% \end{equation}
\begin{equation}\label{eq:cost function ML}
    \begin{split}
    J({\vctheta}) :=&\,  
    \logdet\big(
        \Xitheta^\tr 
        \Sbfytheta^{-1} 
        \Xitheta 
        + 
        \Sbfxtheta^{-1}
    \big) 
    \, + \, 
    \logdet\big(\Sbfxtheta \big) 
    \, + \, 
    \logdet\big(\Sbfytheta \big)
     \\ &\quad
    %\\ &
    +\Big(\big(\mathbf{u}^\tr 
    \mathbf{D}(\vctheta)^\tr 
    - \mathbf{y}^\tr + \mu^{(\mathbf{x})}({\vctheta})^\tr\Xi({\vctheta})^\tr\big)
    % \\&\qquad\quad
    \big(\Xi({\vctheta})\Sbfxtheta \Xi({\vctheta})^\tr 
    %\\ &\qquad 
    + \Sbfytheta \big)^{-1}
    \big(\mathbf{D}(\vctheta) \mathbf{u} - \mathbf{y} + \Xi({\vctheta})\mubfxtheta \big)\Big). 
    \end{split}
\end{equation}
Then, the maximum likelihood problem %\eqref{eq:likelihood0} 
\eqref{eqn:ML_generic}
is equivalent to minimizing $J$ over $\Theta$, i.e., we have
        \begin{equation}\label{eqn:min_J}
            \begin{split}
                \vcthetaML = \argminOp_{{\vctheta}\in\Theta} \ J({\vctheta}) 
            \end{split}
        \end{equation}
\end{proposition}
\begin{proof}
        The proof is provided in Appendix \ref{app:proof 1}.
\end{proof}

% \begin{remark}
%     The above notations $\mxS_\vcq$, $\mxF$, $\mu^{(\mathbf{x})}$ $\mxS^{(\mathbf{x}}$, $\mu^{(\mathbf{y})}$ $\mxS^{(\mathbf{y})}$, $z$, $\Xi$ and $\mathbf{D}$ are all functions of the parameter $\vctheta$. For simplicity, we omit $\vctheta$ in the notation for this paper.
% \end{remark}


\subsection{Maximum Likelihood Estimation: Toward First-Order Approaches}\label{sec:Maximum Likelihood Estimation: Toward first-order approaches}
% In the previous subsection, we demonstrated how to formulate the log-likelihood function. To estimate the optimal parameters, we need to minimize the cost function in \eqref{eq:cost function ML} over $\Theta$, i.e., solving the optimization problem \eqref{eqn:min_J}. 
% The above discussion outlines the formulation of the log-likelihood function. Therefore, to estimate the vector of parameters $\vctheta$ using ML approach, we need to minimize the cost function $J$, defined in \eqref{eq:cost function ML}, i.e., solve the optimization problem presented in \eqref{eqn:min_J}.
%-------
% However, this function is nonconvex with respect to the parameters $\vctheta$, making it challenging to directly find the global optimum. To address this, in this paper, we use first-order approaches to find the suboptimal parameters. Specifically, we first obtain the derivatives of the objective function $J$ with respect to each parameter. Using these derivatives, we apply the gradient descent algorithm until convergence to obtain the suboptimal parameter estimates. 
%-------
% In the above discussion, we have outlined the formulation of the log-likelihood estimation method, and demonstrated that to estimate the vector of parameters $\vctheta$ using ML approach, we need to solve the optimization problem presented in \eqref{eqn:min_J}. One can see that $J$ is a nonlinear function with respect to the parameters $\vctheta$. Thus, to find the the solution of \eqref{eqn:min_J}, we can employ first-order optimization methods, such as the \emph{Steepest Gradient Descent}, \emph{BFGS}, or \emph{limited-memory BFGS}. To this end, we need to obtain $\nabla J(\vctheta)$ by deriving the derivatives of the objective function $J$ with respect to each parameter.
In the preceding discussion, we outlined the formulation of the log-likelihood estimation method and demonstrated that, to estimate the parameter vector $\vctheta$ using the ML approach, we need to solve the optimization problem presented in \eqref{eqn:min_J}. One can easily see that $J$ is a nonlinear function of the parameters $\vctheta$. 
% Accordingly, to solve \eqref{eqn:min_J}, we can employ first-order optimization methods \cite{nocedal1999numerical}, such as \emph{Gradient Descent}, \emph{Broyden-Fletcher-Goldfarb-Shanno} (BFGS), or \emph{limited-memory BFGS}. 
Accordingly, to solve \eqref{eqn:min_J}, we can employ first-order optimization methods \cite{nocedal1999numerical}, such as \emph{Gradient Descent}.  
To implement these methods, it is necessary to obtain the gradient vector $\nabla_\vctheta J$ by deriving the derivatives of the objective function $J$ with respect to each parameter.

Note that $J$ depends on $\mu^{(\mathbf{x})}, \mxS^{(\mathbf{x})}, \mxS^{(\mathbf{y})}, \Xi, \mathbf{D}$, each of which is a function of $\vctheta$. Consequently, we can apply matrix calculus techniques \cite{petersen2008matrix}, particularly \emph{matrix chain rule} outlined in Lemma~\ref{lem:chain_rule} in Appendix~\ref{app:auxiliary lemma}, to derive the gradient vector $\nabla_\vctheta \, J$. More precisely, we first obtain the derivatives of $J$ with respect to the mentioned terms, as given by
\begin{align}
        %%
        \pdv{J(\vctheta)}{\mu^{(\mathbf{x})}} 
        =& \, 
        2 \Xitheta^\tr
        \big(\Sbfytheta + \Xitheta \Sbfxtheta \Xitheta^\tr\big)^{-1}
        \big(\mathbf{D}(\vctheta) \mathbf{u} - \mathbf{y}\big) 
        + 
        2\Xi^\tr(\vctheta) \big(\Sbfytheta + \Xitheta \Sbfxtheta \Xitheta^\tr\big)^{-1}\Xitheta \mubfxtheta ,\label{eq:derivative to mux}\\ 
        %%
        \pdv{J(\vctheta)}{\mxS^{(\mathbf{x})}} =& -\Xitheta^\tr\big(\Sbfytheta + \Xitheta \Sbfxtheta \Xitheta^\tr\big)^{-1} \big(\mathbf{D}(\vctheta)\mathbf{u} - \mathbf{y} + \Xitheta \mubfxtheta\big) \big(\mathbf{D}(\vctheta)\mathbf{u} - \mathbf{y} + \Xitheta \mubfxtheta\big)^\tr 
        \nonumber
        \\
        &\big(\Sbfytheta + \Xitheta \Sbfxtheta \Xitheta^\tr\big)^{-1}\Xitheta  - \Sbfxtheta^{-1}\big(\Sbfxtheta^{-1} + \Xitheta^\tr \Sbfytheta^{-1} \Xitheta\big)^{-1} \Sbfxtheta^{-1} + \Sbfxtheta^{-1},
        \label{eq:derivative to Sx}\\
        %%
        \pdv{J(\vctheta)}{\Xi} =& -2\big(\Sbfytheta + \Xitheta \Sbfxtheta \Xitheta^\tr\big)^{-1} \big(\mathbf{D}(\vctheta)\mathbf{u} - \mathbf{y} + \Xitheta \mubfxtheta\big) \big(\mathbf{D}(\vctheta)\mathbf{u} - \mathbf{y} + \Xitheta \mubfxtheta\big)^\tr (\Sbfytheta 
        \nonumber\\
        &+\Xitheta \Sbfxtheta \Xitheta^\tr)^{-1}\Xitheta \Sbfxtheta +2\big(\Sbfytheta + \Xitheta \Sbfxtheta \Xitheta^\tr\big)^{-1}\big(\mathbf{D}(\vctheta)\mathbf{u}-\mathbf{y} + \Xitheta \mubfxtheta\big) \mubfxtheta^\tr 
        \nonumber\\
        &+ 2\Sbfytheta^{-1} \Xitheta \big(\Xitheta^\tr \Sbfytheta \Xitheta + \Sbfxtheta^{-1}\big)^{-1},
        \label{eq:derivative to Xi}\\
        %%
        \pdv{J(\vctheta)}{\mathbf{D}} =& \, 2\big(\Sbfytheta + \Xitheta \Sbfxtheta \Xitheta^\tr\big)^{-1}\mathbf{D}(\vctheta)\mathbf{u}\mathbf{u}^\tr - 2\big(\Sbfytheta + \Xitheta \Sbfxtheta \Xitheta^\tr\big)^{-1}(\mathbf{y}+\Xitheta \mubfxtheta)\mathbf{u}^\tr
        \nonumber\\
        &+ 4\big(\Sbfytheta + \Xitheta \Sbfxtheta \Xitheta^\tr\big)^{-1}\Xitheta \mubfxtheta \mathbf{u}^\tr,
        \label{eq:derivative to D}\\
        %%
        \pdv{J(\vctheta)}{\mxS^{(\mathbf{y})}} =& -\big(\Sbfytheta + \Xitheta \Sbfxtheta \Xitheta^\tr\big)^{-1} \big(\mathbf{D}(\vctheta)\mathbf{u} - \mathbf{y} + \Xitheta \mubfxtheta\big) \big(\mathbf{D}(\vctheta)\mathbf{u} - \mathbf{y} + \Xitheta \mubfxtheta\big)^\tr \big(\Sbfytheta 
        \nonumber\\
        &+ \Xitheta \Sbfxtheta \Xitheta^\tr\big)^{-1}  - \Sbfytheta^{-1} \Xitheta\big(\Xitheta^\tr \Sbfytheta^{-1} \Xitheta + \Sbfxtheta^{-1}\big)^{-1}\Xitheta^\tr\Sbfytheta^{-1} + \Sbfytheta^{-1}.
        \label{eq:derivative to Sy}
\end{align}
% 
% Subsequently, one needs to derive the derivatives of $J$ with respect to each parameter. To this end, we define matrices $\mxE_{i,j} = \vce_i\vce_j^\tr$ and $f_{i,j}^{\ n}(\mxA) := \sum_{r=0}^{n-1} \mxA^{r}\mxE_{i,j}\mxA^{n-1}$, for $i,j=1,\ldots,\Nx$. 
% %The derivatives of $J$ with respect to each parameter are presented as follows
% Subsequently, one needs to derive the derivatives of $J$ with respect to each parameter. To this end, we define matrices $\mxE_{i,j} = \vce_i\vce_j^\tr$ and $f_{i,j}^{\ n}(\mxA) := \sum_{r=0}^{n-1} \mxA^{r}\mxE_{i,j}\mxA^{n-1}$, for $i,j=1,\ldots,\Nx$. Thus, we have
%
% Subsequently, the derivatives of $J$ with respect to each parameter needs to be determined. To this end, we define matrices $\mxE_{i,j} = \vce_i\vce_j^\tr$ and $f_{i,j}^{\ n}(\mxA) := \sum_{r=0}^{n-1} \mxA^{r}\mxE_{i,j}\mxA^{n-1}$, for $i,j=1,\ldots,\Nx$. 
% %Thus, we have
% Using matrix calculus techniques, similar to the previous steps, we obtain 
% %the result
%-----------
% Additionally, using Lemma~\ref{lem:chain_rule}, namely the matrix chain rule, we have
% \begin{align}
%         \pdv{J(\vctheta)}{\left[ \mxA \right]_{i,j}} &= \pdv{J(\vctheta)}{\mu^{(\mathbf{x})}(\vctheta)}{}^\tr\, \pdv{\mu^{(\mathbf{x})}(\vctheta)}{\left[\mxA \right]_{i,j}} 
%         + 
%         \trace\left(\pdv{J(\vctheta)}{\mxS^{(\mathbf{x})}(\vctheta)}{}^\tr\, \pdv{\mxS^{(\mathbf{x})}(\vctheta)}{\left[ \mxA \right]_{i,j}} \right), 
%         \label{eq:J derivative A1}\\
%         \pdv{J(\vctheta)}{\left[ \mxB \right]_{i,j}} &=  \pdv{J(\vctheta)}{\mu^{(\mathbf{x})}(\vctheta)}^\tr \pdv{\mu^{(\mathbf{x})}(\vctheta)}{\left[\mxB \right]_{i,j}}, 
%         \\
%         %%
%         \pdv{J(\vctheta)}{[\mxC_0]_{i,j}} &= \trace \left( \pdv{J(\vctheta)}{\Xi(\vctheta)}^\tr \left( \mathbf{I}_{\nsD} \otimes \mxE_{i,j} \right) \right)
%         = \sum_{t=1}^\nsD \left[ \pdv{J(\vctheta)}{\Xi(\vctheta)} \right]_{ti,tj}, 
%         \label{eq:J derivative C0}\\
%         %%
%         \pdv{J(\vctheta)}{[\mxC_k]_{i,j}} &= \trace \left( \pdv{J(\vctheta)}{\Xi(\vctheta)}^\tr \left( \mathbf{I}_{\nsD} \otimes \mxE_{i,j} \right) \right)
%         = \sum_{t=1}^\nsD \vcu_{k,t-1} \left[ \pdv{J(\vctheta)}{\Xi(\vctheta)} \right]_{ti,tj}, \qquad \forall k = 1,2,\cdots,\Nu, 
%         \label{eq:J derivative Ci}\\
%         %%
%         \pdv{J(\vctheta)}{[\mxD]_{i,j}} &= \trace\left(\pdv{J(\vctheta)}{\mathbf{D}(\vctheta)}^\tr \left( \mathbf{I}_{\nsD} \otimes \mxE_{i,j} \right) \right)
%          = \sum_{t=1}^\nsD \left[ \pdv{J(\vctheta)}{\mathbf{D}(\vctheta)} \right]_{ti,tj}, 
%          \label{eq:J derivative D}\\
%         %%
%         \pdv{J(\vctheta)}{\left[\mxS_\vcw \right]_{i,j}} &= \trace \left( \pdv{J(\vctheta)}{\mxS^{(\mathbf{x})}(\vctheta)}^\tr \pdv{\mxS^{(\mathbf{x})}(\vctheta)}{\left[ \mxS_\vcw \right]_{i,j}} \right), 
%         \\
%         %%
%         \pdv{J(\vctheta)}{[\mxS_\vcv]_{i,j}} &= \trace\left(\pdv{J(\vctheta)}{\mxS^{(\mathbf{y})}(\vctheta)}^\tr \left( \mathbf{I}_{\nsD} \otimes \mxE_{i,j} \right) \right)
%          = \sum_{t=1}^\nsD \left[ \pdv{J(\vctheta)}{\mxS^{(\mathbf{y})}(\vctheta)} \right]_{ti,tj}, 
%         \label{eq:J derivative Sv}\\
%         %%
%         \pdv{J(\vctheta)}{ \mu_{x_0}(\vctheta) } &= \begin{bmatrix}
%             {\mxA^{0}}^{\tr} & {\mxA^{1}}^\tr & \cdots {\mxA^{\nsD-1}}^\tr
%         \end{bmatrix} \pdv{J(\vctheta)}{\mu^{(\mathbf{x}})(\vctheta)},
%         \label{eq:J derivative mux0}\\
%          %%
%         \pdv{J(\vctheta)}{[\mxS_{x_0}]_{i,j}} &= \trace\left(\pdv{J(\vctheta)}{\mxS^{(\mathbf{x})}(\vctheta)}^\tr \pdv{\mxS^{(\mathbf{x})}(\vctheta)}{\left[ \mxS_{x_0} \right]_{i,j}} \right), 
%         %%
% \end{align}
% for $i,j=1,\ldots,\Nx$.
Subsequently, we need to determine the derivatives of $J$ with respect to each parameter. To this end, we define the matrices $\mxE_{i,j}$ and $f_{i,j}^{\ n}(\mxA)$ respectively as $\mxE_{i,j} = \vce_i\vce_j^\tr$ and $f_{i,j}^{\ n}(\mxA) := \sum_{r=0}^{n-1} \mxA^{r}\mxE_{i,j}\mxA^{n-1}$, for $i,j=1,\ldots,\Nx$. 
%Thus, we have
% Using matrix calculus techniques, similar to the previous step, we obtain 
%the result
% By applying matrix calculus techniques, as in the previous step, we obtain
Similar to the the previous step, we apply matrix calculus techniques. 
Thus, 
%for $i,j=1,\ldots,\Nx$, 
for $i,j=1,\ldots,\Nx$, $\, k = 1,\ldots,\Nu$, $\, l,m=1,\ldots,\Ny$, and $\, t,s = 0,1,\cdots,\nD-1$, we obtain
\begin{align}
    %\begin{split}
        %%
        % \pdv{J(\vctheta)}{\left[ \mxA \right]_{i,j}} &= \pdv{J(\vctheta)}{\mu^{(\mathbf{x})}(\vctheta)}{}^\tr\, \pdv{\mu^{(\mathbf{x})}(\vctheta)}{\left[\mxA \right]_{i,j}} 
        % + 
        % \trace\left(\pdv{J(\vctheta)}{\mxS^{(\mathbf{x})}(\vctheta)}{}^\tr\, \pdv{\mxS^{(\mathbf{x})}(\vctheta)}{\left[ \mxA \right]_{i,j}} \right), 
        %\label{eq: mu_t_x derivative Aij}\\
        \pdv{\mu^{(\mathbf{x})}_t(\vctheta)}{\left[\mxA \right]_{i,j}} &= \begin{cases}
            0 &\text{if }\  t=0,\\
            \mxE_{i,j} \mu_{\vcx_0} &\text{if }\  t=1,\\
            \sum_{r=0}^{t-1} \mxA^{r}\mxE_{i,j}\mxA^{t-1-r}\mu_{\vcx_0} + \sum_{k=0}^{t-2}f_{i,j}^{\ t-1-k}(\mxA)\mxB\vcu_k&\text{if }\  t\geq2,
        \end{cases}
        \label{eq:J derivative A2}\\
        \pdv{\mxS^{(\mathbf{x})}_{t,s}(\vctheta)}{\left[ \mxA \right]_{i,j}} &= \begin{cases}
            %case 1
            0 &\text{if }\  t,s=0,\\
            %case 2
            \mxS_{\vcx_0}\left[ f^{\ s}_{i,j}(\mxA) \right]^\tr &\text{if }\  t=0,s\geq 1,\\
            %case 3
            f^{\ t}_{i,j}(\mxA)\mxS_{\vcx_0} &\text{if }\  t \geq 1, s=0,\\
            %case 4
            \mxA^{t}\mxS_{\vcx_0} \left[ f^{\ s}_{i,j}(\mxA)\right]^\tr + f^{\ t}_{i,j}(\mxA)\mxS_{\vcx_0}\mxA^{s^\tr}  &\text{if }\ 
             t,s=1,\\
            %case 5
            \mxA^{t}\mxS_{\vcx_0} \left[ f^{\ s}_{i,j}(\mxA)\right]^\tr + f^{\ t}_{i,j}(\mxA)\mxS_{\vcx_0}\mxA^{s^\tr} + \mxS_\vcw \left[ f_{i,j}^{\ s-1}(\mxA) \right]^\tr &\text{if }\ 
             t=1,s\geq 2,\\
            %case 6
            \mxA^{t}\mxS_{\vcx_0} \left[ f^{\ s}_{i,j}(\mxA)\right]^\tr + f^{\ t}_{i,j}(\mxA)\mxS_{\vcx_0}\mxA^{s^\tr} + f_{i,j}^{\ t-1}(\mxA) \mxS_\vcw  &\text{if }\ 
             t \geq 2,s = 1,\\
            %case 7
            \mxA^{t}\mxS_{\vcx_0} \left[ f^{\ s}_{i,j}(\mxA)\right]^\tr + f^{\ t}_{i,j}(\mxA)\mxS_{\vcx_0}\mxA^{s^\tr} &\\
            \qquad+ \sum_{k=0}^{\min (s,t)-1} \left( \mxA^{t-1-k}\mxS_\vcw \left[ f_{i,j}^{\ s-k-1}(\mxA) \right]^\tr + f_{i,j}^{\ t-k-1}(\mxA)  \mxS_\vcw \mxA^{(s-k-1)^\tr}  \right) &\text{if }\ 
             t\geq 2,s\geq 2, \label{eq:J derivative A3}\\
        \end{cases}\\
        %%
        % \pdv{J(\vctheta)}{\left[ \mxB \right]_{i,j}} 
        % &=  
        % \pdv{J(\vctheta)}{\mu^{(\mathbf{x})}(\vctheta)}^\tr \pdv{\mu^{(\mathbf{x})}(\vctheta)}{\left[\mxB \right]_{i,j}}, 
        %\quad 
        \pdv{\mu^{(\mathbf{x})}_t(\vctheta)}{\left[\mxB \right]_{i,k}} 
        &= 
        \sum_{\tau=0}^{t-1} \left[\mxA^{t-1-\tau} \right]_i \mathbf{u}_{\tau,k}, 
        %\qquad \forall t,s = 1,2,\cdots,\nD-1, 
        \label{eq:J derivative B}\\
         %%
        % \pdv{J(\vctheta)}{[\mxC_0]_{i,j}} &= \trace \left( \pdv{J(\vctheta)}{\Xi(\vctheta)}^\tr \left( \mathbf{I}_{\nsD} \otimes \mxE_{i,j} \right) \right)
        % = \sum_{t=1}^\nsD \left[ \pdv{J(\vctheta)}{\Xi(\vctheta)} \right]_{ti,tj}, \label{eq:J derivative C0}\\
        %%
        % \pdv{J(\vctheta)}{[\mxC_k]_{i,j}} &= \trace \left( \pdv{J(\vctheta)}{\Xi(\vctheta)}^\tr \left( \mathbf{I}_{\nsD} \otimes \mxE_{i,j} \right) \right)
        % = \sum_{t=1}^\nsD \vcu_{k,t-1} \left[ \pdv{J(\vctheta)}{\Xi(\vctheta)} \right]_{ti,tj}, \qquad \forall k = 1,2,\cdots,\Nu, \label{eq:J derivative Ci}\\
        %%
        % \pdv{J(\vctheta)}{[\mxD]_{i,j}} &= \trace\left(\pdv{J(\vctheta)}{\mathbf{D}(\vctheta)}^\tr \left( \mathbf{I}_{\nsD} \otimes \mxE_{i,j} \right) \right)
        %  = \sum_{t=1}^\nsD \left[ \pdv{J(\vctheta)}{\mathbf{D}(\vctheta)} \right]_{ti,tj}, \label{eq:J derivative D}\\
        %%
        % \pdv{J(\vctheta)}{\left[\mxS_\vcw \right]_{i,j}} &= \trace \left( \pdv{J(\vctheta)}{\mxS^{(\mathbf{x})}(\vctheta)}^\tr \pdv{\mxS^{(\mathbf{x})}(\vctheta)}{\left[ \mxS_\vcw \right]_{i,j}} \right), 
        % \quad 
        \pdv{\mxS^{(\mathbf{x})}_{t,s}(\vctheta)}{\left[ \mxS_\vcw \right]_{i,j}} 
        &= 
        \sum_{\tau=0}^{\min(s,t)-1} \left[\mxA^{t-1-\tau}\right]_i \left[\mxA^{s-1-\tau}\right]_j^\tr,
        %\qquad \forall t,s = 0,1,\cdots,\nD-1, 
        \label{eq:J derivative Sw}\\
        %%
        % \pdv{J(\vctheta)}{[\mxS_\vcv]_{i,j}} &= \trace\left(\pdv{J(\vctheta)}{\mxS^{(\mathbf{y})}(\vctheta)}^\tr \left( \mathbf{I}_{\nsD} \otimes \mxE_{i,j} \right) \right)
        %  = \sum_{t=1}^\nsD \left[ \pdv{J(\vctheta)}{\mxS^{(\mathbf{y})}(\vctheta)} \right]_{ti,tj}, \label{eq:J derivative Sv}\\
        %%
        % \pdv{J(\vctheta)}{ \mu_{x_0}(\vctheta) } &= \begin{bmatrix}
        %     {\mxA^{0}}^{\tr} & {\mxA^{1}}^\tr & \cdots {\mxA^{\nsD-1}}^\tr
        % \end{bmatrix} \pdv{J(\vctheta)}{\mu^{(\mathbf{x}})(\vctheta)},
        % \label{eq:J derivative mux0}\\
         %%
        % \pdv{J(\vctheta)}{[\mxS_{x_0}]_{i,j}} &= \trace\left(\pdv{J(\vctheta)}{\mxS^{(\mathbf{x})}(\vctheta)}^\tr \pdv{\mxS^{(\mathbf{x})}(\vctheta)}{\left[ \mxS_{x_0} \right]_{i,j}} \right), 
        %\quad
        \pdv{\mxS^{(\mathbf{x})}_{t,s}(\vctheta)}{\left[ \mxS_{x_0} \right]_{i,j}} &= \left[ \mxA^t \right]_i \left[ \mxA^s \right]_j^\tr, 
        %\qquad \forall t,s = 0,1,\cdots,\nD-1.
        \label{eq:J derivative Sx0}
    %\end{split}
\end{align}
% Now, we can compute the derivatives of $J$ with respect to each parameter. Define matrix $\mxE_{i,j} = \vce_i\vce_j^\tr$ and $f_{i,j}^{\ n}(\mxA) := \sum_{r=0}^{n-1} \mxA^{r}\mxE_{i,j}\mxA^{n-1}$. The derivatives of $J$ with respect to each parameter are presented as follows,
% \begin{align}
%     %\begin{split}
%         %%
%         \pdv{J}{[\mxD]_{i,j}} &= \trace\left(\pdv{J}{\mathbf{D}}^\tr \left( \mathbf{I}_{\nD} \otimes \mxE_{i,j} \right) \right)
%          = \sum_{t=1}^\nD \left[ \pdv{J}{\mathbf{D}} \right]_{ti,tj}, \label{eq:J derivative 1}\\
%          %%
%         \pdv{J}{[\mxC_0]_{i,j}} &= \trace \left( \pdv{J}{\Xi}^\tr \left( \mathbf{I}_{\nD} \otimes \mxE_{i,j} \right) \right)
%         = \sum_{t=1}^\nD \left[ \pdv{J}{\Xi} \right]_{ti,tj}, \label{eq:J derivative 2}\\
%         %%
%         \pdv{J}{[\mxC_k]_{i,j}} &= \trace \left( \pdv{J}{\Xi}^\tr \left( \mathbf{I}_{\nD} \otimes \mxE_{i,j} \right) \right)
%         = \sum_{t=1}^\nD \vcu_{k,t-1} \left[ \pdv{J}{\Xi} \right]_{ti,tj}, \qquad \forall k = 1,2,\cdots,\Nu, \label{eq:J derivative 3}\\
%         %%
%         \pdv{J}{[\mxS_\vcv]_{i,j}} &= \trace\left(\pdv{J}{\mxS^{(\mathbf{y})}}^\tr \left( \mathbf{I}_{\nD} \otimes \mxE_{i,j} \right) \right)
%          = \sum_{t=1}^\nD \left[ \pdv{J}{\mxS^{(\mathbf{y})}} \right]_{ti,tj}, \label{eq:J derivative 4}\\
%          %%
%         \pdv{J}{[\mxS_{x_0}]_{i,j}} &= \trace\left(\pdv{J}{\mxS^{(\mathbf{x})}}^\tr \pdv{\mxS^{(\mathbf{x})}}{\left[ \mxS_{x_0} \right]_{i,j}} \right), \quad
%         \pdv{\mxS^{(\mathbf{x})}_{t,s}}{\left[ \mxS_{x_0} \right]_{i,j}} = \left[ \mxA^t \right]_i \left[ \mxA^s \right]_j^\tr, \qquad \forall t,s = 0,1,\cdots,\nD-1, \label{eq:J derivative 5}\\
%         %%
%         \pdv{J}{ \mu_{x_0} } &= \begin{bmatrix}
%             \mxA^{0^\tr} & \mxA^{1^\tr} & \cdots \mxA^{\nD-1^\tr}
%         \end{bmatrix} \pdv{J}{\mu^{(\mathbf{x}})}, \label{eq:J derivative 6}\\
%         %%
%         \pdv{J}{\left[\mxS_\vcw \right]_{i,j}} &= \trace \left( \pdv{J}{\mxS^{(\mathbf{x})}}^\tr \pdv{\mxS^{(\mathbf{x})}}{\left[ \mxS_\vcw \right]_{i,j}} \right), \quad \pdv{\mxS^{(\mathbf{x})}_{t,s}}{\left[ \mxS_\vcw \right]_{i,j}} = \sum_{k=0}^{\min(s,t)-1} \left[\mxA^{t-1-k}\right]_i \left[\mxA^{s-1-k}\right]_j^\tr,\qquad \forall t,s = 0,1,\cdots,\nD-1, \label{eq:J derivative 7}\\
%         %%
%         \pdv{J}{\left[ \mxB \right]_{i,j}} &=  \pdv{J}{\mu^{(\mathbf{x})}}^\tr \pdv{\mu^{(\mathbf{x})}}{\left[\mxB \right]_{i,j}}, \quad \pdv{\mu^{(\mathbf{x})}_t}{\left[\mxB \right]_{i,j}} = \sum_{k=0}^{t-1} \left[\mxA^{t-1-k} \right]_i \mathbf{u}_{k,j}, \qquad \forall t,s = 1,2,\cdots,\nD-1, \label{eq:J derivative 8}\\
%         %%
%         \pdv{J}{\left[ \mxA \right]_{i,j}} &= \pdv{J}{\mu^{(\mathbf{x})}}^\tr \pdv{\mu^{(\mathbf{x})}}{\left[\mxA \right]_{i,j}} + \trace\left(\pdv{J}{\mxS^{(\mathbf{x})}}^\tr \pdv{\mxS^{(\mathbf{x})}}{\left[ \mxA \right]_{i,j}} \right), \label{eq:J derivative 9}\\
%         \pdv{\mu^{(\mathbf{x})}_t}{\left[\mxA \right]_{i,j}} &= \begin{cases}
%             0 & t=0\\
%             \mxE_{i,j} \mu_{\vcx_0} & t=1\\
%             \sum_{r=0}^{t-1} \mxA^{r}\mxE_{i,j}\mxA^{t-1-r}\mu_{\vcx_0} + \sum_{k=0}^{t-2}f_{i,j}^{\ t-1-k}(\mxA)\mxB\vcu_k& t\geq2
%         \end{cases},\\
%         \pdv{\mxS^{(\mathbf{x})}_{t,s}}{\left[ \mxA \right]_{i,j}} &= \begin{cases}
%             %case 1
%             0 & t,s=0\\
%             %case 2
%             \mxS^{(\mathbf{x})}_{0,0} \left[ f^{\ s}_{i,j}(\mxA) \right]^\tr & t=0,s\geq 1\\
%             %case 3
%             f^{\ t}_{i,j}(\mxA)\mxS^{(\mathbf{x})}_{0,0} & t \geq 1, s=0\\
%             %case 4
%             \mxA^{t}\mxS^{(\mathbf{x})}_{0,0} \left[ f^{\ s}_{i,j}(\mxA)\right]^\tr + f^{\ t}_{i,j}(\mxA)\mxS^{(\mathbf{x})}_{0,0}\mxA^{s^\tr}  &
%              t,s=1\\
%             %case 5
%             \mxA^{t}\mxS^{(\mathbf{x})}_{0,0} \left[ f^{\ s}_{i,j}(\mxA)\right]^\tr + f^{\ t}_{i,j}(\mxA)\mxS^{(\mathbf{x})}_{0,0}\mxA^{s^\tr} + \mxS_\vcw \left[ f_{i,j}^{\ s-1}(\mxA) \right]^\tr &
%              t=1,s\geq 2\\
%             %case 6
%             \mxA^{t}\mxS^{(\mathbf{x})}_{0,0} \left[ f^{\ s}_{i,j}(\mxA)\right]^\tr + f^{\ t}_{i,j}(\mxA)\mxS^{(\mathbf{x})}_{0,0}\mxA^{s^\tr} + f_{i,j}^{\ t-1}(\mxA) \mxS_\vcw  &
%              t \geq 2,s = 1\\
%             %case 7
%             \mxA^{t}\mxS^{(\mathbf{x})}_{0,0} \left[ f^{\ s}_{i,j}(\mxA)\right]^\tr + f^{\ t}_{i,j}(\mxA)\mxS^{(\mathbf{x})}_{0,0}\mxA^{s^\tr} &\\
%             \qquad+ \sum_{k=0}^{\min (s,t)-1} \left( \mxA^{t-1-k}\mxS_\vcw \left[ f_{i,j}^{\ s-k-1}(\mxA) \right]^\tr + f_{i,j}^{\ t-k-1}(\mxA)  \mxS_\vcw \mxA^{(s-k-1)^\tr}  \right) &
%              t\geq 2,s\geq 2 \label{eq:J derivative 10}\\
%         \end{cases}.
%     %\end{split}
% \end{align}
To bridge \eqref{eq:derivative to mux}--\eqref{eq:derivative to Sy}  
and \eqref{eq:J derivative A2}--\eqref{eq:J derivative Sx0}, we need to employ the matrix chain rule, i.e., using Lemma~\ref{lem:chain_rule}, we have
\begin{align}
        \pdv{J(\vctheta)}{\left[ \mxA \right]_{i,j}} &= \pdv{J(\vctheta)}{\mu^{(\mathbf{x})}(\vctheta)}{}^\tr\, \pdv{\mu^{(\mathbf{x})}(\vctheta)}{\left[\mxA \right]_{i,j}} 
        + 
        \trace\left(\pdv{J(\vctheta)}{\mxS^{(\mathbf{x})}(\vctheta)}{}^\tr\, \pdv{\mxS^{(\mathbf{x})}(\vctheta)}{\left[ \mxA \right]_{i,j}} \right), 
        \label{eq:J derivative A1}\\
        \pdv{J(\vctheta)}{\left[ \mxB \right]_{i,k}} &=  \pdv{J(\vctheta)}{\mu^{(\mathbf{x})}(\vctheta)}^\tr \pdv{\mu^{(\mathbf{x})}(\vctheta)}{\left[\mxB \right]_{i,k}}, 
        \\
        %%
        \pdv{J(\vctheta)}{[\mxC_0]_{l,j}} &= \trace \left( \pdv{J(\vctheta)}{\Xi(\vctheta)}^\tr \left( \mathbf{I}_{\nsD} \otimes \mxE_{l,j} \right) \right)
        = \sum_{t=1}^\nsD \left[ \pdv{J(\vctheta)}{\Xi(\vctheta)} \right]_{tl,tj}, 
        \label{eq:J derivative C0}\\
        %%
        \pdv{J(\vctheta)}{[\mxC_k]_{l,j}} &= \trace \left( \pdv{J(\vctheta)}{\Xi(\vctheta)}^\tr \left( \mathbf{I}_{\nsD} \otimes \mxE_{l,j} \right) \right)
        = \sum_{t=1}^\nsD \vcu_{k,t-1} \left[ \pdv{J(\vctheta)}{\Xi(\vctheta)} \right]_{tl,tj}, 
        %\qquad \forall k = 1,2,\cdots,\Nu, 
        \label{eq:J derivative Ci}\\
        %%
        \pdv{J(\vctheta)}{[\mxD]_{l,k}} &= \trace\left(\pdv{J(\vctheta)}{\mathbf{D}(\vctheta)}^\tr \left( \mathbf{I}_{\nsD} \otimes \mxE_{l,k} \right) \right)
         = \sum_{t=1}^\nsD \left[ \pdv{J(\vctheta)}{\mathbf{D}(\vctheta)} \right]_{tl,tk}, 
         \label{eq:J derivative D}\\
        %%
        \pdv{J(\vctheta)}{\left[\mxS_\vcw \right]_{i,j}} &= \trace \left( \pdv{J(\vctheta)}{\mxS^{(\mathbf{x})}(\vctheta)}^\tr \pdv{\mxS^{(\mathbf{x})}(\vctheta)}{\left[ \mxS_\vcw \right]_{i,j}} \right), 
        \\
        %%
        \pdv{J(\vctheta)}{[\mxS_\vcv]_{l,m}} &= \trace\left(\pdv{J(\vctheta)}{\mxS^{(\mathbf{y})}(\vctheta)}^\tr \left( \mathbf{I}_{\nsD} \otimes \mxE_{l,m} \right) \right)
         = \sum_{t=1}^\nsD \left[ \pdv{J(\vctheta)}{\mxS^{(\mathbf{y})}(\vctheta)} \right]_{tl,tm}, 
        \label{eq:J derivative Sv}\\
        %%
        \pdv{J(\vctheta)}{ \mu_{x_0}(\vctheta) } &= \begin{bmatrix}
            (\mxA^{0})^{\tr} & (\mxA^{1})^\tr & \cdots (\mxA^{\nsD-1})^\tr
        \end{bmatrix} \pdv{J(\vctheta)}{\mu^{(\mathbf{x}})(\vctheta)},
        \label{eq:J derivative mux0}\\
         %%
        \pdv{J(\vctheta)}{[\mxS_{x_0}]_{i,j}} &= \trace\left(\pdv{J(\vctheta)}{\mxS^{(\mathbf{x})}(\vctheta)}^\tr \pdv{\mxS^{(\mathbf{x})}(\vctheta)}{\left[ \mxS_{x_0} \right]_{i,j}} \right), 
        \label{eq:J derivative muS0}
        %%
\end{align}
for $i,j=1,\ldots,\Nx$, $k = 1,\ldots,\Nu$, and $l,m=1,\ldots,\Ny$.
\iffalse
% \iftrue
% for backup
\begin{align}
    %\begin{split}
        %%
        \pdv{J(\vctheta)}{\left[ \mxA \right]_{i,j}} &= \pdv{J(\vctheta)}{\mu^{(\mathbf{x})}(\vctheta)}{}^\tr\, \pdv{\mu^{(\mathbf{x})}(\vctheta)}{\left[\mxA \right]_{i,j}} 
        + 
        \trace\left(\pdv{J(\vctheta)}{\mxS^{(\mathbf{x})}(\vctheta)}{}^\tr\, \pdv{\mxS^{(\mathbf{x})}(\vctheta)}{\left[ \mxA \right]_{i,j}} \right), 
        % \qquad \forall i,j=1,\ldots,\Nx,
        \label{eq:J derivative A1}\\
        \pdv{\mu^{(\mathbf{x})}_t(\vctheta)}{\left[\mxA \right]_{i,j}} &= \begin{cases}
            0 & t=0,\\
            \mxE_{i,j} \mu_{\vcx_0} & t=1,\\
            \sum_{r=0}^{t-1} \mxA^{r}\mxE_{i,j}\mxA^{t-1-r}\mu_{\vcx_0} + \sum_{k=0}^{t-2}f_{i,j}^{\ t-1-k}(\mxA)\mxB\vcu_k& t\geq2,
        \end{cases}\label{eq:J derivative A2}\\
        \pdv{\mxS^{(\mathbf{x})}_{t,s}(\vctheta)}{\left[ \mxA \right]_{i,j}} &= \begin{cases}
            %case 1
            0 & t,s=0,\\
            %case 2
            \mxS_{\vcx_0}\left[ f^{\ s}_{i,j}(\mxA) \right]^\tr & t=0,s\geq 1,\\
            %case 3
            f^{\ t}_{i,j}(\mxA)\mxS_{\vcx_0} & t \geq 1, s=0,\\
            %case 4
            \mxA^{t}\mxS_{\vcx_0} \left[ f^{\ s}_{i,j}(\mxA)\right]^\tr + f^{\ t}_{i,j}(\mxA)\mxS_{\vcx_0}\mxA^{s^\tr}  &
             t,s=1,\\
            %case 5
            \mxA^{t}\mxS_{\vcx_0} \left[ f^{\ s}_{i,j}(\mxA)\right]^\tr + f^{\ t}_{i,j}(\mxA)\mxS_{\vcx_0}\mxA^{s^\tr} + \mxS_\vcw \left[ f_{i,j}^{\ s-1}(\mxA) \right]^\tr &
             t=1,s\geq 2,\\
            %case 6
            \mxA^{t}\mxS_{\vcx_0} \left[ f^{\ s}_{i,j}(\mxA)\right]^\tr + f^{\ t}_{i,j}(\mxA)\mxS_{\vcx_0}\mxA^{s^\tr} + f_{i,j}^{\ t-1}(\mxA) \mxS_\vcw  &
             t \geq 2,s = 1,\\
            %case 7
            \mxA^{t}\mxS_{\vcx_0} \left[ f^{\ s}_{i,j}(\mxA)\right]^\tr + f^{\ t}_{i,j}(\mxA)\mxS_{\vcx_0}\mxA^{s^\tr} &\\
            \qquad+ \sum_{k=0}^{\min (s,t)-1} \left( \mxA^{t-1-k}\mxS_\vcw \left[ f_{i,j}^{\ s-k-1}(\mxA) \right]^\tr + f_{i,j}^{\ t-k-1}(\mxA)  \mxS_\vcw \mxA^{(s-k-1)^\tr}  \right) &
             t\geq 2,s\geq 2, \label{eq:J derivative A3}\\
        \end{cases}\\
        %%
        \pdv{J(\vctheta)}{\left[ \mxB \right]_{i,j}} &=  \pdv{J(\vctheta)}{\mu^{(\mathbf{x})}(\vctheta)}^\tr \pdv{\mu^{(\mathbf{x})}(\vctheta)}{\left[\mxB \right]_{i,j}}, \quad \pdv{\mu^{(\mathbf{x})}_t(\vctheta)}{\left[\mxB \right]_{i,j}} = \sum_{k=0}^{t-1} \left[\mxA^{t-1-k} \right]_i \mathbf{u}_{k,j}, \qquad \forall t,s = 1,2,\cdots,\nD-1, \label{eq:J derivative B}\\
         %%
        \pdv{J(\vctheta)}{[\mxC_0]_{i,j}} &= \trace \left( \pdv{J(\vctheta)}{\Xi(\vctheta)}^\tr \left( \mathbf{I}_{\nsD} \otimes \mxE_{i,j} \right) \right)
        = \sum_{t=1}^\nsD \left[ \pdv{J(\vctheta)}{\Xi(\vctheta)} \right]_{ti,tj}, \label{eq:J derivative C0}\\
        %%
        \pdv{J(\vctheta)}{[\mxC_k]_{i,j}} &= \trace \left( \pdv{J(\vctheta)}{\Xi(\vctheta)}^\tr \left( \mathbf{I}_{\nsD} \otimes \mxE_{i,j} \right) \right)
        = \sum_{t=1}^\nsD \vcu_{k,t-1} \left[ \pdv{J(\vctheta)}{\Xi(\vctheta)} \right]_{ti,tj}, \qquad \forall k = 1,2,\cdots,\Nu, \label{eq:J derivative Ci}\\
        %%
        \pdv{J(\vctheta)}{[\mxD]_{i,j}} &= \trace\left(\pdv{J(\vctheta)}{\mathbf{D}(\vctheta)}^\tr \left( \mathbf{I}_{\nsD} \otimes \mxE_{i,j} \right) \right)
         = \sum_{t=1}^\nsD \left[ \pdv{J(\vctheta)}{\mathbf{D}(\vctheta)} \right]_{ti,tj}, \label{eq:J derivative D}\\
        %%
        \pdv{J(\vctheta)}{\left[\mxS_\vcw \right]_{i,j}} &= \trace \left( \pdv{J(\vctheta)}{\mxS^{(\mathbf{x})}(\vctheta)}^\tr \pdv{\mxS^{(\mathbf{x})}(\vctheta)}{\left[ \mxS_\vcw \right]_{i,j}} \right), \quad \pdv{\mxS^{(\mathbf{x})}_{t,s}(\vctheta)}{\left[ \mxS_\vcw \right]_{i,j}} = \sum_{k=0}^{\min(s,t)-1} \left[\mxA^{t-1-k}\right]_i \left[\mxA^{s-1-k}\right]_j^\tr,\qquad \forall t,s = 0,1,\cdots,\nD-1, \label{eq:J derivative Sw}\\
        %%
        \pdv{J(\vctheta)}{[\mxS_\vcv]_{i,j}} &= \trace\left(\pdv{J(\vctheta)}{\mxS^{(\mathbf{y})}(\vctheta)}^\tr \left( \mathbf{I}_{\nsD} \otimes \mxE_{i,j} \right) \right)
         = \sum_{t=1}^\nsD \left[ \pdv{J(\vctheta)}{\mxS^{(\mathbf{y})}(\vctheta)} \right]_{ti,tj}, \label{eq:J derivative Sv}\\
        %%
        \pdv{J(\vctheta)}{ \mu_{x_0}(\vctheta) } &= \begin{bmatrix}
            {\mxA^{0}}^{\tr} & {\mxA^{1}}^\tr & \cdots {\mxA^{\nsD-1}}^\tr
        \end{bmatrix} \pdv{J(\vctheta)}{\mu^{(\mathbf{x}})(\vctheta)},\label{eq:J derivative mux0}\\
         %%
        \pdv{J(\vctheta)}{[\mxS_{x_0}]_{i,j}} &= \trace\left(\pdv{J(\vctheta)}{\mxS^{(\mathbf{x})}(\vctheta)}^\tr \pdv{\mxS^{(\mathbf{x})}(\vctheta)}{\left[ \mxS_{x_0} \right]_{i,j}} \right), \quad
        \pdv{\mxS^{(\mathbf{x})}_{t,s}(\vctheta)}{\left[ \mxS_{x_0} \right]_{i,j}} = \left[ \mxA^t \right]_i \left[ \mxA^s \right]_j^\tr, \qquad \forall t,s = 0,1,\cdots,\nD-1.\label{eq:J derivative Sx0}
    %\end{split}
\end{align}
\fi
% The introduced ML approach for estimating vector of parameters $\vctheta$ is summarized in Algorithm \ref{Al:ML ID}. 
The proposed ML approach for estimating the parameter vector $\vctheta$ is outlined in Algorithm~\ref{Al:ML ID}.

%-------
\begin{remark}
    Given $\nabla_\vctheta J$, , alternative first-order optimization methods \cite{nocedal1999numerical} can be employed instead of gradient descent. For instance, the \emph{Broyden-Fletcher-Goldfarb-Shanno} (BFGS) algorithm or its \emph{limited-memory} variant (L-BFGS) can be used to achieve faster convergence. 
\end{remark}
%-------
\begin{remark}
One can easily see that the cost function $J$ is nonconvex. Consequently, the gradient descent algorithm, or any other first-order method, may converge to the local optima of $J$ rather than its global optimum, which can potentially affect the estimation performance and the accuracy of parameter identification.
\end{remark}
%-------
\begin{remark}\label{rem:comp_comp}
% The calculation of derivatives in this method demands significant computational resources, with a time complexity of $O(n^3)$. 
% The computation of derivatives in this method requires substantial computational resources, exhibiting a time complexity of
% It can be shown that the introduced derivative calculation used in the proposed ML method has the computationally complexity of $O(n^3)$. One should note that this will result in considerable computational time and requiring substantial resources, particularly when the length of measurement data is extensively large.
% It can be shown that the derivative calculation used in the proposed ML method has a computational complexity of $O(n^3)$. It is important to note that this can lead to significant computational time and require substantial resources, especially when the length of the measurement data is very large.
It can be shown that the derivative calculation used in the proposed ML method has a computational complexity of $O(\nD^3)$. One should note that this can lead to significant computational time and require substantial resources, particularly when the set of measurement data is considerably large.
\end{remark}

% In the next subsection, we show the complexity of Algorithm \ref{Al:ML ID} in a numerical example.
% In the remainder of this section, we elaborate further on the computational complexity of  Algorithm \ref{Al:ML ID} through a simple numerical example.
% In the rest of this section, we provide a detailed analysis of the computational complexity of Algorithm \ref{Al:ML ID} using a straightforward numerical example.
% In the rest of this section, we provide an Monte Carlo empirical analysis for the computational complexity of Algorithm \ref{Al:ML ID}.
In the remainder of this section, we present a Monte Carlo empirical analysis of the computational complexity of Algorithm \ref{Al:ML ID}.

% Thus, in the simulation, it is not suitable for large datasets. In the next section, we present an alternative approach based on the EM algorithm, which is better suited for handling large datasets efficiently. 
\begin{algorithm}[t]
    \caption{ML Estimation Method for Identification of Linear Dynamics with Bilinear Observation Models}\label{Al:ML ID}
    \begin{algorithmic}
        \Statex \textbf{Input:} $\Dcal$.
        \Statex \textbf{Output:} $\vctheta$.
        \State \textbf{Initial guess:} $\hat{\vctheta}_0$
        \State $k \gets 0$
        \While{$\mathbf{1}$}
            \State Current parameters estimates: $\hat{\vctheta} \gets \hat{\vctheta}_k$
            \State Compute $\mxS^{(\mathbf{x})}{(\hat{\vctheta})}, \mxS^{(\mathbf{y})}{(\hat{\vctheta})}, \Xi{(\hat{\vctheta})}, \mathbf{D}{(\hat{\vctheta})}, \mu^{(\mathbf{x})}{(\hat{\vctheta})}$.
            \State Compute $\pdv{J(\vctheta)}{\mu^{(\mathbf{x})}(\vctheta)}|_{\vctheta = \hat{\vctheta}}, \pdv{J(\vctheta)}{\mxS^{(\mathbf{x})}(\vctheta)}|_{\vctheta = \hat{\vctheta}}, \pdv{J(\vctheta)}{\Xi(\vctheta)}|_{\vctheta = \hat{\vctheta}}, \pdv{J(\vctheta)}{\mathbf{D}(\vctheta)}|_{\vctheta = \hat{\vctheta}}$ and $\pdv{J(\vctheta)}{\mxS^{(\mathbf{y})}(\vctheta)}|_{\vctheta = \hat{\vctheta}}$ as in \eqref{eq:derivative to mux}--\eqref{eq:derivative to Sy}. 
            \State Find the derivatives $\pdv{J(\vctheta)}{\vctheta}|_{\vctheta = \hat{\vctheta}}$ 
            \eqref{eq:J derivative A2} -- \eqref{eq:J derivative Sx0} and \eqref{eq:J derivative A1} -- \eqref{eq:J derivative muS0}.
            \State Update $\hat{\vctheta}_{k+1}$ using gradient descent.
            \If{$\lVert \hat{\vctheta}_{k+1} -- \hat{\vctheta}_k \rVert< \epsilon$,}
            \State break
            \Else
            \State $k \gets k+1$
            \EndIf
        \EndWhile
        \State $\vctheta \gets \hat{\vctheta}_k$
    \end{algorithmic}
    \label{alg_1}
\end{algorithm}



% Define $\mxT^{(\mathbf{x})} = \mxS^{(\mathbf{x})^{-1}}$ and $\mxT^{(\mathbf{y})} = \mxS^{(\mathbf{y})^{-1}}$.
% \begin{equation}
% \begin{split}
%     J:= -2\mathrm{log} \, p(\mathbf{y}|\vctheta) =& \, \mathrm{log}\mathrm{det} \, (\Xi^\tr\mxT^{(\mathbf{y})}\Xi + \mxT^{(\mathbf{x})}) - \mathrm{log}\mathrm{det} \, (\mxT^{(\mathbf{x})}) - \mathrm{log}\mathrm{det} \, (\mxT^{(\mathbf{y})}) - \mu^{(\mathbf{x})^\tr} \mxT^{(\mathbf{x})}\mu^{(\mathbf{x})} - (\mathbf{D}\mathbf{u}-\mathbf{y})^\tr \mxT^{(\mathbf{y})} (\mathbf{D}\mathbf{u}-\mathbf{y})\\
%     & \, +\left[ \left( \mxD\mathbf{u} - \mathbf{y} \right)^\tr\mxT^{(\mathbf{y})} \Xi - \mu^{(\mathbf{x})^\tr}\mxT^{(\mathbf{x})}  \right] \left( \Xi^\tr\mxT^{(\mathbf{y})}\Xi + \mxT^{(\mathbf{x})} \right)^{-1} \left[ \left( \mxD\mathbf{u} - \mathbf{y} \right)^\tr\mxT^{(\mathbf{y})} \Xi - \mu^{(\mathbf{x})^\tr}\mxT^{(\mathbf{x})}  \right]^\tr\\
%     =& \, \mathrm{log}\mathrm{det} \, (\Xi^\tr\mxT^{(\mathbf{y})}\Xi + \mxT^{(\mathbf{x})}) - \mathrm{log}\mathrm{det} \, (\mxT^{(\mathbf{x})}) - \mathrm{log}\mathrm{det} \, (\mxT^{(\mathbf{y})})\\
%     &- \, (\mxD\mathbf{u} - \mathbf{y} - \Xi \mu^{(\mathbf{x})})^\tr (\mxS^{(\mathbf{y})} + \Xi\mxS^{(\mathbf{x})}\Xi^\tr)^{-1} (\mxD\mathbf{u} - \mathbf{y} - \Xi \mu^{(\mathbf{x})})\\
%     &- \, 2\mu^{{(\mathbf{x})}^\tr} \Xi^\tr(\Xi\mxS^{(\mathbf{x})}\Xi^\tr + \mxS^{(\mathbf{y})})^{-1}(\mxD\mathbf{u}-\mathbf{y}) - 2(\mxD\mathbf{u}-\mathbf{y})^\tr(\Xi\mxS^{(\mathbf{x})}\Xi^\tr + \mxS^{(\mathbf{y})})^{-1}\Xi\mu^{(\mathbf{x})}.
% \end{split}
% \end{equation}

% \subsection{Maximum Likelihood Computational Complexity Analysis: An Empirical Evaluation}
\subsection{Computational Complexity Analysis of Maximum Likelihood: An Empirical Evaluation}
\label{sec:Complexity Analysis}
% In this subsection, we show the computational complexity of Algorithm \ref{Al:ML ID} through a simple numerical example. Consider the following system
To further elaborate on the computational complexity of Algorithm \ref{Al:ML ID}, we perform Monte Carlo numerical analysis on simple example. To this end, we consider a one-dimensional system governed by the equations
\begin{equation}\label{eqn:simple_system}
\begin{split}
    \vcx_{t+1} &= 0.6\,\vcx_t + 0.45\,\vcu_t + \vcw_t,\\
    \vcy_t &= (0.3 + 0.1\vcu_t)\, \vcx_t + \vcv_t,
\end{split}
\end{equation}
% \begin{align}
%     \vcx_{t+1} &= 0.6\,\vcx_t + 0.45\,\vcu_t + \vcw_t,\\
%     \vcy_t &= (0.3 + 0.1\vcu_t)\, \vcx_t + \vcv_t,
% \end{align}
% with a mean initial state $\mu_{\vcx_0} = 1$,  and, the rest of matrices are set to almost zero value for the ease of discussion. 
with a mean initial state of $\mu_{\vcx_0} = 1$. 
% For the sake of clarity and ease of discussion, the variance of the initial state is set to a near-zero value.
For clarity and simplicity in the discussion, the variance of the initial state is set to a negligible value.
% For simplicity and clarity of discussion, 
% For the clarity and the ease of discussion, the remaining matrices are set to near-zero values.
% For the sake of clarity and the ease of discussion, the remaining matrices are set to near-zero values.
% For the sake of clarity and to facilitate discussion, the remaining matrices are assigned near-zero values.
% For the sake of clarity and ease of discussion, the remaining matrices are set to near-zero values.
%and the covariance values $\mxS_\vcw$, $\mxS_\vcv$ and $\mxS_{\vcx_0}$ set to $5 \times 10^{-4}$ \cmr{[MK: too much small!!!]}.
% The noise covariance 
% i.e., $\mxA=0.6$, $\mxB=0.45$, $\mxC_0=0.3$, $\mxC_1=0.1$, and, the rest of matrices are set to zero for the ease of discussion. The covariance matrices $\mxS_\vcw, \mxS_\vcv$ and $\mxS_{\vcx_0}$ are set as $5e^{-4}$. The initial state is set as $\mu_{\vcx_0} = 1$. 
% We conduct Monte Carlo numerical experiments considering different lengths for the set of data, specifically $\nD = 20, 40, \ldots, 200$. With respect to each of the considered length for the dataset, $100$ different realizations of the initial state, random binary input sequences for the inputs, process noise, and measurement noise are generated, and subsequently, the state and output trajectories are obtained according to \eqref{eqn:simple_system}. Following this, we have employed Algorithm~\ref{Al:ML ID} to estimate the parameters of system, where the parameters are first initialized to half of their true values, i.e., $\hat{\vctheta}_0=\frac{1}{2}\vctheta$. 
% the initial value of the parameters is set to half of the true ones,
%
% We conduct Monte Carlo numerical experiments using datasets of different lengths, specifically $\nD = 20, 40, \ldots, 200$. For each dataset length, $100$ independent realizations of the initial state, random binary input sequences, process noise, and measurement noise are generated. Using these realizations, state and output trajectories are computed according to \eqref{eqn:simple_system}. Subsequently, Algorithm~\ref{Al:ML ID} is applied to estimate the system parameters, starting with an initialization set to half their true values, i.e., $\hat{\vctheta}_0 = \frac{1}{2}\vctheta$.
%
We conduct Monte Carlo numerical experiments considering different lengths for the set of data, specifically $\nD = 20, 40, \ldots, 200$. With respect to each of the considered lengths for the dataset, $100$ different realizations of the initial state, random binary input sequences, process noise, and measurement noise are generated. Subsequently, the state and output trajectories are obtained according to \eqref{eqn:simple_system}. 
%For the sake of clarity in the discussion, we focus on moderately noisy conditions, i.e., with SNR levels ranging from $15$\,dB to $20$\,dB, and adjust the noise variances accordingly.
For the sake of clarity in the discussion, the focus is on moderately noisy conditions, i.e., SNR levels ranging from $15$\,dB to $20$\,dB, and the noise variances are adjusted accordingly.
% Following this, Algorithm~\ref{Al:ML ID} is employed to estimate the parameters of the system, where they are initialized to half of their true values, i.e., $\hat{\vctheta}_0=\frac{1}{2}\vctheta$. 
% We employ Algorithm~\ref{Al:ML ID} to estimate the parameters of the system, where they are first initialized to half of their true values, i.e., $\hat{\vctheta}_0=\frac{1}{2}\vctheta$.
Algorithm~\ref{Al:ML ID} is employed to estimate the system parameters, initialized to half of their true values, i.e., $\hat{\vctheta}_0 = \frac{1}{2}\vctheta$.
% For the sake of discussion clarity, we opt to work in moderately noisy situation, i.e., SNR levels in the range of $15$\,dB to $20$\,dB, and tune the noise variances are tuned accordingly.
% For the sake of clarity in the discussion, we focus on moderately noisy conditions, i.e., with SNR levels ranging from $15$\,dB to $20$\,dB, , and adjust the noise variances accordingly.
%Nonetheless, the overall SNR is in the range of $15$\,dB to $20$\,dB. 

Figure \ref{img:time complexity} shows the results of the performed Monte Carlo experiments. In the figure, $\mu$ and $\sigma$ denote the mean and standard deviation values of the running time, respectively. The shaded region indicates the range $(\mu -\sigma, \mu + \sigma)$, i.e., 68\%-confidence range. The dark blue curve corresponds to the mean running time. The figure highlights how the computational time increases with the length of the dataset, which is essentially follows the point mentioned in Remark~\ref{rem:comp_comp} on $O(\nD^3)$ computational complexity due to the calculation of the derivatives with respect to $\mxA$.
% consistent with the $O(\nD^3)$ computational complexity discussed in Remark~\ref{rem:comp_comp}, which arises from the derivative calculations with respect to $\mxA$. 
%
% Figure \ref{img:time complexity} presents the results of the Monte Carlo experiments. In the figure, $\mu$ and $\sigma$ represent the mean and standard deviation of the running time, respectively. The shaded region corresponds to the interval $(\mu - \sigma, \mu + \sigma)$, representing a 68\%-confidence range. The red curve illustrates the mean running time. The figure highlights how computational time increases with the dataset length, consistent with the $O(\nD^3)$ computational complexity discussed in Remark~\ref{rem:comp_comp}, which arises from the derivative calculations with respect to $\mxA$.  


% It is worth highlighting that the proposed approach may lead to high computational demands, specifically for the cases of large datasets and consequently resulting in becoming impractical in real-world applications. To address this limitation, in the remainder of this paper, we introduce an alternative estimation method  based on the Expectation--Maximization (EM) algorithm, which is significantly more computational efficient and better suited for handling large datasets.
It is important to note that the proposed approach may impose high computational demands, particularly when dealing with large datasets, potentially making it impractical for real-world applications. To address this limitation, in the remainder of this paper, we introduce an alternative estimation method based on the Expectation--Maximization (EM) algorithm, which is significantly more computationally efficient and better suited for large datasets.

% \begin{figure}[t!]
%    \centering
%    \includegraphics[width = 0.5\textwidth]{plots/complexity.eps}
%    \caption{Time complexity analysis of algorithm \ref{Al:ML ID}. The red curve line is the mean running time. The red shaded region contains the interval $(\mu - \sigma, \mu + \sigma)$.}
%    \label{img:time complexity}
% \end{figure}

\begin{figure}[t!]
   \centering
   \includegraphics[width = 0.6\textwidth,trim={2cm 8.7cm 3cm 9cm},clip]{plots/complexity.pdf}
   \caption{Time complexity analysis of algorithm \ref{Al:ML ID}. The red curve line is the mean running time. The red shaded region contains the interval $(\mu - \sigma, \mu + \sigma)$.}
   \label{img:time complexity}
\end{figure}