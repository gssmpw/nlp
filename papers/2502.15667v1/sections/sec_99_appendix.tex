%===============================================================================
\appendix
% \label{sec:Appendix}
% \section{Auxiliary Lemmas}\label{app:auxiliary lemma}
% \subsection{Matrix Inversion Lemma}
% \begin{lemma}[\cite{higham2002accuracy}] 
% Given matrices $\mxA \in \Rbb^{n\times n}$, $\mxB \in \Rbb^{n\times m}$, $\mxC \in \Rbb^{m\times m}$ and $\mxD \in \Rbb^{m\times n}$. If $\mxA$ and $\mxC$ are invertible,
% \begin{equation}\label{eq:matrix inversion lemma}
%     (\mxA+\mxB\mxC\mxD)^{-1} = \mxA^{-1} -\mxA^{-1}\mxB(\mxC^{-1} + \mxD\mxA^{-1}\mxB)^{-1}\mxD\mxA^{-1}.
% \end{equation}
% \end{lemma}

% \begin{lemma}[Sherman--Morrison--Woodbury formula, \cite{higham2002accuracy}] 
% Given matrices $\mxA \in \Rbb^{n\times n}$, $\mxB \in \Rbb^{n\times m}$, $\mxC \in \Rbb^{m\times m}$ and $\mxD \in \Rbb^{m\times n}$. If $\mxA$ and $\mxC$ are invertible,
% \begin{equation}\label{eq:matrix inversion lemma}
%     (\mxA+\mxB\mxC\mxD)^{-1} = \mxA^{-1} -\mxA^{-1}\mxB(\mxC^{-1} + \mxD\mxA^{-1}\mxB)^{-1}\mxD\mxA^{-1}.
% \end{equation}
% \end{lemma}

% \subsection{Chain Rule}\label{app:chain rule}
% Consider a matrix $\mxX \in \Rbb^{n\times m}$. Let $\mxU = f(\mxX)$,
% where $f:\Rbb^{n\times m}\to\Rbb^{p \times q}$. Let $g:\Rbb^{p\times q}\to\Rbb$. The derivative of $g(\mxU)$ with respect to $\mxX$ can be obtained through
% \begin{equation}\label{eq:chain rule}
%     \pdv{g(\mxU)}{\mxX_{i,j}} = \trace\left[\left(\pdv{g(\mxU)}{\mxU}\right)^\tr \pdv{\mxU}{\mxX_{i,j}} \right].
% \end{equation}

\label{sec:Appendix}
\section{Auxiliary Lemmas}\label{app:auxiliary lemma}
%-------
\begin{lemma}[Matrix Inversion Lemma, %or, Sherman--Morrison--Woodbury Formula,
% \cite{higham2002accuracy}] 
\cite{petersen2008matrix}]
\label{lem:matrix_inversion_lemma}
Given matrices $\mxA \in \Rbb^{n\times n}$, $\mxB \in \Rbb^{n\times m}$, $\mxC \in \Rbb^{m\times m}$ and $\mxD \in \Rbb^{m\times n}$. If $\mxA$ and $\mxC$ are invertible, then $\mxA+\mxB\mxC\mxD$ has an inverse as
\begin{equation}\label{eq:matrix inversion lemma}
    (\mxA+\mxB\mxC\mxD)^{-1} = \mxA^{-1} -\mxA^{-1}\mxB(\mxC^{-1} + \mxD\mxA^{-1}\mxB)^{-1}\mxD\mxA^{-1}.
\end{equation}
\end{lemma}
%-------
\begin{lemma}[Chain Rule -- Matrix Version, \cite{petersen2008matrix}] 
\label{lem:chain_rule}
Consider a matrix $\mxX \in \Rbb^{n\times m}$. Let $\mxU = f(\mxX)$,
where $f:\Rbb^{n\times m}\to\Rbb^{p \times q}$. Let $g:\Rbb^{p\times q}\to\Rbb$. Then, for the derivative of $g(\mxU)$ with respect to $\mxX$, we have
\begin{equation}\label{eq:chain rule}
    \pdv{g(\mxU)}{\mxX_{i,j}} = \trace\left[\left(\pdv{g(\mxU)}{\mxU}\right)^\tr \pdv{\mxU}{\mxX_{i,j}} \right],
\end{equation}
for any $i=1,\ldots,n$ and $j=1,\ldots,m$.
\end{lemma}
%-------
\begin{lemma}[Multidimensional Gaussian Integral Formula, \cite{petersen2008matrix}]  
\label{lem:Gaussian_integration}
For any vector $\mu\in \mathbb{R}^{n}$ and any positive definite matrix $\mxS \in \mathbb{R}^{n \times n}$, we have
\begin{equation}\label{eq:Gaussian integration}
    % \int_{\mathbb{R}^{n}} \frac{1}{\left(2\pi\right)^{\frac{1}{2}n}|\mxS|^{\frac{1}{2}}}
    % \exp\big(-\frac{1}{2} 
    %     (\vcx - \mu)^\tr\mxS^{-1} (\vcx - \mu) 
    %     \big) 
    % \drm \vcx 
    % =
    % 1.
    \int_{\mathbb{R}^{n}} 
    \exp\big(-\frac{1}{2} 
        (\vcx - \mu)^\tr\mxS^{-1} (\vcx - \mu) 
        \big) 
    \drm \vcx 
    = 
    (2\pi)^{\frac{1}{2}n}|\mxS|^{\frac{1}{2}}.
\end{equation}
\end{lemma}
%%-------
\begin{lemma}\label{lem:invex}
    Define function $f:\Rbb_+\to \Rbb$ as    
    \begin{equation}
        f(x) := \log(x) + \frac{a}{x}, \qquad \forall\, x\in\Rbb_+,
    \end{equation}
    where $a\in\Rbb^+$. Then, the function $f$ is invex.
    %     The function $f:\Rbb_+\to \Rbb$ defined as    
    % \begin{equation}
    %     f(x) := \log(x) + \frac{a}{x}, \qquad \forall\, x\in\Rbb_+,
    % \end{equation}
    % where $a\in\Rbb^+$, is invex.
\end{lemma}
\begin{proof}
    The function $f(x)$ is radially unbounded since
    \begin{equation}
        \lim_{x \to \infty} \log(x) + \frac{a} {x} = \infty,
    \end{equation}
    and
    \begin{equation}
        \lim_{x \to 0} \log(x) + \frac{a} {x} = \infty.
    \end{equation}
    Furthermore, there is a unique stationary point at $x=a$. Thus, $f(x)$ is invex and has a lower bound $\log(a) + 1$.
\end{proof}
%%-------

\begin{lemma}\label{lem:invexity_logdet_trinv}
    Given $a,b \in \Rbb_{+}$, $\mxP\in \Sbb_{++}^{n\times n}$, 
    define function $f:\Sbb_{++}^{n \times n}\to \Rbb$ as    
    \begin{equation}
        f(\mxX) := a\logdet(\mxX) + b\trace(\mxX^{-1}\mxP), \qquad \forall\, \mxX\in\Sbb_{++}^{n\times n}.
    \end{equation}
    % Then, the function $f$ goes to infinity as $\mxX$ goes to the boundary of $\Sbb_{++}^{n \times n}$ or infinity. Moreover, it admits a unique minimizer on $\Sbb_{++}$, which implies that it is an invex function.
    % Then, the function $f$ diverges to infinity as $\mxX$ approaches the boundary of $\Sbb_{++}^{n \times n}$ or infinity. 
    Then, the function $f(\mxX)$ diverges to infinity as $\mxX$ approaches the boundary of $\Sbb_{++}^{n \times n}$ or infinity. 
    Moreover, $f$ admits a unique minimizer in $\Sbb_{++}^{n \times n}$, which implies that $f$ is an invex function on $\Sbb_{++}^{n \times n}$.    
\end{lemma}
\begin{proof}
    Since $\mxP$ is positive definite, there exists positive scalar $\varepsilon$, such that $\mxP > \varepsilon^2 \mathbf{I}$. Thus, we have
    \begin{equation}
        \trace(\mxX^{-1} \mxP) = \trace(\mxP^{\frac{1}{2}} \mxX^{-1} \mxP^{\frac{1}{2}}) \geq \varepsilon^2 \trace(\mxX^{-1}),
    \end{equation}
    which implies that
    \begin{equation}\label{eqn:f(X)_lower_bound}
    \begin{split}
        f(\mxX) &\geq a \log\det \mxX + \varepsilon^2 b \trace(\mxX^{-1})
        \\ &
        = \sum_{i=1}^n \left( a \log \lambda_i(\mxX) + \frac{b \varepsilon^2}{\lambda_i(\mxX)} \right),
    \end{split}
    \end{equation}
    for any $\mxX$.
    % As $\mxX$ approaches the boundary of $\Sbb_{++}^{n \times n}$ or infinity, its eigenvalues go to zero or infinity, and consequently, due to Lemma~\ref{lem:invex}, we know that $f$ diverges to infinity.
    % As $\mxX$ approaches the boundary of $\Sbb_{++}^{n \times n}$, the smallest eigenvalue of $\mxX$ goes to zero, and consequently, due to Lemma~\ref{lem:invex} and \eqref{eqn:f(X)_lower_bound}, we know that $f$ diverges to infinity. Similarly, when $\mxX$ goes to infinity, its largest eigenvalue diverges to infinity as well, which implies that $f(\mxX)$ goes to infinity according to Lemma~\ref{lem:invex} and \eqref{eqn:f(X)_lower_bound}.    
    % As $\mxX$ approaches the boundary of $\Sbb_{++}^{n \times n}$, its smallest eigenvalue tends to zero. Consequently, by Lemma~\ref{lem:invex} and \eqref{eqn:f(X)_lower_bound}, $f(\mxX)$ diverges to infinity. Similarly, as $\mxX$ grows unbounded, its largest eigenvalue diverges to infinity, which, again by Lemma~\ref{lem:invex} and \eqref{eqn:f(X)_lower_bound}, implies that $f(\mxX)$ also diverges to infinity.
    % As $\mxX$ approaches the boundary of $\Sbb_{++}^{n \times n}$, the smallest eigenvalue of $\mxX$ tends to zero. Consequently, by Lemma~\ref{lem:invex} and \eqref{eqn:f(X)_lower_bound}, $f(\mxX)$ diverges to infinity. Similarly, as $\mxX$ becomes unbounded, its largest eigenvalue diverges to infinity. Applying Lemma~\ref{lem:invex} and \eqref{eqn:f(X)_lower_bound} again, we conclude that $f(\mxX)$ also diverges to infinity.
    As $\mxX$ approaches the boundary of $\Sbb_{++}^{n \times n}$, the smallest eigenvalue of $\mxX$ goes to zero. Consequently, by Lemma~\ref{lem:invex} and \eqref{eqn:f(X)_lower_bound}, $f(\mxX)$ diverges to infinity. Similarly, as $\mxX$ grows unbounded, its largest eigenvalue diverges to infinity. Applying Lemma~\ref{lem:invex} and \eqref{eqn:f(X)_lower_bound} again, we conclude that $f(\mxX)$ also diverges to infinity.
    Now, we show $f$ has a unique minimizer in $\Sbb_{++}^{n \times n}$.  To find all the local minimum, we use the first-order necessary condition for optimality. Accordingly, we obtain
    \begin{equation}
        \begin{split}
            \pdv{f(\mxX)}{\mxX} &= a \mxX^{-1} - b\varepsilon^2 \mxX^{-1}\mxP\mxX^{-1} = 0,
        \end{split}
    \end{equation}
    which implies
    \begin{equation}
        \mxX = \frac{b\varepsilon^2}{a} \mxP^{-1}.
    \end{equation}
    Thus, $f$ has only one stationary point, and, due to the discussion above,  it needs to be the unique minimizer of $f$. Therefore, $f$ is an invex function on $\Sbb_{++}^{n\times n}$, which concludes the proof. 
    %
    % Since $\mxX$ is positive definite, we can decompose it as $\mxX = \mxV^\tr \Lambda \mxV$. Then, we have
    % \begin{equation}
    % \begin{split}
    %     f(\mxX) &= a \log\det(\Lambda) + b \trace(\mxV^\tr \Lambda^{-1} \mxV \mxP)
    %     \\ &
    %     = a \log\det(\Lambda) + b \trace(\mxV^\tr \Lambda^{-1} \mxV \mxP)
    %     \\ &
    %     = a \left( \sum_{i=1}^{n} \log\lambda_i \right) + b \trace(\Lambda^{-1} \mxV \mxP \mxV^\tr),
    % \end{split}
    % \end{equation}
    % where $\lambda_1\geq\cdots\geq\lambda_n$ are the eigenvalues of $\mxX$. 
    % Define $\mxQ := \mxV^\tr \mxP \mxV$. Since $\mxP$ is positive definite, we have 
    % \begin{equation}
    %     [\mxQ]_{i,i} = [\mxV^\tr]_i^\tr \mxP [\mxV^\tr]_i > 0.  
    % \end{equation}
    % Thus, $f(\mxX)$ can be rewritten as 
    % \begin{equation}
    %     f(\mxX) = \sum_{i=1}^n \left(a\log \lambda_i + b \frac{[\mxQ]_{i,i}}{\lambda_i}\right)
    % \end{equation}.
\end{proof}

\begin{lemma}\label{lem:PD}
    For any positive definite matrices $\mxP, \mxQ  \in \Sbb_{++}^{n\times n}$ and any matrices $\mxR,\mxX\in\Rbb^{n\times n}$, we have 
    \begin{equation}
        % \begin{bmatrix}
            (\mathbf{I} - \mxX \mxR)
        % \end{bmatrix}
        \mxP 
        % \begin{bmatrix}
            (\mathbf{I} - \mxX \mxR)
        % \end{bmatrix}
        ^\tr
        +
        \mxX\mxQ\mxX^\tr
        \succ 0.
    \end{equation}
    Moreover, one can show that the lower bound is 
\begin{equation}
    (\mxP^{-1} + \mxR^\tr \mxQ^{-1}\mxR)^{-1}.
\end{equation}
\end{lemma}

\begin{lemma}\label{lem:PD for summarion u_t}
    Let Assumption~\ref{assum:input} holds. Then, the following matrix 
    \begin{equation}
        \sum_{t=0}^{\nD-1}
        \begin{bmatrix}
            1 \\ \vcu_t
        \end{bmatrix}
        \begin{bmatrix}
            1 \\ \vcu_t
        \end{bmatrix}^\tr
    \end{equation}
    is positive definite.
\end{lemma}
\begin{proof}
    We prove this by contradiction. Assume there exists a non zero vector $\vcz$, such that
    \begin{equation}
        \vcz
        \left(
        \sum_{t=0}^{\nD-1}
        \begin{bmatrix}
            1 \\ \vcu_t
        \end{bmatrix}
        \begin{bmatrix}
            1 \\ \vcu_t
        \end{bmatrix}^\tr
        \right)
        \vcz = 0.
    \end{equation}
    The above equation implies that
    \begin{equation}
        \begin{bmatrix}
        1 \\ \vcu_t
        \end{bmatrix}^\tr
        \vcz = 0, \quad \forall t \in 0,1,\cdots,\nD-1.
    \end{equation}
    Thus, we have
    \begin{equation}
        \begin{bmatrix}
            1 & 1 & \cdots & 1\\
            \vcu_0 & \vcu_1 &\cdots & \vcu_{\nD-1}
        \end{bmatrix}^\tr
        \vcz = 0.
    \end{equation}
    From Assumption~\ref{assum:input} the transposed matrix in the above equation is full column rank, which implies $\vcz = 0$. Since we assume $\vcz$ is a non zero vector, we arrive at a contradiction, which concludes the proof.
\end{proof}

\begin{lemma}\label{lem:PD for summarion y_t}
    Let Assumption~\ref{assum:output} holds. Then, the matrices 
    \begin{equation}
        \sum_{t=0}^{\nD-1}
        \begin{bmatrix}
            \vcu_t \\ \vcy_t
        \end{bmatrix}
        \begin{bmatrix}
            \vcu_t \\ \vcy_t
        \end{bmatrix}^\tr,
    \end{equation}
    and
    \begin{equation}
        \sum_{t=0}^{\nD-1}
            \vcy_t \vcy_t^\tr
    \end{equation}
    are positive definite.
\end{lemma}
\begin{proof}
    The proof is similar to Lemma~\ref{lem:PD for summarion u_t}.
\end{proof}


\begin{lemma}\label{lem:PD_phi_psi}
Let Assumption~\ref{assum:input} holds. Define matrices $\Phi$ and $\Psi$ as
\begin{equation}
    \Phi :=    \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} \left[ \sum_{t=0}^{\nD - 1} \left(\begin{bmatrix}
        \vcx_t \\ \vcu_t
    \end{bmatrix}
    \begin{bmatrix}
        \vcx_t  \\ \vcu_t
    \end{bmatrix}^\tr \right)\right],
\end{equation}
and
\begin{equation}
    \Psi :=    \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} \left[ \sum_{t=0}^{\nD - 1} \left(\begin{bmatrix}
        \vcx_t \\ \vcu_t \otimes \vcx_t \\ \vcu_t
    \end{bmatrix}
    \begin{bmatrix}
        \vcx_t \\ \vcu_t \otimes \vcx_t \\ \vcu_t
    \end{bmatrix}^\tr \right)\right],
\end{equation}
respectively. Then $\Phi$ and $\Psi$ are positive definite. % and thus invertible.
\end{lemma}
\begin{proof}
    We show that $\Psi$ is positive definite, which implies that $\Phi$ is positive definite by Schur complement. 
    To show that $\Psi$ is positive definite, we use contradiction. Assume there exists a non zero vector $\vcq = \begin{bmatrix}
        \vcq_1^\tr & \vcq_2^\tr & \vcq_3^\tr
    \end{bmatrix}$
    such that $\vcq^\tr\Psi\vcq = 0$, where $\vcq \in \Rbb^{\Nx+\Nu+\Nx\Nu}$, $\vcq_1 \in \Rbb^{\Nx}$, $\vcq_2 \in \Rbb^{\Nx\Nu}$, and $\vcq_3 \in \Rbb^{\Nu}$. Then, we have
    \begin{equation}\label{eq:Psi=0}
        \sum_{t=0}^{\nD-1} \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} 
        \left[ \vcq^\tr  \begin{bmatrix}
        \vcx_t \\ \vcu_t \otimes \vcx_t \\ \vcu_t
    \end{bmatrix}
    \begin{bmatrix}
        \vcx_t \\ \vcu_t \otimes \vcx_t \\ \vcu_t
    \end{bmatrix}^\tr \vcq  \right] = 0.
    \end{equation}
    Define 
    \begin{equation}
        \xi_t := \begin{bmatrix}
        \vcx_t \\ \vcu_t \otimes \vcx_t \\ \vcu_t
        \end{bmatrix}^\tr \vcq.
    \end{equation} 
    Notice that $\xi_t$ is a scalar. Thus, from \eqref{eq:Psi=0}, we have $\mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} 
        \left[\xi_t^2 \right] = 0, \forall t = 1,\cdots,\nD-1$,
    which implies that 
    \begin{equation}\label{eq:mu_xi_t}
        \mu_{\xi_t} := \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} 
        \left[\xi_t \right] = 0, \quad \forall t = 1,\cdots,\nD-1,
    \end{equation}
    and
    \begin{equation}\label{eq:var_xi_t}
        \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} 
        \left[(\xi_t - \mu_{\xi_t})^2 \right] = 0, \quad \forall t = 1,\cdots,\nD-1.
    \end{equation}
On the one hand, \eqref{eq:var_xi_t} implies that
\begin{equation}
    \sum_{t=0}^{\nD-1}
    \begin{bmatrix}
        \vcq_1^\tr & \vcq_2^\tr
    \end{bmatrix}
    \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})}
    \left[ 
    \left(
    \begin{bmatrix}
        \vcx_t \\ \vcu_t \otimes \vcx_t
    \end{bmatrix}
    -
    \begin{bmatrix}
        \hat{\vcx}_{t|\nD} \\ \vcu_t \otimes \hat{\vcx}_{t|\nD}
    \end{bmatrix}
    \right)
        \left(
    \begin{bmatrix}
        \vcx_t \\ \vcu_t \otimes \vcx_t
    \end{bmatrix}
    -
    \begin{bmatrix}
        \hat{\vcx}_{t|\nD} \\ \vcu_t \otimes \hat{\vcx}_{t|\nD}
    \end{bmatrix}
    \right)^\tr
    \right]
    \begin{bmatrix}
        \vcq_1 \\ \vcq_2
    \end{bmatrix}
    = 0,
\end{equation}
which is equivalent to
\begin{equation}\label{eq:q_1_q_2}
    \begin{bmatrix}
        \vcq_1^\tr & \vcq_2^\tr
    \end{bmatrix}
    \left[
    \sum_{t=0}^{\nD-1}
    \left(
    \mxP_{t|\nD}
    \otimes
    \begin{bmatrix}
         1 \\ \vcu_t   
    \end{bmatrix}
    \begin{bmatrix}
         1 \\ \vcu_t   
    \end{bmatrix}^\tr
    \right)
    \right]
    \begin{bmatrix}
        \vcq_1 \\ \vcq_2
    \end{bmatrix}
    = 0.
\end{equation}
From Lemma~\ref{lem:PD for summarion u_t} and \eqref{eq:P_t|nD}, we know $\sum_{t=0}^{\nD-1}\left(\begin{bmatrix}
         1 \\ \vcu_t   
    \end{bmatrix}
    \begin{bmatrix}
         1 \\ \vcu_t   
    \end{bmatrix}^\tr \right)$ and $\mxP_{t|\nD}$ are positive definite. Thus, \eqref{eq:q_1_q_2} holds if and only if $\vcq_1 = 0$ and $\vcq_2=0$. On the other hand, \eqref{eq:mu_xi_t} implies that
    \begin{equation}
        \hat{x}_{t|\nD}^\tr \vcq_1 + \left(\vcu_t^\tr \otimes \hat{\vcx}_{t|\nD}^\tr \right) \vcq_2 + \vcu_t^\tr \vcq_3 = 0, \quad \forall t = 1,\cdots,\nD-1,
    \end{equation}
    which implies that 
    \begin{equation}
        \begin{bmatrix}
            \vcu_0^\tr \\ \vcu_1^\tr \\ \cdots \\ \vcu_{\nD-1}^\tr
        \end{bmatrix}\vcq_3 = 0.
    \end{equation}
    % \begin{equation}
    %     \begin{bmatrix}
    %         \vcu_0 & \vcu_1 & \cdots & \vcu_{\nD-1}
    %     \end{bmatrix}^\tr\vcq_3 = 0.
    % \end{equation}
    From Assumption~\ref{assum:input}, the above equation implies that $\vcq_3 = 0$. Thus, we arrive at $\vcq = 0$, which contradicts the assumption that $\vcq$ is a non zero vector. Therefore, $\Psi$ is positive definite and thus invertible, which concludes the proof.
\end{proof}
%========================================================
%========================================================
%========================================================
\section{Proof of Proposition 1} \label{app:proof 1}
One can see that the integrand in \eqref{eq: likelihoog int} is the exponential of a quadratic function with respect to $\mathbf{x}$. 
Accordingly, we can employ Lemma~\ref{lem:Gaussian_integration} to evaluate and simplify the integral.
To this end, we define the matrix $\Sqtheta$ and the vector $\vcq(\vctheta)$ as
\begin{equation}\label{eqn:Sq}
    \Sqtheta 
    := 
    \Big(
    \Xi(\vctheta)^\tr\,
    %\big(\Sbfytheta\big)^{-1}\, 
    % \mxS^{(\mathbf{y})^{-1}}\!(\vctheta)\, 
    % \Xi(\vctheta) 
    \Sbfytheta^{-1}\, 
    \Xi(\vctheta)
    + 
    %\big(\Sbfxtheta\big)^{-1}
    \Sbfxtheta^{-1}
    \Big)^{-1}\!,
    %
    %
    % \qquad
    % \cmb{\Xi(\vctheta)^\tr \Sbfytheta^{-1} \Xi(\vctheta) + \Sbfxtheta^{-1}}
\end{equation}
and 
\begin{equation}\label{eq:q function}
\begin{split}
    \vcq(\vctheta)
    &:= 
    -\,
    \Sqtheta
    \Big[
    \Xi(\vctheta)^\tr
    %\big(\Sbfytheta\big)^{-1}\, 
    \Sbfytheta^{-1}\,
    ( \mathbf{D}(\vctheta)\mathbf{u}-\mathbf{y})
    -
    %\big(\Sbfxtheta\big)^{-1}
    \Sbfxtheta^{-1}
    \mubfxtheta
    \Big],
\end{split}
\end{equation}
respectively.
%The integration in \eqref{eq: likelihoog int} is an exponential of a quadratic function with respect to $\mathbf{x}$.
By expanding the exponent term in \eqref{eq: likelihoog int}, we have
\begin{align}
    \big( \mathbf{y}-\mubfythetax & \big)^\tr
    \Sbfytheta^{-1}
    \big( \mathbf{y}-\mubfythetax \big) 
    + 
    \big( \mathbf{x}-\mubfxtheta \big)^\tr
    \Sbfxtheta^{-1} 
    \big( \mathbf{x}-\mubfxtheta \big)
    \nonumber\\
    =& \, 
    \big\| \Xi(\vctheta) \mathbf{x} + \mathbf{D}(\vctheta) \mathbf{u} -  \mathbf{y} \big\|^2_{\Sbfytheta^{-1}} 
    + 
    \big\| \mathbf{x} - \mubfxtheta \big\|^2_{\Sbfxtheta^{-1}} 
    \label{eq: S_q Gaussian}\\
    =& \, 
    \big\| \mathbf{x} - \vcq(\vctheta) \big\|^2_{\Sqtheta^{-1}} 
    - 
    \vcq(\vctheta)^\tr \Sqtheta^{-1} \vcq(\vctheta) 
    + 
    \big(\mathbf{D}(\vctheta)\mathbf{u} - \mathbf{y}\big)^\tr
    \Sbfytheta^{-1}
    \big(\mathbf{D}(\vctheta)\mathbf{u} - \mathbf{y}\big) 
    + 
    \mubfxtheta^\tr\Sbfxtheta^{-1}\mubfxtheta.
    \nonumber
\end{align}
% One can see that the integration in \eqref{eq: likelihoog int} is an exponential of a quadratic function with respect to $\mathbf{x}$. Accordingly, we can employ Lemma \ref{lem:Gaussian_integration} to obtain the value of integrals.
Note that only the first term in the last line of \eqref{eq: S_q Gaussian} depends on $\mathbf{x}$.
Thus, substituting \eqref{eq: S_q Gaussian} into \eqref{eq: likelihoog int} and using the property \eqref{eq:Gaussian integration} introduced in Lemma~\ref{lem:Gaussian_integration}, we can simplify the likelihood function as
\begin{equation}\label{eq:likelihood integrated}
\begin{split}
     p(\mathbf{y}|\vctheta, \mathbf{u}) 
     = 
     (2\pi)^{-\frac{1}{2}\Ny\nsD}\,
     \frac{ |\Sqtheta|^{\frac{1}{2}}}
           {|\Sbfxtheta|^{\frac{1}{2}} |\Sbfytheta|^{\frac{1}{2}}} 
    \exp \bigg( &\frac{1}{2} \Big[ 
        \vcq(\vctheta)^\tr \Sqtheta^{-1} \vcq(\vctheta) 
        - 
        \big(\mathbf{D}(\vctheta)\mathbf{u} - \mathbf{y}\big)^\tr
        \Sbfytheta^{-1}
        \big(\mathbf{D}(\vctheta)\mathbf{u} - \mathbf{y}\big) 
     \\&\qquad \qquad \qquad \qquad \qquad \qquad \quad 
     - \mubfxtheta^\tr \Sbfxtheta^{-1} \mubfxtheta
     \Big] \bigg).
\end{split}
\end{equation}
For further simplification, we define the vector $\vcz(\vctheta)$ and the matrix $\mxF(\vctheta)$ as
\begin{align}\label{eqn:z_theta}
    \vcz(\vctheta) := \mathbf{D}(\vctheta)\mathbf{u} - \mathbf{y},
\end{align}
and
\begin{align}
    \mxF(\vctheta) := \Xi(\vctheta)\Sbfxtheta\Xi(\vctheta)^\tr + \Sbfytheta,
\end{align}
respectively.
Using Lemma \eqref{eq:matrix inversion lemma}, i.e., 
\emph{Matrix Inversion Lemma} or \emph{Sherman--Morrison--Woodbury Formula},  
we have
\begin{align}
        \mxF(\vctheta) ^{-1}
        &= 
        \big(\Sbfytheta + \Xi(\vctheta)\Sbfxtheta\Xi(\vctheta)^\tr\big)^{-1} 
        \nonumber\\
        &= 
        \Sbfytheta^{-1} -\Sbfytheta^{-1}\Xi(\vctheta)
        \big(\Sbfxtheta^{-1} + \Xi(\vctheta)^\tr \Sbfytheta^{-1} \Xi(\vctheta)\big)^{-1}
        \Xi(\vctheta)^\tr\Sbfytheta^{-1} 
        \label{eq:F-1}\\
        &=
        \Sbfytheta^{-1} -\Sbfytheta^{-1}\Xi(\vctheta)
        \Sqtheta
        \Xi(\vctheta)^\tr\Sbfytheta^{-1},
        \nonumber
\end{align}
where the last equality is due to  the definition of matrix $\Sqtheta$ given in \eqref{eqn:Sq}.
Similarly, one can see that
\begin{align}        
        \Sqtheta 
        % &= 
        % \big(\Sbfxtheta^{-1} + \Xi(\vctheta)^\tr\Sbfytheta^{-1}\Xi(\vctheta)\big)^{-1} 
        % \nonumber\\
        &\,= 
        \Sbfxtheta - 
        \Sbfxtheta\Xi(\vctheta)^\tr
        \big(\Sbfytheta + \Xi(\vctheta) \Sbfxtheta \Xi(\vctheta)^\tr\big)^{-1} \Xi(\vctheta)\Sbfxtheta,
        \label{eq:Sq-1}
\end{align}
%
%
%
% Only the first term in the last line is related to $\mathbf{x}$. Substituting \eqref{eq: S_q Gaussian} to \eqref{eq: likelihoog int} and using the property \eqref{eq:Gaussian integration}, the likelihood function can be derived as
% \begin{equation}\label{eq:likelihood integrated}
% \begin{split}
%      p(\mathbf{y}|\vctheta, \mathbf{u}) = \frac{k \, |\Sqtheta|^{\frac{1}{2}}}{|\Sbfxtheta|^{\frac{1}{2}}|\Sbfytheta|^{\frac{1}{2}}} \exp \big[ &\frac{1}{2} \big[ \vcq(\vctheta)^\tr\Sqtheta^{-1}\vcq(\vctheta) - (\mathbf{D}(\vctheta)\mathbf{u}-\mathbf{y})^\tr\Sbfytheta^{-1}(\mathbf{D}(\vctheta)\mathbf{u}-\mathbf{y}) 
%      %\\&\qquad \qquad \qquad \qquad \qquad \qquad \quad - 
%      \mubfxtheta^\tr\Sbfxtheta^{-1}\mubfxtheta\big] \big],
% \end{split}
% \end{equation}
% where $k$ is a constant. For further simplification, in the following, we use the matrix inversion lemma \eqref{eq:matrix inversion lemma},
% \begin{subequations}
%     \begin{align}
%         \mxF(\vctheta)^{-1} 
%         &= 
%         (\Sbfytheta + \Xi(\vctheta)\Sbfxtheta\Xi(\vctheta)^\tr)^{-1} \nonumber\\
%         &= 
%         \Sbfytheta^{-1} -\Sbfytheta^{-1}\Xi(\vctheta)
%         (\Sbfxtheta^{-1} + \Xi(\vctheta)^\tr \Sbfytheta^{-1} \Xi(\vctheta))^{-1}
%         \Xi(\vctheta)^\tr\Sbfytheta^{-1} \nonumber\\
%         &=
%         \Sbfytheta^{-1} -\Sbfytheta^{-1}\Xi(\vctheta)
%         \Sqtheta
%         \Xi(\vctheta)^\tr\Sbfytheta^{-1}
%         \label{eq:F-1} \\
%         \Sqtheta &= (\Sbfxtheta^{-1} + \Xi(\vctheta)^\tr\Sbfytheta^{-1}\Xi(\vctheta))^{-1} \nonumber\\
%         &= \Sbfxtheta -\Sbfxtheta\Xi(\vctheta)^\tr(\Sbfytheta + \Xi(\vctheta) \Sbfxtheta \Xi(\vctheta)^\tr)^{-1}\Xi(\vctheta)\Sbfxtheta,\label{eq:Sq-1}
%     \end{align}
% \end{subequations}
% where $\mxF(\vctheta) = \Xi(\vctheta)\Sbfxtheta\Xi(\vctheta)^\tr + \Sbfytheta$. Define $\vcz(\vctheta) := \mathbf{D}(\vctheta)\mathbf{u} - \mathbf{y}$. 
%
%
% Considering the introduced definitions, for the exponent in \eqref{eq:likelihood integrated}, we first substitute 
% Using \eqref{eq:q function} to further simplify the exponent in \eqref{eq:likelihood integrated},  we substitute $\vcq(\vctheta)$ with the right-hand side of \eqref{eq:q function} and obtain
% Using \eqref{eq:q function}, to simplify the exponent in \eqref{eq:likelihood integrated}, we replace $\vcq(\vctheta)$ with the right-hand side of \eqref{eq:q function} and obtain
We employ \eqref{eq:q function} and \eqref{eqn:z_theta} to simplify the exponent in \eqref{eq:likelihood integrated}, thereby obtaining
% \begin{equation}\label{eq:exponent}
%     \begin{split}
\begin{align}
    % line 1
        \vcq(\vctheta)^\tr\Sqtheta^{-1}&\vcq(\vctheta)\, - \, (\mathbf{D}(\vctheta)\mathbf{u}-\mathbf{y})^\tr\Sbfytheta^{-1}(\mathbf{D}(\vctheta)\mathbf{u}-\mathbf{y}) - \mubfxtheta^\tr\Sbfxtheta^{-1}\mubfxtheta
        \\
    % line 2
        =\,& 
        \big[ \vcz(\vctheta)^\tr\Sbfytheta^{-1}\Xi(\vctheta) - \mubfxtheta^\tr\Sbfxtheta^{-1}\big]
        \, \Sqtheta\, 
        \big[ \vcz(\vctheta)^\tr\Sbfytheta^{-1}\Xi(\vctheta) - \mubfxtheta^\tr\Sbfxtheta^{-1} \big]^\tr
        \nonumber
        \\&
        \qquad\qquad\qquad
        - \vcz(\vctheta)^\tr\Sbfytheta^{-1}\vcz(\vctheta) - \mubfxtheta^\tr\Sbfxtheta^{-1}\mubfxtheta.
\end{align}
By expanding the above equation further and rearranging the terms, we arrive at
\begin{align}
    % line 1
        \vcq(\vctheta)^\tr\Sqtheta^{-1}&\vcq(\vctheta)\, - \, (\mathbf{D}(\vctheta)\mathbf{u}-\mathbf{y})^\tr\Sbfytheta^{-1}(\mathbf{D}(\vctheta)\mathbf{u}-\mathbf{y}) - \mubfxtheta^\tr\Sbfxtheta^{-1}\mubfxtheta
        %\nonumber
        \\
   % line 3
        =\,& 
        \big[ 
            \vcz(\vctheta)^\tr \Sbfytheta^{-1} \Xi(\vctheta) \Sqtheta 
            - 
            \mubfxtheta^\tr\Sbfxtheta^{-1}\Sqtheta 
        \big]
        \big[\Xi(\vctheta)^\tr\Sbfytheta^{-1}\vcz(\vctheta) \big] 
        \nonumber\\&\qquad
        -
        \big[ 
            \vcz(\vctheta)^\tr \Sbfytheta^{-1} \Xi(\vctheta) \Sqtheta 
            - 
            \mubfxtheta^\tr\Sbfxtheta^{-1}\Sqtheta
        \big]
        \big[ \Sbfxtheta^{-1} \mubfxtheta \big] 
        \nonumber
        \\&\qquad
        - 
        \vcz(\vctheta)^\tr \Sbfytheta^{-1} \vcz(\vctheta) 
        - 
        \mubfxtheta^\tr \Sbfxtheta^{-1} \mubfxtheta
        \\
   % line 4
        =\,& 
        \vcz(\vctheta)^\tr\Sbfytheta^{-1}\Xi(\vctheta)\Sqtheta \Xi(\vctheta)^\tr\Sbfytheta^{-1}\vcz(\vctheta)  
        - 
        \mubfxtheta^\tr\Sbfxtheta^{-1}\Sqtheta \Xi(\vctheta)^\tr\Sbfytheta^{-1}\vcz(\vctheta) 
        \nonumber\\&\qquad
        -
        \vcz(\vctheta)^\tr\Sbfytheta^{-1}\Xi(\vctheta)\Sqtheta \Sbfxtheta^{-1}\mubfxtheta 
        + 
        \mubfxtheta^\tr\Sbfxtheta^{-1}\Sqtheta  \Sbfxtheta^{-1}\mubfxtheta
        \nonumber\\&\qquad
        - \vcz(\vctheta)^\tr\Sbfytheta^{-1}\vcz(\vctheta) - \mubfxtheta^\tr\Sbfxtheta^{-1}\mubfxtheta
        \\
   % line 5
       =\,& 
       \vcz(\vctheta)^\tr\Sbfytheta^{-1}\Xi(\vctheta)\mxS_{\vcq}(\vctheta)\Xi(\vctheta)^\tr\Sbfytheta^{-1}\vcz(\vctheta) + \mubfxtheta^\tr\Sbfxtheta^{-1}\mxS_{\vcq}(\vctheta)\Sbfxtheta^{-1}\mubfxtheta 
       \nonumber\\&\qquad
       - 2\vcz(\vctheta)^\tr\Sbfytheta^{-1}\Xi(\vctheta)\Sqtheta\Sbfxtheta^{-1}\mubfxtheta - \vcz(\vctheta)^\tr\Sbfytheta^{-1}\vcz(\vctheta) - \mubfxtheta^\tr\Sbfxtheta^{-1}\mubfxtheta.
\end{align}
% Finally, we can simplify the above equation using \eqref{eq:F-1} and \eqref{eq:Sq-1}, the above equation can written as
Additionally, by applying \eqref{eq:F-1} and \eqref{eq:Sq-1}, the above equation reduces to
\begin{align}
    % line 1
        \vcq(\vctheta)^\tr\Sqtheta^{-1}&\vcq(\vctheta)\, - \, (\mathbf{D}(\vctheta)\mathbf{u}-\mathbf{y})^\tr\Sbfytheta^{-1}(\mathbf{D}(\vctheta)\mathbf{u}-\mathbf{y}) - \mubfxtheta^\tr\Sbfxtheta^{-1}\mubfxtheta
        %\nonumber
        \\
   % line 6
        = \,& -\vcz(\vctheta)^\tr\mxF(\vctheta)^{-1}\vcz(\vctheta) + \mubfxtheta^\tr\Sbfxtheta^{-1}\mxS_{\vcq}(\vctheta)\Sbfxtheta^{-1}\mubfxtheta
        \nonumber\\&\qquad
        - 2\vcz(\vctheta)^\tr\Sbfytheta^{-1}\Xi(\vctheta)\Sqtheta\Sbfxtheta^{-1}\mubfxtheta - \mubfxtheta^\tr\Sbfxtheta^{-1}\mubfxtheta
        \\
    % line 7
        =\,& 
        -\vcz(\vctheta)^\tr\mxF(\vctheta)^{-1}\vcz(\vctheta) - \mubfxtheta^\tr\Xi(\vctheta)^\tr\mxF(\vctheta)^{-1}\Xi(\vctheta)\mubfxtheta - 2\vcz(\vctheta)^\tr\Sbfytheta^{-1}\Xi(\vctheta)\Sqtheta\Sbfxtheta^{-1}\mubfxtheta,
        \label{eq:exponent}
        %\\
\end{align}        
%     \end{split}
% \end{equation}
%
% On the other hand, one can show that the last term in the last line of the above equation is equivalent to $2\vcz(\vctheta)^\tr\mxF(\vctheta)^{-1}\Xi(\vctheta)\mubfxtheta$. More precisely, we have
On the other hand, it can be shown that the last term in the final line of the above equation is equivalent to $2\vcz(\vctheta)^\tr\mxF(\vctheta)^{-1}\Xi(\vctheta)\mubfxtheta$. More precisely, we have
% \begin{equation}
% \begin{split}
\begin{align}
    \vcz(\vctheta)^\tr\Sbfytheta^{-1}
    &
    \Xi(\vctheta) \Sqtheta^{-1} \Sbfxtheta^{-1} \mubfxtheta 
    - 
    \vcz(\vctheta)^\tr \mxF(\vctheta)^{-1} \Xi(\vctheta) \mubfxtheta
    %\nonumber
    \\
    =\,& 
    \vcz(\vctheta)^\tr 
    \Sbfytheta^{-1} \Xi(\vctheta) \Sqtheta^{-1}\Sbfxtheta^{-1}
    \mubfxtheta 
    - 
    \vcz(\vctheta)^\tr
    \Sbfytheta^{-1}
    \Xi(\vctheta)
    \mubfxtheta
    \nonumber
    \\&
    \qquad
    + \vcz(\vctheta)^\tr\Sbfytheta^{-1}\Xi(\vctheta)\Sqtheta^{-1}\Xi(\vctheta)^\tr\Sbfytheta^{-1}\Xi(\vctheta)\mubfxtheta\\
    =\,& 
    \vcz(\vctheta)^\tr\Sbfytheta^{-1}\Xi(\vctheta)\Sqtheta^{-1}(\Sbfxtheta^{-1} + \Xi(\vctheta)^\tr\Sbfytheta^{-1}\Xi(\vctheta))\mubfxtheta - \vcz(\vctheta)^\tr\Sbfytheta^{-1}\Xi(\vctheta)\mubfxtheta
    \\
    =\,& 
    \vcz(\vctheta)^\tr\Sbfytheta^{-1}\Xi(\vctheta)\mubfxtheta -\vcz(\vctheta)^\tr\Sbfytheta^{-1}\Xi(\vctheta)\mubfxtheta
    \\
    =\,& 0,
% \end{split}
% \end{equation}
\end{align}
where the first equality is due to \eqref{eq:F-1}. Thus, \eqref{eq:exponent} can be furthered simplified to 
\begin{equation}\label{eq: simlified exp}
    \begin{split}
        -\,
        \vcz(\vctheta)^\tr \mxF(\vctheta)^{-1} \vcz(\vctheta) 
        \,-\, 
        &
        \mubfxtheta^\tr\Xi(\vctheta)^\tr \mxF(\vctheta)^{-1}  \Xi(\vctheta) \mubfxtheta 
        - 
        2 \vcz(\vctheta)^\tr \mxF(\vctheta)^{-1} \Xi(\vctheta) \mubfxtheta
        \\
        & = 
        -
        \big(\vcz(\vctheta)^\tr + \mubfxtheta^\tr\Xi(\vctheta)^\tr\big)
        \mxF(\vctheta)^{-1}
        \big(\vcz(\vctheta) + \Xi(\vctheta)\mubfxtheta\big).
    \end{split}
\end{equation}
Substituting \eqref{eq: simlified exp} into \eqref{eq:likelihood integrated} and applying the logarithm, we obtain the log-likelihood function as
\begin{equation}
\begin{split}
    \log\, p(\mathbf{y}|\vctheta,\mathbf{u}) 
    &= 
    \frac{1}{2} 
    \Big[
        \logdet\big(\Sqtheta\big)
        - 
        \logdet\big(\Sbfxtheta\big)
        - 
        \logdet\big(\Sbfytheta\big) 
    \\&
    \qquad \qquad \qquad 
        - 
        \big(\vcz(\vctheta)^\tr + \mubfxtheta^\tr\Xi(\vctheta)^\tr\big)
        \mxF(\vctheta)^{-1}
        \big(\vcz(\vctheta) + \Xi(\vctheta)\mubfxtheta\big)
    \Big] + \frac12 \Ny\nD\log(2\pi).
\end{split}
\end{equation}
Thus, due to definition of matrix $\mxF(\vctheta)$ and the strictly increasing monotonicity of the logarithm function, 
the maximum likelihood problem \eqref{eqn:ML_generic}, which entails maximizing \eqref{eq: likelihoog int} over $\Theta$, is equivalent to minimizing the function $J$, as defined in \eqref{eq:cost function ML}, over $\Theta$, i.e., the optimization problem \eqref{eqn:min_J}.
This concludes the proof.
\qed
%========================================================
%========================================================
%========================================================
\section{Proof of Proposition 2}\label{app:proof PD of Expectation}
% To find the expectation, we need the value of following expectations 
% \begin{align}
% \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})}[\vcx_t|\mathbf{y}, \hat{\vctheta}_k] &= \hat{\vcx}_{t|\nD}, \label{eq:expectation 1}\\
% \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})}[\vcx_t \vcx_t^\tr|\mathbf{y}, \hat{\vctheta}_k] &= \hat{\vcx}_{t|\nD}\hat{\vcx}_{t|\nD}^\tr + \mxP_{t|\nD}, \label{eq:expectation 2} \\
% \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})}[\vcx_{t+1} \vcx_t^\tr|\mathbf{y}, \hat{\vctheta}_k] &= \hat{\vcx}_{t+1|{\nD}}\hat{\vcx}_{t|{\nD}}^\tr + \mxP_{t+1|{\nD}}\mxL_{t,k}^\tr, \label{eq:expectation 3}
% \end{align}
% which can be estimated using Kalman filter and RTS smoother in \eqref{eq:mean and covariance} and \eqref{eq:correlation}.
To show \eqref{eq:F_k}-\eqref{eq:H_k} are positive definite, we first expand the right half side of the equations. To this end, we use the following expectations,
\begin{align}
\mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})}[\vcx_t|\mathbf{y}, \hat{\vctheta}_k] &= \hat{\vcx}_{t|\nD}, \label{eq:expectation 1}\\
\mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})}[\vcx_t \vcx_t^\tr|\mathbf{y}, \hat{\vctheta}_k] &= \hat{\vcx}_{t|\nD}\hat{\vcx}_{t|\nD}^\tr + \mxP_{t|\nD}, \label{eq:expectation 2} \\
\mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})}[\vcx_{t+1} \vcx_t^\tr|\mathbf{y}, \hat{\vctheta}_k] &= \hat{\vcx}_{t+1|{\nD}}\hat{\vcx}_{t|{\nD}}^\tr + \mxP_{t+1|{\nD}}\mxL_{t,k}^\tr, \label{eq:expectation 3}
\end{align}
which can be estimated using RTS smoother as in \eqref{eq:mean and covariance} and \eqref{eq:correlation}.
Here, the subscript in $\mxL_{t,k}$ means the recursive updates $\mxL_t$ using the estimated parameter $\hat{\vctheta}_k$. Consider $\mxH_k(\mxM)$, using the above expectations, it can be expanded as
\begin{align}
% \begin{equation}\label{eq:H_theta|hat_theta PD 0}
% \begin{split}
    \mxH_k(\mxM) &= 
        \frac{1}{\nD} \sum_{t=0}^{\nD-1}  \Big[(\hat{\vcx}_{t+1|\nD}\hat{\vcx}_{t+1|\nD}^\tr + \mxP_{t+1|\nD}) -  \begin{bmatrix}\hat{\vcx}_{t+1|{\nD}}\hat{\vcx}_{t|{\nD}}^\tr + \mxP_{t+1|{\nD}}\mxL_{t,k}^\tr &\quad \hat{\vcx}_{t+1|\nD} \vcu_t^\tr  \end{bmatrix} {\mxM}^\tr  \nonumber \\
        &\quad -  {\mxM}\begin{bmatrix}\hat{\vcx}_{t+1|\nD}\hat{\vcx}_{t|\nD}^\tr + \mxP_{t+1|\nD}\mxL_{t,k}^\tr &\quad \hat{\vcx}_{t+1|\nD} \vcu_t^\tr  \end{bmatrix}^\tr + {\mxM} \begin{bmatrix}\hat{\vcx}_{t|\nD}\hat{\vcx}_{t|\nD}^\tr \!+\! \mxP_{t|\nD} & \hat{\vcx}_{t|\nD}\vcu_t^\tr\\ \vcu_t\hat{\vcx}_{t|\nD}^\tr &\vcu_t \vcu_t^\tr\end{bmatrix} {\mxM}^\tr \Big] \\
        %%
        &= \frac{1}{\nD} \sum_{t=0}^{\nD-1} 
        \big[ (\hat{\vcx}_{t+1|\nD} - 
        {\mxM}
        \begin{bmatrix}
            \hat{\vcx}_{t+1|\nD} \\ \vcu_t
        \end{bmatrix}
        )
        (\hat{\vcx}_{t+1|\nD} - 
        {\mxM}
        \begin{bmatrix}
            \hat{\vcx}_{t+1|\nD} \\ \vcu_t
        \end{bmatrix}
        )^\tr
        \nonumber \\
        &\quad + \mxP_{t+1|\nD} -  \mxP_{t+1|\nD}\mxL_{t,k}^\tr {\mxA}^\tr - {\mxA} \mxL_{t,k} \mxP_{t+1|\nD} + {\mxA}\mxP_{t|\nD}{\mxA}^\tr\big]. \label{eq:H_k 2}
% \end{split}
% \end{equation}
\end{align}
To show that the above equation is positive definite, we first expand $\mxP_{t|\nD}$. Using \eqref{eq:mean and covariance}, it can be written as 
% \begin{equation}\label{eq:P_t|nD}
% \begin{split}
\begin{align}
    \mxP_{t|\nD} 
    &= \mxP_{t|t} + \mxL_{t,k}(\mxP_{t+1|\nD} - \mxP_{t+1|t})\mxL_{t,k}^\tr\\
    %%
    &= \mxP_{t|t} - \mxL_{t,k}\mxP_{t+1|t}\mxL_{t,k}^\tr + \mxL_{t,k}\mxP_{t+1|\nD}\mxL_{t,k}^\tr\\
    %%
    &= \mxP_{t|t} - \mxP_{t|t}\hat{\mxA}_k^\tr\mxP_{t+1|t}^{-1}\hat{\mxA}_k\mxP_{t|t} +\mxL_{t,k}\mxP_{t+1|\nD}\mxL_{t,k}^\tr\\
    %%
    &=(\mxP_{t|t}^{-1} + \hat{\mxA}_k^\tr \hat{\mxS}_{\vcw}^{-1} \hat{\mxA}_k)^{-1} + \mxL_{t,k}\mxP_{t+1|\nD}\mxL_{t,k}^\tr.\label{eq:P_t|nD}
\end{align}
% \end{split}
% \end{equation}
In the last equality, we use Lemma~\ref{lem:matrix_inversion_lemma}. Notice that $\hat{\mxA}_k$ is the parameter estimates from the last iteration and thus is known. Since $\mxP_{t|t}$ is positive definite, \eqref{eq:P_t|nD} implies that $\mxP_{t|\nD}$ is also positive definite. By substituting \eqref{eq:P_t|nD} into \eqref{eq:H_k 2}, we have
% \begin{equation}\label{eq:H_theta|hat_theta PD}
% \begin{split}
\begin{align}
    \mxH_k(\mxM) &= 
    \frac{1}{\nD} \sum_{t=0}^{\nD-1} 
        \big[ (\hat{\vcx}_{t+1|\nD} - 
        {\mxM}
        \begin{bmatrix}
            \hat{\vcx}_{t+1|\nD} \\ \vcu_t
        \end{bmatrix}
        )
        (\hat{\vcx}_{t+1|\nD} - 
        {\mxM}
        \begin{bmatrix}
            \hat{\vcx}_{t+1|\nD} \\ \vcu_t
        \end{bmatrix}
        )^\tr \nonumber
        \\& \quad
        + (\mxP_{t+1|\nD}^{\frac{1}{2}} - {\mxA}\mxL_{t,k}\mxP_{t+1|\nD}^{\frac{1}{2}})(\mxP_{t+1|\nD}^{\frac{1}{2}} - {\mxA}\mxL_{t,k}\mxP_{t+1|\nD}^{\frac{1}{2}})^\tr 
        + \mxA(\mxP_{t|t}^{-1} + \hat{\mxA}_k \hat{\mxS}_{\vcw,k} \hat{\mxA}_k)^{-1}{\mxA}^\tr \big] \label{eq:H_theta|hat_theta PD1}\\
    &= 
    \frac{1}{\nD} \sum_{t=0}^{\nD-1} 
        \big[ (\hat{\vcx}_{t+1|\nD} - 
        {\mxM}
        \begin{bmatrix}
            \hat{\vcx}_{t+1|\nD} \\ \vcu_t
        \end{bmatrix}
        )
        (\hat{\vcx}_{t+1|\nD} - 
        {\mxM}
        \begin{bmatrix}
            \hat{\vcx}_{t+1|\nD} \\ \vcu_t
        \end{bmatrix}
        )^\tr \nonumber
        \\& \quad
        + (\mathbf{I} - {\mxA}\mxL_{t,k})\mxP_{t+1|\nD}(\mathbf{I} - {\mxA}\mxL_{t,k})^\tr 
        + \mxA(\mxP_{t|t}^{-1} + \hat{\mxA}_k \hat{\mxS}_{\vcw,k} \hat{\mxA}_k)^{-1}{\mxA}^\tr \big]. \label{eq:H_theta|hat_theta PD2}
\end{align}
% \end{split}
% \end{equation}
Consequently, since the first term in \eqref{eq:H_theta|hat_theta PD2} is positive semi-definite, from Lemma~\ref{lem:PD}, it follows that
\begin{align}
    \mxH_k(\mxM) 
        \succeq
        \left(\mxP_{t+1|\nD}^{-1} + \mxL_{t,k}^\tr (\mxP_{t|t}^{-1} + \hat{\mxA}_k \hat{\mxS}_{\vcw,k} \hat{\mxA}_k) \mxL_{t,k} \right)^{-1}.
\end{align}
% \end{split}
% \end{equation}
Thus, $\mxH_k(\mxM)$ is positive definite for all $\mxM \in \Rbb^{\Nx \times (\Nx + \Nu)}$, with a lower bound which does not depend on $\mxM$.

Next, we show the positive definiteness of $\mxG_k(\mxN)$, which can be expanded as
\begin{align}\!\!\!\!\!\!
    \mxG_k(\mxN) &= \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})}
    \Bigg[\sum_{t=0}^{\nD-1} \left(\vcy_t - \mxN \begin{bmatrix} \vcx_t\\ \vcu_t \otimes \vcx_t \\ \vcu_t\end{bmatrix}  \right)
    \left(\vcy_t - \mxN \begin{bmatrix} \vcx_t\\ \vcu_t \otimes \vcx_t \\ \vcu_t\end{bmatrix}  \right)^\tr\Bigg] \label{eq:G_theta|hat_theta PD 1}\\
    %%
    &= \sum_{t=0}^{\nD-1} 
    \Bigg[ \vcy_t \vcy_t^\tr 
    - \mxN \begin{bmatrix} 
    \hat{\vcx}_{t|\nD}\\ \vcu_t \otimes \hat{\vcx}_{t|\nD} \\ \vcu_t\end{bmatrix}\vcy_t^\tr
    - \vcy_t \begin{bmatrix} \hat{\vcx}_{t|\nD}\\ \vcu_t \otimes \hat{\vcx}_{t|\nD} \\ \vcu_t\end{bmatrix}^\tr \mxN^\tr
    \nonumber \\ &
    \quad + \mxN 
    \begin{bmatrix} 
    %%row 1 column1
    \hat{\vcx}_{t|\nD} \hat{\vcx}_{t|\nD}^\tr + \mxP_{t|\nD} 
    %%row 1 column2
    & \vcu_t^\tr \otimes \left(\hat{\vcx}_{t|\nD} \hat{\vcx}_{t|\nD}^\tr + \mxP_{t|\nD} \right) 
    %%row 1 column3
    & \hat{\vcx}_{t|\nD} \vcu_t^\tr \\ 
    %%row 2 column1
    \vcu_t \otimes \left(\hat{\vcx}_{t|\nD} \hat{\vcx}_{t|\nD}^\tr + \mxP_{t|\nD} \right)
    %%row 2 column2
    & \vcu_t \vcu_t^\tr \otimes \left(\hat{\vcx}_{t|\nD} \hat{\vcx}_{t|\nD}^\tr + \mxP_{t|\nD} \right)
    %%row 2 column3
    & \vcu_t\vcu_t^\tr \otimes \hat{\vcx}_{t|\nD}\\
    %%row 3 column1
    \vcu_t \hat{\vcx}_{t|\nD}^\tr 
    %%row 3 column2
    & \vcu_t \vcu_t^\tr \otimes \hat{\vcx}_{t|\nD}^\tr
    %%row 3 column3
    & \vcu_t \vcu_t^\tr 
    \end{bmatrix}
    \mxN^\tr
    \Bigg] \label{eq:G_theta|hat_theta PD 2}\\
    %%
    &= \sum_{t=0}^{\nD-1} \left[
    \left( \mxN \begin{bmatrix} \hat{\vcx}_{t|\nD}\\ \vcu_t \otimes \hat{\vcx}_{t|\nD} \\ \vcu_t\end{bmatrix} - \vcy_t \right) \left( \mxN \begin{bmatrix} \hat{\vcx}_{t|\nD}\\ \vcu_t \otimes \hat{\vcx}_{t|\nD} \\ \vcu_t\end{bmatrix} - \vcy_t \right)^\tr
    + \mxC 
    \begin{bmatrix}  
    %%row 1 column1
    \mxP_{t|\nD} 
    %%row 1 column2
    & \vcu_t^\tr \otimes\mxP_{t|\nD}\\
    %%row 2 column1
    \vcu_t \otimes \mxP_{t|\nD}
    %%row 2 column2
    & \vcu_t \vcu_t^\tr \otimes \mxP_{t|\nD} 
    \end{bmatrix}
    \mxC^\tr
    \right] \label{eq:G_theta|hat_theta PD 3}\\
    %%
    &= \sum_{t=0}^{\nD-1} \left[
    \left( \mxC \begin{bmatrix} \hat{\vcx}_{t|\nD}\\ \vcu_t \otimes \hat{\vcx}_{t|\nD} \end{bmatrix} + \begin{bmatrix}\mxD & -\mathbf{I}\end{bmatrix}
    \begin{bmatrix} \vcu_t \\ \vcy_t \end{bmatrix} \right) 
    \left( \mxC \begin{bmatrix} \hat{\vcx}_{t|\nD}\\ \vcu_t \otimes \hat{\vcx}_{t|\nD} \end{bmatrix} 
    + \begin{bmatrix}\mxD & -\mathbf{I}\end{bmatrix}
    \begin{bmatrix} \vcu_t \\ \vcy_t \end{bmatrix}
    \right)^\tr\right.
    \nonumber \\ &
    \quad + \mxC \left.
    \begin{bmatrix}  
    %%row 1 column1
    \mxP_{t|\nD} 
    %%row 1 column2
    & \vcu_t^\tr \otimes\mxP_{t|\nD}\\
    %%row 2 column1
    \vcu_t \otimes \mxP_{t|\nD}
    %%row 2 column2
    & \vcu_t \vcu_t^\tr \otimes \mxP_{t|\nD} 
    \end{bmatrix}
    \mxC^\tr
    \right]. \label{eq:G_theta|hat_theta PD 4}
\end{align}
% \begin{equation}
%     \sum_{t=0}^{\nsmD-1}
%     \begin{bmatrix} 
%         %%row 0 column0
%         \vcy_t\vcy_t^\tr 
%         &
%         %%row 0 column1
%         \vcy_t\hat{\vcx}_{t|\nD} ^\tr
%         &
%         %%row 0 column2
%         \vcy_t(\vcu_t^\tr \otimes \hat{\vcx}_{t|\nD}^\tr)
%         &
%         %%row 0 column3
%         \vcy_t\vcu_t ^\tr
%         \\
%         %%row 1 column0
%         \vcy_t\hat{\vcx}_{t|\nD}^\tr 
%         &
%         %%row 1 column1
%         \hat{\vcx}_{t|\nD} \hat{\vcx}_{t|\nD}^\tr + \mxP_{t|\nD} 
%         %%row 1 column2
%         & \vcu_t^\tr \otimes \left(\hat{\vcx}_{t|\nD} \hat{\vcx}_{t|\nD}^\tr + \mxP_{t|\nD} \right) 
%         %%row 1 column3
%         & \hat{\vcx}_{t|\nD} \vcu_t^\tr 
%         \\ 
%         %%row 2 column0
%         \vcy_t^\tr(\vcu_t \otimes \hat{\vcx}_{t|\nD})
%         &
%         %%row 2 column1
%         \vcu_t \otimes \left(\hat{\vcx}_{t|\nD} \hat{\vcx}_{t|\nD}^\tr +    \mxP_{t|\nD} \right)
%         %%row 2 column2
%         & \vcu_t \vcu_t^\tr \otimes \left(\hat{\vcx}_{t|\nD} \hat{\vcx}_{t|\nD}^\tr     + \mxP_{t|\nD} \right)
%         %%row 2 column3
%         & \vcu_t\vcu_t^\tr \otimes \hat{\vcx}_{t|\nD}
%         \\ 
%         %%row 3 column0
%         \vcy_t^\tr\vcu_t
%         &
%         %%row 3 column1
%         \vcu_t \hat{\vcx}_{t|\nD}^\tr 
%         %%row 3 column2
%         & \vcu_t \vcu_t^\tr \otimes \hat{\vcx}_{t|\nD}^\tr
%         %%row 3 column3
%         & \vcu_t \vcu_t^\tr 
%         \end{bmatrix}
%         -\varepsilon 
%         \begin{bmatrix}
%         \mathbf{I} & 0 & 0 & 0\\
%         0          & 0 & 0 & 0\\
%         0          & 0 & 0 & 0\\
%         0          & 0 & 0 & 0
%         \end{bmatrix}
%         \succeq 0.
% \end{equation}
% \begin{equation}
%     \sum_{t=0}^{\nsmD-1}
%     \begin{bmatrix} 
%         %%row 1 column2
%         \hat{\vcx}_{t|\nD} \hat{\vcx}_{t|\nD}^\tr + \mxP_{t|\nD} 
%         %%row 1 column2
%         & 
%         \vcu_t^\tr \otimes \left(\hat{\vcx}_{t|\nD} \hat{\vcx}_{t|\nD}^\tr + \mxP_{t|\nD} \right) 
%         %%row 1 column3
%         & 
%         \hat{\vcx}_{t|\nD} \vcu_t^\tr 
%         %%row 1 column4
%         &
%         \hat{\vcx}_{t|\nD}\vcy_t^\tr 
%         \\ 
%         %%row 2 column1
%         \vcu_t \otimes \left(\hat{\vcx}_{t|\nD} \hat{\vcx}_{t|\nD}^\tr +    \mxP_{t|\nD} \right)
%         %%row 2 column2
%         & 
%         \vcu_t \vcu_t^\tr \otimes \left(\hat{\vcx}_{t|\nD} \hat{\vcx}_{t|\nD}^\tr     + \mxP_{t|\nD} \right)
%         %%row 2 column3
%         & 
%         \vcu_t\vcu_t^\tr \otimes \hat{\vcx}_{t|\nD}
%         %%row 2 column4
%         & 
%         (\vcu_t \otimes \hat{\vcx}_{t|\nD})\vcy_t^\tr
%         \\ 
%         %%row 3 column1
%         \vcu_t \hat{\vcx}_{t|\nD}^\tr 
%         %%row 3 column2
%         & 
%         \vcu_t \vcu_t^\tr \otimes \hat{\vcx}_{t|\nD}^\tr
%         %%row 3 column3
%         & 
%         \vcu_t \vcu_t^\tr
%         %%row 3 column4
%         &
%         \vcu_t\vcy_t^\tr
%         \\
%         %%row 4 column1
%         \vcy_t\hat{\vcx}_{t|\nD}^\tr  
%         &
%         %%row 4 column1
%         \vcy_t(\vcu_t^\tr \otimes \hat{\vcx}_{t|\nD}^\tr)
%         &
%         %%row 4 column2
%         \vcy_t\vcu_t^\tr
%         &
%         %%row 4 column3
%         \vcy_t\vcy_t^\tr
%         \end{bmatrix}
%         -\varepsilon 
%         \begin{bmatrix}
%         0 & 0 & 0 & 0\\
%         0 & 0 & 0 & 0\\
%         0 & 0 & 0 & 0\\
%         0 & 0 & 0 & \mathbf{I}
%         \end{bmatrix}
%         \succeq 0.
% \end{equation}
% $$
%     \sum_{t=0}^{\nsmD-1}
%     \begin{bmatrix} 
%         %%row 1 column2
%         \hat{\vcx}_{t|\nD} \hat{\vcx}_{t|\nD}^\tr 
%         %%row 1 column2
%         & 
%         \vcu_t^\tr \otimes \left(\hat{\vcx}_{t|\nD} \hat{\vcx}_{t|\nD}^\tr \right) 
%         %%row 1 column3
%         & 
%         \hat{\vcx}_{t|\nD} \vcu_t^\tr 
%         %%row 1 column4
%         &
%         \vcy_t\hat{\vcx}_{t|\nD}^\tr 
%         \\ 
%         %%row 2 column1
%         \vcu_t \otimes \left(\hat{\vcx}_{t|\nD} \hat{\vcx}_{t|\nD}^\tr  \right)
%         %%row 2 column2
%         & 
%         \vcu_t \vcu_t^\tr \otimes \left(\hat{\vcx}_{t|\nD} \hat{\vcx}_{t|\nD}^\tr  \right)
%         %%row 2 column3
%         & 
%         \vcu_t\vcu_t^\tr \otimes \hat{\vcx}_{t|\nD}
%         %%row 2 column4
%         & 
%         \vcy_t(\vcu_t \otimes \hat{\vcx}_{t|\nD})^\tr
%         \\ 
%         %%row 3 column1
%         \vcu_t \hat{\vcx}_{t|\nD}^\tr 
%         %%row 3 column2
%         & 
%         \vcu_t \vcu_t^\tr \otimes \hat{\vcx}_{t|\nD}^\tr
%         %%row 3 column3
%         & 
%         0
%         %%row 3 column4
%         &
%         0
%         \\
%         %%row 4 column1
%         \vcy_t\hat{\vcx}_{t|\nD}^\tr  
%         &
%         %%row 4 column1
%         \vcy_t(\vcu_t^\tr \otimes \hat{\vcx}_{t|\nD}^\tr)
%         &
%         %%row 4 column2
%         0
%         &
%         %%row 4 column3
%         0
%         \end{bmatrix}
% +
% $$
% \begin{equation}
%     \sum_{t=0}^{\nsmD-1}
%     \begin{bmatrix} 
%         %%row 1 column2
%         \mxP_{t|\nD} 
%         %%row 1 column2
%         & 
%         \vcu_t^\tr \otimes \mxP_{t|\nD} 
%         %%row 1 column3
%         & 
%         0
%         %%row 1 column4
%         &
%         0
%         \\ 
%         %%row 2 column1
%         \vcu_t \otimes  \mxP_{t|\nD} 
%         %%row 2 column2
%         & 
%         \vcu_t \vcu_t^\tr \otimes  \mxP_{t|\nD} 
%         %%row 2 column3
%         & 
%         0
%         %%row 2 column4
%         & 
%         0
%         \\ 
%         %%row 3 column1
%         0
%         %%row 3 column2
%         & 
%         0
%         %%row 3 column3
%         & 
%         \vcu_t \vcu_t^\tr
%         %%row 3 column4
%         &
%         \vcy_t\vcu_t^\tr
%         \\
%         %%row 4 column1
%         0 
%         &
%         %%row 4 column1
%         0
%         &
%         %%row 4 column2
%         \vcy_t\vcu_t^\tr
%         &
%         %%row 4 column3
%         \vcy_t\vcy_t^\tr
%         \end{bmatrix}
%         -\varepsilon 
%         \begin{bmatrix}
%         0 & 0 & 0 & 0\\
%         0 & 0 & 0 & 0\\
%         0 & 0 & 0 & 0\\
%         0 & 0 & 0 & \mathbf{I}
%         \end{bmatrix}
%         \succeq 0.
% \end{equation}
We show the above equation is positive definite by contradiction. Assume there exists a non zeros vector $\vcz \in \Rbb^{\Ny}$, such that $\vcz^\tr \mxG_k(\mxN) \vcz = 0$. Since both two terms in \eqref{eq:G_theta|hat_theta PD 4} are positive semi-definite, we have
\begin{equation}
\begin{split}
    &\qquad  \vcz^\tr \mxC 
    \sum_{t=0}^{\nD-1}
    \begin{bmatrix}  
    %%row 1 column1
    \mxP_{t|\nD} 
    %%row 1 column2
    & \vcu_t^\tr \otimes\mxP_{t|\nD}\\
    %%row 2 column1
    \vcu_t \otimes \mxP_{t|\nD}
    %%row 2 column2
    & \vcu_t \vcu_t^\tr \otimes \mxP_{t|\nD} 
    \end{bmatrix}
    \mxC^\tr \vcz = 0,\\
    %%
\end{split}
\end{equation}    
which is equivalent to
\begin{equation}
\begin{split}    
    \vcz^\tr \mxC 
    \sum_{t=0}^{\nD-1}
    \left( \begin{bmatrix}  1 \\ \vcu_t \end{bmatrix}
    \begin{bmatrix}  1 \\ \vcu_t \end{bmatrix}^\tr 
    \otimes \mxP_{t|\nD} \right)
    \mxC^\tr \vcz = 0. 
\end{split}
\end{equation}
As we show $\mxP_{t|\nD} \succ 0$ in \eqref{eq:P_t|nD} and from Lemma~\ref{lem:PD for summarion u_t}, the above equation implies $\mxC^\tr \vcz = 0$. On the other hand, from \eqref{eq:G_theta|hat_theta PD 4}, $\vcz^\tr \mxG_k(\mxN) \vcz = 0$ also implies that 
\begin{equation}
    \begin{split}
    &\qquad 
    \sum_{t=0}^{\nD-1} \left[    \vcz^\tr
    \left( \mxC \begin{bmatrix} \hat{\vcx}_{t|\nD}\\ \vcu_t \otimes \hat{\vcx}_{t|\nD} \end{bmatrix} + \begin{bmatrix}\mxD & -\mathbf{I}\end{bmatrix}
    \begin{bmatrix} \vcu_t \\ \vcy_t \end{bmatrix} \right) 
    \left( \mxC \begin{bmatrix} \hat{\vcx}_{t|\nD}\\ \vcu_t \otimes \hat{\vcx}_{t|\nD} \end{bmatrix} 
    + \begin{bmatrix}\mxD & -\mathbf{I}\end{bmatrix}
    \begin{bmatrix} \vcu_t \\ \vcy_t \end{bmatrix}
    \right)^\tr \vcz \right] = 0,\\
    \end{split}
\end{equation}
    % %%
    % &\implies     
    % \sum_{t=0}^{\nD-1} \left[    \vcz^\tr
    % \left(  \begin{bmatrix}\mxD & -\mathbf{I}\end{bmatrix}
    % \begin{bmatrix} \vcu_t \\ \vcy_t \end{bmatrix} \right) 
    % \left( \begin{bmatrix}\mxD & -\mathbf{I}\end{bmatrix}
    % \begin{bmatrix} \vcu_t \\ \vcy_t \end{bmatrix}
    % \right)^\tr \vcz \right] = 0, \\
    %%
which is equivalent to
\begin{equation}
    \begin{split}
    \vcz^\tr
    \begin{bmatrix}\mxD & -\mathbf{I}\end{bmatrix}
    \sum_{t=0}^{\nD-1}
    \begin{bmatrix} 
        \vcu_t\vcu_t^\tr & \vcu_t\vcy_t^\tr\\
        \vcy_t\vcu_t^\tr & \vcy_t\vcy_t^\tr
    \end{bmatrix} 
    \begin{bmatrix}\mxD & -\mathbf{I}\end{bmatrix}^\tr
    \vcz  = 0. \\
    \end{split}
\end{equation}
From Lemma~\ref{lem:PD for summarion y_t}, we know
\begin{align}
  \sum_{t=0}^{\nD-1}
    \begin{bmatrix} 
        \vcu_t\vcu_t^\tr & \vcu_t\vcy_t^\tr\\
        \vcy_t\vcu_t^\tr & \vcy_t\vcy_t^\tr
    \end{bmatrix} \succ 0.  
\end{align}
Thus, we arrive at $\vcz = 0$, which contradicts our assumption that $\vcz$ is a non zero vector. Therefore, $\mxG_k(\mxN)$ is positive definite for all $\mxN \in \Rbb^{\Ny \times (\Nu + \Nx + \Nx\Nu)}$. Now, we give the lower bound of $\mxG_k(\mxN)$. Recall that
\begin{equation}
\begin{split}
    \Psi &=   \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} \left[ \sum_{t=0}^{\nD - 1} \left(\begin{bmatrix}
        \vcx_t \\ \vcu_t \otimes \vcx_t \\ \vcu_t
    \end{bmatrix}
    \begin{bmatrix}
        \vcx_t \\ \vcu_t \otimes \vcx_t \\ \vcu_t
    \end{bmatrix}^\tr \right)\right]
    \\ &
    %%
    =\sum_{t=0}^{\nD-1}
    \begin{bmatrix} 
    %%row 1 column1
    \hat{\vcx}_{t|\nD} \hat{\vcx}_{t|\nD}^\tr + \mxP_{t|\nD} 
    %%row 1 column2
    & \vcu_t^\tr \otimes \left(\hat{\vcx}_{t|\nD} \hat{\vcx}_{t|\nD}^\tr + \mxP_{t|\nD} \right) 
    %%row 1 column3
    & \hat{\vcx}_{t|\nD} \vcu_t^\tr \\ 
    %%row 2 column1
    \vcu_t \otimes \left(\hat{\vcx}_{t|\nD} \hat{\vcx}_{t|\nD}^\tr + \mxP_{t|\nD} \right)
    %%row 2 column2
    & \vcu_t \vcu_t^\tr \otimes \left(\hat{\vcx}_{t|\nD} \hat{\vcx}_{t|\nD}^\tr + \mxP_{t|\nD} \right)
    %%row 2 column3
    & \vcu_t\vcu_t^\tr \otimes \hat{\vcx}_{t|\nD}\\
    %%row 3 column1
    \vcu_t \hat{\vcx}_{t|\nD}^\tr 
    %%row 3 column2
    & \vcu_t \vcu_t^\tr \otimes \hat{\vcx}_{t|\nD}^\tr
    %%row 3 column3
    & \vcu_t \vcu_t^\tr 
    \end{bmatrix}
\end{split}
\end{equation}
as defined in Lemma~\ref{lem:PD_phi_psi}.
Additionally, define matrices $\Pi$ and $\mxY$ as
% \begin{equation}
% \begin{split}
%     %%
%     \Pi &:= \sum_{t=0}^{\nD-1} \vcy_t
%     \begin{bmatrix}
%         \vcx_t \\ \vcu_t \otimes \vcx_t \\ \vcu_t
%     \end{bmatrix},
%     \\
%     %%
%     \mxY &:= \sum_{t=0}^{\nD-1} \vcy_t \vcy_t^\tr. 
% \end{split}
% \end{equation}
\begin{equation}
\begin{split}
    %%
    \Pi &:= \sum_{t=0}^{\nD-1} \vcy_t
    \begin{bmatrix}
        \vcx_t \\ \vcu_t \otimes \vcx_t \\ \vcu_t
    \end{bmatrix},
\end{split}
\end{equation}
and
\begin{equation}
\begin{split}
    \mxY &:= \sum_{t=0}^{\nD-1} \vcy_t \vcy_t^\tr, 
\end{split}
\end{equation}
respectively. From Lemma~\ref{lem:PD_phi_psi}, we know $\Psi$ is positive definite and invertible.  Using these notations, from \eqref{eq:G_theta|hat_theta PD 2}, we have
\begin{equation}
\begin{split}
    \mxG_k(\mxN) &= \mxN\Psi\mxN^\tr - \mxN \Pi^\tr - \Pi \mxN^\tr + \mxY
    \\ &
    = \left(\mxN\Psi^{\frac
    {1}{2}} - \Pi^\tr\Psi^{-\frac{1}{2}} \right) \left(\mxN\Psi^{\frac{1}{2}} - \Pi^\tr\Psi^{-\frac{1}{2}} \right)^\tr + \mxY - \Pi^\tr \Psi^{-1} \Pi,
\end{split}
\end{equation}
which implies that 
\begin{equation}
    \mxG_k(\mxN) \succeq \mxY - \Pi^\tr \Psi^{-1} \Pi.
\end{equation}
Moreover, we have
\begin{equation}
    \mxY - \Pi^\tr \Psi^{-1} \Pi = \mxG_k(\Pi^\tr \Psi^{-1}) \succ 0,
\end{equation}
which implies that $\mxG_k(\mxN) \succeq \lambda_0 \mathbf{I}$, where $\lambda_0 = \lambda_{\min} \left(\mxY - \Pi^\tr \Psi^{-1} \Pi \right) > 0$. Thus, $\mxG_k(\mxN)$ is positive definite with a lower bound which does not depend on $\mxN$.


Finally, for the $\mxF_k(\mu_{\vcx_0})$, we have
\begin{align}
   \mxF_k(\mu_{\vcx_0}) 
   &= 
   \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} \left[(\vcx_0-\vcmu_{\vcx_0}) (\vcx_0-\vcmu_{\vcx_0})^\tr\right]
   \\
   &= 
   \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} 
   \left[\vcx_0\vcx_0^\tr\right] 
   -
   \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} 
   \left[\vcmu_{\vcx_0}\vcx_0^\tr\right]
   -
   \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} 
   \left[\vcx_0\vcmu_{\vcx_0}^\tr\right]
   +
   \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} 
   \left[\vcmu_{\vcx_0}\vcmu_{\vcx_0}^\tr\right] 
    \\
   &= 
   \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} 
   \left[\vcx_0\vcx_0^\tr\right] 
   -
   \vcmu_{\vcx_0}\left(\mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} 
   [\vcx_0]\right)^\tr
   -
   \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} 
    [\vcx_0]\vcmu_{\vcx_0}^\tr
   +
   \vcmu_{\vcx_0}\vcmu_{\vcx_0}^\tr 
    \\
   &= \hat{\vcx}_{0|\nD}\hat{\vcx}_{0|\nD}^\tr +
    \mxP_{0|\nD} - \vcmu_{\vcx_0}\hat{\vcx}_{0|\nD}^\tr - \hat{\vcx}_{0|\nD}\vcmu_{\vcx_0}^\tr + \vcmu_{\vcx_0}\vcmu_{\vcx_0}^\tr
    \\
   &=
   \mxP_{0|\nD} + (\hat{\vcx}_{0|\nD} - \vcmu_{\vcx_0})(\hat{\vcx}_{0|\nD} - \vcmu_{\vcx_0})^\tr 
   \\
   &\succeq \mxP_{0|\nD}.
\end{align}
Accordingly, $\mxP_{0|\nD} \succ 0$, it follows that $\mxF_k(\mu_{\vcx_0})$ is positive definite for all $\mu_{\vcx_0} \in \Rbb^{\Nx}$, with a lower bound which does not depend on $\vcmu_{\vcx_0}$. This concludes the proof.
\qed



% \cmm{
% Next, we show the positive definiteness of $\mxG_k(\mxN)$, which can be expanded as
% \begin{align}
%     \mxG_k(\mxN) - \varepsilon\mathbf{I} &= \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})}
%     \Bigg[\sum_{t=0}^{\nD-1} \left(\vcy_t - \mxN \begin{bmatrix} \vcx_t\\ \vcu_t \otimes \vcx_t \\ \vcu_t\end{bmatrix}  \right)
%     \left(\vcy_t - \mxN \begin{bmatrix} \vcx_t\\ \vcu_t \otimes \vcx_t \\ \vcu_t\end{bmatrix}  \right)^\tr - \varepsilon\mathbf{I} \Bigg] \label{eq:G_theta|hat_theta PD 1}\\
%     %%
%     &= \sum_{t=0}^{\nD-1} 
%     \Bigg[ \vcy_t \vcy_t^\tr 
%     - \mxN \begin{bmatrix} 
%     \hat{\vcx}_{t|\nD}\\ \vcu_t \otimes \hat{\vcx}_{t|\nD} \\ \vcu_t\end{bmatrix}\vcy_t^\tr
%     - \vcy_t \begin{bmatrix} \hat{\vcx}_{t|\nD}\\ \vcu_t \otimes \hat{\vcx}_{t|\nD} \\ \vcu_t\end{bmatrix}^\tr \mxN^\tr
%     \nonumber \\ &
%     \quad + \mxN 
%     \begin{bmatrix} 
%     %%row 1 column1
%     \hat{\vcx}_{t|\nD} \hat{\vcx}_{t|\nD}^\tr + \mxP_{t|\nD} 
%     %%row 1 column2
%     & \vcu_t^\tr \otimes \left(\hat{\vcx}_{t|\nD} \hat{\vcx}_{t|\nD}^\tr + \mxP_{t|\nD} \right) 
%     %%row 1 column3
%     & \hat{\vcx}_{t|\nD} \vcu_t^\tr \\ 
%     %%row 2 column1
%     \vcu_t \otimes \left(\hat{\vcx}_{t|\nD} \hat{\vcx}_{t|\nD}^\tr + \mxP_{t|\nD} \right)
%     %%row 2 column2
%     & \vcu_t \vcu_t^\tr \otimes \left(\hat{\vcx}_{t|\nD} \hat{\vcx}_{t|\nD}^\tr + \mxP_{t|\nD} \right)
%     %%row 2 column3
%     & \vcu_t\vcu_t^\tr \otimes \hat{\vcx}_{t|\nD}\\
%     %%row 3 column1
%     \vcu_t \hat{\vcx}_{t|\nD}^\tr 
%     %%row 3 column2
%     & \vcu_t \vcu_t^\tr \otimes \hat{\vcx}_{t|\nD}^\tr
%     %%row 3 column3
%     & \vcu_t \vcu_t^\tr 
%     \end{bmatrix}
%     \mxN^\tr
%      - \varepsilon\mathbf{I} \Bigg] \label{eq:G_theta|hat_theta PD 2}\\
%     %%
%     &= \sum_{t=0}^{\nD-1} \left[
%     \left( \mxN \begin{bmatrix} \hat{\vcx}_{t|\nD}\\ \vcu_t \otimes \hat{\vcx}_{t|\nD} \\ \vcu_t\end{bmatrix} - \vcy_t \right) \left( \mxN \begin{bmatrix} \hat{\vcx}_{t|\nD}\\ \vcu_t \otimes \hat{\vcx}_{t|\nD} \\ \vcu_t\end{bmatrix} - \vcy_t \right)^\tr
%     + \mxC 
%     \begin{bmatrix}  
%     %%row 1 column1
%     \mxP_{t|\nD} 
%     %%row 1 column2
%     & \vcu_t^\tr \otimes\mxP_{t|\nD}\\
%     %%row 2 column1
%     \vcu_t \otimes \mxP_{t|\nD}
%     %%row 2 column2
%     & \vcu_t \vcu_t^\tr \otimes \mxP_{t|\nD} 
%     \end{bmatrix}
%     \mxC^\tr
%      - \varepsilon\mathbf{I} \right] \label{eq:G_theta|hat_theta PD 3}\\
%     %%
%     &= \sum_{t=0}^{\nD-1} \left[
%     \left( \mxC \begin{bmatrix} \hat{\vcx}_{t|\nD}\\ \vcu_t \otimes \hat{\vcx}_{t|\nD} \end{bmatrix} + \begin{bmatrix}\mxD & -\mathbf{I}\end{bmatrix}
%     \begin{bmatrix} \vcu_t \\ \vcy_t \end{bmatrix} \right) 
%     \left( \mxC \begin{bmatrix} \hat{\vcx}_{t|\nD}\\ \vcu_t \otimes \hat{\vcx}_{t|\nD} \end{bmatrix} 
%     + \begin{bmatrix}\mxD & -\mathbf{I}\end{bmatrix}
%     \begin{bmatrix} \vcu_t \\ \vcy_t \end{bmatrix}
%     \right)^\tr
%     + \mxC 
%     \begin{bmatrix}  
%     %%row 1 column1
%     \mxP_{t|\nD} 
%     %%row 1 column2
%     & \vcu_t^\tr \otimes\mxP_{t|\nD}\\
%     %%row 2 column1
%     \vcu_t \otimes \mxP_{t|\nD}
%     %%row 2 column2
%     & \vcu_t \vcu_t^\tr \otimes \mxP_{t|\nD} 
%     \end{bmatrix}
%     \mxC^\tr
%     \right]. \label{eq:G_theta|hat_theta PD 4}
% \end{align}



% We show the above equation is positive definite by contradiction. Assume there exist a non zeros vector $\vcz \in \Rbb^\Ny$, such that $\vcz^\tr \mxG_k(\mxN) \vcz = 0$. Since both two terms in \eqref{eq:G_theta|hat_theta PD 4} are positive semi-definite, we have
% \begin{equation}
% \begin{split}
%     &\qquad  \vcz^\tr \mxC 
%     \sum_{t=0}^{\nD-1}
%     \begin{bmatrix}  
%     %%row 1 column1
%     \mxP_{t|\nD} 
%     %%row 1 column2
%     & \vcu_t^\tr \otimes\mxP_{t|\nD}\\
%     %%row 2 column1
%     \vcu_t \otimes \mxP_{t|\nD}
%     %%row 2 column2
%     & \vcu_t \vcu_t^\tr \otimes \mxP_{t|\nD} 
%     \end{bmatrix}
%     \mxC^\tr \vcz = 0,\\
%     %%
% \end{split}
% \end{equation}    
% which implies that
% \begin{equation}
% \begin{split}    
%     \vcz^\tr \mxC 
%     \sum_{t=0}^{\nD-1}
%     \left( \begin{bmatrix}  1 \\ \vcu_t \end{bmatrix}
%     \begin{bmatrix}  1 \\ \vcu_t \end{bmatrix}^\tr 
%     \otimes \mxP_{t|\nD} \right)
%     \mxC^\tr \vcz = 0. 
% \end{split}
% \end{equation}
% As we show $\mxP_{t|\nD} \succ 0$ in \eqref{eq:P_t|nD}, from Assumption~\ref{assum:input}, the above equation implies $\mxC^\tr \vcz = 0$. On the other hand, $\vcz^\tr \mxG_k(\mxN) \vcz = 0$ also implies that 
% \begin{equation}
%     \begin{split}
%     &\qquad 
%     \sum_{t=0}^{\nD-1} \left[    \vcz^\tr
%     \left( \mxC \begin{bmatrix} \hat{\vcx}_{t|\nD}\\ \vcu_t \otimes \hat{\vcx}_{t|\nD} \end{bmatrix} + \begin{bmatrix}\mxD & -\mathbf{I}\end{bmatrix}
%     \begin{bmatrix} \vcu_t \\ \vcy_t \end{bmatrix} \right) 
%     \left( \mxC \begin{bmatrix} \hat{\vcx}_{t|\nD}\\ \vcu_t \otimes \hat{\vcx}_{t|\nD} \end{bmatrix} 
%     + \begin{bmatrix}\mxD & -\mathbf{I}\end{bmatrix}
%     \begin{bmatrix} \vcu_t \\ \vcy_t \end{bmatrix}
%     \right)^\tr \vcz \right] = 0,\\
%     \end{split}
% \end{equation}
%     % %%
%     % &\implies     
%     % \sum_{t=0}^{\nD-1} \left[    \vcz^\tr
%     % \left(  \begin{bmatrix}\mxD & -\mathbf{I}\end{bmatrix}
%     % \begin{bmatrix} \vcu_t \\ \vcy_t \end{bmatrix} \right) 
%     % \left( \begin{bmatrix}\mxD & -\mathbf{I}\end{bmatrix}
%     % \begin{bmatrix} \vcu_t \\ \vcy_t \end{bmatrix}
%     % \right)^\tr \vcz \right] = 0, \\
%     %%
% which implies that
% \begin{equation}
%     \begin{split}
%     \vcz^\tr
%     \begin{bmatrix}\mxD & -\mathbf{I}\end{bmatrix}
%     \sum_{t=0}^{\nD-1}
%     \begin{bmatrix} 
%         \vcu_t\vcu_t^\tr & \vcu_t\vcy_t^\tr\\
%         \vcy_t\vcu_t^\tr & \vcy_t\vcy_t^\tr
%     \end{bmatrix} 
%     \begin{bmatrix}\mxD & -\mathbf{I}\end{bmatrix}^\tr
%     \vcz  = 0. \\
%     \end{split}
% \end{equation}
% From Assumption~\ref{assum:output}, we know
% \begin{align}
%   \sum_{t=0}^{\nD-1}
%     \begin{bmatrix} 
%         \vcu_t\vcu_t^\tr & \vcu_t\vcy_t^\tr\\
%         \vcy_t\vcu_t^\tr & \vcy_t\vcy_t^\tr
%     \end{bmatrix} \succ 0.  
% \end{align}
% Thus, $\vcz = 0$ which contradicts our assumption that $\vcz$ is a non zero vector. Therefore, $\mxG_k(\mxN)$ is positive definite for all $\mxN \in \Rbb^{\Ny \times (\Nu + \Nx + \Nx\Nu)}$.




% }
%========================================================
%========================================================
%========================================================
\section{Proof of Proposition 3}\label{app:Stationary Point Uniqueness}
% Define $\vcz_t = \begin{bmatrix}\vcx_t\\ \vcu_t \end{bmatrix}$, for all $t=0,\ldots,\nD-1$. Accordingly, 
% We can rewrite \eqref{eq:J function} as
From \eqref{eq:J function}, we have
% \begin{equation}\label{eq:Q function3}
%     \begin{split}
\begin{align}
        J_k(\vctheta) = \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})}\Bigg[
        &\frac{\nD}{2}\logdet\big(\mxS_\vcv\big) 
        + 
        \frac{1}{2} \logdet\big(\mxS_{\vcx_0}\big)
        +
        \frac{\nD}{2}\logdet\big(\mxS_\vcw\big) 
        \\&
        +\frac{1}{2}\sum_{t=0}^{\nD-1} \trace\left(\mxS_\vcv^{-1} \left(\vcy_t - \mxN \begin{bmatrix} \vcx_t\\ \vcu_t \otimes \vcx_t \\ \vcu_t\end{bmatrix}  \right)
        \left(\vcy_t - \mxN \begin{bmatrix} \vcx_t\\ \vcu_t \otimes \vcx_t \\ \vcu_t\end{bmatrix}  \right)^\tr\right)
        \\ & 
        + \frac{1}{2}\trace\left(\mxS_{\vcx_0}^{-1} \left(\vcx_0-\vcmu_{\vcx_0}\right) \left(\vcx_0-\vcmu_{\vcx_0}\right)^\tr\right) 
        \\ &
        +\frac{1}{2}\sum_{t=0}^{\nD-1} \trace \left( \mxS_\vcw^{-1}\left(\vcx_{t+1} - \mxM \begin{bmatrix}\vcx_t\\ \vcu_t \end{bmatrix}\right)\left(\vcx_{t+1} - \mxM \begin{bmatrix}\vcx_t\\ \vcu_t \end{bmatrix}\right)^\tr\right)\Bigg].
\end{align}
%     \end{split}%
% \end{equation}
To find all local minimums, let the partial derivative with respect to each parameter be zero. By deriving the derivatives, we obtain
% \begin{align*}
%         %%line 1
%         \pdv{J_k(\vctheta)}{\mxM} =& \, \mxS_\vcw^{-1} \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} \left[\sum_{t=1}^{\nD} - \left(\vcx_{t+1} - \mxM \begin{bmatrix}\vcx_t\\ \vcu_t \end{bmatrix} \right)\begin{bmatrix}\vcx_t^\tr & \vcu_t^\tr \end{bmatrix}\right], \\
%         %% line 2
%         \pdv{J_k(\vctheta)}{\mxS_\vcw} =& \, \frac{1}{2}\mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} \sum_{t=1}^{\nD} \left[\nD\mxS_\vcw^{-1} - \mxS_\vcw^{-1} \left(\vcx_{t+1} - \mxM \begin{bmatrix}\vcx_t\\ \vcu_t \end{bmatrix}\right)\left(\vcx_{t+1} - \mxM \begin{bmatrix}\vcx_t\\ \vcu_t \end{bmatrix}\right)^\tr \mxS_\vcw^{-1}\right], \\ 
%         %%line 3
%         \pdv{J_k(\vctheta)}{\mxC} =& \, \mxS_\vcv^{-1} \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} \left[-\sum_{t=0}^{\nD-1}  \vcy_t  (\begin{bmatrix}1\\ \vcu_t \end{bmatrix} \otimes \vcx_t)^\tr + \mxC \left[\sum_{t=0}^{\nD-1} (\begin{bmatrix}1\\ \vcu_t \end{bmatrix} \otimes \vcx_t)(\begin{bmatrix}1\\ \vcu_t \end{bmatrix} \otimes \vcx_t)^\tr\right]\right],\\
%         %%line 4
%         \pdv{J_k(\vctheta)}{\mxS_{\vcv}} =& \, \frac{1}{2}\mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} \sum_{t=0}^{\nD-1} \left[\nD\mxS_\vcv^{-1} - \mxS_\vcv^{-1} \left[\vcy_t  - \mxC (\begin{bmatrix}1\\ \vcu_t \end{bmatrix} \otimes \vcx_t)\right]\left[\vcy_t - \mxC (\begin{bmatrix}1\\ \vcu_t \end{bmatrix} \otimes \vcx_t)\right]^\tr \mxS_\vcv^{-1}\right], \\ 
%         %%line 5
%         \pdv{J_k(\vctheta)}{\vcmu_{\vcx_0}} =& \, \mxS_{\vcx_0}^{-1} \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} \left[-\vcx_0+\vcmu_{\vcx_0}\right],\\
%         %%line 6
%         \pdv{J_k(\vctheta)}{\mxS_{\vcx_0}} =& \, \frac{1}{2}\mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})}  \left[\mxS_{\vcx_0}^{-1} - \mxS_{\vcx_0}^{-1} (\vcx_0-\vcmu_{\vcx_0}) (\vcx_0-\vcmu_{\vcx_0})^\tr\mxS_{\vcx_0}^{-1}\right]. 
% \end{align*}
\begin{align}
        %%line 1
        \pdv{J_k(\vctheta)}{\mxM} =& \, \mxS_\vcw^{-1} \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} \left[\sum_{t=0}^{\nD-1} - \left(\vcx_{t+1} - \mxM \begin{bmatrix}\vcx_t\\ \vcu_t \end{bmatrix} \right)\begin{bmatrix}\vcx_t^\tr & \vcu_t^\tr \end{bmatrix}\right], \\
        %%line 3
        \pdv{J_k(\vctheta)}{\mxN} =& \, \mxS_\vcv^{-1} \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} \left[-\sum_{t=0}^{\nD-1}  \vcy_t  \begin{bmatrix} \vcx_t\\ \vcu_t \otimes \vcx_t \\ \vcu_t\end{bmatrix}^\tr + \mxN \left(\sum_{t=0}^{\nD-1} ( \begin{bmatrix} \vcx_t\\ \vcu_t \otimes \vcx_t \\ \vcu_t\end{bmatrix} \begin{bmatrix} \vcx_t\\ \vcu_t \otimes \vcx_t \\ \vcu_t\end{bmatrix}^\tr\right)\right],\\
        %% line 2
        \pdv{J_k(\vctheta)}{\mxS_\vcw} =& \, \frac{1}{2}\mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} \sum_{t=0}^{\nD-1} \left[\nD\mxS_\vcw^{-1} - \mxS_\vcw^{-1} \left(\vcx_{t+1} - \mxM \begin{bmatrix}\vcx_t\\ \vcu_t \end{bmatrix}\right)\left(\vcx_{t+1} - \mxM \begin{bmatrix}\vcx_t\\ \vcu_t \end{bmatrix}\right)^\tr \mxS_\vcw^{-1}\right], \\ 
        %%line 4
        \pdv{J_k(\vctheta)}{\mxS_{\vcv}} =& \, \frac{1}{2}\mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} \sum_{t=0}^{\nD-1} \left[\nD\mxS_\vcv^{-1} - \mxS_\vcv^{-1} \left(\vcy_t  - \mxN \begin{bmatrix} \vcx_t\\ \vcu_t \otimes \vcx_t \\ \vcu_t\end{bmatrix}\right)\left(\vcy_t - \mxN \begin{bmatrix} \vcx_t\\ \vcu_t \otimes \vcx_t \\ \vcu_t\end{bmatrix}\right)^\tr \mxS_\vcv^{-1}\right], \\ 
        %%line 5
        \pdv{J_k(\vctheta)}{\vcmu_{\vcx_0}} =& \, \mxS_{\vcx_0}^{-1} \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} \left[-\vcx_0+\vcmu_{\vcx_0}\right],\\
        %%line 6
        \pdv{J_k(\vctheta)}{\mxS_{\vcx_0}} =& \, \frac{1}{2}\mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})}  \left[\mxS_{\vcx_0}^{-1} - \mxS_{\vcx_0}^{-1} (\vcx_0-\vcmu_{\vcx_0}) (\vcx_0-\vcmu_{\vcx_0})^\tr\mxS_{\vcx_0}^{-1}\right]. 
\end{align}
Using the expectations in \eqref{eq:expectation 1}-\eqref{eq:expectation 3} again, the partial derivatives can be further expanded as
% \begin{align}
% % \begin{equation}
% \label{eq:J derivative}
% % \begin{split}
%         %% line 1
%         \pdv{J_k(\vctheta)}{\mxM} =& \, \mxS_\vcw^{-1}  \Big[ \sum_{t=1}^{\nD} (-\begin{bmatrix} \hat{\vcx}_{t+1|{\nD}}\hat{\vcx}_{t|{\nD}}^\tr + \mxP_{t+1|{\nD}}\mxH_t^\tr & \quad \hat{\vcx}_{t+1|\nD}\vcu_t^\tr  \end{bmatrix} + \mxM \begin{bmatrix}\hat{\vcx}_{t|\nD}\hat{\vcx}_{t|\nD}^\tr + \mxP_{t|\nD}  &\hat{\vcx}_{t|\nD}\vcu_t^\tr \\\vcu_t\hat{\vcx}_{t|\nD}^\tr &\vcu_t \vcu_t^\tr \end{bmatrix} )\Big], \\
%         %% line 2
%         \pdv{J_k(\vctheta)}{\mxS_\vcw} =& \, \frac{1}{2} \sum_{t=1}^{\nD} \Big[ \nD\mxS_\vcw^{-1} - \mxS_\vcw^{-1} \Big((\hat{\vcx}_{t+1|\nD}\hat{\vcx}_{t+1|\nD}^\tr + \mxP_{t+1|\nD}) -  \begin{bmatrix}\hat{\vcx}_{t+1|{\nD}}\hat{\vcx}_{t|{\nD}}^\tr + \mxP_{t+1|{\nD}}\mxL_t^\tr &\quad \hat{\vcx}_{t+1|\nD} \vcu_t^\tr  \end{bmatrix} \mxM^\tr  \nonumber \\
%         &-  \mxM\begin{bmatrix}\hat{\vcx}_{t+1|\nD}\hat{\vcx}_{t|\nD}^\tr + \mxP_{t+1|\nD}\mxL_t^\tr &\quad \hat{\vcx}_{t+1|\nD} \vcu_t^\tr  \end{bmatrix}^\tr + \mxM \begin{bmatrix}\hat{\vcx}_{t|\nD}\hat{\vcx}_{t|\nD}^\tr \!+\! \mxP_{t|\nD} & \hat{\vcx}_{t|\nD}\vcu_t^\tr\\ \vcu_t\hat{\vcx}_{t|\nD}^\tr &\vcu_t \vcu_t^\tr\end{bmatrix} \mxM^\tr \Big) \mxS_\vcw^{-1} \Big],\\
%         %% line 3
%         \pdv{J_k(\vctheta)}{\mxC} =& \, \mxS_\vcv^{-1} \Big[\sum_{t=0}^{\nD-1} -\vcy_t (\begin{bmatrix}1\\ \vcu_t \end{bmatrix} \otimes \hat{\vcx}_{t|\nD})^\tr + \mxC \Big( \sum_{t=0}^{\nD-1} (\begin{bmatrix}1\\ \vcu_t \end{bmatrix} \begin{bmatrix}1\\ \vcu_t \end{bmatrix}^\tr) \otimes (\hat{\vcx}_{t|{\nD}}\hat{\vcx}_{t|{\nD}}^\tr + \mxP_{t|{\nD}})\Big)\Big],\\
%         %% line 4
%         \pdv{J_k(\vctheta)}{\mxS_\vcv} =& \, \frac{1}{2} \sum_{t=0}^{\nD-1} \Big[ \nD\mxS_\vcv^{-1} - \mxS_\vcv^{-1} \Big[ \vcy_t \vcy_t^\tr - \mxC (\begin{bmatrix}1\\ \vcu_t \end{bmatrix} \otimes \hat{\vcx}_{t|\nD}) \vcy_t^\tr -  \vcy_t (\begin{bmatrix}1\\ \vcu_t \end{bmatrix} \otimes \hat{\vcx}_{t|\nD})^\tr\mxC^\tr \nonumber \\
%         & + \mxC \Big(\sum_{t=0}^{\nD-1} \begin{bmatrix}1\\ \vcu_t \end{bmatrix} \begin{bmatrix}1\\ \vcu_t \end{bmatrix}^\tr \otimes (\hat{\vcx}_{t|\nD}\hat{\vcx}_{t|\nD}^\tr + \mxP_{t|\nD})\Big)\mxC^\tr \Big] \mxS_\vcv^{-1} \Big], \\
%         %% line 5
%         \pdv{J_k(\vctheta)}{\vcmu_{\vcx_0}} =& \, \mxS_{\vcx_0}^{-1} [-\hat{\vcx}_{0|\nD}+\vcmu_{\vcx_0}],\\
%         %%line 6
%         \pdv{J_k(\vctheta)}{\mxS_{\vcx_0}} =& \, \frac{1}{2}  [\mxS_{\vcx_0}^{-1} - \mxS_{\vcx_0}^{-1} (\hat{\vcx}_{0|\nD}\hat{\vcx}_{0|\nD}^\tr + \mxP_{0|\nD} - \hat{\vcx}_{0|\nD}\vcmu_{\vcx_0}^\tr - \vcmu_{\vcx_0}\hat{\vcx}_{0|\nD}^\tr + \vcmu_{\vcx_0}\vcmu_{\vcx_0}^\tr) \mxS_{\vcx_0}^{-1}]. 
% % \end{split}
% % \end{equation}
% \end{align}
\begin{align}
% \begin{equation}
\label{eq:J derivative}
% \begin{split}
        %% line 1
        \pdv{J_k(\vctheta)}{\mxM} =& \, \mxS_\vcw^{-1}  \left[ \sum_{t=0}^{\nD-1} (-\begin{bmatrix} \hat{\vcx}_{t+1|{\nD}}\hat{\vcx}_{t|{\nD}}^\tr + \mxP_{t+1|{\nD}}\mxL_{t,k}^\tr & \quad \hat{\vcx}_{t+1|\nD}\vcu_t^\tr  \end{bmatrix} + \mxM \begin{bmatrix}\hat{\vcx}_{t|\nD}\hat{\vcx}_{t|\nD}^\tr + \mxP_{t|\nD}  &\hat{\vcx}_{t|\nD}\vcu_t^\tr \\\vcu_t\hat{\vcx}_{t|\nD}^\tr &\vcu_t \vcu_t^\tr \end{bmatrix} )\right], \\
        %% line 3
        \pdv{J_k(\vctheta)}{\mxN} =& \, \mxS_\vcv^{-1} \left[\sum_{t=0}^{\nD-1} -\vcy_t \begin{bmatrix} \hat{\vcx}_{t|\nD}\\ \vcu_t \otimes \hat{\vcx}_{t|\nD} \\ \vcu_t\end{bmatrix}^\tr\right. \!\!\!\! 
        \nonumber
        \\& + \mxN \left.\left( \sum_{t=0}^{\nD-1}
        \begin{bmatrix} 
        %%row 1 column1
        \hat{\vcx}_{t|\nD} \hat{\vcx}_{t|\nD}^\tr + \mxP_{t|\nD} 
        %%row 1 column2
        & \vcu_t^\tr \otimes \left(\hat{\vcx}_{t|\nD} \hat{\vcx}_{t|\nD}^\tr + \mxP_{t|\nD} \right) 
        %%row 1 column3
        & \hat{\vcx}_{t|\nD} \vcu_t^\tr \\ 
        %%row 2 column1
        \vcu_t \otimes \left(\hat{\vcx}_{t|\nD} \hat{\vcx}_{t|\nD}^\tr + \mxP_{t|\nD} \right)
        %%row 2 column2
        & \vcu_t \vcu_t^\tr \otimes \left(\hat{\vcx}_{t|\nD} \hat{\vcx}_{t|\nD}^\tr + \mxP_{t|\nD} \right)
        %%row 2 column3
        & \vcu_t\vcu_t^\tr \otimes \hat{\vcx}_{t|\nD}\\
        %%row 3 column1
        \vcu_t \hat{\vcx}_{t|\nD}^\tr 
        %%row 3 column2
        & \vcu_t \vcu_t^\tr \otimes \hat{\vcx}_{t|\nD}^\tr
        %%row 3 column3
        & \vcu_t \vcu_t^\tr 
        \end{bmatrix}
        \right)\right],\\
        %% line 2
        \pdv{J_k(\vctheta)}{\mxS_\vcw} =& \, \frac{1}{2} \sum_{t=0}^{\nD-1} \Big[ \nD\mxS_\vcw^{-1} - \mxS_\vcw^{-1} \Big((\hat{\vcx}_{t+1|\nD}\hat{\vcx}_{t+1|\nD}^\tr + \mxP_{t+1|\nD}) -  \begin{bmatrix}\hat{\vcx}_{t+1|{\nD}}\hat{\vcx}_{t|{\nD}}^\tr + \mxP_{t+1|{\nD}}\mxL_{t,k}^\tr &\quad \hat{\vcx}_{t+1|\nD} \vcu_t^\tr  \end{bmatrix} \mxM^\tr  \nonumber \\
        &-  \mxM\begin{bmatrix}\hat{\vcx}_{t+1|\nD}\hat{\vcx}_{t|\nD}^\tr + \mxP_{t+1|\nD}\mxL_{t,k}^\tr &\quad \hat{\vcx}_{t+1|\nD} \vcu_t^\tr  \end{bmatrix}^\tr + \mxM \begin{bmatrix}\hat{\vcx}_{t|\nD}\hat{\vcx}_{t|\nD}^\tr \!+\! \mxP_{t|\nD} & \hat{\vcx}_{t|\nD}\vcu_t^\tr\\ \vcu_t\hat{\vcx}_{t|\nD}^\tr &\vcu_t \vcu_t^\tr\end{bmatrix} \mxM^\tr \Big) \mxS_\vcw^{-1} \Big],\\
        %% line 4
        \pdv{J_k(\vctheta)}{\mxS_\vcv} =& \, \frac{1}{2} \sum_{t=0}^{\nD-1} \Bigg[ \nD\mxS_\vcv^{-1} - \mxS_\vcv^{-1} \Bigg( \vcy_t \vcy_t^\tr - \mxN \begin{bmatrix} \hat{\vcx}_{t|\nD}\\ \vcu_t \otimes \hat{\vcx}_{t|\nD} \\ \vcu_t\end{bmatrix} \vcy_t^\tr -  \vcy_t \begin{bmatrix} \hat{\vcx}_{t|\nD}\\ \vcu_t \otimes \hat{\vcx}_{t|\nD} \\ \vcu_t\end{bmatrix}^\tr\mxN^\tr \nonumber \\
        & + \mxN \sum_{t=0}^{\nD-1}
        \begin{bmatrix} 
        %%row 1 column1
        \hat{\vcx}_{t|\nD} \hat{\vcx}_{t|\nD}^\tr + \mxP_{t|\nD} 
        %%row 1 column2
        & \vcu_t^\tr \otimes \left(\hat{\vcx}_{t|\nD} \hat{\vcx}_{t|\nD}^\tr + \mxP_{t|\nD} \right) 
        %%row 1 column3
        & \hat{\vcx}_{t|\nD} \vcu_t^\tr \\ 
        %%row 2 column1
        \vcu_t \otimes \left(\hat{\vcx}_{t|\nD} \hat{\vcx}_{t|\nD}^\tr +    \mxP_{t|\nD} \right)
        %%row 2 column2
        & \vcu_t \vcu_t^\tr \otimes \left(\hat{\vcx}_{t|\nD} \hat{\vcx}_{t|\nD}^\tr     + \mxP_{t|\nD} \right)
        %%row 2 column3
        & \vcu_t\vcu_t^\tr \otimes \hat{\vcx}_{t|\nD}\\
        %%row 3 column1
        \vcu_t \hat{\vcx}_{t|\nD}^\tr 
        %%row 3 column2
        & \vcu_t \vcu_t^\tr \otimes \hat{\vcx}_{t|\nD}^\tr
        %%row 3 column3
        & \vcu_t \vcu_t^\tr 
        \end{bmatrix}
        \mxN^\tr \Bigg) \mxS_\vcv^{-1} \Bigg], \\
        %% line 5
        \pdv{J_k(\vctheta)}{\vcmu_{\vcx_0}} =& \, \mxS_{\vcx_0}^{-1} [-\hat{\vcx}_{0|\nD}+\vcmu_{\vcx_0}],\\
        %%line 6
        \pdv{J_k(\vctheta)}{\mxS_{\vcx_0}} =& \, \frac{1}{2}  [\mxS_{\vcx_0}^{-1} - \mxS_{\vcx_0}^{-1} (\hat{\vcx}_{0|\nD}\hat{\vcx}_{0|\nD}^\tr + \mxP_{0|\nD} - \hat{\vcx}_{0|\nD}\vcmu_{\vcx_0}^\tr - \vcmu_{\vcx_0}\hat{\vcx}_{0|\nD}^\tr + \vcmu_{\vcx_0}\vcmu_{\vcx_0}^\tr) \mxS_{\vcx_0}^{-1}]. 
% \end{split}
% \end{equation}
\end{align}
From Lemma~\ref{lem:PD_phi_psi}, we know that
\begin{equation}
    \Phi = \sum_{t=0}^{\nD-1}\begin{bmatrix}\hat{\vcx}_{t|\nD}\hat{\vcx}_{t|\nD}^\tr + \mxP_{t|\nD}  &\hat{\vcx}_{t|\nD}\vcu_t^\tr \\\vcu_t\hat{\vcx}_{t|\nD}^\tr &\vcu_t \vcu_t^\tr \end{bmatrix},
\end{equation}
and
\begin{equation}
    \Psi =\sum_{t=0}^{\nD-1}\begin{bmatrix} 
        %%row 1 column1
        \hat{\vcx}_{t|\nD} \hat{\vcx}_{t|\nD}^\tr + \mxP_{t|\nD} 
        %%row 1 column2
        & \vcu_t^\tr \otimes \left(\hat{\vcx}_{t|\nD} \hat{\vcx}_{t|\nD}^\tr + \mxP_{t|\nD} \right) 
        %%row 1 column3
        & \hat{\vcx}_{t|\nD} \vcu_t^\tr \\ 
        %%row 2 column1
        \vcu_t \otimes \left(\hat{\vcx}_{t|\nD} \hat{\vcx}_{t|\nD}^\tr +    \mxP_{t|\nD} \right)
        %%row 2 column2
        & \vcu_t \vcu_t^\tr \otimes \left(\hat{\vcx}_{t|\nD} \hat{\vcx}_{t|\nD}^\tr     + \mxP_{t|\nD} \right)
        %%row 2 column3
        & \vcu_t\vcu_t^\tr \otimes \hat{\vcx}_{t|\nD}\\
        %%row 3 column1
        \vcu_t \hat{\vcx}_{t|\nD}^\tr 
        %%row 3 column2
        & \vcu_t \vcu_t^\tr \otimes \hat{\vcx}_{t|\nD}^\tr
        %%row 3 column3
        & \vcu_t \vcu_t^\tr 
        \end{bmatrix},
\end{equation}
are positive definite and thus invertible. Therefore, we can set the derived partial derivatives to zero and subsequently obtain \eqref{eq: closed form M}-\eqref{eq: closed form Sx0} by solving the resulting system of equations, which concludes the proof.
\qed
%========================================================
%========================================================
%========================================================
\section{Proof of Corollary 1}\label{app:Proof of Corollary 1}
In Proposition~\ref{pro:local minimum}, we have verified that $\nabla_\vctheta \,  J_k = 0$ has a unique solution. Thus, to conclude the invexity of $J_k$, we need to show that $J_k$ is radially unbounded, i.e., $\lim_{\|\vctheta\|\to\infty} \, J_k(\vctheta) = \infty$ and $\lim_{\vctheta\to\partial\Theta} \, J_k(\vctheta) = \infty$, where $\partial\Theta$ denotes the boundary of $\Theta$.
%
%
% For the first step, notice that the expectation is with respect to $\mathbf{x}$, which can be obtained using estimates from the RTS smoother as in \eqref{eq:mean and covariance} and \eqref{eq:correlation}. These estimates are constant and do not influence whether $J_k$ is invex or not with respect to $\vctheta$. Thus, to prove that $J_k$ is radially unbounded with respect to $\vctheta$, we do not need to consider the expectation in \eqref{eq:Q function3}. 
%To this end, it is enough to examine this property for each parameter. 
One can easily see that $J_k$ is a strongly convex quadratic function with respect to tuple $(\mxM,\mxN,\mu_{\vcx_0})$, 
%$(\mxM$, $\mxN$ and $\mu_{\vcx_0}$, 
which implies the mentioned property with respect to those variables. Therefore, we only need to consider $\mxS_\vcv, \mxS_\vcw$ and $\mxS_{\vcx_0}$. We prove the claim for $\mxS_\vcv$ considering that the other two cases can be shown similarly, i.e., we show that
\begin{equation}
\begin{split}
    V_k := \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} \left[\frac{\nD}{2}\logdet\mxS_\vcv  +\frac{1}{2}\sum_{t=0}^{\nD-1} \left( \left(\vcy_t - \mxN \begin{bmatrix} \vcx_t\\ \vcu_t \otimes \vcx_t \\ \vcu_t\end{bmatrix}\right)^\tr \mxS_\vcv^{-1}
     \left( \vcy_t -  \mxN \begin{bmatrix} \vcx_t\\ \vcu_t \otimes \vcx_t \\ \vcu_t\end{bmatrix}\right) \right)\right]
\end{split}
\end{equation}
is radially unbounded with respect to $\mxS_\vcv$. Since $\mxS_\vcv$ is positive definite, let $\mxS_\vcv := \mxV^\tr \Lambda \mxV$, where $\mxV^\tr \mxV = \mathbf{I}_{\Ny}$. Furthermore, define $\vca_t :=  \mxV \left(\vcy_t - \mxN \begin{bmatrix} \vcx_t\\ \vcu_t \otimes \vcx_t \\ \vcu_t\end{bmatrix} \right)$. Under these notations, we can rewrite $V_k$ as
% \begin{equation}
% \begin{split}
\begin{align}
    V_k &= \frac{\nD}{2} \log\det (\mxV^\tr \Lambda \mxV) + \frac{1}{2} \sum_{t=0}^{\nD-1} \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} \big[ \vca_t^\tr \Lambda^{-1} \vca_t \big]\\
    &= \frac{\nD}{2} \log\det \Lambda + \frac{1}{2} \sum_{t=0}^{\nD-1} \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} \big[ \vca_t^\tr \Lambda^{-1} \vca_t \big]\\
    &= \frac{\nD}{2} \log\det \Lambda + \frac{1}{2} \trace \left( \Lambda^{-1} \sum_{t=0}^{\nD-1} \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} \big[\vca_t \vca_t^\tr \big] \right)\\
    &= \frac{\nD}{2} \log\det \Lambda + \frac{1}{2} \trace \left( \Lambda^{-1} \mxV\mxG_k(\mxN)\mxV^\tr \right)
    \\&
    \geq \frac{\nD}{2} \log\det \Lambda + \frac{\lambda_{\min} \left(\mxY - \Pi^\tr \Psi^{-1} \Pi \right)}{2} \trace \left( \Lambda^{-1} \right).
\end{align}
% \end{split}
% \end{equation}
The last inequality holds since $\mxG_k(\mxN) \succeq \lambda_{\min} \left(\mxY - \Pi^\tr \Psi^{-1} \Pi \right)\mathbf{I}$ and $\mxV$ is full rank. Finally, Lemma~\ref{lem:invexity_logdet_trinv} implies that $V_k$ is radially unbounded, which concludes the proof. 

% if $\, \sum_{t=0}^{\nD-1} \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} \big[\vca_t \vca_t^\tr \big] \succ 0$, which is equivalent to
% \begin{equation}\label{eq:PD}
%     \sum_{t=0}^{\nD-1} \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} \big[\vca_t \vca_t^\tr \big] = \mxV\mxG_k(\mxN)\mxV^\tr \succ 0.
% \end{equation}
\qed
%========================================================
%========================================================
%========================================================
% \section{Proof of Proposition 3}\label{app:Proof of pro PD}
% To show \eqref{eq: closed form M}-\eqref{eq: closed form Sx0} is a feasible solution for \eqref{eq:EM optimization J_k}. We need to prove $\hat{\mxS}_{\vcw,k+1}$, $\hat{\mxS}_{\vcv,k+1}$ and $\hat{\mxS}_{\vcx_0,k+1}$ are positive definite matrices. Consider $\hat{\mxS}_{\vcv,k+1}$ for example. We need to show 
% \begin{equation}\label{eq:hat S_v PD}
%     \begin{split}
%         \hat{\mxS}_{\vcv,k+1}
%         =&  \sum_{t=0}^{\nD-1} \vcy_t  \vcy_t^\tr -  \sum_{t=0}^{\nD-1} \vcy_t(\begin{bmatrix}1\\ \vcu_t \end{bmatrix} \otimes \hat{\vcx}_{t|\nD})^\tr
%          \Big(\sum_{t=0}^{\nD-1} \begin{bmatrix}1\\ \vcu_t \end{bmatrix} \begin{bmatrix}1\\ \vcu_t \end{bmatrix}^\tr \otimes (\hat{\vcx}_{t|\nD}\hat{\vcx}_{t|\nD}^\tr + \mxP_{t|\nD})\Big)^{-1} 
%         \sum_{t=0}^{\nD-1} (\begin{bmatrix}1\\ \vcu_t \end{bmatrix} \otimes \hat{\vcx}_{t|\nD}) \vcy_t^\tr \succ 0\\
%     \end{split}
% \end{equation}
% % To simplify the notation, define
% % \begin{equation}
% %     \begin{split}
% %         \vcp &:= \sum_{t=0}^{\nD-1} (\begin{bmatrix}1\\ \vcu_t \end{bmatrix} \otimes \hat{\vcx}_{t|\nD})\vcy_t^\tr\\
% %         \mxR_1 &:= \sum_{t=0}^{\nD-1} (\begin{bmatrix}1\\ \vcu_t \end{bmatrix}\begin{bmatrix}1\\ \vcu_t \end{bmatrix}^\tr \otimes \hat{x}_{t|\nD} \hat{x}_{t|\nD}^\tr)\\
% %         \mxR_2 &:= \sum_{t=0}^{\nD-1} (\begin{bmatrix}1\\ \vcu_t \end{bmatrix}\begin{bmatrix}1\\ \vcu_t \end{bmatrix}^\tr \otimes \mxP_{t|\nD})\\
% %         \mxY &:= \sum_{t=0}^{\nD-1} \vcy_t\vcy_t^\tr
% %     \end{split}
% % \end{equation} 
% % The above equation can be written as
% % \begin{equation}
% %     \mxY - \vcp^\tr \left(\mxR_1 + \mxR_2 \right)^{-1} \vcp.
% % \end{equation}
% % Using Schur complement, the above equation is positive definite if and only if
% % \begin{equation}\label{eq:PD1}
% %     \mxR_1 + \mxR_2 \succ 0,
% % \end{equation}
% % and
% % \begin{equation}\label{eq:PD2}
% % \begin{split}
% %     \begin{bmatrix}
% %         \mxR_1 + \mxR_2 & \vcp^\tr\\
% %         \vcp & \mxY
% %     \end{bmatrix} 
% %     \succ 0.
% % \end{split}
% % \end{equation}
% % For \eqref{eq:PD1}, since $\mxS_\vcw \succ 0$ and $\mxS_{\vcx_0} \succ 0$, then $\mxP_{t|\nD} \succ 0$. Additionally, we assume $\sum_{t=0}^{\nD-1}\begin{bmatrix}1\\ \vcu_t \end{bmatrix}\begin{bmatrix}1\\ \vcu_t \end{bmatrix}^\tr \succ 0$. Thus, $\mxR_2 \succ 0$. Therefore, \eqref{eq:PD1} holds.
% % For \eqref{eq:PD2}, it is equivalent to
% % \begin{equation}
% %    \begin{bmatrix}
% %        \mathbf{I} & \vcp\mxY^{-1}\\
% %        \mathbf{0} & \mathbf{I}
% %    \end{bmatrix}
% %        \begin{bmatrix}
% %        \mxR_1 + \mxR_2 - \vcp^\tr\mxY\vcp & \mathbf{0}\\
% %        \mathbf{0} & \mxY
% %    \end{bmatrix}
% %        \begin{bmatrix}
% %        \mathbf{I} & \mathbf{0}\\
% %        \mxY^{-1}\vcp^\tr & \mathbf{I}
% %   \end{bmatrix},
% % \end{equation}
% % which is also positive definite since $\mxR_2$ and $\mxY$ are positive definite. Thus, \eqref{eq:PD} holds.
% Using Schur complement, the above equation is positive definite if and only if
% \begin{equation}\label{eq:PD1}
%     \sum_{t=0}^{\nD-1} \left(\begin{bmatrix}1\\ \vcu_t \end{bmatrix}\begin{bmatrix}1\\ \vcu_t \end{bmatrix}^\tr \otimes (\hat{x}_{t|\nD} \hat{x}_{t|\nD}^\tr + \mxP_{t|\nD}) \right) \succ 0,
% \end{equation}
% and
% \begin{equation}\label{eq:PD2}
% \begin{split}
%     \sum_{t=0}^{\nD-1}
%     \begin{bmatrix}
%          \begin{bmatrix}1\\ \vcu_t \end{bmatrix}\begin{bmatrix}1\\ \vcu_t \end{bmatrix}^\tr \otimes (\hat{x}_{t|\nD} \hat{x}_{t|\nD}^\tr + \mxP_{t|\nD})   &  \quad \vcy_t(\begin{bmatrix}1\\ \vcu_t \end{bmatrix} \otimes \hat{\vcx}_{t|\nD})^\tr\\
%          (\begin{bmatrix}1\\ \vcu_t \end{bmatrix} \otimes \hat{\vcx}_{t|\nD})\vcy_t^\tr & \quad \vcy_t\vcy_t^\tr
%     \end{bmatrix} 
%     \succ 0.
% \end{split}
% \end{equation}

% For \eqref{eq:PD1}, $\mxP_{t|\nD}$ is positive definite, since $\mxS_\vcw \succ 0$ and $\mxS_{\vcx_0} \succ 0$. Additionally, we assume $\sum_{t=0}^{\nD-1}\begin{bmatrix}1\\ \vcu_t \end{bmatrix}\begin{bmatrix}1\\ \vcu_t \end{bmatrix}^\tr \succ 0$. Thus, $\sum_{t=0}^{\nD-1} \begin{bmatrix}1\\ \vcu_t \end{bmatrix}\begin{bmatrix}1\\ \vcu_t \end{bmatrix}^\tr \otimes \mxP_{t|\nD} \succ 0$ and \eqref{eq:PD1} holds.

% For \eqref{eq:PD2}, it is equivalent to
% \begin{equation}
% \begin{split}
%     \sum_{t=0}^{\nD-1}
%     \begin{bmatrix}
%          \begin{bmatrix}1\\ \vcu_t \end{bmatrix}\begin{bmatrix}1\\ \vcu_t \end{bmatrix}^\tr \otimes \mxP_{t|\nD}   &  \mathbf{0}\\
%          \mathbf{0} & \quad \vcy_t\vcy_t^\tr
%     \end{bmatrix} 
%     +
%     \sum_{t=0}^{\nD-1}
%     \begin{bmatrix}
%          \begin{bmatrix}1\\ \vcu_t \end{bmatrix}\begin{bmatrix}1\\ \vcu_t \end{bmatrix}^\tr \otimes \hat{x}_{t|\nD} \hat{x}_{t|\nD}^\tr   &  \quad \vcy_t(\begin{bmatrix}1\\ \vcu_t \end{bmatrix} \otimes \hat{\vcx}_{t|\nD})^\tr\\
%          (\begin{bmatrix}1\\ \vcu_t \end{bmatrix} \otimes \hat{\vcx}_{t|\nD})\vcy_t^\tr & \mathbf{0}
%     \end{bmatrix} 
%     \succ 0,
% \end{split}
% \end{equation}
% where the first term is positive definite since $\sum_{t=0}^{\nD-1} \vcy_t\vcy_t^\tr \succ 0$. Thus, \eqref{eq:PD2} also holds, which implies that \eqref{eq:hat S_v PD} holds. Therefore, $\hat{\mxS}_{\vcv,k+1}$ is a positive definite matrix. The other two matrices $\hat{\mxS}_{\vcw,k+1}$ and $\hat{\mxS}_{\vcx_0,k+1}$ can be proved to be positive definite matrices in a similar way. Thus, \eqref{eq: closed form M}-\eqref{eq: closed form Sx0} is a feasible solution of \eqref{eq:EM optimization J_k}, which also implies the recursive feasibility.
% \qed

% \cmm{
% We show this by induction. Assume we have a feasible solution 
% $\hat{\vctheta}_k$. From, \eqref{eq: closed form Sw} and \eqref{eq: closed form M}, we know
% \begin{equation}\label{eq:S_w_k+1}
% \begin{split}
%     \hat{\mxS}_{\vcw,k+1} &= 
%         \frac{1}{\nD} \sum_{t=0}^{\nD-1}  \Big[(\hat{\vcx}_{t+1|\nD}\hat{\vcx}_{t+1|\nD}^\tr + \mxP_{t+1|\nD}) -  \begin{bmatrix}\hat{\vcx}_{t+1|{\nD}}\hat{\vcx}_{t|{\nD}}^\tr + \mxP_{t+1|{\nD}}\mxH_t^\tr &\quad \hat{\vcx}_{t+1|\nD} \vcu_t^\tr  \end{bmatrix} \hat{\mxM}_{k+1}^\tr  \nonumber \\
%         &\quad -  \hat{\mxM}_{k+1}\begin{bmatrix}\hat{\vcx}_{t+1|\nD}\hat{\vcx}_{t|\nD}^\tr + \mxP_{t+1|\nD}\mxH_t^\tr &\quad \hat{\vcx}_{t+1|\nD} \vcu_t^\tr  \end{bmatrix}^\tr + \hat{\mxM}_{k+1} \begin{bmatrix}\hat{\vcx}_{t|\nD}\hat{\vcx}_{t|\nD}^\tr \!+\! \mxP_{t|\nD} & \hat{\vcx}_{t|\nD}\vcu_t^\tr\\ \vcu_t\hat{\vcx}_{t|\nD}^\tr &\vcu_t \vcu_t^\tr\end{bmatrix} \hat{\mxM}_{k+1}^\tr \Big] \\
%         %%
%         &= \frac{1}{\nD} \sum_{t=0}^{\nD-1} 
%         \big[ (\hat{\vcx}_{t+1|\nD} - 
%         \hat{\mxM}_{k+1}
%         \begin{bmatrix}
%             \hat{\vcx}_{t+1|\nD} \\ \vcu_t
%         \end{bmatrix}
%         )
%         (\hat{\vcx}_{t+1|\nD} - 
%         \hat{\mxM}_{k+1}
%         \begin{bmatrix}
%             \hat{\vcx}_{t+1|\nD} \\ \vcu_t
%         \end{bmatrix}
%         )^\tr
%         \\
%         &\quad + \mxP_{t+1|\nD} -  \mxP_{t+1|\nD}\mxH_t^\tr \hat{\mxA}_{k+1}^\tr -\hat{\mxA}_{k+1} \mxH_t \mxP_{t+1|\nD} + \hat{\mxA}_{k+1}\mxP_{t|\nD}\hat{\mxA}_{k+1}^\tr\big].
% \end{split}
% \end{equation}

% To show the above equation is positive definite.First consider $\mxP_{t|\nD}$,
% \begin{equation}
% \begin{split}
%     \mxP_{t|\nD} 
%     &= \mxP_{t|t} + \mxH_t(\mxP_{t+1|\nD} - \mxP_{t+1|t})\mxH_t^\tr\\
%     %%
%     &= \mxP_{t|t} - \mxH_t\mxP_{t+1|t}\mxH_t^\tr + \mxH_t\mxP_{t+1|\nD}\mxH_t^\tr\\
%     %%
%     &= \mxP_{t|t} - \mxP_{t|t}\hat{\mxA}_k^\tr\mxP_{t+1|t}^{-1}\hat{\mxA}_k\mxP_{t|t} +\mxH_t\mxP_{t+1|\nD}\mxH_t^\tr\\
%     %%
%     &=(\mxP_{t|t}^{-1} + \hat{\mxA}_k \hat{\mxS}_{\vcw,k} \hat{\mxA}_k)^{-1} + \mxH_t\mxP_{t+1|\nD}\mxH_t^\tr,
% \end{split}
% \end{equation}
% where the last equality we use Lemma~\ref{lem:matrix_inversion_lemma}. Therefore,
% \begin{equation}
% \begin{split}
%     \hat{\mxS}_{\vcw,k+1} &= 
%     \frac{1}{\nD} \sum_{t=0}^{\nD-1} 
%         \big[ (\hat{\vcx}_{t+1|\nD} - 
%         \hat{\mxM}_{k+1}
%         \begin{bmatrix}
%             \hat{\vcx}_{t+1|\nD} \\ \vcu_t
%         \end{bmatrix}
%         )
%         (\hat{\vcx}_{t+1|\nD} - 
%         \hat{\mxM}_{k+1}
%         \begin{bmatrix}
%             \hat{\vcx}_{t+1|\nD} \\ \vcu_t
%         \end{bmatrix}
%         )^\tr
%         \\& \quad
%         + (\mxP_{t+1|\nD}^{\frac{1}{2}} - \hat{\mxA}_{k+1}\mxH_t\mxP_{t+1|\nD}^{\frac{1}{2}})(\mxP_{t+1|\nD}^{\frac{1}{2}} - \hat{\mxA}_{k+1}\mxH_t\mxP_{t+1|\nD}^{\frac{1}{2}})^\tr 
%         + \hat{\mxA}_{k+1}\mxH_t(\mxP_{t|t}^{-1} + \hat{\mxA}_k \hat{\mxS}_{\vcw,k} \hat{\mxA}_k)^{-1}\mxH_t^\tr\hat{\mxA}_{k+1}^\tr \big]
% \end{split}
% \end{equation}}

\section{Proof of Proposition 4}\label{app:recursive feaisbility}
% We need to show that $\hat{\mxS}_{\vcw,k+1}$, $\hat{\mxS}_{\vcv,k+1}$ and $\hat{\mxS}_{\vcx_0,k+1}$ are positive definite matrices, which subsequently implies that the solution given by \eqref{eq: closed form M}-\eqref{eq: closed form Sx0} is feasible for \eqref{eq:EM optimization J_k}. 
% %We take $\hat{\mxS}_{\vcv,k+1}$ for example. The other two terms can shown in a similar way. 
% To this end, we verify the positive-definiteness only for $\hat{\mxS}_{\vcv,k+1}$, as the similar line of argument holds for other two terms. 
% By substituting \eqref{eq: closed form C} into \eqref{eq:G_theta|hat_theta PD 2} and comparing with \eqref{eq: closed form Sv}, we can observe that
%     \begin{equation}
%        \hat{\mxS}_{\vcv,k+1} = \frac{1}{\nD}\mxG_k(\hat{\mxN}_{k+1}).
%     \end{equation}
% From Proposition~\ref{pro: postive definite}, we show that $\mxG_k(\mxN)$ is positive definite for any $\mxN \in \Rbb^{\Ny \times (\Nu + \Nx + \Nx\Nu)}$, which implies that $\hat{\mxS}_{\vcv,k+1}$ is positive definite and concludes the proof.
We need to show that $\hat{\mxS}_{\vcw,k+1}$, $\hat{\mxS}_{\vcv,k+1}$ and $\hat{\mxS}_{\vcx_0,k+1}$ are positive definite matrices, which subsequently implies that \eqref{eq: closed form M}-\eqref{eq: closed form Sx0} provide a feasible solution for \eqref{eq:EM optimization J_k}. 
% %We take $\hat{\mxS}_{\vcv,k+1}$ for example. The other two terms can shown in a similar way. 
% To this end, we verify the positive-definiteness only for $\hat{\mxS}_{\vcv,k+1}$, as the similar line of argument holds for other two terms. 
For $\hat{\mxS}_{\vcw,k+1}$, by substituting \eqref{eq: closed form C} into \eqref{eq:G_theta|hat_theta PD 2} and comparing with \eqref{eq: closed form Sv}, we have
    \begin{equation}
       \hat{\mxS}_{\vcv,k+1} = \frac{1}{\nD}\mxG_k(\hat{\mxN}_{k+1}).
    \end{equation}
From Proposition~\ref{pro: postive definite}, we know that $\mxG_k(\mxN)$ is positive definite for any $\mxN \in \Rbb^{\Ny \times (\Nu + \Nx + \Nx\Nu)}$, which implies that $\hat{\mxS}_{\vcv,k+1}$ is positive definite.
One can see that similar line of argument holds for $\hat{\mxS}_{\vcv,k+1}$ and $\hat{\mxS}_{\vcx_0,k+1}$, 
which concludes the proof.
\qed

%========================================================
%========================================================
%========================================================



% \section{Proof of Corollary 1}\label{app:Proof of Corollary 1}
% In Proposition~\ref{pro:local minimum}, we haven shown that $\nabla_\vctheta \,  J_k = 0$ has a unique solution. To show that $J_k$ is invex and has a unique local minimum. We need to show that the cost function $J_k$ is radially unbounded, i.e., $\lim_{\|\vctheta\|\to\infty} \, J_k(\vctheta) = \infty$ and $\lim_{\|\vctheta\|\to\partial\Theta} \, J_k(\vctheta) = \infty$, where $\partial\Theta$ is the boundary of $\Theta$.


% % For the first step, notice that the expectation is with respect to $\mathbf{x}$, which can be obtained using estimates from the RTS smoother as in \eqref{eq:mean and covariance} and \eqref{eq:correlation}. These estimates are constant and do not influence whether $J_k$ is invex or not with respect to $\vctheta$. Thus, to prove that $J_k$ is radially unbounded with respect to $\vctheta$, we do not need to consider the expectation in \eqref{eq:Q function3}. 
% To this end, we consider each parameter. First, it is obvious that $J_k$ is quadratic with respect to $\mxM$, $\mxN$ and $\mu_{\vcx_0}$, which means that $J_k$ will go to infinity when one of them goes to infinity. Therefore, we only need to consider $\mxS_\vcv, \mxS_\vcw$ and $\mxS_{\vcx_0}$. We take $\mxS_\vcv$ as an example since the other two can be proved similarly. That is, we need to prove
% \begin{equation}
% \begin{split}
%     V_k := \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} \left[\frac{\nD}{2}\logdet\mxS_\vcv  +\frac{1}{2}\sum_{t=0}^{\nD-1} \left( \left(\vcy_t - \mxN \begin{bmatrix} \vcx_t\\ \vcu_t \otimes \vcx_t \\ \vcu_t\end{bmatrix}\right)^\tr \mxS_\vcv^{-1}
%      \left( \vcy_t -  \mxN \begin{bmatrix} \vcx_t\\ \vcu_t \otimes \vcx_t \\ \vcu_t\end{bmatrix}\right) \right)\right]
% \end{split}
% \end{equation}
% is radially unbounded with respect to $\mxS_\vcv$. Since $\mxS_\vcv$ is positive definite, let $\mxS_\vcv := \mxV^\tr \Lambda \mxV$, where $\mxV^\tr \mxV = \mathbf{I}_{\Ny}$. Furthermore, define $\vca_t :=  \mxV \left(\vcy_t - \mxN \begin{bmatrix} \vcx_t\\ \vcu_t \otimes \vcx_t \\ \vcu_t\end{bmatrix} \right)$. Under these notations, we can rewrite $V_k$ as
% % \begin{equation}
% % \begin{split}
% \begin{align}
%     V_k &= \frac{\nD}{2} \log\det (\mxV^\tr \Lambda \mxV) + \frac{1}{2} \sum_{t=0}^{\nD-1} \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} \big[ \vca_t^\tr \Lambda^{-1} \vca_t \big]\\
%     &= \frac{\nD}{2} \log\det \Lambda + \frac{1}{2} \sum_{t=0}^{\nD-1} \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} \big[ \vca_t^\tr \Lambda^{-1} \vca_t \big]\\
%     &= \frac{\nD}{2} \log\det \Lambda + \frac{1}{2} \trace \left( \Lambda^{-1} \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} \big[\vca_t \vca_t^\tr \big] \right).
% \end{align}
% % \end{split}
% % \end{equation}
% From Lemma~\ref{lem:invexity_logdet_trinv}, we know $V_k$ is radially unbounded if $\, \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} \big[\vca_t \vca_t^\tr \big] \succ 0$, which is equivalent to
% \begin{equation}\label{eq:PD}
%     \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} \big[\vca_t \vca_t^\tr \big] = \mxV\mxG_k(\mxN)\mxV^\tr \succ 0.
% \end{equation}
% The above equation holds since $\mxG_k(\mxN) \succ 0$ and $\mxV$ is full rank, which concludes the proof.
% \qed


% \section{Proof of Corollary 1}\label{app:Proof of Corollary 1}
% In Proposition~\ref{pro:local minimum}, we haven shown that $\nabla_\vctheta \,  J_k = 0$ has a unique solution. To show that $J_k$ is invex and has a unique local minimum. We need to show that the cost function $J_k$ is radially unbounded, i.e., $\lim_{\|\vctheta\|\to\infty} \, J_k(\vctheta) = \infty$ and $\lim_{\|\vctheta\|\to\partial\Theta} \, J_k(\vctheta) = \infty$, where $\partial\Theta$ is the boundary of $\Theta$.


% % For the first step, notice that the expectation is with respect to $\mathbf{x}$, which can be obtained using estimates from the RTS smoother as in \eqref{eq:mean and covariance} and \eqref{eq:correlation}. These estimates are constant and do not influence whether $J_k$ is invex or not with respect to $\vctheta$. Thus, to prove that $J_k$ is radially unbounded with respect to $\vctheta$, we do not need to consider the expectation in \eqref{eq:Q function3}. 
% To this end, we consider each parameter. First, it is obvious that $J_k$ is quadratic with respect to $\mxM$, $\mxN$ and $\mu_{\vcx_0}$, which means that $J_k$ will go to infinity when one of them goes to infinity. Therefore, we only need to consider $\mxS_\vcv, \mxS_\vcw$ and $\mxS_{\vcx_0}$. We take $\mxS_\vcv$ as an example since the other two can be proved similarly. That is, we need to prove
% \begin{equation}
% \begin{split}
%     V_k := \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} \left[\frac{\nD}{2}\logdet\mxS_\vcv  +\frac{1}{2}\sum_{t=0}^{\nD-1} \left[ \left(\vcy_t - \mxC(\begin{bmatrix}1\\ \vcu_t \end{bmatrix} \otimes \vcx_t) \right)^\tr \mxS_\vcv^{-1}
%         \left(\vcy_t - \mxC (\begin{bmatrix}1\\ \vcu_t \end{bmatrix} \otimes \vcx_t) \right) \right]\right]
% \end{split}
% \end{equation}
% is radially unbounded with respect to $\mxS_\vcv$. Since $\mxS_\vcv$ is positive, let $\mxS_\vcv := \mxV^\tr \Lambda \mxV$, where $\mxV^\tr \mxV = \mathbf{I}_{\Ny}$. Furthermore, define $\vca_t :=  \mxV \left(\vcy_t - \mxC (\begin{bmatrix}1\\ \vcu_t \end{bmatrix} \otimes \vcx_t) \right)$. Under these notation, we can rewrite $V_k$ as
% \begin{equation}
% \begin{split}
%     V_k &= \frac{\nD}{2} \log\det (\mxV^\tr \Lambda \mxV) + \frac{1}{2} \sum_{t=0}^{\nD-1} \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} \big[ \vca_t^\tr \Lambda^{-1} \vca_t \big]\\
%     &= \frac{\nD}{2} \log\det \Lambda + \frac{1}{2} \sum_{t=0}^{\nD-1} \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} \big[ \vca_t^\tr \Lambda^{-1} \vca_t \big]\\
%     &= \frac{\nD}{2} \log\det \Lambda + \frac{1}{2} \trace \left( \Lambda^{-1} \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} \big[\vca_t \vca_t^\tr \big] \right).
% \end{split}
% \end{equation}
% From Lemma~\ref{lem:invexity_logdet_trinv}, we know $V_k$ is radially unbounded if $\, \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} \big[\vca_t \vca_t^\tr \big] \succ 0$, which is equivalent to
% \begin{equation}\label{eq:PD}
%     \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})}
%         \Bigg[\sum_{t=0}^{\nD-1} \left(\vcy_t - \mxC(\begin{bmatrix}1\\ \vcu_t \end{bmatrix} \otimes \vcx_t) \right)
%         \left(\vcy_t - \mxC (\begin{bmatrix}1\\ \vcu_t \end{bmatrix} \otimes \vcx_t) \right)^\tr\Bigg] \succ 0
% \end{equation}
% Using the closed form solution in \eqref{eq: closed form C}
% \begin{equation}
%     \begin{split}
%         &\mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})}
%         \Bigg[\sum_{t=0}^{\nD-1} \left(\vcy_t - \mxC(\begin{bmatrix}1\\ \vcu_t \end{bmatrix} \otimes \vcx_t) \right)
%         \left(\vcy_t - \mxC (\begin{bmatrix}1\\ \vcu_t \end{bmatrix} \otimes \vcx_t) \right)^\tr\Bigg]\\
%         =&  \sum_{t=0}^{\nD-1} \vcy_t  \vcy_t^\tr -  \sum_{t=0}^{\nD-1} \vcy_t(\begin{bmatrix}1\\ \vcu_t \end{bmatrix} \otimes \hat{\vcx}_{t|\nD})^\tr
%          \Big(\sum_{t=0}^{\nD-1} \begin{bmatrix}1\\ \vcu_t \end{bmatrix} \begin{bmatrix}1\\ \vcu_t \end{bmatrix}^\tr \otimes (\hat{\vcx}_{t|\nD}\hat{\vcx}_{t|\nD}^\tr + \mxP_{t|\nD})\Big)^{-1} 
%         \sum_{t=0}^{\nD-1} (\begin{bmatrix}1\\ \vcu_t \end{bmatrix} \otimes \hat{\vcx}_{t|\nD}) \vcy_t^\tr.\\
%     \end{split}
% \end{equation}
% The last line is exact \eqref{eq:hat S_v PD} and is positive definite as we have shown in Appendix~\ref{app:Proof of pro PD}, which concludes the proof.
% \qed




% \section{Proof of Corollary 1}\label{app:Proof of Corollary 1}
% In Proposition~\ref{pro:local minimum}, we haven shown that $\nabla_\vctheta \,  J_k = 0$ has a unique solution. To show that $J_k$ is invex and has a unique local minimum. We need to show that the cost function $J_k$ is radially unbounded, i.e., $\lim_{\|\vctheta\|\to\infty} \, J_k(\vctheta) = \infty$.


% % For the first step, notice that the expectation is with respect to $\mathbf{x}$, which can be obtained using estimates from the RTS smoother as in \eqref{eq:mean and covariance} and \eqref{eq:correlation}. These estimates are constant and do not influence whether $J_k$ is invex or not with respect to $\vctheta$. Thus, to prove that $J_k$ is radially unbounded with respect to $\vctheta$, we do not need to consider the expectation in \eqref{eq:Q function3}. 
% To this end, we consider each parameter. First, it is obvious that $J_k$ is quadratic with respect to $\mxM$, $\mxC$ and $\mu_{\vcx_0}$, which means that $J_k$ will go to infinity when one of them goes to infinity. Therefore, we only need to consider $\mxS_\vcv, \mxS_\vcw$ and $\mxS_{\vcx_0}$. We take $\mxS_\vcv$ as an example since the other two can be proved similarly. That is, we need to prove
% \begin{equation}
% \begin{split}
%     V_k := \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} \left[\frac{\nD}{2}\logdet\mxS_\vcv  +\frac{1}{2}\sum_{t=0}^{\nD-1} \left[ \left(\vcy_t - \mxC(\begin{bmatrix}1\\ \vcu_t \end{bmatrix} \otimes \vcx_t) \right)^\tr \mxS_\vcv^{-1}
%         \left(\vcy_t - \mxC (\begin{bmatrix}1\\ \vcu_t \end{bmatrix} \otimes \vcx_t) \right) \right]\right]
% \end{split}
% \end{equation}
% is radially unbounded with respect to $\mxS_\vcv$. Since $\mxS_\vcv$ is positive, let $\mxS_\vcv := \mxV^\tr \Lambda \mxV$, where $\mxV^\tr \mxV = \mathbf{I}_{\Ny}$. Furthermore, define $\vca_t :=  \mxV \left(\vcy_t - \mxC (\begin{bmatrix}1\\ \vcu_t \end{bmatrix} \otimes \vcx_t) \right)$. Under these notation, we can rewrite $V_k$ as
% \begin{equation}
% \begin{split}
%     V_k &= \frac{\nD}{2} \log\det (\mxV^\tr \Lambda \mxV) + \frac{1}{2} \sum_{t=0}^{\nD-1} \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} \big[ \vca_t^\tr \Lambda^{-1} \vca_t \big]\\
%     &= \frac{\nD}{2} \log\det \Lambda + \frac{1}{2} \sum_{t=0}^{\nD-1} \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} \big[ \vca_t^\tr \Lambda^{-1} \vca_t \big].
% \end{split}
% \end{equation}
% First, we show $\sum_{t=0}^{\nD-1} \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} \big[\vca_t \vca_t^\tr \big] \succ 0$, which is equivalent to
% \begin{equation}\label{eq:PD}
%     \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})}
%         \Bigg[\sum_{t=0}^{\nD-1} \left(\vcy_t - \mxC(\begin{bmatrix}1\\ \vcu_t \end{bmatrix} \otimes \vcx_t) \right)
%         \left(\vcy_t - \mxC (\begin{bmatrix}1\\ \vcu_t \end{bmatrix} \otimes \vcx_t) \right)^\tr\Bigg] \succ 0
% \end{equation}
% Using the closed form solution in \eqref{eq: closed form C}
% \begin{equation}
%     \begin{split}
%         &\mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})}
%         \Bigg[\sum_{t=0}^{\nD-1} \left(\vcy_t - \mxC(\begin{bmatrix}1\\ \vcu_t \end{bmatrix} \otimes \vcx_t) \right)
%         \left(\vcy_t - \mxC (\begin{bmatrix}1\\ \vcu_t \end{bmatrix} \otimes \vcx_t) \right)^\tr\Bigg]\\
%         =&  \sum_{t=0}^{\nD-1} \vcy_t  \vcy_t^\tr -  \sum_{t=0}^{\nD-1} \vcy_t(\begin{bmatrix}1\\ \vcu_t \end{bmatrix} \otimes \hat{\vcx}_{t|\nD})^\tr
%          \Big(\sum_{t=0}^{\nD-1} \begin{bmatrix}1\\ \vcu_t \end{bmatrix} \begin{bmatrix}1\\ \vcu_t \end{bmatrix}^\tr \otimes (\hat{\vcx}_{t|\nD}\hat{\vcx}_{t|\nD}^\tr + \mxP_{t|\nD})\Big)^{-1} 
%         \sum_{t=0}^{\nD-1} (\begin{bmatrix}1\\ \vcu_t \end{bmatrix} \otimes \hat{\vcx}_{t|\nD}) \vcy_t^\tr\\
%     \end{split}
% \end{equation}


% % To simplify the notation, define
% % \begin{equation}
% %     \begin{split}
% %         \vcp &:= \sum_{t=0}^{\nD-1} (\begin{bmatrix}1\\ \vcu_t \end{bmatrix} \otimes \hat{\vcx}_{t|\nD})\vcy_t^\tr\\
% %         \mxR_1 &:= \sum_{t=0}^{\nD-1} (\begin{bmatrix}1\\ \vcu_t \end{bmatrix}\begin{bmatrix}1\\ \vcu_t \end{bmatrix}^\tr \otimes \hat{x}_{t|\nD} \hat{x}_{t|\nD}^\tr)\\
% %         \mxR_2 &:= \sum_{t=0}^{\nD-1} (\begin{bmatrix}1\\ \vcu_t \end{bmatrix}\begin{bmatrix}1\\ \vcu_t \end{bmatrix}^\tr \otimes \mxP_{t|\nD})\\
% %         \mxY &:= \sum_{t=0}^{\nD-1} \vcy_t\vcy_t^\tr
% %     \end{split}
% % \end{equation} 
% % The above equation can be written as
% % \begin{equation}
% %     \mxY - \vcp^\tr \left(\mxR_1 + \mxR_2 \right)^{-1} \vcp.
% % \end{equation}
% % Using Schur complement, the above equation is positive definite if and only if
% % \begin{equation}\label{eq:PD1}
% %     \mxR_1 + \mxR_2 \succ 0,
% % \end{equation}
% % and
% % \begin{equation}\label{eq:PD2}
% % \begin{split}
% %     \begin{bmatrix}
% %         \mxR_1 + \mxR_2 & \vcp^\tr\\
% %         \vcp & \mxY
% %     \end{bmatrix} 
% %     \succ 0.
% % \end{split}
% % \end{equation}
% % For \eqref{eq:PD1}, since $\mxS_\vcw \succ 0$ and $\mxS_{\vcx_0} \succ 0$, then $\mxP_{t|\nD} \succ 0$. Additionally, we assume $\sum_{t=0}^{\nD-1}\begin{bmatrix}1\\ \vcu_t \end{bmatrix}\begin{bmatrix}1\\ \vcu_t \end{bmatrix}^\tr \succ 0$. Thus, $\mxR_2 \succ 0$. Therefore, \eqref{eq:PD1} holds.

% % For \eqref{eq:PD2}, it is equivalent to
% % \begin{equation}
% %     \begin{bmatrix}
% %         \mathbf{I} & \vcp\mxY^{-1}\\
% %         \mathbf{0} & \mathbf{I}
% %     \end{bmatrix}
% %         \begin{bmatrix}
% %         \mxR_1 + \mxR_2 - \vcp^\tr\mxY\vcp & \mathbf{0}\\
% %         \mathbf{0} & \mxY
% %     \end{bmatrix}
% %         \begin{bmatrix}
% %         \mathbf{I} & \mathbf{0}\\
% %         \mxY^{-1}\vcp^\tr & \mathbf{I}
% %     \end{bmatrix},
% % \end{equation}
% % which is also positive definite since $\mxR_2$ and $\mxY$ are positive definite. Thus, \eqref{eq:PD} holds.

% Using Schur complement, the above equation is positive definite if and only if
% \begin{equation}\label{eq:PD1}
%     \sum_{t=0}^{\nD-1} \left(\begin{bmatrix}1\\ \vcu_t \end{bmatrix}\begin{bmatrix}1\\ \vcu_t \end{bmatrix}^\tr \otimes (\hat{x}_{t|\nD} \hat{x}_{t|\nD}^\tr + \mxP_{t|\nD}) \right) \succ 0,
% \end{equation}
% and
% \begin{equation}\label{eq:PD2}
% \begin{split}
%     \sum_{t=0}^{\nD-1}
%     \begin{bmatrix}
%          \begin{bmatrix}1\\ \vcu_t \end{bmatrix}\begin{bmatrix}1\\ \vcu_t \end{bmatrix}^\tr \otimes (\hat{x}_{t|\nD} \hat{x}_{t|\nD}^\tr + \mxP_{t|\nD})   &  \quad \vcy_t(\begin{bmatrix}1\\ \vcu_t \end{bmatrix} \otimes \hat{\vcx}_{t|\nD})^\tr\\
%          (\begin{bmatrix}1\\ \vcu_t \end{bmatrix} \otimes \hat{\vcx}_{t|\nD})\vcy_t^\tr & \quad \vcy_t\vcy_t^\tr
%     \end{bmatrix} 
%     \succ 0.
% \end{split}
% \end{equation}
% For \eqref{eq:PD1}, since $\mxS_\vcw \succ 0$ and $\mxS_{\vcx_0} \succ 0$, then $\mxP_{t|\nD} \succ 0$. Additionally, we assume $\sum_{t=0}^{\nD-1}\begin{bmatrix}1\\ \vcu_t \end{bmatrix}\begin{bmatrix}1\\ \vcu_t \end{bmatrix}^\tr \succ 0$. Thus, $\sum_{t=0}^{\nD-1} \begin{bmatrix}1\\ \vcu_t \end{bmatrix}\begin{bmatrix}1\\ \vcu_t \end{bmatrix}^\tr \otimes \mxP_{t|\nD} \succ 0$ and \eqref{eq:PD1} holds.

% For \eqref{eq:PD2}, it is equivalent to
% \begin{equation}
% \begin{split}
%     \sum_{t=0}^{\nD-1}
%     \begin{bmatrix}
%          \begin{bmatrix}1\\ \vcu_t \end{bmatrix}\begin{bmatrix}1\\ \vcu_t \end{bmatrix}^\tr \otimes \mxP_{t|\nD}   &  \mathbf{0}\\
%          \mathbf{0} & \quad \vcy_t\vcy_t^\tr
%     \end{bmatrix} 
%     +
%     \sum_{t=0}^{\nD-1}
%     \begin{bmatrix}
%          \begin{bmatrix}1\\ \vcu_t \end{bmatrix}\begin{bmatrix}1\\ \vcu_t \end{bmatrix}^\tr \otimes \hat{x}_{t|\nD} \hat{x}_{t|\nD}^\tr   &  \quad \vcy_t(\begin{bmatrix}1\\ \vcu_t \end{bmatrix} \otimes \hat{\vcx}_{t|\nD})^\tr\\
%          (\begin{bmatrix}1\\ \vcu_t \end{bmatrix} \otimes \hat{\vcx}_{t|\nD})\vcy_t^\tr & \mathbf{0}
%     \end{bmatrix} 
%     \succ 0,
% \end{split}
% \end{equation}
% where the first term is positive definite since $\sum_{t=0}^{\nD-1} \vcy_t\vcy_t^\tr \succ 0$. Thus, \eqref{eq:PD2} also holds, which implies that \eqref{eq:PD} holds.

% Now, we show that $V_k$ is radially unbounded. We know $\sum_{t=0}^{\nD-1} \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} \big[\vca_t \vca_t^\tr \big] \succ 0$. Thus, we can write $V_k$ as 
% \begin{equation}
%     V_k = \frac{\nD}{2} \log\det \Lambda + \frac{1}{2} \trace \left( \Lambda^{-1} \mathbb{E}_{p(\mathbf{x}|\mathbf{y},\hat{\vctheta}_k,\mathbf{u})} \big[\vca_t \vca_t^\tr \big] \right),
% \end{equation}
% which is invex from Lemma~\ref{lem:invexity_logdet_trinv}.
% \qed
