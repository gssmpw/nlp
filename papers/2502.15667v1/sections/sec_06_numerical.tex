\section{Numerical Experiments} \label{sec:numerical experiments}
% % To evaluate the effectiveness and performance of the proposed algorithms, we conduct numerical experiments using Monte Carlo simulations under different noise levels and dataset sizes. 
% To evaluate the proposed algorithms and verify their effectiveness, we perform extensive numerical experiments, including Monte Carlo simulations under different noise levels and dataset sizes. 
% %
% %The normalized relative error is used as the evaluation metric, providing a quantitative measure of the accuracy of the identified parameters relative to the ground truth. 
% Accordingly, we employ suitable evaluation metrics, providing quantitative measure of the accuracy of the identified parameters relative to the ground truth. 
% %
% % Cross-validation is used to ensure the reliability of the results and enable an unbiased evaluation of the proposed approaches. 
% Furthermore, cross-validation is used to ensure the reliability of the results and enable a proper and unbiased evaluation of the proposed approaches. 
% %
% % In addition to accuracy, the computational complexity of the methods is analyzed, focusing on their scalability and runtime behavior as the dataset size increases. 
% In addition to accuracy, the computational complexity of the methods is analyzed, focusing on their scalability and runtime behavior as the size of dataset increases. 
% %
% % The convergence process of the EM approach is also demonstrated to evaluate the stability and efficiency of the algorithm in iterative optimization. 
% The convergence performance of the EM approach is also empirically demonstrated to further evaluate the efficiency of the algorithm.
% %
% % The results show the methods' effectiveness in estimating system parameters and their potential usage in scenarios involving bilinear observational structures. 
% The results show the efficacy of the proposed methods and their practical effectiveness in estimating system parameters and their potential usage in scenarios involving bilinear observational structures. 
%
% To evaluate the proposed algorithms and verify their effectiveness, we perform extensive numerical experiments, including Monte Carlo simulations under different noise levels and dataset sizes. Accordingly, we employ suitable evaluation metrics, providing quantitative measure of the accuracy of the identified parameters relative to the ground truth. Furthermore, cross-validation is used to ensure the reliability of the results and enable a proper and unbiased evaluation of the proposed approaches. In addition to accuracy, the computational complexity of the methods is analyzed, focusing on their scalability and runtime behavior as the size of dataset increases. The convergence performance of the EM approach is also empirically demonstrated to further evaluate the efficiency of the algorithm. The results show the efficacy of the proposed methods and their practical effectiveness in estimating system parameters and their potential usage in scenarios involving bilinear observational structures. 
%
To evaluate the proposed algorithms and verify their effectiveness, we perform extensive numerical experiments, including Monte Carlo simulations under different noise levels and dataset sizes. Accordingly, we employ suitable evaluation metrics, providing quantitative measures of the accuracy of the identified parameters relative to the ground truth. Furthermore, cross-validation is used to ensure the reliability of the results and enable a proper and unbiased evaluation of the proposed approaches. In addition to accuracy, the computational complexity of the methods is analyzed, focusing on their scalability and runtime behavior as the size of the dataset increases. The convergence performance of the EM approach is also empirically demonstrated to evaluate the efficiency of the algorithm further. The results show the efficacy of the proposed methods, their practical effectiveness in estimating system parameters, and their potential usage in scenarios involving bilinear observational structures. 
%
% To assess the proposed algorithms and verify their effectiveness, we conduct extensive numerical experiments, including Monte Carlo simulations under varying noise levels and dataset sizes. We employ suitable evaluation metrics to provide quantitative measures of parameter accuracy relative to the ground truth. Cross-validation is incorporated to ensure the reliability of the results and facilitate an unbiased evaluation of the proposed approaches. Beyond accuracy, we analyze the computational complexity of the methods, emphasizing their scalability and runtime behavior as dataset size increases. Additionally, the convergence performance of the EM approach is empirically demonstrated to further evaluate the algorithm's efficiency. The results validate the efficacy of the proposed methods, highlighting their practical utility in estimating system parameters and their potential applications in scenarios involving bilinear observational structures.

\textbf{Example 1.} 
%In this example, we evaluate our proposed algorithms through the following system. Consider a time-invariant system as introduced in \eqref{eqn:dynamics1}-\eqref{eqn:dynamics2} with matrices $\mxA$, $\mxB$, $\mxC_0$, $\mxC_1$ and $\mxD$ defined as 
In this example, we evaluate our proposed algorithms considering a time-invariant system as introduced in \eqref{eqn:dynamics1}-\eqref{eqn:dynamics2} with matrices $\mxA$, $\mxB$, $\mxC_0$, $\mxC_1$ and $\mxD$ defined as 
\begin{subequations}\label{eq:system example1_1}
\begin{align}
        \mxA &= \begin{bmatrix}
            0.6 & -0.28 \\ 0.25 & 0.45
        \end{bmatrix},\\
        \mxB &= \begin{bmatrix}
            0.5 \\ -0.5
        \end{bmatrix},\\
        \mxC_0 &= \begin{bmatrix}
            0.5 & -0.15
        \end{bmatrix},\\
        \mxC_1 &= \begin{bmatrix}
            0.15 & 0.1
        \end{bmatrix},\\
        \mxD &= 0.
\end{align}
\end{subequations}
The initial state is set to $\mu_{\mathbf{x}_0} = \begin{bmatrix}
    1 & 1 
\end{bmatrix}^\tr$, and realizations of random binary signals are generated as the control input sequence under different dataset length. 
Following the problem setting introduced in Section \ref{sec:pf}, random sequences of process noise and measurement noise are sampled from Gaussian distributions $\Ncal(\mathbf{0}, \mxS_\vcw)$ and $\Ncal(\mathbf{0}, \mxS_\vcv)$, respectively, where $\mxS_\vcw$ and $\mxS_\vcv$ are designed for different SNR levels. 

To evaluate the performance and robustness of the two proposed algorithms, we perform a Monte Carlo experiment. For each dataset length, given the same input sequences, $N_{\text{MC}} = 100$ different initializations and noise sequences are generated under four different SNR levels, namely $5\,$dB, $10\,$dB, $15\,$dB, and  $20\,$dB.  For each realization, the proposed Algorithm \ref{Al:ML ID} and Algorithm \ref{Al:EM ID} are applied to identify the system. After identification, another random input sequence with length $T = 100$ is generated for validation. The performance is evaluated based on the normalized relative error of output as following,
\begin{equation}\label{eq:Normalized relative error}
    \vcy_{\text{error}} = \sum_{t=0}^{T} \frac{\lVert \vcy_t - \hat{\vcy}_t \rVert} {\lVert \vcy_t \rVert},  
\end{equation}
where $\vcy_t$ and $\hat{\vcy}_t$ are the output trajectories computed using the real system and the identified system, respectively. 

% The results are shown in Figure \ref{img:MC_comparison}. For both approaches, datasets of different sizes ($\nD = 100, 200$) are used. The average run-time for the EM approach is $17.1$ seconds and $34.0$ seconds for $\nD = 100$ and $\nD = 1000$, respectively, while the ML approach requires approximately $255.4$ seconds for $\nD = 100$. The results in Figure \ref{img:MC_comparison} show that, when we increase the SNR levels, the performance of both algorithms improves. For dataset length with 100, the ML approach outperforms the EM approach. However, as the length of the dataset increases, the performance of the EM approach improves significantly, while the ML approach becomes infeasible due to computational demands. In conclusion, the ML approach performs better when the dataset is of length data $\nD=100$, but with enough data, the EM approach performs better because it has less computational demand.
\begin{figure}[t!]
   \centering
   \includegraphics[width=0.7\textwidth,,trim={2cm 6cm 2cm 9.5cm},clip]{plots/MonteCarlo_Comparison.pdf}
   \caption{Comparison of normalized relative error of output trajectory of systems identified by ML and EM approaches.}
   \label{img:MC_comparison}
\end{figure}


\begin{figure}[t!]
   \centering
   \includegraphics[width=0.7\textwidth,trim={2.5cm 10cm 3.1cm 10.5cm},clip]{plots/MonteCarlo_ML_complexity.pdf}
   \caption{Runtime of ML and EM approach with different length of dataset.}
   \label{img:ML_com}
\end{figure}

 
The results are shown in Figure~\ref{img:MC_comparison}, where  data sets of varying sizes ($\nD=
100,200$) are used for both approaches. When comparing the results within each row, we can see that increasing the SNR levels leads to improved performance for both algorithms. Similarly, when comparing the results in each column, increasing the dataset length also enhances the performance of both algorithms. Moreover, when comparing the two algorithms in each subplot, the ML approach outperforms the EM approach for the same data set length and SNR level. However, this improved performance comes at the cost of increased runtime. As shown in Figure~\ref{img:ML_com}, the runtime of the ML approach increases significantly with increasing data set length. In contrast, the runtime of the EM approach remains around 20 seconds for a data set length of 200, which is considerably lower than that of the ML approach. As discussed in Section~\ref{sec:Complexity Analysis}, this difference arises due to the high computational demand to calculate derivatives in the ML approach. Consequently, although ML performs better for shorter data sets, solving the optimization problem for longer data sets becomes challenging, where the EM approach proves to be more effective. In Figure~\ref{img:EM_1000}, the EM approach is applied to a data set of 1000 length, achieving a run-time of approximately 34 seconds and showing better performance than the ML approach with a data set length of 200. 

In conclusion, the ML approach outperforms the EM approach for small data sets. The performance gap becomes smaller for longer length of the data set and higher SNR level. Furthermore, as the size of the data set increases, the ML approach becomes computationally challenging, whereas the EM approach is more efficient and better suited for large data sets.

% Also, for the initial state, process and measurement noise, we assume that
% \begin{subequations}\label{eq:system example1_2}
% \begin{align} 
%         \mxS_\vcw &= \begin{bmatrix}
%             0.05^2 & 0 \\ 0 & 0.05^2
%         \end{bmatrix},
%         \\
%         \mxS_\vcv &= 0.05^2, \\
%         \vcmu_{\vcx_0} &= \begin{bmatrix}
%             1 \\ 1
%         \end{bmatrix},\\
%         \mxS_{\vcx_0} &= \begin{bmatrix}
%             0.05^2 & 0 \\ 0 & 0.05^2
%         \end{bmatrix}.
% \end{align}
% \end{subequations}
% A random binary signal is generated as the control input $\mxU$ for the system, with a length of $\nD = 1000$. Following the problem settings introduced in Section \ref{sec:pf}, the process noise and measurement noise are generated from Gaussian distributions $\Ncal(\mathbf{0}, \mxS_\vcw)$ and $\Ncal(\mathbf{0}, \mxS_\vcv)$, respectively. For the algorithm, we set a stopping condition as $\lVert \hat{\vctheta}_{k+1} - \hat{\vctheta}_k \rVert < \epsilon$, where $\epsilon = 10^{-4}$. 


To further investigate the performance of the Algorithm~\ref{Al:EM ID}, we analyze the relative error 
\begin{equation}\label{eq: relative error parameters}
\begin{split}
    \mxC_{\text{error}} =&  \frac{\lVert \mxC - \hat{\mxC} \rVert} {\lVert \mxC \rVert},\\
    \mxM_{\text{error}} =&  \frac{\lVert \mxM - \hat{\mxM} \rVert} {\lVert \mxM \rVert},
\end{split}
\end{equation}
of the estimated system matrices, where $\mxC = \begin{bmatrix}\mxC_0 & \mxC_1 \end{bmatrix}$ and $\mxM = \begin{bmatrix}\mxA &\mxB \end{bmatrix}$ as defined after \eqref{eq:Q function2}.
\begin{figure}[t!]
   \centering
   \includegraphics[width=0.5\textwidth,trim={1cm 10.5cm 10cm 10.5cm},clip]{plots/MonteCarlo_EM_1000.pdf}
   \caption{Normalized relative error of output trajectory of systems identified by EM approach with 1000 length of data set.}
   \label{img:EM_1000}
\end{figure}


\begin{figure}[t!]
   \centering
   \includegraphics[width=0.8\textwidth,trim={1cm 10.5cm 1cm 10.6cm},clip]{plots/MonteCarlo_Parameter.pdf}
   \caption{Relative error of system matrices under four different SNRs using EM approach with $\nD=1000$.}
   \label{img:MonteCarlo}
\end{figure}


Figure~\ref{img:MonteCarlo} shows the results when we apply Algorithm~\ref{Al:EM ID} and set the length of the data set $\nD=1000$. Generally, the mean relative errors of both $\mxC$ and $\mxM$ are lower for scenarios of higher SNR levels, indicating improved precision in the identified parameters. At lower SNR levels, the performance is more sensitive to initialization. Specifically, while some realizations achieve low relative errors, others exhibit significantly higher errors, resulting in increased variance. However, for larger SNR cases, the variance of the relative errors is relatively smaller, indicating that the algorithm is less dependent on initialization. 

\begin{figure}[t!]
   \centering
   \includegraphics[width=0.9\textwidth,trim={1cm 10cm 1cm 10.8cm},clip]{plots/Parameter_convergence.pdf}
   \caption{The relative error of system matrices estimates with respect to iteration steps.}
   \label{img:convergence}
\end{figure}

Figure~\ref{img:convergence} illustrates a single test of Algorithm~\ref{Al:EM ID}. In this figure, we can  observe the convergence of the relative errors $\mxC_\text{error}$ and $\mxM_\text{error}$. It can be seen that, initially, the relative error is large because of the difference between the initialized parameters and the real parameters. However, as the number of iterations increases, the relative error decreases. The convergence rate is fast in the initial iterations; however, it gets slower later. For $\epsilon = 10^{-5}$, the algorithm stops after about $1100$ EM steps. Figure~\ref{img:validation} shows the performance of the proposed Algorithm~\ref{Al:EM ID}. To this end, another random input sequence of length $T = 80$ is generated to validate the identified system. We can note that the predicted trajectory of the outputs for the identified system is close to that of the real system. The relatively error $\vcy_{\text{error}}$, computed as in \eqref{eq:Normalized relative error} is approximately $10.3\%$. 

\begin{figure}[t!]
   \centering
   \includegraphics[width=0.7\textwidth,trim={5cm 10.5cm 5cm 10.5cm},clip]{plots/validation.pdf}
   \caption{Comparison of real system outputs and identified system outputs. The blue line represents the output trajectory of the real system, the red line corresponds to the trajectory obtained from the identified system, and the yellow line depicts the trajectory generated using the system with initial guess parameters. }
   \label{img:validation}
\end{figure}


\begin{remark}
    As shown in Figure~\ref{img:convergence}, the convergence speed of the EM algorithm is relatively slow. As discussed after \eqref{eq:Q convengence}, with each iteration step, improving $Q(\vctheta | \hat{\vctheta}_k)$ will improve the log-likelihood $\log p(\mathbf{\mathbf{y}}|\vctheta,\mathbf{u})$. However, the rate of improvement can be slow. Furthermore, there is no guarantee that the log-likelihood will converge to the global optimum. More precisely, the EM algorithm may converge to a local optimum. Consequently, in simulations, it may be necessary to restart the algorithm with different initialization to obtain more accurate parameter estimates.
\end{remark}


\textbf{Example 2.} In this example, we consider a resistor–capacitor circuit (RC circuit), where the capacitor is non-ideal as shown in Figure~\ref{fig:Parallel RC}a). The non-ideal capacitor is modeled as in Figure~\ref{fig:Parallel RC}b) \cite{bisquert2000role}. In this model, $R_s$ represents the equivalent series resistance (ESR), which captures the resistive losses due to the internal structure. The parasite inductance, denoted as $L$, accounts for the inductive effects caused by the leads or internal connections. Leakage resistance $R_p$ models the imperfection of the dielectric material, which causes current leakage through the capacitor. 

\input{RC_circuit}

As shown in Figure~\ref{fig:Parallel RC}b), let $I_L$ denotes the current passing through the inductance $L$,  $I_c$ the current passing capacitor $C$, and $V_c$ the voltage across the capacitor. The above RC circuit can be seen as a simplified circuit of a load. To protect the capacitor, limitations are imposed on the thermal effects of the non-ideal capacitor, which can be modeled as $Q = \alpha I_L V$, where $\alpha$ is a heat transfer coefficient and $Q$ is proportional to the temperature and can be measured through a temperature sensor. Define 
\begin{equation}
\begin{split}
    \vcx &= \begin{bmatrix}
    I_L \\ V_c \end{bmatrix},\\
    \vcu  &= V, \\
    \vcy &= Q.\\
\end{split}
\end{equation}
We have %we can omit eq (86)
% \begin{equation}\label{eq:RC circuit}
% \begin{split}
%     V &= I_L R_s + L\frac{dI}{dt} + V_c, \\
%     I_c &= C\frac{dV_c}{dt}, \\
%     I_L &= I_c + \frac{V_c}{R_p}.\\
%     % I & = I_L + \frac{V}{R_0}, \\ 
% \end{split}
% \end{equation}
% which is equivalent to
\begin{equation}\label{eq:RC circuit}
\begin{split}
    \dot{\vcx} &= \begin{bmatrix}
        -\frac{R_L}{L} & -\frac{1}{L} \\
        \frac{1}{C} & -\frac{1}{CR_c} \\
    \end{bmatrix} \vcx + \begin{bmatrix}
        \frac{1}{L} \\ 0
    \end{bmatrix} \vcu, \\
    \vcy &= \begin{bmatrix}
        \alpha & 0
    \end{bmatrix} \vcx \vcu.\\
\end{split}
\end{equation}
In this experiment, the data set $\Dcal$ of length $\nD = 1000$ is generated by an input sequence $\vcu(t) = 12\sin(\omega t)$, where the angular frequency $\omega = 2\pi\times10^{8} \text{rad/s}$ and the sample time is $1\times10^{-9} \text{s}$. The covariance matrices of process noise, measurement noise, and initial states are set as
\begin{subequations}\label{eq:RC covariance}
\begin{align} 
        \mxS_\vcw &= \begin{bmatrix}
            1\times10^{-3} & 0 \\ 0 & 1\times10^{-3}
        \end{bmatrix},
        \\
        \mxS_\vcv &= 1\times10^{-4}, \\
        \mxS_{\vcx_0} &= \begin{bmatrix}
            1\times10^{-3} & 0 \\ 0 & 1\times10^{-3}
        \end{bmatrix},
\end{align}
\end{subequations}
which results in an SNR level of $10\,\text{dB}$. We use the proposed Algorithm~\ref{Al:EM ID} to identify the above system \eqref{eq:RC circuit}. After identification, another input signal sequence $u(t) = 6\sin(\omega t) + 6\sin(0.3\omega t)$, with the same sample time is used for validation.

Figure~\ref{img:validation2} presents the validation results. The estimated thermal dissipation trajectory closely follows the actual trajectory, with some discrepancies observed near the peaks. The mean normalized relative error of the output, calculated using \eqref{eq:Normalized relative error}, is approximately 9.9\%. 


\begin{figure}[t!]
   \centering
   \includegraphics[width=0.7\textwidth,trim={5cm 10cm 5cm 10cm},clip]{plots/non_ideal_capacitor.pdf}
   \caption{Closed-loop cross validation of thermal dissipation. The blue line represents the output trajectory of the real system, the red line corresponds to the trajectory obtained from the identified system, and the yellow line depicts the trajectory generated using the system with initial guess parameters.}
   \label{img:validation2}
\end{figure}