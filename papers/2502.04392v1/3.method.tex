\section{Methodology}

% In this section, we will introduce the detailed implementation of our proposed edge-cloud collaborative framework, .  We will begin by discussing the foundational concept that inspired our approach: the \textit{divide-of-labour} principle. We will explain our design of the "divide" and from there, provide an overview of the various components within the framework, laying out the logical progression of the workflow. Following this, we will present the core designs of the framework, including the construction of the reasoning graph based on sub-task dependency relationships, the design of the extendable LLM adapter which is exactly the edge-cloud model allocator, and the efficient construction of the training dataset.


\subsection{Division-of-Thoughts Framework}
The \textit{division-of-labour} is a prominent concept in economics, famously introduced by Adam Smith. It refers to the separation and allocation of tasks within any economic system or organization, enabling participants to specialize in specific areas based on their unique capabilities. This specialization allows individuals, organizations, and nations to optimize productivity by leveraging specialized skills, equipment, and resources. This concept inspires us to consider a similar approach in the context of edge-cloud collaboration, where user queries can be intricately divided: simpler sub-tasks can be assigned to SLMs while more complex ones are allocated to LLMs. This sub-task-level division of labor is expected to reduce reasoning costs while maintaining reasoning performance, enabling more efficient collaboration. 

Building on this intuition, we develop the \textbf{DoT} edge-cloud collaboration framework. As illustrated in Figure \ref{fig:framework}, our framework is divided into three components. The first component is the \textit{Task Decomposer}, which breaks down the user's query into several simpler and independent sub-tasks.
The second component is the \textit{Task Scheduler}, which is leveraged to determine the pairwise dependencies between sub-tasks and constructing a task dependency graph based on these dependencies.
The third component is the \textit{plug-and-play LLM adapter}, responsible for assigning each sub-task to the appropriate models.
% As the LLM excels in reasoning capabilities but is slow, costly to invoke, and often involves privacy concerns, while the local deployed SLM has weaker reasoning ability but offers faster response times, incurs no API cost, and ensures secure local storage of user data, we aim to significantly reduce costs while maintaining near-constant reasoning quality by assigning simpler sub-tasks to SLMs wherever possible.
The adapter extracts sentence embeddings from the SLM and maps them to difficulty coefficients, which serve as the basis for the model allocation for sub-tasks. Importantly, the training of the adapter does not require modifying the LLM's parameters, ensuring that the LLM's question-answering remains unaffected.
Once the appropriate models have been assigned to each sub-task, reasoning proceeds along the order defined by the constructed dependency graph, leading to the final results.
Figure \ref{fig:compare} clearly demonstrates that, compared to the simple referral strategy, DoT exhibits significant advantages in both accuracy and efficiency.


\subsection{Decomposing User Query}
As emphasized in the \textit{division-of-labour}, an effective division method serves as the foundation for collaborative work. The granularity and accuracy of division directly influence the quality and efficiency of the collaboration. 
Prior research on model collaboration has primarily focused on query-level granularity and relied heavily on manual task decomposition strategies. A typical example is the ToT approach in Game-of-24, which employs a fixed two-step process of proposal generation and evaluation. This rigid, manually-designed workflow lacks generalizability across diverse tasks, underscoring the necessity for automated, flexible, and fine-grained task decomposition methods.
% Previous work has explored the collaboration between large and small models, but the granularity of these efforts has often been limited to the query level, and most rely on manually designed task decomposition strategies. For instance, in the ToT approach to solving the Game-of-24, the task is broken down into two sequential steps: proposing and evaluating, with proposing occurring before evaluating iteratively. However, this rigid and manual workflow is challenging to generalize across various tasks, which highlights the need for a flexible and fine-grained task decomposition method that operates without human intervention.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Figures/Whole-Com.pdf}
    \vspace{-6mm}
    \caption{Advantage of ``Division-and-Allocate'' Strategy}
    \label{fig:compare}
    \vspace{-5mm}
\end{figure}

To address these challenges, we develop a fine-grained task decomposition method based on the powerful \textit{In-Context Learning (ICL)} capabilities of LLMs. We leverage a meta-prompt that incorporates ``chain-of-thought''-like prompting with hand-crafted task decomposing examples, to elicit the inherent \emph{planning} abilities in SLMs. 
For each benchmark, We randomly selected 8 samples, manually performed step-by-step task decomposition, and incorporated these steps into the prompts.
To enable LLMs to independently solve each sub-task and effectively use the answers from preceding tasks as references for subsequent reasoning, we place great emphasis on the independence and clarity of the decomposed sub-tasks and have incorporated targeted cues in the prompts design.


\subsection{Task Scheduling via Dependency Graph}
\label{method:dep}
We employ a three-step approach to schedule all the sub-tasks effectively:
\textbf{Dependency Judgement.} Prompting LLM agent to assess pairwise dependencies between sub-tasks. The prompts are as follows: \textit{"Please list the dependencies in the format 'Subproblem A [xxx] -> Subproblem B [xxx]' indicating that Subproblem A must be completed before Subproblem B can start."}
\textbf{Graph Construction.} Converting dependencies into a dependency graph. Based on the clear directional dependencies, constructing the graph is straightforward. However, we further trace back from the final sub-task node to calculate the depth of each sub-task node in the graph (analogous to the height in a tree structure). Sub-tasks at the same depth are independent of each other and can be inferred in parallel. Depth serves as the inference batch, with batches processed sequentially while tasks within a batch are reasoned in parallel. As shown in Figure \ref{method:flow-Compare}, compared to sequentially reasoning through all sub-tasks, our graph structure can capture more precise inferential relationships while incurring fewer time costs.
\textbf{On-Graph Reasoning}. We start by solving the sub-tasks from the shallowest depth batch, running the sub-tasks within the same batch in parallel. After completing one batch, we proceed to the next. During the reasoning of a specific sub-task, only the results of its prerequisite sub-tasks, rather than all previously solved tasks, are included in the prompts, which enables efficient graph-based reasoning.











% 主要参考toolformer
\subsection{Task Allocation with Reinforced SLM}
\label{method:allocation}


For the decomposed sub-tasks, we aim to assess their difficulty based on the task descriptions and allocate either cloud-based or edge-side models for execution. Using LLMs to evaluate difficulty introduces additional inference costs and often fails to accurately assess the sub-task’s complexity, which motivates us to train a model specifically designed for task allocation with sentence embeddings.
\textit{Sentence embeddings}, which map a sentence to a fixed-size vector capturing its semantic meaning and context, have seen extensive application in natural language processing for their lightweight accessibility and the strong ability to capture sentence semantics. We can obtain a sentence embedding for each sub-task's prompt and then map the embedding to the corresponding difficulty. Given that LLMs are already deployed on the edge side and have been shown to serve as effective sentence embedders, we plan to leverage the local deployed SLM to produce sentence embeddings.

However, autoregressive LLMs lack specialized tokens such as BERT’s [MASK] or [CLS], which are typically used in transformer-based models for embedding tasks. The exploration of Ting Jiang et al.~\cite{jiang2023scaling} helps us overcome this limitation. 
Prompt-based method is refined specifically for autoregressive LLMs by instructing the model to generate the next word that captures the semantic meaning of the input sentence. Specifically, a simple but effective template can be constructed for each piece of text: \textit{This sentence: "[text]" means in one word: "}, where \textit{[text]} represents the input sentence, and the LLM is prompted to generate the next token that encapsulates the core meaning of the sentence in a single word. The last generated token plays a critical role, as we extract its hidden state from the model and use it as the sentence embedding.


\begin{figure}
    \centering
    \includegraphics[width=0.96\linewidth]{Figures/flowCompare.pdf}
    \vspace{-3mm}
    \caption{Illustrating Dependency Graph of Task Scheduling}
    \label{method:flow-Compare}
    \vspace{-5mm}
\end{figure}

\textbf{Boosting SLM with Plug-and-Play Adapter.}
Despite the varying lengths of sub-task descriptions, the sentence embeddings maintain a consistent dimensionality. For efficiency, we append a multi-layer perceptron (MLP) to the transformer module of the LLM to map the embeddings to a difficulty score. Given that both the edge side and the cloud host only one model, we can simply set the scores as 0 (simple, for SLM) or 1 (complex, for LLM). The output from the MLP is translated into a model designation and passed to the agent handling the current reasoning step, providing a concrete allocation strategy for edge-cloud collaboration. 
Moreover, the adapter requires no modifications to SLM's parameters. It essentially serves as a flexible and extensible adapter for the SLM, avoiding the need to store two sets of parameters on the device.

% 数据集构建的示意图就直接放到主图中了
\begin{figure}
    \centering
    \includegraphics[width=1.01\linewidth]{Figures/datasetConstruction_Fig.pdf}
    \vspace{-6mm}
    \caption{Tree Search-Based Dataset Construction Process}
    \label{method:dataConstruction}
    \vspace{-4mm}
\end{figure}


\textbf{Creating Training Data with \textbf{$\alpha$}-Tree Algorithm.}
How cam we construct a dataset to train our adapter?
We develop an efficient allocation optimization strategy, $\alpha-Tree$,  which consists of two key components: the sub-task difficulty ranking based on the $\alpha$-quantile token probability, and a tree-based search strategy guided by the ranking, allowing us to generate a large-scale optimal allocation dataset with low cost and high speed.
$\alpha$-quantile refers to calculating a specific quantile from the token probabilities generated by LLM during inference. Unlike traditional methods that aggregate probabilities through summation or averaging, $\alpha$-quantile focuses on a specific portion of the uncertainty distribution, such as the minimum ($\alpha$ = 0) or a higher percentile (e.g., $\alpha$ = 0.8), which provides a fine-grained measure of uncertainty.
Denote the input context as $x$, and the output tokens as $\hat{y}_i'$. The $\alpha$-quantile value is denoted as:
% There is relevant empirical evidence~\cite{gupta2024language} demonstrating that this approach more effectively captures the uncertainty inherent in the inference process, allowing us to capture nuanced uncertainty information, making deferral decisions more informed and accurate.
\begin{equation}
\begin{aligned}
s_{\text{quant}}(x, \alpha) &\doteq \text{quantile}_{\alpha}\left(p^{(1)}(\hat{y}_1' \mid x), \right.  p(\hat{y}_2' \mid x, \hat{y}_1'), \dots, \\
& \left. \quad p(\hat{y}_n' \mid x, \hat{y}_1', \dots, \hat{y}_{n-1}')\right)
\end{aligned}
\end{equation}

% 在这里插入一段数据集构建的算法流程图
\input{Algorithm/algorithm}

In our implementation, we employ the SLM to answer all decomposed sub-questions, recording the sampling probability corresponding to each token. We then apply a consistent alpha value to extract the $\alpha$-quantile from each probability sequence and ranked the $\alpha$-quantile values for all sub-task responses. Since higher task difficulty leads to greater uncertainty in the model’s answers, resulting in lower token sampling probabilities, we reverse the $\alpha$-quantile ranking to obtain the difficulty order of the sub-tasks.

Based on the $\alpha$-quantile values of the sub-tasks, we set a fixed allocation threshold: sub-tasks with values exceeding this threshold are assigned to SLM, while those below the threshold are assigned to LLM, forming the initial model allocation. From this initial allocation, we perform collaborative reasoning with SLM and LLM. If the reasoning result is correct, we reassign $n$ of the sub-tasks currently handled by LLM, specifically those with the highest probability of being correctly answered, to SLM. Conversely, if the answer is incorrect, we transfer $n$ of the sub-tasks handled by SLM, specifically those with the lowest probability of being misanswered, to LLM. 
As shown in Figure \ref{method:dataConstruction}, the structure of the search exhibits a tree-like form.
The algorithmic workflow for dataset construction is illustrated in Algorithm \ref{algorithm}.




