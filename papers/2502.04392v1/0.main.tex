\documentclass[sigconf]{acmart}

\usepackage{multirow} 
% \usepackage{arydshln}
\usepackage{algorithm}  
\usepackage{algpseudocode}  
\usepackage{amsmath}  
\usepackage{subfigure}
\usepackage{graphicx}
% \usepackage{subcaption}
% \usepackage{caption}
% \usepackage{array}
% \usepackage{subfig}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}  % Use Input in the format of Algorithm  
\renewcommand{\algorithmicensure}{\textbf{Output:}} % Use Output in the format of Algorithm 

\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

% \setcopyright{acmlicensed}


\copyrightyear{2025}
\acmYear{2025}
\setcopyright{cc}
\setcctype{by}
\acmConference[WWW '25]{Proceedings of the ACM Web Conference 2025}{April 28-May 2, 2025}{Sydney, NSW, Australia}
\acmBooktitle{Proceedings of the ACM Web Conference 2025 (WWW '25), April 28-May 2, 2025, Sydney, NSW, Australia}
\acmDOI{10.1145/3696410.3714765}
\acmISBN{979-8-4007-1274-6/25/04}


\begin{document}


\title{Division-of-Thoughts: Harnessing Hybrid Language Model Synergy for Efficient On-Device Agents}

\author{Chenyang Shao}
\affiliation{%
 \institution{Department of Electronic Engineering\\ BNRist, Tsinghua University}
 \city{Beijing}
 \country{China}}
 \email{shaocy24@mails.tsinghua.edu.cn}

\author{Xinyuan Hu}
\affiliation{%
 \institution{Department of Quantitative Theory \& Methods\\Emory University}
 \city{GA}
 \country{USA}}
 \email{nate.hu@emory.edu}

\author{Yutang Lin}
\affiliation{%
 \institution{Department of Electronic Engineering \\Tsinghua University}
 \city{Beijing}
 \country{China}}
 \email{yt-lin21@mails.tsinghua.edu.cn}




% \author{Fengli Xu\raisebox{0.5ex}{\scalebox{0.85}{$\dagger$}}}
\author{Fengli Xu$^{*}$}
\affiliation{%
 \institution{Department of Electronic Engineering\\ BNRist, Tsinghua University\authornote{Corresponding author.}}
 \city{Beijing}
 \country{China}}
 \email{fenglixu@tsinghua.edu.cn}
 
%\renewcommand{\shortauthors}{Trovato et al.}


\begin{abstract}
The rapid expansion of web content has made on-device AI assistants indispensable for helping users manage the increasing complexity of online tasks. The emergent reasoning ability in large language models offer a promising path for next-generation on-device AI agents. However, deploying full-scale Large Language Models (LLMs) on resource-limited local devices is challenging. In this paper, we propose \underline{D}ivision-\underline{o}f-\underline{T}houghts (\textbf{DoT}), a collaborative reasoning framework leveraging the synergy between locally deployed Smaller-scale Language Models (SLMs) and cloud-based LLMs.
DoT leverages a \textit{Task Decomposer} to elicit the inherent planning abilities in language models to decompose user queries into smaller sub-tasks, which allows hybrid language models to fully exploit their respective strengths. Besides, DoT employs a \textit{Task Scheduler} to analyze the pair-wise dependency of sub-tasks and create a dependency graph, facilitating parallel reasoning of sub-tasks and the identification of key steps. To allocate the appropriate model based on the difficulty of sub-tasks, DoT leverages a \textit{Plug-and-Play Adapter}, which is an additional task head attached to the SLM that does not alter the SLM's parameters. To boost adapter's task allocation capability, 
% we propose a self-reinforced tree search algorithm to create a high-quality sub-task allocation dataset.
we propose a self-reinforced training method that relies solely on task execution feedback.
Extensive experiments on various benchmarks demonstrate that our DoT significantly reduces LLM costs while maintaining competitive reasoning accuracy. Specifically, DoT reduces the average reasoning time and API costs by 66.12\% and 83.57\%, while achieving comparable reasoning accuracy with the best baseline methods. \footnote{Code available at: https://github.com/tsinghua-fib-lab/DoT}
\end{abstract}


% \begin{abstract}
% The rapid expansion of web content has made on-device AI assistants indispensable for helping users manage the increasing complexity of online tasks. The emergent reasoning ability in large language models represents a promising direction to develop next generation on-device AI agents. However, deploying full-scale LLMs on resource-limited local devices face significant challenges. In this paper, we present a novel collaborative reasoning framework called \underline{D}ivision-\underline{o}f-\underline{T}houghts (\textbf{DoT}) to fully harness the synergy between locally deployed smaller-scale language model (SLMs) and cloud-based commercial LLMs.
% DoT leverages a \textit{Task Decomposer} to elicit the inherent planning abilities in language models to decompose user queries into smaller sub-tasks, which allows hybrid language models to fully exploit their respective strengths. Besides, DoT also employs a \textit{Task Scheduler} to analyze the pair-wise dependency of sub-tasks and create a dependency graph, facilitating parallel reasoning of sub-tasks and the identification of key steps. To allocate the appropriate model based on the difficulty of sub-tasks, DoT leverages a \textit{Plug-and-Play Adapter}, which is an additional task head attached to the SLM that does not alter the SLM's parameters. To boost the allocation accuracy of the adapter, we propose a self-reinforced tree search algorithm to create a high-quality sub-task allocation dataset. Extensive experiments on various benchmarks demonstrate that our DoT significantly reduces LLM costs while maintaining competitive reasoning accuracy. Specifically, DoT reduces the average reasoning time and API costs by 66.12\% and 83.57\%, while achieving comparable reasoning accuracy with the best baseline methods. \footnote{Our code can be accessed via: https://github.com/PLUTO-SCY/DoT}
% \end{abstract}


\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010178.10010179.10010182</concept_id>
       <concept_desc>Computing methodologies~Natural language generation</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010520.10010521.10010537.10010538</concept_id>
       <concept_desc>Computer systems organization~Client-server architectures</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Natural language generation}
\ccsdesc[300]{Computer systems organization~Client-server architectures}


\keywords{Large Language Model, LLM Reasoning, AI Agents, Edge-Cloud Collaboration}


% \received{14 October 2024}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

\maketitle

\input{1.introduction.tex}
\input{5.related.tex}
\input{2.Preliminary.tex}
\input{3.method.tex}
\input{4.experiment.tex}
\input{6.conclusion.tex}

\begin{acks}
  This work is supported in part by the National Natural Science Foundation of China under 23IAA02114 and 62472241, in part by the joint project of Infinigence AI \& Tsinghua University.
\end{acks}

\clearpage


\input{0.main.bbl}
% \bibliographystyle{ACM-Reference-Format}
% \bibliography{reference}


% \clearpage
\input{7.appendix.tex}

\end{document}

