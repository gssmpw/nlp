\vspace{-4mm}
\section*{Appendix}
 
\appendix

\section{Supplementary Experiments}

\subsection{Generalizability of Adapter}
% The results presented in Table ~\ref{tbl:mainResult} of the paper are based on task-specific training of the adapter. 
We conduct two supplementary experiments to evaluate the adapter's generalization performance across different tasks. 
\textbf{Intra-category generalization}: Within three mathematical benchmarks, train on two and test on the remaining one. 
\textbf{Cross-category generalization}: Train on six benchmarks and test on the remaining one. 
We also evaluate the performance of the adapter in the absence of any training.
Results are shown in Table~\ref{appendix:generalizability}, demonstrating that intra-category generalization within math-related benchmarks is almost as effective as task-specific training. And although there is a slight performance decrease in cross-category application, it still outperforms no training.

\input{Tables/generalization}

\vspace{-2mm}
\subsection{Evaluation of Task Decomposition Quality}

For the quality of the task decomposition, we compare our method with a vanilla prompting strategy, focusing on evaluating the \textit{independence} of subtasks and the impact of the decomposition strategy on both the final reasoning accuracy and the reasoning cost. 
For \textit{independence}, we aim to determine whether the generated subtasks are independent (i.e., without overlapping mathematical computation) and calculate the independence rate for performance ealuation. We first manually annotate the independence of the decomposition results for 50 tasks in the MATH benchmark. Our method achieves 91.3\% in independence rate, significantly outperforming the vanilla prompting strategy, which only reaches 74.2\%. This demonstrates the high quality of our task decomposition approach. 

\input{Tables/independence}

Subsequently, we employ GPT-4o to evaluate the independence of subtasks generated by both methods and compared its results to manual annotations. The GPT-4o assessments achieve a 93\% consistency with human annotations, indicating that GPT-4o can provide annotation quality comparable to that of humans and can be effectively used for large-scale evaluations. 
Therefore, we utilize GPT-4o to evaluate the independence of task decomposition across the entire test set, with the results presented in Table~\ref{tbl:indepedence}. The results indicate that our task decomposition method achieves an average improvement of 15.7\% in subtask independence compared to the baseline, demonstrating a clear advantage. Moreover, this improvement contributes to enhanced reasoning accuracy and reduced reasoning costs.

% \subsection{Accuracy of Dependency Identification}

% During the task scheduling phase of DoT, we employ LLMs to ascertain the dependencies among subtasks through in-context learning, thereby constructing a dependency graph. But, how accurate are LLMs in determining these dependency relationships? 
% To investigate this matter, we have conducted a supplementary experiment, where we manually annotated the subtask dependency graphs for 50 problems from MATH, CSQA, and P3 benchmarks. 
% These manually annotated graphs were treated as the ground-truth and then compared with the results generated by the LLM. 
% The consistency between the two reached 94\%, indicating that LLMs are capable of correctly determining dependency relationships in the vast majority of cases.
% Besides, the ablation study~\ref{exp:ablation} shows dependency identification (dependency graph construction) leads to consistent gains in final performance. This result shows the performance gain of dependency identification outweighs the seldom errors.


\vspace{-2mm}
\subsection{Impact of the Number of Sub-tasks on Reasoning Efficiency}
\input{Tables/scale}

As problems become more complex and the number of decomposed sub-tasks increases, whether our framework can achieve a significant improvement in reasoning efficiency while maintaining accuracy remains a valuable research question.
Therefore, we conduct supplementary experiment, where we compare our method with baseline that \textit{sequentially reason all sub-tasks on SLM} and perform statistical analyses based on varying numbers of sub-tasks. The results in Table~\ref{tbl:scale} demonstrate that our model consistently maintains high reasoning efficiency across tasks with different numbers of sub-task nodes.

Furthermore, as the number of sub-tasks increases, the enhancement in efficiency becomes increasingly pronounced. When the number of subtasks exceeds eight, the reduction in reasoning time surpasses even 30\%. This outcome is inherently logical, for as the quantity of sub-tasks escalates, the task graph grows in complexity, thereby expanding the potential for parallel reasoning and amplifying the advantages in terms of reasoning time.


% \vspace{-2mm}
\section{Benchmarks and Implementation Details}

\subsection{Mathematics}

\begin{itemize}
    \item \textbf{MATH}~\cite{hendrycks2021measuring}. Mathematics Aptitude Test of Heuristics (MATH), comprises 12,500 problems from prestigious U.S. mathematics competitions like AMC and AIME. These problems, collected from platforms such as AoPS, test advanced problem-solving skills beyond standard K-12 math. Each problem includes a step-by-step solution and a final answer. The dataset spans seven subjects: Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus, with difficulty levels ranging from 1 (easy) to 5 (challenging), allowing models to learn and apply various mathematical heuristics.
    
    \item \textbf{CHAMP}~\cite{mao2024champ}. Concept and Hint-Annotated Math Problems (CHAMP). This benchmark features 270 non-routine, competition-level problems sourced from Engel's Problem-Solving Strategies. The problems span five categories: number theory, polynomial, sequence, inequality, and combinatorics, requiring creative strategies and specific tricks. Each problem includes a final checkable answer and a step-by-step solution in natural language. The dataset contains 54 concepts and 330 hints, averaging 1.4 concepts, 1.7 hints, and 6 solution steps per problem. Problem statements average 20.2 words, with solutions averaging 10.9 words per step, highlighting the dataset's complexity and challenge.
    
    \item \textbf{DROP}~\cite{dua2019drop}. Discrete Reasoning Over Paragraphs (DROP), is an English reading comprehension dataset with 96k adversarially-created questions. It challenges systems to resolve references in a question and perform discrete reasoning operations like addition, counting, or sorting over multiple input positions within a paragraph. The dataset is crowdsourced and derived from Wikipedia passages designed for complex questions. DROP demands a deeper understanding of paragraph content than previous datasets, with rigorous validation to ensure the quality of its development and test sets.
\end{itemize}

\subsection{Logic}
\begin{itemize}
    \item \textbf{P3}~\cite{schuster2021programming}. Python Programming Puzzles (P3), introduces a new type of programming challenge for evaluating program synthesis. P3 contains 397 Python-based puzzles, where the goal is to find an input that makes a given function return True. The puzzles span various difficulty levels, from simple string manipulations to complex algorithmic problems. 
    \item \textbf{Scan}~\cite{lake2018generalization}. This benchmark is used to evaluate the sequence-to-sequence learning ability to translate simplified natural language commands into action sequences. SCAN contains 20,910 commands generated by a phrase-structure grammar, which describe basic actions such as "jump" or "walk" and their combinatorial variations (e.g., "jump around left"). The logical structure and constraints involved in the translation process make SCAN ideal for assessing LLM reasoning capabilities.
\end{itemize}

\vspace{-2mm}
\subsection{Commonsense}
\begin{itemize}
    \item \textbf{COMMONSENSEQA}~\cite{talmor2018commonsenseqa}. This benchmark is a challenging dataset designed to test commonsense question answering. It consists of 12,247 multiple-choice questions created using concepts from CONCEPTNET. Each question is authored by crowd-workers to differentiate between multiple target concepts that share a semantic relation with a source concept, encouraging the use of prior knowledge and complex reasoning. 
    % COMMONSENSEQA pushes beyond simple associations, making it a more difficult task compared to traditional question-answering benchmarks.
\end{itemize}

\vspace{-2mm}
\subsection{On-device AI Assistant Application}
\begin{itemize}
    % \item \textbf{TravelPlanner}~\cite{xie2024travelplanner}. TravelPlanner is a benchmark designed to evaluate language agents in handling complex travel planning tasks with multiple constraints. It simulates real-world travel scenarios that involve user preferences, commonsense rules, and the need for flexible decision-making. The benchmark provides access to nearly four million data records and includes 1,225 well-crafted planning queries along with reference plans. These queries are categorized into nine groups based on trip duration and the number of hard constraints. The dataset is divided into training (45 queries with human-annotated plans), validation (180 queries), and test sets (1,000 queries), offering a comprehensive evaluation of agents' ability to utilize tools and create feasible travel plans.
    \item \textbf{Webshop}~\cite{yao2022webshop}. WebShop serves as a simulated e-commerce environment featuring 1.18 million real-world products and 12,087 crowd-sourced text instructions. Designed to evaluate language agents, it challenges them to navigate diverse web pages and perform actions based on natural language product specifications. Agents encounter obstacles such as interpreting compositional instructions, reformulating queries, and understanding noisy text on webpages, while strategically exploring to fulfill product requirements. The modular design of WebShop separates website navigation from task-specific elements, allowing for easy adaptation to new tasks and domains. This dataset provides a robust platform for assessing the capabilities of language agents in an interactive, real-world-inspired setting, emphasizing their ability to comprehend and act on complex instructions.
    % \item \textbf{GAIA}.~\cite{mialon2023gaia} GAIA is a benchmark designed to evaluate General AI Assistants by presenting real-world questions that require fundamental skills such as reasoning, multi-modal understanding, web browsing, and tool-use proficiency. Comprising 466 human-designed questions, GAIA covers a wide range of practical tasks, including daily activities, science, and general knowledge. The questions are straightforward for humans but challenging for AI systems, highlighting the gap in performance between AI (15\% for GPT-4 with plugins) and humans (92\%). GAIA aims to test an AI's ability to handle varied tools and scenarios, offering a comprehensive framework to assess progress toward Artificial General Intelligence (AGI).
\end{itemize}


\vspace{-2mm}
\subsection{Implementation Details across Benchmarks}
\label{appendix:imple_details}
%如何分解、如何构建依赖图、如何推理、如何评估
\textbf{MATH.}
For MATH tasks, we employ a structured workflow comprising four key stages: Task Decomposition, Model Allocation, Dependency Graph Construction, and Step-by-Step Reasoning Based on the Graph.
In the task decomposition phase, we prompt the LLM with exemplars of manually decomposed complex problems. The LLM is then instructed to generate manageable subtasks that collectively solve the primary challenge. Subsequently, we task the LLM with establishing subtask dependencies, where a relationship $Step_i \rightarrow Step_j$ indicates that $Step_i$ must precede $Step_j$.
Using the derived dependencies, we construct a reasoning graph via Breadth-First Search (BFS). Subtasks at the same depth are processed in parallel. Upon completion of all subtasks, we conduct a final query to obtain the ultimate solution. This solution undergoes LLM-based evaluation through comparison with the ground truth.

\textbf{CHAMP.}
For CHAMP tasks, the process is largely similar to the MATH process with the primary distinction being the specific few-shot examples of human-written decompositions provided.

\textbf{DROP.}
For DROP tasks, we adapt our approach to accommodate the format of questions based on given texts. The prompt for each step incorporates relevant background information provided in the dataset while maintaining the core implementation structure used in other benchmarks.

\textbf{P3.}
The Programming Puzzle tasks present a unique challenge, requiring the LLM to generate inputs that yield a 'True' output for a given function. In addition to the puzzle description, we provide the LLM with the expected data type of the final input. The evaluation process for P3 differs from other benchmarks: we execute the program using the LLM-generated input and assess the correctness based on the program's output.

\textbf{SCAN.}
For SCAN tasks focused on translating natural language instructions into action sequences, the steps until evaluation stay the same. The final phase involves converting the model's natural language outputs into standardized action sequences using few-shot examples. The evaluation is conducted by directly comparing these sequences with the true answers, ensuring the outputs accurately match the expected actions.

\textbf{COMMONSENSEQA.}
For CSQA tasks, presented as multiple-choice questions based on common sense, our implementation has an identical structure as other tasks, while both the problem and its options are presented to the LLM for the following task decomposition and reasoning. The Final Evaluation consists of the LLM choosing the most plausible answer from the provided options. This selected answer is cleaned and will directly compare with the correct choice.

\textbf{WebShop.} 
The WebShop task presents unique challenges due to its interactive nature, where each action influences subsequent states and available options. Unlike our other experiments, WebShop is not perfect for the construction of a complete, predefined reasoning graph. Instead, our framework dynamically generates and executes sub-tasks based on the current state of the shopping session, in which the shopping process is divided into high-level sub-tasks/thoughts. We utilized the Model Allocation for each step when generating an action, either be `think[]`, `click[]`, or `plan[]`. The first step would be to generate a keyword from the instructions to search on the website. Then, the top \textit{N = 10} matched items are recorded for the following procedures. With the conversation from the webpage, we implemented the task decomposition process to get a roadmap for the following actions. Prompting each step at once to the LLMs, the detailed information about each item will be recorded in the search history. Then, an evaluation and comparison process is prompted to choose the best item from the list. Lastly, the LLM will be prompted dynamically based on the previous steps, and self-navigated to complete the final purchase. This implementation demonstrates our framework's flexibility in handling tasks with dynamic, state-dependent decision-making processes.

\vspace{-3mm}
\section{Prompts}
\subsection{Task Decomposition}
I will now give you a [Based on the type of problem]. The type of problem is {type}. Please break this problem down into several easy-to-solve steps.

1 examples are as follows:
[Manual Written Examples]

Now the command is {question}, please decompose it into easy-to-solve steps like the examples.
Answer Format: (Please write each broken-down question step on a separate line, starting with a number.)

To solve the question "xxx", we need to know:
"1. question $step_1$",
"2. question $step_2$",
"3. question $step_3$".
...

\vspace{-2mm}
\subsection{Dependency Construction}
\textit{System Prompt:}\\
Now we have a problem, which we have broken down into many sub-problems. I want you to understand the connection between these sub-problems\\
\textit{User Prompt:\\}
    The init problem is {question}. And the sub-problems are {steps}. Please provide your understanding of the relationships between these sub-problems. Your response must be concise.

    Now we need to create standardized connections for the relationships between these sub-problems.
    Now Given the following subtasks for question: [question], determine the dependencies between them:

    [List of Steps]
        
    Please list the dependencies in the format 'Subproblem A [xxx] -> Subproblem B [xxx]' indicating that Sub-problem A must be completed before Sub-problem B can start.
    Please identify any potential conditional dependencies from a logical perspective.
    
    Answer format: (Please strictly follow the format. Each dependency should be separated by a new line. No explanation is required.)\\
    Step $ID_i$ [ sub-problem i ] -> Step $ID_j$ [ sub-problem j ]\\
    Step $ID_j$ [ sub-problem m ] -> Step $ID_n$ [ sub-problem n ] ...
\vspace{-2mm}
\subsection{Subtask Reasoning}
\textit{System Prompt:}\\
Here is a math word problem. I will first provide a passage of the problem to set the context. Then, I will ask a specific question that requires you to use the information from the problem description, along with calculation and reasoning, to solve it. \\
Passage:
[passage] Question: [question]

I have broken this math question down into several smaller questions. I will assign you sub-questions one by one, and provide the results of previous sub-questions as a reference for your reasoning.
Please solve the question according to mathematical logic.

\textit{For each steps}
So far, the answers to the resolved sub-questions are as follows: The format is Sub-question-Id: xxx; Sub-question: xxx; Answer: xxx.
Sub-question-Id: [Corresponding ID]; Sub-question: [Corresponding Step]; Answer: [Corresponding Solution for the step]\\
Among them, sub-questions {predecessors} are directly related to this sub-question, so please pay special attention to them.
The sub-question to solve now is xxx: \{subtask\}
Based on the information above, please provide a concise and clear answer

% \subsection{Final Answer Conclusion}
% Now that all the sub-questions have been solved, so what is the final answer?
% Please give the final answer without any additional explanation or clarification.

% \subsection{Model-Eval}
% Here is a [Problem type] problem with a standard answer and a student's solution. Please help me determine if the student's solution is correct.\\
% Problem: \{question\}

% Standard answer: \{True Answer\}

% Answer: \{Answer concluded by Model\}\\

% If the student's answer is correct, just output True; otherwise, just output False. No explanation is required.

% \subsection{ToT Evaluation}
% Please provide a confidence rating for the accuracy of this solution, on a scale from 1 to 5. Only output the number.



% \subsection{SCAN Action Sequence Mapping}
% Now I have a pseudo action sequence expression with parentheses and multiplication. I need you to help me convert this into a sequence of actions without an operator sign.

% [Examples of mapping procedure]

% The pseudo action sequence to be converted is as follows: {sentence} Please change it to the action sequences.

% Please JUST answer the result.


% \subsection{Webshop Prompts}
% Given the special characteristic of Webshop tasks as requiring a dynamic interaction with the shopping environment, we made a unique prompting structure for this task.
% \subsubsection{\textbf{System Role\\}} 
% You are an online webshop agent. You are instructed to complete a shopping process. You have to generate the next step in the process. Your action should take into account the most current observation (which is the page you are on) and the previous actions taken. 
% Note: If no item absolutely meets the requirement, choose the one that meets most requirements.

% There are three types of actions you can output:

% 1. search[query]: You can search for a product based on the query. The query is a string that describes the product you are looking for

% 2. think[thoughts]: You can think about the current state of the shopping process and decide what to do next.

% 3. click[button]: You can click on a button on the page to navigate to another page. Where the button are presented in the observation that is bracketed by []. If you think a product is the best choice, you can click on the "Buy Now" button to end the process.

% Example of a valid output:\\
% search[noise cancelling cosy cost usb microphone]\\
% think[I want to compare the features of the products]\\
% click[Buy Now]\\
% click[< Prev]\\
% Note Don't output Action in front of the action. The action should be in the format of [action][content].

% \subsubsection{\textbf{Webshop Task Decomposition\\}}
% I have an online shopping request with some constraints and I need to find the best options, and I have searched for the key words with some top results. You should help me to decompose the question into sub-steps that should be done for the following process. You will have the tools to help you with the online shopping.

% Here is a example of decomposed tasks:
% Given the information of the current state
% 1
% Action: reset
% Observation: 
% WebShop 
% Instruction:  
% i want a noise cancelling cosycost usb microphone, and price lower than 60.00 dollars 
% [Search] 

% Action: search[noise cancelling cosycost usb microphone]
% Observation: 
% [Back to Search] 
% Page 1 (Total results: 50) 
% [Next >] 
% [B0972Q1T8T] 
% Cosycost USB Microphone, Condenser Computer PC Gaming Microphone for PS4/5 Laptop Windows Mac OS Android Phone,Noise Cancelling Instant Mute,Studio Mic for Voice,Music Recording, Podcasting,Streaming 
% \$32.99 
% [B072L2D6LY] 
% Andrea Communications NC-255VM USB On-Ear Stereo USB Computer Headset with Noise-Canceling Microphone, in-Line Volume/Mute Controls, and Plug 
% \$34.59 
% [B071H84LTJ] 
% Andrea Communications NC-455VM USB Over-Ear Circumaural Stereo USB Computer Headset with Noise-Canceling Microphone, in-Line Volume/Mute Controls, and Plug 
% \$49.24 

% Example of decomposed tasks:

% To solve the question, we need to clarify/solve:\\
% 1. Click and check item B0972Q1T8T to get more detailed information.\\
% 2. Click and check item B072L2D6LY to get more detailed information.\\
% 3. Click and check item B071H84LTJ for more information.\\
% 4. Based on the more detailed information, I will compare to see which one fulfill by request.\\

% Now, the current state is \{prompt\}

% Based on the current process, please decompose it into sub-steps. 
% Make sure to answer in this format Answer Format: (Please write each broken-down question on a separate line, starting with a number.)
% To solve the question "xxx", we need to clarify/solve:\\
% "1. Click and check xxxx",\\
% "2. Click and check xxxx",\\
% "3. sub-question 3".

% \subsubsection{\textbf{ToT Evaluation\\}}
% Please evaluate the following shopping process based on these criteria:
% 1. Relevance to the original request
% 2. Efficiency of the search and decision-making
% 3. Comparison of multiple options
% 4. Attention to product details and customer requirements

% Background information:
% \{init prompt\} 
% \{prompt\}

% Shopping process:
% \{search action\}

% Rate the overall quality of this shopping process on a scale from 1 to 10, where 10 is the best. 
% Only provide the numerical score as your answer.

% \subsubsection{\textbf{Reasoning\\}}
% Here is an example of the shopping process:

% [One Manual Written Example]

% This is the current instruction:\\
% Instruction:
% [Shopping Request]

% Action: XXX

% Observation: XXX

% (Here The Action and Observation will be updated dynamically based on the action generated by LLMs)

