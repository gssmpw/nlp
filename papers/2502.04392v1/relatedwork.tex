\section{Related Work}
\subsection{Reasoning with LLMs}

Recent years have seen remarkable advancements in LLMs, with increasing model scale leading to powerful emergent reasoning capabilities.~\cite{wei2022emergent, xu2025towards}. Moreover, various prompt engineering techniques have been proposed to further extend LLMs' reasoning capability.
Chain-of-Thought (CoT)~\cite{wei2022chain} improves reasoning performance by incorporating manually crafted decomposition steps into the prompt, allowing the LLM to follow the step-by-step resolution process. Building on this, Zero-shot CoT~\cite{kojima2022large} achieves similar effects by simply adding the phrase "Let's think step by step," enabling LLMs to automatically decompose and execute tasks.
From the linear structure of CoT, more complex frameworks have been introduced, such as Tree-of-Thought~\cite{yao2023tree} and Graph-of-Thought~\cite{besta2024graph}, which push the boundaries of reasoning through branching reasoning paths. However, these methods face increased resource demands and time complexity, and they heavily rely on manually predefined steps tailored to specific tasks, which significantly limits their generalizability. Additionally, there are some approaches, differing from the idea of predefined reasoning structures, that focus on how to decompose a complex problem into simpler ones for more effective resolution~\cite{ zhou2022least, khot2022decomposed, shang2024agentsquare, shang2024defint}.
% 审稿人要补充的讨论
% ~\cite{khot2022decomposed} proposes a regressive task decomposition strategy, where the current task is iteratively decomposed into several subsequent subtasks in a nested manner, enhancing the generalizability of task decomposition to a certain extent. However, their approach is still limited by the recursive linear logic structure, making it unable to implement more flexible reasoning strategies.

Underlying these methods is the intuition of task decomposition and extensive empirical evidence has demonstrated its effectiveness. This inspires us to explore the potential of task decomposition in edge-cloud collaboration, allowing part of the task to be addressed by cloud-based models while the rest is handled by on-device models, thereby achieving a fine-grained collaborative framework.


% There are some automated task decomposition approaches~\cite{khot2022decomposed, zhou2022least} guiding LLMs to break down complex problems into simpler subtasks and sequentially solve them before summarizing a final answer. However, these methods are constrained by their linear reasoning paths, which perform poorly on complex tasks. To address this, we propose a comprehensive task decomposition framework that includes task breakdown, dependency assessment, and dependency graph construction. This approach not only improves reasoning performance and generalization but also facilitates efficient collaboration.

\subsection{On-device LLM Agent}
The rise of LLMs has revolutionized AI applications, sparking interest in personal AI assistants and mobile automation tools. As on-device intelligent agents gain popularity, users expect seamless, real-time AI support on their smartphones~\cite{gong2024population}. However, the limited computational and storage capabilities of edge devices pose challenges in deploying powerful models for these agents. Under these constraints, edge-cloud collaboration is a practical solution.
Apple's latest research~\cite{gunter2024apple} exemplifies this synergy by combining an efficient on-device model, AFM-on-device, with a powerful cloud-based model, AFM-server. This approach balances device limitations with the high-performance needs of Apple’s AI features. Similarly, \cite{chen2024octo} addresses edge-device limitations by splitting task planning and execution between two models—Octo-planner and Octopus—focused on efficiency and adaptability. Both approaches highlight the trend of edge-cloud collaboration to ensure powerful, low-latency AI experiences, though Apple leans more on cloud support while Octo-planner emphasizes on-device optimization.

% Despite these advancements, a trade-off remains between the power of cloud models and the real-time performance requirements of on-device models. This shift towards edge-cloud collaboration acknowledges that edge devices, while more capable, are still constrained in hosting large-scale language models. Their computational and storage limitations make it difficult to support models with 10 billion parameters or more, which often define state-of-the-art performance in many AI tasks.