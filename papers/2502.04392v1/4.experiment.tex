\vspace{-2mm}
\section{Experiments}
\input{Tables/mainResult}

\subsection{Experimental Setup}

\textbf{Benchmarks.}
We validate the effectiveness of our framework across seven open-source benchmarks.
For each benchmark, we randomly select 8 instances for manual annotation to serve as in-context learning examples of task decomposition. Additionally, we randomly extract 200 questions from each benchmark to constitute our test set. The benchmarks are categorized into four groups:
\begin{itemize}
    \item \textbf{Logical Reasoning.} 
    We select P3~\cite{schuster2021programming} and SCAN~\cite{lake2018generalization} for evaluation.
    These benchmarks place a stronger emphasis on logic, involving challenges such as traversal, backward reasoning, and anomaly detection, which require a higher level of logical coherence and rigor.
    \item \textbf{Math Problems Solving.} This category of benchmarks primarily addresses mathematical problems, involving computation, mathematical knowledge, and problem-solving techniques. It is widely regarded as a crucial test of LLMs' reasoning abilities. We select three benchmarks for this category: MATH~\cite{hendrycks2021measuring}, CHAMP~\cite{mao2024champ}, and DROP~\cite{dua2019drop}.
    \item \textbf{Commonsense Reasoning.}  CSQA~\cite{talmor2018commonsenseqa} is a widely used commonsense reasoning dataset that places less emphasis on reasoning capabilities but requires a broader knowledge base. The difference in LLMs' parameter scales leads to disparities in the knowledge systems they possess, making CSQA well-suited for evaluating our collaborative reasoning scenario.
    \item \textbf{Web Browsing.} We choose WebShop~\cite{yao2022webshop} which tests LLMs as interactive agents in real-world scenarios. It challenges LLM agents to navigate web pages, interpret complex queries, and take appropriate actions to fulfill user requests.
    This benchmark closely aligns with AI assistant scenarios, encompassing  instructions understanding, query reformulation, and strategic information exploration.
    %This benchmark is highly aligned with the scenario requirements of AI assistants, including understanding compositional instructions, reformulating queries, and strategically exploring information to meet specific requirements.
\end{itemize}


\textbf{Baselines and Prompting Methods.} 
Given the constraints of the edge-cloud collaborative setting, we establish three baselines.

\begin{itemize}
    \item \textbf{CoT~\cite{wei2022chain}.} This baseline decomposes the problem into sub-steps, solving each sequentially. We use a single LLM throughout, executing each step only once without iteration.
    \item \textbf{ToT~\cite{yao2023tree}.} ToT explores multiple solutions (M=5) for each sub-step. We use a scoring mechanism to retain the N=3 top-scored paths, with the highest-scored path determining the final answer. ToT also employs a single LLM for reasoning.
    \item \textbf{DataShunt~\cite{chen2024data}.} This approach dynamically selects between on-device SLM and cloud-based LLM at the start of each task. It prompts the LLM to evaluate task complexity, assigning a \textit{solving model} that is then used consistently for all sub-steps.
\end{itemize}
In the task decomposition processes involved in CoT and ToT, we employed the same prompting method used in our DoT, applying eight hand-crafted examples to each benchmark. These examples are randomly sampled from the dataset and are orthogonal to the test set. The specific prompts can be found in Appendix \ref{appendix:imple_details}.


\textbf{Selection and Deployment of LLMs}
We used GPT-4o~\cite{GPT-4o} as cloud-based LLM and Llama 3-8B~\cite{LLaMA3} as on-device LLM as its parameter scale is theoretically deployable on edge devices.
The Llama 3-8B model is deployed on a local A100 GPU with a context length limit of 8192. The parameter count of our adapter is only 13,109,249, approximately 1/800th of the parameters in local Llama.

\textbf{Evaluation.} 
For evaluation, we establish three metrics: $Acc$, $C_{time}$, $C_{API}$. Six of the benchmarks, excluding WebShop, have deterministic results. For instance, the results for CSQA are single-letter options, allowing direct determination of correctness. Similarly, for P3, the reasoning output can be passed into a problem function, and if the function returns True, the reasoning is deemed correct.
% It is worth noting that, although benchmarks like MATH and CHAMP have deterministic results, the presence of mathematical formulas complicates direct evaluation. Therefore, we rely on a large language model (GPT-4o) to verify the results, but the time and token costs for evaluation are not included in the total cost.
For WebShop, we use the evaluation environment provided by the original benchmark to average the reward of all purchased items on the level of satisfaction according to the constraints.

% \textbf{Parameter configuration.} xxx

\vspace{-3mm}
\subsection{Main Results}

Comparison between our method and baselines is shown in Table 1, we have highlighted in bold the highest accuracy results among the five baseline experiments on each benchmark, while the associated costs are underlined. We compute the relative improvement of our results compared to the baseline with the highest accuracy.

The experimental results demonstrate that our approach significantly reduces costs while maintaining accuracy within an acceptable range of decline. Across seven benchmarks, the relative changes in accuracy compared to the best baseline results are: -2.38\%, -7.35\%, -13.89\%, -6.35\%, 1.75\%, 5.59\%, and 0\%. Notably, positive improvements are observed in some cases, suggesting that our framework has the potential to enhance LLMs' reasoning capability. This enhancement is further discussed in ablation study~\ref{exp:ablation}. At the same time, our approach achieves a substantial reduction in cost compared to the baseline with the highest accuracy, with an average time reduction of 66.12\% and an average API cost reduction of 83.57\% which far exceeded the decline in accuracy.

Moreover, on P3 and SCAN benchmarks, CoT achieves the best performance, indicating that a sequential linear reasoning structure is better suited for these task types. In contrast, on more complex mathematical reasoning benchmarks like MATH, CHAMP, and WebShop, ToT achieves the highest reasoning accuracy, demonstrating the effectiveness of the multiple-proposal strategy. However, our approach incurs less than half of CoT's cost and only one-tenth of ToT's cost while still achieving comparable performance, highlighting the broad applicability of our graph-structured reasoning and the effectiveness of the edge-cloud collaboration. 
% Additionally, we were surprised that Llama 3-8B exhibits a limited ability to solve dynamic environmental interaction tasks such as Webshop.  

% 加了个图，记得写分析
To evaluate whether our collaborative approach fully leverages locally deployed SLMs for reasoning, we calculate the proportion of reasoning time by the SLMs relative to the overall reasoning time cost, as well as the percentage of sub-tasks assigned to SLMs. These metrics were compared against the baseline \textit{DataShunt}, and the results are presented in Figure \ref{exp: SLM}. 
The stacked bar chart illustrates the specific values of SLM and LLM, while the line graph reflects the proportion of SLM.
As shown in the figure, across all benchmarks, our allocation strategy makes more comprehensive use of SLMs, with the reasoning time and task allocation percentages exceeding those of the baseline by an average of 11.99\% and 21.92\%, respectively.
\vspace{-5mm}
\begin{figure}[h]
    \centering
    % \subfigcapskip=-3pt % 设置子图与子标题之间的距离
    \subfigbottomskip = -3pt
    \subfigure{
        \includegraphics[width=0.9\linewidth]{Figures/SLM-time-proportion.pdf}
         % 控制子图之间的垂直距离
    }
    \quad
    % 第2个子图
    \subfigure{
        \includegraphics[width=0.9\linewidth]{Figures/SLM-num-proportion.pdf}
    }
    \vspace{-5mm} % 控制子图之间的垂直距离
    \caption{Proportion of SLMs in time cost and \# sub-tasks.}
    \label{exp: SLM}
    \vspace{-4mm}
\end{figure}




\input{Tables/dataConsResult}
\input{Tables/ablation}
\begin{figure*}[h]
    \centering

    % 使用 parbox 控制两边的子标题和内容
    \parbox[b]{0.47\linewidth}{
        \centering
        \subfigure[MATH]{\includegraphics[width=0.49\linewidth]{Figures/MATH-trade-off_api.pdf}}
        \hfill
        \subfigure[CHAMP]{\includegraphics[width=0.49\linewidth]{Figures/CHAMP-trade-off_api.pdf}}
        \vskip 0.1em % 第一行和第二行的间距
        \subfigure[P3]{\includegraphics[width=0.49\linewidth]{Figures/P3-trade-off_api.pdf}}
        \hfill
        \subfigure[CSQA]{\includegraphics[width=0.49\linewidth]{Figures/CSQA-trade-off_api.pdf}}
        \\
        \textbf{$\textbf{Acc}-\textbf{C}_{\textbf{API}}$ trade-off}
    }
    \hspace{2em} % 左右分组间距
    \parbox[b]{0.47\linewidth}{
        \centering
        \subfigure[MATH]{\includegraphics[width=0.49\linewidth]{Figures/MATH-trade-off_time.pdf}}
        \hfill
        \subfigure[CHAMP]{\includegraphics[width=0.49\linewidth]{Figures/CHAMP-trade-off_time.pdf}}
        \vskip 0.1em % 第一行和第二行的间距
        \subfigure[P3]{\includegraphics[width=0.49\linewidth]{Figures/P3-trade-off_time.pdf}}
        \hfill
        \subfigure[CSQA]{\includegraphics[width=0.49\linewidth]{Figures/CSQA-trade-off_time.pdf}}
        \\
        \textbf{$\textbf{Acc}-\textbf{C}_{\textbf{Time}}$ trade-off}
    }

    \vspace{-3mm}
    \caption{$\textbf{Acc}-\textbf{Cost}$ trade-off curves on 4 benchmarks.}
    \label{exp: trade-off}
    \vspace{-4mm}

\end{figure*}



\vspace{-2mm}
\subsection{Efficiency \& Quality of Dataset Construction}

To validate the effectiveness of our method for difficulty assessment and optimal allocation search in dataset construction, we compared our $\alpha$-tree with other baseline search methods. We establish two baseline methods: \textbf{Zero-shot LLM}: This approach utilizes a LLM to assess the difficulty of sub-tasks. The evaluation model employs GPT-4o, utilizing a question-and-answer format that incorporates the task content, all sub-tasks, and the current sub-task into the prompt. \textbf{Binary-Search}: This method employs a binary search strategy. Initially, all sub-tasks are processed uniformly using GPT-4o. If the reasoning results are correct, half of the sub-tasks are randomly assigned to Llama 3-8B, with this random assignment repeated N times until successful. In the second round of binary search, half of the remaining sub-tasks assigned to the larger model are again randomly allocated to the smaller model. This process continues until all sub-tasks are either handled by the smaller model or incorrect reasoning persists. We set N to 5.

For our $\alpha$-tree approach, which sequentially searches for optimal solutions according to difficulty, we tested with n=1 and n=2 (where n represents the number of models whose allocation is altered at each search step). The comparison metrics included two quality assessment indicators: \textit{SLM Ratio}: the proportion of the smaller on-device model in the final allocation scheme, and \textit{SR}: success rate of reasoning with the final allocation scheme applied. Additionally, we considered two cost metrics: \textit{\# Evaluation}: each evaluation requires a complete reasoning pass over all sub-tasks based on the allocation strategy, and \textit{API Cost}, which refers to the expense incurred when invoking the cloud-based LLM API during dataset construction.

We visualize the quality and cost of the constructed dataset on the MATH benchmark. As shown in Table \ref{tbl:allocation}, in terms of construction quality, our $\alpha$-tree achieved the highest proportion of small model usage while maintaining the highest reasoning accuracy. This demonstrates that our method finds a more efficient collaborative allocation strategy, ensuring high-quality reasoning while minimizing inference costs. \textit{Zero-shot LLM} resulted in the lowest proportion of SLM usage, indicating that LLMs fail to assess the difficulty of sub-tasks. \textit{Binary-Search} also showed a lower SLM proportion compared to ours. While increasing the number of search attempts (N) could potentially yield better results through random search, this unordered and purely random strategy will incur significant costs. Even with the current 5-time search, the cost is already several times higher than our approach. Further scaling will bring unsustainable cost. Naturally, \textit{LLM-Eval} has the lowest cost, but its construction quality is significantly inferior.

\vspace{-4mm}

\subsection{Ablation Study}
\label{exp:ablation}

% To validate the effectiveness of each design aspect of our model, we conducted ablation studies focusing on two key components. The first ablation removes the dependency graph construction and on-graph reasoning, replacing it with a sequential reasoning resolution. The second ablation removes the model allocation mechanism, instead using the cloud-based LLM exclusively for reasoning. Notably, the second ablation experiment provides a direct assessment of whether our reasoning workflow can enhance the reasoning performance of LLMs. 

% The experimental results are presented in Table \ref{tbl:ablation}. We were pleasantly surprised to discover that, without considering reasoning costs, using the large cloud-based model exclusively for reasoning can greatly improve the reasoning accuracy. This improvement was particularly significant on the challenging MATH and CHAMP benchmarks, where accuracy increased from 63\% to 78\% and from 57\% to 64\%, respectively, compared to the \textit{ToT (GPT-4o)}, which had the highest accuracy among baselines. Improvements on other benchmarks were relatively limited. Understandably, relying solely on the cloud-based model results in several times the computational cost compared to the collaborative approach, but this cost remains acceptable when compared to ToT. Even when using the smaller model exclusively, its accuracy was significantly improved over \textit{CoT (Llama 3-8B)} and \textit{ToT (Llama 3-8B)}. For example, accuracy on Puzzle increased from 5.5\% to 29\%, and on SCAN from 17\% to 52\%. Additionally, because the smaller model is deployed on edge devices, the overall cost was greatly reduced. Additionally, the ablation results after removing the dependency graph construction and on-graph reasoning (\textit{DoT w/o Graph}) further demonstrate the critical role of the graph structure in reasoning.

% Notably, the second ablation experiment provides a direct assessment of whether our reasoning workflow can enhance the reasoning performance of LLMs. 

In this ablation study, we demonstrate the impact of the graph structure and task allocation mechanisms on the efficiency and effectiveness of our DoT model. Upon removing the graph structure, a sequential reasoning framework is used instead. In the absence of task allocation, a single language model or simple referral strategy is adopted. As shown in Table \ref{tbl:ablation}, the full DoT model, which includes both components, achieves high accuracy across various tasks while maintaining low time and API costs. When the graph component is removed, both accuracy and efficiency are reduced, indicating the impact of graph structure on enhancing reasoning performance and the effectiveness of graph-based parallel execution.

Additionally, removing the task allocation mechanism leads to higher API costs and increased time without substantial accuracy gains. For example, in the P3 task, removing allocation and solely using GPT-4o incurs a cost of 4.80\textcent compared to DoT’s 1.58\textcent, despite only a modest increase in accuracy (43\% vs. 41\%). This demonstrates that the allocation mechanism helps balance accuracy with efficiency.

% Across all tasks, our full model maintains high accuracy with consistently lower time and API costs compared to variations lacking graph or allocation components. These results validate the effectiveness of our proposed graph and task allocation mechanisms in achieving a cost-efficient balance between accuracy and computational demands, making our model especially suitable for resource-constrained applications.

Moreover, We are surprised to discover that, without considering costs, using the cloud-based LLM exclusively can greatly improve the reasoning accuracy on the challenging MATH and CHAMP benchmarks, where accuracy increased from 63\% to 78\% and from 57\% to 64\%, respectively, compared to the best baseline.


\vspace{-3mm}
\subsection{Trade-off Between Accuracy and Cost}

The ablation study demonstrates that our on-graph reasoning method significantly enhances the reasoning capability of LLMs, though at a higher computational cost. In this section, we delve into the detailed relationship between cost and reasoning accuracy to identify the achievable performance upper bound under different budget constraints. 
Based on our original task allocation strategy, we generate diverse model allocation schemes by proportionally assigning more sub-tasks to either the cloud-based LLM or the local deployed SLM according to the difficulty level of sub-tasks, resulting in the various data points shown in the figure. 
We also introduce \textit{DataShunt} as a baseline, where the proportion of tasks handled by the LLMs is adjusted by tuning the LLM evaluation thresholds. 
We test our method on four benchmarks, plotting the trade-off curves for accuracy versus API cost and accuracy versus time cost. 
As illustrated in Figure \ref{exp: trade-off}, our curve consistently remains above the baseline curve. Under the same accuracy, both the token cost and time expenditure of DoT are significantly lower than those of DataShunt, demonstrating that the model allocation method of DoT is quite efficient. Furthermore, as the budget increases, our DoT approach achieves a higher performance upper bound.

% \begin{figure}[h]
%     \centering
%     % \subfigcapskip=-3pt % 设置子图与子标题之间的距离

%     % 第1个子图
%     \subfigure[MATH]{
%         \includegraphics[width=0.45\linewidth]{Figures/MATH-trade-off_api.pdf}
%     }
%     \quad
%     % 第2个子图
%     \subfigure[CHAMP]{
%         \includegraphics[width=0.45\linewidth]{Figures/CHAMP-trade-off_api.pdf}
%     }

%     \vspace{-3mm} % 控制子图之间的垂直距离

%     % 第3个子图
%     \subfigure[P3]{
%         \includegraphics[width=0.45\linewidth]{Figures/P3-trade-off_api.pdf}
%     }
%     \quad
%     % 第4个子图
%     \subfigure[CSQA]{
%         \includegraphics[width=0.45\linewidth]{Figures/CSQA-trade-off_api.pdf}
%     }

%     \vspace{-0mm} % 控制子图之间的垂直距离

%     \caption{$\textbf{Acc}-\textbf{C}_{\textbf{API}}$ trade-off curves on 4 benchmarks.}
%     \label{exp: trade-off_api}
% \end{figure}




% \begin{figure}[h]
%     \centering
%     % \subfigcapskip=-3pt % 设置子图与子标题之间的距离

%     % 第1个子图
%     \subfigure[MATH]{
%         \includegraphics[width=0.45\linewidth]{Figures/MATH-trade-off_time.pdf}
%     }
%     \quad
%     % 第2个子图
%     \subfigure[CHAMP]{
%         \includegraphics[width=0.45\linewidth]{Figures/CHAMP-trade-off_time.pdf}
%     }

%     \vspace{-3mm} % 控制子图之间的垂直距离

%     % 第3个子图
%     \subfigure[P3]{
%         \includegraphics[width=0.45\linewidth]{Figures/P3-trade-off_time.pdf}
%     }
%     \quad
%     % 第4个子图
%     \subfigure[CSQA]{
%         \includegraphics[width=0.45\linewidth]{Figures/CSQA-trade-off_time.pdf}
%     }

%     \vspace{-0mm} % 控制子图之间的垂直距离

%     \caption{$\textbf{Acc}-\textbf{C}_{\textbf{Time}}$ trade-off curves on 4 benchmarks.}
%     \label{exp: trade-off_time}
% \end{figure}












