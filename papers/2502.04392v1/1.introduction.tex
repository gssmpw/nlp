\section{Introduction}

As web content continues to grow exponentially, on-device AI assistants have become essential tools for helping users navigate the increasingly complex online landscape. This trend has led to the widespread adoption of personal assistants such as Google Assistant, Apple Siri, Amazon Alexa, Alibaba Tmall Genie, and Xiaomi Xiao AI~\cite{kepuska2018next,mari2019voice}, which have demonstrated their effectiveness in helping users digest enormous web content for tasks like web browsing~\cite{lai2024autowebglm, chen2024large, zhou2024synergizing}, content searches~\cite{sharma2024generative}, online shopping~\cite{nie2024hybrid}, and travel planning~\cite{shao2024beyond, chiu2009towards}. These AI-powered agents enable web applications to harness the rapid advancements in AI technology, delivering a more personalized and convenient user experience. Amid recent AI breakthroughs in Large Language Models (LLMs), the emergent capabilities of commonsense reasoning~\cite{wei2022chain} and in-context learning~\cite{dong2022survey} are widely regarded as a key component for the next generation of on-device agents~\cite{gunter2024apple}. Therefore, revolutionizing AI personal assistants with LLM agents has become an important research problem and a critical focus for applications~\cite{li2024personal, li2024limp}.


However, deploying LLM agents on local devices presents significant challenges, as it is impractical to run LLMs with trillions of parameters on resource-constrained devices such as smartphones and personal computers~\cite{xu2024survey}. Conversely, relying solely on cloud-based commercial LLMs raises concerns over privacy risks, unreliable connections, and high monetary costs~\cite{li2024personal}. Recent research has focused on training smaller-scale language models and developing model compression techniques~\cite{xu2024device,lu2024small,lin2024awq}, with the goal of creating sub-10B parameter models that can be practically deployed on local devices, such as Llama 3 series~\cite{dubey2024llama} and Phi 3 series~\cite{abdin2024phi}. However, this approach introduces additional computational costs for training or compressing these models and inevitably results in performance degradation compared to full-size LLMs. Preliminary efforts have also explored the potential of edge-cloud collaboration, where tasks exceeding the capabilities of locally deployed \underline{S}maller-scale \underline{L}anguage \underline{M}odels (SLMs) are rerouted to more powerful cloud-based LLMs~\cite{chen2023frugalgpt,gupta2024language}. Despite this, the significant performance gap between SLMs and LLMs often leads to a suboptimal trade-off between reasoning capabilities and cost. 


To address this problem, our work draws inspiration from the fundamental economic concept of ``division of labour'', which posits that breaking down complex tasks into finer components often leads to more efficient solutions by allowing collaborative partners to fully exploit their respective strengths. Building on this idea, we propose a novel \underline{D}ivision-\underline{o}f-\underline{T}houghts (\textbf{DoT}) framework to fully harness the synergy between locally deployed SLMs and cloud-based LLMs through sophisticated task decomposition and optimized sub-tasks allocation. Specifically, DoT leverages a \emph{Task Decomposer}, a meta-prompt that combines ``chain-of-thought''-like prompting~\cite{wei2022chain} with carefully curated task decomposition examples. This approach taps into the inherent planning abilities of language models~\cite{song2023llm}, enabling them to decompose user queries into smaller sub-tasks. Our core insight is that even complex user queries often contain a significant portion of simple sub-tasks that can be adequately handled by SLMs. Therefore, decomposing user queries can lead to optimized collaboration on sub-task level, i.e., unleashing the potential of “Division-of-Thoughts”. Moreover, DoT employs a \emph{Task Scheduler} that analyzes the pair-wise dependency between sub-tasks and creates a dependency graph. This facilitates efficient and accurate scheduling by identifying parallel sub-tasks and assessing the structural importance of each task. More importantly, we design a self-reinforced training method to boost the task allocation capability of SLM, requiring no human annotation, but only the feedback of task execution. Our training method leverages a novel tree search algorithm that accounts for both the uncertainty of language models and the task performance to optimize sub-task allocation decisions. As a result, we can create a high-quality sub-task allocation dataset without human supervision, based on which, we train a lightweight adapter that significantly boosts the SLM's ability to allocate sub-tasks. It is important to note that the adapter fully preserves the general capabilities of the SLM, as it does not modify SLM's parameters. Instead, it introduces a detachable decoding head specialized for sub-task allocation.   




We conduct extensive experiments on seven widely adopted LLM agent benchmarks, covering a variety of scenarios, including logical reasoning, web browsing, math problems solving, and commonsense reasoning. The results demonstrate that our DoT framework significantly reduces LLM costs while maintaining reasoning accuracy comparable to the best baseline methods across all benchmarks. Specifically, the average reasoning time and API costs are reduced by 66.12\% and 83.57\%, respectively, compared to the most accurate baselines. Additionally, an ablation study confirms the effectiveness of our key model design choices. Moreover, the DoT framework consistently achieves superior cost-accuracy trade-offs compared to task referral methods that do not leverage ``Division-and-Allocate'' strategy, where the performance gain are particularly large at low budget setting. We further demonstrate that DoT exhibits strong generalization across benchmarks.


To summarize, our contributions are three-fold:

\begin{itemize}
    \item We present a novel Division-of-Thoughts (DoT) framework that fully exploits the synergy between locally deployed SLM and cloud-based LLM to power on-device agents with cost-effective reasoning. 

    % \item We design a \emph{Task Scheduler} to extract the dependency graph among sub-tasks, facilitating optimal sub-task scheduling.  
    
    \item We propose a self-reinforced training method to boost the SLM's task allocation accuracy without additional human annotation, and also preserve the general reasoning capabilities of SLM with a detachable, plug-and-play adapter.   

    \item We conduct extensive experiments on a wide range of benchmarks to evaluate the effectiveness of our DoT framework and found that DoT can significantly reduce LLM costs while maintaining high reasoning accuracy.
    
\end{itemize}



\begin{figure*}
    \centering
    \includegraphics[width=0.8\linewidth]{Figures/framework.pdf}
    \vspace{-3mm}
    \caption{Overview of Our Proposed DoT Framework.}
    \label{fig:framework}
\end{figure*}