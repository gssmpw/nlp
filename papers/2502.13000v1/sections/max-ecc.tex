\section{Improved \maxecc{} Algorithms}
\textbf{Preliminaries.} For a positive integer $n$, let $[n] = \{1,2, \hdots n\}$.
We use bold lowercase letters to denote vectors, and indicate the $i$th of entry of a vector $\textbf{x} \in \mathbb{R}^n$ by $x_i$. For a set $S$ and a positive integer $t$, let $\binom{S}{t}$ denote all subsets of $S$ of size $t$.
An instance of edge-colored clustering is given by a hypergraph $H = (V,E, \ell)$ where $V$ is a node set, $E$ is a set of hyperedges (usually just called \emph{edges}), and $\ell \colon E \rightarrow [k]$ is a mapping from edges to a color set $[k] = \{1,2, \hdots, k\}$.  For $e \in E$, we let $\omega_e \geq 0 $ denote a nonnegative weight associated with $e$, which equals 1 for all edges in the unweighted version of the problem. For a color $c \in [k]$, let $E_c \subseteq E$ denote the edges of color $c$. We use $r$ to denote the \emph{rank} of $H$, i.e., the maximum hyperedge size.

The goal of ECC is to construct a map $\lambda \colon V \rightarrow [k]$ that associates each node with a color, in order to optimize some function on edge \emph{satisfaction}. Edge $e \in E$ is \emph{satisfied} if $\ell(e) = \lambda(v)$ for each $v \in e$, and is otherwise \emph{unsatisfied}. The two most common objectives are maximizing the number of satisfied edges (\maxecc{}) or minimizing the number of unsatisfied edges (\minecc{}).

\textbf{Challenges in approximating \maxecc{}.} Although \minecc{} and \maxecc{} are equivalent at optimality, the latter is far more challenging to approximate. Due to an approximation-preserving reduction from \textsc{Independent Set}, it is NP-hard
to approximate \maxecc{} in hypergraphs of unbounded rank $r$ to within a factor $|E|^{1-\varepsilon}$~\cite{veldt2023optimal,zuckerman2006linear}. There also are simple instances (e.g., a triangle with 3 colors) where a simple $2$-approximation of~\citet{amburg2020clustering} for \minecc{} (round variables of an LP relaxation to 0 if they are strictly below $1/2$, otherwise round to 1) fails to satisfy \emph{any} edges.
These challenges do not rule out the possibility of approximating \maxecc{} when $r$ is constant. Indeed, there are many approximations for graph \maxecc{} ($r=2$), but these require lengthy proofs, rely fundamentally on the assumption that the input is a graph, and do not easily extend even to the $r = 3$ case. Here we provide a generalized approach that gives the first approximation guarantees for hypergraph \maxecc{}, when $r$ is constant. Our approach also provides a simplified way to approximate graph \maxecc{}; we design and analyze a refined algorithm that achieves a new best approximation factor for graph \maxecc{}, improving on a long line of previous algorithms~\cite{angel2016clustering,alhamdan2019approximability,ageev2015improved,ageev20200}.

\subsection{Technical preliminaries for LP rounding algorithms}
\maxecc{} can be cast as a binary linear program (BLP):
\begin{align}
	\label{eq:maxecc}
	\begin{aligned}
		\text{max} \quad  & \textstyle \sum_{e \in E} \omega_e z_e \\
		\text{s.t.} \quad & \forall v \in V:                       \\
		                  & \forall c \in [k], e \in E_c:          \\
		                  & x_v^c, z_e \in \{0, 1\}
	\end{aligned}
	\begin{aligned}
		      &                                      \\
		      & \textstyle \sum_{c=1}^k x_v^c = 1    \\
		\quad & x_v^c \geq z_e \quad \forall v \in e \\
		      & \forall c \in [k], v \in V, e \in E.
	\end{aligned}
\end{align}
Setting $x_{u}^c$ to 1 indicates that node $u$ is given color $c$ (i.e., $\lambda(u) = c$). We use $\textbf{x}_u = \begin{bmatrix} x_u^1 & x_u^2 & \cdots & x_u^k \end{bmatrix}$ to denote the vector of variables for node $u$. For edge $e \in E$, the constraints are designed in such a way that $z_e = 1$ if and only if $e$ is satisfied. A binary LP for \minecc{} can be obtained by changing the objective function to $\min \sum_{e \in E} \omega_e (1-z_e)$.

The LP relaxation for \maxecc{} can be obtained by relaxing the binary constraints in Binary Linear  Program~\eqref{eq:maxecc} to linear constraints $0 \leq x_v^c \leq 1$ and $0 \leq z_e \leq 1$. Solving this LP gives a fractional node-color assignment $x_v^c \in [0,1]$. The closer $x_v^c$ is to 1, the stronger this indicates node $v$ should be given color $c$. Our task is to round variables to assign one color to each node in a way that satisfies provable approximation guarantees.

Our algorithms (Algorithms~\ref{alg:hyper_maxecc} and~\ref{alg:graph_max_ecc}) are randomized. Both use the same random process to identify colors that a node ``wants''. For each $c \in [k]$ we {independently} generate a uniform random \emph{color threshold} $\alpha_c \in [0,1]$. If $x_u^c > \alpha_c$, we say that \emph{node $u$ wants color $c$}, or equivalently that color $c$ wants node $u$. Because a node may want more than one color, we use a random process to choose one color to assign, informed by the colors the node wants. Our approximation proofs rely on (often subtle) arguments about events that are independent from each other. We begin by presenting several useful observations that will aid in proving our results.

Our first observation is that if we can bound the expected cost of every edge in terms of the LP upper bound, it provides an overall expected approximation guarantee.
\begin{observation}
	\label{obs:prob}
	Let $\mathcal{A}$ be a randomized ECC algorithm and $p \in [0,1]$ be a fixed constant. If for each $e \in E$ we have $\prob[\text{$e$ is satisfied by  $\mathcal{A}$}] \geq p z_e$, then $\mathcal{A}$ is a $p$-approximation.
\end{observation}
Let $X_u^c$ denote the event that $u$ wants $c$, and $Z_e$ be the event that every node in edge $e \in E$ wants color $c = \ell(e)$.
\begin{observation}
	\label{obs:xuc}
	For each node $v \in V$, the events $\{X_v^c\}_{c \in [k]}$ are independent, and $\prob[X_v^c] = \prob[\alpha_c < x_v^c] = x_v^c \leq 1$.
\end{observation}
\begin{observation}
	\label{obs:edgewants}
	$\prob[Z_e] = \prob[\cap_{v \in e} X_v^c] = \min_{v \in e} x_v^c = z_e$.
\end{observation}

For an edge $e$ and color $i \neq \ell(e)$, we frequently wish to quantify the possibility that some node in $e$ wants color $i$, as this opens up the possibility that $e$ will be unsatisfied because some node $v \in e$ is given color $i$. Towards this goal, for each $e \in E$ and color $i \in [k]$ we identify one node in $e$ that has the highest likelihood of wanting $i$. Formally, we identify some \emph{representative} node $v \in e$ satisfying $x_v^i \geq x_u^i$ for every $u \in e$ (breaking ties arbitrarily if multiple nodes satisfy this), and we define $\sigma_e(i) = v$.
The definition of $\sigma_e(i)$ implies the following useful observation:
\begin{observation}
	\label{obs:cv}
	% One or more nodes in $e$ wants color $i$ if and only if $v = \sigma(i)$ wants color $i$. Equivalently, 
	Color $i$ does not want \textbf{any} nodes in $e$ $\iff$ color $i$ does not want $v = \sigma_e(i)$.
\end{observation}
Variations of Observations~\ref{obs:prob}-\ref{obs:edgewants} have often been used to prove guarantees for previous randomized ECC algorithms. However, Observation~\ref{obs:cv} is new and is key to our analysis.
% and is a key factor that allows us to design an algorithm that applies to hypergraph \maxecc{} and provides a much simpler way to get an approximation for graph \maxecc{}.
%Prior algorithms for graph \maxecc{} typically bound the probability of satisfying an edge $e$ by considering 
%as it will allow us to bound the probability of satisfying an edge $e$ in terms of certain independent events (e.g., the event that a certain color $i$ wants its representative node $v = \sigma_e(i)$) rather than reasoning about. 
% Using $\sigma_e$ to partition colors (based on which node $v$ is the representative for each color) will allow us to 


\subsection{Hypergraph MaxECC Algorithm}
%Our hypergraph \maxecc{} algorithm (Algorithm~\ref{alg:hyper_maxecc}) generates a uniform random priority for colors $\pi$, and independent random color thresholds $\{\alpha_c\}$ to determine which nodes and colors ``want'' each other. A node is then assigned to the highest priority color it wants. 
%After generating a uniform random permutation $\pi$ to encode a priority for and color thresholds $\{\alpha_c\}$, 
In addition to color thresholds $\{\alpha_c\}$, our hypergraph \maxecc{} algorithm (Algorithm~\ref{alg:hyper_maxecc}) generates a uniform random permutation  $\pi$ to define priorities for colors. A node is then assigned to the highest priority color it wants.
\begin{algorithm}[t]
	\caption{Approximation alg. for hypergraph \maxecc{}}
	\label{alg:hyper_maxecc}
	\begin{algorithmic}
		\STATE Obtain optimal variables $\{z_e; x_v^c\}$ for the LP relaxation
		\STATE $\pi \leftarrow \text{uniform random ordering of colors } [k]$
		\STATE For $c \in [k]$, $\alpha_c \leftarrow $ uniform random threshold in $[0,1]$
		\FOR{$v \in V$}
		\STATE $\mathcal{W} = \{c \in [k] \colon \alpha_c < x_v^c\}$
		\IF{$|\mathcal{W}| > 0$}
		\STATE $\lambda(v) \leftarrow \argmax_{c \in \mathcal{W}} \pi(c)$
		\ELSE
		\STATE $\lambda(v) \leftarrow $ arbitrary color
		\ENDIF
		\ENDFOR
	\end{algorithmic}
\end{algorithm}

\begin{theorem}
	\label{thm:hypermaxecc}
	Algorithm \ref{alg:hyper_maxecc} is a $\left(\frac{1}{r+1} \left(\frac{2}{e}\right)^r\right)$-approximation algorithm for \maxecc{} in hypergraphs with rank $r$.
\end{theorem}
\begin{proof}
	Fix an arbitrary edge $e \in E$ and let $c = \ell(e)$. Let $T_e$ denote the event that $e$ is satisfied.
	By Observation~\ref{obs:prob}, it suffices to show $\prob[T_e] \geq \frac{z_e}{r+1} \left(\frac2e\right)^r$.

	Let $C = [k]\setminus \{c\}$.
	To partition the color set $C$, for each $v \in e$ we define
	%	\begin{equation*}
	$C_v = \{i \in C \colon \sigma_e(i) = v\}$.
	%	\end{equation*}
	Recall that $\sigma_e(i)$ identifies a node $v \in e$ satisfying $x_v^i = \max_{u \in e} x_u^i$.
	Let $A_v$ denote the event that at most one color in $C_v$ wants one or more nodes in $e$. From Observation~\ref{obs:cv}, $A_v$ is equivalent to the event that  $v$ wants at most 1 color in $C_v$. Thus, the probability of $A_v$ is the probability that at most one of the events $\{X_v^i \colon i \in C_v \}$ happens. Observation~\ref{obs:xuc} gives $\sum_{i = 1}^k \prob[X_v^i] = \sum_{i = 1}^k x_v^i = 1$, allowing us to repurpose a supporting lemma of~\citet{angel2016clustering} on graph \maxecc{} to see that $\prob[A_v] \geq 2/e$ (Lemma~\ref{lem:angel} in Appendix~\ref{app:maxecc}).

	%	Observe now that the events $\{A_v \colon v \in e \}$ are mutually independent because the sets $\{C_v \colon v \in e\}$ are disjoint sets of colors, and the color thresholds $\{\alpha_i \colon i \in C\}$ are all drawn independently from each other. Furthermore, each $A_v$ is independent from $X_v^c$, since $c \notin C_v$. 
	%	
	Because color thresholds $\{\alpha_i\}$ are drawn independently for each color, and because color sets $\{C_v \colon v \in e\}$ are disjoint from each other and from $c$, the events $\{A_v, X_v^c \colon v \in e\}$ are mutually independent.
	Thus, using Observation~\ref{obs:edgewants} gives
	\begin{align*}
		\prob\left[ \left(\bigcap_{v \in e} A_v \right) \cap Z_e\right ] & = \prob[Z_e]\cdot \prod_{v \in e} \prob[A_v] \geq z_e \cdot \left(\frac2e \right)^r.
	\end{align*}
	If the joint event $J = (\cap_{v \in e} A_v) \cap Z_e$ holds, this means every node in $e$ wants color $c$, and at most $r$ distinct other colors (one for each node in $v \in e$ since there is one set $C_v$ for each $v \in e$) want one or more nodes in $e$. Conditioned on $J$, $e$ is satisfied if the color $c$ has a higher priority (determined by $\pi$) than the other $r$ colors, which happens with probability $1/(r+1)$. Thus,
	\begin{align*}
		{	\prob[T_e] \geq \prob\left[T_e \mid J\right] \prob\left[ J \right] \geq \frac{z_e}{r+1}\left(\frac2e\right)^r.}
	\end{align*}
\end{proof}

\textbf{Comparison with prior graph \maxecc{} algorithms.}
Algorithm~\ref{alg:hyper_maxecc} achieves a ${4}/({3e^2}) \approx 0.18$ approximation factor for graph \maxecc{} ($r = 2$). This improves upon the first ever approximation factor of $1/e^2 \approx 0.135$ for graph \maxecc{}~\cite{angel2016clustering}, and comes with a significantly simplified proof. There are two interrelated factors driving this simplified and improved guarantee.
The first is our use of Observation~\ref{obs:cv}.
\citet{angel2016clustering} bound the probability of satisfying an edge $(u,v) \in E_c$ using a delicate argument about certain dependent events we can denote by $B_v$ (the event that $v$ wants at most 1 color from $C = [k]\setminus \{c\}$) and $B_u$ (defined analogously for $u$).
%Let $\hat{A}_{u \land v}$ be the event that $u$ and $v$ each simulteneously want at most one color in $C$. 
The algorithm of \citet{angel2016clustering} requires proving that $\prob[B_{u} \cap B_v] \geq \prob[B_{u}] \prob[B_v]$, even though $B_u$ and $B_v$ are dependent. The proof of this result (Proposition 1 in~\citet{angel2016clustering}) is interesting but also lengthy, and does not work for $r > 2$. Subsequent approximation algorithms for graph \maxecc{} in turn rely on complicated generalizations of this proposition~\cite{ageev2015improved,ageev20200,alhamdan2019approximability}. In contrast, Theorem~\ref{thm:hypermaxecc} deals with mutually independent events $\{A_v \colon v \in e\}$. Observation~\ref{obs:cv} provides the key insight as to why it suffices to consider these events when bounding probabilities, leading to a far simpler analysis that extends easily to hypergraphs.

%Observation~\ref{obs:cv} works in tandem with our 
Our second key factor is the use of a global color ordering $\pi$. \citet{angel2016clustering} and other results for graph \maxecc{} apply a two-stage approach where Stage 1 identifies which nodes ``want'' which colors, and Stage 2 assigns nodes to colors \emph{independently} for each node. To illustrate the difference, consider the probability of satisfying an edge $(u,v) \in E_c$ if we condition on $u$ and $v$ both wanting $c$ and each wanting at most one other color. The algorithm of~\citet{angel2016clustering} has a $1/4$ chance of satisfying the edge (each node gets color $c$ with probability $1/2$), whereas Algorithm~\ref{alg:hyper_maxecc} has a $1/3$ chance (the probability that $c$ is given higher global priority than the other two colors). This is precisely why Algorithm~\ref{alg:hyper_maxecc}'s approximation guarantee is a factor $4/3$ larger than the guarantee of~\citet{angel2016clustering}.


\begin{algorithm}[t]
	\caption{$0.38$-approximation alg. for graph \maxecc{}}
	\label{alg:graph_max_ecc}
	\begin{algorithmic}
		\STATE Obtain optimal variables $\{z_e; x_v^c\}$ for the LP relaxation
		\STATE $\pi \leftarrow \text{uniform random ordering of colors } [k]$
		% \FOR{$c \in [k]$}
		%     \STATE $\alpha_c \leftarrow \text{ a uniform random value in } [0, 1]$
		% \ENDFOR
		\STATE For $c \in [k]$, $\alpha_c \leftarrow $ uniform random threshold in $[0,1]$
		\FOR{$v \in V$}
		\STATE $S_v \leftarrow \left\{c \in [k] \mid x_v^c \geq 2/3 \right\}$; $W_v \leftarrow [k] \setminus S_v$
		% \STATE
		\STATE $W'_v = \left\{ c \in W_v \mid \alpha_c < x_v^c \right\}$
		% \STATE
		\IF{$|W'_v| > 0$}
		\STATE $\lambda(v) \leftarrow \argmax_{i \in W'_v} \pi(i)$
		\ELSIF{$\exists c \text{ s.t. } S_v = \left\{ c \right\}$}
		\STATE $\lambda(v) \leftarrow c$
		\ELSE
		\STATE $\lambda(v) \leftarrow \text{ arbitrary color}$
		\ENDIF
		\ENDFOR
	\end{algorithmic}
\end{algorithm}


\subsection{Graph MaxECC Algorithm}
Although Algorithm~\ref{alg:hyper_maxecc} does not improve on the $0.3622$-approximation of~\citet{ageev20200} for graph \maxecc{}, we can incorporate its distinguishing features (the color ordering $\pi$ and Observation~\ref{obs:cv}) into a refined algorithm with a $154/405 \approx 0.38$ approximation factor.

Our refined algorithm (Algorithm \ref{alg:graph_max_ecc}) for graphs solves the LP relaxation, generates color thresholds $\{\alpha_i \colon i \in [k]\}$, and generates a color ordering $\pi$ in the same way as Algorithm~\ref{alg:hyper_maxecc}. It differs in that it partitions colors for each $v$ into colors that are \emph{strong} or \emph{weak} for $v$, given respectively by the sets
\begin{align*}
	S_v = \{ i \in [k] \colon x_v^i \geq 2/3\} \text{ and } W_v & = [k] - S_v.
\end{align*}
Since $\sum_{i = 1}^k x_v^i = 1$, we know $|S_v| \in \{0,1\}$. We say color $i$ is \textit{strong for $v$} if $S_v = \{i\}$, otherwise $i$ is \textit{weak for $v$}. Note that a color being \emph{strong} or \emph{weak} for $v$ is based on a fixed and non-random LP variable. This is separate from the notion of a color \emph{wanting} $v$, which is a random event.

Algorithm~\ref{alg:graph_max_ecc} first checks if $v$ wants any {weak} colors. If so, it assigns $v$ the weak color of highest priority (using $\pi$) that $v$ wants. If $v$ wants no weak colors but has a strong color, then $v$ is assigned the strong color. Prioritizing weak colors in this way appears counterintuitive, since if $v$ has a strong color $c$ it suggests that $v$ should get color $c$. However, note that $x_v^c \geq 2/3$ still implies $v$ will get color $c$ with high probability, since a large value for $x_v^c$ makes it less likely $v$ will want any weak colors. Meanwhile, prioritizing weak colors enables us to lower bound the probability that an edge $e = (u,v)$ is satisfied even if $\ell(e)$ is weak for $u$ or $v$.

In order to improve on the extensively studied problem of \maxecc{} in graphs, our analysis is much more involved than the proof of Theorem~\ref{thm:hypermaxecc} and requires proving several detailed technical lemmas that may be of interest in their own right. In order to present the lemmas used, we use the following definition.

\begin{definition}
	\label{def:prob}
	Let $m$ be a nonnegative integer and $t$ be an integer. Given a vector $\vx \in \mathbb{R}^m$, if $m \geq t > 0$, we define the function $P(\vx, t)$ as follows:
	\[
		P(\vx, t) = \sum_{I \in {[m] \choose t}} \left( \prod_{i \in I} x_i \prod_{j\in [m] \setminus I} (1 - x_j) \right),
	\]
	and for other choices of $t$ and $m$ we define
	\begin{equation*}
		P(\vx, t) = \begin{cases}
			0                         & \text{ if $m < t$ or $t < 0$} \\
			%			0 & \text{ if $t < 0$} \\
			1                         & \text{ if $0 = m = t$}        \\
			\prod_{i = 1}^m (1 - x_i) & \text{ if $0 = t < m$.}
		\end{cases}
	\end{equation*}
\end{definition}
To provide intuition, $P(\vx,t)$ is defined to encode the probability that $t$ events from a set of $m$ independent events happen. In more detail, consider a set of $m$ mutually independent events $\mathcal{X} = \{X_1, X_2, \cdots, X_m\}$, where the probability that the $i$th event happens is $x_i = \prob[X_i]$, the $i$th entry of $\vx$. The function $P$ in Definition~\ref{def:prob} is the probability that exactly $t$ of the events in $\mathcal{X}$ happen. In our proofs for \maxecc{}, we will apply this with $\mathcal{X}$ representing a subset of colors in the ECC instance, while the vector $\vx$ encodes LP variables $\{x_u^i\}$ for some node $u$ and that set of colors (which by Observation~\ref{obs:xuc} are probabilities for wanting those colors). Lemmas~\ref{lem:bounding} and \ref{lem:bounding_constraints} will aid in bounding the probability that a node is assigned a certain color, conditioned on how many other colors it wants.


The technical lemmas used in the proof of Theorem~\ref{thm:graphecc} are given below. The proofs for these theorems can be found in Appendix~\ref{app:maxecc}.
\begin{restatable}{lemma}{lemdependentynx}
	\label{lem:dependent_y_n_x}
	Consider an edge $e = (u,v)$ of color $c$ and let $c \in W_u$. When running Algorithm~\ref{alg:graph_max_ecc}, let $X_u^c$ be the event that $u$ wants $c$, $Y_u^c$ be the event that $u$ is assigned $c$ by the algorithm, and let $N_v$ be the event that $v$ wants no colors in $[k]\setminus\{c\}$.
	Then the following inequality holds:
	\[
		\prob\left[Y_u^c \mid N_v \cap X_u^c\right] \geq \prob\left[Y_u^c \mid X_u^c\right].
	\]
\end{restatable}

\begin{restatable}{lemma}{lemsumtoprod}
	\label{lem:sum_to_prod}
	Let $\beta \in [0,1]$ be a constant and $\mathbf{x} \in \mathbb{R}^m_{\geq 0}$ be a length $m$ nonnegative vector satisfying $\sum_{t=1}^m x_t \leq \beta$, then we have $\prod_{t=1}^m (1 - x_t) \geq 1 - \beta$.
\end{restatable}

\begin{restatable}{lemma}{lembounding}
	\label{lem:bounding}
	Let $m \geq 2$ be an integer and
	$a_0, a_1, \hdots, a_m$ be values such that for every $t \in [0, m-2]$
	\begin{align*}
		a_{t+1}   & \leq a_t            \\
		2 a_{t+1} & \leq a_t + a_{t+2}.
	\end{align*}
	Let $\mathcal{D} = \{ \vx \in [0,2/3]^m \colon \sum_{i = 1}^m x_i \leq 1 \}$ be the domain of a function
	$f : \mathcal{D} \rightarrow \mathbb{R}$ defined as
	\[
		f(\vx) = \sum_{t=0}^m a_t P(\vx, t).
	\]
	Then $f$ is minimized (over domain $\mathcal{D}$) by any vector $\vx^*$ with one entry set to $2/3$, one entry set to $1/3$, and every other entry set to $0$. Furthermore, we have
	\begin{equation}
		\label{eq:minval}
		f(\vx^*) = \frac{2}{9}(a_0 + a_2) + \frac{5}{9}a_1.
	\end{equation}
\end{restatable}

\begin{restatable}{lemma}{lemboundingconstraints}
	\label{lem:bounding_constraints}
	The inequalities $a_t \geq a_{t+1}$ and $a_t + a_{t+2} \geq 2a_{t+1}$ hold for sequence $a_t = \frac{1}{1+g+t}$ where $g \geq 0$ is an arbitrary fixed integer. These inequalities also hold for the sequence
	\begin{align}
		\label{eq:complicatedat}
		a_t & = \frac{2}{9}\left( \frac{1}{t+1} + \frac{1}{t+3} \right) + \frac{5}{9} \left( \frac{1}{t+2} \right).
	\end{align}
\end{restatable}

Using the above technical lemmas, we prove the following result for an arbitrary edge $e$, which by Observation~\ref{obs:prob} proves our approximation guarantee.
\begin{theorem}
	\label{thm:graphecc}
	For every $e \in E$, $\prob[\text{$e$ is satisfied}] \geq p z_e$ where $p = {154}/{405} > 0.3802$ when running Algorithm~\ref{alg:graph_max_ecc}.
\end{theorem}
\vspace{-10pt}
\begin{proof}

	Fix $e = (u,v)$, let $c = \ell(e)$, and set $C = [k] \backslash \{c\}$.
	Define $C_v = \{i \in C \colon x_v^c \geq x_u^c\}$ and $C_u = C \setminus C_v$. As before, $T_e$ is the event that $e$ is satisfied and $X_v^i$ is the event that $v$ wants color $i$. Let $Y_v^i$ be the event that $v$ is \emph{assigned} color $i$ by Algorithm~\ref{alg:graph_max_ecc}, and $N_v$ be the event that $v$ wants \emph{no} colors in $C$. If $c$ is strong for $v$, event $N_v$ is equivalent to event $Y_v^c$. Let $W_v'$ denote colors in $W_v$ that {want} $v$. Define $X_u^i$, $Y_u^i$, $W_u'$, and $N_u$ analogously for $u$. The proof is separated into (increasingly difficult) cases, based on how many of $\{u,v\}$ have $c$ as a strong color.

	\bigbreak

	\noindent
	\textbf{Case 1:} $c$ is strong for both $u$ and $v$, i.e., $S_u = S_v = \{c\}$.\\
	Let $W_v'$ denote colors in $W_v$ that want $v$, and define $W_u'$ analogously for $u$.
	Since $c$ is the strong color for both $u$ and $v$, $e$ is satisfied if and only if $W'_u = W'_v = \emptyset$. Additionally, because nodes can only have a single strong color and we know $c$ is strong for both $u$ and $v$ we know that all other colors must be weak for both $u$ and $v$. Using the property that probabilities regarding separate colors are independent we have
	\begin{align*}
		\prob\left[e \text{ is satisfied}\right] & = \prob\left[W'_u = W'_v = \emptyset\right]  = \prod_{i \in C}\prob\left[ i \notin W'_u \cup W'_v \right].
	\end{align*}
	Consider a color $i \in C_v$, which by definition means $x_v^i \geq x_u^i$. There are three options for the random threshold $\alpha_i$:
	\begin{itemize}
		\item $\alpha_i < x_u^i \leq x_v^i$, which happens with probability $x_u^i$ and implies that $i \in W'_u \cap W'_v$,
		\item $x_u^i < \alpha_i \leq x_v^i$, which happens with probability $x_v^i - x_u^i$ and implies $i \in W'_v$, $i \notin W'_u$, and
		\item $x_u^i \leq x_v^i < \alpha_i$, which happens with probability $1 - x_v^i$ and implies $i \notin W_u' \cup W_v'$.
	\end{itemize}
	Thus, for $i \in C_v$ we have $\prob\left[i \notin W'_u \cup W'_v\right] = (1 - x_v^i)$ and similarly for $i \in C_u$ we have $\prob\left[i \notin W'_u \cup W'_v\right] = (1 - x_u^i)$. This gives
	\begin{align*}
		\prod_{i \in C}\prob\left[ i \notin W'_u \cup W'_v \right] & = \prod_{i \in C_v}(1 - x_v^i)\prod_{i \in C_u}(1 - x_u^i).
	\end{align*}
	The fact that $c$ is strong for both $u$ and $v$ means $x_v^i \geq 2/3$ and $x_u^i \geq 2/3$. From the equality constraint in the LP we see that for $w\in\{u,v\}$ we have $\sum_{i \in C}x_w^i \leq 1 - 2/3 = 1/3$. Applying Lemma \ref{lem:sum_to_prod} gives
	\begin{align*}
		\prod_{i \in C_v}(1 - x_v^i)\prod_{i \in C_u}(1 - x_u^i) & \geq \frac{2}{3}\frac{2}{3} = \frac{4}{9} \geq \frac{4}{9}z_e.
	\end{align*}

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	\bigbreak

	\noindent
	\textbf{Case 2:} $c$ is strong for one of $u$ or $v$. \\
	Without loss of generality we say $S_v = \{c\}$ and $c \in W_u$.
	Edge $e$ is satisfied if and only if $Y_u^c \cap Y_v^c$ holds.
	Because $c$ is strong for $v$, event $Y_v^c$ holds if and only if $N_v$ holds. Using the fact that $Y_u^c = Y_u^c \cap X_u^c$, we can write
	\[
		\prob\left[e \text{ is satisfied}\right] = \prob\left[Y_u^c \cap X_u^c \cap N_v\right].
	\]
	Using Bayes' Theorem, the fact that $X_u^c$ and $N_v$ are independent\footnote{Independence follows from the fact that $X_u^c$ is concerned with color $c$, while $N_v$ is concerned with a disjoint color set $C = [k]\backslash \{c\}$.}, Observation~\ref{obs:xuc} ($\prob[X_u^c] = x_u^c$), and Lemma~\ref{lem:dependent_y_n_x}, we see that
	\begin{align*}
		\prob\left[Y_u^c \cap X_u^c \cap N_v\right] & = \prob\left[ Y_u^c \mid N_v \cap X_u^c \right] \prob\left[ N_v \cap X_u^c \right]                & \text{(Bayes' Theorem)}                     \\
		                                            & = \prob\left[ Y_u^c \mid N_v \cap X_u^c \right] \prob\left[ N_v \right] \prob\left[ X_u^c \right] & \text{($X_u^c$ and $N_v$ are indepdendent)} \\
		                                            & = \prob\left[ Y_u^c \mid N_v \cap X_u^c \right] \prob\left[ N_v \right] x_u^c                     & \text{(Observation~\ref{obs:prob})}         \\
		                                            & \geq \prob\left[ Y_u^c \mid N_v \cap X_u^c \right] \prob\left[ N_v \right] z_e                    & \text{(LP constraint $x_u^c \geq z_e$)}     \\
		                                            & \geq \prob\left[ Y_u^c \mid X_u^c \right] \prob\left[ N_v \right] z_e                             & \text{(Lemma~\ref{lem:dependent_y_n_x}).}
	\end{align*}
	Using $\overline{X}_v^i$ to indicate that $v$ \emph{does not} want $i$, we get
	\[
		\prob\left[ N_v \right] = \prob\left[ \bigcap_{i \in C} \overline{X}_v^i \right] = \prod_{i \in C}(1 - x_v^i).
	\]
	As in Case 1, because $c$ is strong for $v$, we know that $\sum_{i \in C} x_v^i \leq 1/3$ and applying Lemma \ref{lem:sum_to_prod} gives us that $\prob\left[ N_v \right] \geq 2/3$.


	Finally we must bound $\prob\left[ Y_u^c \mid X_u^c \right]$. Since
	$c$ is weak for $u$ (i.e., $c \in W_u$), it will be convenient to consider all weak colors for $u$ other than $c$, which we will denote by $\hat{W}_u = W_u \backslash \{c\}$. We will then use $\hat{W}_u' = \{ i \in \hat{W}_u \colon \alpha_i < x_u^i\}$ to denote the set of colors in $\hat{W}_u$ that $u$ wants. Note that $X_u^c$ is independent of the number of colors in $\hat{W}_u$ that $u$ wants.

	Because of the global ordering of colors, conditioned on $u$ wanting $c$ and wanting exactly $t$ colors in $\hat{W}_u$, there is a $1/(t+1)$ chance that $c$ is chosen first by the permutation $\pi$, and is therefore assigned to $u$. Formally:
	\begin{equation*}
		\prob\left[ Y_u^c \mid X_u^c  \cap (|\hat{W}'_u| = t)\right] = \frac{1}{t+1}.
	\end{equation*}
	Define $p_t = \prob[ |\hat{W}'_u| = t]$ to be the probability that exactly $t$ colors from $\hat{W}_u$ are wanted by $u$. Using the vector $\vx_u[\hat{W}_u]$ to encode LP variables $\{x_u^i \colon i \in \hat{W}_u\}$ for node $u$ and the colors in $\hat{W}_u$, we see that $p_t = P(\vx_v[\hat{W}_u], t)$. Therefore, the law of total probability (combined with the fact that $X_u^c$ is independent from $|\hat{W}_u'|$) gives
	\begin{equation}
		\label{eq:mainprob}
		\prob\left[ Y_u^c \mid X_u^c \right]
		%		= p_0 + \frac{1}{2}p_1 + \cdots + \frac{1}{|\hat{W}_u| + 1}p_{|\hat{W}_u|}.
		= \sum_{t = 0}^{|\hat{W}_u|} \prob\left[ Y_u^c \mid X_u^c  \cap (|\hat{W}'_u| = t)\right] \prob[|\hat{W}'_u| = t] = \sum_{t = 0}^{|\hat{W}_u|} \frac{1}{1+t} p_t.
	\end{equation}
	Eq.~\eqref{eq:mainprob} is exactly of the form in Lemma \ref{lem:bounding} with $a_t = 1/(t+1)$. From Lemma \ref{lem:bounding_constraints}, we know this sequence $\{a_t\}$ satisfies the constraints needed by Lemma \ref{lem:bounding}, and applying Lemma~\ref{lem:bounding} shows $\prob\left[ Y_u^c \mid X_u^c \right] \geq 31/54$. Combining this with the previously established bound $\prob\left[ N_v \right] \geq 2/3$ gives
	\begin{align*}
		\prob\left[e \text{ is satisfied}\right] & \geq \prob\left[ Y_u^c \mid X_u^c \right] \prob\left[ N_v \right] z_e  \geq \frac{31}{54} \cdot \frac{2}{3} z_e = \frac{31}{81}z_e > \frac{154}{405} z_e.
	\end{align*}

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	\bigbreak

	\noindent
	\textbf{Case 3:} $c$ is weak for both $u$ and $v$, i.e., $c \in W_u \cap W_v$.\\
	First we condition the probability of satisfying $e$ on the event $X_u^c \cap X_v^c$. If one of $X_u^c$ or $X_v^c$ does not happen, then $e$ cannot be satisfied, so we have
	\begin{align*}
		\prob\left[ \text{$e$ is satisfied} \right] & = \prob\left[ \text{$e$ is satisfied} \mid X_u^c \cap X_v^c \right] \prob\left[ X_v^c \cap X_u^c \right] = \prob\left[ \text{$e$ is satisfied} \mid X_u^c \cap X_v^c \right] z_e.
	\end{align*}
	Define $D = (W_u \cup W_v) \backslash \{c\}$ to be the set of weak colors (excluding $c$) that want at least one of $\{u,v\}$. If we condition on $u$ and $v$ both wanting $c$, it is still possible that $e$ will not be satisfied. This will happen if a weak color $i \in D$ wants either $u$ or $v$, and the permutation $\pi$ prioritizes color $i$ over $c$. We bound the probability of satisfying $e$ by conditioning on the number of colors in $D$ that want one or both of $\{u,v\}$.

	Using the same logic as in Observation~\ref{obs:cv}, we can present a simpler way to characterize whether a color $i \in D$ wants at least one of $\{u,v\}$. Formally, we partition $D$ into the sets:
	\begin{align*}
		D_u & = \{i \in D \colon x_u^i \geq x_v^i \} \\
		D_v & = D - D_v.
	\end{align*}
	Note that a color $i \in D_u$ wants one or both nodes in $e$ if and only if $i$ wants $u$, and color $i \in D_v$ wants one or both nodes in $e$ if and only if $i$ wants $v$.
	Mirroring our previous notation, let $D_u'$ denote the set of colors in $D_u$ that want node $u$ (based on the random color thresholds) and define $D_v'$ analogously. The set $D_u' \cup D_v'$ is then the set of weak colors that want one or more node in $e$. Thus, conditioned on $|D_u' \cup D_v'| = t$ and conditioned on $u$ and $v$ wanting $c$, in order for $e$ to be satisfied $c$ must come before all $t$ colors in $|D_u' \cup D_v'|$ in the random permutation $\pi$. Formally, this means
	\[
		\prob\left[ \text{$e$ is satisfied} \mid X_u^c \cap X_v^c \cap (|D'_u \cup D'_v| = t) \right] = \frac{1}{1 + t}.
	\]

	Again using the law of total probability and the fact that $X_u^c \cap X_v^c$ is independent from $|D_u' \cup D_v'|$, we see
	\begin{align}
		\label{eq:dupdvp}
		\prob[ e & \text{ is satisfied} \mid X_u^c \cap X_v^c ]
		= \sum_{t = 0}^{|D|} \frac{1}{1+t} \prob\left[ |D'_u \cup D'_v| = t \right].
	\end{align}
	Observe now that $D_u$ and $D_v$ are disjoint color sets, which implies that $|D_u'|$ and $|D_v'|$ are independent random variables. This allows us to decouple $D_u'$ and $D_v'$ in the above expression since $\prob\left[ |D'_u \cup D'_v| = t \right] = \prob\left[ |D'_u| + |D'_v| = t \right]$. We define
	\[
		p_i = \prob\left[ |D'_u| = i \right] \text{ and } q_i = \prob\left[ |D'_v| = i \right],
	\]
	so that we can write
	\[
		\prob\left[ |D'_u| + |D'_v| = t \right] = \sum_{\substack{i, j \in [0, t] \\ i + j = t}} p_i q_j = \sum_{i = 0}^{|D_u|} p_i q_{t-i} .
	\]
	This allows us to re-write Eq.~\eqref{eq:dupdvp} as
	\begin{align}
		\label{eq:decoupled}
		\prob[ e \text{ is satisfied} \mid X_u^c \cap X_v^c ] & = \sum_{i = 0}^{|D_u|} \sum_{j = 0}^{|D_v|} \frac{1}{1+i+j} p_i q_j.
	\end{align}

	Without loss of generality we now assume $|D_u| \leq |D_v|$, and proceed case by case depending on the sizes of $D_u$ and $D_v$.
	%	As a shorthand we use $\rho = \prob[ e \text{ is satisfied} \mid X_u^c \cap X_v^c ]$. 
	Our goal is to show that for all cases $\prob[ e \text{ is satisfied} \mid X_u^c \cap X_v^c ] \geq 154/405$.
	In the cases below, we use the vector $\vx_u[D_u]$ to encode LP variables for node $u$ and colors in $D_u$ so that $p_i = P(\vx_u[D_u], i)$, and likewise $q_i = P(\vx_v[D_v], i)$, to match with notation in Definition~\ref{def:prob} and Lemma~\ref{lem:bounding}.

	\bigbreak

	\noindent
	\textbf{Case 3a.} $|D_u| = |D_v| = 0$. \\
	No weak colors other than $c$ want $u$ or $v$, so $\prob[ e \text{ is satisfied} \mid X_u^c \cap X_v^c ] = 1$.

	\bigbreak

	\noindent
	\textbf{Case 3b.} $|D_u| = 0$ and $|D_v| = 1$. \\
	Letting $a$ be the single color in $D_v$,
	\begin{align*}
		\prob[ e \text{ is satisfied} \mid X_u^c \cap X_v^c ] & = p_0 q_0 + \frac{1}{2} p_0 q_1 = q_0 + \frac{1}{2} q_1 = (1 - x_v^a) + \frac{1}{2} x_v^a.
	\end{align*}
	Because $a$ is a weak color for $v$, we know $x_v^a \in [0, \frac{2}{3}]$, and the minimum value the probability can obtain is $\frac{2}{3} > \frac{154}{405}$.

	\bigbreak

	\noindent
	\textbf{Case 3c.} $|D_u| = |D_v| = 1$. \\
	Letting $a$ be the color in $D_v$ and $b$ be the color in $D_u$,
	\begin{align*}
		\prob[ e \text{ is satisfied} \mid X_u^c \cap X_v^c ] & = p_0 q_0 + \frac{1}{2}(p_0 q_1 + p_1 q_0) + \frac{1}{3} p_1 q_1 \\ &= (1 - x_u^b)(1 - x_v^a) + \frac{1}{2}((1 - x_u^b)x_v^a + x_u^b(1 - x_v^a)) + \frac{1}{3} x_u^b x_v^a.
	\end{align*}
	Because $a$ is weak for $v$ and $b$ is weak for $u$, we know $x_v^a, x_u^b \in [0, \frac{2}{3}]$. This gives a minimum value for the probability of $\frac{13}{27} > \frac{154}{405}$.

	\bigbreak

	\noindent
	\textbf{Case 3d.} $|D_u| = 0$ and $|D_v| > 1$. \\
	In this case $p_0 = 1$, and Eq.~\eqref{eq:decoupled} simplifies to
	\begin{equation*}
		\prob[ e \text{ is satisfied} \mid X_u^c \cap X_v^c ] = \sum_{j = 0}^{|D_v|} \frac{1}{j+1} q_j \geq \frac{31}{54},
	\end{equation*}
	where we have applied Lemma \ref{lem:bounding} with $a_j = \frac{1}{j+1}$.

	\bigbreak

	\noindent
	\textbf{Case 3e.} $|D_u| = 1$ and $|D_v| > 1$. \\
	Eq.~\eqref{eq:decoupled} simplifies to
	\begin{align*}
		\prob[ e \text{ is satisfied} \mid X_u^c \cap X_v^c ] & = p_0 \sum_{j = 0}^{|D_v|} \frac{q_j}{j+1}  + p_1 \sum_{j = 0}^{|D_v|} \frac{q_j}{j+2} \geq p_0 \frac{31}{54} + p_1 \frac{19}{54},
	\end{align*}
	where we applied Lemma \ref{lem:bounding} twice, once with $a_j = \frac{1}{j+1}$ and once with $a_j = \frac{1}{j+2}$. The right hand side of the above bound has a minium value of $\frac{23}{54}$.

	\bigbreak

	\noindent
	\textbf{Case 3f.} $|D_u| > 1$ and $|D_v| > 1$.
	From Eq.~\eqref{eq:decoupled} we know
	\begin{align*}
		\prob[ e \text{ is satisfied} \mid X_u^c \cap X_v^c ] & = \sum_{i = 0}^{|D_u|}  p_i \left(\sum_{j = 0}^{|D_v|} \frac{1}{1+i+j}  q_j \right).
	\end{align*}
	We then apply Lemma~\ref{lem:bounding} $|D_u|+1$ times, once for each choice of $i \in \{0,1, \hdots, |D_u|\}$. The $i$th time we apply it, we use the sequence $a_j = \frac{1}{1+i+j}$ to get
	\begin{align}
		\label{eq:lemdvtimes}
		\sum_{j = 0}^{|D_v|} \frac{1}{1+i+j}  q_j \geq \frac{2}{9}\left( \frac{1}{i+1} + \frac{1}{i+3} \right) + \frac{5}{9} \left( \frac{1}{i+2} \right).
	\end{align}
	The right hand of~\eqref{eq:lemdvtimes} defines a new sequence
	\begin{equation}
		\label{eq:uglyai}
		a_i = \left( \frac{1}{i+1} + \frac{1}{i+3} \right) + \frac{5}{9} \left( \frac{1}{i+2} \right).
	\end{equation}
	We know from Lemma~\ref{lem:bounding_constraints} that this sequence $\{a_i\}$ in Eq.~\eqref{eq:uglyai} satisfies the conditions from Lemma~\ref{lem:bounding}, so applying Lemma~\ref{lem:bounding} one more time and combining all steps gives
	\begin{align*}
		\prob[ e \text{ is satisfied} \mid X_u^c \cap X_v^c ] & = \sum_{i = 0}^{|D_u|}  p_i \left(\sum_{j = 0}^{|D_v|} \frac{1}{1+i+j}  q_j \right) \\
		&\geq \sum_{i = 0}^{|D_u|}  p_i \left( \frac{2}{9}\left( \frac{1}{i+1} + \frac{1}{i+3} \right) + \frac{5}{9} \left( \frac{1}{i+2} \right) \right) \geq \frac{154}{405}.
	\end{align*}
	Therefore, we have shown in all possible subcases of Case 3 that $\prob\left[ \text{$e$ is satisfied} \mid X_u^c \cap X_v^c \right] \geq \frac{154}{405}$.

	Cases 1,2, and 3 all together show that for every $e \in E$ we have
	\[
		\prob\left[ \text{$e$ is satisfied} \right] \geq \frac{154}{405} z_e.
	\]
\end{proof}