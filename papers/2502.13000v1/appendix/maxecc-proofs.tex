\section{\maxecc{} Proofs}
\label{app:maxecc}
We use the following lemma as a small step in our approximation guarantee for hypergraph \maxecc{} (Theorem~\ref{thm:hypermaxecc}). The result was originally used in designing a $1/e^2$-approximation algorithm for graph \maxecc{}~\cite{angel2016clustering}. A full proof can be found in the work of~\cite{angel2016clustering}.
\begin{lemma}[Lemma 4 of~\citet{angel2016clustering}]
	\label{lem:angel}
	Let $\left\{X_1, X_2, \dots, X_j \right\}$ be a set of independent events satisfying $\sum_{i=1}^j \prob[X_i] \leq 1$, then the probability that {at most one} of them happens is greater than or equal to $2/e$.
\end{lemma}

The remainder of our supporting lemmas are new results that we prove for our approximation algorithm for graph \maxecc{}.

\lemdependentynx*
\begin{proof}
	For a node $u$ and color $i$ we use $\overline{X}_u^c$ to denote the event that $u$ does not want $c$.

	We first observe that it is sufficient to show positive correlation between $Y_u^c$ and $N_v$. To see this,
	we use Bayes' Theorem and the independence of $X_u^c$ and $N_v$ to write
	\begin{align*}
		\pgiv{Y_u^c}{N_v \cap X_u^c} & = \frac{\proba{Y_u^c \cap N_v \cap X_u^c}}{\proba{N_v \cap X_u^c}}                   \\
		                             & = \frac{\pgiv{N_v \cap X_u^c}{Y_u^c}\proba{Y_u^c}}{\proba{N_v \cap X_u^c}}           \\
		                             & = \frac{\pgiv{N_v \cap X_u^c}{Y_u^c}\proba{Y_u^c}}{\proba{N_v} \cdot \proba{X_u^c}},
	\end{align*}
	as well as
	\[
		\pgiv{Y_u^c}{X_u^c} = \frac{\proba{X_u^c \cap Y_u^c}}{\proba{X_u^c}} = \frac{\pgiv{X_u^c}{Y_u^c}\proba{Y_u^c}}{\proba{X_u^c}} = \frac{1\cdot \proba{Y_u^c}}{\proba{X_u^c}}.
	\]
	By rearranging terms, we see that our claim is equivalent to
	\[
		\pgiv{N_v \cap X_u^c}{Y_u^c} = \pgiv{N_v}{Y_u^c} \geq \proba{N_v}.
	\]
	We complete the proof by demonstrating the equivalent inequality $\pgiv{Y_u^c}{N_v} \geq \proba{Y_u^c}$. Impose an arbitrary order $c_1, c_2 \ldots, c_k$ on the color set, and without loss of generality assume that $\ell(e) = c = c_k$. In what follows, we always write $c$ for $c_k$ and whenever considering a subscripted color $c_i$ we assume that $i \in W_u \setminus \{c\}$.

	If node $u$ has a strong color, then without loss of generality we assume that that strong color is $c_{k-1}$.
	The remainder of the proof is written for the case where $S_u = \emptyset$, in which case the set of weak colors (other than $c$) that might want node $u$ is $W_u \setminus \{c\} = \{c_1, c_2, \hdots, c_{k-1}\}$. In other words, $W_u$ is the set of colors with indices in $[k-1]$. If instead we had $S_u = \{c_{k-1}\}$ and $W_u \setminus \{c\} = \{c_1, c_2, \hdots, c_{k-2}\}$, the proof works in exactly the same way if we instead considered the set of colors associated with indices in $[k-2]$ instead of indices in $[k-1]$.


	Now, consider all possible subsets of $[k - 1]$ which may define (according to our ordering) those weak colors (besides $c$) which are wanted by $u$. For a particular subset $S \subseteq [k - 1]$,
	we write $X_u^S$ for the event that $u$ wants exactly (not considering $c$) the colors in $S$. In the following, we write $w(S) = \frac{1}{|S| + 1}$ and call this quantity the \emph{weight} of $S$.
	The events $X_u^S$ partition the sample space,
	so we may write
	\begin{align*}
		\proba{Y_u^c} & = \sum_{S \subseteq [k - 1]} \pgiv{Y_u^c}{X_u^S}\cdot\proba{X_u^S} = \proba{X_u^c}\cdot\sum_{S \subseteq [k - 1]} w(S)\cdot\proba{X_u^S}.
	\end{align*}
	%
	In the above, the second equality follows from the observation that, conditioned on $X_u^S$ for any $S \subseteq [k - 1]$, the event $Y_u^c$ occurs if and only if (a) $u$ wants $c$ and (b) $c$ precedes each color in $S$ in the global ordering of colors.
	The conditions (a) and (b) are independent, and occur with probabilities $\proba{X_u^c}$ and $w(S)$, respectively.
	We will now make use of the independence
	of the $X_u^{c_i}$ events to write $\proba{X_u^S}$ as the product of $k - 1$ probabilities.
	\begin{align}\label{eq:yuc}
		\proba{Y_u^c} & = \proba{X_u^c}\cdot\sum_{S \subseteq [k - 1]} \left[ w(S)\cdot \left( \prod_{i \in S}\proba{X_u^{c_i}}\right)\cdot\left(\prod_{j \notin S}\proba{\overline{X}_u^{c_j}}\right) \right].
	\end{align}

	We want to write a similar expression for $\proba{Y_u^c | N_v}$. We begin by using similar reasoning as above to write
	\begin{align*}
		\pgiv{Y_u^c}{N_v} & = \sum_{S \subseteq [k - 1]} \pgiv{Y_u^c}{X_u^S \cap N_v}\cdot\pgiv{X_u^S}{N_v} \\
		                  & = \pgiv{X_u^c}{N_v}\cdot \sum_{S \subseteq [k - 1]} w(S)\cdot\pgiv{X_u^S}{N_v}  \\
		                  & = \proba{X_u^c}\cdot \sum_{S \subseteq [k - 1]} w(S)\cdot\pgiv{X_u^S}{N_v}.
	\end{align*}
	%
	Next, we observe that the $X_u^{c_i}$ events remain independent even when conditioned on $N_v$. This allows us to write

	\begin{align*}
		\pgiv{Y_u^c}{N_v} = \proba{X_u^c}\cdot\sum_{S \subseteq [k - 1]} \left[ w(S)\cdot \left( \prod_{i \in S}\pgiv{X_u^{c_i}}{N_v}\right)\cdot\left(\prod_{j \notin S}\pgiv{\overline{X}_u^{c_j}}{N_v}\right) \right].
	\end{align*}
	To simplify this expression further, we claim that for every $i \in [k - 1]$,
	\[\pgiv{X_u^{c_i}}{N_v} = \pgiv{X_u^{c_i}}{\overline{X}_v^{c_i}}. \]
	This can be derived from the observation
	that $\bigcap_{j \neq i} \overline{X}_v^{c_j}$ is independent of $X_u^{c_i}$ when conditioned on $\overline{X}_v^{c_i}$, as well as being unconditionally independent of $X_v^{c_i}$.
	We then manipulate the definition of conditional probability to see that
	\begin{align*}
		\pgiv{X_u^{c_i}}{N_v} & = \frac{\proba{X_u^{c_i} \cap \bigcap_{j \neq i} \overline{X}_v^{c_j} \cap \overline{X}_v^{c_i}}}{\proba{\bigcap_{j \neq i} \overline{X}_v^{c_j} \cap \overline{X}_v^{c_i}}}        \\
		                      & = \frac{\proba{X_u^{c_i} \cap \bigcap_{j \neq i} \overline{X}_v^{c_j} \cap \overline{X}_v^{c_i}}}{\proba{\bigcap_{j \neq i} \overline{X}_v^{c_j}}\cdot\proba{\overline{X}_v^{c_i}}} \\
		                      & = \frac{\pgiv{X_u^{c_i} \cap \bigcap_{j \neq i} \overline{X}_v^{c_j}}{\overline{X}_v^{c_i}}}{\proba{\bigcap_{j \neq i} \overline{X}_v^{c_j}}}                                       \\
		                      & = \frac{\pgiv{X_u^{c_i}}{\overline{X}_v^{c_i}}\cdot\pgiv{\bigcap_{j \neq i} \overline{X}_v^{c_j}}{\overline{X}_v^{c_i}}}{\proba{\bigcap_{j \neq i} \overline{X}_v^{c_j}}}           \\
		                      & = \frac{\pgiv{X_u^{c_i}}{\overline{X}_v^{c_i}}\cdot\proba{\bigcap_{j \neq i} \overline{X}_v^{c_j}}}{\proba{\bigcap_{j \neq i} \overline{X}_v^{c_j}}}                                \\
		                      & = \pgiv{X_u^{c_i}}{\overline{X}_v^{c_i}}.
	\end{align*}

	We can now write a suitable counterpart to Equation~\eqref{eq:yuc}:
	\begin{align}\label{eq:yuc-conditional-nv}
		\pgiv{Y_u^c}{N_v} = \proba{X_u^c}\cdot\sum_{S \subseteq [k - 1]} \left[ w(S)\cdot \left( \prod_{i \in S}\pgiv{X_u^{c_i}}{\overline{X}_v^{c_i}}\right)\cdot\left(\prod_{j \notin S}\pgiv{\overline{X}_u^{c_j}}{\overline{X}_v^{c_j}}\right) \right].
	\end{align}

	We will complete the proof by showing that the RHS of Equation~\eqref{eq:yuc} is a lower bound for the RHS of Equation~\eqref{eq:yuc-conditional-nv}. We accomplish this in $k - 1$ steps, one for each color besides $c$.
	In the first step, we begin by rearranging the terms of Equation~\eqref{eq:yuc-conditional-nv} to isolate $\pgiv{X_u^{c_1}}{\overline{X}_v^{c_1}}$ and $\pgiv{\overline{X}_u^{c_1}}{\overline{X}_v^{c_1}}$:

	\begin{align*}
		\begin{split}
			\pgiv{Y_u^c}{N_v} = \proba{X_u^c} & \cdot \biggl[ \pgiv{X_u^{c_1}}{\overline{X}_v^{c_1}}\cdot\sum_{S \ni 1} w(S)\cdot \left( \prod_{1 \neq i \in S}\pgiv{X_u^{c_i}}{\overline{X}_v^{c_i}}\right)\cdot\left(\prod_{j \notin S}\pgiv{\overline{X}_u^{c_j}}{\overline{X}_v^{c_j}}\right)                \\
			                                  & + \pgiv{\overline{X}_u^{c_1}}{\overline{X}_v^{c_1}}\cdot \sum_{S \not\ni 1} w(S) \cdot  \left( \prod_{i \in S}\pgiv{X_u^{c_i}}{\overline{X}_v^{c_i}}\right)\cdot\left(\prod_{1 \neq j \notin S}\pgiv{\overline{X}_u^{c_j}}{\overline{X}_v^{c_j}}\right) \biggr].
		\end{split}
	\end{align*}

	In the above, we refer to the first and second sums as the $c_1$-\emph{wanted coefficient} and the $c_1$-\emph{not-wanted coefficient}, respectively.
	Observe that there exists a natural bijection between the summands in these two coefficients. Specifically, we map the summand in the $c_1$-wanted coefficient
	corresponding to set $S$ to the summand in the $c_1$-not-wanted coefficient corresponding to the set $S \setminus \{1\}$. These two summands are identical up to the
	difference between $w(S)$ and $w(S \setminus \{1\})$. By definition, the former weight is smaller. Hence, the $c_1$-wanted coefficient is smaller than
	the $c_1$-not-wanted coefficient.

	Next, we claim that $\pgiv{X_u^{c_1}}{\overline{X}_v^{c_1}} \leq \proba{X_u^{c_1}}$, and (equivalently) $\pgiv{\overline{X}_u^{c_1}}{\overline{X}_v^{c_1}} \geq \proba{\overline{X}_u^{c_1}}$.
	To prove this claim, we consider two cases. If $x_u^{c_1} \leq x_v^{c_1}$, then $\pgiv{X_u^{c_1}}{\overline{X}_v^{c_1}} = 0 \leq x_u^{c_1} = \proba{X_u^{c_1}}$. Otherwise, $x_u^{c_1} > x_v^{c_1}$, and we
	have $\pgiv{X_u^{c_1}}{\overline{X}_v^{c_1}} = \frac{x_u^{c_1} - x_v^{c_1}}{1 - x_v^{c_1}} = x_u^{c_1}\frac{1 - x_v^{c_1}/x_u^{c_1}}{1 - x_v^{c_1}} \leq x_u^{c_1} = \proba{X_u^{c_1}}$.

	We now rewrite our expression for $\pgiv{Y_u^c}{N_v}$ in terms of \emph{unconditional} probabilities of $u$ (not) wanting color $c_1$. In the following, let
	$0 \leq d = \proba{X_u^{c_1}} - \pgiv{X_u^{c_1}}{\overline{X}_v^{c_1}}$.

	\begin{align*}
		\begin{split}
			\pgiv{Y_u^c}{N_v} = \proba{X_u^c} & \cdot \biggl[ (\proba{X_u^{c_1}} - d)\cdot\sum_{S \ni 1} w(S)\cdot \left( \prod_{1 \neq i \in S}\pgiv{X_u^{c_i}}{\overline{X}_v^{c_i}}\right)\cdot\left(\prod_{j \notin S}\pgiv{\overline{X}_u^{c_j}}{\overline{X}_v^{c_j}}\right)                \\
			                                  & + (\proba{\overline{X}_u^{c_1}} + d)\cdot \sum_{S \not\ni 1} w(S) \cdot  \left( \prod_{i \in S}\pgiv{X_u^{c_i}}{\overline{X}_v^{c_i}}\right)\cdot\left(\prod_{1 \neq j \notin S}\pgiv{\overline{X}_u^{c_j}}{\overline{X}_v^{c_j}}\right) \biggr].
		\end{split}
	\end{align*}

	Our observation that the $c_1$-wanted coefficient is smaller than the $c_1$-not-wanted coefficient yields the bound we desire:
	\begin{align*}
		\begin{split}
			\pgiv{Y_u^c}{N_v} \geq \proba{X_u^c} & \cdot \biggl[ \proba{X_u^{c_1}}\cdot\sum_{S \ni 1} w(S)\cdot \left( \prod_{1 \neq i \in S}\pgiv{X_u^{c_i}}{\overline{X}_v^{c_i}}\right)\cdot\left(\prod_{j \notin S}\pgiv{\overline{X}_u^{c_j}}{\overline{X}_v^{c_j}}\right)                \\
			                                     & + \proba{\overline{X}_u^{c_1}}\cdot \sum_{S \not\ni 1} w(S) \cdot  \left( \prod_{i \in S}\pgiv{X_u^{c_i}}{\overline{X}_v^{c_i}}\right)\cdot\left(\prod_{1 \neq j \notin S}\pgiv{\overline{X}_u^{c_j}}{\overline{X}_v^{c_j}}\right) \biggr].
		\end{split}
	\end{align*}

	This completes the first of $k - 1$ steps. In the next step, we begin by rearranging terms in the inequality above to isolate $\pgiv{X_u^{c_2}}{\overline{X}_v^{c_2}}$ and $\pgiv{\overline{X}_u^{c_2}}{\overline{X}_v^{c_2}}$. We then apply the same analysis, yielding another lower bound on $\pgiv{Y_u^c}{N_v}$, this time written
	in terms of unconditional probabilities of $u$ (not) wanting colors $c_1$ and $c_2$, and conditional probabilities of $u$ (not) wanting colors $c_3, c_4, \ldots c_{k-1}$. After $k - 1$ steps, our lower bound becomes identical (up to rearranging terms) to the RHS of Equation~\ref{eq:yuc}, as desired.
\end{proof}

\lemsumtoprod*
\begin{proof}
	Observe that the claim is trivially true when $m = 1$. We proceed via induction on $m$. For $m > 1$, we have $\sum_{t = 1}^{m - 1} \leq \beta - x_m$, so by the inductive hypothesis
	\[ \prod_{t = 1}^m (1 - x_t) = (1 - x_m)\cdot \prod_{t=1}^{m-1}(1 - x_t) \geq (1 - x_m)(1 - \beta + x_m) = 1 - \beta + x_m(\beta - x_m) \geq 1 - \beta.\]
\end{proof}

\lembounding*
\begin{proof}
	Let $\mathcal{S} = \{\vx \in [0,2/3]^m \colon \sum_{i = 1}^m x_i = 1\} \subsetneq \mathcal{D}$ be the set of input vectors for $f$ whose entries sum exactly to 1. Let $\mathcal{I}\subsetneq \mathcal{S}$ be the set of vectors in $\mathcal{S}$ with one entry equal to $2/3$, one entry equal to $1/3$, and all other entries equal to 0. More formally, if $\vx \in \mathcal{I}$, this means there exists two distinct indices $\{i,j\}$ such that $x_i = 2/3$, $x_j = 1/3$, and $x_k = 0$ for every $k \notin \{i,j\}$. Our goal is to prove there exists some $\vx \in \mathcal{I}$ that minimizes $f$. We prove this in two steps: (1) we show that there exists a minimizer of $f$ in $\mathcal{S}$, and then (2) we show how to convert an arbitrary vector $\vx \in \mathcal{S}$ into a new vector $\vx^* \in \mathcal{I}$ satisfying $f(\vx^*) \leq f(\vx)$. Given $\vx^* \in \mathcal{I}$, the value $f(\vx^*)$ in Eq.~\eqref{eq:minval} amounts to a simple function evaluation, realizing that $P(\vx^*,t) = 0$ for $t \geq 3$.

	\paragraph{Step 1: Proving minimizers exist in $\mathcal{S}$.}

	Let $\vx \in \mathcal{D} \setminus \mathcal{S}$, and let $y \in  [m]$ be an entry such that $x_y < 2/3$. Set 
	\begin{equation*}
	\delta = \min \left\{ 1 - \sum_{i = 1}^m x_i, \frac{2}{3}- x_y\right\} > 0
	\end{equation*} and define a new vector $\vx' \in \mathcal{D}$ by
	\begin{equation}
		x_i' = \vx'(i) = \begin{cases}
			x_i          & \text{ if $i \neq y$} \\
			x_i + \delta & \text{otherwise}.
		\end{cases}
	\end{equation}
	By our choice of $\delta$ we know that $\vx' \in \mathcal{D}$, and we will prove this satisfies $f(\vx') \leq f(\vx)$. To do so, we re-write $f(\vx)$ in a way that isolates terms involving $x_y.$ Let $\noty{} = [m] \setminus \{y\}$. Recall that $f$ is a linear combination of terms $P(\vx,t)$, where $P(\vx,t)$ represents the probability that exactly $t$ events out of a set of $m$ independent events occur. Entry $x_i$ of $\vx$ is the probability that $i$th event occurs. We can therefore re-write:
	\begin{align}
		\label{eq:pxtwithy}
		P(\vx,t) & = P(\vx[\noty{}],t) (1-x_y) + P(\vx[\noty{}], t-1) x_y.
	\end{align}
	To explain this in more detail, we use a slight abuse of terminology and refer to $[m]$ as a set of events. The first term in Eq.~\eqref{eq:pxtwithy} is the probability that exactly $t$ events in $\noty{}$ occur and that event $y$ does not occur. The second term is the probability that $y$ does occur and exactly $t -1$ events in $\noty{}$ occur. We therefore re-write $f(\vx)$ as
	\begin{align}
		f(\vx) & = \sum_{t=0}^m a_t P(\vx, t)                                                                       & (\text{definition of $f$}) \notag            \\
		       & = \sum_{t=0}^m a_t \left(P(\vx[\noty{}],t) (1-x_y) + P(\vx[\noty{}], t-1) x_y \right)              & (\text{from Eq.~\eqref{eq:pxtwithy}}) \notag \\
		       & = \sum_{t=0}^m a_t P(\vx[\noty{}],t) (1-x_y) + \sum_{t=0}^m a_t P(\vx[\noty{}], t-1) x_y           & (\text{separating terms}) \notag             \\
		       & = \sum_{t=0}^m a_t P(\vx[\noty{}],t) (1-x_y) + \sum_{t=-1}^{m-1} a_{t+1} P(\vx[\noty{}], t) x_y    & (\text{change of variables}) \notag          \\
		       & = \sum_{t=0}^{m-1} a_t P(\vx[\noty{}],t) (1-x_y) + \sum_{t=0}^{m-1} a_{t+1} P(\vx[\noty{}], t) x_y & \label{eq:fvx_p_0}                           \\
		       & = \sum_{t=0}^{m-1} P(\vx[\noty{}],t) \left( a_t(1-x_y) + a_{t+1}x_y \right)                        & \label{eq:last}
	\end{align}
	In Eq.~\eqref{eq:fvx_p_0} we have used the fact that $P(\vx[\noty{}],\ell) = 0$ if $\ell < 0$ or $\ell > m-1$.
	Observe that if we replace the value of $x_y$ with $x_y + \delta$ in Eq.~\eqref{eq:last} this gives the value of $f(\vx')$. We can then see that
	\begin{align*}
		f(\vx') & = \sum_{t=0}^{m-1} P(\vx[\noty{}],t) \left( a_t(1-x_y - \delta) + a_{t+1}(x_y + \delta) \right)       \\
		        & = \sum_{t=0}^{m-1} P(\vx[\noty{}],t) \left( a_t(1-x_y) + a_{t+1}x_y + \delta (a_{t+1} - a_t)  \right) \\
		        & = f(\vx) + \delta \sum_{t=0}^{m-1} (a_{t+1} - a_t).
	\end{align*}
	Using the constraint $a_{t+1} \leq a_t$ we notice that the last term is always less that or equal to zero. Hence, we have $f(\vx') \leq f(\vx)$. Recall that $\delta = \min\{ 1 - \sum_{i =1}^m x_i, 2/3 - x_y\}$. If $\delta = 1 - \sum_{i=1}^m x_i$, then we can see that $\vx' \in \mathcal{S}$ and we are done. Otherwise, $\delta = 2/3- x_y$, and $x_y' = 2/3$. We can then apply the same exact procedure again, by increasing the value of some index $x_z$ (where $z \neq y$), which we know satisfies $x_z \leq 1/3$. The second application of this procedure is guaranteed to produce a vector in $\mathcal{S}$.


	\paragraph{Step 2: Editing a vector from $\mathcal{S}$ to $\mathcal{I}$.}
	The proof structure for Step 2 is very similar as Step 1, but is more involved as it requires working with two entries of $\vx$ rather than one. Let $\vx \in \mathcal{S} \setminus \mathcal{I}$, which means we can identify two entries $x_y$ and $x_z$ in the vector (with $y \neq z$) satisfying the inequality
	\begin{equation}
		\label{eq:yzinequality}
		0 < x_y \leq x_z < 2/3.
	\end{equation}
	Given these entries, set $\delta = \min\{x_y, 2/3 - x_z \}$ and define a new vector $\vx'$ by
	\begin{equation}
		x_i' = \vx'(i) = \begin{cases}
			x_i          & \text{ if $i \notin \{y,z\}$} \\
			x_i - \delta & \text{ if $i = y$}            \\
			x_i + \delta & \text{ if $i = z$.}
		\end{cases}
	\end{equation}
	Observe that $\vx' \in \mathcal{S}$ and that either $x_y' = 0$ or $x_z' = 2/3$. We will show that $f(\vx') \leq f(\vx)$, by re-writing $f(\vx)$ in a way that isolates terms involving $x_y$ and $x_z$. Let $\notyz{} = [m] \backslash \{y,z\}$. Similar to the proof of Step 1, we can re-write $P(\vx,t)$ as
	% $f$ by definition is a linear combination of terms $P(\vx,t)$, and that 
	% $P(\vx,t)$ represents the probability that exactly $t$ events out of a set of $m$ independent events occur, we can write
	% . Entry $x_i$ of $\vx$ is the probability that the $i$th event occurs. We can therefore re-write:
	\begin{align}
		P(\vx,t) & = P(\vx[\notyz{}],t) (1-x_y)(1-x_z) \label{eq:pxt1}                               \\
		         & \;\;+ P(\vx[\notyz{}], t-1) \left[x_y (1-x_z) + (1-x_y)x_z\right] \label{eq:pxt2} \\
		         & \;\; + P(\vx[\notyz{}], t-2) x_y x_z. \label{eq:pxt3}
	\end{align}
	Line~\eqref{eq:pxt1} captures the probability that exactly $t$ events from the set $\notyz{}$ happen, and that neither of the events $y$ or $z$ happen. Line~\eqref{eq:pxt2} is the probability that $t-1$ events from $\notyz{}$ happen, and exactly one of the events $\{y,z\}$ happens. Finally, line~\eqref{eq:pxt3} is the probability that both $y$ and $z$ happen, and $t-2$ of the events in $\notyz{}$ happen. For simplicity we now define
	\begin{align*}
		\alpha & = (1-x_y)(1-x_z)           \\
		\beta  & = x_y (1-x_z) + (1-x_y)x_z \\
		\gamma & = x_y x_z,
	\end{align*}
	so that we can re-write $f(\vx)$ as follows:
	\begin{align}
		f(\vx) & = \sum_{t=0}^m a_t P(\vx, t)                                                                                                                                & (\text{def. of $f$}) \notag      \\
		       & = \sum_{t=0}^m a_t \left(P(\vx[\notyz{}],t)\alpha + P(\vx[\notyz{}], t-1) \beta + P(\vx[\notyz{}], t-2) \gamma  \right)                                     & (\text{exp. $P(\vx,t)$}) \notag  \\
		       & = \sum_{t=0}^m a_t P(\vx[\notyz{}],t)\alpha + \sum_{t=0}^m a_t P(\vx[\notyz{}], t-1) \beta +  \sum_{t=0}^m a_t P(\vx[\notyz{}], t-2) \gamma                 & (\text{sep. sums}) \notag        \\
		       & = \sum_{i=0}^m a_i P(\vx[\notyz{}],i)\alpha + \sum_{j=-1}^{m-1} a_{j+1} P(\vx[\notyz{}], j) \beta +  \sum_{k=-2}^{m-2} a_{k+2} P(\vx[\notyz{}], k) \gamma   & (\text{redef. variables}) \notag \\
		       & = \sum_{i=0}^m a_i P(\vx[\notyz{}],i)\alpha + \sum_{j=0}^{m-1} a_{j+1} P(\vx[\notyz{}], j) \beta +  \sum_{k=0}^{m-2} a_{k+2} P(\vx[\notyz{}], k) \gamma     & \label{eq:littlel}               \\
		       & = \sum_{i=0}^{m-2} a_i P(\vx[\notyz{}],i)\alpha + \sum_{j=0}^{m-2} a_{j+1} P(\vx[\notyz{}], j) \beta +  \sum_{k=0}^{m-2} a_{k+2} P(\vx[\notyz{}], k) \gamma & \label{eq:bigl}                  \\
		       & = \sum_{t=0}^{m-2} P(\vx[\notyz{}],t) \left( a_t \alpha +  a_{t+1}  \beta +   a_{t+2}  \gamma \right).                                                       & \label{eq:rewritef}
	\end{align}
	In Eq.~\eqref{eq:littlel} and Eq.~\eqref{eq:bigl} we have used the fact that $P(\vx[\notyz{}],\ell) = 0$ if $\ell < 0$ or $\ell > m-2$. Consider now the vector $\vx'$ obtained by perturbing entries $x_y$ and $x_z$, and define
	\begin{equation*}
		\begin{array}{llll}
			\alpha' & = (1-x_y')(1-x_z')                                               \\
			        & = (1-(x_y-\delta))(1-(x_z +\delta))                              \\
			        & = \alpha + \delta(x_y-x_z) - \delta^2                            \\                                                                                                           \\
			\beta'  & = x_y' (1-x_z') + (1-x_y')x_z'                                   \\
			        & = (x_y-\delta) (1-(x_z +\delta)) + (1-(x_y-\delta))(x_z +\delta) \\
			        & = \beta + 2\delta (x_z - x_y) +2  \delta^2                       \\                                                                     \\
			\gamma' & = x_y' x_z'                                                      \\
			        & = (x_y-\delta) (x_z +\delta)                                     \\
			        & = \gamma + \delta (x_y - x_z) - \delta^2.
		\end{array}
	\end{equation*}
	In the expression for $f(\vx)$ given in Eq.~\eqref{eq:rewritef}, the entries $x_y$ and $x_z$ appear only in the terms $\alpha$, $\beta$, and $\gamma$. Therefore, if we replace $\{\alpha, \beta, \gamma\}$ with $\{\alpha', \beta', \gamma'\}$, we obtain a similar expression for $f(\vx')$. We can then see that

	\begin{align*}
		f(\vx') - f(\vx) & = \sum_{t=0}^{m-2} P(\vx[\notyz{}],t) \left( a_t (\alpha'-\alpha) +  a_{t+1}  (\beta'-\beta) +   a_{t+2}  (\gamma'-\gamma) \right).
	\end{align*}

	Isolating the expression inside the summation we can see

	\begin{align*}
		   & a_t (\alpha'-\alpha) +  a_{t+1}  (\beta'-\beta) +   a_{t+2}  (\gamma'-\gamma)                                             \\
		=~ & a_t (\delta(x_y-x_z) - \delta^2) + a_{t+1}  (2\delta (x_z - x_y) +2  \delta^2) + a_{t+2}  (\delta (x_y - x_z) - \delta^2) \\
		=~ & a_t (\delta(x_y-x_z) - \delta^2) -  2a_{t+1}  (\delta (x_y - x_z) -  \delta^2) + a_{t+2}  (\delta (x_y - x_z) - \delta^2) \\
		=~ & (a_t - 2a_{t+1} + a_{t+2}) (\delta(x_y - x_z) - \delta^2).
	\end{align*}

	Plugging this back into the equation for $f(\vx') - f(\vx)$ and using the fact that $a_t - 2a_{t+1} + a_{t+2} \geq 0$ and $(x_y - x_z) - \delta < 0$, we have

	\[
		f(\vx') - f(\vx) = \sum_{t=0}^{m-2} P(\vx[\notyz{}],t) \left( (a_t - 2a_{t+1} + a_{t+2}) (\delta(x_y - x_z) - \delta^2)\right)\leq 0.
	\]
	% At this point we have moved entries $x_y$ and $x_z$ closer to the interval bounds $\{0,2/3\}$ while decreasing (or holding constant) the output of function $f$. We now wish choose the largest value of $\delta > 0$ (i.e., the maximum perturbation for $x_y$ and $x_z$) that still guarantees $\vx' \in [0,2/3]$. This is equivalent to finding the maximum value of $\delta$ such that
	% \begin{align*}
	% 0  \leq x_y - \delta &\implies \delta \leq x_y \\
	% x_z + \delta \leq 2/3 &\implies \delta \leq 2/3 - x_z,
	% \end{align*}
	% so we choose $\delta = \min\{x_y, 2/3 - x_z \}$.

	If we start with an arbitrary vector $\vx \in \mathcal{S}$, applying one iteration of the above procedure will ensure that \emph{at least} one entry of $\vx$ will move to one of the endpoints $\{0,2/3\}$ of the interval $[0,2/3]$. Therefore, applying this procedure $m$ (or fewer) times to an arbitrary vector $\vx \in \mathcal{S}$ will produce a vector $\vx^* \in \mathcal{I}$ that satisfies $f(\vx^*) \leq f(\vx)$.
\end{proof}

\lemboundingconstraints*
\begin{proof}
	% Observe the inequalities hold for $a_t = \frac{1}{1+g+t}$. And for the second sequence the first inequality holds. 
	The first inequality (monotonicity) is easy to check for both sequences. The most involved step is checking that the second inequality holds for the sequence in~\eqref{eq:complicatedat}. To show it holds, we note that the inequality $a_t +  a_{t+2} \geq 2a_{t+1}$ effectively corresponds to a discrete version of convexity. We first extend the definition of the sequence in~\eqref{eq:complicatedat} to all positive reals by defining the function
	\begin{equation*}
		h(x) = \frac{2}{9}\left( \frac{1}{x+1} + \frac{1}{x+3} \right) + \frac{5}{9} \left( \frac{1}{x+2} \right),
	\end{equation*}
	which we can easily prove is convex over the interval $[0,\infty)$ by noting that its second derivative is
	\[
		h''(x) = \frac{1}{9}\left( \frac{10}{(x+2)^3} + \frac{4}{(x+3)^3} + \frac{4}{(x+1)^3} \right),
	\]
	which is greater than $0$ for all $x \geq 0$. From the definition of convexity for any $\beta \in [0, 1]$ and any $x, y \geq 0$ we have
	\[
		h(x\beta + (1-\beta) y) \leq \beta h(x) + (1-\beta) h(y).
	\]
	Now if we consider the specific values $x=t$, $y=t+2$, and $\beta=1/2$ we get
	\begin{align*}
		h\left(t\frac{1}{2} + \frac{1}{2} (t+2)\right) & \leq \frac{1}{2} h(t) + \frac{1}{2} h(t+2) \\
		\implies  h(t+1)                               & \leq \frac{1}{2} h(t) + \frac{1}{2} h(t+2) \\
		\implies  2 h(t+1)                             & \leq h(t) + h(t+2)                         \\
		\implies 2a_{t+1}                              & \leq a_t + a_{t+2}.
	\end{align*}
	% where $\beta = 1/2$, $x = t$, and $z = t+2$ to recover the desired inequalities. 
	One can use a similar approach to show (even more easily) that the sequence $a_t = 1/(1+g+t)$ also satisfies the second inequality.
\end{proof}
