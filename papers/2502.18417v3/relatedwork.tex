\section{Related Work}
\paragraph{Face swap}
There are a large number of face-swapping methods. Conceptually, they can be divided into several different groups. Methods from the first group \cite{simswap2020, hififace2021, ghost2022} extract the identity vector and some other features of the source face and use a generative model to blend them with the attributes of the target. Often, such models rely on the ArcFace \cite{deng2019arcface} model and a 3D shape-aware identity extractor, which allows encoding the 3D geometry of the face.
There are also approaches \cite{zhu2021megafs} based on StyleGAN2 \cite{Karras2019stylegan2}. They propose inverting the source and driving images into the latent space and feeding them into the StyleGAN2 generator to perform the swap. This approach allows for higher-resolution results but is sensitive to input data and does not perform well on strong rotations or small details of images.
With the development of diffusion models, approaches to face replacement using this method have emerged \cite{zhao2023diffswap, chen2024hifivfshighfidelityvideo}. Diffusion models allow for high-quality results, but they are typically slow and require significant computational power and sufficient VRAM.

\paragraph{Head swap}
The task of head swap is covered by a limited number of works. DeepFaceLab \cite{perov2020deepfacelab} is the first approach enabling this capability. However, it requires a large amount of source data for training, and poorly performs color transfer and fusion of generated head with the background. StylePoseGAN \cite{sarkar2021style} performs head swap by conditioning StyleGAN \cite{Karras2019stylegan2} on pose and combined texture map, with body parts taken from target and head â€” from source. 
Still, it tends to modify the background and skin color of the target. HeSer \cite{shu2022few} tackles these issues by designing a separate module for each task. First, it uses head reenactment model based on \cite{Burkov_2020_CVPR} to align source head with target in pose and expression. In the second stage a reference is created for skin color transfer based on correlation between pixels from the same head parts. Together with background inpainting prior, it is used to condition blending UNet \cite{ronneberger2015u} which fuses the head with background. While this method outperforms the competitors, it suffers from identity leakage of target and is unable to color head parts present in source image but absent in the driving one.  
While previous methods are based on GANs \cite{goodfellow2014generative}, there have been attempts \cite{wang2022hs, han2023generalist} to use diffusion models \cite{ho2020denoising} instead. However, currently these approaches face issues of pose controllability, preservation of target skin color and overall realism of generated head.


\vspace{-8pt}
\paragraph{Head reenactment} 
Head reenactment methods can be generally categorized as either warping-based \cite{Siarohin_2019_NeurIPS, zakharov2020fast, Drobyshev22MP, zhang2022metaportrait, wang2022latent} or reconstruction-based \cite{zielonka2022insta, NEURIPS2023_937ae0e8, qian2024gaussianavatars, chu2024gpavatar, deng2024portrait4d}. Warping-based approaches utilize motion and facial expression descriptors of the target to deform source image. These descriptors can be based on keypoints \cite{Siarohin_2019_NeurIPS, zakharov2020fast}, blendshapes from parametric models \cite{ren2021pirenderer, yin2022styleheat} or latent representations. The latter usually achieves better expressiveness, however, it requires careful disentanglemet of motion from the appearance of the target. This can be achieved via the use of special losses \cite{Drobyshev22MP, drobyshev2024emoportraits, Pang_2023_CVPR} or additional regularization embedded into the architecture \cite{Pang_2023_CVPR}. However, warping-based approaches generally perform well only if the difference between source and target poses is small.
Reconstruction-based methods \cite{zielonka2022insta, NEURIPS2023_937ae0e8, qian2024gaussianavatars, chu2024gpavatar, deng2024portrait4d} construct latent model of source head, and therefore are robust to larger pose deviations. These methods often utilize implicit representations such as TriPlanes \cite{ma2023otavatar, ye2024real3dportrait} and NeRF \cite{zielonka2022insta, zheng2022imavatar, bai2023learning}, or explicit  ones, such as voxels \cite{xu2023avatarmav}, point clouds \cite{zheng2023pointavatar} and meshes \cite{Khakhulin2022ROME, grassal2022neural}, with particularly photorealistic results achieved recently with Gaussian splatting \cite{qian2024gaussianavatars, giebenhain2024npga}. However, reconstruction with these approaches requires an additional step of per-frame estimation of camera parameters, which increases runtime. Also, due to high computational cost of rendering, the resolution of output images does not exceed $256 \times 256$ and upsampling to higher resolutions is performed by an additional network.