\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[arxiv]{icml2025}
% WE NEED TO UNCOMMENT ONE LINE FOR CAMERA-READY (JUST SEARCH UNCOMMENT IN .sty)
% \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{siunitx}
\sisetup{output-exponent-marker=\text{e}}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{tcolorbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\newcommand{\cfour}{C4}
\newcommand{\thepile}{The Pile}
\newcommand{\thepileuc}{The Pile UC}
\newcommand{\thepiledd}{The Pile Deduped}
\newcommand{\fineweb}{FineWeb-Edu}
\newcommand{\refinedweb}{RefinedWeb}
\newcommand{\slimpajama}{SlimPajama}

\newcommand{\hellaswag}{HellaSwag}
\newcommand{\piqa}{PIQA}
\newcommand{\arceasy}{ARC-Easy}
\newcommand{\arcchallenge}{ARC-Challenge}
\newcommand{\openbookqa}{OpenBookQA}
\newcommand{\commonsenseqa}{CommonsenseQA}
\newcommand{\winogrande}{WinoGrande}
\newcommand{\mmlu}{MMLU}
\newcommand{\socialiqa}{SocialIQA}
\newcommand{\copa}{COPA}

\newcommand{\mamba}{Mamba}
\newcommand{\llama}{Llama}
\newcommand{\gpt}{GPT}

\newcommand{\huggingface}{Hugging~Face}

\crefname{figure}{Fig.}{Figs.}
\crefname{appendix}{App.}{Apps.}
\crefname{equation}{Eq.}{Eqs.}
% \crefname{section}{Sec.}{Secs.}
% Use section sign for sections
\crefname{section}{}{}
\creflabelformat{section}{#2§#1#3}

\definecolor{sbblue}{HTML}{5273AD}
\definecolor{sborange}{HTML}{D78357}
\definecolor{sbgreen}{HTML}{5fA86C}
\definecolor{sbred}{HTML}{BD4C53}

\newcommand{\todotw}[1]{\textcolor{purple}{#1}}

\newcounter{takeaway}
\setcounter{takeaway}{0}

\newcommand{\takeaway}[1]{%
    \stepcounter{takeaway}%
    \begin{center}
    \begin{tcolorbox}[colframe=sbblue, colback=sbblue!30, coltext=black, width=\columnwidth, left=.7mm, right=.7mm, top=1mm, bottom=1mm, boxrule=.5mm]
        \color{sbblue!50!black}\textbf{Takeaway \thetakeaway}\hspace{.5em}#1
    \end{tcolorbox}
    \end{center}
}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here
\icmltitlerunning{LLMs on the Line}

\begin{document}

\twocolumn[
\icmltitle{LLMs on the Line: Data Determines Loss-to-Loss Scaling Laws}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Prasanna Mayilvahanan}{equal,mpi,ellis,tueai,uni}
\icmlauthor{Thaddäus Wiedemer}{equal,mpi,ellis,tueai,uni}
\icmlauthor{Sayak Mallick}{mpi,uni}
\icmlauthor{Matthias Bethge}{ellis,tueai,uni}
\icmlauthor{Wieland Brendel}{mpi,ellis,tueai}
\end{icmlauthorlist}

\icmlaffiliation{mpi}{Max Planck Institute for Intelligent Systems}
\icmlaffiliation{ellis}{ELLIS Institute Tübingen}
\icmlaffiliation{tueai}{Tübingen AI Center}
\icmlaffiliation{uni}{University of Tübingen}

\icmlcorrespondingauthor{Prasanna Mayilvahanan}{prasanna.mayilvahanan@gmail.com}
\icmlcorrespondingauthor{Thaddäus Wiedemer}{thaddaeus.wiedemer@gmail.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{LLMs, scaling laws, data-centric ML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Scaling laws guide the development of large language models (LLMs) by offering estimates for the optimal balance of model size, tokens, and compute.
More recently, loss-to-loss scaling laws that relate losses across pretraining datasets and downstream tasks have emerged as a powerful tool for understanding and improving LLM performance.
In this work, we investigate which factors most strongly influence loss-to-loss scaling.
Our experiments reveal that the pretraining data and tokenizer determine the scaling trend.
In contrast, model size, optimization hyperparameters, and even significant architectural differences, such as between transformer-based models like \llama{} and state-space models like \mamba{}, have limited impact.
Consequently, practitioners should carefully curate suitable pretraining datasets for optimal downstream performance, while architectures and other settings can be freely optimized for training efficiency.
\end{abstract}



\section{Introduction}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/fig01-overview_latest.pdf}
    \vspace{-8mm}
    \caption{
        \textbf{LLMs' loss-to-loss scaling follows power laws primarily shaped by the choice of pretraining data and tokenizer}.
        Using \llama{} trained on \fineweb{} as a \textcolor{sbblue}{\textbf{baseline}}, we intervene on various factors to assess their impact on train-to-test loss scaling.
        \textcolor{sbgreen}{\textbf{Changing the pretraining data}} has the largest effect, followed by the \textcolor{sborange}{\textbf{choice of tokenizer}}.
        Switching the architecture, e.g., from \llama{} to \mamba{}, has limited impact, while factors like model size, context length, and optimizer settings exert little-to-no influence.
    }
    \label{fig:overview}
\end{figure}


Scaling laws have long guided Large Language Model (LLM) pretraining, determining model and data size under a fixed compute budget~\citep{kaplan2020scalinglawsneurallanguage,hoffmann2022trainingcomputeoptimallargelanguage,grattafiori2024llama3herdmodels}.
Typically, scaling laws relate model performance, usually measured as training or validation loss, to total compute measured in floating point operations (FLOPs).
FLOPs account for both parameter count and the number of training tokens.
While useful for pretraining, scaling laws do not capture how well a model ultimately performs on downstream tasks~\citep{gadre2024languagemodelsscalereliably,schaeffer2024predictingdownstreamcapabilitiesfrontier,du2025understandingemergentabilitieslanguage}.
Consequently, multiple works have begun to investigate \emph{downstream scaling laws}: Scaling laws that directly predict downstream loss from FLOPs~\citep{schaeffer2024predictingdownstreamcapabilitiesfrontier,gadre2024languagemodelsscalereliably}.

\citet{losstolosspredictionscalinglawsbrandfonbrener2024} show that \emph{downstream scaling laws} can be decomposed into compute-to-train-loss scaling laws and (train)-loss-to-(test)-loss scaling laws.
The combination of \emph{compute-to-loss} and \emph{loss-to-loss} scaling laws enables efficient and accurate prediction of a model's downstream performance.
% Moreover, \emph{compute-to-(test)-loss} scaling often optimizes for a single task or average performance across tasks, whereas \emph{loss-to-loss} (especially test-to-test) scaling laws can help tune a model's performance across a broader range of downstream tasks, e.g., to ensure broad or robust generalization.
Moreover, holistic \emph{downstream scaling laws} often optimize for a single task or average performance across tasks~\citep{gadre2024languagemodelsscalereliably,schaeffer2024predictingdownstreamcapabilitiesfrontier}, whereas \emph{loss-to-loss} (especially test-to-test) scaling laws can help tune a model's performance across a broader range of downstream tasks, e.g., to ensure broad or robust generalization.

While the impact of design choices like pretraining distribution, architecture, tokenizer, optimizer settings, etc. on compute-to-loss scaling laws is fairly well understood~\citep{kaplan2020scalinglawsneurallanguage,hoffmann2022trainingcomputeoptimallargelanguage,tay2022scalinglawsvsmodel,wang2024scalinglawsmodelarchitectures,porian2025resolvingdiscrepanciescomputeoptimalscaling,du2025understandingemergentabilitieslanguage}, a similar understanding is missing for loss-to-loss scaling laws.
To close this gap, we extend the work of \citet{losstolosspredictionscalinglawsbrandfonbrener2024, du2025understandingemergentabilitieslanguage}, which analyze loss-to-loss relationships within a single architectural and training setup.
Adding to that, our study systematically explores how multiple factors influence scaling laws across a diverse range of architectures and training configurations.

Our analysis additionally draws inspiration from a body of work in robustness evaluation of vision (and later language) models~\citep{taori2020measuringrobustnessnaturaldistribution,miller2021accuracylinestrongcorrelation,fang2022datadeterminesdistributionalrobustness,awadalla2022exploringlandscapedistributionalrobustness}.
These works show that model performance on different distributions is frequently strongly correlated, and most model and training settings have little-to-no impact on the task-to-task scaling trend of model performance.
We treat loss-to-loss curves similarly and perform a series of interventions using over 6000 model checkpoints to understand what design choices causally affect scaling trends.

\begin{center}
    \begin{tcolorbox}[colframe=sbblue, colback=sbblue!30, coltext=black, width=\columnwidth, left=.7mm, right=.7mm, top=1mm, bottom=1mm, boxrule=.5mm]
        \color{sbblue!50!black}
        We make three main observations, illustrated in \cref{fig:overview}:
        \\
        \begin{enumerate}
            \item LLMs' loss-to-loss scaling consistently follows shifted power laws.
            \item Pretraining data and tokenizer are the most salient factors for these scaling laws.
            \item In contrast, architecture plays a minor role, while model size, context length, and optimizer settings have negligible impact on loss-to-loss scaling.
        \end{enumerate}
    \end{tcolorbox}
\end{center}
Further, we put our observations in the context of downstream scaling laws and discuss the relationship between loss-to-loss and compute-to-loss scaling laws.
Our results indicate that different LLM architectures might encode very similar inductive biases, freeing practitioners to optimize architectures for training efficiency without adversely affecting downstream scaling laws.



\section{From Scaling Laws to Interventions}\label{sec:related}
\paragraph{Compute-to-Train Scaling Laws}
Scaling laws aim to optimize model size and token allocation within a fixed compute budget (expressed in FLOPs) by modeling the relationship between parameters, training tokens, and training loss~\citep{hestness2017deeplearningscalingpredictable,kaplan2020scalinglawsneurallanguage, hoffmann2022trainingcomputeoptimallargelanguage}. However, these laws are inherently shaped by the data distribution, architecture, and optimization settings~\citep{tay2022scalinglawsvsmodel,wang2024scalinglawsmodelarchitectures,losstolosspredictionscalinglawsbrandfonbrener2024,porian2025resolvingdiscrepanciescomputeoptimalscaling}, making their application across setups non-trivial.

\paragraph{Compute-to-Downstream Scaling Laws}
Recent works extend scaling laws to directly predict downstream task performance from compute
~\citep{gadre2024languagemodelsscalereliably, isik2024scalinglawsdownstreamtask,du2025understandingemergentabilitieslanguage}.
While some initial works attempt to map compute budgets to accuracy on individual tasks, multiple tasks, or aggregate benchmarks, this mapping is usually noisy due to several transformations in the accuracy computation that degrade the statistical relationship~\citep{schaeffer2024predictingdownstreamcapabilitiesfrontier}.
More recent efforts instead use the model's average loss on the correct answers of the task as a proxy~\citep{madaan2024quantifyingvarianceevaluationbenchmarks, losstolosspredictionscalinglawsbrandfonbrener2024}.
Such compute-to-downstream scaling laws provide a more practical perspective on scaling but are still specific to a given training setup.

\paragraph{Loss-to-Loss Scaling Laws}
Loss-to-loss scaling laws aim to improve the transferability of scaling insights between training setups by examining the relationship between training (or validation) and test losses, between different validation losses, or between different test losses~\citep{losstolosspredictionscalinglawsbrandfonbrener2024}.
This perspective is crucial for several reasons.
First, train-to-train (or validation-to-validation) scaling implies how scaling laws transfer across datasets~\citep{losstolosspredictionscalinglawsbrandfonbrener2024}.
Second, incorporating train-to-test (or validation-to-test) scaling laws alongside compute-to-train scaling laws provides more precise insight into how compute budgets translate to downstream performance and can help study emergent abilities of models~\citep{du2025understandingemergentabilitieslanguage}.
Third, while compute-to-loss scaling laws often target a single downstream task or average task performance, train-to-test and test-to-test scaling laws can help tune a model's performance across diverse tasks, e.g., to foster the development of generalist LLMs with a balanced task performance.

\paragraph{Accuracy on the Line}
Our work is inspired by robustness research in image classification.
Prior studies~\citep{taori2020measuringrobustnessnaturaldistribution,miller2021accuracylinestrongcorrelation,fang2022datadeterminesdistributionalrobustness} demonstrate a strong and consistent correlation between in-distribution and out-of-distribution (OOD) \emph{accuracy} across various image classification models and settings.
We are not the first to observe the similarity to LLMs, where recent
works~\citep{gadre2024languagemodelsscalereliably,losstolosspredictionscalinglawsbrandfonbrener2024,du2025understandingemergentabilitieslanguage} highlight strong scaling trends (linear or power-law-like) between \emph{losses}.
However, these studies are typically constrained to a single architecture or training setup.
In contrast, we examine trends across a wide range of architectures and training conditions (see \cref{sec:results}), showing for the first time that loss-to-loss scaling follows consistent laws across settings.

\paragraph{Robustness Interventions}
Accuracy-to-accuracy relationships in the vision, vision-language, and language domains have also been used to study how scaling laws shift under robustness interventions like dataset size, adversarial training, architectural details, loss functions, supervision type, or OOD shifts~\citep{taori2020measuringrobustnessnaturaldistribution,fang2022datadeterminesdistributionalrobustness,awadalla2022exploringlandscapedistributionalrobustness,mayilvahanan2024doesclipsgeneralizationperformance,mayilvahanan2024searchforgottendomaingeneralization,wiedemer2024pretraining}.
For vision-language models, \citet{taori2020measuringrobustnessnaturaldistribution,fang2022datadeterminesdistributionalrobustness} find that most interventions do not impact OOD performance; only increasing data diversity has a significant positive impact.
Their findings suggest that curating better datasets is crucial for training vision and vision-language models that generalize broadly.

Motivated by these insights, we aim to uncover the factors determining loss-to-loss scaling to guide practitioners in developing models for specific downstream performance.
Our insights complement the findings from \citet{awadalla2022exploringlandscapedistributionalrobustness}, who show that accuracy-accuracy scaling trends in comprehension tasks are agnostic to architecture type (e.g., encoder-only, encoder-decoder, decoder-only) after fine-tuning. In contrast to their study, we focus on zero-shot generalization across a diverse set of tasks, specifically investigating state-of-the-art decoder-only architectures such as \gpt{}~\cite{Radford2019LanguageMAGPT2}, \llama{}~\citep{grattafiori2024llama3herdmodels}, and \mamba{}~\cite{gu2024mambalineartimesequencemodeling,dao2024transformersssmsgeneralizedmodelsmamba2}.


\section{Fitting Loss-to-Loss Scaling Laws}\label{sec:fit-l2l}
\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figs/fig02-l2l-abb_fineweb-edu_mamba_tiktoken_xfineweb-edu_latest.pdf}
    \vspace{-8mm}
    \caption{
        \textbf{Loss-to-loss scaling consistently obeys power laws.}
        We extend results from \citet{losstolosspredictionscalinglawsbrandfonbrener2024} by many architectures, training settings, and validation/test sets.
        We show illustrative shifted power laws for \mamba{} trained on \fineweb{} here; more configurations and test sets can be found in \cref{app:l2l_trends}.
        For clarity, scatter plots display a random sample of all data points; all points are used to fit the scaling laws.
    }
    \label{fig:l2l-trends}
\end{figure}
\begin{figure}[t]
  \centering
  \includegraphics[width=\columnwidth]{figs/fig03-schematic_latest.pdf}
  \vspace{-8mm}
  \caption{
    \textbf{Schematic of our causal analysis}.
    Checkpoints of a \textcolor{sbblue}{\textbf{base model}} trained on different numbers of tokens and with different seeds lie on the same loss-to-loss line.
    Better-performing models (typically with higher compute) achieve lower loss (towards the bottom left).
    We intervene on training settings (e.g., pretraining data, architecture, etc.) and retrain from scratch, yielding new models that again constitute loss-to-loss lines.
    An \textcolor{sbgreen}{\textbf{effective~intervention}} produces models on a new line; an \textcolor{sbred}{\textbf{ineffective intervention}} yields models that lie on the line of the \textcolor{sbblue}{\textbf{base model}}.
  }
  \label{fig:schematic}
\end{figure}
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figs/fig04-intervention_pretr_fw_latest.pdf}
    \vspace{-8mm}
    \caption{\textbf{Pretraining data has a substantial impact on loss-to-loss scaling laws}. Models are matched on architecture and tokenizer.}
    \label{fig:intervention_pretrain}
\end{figure*}

We focus our analysis on train-to-train and train-to-test scaling.
Combined with known compute-to-train scaling laws, these loss-to-loss scaling laws paint a complete picture of a model's downstream performance given a compute budget and characterize a model's downstream performance distribution across tasks~\citep{losstolosspredictionscalinglawsbrandfonbrener2024}.

As is standard in the recent literature, we report test loss as a proxy for downstream performance.
Following \citet{losstolosspredictionscalinglawsbrandfonbrener2024, madaan2024quantifyingvarianceevaluationbenchmarks,schaeffer2024predictingdownstreamcapabilitiesfrontier}, we track the test loss as a model's loss on only the correct answer given the question as context.
This is sometimes called the \emph{cloze formulation} of a task since the model is essentially evaluated on its ability to fill in blanks.

\citet{losstolosspredictionscalinglawsbrandfonbrener2024} predict train-to-train and train-to-test scaling laws to follow a shifted power law\footnote{\citet{losstolosspredictionscalinglawsbrandfonbrener2024} do not state a train-to-train scaling law for non-compute-matched data. Our form here follows from their Eq.~4 when assuming an irreducible error as in Eqs.~6,~7.}
\begin{equation}\label{eq:power_law}
    L_y\left( f_p^{N, D} \right) \approx K \cdot \Big( L_x\left( f_p^{N, D} \right) - E_{x|p} \Big)^\kappa + E_{y|p},
\end{equation}
where $L_x, L_y$ are the losses on datasets $\mathcal D_x, \mathcal D_y$ shown on the x- and y-axis.
$f_p^{N, D}$ is a model trained with $N$ parameters on $D$ tokens on the pretraining set $\mathcal D_p$, and $K$ and $\kappa$ are parameters to be fit.
$E_{x|p}, E_{y|p}$ are the irreducible errors (i.e., minimum loss) that $f_p$ trained on $\mathcal D_p$ can achieve on the datasets $\mathcal D_x, \mathcal D_y$.

Given a model configuration, we obtain 200 to 800 by recording checkpoints throughout training, across model sizes, and for various seeds.
All points are used to first estimate $E_{x|p}, E_{y|p}$ from individual compute-to-loss scaling laws
% of the form
% \begin{equation}
%     L
% \end{equation}
and then fit $K, \kappa$.
% Our scatter plots as in \cref{fig:l2l-trends} only show a random sample to keep the plots clean.

Note that to study the impact of various training settings, we show losses of \emph{the same model} on \emph{different datasets} on the x- and y-axis.
This should not be confused with some curves in \citet{losstolosspredictionscalinglawsbrandfonbrener2024} that showed the losses of \emph{different} compute-matched models on the x- and y-axis.

With this setup, we can now analyze the loss-to-loss scaling laws of models trained with different configurations.
\citet{losstolosspredictionscalinglawsbrandfonbrener2024} only showed loss-to-loss scaling a single architecture with a fixed training recipe: Olmo~\citep{groeneveld2024olmo}.
We extend their analysis by multiple architectures, pretraining sets, tokenizers, and training settings, all listed in \cref{sec:results}.
As an illustrative example, we show loss-to-loss scaling for \mamba{} trained on \fineweb{} in \cref{fig:l2l-trends}; more examples with additional test sets are listed in \cref{app:l2l_trends} and throughout \cref{sec:results}.

Overall, across models, datasets, tokenizers, and optimization hyperparameters shifted power laws describe loss-to-loss scaling well (\cref{eq:power_law}).
On some datasets, the performance of models with high loss (towards the top right of each curve) is not captured perfectly by the power law formulation proposed by \citet{losstolosspredictionscalinglawsbrandfonbrener2024}.
This is unsurprising, given that these data points typically represent models in early training stages but might hint at a refined formulation of \cref{eq:power_law} for the high-loss regime.

Note also that loss-to-loss scaling follows a power law even for datasets on which the model never reaches high accuracy.
E.g., \mamba{} in \cref{fig:l2l-trends} never surpasses chance performance on \arcchallenge{} and \openbookqa{}, yet \cref{eq:power_law} describes the test loss equally well.
This underlines the usefulness of loss-to-loss scaling laws to study model behavior.

\takeaway{Across architectures and training settings, loss-to-loss scaling generally follows a shifted power law as described in \cref{eq:power_law} and illustrated in \cref{fig:l2l-trends}.}



\section{A Causal Analysis of Loss-to-Loss Scaling}\label{sec:results}
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figs/fig05-intervention_token_fw_latest.pdf}
    \vspace{-8mm}
    \caption{\textbf{The tokenizer has a moderate impact on loss-to-loss scaling laws}. Models are matched on pretraining data and architecture.}
    \label{fig:intervention_tokenizer}
\end{figure*}

We now perform interventions on the model and training configurations to find what factors cause the exact shape of loss-to-loss scaling laws.

Our basic procedure is outlined in \cref{fig:schematic}.
As mentioned in \cref{sec:related}, our approach is motivated by similar studies in the robustness literature.
In contrast to that setting, we here lack paired in-distribution and out-of-distribution datasets.
% (akin to, e.g., ImageNet~\citep{} and ImageNet-Sketch~\citep{} in vision).
Instead, we simply consider all combinations of validation and test sets.
For ease of visualization when intervening on the pretraining data, we always show \fineweb{}  validation loss on the x-axis, even for models trained on different pretraining distributions.
This choice is arbitrary and does not affect our results; see \cref{app:x-axis}.
Similarly, we here report results for scaling laws of \emph{average} validation and test loss; results for individual losses can be found in \cref{app:no-avg}.

For our analysis, we consider the impact of pretraining data, tokenizer, architecture, model size, context length, and optimizer settings.

\paragraph{Pretraining Sets}
Our models are trained on \fineweb{}~\citep{penedo2024finewebdatasetsdecantingweb}, \cfour{}~\citep{dodge2021documentinglargewebtextcorporac4}, and an uncopyrighted version of \thepile{} dubbed \thepileuc{}.
Some models from \huggingface{} are trained on the original version of \thepile{}~\citep{gao2020thepile800gbdatasetdiverse} and \thepiledd{}~\citep{biderman2023pythiasuiteanalyzinglarge}, a deduplicated version.

\paragraph{Validation Sets}
Models are evaluated on \num{5000} sequences sampled from the validation sets of \fineweb{}, \cfour{}, \thepile{}, \refinedweb{}~\citep{penedo2023refinedwebdatasetfalconllm}, and \slimpajama{}~\citep{shen2024slimpajamadcunderstandingdatacombinations}.

\paragraph{Test Sets}
% We use LM Evaluation Harness framework~\citep{eval-harness} to assess model performance on \hellaswag{}~\citep{zellers2019hellaswagmachinereallyfinish}, \mmlu{}~\citep{hendrycks2021measuringmassivemultitasklanguagemmlu}, \winogrande{}~\citep{sakaguchi2019winograndeadversarialwinogradschema}, \piqa{}~\citep{bisk2019piqareasoningphysical}, \socialiqa{}~\citep{sap2019socialiqasocial}, \commonsenseqa{}~\citep{talmor2019commonsenseqaquestionansweringchallenge}, \openbookqa{}~\citep{mihaylov2018suitarmorconductelectricityopenbookqa}, \copa{}~\citep{Gordon2011ChoiceOPcopa}, as well as \arceasy{} and \arcchallenge{}~\citep{clark2018thinksolvedquestionansweringarc}.
We use LM Evaluation Harness framework~\citep{eval-harness} to assess model performance on \hellaswag{}~\citep{zellers2019hellaswagmachinereallyfinish}, \copa{}~\citep{Gordon2011ChoiceOPcopa}, \winogrande{}~\citep{sakaguchi2019winograndeadversarialwinogradschema}, \piqa{}~\citep{bisk2019piqareasoningphysical}, \openbookqa{}~\citep{mihaylov2018suitarmorconductelectricityopenbookqa}, \socialiqa{}~\citep{sap2019socialiqasocial}, \commonsenseqa{}~\citep{talmor2019commonsenseqaquestionansweringchallenge}, \mmlu{}~\citep{hendrycks2021measuringmassivemultitasklanguagemmlu}, as well as \arceasy{} and \arcchallenge{}~\citep{clark2018thinksolvedquestionansweringarc}.

\paragraph{Architectures}
We train \llama{}-3~\citep{grattafiori2024llama3herdmodels} with \SI{417}{M} parameters and \mamba{}~\citep{gu2024mambalineartimesequencemodeling} with \SI{420}{M} parameters using the Lingua framework~\citep{meta_lingua}, following Chinchilla scaling laws~\citep{hoffmann2022trainingcomputeoptimallargelanguage}.
We supplement our analysis with pretrained \gpt{}~\citep{Black2021gptneoo,black2022gptneox20bopensourceautoregressivelanguage,biderman2023pythiasuiteanalyzinglarge}, \llama{}~\citep{penedo2024finewebdatasetsdecantingweb}, and \mamba{}~\cite{gu2024mambalineartimesequencemodeling,dao2024transformersssmsgeneralizedmodelsmamba2} variants from \huggingface~\citep{wolf2020huggingfacestransformersstateoftheartnatural}.

\paragraph{Tokenizers}
We train \llama{} and \mamba{} with either a \texttt{tiktoken} tokenizer (\SI{128}{k} vocabulary size) or the \texttt{gpt2} tokenizer (\num{50257} vocabulary size).
Pretrained models from \huggingface{} use an almost identical \gpt{}-2 tokenizer, dubbed \texttt{gpt2-HF}.
This version does not explicitly pad text with beginning and end-of-sequence tokens.
A few \huggingface{} \gpt{} models instead use the \texttt{gpt-neox} tokenizer with a slightly different vocab size of \num{50254}, which results in a different internal mapping compared to \texttt{gpt2}, 


\subsection{Pretraining Data, Tokenizer, and Architecture}\label{sec:interventions_main}
First, we jointly examine the effect of pretraining data, architecture, and tokenizer.
Since we face limited compute to train models from scratch, we do not have checkpoints for all possible combinations of these factors.
Instead, we analyze the effect of an intervention on each factor when matching models in the two other factors.
Note that we do not have sufficient checkpoints for some \huggingface{} models to fit a power law.
Nevertheless, in all these cases, the available data points follow a clearly discernible trend.

\paragraph{Effect of Pretraining Data}\label{sec:intervention_pretrain}
\cref{fig:intervention_pretrain} illustrates the substantial impact pretraining data has on loss-to-loss scaling.
Across architectures and compute (in different columns), changing the pretraining data leads to a large shift in the loss-to-loss curve.
The only exception is the last column, where we compare \huggingface{} models trained on \thepile{} and a deduplicated version. Models trained on either version lie on the same curve, suggesting that the deduplication procedure successfully reduced the dataset size while producing a similar distribution that does not significantly impact loss-to-loss scaling.

\takeaway{With fixed architecture and tokenizer, changing the pretraining data leads to \emph{substantial} shifts in loss-to-loss scaling laws; see \cref{fig:intervention_pretrain}.}

\paragraph{Effect of Tokenizer}\label{sec:intervention_tokenizer}
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figs/fig06-intervention_archi_fw_latest.pdf}
    \vspace{-8mm}
    \caption{\textbf{Architecture has limited impact on loss-to-loss scaling laws}. Models are matched on pretraining data and tokenizer.}
    \label{fig:intervention_arch}
\end{figure*}
\cref{fig:intervention_tokenizer} shows that the tokenizer, too, affects loss-to-loss scaling laws, albeit less strongly than pretraining data.
It is interesting to see how slight deviations in the tokenizer can have a pronounced effect, particularly for train-to-train scaling laws.
While the slight vocabulary size difference between \texttt{gpt2-HF} and \texttt{gpt-neox} has little impact on loss-to-loss scaling (last column), the different handling of special tokens in \texttt{gpt2} and \texttt{gpt2-HF} does.
To the best of our knowledge, this effect has not been observed before and could be explored in future work.

\takeaway{With fixed architecture and pretraining data, changing the tokenizer leads to \emph{moderate} changes in loss-to-loss scaling laws; see \cref{fig:intervention_tokenizer}.}

\paragraph{Effect of Architecture}\label{sec:intervention_arch}
Lastly, \cref{fig:intervention_arch} illustrates that changing the architecture results in only very slight changes in the loss-to-loss curves across pretraining data and tokenizer settings.
Unlike pretraining data and tokenizer, architecture has little influence on train-to-train and train-to-test scaling.
This is particularly surprising given the significant architectural differences between \llama{} or \gpt{} (transformer-based models) and \mamba{} (a state-space model). These results raise an important question: Do current architectures encode distinct inductive biases or converge to similar solutions given the same training data?
Further research is needed to understand the implications of this finding.

\takeaway{With fixed pretraining data and tokenizer, changing the architecture has \emph{limited} impact on loss-to-loss scaling laws --- raising questions about the distinctiveness of their inductive biases; see \cref{fig:intervention_arch}.}



\subsection{Model Size, Context Length, and Optimization}\label{sec:interventions_additional}
We now examine the effect of other common design decisions, such as the number or width of layers, the context length, optimizer, learning schedule, learning rate, and weight decay.
In contrast to \cref{sec:interventions_main}, we can perform these interventions separately since we can compare among our own \llama{} and \mamba{} models whose training settings are matched by default.

To provide a more succinct overview, we only show train-to-train scaling laws in this section; additional train-to-test scaling laws for the same intervention can be found in \cref{app:add-train-test}.
We also do not show fitted power laws here since we display many more models per plot than in \cref{sec:interventions_main}, and the scaling trends are clearly discernible.

\paragraph{Effect of Model Size}
\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figs/fig07-intervention_size_FineWeb-Edu_val_latest.pdf}
    \vspace{-6mm}
    \caption{
        \textbf{Model size does not affect loss-to-loss scaling}.
        The distinct lines correspond to different pretraining distributions (see \cref{fig:intervention_pretrain}), reinforcing that their influence is consistent across scales.
    }
    \label{fig:intervention_size}
\end{figure}

We first examine the influence of model size by training \llama{} and \mamba{} models with varying depths and widths (see \cref{app:model-details} for details).
\cref{fig:intervention_size} shows the results: Despite significant differences in parameter count, the loss-to-loss scaling trends remain unchanged.
These findings align well with \citet{du2025understandingemergentabilitieslanguage}, who observed that model size has little effect on loss-to-loss scaling for \gpt{} models. We extend this conclusion to \llama{} and \mamba{} and across multiple pretraining distributions.

\paragraph{Effect of Context Length}
\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figs/fig08-intervention_ctx_FineWeb-Edu_val_latest.pdf}
    \vspace{-6mm}
    \caption{
        \textbf{Context length does not affect loss-to-loss scaling}.
        Again, distinct lines correspond to different pretraining distributions (compare \cref{fig:intervention_pretrain}), validating their consistent impact.
    }
    \label{fig:intervention_ctx}
\end{figure}

We next investigate the effect of varying the context length between \num{1024}, \num{2048}, and \num{3076} tokens.
As shown in \cref{fig:intervention_ctx}, this change does not meaningfully affect the loss-to-loss scaling curves.

\paragraph{Effect of Optimization Settings}
\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figs/fig09-intervention_optim_FineWeb-Edu_val_latest.pdf}
    \vspace{-6mm}
    \caption{\textbf{Optimization settings do not affect loss-to-loss scaling}.}
    \label{fig:intervention_optim}
\end{figure}

Finally, we evaluate a range of common optimization settings:
We consider the Adam~\citep{kingma2017adammethodstochasticoptimizationadamo} and AdamW~\citep{loshchilov2019decoupledweightdecayregularizationadamw} optimizers, cosine~\citep{loshchilov2017sgdrstochasticgradientdescentcosine} and WSD~\citep{hu2024minicpmunveilingpotentialsmallwsd} schedules, learning rates of \num{3e-4} and \num{3e-3}, and a weight decay of \num{0.1} or \num{3.3e-2}.
In our training setup, models using the Adam optimizer generally did not converge, and we exclude them from the analysis.
Variations of the other settings do not affect loss-to-loss scaling coefficients, as shown in \cref{fig:intervention_optim}.

\takeaway{Model size (\cref{fig:intervention_size}), context length (\cref{fig:intervention_ctx}), and optimization settings (\cref{fig:intervention_optim}) have \emph{negligible} impact on loss-to-loss scaling laws.}

Given the limited impact of the factors studied in this section, the conclusions from \cref{sec:interventions_main} should generalize well across variations in model size, context length, and optimization settings.
For example, the substantial impact of the pretraining distribution can also be observed in \cref{fig:intervention_size,fig:intervention_ctx}.



\section{Discussion and Future Work}
Our findings add to the understanding of loss-to-loss scaling laws and reinforce prior results from vision and vision-language research~\citep{taori2020measuringrobustnessnaturaldistribution,fang2022datadeterminesdistributionalrobustness} on the importance of choosing the pretraining data.

\paragraph{Implications for Optimizing Downstream Performance}
% This has significant implications:
Our results emphasize that the data distribution is the key for achieving a desireable loss-to-loss scaling and a in turn achieve a great downstream performance. Conversely, since architecture has little impact on the train-to-test conversion, it can be freely optimized for better compute scaling without affecting downstream scaling or performance.

\paragraph{Implications for Balancing Performance}
If the aim is not only optimal average downstream performance but also a specific weighting between different tasks, e.g., to ensure a balanced downstream performance, individual train-to-test scaling laws can be used to tune a model's performance.
Here, too, the pretraining data has the largest impact and practitioners should thus consider the final application of their model already during the data curation stage.
% If practitioners aim to achieve a specific test-to-test scaling trend—say, one that ensures a balanced downstream performance—they must intervene at the data level, as architecture, optimization hyperparameters, and other factors have negligible impact. At the same time, since these factors do not meaningfully affect train-loss to test-loss scaling exponents, they can be freely optimized for better compute-to-train-loss scaling without sacrificing generalization.
Ultimately, our findings underscore that pretraining data curation, rather than architectural innovation, can be the primary driver in developing robust, generalist models.
% —freeing researchers to optimize architecture and optimization parameters purely for efficiency gains without impacting downstream performance.

\paragraph{On Architectural Biases}
The limited impact of even drastically different architectures on loss-to-loss scaling behavior illustrated in \cref{sec:interventions_main} and \cref{fig:intervention_arch} suggest that architectures trained on the same data may implicitly learn highly similar representations.
% Sec. ~\ref{sec:intervention_arch} demonstrates that fundamentally different architectures exhibit similar loss-to-loss scaling behavior when trained on the same pretraining data. This suggests that these architectures may implicitly learn highly similar representations.
This might seem intuitive, as all models minimize the same loss function.
One might expect them to converge toward comparable solutions when the training loss approaches zero~\citep{roeder2020linearidentifiabilitylearnedrepresentations}.
However, even checkpoints of our smaller models, when trained on fewer tokens, follow the same scaling across architectures.
% However, it is striking that even smaller models or those trained on fewer tokens in our study still follow the same scaling trends.
Understanding whether this implies representational and behavioral similarity remains an intriguing open question.
Beyond this, it remains to be seen whether it is possible to formulate architectures that fit the data well but exhibit different scaling trends.
% Beyond this, a more intriguing question arises: Should we expect architectures to go beyond the data—that is, to implicitly reweight or extrapolate from the training distribution in ways that enhance generalization? Investigating whether and how models achieve such domain reweighting is a crucial direction for future research.

\paragraph{On New Training Paradigms}
Our study intentionally focuses on models trained with standard loss functions and conventional training settings to guide practitioners.
% to evaluate whether current setups yield fundamentally different scaling trends. Given that they do not, our findings highlight the need for innovative training approaches to break past existing scaling limitations.
The limited impact of existing paradigms does not preclude innovative training approaches from improving loss-to-loss scaling.
% In fact, the limited impact of standard training settings highlights a need for innovative training paradigms that break past existing scaling limitations.
In fact, a recent work by \citet{saunshi2024inductivebiasstackingimproving} demonstrates that gradually increasing model depth and initializing based on layers from a smaller model produces markedly different scaling behavior, particularly in how perplexity translates to downstream accuracy.
Similar structured growth approaches could offer new pathways for improving scaling efficiency and generalization for decoder-only LLMs trained with next-token prediction. We leave this exercise for future work.
% Applying such structured growth strategies to decoder-only LLMs trained with next-token prediction could offer new pathways for improving scaling efficiency and generalization. Exploring these alternative paradigms for LLM training presents an exciting avenue for future research.

\paragraph{On the Exhaustiveness of Interventions in \cref{sec:interventions_main}}
Our study clearly distinguishes between factors with substantial and limited impact on loss-to-loss scaling.
While our conclusions are inherently shaped by the specific settings we explored, the observed trends provide strong empirical evidence for these distinctions.
% Pretraining data and tokenizer emerge as the most consequential factors, consistently shifting scaling trends.
Given the strong and consistent impact of pretraining data and tokenizer, we can confidently conclude that these interventions affect loss-to-loss scaling.
While we observed only a limited impact of the architecture, this effect was also consistent across major state-of-the-art architectures including \llama{}, \gpt{}, and \mamba{} --- which collectively represent the dominant paradigms in large-scale language modeling.
% Across all these architectures, the effect on scaling trends remains minimal.
 Given this exhaustive set, it is hard to argue that other architectures would meaningfully alter loss-to-loss scaling.

\paragraph{On the Exhaustiveness of Interventions in \cref{sec:interventions_additional}}
Across the wide range of size configurations (\cref{app:model-details}) we test, all models exhibit very consistent loss-to-loss scaling.
Similarly, the effect we observed for different context lengths is very consistent within our test range (1024, 2048, 3076), which aligns with commonly used configurations~\citep{Black2021gptneoo,wang2021gptj,biderman2023pythiasuiteanalyzinglarge,penedo2024finewebdatasetsdecantingweb,black2022gptneox20bopensourceautoregressivelanguage}.
While we acknowledge the possibility that larger models or longer context lengths could influence loss-to-loss scaling, such an effect --- if present --- is unlikely.
For optimization settings, we again consider configurations widely used in LLM training~\citep{shoeybi2020megatronlmtrainingmultibillionparameter,nanogpt,meta_lingua}, including variations in optimizer type, learning rate, weight decay, and scheduling.
While our results indicate that these choices do not meaningfully alter loss-to-loss scaling within the explored settings, we acknowledge that the space of optimization techniques is vast, and our list is not exhaustive.
It remains possible that a principled optimization strategy, different from current best practices, could induce new scaling behaviors.
However, our findings suggest that optimization settings are not a primary driver of loss-to-loss scaling trends within the bounds of conventional language model training.

\paragraph{On the Goodness of Fit of Scaling Laws}
For a given model size, we train on a number of tokens up to the Chinchilla-optimal amount (e.g., \SI{8.4}{B} tokens for a \SI{420}{M} parameter model) and maintain a constant warmup of \num{5000} steps, a learning rate of \num{3e-3}, and a one-cycle cosine decay schedule.
% We train models for up to 8.4 billion tokens (the Chinchilla-optimal amount for a \SI{420}{M} parameter model, i.e., $\SI{420}{M} \times 20$) and use intermediate checkpoints for fitting scaling laws. We maintain a constant warmup of 5000 steps, a learning rate of 3e-3, and a one-cycle cosine decay schedule.
% We use both the final checkpoint for models of different sizes and intermediate checkpoints with the same size to fit scaling laws.
We use intermediate checkpoints as a proxy for models trained on fewer tokens.
While \cite{hoffmann2022trainingcomputeoptimallargelanguage} suggest adapting the scheduler in this case to align with the number of tokens, we are constrained by compute and cannot train thousands of models from scratch.
While using intermediate checkpoints alongside models of different sizes may influence the specific shape of the fitted compute-to-loss scaling laws, we find that the overall quality of fit benefits greatly from the additional data points.
Since we only use compute-to-loss scaling laws to estimate the entropy terms in \cref{eq:power_law} and are interested primarily in the impact of interventions on loss-to-loss scaling, we do not expect this choice to significantly impact our conclusions.
We also not that using intermediate checkpoints for fitting scaling laws is not unprecedented in the literature~\citep{schaeffer2024predictingdownstreamcapabilitiesfrontier,losstolosspredictionscalinglawsbrandfonbrener2024}.
% in our compute to loss fitsThis choice may impact the precision of compute-to-loss fits (i.e., finding the optimal curvature) and the ideal model size/token allocation under a fixed compute budget. However, this is not a concern for our study, \textbf{as we are only interested in how robustness interventions affect the loss-to-loss scaling trends rather than the absolute fits}. Furthermore, we fit the compute-to-loss scaling law primarily to extract the entropy term in \cref{eq:power_law}, which depends only on the minimum loss achieved by our largest trained model—typically aligned with Chinchilla scaling, as previously noted. Additionally, it is also not uncommon to use intermediate checkpoints for fitting scaling laws~\citep{schaeffer2024predictingdownstreamcapabilitiesfrontier,losstolosspredictionscalinglawsbrandfonbrener2024}.

% \paragraph{On the relationship between compute to loss and loss to loss scaling laws} Might skip this given implications, let's see

% \paragraph{Estimation error and better downstream scaling laws}


\section{Conclusion}
In this work, we systematically investigate loss-to-loss scaling in LLMs, identifying key factors that shape its behavior.
Our large-scale interventional analysis --- spanning over 6000 model checkpoints across architectures, tokenizers, and training setups --- reveals that loss-to-loss scaling consistently follows shifted power-law trends, enabling predicting test performance from training loss.

We identify pretraining data and tokenizer as the dominant factors shaping these scaling laws, highlighting the importance of data curation.
Architecture has limited impact, with models as different as LLaMA (transformer-based) and Mamba (a state-space model) exhibiting nearly identical scaling when trained on the same data and tokenizer. 
Model size, context length, and optimization settings have negligible influence, such that loss-to-loss scaling remains stable across different configurations.

% Our findings challenge the assumption that architecture drives generalization differences, suggesting that LLMs share similar inductive biases. 
Our findings underline the importance of pretraining data for downstream performance and robustness and suggest that different LLM might share similar architectural biases.
% Our findings also have broader implications.
Given our observations, practitioners should prioritize curating high-quality pretraining data to optimize downstream performance, while architectures and training settings can be adjusted freely for efficiency.

\clearpage
\section*{Impact}
This paper presents work whose goal is to advance the field
of Machine Learning. There are many potential societal
consequences of our work, none of which we feel must be
specifically highlighted here.

\section*{Author Contributions}
This project was co-led and coordinated by PM and TW. TW and PM jointly developed the method, incorporating insights from WB and MB. PM was responsible for training and evaluating the models. SM assisted with certain evaluations and provided some details for \huggingface{} models. TW analyzed the data and conducted the scaling law and loss-to-loss fits. Both TW and PM contributed to writing the manuscript, with feedback from WB. TW created all the figures with feedback from PM.

\section*{Acknowledgements}
We would like to thank (in alphabetical order) Ameya Prabhu, Attila Juhos, Evgenia Rusak, Fanfei Li, Jack Brady, Thomas Klein, and Vishaal Udandarao for helpful discussions and feedback.

This work was supported by the German Federal Ministry of Education and Research (BMBF): Tübingen AI Center, FKZ: 01IS18039A. WB acknowledges financial support via an Emmy Noether Grant funded by the German Research Foundation (DFG) under grant no. BR 6382/1-1 and via the Open Philantropy Foundation funded by the Good Ventures Foundation. WB is a member of the Machine Learning Cluster of Excellence, EXC number 2064/1 – Project number 390727645. This research utilized compute resources at the Tübingen Machine Learning Cloud, DFG FKZ INST 37/1057-1 FUGG.

We thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting PM and TW.

\bibliography{example_paper}
\bibliographystyle{icml2025}

\clearpage
\appendix
\onecolumn
% \begin{figure}
%     \centering
%     \includegraphics[width=0.5\linewidth]{figs/fig_fit_vs_accuracy_llama_c4_20250124_173122.png}
%     \caption{\textbf{Outliers are not predicted by relative accuracy}.}
%     \label{fig:fit_v_accuracy}
% \end{figure}

\section{Model Details}\label{app:model-details}
In addition to the models discussed in \cref{sec:interventions_main}, we add additional details of \llama{} and \mamba{} models we trained of different depths and widths. 

\begin{table}[]
    \caption{Details of the models we trained from scratch using the tiktoken tokenizer.}\label{tab:model_card}
    \vskip 0.1in
    \centering
    \begin{tabular}{l S[table-format=4] S[table-format=2] S[table-format=3,table-space-text-post={\,M}]}
    \toprule
    \textbf{Architecture} & \textbf{Width} & \textbf{Depth} & \textbf{Parameters}\\
    \midrule
    \llama{} & 1024 & 12 & 416\,M \\
    \llama{} & 1024 & 8 & 365\,M \\ 
    \llama{} & 1024 & 4 & 314\,M \\ 
    \llama{} & 512 & 12 & 172\,M \\ 
    \llama{} & 512 & 8 & 158\,M \\ 
    \llama{} & 512 & 4 & 145\,M \\ 
    \llama{} & 256 & 12 & 76\,M \\ 
    \llama{} & 256 & 8 & 72\,M \\ 
    \llama{} & 256 & 4 & 59\,M \\ 
    \midrule
    \mamba{} & 1024 & 24 & 420\,M \\
    \mamba{} & 1024 & 16 & 367\,M \\ 
    \mamba{} & 1024 & 8 & 315\,M \\ 
    \mamba{} & 512 & 24 & 172\,M \\ 
    \mamba{} & 512 & 16 & 158\,M \\ 
    \mamba{} & 512 & 8 & 145\,M \\ 
    \mamba{} & 256 & 24 & 76\,M \\ 
    \mamba{} & 256 & 16 & 73\,M \\ 
    \mamba{} & 256 & 8 & 69\,M \\ 
    \bottomrule
    \end{tabular}
\end{table}

\section{Loss-to-Loss Scaling Across Settings}\label{app:l2l_trends}
% We supplement \cref{fig:l2l-trends} from \cref{sec:fit-l2l} with additional architecture-pretraining pairings and evaluations on \commonsenseqa{}, \socialiqa{}, and \mmlu{} in \cref{fig:l2l-llama-fw,fig:l2l-llama-c4,fig:l2l-llama-pile,fig:l2l-mamba-fw,fig:l2l-mamba-c4,fig:l2l-mamba-pile}.

We supplement \cref{fig:l2l-trends} from \cref{sec:fit-l2l} with additional architecture-pretraining pairings in \cref{fig:l2l-llama-fw,fig:l2l-llama-c4,fig:l2l-llama-pile,fig:l2l-mamba-fw,fig:l2l-mamba-c4,fig:l2l-mamba-pile}.

\begin{figure}
    \centering
    \includegraphics[width=.55\linewidth]{figs/fig02-l2l-all_fineweb-edu_llama_tiktoken_xfineweb-edu_latest.pdf}
    \caption{
        \textbf{Loss-to-Loss Scaling for \fineweb{}-trained \llama{}}.
    }
    \label{fig:l2l-llama-fw}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=.55\linewidth]{figs/fig02-l2l-all_c4_llama_tiktoken_xc4_latest.pdf}
    \caption{
        \textbf{Loss-to-Loss Scaling for \cfour{}-trained \llama{}}.
    }
    \label{fig:l2l-llama-c4}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=.55\linewidth]{figs/fig02-l2l-all_the-pile-uc_llama_tiktoken_xfineweb-edu_latest.pdf}
    \caption{
        \textbf{Loss-to-Loss Scaling for \thepile{}-trained \llama{}}.
    }
    \label{fig:l2l-llama-pile}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=.55\linewidth]{figs/fig02-l2l-all_fineweb-edu_mamba_tiktoken_xfineweb-edu_latest.pdf}
    \caption{
        \textbf{Loss-to-Loss Scaling for \fineweb{}-trained \mamba{}}.
    }
    \label{fig:l2l-mamba-fw}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=.55\linewidth]{figs/fig02-l2l-all_c4_mamba_tiktoken_xc4_latest.pdf}
    \caption{
        \textbf{Loss-to-Loss Scaling for \cfour{}-trained \mamba{}}.
    }
    \label{fig:l2l-mamba-c4}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=.55\linewidth]{figs/fig02-l2l-all_the-pile-uc_mamba_tiktoken_xfineweb-edu_latest.pdf}
    \caption{
        \textbf{Loss-to-Loss Scaling for \thepile{}-trained \mamba{}}.
    }
    \label{fig:l2l-mamba-pile}
\end{figure}



\section{Intervention Results for Different Choices of x-Axis}\label{app:x-axis}
We show variations of \cref{fig:intervention_pretrain,fig:intervention_tokenizer,fig:intervention_arch} from \cref{sec:interventions_main} with \cfour{} validation loss as the x-axis in \cref{fig:intervention_pretrain_c4,fig:intervention_tokenizer_c4,fig:intervention_arch_c4}.
Variations for \cref{fig:intervention_size,fig:intervention_ctx,fig:intervention_optim} from \cref{sec:interventions_additional} are shown in \cref{fig:intervention_size_c4,fig:intervention_ctx_c4,fig:intervention_optim_c4}.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/fig04-intervention_pretr_c4_latest.pdf}
    \caption{
        \textbf{Pretraining data has a substantial impact on loss-to-loss scaling laws}.
    }
    \label{fig:intervention_pretrain_c4}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/fig05-intervention_token_c4_latest.pdf}
    \caption{
        \textbf{The tokenizer has a moderate impact on loss-to-loss scaling laws}.
    }
    \label{fig:intervention_tokenizer_c4}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/fig06-intervention_archi_c4_latest.pdf}
    \caption{
        \textbf{Architecture has limited impact on loss-to-loss scaling laws}.
    }
    \label{fig:intervention_arch_c4}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=.6\linewidth]{figs/fig07-intervention_size_C4_val_latest.pdf}
    \caption{\textbf{Model size does not affect train-to-test scaling}.}
    \label{fig:intervention_size_c4}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=.6\linewidth]{figs/fig08-intervention_ctx_C4_val_latest.pdf}
    \caption{\textbf{Context length does not affect train-to-test scaling}.}
    \label{fig:intervention_ctx_c4}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=.6\linewidth]{figs/fig09-intervention_optim_C4_val_latest.pdf}
    \caption{\textbf{Optimizer settings do not affect train-to-test scaling}.}
    \label{fig:intervention_optim_c4}
\end{figure}


\section{Intervention Results without Averaging}\label{app:no-avg}
The causal analysis in \cref{sec:results} was performed on scaling laws for \emph{average} validation or test loss. \cref{fig:intervention_pretrain_specific1,fig:intervention_pretrain_specific2,fig:intervention_tokenizer_specific1,fig:intervention_tokenizer_specific2,fig:intervention_arch_specific1,fig:intervention_arch_specific2} show illustrative results on scaling laws for individual datasets.

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{figs/fig_intervention_arch_20250127_182119.pdf}
%     \caption{Non-averaged version of \cref{fig:ablation_arch}.}
%     \label{fig:ablation_arch_full}
% \end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/fig04-intervention_pretr_spec1_latest.pdf}
    \caption{
        \textbf{Pretraining data has a substantial impact on loss-to-loss scaling laws}.
    }
    \label{fig:intervention_pretrain_specific1}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/fig04-intervention_pretr_spec2_latest.pdf}
    \caption{
        \textbf{Pretraining data has a substantial impact on loss-to-loss scaling laws}.
    }
    \label{fig:intervention_pretrain_specific2}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/fig05-intervention_token_spec1_latest.pdf}
    \caption{
        \textbf{The tokenizer has a moderate impact on loss-to-loss scaling laws}.
    }
    \label{fig:intervention_tokenizer_specific1}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/fig05-intervention_token_spec2_latest.pdf}
    \caption{
        \textbf{The tokenizer has a moderate impact on loss-to-loss scaling laws}.
    }
    \label{fig:intervention_tokenizer_specific2}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/fig06-intervention_archi_spec1_latest.pdf}
    \caption{
        \textbf{Architecture has limited impact on loss-to-loss scaling laws}.
    }
    \label{fig:intervention_arch_specific1}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/fig06-intervention_archi_spec2_latest.pdf}
    \caption{
        \textbf{Architecture has limited impact on loss-to-loss scaling laws}.
    }
    \label{fig:intervention_arch_specific2}
\end{figure}



\section{Additional Train-to-Test Scaling Laws}\label{app:add-train-test}
We provide train-to-test scaling laws for the interventions performed in \cref{sec:interventions_additional} and \cref{fig:intervention_size,fig:intervention_ctx,fig:intervention_optim} in \cref{fig:intervention_size_test,fig:intervention_ctx_test,fig:intervention_optim_test,fig:intervention_size_fw_test,fig:intervention_ctx_fw_test,fig:intervention_optim_fw_test}.

\begin{figure}[t]
    \centering
    \includegraphics[width=.6\linewidth]{figs/fig07-intervention_size_C4_test_latest.pdf}
    \caption{\textbf{Model size does not affect train-to-test scaling}.}
    \label{fig:intervention_size_fw_test}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=.6\linewidth]{figs/fig07-intervention_size_FineWeb-Edu_test_latest.pdf}
    \caption{\textbf{Context length does not affect train-to-test scaling}.}
    \label{fig:intervention_ctx_fw_test}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=.6\linewidth]{figs/fig08-intervention_ctx_C4_test_latest.pdf}
    \caption{\textbf{Optimizer settings do not affect train-to-test scaling}.}
    \label{fig:intervention_optim_fw_test}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=.6\linewidth]{figs/fig08-intervention_ctx_FineWeb-Edu_test_latest.pdf}
    \caption{\textbf{Model size does not affect train-to-test scaling}.}
    \label{fig:intervention_size_test}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=.6\linewidth]{figs/fig09-intervention_optim_C4_test_latest.pdf}
    \caption{\textbf{Context length does not affect train-to-test scaling}.}
    \label{fig:intervention_ctx_test}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=.6\linewidth]{figs/fig09-intervention_optim_FineWeb-Edu_test_latest.pdf}
    \caption{\textbf{Optimizer settings do not affect train-to-test scaling}.}
    \label{fig:intervention_optim_test}
\end{figure}

\end{document}