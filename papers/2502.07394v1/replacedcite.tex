\section{Related Work}
\label{sec:related}
% Anomaly prediction and predictive maintenance
In anomaly detection, the goal is to detect datapoints from a larger set of samples that are significantly different to the other samples.
It is a long-studied problem domain that, especially in recent years, is approached more and more with machine learning and deep learning methods ____.
Related to anomaly detection is failure prediction in the field of predictive maintenance, where the prediction task is to determine whether or not a machine, part, etc. is about to have a critical failure (and why).
Specifically, failure prediction is only concerned with critical failures that are non-recoverable, i.e., where the process has to be halted and cannot recover itself.
To tackle this prediction task, as with anomaly detection, machine learning and deep learning is becoming more and more popular ____.
In this work, we want to focus on one particular method to predict failures, namely to observe the reconstruction error of Autoencoders, in the context of detecting failures on the MetroPT2 dataset ____.
In ____ the authors evaluated multiple AE architectures trained on a portion of failure-free data.
The best model, a Wasserstein Autoencoder GAN architecture, achieved a perfect $F_1$ score, meaning no false alarms were signalled, and both failures in the MetroPT2 dataset were found.
In ____, the authors had a similar approach but found an LSTM-based Autoencoder to perform best.
In contrast to ____, who predicted 30-minute windows of data, the authors in ____ identified compressor cycles using the \texttt{COMP} sensor and predicted each of these cycles as anomalous or not.
In addition, each compressor cycle window was discretized into non-overlapping bins, on which summary statistics such as the mean, min and max values were calculated ____.

% Explainablity approaches
Since the goal of predictive maintenance and failure prediction is to be applied in real-world settings, the question of why these models learn (and predict) the way they do emerges naturally.
Thus, tools from Explainable Machine Learning are often used to investigate these deep learning models since they are not inherently interpretable ____.
One kind of explanations are feature attributions, where the explanation comes in the form of one real value per feature, indicating the \textit{importance} to the prediction, such as LIME ____ and SHAP ____ values.
One downside of these approaches is that it is often unclear and ambiguous how these values should be interpreted.
Additionally, most methods assume feature independence for faster computation, which can lead to problems when investigating time-series data, a data type often found in industry applications.
Thus, previous works on failure prediction on the MetroPT2 dataset favor rule-based explanations ____.
Rules have the advantage of being very easy to interpret, as long as they are not excessively long and as long as the antecedents (e.g., the features) are interpretable themselves.

The authors of ____ utilized an explainability layer including AMRules ____, an online rule-learning approach to model the Autoencoders reconstruction error based on the raw input values.
Specifically, Chebyshev sampling ____ is used to account for the fact that failures are rare events.
Afterwards, the rules that have the highest support for the failure duration are returned as an explanation for that failure.
We argue that their rule-based approach could be improved by addressing the following two issues:
\begin{itemize}
    \item The rules are trained on individual timesteps from raw data to predict the reconstruction error, i.e., each one-second vector of sensor values is an individual training example. This results in hard-to-interpret rules since the resolution is too fine-grained to find easy, general rules covering the failures.
    \item Choosing the reconstruction error as a target for rule learning results in overcomplicating the rules unnecessarily. We argue that choosing the output of the failure detection pipeline, i.e., $ \fail{} > 0.5 $ as a binary label, will lead to more straightforward to comprehend rules.
\end{itemize}