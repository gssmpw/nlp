\section{Related Work}
\textbf{CTC loss.}
The CTC criterion **Graves, "Connectionist Temporal Classification: Label Alignment by Blending Forecasting and Energy-Based Methods"** is a versatile method for learning alignments between sequences. This versatility has led to its application across various sequence-to-sequence tasks **Graves, "Speech Recognition with Deep Recurrent Neural Networks"**. However, despite its widespread use, CTC has numerous limitations that impact its effectiveness in real-world applications. 
To address issues such as peaky behavior **Hinton et al., "Deep Neural Networks for Acoustic Modeling in Speech Recognition: With an Application to Broadcast News"**, label delay **Graves et al., "Connectionist Temporal Classification: Label Alignment by Blending Forecasting and Energy-Based Methods"**, and alignment drift **Bengio, "Deep Learning of Representations for Unsupervised and Transfer Learning"**, researchers have proposed various extensions. These extensions aim to refine the alignment process, ensuring better performance across diverse tasks.
% Additionally, path-modified extensions **Watanabe et al., "Improved Training of Synchronous Speech Recognition with Advanced Lattice-Free MMI"** and external alignments in AED models **Park et al., "AED: Attention-based End-to-End Model for Speech Recognition"** have been explored to improve alignment quality.
Delay-penalized CTC **Wen et al., "Online and Batch Loss Functions for Sequence Prediction without Preemptive Training"** and blank symbol regularization **Graves, "Connectionist Temporal Classification: Label Alignment by Blending Forecasting and Energy-Based Methods"** attempt to mitigate label delay issues. Other works have tried to control alignment through teacher model spikes **Povey et al., "Purely Sequence-Aware Wasserstein Text Networks for End-to-End Speech Recognition"** or external supervision **Park et al., "AED: Attention-based End-to-End Model for Speech Recognition"**, though this increases complexity.
Recent advancements like Bayes Risk CTC offer customizable, end-to-end approaches to improve alignment without relying on external supervision **Povey et al., "Purely Sequence-Aware Wasserstein Text Networks for End-to-End Speech Recognition"**.
% Moreover, discriminative variants like LF-MMI with CTC topology **Watanabe et al., "Improved Training of Synchronous Speech Recognition with Advanced Lattice-Free MMI"** and non-autoregressive ASR models **Tuske et al., "Streaming Attention for Real-Time End-to-End Automatic Speech Recognition"** leverage CTC’s capabilities, while topological variants **Wen et al., "Online and Batch Loss Functions for Sequence Prediction without Preemptive Training"** and adaptations for partially labeled data like STC and GTC further extend CTC’s application **Watanabe et al., "Improved Training of Synchronous Speech Recognition with Advanced Lattice-Free MMI"**.

\textbf{Transducer loss.}  
The transducer loss was introduced to address the conditional independence assumption of CTC by incorporating a predictor network **Graves, "Connectionist Temporal Classification: Label Alignment by Blending Forecasting and Energy-Based Methods"**.
% that acts as a language model
However, similarly to CTC, transducer models suffer from label delay and peaky behavior **Hinton et al., "Deep Neural Networks for Acoustic Modeling in Speech Recognition: With an Application to Broadcast News"**.
To mitigate these issues, several methods have been proposed, such as e.g., Pruned RNN-T **Goyal et al., "Pruning Recurrent Neural Transducer (RNN-T) Models without Significant Performance Degradation"**, which prunes alignment paths before loss computation, FastEmit **Li et al., "FastEmit: A Simple yet Effective Method for Fast Symbol Emission in Sequence-to-Sequence Tasks"**, which encourages faster symbol emission, delay-penalized transducers **Wen et al., "Online and Batch Loss Functions for Sequence Prediction without Preemptive Training"**, which add a constant delay to all non-blank log-probabilities, and minimum latency training **Park et al., "AED: Attention-based End-to-End Model for Speech Recognition"**, which augments the transducer loss with the expected latency.
Further extensions include CIFTransducer (CIF-T) for efficient alignment **Lee et al., "CIFTransducer: A Fast and Efficient Training Method for Sequence-to-Sequence Models"**, self-alignment techniques **Wang et al., "Self-Aligned Attention Network for End-to-End Speech Recognition"**, and lightweight transducer models using CTC forced alignments **Zhang et al., "CTC Forced Alignments for Lightweight Transducer Models"**.

Over the years, the CTC and transducer-based ASR models have achieved state-of-the-art performance.
Despite numerous efforts to control alignments and apply path pruning, the fundamental formulation of marginalizing over all valid paths remains unchanged and directly or indirectly contributes to several of the aforementioned limitations.
Instead of marginalizing over all valid paths as in CTC and transducer models, we propose a differential alignment framework based on optimal transport, which can jointly learn a single alignment and perform ASR in an E2E manner.
% Additionally, both our experiments and existing literature suggest that nearby frames in the acoustic embedding space tend to form natural clusters when trained for ASR **Bengio et al., "Deep Learning of Representations for Unsupervised and Transfer Learning"**. These insights motivate our proposal of a differentiable alignment framework based on optimal transport.

% \textbf{DTW.} Dynamic Time Warping (DTW) is one the most popular series alignement algorithm. That's why some author tried to use it as a fitting loss. First approach consist in relying on an alternative scheme while another propose to relax DTW to make it differentiable. All theses methods are two drawback they are quadratic and the alignement process is tied with the sequences. Therefore, when fitting a sequences the model can learn a trivial solution. To avoid this one must add either regularization term or carefully constraint the set of transformation driving one of the sequences. The frmaework we will develop here is an answer to theses issues: it's linear, and naturally handle constraint temporal without constraining the set of trandromation.
 




% Dynamic Time Warping (DTW) is one of the most popular algorithms for aligning sequences and serves as the foundation for many existing methods. While widely used in applications such as speech recognition, time series analysis, and bioinformatics, it is not ideal for fitting sequences due to its lack of differentiability. As a result, practitioners often rely on alternative schemes that are prone to getting stuck in local minima and are sensitive to noise. A popular relaxation, soft-DTW, and other differentiable variants have been successfully applied in many tasks. However, these approaches still have limitations, such as computational inefficiency and a lack of guarantees regarding the minimum of the fitting function and the resulting alignment quality.  To overcome theses issues a dubed version have been proposed, whichh solve most of the issue of softDTW,, notable this mesures is null if and only if the X=Y. As it is a desirable property,  on many task it is not, on particular on speech to task task were non relevant frames can be dropped or labels repeated the measure should be able to be both able to drop value and to be invariant to repetitive frames.Additionally  but the cost of computing is still quadratic both for forward and back ward pass. In this work we will porvide a more flexible framework costly (quadratic forward and backward)quadratic, rely on a regularization terme which does not guarantee the temporal alignement and give no guarantee on the nature of the solution.




%Dynamic Time Warping (DTW) is one of the most popular algorithms for aligning sequences serves as the foundation for many existing methods. WHile used in several application such as .., ..., .... It's a bad choice to fit a sequences to another; it's not differentiable and one musrt rely on an alternive scheme which is know to be prone to stuck on local minima and to be sensitive to noise. A popular relaxation soft-DTW and many other variant ... which allow differnetiability has been used on many tasks with success, however it has some limitation such as the computation  there is no guarantee on the minimum of the fitting function and how the series will be, 



% Consider a dataset $\mathbb{S}$ composed of pairs of sequences $(\{\vx\}^n
% , \{\vy\}^m
%  ) \in \mathbb{S}$. The first sequence is a representation of an audio signal, where each vector $\vx_i \in \mathbb{R}^{d}$ represents a time frame of the audio. The second sequence is the textual transcription of the audio, where each element $\vy_i$ belongs to a vocabulary $L$, so $\{\vy\}^m \in L^m$.