    %%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables


% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\usepackage{stmaryrd}
\newcommand{\balpha}{\boldsymbol{\alpha}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\bgamma}{\boldsymbol{\gamma}}
\newcommand{\bpi}{\boldsymbol{\pi}}
\newcommand{\modified}[1]{\textcolor{blue}{#1}}
\usepackage{multirow}
\newcommand{\TODO}[1]{\textcolor{red}{TODO: #1}}
% \newcommand{\yk}[1]{\textcolor{blue}{Y.K.: #1}}
\newcommand{\sk}[1]{\textcolor{cyan}{#1}}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:

\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
%
\usepackage{soul} %to strike text 
\newcommand{\yac}[1]{\textcolor{red}{#1}}%color text for change

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{OTTC: A Differentiable Alignment Framework for Sequence-to-Sequence Tasks}

\begin{document}

\twocolumn[
\icmltitle{A Differentiable Alignment Framework for \\ Sequence-to-Sequence Modeling via Optimal Transport}
% \icmltitle{OTTC: A differentiable alignment approach to automatic speech recognition}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Yacouba Kaloga}{equal,yyy}
\icmlauthor{Shashi Kumar}{equal,yyy,comp}
\icmlauthor{Petr Motlicek}{yyy,sch}
\icmlauthor{Ina Kodrasi}{yyy}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Idiap Research Institute, Switzerland}
\icmlaffiliation{comp}{Ecole Polytechnique F\'ed\'erale de Lausanne, Switzerland}
\icmlaffiliation{sch}{Brno University of Technology, Czech Republic}

\icmlcorrespondingauthor{Yacouba Kaloga}{yacouba.kaloga@idiap.ch}
\icmlcorrespondingauthor{Shashi Kumar}{shashi.kumar@idiap.ch}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Optimal Transport, Sequence to Sequence, Alignment, Speech, ASR}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

% Accurate sequence-to-sequence alignment is critical for various applications, such as medical speech analysis or language learning tools relying on Automatic Speech Recognition (ASR). State-of-the-art end-to-end (E2E) ASR systems such as the Connectionist Temporal Classification (CTC) and transducer-based models maximize the marginal probability over all valid alignments within the probability lattice over the vocabulary during training which incentivizes the model to assign relatively larger frames to blank, resulting in peaky behavior.
% Moreover, research has shown that most alignments are highly improbable, with the model often concentrating on a limited set, undermining the purpose of considering all possible alignments.
\begin{abstract}
Accurate sequence-to-sequence (seq2seq) alignment is critical for applications like medical speech analysis and language learning tools relying on automatic speech recognition (ASR).
State-of-the-art end-to-end (E2E) ASR systems, such as the Connectionist Temporal Classification (CTC) and transducer-based models, suffer from peaky behavior and alignment inaccuracies.
In this paper, we propose a novel differentiable alignment framework based on one-dimensional optimal transport, enabling the model to learn a single alignment and perform ASR in an E2E manner.
We introduce a pseudo-metric, called Sequence Optimal Transport Distance (SOTD), over the sequence space and discuss its theoretical properties.
Based on the SOTD, we propose Optimal Temporal Transport Classification (OTTC) loss for ASR and contrast its behavior with CTC.
Experimental results on the TIMIT, AMI, and LibriSpeech datasets show that our method considerably improves alignment performance, though with a trade-off in ASR performance when compared to CTC.
We believe this work opens new avenues for seq2seq alignment research, providing a solid foundation for further exploration and development within the community.
% We believe this work opens up a potential new direction for research in ASR, offering a foundation for the community to further explore and build upon.
\end{abstract}



\section{Introduction}
\label{sec:intro}
Sequence-to-sequence (seq2seq) alignment is a fundamental challenge in automatic speech recognition (ASR) systems, where, beyond text prediction, precise alignment of text to the corresponding speech is crucial for many applications.
For example, in the medical domain, accurate alignment helps speech and language pathologists pinpoint specific speech segments for analyzing pathological cues, such as stuttering or voice disorders. In real-time subtitling, precise alignment ensures that subtitles are synchronized with spoken words, which is crucial for live broadcasts and streaming content. In language learning tools, ASR systems use alignment to provide feedback on pronunciation and fluency, allowing learners to compare their speech to target pronunciations. In these ASR-driven applications, while word error rate (WER) is an important performance metric, frame-level and word-level alignment accuracy are equally important for improving the system's applicability and responsiveness.

In the literature, two primary approaches to ASR have emerged, \textit{i.e.}, hybrid systems and end-to-end (E2E) models.
In hybrid approaches, a deep neural network-hidden Markov model (DNN-HMM) \cite{morgan1990continuous,bourlard2012connectionist, young1996review, povey2005discriminative, abdel2012applying, graves2013hybrid, dahl2011context} system is typically trained, where the DNN is optimized by minimizing cross-entropy loss on the forced alignments generated for each frame of audio embeddings from a hidden Markov model-Gaussian mixture model (HMM-GMM).
One notable disadvantage of the hybrid approach is that the model cannot be optimized in an E2E manner, which may result in suboptimal performance \cite{hannun2014deep}.
More recently, E2E models for ASR have become very popular due to their superior performance.
% {\textcolor{red}{I leave the paragraph you had written below and untouched, since I am not sure what I am doing at all. But, 1) I feel that your introduction is a bit too short and 2) it does not highlight enough what problem you are tackling... If I have understood correctly what it is that you are doing, I would change your paragraph as below: Note that I am very likely saying wrong things about how these models work, so please correct my mistakes if you go for this version.)}}
% {\color{blue}{There are three popular approaches for training an E2E model: (i) attention-based encoder-decoder (AED) models \cite{chan2015listen, whisper, watanabe2017hybrid, prabhavalkar2023end}, (ii) using Connectionist Temporal Classification (CTC) loss \cite{graves2006connectionist,graves2014towards}, and (iii) neural Transducer-based models \cite{graves2012sequence, kuang2022pruned, graves2013speech}.
% AED models use an encoder to convert the input audio sequence into a hidden representation. The decoder then generates the output text sequence by focusing on specific parts of the input through an attention mechanism. Although AED models generally offer an advantageous performance, the decoder is typically auto-regressive and requires access to entire acoustic frames, making it inapplicable to real-time streaming applications and to do teacher-student training with soft labels.
% Training AED models also requires comparatively large amounts of data, which can be prohibitive in low-resource setups.
% In contrast to AED models, CTC and transducer models predict output tokens without requiring explicit alignment, allowing them to handle variable-length input and output sequences more flexibly.
% While CTC predicts output tokens independently at each frame and relies on dynamic programming to compute possible alignments, transducers jointly model both token prediction and alignment.
% Both models are applicable to real-time streaming applications and rely on maximizing the marginal probability of the correct sequence of tokens (transcript) over all possible valid paths.
% However, recent research has shown that only a few paths contribute meaningfully to the final output~\cite{reference_here}.
% Unfortunately, there is no direct way to identify these prominent paths a priori in E2E models. Can you elaborate more on why this is bad? One or two more sentences, be it computational complexity or whatever other disadvantage there is by considering all valid paths in CTC and transducers that you are resolving with your OT?
% }}
There are three popular approaches for training an E2E model: (i) attention-based encoder-decoder (AED) models \cite{chan2015listen, whisper, watanabe2017hybrid, prabhavalkar2023end}, (ii) using Connectionist Temporal Classification (CTC) loss \cite{graves2006connectionist,graves2014towards}, and (iii) neural Transducer-based models \cite{graves2012sequence, kuang2022pruned, graves2013speech}.
AED models use an encoder to convert the input audio sequence into a hidden representation.
The decoder, typically auto-regressive, generates the output text sequence by attending to specific parts of the input through an attention mechanism, often referred to as soft alignment \cite{mtst} between the audio and text sequences.
This design, however, can make it challenging to obtain word-level timestamps and to do teacher-student training with soft labels.
Training AED models also requires a comparatively large amount of data, which can be prohibitive in low-resource setups.
In contrast to AED models, CTC and transducer-based models maximize the marginal probability of the correct sequence of tokens (transcript) over all possible valid alignments (paths), often referred to as hard alignment \cite{mtst}.
However, recent research has shown that only a few paths, which are dominated by blank labels, contribute meaningfully to the marginalization, leading to the well-known peaky behavior that can result in suboptimal ASR performance \cite{ctc-peaky}.
Unfortunately, it is not possible to directly identify these prominent paths, or those that do not disproportionately favor blank labels, in advance within E2E models.
% \TODO{computational complexity disadvantage}
This observation serves as the main motivation of our work.


% \begin{itemize}
%     \item describe asr briefly. In literature (historically), primary two direction: hybrid modeling and end-to-end.
%     \item briefly, in hybrid, we have AM, LM and lexicon and explicit forced-alignment generation using HMM-GMM, but can't optimize end to end (E2E)
%     \item in end to end, ctc, transducer, and encoder-decoder and shows better results than hybrid setup. Mention one big disadvantage of encoder-decoder (can't get word level timestamps, difficult to do soft KD, high data requirements). CTC and transducer are more natural way of doing ASR.
%     \item CTC and transducer marginalize over all possible paths due to the nature of their formulation. However, recent research shows that only few paths are prominent to contribute to marginal log-likelihood. This serves as one of our motivation.
%     \item Moreover, looking at acoustic embedding space of CTC/transducer based models, we naturally see some form of clustering happening. back up using papers.
%     \item in this work, we introduce a novel end-to-end differentiable alignment framework for ASR.
%     \item describe our approach some more
% \end{itemize}


In this paper, we introduce the Optimal Temporal Transport Classification (OTTC) loss function, a novel approach to ASR where our model jointly learns temporal sequence alignment and audio frame classification. OTTC is derived from the Sequence Optimal Transport Distance (SOTD) framework, which is also introduced in this paper and defines a pseudo-metric for finite-length sequences. At the core of this framework is a novel, parameterized, and differentiable alignment model based on one-dimensional optimal transport, offering both simplicity and efficiency, with linear time and space complexity relative to the largest sequence size. This design allows OTTC to be fast and scalable, maximizing the probability of exactly one path, which, as we demonstrate, helps avoid the peaky behavior commonly seen in CTC based models.
%
% we propose a novel, parameterized, and differentiable alignment model based on one-dimensional optimal transport, which is both straightforward and efficient, exhibiting linear complexity with respect to the largest sequence size. Building upon this foundation, we introduce a framework to construct a family of pseudo-metrics, named Sequence Optimal Transport Distance (SOTD), for finite-length sequences, and explore their properties. 
% Finally, leveraging these contributions, --- This approach is equivalent to maximizing the probability of exactly one path, enhancing vocabulary localization while maintaining linear complexity.
%
%
% In this paper, we propose a novel, parameterized, and differentiable alignment model based on one-dimensional optimal transport, which is both straightforward and efficient, exhibiting linear complexity with respect to the largest sequence size. Building upon this foundation, we introduce a framework to construct a family of pseudo-metrics, named Sequence Optimal Transport Distance (SOTD), for finite-length sequences, and explore their properties. 
% Finally, leveraging these contributions, we present a new approach to ASR, where our model jointly learns temporal sequence alignment and classification via the Optimal Temporal Transport Classification (OTTC) loss function. This approach is equivalent to maximizing the probability of exactly one path, enhancing vocabulary localization while maintaining linear complexity.
%
%
%

To summarize, our contributions are the following:
\begin{itemize}
\item We propose a novel, parameterized, and differentiable sequence-to-sequence alignment model with linear complexity both in time and space.
\item We introduce a new framework, \textit{i.e.}, SOTD, to compare finite-length sequences, examining its theoretical properties and providing guarantees on the existence and characteristics of a minimum.
\item We derive a new loss function, \textit{i.e.}, OTTC, specifically designed for ASR tasks.
\item Finally, we conduct proof-of-concept experiments on the TIMIT~\cite{garofolo1993timit}, AMI~\cite{ami}, and Librispeech~\cite{panayotov2015librispeech} datasets, demonstrating that our method mitigates the peaky beahavior, improves alignment performance, and achieves promising results in E2E ASR.
% \yac{\st{Finally, we conduct proof-of-concept experiments on the English Librispeech}} \cite{panayotov2015librispeech} \yac{\st{and AMI}} \cite{ami} \yac{\st{datasets, demonstrating that our method achieves promising performance in E2E ASR while addressing the peaky behavior issues.}}%, labels delay and alignement drift.
\end{itemize}


% To summarize, our contributions are the following:
% \begin{enumerate}
% \item We propose a novel, parameterized, and differentiable sequences-to-sequences alignment model linear in complexity.
% \item We introduce a new framework, SOTD to compare sequences of finite length, and we investigate theoritical properties and provide guarantee.
% \item We derive a new loss OTTC to perm ASR tasks.
% \end{enumerate}


% In summary, the main contributions of this paper are two-fold~:
% \begin{itemize}
%     \item We propose a novel, parameterized, and differentiable sequences-to-sequences alignment model based on 1d optimal transport, offering straightforward implementation and linear complexity relative to the largest sequence size.%We propose a parameterized and differentiable alignment model based on 1d optimal transport, which, to the best of our knowledge, is novel in the literature. This model is both straightforward and highly efficient, with linear complexity relative to the size of the largest sequence. 

%     \item Based on these alignment model, we introduce a novel framework to construct a family of pseudo-metrics, Sequences Optimal Transport Distance (SOTD), for sequences of finite length. We explore the properties of these metrics and their invariances, with the potential to benefit various sequence-to-sequence tasks. This framework offers a potential avenue for new approaches to tackle challenges in this area.

%     \item Finally building on the previous contributions, we propose a new approach to Automatic Speech Recognition (ASR). Our method jointly learns to align temporal sequences and classify them through a novel loss function, Optimal Temporal Transport Classification (OTTC), derived from our framework. This loss function is characterized by linear complexity and enhances the ability to accurately locate vocabulary within audio.
% \end{itemize}

\section{Related Work}
\textbf{CTC loss.}
The CTC criterion~\cite{graves2006connectionist} is a versatile method for learning alignments between sequences. This versatility has led to its application across various sequence-to-sequence tasks~\cite{st1,reorder,mtst,mt1,orc,gesture}. However, despite its widespread use, CTC has numerous limitations that impact its effectiveness in real-world applications. 
To address issues such as peaky behavior~\cite{ctc-peaky}, label delay~\cite{tian2022bayes}, and alignment drift~\cite{sak2015learning}, researchers have proposed various extensions. These extensions aim to refine the alignment process, ensuring better performance across diverse tasks.
% Additionally, path-modified extensions \cite{mahadeokar2021alignment} and external alignments in AED models \cite{kim2021reducing} have been explored to improve alignment quality.
Delay-penalized CTC \cite{yao2023delay} and blank symbol regularization \cite{yang2023blank,zhao2022investigating,bluche2015framewise} attempt to mitigate label delay issues. Other works have tried to control alignment through teacher model spikes \cite{align1,align2} or external supervision \cite{externalali,7404851,9003863}, though this increases complexity.
Recent advancements like Bayes Risk CTC offer customizable, end-to-end approaches to improve alignment without relying on external supervision \cite{tian2022bayes}.
% Moreover, discriminative variants like LF-MMI with CTC topology \cite{hadian2018end} and non-autoregressive ASR models \cite{higuchi2020mask} leverage CTC’s capabilities, while topological variants \cite{zhao2022investigating, laptev22_interspeech} and adaptations for partially labeled data like STC and GTC further extend CTC’s application \cite{pratap2022star, moritz2021semi}.

\textbf{Transducer loss.}  
The transducer loss was introduced to address the conditional independence assumption of CTC by incorporating a predictor network \cite{graves2012sequence}.
% that acts as a language model
However, similarly to CTC, transducer models suffer from label delay and peaky behavior~\cite{yu2021fastemit}.
To mitigate these issues, several methods have been proposed, such as e.g., Pruned RNN-T \cite{kuang2022pruned}, which prunes alignment paths before loss computation, FastEmit \cite{yu2021fastemit}, which encourages faster symbol emission, delay-penalized transducers \cite{kang2023delay}, which add a constant delay to all non-blank log-probabilities, and minimum latency training \cite{shinohara2022minimum}, which augments the transducer loss with the expected latency.
Further extensions include CIFTransducer (CIF-T) for efficient alignment \cite{zhang2023say}, self-alignment techniques \cite{kim2021reducing}, and lightweight transducer models using CTC forced alignments \cite{wan24_interspeech}.

Over the years, the CTC and transducer-based ASR models have achieved state-of-the-art performance.
Despite numerous efforts to control alignments and apply path pruning, the fundamental formulation of marginalizing over all valid paths remains unchanged and directly or indirectly contributes to several of the aforementioned limitations.
Instead of marginalizing over all valid paths as in CTC and transducer models, we propose a differential alignment framework based on optimal transport, which can jointly learn a single alignment and perform ASR in an E2E manner.
% Additionally, both our experiments and existing literature suggest that nearby frames in the acoustic embedding space tend to form natural clusters when trained for ASR \cite{}. These insights motivate our proposal of a differentiable alignment framework based on optimal transport.

% \textbf{DTW.} Dynamic Time Warping (DTW) is one the most popular series alignement algorithm. That's why some author tried to use it as a fitting loss. First approach consist in relying on an alternative scheme while another propose to relax DTW to make it differentiable. All theses methods are two drawback they are quadratic and the alignement process is tied with the sequences. Therefore, when fitting a sequences the model can learn a trivial solution. To avoid this one must add either regularization term or carefully constraint the set of transformation driving one of the sequences. The frmaework we will develop here is an answer to theses issues: it's linear, and naturally handle constraint temporal without constraining the set of trandromation.
 




% Dynamic Time Warping (DTW) is one of the most popular algorithms for aligning sequences and serves as the foundation for many existing methods. While widely used in applications such as speech recognition, time series analysis, and bioinformatics, it is not ideal for fitting sequences due to its lack of differentiability. As a result, practitioners often rely on alternative schemes that are prone to getting stuck in local minima and are sensitive to noise. A popular relaxation, soft-DTW, and other differentiable variants have been successfully applied in many tasks. However, these approaches still have limitations, such as computational inefficiency and a lack of guarantees regarding the minimum of the fitting function and the resulting alignment quality.  To overcome theses issues a dubed version have been proposed, whichh solve most of the issue of softDTW,, notable this mesures is null if and only if the X=Y. As it is a desirable property,  on many task it is not, on particular on speech to task task were non relevant frames can be dropped or labels repeated the measure should be able to be both able to drop value and to be invariant to repetitive frames.Additionally  but the cost of computing is still quadratic both for forward and back ward pass. In this work we will porvide a more flexible framework costly (quadratic forward and backward)quadratic, rely on a regularization terme which does not guarantee the temporal alignement and give no guarantee on the nature of the solution.




%Dynamic Time Warping (DTW) is one of the most popular algorithms for aligning sequences serves as the foundation for many existing methods. WHile used in several application such as .., ..., .... It's a bad choice to fit a sequences to another; it's not differentiable and one musrt rely on an alternive scheme which is know to be prone to stuck on local minima and to be sensitive to noise. A popular relaxation soft-DTW and many other variant ... which allow differnetiability has been used on many tasks with success, however it has some limitation such as the computation  there is no guarantee on the minimum of the fitting function and how the series will be, 



% Consider a dataset $\mathbb{S}$ composed of pairs of sequences $(\{\vx\}^n
% , \{\vy\}^m
%  ) \in \mathbb{S}$. The first sequence is a representation of an audio signal, where each vector $\vx_i \in \mathbb{R}^{d}$ represents a time frame of the audio. The second sequence is the textual transcription of the audio, where each element $\vy_i$ belongs to a vocabulary $L$, so $\{\vy\}^m \in L^m$. 

\section{Problem Formulation}

% Let consider $\mathcal{U}_{\leq N}^d = \bigcup_{n \leq N } \mathcal{U}_n^d$, the set of all $d$-dimensional vector sequences of length at most $N$. Let's consider a distribution $\mathcal{D}_{\mathcal{U}_{\leq N }^d\times\mathcal{U}_{\leq N}^d}$ and a pairs of sequences $ (\{\vx_i\}_{i=1}^{n}
%  , \{\vy_i\}_{i=1}^{m}
%   ) $ (that we will denote $\{\vx\}_n$ and 
%  $\{\vy\}_m$ for simplicity) drawn from $\mathcal{D}_{\mathcal{U}_{\leq N}^d \times\mathcal{U}_{\leq N}^d }$. In this work, the first sequence $\{\vx\}_n$ represents an audio signal, where each vector $\vx_i \in \mathbb{R}^{d}$ corresponds to a time frame in the audio. The second sequence $\{\vy\}_m$ is the textual transcription of the audio, where each element $\vy_i$ belongs to a predefined vocabulary $L = \{l_1,\dots,l_{|L|}\}$, such that $\{\vy\}^m \in L^m$. The goal is to train a classifier that can accurately predict the target sequence $\{\vy\}_m$ from the input sequence $\{\vx\}_n$, enabling it to generalize and transcribe unseen audio sequences. Typically, $n \neq m$, creating challenges for accurate transcription as there is no natural alignment between the two sequences. The framework that we are about to introduce to tackle this problem extends beyond transcription and offers greater flexibility: the elements $\vy_i$ in the target sequence are not restricted to a discrete vocabulary but could also belong to a continuous space, expanding its applicability across a wide range of sequence-to-sequence tasks.
 We define $\mathcal{U}_{\leq N}^d = \bigcup_{n \leq N } \mathcal{U}_n^d$ to be the set of all $d$-dimensional vector sequences of length at most $N$. 
 Let us consider a distribution $\mathcal{D}_{\mathcal{U}_{\leq N }^d\times\mathcal{U}_{\leq N}^d}$ and pairs of sequences $ (\{\vx_i\}_{i=1}^{n}
 , \{\vy_i\}_{i=1}^{m}
  ) $ of length $n$ and $m$ drawn from $\mathcal{D}_{\mathcal{U}_{\leq N}^d \times\mathcal{U}_{\leq N}^d }$. 
  For notational simplicity, the sequences of the pairs $ (\{\vx_i\}_{i=1}^{n}
 , \{\vy_i\}_{i=1}^{m}
  ) $ will be respectively denoted by $\{\vx\}_n$ and 
 $\{\vy\}_m$ in the following.
  The goal in sequence-to-sequence tasks is to train a classifier that can accurately predict the target sequence $\{\vy\}_m$ from the input sequence $\{\vx\}_n$, enabling it to generalize to unseen examples. Typically, $n \neq m$, creating challenges for accurate prediction as there is no natural alignment between the two sequences. 
In this paper, we introduce a framework to address this class of problems, applying it specifically to the ASR domain. In this context, the first sequence $\{\vx\}_n$ represents an audio signal, where each vector $\vx_i \in \mathbb{R}^{d}$ corresponds to a time frame in the acoustic embedding space. The second sequence $\{\vy\}_m$ is the textual transcription of the audio, where each element $\vy_i$ belongs to a predefined vocabulary $L = \{l_1, \dots, l_{|L|}\}$, such that $\{\vy\}_m \in L^m$, where $L^m$ denotes the set of all $m$-length sequences formed from the vocabulary $L$. 
%The second sequence $\{\vy\}_m$ is the textual transcription of the audio, where each element $\vy_i$ belongs to a predefined vocabulary $L = \{l_1,\dots,l_{|L|}\}$, such that $\{\vy\}^m \in L^m$, were $L^m$ denote the space set of $m$-length sequences made from the vocabulary $L$.
%{\color{red}{Shouldn't this last $\{\vy\}_m$ be $\{\vy\}_m$? Further, $L^m$} is undefined or?}
%The goal is to train a classifier that can accurately predict the target sequence $\{\vy\}_m$ from the input sequence $\{\vx\}_n$, enabling it to generalize and transcribe unseen audio sequences.  The framework that we are about to introduce to tackle this problem extends beyond transcription and offers greater flexibility: the elements $\vy_i$ in the target sequence are not restricted to a discrete vocabulary but could also belong to a continuous space, expanding its applicability across a wide range of sequence-to-sequence tasks.
 
 %Without loss of generality (as we can always introduce void labels), we will assume from now on that any consecutive labels are distinct, \textit{i.e.}, $\vy_i \neq \vy_{i+1}$ for all $i$.


\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{fig1ottc5.pdf}
    \vspace{-0.4cm}
\caption{\textbf{\textit{Example of an alignment between embeddings of frames and target sequence.}} Red bullets represent the elements of the target sequence $\{\vy\}_m$, while the blue bullets indicate the frame embeddings $\{\vx\}_n$. In OTTC, the alignment guides the prediction model $F$ in determining which frames should map to which labels. Additionally, the alignment model has the flexibility to leave some frames unaligned, as represented by the blue-and-white bullets, allowing those frames to be dropped during inference.}
    \label{fig:dma}
    % \vspace{-0.4cm}
\end{figure}



\section{Optimal Temporal Transport Classification}
The core idea is to model the alignment between two sequences as a mapping to be learned along with the frame labels (see Figure~\ref{fig:dma}).
%{\color{red}{Unlike audio frames, where explicit target or ground truth labels are provided, such alignments are not directly available.}} 
As the classification of audio frames improves, inferring the correct alignment becomes easier. Conversely, accurate alignments also improve frame classification. This mutual reinforcement between alignment and classification highlights the benefit of addressing both tasks simultaneously, contrasting with traditional hybrid models that treat them as separate tasks~\cite{morgan1990continuous}. To achieve this, we propose the Sequence Optimal Transport Distance (SOTD), a framework for constructing pseudo-metrics over the sequence space $\mathcal{U}_{\leq N}^d$, based on a differentiable, parameterized model that learns to align sequences. Using this framework, we derive the Optimal Temporal Transport Classification (OTTC) loss, which allows the model to learn both the alignment and the classification in a unified manner.% To achieve this, we first define an alignment model, which serves as the foundation for constructing the SOTD pseudo-metrics framework. After investigating the theoretical properties and guarantees of this framework, we apply it to develop OTTC, which is used to perform ASR.

\textbf{Notation.} We denote $\llbracket 1,n\rrbracket = \{1,\dots,n\}$.
% To achieve this, we propose a differentiable, parameterized model that learns to align sequences. This model forms the basis of our Sequence Optimal Transport Distance (SOTD), a framework for constructing pseudo-metrics over the sequence space $\mathcal{U}_{\leq N}^d$. Using this framework, we derive the Optimal Temporal Transport Classification (OTTC) loss, which allows the model to learn both alignment and classification in a unified manner.


% To achieve this, we propose a differentiable, parameterized model that aligns sequences in a way that is learned jointly with the classification process. This model serves as the foundation for our Sequence Optimal Transport Distance (SOTD), a framework to build pseudo-metric over the sequence space $\mathcal{U}_{\leq N}^d$. Leveraging this distance, we then introduce the Optimal Temporal Transport Classification (OTTC) loss, which enables the model to simultaneously learn both alignment and classification in a unified framework.
% To achieve this, we introduce a differentiable, parameterized model capable of aligning any sequence to another. Building on this, we develop a framework of pseudo-metrics Sequences Optimal Transport Distance (SOTD) over the sequence space $\mathcal{U}_{\leq N}^d$. Finally, we apply this framework to create the Optimal Temporal Transport Classification loss (OTTC) , enabling the joint learning of alignment and classification.

%model that unifies both processes.
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{fig2ottc5-2.pdf}
    \vspace{-0.4cm}
    \caption{\textbf{\textit{Discrete monotonic alignment as 1D OT solution.}} %The red bullets represent the elements of the target sequence $\{\vy\}_m$, while the blue bullets indicate the frame embeddings $\{\vx\}_n$.
    A discrete monotonic alignment represents a temporal alignment between two sequences (target on top, frame embeddings on bottom). It can be modeled by $\boldsymbol{\gamma}_n^{m,\boldsymbol{\beta}}$, as illustrated in the graph. The thickness of the links reflects the amount of mass $\boldsymbol{\gamma}_n^{m,\boldsymbol{\beta}}(\balpha)_{i,j}$ transported, with thicker links corresponding to higher mass. }
    \label{fig:dma2}
\end{figure}


\subsection{Preliminaries}
\textbf{Definition 1.} \textit{\textbf{Discrete monotonic alignment}. Given two sequences $\{\mathbf{x}\}_n$ and $\{\mathbf{y}\}_m$, and a set of index pairs $\mathbf{A} \subset \llbracket 1,n \rrbracket \times \llbracket 1,m\rrbracket$ representing their alignment, we say that $\mathbf{A}$ is a discrete monotonic alignment between the two sequences if:}

\begin{itemize}
    \item \textbf{Complete alignment of $\{\mathbf{y}\}_m$:} Every element of $\{\mathbf{y}\}_m$ is aligned, \textit{i.e.}, 
    \[
    \forall j \in \llbracket 1,m \rrbracket, \exists k \in \llbracket 1,n \rrbracket, \ (k,j) \in \mathbf{A}.
    \]
    
    \item \textbf{Monotonicity:} The alignment is monotonic, meaning that for all $(i,j), (k,l) \in \mathbf{A}$
    \[
    i \leq k \ \Rightarrow \ j \leq l. \quad %\text{or} \quad i \geq k \ \text{and} \ j \geq l.
    \]
\end{itemize}

Discrete monotonic alignments model the relationship between temporal sequences, such as those in ASR, by determining which frame should predict which target. The conditions imposed on the target sequence $\{\mathbf{y}\}_m$ ensure that no target element is omitted, while the absence of similar constraints on the source sequence $\{x\}_n$ allows certain audio frames to be considered irrelevant and dropped (see Figure~\ref{fig:dma2}). The monotonicity condition preserves the temporal order, ensuring the sequential structure is maintained. In the following sections, we will develop a model capable of differentiating within the space of discrete monotonic alignments.


\subsection{Differentiable Temporal Alignment with Optimal Transport}
\label{subsec:diff-alignment-ot}

In the following, we introduce 1D OT and define our alignment model.
Consider the 1D discrete distributions $\mu[\balpha, n]$ and $\nu[\bbeta,m]$ expressed as superpositions of $\delta$ measures, \textit{i.e.}, a distribution that is zero everywhere except at a single point, where it integrates to 1:
\begin{equation}
        \mu[\balpha, n] = \sum_{i=1}^n \alpha_i \delta_{i}
  \text{\:\:\:\:\:and\:\:\:\:\:} \nu[\bbeta,m] = \sum_{i=1}^m \beta_i \delta_{i}.
\end{equation}
{{The bins of $\mu[\balpha,n]$ and $\nu[\bbeta,m]$ are $\llbracket 1,n \rrbracket$ and $\llbracket 1,m \rrbracket$, respectively, whereas the weights $\alpha_i$ and $\beta_i$}}
%The bins of $\mu[\balpha,n]$ and $\nu[\bbeta,n]$ respectively $\{1,\dots,n\}$ and $\{1,\dots,m\}$; 
are components of the vectors $\balpha \in \Delta^n$ and $\bbeta \in \Delta^m$, with $\Delta^n$ the simplex set defined as $\Delta^n  =  \{ \mathbf{v}  \in \R^n  | {0 \leq  v_i \leq  1, \sum_{i=1}^n v _ {i} = 1 } \}
 \subset    \R  ^ {n}$. 
% \begin{equation}
% \Delta^n  =  \{ \mathbf{v}  \in \R^n  | {0 \leq  v_i \leq  1, \sum_{i=1}^n v _ {i} = 1 } \}
%  \subset    \R  ^ {n}. \end{equation}
OT theory provides an elegant and versatile framework for computing distances between distributions such as $\mu[\balpha, n]$ and $\nu[\bbeta, m]$, depending on the choice of the cost function~\cite{peyre2019computational} (chapter 2.4). One such distance is the 2-Wasserstein distance $\mathcal{W}_2$, which measures the minimal cost of transporting the weight of one distribution to match the other. This distance is defined as
% Optimal transport theory provides an elegant framework for calculating the distance between distributions like $\mu[\balpha, n]$ and $\nu[\bbeta, m]$. This distance is the minimal cost required to move the weight of one distribution to match the other. Known as the Wasserstein distance $\mathcal{W}_2$, it is defined as:

\begin{equation}
    \mathcal{W}_2(\mu[\balpha,n],\nu[\bbeta, m] ) = \min_{\bgamma \in \Gamma^{\balpha,\bbeta}}  \sum_{i,j=1}^{n,m}\gamma_{i,j} |\!| i - j|\!|_2^2,
\end{equation}
were $|\!| i - j|\!|_2^2$ is the cost of moving weight from bin $i$ {{to}} bin $j$ and $\gamma_{i,j}$ is the amount of mass moved from $i$ to $j$. The optimal coupling matrix $\bgamma^*$ is searched within the set of valid couplings $\Gamma^{\balpha,\bbeta}$ defined as 
\begin{equation}
    \Gamma^{\balpha,\bbeta} = \{\bgamma \in \R_{+}^{n \times m}  | \bgamma \mathbf{1}_m =  \balpha  \:\: \text{and} \:\:  \bgamma^T \mathbf{1}_n  = \bbeta \}.
\end{equation}

This constraint ensures that the coupling conserves mass, accurately redistributing all weights between the bins. A key property of optimal transport in 1D is its monotonicity~\cite{Peyr2019NumericalOT}. Specifically, if there is mass transfer between bins $i$ and $j$ (\textit{i.e.}, $\gamma_{i,j}^* > 0$) and similarly between bins $k$ and $l$ (\textit{i.e.}, $\gamma_{k,l}^* > 0$), then it must hold that $i \leq k \Rightarrow j \leq l$. Consequently, when $\boldsymbol{\beta}$ has no zero components -- meaning that every bin from $\nu$ is reached by the transport -- the set $\{(i,j) \in [\!|1,n|\!] \times [\!|1,m|\!] \ | \ \bgamma_{i,j}^{*} > 0\}$ satisfies the conditions of Definition 1, thereby forming a discrete monotonic alignment. This demonstrates that the optimal coupling can effectively model such alignments (see Figure~\ref{fig:dma2}).

 

% effectively creating an alignment that satisfies temporal constraints by construction. %Such coupling will be referred as monotonic in this paper.

\textbf{Note:} In the 1D case, the solution $\bgamma^{*}$ is unique and depends only on the number of distinct bins and their weights, not their specific values. Thus, the choice of $\llbracket 1,n \rrbracket$ and $\llbracket 1,m \rrbracket$ as bins is arbitrary~\cite{Peyr2019NumericalOT}.

\textbf{Parameterized and differentiable temporal alignment.} Given any sequences length $n$ and $m$ and $\bbeta$ with no zero components, we can define the alignment function $\bgamma_n^{m,\bbeta}$
\begin{align}
    \bgamma_n^{m,\bbeta} : \:\:&\Delta^n \to \Gamma^{*,\bbeta}[n] \notag \\
     &\balpha \mapsto    \bgamma^{*} = \argmin_{\bgamma\in \Gamma}  \mathcal{W}(\mu[\balpha,n],\nu[\bbeta, m] ),%\sum_{i,j=1}^{n,m}\gamma_{i,j} \dot |\!| i - j|\!|_2^2,
     \label{eq:diffali}
 \end{align}
where $\Gamma^{*,\bbeta}[n]$ is the space of all 1D transport solutions 
between $\mu[\balpha,n]$ and $\nu[\bbeta,m]$ for any $\balpha$. {Differently from $\boldsymbol{\beta}$}, $\balpha$ may have zero components, giving the model the flexibility to suppress certain bins, which acts similarly to a blank token in traditional models.
In the context of ASR, $\balpha$ and $\bbeta$ can be referred to as OT weights and label weights, respectively.
%The non-zero components condition on $\bbeta$ ensure that we are aligning with an $m$-length sequence and not a shorter one, as each bin with a zero component effectively reduces the size of the distribution. Conversely, we allow $\balpha$ to have zero components, giving the model the flexibility to suppress certain bins, which will acts similarly to a blank token in traditional models. 

\textbf{Lemma 1:} \textit{The function $\balpha \mapsto \bgamma_n^{m,\bbeta}(\balpha)$ is bijective from $\Delta^n $ to $\Gamma^{*,\bbeta}[n]$ .}

\textit{Proof.} The proof can be found in Appendix~\ref{ax:properties_l1}.

\textbf{Proposition 1}. \textit{\textbf{Discrete Monotonic Alignment Approximation Equivalence.}  For any $\bbeta$ that satisfies the condition above, any discrete set of alignments $\boldsymbol{A} \subset [\!|1,n|\!] \times [\!|1,m|\!]$ between sequences of lengths $n$ and $m$ can be modeled by $\bgamma_n^{m, \bbeta}$ through the appropriate selection of $\balpha$, \textit{i.e.},} 

\begin{equation}
  \forall \mathbf{A},\exists \balpha \in \Delta^n , (i,j) \in \boldsymbol{A} \Longleftrightarrow  \bgamma_n^{m,\bbeta}(\balpha)_{i,j} >0.
\end{equation}

\textit{Proof.} The proof can be found in Appendix~\ref{ax:properties_p1}.


Thus, we have defined a family of alignment functions $\bgamma_n^{m, \boldsymbol{\beta}}$ that are capable of modeling any discrete monotonic alignment, which can be chosen or adapted based on the specific task at hand. The computational cost of these alignment functions is low, as the bins are already sorted, eliminating the need for additional sorting. This results in linear complexity $O(\max(n, m))$ depending on the length of the longest sequence (see Algorithm~\ref{ax:algo1} in the Appendix). Furthermore, these alignments are differentiable, with $\bgamma_n^{m, \bbeta}(\balpha)_{i,j}$ explicitly expressed in terms of $\balpha$ and $\bbeta$, allowing direct computation of the derivative $\frac{d \bgamma_n^{m,\bbeta}(\balpha)_{i,j}}{d \balpha}$ via its analytical form.

% Thus, we have a family of alignment functions, $\bgamma_n^{m, \boldsymbol{\beta}}$, that can model any discrete monotonic alignment and can be chosen or adapted based on the specific task at hand. 
% The computational cost of these alignment functions is low. Since the bins are already sorted, there is no need for additional sorting, resulting in linear complexity, $O(\max(n, m))$, based on the length of the longest sequence. The algorithm can be found in Appendix~\ref{ax:algo1}. Furthermore these alignement are differentiable, the algorithm shown that $\bgamma_n^{m, \bbeta}(\balpha)_{i,j}$, indeed can be explicitly expressed in terms of $\balpha$ (and $\bbeta$), allowing for the direct computation of the derivative $\frac{d \bgamma_n^{m,\bbeta}(\balpha)_{i,j}}{d \balpha}$ using its analytical form. 


\subsubsection{Sequences-to-Sequences Distance}
\label{sec:s2sd}
Here, we use the previously designed alignment functions to build a pseudo-metric over sets of sequences $\mathcal{U}_{\leq N}^d $. %This pseudo-metric will specifically not distinguish between a sequence and a similar sequenits copies with repetitive elements, as it convenient for ASR task.

\textbf{Definition 1.} \textbf{\textit{Sequences Optimal Transport Distance (SOTD).}} \textit{Consider an $n$-length sequence $\{\vx\}_n \in \mathcal{U}_{\leq N}^d$, an $m$-length sequence $\{\vy\}_m \in \mathcal{U}_{\leq N}^d$, $p = \max(n,m)$, and $q = \min(n,m)$. Let $C : \mathbb{R}^d \times \mathbb{R}^d \to \R_{+}$%[0, +\infty[$
, be a differentiable positive cost function. Considering $r\in \mathbb{N}^{*}$ and a family of vectors $\{\bbeta\}_{N} = \{\bbeta_1 \in \Delta^1,\bbeta_2 \in \Delta^2, \dots,\bbeta_N \in \Delta^N\}$ without zero components, {{we define the SOTD}} $\mathcal{S}_{r}$ as}

\begin{small}
\begin{align}
   \mathcal{S}_{r}(\{\vx\}_n,\{\vy\}_m)  = \min_{\balpha \in \Delta^n} \Big( \sum_{i,j=1}^{n,m} \bgamma_p^{q,\bbeta_q}(\balpha)_{i,j} \cdot C(\vx_i,\vy_j)^r \Big) ^{1/r}.
   \label{eq:SOTD}
 \end{align}
\end{small}



Note that $\bbeta_q$ obviously depends on $q$, but could a priori depend on $\{\vx\}_n$ and $\{\vy\}_m$. To simplify the notation, we only denote its dependence on $q$. However, all the results in this section remain valid under such dependencies, as long as $\bbeta_q$ components never becomes zero.

\textbf{Proposition 2.} \textit{\textbf{Validity of the definition.} SOTD is well-defined, meaning that a solution to the problem always exists, although it may not be unique.}

\textit{Proof.} The proof and the discussion about the non-unicity is conducted in Appendix~\ref{ax:properties_p2}.




% \textbf{Proposition 3.} DTW IF TIME.

\textbf{Proposition 3.} \textit{\textbf{SOTD is a Pseudo-Metric.}} \textit{If the cost matrix $C$ is a metric on $\mathbb{R}^d$, then $\mathcal{S}_{r}$ defines a pseudo-metric over the space sequences with at most $N$ elements $\mathcal{U}_{\leq N}^d$.}

% \begin{itemize}
%     \item \textit{\textbf{Pseudo-seperation.}} $\mathcal{S}_{r}(\{\vx\}_n,\{\vx\}_n) = 0$.
%     \item \textit{\textbf{Symmetry.}} $\mathcal{S}_{r}(\{\vx\}_n,\{\vy\}_m) = \mathcal{S}_{r}(\{\vy\}_m,\{\vx\}_n) $.
%     \item \textit{\textbf{Triangular inequality.}} $\mathcal{S}_{r}(\{\vx\}_n,\{\vz\}_o) \leq \mathcal{S}_{r}(\{\vx\}_n,\{\vy\}_m) + \mathcal{S}_{r}(\{\vy\}_m,\{\vz\}_o)$.
% \end{itemize}

\textit{Proof.} The proof can be found in Appendix~\ref{ax:properties_p3}.

Since $\mathcal{S}_{r}$ is a pseudo-metric, there are sequences $\{\vx\}_n \neq\{\vy\}_m$ such that $\mathcal{S}_{r}(\{\vx\}_n,\{\vy\}_m) = 0$. The following proposition describes the conditions when this occurs.


\textbf{Proposition 4.} \textbf{\textit{Non-Separation Condition.}} \textit{Let $\mathcal{A}$ be the sequence aggregation operator which removes consecutive duplicates, \textit{i.e.}, $\mathcal{A}(\{\dots, \vx,\vx, \dots\}) = \{\dots, \vx, \dots\}$. Let $\mathcal{P}_{\balpha}$ be the sequence pruning operator which removes any element $\vx_i$ from sequences corresponding to an $\alpha_i = 0$, \textit{i.e.}, $\mathcal{P}_{\alpha}(\{\dots,\vx_{i-1}, \vx_i,\vx_{i+1}, \dots\}) = \{\dots,\vx_{i-1}, \vx_{i+1}, \dots\}$ iff $\alpha_i = 0$. 
Further, let us consider $\{\vx\}_n$ and $\{\vy\}_m$  such that $\{\vx\}_n \neq\{\vy\}_m$. 
Without loss of generality, we assume that $n \geq m $. Then}
\begin{equation}
\small
 \mathcal{S}_{r}(\{\vx\}_n,\{\vy\}_m) = 0
\text{\:\:iff\:\:} \mathcal{A}(\mathcal{P}_{\alpha^{*}}(\{\vx\}_n)) = \mathcal{A}(\{\vy\}_m),
\end{equation}
    % \left\{
    % \begin{array}{ll}
    %     \mathcal{A}(\{\vx\}_n) = \mathcal{A}(\{\vy\}_m) \\
    %    \mathcal{A}(\mathcal{P}_{\alpha^{*}}(\{\vx\}_n)) = \mathcal{A}(\{\vy\}_m)
    % \end{array} 
    % \right.
% \begin{equation}
% \mathcal{A}(\{\vx\}_n) = \mathcal{A}(\{\vy\}_m)     
% \implies \mathcal{S}_{r}(\{\vx\}_n,\{\vy\}_m) = 0  
% \end{equation}
\textit{where $\balpha^{*}$ is a minimum for which  $\mathcal{S}_{r}(\{\vx\}_n,\{\vy\}_m) = 0$. It should be noted that this condition holds also when $C$ is neither symmetric nor satisfies the triangular inequality, but is separated (like the cross-entropy for example).}

\textit{Proof.} See Appendix~\ref{ax:properties_p4}.

The consequence of the previous proposition is that we can learn a transformation through gradient descent using a trainable network $F$  which maps input sequences \(\{\vx\}_n \) to target sequences \( \{\vy\}_m \) (with \( n \geq m \)) by solving the optimization problem:

\begin{align}
    \min_{F} \mathcal{S}_{r}(F(\{\vx\}_n), \{\vy\}_m)
    % &= \\ \min_{F, \balpha \in \Delta^n} \Bigg( \sum_{i,j=1}^{n,m} \bgamma_p^{q,\bbeta_q}(\balpha)_{i,j} \cdot
    % & \quad C(F(\{\vx\}_n)_i, \vy_j)^r \Bigg)^{1/r}.
\end{align}



We are then guaranteed that a solution 
$F^{*}\{\vx\}_n$ allows us to recover the sequence $\mathcal{A}(\{\vy\}_m)$. In cases where retrieving repeated elements in 
$\{\vy\}_m$ (e.g., double letters) is important, we can intersperse blank labels 
$\boldsymbol{\phi} \notin L$  between repeated labels as follows: $ \{\vy\}_m = \{\dots, l_i,l_i, \dots \} \rightarrow \{\dots, l_i,\phi,l_i, \dots \}.$ 


    \textbf{Note on Dynamic Time Warping (DTW):} It is important to highlight the distinction between our approach and DTW-based~\cite{Itakura1975MinimumPR} alignment methods, particularly the differentiable variations such as soft-DTW~\cite{cuturi2018softdtwdifferentiablelossfunction}. These methods generally have quadratic complexity~\cite{cuturi2018softdtwdifferentiablelossfunction}, making them significantly more computationally expensive than ours. Furthermore, in DTW-based methods, the alignment emerges as a consequence of the sequences themselves. When the function $F$ is powerful, the model can collapse by generating a sequence $F(\{\vx\}_n)$ that induces a trivial alignment~\cite{DBLP:journals/corr/abs-2103-17260} (see Appendix~\ref{ax:abla}, where we conducted experiments using soft-DTW for ASR to illustrate this). To mitigate this issue, regularization losses~\cite{DBLP:journals/corr/abs-2103-17260,meghanani2024laserlearningaligningselfsupervised} or constraints on the capacity of $F$~\cite{vayer2022timeseriesalignmentglobal,Zhou2009CanonicalTW} are commonly introduced. However, using regularization losses lacks theoretical guarantees and introduces additional hyperparameters. Furthermore, constraining the capacity of $F$, although more theoretically sound, makes tasks requiring powerful encoders on large datasets impractical. In contrast, %to these DTW-based approaches, 
our method decouples the computation of the alignment from the transformation function $F$, offering more flexibility to the model as well as built-in temporal alignment constraints and theoretical guarantees against collapse.

% We are then guarantee that a solution $F^{*}\{\vx\}_n$  allows to retrieve $\mathcal{A}(\{\vy\}_m)$; in a case were retrieving the repetitions in $\{\vy\}_m$ matter (like double letter), we simply need to intercalate a blank labels $\boldsymbol{\phi} \notin L$ between each repeted labels~: $ \{\vy\}_m = \{\dots, l_i,l_i, \dots \} \rightarrow \{\dots, l_i,\phi,l_i, \dots \}.$ 



% the reciprocal is false (see Annxe) because de optimalalpha may skip some frame, but if we impose the solution in the Definition 1 to have non-zero components the reciprocal is true. In such case if we introduce the equivalente ralshionship  $\{\vx\}_n \sim_R  \{\vy\}_m$ iff $\mathcal{A}(\{\vx\}_n) = \mathcal{A}(\{\vy\}_m)$ and defining the quotient space $\mathcal{U}_{\leq N}^d/ \sim_R$, which is the space of all sequnces of length at most $N$, were all sequences which are the same up to some repetition are considered the same. Thus $\mathcal{S}_{r}$ define a proper metric over $\mathcal{U}_{\leq N}^d/ \sim_R$, which does not sepearate identic sequences p to some repetitions. This properties may be important for other task but for us we prefer to have the ability to skip some frame, so we do not impose any restriction on $\alpha$.
 
%$\boldsymbol{\hat\gamma}^{m,\beta}_n(\boldsymbol{\sigma}) %mathbf{1}_n = \bbeta$ and 
%It's clear that $\boldsymbol{\hat\gamma}^T \mathbf{1}_n = \bbeta $ and  $\sum_{i,j=1}^{n,m} \bgamma_n^{m,\bbeta}(\balpha)_{i,j} \cdot C(\vx_i,\vy_j) =  \sum_{i,j=1}^{n,m} \boldsymbol{\hat\gamma}\boldsymbol_{i,j} \cdot C(\vx_i,\vy_j)$ so $\boldsymbol{\sigma} = {\bgamma^{n,\bbeta}_m}^{-1}(\boldsymbol{\hat\gamma})$ is also a minimum and $\boldsymbol{\sigma} \neq \balpha$ because $\sigma_k =\alpha^{*}_k - \epsilon$.





% \textit{The loss is completely insensitive to repetitive elements in the sequences, \textit{i.e.}, if consecutive duplicates appear in either sequence, the loss remains unchanged.}

    

% \textbf{Proposition 2.} \textit{Let $\mathcal{A}$ be the sequence aggregation operator, which removes consecutive duplicates (\textit{i.e.}, $\mathcal{A}(\{\dots, [\vx, \dots,\vx], \dots\}) = \{\dots, \vx, \dots\}$). Let $\mathcal{A}_{\{\vx\}_n}$ denote the aggregation operator on $\Delta^n$, which groups indices where consecutive elements in $\{\vx\}_n$ are identical (i.e, $\mathcal{A}([{\dots, \alpha_i, \dots,\alpha_{i+k}, \dots}]^T) = [{\dots, \alpha_i + \dots +\alpha_{i+k}, \dots}]^T$ iff $\vx_i = \dots = \vx_{i+k}$).} Then~:

% \begin{align}
%     \mathcal{S}^{2sd}_c(\{\vx\}_n,\{\vy\}_m)
%     = 
%      \mathcal{S}^{2sd}_c(\mathcal{A}(\{\vx\}_n),\mathcal{A}(\{\vx\}_n))
%    \label{eq:lossequal}
%  \end{align}


% \textit{Proof.} We can simply notice that, $\forall \alpha \in \R^n $~:

% \begin{align}
%    \mathcal{L}_{OTTC}^{\bbeta} [\balpha](\{\vx\}^n,\{\vy_i\}_{i=1}^m) =   \mathcal{L}_{OTTC}^{\bbeta} [\mathcal{A}_{\{\vx\}_n}(\balpha)](\mathcal{A}(\{\vx\}^n),\mathcal{A}(\{\vy_i\}_{i=1}^m))
%    \label{eq:lossequal}
%  \end{align}

 
%\textit{Given a discriminative triplet $(\mathbb{X}, \mathbb{Y}, C)$, an $n$-dimensional vector $\balpha$, and an $m$-dimensional vector $\bbeta$ with no zero components: an OTTC loss is a loss function taking the following form}~:

% \begin{align}
% \sum_{i,j=1}^{n,m} \bgamma_n^{m,\bbeta}(\balpha)_{i,j} \cdot C(\vx_i,\vy_j) =  \sum_{i,j=1}^{n,m} \bgamma_n^{m,\bbeta}(\boldsymbol{\mathcal{A}_{\{\vx\}_n}(\alpha)})_{i,j} \cdot C(\mathcal{A}(\vx_i),\mathcal{A}(\vy_j))
%    \label{eq:loss}
%  \end{align}

  

\subsection{Application to ASR: OTTC Loss}
\label{sec:ottc-loss}

In ASR, the target sequences $\{\vy\}_m$ are $d$-dimensional one-hot encoding of elements from the set $L \cup \{\phi\}$, where $\phi$ is a blank label used to separate repeated labels. The encoder $F$ predicts the label probabilities for each audio frame, such that

\begin{equation}
\small
F(\{\vx\}_n) = \{[p_{l_1}(\vx_i), \dots, p_{l_{|L|+1}}(\vx_i)]^T\}_{i=1}^{n}.
\end{equation}

The alignment between $F(\{\vx\}_n)$ and $\{\vy\}_m$ is parameterized by $\balpha[\{\vx\}°_n, W] \in \Delta^n$, defined as 
\begin{small}
\begin{align}
    \balpha[\{\vx\}_n, W] 
    = \softmax(W(\vx_1),\dots,W(\vx_n))^T
    % \left[\frac{e^{W(\vx_1)}}{\sum_{i=1}^n e^{W(\vx_i)}}, \dots, \frac{e^{W(\vx_n)}}{\sum_{i=1}^n e^{W(\vx_i)}}\right]^T
    % ,
\end{align}
\end{small}

where $W$ is a network that outputs a scalar for each frame $\vx_i$. 
Using the framework built in Section~\ref{sec:s2sd} (with $r= 1$ and $C=C_{e}$, where $C_e$ is the cross-entropy) to predict $\{\vy\}_m$ from $\{\vx\}_n$, we train both $W$ and $F$ by minimizing the OTTC objective

\begin{small}
\begin{align}
   \mathcal{L}_{OTTC} = - \sum_{i,j=1}^{n,m} \bgamma_n^{m,\bbeta_m}(\balpha[\{\vx\}_n, W])_{i,j} \cdot \log p_{\vy_j} (\vx_i).
   \label{eq:loss}
\end{align}
\end{small}

The choice of the cross-entropy $C_e$ as the cost function arises naturally from the probabilistic encoding of the predicted output of $F$ and the one-hot encoding of the target sequence. Additionally, since $C_e$ is differentiable, it makes the OTTC loss differentiable with respect to $F$, while the differentiability of the OTTC with respect to $W$ stems from the differentiability of $\bgamma_n^{m,\bbeta_m}$ with respect to its input $\balpha[\{\vx\}_{n}, W]$. Thus, by following the gradient of this loss, we jointly learn both the alignment (via $W$) and the classification (via $F$). 

\textbf{Note:} The notation $\bgamma_n^{m,\bbeta}$ in Eq.~\ref{eq:loss} is valid in the context of ASR since $n \geq m$.

% This vector defines the alignment between $\{\vx\}_n$ and $\{\vy\}_m$, represented as $\bgamma_n^{m,\bbeta}(\balpha[\{\vx\}^n, A])$. Given these alignments, we can then compute the loss to minimize, which is simply the cross-entropy between the aligned elements of the sequencesn hence he OTTC (Optimal Temporal Transport Classification) objective to minimize is: %. More generally, for a batch of sequence pairs, t

%In what follows, we will delve deeper into the convergence behavior and key solution properties of this loss function, providing a more detailed analysis of its theoretical characteristics.


% \subsubsection{Properties of OTTC.}

%In order to have more general results on the following results we define a broader class of OTTC loss from which belong  Eq.~\ref{eq:loss}. %We denote by $\mathcal{A}$ the aggregation operator, which, when applied to any sequence ${\vx_i}$, removes consecutive duplicates. Formally, $\mathcal{A}(\{\dots, \vx, \vx, \dots\}) = \{\dots, \vx, \dots\}$.



% \textbf{Definition 1.} \textbf{\textit{Discriminative triplet}}. \textit{A discriminative triplet is a triplet $(\mathbb{X}, \mathbb{Y}, C)$, where $\mathbb{X} \subset \mathbb{R}^d$, $\mathbb{Y} \subset \mathbb{R}^d$, and $C : \mathbb{X} \times \mathbb{Y} \to [0,+\infty]$ is a cost function such that, for all $x \in \mathbb{X}$ and for all $(y, \hat{y}) \in \mathbb{Y}^2$, if $C(x, y) = C(x, \hat{y})$, then $y = \hat{y}$.}

% \textbf{Definition 2.} \textbf{\textit{OTTC loss}}. 
% \textit{Given a discriminative triplet $(\mathbb{X}, \mathbb{Y}, C)$, an $n$-dimensional vector $\balpha$, and an $m$-dimensional vector $\bbeta$ with no zero components: an OTTC loss is a loss function taking the following form}~:


% \textbf{Definition 1.} \textbf{\textit{Discriminative triplet}}. \textit{A discriminative triplet is a triplet $(\mathbb{X}, \mathbb{Y}, C)$, where $\mathbb{X} \subset \mathbb{R}^d$, $\mathbb{Y} \subset \mathbb{R}^d$, and $C : \mathbb{X} \times \mathbb{Y} \to [0,+\infty]$ is a cost function such that, for all $x \in \mathbb{X}$ and for all $(y, \hat{y}) \in \mathbb{Y}^2$, if $C(x, y) = C(x, \hat{y})$, then $y = \hat{y}$.}

% \textbf{Definition 2.} \textbf{\textit{OTTC loss}}. 
% \textit{Given a discriminative triplet $(\mathbb{X}, \mathbb{Y}, C)$, an $n$-dimensional vector $\balpha$, and an $m$-dimensional vector $\bbeta$ with no zero components: an OTTC loss is a loss function taking the following form}~:


% \begin{align}
%    \mathcal{L}_{OTTC}^{\bbeta} [\balpha](\{\vx\}^n,\{\vy_i\}_{i=1}^m)=  \sum_{i,j=1}^{n,m} \bgamma_n^{m,\bbeta}(\balpha)_{i,j} \cdot C(\vx_i,\vy_i).
%    \label{eq:generalloss}
%  \end{align}

% \textbf{Preliminary remarks.} For the following we will consider sequences $\{\vx\}_n$ and $\{\vy\}_m$ such that $\vy_i \neq \vy_{i+1}$ and $\vx_i \neq \vx_{i+1}$ for all $i$, $j$. Indeed, from the loss perpesctive it makes no differences to align $\vx_i$ and $\vy_j =\vy $ ($\gamma(\balpha)_{i,j}C(\vx_i,\vy_j))$) 
%  or to align $\vx_i$ and $\vy_j = \vy$, \dots, $\vy_{j+k} = \vy$.%  ($\gamma(\balpha')_{i,j}C(\vx_i,\vy_j)) + \dots  +\gamma(\balpha')_{i,j}C(\vx_i,\vy_{j+k}))$)
%  . As long as ~: %$\balpha = [\boldsymbol{\alpha'}_1,\dots,\balpha_{i-1},\balpha_1,\balpha_1,\dots,\balpha_1]^T$
% % \boldsymbol{\alpha'}_{i,j} +\dots + \boldsymbol{\alpha'}_{i,j+k}$

% \begin{align}
% \boldsymbol{\alpha'} = [\balpha_1,\dots,\balpha_{i-1},\boldsymbol{\alpha'}_{i,j} +\dots + \boldsymbol{\alpha'}_{i,j+k}\balpha_1,\dots,\balpha_n]^T
%    \label{eq:constraint}
%  \end{align}

 
 


% \textit{Proof.} It is sufficient to expand both expressions. 


%This property highlights that it makes no difference whether we align an item to another single item or to a repetition of the same item, as long as the total "mass" remains the same. Therefore, we can restrict the following analysis to minimization problem of OTTC to sequences without consecutive duplicates and derive the solution for sequences with consecutive duplicates by applying the reverse of the aggregation operators, $\mathcal{A}$ and $\mathcal{A}_{\{\vx\}_n}$. While these operators are not strictly invertible, it is straightforward to identify one of the antecedents that satisfies the original sequence.




%\textbf{Proposition 3.} 
%\textit{Given two sequences $\{\vx\}_n$ and $\{\vy\}_m$ such that $\vy_i \neq \vy_{i+1}$ and $\vx_j \neq \vx_{j+1}$ for all $i$, $j$. The optimal alignment $\balpha^{*} = \argmin_{\balpha} \mathcal{L}_{OTTC}^{\bbeta}$ verifies~:}

% \begin{itemize}
%     \item \textit{The solution always exists but is not necessarily unique.}
%     \item \textit{The alignment induced by the solution from the longer to the shorter sequence is injective, \textit{i.e.}, each item of the shorter sequence is assigned to exactly one item in the longer sequence.} 
% \end{itemize}

%\textit{Proof.} Since $\bgamma_n^{m,\bbeta}$ (denoted $\bgamma$ when their is no ambiguitee) is differentiable so continuous, it follows that $\balpha \mapsto \mathcal{L}_{OTTC}^{\bbeta}[\balpha](\{\mathbf{x}_i\}_{i=1}^n, \{\mathbf{y}_i\}_{i=1}^m)$ is continuous over $\Delta^n$. %Given that $\Delta^n$ is a compact set and every continuous function on a compact space is bounded and attains its bounds, the existence of an optimal solution $\balpha^{*}$ follows. For the second proposition, let's imagine that the minimum is reached twice by two distinct solution~: $\balpha^{*}$ and $\boldsymbol{\sigma}^{*}$. Therefore~:

% \begin{equation}
%     \sum_{i,j=1}^{n,m} \bgamma(\balpha)_{i,j} \cdot C(\vx_i,\vy_i). = \sum_{i,j=1}^{n,m} \bgamma(\boldsymbol{\sigma})_{i,j} \cdot C(\vx_i,\vy_i).
% \end{equation}

% \textit{i.e.};

% \begin{equation}
%     \sum_{\substack{i,j=1 \\ \bgamma(\balpha)_{i,j} - \bgamma(\boldsymbol{\sigma})_{i,j} = 0} }^{n,m} ( \bgamma(\balpha)_{i,j} - \bgamma(\boldsymbol{\sigma})_{i,j} ) \cdot C(\vx_i,\vy_i) + \sum_{\substack{i,j=1 \\ \bgamma(\balpha)_{i,j} - \bgamma(\boldsymbol{\sigma})_{i,j} \neq 0} }^{n,m}( \bgamma(\balpha)_{i,j} - \bgamma(\boldsymbol{\sigma})_{i,j} ) \cdot C(\vx_i,\vy_i) = 0.
% \end{equation}

% then,

%  \begin{equation}
%  \sum_{\substack{i,j=1 \\ \bgamma(\balpha)_{i,j} - \bgamma(\boldsymbol{\sigma})_{i,j} \neq 0} }^{n,m}( \bgamma(\balpha)_{i,j} - \bgamma(\boldsymbol{\sigma})_{i,j} ) \cdot C(\vx_i,\vy_i) = 0.
% \end{equation}
% This sum has necessary at least one term otherwise, would have $\bgamma(\balpha) = \bgamma(\boldsymbol{\sigma})$, \textit{i.e.}, $\balpha$ = $\boldsymbol{\sigma}$ (since $\gamma$ is bijective). For

% \textbf{Inference time.} As in CTC and transducers, any two consecutive frames $\vx_k^b$ and $\vx_{k+1}^b$ with the same label are collapsed into one (\textit{i.e.}, $\mathcal{B}(\dots l_k l_k \dots) = \dots l_k \dots$), and any blank token is removed (\textit{i.e.}, $\mathcal{B}(\dots l_k \phi l_k \dots) = \dots l_k l_k \dots$). However, unlike CTC or transducers, we do not need to introduce a blank token to skip the classification of a frame. When $\balpha[{\vx_i^b}_{i=1}^{n_b}, A]_k = 0$, the frame $\vx_k$ is automatically excluded from the loss function (Eq.~\ref{eq:loss}) and simply ignored during inference. In the OTTC framework, the blank token is only used as a label inserted between any consecutive, repeated labels (\textit{i.e.}, $\{\dots, l_k , l_k ,\dots\} \to  \{\dots, l_k , \phi, l_k ,\dots\} $). The usage of blank tokens and the selection of $\beta_b$ will be further discussed in Section XXX. Additionally, decoding methods such as language model fusion or keyword boosting, commonly applied in the literature, are still applicable to our framework.


% At the inference time, like in CTC and transducer every two consecutives frames $\vx_k^b$ and $\vx_{k+1}^b$ with the same labels care collapsed to one (\textit{i.e.}, $\mathcal{B}(\dots l_k l_k \dots) = \dots l_k \dots $) and every blank token is deleted (\textit{i.e.}, $\mathcal{B}(\dots l_k \phi l_k \dots) = \dots l_k l_k \dots $). The one difference is it unlike CTC or transducers, we do not need to introduce a blank token to skip the classification of a frame. Indeed, when $\balpha[\{\vx_i^b\}_{i=1}^{n}, A]_k = 0$, the frame $\vx_k$ is effectively excluded from the loss function (Eq.~\ref{eq:loss}). During inference, we simply ignore any frame $k$ for which $\balpha[\{\vx_i^b\}_{i=1}^{n_b}, A]_k = 0$.  The blank token in OTTC settings is solely use as label inserted between any consecutives label which is repeated \textit{i.e.}, if $\{\vy_1 \dots\}_{i=1}^n$, depending on the context this will be discusses ... as well as the choice for $\beta_b$ will be discussed on XXX section. Anyway, more generally all all litterature developped to decode like LM fusion or keyword boosting still apply to our framework.



% % Additionnaly when two consecutives frames $\vx_k^b$ and $\vx_{k+1}^b$ has the label 




% % The inference time is exactly the same as for CTC and transducer base model, 


% % It is worth noting that, unlike CTC or transducers, we do not need to introduce a blank token to skip the classification of a frame. Indeed, when $\balpha[\{\vx_i^b\}_{i=1}^{n_b}, A]_k = 0$, the frame $\vx_k^$ is effectively excluded from the loss function (Eq.~\ref{eq:loss}). During inference, we simply ignore any frame $k$ for which $\balpha[\{\vx_i^b\}_{i=1}^{n_b}, A]_k = 0$. Additionnaly when two consecutives frames $\vx_k^b$ and $\vx_{k+1}^b$ has the label 





% %This loss is fully differentiable with respect to the networks $A$ and $F$, the goal of the training is to found the best of the network to minimise this. One can note that the alignement fully depens on 
% % \begin{align}
% %     \sum_{i,j=1}^{n,m} \bgamma_n^{m,\bbeta}(\balpha[\{\vx_i\}_{i=1}^n, A])_{i,j} \cdot \log p_{l_j} (\vx_i)
% %  \end{align}

% DISCUSS SOME PROPERTIES of this loss


\subsection{Link with CTC Loss}
\label{subsec:link-with-ctc}
%In CTC, alignment is carried out using $\phi$ labels. To establish the link between CTC and OTTC we will suppose that the target is such that $\{\vy\}_m = \{\phi, l_{k_1},\phi, \dots, \phi, l_{\frac{m-1}{2}}, \phi \}$. 

% {\textcolor{red}{In this section, we contrast the CTC loss with the proposed OTTC loss.}}
In this section, we link the CTC and the proposed OTTC losses. 
In the context of CTC, we denote by $\mathcal{B}$ the mapping which reduces any sequences by deleting repeated vocabulary (similarly  to the previously defined $\mathcal{A}$ mapping in Proposition 5) \emph{and then} deleting the blank token $\phi$ {{(e.g., $\mathcal{B}(\{GGOO \phi ODD\}) = \{GOOD\}$)}}. The objective of CTC is to maximise the probability of all possible paths $\{\bpi\}_n$ of length $n$ through minimizing

\begin{footnotesize}
\begin{align}
   - \mkern-25mu \sum_{\{\bpi\}_n \in \mathcal{B}^{-1}(\{\vy\}_m)}   \log p(\{\bpi\}_n) =    - \mkern-25mu  \sum_{\{\bpi\}_n \in \mathcal{B}^{-1}(\{\vy\}_m)}    \log \prod_{i=1}^np(\bpi_i), 
 \end{align}
\end{footnotesize}


where $\{\bpi\} \in L^n$ is an $n$-length sequence and $\mathcal{B} ^{-1}(\{\vy\}_m)$ is the set of all sequences collapsed by $\mathcal{B}$ into $\{\vy\}_m$. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=1.05\linewidth]{ctcpathv3.pdf}
    \vspace{-0.4cm}
    \caption{\textbf{\textit{A CTC alignment.} }
Here, we illustrate one of the valid alignments for CTC. The CTC loss maximizes the marginal probability over all such possible alignments.}
    \label{fig:ctcpath}
\end{figure}
% These paths are also called alignments because they induce an alignment.
%{\textcolor{red}{You should make this next couple of sentences and equation more concise. I don't think it can be understood what you are trying to say and how the equation helps support the point you are trying to make.}}

Let us consider a path $\{\bpi\}_n \in \mathcal{B} ^{-1}(\{\vy\}_m)$. Such a path can be seen as an alignment (see Figure~\ref{fig:ctcpath}), where $\{\vx_i\}$ and $\{\vy_j\}$ are aligned iff $\bpi_i = \vy_j$. By denoting $\boldsymbol{A}_\pi$ as the corresponding discrete monotonic alignment, one can write:%  Let introduce $A \subset [\!|1,n|\!]\times[\!| 1,m|\!]$ such that  $(i,j) \in A_i $ iff $\bpi_i = \vy_j$. Then~:
\begin{small}
    \begin{align}
\log p(\{\bpi\}_n) = \sum_{i=1}^n \log p_{\bpi_i}(\vx_i) & = \:\:\:\:\: - \mkern-20mu \sum_{\substack{ i,j =1\\ (i,j) \in \mathbf{A}_{\bpi} }}^{n,m} \mkern-10muC_e(\bpi_j,\vy_i) \notag \\
&\mkern-20mu \stackrel{\exists \balpha \in \Delta^n}{=} -\mkern-40mu\sum_{\substack{ i,j =1\\ \bgamma_p^{n,\bbeta_m}(\balpha)_{i,j} >0 }}^{n,m}  \mkern-35mu C_e(\bpi_j,\vy_i),
 \end{align}
\end{small}with \(C_e\) representing the cross-entropy.
\textit{The last equality arises from Proposition 1 and the fact that $\boldsymbol{A}_\pi$ represents a discrete monotonic alignment.
}% (since the set of path $\pi_n$ is discrete while $\bgamma_n^{n,\beta}(\balpha)$ isn't.)}

The continuous relaxation (\textit{i.e.} making the problem continuous with respect to alignment) of the last term in this sequence of equalities results in $- \mathcal{L}_{OTTC}$. Therefore, OTTC can be seen as relaxation of the probability associated with a single path, enabling a differentiable path search mechanism. Essentially, OTTC optimization focuses on maximizing the probability of exactly one path, in contrast to CTC, which maximizes the probability across all valid paths.

Additionally, OTTC does not incentivize paths containing many blank tokens, unlike CTC. In CTC, the peaky behavior arises because maximizing the marginal probability over all valid paths can incentivize the model to assign more frames to the blank token \cite{ctc-peaky}. In contrast, OTTC does not rely on a blank token to indicate that a frame $i$ should not be classified (blank tokens are only used to separate consecutive tokens). Instead, the model simply sets the corresponding weight $\alpha_i$ to 0 (see Figure~\ref{fig:dma2}). This mechanism avoids the peaky behavior exhibited by CTC.

% \yac{
% \st{Additionally, OTTC does not incentivize paths containing many blank tokens, unlike CTC, as blanks are solely used to separate repeated labels (e.g., consecutive tokens). Instead of relying on a blank token to indicate that a frame $i$ should not be classified, the model can simply set the corresponding weight $\alpha_i$ to 0 (see Figure}~\ref{fig:dma2}).}


% It is worth noting that through certain design choices, OTTC offers flexibility in constraining the search space for the best path, allowing more control over how the model aligns frames to targets. This can be advantageous in avoiding unnecessary blank predictions. For example, instead of using a target sequence like $\{\vy\}_m = \{\phi, l{k_1}, \phi, \dots, \phi, l_{\frac{m-1}{2}}, \phi\}$, which includes blank labels between each letter, we can use a simpler target $\{\vy\}_m = \{l_1, \dots, l_m\}$ with a blank label only to separate repeated labels (e.g., consecutive letters). In this scenario, unlike CTC, OTTC does not incentivize predicting blank labels except when separating identical characters. This design choice effectively mitigates the "peaky" behavior commonly observed in CTC-based models, where there is a tendency to overuse blank predictions. Instead of relying on a blank token to indicate that a frame $i$
% i should not be classified, the model can simply set the corresponding weight $\alpha_i$ to $0$.

% \textbf{Peaky behaviour.}

% \textbf{Soft Knowledge Distillation (KD).}

% \textbf{Audio-text forced alignment.}

% \textbf{Decoding techniques.}

% \begin{itemize}
%     \item more robust to errors in reference text: \TODO{experiement on libri 100h, maybe controlled?}
%     \item no issue of peaky behaviour: \TODO{show through plotting the alignment that most of the frames are assigned to blanks as happens in CTC}
%     \item handles soft knowledge distillation naturally: \TODO{if we can, exp on 100h or possibly not required as it can be clear from alignment that peaky behavior is not there, so KLD is well-defined}
%     \item useful in aligning long audio with text owing to low memory and time complexity requirements. Both CTC and transducer take quadratic time and space complexity due to the requirement of backtracking. : \TODO{just by detailing the space/time complexity}
%     \item all current decoding techniques like LM fusion, keyword boosting still apply to our framework. \TODO{experiment on libri 960h}
% \end{itemize}

% \subsection{extension to other seq-to-seq tasks}
% \TODO{show an example formulation for machine translation or any text mapping task, maybe? bottleneck comes from the fact that output length can't be more than input length for OTTC, if we find something then nice, otherwise removable} OK

\section{Experimental Setup}
\label{sec:exp-setup}
To demonstrate the viability of the proposed OTTC loss framework, we conduct several proof-of-concept experiments on the ASR task.
To this end, we compare alignment quality and ASR performance using the proposed OTTC framework and the existing CTC-based model. Note that an efficient batched implementation of OTTC along with the full code to reproduce our experimental results will be made publicly available.

% \yac{\st{To demonstrate the viability of the proposed OTTC loss framework, we conduct proof-of-concept experiments on the ASR task, which is an important problem from the perspective of seq2seq learning. 
% To this end, we compare results obtained through the OTTC loss framework in terms of the Word Error Rate (WER) with those obtained from a CTC-based model. Note that an efficient batched implementation of OTTC along with the full code to reproduce our experimental results will be made publicly available.}}

% \subsection{dataset}
% \label{subsec:dataset}
%\textbf{Implementation.} 

\textbf{Datasets.} \enspace We conduct our experiments on popular open-source datasets, \textit{i.e.}, the TIMIT~\cite{garofolo1993timit}, AMI~\cite{ami}, and LibriSpeech~\cite{panayotov2015librispeech}. TIMIT is a 5-hour English dataset with time-aligned transcriptions, including exact time-frame phoneme transcriptions, making it a standard benchmark for ASR and phoneme segmentation tasks. We report results on the standard eval set. AMI is an English spontaneous meeting speech corpus that serves as a good benchmark to evaluate our approach in a realistic conversational scenario, due to its spontaneous nature and prior use in alignment evaluation \cite{rastorgueva23_interspeech}. For our experiments on this dataset, we train models on the individual head microphone (IHM) split comprising 80 hours of audio, and report results on the official eval set. LibriSpeech is an English read-speech corpus derived from audiobooks, containing 1000 hours of data. It is a standard benchmark for reporting ASR results. For our experiments, we train models on the official 100-hour, 360-hour, and 960-hour splits, and report results on the two official test sets.


% \st{We conduct our experiments on popular open-source datasets, \textit{i.e.}, the LibriSpeech}~\cite{panayotov2015librispeech} \st{and AMI}~\cite{ami}\st{ datasets. LibriSpeech is an English read-speech corpus derived from audiobooks, containing 1000 hours of data. For our experiments on this dataset, we train models on the official 100-hour, 360-hour, and 960-hour splits, and report results on the two official test sets. 
% % 2) Multilingual LibriSpeech (MLS) \cite{pratap2020mls}, a multilingual read-speech corpus derived from audiobooks. We train our models on French (fr, 1076h), Spanish (es, 917h), Italian (it, 247h), and Portuguese (pt, 160h) and report results on their respective official test sets; 
% AMI is an English spontaneous meeting speech corpus, which differs significantly from read-speech. For our experiments on this dataset, we train models on the individual head microphone (IHM) split comprising 80 hours of audio, and report results on the official dev and eval sets.}

\begin{table*}[t]
\centering
\renewcommand{\arraystretch}{1.2} % Increase row height for readability
\setlength{\tabcolsep}{7.5pt} % Adjust column spacing for balance
\caption{Alignment performance of the CTC loss-based ASR model and our proposed OTTC loss-based ASR model on the TIMIT and AMI datasets. Peaky behavior is measured as the percentage of frames assigned to blank or space symbols (lower is better) - $^\dagger$For TIMIT, we subtract the percentage of real silence, as it is available, unlike in AMI. The F1 score reflects the accuracy of the starting frame of the predicted tokens. IDR quantifies the overlap between reference and predicted word segments (higher is better).}
\label{table:alignment_results}
\begin{tabular}{l | ccc | ccc }
\toprule
\multirow{2}{*}{Model} & \multicolumn{3}{c|}{\textbf{TIMIT (Phoneme Level)}} & \multicolumn{3}{c}{\textbf{AMI (Word Level)}} \\
& Peaky$^\dagger$ & F1 Score & IDR & Peaky & F1 Score & IDR \\
% & Peaky$^\dagger$ & Precision & Recall & F1 Score & IDR & Peaky & Precision & Recall & F1 Score & IDR \\
\midrule
CTC & 53.51 & 88.77 & 26.98 & 81.93 & 83.94 & 16.75 \\
OTTC & 0.76 & 89.27 & 76.72 & 54.75 & 84.81 & 42.84 \\
\bottomrule
\end{tabular}
\vspace{-0.4cm}
\end{table*}
% \subsection{Model Architecture}
% \label{subsec:arch}
\textbf{Model architecture.} \enspace We use the 300M parameter version of the well-known Wav2Vec2-large \cite{baevski2020wav2vec} as the base model for acoustic embeddings in all the experiments conducted in this work. The Wav2Vec2 is a self-supervised model pre-trained on 60K hours of unlabeled English speech.
For the baseline CTC-based models, we stack a dropout layer followed by a linear layer for logits prediction, termed the \textit{logits prediction head}.
For the proposed OTTC loss based models, we use a dropout and a linear layer (identical to the baseline) for logits prediction.
In addition, as described in Section \ref{sec:ottc-loss},  we apply a dropout layer followed by two linear layers on top of the Wav2Vec2-large model for OT weight prediction, with a GeLU \cite{hendrycks2016gaussian} non-linearity in between, termed the \textit{OT weights prediction head}.
Note that the output from the Wav2Vec2-large model is used as input for both the logit and OT weight prediction heads, and the entire model is trained using the OTTC loss.
% During inference, only the logits prediction head is required (\sk{from framework, skipping of frame for prediction is allowed, but never observed in our experiments, so this statement is still valid}).

% \subsection{Training details}
% \label{subsec:train-details}

% \begin{table*}[t]
% \centering
% \caption{WER(\%) comparison between the CTC loss-based ASR model and our proposed OTTC loss-based ASR model. On the LibriSpeech dataset, models are trained using the three official training splits with varying amounts of supervised data, and results are reported on the two official test sets. For the AMI dataset, models are trained on the IHM split, and results are reported on both the dev and eval sets. Note that for WER, lower is better.\\}
% \label{table:results}
% \begin{tabular}{l | cc | cc | cc | cc}
% \toprule
% \multirow{2}{*}{Model} & \multicolumn{2}{c|}{\textbf{100h-LibriSpeech}} & \multicolumn{2}{c|}{\textbf{360h-LibriSpeech}} & \multicolumn{2}{c|}{\textbf{960h-LibriSpeech}} & \multicolumn{2}{c}{\textbf{AMI-IHM}} \\
% & test-clean & test-other & test-clean & test-other & test-clean & test-other & dev & eval \\
% \midrule
% CTC & 4.93 & 12.09 & 3.53 & 10.04 & 2.9 & 7.46 & 15.8 & 13.9 \\
% \midrule
% OTTC & 7.43 & 17.34 & 5.19 & 13.49 & 4.24 & 10.36 & 18.5 & 16.8 \\
% \bottomrule
% \end{tabular}
% \end{table*}
\textbf{Performance metrics.} \enspace ASR performance is evaluated using the standard WER. Alignment quality is assessed using three metrics, \textit{i.e.}, peaky behavior, starting frame accuracy, and Intersection Duration Ratio (IDR).
Peaky behavior in CTC models is characterized by a large proportion of audio frames being assigned to blank or space symbols (non-alphabet symbols)~\cite{ctc-peaky}. To quantify this, we compute the average percentage of frames mapped to these symbols.
Starting frame accuracy is evaluated using the F1 score, following the methodology in~\cite{rastorgueva23_interspeech}. On TIMIT, where ground truth alignments are available, we assess timestamps at the phoneme level. In contrast, on AMI, where ground truth alignments are unavailable, we use forced alignment as in~\cite{rastorgueva23_interspeech}, but restrict our evaluation to word-level timestamps which are generally more reliable than phoneme-, letter-, or subword-level timestamps.
Please note that the F1 score used in the state-of-the-art literature is based only on the starting frame of the predicted token, which does not fully reflect alignment quality. 
%While these metrics evaluate alignment quality, they do not capture the predicted duration of the words. 
To address this, we introduce IDR, which measures the overlap between predicted and reference word segments, normalized by the reference duration. This provides a finer-grained assessment of temporal alignment.

\textbf{Training details.} \enspace In all our experiments, we use the AdamW optimizer \cite{loshchilov2018decoupled} for training.
For TIMIT and LibriSpeech, the initial learning rate is set to $lr\!=2e^{-4}$, with a linear warm-up for the first $500$ steps followed by a linear decay until the end of training.
For AMI, the initial learning rate is set to $lr\!=1.25e^{-3}$, with a linear warm-up during the first $10\%$ of the steps, also followed by linear decay.
We train both CTC-based and OTTC-based models for 40 epochs, reporting the test set WER at the final epoch. In our OTTC-based models, both the logits and OT weight prediction heads are trained for the first 30 epochs. During the final 10 epochs, the \textit{OT weight prediction head} is fixed, while training continues on the \textit{logits prediction head}.
For experiments on the LibriSpeech (\textit{resp.} TIMIT) dataset, we use character-level ({resp.} phoneme-level) tokens to encode text. 
Given the popularity of subword-based units for encoding text \cite{sennrich2016neural}, we sought to observe the behavior of OTTC-based models when tokens are subword-based, where a token can contain more than one character.
For the experiments on the AMI dataset, we use the SentencePiece tokenizer \cite{kudo2018sentencepiece} to train subwords from the training text.
Greedy decoding is used for both the CTC and OTTC models to generate the hypothesis text.

\textbf{Choice of label weights ($\bbeta_q$).}
To simplify the training setup for our OTTC-based models, we use a fixed and uniform $\bbeta_q$ (see Sections \ref{subsec:diff-alignment-ot} \& \ref{sec:ottc-loss}), where the length $q$ of $\bbeta$ is equal to the total number of tokens in the text after augmenting with the blank ($\phi$) label between repeating characters.
% \subsection{Curriculum learning}
% \label{subsec:cur-learning}


\section{Results and Discussion}
\label{sec:results}
% \sk{notes from justification in unsupervised ASR paper, so results reported on both dev and test, clean and other}
% {\textbf{Results.}}
{\textbf{Alignment quality.}} \enspace We begin by analyzing the alignment performance of the models on the TIMIT and AMI datasets, with results shown in Table~\ref{table:alignment_results}.
Our proposed OTTC model consistently outperforms the CTC-based models across all alignment metrics on both the datasets.
A key observation is the significant difference in the percentage of frames assigned to non-alphabet symbols by the CTC models, which highlights the peaky behavior inherent in these models. Specifically, CTC models tend to assign a large proportion of frames to the blank or space symbols, reflecting a misalignment in predicting word boundaries. In contrast, the OTTC model avoids this issue, preventing the extreme peaky behavior observed in CTC models.
While the OTTC model also outperforms the CTC model in F1 score, the margin of improvement is smaller. However, the IDR reveals a substantial advantage for OTTC, with a significant improvement over CTC. This indicates that CTC models often either delay the prediction of word starts or assigns too few frames to non-blank symbols, reinforcing the peaky behavior.
Additionally, the performance improvement on the AMI dataset is particularly significant, given its nature of spontaneous meeting speech. This demonstrates how effectively the OTTC loss adapts to varying speaking rates, showcasing the robustness of our framework in learning alignments despite speech variability.
% This highlights how well the OTTC loss adapts to varying speaking rates, demonstrating the robustness of our framework in learning alignments despite speech variability.
 % even when using a fixed and uniform $\bbeta_q$ (independent of acoustic frames)

\begin{table*}[t]
\centering
\caption{Word Error Rate (WER\%) comparison between the CTC loss-based ASR model and our proposed OTTC loss-based ASR model on the TIMIT, AMI, and LibriSpeech datasets. Lower WER is better.}
\label{table:wer_results}
\begin{tabular}{l | c | c| cc | cc | cc }
\toprule
\multirow{2}{*}{Model} & \textbf{TIMIT} & \textbf{AMI} & \multicolumn{2}{c|}{\textbf{100h-LibriSpeech}} & \multicolumn{2}{c|}{\textbf{360h-LibriSpeech}} & \multicolumn{2}{c}{\textbf{960h-LibriSpeech}} \\
& eval & eval & test-clean & test-other & test-clean & test-other & test-clean & test-other  \\
\midrule
CTC & 8.38 & 11.75 & 3.36 & 7.36 & 2.77 & 6.58 & 2.20 & 5.23  \\
OTTC & 8.76 & 14.27 & 3.77 & 8.55 & 3.00 & 7.44 & 2.52 & 6.16  \\
\bottomrule
\end{tabular}
\vspace{-0.4cm}
\end{table*}
{\textbf{WER.}} \enspace Next, we report the ASR performance in terms of WER in Table~\ref{table:wer_results}.
On the TIMIT dataset, the OTTC model shows a slightly higher WER compared to the CTC model, and while the performance gap is larger on the AMI dataset, it's encouraging to observe consistent performance despite the varied nature of speech.
On the LibriSpeech dataset, using the 100-hour training split, the OTTC model achieves a WER of $3.77\%$ on test-clean.
% This result demonstrates the model's remarkable alignment learning capability, even when the OT weights for the labels ($\bbeta_q$) are uniform and independent of the acoustic embedding information.
As we scale the training dataset (100h $\rightarrow$ 360h $\rightarrow$ 960h), we observe a monotonic improvement in WER for the proposed OTTC-based models, similarly to the CTC-based models.
Although the WERs achieved by the OTTC-based models are typically higher than the CTC-based models, the presented results underscore the experimental validity of the SOTD as a metric and demonstrate that learning a single alignment can yield promising results in E2E ASR.
%4.77 at epoch 30 for 960h
% \TODO{discussion about the effect of fixing alignment for last 10 epochs}

\begin{figure}[h!]
    \centering
\includegraphics[width=1\linewidth]{bv2.pdf}
\vspace{-0.5cm}
\caption{\textbf{\textit{CTC and OTTC alignments.} }
Phoneme-level transcription of CTC and OTTC, compared to a reference from TIMIT.}
% \st{CTC shows a high occurrence of blank tokens with sparse non-blank assignments, resulting in peaky behavior. OTTC rarely aligns frames to blank tokens, avoiding this peaky pattern.}
    \label{fig:ctcpeaky}
\end{figure}
{\textbf{Qualitative alignment comparison.}} \enspace Apart from quantitative alignment comparison (Table~\ref{table:alignment_results}), we show an alignment from the CTC- and OTTC-based models in Figure \ref{fig:ctcpeaky}.
For CTC, it can be seen that the best path aligns most frames to the blank token, resulting in a peaky behavior \cite{ctc-peaky}.
In contrast, the OTTC model learns to align all frames to non-blank tokens. This effectively mitigates the peaky behavior observed in the CTC model.
Note that OTTC allows dropping frames during alignment (see Section~\ref{subsec:link-with-ctc}), however, in practice, we observed that only a few frames are dropped.
% See more alignment examples in Appendix~\ref{appendix:ctc-ottc-align}.
For additional insights, we plot the evolution of the alignment for the OTTC model during the course of training in Figures~\ref{fig:ottc-align-evolution} \& \ref{fig:ottc-align-evolution2}.
It is evident that the alignment learned early in the training process remains relatively stable as training progresses. The most notable changes occur at the extremities of the predicted label clusters. This observation led us to the decision to freeze the OT weight predictions for the final 10 epochs, otherwise, even subtle changes in alignment could adversely impact the logits predictions because same base model is shared for predicting both the logits and the alignment OT weights.
  % due to their tight coupling within the OTTC loss function.
% More such examples of OTTC alignment can be found in Appendix~\ref{appendix:align}.

In summary, the presented results demonstrate that the proposed OTTC models achieve significant improvements in alignment performance, effectively mitigating the peaky behavior observed in CTC models. Although there is an increase in WER, the improvement in alignment accuracy indicates better temporal modeling. This enhanced alignment could benefit tasks that require precise timing information, such as speech segmentation, event detection, and applications in the medical domain, where accurate temporal alignment is crucial for tasks like clinical transcription or patient monitoring.
% While we considered fixed label weights ($\{\bbeta\}_N$) in our experiments, the framework allows for learnable label weights.
% However, without proper constraints on the minimum values of the label weights, this could lead to a degenerate solution where all acoustic frames align with a random label, causing alignment collapse.
% We envision that learning label weights with suitable constraints can bridge the performance gap with CTC models.
% Furthermore, our framework effectively addresses the peaky behavior commonly seen in CTC models, resulting in improved alignments.
% As can be seen from the alignment plots and the results that the OTTC model shows comparable performance in ASR while effectively mitigating peaky-behaviour.
% However, there is still a significant performance degradation from CTC based models.
\begin{figure}[t!]
% \vspace{-1cm}
    \centering
\includegraphics[width=1\linewidth]{alignemen_tv2.pdf}
\vspace{-0.5cm}
\caption{\textbf{\textit{Evolution of alignment in the OTTC model during the course of training.} }
The red bullets represent elements of the target sequence $\{\vy\}_m$, while the blue bullets indicate the predicted OT weights for each frame. The size of the blue bullets is proportional to the predicted OT weight.}
    \label{fig:ottc-align-evolution}
\vspace{-0.55cm}
\end{figure}
% \vspace{-0.4cm}
 \vspace{-0.2cm}
\section{Conclusion and Future Work}
% \vspace{-0.2cm}
Learning effective sequence-to-sequence mapping along with its corresponding alignment has diverse applications across various fields.
Building upon our core idea of modeling the alignment between two sequences as a learnable mapping while simultaneously predicting the target sequence, we define a pseudo-metric known as the Sequence Optimal Transport Distance (SOTD) over sequences.
Our formulation of SOTD enables the joint optimization of target sequence prediction and alignment, which is achieved through one-dimensional optimal transport.
We theoretically show that the SOTD indeed defines a distance with guaranteed existence of a solution, though uniqueness is not assured.
We then derive the Optimal Temporal Transport Classification (OTTC) loss for ASR where the task is to map acoustic frames to text.
Experiments across multiple datasets demonstrate that our method significantly improves alignment performance while successfully avoiding the peaky behavior commonly observed in CTC-based models.
Other sequence-to-sequence tasks could be investigated using the proposed framework, particularly those involving the alignment of multiple sequences, such as audio, video, and text.

% Although OTTC models perform well in ASR, there is still pronounced degradation compared to CTC models.
% While we use fixed label weights in our experiments, the framework supports learnable label weights, a promising direction for future work.
% Additionally, exploring alternative curriculum learning strategies between alignment and logits during training could enhance performance.
% Finally, other sequence-to-sequence tasks could be investigated using the proposed framework, particularly those involving the alignment of multiple sequences, such as audio, video, and text.


% \section{Electronic Submission}
% \label{submission}

% Submission to ICML 2025 will be entirely electronic, via a web site
% (not email). Information about the submission process and \LaTeX\ templates
% are available on the conference web site at:
% \begin{center}
% \textbf{\texttt{http://icml.cc/}}
% \end{center}

% The guidelines below will be enforced for initial submissions and
% camera-ready copies. Here is a brief summary:
% \begin{itemize}
% \item Submissions must be in PDF\@. 
% \item If your paper has appendices, submit the appendix together with the main body and the references \textbf{as a single file}. Reviewers will not look for appendices as a separate PDF file. So if you submit such an extra file, reviewers will very likely miss it.
% \item Page limit: The main body of the paper has to be fitted to 8 pages, excluding references and appendices; the space for the latter two is not limited in pages, but the total file size may not exceed 10MB. For the final version of the paper, authors can add one extra page to the main body.
% \item \textbf{Do not include author information or acknowledgements} in your
%     initial submission.
% \item Your paper should be in \textbf{10 point Times font}.
% \item Make sure your PDF file only uses Type-1 fonts.
% \item Place figure captions \emph{under} the figure (and omit titles from inside
%     the graphic file itself). Place table captions \emph{over} the table.
% \item References must include page numbers whenever possible and be as complete
%     as possible. Place multiple citations in chronological order.
% \item Do not alter the style template; in particular, do not compress the paper
%     format by reducing the vertical spaces.
% \item Keep your abstract brief and self-contained, one paragraph and roughly
%     4--6 sentences. Gross violations will require correction at the
%     camera-ready phase. The title should have content words capitalized.
% \end{itemize}

% \subsection{Submitting Papers}

% \textbf{Anonymous Submission:} ICML uses double-blind review: no identifying
% author information may appear on the title page or in the paper
% itself. \cref{author info} gives further details.

% \medskip

% Authors must provide their manuscripts in \textbf{PDF} format.
% Furthermore, please make sure that files contain only embedded Type-1 fonts
% (e.g.,~using the program \texttt{pdffonts} in linux or using
% File/DocumentProperties/Fonts in Acrobat). Other fonts (like Type-3)
% might come from graphics files imported into the document.

% Authors using \textbf{Word} must convert their document to PDF\@. Most
% of the latest versions of Word have the facility to do this
% automatically. Submissions will not be accepted in Word format or any
% format other than PDF\@. Really. We're not joking. Don't send Word.

% Those who use \textbf{\LaTeX} should avoid including Type-3 fonts.
% Those using \texttt{latex} and \texttt{dvips} may need the following
% two commands:

% {\footnotesize
% \begin{verbatim}
% dvips -Ppdf -tletter -G0 -o paper.ps paper.dvi
% ps2pdf paper.ps
% \end{verbatim}}
% It is a zero following the ``-G'', which tells dvips to use
% the config.pdf file. Newer \TeX\ distributions don't always need this
% option.

% Using \texttt{pdflatex} rather than \texttt{latex}, often gives better
% results. This program avoids the Type-3 font problem, and supports more
% advanced features in the \texttt{microtype} package.

% \textbf{Graphics files} should be a reasonable size, and included from
% an appropriate format. Use vector formats (.eps/.pdf) for plots,
% lossless bitmap formats (.png) for raster graphics with sharp lines, and
% jpeg for photo-like images.

% The style file uses the \texttt{hyperref} package to make clickable
% links in documents. If this causes problems for you, add
% \texttt{nohyperref} as one of the options to the \texttt{icml2025}
% usepackage statement.


% \subsection{Submitting Final Camera-Ready Copy}

% The final versions of papers accepted for publication should follow the
% same format and naming convention as initial submissions, except that
% author information (names and affiliations) should be given. See
% \cref{final author} for formatting instructions.

% The footnote, ``Preliminary work. Under review by the International
% Conference on Machine Learning (ICML). Do not distribute.'' must be
% modified to ``\textit{Proceedings of the
% $\mathit{42}^{nd}$ International Conference on Machine Learning},
% Vancouver, Canada, PMLR 267, 2025.
% Copyright 2025 by the author(s).''

% For those using the \textbf{\LaTeX} style file, this change (and others) is
% handled automatically by simply changing
% $\mathtt{\backslash usepackage\{icml2025\}}$ to
% $$\mathtt{\backslash usepackage[accepted]\{icml2025\}}$$
% Authors using \textbf{Word} must edit the
% footnote on the first page of the document themselves.

% Camera-ready copies should have the title of the paper as running head
% on each page except the first one. The running title consists of a
% single line centered above a horizontal rule which is $1$~point thick.
% The running head should be centered, bold and in $9$~point type. The
% rule should be $10$~points above the main text. For those using the
% \textbf{\LaTeX} style file, the original title is automatically set as running
% head using the \texttt{fancyhdr} package which is included in the ICML
% 2025 style file package. In case that the original title exceeds the
% size restrictions, a shorter form can be supplied by using

% \verb|\icmltitlerunning{...}|

% just before $\mathtt{\backslash begin\{document\}}$.
% Authors using \textbf{Word} must edit the header of the document themselves.

% \section{Format of the Paper}

% All submissions must follow the specified format.

% \subsection{Dimensions}




% The text of the paper should be formatted in two columns, with an
% overall width of 6.75~inches, height of 9.0~inches, and 0.25~inches
% between the columns. The left margin should be 0.75~inches and the top
% margin 1.0~inch (2.54~cm). The right and bottom margins will depend on
% whether you print on US letter or A4 paper, but all final versions
% must be produced for US letter size.
% Do not write anything on the margins.

% The paper body should be set in 10~point type with a vertical spacing
% of 11~points. Please use Times typeface throughout the text.

% \subsection{Title}

% The paper title should be set in 14~point bold type and centered
% between two horizontal rules that are 1~point thick, with 1.0~inch
% between the top rule and the top edge of the page. Capitalize the
% first letter of content words and put the rest of the title in lower
% case.

% \subsection{Author Information for Submission}
% \label{author info}

% ICML uses double-blind review, so author information must not appear. If
% you are using \LaTeX\/ and the \texttt{icml2025.sty} file, use
% \verb+\icmlauthor{...}+ to specify authors and \verb+\icmlaffiliation{...}+ to specify affiliations. (Read the TeX code used to produce this document for an example usage.) The author information
% will not be printed unless \texttt{accepted} is passed as an argument to the
% style file.
% Submissions that include the author information will not
% be reviewed.

% \subsubsection{Self-Citations}

% If you are citing published papers for which you are an author, refer
% to yourself in the third person. In particular, do not use phrases
% that reveal your identity (e.g., ``in previous work \cite{langley00}, we
% have shown \ldots'').

% Do not anonymize citations in the reference section. The only exception are manuscripts that are
% not yet published (e.g., under submission). If you choose to refer to
% such unpublished manuscripts \cite{anonymous}, anonymized copies have
% to be submitted
% as Supplementary Material via OpenReview\@. However, keep in mind that an ICML
% paper should be self contained and should contain sufficient detail
% for the reviewers to evaluate the work. In particular, reviewers are
% not required to look at the Supplementary Material when writing their
% review (they are not required to look at more than the first $8$ pages of the submitted document).

% \subsubsection{Camera-Ready Author Information}
% \label{final author}

% If a paper is accepted, a final camera-ready copy must be prepared.
% %
% For camera-ready papers, author information should start 0.3~inches below the
% bottom rule surrounding the title. The authors' names should appear in 10~point
% bold type, in a row, separated by white space, and centered. Author names should
% not be broken across lines. Unbolded superscripted numbers, starting 1, should
% be used to refer to affiliations.

% Affiliations should be numbered in the order of appearance. A single footnote
% block of text should be used to list all the affiliations. (Academic
% affiliations should list Department, University, City, State/Region, Country.
% Similarly for industrial affiliations.)

% Each distinct affiliations should be listed once. If an author has multiple
% affiliations, multiple superscripts should be placed after the name, separated
% by thin spaces. If the authors would like to highlight equal contribution by
% multiple first authors, those authors should have an asterisk placed after their
% name in superscript, and the term ``\textsuperscript{*}Equal contribution"
% should be placed in the footnote block ahead of the list of affiliations. A
% list of corresponding authors and their emails (in the format Full Name
% \textless{}email@domain.com\textgreater{}) can follow the list of affiliations.
% Ideally only one or two names should be listed.

% A sample file with author names is included in the ICML2025 style file
% package. Turn on the \texttt{[accepted]} option to the stylefile to
% see the names rendered. All of the guidelines above are implemented
% by the \LaTeX\ style file.

% \subsection{Abstract}

% The paper abstract should begin in the left column, 0.4~inches below the final
% address. The heading `Abstract' should be centered, bold, and in 11~point type.
% The abstract body should use 10~point type, with a vertical spacing of
% 11~points, and should be indented 0.25~inches more than normal on left-hand and
% right-hand margins. Insert 0.4~inches of blank space after the body. Keep your
% abstract brief and self-contained, limiting it to one paragraph and roughly 4--6
% sentences. Gross violations will require correction at the camera-ready phase.

% \subsection{Partitioning the Text}

% You should organize your paper into sections and paragraphs to help
% readers place a structure on the material and understand its
% contributions.

% \subsubsection{Sections and Subsections}

% Section headings should be numbered, flush left, and set in 11~pt bold
% type with the content words capitalized. Leave 0.25~inches of space
% before the heading and 0.15~inches after the heading.

% Similarly, subsection headings should be numbered, flush left, and set
% in 10~pt bold type with the content words capitalized. Leave
% 0.2~inches of space before the heading and 0.13~inches afterward.

% Finally, subsubsection headings should be numbered, flush left, and
% set in 10~pt small caps with the content words capitalized. Leave
% 0.18~inches of space before the heading and 0.1~inches after the
% heading.

% Please use no more than three levels of headings.

% \subsubsection{Paragraphs and Footnotes}

% Within each section or subsection, you should further partition the
% paper into paragraphs. Do not indent the first line of a given
% paragraph, but insert a blank line between succeeding ones.

% You can use footnotes\footnote{Footnotes
% should be complete sentences.} to provide readers with additional
% information about a topic without interrupting the flow of the paper.
% Indicate footnotes with a number in the text where the point is most
% relevant. Place the footnote in 9~point type at the bottom of the
% column in which it appears. Precede the first footnote in a column
% with a horizontal rule of 0.8~inches.\footnote{Multiple footnotes can
% appear in each column, in the same order as they appear in the text,
% but spread them across columns and pages if possible.}

% \begin{figure}[ht]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=\columnwidth]{icml_numpapers}}
% \caption{Historical locations and number of accepted papers for International
% Machine Learning Conferences (ICML 1993 -- ICML 2008) and International
% Workshops on Machine Learning (ML 1988 -- ML 1992). At the time this figure was
% produced, the number of accepted papers for ICML 2008 was unknown and instead
% estimated.}
% \label{icml-historical}
% \end{center}
% \vskip -0.2in
% \end{figure}

% \subsection{Figures}

% You may want to include figures in the paper to illustrate
% your approach and results. Such artwork should be centered,
% legible, and separated from the text. Lines should be dark and at
% least 0.5~points thick for purposes of reproduction, and text should
% not appear on a gray background.

% Label all distinct components of each figure. If the figure takes the
% form of a graph, then give a name for each axis and include a legend
% that briefly describes each curve. Do not include a title inside the
% figure; instead, the caption should serve this function.

% Number figures sequentially, placing the figure number and caption
% \emph{after} the graphics, with at least 0.1~inches of space before
% the caption and 0.1~inches after it, as in
% \cref{icml-historical}. The figure caption should be set in
% 9~point type and centered unless it runs two or more lines, in which
% case it should be flush left. You may float figures to the top or
% bottom of a column, and you may set wide figures across both columns
% (use the environment \texttt{figure*} in \LaTeX). Always place
% two-column figures at the top or bottom of the page.

% \subsection{Algorithms}

% If you are using \LaTeX, please use the ``algorithm'' and ``algorithmic''
% environments to format pseudocode. These require
% the corresponding stylefiles, algorithm.sty and
% algorithmic.sty, which are supplied with this package.
% \cref{alg:example} shows an example.

% \begin{algorithm}[tb]
%    \caption{Bubble Sort}
%    \label{alg:example}
% \begin{algorithmic}
%    \STATE {\bfseries Input:} data $x_i$, size $m$
%    \REPEAT
%    \STATE Initialize $noChange = true$.
%    \FOR{$i=1$ {\bfseries to} $m-1$}
%    \IF{$x_i > x_{i+1}$}
%    \STATE Swap $x_i$ and $x_{i+1}$
%    \STATE $noChange = false$
%    \ENDIF
%    \ENDFOR
%    \UNTIL{$noChange$ is $true$}
% \end{algorithmic}
% \end{algorithm}

% \subsection{Tables}

% You may also want to include tables that summarize material. Like
% figures, these should be centered, legible, and numbered consecutively.
% However, place the title \emph{above} the table with at least
% 0.1~inches of space before the title and the same after it, as in
% \cref{sample-table}. The table title should be set in 9~point
% type and centered unless it runs two or more lines, in which case it
% should be flush left.

% % Note use of \abovespace and \belowspace to get reasonable spacing
% % above and below tabular lines.

% \begin{table}[t]
% \caption{Classification accuracies for naive Bayes and flexible
% Bayes on various data sets.}
% \label{sample-table}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{lcccr}
% \toprule
% Data set & Naive & Flexible & Better? \\
% \midrule
% Breast    & 95.9$\pm$ 0.2& 96.7$\pm$ 0.2& $\surd$ \\
% Cleveland & 83.3$\pm$ 0.6& 80.0$\pm$ 0.6& $\times$\\
% Glass2    & 61.9$\pm$ 1.4& 83.8$\pm$ 0.7& $\surd$ \\
% Credit    & 74.8$\pm$ 0.5& 78.3$\pm$ 0.6&         \\
% Horse     & 73.3$\pm$ 0.9& 69.7$\pm$ 1.0& $\times$\\
% Meta      & 67.1$\pm$ 0.6& 76.5$\pm$ 0.5& $\surd$ \\
% Pima      & 75.1$\pm$ 0.6& 73.9$\pm$ 0.5&         \\
% Vehicle   & 44.9$\pm$ 0.6& 61.5$\pm$ 0.4& $\surd$ \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}

% Tables contain textual material, whereas figures contain graphical material.
% Specify the contents of each row and column in the table's topmost
% row. Again, you may float tables to a column's top or bottom, and set
% wide tables across both columns. Place two-column tables at the
% top or bottom of the page.

% \subsection{Theorems and such}
% The preferred way is to number definitions, propositions, lemmas, etc. consecutively, within sections, as shown below.
% \begin{definition}
% \label{def:inj}
% A function $f:X \to Y$ is injective if for any $x,y\in X$ different, $f(x)\ne f(y)$.
% \end{definition}
% Using \cref{def:inj} we immediate get the following result:
% \begin{proposition}
% If $f$ is injective mapping a set $X$ to another set $Y$, 
% the cardinality of $Y$ is at least as large as that of $X$
% \end{proposition}
% \begin{proof} 
% Left as an exercise to the reader. 
% \end{proof}
% \cref{lem:usefullemma} stated next will prove to be useful.
% \begin{lemma}
% \label{lem:usefullemma}
% For any $f:X \to Y$ and $g:Y\to Z$ injective functions, $f \circ g$ is injective.
% \end{lemma}
% \begin{theorem}
% \label{thm:bigtheorem}
% If $f:X\to Y$ is bijective, the cardinality of $X$ and $Y$ are the same.
% \end{theorem}
% An easy corollary of \cref{thm:bigtheorem} is the following:
% \begin{corollary}
% If $f:X\to Y$ is bijective, 
% the cardinality of $X$ is at least as large as that of $Y$.
% \end{corollary}
% \begin{assumption}
% The set $X$ is finite.
% \label{ass:xfinite}
% \end{assumption}
% \begin{remark}
% According to some, it is only the finite case (cf. \cref{ass:xfinite}) that is interesting.
% \end{remark}
% %restatable

% \subsection{Citations and References}

% Please use APA reference format regardless of your formatter
% or word processor. If you rely on the \LaTeX\/ bibliographic
% facility, use \texttt{natbib.sty} and \texttt{icml2025.bst}
% included in the style-file package to obtain this format.

% Citations within the text should include the authors' last names and
% year. If the authors' names are included in the sentence, place only
% the year in parentheses, for example when referencing Arthur Samuel's
% pioneering work \yrcite{Samuel59}. Otherwise place the entire
% reference in parentheses with the authors and year separated by a
% comma \cite{Samuel59}. List multiple references separated by
% semicolons \cite{kearns89,Samuel59,mitchell80}. Use the `et~al.'
% construct only for citations with three or more authors or after
% listing all authors to a publication in an earlier reference \cite{MachineLearningI}.

% Authors should cite their own work in the third person
% in the initial version of their paper submitted for blind review.
% Please refer to \cref{author info} for detailed instructions on how to
% cite your own papers.

% Use an unnumbered first-level section heading for the references, and use a
% hanging indent style, with the first line of the reference flush against the
% left margin and subsequent lines indented by 10 points. The references at the
% end of this document give examples for journal articles \cite{Samuel59},
% conference publications \cite{langley00}, book chapters \cite{Newell81}, books
% \cite{DudaHart2nd}, edited volumes \cite{MachineLearningI}, technical reports
% \cite{mitchell80}, and dissertations \cite{kearns89}.

% Alphabetize references by the surnames of the first authors, with
% single author entries preceding multiple author entries. Order
% references for the same authors by year of publication, with the
% earliest first. Make sure that each reference includes all relevant
% information (e.g., page numbers).

% Please put some effort into making references complete, presentable, and
% consistent, e.g. use the actual current name of authors.
% If using bibtex, please protect capital letters of names and
% abbreviations in titles, for example, use \{B\}ayesian or \{L\}ipschitz
% in your .bib file.

% \section*{Accessibility}
% Authors are kindly asked to make their submissions as accessible as possible for everyone including people with disabilities and sensory or neurological differences.
% Tips of how to achieve this and what to pay attention to will be provided on the conference website \url{http://icml.cc/}.

% \section*{Software and Data}

% If a paper is accepted, we strongly encourage the publication of software and data with the
% camera-ready version of the paper whenever appropriate. This can be
% done by including a URL in the camera-ready copy. However, \textbf{do not}
% include URLs that reveal your institution or identity in your
% submission for review. Instead, provide an anonymous URL or upload
% the material as ``Supplementary Material'' into the OpenReview reviewing
% system. Note that reviewers are not required to look at this material
% when writing their review.

% % Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include acknowledgements.  Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.

\section*{Impact Statement}
We introduce the OTTC framework, which improves alignment performance in speech processing by mitigating the peaky behavior of CTC models. Our method shows considerable gains in alignment accuracy across multiple datasets and holds great promise for applications in automatic transcription, speaker diarization, and medical speech analysis. In particular, it can aid clinicians working with ASR tools for patients with conditions like stuttering and dysarthria, where precise temporal alignment enables more reliable transcriptions. This enhanced alignment could help clinicians save time and resources by streamlining speech analysis, ultimately improving the efficiency and accessibility of ASR-based tools in the medical field.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Appendix}
\subsection{Algorithm and Implementation Details}
\label{ax:algo}

\subsubsection{Alignment Computation}
\label{ax:algo1}

The algorithm to compute $\bgamma_{n}^{m,\bbeta}$ is given in Algorithm~\ref{algo:gamma}. This algorithm computes the 1D optimal transport between $\mu[\balpha,n]$ and $\nu[\bbeta,m]$, exploiting the monotonicity of transport in this dimension. To do so the first step consist in sorting the bins which has the complexity $O(n\log n) + O(m\log m)  = O(\max(n,m)\log \max(n,m))$. Then we transfer the probability mass from one distribution to another, moving from the smallest bins to the largest. A useful way to visualize this process is by imagining that the bins of \(\mu\) each contain a pot with a volume of \(a_i\) filled with water, while the bins of \(\nu\) each contain an empty pot with a volume of \(b_j\). The goal is to fill the empty pots of \(\nu\) using the water from the pots of \(\mu\). At any given step of the process, we always transfer water from the smallest non-empty pot of \(\mu\) to the smallest non-full pot of \(\nu\). The volume of water transferred from \(i\) to \(j\) is denoted by \(\gamma_{i,j}\). An example of this process is provided in Figure~\ref{fig:algo1}.

In the worst case, this process requires \(O(n + m)\) comparisons. However, since the bins are already sorted in SOTD, the overall complexity remains \(O(n+m) = O(\max(n , m)\)). In practice, this algorithm is not directly used in this work, as we never compute optimal transport solely; it is provided here to illustrate that the dependencies of $\bgamma_{n}^{m,\bbeta}$ on $\balpha$ are explicit, making it differentiable with respect to $\balpha$. An efficient batched implementation version for computing SOTD will be released soon.



\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{fig_appendix_algo1v2.pdf}
    \caption{\textbf{\textit{1D OT transport computation.}} 
    Illustration of the optimal transport process, computed iteratively by transferring probability mass from the smallest bins to the largest.}
    \label{fig:algo1}
\end{figure}


\begin{algorithm}[h]
\caption{ : \text{Transport Computation}  - $\bgamma_{n}^{m,\bbeta}(\balpha)$ } 
\label{algo:gamma}
\begin{algorithmic} 
\ENSURE Compute $\bgamma_{n}^{m,\bbeta}(\balpha)$.% Build the distance between two discrete distributions  $\mu$ and $\nu$ in $\mathcal{P}(\mathbb{R}^{p})$.
\REQUIRE $\balpha \in \R^{n}$.
\STATE Set $\bgamma \in \R^{n \times m}= \mathbf{0}_{n\times m}$.
\STATE Set $i,j=0$.

% \FOR{each epoch $k \in \{1,\dots,p\}$}
% \STATE Get $\sigma_\mu^k$, $\sigma_\nu^k$ sort permutation of supports vectors $k$-th components.
% \STATE i.e  $\vect{x}_{\sigma_\mu^k(0)}(k) \leq \dots \leq \vect{x}_{\sigma_\mu^k(n-1)}(k)  $ and $\vect{y}_{\sigma_\nu^k(0)}(k) \leq \dots \leq \vect{y}_{\sigma_\nu^k(m-1)}(k) $.
% \STATE Set $T =$ \textit{true}. Set $i,j = 0, 0$.
% \STATE Set $w_\mu,w_\nu = a_{\sigma_\mu^k(0)}, b_{\sigma_\nu^k(0)}$.

\WHILE{$T$ == \textit{True}}
\IF{$\alpha_i < \beta_j$}
\STATE $\bgamma_{i,j} = \beta_j - \alpha_i $
\STATE $i= i+ 1$
\IF{$i == n$}
\STATE $T=$ \textit{false}
\ENDIF
\STATE$\beta_j = \beta_j - \alpha_i$
\ELSE
\STATE $\bgamma_{i,j} = \alpha_i - \beta_j $

\STATE $j= j+ 1$
\IF{$j == m$}
\STATE $T=$ \textit{false}
\ENDIF
\STATE$\alpha_i = \alpha_i - \beta_j$
\ENDIF
\ENDWHILE
% \ENDFOR
\STATE \textbf{Return} \(\bgamma\)
\end{algorithmic}
\end{algorithm}

\subsection{Properties of OTTC}
\label{ax:properties}
Here can be found proof and more insight about the properties of SOTD, $\mathcal{S}_{r}$.

\subsubsection{Lemma 1~: Bijectivity}
\label{ax:properties_l1}

\textbf{Proof of Lemma 1.} \textit{Surjectivity}: The surjectivity come from definition of $\Gamma^{\boldsymbol{*,\beta}}[n]$. \textit{Injectivity}: Suppose $\bgamma_n^{m,\bbeta}(\balpha) = \bgamma_n^{m,\bbeta}(\boldsymbol{\sigma})$, so $\balpha = [\sum_{j=1}^m\bgamma_n^{m,\bbeta}(\balpha)_{i,j}, \dots, \sum_{j=1}^m\bgamma_n^{m,\bbeta}(\balpha)_{i,j} ]^T = [\sum_{j=1}^m\bgamma_n^{m,\bbeta}(\boldsymbol{\sigma})_{i,j}, \dots, \sum_{j=1}^m\bgamma_n^{m,\bbeta}(\boldsymbol{\sigma})_{i,j} ]^T = \boldsymbol{\sigma}$ (because $\bgamma_n^{m,\bbeta}(\balpha) \in \Gamma^{\balpha,\bbeta}$ and $\bgamma_n^{m,\bbeta}(\boldsymbol{\sigma}) \in \Gamma^{\boldsymbol{\sigma},\bbeta}$), which conclude the proof.

\subsubsection{Proposition 1~: Discrete Monotonic Alignment Approximation Equivalence.}
\label{ax:properties_p1}
\textbf{Proof of proposition 1}. Let's consider the following proposition $P(k)$~:


\begin{equation}
 P(k): \exists \balpha^i \in \Delta^n ,  \forall i, \forall j \leq k,(i,j) \in \boldsymbol{A} \Longleftrightarrow  \bgamma_n^{m,\bbeta}(\balpha^i)_{i,j} >0.
\end{equation}


% \textbf{Initialisation - $P(1)$.} $P(1)$ is true. Consider the set $E_1 = \{j \ | \ (1,j) \in \mathbf{A}\}$, which can be written as $E_1 = \{1, 2, \dots, \max(E_1)\}$ since $A$ is a discrete monotonic alignment. Define $\balpha^1 = [\beta_1, \dots, \beta_{\max(E_1)}, \dots]^T$ were the remaining component are choosen to sum to 1.

% Since the alignment $\gamma_n^{m,\bbeta}$ is computed monotonically (see Appendix~\ref{ax:algo1}), $\bgamma_n^{m,\bbeta}(\balpha^1)_{1,j} > 0$ if and only if $\alpha^1_1 \leq \beta_1 + \dots + \beta_j$, which corresponds exactly to the set of indices $j \in E_1$, \textit{i.e.}, the set of aligned indices in $\mathbf{A}$. This proves $P(1)$.

% \textbf{Heredity - $P(k) \Rightarrow P(k+1)$.} The proof follows similarly to $P(1)$. Except that, we need to consider $E_{k+1} = \{j \ | \ (k+1,j)  \in \mathbf{A}\} = \{\max(E_{k}),\max(E_{k})+1,\dots,\max(E_{k})+ |E_{k+1}|\}$ and $\balpha^{k+1} = [\beta_1, \dots, \beta_{\max(E_{k+1}) }, \dots]^T$.

% By induction, the proposition holds for all $n$, therefore the proposition 1 (\textit{i.e.}, $P(n)$) is true. The $\alpha$ verifying the condition is simply~:
% $$\balpha = [\sum_{ j \in E_{1}} \beta_j,\dots,\sum_{ j \in E_{n}} \beta_j]^T$$


\textbf{Initialisation - $P(1)$.} $P(1)$ is true. Consider the set $E_1 = \{j \in \llbracket 1, m\rrbracket \ | \ (1,j) \in \mathbf{A}\}$, which can be written as $E_1 = \{1, 2, \dots, \max(E_1)\}$ since $A$ is a discrete monotonic alignment. Define $\balpha^1 = [\sum_{j \in E_1} \beta_j, \dots]^T$, where the remaining coefficients are chosen to sum to 1.

Since the alignment $\gamma_n^{m,\bbeta}$ is computed monotonically (see Appendix~\ref{ax:algo1}), $\bgamma_n^{m,\bbeta}(\balpha^1)_{1,j} > 0$ if and only if $\alpha^1_1 \leq \beta_1 + \dots + \beta_j$, which corresponds exactly to the set of indices $j \in E_1$, \textit{i.e.}, the aligned indices in $\mathbf{A}$. This proves $P(1)$.

\textbf{Heredity - $P(k) \Rightarrow P(k+1)$.} The proof follows similarly to $P(1)$. However two cases need to be considered~:
\begin{itemize}
    \item When $(k+1,\max(E_k)) \in \mathbf{A}$, in this cases we must consider $E_{k+1} = \{j \in \llbracket 1, m\rrbracket | \ (k+1,j) \in \mathbf{A}\} = \{\max(E_{k}) =  \min(E_{k+1}), \min(E_{k+1})+1, \dots, \max(E_{k+1})\}$ (because $\bbeta$ has no components) and define  $\balpha^{k+1} = [\alpha^{1}_1, \dots, \alpha^{k}_k - \frac{\beta_{\max(E_{k})}}{2}, \sum_{j \in E_{k+1}} \beta_j - \frac{\beta_{\max(E_{k})}}{2} , \dots]^T$, where the remaining parameters are chosen to sum to 1.
    \item When $(k+1,\max(E_k)) \notin \mathbf{A}$, we must consider $E_{k+1} = \{j \in \llbracket 1, m\rrbracket | \ (k+1,j) \in \mathbf{A}\} = \{\max(E_{k}) \neq  \min(E_{k+1}), \min(E_{k+1})+1, \dots, \max(E_{k+1})\}$ (because $\bbeta$ has no components) and define  $\balpha^{k+1} = [\alpha^{1}_1, \dots, \alpha^{k}_k , \sum_{j \in E_{k+1}} \beta_j, \dots]^T$, where the remaining parameters are chosen to sum to 1.

\end{itemize}

By induction, the proposition holds for all $n$. Therefore, Proposition 1 (\textit{i.e.}, $P(n)$) is true. An $\balpha$ verifying the condition is~:
$$\balpha = [\alpha_1^1,\dots,\alpha_n^n]^T$$

\subsubsection{Proposition 2~:Validity of SOTD definition}
\label{ax:properties_p2}
\textbf{Proof of proposition 2.} \quad Since $\bgamma_n^{m,\bbeta}$ is differentiable so continuous, it follows that $\balpha \mapsto  \sum_{i,j=1}^{n,m} \bgamma_n^{m,\bbeta}(\balpha)_{i,j} \cdot C(\vx_i,\vy_j)$ is continuous over $\Delta^n$. Given that $\Delta^n$ is a compact set and every continuous function on a compact space is bounded and attains its bounds, the existence of an optimal solution $\balpha^{*}$ follows. 

\textbf{Non-unicity of the solution.} The non unicity come from that if their is a solution $\balpha^*$ and two integer $k$, $l$ such that $\bgamma_n^{m,\bbeta}(\balpha^*)_{k,l} \geq \epsilon > 0 $  and $\bgamma_n^{m,\bbeta}(\balpha^*)_{k+1,l} \geq \epsilon > 0 $  and  $C(\vx_k,\vy_l) =  C(\vx_{k+1},\vy_{l})$, therefore the transport $\hat\gamma$ such that~:
 
 \begin{itemize}
     \item $\forall i\in[\!|1,n|\!], j, \in [\!|1,m|\!], (i,j) \neq (k,l)$ , $\hat\gamma_{i,j} = \bgamma_n^{m,\bbeta}(\balpha^*)_{i,j} $.
     \item  $\hat\gamma_{k,l} = \bgamma_n^{m,\bbeta}(\balpha^*)_{k,l} - \epsilon/2$
     \item  $\hat\gamma_{k+1,l} = \bgamma_n^{m,\bbeta}(\balpha^*)_{k+1,l} + \epsilon/2$
 \end{itemize}
  

provide a distinct solution. Let's denote $\boldsymbol{\sigma} = \{\bgamma_n^{m,\bbeta}\}^{-1}(\hat\gamma_{i,j})$. % [\sum_{j=1}^m \boldsymbol{\hat\gamma}_{1,j},\dots,\sum_{j=1}^m \boldsymbol{\hat\gamma}_{n,j}]^T$. %Let show that $\boldsymbol{\sigma} \neq \boldsymbol{\alpha^*}$ and $\boldsymbol{\sigma}$ is a solution.
First $\boldsymbol{\sigma} \neq \balpha$ because $\sigma_k = \sum_{l=1}^{m}  \hat\gamma_{k,l} =  \sum_{l=1}^{m} \bgamma_n^{m,\bbeta}(\balpha^{*})_{k,l} - \epsilon/2= \alpha^{*}_k - \epsilon/2$. Second, it's clear that $\sum_{i,j=1}^{n,m} \bgamma_n^{m,\bbeta}(\balpha^{*})_{i,j} \cdot C(\vx_i,\vy_j) =  \sum_{i,j=1}^{n,m} \bgamma^{m,\bbeta_n}(\boldsymbol{\sigma})_{i,j} \cdot C(\vx_i,\vy_j)$. Then $\boldsymbol{\sigma}$ is distinct solution.


\subsubsection{Proposition 3~: SOTD is a pseudo Metric}
\label{ax:properties_p3}

\textbf{Proof of proposition 3.} \quad \textit{\textbf{Pseudo-separation.}} It's clear that $\mathcal{S}_{r}(\{\vx\}_n, \{\vx\}_n) = 0 $, this value is attained for $\alpha^{*} = \bbeta_n$; where the corresponding alignment $\bgamma_n^{n,\bbeta_n}(\balpha^{*})$ corresponds to a one-to-one alignment. Since the two sequences are identical, all the costs are zero.


\textit{\textbf{Symmetry}}. We have $\mathcal{S}_{r}(\{\vx\}_n,\{\vy\}_mm) = \mathcal{S}_{r}(\{\vy\}_m,\{\vx\}_n)$ because the expression for $\mathcal{S}_{r}$ in Eq.~\ref{eq:SOTD} is symmetric. Specifically, because % note that $\bgamma_n^{m,\bbeta} = \bgamma_{\max{(n,m)}}^{\min(n,m),\bbeta}$ and that 
$C$ is symmetric as it is a metric.

\textit{\textbf{Triangular inequality.}} Consider three sequences $\{\vx\}_n$, $\{\vy\}_m$ and $\{\vz\}_o$. Let $p = \max (n,m)$,  $q = \min (n,m)$,  $u = \max (m,o)$, $v = \min (m,o)$. Define the optimal alignments $\bgamma_p^{q,\bbeta_q}(\balpha^{*})$ between $\{\vx\}_n$ and $\{\vy\}_m$; and $\bgamma_u^{v,\bbeta_v}(\boldsymbol{\rho}^{*})$ between $\{\vy\}_m$ and $\{\vz\}_o$. $\forall i \in [\!|1,n|\!], \forall j,k \in [\!|1,m|\!], \forall l \in [\!|1,o|\!]$, we define~:

\begin{equation}
    \gamma^{xy}_{i,j} = \left\{
    \begin{array}{ll}
        \bgamma_p^{q,\bbeta_q}(\balpha^{*})_{i,j} & \mbox{if } n \geq m \\
        \bgamma_p^{q,\bbeta_q}(\balpha^{*})_{j,i} & \mbox{otherwise.}
    \end{array} 
\right. 
\end{equation}


\begin{equation}
    \gamma^{yz}_{k,l} = \left\{
    \begin{array}{ll}
        \bgamma_u^{v,\bbeta_v}(\boldsymbol{\rho}^{*})_{k,l} & \mbox{if } k \geq l \\
        \bgamma_u^{v,\bbeta_v}(\boldsymbol{\rho}^{*})_{l,k} & \mbox{otherwise.}
    \end{array} 
\right. 
\end{equation}


\begin{equation}
    \gamma^{yy}_{j,k} =
        \bgamma_p^{q,\boldsymbol{\sigma}^{*}}(\bbeta_q)_{j,k}
\end{equation}

and we define~: 

\begin{equation}
    b_j = 
    \left\{
    \begin{array}{ll}
        \sum_{i=1}^n \gamma^{xy}_{i,j}& \mbox{if } > 0 \\
        1 & \mbox{otherwise.}
    \end{array} 
\right. 
\end{equation}

\begin{equation}
    c_{k} = \left\{
    \begin{array}{ll}
        \sum_{l=1}^o \gamma^{yz}_{k,l} & \mbox{if } > 0 \\
        1 & \mbox{otherwise.}
    \end{array} 
\right. 
\end{equation}

So $\gamma^{xy}$ is the optimal transport between $\mu[\boldsymbol{\alpha^{*}},p]$ and $\nu[\boldsymbol{\beta}_q, q]$;  
$\gamma^{yy}$ is the optimal transport between $\mu[\bbeta_q,q]$ and $\nu[\boldsymbol{\sigma}^{*}, u]$ and  
$\gamma^{yz}$ is the optimal transport between $\mu[\boldsymbol{\sigma}^{*},u]$ and $\nu[\bbeta_v, v]$, since in 1D optimal transport can be composed, the composition    $\frac{\gamma^{xy}_{i,j} \gamma^{yy}_{j,k} \gamma^{yz}_{k,l} }{ b_j c_k}$ is an optimal transport between $\mu[\boldsymbol{\alpha^{*}},p]$ and  $\nu[\bbeta_v, v]$. Therefore by bijectivity of $\bgamma_{\max(p,v)}^{\min(p,v),\bbeta_{\min(p,v)}}$, there is a $\boldsymbol\theta \in \R^{\max(p,v)}$ such that~:

\begin{equation}
\label{eq:comp}
\bgamma_{\max(p,v)}^{\min(p,v),\bbeta_{\min(p,v)}}(\boldsymbol\theta) = \frac{\gamma^{xy}_{i,j} \gamma^{yy}_{j,k} \gamma^{yz}_{k,l} }{ b_j c_k}
\end{equation}

Thus, by the definition of $\mathcal{S}_{r}(\{\vx\}_n,\{\vz\}_o)$:


\begin{equation}
       \mathcal{S}_{r}(\{\vx\}_n,\{\vz\}_o) \leq \Big( \sum_{i,l=1}^{n,o} \sum_{j,
       k=1}^{m,m} \bgamma_{\max(p,v)}^{\min(p,v),\bbeta_{\min(p,v)}}(\boldsymbol\theta) \cdot C(\vx_i,\vz_l)^r \Big) ^{1/r}
\end{equation}

\begin{equation}
       \mathcal{S}_{r}(\{\vx\}_n,\{\vz\}_o) \leq \Big( \sum_{i,l=1}^{n,o} \sum_{j,
       k=1}^{m,m} \frac{\gamma^{xy}_{i,j} \gamma^{yy}_{j,k} \gamma^{yz}_{k,l} }{ b_j c_k} \cdot C(\vx_i,\vz_l)^r \Big) ^{1/r}
\end{equation}


\begin{equation}
       \mathcal{S}_{r}(\{\vx\}_n,\{\vz\}_o) \leq \Big( \sum_{i,l=1}^{n,o} \sum_{j,
       k=1}^{m,m} \frac{\gamma^{xy}_{i,j} \gamma^{yy}_{j,k} \gamma^{yz}_{k,l} }{ b_j c_k} \cdot (C(\vx_i,\vy_j) + C(\vy_j,\vy_k) + C(\vy_k,\vz_l) ) ^r \Big) ^{1/r}
\end{equation}

Applying the Minkowski inequality:

\begin{align}
       \mathcal{S}_{r}(\{\vx\}_n,\{\vz\}_o) \leq &\Big( \sum_{i,l=1}^{n,o} \sum_{j,
       k=1}^{m,m} \frac{\gamma^{xy}_{i,j} \gamma^{yy}_{j,k} \gamma^{yz}_{k,l} }{ b_j c_k} \cdot (C(\vx_i,\vy_j)  ) ^r \Big) ^{1/r} +\\ &\Big( \sum_{i,l=1}^{n,o} \sum_{j,
       k=1}^{m,m} \frac{\gamma^{xy}_{i,j} \gamma^{yy}_{j,k} \gamma^{yz}_{k,l} }{ b_j c_k} \cdot (C(\vy_j,\vy_k)  ) ^r \Big) ^{1/r} +\\ &\Big( \sum_{i,l=1}^{n,o} \sum_{j,
       k=1}^{m,m} \frac{\gamma^{xy}_{i,j} \gamma^{yy}_{j,k} \gamma^{yz}_{k,l} }{ b_j c_k} \cdot (C(\vy_k,\vz_l)  ) ^r \Big) ^{1/r}
\end{align}

Then~:

\begin{align}
       \mathcal{S}_{r}(\{\vx\}_n,\{\vz\}_o) \leq &\Big( \sum_{i,j=1}^{n,m}  \gamma^{xy}_{i,j}   \cdot C(\vx_i,\vy_j) ^r \Big) ^{1/r} +\\ &\Big( \sum_{j,
       k=1}^{m,m} \gamma^{yy}_{j,k}   \cdot C(\vy_j,\vy_k)  ^r \Big) ^{1/r} +\\ &\Big( \sum_{ k,l=1}^{m,  o}  \gamma^{yz}_{k,l}  \cdot C(\vy_k,\vz_l)   ^r \Big) ^{1/r}
\end{align}

By definition~:
\begin{align}
       \mathcal{S}_{r}(\{\vx\}_n,\{\vz\}_o) \leq \mathcal{S}_{r}(\{\vx\}_n,\{\vy\}_m) + \mathcal{S}_{r}(\{\vy\}_m,\{\vy\}_m) + \mathcal{S}_{r}(\{\vy\}_m,\{\vz\}_o)
\end{align}

So finally since $\mathcal{S}_{r}(\{\vy\}_m,\{\vy\}_m) = 0$, the triangular inequality holds~:

\begin{align}
       \mathcal{S}_{r}(\{\vx\}_n,\{\vz\}_o) \leq \mathcal{S}_{r}(\{\vx\}_n,\{\vy\}_m) + \mathcal{S}_{r}(\{\vy\}_m,\{\vz\}_o).
\end{align}

This concludes the proof. 

\textbf{Note:} If $\bbeta$'s depends on $\{\vx\}_n$, $\{\vy\}_m$ and $\{\vz\}_m$, we need to introduce the appropriate $\gamma^{zz}$
 to construct the composition in Equation~\ref{eq:comp}, ensuring the proof remains valid.
 

\subsubsection{Proposition 4~: Non-separation condition}
\label{ax:properties_p4}
\textit{Proof.} Suppose $\mathcal{S}_{r}(\{\vx\}_n,\{\vy\}_m) = 0$, and $\mathcal{A}(\mathcal{P}_{\alpha^{*}}(\{\vx\}_n)) \neq \mathcal{A}(\{\vy\}_n)$. So~:%Without loss of generality let's suppose that $n\geq m$, so~:

\begin{align}
  \sum_{i,j=1}^{n,m} \bgamma_n^{m,\bbeta}(\balpha^{*})_{i,j} \cdot C(\vx_i,\vy_j)^r  = 0
   %\label{eq:SOTD}
 \end{align}

 Let $\mathcal{A}_{\{\vx\}_n}$ denote the aggregation operator on $\Delta^n$, which groups indices where consecutive elements in $\{\vx\}_n$ are identical (i.e, $\mathcal{A}([{\dots, \alpha_i, \dots,\alpha_{i+k}, \dots}]^T) = [{\dots, \alpha_i + \dots +\alpha_{i+k}, \dots}]^T$ iff $\vx_i = \dots = \vx_{i+k}$). By expanding the right term, we show that; $\forall \balpha \in \Delta^n$~:

\begin{align}
  \sum_{i,j=1}^{n,m} \bgamma_n^{m,\bbeta}(\balpha)_{i,j} \cdot C(\vx_i,\vy_j)^r      =   \sum_{i,j=1}^{n,m} \bgamma_n^{m,\boldsymbol{\mathcal{A}_{\{\vy\}_m}(\beta)}}(\mathcal{A}_{\{\vx\}_n}(\balpha))_{i,j} \cdot C(\mathcal{A}(\mathcal{P}_{\balpha}(\{\vx\}_n)),\mathcal{A}(\{\vy\}_n))^r  
 \end{align}

Therefore~:
\begin{align}
 \sum_{i,j=1}^{n,m} \bgamma_n^{m,\boldsymbol{\mathcal{A}_{\{\vy\}_m}(\beta)}}(\mathcal{A}_{\mathcal{P}_{\alpha}\{\vx\}_n}(\balpha^{*}))_{i,j} \cdot C(\mathcal{A}(\mathcal{P}_{\alpha^{*}}(\{\vx\}_n)),\mathcal{A}(\{\vy\}_n))^r   = 0
 \label{eq:null}
 \end{align}

Since $\mathcal{A}(\mathcal{P}_{\alpha^{*}}(\{\vx\}_n)) \neq \mathcal{A}(\{\vy\}_n)$ their is a $k\in[\!|1,m|\![$ such that~:
 
\begin{align}
 \forall k' <k , \mathcal{A}(\{\vx\}_n)_{k'} = \mathcal{A}(\{\vy\}_n)_{k'} \textbf{\:\:\:and\:\:\:} \mathcal{A}(\{\vx\}_n)_{k} \neq\mathcal{A}(\{\vy\}_n)_{k} 
 \end{align}

Because the optimal alignment is monotonous and lead to a 0 cost, necessarily~:

\begin{align}
 \forall k' <k , \mathcal{A}_{\mathcal{P}_{\alpha}(\{\vx\}_n)}(\balpha^{*})_{k'} = \mathcal{A}_{\{\vy\}_m } (\bbeta)_{k'}
 \end{align}
 which is the only way to have alignment between the $k$ first element which led to 0 cost. Because of the monotonicity of $\bgamma_n^{m,\boldsymbol{\mathcal{A}_{\{\vy\}_m}(\beta)}}(\mathcal{A}_{\mathcal{P}_{\alpha}\{\vx\}_n}(\balpha^{*}))$ the next alignment $(s,t)$ is between the next element with a non zeros weights for both sequences. Since $\beta$ has non zero component and by the definition of $\mathcal{P}_{\alpha}$, $s = k$ and $t=k$. Therefore the term $\bgamma_n^{m,\boldsymbol{\mathcal{A}_{\{\vy\}_m}(\beta)}}(\mathcal{A}_{\mathcal{P}_{\alpha^{*}}(\{\vx\}_n)}(\balpha^{*}))_{k,k}$ is non null and the term~:

 $$\bgamma_n^{m,\boldsymbol{\mathcal{A}_{\{\vy\}_m}(\beta)}}(\mathcal{A}_{\mathcal{P}_{\alpha}\{\vx\}_n}(\balpha^{*})) C(\mathcal{A}(\mathcal{P}_{\alpha^{*}}(\{\vx\}_n) ,\mathcal{A}(\{\vy\}_n)_{k})$$ 

 belong to the sum in depicted in Eq.~\ref{eq:null}. So $C(\mathcal{A}(\mathcal{P}_{\alpha^{*}}(\{\vx\}_n))  ,\mathcal{A}(\{\vy\}_n)_{k}) = 0$ \textit{i.e.}, $\mathcal{A}(\mathcal{P}_{\alpha^{*}}(\{\vx\}_n)) = \mathcal{A}(\{\vy\}_n)_{k}$ because $C$ is separated. Here a contradiction so we can conclude that~:

$$\mathcal{A}(\mathcal{P}_{\alpha^{*}}(\{\vx\}_n)) = \mathcal{A}(\{\vy\}_n)$$.

\subsection{Supplementary Experimental Insights}

% \subsubsection{Expanded Results}
% To further evaluate the proposed OTTC framework, we experimented with Wav2Vec2-large~\cite{baevski2020wav2vec} as the pre-trained model instead of XLSR, following the same LibriSpeech experimental setup described in Section~\ref{sec:exp-setup}. The results shown in Table~\ref{table:results-wav2vec2} indicate that using this pre-trained model further narrows the performance gap between OTTC and CTC.

% \begin{table}[h]
% \centering
% \caption{WER(\%) comparison between the CTC loss-based ASR model and our proposed OTTC loss-based ASR model using Wav2Vec2-large as the pretrained model for the LibriSpeech dataset. Models are trained using the three official training splits with varying amounts of supervised data. Results are reported for the two official test sets.\\}
% \label{table:results-wav2vec2}
% \begin{tabular}{l | cc | cc | cc}
% \toprule
% \multirow{2}{*}{Model} & \multicolumn{2}{c|}{\textbf{100h-LibriSpeech}} & \multicolumn{2}{c|}{\textbf{360h-LibriSpeech}} & \multicolumn{2}{c}{\textbf{960h-LibriSpeech}} \\
% & test-clean & test-other & test-clean & test-other & test-clean & test-other \\
% \midrule
% CTC & 3.36 & 7.36 & 2.77 & 6.58 & 2.20 & 5.23 \\
% \midrule
% OTTC & 3.77 & 8.55 & 3.00 & 7.44 & 2.52 & 6.16 \\
% \bottomrule
% \end{tabular}
% \end{table}

\subsubsection{Ablation Studies}
\label{ax:abla}
This section explores the effects of various design choices and configurations on the performance of the proposed OTTC framework and provides additional insights on its comparison to soft-DTW.

\textbf{Training with single-path alignment from CTC.}
A relevant question that arises is whether the gap between the OTTC and CTC models arises from the use of a single alignment in OTTC rather than marginalizing over all possible alignments. To investigate this, we conducted a comparison with a single-path alignment approach. Specifically, we first obtained the best path (forced alignment using the Viterbi algorithm) from a trained CTC-based model on the same dataset. A new model was then trained to learn this single best path using Cross-Entropy.
On the 360-hour LibriSpeech setup with Wav2Vec2-large as the pre-trained model, this single-path approach achieved a WER of 7.04\% on the test-clean set and 13.03\% on the test-other set. In contrast, under the same setup, the OTTC model achieved considerably better results, with a WER of 3.00\% on test-clean and 7.44\% on test-other (see Table~\ref{table:wer_results}).
These findings indicate that the OTTC model is effective with learning a single alignment, which may be sufficient for achieving competitive ASR performance.

\textbf{Fixed OT weights prediction ($\balpha$).}
We conducted an additional ablation experiment where we replaced the learnable \textit{OT weight prediction head} with fixed and uniform OT weights ($\balpha$). This approach removes the model's ability to search for the best path, assigning instead a frame to the same label during training. Consequently, the model loses the localization of the text-tokens in the audio.
For this experiment, we used the 360-hour LibriSpeech setup with Wav2Vec2-large as the pre-trained model. The results show a WER of 3.51\% on test-clean, compared to 2.77\% for CTC and 3.00\% for OTTC with learnable OT weights. On test-other, the WER was 8.24\%, compared to 6.58\% for CTC and 7.44\% for OTTC with learnable OT weights. These results demonstrate that while using fixed OT weights leads to a slight degradation in performance, the localization property is completely lost, highlighting the importance of learnable OT weights for preserving both performance and localization in the OTTC model.


\textbf{Impact of freezing OT weights prediction head across epochs.}
In our investigations so far, we arbitrarily selected the number of epochs for which the \textit{OT weights prediction head} ($\balpha$ predictor) remained frozen (see Section~\ref{sec:results}), as a hyperparameter without any tuning. To further understand its impact, we conducted additional experiments on the 360h-LibriSpeech setup using the Wav2Vec2-large model while freezing the \textit{OT weights prediction head} for the last 5 and 15 epochs. When frozen for the last 5 epochs, we achieve a WER of 3.01\%, whereas when frozen for the last 15 epochs, the WER is 3.10\%. As shown in the Table~\ref{table:wer_results}, freezing the OT head for the last 10 epochs results in a WER of 3.00\%. Based on these results, it appears that the model’s performance doesn't change considerably when the model is trained for a few more epochs after freezing the alignment part of the OTTC model.
% {\textbf{Additional insights.}}
% {\emph{Training OTTC models.}}
% As described in Section~\ref{sec:exp-setup}, the \textit{OT weights prediction head} ($\balpha$ predictor) remains frozen during the last 10 epochs of training (out of a total of 40 epochs) for the OTTC models.
% In the 960h-LibriSpeech training setup, we observed a WER of $2.89$\% on test-clean at epoch 30 for the OTTC model, resulting in an $13$\% relative reduction by epoch 40.
% Interestingly, when the model is trained for the full 40 epochs without freezing the \textit{OT weights prediction head}, no meaningful improvement in WER is observed between epochs 30 and 40.
% This suggests that the alignment stabilizes early in the training, with the OTTC model learning sufficiently robust alignments by epoch $30$. Consequently, further joint optimization of both the alignment and logit prediction may be unnecessary in the later stages, as the alignment undergoes minimal changes beyond that point.
% However, given the mutual reinforcement between the correctness of alignments and classification in the OTTC loss, we hypothesize that an improved curriculum learning framework \cite{hacohen2019power} could further improve ASR performance, which we leave for future exploration.

% \textbf{Learnable $\bbeta$.}
% To show the importance of making $\bbeta$ learnable, we first experiment with learning $\bbeta$ using a trainable transformer decoder layer with tokenized reference text as input. We observe a degenerate solution in which all label weights ($\bbeta$) are assigned to a single token while all other tokens receive zero label weights, resulting in a WER of 100\%. Intuitively, this behavior is to be expected because the model can learn this shortcut, which still minimizes the OTTC loss (the loss goes to zero) as there are no constraints in the loss to prevent it. Next, we impose a constraint on the learnable $\bbeta$ values, ensuring they cannot fall below a certain threshold. However, we observe a slight degradation in performance, with around 1\% degradation in WER for the 360-hour LibriSpeech setup.

\textbf{Oracle experiment.}
We believe that the proposed OTTC framework has the potential to outperform CTC models by making $\bbeta$ learnable with suitable constraints or by optimizing the choice of static $\bbeta$. To illustrate this potential, we conduct an oracle experiment where we first force-align audio frames and text tokens using a CTC-based model trained on the same data. This alignment is then used to calculate the $\bbeta$ values. For example, given the target sentence $YES$ and the best valid path from the Viterbi algorithm $(\phi Y \phi \phi E E S)$, we re-labeled it to $(\phi Y \phi E S)$ and set $\bbeta = [1/7, 1/7, 2/7, 2/7, 1/7]$. This approach enabled OTTC to learn a uniform distribution for $\balpha$, mimicking CTC's highest probability path. As a result, in both the 100h-LibriSpeech and 360h-LibriSpeech setups, the OTTC model converged much faster and matched the performance of CTC. This experiment underscores the critical role of $\bbeta$, suggesting that a better strategy for its selection or training will lead to further improvements.


\textbf{Comments on soft-DTW.}
In soft-DTW, only the first and last elements of sequences are guaranteed to align, while all in-between frames or targets may be ignored; \textit{i.e.}, there is no guarantee that soft-DTW will yield a discrete monotonic alignment. A ``powerful" transformation $F$ can map $\mathbf{x}$ to $F(${$\mathbf{x}$}$)$ in such a way that soft-DTW ignores the in-between transformed frames ($F(${$\mathbf{x}$}$)$) and targets ({$\mathbf{y}$}), which we refer to as a collapse (Section~\ref{sec:s2sd}). This is why transformations learned through sequence comparison are typically constrained (e.g., to geometric transformations like rotations)~\cite{vayer2022timeseriesalignmentglobal}. 
Since transformer architectures are powerful, they are susceptible to collapse as demonstrated by the following experiment we conducted using soft-DTW as the loss function.
On the 360h-LibriSpeech setup with Wav2Vec2-large model, the best WER achieved using soft-DTW is 39.43\%. In comparison, CTC yields 2.77\% whereas the proposed OTTC yields 3.00\%. A key advantage of our method is that, by construction, such a collapse is not possible.

% Note that DTW depends on a parameter \(\gamma\). The results above were obtained with the default value \(\gamma = 0.1\). This parameter needs to be chosen carefully, as the model's performance highly depends on it. We trained soft-DTW on 100h-LibriSpeech (for time reasons) with \(\gamma = 0.01\), \(\gamma = 0.1\), and \(\gamma = 1\), yielding WER results of 108.13\%, 19.51\%, and 22.47\%, respectively. It can be observed that one \(\gamma\) value led to a collapse of the model, illustrating the risks of this approach. Additionally, the sensitivity to \(\gamma\) is evident: the same \(\gamma\) value resulted in completely different performance between 360h-LibriSpeech and 100h-LibriSpeech, highlighting how alignment depends on the sentences considered with such approach.

\subsubsection{Alignment Analysis}

% \textbf{Peaky behaviour.}
% The peaky behavior of CTC models is characterized by a significant proportion of audio frames being assigned to either the blank symbol or the space symbol (non-alphabet symbols)~\cite{ctc-peaky}. To quantitatively assess the model's peaky behavior, we calculated the average percentage of audio frames assigned to these two special symbols. For the test-clean set, we found that 60.3\% of total frames in CTC models were assigned to these special symbols. In contrast, the OTTC model assigned only 22.9\% of frames to these symbols. This highlights the effectiveness of the alignment achieved by our proposed framework, which decisively avoids the extreme peaky behavior exhibited by CTC models.

% \begin{table}[t]
% \centering
% \caption{Alignment performance metrics for CTC and OTTC models, including Precision, Recall, F1 score, and Intersection Duration Ratio.\\}
% \label{tab:alignment_results}
% \begin{tabular}{l|c c c|c}
% \toprule
% \textbf{Model} & \textbf{Precision (\%)} & \textbf{Recall (\%)} & \textbf{F1 Score (\%)} & \textbf{Intersection Duration Ratio (\%)} \\
% \midrule
% CTC & 84.26 & 83.62 & 83.94 & 17.19 \\
% \midrule
% OTTC & 84.77 & 84.85 & 84.81 & 42.12 \\
% \bottomrule
% \end{tabular}
% \end{table}
% \textbf{Quantitative alignment evaluations.}
% In addition to the peaky behavior, alignment accuracy serves as another crucial evaluation metric. Since ground truth alignments are unavailable, we assess alignment accuracy through forced alignment, a method previously applied to the AMI dataset~\cite{rastorgueva23_interspeech}.
% Following the methodology in~\cite{rastorgueva23_interspeech}, we calculated precision, recall, and F1 score. Note that we only considered word-level timestamps, as they are typically less erroneous than individual phoneme, letter, or sub-word level timestamps.
% As shown in Table~\ref{tab:alignment_results}, the OTTC model shows better alignment performance. However, these metrics do not provide insight into the predicted duration of the words. To address this, we additionally compute the Intersection Duration Ratio. 
% This metric calculates the duration of the overlap between the reference and predicted word segments, dividing it by the total reference duration of those words.
% The results are shown in Table~\ref{tab:alignment_results}. This results highlight that, on average, the CTC model either predicts the start of words with significant delay, or assigns very few audio frames to non-blank symbols, resulting in a peaky behavior.

\textbf{Temporal evolution of alignment.}
An example of the evolution of the alignment in the OTTC model during training for 40 epochs without freezing \textit{OT weights prediction head} is shown in Figure~\ref{fig:ottc-align-evolution2}.
Note that during the initial phase of training, there is significant left/right movement of boundary frames for all groups. As training progresses, the movement typically stabilizes to around 1-2 frames.
While this can be considered ``relatively stable" in terms of alignment, the classification loss (\textit{i.e.}, cross-entropy) in the OTTC framework is still considerably affected by these changes. This change of the loss is what impacts the final performance and the performance difference between freezing or not-freezing the alignments.
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{ottc_alignmntv3.pdf}
\caption{\textbf{\textit{Alignment evolution in the OTTC model during training for 40 epochs without freezing OT weights prediction head ($\balpha$ predictor).}} On the $x$-axis, each pixel corresponds to one audio frame, while the $y$-axis represents the epoch. Frames grouped by tokens are shown in alternating colors (yellow and dark blue), with the boundaries of each group highlighted in light blue/green. One can note that during the initial phase of training, there is significant left/right movement of boundary frames for all groups. As training progresses, the movement typically stabilizes to around 1-2 frames.}
    \label{fig:ottc-align-evolution2}
\end{figure}

 % \section{Failure Modes}
 % \TODO{leading to degenarate solutions in case of learnable label weights}

% \section{Comparison of CTC and OTTC alignments}
% \label{appendix:ctc-ottc-align}
% \begin{figure}[th]
%     \centering
%     \includegraphics[width=1.0\linewidth]{a.pdf}
% %     \caption{\textbf{\textit{Comparison of CTC and OTTC alignments.} }
% % For CTC, the path with highest probability is shown. CTC shows a high occurrence of blank tokens with sparse non-blank assignments, resulting in peaky behavior. OTTC rarely aligns frames to blank tokens, avoiding this peaky pattern.}
%     \label{fig:ctcpeaky2}
% \end{figure}

% \section{OTTC alignments}
% \label{appendix:align}


% \section{You \emph{can} have an appendix here.}

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
