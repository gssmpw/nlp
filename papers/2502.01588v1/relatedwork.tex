\section{Related Work}
\textbf{CTC loss.}
The CTC criterion~\cite{graves2006connectionist} is a versatile method for learning alignments between sequences. This versatility has led to its application across various sequence-to-sequence tasks~\cite{st1,reorder,mtst,mt1,orc,gesture}. However, despite its widespread use, CTC has numerous limitations that impact its effectiveness in real-world applications. 
To address issues such as peaky behavior~\cite{ctc-peaky}, label delay~\cite{tian2022bayes}, and alignment drift~\cite{sak2015learning}, researchers have proposed various extensions. These extensions aim to refine the alignment process, ensuring better performance across diverse tasks.
% Additionally, path-modified extensions \cite{mahadeokar2021alignment} and external alignments in AED models \cite{kim2021reducing} have been explored to improve alignment quality.
Delay-penalized CTC \cite{yao2023delay} and blank symbol regularization \cite{yang2023blank,zhao2022investigating,bluche2015framewise} attempt to mitigate label delay issues. Other works have tried to control alignment through teacher model spikes \cite{align1,align2} or external supervision \cite{externalali,7404851,9003863}, though this increases complexity.
Recent advancements like Bayes Risk CTC offer customizable, end-to-end approaches to improve alignment without relying on external supervision \cite{tian2022bayes}.
% Moreover, discriminative variants like LF-MMI with CTC topology \cite{hadian2018end} and non-autoregressive ASR models \cite{higuchi2020mask} leverage CTC’s capabilities, while topological variants \cite{zhao2022investigating, laptev22_interspeech} and adaptations for partially labeled data like STC and GTC further extend CTC’s application \cite{pratap2022star, moritz2021semi}.

\textbf{Transducer loss.}  
The transducer loss was introduced to address the conditional independence assumption of CTC by incorporating a predictor network \cite{graves2012sequence}.
% that acts as a language model
However, similarly to CTC, transducer models suffer from label delay and peaky behavior~\cite{yu2021fastemit}.
To mitigate these issues, several methods have been proposed, such as e.g., Pruned RNN-T \cite{kuang2022pruned}, which prunes alignment paths before loss computation, FastEmit \cite{yu2021fastemit}, which encourages faster symbol emission, delay-penalized transducers \cite{kang2023delay}, which add a constant delay to all non-blank log-probabilities, and minimum latency training \cite{shinohara2022minimum}, which augments the transducer loss with the expected latency.
Further extensions include CIFTransducer (CIF-T) for efficient alignment \cite{zhang2023say}, self-alignment techniques \cite{kim2021reducing}, and lightweight transducer models using CTC forced alignments \cite{wan24_interspeech}.

Over the years, the CTC and transducer-based ASR models have achieved state-of-the-art performance.
Despite numerous efforts to control alignments and apply path pruning, the fundamental formulation of marginalizing over all valid paths remains unchanged and directly or indirectly contributes to several of the aforementioned limitations.
Instead of marginalizing over all valid paths as in CTC and transducer models, we propose a differential alignment framework based on optimal transport, which can jointly learn a single alignment and perform ASR in an E2E manner.
% Additionally, both our experiments and existing literature suggest that nearby frames in the acoustic embedding space tend to form natural clusters when trained for ASR \cite{}. These insights motivate our proposal of a differentiable alignment framework based on optimal transport.

% \textbf{DTW.} Dynamic Time Warping (DTW) is one the most popular series alignement algorithm. That's why some author tried to use it as a fitting loss. First approach consist in relying on an alternative scheme while another propose to relax DTW to make it differentiable. All theses methods are two drawback they are quadratic and the alignement process is tied with the sequences. Therefore, when fitting a sequences the model can learn a trivial solution. To avoid this one must add either regularization term or carefully constraint the set of transformation driving one of the sequences. The frmaework we will develop here is an answer to theses issues: it's linear, and naturally handle constraint temporal without constraining the set of trandromation.
 




% Dynamic Time Warping (DTW) is one of the most popular algorithms for aligning sequences and serves as the foundation for many existing methods. While widely used in applications such as speech recognition, time series analysis, and bioinformatics, it is not ideal for fitting sequences due to its lack of differentiability. As a result, practitioners often rely on alternative schemes that are prone to getting stuck in local minima and are sensitive to noise. A popular relaxation, soft-DTW, and other differentiable variants have been successfully applied in many tasks. However, these approaches still have limitations, such as computational inefficiency and a lack of guarantees regarding the minimum of the fitting function and the resulting alignment quality.  To overcome theses issues a dubed version have been proposed, whichh solve most of the issue of softDTW,, notable this mesures is null if and only if the X=Y. As it is a desirable property,  on many task it is not, on particular on speech to task task were non relevant frames can be dropped or labels repeated the measure should be able to be both able to drop value and to be invariant to repetitive frames.Additionally  but the cost of computing is still quadratic both for forward and back ward pass. In this work we will porvide a more flexible framework costly (quadratic forward and backward)quadratic, rely on a regularization terme which does not guarantee the temporal alignement and give no guarantee on the nature of the solution.




%Dynamic Time Warping (DTW) is one of the most popular algorithms for aligning sequences serves as the foundation for many existing methods. WHile used in several application such as .., ..., .... It's a bad choice to fit a sequences to another; it's not differentiable and one musrt rely on an alternive scheme which is know to be prone to stuck on local minima and to be sensitive to noise. A popular relaxation soft-DTW and many other variant ... which allow differnetiability has been used on many tasks with success, however it has some limitation such as the computation  there is no guarantee on the minimum of the fitting function and how the series will be, 



% Consider a dataset $\mathbb{S}$ composed of pairs of sequences $(\{\vx\}^n
% , \{\vy\}^m
%  ) \in \mathbb{S}$. The first sequence is a representation of an audio signal, where each vector $\vx_i \in \mathbb{R}^{d}$ represents a time frame of the audio. The second sequence is the textual transcription of the audio, where each element $\vy_i$ belongs to a vocabulary $L$, so $\{\vy\}^m \in L^m$.