\section{Related Work}
% \xiao{need list some important work, vigorous related}
\textbf{Concentration inequalities.}
% The development of concentration inequalities \cite{concentration04}
% % \guoqiang{references?} 
% plays a crucial role in statistical learning and probability theory. A method \cite{jason04} analogous to the Hoffding inequality was introduced to tackle the summation of graph-dependent random variables. Building on this foundation, \citet{usunier2005generalization,ralaivola2015entropy} proposed new concentration inequalities that not only extended the results~\cite{jason04} but also offered broader applications. Furthermore, \citet{ruiray22} provided additional proofs that strengthened the theoretical framework.
The development of concentration inequalities \cite{concentration13initial} is paramount in both statistical learning~\cite{statistics97} and probability theory~\cite{07probability}. A methodology analogous to the Hoeffding inequality was introduced to address the summation of graph-dependent random variables~\cite{jason04}. Building upon this foundational work, \citet{usunier2005generalization} and~\citet{ralaivola2015entropy} proposed new concentration inequalities that not only extended the results of \cite{jason04} but also broadened their applicability. Furthermore, \citet{ruiray22} provided additional proofs that further reinforced the theoretical framework surrounding these inequalities.

% \textbf{Generalization.} In the context of MTL generalization bounds, some studies have utilized tools such as Rademacher Complexity (RC) \cite{maurer06rade,maurer06linear,kakade12}, Vapnik-Chervonenkis (VC) dimension,  \cite{vcoriginal00}
% % \guoqiang{the reference is not accurate and we should give the credit to the original author}
% % \cite{ben08,qi2024multimatch}
% and Covering Numbers (CN) \cite{baxter00,maurer2016benefit}, producing convergence results of $O(\frac{1}{\sqrt{n}})$. Moreover, other research has applied Local Rademacher complexity (LRC) \cite{yousefi18,watkins2023optimistic}, which can achieve a generalization bound of $O(\frac{1}{n})$ under optimal conditions, although these results are typically confined to i.i.d. scenarios. Motivated by \citet{jason04,ralaivola2015entropy,wu2023macro-auc}, this paper introduces a novel concentration inequality applicable to multiple graph-dependent variables and derives the generalization bound 
% % \guoqiang{form?}
% , resulting in several significant theoretical contributions. Specifically, in applications such as Macro-AUC, the new concentration inequality exhibits a convergence rate of $O(\frac{1}{n})$ (see Appendix \ref{section: D} for further details).
\textbf{Generalization.} In the realm of multi-task learning (MTL) generalization bounds, research has leveraged tools such as Rademacher Complexity (RC) \cite{maurer06rade,maurer06linear,kakade12}, Vapnik-Chervonenkis (VC) dimension \cite{vcoriginal00}, and Covering Numbers (CN) \cite{baxter00,maurer2016benefit} to establish convergence results of $O(\frac{1}{\sqrt{n}})$. Furthermore, Local Rademacher complexity (LRC) \cite{yousefi18,watkins2023optimistic} has been applied, achieving a generalization bound of $O(\frac{1}{n})$ under optimal conditions, albeit primarily in independent and identically distributed (i.i.d.) settings. Motivated by the work of \citet{jason04,ralaivola2015entropy,wu2023macro-auc}, this paper introduces a new concentration inequality tailored for multi-graph dependent variables, thereby yielding a risk bound with several significant theoretical implications. In particular, when applied to Macro-AUC Optimization, the proposed concentration inequality exhibits a convergence rate of $O(\frac{1}{n})$ (see Appendix~\ref{section: D} for additional related work).
% \ref{section: F}, \ref{section: G}, \ref{section: MTL gereral bound}, \ref{section: A}, \ref{section: B},  \ref{section: E}, \ref{section: C}, \ref{section: D}.

% \guoqiang{ssss?

% Motivation: Previous theoretical results in $O(\frac{1}{\sqrt{n}})$, how to improve it to $O(\frac{\log n}{n})$, under some condition? 
% Then, how to inspire new learning algorithms?

% What is the problem?

% How to solve it?

% What is the result?

% Technical non-triviality: potential dependent examples -> new concentration inequality + new generalization bound  }


% \cite{usunier2005generalization}



% \xiao{if need to list some related work or not???}