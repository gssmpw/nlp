\section{Related Work}
Legal evaluation datasets have traditionally focused on tasks that language models learn through fine tuning. These datasets, often derived from public online sources or expert annotations, include
tasks such as document review ____, judgment prediction ____, case
summarization ____, information extraction ____, among others ____. Although valuable, they do not fully capture the broader capabilities of LLMs in legal contexts.

Recent efforts have shifted towards developing benchmarks specifically for LLMs. MMLU ____, an English multiple-choice test specifically designed for LLMs, includes a subset of legal questions
useful for preliminary assessments, but not always aligned with specific legal systems or jurisdictions. In contrast, professional certification exams offer more tailored
evaluations ____, but often fall short in comprehensively assessing LLMsâ€™ practical use cases.

LegalBench ____ marks the first collaborative effort to benchmark legal reasoning for American law. It integrates existing and expert-crafted
datasets to assess practical legal reasoning skills, such as issue identification, rule applicability, and
text interpretation. This structured approach helps legal professionals understand the utility and limitations of models. Other benchmarks ____ similarly group tasks to separately evaluate legal knowledge, inference, and application.