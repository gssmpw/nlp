\section{Related Work}
Legal evaluation datasets have traditionally focused on tasks that language models learn through fine tuning. These datasets, often derived from public online sources or expert annotations, include
tasks such as document review \citep{NEURIPS_DATASETS_AND_BENCHMARKS2021_6ea9ab1b,wang-etal-2023-maud}, judgment prediction \citep{chalkidis-etal-2019-neural,malik-etal-2021-ildc}, case
summarization \citep{NEURIPS2022_552ef803}, information extraction \citep{yao-etal-2022-leven}, among others \citep{chalkidis-etal-2022-lexglue,niklaus-etal-2023-lextreme,NEURIPS2022_d15abd14}. Although valuable, they do not fully capture the broader capabilities of LLMs in legal contexts.

Recent efforts have shifted towards developing benchmarks specifically for LLMs. MMLU \citep{hendrycks2021measuring}, an English multiple-choice test specifically designed for LLMs, includes a subset of legal questions
useful for preliminary assessments, but not always aligned with specific legal systems or jurisdictions. In contrast, professional certification exams offer more tailored
evaluations \citep{Zhong_Xiao_Tu_Zhang_Liu_Sun_2020,doi:10.1098/rsta.2023.0254,junior2024jurulegalbrazilianlarge}, but often fall short in comprehensively assessing LLMsâ€™ practical use cases.

LegalBench \citep{guha2023legalbenchcollaborativelybuiltbenchmark} marks the first collaborative effort to benchmark legal reasoning for American law. It integrates existing and expert-crafted
datasets to assess practical legal reasoning skills, such as issue identification, rule applicability, and
text interpretation. This structured approach helps legal professionals understand the utility and limitations of models. Other benchmarks \citep{fei2023lawbenchbenchmarkinglegalknowledge,dai2024laiwchineselegallarge} similarly group tasks to separately evaluate legal knowledge, inference, and application.