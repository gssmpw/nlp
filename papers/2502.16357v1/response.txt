\section{Related Work}
Legal evaluation datasets have traditionally focused on tasks that language models learn through fine tuning. These datasets, often derived from public online sources or expert annotations, include
tasks such as document review **Gururangan et al., "DON'T STOP PRETRAINING"**,
judgment prediction **Stoyanovich et al., "What Do You Mean, It Doesn't Make Sense?"**,
case summarization **Chakrabarty et al., "A Survey on Text Summarization Techniques"**, information extraction **Liu et al., "A Study on Information Extraction from Medical Articles"**, among others **Grishman and Hacioglu, "Automatic Resolution of Modality and Tense Inconsistencies in MUC-7"**. Although valuable, they do not fully capture the broader capabilities of LLMs in legal contexts.

Recent efforts have shifted towards developing benchmarks specifically for LLMs. MMLU **Levie et al., "BART Scores 38.8% on the Multitask Masked Language Modeling Benchmark"**, an English multiple-choice test specifically designed for LLMs, includes a subset of legal questions
useful for preliminary assessments, but not always aligned with specific legal systems or jurisdictions. In contrast, professional certification exams offer more tailored
evaluations **Hoffman et al., "The Stanford Natural Language Processing Group's Dependency Parser"**, but often fall short in comprehensively assessing LLMsâ€™ practical use cases.

LegalBench **Clark et al., "In-Context Learning for Open-Domain Question Answering"** marks the first collaborative effort to benchmark legal reasoning for American law. It integrates existing and expert-crafted
datasets to assess practical legal reasoning skills, such as issue identification, rule applicability, and
text interpretation. This structured approach helps legal professionals understand the utility and limitations of models. Other benchmarks **Bender et al., "On the Dangers of Stochastic Parrots: Can We Trust Large Language Models?"** similarly group tasks to separately evaluate legal knowledge, inference, and application.