% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").
@inproceedings{fadeeva-etal-2024-fact,
    title = "Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification",
    author = "Fadeeva, Ekaterina  and
      Rubashevskii, Aleksandr  and
      Shelmanov, Artem  and
      Petrakov, Sergey  and
      Li, Haonan  and
      Mubarak, Hamdy  and
      Tsymbalov, Evgenii  and
      Kuzmin, Gleb  and
      Panchenko, Alexander  and
      Baldwin, Timothy  and
      Nakov, Preslav  and
      Panov, Maxim",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.558/",
    doi = "10.18653/v1/2024.findings-acl.558",
    pages = "9367--9385",
    abstract = "Large language models (LLMs) are notorious for hallucinating, i.e., producing erroneous claims in their output. Such hallucinations can be dangerous, as occasional factual inaccuracies in the generated text might be obscured by the rest of the output being generally factually correct, making it extremely hard for the users to spot them. Current services that leverage LLMs usually do not provide any means for detecting unreliable generations. Here, we aim to bridge this gap. In particular, we propose a novel fact-checking and hallucination detection pipeline based on token-level uncertainty quantification. Uncertainty scores leverage information encapsulated in the output of a neural network or its layers to detect unreliable predictions, and we show that they can be used to fact-check the atomic claims in the LLM output. Moreover, we present a novel token-level uncertainty quantification method that removes the impact of uncertainty about what claim to generate on the current step and what surface form to use. Our method Claim Conditioned Probability (CCP) measures only the uncertainty of a particular claim value expressed by the model. Experiments on the task of biography generation demonstrate strong improvements for CCP compared to the baselines for seven different LLMs and four languages. Human evaluation reveals that the fact-checking pipeline based on uncertainty quantification is competitive with a fact-checking tool that leverages external knowledge."
}

@misc{niklaus2025lawinstructresourcestudyinglanguage,
      title={LawInstruct: A Resource for Studying Language Model Adaptation to the Legal Domain}, 
      author={Joel Niklaus and Lucia Zheng and Arya D. McCarthy and Christopher Hahn and Brian M. Rosen and Peter Henderson and Daniel E. Ho and Garrett Honke and Percy Liang and Christopher Manning},
      year={2025},
      eprint={2404.02127},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.02127}, 
}

@misc{wu2024susgengptdatacentricllmfinancial,
      title={SusGen-GPT: A Data-Centric LLM for Financial NLP and Sustainability Report Generation}, 
      author={Qilong Wu and Xiaoneng Xiang and Hejia Huang and Xuan Wang and Yeo Wei Jie and Ranjan Satapathy and Ricardo Shirota Filho and Bharadwaj Veeravalli},
      year={2024},
      eprint={2412.10906},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.10906}, 
}


@misc{zhang2024alpacareinstructiontunedlargelanguagemodels,
      title={AlpaCare:Instruction-tuned Large Language Models for Medical Application}, 
      author={Xinlu Zhang and Chenxin Tian and Xianjun Yang and Lichang Chen and Zekun Li and Linda Ruth Petzold},
      year={2024},
      eprint={2310.14558},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.14558}, 
}


@misc{fatemi2024comparativeanalysisinstructionfinetuning,
      title={A Comparative Analysis of Instruction Fine-Tuning LLMs for Financial Text Classification}, 
      author={Sorouralsadat Fatemi and Yuheng Hu and Maryam Mousavi},
      year={2024},
      eprint={2411.02476},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.02476}, 
}

@misc{zhao2024longalignmentsimpletoughtobeat,
      title={Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction Fine-Tuning}, 
      author={Hao Zhao and Maksym Andriushchenko and Francesco Croce and Nicolas Flammarion},
      year={2024},
      eprint={2402.04833},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.04833}, 
}

@misc{zhou2023limaalignment,
      title={LIMA: Less Is More for Alignment}, 
      author={Chunting Zhou and Pengfei Liu and Puxin Xu and Srini Iyer and Jiao Sun and Yuning Mao and Xuezhe Ma and Avia Efrat and Ping Yu and Lili Yu and Susan Zhang and Gargi Ghosh and Mike Lewis and Luke Zettlemoyer and Omer Levy},
      year={2023},
      eprint={2305.11206},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.11206}, 
}

@misc{liu2024makesgooddataalignment,
      title={What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning}, 
      author={Wei Liu and Weihao Zeng and Keqing He and Yong Jiang and Junxian He},
      year={2024},
      eprint={2312.15685},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.15685}, 
}

@article{dubois2024length,
  title={Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators},
  author={Dubois, Yann and Galambosi, Bal{\'a}zs and Liang, Percy and Hashimoto, Tatsunori B},
  journal={arXiv preprint arXiv:2404.04475},
  year={2024}
}

@misc{zheng2023judgingllmasajudgemtbenchchatbot,
      title={Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena}, 
      author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric P. Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
      year={2023},
      eprint={2306.05685},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.05685}, 
}

@misc{kang2024unfamiliarfinetuningexamplescontrol,
      title={Unfamiliar Finetuning Examples Control How Language Models Hallucinate}, 
      author={Katie Kang and Eric Wallace and Claire Tomlin and Aviral Kumar and Sergey Levine},
      year={2024},
      eprint={2403.05612},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.05612}, 
}

@inproceedings{gekhman-etal-2024-fine,
    title = "Does Fine-Tuning {LLM}s on New Knowledge Encourage Hallucinations?",
    author = "Gekhman, Zorik  and
      Yona, Gal  and
      Aharoni, Roee  and
      Eyal, Matan  and
      Feder, Amir  and
      Reichart, Roi  and
      Herzig, Jonathan",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.444/",
    doi = "10.18653/v1/2024.emnlp-main.444",
    pages = "7765--7784",
    abstract = "When large language models are aligned via supervised fine-tuning, they may encounter new factual information that was not acquired through pre-training. It is often conjectured that this can teach the model the behavior of hallucinating factually incorrect responses, as the model is trained to generate facts that are not grounded in its pre-existing knowledge. In this work, we study the impact of such exposure to new knowledge on the capability of the fine-tuned model to utilize its pre-existing knowledge. To this end, we design a controlled setup, focused on closed-book QA, where we vary the proportion of the fine-tuning examples that introduce new knowledge. We demonstrate that large language models struggle to acquire new factual knowledge through fine-tuning, as fine-tuning examples that introduce new knowledge are learned significantly slower than those consistent with the model`s knowledge. However, we also find that as the examples with new knowledge are eventually learned, they linearly increase the model`s tendency to hallucinate. Taken together, our results highlight the risk in introducing new factual knowledge through fine-tuning, and support the view that large language models mostly acquire factual knowledge through pre-training, whereas fine-tuning teaches them to use it more efficiently."
}
@inproceedings{han-etal-2024-rag,
    title = "{RAG}-{QA} Arena: Evaluating Domain Robustness for Long-form Retrieval Augmented Question Answering",
    author = "Han, Rujun  and
      Zhang, Yuhao  and
      Qi, Peng  and
      Xu, Yumo  and
      Wang, Jenyuan  and
      Liu, Lan  and
      Wang, William Yang  and
      Min, Bonan  and
      Castelli, Vittorio",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.249/",
    doi = "10.18653/v1/2024.emnlp-main.249",
    pages = "4354--4374",
    abstract = "Question answering based on retrieval augmented generation (RAG-QA) is an important research topic in NLP and has a wide range of real-world applications. However, most existing datasets for this task are either constructed using a single source corpus or consist of short extractive answers, which fall short of evaluating large language model (LLM) based RAG-QA systems on cross-domain generalization. To address these limitations, we create Long-form RobustQA (LFRQA), a new dataset comprising human-written long-form answers that integrate short extractive answers from multiple documents into a single, coherent narrative, covering 26K queries and large corpora across seven different domains. We further propose RAG-QA Arena by directly comparing model-generated answers against LFRQA`s answers using LLMs as evaluators. We show via extensive experiments that RAG-QA Arena and human judgments on answer quality are highly correlated. Moreover, only 41.3{\%} of the most competitive LLM`s answers are preferred to LFRQA`s answers, demonstrating RAG-QA Arena as a challenging evaluation platform for future research."
}


@misc{hendrycks2021measuringmassivemultitasklanguage,
      title={Measuring Massive Multitask Language Understanding}, 
      author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
      year={2021},
      eprint={2009.03300},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2009.03300}, 
}

@misc{lin2022truthfulqameasuringmodelsmimic,
      title={TruthfulQA: Measuring How Models Mimic Human Falsehoods}, 
      author={Stephanie Lin and Jacob Hilton and Owain Evans},
      year={2022},
      eprint={2109.07958},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2109.07958}, 
}

@inproceedings{min-etal-2023-factscore,
    title = "{FA}ct{S}core: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation",
    author = "Min, Sewon  and
      Krishna, Kalpesh  and
      Lyu, Xinxi  and
      Lewis, Mike  and
      Yih, Wen-tau  and
      Koh, Pang  and
      Iyyer, Mohit  and
      Zettlemoyer, Luke  and
      Hajishirzi, Hannaneh",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.741/",
    doi = "10.18653/v1/2023.emnlp-main.741",
    pages = "12076--12100",
    abstract = "Evaluating the factuality of long-form text generated by large language models (LMs) is non-trivial because (1) generations often contain a mixture of supported and unsupported pieces of information, making binary judgments of quality inadequate, and (2) human evaluation is time-consuming and costly. In this paper, we introduce FACTSCORE, a new evaluation that breaks a generation into a series of atomic facts and computes the percentage of atomic facts supported by a reliable knowledge source. We conduct an extensive human evaluation to obtain FACTSCOREs of people biographies generated by several state-of-the-art commercial LMs{---}InstructGPT, ChatGPT, and the retrieval-augmented PerplexityAI{---}and report new analysis demonstrating the need for such a fine-grained score (e.g., ChatGPT only achieves 58{\%}). Since human evaluation is costly, we also introduce an automated model that estimates FACTSCORE using retrieval and a strong language model, with less than a 2{\%} error rate. Finally, we use this automated metric to evaluate 6,500 generations from a new set of 13 recent LMs that would have cost {\$}26K if evaluated by humans, with various findings: GPT-4 and ChatGPT are more factual than public models, and Vicuna and Alpaca are some of the best public models. FACTSCORE is available for public use via {\textquoteleft}pip install factscore{\textquoteleft}."
}

@misc{zhao2024wildhallucinationsevaluatinglongformfactuality,
      title={WildHallucinations: Evaluating Long-form Factuality in LLMs with Real-World Entity Queries}, 
      author={Wenting Zhao and Tanya Goyal and Yu Ying Chiu and Liwei Jiang and Benjamin Newman and Abhilasha Ravichander and Khyathi Chandu and Ronan Le Bras and Claire Cardie and Yuntian Deng and Yejin Choi},
      year={2024},
      eprint={2407.17468},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.17468}, 
}

@misc{grattafiori2024llama3herdmodels,
      title={The Llama 3 Herd of Models}, 
      author={Meta},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}


@misc{vashurin2025benchmarkinguncertaintyquantificationmethods,
      title={Benchmarking Uncertainty Quantification Methods for Large Language Models with LM-Polygraph}, 
      author={Roman Vashurin and Ekaterina Fadeeva and Artem Vazhentsev and Lyudmila Rvanova and Akim Tsvigun and Daniil Vasilev and Rui Xing and Abdelrahman Boda Sadallah and Kirill Grishchenkov and Sergey Petrakov and Alexander Panchenko and Timothy Baldwin and Preslav Nakov and Maxim Panov and Artem Shelmanov},
      year={2025},
      eprint={2406.15627},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.15627}, 
}

@Inbook{Wilcoxon1992,
author="Wilcoxon, Frank",
editor="Kotz, Samuel
and Johnson, Norman L.",
title="Individual Comparisons by Ranking Methods",
bookTitle="Breakthroughs in Statistics: Methodology and Distribution",
year="1992",
publisher="Springer New York",
address="New York, NY",
pages="196--202",
abstract="The comparison of two treatments generally falls into one of the following two categories: (a) we may have a number of replications for each of the two treatments, which are unpaired, or (b) we may have a number of paired comparisons leading to a series of differences, some of which may be positive and some negative. The appropriate methods for testing the significance of the differences of the means in these two cases are described in most of the textbooks on statistical methods.",
isbn="978-1-4612-4380-9",
doi="10.1007/978-1-4612-4380-9_16",
url="https://doi.org/10.1007/978-1-4612-4380-9_16"
}


@software{Tunstall_The_Alignment_Handbook,
  author = {Tunstall, Lewis and Beeching, Edward and Lambert, Nathan and Rajani, Nazneen and Huang, Shengyi and Rasul, Kashif and Bartolome, Alvaro and M. Rush, Alexander and Wolf, Thomas},
  license = {Apache-2.0},
  title = {{The Alignment Handbook}},
  url = {https://github.com/huggingface/alignment-handbook},
  version = {0.3.0.dev0}
}

@article{Loshchilov2017FixingWD,
  title={Fixing Weight Decay Regularization in Adam},
  author={Ilya Loshchilov and Frank Hutter},
  journal={ArXiv},
  year={2017},
  volume={abs/1711.05101},
  url={https://api.semanticscholar.org/CorpusID:3312944}
}

@inproceedings{kwon2023efficient,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},
  year={2023}
}

@misc{xu2024magpiealignmentdatasynthesis,
      title={Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing}, 
      author={Zhangchen Xu and Fengqing Jiang and Luyao Niu and Yuntian Deng and Radha Poovendran and Yejin Choi and Bill Yuchen Lin},
      year={2024},
      eprint={2406.08464},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.08464}, 
}


@misc{yang2024logulongformgenerationuncertainty,
      title={LoGU: Long-form Generation with Uncertainty Expressions}, 
      author={Ruihan Yang and Caiqi Zhang and Zhisong Zhang and Xinting Huang and Sen Yang and Nigel Collier and Dong Yu and Deqing Yang},
      year={2024},
      eprint={2410.14309},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.14309}, 
}

@misc{yang2024alignmenthonesty,
      title={Alignment for Honesty}, 
      author={Yuqing Yang and Ethan Chern and Xipeng Qiu and Graham Neubig and Pengfei Liu},
      year={2024},
      eprint={2312.07000},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.07000}, 
}

@misc{xu2024rejectionimprovesreliabilitytraining,
      title={Rejection Improves Reliability: Training LLMs to Refuse Unknown Questions Using RL from Knowledge Feedback}, 
      author={Hongshen Xu and Zichen Zhu and Situo Zhang and Da Ma and Shuai Fan and Lu Chen and Kai Yu},
      year={2024},
      eprint={2403.18349},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.18349}, 
}

@misc{zhang2024rtuninginstructinglargelanguage,
      title={R-Tuning: Instructing Large Language Models to Say `I Don't Know'}, 
      author={Hanning Zhang and Shizhe Diao and Yong Lin and Yi R. Fung and Qing Lian and Xingyao Wang and Yangyi Chen and Heng Ji and Tong Zhang},
      year={2024},
      eprint={2311.09677},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.09677}, 
}

@misc{band2024linguisticcalibrationlongformgenerations,
      title={Linguistic Calibration of Long-Form Generations}, 
      author={Neil Band and Xuechen Li and Tengyu Ma and Tatsunori Hashimoto},
      year={2024},
      eprint={2404.00474},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.00474}, 
}

