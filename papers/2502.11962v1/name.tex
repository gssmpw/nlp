% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\newif\ifarxiv
\arxivfalse % make it 'true' or 'false' if you want to keep the anonymized or public version

\ifarxiv
    \usepackage{acl}
\else
    \usepackage[review]{acl}
\fi

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% Aditional packages and custom settings
\usepackage{enumitem}
\usepackage{array}
\usepackage{amsthm}

\usepackage{listings}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{soul,xcolor}
\usepackage{longtable}
\sethlcolor{yellow}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{bm}
\newtheorem{definition}{Definition}
\usepackage{calligra}
\newcommand{\specialfont}[1]{{\itshape\calligra #1}}

\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnote size acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=1.5pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
 \usepackage[most]{tcolorbox}

\newtcolorbox{myquotebox}{
  colback=white!0, % Transparent background
  colframe=black, % Black frame
  rounded corners,
  boxrule=0.5pt, % Frame thickness
  title=Prompt:,
  left=2mm, % Left margin within the box
  right=2mm, % Right margin within the box
  top=1mm, % Top margin within the box
  bottom=1mm % Bottom margin within the box
}
\usepackage{todonotes}
\usepackage{xspace}
\newcommand{\fixme}[2][]{\todo[color=goldenrod,size=\scriptsize,fancyline,caption={},#1]{#2}} % to mark stuff that you know is missing or wrong when you write the text
\newcommand{\note}[4][]{\todo[author=#2,color=#3,size=\scriptsize,fancyline,caption={},#1]{#4}} % default note settings, used by macros below.
\newcommand{\mrinmaya}[2][]{\note[#1]{mrinmaya}{green}{#2}\xspace}
\newcommand{\jingwei}[2][]{\note[#1]{jingwei}{yellow}{#2}\xspace}

\definecolor{lightgrey}{RGB}{158, 158, 158}
\definecolor{goldenrod}{rgb}{0,0,0.8}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}
\definecolor{pink}{RGB}{219, 48, 122}
\definecolor{forestgreen}{RGB}{34,139,34}
\definecolor{goldenrod}{RGB}{218,165,32}
\definecolor{sepia}{RGB}{112,66,20}

\usepackage[capitalise]{cleveref}
\crefname{figure}{Fig.}{Figs.}
\crefname{table}{Table}{Tables}
\crefname{appendix}{App.}{App.}
\crefname{section}{§}{§§}
\crefname{equation}{Eq.}{Eqs.}

\newcommand{\algname}{UHIT}

\newcommand{\smallgreen}[1]{\textcolor{deepgreen}{\scriptsize #1}}
\newcommand{\smallred}[1]{\textcolor{red}{\scriptsize #1}}
\newcommand\myparagraph[1]{
\vskip 0.05in 
\noindent{\bf {#1}}}
\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

%\title{The Helpfulness-Truthfulness Dilemma: Towards Uncertainty-Aware Honesty in Instruction Fine-Tuning}
%% \title{Navigating the Helpfulness-Truthfulness Tightrope: Towards Uncertainty-Aware Honesty in Instruction Fine-Tuning}
\title{Balancing Helpfulness and Truthfulness: Towards Uncertainty-Aware Honesty in Instruction Fine-Tuning}
% \title{Helpfulness vs. Truthfulness: Towards Uncertainty-Aware Honesty in Instruction Fine-Tuning}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}

\end{abstract}

\section{Introduction}
Given its straightforward implementation and usually satisfactory performance, Instruction Fine-tuning (IFT) has been widely adopted by practitioners to adapt general Large Language Models (LLMs) to specific tasks and domains \citep{niklaus2025lawinstructresourcestudyinglanguage,wu2024susgengptdatacentricllmfinancial,fatemi2024comparativeanalysisinstructionfinetuning,zhang2024alpacareinstructiontunedlargelanguagemodels}. In general-purpose post-training, researchers also find that a small amount of high-quality IFT data can transform LLMs into helpful AI assistants \citep{zhao2024longalignmentsimpletoughtobeat,liu2024makesgooddataalignment,zhou2023limaalignment}. To align LLMs to human preference, these IFT datasets are carefully collected to have responses with helpful styles and informative details. However, recent research shows that fine-tuning LLMs on new knowledge (w.r.t. old knowledge obtained in pre-training) may encourage hallucinates \citep{gekhman-etal-2024-fine,kang2024unfamiliarfinetuningexamplescontrol}. Such pre-training-fine-tuning knowledge descrepancy can be prevalent in helpfulness-focused IFT data as human prefers comprehensive and informative answers in information-seeking queries\footnote{For example, in LIMA \citep{zhou2023limaalignment}, LLMs are tuned to cite (Klämbt, 2009) to prove ``brain glial cells migrates'', which is porbably too niche to be acquired during pre-training.}. Therefore, there exists a helpfulness-truthfulness trade-off as more IFT for helpful behavior (e.g., in-depth information seeking) may cause more hallucinations. 

In this paper, we investigate the helpfulness-truthfulness trade-off by answering two research questions in a logical order: \myparagraph{RQ1. Does helpfulness-focused IFT encourages hallucinations in long-form generation?} \citet{gekhman-etal-2024-fine} experiment on short-form question answering (QA), showing that IFT LLMs on Unknown QAs will cause severe hallucination under some extreme conditions (e.g., overfitted-training for $\geq 20$ epochs; portion of Unknown QA $\geq 25\%$). %They also find that Maybe Known QAs (i.e., QAs where LLMs' knowledge is between known and unknown) contribute to higher truthfulness. 
However, whether helpfulness-focused IFT encourages hallucination remains an unanswered question, because helpfulness-focused IFT (1) aims at helpful long-form generation where the short-form QA observation may not transfer to; (2) rarely overfits as the data are typically well-diversified, and training epoch number is usually limited for generalizability. Furthermore, \citet{zhao2024longalignmentsimpletoughtobeat} find that helpfulness-focused IFT does not decrease (compared to base LLM without IFT) performance on factual knowledge benchmarks such as MMLU \citep{hendrycks2021measuringmassivemultitasklanguage} and TruthfulQA \citep{lin2022truthfulqameasuringmodelsmimic}.

To study \textbf{RQ1}, we experiment different IFT data combinations of LIMA \citep{zhou2023limaalignment} and RAG-QA \citep{han-etal-2024-rag}, and evaluate the out-of-distribution (OOD) long-form generation truthfulness using FactScore \citep{min-etal-2023-factscore} and WildFactScore \citep{zhao2024wildhallucinationsevaluatinglongformfactuality}. Evaluation shows that scaling-up IFT data with unfamiliar knowledge will decrease truthfulness. Enhancing helpfulness (e.g., adding LIMA datapoints) will further decrease truthfulness, while enhancing truthfulness (e.g., removing unfamiliar knowledge from IFT) will decrease helpfulness. After verifying the existance of helpfulness-truthfulness trade-off in helpfulness-focused IFT, we investigate: % as IFT data, which has diversified prompts and requires niche knowledge. This mimic a real-world setting where practitioners use IFT to adapt LLMs to domain-specific question answerers. Then we evaluate the OOD truthfulness of  

\myparagraph{RQ2. How to keep helpfulness while enhancing truthfulness?} To ensure helpfulness, LLMs should keep answer informativeness while warn users about its uncertain claims. Therefore, we propose \algname\space 
 (\underline{U}ncertainty-aware \underline{H}onesty \underline{I}nstruction \underline{T}uning), an IFT paradigm that tunes LLMs to report uncertainty after responses. Specifically, \algname\space first probes target LLM's uncertainty about knowledge in the original responses of a helpfulness-focused IFT dataset. Then, it append a ``reflection'' section after the original responses, which contains the uncertain knowledge. Intuitively, \algname\space preserve the helpfulness by keeping the original responses while enhancing honesty through uncertainty reporting. Extensive evaluations show that \algname-tuned LLM to some extent learns its own uncertainty, while preserving helpfulness and truthfulness. To sum up, our contributions involve


\section{Preliminary}

\subsection{Metrics}


\subsection{The Helpfulness and Truthfulness Tradeoff in Long Form Generation}
\subsubsection{The Unfamiliar Samples Effect in Instruction Fine-tuning}\label{sec:unfamiliar}
Domain practitioners often use domain-specific IFT datasets to fine-tune LLMs to enhance their helpfulness in downstream domain tasks. However, it is often that these domain-specific datasets contain niche domain knowledge that is unfamiliar to the base pre-trained model. Recent research found that fine-tuning LLMs on unfamiliar knowledge may increase their tendency to hallucinate, i.e., harming the truthfulness of the models \cite{gekhman-etal-2024-fine, kang2024unfamiliarfinetuningexamplescontrol}. The studies have been limited to short QA fine-tuning and evaluation, where whether this behavior extends to long-form generation remains unknown; however, long-form generation can be a more common case in many downstream domains.

To simulate a domain-specific IFT setting where (1) the datasets contain much unfamiliar knowledge and (2) finetunes for long-form generation, we choose RAG-QA \cite{han-etal-2024-rag}, where it contains long-form answers to domain questions that are written by human with the presence of retrieved documents. We gradually increased the proportion of RAG-QA used in fine-tuning to see the effect of increasing unfamiliar knowledge used in fine-tuning on the helpfulness and the truthfulness of the model. The result in Table \ref{} has shown that as we increased the proportion of RAG-QA from 10\% to 100\% in fine-tuning LLama-3.1-8B, the truthfulness and the helpfulness of the model decreased. The deterioration of helpfulness is expected as the dataset is designed to be tuned for helpfulness. On the other side, the deterioration of the truthfulness suggests:

\vspace{5pt} % Indentation before
\begin{minipage}{0.9\linewidth} 
    \emph{As \textbf{more unfamiliar examples} are used in the IFT process, the model becomes \textbf{less truthful} and tends to hallucinate more often in \textbf{long-form generation}.}
\end{minipage}


\subsubsection{How to Enhance Helpfulness?}
A natural approach to improving the helpfulness of the model while fine-tuning LLMs to adapt to domain tasks is to add a portion of the helpfulness IFT dataset along with the domain-specific dataset. To investigate the effect of adding the helpfulness IFT dataset during domain-specific fine-tuning, we added LIMA \cite{zhou2023limaalignment} with the RAG-QA dataset in fine-tuning. By comparing the results of models tuned using only RAG-QA with models tuned using LIMA+RAG-QA shown in Table \ref{}, we found that although this approach can successfully increase the helpfulness of the model, it severely deteriorates the truthfulness performance even if the number of unfamiliar samples included in fine-tuning remains the same. This suggests:

\vspace{5pt} % Indentation before
\begin{minipage}{0.9\linewidth} % Adjust width as needed
    \emph{\textbf{The Existence of Helpfulness and Truthfulness Tradeoff} -- Naively including helpfulness in the IFT dataset while fine-tuning LLM to downstream domain tasks cannot ensure the simultaneous improvement of helpfulness and truthfulness of LLMs.}
\end{minipage}
\vspace{-2pt}


\subsubsection{How to Enhance Truthfulness?}
\paragraph{Finding Unfamiliar Samples} As shown in Section \ref{sec:unfamiliar}, unfamiliar samples in fine-tuning harm the truthfulness of the model, the intuitive solution to this problem is to remove all of the unfamiliar samples in the dataset before fine-tuning. To measure the "unfamiliarity" of the sample to the pre-trained model, we adopt Claim Conditioned Probability (CCP) \cite{fadeeva-etal-2024-fact} to measure the uncertainty of all the claims within the responses in the datasets. Specifically, given an instruction–response pair \((I, R)\) from a dataset and a target model \(M\) for uncertainty estimation, where the response is \(R = \{x_1, x_2, \ldots, x_n\}\)
and each \(x_i\) is a generated token; the CCP algorithm first extracts a set \(\mathcal{C}\) of atomic factual claims (i.e., coherent factual segments) from \(R\). For each token \(x_j\) within a claim \(C \in \mathcal{C}\), the model \(M\) samples the top-\(K\) alternatives \(\{x_j^1, x_j^2, \ldots, x_j^K\}\) with probabilities \(P(x_j^k \mid x_{<j})\), where \(x_{<j} = \{x_1, x_2, \ldots, x_{j-1}\}\). A natural language inference (NLI) model is then used to label each alternative \(x_j^k\) based on whether it preserves the meaning of \(x_j\); those labeled as entailment form the set 
\[
M(x_j)=\{x_j^k \mid \text{NLI}(x_j^k, x_j)=e\},
\]
and those labeled as either entailment or contradiction form 
\[
CT(x_j)=\{x_j^k \mid \text{NLI}(x_j^k, x_j) \in \{e,c\}\}.
\]
The token-level uncertainty is computed as
\[
\text{CCP}(x_j) = \frac{\sum_{x_j^k \in M(x_j)} P(x_j^k \mid x_{<j})}{\sum_{x_j^l \in CT(x_j)} P(x_j^l \mid x_{<j})},
\]
and the overall uncertainty for a claim is aggregated by
\[
\text{CCP}_{\text{claim}}(C) = 1 - \prod_{x_j \in C} \text{CCP}(x_j).

Using CCP, for each response \(R\) we obtain a set of atomic claims with their corresponding uncertainty values, that is, \[ \{(C_1, \text{CCP}_{\text{claim}}(C_1)),\,(C_2, \text{CCP}_{\text{claim}}(C_2)),\,\ldots,\,(C_m, \text{CCP}_{\text{claim}}(C_m))\}, \] that represents how uncertain the model is about the factual correctness of each claim.
\section{Uncertainty-Aware Fine-tuning}

\section{Results} 

\ifarxiv
\section*{Acknowledgements} 
This paper has received funding from the Swiss
National Science Foundation (SNSF) under the project `How sustainable is sustainable finance? Impact evaluation and automated greenwashing detection' (Grant Agreement No. 100018\_207800).
\else

\fi
% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

\appendix

\section{Example Appendix}
\label{sec:appendix}

This is an appendix.

\end{document}
