\section{Related Work and Conclusion}
Kornblith, "Do Better Evaluation"__Miao, "Self-Improving Neural Conversation Models"
observe that helpfulness-purposed IFT does not degrade performance on factual-knowledge benchmarks. Our work shows that even in early epochs of IFT (only 3 epochs on diverse data), incorporating unfamiliar knowledge can still harm OOD truthfulness.
To enhance honesty, prior work uses non-helpfulness-purposed data to improve LLM calibration Radford et al., "Improving Language Understanding by Generative Models"__Li et al., "Towards Distantly Supervised Open-Domain Question Answering"_, but fails to cover how to incorporate human-written helpful-purposed IFT data to preserve helpfulness. Hence, we propose \algname\space to fill in this gap. \algname\space also differs from prior work by using direct claim-level uncertainty Stengel et al., "A Neural Conversational Model"__Henderson et al., "Using Deep Learning for Question Answering over Freebase with Word Embeddings",  for honesty alignment, rather than using LLM answer correctness as a proxy for uncertainty.