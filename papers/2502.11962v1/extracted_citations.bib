@misc{band2024linguisticcalibrationlongformgenerations,
      title={Linguistic Calibration of Long-Form Generations}, 
      author={Neil Band and Xuechen Li and Tengyu Ma and Tatsunori Hashimoto},
      year={2024},
      eprint={2404.00474},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.00474}, 
}

@inproceedings{fadeeva-etal-2024-fact,
    title = "Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification",
    author = "Fadeeva, Ekaterina  and
      Rubashevskii, Aleksandr  and
      Shelmanov, Artem  and
      Petrakov, Sergey  and
      Li, Haonan  and
      Mubarak, Hamdy  and
      Tsymbalov, Evgenii  and
      Kuzmin, Gleb  and
      Panchenko, Alexander  and
      Baldwin, Timothy  and
      Nakov, Preslav  and
      Panov, Maxim",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.558/",
    doi = "10.18653/v1/2024.findings-acl.558",
    pages = "9367--9385",
    abstract = "Large language models (LLMs) are notorious for hallucinating, i.e., producing erroneous claims in their output. Such hallucinations can be dangerous, as occasional factual inaccuracies in the generated text might be obscured by the rest of the output being generally factually correct, making it extremely hard for the users to spot them. Current services that leverage LLMs usually do not provide any means for detecting unreliable generations. Here, we aim to bridge this gap. In particular, we propose a novel fact-checking and hallucination detection pipeline based on token-level uncertainty quantification. Uncertainty scores leverage information encapsulated in the output of a neural network or its layers to detect unreliable predictions, and we show that they can be used to fact-check the atomic claims in the LLM output. Moreover, we present a novel token-level uncertainty quantification method that removes the impact of uncertainty about what claim to generate on the current step and what surface form to use. Our method Claim Conditioned Probability (CCP) measures only the uncertainty of a particular claim value expressed by the model. Experiments on the task of biography generation demonstrate strong improvements for CCP compared to the baselines for seven different LLMs and four languages. Human evaluation reveals that the fact-checking pipeline based on uncertainty quantification is competitive with a fact-checking tool that leverages external knowledge."
}

@inproceedings{gekhman-etal-2024-fine,
    title = "Does Fine-Tuning {LLM}s on New Knowledge Encourage Hallucinations?",
    author = "Gekhman, Zorik  and
      Yona, Gal  and
      Aharoni, Roee  and
      Eyal, Matan  and
      Feder, Amir  and
      Reichart, Roi  and
      Herzig, Jonathan",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.444/",
    doi = "10.18653/v1/2024.emnlp-main.444",
    pages = "7765--7784",
    abstract = "When large language models are aligned via supervised fine-tuning, they may encounter new factual information that was not acquired through pre-training. It is often conjectured that this can teach the model the behavior of hallucinating factually incorrect responses, as the model is trained to generate facts that are not grounded in its pre-existing knowledge. In this work, we study the impact of such exposure to new knowledge on the capability of the fine-tuned model to utilize its pre-existing knowledge. To this end, we design a controlled setup, focused on closed-book QA, where we vary the proportion of the fine-tuning examples that introduce new knowledge. We demonstrate that large language models struggle to acquire new factual knowledge through fine-tuning, as fine-tuning examples that introduce new knowledge are learned significantly slower than those consistent with the model`s knowledge. However, we also find that as the examples with new knowledge are eventually learned, they linearly increase the model`s tendency to hallucinate. Taken together, our results highlight the risk in introducing new factual knowledge through fine-tuning, and support the view that large language models mostly acquire factual knowledge through pre-training, whereas fine-tuning teaches them to use it more efficiently."
}

@misc{xu2024rejectionimprovesreliabilitytraining,
      title={Rejection Improves Reliability: Training LLMs to Refuse Unknown Questions Using RL from Knowledge Feedback}, 
      author={Hongshen Xu and Zichen Zhu and Situo Zhang and Da Ma and Shuai Fan and Lu Chen and Kai Yu},
      year={2024},
      eprint={2403.18349},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.18349}, 
}

@misc{yang2024alignmenthonesty,
      title={Alignment for Honesty}, 
      author={Yuqing Yang and Ethan Chern and Xipeng Qiu and Graham Neubig and Pengfei Liu},
      year={2024},
      eprint={2312.07000},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.07000}, 
}

@misc{yang2024logulongformgenerationuncertainty,
      title={LoGU: Long-form Generation with Uncertainty Expressions}, 
      author={Ruihan Yang and Caiqi Zhang and Zhisong Zhang and Xinting Huang and Sen Yang and Nigel Collier and Dong Yu and Deqing Yang},
      year={2024},
      eprint={2410.14309},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.14309}, 
}

@misc{zhang2024rtuninginstructinglargelanguage,
      title={R-Tuning: Instructing Large Language Models to Say `I Don't Know'}, 
      author={Hanning Zhang and Shizhe Diao and Yong Lin and Yi R. Fung and Qing Lian and Xingyao Wang and Yangyi Chen and Heng Ji and Tong Zhang},
      year={2024},
      eprint={2311.09677},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.09677}, 
}

@misc{zhao2024longalignmentsimpletoughtobeat,
      title={Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction Fine-Tuning}, 
      author={Hao Zhao and Maksym Andriushchenko and Francesco Croce and Nicolas Flammarion},
      year={2024},
      eprint={2402.04833},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.04833}, 
}

