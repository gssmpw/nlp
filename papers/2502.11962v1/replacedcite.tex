\section{Related Work and Conclusion}
____ studied that in short-form QA, overfitting LLMs on unknown QA pairs (e.g., training for 20+ epochs) can cause severe hallucinations, which can be mitigated by early stopping (e.g., under 5 epochs). In contrast, ____ observe that helpfulness-purposed IFT does not degrade performance on factual-knowledge benchmarks. Our work shows that even in early epochs of IFT (only 3 epochs on diverse data), incorporating unfamiliar knowledge can still harm OOD truthfulness.
To enhance honesty, prior work uses non-helpfulness-purposed data to improve LLM calibration ____, but fails to cover how to incorporate human-written helpful-purposed IFT data to preserve helpfulness. Hence, we propose \algname\space to fill in this gap. \algname\space also differs from prior work by using direct claim-level uncertainty ____ for honesty alignment, rather than using LLM answer correctness as a proxy for uncertainty.