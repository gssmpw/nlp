\section{Related Work and Conclusion}
\citet{gekhman-etal-2024-fine} studied that in short-form QA, overfitting LLMs on unknown QA pairs (e.g., training for 20+ epochs) can cause severe hallucinations, which can be mitigated by early stopping (e.g., under 5 epochs). In contrast, \citet{zhao2024longalignmentsimpletoughtobeat} observe that helpfulness-purposed IFT does not degrade performance on factual-knowledge benchmarks. Our work shows that even in early epochs of IFT (only 3 epochs on diverse data), incorporating unfamiliar knowledge can still harm OOD truthfulness.
To enhance honesty, prior work uses non-helpfulness-purposed data to improve LLM calibration \citep{band2024linguisticcalibrationlongformgenerations,yang2024logulongformgenerationuncertainty,yang2024alignmenthonesty,xu2024rejectionimprovesreliabilitytraining,zhang2024rtuninginstructinglargelanguage}, but fails to cover how to incorporate human-written helpful-purposed IFT data to preserve helpfulness. Hence, we propose \algname\space to fill in this gap. \algname\space also differs from prior work by using direct claim-level uncertainty \citep{fadeeva-etal-2024-fact} for honesty alignment, rather than using LLM answer correctness as a proxy for uncertainty.