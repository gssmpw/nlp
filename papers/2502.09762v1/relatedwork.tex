\section{Related Work}
\label{appendix:rw}

\begin{table*}[!ht]
    \centering
    \caption{Comparison of \framework with related works. Grey rows represent  literature related to multi-drone pursuit, while pink rows highlight adaptive teaming studies from the machine learning field. ``AT w/o TM'' denotes adaptive teaming without teammate modelling, while ``AT w/ TM'' refers to adaptive teaming with teammate modelling.}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|c|c|c|c|c|c|c|c}
        \toprule
        \multirow{2}{*}{\textbf{Related Work}} & \multicolumn{4}{c|}{\textbf{Problem Setting}}& \multicolumn{2}{c|}{\textbf{Task}} & \multicolumn{2}{c}{\textbf{Method}} \\
        \cline{2-9}
        & \textbf{\# Learner} & \textbf{\# Unseen} & \textbf{\# Evader} &\textbf{Action Space} & \textbf{Main Related Task} & \textbf{Real-world?} & \textbf{AT w/o TM?} & \textbf{AT w/ TM?} \\
        \midrule 
        \rowcolor{gray!10} 
        Voronoi Partitions~\cite{zhou_cooperative_2016} & Multi & 0 & 1  & Continuous &  Pursuit–evasion Game & \No & \No & \No \\ 
        \cline{1-9}
        \rowcolor{gray!10} 
        Bio-pursuit~\cite{janosov_group_2017}  & Multi & 0 & Multi & Continuous &  Prey–predator Game & \No & \No & \No \\
        \cline{1-9}
        \rowcolor{gray!10} 
        Uncertainty-pursuit~\cite{shah_multi-agent_2019} & Multi & 0 & 1 & Continuous & Pursuit–evasion Game & \No & \No & \No \\ 
        \cline{1-9}
        \rowcolor{gray!10}
        M3DDPG~\cite{li_robust_2019} & Multi & 0 & 1 & Continuous &  Prey–predator Game & \No & \No & \No \\ 
        \cline{1-9}
        \rowcolor{gray!10}
        Pursuit-TD3\cite{de_souza_decentralized_2021} & Multi & 0 & 1 & Continuous &  \textbf{\textcolor{blue}{Multi-drone Pursuit}} & \Yes & \No & \No \\ 
        \cline{1-9}
        \rowcolor{gray!10} 
        DACOOP-A\cite{ZhangDACOOP2023} & Multi & 0 & 1 & Discrete &  \textbf{\textcolor{blue}{Multi-drone Pursuit}} & \Yes & \No & \No \\ 
        \cline{1-9}
        \rowcolor{gray!10} 
        GM-TD3~\cite{zhang2024multi}  & Multi & 0 & 1 & Continuous & Prey–predator Game & \No & \No & \No \\ 
        \cline{1-9}
        \rowcolor{gray!10} 
        DualCL~\cite{chen2024dualcurriculumlearningframework} & Multi & 0 & 1 & Continuous & \textbf{\textcolor{blue}{Multi-drone Pursuit}} & \No & \No & \No \\ 
        \cline{1-9}
        \rowcolor{gray!10}
        HOLA-Drone~\cite{hola-drone}  &  1 & Multi & Multi & Continuous & \textbf{\textcolor{blue}{Multi-drone Pursuit}} & \Yes & \Yes  & \No \\ 
        \midrule
        \rowcolor{pink!30} 
        Other-play~\cite{hu2020other} & 1 & 1 & 0 &  Discrete & Lever Game; Hanabi &  \No & \Yes  & \No \\ 
        \cline{1-9}
        \rowcolor{pink!30} 
        Overcooked-AI ~\cite{carroll2019utility} & 1 & 1 & 0 &  Discrete & Overcooked &  \No & \Yes  & \No \\ 
        \cline{1-9}
        \rowcolor{pink!30} 
        TrajDi~\cite{TrajDi}  & 1 & 1 & 0 &  Discrete & Overcooked &  \No & \Yes  & \No \\ 
        \cline{1-9}
        \rowcolor{pink!30} 
        MEP~\cite{MEP}  & 1 & 1 & 0 &  Discrete & Overcooked &  \No & \Yes  & \No \\ 
        \cline{1-9}
        \rowcolor{pink!30} 
        LIPO~\cite{charakorn2023generating}  & 1 & 1 & 0 &  Discrete & Overcooked &  \No & \Yes  & \No \\ 
        \cline{1-9}
        \rowcolor{pink!30} 
        COLE~\cite{li2024tackling}  & 1 & 1 & 0 &  Discrete & Overcooked &  \No & \Yes  & \No \\ 
        \cline{1-9}
        \rowcolor{pink!30} 
        ZSC-Eval~\cite{wang2024zsc}  & 1 & 1 & 0 &  Discrete & Overcooked &  \No & \Yes  & \No \\ 
        \cline{1-9}
       \rowcolor{pink!30}  
       PLASTIC~\cite{barrett2017making}  &  1 & Multi & Multi & Discrete & Prey-predator Game &  \No & \No & \Yes \\ 
       \cline{1-9} 
        \rowcolor{pink!30} 
        AATeam~\cite{chen2020aateam}  & 1 & 1 & 2 &  Discrete & Half Field Offense &  \No & \No & \Yes  \\  
        \cline{1-9} 
        \rowcolor{pink!30} 
        LIAM~\cite{papoudakis2021agent}  &  1 & Multi & %\rowcolor{pink!30} 
        Multi & Discrete & LBF; Prey-predator Game  &  \No & \No & \Yes   \\ 
        \cline{1-9} 
       \rowcolor{pink!30}  
       GPL~\cite{rahman2021towards}  &  1 & Multi & Multi & Discrete & LBF; Wolfpack; FortAttack &  \No & \No & \Yes   \\ 
       \cline{1-9} 
       \rowcolor{pink!30}   
       CIAO~\cite{jianhong2024oaht} &  1 & Multi & Multi & Discrete &LBF; Wolfpack &  \No & \No & \Yes \\ 
       \cline{1-9} 
       \rowcolor{pink!30}   
       NAHT~\cite{wang2024n}  & Multi & Multi & Multi & Discrete & StarCraft2 &  \No & \No & \Yes \\
         \midrule
         \rowcolor{orange!30} 
         Our ATMDP  & Multi & Multi & Multi & Continuous & \textbf{\textcolor{blue}{Multi-drone Pursuit}} &  \Yes & \Yes & \Yes \\
         \bottomrule
    \end{tabular}
    }
    \label{appendix:tab_rw}
\end{table*}

In this work, we provide a comprehensive review of related research on multi-drone pursuit and adaptive teaming in machine learning, with a detailed comparison presented in Table~\ref{tab:review}.

\textbf{Multi-agent pursuit-evasion. } 
Multi-agent pursuit-evasion is closely related to the multi-drone pursuit task. 
Most existing methods rely on pre-coordinated strategies specifically designed for particular pursuit-evasion scenarios.
Traditional approaches often rely on heuristic~\cite{janosov_group_2017} or optimisation-based strategies~\cite{zhou_cooperative_2016,shah_multi-agent_2019}. For example, \citet{janosov_group_2017} proposes a bio-inspired model that uses local interaction rules to enhance group chasing success in Prey–Predator Games. Similarly, the Voronoi partitions algorithm~\cite{zhou_cooperative_2016} and the uncertainty-pursuit algorithm~\cite{shah_multi-agent_2019} employ decentralised frameworks to optimise the evader’s Voronoi partition and reachable area, respectively.
In recent years, deep reinforcement learning (DRL) has been widely adopted for pre-coordinated multi-drone pursuit tasks. M3DDPG~\cite{li_robust_2019} and GM-TD3~\cite{zhang2024multi} extend standard DRL algorithms, such as TD3~\cite{TD3} and DDPG~\cite{DDPG}, specifically for multi-agent pursuit in simulated environments. Pursuit-TD3~\cite{de_souza_decentralized_2021} applies the TD3 algorithm to pursue a target with multiple homogeneous agents, validated through both simulations and real-world drone demonstrations. \citet{ZhangDACOOP2023} introduces DACOOP-A, a cooperative pursuit algorithm that enhances reinforcement learning with artificial potential fields and attention mechanisms, validated in real-world drone systems. DualCL~\cite{chen2024dualcurriculumlearningframework} addresses multi-UAV pursuit-evasion in diverse environments and demonstrates zero-shot transfer capabilities to unseen scenarios, though only in simulation.
The most recent work, HOLA-Drone~\cite{hola-drone}, claims to be the first zero-shot coordination framework for multi-drone pursuit. However, it is limited to controlling a single learner, restricting its applicability to broader multi-agent settings. 


\textbf{Adaptive Teaming. }The adaptive teaming paradigm can be broadly categorised into two approaches: adaptive teaming without teammate modelling (AT w/o TM) and adaptive teaming with teammate modelling (AT w/ TM), which correspond to the zero-shot coordination (ZSC) and ad-hoc teamwork (AHT) problems in the machine learning community, respectively.
AT w/o TM focuses on enabling agents to coordinate with unseen teammates without explicitly modelling their behaviours. Other-Play~\cite{hu2020other} introduces an approach that leverages symmetries in the environment to train robust coordination policies, applied to discrete-action tasks like the Lever Game and Hanabi. Similarly, methods such as Overcooked-AI~\cite{carroll2019utility}, TrajDi~\cite{TrajDi}, MEP~\cite{MEP}, LIPO~\cite{charakorn2023generating}, and ZSC-Eval~\cite{wang2024zsc} study collaborative behaviours in Overcooked, where agents learn generalisable coordination strategies with diverse unseen partners. While these approaches demonstrate promising results, they are limited to single-learner frameworks in simplified, discrete-action domains like Overcooked and Hanabi. They lack scalability to multi-agent settings, continuous action spaces, and the complexities of real-world applications.

AT w/ TM, on the other hand, explicitly models the behaviour of unseen teammates to facilitate effective collaboration. Early methods like PLASTIC~\cite{barrett2017making} reuse knowledge from previous teammates or expert input to adapt to new teammates efficiently. AaTeam~\cite{chen2020aateam} introduces attention-based neural networks to dynamically process and respond to teammates’ behaviours in real-time.
More advanced approaches, such as LIAM~\cite{papoudakis2021agent}, employ encoder-decoder architectures to model teammates using local information from the controlled agent. GPL~\cite{rahman2021towards} and CIAO~\cite{jianhong2024oaht} leverage graph neural networks (GNNs) to address the challenges of dynamic team sizes in AHT. Extending this further, NAHT~\cite{wang2024n} enables multiple learners to collaborate and interact with diverse unseen partners in N-agent scenarios.
Despite their progress, these methods remain confined to discrete action spaces and simulated benchmarks, limiting their applicability to real-world, continuous-action tasks. 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/OPT.pdf}
    \caption{Overview of our proposed open-ended population training algorithm.}
    \label{fig:opt}
\end{figure}