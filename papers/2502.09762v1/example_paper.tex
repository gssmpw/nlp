%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
\PassOptionsToPackage{table}{xcolor}
\documentclass{article}
\input{math_commands}
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption} % For subfigures
% \usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{lipsum}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{xspace}
% \usepackage{minted}
% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}


\newcommand{\ying}[1]{{\bf \color{blue} [Ying: #1]}}
\newcommand{\qrz}[1]{\textcolor{cyan}{\emph{[QRZhang: #1]}}}


\newcommand{\framework}{AT-MDP framework\xspace}
\newcommand{\problem}{AT-MDP problem\xspace}
\newcommand{\Yes}{\textbf{\textcolor{red}{Yes}}}
\newcommand{\No}{\textcolor{black}{No}}
\newcommand{\easy}{\texttt{4p2e1o}\xspace} 
\newcommand{\medium}{\texttt{4p2e3o}\xspace} 
\newcommand{\hard}{\texttt{4p2e5o}\xspace} 
\newcommand{\superhard}{\texttt{4p3e5o}\xspace} 

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{multicol}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\usepackage{float}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\newcommand{\wei}[1]{\textcolor{purple}{[WP: #1]}}

% Import listings and xcolor packages
\usepackage{listings}
% Load xcolor with table support [table,xcdraw]

\definecolor{strcolor}{HTML}{1B51A5}
\definecolor{keycolor}{HTML}{278559}
\lstdefinelanguage{json}{
    basicstyle=\ttfamily\footnotesize,            % Use a monospaced font
    numbers=none,                    % Show line numbers
    numberstyle=\tiny,               % Line number font size
    stepnumber=1,                    % Line number increment
    numbersep=1pt,                   % Space between line numbers and code
    showstringspaces=false,          % Do not display string spaces
    breaklines=true,                 % Enable line wrapping
    frame=shadowbox,                 % Add shadow box around the code
    rulesepcolor=\color{gray!30},    % Shadow colour
    backgroundcolor=\color{white}, % Background colour
    stringstyle=\color{strcolor},      % String colour
    keywordstyle=\bfseries\color{keycolor}, % Key colour
    morestring=[b]"                  % Double quotes as string delimiter
}
\linespread{0.99}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Adaptive Teaming in Multi-Drone Pursuit: Simulation, Training, and Deployment}

\begin{document}

\twocolumn[
\icmltitle{Adaptive Teaming in Multi-Drone Pursuit: \\ Simulation, Training, and Deployment}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Yang Li}{uom}
\icmlauthor{Junfan~Chen}{sjtu}
\icmlauthor{Feng~Xue}{syu}
\icmlauthor{Jiabin~Qiu}{nju}
\icmlauthor{Wenbin Li}{nju}
\icmlauthor{Qingrui~Zhang}{syu}
\icmlauthor{Ying~Wen}{sjtu}
%\icmlauthor{}{sch}
\icmlauthor{Wei~Pan}{uom}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{uom}{The University of Manchester}
\icmlaffiliation{syu}{Sun Yat-sen University}
\icmlaffiliation{nju}{Nanjing University}
\icmlaffiliation{sjtu}{Shanghai Jiao Tong University}

\icmlcorrespondingauthor{Ying Wen}{ying.wen@sjtu.edu.cn}
\icmlcorrespondingauthor{Wei Pan}{wei.pan@manchester.ac.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
% \wei{better change to US English, not UK english, like **sation, modeling}
Adaptive teaming, the ability to collaborate with unseen teammates without prior coordination, remains an underexplored challenge in multi-robot collaboration. 
This paper focuses on adaptive teaming in multi-drone cooperative pursuit, a critical task with real-world applications such as border surveillance, search-and-rescue, and counter-terrorism.
We first define and formalize the \textbf{A}daptive Teaming in \textbf{M}ulti-\textbf{D}rone \textbf{P}ursuit (AT-MDP) problem and introduce \framework, a comprehensive framework that integrates simulation, algorithm training and real-world deployment. \framework provides a flexible experiment configurator and interface for simulation, a distributed training framework with an extensive algorithm zoo (including two newly proposed baseline methods) and an unseen drone zoo for evaluating adaptive teaming, as well as a real-world deployment system that utilizes edge computing and Crazyflie drones.
To the best of our knowledge, \framework is the first adaptive framework for continuous-action decision-making in complex real-world drone tasks, enabling multiple drones to coordinate effectively with unseen teammates. 
Extensive experiments in four multi-drone pursuit environments of increasing difficulty confirm the effectiveness of \framework, while real-world deployments further validate its feasibility in physical systems.
Videos and code are available at \url{https://sites.google.com/view/at-mdp}.

\end{abstract}

\section{Introduction}
% \wei{i use linespread.. final stage if you can't compress anymore!!!}
Multi-drone pursuit is an increasingly critical task with wide-ranging real-world applications, such as disaster response, border surveillance, search-and-rescue operations, and many more~\cite{chung2011search,ZhangDACOOP2023,queralta2020collaborative}. All of these scenarios are highly dependent on the ability of autonomous drones to coordinate effectively in dynamic environments to pursue and track uncooperative targets.
% Scenarios such as disaster response, border surveillance, search-and-rescue operations and even national security rely heavily on the ability of autonomous drones to coordinate effectively in dynamic environments.
Most existing approaches rely on pre-coordinated strategies, where agents either follow predefined coordination mechanisms, such as conventions, role assignments, and communication protocols, or learn them through extensive interaction over time~\cite{Rahman2024Mini}. These methods generally fall into two categories: traditional optimization-based approaches~\cite{shah_multi-agent_2019,janosov_group_2017,zhou_cooperative_2016},and reinforcement learning-based approaches~\cite{ZhangDACOOP2023,chen2024dualcurriculumlearningframework,qi_cascaded_2024,de_souza_decentralized_2021,li_robust_2019,matignon_hysteretic_2007}.

% potential application case to refer from https://msl.stanford.edu/papers/correll_multi-agent_2019.pdf 
% The advent of consumer multi-rotors has created a potential hazard for airports
% and other sensitive airspace, as pilots can easily fly UAVs into restricted areas
% (either intentionally or accidentally).3 In recent years, numerous airports have
% been shut down due to rogue drones.4 Some countries are even attempting to
% train eagles to disable unauthorized UAVs flying in sensitive areas.5 In this
% paper we propose an online cooperative pursuit algorithm for a team of drones
% to pursue and ultimately disable a rogue evader drone.

% However, real-world scenarios often involve dynamic and unpredictable changes in team composition, requiring drones to adapt to unforeseen teammates in real-time rather than relying on pre-coordinated teaming. 
\begin{figure}[!t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/intro.pdf}
    \vspace{-3mm}
    \caption{\textbf{Pre-coordinated teaming: }Agents coordinate using predefined mechanisms—such as conventions, role assignment protocols, and communication strategies—or by learning these mechanisms through extensive interaction over time~\cite{Rahman2024Mini}.
    \textbf{Adaptive teaming:} The assumed convention or mechanism is disrupted or fails, requiring adaptation to unseen teammates without prior coordination methods.
    % In the left figure, all pursuers are controlled by a prior coordination method or a learned strategy. In contrast, in the right figure, under adaptive teaming, some teammates do not follow a prior coordination method but are instead controlled by an unseen policy.
    }
    \vspace{-5mm}
    \label{fig:intro}
\end{figure}


However, real-world scenarios are inherently uncertain, and unpredictable, often necessitating rapid adaptation to evolving team compositions. Instead of relying on pre-coordinated teaming, drones must seamlessly integrate with unforeseen teammates in real time.
% \ying{cites?} no cite here, Chatgpt and me imagine it
For example, in disaster response, damaged drones may need rapid replacement to maintain search coverage, while in border surveillance, specialized drones might temporarily join to intercept high-priority targets.
Adaptive teaming is capable of addressing this critical gap by enabling drones to dynamically adjust their coordination strategies with unforeseen teammates, without relying on prior coordination, as shown in Fig.~\ref{fig:intro}.

To address the challenge of working with unseen drone teammates, we first define and formalize the \textbf{A}daptive Teaming in \textbf{M}ulti-\textbf{D}rone \textbf{P}ursuit (AT-MDP) problem and propose the \framework, a comprehensive framework that seamlessly integrates simulation, training, and real-world deployment.
Specifically, the simulation component enables flexible customization of multi-drone pursuit scenarios through an environment configurator and provides a realistic simulated environment for training and evaluation via the environment interface. The training component leverages a distributed framework for algorithm learning, incorporating a comprehensive algorithm zoo that includes two newly proposed baseline methods designed for adaptive teaming, along with an unseen drone zoo to assess generalization to diverse teammate behaviors. Finally, the deployment component bridges simulation to real-world applications by integrating a motion capture system and edge nodes, enabling real-time data exchange and decision-making for physical drone coordination.

\framework also advances adaptive teaming in machine learning by enabling multiple drones to dynamically collaborate with unseen teammates in complex, continuous-action environments, pushing the boundaries of multi-agent coordination in real-world scenarios.
In machine learning, zero-shot coordination (ZSC)~\cite{hu2020other} and ad-hoc teamwork (AHT)~\cite{stone2010ad} offer alternative paradigms for handling unseen partners. 
However, most of the research in this area is confined to training a single learner in simulated 2D video games with discrete action spaces, such as Overcooked~\cite{li2023cooperative,wang2024zsc}, Hanabi~\cite{hu2020other,anyplay,canaan2022generating,bard2020hanabi}, and Predator-Prey~\cite{barrett2011empirical,papoudakis2021agent}. 
A recent advancement, NAHT~\cite{wang2024n}, extends AHT to control multiple learners interacting with multiple unseen partners. However, it remains restricted to the SMAC benchmark based on video games~\cite{samvelyan2019starcraft} with discrete actions, limiting its applicability to real-world continuous-action drone tasks.

To validate \framework, we conducted extensive experiments in four multi-drone pursuit environments of increasing difficulty, evaluating their adaptability to diverse unseen teammate dynamics. The experiments incorporate multiple unseen drone protocols to ensure a broad spectrum of coordination challenges. The results show that our proposed baseline methods effectively address \problem, consistently outperforming existing approaches in coordination success and adaptability. 
Furthermore, real-world deployment on Crazyflie drones demonstrates the feasibility of \framework in physical systems, while highlighting opportunities for further improvements in algorithms, more realistic task scenarios, and enhanced sim-to-real transfer.
% \ying{It would be more appropriate to say that there is room for improvement (e.g., in algorithms or sim-to-real deployment), which enhances the significance of this work.}.
A case study in a high-complexity scenario further highlights the emergence of adaptive strategies in dynamic, real-world-inspired conditions.

The main contributions of this paper are threefold:\textbf{\textit{ (1) }}To the best of our knowledge, this work is the first to formally define and formulate the adaptive teaming problem in multi-drone pursuit, advancing multi-agent coordination in complex, continuous-action real-world scenarios.
\textbf{\textit{ (2) }} We propose a comprehensive \framework that integrates flexible simulation, algorithm training, and real-world deployment, paving the way for further research in this area;
\textbf{\textit{ (3) }} We validate the effectiveness and feasibility of our baseline methods through extensive simulations and real-world experiments with diverse, previously unseen drone partners.

\begin{table*}[!ht]
    \centering
    \caption{Comparison of \framework with main related works. Grey rows represent literature on multi-drone pursuit, while pink rows highlight adaptive teaming studies from the machine learning field. ``AT w/o TM'' and ``AT w/ TM' denotes adaptive teaming without and with, respectively. \textbf{ A complete tabular comparison of related works is provided in Table~\ref{appendix:tab_rw} in the Appendix.}}
    \resizebox{0.9\linewidth}{!}{
    \begin{tabular}{c|c|c|c|c|c|c|c|c}
        \toprule
        \multirow{2}{*}{\textbf{Related Work}} & \multicolumn{4}{c|}{\textbf{Problem Setting}}& \multicolumn{2}{c|}{\textbf{Task}} & \multicolumn{2}{c}{\textbf{Method}} \\
        \cline{2-9}
        & \textbf{\# Learner} & \textbf{\# Unseen} & \textbf{\# Evader} &\textbf{Action Space} & \textbf{Main Related Task} & \textbf{Real-world?} & \textbf{AT w/o TM?} & \textbf{AT w/ TM?} \\
        \midrule 
        \rowcolor{gray!10} 
        Voronoi Partitions~\cite{zhou_cooperative_2016} & Multi & 0 & 1  & Continuous &  Pursuit–evasion Game & \No & \No & \No \\ 
        \cline{1-9}
        \rowcolor{gray!10} 
        Bio-pursuit~\cite{janosov_group_2017}  & Multi & 0 & Multi & Continuous &  Prey–predator Game & \No & \No & \No \\
        \cline{1-9}
        \rowcolor{gray!10}
        M3DDPG~\cite{li_robust_2019} & Multi & 0 & 1 & Continuous &  Prey–predator Game & \No & \No & \No \\ 
        \cline{1-9}
        \rowcolor{gray!10}
        DualCL~\cite{chen2024dualcurriculumlearningframework} & Multi & 0 & 1 & Continuous & \textbf{\textcolor{black}{Multi-drone Pursuit}} & \No & \No & \No \\ 
        \cline{1-9}
        \rowcolor{gray!10}
        Pursuit-TD3~\cite{de_souza_decentralized_2021} & Multi & 0 & 1 & Continuous &  \textbf{\textcolor{black}{Multi-drone Pursuit}} & \Yes & \No & \No \\ 
        \cline{1-9}
        \rowcolor{gray!10} 
        DACOOP-A~\cite{ZhangDACOOP2023} & Multi & 0 & 1 & Discrete &  \textbf{\textcolor{black}{Multi-drone Pursuit}} & \Yes & \No & \No \\  
        \cline{1-9}
        \rowcolor{gray!10} 
        HOLA-Drone~\cite{hola-drone}  &  1 & Multi & Multi & Continuous & \textbf{\textcolor{black}{Multi-drone Pursuit}} & \Yes & \Yes  & \No \\ 
        \midrule
        \rowcolor{pink!30} 
        Other-play~\cite{hu2020other} & 1 & 1 & 0 &  Discrete & Lever Game; Hanabi &  \No & \Yes  & \No \\ 
        \cline{1-9} 
        \rowcolor{pink!30} 
        COLE~\cite{li2024tackling}  & 1 & 1 & 0 &  Discrete & Overcooked &  \No & \Yes  & \No \\ 
        \cline{1-9}
       \rowcolor{pink!30}  
       PLASTIC~\cite{barrett2017making}  &  1 & Multi & Multi & Discrete & Prey-predator Game &  \No & \No & \Yes \\ 
       \cline{1-9} 
        \rowcolor{pink!30} 
        AATeam~\cite{chen2020aateam}  & 1 & 1 & 2 &  Discrete & Half Field Offense &  \No & \No & \Yes  \\  
       \cline{1-9} 
       \rowcolor{pink!30}   
       CIAO~\cite{jianhong2024oaht} &  1 & Multi & Multi & Discrete &LBF; Wolfpack &  \No & \No & \Yes \\ 
       \cline{1-9} 
       \rowcolor{pink!30}   
       NAHT~\cite{wang2024n}  & Multi & Multi & Multi & Discrete & StarCraft2 &  \No & \No & \Yes \\
         \midrule
         \rowcolor{orange!30} 
         Our ATMDP  & Multi & Multi & Multi & Continuous & \textbf{\textcolor{black}{Multi-drone Pursuit}} &  \Yes & \Yes & \Yes \\
         \bottomrule
    \end{tabular}
    }
    \vspace{-5mm}
    \label{tab:review}
\end{table*}

\section{Problem Formulation and Related Work}

In this section, we first formalize the concept of adaptive teaming in multi-drone pursuit. Next, we discuss the relationship between our work and prior research, as summarized in Table~\ref{tab:review}. 
A more detailed literature review and complete tabular comparison of related works can be found in the Appendix~\ref{appendix:rw}.

\subsection{Problem Formulation}
\begin{definition}[Adaptive Teaming in Multi-Drone Pursuit]
Adaptive teaming in multi-drone pursuit involves training a set of $N \in \{1, 2, \dots\}$ drone agents, referred to as learners, to dynamically coordinate with $M \in \{1, 2, \dots\}$ previously unseen partners. The objective is to pursue $K \in \{1, 2, \dots\}$ targets without collisions, optimizing the overall return.
\end{definition}

Let $\gC$ represent the cooperative team, comprising $N$ learners and $M$ uncontrolled teammates. The set of uncontrolled teammates is denoted by $\gU$. In the multi-drone pursuit task, there exists a set of opponents, denoted as $\gE$.

Adaptive teaming can be effectively modeled as an extended \textbf{A}daptive \textbf{T}eaming \textbf{Dec}entralized \textbf{P}artially \textbf{O}bservable \textbf{M}arkov \textbf{D}ecision \textbf{P}rocess (AT-Dec-POMDP). AT-Dec-POMDP is defined by the tuple $(\gS, \gC, \gA, \gP,\gP_\text{u}, r, \gO, \gamma, T)$, where:
where $\gS$ is the joint state space; $\gC$ is the set of cooperative agents, including learners ($\gN$) and uncontrolled teammates ($\gM$); $\gA = \times_{j=1}^C \gA^j$ is the joint action space, where $C = N + M$ is the team size; $\gP_\text{u}(\gM | \gU)$ is the uncontrolled teammate sampling function, which defines the probability of sampling a subset $\gM \subseteq \gU$ of size $M$ from the set of all uncontrolled teammates $\gU$; $\gP(s’|s, a)$ is the transition probability function, representing the probability of transitioning to state $s’ \in \gS$ given the current state $s \in \gS$ and joint action $a \in \gA$; $r(s, a)$ is the reward function, representing the team’s reward in state $s$ after taking action $a$; $\gO$ is the joint observation space, with $\gO(o|s)$ describing the probability of generating observation $o$ given state $s$; $\gamma \in [0, 1]$ is the discount factor; and $T$ is the task horizon.

At each time step $t$, the AT-Dec-POMDP is in state $s_t \in \gS$ and generates a joint observation $o_t = (o_t^1, \dots, o_t^C) \sim \gO(\cdot | s_t)$. Each agent $j \in \gC$ observes $o_t^j$ and maintains its own action-observation trajectory $\tau_t^j = (o_0^j, a_0^j, \dots, o_{t-1}^j, a_{t-1}^j, o_t^j)$. 
Furthermore, we denote \(\gT^j\) as the set of all possible partial observations and action histories for the agent \(j\).
Using its policy $\pi^j$, the agent selects an action $a_t^j \in \gA^j$. 
The policies of the $N$ learners, denoted by $\pi^i$ for $i \in \gN$, are learnable.

We define the policy in two approaches: with and without teammate modeling. Adaptive teaming without teammate modeling is closely related to the zero-shot coordination problem~\cite{hu2020other,carroll2019utility}, where learners must rapidly coordinate with unseen teammates. Specifically, the policy for a learner $i$ is represented as $\pi^i(a_t^i \mid \tau_t^i)$, where $\tau_t^i$ denotes the learner's observation history up to time $t$.
On the other hand, adaptive teaming with teammate modeling aligns closely with the ad-hoc teamwork paradigm~\cite{stone2010ad}, where learners explicitly model their teammates using a separate teammate model $f$. In this case, the policy is defined as $\pi^i(a_t^i \mid \tau_t^i, f(\tau_t^i))$.
The joint action $a_t = (a_t^1, \dots, a_t^C)$ determines the next state $s_{t+1} \sim \gP(s_{t+1} \mid s_t, a_t)$, and all agents receive a shared reward $r(s_t, a_t)$.

The goal of adaptive teaming is to learn policies $\{\pi^i\}_{i \in \gN}$ that maximize the expected discounted return:
$
    \gJ = \mathbb{E}[R(\tau)] = \mathbb{E}\left[\sum_{t=0}^T \gamma^t r(s_t, a_t)\right],
    \label{eq:obj}
$ where $\tau$ denotes the joint trajectory.

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/system.pdf}
    \vspace{-3mm}
    \caption{\textbf{Overview of \framework.}
    \framework consists of three core components: Simulation, Training, and Deployment. The Simulation module provides an Environment Configurator and Interface for flexible scenario customization and a simulated training environment. The Training module operates in a distributed framework, featuring an Algorithm Zoo  and an Unseen Drone Zoo for evaluating adaptive teaming. The Deployment module integrates learned policies into real-world tasks using a motion capture system and edge nodes for real-time decision-making, seamlessly bridging simulation, training, and deployment.
    }
    \vspace{-3mm}
    \label{fig:system}
\end{figure*}

\subsection{The Relation to Prior Works}
% \ying{The logics here are not clear to me? maybe we can re-organize the content from the differences between our setting and existing settings? }

In this section, we highlight the key distinctions between \framework and prior works. Tables~\ref{tab:review} and Table~\ref{appendix:tab_rw} provide a comprehensive comparison across various dimensions, including problem formulation, scope of tasks, and methodological approach. \framework introduces a novel adaptive teaming framework tailored for multi-drone pursuit, setting it apart from existing methods. 
By pushing the boundaries of existing research, \framework paves the way for breakthroughs in multi-drone applications for disaster response, border surveillance, and counter-terrorism.

\textbf{Comparison with Multi-Drone Pursuit Literature (Gray Rows).}  
The studies highlighted in gray focus on pursuit-evasion tasks, with several works~\cite{zhou_cooperative_2016,janosov_group_2017,li_robust_2019,chen2024dualcurriculumlearningframework} restricted to simulated environments without real-world deployment. Others, such as Pursuit-TD3~\cite{de_souza_decentralized_2021} and DACOOP-A~\cite{ZhangDACOOP2023}, operate under pre-coordinated settings, where all pursuers are jointly trained to optimize performance in real-world drone pursuit tasks. 
Unlike these approaches, while HOLA-Drone~\cite{hola-drone} is a recent attempt at adaptive teaming, it is limited to a single learner coordinating with multiple teammates without explicit teammate modeling.
\framework overcomes these limitations by enabling multiple learners to dynamically adapt and coordinate with unseen teammates in continuous-action real-world multi-drone pursuit tasks.

\textbf{Comparison with Adaptive Teaming Literature (Pink Rows).}
In the machine learning community, zero-shot coordination (ZSC) and ad-hoc teamwork (AHT) are closely related to adaptive teaming but have been explored primarily in discrete-action video game environments, as shown in the pink rows of Table~\ref{tab:review}.
Almost all ZSC methods, including Other-play~\cite{HARL}, TrajDi~\cite{TrajDi}, MEP~\cite{MEP}, LIPO~\cite{charakorn2023generating}, COLE~\cite{li2024tackling}, and ZSC-Eval~\cite{wang2024zsc}, adopt a two-player game formulation with discrete action spaces.
For AHT, most existing methods can be interpreted as a specific case of the AT-Dec-POMDP formulation with $N=1$, where a single learner adapts to unseen teammates~\cite{stone2010ad,rahman2022generating,Rahman2024Mini,barrett2017making,chen2020aateam,wang2024open}.
The recent NAHT~\cite{wang2024n} extends AHT to a multi-learner setting, closely resembling the AT-Dec-POMDP formulation with teammate modeling.
However, NAHT remains limited to discrete-action decision-making in video game environments, whereas \framework advances adaptive teaming by enabling continuous-action decision-making in real-world multi-drone pursuit tasks. 


\section{The Framework: AT-MDP}
\label{sec_mdpat}
To address the adaptive teaming problem in the multi-drone pursuit task, we propose a comprehensive framework called \framework, encompassing three key components: Simulation, Training, and Deployment, as illustrated in Fig.~\ref{fig:system}.
The Simulation component allows flexible customization of multi-drone pursuit environments through the environment configurator, while the environment interface provides a simulated environment for training and evaluation.
The Training component leverages a distributed training framework to train algorithms through interaction with the environment interface within the simulation. It also includes an algorithm zoo that features a series of baseline methods for solving \problem, as well as an unseen drone zoo that serves as a repository of various unseen drone policies to test adaptive teaming capabilities.
Finally, the Deployment component facilitates real-world applications using a motion capture system and edge nodes, enabling real-time data exchange and decision-making.
Section~\ref{sec:4.1} introduces the simulation and deployment modules in detail, while Section~\ref{sec:4.2} provides a detailed explanation of the training component.


\subsection{Training}
\label{sec:4.2}


The training module comprises three key components: a distributed training framework, an algorithm zoo, and an unseen drone zoo. 
To enable efficient interaction with environments, \framework supports distributed environment sampling, ensuring scalability and adaptability in training processes.
The algorithm zoo and the unseen drone zoo are the core components of the training module, providing a series of baseline methods for \problem and a diverse population of drone policies for evaluation.

\textbf{Algorithm Pool.} 
The algorithm pool in \framework includes MARL-based pre-coordinated approaches, self-play frameworks, population-based training (PBT) strategies, and our proposed methods for adaptive teaming. 
For MARL-based approaches, we use the MAPPO algorithm~\cite{yu2022surprising}, a strong baseline designed for multi-drone pursuit tasks, which leverages centralized training with decentralized execution for effective coordination. Self-play and PBT~\cite{carroll2019utility} serve as widely used methods for zero-shot coordination, with self-play enabling agents to iteratively learn through interactions with copies of themselves and PBT allowing a population of models to explore various strategies and share knowledge for improved generalization. Both self-play and PBT are implemented using the Independent PPO~\cite{PPO} framework, ensuring scalable and robust training for adaptive teaming tasks.

In addition to existing baseline methods, we propose a novel \textit{Open-Ended Population Training (OPT) algorithm to better address \problem without teammate modeling.}
It begins by defining a population of drone strategies, \(\Pi = \{\pi_1, \pi_2, \cdots, \pi_n\}\), where interactions among \(C\) teammates are represented as a hypergraph \(\mathcal{G} = (\Pi, \mathcal{E}, \mathbf{w})\). Here, \(\Pi\) represents the set of strategies, \(\mathcal{E}\) denotes the interaction relationships, and \(\mathbf{w}\) captures the results of these interactions. To evaluate coordination ability, the \textit{preference hypergraph} \(\mathcal{P}\mathcal{G}\) is derived from \(\mathcal{G}\), retaining only hyperedges that represent optimal teammate configurations. The cooperative ability of each strategy is quantified by its \textit{preference centrality}:
$\eta_\Pi(i) = \frac{d_{\mathcal{P}\mathcal{G}}(i)}{d_{\mathcal{G}}(i)},$
where \(d_{\mathcal{P}\mathcal{G}}(i)\) and \(d_{\mathcal{G}}(i)\) are the incoming degrees in \(\mathcal{P}\mathcal{G}\) and \(\mathcal{G}\), respectively.  
To enhance adaptability, the \textit{max-min preference oracle} alternates between two steps: identifying the most challenging teammate subsets (minimization) and updating the learner set to maximize performance against these subsets (maximization). The oracle updates the learner set by solving:
$
\mathcal{N}^\prime = \arg\max_{\mathcal{N}} \min_{\mathcal{M} \subseteq \Pi_{-\mathcal{N}}} \phi_\mathcal{M}(\mathcal{N}),
$
where $\mathcal{N}$ is the learners, $\Pi_{-\gN}$  the objective \(\phi_{\mathcal{M}}(\cdot)\) is formulated using the extended curry operator~\cite{balduzzi2019open}, which was originally developed for two-player games. The function \(\phi\) evaluates the interactions between \(N\) learners and \(M\) teammates, producing a real-valued performance outcome.
The max-min preference oracle forms the foundation of the OPT algorithm, which evolves the strategy population \(\Pi\) and iteratively refines the learner set \(\mathcal{N}\) over generations. By dynamically recalibrating the training objectives, OPT promotes robust coordination with diverse, unseen teammates, ensuring adaptability in complex real-world scenarios.
\textit{For detailed algorithmic steps and implementation, please refer to the Appendix~\ref{appendix:opt}.}

We propose an \textit{adaptive teaming with modeling (ATM) baseline} to efficiently model unseen drone teammates. ATM incorporates an additional teammate modeling network built on MAPPO~\cite{yu2022surprising}. The modeling network, denoted as \(f\), generates team encoding vectors that characterize the strategies of teammates. Specifically, the teammate modeling network \(f_{\text{tm}}: \gT^i \rightarrow \mathbb{R}^n\), parameterized by \(\theta_{tm}\), processes the modeling agent's history of local observations and actions. The resulting team encoding vector is integrated into the actor network within the MAPPO architecture. \textit{For detailed algorithmic implementation, please refer to Appendix~\ref{appendix:atm}.}

\textbf{Unseen Drone Zoo. }
To efficiently evaluate adaptive teaming performance, we construct an unseen drone zoo consisting of rule-based methods, traditional optimization-based methods, and learning-based approaches. This pool introduces diverse and challenging teammate behaviors to ensure a robust evaluation of \framework in multi-drone pursuit tasks, particularly in continuous-action settings.
Specifically, the unseen drone zoo includes three types of algorithms to evaluate adaptive teaming performance. The \textit{Greedy Drone} follows a rule-based approach, pursuing the closest target while dynamically avoiding obstacles and teammates. The \textit{VICSEK Drone}, based on the VICSEK algorithm~\cite{Janosov2017Group,ZhangDACOOP2023,hola-drone}, adopts a bioinspired strategy to mimic swarm-like behaviors, optimizing its path toward the evader while applying repulsive forces to avoid collisions. Finally, the \textit{Self-Play Drones} use an IPPO-based self-play framework, generating diverse behaviors through randomized training, introducing complexity and variability into the evaluation process.
For detailed algorithmic description and implementation, please refer to the Appendix~\ref{appendix:unseen_zoo}.

\subsection{Simulation and Deployment}
\label{sec:4.1}

\textbf{Simulation.} 
The simulation module offers a user-friendly and customizable framework through the environment configurator and interface, enabling seamless configuration of multi-drone pursuit scenarios. Fig.~\ref{fig:system} (bottom left) shows three examples of custom environments with photos from our real deployed system.

The environment configurator enables users to define all aspects of the simulation by categorizing configuration parameters into three key sections: players, site, and task, as shown in Fig.~\ref{fig:env_json} in Appendix~\ref{appendix:env_config}. This modular design ensures flexibility for a variety of experimental setups. 
The players section configures participants, including learners, unseen teammates, and evaders, specifying parameters such as their numbers, velocities, and the inclusion of an unseen drone zoo. 
The site section allows users to customize the physical environment, including map dimensions and obstacle layouts, enabling diverse experimental terrains. Finally, the task section defines the task-specific parameters that govern the rules and objectives of the pursuit scenario.


Environment interface provides a Gymnasium-based interface for reinforcement learning-based training and evaluation. This interface ensures seamless interaction between the training algorithms and the simulation, enabling efficient development and testing of adaptive teaming strategies.
By integrating these detailed configurations, the simulation module supports diverse and customized experimental setups while maintaining efficiency for training. Moreover, it bridges the gap between simulation and real-world multi-drone applications, facilitating a robust pipeline for research and deployment.

\textbf{Deployment.} 
The \framework provides a robust real-world deployment solution, integrating edge computing nodes such as the Nvidia Jetson AGX Orin, the OptiTrack motion capture system, and CrazyFlie drones. As shown on the right of Fig.~\ref{fig:system}, the deployment workflow combines all components into a unified decision-making and execution pipeline.
The learners’ policies and unseen drone partners, sampled from the unseen drone pool in the training module, are deployed on the edge nodes, which serve as inference engines for these policies.
In this setup, the Jetson AGX Orin edge computing node acts as the core computational unit, offering compact, energy-efficient, on-site processing capabilities. It enables real-time policy inference to support efficient drone decision-making during pursuit tasks. 
Currently deployed alongside CrazyFlie drones, its lightweight design holds potential for future integration directly onto drones, enabling fully autonomous and decentralized multi-drone systems.

Observations from the physical environment, including drone positions and movements, are captured by the OptiTrack motion capture system, which provides high-precision localization and tracking. 
The processed observations are sent to the edge nodes, which supply real-time data to the inference engine. The inference engine interprets the data and generates appropriate actions based on the policies deployed. 
These actions are then communicated to the drones for real-time execution. The drones provide continuous feedback to the system, facilitating dynamic adjustments and enhancing coordination among agents.

\begin{table*}[!ht]
\centering
\vspace{-5mm}
\caption{Performance comparison across different difficulty levels for adaptive teaming without teammate modeling. The means and standard
deviations are calculated over five different random seeds, with each seed undergoing 50 repeated
runs.}
\resizebox{0.85\linewidth}{!}{
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|c}
\toprule
\multirow{2}{*}{\textbf{ENV}} & \multirow{2}{*}{\textbf{Metrics}} &\multicolumn{3}{c|}{\textbf{Unseen Zoo 1}} & \multicolumn{3}{c|}{\textbf{Unseen Zoo 2}} & \multicolumn{3}{c|}{\textbf{Unseen Zoo 3}} & \multicolumn{3}{c}{\textbf{Unseen Zoo 4}} \\
\cline{3-14}
                  & & \textbf{PBT}  &\textbf{ MAPPO} & \textbf{OPT}  & \textbf{PBT}  & \textbf{MAPPO} & \textbf{OPT}  & \textbf{PBT}  & \textbf{MAPPO} & \textbf{OPT}  & \textbf{PBT}  & \textbf{MAPPO} & \textbf{OPT}  \\
                  \midrule
\multirow{8}{*}{\medium}
& \multirow{2}{*}{\textbf{SUC} $\uparrow$} & 65.20 & 74.00 &\textbf{ 77.67 }& 62.00 & 70.80 & \textbf{86.00} & 54.80 & 65.20 & \textbf{77.20} & 64.80 & 74.80 & \textbf{82.80} \\
& & $\pm$ 17.75 & $\pm$ 4.69 & $\pm$ 6.81 & $\pm$ 12.41 & $\pm$ 9.23 & $\pm$ 3.46 & $\pm$ 11.54 & $\pm$ 7.69 & $\pm$ 3.03 & $\pm$ 15.01 & $\pm$ 5.40 & $\pm$ 1.79 \\  \cline{2-14}
% split line
& \multirow{2}{*}{\textbf{COL} $\downarrow$} & 32.40 & 25.60 & \textbf{22.33} & 27.60 & 22.40 & \textbf{13.20} & 39.20 & 34.00 & \textbf{22.40} & 31.60 & 24.00 & \textbf{16.80} \\
& & $\pm$ 15.19 & $\pm$ 4.34 & $\pm$ 6.81 & $\pm$ 13.30 & $\pm$ 12.60 & $\pm$ 4.15 & $\pm$ 12.30 & $\pm$ 7.35 & $\pm$ 3.58 & $\pm$ 14.38 & $\pm$ 4.90 & $\pm$ 2.28 \\ \cline{2-14}
% split line 
& \multirow{2}{*}{\textbf{AST} $\downarrow$} & 313.48 & 303.21 & \textbf{259.17} & 435.65 & 386.32 & \textbf{370.95} & 380.28 & 376.14 & \textbf{314.97} & 380.85 & 328.96 & \textbf{306.35} \\
&  & $\pm$ 86.38 & $\pm$ 51.91 & $\pm$ 34.28 & $\pm$ 113.16 & $\pm$ 29.18 & $\pm$ 31.36 & $\pm$ 68.19 & $\pm$ 34.65 & $\pm$ 27.68 & $\pm$ 85.25 & $\pm$ 34.36 & $\pm$ 25.86 \\ \cline{2-14}
% split line
& \multirow{2}{*}{\textbf{REW} $\uparrow$} & 123.60 & 132.34 & \textbf{138.28} & 130.51 & 136.40 & \textbf{149.89} & 114.00 & 121.98 & \textbf{135.71} & 126.15 & 133.11 & \textbf{143.55} \\
& & $\pm$ 22.68 & $\pm$ 4.68 & $\pm$ 7.95 & $\pm$ 8.77 & $\pm$ 8.50 & $\pm$ 3.28 & $\pm$ 10.13 & $\pm$ 9.07 & $\pm$ 3.46 & $\pm$ 13.39 & $\pm$ 4.68 & $\pm$ 1.29 \\
\midrule
% split line
\multirow{8}{*}{\easy}  
& \multirow{2}{*}{\textbf{SUC} $\uparrow$} & 68.80 & 69.20 & \textbf{80.80} & 63.20 & 64.40 & \textbf{73.20} & 59.20 & 53.20 & \textbf{67.60} & 62.40 & 63.20 & \textbf{75.60} \\
& & $\pm$ 5.02 & $\pm$ 11.71 & $\pm$ 7.56 & $\pm$ 15.27 & $\pm$ 12.20 & $\pm$ 3.03 & $\pm$ 8.07 & $\pm$ 14.11 & $\pm$ 5.18 & $\pm$ 3.85 & $\pm$ 14.46 & $\pm$ 5.55 \\ \cline{2-14}
% split line
& \multirow{2}{*}{\textbf{COL} $\downarrow$} & 30.80 & 30.40 & \textbf{19.20} & 31.20 & 28.80 & \textbf{23.20} & 36.80 & 43.20 &\textbf{ 32.00} & 34.80 & 35.60 & \textbf{23.60} \\
&  & $\pm$ 5.22 & $\pm$ 12.03 & $\pm$ 7.56 & $\pm$ 12.85 & $\pm$ 15.53 & $\pm$ 1.79 & $\pm$ 10.73 & $\pm$ 15.59 & $\pm$ 5.83 & $\pm$ 5.40 & $\pm$ 14.52 & $\pm$ 6.54 \\ \cline{2-14}
% split line
& \multirow{2}{*}{\textbf{AST} $\downarrow$} & 352.24 & 317.55 & \textbf{298.93} & 399.87 & 383.30 & \textbf{356.65} & 446.67 & 395.92 & \textbf{353.35} & 360.08 & 375.41 & \textbf{313.60} \\
& & $\pm$ 30.08 & $\pm$ 25.07 & $\pm$ 29.19 & $\pm$ 86.90 & $\pm$ 75.08 & $\pm$ 26.78 & $\pm$ 72.36 & $\pm$ 51.69 & $\pm$ 35.29 & $\pm$ 48.52 & $\pm$ 22.18 & $\pm$ 34.18 \\ \cline{2-14}
% split line
& \multirow{2}{*}{\textbf{REW}  $\uparrow$} & 122.05 & 126.73 &\textbf{ 138.24} & 125.85 & 127.30 & \textbf{134.26} & 112.66 & 109.06 & \textbf{120.83} & 118.73 & 120.00 & \textbf{131.50} \\
& & $\pm$ 6.91 & $\pm$ 9.92 & $\pm$ 6.28 & $\pm$ 13.38 & $\pm$ 10.62 & $\pm$ 3.04 & $\pm$ 13.42 & $\pm$ 11.94 & $\pm$ 6.84 & $\pm$ 5.91 & $\pm$ 13.22 & $\pm$ 5.76 \\
\midrule
% split line
% split line
\multirow{8}{*}{\hard} 
& \multirow{2}{*}{\textbf{SUC} $\uparrow$} & 40.40 & 47.00 & \textbf{50.00} & 62.40 & 72.00 & \textbf{75.20} & 67.60 & 65.50 & \textbf{74.00} & 54.80 & 58.50 & \textbf{67.60} \\
& & $\pm$ 2.61 & $\pm$ 10.39 & $\pm$ 11.64 & $\pm$ 3.29 & $\pm$ 3.65 & $\pm$ 6.72 & $\pm$ 14.59 & $\pm$ 3.42 & $\pm$ 4.24 & $\pm$ 2.28 & $\pm$ 5.97 & $\pm$ 6.39 \\ \cline{2-14}
% split line
& \multirow{2}{*}{\textbf{COL} $\downarrow$} & 59.60 & 53.00 & \textbf{49.86} & 35.20 & 24.00 & \textbf{22.80} & 31.20 & 33.50 & \textbf{25.20} & 44.00 & 40.50 & \textbf{32.00 }\\
& & $\pm$ 2.61 & $\pm$ 10.39 & $\pm$ 11.62 & $\pm$ 3.90 & $\pm$ 5.89 & $\pm$ 7.43 & $\pm$ 14.74 & $\pm$ 3.42 & $\pm$ 3.35 & $\pm$ 2.45 & $\pm$ 5.74 & $\pm$ 6.16 \\ \cline{2-14}
% split line
& \multirow{2}{*}{\textbf{AST} $\downarrow$} & 333.96 & \textbf{313.53} & 331.98 & \textbf{349.48} & 419.21 & 396.66 & 287.18 & 348.23 & \textbf{281.45} & \textbf{294.41} & 340.94 & 313.79 \\
& & $\pm$ 34.83 & $\pm$ 51.81 & $\pm$ 80.30 & $\pm$ 17.64 & $\pm$ 50.81 & $\pm$ 21.48 & $\pm$ 18.91 & $\pm$ 55.20 & $\pm$ 40.77 & $\pm$ 38.79 & $\pm$ 34.95 & $\pm$ 26.78 \\ \cline{2-14}
% split line
& \multirow{2}{*}{\textbf{REW} $\uparrow$} & 90.16 & 93.16 &\textbf{ 99.31} & 116.19 & 128.45 &\textbf{ 134.02 }& 120.48 & 124.92 & \textbf{128.45} & 107.56 & 107.08 & \textbf{119.93} \\
& & $\pm$ 7.47 & $\pm$ 12.00 & $\pm$ 17.70 & $\pm$ 8.28 & $\pm$ 4.27 & $\pm$ 6.12 & $\pm$ 19.11 & $\pm$ 8.33 & $\pm$ 3.79 & $\pm$ 5.92 & $\pm$ 5.44 & $\pm$ 6.11 \\
\midrule
\multirow{8}{*}{\superhard}  
& \multirow{2}{*}{\textbf{SUC} $\uparrow$} & \textbf{34.80} & \textbf{34.80} & 31.14 & 50.40 & 59.20 & \textbf{67.20} & 57.20 & 54.80 & \textbf{60.40} & 42.00 & 54.80 & \textbf{60.00} \\
& & $\pm$ 5.22 & $\pm$ 5.40 & $\pm$ 17.20 & $\pm$ 7.27 & $\pm$ 5.40 & $\pm$ 5.76 & $\pm$ 6.57 & $\pm$ 7.43 & $\pm$ 5.18 & $\pm$ 6.78 & $\pm$ 9.44 & $\pm$ 6.00 \\ \cline{2-14}
% split line
& \multirow{2}{*}{\textbf{COL} $\downarrow$} & \textbf{62.80} & 64.80 & 67.57 & 46.80 & 36.00 & \textbf{28.80} & 40.40 & 38.00 & \textbf{36.40} & 55.60 & 41.60 & \textbf{38.40} \\
& & $\pm$ 4.60 & $\pm$ 5.02 & $\pm$ 15.85 & $\pm$ 7.56 & $\pm$ 4.69 & $\pm$ 6.42 & $\pm$ 8.05 & $\pm$ 5.83 & $\pm$ 4.34 & $\pm$ 4.77 & $\pm$ 8.65 & $\pm$ 5.18 \\ \cline{2-14}
% split line
& \multirow{2}{*}{\textbf{AST} $\downarrow$}  & \textbf{431.44} & 509.56 & 459.55 & 457.68 & 489.79 & \textbf{394.81} & 446.07 & 555.10 & \textbf{407.85} & 425.94 & 510.96 & \textbf{416.91} \\
& & $\pm$ 67.14 & $\pm$ 11.17 & $\pm$ 91.71 & $\pm$ 33.64 & $\pm$ 51.33 & $\pm$ 48.65 & $\pm$ 38.97 & $\pm$ 49.11 & $\pm$ 63.06 & $\pm$ 60.10 & $\pm$ 38.53 & $\pm$ 76.21 \\ \cline{2-14}
% split line
& \multirow{2}{*}{\textbf{REW} $\uparrow$} & \textbf{141.98} & 131.29 & 116.57 & 168.82 & 187.30 & \textbf{193.78} & 182.04 & 187.32 & \textbf{196.07} & 146.74 & 172.06 & \textbf{174.59 }\\
& & $\pm$ 9.66 & $\pm$ 15.98 & $\pm$ 43.94 & $\pm$ 10.17 & $\pm$ 9.08 & $\pm$ 4.05 & $\pm$ 7.17 & $\pm$ 8.97 & $\pm$ 9.91 & $\pm$ 11.93 & $\pm$ 17.01 & $\pm$ 18.68 \\
% split line
\bottomrule
\end{tabular}}
\vspace{-3mm}
\end{table*}

\section{Experiment}
\label{sec:exp}
In this section, we evaluate the performance of baseline methods from the algorithm zoo and validate their feasibility in real-world multi-drone pursuit scenarios. The experiments are structured into two main parts: (1) adaptive teaming without teammate modeling, detailed in Section~\ref{sec:exp_1}, and (2) adaptive teaming with teammate modeling, discussed in Section~\ref{sec:exp_2}. Each subsection presents the experimental setup, key results, and in-depth analysis. Additionally, Section~\ref{sec:exp_case} provides a case study demonstrating real-world deployment, showcasing how adaptive learners coordinate with unseen drone partners to execute a multi-stage capture strategy. Further details on the real-world implementation are available in Appendix~\ref{appendix:real}.

We begin by introducing the shared experimental setups applicable to both groups of experiments. 

\textbf{Environments. } 
To systematically evaluate the performance of adaptive teaming, we leverage the environment configurator to design four multi-drone pursuit environments with varying levels of difficulty, denoted as \texttt{4p2e3o}, \texttt{4p2e1o}, \texttt{4p2e5o}, and \texttt{4p3e5o}. 
Fig.~\ref{fig:app_env} in the Appendix gives the screenshots of the four environments deployed in real world.
The notation represents the number of pursuers (p), evaders (e), and obstacles (o) in each setting. Each environment is initialized with defined spawn areas, where the evaders spawn within a 3.2m wide and 0.6m high region, while the pursuers spawn in a similarly sized region. 
The obstacle layouts introduce additional complexity.
Environment \texttt{4p2e3o} include three distributed barriers with different shapes: two cubes in the left area and one cylinder in the right area.
Although Environment \texttt{4p2e1o} only includes a cuboid obstacle in the middle, the difficulty is slight higher than \texttt{4p2e3o} for evaders have large space to escape.
In general, the difficulity of the two environments is easy.
Those environments with \texttt{5o} feature densely packed obstacles that significantly constrain movement. 
In terms of difficulty, \texttt{4p2e3o} is categorized as easy due to fewer obstacles and ample space for pursuit, \texttt{4p2e1o} presents a moderate challenge with a single strategically placed obstacle, \texttt{4p2e5o} is considered hard as it requires advanced maneuvering in a confined space, and \texttt{4p3e5o} is the most difficult (superhard) due to an additional evader and dense obstacle layout, requiring highly coordinated teamwork for successful pursuit. 


\textbf{Evaluation Metrics. } 
To evaluate the performance of the methods, we use the following metrics: \textbf{1. Success rate (SUC)}, which measures the proportion of tasks successfully completed. An episode is deemed successful if the pursuers capture both evaders, defined as reducing the distance between an evader and a pursuer to less than 0.2 metres; \textbf{2. Collision rate (COL)}, which tracks the frequency of collisions during task execution. A collision is recorded if the distance between any two pursuers is less than 0.2 metres or if the distance between a drone and an obstacle is less than 0.1 metres; \textbf{3. Average success timesteps (AST)}, which indicates the average number of steps taken to complete a task; \textbf{4. Average reward (REW)}, which reflects the overall efficiency and quality of the agent’s performance across episodes. These metrics collectively provide a comprehensive evaluation of the proposed method’s adaptability and effectiveness.

\subsection{Adaptive teaming without teammate modeling}
\label{sec:exp_1}
\subsubsection{Evaluation Protocol}

To ensure a rigorous and fair comparison of adaptive teaming, we standardize the evaluation process across all methods. Since AT-MDP is a newly introduced problem with no existing algorithms specifically designed for it, we establish  baselines using PBT~\cite{carroll2019utility}, MAPPO~\cite{yu2022surprising}, and our proposed OPT.

\textbf{Unseen Drone Partners. }
Unseen drone partners are a critical component of the evaluation protocol. To thoroughly assess the proposed method, we evaluate its performance across four distinct unseen drone zoos, with drone policies selected from the unseen drone pool in the AT-MDP framework. Each zoo represents a unique combination of drone behaviors, designed to create diverse and challenging scenarios that test the adaptability and robustness of the proposed method.
\textbf{(1) Unseen Zoo 1} and \textbf{(2) Unseen Zoo 3} include a Greedy drone policy and a VICSEK drone policy~\cite{Janosov2017Group}, respectively, providing additional diversity in drone behavior and coordination strategies.
\textbf{(3) Unseen Zoo 3} consists of two drone policies trained using a self-play PPO algorithm~\cite{PPO,carroll2019utility} with different random seeds. These policies are selected to reflect varying levels of coordination ability: one with strong coordination skills and another with relatively weaker performance. For example, in a 4p2e1o scenario, the success rates (SUC) of the two policies, when evaluated against themselves, are 70\% and 54\%, respectively.
\textbf{(4) Unseen Zoo 4} combines all policies from the other three zoos into a single zoo. At the beginning of each episode, unseen drone partners are randomly selected from this combined pool, introducing additional variability and unpredictability to the evaluation process. 

\subsubsection{Main Results}

\textbf{Performance Across Different Environments. }  
As the complexity of the environments increases (from \texttt{4p2e3o} to \texttt{4p3e5o}), the success rate (SUC) generally decreases across all methods, highlighting the increasing difficulty in coordinating with unseen teammates. In simpler environments like \texttt{4p2e3o} and \texttt{4p2e1o}, OPT consistently outperforms other baselines, achieving higher success rates and lower collision rates. In more challenging settings (\texttt{4p2e5o} and \texttt{4p3e5o}), performance gaps become more pronounced, with MAPPO and PBT struggling to generalise effectively.

\textbf{Effectiveness of OPT in Adaptive Teaming. } OPT achieves the highest success rates in most cases, particularly in Unseen Zoo 2 and Unseen Zoo 4, demonstrating its robustness in adapting to diverse teammate behaviors. It significantly reduces AST, meaning it captures evaders faster than other baselines, suggesting that its learning process leads to more efficient pursuit strategies. Moreover, OPT consistently achieves higher reward values, indicating its superior adaptability in unseen teaming scenarios.

\textbf{Impact of Unseen Drone Zoos. }   The performance of all methods varies significantly across different unseen drone zoos, each introducing distinct teammate behaviors. Unseen zoo 1 (Greedy agents) and Unseen zoo 2 (VICSEK-based agents) exhibit more predictable behaviors, allowing for more effective coordination, with all methods achieving relatively higher SUC scores. 
Unseen zoo 3, composed entirely of PPO-based self-play policies with two different skill levels, presents the most challenging setting, where all baselines experience performance drops due to the increased adaptability and unpredictability of teammates.
Unseen zoo 4, which includes a mix of all drone types with random selection, further increases variability, testing the generalization ability of each method. Notably, OPT remains the most stable approach, demonstrating robust adaptability across diverse teammate dynamics.
\vspace{-3mm}
% \subsubsection{Case Study}
% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{figures/case_1.pdf}
%     \caption{\textbf{Case Study:} This example illustrates the capture strategy formed by learners and unseen drone partners from unseen zoo 3 in the environment \medium. The figures represent key frames from the scenario. The red circles denote pursuers, while the black squares represent evaders. In this scenario, four pursuers collaboratively surround an evader (frame 1), tighten their formation into a circle (frame 2), and successfully capture the target (frame 3).}
%     \label{fig:enter-label}
% \end{figure}

\subsection{Adaptive teaming with teammate modeling}
\label{sec:exp_2}
We also conduct a group of experiments about adatptive teaming with teammates modeling. As shown Table.~\ref{tab:exp_main_2} presents the performance comparison between MAPPO and ATM across different difficulty levels. As the environment complexity increases (from \texttt{4p2e3o} to \texttt{4p3e5o}), both methods exhibit a decline in success rate (SUC) and an increase in average steps to capture (AST), highlighting the growing challenge of coordinating with teammates in more constrained and adversarial settings. 
ATM consistently outperforms MAPPO in success rate across all environments, demonstrating its improved adaptability in dynamic multi-agent interactions. Notably, ATM achieves a significantly higher success rate in \texttt{4p2e3o} and \texttt{4p2e1o}, suggesting its effectiveness in simpler settings. However, as the complexity increases (\texttt{4p2e5o} and \texttt{4p3e5o}), the performance gap narrows, indicating the growing challenge of maintaining robust coordination under more difficult conditions. 
In terms of collision rate (COL), ATM generally results in lower or comparable values to MAPPO, suggesting that its teammate modeling mechanism helps mitigate unnecessary collisions. The reward metric (REW) further supports these findings, as ATM achieves higher values across all settings, reflecting its superior learning of adaptive strategies. 

\vspace{-2mm}
\subsection{Case Study}
\label{sec:exp_case}
To further illustrate the effectiveness of our adaptive teaming approach, we present a case study in the \texttt{4p3e5o} environment, categorized as superhard due to its high complexity, featuring four pursuers, three evaders, and five obstacles. 
The unseen teammates in this scenario are sampled from Unseen Zoo 3, which consists entirely of PPO-based self-play policies trained at two different skill levels, introducing high adaptability and unpredictability.

This case study demonstrates how the ATM learners effectively coordinate with their unseen drone partners to execute a multi-stage capture strategy. Fig.~\ref{fig:case_study} illustrates key frames from the scenario. In Frame 1, four pursuers initiate a collaborative approach, positioning themselves strategically to encircle all three evaders while maintaining an adaptive formation. In Frame 2, two pursuers successfully capture one of the evaders while the other two tighten their formation, preventing the remaining evaders from escaping. In Frames 3 \& 4, the remaining two evaders are captured one by one as the pursuers continue refining their positioning and coordination, effectively closing all escape routes.


\begin{table}[t]
\vspace{-3mm}
\centering
\caption{Performance comparison across different difficulty levels. Metrics are reported as mean $\pm$ standard deviation. 
% \ying{Table layout can be optimized}}
}
\resizebox{0.9\linewidth}{!}{%
\begin{tabular}{l|cc|cc|cc|cc}
\toprule
\multirow{2}{*}{\textbf{Metrics}} & \multicolumn{2}{c|}{\textbf{\medium}} & \multicolumn{2}{c|}{\textbf{\easy}} & \multicolumn{2}{c|}{\textbf{\hard}} & \multicolumn{2}{c}{\textbf{\superhard}} \\
\cline{2-9}
& \textbf{MAPPO} & \textbf{ATM} & \textbf{MAPPO} & \textbf{ATM} & \textbf{MAPPO} & \textbf{ATM} & \textbf{MAPPO} & \textbf{ATM} \\
\midrule
% \textbf{SUC} $\uparrow$ & 72.80 $\pm$ 4.15 & 77.20 $\pm$ 6.87 & 71.60 $\pm$ 3.29 & 72.00 $\pm$ 4.00 & 52.40 $\pm$ 5.55 & 72.40 $\pm$ 5.55 & 51.20 $\pm$ 7.95 & 47.60 $\pm$ 22.78 \\
% \textbf{COL} $\downarrow$ & 26.40 $\pm$ 4.56 & 22.00 $\pm$ 7.62 & 26.40 $\pm$ 3.58 & 26.40 $\pm$ 6.07 & 47.60 $\pm$ 5.55 & 26.80 $\pm$ 6.10 & 48.00 $\pm$ 7.48 & 51.20 $\pm$ 21.05 \\
% \textbf{AST} $\downarrow$ & 297.46 $\pm$ 37.98 & 277.55 $\pm$ 17.55 & 370.15 $\pm$ 65.15 & 326.03 $\pm$ 48.37 & 289.01 $\pm$ 26.45 & 283.59 $\pm$ 45.80 & 408.28 $\pm$ 45.57 & 407.04 $\pm$ 79.02 \\
% \textbf{REW} $\uparrow$ & 133.03 $\pm$ 4.21 & 136.84 $\pm$ 6.91 & 128.79 $\pm$ 1.79 & 130.08 $\pm$ 4.71 & 104.79 $\pm$ 3.76 & 128.45 $\pm$ 6.17 & 164.19 $\pm$ 14.67 & 158.35 $\pm$ 40.09 \\
\multirow{2}{*}{\textbf{SUC} $\uparrow$}  & 72.80 & 77.20 & 71.60 & 72.00 & 52.40 & 72.40 & 51.20 & 62.00 \\ 
& ($\pm$ 4.15) & ($\pm$ 6.87) & ($\pm$ 3.29) & ($\pm$ 4.00) & ($\pm$ 5.55) & ($\pm$ 5.55) & ($\pm$ 7.95) & ($\pm$ 8.00) \\
\cline{1-9}
\multirow{2}{*}{\textbf{COL} $\downarrow$} & 26.40 & 22.00 & 26.40 & 26.40 & 47.60 & 26.80 & 48.00 & 38.00 \\
& ($\pm$ 4.56) & ($\pm$ 7.62) & ($\pm$ 3.58) & ($\pm$ 6.07) & ($\pm$ 5.55) & ($\pm$ 6.10) & ($\pm$ 7.48) & ($\pm$ 8.00) \\
\cline{1-9}
\multirow{2}{*}{\textbf{AST} $\downarrow$}  &  297.46 & 277.55 & 370.15 & 326.03 & 289.01 & 283.59 & 408.28 & 372.33 \\
&($\pm$ 37.98) & ($\pm$ 17.55) & ($\pm$ 65.15) & ($\pm$ 48.37) & ($\pm$ 26.45) & ($\pm$ 45.80) & $(\pm$ 45.57) & ($\pm$ 17.39) \\
\cline{1-9}
\multirow{2}{*}{\textbf{REW} $\uparrow$}  & 133.03 & 136.84 & 128.79 & 130.08 & 104.79 & 128.45 & 164.19 & 180.54 \\
& ($\pm$ 4.21) & ($\pm$ 6.91) & ($\pm$ 1.79) & ($\pm$ 4.71) & ($\pm$ 3.76) & ($\pm$ 6.17) & ($\pm$ 14.67) & ($\pm$ 15.67) \\
\bottomrule
\end{tabular}%
}
\vspace{-8mm}
\label{tab:exp_main_2}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/case_2.pdf}
    \caption{\textbf{Case Study:} This example demonstrates the capture strategy executed by ATM learners and unseen drone partners from unseen zoo 3 in the superhard environment \superhard. The red circles denote pursuers, and the black squares represent evaders. 
    In this scenario, four pursuers collaboratively surround all three evader (1), two pursuers capture one of evaders while other two pursuers continuously tighten their formation (2), and rest of two evaders are then successfully captured one by one (3 \& 4)}
    \label{fig:case_study}
\vspace{-3mm}
\end{figure}

% \begin{table}[t]
% \centering
% \resizebox{\linewidth}{!}{
% \begin{tabular}{c|c|c|c|c|c|c|c|c}
% \toprule
% \multirow{2}{*}{\textbf{Metrics}} &\multicolumn{2}{c|}{\textbf{\medium}} & \multicolumn{2}{c|}{\textbf{\easy}} & \multicolumn{2}{c|}{\textbf{\hard}} & \multicolumn{2}{c}{\textbf{\superhard}} \\
% \cline{2-9}
%                   & \textbf{MAPPO}  &\textbf{ATM} & \textbf{MAPPO}  & \textbf{ATM}  & \textbf{MAPPO}  & \textbf{ATM}  & \textbf{MAPPO}  & \textbf{ATM} \\
%                   \midrule
% \multirow{2}{*}{\textbf{SUC} $\uparrow$}  & 72.80 & 77.20 & 71.60 & 72.00 & 52.40 & 72.40 & 51.20 & 47.60 \\ 
% &$\pm$ 4.15 & $\pm$ 6.87 & $\pm$ 3.29 & $\pm$ 4.00 & $\pm$ 5.55 & $\pm$ 5.55 & $\pm$ 7.95 & $\pm$ 22.78 \\
% \cline{1-9}
% \multirow{2}{*}{\textbf{COL} $\downarrow$} & 26.40 & 22.00 & 26.40 & 26.40 & 47.60 & 26.80 & 48.00 & 51.20 \\
% & $\pm$ 4.56 & $\pm$ 7.62 & $\pm$ 3.58 & $\pm$ 6.07 & $\pm$ 5.55 & $\pm$ 6.10 & $\pm$ 7.48 & $\pm$ 21.05 \\
% \cline{1-9}
% \multirow{2}{*}{\textbf{AST} $\downarrow$}  &  297.46 & 277.55 & 370.15 & 326.03 & 289.01 & 283.59 & 408.28 & 407.04 \\
% &$\pm$ 37.98 & $\pm$ 17.55 & $\pm$ 65.15 & $\pm$ 48.37 & $\pm$ 26.45 & $\pm$ 45.80 & $\pm$ 45.57 & $\pm$ 79.02 \\
% \cline{1-9}
% \multirow{2}{*}{\textbf{REW} $\uparrow$}  & 133.03 & 136.84 & 128.79 & 130.08 & 104.79 & 128.45 & 164.19 & 158.35 \\
% & $\pm$ 4.21 & $\pm$ 6.91 & $\pm$ 1.79 & $\pm$ 4.71 & $\pm$ 3.76 & $\pm$ 6.17 & $\pm$ 14.67 & $\pm$ 40.09 \\
% \bottomrule
% \end{tabular}
% }
% \end{table}
\vspace{-3mm}
\section{Conclusion}
In this paper, we define and formalize the adaptive teaming in the multi-drone pursuit problem and introduce \framework, a comprehensive framework that integrates simulation, training, and real-world deployment to address this challenge. \framework provides a user-friendly and flexible environment configurator, a distributed training framework incorporating an extensive algorithm zoo and an unseen drone zoo, and a real-world deployment system leveraging edge computing and Crazyflie drones. 
To the best of our knowledge, \framework is the first adaptive teaming framework designed for continuous-action multi-drone pursuit tasks, extending multi-agent coordination beyond pre-coordinated strategies. Our experiments in four environments of increasing difficulty validate the effectiveness of our proposed baseline methods, demonstrating their adaptability to unseen teammates and superior performance compared to existing approaches. Furthermore, real-world experiments confirm the feasibility of \framework in physical drone systems, bridging the gap between simulation and real-world deployment.

\textbf{Limitations and Future Work.} While \framework successfully bridges simulation and real-world deployment, the current real-world system is relatively simple, relying on Crazyflie drones without onboard perception, LiDAR, or computer vision capabilities. Furthermore, the scenarios tested, while diverse, could be further enhanced to better reflect real-world complexities. Future work will focus on integrating perception modules, expanding the framework to more realistic environments, and developing a multi-drone system equipped with edge computing devices, such as Jetson, to enable fully autonomous adaptive teaming. 

\clearpage
\section{Impact Statement}

This paper advances the field of machine learning and multi-robot systems by introducing Adaptive Teaming in Multi-Drone Pursuit (AT-MDP) and proposing \framework, a comprehensive framework for simulation, training, and real-world deployment. 
Our work contributes to multi-drone system by enabling multiple drones to dynamically coordinate with unseen teammates in continuous-action real-world tasks.

The potential societal benefits of this research include improvements in search-and-rescue operations, disaster response, border surveillance, and autonomous security systems, where adaptive coordination among unseen agents can enhance efficiency and safety. Additionally, the open-source implementation of our framework fosters reproducibility and further research in adaptive teaming and multi-agent collaboration.

From an ethical perspective, while multi-drone pursuit has positive real-world applications, the technology also has potential risks if misused, such as surveillance or adversarial applications. We emphasize the importance of responsible AI development and adherence to ethical guidelines in deploying autonomous systems. Future work should consider human-in-the-loop oversight and safety constraints to ensure the ethical use of AI-driven multi-agent systems.
\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\input{appendix}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
