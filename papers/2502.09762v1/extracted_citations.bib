@article{DDPG,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, TP},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}

@inproceedings{MEP,
  title={Maximum entropy population-based training for zero-shot human-ai coordination},
  author={Zhao, Rui and Song, Jinming and Yuan, Yufeng and Hu, Haifeng and Gao, Yang and Wu, Yi and Sun, Zhongqian and Yang, Wei},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={5},
  pages={6145--6153},
  year={2023}
}

@inproceedings{TD3,
  title={Addressing function approximation error in actor-critic methods},
  author={Fujimoto, Scott and Hoof, Herke and Meger, David},
  booktitle={International conference on machine learning},
  pages={1587--1596},
  year={2018},
  organization={PMLR}
}

@inproceedings{TrajDi,
  title={Trajectory diversity for zero-shot coordination},
  author={Lupu, Andrei and Cui, Brandon and Hu, Hengyuan and Foerster, Jakob},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={7204--7213},
  year={2021},
  organization={PMLR}
}

@article{ZhangDACOOP2023,
author = {Zhang, Zheng and Zhang, Dengyu and Zhang, Qingrui and Pan, Wei and Hu, Tianjiang},
year = {2023},
month = {11},
pages = {1-8},
title = {{DACOOP-A}: Decentralized Adaptive Cooperative Pursuit via Attention},
volume = {PP},
journal = {IEEE Robotics and Automation Letters},
doi = {10.1109/LRA.2023.3331886}
}

@article{barrett2017making,
  title={Making friends on the fly: Cooperating with new teammates},
  author={Barrett, Samuel and Rosenfeld, Avi and Kraus, Sarit and Stone, Peter},
  journal={Artificial Intelligence},
  volume={242},
  pages={132--171},
  year={2017},
  publisher={Elsevier}
}

@article{carroll2019utility,
  title={On the utility of learning about humans for human-ai coordination},
  author={Carroll, Micah and Shah, Rohin and Ho, Mark K and Griffiths, Tom and Seshia, Sanjit and Abbeel, Pieter and Dragan, Anca},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{charakorn2023generating,
  title={Generating diverse cooperative agents by learning incompatible policies},
  author={Charakorn, Rujikorn and Manoonpong, Poramate and Dilokthanakul, Nat},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023}
}

@inproceedings{chen2020aateam,
  title={Aateam: Achieving the ad hoc teamwork by employing the attention mechanism},
  author={Chen, Shuo and Andrejczuk, Ewa and Cao, Zhiguang and Zhang, Jie},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={05},
  pages={7095--7102},
  year={2020}
}

@misc{chen2024dualcurriculumlearningframework,
      title={A Dual Curriculum Learning Framework for Multi-UAV Pursuit-Evasion in Diverse Environments}, 
      author={Jiayu Chen and Guosheng Li and Chao Yu and Xinyi Yang and Botian Xu and Huazhong Yang and Yu Wang},
      year={2024},
      eprint={2312.12255},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2312.12255}, 
}

@article{de_souza_decentralized_2021,
	title = {Decentralized {Multi}-{Agent} {Pursuit} {Using} {Deep} {Reinforcement} {Learning}},
	volume = {6},
	issn = {2377-3766},
	url = {https://ieeexplore.ieee.org/abstract/document/9387125},
	doi = {10.1109/LRA.2021.3068952},
	abstract = {Pursuit-evasion is the problem of capturing mobile targets with one or more pursuers. We use deep reinforcement learning for pursuing an omnidirectional target with multiple, homogeneous agents that are subject to unicycle kinematic constraints. We use shared experience to train a policy for a given number of pursuers, executed independently by each agent at run-time. The training uses curriculum learning, a sweeping-angle ordering to locally represent neighboring agents, and a reward structure that encourages a good formation and combines individual and group rewards. Simulated experiments with a reactive evader and up to eight pursuers show that our learning-based approach outperforms recent reinforcement learning techniques as well as non-holonomic adaptations of classical algorithms. The learned policy is successfully transferred to the real-world in a proof-of-concept demonstration with three motion-constrained pursuer drones.},
	number = {3},
	urldate = {2024-06-06},
	journal = {IEEE Robotics and Automation Letters},
	author = {de Souza, Cristino and Newbury, Rhys and Cosgun, Akansel and Castillo, Pedro and Vidolov, Boris and Kulić, Dana},
	month = jul,
	year = {2021},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {cooperating robots, Drones, Games, Kinematics, Multi-robot systems, reinforcement learning, Reinforcement learning, Task analysis, Training, Trajectory},
	pages = {4552--4559}
}

@article{hola-drone,
  author       = {Yang Li and
                  Dengyu Zhang and
                  Junfan Chen and
                  Ying Wen and
                  Qingrui Zhang and
                  Shaoshuai Mou and
                  Wei Pan},
  title        = {HOLA-Drone: Hypergraphic Open-ended Learning for Zero-Shot Multi-Drone
                  Cooperative Pursuit},
  journal      = {CoRR},
  volume       = {abs/2409.08767},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2409.08767},
  doi          = {10.48550/ARXIV.2409.08767},
  eprinttype    = {arXiv},
  eprint       = {2409.08767},
  timestamp    = {Mon, 14 Oct 2024 08:21:03 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2409-08767.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{hu2020other,
  title={“other-play” for zero-shot coordination},
  author={Hu, Hengyuan and Lerer, Adam and Peysakhovich, Alex and Foerster, Jakob},
  booktitle={International Conference on Machine Learning},
  pages={4399--4410},
  year={2020},
  organization={PMLR}
}

@article{janosov_group_2017,
	title = {Group chasing tactics: how to catch a faster prey?},
	volume = {19},
	issn = {1367-2630},
	shorttitle = {Group chasing tactics},
	url = {http://arxiv.org/abs/1701.00284},
	doi = {10.1088/1367-2630/aa69e7},
	abstract = {We propose a bio-inspired, agent-based approach to describe the natural phenomenon of group chasing in both two and three dimensions. Using a set of local interaction rules we created a continuous-space and discrete-time model with time delay, external noise and limited acceleration. We implemented a unique collective chasing strategy, optimized its parameters and studied its properties when chasing a much faster, erratic escaper. We show that collective chasing strategies can significantly enhance the chasers' success rate. Our realistic approach handles group chasing within closed, soft boundaries - contrasting most of those published in the literature with periodic ones -- and resembles several properties of pursuits observed in nature, such as the emergent encircling or the escaper's zigzag motion.},
	number = {5},
	urldate = {2025-01-25},
	journal = {New Journal of Physics},
	author = {Janosov, Milán and Virágh, Csaba and Vásárhelyi, Gábor and Vicsek, Tamás},
	month = may,
	year = {2017},
	note = {arXiv:1701.00284 [physics]},
	keywords = {/unread, Physics - Biological Physics, Quantitative Biology - Populations and Evolution, Statistics - Applications},
	pages = {053003},
	file = {Janosov et al_2017_Group chasing tactics.pdf:/home/gz/Zotero/storage/GMAXAL2S/Janosov et al_2017_Group chasing tactics.pdf:application/pdf;Snapshot:/home/gz/Zotero/storage/X8QRLPU6/1701.html:text/html},
}

@inproceedings{jianhong2024oaht,
author = {Wang, Jianhong and Li, Yang and Zhang, Yuan and Pan, Wei and Kaski, Samuel},
title = {Open ad hoc teamwork with cooperative game theory},
year = {2024},
publisher = {JMLR.org},
abstract = {Ad hoc teamwork poses a challenging problem, requiring the design of an agent to collaborate with teammates without prior coordination or joint training. Open ad hoc teamwork (OAHT) further complicates this challenge by considering environments with a changing number of teammates, referred to as open teams. One promising solution in practice to this problem is leveraging the generalizability of graph neural networks to handle an unrestricted number of agents with various agent-types, named graph-based policy learning (GPL). However, its joint Q-value representation over a coordination graph lacks convincing explanations. In this paper, we establish a new theory to understand the representation of the joint Q-value for OAHT and its learning paradigm, through the lens of cooperative game theory. Building on our theory, we propose a novel algorithm named CIAO, based on GPL's framework, with additional provable implementation tricks that can facilitate learning. The demos of experimental results are available on https://sites.google.com/view/ciao2024, and the code of experiments is published on https://github.com/hsvgbkhgbv/CIAO.},
booktitle = {Proceedings of the 41st International Conference on Machine Learning},
articleno = {2086},
numpages = {29},
location = {Vienna, Austria},
series = {ICML'24}
}

@article{li2024tackling,
  title={Tackling cooperative incompatibility for zero-shot human-ai coordination},
  author={Li, Yang and Zhang, Shao and Sun, Jichen and Zhang, Wenhao and Du, Yali and Wen, Ying and Wang, Xinbing and Pan, Wei},
  journal={Journal of Artificial Intelligence Research},
  volume={80},
  pages={1139--1185},
  year={2024}
}

@article{li_robust_2019,
	title = {Robust {Multi}-{Agent} {Reinforcement} {Learning} via {Minimax} {Deep} {Deterministic} {Policy} {Gradient}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4327},
	doi = {10.1609/aaai.v33i01.33014213},
	abstract = {Despite the recent advances of deep reinforcement learning (DRL), agents trained by DRL tend to be brittle and sensitive to the training environment, especially in the multi-agent scenarios. In the multi-agent setting, a DRL agent’s policy can easily get stuck in a poor local optima w.r.t. its training partners – the learned policy may be only locally optimal to other agents’ current policies. In this paper, we focus on the problem of training robust DRL agents with continuous actions in the multi-agent learning setting so that the trained agents can still generalize when its opponents’ policies alter. To tackle this problem, we proposed a new algorithm, MiniMax Multi-agent Deep Deterministic Policy Gradient (M3DDPG) with the following contributions: (1) we introduce a minimax extension of the popular multi-agent deep deterministic policy gradient algorithm (MADDPG), for robust policy learning; (2) since the continuous action space leads to computational intractability in our minimax learning objective, we propose Multi-Agent Adversarial Learning (MAAL) to efficiently solve our proposed formulation. We empirically evaluate our M3DDPG algorithm in four mixed cooperative and competitive multi-agent environments and the agents trained by our method significantly outperforms existing baselines.},
	language = {en},
	number = {01},
	urldate = {2024-06-06},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Li, Shihui and Wu, Yi and Cui, Xinyue and Dong, Honghua and Fang, Fei and Russell, Stuart},
	month = jul,
	year = {2019},
	note = {Number: 01},
	pages = {4213--4220},
	file = {Full Text PDF:D\:\\dengyu\\Zotero\\storage\\KG8AAWLK\\Li et al. - 2019 - Robust Multi-Agent Reinforcement Learning via Mini.pdf:application/pdf},
}

@article{papoudakis2021agent,
  title={Agent modelling under partial observability for deep reinforcement learning},
  author={Papoudakis, Georgios and Christianos, Filippos and Albrecht, Stefano},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={19210--19222},
  year={2021}
}

@inproceedings{rahman2021towards,
  title={Towards open ad hoc teamwork using graph-based policy learning},
  author={Rahman, Muhammad A and Hopner, Niklas and Christianos, Filippos and Albrecht, Stefano V},
  booktitle={International conference on machine learning},
  pages={8776--8786},
  year={2021},
  organization={PMLR}
}

@inproceedings{shah_multi-agent_2019,
	address = {Cham},
	title = {Multi-agent {Cooperative} {Pursuit}-{Evasion} {Strategies} {Under} {Uncertainty}},
	isbn = {978-3-030-05816-6},
	doi = {10.1007/978-3-030-05816-6_32},
	abstract = {We present a method for a collaborative team of pursuing robots to contain and capture a single evading robot. The main challenge is that the pursuers do not know the position of the evader exactly nor do they know the policy of the evader. Instead, the pursuers maintain an estimate of the evader’s position over time from noisy online measurements. We propose a policy by which the pursuers move to maximally reduce the area of space reachable by the evader given the uncertainty in the evader’s position estimate. The policy is distributed in the sense that each pursuer only needs to know the positions of its closest neighbors. The policy guarantees that the evader’s reachable area is non-increasing between measurement updates regardless of the evader’s policy. Furthermore, we show in simulations that the pursuers capture the evader despite the position uncertainty provided that the pursuer’s measurement noise decreases with the distance to the evade.},
	language = {en},
	booktitle = {Distributed {Autonomous} {Robotic} {Systems}},
	publisher = {Springer International Publishing},
	author = {Shah, Kunal and Schwager, Mac},
	editor = {Correll, Nikolaus and Schwager, Mac and Otte, Michael},
	year = {2019},
	keywords = {Game theoretic control, Multi-agent pursuit-evasion, Reachability methods},
	pages = {451--468},
	file = {Full Text PDF:D\:\\dengyu\\Zotero\\storage\\CGZV2DIE\\Shah and Schwager - 2019 - Multi-agent Cooperative Pursuit-Evasion Strategies.pdf:application/pdf},
}

@article{wang2024n,
  title={N-Agent Ad Hoc Teamwork},
  author={Wang, Caroline and Rahman, Arrasy and Durugkar, Ishan and Liebman, Elad and Stone, Peter},
  journal={arXiv preprint arXiv:2404.10740},
  year={2024}
}

@inproceedings{wang2024zsc,
  title={Zsc-eval: An evaluation toolkit and benchmark for multi-agent zero-shot coordination},
  author={Wang, Xihuai and Zhang, Shao and Zhang, Wenhao and Dong, Wentao and Chen, Jingxiao and Wen, Ying and Zhang, Weinan},
  booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  year={2024}
}

@article{zhang2024multi,
  title={Multi-UAV Cooperative Pursuit of a Fast-Moving Target UAV Based on the GM-TD3 Algorithm},
  author={Zhang, Yaozhong and Ding, Meiyan and Yuan, Yao and Zhang, Jiandong and Yang, Qiming and Shi, Guoqing and Jiang, Frank and Lu, Meiqu},
  journal={Drones},
  volume={8},
  number={10},
  pages={557},
  year={2024},
  publisher={MDPI}
}

@article{zhou_cooperative_2016,
	title = {Cooperative pursuit with {Voronoi} partitions},
	volume = {72},
	issn = {0005-1098},
	url = {https://www.sciencedirect.com/science/article/pii/S0005109816301911},
	doi = {10.1016/j.automatica.2016.05.007},
	abstract = {This work considers a pursuit–evasion game in which a number of pursuers are attempting to capture a single evader. Cooperation among multiple agents can be difficult to achieve, as it may require the selection of actions in the joint input space of all agents. This work presents a decentralized, real-time algorithm for cooperative pursuit of a single evader by multiple pursuers in bounded, simply-connected planar domains. The algorithm is based on minimizing the area of the generalized Voronoi partition of the evader. The pursuers share state information but compute their inputs independently. No assumptions are made about the evader’s control strategies other than requiring the evader control inputs to conform to a speed limit. Proof of guaranteed capture is shown when the domain is convex and the players’ motion models are kinematic. Simulation results are presented showing the efficiency and effectiveness of this strategy.},
	urldate = {2024-06-06},
	journal = {Automatica},
	author = {Zhou, Zhengyuan and Zhang, Wei and Ding, Jerry and Huang, Haomiao and Stipanović, Dušan M. and Tomlin, Claire J.},
	month = oct,
	year = {2016},
	keywords = {Cooperative pursuit, Pursuit–evasion games, Voronoi},
	pages = {64--72},
	file = {ScienceDirect Snapshot:D\:\\dengyu\\Zotero\\storage\\JFPX434F\\S0005109816301911.html:text/html},
}

