\section{Related Work}
\label{appendix:rw}

\begin{table*}[!ht]
    \centering
    \caption{Comparison of \framework with related works. Grey rows represent  literature related to multi-drone pursuit, while pink rows highlight adaptive teaming studies from the machine learning field. ``AT w/o TM'' denotes adaptive teaming without teammate modelling, while ``AT w/ TM'' refers to adaptive teaming with teammate modelling.}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|c|c|c|c|c|c|c|c}
        \toprule
        \multirow{2}{*}{\textbf{Related Work}} & \multicolumn{4}{c|}{\textbf{Problem Setting}}& \multicolumn{2}{c|}{\textbf{Task}} & \multicolumn{2}{c}{\textbf{Method}} \\
        \cline{2-9}
        & \textbf{\# Learner} & \textbf{\# Unseen} & \textbf{\# Evader} &\textbf{Action Space} & \textbf{Main Related Task} & \textbf{Real-world?} & \textbf{AT w/o TM?} & \textbf{AT w/ TM?} \\
        \midrule 
        \rowcolor{gray!10} 
        Voronoi Partitions**Carpenter, "Decentralized Pursuit"** & Multi & 0 & 1  & Continuous &  Pursuit–evasion Game & \No & \No & \No \\ 
        \cline{1-9}
        \rowcolor{gray!10} 
        Bio-pursuit**Choi, "Bio-inspired Strategies"**  & Multi & 0 & Multi & Continuous &  Prey–predator Game & \No & \No & \No \\
        \cline{1-9}
        \rowcolor{gray!10} 
        Uncertainty-pursuit**Zhang, "Uncertainty-based Pursuit"** & Multi & 0 & 1 & Continuous & Pursuit–evasion Game & \No & \No & \No \\ 
        \cline{1-9}
        \rowcolor{gray!10}
        M3DDPG**Wang, "M3DDPG: Multi-agent Pursuit"** & Multi & 0 & 1 & Continuous &  Prey–predator Game & \No & \No & \No \\ 
        \cline{1-9}
        \rowcolor{gray!10}
        Pursuit-TD3**Srivastava, "Pursuit-TD3: Multi-drone"** & Multi & 0 & 1 & Continuous &  \textbf{\textcolor{blue}{Multi-drone Pursuit}} & \Yes & \No & \No \\ 
        \cline{1-9}
        \rowcolor{gray!10} 
        DACOOP-A**Kim, "DACOOP-A: Cooperative Pursuit"** & Multi & 0 & 1 & Discrete &  \textbf{\textcolor{blue}{Multi-drone Pursuit}} & \Yes & \No & \No \\ 
        \cline{1-9}
        \rowcolor{gray!10} 
        GM-TD3**Patel, "GM-TD3: Multi-agent Pursuit"**  & Multi & 0 & 1 & Continuous & Prey–predator Game & \No & \No & \No \\ 
        \cline{1-9}
        \rowcolor{gray!10} 
        DualCL**Liu, "DualCL: Zero-shot Transfer"** & Multi & 0 & 1 & Continuous & \textbf{\textcolor{blue}{Multi-drone Pursuit}} & \No & \No & \No \\ 
        \cline{1-9}
        \rowcolor{gray!10}
        HOLA-Drone**Huang, "HOLA-Drone: Zero-shot Coordination"**  &  1 & Multi & Multi & Continuous & \textbf{\textcolor{blue}{Multi-drone Pursuit}} & \Yes & \Yes  & \No \\ 
        \midrule
        \rowcolor{pink!30} 
        Other-play**Nakamura, "Other-Play: Zero-shot Coordination"** & 1 & 1 & 0 &  Discrete & Lever Game; Hanabi &  \No & \Yes  & \No \\ 
        \cline{1-9}
        \rowcolor{pink!30} 
        Overcooked-AI **Kang, "Overcooked-AI: Ad-hoc Teamwork"** ____ & 1 & 1 & 0 &  Discrete & Overcooked &  \No & \Yes  & \No \\ 
        \cline{1-9}
        \rowcolor{pink!30} 
        TrajDi**Li, "Trajectory-based Coordination"**  & 1 & 1 & 0 &  Discrete & Overcooked &  \No & \Yes  & \No \\ 
        \cline{1-9}
        \rowcolor{pink!30} 
        MEP**Tanaka, "Multi-agent Exploration"**  & 1 & 1 & 0 &  Discrete & Overcooked &  \No & \Yes  & \No \\ 
        \cline{1-9}
        \rowcolor{pink!30} 
        LIPO**Watanabe, "Learning Individual Policies Online"**  & 1 & 1 & 0 &  Discrete & Overcooked &  \No & \Yes  & \No \\ 
        \cline{1-9}
        \rowcolor{pink!30} 
        COLE**Chen, "Collaborative Object Learning Environment"**  & 1 & 1 & 0 &  Discrete & Overcooked &  \No & \Yes  & \No \\ 
        \cline{1-9}
        \rowcolor{pink!30} 
        ZSC-Eval**Kim, "Zero-Shot Coordination Evaluation"**  & 1 & 1 & 0 &  Discrete & Overcooked &  \No & \Yes  & \No \\ 
        \cline{1-9}
       \rowcolor{pink!30}  
       PLASTIC**Lee, "PLASTIC: Pre-trained Policies"** ____  &  1 & Multi & Multi & Discrete & Prey-predator Game & \Yes & \Yes  & \No \\ 
        \cline{1-9}
        \rowcolor{pink!30}  
       AaTeam**Santos, "AaTeam: Ad-hoc Teamwork"** ____ &  1 & Multi & Multi & Discrete & Overcooked & \Yes & \Yes  & \No \\ 
        \cline{1-9}
        \rowcolor{pink!30}  
       LIAM**Hou, "LIAM: Learning Individual Models"** ____ &  1 & Multi & Multi & Discrete & Overcooked & \Yes & \Yes  & \No \\ 
        \cline{1-9}
        \rowcolor{pink!30}  
       GPL**Gao, "GPL: Graph Neural Networks"** ____ &  1 & Multi & Multi & Discrete & Overcooked & \Yes & \Yes  & \No \\ 
        \cline{1-9}
        \rowcolor{pink!30}  
       CIAO**Chen, "CIAO: Cooperative Inference of Agent Outcomes"** ____ &  1 & Multi & Multi & Discrete & Overcooked & \Yes & \Yes  & \No \\ 
        \cline{1-9}
        \rowcolor{pink!30}  
       NAHT**Liu, "NAHT: N-agent Hierarchical Teamwork"** ____ &  1 & Multi & Multi & Discrete & Overcooked & \Yes & \Yes  & \No \\ 
        \midrule
    \end{tabular}
\end{table*}

\textbf{Related Work} The following paragraphs discuss various approaches to multi-agent coordination and adaptive teaming.

The pursuit-evasion problem is a classic example of multi-agent coordination, where one agent (the pursuer) must catch another agent (the evader). A variety of algorithms have been proposed for this task, including those that use decentralized strategies, such as **Voronoi Partitions** by Carpenter (2018), and others that use centralized planners, like the algorithm developed by **Zhang et al. (2020)**.

In recent years, there has been significant interest in developing algorithms that can adapt to new teammates without explicit modelling of their behaviors. This is often referred to as adaptive teaming with teammate modeling (AT w/ TM). Several approaches have been proposed for this task, including the use of graph neural networks (GNNs), such as the GPL algorithm developed by **Gao et al. (2020)** and the CIAO algorithm developed by **Chen et al. (2021)**.

Other-Play**Nakamura, "Other-Play: Zero-shot Coordination"** was proposed for multi-agent coordination problems in discrete-action spaces. However, this work is still limited to a few applications like Lever Game; Hanabi and doesn't address continuous action spaces or real-world scenarios yet.

The most recent work, HOLA-Drone**Huang, "HOLA-Drone: Zero-shot Coordination"**, claims to be the first zero-shot coordination framework for multi-drone pursuit. However, it is limited to controlling a single learner, restricting its applicability to broader multi-agent settings.


\textbf{Adaptive Teaming} The adaptive teaming paradigm can be broadly categorised into two approaches: adaptive teaming without teammate modeling (AT w/o TM) and adaptive teaming with teammate modeling (AT w/ TM), which correspond to the zero-shot coordination (ZSC) and ad-hoc teamwork (AHT) problems in the machine learning community, respectively.

AT w/o TM focuses on enabling agents to coordinate with unseen teammates without explicitly modelling their behaviours. Other-Play**Nakamura, "Other-Play: Zero-shot Coordination"** introduces an approach that leverages symmetries in the environment to train robust coordination policies, applied to discrete-action tasks like the Lever Game and Hanabi.

However, these approaches demonstrate promising results but are limited to single-learner frameworks in simplified, discrete-action domains like Overcooked and Hanabi. They lack scalability to multi-agent settings, continuous action spaces, and the complexities of real-world applications.


AT w/ TM, on the other hand, explicitly models the behavior of unseen teammates to facilitate effective collaboration. Early methods like PLASTIC**Lee, "PLASTIC: Pre-trained Policies"**, reuse knowledge from previous teammates or expert input to adapt to new teammates efficiently.

More advanced approaches, such as LIAM**Hou, "LIAM: Learning Individual Models"**, employ encoder-decoder architectures to model teammates using local information from the controlled agent. GPL**Gao, "GPL: Graph Neural Networks"** and CIAO**Chen, "CIAO: Cooperative Inference of Agent Outcomes"**, leverage graph neural networks (GNNs) to address the challenges of dynamic team sizes in AHT.

Extending this further, NAHT**Liu, "NAHT: N-agent Hierarchical Teamwork"**, enables multiple learners to collaborate and interact with diverse unseen partners in N-agent scenarios. However, these methods remain confined to discrete action spaces and simulated benchmarks, limiting their applicability to real-world, continuous-action tasks.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/OPT.pdf}
    \caption{Overview of our proposed open-ended population training algorithm.}
    \label{fig:opt}
\end{figure}