

\section{Related Work}
\label{appendix:rw}

\begin{table*}[!ht]
    \centering
    \caption{Comparison of \framework with related works. Grey rows represent  literature related to multi-drone pursuit, while pink rows highlight adaptive teaming studies from the machine learning field. ``AT w/o TM'' denotes adaptive teaming without teammate modelling, while ``AT w/ TM'' refers to adaptive teaming with teammate modelling.}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|c|c|c|c|c|c|c|c}
        \toprule
        \multirow{2}{*}{\textbf{Related Work}} & \multicolumn{4}{c|}{\textbf{Problem Setting}}& \multicolumn{2}{c|}{\textbf{Task}} & \multicolumn{2}{c}{\textbf{Method}} \\
        \cline{2-9}
        & \textbf{\# Learner} & \textbf{\# Unseen} & \textbf{\# Evader} &\textbf{Action Space} & \textbf{Main Related Task} & \textbf{Real-world?} & \textbf{AT w/o TM?} & \textbf{AT w/ TM?} \\
        \midrule 
        \rowcolor{gray!10} 
        Voronoi Partitions~\cite{zhou_cooperative_2016} & Multi & 0 & 1  & Continuous &  Pursuit–evasion Game & \No & \No & \No \\ 
        \cline{1-9}
        \rowcolor{gray!10} 
        Bio-pursuit~\cite{janosov_group_2017}  & Multi & 0 & Multi & Continuous &  Prey–predator Game & \No & \No & \No \\
        \cline{1-9}
        \rowcolor{gray!10} 
        Uncertainty-pursuit~\cite{shah_multi-agent_2019} & Multi & 0 & 1 & Continuous & Pursuit–evasion Game & \No & \No & \No \\ 
        \cline{1-9}
        \rowcolor{gray!10}
        M3DDPG~\cite{li_robust_2019} & Multi & 0 & 1 & Continuous &  Prey–predator Game & \No & \No & \No \\ 
        \cline{1-9}
        \rowcolor{gray!10}
        Pursuit-TD3\cite{de_souza_decentralized_2021} & Multi & 0 & 1 & Continuous &  \textbf{\textcolor{blue}{Multi-drone Pursuit}} & \Yes & \No & \No \\ 
        \cline{1-9}
        \rowcolor{gray!10} 
        DACOOP-A\cite{ZhangDACOOP2023} & Multi & 0 & 1 & Discrete &  \textbf{\textcolor{blue}{Multi-drone Pursuit}} & \Yes & \No & \No \\ 
        \cline{1-9}
        \rowcolor{gray!10} 
        GM-TD3~\cite{zhang2024multi}  & Multi & 0 & 1 & Continuous & Prey–predator Game & \No & \No & \No \\ 
        \cline{1-9}
        \rowcolor{gray!10} 
        DualCL~\cite{chen2024dualcurriculumlearningframework} & Multi & 0 & 1 & Continuous & \textbf{\textcolor{blue}{Multi-drone Pursuit}} & \No & \No & \No \\ 
        \cline{1-9}
        \rowcolor{gray!10}
        HOLA-Drone~\cite{hola-drone}  &  1 & Multi & Multi & Continuous & \textbf{\textcolor{blue}{Multi-drone Pursuit}} & \Yes & \Yes  & \No \\ 
        \midrule
        \rowcolor{pink!30} 
        Other-play~\cite{hu2020other} & 1 & 1 & 0 &  Discrete & Lever Game; Hanabi &  \No & \Yes  & \No \\ 
        \cline{1-9}
        \rowcolor{pink!30} 
        Overcooked-AI ~\cite{carroll2019utility} & 1 & 1 & 0 &  Discrete & Overcooked &  \No & \Yes  & \No \\ 
        \cline{1-9}
        \rowcolor{pink!30} 
        TrajDi~\cite{TrajDi}  & 1 & 1 & 0 &  Discrete & Overcooked &  \No & \Yes  & \No \\ 
        \cline{1-9}
        \rowcolor{pink!30} 
        MEP~\cite{MEP}  & 1 & 1 & 0 &  Discrete & Overcooked &  \No & \Yes  & \No \\ 
        \cline{1-9}
        \rowcolor{pink!30} 
        LIPO~\cite{charakorn2023generating}  & 1 & 1 & 0 &  Discrete & Overcooked &  \No & \Yes  & \No \\ 
        \cline{1-9}
        \rowcolor{pink!30} 
        COLE~\cite{li2024tackling}  & 1 & 1 & 0 &  Discrete & Overcooked &  \No & \Yes  & \No \\ 
        \cline{1-9}
        \rowcolor{pink!30} 
        ZSC-Eval~\cite{wang2024zsc}  & 1 & 1 & 0 &  Discrete & Overcooked &  \No & \Yes  & \No \\ 
        \cline{1-9}
       \rowcolor{pink!30}  
       PLASTIC~\cite{barrett2017making}  &  1 & Multi & Multi & Discrete & Prey-predator Game &  \No & \No & \Yes \\ 
       \cline{1-9} 
        \rowcolor{pink!30} 
        AATeam~\cite{chen2020aateam}  & 1 & 1 & 2 &  Discrete & Half Field Offense &  \No & \No & \Yes  \\  
        \cline{1-9} 
        \rowcolor{pink!30} 
        LIAM~\cite{papoudakis2021agent}  &  1 & Multi & %\rowcolor{pink!30} 
        Multi & Discrete & LBF; Prey-predator Game  &  \No & \No & \Yes   \\ 
        \cline{1-9} 
       \rowcolor{pink!30}  
       GPL~\cite{rahman2021towards}  &  1 & Multi & Multi & Discrete & LBF; Wolfpack; FortAttack &  \No & \No & \Yes   \\ 
       \cline{1-9} 
       \rowcolor{pink!30}   
       CIAO~\cite{jianhong2024oaht} &  1 & Multi & Multi & Discrete &LBF; Wolfpack &  \No & \No & \Yes \\ 
       \cline{1-9} 
       \rowcolor{pink!30}   
       NAHT~\cite{wang2024n}  & Multi & Multi & Multi & Discrete & StarCraft2 &  \No & \No & \Yes \\
         \midrule
         \rowcolor{orange!30} 
         Our ATMDP  & Multi & Multi & Multi & Continuous & \textbf{\textcolor{blue}{Multi-drone Pursuit}} &  \Yes & \Yes & \Yes \\
         \bottomrule
    \end{tabular}
    }
    \label{appendix:tab_rw}
\end{table*}

In this work, we provide a comprehensive review of related research on multi-drone pursuit and adaptive teaming in machine learning, with a detailed comparison presented in Table~\ref{tab:review}.

\textbf{Multi-agent pursuit-evasion. } 
Multi-agent pursuit-evasion is closely related to the multi-drone pursuit task. 
Most existing methods rely on pre-coordinated strategies specifically designed for particular pursuit-evasion scenarios.
Traditional approaches often rely on heuristic~\cite{janosov_group_2017} or optimisation-based strategies~\cite{zhou_cooperative_2016,shah_multi-agent_2019}. For example, \citet{janosov_group_2017} proposes a bio-inspired model that uses local interaction rules to enhance group chasing success in Prey–Predator Games. Similarly, the Voronoi partitions algorithm~\cite{zhou_cooperative_2016} and the uncertainty-pursuit algorithm~\cite{shah_multi-agent_2019} employ decentralised frameworks to optimise the evader’s Voronoi partition and reachable area, respectively.
In recent years, deep reinforcement learning (DRL) has been widely adopted for pre-coordinated multi-drone pursuit tasks. M3DDPG~\cite{li_robust_2019} and GM-TD3~\cite{zhang2024multi} extend standard DRL algorithms, such as TD3~\cite{TD3} and DDPG~\cite{DDPG}, specifically for multi-agent pursuit in simulated environments. Pursuit-TD3~\cite{de_souza_decentralized_2021} applies the TD3 algorithm to pursue a target with multiple homogeneous agents, validated through both simulations and real-world drone demonstrations. \citet{ZhangDACOOP2023} introduces DACOOP-A, a cooperative pursuit algorithm that enhances reinforcement learning with artificial potential fields and attention mechanisms, validated in real-world drone systems. DualCL~\cite{chen2024dualcurriculumlearningframework} addresses multi-UAV pursuit-evasion in diverse environments and demonstrates zero-shot transfer capabilities to unseen scenarios, though only in simulation.
The most recent work, HOLA-Drone~\cite{hola-drone}, claims to be the first zero-shot coordination framework for multi-drone pursuit. However, it is limited to controlling a single learner, restricting its applicability to broader multi-agent settings. 


\textbf{Adaptive Teaming. }The adaptive teaming paradigm can be broadly categorised into two approaches: adaptive teaming without teammate modelling (AT w/o TM) and adaptive teaming with teammate modelling (AT w/ TM), which correspond to the zero-shot coordination (ZSC) and ad-hoc teamwork (AHT) problems in the machine learning community, respectively.
AT w/o TM focuses on enabling agents to coordinate with unseen teammates without explicitly modelling their behaviours. Other-Play~\cite{hu2020other} introduces an approach that leverages symmetries in the environment to train robust coordination policies, applied to discrete-action tasks like the Lever Game and Hanabi. Similarly, methods such as Overcooked-AI~\cite{carroll2019utility}, TrajDi~\cite{TrajDi}, MEP~\cite{MEP}, LIPO~\cite{charakorn2023generating}, and ZSC-Eval~\cite{wang2024zsc} study collaborative behaviours in Overcooked, where agents learn generalisable coordination strategies with diverse unseen partners. While these approaches demonstrate promising results, they are limited to single-learner frameworks in simplified, discrete-action domains like Overcooked and Hanabi. They lack scalability to multi-agent settings, continuous action spaces, and the complexities of real-world applications.

AT w/ TM, on the other hand, explicitly models the behaviour of unseen teammates to facilitate effective collaboration. Early methods like PLASTIC~\cite{barrett2017making} reuse knowledge from previous teammates or expert input to adapt to new teammates efficiently. AaTeam~\cite{chen2020aateam} introduces attention-based neural networks to dynamically process and respond to teammates’ behaviours in real-time.
More advanced approaches, such as LIAM~\cite{papoudakis2021agent}, employ encoder-decoder architectures to model teammates using local information from the controlled agent. GPL~\cite{rahman2021towards} and CIAO~\cite{jianhong2024oaht} leverage graph neural networks (GNNs) to address the challenges of dynamic team sizes in AHT. Extending this further, NAHT~\cite{wang2024n} enables multiple learners to collaborate and interact with diverse unseen partners in N-agent scenarios.
Despite their progress, these methods remain confined to discrete action spaces and simulated benchmarks, limiting their applicability to real-world, continuous-action tasks. 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/OPT.pdf}
    \caption{Overview of our proposed open-ended population training algorithm.}
    \label{fig:opt}
\end{figure}

\section{Open-ended Population Training Algorithm}
\label{appendix:opt}

In this section, we define a population of drone strategies, denoted as $\Pi = \{\pi_1, \pi_2, \cdots, \pi_n\}$. For the task involving $C$ teammates, the interactions within the population $\Pi$ are modeled as a hypergraph $\mathcal{G}$. Formally, the hypergraph is represented by the tuple $(\Pi, \mathcal{E}, \mathbf{w})$, where the node set $\Pi$ represents the strategies, $\mathcal{E}$ is the hyperedge set capturing interaction relationships among teammates, and $\mathbf{w}$ is the weight set representing the corresponding average outcomes. 
The left subfigure of Fig.~\ref{fig:model} illustrates an example of a hypergraph representation with five nodes and a fixed hyperedge length of 4.

Building on the concept of preference hypergraphs~\cite{hola-drone}, we use the preference hypergraph to represent the population and assess the coordination ability of each node. The \textbf{preference hypergraph} $\mathcal{P}\mathcal{G}$ is derived from the hypergraph $\mathcal{G}$, where each node has a direct outgoing hyperedge pointing to the teammates with whom it achieves the highest weight in $\mathcal{G}$. 
Formally, $\mathcal{P}\mathcal{G}$ is defined by the tuple $(\Pi, \mathcal{E}_\mathcal{P})$, where the node set $\Pi$ represents the strategies, and $\mathcal{E}_\mathcal{P}$ denotes the set of outgoing hyperedges. As shown in the right subfigure of Fig.~\ref{fig:model}, the dotted line highlights the outgoing edge. 
For instance, node $2$ has a single outgoing edge $(2, 3, 5, 4)$ because it achieves the highest outcome, i.e., a weight of 45, with those teammates in $\mathcal{G}$, as depicted in the left subfigure.

Intuitively, a node in $\mathcal{P}\mathcal{G}$ with higher cooperative ability will have more incoming hyperedges, as other agents prefer collaborating with it to achieve the highest outcomes. Therefore, we extend the concept of \textbf{preference centrality}~\cite{li2023cooperative} to quantify the cooperative ability of each node. Specifically, for any node $i \in \Pi$, the preference centrality is defined as 
\begin{equation}
    \eta_\Pi(i) = \frac{d_{\mathcal{P}\mathcal{G}}(i)}{d_{\mathcal{G}}(i)},
\end{equation}
where $d_{\mathcal{P}\mathcal{G}}(i)$ denotes the incoming degree of node $i$ in $\mathcal{P}\mathcal{G}$, and $d_{\mathcal{G}}(i)$ represents the degree of node $i$ in $\mathcal{G}$.

\textbf{Max-Min Preference Oracle. }
Building on the basic definition of the preference hypergraph representation, we introduce the concept of Preference Optimality to describe the goal of our training process.



\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/hypergraph.pdf}
    \caption{An example of a hypergraph representation (left) and its corresponding preference hypergraph (right) with five strategies in the population.}
    \vspace{-3mm}
    \label{fig:model}
\end{figure}

\begin{definition}[Preference Optimal]
    A set of learners \(\mathcal{N}^\star\) of size \(N\) is said to be \textbf{Preference Optimal (PO)} in a hypergraph \(\mathcal{G} = (\Pi, \mathcal{E}, \mathbf{w})\) if, for any set \(\hat{\mathcal{N}} \subseteq \Pi\) of size \(N\), the following condition holds:
    \begin{equation}
        \sum_{s \in \mathcal{N}^\star} \eta_\Pi(s) \geq \sum_{s \in \hat{\mathcal{N}}} \eta_\Pi(s),
    \end{equation}
    where \(\eta_\Pi(s)\) denotes the preference centrality of learner \(s\) in the hypergraph \(\mathcal{G}\).
\end{definition}

While achieving a preference-optimal oracle is desirable, it becomes impractical or prohibitively expensive in large, diverse populations. Therefore, we propose the \textbf{max-min preference oracle}, abbreviated as \textit{oracle} in the rest of this paper, to ensure robust adaptability and maximize cooperative performance under the worst-case teammate scenarios.

To formalize the objective, we split the strategy population \(\Pi\) into a learner set \(\mathcal{N}\) and a non-learner set \(\Pi_{-\mathcal{N}}\), where \(\Pi_{-\mathcal{N}} \cap \mathcal{N} = \emptyset\) and \(\Pi_{-\mathcal{N}} \cup \mathcal{N} = \Pi\). The objective function \(\phi\) is defined as:
\begin{equation}
    \phi : \underbrace{\mathcal{N} \times \cdots \times \mathcal{N}}_{N \text{ learners}} \times \underbrace{\Pi_{-\mathcal{N}} \times \cdots \times \Pi_{-\mathcal{N}}}_{M \text{ teammates}} \rightarrow \mathbb{R}.
\end{equation}

The max-min preference oracle updates the learner set by solving:
\begin{equation}
\begin{aligned}
    \mathcal{N}^\prime &= oracle(\mathcal{N}, \phi_{\mathcal{M}}(\cdot)) := \arg\max_{\mathcal{N}} \min_{\mathcal{M} \subseteq \Pi_{-\mathcal{N}}} \phi_\mathcal{M}(\mathcal{N}),
\end{aligned}
\end{equation}
where the objective \(\phi_{\mathcal{M}}(\cdot)\) is derived using the extended curry operator~\cite{balduzzi2019open}, originally designed for two-player games, and is expressed as:
\begin{equation}
\begin{aligned}
    &\left[\underbrace{\mathcal{N} \times \cdots \times \mathcal{N}}_{N \text{ learners}} \times \underbrace{\Pi_{-\mathcal{N}} \times \cdots \times \Pi_{-\mathcal{N}}}_{M \text{ teammates}} \rightarrow \mathbb{R} \right] \\
    \rightarrow &\left[
    \underbrace{\Pi_{-\mathcal{N}} \times \cdots \times \Pi_{-\mathcal{N}}}_{M \text{ teammates}} \rightarrow
    \left[
    \underbrace{\mathcal{N} \times \cdots \times \mathcal{N}}_{N \text{ learners}} \rightarrow \mathbb{R}
    \right]
    \right].
\end{aligned}
\end{equation}

Intuitively, \textit{oracle} alternates between two key steps: the minimization step and the maximization step. In the minimization step, the objective is to identify the subset of teammates \( \mathcal{M}^* \subset \Pi_{-\gN} \) that minimizes the performance outcome of the current learner set \( \mathcal{N} \), i.e. the worst partners. This is formulated as:  
\[
\mathcal{M}^* = \arg\min_{\mathcal{M} \subset \Pi_{-\gN}} \phi_\mathcal{M}(\mathcal{N}).
\]  
In the maximization step, the learner set \( \mathcal{N} \) is updated to maximize its performance outcome against the identified subset \( \mathcal{M}^* \). This is defined as:  
\[
\mathcal{N}^* = \arg\max_{\mathcal{N}} \phi(\mathcal{N}, \mathcal{M}^*).
\]  

\textbf{Open-Ended Population Training algorithm. }
To achieve robust adaptability and dynamic coordination in multi-agent systems, we integrate the max-min preference oracle into an open-ended learning framework, referred to as the \textit{Open-ended Population Training} (OPT) algorithm. 
The OPT algorithm dynamically adjusts the training objective as the population evolves, enabling continuous improvement and effective coordination with unseen partners.
Unlike conventional fixed-objective training, the OPT approach iteratively expands the strategy population \(\Pi\) and refines the learner set \(\mathcal{N}\). At each generation $t$, the framework recalibrates the training objective \(\phi\) based on new extended population $\Pi_t$ to account for the evolving interactions among agents within the population. 

As shown in Fig.~\ref{fig:model}, the Open-Ended Population Training (OPT) algorithm consists of two key modules: the min-step solver and the max-step trainer. At each generation \(t\), the updated learner set \(\mathcal{N}_{t}\) from the previous generation \(t-1\) is incorporated into the population \(\Pi_{t-1}\), resulting in an expanded population \(\Pi_{t}\).

\textbf{Min-step Solver. }The role of the min-step solver is to first construct the preference hypergraph representation of the interactions within the updated population \(\Pi_{t}\). Here, we only need to build a subgraph of the entire preference hypergraph in \(\Pi_{t}\), denoted as \(\mathcal{P}\mathcal{G}_t^\prime\). To obtain \(\mathcal{P}\mathcal{G}_t^\prime\), we focus on constructing the hyperedges in the hypergraph \(\mathcal{G}_t^\prime\) that connect to the learner set \(\mathcal{N}_t\).
For instance, if \(\Pi_{t}\) consists of a learner set \(\mathcal{N}_{t}\) of size \(N\) and a non-learner set \(\Pi_{-\mathcal{N}_{t}}\), any hyperedge \(e\) in \(\mathcal{G}_t^\prime\) connects \(N\) nodes from \(\mathcal{N}_{t}\) and all possible \(M\) nodes from \(\Pi_{-\mathcal{N}_{t}}\). The preference hypergraph \(\mathcal{P}\mathcal{G}_t^\prime\) is then derived from \(\mathcal{G}_t^\prime\) by retaining only the outgoing hyperedge with the highest weight for each node.

The min-step solver uses the reciprocal of the preference centrality in \(\mathcal{P}\mathcal{G}_t^\prime\) to evaluate the worst-case partners. To enhance robustness, the solver does not deterministically select the worst-case partners as \(\mathcal{M}^* = \arg\min_{\mathcal{M} \subset \Pi_{-\mathcal{N}}} \phi_\mathcal{M}(\mathcal{N})\). Instead, it outputs a mixed strategy \(\rho_t\), defined as:  
\begin{equation}
    \rho_t = \arg\min_{P(\Pi_{-\mathcal{N}_t})} \mathbb{E}_{\mathcal{M} \sim P}[\phi_\mathcal{M}(\mathcal{N}_t)].\label{eq:final_min}
\end{equation}
In practice, the mixed strategy \(\rho_t\) is obtained by normalizing the reciprocal of the preference centrality, assigning higher probabilities to worse partners.



\textbf{Max-Step Solver. }
Given the mixed strategies \(\rho_t\), the max-step solver iteratively samples the worst-case partners, referred to as the profile, \(\mathcal{M} \sim \rho_t\), from the non-learner set \(\Pi_{-\mathcal{N}_t}\). It simulates interactions between the sampled profile and the learners to generate training data, with the objective of maximizing the reward \(\phi(\mathcal{N}_t, \mathcal{M}) = \mathbb{E}_{\mathcal{N}_t, \mathcal{M}}[R(\tau)]\), as shown in Eq.~\ref{eq:obj}.
The max-step oracle could be rewritten as 
\begin{equation}
    \mathcal{N}_{t+1} = \arg\max_{\mathcal{N}} \mathbb{E}_{\mathcal{M}^\star \sim \rho_t} \phi(\mathcal{N}, \mathcal{M}^*).
    \label{eq:final_max}
\end{equation}

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/env.pdf}
    \caption{Illustration of four multi-drone pursuit environments in real world. The environments vary in the number of pursuers (p), evaders (e), and obstacles (o), denoted as \texttt{4p2e3o}, \texttt{4p2e1o}, \texttt{4p2e5o}, and \texttt{4p3e5o}. 
    Each setup introduces different levels of complexity, testing the adaptability and coordination capabilities of the agents.}
    \label{fig:app_env}
\end{figure}

The strategy network for adaptive teaming, supports both AHT and ZSC paradigms. In the AHT paradigm, the network uses a teammate modeling network (\(f\)) to infer teammate types from the observation history (\(\tau_t^{i}\)). These predicted vector, combined with the agent’s observation history (\(\tau_t^i\)), are input into a PPO-based policy network. This policy network includes an Actor Network (\(\pi_\theta\)) for generating the agent’s action (\(a_t^i\)) and a Critic Network (\(V_\pi\)) for evaluating the policy.

In contrast, the ZSC paradigm simplifies the process by directly feeding the agent’s observation history (\(\tau_t^i\)) into the actor and critic networks, bypassing explicit teammate modeling. This approach enables the agent to coordinate with unseen teammates without prior knowledge or additional inference mechanisms.

The max-step solver ultimately generates an approximate best response $\gN_{t+1}$ to the worst-case partners, enhancing the agent’s adaptive coordination capabilities.

\section{Unseen Drone Zoo}
\label{appendix:unseen_zoo}
\textit{Rule-Based Method: Greedy Drone. }  
The Greedy Drone pursues the closest target by continuously aligning its movement with the target’s position. Its state information includes its own position, orientation, distances and angles to teammates and evaders, and proximity to obstacles or walls. When obstacles or other agents enter its evasion range, the Greedy Drone dynamically adjusts its direction to avoid collisions, prioritising immediate objectives over team coordination.

\textit{Traditional Method: VICSEK Drone. }  
Based on the commonly used VICSEK algorithm~\cite{Janosov2017Group,ZhangDACOOP2023,hola-drone}, the VICSEK Drone adopts a bio-inspired approach to mimic swarm-like behaviours. It computes and updates a velocity vector directed towards the evader, optimising the tracking path based on the agent’s current environmental state. To avoid nearby obstacles or agents, the VICSEK Drone applies repulsive forces with varying magnitudes. While the calculated velocity vector includes both magnitude and orientation, only the orientation is implemented in our experiments, making it a scalable and practical teammate model for multi-drone coordination.

\textit{Learning-Based Method: Self-Play Drones.}  
For the learning-based approach, we employ an IPPO-based self-play algorithm, generating diverse drone behaviours by training agents with different random seeds. This approach simulates a wide range of adaptive strategies, introducing stochasticity and complexity to the evaluation process.


\begin{figure}[t]
    \centering
\begin{minipage}{0.5\linewidth}
\begin{lstlisting}[language=json]
{
    "players": {
        "num_p": 4, 
        "num_e": 2, 
        "num_ctrl": 2, 
        "num_unctrl": 2, 
        "random_respawn": True,
        "respawn_region":  {***},
        "reception_range": 2,
        "velocity_p": 0.3,
        "velocity_e": 0.6,
        "unseen_drones": [***]
    },
    "site":{
        "boundary": {
            "width" : 3.6, 
            "height" : 5, 
        },
        "obstacles": {
            "obstacle1":{***}
        },
    },
    "task":{
        "task_name": 4p2e1o,
        "capture_range": 0.2,
        "safe_radius": 0.1,
        "task_horizon": 100, 
        "fps": 10,
    }
}
\end{lstlisting}
\end{minipage}
    \caption{An example of environment configuration file.}
    \label{fig:env_json}
\end{figure}

\section{Environment Configurator}
\label{appendix:env_config}

The \framework environment configurator allows users to define and modify multi-drone pursuit scenarios through a structured JSON file. Fig.~\ref{fig:env_json} provides an example configuration file that specifies key parameters across three categories: \textit{players}, \textit{site}, and \textit{task}.

\textbf{Players Configuration:}  
This section defines the number and roles of agents in the environment, including the number of pursuers (\texttt{num\_p}), evaders (\texttt{num\_e}), controlled agents (\texttt{num\_ctrl}), and unseen teammates (\texttt{num\_unctrl}). Additional parameters such as random respawn behavior, reception range, and velocity settings further customize agent interactions. The \texttt{unseen\_drones} field allows users to specify different unseen teammate models from the unseen drone zoo.

\textbf{Site Configuration:}  
This section defines the physical properties of the environment, including its boundary dimensions (\texttt{width}, \texttt{height}) and obstacle placements. Obstacles can be configured individually to introduce varying levels of complexity.

\textbf{Task Configuration:}  
This section sets the pursuit task parameters, including the capture range (\texttt{capture\_range}), safety radius (\texttt{safe\_radius}), task duration (\texttt{task\_horizon}), and simulation frame rate (\texttt{fps}). The \texttt{task\_name} field provides a label for different predefined environment scenarios.

This modular configuration enables flexible environment customization, facilitating experiments across diverse multi-drone pursuit scenarios.

\section{Real-world Deployment}
\label{appendix:real}
To validate the feasibility of our algorithms in real-world scenarios, we design and conduct hardware experiments using Crazyflie drones, the FZMotion motion capture system, and the Crazyswarm validation platform.
The FZMotion motion capture system is responsible for real-time position measurement of the drones. It transmits the positional data in point cloud format to Crazyswarm, where the information is processed as input for the algorithm.
The Crazyswarm platform is deployed on two different edge nodes, i.e. a laptop ( Lenovo ThinkPad T590) and a Jetson Orin Nano, which handles the reception of drone position data from the motion capture system, executes the adaptive teaming algorithm, and transmits control commands to the Crazyflie drones via Crazyradio PA. Upon receiving these commands, the Crazyflie drones execute the prescribed maneuvers. The onboard Mellinger controller ensures accurate trajectory tracking, allowing the drones to follow the control signals with high precision.
This real-world deployment setup enables the direct evaluation of our learned policies in physical drone systems, bridging the gap between simulation and real-world execution.

\begin{table}[t]
        \centering
        \caption{Implementation hyperparameters of ATM baseline algorithm.}
        \begin{tabular}{@{}cc|cc@{}}
            \toprule
            \textbf{Parameters} & \textbf{Values} & \textbf{Parameters} & \textbf{Values}\\ \midrule
            Batch size & 1024 & Minibatch size & 256\\ 
            Lambda (\(\lambda\)) & 0.99  & Generalized advantage estimation lambda (\(\lambda_{gae}\)) & 0.95\\ 
            Learning rate & 3e-4 & Value loss coefficient($c_1$) & 1 \\ 
            Entropy coefficient(\(\epsilon_{clip}\)) & 0.01 & PPO epoch & 20 \\ 
            Total environment step & 1e6 &  History length & 1 \\ 
            Embedding size & 16 & Hidden size & 128 \\
            \bottomrule
        \end{tabular}
        \label{tab:ATM_params}
    \end{table}


\section{Implementation Details of Adaptive Teaming with Modeling}
\label{appendix:atm}

Adaptive Teaming with Modeling (ATM) extends the MAPPO algorithm~\cite{yu2022surprising} by incorporating an additional teammate modeling network \( f \). 
This network generates team encoding vectors to represent the behavioral characteristics of unseen teammates, improving coordination in multi-drone pursuit.
The modeling network \( f \) processes three types of inputs: (1) observed evader states history, (2) self-observed states history, and (3) relative positions history between agents. These inputs are transformed using independent fully connected layers, aggregated through weighted averaging, and combined into a unified team representation. The final embedding is a fixed-dimensional vector, integrated into the actor network of MAPPO to enhance decision-making in adaptive teaming scenarios.
The details of hyperparameters are listed in Table~\ref{tab:ATM_params}.

% \section{Implementation Details}
% \label{appendix:implementation}


%     In our experiments, the reinforcement learning algorithm for agents in the training method is primarily PPO and its related variants. The relevant parameters are shown in Table.~\ref{tab:ppo_params}. 
    
%     \begin{table}[h]
%         \centering
%         \caption{Implementation hyperparameters of PPO-Based.}
%         \begin{tabular}{@{}cc|cc@{}}
%             \toprule
%             \textbf{Parameters} & \textbf{Values} & \textbf{Parameters} & \textbf{Values}\\ \midrule
%             Batch size & 1024 & Minibatch size & 256\\ 
%             Lambda (\(\lambda\)) & 0.99  & Generalized advantage estimation lambda (\(\lambda_{gae}\)) & 0.95\\ 
%             Learning rate & 3e-4 & Value loss coefficient($c_1$) & 1 \\ 
%             Entropy coefficient(\(\epsilon_{clip}\)) & 0.01 & PPO epoch & 20 \\ 
%             Total environment step & 1e6\\ 
%             \bottomrule
%         \end{tabular}
%         \label{tab:ppo_params}
%     \end{table}

%     \textbf{Multi-Agent Proximal Policy Optimization (MAPPO):}
%     MAPPO is a variant of PPO that extends it to multi-agent settings. During training, agents share information, but during execution, each agent makes decisions independently based only on its own observations and state. Since MAPPO follows the same learning process as PPO, it uses identical hyperparameters. In our experiments, MAPPO agents are primarily initialized using the SP method.
    
%     \textbf{Self-Play (SP):}  
%     Throughout the training process, a single agent is employed. This agent executes actions in a distributed manner based on the current state of each pursuer in the environment, effectively achieving a self-play mechanism.
    
%     \textbf{Population-based Training (PBT):}  
%     At the beginning of training, a population of agents with a predefined size is initialized, where each agent is a PPO model with an identical architecture. During training, all agents are trained independently while sharing the same task and dataset. At fixed intervals, the performance of all models is evaluated, and low-performing models are replaced with high-performing ones. The newly replaced models undergo slight perturbations in their hyperparameters (mutation) to explore potentially better configurations. The relevant parameters are shown in Table.~\ref{tab:pbt_params}. 

%     \begin{table}[h]
%         \centering
%         \caption{Implementation hyperparameters of PBT.}
%         \resizebox{\linewidth}{!}{
%         \begin{tabular}{@{}cc|cc@{}}
%             \toprule
%             \textbf{Parameters} & \textbf{Values} & \textbf{Parameters} & \textbf{Values}\\ \midrule
%             Population Size & depends on tasks (\textgreater{}=number pursuers) & iteration per selection & depends on tasks (\textgreater{}=number pursuers -  1) \\ 
%             mutate probability & 0.33  & epsilon clip & 0.01\\ 
%             \bottomrule
%         \end{tabular}}
%         \label{tab:pbt_params}
%     \end{table}
