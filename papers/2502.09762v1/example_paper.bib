@article{wang2024open,
  title={Open Ad Hoc Teamwork with Cooperative Game Theory},
  author={Wang, Jianhong and Li, Yang and Zhang, Yuan and Pan, Wei and Kaski, Samuel},
  journal={arXiv preprint arXiv:2402.15259},
  year={2024}
}

@inproceedings{balduzzi2019open,
  title={Open-ended learning in symmetric zero-sum games},
  author={Balduzzi, David and Garnelo, Marta and Bachrach, Yoram and Czarnecki, Wojciech and Perolat, Julien and Jaderberg, Max and Graepel, Thore},
  booktitle={International Conference on Machine Learning},
  pages={434--443},
  year={2019},
  organization={PMLR}
}

@inproceedings{xie2021learning,
  title={Learning latent representations to influence multi-agent interaction},
  author={Xie, Annie and Losey, Dylan and Tolsma, Ryan and Finn, Chelsea and Sadigh, Dorsa},
  booktitle={Conference on robot learning},
  pages={575--588},
  year={2021},
  organization={PMLR}
}

@article{rahman2022generating,
  title={Generating teammates for training robust ad hoc teamwork agents via best-response diversity},
  author={Rahman, Arrasy and Fosong, Elliot and Carlucho, Ignacio and Albrecht, Stefano V},
  journal={arXiv preprint arXiv:2207.14138},
  year={2022}
}

@article{li2024tackling,
  title={Tackling cooperative incompatibility for zero-shot human-ai coordination},
  author={Li, Yang and Zhang, Shao and Sun, Jichen and Zhang, Wenhao and Du, Yali and Wen, Ying and Wang, Xinbing and Pan, Wei},
  journal={Journal of Artificial Intelligence Research},
  volume={80},
  pages={1139--1185},
  year={2024}
}

@inproceedings{charakorn2023generating,
  title={Generating diverse cooperative agents by learning incompatible policies},
  author={Charakorn, Rujikorn and Manoonpong, Poramate and Dilokthanakul, Nat},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023}
}

@article{wang2024n,
  title={N-Agent Ad Hoc Teamwork},
  author={Wang, Caroline and Rahman, Arrasy and Durugkar, Ishan and Liebman, Elad and Stone, Peter},
  journal={arXiv preprint arXiv:2404.10740},
  year={2024}
}

@inproceedings{barrett2014cooperating,
  title={Cooperating with unknown teammates in robot soccer},
  author={Barrett, Samuel and Stone, Peter},
  booktitle={Workshops at the Twenty-Eighth AAAI Conference on Artificial Intelligence},
  year={2014}
}

@article{barrett2017making,
  title={Making friends on the fly: Cooperating with new teammates},
  author={Barrett, Samuel and Rosenfeld, Avi and Kraus, Sarit and Stone, Peter},
  journal={Artificial Intelligence},
  volume={242},
  pages={132--171},
  year={2017},
  publisher={Elsevier}
}

@inproceedings{rahman2021towards,
  title={Towards open ad hoc teamwork using graph-based policy learning},
  author={Rahman, Muhammad A and Hopner, Niklas and Christianos, Filippos and Albrecht, Stefano V},
  booktitle={International conference on machine learning},
  pages={8776--8786},
  year={2021},
  organization={PMLR}
}

@article{carroll2019utility,
  title={On the utility of learning about humans for human-ai coordination},
  author={Carroll, Micah and Shah, Rohin and Ho, Mark K and Griffiths, Tom and Seshia, Sanjit and Abbeel, Pieter and Dragan, Anca},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{yu2022surprising,
  title={The surprising effectiveness of ppo in cooperative multi-agent games},
  author={Yu, Chao and Velu, Akash and Vinitsky, Eugene and Gao, Jiaxuan and Wang, Yu and Bayen, Alexandre and Wu, Yi},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24611--24624},
  year={2022}
}

@article{jaderberg2017population,
  title={Population based training of neural networks},
  author={Jaderberg, Max and Dalibard, Valentin and Osindero, Simon and Czarnecki, Wojciech M and Donahue, Jeff and Razavi, Ali and Vinyals, Oriol and Green, Tim and Dunning, Iain and Simonyan, Karen and others},
  journal={arXiv preprint arXiv:1711.09846},
  year={2017}
}

@misc{PPO,
      title={Proximal Policy Optimization Algorithms}, 
      author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
      year={2017},
      eprint={1707.06347},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{yourdshahi2018towards,
  title={Towards large scale ad-hoc teamwork},
  author={Yourdshahi, Elnaz Shafipour and Pinder, Thomas and Dhawan, Gauri and Marcolino, Leandro Soriano and Angelov, Plamen},
  booktitle={2018 IEEE International Conference on Agents (ICA)},
  pages={44--49},
  year={2018},
  organization={IEEE}
}

@inproceedings{barrett2011empirical,
  title={Empirical evaluation of ad hoc teamwork in the pursuit domain},
  author={Barrett, Samuel and Stone, Peter and Kraus, Sarit},
  booktitle={The 10th International Conference on Autonomous Agents and Multiagent Systems-Volume 2},
  pages={567--574},
  year={2011}
}

@article{papoudakis2021agent,
  title={Agent modelling under partial observability for deep reinforcement learning},
  author={Papoudakis, Georgios and Christianos, Filippos and Albrecht, Stefano},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={19210--19222},
  year={2021}
}

@article{bard2020hanabi,
  title={The hanabi challenge: A new frontier for ai research},
  author={Bard, Nolan and Foerster, Jakob N and Chandar, Sarath and Burch, Neil and Lanctot, Marc and Song, H Francis and Parisotto, Emilio and Dumoulin, Vincent and Moitra, Subhodeep and Hughes, Edward and others},
  journal={Artificial Intelligence},
  volume={280},
  pages={103216},
  year={2020},
  publisher={Elsevier}
}

@article{canaan2022generating,
  title={Generating and Adapting to Diverse Ad Hoc Partners in Hanabi},
  author={Canaan, Rodrigo and Gao, Xianbo and Togelius, Julian and Nealen, Andy and Menzel, Stefan},
  journal={IEEE Transactions on Games},
  volume={15},
  number={2},
  pages={228--241},
  year={2022},
  publisher={IEEE}
}

@inproceedings{chen2020aateam,
  title={Aateam: Achieving the ad hoc teamwork by employing the attention mechanism},
  author={Chen, Shuo and Andrejczuk, Ewa and Cao, Zhiguang and Zhang, Jie},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={05},
  pages={7095--7102},
  year={2020}
}

@inproceedings{gu2021online,
  title={Online ad hoc teamwork under partial observability},
  author={Gu, Pengjie and Zhao, Mengchen and Hao, Jianye and An, Bo},
  booktitle={International conference on learning representations},
  year={2021}
}

@inproceedings{stone2009leading,
  title={Leading a best-response teammate in an ad hoc team},
  author={Stone, Peter and Kaminka, Gal A and Rosenschein, Jeffrey S},
  booktitle={International Workshop on Agent-Mediated Electronic Commerce},
  pages={132--146},
  year={2009},
  organization={Springer}
}

@article{hola-drone,
  author       = {Yang Li and
                  Dengyu Zhang and
                  Junfan Chen and
                  Ying Wen and
                  Qingrui Zhang and
                  Shaoshuai Mou and
                  Wei Pan},
  title        = {HOLA-Drone: Hypergraphic Open-ended Learning for Zero-Shot Multi-Drone
                  Cooperative Pursuit},
  journal      = {CoRR},
  volume       = {abs/2409.08767},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2409.08767},
  doi          = {10.48550/ARXIV.2409.08767},
  eprinttype    = {arXiv},
  eprint       = {2409.08767},
  timestamp    = {Mon, 14 Oct 2024 08:21:03 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2409-08767.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{anyplay,
  author       = {Keane Lucas and
                  Ross E. Allen},
  editor       = {Piotr Faliszewski and
                  Viviana Mascardi and
                  Catherine Pelachaud and
                  Matthew E. Taylor},
  title        = {Any-Play: An Intrinsic Augmentation for Zero-Shot Coordination},
  booktitle    = {21st International Conference on Autonomous Agents and Multiagent
                  Systems, {AAMAS} 2022, Auckland, New Zealand, May 9-13, 2022},
  pages        = {853--861},
  publisher    = {International Foundation for Autonomous Agents and Multiagent Systems
                  {(IFAAMAS)}},
  year         = {2022},
  url          = {https://www.ifaamas.org/Proceedings/aamas2022/pdfs/p853.pdf},
  doi          = {10.5555/3535850.3535946},
  timestamp    = {Mon, 18 Jul 2022 17:13:00 +0200},
  biburl       = {https://dblp.org/rec/conf/atal/LucasA22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{wang2024zsc,
  title={Zsc-eval: An evaluation toolkit and benchmark for multi-agent zero-shot coordination},
  author={Wang, Xihuai and Zhang, Shao and Zhang, Wenhao and Dong, Wentao and Chen, Jingxiao and Wen, Ying and Zhang, Weinan},
  booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  year={2024}
}

@inproceedings{MEP,
  title={Maximum entropy population-based training for zero-shot human-ai coordination},
  author={Zhao, Rui and Song, Jinming and Yuan, Yufeng and Hu, Haifeng and Gao, Yang and Wu, Yi and Sun, Zhongqian and Yang, Wei},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={5},
  pages={6145--6153},
  year={2023}
}

@inproceedings{HSP,
  author       = {Chao Yu and Jiaxuan Gao and Weilin Liu and Botian Xu and Hao Tang and Jiaqi Yang and Yu Wang and Yi Wu},
  title        = {Learning Zero-Shot Cooperation with Humans, Assuming Humans Are Biased},
  booktitle    = {The Eleventh International Conference on Learning Representations,
                  {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
  publisher    = {OpenReview.net},
  year         = {2023},
  url          = {https://openreview.net/pdf?id=TrwE8l9aJzs},
  timestamp    = {Fri, 30 Jun 2023 14:55:53 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/0005GLXTYWW23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{HARL,
  title={On the utility of learning about humans for human-ai coordination},
  author={Micah Carroll and Rohin Shah and Mark K. Ho and Thomas L. Griffiths and Sanjit A. Seshia and Pieter Abbeel and Anca Dragan},
 year={2020},
  journal={Advances in neural information processing systems},
  volume={32}
}

@article{FCP,
  title={Collaborating with humans without human data},
  author={Strouse, DJ and McKee, Kevin and Botvinick, Matt and Hughes, Edward and Everett, Richard},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={14502--14515},
  year={2021}
}

@inproceedings{TrajDi,
  title={Trajectory diversity for zero-shot coordination},
  author={Lupu, Andrei and Cui, Brandon and Hu, Hengyuan and Foerster, Jakob},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={7204--7213},
  year={2021},
  organization={PMLR}
}

@inproceedings{li2023cooperative,
  title={Cooperative open-ended learning framework for zero-shot coordination},
  author={Li, Yang and Zhang, Shao and Sun, Jichen and Du, Yali and Wen, Ying and Wang, Xinbing and Pan, Wei},
  booktitle={International Conference on Machine Learning},
  pages={20470--20484},
  year={2023},
  organization={PMLR}
}

@inproceedings{hu2020other,
  title={“other-play” for zero-shot coordination},
  author={Hu, Hengyuan and Lerer, Adam and Peysakhovich, Alex and Foerster, Jakob},
  booktitle={International Conference on Machine Learning},
  pages={4399--4410},
  year={2020},
  organization={PMLR}
}

@inproceedings{stone2010ad,
  title={Ad hoc autonomous agent teams: Collaboration without pre-coordination},
  author={Stone, Peter and Kaminka, Gal and Kraus, Sarit and Rosenschein, Jeffrey},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={24},
  number={1},
  pages={1504--1509},
  year={2010}
}

@inproceedings{madden_modeling_2011,
	title = {Modeling the effects of mass and age variation in wolves to explore the effects of heterogeneity in robot team composition},
	url = {https://ieeexplore.ieee.org/document/6181362},
	doi = {10.1109/ROBIO.2011.6181362},
	abstract = {No two wolves are the same. Some of the variations between individuals such as variation in mass and age have major implications on their hunting abilities. The predatory success of a wolf pack has been found to depend on its composition of individuals along these dimensions. Building from a system for simulating wolf hunting behavior in earlier work [1], this paper explores the effects of heterogeneity among individuals in a team and the utility of these variations for the team as a whole. The implications for robot team control and organization are presented.},
	urldate = {2024-06-06},
	booktitle = {2011 {IEEE} {International} {Conference} on {Robotics} and {Biomimetics}},
	author = {Madden, John D. and Arkin, Ronald C.},
	month = dec,
	year = {2011},
	keywords = {Aging, Animals, Biological system modeling, Laboratories, Mobile robots, Robot kinematics},
	pages = {663--670},
	file = {IEEE Xplore Abstract Record:D\:\\dengyu\\Zotero\\storage\\VHWMPVZA\\6181362.html:text/html;IEEE Xplore Full Text PDF:D\:\\dengyu\\Zotero\\storage\\TXC3QYAZ\\Madden and Arkin - 2011 - Modeling the effects of mass and age variation in .pdf:application/pdf},
}

@article{angelani_collective_2012,
	title = {Collective {Predation} and {Escape} {Strategies}},
	volume = {109},
	doi = {10.1103/PhysRevLett.109.118104},
	number = {11},
	journal = {Phys. Rev. Lett.},
	author = {Angelani, Luca},
	year = {2012},
	file = {Phys. Rev. Lett. 109, 118104 (2012) - Collective Predation and Escape Strategies:D\:\\dengyu\\Zotero\\storage\\YWQUE7EA\\PhysRevLett.109.html:text/html},
}

@inproceedings{TD3,
  title={Addressing function approximation error in actor-critic methods},
  author={Fujimoto, Scott and Hoof, Herke and Meger, David},
  booktitle={International conference on machine learning},
  pages={1587--1596},
  year={2018},
  organization={PMLR}
}

@article{DDPG,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, TP},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}

@inproceedings{jianhong2024oaht,
author = {Wang, Jianhong and Li, Yang and Zhang, Yuan and Pan, Wei and Kaski, Samuel},
title = {Open ad hoc teamwork with cooperative game theory},
year = {2024},
publisher = {JMLR.org},
abstract = {Ad hoc teamwork poses a challenging problem, requiring the design of an agent to collaborate with teammates without prior coordination or joint training. Open ad hoc teamwork (OAHT) further complicates this challenge by considering environments with a changing number of teammates, referred to as open teams. One promising solution in practice to this problem is leveraging the generalizability of graph neural networks to handle an unrestricted number of agents with various agent-types, named graph-based policy learning (GPL). However, its joint Q-value representation over a coordination graph lacks convincing explanations. In this paper, we establish a new theory to understand the representation of the joint Q-value for OAHT and its learning paradigm, through the lens of cooperative game theory. Building on our theory, we propose a novel algorithm named CIAO, based on GPL's framework, with additional provable implementation tricks that can facilitate learning. The demos of experimental results are available on https://sites.google.com/view/ciao2024, and the code of experiments is published on https://github.com/hsvgbkhgbv/CIAO.},
booktitle = {Proceedings of the 41st International Conference on Machine Learning},
articleno = {2086},
numpages = {29},
location = {Vienna, Austria},
series = {ICML'24}
}

@article{samvelyan2019starcraft,
  title={The starcraft multi-agent challenge},
  author={Samvelyan, Mikayel and Rashid, Tabish and De Witt, Christian Schroeder and Farquhar, Gregory and Nardelli, Nantas and Rudner, Tim GJ and Hung, Chia-Man and Torr, Philip HS and Foerster, Jakob and Whiteson, Shimon},
  journal={arXiv preprint arXiv:1902.04043},
  year={2019}
}

@article{zhou_cooperative_2016,
	title = {Cooperative pursuit with {Voronoi} partitions},
	volume = {72},
	issn = {0005-1098},
	url = {https://www.sciencedirect.com/science/article/pii/S0005109816301911},
	doi = {10.1016/j.automatica.2016.05.007},
	abstract = {This work considers a pursuit–evasion game in which a number of pursuers are attempting to capture a single evader. Cooperation among multiple agents can be difficult to achieve, as it may require the selection of actions in the joint input space of all agents. This work presents a decentralized, real-time algorithm for cooperative pursuit of a single evader by multiple pursuers in bounded, simply-connected planar domains. The algorithm is based on minimizing the area of the generalized Voronoi partition of the evader. The pursuers share state information but compute their inputs independently. No assumptions are made about the evader’s control strategies other than requiring the evader control inputs to conform to a speed limit. Proof of guaranteed capture is shown when the domain is convex and the players’ motion models are kinematic. Simulation results are presented showing the efficiency and effectiveness of this strategy.},
	urldate = {2024-06-06},
	journal = {Automatica},
	author = {Zhou, Zhengyuan and Zhang, Wei and Ding, Jerry and Huang, Haomiao and Stipanović, Dušan M. and Tomlin, Claire J.},
	month = oct,
	year = {2016},
	keywords = {Cooperative pursuit, Pursuit–evasion games, Voronoi},
	pages = {64--72},
	file = {ScienceDirect Snapshot:D\:\\dengyu\\Zotero\\storage\\JFPX434F\\S0005109816301911.html:text/html},
}

@article{pierson_intercepting_2017,
	title = {Intercepting {Rogue} {Robots}: {An} {Algorithm} for {Capturing} {Multiple} {Evaders} {With} {Multiple} {Pursuers}},
	volume = {2},
	issn = {2377-3766},
	shorttitle = {Intercepting {Rogue} {Robots}},
	url = {https://ieeexplore.ieee.org/document/7801073},
	doi = {10.1109/LRA.2016.2645516},
	abstract = {We propose a distributed algorithm for the cooperative pursuit of multiple evaders using multiple pursuers in a bounded convex environment. The algorithm is suitable for intercepting rogue drones in protected airspace, among other applications. The pursuers do not know the evaders' policy, but by using a global “area-minimization” strategy based on a Voronoi tessellation of the environment, we guarantee the capture of all evaders in finite time. We present a decentralized version of this policy applicable in two-dimensional (2-D) and 3-D environments, and show in multiple simulations that it outperforms other decentralized multipursuer heuristics. Experiments with both autonomous and human-controlled robots were conducted to demonstrate the practicality of the approach. Specifically, human-controlled evaders are not able to avoid capture with the algorithm.},
	number = {2},
	urldate = {2024-06-06},
	journal = {IEEE Robotics and Automation Letters},
	author = {Pierson, Alyssa and Wang, Zijian and Schwager, Mac},
	month = apr,
	year = {2017},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {autonomous agents, Distributed robot systems, Drones, Games, path planning for multiple mobile robots or agents, Robot kinematics, Three-dimensional displays, Two dimensional displays, Wildlife},
	pages = {530--537},
	file = {IEEE Xplore Abstract Record:D\:\\dengyu\\Zotero\\storage\\5DYFE7YE\\7801073.html:text/html;IEEE Xplore Full Text PDF:D\:\\dengyu\\Zotero\\storage\\H9I3K4C4\\Pierson et al. - 2017 - Intercepting Rogue Robots An Algorithm for Captur.pdf:application/pdf},
}

@inproceedings{matignon_hysteretic_2007,
	title = {Hysteretic {Q}-learning : an algorithm for {Decentralized} {Reinforcement} {Learning} in {Cooperative} {Multi}-{Agent} {Teams}},
	shorttitle = {Hysteretic {Q}-learning},
	url = {https://ieeexplore.ieee.org/abstract/document/4399095},
	doi = {10.1109/IROS.2007.4399095},
	abstract = {Multi-agent systems (MAS) are a field of study of growing interest in a variety of domains such as robotics or distributed controls. The article focuses on decentralized reinforcement learning (RL) in cooperative MAS, where a team of independent learning robots (IL) try to coordinate their individual behavior to reach a coherent joint behavior. We assume that each robot has no information about its teammates' actions. To date, RL approaches for such ILs did not guarantee convergence to the optimal joint policy in scenarios where the coordination is difficult. We report an investigation of existing algorithms for the learning of coordination in cooperative MAS, and suggest a Q-learning extension for ILs, called hysteretic Q-learning. This algorithm does not require any additional communication between robots. Its advantages are showing off and compared to other methods on various applications: bi-matrix games, collaborative ball balancing task and pursuit domain.},
	urldate = {2024-06-06},
	booktitle = {2007 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems}},
	author = {Matignon, Laetitia and Laurent, Guillaume J. and Le Fort-Piat, Nadine},
	month = oct,
	year = {2007},
	note = {ISSN: 2153-0866},
	keywords = {Convergence, Distributed control, Game theory, Hysteresis, Intelligent robots, Learning, Multiagent systems, Robot kinematics, Stochastic processes, USA Councils},
	pages = {64--69},
	file = {IEEE Xplore Full Text PDF:D\:\\dengyu\\Zotero\\storage\\IEEY5XKZ\\Matignon et al. - 2007 - Hysteretic Q-learning  an algorithm for Decentral.pdf:application/pdf},
}

@article{de_souza_decentralized_2021,
	title = {Decentralized {Multi}-{Agent} {Pursuit} {Using} {Deep} {Reinforcement} {Learning}},
	volume = {6},
	issn = {2377-3766},
	url = {https://ieeexplore.ieee.org/abstract/document/9387125},
	doi = {10.1109/LRA.2021.3068952},
	abstract = {Pursuit-evasion is the problem of capturing mobile targets with one or more pursuers. We use deep reinforcement learning for pursuing an omnidirectional target with multiple, homogeneous agents that are subject to unicycle kinematic constraints. We use shared experience to train a policy for a given number of pursuers, executed independently by each agent at run-time. The training uses curriculum learning, a sweeping-angle ordering to locally represent neighboring agents, and a reward structure that encourages a good formation and combines individual and group rewards. Simulated experiments with a reactive evader and up to eight pursuers show that our learning-based approach outperforms recent reinforcement learning techniques as well as non-holonomic adaptations of classical algorithms. The learned policy is successfully transferred to the real-world in a proof-of-concept demonstration with three motion-constrained pursuer drones.},
	number = {3},
	urldate = {2024-06-06},
	journal = {IEEE Robotics and Automation Letters},
	author = {de Souza, Cristino and Newbury, Rhys and Cosgun, Akansel and Castillo, Pedro and Vidolov, Boris and Kulić, Dana},
	month = jul,
	year = {2021},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {cooperating robots, Drones, Games, Kinematics, Multi-robot systems, reinforcement learning, Reinforcement learning, Task analysis, Training, Trajectory},
	pages = {4552--4559}
}

@article{li_robust_2019,
	title = {Robust {Multi}-{Agent} {Reinforcement} {Learning} via {Minimax} {Deep} {Deterministic} {Policy} {Gradient}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4327},
	doi = {10.1609/aaai.v33i01.33014213},
	abstract = {Despite the recent advances of deep reinforcement learning (DRL), agents trained by DRL tend to be brittle and sensitive to the training environment, especially in the multi-agent scenarios. In the multi-agent setting, a DRL agent’s policy can easily get stuck in a poor local optima w.r.t. its training partners – the learned policy may be only locally optimal to other agents’ current policies. In this paper, we focus on the problem of training robust DRL agents with continuous actions in the multi-agent learning setting so that the trained agents can still generalize when its opponents’ policies alter. To tackle this problem, we proposed a new algorithm, MiniMax Multi-agent Deep Deterministic Policy Gradient (M3DDPG) with the following contributions: (1) we introduce a minimax extension of the popular multi-agent deep deterministic policy gradient algorithm (MADDPG), for robust policy learning; (2) since the continuous action space leads to computational intractability in our minimax learning objective, we propose Multi-Agent Adversarial Learning (MAAL) to efficiently solve our proposed formulation. We empirically evaluate our M3DDPG algorithm in four mixed cooperative and competitive multi-agent environments and the agents trained by our method significantly outperforms existing baselines.},
	language = {en},
	number = {01},
	urldate = {2024-06-06},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Li, Shihui and Wu, Yi and Cui, Xinyue and Dong, Honghua and Fang, Fei and Russell, Stuart},
	month = jul,
	year = {2019},
	note = {Number: 01},
	pages = {4213--4220},
	file = {Full Text PDF:D\:\\dengyu\\Zotero\\storage\\KG8AAWLK\\Li et al. - 2019 - Robust Multi-Agent Reinforcement Learning via Mini.pdf:application/pdf},
}

@inproceedings{Rahman2024Mini,
  author       = {Muhammad Rahman and
                  Jiaxun Cui and
                  Peter Stone},
  editor       = {Michael J. Wooldridge and
                  Jennifer G. Dy and
                  Sriraam Natarajan},
  title        = {Minimum Coverage Sets for Training Robust Ad Hoc Teamwork Agents},
  booktitle    = {Thirty-Eighth {AAAI} Conference on Artificial Intelligence, {AAAI}
                  2024, Thirty-Sixth Conference on Innovative Applications of Artificial
                  Intelligence, {IAAI} 2024, Fourteenth Symposium on Educational Advances
                  in Artificial Intelligence, {EAAI} 2014, February 20-27, 2024, Vancouver,
                  Canada},
  pages        = {17523--17530},
  publisher    = {{AAAI} Press},
  year         = {2024},
  url          = {https://doi.org/10.1609/aaai.v38i16.29702},
  doi          = {10.1609/AAAI.V38I16.29702},
  timestamp    = {Tue, 19 Nov 2024 15:59:16 +0100},
  biburl       = {https://dblp.org/rec/conf/aaai/RahmanCS24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{qi_cascaded_2024,
	title = {Cascaded {Attention}: {Adaptive} and {Gated} {Graph} {Attention} {Network} for {Multiagent} {Reinforcement} {Learning}},
	volume = {35},
	issn = {2162-2388},
	shorttitle = {Cascaded {Attention}},
	url = {https://ieeexplore.ieee.org/abstract/document/9913678},
	doi = {10.1109/TNNLS.2022.3197918},
	abstract = {Modeling the interactive relationships of agents is critical to improving the collaborative capability of a multiagent system. Some methods model these by predefined rules. However, due to the nonstationary problem, the interactive relationship changes over time and cannot be well captured by rules. Other methods adopt a simple mechanism such as an attention network to select the neighbors the current agent should collaborate with. However, in large-scale multiagent systems, collaborative relationships are too complicated to be described by a simple attention network. We propose an adaptive and gated graph attention network (AGGAT), which models the interactive relationships between agents in a cascaded manner. In the AGGAT, we first propose a graph-based hard attention network that roughly filters irrelevant agents. Then, normal soft attention is adopted to decide the importance of each neighbor. Finally, gated attention further refines the collaborative relationship of agents. By using cascaded attention, the collaborative relationship of agents is precisely learned in a coarse-to-fine style. Extensive experiments are conducted on a variety of cooperative tasks. The results indicate that our proposed method outperforms state-of-the-art baselines.},
	number = {3},
	urldate = {2024-06-06},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Qi, Shuhan and Huang, Xinhao and Peng, Peixi and Huang, Xuzhong and Zhang, Jiajia and Wang, Xuan},
	month = mar,
	year = {2024},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Adaptation models, Cascaded attention, Collaboration, Color, Logic gates, Multi-agent systems, multiagent coordination, Protocols, reinforcement learning (RL), Task analysis},
	pages = {3769--3779},
	file = {IEEE Xplore Abstract Record:D\:\\dengyu\\Zotero\\storage\\VB2QZYFJ\\9913678.html:text/html;IEEE Xplore Full Text PDF:D\:\\dengyu\\Zotero\\storage\\55FNUEIJ\\Qi et al. - 2024 - Cascaded Attention Adaptive and Gated Graph Atten.pdf:application/pdf},
}

@inproceedings{shah_multi-agent_2019,
	address = {Cham},
	title = {Multi-agent {Cooperative} {Pursuit}-{Evasion} {Strategies} {Under} {Uncertainty}},
	isbn = {978-3-030-05816-6},
	doi = {10.1007/978-3-030-05816-6_32},
	abstract = {We present a method for a collaborative team of pursuing robots to contain and capture a single evading robot. The main challenge is that the pursuers do not know the position of the evader exactly nor do they know the policy of the evader. Instead, the pursuers maintain an estimate of the evader’s position over time from noisy online measurements. We propose a policy by which the pursuers move to maximally reduce the area of space reachable by the evader given the uncertainty in the evader’s position estimate. The policy is distributed in the sense that each pursuer only needs to know the positions of its closest neighbors. The policy guarantees that the evader’s reachable area is non-increasing between measurement updates regardless of the evader’s policy. Furthermore, we show in simulations that the pursuers capture the evader despite the position uncertainty provided that the pursuer’s measurement noise decreases with the distance to the evade.},
	language = {en},
	booktitle = {Distributed {Autonomous} {Robotic} {Systems}},
	publisher = {Springer International Publishing},
	author = {Shah, Kunal and Schwager, Mac},
	editor = {Correll, Nikolaus and Schwager, Mac and Otte, Michael},
	year = {2019},
	keywords = {Game theoretic control, Multi-agent pursuit-evasion, Reachability methods},
	pages = {451--468},
	file = {Full Text PDF:D\:\\dengyu\\Zotero\\storage\\CGZV2DIE\\Shah and Schwager - 2019 - Multi-agent Cooperative Pursuit-Evasion Strategies.pdf:application/pdf},
}

@article{muro_wolf-pack_2011,
	title = {Wolf-pack (\textit{{Canis} lupus}) hunting strategies emerge from simple rules in computational simulations},
	volume = {88},
	issn = {0376-6357},
	url = {https://www.sciencedirect.com/science/article/pii/S0376635711001884},
	doi = {10.1016/j.beproc.2011.09.006},
	abstract = {We have produced computational simulations of multi-agent systems in which wolf agents chase prey agents. We show that two simple decentralized rules controlling the movement of each wolf are enough to reproduce the main features of the wolf-pack hunting behavior: tracking the prey, carrying out the pursuit, and encircling the prey until it stops moving. The rules are (1) move towards the prey until a minimum safe distance to the prey is reached, and (2) when close enough to the prey, move away from the other wolves that are close to the safe distance to the prey. The hunting agents are autonomous, interchangeable and indistinguishable; the only information each agent needs is the position of the other agents. Our results suggest that wolf-pack hunting is an emergent collective behavior which does not necessarily rely on the presence of effective communication between the individuals participating in the hunt, and that no hierarchy is needed in the group to achieve the task properly.},
	number = {3},
	urldate = {2024-06-06},
	journal = {Behavioural Processes},
	author = {Muro, C. and Escobedo, R. and Spector, L. and Coppinger, R. P.},
	month = nov,
	year = {2011},
	keywords = {Collective behavior, Emergence, Wolf-pack hunting},
	pages = {192--197},
	file = {ScienceDirect Snapshot:D\:\\dengyu\\Zotero\\storage\\EK6ACV8S\\S0376635711001884.html:text/html},
}
@Article{chung2011search,
  title={Search and pursuit-evasion in mobile robotics: A survey},
  author={Chung, Timothy H and Hollinger, Geoffrey A and Isler, Volkan},
  journal={Autonomous robots},
  volume={31},
  pages={299--316},
  year={2011},
  publisher={Springer}
}

@article{ZhangDACOOP2023,
author = {Zhang, Zheng and Zhang, Dengyu and Zhang, Qingrui and Pan, Wei and Hu, Tianjiang},
year = {2023},
month = {11},
pages = {1-8},
title = {{DACOOP-A}: Decentralized Adaptive Cooperative Pursuit via Attention},
volume = {PP},
journal = {IEEE Robotics and Automation Letters},
doi = {10.1109/LRA.2023.3331886}
}

@article{queralta2020collaborative,
  title={Collaborative multi-robot search and rescue: Planning, coordination, perception, and active vision},
  author={Queralta, Jorge Pena and Taipalmaa, Jussi and Pullinen, Bilge Can and Sarker, Victor Kathan and Gia, Tuan Nguyen and Tenhunen, Hannu and Gabbouj, Moncef and Raitoharju, Jenni and Westerlund, Tomi},
  journal={Ieee Access},
  volume={8},
  pages={191617--191643},
  year={2020},
  publisher={IEEE}
}

@article{pymarl,
  title = {{The} {StarCraft} {Multi}-{Agent} {Challenge}},
  author = {Mikayel Samvelyan and Tabish Rashid and Christian Schroeder de Witt and Gregory Farquhar and Nantas Nardelli and Tim G. J. Rudner and Chia-Man Hung and Philiph H. S. Torr and Jakob Foerster and Shimon Whiteson},
  journal = {CoRR},
  volume = {abs/1902.04043},
  year = {2019},
}

@inproceedings{epymarl,
   title={Benchmarking Multi-Agent Deep Reinforcement Learning Algorithms in Cooperative Tasks},
   author={Georgios Papoudakis and Filippos Christianos and Lukas Schäfer and Stefano V. Albrecht},
   booktitle = {Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks (NeurIPS)},
   year={2021},
   url = {http://arxiv.org/abs/2006.07869},
   openreview = {https://openreview.net/forum?id=cIrPX-Sn5n},
   code = {https://github.com/uoe-agents/epymarl},
}

@article{hu2022marllib,
  author  = {Siyi Hu and Yifan Zhong and Minquan Gao and Weixun Wang and Hao Dong and Xiaodan Liang and Zhihui Li and Xiaojun Chang and Yaodong Yang},
  title   = {MARLlib: A Scalable and Efficient Multi-agent Reinforcement Learning Library},
  journal = {Journal of Machine Learning Research},
  year    = {2023},
}

@article{zhang2024multi,
  title={Multi-UAV Cooperative Pursuit of a Fast-Moving Target UAV Based on the GM-TD3 Algorithm},
  author={Zhang, Yaozhong and Ding, Meiyan and Yuan, Yao and Zhang, Jiandong and Yang, Qiming and Shi, Guoqing and Jiang, Frank and Lu, Meiqu},
  journal={Drones},
  volume={8},
  number={10},
  pages={557},
  year={2024},
  publisher={MDPI}
}

@article{schulman2017equivalence,
  title={Equivalence between policy gradients and soft q-learning},
  author={Schulman, John and Chen, Xi and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1704.06440},
  year={2017}
}

@article{Janosov2017Group,
author = {Janosov, Milan and Virágh, Csaba and Vásárhelyi, Gábor and Vicsek, Tamás},
year = {2017},
month = {05},
pages = {},
title = {Group chasing tactics: How to catch a faster prey},
volume = {19},
journal = {New Journal of Physics},
doi = {10.1088/1367-2630/aa69e7}
}

@article{janosov_group_2017,
	title = {Group chasing tactics: how to catch a faster prey?},
	volume = {19},
	issn = {1367-2630},
	shorttitle = {Group chasing tactics},
	url = {http://arxiv.org/abs/1701.00284},
	doi = {10.1088/1367-2630/aa69e7},
	abstract = {We propose a bio-inspired, agent-based approach to describe the natural phenomenon of group chasing in both two and three dimensions. Using a set of local interaction rules we created a continuous-space and discrete-time model with time delay, external noise and limited acceleration. We implemented a unique collective chasing strategy, optimized its parameters and studied its properties when chasing a much faster, erratic escaper. We show that collective chasing strategies can significantly enhance the chasers' success rate. Our realistic approach handles group chasing within closed, soft boundaries - contrasting most of those published in the literature with periodic ones -- and resembles several properties of pursuits observed in nature, such as the emergent encircling or the escaper's zigzag motion.},
	number = {5},
	urldate = {2025-01-25},
	journal = {New Journal of Physics},
	author = {Janosov, Milán and Virágh, Csaba and Vásárhelyi, Gábor and Vicsek, Tamás},
	month = may,
	year = {2017},
	note = {arXiv:1701.00284 [physics]},
	keywords = {/unread, Physics - Biological Physics, Quantitative Biology - Populations and Evolution, Statistics - Applications},
	pages = {053003},
	file = {Janosov et al_2017_Group chasing tactics.pdf:/home/gz/Zotero/storage/GMAXAL2S/Janosov et al_2017_Group chasing tactics.pdf:application/pdf;Snapshot:/home/gz/Zotero/storage/X8QRLPU6/1701.html:text/html},
}

@misc{chen2024dualcurriculumlearningframework,
      title={A Dual Curriculum Learning Framework for Multi-UAV Pursuit-Evasion in Diverse Environments}, 
      author={Jiayu Chen and Guosheng Li and Chao Yu and Xinyi Yang and Botian Xu and Huazhong Yang and Yu Wang},
      year={2024},
      eprint={2312.12255},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2312.12255}, 
}