\section{Related Work}
\label{sec:appendix1}
\begin{table}[t!]
    \centering
    \begin{tabular}{|c| c| c|c|c|}
    \hline
         Model & Dataset & $r_1$& $r_K$& $K$ \\
         \hline
         \hline
         \multirow{5}{*}{BERT}& SST2 &0.15& 0.09& 3\\
             & Reuters & 0.15 & 0.09& 3\\
             & 20 News & 0.15 & 0.09& 3\\
              & PAN19-P1 & 0.35 & 0.0 & 3\\
              & PAN19-P2 & 0.3 & 0.0 & 3\\
        \hline
        \multirow{5}{*}{RoBERTa}& SST2 &0.15& 0.09& 3\\
             & Reuters & 0.15 & 0.09& 3\\
             & 20 News & 0.15 & 0.0 & 3\\
              & PAN19-P1 & 0.35 & 0.0 & 3\\
              & PAN19-P2 & 0.3 & 0.0 & 3\\
             \hline
         
    \end{tabular}
    \caption{Masking ratios used in our experiments.}
    \label{table_hyperparams}
\end{table}
\begin{table*}[t]
  \centering
  \setlength\tabcolsep{0.23em}
  \small{
  \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
      \multirow{2}{*}{Model} &Fine-Tuning  & \multicolumn{2}{|c|}{Micro F1-Score}&Accuracy&\multicolumn{2}{|c|}{Accuracy/Macro F1-Score}\\
     \cline{3-7}
     & Strategy& 
     % \multicolumn{2}{c}{Text Categorization} & Sentiment Analysis & \multicolumn{2}{c}{Authorship Attribution}
     Reuters & 20 News & SST2& PAN19-P1&
     PAN19-P5\\
     \hline
    \hline 
    \multirow{2}{*}{BERT$_{\text{base}}$}& TICBM & $90.53_{\pm0.15}$ & $85.38_{\pm0.08}$ & $94.03_{\pm0.16}$  & $64.43_{\pm3.19}/52.17_{\pm8.35}$ & $63.90_{\pm2.51} / 35.23_{\pm3.56}$\\
    & TIACBM & $\mathbf{91.20}_{\pm0.20}$ & $\mathbf{85.65}_{\pm0.10}$ & $\mathbf{94.61}_{\pm0.08}$ & $\mathbf{77.32}_{\pm9.33}/\mathbf{60.60}_{\pm7.37}$ &$\mathbf{69.94}_{\pm1.98}/\mathbf{44.2}_{\pm2.67}$ \\
    \hline
    
    \multirow{2}{*}{RoBERTa$_{\text{base}}$}& TICBM & $90.35_{\pm0.09}$ & $85.45_{\pm0.13}$ & $94.92_{\pm0.14}$& $90.93_{\pm1.22} / 78.83_{\pm0.98}$ &  $67.92_{\pm0.95} / 39.26_{\pm1.61}$ \\
    & TIACBM & $\mathbf{91.06}_{\pm0.19}$ & $\mathbf{85.93}_{\pm0.18}$& $\mathbf{95.04}_{\pm 0.18}$&$\mathbf{93.98}_{\pm0.70}/\mathbf{83.78}_{\pm2.55}$ & $\mathbf{68.38}_{\pm1.53}/\mathbf{41.86}_{\pm2.15}$\\
    % \multicolumn{2}{c}{\textbf{SST2 (Sentiment CL)}} \\
    % CBM-BERT &   94.03$\pm$0.16/ - \\
    % CBM-RoBERTa & 94.92$\pm$0.14/ - \\
    % \textbf{Senti-CL-BERT} & \textbf{94.61 ± 0.08} / - \\
    % \textbf{Senti-CL-RoBERTa} & \textbf{95.04 ± 0.18} / \\
    % \hline
    % \multicolumn{2}{c}{\textbf{Reuters-21578 (Content words CL)1h30}} \\
    % CBM-BERT &   - / 90.53$\pm$0.15 \\
    % CBM-RoBERTa & / 90.35$\pm$0.09 \\
    % \textbf{Content-CL-BERT} & - / \textbf{91.20 ± 0.2}  \\
    % \textbf{Content-CL-RoBERTa} & - / \textbf{91.06 ± 0.19}  \\
    % \hline
    % \multicolumn{2}{c}{\textbf{20NewsGroup (Content words CL)}} \\
    % CBM-BERT &   85.38$\pm$0.08/ - \\
    % CBM-RoBERTa & 85.45$\pm$0.13 / - \\
    % \textbf{Content-CL-BERT} & \textbf{85.65±0.1}  \\
    % \textbf{Content-CL-RoBERTa} & \textbf{85.93±0.18}  \\
    % \hline
    % \multicolumn{2}{c}{\textbf{PAN19 - Problem1 (Stylistic words CL) }} \\
    % CBM-BERT &   78.6$\pm$9.73 / 63.53$\pm$2.33 \\
    % CBM-RoBERTa & 90.93$\pm$1.22 / 78.83$\pm$0.98 \\
    % \textbf{Style-CL-BERT} & \textbf{85.62}$\pm$\textbf{3.55}$/$\textbf{71.62}$\pm$\textbf{1.88} \\
    % \textbf{Style-CL-RoBERTa} & \textbf{93.98}$\pm$\textbf{0.7}$/$\textbf{83.78}$\pm$\textbf{2.55} \\
    % \hline
    % \multicolumn{2}{c}{\textbf{PAN19 - Problem5 (Stylistic words CL)}} \\
    % CBM-BERT &  63.9$\pm$2.51  / 35.23$\pm$3.56 \\
    % CBM-RoBERTa & 67.92$\pm$0.95 / 39.26$\pm$1.61 \\
    % \textbf{Style-CL-BERT} & \textbf{69.94}$\pm$\textbf{1.98}$/$\textbf{44.2}$\pm$\textbf{2.67} \\
    % \textbf{Style-CL-RoBERTa} & \textbf{68.38}$\pm$\textbf{1.53}$/$\textbf{41.86}$\pm$\textbf{2.15} \\
    \hline
  \end{tabular}
  }
  \caption{\label{curriulum_vs_anticurriculum} Comparison between curriculum (easy-to-hard) and anti-curriculum (hard-to-easy) approaches. Both methods benefit from a cyclic schedule and task-specific information.}
\end{table*}
Our framework is mostly related to work on curriculum learning ____. ____ divide curriculum learning methods into data-level ____, model-level ____, task-level ____, and objective-level ____ strategies. Most of existing studies focus on computer vision ____ and reinforcement learning ____. Methods in these domains are vaguely related to our approach, with some exceptions that employ curriculum based on masked image modeling (MIM) ____. In the image domain, the MIM approach was explored from multiple perspectives, which led to the development of adaptive masking strategies based on curriculum learning ____, that can produce more robust representations. A notable finding is that an easy-to-hard curriculum works generally well for image masking ____. In contrast, analogous studies focusing on text ____ suggest that a hard-to-easy curriculum, i.e.~using a decaying masking ratio, is more appropriate for text. Our results confirm the observations of ____ and ____, although we reset the masking ratio during training, resulting in a cyclic decaying masking ratio.

Curriculum learning methods specifically designed for text ____ are also related to our work. Some of the most popular approaches rely on text length ____ and model competence ____ to organize the samples from easy to hard. Recent approaches are based on more complex strategies. For instance, the state-of-the-art curriculum learning method proposed by ____ employs data cartography ____ while training a baseline model to obtain the variability and confidence of each sample. The training data is further mapped as easy, ambiguous or hard. The model is then retrained via an easy-to-hard curriculum. To boost performance, the method employs stratified sampling as well as a continuous function to map the data points, resulting in a method called Cart-Stra-CL++.

Different from related curriculum and anti-curriculum learning methods ____, we design an anti-curriculum strategy for the fine-tuning stage of pre-trained language models, leveraging knowledge about the downstream tasks. Moreover, our novel design leads to superior performance on a range of downstream tasks.
%