% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{algpseudocode}
\usepackage[ruled,vlined]{algorithm2e}

\usepackage{etoolbox} % For \patchcmd

\makeatletter
\patchcmd{\algocf@makecaption@ruled}{\hsize}{\textwidth}{}{} % Caption to stretch full text width
\patchcmd{\@algocf@start}{-1.5em}{0em}{}{} % For // to right margin
\makeatother 
\RestyleAlgo{ruled}
\usepackage{hyperref}
% Standard package includes
\usepackage{times}
\usepackage{latexsym}
 \usepackage{multirow} 
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets
\usepackage{comment}
% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Task-Informed Anti-Curriculum by Masking\\ 
Improves Downstream Performance on Text}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Andrei Jarca, Florinel Alin Croitoru \and {Radu Tudor Ionescu}\thanks{Corresponding author: \texttt{raducu.ionescu@gmail.com}.}\\
  University of Bucharest\\
 Bucharest, Romania}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
Masked language modeling has become a widely adopted unsupervised technique to pre-train language models. However, the process of selecting tokens for masking is random, and the percentage of masked tokens is typically fixed for the entire training process. In this paper, we propose to adjust the masking ratio and to decide which tokens to mask based on a novel task-informed anti-curriculum learning scheme. First, we harness task-specific knowledge about useful and harmful tokens in order to determine which tokens to mask. Second, we propose a cyclic decaying masking ratio, which corresponds to an anti-curriculum schedule (from hard to easy). We exemplify our novel task-informed anti-curriculum by masking (TIACBM) approach across three diverse downstream tasks: sentiment analysis, text classification by topic, and authorship attribution. Our findings suggest that TIACBM enhances the ability of the model to focus on key task-relevant features, contributing to statistically significant performance gains across tasks. We release our code at \url{https://github.com/JarcaAndrei/TIACBM}.
\end{abstract}

% Curriculum learning (CL) has been extensively studied as a methodology in machine learning, where models are progressively exposed to more complex data to enhance generalization and improve performance. In this paper, we explore the application of CL through a novel masking-based approach across three diverse text classification tasks: sentiment analysis, general text classification, and author identification. While CL has been primarily utilized for structured progression in training, the specific role of masking techniques in facilitating such a progression has yet to be fully explored. To address this gap, we analyze the impact of masking sentiment-bearing terms in sentiment analysis, content words in text classification, and stylistic markers such as punctuation and stop words in author identification. Each use case is examined to assess how the strategic removal of these features influences model robustness, generalization, and accuracy across tasks. 

\section{Introduction}

Nowadays, masked language modeling (MLM) \cite{Devlin-NAACL-2019} is one of the most popular frameworks used to pre-train language models, as it enables the use of vast amounts of unlabeled data. However, the process of selecting tokens for masking is generally based on random selection, while the percentage of masked tokens is typically fixed for the entire training process \cite{Wettig-ECAL-2023}. To the best of our knowledge, there are only two studies attempting to dynamically adapt the masking ratio \cite{Ankner-EACL-2024,Yang-ACL-2023}. These studies concur that the optimal schedule is to use a decaying masking ratio during training. Interestingly, we find that this observation is deeply connected to the curriculum learning paradigm. Curriculum learning is a training strategy formulated by \citet{Bengio-ICML-2009}, where neural models learn the data in a systematic manner, starting with easy samples and gradually adding more difficult samples as the learning progresses. Intuitively, using a higher masking ratio makes the learning task more difficult. Hence, employing a decaying masking ratio corresponds to an anti-curriculum strategy \cite{Liu-CVPR-2022, Soviany-IJCV-2022}. We discuss additional related works in Appendix \ref{sec:appendix1}.

In this paper, we further develop and explore anti-curriculum learning based on MLM to fine-tune a pre-trained model on downstream tasks. We propose a novel task-informed anti-curriculum by masking (TIACBM) scheme, which employs a cyclic decaying masking ratio and relies on task-specific knowledge to decide which tokens to mask. Our most important contribution is to harness task-specific knowledge about useful and harmful tokens in order to select tokens for masking. For example, in sentiment analysis, the masking probability of a token is determined based on its polarity scores recorded in SentiWordNet 3.0 \cite{Baccianella-LREC-2010}. In text categorization by topic and authorship attribution, masking a token is conditioned by its part-of-speech tag. While content words receive higher masking probabilities for text categorization by topic, function words and punctuation tokens are more likely to be masked in authorship attribution.

We conduct fine-tuning experiments on four datasets: SST-2 \cite{Socher-EMNLP-2013}, 20 Newsgroups \cite{Lang-ICML-1995}, Reuters-21578 \cite{Lewis-UCI-1987}, and PAN 2019 Cross-Domain Authorship Attribution \cite{Kestemont-CLEF-2019}. Our experiments cover a diverse set of downstream tasks, including sentiment analysis, text classification by topic, and authorship attribution. We compare with several baselines, including conventional fine-tuning and standard MLM, and state-of-the-art training strategies based on anti-curriculum learning \cite{Ankner-EACL-2024} as well as curriculum learning \cite{Poesina-ACL-2024}. The results show that our strategy outperforms its competitors across all datasets. Moreover, TIACBM brings statistically significant performance gains, showing that harnessing task knowledge to mask tokens during fine-tuning is beneficial for multiple downstream tasks.

In summary, our contribution is threefold:
\begin{itemize}
    \item \vspace{-0.2cm} We propose to leverage task-specific knowledge to determine the probability distribution used to mask input tokens in MLM.
    \item \vspace{-0.2cm} We introduce a cyclic decaying masking ratio that boosts performance over existing anti-curriculum learning strategies for MLM.
    \item \vspace{-0.2cm} We apply a dynamic MLM strategy to fine-tune a pre-trained model on downstream tasks, showing that MLM is not only beneficial for pre-training, but also for the fine-tuning stage.
\end{itemize}



% Thus, our contributions are two-fold:
% \begin{itemize}
%     \item \textbf{Masking-based Curriculum Learning:} We propose a novel curriculum learning approach where we progressively reduce the number of masked linguistic features (e.g., sentiment-related terms, content words, punctuation, and stop words) over training epochs, starting with a higher probability of masking. This approach forces the model to first learn the most relevant features of the task under more challenging conditions, without relying on easy-to-detect cues, thereby aligning with a hard-to-easy curriculum learning strategy.
    
%     %poate si asta nush
%     %\item \textbf{Sentiment and Attention-based Masking Strategy:} We introduce a masking strategy driven by sentiment and attention mechanisms, which prioritizes the masking of specific linguistic terms (such as sentiment-bearing words, content words, and stylistic markers).
    
%     \item \textbf{Empirical Evaluation Across Tasks:} We empirically demonstrate the effectiveness of our masking-based curriculum learning approach in sentiment analysis, general text classification, and author identification. We compare the proposed method to existing training regimes, showcasing its improvements in classification accuracy and generalization across diverse NLP tasks.
% \end{itemize}

\section{Method}

% To this end, we analyze the impact of CL in three different tasks: Sentiment Based CL (Senti-CL), Content Words CL (Content-CL), and Stylistic Words CL (Style-CL). 
% We thus hypothesize that a hard-to-easy curriculum design would be advantageous for fine-tuning. Therefore, we employ a higher masking ratio in the early epochs, anneal it, and cycle the schedule to avoid curriculum collapse.
% Afterwards, we prioritize masking highly subjective words for Senti-CL, content words, and stylistic words with a with high attention score. This is done by dynamically adjusting the masking ratio, thus less important words are masked with a smaller chance. 

% We construct a generic masking algorithm that is adjusted based on the three tasks, as seen in Algorithm \ref{TableAlg}. While the masking probabilities are set prior to training, the approach dynamically adjusts the probability of masking of each token, by leveraging their scores, depending on the task.

% Drawing on the findings of \citet{Ankner-EACL-2024} and \citet{Yang-ACL-2023}, who recommend a high-to-low masking approach for MLM training, 

We propose a novel task-informed anti-curriculum by masking to fine-tune pre-trained language models. Specifically, we employ a cyclic decaying masking ratio which encourages a progressive adaptation of the model over time. In addition, we harness task-specific knowledge to determine which tokens need to be masked, ensuring that the model focuses on the most relevant words in the sentence. By combining a dynamic masking ratio with selective token masking, our strategy can boost performance on complex downstream tasks.

Prior to the start of the training, we create a vector $\mathbf{r}=\{r_1 \geq \dots \geq r_K\} \in [0,1]^K$ containing $K$ masking ratios, which represents the anti-curriculum schedule. Note that $K \ll T$, where $T$ is the total number of training iterations. Thus, after every $K$ iterations, we reuse the masking ratios starting with $r_1$. In Algorithm~\ref{masking_alg}, we formally present how the masking is performed for a training text sample, given as a sequence of tokens $\mathbf{x}$. In the first step, we compute the number of tokens to be masked $N$, based on the sequence length and the masking ratio $r_t$ of the current training iteration $t$. In the second step, for each token $x_i$, we call a task-specific function to compute its importance, taking into account the token and the surrounding text. Additionally, in this step, we also normalize the importance scores to obtain probability values. Finally, in the third and last step, we build a categorical distribution from these probabilities and sample $N$ tokens to mask. We further describe and motivate the $\verb|task_relevance|$ function for each task.
%We also explain how the importance score is converted into a probability used for masking.

%Thus, we hypothesized that our tasks are suited for a hard to easy curriculum learning environment. 

% All three tasks have the same core principle at hand, starting with a higher masking percentage and annealing it. Further, the most important features for each task have a higher masking percentage at the start, thus training the model on harder samples at this step. Although the masking schedules in our case vary from task to task, they remain in a cycling and anti-curriculum fashion. This determined the model to not depend on a single type of feature and resulted in improved generalization and avoided overfitting. Moreover, this approach proved to increase robustness due to the constant cycle of high-to-low masking for the simple reason of not letting the model get biased by the initial masked features. Experiments on different non-cycling or easy-to-hard environments, caused the model to suffer from curriculum collapse, which empirically shows the advantages of the proposed methods.

\SetNlSty{textt}{}{.}
\SetCommentSty{mycommfont}
\begin{algorithm}[!t]
\small{
\caption{{TIACBM}}
\label{masking_alg}
\SetAlgoLined
\SetKwInOut{Input}{Input}  
\textbf{Input}: $\mathbf{x}$ -- training sequence of tokens;\\
$\mathbf{r} = \{r_1, \dots, r_K\}$ -- decaying masking ratio schedule;\\
$\verb|task_relevance|$ -- function that computes the task-specific importance of a word; \\
$t$ -- current training iteration.\\
\textbf{Output}: $\hat{\mathbf{x}}$ -- masked sequence of tokens. \\
{\color{blue}{\tcc{1: Determine the number of masked tokens.}}}\nl
$r_t \gets \mathbf{r}[t \mod K]$;\\
$N \gets \lfloor \left|\mathbf{x}\right| \cdot r_t \rfloor$;\\
{\color{blue}{\tcc{2: Compute the task-specific importance and normalize the values to obtain a probability distribution over all tokens.}}}\nl
$\mathbf{s} \gets \{s_i = \verb|task_relevance|(x_i, \mathbf{x}), \forall i=1, \dots, \left|\mathbf{x}\right| \}$;\\
$\mathbf{p} \gets \left\{ p_i =  \frac{s_i}{\Sigma_{j=1}^{\left|\mathbf{s}\right|} s_j}, \forall i=1, \dots, \left|\mathbf{s}\right|\right\}$\\
% $I \gets \verb|argsort|$$_{i=1, \dots ,\left|\mathbf{x}\right|}\{s_i\}$;{\color{blue}{\tcp{\hspace{0.01in}desc order}}}
{\color{blue}{\tcc{3: Mask the tokens.}}}\nl
$n \gets 0$;\\
$\hat{\mathbf{x}} \gets \mathbf{x}$; \\

\While{$n < N $}{
        $ i ~\sim \mathrm{Categorical}(\left|\mathbf{x}\right|, \mathbf{p})$;\\
        $\hat{x}_i \gets \mbox{[MASK]}$; \\
        $n \gets n + 1$;
}
        
\nl \textbf{return} $\hat{\mathbf{x}}$ 
}
\end{algorithm}
%------------------------------------------------------------------------


% \begin{comment}
% \begin{algorithm}
% \caption{Generic Masking Algorithm}
% \begin{algorithmic}[1]
% \FOR{each $idx_{pos}, token_{w}$ in $tokens$}
%     \IF{$token \notin \text{special\_tokens}$}
%         \STATE \textbf{(M1) Sentiment-based Masking}
%         \STATE \quad $p_e \gets p_e \times (2 - objectivity[token_w])$
        
%         \STATE \textbf{(M2-3) Word-based Masking}
%         \STATE \quad $p_e \gets p_e \times (1 + tok\_imp[idx_{pos}])$
%         \IF{$random() < p_e$}
%             \STATE $tokens[idx] \gets [MASK]$
%         \ENDIF
%     \ENDIF
% \ENDFOR
% \end{algorithmic}
% \end{algorithm}
% \end{comment}

% \begin{algorithm}[t]
% \caption{Task-Informed Anti-Curriculum by Masking}
% \label{TableAlg}
% \begin{algorithmic}[1]
% \STATE $p_e:$ Masking Probability at Current Epoch 
% \STATE $word_T:$ Chosen Word Based on Method 
% \FOR{each $i$ in $word\_token\_map[word_T]$}
%     \FOR{each $id$ in $i$}
%         \STATE \textbf{(M1) Sentiment-based Masking}
%         \STATE \quad $p_e \gets p_e \times (2 - objectivity[id])$
        
%         \STATE \textbf{(M2-3) Word-based Masking}
%         \STATE \quad $p_e \gets p_e \times (1 + tok\_imp[id])$
%         \IF{$random() < p_e$}
%             \STATE $tokens[id] \gets [MASK]$
%         \ENDIF
%     \ENDFOR
% \ENDFOR
% \end{algorithmic}
% \end{algorithm}


\paragraph{Sentiment analysis.}
For polarity classification, we hypothesize that the most subjective words represent the most important features, and masking them will result in a hard-to-easy curriculum. The core foundation of this approach lies in using the SentiWordNet 3.0 sentiment lexicon~\citep{Baccianella-LREC-2010}. For each sentence, we analyze the most probable synset of each word, using a generic Lesk algorithm, and search it in the lexicon. 
This process aims to determine the most likely positive $\left(s_{\text{pos}}\right)$ and negative scores $\left(s_{\text{neg}}\right)$ for the current word. Both scores range from 0 to 1, with higher values indicating stronger positive or negative connotations, respectively. We emphasize that these scores are linked together and their sum is lower or equal to $1$. Based on these values, \citet{Baccianella-LREC-2010} determine the objectivity score for each word as: 
%    \(\text{ObjScore} = 1 - (\text{PosScore} + \text{NegScore})\)
\begin{equation}
    \label{objectivity_score}
    o = 1 - (s_{\text{pos}} + s_{\text{neg}}).
\end{equation}
In contrast, we leverage the subjectivity score as an importance measure to form the vector $\mathbf{s}$ for a given input $\mathbf{x}$ in Algorithm~\ref{masking_alg}. Accordingly, we compute the importance score $s_i$ for each token $x_i$ as follows:
\begin{equation}
    \label{sent_importance_score}
    s_i =  (s_{\text{pos}}^i + s_{\text{neg}}^i), \forall i = 1,\dots, \left|\mathbf{x}\right|.
\end{equation}

% We then sort the words by their objectivity score, thus prioritizing the masking of the most subjective words first. Since the SST2 data set has short sentences and often times the number of found words in the lexicon varies, resulting in either a very high or very low number of words masked, regardless of probability, we balance this by limiting the possible total number of masked words as such: %\(\text{num\_to\_mask} = \lfloor 3 \cdot \text{total\_words} \cdot \text{mask\_prob} \rfloor \). 
% \begin{align*}
%     num\_to\_mask = 3 \cdot len(tokens) \cdot p_e  
% \end{align*}
% In the case of SST2, the best masking schedule is defined as:
% \begin{align*}
%     p_e = 0.15 - 0.03 \times (e \mod 3), e = epoch
% \end{align*}
% %\(\text{mask\_prob} = 0.15 - 0.03 \times (\text{epoch} \mod 3)\). 
% % Since the experiments are performed using AutoTokenizer and the tokens will be split, we also map each word with all the positions of all tokens as indexes. In another terms, all the tokens will be associated with the objectivity score of their pre-tokenized word.

% The cycling feature is essential to avoid curriculum collapse, while also taking into account the dynamic adjustment of the masking probability in Algorithm \ref{TableAlg}, which at most will double the percentage. We thus mask the most subjective tokens first and in the case we have not reached the maximum masked tokens, the method will continue masking ambiguous and then go to the most objective tokens. 

\paragraph{Text categorization.}
For text categorization, we hypothesize that content words (nouns, verbs, adjectives, adverbs, and proper names) are more relevant. Consequently, we assign an importance score of 0 to every other part of speech. To compute the importance scores for content words of a given sequence, we also draw upon the knowledge of the pre-trained language model. Before fine-tuning, we extract the attention weights from each attention block and each attention head, given by:
\begin{equation}
    \label{eq_attention_weights}
    A_b^h =  \mbox{softmax}\left(\frac{Q_b^h(K_b^h)^T}{\sqrt{d}}\right),
\end{equation}
where $b$ iterates over the self-attention blocks and $h$ iterates over the attention heads, $Q_l^h \in \mathbb{R}^{\left|\mathbf{x}\right| \times d}$ and $K_l^h \in \mathbb{R}^{\left|\mathbf{x}\right| \times d}$ are the query and key matrices of the attention block $b$ and head $h$. The result, $A_b^h \in \mathbb{R}^{\left|\mathbf{x}\right| \times \left|\mathbf{x}\right|}$, is a square matrix containing the similarity between each token found in the input sequence $\mathbf{x}$ and all the other tokens. We further compute an importance vector $\mathbf{a}$ by averaging the attention matrices, as follows:
\begin{equation}
    \label{cls_content_importance_score}
    \mathbf{a} = \frac{1}{B\cdot H \cdot \left|\mathbf{x}\right|} \sum_{h=1}^{H} \sum_{b=1}^B\sum_{j=1}^{\left|\mathbf{x}\right|}{A_{b,j}^h}, 
\end{equation}
where $B$ is the number of attention blocks and $H$ is the number of heads. The final importance scores $\mathbf{s}$ employed in Algorithm~\ref{masking_alg} are computed as:
\begin{equation}
    \label{authorship_importance_score}
    s_i\!=\!\begin{cases}
        a_i, \text{if } x_i \text{ is a content word}\!\!\!\!\\
        0,  \text{otherwise}
    \end{cases}
     \!\!, \forall i\!=\!1, ...,\!\left|\mathbf{x}\right|.
\end{equation}
% The chosen data sets are Reuters-21578 as Multi-Label Text Classification and 20NewsGroup for Text Classification. We postulate that content words are most significant in classification tasks. For the 20NewsGroup scenario, the part of speeches are gathered from spacy and only words strictly defined as content words (i.e. Nouns, Verbs, Adjectives, Adverbs, and Names) are mapped from words to tokens, similarly to \ref{subsubSENT}. Finally, only content words are masked in this case, while the probability percentage is dynamically adjusted based on the importance of the current content token, as seen in \ref{TableAlg}. Thus, the most important content words are masked with higher probability, resulting an a hard to easy schedule. 

% In the case of Reuters-21578, the approach remains the same, but we drop all words, except the main content words and perform the masking in this fashion. This is due to the high imbalance of the classes of Reuters.

% The ideal curriculum in both data sets remains the same to the Sentiment-Based CL [\ref{subsubSENT}], though in this case, cycling through $[0.35, 0.3, 0.25]$ or other anti-curriculum variants with higher differences between epochs proved to perform almost the same.

\paragraph{Authorship attribution.}
For the authorship attribution task, we compute the importance score vector $\mathbf{s}$ using a procedure similar to the one described for text classification. However, instead of using the content words for masking, we mask functional words, such as adpositions, determiners, conjunctions, symbols, particles, and punctuation. Authors exhibit consistent writing style patterns, which are reflected in their use of these functional words \cite{Kestemont-CLFL-2014}. Consequently, Eq.~\eqref{authorship_importance_score} is modified as follows:
\begin{equation}
    \label{cls_importance_score}
    s_i\!=\!\begin{cases}
        a_i, \text{if } x_i \text{ is a functional word}\!\!\!\!\\
        0,  \text{otherwise}
    \end{cases}
    \!\!, \forall i\!=\!1, ...,\!\left|\mathbf{x}\right|,
\end{equation}
where $a_i$ is computed as in Eq.~\eqref{cls_content_importance_score}.
% This approach is nearly identical to the Content Based CL, though on Stylistic Features. We do not drop any data, since PAN19 (Problem 1 and 5) on Author Identification is already quite small, while also masking only Adpositions, Determiners, Conjunctions, Coordinating Conjunctions, Subordinating Conjunctions, Symbols, Particles, and Punctuations. Authors have unique patterns, habits, and preferences that they consistently exhibit in writing. These subtleties are demonstrated by stylistic features.

% Since Bert tends to converge early for PAN19, we employ the following curriculum: $[0, 0, 0, \textbf{0.3}, \textbf{0}, \textbf{0.25}, 0.3, 0, 0.25 \dots]$. In this way, the baseline model usually converges in the first 2-3 epochs, allowing our approach to learn the most at the start, and later converging to the global minima. Intermittent no masking epochs are still needed in order to not completely overfit the model, since the train data is substantially smaller than the test set. de pus acelasi curriculum, dar fara 3 epoci de $0\%$ la inceput, pt roberta pan19 problem 1, sa verific si daca merge mai bine pe pan 19 prb 5 tot asa.

\section{Experiments and Results}

\subsection{Data sets}

We evaluate TIACBM on the three tasks, namely sentiment analysis, text categorization and authorship 
attribution. 

\noindent
\textbf{Reuters-21578.} Reuters-21578~\cite{Lewis-UCI-1987} is a multi-label text categorization dataset containing 12,449 training and 5,458 test instances. The dataset gathers documents from 90 categories.

\noindent
\textbf{20 Newsgroups.} 20 Newsgroups~\cite{Lang-ICML-1995} is a multi-class dataset for text categorization, which comprises 11,314 training instances and 7,532 test instances belonging to 20 classes. 

\noindent
\textbf{SST2.} The SST2 dataset \cite{Socher-EMNLP-2013} is a popular benchmark for sentiment analysis, comprising 67,349 training and 872 validation samples, which are labeled either as positive or negative.

\noindent
\textbf{PAN19.} For authorship attribution, we use the PAN19~\cite{Kestemont-CLEF-2019} dataset. We report results for Problem 0001 (P1) and Problem 0005 (P5). We discard the unknown files when reporting the metrics in our experiments. Both problems have 9 authors, with 7 training files each. There are 561 test files for P1, and 264 test files for P5. 

% For the sentiment use case, we also use the SentiWordNet 3.0 sentiment lexicon [\citet{Baccianella-LREC-2010}] to gather the objectivity score.

\begin{table*}[t]
  \centering
  \setlength\tabcolsep{0.1em}
  \small{
  \begin{tabular}{|c|c|c|c|c|c|c|}

  %   \hline
  %   \multirow{3}{*}{Model}     & \multirow{3}{*}{ \begin{tabular}{c}
  %   Fine-Tuning\\
  %   Strategy
  % \end{tabular}}     & \multicolumn{2}{|c|}{Text}  & Sentiment & \multicolumn{2}{|c|}{Authorship}   \\  
  %        &      &  \multicolumn{2}{|c|}{Categorization} &  Analysis  &  \multicolumn{2}{|c|}{Attribution} \\
  %        \cline{3-7}
  %        &  & Reuters-21578 & 20 News Group & SST2 & PAN19-P1 & PAN19-P5 \\
    \hline
      \multirow{2}{*}{Model} &Fine-Tuning  & \multicolumn{2}{|c|}{Micro F1-Score}&Accuracy&\multicolumn{2}{|c|}{Accuracy/Macro F1-Score}\\
     \cline{3-7}
     & Strategy& 
     % \multicolumn{2}{c}{Text Categorization} & Sentiment Analysis & \multicolumn{2}{c}{Authorship Attribution}
     Reuters & 20 News & SST2& PAN19-P1&
     PAN19-P5\\
    \hline
    \hline
    \multirow{7}{*}{BERT$_{\text{base}}$} 
       &
       Conventional & $90.61_{\pm0.28}$ &$84.63_{\pm0.28}$   & $93.38_{\pm0.14}$ & $69.76_{\pm15.11}/58.24_{\pm6.06}$&$66.10_{\pm1.56}$/$36.10_{\pm4.22}$\\

    &  Constant & $90.81_{\pm0.24}$& $84.98_{\pm0.12}$ & $93.94_{\pm0.15}$ & $63.70_{\pm9.96}/47.50_{\pm7.26}$&$65.76_{\pm2.92}/36.30_{\pm4.56}$\\
             
    &  \cite{Poesina-ACL-2024} & $90.72_{\pm0.13}$ & $82.30_{\pm0.25}$ & $94.00_{\pm0.14}$ &$55.23_{\pm3.48}/44.76_{\pm2.56}$&$68.66_{\pm2.00}/40.72_{\pm2.64}$\\
    
     &  \cite{Ankner-EACL-2024} & $90.99_{\pm0.05}$& $85.39_{\pm0.23}$ & $93.83_{\pm0.20}$ & $54.38_{\pm16.10}/46.03_{\pm8.53}$&$65.55_{\pm0.85}/36.60_{\pm4.28}$\\

    &  Cyclic Decaying & $90.96_{\pm0.12}$& $84.88_{\pm0.08}$ & $94.10_{\pm0.20}$&$50.56_{\pm15.22}/51.94_{\pm8.99}$&$69.28_{\pm3.27}/42.94_{\pm2.45}$\\

     &  TIACBM (ours) & $\mathbf{91.20}_{\pm0.20}$&  $\mathbf{85.65}_{\pm0.10}$& $\mathbf{94.61}_{\pm0.08}$ &$\mathbf{77.32}_{\pm9.33}/\mathbf{60.60}_{\pm7.37}$&$\mathbf{69.94}_{\pm1.98}/\mathbf{44.20}_{\pm2.67}$\\
    \hline
       \multirow{7}{*}{RoBERTa$_{\text{base}}$} &
       Conventional & $90.55_{\pm0.18}$ & $84.49_{\pm0.11}$ &$94.56_{\pm0.09}$ &$89.20_{\pm3.01}/76.92_{\pm4.64}$&$67.42_{\pm2.90}/38.30_{\pm4.32}$\\

     &  Constant &$90.49_{\pm0.11}$ &$85.10_{\pm0.30}$  &$94.88_{\pm0.26}$ &$92.44_{\pm0.51}/78.84_{\pm2.61}$&$64.00_{\pm4.33}/33.62_{\pm5.41}$\\


    &  \cite{Poesina-ACL-2024}  &$90.52_{\pm0.14}$  &$79.89_{\pm0.34}$ &$94.81_{\pm0.23}$&$90.18_{\pm1.03}/78.70_{\pm2.13}$&$65.16_{\pm1.26}/33.36_{\pm0.95}$\\
         
     &  \cite{Ankner-EACL-2024} &$90.42_{\pm0.09}$ &$85.33_{\pm0.17}$  &$94.24_{\pm0.19}$ &$91.76_{\pm1.53}/80.50_{\pm2.07}$&$65.10_{\pm0.49}/37.64_{\pm2.78}$\\

     &  Cyclic Decaying &$90.70_{\pm0.14}$ &$84.74_{\pm0.20}$ &$94.70_{\pm0.14}$ &$91.36_{\pm1.24}/78.84_{\pm2.21}$&$67.80_{\pm2.89}/39.36_{\pm2.63}$\\
    
     &  TIACBM (ours) &$\mathbf{91.06}_{\pm0.19}$ &$\mathbf{85.93}_{\pm0.18}$  &$\mathbf{95.04}_{\pm0.18}$ &$\mathbf{93.98}_{\pm0.70}/\mathbf{83.78}_{\pm2.55}$&$\mathbf{68.38}_{\pm1.53}/\mathbf{41.86}_{\pm2.15}$\\
    \hline
  \end{tabular}
  }\vspace{-0.25cm}
    \caption{Results on text classification (Reuters-21578, 20 Newsgroups), sentiment analysis (SST2) and authorship attribution (PAN19). Cochran's Q testing confirms that the results of TIACBM are always statistically better than conventional fine-tuning (p-value$<0.001$). Top scores for each architecture and metric are highlighted in bold.}
  \label{tab_strategies}
  \vspace{-0.4cm}
\end{table*}

\subsection{Baselines}
\label{experiments}

We compare TIACBM with five fine-tuning strategies, which are described in detail below.

\noindent
\textbf{Conventional.} This is the standard fine-tuning approach, which does not involve MLM. It uses the CB-NTR loss \cite{Huang-EMNLP-2021} for Reuters-21578, due to its long-tail distribution.

\noindent
\textbf{Constant.} This fine-tuning strategy uses a constant masking ratio to mask input tokens. The masking ratio is set to 15\%, following \citet{Devlin-NAACL-2019}.

\noindent
\textbf{Cart-Stra-CL++.} This is a state-of-the-art easy-to-hard curriculum approach introduced by \citet{Poesina-ACL-2024}. This method needs to perform data cartography for the baseline fine-tuned with the conventional regime, before employing the curriculum. This essentially doubles the training time.

\noindent
\textbf{Decaying Masking Ratio.} The decaying masking ratio, a.k,a.~anti-curriculum by masking, is proposed by \citet{Ankner-EACL-2024}. This training strategy can be seen as an ablated version of our approach, which is obtained by dropping the cyclical regime ($K=T$) and by discarding task-specific information.

\noindent
\textbf{Cyclic Decaying Masking Ratio.} This is an ablated version of our approach, which simply discards the task-specific information.

% We considered Cart-Stra-CL++ \cite{Poesina-ACL-2024} as the first baseline across all experiments.


% We train the same baseline model as ours to obtain the data cartography and applied Cart-CL++ in its easy-to-hard fashion. Since we train PAN19 on chunks due to the structure of the author-file relationship, we altered the process for Cart-CL++, by obtaining the cartography per chunk, and later training directly on the sorted chunks. We also adjust the number of epochs we used on our experiments to steps, by the ratios of Cart-CL++ (1:2:10), and our batch sizes. In the case of Cart-Stra-CL++, we employ a ratio of (1:2:3:4:15). For Reuters-21578, we further benchmark our method against CB-NTR \cite{Huang-EMNLP-2021} in addition to Cart-CL++. 

\subsection{Experimental Setup}
We employ two pre-trained language models,  BERT$_{\text{base}}$ and RoBERTa$_{\text{base}}$, in order to evaluate the various fine-tuning strategies.
%The experiments on the first two tasks are ran three times, while the last task is ran for five times.
We run all the experiments three times and we report the mean and standard deviation.
%All experiments are tested using a default environment, without performing hyper-parameter optimization.
% Learning rates are adjusted in cases where the baseline model overfits significantly, though all parameters remain the same over all experiments.
We run the fine-tuning for $15$ epochs for sentiment analysis, using a learning rate of $5\cdot 10^{-5}$, a batch size of $64$ and a max token length of $100$. For text categorization, we train for $30$ epochs, using a learning rate of $10^{-4}$, a batch size of $32$ and a max token length of $512$. For authorship attribution, we use $30$ epochs, a batch size of $8$ and a max token length of $512$. For PAN19-P1, we use a learning rate of $10^{-4}$ for BERT and $10^{-5}$ for RoBERTa. For PAN19-P5, we set the learning rate to $5\cdot 10^{-5}$ for both language models. 
% For RoBERTa on task classification, we employ a learning rate of $5*10^{-5}$, while the rest remain the same with BERT's. 
We use the cross-entropy loss for all datasets, except on Reuters-21578. In this case, the baseline language model is fine-tuned with the CB-NTR loss \cite{Huang-EMNLP-2021}. We keep the same loss for all fine-tuning strategies on Reuters-21578. In terms of optimizers, we use Adamax for the sentiment analysis task, and AdamW for the others. We keep these parameters consistent across all baseline methods and TIACBM. We release our code to reproduce the results at: \url{https://anonymous.4open.science/r/TIACBM-1234}.

% For the content/stylistic words based CL we use en\_core\_web\_lg from spacy to extract the part of speeches. We chose Adamax for the sentiment task and AdamW for all the rest. We employ CrossEntropyLoss for all experiments, except on Reuters-21578. In this case, we experiment using the CB-NTR loss criterion \citet{Huang-EMNLP-2021}, attempting to assess the performance of our approach on other state-of-the-art methods. 


% \begin{table}[H]
%   \centering
%   \begin{tabular}{lc}
%     \hline
%     \textbf{Parameter} & \textbf{SST2} \\
%     \hline
%     Epochs & 15 \\
%     LR & $5 \times 10^{-5}$ \\
%     Batch size & 64 \\
%     max\_len & 100 \\
%     \hline
%   \end{tabular}
%   \begin{tabular}{lc}
%     \hline
%     \textbf{Parameter} & \textbf{20News, Reuters-21578} \\
%     \hline
%     Epochs & 30 \\
%     LR & $1 \times 10^{-4}$  \\
%     Batch size & 32  \\
%     max\_len & 512 \\
%     \hline
%   \end{tabular}
%   \begin{tabular}{lc}
%     \hline
%     \textbf{Parameter} & \textbf{PAN19 - Problems 1 \& 5} \\
%     \hline
%     Epochs & 30 \\
%     LR & $1 \times 10^{-5}$ (P1)  \\
%     LR & $5 \times 10^{-5}$ (P5) \\
%     Batch size & 8  \\
%     max\_len & 512 \\
%     \hline
%   \end{tabular}
%   \caption{Parameters for Tasks with Different Learning Rates and Configurations.}
%   \label{tab:parameters}
% \end{table}

\subsection{Results}
% \vspace{-0.15cm}

We compare our approach against the competing fine-tuning strategies in Table~\ref{tab_strategies}. %Moreover, we also include in this comparison the models fine-tuned with the conventional training regime and those for which we select tokens for masking without relying on task-specific information. The latter are denoted by Random in Table~\ref{tab_strategies}. Additionally, we report the results on linear anti-curriculum (decaying) by 15\% on Reuters-21578, 20 News, SST2, and by 30\% on PAN19-P1 and PAN19-P5. We use identical percentages for the corresponding experiments with TIACBM.
Our methods consistently improve performance across downstream tasks, while exhibiting lower variability. Moreover, TIACBM brings consistent gains across both BERT and RoBERTa. On SST2, TIACBM provides an increase of $1.23\%$ over the baseline, and $0.61\%$ over the state-of-the-art method of \citet{Poesina-ACL-2024}, suggesting that masking the harder (subjective) words towards the easier (objective) words, in a cyclic fashion, improves the performance of the model on sentiment analysis. On Reuters-21578, our approach boosts the micro $F_1$ score by $0.59\%$, reaching a top result of $91.48\%$, surpassing the state-of-the-art model based on CB-NTR \cite{Huang-EMNLP-2021}. On 20 Newsgroups, TIACBM outperforms the baseline by $1.44\%$ and Cart-Stra-CL++ \cite{Poesina-ACL-2024} by $6.04\%$, highlighting the effectiveness of content word masking. For PAN19, TIACBM increases both accuracy and macro $F_1$ score by $2.36\%$ and $15.84\%$, respectively, demonstrating robust generalization even with limited training data.
Compared with the approach of \citet{Ankner-EACL-2024}, we observe that TIACM brings higher performance gains across all tasks, mainly due to the task-specific information harnessed by our approach.

% This is also seen from the experiments without SentiWordNet and uniform masking. 
%display considerable increases across all experiments, while maintain a lower standard deviation, thus providing validity through empirical analysis.
% Cart-CL++ maintains a higher accuracy on the SST2 data set, although it underperforms on the rest, indicating that for data sets with many labels, Cart-CL++ is not a good fit. 
% This can be attributed to large amounts of noise during cartography mapping.


% In the case of Reuters-21578, our model achieves a mean increase of $0.59\%$ for the Micro F1 Score. We also report our Top 1 result for Reuters-21578, as $91.48\%$, thus showing an increase over the state-of-the-art model (CB-NTR) for this data set. It should be noted that Content-CL and Cart-CL++ are used on top of CB-NTR, thus leveraging its loss function. For 20NewsGroup, Content-CL shows an increase of $1.02\%$ over the baseline and $3.35\%$ over Cart-CL++. Both results from Reuters-21578 and 20NewsGroup indicate the superiority of content word masking, compared to the state-of-the-art.

% For PAN19, Style-CL showcases increases between $5.02\%$ and $14.04\%$ over the baselines and Cart-CL++, across both Accuracy and Macro F1 Score. This indicates the ability of our approach to generalize well from a small training sample and avoiding overfitting. 

% \begin{table}[h]
%   \centering
%   \begin{tabular}{ll}
%     \hline
%     \textbf{Temp\_BERT} & \textbf{Accuracy / F1-score} \\
%     \hline
%     \multicolumn{2}{c}{\textbf{SST2 (Sentiment CL)}} \\
%     Conventional & 93.38 ± 0.14 / - \\
%     Cart-CL++ & 94.00 ± 0.14 / - \\
%     Without SentiWN & 94.10 ± 0.20 / - \\
%     Decaying 15\% & 93.83$\pm$0.2 / - \\
%     Uniform 15\% & 93.94 ± 0.15 / - \\
%     \textbf{Senti-CL (Ours)} & \textbf{94.61 ± 0.08} / - \\
%     \hline
%     \multicolumn{2}{c}{\textbf{Reuters-21578 (Content words CL)}} \\
%     CB-NTR & - / 90.61 ± 0.28 \\ 
%     Cart-CL++ & - / 90.34 ± 0.05 \\
%     Cart-Stra-CL++ & - / 90.72$\pm$0.13 \\
%     Mask Any Words& - / 90.96 ± 0.12 \\
%     Decaying 15\% & - / 90.99$\pm$0.05 \\
%     Uniform 15\% & - / 90.81 ± 0.24 \\
%     \textbf{Content-CL (Ours)} & - / \textbf{91.20 ± 0.20} \\
%     \hline
%     \multicolumn{2}{c}{\textbf{20NewsGroup (Content words CL)}} \\
%     Conventional & 84.63 ± 0.28  \\
%     Cart-CL++ & 82.30 ± 0.25 \\
%     Mask Any Words & 84.88 ± 0.08 \\
%     Decaying 15\% & 85.39$\pm$0.23 \\
%     Uniform 15\% & 84.98 ± 0.12 \\
%     \textbf{Content-CL (Ours)} & \textbf{85.65 ± 0.10} \\
%     \hline
%     \multicolumn{2}{c}{\textbf{PAN19 - Problem1 (Stylistic words CL)}} \\
%     Conventional & 80.6 $\pm$ 1.69$/$66.8 $\pm$ 3.37 \\
%     Cart-CL++ & 67.54$\pm$13.7 / 57.58$\pm$4.2 \\
%     Mask Any Words & 80.85$\pm$1.51 / 64.6$\pm$3.4 \\
%     Decaying 30\% & 67.88$\pm$6.89/58.1$\pm$2.23 \\
%     Uniform 30\% &  57.86$\pm$11.79$/$51.48$\pm$4.46 \\
%     \textbf{Style-CL (Ours)} & \textbf{85.62}$\pm$\textbf{3.55}$/$\textbf{71.62}$\pm$\textbf{1.88} \\
%     \hline
%     \multicolumn{2}{c}{\textbf{PAN19 - Problem5 (Stylistic words CL)}} \\
%     Conventional & 66.1 $\pm$ 1.56$/$36.1 $\pm$ 4.22 \\
%     Cart-CL++ & 68.66$\pm$2$/$40.72$\pm$2.64 \\
%     Mask Any Words & 69.28$\pm$3.27 / 42.94$\pm$2.45 \\ 
%     Decaying 30\% & 65.55$\pm$0.85 / 36.6$\pm$4.28 \\
%     Uniform 30\% & 65.76$\pm$2.92$/$36.3$\pm$4.56 \\
%     \textbf{Style-CL (Ours)} & \textbf{69.94}$\pm$\textbf{1.98}$/$\textbf{44.2}$\pm$\textbf{2.67} \\
%     \hline
%   \end{tabular}
%   \caption{\label{task-results} Performance results for each dataset and task. Values in bold represent the best result. PAN19 metrics are gathered by the Macro F1 score with the associated accuracy}
% \end{table}

% \begin{table}[h]
%   \centering
%   \begin{tabular}{ll}
%     \hline
%     \textbf{Temp\_Roberta} & \textbf{Accuracy / F1-score} \\
%     \hline
%     \multicolumn{2}{c}{\textbf{SST2 (Sentiment CL)}} \\
%     Conventional & 94.56 ± 0.09 /  \\
%     Cart-CL++ &  94.81 ± 0.23  / - \\
%     Without SentiWN &  94.7±0.14  / - \\
%     Decaying 15\% & 94.24$\pm$0.19 / - \\
%     Uniform 15\% &  94.88±0.26  / - \\
%     \textbf{Senti-CL (Ours)} & \textbf{95.04 ± 0.18} / \\
%     \hline
%     \multicolumn{2}{c}{\textbf{Reuters-21578 (Content words CL)}} \\
%     CB-NTR & - / 90.55 ± 0.18 \\ 
%     Cart-Stra-CL++ & - / 90.52$\pm$0.14 \\
%     Mask Any Words& - / 90.7±0.14  \\
%     Decaying 15\% & - / 90.42$\pm$0.09 \\
%     Uniform 15\% & - / 90.49 ± 0.11 \\
%     \textbf{Content-CL (Ours)} & - / \textbf{91.06 ± 0.19}  \\
%     \hline
%     \multicolumn{2}{c}{\textbf{20NewsGroup (Content words CL)}} \\
%     Conventional &  84.49$\pm$0.11 \\
%     Cart-CL++ & 79.89 ± 0.34  \\
%     Mask Any Words &  84.74$\pm$0.2 \\
%     Decaying 15\% & 85.33$\pm$0.17 \\
%     Uniform 15\% & 85.1$\pm$0.3  \\
%     \textbf{Content-CL (Ours)} & \textbf{85.93±0.18}  \\
%     \hline
%     \multicolumn{2}{c}{\textbf{PAN19 - Problem1 (Stylistic words CL) }} \\
%     Conventional &  89.2$\pm$3.01 $/$ 76.92$\pm$4.64  \\
%     Cart-CL++ & 90.18$\pm$1.03 / 78.7$\pm$2.13\\
%     Mask Any Words & 91.36$\pm$1.24 / 78.48$\pm$2.21 \\
%     Decaying 30\% & 91.76$\pm$1.53 / 80.5$\pm$2.07 \\
%     Uniform 30\% &  92.44$\pm$0.51$/$78.84$\pm$2.61 \\
%     \textbf{Style-CL (Ours)} & \textbf{93.98}$\pm$\textbf{0.7}$/$\textbf{83.78}$\pm$\textbf{2.55} \\
%     \hline
%     \multicolumn{2}{c}{\textbf{PAN19 - Problem5 (Stylistic words CL)}} \\
%     Conventional & 67.42$\pm$2.9 $/$ 38.3$\pm$4.32 \\
%     Cart-CL++ & 65.16$\pm$1.26$/$33.36$\pm$0.95 \\
%     Mask Any Words & 67.8$\pm$2.89 / 39.36$\pm$2.63 \\ 
%     Decaying 30\% & 65.1$\pm$0.49 / 37.64$\pm$2.78 \\
%     Uniform 30\% & 64$\pm$4.33$/$33.625$\pm$5.41 \\
%     \textbf{Style-CL (Ours)} & \textbf{68.38}$\pm$\textbf{1.53}$/$\textbf{41.86}$\pm$\textbf{2.15} \\
%     \hline
%   \end{tabular}
%   \caption{\label{task-results1} Performance results for each dataset and task. Values in bold represent the best result. PAN19 metrics are gathered by the Macro F1 score with the associated accuracy}
% \end{table}


\section{Conclusion}
\vspace{-0.2cm}
We proposed a novel task-informed anti-curriculum by masking approach (TIACBM), and we evaluated its effectiveness on three tasks: sentiment analysis, text categorization by topic, and authorship attribution. The proposed method leverages information about the downstream tasks to decide which tokens to select for masking in a novel anti-curriculum by masking framework.
On all the three tasks, our method achieved better results across all experiments, outperforming both baselines and state-of-the-art methods. Moreover, our method performed well on both multi-label and multi-class classification, while also proving resilience against imbalanced datasets, such as Reuters-21578. Additionally, we also showed that TIACBM is effective in scenarios with a low number of training samples, as it is the case of PAN19.

\section{Limitations} We present a novel method to consistently improve the performance of language models on downstream tasks. However, there is no universal anti-curriculum (masking ratio) schedule that can work for all models or data sets, representing an important parameter to be optimized by the user. Still, a general empirical statement proven in our work is that cycling anti-curriculum schedulers are superior in NLP, when it comes to curriculum by masking. Additionally, our method computes token relevance using a task-specific approach and this can be challenging to design for some tasks. We show on two tasks that language model attention weights can effectively serve this purpose. %Lastly, as with any data-level curriculum learning approach, an extra step is required during training. The additional computational time may be minimal or significant, depending on the task-specific method used.

\section{Ethics Statement}
To our knowledge, the proposed method poses no immediate risk. However, it can be adapted for generative modeling, which raises concerns about its potential misuse for malicious purposes, such as fake content generation.
% Moreover, as with any curriculum learning approach, there is a need to perform an additional step in the model's training process. Depending on the data set, masking tokens will lengthen the duration of training, though it will not directly interfere with the model's performance afterward. This, of course, is paramount in order to feed the model with data in an easy-to-hard or hard-to-easy manner. Thus, while there is a small margin for error when choosing an adequate masking schedule, and requiring extensive duration before training, our approach enhances curriculum learning by leveraging a novel paradigm (TIACBM), suitable in numerous NLP environments. 


\begin{comment}
\begin{table}
  \centering
  \begin{tabular}{lll}
    \hline
    \textbf{Data/Task} & \textbf{Method} & \textbf{Accuracy / F1-score } \\
    \hline
    \multirow{5}{*}{\shortstack{SST2 \\ Task 1}}  
      & Conv & 85.2 ± 1.3 / 84.5 ± 1.5 \\
    & W/o SWN & 84.9 ± 1.1 / 84.0 ± 1.2 \\
    & \citet{Poesina-ACL-2024}& 85.5 ± 1.4 / 84.8 ± 1.6 \\
    & Uniform & 86.0 ± 1.2 / 85.3 ± 1.3 \\
    & Ours & 84.7 ± 1.5 / 83.9 ± 1.4 \\
    \hline
    \multirow{5}{*}{\shortstack{Reuters \\ Task 2}}  
      & Method 1 & 78.5 ± 2.1 / 77.8 ± 2.3 \\
    & Method 2 & 79.0 ± 2.3 / 78.2 ± 2.1 \\
    & Method 3 & 77.8 ± 1.9 / 77.1 ± 2.0 \\
    & Method 4 & 78.9 ± 2.2 / 78.3 ± 2.4 \\
    & Method 5 & 78.3 ± 1.8 / 77.5 ± 1.9 \\
    \hline
    \multirow{5}{*}{\shortstack{20 News \\ Task 2}}  
      & Method 1 & 81.4 ± 1.8 / 80.7 ± 1.9 \\
    & Method 2 & 82.0 ± 1.6 / 81.3 ± 1.7 \\
    & Method 3 & 80.9 ± 1.7 / 80.2 ± 1.8 \\
    & Method 4 & 81.7 ± 1.9 / 81.1 ± 2.0 \\
    & Method 5 & 81.2 ± 1.5 / 80.5 ± 1.6 \\
    \hline
    \multirow{5}{*}{\shortstack{PAN19 \\ Task 3 (P1)}}  
      & Method 1 & 88.9 ± 1.2 / 87.6 ± 1.4 \\
    & Method 2 & 89.1 ± 1.1 / 88.0 ± 1.2 \\
    & Method 3 & 88.5 ± 1.3 / 87.2 ± 1.5 \\
    & Method 4 & 89.4 ± 1.0 / 88.3 ± 1.1 \\
    & Method 5 & 88.7 ± 1.2 / 87.5 ± 1.3 \\
    \hline
    \multirow{5}{*}{\shortstack{PAN19 \\ Task 3 (P5)}}  
      & Method 1 & 90.3 ± 0.9 / 89.8 ± 1.0 \\
    & Method 2 & 90.6 ± 0.8 / 90.1 ± 0.9 \\
    & Method 3 & 89.9 ± 1.0 / 89.4 ± 1.1 \\
    & Method 4 & 90.5 ± 0.9 / 90.0 ± 1.0 \\
    & Method 5 & 90.1 ± 0.8 / 89.6 ± 0.9 \\
    \hline
  \end{tabular}
  \caption{\label{task-results} Performance results for each dataset and task. Each dataset has five Accuracy / F1-score values corresponding to different methods, with mean ± standard deviation.}
\end{table}
\end{comment}





% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

\appendix

\section{Related Work}
\label{sec:appendix1}
\begin{table}[t!]
    \centering
    \begin{tabular}{|c| c| c|c|c|}
    \hline
         Model & Dataset & $r_1$& $r_K$& $K$ \\
         \hline
         \hline
         \multirow{5}{*}{BERT}& SST2 &0.15& 0.09& 3\\
             & Reuters & 0.15 & 0.09& 3\\
             & 20 News & 0.15 & 0.09& 3\\
              & PAN19-P1 & 0.35 & 0.0 & 3\\
              & PAN19-P2 & 0.3 & 0.0 & 3\\
        \hline
        \multirow{5}{*}{RoBERTa}& SST2 &0.15& 0.09& 3\\
             & Reuters & 0.15 & 0.09& 3\\
             & 20 News & 0.15 & 0.0 & 3\\
              & PAN19-P1 & 0.35 & 0.0 & 3\\
              & PAN19-P2 & 0.3 & 0.0 & 3\\
             \hline
         
    \end{tabular}
    \caption{Masking ratios used in our experiments.}
    \label{table_hyperparams}
\end{table}
\begin{table*}[t]
  \centering
  \setlength\tabcolsep{0.23em}
  \small{
  \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
      \multirow{2}{*}{Model} &Fine-Tuning  & \multicolumn{2}{|c|}{Micro F1-Score}&Accuracy&\multicolumn{2}{|c|}{Accuracy/Macro F1-Score}\\
     \cline{3-7}
     & Strategy& 
     % \multicolumn{2}{c}{Text Categorization} & Sentiment Analysis & \multicolumn{2}{c}{Authorship Attribution}
     Reuters & 20 News & SST2& PAN19-P1&
     PAN19-P5\\
     \hline
    \hline 
    \multirow{2}{*}{BERT$_{\text{base}}$}& TICBM & $90.53_{\pm0.15}$ & $85.38_{\pm0.08}$ & $94.03_{\pm0.16}$  & $64.43_{\pm3.19}/52.17_{\pm8.35}$ & $63.90_{\pm2.51} / 35.23_{\pm3.56}$\\
    & TIACBM & $\mathbf{91.20}_{\pm0.20}$ & $\mathbf{85.65}_{\pm0.10}$ & $\mathbf{94.61}_{\pm0.08}$ & $\mathbf{77.32}_{\pm9.33}/\mathbf{60.60}_{\pm7.37}$ &$\mathbf{69.94}_{\pm1.98}/\mathbf{44.2}_{\pm2.67}$ \\
    \hline
    
    \multirow{2}{*}{RoBERTa$_{\text{base}}$}& TICBM & $90.35_{\pm0.09}$ & $85.45_{\pm0.13}$ & $94.92_{\pm0.14}$& $90.93_{\pm1.22} / 78.83_{\pm0.98}$ &  $67.92_{\pm0.95} / 39.26_{\pm1.61}$ \\
    & TIACBM & $\mathbf{91.06}_{\pm0.19}$ & $\mathbf{85.93}_{\pm0.18}$& $\mathbf{95.04}_{\pm 0.18}$&$\mathbf{93.98}_{\pm0.70}/\mathbf{83.78}_{\pm2.55}$ & $\mathbf{68.38}_{\pm1.53}/\mathbf{41.86}_{\pm2.15}$\\
    % \multicolumn{2}{c}{\textbf{SST2 (Sentiment CL)}} \\
    % CBM-BERT &   94.03$\pm$0.16/ - \\
    % CBM-RoBERTa & 94.92$\pm$0.14/ - \\
    % \textbf{Senti-CL-BERT} & \textbf{94.61 ± 0.08} / - \\
    % \textbf{Senti-CL-RoBERTa} & \textbf{95.04 ± 0.18} / \\
    % \hline
    % \multicolumn{2}{c}{\textbf{Reuters-21578 (Content words CL)1h30}} \\
    % CBM-BERT &   - / 90.53$\pm$0.15 \\
    % CBM-RoBERTa & / 90.35$\pm$0.09 \\
    % \textbf{Content-CL-BERT} & - / \textbf{91.20 ± 0.2}  \\
    % \textbf{Content-CL-RoBERTa} & - / \textbf{91.06 ± 0.19}  \\
    % \hline
    % \multicolumn{2}{c}{\textbf{20NewsGroup (Content words CL)}} \\
    % CBM-BERT &   85.38$\pm$0.08/ - \\
    % CBM-RoBERTa & 85.45$\pm$0.13 / - \\
    % \textbf{Content-CL-BERT} & \textbf{85.65±0.1}  \\
    % \textbf{Content-CL-RoBERTa} & \textbf{85.93±0.18}  \\
    % \hline
    % \multicolumn{2}{c}{\textbf{PAN19 - Problem1 (Stylistic words CL) }} \\
    % CBM-BERT &   78.6$\pm$9.73 / 63.53$\pm$2.33 \\
    % CBM-RoBERTa & 90.93$\pm$1.22 / 78.83$\pm$0.98 \\
    % \textbf{Style-CL-BERT} & \textbf{85.62}$\pm$\textbf{3.55}$/$\textbf{71.62}$\pm$\textbf{1.88} \\
    % \textbf{Style-CL-RoBERTa} & \textbf{93.98}$\pm$\textbf{0.7}$/$\textbf{83.78}$\pm$\textbf{2.55} \\
    % \hline
    % \multicolumn{2}{c}{\textbf{PAN19 - Problem5 (Stylistic words CL)}} \\
    % CBM-BERT &  63.9$\pm$2.51  / 35.23$\pm$3.56 \\
    % CBM-RoBERTa & 67.92$\pm$0.95 / 39.26$\pm$1.61 \\
    % \textbf{Style-CL-BERT} & \textbf{69.94}$\pm$\textbf{1.98}$/$\textbf{44.2}$\pm$\textbf{2.67} \\
    % \textbf{Style-CL-RoBERTa} & \textbf{68.38}$\pm$\textbf{1.53}$/$\textbf{41.86}$\pm$\textbf{2.15} \\
    \hline
  \end{tabular}
  }
  \caption{\label{curriulum_vs_anticurriculum} Comparison between curriculum (easy-to-hard) and anti-curriculum (hard-to-easy) approaches. Both methods benefit from a cyclic schedule and task-specific information.}
\end{table*}
Our framework is mostly related to work on curriculum learning \cite{Bengio-ICML-2009}. \citet{Soviany-IJCV-2022} divide curriculum learning methods into data-level \cite{Chang-EACL-2021,Gong-CIKM-2021,Kocmi-RANLP-2017,Liu-IJCAI-2018,Nagatsuka-NGC-2023}, model-level \cite{Croitoru-IJCV-2025,Sinha-NeurIPS-2020}, task-level \cite{Liu-IJCAI-2020,Narvekar-AAMAS-2016}, and objective-level \cite{Pathak-ICMLA-2019} strategies. Most of existing studies focus on computer vision \cite{Croitoru-IJCV-2025,Huang-CVPR-2020,Sinha-NeurIPS-2020} and reinforcement learning \cite{Fang-NeurIPS-2019,Florensa-CoRL-2017}. Methods in these domains are vaguely related to our approach, with some exceptions that employ curriculum based on masked image modeling (MIM) \cite{Jarca-ECAI-2024,Madan-WACV-2024}. In the image domain, the MIM approach was explored from multiple perspectives, which led to the development of adaptive masking strategies based on curriculum learning \cite{Madan-WACV-2024}, that can produce more robust representations. A notable finding is that an easy-to-hard curriculum works generally well for image masking \cite{Jarca-ECAI-2024}. In contrast, analogous studies focusing on text \cite{Ankner-EACL-2024,Yang-ACL-2023} suggest that a hard-to-easy curriculum, i.e.~using a decaying masking ratio, is more appropriate for text. Our results confirm the observations of \citet{Ankner-EACL-2024} and \citet{Yang-ACL-2023}, although we reset the masking ratio during training, resulting in a cyclic decaying masking ratio.

Curriculum learning methods specifically designed for text \cite{Gong-CIKM-2021,Kocmi-RANLP-2017,Liu-IJCAI-2018,Liu-ACL-2020,Zhan-AAAI-2021} are also related to our work. Some of the most popular approaches rely on text length \cite{Nagatsuka-NGC-2023} and model competence \cite{Platanios-NAACL-2019} to organize the samples from easy to hard. Recent approaches are based on more complex strategies. For instance, the state-of-the-art curriculum learning method proposed by \citet{Poesina-ACL-2024} employs data cartography \cite{Swayamdipta-EMNLP-2020} while training a baseline model to obtain the variability and confidence of each sample. The training data is further mapped as easy, ambiguous or hard. The model is then retrained via an easy-to-hard curriculum. To boost performance, the method employs stratified sampling as well as a continuous function to map the data points, resulting in a method called Cart-Stra-CL++.

Different from related curriculum and anti-curriculum learning methods \cite{Ankner-EACL-2024,Poesina-ACL-2024,Yang-ACL-2023}, we design an anti-curriculum strategy for the fine-tuning stage of pre-trained language models, leveraging knowledge about the downstream tasks. Moreover, our novel design leads to superior performance on a range of downstream tasks.
% \section{}
\section{Masking Ratio Schedules}
We fix the decaying masking ratio schedules $\mathbf{r}= \{r_1, \dots, r_K\}$, that are employed in Algorithm~\ref{masking_alg}, through validation. For both BERT and RoBERTa, we mention the maximum and minimum masking ratios in Table~\ref{table_hyperparams}, as well as the length $K$. We cycle through the schedules every $K$ epochs. %, with the exception of cases with 0\% starting epochs (PAN19).

% sa verific oleaca
% BERT
% SST2 = [0.15, 0.12, 0.09]*3
% Reuters = [0.15, 0.12, 0.09]*10
% 20news = [0.15, 0.12, 0.09]*10
% PanP1 = [0,0,0,0.3,0,0.25,0.3,0,0.25...]
% PanP5 = [0,0,0,0.3,0,0.25,0.3,0,0.25...]

% Roberta
% SST2 = [0.15, 0.12, 0.09]*3
% Reuters = [0.15,0.12,0.09]*10
% 20News = \textbf{[0.15, 0.075, 0]}*10
% PanP1 = \textbf{[0.35,0,0.25]}*10
% PanP5 = [0,0,0,0.3,0,0.25,0.3,0,0.25...]

\section{Computational Resources}
The experiments are carried out on a machine with 64GB of RAM, an AMD Ryzen 7 7800x3d CPU, and an Nvidia GeForce RTX 4090 GPU. Our most expensive experiments are performed on 20 Newsgroups, with 112 \texttt{mins} (3 \texttt{mins} per epoch) for TIACBM, and about 180 \texttt{mins} for decaying runs, due to the need of masking at every epoch. The masking step takes between 2-3 \texttt{mins}. In the case of SST2, the experiments require 6 \texttt{mins} per epoch, while the masking step requires 0.5 \texttt{mins}. The timetable for PAN19 varies from 0.5 \texttt{mins} per epoch to 1 \texttt{mins} per epoch, regardless of the approach, while the masking takes roughly 3-4 \texttt{secs}. Finally, the masking step on Reuters-21578 takes between 1-3 \texttt{mins}, depending if non-content words need to be discarded or not. An average epoch takes 1.5 \texttt{mins}.

\section{Curriculum vs Anti-Curriculum}

In Table~\ref{curriulum_vs_anticurriculum}, we compare our anti-curriculum method (TIACBM) with a reversed masking ratio schedule, which implements an easy-to-hard curriculum learning. Both approaches benefit from a cyclic schedule and task-specific information. The anti-curriculum approach consistently outperforms its counterpart, validating our choice based on hard-to-easy curriculum.


\end{document}
