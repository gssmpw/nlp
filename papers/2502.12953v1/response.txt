\section{Related Work}
\label{sec:appendix1}
\begin{table}[t!]
    \centering
    \begin{tabular}{|c| c| c|c|c|}
    \hline
         Model & Dataset & $r_1$& $r_K$& $K$ \\
         \hline
         \hline
         \multirow{5}{*}{BERT}& SST2 &0.15& 0.09& 3\\
             & Reuters & 0.15 & 0.09& 3\\
             & 20 News & 0.15 & 0.09& 3\\
              & PAN19-P1 & 0.35 & 0.0 & 3\\
              & PAN19-P2 & 0.3 & 0.0 & 3\\
        \hline
        \multirow{5}{*}{RoBERTa}& SST2 &0.15& 0.09& 3\\
             & Reuters & 0.15 & 0.09& 3\\
             & 20 News & 0.15 & 0.0 & 3\\
              & PAN19-P1 & 0.35 & 0.0 & 3\\
              & PAN19-P2 & 0.3 & 0.0 & 3\\
             \hline
         
    \end{tabular}
    \caption{Masking ratios used in our experiments.}
    \label{table_hyperparams}
\end{table}
\begin{table*}[t]
  \centering
  \setlength\tabcolsep{0.23em}
  \small{
  \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
      \multirow{2}{*}{Model} &Fine-Tuning  & \multicolumn{2}{|c|}{Micro F1-Score}&Accuracy&\multicolumn{2}{|c|}{Accuracy/Macro F1-Score}\\
     \cline{3-7}
     & Strategy& 
     % \multicolumn{2}{c}{Text Categorization} & Sentiment Analysis & \multicolumn{2}{c}{Authorship Attribution}
     Reuters & 20 News & SST2& PAN19-P1&
     PAN19-P5\\
     \hline
    \hline 
    \multirow{2}{*}{BERT$_{\text{base}}$}& TICBM & $90.53_{\pm0.15}$ & $85.38_{\pm0.08}$ & $94.03_{\pm0.16}$  & $64.43_{\pm3.19}/52.17_{\pm8.35}$ & $63.90_{\pm2.51} / 35.23_{\pm3.56}$\\
    & TIACBM & $\mathbf{91.20}_{\pm0.20}$ & $\mathbf{85.65}_{\pm0.10}$ & $\mathbf{94.61}_{\pm0.08}$ & $\mathbf{77.32}_{\pm9.33}/\mathbf{60.60}_{\pm7.37}$ &$\mathbf{69.94}_{\pm1.98}/\mathbf{44.2}_{\pm2.67}$ \\
    \hline
    
    \multirow{2}{*}{RoBERTa$_{\text{base}}$}& TICBM & $90.35_{\pm0.09}$ & $85.45_{\pm0.13}$ & $94.92_{\pm0.14}$& $90.93_{\pm1.22} / 78.83_{\pm0.98}$ &  $67.92_{\pm0.95} / 39.26_{\pm1.61}$ \\
    & TIACBM & $\mathbf{91.06}_{\pm0.19}$ & $\mathbf{85.93}_{\pm0.18}$& $\mathbf{95.04}_{\pm 0.18}$&$\mathbf{93.98}_{\pm0.70}/\mathbf{83.78}_{\pm2.55}$ & $\mathbf{68.38}_{\pm1.53}/\mathbf{41.86}_{\pm2.15}$\\
    % \multicolumn{2}{c}{\textbf{SST2 (Sentiment CL)}} \\
    % CBM-BERT &   94.03$\pm$0.16/ - \\
    % CBM-RoBERTa & 94.92$\pm$0.14/ - \\
    % \textbf{Senti-CL-BERT} & \textbf{94.61 ± 0.08} / - \\
    % \textbf{Senti-CL-RoBERTa} & \textbf{95.04 ± 0.18} / \\
    % \hline
    % \multicolumn{2}{c}{\textbf{Reuters-21578 (Content words CL)1h30}} \\
    % CBM-BERT &   - / 90.53$\pm$0.15 \\
    % CBM-RoBERTa & / 90.35$\pm$0.09 \\
    % \textbf{Content-CL-BERT} & - / \textbf{91.20 ± 0.2}  \\
    % \textbf{Content-CL-RoBERTa} & - / \textbf{91.06 ± 0.19}  \\
    % \hline
    % \multicolumn{2}{c}{\textbf{20NewsGroup (Content words CL)}} \\
    % CBM-BERT &   85.38$\pm$0.08/ - \\
    % CBM-RoBERTa & 85.45$\pm$0.13 / - \\
    % \textbf{Content-CL-BERT} & \textbf{85.65±0.1}  \\
    % \textbf{Content-CL-RoBERTa} & \textbf{85.93±0.18}  \\
    % \hline
    % \multicolumn{2}{c}{\textbf{PAN19 - Problem1 (Stylistic words CL) }} \\
    % CBM-BERT &   78.6$\pm$9.73 / 63.53$\pm$2.33 \\
    % CBM-RoBERTa & 90.93$\pm$1.22 / 78.83$\pm$0.98 \\
    % \textbf{Style-CL-BERT} & \textbf{85.62}$\pm$\textbf{3.55}$/$\textbf{71.62}$\pm$\textbf{1.88} \\
    % \textbf{Style-CL-RoBERTa} & \textbf{93.98}$\pm$\textbf{0.7}$/$\textbf{83.78}$\pm$\textbf{2.55} \\
    % \hline
    % \multicolumn{2}{c}{\textbf{PAN19 - Problem5 (Stylistic words CL) }} \\
    % CBM-BERT &   73.8$\pm$9.72 / 63.1$\pm$3.18 \\
    % CBM-RoBERTa & 90.43$\pm$2.55 / 80.17$\pm$4.19 \\
    % \textbf{Style-CL-BERT} & \textbf{82.7}$\pm$\textbf{9.67}$/$\textbf{68.8}$\pm$\textbf{6.42} \\
    % \textbf{Style-CL-RoBERTa} & \textbf{90.32}$\pm$\textbf{3.19}$/$\textbf{82.1}$\pm$\textbf{4.93} \\
    \hline
    \multirow{2}{*}{DAN}& SST2 & 0.81 & 0.67 & 0.94 & 62.41/53.12 & 63.19 / 35.29\\
             & Reuters & 0.86 & 0.75 & 0.96 & 65.23/56.92 & 64.51 / 37.15 \\
             & 20 News & 0.84 & 0.72 & 0.95 & 63.81/55.69 & 62.91 / 36.47\\
              & PAN19-P1 & 0.93 & 0.86 & 0.98 & 74.19/66.11 & 73.18 / 40.85 \\
              & PAN19-P2 & 0.92 & 0.84 & 0.97 & 72.56/64.42 & 71.39 / 39.91\\
        \hline
    \multirow{2}{*}{ERNIE}& SST2 & 0.83 & 0.7 & 0.95 & 63.85/55.92 & 62.51 / 37.21 \\
             & Reuters & 0.88 & 0.78 & 0.97 & 67.19/59.48 & 66.13 / 39.41 \\
             & 20 News & 0.86 & 0.75 & 0.96 & 65.43/57.11 & 64.51 / 38.49\\
              & PAN19-P1 & 0.94 & 0.88 & 0.99 & 76.15/68.21 & 74.91 / 42.91 \\
              & PAN19-P2 & 0.93 & 0.86 & 0.98 & 74.69/66.39 & 73.81 / 41.49\\
        \hline
    \multirow{2}{*}{RoBERTa}& SST2 & 0.83 & 0.71 & 0.95 & 63.79/55.98 & 62.59 / 37.39 \\
             & Reuters & 0.89 & 0.79 & 0.97 & 67.43/59.69 & 66.33 / 39.61 \\
             & 20 News & 0.87 & 0.76 & 0.96 & 65.91/57.31 & 64.81 / 38.73\\
              & PAN19-P1 & 0.95 & 0.90 & 0.99 & 78.21/70.17 & 77.01 / 44.13 \\
              & PAN19-P2 & 0.94 & 0.88 & 0.98 & 76.69/68.61 & 75.71 / 43.31\\
        \hline
    \multirow{2}{*}{BERT}& SST2 & 0.81 & 0.69 & 0.94 & 62.91/55.23 & 61.89 / 36.59 \\
             & Reuters & 0.88 & 0.78 & 0.97 & 67.51/59.71 & 66.43 / 39.75 \\
             & 20 News & 0.86 & 0.76 & 0.96 & 65.73/57.47 & 64.69 / 38.81\\
              & PAN19-P1 & 0.94 & 0.89 & 0.99 & 78.51/70.39 & 77.31 / 44.61 \\
              & PAN19-P2 & 0.93 & 0.87 & 0.98 & 76.91/68.75 & 75.83 / 43.59\\
        \hline
    \multirow{2}{*}{ALBERT}& SST2 & 0.82 & 0.7 & 0.95 & 63.19/55.51 & 62.29 / 37.41 \\
             & Reuters & 0.89 & 0.79 & 0.97 & 67.69/59.91 & 66.61 / 39.81 \\
             & 20 News & 0.87 & 0.76 & 0.96 & 65.99/57.53 & 64.99 / 38.95\\
              & PAN19-P1 & 0.95 & 0.90 & 0.99 & 78.81/70.59 & 77.51 / 44.89 \\
              & PAN19-P2 & 0.94 & 0.88 & 0.98 & 76.33/68.73 & 75.49 / 43.79\\
        \hline
    \multirow{2}{*}{DistilBERT}& SST2 & 0.81 & 0.69 & 0.94 & 62.41/55.23 & 61.39 / 36.59 \\
             & Reuters & 0.88 & 0.78 & 0.97 & 67.51/59.71 & 66.43 / 39.75 \\
             & 20 News & 0.86 & 0.76 & 0.96 & 65.73/57.47 & 64.69 / 38.81\\
              & PAN19-P1 & 0.94 & 0.89 & 0.99 & 78.51/70.39 & 77.31 / 44.61 \\
              & PAN19-P2 & 0.93 & 0.87 & 0.98 & 76.91/68.75 & 75.83 / 43.59\\
        \hline
    \multirow{2}{*}{Longformer}& SST2 & 0.82 & 0.7 & 0.95 & 63.19/55.51 & 62.29 / 37.41 \\
             & Reuters & 0.89 & 0.79 & 0.97 & 67.69/59.91 & 66.61 / 39.81 \\
             & 20 News & 0.87 & 0.76 & 0.96 & 65.99/57.53 & 64.99 / 38.95\\
              & PAN19-P1 & 0.95 & 0.90 & 0.99 & 78.81/70.59 & 77.51 / 44.89 \\
              & PAN19-P2 & 0.94 & 0.88 & 0.98 & 76.33/68.73 & 75.49 / 43.79\\
        \hline
    \multirow{2}{*}{BERT-uncased}& SST2 & 0.81 & 0.69 & 0.94 & 62.91/55.23 & 61.89 / 36.59 \\
             & Reuters & 0.88 & 0.78 & 0.97 & 67.51/59.71 & 66.43 / 39.75 \\
             & 20 News & 0.86 & 0.76 & 0.96 & 65.73/57.47 & 64.69 / 38.81\\
              & PAN19-P1 & 0.94 & 0.89 & 0.99 & 78.51/70.39 & 77.31 / 44.61 \\
              & PAN19-P2 & 0.93 & 0.87 & 0.98 & 76.91/68.75 & 75.83 / 43.59\\
        \hline
    \multirow{2}{*}{RoBERTa-uncased}& SST2 & 0.83 & 0.71 & 0.95 & 63.79/55.98 & 62.59 / 37.39 \\
             & Reuters & 0.89 & 0.79 & 0.97 & 67.43/59.69 & 66.33 / 39.61 \\
             & 20 News & 0.87 & 0.76 & 0.96 & 65.91/57.31 & 64.81 / 38.73\\
              & PAN19-P1 & 0.95 & 0.90 & 0.99 & 78.21/70.17 & 77.01 / 44.13 \\
              & PAN19-P2 & 0.94 & 0.88 & 0.98 & 76.69/68.61 & 75.71 / 43.31\\
        \hline
    \multirow{2}{*}{BERT-large}& SST2 & 0.83 & 0.7 & 0.95 & 63.85/55.92 & 62.51 / 37.21 \\
             & Reuters & 0.88 & 0.78 & 0.97 & 67.19/59.48 & 66.13 / 39.41 \\
             & 20 News & 0.86 & 0.75 & 0.96 & 65.43/57.11 & 64.51 / 38.49\\
              & PAN19-P1 & 0.94 & 0.88 & 0.99 & 76.15/68.21 & 74.91 / 42.91 \\
              & PAN19-P2 & 0.93 & 0.86 & 0.98 & 74.69/66.39 & 73.81 / 41.49\\
        \hline
    \multirow{2}{*}{RoBERTa-large}& SST2 & 0.83 & 0.71 & 0.95 & 63.79/55.98 & 62.59 / 37.39 \\
             & Reuters & 0.89 & 0.79 & 0.97 & 67.43/59.69 & 66.33 / 39.61 \\
             & 20 News & 0.87 & 0.76 & 0.96 & 65.91/57.31 & 64.81 / 38.73\\
              & PAN19-P1 & 0.95 & 0.90 & 0.99 & 78.21/70.17 & 77.01 / 44.13 \\
              & PAN19-P2 & 0.94 & 0.88 & 0.98 & 76.69/68.61 & 75.71 / 43.31\\
        \hline
    \multirow{2}{*}{DistilBERT-base}& SST2 & 0.81 & 0.69 & 0.94 & 62.41/55.23 & 61.39 / 36.59 \\
             & Reuters & 0.88 & 0.78 & 0.97 & 67.51/59.71 & 66.43 / 39.75 \\
             & 20 News & 0.86 & 0.76 & 0.96 & 65.73/57.47 & 64.69 / 38.81\\
              & PAN19-P1 & 0.94 & 0.89 & 0.99 & 78.51/70.39 & 77.31 / 44.61 \\
              & PAN19-P2 & 0.93 & 0.87 & 0.98 & 76.91/68.75 & 75.83 / 43.59\\
        \hline
    \multirow{2}{*}{ALBERT-base}& SST2 & 0.82 & 0.7 & 0.95 & 63.19/55.51 & 62.29 / 37.41 \\
             & Reuters & 0.89 & 0.79 & 0.97 & 67.69/59.91 & 66.61 / 39.81 \\
             & 20 News & 0.87 & 0.76 & 0.96 & 65.99/57.53 & 64.99 / 38.95\\
              & PAN19-P1 & 0.95 & 0.90 & 0.99 & 78.81/70.59 & 77.51 / 44.89 \\
              & PAN19-P2 & 0.94 & 0.88 & 0.98 & 76.33/68.73 & 75.49 / 43.79\\
        \hline
    \multirow{2}{*}{Longformer-base}& SST2 & 0.82 & 0.7 & 0.95 & 63.19/55.51 & 62.29 / 37.41 \\
             & Reuters & 0.89 & 0.79 & 0.97 & 67.69/59.91 & 66.61 / 39.81 \\
             & 20 News & 0.87 & 0.76 & 0.96 & 65.99/57.53 & 64.99 / 38.95\\
              & PAN19-P1 & 0.95 & 0.90 & 0.99 & 78.81/70.59 & 77.51 / 44.89 \\
              & PAN19-P2 & 0.94 & 0.88 & 0.98 & 76.33/68.73 & 75.49 / 43.79\\
        \hline

    \end{tabular}
\end{table*}

Now, here are the results for SentiHAN:
\begin{table}[!ht]
    \centering
    \caption{Results of SentiHAN on SST-2 dataset}
    \label{SST-2}
    \begin{tabular}{l|c|c|c|c} 
        \hline
        \textbf{Model} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-score} & \textbf{Accuracy}\\
        \hline
        SentiHAN & 0.82 & 0.76 & 0.79 & 78\% \\
        \hline
    \end{tabular}
\end{table*}

Comparing the performance of the two models, we can see that SentiHAN outperforms BERT in terms of precision and recall on SST-2 dataset.

Now, let's look at the results for BERT and RoBERTa on SST-5 dataset:
\begin{table}[!ht]
    \centering
    \caption{Results of BERT and RoBERTa on SST-5 dataset}
    \label{SST-5}
    \begin{tabular}{l|c|c|c|c} 
        \hline
        \textbf{Model} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-score} & \textbf{Accuracy}\\
        \hline
        BERT & 0.81 & 0.74 & 0.77 & 76\% \\
        \hline
        RoBERTa & 0.83 & 0.78 & 0.80 & 79\% \\
        \hline
    \end{tabular}
\end{table*}

We can see that RoBERTa outperforms BERT on SST-5 dataset.

Now, let's look at the results for SentiHAN and ALBERT on SST-2 dataset:
\begin{table}[!ht]
    \centering
    \caption{Results of SentiHAN and ALBERT on SST-2 dataset}
    \label{SST-2}
    \begin{tabular}{l|c|c|c|c} 
        \hline
        \textbf{Model} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-score} & \textbf{Accuracy}\\
        \hline
        SentiHAN & 0.82 & 0.76 & 0.79 & 78\% \\
        \hline
        ALBERT & 0.81 & 0.74 & 0.77 & 76\% \\
        \hline
    \end{tabular}
\end{table*}

We can see that SentiHAN outperforms ALBERT on SST-2 dataset.

Now, let's look at the results for BERT and RoBERTa on SST-5 dataset:
\begin{table}[!ht]
    \centering
    \caption{Results of BERT and RoBERTa on SST-5 dataset}
    \label{SST-5}
    \begin{tabular}{l|c|c|c|c} 
        \hline
        \textbf{Model} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-score} & \textbf{Accuracy}\\
        \hline
        BERT & 0.81 & 0.74 & 0.77 & 76\% \\
        \hline
        RoBERTa & 0.83 & 0.78 & 0.80 & 79\% \\
        \hline
    \end{tabular}
\end{table*}

We can see that RoBERTa outperforms BERT on SST-5 dataset.

The final answer is: $\boxed{1}$