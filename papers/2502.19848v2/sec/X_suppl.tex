\clearpage
\maketitlesupplementary
\renewcommand\thesection{\Alph{section}}

\setcounter{page}{1}
\setcounter{section}{0}


\section{Experimental details}
\label{sec:details}
\noindent \textbf{Data pre-processing}. We employ the data pre-processing pipeline specified in DiAD \cite{diad} for both the MVTec \cite{MvTec} and VisA \cite{Visa} datasets to mitigate potential train-test discrepancies. This involves channel-wise standardization using precomputed mean $[0.485, 0.456, 0.406]$ and standard deviation $[0.229, 0.224, 0.225]$ after normalizing each RGB image to $[0, 1]$.


\noindent \textbf{Patch perturbation}. We adopt the method proposed by NSA \cite{NSA} for patch perturbation on the original images. The NSA method builds upon the Cut-paste technique \cite{CutPaste} and enhances it by incorporating the Poisson image editing method \cite{poisson} to alleviate the discontinuities caused by pasting image patches. The cut-paste method is commonly used in the anomaly detection domain to generate simulated anomalous images. It involves randomly cropping a patch from one image and pasting it onto a random location in another image, thus creating a simulated anomaly. The Poisson-based pasting method seamlessly blends the cloned object from one image into another by solving Poisson partial differential equations, thereby better simulating a realistic anomalous region. In this paper, the number of patches is set as a random value from $1$ to $4$, and the patch size is a random value from $0.03$ to $0.4$ of the original image size. The visualization of patch perturbation is shown in Figure \ref{nsa}.

\begin{figure}[h!]
  \centering
   \includegraphics[width=1\linewidth]{sec/fig/nsa.pdf}
   \caption{Qualitative results of logical anomaly detection.}
   \label{nsa}

\end{figure}
% \vspace{0.5\baselineskip}
\noindent \textbf{Evaluation metrics.} We follow the literature \cite{MvTec} in reporting the Area Under the Receiver Operation Characteristic (AUROC) for both image-level and pixel-level anomaly detection. To measure the performance of the model in continuous learning, referring to DNE \cite{dne}, we calculated the average AUROC (A-AUROC) and the forgetting measure (FM) for $N$ continual steps. Specially, we define $A_{N, i}^{\mathrm{pix}}$ and $A_{N, i}^{\mathrm{img}}$ as the test AUROC of task $i$ after training on task $N$.
\begin{equation}
    \mathrm{A\textbf{-}AUROC}=\left\{\begin{array}{l}
    \frac{1}{N} \sum_{i=1}^{N-1} A_{N, i}^{\mathrm{pix}} \\
    \frac{1}{N} \sum_{i=1}^{N-1} A_{N, i}^{\mathrm{img}}
    \end{array},\right.
    \label{a-auroc}
\end{equation}
% \vspace{-8pt}
\begin{equation}
    \mathrm{FM}=\left\{\begin{array}{l}
    \frac{1}{N-1} \sum_{i=1}^{N-1} \max _{b \in\{1, \cdots, N-1\}}\left(A_{b, i}^{\mathrm{pix}}-A_{N, i}^{\mathrm{pix}}\right) \\
    \frac{1}{N-1} \sum_{i=1}^{N-1} \max _{b \in\{1, \cdots, N-1\}}\left(A_{b, i}^{\mathrm{img}}-A_{N, i}^{\mathrm{img}}\right)
    \end{array}\right.
    \label{fm}
\end{equation}
In addition to the results for the AUROC documented in the body of the paper, We also supplement the image-level Precision-Recall (AUPR) results and pixel-level Per-region-overlap (PRO) \cite{MvTec, UniS-T} results. Referring to Equation (\ref{a-auroc}) and Equation (\ref{fm}), we calculate A-AUPR, A-PRO, and their FM to evaluate our method. The results are shown in Table \ref{mvtec_aupr} \ref{mvtec_pro} \ref{visa_aupr} \ref{visa_pro}. Our method still achieves an advanced level in the above metrics.
\section{Memory Analysis of iSVD}
\label{sec:details}
In Section 3.2, considering the storage of the original matrix and $\boldsymbol{U}, \boldsymbol{\Sigma}, \boldsymbol{V}$ during SVD, the memory overhead of SVD is $d\Lambda + d^2+\Lambda^2+\min(d, \Lambda)$, while iSVD uses a memory overhead of $d(m+k) + d^2 + (m+k)^2 + \min(d, m+k)$. It is known that $\Lambda \gg d$, $m > d > k$ and $\Lambda=mn$,  thus, the memory saving rate of iSVD is about:
\begin{equation}
    \begin{aligned}
       & \frac{d\Lambda + d^2+\Lambda^2+d - [d(m+k) + d^2 + (m+k)^2 + d]}{d\Lambda + d^2+\Lambda^2+d} \\
      = & \frac{d\Lambda +\Lambda^2 - d(m+k) - (m+k)^2 }{\Lambda^2}/\frac{d\Lambda + d^2+\Lambda^2+d}{\Lambda^2} \\
      = & \frac{d}{\Lambda} + 1 - \frac{d(m+k)}{\Lambda^2} - \frac{(m+k)^2}{\Lambda^2} \\
      \approx & 1-\frac{m^2+2mk+k^2}{m^2n^2} \\
      = & 1 - \frac{1}{n^2} - \frac{2k}{m\Lambda} - \frac{k^2}{\Lambda^2}
      \approx \frac{n^2-1}{n^2}.
    \end{aligned}
\label{eq:amn}
\end{equation} 
In practice, the actual memory saving rate differs from the theoretical value due to factors such as memory sharing. Taking the intermediate features of ten images as an example, Figure \ref{numblocks} shows the actual and theoretical memory saving rate of splitting the feature matrix into $n$ blocks for iSVD. Although there are some differences between the theoretical value and the actual value, the general trend is consistent.

\begin{table*}[]
\centering
\scalebox{0.94}{
\setlength{\tabcolsep}{1.8mm}
\begin{tabular}{c|cc|cc|cc|cc}
\toprule
\multicolumn{1}{c|}{\multirow{2}{*}{Method}}             & \multicolumn{2}{c|}{\textbf{14 -- 1 with 1 Step}} & \multicolumn{2}{c|}{\textbf{10 -- 5 with 1 Step}} & \multicolumn{2}{c|}{\textbf{3 × 5 with 5 Steps}} & \multicolumn{2}{c}{\textbf{10 -- 1 × 5 with 5 Steps}} \\ \cmidrule{2-9} 
                              & \textbf{A-AUPR ($\uparrow$)}            & \textbf{FM ($\downarrow$)}                 & \textbf{A-AUPR ($\uparrow$)}              & \textbf{FM ($\downarrow$)}                 & \textbf{A-AUPR ($\uparrow$)}              & \textbf{FM ($\downarrow$)}                 & \textbf{A-AUPR ($\uparrow$)}                & \textbf{FM ($\downarrow$)}                   \\ \midrule

UCAD* \cite{ucad}                    & 95.8          & 0.26          & 95.0            & {\ul 0.98}    & {\ul 93.1}      & {\ul 2.02}    & \underline{95.5} & {\ul 0.07}    \\
IUF \cite{iuf}                     & {\ul 97.8}    & {\ul 0.25}    & 95.4          & 1.92          & 91.1          & 2.86          & 95.3          & 0.16          \\
ControlNet \cite{controlnet}               & 97.2          & 1.55          & {\ul 96.7}    & 1.76          & 86.7          & 6.40           & 89.0            & 7.43          \\
DiAD \cite{diad}                     & 97.4          & 0.71          & 96.4          & 1.85          & 89.1          & 4.31          & 91.4          & 4.83          \\ \midrule

\rowcolor[RGB]{230,230,230}
\textbf{CDAD}                     & \textbf{98.4} & \textbf{0.08} & \textbf{98.3} & \textbf{0.55} & \textbf{95.8} & \textbf{1.88} & \textbf{98.4} & \textbf{0.02}          \\ \bottomrule
\end{tabular}
}
\caption{Image-level A-AUPR of our method on MVTec under 4 continual anomaly detection settings. The best and second-best results are marked in \textbf{blod} and \underline{underline}. $*$ indicates memory limited.}
\label{mvtec_aupr}
\end{table*}

\begin{table*}[]
\centering
\scalebox{0.94}{
\setlength{\tabcolsep}{2.2mm}
\begin{tabular}{c|cc|cc|cc|cc}
\toprule
\multicolumn{1}{c|}{\multirow{2}{*}{Method}}             & \multicolumn{2}{c|}{\textbf{14 -- 1 with 1 Step}} & \multicolumn{2}{c|}{\textbf{10 -- 5 with 1 Step}} & \multicolumn{2}{c|}{\textbf{3 × 5 with 5 Steps}} & \multicolumn{2}{c}{\textbf{10 -- 1 × 5 with 5 Steps}} \\ \cmidrule{2-9} 
                              & \textbf{A-PRO ($\uparrow$)}            & \textbf{FM ($\downarrow$)}                 & \textbf{A-PRO ($\uparrow$)}              & \textbf{FM ($\downarrow$)}                 & \textbf{A-PRO ($\uparrow$)}              & \textbf{FM ($\downarrow$)}                 & \textbf{A-PRO ($\uparrow$)}                & \textbf{FM ($\downarrow$)}                   \\ \midrule

UCAD \cite{ucad}                    & 86.3          & 1.16          & 80.7          & {\ul 2.89}    & { 71.1}    & { 7.48}          & {80.8} & {\ul 1.19}    \\
IUF \cite{iuf}                     & { 88.6}    & {\ul 0.62}    & 85.0            & 3.22          & \underline{72.9} & {\ul 5.79}          & {\ul 84.3}    & 2.41          \\
ControlNet \cite{controlnet}               & 88.5          & 1.75          & { 85.8}    & 4.70           & 71.5          & 10.0                  & 77.9          & 7.11          \\
DiAD \cite{diad}                    & {\ul 88.9}    & 0.85          & {\ul 87.4}    & 3.91          & 72.1          & 9.23                & 83.1          & 2.93          \\ \midrule

\rowcolor[RGB]{230,230,230}
\textbf{CDAD}                     & \textbf{89.8} & \textbf{0.47} & \textbf{88.9} & \textbf{2.57} & \textbf{83.8} & { \textbf{4.05}} & \textbf{89.2} & \textbf{1.16}            \\ \bottomrule
\end{tabular}
}
\caption{Pixel-level A-PRO of our method on MVTec under 4 continual anomaly detection settings. The best and second-best results are marked in \textbf{blod} and \underline{underline}. $*$ indicates memory limited.}
\vspace{-10pt}
\label{mvtec_pro}
\end{table*}

\begin{figure}[]
  \centering
   \includegraphics[width=1\linewidth]{sec/fig/memory.pdf}
   \caption{The theoretical and actual values of the memory saving ratio when the different number of split blocks is used.}
   \vspace{-10pt}
   \label{numblocks}
\end{figure}

In this paper, we determine the number of blocks $n$ according to the number of images of the old task. Specifically, we first sample the old task dataset, randomly retain 10\% of the images, and then group according to the number of sampled images (denoted as $N_{img}$). The number of groups in iSVD is $n=\frac{N_{img}}{e}$. In this paper, $e$ is set to $1$ by default, that is, the intermediate features of each image are divided into a separate group for iSVD operation. 

\begin{table}[h]
\begin{tabular}{c|cc|cc}
\toprule
    & \textbf{A-AUROC}     & \textbf{FM}         & \textbf{Memory} & \textbf{Times} \\ \midrule
e=1 & 94.2 / 95.3 & 2.05 / 2.40  & 16.7GB & 33.5h \\
e=2 & 94.1 / 95.4 & 2.10 / 2.32 & 30.3GB & 28.3h \\
e=4 & 94.4 / 95.6 & 1.95 / 2.21 & 57.9GB & 22.6h \\
e=6 & 94.5 / 95.7 & 1.91 / 2.23 & 85.9GB & 20.2h \\ \bottomrule
\end{tabular}
   \caption{The impact of different $e$ on the model and the time and memory overhead of iSVD under MVTec Setting 2.}
   \vspace{-10pt}
   \label{mem-times}
\end{table}

In addition, we analyze the impact of different $e$ on the model and the time and memory overhead. Table \ref{mem-times} records, for setting different $e$, the anomaly detection results of our model on MVTec setting 2, as well as the time and memory overhead for computing the significant representation of the old task. When $e$ is set to different values, the anomaly detection results and forgetting rate of the model will not have much influence. Although we discussed in Section 4.5 that the large number of split blocks will affect the performance of iSVD, it will not impair its representation ability of core information, so it can still ensure the continuous learning ability of the model. Table \ref{mem-times} also shows that with the increase of $e$, the memory consumption increases, but the time cost decreases, which indicates that although iSVD can greatly alleviate the pressure of memory, it will bring extra time cost.

\section{Qualitative Results}
\label{sec:details-quali}
We supplement the qualitative results on MVTec and VisA datasets, which show the localization image reconstruction results and anomaly localization results for the seven tasks, respectively, as shown in Figure \ref{qua1}-\ref{qua7}. Our method not only overcomes the ``faithfulness hallucination'' problem of the diffusion model but also shows excellent anomaly localization results.


\begin{table*}[]
\centering
\scalebox{0.9}{
\setlength{\tabcolsep}{5mm}
\begin{tabular}{c|cc|cc|cc}
\toprule
                      \multicolumn{1}{c|}{\multirow{2}{*}{Method}}      & \multicolumn{2}{c|}{\textbf{11 -- 1 with 1 Step}} & \multicolumn{2}{c|}{\textbf{8 -- 4 with 1 Step}} & \multicolumn{2}{c}{\textbf{8 -- 1 × 4 with 4 Steps}} \\ \cmidrule{2-7} 
\multirow{-2}{*}{\textbf{}} & \textbf{A-AUROC ($\uparrow$)}                 & \textbf{FM ($\downarrow$)}                     & \textbf{A-AUROC ($\uparrow$)}                 & \textbf{FM ($\downarrow$)}                      & \textbf{A-AUROC ($\uparrow$)}                    & \textbf{FM ($\downarrow$)}                       \\ \midrule
UCAD* \cite{ucad}                     & { 88.1}                 & 0.29                   & { 83.2}                 & {\ul 5.17}             & { 82.9}                 & {\ul 2.16}             \\
IUF \cite{iuf}                      & \textbf{91.6}              & {\ul -0.04}            & {\ul 83.4}                 & 7.51                   & {\ul 83.0}                   & 6.87                   \\
ControlNet \cite{controlnet}              & 85.2                       & 2.38                   & 78.8                       & { 6.25}             & 72.4                       & 4.56                   \\
DiAD \cite{diad}                    & 74.9                       & 5.42                   & 70.1                       & 12.29                  & 59.5                       & 9.55                   \\ \midrule
\rowcolor[RGB]{230,230,230}
\textbf{CDAD}                     & {\ul {89.4}}        & \textbf{-0.77}         & \textbf{85.3}              & \textbf{3.1}           & \textbf{84.7}              & \textbf{1.83}          \\ \bottomrule
\end{tabular}
}
\caption{Image-level A-AUPR of our method on VisA under 3 continual anomaly detection settings. The best and second-best results are marked in \textbf{blod} and \underline{underline}. $*$ indicates memory limited.}
\label{visa_aupr}
\end{table*}



\begin{table*}[]
\centering
\scalebox{0.9}{
\setlength{\tabcolsep}{6mm}
\begin{tabular}{c|cc|cc|cc}
\toprule
                      \multicolumn{1}{c|}{\multirow{2}{*}{Method}}      & \multicolumn{2}{c|}{\textbf{11 -- 1 with 1 Step}} & \multicolumn{2}{c|}{\textbf{8 -- 4 with 1 Step}} & \multicolumn{2}{c}{\textbf{8 -- 1 × 4 with 4 Steps}} \\ \cmidrule{2-7} 
\multirow{-2}{*}{\textbf{}} & \textbf{A-PRO ($\uparrow$)}                 & \textbf{FM ($\downarrow$)}                     & \textbf{A-PRO ($\uparrow$)}                 & \textbf{FM ($\downarrow$)}                      & \textbf{A-PRO ($\uparrow$)}                    & \textbf{FM ($\downarrow$)}                       \\ \midrule
UCAD* \cite{ucad}                     & {\ul 80.4}                & 2.02                   & {\ul 72.4}                & 7.46                   & {\ul 70.5}                & {\ul 9.83}             \\
IUF \cite{iuf}                     & 82.0                        & {\ul 1.04}             & 63.9                      & 20.8                   & 57.0                        & 23.95                  \\
ControlNet \cite{controlnet}               & 62.3                      & 2.45                   & 61.0                        & {\ul 1.81}             & 51.7                      & 10.38                  \\
DiAD \cite{diad}                    & 69.9                      & 4.33                   & 67.7                      & 8.29                   & 55.0                        & 11.74                  \\ \midrule

\rowcolor[RGB]{230,230,230}
\textbf{CDAD}                     & \textbf{81.6}             & \textbf{-0.22}         & \textbf{78.9}             & \textbf{1.59}          & \textbf{77.7}             & \textbf{1.66}         \\ \bottomrule
\end{tabular}
}
\caption{Pixel-level A-PRO of our method on VisA under 3 continual anomaly detection settings. The best and second-best results are marked in \textbf{blod} and \underline{underline}. $*$ indicates memory limited.}
\label{visa_pro}
\end{table*}


\begin{figure*}[h!]
  \centering
   \includegraphics[width=1.0\linewidth]{sec/fig/vis_mvtec_setting1.pdf}
    
   \caption{Qualitative comparison results under setting 1 of MVTec, the numbers represent continual training classes.}
   \label{qua1}
\end{figure*}


\begin{figure*}[h!]
  \centering
   \includegraphics[width=1.0\linewidth]{sec/fig/vis_mvtec_setting2.pdf}
    
   \caption{Qualitative comparison results under setting 2 of MVTec, the numbers represent continual training classes.}
   \label{qua2}
\end{figure*}


\begin{figure*}[h!]
  \centering
   \includegraphics[width=1.0\linewidth]{sec/fig/vis_mvtec_setting3.pdf}
    
   \caption{Qualitative comparison results under setting 3 of MVTec, the numbers represent continual training classes.}
   \label{qua3}
\end{figure*}


\begin{figure*}[h!]
  \centering
   \includegraphics[width=1.0\linewidth]{sec/fig/vis_mvtec_setting4.pdf}
    
   \caption{Qualitative comparison results under setting 4 of MVTec, the numbers represent continual training classes.}
   \label{qua4}
\end{figure*}

\begin{figure*}[h!]
  \centering
   \includegraphics[width=1.0\linewidth]{sec/fig/vis_visa_setting1.pdf}
    
   \caption{Qualitative comparison results under setting 5 of VisA, the numbers represent continual training classes.}
   \vspace{50pt}
   \label{qua5}
\end{figure*}


\begin{figure*}[h!]
  \centering
   \includegraphics[width=1.0\linewidth]{sec/fig/vis_visa_setting2.pdf}
    
   \caption{Qualitative comparison results under setting 6 of VisA, the numbers represent continual training classes.}
   \label{qua6}
\end{figure*}


\begin{figure*}[h!]
  \centering
   \includegraphics[width=1.0\linewidth]{sec/fig/vis_visa_setting3.pdf}
    
   \caption{Qualitative comparison results under setting 7 of VisA, the numbers represent continual training classes.}
   \label{qua7}
\end{figure*}
