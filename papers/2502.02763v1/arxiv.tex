%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}
\pdfoutput=1

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
%\usepackage{subfig}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
%\usepackage{graphicx}
%\usepackage{cite}
%\usepackage{subfig}
%\usepackage{booktabs}
\usepackage{xcolor}

% Attempt to make hyperref and algorithmic work together better:
%\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{arxiv}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Rethinking Vision Transformer for Object Centric Foundation Models}

\begin{document}

\twocolumn[
\icmltitle{Rethinking Vision Transformer for Object Centric Foundation Models}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Manuel Traub}{cogmod}
\icmlauthor{Martin V. Butz}{cogmod}
\end{icmlauthorlist}

\icmlaffiliation{cogmod}{Cognitive Modeling, Department of Computer Science and Department of Psychology,
University of Tübingen
Sand 14, 72076 Tübingen, Germany}


\icmlcorrespondingauthor{Manuel Traub}{manuel.traub@uni-tuebingen.de}


% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Recent state-of-the-art object segmentation mechanisms, such as the Segment Anything Model (SAM) and FastSAM, first encode the full image over several layers and then focus on generating the mask for one particular object or area.
We present an off-grid Fovea-Like Input Patching (FLIP) approach, which selects image input and encodes it from the beginning in an object-focused manner.
While doing so, it separates locational encoding from an object-centric perceptual code.
FLIP is more data-efficient and yields improved segmentation performance when masking relatively small objects in high-resolution visual scenes.
On standard benchmarks such as Hypersim, KITTI-360, and OpenImages, FLIP achieves Intersection over Union (IoU) scores that approach the performance of SAM with much less compute effort.
It surpasses FastSAM in all IoU measurements.
We also introduce an additional semi-natural but highly intuitive dataset where FLIP outperforms SAM and FastSAM overall and particularly on relatively small objects. 
Seeing that FLIP is an end-to-end object-centric segmentation approach, it has high potential particularly for applications that benefit from computationally efficient, spatially highly selective object tracking.
\end{abstract}



\section{Introduction}

Object-centric models have emerged as a powerful paradigm for structured perception in visual tasks. 
They offer the potential to represent complex scenes in a more interpretable and compositional manner. While traditional architectures such as Convolutional Neural Networks (CNNs) \cite{liu2022convnet} and Vision Transformers (ViTs) \cite{dosovitskiy2020image} have demonstrated impressive performance on large-scale datasets, they often lack the nuanced object-level understanding required for robust scene parsing. Furthermore, these models typically require massive amounts of labeled data and exhibit vulnerabilities to adversarial perturbations.


Recent advances in object-centric learning, such as Slot Attention \cite{locatello2020object}, have sought to address these challenges by enabling the model to discover and represent objects within a scene as distinct entities. Models like SAVi++ \cite{Elsayed2022}, VideoSAUR \cite{zadaianchuk2024object}, and Loci \cite{Traub:2023,traub2024learning,traub2024loci} have advanced the state-of-the-art in unsupervised object-centric learning. 
They are able to disentangle objects from complex backgrounds and track their identities over time. Still, these approaches struggle to scale effectively to more complex, real-world data. 

\begin{figure}[!b]
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/results/params-vs-model-size2.pdf}
    \caption{Comparison of segmentation performance (Mean IoU) on our ObjaScale dataset versus model size (number of parameters, in millions). FLIP variants achieve superior performance with significantly fewer parameters compared to SAM variants.}
    \label{fig:params_vs_size}
\end{figure}

In contrast, the Segment Anything (SAM) model \cite{kirillov2023segment} has introduced a paradigm shift in object-centric learning. 
SAM learns from a vast array of diverse data that is segmented by a two stage segmentation process. 
First, a powerful transformer-based foundational model encodes the complete image. 
Second, a query-based focusing mechanism specifies which object to segment. 
Only this second mechanism targets one image area or object and leads to the production of the targeted output mask. SAM marks the state-of-the-art in object segmentation tasks. 
However, despite its impressive performance, SAM has its limits. 
First, the transformer-based encoder requires very large computational resources. 
Second, the encoder backbone encodes the complete image, potentially wasting processing resources, particularly when small objects are to be segmented. 

The FastSAM model \cite{zhao2023fast} addresses the former limit. FastSAM replaces the first stage transformer-based encoder architecture in SAM with a convolutional ANN approach (CNN), which is pre-trained to segment the whole image. The query then targets the latent encodings and produces one area- or object-specific mask selectively. As a result, FastSAM yields performance close-to SAM, but is trained on only 2\% of the original SAM dataset, has much fewer parameters, and partially runs an order of magnitude faster.

The latter limit remains an open challenge. 
At this point nearly all segmentation techniques, including SAM and FastSAM, rely on a full image encoder. 
This is also the case for most object centric models, such as SlotAttention, VideoSAUR, and related work \cite{locatello2020object,Elsayed2022,singh2022simple,zadaianchuk2024object}, which first encode the entire image before assigning information to slots.
The challenge to computationally efficiently segment and track small but potentially high-resolution objects across diverse and complex scenes remains. 

To this end, we introduce FLIP: a fovea-like input patching approach that is integrated in an object-centric, off-grid vision framework. FLIP dynamically adapts its processing pipeline to the object's size and spatial characteristics. It ignores currently irrelevant image subregions and focusses on critical regions with a flexible, multi-resolution approach.
Our key contributions are:
\begin{itemize}

    \item \textbf{Off-Grid, Scale-Invariant Object Encoding}: We introduce a fovea-inspired patch sampling method that directly encodes image regions off-grid, adaptively focusing on objects of interest in a multi-resolution fashion. This scale-invariant approach is robust to large variations in object size and image resolution. It enables the detailed encoding of very small objects even in high-resolution scenes.
    
    \item \textbf{Interpretable Object-Centric Latent Representation}: Building on insights from cognitive science, FLIP explicitly segregates perceptual features from positional cues into distinct latent embeddings. This structured representation not only boosts segmentation accuracy but also improves explainability, as it allows for a separated assessment of object identity and spatial placement.

    \item \textbf{State-of-the-Art Segmentation Performance with High Parameter Efficiency}: Despite using significantly fewer parameters compared to state-of-the-art models like SAM or FastSAM, FLIP achieves competitive (SAM) or superior (FastSAM) segmentation accuracy on benchmarks such as Hypersim, KITTI-360, and OpenImages. 

    \item \textbf{Superior performance on novel dataset \emph{ObjaScale}} For further evaluation, we present a segmentation dataset that contains high-resolution real-world backgrounds combined with objects of varying scales, rendered in Blender. FLIP outperforms all other methods on ObjaScale, even with the smallest parameter settings (Figure~\ref{fig:params_vs_size}). Note that ObjaScale is used for evaluation only, neither FLIP nor SAM or FastSAM were trained on it.
\end{itemize}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/flip-sampling.pdf}  
    \caption{
        The multi-resolution patch sampling procedure in FLIP sequentially samples tiny (\textcolor{blue}{blue}), small (\textcolor{green}{green}), and large (\textcolor{red}{red}) patches given an object-centered 2D Gaussian.
        Overlapping patches are prohibited, thus generating a fovea-like distribution of patches across multiple resolutions.
    }
    \label{fig:flip_sampling}
\end{figure}

\begin{figure*}[!t] 
    \centering 
    \subfloat{ 
        \includegraphics[width=\textwidth]{figures/results/imgs/object-input-masked-02xx-masked-full.png} 
        \label{fig_a}
    } 
    \\
    \centering
    \begin{minipage}{1.0145\textwidth} % Ensure the images fit within the full width
        \centering
        \subfloat{ 
            \includegraphics[width=0.18\textwidth]{figures/results/imgs/object-rgb-out-masked-0282-rgb.jpg} 
            \label{fig_b}
        } 
        \hfill
        \subfloat{ 
            \includegraphics[width=0.18\textwidth]{figures/results/imgs/object-rgb-out-masked-0299-rgb-zoomed.jpg}
            \label{fig_c} 
        } 
        \hfill
        \subfloat{ 
            \includegraphics[width=0.18\textwidth]{figures/results/imgs/object-rgb-out-masked-0297-rgb-zoomed.jpg}
            \label{fig_d} 
        } 
        \hfill
        \subfloat{ 
            \includegraphics[width=0.18\textwidth]{figures/results/imgs/object-rgb-out-masked-0293-rgb-zoomed.jpg}
            \label{fig_e} 
        } 
        \hfill
        \subfloat{ 
            \includegraphics[width=0.18\textwidth]{figures/results/imgs/object-rgb-out-masked-0289-rgb-zoomed.jpg}
            \label{fig_f} 
        }
    \end{minipage}
     
    \caption{Visualization of our FLIP (Fovea-Like Input Patching) approach applied to an image from the KITTI-360 dataset, showcasing potential applications in autonomous driving. The figure illustrates how our model dynamically focuses on multiple objects within a complex urban scene by allocating multi-resolution patches centered around estimated object locations. Higher-resolution patches (smaller sizes) are concentrated on critical areas such as vehicles and road-signs, emulating a foveal vision system, while lower-resolution patches (larger sizes) cover peripheral regions to enable the consideration of the surrounding context. Patches are color-coded by size: \textcolor{purple}{purple} for $16 \times 16$ patches, \textcolor{yellow}{yellow} for $8 \times 8$, \textcolor{green}{green} for $4 \times 4$, \textcolor{blue}{blue} for $2 \times 2$, and \textcolor{red}{red} for $1 \times 1$.} 
    \label{fig:imgs_title} 
\end{figure*}

\section{Related Work} 

Several lines of research contain methods that share thematically similar ideas in handling multi-scale objects, dynamic sampling, or biologically inspired foveation.
\emph{Deformable Convolutional Networks} \cite{dai2017deformable} and their successors \cite{zhu2019deformable,xiong2024efficient} introduce learnable offsets or selective kernels to better handle varying spatial structures.
\emph{Focal Sparse Convolutional Networks} \cite{chen2022focal} focus computation on salient 3D regions in point clouds. Additionally, biologically inspired foveation has been explored in works like \cite{lukanov2021biologically,kaplanyan2019deepfovea,thavamani2021fovea}.

\section{Methods}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/arch/arch-v2.pdf} 
    \caption{
    Overview of the FLIP architecture. The Foveal Patching module dynamically samples multi-resolution patches centered around objects of interest. These patches are embedded into a unified latent space using resolution-specific Patch Embedding Modules (\(E_{r_0})\) to \(E_{r_K}\)). The Vision Transformer Encoder processes the embedded patches, generating separate Perceptual (\(z_p\)) and Positional Codes (\(z_{\text{pos}}\)). The ViT predictor combines these codes to predict the object instance segmentation mask, reconstructing it via Patch De-embedding Modules (\(D_{r_0}\) to \(D_{r_K})\)).
    }
    \label{fig:architecture}
\end{figure*}


In this section, we present the overall Fovea-Like Input Patching (FLIP) architecture. Its main processing pipeline is shown in Figure~\ref{fig:architecture}. FLIP is a supervised vision model for efficient object-centric segmentation. It combines a fovea-inspired patching mechanism with a Vision Transformer (ViT)-based encoder. The encoded information is then used to generate the targeted object’s segmentation mask, location, and bounding box, thereby disentangling perceptual and positional information.

The fovea-like patching mechanism dynamically selects multi-resolution patches based on a 2D Gaussian distribution, which has an effect similar to the query in SAM and FastSAM but acts directly on the input image.
High-resolution patches focus on the object center, capturing fine details, while coarser patches cover peripheral regions, which inform FLIP about the surrounding context. 


The ViT encoder processes the sampled patches and outputs two representations: a perceptual code (\(z_p\)), which captures the object’s appearance and shape, and a positional code (\(z_{\text{pos}}\)), which encodes location, orientation, and scale. 

The predictor combines \(z_p\) and \(z_{\text{pos}}\) to predict the segmentation mask. The positional code guides dynamic resolution-dependent decoding, ensuring high-resolution predictions at the object's spatial extent, while the perceptual code provides detailed object-specific information. This design enables FLIP to excel at segmenting small objects in high-resolution scenes with much lower computational overhead when compared to SAM and FastSAM.

FLIP is trained end-to-end with supervised losses for mask prediction, bounding box localization, and positional accuracy. In the following subsections, we provide the mathematical details of the fovea-like sampling mechanism, the ViT encoder, and the ViT predictor.



\subsection{Fovea-like Input Patching}
We equip our ViT-based model with a multi-resolution, fovea-inspired patching mechanism that centers around the object of interest, thereby preserving high-resolution detail at the object center and coarser coverage in peripheral regions.  Specifically, we derive the object center \(\mu = (\mu_x, \mu_y)\) and covariance \(\Sigma\) from the ground-truth mask, yielding a 2D Gaussian \(\mathcal{N}(\mu,\Sigma)\) that approximates the object’s spatial extent and orientation in compressed form. 
From $\Sigma$, we extract rotation \((\theta_a,\theta_b)\) and scale \((\sigma_x,\sigma_y)\) via a standard eigenvalue decomposition of the covariance matrix. 
For input sampling, the Gaussian serves as a spatial input query—similar to the prompts in SAM and FastSAM—from which we then draw patches at multiple scales.



We define $K$ patch sizes $p_1 < p_2 < \dots < p_K$, from smallest (highest resolution) to largest. We fix the total number of patches $N$ and choose $N_i$ patches for patch size $p_i$ as follows. First, we compute default numbers $\hat N_i$ by approximating the area covered by the Gaussian's 99\% inner density mass from $\Sigma$ and dividing it by $p_i^2$. 
Then, starting at the coarsest resolution (i.e., $p_K$), we compute $N_i = min(\hat N_i, N - \sum_{j=i+1}^K N_j)$.
As a result, all chosen patch sizes are smaller than the Gaussian inner area and the number of chosen patches per size distributes itself from coarse to fine until the total number of patches $N$ is reached($N = \sum_i^K N_i$).



We sample $N_i$ patches from $\mathcal{N}(\mu,\Sigma)$. Higher-resolution patches (small $p_i$) are drawn first.
Next, progressively larger patches (lower resolution) are sampled from the remaining density excluding the patches that overlap with patches that were already chosen (cf. Figure \ref{fig:flip_sampling}). 
Each sampled patch is first flattened and then mapped to a common embedding space via resolution-specific encoders \(E_{r_i}\) (similarly to \citealp{jaegle2021perceiver}), 
yielding a set of tokens $T = \{t_1,...,t_N\}$ of equal tensor size. 
These embedded tokens are concatenated and fed to the main ViT layers.
Note that because sampling depends on $\mu$ and $\Sigma$ only, the approach is in principle independent of the full image size.


\subsection{Object-Centric Attention}

For further encoding, we use modified Transformer blocks that preserve the “what vs.\ where” separation.
Figure~\ref{fig:attention_block} shows the design of one block. 
Inspired by \cite{yu2023lape}, we inject positional embeddings in every layer, but only into the queries and keys (never the values), ensuring spatial cues influence attention without affecting perceptual content.
The first layer $l=1$ block receives as input the set of tokens $T$, that is, $X_1=T$. 
Further, all blocks receive the following positional encoding $PE$, in the form of \emph{relative positional embeddings}. 
We compute these embeddings relative to the 2D Gaussian input query, shifting each token coordinate \((x_i,y_i)\) by \((\mu_x,\mu_y)\), rotating it by \((\theta_a,\theta_b)\), and scaling it by \((\sigma_x,\sigma_y)\). 
We then generate the embedding \(\mathrm{PE}\) via an MLP. This embedding is then added to queries and keys:
\begin{align*}
    \mathrm{PE}_i  &= MLP_i(\hat x_i,\hat y_i), \quad
    \mathrm{PE}_j  = MLP_j(\hat x_j,\hat y_j), \\
    \mathbf{Q}'_i &= \mathbf{Q}_i + \mathrm{PE}_i, \quad
    \mathbf{K}'_j = \mathbf{K}_j + \mathrm{PE}_j.
\end{align*}

We adopt a pre-layer normalization scheme \cite{xiong2020layer}, such that LayerNorm precedes both the self-attention and FFN blocks. Consequently, the Transformer layer computations for input \(X_l\) proceed as:
\begin{align*}
\hat{X} &= \mathrm{LN}(X_l), \\
Q &= \hat{X} W_Q, \quad K = \hat{X} W_K, \quad V = \hat{X} W_V, \\
Q' &= Q + \mathrm{PE}, \quad K' = K + \mathrm{PE}, \\
%\mathrm{Attention}(Q', K', V) &= \mathrm{softmax}\left(\frac{Q' K'^T}{\sqrt{d_k}}\right) V, \\
%Y &= X_l + \mathrm{Attention}(Q', K', V), \\
Y &= X_l + \mathrm{softmax}\left(\frac{Q' K'^T}{\sqrt{d_k}}\right) V, \\
X_{l+1} &= Y + \mathrm{FFN}(\mathrm{LN}(Y)),
\end{align*}
where \(d_k\) is the key dimension. By avoiding positional biases in \(V\), the model attends spatially via \(\mathrm{PE}\) without affecting the perceptual content. This design aligns with our object-centric principle: positional information modulates attention weights (queries, keys), and perceptual information flows through \(V\).
Depending on the chosen network sizes, we use three, six, or seven of these encoding blocks. 


\begin{figure}[t]
    \centering
    \includegraphics[width=0.3\textwidth]{figures/attention-block/attention-block.pdf} 
    \caption{Architecture of the Object-Centric Attention block in FLIP. Positional embeddings influence only the queries and keys, maintaining separation of spatial and perceptual information for efficient object-centric processing.}
    \label{fig:attention_block}
\end{figure}




\subsection{Learning What and Where}

Following the Loci family of models \cite{Traub:2023,traub2024learning,traub2024loci}, which draw inspiration from the dorsal-ventral visual pathway separation in the brain, FLIP finally enforces a strict separation between perceptual and positional information at the end of the encoding pipeline. This separation ensures that the model independently processes “what” an object is and “where” it is located, encouraging a more interpretable object segmentation.
The input to this separation comes from the output of the last object-centric attention block, which can be interpreted as consisting of $N$ encoded tokens $t_i$ and associated image coordinates \((x_i,y_i)\).


\paragraph{Perception Code \(\mathbf{z}_p\).}
To generate the final perception code $\mathbf{z}_p$, we combine all tokens by average pooling them and refining them via a cross-attention-layer, using the average pooled value as the query:
\begin{align*}
\mathbf{z}_{avg} 
&= \tfrac{1}{N}\!\sum_{i=1}^N \mathbf{t}_i \\
\mathbf{z}_p &= \mathbf{z}_{avg} + \text{CrossAttentionLayer}(\mathbf{z}_{avg},\,T).
\end{align*}
This yields a compact representation of the object’s appearance.

\paragraph{Positional Code \(\mathbf{z}_{\text{pos}}\).}
To infer \(\mathbf{z}_{\text{pos}}\), cross-attention modules on token coordinates obtain object-focused mean position, scale, and orientation estimates. First, we compute the object center:
\[
\mathbf{q}_\mu = W_q^\mu \,\mathbf{1}, 
\quad
\mathbf{k}_{\mu,i} = W_k^\mu\, t_i,
\quad
a_{\mu,i} = \text{softmax}\bigl(\mathbf{q}_\mu \!\cdot\!\mathbf{k}_{\mu,i}^\top\bigr),
\]
\[
\mu_x = \sum_{i=1}^N a_{\mu,i}\,x_i, 
\quad
\mu_y = \sum_{i=1}^N a_{\mu,i}\,y_i.
\]
Similarly, additional cross-attention heads estimate \(\sigma_x,\sigma_y\) (spread) and \(\theta_a,\theta_b\) (rotation). Concatenating these yields:
\[
\mathbf{z}_{\text{pos}} = [\mu_x,\,\mu_y,\,\sigma_x,\,\sigma_y,\,\theta_a,\,\theta_b],
\]
which enables us in principle to use this positional output also as an input query. This option is however not further explored in this paper.



\subsection{Mask Prediction}
To generate mask predictions, we first choose the region of the image that is expected to contain the object. 
FLIP then generates output patches that cover this region, predicting the mask via the patches. 
To determine the output region we take the positional code \( \mathbf{z}_{\text{pos}} \) and compute the rotated coordinates via:  
\begin{align*}
x_{\text{rot}} &= \theta_a\,(x - \mu_x) - \theta_b\,(y - \mu_y) \\
y_{\text{rot}} &= \theta_b\,(x - \mu_x) + \theta_a\,(y - \mu_y),
\end{align*}
with \( \sqrt{\theta_a^2+\theta_b^2}=1 \) and \( \sigma_x,\sigma_y \) clipped to \( \ge \epsilon \). The Gaussian map is  
\[
G(x,y) = \exp\!\Bigl(-\frac{1}{2}\Bigl[\bigl(\frac{x_{\text{rot}}}{\sigma_x}\bigr)^2 + \bigl(\frac{y_{\text{rot}}}{\sigma_y}\bigr)^2\Bigr]\Bigr),
\]
thresholded at \( 0.01 \) (its 99\% confidence region).

We fully cover this region, by selecting \( N_{out} \) patches using \textbf{two resolutions}. The patch ratio for these resolutions is chosen adaptively from \( \sigma_x, \sigma_y \) to guarantee that exactly \( N_{out} \) patches are needed to cover the determined output region.
For each patch \( (x_i,y_i) \) we form an output token:
\[
t_{\mathrm{out},i} = \mathbf{z}_{p} + \mathrm{PE}(x_i,y_i),
\]
where \( \mathbf{z}_{p} \) is the perception code.
These tokens are then processed through two Transformer blocks, whose block structure is identical to the encoder blocks. 
% to incorporate global context and refine the embeddings.
Finally, a resolution-specific de-embedding module \( D_{r_i} \) reconstructs each patch $\hat P_i$:
\[
\hat{P}_i = D_{r_i}\bigl(t_{\mathrm{out},i}\bigr).
\]
By placing each patch \( \hat{P}_i \) at its respective position \( (x_i,y_i) \) we cover the output region with patch-respective mask predictions and set the remaining area to zero (i.e., not part of the mask). 
This concludes the mask prediction process. 

\begin{figure*}[!t] 
    \centering 
    \subfloat{ 
        \includegraphics[width=0.15\textwidth]{figures/results/imgs/object-input-masked-0010-masked.jpg} 
    } 
    \hfill 
    \subfloat{ 
        \includegraphics[width=0.15\textwidth]{figures/results/imgs/object-input-masked-0015-masked.jpg}
    } 
    \hfill 
    \subfloat{ 
        \includegraphics[width=0.15\textwidth]{figures/results/imgs/object-input-masked-0024-masked.jpg}
    } 
    \hfill 
    \subfloat{ 
        \includegraphics[width=0.15\textwidth]{figures/results/imgs/object-input-masked-0049-masked.jpg}
    }
    \hfill 
    \subfloat{ 
        \includegraphics[width=0.15\textwidth]{figures/results/imgs/object-input-masked-0054-masked.jpg}
    }
    \hfill 
    \subfloat{ 
        \includegraphics[width=0.15\textwidth]{figures/results/imgs/object-input-masked-0055-masked.jpg}
    }
    \\
    \subfloat{ 
        \includegraphics[width=0.15\textwidth]{figures/results/imgs/object-rgb-out-masked-0010-rgb.jpg} 
        \label{fig}
    } 
    \hfill 
    \subfloat{ 
        \includegraphics[width=0.15\textwidth]{figures/results/imgs/object-rgb-out-masked-0015-rgb.jpg}
    } 
    \hfill 
    \subfloat{ 
        \includegraphics[width=0.15\textwidth]{figures/results/imgs/object-rgb-out-masked-0024-rgb.jpg}
    } 
    \hfill 
    \subfloat{ 
        \includegraphics[width=0.15\textwidth]{figures/results/imgs/object-rgb-out-masked-0049-rgb.jpg}
    }
    \hfill 
    \subfloat{ 
        \includegraphics[width=0.15\textwidth]{figures/results/imgs/object-rgb-out-masked-0054-rgb.jpg}
    }
    \hfill 
    \subfloat{ 
        \includegraphics[width=0.15\textwidth]{figures/results/imgs/object-rgb-out-masked-0055-rgb.jpg}
    }
    \caption{Examples of multi-resolution patch inputs (top row) and corresponding mask predictions (bottom row) from FLIP. Input patches are color-coded by size: \textcolor{purple}{purple} ($16 \times 16$), \textcolor{yellow}{yellow} ($8 \times 8$), \textcolor{green}{green} ($4 \times 4$), \textcolor{blue}{blue} ($2 \times 2$), and \textcolor{red}{red} ($1 \times 1$). Higher-resolution patches focus on object centers for detail, while lower-resolution patches cover peripheral regions for efficiency. Mask predictions show accurate segmentation with optimized resource allocation.}
    \label{fig:imgs_results} 
\end{figure*}

\subsection{Training}

We train FLIP \emph{only} on the Segment Anything Dataset, performing approximately 1.5\,M updates at a batch size of 256, covering about 35.84\,\% of the dataset. Each training example has an associated object mask, from which we compute a 2D Gaussian distribution capturing the object’s center, scale, and orientation. To improve generalization, we apply random perturbations to this Gaussian by slightly shifting \(x\) and \(y\), stretching or compressing \(\sigma_x\) and \(\sigma_y\), and introducing small rotations. 
Tokens are sampled directly from this perturbed 2D Gaussian, with the total number of tokens, $N$, varied around a predefined mean \(\mu\) during training to further improve generalization.


The training consists of three phases:

\begin{itemize}
    \item \textbf{Phase 1}: Training begins with a mean token count \(\mu = 128\) and half-resolution inputs for 800k steps.
    \item \textbf{Phase 2}: The token count increases to \(\mu = 512\) with full-resolution inputs for the next 600k steps.
    \item \textbf{Phase 3}: A final fine-tuning phase uses \(\mu = 4096\) tokens for 100k steps, matching the number of patches used by SAM.
\end{itemize}


We train three model sizes FLIP-S (2.6M parameters), FLIP-M (6.5M parameters), and FLIP-L (26.5M parameters). Notably, these models are significantly smaller in parameter count than SAM variants (up to 641.1M) and FastSAM-x (72.2M), with only FastSAM-s falling in a similar range (11.8M).

We optimize three losses to predict accurate masks, bounding boxes, and  positional (\(\mathbf{z}_{\text{pos}}\)) codes.


\begin{itemize}
    \item \textbf{Mask Loss:} Binary cross-entropy loss between predicted mask logits \(\hat{M}\) and ground truth \(M\):
    \[
    s_{\text{mask},i} = \frac{1}{\sum_{j=1}^{N_i} M_{ij}}
    \]
    \[
    \mathcal{L}_{\text{mask}} = \frac{1}{B} \sum_{i=1}^B s_{\text{mask},i} \sum_{j=1}^{N_i} \ell_{\text{BCE}}(\hat{M}_{ij}, M_{ij}),
    \]
    where \(B\) is the batch size.

    \item \textbf{Position Loss:} Penalizes deviations in mean position (\(\mu\)), scale (\(\sigma\)), and rotation (\(\theta\)):
    \[
    \mathcal{L}_{\text{pos}} = \frac{1}{B} \sum_{i=1}^B  \bigl(\ \frac{1}{\sigma_i}|\hat{\mu}_i - \mu_i\|^2 + \|\hat{\sigma}_i - \sigma_i\|^2 + \|\hat{\theta}_i - \theta_i\|^2\bigr).
    \]

    \item \textbf{Bounding Box Loss:} Matches predicted bounding boxes \(\hat{B}_i\) to ground truth \(B_i\):
    \[
    \mathcal{L}_{\text{bbox}} = \frac{1}{B} \sum_{i=1}^B \frac{1}{\sigma_i} \|\hat{B}_i - B_i\|^2.
    \]
\end{itemize}

\paragraph{Total Loss}
The overall loss combines mask, position, and bounding box losses:
\[
\mathcal{L}_{\text{stage1}} = \mathcal{L}_{\text{mask}} + \lambda_{\text{pos}} \mathcal{L}_{\text{pos}} + \lambda_{\text{bbox}} \mathcal{L}_{\text{bbox}},
\]
where \(\lambda_{\text{pos}}\) and \(\lambda_{\text{bbox}}\) control weightings. 





\section{Results}

We evaluate FLIP on our newly constructed synthetic dataset ObjaScale, which we designed to stress-test scale invariance, and on three standard benchmarks: Hypersim, KITTI-360, and OpenImages \cite{roberts2021hypersim,liao2022kitti,kuznetsova2020open}. Notably, FLIP was neither trained nor fine-tuned on the ObjaScale dataset. 
Our experiments compare FLIP against two state-of-the-art segmentation methods, SAM~\cite{kirillov2023segment} and FastSAM~\cite{zhao2023fast}, focusing on handling small objects, achieving high IoU, and balancing parameter/runtime trade-offs.
\begin{figure*}[t]
\centering
\subfloat[SAM-H]{\includegraphics[width=0.3\textwidth]{figures/results/heatmaps/heatmap_sam_h_bbox.pdf}}
\hfill
\subfloat[FastSAM-x]{\includegraphics[width=0.3\textwidth]{figures/results/heatmaps/heatmap_fast_sam_bbox.pdf}}
\hfill
\subfloat[FLIP-L]{\includegraphics[width=0.3\textwidth]{figures/results/heatmaps/heatmap_flip4096.pdf}}
\caption{IoU (ObjaScale) heatmaps illustrating relative vs.\ absolute mask size. FLIP-L retains strong accuracy even for small objects, provided there are enough pixels (e.g., \(\ge 10 \times 10\)).}
\label{fig:combined}
\end{figure*}  
\begin{table*}[!t]
\centering
\caption{Comparison of Mean IoU (\%) and Std IoU (\%) across different datasets.}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c ccc c c c c}
\toprule

\textbf{Model} & 
\textbf{Size (M)} &
\multicolumn{3}{c}{\textbf{Time (ms)}} &
\textbf{Hypersim} & 
\textbf{KITTI-360} & 
\textbf{OpenImage} & 
\textbf{ObjaScale (ours)} \\

\cmidrule(lr){3-5}
\cmidrule(lr){6-6}
\cmidrule(lr){7-7}
\cmidrule(lr){8-8}
\cmidrule(lr){9-9}
% You can place \cmidrule for other columns if needed

& & \textbf{Pre/Pos} & \textbf{Model} & \textbf{Total} & Mean \textcolor{gray}{± Std} IoU (\%) &Mean \textcolor{gray}{± Std} IoU (\%) & Mean \textcolor{gray}{± Std} IoU (\%)& Mean \textcolor{gray}{± Std} IoU (\%) \\

\midrule
FastSAM-s     & 11.8 & 4.56 & 4.84 & 9.40   & 36.80 \textcolor{gray}{± 34.62} & 37.71 \textcolor{gray}{± 32.76} & 61.14 \textcolor{gray}{± 32.10} & 48.20 \textcolor{gray}{± 35.69} \\
FastSAM-x     & 72.2 & 4.54  & 18.46 & 23.00  & 43.39 \textcolor{gray}{± 35.95} & 38.92 \textcolor{gray}{± 34.06} & 69.31 \textcolor{gray}{± 29.25} & 47.11 \textcolor{gray}{± 36.41} \\
\midrule
FLIP-S (ours) & 2.1 & 22.58 &  9.63 & 32.21  & 61.51 \textcolor{gray}{± 27.16} & 59.30 \textcolor{gray}{± 19.58} & 70.75 \textcolor{gray}{± 23.51} & 77.69 \textcolor{gray}{± 19.83} \\
FLIP-M (ours) & 6.5  & 23.84 & 15.04 & 38.88  & 64.36 \textcolor{gray}{± 28.32} & 61.45 \textcolor{gray}{± 19.64} & 76.76 \textcolor{gray}{± 21.73} & 82.71 \textcolor{gray}{± 18.24} \\
FLIP-L (ours) & 26.5 & 22.54 & 18.09 & 40.63  & 66.83 \textcolor{gray}{± 27.74} & 61.85 \textcolor{gray}{± 19.56} & 78.37 \textcolor{gray}{± 21.77} & 84.94 \textcolor{gray}{± 17.16} \\
\midrule
SAM-B         & 93.7 & 11.06 & 63.40 & 74.46  & 71.46 \textcolor{gray}{± 20.88} & 62.38 \textcolor{gray}{± 21.41} & 84.72 \textcolor{gray}{± 15.38} & 71.38 \textcolor{gray}{± 25.36} \\
SAM-L         & 312.3 & 11.07 & 139.17 & 150.24 & 72.13 \textcolor{gray}{± 21.21} & 62.73 \textcolor{gray}{± 20.31} & 86.94 \textcolor{gray}{± 13.41} & 72.68 \textcolor{gray}{± 25.22} \\
SAM-H         & 641.1 & 10.96 & 222.00 & 232.96 & 72.37 \textcolor{gray}{± 21.65} & 62.47 \textcolor{gray}{± 20.52} & 87.06 \textcolor{gray}{± 13.53} & 73.76 \textcolor{gray}{± 24.59} \\
\bottomrule
\end{tabular}%
}
\label{tab:evaluation_results}
\end{table*}
\subsection{Experimental Setup and Dataset Creation}

We selected 68 diverse categories from Objaverse~\cite{deitke2023objaverse} and combined each with high-resolution HDRI Haven~\cite{hdrihaven} backgrounds in Blender, yielding 10,200 synthetic images, which form our ObjaScale dataset. Object scale and image resolution (ranging from $512$ to $8192$) were randomized, producing masks spanning minuscule ($<\!0.0001\%$) to large fractions of the image (Fig.~\ref{fig:dataset}). For SAM and FastSAM, bounding-box prompts computed from ground-truth masks were used; FLIP employed 2D Gaussian prompts by design.

\subsection{Performance vs.\ Relative Mask Size}

On the ObjaScale dataset, Figure~\ref{fig_1} plots IoU against relative mask size. FLIP-L consistently surpasses the other models for small objects (\(<0.1\%\) of the image). FastSAM-x drops steeply, while SAM-H tapers more moderately. By adaptively sampling around the actual pixel area, FLIP-L retains strong IoU scores even when objects occupy a tiny fraction of the scene.


\subsection{Heatmap Analysis and Small-Object Segmentation}

Figure~\ref{fig:combined} highlights segmentation performance with respect to both relative and absolute mask size on ObjaScale. SAM-H struggles when the mask ratio diminishes, and FastSAM-x fails below a mask to image ratio of $0.01\%$. FLIP-L, by contrast, primarily depends on whether the object covers enough pixels to support a sufficiently detailed patch representation.




\subsection{Evaluation on Benchmark Datasets}

Table~\ref{tab:evaluation_results} compares Mean and Std IoU across Hypersim, KITTI-360, OpenImages and ObjaScale. FLIP exceeds the best FastSAM variant and remains close to SAM, while requiring substantially fewer parameters and competitive inference time. 
Notably, this is despite its input patch sampling and output patch selection (pre-/pos-processing times) are not yet optimized for inference time.

\begin{figure*}[!t] 
    \centering 
    \subfloat[Fire Hydrant]{ 
        \includegraphics[width=0.22\textwidth]{figures/results/dataset/rgb_001_1024.jpg} 
    } 
    \hfill 
    \subfloat[Apple]{ 
        \includegraphics[width=0.22\textwidth]{figures/results/dataset/rgb_002_1024.jpg} 
    } 
    \hfill 
    \subfloat[Truck]{ 
        \includegraphics[width=0.22\textwidth]{figures/results/dataset/rgb_003_1024.jpg} 
    } 
    \hfill 
    \subfloat[Airplane]{ 
         \includegraphics[width=0.22\textwidth]{figures/results/dataset/rgb_004_1024.jpg} 
    } 
    \caption{Examples from our synthetic dataset. Objects from various categories—(a) Fire Hydrant, (b) Apple, (c) Truck, and (d) Airplane—are rendered with high-resolution HDRI Haven backgrounds. The dataset includes diverse objects and scene compositions, with varying object scales, resolutions, and viewing angles to challenge segmentation models. Best viewed zoomed in.} 
    \label{fig:dataset} 
\end{figure*}
\subsection{Varying the Patch Token Budget}

Lastly, we evaluated how limiting FLIP’s patch sampling impacts performance. Figure~\ref{fig:token_eval} shows FLIP-L on OpenImages for different token counts; even with only 100 patches, IoU remains near its peak, underscoring robust efficiency under constrained computational budgets.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/results/num_tokens.pdf}
    \caption{FLIP-L performance on OpenImages under different numbers of sampled patches.}
    \label{fig:token_eval}
\end{figure}

Overall, these results highlight FLIP’s ability to accurately segment objects across scales, remain parameter-efficient, and adapt to variable computational budgets while matching or exceeding competing methods.

\begin{figure}[t]
\centering
\includegraphics[width=0.95\columnwidth]{figures/results/bbox-plot/plot_bbox_R.pdf}
\caption{Intersection over Union (IoU) vs.\ relative mask size. FLIP maintains high segmentation accuracy even at extreme scales. Shaded regions show 95\% confidence intervals.}
\label{fig_1}
\end{figure}

\section{Conclusion}

We have introduced FLIP, a novel object-centric fovea-like input patching-based vision model that enhances scalable and data-efficient learning in complex visual scenes. FLIP implements a scale-invariant multi-resolution patch sampling mechanism, focusing high-resolution sampling around object centers using Gaussian distributions. As a result, FLIP effectively concentrates on critical regions while maintaining computational efficiency.

Our dynamic resolution mask prediction module and dedicated spatial attention mechanism, which separates perceptual and positional codes by applying positional embeddings exclusively to queries and keys, further improves segmentation accuracy. This approach aligns with principles well-known in cognitive modeling and enhances spatial selectivity without compromising perceptual content.

Experimental results on our newly introduced ObjaScale dataset reveal that current state-of-the-art segmentation models lack a critical feature: effectively segmenting very small objects. FLIP fills this gap by maintaining strong accuracy for tiny objects through its fovea-like, scale-invariant patch sampling. Across synthetic datasets and standard benchmarks such as Hypersim, KITTI-360, and OpenImages, FLIP achieves competitive Intersection over Union (IoU) scores with higher efficiency. These findings highlight FLIP's potential for significant impact in computer vision applications that require both robust and highly data-efficient, generalizable, fine-grained object segmentations.

\section{Acknowledgement}
This work received funding from the Deutsche Forschungs-
gemeinschaft (DFG, German Research Foundation) under
Germany’s Excellence Strategy – EXC number 2064/1 –
Project number 390727645 as well as from the Cyber Valley
in Tübingen, CyVy-RF-2020-15. The authors thank the
International Max Planck Research School for Intelligent
Systems (IMPRS-IS) for supporting Manuel Traub, and the
Alexander von Humboldt Foundation for supporting Martin
Butz.

\bibliography{literature}
\bibliographystyle{arxiv}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix

\end{document}

