\section{Related Work}
\noindent \textbf{Large Vision-Language Models} ~ ~ Large Vision-Language Models, which integrate vision encoders, connectors, and large language models to enhance cross-modal understanding, have emerged as powerful frameworks that combine visual and textual information for a wide range of tasks. These models can be systematically categorized based on the type of connector. The first category comprises approaches utilizing cross-attention-based connectors, such as **Araei et al., "Flamingo"** and **Hessel et al., "CogVLM"**, which exploit attention mechanisms to facilitate the exchange of information between the vision encoder and the language model. The second category includes methods that employ query-based connectors, such as **Li et al., "BLIP-2"**, **Chen et al., "Instruct-BLIP"**, **Xu et al., "mPLUG-owl2"**, and **Wang et al., "Qwen-VL"**, wherein queries are leveraged to orchestrate the interaction between visual and textual modalities, thereby enhancing the alignment and coherence of visual and linguistic representations. Furthermore, projection-based connector methods, exemplified by **Chen et al., "LLaVA"**, **Li et al., "Mini-GPT4"**, **Wang et al., "DeepSeek-VL"**, and **Xu et al., "Mini-Gemini"**, project visual data into a shared embedding space, thereby fostering seamless integration with textual information.
These innovations offer a range of solutions for cross-modal understanding, driving the potential applications of intelligent systems in multi-task learning.

% The innovation of various methods offers a range of solutions for cross-modal understanding, thereby driving the potential applications of intelligent systems in multi-task learning.


\noindent \textbf{MedVQA Dataset on radiology} ~ ~ Medical visual question answering (MedVQA) datasets play a pivotal role in advancing AI-driven clinical decision-making. **Kim et al., "VQA-RAD"**, as an early pioneering work, introduces a meticulously curated dataset for radiology images, featuring clinician-generated questions and corresponding answers tailored to clinically relevant tasks. **Rajpurkar et al., "SLAKE"** stands out as a large, bilingual dataset, enriched with extensive semantic annotations and spanning a wide range of radiological modalities. 

**Johnson et al., "MIMIC-CXR"** provides a vast collection of 371,920 chest X-rays from 65,079 patients, serving as the foundation for numerous subsequent studies. The comparison of these datasets is shown in Table \ref{tab:dataset-comparison}.  **Johnson et al., "MIMIC-CXR-VQA"** seamlessly integrates chest X-rays with Electronic Health Records (EHRs), facilitating multi-modal question answering with an emphasis on region-specific queries. The **Xu et al., "Medical-Diff-VQA"** is notable for its inclusion of seven distinct question types, particularly focusing on the comparative analysis of current and reference images for diagnostic purposes. **Rajpurkar et al., "GEMeX"** offers a large-scale, explainable VQA benchmark, complete with detailed visual and textual explanations, thus addressing the growing need for a diverse array of clinical questions. Most of these datasets primarily focus on observations from a single image. While Medical-Diff-VQA contains 131k QA pairs that address multi-image changes, its structure is relatively simple and only accounts for global-level differences. Our MMXU-\textit{dev} dataset is the first to understand complex changes in the same regions across multiple chest X-ray images of the same patient at the regional level, spanning several visits.