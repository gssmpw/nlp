[
  {
    "index": 0,
    "papers": [
      {
        "key": "alayrac2022flamingo",
        "author": "Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others",
        "title": "Flamingo: a visual language model for few-shot learning"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "wang2023cogvlm",
        "author": "Wang, Weihan and Lv, Qingsong and Yu, Wenmeng and Hong, Wenyi and Qi, Ji and Wang, Yan and Ji, Junhui and Yang, Zhuoyi and Zhao, Lei and Song, Xixuan and others",
        "title": "Cogvlm: Visual expert for pretrained language models"
      },
      {
        "key": "hong2024cogvlm2",
        "author": "Hong, Wenyi and Wang, Weihan and Ding, Ming and Yu, Wenmeng and Lv, Qingsong and Wang, Yan and Cheng, Yean and Huang, Shiyu and Ji, Junhui and Xue, Zhao and others",
        "title": "Cogvlm2: Visual language models for image and video understanding"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "li2023blip",
        "author": "Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven",
        "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "dai2023instructblip",
        "author": "Dai, Wenliang and Li, Junnan and Li, D and Tiong, AMH and Zhao, J and Wang, W and Li, B and Fung, P and Hoi, S",
        "title": "Instructblip: Towards general-purpose vision-language models with instruction tuning. arxiv 2023"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "ye2024mplug",
        "author": "Ye, Qinghao and Xu, Haiyang and Ye, Jiabo and Yan, Ming and Hu, Anwen and Liu, Haowei and Qian, Qi and Zhang, Ji and Huang, Fei",
        "title": "mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "bai2023qwen",
        "author": "Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren",
        "title": "Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "liu2023llava",
        "author": "Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",
        "title": "Visual Instruction Tuning"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "zhu2023minigpt",
        "author": "Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed",
        "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "lu2024deepseek",
        "author": "Lu, Haoyu and Liu, Wen and Zhang, Bo and Wang, Bingxuan and Dong, Kai and Liu, Bo and Sun, Jingxiang and Ren, Tongzheng and Li, Zhuoshu and Yang, Hao and others",
        "title": "Deepseek-vl: towards real-world vision-language understanding"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "li2024mini",
        "author": "Li, Yanwei and Zhang, Yuechen and Wang, Chengyao and Zhong, Zhisheng and Chen, Yixin and Chu, Ruihang and Liu, Shaoteng and Jia, Jiaya",
        "title": "Mini-gemini: Mining the potential of multi-modality vision language models"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "Lau2018",
        "author": "Lau, Jason and Gayen, Soumya and Ben, Asma and Demner-Fushman, Dina",
        "title": "A dataset of clinically generated visual questions and answers about radiology images"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "liu2021slake",
        "author": "Liu, Bo and Zhan, Li-Ming and Xu, Li and Ma, Lin and Yang, Yan and Wu, Xiao-Ming",
        "title": "Slake: A semantically-labeled knowledge-enhanced dataset for medical visual question answering"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "johnson2019mimic",
        "author": "Johnson, Alistair EW and Pollard, Tom J and Berkowitz, Seth J and Greenbaum, Nathaniel R and Lungren, Matthew P and Deng, Chih-ying and Mark, Roger G and Horng, Steven",
        "title": "MIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text reports"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "bae2024ehrxqa",
        "author": "Bae, Seongsu and Kyung, Daeun and Ryu, Jaehee and Cho, Eunbyeol and Lee, Gyubok and Kweon, Sunjun and Oh, Jungwoo and Ji, Lei and Chang, Eric and Kim, Tackeun and others",
        "title": "Ehrxqa: A multi-modal question answering dataset for electronic health records with chest x-ray images"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "hu2023medical",
        "author": "Hu, Xinyue and Gu, L and An, Q and Zhang, M and Liu, L and Kobayashi, K and Harada, T and Summers, R and Zhu, Y",
        "title": "Medical-Diff-VQA: a large-scale medical dataset for difference visual question answering on chest x-ray images"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "liu2024gemex",
        "author": "Liu, Bo and Zou, Ke and Zhan, Liming and Lu, Zexin and Dong, Xiaoyu and Chen, Yidi and Xie, Chengqiang and Cao, Jiannong and Wu, Xiao-Ming and Fu, Huazhu",
        "title": "GEMeX: A Large-Scale, Groundable, and Explainable Medical VQA Benchmark for Chest X-ray Diagnosis"
      }
    ]
  }
]