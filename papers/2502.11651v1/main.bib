# MedVQA
@inproceedings{hu2024omnimedvqa,
  title={Omnimedvqa: A new large-scale comprehensive evaluation benchmark for medical lvlm},
  author={Hu, Yutao and Li, Tianbin and Lu, Quanfeng and Shao, Wenqi and He, Junjun and Qiao, Yu and Luo, Ping},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={22170--22183},
  year={2024}
}
@inproceedings{sun-etal-2024-self,title = "Self-Training Large Language and Vision Assistant for Medical Question Answering",author = "Sun, Guohao and Qin, Can and Fu, Huazhu and Wang, Linwei and Tao, Zhiqiang",editor = "Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung",booktitle = "EMNLP:2024:main",month = nov,year = "2024",address = "Miami, Florida, USA",publisher = "acl",
doi = "10.18653/v1/2024.emnlp-main.1119",pages = "20052--20060",
url = {2024.emnlp-main.1119/},
}
@inproceedings{saeed-2024-medifact,title = "{M}edi{F}act at {MEDIQA}-{M}3{G} 2024: Medical Question Answering in Dermatology with Multimodal Learning",author = "Saeed, Nadia",editor = "Naumann, Tristan and Ben Abacha, Asma and Bethard, Steven and Roberts, Kirk and Bitterman, Danielle",booktitle = "CLINICALNLP:2024:1",month = jun,year = "2024",address = "Mexico City, Mexico",publisher = "acl",
doi = "10.18653/v1/2024.clinicalnlp-1.31",pages = "339--345",
url = {2024.clinicalnlp-1.31/},
}
# MRG
@inproceedings{yin-etal-2025-kia,title = "{KIA}: Knowledge-Guided Implicit Vision-Language Alignment for Chest {X}-Ray Report Generation",author = "Yin, Heng and Zhou, Shanlin and Wang, Pandong and Wu, Zirui and Hao, Yongtao",editor = "Rambow, Owen and Wanner, Leo and Apidianaki, Marianna and Al-Khalifa, Hend and Eugenio, Barbara Di and Schockaert, Steven",booktitle = "COLING:2025:main",month = jan,year = "2025",address = "Abu Dhabi, UAE",publisher = "acl",
pages = "4096--4108",
url = {2025.coling-main.276/},
}
@inproceedings{huang-etal-2025-cmeaa,title = "{C}m{EAA}: Cross-modal Enhancement and Alignment Adapter for Radiology Report Generation",author = "Huang, Xiyang and Han, Yingjie and L, Yx and Li, Runzhi and Wu, Pengcheng and Zhang, Kunli",editor = "Rambow, Owen and Wanner, Leo and Apidianaki, Marianna and Al-Khalifa, Hend and Eugenio, Barbara Di and Schockaert, Steven",booktitle = "COLING:2025:main",month = jan,year = "2025",address = "Abu Dhabi, UAE",publisher = "acl",
pages = "8546--8556",
url =  {2025.coling-main.571/},
}
@inproceedings{bu-etal-2024-dynamic,title = "Dynamic Knowledge Prompt for Chest {X}-ray Report Generation",author = "Bu, Shenshen and Song, Yujie and Li, Taiji and Dai, Zhiming",editor = "Calzolari, Nicoletta and Kan, Min-Yen and Hoste, Veronique and Lenci, Alessandro and Sakti, Sakriani and Xue, Nianwen",booktitle = "LREC:2024:main",month = may,year = "2024",address = "Torino, Italia",publisher = "ELRA and ICCL",pages = "5425--5436"
}
@inproceedings{zhou-wang-2024-divide,title = "Divide and Conquer Radiology Report Generation via Observation Level Fine-grained Pretraining and Prompt Tuning",author = "Zhou, Yuanpin and Wang, Huogen",editor = "Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung",booktitle = "EMNLP:2024:main",month = nov,year = "2024",address = "Miami, Florida, USA",publisher = "acl",
doi = "10.18653/v1/2024.emnlp-main.433",pages = "7597--7610",
url = {2024.emnlp-main.433/},
}


########################## Dataset ##########################
# GeMeX
@article{liu2024gemex,
  title={GEMeX: A Large-Scale, Groundable, and Explainable Medical VQA Benchmark for Chest X-ray Diagnosis},
  author={Liu, Bo and Zou, Ke and Zhan, Liming and Lu, Zexin and Dong, Xiaoyu and Chen, Yidi and Xie, Chengqiang and Cao, Jiannong and Wu, Xiao-Ming and Fu, Huazhu},
  journal={arXiv preprint arXiv:2411.16778},
  year={2024}
}
# MIMIC
@article{johnson2019mimic,
  title={MIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text reports},
  author={Johnson, Alistair EW and Pollard, Tom J and Berkowitz, Seth J and Greenbaum, Nathaniel R and Lungren, Matthew P and Deng, Chih-ying and Mark, Roger G and Horng, Steven},
  journal={Scientific data},
  volume={6},
  number={1},
  pages={317},
  year={2019},
  publisher={Nature Publishing Group UK London}
}
# MIMIC-CXR-VQA
@article{bae2024ehrxqa,
  title={Ehrxqa: A multi-modal question answering dataset for electronic health records with chest x-ray images},
  author={Bae, Seongsu and Kyung, Daeun and Ryu, Jaehee and Cho, Eunbyeol and Lee, Gyubok and Kweon, Sunjun and Oh, Jungwoo and Ji, Lei and Chang, Eric and Kim, Tackeun and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
# Medical-Diff-VQA
@article{hu2023medical,
  title={Medical-Diff-VQA: a large-scale medical dataset for difference visual question answering on chest x-ray images},
  author={Hu, Xinyue and Gu, L and An, Q and Zhang, M and Liu, L and Kobayashi, K and Harada, T and Summers, R and Zhu, Y},
  journal={PhysioNet},
  volume={12},
  pages={13},
  year={2023}
}
# SLAKE
@inproceedings{liu2021slake,
  title={Slake: A semantically-labeled knowledge-enhanced dataset for medical visual question answering},
  author={Liu, Bo and Zhan, Li-Ming and Xu, Li and Ma, Lin and Yang, Yan and Wu, Xiao-Ming},
  booktitle={2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)},
  pages={1650--1654},
  year={2021},
  organization={IEEE}
}
# RAD-VQA
@article{Lau2018,
  author = {Lau, Jason and Gayen, Soumya and Ben, Asma and Demner-Fushman, Dina},
  title = {A dataset of clinically generated visual questions and answers about radiology images},
  journal = {Scientific Data},
  year = {2018},
  volume = {5},
  number = {1},
  pages = {180251},
  doi = {10.1038/sdata.2018.251},
  issn = {2052-4463},
  abstract = {Radiology images are an essential part of clinical decision making and population screening, e.g., for cancer. Automated systems could help clinicians cope with large amounts of images by answering questions about the image contents. An emerging area of artificial intelligence, Visual Question Answering (VQA) in the medical domain explores approaches to this form of clinical decision support. Success of such machine learning tools hinges on availability and design of collections composed of medical images augmented with question-answer pairs directed at the content of the image. We introduce VQA-RAD, the first manually constructed dataset where clinicians asked naturally occurring questions about radiology images and provided reference answers. Manual categorization of images and questions provides insight into clinically relevant tasks and the natural language to phrase them. Evaluating with well-known algorithms, we demonstrate the rich quality of this dataset over other automatically constructed ones. We propose VQA-RAD to encourage the community to design VQA tools with the goals of improving patient care.},
url = {https://doi.org/10.1038/sdata.2018.251}
}

@article{lorkowski2022medical,
  title={Medical records: A historical narrative},
  author={Lorkowski, Jacek and Pokorski, Mieczyslaw},
  journal={Biomedicines},
  volume={10},
  number={10},
  pages={2594},
  year={2022},
  publisher={MDPI}
}

########################## LVLM ##########################
# QwenVL
@article{bai2023qwen,
  title={Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.12966},
  volume={1},
  number={2},
  pages={3},
  year={2023}
}
# Llava
@misc{liu2023llava,
      title={Visual Instruction Tuning}, 
      author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
      publisher={NeurIPS},
      year={2023},
}
# InternVL
@inproceedings{chen2024internvl,
  title={Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks},
  author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={24185--24198},
  year={2024}
}
#IDEFICS2
@article{laurenccon2024matters,
  title={What matters when building vision-language models?},
  author={Lauren{\c{c}}on, Hugo and Tronchon, L{\'e}o and Cord, Matthieu and Sanh, Victor},
  journal={arXiv preprint arXiv:2405.02246},
  year={2024}
}

# MiniGPT4
@article{zhu2023minigpt,
  title={Minigpt-4: Enhancing vision-language understanding with advanced large language models},
  author={Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:2304.10592},
  year={2023}
}
# DeepseekVL
@article{lu2024deepseek,
  title={Deepseek-vl: towards real-world vision-language understanding},
  author={Lu, Haoyu and Liu, Wen and Zhang, Bo and Wang, Bingxuan and Dong, Kai and Liu, Bo and Sun, Jingxiang and Ren, Tongzheng and Li, Zhuoshu and Yang, Hao and others},
  journal={arXiv preprint arXiv:2403.05525},
  year={2024}
}
# GPT4V
@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}
@inproceedings{TheC3,
  title={The Claude 3 Model Family: Opus, Sonnet, Haiku},
  author={Anthropic},
  journal={arXiv preprint arXiv:2403.05525},
  url={https://api.semanticscholar.org/CorpusID:268232499},
}
# Flamingo
@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={23716--23736},
  year={2022}
}
# cogvlm
@article{wang2023cogvlm,
  title={Cogvlm: Visual expert for pretrained language models},
  author={Wang, Weihan and Lv, Qingsong and Yu, Wenmeng and Hong, Wenyi and Qi, Ji and Wang, Yan and Ji, Junhui and Yang, Zhuoyi and Zhao, Lei and Song, Xixuan and others},
  journal={arXiv preprint arXiv:2311.03079},
  year={2023}
}
@article{hong2024cogvlm2,
  title={Cogvlm2: Visual language models for image and video understanding},
  author={Hong, Wenyi and Wang, Weihan and Ding, Ming and Yu, Wenmeng and Lv, Qingsong and Wang, Yan and Cheng, Yean and Huang, Shiyu and Ji, Junhui and Xue, Zhao and others},
  journal={arXiv preprint arXiv:2408.16500},
  year={2024}
}
# BLIP-2
@inproceedings{li2023blip,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  booktitle={International conference on machine learning},
  pages={19730--19742},
  year={2023},
  organization={PMLR}
}
# instruct-blip
@article{dai2023instructblip,
  title={Instructblip: Towards general-purpose vision-language models with instruction tuning. arxiv 2023},
  author={Dai, Wenliang and Li, Junnan and Li, D and Tiong, AMH and Zhao, J and Wang, W and Li, B and Fung, P and Hoi, S},
  journal={arXiv preprint arXiv:2305.06500},
  volume={2},
  year={2023}
}
@inproceedings{ye2024mplug,
  title={mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration},
  author={Ye, Qinghao and Xu, Haiyang and Ye, Jiabo and Yan, Ming and Hu, Anwen and Liu, Haowei and Qian, Qi and Zhang, Ji and Huang, Fei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13040--13051},
  year={2024}
}
@article{li2024mini,
  title={Mini-gemini: Mining the potential of multi-modality vision language models},
  author={Li, Yanwei and Zhang, Yuechen and Wang, Chengyao and Zhong, Zhisheng and Chen, Yixin and Chu, Ruihang and Liu, Shaoteng and Jia, Jiaya},
  journal={arXiv preprint arXiv:2403.18814},
  year={2024}
}
########################## LLM ##########################
# Deepseek
@article{guo2025deepseek,
  title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}
# InternLM
@article{cai2024internlm2,
  title={Internlm2 technical report},
  author={Cai, Zheng and Cao, Maosong and Chen, Haojiong and Chen, Kai and Chen, Keyu and Chen, Xin and Chen, Xun and Chen, Zehui and Chen, Zhi and Chu, Pei and others},
  journal={arXiv preprint arXiv:2403.17297},
  year={2024}
}
# QwenLM
@article{yang2024qwen2,
  title={Qwen2. 5 Technical Report},
  author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}
# Llama
@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

########################## Medical-LLM ##########################
# chatdoctor
@article{li2023chatdoctor,
  title={Chatdoctor: A medical chat model fine-tuned on a large language model meta-ai (llama) using medical domain knowledge},
  author={Li, Yunxiang and Li, Zihan and Zhang, Kai and Dan, Ruilong and Jiang, Steve and Zhang, You},
  journal={Cureus},
  volume={15},
  number={6},
  year={2023},
  publisher={Cureus Inc.}
}
# PMC-Llama
@article{wu2024pmc,
  title={PMC-LLaMA: toward building open-source language models for medicine},
  author={Wu, Chaoyi and Lin, Weixiong and Zhang, Xiaoman and Zhang, Ya and Xie, Weidi and Wang, Yanfeng},
  journal={Journal of the American Medical Informatics Association},
  pages={ocae045},
  year={2024},
  publisher={Oxford University Press}
}
# llava-med
@article{li2024llava,
  title={Llava-med: Training a large language-and-vision assistant for biomedicine in one day},
  author={Li, Chunyuan and Wong, Cliff and Zhang, Sheng and Usuyama, Naoto and Liu, Haotian and Yang, Jianwei and Naumann, Tristan and Poon, Hoifung and Gao, Jianfeng},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
# huatuo-o1
@article{chen2024huatuogpto1medicalcomplexreasoning,
  title={Huatuogpt-o1, towards medical complex reasoning with llms},
  author={Chen, Junying and Cai, Zhenyang and Ji, Ke and Wang, Xidong and Liu, Wanlong and Wang, Rongsheng and Hou, Jianye and Wang, Benyou},
  journal={arXiv preprint arXiv:2412.18925},
  year={2024}
}
@article{alkhaldi2024minigpt,
  title={Minigpt-med: Large language model as a general interface for radiology diagnosis},
  author={Alkhaldi, Asma and Alnajim, Raneem and Alabdullatef, Layan and Alyahya, Rawan and Chen, Jun and Zhu, Deyao and Alsinan, Ahmed and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:2407.04106},
  year={2024}
}
@article{thawkar2023xraygpt,
  title={Xraygpt: Chest radiographs summarization using medical vision-language models},
  author={Thawkar, Omkar and Shaker, Abdelrahman and Mullappilly, Sahal Shaji and Cholakkal, Hisham and Anwer, Rao Muhammad and Khan, Salman and Laaksonen, Jorma and Khan, Fahad Shahbaz},
  journal={arXiv preprint arXiv:2306.07971},
  year={2023}
}


@misc{ye2020weakly,
    title={Weakly Supervised Lesion Localization With Probabilistic-CAM Pooling},
    author={Wenwu Ye and Jin Yao and Hui Xue and Yi Li},
    year={2020},
    eprint={2005.14480},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@InProceedings{10.1007/978-3-031-73471-7_10,
author="Nisar, Hareem
and Anwar, Syed Muhammad
and Jiang, Zhifan
and Parida, Abhijeet
and Sanchez-Jacob, Ramon
and Nath, Vishwesh
and Roth, Holger R.
and Linguraru, Marius George",
editor="Deng, Zhongying
and Shen, Yiqing
and Kim, Hyunwoo J.
and Jeong, Won-Ki
and Aviles-Rivero, Angelica I.
and He, Junjun
and Zhang, Shaoting",
title="D-Rax: Domain-Specific Radiologic Assistant Leveraging Multi-modal Data and eXpert Model Predictions",
booktitle="Foundation Models for General Medical AI",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="91--102",
isbn="978-3-031-73471-7"
}
@article{wu2021chest,
  title={Chest imagenome dataset for clinical reasoning},
  author={Wu, Joy T and Agu, Nkechinyere N and Lourentzou, Ismini and Sharma, Arjun and Paguio, Joseph A and Yao, Jasper S and Dee, Edward C and Mitchell, William and Kashyap, Satyananda and Giovannini, Andrea and others},
  journal={arXiv preprint arXiv:2108.00316},
  year={2021}
}

@article{wu2024medical,
  title={Medical long-tailed learning for imbalanced data: bibliometric analysis},
  author={Wu, Zheng and Guo, Kehua and Luo, Entao and Wang, Tian and Wang, Shoujin and Yang, Yi and Zhu, Xiangyuan and Ding, Rui},
  journal={Computer Methods and Programs in Biomedicine},
  pages={108106},
  year={2024},
  publisher={Elsevier}
}
@inproceedings{rajbhandari2020zero,
  title={Zero: Memory optimizations toward training trillion parameter models},
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--16},
  year={2020},
  organization={IEEE}
}

@article{he2024pefomed,
  title={Pefomed: Parameter efficient fine-tuning on multimodal large language models for medical visual question answering},
  author={He, Jinlong and Li, Pengfei and Liu, Gang and Zhao, Zixu and Zhong, Shenjun},
  journal={arXiv preprint arXiv:2401.02797},
  year={2024}
}