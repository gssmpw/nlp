\section{Related Work}
\noindent \textbf{Large Vision-Language Models} ~ ~ Large Vision-Language Models, which integrate vision encoders, connectors, and large language models to enhance cross-modal understanding, have emerged as powerful frameworks that combine visual and textual information for a wide range of tasks. These models can be systematically categorized based on the type of connector. The first category comprises approaches utilizing cross-attention-based connectors, such as Flamingo~\cite{alayrac2022flamingo} and CogVLM~\cite{wang2023cogvlm,hong2024cogvlm2}, which exploit attention mechanisms to facilitate the exchange of information between the vision encoder and the language model. The second category includes methods that employ query-based connectors, such as BLIP-2~\cite{li2023blip}, Instruct-BLIP~\cite{dai2023instructblip}, mPLUG-owl2~\cite{ye2024mplug}, and Qwen-VL~\cite{bai2023qwen}, wherein queries are leveraged to orchestrate the interaction between visual and textual modalities, thereby enhancing the alignment and coherence of visual and linguistic representations. Furthermore, projection-based connector methods, exemplified by LLaVA~\cite{liu2023llava}, Mini-GPT4~\cite{zhu2023minigpt}, DeepSeek-VL~\cite{lu2024deepseek}, and Mini-Gemini~\cite{li2024mini}, project visual data into a shared embedding space, thereby fostering seamless integration with textual information.
These innovations offer a range of solutions for cross-modal understanding, driving the potential applications of intelligent systems in multi-task learning.

% The innovation of various methods offers a range of solutions for cross-modal understanding, thereby driving the potential applications of intelligent systems in multi-task learning.


\noindent \textbf{MedVQA Dataset on radiology} ~ ~ Medical visual question answering (MedVQA) datasets play a pivotal role in advancing AI-driven clinical decision-making. VQA-RAD~\cite{Lau2018}, as an early pioneering work, introduces a meticulously curated dataset for radiology images, featuring clinician-generated questions and corresponding answers tailored to clinically relevant tasks. SLAKE~\cite{liu2021slake} stands out as a large, bilingual dataset, enriched with extensive semantic annotations and spanning a wide range of radiological modalities. 

MIMIC-CXR~\cite{johnson2019mimic} provides a vast collection of 371,920 chest X-rays from 65,079 patients, serving as the foundation for numerous subsequent studies. The comparison of these datasets is shown in Table \ref{tab:dataset-comparison}.  MIMIC-CXR-VQA~\cite{bae2024ehrxqa} seamlessly integrates chest X-rays with Electronic Health Records (EHRs), facilitating multi-modal question answering with an emphasis on region-specific queries. The Medical-Diff-VQA~\cite{hu2023medical} is notable for its inclusion of seven distinct question types, particularly focusing on the comparative analysis of current and reference images for diagnostic purposes. GEMeX~\cite{liu2024gemex} offers a large-scale, explainable VQA benchmark, complete with detailed visual and textual explanations, thus addressing the growing need for a diverse array of clinical questions. Most of these datasets primarily focus on observations from a single image. While Medical-Diff-VQA contains 131k QA pairs that address multi-image changes, its structure is relatively simple and only accounts for global-level differences. Our MMXU-\textit{dev} dataset is the first to understand complex changes in the same regions across multiple chest X-ray images of the same patient at the regional level, spanning several visits.