\section{Related Work}
\noindent \textbf{Large Vision-Language Models} ~ ~ Large Vision-Language Models, which integrate vision encoders, connectors, and large language models to enhance cross-modal understanding, have emerged as powerful frameworks that combine visual and textual information for a wide range of tasks. These models can be systematically categorized based on the type of connector. The first category comprises approaches utilizing cross-attention-based connectors, such as Flamingo____ and CogVLM____, which exploit attention mechanisms to facilitate the exchange of information between the vision encoder and the language model. The second category includes methods that employ query-based connectors, such as BLIP-2____, Instruct-BLIP____, mPLUG-owl2____, and Qwen-VL____, wherein queries are leveraged to orchestrate the interaction between visual and textual modalities, thereby enhancing the alignment and coherence of visual and linguistic representations. Furthermore, projection-based connector methods, exemplified by LLaVA____, Mini-GPT4____, DeepSeek-VL____, and Mini-Gemini____, project visual data into a shared embedding space, thereby fostering seamless integration with textual information.
These innovations offer a range of solutions for cross-modal understanding, driving the potential applications of intelligent systems in multi-task learning.

% The innovation of various methods offers a range of solutions for cross-modal understanding, thereby driving the potential applications of intelligent systems in multi-task learning.


\noindent \textbf{MedVQA Dataset on radiology} ~ ~ Medical visual question answering (MedVQA) datasets play a pivotal role in advancing AI-driven clinical decision-making. VQA-RAD____, as an early pioneering work, introduces a meticulously curated dataset for radiology images, featuring clinician-generated questions and corresponding answers tailored to clinically relevant tasks. SLAKE____ stands out as a large, bilingual dataset, enriched with extensive semantic annotations and spanning a wide range of radiological modalities. 

MIMIC-CXR____ provides a vast collection of 371,920 chest X-rays from 65,079 patients, serving as the foundation for numerous subsequent studies. The comparison of these datasets is shown in Table \ref{tab:dataset-comparison}.  MIMIC-CXR-VQA____ seamlessly integrates chest X-rays with Electronic Health Records (EHRs), facilitating multi-modal question answering with an emphasis on region-specific queries. The Medical-Diff-VQA____ is notable for its inclusion of seven distinct question types, particularly focusing on the comparative analysis of current and reference images for diagnostic purposes. GEMeX____ offers a large-scale, explainable VQA benchmark, complete with detailed visual and textual explanations, thus addressing the growing need for a diverse array of clinical questions. Most of these datasets primarily focus on observations from a single image. While Medical-Diff-VQA contains 131k QA pairs that address multi-image changes, its structure is relatively simple and only accounts for global-level differences. Our MMXU-\textit{dev} dataset is the first to understand complex changes in the same regions across multiple chest X-ray images of the same patient at the regional level, spanning several visits.