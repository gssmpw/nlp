\section{Model}
\label{S:model}

%\subsection{Multi-Task Markov Decision Processes}

We consider the following general model of \emph{multi-task MDPs (MT-MDP)}.
%, which we show presently can effectively capture a number of important RL problem domains.
Suppose we have a \emph{dynamic environment} $\mc{E} = (\mc{S},\mc{A},h,\gamma,\rho)$ where $\mc{S}$ is a state space, $\mc{A}$ an action space,  $h$ the decision horizon, $\gamma$ the discount factor, and $\rho$ the initial state distribution.
Let a \emph{task} $\tau = (\mc{T},r)$ in which $\mc{T}$ is the transition model where $\mc{T}(s,a)$ is a probability distribution over next state $s'$ as a function of current state-action pair $(s,a)$ and $r(s,a)$ the reward function.
%, and $\gamma$ the temporal discount factor.
A Markov decision process (MDP) is thus a composition of the dynamic environment and task, $(\mc{E},\tau)$.

Let $\Gamma$ be a distribution over tasks $\tau$.
We define a \emph{MT-MDP} $\mc{M}$ as the tuple $(\mc{E},\Gamma)$, as in typical meta-RL models~\citep{beck2023survey,wang2024theoretical}.
%borrowing the traditional language in multi-task RL, but with the substantive difference that we use the distribution $\Gamma$ as the target of learning, rather than a finite set of target tasks, as in meta-learning~\citep{chen2021generalization,wang2024theoretical}.
%This naturally facilitates a discussion of \emph{generalization} for MT-MDP in the sense of efficacy on previously unobserved tasks, akin to theoretical frameworks for meta-learning~\citep{chen2021generalization,wang2024theoretical}.
Additionally, we define a \emph{finite-sample} variant of MT-MDP, \emph{FS-MT-MDP}, as $\mc{M}_n = (\mc{E},\tau_1,\ldots,\tau_n)$, where $\tau_i \sim \Gamma$. 
An FS-MT-MDP thus corresponds to multi-task RL~\citep{zhang2021survey}.

%\subsection{Learning a Policy Committee} 

%In conventional RL, the goal of learning is to obtain a policy $\pi(s)$ which maps states $s$ to actions $a$ so as to maximize total discounted reward,
%\[
%V^\pi = \mathbb{E}\left[\sum_{t=0}^h \gamma^t r(s_t,a_t)|a_t = \pi(s_t)\right],
%\]
%where the expectation is with respect to $\mc{T}$ and $\rho$.

%Now, this is a natural goal for a single task; MT-MDP, however, involves a distribution over tasks.

\begin{comment}
    At the high level, our goal is to learn a \emph{committee of policies} $\Pi = \{\pi_1,\ldots,\pi_K\}$ such that for most tasks, there exists at least one policy $\pi \in \Pi$ that is effective.
\end{comment}

At the high level, our goal is to learn a \emph{committee of policies} $\Pi$ such that for most tasks, there exists at least one policy $\pi \in \Pi$ that is effective.\footnote{Note that this also admits policies that can further depend on task embeddings, with this dependence being different in different clusters.}
%Note that each committee member could be a network that corresponds to different policies for different tasks.
Next, we formalize this problem.
%If we have a FS-MT-MDP $\mc{M}_n$ for a sample of $n$ tasks, $K=n$ yields conventional RL where we learn a policy independently for each task.
%However, most interesting cases involve $n > K$.
%For example, generalizing to tasks only seen at decision time may necessitate generating many tasks (large $n$) for training.
%However, even RL for a given task is a complex and time consuming matter, and thus we either expect to have a hard limit on $K$ (how many policies we can practically train, or wish to train given resource constraints), or would like to keep $K$ as small as possible without significantly compromising performance.
%To address this, 

Let $V_\tau^\pi$ be the value of a policy $\pi$ for a given task $\tau$, i.e.,
\[
V_\tau^\pi = \mathbb{E}\left[\sum_{t=0}^h \gamma^t r_\tau(s_t,a_t)|a_t = \pi(s_t)\right],
\]
where the expectation is with respect to $\mc{T}_\tau$ and $\rho$.
Let $V^*_\tau$ denote an optimal policy for a task $\tau$.
Define $V_\tau^\Pi = \max_{\pi \in \Pi} V_\tau^\pi$, that is, we let the value of a committee $\Pi$ to a task $\tau$ be the value of the \emph{best} policy in the committee for this task.
There are a number of reasons why this evaluation of a committee is reasonable.
As an example, if a policy implements responses to prompts for conversational agents and $\Pi$ is small, we can present multiple responses if there is significant semantic disagreement among them, and let the user choose the most appropriate.
In control settings, we can rely on domain experts who can use additional semantic information associated with each $\pi \in \Pi$ and the tasks, such as the descriptions of tasks $\pi$ was effective for at training time, and similar descriptions to test-time tasks, to choose a policy.
Moreover, as we show in Section~\ref{S:fewshot}, this framework leads naturally to effective few-shot adaptation, which requires neither user nor expert input to determine the best policy.

%For example, given a committee of policies, an expert may use prior knowledge and experience to determine which of the candidate policies is most appropriate in their given situations.
%As we shall see below, we can often provide useful auxiliary information to this end that is endogenous to our proposed algorithmic approaches.
%Alternatively, an expert may briefly experiment with each policy offline on tasks that are closely related to the target task, and then choose the best-performing one at decision time.
%This can be viewed as a form of few-shot adaptation in meta-RL~\citep{}, and we discuss it further below.

One way to define the value of a policy committee $\Pi$ with respect to a given MT-MDP and FS-MT-MDP is, respectively, as $V^\Pi_{\mc{M}} = \mathbb{E}_{\tau \sim \Gamma} \left[V_\tau^\Pi\right]$ and $V_{\mc{M}_n}^\Pi = \frac{1}{n}\sum_{i=1}^n V_{\tau_i}^\Pi$.
%\begin{equation}
    %\label{E:committee_value}
%    V^\Pi_{\mc{M}} = \mathbb{E}_{\tau \sim \Gamma} \left[V_\tau^\Pi\right] \quad \mathrm{and} \quad V_{\mc{M}_n}^\Pi = \frac{1}{n}\sum_{i=1}^n V_{\tau_i}^\Pi.
%\end{equation}
%There are two issues with this definition, however.
The key problem with these learning goals is that  
%First, it only captures \emph{average} (or expected) performance on tasks; for some of the tasks, performance can in fact be quite poor.
%This is consequential when tasks are associated with people (e.g., users), as the result can be extremely unfair to some.
%Second, if 
when the set of tasks is highly diverse, different tasks can confound learning efficacy for one another.
For example, suppose that we have five tasks corresponding to target velocities of $10, 12, 20, 22, 100$, and the task succeeds if a policy implements its target velocity sufficiently closely (say, within $1$).
If we either train a single policy for all tasks, or divide them into two clusters, the outlier target velocity of $100$ will confound training for the others.
More generally, if any policy is trained on a set of tasks that require different skills (e.g., a cluster of tasks that includes outliers), the conflicting reward signals will cause negative transfer and poor performance.
%For example, if we have several groups of tasks such that within-group tasks are quite similar to one another, but with tasks differing significantly (e.g., requiring fundamentally different skills) across groups, learning a single policy that is effective for all tasks will be extremely challenging, with tasks from different groups sending conflicting reward signals.
%outlier tasks can serve to confound learning efficacy on the rest.
%there may be some outlier tasks that are rare, and impossible to perform well except using policies customized to these.
%For a sufficiently small $K$ (e.g., $K=1$), then, we may face a tradeoff between learning a committee of policies that are mediocre on all tasks (because the presence of the outlier task during training causes poor policies to be learned for the rest), and learning a committee that is highly performant on nearly all tasks, but fails on a few outliers. \luise{Later we shall demonstrate the advantage of our algorithms by presenting the total statistics of individual utilities through histograms like Figure~\ref{fig:hist_4mode_K3E3}.}

%We address these diverse considerations by proposing two general objectives for MT-MDP and FS-MT-MDP.
%In the first, we have a fixed $K$, and aim to learn a committee that performs well on as many tasks as possible.
%In the second, we are given a target fraction (probability) of tasks that must have high performance, and identify the smallest $K$ to achieve this goal.

We address this limitation by defining the goal of policy committee learning differently.
First, we formalize what it means for a committee $\Pi$ to have a \emph{good} policy for \emph{most} of the tasks.
%we define our key concept of an %$(\epsilon,1-\delta)$-cover for a policy committee $\Pi$ that enables broad flexibility for constructing such committees depending on domain and application needs. 
\begin{definition}
\label{D:committee-cover}
A policy committee $\Pi$ is an \emph{$(\epsilon,1-\delta)$-cover} for a \emph{task distribution} $\Gamma$ if $V_\tau^\Pi \ge V_\tau^* - \epsilon$ with probability at least $1-\delta$ with respect to $\Gamma$.  $\Pi$ is an \emph{$(\epsilon,1-\delta)$-cover} for \emph{a set of tasks} $\{\tau_1,\ldots,\tau_n\}$ if $V_\tau^\Pi \ge V_\tau^* - \epsilon$ for at least a fraction $1-\delta$ of tasks.
\end{definition}
Clearly, an $(\epsilon,1-\delta)$ cover need not exist for an arbitrary committee $\Pi$ (if the committee is too small to cover enough tasks sufficiently well).
There are, however, three knobs that we can adjust: $K$, $\epsilon$, and $\delta$.
Next, we fix $\epsilon$ as exogenous, treating it effectively as a domain-specific hyperparameter, and suppose that $K$ is a pre-specified bound on the maximum size of the committee.
%, and use the remaining flexibility to define two problems in MT-MDP: 

\begin{problem}
    \label{P:fix_K}
    Fix the maximum committee size $K$ and $\epsilon$.  Our goal is to find $\Pi$ which is a $(\epsilon,1-\delta)$-cover for the smallest $\delta \in [0,1]$.
\end{problem}

%\begin{problem}
%    \label{P:fix_delta}
%    Fix the coverage goal $\delta$ and $\epsilon$.  Our goal is to find $\Pi$ which is a $(\epsilon,1-\delta)$-cover for the smallest committee size $K \in [1..n]$.
%\end{problem}
%In the second problem statement, 
%Here, $[1..n]$ denotes a set of all integers between $1$ and $n$.
%Note that if we aim to cover a finite set of tasks $\{\tau_1,\ldots,\tau_n\}$ (a typical case when we design practical algorithms), the problem is feasible in the sense that 
%both problems are feasible for any $\epsilon > 0$.
%In the case of Problem~\eqref{P:fix_K}, $\delta=1$ is trivially achievable.
%For Problem~\eqref{P:fix_delta}, it is feasible for any $\delta \ge 0$, since $K=n$ allows us to learn an optimal policy for each task $\tau$, which is then an $(\epsilon,0)$-cover for all tasks.

\iffalse
A natural variation is to maximize efficacy of $\Pi$ on a fraction $1-\delta$ of policies in the following sense for FS-MT-MDP:
\begin{equation}
    \label{E:p2_alt}
    V_{n,(1-\delta)}^\Pi = \frac{1}{n(1-\delta)}\sum_{i=1}^{n(1-\delta)} \{V_{\tau_i}^\Pi\}_{(i)},
\end{equation}
where $\{V_{\tau_i}^\Pi\}$ is an ordered set, and $\{V_{\tau_i}^\Pi\}_{(i)}$ is the $i$th item in descending order.
The counterpart $V_{n,(1-\delta)}^\Pi$ is the expectation of the top $1-\delta$ percentile of tasks $\tau$ in terms of their committee value functions $V_\tau^\Pi$.
We can view Problems~\ref{P:fix_K} and~\ref{P:fix_delta} as proxies for maximizing Objective~\ref{E:p2_alt}; for example, the former is a natural proxy if we fix $K$.
Henceforth, our theoretical and algorithmic discussion will largely focus on Problems~\ref{P:fix_K} and~\ref{P:fix_delta}; in our experiments, we show that this turns out to be a good proxy in many cases for Objective~\ref{E:p2_alt}, and even for optimizing $V^\Pi$.
Next, we illustrate the generality afforded by our model, as well as the notion of $(\epsilon,1-\delta)$-cover by mapping this to the important research subfields or applications of RL.
\fi

As a corollary to Theorem 2.6 in \citet{skalse2023misspecification}, we now present sufficient conditions under which a policy committee can yield higher expected rewards than a single policy. The precise definition of these conditions is provided in Appendix~\ref{A:starc}.
%In real life, most tasks would be different enough that their reward functions won't satisfy these restrictive conditions.

\begin{corollary}[Theorem 2.6, \cite{skalse2023misspecification}] \label{cor:starc}
   Unless two tasks $\tau_1, \tau_2$ have only their reward functions $r_1$ and $r_2$ differ by potential shaping, positive linear scaling, and $S_1$-redistribution, we have $V^{\Pi}_{\tau_1} \ge V^{\pi}_{\tau_1}$, $V^{\Pi}_{\tau_2} \ge V^{\pi}_{\tau_2}$, and $V^{\Pi}_{\tau_1}+V^{\Pi}_{\tau_1} > V^{\pi}_{\tau_1}+ V^{\pi}_{\tau_2}$ for any single policy $\pi$.
\end{corollary}

Next, we present algorithmic approaches for this problem.
Subsequently, Section~\ref{S:fewshot}, as well as our experimental results, vindicate our choice of this objective.
%serve to justify the choice of this objective.