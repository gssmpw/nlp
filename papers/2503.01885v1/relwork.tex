\noindent\textbf{Related Work: }
%Our work is closely connected to three strands in the broad RL literature: multi-task RL, meta-RL, and personalized RL.
%\luise{tension between negative interference and generalisation: Several works have focused on the problem of negative interference (Du et al., 2018; Suteu & Guo, 2019; Yu et al., 2020a) where the gradients corresponding to the different tasks interfere negatively with each other. }
%\cite{chu2024meta}Meta-Reinforcement Learning via Exploratory Task Clustering
Our work is closely related to three key areas within the broader reinforcement learning literature: multi-task RL, meta-RL, and personalized RL.

\noindent\textit{Multi-Task RL (MTRL):}
%A major advantage of MTRL over single-task learning is the ability to share knowledge across tasks, a concept extensively explored in various studies proposing different methods to utilize task relationships
The broad goal of MTRL approaches is to leverage inter-task relationships to learn policies that are effective across multiple tasks~\citep{yang2020multi,sodhani2021multi,sun2022paco}. 
%However, naive knowledge-sharing across tasks can lead to negative transfer, as not all tasks benefit from shared knowledge. Consequently, learning a task-specific skill may distract from the learning of other tasks. 
An important challenge in MTRL is task interference.
One class of approaches aims to mitigate this issue through gradient alignment techniques~\citep{hessel2019multi,yu2020gradient}.
%A notable area of research examines task interference in MTRL through the lens of gradient alignment. 
%\citet{yu2020gradient} tackles it by projecting the gradient of a task to the orthogonal direction of all the other tasks, while~\citet{hessel2019multi} addresses it via synchronizing the gradient magnitude across tasks. 
An alternative series of approaches addresses this issue
%Numerous methods in the literature aim to address task interference issues 
from a representation learning perspective, enabling the learning of policies that explicitly condition on task embeddings~\cite{hendawy2024multi,lan2024contrastive,sodhani2021multi}.
%\citet{sodhani2021multi} learn a mixture of state encoders shared across tasks, that helps generate diverse representations through an attention mechanism.
%\citet{lan2024contrastive} introduce the Contrastive Modules with Temporal Attention (CMTA) framework, which leverages contrastive learning to ensure the modules are distinct from one another and integrates shared modules at a finer granularity than the task level using temporal attention. 
%Recently,~\citet{hendawy2023multi} proposed an approach called Mixture of Orthogonal Experts (MOORE) that captures common structures among tasks by employing orthogonal representations to enhance diversity. MOORE utilizes a Gram-Schmidt process to create a shared subspace of representations derived from a mixture of experts. 
However, most MTRL methods still face challenges when tasks are diverse, particularly when it comes to generalizing to \emph{previously unseen tasks}.
%While all previous MTRL approaches focus on learning a policy to efficiently address a predefined set of tasks, our focus is to learn a set of policies such that at least one policy in the set is near-optimal for most \emph{previously unseen tasks}.

\noindent\textit{Meta-RL:}
The goal of meta-RL is an ability to quickly adapt to unseen tasks---what we refer to as \emph{few-shot learning}.
Meta-RL methods can be categorized broadly into two categories: (i) gradient-based and (ii) context-based (where context may include task-specific features).
Gradient-based approaches 
%have been developed to address the few-shot adaptation challenge. These approaches 
focus on learning a shared initialization of a model across tasks that is explicitly trained to facilitate few-shot learning~\citep{finn2017model,stadie2018importance,mendonca2019guided,zintgraf2019fast}.
However, they perform poorly in zero-shot generalization, and tend to require a large number of adaptation steps.
Context-based methods learn a context representation and use it as a policy input~\citep{bing2023meta,gupta2018meta,duan2016rl,lee2020stochastic,lee2020context,lee2023parameterizing,rakelly2019efficient}. %by employing RNN or LSTM-based neural networks to encode collected experiences into a latent context embedding, and then act by conditioning the policy on the learned context. 
However, these approaches often exhibit mode-seeking behavior and struggle to generalize, particularly when the number of training tasks is small.
While some recent approaches, such as \citet{bing2023meta}, attempt to improve performance by using natural language task embedding, they still require a large number of training tasks to succeed.
%However, they are susceptible to distribution shifts at inference time, as the encoded context and the policy derived from that context often struggle to generalize to out-of-distribution tasks. 
%Additionally, the parameters of the latent context encoder are trained to predict reward and/or transition dynamics based on the context, typically involving the minimization of a KL divergence-based loss. 
%Consequently, the learned context tends to exhibit mode-seeking behavior, which poses a significant limitation in situations that require capturing diverse, multi-modal context (such as in Meta-World). 
%Recently, \citet{bing2023meta} attempt to address this issue in non-parametric tasks by using task-specific detailed natural language instructions, but this approach still suffers from poor sample efficiency with respect to training tasks.
%Several gradient-based methods
%, allowing the agent to achieve strong performance on unseen target tasks with only a few gradient updates. 
%These approaches are not well-suited for zero-shot generalization problems, as they typically require numerous gradient steps through the policy to learn an effective policy for a given task. Finally, since meta-RL methods prioritize rapid adaptation, they often fall short of state-of-the-art MTRL performance on in-sample (training) tasks. In this work, we aim to close this gap by developing a framework that excels in both in-sample and out-of-sample tasks.

\noindent\textit{Personalized RL:}
\citet{ackermann2021unsupervised} and~\citet{ivanov2024personalized} proposed addressing task diversity by clustering RL tasks and training a distinct policy for each cluster.
Both use EM-style approaches to jointly cluster the tasks and learn a set of cluster-specific policies.
Our key contribution is to leverage an explicit parametric task representation, and reformulate the objective as a flexible  \emph{coverage} problem for an unknown distribution of tasks.
This enables us to achieve task sample efficiency both in theory and practice.
%, each with distinct preferences, through interaction with a small set of representative policies. 
%Although the personalized RL framework has some similarities to our approach, we adopt a broader setup allowing for variations both in rewards and transition dynamics, since many real-world scenarios warrant mastering a diverse set of tasks that comprise different dynamics.
%often necessitates acting optimally amidst varying transition dynamics and preferences. 
In particular, we empirically show that our \textsc{pacman} method significantly outperforms the state-of-the-art personalized RL approach.
%across diverse evaluation settings even when only rewards vary. 

\noindent\textit{Reward-Free RL:} Another related line of research is reward-free RL~\citep{agarwal2023provable,cheng2022provable,jin2020reward}.
However, results in this space make strong assumptions, such as assuming that the action space is finite or linear dynamics.
Our results, on the other hand, make no assumptions on the dynamics and do not depend on the size or dimension of the state or action space.
%A common example is that the action space is typically assumed to be finite. The papers that do not assume this require instead linearity (of both the dynamics and rewards), and/or make assumptions (such as low-rank) on the transition model. 
%We do not assume linearity, make no assumptions about the nature of the dynamics, and allow actions to be either discrete or in a continuous vector space (none of our results depend on the size or dimension of state or action space). Thus, one of our key results in few-shot generalization depends only on the number of policies 
 %(i.e., sample complexity is linear in 
%, along with other problem-specific features), but has no dependence on state or action space dimension.