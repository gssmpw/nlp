\section{Introduction}
Reinforcement learning (RL) has achieved remarkable success in a variety of domains, from robotic control~\citep{lillicrap2015continuous} to game playing~\citep{xu2018meta}. However, many real-world applications involve highly diverse sets of tasks, making it impractical to rely on a single, fixed policy. In these settings, both the reward structures and the transition dynamics can vary significantly across tasks. Existing approaches, 
%to this challenge
such as multi-task RL (MTRL) and meta-reinforcement learning (meta-RL), struggle to generalize effectively for diverse and previously unseen tasks.

Multi-task RL methods typically train a single policy or a shared representation across tasks~\citep{vithayathil2020survey}. However, they often face negative transfer, where optimizing for one task degrades performance on others~\citep{zhang2022survey}. 
%This limitation becomes more pronounced when tasks require drastically different strategies, as the policy is forced to handle conflicting objectives. 
On the other hand, meta-RL approaches, such as Model-Agnostic Meta-Learning (MAML)~\citep{finn2017model} and PEARL~\citep{rakelly2019efficient}, aim to enable fast adaptation to new tasks but rely heavily on fine-tuning at test time, which can be computationally expensive and ineffective in environments with high variability in both rewards and transitions, like Meta-World benchmark tasks~\citep{yu2021metaworldbenchmarkevaluationmultitask}. 
%Furthermore, these methods typically underperform on the training tasks compared to MTRL due to the generalization trade-off inherent in meta-learning. 
%, but this line of work focuses predominantly on rewards while assuming shared transition dynamics.  

% TODO: Put a more convincing example and figure to show limitations of MTRL and Meta-RL and provide an intuitive motivation for our approach 

%According to \emph{script theory}, human cognition enables effective generalization in an open world by learning a collection of \emph{scripts}, or behavioral patterns~\citep{abelson1981psychological,schank1983dynamic}, one of which can then be selected and adapted as needed to particular predicaments. 
Several lines of work attempt to address the problem of diverse tasks and negative transfer.
The first is to learn policies through MTRL or meta-RL that explicitly take task representation as input~\citep{lan2024contrastive,grigsby2024amagoscalableincontextreinforcement,sodhani2021multi,zintgraf2020varibad}.
However, such approaches rely on shared parameters, which limit the model's flexibility. When tasks are highly diverse, using a single neural network— even with a multi-head architecture— may not generalize effectively. Moreover, the greater the task variation, the more data is typically required to train policies that can effectively condition on task embeddings, and the model will often generalize poorly when the number of training tasks is small.

%Recently, several approaches proposed addressing the problem of diverse tasks and negative transfer 
Several alternative methods have thus emerged that propose clustering tasks and training distinct policies for each cluster~\citep{ackermann2021unsupervised,ivanov2024personalized}.
The associated algorithmic approaches 
%These approaches 
commonly leverage EM-style methods that interleave RL and task clustering.
In practice, however, they also require a large number of training tasks to be effective and consequently perform poorly when the number of training tasks is small.
In addition, if the number of clusters is too small, some clusters may still contain too diverse a set of tasks, with concomitant negative transfer remaining a significant issue.
%and in practice underperform conventional multi-task and meta-RL methods when the number of training tasks is small, as we show in the experiments.

We propose \textsc{pacman}, a novel framework and algorithmic approach for learning {\bf policy committees} that enables (a) sample efficient generalization (Theorem~\ref{thm:greedy_gaurantee_final}), (b) few-shot adaptation guarantees that are \emph{independent of the dimension of the state or action space} (Theorem~\ref{thm:online_repetition}), and (c) significant improvements in performance compared to 11 state-of-the-art baselines on both MTRL and few-shot adaptation metrics (Section~\ref{S:exp}).
The key idea behind \textsc{pacman} is to leverage a parametric task representation which allows clustering tasks in the parameter space first, followed by reinforcement learning for each cluster. The benefit of our approach is illustrated in Figure~\ref{fig:intro_comparison}: even though the single-policy baseline uses a mixture-of-experts approach~\citep{hendawy2024multi}, its performance is poor since tasks require fundamentally distinct skills.
In contrast, learning a policy committee allows learning custom policies for distinct classes of tasks.
%is a mixture-of-experts
%while the baseline MOORE~\citep{hendawy2024multi} has already adopted a mixture of orthogonal experts for state encoding, the lack of flexibility to decide which tasks to tackle 
%in the first place
% while it helps handling diverse in sample tasks, but it struggle to tackle ood tasks as these orthogonal represntations are optimized with in sample tasks
%still 
%leads to mediocre behavior for tasks requiring distinct skills --- ``Drawer Close" versus ``Door open'' for instance. By forming a committee instead, we were able to assign a highly skilled member to each task, improving overall performance across tasks.

\begin{figure}[t] % 't' for top of the page
    \centering
    \includegraphics[width=0.4\textwidth]{main_texts/figures/figg1.png}
    \caption{Performance on a single task across committee members compared to a MTRL policy.}
    \label{fig:intro_comparison}
\end{figure}
A crucial insight in our approach is to define the objective of clustering as obtaining \emph{high coverage} of the (unknown) task distribution, as opposed to \emph{insisting on a full coverage}---that is, achieving near-optimal performance on a randomly drawn task with high probability.
This enables us to devise efficient algorithms with provable performance guarantees and polynomial task sample complexity, as well as obtain few-shot adaptation guarantees that depends only on the number of clusters, but not on the size of state and action space.
%This yields an approach that is highly sample efficient, and is thus effective even with a relatively small number of training tasks.
%Although parametric representations have been used in multi-task and meta-RL which train policies that take task parameters as an input, these also require a large number of training tasks to be effective, whereas we show in the experiments that \textsc{pacman} is far more sample efficient.
Moreover, while many MTRL and meta-RL problems of interest are non-parametric, we show that we can leverage high-level natural language task descriptions to obtain highly effective embeddings of tasks, akin to \citet{sodhani2021multi}, and \citet{hendawy2024multi}, but without requiring subtask decomposition.
%like \citet{bing2023meta}.
Consequently, our approach can be effectively applied to a broad class of non-parametric MTRL and meta-RL domains as well.

%Inspired by this concept, we propose a novel {\bf policy committee framework} designed to efficiently handle environments with {\bf diverse tasks} in the sense that both reward functions and transition dynamics can vary significantly across tasks. Instead of learning a single policy or relying on complex fine-tuning, our framework learns a set of policies—a committee—where each policy is specialized to handle a specific subset of tasks. This allows for task-specific expertise while maintaining generalization across a wide range of task variations.
%We refer to our approach as \textsc{pacman}.

%The idea of clustering tasks has been recently explored by \citet{ackermann2021unsupervised} and \citet{ivanov2024personalized}, who leverage EM-style methods to cluster tasks during reinforcement learning.

%To summarize, our key contributions are as follows:
In summary, we make the following contributions:
\begin{itemize}[itemsep=0pt,topsep=0pt,leftmargin=*]

    \item {\bf Theoretically Grounded Framework for Learning Policy Committees:} We present the first approach for learning policy committees in MTRL and meta-RL domains that offers provable performance guarantees, in addition to practical algorithms. In particular, we provide sample efficient generalization (Theorem~\ref{thm:greedy_gaurantee_final}) and few-shot adaptation guarantees that are independent of the size of the state or action space (Theorem~\ref{thm:online_repetition}).

    \item {\bf Simple and Effective Policy Committees}: \textsc{pacman} is design to handle high task diversity while using a small number of policies to achieve high efficacy.
    %Specifically, by learning a small set of policies that cover a broad range of tasks, we reduce the computational complexity compared to methods that require a separate policy for each task or complex adaptation procedures. 
    Additionally, our approach leverages LLM-based task embeddings for non-parametric tasks, which provides a highly general and scalable solution for a broad array of environments.
    %to environments where tasks cannot be easily parameterized.
    
    %\item {\bf Theoretical Analysis:} We first provide a general computational impossibility result, showing that even the problem of identifying the optimal sets of tasks for policy committee training is inapproximable. However, we also present an efficient algorithmic approach with worst-case approximation guarantees in the special case when task embedding dimension is constant, and a general gradient-based approach, albeit with weaker guarantees. Finally, we theoretically demonstrate few-shot efficacy of our approach by showing that it has 
    %sublinear regret with respect to the \emph{optimal} policy (not merely the best one in the committee), and 
    %sample complexity that is linear in the size of the committee and \emph{independent of the size of the state and action space}.
    %providing both regret and sample complexity bounds. In particular, we show that sample complexity of few-shot learning is linear in the size of the committee.
    
    \item {\bf Empirical Validation:}     We demonstrate the efficacy of \textsc{pacman} through extensive experiments on challenging multi-task benchmarks, including MuJoCo and Meta-World. %\luise{We designed experiments in a way to show \textbf{sample efficiency} of our method. Instead of a 45 and 5 train, test split we do 30 and 20 for Meta-World. And instead of constantly requiring new tasks from the task distribution for Mujoco-meta learing baseline, we fix the number of task samples to be only 100.} 
    Our policy committee framework consistently outperforms state-of-the-art multi-task RL, meta-RL, and task clustering baselines in both zero-shot (MTRL) and few-shot (meta-RL) settings, achieving better generalization and faster adaptation across diverse tasks.
\end{itemize}
