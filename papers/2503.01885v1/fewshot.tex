\subsection{Few-Shot Adaptation}

\label{S:fewshot}
A particularly useful consequence of learning a policy committee $\Pi$ that is a $(\epsilon,1-\delta)$-cover is that we can leverage it in meta-learning for few-shot adaptation. The algorithmic idea is straightforward: evaluate each of $K$ policies in $\Pi$ by computing a sample average sum of rewards over $N$ randomly initialized episodes, and choose the best policy $\pi \in \Pi$ in terms of empirical average reward.
%This yields the following sample complexity bound.

In particular, suppose that $\gamma = 1$.
We now show that this translates into a few-shot sample complexity on a previously unseen task $\tau$ that is linear in $K$ (the size of the committee). Details of the proof are in Appendix~\ref{A:adaptation}.
%\begin{definition}
    %We define the best expert as $\pi^* = \mathrm{argmax}_{\pi\in \Pi} V^{\pi}$, the expert which yields the highest expected average cumulative reward $V^{\pi}$ from its steady-state  $\mu_\pi$. The cumulative regret, $r(n)$ after $n$ iterations of a multi-armed bandit algorithm is defined as: $r(n)=nV^{*}-\sum_{k=0}^{n}\frac{1}{T_k}\sum_{t=t_k}^{t_{k+1}-1} r_t$ By Theorem~\ref{thm:linear_reg} $V^{\pi*}$ is at least $2LT\epsilon-$optimal to $V*$ since $V^{\pi*} \ge V^{\pi'} $ where $\pi'\in \Pi$ is inside the cover.\end{definition}

%\begin{theorem}(Regret decomposition identity). If the induced Markov chains for each policy committee member $\pi \in \Pi$ are irreducible and aperiodic, then the expected cumulative regret at time n can be bounded with:$\mathbb{E}[r(n)] \le \sum \mathbb{E}[T(n)][\delta_\pi+\frac{C_\pi}{T_0(1-\beta_pi)}]+\frac{C_*}{}(1-\beta_*)\sum_{k=0}^{n-1}\frac{1}{T_k}$ \end{theorem}


%TODO: formal bounds for few-shot adaptation; idea 1: $K$ policy evaluation steps followed by playing the best policy; what is the bound on regret?  How does it compare to regret bounds in standard RL?  Is there a UCB bound that we can use and treat policies in $\Pi$ as ``experts''? 

%Having solved the committee formation problem, we can now utilize the RLPA algorithm to quickly identify the best policy member in the committee with regret bound measured \emph{against the optimal policy for the underlying MDP}, rather than by the best policy in the committee.

%One application 


\iffalse
We can then leverage \emph{online learning} approaches which treat $\Pi$ as a set of policy \emph{experts}.
To illustrate, we can use the RLPA algorithm due to \citet{azar2013regret}, which combined with our methods above can yield provable regret bounds, as we show next.


\fi

\iffalse
\begin{theorem}\citep{azar2013regret}
\label{T:regret}
   Let $f$ be an increasing function and $S^+$ the span of the best policy in $\Pi$ with average reward $\mu_\Pi^*$.
   Under Assumption~\ref{A:online}, for any number of trials $N \ge f^{-1}(S^+)$ the regret of the RLPA algorithm with $K$ deterministic policies with respect to $\mu_\Pi^*$ is bounded by
$\Delta(s)\le 24(f(N)+1)\sqrt{3NK(\log(N/\alpha))}+\sqrt{N} +6f(N)K(\log_2(S^+)+2\log_2(N))$
with probability at least $1-\alpha$ for any initial state $s \in \mathcal{S}$.
\end{theorem}




\begin{corollary}\label{cor:RLPA}
Suppose $\Pi$ with $|\Pi|=K$ is an $(\epsilon,1-\delta)$-cover for $\Gamma$. Then under Assumption~\ref{A:online}, with probability at least $1-\delta-\alpha$, for any number of trials $N \ge f^{-1}(S^+)$, the regret of a  task $\tau \sim \Gamma$ with respect to the optimal average reward $\mu^* = V_\tau^*/h$ for $\tau$ is bounded by
$\Delta(s)\le 24(f(N)+1)\sqrt{3NK(\log(N/\alpha))}+\sqrt{N} +6f(N)K(\log_2(N^+)+2\log_2(N))+\frac{\epsilon}{h}$, for any $s\in \mathcal{S}$.
\end{corollary}
A notable aspect of this result is that while conventional regret in online learning, such as in Theorem~\ref{T:regret}, is measured with respect to the best policy in the (small) set of experts, we obtain low regret with respect to the \emph{optimal} policy for the task faced at execution time.
This is the key consequence of the committee $\Pi$ constituting a $(\epsilon,1-\delta)$ cover.
\fi

%on the efficacy of policy committees.
%For small $K$,  Azumaâ€™s inequality have guaranteed us high confidence that we can also evaluate each policy repeatedly and select the one with the highest reward.

%We now connect this result with the notion of $(\epsilon,1-\delta)$-cover that we have, noting that $V^\pi_\tau = h\mu^\pi_\tau$ for any $\pi,\tau$.
\begin{theorem}\label{thm:online_repetition}
Suppose $\Pi$ is a $(\epsilon,1-\delta)$-cover for $\Gamma$ and let $\tau \sim \Gamma$. Under some mild conditions,
if we run 
%the number of episodes ran for each policy 
$p \ge\frac{32h(H+1)^2\log(4/\alpha)}{(\beta-2H)^2}$ episodes for each policy $\pi \in \Pi$, where $H$ is a constant, the policy $\pi$ that maximizes the empirical return yields $V_\tau^\pi \ge V_\tau^* -\epsilon-\beta$ with probability at least $1-\delta-\alpha$, where $V_\tau^*$ is the optimal reward for $\tau$.
\end{theorem}

%\vspace{-15pt}

%\vspace{-5pt}
%While this result assumes that each policy $\pi \in \Pi$ has been fixed for the duration of adaptation (that is, we are only doing policy evaluation for each), in practice a simple improvement is to actually fine-tune each policy as we get more experience.
%This is the variation that we use in the experiments below.

%Notably, this suggests that a simple few-shot learning algorithm in which we perform policy evaluation for each of $K$ policies, and then use the one with the best empirical performance, will lead to effective few-shot learning.

