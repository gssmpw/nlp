\section{Experiments}
\label{S:exp}
%In this section we study the effectiveness of our methodology across zero and few shot adaptations.
%\subsection{Experiment Setup}
%\subsection{Environments}
We study the effectiveness of our approach---\textsc{pacman}---in two environments, \emph{MuJoCo}~\citep{todorov2012mujoco} and \emph{Meta-World}~\citep{pmlr-v100-yu20a}.
In the former, the tasks are low-dimensional and parametric, and we only vary the reward functions, whereas the latter has non-parametric robotic manipulation tasks with varying reward and transition dynamics.

\noindent\textbf{MuJoCo }
We selected two commonly used MuJoCo environments. The first is HalfCheetahVel where the agent has to run at different velocities, and rewards are based on the distance to a target velocity.
The second is HumanoidDir where the agent has to move along the preferred direction, and the reward is the distance to the target direction. 
In both, we generate diverse rewards by randomly generating target velocity and direction, respectively, and use 100 tasks for training and another 100 for testing (in both zero-shot and few-shot settings), with parameters generated from a Gaussian mixture model with 5 Gaussians.
In few-shot cases, we draw a single task for fine-tuning, and average the result over 10 tasks.
For clustering, we use $K=3, \epsilon = .6$, and use the gradient-based approach initialized with the result of the \emph{Greedy Intersection} algorithm.
For few-shot learning, we fine-tune all methods for 100 epochs. 

%We compare our method with both single-policy paradigms, i.e. the state of art meta-learning methods VariBAD and Rl2; as well the only multi-policy paradigm known to us from Personalization \cite{ivanov2024personalized}.

%The advantage of our method is that we can wrap our clustering method over the best-performing learning, which is VariBAD in our case. However, as we are aware of that VariBAD can solve low dimensional task distributions such as MuJoCo really well provided that they can train on \emph{infinite} tasks. However, our goal is to be sample efficient.

\noindent\textbf{Meta-World }
%It is a well-known multitask and meta-learning benchmark~\citep{pmlr-v100-yu20a}. 
We focus on the set of robotic manipulation tasks in MT50, of which we use \emph{30 for training and 20 for testing}.
This makes the learning problem significantly more challenging than typical in prior MTRL and meta-RL work, where training sets are much larger compared to test sets (5 tests and 40 trains in the traditional MT45 setting).
%There are 50 tasks that a robotic arm is required to perform, ranging from opening a door to picking up and placing an object. Compared to MuJoCo, an immediate issue is that there is no clear way to parameterize such a varied task space without interacting with the environment. 
We leverage an LLM  to generate a parameterization (Section~\ref{S:llmembedding}) of the task.
%In general we try to describe dynamics information in the description. 
Specifically, text descriptions (see Appendix~\ref{S:llmdesc}) are fed to \enquote{Phi-3 Mini-128k Instruct}~\citep{microsoft_phi3} and we compute the channel-wise mean over the features of penultimate layer as a 50 dimensional parameterization for each task. 
%We then take 20 tasks to be held out (referred to as test tasks) and we cluster the remaining 30 tasks by assigning to clusters following the objective given in  \ref{E:relaxK}. 
We use $K=3$ and $\epsilon = .7$.
%, which allows us to obtain an $(\epsilon,1)$-parameter-cover for the set of training tasks in terms of $\ell_\infty$ norm with respect to the LLM-based task embedding.
%we cover the entire task space under an $L_{\infty}$ criteria. 
%These clusters correspond to comittee members. Each policy in our committee is trained with \cite{hendawy2023multi} using the subset of tasks as assigned to corresponding committee member. During evaluation, at each time step we take the best committee member's performance as our result. At evaluation time we rollout a test episode to evaluate performance on both training and test tasks. 
%We find our method outperforms multitask and metalearning baselines in both zero and few shot scenarios. 

\subsection{Baselines and Evaluation}

We compare our approaches to 11 state-of-the-art baselines.
Five of these are designed for MTRL: 1) CMTA~\citep{lan2024contrastive}, 2) MOORE~\cite{hendawy2024multi}, 3) CARE~\citep{sodhani2021multi}, 4) soft modularization (Soft)~\citep{yang2020multi}, and 5) Multi task SAC~\citep{yu2020gradient}.
Five more are meta-RL algorithms: 1) MAML~\citep{DBLP:journals/corr/FinnAL17}, 2) RL2~\citep{duan2016rl}, 3) PEARL~\citep{rakelly2019efficient}, 4) VariBAD~\citep{zintgraf2020varibad}, and 5) AMAGO~\citep{grigsby2024amagoscalableincontextreinforcement}.
Finally, we also compare to the state-of-the-art approach using expectationâ€“maximization (EM) to learn a policy committee (EM)~\citep{ivanov2024personalized}.

Our evaluation involves three settings: \emph{training}, \emph{test}, and \emph{few-shot}.
The training evaluation corresponds to standard MTRL.
The test evaluation uses a test set to evaluate all approaches with no fine-tuning.
Finally, our few-shot test evaluation allows a short round of fine-tuning on the test data.
For \textsc{pacman} we select the best-performing policy for training and test, and use the proposed few-shot approach to \emph{learn} the best policy through empirical policy evaluation for the few-shot test setting (see Section~\ref{S:fewshot}).
In all figures, error bounds are 1 sample standard deviation.

%In the Meta-World environment, all 10 serve as baselines.

%We compare to Contrastive Modules with Temporal Attention (CMTA)\citep{lan2024contrastive}, Multi-Task Reinforcement Learning with Mixture of Orthogonal Experts (MOORE) \citep{hendawy2023multi}, Soft Modulation (Soft) \citep{DBLP:journals/corr/abs-2003-13661}, Contextual Attention Based Representation (CARE) \citep{sodhani2021multi}, and Multi task SAC \citep{pmlr-v100-yu20a} in the multitask setting. We compare to Varibad \citep{zintgraf2020varibad}, MAML \citep{DBLP:journals/corr/FinnAL17}, PEARL \citep{rakelly2019efficient}, and RL2 \citep{duan2016rl} for metalearning.  
%\vspace{-3MM}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{main_texts/figures/mujoco-main.png}
    \caption{HalfCheetahVel train (left) and test (right) comparisons.}
    \label{F:mujoco-main}
\end{figure}
\subsection{Results}
%\vspace{-3pt}
\iffalse
\begin{figure}[ht]
\label{F:mujoco-train}
    \centering
    \begin{minipage}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{main_texts/figures/cheeta_train.jpg}
        
    \end{minipage}
    \begin{minipage}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{main_texts/figures/cheetah.jpg}
        
    \end{minipage}
    \begin{minipage}[b]{0.24\textwidth}
        \centering
        \includegraphics[height= 2.4cm, width=0.9\textwidth]{main_texts/figures/train_new.png}
        
    \end{minipage}
    \begin{minipage}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{main_texts/figures/Test.png}
        
    \end{minipage}


    
    \caption{Left: Training and zero-shot test for HalfcheetahVel. Red, blue, green, yellow curves stand for PACMAN, VariBAD, EM, and RL2. Right: Training and zero-shot test for Meta-World.  }
\end{figure}



\begin{figure*}[ht]\label{F:mujoco-train}
\centering
\includegraphics[width=0.99\textwidth]{main_texts/figures/Amago_final.png}
\caption{Left: Training and zero-shot test for HalfcheetahVel. Red, blue, green, yellow curves stand for PACMAN, VariBAD, EM, and RL2. Right: Training and zero-shot test for Meta-World.  }
\end{figure*}
\fi


\begin{comment}
\begin{figure}[t]
    \centering
     \includegraphics[width=0.16\linewidth]{main_texts/figures/cheeta_train.jpg}
    \includegraphics[width=0.16\linewidth]{main_texts/figures/cheetah.jpg}
    % \includegraphics[width=0.24\linewidth]{main_texts/figures/humanoid-3.jpg}
     \includegraphics[width=0.16\linewidth]{main_texts/figures/Train.png}
     
     % \includegraphics[width=0.24\linewidth]{main_texts/figures/humanoid-2.jpg}
     \includegraphics[width=0.16\linewidth]{main_texts/figures/test.png}
      
    \caption{MuJoCo results. Left two: Halfcheetah (velocity) train and zero-shot test. Right two: Humanoid (direction) train and zero-shot test. Line colors are consistent across plots.
    %Train,  Humanoid-Dir Zero-shot. The colour for each method persists across four graphs.
    }
    \label{F:mujoco-train}
\end{figure}
\end{comment}
\noindent\textbf{MuJoCo}
In the MuJoCo environment, we focus on \emph{personalization}, varying only reward functions and focusing on the ability to generalize to a diverse set of rewards.
Consequently, our baselines here include meta-RL approaches (RL2, VariBAD, AMAGO) and EM (personalized RL, which requires the dynamics to be shared across tasks, therefore not applicable for Meta-World), and \textsc{pacman} uses VariBAD as the within-cluster RL method.
%In this environment, our approach wraps VariBAD.

Figure~\ref{F:mujoco-main} presents the MuJoCo results for the training and test evaluations in HalfCheetahVel.
We can see that \textsc{pacman} consistently outperforms the baselines in both evaluations, with VariBAD the only competitive baseline. The advantage of \textsc{pacman} is also pronounced in HumanoidDir, whose results are deferred to Appendix \ref{S:appendix-metaworld}.
%Notably, the advantage of \textsc{pacman} is particularly pronounced on test data.

\begin{table}[h]
%[7]{r}{0.53\textwidth}
    %\vspace{-20pt}
\caption{Few-shot learning effectiveness (MuJoCo).}
\small
\label{T:mujoco-fewshot}
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        & \textbf{Halfcheetah} & \textbf{Humanoid} \\
        \hline
        RL2     & -314.37 $\pm$ 1.15 & 946.17 $\pm$ 0.73  \\
        VariBAD & -137.99 $\pm$ 1.14  & 1706.38 $\pm$ 0.75  \\
        EM      & -325.29 $\pm$ 1.84  & 947.06 $\pm$ 0.84  \\
        Amago   & -279.13 $\pm$ 0.67 & 1533.45 $\pm$ 0.49\\
        \hline
        \textbf{PACMAN}  & \textbf{-54.03 $\pm$ 1.34} & \textbf{2086.50 $\pm$ 0.89} \\
        \hline
    \end{tabular}
\end{table}



%What is of particular interest is 
The few-shot comparison is provided in Table~\ref{T:mujoco-fewshot}, where the advantage of \textsc{pacman} is especially notable.
In HalfcheetahVel, the improvement over the best baseline is by a factor of more than 2.5, while in HumanoidDir it is over 22\%.
%From this, we can see the significant value of the \textsc{pacman} committee learning approach for few-shot adaptation.
%The results demonstrated that our method has a clear advantage.
\begin{comment}
    \begin{table*}[h]
\scriptsize
%footnotesize
\vspace{-10pt}
    \centering
    \caption{\small{Meta-World performance on 30 in-sample training tasks (left) and 20 out-of-sample test tasks (right). 
    %at 500,000 and 1,000,000 steps in the zero shot setting. 
    Performance is a moving average success rate for the last 2000 evaluation episodes over 3 seeds. Error bound is 1 sample standard deviation.
    }}
    \label{T:mw-train}
    \smallskip
     %\scriptsize
    \footnotesize
    %\resizebox{0.8\textwidth}{!}{  % Adjusts the table size to fit the text width
    \begin{tabular}{cc}
    
    
    \begin{tabular}{|c|c|c|}
   
    %\footnotesize
        \hline
        \multicolumn{3}{|c|}{Train} \\\hline
        \textbf{Method} & \textbf{500K Steps} & \textbf{1M Steps} \\
        \hline
        %PEARL   & 0.00 $\pm$ 0     & 0.00 $\pm$ 0     \\
        %RL2     & 0.00 $\pm$ 0.01  & 0.01 $\pm$ 0.02  \\
        %VariBAD & 0.00 $\pm$ 0.01  & 0.03 $\pm$ 0.05  \\
        %MAML    & 0.02 $\pm$ 0.03  & 0.03 $\pm$ 0.05  \\
        Soft    & 0.20 $\pm$ 0.08  & 0.28 $\pm$ 0.08  \\
        MTTE    & 0.29 $\pm$ 0.09  & 0.46 $\pm$ 0.11  \\
        CARE    & 0.43 $\pm$ 0.08  & 0.52 $\pm$ 0.09  \\
        CMTA    & 0.43 $\pm$ 0.09  & 0.53 $\pm$ 0.08  \\
        MOORE   & 0.44 $\pm$ 0.06  & 0.55 $\pm$ 0.01     \\\hline
        \textbf{PACMAN}  & \textbf{0.55} $\pm$ \textbf{0.04}  & \textbf{0.60} $\pm$ \textbf{0.05}  \\
        \hline
    \end{tabular}
    % Zero Shot
    \begin{tabular}{|c|c|c|}
        \hline
        \multicolumn{3}{|c|}{Test (zero-shot)} \\\hline
        \textbf{Method} & \textbf{500K Steps} & \textbf{1M Steps} \\
        \hline
        %PEARL   & 0.00 $\pm$ 0.00     & 0.00 $\pm$ 0.00     \\
        %RL2     & 0.00 $\pm$ 0.00  & 0.00 $\pm$ 0.00  \\
        %VariBAD & 0.00 $\pm$ 0.00  & 0.00 $\pm$ 0.00  \\
        %MAML    & 0.015 $\pm$ 0.04  & 0.005 $\pm$ 0.009  \\
        Soft    & 0.24 $\pm$ 0.10  & 0.29 $\pm$ 0.08  \\
        MTTE    & 0.30 $\pm$ 0.09  & 0.45 $\pm$ 0.11  \\
        CARE    & 0.43 $\pm$ 0.08  & 0.49 $\pm$ 0.08  \\
        CMTA    & 0.40 $\pm$ 0.08  & 0.51 $\pm$ 0.07  \\
        MOORE   & 0.42 $\pm$ 0.07  & 0.58 $\pm$ 0.00     \\\hline
        \textbf{PACMAN}  & \textbf{0.53} $\pm$ \textbf{0.05}  & \textbf{0.61} $\pm$ \textbf{0.05}  \\
        \hline
    \end{tabular}
    
   
    \end{tabular}
\end{table*}
\end{comment}
%\vspace{-4pt}
%We find that PACMAN exceeds well known baselines. We find that current metalearning algorithms do not perform well on Metaworld, even in cases where the algorithms should theoretically be able to adapt. We note that these works primarily were analyzed using Mujoco, which is a much simpler and in many cases only have one parameter difference (ie Half Cheetah move forward vs Half Cheetah move backwards). The Metaworld tasks are sufficiently diverse in not only in goal but also in dynamics i.e. tasks require contain different objects and control mechanics. We find that Multitask methods do not adapt well to the new tasks, and begin to over generalize harming performance. By contrast PACMAN, does not suffer such degradation in the few shot setting. 
%\vspace{1mm}

\noindent\textbf{Meta-World} Next, we turn to the complex multi-task Meta-World environment.
In this environment, our approach uses MOORE for within-cluster training.
Figure~\ref{F:MetaWorld-main} presents the results for training and test evaluations, where we compare to the MTRL baselines (all meta-RL baselines are significantly worse on these metrics, likely because the goals of these algorithms are primarily efficacy in few-shot settings).
%The results for both training (standard MTRL setting) and zero-shot test evaluations show 
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{main_texts/figures/MetaWorld-main.png}
    \caption{MetaWorld train (left) and test (right) comparisons.}
    \label{F:MetaWorld-main}
\end{figure}

We observe that \textsc{pacman} significantly outperforms all baselines in both train and test cases (e.g., $\sim$25\% improvement over the best baseline after 500K steps).
%, though the gap is bridged somewhat after 1M steps.
%This shows that \textsc{pacman} trains considerably faster in this setting.

\begin{table}[h]
%[11]{r}{0.38\textwidth}
%    \vspace{-10pt}
    \centering
    \caption{\small{Few-Shot Learning Results.}}
    \vspace{1mm}
    %at 6,000 and 12,000 updates. 
   
    \label{T:mw-fewshot}
    %\small % Adjust font size
    \scriptsize
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Method} & \textbf{6K Updates} & \textbf{12K Updates} \\
        \hline
        MAML    & 0.0025 $\pm$ 0.006  & 0.01 $\pm$ 0.03  \\
        PEARL   & 0.03 $\pm$ 0.03  & 0.27 $\pm$ 0.07  \\
        RL2     & 0.007 $\pm$ 0.01 & 0.02 $\pm$ 0.02  \\
        VariBAD & 0.025 $\pm$ 0.06  & 0.027 $\pm$ 0.07  \\
        AMAGO   & 0.08 $\pm$ 0.09  & .093 $\pm$ 0.09 \\\hline
        Soft    & 0.27 $\pm$ 0.07  & 0.26 $\pm$ 0.08  \\
        MTTE    & 0.37 $\pm$ 0.08  & 0.40 $\pm$ 0.10  \\
        CARE    & 0.39 $\pm$ 0.05  & 0.40 $\pm$ 0.06  \\
        CMTA    & 0.45 $\pm$ 0.07  & 0.34 $\pm$ 0.08  \\
        MOORE   & 0.41 $\pm$ 0.08  & 0.44 $\pm$ 0.11  \\\hline
        \textbf{PACMAN}  & \textbf{0.53} $\pm$ \textbf{0.02}  & \textbf{0.60} $\pm$ \textbf{0.02}  \\
        \hline
    \end{tabular}
\end{table}
%Considering next the few-shot learning problem, the advantage of \textsc{pacman} over both MTRL and meta-RL baselines is particularly notable.
The results for few-shot learning are provided in Table~\ref{T:mw-fewshot}. Performance is a moving average success rate for the last 2000 evaluation episodes over 3 seeds. 
Here, the advantage of \textsc{pacman} over all baselines is particularly notable.
First, somewhat surprisingly, the meta-RL baselines, with the exception of PEARL, underperform MTRL baselines in this setting.
This is because our evaluation is significantly more challenging, with only 30 training tasks but with 20 diverse test tasks, and the adaptation phase has a very short (6-12K updates) time horizon for few-shot training, than typical in prior work.
In contrast, MTRL methods fare reasonably well.
The proposed \textsc{pacman} approach, however, significantly outperforms all the baselines.
For example, only 12K updates suffice to reliably identify the best policy (comparing with zero-shot results in Table~\ref{T:mw-fewshot}), with the result outperforming the best baseline by $>$36\%.

\subsection{Further Empirical Investigation of Our Algorithm } \label{S:further}

%We consider two ablations of our approach.
%First we compare the difference between our method with both the popular clustering methods and the random clustering. 
%The first compares our clustering approach to a standard clustering method; we use k-means for this purpose.The second varies the committee size $K$.
\iffalse
\begin{figure}[h!]
\quad
%\centering
%    \includegraphics[width=0.24\linewidth]{main_texts/figures/ablation-2.jpg}
    \includegraphics[width=0.24\linewidth]{main_texts/figures/clustering_new.jpg}
%    \includegraphics[width=0.24\textwidth]{main_texts/figures/Train_K_Comparison.png}
    \includegraphics[width=0.24\linewidth]{main_texts/figures/cheetahKKK.jpg} 
    \includegraphics[width=0.24\linewidth]{main_texts/figures/Train_K_Comparison.png}
    \includegraphics[width=0.24\linewidth]{main_texts/figures/Test_K_Comparison.png}\\
\smallskip
\footnotesize
    \quad\quad\quad\quad\quad\quad  (a) \quad\quad\quad\quad\quad\quad\quad\quad\quad\   (b)
    \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad (c)
    \quad\quad\quad\quad\quad\quad\quad\quad\quad  (d)
    %\includegraphics[width=0.32\textwidth]{main_texts/figures/Test_K_Comparison_few.png}
     \caption{(a) \textsc{pacman} ablation with k-means ($K=3$; MuJoCo), (b) varying $K$ (zero-shot, MuJoCo), (c) and (d) varying $K$ (training and zero-shot test, respectively, Meta-World).}

    \label{F:ablations_main}
\end{figure}


\begin{figure*}[ht]
\centering
\label{F:ablations_main}
\includegraphics[width=\textwidth]{main_texts/figures/updated_fig.png}
 %\quad\quad\quad\quad\quad\quad  (a) (b) varying $K$ (zero-shot, MuJoCo)\quad\quad\quad\quad\quad\quad\quad\quad\quad\   (b)
  %  \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad (c)
 %   \quad\quad\quad\quad\quad\quad\quad\quad\quad  (d)
\caption{From left to right: (a) \textsc{pacman} ablation with different clustering methods ($K=1,2,3,4$; MuJoCo), (b) and (c) varying $K$ (training and zero-shot test, respectively, Meta-World).}
\end{figure*}


Second, Table~\ref{T:fewshot_ablation_metaworld} demonstrates a clear advantage of utilizing a policy committee. Here, in few-shot settings, even using $K=2$ already results in considerable improvement over the best baseline (MOORE), with $K=3$ a significant further boost. Another thing to note is that increasing
$K$ is not always better. The results in both Figure~2(b) and (c), and Table~\ref{T:fewshot_ablation_metaworld} show that as the number of tasks becomes increasingly partitioned, the generalization ability of each committee member may weaken. Hence the performance for $K=4$ is worse than $K=3$. We also conducted the same experiments for Mujoco, the details are in Appendix~\ref{A:hist}.




\fi

We investigate our algorithmic contribution in two ways. 
First, we compare our method for task clustering with three common clustering methods: KMeans++~\citep{arthur2006k}, DBScan~\citep{khan2014dbscan}, GMM~\citep{bishop2006pattern}, as well as with random clustering. 

\begin{table}[h]
%{r}{0.5\textwidth}
\vspace{-5pt}
\centering
\caption{Comparison of clustering algorithms in HalfCheetahVel.}
\label{T:clustering_ablation_mujoco}
\footnotesize
\begin{tabular}{|l|c|c|}
\hline
\textbf{Method} & \textbf{6 Million Frames} & \textbf{12 Million Frames} \\
\hline
KMeans++    & $-105.91  \pm  3.44$  & $-89.50 \pm 3.10$  \\

DBScan   & $-234.05 \pm 9.56 $ & $-213.42  \pm 4.09$  \\
GMM         & $-239.32  \pm 11.27$ & $ -199.86 \pm 9.54$  \\
Random    & $-274.13\pm 16.76$  & $-258.07 \pm  13.62$  \\
\hline
\textbf{PACMAN}     & \textbf{ -97.42 $\pm$  3.70}  & \textbf{-74.20 $\pm$  6.76} \\\hline

\end{tabular}
\end{table}

As shown in Table~\ref{T:clustering_ablation_mujoco}, the proposed approach outperforms all baselines.
In Appendix~\ref{A:hist} we show that the distributional improvement is even more pronounced.
%First, it shows that our clustering method indeed has the best performance. 
%We emphasize that our improvement compared to KMeans is nontrivial, and a more detailed explanation is provided in the Appendix~\ref{A:hist}. 

%Then we show how changing the number of policies in the committee influences its performance.
Additionally, we consider the impact of varying $K$, and show that efficacy of \textsc{pacman} is non-monotonic in $K$.
The reason is that once $K$ is sufficiently large to cover the entire set of training tasks, increasing it further reduces the number of training tasks in individual clusters and thereby hurts generalization, both within clusters and to previously unseen tasks.
Finally, we also consider the impact of changing the value of $\epsilon$, and observe that the results are relatively robust to small changes in $\epsilon$.
%since setting it too high can lead to clusters that include too few tasks for effective training.
See Appendix~\ref{S:appendix-metaworld} for further details on these ablations.
%, as well as those varying $\epsilon$.

%, the results for $K=4$ demonstrate that larger $K$ is not always better. $K=3$ provides a full cover over tasks, 

%the proposed approach, which is designed specifically with policy committee learning in mind, 
%Second, Figure~2(b) and (c) shows that with fixed $\epsilon$, $K>1$ yields significant improvement over using a single policy in Meta-World, demonstrating the value of using policy committees in settings with diverse tasks. However, we found that increasing K beyond what already provides full coverage for a given  significantly harms performance. The reason is that it is useful to have sufficient tasks for each cluster to enable effective and robust per-cluster training.
%In both cases, the improvement is particularly notable going from $K=1$ to $K=2$
%, although using an additional policy allows considerable further improvement.






%This picture is further bolstered in few-shot settings, as we show for Meta-World in Table~\ref{T:fewshot_ablation_metaworld}.









%\vspace{-3mm}
%Second, $K>1$ yields a dramatic advantage compared to having a single policy both in MuJoCo and Meta-World environments.
%As we can see from Figure~\ref{F:ablations_main} (left)
%the proposed approach significantly outperforms using k-means clustering.
%Additionally (Figure~\ref{F:ablations_main} (right)), in this setting we see that increasing $K$ to 3 considerably improves performance compared to $K \in \{1,2\}$.
%We observe similar results in the Meta-World environment (see Appendix~\ref{S:appendix-metaworld}).
%\luise{additionally, we see from the right figure that both $K$ and $\epsilon$ are both important factors. Using our gradient based SoftI, K=2 and K=3 both obtain one hundred percent coverage with fixed epsilon .6; but K=2 if worse. Moreover, K=1 with epsilon .6 only covers around $60\%$ of the train tasks, and has the worst behaviour within 10M training frames.}
%Additionally, we see from the right figure that even using only a committee of two significantly improves performance, with $K=3$ yielding meaningful additional improvement.

%\paragraph{Clustering Algorithms}
%To ablate the effect our original clustering algorithm, we have also run experiments on Mujoco environment with three policies on the one hundred tasks separated with KMeans. Our clustering method (softI) promotes ``near-optimal" outcomes for more tasks than traditional clustering method such as KMeans.










%\subsection{$K$}

%Below are the ablations for varying committee size $k$ in the multitask setting. We note that there seems to be diminishing returns on adding new committee members. In this case $K=1$ could not cover all tasks with $\epsilon=.7$. $K=2$ and $K=3$ provide full coverage. We empirically find best performance with full coverage after tuning $\epsilon$ based on the distance between tasks in task space. 


%\begin{figure*}[h]
%\centering
%    \begin{tabular}{cc}
    
%    \includegraphics[width=0.45\textwidth]{main_texts/figures/Train_K_Comparison.png} &
%    \includegraphics[width=0.45\textwidth]{main_texts/figures/Test_K_Comparison.png} 

%    \end{tabular}

%    \caption{Ablation of committee size $K$.}
%    \label{G:Metaworld_K_ablation}
%\end{figure*}





