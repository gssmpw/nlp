@ARTICLE{784232,
  author={Jensen, C.A. and Reed, R.D. and Marks, R.J. and El-Sharkawi, M.A. and Jae-Byung Jung and Miyamoto, R.T. and Anderson, G.M. and Eggen, C.J.},
  journal={Proceedings of the IEEE}, 
  title={Inversion of feedforward neural networks: algorithms and applications}, 
  year={1999},
  volume={87},
  number={9},
  pages={1536-1549},
  keywords={Neural networks;Feedforward neural networks;Training data;Sonar;Power system security;Neurons;Performance analysis;Control systems;Power generation;Multi-layer neural network},
  doi={10.1109/5.784232}}

@ARTICLE{8677282,
  author={Al-Rubaie, Mohammad and Chang, J. Morris},
  journal={IEEE Security \& Privacy}, 
  title={Privacy-Preserving Machine Learning: Threats and Solutions}, 
  year={2019},
  volume={17},
  number={2},
  pages={49-58},
  keywords={Data models;Image reconstruction;Computational modeling;Training;Predictive models;Feature extraction;Testing},
  doi={10.1109/MSEC.2018.2888775}
}

@INPROCEEDINGS{9833677,
  author={Balle, Borja and Cherubin, Giovanni and Hayes, Jamie},
  booktitle={2022 IEEE Symposium on Security and Privacy (SP)}, 
  title={Reconstructing Training Data with Informed Adversaries}, 
  year={2022},
  volume={},
  number={},
  pages={1138-1156},
  keywords={Training;Privacy;Differential privacy;Computational modeling;Training data;Machine learning;Data models;machine learning;neural networks;reconstruction attacks;differential privacy},
  doi={10.1109/SP46214.2022.9833677}}

@article{KINDERMANN1990277,
author = {J Kindermann and A Linden},
title = {Inversion of neural networks by gradient descent},
journal = {Parallel Computing},
volume = {14},
number = {3},
pages = {277-286},
year = {1990},
issn = {0167-8191},
doi = {https://doi.org/10.1016/0167-8191(90)90081-J},
url = {https://www.sciencedirect.com/science/article/pii/016781919090081J},
keywords = {Inversion, multilayer perceptrons, backpropagation, generalization, robustness},
abstract = {Inversion answers the question of which input patterns to a trained multilayer neural network approximate a given output target. This method is a tool for visualization of the information processing capability of a network stored in its weights. This knowledge about the network enables one to make informed decisions on the improvement of the training task and the choice of training sets. An inversion algorithm for multilayer perceptrons is derived from the backpropagation scheme. We apply inversion to networks for digit recognition. We observe that the multilayer perceptrons perform well with respect to generalization, i.e. correct classification of untrained digits. They are however bad on rejection of counterexamples, i.e. random patterns. Inversion gives an explanation for this drawback. We suggest an improved training scheme, and we show that a tradeoff exists between generalization and rejection of counterexamples.}
}

@article{SAAD200778,
title = {Neural network explanation using inversion},
journal = {Neural Networks},
volume = {20},
number = {1},
pages = {78-93},
year = {2007},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2006.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0893608006001730},
author = {Emad W. Saad and Donald C. Wunsch},
keywords = {Rule extraction, Neural network explanation, Explanation capability of neural networks, Inversion, Hyperplanes, Evolutionary algorithm, Pedagogical},
abstract = {An important drawback of many artificial neural networks (ANN) is their lack of explanation capability [Andrews, R., Diederich, J., & Tickle, A. B. (1996). A survey and critique of techniques for extracting rules from trained artificial neural networks. Knowledge-Based Systems, 8, 373–389]. This paper starts with a survey of algorithms which attempt to explain the ANN output. We then present HYPINV,11HYPINV stands for an algorithm which extracts HYPerplanes using INVersion. a new explanation algorithm which relies on network inversion; i.e.calculating the ANN input which produces a desired output. HYPINV is a pedagogical algorithm, that extracts rules, in the form of hyperplanes. It is able to generate rules with arbitrarily desired fidelity, maintaining a fidelity–complexity tradeoff. To our knowledge, HYPINV is the only pedagogical rule extraction method, which extracts hyperplane rules from continuous or binary attribute neural networks. Different network inversion techniques, involving gradient descent as well as an evolutionary algorithm, are presented. An information theoretic treatment of rule extraction is presented. HYPINV is applied to example synthetic problems, to a real aerospace problem, and compared with similar algorithms using benchmark problems.}
}

@inproceedings{Wong2017NeuralNI,
  title={Neural network inversion beyond gradient descent},
  author={Eric Wong},
  year={2017},
  booktitle={WOML NIPS},
  url={https://api.semanticscholar.org/CorpusID:208231247}
}

@inproceedings{ad,
author = {Yang, Ziqi and Zhang, Jiyi and Chang, Ee-Chien and Liang, Zhenkai},
title = {Neural Network Inversion in Adversarial Setting via Background Knowledge Alignment},
year = {2019},
isbn = {9781450367479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319535.3354261},
doi = {10.1145/3319535.3354261},
abstract = {The wide application of deep learning technique has raised new security concerns about the training data and test data. In this work, we investigate the model inversion problem under adversarial settings, where the adversary aims at inferring information about the target model's training data and test data from the model's prediction values. We develop a solution to train a second neural network that acts as the inverse of the target model to perform the inversion. The inversion model can be trained with black-box accesses to the target model. We propose two main techniques towards training the inversion model in the adversarial settings. First, we leverage the adversary's background knowledge to compose an auxiliary set to train the inversion model, which does not require access to the original training data. Second, we design a truncation-based technique to align the inversion model to enable effective inversion of the target model from partial predictions that the adversary obtains on victim user's data. We systematically evaluate our approach in various machine learning tasks and model architectures on multiple image datasets. We also confirm our results on Amazon Rekognition, a commercial prediction API that offers "machine learning as a service". We show that even with partial knowledge about the black-box model's training data, and with only partial prediction values, our inversion approach is still able to perform accurate inversion of the target model, and outperform previous approaches.},
booktitle = {Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security},
pages = {225–240},
numpages = {16},
keywords = {deep learning, model inversion, neural networks, privacy, security},
location = {London, United Kingdom},
series = {CCS '19}
}

@misc{buzaglo2023reconstructingtrainingdatamulticlass,
      title={Reconstructing Training Data from Multiclass Neural Networks}, 
      author={Gon Buzaglo and Niv Haim and Gilad Yehudai and Gal Vardi and Michal Irani},
      year={2023},
      eprint={2305.03350},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.03350}, 
}

@misc{haim2022reconstructingtrainingdatatrained,
      title={Reconstructing Training Data from Trained Neural Networks}, 
      author={Niv Haim and Gal Vardi and Gilad Yehudai and Ohad Shamir and Michal Irani},
      year={2022},
      eprint={2206.07758},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2206.07758}, 
}

@misc{kumar2019modelinversionnetworksmodelbased,
      title={Model Inversion Networks for Model-Based Optimization}, 
      author={Aviral Kumar and Sergey Levine},
      year={2019},
      eprint={1912.13464},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1912.13464}, 
}

@misc{liu2022landscapelearningneuralnetwork,
      title={Landscape Learning for Neural Network Inversion}, 
      author={Ruoshi Liu and Chengzhi Mao and Purva Tendulkar and Hao Wang and Carl Vondrick},
      year={2022},
      eprint={2206.09027},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2206.09027}, 
}

@misc{oz2024reconstructingtrainingdatareal,
      title={Reconstructing Training Data From Real World Models Trained with Transfer Learning}, 
      author={Yakir Oz and Gilad Yehudai and Gal Vardi and Itai Antebi and Michal Irani and Niv Haim},
      year={2024},
      eprint={2407.15845},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.15845}, 
}

@InProceedings{pmlr-v162-guo22c,
  title = 	 {Bounding Training Data Reconstruction in Private (Deep) Learning},
  author =       {Guo, Chuan and Karrer, Brian and Chaudhuri, Kamalika and van der Maaten, Laurens},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {8056--8071},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/guo22c/guo22c.pdf},
  url = 	 {https://proceedings.mlr.press/v162/guo22c.html},
  abstract = 	 {Differential privacy is widely accepted as the de facto method for preventing data leakage in ML, and conventional wisdom suggests that it offers strong protection against privacy attacks. However, existing semantic guarantees for DP focus on membership inference, which may overestimate the adversary’s capabilities and is not applicable when membership status itself is non-sensitive. In this paper, we derive the first semantic guarantees for DP mechanisms against training data reconstruction attacks under a formal threat model. We show that two distinct privacy accounting methods—Renyi differential privacy and Fisher information leakage—both offer strong semantic protection against data reconstruction attacks.}
}

@InProceedings{pmlr-v206-wang23g,
  title = 	 {Reconstructing Training Data from Model Gradient, Provably},
  author =       {Wang, Zihan and Lee, Jason and Lei, Qi},
  booktitle = 	 {Proceedings of The 26th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {6595--6612},
  year = 	 {2023},
  editor = 	 {Ruiz, Francisco and Dy, Jennifer and van de Meent, Jan-Willem},
  volume = 	 {206},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {25--27 Apr},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v206/wang23g/wang23g.pdf},
  url = 	 {https://proceedings.mlr.press/v206/wang23g.html},
  abstract = 	 {Understanding when and how much a model gradient leaks information about the training sample is an important question in privacy. In this paper, we present a surprising result: Even without training or memorizing the data, we can fully reconstruct the training samples from a single gradient query at a randomly chosen parameter value. We prove the identifiability of the training data under mild assumptions: with shallow or deep neural networks and wide range of activation functions. We also present a statistically and computationally efficient algorithm based on tensor decomposition to reconstruct the training data. As a provable attack that reveals sensitive training data, our findings suggest potential  severe threats to privacy, especially in federated learning.}
}

@inproceedings{suhail2024network,
title={Network Inversion of Binarised Neural Nets},
author={Pirzada Suhail},
booktitle={The Second Tiny Papers Track at ICLR 2024},
year={2024},
url={https://openreview.net/forum?id=zKcB0vb7qd}
}

@inproceedings{suhail2024networkcnn,
    title={Network Inversion of Convolutional Neural Nets},
    author={Pirzada Suhail and Amit Sethi},
    booktitle={Muslims in ML Workshop co-located with NeurIPS 2024},
    year={2024},
    url={https://openreview.net/forum?id=f9sUu7U1Cp}
}

@misc{suhail2024networkinversionapplications,
      title={Network Inversion and Its Applications}, 
      author={Pirzada Suhail and Hao Tang and Amit Sethi},
      year={2024},
      eprint={2411.17777},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2411.17777}, 
}

@misc{wang2021reconstructingtrainingdatadiverse,
      title={Reconstructing Training Data from Diverse ML Models by Ensemble Inversion}, 
      author={Qian Wang and Daniel Kurz},
      year={2021},
      eprint={2111.03702},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2111.03702}, 
}

@misc{wang2023reconstructingtrainingdatamodel,
      title={Reconstructing Training Data from Model Gradient, Provably}, 
      author={Zihan Wang and Jason D. Lee and Qi Lei},
      year={2023},
      eprint={2212.03714},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2212.03714}, 
}

@misc{xu2021privacypreservingmachinelearningmethods,
      title={Privacy-Preserving Machine Learning: Methods, Challenges and Directions}, 
      author={Runhua Xu and Nathalie Baracaldo and James Joshi},
      year={2021},
      eprint={2108.04417},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2108.04417}, 
}

