\section{Related Works}
Privacy concerns in machine learning have led to extensive research in Privacy-Preserving Machine Learning (PPML), particularly in mitigating risks related to membership inference, attribute inference, and model inversion attacks. A foundational study by \cite{8677282} provides an overview of privacy threats in ML, including model inversion, and explores defenses such as differential privacy, homomorphic encryption, and federated learning. Similarly, \cite{xu2021privacypreservingmachinelearningmethods} introduces the Phase, Guarantee, and Utility (PGU) triad, a framework to evaluate PPML techniques across different phases of the ML pipeline. These studies highlight the need for privacy-preserving methods but primarily focus on algorithmic-level defenses rather than evaluating inherent vulnerabilities in different model architectures. Unlike these approaches, our study investigates the privacy risks posed by architectural design choices by analyzing how MLPs, CNNs, and ViTs differ in their susceptibility to model inversion attacks.

Network inversion has emerged as a powerful technique to understand how neural networks encode and manipulate training data. Initially developed for interpretability \cite{KINDERMANN1990277,784232}, it has since been shown to reconstruct sensitive training samples, raising significant privacy concerns \cite{Wong2017NeuralNI,ad}. Early works on network inversion focused on fully connected networks (MLPs) \cite{KINDERMANN1990277,SAAD200778}, demonstrating that they tend to memorize training data, making them vulnerable to inversion attacks. Evolutionary inversion procedures \cite{784232} improved the ability to capture input-output relationships, providing deeper insights into model memorization behavior. More recent studies extended these inversion techniques to CNNs \cite{ad}, showing that hierarchical feature extraction does not necessarily prevent training data leakage. The introduction of ViTs has further complicated this issue, as their global self-attention mechanisms process data differently than CNNs, raising new questions about how they store training information and whether their memorization patterns lead to higher or lower inversion risks.

In adversarial settings, model inversion attacks aim to reconstruct sensitive data by exploiting a model's predictions, gradients, or weights \cite{ad,kumar2019modelinversionnetworksmodelbased}. These attacks have been shown to succeed even without direct access to the training process, as demonstrated by \cite{9833677}, where an adversary with auxiliary knowledge reconstructs sensitive samples. Gradient-based inversion attacks further exacerbate these risks by leaking sensitive training information through shared gradients in federated learning setups \cite{pmlr-v206-wang23g}.

To improve the stability of inversion processes, recent works have explored novel optimization techniques. For example, \cite{liu2022landscapelearningneuralnetwork} proposed learning a loss landscape to make gradient-based inversion faster and more stable. Alternative approaches, such as encoding networks into Conjunctive Normal Form (CNF) and solving them using SAT solvers, offer deterministic solutions for inversion, as introduced by \cite{suhail2024network}. Although computationally expensive, these methods ensure diversity in the reconstructed samples by avoiding shortcuts in the optimization process.

Model Inversion (MI) attacks have also been extended to scenarios involving ensemble techniques, where multiple models trained on shared subjects or entities are used to guide the reconstruction process. The concept of ensemble inversion, as proposed by \cite{wang2021reconstructingtrainingdatadiverse}, enhances the quality of reconstructed data by leveraging the diversity of perspectives provided by multiple models. By incorporating auxiliary datasets similar to the presumed training data, this approach achieves high-quality reconstructions with sharper predictions and higher activations. This work highlights the risks posed by adversaries exploiting shared data entities across models, emphasizing the importance of robust defense mechanisms.

Reconstruction methods for training data have evolved significantly, focusing on improving the efficiency and accuracy of recovering data from models. Traditional optimization-based approaches relied on iteratively refining input data to match a modelâ€™s outputs or activations \cite{Wong2017NeuralNI}. More recent advancements have leveraged generative models, such as GANs and autoencoders, to synthesize high-quality reconstructions. These techniques aim to approximate the distribution of training data while maintaining computational efficiency. In the context of privacy risks, works like \cite{haim2022reconstructingtrainingdatatrained} demonstrated that significant portions of training data could be reconstructed from neural network parameters in binary classification settings. This work was later extended to multi-class classification by \cite{buzaglo2023reconstructingtrainingdatamulticlass}, showing that higher-quality reconstructions are possible and revealing the impact of regularization techniques, such as weight decay, on memorization behavior.

The ability to reconstruct training data from model gradients also presents a critical privacy challenge. The study by \cite{wang2023reconstructingtrainingdatamodel} demonstrated that training samples could be fully reconstructed from a single gradient query, even without explicit training or memorization. Recent advancements like \cite{oz2024reconstructingtrainingdatareal}, adapt reconstruction schemes to operate in the embedding space of large pre-trained models, such as DINO-ViT and CLIP. This approach, which introduces clustering-based methods to identify high-quality reconstructions from numerous candidates, represents a significant improvement over earlier techniques that required access to the original dataset. While \cite{pmlr-v162-guo22c} extended differential privacy guarantees to training data reconstruction attacks.

In this paper, we build upon prior work on network inversion and training data reconstruction. Drawing from works like \cite{suhail2024networkcnn} and \cite{suhail2024networkinversionapplications}, we employ network inversion methods to understand the internal representations of neural networks and the patterns they memorize during training. Our study systematically compares the privacy-preserving properties of different vision classifier architectures, including MLPs, CNNs, and ViTs, using network inversion-based reconstruction techniques \cite{suhail2024net}. By evaluating these techniques across datasets such as MNIST, FashionMNIST, CIFAR-10, and SVHN, we explore the impact of architectural differences, input processing mechanisms, and weight structures on the susceptibility of models to inversion attacks.