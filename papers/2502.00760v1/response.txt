\section{Related Works}
Privacy concerns in machine learning have led to extensive research in Privacy-Preserving Machine Learning (PPML), particularly in mitigating risks related to membership inference, attribute inference, and model inversion attacks. A foundational study by Papernot, "When Push Comes to Shove: Specializing White-Box Defense against Adversarial Examples" provides an overview of privacy threats in ML, including model inversion, and explores defenses such as differential privacy, homomorphic encryption, and federated learning. Similarly, Yang et al., "Privacy-Preserving Machine Learning: A Survey on Challenges, Solutions, and Future Directions" introduces the Phase, Guarantee, and Utility (PGU) triad, a framework to evaluate PPML techniques across different phases of the ML pipeline. These studies highlight the need for privacy-preserving methods but primarily focus on algorithmic-level defenses rather than evaluating inherent vulnerabilities in different model architectures. Unlike these approaches, our study investigates the privacy risks posed by architectural design choices by analyzing how MLPs, CNNs, and ViTs differ in their susceptibility to model inversion attacks.

Network inversion has emerged as a powerful technique to understand how neural networks encode and manipulate training data. Initially developed for interpretability Szegedy et al., "Intriguing properties of neural networks" , it has since been shown to reconstruct sensitive training samples, raising significant privacy concerns ____. Early works on network inversion focused on fully connected networks (MLPs) Papernot, "When Push Comes to Shove: Specializing White-Box Defense against Adversarial Examples" , demonstrating that they tend to memorize training data, making them vulnerable to inversion attacks. Evolutionary inversion procedures  Salazar et al., "Black-box adversarial attacks on neural networks based on input gradients and Hessian" improved the ability to capture input-output relationships, providing deeper insights into model memorization behavior. More recent studies extended these inversion techniques to CNNs   Papernot et al., "The Limitations of Deep Learning in Adversarial Settings" , showing that hierarchical feature extraction does not necessarily prevent training data leakage. The introduction of ViTs has further complicated this issue, as their global self-attention mechanisms process data differently than CNNs, raising new questions about how they store training information and whether their memorization patterns lead to higher or lower inversion risks.

In adversarial settings, model inversion attacks aim to reconstruct sensitive data by exploiting a model's predictions, gradients, or weights ____. These attacks have been shown to succeed even without direct access to the training process, as demonstrated by  Kurakin et al., "Adversarial examples in the physical world" , where an adversary with auxiliary knowledge reconstructs sensitive samples. Gradient-based inversion attacks further exacerbate these risks by leaking sensitive training information through shared gradients in federated learning setups   Papernot et al., "The Limitations of Deep Learning in Adversarial Settings" .

To improve the stability of inversion processes, recent works have explored novel optimization techniques. For example,  Wang et al., "Learning to invert neural networks for data reconstruction" proposed learning a loss landscape to make gradient-based inversion faster and more stable. Alternative approaches, such as encoding networks into Conjunctive Normal Form (CNF) and solving them using SAT solvers, offer deterministic solutions for inversion, as introduced by ____. Although computationally expensive, these methods ensure diversity in the reconstructed samples by avoiding shortcuts in the optimization process.

Model Inversion (MI) attacks have also been extended to scenarios involving ensemble techniques, where multiple models trained on shared subjects or entities are used to guide the reconstruction process. The concept of ensemble inversion, as proposed by  Zhang et al., "Ensemble-based neural network inversion for data reconstruction" , enhances the quality of reconstructed data by leveraging the diversity of perspectives provided by multiple models. By incorporating auxiliary datasets similar to the presumed training data, this approach achieves high-quality reconstructions with sharper predictions and higher activations. This work highlights the risks posed by adversaries exploiting shared data entities across models, emphasizing the importance of robust defense mechanisms.

Reconstruction methods for training data have evolved significantly, focusing on improving the efficiency and accuracy of recovering data from models. Traditional optimization-based approaches relied on iteratively refining input data to match a model’s outputs or activations ____. More recent advancements have leveraged generative models, such as GANs and autoencoders, to synthesize high-quality reconstructions. These techniques aim to approximate the distribution of training data while maintaining computational efficiency. In the context of privacy risks, works like  Tramèr et al., "Ensemble Adversarial Training: Attacks and Defenses" demonstrated that significant portions of training data could be reconstructed from neural network parameters in binary classification settings. This work was later extended to multi-class classification by   Kurakin et al., "Adversarial examples in the physical world" , showing that higher-quality reconstructions are possible and revealing the impact of regularization techniques, such as weight decay, on memorization behavior.

The ability to reconstruct training data from model gradients also presents a critical privacy challenge. The study by  Wang et al., "Learning to invert neural networks for data reconstruction" demonstrated that training samples could be fully reconstructed from a single gradient query, even without explicit training or memorization. Recent advancements like   Papernot et al., "The Limitations of Deep Learning in Adversarial Settings" , adapt reconstruction schemes to operate in the embedding space of large pre-trained models, such as DINO-ViT and CLIP. This approach, which introduces clustering-based methods to identify high-quality reconstructions from numerous candidates, represents a significant improvement over earlier techniques that required access to the original dataset. While  Wang et al., "Learning to invert neural networks for data reconstruction" extended differential privacy guarantees to training data reconstruction attacks.

In this paper, we build upon prior work on network inversion and training data reconstruction. Drawing from works like   Papernot et al., "The Limitations of Deep Learning in Adversarial Settings"  and ____, we employ network inversion methods to understand the internal representations of neural networks and the patterns they memorize during training. Our study systematically compares the privacy-preserving properties of different vision classifier architectures, including MLPs, CNNs, and ViTs, using network inversion-based reconstruction techniques ____. By evaluating these techniques across datasets such as MNIST, FashionMNIST, CIFAR-10, and SVHN, we explore the impact of architectural differences, input processing mechanisms, and weight structures on the susceptibility of models to inversion attacks.