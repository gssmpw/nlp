\section{Related Work}
\label{rel}
\noindent\textbf{Instance-level Image-to-3D Methods.}
Instance-level image-to-3D approaches \cite{wonder3d, zero123++,instantmesh,vqadiff} aim to generate 3D representations from a single image. Specifically, Diffusion Models \cite{zero123++, wonder3d} have demonstrated strong zero-shot prediction abilities, benefiting from training on large-scale datasets such as Objaverse \cite{obj}. Since reconstruction quality is usually prioritized over efficiency, they take several minutes and even tens of minutes to reconstruct 3D models, which limits their applicability in real-time scenarios. While some methods \cite{instantmesh,vqadiff} can achieve faster image-to-3D generation, they generally do not incorporate incremental optimization of the 3D model.\\
\noindent\textbf{6D Pose Estimation Methods.}
The implementation \footnote{Their pose estimation networks require dense posed reference images, which are sampled from a textured CAD model in their pipelines.} of most existing 6D pose estimation methods, such as FoundationPose \cite{fp}, GigaPose \cite{gigapose}, and SAM6D \cite{sam6d}, requires a textured CAD model in advance, which requires intensive time and labor to craft \cite{lpr}. For example, when handling an unseen object, FoundationPose \cite{fp} requires running BundleSDF \cite{bundlesdf} to generate the reference 3D model, which takes tens of minutes to complete. Similarly, OnePose \cite{onepose} and OnePose++ \cite{onepose++} propose recording a video scan of the object and utilizing Structure-from-Motion \cite{colmap} to reconstruct the object. FS6D \cite{fs6d} does not reconstruct the object from reference images but requires reference images with pose labels. However, the prior operation involving the reference 3D model or reference image sampling may be impractical in real-world settings, especially when instant robotic action is required at first sight of the object. Recently, Zero123-6D \cite{zero1236d} proposes leveraging the Diffusion Model for 6D pose estimation. However, the category-level pose estimation strategy of Zero123-6D does not fully utilize the instance-level object generation ability of Diffusion Models \cite{wonder3d,zero123++} and, like previous methods \cite{fp,gigapose,sam6d,onepose,onepose++,fs6d}, does not consider further model optimization during pose estimation. In contrast, object-SLAM methods \cite{xu2019mid, bundlesdf} reconstruct objects in real-time without prior knowledge, tracking and optimizing the geometry \cite{mvdeepsdf} and appearance \cite{yanglearning,unigaussian} from scratch. However, they struggle to provide complete models when observations are scarce. Incorporating advancements in object generation, the object-level mapping proposed in \cite{xu2022learning} introduces DeepSDF-like~\cite{mvdeepsdf} generative priors to constrain object shapes, enabling the estimation of complete shapes and poses under occlusions. However, it is limited to a single category due to the shortcomings of these generative priors. Concurrently, GOE \cite{liao2024toward} extends this approach by leveraging a multi-category 3D diffusion prior, but its optimization efficiency remains limited.

\begin{figure*}
	\centering
	\includegraphics[width=17.5cm]{Figures/overview.png}
	\caption{\textbf{Overview of HIPPo.} Given a video consisting of RGB-D frames, Grounding DINO \cite{grounding} is first applied to segment the object based on a prompt. 
    %
    Next, the proposed HIPPo Dreamer, built on a multiview Diffusion Model and a 3D reconstruction foundation model, generates a 3D mesh of the object from the first detected frame in a few seconds. 
    %
   Then, the diffusion prior mesh is provided to the pose estimation network to estimate the 6D pose in real time.
    %
   Meanwhile, the mesh optimization module monitors viewpoint changes through a predefined viewpoint sphere and triggers mesh optimization when the viewpoint varies dramatically. The module then replaces the diffusion prior with more reliable appearance and geometry from online measurements.
     }
	\label{overview}
  
\end{figure*}