\section{Related Work}
\subsection{Prompt Learning for Vision-Language Models}
Recently, the strong generalization capability of CLIP has made it a foundation for many methods in adapting pre-trained Vision-Language models (VLMs) for downstream tasks. 
Prompt tuning is a widely used technique in VLMs for learning downstream tasks. 
The use of text prompts, which are instructions provided to the language branch of VLMs, is a common practice to enhance task understanding. Full fine-tuning____ and linear probe are two common methods used to adapt VLMs to downstream tasks. %Nevertheless, the limitations of both methods have spurred the investigation of novel techniques inspired by prompt tuning in Natural Language Processing (NLP). 
Some methods____ fine-tune the CLIP model specifically for few-shot image recognition by optimizing a continuous set of token embeddings in the language branch. 
Moreover, some methods
constrain the prompts to contain the essential general knowledge and prior distribution learning____. 
In addition to multi-modal prompt tuning____, some methods____ apply adversarial training perspective for prompt tuning to effectively align its V-L representations for pre-trained CLIP.