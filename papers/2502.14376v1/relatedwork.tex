\section{Related Work}
\subsection{Prompt Learning for Vision-Language Models}
Recently, the strong generalization capability of CLIP has made it a foundation for many methods in adapting pre-trained Vision-Language models (VLMs) for downstream tasks. 
Prompt tuning is a widely used technique in VLMs for learning downstream tasks. 
The use of text prompts, which are instructions provided to the language branch of VLMs, is a common practice to enhance task understanding. Full fine-tuning~\cite{cao2024controlled} and linear probe are two common methods used to adapt VLMs to downstream tasks. %Nevertheless, the limitations of both methods have spurred the investigation of novel techniques inspired by prompt tuning in Natural Language Processing (NLP). 
Some methods~\cite{zhou2022learning,zhou2022conditional,yao2023tcp} fine-tune the CLIP model specifically for few-shot image recognition by optimizing a continuous set of token embeddings in the language branch. 
Moreover, some methods
constrain the prompts to contain the essential general knowledge and prior distribution learning~\cite{lu2022prompt}. 
In addition to multi-modal prompt tuning~\cite{lee2023multimodal,khattak2023maple,khattak2023self}, some methods~\cite{zhou2024few,li2024one} apply adversarial training perspective for prompt tuning to effectively align its V-L representations for pre-trained CLIP.