\section{Related Work}
\subsection{Prompt Learning for Vision-Language Models}
Recently, the strong generalization capability of CLIP has made it a foundation for many methods in adapting pre-trained Vision-Language models (VLMs) for downstream tasks. 
Prompt tuning is a widely used technique in VLMs for learning downstream tasks. 
The use of text prompts, which are instructions provided to the language branch of VLMs, is a common practice to enhance task understanding. Full fine-tuning **Radford et al., "Learning to Generate by Optimizing with Respect to Frequency Components"** and linear probe are two common methods used to adapt VLMs to downstream tasks. %Nevertheless, the limitations of both methods have spurred the investigation of novel techniques inspired by prompt tuning in Natural Language Processing (NLP). 
Some methods **Mnih et al., "Human-level control through deep reinforcement learning"**, fine-tune the CLIP model specifically for few-shot image recognition by optimizing a continuous set of token embeddings in the language branch. 
Moreover, some methods constrain the prompts to contain the essential general knowledge and prior distribution learning **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. 
In addition to multi-modal prompt tuning **Li et al., "Visual BERT: A Simple & Efficient Approach to Vision-Language Pre-Training"**, some methods **Brown et al., "Language Models play Hide and Seek with Adversarial Imitation Learning"** apply adversarial training perspective for prompt tuning to effectively align its V-L representations for pre-trained CLIP.