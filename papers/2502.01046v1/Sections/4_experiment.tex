
\section{Experimental Results}
\label{sec:result}


\subsection{Experimental Setups}
\paragraph{Datasets.~~\xspace}
All our models are pre-trained on three datasets with pairs of face video and speech: RAVDESS~\cite{RAVDESS}, MEAD~\cite{MEAD:conf/eccv/WangWSYWQHQL20, EAT:conf/iccv/GanYYSY23}, and MELD-FAIR~\cite{meldfair:journals/ijon/CarneiroWW23}. 
For data pre-processing, we first resample the audio to 16 kHz, and apply a speech separation model SepFormer~\cite{sepformer/SubakanRCBZ21} to enhance voice. Then, we introduce Whisper~\cite{whisper/RadfordKXBMS23} to filter non-aligned text-speech pairs.
Then, all models are trained on a combination of all three datasets. 
The RAVDESS and MEAD of the combined one are randomly segmented into training, validation, and test sets without any speaker overlap. For the MELD-FAIR, we follow the original splits. 
Additionally, these datasets lack sufficient semantic units in real-world environments, making it challenging to train a TTS model. We incorporate a 10-hour subset from LRS3~\cite{LRS3/abs-1809-00496} for pre-training, allowing the model to be comparable to Face-TTS trained on 400 hours of LRS3. 
Finally, the combined dataset comprises 31.33 hours of audio recordings and 26,767 utterances across 7 basic emotions (\ie~angry, disgust, fear, happy, neutral, sad, and surprised) and 953 speakers.


\paragraph{Evaluation metrics.~~\xspace}
For \taskname, we evaluate the generation performance based on naturalness (\ie~speech quality) and expressiveness. For the naturalness, we employ Mel Cepstral Distortion (MCD)~\cite{visualvoicecloning/ChenTQZLW22} to assess discrepancies between generated and target speech. Additionally, the Word Error Rate (WER)~\cite{WER/WangSZRBSXJRS18,whisper/RadfordKXBMS23} is used to gauge intelligibility. 
For the expressiveness, we calculate cosine similarity metrics based on emotion embeddings~\cite{emo2vec:conf/acl/MaZYLGZ024} and x-vectors~\cite{xvector_tts/DuGCY23} to assess emotion similarity (EmoSim) and speaker identity similarity (SpkSim), as well as the Root Mean Square Error (RMSE) for F0~\cite{RMSEf0:conf/asru/HayashiTKTT17}.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.92\linewidth]{Figures/main_mel.pdf}
    \caption{\textbf{Speech qualitative results.} The red rectangles highlight key regions with acoustic differences or over-smoothing issues, and the red dotted circle shows similar F0 contours with ground truth. Zoom in for more details.}
    \label{fig:mel}
\end{figure*}

\paragraph{Implementation details.~~\xspace}
We implement \methodname based on DiT architecture~\cite{DiT:conf/iccv/PeeblesX23}. We use a log-linear noise schedule $\sigma(t)$~\cite{SEDD:conf/icml/LouME24} where the expectation of the number of masked tokens is linear with $t$. During training, we use the AdamW optimizer~\cite{adamw/LoshchilovH19} with a learning rate of 1e-4, batch size 32. 
The total number of iterations is 300k. 
During inference, we use the Euler sampler to conduct the reverse process with 96 steps. We set the joint guidance scale $w_0=1.9$, and compositional scales $w_1=w_2=1.0,$ $w_3=1.6$. 

\subsection{Quantitative Evaluation}

For quantitative evaluation, we compare \methodname with previous state-of-the-art (SOTA) methods, categorized into two paradigms based on the type of guidance. 
The acoustic-guided methods customize identity using acoustic prompts, and drive emotion generation from either visual or acoustic cues, while the visual-guided methods aim to customize both identity and emotion only from visual conditions.

\paragraph{Objective evaluation.~~\xspace}
As shown in Tab.~\ref{tab:main}, compared with Face-TTS, we achieve 17.35\% and 47.11\% improvements in terms of EmoSim and SpkSim, reflecting the great ability to maintain voice-identity while enhancing consistency.
For prosody modeling, we can estimate a more precise F0 contour with relative 14.95\% gains, exhibiting more natural speech expressiveness. The MCD improved by a relative 18.96\%, indicating minimal acoustic difference with the target speech.
While Face-TTS achieves a better WER by utilizing over 10 times the data, \methodname significantly improves naturalness and consistency with fewer data. 


Notably, we observe that the visual-guided \methodname even outperforms the acoustic-guided methods, which are the most efficient for speech generation by leveraging isomorphic features. It demonstrates that \methodname bridges the cross-modal gap using only heterogeneous face features. 
Furthermore, we introduce the acoustic-guided \textsc{DEmoFace}$^*$ replacing face condition $\bm{c}_\text{id}$ with speech condition $\bm{c}_\text{ge2e}$, which gains greater improvements than other acoustic-guided methods in all metrics by a large margin. 


\paragraph{Subjective evaluation.~~\xspace}
We further conduct the subjective evaluation with 15 participants, to compare our \methodname with SOTA methods.
Specifically, we introduce five mean opinion scores (MOS) with rating scores from 1 to 5 in 0.5 increments, including $\text{MOS}_\text{nat}$, $\text{MOS}_\text{con}$ for speech naturalness (\ie~quality) and consistency (\ie~emotion and speaker similarity). We randomly generate 10 samples from the test set. 
The scoring results of the user study are presented in Tab.~\ref{tab:userstudy}. \methodname demonstrates a clear advantage over SOTA methods in both metrics, particularly in achieving higher $\text{MOS}_\text{nat}$ with 28\% relative improvement, which validates the effectiveness of our method. Furthermore, compared to acoustic-guided EmoSpeech, we achieve better $\text{MOS}_\text{con}$, demonstrating our ability to generate speech with greater emotional and identity consistency.

\input{Tables/main_userstudy}

\subsection{Qualitative Results}
\paragraph{Qualitative comparisons.~~\xspace} 
As shown in Fig.~\ref{fig:mel}, from mel-spectrogram in the first row, we observe severe temporal differences and over-smoothing issues in Face-TTS, EmoSpeech, and V2C-Net, causing duration asynchronization and quality degradation. 
Furthermore, from the F0 curve in the third row, the other baselines exhibit distinct F0 contours showing different pitch, emotion, and intonation with the ground truth (GT).
In contrast, our results are closer to the GT, benefiting from enhanced multi-conditional generation and dynamic synchronization capabilities of \methodname. 



\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.92\linewidth]{Figures/TSNE.pdf}
    \caption{\textbf{t-SNE visualization} of x-vectors from synthesis speeches. Each color represents a different speaker.}
    \label{fig:xvector-TSNE}
\end{figure}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{Figures/main_curve.pdf}
    \caption{\textbf{Ablation study on curriculum learning}. (a) Feature distribution across RVQ levels, with low-level features showing low-frequency patterns. (b)-(d) For the baseline without curriculum learning, we vary the number of training epochs compared with three metrics on the validation set. The effect is evident for WER and EmoSim while slight on SpkSim.}
    \label{fig:CL}
\end{figure*}

\paragraph{Visualization of speaker embeddings.~~\xspace}
To explore the speaker diversity, we utilize t-SNE technique~\cite{T-SNE} to visualize the distribution of x-vectors extracted from the synthesis speech. 
As shown in Fig.~\ref{fig:xvector-TSNE}, Face-TTS exhibits wrong mixing among different speakers and genders, indicating that they may not generate voices with distinguishable styles. 
In contrast, we effectively cluster the speeches from the same speaker, while maintaining stronger speaker-discriminative properties.


\subsection{Ablation Studies}

\paragraph{Ablation on curriculum learning.~~\xspace} 
To demonstrate the effectiveness of curriculum learning, we input all RVQ-level tokens during the whole training process as the variant (a) in Tab.~\ref{tab:ablation}. 
Based on the fact that RVQ Codec preserves semantic information in low-level tokens while retaining acoustic details in high-level tokens~\cite{rvq_level:journals/corr/abs-2410-04380}, Fig.~\ref{tab:ablation} shows:
(1) we achieve better WER and EmoSim than baseline during early training, as prioritized low-level learning effectively captures low-level information; 
(2) SpkSim initially lags due to unseen high-level tokens but improves as they are progressively introduced. 
The results highlight the effectiveness of curriculum learning. 

\input{Tables/main_ablation}
\paragraph{Ablation on identity alignment.~~\xspace}
Due to the heterogeneous differences between speaker features from vision and speech, accurate cross-modal customization is challenging. As shown in Tab.~\ref{tab:ablation}, without identity alignment, the SpkSim drops by 9\%, while noise in the visual features negatively impacts speech generation, causing a decline in all metrics.


\paragraph{Ablation on EPFG.~~\xspace}
% Sim
The EPFG enables an effective generalization across combinations of multiple conditions, even those unseen during pre-training. From Tab.~\ref{tab:ablation}, we observe that EmoSim, SpkSim, and WER metrics have degraded when all conditions are treated as a unified one, showing the incorporation of EPFG can significantly enhance multi-conditional generation quality. 
Furthermore, we conduct a grid search for all parameters of the EPFG on the validation set. As shown in Fig.~\ref{fig:timestep}(a), axes denote two parameter combinations (\ie~$(w_0\in[1.0,2.0],w_1\in[1.0,1.4]),(w_1\in[1.0,1.4],w_2\in[1.0,2.0])$), the color of the grid indicates normalized performance score, and the red rectangle marking the final parameter combination we select. 
We observe that performance degradation (\ie~the light area) with complex entanglement, as the unconditional score dominates with low guidance scales across conditions.

\begin{figure}[t]
    \centering
        \includegraphics[width=1.0\linewidth]{Figures/ablation.pdf}
    \caption{(a) Parameters grid search for the EPFG, with axes as two-parameter combinations, colors as normalized performance. (b) Effect on sampling steps.}
    \label{fig:timestep}
\end{figure}




\paragraph{Ablation on sampling steps.~~\xspace}
To explore the effectiveness of sampling steps, we first normalize each metric to $[0,1]$ and obtain the average performance. 
As shown in Fig.~\ref{fig:timestep}(b), the performance improves with more steps, saturating at 32 steps. It demonstrates that \methodname achieves acceptable generation quality with just 32 steps. To balance performance with efficiency, we utilize 96 steps in this paper.