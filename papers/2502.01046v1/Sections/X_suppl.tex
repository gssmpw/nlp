
\setcounter{footnote}{0}
\setcounter{equation}{0}
\setcounter{table}{0}
\setcounter{figure}{0}

\renewcommand{\theequation}{A-\arabic{equation}}    
\renewcommand{\thetable}{A-\arabic{table}}    
\renewcommand{\thefigure}{A-\arabic{figure}}    

\section*{Appendix}
This appendix provides the following extra contents: 
\begin{itemize}
\item Appendix~\ref{sec:define} shows detailed notations and definitions; 
\item Appendix~\ref{sec:ap_discrete_diffusion} provides a preliminary of the discrete diffusion model; 
\item Appendix~\ref{sec:pfg} presents a detailed derivation of enhanced predictor-free guidance; 
\item Appendix~\ref{sec:dataset} includes the statics of datasets used in this paper;  \item Appendix~\ref{sec:model} supplements the experimental details of our \methodname
and each baseline method; 
\item Appendix~\ref{sec:results} contains extra experimental results; \item Appendix~\ref{sec:user} incorporates the details of subjective evaluation; and 
\item Appendix~\ref{sec:impact}  discusses the social impact and limitations.
\end{itemize}
\section{Detailed Notations and Definitions}
\label{sec:define}
Tab.~\ref{tab:definition} provides the notations and definitions of variables used in the paper.
\begin{table}[h]
    \centering
    \begin{tabular}{r|l}
    \toprule
    \textbf{Notation} & \textbf{Definition} \\ 
    \hline
     $\bm{x}$& Vector variables representing a sequence of tokens.
    \\ 
    $x^i,\hat{x},x^i_t,\hat{x}^i_t$& Scalar variables representing token or state in discrete diffusion process. 
    \\ 
    $\mathcal{X}^d$& State space $\{1, \ldots, n\}^d$ of token sequence length $d$ and token dimension $n$.
    \\ $\mQ_t$& Diffusion forward matrix (\ie transition rate matrix) at $t$ time.
    \\ $\bar{\mQ}_t$& Diffusion reverse matrix at $t$ time.
    \\ $\mQ^\text{tok}$& Token-level transition rate matrix filled with absorbing state \texttt{[MASK]}. 
    \\ $\delta_{x\hat{x}}$& Kronecker delta function of two variables $x, \hat{x}$, which is 1 if the variables are equal, and 0 otherwise.
    \\ $p$& Probability distribution of the forward diffusion process characterized by $\mQ_t$.
    \\ $q_\theta$& Probability distribution of the reverse diffusion process characterized by score model $s_\theta$.
    \\ $\mP_{t|0}$& Transition probability matrix from time $0$ to time $t$.
    \\ $s_\theta$& Score network to estimate the ratio (\ie~ concrete score) $\frac{p(x^i_{t- \Delta t})}{p(x^i_t)}$.
    \\ $c_{\hat{{x}}_t {x}_t}$& Concrete score $\frac{p\left({\hat{{x}}_t} \mid {x}_0\right)}{p\left({x}_t \mid {x}_0\right)}$.
    \\ $\text{N}(c)$& Normalizing constant function of denoising score entropy loss.
    \\ $r_i$& $i$-th level of RVQ tokens, where $1\leq i\leq 12$. $r_l$ denotes the current max level during our curriculum training.
    \\ $C_\text{code}$& Codebook size.
    \\ $d_\text{tok}$& Length of the token sequence.
    \\ $\bm{c}$& Condition set including $\bm{c}_\text{id},\bm{c}_\text{emo},\bm{c}_\text{text}$.
    \\ $\alpha_1,\gamma_1,\beta_1$& Scale and shift parameters for the adaptive layer normalization.\\
    \bottomrule
    \end{tabular}
    \caption{Detailed Notations and Definitions}
    \label{tab:definition}
\end{table}


\section{Preliminary: Discrete Diffusion Models}
\label{sec:ap_discrete_diffusion}

Continuous Diffusion Models (CDM) have been one of the most prominent and active areas in generative modeling~\cite{videodiff_latent/BlattmannRLD0FK23,styletts2/LiHRMM23,mmgeneration/RuanMYH0FYJG23}
, which has shown state-of-the-art performance in various fields. 
However, for speech generation, the high-dimensional speech features and excessive diffusion steps lead to high resource usage and inefficient inference, frustrating practical application. 
The fundamental way lies in compressing the speech feature space, such as a discrete space. 

In recent years, Discrete diffusion models (DDM) have shown promise in language modeling~\cite{D3PM:conf/nips/AustinJHTB21,ConcreteScoreMatch:conf/nips/MengCSE22,SEDD:conf/icml/LouME24,RADD:journals/corr/abs-2406-03736}, which are characterized by a forward and reverse process like continuous diffusion models~\cite{DDIM/SongME21,DDPM/HoJA20}. Nevertheless, DDM has yet to be explored in speech generation. In this study, we introduce the DDM for speech token generation. Below, we outline the forward and reverse processes, along with the training objective.

\paragraph{Forward diffusion process.~~\xspace}
Given a sequence of tokens $\bm{x} = x^1 \ldots x^d$ from a state space of length $d$ like $\mathcal{X}^d = \{1, \ldots, n\}^d$. The continuous-time discrete Markov chain at time $t$ is characterized by the diffusion matrix $\mQ_t\in \mathbb{R}^{n^d\times n^d}$ (\ie~transition rate matrix), as follows:
\begin{equation}
\label{eq:ap_delta_transition}
    p({x}_{t+ \Delta t}^i|x_t^i) =  \delta_{{x}^i_{t+ \Delta t}x^i_t} + \mQ_t({x}^i_{t+ \Delta t},x^i_t)\Delta t + o(\Delta t),
\end{equation}
where $x^i_t$ denotes $i$-th element of $\bm{x}_t$, $\mQ_t({x}^i_{t+ \Delta t},x^i_t)$ is the $({x}^i_{t+ \Delta t},x^i_t)$ element of $\mQ_t$, denoting the transition rate from state $x^i_t$ to state ${x}^i_{t+ \Delta t}$ at time $t$, and $\delta$ is the Kronecker delta. Since the exponential size of $\mQ_t$, existing works~\cite{SEDD:conf/icml/LouME24,RADD:journals/corr/abs-2406-03736} propose to assume dimensional independence, conducting a one-dimensional diffusion process for each dimension with the same token-level diffusion matrix $\mQ_t^\text{tok}=\sigma(t)\mQ^\text{tok}\in \mathbb{R}^{n\times n}$, where $\sigma(t)$ is the noise schedule and $\mQ^\text{tok}$ is designed to diffuse towards an absorbing state \texttt{[MASK]} in this study. Then the forward equation is formulated as follows: 
\begin{equation}
\label{eq:ap_forward_diffusion}
    \mP({x}^i_t,x^i_0) = \exp\left(\bar{\sigma}(t) \mQ^\text{tok}({x}^i_t,x^i_0) \right), 
\end{equation}
where transition probability matrix $\mP({x}^i_t,x^i_0) := p({x}^i_t|x_0)$, and cumulative noise $\bar{\sigma}(t) = \int_0^t \sigma(s)ds$. There are two probabilities in the $\mP_{t|0}$: \( 1 - e^{-\bar{\sigma}(t)} \) for replacing the current tokens with \texttt{[MASK]}, \( e^{-\bar{\sigma}(t)} \) for keeping it unchanged, where the diffusion transition rate matrix $\mQ^\text{tok}$ is defined as:
\begin{align}
    \mQ^\text{tok}=\left[\begin{array}{ccccc}
    \footnotesize
            -1     & 0      & \cdots & 0      & 1      \\
            0      & -1     & \cdots & 0      & 1      \\
            \vdots & \vdots & \ddots & \vdots & \vdots \\
            0      & 0      & \cdots & -1     & 1      \\
            0      & 0      & \cdots & 0      & 0
    \end{array}\right]. 
    \label{eq:ap_absorb_q}
\end{align}

Therefore, we can parallel sample the corrupted sequence $\bm{x}_t$ directly from $\bm{x}_0$ in one step. During the inference, we start from $\bm{x}_{T}$ filled with \texttt{[MASK]} tokens and iteratively sample new set of tokens $\bm{x}_{t-1}$ from $p_{\theta}(\bm{x}_{t-1}|\bm{x}_{t})$. 


\paragraph{Reverse denoising process.~~\xspace}
As the transition rate matrix $\mQ^\text{tok}_t$ is known, the reverse process can be given by a reverse transition rate matrix $\bar{\mQ}_t$~\cite{SCDDM:conf/iclr/SunYDSD23,kelly2011reversibility}
, where $\bar{\mQ}_t(x^i_{t- \Delta t},x^i_t)=\frac{ p(x^i_{t- \Delta t})}{p(x^i_t)} \mQ^\text{tok}_t(x^i_t,x^i_{t- \Delta t})$ and $x^i_{t- \Delta t}\neq x^i_t$, or $\bar{\mQ}_t(x^i_{t- \Delta t}, x^i_t) =  - \sum_{z \neq x_t} \bar{\mQ}_t(z,x^i_t)$. 
The reverse equation is formulated as follows: 
\begin{equation}
    \label{eq:ap_backward}
    p({x}^i_{t- \Delta t}|x^i_t) =  \delta_{{x}^i_{t- \Delta t}x^i_t} + \bar{\mQ}_t({x}^i_{t- \Delta t},x^i_t)\Delta t + o(\Delta t),\\
\end{equation}
where we can estimate the ratio $\frac{p(x^i_{t- \Delta t})}{p(x^i_t)}$ (which is known as the \textit{concrete score}~\cite{SEDD:conf/icml/LouME24,ConcreteScoreMatch:conf/nips/MengCSE22} to measure the \textit{transition probability or closeness} from a state $x^i$ at time $t$  to a state $\hat{x}^i$ at time $t- \Delta t$) of $\bar{\mQ}_t$ by a score network $s_\theta({x}^i_t,t)_{x^i_{t- \Delta t}} \approx [\frac{p(x^i_{t- \Delta t})}{p(x^i_t)}]_{x^i_{t}\neq x^i_{t- \Delta t}}$. 
So that the reverse matrix is parameterized to model the reverse process $q_\theta({x}^i_{t- \Delta t}|x^i_t)$ (\ie~parameterize the concrete score). 

\paragraph{Training objective.~~\xspace}
Denoising score entropy (DSE)~\cite{SEDD:conf/icml/LouME24} is introduced to train the score network $s_\theta$:
\begin{equation}
\begin{aligned}
       \int_0^T \mathbb{E}_{\bm{x}_t \sim p\left(\bm{x}_t \mid \bm{x}_0\right)} \sum_{{\hat{\bm{x}}_t} \neq \bm{x}_t} \mQ_t\left( \hat{{x}}^i_t,{x}^i_t\right)  \Big[s_\theta\left({x}^i_t, t\right)_{\hat{{x}}^i_t}  
        - c_{\hat{{x}}^i_t {x}^i_t} \log s_\theta\left({x}^i_t, t\right)_{\hat{{x}}^i_t}+  \text{N}(c_{\hat{{x}}^i_t {x}^i_t})\Big] dt ,
\end{aligned}
\label{eq:ap_score_entropy}
\end{equation}
where the concrete score $c_{\hat{{x}}^i_t {x}^i_t} = \frac{p\left({\hat{{x}}^i_t} \mid {x}^i_0\right)}{p\left({x}^i_t \mid {x}^i_0\right)}$ and a normalizing constant function $\text{N}(c):= c \log c - c$ that ensures loss non-negative.
% where the concrete score $c_{\hat{x}^i_t x^i_t} = \frac{p\left({\hat{x}^i_t} \mid x^i_0\right)}{p\left(x^i_t \mid x^i_0\right)}$ and a normalizing constant function $K(c):= c \log c - c$. 
After training, we can replace the concrete score with the trained score network on~\cref{eq:ap_backward}, conducting the sampling process.


\section{Derivation of Enhanced Predictor-free Guidance}
\label{sec:pfg}
% \paragraph{Enhanced predictor-free guidance under energy-based model view.~~\xspace}
For the discrete diffusion model, given the random variable value $\bm{x}_t=x^1_t \ldots x^d_t$ from a state space of length $d$ with the absorbing state \texttt{[MASK]} at time $t$, the \textit{unconditional} probability distribution $p({x}^i_{t- \Delta t}|x^i_t) = \delta_{{x}^i_{t- \Delta t}x^i_t} + \bar{\mQ}_t({x}^i_{t- \Delta t},x^i_t)\Delta t + o(\Delta t)$ during sampling as \cref{eq:ap_backward} shown. For the \textit{conditional} probability distribution $p({x}^i_{t- \Delta t}|x^i_t, \bm{c})$, the key is to obtain the conditional transition rate matrix $\bar{\mQ}_t({x}^i_{t- \Delta t},x^i_t|\bm{c})$. 

Firstly, following~\cite{CFG_DDM:journals/corr/abs-2406-01572} to simplify the notation, we define $x_t^i$ as $x_t$ and utilize the properties of the Kronecker delta $\delta$ (\ie~the function is 1 if the variables are equal, and 0 otherwise) to derive another form of the unconditional probability distribution $p({x}_{t- \Delta t}=\hat{x}|x_t=x)$:
\begin{equation}
\label{eq:upd_form}
\begin{aligned}
    p ( x_{t-\Delta t} = \hat{x} | x_{t} = x ) & = \delta_{{x}_{t- \Delta t}=\hat{x}, x_t=x} + \bar{\mQ}_t({x}_{t- \Delta t}=\hat{x},x_t=x)\Delta t + o(\Delta t) \\
    & = \delta_{\hat{x}, x} + \delta_{\hat{x}, x}\bar{\mQ}_t(\hat{x},x)\Delta t + (1-\delta_{\hat{x}, x})\bar{\mQ}_t(\hat{x},x)\Delta t + o(\Delta t) \\
    & = \delta_{\hat{x}, x}\left(1+\bar{\mQ}_t(x,x)\Delta t \right) + (1-\delta_{\hat{x}, x})\bar{\mQ}_t(\hat{x},x)\Delta t + o(\Delta t).
\end{aligned}    
\end{equation}

Then, we utilize the formulation in~\cref{eq:upd_form} and the Bayes rule to build the conditional probability distribution $p({x}_{t- \Delta t}=\hat{x}|x_t=x, \bm{c})$, combining predictive distribution $p(\bm{c}|x)$ and unconditional distribution $p({x}_{t- \Delta t}=\hat{x}|x_t=x)$ as:
\begin{equation}
\label{eq:bayes_cond_p_1}
\begin{aligned}
 p ( x_{t-\Delta t} =\hat{x} | x_{t}=x, \bm{c} ) &=\frac{p ( \bm{c} | x_{t-\Delta t}=\hat{x}, x_{t}=x ) p ( x_{t-\Delta t}=\hat{x} | x_{t}=x )} {p ( \bm{c} | x_{t}=x )} \\
& =\frac{p ( \bm{c} | x_{t-\Delta t}=\hat{x}, x_{t}=x ) p ( x_{t-\Delta t}=\hat{x} | x_{t}=x )}{\sum\limits_{x'} p ( \bm{c} | x_{t-\Delta t}=x',x_{t}=x )p (x_{t-\Delta t}=x'|x_{t}=x )} \\
& =\frac{\frac{p ( \bm{c} | x_{t-\Delta t}=\hat{x}, x_{t}=x ) }{p ( \bm{c} | x_{t-\Delta t}={x}, x_{t}=x ) }\left[\delta_{\hat{x}, x}\left(1+\bar{\mQ}_t(x,x)\Delta t \right) + (1-\delta_{\hat{x}, x})\bar{\mQ}_t(\hat{x},x)\Delta t + o(\Delta t)\right] }{\sum\limits_{x'} \frac{p ( \bm{c} | x_{t-\Delta t}=x',x_{t}=x )}{p ( \bm{c} | x_{t-\Delta t}=x,x_{t}=x )}\left[\delta_{\hat{x}, x}\left(1+\bar{\mQ}_t(x,x)\Delta t \right) + (1-\delta_{{x'}, x})\bar{\mQ}_t({x'},x)\Delta t + o(\Delta t)\right] },
\end{aligned}
\end{equation}
where we use~\cref{eq:upd_form} to replace $p ( x_{t-\Delta t}=\hat{x} | x_{t}=x )$ in~\ref{eq:bayes_cond_p_1} and define $p_c(\hat{x},x) \equiv p(\bm{c} | x_{t-\Delta t}=\hat{x}, x_{t}=x )$. We further simplify the formulation:
\begin{equation}
\label{eq:bayes_cond_p_2}
\begin{aligned}
p ( x_{t-\Delta t} =\hat{x} | x_{t}=x, \bm{c} )  & =\frac{\frac{p_c(\hat{x},x)}{p_c(x,x)}\left[\delta_{\hat{x}, x}\left(1+\bar{\mQ}_t(x,x)\Delta t \right) + (1-\delta_{\hat{x}, x})\bar{\mQ}_t(\hat{x},x)\Delta t + o(\Delta t)\right] }{\sum_{x'} \frac{p_c({x'},x)}{p_c(x,x)}\left[\delta_{\hat{x}, x}\left(1+\bar{\mQ}_t(x,x)\Delta t \right) + (1-\delta_{{x'}, x})\bar{\mQ}_t({x'},x)\Delta t + o(\Delta t)\right] }\\
& =\frac{\delta_{\hat{x}, x}\left(1+\bar{\mQ}_t(x,x)\Delta t \right) + \frac{p_c(\hat{x},x)}{p_c(x,x)} (1-\delta_{\hat{x}, x})\bar{\mQ}_t(\hat{x},x)\Delta t + o(\Delta t) }{\left(1+\bar{\mQ}_t(x,x)\Delta t \right) + \sum_{x'\neq x}\frac{p_c({x'},x)}{p_c(x,x)}\bar{\mQ}_t({x'},x)\Delta t + o(\Delta t)}\\
& =\frac{\delta_{\hat{x}, x}\left(1+\bar{\mQ}_t(x,x)\Delta t \right) + \frac{p_c(\hat{x},x)}{p_c(x,x)} (1-\delta_{\hat{x}, x})\bar{\mQ}_t(\hat{x},x)\Delta t + o(\Delta t) }{1+f({\Delta t,x,x'})},
\end{aligned}
\end{equation}
where $f$ is a function of $\Delta t$, and as $\Delta t\rightarrow0$ we can use  Taylor expansion of $\frac{1} {1+f({\Delta t,x,x'})} \approx1-f({\Delta t,x,x'})+o ( \Delta t^{2} )$:
\begin{equation}
\label{eq:bayes_cond_p_3}
\begin{aligned}
p ( x_{t-\Delta t} =\hat{x} | x_{t}=x, \bm{c} )   =&[\delta_{\hat{x}, x}\left(1+\bar{\mQ}_t(x,x)\Delta t \right) + \frac{p_c(\hat{x},x)}{p_c(x,x)} (1-\delta_{\hat{x}, x})\bar{\mQ}_t(\hat{x},x)\Delta t + o(\Delta t)]\\
&\times [1-\bar{\mQ}_t(x,x)\Delta t - \sum_{x'\neq x}\frac{p_c({x'},x)}{p_c(x,x)}\bar{\mQ}_t({x'},x)\Delta t + o(\Delta t)]\\
 =& \delta_{\hat{x}, x}\left(1 -  \sum_{x'\neq x} \frac{p_c({x'},x)}{p_c(x,x)}\bar{\mQ}_t({x'},x)\Delta t\right) + (1-\delta_{\hat{x}, x})\frac{p_c(\hat{x},x)}{p_c(x,x)} \bar{\mQ}_t(\hat{x},x)\Delta t + o(\Delta t). 
\end{aligned}
\end{equation}

From the expression in~\cref{eq:upd_form} and property of the $\bar{\mQ}$ (\ie~$\bar{\mQ}_t(x,x) + \sum_{x'\neq x}\bar{\mQ}_t(x',x)=0$)~\cite{SEDD:conf/icml/LouME24}, we can derive our conditional transition rate matrix:
\begin{equation}
\label{eq:q_cond_matrix}
\begin{aligned}
\bar{\mQ}_t(\hat{x},x|\bm{c}) = \frac{p_c(\hat{x},x)}{p_c(x,x)} \bar{\mQ}_t(\hat{x},x),
% = \frac{p(\bm{c} | x_{t-\Delta t}=\hat{x}, x_{t}=x )}{p(\bm{c} | x_{t-\Delta t}={x}, x_{t}=x )} \bar{\mQ}_t(\hat{x},x),
\end{aligned}
\end{equation}
where can be deduced that the matrix also satisfies the same property $\bar{\mQ}_t(x,x|\bm{c}) + \sum_{x'\neq x}\bar{\mQ}_t(x',x|\bm{c})=0$. Therefore, we can rewrite~\cref{eq:bayes_cond_p_3} as:
\begin{equation}
\label{eq:bayes_cond_p_4}
\begin{aligned}
p ( x_{t-\Delta t} =\hat{x} | x_{t}=x, \bm{c} )  & = \delta_{\hat{x}, x}\left(1 + \bar{\mQ}_t(x,x|\bm{c})\Delta t\right) + (1-\delta_{\hat{x}, x})\bar{\mQ}_t(\hat{x},x|\bm{c})\Delta t + o(\Delta t)\\
& = \delta_{\hat{x}, x} + \bar{\mQ}_t(\hat{x},x|\bm{c})\Delta t + o(\Delta t).
\end{aligned}
\end{equation}

Furthermore, to achieve predictor-free guidance~\cite{CFG/abs-2207-12598,CFG_DDM:journals/corr/abs-2406-01572}, we use the Bayes rule to relive the dependence on any predictor/classifier: 
\begin{equation}
\label{eq:PFG_bayes}
\begin{aligned}
\bar{\mQ}_t(\hat{x},x|\bm{c}) &= \frac{p(\bm{c} | x_{t-\Delta t}=\hat{x}, x_{t}=x )}{p(\bm{c} | x_{t-\Delta t}={x}, x_{t}=x )} \bar{\mQ}_t(\hat{x},x)\\
&= \frac{p(x_{t-\Delta t}=\hat{x}|x_{t}=x,\bm{c})\bcancel{p(\bm{c}|x_t=x)}}{p(x_{t-\Delta t}=\hat{x}|x_{t}=x)} \frac{p(x_{t-\Delta t}=x|x_{t}=x)}{p(x_{t-\Delta t}=x|x_{t}=x,\bm{c})\bcancel{p(\bm{c}|x_t=x)}}\bar{\mQ}_t(\hat{x},x)\\
&= \frac{p(x_{t-\Delta t}=\hat{x}|x_{t}=x,\bm{c})}{p(x_{t-\Delta t}=x|x_{t}=x,\bm{c})}\frac{p(x_{t-\Delta t}=x|x_{t}=x)}{p(x_{t-\Delta t}=\hat{x}|x_{t}=x)}\bar{\mQ}_t(\hat{x},x)\\
&\approx \frac{s_\theta(x,t,\bm{c})_{\hat{x}}}{s_\theta(x,t)_{\hat{x}}}\bar{\mQ}_t(\hat{x},x),
\end{aligned}
\end{equation}
where we utilize the concrete score $s_\theta(x,t,\bm{c})_{\hat{x}}$ in~\cref{eq:backward} to estimate the ratio like $\frac{p(x_{t-\Delta t}=\hat{x}|x_{t}=x,\bm{c})}{p(x_{t-\Delta t}=x|x_{t}=x,\bm{c})}$. Similar to previous methods, we can introduce a guidance scale $w$ (\ie~ guidance strength) as:
\begin{equation}
\label{eq:PFG_bayes1}
\begin{aligned}
\bar{\mQ}_t(\hat{x},x|\bm{c})\approx {\frac{s^w_\theta(x,t,\bm{c})_{\hat{x}}}{s^w_\theta(x,t)_{\hat{x}}}}\bar{\mQ}_t(\hat{x},x)={\frac{s^w_\theta(x,t,\bm{c})_{\hat{x}}}{s^w_\theta(x,t)_{\hat{x}}}}\left(s_\theta(x,t)_{\hat{x}}\bar{\mQ}^\text{tok}_t(\hat{x},x)\right)={\frac{s^w_\theta(x,t,\bm{c})_{\hat{x}}}{s^{w-1}_\theta(x,t)_{\hat{x}}}}\mQ^\text{tok}_t(\hat{x},x)
\end{aligned}
\end{equation}
where $\mQ^\text{tok}$ is the fixed diffusion transition rate matrix in~\cref{eq:absorb_q}. 
Since $\bm{c} = \{\bm{c}_1,\ldots,\bm{c}_k\}$ contains $k$ independent conditions, we can rewrite~\cref{eq:PFG_bayes} into multi-conditional form as:
\begin{equation}
\label{eq:PFG_bayes3}
\begin{aligned}
\bar{\mQ}_t(\hat{x},x|\bm{c}_1,\ldots,\bm{c}_k) &= \bar{\mQ}_t(\hat{x},x)\prod\limits_{i=1}^{k}\frac{p(\bm{c}_i | x_{t-\Delta t}=\hat{x}, x_{t}=x )}{p(\bm{c}_i | x_{t-\Delta t}={x}, x_{t}=x )} \\
&= \bar{\mQ}_t(\hat{x},x) \prod\limits_{i=1}^{k} \frac{p(x_{t-\Delta t}=\hat{x}|x_{t}=x,\bm{c}_i)}{p(x_{t-\Delta t}=x|x_{t}=x,\bm{c}_i)}\frac{p(x_{t-\Delta t}=x|x_{t}=x)}{p(x_{t-\Delta t}=\hat{x}|x_{t}=x)}\\
&\approx \bar{\mQ}_t(\hat{x},x) \prod\limits_{i=1}^{k}\frac{s^{w_i}_\theta(x,t,\bm{c}_i)_{\hat{x}}}{s^{w_i}_\theta(x,t)_{\hat{x}}}\\
&=\left[s_\theta(x,t)_{\hat{x}}\prod\limits_{i=1}^{k}\frac{s^{w_i}_\theta(x,t,\bm{c}_i)_{\hat{x}}}{s^{w_i}_\theta(x,t)_{\hat{x}}}\right]\mQ^\text{tok}_t(\hat{x},x).
\end{aligned}
\end{equation}




Energy-Based Models (EBMs)~\cite{EBM:conf/iccv/GuoMJYYL23,EBM:conf/icml/GengHJZCHL24,compositional:conf/eccv/LiuLDTT22} are a class of generative models and also known as non-normalized probabilistic models. Given speech token sequence $\bm{x}$ and a learnable neural network $f_\theta$, the probability distribution of EBM can be formulated as:
\begin{equation}
\label{eq:ebm1}
\begin{aligned}
    p_\theta\left(\bm{x}\right) = \frac{e^{f_\theta(x)}}{Z},
\end{aligned}
\end{equation}
where $Z=\sum_{\bm{x}\in \mathcal{X}}e^{f_\theta(x)}$ is a normalizing constant, and $f_\theta$ is the energy function. 
Inspired by the formulation of the EBM, the score can also be formulated as $\hat{s}_\theta(x)_{\hat{x}}\approx \frac{p_{\theta} ( \hat{x} )} {p_{\theta} ( x )}=\frac{e^{f_{\theta} ( \hat{x} )} / Z} {e^{f_{\theta} ( x )} / Z}=\frac{e^{f_{\theta} ( \hat{x} )}} {e^{f_{\theta} ( x )}}$, where $x = x_t, \hat{x} = x_{t- \Delta t}$, $Z$ is the normalizing constant, and $f_\theta$ is the energy function. As we typically define the energy function as a sum of multiple terms~\cite{kim2016deep}, we can associate each term with the joint and compositional ones, and the final probability distribution is expressed as a product of both.
Hence, we can obtain the modulated score $\hat{s}_\theta\left({x}, t\right)_{\hat{x}}$ by multiplying the compositional score and joint score (\ie~sum up the energy functions):
\begin{equation}
\label{eq:pfg_ap}
    \hat{s}^{(w)}_\theta\left({x},t\right)_{\hat{x}} \!=\!   \underbrace{s_\theta(x,t)_{\hat{x}}\prod\limits_{k=1}^{K} \tfrac{s^{w_i} _\theta(x,t,\bm{c}_k)_{\hat{x}}}{s^{w_i} _\theta(x,t)_{\hat{x}}}}_\text{Compositional} \cdot \underbrace{\tfrac{s^{w_0} _\theta(x,t,\bm{c})_{\hat{x}}}{s^{w_0-1} _\theta(x,t)_{\hat{x}}}}_\text{Joint},
\end{equation}
where $\bm{c}=\{\bm{c}_\text{id},\bm{c}_\text{emo},\bm{c}_\text{text}\}$, $w_0$ controls the scale of guidance strength for the joint injection of all conditions, while $w_i$ for $1\leq i \leq k$ is assigned to each independent attribute (\ie~ identity, emotion, and semantics with $k=3$). Therefore, we can rewrite~\cref{eq:bayes_cond_p_4} as:
\begin{equation}
\label{eq:bayes_cond_p_5}
\begin{aligned}
p ( x_{t-\Delta t} =\hat{x} | x_{t}=x, \bm{c} )  & = \delta_{\hat{x}, x} + \bar{\mQ}_t(\hat{x},x|\bm{c})\Delta t + o(\Delta t)\\
& \approx \delta_{\hat{x}, x} + \hat{s}^{(w)}_\theta\left({x},t\right)_{\hat{x}}\mQ^\text{tok}_t(\hat{x},x)\Delta t + o(\Delta t)
\end{aligned}
\end{equation}

\section{Datasets}
\label{sec:dataset}
\subsection{Dataset Statistics}
All our models are pre-trained on three datasets with pairs of face video and speech: RAVDESS~\cite{RAVDESS}, MEAD~\cite{MEAD:conf/eccv/WangWSYWQHQL20, EAT:conf/iccv/GanYYSY23}, and MELD-FAIR~\cite{meldfair:journals/ijon/CarneiroWW23}. 
The RAVDESS contains 1,440 English utterances voiced by 12 male and 12 female actors with eight different emotions. The MEAD is a talking-face video corpus featuring 60 actors and actresses talking with eight different emotions at three different intensity levels. The MELD-FAIR introduces a novel pre-processing pipeline to fix noisy alignment issues of the MEAD~\cite{MEAD:conf/eccv/WangWSYWQHQL20} consisting of text-audio-video pairs extracted from the \textit{Friends} TV series. 
Then, for the training, we train all our models using a combination of all three datasets. 
The RAVDESS and MEAD of the combined one are randomly segmented into training, validation, and test sets without any speaker overlap. In contrast, we follow the original splits of the MELD-FAIR dataset with speaker overlap. 
Additionally, these datasets lack sufficient semantic units in real-world environments, making it challenging to train a TTS model. We incorporate a 10-hour subset from LRS3~\cite{LRS3/abs-1809-00496} for pre-training, allowing the model to be comparable to Face-TTS trained on 400 hours of LRS3. 
Finally, the combined dataset comprises 31.33 hours of audio recordings and 26,767 utterances across 7 basic emotions (\ie~ angry, disgust, fear, happy, neutral, sad, and surprised) and 953 speakers.

\subsection{Data Preprocessing Details}
For data pre-processing, considering the presence of non-primary speakers and background noise such as audience interactions in the recordings, we first resample the audio to a single-channel 16-bit at 16 kHz format, then apply SepFormer~\cite{sepformer/SubakanRCBZ21}, a state-of-the-art model in speech separation, to isolate the primary speaker’s audio and reduce noise from other voices. Then, we introduce an automatic speech recognition model Whisper~\cite{whisper/RadfordKXBMS23} to filter non-aligned text-speech pairs (\ie~ WER higher than 10\%).



\section{Model Details}
\label{sec:model}
\subsection{Implementation Details of \methodname}
\label{sec:our_model}
Table~\ref{tab:model_detail} shows more details about our \methodname. 
Firstly, for our multimodal diffusion transformer, it contains 12 MM-DiT blocks, with channel numbers 768, attention heads 12 for each block. We train the model using the AdamW optimizer~\cite{adamw/LoshchilovH19} with $\beta_1=0.9$, $\beta_2=0.999$, a learning rate of 1e-4, batch size 32, and a 24GB NVIDIA RTX 4090 GPU. The total number of iterations is 300k. For a fair comparison, we do not perform any pre-training or fine-tuning on the test set. During inference, we use the Euler sampler with 96 steps following~\cite{SEDD:conf/icml/LouME24}. 

Secondly, we train our identity encoder achiving face-speech alignment on a 24GB NVIDIA 4090 GPU, with a total batch size of 12 samples. We use the AdamW optimizer~\cite{adamw/LoshchilovH19} with $\beta_1=0.9$, $\beta_2=0.999$, $\epsilon=10\text{e-9}$. It takes 80k steps for training until convergence.

Lastly, we design frame-level duration predictor to predict the target speech duration during inference, which obtains the total duration of the target speech through summing up the phoneme-level inputs. We directly estimate the total target speech duration instead of the phoneme-level durations. The duration predictor has three convolution layers and a MLP architecture to predict duration from the frozen SpeechT5~\cite{speecht5:conf/acl/AoWZ0RW0KLZWQ0W22} encoder. 
The predictor is trained using the AdamW optimizer with 0.9 and 0.999. The initial learning rate is set to 1e-4 with a learning rate decay of 0.999. We use a total batch size of 32 and train the model with 1 NVIDIA 4090 GPUs at least 100k steps. 


\begin{table}[h]
\centering
\begin{tabular}{@{}cll@{}}
\toprule
\textbf{Model}                & \textbf{Configuration}       & \textbf{Parameter}                        \\ \midrule
\multirow{9}{*}{Multimodal Diffusion Transformer} 
                              & In / Out Channels &  1 / 1                                  \\
                              & Number of Transformer Blocks &  12                       \\
                              & Hidden Channel &  768        \\
                              & Attention Heads &  12                                \\
                              & $\bm{c}^\text{id}$ Identity Embedding Dimension &  256                    \\
                              & $\bm{c}^\text{emo}$ Emotion Embedding Dimension &  128                    \\
                              & $\bm{c}^\text{text}$ Text Embedding Dimension &  768                    \\
                              & Activate Function &  SiLU                           \\ 
                              & Dropout &  0.1                           \\\midrule
\multirow{9}{*}{Speech Codec} 
                              & Input &  Waveform                                \\
                              & Sampling Rate &  24kHz                                \\
                              & Hopsize &  480                       \\
                              & Number of RVQ Blocks &  12          \\
                              & Codebook size &  1024          \\
                              & Coodbook Dimension &  8          \\
                              & Decoder Hidden Dimension &  512          \\
                              & Decoder Kernel Size &  12          \\
                              & Number of Decoder Blocks &  30          \\\midrule            
\multirow{4}{*}{Identity Encoder} 
                            & ArcFace-Net Output Dimension &  512          \\
                              & FaceNet Output Dimension & 512                              \\
                              & MLP Channels &  (512, 512, 256, 256)                      \\
                              & Activate Function &  GeLU                  \\ \midrule
\multirow{4}{*}{Duration Predictor} 
                        & Input &  SpeechT5 Text Embedding           \\
                          & Conv Channel & 256                              \\
                          & Conv Kernel & 5                              \\
                          & MLP Channels &  (256, 1)                      \\
                          & Activate Function &  ReLU                  \\ \bottomrule
\end{tabular}
\caption{Implementation details about our \methodname.}
\label{tab:model_detail}
\end{table}



\subsection{Implementation Details of Baselines}

\paragraph{EmoSpeech.~~\xspace}
Accroding to EmoSpeech~\cite{emospeech:conf/ssw/DiatlovaS23} official code\footnote{\href{https://github.com/deepvk/emospeech}{https://github.com/deepvk/emospeech}}, we reproduce training process on our pre-training dataset. 
EmoSpeech introduces a conditioning mechanism that captures the relationship between speech intonation and the emotional intensity assigned to each token in the sequence.
Then we train EmoSpeech following the original setting of its paper. We use the Adam optimizer~\cite{adam/KingmaB14} with $\beta_1$ = 0.5, $\beta_2$ = 0.9, $\epsilon = 10^{-9}$ and follow the same learning rate schedule in vanilla transformer~\cite{attentionallyou/VaswaniSPUJGKP17}. It takes 300k steps with batch size 64 for training until convergence in a single GPU. In the inference process, the output mel-spectrograms of the EmoSPeech are also transformed into speech samples using the pre-trained vocoder\footnote{\href{https://github.com/jik876/hifi-gan}{https://github.com/jik876/hifi-gan}}.


\paragraph{FastSpeech 2.~~\xspace}
Since FastSpeech 2~\cite{fastspeech2/0006H0QZZL21} is not open source and emotion-awareness, we reproduce its method on our pre-training dataset based on the code\footnote{\href{https://github.com/ming024/FastSpeech2}{https://github.com/ming024/FastSpeech2}} and its emotion-aware version on V2C-Net~\cite{visualvoicecloning/Cong0QZWWJ0H23}. 
To model the emotion-awareness in FastSpeech 2, following previous methods~\cite{visualvoicecloning/ChenTQZLW22,visualvoicecloning/Cong0QZWWJ0H23}, we utilize emotion embeddings from an emotion encoder I3D~\cite{i3d/CarreiraZ17} and speaker embeddings extracted via a generalized end-to-end speaker verification model~\cite{GE2E/WanWPL18} as additional inputs. These embeddings are projected and added to hidden embeddings before the variance adaptor. 
Then we train FastSpeech 2 following the original setting of its paper. We use the Adam optimizer~\cite{adam/KingmaB14} with $\beta_1$ = 0.9, $\beta_2$ = 0.98, $\epsilon = 10^{-9}$ and follow the same learning rate schedule in vanilla transformer~\cite{attentionallyou/VaswaniSPUJGKP17}. It takes 300k steps with batch size 48 for training until convergence. In the inference process, the output mel-spectrograms of the FastSpeech 2 are also transformed into speech samples using the pre-trained vocoder.

\paragraph{V2C-Net.~~\xspace}
The V2C-Net~\cite{visualvoicecloning/ChenTQZLW22} is not open source, so we reproduce its method based on its original paper and project\footnote{\href{https://github.com/chenqi008/V2C}{https://github.com/chenqi008/V2C}}. To exploit the emotion from the reference video, it utilizes an emotion encoder I3D~\cite{i3d/CarreiraZ17} to calculate the emotion embedding and proposes a speaker encoder comprising 3 LSTM layers and a linear layer to explore the voice characteristics of different speakers.
Then we train V2C-Net on our pre-training dataset according to the setup outlined in the original paper. The Adam optimizer~\cite{adam/KingmaB14} is employed with hyperparameters set to $\beta_1 = 0.9$, and $\beta_2 = 0.98$. The learning rate schedule followed the approach used in the vanilla transformer~\cite{attentionallyou/VaswaniSPUJGKP17}. It takes 300k steps with a batch size of 48. During inference, the generated mel-spectrogram is converted into speech using the pre-trained vocoder.

\paragraph{HPM.~~\xspace}
According to HPM~\cite{visualvoicecloning/Cong0QZWWJ0H23} official code\footnote{\href{https://github.com/GalaxyCong/HPMDubbing}{https://github.com/GalaxyCong/HPMDubbing}}, we reproduce training process on our pre-training dataset. 
It utilizes an emotion face-alignment network (EmoFAN)~\cite{emofan/ToisoulKBTP21} to capture the valence and arousal information from facial expressions and also utilizes an emotion encoder I3D~\cite{i3d/CarreiraZ17} to calculate the emotion embedding. 
For training, we use Adam~\cite{adam/KingmaB14} with learning rate $10^{-5}$, $\beta_1$ = 0.9, $\beta_2$ = 0.98, $\epsilon = 10^{-9}$ to optimize the HPM. It takes 500k steps with batch size 16. During inference, the generated mel-spectrogram is converted into speech using the pre-trained vocoder.


\paragraph{StyleDubber.~~\xspace}
According to StyleDubber~\cite{styledubber:conf/acl/CongQLBZH00H24} official code\footnote{\href{https://github.com/GalaxyCong/StyleDubber}{https://github.com/GalaxyCong/StyleDubber}}, we reproduce training process on our dataset \datasetname. StyleDubber introduces the cross-attention to enhance the relevance between textual phonemes of the script and the reference audio as well as visual emotion.
For training, we use Adam~\cite{adam/KingmaB14} with learning rate $0.00625$, $\beta_1$ = 0.9, $\beta_2$ = 0.98, $\epsilon = 10^{-9}$ to optimize the model. It takes 300k steps with batch size 64. During inference, the generated mel-spectrogram is converted into speech using the pre-trained vocoder.


\paragraph{Face-TTS.~~\xspace}
We use the official-released pre-trained model\footnote{\href{https://github.com/naver-ai/facetts}{https://github.com/naver-ai/facetts}} of the Face-TTS~\cite{FaceTTS:conf/icassp/LeeCC23}, which is pre-trained on multiple large-scale TTS datasets (such as LRS3~\cite{LRS3/abs-1809-00496}, VoxCeleb2~\cite{VoxCeleb2:conf/interspeech/ChungNZ18}, and LJSpeech~\cite{ljspeech17}, etc.). 
Following its original inference pipeline, the input face image is resized into
224$\times$224 pixels and embeds onto 512-dimensional vector. The output speech is decoded from their released vocoder in 16kHz sampling rate.



\section{Additional Results}
\label{sec:results}
We conduct extra experiments under our acoustic-guided version \methodname$^*$, as shown in Fig.~\ref{fig:mel-speech-guided}, from mel-spectrograms in the second row, the other baselines show severe over-smoothing issues, resulting quality degradation. 
Furthermore, from the F0 curve in the second row, the other baselines exhibit distinct F0 contours showing different pitch, emotion, and intonation with the GT.
Our results are closer to the GT with those acoustic-guided methods. 

\textbf{\textit{For More audio samples please refer to our supplementary material.}}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{Figures/main_mel_SpeechGT.pdf}
    \caption{\textbf{Speech qualitative results} on acoustic-guided version \methodname$^*$. The red rectangles highlight key regions with acoustic differences or over-smoothing issues, and the red dotted circle shows similar F0 contours with ground truth. }
    \label{fig:mel-speech-guided}
\end{figure}

\newpage
\section{User Evaluation}
\label{sec:user}
We conduct the subjective evaluation with 15 participants, to compare our \methodname with SOTA methods.
Specifically, we introduce five mean opinion scores (MOS) with rating scores from 1 to 5 in 0.5 increments, including $\text{MOS}_\text{nat}$, $\text{MOS}_\text{con}$ for speech naturalness (\ie~quality) and consistency (\ie~emotion and speaker similarity). We randomly generate 10 samples from the test set. 
Here, we give definitions of both MOS scores on Tables~\ref{table:mos_smos}, and the user evaluation interface is shown in Fig.~\ref{fig:userstudy}.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{Figures/userstudy.pdf}
    \caption{User evaluation interface.}
    \label{fig:userstudy}
\end{figure}

\begin{table}[thbp]
\centering
\small
\begin{tabular}{>{\centering\arraybackslash}m{1.cm} >{\centering\arraybackslash}m{1.cm} p{6cm} p{6cm}}
\toprule
\textbf{\textit{Level}} & \textbf{\textit{Value}} & \textbf{$\text{MOS}_\text{Nat}$ \textit{ Description}} & \textbf{$\text{MOS}_\text{Con}$ \textit{ Description}} \\ 
\midrule
\textbf{Excellent} & 5 & \textbf{Natural and Clear:} The speech is natural, smooth, and clear with no rhythm or semantic issues. Easy to understand, quality is high. & \textbf{Highly Consistent:} The generated speech closely aligns with the gender, age, nationality, and emotion highly depicted in the image. \\ 
\textbf{Good} & 4 & \textbf{Minor Issues:} Speech is mostly clear, with minor rhythm issues. Quality is acceptable. & \textbf{Fairly Consistent:} The generated speech shows slight differences from the gender, age, or emotion depicted in the image, at least two of them are about consistent. \\ 
\textbf{Fair} & 3 & \textbf{Noticeable Flaws:} Speech is somewhat unnatural, with noticeable errors and noise. Requires effort to understand, quality is average. & \textbf{Moderately Consistent:} The generated speech has some resemblance to the gender, age, and emotion depicted in the image, but noticeable discrepancies exist in either emotion or gender (at least one shows similarity).\\ 
\textbf{Bad} & 2 & \textbf{Hard to Understand:} Speech is disfluent, with abnormal rhythm and unclear words. Hard to understand, the quality is low. & \textbf{Low Consistent:} The generated speech significantly differs from the gender, emotion, and age depicted in the target image, with no consistent attribute. \\ 
\textbf{Poor} & 1 & \textbf{Unintelligible:} Speech is very unclear, disfluent, and nearly incomprehensible. Quality is unacceptable. & \textbf{Barely Consistent:}The generated speech markedly diverges from any attribute depicted in the image. \\ 
\bottomrule
\end{tabular}
\caption{$\text{MOS}_\text{Nat}$ and $\text{MOS}_\text{Con}$ descriptions.}
\label{table:mos_smos}
\end{table}


\section{Social Impact and Limitation}
\label{sec:impact}
\paragraph{Social impact.~~\xspace}
Our method achieves speech generation consistent with identity and emotion, opening up new possibilities in the face-to-speech field. 
Nevertheless, it also introduces several ethical concerns, when using another person’s facial or speech features without explicit authorization.
The ability of our method to replicate voice identity attributes raises fears about generating deepfakes. Such content has the potential to deceive audiences or damage reputations without the approval of the individuals involved.
We emphasize the necessity of clear usage guidelines and consent agreements for using our published models, to ensure responsible application while respecting individual privacy and rights.

\paragraph{Limitation.~~\xspace}
Despite achieving advanced performance, we struggle to precisely reconstruct a person’s true voice due to visual-voice biases within the dataset, tending to produce average-sounding speech. 
