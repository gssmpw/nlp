\section{Related Work}
\label{sec:relatedwork}

\subsection{Residual Vector Quantization for TTS}
Neural audio codecs~\cite{SoundStream:journals/taslp/ZeghidourLOST22, speechtokenizer:conf/iclr/ZhangZLZQ24} enable discrete speech modeling by reconstructing high-quality audio at low bitrates. 
Residual Vector Quantization (RVQ)~\cite{vasuki2006review,Srcodec:conf/icassp/ZhengTXX24} is a standard technique for the codecs that quantizes audio frames by multiple hierarchical layers of quantizers. 
Recent models~\cite{SPEAR-TTS:journals/tacl/KharitonovVBMGP23,nautralspeech2:conf/iclr/ShenJ0LL00Z024} rely on codecs with the RVQ to synthesize speech, and show promising performance on naturalness. 
For example, VALL-E~\cite{valle:journals/corr/abs-2301-02111} employs Encodec~\cite{encodec:journals/tmlr/DefossezCSA23} to transform the speech into a sequence of discrete tokens, then uses an auto-regressive (AR) model to predict tokens. 
However, existing methods are mainly based on the AR manner leading to unstable and inefficient sampling. We propose a discrete diffusion framework \methodname to reconstruct tokens from the RVQ codec, achieving faster and more diverse sampling by parallel iterative refinement. 

\subsection{Face-driven TTS}
Face-driven TTS (F2S) aims to synthesize speech based on visual information about the speaker~\cite{hearingface:conf/asru/PlusterWQW21,hyface:journals/corr/abs-2408-09802,Face2Speech:conf/interspeech/GotoOSTM20,facestylespeech:journals/corr/abs-2311-05844}. Previous F2S methods focus on how to learn visual representation from speech supervision. For example, Goto~\etal~\cite{Face2Speech:conf/interspeech/GotoOSTM20} propose a supervised generalized end-to-end loss to minimize the distance between visual embedding and vocal speaker embedding. 
However, existing methods ignore the rich emotional cues inherent in the face, which often generate over-smoothing speech lacking diverse emotion naturalness. 
Although, Kang~\etal~\cite{facestylespeech:journals/corr/abs-2311-05844} additionally introduce speech prosody codes to enhance the naturalness. They still depend on the speech prompt to achieve natural generation, which do not satisfy the requirements of \taskname. 
In contrast, our \methodname only utilizes visual cues to form the emotional auditory expectations without relying on any vocal features. 

\subsection{Emotional TTS}

Emotional TTS aims to enhance synthesized speech with emotional expressiveness~\cite{mmtts_emo:journals/corr/abs-2404-18398,emodiff:conf/icassp/GuoDCY23,visualvoicecloning/ChenTQZLW22}. Existing methods can be divided into two categories based on how to integrate emotion information into TTS systems. 
For emotion label conditioning, EmoDiff~\cite{emodiff:conf/icassp/GuoDCY23} introduces a diffusion model with soft emotion labels as a classifier guidance. 
In contrast, V2C-Net~\cite{visualvoicecloning/ChenTQZLW22} employs emotion and speaker embeddings from reference face and speech individually for speech customization. 
However, previous methods do not explore how to learn both speaker identity and emotion from the face image. Our \methodname offers a novel perspective of the relationship between auditory expectations and visual cues for TTS without relying on any vocal cues. 