\section{Introduction}
\label{sec:intro}

When we encounter a person's face on platforms like Instagram or Facebook without hearing their voice, our minds instinctively generate auditory expectations based on visual cues. 
These expectations are shaped by our experiences and cultures, influencing how we perceive individuals to sound based on their external appearance, such as age, gender, nationality, or emotion~\cite{paris2017visual,taitelbaum2016}. These preconceived notions drive us to form judgments about othersâ€™ voices even before they speak. 

In recent years, face-guided Text-to-Speech (TTS)~\cite{Face2Speech:conf/interspeech/GotoOSTM20,facestylespeech:journals/corr/abs-2311-05844,facespeak:conf/cvpr/JangKAKYJKKC24}, also known as Face-to-Speech or F2S, has attracted growing interest with diverse applications, such as virtual character dubbing and assistance for individuals with expressive language disorders. The goal of F2S is to create voices that are consistent with the guided face. 
However, users increasingly expect generated speech that not only replicates speakers' identities but also conveys rich emotional expression, enhancing their experience in human-machine interactions. This expectation is beyond the scope of F2S tasks (Fig.~\ref{fig:task}(a)), lacking explicit guidance to produce desired emotional speech. 

Considering that facial expressions are the most direct indicators of emotion, we propose an extension to F2S task grounded in visual cues, termed \textbf{emotional Face-to-Speech (\taskname)}. An example of the proposed \taskname task is illustrated in Fig.~\ref{fig:task}(b). 
Unlike the conventional F2S task, which converts text to speech guided solely by identity embeddings extracted from a reference face, our \taskname task further decouples identity and emotion from the facial input, producing speech that preserves the speaker's identity while enriching it with the emotional expression derived from the reference face. 
While the new \taskname task may initially appear to only require the generated speech to convey emotions, it raises several novel challenges.
On one hand, traditional F2S methods are insufficient for \taskname, as they focus on converting text into speech that reflects facial characteristics without considering emotional states. 
On the other hand, previous expressive TTS methods often focus on generating speech tied to either a specific identity or a specific emotion, but customizing both simultaneously remains a significant challenge---particularly in the absence of speech prompts.

\begin{figure*}
    \centering
    \includegraphics[width=0.93\linewidth]{Figures/task.pdf}
    \caption{
    \textbf{Tasks comparison.} (a) Conventional Face-to-Speech (F2S). (b) The introduced Emotional Face-to-Speech (eF2S). Given text and face prompts, the model is expected to generate speech that aligns with both the facial identity and emotional expression. Our \taskname offers a novel perspective for generating consistent speech without relying on any vocal cues.
    }
    \label{fig:task}
\end{figure*}

To address these challenges, we propose a novel discrete \textbf{D}iffusion framework for \textbf{Emo}tional \textbf{Face}-to-speech, called \textbf{\methodname}, which is the first attempt to generate consistent auditory expectations (\ie~identity and emotions) directly from visual cues. 
Specifically, to mitigate the one-to-many issues inherent in continuous speech features~\cite{foundationTTS:journals/corr/abs-2303-02939}, we begin by discretizing the speech generation process utilizing neural audio codec with Residual Vector Quantization (RVQ). 
Considering the low-to-high frequency distributions across different RVQ  levels, we then introduce a discrete diffusion model with a coarse-to-fine curriculum learning, to enhance both training efficiency and diversity of generated speech. 
Building on this, we propose a multimodal DiT (MM-DiT) for the reverse diffusion process, which dynamically aligns speech and text prompt, and customizes face-style linguistic expressions. 
Furthermore, for multi-conditional generation, we develop enhanced predictor-free guidance (EPFG) on the discrete diffusion model, boosting efficient response to the global condition while facilitating the decoupling of local conditions. 
Comprehensive experiments demonstrate that \methodname achieves diverse and high-quality speech synthesis, outperforming the state-of-the-art models in naturalness and consistency---exceeding even speech-guided methods. 
The contributions of this paper are summarized below:
\begin{itemize}[leftmargin=12pt]
\item We introduce an extension to F2S task, named Emotional Face-to-Speech (\taskname), which is the first attempt to customize a consistent vocal style solely from the face. 
\item We propose a novel discrete diffusion framework for speech generation, which incorporates multimodal DiT and RVQ-based curriculum learning, achieving high-fidelity generation and efficient training. 
\item We devise enhanced predictor-free guidance to boost sampling quality in multi-conditional scenarios of \taskname.
\item Extensive experimental results demonstrate that \methodname can generate more consistent, natural speech with enhanced emotions compared to previous methods.
\end{itemize}
