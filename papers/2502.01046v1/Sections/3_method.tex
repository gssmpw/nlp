
\section{Preliminary: Discrete Diffusion Models}
\label{subsec:discrete_diffusion}

Continuous Diffusion Models (CDM)~\cite{videodiff_latent/BlattmannRLD0FK23,styletts2/LiHRMM23,mmgeneration/RuanMYH0FYJG23} have achieved state-of-the-art results in generative modeling, but face challenges in speech generation due to high-dimensional speech features and excessive diffusion steps, frustrating practical application. 
The fundamental solution lies in compressing the speech feature space, such as a discrete space. 

Recently, Discrete Diffusion Models (DDMs) have shown promise in language modeling~\cite{ConcreteScoreMatch:conf/nips/MengCSE22,SEDD:conf/icml/LouME24} and speech generation~\cite{diffsound:journals/taslp/YangYWWWZY23,DCTTS:conf/icassp/WuLLY24}. We emphasize that DDM has yet to be explored in multi-conditional speech generation with high-quality audio compression. In this paper, to our knowledge, we take the first attempt to generate RVQ-based speech tokens with DDM. 
Below, we outline the forward and reverse processes of the DDM, along with its training objective.

\paragraph{Forward diffusion process.~~\xspace}
Given a sequence of tokens $\bm{x} = x^1 \ldots x^d$ from a state space of length $d$ like $\mathcal{X}^d = \{1, \ldots, n\}^d$. The continuous-time discrete Markov chain at time $t$ is characterized by the diffusion matrix $\mQ_t\in \mathbb{R}^{n^d\times n^d}$ (\ie~transition rate matrix), as follows:
\begin{equation}
\label{eq:delta_transition}
    p({x}_{t+ \Delta t}^i|x_t^i) =  \delta_{{x}^i_{t+ \Delta t}x^i_t} + \mQ_t({x}^i_{t+ \Delta t},x^i_t)\Delta t + o(\Delta t),
\end{equation}
where $x^i_t$ denotes $i$-th element of $\bm{x}_t$, $\mQ_t({x}^i_{t+ \Delta t},x^i_t)$ is the $({x}^i_{t+ \Delta t},x^i_t)$ element of $\mQ_t$, denoting the transition rate from state $x^i_t$ to state ${x}^i_{t+ \Delta t}$ at time $t$, and $\delta$ is Kronecker delta. 
Since the exponential size of $\mQ_t$, existing works~\cite{SEDD:conf/icml/LouME24,RADD:journals/corr/abs-2406-03736} propose to assume dimensional independence, conducting a one-dimensional diffusion process for each dimension with the same token-level diffusion matrix $\mQ_t^\text{tok}=\sigma(t)\mQ^\text{tok}\in \mathbb{R}^{n\times n}$, where $\sigma(t)$ is the noise schedule and $\mQ^\text{tok}$ is designed to diffuse towards an absorbing state \texttt{[MASK]}. Then the forward equation is formulated as $\mP({x}^i_t,x^i_0) = \exp\left(\bar{\sigma}(t) \mQ^\text{tok}({x}^i_t,x^i_0) \right)$, where transition probability matrix $\mP({x}^i_t,x^i_0) := p({x}^i_t|x_0)$, and cumulative noise $\bar{\sigma}(t) = \int_0^t \sigma(s)ds$. There are two probabilities in the $\mP_{t|0}$: \( 1 - e^{-\bar{\sigma}(t)} \) for replacing the current tokens with \texttt{[MASK]}, \( e^{-\bar{\sigma}(t)} \) for keeping it unchanged.

\paragraph{Reverse denoising process.~~\xspace}
As the diffusion matrix $\mQ^\text{tok}_t$ is known, the reverse process can be given by a reverse transition rate matrix $\bar{\mQ}_t$~\cite{SCDDM:conf/iclr/SunYDSD23,kelly2011reversibility}, where $\bar{\mQ}_t(x^i_{t- \Delta t},x^i_t)=\frac{ p(x^i_{t- \Delta t})}{p(x^i_t)} \mQ^\text{tok}_t(x^i_t,x^i_{t- \Delta t})$ and $x^i_{t- \Delta t}\neq x^i_t$, or $\bar{\mQ}_t(x^i_{t- \Delta t}, x^i_t) =  - \sum_{z \neq x_t} \bar{\mQ}_t(z,x^i_t)$. 
The reverse equation is formulated as follows: 
\begin{equation}
    \label{eq:backward}
    p({x}^i_{t- \Delta t}|x^i_t) =  \delta_{{x}^i_{t- \Delta t}x^i_t} + \bar{\mQ}_t({x}^i_{t- \Delta t},x^i_t)\Delta t + o(\Delta t),\\
\end{equation}
where we can estimate the ratio $\frac{p(x^i_{t- \Delta t})}{p(x^i_t)}$ (which is known as the \textit{concrete score}~\cite{SEDD:conf/icml/LouME24,ConcreteScoreMatch:conf/nips/MengCSE22} to measure the \textit{transition probability or closeness} from a state $x^i$ at time $t$  to a state $\hat{x}^i$ at time $t- \Delta t$) of $\bar{\mQ}_t$ by a score network $s_\theta({x}^i_t,t)_{x^i_{t- \Delta t}} \approx [\frac{p(x^i_{t- \Delta t})}{p(x^i_t)}]_{x^i_{t}\neq x^i_{t- \Delta t}}$. 
So that the reverse matrix is parameterized to model the reverse process $q_\theta({x}^i_{t- \Delta t}|x^i_t)$ (\ie~parameterize the concrete score). 

\paragraph{Training objective.~~\xspace}
Denoising score entropy (DSE)~\cite{SEDD:conf/icml/LouME24} is introduced to train the score network $s_\theta$:
\begin{equation}
\begin{aligned}
       \int_0^T \mathbb{E}_{\bm{x}_t \sim p\left(\bm{x}_t \mid \bm{x}_0\right)} \sum_{{\hat{\bm{x}}_t} \neq \bm{x}_t} \mQ_t\left( \hat{{x}}^i_t,{x}^i_t\right)  \Big[s_\theta\left({x}^i_t, t\right)_{\hat{{x}}^i_t}  & \\
        - c_{\hat{{x}}^i_t {x}^i_t} \log s_\theta\left({x}^i_t, t\right)_{\hat{{x}}^i_t}+  \text{N}(c_{\hat{{x}}^i_t {x}^i_t})\Big] dt &,
\end{aligned}
\label{eq:score_entropy}
\end{equation}
where the concrete score $c_{\hat{{x}}^i_t {x}^i_t} = \frac{p\left({\hat{{x}}^i_t} \mid {x}^i_0\right)}{p\left({x}^i_t \mid {x}^i_0\right)}$ and a normalizing constant function $\text{N}(c):= c \log c - c$ that ensures loss non-negative. 
During sampling, we can replace the concrete score with the trained score network on~\cref{eq:backward}.



\section{Methodology}
\label{sec:method}
In this section, we describe our \methodname, the first RVQ-based discrete diffusion for eF2S. 
We present the task  formulation in Sec.~\ref{subsec:task} and an overview of \methodname in Sec.~\ref{subsec:overview}. 

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.97\linewidth]{Figures/main.pdf}
    \caption{\textbf{Overall framework of \methodname}. The MM-DiT inputs masked token $x_t^{r_1:r_{12}}$, time $t$, and condition set $\bm{c}$ to synthesize speech, consisting of $N$ blocks for conditioning and $12$ linear heads to predict concrete scores. 
    During training, we propose a curriculum learning that first inputs low-level tokens and refines them by adding high-level tokens progressively. 
    During sampling, an Euler sampler with our EPFG refines the tokens, while a codec decoder reconstructs the waveform. }
    \label{fig:main}
\end{figure*}

\subsection{Task Formulation for \taskname}
\label{subsec:task}
Given a triplet of multimodal-driven conditions $\bm{c} = \{\bm{c}_\text{id}, \bm{c}_\text{emo}, \bm{c}_\text{text}\}$, which  correspond to reference identity, emotion, and text, respectively, the \taskname task aims to synthesize speech based on the $\bm{c}$. 
More precisely, the synthesized speech content aligns with the text condition  $\bm{c}_\text{text}$, while its voice identity and emotional attributes correspond to  the identity condition $\bm{c}_\text{id}$ and emotion condition $\bm{c}_\text{emo}$, respectively---both extracted from the input face. 



\subsection{Overview of \methodname}
\label{subsec:overview}
Fig.~\ref{fig:main} illustrates the overview of \methodname. The MM-DiT comprises $N$ blocks for conditional information injection and 12 linear heads for concrete score prediction. The masked tokens $x_t^{r_1:r_{12}}$ are obtained via speech tokenization and forward diffusion, with face and text conditioners forming the condition set $\bm{c}$. 
Meanwhile, identity and emotion conditions with time are injected through adaptive layer normalization (AdaLN)~\cite{DiT:conf/iccv/PeeblesX23}, and the text condition is injected with cross-attention. 
During training, we propose a curriculum learning algorithm, which first inputs low-level tokens $\bm{x}_t^{r_1:r_{l-1}}$ and refines them by adding high-level token $\bm{x}_t^{r_{l}}$ progressively. 
During sampling, we utilize an Euler sampler with our EPFG to iteratively refine the generated tokens, while a codec decoder reconstructs the waveform.
Notably, when ground truth speech is provided during training, the reference features $\bm{c}_\text{ge2e}$ are extracted from the GE2E~\cite{GE2E/WanWPL18} to guide identity customization. During inference, we use the cross-modal aligned face encoder to extract the $\bm{c}_\text{id}$ instead of $\bm{c}_\text{ge2e}$. 

Next, we detail the key components in \methodname.

\subsection{Conditional Concrete Score Modeling}
\label{subsec:f2a}
For the concrete score $s_\theta$ modeling, we first define the tokenization and forward diffusion processes, followed by a description of the conditioners and architecture, and conclude with the modulation of the concrete score using EPFG. 

\paragraph{RVQ speech tokenization.~~\xspace}
We utilize the recent RVQ-based codec~\cite{maskgct:journals/corr/abs-2409-00750} as the tokenizer, which achieves hierarchical modeling of diverse information across different RVQ layers. Given a single-channel speech signal, the tokenizer compresses it to the output tokens $\bm{x}^{r_1:r_{12}} = \{1,\ldots,C_\text{code}\}^{12\times d_\text{tok}}$, where $r_i$ is the $i$-th RVQ level of token, $d_\text{tok}$ is the length of the token sequence, respectively. The number of RVQ layers is 12 with a codebook size $C_\text{code}=1,024$ in each layer. 


\paragraph{RVQ token diffusion process.~~\xspace}
Given the hierarchical structure of RVQ tokens, following the previous diffusion process~\cite{SEDD:conf/icml/LouME24}, we randomly corrupt each level token $\bm{x}^{r_i}_t$ at timestep $t$. 
Specifically, we first extract input tokens  $\bm{x}^{r_1:r_l}_0$ from the codec encoder according to the curriculum training stage, where $r_l$ denotes the max level for the current input. We then conduct the diffusion process as defined in~\cref{eq:delta_transition} for $\bm{x}^{r_i}_t$, where $1\leq i\leq l$. 


\paragraph{Conditioners.~~\xspace} 
For face conditioner, as presented in Fig.~\ref{fig:main}, we build identity encoder and emotion encoder to learn identity embedding  $\bm{c}_\text{id}$ and emotion embedding $\bm{c}_\text{emo}$, respectively. 
Specifically, we first employ a composite identity embedding by introducing two face recognition models ArcFace~\cite{arcface:journals/pami/DengGYXKZ22} and FaceNet~\cite{facenet:conf/cvpr/SchroffKP15}, then utilize a multilayer perceptron (MLP) for transformation and shape alignment. 
To precisely model the high-fidelity vocal style associated with the face, we extract the speech speaker embedding $\bm{c}_\text{ge2e}$ from the speaker recognition model GE2E~\cite{GE2E/WanWPL18}, and make $\bm{c}_\text{id}$ aligned with the $\bm{c}_\text{ge2e}$ across modalities using cosine similarity, L1, and L2 losses, as detailed in Sec.~\ref{subsec:training}. 
For the emotion embedding, we employ a strong facial expression recognition model Poster2~\cite{posterv2:journals/corr/abs-2301-12149}. Since the continuous emotion embedding of backbone is insufficient for decoupling identity information~\cite{DBLP:conf/cvpr/LiuM0HFL0C24}, we leverage the predicted label and a learnable embedding layer to learn identity-agnostic embedding $\bm{c}_\text{emo}$. 

For text conditioner, we introduce a text encoder to learn text embedding $\bm{c}_\text{text}$. Specifically, raw text is preprocessed into an International Phonetic Alphabet (IPA) phone sequence using a standard IPA phonemizer. Next, embedding is extracted from a pre-trained text-speech encoder SpeechT5~\cite{speecht5:conf/acl/AoWZ0RW0KLZWQ0W22}, and is then subsequently projected into the hidden state via an MLP. 

\paragraph{Multimodal DiT.~~\xspace} 
We propose the Multimodal DiT (MM-DiT), which differs from DiT~\cite{DiT:conf/iccv/PeeblesX23} in three aspects. 
(1) \emph{Input}, the masked speech tokens $\bm{x}^{r_1:r_{12}}_t$ at timestep $t$ are fed to embedding layers and subsequently averaged to serve as the input. 
(2) \emph{Conditioning}, to customize face-style speech generation, we concatenate $\bm{c}_\text{id}$ and $\bm{c}_\text{emo}$ along with the timestep embedding, is passed through an MLP to inject the global face-style condition. The MLP aims to regress the scale and shift parameters $\alpha_1,\gamma_1,\beta_1,\alpha_2,\gamma_2,\beta_2$ for the AdaLN. Additionally, to learn face-style linguistic expressions, we apply cross-attention with rotary position embeddings~\cite{rotary:journals/ijon/SuALPBL24} enabling dynamic alignment with text $\bm{c}_\text{text}$. 
(3) \emph{Output}, we incorporate 12 linear heads including a combination of AdaLN and linear layer to predict concrete scores for each RVQ level. 


\paragraph{Enhanced predictor-free guidance.~~\xspace} 
% \label{subsec:cfg}
% \paragraph{Enhanced PFG.~~\xspace} 
Several guidance tricks can boost sampling quality for the conditional generation, such as predictor-free guidance (PFG)~\cite{CFG_DDM:journals/corr/abs-2406-01572, CFG/abs-2207-12598}. However, given $K$ conditions $\bm{c} = \{\bm{c}_1,\ldots,\bm{c}_K\}$, these guidance methods are not readily amenable to multi-conditional scenarios~\cite{compositional:conf/eccv/LiuLDTT22}. 
From the perspective of Energy-Based Models (EBMs), we propose an Enhanced PFG (EPFG) enhancing the efficient response to global condition while facilitating the decoupling of local conditions.

Specifically, to simplify the notation, we define $x^i_t,x^i_{t-\Delta t}$ as $x,\hat{x}$. The key of conditional sampling process is to estimate the concrete score $ \hat{s}_\theta\left({x},t,\bm{c}\right)_{\hat{x}} \approx \frac{ p(\hat{x})}{p(x)}$ linked to the transition probability. 
Using Bayes rule, we can obtain a compositional concrete score $\hat{s}_\theta\left(x,t,\bm{c}\right)_{\hat{x}}$ from $p (x_{t- \Delta t} = \hat{x} | x_{t}=x, \bm{c} )$ based on~\cref{eq:backward}: 
\begin{equation}
\label{eq:cond_score}
\hat{s}_\theta\left(x,t,\bm{c}\right)_{\hat{x}} = s_\theta(x,t)_{\hat{x}}\prod\limits_{k=1}^{K}\frac{s_\theta(x,t,\bm{c}_k)_{\hat{x}}}{s_\theta(x,t)_{\hat{x}}}. 
\end{equation}
Instead of sampling from $\hat{s}_\theta\left(x,t,\bm{c}\right)_{\hat{x}}$, we can utilize temperature sampling~\cite{temperature:conf/nips/KingmaD18,temperature:conf/interspeech/MehtaKLBSH23} for more controllable generated outputs by introducing $\hat{s}^{(w)}_\theta\left(x,t,\bm{c}\right)_{\hat{x}}=s_\theta(x,t)_{\hat{x}}\prod_{k=1}^{K}\frac{s^{w_k}_\theta(x,t,\bm{c}_k)_{\hat{x}}}{s^{w_k}_\theta(x,t)_{\hat{x}}}$, where $w_k$ denotes the guidance scale. However, this compositional guidance lacks interactions among local conditions and struggles to guide sampling with a global consistent direction. Inspired by the formulation of the EBM, the score can also be formulated as $\hat{s}_\theta(x)_{\hat{x}}\approx \frac{p_{\theta} ( \hat{x} )} {p_{\theta} ( x )}=\frac{e^{f_{\theta} ( \hat{x} )} / Z} {e^{f_{\theta} ( x )} / Z}=\frac{e^{f_{\theta} ( \hat{x} )}} {e^{f_{\theta} ( x )}}$, where $Z$ is the normalizing constant, and $f_\theta$ is the energy function. 
Hence, we can associate compositional and joint conditions by summing up the energy functions, and finally obtain the modulated score by multiplying both scores:
\begin{equation}
\label{eq:pfg}
    \hat{s}^{(w)}_\theta\left({x},t\right)_{\hat{x}} \!=\!   \underbrace{s_\theta({x},t)_{\hat{x}}\prod\limits_{k=1}^{K} \tfrac{s^{w_i} _\theta({x},t,\bm{c}_k)_{\hat{x}}}{s^{w_i} _\theta({x},t)_{\hat{x}}}}_\text{Compositional} \cdot \underbrace{\tfrac{s^{w_0} _\theta({x},t,\bm{c})_{\hat{x}}}{s^{w_0-1} _\theta({x},t)_{\hat{x}}}}_\text{Joint},
\end{equation}
where $\bm{c}=\{\bm{c}_\text{id},\bm{c}_\text{emo},\bm{c}_\text{text}\}$, $w_0$ controls the scale of guidance strength for the joint injection of all conditions, while $w_i$ for $1\leq k \leq K$ is assigned to each independent attribute. Please refer to Appendix~\ref{sec:pfg} for detailed derivation.
 

\subsection{Curriculum-based Training and Inference}
\label{subsec:training}
\paragraph{Training.~~\xspace}
Curriculum learning aims to progressively train the model from simple to hard tasks, with the key challenge of identifying samples varying in difficulty.
Previous studies show that neural networks prioritize low-frequency information first~\cite{lowfreq_DBLP:conf/icml/RahamanBADLHBC19}. Fig.~\ref{fig:CL}(a) shows different frequency distributions across RVQ levels, with low-level features exhibiting low-frequency patterns. 
Therefore, we reveal curriculum learning for RVQ-based tokens from the frequency perspective. As shown in Fig.~\ref{fig:main}, we gradually introduce higher-level tokens $\bm{x}^{r_l}_t$ every 3 epochs, starting from previous low-level (\ie~low-frequency) tokens $\bm{x}^{r_1:r_{l-1}}_t$ to high-level tokens $\bm{x}^{r_1:r_l}_t$, facilitating effective training. 


Furthermore, the training procedure for our \methodname contains two stages: concrete score prediction and identity feature alignment. In the concrete score prediction, the training objective is the multi-level DSE loss based on~\cref{eq:score_entropy} with the sum across current $r_l$ RVQ levels as $\mathcal{L}_\text{score} = \sum_{i=1}^{l}\mathcal{L}_\text{DSE}(\bm{x}^{r_i},t,\bm{c})$. For conducting multi-conditional PFG in~\cref{eq:pfg}, we randomly set $\varnothing$ with 10\% probability for each condition, and enforce all conditions set to $\varnothing$ for 10\% samples. 
In the feature alignment, we introduce cosine similarity, L1, and L2 losses to align the visual identity vectors with speech speaker vectors. With these compositional losses, the training objective for the face encoder is as $\mathcal{L}_\text{align} = 1-\mathrm{cos}(\bm{c}_\text{id},\bm{c}_\text{ge2e}) + \mathrm{L1}(\bm{c}_\text{id},\bm{c}_\text{ge2e})+\mathrm{L2}(\bm{c}_\text{id},\bm{c}_\text{ge2e})$.
Notably, to avoid information degradation with teacher-student distillation, we directly train the \methodname with ground truth targets $\bm{c}_\text{ge2e}$ and use the aligned face identity embedding $\bm{c}_\text{id}$ during inference phase.

\input{Tables/main_results}

\paragraph{Inference.~~\xspace}
During inference, we introduce a frame-level duration predictor to estimate speech durations, initializing the input length $d_\text{tok}$. Then the reverse process is executed with Euler sampling~\cite{SEDD:conf/icml/LouME24} and EPFG with 96 steps. The details of the duration predictor refer to Appendix~\ref{sec:our_model}.
