%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
% \newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[]{icml2025}
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
% \usepackage[textsize=tiny]{todonotes}
\input{preamble}
\input{import}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Emotional Face-to-Speech}

\begin{document}

\twocolumn[
\icmltitle{Emotional Face-to-Speech}
% \\\shan{Emotional Text-to-Speech with Expressive Face}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Jiaxin Ye}{istbi}
\icmlauthor{Boyuan Cao}{istbi}
\icmlauthor{Hongming Shan}{istbi} 
\end{icmlauthorlist}

\icmlaffiliation{istbi}{Institute of Science and Technology for Brain-Inspired Intelligence, Fudan University, Shanghai, China} 
\icmlcorrespondingauthor{Hongming Shan}{hmshan@fudan.edu.cn}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Generative Model, ICML}
\icmlkeywords{Generative Model}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility. 
%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.
\printAffiliationsAndNotice{}

\input{Sections/0_abstract}
\input{Sections/1_intro}
\input{Sections/2_relatedwork}
\input{Sections/3_method}
\input{Sections/4_experiment}
\input{Sections/5_conclusion}

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

% \bibliography{main}
\bibliographystyle{icml2025}

\begin{thebibliography}{74}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Afouras et~al.(2018)Afouras, Chung, and Zisserman]{LRS3/abs-1809-00496}
Afouras, T., Chung, J.~S., and Zisserman, A.
\newblock {LRS3-TED:} a large-scale dataset for visual speech recognition.
\newblock \emph{CoRR}, abs/1809.00496, 2018.

\bibitem[Ao et~al.(2022)Ao, Wang, Zhou, Wang, Ren, Wu, Liu, Ko, Li, Zhang, Wei, Qian, Li, and Wei]{speecht5:conf/acl/AoWZ0RW0KLZWQ0W22}
Ao, J., Wang, R., Zhou, L., Wang, C., Ren, S., Wu, Y., Liu, S., Ko, T., Li, Q., Zhang, Y., Wei, Z., Qian, Y., Li, J., and Wei, F.
\newblock {SpeechT5}: {Unified}-modal encoder-decoder pre-training for spoken language processing.
\newblock In \emph{{Proc. Annu. Meeting Assoc. Comput. Linguistics}}, pp.\  5723--5738. Association for Computational Linguistics, 2022.

\bibitem[Austin et~al.(2021)Austin, Johnson, Ho, Tarlow, and van~den Berg]{D3PM:conf/nips/AustinJHTB21}
Austin, J., Johnson, D.~D., Ho, J., Tarlow, D., and van~den Berg, R.
\newblock Structured denoising diffusion models in discrete state-spaces.
\newblock In \emph{{Adv. Neural Inform. Process. Syst.}}, pp.\  17981--17993, 2021.

\bibitem[Blattmann et~al.(2023)Blattmann, Rombach, Ling, Dockhorn, Kim, Fidler, and Kreis]{videodiff_latent/BlattmannRLD0FK23}
Blattmann, A., Rombach, R., Ling, H., Dockhorn, T., Kim, S.~W., Fidler, S., and Kreis, K.
\newblock Align your latents: High-resolution video synthesis with latent diffusion models.
\newblock In \emph{{IEEE Conf. Comput. Vis. Pattern Recog.}}, pp.\  22563--22575, 2023.

\bibitem[Carneiro et~al.(2023)Carneiro, Weber, and Wermter]{meldfair:journals/ijon/CarneiroWW23}
Carneiro, H. C.~C., Weber, C., and Wermter, S.
\newblock Whose emotion matters? speaking activity localisation without prior knowledge.
\newblock \emph{Neurocomputing}, 545:\penalty0 126271, 2023.

\bibitem[Carreira \& Zisserman(2017)Carreira and Zisserman]{i3d/CarreiraZ17}
Carreira, J. and Zisserman, A.
\newblock Quo vadis, action recognition? {A} new model and the kinetics dataset.
\newblock In \emph{{IEEE Conf. Comput. Vis. Pattern Recog.}}, pp.\  4724--4733, 2017.

\bibitem[Chen et~al.(2022)Chen, Tan, Qi, Zhou, Li, and Wu]{visualvoicecloning/ChenTQZLW22}
Chen, Q., Tan, M., Qi, Y., Zhou, J., Li, Y., and Wu, Q.
\newblock {V2C:} {Visual} voice cloning.
\newblock In \emph{{IEEE Conf. Comput. Vis. Pattern Recog.}}, pp.\  21210--21219, 2022.

\bibitem[Chung et~al.(2018)Chung, Nagrani, and Zisserman]{VoxCeleb2:conf/interspeech/ChungNZ18}
Chung, J.~S., Nagrani, A., and Zisserman, A.
\newblock {VoxCeleb2}: Deep speaker recognition.
\newblock In Yegnanarayana, B. (ed.), \emph{{Annu. Conf. Int. Speech Commun. Assoc.}}, pp.\  1086--1090, 2018.

\bibitem[Cong et~al.(2023)Cong, Li, Qi, Zha, Wu, Wang, Jiang, Yang, and Huang]{visualvoicecloning/Cong0QZWWJ0H23}
Cong, G., Li, L., Qi, Y., Zha, Z., Wu, Q., Wang, W., Jiang, B., Yang, M., and Huang, Q.
\newblock Learning to dub movies via hierarchical prosody models.
\newblock In \emph{{IEEE Conf. Comput. Vis. Pattern Recog.}}, pp.\  14687--14697, 2023.

\bibitem[Cong et~al.(2024)Cong, Qi, Li, Beheshti, Zhang, van~den Hengel, Yang, Yan, and Huang]{styledubber:conf/acl/CongQLBZH00H24}
Cong, G., Qi, Y., Li, L., Beheshti, A., Zhang, Z., van~den Hengel, A., Yang, M., Yan, C., and Huang, Q.
\newblock {StyleDubber}: Towards multi-scale style learning for movie dubbing.
\newblock In \emph{{Findings Proc. Annu. Meeting Assoc. Comput. Linguistics}}, pp.\  6767--6779, 2024.

\bibitem[D{\'{e}}fossez et~al.(2023)D{\'{e}}fossez, Copet, Synnaeve, and Adi]{encodec:journals/tmlr/DefossezCSA23}
D{\'{e}}fossez, A., Copet, J., Synnaeve, G., and Adi, Y.
\newblock High fidelity neural audio compression.
\newblock \emph{Trans. Mach. Learn. Res.}, 2023, 2023.

\bibitem[Deng et~al.(2022)Deng, Guo, Yang, Xue, Kotsia, and Zafeiriou]{arcface:journals/pami/DengGYXKZ22}
Deng, J., Guo, J., Yang, J., Xue, N., Kotsia, I., and Zafeiriou, S.
\newblock Arcface: Additive angular margin loss for deep face recognition.
\newblock \emph{{IEEE} Trans. Pattern Anal. Mach. Intell.}, 44\penalty0 (10):\penalty0 5962--5979, 2022.

\bibitem[Diatlova \& Shutov(2023)Diatlova and Shutov]{emospeech:conf/ssw/DiatlovaS23}
Diatlova, D. and Shutov, V.
\newblock {EmoSpeech}: {Guiding} {FastSpeech2} towards emotional text to speech.
\newblock In \emph{{ISCA} Speech Synthesis Worksh.}, pp.\  106--112, 2023.

\bibitem[Du et~al.(2023)Du, Guo, Chen, and Yu]{xvector_tts/DuGCY23}
Du, C., Guo, Y., Chen, X., and Yu, K.
\newblock Speaker adaptive text-to-speech with timbre-normalized vector-quantized feature.
\newblock \emph{{{IEEE} {ACM} Trans. Audio Speech Lang. Process.}}, 31:\penalty0 3446--3456, 2023.

\bibitem[Gan et~al.(2023)Gan, Yang, Yue, Sun, and Yang]{EAT:conf/iccv/GanYYSY23}
Gan, Y., Yang, Z., Yue, X., Sun, L., and Yang, Y.
\newblock Efficient emotional adaptation for audio-driven talking-head generation.
\newblock In \emph{{Int. Conf. Comput. Vis.}}, pp.\  22577--22588, 2023.

\bibitem[Geng et~al.(2024)Geng, Han, Jiang, Zhang, Chen, Hauberg, and Li]{EBM:conf/icml/GengHJZCHL24}
Geng, C., Han, T., Jiang, P., Zhang, H., Chen, J., Hauberg, S., and Li, B.
\newblock Improving adversarial energy-based model via diffusion process.
\newblock In \emph{Int. Conf. on Mach. Learn.}, 2024.

\bibitem[Goto et~al.(2020)Goto, Onishi, Saito, Tachibana, and Mori]{Face2Speech:conf/interspeech/GotoOSTM20}
Goto, S., Onishi, K., Saito, Y., Tachibana, K., and Mori, K.
\newblock {Face2Speech}: {Towards} multi-speaker text-to-speech synthesis using an embedding vector predicted from a face image.
\newblock In \emph{{Annu. Conf. Int. Speech Commun. Assoc.}}, pp.\  1321--1325, 2020.

\bibitem[Guo et~al.(2023{\natexlab{a}})Guo, Ma, Jiang, Yuan, Yu, and Luo]{EBM:conf/iccv/GuoMJYYL23}
Guo, Q., Ma, C., Jiang, Y., Yuan, Z., Yu, Y., and Luo, P.
\newblock {EGC:} {Image} generation and classification via a diffusion energy-based model.
\newblock In \emph{Int. Conf. Comput. Vis.}, pp.\  22895--22905, 2023{\natexlab{a}}.

\bibitem[Guo et~al.(2023{\natexlab{b}})Guo, Du, Chen, and Yu]{emodiff:conf/icassp/GuoDCY23}
Guo, Y., Du, C., Chen, X., and Yu, K.
\newblock {Emodiff}: {Intensity} controllable emotional text-to-speech with soft-label guidance.
\newblock In \emph{{IEEE Conf. Acoust. Speech Signal Process.}}, pp.\  1--5, 2023{\natexlab{b}}.

\bibitem[Hayashi et~al.(2017)Hayashi, Tamamori, Kobayashi, Takeda, and Toda]{RMSEf0:conf/asru/HayashiTKTT17}
Hayashi, T., Tamamori, A., Kobayashi, K., Takeda, K., and Toda, T.
\newblock An investigation of multi-speaker training for wavenet vocoder.
\newblock In \emph{{IEEE Autom. Speech Recognit. Understanding Worksh.}}, pp.\  712--718, 2017.

\bibitem[Ho \& Salimans(2021)Ho and Salimans]{CFG/abs-2207-12598}
Ho, J. and Salimans, T.
\newblock Classifier-free diffusion guidance.
\newblock In \emph{{Adv. Neural Inform. Process. Syst. Worksh}}, pp.\  1--14, 2021.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{DDPM/HoJA20}
Ho, J., Jain, A., and Abbeel, P.
\newblock Denoising diffusion probabilistic models.
\newblock In \emph{{Adv. Neural Inform. Process. Syst.}}, 2020.

\bibitem[Ito \& Johnson(2017)Ito and Johnson]{ljspeech17}
Ito, K. and Johnson, L.
\newblock The lj speech dataset.
\newblock \url{https://keithito.com/LJ-Speech-Dataset/}, 2017.

\bibitem[Jang et~al.(2024)Jang, Kim, Ahn, Kwak, Yang, Ju, Kim, Kim, and Chung]{facespeak:conf/cvpr/JangKAKYJKKC24}
Jang, Y., Kim, J., Ahn, J., Kwak, D., Yang, H., Ju, Y., Kim, I., Kim, B., and Chung, J.~S.
\newblock Faces that speak: {Jointly} synthesising talking face and speech from text.
\newblock In \emph{{IEEE Conf. Comput. Vis. Pattern Recog.}}, pp.\  8818--8828, 2024.

\bibitem[Kang et~al.(2023)Kang, Han, and Yang]{facestylespeech:journals/corr/abs-2311-05844}
Kang, M., Han, W., and Yang, E.
\newblock Face-stylespeech: Improved face-to-voice latent mapping for natural zero-shot speech synthesis from a face image.
\newblock \emph{CoRR}, abs/2311.05844, 2023.

\bibitem[Kelly(2011)]{kelly2011reversibility}
Kelly, F.~P.
\newblock \emph{Reversibility and stochastic networks}.
\newblock Cambridge University Press, 2011.

\bibitem[Kharitonov et~al.(2023)Kharitonov, Vincent, Borsos, Marinier, Girgin, Pietquin, Sharifi, Tagliasacchi, and Zeghidour]{SPEAR-TTS:journals/tacl/KharitonovVBMGP23}
Kharitonov, E., Vincent, D., Borsos, Z., Marinier, R., Girgin, S., Pietquin, O., Sharifi, M., Tagliasacchi, M., and Zeghidour, N.
\newblock Speak, read and prompt: {High}-fidelity text-to-speech with minimal supervision.
\newblock \emph{Trans. Assoc. Comput. Linguistics}, 11:\penalty0 1703--1718, 2023.

\bibitem[Kim \& Bengio(2016)Kim and Bengio]{kim2016deep}
Kim, T. and Bengio, Y.
\newblock Deep directed generative models with energy-based probability estimation.
\newblock \emph{CoRR}, abs/1606.03439, 2016.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{adam/KingmaB14}
Kingma, D.~P. and Ba, J.
\newblock Adam: {A} method for stochastic optimization.
\newblock In \emph{{Int. Conf. Learn. Represent.}}, 2015.

\bibitem[Kingma \& Dhariwal(2018)Kingma and Dhariwal]{temperature:conf/nips/KingmaD18}
Kingma, D.~P. and Dhariwal, P.
\newblock {Glow}: {Generative} flow with invertible 1x1 convolutions.
\newblock In \emph{{Adv. Neural Inform. Process. Syst.}}, pp.\  10236--10245, 2018.

\bibitem[Lee et~al.(2023)Lee, Chung, and Chung]{FaceTTS:conf/icassp/LeeCC23}
Lee, J., Chung, J.~S., and Chung, S.
\newblock Imaginary voice: Face-styled diffusion model for text-to-speech.
\newblock In \emph{{IEEE Conf. Acoust. Speech Signal Process.}}, pp.\  1--5, 2023.

\bibitem[Lee et~al.(2024)Lee, Oh, Hwang, and Lee]{hyface:journals/corr/abs-2408-09802}
Lee, J., Oh, Y., Hwang, I., and Lee, K.
\newblock Hear your face: {Face}-based voice conversion with {F0} estimation.
\newblock \emph{CoRR}, abs/2408.09802, 2024.

\bibitem[Li et~al.(2024)Li, Cheng, He, Peng, and Hauptmann]{mmtts_emo:journals/corr/abs-2404-18398}
Li, X., Cheng, Z., He, J., Peng, X., and Hauptmann, A.~G.
\newblock {MM-TTS:} {A} unified framework for multimodal, prompt-induced emotional text-to-speech synthesis.
\newblock \emph{CoRR}, abs/2404.18398, 2024.

\bibitem[Li et~al.(2023)Li, Han, Raghavan, Mischler, and Mesgarani]{styletts2/LiHRMM23}
Li, Y.~A., Han, C., Raghavan, V.~S., Mischler, G., and Mesgarani, N.
\newblock {StyleTTS} 2: {Towards} human-level text-to-speech through style diffusion and adversarial training with large speech language models.
\newblock In \emph{{Adv. Neural Inform. Process. Syst.}}, 2023.

\bibitem[Liu et~al.(2022)Liu, Li, Du, Torralba, and Tenenbaum]{compositional:conf/eccv/LiuLDTT22}
Liu, N., Li, S., Du, Y., Torralba, A., and Tenenbaum, J.~B.
\newblock Compositional visual generation with composable diffusion models.
\newblock In \emph{{Eur. Conf. Comput. Vis.}}, volume 13677, pp.\  423--439, 2022.

\bibitem[Liu et~al.(2024)Liu, Ma, Zhang, Hu, Fan, Lv, Ding, and Cheng]{DBLP:conf/cvpr/LiuM0HFL0C24}
Liu, R., Ma, B., Zhang, W., Hu, Z., Fan, C., Lv, T., Ding, Y., and Cheng, X.
\newblock Towards a simultaneous and granular identity-expression control in personalized face generation.
\newblock In \emph{{IEEE Conf. Comput. Vis. Pattern Recog.}}, pp.\  2114--2123, 2024.

\bibitem[Livingstone \& Russo(2018)Livingstone and Russo]{RAVDESS}
Livingstone, S.~R. and Russo, F.~A.
\newblock The ryerson audio-visual database of emotional speech and song ({RAVDESS}): {A} dynamic, multimodal set of facial and vocal expressions in north american english.
\newblock \emph{PLOS ONE}, 13\penalty0 (5):\penalty0 e0196391, 2018.

\bibitem[Loshchilov \& Hutter(2019)Loshchilov and Hutter]{adamw/LoshchilovH19}
Loshchilov, I. and Hutter, F.
\newblock Decoupled weight decay regularization.
\newblock In \emph{{Int. Conf. Learn. Represent.}}, 2019.

\bibitem[Lou et~al.(2024)Lou, Meng, and Ermon]{SEDD:conf/icml/LouME24}
Lou, A., Meng, C., and Ermon, S.
\newblock Discrete diffusion modeling by estimating the ratios of the data distribution.
\newblock In \emph{{Int. Conf. on Mach. Learn.}}, 2024.

\bibitem[Ma et~al.(2024)Ma, Zheng, Ye, Li, Gao, Zhang, and Chen]{emo2vec:conf/acl/MaZYLGZ024}
Ma, Z., Zheng, Z., Ye, J., Li, J., Gao, Z., Zhang, S., and Chen, X.
\newblock emotion2vec: Self-supervised pre-training for speech emotion representation.
\newblock In \emph{{Findings Proc. Annu. Meeting Assoc. Comput. Linguistics}}, pp.\  15747--15760. Association for Computational Linguistics, 2024.

\bibitem[Mao et~al.(2023)Mao, Xu, Yin, Chang, Nie, and Huang]{posterv2:journals/corr/abs-2301-12149}
Mao, J., Xu, R., Yin, X., Chang, Y., Nie, B., and Huang, A.
\newblock {POSTER} {V2:} {A} simpler and stronger facial expression recognition network.
\newblock \emph{CoRR}, abs/2301.12149, 2023.

\bibitem[Mehta et~al.(2023)Mehta, Kirkland, Lameris, Beskow, Sz{\'{e}}kely, and Henter]{temperature:conf/interspeech/MehtaKLBSH23}
Mehta, S., Kirkland, A., Lameris, H., Beskow, J., Sz{\'{e}}kely, {\'{E}}., and Henter, G.~E.
\newblock Overflow: Putting flows on top of neural transducers for better {TTS}.
\newblock In \emph{{Annu. Conf. Int. Speech Commun. Assoc.}}, pp.\  4279--4283, 2023.

\bibitem[Meng et~al.(2022)Meng, Choi, Song, and Ermon]{ConcreteScoreMatch:conf/nips/MengCSE22}
Meng, C., Choi, K., Song, J., and Ermon, S.
\newblock Concrete score matching: Generalized score matching for discrete data.
\newblock In \emph{{Adv. Neural Inform. Process. Syst.}}, 2022.

\bibitem[Nishimura et~al.(2024)Nishimura, Hirose, Ohi, Nakayama, and Inoue]{rvq_level:journals/corr/abs-2410-04380}
Nishimura, Y., Hirose, T., Ohi, M., Nakayama, H., and Inoue, N.
\newblock {HALL-E:} hierarchical neural codec language model for minute-long zero-shot text-to-speech synthesis.
\newblock \emph{CoRR}, abs/2410.04380, 2024.

\bibitem[Nisonoff et~al.(2024)Nisonoff, Xiong, Allenspach, and Listgarten]{CFG_DDM:journals/corr/abs-2406-01572}
Nisonoff, H., Xiong, J., Allenspach, S., and Listgarten, J.
\newblock Unlocking guidance for discrete state-space diffusion and flow models.
\newblock \emph{CoRR}, abs/2406.01572, 2024.

\bibitem[Ou et~al.(2024)Ou, Nie, Xue, Zhu, Sun, Li, and Li]{RADD:journals/corr/abs-2406-03736}
Ou, J., Nie, S., Xue, K., Zhu, F., Sun, J., Li, Z., and Li, C.
\newblock Your absorbing discrete diffusion secretly models the conditional distributions of clean data.
\newblock \emph{CoRR}, abs/2406.03736, 2024.

\bibitem[Paris et~al.(2017)Paris, Kim, and Davis]{paris2017visual}
Paris, T., Kim, J., and Davis, C.
\newblock Visual form predictions facilitate auditory processing at the n1.
\newblock \emph{Neuroscience}, 343:\penalty0 157--164, 2017.

\bibitem[Peebles \& Xie(2023)Peebles and Xie]{DiT:conf/iccv/PeeblesX23}
Peebles, W. and Xie, S.
\newblock Scalable diffusion models with transformers.
\newblock In \emph{{Int. Conf. Comput. Vis.}}, pp.\  4172--4182, 2023.

\bibitem[Pl{\"{u}}ster et~al.(2021)Pl{\"{u}}ster, Weber, Qu, and Wermter]{hearingface:conf/asru/PlusterWQW21}
Pl{\"{u}}ster, B., Weber, C., Qu, L., and Wermter, S.
\newblock Hearing faces: {Target} speaker text-to-speech synthesis from a face.
\newblock In \emph{{IEEE Autom. Speech Recognit. Understanding Worksh.}}, pp.\  757--764, 2021.

\bibitem[Radford et~al.(2023)Radford, Kim, Xu, Brockman, McLeavey, and Sutskever]{whisper/RadfordKXBMS23}
Radford, A., Kim, J.~W., Xu, T., Brockman, G., McLeavey, C., and Sutskever, I.
\newblock Robust speech recognition via large-scale weak supervision.
\newblock In \emph{{Int. Conf. on Mach. Learn.}}, volume 202, pp.\  28492--28518, 2023.

\bibitem[Rahaman et~al.(2019)Rahaman, Baratin, Arpit, Draxler, Lin, Hamprecht, Bengio, and Courville]{lowfreq_DBLP:conf/icml/RahamanBADLHBC19}
Rahaman, N., Baratin, A., Arpit, D., Draxler, F., Lin, M., Hamprecht, F.~A., Bengio, Y., and Courville, A.~C.
\newblock On the spectral bias of neural networks.
\newblock In \emph{{Int. Conf. on Mach. Learn.}}, volume~97, pp.\  5301--5310, 2019.

\bibitem[Ren et~al.(2021)Ren, Hu, Tan, Qin, Zhao, Zhao, and Liu]{fastspeech2/0006H0QZZL21}
Ren, Y., Hu, C., Tan, X., Qin, T., Zhao, S., Zhao, Z., and Liu, T.
\newblock {FastSpeech} 2: {Fast} and high-quality end-to-end text to speech.
\newblock In \emph{{Int. Conf. Learn. Represent.}}, 2021.

\bibitem[Ruan et~al.(2023)Ruan, Ma, Yang, He, Liu, Fu, Yuan, Jin, and Guo]{mmgeneration/RuanMYH0FYJG23}
Ruan, L., Ma, Y., Yang, H., He, H., Liu, B., Fu, J., Yuan, N.~J., Jin, Q., and Guo, B.
\newblock {MM}-diffusion: Learning multi-modal diffusion models for joint audio and video generation.
\newblock In \emph{{IEEE Conf. Comput. Vis. Pattern Recog.}}, pp.\  10219--10228, 2023.

\bibitem[Schroff et~al.(2015)Schroff, Kalenichenko, and Philbin]{facenet:conf/cvpr/SchroffKP15}
Schroff, F., Kalenichenko, D., and Philbin, J.
\newblock Facenet: {A} unified embedding for face recognition and clustering.
\newblock In \emph{{IEEE Conf. Comput. Vis. Pattern Recog.}}, pp.\  815--823, 2015.

\bibitem[Shen et~al.(2024)Shen, Ju, Tan, Liu, Leng, He, Qin, Zhao, and Bian]{nautralspeech2:conf/iclr/ShenJ0LL00Z024}
Shen, K., Ju, Z., Tan, X., Liu, E., Leng, Y., He, L., Qin, T., Zhao, S., and Bian, J.
\newblock {NaturalSpeech} 2: {Latent} diffusion models are natural and zero-shot speech and singing synthesizers.
\newblock In \emph{{Int. Conf. Learn. Represent.}}, 2024.

\bibitem[Song et~al.(2021)Song, Meng, and Ermon]{DDIM/SongME21}
Song, J., Meng, C., and Ermon, S.
\newblock Denoising diffusion implicit models.
\newblock In \emph{{Int. Conf. Learn. Represent.}}, 2021.

\bibitem[Su et~al.(2024)Su, Ahmed, Lu, Pan, Bo, and Liu]{rotary:journals/ijon/SuALPBL24}
Su, J., Ahmed, M. H.~M., Lu, Y., Pan, S., Bo, W., and Liu, Y.
\newblock Roformer: Enhanced transformer with rotary position embedding.
\newblock \emph{Neurocomputing}, 568:\penalty0 127063, 2024.

\bibitem[Subakan et~al.(2021)Subakan, Ravanelli, Cornell, Bronzi, and Zhong]{sepformer/SubakanRCBZ21}
Subakan, C., Ravanelli, M., Cornell, S., Bronzi, M., and Zhong, J.
\newblock Attention is all you need in speech separation.
\newblock In \emph{{IEEE Conf. Acoust. Speech Signal Process.}}, pp.\  21--25, 2021.

\bibitem[Sun et~al.(2023)Sun, Yu, Dai, Schuurmans, and Dai]{SCDDM:conf/iclr/SunYDSD23}
Sun, H., Yu, L., Dai, B., Schuurmans, D., and Dai, H.
\newblock Score-based continuous-time discrete diffusion models.
\newblock In \emph{{Int. Conf. Learn. Represent.}}, 2023.

\bibitem[Taitelbaum-Swead \& Fostick(2016)Taitelbaum-Swead and Fostick]{taitelbaum2016}
Taitelbaum-Swead, R. and Fostick, L.
\newblock Auditory and visual information in speech perception: {A} developmental perspective.
\newblock \emph{Clinical linguistics \& phonetics}, 30\penalty0 (7):\penalty0 531--545, 2016.

\bibitem[Van~der Maaten \& Hinton(2008)Van~der Maaten and Hinton]{T-SNE}
Van~der Maaten, L. and Hinton, G.
\newblock Visualizing data using t-{SNE}.
\newblock \emph{J. Mach. Learn. Res.}, 9\penalty0 (11), 2008.

\bibitem[Vasuki \& Vanathi(2006)Vasuki and Vanathi]{vasuki2006review}
Vasuki, A. and Vanathi, P.
\newblock A review of vector quantization techniques.
\newblock \emph{{IEEE Potentials}}, 25\penalty0 (4):\penalty0 39--47, 2006.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{attentionallyou/VaswaniSPUJGKP17}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N., Kaiser, L., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{{Adv. Neural Inform. Process. Syst.}}, pp.\  5998--6008, 2017.

\bibitem[Wan et~al.(2018)Wan, Wang, Papir, and L{\'{o}}pez{-}Moreno]{GE2E/WanWPL18}
Wan, L., Wang, Q., Papir, A., and L{\'{o}}pez{-}Moreno, I.
\newblock Generalized end-to-end loss for speaker verification.
\newblock In \emph{{IEEE Conf. Acoust. Speech Signal Process.}}, pp.\  4879--4883, 2018.

\bibitem[Wang et~al.(2023)Wang, Chen, Wu, Zhang, Zhou, Liu, Chen, Liu, Wang, Li, He, Zhao, and Wei]{valle:journals/corr/abs-2301-02111}
Wang, C., Chen, S., Wu, Y., Zhang, Z., Zhou, L., Liu, S., Chen, Z., Liu, Y., Wang, H., Li, J., He, L., Zhao, S., and Wei, F.
\newblock Neural codec language models are zero-shot text to speech synthesizers.
\newblock \emph{CoRR}, abs/2301.02111, 2023.

\bibitem[Wang et~al.(2020)Wang, Wu, Song, Yang, Wu, Qian, He, Qiao, and Loy]{MEAD:conf/eccv/WangWSYWQHQL20}
Wang, K., Wu, Q., Song, L., Yang, Z., Wu, W., Qian, C., He, R., Qiao, Y., and Loy, C.~C.
\newblock {MEAD:} {A} large-scale audio-visual dataset for emotional talking-face generation.
\newblock In \emph{{Eur. Conf. Comput. Vis.}}, volume 12366 of \emph{Lecture Notes in Computer Science}, pp.\  700--717, 2020.

\bibitem[Wang et~al.(2018)Wang, Stanton, Zhang, Skerry{-}Ryan, Battenberg, Shor, Xiao, Jia, Ren, and Saurous]{WER/WangSZRBSXJRS18}
Wang, Y., Stanton, D., Zhang, Y., Skerry{-}Ryan, R.~J., Battenberg, E., Shor, J., Xiao, Y., Jia, Y., Ren, F., and Saurous, R.~A.
\newblock Style tokens: {Unsupervised} style modeling, control and transfer in end-to-end speech synthesis.
\newblock In \emph{{Int. Conf. on Mach. Learn.}}, volume~80, pp.\  5167--5176, 2018.

\bibitem[Wang et~al.(2024)Wang, Zhan, Liu, Zeng, Guo, Zheng, Zhang, Zhang, and Wu]{maskgct:journals/corr/abs-2409-00750}
Wang, Y., Zhan, H., Liu, L., Zeng, R., Guo, H., Zheng, J., Zhang, Q., Zhang, S., and Wu, Z.
\newblock {MaskGCT}: Zero-shot text-to-speech with masked generative codec transformer.
\newblock \emph{CoRR}, abs/2409.00750, 2024.

\bibitem[Wu et~al.(2024)Wu, Li, Liu, and Yang]{DCTTS:conf/icassp/WuLLY24}
Wu, Z., Li, Q., Liu, S., and Yang, Q.
\newblock {DCTTS:} discrete diffusion model with contrastive learning for text-to-speech generation.
\newblock In \emph{{IEEE Conf. Acoust. Speech Signal Process.}}, pp.\  11336--11340. {IEEE}, 2024.

\bibitem[Xue et~al.(2023)Xue, Liu, He, Tan, Liu, Lin, and Zhao]{foundationTTS:journals/corr/abs-2303-02939}
Xue, R., Liu, Y., He, L., Tan, X., Liu, L., Lin, E., and Zhao, S.
\newblock Foundationtts: Text-to-speech for {ASR} customization with generative language model.
\newblock \emph{CoRR}, abs/2303.02939, 2023.

\bibitem[Yang et~al.(2023)Yang, Yu, Wang, Wang, Weng, Zou, and Yu]{diffsound:journals/taslp/YangYWWWZY23}
Yang, D., Yu, J., Wang, H., Wang, W., Weng, C., Zou, Y., and Yu, D.
\newblock Diffsound: Discrete diffusion model for text-to-sound generation.
\newblock \emph{{IEEE} {ACM} Trans. Audio Speech Lang. Process.}, 31:\penalty0 1720--1733, 2023.

\bibitem[Zeghidour et~al.(2022)Zeghidour, Luebs, Omran, Skoglund, and Tagliasacchi]{SoundStream:journals/taslp/ZeghidourLOST22}
Zeghidour, N., Luebs, A., Omran, A., Skoglund, J., and Tagliasacchi, M.
\newblock {SoundStream}: {An} end-to-end neural audio codec.
\newblock \emph{{IEEE} {ACM} Trans. Audio Speech Lang. Process.}, 30:\penalty0 495--507, 2022.

\bibitem[Zhang et~al.(2024)Zhang, Zhang, Li, Zhou, and Qiu]{speechtokenizer:conf/iclr/ZhangZLZQ24}
Zhang, X., Zhang, D., Li, S., Zhou, Y., and Qiu, X.
\newblock {SpeechTokenizer}: {Unified} speech tokenizer for speech language models.
\newblock In \emph{{Int. Conf. Pattern Recog.}}, 2024.

\bibitem[Zheng et~al.(2024)Zheng, Tu, Xiao, and Xu]{Srcodec:conf/icassp/ZhengTXX24}
Zheng, Y., Tu, W., Xiao, L., and Xu, X.
\newblock Srcodec: {Split}-residual vector quantization for neural speech codec.
\newblock In \emph{{IEEE Conf. Acoust. Speech Signal Process.}}, pp.\  451--455, 2024.

\end{thebibliography}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\input{Sections/X_suppl}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
