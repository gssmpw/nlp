\documentclass{article}
\usepackage{amsmath, amssymb}
\usepackage{amsthm}

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}

\begin{document}


\begin{proposition}
Consider the following sequence of operations:
\begin{align*}
    & \quad \tilde{x} = \mathrm{RMSNorm}(x), \\
    & \quad a = \mathrm{ReLU}(\tilde{x} W^{(1)} + b^{(1)})W^{(2)} + b^{(2)}, \\
    & \quad o = x + a.
\end{align*}
Then,
\begin{equation}
    \frac{\partial \mathcal{L}(o)}{\partial W_{i,j}^{(2)}} = h_{i} (\hat{p}_{j} - y_{j}),  
\end{equation}
where $h := \mathrm{ReLU}\left(x W^{(1)} + b^{(1)}\right)$, $\hat{p} := \mathrm{softmax}(o)$, and $y$ is the label (one-hot vector).
\end{proposition}

\begin{proof}
By the chain rule,
\begin{equation}
\frac{\partial \mathcal{L}(o)}{\partial W_{i,j}^{(2)}} = 
    \underbrace{\frac{\partial \mathcal{L}(o)}{\partial o}}_{\text{(a)}: 1 \times D} \times 
    \underbrace{\frac{\partial o}{\partial a}}_{\text{(b)}: D \times D} \times 
    \underbrace{\frac{\partial a}{\partial W_{i,j}^{(2)}}}_{\text{(c)}: D \times 1}.    
\end{equation}

(a) It is known that 
\begin{equation}
    \frac{\partial \mathcal{L}(o)}{\partial o_{k}} = \hat{p}_{k} - y_{k}.
\end{equation}
So,
\begin{equation}
\frac{\partial \mathcal{L}(o)}{\partial o} =   
\begin{bmatrix}
    \hat{p}_{1} - y_{1} & \hat{p}_{2} - y_{2} & \cdots & \hat{p}_{D} - y_{D}
    \end{bmatrix}.
\end{equation}

(b) Since $o = x + a$,
\begin{equation}
    \frac{\partial o}{\partial a} = I.
\end{equation}

(c) Recall that 
\begin{equation}
    a := \mathrm{ReLU}(\tilde{x} W^{(1)} + b^{(1)}) W^{(2)} + b^{(2)}.
\end{equation}

For convenience, let
\begin{equation}
    h := \mathrm{ReLU}(\tilde{x} W^{(1)} + b^{(1)}).
\end{equation}

Then, we have
\begin{equation}
    \frac{\partial a_{k}}{\partial W_{i,j}^{(2)}} = 
    \frac{\partial}{\partial W_{i,j}^{(2)}} 
    \left( \sum_{p=1}^{H} h_{p} W_{p,k}^{(2)} + b_{k}^{(2)} \right) = h_{i} \, \delta_{k,j}.
\end{equation}

In vector representation,
\begin{equation}
    \frac{\partial a}{\partial W_{i,j}^{(2)}} =
    \begin{bmatrix}
    0 & \cdots & h_{i} & \cdots & 0
    \end{bmatrix}^\top,
\end{equation}
where the only nonzero entry is in the $j$-th component.

Thus, by putting these all together,
\begin{equation}
    \frac{\partial \mathcal{L}(o)}{\partial W_{i,j}^{(2)}} = h_{i} (\hat{p}_{j} - y_{j}).    
\end{equation}
\end{proof}



\begin{proposition}
Consider the following sequence of operations:
\begin{align*}
    & \quad \tilde{x} = \mathrm{RMSNorm}(x), \\
    & \quad a = \mathrm{ReLU}(\tilde{x} W^{(1)} + b^{(1)})W^{(2)} + b^{(2)}, \\
    & \quad \tilde{a} = \mathrm{RMSNorm}(a), \\
    & \quad o = x + \tilde{a}.
\end{align*}
Then,
\begin{equation}
    \left\lVert \frac{\partial \mathcal{L}(o)}{\partial W_{i,j}^{(2)}} \right\rVert 
\leq  
\frac{4\gamma \sqrt{D} \| h \|}{\|a\|}, 
\end{equation}
where $\gamma$ is the scaling parameter used in $\mathrm{RMSNorm}(\cdot)$, $D$ is the dimensionality, and $h := \mathrm{ReLU}\left(x W^{(1)} + b^{(1)}\right)$.
\end{proposition}

\begin{proof}
By the chain rule,
\begin{equation}
\frac{\partial \mathcal{L}(o)}{\partial W_{i,j}^{(2)}} = 
    \underbrace{\frac{\partial \mathcal{L}(o)}{\partial o}}_{\text{(a)}: 1 \times D} \times 
    \underbrace{\frac{\partial o}{\partial \tilde{a}}}_{\text{(b)}: D \times D} \times 
    \underbrace{\frac{\partial \tilde{a}}{\partial a}}_{\text{(c)}: D \times D} \times 
    \underbrace{\frac{\partial a}{\partial W_{i,j}^{(2)}}}_{\text{(d)}: D \times 1}.    
\end{equation}


(a) We have
\begin{equation}
    \left\| \frac{\partial \mathcal{L}(o)}{\partial o} \right\| = \| \hat{p} - y \| \leq \| \hat{p} \| + \| y \| = 2.    
\end{equation}

(b) We also have
\begin{equation}
    \left\| \frac{\partial o}{\partial \tilde{a}} \right\| = \| I \| = 1.
\end{equation}


(c) Recall that
\begin{equation}
    \tilde{a} := \mathrm{RMSNorm}(a) = \gamma \cdot \frac{a}{\sqrt{\frac{1}{D} \sum_{k=1}^D a_{k}^2 + \epsilon}}.    
\end{equation}

Then, $\frac{\partial \tilde{a}}{\partial a}$ is the Jacobian matrix $J$ of $\mathrm{RMSNorm}(\cdot)$. For brevity, let
\begin{equation}
    \alpha := \frac{1}{D} \sum_{k=1}^D (a_{k})^2.
v\end{equation}

Then,
\begin{align}
J_{p,q} = \frac{\partial \tilde{a}_{p}}{\partial a_{q}} 
&= \gamma \cdot \frac{\partial}{\partial a_{q}} \left( \frac{a_{p}}{\sqrt{\alpha + \epsilon}} \right) \\
&= \gamma \cdot \frac{1}{\sqrt{\alpha + \epsilon}} \frac{\partial a_{p}}{\partial a_{q}} 
+ \gamma \cdot a_{p} \frac{\partial}{\partial a_{q}} \left( \frac{1}{\sqrt{\alpha + \epsilon}} \right) \\
&= \gamma \cdot \frac{1}{\sqrt{\alpha + \epsilon}} \delta_{p,q} 
- \gamma \cdot \frac{a_{p} a_{q}}{D (\alpha + \epsilon)^{3/2}}.
\end{align}

In matrix representation,
\begin{equation}
    J = \underbrace{\frac{\gamma}{\sqrt{\alpha + \varepsilon}} I}_{A} 
    - \underbrace{\frac{\gamma}{D(\alpha + \varepsilon)^{3/2}} \left(a\right)^\top \left(a\right)}_{B}.
\end{equation}

Then, we have
\begin{equation}
    \|A\| = \left\|\frac{\gamma}{\sqrt{\alpha + \varepsilon}} I \right\| 
    = \frac{\gamma}{\sqrt{\alpha + \varepsilon}} \|I\| 
    = \frac{\gamma}{\sqrt{\alpha + \varepsilon}},
\end{equation}
and
\begin{equation}
    \|B\| = \left\|\frac{\gamma}{D(\alpha + \varepsilon)^{3/2}} \left(a\right)^\top \left(a\right)\right\| 
    = \frac{\gamma}{D(\alpha + \varepsilon)^{3/2}} \times D\alpha 
    = \frac{\gamma \alpha}{(\alpha + \varepsilon)^{3/2}}.
\end{equation}

So, we have
\begin{equation}
    \| J \| = \| A - B \| \leq \| A \| + \| B \| \leq \frac{2\gamma}{\sqrt{\alpha}} = \frac{2\gamma \sqrt{D}}{\|a\|}.
\end{equation}

(d) Since
\begin{equation}
    \frac{\partial a}{\partial W_{i,j}^{(2)}} =
    \begin{bmatrix}
    0 & \cdots & h_{i} & \cdots & 0
    \end{bmatrix}^\top,
\end{equation}
we have
\begin{equation}
    \left\| \frac{\partial a}{\partial W_{i,j}^{(2)}} \right\| \leq \| h \|.
\end{equation}

Thus,
\begin{equation}
    \left\| \frac{\partial \mathcal{L}(o)}{\partial W_{i,j}^{(2)}} \right\| \leq 2 \times 1 \times \frac{2\gamma \sqrt{D}}{\|a\|} \times \| h \| = \frac{4\gamma \sqrt{D} \| h \|}{\|a\|}.
\end{equation}

\end{proof}


\begin{proposition}
Consider the following sequence of operations:
\begin{align*}
    & \quad a = \mathrm{ReLU}(x W^{(1)} + b^{(1)})W^{(2)} + b^{(2)}, \\
    & \quad o = x + a, \\
    & \quad \tilde{o} = \mathrm{RMSNorm}(o). \\
\end{align*}
Then,
\begin{equation}
    \left\lVert \frac{\partial \mathcal{L}(\tilde{o})}{\partial W_{i,j}^{(2)}} \right\rVert 
\leq  
\frac{4\gamma \sqrt{D} \| h \|}{\|x + a\|}, 
\end{equation}
where $\gamma$ is the scaling parameter used in $\mathrm{RMSNorm}(\cdot)$, $D$ is the dimensionality, and $h := \mathrm{ReLU}\left(x W^{(1)} + b^{(1)}\right)$.
\end{proposition}

\begin{proof}
The proof is analogous to the proof of the previous proposition.
\end{proof}



\end{document}
