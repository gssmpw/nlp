% \section{Comparison of Normalization Strategies} \label{sec:ln_in_transformer}
\section{Comparative Analysis} \label{sec:ln_in_transformer}

In this section, we discuss how different placements of layer normalization (LN \footnote{Unless stated otherwise, LN refers to both LayerNorm and RMSNorm.}) in Transformer architecture affect both training stability and the statistics of hidden states (activations \footnote{We use ``hidden state'' and ``activation'' interchangeably.}).

\subsection{Post- \& Pre-Normalization in Transformers}
\label{subsec:post_pre_ln}
\paragraph{Post-LN.}
The Post-Layer Normalization (Post-LN) \citep{attentionisallyouneed} scheme, normalization is applied \emph{after} summing the module’s output and residual input:
\begin{equation}
    y_{l} = \mathrm{Norm}\bigl(x_l + \mathrm{Module}(x_l)\bigr),
    \label{eq:post_ln}
\end{equation}
where $x_l$ is the input hidden state of $l$-th layer, $y_{l}$ is the output hidden state of $l$-th layer, and $\mathrm{Module}$ denotes Attention or Multi-Layer Perceptron (MLP) module in the Transformer sub-layer. $\mathrm{Norm}$ denotes normalization layers such as RMSNorm or LayerNorm. It is known that by stabilizing the activation variance at a constant scale, Post-LN prevents activations from growing. However, several evidence~\citep{onlayer, transformersgetstable} suggest that Post-LN can degrade gradient flow in deeper networks, leading to vanishing gradients and slower convergence.


\paragraph{Pre-LN.}
The Pre-Layer Normalization (Pre-LN)~\citep{llama3} scheme, normalization is applied to the module's input \emph{before} processing:
\begin{equation}
    y_l = x_l + \mathrm{Module}\bigl(\mathrm{Norm}(x_l)\bigr).
    \label{eq:pre_ln}
\end{equation}
As for Llama $3$ architecture, a final LN is applied to the network output. Pre-LN improves gradient flow during backpropagation, stabilizing early training \citep{onlayer}. Nonetheless, in large-scale Transformers, even Pre-LN architectures are not immune to instability during training~\citep{smallproxies, attentioncollapse}. As shown in Figure~\ref{fig:LN Placement}, unlike Post-LN—which places LN at position $C$—Pre-LN, which places LN only at position $A$, can lead to a “highway” structure that is continuously maintained throughout the entire model if the module produces an output with a large magnitude. This phenomenon might be related to the ``massive activations'' observed in trained models \citep{massiveactivation, mlpswiglu}. 

\begin{figure}[t]
% \vskip -0.1in
    \centering
    \begin{minipage}[t]{0.45\linewidth}
        \vspace{0pt}
        \centering
        \includegraphics[width=.75\linewidth]{Figures/method_abc_ver3.png}
    \end{minipage}
    \centering
    \begin{minipage}[t]{0.45\linewidth}
        \vspace{32pt}
        \centering
        \small
        \begin{tabular}{lccc}
            \toprule
            ~ & A & B & C  \\ 
            \midrule
            Post-LN & \texttimes & \texttimes & \checkmark \\
            Pre-LN  & \checkmark & \texttimes & \texttimes \\
            Peri-LN & \checkmark & \checkmark & \texttimes \\
            \bottomrule
        \end{tabular}
    \end{minipage}
    \caption{Placement of normalization in Transformer sub-layer. }
    \label{fig:LN Placement}
    \vskip -0.1in
\end{figure}

\begin{table}
\caption{Intuitive comparison of normalization strategies.}
\label{tab:variance_summary}
\small
\begin{tabular}{lcc}
\toprule
\textbf{Strategy} & \textbf{Variance Growth} & \textbf{Gradient Stability} \\
\midrule
\textbf{Post-LN} & Mostly constant & Potential for vanishing \\
\textbf{Pre-LN} & Exponential in depth & Potential for explosion \\
\textbf{Peri-LN} & $ \approx \text{Linear}$ in depth & Self-regularization \\
\bottomrule
\end{tabular}
\vskip -0.1in
\end{table} 



\subsection{Variance Behavior from Initialization to Training}
\label{subsec:variance_growth}


As discussed by \citet{onlayer} and \citet{transformersgetstable}, Transformer models at \emph{initialization} exhibit near-constant hidden-state variance under Post-LN and linearly increasing variance under Pre-LN. Most of the previous studies have concentrated on this early-stage behavior. However, Recent studies have also reported large output magnitudes in both the pre-trained attention and MLP modules \citep{vit22b, smallproxies, mlpswiglu}. To bridge the gap from initialization to the fully trained stage, we extend our empirical observations in Figure~\ref{fig:3iter} beyond initial conditions by tracking how these variance trends evolve at intermediate points in training. 

We find that Post-LN maintains a roughly constant variance, which helps avert exploding activations. Yet as models grow deeper and training proceeds, consistently normalizing $x_l + \mathrm{Module}(x_l)$ can weaken gradient flow, occasionally causing partial vanishing gradients and slower convergence. In contrast, Pre-LN normalizes $x_l$ before the module but leaves the module output unnormalized, allowing hidden-state variance to accumulate exponentially once parameter updates amplify the input. Although Pre-LN preserves gradients more effectively in earlier stages, this exponential growth in variance can lead to “massive activations” \citep{massiveactivation}, risking numeric overflow and destabilizing large-scale training. We reconfirm this in Section~\ref{sec:experiments}.

\paragraph{Takeaways.}
\begin{itemize}
\item \textit{Keeping the Highway Clean: Post-LN’s Potential for Gradient Vanishing and Slow Convergence.} When layer normalization is placed directly on the main path (Placement $C$ in Figure \ref{fig:LN Placement}), it can cause gradient vanishing and introduce fluctuations in the gradient scale, potentially leading to instability. 

\item \textit{Maintaining a Stable Highway: Pre-LN May Not Suffice for Training Stability.} Pre-LN does not normalize the main path of the hidden states, thereby avoiding the issues that Post-LN encounters. Nevertheless, a structural characteristic of Pre-LN is that any large values arising in the attention or MLP modules persist through the residual identity path. In particular, as shown in Figure~\ref{fig:3iter}, the exponentially growing magnitude and variance of the hidden states in the forward path may lead to numerical instability and imbalance during training.
\end{itemize}

Recent open-sourced Transformer architectures have adopted normalization layers in unconventional placements. Models like Gemma$2$ and OLMo$2$ utilize normalization layers at the module output (Output-LN), but the benefits of these techniques remain unclear \citep{gemma2, olmo2}. To investigate the impact of adding an Output-LN, we explore the peri-layer normalization architecture.


\subsection{Placing Module Output Normalization}
\label{subsec:peri_ln}

\paragraph{Peri-LN.}
The Peri-Layer Normalization (Peri-LN) applies LN twice within each layer---before and after the module---and further normalizes the input and final output embeddings. Formally, for the hidden state $x_l$ at layer $l$:
\begin{enumerate}
    \item \textit{Initial Embedding Normalization:}
    \[
      y_o = \mathrm{Norm}(x_o),
    \]
    \item \textit{Input- \& Output-Normalization per Layer:}
    \[
      y_l = x_l + \mathrm{Norm}\Bigl(\mathrm{Module}\bigl(\mathrm{Norm}(x_l)\bigr)\Bigr),
    \]
    \item \textit{Final Embedding Normalization:}
    \[
      y_L = \mathrm{Norm}(x_L),
    \]
\end{enumerate}
where $x_o$ denotes the output of the embedding layer, the hidden input state. $y_0$ represents the normalized input hidden state. $x_L$ denotes the hidden state output by the final layer \(L\) of the Transformer sub-layer. This design unifies pre- and output-normalization to regulate variance from both ends. For clarity, the locations of normalization layers in the Post-, Pre-, and Peri-LN architectures are illustrated in Figure~\ref{fig:LN Placement}.


\paragraph{Controlling Variance \& Preserving Gradients.}
% \paragraph{Roll of Output Layer Normalization.}
By normalizing both the input and output of each sub-layer, Peri-LN constrains the \emph{residual spikes} common in Pre-LN, while retaining a stronger gradient pathway than Post-LN. Concretely, if $\mathrm{Norm}(\mathrm{Module}(\mathrm{Norm}(x_l)))$ has near-constant variance $\beta_0$, then
\[
  \mathrm{Var}(x_{l+1}) \;\approx\; \mathrm{Var}(x_l) + \beta_0,
\]
leading to \emph{linear or sub-exponential} hidden state growth rather than exponential blow-up.  We empirically verify this effect in Section~\ref{subsec:growth of hidden state}. 



\begin{figure*}[t]
\vskip -0.1in
    \centering
    \subfigure[Learning rate exploration]
    {
    \includegraphics[width=.3\linewidth]{Figures/pretrain_lrsweep.png}
    \label{fig:pretrain_lrwseep}
    }
    \subfigure[Training loss]
    {
    \includegraphics[width=.295\linewidth]{Figures/hcx_text_400M_dclm_000_30B_warmup10_lr5e4.csv_best_loss_trainingloss_per_tokens.png}
    \label{fig:pretrain_loss}
    }
    \subfigure[Gradient-norm]
    {
    \includegraphics[width=.288\linewidth]{Figures/hcx_text_400M_dclm_000_30B_warmup10_lr5e4.csv_best_loss_warmup10_gradnorm_per_tokens.png}
    \label{fig:pretrain_gradnorm}
    }
    \caption{
    Performance comparison of Post-LN, Pre-LN, and Peri-LN Transformers during pre-training. Figure \ref{fig:pretrain_lrwseep} llustrates the pre-training loss across learning rates. Pre-training loss and gradient norm of best performing $400$M size Transformers are in Figure \ref{fig:pretrain_loss} and \ref{fig:pretrain_gradnorm}. Consistent trends were observed across models of different sizes.
    }
    \label{fig:pretraining}
\vskip -0.1in
\end{figure*}

\begin{figure*}[t]
    \centering
    \subfigure[Training loss]
    {
    \includegraphics[width=.3\linewidth]{Figures/fix_gamma_loss_400M.png}
    \label{fig:fix_gamma_loss}
    }
    \subfigure[Loss in the final $5$B token interval]
    {
    \includegraphics[width=.3\linewidth]{Figures/fix_gamma_zoom_loss_400M.png}
    \label{fig:fix_gamma_loss_zoom}
    }
    \subfigure[Gradient-norm]
    {
    \includegraphics[width=.3\linewidth]{Figures/fix_gamma_gradnorm_400M.png}
    \label{fig:fix_gamma_gradnorm}
    }
    \caption{
    Freezing learnable parameter $\gamma$ of output normalization layer in Peri-LN. we set $\gamma$ to its initial value of $1$ and keep it fixed.
    }
    \label{fig:frozen_gamma}
\vskip -0.1in
\end{figure*}

\paragraph{Open-Sourced Peri-LN Models: Gemma$2$ \& OLMo$2$.}
Both Gemma$2$ and OLMo$2$, which apply output layer normalization, employ the same peri-normalization strategy within each Transformer layer. However, neither model rigorously examines how this placement constrains variance or mitigates large residual activations. Our work extends Gemma$2$ and OLMo$2$ by offering both theoretical and empirical perspectives within the Peri-LN scheme. Further discussion of the OLMo$2$ is provided in Appendix~\ref{appendix:olmo2}.

\subsection{Stability Analysis in Normalization Strategies}
\label{subsec:theory_insights}
We analyze training stability in terms of the magnitude of activation. To this end, we examine the gradient norm with respect to the weight of the final layer in the presence of massive activation. For the formal statements and detailed proofs, refer to Appendix~\ref{appendix:theory_proof}.

\begin{proposition}[Informal]
\label{prop:theory}
Let $\mathcal{L}(\cdot)$ be the loss function, and let $W^{(2)}$ denote the weight of the last layer of $\mathrm{MLP}(\cdot)$. Let $\gamma$ be the scaling parameter in $\mathrm{Norm}(\cdot)$, and let $D$ be the dimension. Then, the gradient norm for each normalization strategy behaves as follows.

\medskip
\noindent 
\textbf{(1) Pre-LN (exploding gradient).} Consider the following sequence of operations:
\begin{equation}
\tilde{x} = \mathrm{Norm}(x), a = \mathrm{MLP}(\tilde{x}), o = x + a,
\end{equation}
then
\begin{equation}
\left\lVert \frac{\partial \mathcal{L}(o)}{\partial W_{i,j}^{(2)}} \right\rVert \;\propto\; \| h_{i} \|,
\end{equation}
where $h := \mathrm{ReLU}\left(\tilde{x} W^{(1)} + b^{(1)}\right)$. In this case, when a massive activation $\|h\|$ occurs, an exploding gradient $\|\partial \mathcal{L} / \partial W^{(2)}\|$ can arise, leading to training instability.

\medskip
\noindent
\textbf{(2) Peri-LN (self-regularizing gradient).} Consider the following sequence of operations:
\begin{equation}
\tilde{x} = \mathrm{Norm}(x), a = \mathrm{MLP}(\tilde{x}), \tilde{a} = \mathrm{Norm}(a), o = x + \tilde{a},
\end{equation}
then
\begin{equation}
\left\lVert \frac{\partial \mathcal{L}(o)}{\partial W_{i,j}^{(2)}} \right\rVert 
\;\le\; \frac{4\,\gamma\,\sqrt{D}\,\|h\|}{\|a\|}, 
\end{equation}
where $h := \mathrm{ReLU}\left(\tilde{x} W^{(1)} + b^{(1)}\right)$. In this case, even when a massive activation $\|h\|$ occurs, $\mathrm{Norm}(\cdot)$ introduces a damping factor $\|a\|$, which ensures that the gradient norm $\|\partial \mathcal{L} / \partial W^{(2)}\|$ remains bounded.

\medskip
\noindent
\textbf{(3) Post-LN (vanishing gradient).} Consider the following sequence of operations:
\begin{equation}
a = \mathrm{MLP}(x), o = x + a, \tilde{o} = \mathrm{Norm}(o),
\end{equation}
then
\begin{equation}
\left\lVert \frac{\partial \mathcal{L}(\tilde{o})}{\partial W_{i,j}^{(2)}} \right\rVert 
\;\le\; \frac{4\,\gamma\,\sqrt{D}\,\|h\|}{\|x + a\|}, 
\end{equation}
where $h := \mathrm{ReLU}\left(x W^{(1)} + b^{(1)}\right)$. In this case, when a massive activation $\|h\|$ occurs, $\mathrm{Norm}(\cdot)$ introduces an overly suppressing factor $\|x+a\|$, which contains a separate huge residual signal $x$, potentially leading to a vanishing gradient $\|\partial \mathcal{L} / \partial W^{(2)}\|$.
\vskip -0.1in
\end{proposition}

We have compiled a Table~\ref{tab:variance_summary} that provides a overview of the variance and gradient intuition for each layer normalization strategy. %Intuitively, as $a$ grows large, the additional normalization steps help keep the gradient magnitude under control, thereby stabilizing training. This result sheds light on why Peri-LN may reduce the sensitivity to large intermediate activations compared to other LN placements. 

