\section{Introduction}
\label{sec:intro}

Building on a rapidly expanding lineage of Transformer-based large language models, open-source models have shown remarkable impact \citep{chinchilla, llama3, guo2025deepseek}. As the demand for larger and more powerful models grows, various training stabilization techniques have been introduced \citep{tensorprogram5, attentioncollapse, ngpt}. Among these, the choice of where and how to apply layer normalization (LN: LayerNorm or RMSNorm; \citealp{LN, RMSNorm}) critically influences model convergence \citep{onlayer, transformersgetstable, smallproxies, embeddingln}. However, their immense computational requirements have restricted deeper exploration of the underlying Transformer structure. Are we truly employing the optimal LN placement? In practice, fully revealing the results of massive resource investments can be challenging \citep{gemma2}. Despite its importance, there is still no consensus on a single best LN placement strategy. 

Two prominent LN placements have been widely explored. Post-LN \citep{attentionisallyouneed} normalizes the hidden state after adding the sub-layer output to the residual stream (that is, $\mathrm{Norm}(x + \mathrm{Module}(x))$ where $x$ is input hidden state. $\mathrm{Norm}$ is LN). This helps constrain the variance of hidden states but may inadvertently weaken gradient signals, particularly in deeper models \citep{transformersgetstable}. Pre-LN \citep{llama3}, by contrast, normalizes before passing the hidden state to the sub-layer (that is, $x + \mathrm{Module}(\mathrm{Norm}(x))$). While this can enhance gradient propagation, it also admits so-called “massive activations,” where hidden states grow exponentially across layers \citep{massiveactivation, smallproxies, attentioncollapse}.

Previous studies on deep convolutional neural networks (CNNs) have analyzed the impact of batch normalization on variance changes during the initialization stage of ResNet architectures, demonstrating its relationship to model performance \citep{cnnvariance}. They noted that, in models without normalization, hidden activation growth \emph{at initialization} can be exponential, leading to poor performance and stability. In contrast, in pre-normalized CNNs, the variance of hidden activations was shown to increase linearly as model depth grows. 
In the same vein, \citet{transformersgetstable} reported that, for Transformer architectures as well, the variance in the forward propagation of Transformer-based language models \emph{at initialization} increases linearly with depth.
However, in the context of Transformer architectures, we observed that this variance growth at initialization does not persist as training progresses as shown in Figure \ref{fig:3iter}.
Section~\ref{sec:ln_in_transformer} and~\ref{sec:experiments} provide a more detailed discussion of these hidden-state growth patterns.

Beyond these two common strategies, Post-LN and Pre-LN, a third LN placement has quietly emerged in large-scale open-source models: applying LN around the sub-layer, i.e., on both its input and output. 
Although recent open-source models \citep{gemma2, olmo2} have quietly adopted such designs and demonstrated promising performance on a large scale, these efforts often appeared isolated, lacking a conceptual unifying framework or a thorough investigation into their benefits. In this paper, we coin the term \emph{Peri-LN}\footnote{``Peri-'' means ``around,'' reflecting that LN encapsulates the entire sub-layer.} to unify these scattered approaches and highlight an underexplored avenue for stabilizing large-scale Transformer training. While prior work offers encouraging empirical results, there is still a lack of in-depth analysis explaining \emph{why} and \emph{how} Peri-LN can help mitigate issues that frequently arise with Post-LN and Pre-LN. By probing the forward and backward dynamics of Peri-LN, we provide fresh insights into where and when it may offer advantages over more widely adopted LN placements.

Accordingly, this paper revisits LN placement in Transformers from both analytical and empirical perspectives. In particular, we:

\begin{enumerate}
    \item Present an in-depth empirical analysis of Post-LN and Pre-LN in large-scale Transformers, examining how variance and gradient properties evolve \emph{beyond initialization}. By studying forward and backward propagation statistics, we clarify why Post-LN may suffer vanishing gradients and why Pre-LN can exhibit exponential growth of hidden state.
    \item Investigate Peri-LN to understand how normalizing both the inputs and outputs of each module moderates hidden-state behavior during forward and backward propagation, providing a systematic perspective on this underexplored alternative. We show that Peri-LN not only improves gradient stability and final loss but also plays a critical role in reducing hidden-state redundancy, as demonstrated by our ablation studies. 
    \item Provide quantitative evidence on how large activation influences training stability, benchmark performance, and redundancy in hidden states. Drawing on theoretical bounds for gradient propagation as well as extensive experiments, we demonstrate that Peri-LN can mitigate excessive variance growth and gradient instability more effectively than commonly used alternatives. 
\end{enumerate}

%We conduct large-scale experiments on Transformers with up to 3.2B parameters, examining variance growth trends, gradient behavior, and final performance across a variety of training regimes. Our findings suggest that placing LN on module outputs (and possibly inputs) can help contain “massive activations” while preserving beneficial gradient flow, offering a promising balance for stable optimization. By unifying these approaches under the term Peri-LN, we seek to consolidate existing variants and encourage deeper investigation into this underexplored alternative.



