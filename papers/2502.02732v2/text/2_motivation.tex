
\begin{figure}[t]
\vskip -0.1in
    \centering
    \includegraphics[width=.47\linewidth]{Figures/test_Growth_Comparison_First_3_Iter_X_of_onehot_Variance_BLOCK_OUTPUT_1B_post_no_broken.png} \includegraphics[width=.47\linewidth]{Figures/test_Growth_Comparison_First_3_Iter_X_of_onehot_Variance_BLOCK_OUTPUT_pre_1B.png} 
    \vskip -0.05in
    \caption{Illustration of hidden-state variance across different model depths and training iterations. From the initialization stage up to the point where 6.3 billion tokens were trained, we observed the variance growth of hidden states for Pre-LN and Post-LN architectures. The analysis was conducted using a $1.5$B-parameter model, and consistent trends were observed across models of different sizes. Detailed settings and more results are in Section~\ref{subsec:growth of hidden state}.}
    \label{fig:3iter}
    \vskip -0.1in
\end{figure}

\section{Background and Motivation}
The analysis of activation variance at model initialization has long been central to understanding normalization layers and enhancing stability in convolutional neural networks (CNNs) \citep{cnnvariance, identity, BrockDSS21}. \citet{cnnvariance} showed that batch normalization in residual blocks can bias networks toward the identity function, thereby stabilizing gradients and improving overall training dynamics. 

Similar investigations have emerged for Transformer architectures, examining how variance propagates and how gradients behave in both post-layer normalization (Post-LN) \citep{attentionisallyouneed} and pre-layer normalization (Pre-LN) \citep{llama3} configurations \citep{onlayer, transformersgetstable, smallproxies, mixln}. Early work comparing Post- and Pre-LN primarily focused on gradient scales and loss behavior. \citet{onlayer} observed that Pre-LN architectures tend to exhibit more stable gradients, but can still encounter issues such as gradient spikes and divergence, especially in deeper models or large-scale pre-training scenarios \citep{attentioncollapse, smallproxies, mlpswiglu, embeddingln}. 

Among these challenges, the phenomenon of ``massive activations'' has attracted particular attention \citep{llm.int8,yu2024super,mlpswiglu}. \citet{massiveactivation} identified that in Pre-LN architectures, large spikes in activation magnitude can persist across layers due to residual connections. These massive activations act as fixed biases, potentially narrowing the model’s focus to certain tokens and may influence generalization. However, the underlying mechanisms behind these large values—and their exact impact on the training process—remain not yet well understood.

Analytical work has provided theoretical frameworks to explain phenomena like gradient explosion and vanishing in Transformers. For instance, \citet{transformersgetstable} introduced a signal propagation theory that details how activation variance and gradient instability can evolve with depth, identifying critical factors that impair stability and performance. Recent studies have discussed how Pre-LN architectures can allow large values from Attention or MLP modules to flow unimpeded through residual connections \citep{moeut, mlpswiglu, attentioncollapse, smallproxies}, but the precise impact of this behavior on large-scale training remains insufficiently explored.

These observations underscore the ongoing need to clarify how activation dynamics, normalization strategies, and architectural choices interact, especially in large-scale models. In response, this work aims to deepen our understanding of activation evolution during Transformer training under different normalization architectures, focusing on the role of massive activations and their effects on overall stability and performance.

We defer an extended discussion of the related literature to Appendix~\ref{appendix:relatedwork}, owing to space limitations.
