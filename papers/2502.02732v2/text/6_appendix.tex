\onecolumn
\section{Related Work} \label{appendix:relatedwork}
\paragraph{Activation Dynamics in Large Language Models.}
Studies on the distribution and magnitude of activations in deep neural networks have revealed that certain outlier features can significantly affect model behavior and efficiency. \citet{llm.int8} examined Transformer architectures, highlighting how specific feature dimensions may exhibit unusually large values (outliers) that disrupt quantization and overall system performance. Extending this line of work, \citet{massiveactivation} identified the occurrence of ``massive activations''---extremely large activation values that persist across multiple layers. Unlike standard outliers, these massive activations remain relatively invariant to different inputs, effectively functioning as implicit bias terms in large language models (LLMs). Notably, such extreme values can skew the self-attention mechanism, causing the model to attend disproportionately to certain tokens. These observations demonstrate that even with standard normalization layers in place, hidden biases may linger in internal representations, underscoring the importance of deeper analyses of activation behavior in LLMs.

\paragraph{Variance Control and Normalization in Convolutional Networks.}
The interplay between activation variance and training stability has also been extensively explored in convolutional neural networks (CNNs). \citet{cnnvariance} showed that Batch Normalization (BN) stabilizes the training of residual networks by effectively downscaling activation variance in the residual branches, thereby improving gradient behavior. However, BN imposes certain constraints, such as dependence on batch size and additional computational overhead for estimating batch statistics. Consequently, several normalization-free or alternative normalization approaches have been investigated. For instance, \citet{resnet_scale} introduced ``Normalizer-Free ResNets,'' which manage activation variance through learnable scaling parameters. This approach achieved competitive performance without relying on BN, highlighting the critical role of effective variance control in fostering stable optimization and strong generalization in CNNs.

\paragraph{Layer Normalization in Transformers.}
Training stability in Transformer architectures is closely tied to the choice and placement of layer normalization (LN). \citet{onlayer} reported that Transformers employing a Post-LN structure often suffer from gradient instabilities at initialization, requiring a careful learning-rate warm-up phase to mitigate these issues. In contrast, Pre-LN helps maintain more stable gradients during the early stages of training. However, \citet{transformersgetstable} showed that while Post-LN can lead to vanishing or exploding gradients in deep Transformers, Pre-LN may induce hyperbolic gradient growth. These findings illustrate the nuanced trade-offs of normalization placement and draw parallels to earlier CNN studies, where careful management of activation variance proved essential for stable deep learning.

\paragraph{Gradient Propagation and Depth Scaling}
Ensuring consistent gradient propagation across many layers is pivotal for stable training in very deep models. \citet{tensorprogram4} (Tensor Programs IV) introduced the concept of Maximal Update Parametrization ($\mu$P) in the infinite-width regime to preserve feature learning, preventing gradients from collapsing into kernel-like dynamics. Building on this, \citet{tensorprogram6} (Tensor Programs VI) proposed Depth-$\mu$P, which scales residual branches and learning rates according to network depth. Their theoretical analysis indicates that improper depth-dependent scaling leads to vanishing or exploding gradients, ultimately diminishing the diversity of learned representations. These insights highlight the necessity for principled scaling strategies and careful initialization to maintain robust gradient flow in deep architectures.

\smallskip
\noindent
\paragraph{Summary.}
Taken together, these studies underscore the importance of managing activation variance and hidden biases to achieve stable training and expressive internal representations in modern deep networks. In Transformer-based models, normalization choice and placement---such as Post-LN, Pre-LN, or other variants---play a significant role in controlling gradient dynamics and overall performance. While Post-LN and Pre-LN have received significant attention, we focus on a comparative analysis that includes \textit{Peri-LN}, an alternative normalization placement that has thus far been underexplored but holds potential for enhancing training stability and model performance.


\newpage

\section{Detailed Experimental Setting} \label{appendix:exp_settings}
In this section, we provide detailed configurations of both the pretraining and supervised fine-tuning to reproduce our results.
\subsection{Configurations on Pre-Training}
The common training settings are provided in Table~\ref{tab:trainingconfigurations}. Embedding settings for the language models are listed in Table~\ref{tab:embedding config}. For the model architecture, we primarily follow the Llama~$3$ architecture \citep{llama3}. In the MLP module, we use SwiGLU activations. Additional details regarding the model configurations are shown in Table~\ref{tab:modelsize}. Note that embedding parameters are excluded from the model size. Unless otherwise noted, most training and model settings follow those of the DCLM experiments \citep{dclm}. During the pretraining stage, each model was trained under a controlled random seed.

\begin{table}[ht]
    \centering
    \caption{Common configurations. \textit{LR Schedule} denotes learning rate schedule. }
    \label{tab:trainingconfigurations}
    \begin{tabular}{ccccccc}
    \toprule
        Global Batch Size  &  Weight Decay & Iterations & Optimizer & LR Schedule & Warmup\\
    \toprule
        $256$ & $0.033$ & $14400$ & Adam & Cosine & $10$\% \\ 
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[ht]
    \centering
    \caption{Embedding configurations.}
    \label{tab:embedding config}
    \begin{tabular}{ccc}
    \toprule
        Max Position Embeddings  &  Position Embedding Type & Untie-embeddings-and-output-weights\\
    \toprule
        $8192$ & Rope & $True$ \\ 
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[ht]
    \centering
    \caption{Model configurations.}
    \label{tab:modelsize}
    \begin{tabular}{lcccc}
    \toprule
        Size & $n_{layers}$ & $n_{heads}$ & $d_{model}$ & $d_{head}$\\
    \toprule
        $400$M & $24$ & $8$ & $1024$ & $128$ \\ 
        $1.5$B & $24$ & $16$ & $2048$ & $128$  \\ 
        $3.2$B & $32$ & $16$ & $2560$ & $160$ \\ 
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Configurations on Supervised Fine-Tuning}\label{appendix:SFTsetup}
 To examine downstream task performance after instruction tuning, we employed a high-quality LIMA alignment training set consisting of $1,000$ samples~\citep{lima}. Our supervised fine-tuning configuration was slightly modified from the original setup of LIMA: we fine-tuned the model for $15$ epochs with a batch size of $128$. The optimizer was Adam with an initial learning rate of $1$e-$5$ and a cosine learning rate schedule. We selected the best checkpoints for each model by evaluating on OpenBookQA~\citep{OpenBookQA2018}, CommonSenseQA~\citep{talmor-etal-2019-commonsenseqa}, and the NLI dataset in GLUE~\citep{wang-etal-2018-glue}.


\newpage
\section{Additional Results on Pre-Training Study} \label{appendix:additionalresults_pretraining}

\subsection{Post-Layer Normalization Architecture \& Learning Rate Exploration} \label{appendix:postln}
In order to identify the optimal performance configuration for Post-LN within the experimental setup, we conducted a learning rate exploration as shown in Figure~\ref{fig:pretrain_lrwseep_post}. Because the appropriate learning rate for Post-LN fell into a much lower range than those for Pre-LN and Peri-LN, we treated it separately. For each Post-LN setting, the best learning rate was determined as the one yielding the lowest final training loss, with the random seed held constant during this selection process.


\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.25\linewidth]{Figures/pretrain_lrsweep_post.png} 
    \caption{Learning rate explorations for Post-LN architectures.}
    \label{fig:pretrain_lrwseep_post}
\end{figure}




\subsection{Best Performing Checkpoints Comparisons of Other Model Sizes} \label{appendix:best_perform_loss_others}
As an extension of Section~\ref{subsec:pretrain}, we present below the results for additional model sizes that were omitted previously due to space constraints. In Figures \ref{fig:pretraining_others}, we compare the pre-training loss and the gradient norm curve at each LN strategy’s best-performing learning rate of $3.2$B and $1.5$B size models. 

\begin{figure}[ht!]
    \centering
    \subfigure[Training loss for $3.2$B]
    {
    \includegraphics[width=.2\linewidth]{Figures/hcx_text_3B_dclm_000_30B_warmup10_lr2e4.csv_best_loss_trainingloss_per_tokens.png}
    }
    \subfigure[Gradient-norm for $3.2$B]
    {
    \includegraphics[width=.2\linewidth]{Figures/hcx_text_3B_dclm_000_30B_warmup10_lr2e4.csv_best_loss_warmup10_gradnorm_per_tokens.png}
    }
    \\
    \subfigure[Training loss for $1.5$B]
    {
    \includegraphics[width=.2\linewidth]{Figures/hcx_text_1B_dclm_000_30B_warmup10_lr3e4.csv_best_loss_trainingloss_per_tokens.png}
    }
    \subfigure[Gradient-norm for $1.5$B]
    {
    \includegraphics[width=.2\linewidth]{Figures/hcx_text_1B_dclm_000_30B_warmup10_lr3e4.csv_best_loss_warmup10_gradnorm_per_tokens.png}
    }
    \caption{
    Performance comparison of Post-LN, Pre-LN, and Peri-LN Transformers during pre-training for other two. 
    }
    \label{fig:pretraining_others}
\end{figure}

\subsection{Gradient Spikes during Pre-Training}
Although this observation is empirical, it remains sufficiently noteworthy to discuss. During extensive pre-training ablation experiments, we frequently observed gradient-norm spikes. Specifically, when examining how these spikes vary with different learning rates, we found that Pre-LN exhibited irregular spikes regardless of the learning rate setting, with a particularly high occurrence in the early stages of training. In contrast, \emph{such spikes were rarely observed under Peri-LN.}

\newpage

\section{Proof of Theoretical Insight} \label{appendix:theory_proof}

To support the claim that Peri-LN enhances the stability of training in such cases, we analyze the gradient norm in the final layer. For simplicity, we use RMSNorm and ReLU here.

\begin{proposition}
Consider the following sequence of operations:
\begin{align*}
    & \quad \tilde{x} = \mathrm{RMSNorm}(x), \\
    & \quad a = \mathrm{ReLU}(\tilde{x} W^{(1)} + b^{(1)})W^{(2)} + b^{(2)}, \\
    & \quad o = x + a.
\end{align*}
Then,
\begin{equation}
    \frac{\partial \mathcal{L}(o)}{\partial W_{i,j}^{(2)}} = h_{i} (\hat{p}_{j} - y_{j}),  
\end{equation}
where $h := \mathrm{ReLU}\left(x W^{(1)} + b^{(1)}\right)$, $\hat{p} := \mathrm{softmax}(o)$, and $y$ is the label (one-hot vector).
\end{proposition}

\begin{proof}
By the chain rule,
\begin{equation}
\frac{\partial \mathcal{L}(o)}{\partial W_{i,j}^{(2)}} = 
    \underbrace{\frac{\partial \mathcal{L}(o)}{\partial o}}_{\text{(a)}: 1 \times D} \times 
    \underbrace{\frac{\partial o}{\partial a}}_{\text{(b)}: D \times D} \times 
    \underbrace{\frac{\partial a}{\partial W_{i,j}^{(2)}}}_{\text{(c)}: D \times 1}.    
\end{equation}

(a) It is known that 
\begin{equation}
    \frac{\partial \mathcal{L}(o)}{\partial o_{k}} = \hat{p}_{k} - y_{k}.
\end{equation}
So,
\begin{equation}
\frac{\partial \mathcal{L}(o)}{\partial o} =   
\begin{bmatrix}
    \hat{p}_{1} - y_{1} & \hat{p}_{2} - y_{2} & \cdots & \hat{p}_{D} - y_{D}
    \end{bmatrix}.
\end{equation}

(b) Since $o = x + a$,
\begin{equation}
    \frac{\partial o}{\partial a} = I.
\end{equation}

(c) Recall that 
\begin{equation}
    a := \mathrm{ReLU}(\tilde{x} W^{(1)} + b^{(1)}) W^{(2)} + b^{(2)}.
\end{equation}

For convenience, let
\begin{equation}
    h := \mathrm{ReLU}(\tilde{x} W^{(1)} + b^{(1)}).
\end{equation}

Then, we have
\begin{equation}
    \frac{\partial a_{k}}{\partial W_{i,j}^{(2)}} = 
    \frac{\partial}{\partial W_{i,j}^{(2)}} 
    \left( \sum_{p=1}^{H} h_{p} W_{p,k}^{(2)} + b_{k}^{(2)} \right) = h_{i} \, \delta_{k,j}.
\end{equation}

In vector representation,
\begin{equation}
    \frac{\partial a}{\partial W_{i,j}^{(2)}} =
    \begin{bmatrix}
    0 & \cdots & h_{i} & \cdots & 0
    \end{bmatrix}^\top,
\end{equation}
where the only nonzero entry is in the $j$-th component.

Thus, by putting these all together,
\begin{equation}
    \frac{\partial \mathcal{L}(o)}{\partial W_{i,j}^{(2)}} = h_{i} (\hat{p}_{j} - y_{j}).    
\end{equation}
\end{proof}



\begin{proposition}
Consider the following sequence of operations:
\begin{align*}
    & \quad \tilde{x} = \mathrm{RMSNorm}(x), \\
    & \quad a = \mathrm{ReLU}(\tilde{x} W^{(1)} + b^{(1)})W^{(2)} + b^{(2)}, \\
    & \quad \tilde{a} = \mathrm{RMSNorm}(a), \\
    & \quad o = x + \tilde{a}.
\end{align*}
Then,
\begin{equation}
    \left\lVert \frac{\partial \mathcal{L}(o)}{\partial W_{i,j}^{(2)}} \right\rVert 
\leq  
\frac{4\gamma \sqrt{D} \| h \|}{\|a\|}, 
\end{equation}
where $\gamma$ is the scaling parameter used in $\mathrm{RMSNorm}(\cdot)$, $D$ is the dimensionality, and $h := \mathrm{ReLU}\left(x W^{(1)} + b^{(1)}\right)$.
\end{proposition}

\begin{proof}
By the chain rule,
\begin{equation}
\frac{\partial \mathcal{L}(o)}{\partial W_{i,j}^{(2)}} = 
    \underbrace{\frac{\partial \mathcal{L}(o)}{\partial o}}_{\text{(a)}: 1 \times D} \times 
    \underbrace{\frac{\partial o}{\partial \tilde{a}}}_{\text{(b)}: D \times D} \times 
    \underbrace{\frac{\partial \tilde{a}}{\partial a}}_{\text{(c)}: D \times D} \times 
    \underbrace{\frac{\partial a}{\partial W_{i,j}^{(2)}}}_{\text{(d)}: D \times 1}.    
\end{equation}


(a) We have
\begin{equation}
    \left\| \frac{\partial \mathcal{L}(o)}{\partial o} \right\| = \| \hat{p} - y \| \leq \| \hat{p} \| + \| y \| = 2.    
\end{equation}

(b) We also have
\begin{equation}
    \left\| \frac{\partial o}{\partial \tilde{a}} \right\| = \| I \| = 1.
\end{equation}


(c) Recall that
\begin{equation}
    \tilde{a} := \mathrm{RMSNorm}(a) = \gamma \cdot \frac{a}{\sqrt{\frac{1}{D} \sum_{k=1}^D a_{k}^2 + \epsilon}}.    
\end{equation}

Then, $\frac{\partial \tilde{a}}{\partial a}$ is the Jacobian matrix $J$ of $\mathrm{RMSNorm}(\cdot)$. For brevity, let
\begin{equation}
    \alpha := \frac{1}{D} \sum_{k=1}^D (a_{k})^2.
v\end{equation}

Then,
\begin{align}
J_{p,q} = \frac{\partial \tilde{a}_{p}}{\partial a_{q}} 
&= \gamma \cdot \frac{\partial}{\partial a_{q}} \left( \frac{a_{p}}{\sqrt{\alpha + \epsilon}} \right) \\
&= \gamma \cdot \frac{1}{\sqrt{\alpha + \epsilon}} \frac{\partial a_{p}}{\partial a_{q}} 
+ \gamma \cdot a_{p} \frac{\partial}{\partial a_{q}} \left( \frac{1}{\sqrt{\alpha + \epsilon}} \right) \\
&= \gamma \cdot \frac{1}{\sqrt{\alpha + \epsilon}} \delta_{p,q} 
- \gamma \cdot \frac{a_{p} a_{q}}{D (\alpha + \epsilon)^{3/2}}.
\end{align}

In matrix representation,
\begin{equation}
    J = \underbrace{\frac{\gamma}{\sqrt{\alpha + \varepsilon}} I}_{A} 
    - \underbrace{\frac{\gamma}{D(\alpha + \varepsilon)^{3/2}} \left(a\right)^\top \left(a\right)}_{B}.
\end{equation}

Then, we have
\begin{equation}
    \|A\| = \left\|\frac{\gamma}{\sqrt{\alpha + \varepsilon}} I \right\| 
    = \frac{\gamma}{\sqrt{\alpha + \varepsilon}} \|I\| 
    = \frac{\gamma}{\sqrt{\alpha + \varepsilon}},
\end{equation}
and
\begin{equation}
    \|B\| = \left\|\frac{\gamma}{D(\alpha + \varepsilon)^{3/2}} \left(a\right)^\top \left(a\right)\right\| 
    = \frac{\gamma}{D(\alpha + \varepsilon)^{3/2}} \times D\alpha 
    = \frac{\gamma \alpha}{(\alpha + \varepsilon)^{3/2}}.
\end{equation}

So, we have
\begin{equation}
    \| J \| = \| A - B \| \leq \| A \| + \| B \| \leq \frac{2\gamma}{\sqrt{\alpha}} = \frac{2\gamma \sqrt{D}}{\|a\|}.
\end{equation}

(d) Since
\begin{equation}
    \frac{\partial a}{\partial W_{i,j}^{(2)}} =
    \begin{bmatrix}
    0 & \cdots & h_{i} & \cdots & 0
    \end{bmatrix}^\top,
\end{equation}
we have
\begin{equation}
    \left\| \frac{\partial a}{\partial W_{i,j}^{(2)}} \right\| \leq \| h \|.
\end{equation}

Thus,
\begin{equation}
    \left\| \frac{\partial \mathcal{L}(o)}{\partial W_{i,j}^{(2)}} \right\| \leq 2 \times 1 \times \frac{2\gamma \sqrt{D}}{\|a\|} \times \| h \| = \frac{4\gamma \sqrt{D} \| h \|}{\|a\|}.
\end{equation}

\end{proof}


\begin{proposition}
Consider the following sequence of operations:
\begin{align*}
    & \quad a = \mathrm{ReLU}(x W^{(1)} + b^{(1)})W^{(2)} + b^{(2)}, \\
    & \quad o = x + a, \\
    & \quad \tilde{o} = \mathrm{RMSNorm}(o). \\
\end{align*}
Then,
\begin{equation}
    \left\lVert \frac{\partial \mathcal{L}(\tilde{o})}{\partial W_{i,j}^{(2)}} \right\rVert 
\leq  
\frac{4\gamma \sqrt{D} \| h \|}{\|x + a\|}, 
\end{equation}
where $\gamma$ is the scaling parameter used in $\mathrm{RMSNorm}(\cdot)$, $D$ is the dimensionality, and $h := \mathrm{ReLU}\left(x W^{(1)} + b^{(1)}\right)$.
\end{proposition}

\begin{proof}
The proof is analogous to the proof of the previous proposition.
\end{proof}


\newpage
\section{Additional Results on Growth of Hidden State}
In this section, we examine the $400$M- and $3.2$B-parameter models, which were omitted in Section~\ref{subsec:growth of hidden state} due to space constraints. As illustrated in Figures~\ref{fig:growth_of_hidden_state_3B} and \ref{fig:growth_of_hidden_state_400M}, these models exhibit the same overall trend.

\begin{figure}[ht!]
    \centering
    \subfigure[Absolute magnitude growth]
    {
    \includegraphics[width=0.35\linewidth]{Figures/Growth_Comparison_by_Iter_X_of_onehot_Absoulte_Mean_BLOCK_OUTPUT_3B_post_14000.png} 
    }
    \subfigure[Variance growth]
    {
    \includegraphics[width=0.375\linewidth]{Figures/Growth_Comparison_by_Iter_X_of_onehot_Variance_BLOCK_OUTPUT_3B_post_14000.png}
    }
    \caption{The forward growth patterns of hidden state for different architectures highlight the structural impact of normalization placement. $3.2$B size model.}
    \label{fig:growth_of_hidden_state_3B}
\end{figure}

\begin{figure}[ht!]
    \centering
    \subfigure[Absolute magnitude growth]
    {
    \includegraphics[width=0.35\linewidth]{Figures/Growth_Comparison_by_Iter_X_of_onehot_Absoulte_Mean_BLOCK_OUTPUT_400M_post_14000.png} 
    }
    \subfigure[Variance growth]
    {
    \includegraphics[width=0.375\linewidth]{Figures/Growth_Comparison_by_Iter_X_of_onehot_Variance_BLOCK_OUTPUT_400M_post_14000.png} 
    }
    \caption{The forward growth patterns of hidden state for different architectures highlight the structural impact of normalization placement. $400$M size model.}
    \label{fig:growth_of_hidden_state_400M}
\end{figure}



\newpage
\section{Additional Experimental Results on Ablation Study} \label{appendix:additionalresults}

\subsection{Amount of Training Tokens}
In order to investigate whether the learning behavior of each LN strategy varies with the number of training tokens, we conducted an additional round of learning-rate exploration for both the Pre-LN and Peri-LN architectures. As shown in Figure~\ref{fig:tokensweep}, even as the number of training tokens increases, there is no observable shift in the optimal learning-rate range. Based on these findings, we conclude that our overall results \emph{remain consistent}, even when the training token count is further increased. Furthermore, although a learning rate of \(5\times10^{-3}\) leads to divergence in the smaller-scale experiments with $8$B or $16$B training tokens, it does not do so in the $30$B-token setting. We attribute this discrepancy to the $10$\% warmup rate, suggesting that the warmup phase may be insufficient for the smaller-scale experiments.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.42\linewidth]{Figures/1B_warmup10_minloss_token_comp_styled.png} 
    \caption{Learning rate explorations of Pre-\& Peri-LN architecture with sequence length $2048$ configuration.}
    \label{fig:tokensweep}
\end{figure}


\subsection{Sequence Length}
In language models, the number of iterations per token is influenced by the sequence length, which in turn, along with the batch size, affects training statistics. We conducted an experiment to determine whether the performance trend changes when the sequence length is reduced from 8192, as set in the main text, to 2048. As shown in Figure~\ref{fig:sqlen2k}, Peri-LN still surpasses Pre-LN in the learning rate exploration.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.42\linewidth]{Figures/sqlen2k_comp.png} 
    \caption{Learning rate explorations of Pre-\& Peri-LN architecture with sequence length $2048$ configuration.}
    \label{fig:sqlen2k}
\end{figure}

\newpage

\subsection{Warm-up}
Warmup is widely recognized to influence training stability. To investigate whether a $10$\% warmup rate might unfairly disadvantage Pre-LN, we conducted an additional learning-rate exploration using a $30$\% warmup rate. As illustrated in Figure~\ref{fig:warmup30}, the overall trend remained unchanged, and Peri-LN continued to exhibit better performance than Pre-LN in terms of loss. Furthermore, we observed that increasing the warmup rate from $10$\% to $30$\% did not reduce the frequency of gradient norm spikes in Pre-LN.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.4\linewidth]{Figures/pretrain_lrsweep_warmup30.png} 
    \caption{Learning rate explorations of Pre-\& Peri-LN architecture with warmup $30$\% configuration.}
    \label{fig:warmup30}
\end{figure}

\subsection{RMSNorm \& LayerNorm} \label{appendix:rmsnorm}
As illustrated in Figure~\ref{fig:layernorm}, we conducted experiments in which RMSNorm and LayerNorm were interchanged. Consistent with the findings reported in \citep{olmo2}, we did not observe any notable performance differences in our RMSNorm and LayerNorm replacement experiments. Learning rate was set to $2$e-$3$ (best performance learning rate).


\begin{figure}[ht!]
    \centering
    \subfigure[Pre-training loss curve]
    {
    \includegraphics[width=0.40\linewidth]{Figures/layernorm_trainingloss.png} 
    }
    \subfigure[Gradient-norm curve]
    {
    \includegraphics[width=0.38\linewidth]{Figures/layernorm_gradnorm.png}
    }
    \caption{LayerNorm vs. RMSNorm on Peri-LN architecture. $400$M size model.}
    \label{fig:layernorm}
\end{figure}

\newpage

\subsection{Embedding Layer Normalization of Peri-Layer Normalization Transformers}\label{appendix:embeddingln}
Motivated by \citet{spikenomore}, we empirically explore the addition of embedding layer normalization to improve training stability and overall model performance in Transformer architectures. As illustrated in Figures~\ref{fig:embedding_400M}, \ref{fig:embedding_1B}, and \ref{fig:embedding_3B}, incorporating Embedding LN in the Peri-LN architecture yields a slight improvement in pre-training loss. Furthermore, our empirical observations suggest that this effect becomes more pronounced in smaller models.

\begin{figure}[ht!]
    \centering
    \subfigure[Pre-training loss curve]
    {
    \includegraphics[width=0.27\linewidth]{Figures/embedding_400M_trainingloss.png} 
    }
    \subfigure[Gradient-norm curve]
    {
    \includegraphics[width=0.25\linewidth]{Figures/embedding_400M_gradnorm.png}
    }
    \caption{Loss and Gradient-norm curves comparing the presence and absence of Embedding LN in the Peri-LN architecture. $400$M size model.}
    \label{fig:embedding_400M}
\end{figure}


\begin{figure}[ht!]
    \centering
    \subfigure[Pre-training loss curve]
    {
    \includegraphics[width=0.27\linewidth]{Figures/embedding_1B_trainingloss.png} 
    }
    \subfigure[Gradient-norm curve]
    {
    \includegraphics[width=0.25\linewidth]{Figures/embedding_1B_gradnorm.png}
    }
    \caption{Loss and Gradient-norm curves comparing the presence and absence of Embedding LN in the Peri-LN architecture. $1.5$B size model.}
    \label{fig:embedding_1B}
\end{figure}

\begin{figure}[ht!]
    \centering
    \subfigure[Pre-training loss curve]
    {
    \includegraphics[width=0.27\linewidth]{Figures/embedding_3B_trainingloss.png} 
    }
    \subfigure[Gradient-norm curve]
    {
    \includegraphics[width=0.25\linewidth]{Figures/embedding_3B_gradnorm.png}
    }
    \caption{Loss and Gradient-norm curves comparing the presence and absence of Embedding LN in the Peri-LN architecture. $3.2$B size model.}
    \label{fig:embedding_3B}
\end{figure}


\newpage
\section{Output-Layer Normalization with QK-Norm Architecture} \label{appendix:olmo2}
Query and Key layer-normalization (QK-Norm) has been widely studied in modern Transformer architectures \citep{smallproxies, attentioncollapse, olmo2}. In particular, \citet{olmo2} reported that QK-Norm combined with module output layer-normalization (output-LN, $B$ in Figure \ref{fig:qk-norm} referred to as “reordered norm” in the OLMo$2$ paper) improves both training loss and stability. As shown in Figure \ref{fig:qk-norm}, QK-Norm is applied after the Query and Key projections, similar to output-LN. From another perspective, QK-Norm is also applied immediately before the attention calculation, akin to a Pre-LN approach. In our view, QK-Norm and Pre-LN (placed at $A^2$ and $A$ respectively in Figure \ref{fig:qk-norm}) serve the same role but differ in certain details. As shown in Figures~\ref{fig:olmo_400M}, \ref{fig:olmo_1B}, and \ref{fig:olmo_3B}, the two architectures exhibit comparable performance overall in terms of both training loss and stability.. However, Peri-LN provides a slight performance advantage over the OLMo$2$-style Peri-LN in the $400$M- and $1$B-parameter models.

\begin{figure}[ht!]
    \centering
        \includegraphics[width=.6\linewidth]{Figures/QKNorm.png}
    \caption{QK-layer normalization in the Attention module.}
    \label{fig:qk-norm}
\end{figure}

\begin{figure}[ht!]
    \centering
    \subfigure[Pre-training loss curve]
    {
    \includegraphics[width=0.30\linewidth]{Figures/olmo2_400M_zoom_trainingloss.png} 
    }
    \subfigure[Gradient-norm curve]
    {
    \includegraphics[width=0.275\linewidth]{Figures/olmo2_400M_grad.png}
    }
    \caption{Comparison of pre-training loss and gradient norm between OLMo2-Style Peri-LN and the Peri-LN architecture. To ensure an accurate comparison, we present the pre-training loss over the final $5$B tokens. $400$M size model.}
    \label{fig:olmo_400M}
\end{figure}

\newpage

\begin{figure}[ht!]
    \centering
    \subfigure[Pre-training loss curve]
    {
    \includegraphics[width=0.30\linewidth]{Figures/olmo2_1B_zoom_trainingloss.png} 
    }
    \subfigure[Gradient-norm curve]
    {
    \includegraphics[width=0.275\linewidth]{Figures/olmo2_1B_grad.png}
    }
    \caption{Comparison of pre-training loss and gradient norm between OLMo2-Style Peri-LN and the Peri-LN architecture. To ensure an accurate comparison, we present the pre-training loss over the final $5$B tokens. $1.5$B size model.}
    \label{fig:olmo_1B}
\end{figure}


\begin{figure}[ht!]
    \centering
    \subfigure[Pre-training loss curve]
    {
    \includegraphics[width=0.30\linewidth]{Figures/olmo2_3B_zoom_trainingloss.png} 
    }
    \subfigure[Gradient-norm curve]
    {
    \includegraphics[width=0.275\linewidth]{Figures/olmo2_3B_grad.png}
    }
    \caption{Comparison of pre-training loss and gradient norm between OLMo2-Style Peri-LN and the Peri-LN architecture. To ensure an accurate comparison, we present the pre-training loss over the final $5$B tokens. $3.2$B size model.}
    \label{fig:olmo_3B}
\end{figure}

% \section{Peri-LN with QK-Norm}
% Following \citet{smallproxies}, which reported a slight improvement in loss by incorporating QK-norm into a Pre-LN architecture, we explored whether Peri-LN would show a similar performance gain. As illustrated in Figure~\ref{fig:peri-qk-ln}, adding QK-norm to Peri-LN indeed yielded better performance. In this experiment, the Peri-LN variant equipped with QK-norm used LayerNorm instead of RMSNorm. 

% \section{Output Layer-Normalization Only Architecture} \label{appendix:placementBonly}
% In this section, we empirically explore output layer-normalization only architecture. For clarity, using only placement $B$ in Figure~\ref{fig:LN Placement} without any other LNs.

% \section{Layer-Dropping} \label{appendix:layerdrop}
% Motivated by \citet{layerdrop}, we drop the transformer blocks one by one and evaluate the performance after each drop.

% \section{Outlier Activation of Attention and MLP Module}\label{appendix:outlierofeachmodule}

\newpage

\section{Additional Details on Evaluation} 
\label{appendix:morebenchmarks}

\subsection{Detailed Configurations}
We utilized the Language Model Evaluation Harness\footnote{\url{https://github.com/EleutherAI/lm-evaluation-harness}}with the HuggingFace Transformers library \citep{eval-harness, wolf-etal-2020-transformers} to assess overall performance. We employ five different evaluation benchmarks: ARC \citep{clark2018thinksolvedquestionanswering},  HellaSwag \citep{hellaswag}, PIQA \citep{bisk2020piqa}, SIQA \citep{siqa}, Winogrande \citep{sakaguchi2021wino}.  During the pretraining stage, each model was trained under a controlled random seed. We used the training loss at iteration $14,000$—corresponding to the completion of $30$B tokens—as our main reference point. When calculating the evaluation score, diverged checkpoints were excluded.

\subsection{Detailed Results on Benchmark Evaluations} \label{appendix:detailed_evals}

In this section, we present the evaluation results for each model trained with five different training seeds. We exclude any diverged scores and average the remaining values, which are then reported in Table~\ref{tab:pre-train} in the main text.

\subsubsection{Pre-Training}
\begin{table}[!ht]
\vskip -0.1in
\small
\caption{Detailed results on pre-training the Peri-LN architecture. These results are averaged to produce the values reported in Table~\ref{tab:pre-train}. \textit{SEED} denotes pre-training seed.}
    \centering
    \begin{tabular}{lcccccc}
    \toprule
        Peri-LN & SEED & ARC-Easy & HellaSwag & PIQA & SIQA & Winogrande \\ 
        \toprule
        ~ & 1 & 0.5758 & 0.3803 & 0.6980 & 0.4115 & 0.5225 \\ 
        ~ & 2 & 0.5728 & 0.3739 & 0.6915 & 0.4017 & 0.5367 \\ 
        400M & 3 & 0.5842 & 0.3745 & 0.6986 & 0.4125 & 0.5249 \\ 
        ~ & 4 & 0.5800 & 0.3722 & 0.6959 & 0.4038 & 0.5209 \\ 
        ~ & 5 & 0.5627 & 0.3719 & 0.6899 & 0.4028 & 0.5320 \\ 
        \midrule
        ~ & 1 & 0.6599 & 0.4437 & 0.7339 & 0.4304 & 0.5714 \\ 
        ~ & 2 & 0.6591 & 0.4394 & 0.7399 & 0.4145 & 0.5699 \\ 
        1.5B & 3 & 0.6625 & 0.4357 & 0.7372 & 0.4166 & 0.5627 \\ 
        ~ & 4 & 0.6633 & 0.4367 & 0.7345 & 0.4222 & 0.5667 \\ 
        ~ & 5 & 0.6637 & 0.4416 & 0.7361 & 0.4335 & 0.5612 \\ 
        \midrule
        ~ & 1 & 0.6953 & 0.4734 & 0.7443 & 0.4417 & 0.5872 \\ 
        ~ & 2 & 0.6839 & 0.4684 & 0.7427 & 0.4324 & 0.6054 \\ 
        3.2B & 3 & 0.6902 & 0.4680 & 0.7486 & 0.4243 & 0.5967 \\ 
        ~ & 4 & 0.6864 & 0.4700 & 0.7427 & 0.4273 & 0.5935 \\ 
        ~ & 5 & 0.6806 & 0.4698 & 0.7372 & 0.4243 & 0.6054 \\ 
        \bottomrule
    \end{tabular}
    \vskip -0.1in
\end{table}

\begin{table}[ht]
\vskip -0.1in
\small
    \centering
    \caption{Detailed results on pre-training the Pre-LN architecture. These results are averaged to produce the values reported in Table~\ref{tab:pre-train}. \textit{SEED} denotes pre-training seed.}
    \begin{tabular}{lcccccc}
    \toprule
        Pre-LN & SEED & ARC-Easy & HellaSwag & PIQA & SIQA & Winogrande \\ 
        \toprule
        ~ & 1 & 0.5669 & 0.3609 & 0.7008 & 0.4002 & 0.5359 \\ 
        ~ & 2 & ~ & ~ & Diverged & ~ & ~ \\ 
        400M & 3 & 0.5354 & 0.3328 & 0.6741 & 0.3905 & 0.4957 \\ 
        ~ & 4 & ~ & ~ & Diverged & ~ & ~ \\ 
        ~ & 5 & 0.5438 & 0.3314 & 0.6888 & 0.4012 & 0.4949 \\ 
        \midrule
        ~ & 1 & 0.6326 & 0.4259 & 0.7242 & 0.4263 & 0.5691 \\ 
        ~ & 2 & 0.6019 & 0.3924 & 0.7111 & 0.3992 & 0.5627 \\ 
        1.5B & 3 & 0.6077 & 0.3932 & 0.7008 & 0.4125 & 0.5272 \\ 
        ~ & 4 & 0.6111 & 0.3886 & 0.7187 & 0.4135 & 0.5225 \\ 
        ~ & 5 & 0.6221 & 0.3941 & 0.7160 & 0.4099 & 0.5438 \\ 
        \midrule
        ~ & 1 & 0.6688 & 0.4588 & 0.7470 & 0.4273 & 0.5919 \\ 
        ~ & 2 & ~ & ~ & Diverged & ~ & ~ \\ 
        3.2B & 3 & ~ & ~ & Diverged & ~ & ~ \\ 
        ~ & 4 & 0.6359 & 0.4259 & 0.7301 & 0.4263 & 0.5564 \\ 
        ~ & 5 & ~ & ~ & Diverged & ~ & ~ \\ 
        \bottomrule
    \end{tabular}
    \vskip -0.1in
\end{table}

\begin{table}[!ht]
\small
\vskip -0.1in
    \centering
    \caption{Detailed results on pre-training the Post-LN architecture. These results are averaged to produce the values reported in Table~\ref{tab:pre-train}. \textit{SEED} denotes pre-training seed.}
    \begin{tabular}{lcccccc}
    \toprule
        Post-LN & SEED & ARC-Easy & HellaSwag & PIQA & SIQA & Winogrande \\ 
        \toprule
        ~ & 1 & 0.3413 & 0.2881 & 0.6311 & 0.3378 & 0.5067 \\ 
        ~ & 2 & 0.3691 & 0.2886 & 0.6132 & 0.3337 & 0.5099 \\ 
        400M & 3 & 0.3632 & 0.2889 & 0.6257 & 0.3603 & 0.5051 \\ 
        ~ & 4 & 0.3603 & 0.2920 & 0.6262 & 0.3490 & 0.5012 \\ 
        ~ & 5 & 0.3510 & 0.2880 & 0.6170 & 0.3434 & 0.5209 \\ 
        \midrule
        ~& 1 & 0.4268 & 0.3121 & 0.6659 & 0.3628 & 0.5185 \\ 
        ~ & 2 & 0.4196 & 0.3150 & 0.6654 & 0.3639 & 0.5004 \\ 
        1.5B & 3 & ~ & ~ & Diverged & ~ & ~ \\ 
        ~ & 4 & 0.4285 & 0.3212 & 0.6730 & 0.3511 & 0.4775 \\ 
        ~ & 5 & 0.4419 & 0.3193 & 0.6643 & 0.3557 & 0.5154 \\ 
        \midrule
        ~ & 1 & 0.4731 & 0.3427 & 0.6774 & 0.3664 & 0.5343 \\ 
        ~ & 2 & 0.4638 & 0.3326 & 0.6779 & 0.3577 & 0.4917 \\ 
        3.2B & 3 & 0.3956 & 0.3321 & 0.6143 & 0.3408 & 0.5067 \\ 
        ~ & 4 & 0.4663 & 0.3380 & 0.6692 & 0.3685 & 0.5178 \\ 
        ~ & 5 & 0.4663 & 0.3340 & 0.6839 & 0.3577 & 0.5043 \\ 
        \bottomrule
    \end{tabular}
    \vskip -0.1in
\end{table}

\subsubsection{Supervised Fine-Tuning}

\begin{table}[ht!]
\vskip -0.1in
\small
    \centering
        \caption{Detailed results on SFT with Peri-LN architecture. These results are averaged to produce the values reported in Table~\ref{tab:pre-train}. \textit{SEED} denotes pre-training seed.}
    \begin{tabular}{lcccccc}
    \toprule
        Peri-LN & SEED & ARC-Easy & HellaSwag & PIQA & SIQA & Winogrande \\ 
        \toprule
        ~ & 1 & 0.5800 & 0.3819 & 0.6991 & 0.4145 & 0.5328 \\ 
        ~ & 2 & 0.5783 & 0.3783 & 0.6921 & 0.4038 & 0.5391 \\ 
        400M & 3 & 0.5888 & 0.3806 & 0.6980 & 0.4222 & 0.5288 \\ 
        ~ & 4 & 0.5892	&0.3738&	0.6948&	0.4089	&0.5099 \\ 
        ~ & 5 & 0.5783 & 0.3757 & 0.6991 & 0.4099 & 0.5312 \\ 
    \midrule
        ~ & 1 & 0.6633 & 0.4502 & 0.7356 & 0.4304 & 0.5746 \\ 
        ~ & 2 & 0.6641 & 0.4437 & 0.7405 & 0.4207 & 0.5706 \\ 
        1.5B & 3 & 0.6671 & 0.4454 & 0.7454 & 0.4207 & 0.5620 \\ 
        ~ & 4 & 0.6700	&0.4455	&0.7378& 0.4284&	0.5659\\ 
        ~ & 5 & 0.6688 & 0.4478 & 0.7421 & 0.4324 & 0.5620 \\
        \midrule
        ~ & 1 & 0.7058 & 0.4810 & 0.7486 & 0.4422 & 0.5880 \\ 
        ~ & 2 & 0.6898 & 0.4774 & 0.7437 & 0.4391 & 0.6054 \\ 
        3.2B & 3 & 0.6995 & 0.4770 & 0.7481 & 0.4278 & 0.5912 \\ 
        ~ & 4 & 0.6911 & 0.4777 & 0.7432 & 0.4350 & 0.5943 \\ 
        ~ & 5 & 0.6894 & 0.4781 & 0.7448 & 0.4319 & 0.6046 \\ 
        \bottomrule
    \end{tabular}
    \vskip -0.1in
\end{table}


\begin{table}[!ht]
\vskip -0.1in
\small
    \centering
            \caption{Detailed results on SFT with Pre-LN architecture. These results are averaged to produce the values reported in Table~\ref{tab:pre-train}. \textit{SEED} denotes pre-training seed.}
    \begin{tabular}{lcccccc}
            \toprule
        Pre-LN & SEED & ARC-Easy & HellaSwag & PIQA & SIQA & Winogrande \\ 
        \toprule
        ~ & 1 & 0.5762 & 0.3625 & 0.7078 & 0.4058 & 0.5343 \\ 
        ~ & 2 & ~ & ~ & N/A & ~ & ~ \\ 
        400M & 3 & 0.5370 & 0.3339 & 0.6757 & 0.3905 & 0.4972 \\ 
        ~ & 4 & ~ & ~ & N/A & ~ & ~ \\ 
        ~ & 5 & 0.5509 & 0.3372 & 0.6893 & 0.4074 & 0.4886 \\ 
        \midrule
        ~ & 1 & 0.6385 & 0.4310 & 0.7247 & 0.4227 & 0.5620 \\ 
        ~ & 2 & 0.6035 & 0.3934 & 0.7095 & 0.4038 & 0.5572 \\ 
        1.5B & 3 & 0.6098 & 0.3944 & 0.7035 & 0.4150 & 0.5257 \\ 
        ~ & 4 & 0.6208 & 0.3929 & 0.7182 & 0.4161 & 0.5272 \\ 
        ~ & 5 & 0.6258 & 0.4017 & 0.7171 & 0.4181 & 0.5391 \\
        \midrule
        ~ & 1 & 0.6785 & 0.4681 & 0.7568 & 0.4345 & 0.5825 \\ 
        ~ & 2 & ~ & ~ & N/A & ~ & ~ \\ 
        3.2B & 3 & ~ & ~ & N/A & ~ & ~ \\ 
        ~ & 4 & 0.6427 & 0.4293 & 0.7274 & 0.4299 & 0.5580 \\ 
        ~ & 5 & ~ & ~ & N/A & ~ & ~ \\ 
        \bottomrule
    \end{tabular}
    \vskip -0.1in
\end{table}

\begin{table}
\vskip -0.1in
\small
    \centering
    \caption{Detailed results on SFT with Post-LN architecture. These results are averaged to produce the values reported in Table~\ref{tab:pre-train}. \textit{SEED} denotes pre-training seed.}
    \begin{tabular}{lcccccc}
        \toprule
        Post-LN & SEED & ARC-Easy & HellaSwag & PIQA & SIQA & Winogrande \\ 
        \toprule
        ~ & 1 & 0.4428 & 0.3307 & 0.6583 & 0.3797 & 0.5099 \\
        ~ & 2 & 0.4280 & 0.3208 & 0.6404 & 0.3746 & 0.5178 \\ 
        400M & 3 & 0.4693 & 0.3241 & 0.6578 & 0.3905 & 0.5122 \\ 
        ~ & 4 & 0.4680 & 0.3247 & 0.6610 & 0.3726 & 0.4830 \\ 
        ~ & 5 & 0.4520 & 0.3283 & 0.6572 & 0.3849 & 0.5225 \\ 
        \midrule
        ~ & 1 & 0.5316 & 0.3774 & 0.6980 & 0.3889 & 0.5359 \\
        ~ & 2 & 0.4731 & 0.3316 & 0.6719 & 0.3813 & 0.5028 \\ 
        1.5B & 3 & ~ & ~ & N/A & ~ & ~ \\ 
        ~ & 4 & 0.5387 & 0.3546 & 0.6779 & 0.3864 & 0.4909 \\ 
        ~ & 5 & 0.5261 & 0.3510 & 0.6752 & 0.3767 & 0.5209 \\ 
        \midrule
        ~ & 1 & 0.5623 & 0.4029 & 0.7008 & 0.3920 & 0.5051 \\
        ~ & 2 & 0.5417 & 0.3644 & 0.6823 & 0.3833 & 0.5264 \\ 
        3.2B & 3 & 0.4444 & 0.3604 & 0.6333 & 0.3618 & 0.5043 \\ 
        ~ & 4 & 0.5400 & 0.3645 & 0.6844 & 0.3823 & 0.5020 \\ 
        ~ & 5 & 0.5341 & 0.3677 & 0.6942 & 0.3976 & 0.5012 \\ 
        \bottomrule
    \end{tabular}
    \vskip -0.1in
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

