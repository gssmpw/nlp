
\section{Conclusion}\label{sec:conclusion}
We explore the placement of layer normalization within the Transformer architecture to better understand its role during training. By systematically comparing Post-LN, Pre-LN, and Peri-LN, we highlight their distinct impacts on stability, gradient propagation, and optimization dynamics. Our findings suggest that placing LN on module outputs in addition to the Pre-LN can help manage large activations while preserving beneficial gradient flow, thereby offering a promising balance for stable optimization. By unifying these approaches under the term \emph{Peri-LN}, we seek to consolidate existing variants and encourage deeper investigation into this underexplored alternative.

%We explore the placement of layer normalization within the Transformer architecture to understand its role during training. By systematically comparing Post-LN, Pre-LN, and Peri-LN, we highlight their distinct impacts on stability, gradient propagation, and optimization dynamics. Our findings suggest that placing LN additionally on module outputs can help contain massive activations while preserving beneficial gradient flow, offering a promising balance for stable optimization. By unifying these approaches under the term \emph{Peri-LN}, we seek to consolidate existing variants and encourage deeper investigation into this underexplored alternative.

