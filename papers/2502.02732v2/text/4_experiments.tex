\begin{table*}[t]
\vskip -0.1in
\caption{Average benchmark scores (with standard deviations) across $5$ different training seeds for Post-, Pre-, and Peri-Layer Normalization language models. Each model size excludes the embedding parameters. \textit{Loss} denotes the evaluation loss on random samples of the C$4$ dataset \citep{raffel2020c4}. \textit{Arch.} denotes architecture, and \textit{Avg.} denotes the averaged benchmark score across tasks. \textit{SFT avg.} denotes the averaged benchmark score across tasks of instruction fine-tuned models. When calculating the evaluation score, diverged checkpoints were excluded. }
\label{tab:pre-train}
% 표준편차 매크로 사용 
\newcommand{\pmstd}[1]{{\scriptsize $\pm #1$}}
    \centering
    \small
    \begin{tabular}{llcccccccc}
    \toprule
        Size & Arch.& ARC-Easy & HellaSwag & PIQA  & SIQA & Winogrande & Avg. $\uparrow$ & Loss $\downarrow$ & SFT Avg. $\uparrow$ \\ 
        \toprule
~ & Post-LN & $35.70$ \pmstd{1.09} & $28.91$ \pmstd{0.16} & $62.26$ \pmstd{0.73} & $34.48$ \pmstd{1.04} & $50.88$ \pmstd{0.75} & $42.45$ & $7.46$ & $46.44$\\ 

$400$M & Pre-LN & $54.87$ \pmstd{1.63} & $34.17$ \pmstd{1.66} & $68.79$ \pmstd{1.34} & $39.73$ \pmstd{0.59} & $50.88$ \pmstd{2.35} & $49.69$ & $3.43$ & $49.96$\\ 

~ & Peri-LN & $ \textbf{57.51}$ \pmstd{0.81} & $ \textbf{37.46}$ \pmstd{0.34} & $ \textbf{69.48}$ \pmstd{0.39}   & $ \textbf{40.64}$ \pmstd{0.51} & $ \textbf{52.74}$ \pmstd{0.67} & \textbf{51.57}& \textbf{3.34} & \textbf{51.96}\\ 
\midrule
~ & Post-LN & $42.92$ \pmstd{0.93} & $31.69$ \pmstd{0.41} & $66.72$ \pmstd{0.40} & $35.84$ \pmstd{0.61} & $50.30$ \pmstd{1.87} & $45.49$ & $5.38$ & $48.95$\\ 

$1.5$B & Pre-LN & $61.51$ \pmstd{1.22} & $39.88$ \pmstd{1.53} & $71.41$ \pmstd{0.88} & $41.23$ \pmstd{0.97} & $54.51$ \pmstd{2.07}  & $53.71$ & $3.29$ & $53.89$ \\ 

~ & Peri-LN & $ \textbf{66.17} $ \pmstd{0.21} & $ \textbf{43.94} $ \pmstd{0.34} & $ \textbf{73.63} $ \pmstd{0.24} & $ \textbf{42.34} $ \pmstd{0.83}   & $ \textbf{56.64} $ \pmstd{0.44} & \textbf{56.55} & \textbf{3.18} & \textbf{56.94} \\ 
\midrule

~ & Post-LN & $45.30$ \pmstd{3.23} & $33.59$ \pmstd{0.44} & $66.45$ \pmstd{2.86} & $35.82$ \pmstd{1.09} & $51.10$ \pmstd{1.60}
  & $46.45$ & $4.43$ & $49.33$ \\ 

$3.2$B & Pre-LN & $65.24$ \pmstd{2.32} & $44.23$ \pmstd{2.32} & $73.86$ \pmstd{1.19} & $42.68$ \pmstd{0.07} & $57.42$ \pmstd{2.51}   & $56.69$ & $3.20$ & $57.08$ \\ 

~ & Peri-LN & $ \textbf{68.73} $ \pmstd{0.57} & $ \textbf{46.99} $ \pmstd{0.21} & $ \textbf{74.31} $ \pmstd{0.41} & $ \textbf{43.00} $ \pmstd{0.73}   & $ \textbf{59.76} $ \pmstd{0.78} & \textbf{58.56} & \textbf{3.11} & \textbf{59.02} \\ 
    \bottomrule
    \end{tabular}
\vskip -0.1in
\end{table*}


\section{Experiments} \label{sec:experiments}
In this section, we provide a comprehensive empirical comparison of Post-, Pre-, and Peri-Layer Normalization (LN) across large-scale Transformer pre-training and subsequent evaluations on the language domain. %Drawing on multiple model sizes and training seeds, our results show that Peri-LN effectively constrains runaway activations while preserving gradient stability, ultimately delivering more stable performance in both pre-training and downstream tasks.

\subsection{Experimental Setting} \label{subsec:settings}
Excluding the embedding parameters, model sizes were set to $400$M, $1.5$B, and $3.2$B parameters, respectively. Each model was trained on $30$ billion tokens. For a fair comparison, we pre-trained each model with \emph{five different training seeds}. We conducted a systematic exploration of learning rates, ranging from \(1 \times 10^{-4}\) to \(5 \times 10^{-3}\). The global batch size is set to $256$. We use Adam optimizer with cosine learning rate scheduler. The sequence length was set to $8192$, and the weight decay coefficient was fixed at $0.033$. We employed Megatron-LM\footnote{\url{https://github.com/NVIDIA/Megatron-LM}} to pre-train the Transformers under each layer normalization strategy. We used the DCLM-baseline dataset \citep{dclm} for our experiments, along with the ``cl$100$k\_base'' version of the TikToken tokenizer\footnote{\url{https://github.com/openai/tiktoken}}. Unless otherwise noted, most training and model settings followed those of the DCLM experiments. For layer normalization, we primarily employed RMSNorm. Further details are provided in Appendix~\ref{appendix:exp_settings}.

\subsection{Pre-Training Large Language Models}\label{subsec:pretrain}
Figure \ref{fig:pretrain_lrwseep} illustrates the pre-training loss across learning rates for models ranging in size from $400$M to $3.2$B parameters. Notably, the Peri-LN architecture consistently achieves superior loss curves over this entire model size. Since Pre-LN shows best performance at learning rate $2 \times 10^{-3}$ across all model size, we set this to the default learning rate for Pre-LN and Peri-LN. Unlike Pre-LN, Post-LN’s appropriate learning rate lies in a lower range, so we have provided a separate summary in Appendix \ref{appendix:postln}. In Figures \ref{fig:pretrain_loss} and \ref{fig:pretrain_gradnorm}, we compare the pre-training loss and the gradient norm curve at each LN strategy’s best-performing learning rate of $400$M size models. The same trend was observed across different model sizes. In particular, when sweeping over training seeds, Pre-LN and Post-LN exhibited many spikes in the gradient norm curve, whereas Peri-LN showed relatively few spikes, supporting Proposition~\ref{prop:theory}. This reduction is consistent across both large and small learning rates. %For further results, see Appendix \ref{appendix:additionalresults_pretraining}.

We provide additional results in Appendix \ref{appendix:additionalresults}, including experiments using LayerNorm (instead of RMSNorm), analyses of varying sequence lengths, the effect of different training-token counts, and an ablation study on embedding LN. We also investigate pre-training using the OLMo$2$-style Peri-LN architecture in Appendix \ref{appendix:olmo2}. These findings are likewise consistent with those previously reported.

\begin{figure*}[t]
\vskip -0.1in
    \centering
    \subfigure[Absolute magnitude growth]
    {
    \includegraphics[width=0.35\linewidth]{Figures/Growth_Comparison_X_of_onehot_Absoulte_Mean_1B.png} \label{fig:massive_activation}
    }
    \subfigure[Variance growth]
    {
    \includegraphics[width=0.403\linewidth]{Figures/Growth_Comparison_by_Iter_X_of_onehot_Variance_BLOCK_OUTPUT_1B_post_14000.png} \label{fig:massive_variance}
    }
    \caption{This figure shows the forward growth patterns of hidden states for different architectures, highlighting the structural impact of normalization placement. Each model contains $1.5$ billion parameters (excluding the embedding size). We confirmed that the observed trend remains consistent across all model sizes.}
    \label{fig:growth_of_hidden_state}
\vskip -0.1in
\end{figure*}

\begin{figure*}[t]
    \centering
    \subfigure[Grad-norm at init]
    {
    \includegraphics[width=.23\linewidth]{Figures/layerwise_gradnorm_1B_at_init.png}
    \label{fig:layerwise_gradnorm_init}
    }
    \subfigure[Grad-norm at final]
    {
    \includegraphics[width=.23\linewidth]{Figures/layerwise_gradnorm_1B_at_iter14000.png}
    \label{fig:layerwise_gradnorm_final}
    }
    \subfigure[Grad-variance at init]
    {
    \includegraphics[width=.23\linewidth]{Figures/layerwise_gradvar_1B_at_init.png}
    \label{fig:layerwise_gradvar_init}
    }
    \subfigure[Grad-variance at final]
    {
    \includegraphics[width=.23\linewidth]{Figures/layerwise_gradvar_1B_at_iter14000.png}
    \label{fig:layerwise_gradvar_final}
    }
    \caption{Comparison of the backward gradient norm and variance for Post-LN, Pre-LN, and Peri-LN Transformers at initialization and at the final stage. Model size is $1.5$B. We confirmed that the observed trend remained consistent across all model sizes. \textit{init} denotes the initialization of the model parameters.}
    \label{fig:layerwise_gradient}
\vskip -0.1in
\end{figure*}

\subsection{Benchmark Evaluations \& Instruction Tuning} \label{subsec:sft}
To evaluate how well the pre-trained model’s training loss aligns with its benchmark performance, we conducted five separate benchmark evaluations. Furthermore, to examine how supervised fine-tuning (SFT) improves scores and instruction-following capabilities under different layer normalization strategies in Transformer architectures, we performed additional training using the LIMA dataset \citep{instructgpt, lima}. Diverged checkpoints were excluded when calculating the evaluation score (mostly occurs in Pre-LN). Additional training hyperparameters for SFT are given in Appendix~\ref{appendix:SFTsetup}. As shown in Table~\ref{tab:pre-train}, Peri-LN consistently demonstrates superior performance across all model sizes. Additionally, we note that, beyond the improved scores, the standard deviation of the benchmark results across different training seeds is reduced by more than half with Peri-LN. From this, we observe that Peri-LN \emph{helps maintain consistency} not only in gradient stability and final loss but also in benchmark performance. For the evaluation loss, we used $10$K random samples from the C$4$ dataset \citep{raffel2020c4}. Detailed configurations and individual scores are provided in Appendix~\ref{appendix:morebenchmarks}. 

\subsection{Systematic Analysis}
Despite emerging empirical evidence that Peri-LN can stabilize training and bolster performance, many open questions remain: \emph{Which} design factors are crucial for reaping its benefits at scale (\S\ref{subsec:frozengamma})? \emph{How} does it regulate hidden-state variance and gradient flow more effectively (\S\ref{subsec:growth of hidden state}, \S\ref{subsec:grad_norm_var})? And \emph{why} might certain variants outperform standard LN placements in large Transformer architectures (\S\ref{subsec:hidden_state_representation})? Addressing these points, we present a systematic analysis of Peri-LN’s mechanics at following sub-sections.


\subsubsection{Learnable Parameter $\gamma$ of RMSNorm} \label{subsec:frozengamma}
To investigate the impact of module output normalization on training stability, as proposed in the Proposition~\ref{prop:theory}, we fixed the learnable parameter $\gamma$ of RMSNorm to $1$, isolating the effect of normalization. As shown in Figure \ref{fig:frozen_gamma}, we observe that simply normalizing the output of each module reduces gradient spikes and provides improvements in the loss comparing to the pre-normalization only transformers. Nonetheless, we also confirmed that allowing $\gamma$ to be learnable yields slightly better performance. We confirm that this trend remains consistent in both the $400$M and $1.5$B models. In this experiment, we omitted Peri-LN's embedding layer normalization in order to isolate and evaluate the precise role and benefits of output layer normalization.

\begin{figure*}[t]
% \vskip -0.1in
    \centering
    \subfigure[At initialization]
    {
    \includegraphics[width=0.2\linewidth]{Figures/angular_distance_between_layers_heatmap_post_400M_iter0_seqlen256_sample256_seed1.png} 
    \includegraphics[width=0.2\linewidth]{Figures/angular_distance_between_layers_heatmap_pre_400M_iter0_seqlen256_sample256_seed1.png} 
    \includegraphics[width=0.258\linewidth]{Figures/angular_distance_between_layers_heatmap_peri_400M_iter0_seqlen256_sample256_seed1.png} 
    \label{fig:pattern_hidden_state_init}
    } 
    \subfigure[Learnable scale $\gamma$ in Output-LN]
    {
    \includegraphics[width=0.27\linewidth]{Figures/mlp_output_ln_gamma_across_layer_dev--hcx-text-400M-inputNorm-dclm-000-30B-warmup10-lr2e3.png} 
    \label{fig:scale_gamma}
    }    
    \subfigure[After $30$B tokens training]
    {
    \includegraphics[width=0.2\linewidth]{Figures/angular_distance_between_layers_heatmap_post_400M_iter14000_seqlen256_sample256_seed1.png}
    \includegraphics[width=0.2\linewidth]{Figures/angular_distance_between_layers_heatmap_pre_400M_iter14000_seqlen256_sample256_seed1.png}
    \includegraphics[width=0.258\linewidth]{Figures/angular_distance_between_layers_heatmap_peri_400M_iter14000_seqlen256_sample256_seed1.png} \label{fig:pattern_hidden_state_fin}
    }
    \subfigure[Frozen $\gamma$ in Output-LN]
    {
    \includegraphics[width=0.258\linewidth]{Figures/angular_distance_between_layers_heatmap_frozengamma_peri_400M_iter14000_seqlen256_sample256_seed1.png} \label{fig:frozen_gamma_pattern}
    }   
    \caption{Angular distance of hidden state is presented in Figure ~\ref{fig:pattern_hidden_state_init},~\ref{fig:pattern_hidden_state_fin}, and~\ref{fig:frozen_gamma_pattern}. In Figure~\ref{fig:scale_gamma}, we monitor $\gamma$ of every Output-LN in Peri-LN during training. We use $30$B tokens trained $400$M size model in this experiments.}
    \label{fig:pattern_hidden_state}
    \vskip -0.1in
\end{figure*}


\subsubsection{Growth of Hidden State} \label{subsec:growth of hidden state}

To examine in greater depth how Peri-LN affects the forward propagation, we analyzed the absolute magnitude and variance of the hidden states using $1,000$ samples from the Wikitext dataset \citep{merity2016pointer}. Figure~\ref{fig:growth_of_hidden_state} illustrates how normalization strategies influence the forward-path hidden states over the course of training.

Two particular aspects of hidden-state behavior were noted: First, with respect to model depth, Post-LN maintains stable hidden-state magnitude and variance owing to the presence of a normalization layer in the main path (“Highway”). By contrast, because Pre-LN does not normalize the outputs of each Attention and MLP module, the magnitude and variance of the hidden states exhibit exponential growth following the addition operation in the residual path. For the Peri-LN architecture, which leverages Output-LN, we observed that hidden states remain comparatively well-managed. Next, regarding changes over training iterations, Post-LN continues to normalize each block’s output, which prevents any substantial increase or decrease in the trend as training progresses. Meanwhile, Pre-LN exhibits a relatively linear variance distribution at initialization, but escalates exponentially to extremely large values over successive iterations. In contrast, Peri-LN fluctuates more moderately, owing to the role of Output-LN in controlling hidden-state magnitude and variance throughout training.

\subsubsection{Layer-wise Gradient Norm \& Variance} \label{subsec:grad_norm_var}
Ensuring a uniform gradient flow in large-scale model training is crucial for balanced learning across the entire network \citep{tensorprogram4, tensorprogram6}. As shown in Figure~\ref{fig:layerwise_gradient}, in Post-LN, gradients decrease as they propagate backward through the layers in the final stage of training, which can lead to vanishing gradients in lower-index layers. In Pre-LN, gradients increase as they propagate backward through the layers at initialization, potentially causing explosive gradients in the early phase of training. Both strategies display non-uniform gradient distributions—either vanishing or exploding—at different stages of training. On the other hand, Peri-LN demonstrates a consistent, layer-wise gradient distribution at both initialization and the end of training. By maintaining comparatively uniform gradients with lower variance across layers, Peri-LN avoids the extremes of vanishing or exploding behaviors. This stability is particularly beneficial in deeper architectures, where balanced gradient flow is essential for effective backpropagation. 

\subsubsection{Hidden State Representation} \label{subsec:hidden_state_representation}
We utilize angular distance \citep{mixln} as a metric to assess redundancy between hidden states at initialization and after training. This approach allows us to quantify how similar or distinct the representations were across layers. As shown in Figure~\ref{fig:pattern_hidden_state_init}, Post-LN exhibits smaller angular distances due to the LN being located on the main path, whereas Pre-LN and Peri-LN begin with very similar states. As shown in Figure~\ref{fig:pattern_hidden_state_fin}, at the end of training, Pre-LN tends to produce more redundant hidden state representations compared to the others. This phenomenon may stem from Pre-LN’s repeated residual additions, which amplify certain representations over others.

To investigate this further, we focus on module output normalization, which is the main distinguishing factor between Pre-LN and Peri-LN. As shown in Figure~\ref{fig:scale_gamma}, the learnable scale starts around $1$ in the early stages of training and gradually changes with increasing depth. Because Peri-LN preserves the identity path, it appears to adjust the scale of the module output accordingly. This suggests that the exponential growth of the main path’s magnitude in Pre-LN diminishes the relative contribution of individual modules, resulting in more redundant hidden representations. To support this, Figure~\ref{fig:frozen_gamma_pattern} shows that fixing the learnable scale of Peri-LN’s module output LN at $1$ causes the main path contribution to decrease in deeper layers. This finding confirms the role of module output normalization in controlling hidden state redundancy.