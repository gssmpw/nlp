\input{figures/fig-rewrite-training-pipeline}

% Given an annotated dataset $\mathcal{D}$, the scarcity of such annotated datasets significantly constrains the development of AI models.
% To address this, we propose a novel pipeline, \pipeline, which leverages LLMs to automatically augment the annotations of $\mathcal{D}$, thereby generating an enhanced dataset $\mathcal{D}^+$ to facilitate the training of AI models. 
% 
To facilitate the practical re-implementation, we showcase details of \pipeline, using \olddataset~\cite{EZJ21}, a widely adopted dataset for molecular generation research (Section~\ref{subsec:preliminary}-\ref{subsec:training}). 
% 
% Section~\ref{subsec:preliminary} provides background knowledge of \olddataset and outlines the preliminary model and tasks. 
% % 
% Following this, Section~\ref{subsec:language_augmentation} details the workflow of \pipeline, highlighting how \pipeline generates \newdataset. 
% Section~\ref{subsec:training} describes the integration of this augmented dataset to train a set of new models, \newmodel, as illustrated in Figure~\ref{fig:rewrite_training_pipeline}.
% % 
In addition, Section~\ref{subsec:extension_broad_application} describes extensive implementations of \pipeline across broad applications in \emph{image}, \emph{text} and \emph{graph} tasks. 

\subsection{Showcasing \olddataset}
\label{subsec:preliminary}

% This paper aims to enhance the molecule generation and captioning tasks with language augmentation. 
% Specifically, we showcase our framework (as illustrated in Figure~\ref{fig:rewrite_training_pipeline}) using \olddataset~\cite{EZJ21}, a freely available dictionary of $33,010$ molecular entities centred on chemical compounds. 
% Each molecule is represented using a SMILES string~\cite{W88} and associated with a high-quality, manually annotated caption supporting various computational and experimental studies.
% This section provides preliminary background knowledge and outlines the tasks and research questions addressed in this paper. 

\spara{\olddataset.}
\olddataset contains $33\,010$ molecular entities centred on chemical compounds. 
Each molecule is represented using a SMILES string~\cite{W88} and associated with a high-quality, manually annotated caption supporting various computational and experimental studies.
% 
Given a molecule, we formally represent it as $\mathcal{M} = (S, C)$, where $S$ and $C$ denote its SMILES string and associated caption. 
Examples are illustrated in Figure~\ref{fig:rewrite_training_pipeline}-(A). 
% 
Consequently, \olddataset can be formally represented as $\mathcal{D} = \{ \mathcal{M}_1, \mathcal{M}_2, \dots, \mathcal{M}_n \}$. 
$\mathcal{S} = \{ S_1, S_2, \dots, S_n \}$ represents the SMILES string set and $\mathcal{C} = \{ C_1, C_2, \dots, C_n \}$ the caption (annotation) set.
% 
The dataset is publicly available with a fixed split: $\mathcal{D}_{\textsc{train}}$ ($80\%$), $\mathcal{D}_{\textsc{valid}}$ ($10\%$) and $\mathcal{D}_{\textsc{test}}$ ($10\%$), allowing researchers to consistently train and evaluate their models.

\spara{Tasks.}
\olddataset supports two molecular tasks: \emph{(1)} text-based \emph{de novo} molecule generation (\textsc{gen}) and \emph{(2)} molecule captioning (\textsc{cap}). 
% 
The goal of text-based \emph{de novo} molecule generation is to train a model which can generate a variety of possible new molecules with desired properties as described in the text. 
% 
Specifically, for \olddataset, we aim to learn a model $f_{\textsc{gen}}: \mathcal{C} \to \hat{\mathcal{S}} $ by minimising the loss function value $\min_{\Psi} \mathcal{L}(\hat{\mathcal{S}}_{\textsc{train}}, \mathcal{S}_{\textsc{train}})$, where $\Psi$ represents the set of trainable parameters of $f_{\textsc{gen}}$. 
% 
The target of molecule captioning is to generate descriptions of the components and chemical functionality of a molecule. 
Similarly, we aim to learn a model $f_{\textsc{cap}}: \mathcal{S} \to \hat{\mathcal{C}} $, by minimising loss function value $\min_{\Phi} \mathcal{L}(\hat{\mathcal{C}}_{\textsc{train}}, \mathcal{C}_{\textsc{train}})$, where $\Phi$ represents the set of trainable parameters of $f_{\textsc{cap}}$. 

\spara{\oldmodel.}
The molecule generation tasks can be considered translation tasks, and sequence-to-sequence models serve as solid solutions. 
One fundamental method in this category is \oldmodel~\cite{ELRHCJ22}, an improved version of T5~\cite{RSRLNMZLL20}. 
% 
\oldmodel initialise an encoder-decoder transformer model using public checkpoints of T5. 
The model is then pre-trained on a combined dataset of C4~\cite{RSRLNMZLL20} and ZINC~\cite{SI15} for $1$ million steps.
Finally, it undergoes $50,000$ steps of fine-tuning on \olddataset for two molecular tasks. 
% 
Since these tasks are formulated as sequence-to-sequence tasks, the model is trained using standard maximum likelihood, such as cross-entropy loss  ($\mathcal{L}_{\text{CE}}$). 
Take the fine-tuning phase as an example. 
The parameters are optimised to match the generated text with $\mathcal{D}_{\textsc{train}}$'s text:
\begin{align}
\begin{split}
    % \mathcal{L}_{\text{CE}}^{\textsc{gen}}(\mathcal{D}_{\textsc{train}}) &= - \frac{\sum\limits_{i = 1}^{n} \log p(S_{\textsc{train}, i} \;\vert\; C_{\textsc{train}, i})}{n}, \\
    % \mathcal{L}_{\text{CE}}^{\textsc{cap}}(\mathcal{D}_{\textsc{train}}) &= - \frac{\sum\limits_{i = 1}^{n} \log p(C_{\textsc{train}, i} \;\vert\; S_{\textsc{train}, i})}{n},
    \mathcal{L}_{\text{CE}}^{\textsc{gen}} &= - \frac{\sum\limits_{i = 1}^{n} \log p(S_{\textsc{train}, i} \;\vert\; C_{\textsc{train}, i})}{n}, \\
    \mathcal{L}_{\text{CE}}^{\textsc{cap}} &= - \frac{\sum\limits_{i = 1}^{n} \log p(C_{\textsc{train}, i} \;\vert\; S_{\textsc{train}, i})}{n},
\end{split}
\label{eq:molt5}
\end{align}
where $n$ is the number of molecules in $\mathcal{D}_{\textsc{train}}$ and $p(S_{\textsc{train}, i} \;\vert\; C_{\textsc{train}, i})$ is the probability assigned by $f_{\textsc{gen}}$ to the $i$-th true SMILES string $S_{\textsc{train}, i}$ given the $i$-th caption $C_{\textsc{train}, i}$. 
% 
This optimisation increases the probability of generating the correct outputs given the corresponding inputs.
% 
Following this way, \oldmodel provides three trained variants of varying sizes: \oldmodel-Small ($77M$ parameters), \oldmodel-Base ($250M$ parameters) and \oldmodel-Large ($800M$ parameters). 

% In this section, we outline the workflow of our methodology to leverage the language augmentation capabilities of LLMs to generate \newdataset and integrate this augmented data during \newmodel training.
% To achieve this goal, we comprise two main steps: 
% (1) Generate Language-Augmented Dataset: Create \newdataset, an extended version of \olddataset, using language augmentation techniques (Section~\ref{subsec:language_augmentation}, as illustrated in Figure~\ref{fig:rewrite_training_pipeline});
% (2) Train New Models: Train a set of new models, \newmodel, based on the augmented dataset \newdataset (Section~\ref{subsec:training}, as illustrated in Figure~\ref{fig:rewrite_training_pipeline}).

\subsection{Automatic Annotation Augmentation}
\label{subsec:language_augmentation}

As shown in Equation~\ref{eq:molt5}, the number of training instances directly affects the amount of information injected into the model. 
\citet{ELRHCJ22} discuss the potential limitations caused by the limited data in \olddataset. 
% To address this, we introduce \pipeline to automatically enhance \olddataset using the language augmentation capabilities of LLMs.
% 
A recent breakthrough known as In-Context Learning (ICL) has enhanced the adaptability of LLMs by enabling them to acquire contextual knowledge during inference, eliminating the need for extensive fine-tuning~\cite{CTR20}. 
To harness ICL for \olddataset augmentation, we first formulate a prompt for querying LLMs.
% 
The goal in prompt engineering is to find the correct way to formulate a question $Q$ in such a way that an LLM ($f_{\textsc{LLM}}$) will return the corresponding answer $A$ essentially represented as $A = f_{\textsc{LLM}}(Q)$. 
% 
In this work, we design the prompt as shown in Appendix~\ref{sec:appendix_prompt_details}. 
% 
Precisely, the prompt consists of two components: \emph{Instruction}: Provides general guidance to the LLM, clarifying its role in the conversation; \emph{Message}: Tasks the LLM to rewrite the molecule caption, considering the chemical expertise and given information.

Given an instance $\mathcal{M}_i = (S_i, C_i)$ from $\mathcal{D}_{\textsc{train}}$, we can generate $k$ rewritten captions $\{ C_{i, 1}, C_{i, 2}, \dots, C_{i, k} \}$ with multiple rounds of queries. 
This results in an augmented instance, $\mathcal{M}^{+}_i = (S_i, C_{i, 0}, C_{i, 1}, C_{i, 2} \dots, C_{i, k})$, and an augmented dataset, \newdataset ($\mathcal{D}^{+}$).
Specifically, $\mathcal{D}^{+} = (\mathcal{D}^{+}_{\textsc{train}}, \mathcal{D}_{\textsc{valid}}, \mathcal{D}_{\textsc{test}})$, where $\mathcal{D}^{+}_{\textsc{train}} = \{ \mathcal{M}^+_1, \mathcal{M}^+_2, \dots \}$.
Each SMILES string of $\mathcal{D}^{+}_{\textsc{train}}$ is associated with $k+1$ captions. 
This language augmentation process introduces diversity in sentence structure and vocabulary while preserving the essential knowledge about the molecules.
% 
In our experiments, we adopt two open-source LLMs (Llama 2-70b~\cite{TMSA23} and Llama 3-70b~\cite{TMSA23}) and two closed-source LLMs (GPT 3.5-turbo~\cite{AAAA23} and Gemini Pro~\cite{Gemini24}) to generate four rewritten captions. 
We demonstrate some generated example captions in Table~\ref{table:rewritten_captions} of Appendix~\ref{sec:appendix_caption_details}. 
% More examples can be found in Appendix~\ref{sec:appendix_caption_details}. 

\subsection{Training on Augmented Dataset}
\label{subsec:training}

% \input{figures/fig-training-pipeline}

After generating $k$ new captions for each molecule of the training dataset $\mathcal{D}_{\textsc{train}}$. 
We proceed to train a model based on \newdataset to perform the molecular tasks, \ie, text-based \emph{de novo} molecule generation (\textsc{gen}) and molecule captioning (\textsc{cap}). 
% 
In this work, we initialise encoder-decoder transformer models using the available \oldmodel variants, as introduced in Section~\ref{subsec:preliminary}. 
We then train a novel model, \newmodel, using a cross-entropy loss:
\begin{align}
\begin{split}
    % \mathcal{L}_{\text{CE}}^{\textsc{gen}}(\mathcal{D}^{\textcolor{orange}{+}}_{\textsc{train}}) &= - \frac{\sum\limits_{i = 1}^{n} \textcolor{orange}{\sum\limits_{j = 1}^{k+1}} \log p(S_{\textsc{train}, i} \vert C^{\textcolor{orange}{+}}_{\textsc{train}, i, \textcolor{orange}{j}})}{\textcolor{orange}{(k+1)}n},\\
    % \mathcal{L}_{\text{CE}}^{\textsc{cap}}(\mathcal{D}^{\textcolor{orange}{+}}_{\textsc{train}}) &= - \frac{\sum\limits_{i = 1}^{n} \textcolor{orange}{\sum\limits_{j = 1}^{k+1}} \log p(C^{\textcolor{orange}{+}}_{\textsc{train}, i, \textcolor{orange}{j}} \vert S_{\textsc{train}, i})}{\textcolor{orange}{(k+1)}n},
    \mathcal{L}_{\text{CE}}^{\textsc{gen}} &= - \frac{\sum\limits_{i = 1}^{n} \textcolor{orange}{\sum\limits_{j = 1}^{k+1}} \log p(S_{\textsc{train}, i} \vert C^{\textcolor{orange}{+}}_{\textsc{train}, i, \textcolor{orange}{j}})}{\textcolor{orange}{(k+1)}n},\\
    \mathcal{L}_{\text{CE}}^{\textsc{cap}} &= - \frac{\sum\limits_{i = 1}^{n} \textcolor{orange}{\sum\limits_{j = 1}^{k+1}} \log p(C^{\textcolor{orange}{+}}_{\textsc{train}, i, \textcolor{orange}{j}} \vert S_{\textsc{train}, i})}{\textcolor{orange}{(k+1)}n},
\end{split}
\label{eq:lamolt5}
\end{align}
where $p(S_{\textsc{train}, i} \;\vert\; C^+_{\textsc{train}, i, j})$ is the probability assigned by $f_{\textsc{gen}}$ to the $i$-th true SMILES string $S_{\textsc{train}, i}$ given the $i$-th molecule's $j$-th caption $C_{\textsc{train}, i, j}$. 
% 
The critical addition to the \oldmodel is the augmented caption set $C^+_{\textsc{train}, i, \{0, 1, \dots, k \}}$ for each SMILES string $S_{\textsc{train}, i}$. 
For the molecule generation task, \newmodel is trained to generate the correct SMILES string $S_{\textsc{train}, i}$ by giving different caption inputs $C^+_{\textsc{train}, i, \{0, 1, \dots, k \}}$. 
By introducing diversity into the training data, we enhance the effectiveness and robustness of the model in generating SMILES strings (validated in Section~\ref{subsec:molecule_generation}). 
% 
Meanwhile, for the molecule captioning task, \newmodel is trained to generate various captions for each SMILES string. 
Despite these captions having different sentence structures and vocabularies, they preserve the essential knowledge about the molecules.
This training enhances the model’s ability to generate more semantically precise and meaningful captions (validated in Section~\ref{subsec:molecule_captioning}). 
% 
Overall, this novel and effective methodology augments the biomedical dataset without requiring human efforts, significantly contributing to the performance of \newmodel.

\subsection{Extension to Broad Applications}
\label{subsec:extension_broad_application}
% Section~\ref{subsec:preliminary} and Section~\ref{subsec:language_augmentation} demonstrate a detailed example implementation of \pipeline on the \olddataset for challenging molecular generation tasks. 
To further demonstrate the versatility of \pipeline, we extend its application to several additional datasets, including \bace, \hiv, \esol~\cite{HFZDRLCL20}, and \image~\cite{SDGS18}. 
These datasets support a variety of crucial tasks, \eg, \emph{image captioning}, \emph{text understanding}, and \emph{graph property prediction}.
% The implementation of \pipeline on these additional datasets showcases its ability to enhance annotations across different domains, thereby facilitating the training of more effective AI models. 
% 
Due to the paper length constraints, the detailed implementations of \newmodel on these additional datasets are presented in Appendix~\ref{sec:appendix_additional_experiments}. 
Experimental results derived from these implementations will be discussed in Section~\ref{subsec:broad_application}. 
