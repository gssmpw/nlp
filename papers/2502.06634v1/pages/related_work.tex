\spara{Molecule Language Models.} 
MLMs have recently seen significant advancements, leveraging NLP techniques to understand and generate molecules. 
Early works such as ChemBERTa~\cite{CGR20} and Text2Mol~\cite{EZJ21} adapt transformer-based architectures for molecular representation learning. 
MolGPT~\cite{BAVP22} and \oldmodel~\cite{ELRHCJ22} demonstrate the ability to predict molecular properties and generate novel compounds, highlighting the potential of language models in biological research. 
% 
However, the effectiveness of MLMs is often constrained by the limited availability of annotated molecular data.
Meanwhile, manual molecular data annotation is often a costly and time-consuming process, necessitating specialised equipment and extensive human labour~\cite{DGH16}. 

\spara{Data Augmentation in MLMs.}
Data augmentation has emerged as a critical strategy to address the scarcity of high-quality datasets.
% 
Take the following studies on \olddataset, \citet{CGBWLM23}, \citet{LZXWXQZL23}, \citet{PZZWGWXY23} and \citet{PWGLFZXQY24} introduce additional chemical databases (PubChem~\cite{KCCGHHLSTY23}, Drugbank~\cite{WFGLMGSJLS18}, UniProt~\cite{U23}, PubMed~\cite{W20}, \etc) as to enrich model with extra knowledge, and design auxiliary tasks to train advanced models. 
However, these methods depend on supplemental datasets and the domain expertise required to shape the tasks.
% 
On the other hand, LLMs have experienced exponential growth in both size and capability in recent years~\cite{BMRS20}. 
A wide range of NLP applications have been reshaped by LLMs~\cite{AMAV23,ZZM242}.
Notably, MolReGPT~\cite{LLFWLTL24} leverage the built-in general human knowledge of LLMs to perform the molecule-caption translation tasks. 
% Nevertheless, different knowledge retrieval strategies may significantly affect the effectiveness. 
Despite the widespread use of LLMs for tasks like literature review and table interpretation \cite{AAAA23,TMSA23}, their application in biology remains challenging \cite{LJRHHNPWR24}. The effectiveness of MolReGPT is highly dependent on the chosen retrieval strategy, and it is often outperformed by smaller, existing models.
 
In another related work, \citet{WYHYMW24} utilise LLMs and human annotators to augment text for each image to improve contrastive learning of CLIP~\cite{RKHRGASAMCKS21}, yet these pipelines still require human supervision.
% 
Furthermore, \citet{ZZM24} reveal the limited capability of LLMs in understanding domain-specific data, \eg, biology and physics, making LLM-based synthetic data generation challenging in many applications (see our analysis in Section~\ref{subsec:analysis}). 
% 
In contrast, our novel \pipeline pipeline is fully automated and tailored for domain-specific applications where data scarcity limits the effectiveness of current technologies.
