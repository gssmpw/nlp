\subsection{Experimental Settings}
\label{subsec:experimental_settings}

\para{Dataset.}
We use our generated \newdataset dataset for training \newmodel.
One annotation is the original present in \olddataset, and the other two are LLM-generated.
We adopt two conventional LLMs, GPT 3.5-turbo and Gemini Pro, to generate annotations in the main dataset from most experiments. 
We additionally experiment in Section~\ref{subsec:analysis} with annotations generated by two open-source LLMs, \eg, Llama 2-70b and Llama 3-70b. 

\spara{Baselines.}
We mainly consider three families of methods: 
\textbf{(1)} Methods included in the benchmark paper~\cite{ELRHCJ22}, including RNN~\cite{CMGBBSB14}, Transformer~\cite{VSPUJGKP17}, T5~\cite{RSRLNMZLL20} and \oldmodel~\cite{ELRHCJ22}. 
\textbf{(2)} The state-of-the-art methods (reported on the leaderboards~\cite{denovo,captioning}) without additional datasets, including Text+Chem T5~\cite{CGBWLM23}, TGM-DLM~\cite{GLWW24} and MolReGPT~\cite{LLFWLTL24}. 
They rely on the same datasets as \newmodel, we report their results in Table~\ref{table:results_mol_generation}, \ref{table:results_mol_caption}
\textbf{(3)} The state-of-the-art methods incorporating extra datasets. 
For the text-based \emph{de novo} molecule generation task, we include BioT5~\cite{PZZWGWXY23} and BioT5+~\cite{PWGLFZXQY24}; for the molecule captioning task, we include BioT5, MolXPT~\cite{LZXWXQZL23}. 
For a fair comparison, we report their results in Figure~\ref{fig:compare_sota}. 
% 
A detailed description of each baseline method can be found in Appendix~\ref{sec:appendix_baseline}.

\spara{Training Setup.}
We train \newmodel-Small and -Base for as little as $1500$ epochs and \newmodel-Large for $200$ epochs. 
This project used $\sim11500$ H100 GPU hours. 
Detailed hyper-parameter settings and checkpoints are available online\footnote{The augmented dataset and trained models are available at \url{https://anonymous.4open.science/r/LaMolT5-D3C3}.}. 

\spara{Evaluation Setup.}
We adopt the benchmark evaluation setup following~\cite{ELRHCJ22}. 
For the text-based \emph{de novo} molecule generation task, we employ: SMILES \emph{BLEU} score~\cite{PRWZ02}, \emph{Levenshtein} distance~\cite{MVM09}, Fréchet ChemNet Distance (\emph{FCD})~\cite{PRUHK18}, \emph{MACCS FTS}~\cite{DLHN02}, \emph{RDK FTS}~\cite{SSL15} \emph{Morgan FTS}~\cite{RH10}, \emph{Exact} score~\cite{ELRHCJ22}, \emph{Validity}~\cite{ELRHCJ22}, and \emph{Text2Mol}~\cite{EZJ21}. 
For the molecule captioning task, we measure the \emph{BLEU} score, \emph{ROUGE}~\cite{L04}, \emph{METEOR}~\cite{BL05}, and \emph{Text2Mol} of the generated annotation compared to the ground-truth. 
Detailed descriptions can be found in Appendix~\ref{sec:appendix_evaluation}.

\subsection{Molecule Generation}
\label{subsec:molecule_generation}

\input{tables/table-results-mol-generation}
\input{tables/table-example-mol-generation}

\para{Observation 1: \newmodel significantly elevates the performance of \oldmodel.} 
The \emph{de novo} molecule generation results across nine evaluation metrics in Table~\ref{table:results_mol_generation} reveal that \newmodel achieves the highest performance on all measures.
In addition, \newmodel consistently delivers substantial enhancements over \oldmodel, with improvements up to $301\%$ in terms of Exact score, which measures the number of times the output corresponds to the ground truth. 
This consistent and notable performance underscores the effectiveness of \newdataset and \newmodel. 
Moreover, it illustrates the efficacy of our automatic annotation-augmentation pipeline, \pipeline, on biomedical datasets.

\spara{Observation 2: The small $77M$ parameters \newmodel  outperforms the $800M$ \oldmodel.}
The results of Table~\ref{table:results_mol_generation} indicate that the small-size variant of \newmodel ($77M$) outperforms the large-size variant of \oldmodel ($800M$) in seven different evaluation metrics (Levenshtein, MACCS FTS, RDK FTS, Morgan FTS, FCD, Text2Mo, Validity) on the molecule generation task.
On the other two evaluation metrics (BLEU and Exact), \newmodel-Small achieves competitive performance compared to \oldmodel-Large. 
\newmodel achieves impressive results by leveraging the annotation-augmented dataset, \newdataset, which introduces diversity in sentence structure and vocabulary while maintaining the core molecular knowledge. 

\subsection{Molecule Captioning}
\label{subsec:molecule_captioning}

\input{tables/table-results-mol-caption}
\input{tables/table-example-mol-caption}

\spara{Observation 3: \newmodel generates coherent descriptions.}
The results in Table~\ref{table:results_mol_caption} highlight the superior performance of the \newmodel in the molecule captioning task. 
\newmodel excels in the Text2Mol metric, which provides a comprehensive assessment of the \textit{semantic alignment} ---throuch cosine similarity-- between generated descriptions and their corresponding molecules.
% Text2Mol provides a comprehensive assessment of model effectiveness through cosine similarity between ground truth and generated descriptions. 
\newmodel variants achieve improvements up to $23\%$ in \newmodel-Small over the corresponding \oldmodel variants. 
These results underscore \newmodel’s enhanced ability to capture the intricate semantics of molecule descriptions, making it a highly effective model for this task.

\spara{Observation 4: \newmodel exhibits lower ROUGE score.}
\newmodel exhibits lower ROUGE scores than \oldmodel, as ROUGE emphasises exact n-gram overlaps, which do not fully capture the semantic accuracy of the generated text. 
The \newdataset, by introducing variability in sentence structure and vocabulary, contributes to this discrepancy. 
During training, \newmodel prioritises achieving higher semantic coherence, potentially at the expense of exact word or phrase matches. 
As we argue above, although \newmodel excels in capturing the overall semantics of molecule descriptions, this focus leads to reduced performance on ROUGE scores.


\subsection{Analysis}
\label{subsec:analysis}

\input{figures/fig-compare-sota}

\para{Comparison with state-of-the-art methods.}
Figure~\ref{fig:compare_sota} demonstrates the performance of top-$3$ leaderboard SOTA models.
\newmodel variants take the first two overall ranks. 
Notably, \newmodel establishes new SOTA results on molecule generation task. 
Although BioT5 outperforms \newmodel on the molecule captioning task, \emph{BioT5 leverages external data}, offering an additional advantage. 
When comparing models that do not incorporate external knowledge, \newmodel achieves the best performance, solidifying its position as the top model in this domain. 
% 
Moreover, the small-size variant delivers highly competitive results with significantly fewer parameters than other leading models: \newmodel-Small has $99\%$ fewer parameters than MolRecGPT but delivers superior performance. 
This efficiency makes \newmodel-Small an attractive option for applications requiring high performance with reduced computational resources.

% \input{figures/figure-ablation}

\spara{Performance with different augmentations.} 
Figure~\ref{fig:ablation} shows the performance of \newmodel with different augmentation strategies during training. 
\emph{(i)} The combination of two augmented annotations generated by \pipeline demonstrates a consistent improvement to using only one augmentation in performance, leading to more robust learning. 
\emph{(ii)} Conventional data augmentation strategies, \eg, EDA~\cite{WZ19}, Mixup~\cite{ZCDL18} are not feasible solutions in the context of biological data. 
We argue that the pre-training stage involved general knowledge about molecules, and easy text wrapping does not provide enough diversity to enhance the information. 
\emph{(iii)} Relying on captions generated directly by LLMs not only fails to improve performance but actually degrades it.
Specifically, including these directly generated annotations results in lower BLEU scores than the original \oldmodel, underscoring that such annotations might introduce noise or lack the molecular knowledge needed for effective training.
Such results echo the empirical studies of \cite{ZZM24} and affirm the necessity of \pipeline design. 

\input{figures/fig-different-captions}

\spara{Performance of \pipeline using different LLMs.}
Figure~\ref{fig:performance_diff_llms} show the molecule generation performance based on annotations augmented by different LLMs. 
% 
\newmodel trained on \newdataset annotations generated by both open- and closed-source LLMs consistently outperforms \oldmodel. 
This highlights the versatility of our proposed annotation augmentation pipeline, \pipeline, in practical applications. 
% 
Expectedly, \newmodel trained on closed-source LLMs (GPT 3.5-turbo + Gemini Pro) outperforms the one trained on open-source LLMs (Llama 2-70b + Llama 3-70b). 
As LLMs continue to improve in performance and in-context learning capabilities, \newmodel can benefit directly from these advancements. 

\subsection{Broad Applications}
\label{subsec:broad_application}

\input{tables/table-results-more-dataset}

\spara{Results of \emph{image}, \emph{text} and \emph{graph} tasks.}
To further demonstrate the versatility of \pipeline, we perform extended experiments on several additional datasets, including \bace, \hiv, \esol, and \image, which support a variety of crucial tasks, \eg, \emph{image captioning}, \emph{text understanding}, and \emph{graph property prediction}.
Due to the page limit, we describe the detailed implementations in Appendix~\ref{sec:appendix_additional_experiments}. 
Results in Table~\ref{table:results_more_dataset} show that \pipeline significantly enhances performance across these diverse applications. 
This improvement highlights \pipeline’s potential to be a valuable tool in a wide range of AI tasks, offering substantial gains in accuracy and efficiency.
