\section{Proposed Prompt Details}
\label{sec:appendix_prompt_details}

In this work, we design the prompt as follows:
\begin{mybox}
    \textbf{Instruction: }
    You are now a chemical specialist in rewriting captions for a molecule in SMILES format. Make sure those captions describe the given molecule correctly and precisely based on your two inputs (SMILES and Caption of it). 
    Also, make sure your rewriting captions do not include the input SMILES.
    Write the response without using linebreaks, newlines, or special characters such as ``\textbackslash t'' or ``\textbackslash n''.
    % 
    \\
    \textbf{Message: }
    \emph{SMILES string of target molecule:} [SMILES]. \\
    \emph{Description of the molecule:} [caption]. \\
    \emph{Task:} Rewrite the following molecule with its SMILES and caption. 
    The newly rewritten caption should be elaborate, descriptive, and concise, highlighting the key structural features and biological activities of the molecule. Only output rewritten caption without any header and linebreak. \\
    \emph{Answer:}
\end{mybox}

\section{Caption Augmentation Details} 
\label{sec:appendix_caption_details}

\input{tables/table-rewrite-caption}

This section presents some examples of augmented captions. 
Particularly, we provide the SMILES string, the original caption of \olddataset and four different captions rewritten by four LLMs, \ie, two open-source LLMs (Llama 2-70b~\cite{TMSA23} and Llama 3-70b~\cite{TMSA23}) and two closed-source LLMs (GPT 3.5-turbo~\cite{AAAA23} and Gemini Pro~\cite{Gemini24}). 

\begin{enumerate}[leftmargin=*]\itemsep0em
    \item 
    \textbf{SMILES string:} [18FH] \\
    \textbf{Original caption:} The molecule is the radioactive isotope of fluorine with relative atomic mass 18.000938. The longest-lived fluorine radionuclide with half-life of 109.77 min. \\
    \textbf{GPT 3.5-turbo:} The molecule is the radioactive isotope of fluorine with relative atomic mass 18.000938 and atomic number 9. It is the longest-lived fluorine radionuclide with half-life of 109.77 min. It is used in positron emission tomography (PET) for imaging the brain, heart, and other organs. \\
    \textbf{Gemini Pro:} This molecule is a radioactive isotope of fluorine with a relative atomic mass of 18.000938. It is the longest-lived fluorine radionuclide with a half-life of 109.77 minutes. \\
    \textbf{Llama 2-70b:} This molecule is a radioactive isotope of fluorine, with a relative atomic mass of 18.000938. It is the longest-lived fluorine radionuclide, with a half-life of 109.77 minutes. It has a number of applications in medical imaging and research, including positron emission tomography (PET) scans and fluorodeoxyglucose (FDG) tracers. \\
    \textbf{Llama 3-70b:} This molecule is fluorine-18 a radioactive isotope of fluorine with a relative atomic mass of 18.000938 and a half-life of 109.77 minutes making it the longest-lived fluorine radionuclide. \\
    % 
    \item 
    \textbf{SMILES string:} CCN(CC)CCN1C(=O)CN=C\\(C2=C1C=CC(=C2)Cl)C3=CC=CC=C3F \\
    \textbf{Original caption:} The molecule is a 1,4-benzodiazepinone that is 1,3-dihydro-2H-1,4-benzodiazepin-2-one substituted by a 2-(diethylamino)ethyl group, 2-fluorophenyl group and chloro group at positions 1, 5 and 7, respectively. It is a partial agonist of GABAA receptors and used for the treatment of insomnia. It has a role as a sedati gve, an anticonvulsant, a GABAA receptor agonist and an anxiolytic drug. It is a 1,4-benzodiazepinone, an organochlorine compound, a member of monofluorobenzenes and a tertiary amino compound. \\
    \textbf{GPT 3.5-turbo:} A 1,4-benzodiazepinone with a 1,3-dihydro-2H-1,4-benzodiazepin-2-one core, this molecule features a 2-(diethylamino)ethyl group, 2-fluorophenyl group, and chloro group at positions 1, 5, and 7, respectively. As a partial agonist of GABAA receptors, it exhibits sedative, anticonvulsant, and anxiolytic effects. Its therapeutic applications include the treatment of insomnia, making it a valuable pharmaceutical agent in managing sleep disorders. \\
    \textbf{Gemini Pro:} The molecule is a 1,4-benzodiazepinone derivative with a 2-(diethylamino)ethyl side chain and substitutions of a 2-fluorophenyl group at position 5 and a chloro group at position 7. Known for its partial agonist activity on GABAA receptors, it is commonly used as a sedative, anticonvulsant, and anxiolytic agent for the treatment of insomnia. Its chemical structure includes a diazepine ring fused to a benzene ring with specific substituents, making it a potent therapeutic compound with diverse pharmacological properties. \\
    \textbf{Llama 2-70b:} This molecule is a 1,4-benzodiazepinone, a type of sedative drug that selectively activates GABAA receptors. It has a 2-(diethylamino)ethyl group, 2-fluorophenyl group, and chloro group attached at positions 1, 5, and 7, respectively. Its role as a partial agonist of GABAA receptors makes it useful for treating insomnia, and it also has anticonvulsant, anxiolytic, and sedative properties. This compound belongs to the class of organochlorine compounds and is a tertiary amino compound. \\
    \textbf{Llama 3-70b:} This molecule is a 1,4-benzodiazepinone featuring a 2-(diethylamino)ethyl substituent at position 1, a 2-fluorophenyl group at position 5, and a chloro group at position 7, functioning as a partial GABAA receptor agonist used to treat insomnia, and possessing sedative, anticonvulsant, and anxiolytic properties. \\
    % 
    \item
    \textbf{SMILES string:} C1=CC=C2C(=C1)C(=CN2)\\C3=NC=CS3 \\
    \textbf{Original caption:} The molecule is an indole phytoalexin that is indole substituted at position 3 by a 1,3-thiazol-2-yl group. It has a role as a metabolite. It is an indole phytoalexin and a member of 1,3-thiazoles. \\
    \textbf{GPT 3.5-turbo:} Indole phytoalexin with a 1,3-thiazol-2-yl substitution at position 3. It plays a role as a metabolite and exhibits indole phytoalexin and 1,3-thiazole properties. \\
    \textbf{Gemini Pro:} The molecule is a biologically active compound containing an indole ring substituted at position 3 with a 1,3-thiazol-2-yl group. It acts as an indole phytoalexin and plays a role as a metabolite. Its structure consists of fused aromatic rings and a sulfur-containing heterocycle, making it a unique and potentially potent agent in biological systems. \\
    \textbf{Llama 2-70b:} This indole phytoalexin features a unique molecular structure where an indole ring is substituted at position 3 by a 1,3-thiazol-2-yl group, resulting in a distinct chemical compound with significant biological activity. As a metabolite, it plays a crucial role in various physiological processes and is also a member of the 1,3-thiazole class of compounds, which are characterized by their sulfur-containing heterocyclic ring. \\
    \textbf{Llama 3-70b:} This molecule is a substituted indole featuring a 1,3-thiazol-2-yl group at position 3, classified as an indole phytoalexin and a metabolite, also belonging to the class of 1,3-thiazoles. \\
    % 
    \item 
    \textbf{SMILES string:} CN(CC(=O)O)N=O \\
    \textbf{Original caption:} The molecule is a nitrosamine that is sarcosine in which the hydrogen attached to the nitrogen has been replaced by a nitroso group. It has a role as a carcinogenic agent. It is a non-proteinogenic amino acid derivative and a nitrosamine. It derives from a sarcosine. \\
    \textbf{GPT 3.5-turbo:} The molecule is a nitrosamine that is sarcosine in which the hydrogen attached to the nitrogen has been replaced by a nitroso group. It has a role as a carcinogenic agent. It is a non-proteinogenic amino acid derivative and a nitrosamine. It derives from a sarcosine.	This molecule is a nitrosamine containing a nitroso group attached to a sarcosine derivative. It is known for its carcinogenic properties and is classified as a non-proteinogenic amino acid derivative. \\
    \textbf{Gemini Pro:} Sarcosine N-nitrosamide, a nitrosamine with a nitroso group replacing the hydrogen atom on the nitrogen atom of the sarcosine molecule, exerts carcinogenic effects. This non-proteinogenic amino acid derivative exhibits a potential role as a carcinogen and is derived from sarcosine. \\
    \textbf{Llama 2-70b:} This nitrosamine is a modified form of sarcosine, where the hydrogen atom attached to the nitrogen has been replaced by a nitroso group, resulting in a distinct chemical structure. As a non-proteinogenic amino acid derivative, it exhibits unique properties and functions, and its presence has been linked to carcinogenic effects. \\
    \textbf{Llama 3-70b:} This molecule is a carcinogenic nitrosamine derived from sarcosine where a nitroso group replaces the hydrogen attached to the nitrogen atom making it a non-proteinogenic amino acid derivative. \\
    % 
    \item 
    \textbf{SMILES string:} CC(=O)N[C@@H]1[C@H]\\([C@@H]([C@H](O[C@H]1O)CO)O[C@@H]\\2[C@@H]([C@H]([C@@H]([C@H](O2)CO)\\O)O[C@@H]3[C@H]([C@H]([C@@H]([C@\\H](O3)CO)O)O)O)NC(=O)C)O \\
    \textbf{Original caption:} The molecule is an amino trisaccharide consisting of alpha-D-mannopyranose, 2-acetamido-2-deoxy-alpha-D-glucopyranose and 2-acetamido-2-deoxy-beta-D-glucopyranose residues joined in sequence by (1->3) and (1->4) glycosidic bonds. It is a glucosamine oligosaccharide, an amino trisaccharide and a member of acetamides. It derives from an alpha-D-Manp-(1->3)-alpha-D-GlcpNAc and an alpha-D-GlcpNAc-(1->4)-beta-D-GlcpNAc. \\
    \textbf{GPT 3.5-turbo:} The molecule is an amino trisaccharide consisting of alpha-D-mannopyranose, 2-acetamido-2-deoxy-alpha-D-glucopyranose and 2-acetamido-2-deoxy-beta-D-glucopyranose residues joined in sequence by (1->3) and (1->4) glycosidic bonds. It is a glucosamine oligosaccharide, an amino trisaccharide and a member of acetamides. It derives from an alpha-D-Manp-(1->3)-alpha-D-GlcpNAc and an alpha-D-GlcpNAc-(1->4)-beta-D-GlcpNAc.	The molecule is a complex amino trisaccharide composed of alpha-D-mannopyranose, 2-acetamido-2-deoxy-alpha-D-glucopyranose, and 2-acetamido-2-deoxy-beta-D-glucopyranose residues linked together via (1->3) and (1->4) glycosidic bonds. This glucosamine oligosaccharide is a member of acetamides and plays a crucial role in various biological processes due to its unique sugar composition and glycosidic linkages. \\
    \textbf{Gemini Pro:} This molecule is an amino trisaccharide composed of an alpha-D-mannopyranose unit, a 2-acetamido-2-deoxy-alpha-D-glucopyranose unit, and a 2-acetamido-2-deoxy-beta-D-glucopyranose unit. These units are linked by (1->3) and (1->4) glycosidic bonds. This amino trisaccharide is a member of the acetamides family and is derived from an alpha-D-Manp-(1->3)-alpha-D-GlcpNAc and an alpha-D-GlcpNAc-(1->4)-beta-D-GlcpNAc. \\
    \textbf{Llama 2-70b:} This molecule is a glucosamine oligosaccharide composed of three residues: alpha-D-mannopyranose, 2-acetamido-2-deoxy-alpha-D-glucopyranose, and 2-acetamido-2-deoxy-beta-D-glucopyranose. These residues are joined in sequence by (1->3) and (1->4) glycosidic bonds. It is an amino trisaccharide, derived from an alpha-D-Manp-(1->3)-alpha-D-GlcpNAc and an alpha-D-GlcpNAc-(1->4)-beta-D-GlcpNAc. \\
    \textbf{Llama 3-70b:} This molecule is a trisaccharide composed of alpha-D-mannopyranose, 2-acetamido-2-deoxy-alpha-D-glucopyranose, and 2-acetamido-2-deoxy-beta-D-glucopyranose residues linked by (1->3) and (1->4) glycosidic bonds, classified as a glucosamine oligosaccharide and an acetamide derivative. \\
\end{enumerate}

\section{Evaluation Setup}
\label{sec:appendix_evaluation}

Following the benchmark settings of \olddataset, we train \newmodel on the training dataset of \newdataset and evaluate it on the test dataset. 
% 
Since we are considering two molecular tasks: text-based \emph{de novo} molecule generation and molecule captioning, we employ two evaluation metric sets. 
 
To evaluate the molecule generation task, we employ eight metrics following previous work~\cite{ELRHCJ22}: SMILES \textbf{BLEU} score~\cite{PRWZ02}, \textbf{Levenshtein} distance~\cite{MVM09}, Fréchet ChemNet Distance (\textbf{FCD})~\cite{PRUHK18}, \textbf{MACCS FTS}~\cite{DLHN02}, \textbf{RDK FTS}~\cite{SSL15} \textbf{Morgan FTS}~\cite{RH10}, \textbf{Exact} score~\cite{ELRHCJ22}, and \textbf{Validity}~\cite{ELRHCJ22}.
% 
Notably, there are three fingerprint metrics: MACCS FTS, RDK FTS and Morgan FTS. 
FTS stands for fingerprint Tanimoto similarity. 
MACCS, RDK, and Morgan are each fingerprinting methods for molecules. 
The fingerprints of two molecules are compared using Tanimoto similarity, and the average similarity over the evaluation matrix is reported. 
Additionally, we report exact SMILES string matches \ie, Levenshtein distance and SMILES BLEU score. 
Exact score and Validity are the percentage of generated molecules that exactly match the ground truth and the percentage of generated strings that are valid.

To evaluate the molecule captioning task, we employ three natural language generation metrics, \eg, Caption \textbf{BLEU} score~\cite{PRWZ02}, \textbf{ROUGE}~\cite{L04}, and \textbf{METEOR}~\cite{BL05}. 
BLEU measures the precision of n-grams between generated and reference texts, ROUGE evaluates recall and precision of overlapping units such as n-grams or word sequences, and METEOR combines precision, recall, and synonym matching for a more holistic evaluation of text generation quality.

Furthermore, the cross-modal evaluation metric \textbf{Text2Mol}~\cite{EZJ21} aims to train a retrieval model to rank molecules given their text descriptions. 
Unlike traditional metrics that rely on words or n-grams, the ranking function of Text2Mol uses cosine similarity between the ground truth molecule/description and the generated description/molecule, respectively. 
It can offer a more integrated assessment to measure the comprehensive semantics of molecule/description. 
Therefore, we adopt this metric as an essential assessment to understand the effectiveness of different models. 

\section{Baseline Models Description}
\label{sec:appendix_baseline}

This section presents brief descriptions of baseline models included in this work. 

\spara{RNN}~\cite{CMGBBSB14}.
It introduces a novel approach for improving statistical machine translation through the use of Recurrent Neural Networks (RNNs). 
They propose an encoder-decoder architecture that learns continuous-space representations for phrases. The encoder processes an input phrase and compresses it into a fixed-dimensional vector, while the decoder uses this vector to generate the target phrase. 
This method allows for better handling of variable-length input sequences and capturing long-term dependencies in phrases, leading to significant improvements in translation quality compared to traditional models.

\spara{Transformer}~\cite{VSPUJGKP17}.
It introduces the Transformer model, a novel neural network architecture designed for sequence transduction tasks, such as machine translation. 
The Transformer model relies entirely on attention mechanisms to capture dependencies between input and output without using recurrent or convolutional layers. 
This self-attention mechanism allows for greater parallelization and better handling of long-range dependencies compared to previous models. 
They demonstrate that the Transformer achieves state-of-the-art performance on translation tasks, significantly improving both training speed and translation quality. 

\spara{T5}~\cite{RSRLNMZLL20}.
It presents the Text-to-Text Transfer Transformer (T5), a model designed to unify various NLP tasks by converting all tasks into a text-to-text format. 
They explore the capabilities of transfer learning within this framework, demonstrating that the same model, training objective, hyperparameters, and architecture can be applied across a wide range of NLP tasks. 
By pre-training on a massive and diverse dataset and fine-tuning specific tasks, T5 achieves state-of-the-art performance on numerous benchmarks. 
Additionally, they propose the \olddataset dataset. 

\spara{\oldmodel}~\cite{ELRHCJ22}.
It explores the novel concept of bridging the gap between molecular representations and natural language descriptions. 
They propose a model, \oldmodel, that translates molecular structures into textual descriptions and vice versa. 
This interdisciplinary approach leverages advances in natural language processing and cheminformatics, using techniques such as neural networks to encode and decode information between these two domains. 
\oldmodel is the fundamental model that motivates our work. 

\spara{Text+Chem T5}~\cite{CGBWLM23}. 
Text+Chem T5 is a novel multi-task, multi-domain language model designed to bridge the gap between natural language and chemical language tasks. 
Built on the T5 architecture, it is specifically designed to handle tasks spanning both textual and chemical domains. 
This model can effectively translate between natural and chemical languages, enabling it to perform a variety of tasks such as chemical reaction prediction (forward and retrosynthesis), text-conditional de novo molecule generation, molecular captioning, and paragraph-to-action conversion for chemical procedures.

\spara{TGM-DLM}~\cite{GLWW24}. 
TGM-DLM employs a Transformer-based architecture with cross-attention to incorporate textual guidance. 
It is trained using two objectives: denoising embeddings with text guidance and recovering uncorrupted SMILES strings from deliberately corrupted ones. 
This training strategy enhances the model's ability to generate valid and relevant molecular structures.
% 
The model demonstrates superior performance compared to autoregressive models like MolT5-Base, achieving this without additional data resources or pre-training. 

\spara{MolReGPT}~\cite{LLFWLTL24}. 
MolReGPT is a novel framework leveraging LLMs like GPT to advance molecule discovery through molecule-caption translation. 
Unlike traditional methods, which rely heavily on domain experts, computational resources, or domain-specific pre-training, MolReGPT uses ICL few-shot learning. 
This approach enables LLMs to perform molecule understanding and text-based molecule generation by retrieving and learning from similar molecules and their descriptions from a local database. 

% \spara{\oldmodel-Large-HV~\cite{ELRHCJ22}.} 
% Another version of \oldmodel. 

% \spara{TGM-DLM~\cite{GLWW24}.}
% It is a novel approach for generating molecules based on specific textual descriptions using a diffusion language model. 
% Unlike traditional SMILES-based molecule generation methods that rely on autoregressive architectures, TGM-DLM employs a two-phase diffusion process to update token embeddings collectively and iteratively. 

\spara{BioT5}~\cite{PZZWGWXY23}.
It is a pre-training framework designed to enhance drug discovery by integrating molecules, proteins, and natural language. 
This framework addresses limitations in current models, such as generating invalid molecular SMILES, underutilising contextual information, and treating structured and unstructured knowledge equally. 
BioT5 utilises SELFIES~\cite{KHNFG20} for robust molecular representations and extracts relevant knowledge from the context surrounding bio-entities in unstructured biological literature. 

\spara{MolXPT}~\cite{LZXWXQZL23}. 
It is a unified language model that integrates text and molecular representations for enhanced molecular modelling. 
MolXPT leverages the success of Generative Pre-trained Transformers (GPT) by pre-training on SMILES sequences wrapped in relevant textual context. 
This involves detecting molecule names in text, replacing them with corresponding SMILES, and thus allowing mutual information exchange between text and molecule representations. 

\spara{BioT5+}~\cite{PWGLFZXQY24}.
BioT5+ is designed to bridge the gap between molecular data and textual descriptions in biological research. 
Building upon the BioT5 framework, BioT5+ introduces several innovations, including the integration of IUPAC nomenclature for molecules, which enhances its ability to understand molecular structures in both scientific literature and formal representations like SMILES and SELFIES. 
By employing multi-task instruction tuning, BioT5+ can generalise across diverse biological tasks, such as classification, regression, and generation, making it versatile for applications ranging from molecule property prediction to drug discovery. 

\section{Additional Experimental Results and Discussion}
\label{sec:appendix_additional_results_and_discussion}

% \input{tables/table-example-mol-caption}

\spara{Anecdotal molecule generation examples.}
Table~\ref{table:example_generated_molecule} shows some example molecules generated by \oldmodel-Small and \newmodel-Small, and the ground-truth molecules from \newdataset. 
From these examples, we can find that \newmodel-Small can generate accurate molecules similar to the ground truth, while \oldmodel-Small is making mistakes. 
% 
(1) is an interesting case since (\emph{i}) its ground truth SMILES string has a long length, $88$ characters for which \newmodel-Small is able to generate an exact match; (\emph{ii}) indicates that \newmodel-Small can understand crystalline solids, like indolylmethylglucosinolate, in the annotation. 
% 
In another interesting case, (2), \newmodel-Small not only understands chemical compounds but also can understand chemical treatments, \eg, replacement, mentioned in the annotation. 
% 
These examples showcase the superiority of \newmodel for the text-based \emph{de novo} molecule generation task.

% \spara{Observation 7: \newmodel generates more accurate and semantically richer molecule descriptions.}
\spara{Anecdotal molecule captioning examples.}
Table~\ref{table:example_generated_caption} shows examples of molecule descriptions generated by \oldmodel-Small and \newmodel-Small, alongside the ground-truth descriptions. \newmodel-Small can generate more accurate and detailed descriptions that align closely with the ground truth, whereas \oldmodel-Small often misses important semantic details.
% % 
% In (1), both models describe the molecule as an indolylmethylglucosinolate. 
% However, \newmodel-Small provides additional context by specifying the pH and detailing the major species, enhancing the description’s accuracy and informativeness.
% 
In (1), \oldmodel-Small omits critical details about the specific role and structure. 
In contrast, \newmodel-Small correctly identifies the molecule as derived from malonic acid and mentions its role as an Escherichia coli metabolite and mouse metabolite.
% 
These examples show the superiority of \newmodel-Small in generating detailed and accurate molecule descriptions, making it a more effective model for the molecule captioning task.

\section{Additional Experiments}
\label{sec:appendix_additional_experiments}

Section~\ref{subsec:preliminary} and Section~\ref{subsec:language_augmentation} demonstrate a detailed example implementation of \pipeline on the \olddataset for challenging molecular generation tasks. 
To further demonstrate the versatility of \pipeline, we extend its application to several additional datasets, \bace~\cite{HFZDRLCL20}, \hiv~\cite{HFZDRLCL20}, \esol~\cite{HFZDRLCL20}, and \image~\cite{SDGS18}. 
These datasets support a variety of crucial tasks, such as \emph{image captioning}, \emph{text understanding}, and \emph{graph property prediction}.

\subsection{Dataset and Task}
\label{subsec:appendix_dataset_and_task}

We consider four benchmark datasets, which contain \emph{image}, \emph{text} and \emph{graph} data. 
\begin{enumerate}[leftmargin=*]\itemsep0em
    \item \bace. The \bace dataset provides quantitative ($\mathrm{IC}_{50}$) and qualitative (binary label) binding results for a set of inhibitors of human b-secretase 1 (BACE-1). 
    All data are experimental values reported in the scientific literature over the past decade, some with detailed crystal structures available. 
    \textbf{Task:} \bace merged a collection of 1,522 compounds with their 2D structures and binary labels, built as a classification task. 
    \item \hiv. The HIV dataset was introduced by the Drug Therapeutics Program (DTP) AIDS Antiviral Screen, which tested the ability to inhibit HIV replication for 41,127 compounds. 
    Screening results were evaluated and placed into three categories: confirmed inactive (CI), confirmed active (CA) and confirmed moderately active (CM).
    We further combine the latter two labels, making it a classification task between inactive (CI) and active (CA and CM). 
    \textbf{Task:} As we are more interested in discovering new categories of HIV inhibitors based on the available \emph{text} and \emph{graph} structure information.  
    \item \esol. \esol is a small dataset consisting of water solubility data for 1,128 compounds. 
    \textbf{Task:} We intend to estimate solubility directly from chemical \emph{graph} structures (as encoded in \emph{text} SMILES strings). 
    \item \image. \image is a large-scale dataset comprising around $3.3$ million image-caption pairs. 
    It is designed for automatic image captioning tasks and represents a significant step forward in terms of the variety and volume of data compared to previous datasets like MS-COCO. 
    \textbf{Task:} We follow the settings of \cite{FKIKT23} to train CLIP model~\cite{RKHRGASAMCKS21} and test it on ImageNet~\cite{DDSLLF09}. 
\end{enumerate}

\subsection{Automatic Annotation Augmentation}
\label{subsec:appendix_auto_annotation_augmentation}

Given \bace, \hiv, and \esol datasets, we first generate descriptions following the instruction of \cite{ZZM24}. 
Consequently, we query LLMs to augment these descriptions as described in Section~\ref{subsec:language_augmentation}. 
The prompt is designed as follows:
\begin{mybox}
    \textbf{Instruction: }
    You are now a chemical specialist in rewriting descriptions for a molecule in SMILES format. 
    Make sure those descriptions describe the given molecule correctly and precisely based on your two inputs (SMILES and Description of it). 
    Also, make sure your rewriting captions do not include the input SMILES.
    % Write the response without using linebreaks, newlines, or special characters such as ``\textbackslash t'' or ``\textbackslash n''.
    % 
    \\
    \textbf{Message: }
    \emph{SMILES string of target molecule:} [SMILES]. \\
    \emph{Description of the molecule:} [description]. \\
    \emph{Task:} Rewrite the following molecule with its SMILES and description. 
    The newly rewritten caption should be elaborate, descriptive, and concise, highlighting the key structural features and biological activities of the molecule. 
    Only output rewritten caption without any header and linebreak. \\
    \emph{Answer:}
\end{mybox}

\image has available annotations for each image. 
We leverage LLMs to augment their annotations using this prompt:
\begin{mybox}
    \textbf{Instruction: }
    You are now a specialist in rewriting descriptions for an image. 
    Make sure those descriptions describe the given image correctly and precisely. 
    \\
    \textbf{Message: }
    \emph{Description of the image:} [description]. \\
    \emph{Task:} Rewrite the following description. 
    The newly rewritten caption should be elaborate, descriptive, and concise, highlighting the key knowledge of the molecule. 
    Only output rewritten caption without any header and linebreak. \\
    \emph{Answer:}
\end{mybox}

In this paper, we utilise two closed-source LLMs (GPT 3.5-turbo~\cite{AAAA23} and Gemini Pro~\cite{Gemini24}) to generate two rewritten annotations for the above-mentioned datasets. 


\subsection{Training on Augmented Dataset}
\label{subsec:appendix_trainng_on_augmented_dataset}

After obtaining the augmented datasets (\bace, \hiv, and \esol), we simply combine three annotations of each molecule as the input features and integrate them within the LM and GNN models. 
Other training implementations follow the instruction of \cite{ZZM24}. 
% 
About the \image dataset, we follow the implementation of \cite{FKIKT23} to integrate the augmented annotations with the CLIP model and evaluate them. 

Results in Table~\ref{table:results_more_dataset} show that \pipeline significantly enhances performance across these diverse applications. 
This improvement highlights \pipeline’s potential to be a valuable tool in a wide range of AI tasks, offering substantial gains in accuracy and efficiency.
