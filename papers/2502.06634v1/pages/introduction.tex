\input{figures/figure-ablation}

Artificial Intelligence (AI) has garnered increasing attention due to its transformative potential in broad real-world applications, including biology~\cite{SLGBHBL19,PZZWGWXY23,ZM23}. 
Take a recent new trend as an example: researchers intend to jointly model SMILES~\cite{W88}
% \footnote{Simplified Molecular Input Line Entry System, a widely used tool to represent molecules} 
strings and scientific text to obtain shared representations across the two modalities.
For instance, \citet{ELRHCJ22} innovatively propose \oldmodel, a model based on the T5 architecture~\cite{RSRLNMZLL20}, pre-trained on ZINC~\cite{SI15} by predicting the masked text parts.  
Consequently, they fine-tune the model on \olddataset~\cite{EZJ21} to learn how to map between SMILES representations of molecules and their corresponding annotations (captions) to support molecular tasks. 

However, the development of AI faces a fundamental setback: \emph{the scarcity of high-quality annotated data}. 
% 
For instance, molecular data annotation is often a costly and time-consuming process~\cite{DGH16}. 
This limitation restrains the development of AI approaches, as models grow in size and expressiveness, they require larger and more diverse annotated datasets to achieve high performance and generalisability~\cite{DCLT19,HGC23}.
Therefore, one viable alternative for AI in practice is to resort to \emph{effective data augmentation strategies.}

Back to the example \olddataset dataset, a set of studies attempted various solutions to address the scarcity limitation, following the \oldmodel. 
\citet{CGBWLM23}, \citet{LZXWXQZL23}, \citet{PZZWGWXY23} and \citet{PWGLFZXQY24} introduce extra chemical databases and auxiliary tasks to train advanced models. 
However, the heavy dependencies on supplemental datasets limit their practical versatility, and the auxiliary task definition requires domain expertise. 
% 
On the other hand, \citet{LLFWLTL24} use the general human knowledge embedded in Large Language Models (LLMs) to perform the molecule-caption translation tasks. 
Nevertheless, despite the widespread use of LLMs in literature review, table interpretation, \etc, their application in biology is not straightforward \cite{LJRHHNPWR24}.
The effectiveness of \cite{LLFWLTL24} depends heavily on the specific retrieval strategy used, and its performance is surpassed by existing smaller models. 
Additionally, \citet{ZZM24} and our empirical studies in Figures~\ref{fig:ablation},\ref{fig:compare_sota} demonstrate the limitations of using LLMs for data augmentation.

This paper proposes an effective automatic pipeline (see Figure~\ref{fig:rewrite_training_pipeline}), \pipeline, to effectively augment annotations of datasets with no human supervision.
Once the augmented datasets are generated, existing methods can be conveniently re-trained for significant performance boosting. 

We showcase the effectiveness of \pipeline by creating an enhanced dataset, \newdataset, where we leverage the in-context learning~\cite{LYFJ23} capability of LLMs to rewrite the annotation of each molecule in \olddataset. 
These rewritten annotations preserve essential molecular information while providing more varied sentence structures and vocabulary (see our analysis in Section~\ref{subsec:analysis}). 
% 
After the annotation augmentation process, each molecule in \newdataset is accompanied by diverse annotations. 
Using these annotations, we proceed to train \newmodel, using the benchmark \oldmodel architecture, to support molecular tasks. 
During training, \newmodel aims to learn a mapping function between the space of molecules and the augmented annotations, thereby enhancing the overall performance of the models.

We systematically evaluate the effectiveness of \newmodel on challenging text-based \emph{de novo} molecule generation and molecule captioning tasks. 
Through extensive experiments on the benchmark evaluation pipeline, we demonstrate that \newmodel significantly elevates the performance of \oldmodel, which was trained using the same architecture on the original \olddataset dataset. 
Notably, \newmodel achieves improvements of up to $301\%$ on the molecule generation task and $9.51\%$ on the molecule captioning task.
% 
Additionally, the small-size variant of \newmodel ($77M$ parameters) outperforms the large-size variant of \oldmodel ($800M$ parameters) for the molecule generation task. 
% 
Compared with other leading methods reported on the leaderboard, \newmodel achieves new state-of-the-art performance with $99\%$ fewer parameters.
% 
More importantly, \pipeline effectively boosts the performance of other applications, including \emph{image captioning}, \emph{text understanding} and \emph{graph property prediction}, affirming its versatility.

\smallskip 
\noindent \textbf{Our contributions} are as follows: 
\textbf{(1)} A fully automated pipeline for domain-specific applications where limited data availability restricts the effectiveness of existing technologies. 
\textbf{(2)} A set of lightweight open-source models tailored to address challenging molecular tasks. 
\textbf{(3)} Empirical studies demonstrating the necessity and effectiveness of \pipeline across multiple applications.
