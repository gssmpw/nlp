Recent advancements in AI for biological research focus on integrating molecular data with natural language to accelerate drug discovery. 
However, the scarcity of high-quality annotations limits progress in this area. 
% 
This paper introduces \pipeline, a Language-based Automatic Annotation Augmentation framework that leverages large language models to augment existing datasets, thereby improving AI training. 
We demonstrate the effectiveness of \pipeline by creating an enhanced dataset, \newdataset, where we systematically rewrite the annotations of molecules from an established dataset. 
These rewritten annotations preserve essential molecular information while providing more varied sentence structures and vocabulary. 
Using \newdataset, we train \newmodel based on a benchmark architecture to learn the mapping between molecular representations and augmented annotations.

Experimental results on text-based \emph{de novo} molecule generation and molecule captioning demonstrate that \newmodel outperforms state-of-the-art models. Notably, incorporating \pipeline leads to improvements of up to $301\%$ over the benchmark architecture.
% 
Furthermore, we validate the effectiveness of \pipeline notable applications in \emph{image}, \emph{text} and \emph{graph} tasks, affirming its versatility and utility.
\footnote{The augmented dataset and trained models are available at \url{https://github.com/zhiqiangzhongddu/LA3}.}
