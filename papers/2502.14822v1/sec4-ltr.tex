\section{Learning-to-Rank Model Architectures}
\label{sec:ltr}
Different from traditional IR models discussed in \cref{sec:traditional}, Learning-to-Rank (LTR) %dated early to early 90s, 
leverages the idea of supervised ML on extensively crafted numerical features~\cite{burges2005learning,burges2010ranknet,qin2013mslr}. 
For each $(\mathcal{Q}_i, \mathcal{D}_i)$ pair, a $k$-dimensional feature vector $\mathbf{x}_i \in \mathbb{R}^{k}$ and a relevance label $\mathbf{y}_i$ is provided to the ranking model $f$ parameterized by $\theta$. Denote the loss function as $l(\cdot)$, the ranking is trained to minimize the empirical loss on labeled training set $\Psi$: 
$\mathcal{L} = 1 / |\Psi| \sum_{(\mathbf{x}_i, \mathbf{y}_i) \in \Psi} l(f_{\theta}(\mathbf{x}_i), \mathbf{y_i}). $

Explorations in LTR models can be grouped into two directions: 
\textit{ML-based models} and \textit{neural LTR models}.
Under the scope of ML models, \textsc{RankSVM}~\cite{joachims2006training} is a pairwise LTR model based on \textsc{SVM}.~\citet{burges2005learning} studied decision trees and \citet{wu2010adapting} proposed \textsc{LambdaMART} based on Gradient Boosted Decision Trees~\cite[\textsc{GBDT},][]{friedman2001greedy,ke2017lightgbm}. 
Unsurprisingly, early works also explored neural LTR models. \textsc{RankNet}~\cite{burges2005learning} and \textsc{LambdaRank}~\cite{burges2006learning} parameterize the LTR model with neural networks.
Recent works such as \textsc{GSF}~\cite{ai2019learning} and \textsc{ApproxNDCG}~\cite{bruch2019revisiting} use multiple fully connected layers. \textsc{DLCM} is based on RNN and reranks document list from \textsc{LambdaRank}, while \textsc{SetRank} uses self-attention to encode the entire list and perform a joint scoring. \citet{qin2021neural} conducted rigorous study of benchmarking neural ranking models against \textsc{GBDT}-based models. 
See~\cref{tab:ltr_model_appendix} for a list of LTR models and prior surveys on LTR techniques~\cite{liu2009ltr,li2011learning}.

LTR techniques utilize human-crafted numerical features and metadata like PageRank score~\cite{brin1998anatomy} and click count as features and are still widely used in modern search systems~\cite{google2019search}. However, it lacks the flexibility to be directly applied to raw text data and still cannot overcome the lexical mismatch problem, which is the main focus of neural ranking methods (\cref{sec:neural_ranking}).
