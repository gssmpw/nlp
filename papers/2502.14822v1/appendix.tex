
\onecolumn
\label{appendix}

\section{Supplement Materials on Traditional IR Models}
\label{appendix:traditional}
\noindent \textbf{Boolean Models} 
The most basic Boolean model can be extended by incorporating term weighting, allowing both queries and documents to be represented as sets of weighted terms. Then, the logical implication $\mathcal{D} \rightarrow \mathcal{Q}$ is also weighted.
The commonly used weighted approaches for the logical implication $\mathcal{D} \rightarrow \mathcal{Q}$ include using a fuzzy set extension of the Boolean logic~\cite{radecki1979fuzzy,kraft1983fuzzy} and $p$-norm~\cite{salton1983extended}. \\

\noindent \textbf{Vector Space Model} 
The weights $q_i$ or $d_i$ can be represented by other sophisticated schema, such as TF-IDF~\cite{sparck1972statistical} and BM25~\cite{robertson1995okapi}. The extracted abundant features can improve the capacity and accuracy of the vector space models.
Besides, given the vector representations of query $\mathcal{Q}$ and document $\mathcal{D}$, the most commonly used is cosine similarity, defined as
$$
\text{sim}(\mathcal{Q},\mathcal{D}) = \frac{\mathcal{Q}\cdot\mathcal{D}}{|\mathcal{Q}|\times|\mathcal{D}|}, 
$$
where $\mathcal{Q}\cdot\mathcal{D}$ is the dot product and  $|\mathcal{Q}|,|\mathcal{D}|$ denotes the length of the vector. \\

\noindent \textbf{Probabilistic Model} 
Except for the binary independence retrieval model, more sophisticated probabilistic models are proposed in the literature~\cite{wong1989probability,fuhr1992probabilistic}, such as the inter-dependency between terms~\cite{van1979information}.\\

\noindent \textbf{Statistic Language Model} 
The statistic language models for IR~\cite{miller1999hidden,berger1999information,song1999general} also encounter the problem of the zero occurrences of the query term $t_i$, i.e., the probability $\mathcal{P}(\mathcal{Q}|\theta_{D})$ becomes zero, if a query term $t_i$ does not appear in the document. This is too restrictive for IR, as a document can still be relevant even if it contains only some of the query terms. To address this zero-probability issue, smoothing techniques are applied, assigning small probabilities to terms that do not appear in the document. The principle behind smoothing is that any text used to model a language captures only a limited subset of its linguistic patterns (or terms, in this case).
The commonly used smoothing methods~\cite{zhai2004study,zhai2008statistical} include Jalinek-Mercer smoothing, Dirichlet smoothing, etc.

\section{Supplement Materials on Learning-to-Rank Architecture and Training Strategy}
\label{appendix:ltr}

We present a list of learning-to-rank works and their backbone architectures in~\cref{tab:ltr_model_appendix}. 
A significant portion of the literature focuses on loss functions and feature transformers~\cite{qin2021neural,bruch2019revisiting,burges2010ranknet}. 
Additionally, some studies focus on unbiased relevance estimation using biased feedback~\cite{wang2016learning,joachims2017unbiased,joachims2017accurately,ai2018unbiased,ai2018dla,wang2018position,hu2019unbiased,ren2022unbiased}
while other focus on jointly optimizing effectiveness and fairness of the ranking systems~\cite{singh2018fairness,biega2018equity,morik2020controlling,patro2020fairrec,oosterhuis2021computationally,yang2023vertical,yang2023marginal,yang2023fara}.
We omit detailed discussions here and refer readers to the original papers. 

\input{tables/ltr-model}


\section{Supplement Materials on Neural Ranking Models}
\label{appendix:neural_ranking}
\noindent \textbf{Representation-based Models} Representation-based neural ranking models can be regarded as extensions of vector space models (\cref{sec:traditional}), which independently encode queries and documents into a latent vector space. The relevance ranking of a document is determined by computing the similarity (e.g., cosine similarity) between the query and document embeddings.

The Deep Structured Semantic Model (\textsc{DSSM})~\citep{huang2013dssm} is an early example of a representation-based neural ranking model. It utilizes word hashing and multilayer perceptrons (MLPs) to independently encode term vectors of queries and documents into a shared semantic space, enabling the computation of ranking scores based on the cosine similarity of their embeddings. 
Research has focused on enhancing \textsc{DSSM} by modifying its encoder network to improve the model's ability to capture richer semantic and contextual information. For instance, Convolutional \textsc{DSSM}~\citep{shen2014cdssm} leverages a CNN architecture to project vectors within a context window to a local contextual feature vector. These local features are then aggregated using a max-pooling layer to produce a representation of the entire query or document. 
Another variation of \textsc{DSSM}replaces MLPs with a Long Short-Term Memory (LSTM) network~\citep{palangi2016deep,wan2016deep} . By leveraging LSTM's memory mechanism, such models can capture both local and global context information without the pooling layer, thus better suited for handling longer documents.\\

\noindent \textbf{Interaction-based Models} Interaction-based models process queries and documents jointly through neural networks. 
The model's output is typically a score that measures the relevance of the input query-document pair.
Various network architectures have been proposed to jointly encode queries and documents. For instance, \textsc{MatchPyramid}~\citep{pang2016matchpyramid} employs CNN over the interaction matrix between query and document terms. This approach treats the interaction matrix as an image, allowing the CNN to capture local matching patterns. The model then aggregates these patterns through convolution and pooling operations to produce a relevance score, effectively modeling the hierarchical matching structures between queries and documents~\citep{hu2014convolutional}.
Building upon the concept of interaction-focused models, \citet{guo2016deep} highlighted the importance of exact term matches in neural ranking models and proposed the Deep Relevance Matching Model (\textsc{DRMM}). The model constructs matching histograms for each query term to capture the distribution of matching signals across document terms. 
These histograms are then processed through a feed-forward neural network to learn hierarchical matching patterns.
\citet{xiong2017knrm} introduced the Kernel-Based Neural Ranking Model (\textsc{K-NRM}), which further advanced interaction-based approaches. 
K-NRM employs a translation matrix to compute interactions between query and document terms based on their embeddings. It then applies Radial Basis Function (RBF) kernels to transform these word-level interactions into informative ranking features. Later, they extended the RBF kernel approach to a convolutional neural network~\citep{dai2018cknrm}.\\ 


\noindent \textbf{Word Embeddings} In addition to advancements in network architecture, pre-trained textual representations have also contributed to neural ranking models' performance~\citep{guo2016semantic}. GloVe~\citep{pennington-etal-2014-glove} and Word2Vec~\citep{mikolov2013efficient} learn dense vector representations for each vocabulary term from large-scale text corpora. Pre-trained embeddings provide semantic-based term representations to enable neural ranking models to focus on learning relevance matching patterns. Both representation-based and interaction-based models adopt pre-trained word embeddings as input representations to their networks, facilitating training convergence and improved performance~\citep{levy-etal-2015-improving}. Interaction-based models with cross-lingual word embeddings~\citep{joulin2018loss} for cross-lingual reranking have also been explored~\citep{yu2020study}.\\

\noindent\cref{tab:neural_ranking_appendix} shows a list of neural ranking models and backbone architectures. Researchers have explored different backbone neural network architectures in this era, including Convolutional Neural Network~\cite[CNN,][]{lecun1989backpropagation,lecun1998gradient}, Long Short Term Memory~\cite[LSTM,][]{hochreiter1997long} and kernel methods~\cite{vert2004primer,chang2010training,xiong2017knrm}. 

Notably, a line of research explores integrating kernel methods with the \textsc{transformer} architecture~\cite{vaswani2017attention}. 
The main distinction between this line of research and the models discussed in~\cref{sec:transformer} is that the transformer modules here are not pre-trained on large-scale corpora like Wikipedia and C4~\cite{devlin-etal-2019-bert,raffel2020transfer}. We consider this line of research as an intersection between neural ranking models (\cref{sec:neural_ranking}) and retrieval with pre-trained transformers (\cref{sec:transformer}).
\textsc{TK}~\cite{hofstatter2020interpretable} uses a shallow transformer neural network (up to 3 layers) to encode the query $\mathcal{Q}$ and document $\mathcal{D}$ separately. After encoding, the contextualized representations are input to one single interaction match matrix, similar to model architecture shown in~\cref{fig:architecture:interaction}. The entire model is trained end-to-end and is able to achieve better performance-efficiency trade-off compared to \textsc{BERT}-based reranker~\cite{nogueira2019multi}.
The main bottleneck of applying transformer architectures to long document reranking is $O(n^2)$ time complexity, where $n$ denotes the document length. \textsc{TKL}~\cite{hofstatter2020local} further improves upon $\textsc{TK}$ with a local attention mechanism and leads to performance improvement on long document ranking.

\input{tables/neural-ranking-model}

\section{Supplement Materials on  Pre-trained Language Models for Information Retrieval}
\label{appendix:pretrained_transformer}

We show a list of models and their corresponding architectures in~\cref{tab:bert_model_appendix}, including reranking models, learned dense retrieval, multi-vector representations and learned sparse retrieval. A majority of the models use \textsc{BERT}~\cite{devlin-etal-2019-bert} as the backbone language models, with a few exceptions using \textsc{DistilBERT}~\cite{sanh2019distilbert}, \textsc{RoBERTa}~\cite{liu2019roberta} and encoder part of \textsc{T5} family models~\cite{raffel2020transfer,sanh2022multitask,mo2023convgqr,chung2024scaling}.

One line of work aims to combine the benefits of learned dense retrieval and sparse retrieval.~\cite{gao2021complement,gao-etal-2021-coil,ma2021replication,lin2021few,cormack2009reciprocal}.
Ranklist fusion techniques~\cite[e.g., Reciprocal Rank Fusion,][]{cormack2009reciprocal} directs fuses ranklists from different retrievers and has been shown to improve retrieval performance. \textsc{COIL}~\cite{gao-etal-2021-coil} proposes to enhance traditional bag-of-words retrieval method with semantic embeddings from \bert~encoder. \textsc{uniCOIL}~\cite{lin2021few} further simplifies reduces the dimension of semantic embeddings to 1 --- equivalent to learned term weight in learned sparse retrieval models like \textsc{SPLADE}~\cite{formal21splade,formal21spladev2}. 

A few works fall into the intersection of learned sparse retrieval and multi-vector representations. For example, \textsc{SLIM}~\cite{li2023slim} first maps each contextualized token vector to a sparse, high-dimensional lexical space before performing late interaction between these sparse token embeddings. \textsc{SPLATE}~\cite{formal2024splate} take an alternative approach to first encodes contextualized token vectors, then maps these token vectors to a sparse vocabulary space with a partially learned \textsc{SPLADE} module. Both models achieve performance improvement compared to learned sparse retrieval baselines such as \textsc{SPLADE}~\cite{formal21splade,formal21spladev2}.

Instead of improving retrieval performance from the modeling perspective, a separate line of works aim to enhance the backbone language models via domain adaptation or continued pre-training, which has been proven successful by prior works in NLP~\cite{howard2018universal,gururangan-etal-2020-dont}. 
\citet{Lee2019LatentRF} propose to pre-train \textsc{BERT} model with Inverse-Cloze Task~\cite{Taylor1953ClozePA} for better text representations.
\textsc{Condenser}~\cite{gao-callan-2021-condenser} propose to ``condense'' text representations into \texttt{[CLS]} token via a dedicated pre-training architecture and corresponding training objective. 
\textsc{COCO-DR} further extends upon \textsc{Condenser} via a technique named implicit Distributionally Robust Optimization to mitigate distribution shift problem in dense retrieval. We refer readers to original papers for details.
\input{tables/bert-model}

As we noted in~\cref{sec:conclusion}, one desiderata of future IR models is interpretability and truthfulness. A few works have attempted to interpret transformer-based neural retrieval models' representations, i.e., mechanistic interpretability~\cite{elhage2021mathematical,saphra2024mechanistic}. 
For example, \citet{macavaney-etal-2022-abnirml} showed that neural retrieval models rely less on exact match signals and instead encodes rich semantic information. \citet{ram-etal-2023-token} project dense retrievers' intermediate representations to vocabulary space and show the connection of dense retrieval and traditional bag-of-words sparse retrieval methods. 
Instead of providing model-intrinsic explanations, a few works design IR systems to provide model-agnostic explanations~\cite{rahimi2021explaining,yu2022towards,xu2024cfe2} in order to meet certain desiderata such as faithfulness~\cite{jacovi-goldberg-2020-towards,xu2023reusable}. As IR systems become an integral part of other applied ML domains, we believe it is important to study and design interpretable, truthful and trustworthiness IR models. 



\section{Supplement Materials on LLM for IR}
\label{appendix:llm4ir}

We summarize a list of works that study LLM for retrieval (\cref{tab:llm4retrieval_appendix}) and reranking (\cref{tab:llm4reranking_appendix}). For generative retrieval, we point to a dedicated survey~\cite{li2024matching}. Another comprehensive survey~\cite{mo2024survey} could be referred to for conversational information retrieval. Modern IR systems require extensive labeled data to achieve good performance. One line of work studies the proposal of using LLMs for synthesizing training data~\cite{bonifacio2022inpars,boytsov2024inpars,dai2023promptagator,lee2024gecko,mo2024chiq,mo2024convsdg}. From the evaluation perspective, LLMs' superior natural language understanding capability also raise the question of whether they can be used for relevance judgments. A separate line of work tackle the relevance judgments problem~\cite{faggioli2023perspectives,faggioli2024determines,clarke2024llm}. As our focus of this survey is on model architectures, we skip the discussion and point to original papers for details.

\input{tables/llm4ir}

