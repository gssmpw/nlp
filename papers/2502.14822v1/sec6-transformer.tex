

\section{IR with Pre-trained Transformers}
\label{sec:transformer}



{\bert}~\cite{devlin-etal-2019-bert} 
quickly changed the research paradigm in NLP and IR community. Its success can be attributed to two factors: (1) the multi-head attention architecture~\cite{vaswani2017attention} admits fine-grained, contextualized representations; (2) large-scale pre-training allows \bert~to encode rich semantic, lexical information and world knowledge. The expressiveness of \bert~has been extensively studied by prior works, e.g., \citet{rogers-etal-2020-primer,tenney-etal-2019-bert,clark2019does}.
In this section, we discuss IR architectures based on pre-trained transformers, with a focus on {\bert}-type encoder models.

\paragraph{Text Reranking}
\citet{nogueira2019multi} first employed {\bert}~model for reranking candidate passages from a first-stage retriever. Their model \textsc{monoBERT} takes as input the sequence of concatenated $(\mathcal{Q}, \mathcal{D})$ as input, and outputs a relevance score $s$ with a linear layer on top of the {\bert}~model (\cref{fig:architecture:transformer-reranker}).
The schema has later been proved to be effective on other pre-trained encoders~\cite{comparing2021zhang} and encoder-decoder architectures~\cite{nogueira2020documentrankingpretrainedsequencetosequence}.
Two directions have been investigated following this direction:\
(1) how to support long documents as {\bert}-like encoders were limited to 512 input tokens;
(2) how to leverage more than the representation of CLS token from the pre-trained models.

In the first direction, one strategy is to segment the long document into shorter passages, score each passage individually, then aggregate the scores to get a document-level relevance score~\cite{Yilmaz2019CrossDomainMO,Dai2019DeeperTU}. In this case, the underlying model architecture remains unchanged. A different line of work seeks to perform feature-level aggregation. \textsc{PARADE}~\cite{Li2020PARADEPR} uses an additional neural network to aggregate contextualized representations from CLS tokens of passages to get the final document relevance score. 

In the second direction,
\citet{macavaney2019cedr} discovered via \textsc{CEDR} that the effectiveness of reranker could be enhanced when aggregating the contextualized representations with neural ranking models such as
\textsc{K-NRM} and \textsc{DRMM}.
\citet{zhang-etal-2024-celi} later observed that integrating token representations with late interactions could also effectively improve the reranking robustness on out-of-domain scenarios, 
especially for long queries. 

\paragraph{Learned Dense Retrieval}
Here we use the term "bi-encoder"~\cite{Humeau2020Poly-encoders} to refer to the model architecture commonly used for dense retrieval.\footnote{The term ``bi-encoder'' is known by many other names, such as two-tower architecture, embedding models. We refer to it as ``bi-encoder'' in contrast to ``cross-encoder'', reranker architectures that take concatenated $(\mathcal{Q}, \mathcal{D})$ as input.}
Bi-encoder uses the backbone to encoder query $\mathcal{Q}$ and document $\mathcal{D}$ separately, then uses the encoded dense vector representations to compute the relevance score with similarity functions such as dot product or cosine similarity~\cite{Xiong2020ApproximateNN,Karpukhin2020DensePR}. After training, the model encodes the collection into a dense vector index, and retrieval is performed with fast nearest neighbor search techniques~\cite{johnson2019billion,Malkov2016EfficientAR}.
Different from representation-based neural ranking models where distinct architectures are proposed (\cref{sec:neural_ranking}), existing dense retrieval methods are mostly based on pre-trained transformer language models but vary in training strategy (detailed in~\cref{appendix:pretrained_transformer}). 



\paragraph{Learned Sparse Retrieval}

Similar to learned dense retrieval, learned sparse retrieval (LSR) also uses bi-encoder architecture with language models as the backbone, to transform documents into a static index for later retrieval~\citep{zamani2018neural}. In order to utilize traditional inverted index method for faster retrieval~\cite{bruch2024efficient}, query $\mathcal{Q}$ and document $\mathcal{D}$ are represented as sparse vectors, whose dimensionality typically matches the vocabulary size of the backbone pre-trained transformer model~\citep{yu2024improved}. Here sparsity is enforced through regularization~\citep{formal21splade,Paria2020Minimizing} and usually serves as a trade-off between effectiveness and efficiency. At a higher level, LSR can be viewed as a way to learn token importance or ``impact'' scores from data~\cite{Dai2019DeeperTU,bai2020sparterm,mallia2021learning}, in contrast to static formulas like \textsc{BM25}.

\paragraph{Multi-Vector Representations}
Learned dense retrieval's bi-encoder architecture encodes queries and documents into single feature vectors separately, and estimate relevance via a similarity function. This enables efficient training, indexing, and inference, but the lack of interactions between query and document terms potentially limits performance. In contrast, the "all-to-all" interaction of cross-encoder models are computationally expensive. 
Research has explored representing queries and documents using multiple vectors and developing corresponding relevance estimators. \textsc{Poly-Encoder}~\cite{Humeau2020Poly-encoders} computes a fixed number of vectors per query, and aggregate with softmax attention against document vectors. Due to the use of softmax operator, fast nearest neighbor search technique cannot be trivially applied.
\textsc{ME-BERT}~\cite{luan2021sparse} proposes to represent documents with $m$ vectors, and uses the maximum similarity between query vectors and document vectors to estimate relevance. 
\textsc{ColBERT}~\cite{khattab2020colbert} takes the multi-vector representation idea further and represents each token in query and document as a contextualized vector. Each query vector interacts with each document vector via a MaxSim operator, and the relevance score is computed by summing scalar outputs of these operators over query terms. \textsc{ColBERT} is trained end-to-end and achieves strong performance on public retrieval benchmarks. Numerous other studies further investigate token selection and aggregation operations, see~\cref{appendix:pretrained_transformer} for details.
