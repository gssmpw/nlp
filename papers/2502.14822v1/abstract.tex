\begin{abstract}
This survey examines the evolution of model architectures in information retrieval (IR), focusing on two key aspects:\ backbone models for feature extraction and end-to-end system architectures for relevance estimation. 
The review intentionally separates architectural considerations from training methodologies to provide a focused analysis of structural innovations in IR systems.
We trace the development from traditional term-based methods to modern neural approaches, particularly highlighting the impact of transformer-based models and subsequent large language models (LLMs). 
We conclude by discussing emerging challenges and future directions, including architectural optimizations for performance and scalability, handling of multimodal, multilingual data, and adaptation to novel application domains beyond traditional search paradigms.
\end{abstract}