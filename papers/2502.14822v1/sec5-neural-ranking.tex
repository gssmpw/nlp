
\section{Neural Ranking Models}
\label{sec:neural_ranking}
\input{figures/model_figure_temp}

Different from feature engineering of LTR (\cref{sec:ltr}), neural ranking models utilize deep neural networks to learn feature representations directly from raw text and again use neural networks for relevance estimation\footnote{We define neural information retrieval as retrieval models based on neural networks prior to pre-trained transformers. More models details are deferred to~\cref{appendix:neural_ranking} and detailed surveys~\cite{onal2018neural,mitra2018introduction,xu2018deep}}. 
%Relevance is not an inherent property of a document but is determined in relation to a specific query. 
Depending on how queries interact with documents during network processing, neural ranking models can be roughly divided into \textit{representation-based models} and \textit{interaction-based models}~\citep{guo2016deep}.


Representation-based models can be regarded as extensions of vector space models (\cref{sec:traditional}), which independently encode queries and documents into a latent vector space, as illustrated in~\cref{fig:architecture:representation}.
The Deep Structured Semantic Model (\textsc{DSSM})~\citep{huang2013dssm} is an early example. 
It utilizes word hashing and multilayer perceptrons (MLPs) to independently encode term vectors of queries and documents, enabling the computation of ranking scores based on the cosine similarity of their embeddings. 
Later works modify \textsc{DSSM}'s encoder network to better capture richer semantic and contextual information. Convolutional \textsc{DSSM}~\citep{shen2014cdssm} leverages a CNN architecture to project vectors within a context window to a local contextual feature vector. 
A variation of \textsc{DSSM} replaces MLPs with a Long Short-Term Memory (LSTM) network~\citep{hochreiter1997long,palangi2016deep,wan2016deep}, utilizing its memory mechanism to capture local and global context information.


Interaction-based models~\cref{fig:architecture:interaction}, on the other hand, process queries and documents jointly through neural networks. 
The model's output is typically a scalar relevance score of the input query-document pair.
Various network architectures have been proposed under this paradigm. \textsc{MatchPyramid}~\citep{pang2016matchpyramid} employs CNN over the interaction matrix between query and document terms. The interaction matrix is treated as an image, allowing the CNN to capture local matching patterns through convolution and pooling operations~\citep{hu2014convolutional}. 
Building upon the concept of interaction-focused models, \citet{guo2016deep} highlight the importance of exact term matches in neural ranking models and proposed the Deep Relevance Matching Model (\textsc{DRMM}). The model constructs matching histograms for each query term to capture the distribution of matching signals across document terms. 
Kernel-Based Neural Ranking Model~\cite[\textsc{K-NRM,}][]{xiong2017knrm} further advances interaction-based approaches. 
It employs radial basis function kernels to transform the query-document interaction matrix informative ranking features. \textsc{Conv-KNRM}~\citep{dai2018cknrm} later extends it to convolutional kernels.

In addition to the development of network architecture, pre-trained embeddings~\cite{mikolov2013efficient,pennington-etal-2014-glove} provide semantic-based term representations to enable neural ranking models to focus on learning relevance matching patterns, improving training convergence and retrieval performance on both representation-based and interaction-based models~\citep{levy-etal-2015-improving}.

