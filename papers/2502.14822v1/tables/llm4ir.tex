
\begin{table*}[t!]
\centering
% \small
\rowcolors{2}{gray!15}{white} % Alternating row colors
\resizebox{1\textwidth}{!}
{
\begin{tabular}{llll}
\toprule
\textbf{Name} & \textbf{Architecture} & \textbf{Backbone LM} & \textbf{Training strategy} \\
\midrule
\textsc{CPT-Text}~\cite{neelakantan2022text} & LLM Encoder & \textsc{GPT-3} & Listwise Loss \\
\textsc{SGPT-BE}~\cite{muennighoff2022sgpt} & LLM Encoder & \textsc{GPT-J} \& \textsc{GPT-NeoX} & Listwise Loss \\
\textsc{GTR}~\cite{ni-etal-2022-large} & LLM Encoder & \textsc{T5} & Listwise Loss \\
\textsc{RepLlama}~\cite{ma2024fine} & LLM Encoder & \textsc{Llama} & Listwise Loss \\
\textsc{E5-Mistral}~\cite{wang2023improving} & LLM Encoder & \textsc{Mistral} & Synthetic Data + Listwise Loss \\
\textsc{MambaRetriever}~\cite{zhang2024mambaretriever} & LLM Encoder & \textsc{Mamba} & Listwise Loss \\
\textsc{LLM2Vec}~\cite{behnamghader2024llmvec} & LLM Encoder & \textsc{Llama} \& \textsc{Mistral} & Adaptation + Contrastive Pre-training \\
\textsc{Grit-LM}~\cite{muennighoff2025generative} & LLM & \textsc{Mistral} \& \textsc{Mixtral 8x7B} & Generative/Embedding Joint Training \\ 
\textsc{NVEmbed}~\cite{lee2024nvembed} & LLM Encoder & \textsc{Mistral} & Adaptation + Synthetic Data + Listwise Loss \\
\bottomrule
\end{tabular}
}
\caption{Summary of IR model architecture utilizing large language models as retrieval backbone.}
\label{tab:llm4retrieval_appendix}
\end{table*}

\begin{table*}[ht]
\centering
% \small
\rowcolors{2}{gray!15}{white} % Alternating row colors
\resizebox{1\textwidth}{!}
{
\begin{tabular}{llll}
\toprule
\textbf{Name} & \textbf{Architecture} & \textbf{Backbone LM} & \textbf{Training / Prompting Strategy} \\
\midrule
\multicolumn{4}{l}{\emph{Fine-tune LLM for Reranking}} \\
\textsc{monoT5}~\cite{nogueira2020documentrankingpretrainedsequencetosequence} & LM & \textsc{T5} & Classification \\
\citet{nogueira-dos-santos-etal-2020-beyond} & LM & \textsc{BART} & Unlikelihood \\
\textsc{QLM-T5}~\cite{zhuang2021deep} & LM & \textsc{T5} & Language Modeling \\
\textsc{duoT5}~\cite{pradeep2021expando} & LM & \textsc{T5} & Pairwise Loss \\
\textsc{RankT5}~\cite{zhuang2023rankt5} & LLM Encoder + Prediction Layer & \textsc{T5} & Listwise Loss \\
\textsc{ListT5}~\cite{yoon-etal-2024-listt5} & Fusion-in-decoder & \textsc{T5} & Listwise Loss \\
\textsc{SGPT-CE}~\cite{muennighoff2022sgpt} & LM & \textsc{GPT-J} \& \textsc{GPT-Neo} & Listwise Loss \\
\textsc{RankLlama}~\cite{ma2024fine} & LLM Encoder + Prediction Layer & \textsc{Llama} & Listwise Loss \\
\textsc{RankMamba}~\cite{xu2024rankmamba} & LLM Encoder + Prediction Layer & \textsc{Mamba} & Listwise Loss \\
\textsc{RankVicuna}~\cite{pradeep2023rankvicunazeroshotlistwisedocument} & LM & \textsc{Vicuna} & Listwise \\
\textsc{RankZephyr}~\cite{pradeep2023rankzephyreffectiverobustzeroshot} & LM & \textsc{Zephyr} & Listwise \\
\citet{zhang2023rank} & LM & \textsc{Code-LLaMA-Instruct} & Listwise \\
\citet{liu2024leveraging} & Embedding + LM & \textsc{Mistral} & Listwise \\

\multicolumn{4}{l}{\emph{Prompt LLM for Reranking}} \\
\citet{zhuang-etal-2023-open} & LM & Multiple & Pointwise Prompting \\
\citet{zhuang-etal-2024-beyond} & LM & \textsc{Flan-PaLM-S} & Pointwise Prompting \\
\textsc{UPR}~\cite{sachan-etal-2022-improving} & LM & \textsc{T5} \& \textsc{GPT-Neo} & Pointwise Prompting \\
\textsc{PRP}~\cite{qin-etal-2024-large} & LM & \textsc{Flan-UL2} & Pairwise Prompting \\
\citet{yan-etal-2024-consolidating} & LM & \textsc{Flan-UL2} & Pairwise Prompting \\
\citet{zhuang2024setwise} & LM & \textsc{Flan-T5} & Pairwise \& Setwise Prompting \\
\textsc{LRL}~\cite{ma2023zeroshotlistwisedocumentreranking} & LM & \textsc{GPT-3} & Listwise Prompting \\
\textsc{RankGPT-3.5}~\cite{sun2023chatgpt} & LM & \textsc{GPT-3.5} & Listwise Prompting \\
\textsc{RankGPT-4}~\cite{sun2023chatgpt} & LM & \textsc{GPT-4} & Listwise Prompting \\

\bottomrule
\end{tabular}
}
\caption{Summary of IR model architecture utilizing large language models for reranking. \citet{nogueira-dos-santos-etal-2020-beyond} and \citet{zhuang2021deep} revisit the statistic language model problem with modern transformer-based models, including \textsc{BART}~\cite{lewis-etal-2020-bart} \textsc{T5}~\cite{raffel2020transfer} and \textsc{GPT-2}~\cite{Radford2019LanguageMA}.}
\label{tab:llm4reranking_appendix}
\end{table*}
