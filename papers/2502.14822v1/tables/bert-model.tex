\begin{table*}[t!]
\centering
% \small
\rowcolors{2}{gray!15}{white} % Alternating row colors
\resizebox{1\textwidth}{!}
{
\begin{tabular}{lllll}
\toprule
\textbf{Name} & \textbf{Model} & \textbf{Architecture} & \textbf{Backbone LM} & \textbf{Training strategy} \\
\midrule
\textsc{monoBERT}~\cite{nogueira2019multi} & Reranking & Cross-encoder & \textsc{BERT} & Classification \\
\textsc{CEDR}~\cite{macavaney2019cedr} & Reranking & Cross-encoder & \textsc{BERT} & Hard Negatives \\
\textsc{BERT-MaxP}~\cite{Dai2019DeeperTU} & Reranking & Cross-encoder & \textsc{BERT} & Pairwise Loss \\
\citet{gao2020understanding} & Reranking & Cross-encoder & \textsc{BERT} & Distillation \\
\textsc{TART-full}~\cite{asai-etal-2023-task} & Reranking & Cross-encoder & \textsc{Flan-T5-Enc} & Instruction Tuning \\
\textsc{DPR}~\cite{Karpukhin2020DensePR} & LDR & Bi-encoder & \textsc{BERT} & Hard Negatives \\
\textsc{ANCE}~\cite{Xiong2020ApproximateNN} & LDR & Bi-encoder & \textsc{RoBERTa} & Hard Negatives \\
\textsc{RepBERT}~\cite{zhan2020repbert} & LDR & Bi-encoder & \textsc{BERT} & In-batch negatives \\
\textsc{Margin-MSE}~\cite{hofstatter2020improving} & LDR & Bi-encoder & \textsc{DistilBERT} & Distillation \\
\textsc{TAS-B}~\cite{hofstatter2021efficiently} & LDR & Bi-encoder & \textsc{BERT} & Distillation \\
\textsc{RocketQA}~\cite{Qu2020RocketQAAO} & LDR & Bi-encoder & \textsc{ERNIE} & Hard Negatives \\
\textsc{RocketQA-v2}~\cite{ren-etal-2021-rocketqav2} & LDR & Bi-encoder & \textsc{ERNIE} & Distillation \\
\textsc{GTR}~\cite{ni-etal-2022-large} & LDR & Bi-encoder & \textsc{EncT5} & Hard Negatives \\
\textsc{TART-dual}~\cite{asai-etal-2023-task} & LDR & Bi-encoder & \textsc{Contriever} & Instruction Tuning \\
\textsc{Poly-encoder}~\cite{Humeau2020Poly-encoders} & Multi-vector & Misc & \textsc{BERT} & In-batch Negatives \\
\textsc{ME-BERT}~\cite{luan2021sparse} & Multi-vector & Bi-encoder & \textsc{BERT} & Hard Negatives \\
\textsc{ColBERT}~\cite{khattab2020colbert} & Multi-vector & Bi-encoder & \textsc{BERT} & Pairwise Loss \\
\textsc{COIL}~\cite{gao-etal-2021-coil} & Multi-vector & Bi-encoder & \textsc{BERT} & Hard Negatives\\
\textsc{ColBERT-v2}~\cite{santhanam-etal-2022-colbertv2} & Multi-vector & Bi-encoder & \textsc{BERT} & Distillation \\
\textsc{ColBERTer}~\cite{hofstatter2022introducing} & Multi-vector & Bi-encoder & \textsc{BERT} & Distillation \\
\textsc{DeepCT}~\cite{dai2019context} & LSR & Bi-encoder & \textsc{BERT} & Unsupervised \\
\textsc{SparTerm}~\cite{bai2020sparterm} & LSR & Bi-encoder & \textsc{BERT} & Hard Negatives \\
\textsc{SPLADE}~\cite{formal21splade} & LSR & Bi-encoder & \textsc{BERT} & Hard Negatives \\
\textsc{SPLADE-v2}~\cite{formal21spladev2} & LSR & Bi-encoder & \textsc{BERT} & Distillation \\
\textsc{DeepImpact}~\cite{mallia2021learning} & LSR & Bi-encoder & \textsc{BERT} & Hard Negatives \\
\textsc{SparseEmbed}~\cite{kong2023sparseembed} & LSR & Bi-encoder & \textsc{BERT} & Hard Negatives \\
\textsc{SLIM}~\cite{li2023slim}  & LSR + Multi-vector & Bi-encoder & \textsc{BERT} & Hard Negatives \\
\textsc{SLIM++}~\cite{li2023slim}  & LSR + Multi-vector & Bi-encoder & \textsc{BERT} & Distillation \\
\textsc{SPLATE}~\cite{formal2024splate} & LSR + Multi-vector & Bi-encoder & \textsc{BERT} & Distillation \\

\bottomrule
\end{tabular}
}
\caption{Summary of IR model architecture for passage retrieval and passage ranking based on pre-trained transformers. LDR and LSR denote learned dense retrieval and learned sparse retrieval, respectively. \textsc{DeepCT}~\cite{dai2019context} is trained without labeled training set. The "late interaction" mechanism introduced in~\cite{khattab2020colbert,santhanam-etal-2022-colbertv2} can be considered a special case of multi-vector architecture. Hard negatives and in-batch negatives means listwise loss function is used.}
\label{tab:bert_model_appendix}
\end{table*}

