
\section{Large Language Models for IR}
\label{sec:llm4ir}
LLMs have exhibited proficiency in language understanding and generation, are trained to align with human preferences~\cite{achiam2023gpt,team2023gemini,bai2022constitutional} and able to perform complex tasks such as reasoning~\cite{wei2022chain,guo2025deepseek} and planning~\cite{song2023llm}.\footnote{In this work, we use term LLM to denote language models which are trained for text generation, including encoder-decoder models and decoder-only models.}
In this section, we briefly discuss some works that utilize LLMs for IR tasks.

\paragraph{LLM as Retriever}
Adopting an LLM as the backbone for bi-encoder retrieval model has achieved performance improvement compared to smaller-sized models like \bert. 
\citet{neelakantan2022text} fine-tuned a series of off-the-shelf \textsc{GPT} models towards text and code representation. 
They empirically verified that the bi-encoder retriever's performance can benefit from increased backbone language model capacity. Common text retrieval benchmarks like BEIR~\cite{thakur2021beir} are currently dominated by LLM-based retrievers. 
A parallel line of research has explored adapting unidirectional LLM architectures into bidirectional ones to enhance representational power. 
\textsc{LLM2Vec}~\cite{behnamghader2024llmvec} enables bidirection and further trains \textsc{Llama}-2~\cite{touvron2023llama} with specific adaptive tasks. \textsc{NV-Embed}~\cite{lee2024nvembed} introduces a new latent attention layer and leads to improve on MTEB benchmark~\cite{muennighoff-etal-2023-mteb}.

\paragraph{LLM as Reranker}
Works discussed in~\cref{sec:transformer} have explored fine-tuning \textsc{BERT}-type encoder models as cross-encoder reranker. Later works further expand this paradigm to encoder-decoder models like \textsc{T5}~\cite{raffel2020transfer} and decoder models like \textsc{Llama}~\cite{touvron2023llama}. \citet{nogueira2020documentrankingpretrainedsequencetosequence} fine-tuned \textsc{T5} models with classification loss for passage reranking. \citet{zhuang2023rankt5} proposed to fine-tune \textsc{T5} to produce a numerical relevance score, and optimize the model with ranking losses like \textsc{RankNet}~\cite{burges2010ranknet}. \textsc{ListT5}~\cite{yoon-etal-2024-listt5} adopts the Fusion-in-Decoder architecture~\cite{izacard-grave-2021-leveraging} to learn a listwise reranker.
\textsc{RankLlama}~\cite{ma2024fine} fine-tunes decoder model for pointwise reranking and achieves better performance compared to \textsc{T5}-based rerankers.  
Leveraging the long-context ability of LLMs, a reranking paradigm is introduced, where LLM-based rerankers directly rerank a list of documents rather than scoring each document individually~\cite{ma2023zeroshotlistwisedocumentreranking,zhang2023rank,pradeep2023rankvicunazeroshotlistwisedocument,pradeep2023rankzephyreffectiverobustzeroshot}.
Instead of using raw passages, \citet{liu2024leveraging} used passage embeddings as input to LLMs and trained corresponding rerankers to achieve improved efficiency. 
Aforementioned studies still rely on labeled data and gradient updates to backbone LLMs. With the rise of instruction-following LLMs, researchers have explored using LLMs as unsupervised rerankers through prompting techniques. 
As this line of research does not introduce architectural changes to existing LLMs, we refer to a recent survey~\cite{zhu2023large} for further details.

\paragraph{Generative Retrieval}
Traditional IR systems follow the "index-retrieval-rank" paradigm~\cite{schutze2008introduction}. Although effective, jointly optimizing the separate index, retrieval, and reranking modules can be challenging. 
A recent line of research aims to bypass the indexing step by using autoregressive language models to directly generate document identifiers (DocIDs). 
\textsc{DSI}~\cite{tay2022transformer} first constructs semantically structured DocIDs, then fine-tunes \textsc{T5} models with labeled data. In the decoding phase, \textsc{DSI} uses all DocIDs to construct a trie and performs constrained decoding. 
Followup works~\cite{wang2022neuralcorpus,bevilacqua2022autoregressive} further improve upon this paradigm with strategies to construct semantic DocIDs and enable robust training. 
A significant challenge for generative retrieval is scalability to larger corpus~\cite{pradeep-etal-2023-generative}. 
\citet{zeng2024scalable} utilized the residual quantization technique~\cite{chen2010approximate} to construct hierarchical DocIDs and achieved comparable performance to dense retrievers on MS MARCO dataset. 
Generative retrieval is an active research area;~see~\cite{li2024matching} for a more comprehensive review.

\paragraph{Remarks}
We note that the adoption of LLMs in IR model architectures primarily follows two main themes: (1) feature extraction, and (2) relevance estimation, as discussed in~\cref{sec:intro}. For example, LLMs' semantic knowledge enables their strong performance in being the backbone of a retriever; and instruction-following LLMs can be directly prompted for relevance estimation. Generative retrieval and cross-encoder LLM reranking models are trained end-to-end for both feature extraction and relevance estimation. 
While LLMs have shown promise, several challenges and open questions remain, which leaves room for discussion (\cref{sec:future_direction}).