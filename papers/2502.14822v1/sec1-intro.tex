\section{Introduction}
\label{sec:intro}

\input{figures/structure}

Information Retrieval (IR) aims to retrieve relevant information sources to satisfy users' information needs. 
In the past decades, IR has become indispensable for efficiently and effectively accessing vast amounts of information across various applications.  
Beyond its traditional role,
IR now also plays a critical role in assisting large language models (LLMs) to generate grounded and factual responses.
Research in IR primarily centers on two key aspects: (1) extracting better query and document feature representations, and (2) developing more accurate relevance estimators.
The approaches for extracting query and document features have evolved from traditional term-based methods, such as boolean logic and vector space models, to modern solutions such as dense retrieval based on pre-trained language models~\cite{lin2022pretrained}.

Relevance estimators have evolved alongside advances in feature representations. Early approaches, including probabilistic and statistical language models, computed relevance with simple similarity functions based on term-based features.
Learning-to-rank (LTR) techniques later emerged, incorporating machine learning models and multi-layer neural networks for relevance estimation~\cite{li2011learning}. The success of LTR methods can be largely attributed to their extensive use of manually engineered features, derived from both statistical properties of text terms and user behavior data collected from web browsing traffic~\cite{qin2013mslr}.
In 2010s, a vast literature explored neural rerankers in different architectures to capture the semantic similarity between queries and documents.
Then pre-trained transformers, represented by \textsc{BERT}~\cite{devlin-etal-2019-bert},
quickly revolutionized the model design,
leading to an era where retrieval and ranking models adopt simpler architectures for relevance estimation, such as dot product operations and MLP layer prediction heads, which operate on learned neural representations~\cite{Karpukhin2020DensePR,nogueira2020documentrankingpretrainedsequencetosequence,lin2022pretrained}.

Recent advancements of LLMs have revolutionized applied machine learning (ML) communities, including IR. One intriguing property of LLMs is that they can be used for feature extraction and relevance estimation, achieving strong performance without extensive training~\cite[\interalia]{ni-etal-2022-sentence,neelakantan2022text,behnamghader2024llmvec,sun2023chatgpt,qin-etal-2024-large}.
The rise of LLMs in IR builds upon a rich foundation of transformer-based pre-trained language models that have evolved from earlier neural architectures. These include Transformers~\cite{vaswani2017attention}, Recurrent Neural Networks~\cite[RNN,][]{elman1990findingstructureintime}, Attention~\cite{bahdanau2014neural}
and pre-trained static neural representations such as Word2Vec~\cite{mikolov2013efficient} and GloVe~\cite{pennington-etal-2014-glove}. 
 
This work reviews the evolution of model architectures in IR (with an overview in~\cref{fig:structure}). Here, the meaning of model architecture is twofold: it describes (1) backbone models for extracting query and document feature representations, and (2) end-to-end system architectures that process raw inputs, perform feature extraction, and estimate relevance.
Different from prior works and surveys~\cite{lin2022pretrained,zhu2023large}, we intentionally separate our discussion of model architectures from training methodologies and deployment best practices to provide a focused architectural analysis.
The shift towards neural architectures, particularly Transformer-based models, has fundamentally transformed IR by enabling rich, contextualized representations and improved handling of complex queries. While this evolution has enhanced retrieval precision, it also presents new challenges, especially with the emergence of LLMs.
These challenges include the need for architectural innovations to optimize performance and scalability, handle multimodal and multilingual data, and incorporate domain-specific knowledge. Moreover, as IR systems are increasingly integrated into diverse applications\,---\,from robotics~\cite{xie2024embodiedrag}, autonomous agents~\cite{wu2023autogen} to protein structure discovery~\cite{jumper2021highly}\,---\,the field must evolve beyond traditional search paradigms. We conclude this survey by examining these challenges and discuss their implications for the future of IR model architectures research. 
