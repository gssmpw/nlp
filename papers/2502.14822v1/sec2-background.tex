\section{Background and Terminology}
\label{sec:background}

We focus on the classical \textit{ad hoc} retrieval task.

\paragraph{Task Definition and Evaluation} Given a query $\mathcal{Q}$, the task is to find a ranked list of $k$ documents, denoted as $\lbrace \mathcal{D}_1, \mathcal{D}_2, \ldots, \mathcal{D}_k \rbrace$, that exhibit the highest relevance to $\mathcal{Q}$. 
This is achieved either by \textit{retrieving} top-$k$ documents from a large collection $\mathcal{C}$ ($|\mathcal{C}| \gg |k|$ ), which typically comprises millions or billions of documents,
or by \textit{reranking} the top-$k$ candidates returned by a retriever. 
System performance is measured using standard IR metrics such as Mean Reciprocal Rank, Recall, and normalized Discounted Cumulative Gain (nDCG).

\paragraph{Query and Document}
A \textit{query} expresses an information need and serves as input to the \textit{ad hoc} retrieval system. We denote \textit{document} as the atomic unit for retrieval and ranking. Our discussions are primarily based on text-based documents, but it can also refer to a webpage or an email, depending on the actual IR application of interest.

\paragraph{Disentangling Model Architecture with Training Strategies}
Similar to other applied ML domains, the design of IR model architectures is paired with training strategies and deployment best practices.
In this paper, we seek to disentangle the two and focus solely on architectures.
We refer to prior surveys for a more focused review of training strategies and related topics~\cite{schutze2008introduction,lin2022pretrained,song2023llm}.
