\section{Related Work}
\paragraph{Computable Learnability.}
Following the work of \cite{ben2019learning}, who showed that the learnability of certain basic learning problems is undecidable within ZFC, \cite{agarwal2020learnability} formally integrated the notion of computability within the standard PAC learning framework of \cite{valiant1984theory}.
This novel set-up,  called \emph{computable} PAC (CPAC) learning, requires that both learners and the hypotheses they output be computable.
With follow-up works by \cite{sterkenburg2022characterizations} and \cite{delle2023find}, CPAC learnability in the binary classification setting was shown to be fully characterized by the finiteness of the \emph{effective} VC dimension, formally defined by \cite{delle2023find}.
Computability has since been studied in the context of different learning problems: \cite{ackerman2022computable} extended CPAC learning to continuous domains, \cite{hasrati2023computable} and \cite{delle2024effective} to online learning, and  \cite{gourdeau2024computability} to adversarially robust learning.


\paragraph{Multiclass Learnability.}
While the study of computable learnability is a very recent field of study, multiclass learnability has been the subject of extensive research efforts in the past decades.
In the binary classification setting, the VC dimension characterizes PAC learnability \citep{vapnik1971uniform,ehrenfeucht1989general,blumer1989learnability}.
However, the landscape of learnability in the multiclass setting is much more complex.
The PAC framework was extended to the multiclass setting in the works of \cite{natarajan1988two} and \cite{natarajan1989learning}, which gave a lower bound with respect to the Natarajan dimension, and an upper with the graph dimension.
Later, \cite{ben1992characterizations} generalized the notion of dimension for multiclass learning, and provided a meta-characterization of learnability: (only) dimensions that are \emph{distinguishers} characterize learnability in the finite label space setting, with distinguishers encompassing both the Natarajan and graph dimensions.
\cite{haussler1995generalization} later generalized the Sauer-Shelah-Perles Lemma for these families of functions.
\cite{daniely2015multiclass}, originally \citep{daniely2011multiclass}, identified ``good'' and ``bad'' ERMs with vastly different sample complexities, which, in the case of infinite label space, leads to learning scenarios where the ERM principle fails.
\cite{daniely2014optimal} introduced a new dimension, the DS dimension, which they proved was a necessary condition for learnability. 
The breakthrough work of \cite{brukhim2022characterization} closed the problem by showing it is also sufficient for arbitrary label space.
Different multiclass methods and learning paradigms have moreover been explored by \cite{daniely2012multiclass,rubinstein2006shifting,daniely2015inapproximability}.
Finally, multiclass PAC learning has also been studied in relation to boosting \citep{brukhim2021multiclass,brukhim2023improper,brukhim2024multiclass}, universal learning rates \citep{kalavasis2022multiclass,hanneke2023universal}, sample compression  \citep{pabbaraju2024multiclass} and regularization \citep{asilis2024regularization}.