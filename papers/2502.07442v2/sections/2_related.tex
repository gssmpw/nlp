\section{Related Work}\label{sec:related}
Documents inherently exhibit a hierarchical structure, where parent-child relationships are fundamental across various formats, such as figure-caption and section-paragraph associations in reports \cite{ding2024deep}. Accurately modeling these relationships is crucial for document structure parsing. Prior works such as DocStruct \cite{wang2020docstruct}, SERA \cite{zhang2021entity}, and KVPFormer \cite{hu2023question} have significantly contributed to this task by leveraging deep learning models for entity linking. However, these methods rely on conventional distance metrics, which may struggle to differentiate entities with highly similar feature representations \cite{duong2025scalable}.

Recent advancements in matching take inspiration from CLIP \cite{radford2021learning}, instead of cross-modal learning between text and images, document structure parsing deals with entities of the same modality. However, we also need feature representations for child $fc$ and for parent $fp$. In terms of feature separation, large margin loss functions such as LMCot~\cite{duong2022large}, ArcFace~\cite{deng2019arcface}, and NormFace~\cite{wang2017normface} improve feature extraction by enforcing larger inter-class separability while maintaining intra-class compactness.

Beyond deep learning-based approaches, heuristic rules and greedy algorithms have proven effective in competitive settings\cite{duong2025addressing}. In the next section, we detail our integration of large margin loss for matching and the heuristic rules applied in the competition.  