\section{Method}
In this section we introduce our loss function with improvements from large margin loss. And then the heuristic rules we observed to increase the performance of the model.
\label{sec:method}
\subsection{Margin loss for matching}
Starting with the loss function in CLIP \cite{radford2021learning}:
\begin{equation}
\label{eq:clip}
\small
    L_{\text{CLIP}}=-\frac{1}{N_c}\sum\limits_{i=1}^{N}{\log }\frac{ e^{ fc_{i} \times  fp_{y_i}^{T} }}{\sum\limits_{j=1}^{N_p}{{e}^{fc_{i} \times fp_{j}^{T} }}},
\end{equation}
where \( N_c \) and \( N_p \) denote the number of child entities requiring a parent and the number of potential parent entities, respectively. The feature representations of child and parent entities are denoted as \( fc_i \) and \( fp_i \), where \( i \) is the index of the entity. The term \( fp_{l_i} \) corresponds to the feature of the actual parent of the \( i \)-th child entity, where \( y_i \) indicates its parentâ€™s index. In other words, the numerator captures the similarity between a child and its correct parent, while the denominator sums over all potential parent entities.

Then we can reformulate the equation \ref{eq:clip} as:
\begin{equation}
\label{eq:clip2}
\small
    L_{\text{CLIP}}=-\frac{1}{N_c}\sum\limits_{i=1}^{N}{\log }\frac{ e^{ 
 s\left\|fc_{i}\right\| \left\|fp_{y_i}^{T}\right\|  \cos\left( \theta_{i,y_i} \right) }}{\sum\limits_{j=1}^{N_p}{{e}^{ s\left\|fc_{i}\right\|\left\|fp_{j}^{T}\right\| \cos\left( \theta_{i,j} \right) }}},
\end{equation}
where $\theta_{i,j}$ is the angle between the child feature vector $fc_{i}$ and the parent feature vector $fp_{j}$. A scaling factor \( s \) is then introduced to restore the magnitude after normalization.

Following NormFace \cite{wang2017normface}, we normalize the feature vectors so that \( \left\|fc_{i}\right\|=\left\|fp_{j}\right\|=1 \) for all \( i, j \). Building upon ArcFace \cite{deng2019arcface}, we further introduce a margin parameter \( m \), which strengthens the relationship between parent-child pairs while increasing the separation between unrelated entities. By this way, the loss function is rewritten as:

\begin{equation}
\label{eq:loss}
\small
    L_{\text{matching}}=-\frac{1}{N_c}\sum\limits_{i=1}^{N}{\log }\frac{ e^{ s \times \cos\left( \theta_{i,y_i} + m \right) }}{e^{ s \times \cos\left( \theta_{i,y_i} + m \right) } + \sum\limits_{j=1, j\neq i}^{N_p}{{e}^{ s \times \cos\left( \theta_{i,j}\right) }}},
\end{equation}

Thus, we effectively incorporate advancements in large margin cosine loss into feature matching learning. With this loss function, the feature extraction process is optimized such that, even with the margin parameter \( m \), features of related parent-child entities remain closer in the learned space compared to unrelated ones. This encourages the model to form more discriminative representations, enhancing the accuracy of hierarchical relationship inference.

\subsection{Greedy algorithms}
In addition to the loss function introduced earlier, we identified several structural patterns that improve both the accuracy and computational efficiency of our approach. These heuristics allow for a more efficient assignment of parent-child relationships, reducing the need for exhaustive pairwise comparisons.
First, entities belonging to specific categories never have a parent, including:  \texttt{abstract}, \texttt{appendix\_list}, \texttt{cross}, \texttt{figure}, \texttt{form\_title}, \texttt{list\_of\_figures}, \texttt{list\_of\_tables}, \texttt{other}, \texttt{references}, \texttt{report\_title}, \texttt{section}, \texttt{summary}, \texttt{table}, \texttt{table\_of\_contents}, and \texttt{title}.

Second, a strong hierarchical relationship exists among the categories:  
\texttt{section}, \texttt{subsection}, \texttt{subsubsection}, \texttt{subsubsubsection}, and \texttt{paragraph}. These entities follow a strict sequential structure, where each entity is assigned as a child to the nearest preceding entity in the order.

Third, specific entities exhibit predefined parent-child dependencies:  
\begin{itemize}
    \item \texttt{table\_caption} is always a child of \texttt{table}.  
    \item \texttt{figure\_caption} is always a child of \texttt{figure}.  
    \item \texttt{form} is always a child of \texttt{summary}, \texttt{abstract}, \texttt{section}, \texttt{subsection}, \texttt{subsubsection}, or \texttt{subsubsubsection}.  
    \item \texttt{list} is a child of \texttt{paragraph}, \texttt{section}, \texttt{subsection}, \texttt{subsubsection}, or \texttt{subsubsubsection}.  
    \item \texttt{form\_body} is a child of \texttt{form\_title}, \texttt{summary}, \texttt{abstract}, \texttt{section}, \texttt{subsection}, \texttt{subsubsection}, or \texttt{subsubsubsection}.  
\end{itemize}  
By enforcing these structured dependencies, we further improve the accuracy of entity relationship prediction while maintaining computational efficiency.