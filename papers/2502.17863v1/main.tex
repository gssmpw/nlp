%%%% ijcai25.tex

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

\usepackage{ijcai25}
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}
\usepackage{amssymb}
\usepackage{appendix}
\hyphenpenalty=8000

\title{A Survey: Spatiotemporal Consistency in Video Generation}
\author{
	Zhiyu Yin$^1$\and
	Kehai Chen$^1$\and
	Xuefeng Bai$^1$\and
	Ruili Jiang$^1$\and
    Juntao Li$^2$\and
    Hongdong Li$^3$\and
    Jin Liu$^3$\and
    Yang Xiang$^4$\and
    Jun Yu$^1$\And
    Min Zhang$^1$\\
\affiliations
	$^1$School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen\\
	$^2$School of Computer Science and Technology, Soochow University\\
	$^3$School of Computer Science and Technology, Central South University\\
	$^4$Peng Cheng Laboratory\\
}

   
    



\begin{document}
	\maketitle
   \begin{abstract}
    Video generation, by leveraging a dynamic visual generation method, pushes the boundaries of Artificial Intelligence Generated Content (AIGC). Video generation presents unique challenges beyond static image generation, requiring both high-quality individual frames and temporal coherence to maintain consistency across the spatiotemporal sequence. Recent works have aimed at addressing the spatiotemporal consistency issue in video generation, while few literature review has been organized from this perspective. This gap hinders a deeper understanding of the underlying mechanisms for high-quality video generation. In this survey, we systematically review the recent advances in video generation, covering five key aspects: foundation models, information representations, generation schemes, post-processing techniques, and evaluation metrics. We particularly focus on their contributions to maintaining spatiotemporal consistency. Finally, we discuss the future directions and challenges in this field, hoping to inspire further efforts to advance the development of video generation.
    \end{abstract}
    \section{Introduction}
         Artificial Intelligence Generated Content (AIGC)~\cite{yang2023diffusion,li2024survey,liu2024sora} has become a mainstream application of AI, leveraging algorithms and models to create new content that meets user needs, greatly impacting people's production and daily life. Among them, video, as an important form of visual content, rapidly displays a series of images or frames in succession, creating the perception of motion. It offers a rich and dynamic medium for expressing and transmitting information. In recent years, there have been transformative advancements in video generation fields~\cite{yang2023diffusion,li2024survey,liu2024sora,melnik2024video,xiong2024autoregressive}. Major AI institutions and companies have invested substantial resources into developing video generation products that meet basic level, such as Runway's Gen series \footnote{\url{https://runwayml.com/research/gen-1}}\footnote{\url{https://runwayml.com/research/gen-2}}\footnote{\url{https://runwayml.com/research/introducing-gen-3-alpha}}~\cite{esser2023structure}, Google DeepMind's Veo\footnote{\url{https://deepmind.google/technologies/veo/veo-1/}}\footnote{\url{https://deepmind.google/technologies/veo/veo-2/})} and OpenAI's Sora\footnote{\url{https://openai.com/sora/}}.\par
         
         Video can be viewed as a sequence of images~\cite{ho2022imagen,hong2022cogvideo}, with each image referred to as a video frame. These frames are arranged in a sequence that maintains the continuity in time. Therefore, the video generation task can be viewed as modeling a sequence of frames with temporal relationships. In this task, researchers mainly focus on two types of consistency: spatial consistency and temporal consistency~\cite{tran2015learning,zhou2019spatio,yang2023diffusion,bar2024lumiere,li2024survey,liu2024sora,xiong2024autoregressive}. Spatial consistency concerns the details and elements within each frame (such as the color, shape, position of objects, etc.), which should remain consistent across frames to prevent spatial distortions, video flickering, and other issues in the generation process. Temporal consistency, on the other hand, studies the changes between consecutive frames. The changes in elements such as objects, scenes, lighting and motion should be smooth and coherent, without abrupt jumps or unnatural variations. \par
         
         In this survey, we comprehensively review the works that aimed to maintain spatiotemporal consistency in various aspects of video generation, including foundation models~\cite{melnik2024video}, information representations~\cite{yang2023diffusion}, generation schemes~\cite{liu2024sora}, post-processing techniques~\cite{liu2024sora}, and evaluation metrics~\cite{li2024survey}. Specifically, the foundation models ensure the spatiotemporal consistency of video generation from a mathematical perspective. The information representations utilize extraction methods to obtain an efficient spatiotemporal representation. The generation schemes, combining video characteristics with foundation models and representations, allow for the creation of high-quality, coherent videos. Post-processing techniques optimize along the spatial or temporal dimensions to enhance the quality and improve visual effects of the generated videos. Evaluation metrics guide and assess the generated content from different perspectives.
         
        \paragraph{Contributions:} The contributions of this survey are primarily in three aspects. (1) We review the research progress in video generation from the perspective of spatiotemporal consistency, which is novel compared to existing video generation surveys~\cite{li2024survey,liu2024sora,xiong2024autoregressive}. (2) We summarize current advances in video generation and introduces their contributions to maintaining spatiotemporal consistency. (3) We discuss future promising directions and challenges in this field with respect to spatiotemporal consistency. Relevant reference works are listed in Appendix Table \ref{tab : table 1}. We hope that this survey will contribute to the development of advanced video generation technologies in the future.
        
        \paragraph{Sections Structure:} 
        In Section \ref{Foundation Models}, we summarize the principles and applications of four foundation generation models. In Section \ref{Information Representations}, we introduce several effective visual information representation methods. In Section \ref{Generation Schemes}, we review four video generation schemes based on video generation concepts. In Section \ref{Post-processing Techniques}, we present commonly used post-processing techniques that enhance video performance. In Section \ref{Evaluation Metrics}, we outline various video evaluation metrics. In Section \ref{Future Directions and Challenges}, we discuss the potential future directions and ongoing challenges in this field. In Section \ref{Conclusions}, we provide a summary of our work and express our aspirations for the advancement of future research.
        
    \section{Foundation Models}
    \label{Foundation Models}
        In this section, we summarize the foundation models related to video generation, including Generative Adversarial Network Model (GAN), Autoregressive Model, Diffusion Model, and Mask Model. For each model, we briefly present its basic mathematical principles and provide an explanation of how to maintain spatiotemporal consistency.
        \subsection{Generative Adversarial Network Model (GAN)}
            GAN is an unsupervised neural network model that typically consists of a generator and a discriminator~\cite{tulyakov2018mocogan,skorokhodov2022stylegan}. The generator is responsible for producing data that is difficult to distinguish from real data, while the discriminator's role is to differentiate between real and generated data. The model is trained through a zero-sum game between these two components to generate high-quality data.  
            The target optimization loss function between the generator $G$ and the discriminator $D$ can be formulated as: 
            \begin{equation} 
                \min_{G}(\max_{D}(V(G,D))), 
            \end{equation}
            where $V(G,D)$ is equal to
            \begin{equation} 
            	\Bbb{E}_{x\sim{p_{data}(x)}}[logD(x)]+\Bbb{E}_{z\sim{p_{z}(z)}}[log(1-D(G(z)))].
            \end{equation} \par
            In this setup, real data $x$ is sampled from the data distribution $p_{data}(x)$, and random data $z$ is sampled from a prior distribution $p_z (z)$ (usually a Gaussian distribution) and then input into the generator to produce generated data. The adversarial loss function~\cite{tulyakov2018mocogan} aims to continuously enhance the capabilities of the generator and discriminator through adversarial training, allowing the generator to produce data that closely resembles real data.\par
            To ensure the spatiotemporal consistency of the generated video, Saxena and Cao~\shortcite{saxena2019d} proposed a Deep GAN (D-GAN) that learns spatiotemporal features for more accurate spatiotemporal prediction. Some researchers have improved the discriminator and loss functions~\cite{zhang2022towards}, optimizing temporal modeling while considering the quality of each frame.
            
        \subsection{Autoregressive Model}
            According to the previous description, video data can be regarded as a sequence of frames. Based on the mathematical definition of autoregressive models, the generation of the current frame is conditioned on the preceding frames.~\cite{hong2022cogvideo,xiong2024autoregressive}.
            \begin{equation}
            	p(x)=\prod_{i=1}^N p(x_i \vert x_0,x_1,x_2,â€¦,x_{i-1};\theta),
            \end{equation}
            where $p(x_i \vert x_0,x_1,x_2,â€¦,x_{i-1};\theta)$ represents the conditional probability of generating the current frame based on the previous frames, and $\theta$ denotes the model parameters. The optimization objective is to minimize the negative log-likelihood (NLL) loss, which is given as:
            \begin{equation}
            	L(\theta)=\sum_{i=1}^N log(p(x_i \vert x_0,x_1,x_2,â€¦,x_{i-1}; \theta)).
            \end{equation} \par
            The autoregressive model~\cite{hong2022cogvideo,wu2022nuwaa,li2024survey,xiong2024autoregressive} ensures that generating each frame with high quality, while also captures the dependencies between frames, thus maintaining the spatiotemporal consistency of the generated video.
           
        \subsection{Diffusion Model}
            The diffusion model was initially used in the image generation domain and later transferred into video generation field~\cite{yang2023diffusion,melnik2024video}. The diffusion model defines a Markov chain of diffusion steps, where random noise is gradually added to the original data until it becomes pure Gaussian noise. The formulation is defined as:
            \begin{align}
            	q&(x_t \vert x_{t-1})=N(x_t;\sqrt{1-\beta_t}x_{t-1},\beta_tI), \\
            	q&(x_{1:T} \vert x_0)=\prod_{i=1}^T q(x_t \vert x_{t-1}),
            \end{align}
            where $\beta_t\in(0,1)$ controls the variance of the Gaussian distribution, $I$ is the identity matrix.\par
            Then, the model learns the reverse diffusion process to generate data, where the process progressively denoises to recover the original data distribution. The specific formula is as follows:
            \begin{align}
            	p_{\theta}(x_{t-1} \vert x_t)&=N(x_{t-1};\mu_{\theta}(x_t,t),\Sigma_{\theta}(x_t,t)), \\
            	p_{\theta}(x_{0:T})&=\prod_{i=1}^Tp_{\theta}(x_{t-1} \vert x_t),
            \end{align}                    
            where $\mu_{\theta}(x_t,t),\Sigma_{\theta}(x_t,t)$ are the mean and variance estimated by a neural network. \par
            This neural network is optimized by minimizing the distance between the real noise $\epsilon_t$ and the predicted noise $\epsilon_{\theta}$.
            \begin{equation}
            	L_t(\theta)=\Bbb{E}_{t\sim{[1,t]},x_t,\epsilon_t}  
            	\left\|\epsilon_t-\epsilon_{\theta}(x_t,t)\right\|^2.
            \end{equation} \par
            Some methods have been proposed to enhance the spatiotemporal consistency of the generated videos. For example, the Latent Diffusion Model~\cite{he2022latent} maps the original data into a latent feature space, enabling a more efficient spatiotemporal representation. Moreover, noise is a crucial component in diffusion models, which determines the generated content. Choosing random noise can easily lead to suboptimal results. Blattmann et al.~\shortcite{blattmann2023align} addressed this by aligning the diffusion modelâ€™s upsamplers in the temporal domain, transforming them into temporally consistent super-resolution models. The noise estimation network (usually U-Net network) determines the model's denoising capability and generation quality. Peebles and Xie~\shortcite{peebles2023scalable} replaced the U-Net backbone with a transformer (DiT), showing impressive capabilities in generating high-quality video.
                                           
        \subsection{Mask Model}
            Mask models were initially designed for natural language modeling and later applied to image and video domains~\cite{liu2024sora}. Given a time sequence $(x_0,x_1,x_2,â€¦x_i,â€¦,x_T )$, where $x_i$ can represent the basic element of language or visual data. The element of the sequence will be randomly masked, then the model is tasked with predicting the masked portions as accurately as possible to reconstruct the original sequence. By focusing on specific regions or components of the videos, the model shows efficacy in training. \par
            One notable study was the Gupta team study~\shortcite{gupta2022maskvit}, where they proposed a variable masking ratio mask model for video prediction. In the inference stage, the model gradually reduces the masking rate according to the mask scheduling function, thereby iteratively refining the generation of all the marked elements. Yu et al.~\shortcite{yu2023magvit} presented a condition video generation model using a multivariate mask. Hu and Ommer~\shortcite{hu2024mask} leveraged the mask strategy to bridge masked generation and non-autoregressive diffusion models as well as generative and discriminative tasks.
            
    \section{Information Representations}  
    \label{Information Representations}
        Video data typically have high-dimensional structures and contain a large amount of redundant information. Directly storing and processing this data can impose significant computational and storage burdens. Therefore, we need to extract deep spatiotemporal information from the original data and represent it in a low-dimensional form to facilitate model learning~\cite{zhang2020neural,yang2023diffusion,zhang2023universal,liu2024sora}. In this section, we summarize some commonly used methods for information representation.
        \subsection{Spatiotemporal Convolution}
            Convolution Network (CNN)~\cite{tran2015learning,liu2024sora} is a classic technique for extracting information from vision data. Compared to 2D image data, video data extends the time dimension. Therefore, traditional 2D CNN methods need to be modified to accommodate video information representation~\cite{tran2015learning}. For instance, using a convolution kernel of size $k_h\times k_w\times k_d$, where $k_d$ represents the time dimension, meaning that $k_d$ consecutive frames are treated as a group for convolution.\par
            Tran et al.~\shortcite{tran2015learning} proposed that the standard CNN 3D network (C3D) can learn the joint spatial and temporal features of videos.  Carreira and Zisserman~\shortcite{carreira2017quo} accelerated the training by initializing 3D convolutional layers with pre-trained 2D CNN weights from large-scale image datasets like ImageNet, thereby improving video representation performance (I3D).  Huang et al.~\shortcite{huang2021r} proposed R (2+1) D, which decomposes the 3D convolution operation into two consecutive subconvolution blocks: a 2D spatial convolution and a 1D temporal convolution, each responsible for extracting spatial and temporal information, respectively.
            
        \subsection{Spatiotemporal Patch}
            In convolution operations, the filter extracts information from a spatiotemporal region of the size of the convolution kernel. Related works have adopted this idea and introduced the concept of patch~\cite{melnik2024video}. Spatial patch~\cite{li2024survey}, originally proposed for the image domain, involves dividing the input image into patches based on pixels or feature regions. Spatiotemporal patch~\cite{liu2024sora} is improved specifically for video tasks, such as action recognition, event detection, etc. It divides the input data both spatially and temporally, enabling the representation of visual information in local spatiotemporal regions.
            
            Patch-based learning is a novel training technique~\cite{wang2024patch} which trains on patches instead of full images reducing the computational burden. The Vision Transformer (ViT)~\cite{girdhar2023omnimae} was the first to apply patches to process visual information, marking a milestone for Transformer~\cite{vaswaniAttentionAllYou2017a} applied in the Computer Vision (CV) field. Patch can also serve as the basic unit in video generation. Meta AI introduced OmniMAE~\cite{girdhar2023omnimae}, which uses a patch-based masked autoencoding approach to predict both images and videos. 

         \subsection{Self-Attention Mechanism}
            As Transformer has become the mainstream architecture, self-attention mechanism has become one of the effective strategies adopted by various models. There are mainly four commonly used self-attention mechanisms~\cite{villegas2022phenaki,liu2024sora}: spatial self-attention, temporal self-attention, spatiotemporal self-attention and causal self-attention.
            \begin{itemize}
            	\item {\bf Spatial Self-Attention:} It is a classic patch-based self-attention mechanism that calculates the similarity between a query patch and other patches in the same frame~\cite{girdhar2023omnimae}.
            	
            	\item {\bf Temporal Self-Attention:} It focuses on the temporal dimension, computing the similarity between patches at the same position across a sequence of consecutive frames~\cite{singer2022make}.
            	
            	\item {\bf Spatiotemporal Self-Attention:} It simultaneously focuses on both spatial and temporal correlations, processing the similarity between the query patch and all patches across all video frames~\cite{bar2024lumiere}. Considering the quadratic complexity of attention computation, some works adopt a window-based local attention mechanism, which only focuses on the attention computation within the same spatiotemporal window, significantly reducing the computational overhead~\cite{hong2022cogvideo}.
            	
            	\item {\bf Causal Self-Attention:} It means that each frame or patch attends to the patches within the same frame and to corresponding patches from previous frames, but it cannot attend to future frames~\cite{villegas2022phenaki}. This ensures that the sequence remains temporally consistent and avoids any future information influencing past content. Wu et al.~\shortcite{wu2022nuwab} proposed sparse causal attention, which only attends to patches in a limited number of previous frames, typically the first frame and the previous frame. NÃ¼wa~\cite{wu2022nuwaa} adopts 3D Nearby Attention (3DNA), which focuses on patches that are spatially and temporally close to each other. Villegas et al.~\shortcite{villegas2022phenaki} applied a tokenization method based on causal attention, which allows it to handle videos of variable lengths.
                
            \end{itemize}


        
            
        \subsection{Variational Autoencoder (VAE)}
     
            Autoencoders use unsupervised learning-based networks to achieve data extraction and representation. Among them, VAE~\cite{yan2021videogpt,hong2022cogvideo,song2024univst} is a mainstream type of autoencoder that introduces variational inference and optimization of latent space distribution. It includes two parts: the Encoder and the Decoder. The Encoder maps the raw data to a low-dimensional latent feature space, while the Decoder reconstructs the latent feature representation back to the original input.\par
            Yan et al.~\shortcite{yan2021videogpt} introduced VideoGPT, which uses a Vector Quantized-VAE (VQ-VAE) based on 3D CNN and axial self-attention to learn discrete representations of videos. Polyak et al.~\shortcite{polyak2024movie} proposed a temporal autoencoder (TAE), which maps raw image and video data to a spatiotemporal latent space. The model takes sampled noise and user prompts as inputs to generate latent output. In the technical analysis of the Sora model~\cite{liu2024sora}, some researchers believed that its ability to handle different forms of visual input lies in learning a unified latent representation. The video is first transformed into a low-dimensional latent space and then decomposed into a series of spatiotemporal patches. Using such latent patches helps the model better accommodate the diversity of inputs.  
            \par
            
       
            	
        \subsection{Visual Encoder}
            A visual encoder~\cite{yin2023nuwa} is a commonly used network structure in the field of CV, designed to transform input images, videos, or other visual data into feature representations. Some works combine the previously introduced techniques with the visual encoder~\cite{melnik2024video}, making it an important component of the model.\par
            Common visual encoders include ViT and CLIP. ViT~\cite{girdhar2023omnimae}is a Transformer-based encoder that divides images into patches to effectively capture the global dependencies of the image. CLIP~\cite{ge2023preserve,zhang2023i2vgen} is a joint learning model of a visual encoder and a text encoder, which uses contrastive learning between images and text to map data into a shared feature space. It is suitable for cross-modal understanding and generation tasks. Moreover, Phenaki~\cite{villegas2022phenaki} adopts the C-ViViT encoder architecture, which combines both spatial and causal attention mechanisms to effectively represent video data.      	
           
    \section{Generation Schemes}
    \label{Generation Schemes}
        The generation schemes refer to the procedure of video generation given certain foundation models and information representation methods.
        In this section, we summarize four generation schemes designed on the basis of different conceptual approaches, which can be viewed in Figure \ref{Fig : fiure 1}: the decoupled scheme based on the idea of decomposition and composition, the hierarchical scheme based on the top-down hierarchical approach, the multi-stage scheme based on gradual quality enhancement, and the latent model scheme based on latent space.

        \begin{figure*}
            \includegraphics[width=\linewidth]{scheme.png}
            \caption{Diagram of video generation schemes.} \label{Fig : fiure 1}
        \end{figure*}

        \subsection{Decoupled Scheme}
             In video generation, there are elements of both variability and invariability~\cite{liu2024sora}. Invariability mainly refers to the objects or backgrounds in the video that remain almost consistent across several frames. Variability, on the other hand, refers to changes such as the motion trajectories, lighting and other dynamic changes.
             To leverage this characteristic, decoupled scheme~\cite{liu2024sora} has been proposed for video generation. It sets up different modules to analyze and process elements of both variability and invariability independently, and then merge them to form a complete video. \par
            The two-stream network~\cite{simonyan2014two} introduced the concepts of spatial content stream and temporal motion stream, which have shown good results in video understanding and action recognition. MoCoGAN and StyleGAN-V~\cite{tulyakov2018mocogan,skorokhodov2022stylegan}separated the processed latent features into content encode and motion trajectory, which are then passed through content mapping network and motion mapping network, respectively. The two parts are synthesized into final outputs by a synthesis network. Liu et al.~\shortcite{liu2023dual} combined diffusion models with a two-stream network (DSDN) to achieve alignment between the content domain and motion domain. It improved the consistency of content changes during video generation. This method is also effective in human video generation. Text2Performer~\cite{jiang2023text2performer}introduced two novel designs: decomposed human representation and diffusion-based motion sampler, which together enable the generation of flexible and diverse human videos. 
            
        \subsection{Hierarchical Scheme}
            In the field of video generation, some researchers have borrowed the idea of divide-and-conquer and proposed hierarchical architectures for video generation~\cite{hong2022cogvideo,li2024survey}. First, the global model roughly outlines the storyline of the video, such as the key frame sequences. Then the local model focuses on the details, performing alignment, inpainting, and refinement. \par
            Wu et al.~\shortcite{wu2022nuwab} presented NUWA-Infinity, an autoregressive-over-autoregressive video generation architecture. In this architecture, the global autoregressive model considers the dependencies between patches, while the local autoregressive model focuses on the dependencies of visual tokens within each patch, enabling the generation of globally consistent and locally detailed high-quality videos. Yin et al.~\shortcite{yin2023nuwa} proposed NUWA-XL, which adopts a diffusion-over-diffusion architecture. The global diffusion model is first used to generate key frames, and then local diffusion is applied iteratively to complete intermediate frames, allowing the video length to increase exponentially. Skorokhodov et al.~\shortcite{skorokhodov2024hierarchical} introduced a novel hierarchical strategy, Patch Diffusion Models (PDMs), which do not operate on full-resolution inputs but instead propagate context information from low-scale to high-scale patches in a hierarchical manner, ensuring global consistency.
            
        \subsection{Multi-Staged Scheme}
            Directly generating high-resolution, high-quality videos typically requires significant computational and time costs~\cite{li2024survey}. Therefore, some researchers have proposed stage-wise schemes to alleviate this issue. In these schemes, video generation is a multi-staged process~\cite{ho2022imagen}. The first stage generates a set of low-quality initial frames, and subsequent stages gradually improve the videoâ€™s quality, resolution, and frame rate based on these initial frames and user prompts. \par
            Cascade model~\cite{singer2022make,ge2023preserve} is an effective application of stage-wise scheme. In this model, the base module generates a sparse and low-resolution frame sequence. Then, a series of refinement modules, such as super-resolution generation modules for spatial and temporal dimensions, are applied to enhance the resolution and frame rate of the output frames. Zhang et al.~\shortcite{zhang2023i2vgen} proposed I2VGen-XL, a cascade model that decouples video semantic accuracy and exceptional quality to enhance performance. Specifically, the base stage uses a CLIP visual encoder to ensure high semantic alignment. Then, the refinement stage improves the video's resolution and enhances the spatiotemporal continuity and clarity of the video.
            
        \subsection{Latent Model Scheme} 
            The current mainstream video generation architectures do not directly generate videos based on pixels or patches, but instead use information representation techniques (e.g., VAE) to map the original data to a latent feature space~\cite{yan2021videogpt,he2022latent,liu2024sora,polyak2024movie}. 
            This scheme generally consists of three parts: the encoder module, the decoder module, and the backbone network module. The encoder and decoder module apply the visual information representation techniques mentioned earlier to represent and reconstruct the video data. The backbone network module is often combined with models such as GAN models, autoregressive models, and diffusion models~\cite{he2022latent,skorokhodov2022stylegan,wu2022nuwab,skorokhodov2024hierarchical} to process the latent features. Latent models are better at maintaining the spatiotemporal consistency of videos, especially in tasks involving long videos and complex scenes~\cite{li2024survey}. By modeling in the latent space, they can effectively capture the dependencies between video frames and temporal features, thereby generating smoother and more coherent video content.\par
            For conditional generation tasks, such as the Text-to-Video Generation (T2V) task, a control module will be added to the scheme~\cite{zhang2023i2vgen}. It maps the text condition into a shared latent space with visual information by using a pre-trained language model like CLIP ~\cite{xiong2024autoregressive}. This allows for the alignment of text semantics and visual content, enabling video content generation with better flexibility and controllability.\par
            
    \section{Post-processing Techniques}
    \label{Post-processing Techniques}
        The post-processing techniques for video generation primarily aim to improve the quality of the generated video or modify its style and lighting to enhance its visual effects. This section summarizes some post-processing techniques, including frame interpolation, video super-resolution, video stabilization, deblurring, video stylization and relighting.
        \subsection{Frame Interpolation}
            Frame interpolation~\cite{ge2023preserve,singer2022make} is a common CV algorithm used to insert frames into a video to increase the frame rate and enhance the coherence and smoothness of the video. Common frame interpolation techniques include Optical Flow methods and Deep Learning methods~\cite{ho2022imagen}.\par
            The basic principle of the Optical Flow method is to predict the position of pixels in the next frame by analyzing the pixel changes between adjacent frames. Xue et al.~\shortcite{xue2019video} proposed task-oriented flow, a motion representation, which is learned through joint training to adapt to specific tasks like frame interpolation and super-resolution. \par
            Deep Learning-based frame interpolation is typically based on CNNs or GANs. CNNs ~\cite{ho2022imagen,ge2023preserve} learn the features of adjacent frames to generate high-quality interpolation frames. GANs~\cite{zhang2022towards}, on the other hand, use adversarial learning to continually optimize the generator's performance, reducing distortions and blurring, thus improving the interpolation results.
            
        \subsection{Video Super-Resolution}
            Super-resolution technology~\cite{ho2022imagen}, also known as super-sampling, enhances the resolution of video frames using hardware or software methods, converting low-resolution images into high-resolution ones. In this field, super-resolution techniques are generally classified into two categories: spatial-based and temporal-based~\cite{ho2022imagen,singer2022make,ge2023preserve}.
            \begin{itemize}
            	\item  {\bf Spatial-based super-resolution (SSR).}~\cite{ho2022imagen} This method focuses on enhancing the resolution of each frame in the video. It aims to improve the spatial detail in each frame by using techniques such as CNNs.
            	
            	\item {\bf Temporal-based super-resolution (TSR).}~\cite{singer2022make} This approach leverages the information across multiple frames over time to improve the resolution. Temporal-based super-resolution methods typically enhance the frame rate and detail by utilizing motion information and temporal coherence, thus providing smoother high-resolution outputs.
            \end{itemize}
                      
        \subsection{Video Stabilization}
            Video jittering is one of the common issues in video generation, referring to unstable frame variations in the video~\cite{tulyakov2018mocogan,skorokhodov2022stylegan}. It typically manifests as sudden displacements, rotations, or distortions, which disrupt the coherence between frames and reduce the overall video quality. As a result, video stabilization techniques are proposed to mitigate the jittering effects in videos.\par
            Zhang et al.~\shortcite{zhang2022towards} conducted a study on video generation across different time spans (short-range, medium-range, and long-range), finding noticeable periodic jittering in long-video generation from previous works. To address this, they introduced B-spline controlled interpolation and low-rank constraints, which help alleviate the jittering phenomenon. Yang et al.~\shortcite{yang2023rerender} combined various advanced techniques to improve video rendering and enhancement functions. The work employed motion estimation and compensation algorithms to identify and eliminate jitter in videos, thereby improving video stability.

        \subsection{Deblurring}
            Video deblurring~\cite{liu2024sora} is a method to improve video quality and enhance video resolution, with the main goal of recovering clear images from blurred video frames. Video deblurring is widely utilized in high-resolution imaging applications~\cite{he2025domain}, such as medical imaging, film generation, and other fields, to improve the viewing experience.\par
            Zhou et al.~\shortcite{zhou2019spatio} proposed the Spatiotemporal Filter Adaptive Network (STFAN), which extracts blur features from the previous blurred and restored images, and aligns them with the current frame to remove the blur caused by changes in the feature space. He et al.~\shortcite{he2025domain} proposed a domain-adaptive deblurring model approach, which extracts relatively clear regions as pseudo-clear images. The model then applies the deblurring technique in combination with the pseudo-clear images to generate blurred images, achieving domain adaptation effects for unseen domains. 
            
        \subsection{Video Stylization}
            Generating stylized videos~\cite{skorokhodov2022stylegan} involves using deep learning and CV techniques to transfer an artistic style to the video content, creating visually appealing effects. Stylized videos must maintain consistency in the style across each frame while ensuring temporal consistency throughout the video to avoid abrupt jumps or discontinuities in the visual flow.\par
            Song et al.~\shortcite{song2024univst} proposed a unified framework for localized video style transfer. The work used a training-free video style transfer mechanism that operates mainly in the latent and attention layers, which reduces detail loss while ensuring content consistency. Ye et al.~\shortcite{ye2024stylemaster} proposed a video stylization generation and transformation method called StyleMaster. This approach simultaneously considers global style and local textures, enhancing the consistency of the style. For video content, a gray-block control network was introduced to achieve content control.
            
        \subsection{Relighting}
            Video relighting~\cite{liu2024sora} is also an emerging direction in the field of CV, especially in portrait videos. It involves performing 3D perception on portrait videos and transforming 2D facial features into a 3D relightable representation. This enables re-rendering portrait videos under different viewing angles and lighting conditions. The process also requires maintaining spatiotemporal consistency.\par
            Choi et al.~\shortcite{choi2023personalized} developed a personalized video relighting network architecture that can effectively separate intrinsic appearance features, such as facial shape, from the source lighting. These features are then combined with target lighting to generate relighted images. Cai et al.~\shortcite{cai2024real} proposed the first real-time 3D perception-based portrait video relighting method. Their approach uses a three-plane dual encoder to encode geometric materials and portrait lighting effects separately, and then a special network optimizes the temporal consistency of the video.
            
    \section{Evaluation Metrics}
    \label{Evaluation Metrics}
        This section summarizes some evaluation metrics for video generation and divides them into three categories: generation quality assessment, video smooth assessment, and user subjective evaluation~\cite{jiangkuo2024chinese,liu2024sora}. Among these, generation quality assessment focuses more on spatial consistency, smooth assessment emphasizes temporal consistency, and user evaluation centers on subjective user experience. 
        \subsection{Generation Quality Assessment}
        \paragraph{PSNR (Peak Signal-to-Noise Ratio):}
          PSNR~\cite{he2025domain} is a widely used metric that measures the quality of the generated image by comparing it with the original image. It is generally represented as the ratio of the maximum pixel value of the original image to the difference in pixel values between the original and generated images.
        \paragraph{SSIM (Structural Similarity Index):}
          SSIM~\cite{li2024survey} is an image quality metric that considers the changes in brightness, contrast, and structural information between the original and generated images.
        \paragraph{FrÃ©chet Inception Distance (FID):}
          FID~\cite{blattmann2023align} uses a network to extract abstract features of images and analyzes the distance between the generated and original images in feature space, reflecting the distance between the generated distribution and the original distribution.
        \paragraph{FVD (FrÃ©chet Video Distance):}
          FVD~\cite{bar2024lumiere} is an extension of FID in the video domain, replacing the feature extraction network with a video feature extraction network, evaluating the distance between the generated video and the original video.
        \paragraph{Inception Score (IS):}
          IS~\cite{bar2024lumiere} measures the diversity and authenticity of generated images.
        \subsection{Video Smoothness Assessment}
        \paragraph{Temporal Consistency:}
          Temporal consistency~\cite{cai2024real} assessment introduces Time Change Consistency (TCC) and Temporal Motion Consistency (TMC), which means maintaining the consistency of changes and motions (optical flow) between consecutive frames in the generated video, aligning with the corresponding true depth.  
        \paragraph{FPS (Frames Per Second):}
          FPS~\cite{bar2024lumiere} measures the frame rate of the video. The generation process typically needs to meet certain frame rate requirements to achieve a smooth video.  
        \subsection{User Subjective Evaluation}
        \paragraph{Mean Opinion Score (MOS):}
          MOS~\cite{li2024survey} is the arithmetic average of the user's subjective ratings for the generated video.

    \section{Future Directions and Challenges}
    \label{Future Directions and Challenges}
        Recently, with the growing user demand, video generation is increasingly moving towards more complex and diverse directions, bringing both opportunities and challenges. 
        \paragraph{Long Video Generation.} Extending the length of generated videos is a growing trend in development~\cite{li2024survey}. As the length increases, the videos will contain more redundant and variable visual information. For example, generating movie or game videos requires addressing consistency issues across long time spans, which brings significant challenges in terms of both computational time and space consumption. 
        \paragraph{Personalized Video Generation.} Personalized generation~\cite{ye2024stylemaster} needs to align with the user's specific needs or preferences, controlling the details of the generated video. This requires the models to better understand user prompts and generate more detailed content. It will bring more complicated issues of consistency.
        \paragraph{Video Emotion Expression.} For users, video is not only a medium for conveying information but also a way of expressing emotions~\cite{liu2024sora}. Whether the generated video can truly convey human emotions and evoke emotional resonance from the audience will determine whether AIGC can genuinely replace humans in the generation task. 
        \paragraph{Video Generation Evaluation.} Current evaluation metrics for video generation~\cite{li2024survey} are mostly borrowed from the image field. They overlook the temporal information in videos and are unable to evaluate dynamic content. Therefore, a complete and comprehensive video evaluation system is urgently needed in this field. 
             
    \section{Conclusions}
    \label{Conclusions}
        Spatial and temporal consistency is one of the key challenges in the video generation process. In this survey, we provide a comprehensive summary of recent video generation techniques from the perspective of spatiotemporal consistency and analyzes their contributions to maintaining this consistency. We conclude them in the following: 1) efficient spatiotemporal information representation is beneficial for complex video generation tasks; 2) the design of the video generation pipeline needs to balance both spatial consistency and temporal consistency; 3) video generation is gradually evolving toward more complex and refined directions, which will emerge both new opportunities and challenges. We hope that our work will make a meaningful contribution to the future of video generation.
        \par
        
     \bibliographystyle{Style}
     \bibliography{Reference}
        
     \newpage
     \onecolumn
     \appendix
     \section{Related Works}
         \begin{table*}[htbp]
     	
     	\begin{tabular}{cccccc}
     		\toprule
     		Model Name      & Year & Representation & Backbone        & Task                  & Group                \\
     		\midrule
     		MoCoGAN    		& 2018 &       -        &       GAN       & Generation            & Snap                 \\
     		STFAN      		& 2019 &       -        &    CNN-based    & Deblurring            & SenseTime            \\             
     		VideoGPT   		& 2021 &    VQ-VAE      & Autoregressive  & Generation            & UC Berkeley          \\
     		NUWA	   		& 2021 &  BPE, VQ-VAE   & Autoregressive  & Generation	          & Microsoft, PKU       \\
     		StyleGAN-V 		& 2021 &   GAN, CNN	    &       GAN	      & Generation	          &   KAUST              \\
     		OmniMAE	   		& 2022 &   ViT-based	&    Mask Model	  & Visual Architecture   &   Meta AI            \\
     		Make-A-Video	& 2022 &   BPE, CLIP	&    Diffusion	  & Generation	          &   Meta AI            \\
     		LVDM	        & 2022 &    3D CNN	    &    Diffusion	  & Generation	          &   HKUST              \\
     		CogVideo	    & 2022 & 	VQ-VAE	    &   Transformer	  & Generation	          &    THU               \\
     		Tune-A-Video	& 2022 &       -	    &    Diffusion    &	Generation, Editing   &    NUS               \\
     		Phenaki	        & 2022 &  C-ViViT, T5X	&    Mask Model	  & Generation	          &  Google Brain        \\
     		NUWA-Infinity	& 2022 &	VQGAN	    & Autoregressive  & Generation	          &  Microsoft           \\
     		Imagen-Video	& 2022 &	 T5	        &    Diffusion	  & Generation	          &  Google              \\
     		MaskViT	        & 2022 &	VQGAN	    &    Mask Model	  & Prediction	          &  Stanford            \\
     		MAGVIT	        & 2022 &	3D-VQ	    &    Mask Model	  &  Synthesis	          &    CMU               \\
     		Gen-1	        & 2023 &  MiDaS, CLIP	&    Diffusion	  &   Editing	          &   Runway             \\
     		Gen-2	        & 2023 &      -	        &        -	      & Generation, Editing	  &   Runway             \\
     		PYoCo	        & 2023 &   T5, CLIP	    &    Diffusion	  & Generation, Synthesis &	University of Maryland \\
     		Text2Performer	& 2023 &	VQ-VAE	    &    Diffusion	  & Generation	          &    NTU               \\
     		Video LDM	    & 2023 &	3D-CNN	    &    Diffusion	  & Synthesis	          &  LMU Munich          \\
     		DSDN	        & 2023 & 	VQ-VAE	    &    Diffusion	  & Generation	          &    NJUST             \\
     		NUWA-XL	        & 2023 &  CLIP, T-KLVAE	&    Diffusion	  & Generation	          & USTC, Microsoft      \\
     		Rerender A Video& 2023 &	VQ-VAE	    &    Diffusion	  &   Editing	          &    NTU               \\
     		LCFN	        & 2023 &	U-Net	    &      U-Net	  & Relighting	          &    UNC               \\
     		I2VGen-XL	    & 2023 &	VQGAN, CLIP	&    Diffusion	  & Synthesis	          &   Alibaba            \\
     		UniVST	        & 2024 &	VAE	        &    Diffusion	  &   Stylize	          &    XMU               \\
     		Lumiere	        & 2024 &   ST-Unet	    &    Diffusion	  & Generation	          &   Google             \\
     		HPDM	        & 2024 & 	DCF	        &    Diffusion	  & Generation	          &    Snap              \\
     		Movie Gen	    & 2024 & 	TAE	        &   Transformer	  & Generation, Editing	  &   Meta AI            \\
     		Sora	        & 2024 &    VAE	        &    Diffusion	  & Generation, Editing	  &   OpenAI             \\
     		StyleMaster	    & 2024 &    CLIP	    &    Diffusion	  &   Stylize	          &    HKUST             \\
     		DBCGM	        & 2024 &     -	        &    Diffusion	  & Deblurring            & 	NYCU             \\
     		
     		\bottomrule
     	\end{tabular}
     	\caption{Summary of Video Works.}
     	\label{tab : table 1}
     \end{table*}

\end{document}