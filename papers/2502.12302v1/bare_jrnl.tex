%% bare_jrnl.tex
%% V1.4a
%% 2014/09/17
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8a or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE!
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_conf_compsoc.tex,
%%                    bare_jrnl_compsoc.tex, bare_jrnl_transmag.tex
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices and paper sizes can       ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/

\newif\ifarxiv
\arxivtrue % Uncomment to compile the document for arXiv, comment it out if IEEE

\ifarxiv
\documentclass[twocolumn]{article}
\newcommand{\IEEEmembership}[1]{}
\newenvironment{IEEEkeywords}{%
  \bfseries\textit{Index Terms}---%
}{%
}
\newenvironment{IEEEbiography}[2][]{}{}

\usepackage[letterpaper, top=0.6in, bottom=0.5in, inner=0.75in, outer=0.75in, headheight=0.3in, footskip=0.3in]{geometry}
\usepackage{fancyhdr}
\usepackage{times}
\usepackage{booktabs}
% Define the footer style
\fancyhf{} % clear all header and footer fields
\fancyfoot[C]{\thepage} % except the center
\renewcommand{\headrulewidth}{0pt} % no line in header area
\renewcommand{\footrulewidth}{0pt} % no line in footer area
\pagestyle{fancy}

\else
    \documentclass[journal]{IEEEtran}
\fi


\usepackage[pdftex]{graphicx}
\usepackage[cmex10]{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{xcolor}
\usepackage{array}
\usepackage{booktabs}
\usepackage{stfloats}
\usepackage{hyperref}
\usepackage{url}
\usepackage{verbatim}
\usepackage[sort&compress, numbers]{natbib}
\usepackage{caption}
\usepackage{tablefootnote}
\usepackage{amssymb}
\captionsetup[table]{skip=10pt}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{dirtytalk}


% correct bad hyphenation here
%\hyphenation{}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Chaotic Map based Compression Approach to Classification}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

\ifarxiv
   \author{Harikrishnan~N~B\thanks{Harikrishnan~N~B is with Department of Computer Science and Information Systems, BITS Pilani K K Birla Goa Campus, 403726, Goa, India (e-mail: harikrishnannb@goa.bits-pilani.ac.in)\\
   Also at: Adjunct Faculty, Consciousness Studies Programme, National Institute of Advanced Studies, Indian Institute of Science Campus, Bengaluru, 560012.}, 
  Anuja~Vats\thanks{Anuja Vats is with the Department of Computer Science, Norwegian Institute of Science and Technology, Gjøvik, Norway (e-mail: anuja.vats@ntnu.no)}, 
  Nithin~Nagaraj\thanks{Nithin~Nagaraj is with the Complex Systems Programme, National Institute of Advanced Studies, Indian Institute of Science Campus, Bengaluru, 560012, Karnataka, India (e-mail: nithin@nias.res.in)}, 
  Marius~Pedersen\thanks{Marius Pedersen is also with the Department of Computer Science, Norwegian Institute of Science and Technology, Gjøvik, Norway (e-mail: marius.pedersen@ntnu.no)}, }% <-this % stops a space
    
\else
    

\fi

% note the % following the last \IEEEmembership and also \thanks -
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
%
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~13, No.~9, September~2014}%
{Vats \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
%
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2014 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
Modern machine learning approaches often prioritize performance at the cost of increased complexity, computational demands, and reduced interpretability. This paper introduces a novel framework that challenges this trend by reinterpreting learning from an information-theoretic perspective, viewing it as a search for encoding schemes that capture intrinsic data structures through compact representations. Rather than following the conventional approach of fitting data to complex models, we propose a fundamentally different method that maps data to intervals of initial conditions in a dynamical system. Our GLS (Generalized L\"uroth Series) coding compression classifier employs skew tent maps - a class of chaotic maps - both for encoding data into initial conditions and for subsequent recovery. The effectiveness of this simple framework is noteworthy, with performance closely approaching that of well-established machine learning methods. On the breast cancer dataset, our approach achieves 92.98\% accuracy, comparable to Naive Bayes at 94.74\%. While these results do not exceed state-of-the-art performance, the significance of our contribution lies not in outperforming existing methods but in demonstrating that a fundamentally simpler, more interpretable approach can achieve competitive results. 
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Compression, Generalized L\"uroth Series, Learning, Minimum Description Length, Coding Theory
\end{IEEEkeywords}




\section{Introduction}\label{sec:introduction}
Any data-driven learning fundamentally constitutes an optimization process of encoding the information available in the data. This encoding is motivated by two constraints: maximizing representational fidelity and minimizing encoding length. 
Formally, an optimal encoder $E$ possesses two critical properties: Information Preservation: $E$ faithfully captures the essential statistical characteristics and structural regularities inherent in the data.
Descriptive Efficiency: $E$ enables significant bit-level compression by exploiting fundamental data redundancies and regularities.

The pursuit of such encodings is mathematically formalized through the Minimum Description Length (MDL) principle \cite{rissanen1978modeling} (an information-theoretic reformulation of Occam's razor, \cite{solomonoff1964formal}). MDL attempts to quantify the trade-off between model complexity and representational accuracy by minimizing the total description length—comprising both of the model's complexity and the data encoding.
Conceptually, this reduces to the fact that the most informative model is one that achieves maximal data compression while maintaining predictive capacity.

The MDL principle, although biologically inspired (\cite{motiwala2022efficient, chater2003simplicity}) and known to be theoretically optimal for model selection in machine learning (\citep{hansen2001model}), has not been adopted as the primary motivation for model training and representation learning. Traditionally, model selection often prioritizes alternative criteria for achieving optimal fit, rather than adhering to the principle of minimizing the description length of the model or its encoded representation. Regardless, MDL is the theoretical foundation for common loss functions. The cross-entropy loss used in classification tasks is an MDL optimal encoding of the data given the model (\cite{hinton1993keeping, adriaans2007mdl, marton2005compression}). Specifically, for a probabilistic model $h$, the probability $- \sum_i^N log_2 p(y_{i}|x_{i})$, which corresponds the categorical cross-entropy loss turns out to be the optimal cost associated with transmitting the labels y with this codelength. Similarly, the squared error loss corresponds to MDL coding under Gaussian assumptions (\cite{galland2003mdl, hinton1993keeping}). Despite this, the conventional method of estimating MDL code lengths through variational inference technique originally proposed to minimize description complexity in \cite{hinton1993keeping}—has demonstrated low efficiency (\cite{blier2018description}). Similarly, while techniques such as regularization (\cite{saito1997mdl}) implicitly implement a form of MDL by penalizing overly complex models, fewer approaches target explicit search for simpler models or data compression (\cite{marton2005compression}) during training (\cite{blier2018description, lan2022minimum}). This is particularly challenging because modern learning is synonymous with machine learning and deep learning frameworks which are inherently complex by design, with simplification efforts aiming to sparsify within a vast parameter space.

Thus, algorithmic approaches to data encoding and representation learning currently occupy a small subspace within the landscape of potential learning methodologies that simultaneously prioritize succinctness and informational sufficiency. 
In this paper we challenge the currently pravelant notion of learning from complex models and introduce an extremely simple approach to learning, especially in terms of model complexity and interpretability. We show that by reinterpreting learning from the lens of information theory and Chaos, the complexity of big models can be reduced to search for good initial conditions. Our proposed methodology consists of a model that is described by a very small number of parameters that scale linearly with the number of classes, precisely the parameter count is \(4 \times \text{number of classes}\) (so \(4 \times 3 \) for Iris dataset), with only one hyperparameter for learning. 
This simplification becomes possible through a novel reinterpretation; by treating data generation as a dynamical chaotic system and the datapoints itself as an orbit. Within this framework, the modeling process is reconceptualized as estimating the system's initial conditions necessary to recovering the orbit. Under MDL, the optimal initial condition is one that represents the data with the most compact description length.

\subsection*{Data generation process as a chaotic dynamical system }
As stated before, in this work rather than relying on parametric distributions or complex neural architectures, we reconceptualize the data generating process itself as a chaotic dynamical system. 
%In this framework, similar data points emerge from nearby initial conditions in the system's phase space. 
This approach leverages fundamental properties of chaotic systems - including topological transitivity, dense periodic orbits, and strange attractors - which naturally align with observed patterns in real-world (\cite{soramaki2007topology, stein2013ecological, palmer1993extended}).
 
% In our framework, each observed data point represents a state in the phase space of a chaotic dynamical system at time t. \av{Check this : Crucially, we hypothesize that all instances of a particular object class (e.g., different examples of trees) originate from a connected interval of initial conditions in the system's phase space. This interval forms a natural "seed region" for that class, with the chaotic dynamics transforming these seeds into the diverse but related observations we ultimately observe.} The presence of stable attracting orbits ensures that despite the system's sensitivity to initial conditions, trajectories from similar initial states maintain certain shared characteristics - mirroring how objects within a class share common features while exhibiting natural variation.
This interpretation fundamentally reframes the inference problem: rather than fitting a complex function, we seek to reverse-engineer the interval of initial conditions that gave rise to observed data points through careful backward iteration of the dynamical system. Under the MDL principle, this interval serves as maximally efficient codes for the data, as they capture the generative essence of each observation. 
The contributions of this work are : 
\begin{itemize}
    \item We introduce a novel framework that implements MDL principles through chaotic dynamical systems, reframing data generation and representation learning in terms of initial conditions and their evolution.
    \item Through simple and well-known techniques such as GLS Coding and back-iteration for learning and inference, our method
    \begin{itemize}
        \item Requires significantly less description length for both model specification and data encoding
        % \item Achieves comparable performance to complex machine learning models on several benchmark datasets \av{here write results on datasets, like our method of cancer dataset acheive x\% accuracy, and clses the gap with SVM/other etc etc.}
         \item Provides better interpretability through its compact parameter space.
    \end{itemize}
\item We provide an initial exploration into why chaotic dynamics can effectively capture complex data patterns through simple initial conditions, establishing a connection between dynamical systems theory and efficient data representation.
\end{itemize}

\section{Proposed Method}\label{sec:proposed_method}
In this section, we present the necessary background to understand the proposed method. We begin with an overview of Generalized L\"uroth Series (GLS) coding and explain its Shannon optimality under the assumption of independence. Following this, we introduce the proposed method in detail.
\subsection{GLS Coding}
GLS maps are chaotic, piecewise linear maps. Notable examples include binary maps and skew tent maps. In this work, we employ the skew tent map for GLS coding, defined mathematically as:  

\begin{equation}
T(x) =
\begin{cases} 
\frac{x}{b}, & 0 \leq x < b \\
\frac{(1 - x)}{1-b}, & b \leq x < 1,
\end{cases}
\end{equation}  

where \( b \in (0,1) \) controls the skewness of the tent map. Figure~\ref{fig:tent_map} illustrates the map for \( b = 0.3 \).  

\begin{figure}[h]
    \centering
    \includegraphics[width=0.35\textwidth]{tent_map.png}
    \caption{Plot of the skew tent map with \( b=0.3 \).}
    \label{fig:tent_map}
\end{figure}  

The trajectory of the tent map, starting from an initial value \( x_0 \), is defined as:  
\[
x_0 \rightarrow T(x_0) \rightarrow T^2(x_0) \rightarrow \dots \rightarrow T^n(x_0) \rightarrow \dots
\]
where \( T^n(x_0) = T(T^{n-1}(x_0)) \). The trajectory can be denoted as:  
\[
x(t) = [x_0, T(x_0), T^2(x_0), \dots, T^n(x_0), \dots]
\]
This sequence is obtained through \textbf{forward iteration}. Figure~\ref{fig:tent_map_trajectory} depicts the first 100 values of the trajectory after discarding the initial 500 transient iterations.  

\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{trajectory.png}
    \caption{Trajectory of the skew tent map for the first 100 values after removing the initial 500 transients. The parameter \( b \) and initial value \( x_0 \) are 0.34 and 0.26, respectively.}
    \label{fig:tent_map_trajectory}
\end{figure}  

The trajectory \( x(t) \) can be mapped to a symbolic sequence \( SS(t) \), defined as:  
\begin{eqnarray}
SS(t_i) =
\begin{cases} 
0, &  x(t_i) < b,  \\
1, & b \leq x(t_i) < 1.\\
\end{cases}
\end{eqnarray}
Given any binary symbolic sequence \( SS(t) \) of length \( N \), we determine the interval containing the initial condition \( x_0 \) using \textbf{backward iteration} on the skew tent map, as outlined in Algorithm~\ref{alg:backiteration}. For backward iteration, the parameter $b$ has to be specified. \begin{algorithm}[!ht]
\caption{Initial Condition Reconstruction via Skew Tent Map Backward Iteration}
\label{alg:backiteration}
\begin{algorithmic}
\Require Symbolic sequence $z = (z_1, z_2, \ldots, z_n)$, parameter $b$
% \Ensure Initial condition $x_0$
\State $SS \gets (z_n, z_{n-1}, \ldots, z_1)$  \Comment{reverse sequence}
\If{$SS_0 = 0$}
\State $L \gets 0$, $U \gets b$
\Else
\State $L \gets b$, $U \gets 1$
\EndIf
\For{$i \gets 1$ to $n$}
\If{$SS_i = 0$}
\State $L \gets L \cdot b$
\State $U \gets U \cdot b$
\Else
\State $L \gets L \cdot b - L + 1$
\State $U \gets U \cdot b - U + 1$
\EndIf
\If{$L > U$}
    \State swap($L$, $U$)
\EndIf
\EndFor
\State \Return $x_0 \gets \frac{L + U}{2}, \textbf{Interval} [L, U]$
\end{algorithmic}
\end{algorithm}


The skewness parameter \( b \) is chosen as 
\[
b = \frac{n_{\text{zeros}}}{N},
\]

where \( n_{\text{zeros}} \) is the number of zeros in \( SS(t) \).  

Backward iterating on the sequence yields an interval \([L, U]\) such that any \( x_0 \in [L, U] \) reproduces the exact symbolic sequence under forward iteration. A common choice for \( x_0 \) is the midpoint:  
\[
x_0 = \frac{L + U}{2}.
\]
The initial condition \( x_0 \) is then iterated forward under the skew tent map for \( N \) steps, generating a chaotic trajectory \( x(t) \):  
\[
x_0 \rightarrow T(x_0) \rightarrow T^2(x_0) \rightarrow \dots \rightarrow T^N(x_0).
\]
The corresponding symbolic sequence \( SS^*(t) \) is obtained using the thresholding rule:  
\begin{equation}
SS^*(t_i) =
\begin{cases} 
0, &  x(t_i) < b,  \\
1, & b \leq x(t_i) < 1.\\
\end{cases}
\end{equation}
Since \( x_0 \) was chosen from the computed interval, forward iteration for \( N \) steps reconstructs the original sequence exactly, i.e.,  

\[
SS^*(t) = SS(t).
\]

In theory, any value of \( b \in (0,1) \) can be used to reconstruct the symbolic sequence without error. However, as shown in~\cite{nagaraj2009arithmetic}, selecting  

\[
b = \frac{n_{\text{zeros}}}{N}
\]

is Shannon-optimal. This optimality is established by analyzing the number of bits required to encode the interval \([L, U]\), given by $
\lceil -\log_2 (U - L) \rceil$~\cite{nagaraj2009arithmetic}. This can be empirically understood through a simple experiment. Consider that a binary sequence of fixed length (\( N = 20 \)) is generated randomly, where the probability of a zero is set to \( 0.45 \). This ensures that the generated sequences maintain the given probability distribution statistically. The experiment iterates over different values of \( b \) in the range starting from $0.01$ to $0.99$ using 100 linearly spaced values. For each \( b \), $100$ random symbolic sequences are generated. Using the \textbf{backward iteration} method on the skew tent map, the interval \([L, U]\) containing the initial condition is determined. The \textbf{compressed file size} is computed using the formula: $\lceil -\log_2(U - L) \rceil$ as mentioned in~\cite{nagaraj2009arithmetic}. The average compressed size is then calculated across the 100 trials for each \( b \).

The graph showing the relationship between average compressed file size vs $b$ is depicted in Figure~\ref{fig:compressed_file_size}. The optimal \( b = 0.45 \) (equal to the probability of zeros in the sequence) is highlighted with a red dashed vertical line. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{compressed-file-size.png}
    \caption{Plot of the average compressed file size as a function of the skew parameter $b$. 
    The red dashed line highlights the optimal value \( b = 0.45 \), corresponding to the empirical probability of zeros in the symbolic sequence.}
    \label{fig:compressed_file_size}
\end{figure} 

The compressed file size is expected to be minimized around this optimal \( b \), as established in prior research on Shannon-optimal coding in~\cite{nagaraj2009arithmetic}. The results illustrate that choosing \( b \) based on symbol probabilities leads to the most efficient encoding, verifying the theoretical foundations of GLS coding.

\section{Proposed Algorithm}
The GLS coding method described in the previous section, which utilizes the skew tent map to encode the interval containing the initial condition, assumes that the symbolic sequence is independently and identically distributed (i.i.d.). However, this assumption is often violated in real-world data, particularly in time series and image data, where strong correlations exist between neighboring values. To account for these dependencies, instead of using the first return map (\(T(x)\) vs. \(x\)), we employ the second return map (\(T(T(x))\) vs. \(x\)). In this approach, the skewness of the map is determined based on the empirical probabilities of non-overlapping consecutive symbol pairs: \(p_{00}\), \(p_{01}\), \(p_{10}\), and \(p_{11}\). These probabilities are estimated from the given symbolic sequence, allowing the GLS coding scheme to better capture the underlying structure of correlated data. 

The equation for the second return skew tent map determined based on the empirical probabilities of non-overlapping consecutive symbol pairs: \(p_{00}\), \(p_{01}\), \(p_{10}\), and \(p_{11}\) obtained from the symbolic sequence is provided below:
\begin{comment} 
\begin{equation}
T(T(x)) =
\begin{cases} 
\dfrac{x}{p_{00}}, & 0 \leq x < p_{00} \\[8pt]
\dfrac{(p_{00} + p_{01} - x)}{p_{01}}, & p_{00} \leq x < p_{00} + p_{01} \\[8pt]
\dfrac{x - (p_{00} + p_{01})}{p_{11}}, & p_{00} + p_{01} \leq x < p_{00} + p_{01} + p_{11} \\[8pt]
\dfrac{1 - x}{p_{10}}, & p_{00} + p_{01} + p_{11} \leq x < 1.
\end{cases}
\end{equation}
\end{comment}
\begin{equation*}
T(T(x)) = \begin{cases} 
\dfrac{x}{p_{00}}, & 0 \leq x < p_1 \\[8pt]
\dfrac{p_{00} + p_{01} - x}{p_{01}}, & p_1 \leq x < p_2 \\[8pt]
\dfrac{x - (p_{00} + p_{01})}{p_{11}}, & p_2 \leq x < p_3 \\[8pt]
\dfrac{1 - x}{p_{10}}, & p_3 \leq x < 1
\end{cases}
\end{equation*}
where $p_1 = p_{00}$, $p_2 = p_{00} + p_{01}$, $p_3 = p_{00} + p_{01} + p_{11}$.
\begin{figure}[!tbp]
    \centering
    \includegraphics[width=0.35\textwidth]{skew_tent_map_with_regions.png}
    \caption{Plot of the second return skew tent map with symbolic sequence regions labeled as \( 00, 01, 11, 10 \). The arrows indicate the corresponding intervals for each sequence. The skewness parameters used are \( p_{00} = \frac{2}{5} \), \( p_{01} = \frac{1}{5} \), \( p_{11} = \frac{1}{5} \), and \( p_{10} = \frac{1}{5} \).}
    \label{fig:second_return_skew_tent_map}
\end{figure}
The second return map assumes that each pair of consecutive, non-overlapping symbolic sequences is independent of the subsequent pairs in the sequences.
For eg. given the symbolic sequence: $SS = [0,0,1,0,0,1,1,1,0,0]$, we extract the non-overlapping consecutive symbol pairs:$
(0,0), (1,0), (0,1), (1,1), (0,0)$. Now we will count their occurrences:
\begin{itemize}
    \item \( (0,0) \) appears 2 times.
    \item \( (0,1) \) appears 1 time.
    \item \( (1,0) \) appears 1 time.
    \item \( (1,1) \) appears 1 time.
\end{itemize}
The total number of pairs is \(5\). The probabilities for the $(i,j)$ is computed as: 
\[ 
p_{ij} = \frac{\text{count}(ij)}{\text{total pairs}}
\]
$p_{00} = \frac{2}{5}$,  $p_{01} = \frac{1}{5}$,
$p_{10} =  \frac{1}{5}$, $p_{11} = \frac{1}{5}$.

Figure~\ref{fig:second_return_skew_tent_map} shows the second return map with the regions mapped as $00, 01, 11, 10$.
% \subsubsection*{Laplace Smoothing : Avoiding zero counts}
% However, in rare cases, it may happen that a certain combination of symbol pair is not available in the data. for example, consider the symbolic sequence \( SS = [0, 0, 1, 0] \). While it contains pairs \( (0,0) \) and \( (1,0) \), each occurring once, other pairs \( (0,1) \) and \( (1,1) \) are absent in the sequence, thus leading to an intial count of zero. To avoid this, we apply \textit{Laplace smoothing} by initializing each pair’s count to $\alpha$. If $\alpha=1$ The results are adjusted to the following counts:

% \begin{itemize}
%     \item \( (0,0) \) appears 2 times.
%     \item \( (0,1) \) appears 1 time.
%     \item \( (1,0) \) appears 2 times.
%     \item \( (1,1) \) appears 1 time.
% \end{itemize}

% The corresponding probability estimates are computed as:

% \[
% p_{00} = \frac{2}{6}, \quad p_{01} = \frac{1}{6}, \quad p_{10} = \frac{2}{6}, \quad p_{11} = \frac{1}{6}
% \]

% This formulation ensures numerical stability and will be used in Algorithm~\ref{alg:GLS_Coding} only if we encounter a zero count for any pair while calculating the average probability of pairs. 


\subsection{GLS Coding Compression Classifier Using Second Return Skew Tent Map }

We first describe the back-iteration algorithm for the second-return skew tent map, where the skewness parameters \( p_{00}, p_{01}, p_{11}, p_{10} \) are derived from the given symbolic sequence. Similar to the back-iteration process using the first-return map, this algorithm determines the interval in which the initial condition must lie to generate the desired symbolic sequence.  The algorithm outlined below computes the initial condition, which, when iterated forward and thresholded using the values \( p_{00}, p_{01}, p_{11}, p_{10} \), reconstructs the symbolic sequence in reverse order, which is then reversed again at the end to match the original symbolic sequence.
\subsection{Training: GLS Coding Compression Classifier Using Second Return Skew Tent Map\label{sec:training} }

The training algorithm for the GLS coding compression classifier using the second return skew tent map is as follows. First, the normalized training data\footnote{The normalization is defined as \(X_{\text{norm}} = \frac{X - \min(X)}{\max(X) - \min(X)}\), applied separately to each feature.} is binarized by applying a threshold\footnote{The threshold is a hyperparameter that must be determined through cross-validation.}, converting each feature into a binary sequence. For each class, the algorithm extracts data instances and computes the empirical probabilities \( p_{00}, p_{01}, p_{11}, p_{10} \) by analyzing non-overlapping consecutive symbol pairs in each instance. These probabilities capture the transition frequencies between symbols within class-specific data. To obtain a representative distribution for the class, the algorithm computes the average probabilities across all samples belonging to that class. If any of the average probability of pairs is 0, then we do laplace smoothing. The exact formulation for laplace smoothing applied is provided below:

\[
\hat{p}_{00} = \frac{\sum_{i=1}^{M} p_{i, (0,0)} + \alpha}{M + 4 \cdot \alpha},
\]

\[
\hat{p}_{01}  = \frac{\sum_{i=1}^{M} p_{i, (0,1)} + \alpha}{M + 4 \cdot \alpha},
\]

\[
\hat{p}_{11}  = \frac{\sum_{i=1}^{M} p_{i, (1,1)} + \alpha}{M + 4 \cdot \alpha},
\]

\[
\hat{p}_{10}  = \frac{\sum_{i=1}^{M} p_{i, (1,0)} + \alpha}{M + 4 \cdot \alpha},
\]

where:
\begin{itemize}
  \item \( p_{i, (a,b)} \) is the probability of the pair \((a,b)\) in the \(i^{\text{th}}\) data instance.
  \item \( M \) is the number of data instance of a particular class.
  \item \( \alpha \) is the smoothing parameter added to avoid zero values and ensure stability. In our experiments, we choose $\alpha = 0.001$.
\end{itemize}


The final output is a dictionary containing these averaged probability distributions, which serve as the fundamental statistical parameters for GLS-based compression and classification. The complete training procedure is outlined in Algorithm~\ref{alg:GLS_Coding}. Another thing to note is that this compressed dictionary, $size = \text{No. of classes} \times 4$ is the total size of our model, significantly smaller than most machine learning models.
 
\begin{algorithm}[!h]
\caption{Back Iteration Algorithm for Second-Return Skew Tent Map}
\label{alg:back_iteration}
\begin{algorithmic}[1]
\Require \( p_{00}, p_{01}, p_{11}, p_{10} \) (Second Return Skew Tent Map Parameters), \( \mathbf{S}=[S_1, S_2, \ldots, S_N] \) (Symbolic Sequence)
% \Ensure Initial condition \( x_0 \) that generates the symbolic sequence when forward iterated
% \State \textbf{Reverse} the symbolic sequence: \( \mathbf{S} \leftarrow \text{reverse}(\mathbf{S}) \)
\State Initialize interval \( [L, U] \) as an empty array
\State \textbf{Extract} consecutive non-overlapping symbol pairs from \( \mathbf{S} \)

\State \textbf{Set initial interval} based on the first symbol pair:
\If{\( (S_1, S_2) = (0,0) \)}
    \State \( L \gets 0 \), \( U \gets p_{00} \)
\ElsIf{\( (S_1, S_2) = (0,1) \)}
    \State \( L \gets p_{00} \), \( U \gets p_{00} + p_{01} \)
\ElsIf{\( (S_1, S_2) = (1,1) \)}
    \State \( L \gets p_{00} + p_{01} \), \( U \gets p_{00} + p_{01} + p_{11} \)
\ElsIf{\( (S_1, S_2) = (1,0) \)}
    \State \( L \gets 1 - (p_{00} + p_{01} + p_{11}) \), \( U \gets 1 \)
\EndIf

\For{each subsequent pair \( (S_i, S_{i+1}) \) in \( \mathbf{S}, \) where $i$ starts from $3$}
    \If{\( (S_i, S_{i+1}) = (0,0) \)}
        \State \( L \gets L \cdot p_{00} \), \( U \gets U \cdot p_{00} \)
    \ElsIf{\( (S_i, S_{i+1}) = (0,1) \)}
        \State \( L \gets p_{00} + p_{01} - p_{01} \cdot L \)
        \State \( U \gets p_{00} + p_{01} - p_{01} \cdot U \)
    \ElsIf{\( (S_i, S_{i+1}) = (1,1) \)}
        \State \( L \gets p_{11} \cdot L + p_{00} + p_{01} \)
        \State \( U \gets p_{11} \cdot U + p_{00} + p_{01} \)
    \ElsIf{\( (S_i, S_{i+1}) = (1,0) \)}
        \State \( L \gets 1 - p_{10} \cdot L \)
        \State \( U \gets 1 - p_{10} \cdot U \)
    \EndIf
    \If{\( L > U \)}
        \State Swap \( L \) and \( U \)
    \EndIf
\EndFor

\State \Return \( x_0 = \frac{L + U}{2} \), Interval \( [L, U] \)
\end{algorithmic}
\end{algorithm}

\subsection{TESTING: GLS CODING COMPRESSION CLASSIFIER USING SECOND RETURN SKEW TENT MAP}
During the testing phase of the GLS coding compression classifier, each test instance is first binarized using the same threshold applied during training. Given \( n \) classes, there exist \( n \) sets of average empirical probabilities \( \bar{p}_{00}, \bar{p}_{01}, \bar{p}_{11}, \bar{p}_{10} \), where each set corresponds to a specific class and is precomputed from the training data. For a given test instance, the classifier performs back-iteration separately for each class using its respective probability set, thereby computing the interval \([L, U]\). The compressed file size for each class is then determined as  
\[
\lceil- \log_2 (U - L) \rceil.
\]
The class label is assigned based on the smallest compressed file size, i.e., the predicted class \( c^* \) is given by  

\[
c^* = \arg\min_{c} \lceil -\log_2 (U_c - L_c) \rceil,
\]

where \( U_c \) and \( L_c \) denote the interval bounds computed for class \( c \). This approach ensures that the test instance is classified into the class that achieves the most efficient compression. The algorithm for testing is shown in Algorithm~\ref{alg:GLS_Coding_Predict}.

\begin{algorithm}[!h]
\caption{GLS Coding Training using Second Return Map with Laplace Smoothing}
\label{alg:GLS_Coding}
\begin{algorithmic}[1]
    \Require Normalized training data $X_{train}^{norm}$, Training labels $y_{train}$, Threshold $\tau$, Laplace smoothing parameter $\alpha$
    \Ensure Average class probabilities $\mathcal{P}_{avg}$
    \State Convert training data to binary: $X_{train}^{bin} \gets (X_{train}^{norm} \geq \tau)$
    \State Identify number of classes: $n_{classes} \gets |\text{unique}(y_{train})|$
    \State Initialize $\mathcal{P}_{avg} \gets \emptyset$
    \For{each class $c$ in $\{0,1,\dots,n_{classes}-1\}$}
        \State Select class samples: $X_c \gets X_{train}^{bin}[y_{train} = c]$
        \State Initialize probability list $\mathcal{P}_c \gets \emptyset$
        \For{each sample $x \in X_c$}
            \State $\text{Compute pair probabilities}^*$ $p_{00}, p_{01}, p_{11}, p_{10} \gets \text{Compute Pair Probabilities}(x)$
            \State Append $\{(0,0): p_{00}, (0,1): p_{01}, (1,1): p_{11}, (1,0): p_{10}\}$ to $\mathcal{P}_c$
        \EndFor
        \State Compute average probabilities:
        \State $\bar{p}_{00} \gets \frac{1}{|X_c|} \sum_{p \in \mathcal{P}_c} p_{00}$
        \State $\bar{p}_{01} \gets \frac{1}{|X_c|} \sum_{p \in \mathcal{P}_c} p_{01}$
        \State $\bar{p}_{11} \gets \frac{1}{|X_c|} \sum_{p \in \mathcal{P}_c} p_{11}$
        \State $\bar{p}_{10} \gets \frac{1}{|X_c|} \sum_{p \in \mathcal{P}_c} p_{10}$

        \If{$\bar{p}_{00} = 0 \lor \bar{p}_{01} = 0 \lor \bar{p}_{11} = 0 \lor \bar{p}_{10} = 0$}
            \State \textbf{Laplace smoothing:}
            \State $\hat{p}_{00} \gets \frac{\sum_{p \in \mathcal{P}_c} p_{00} + \alpha}{|X_c| + 4 \alpha}$
            \State $\hat{p}_{01} \gets \frac{\sum_{p \in \mathcal{P}_c} p_{01} + \alpha}{|X_c| + 4 \alpha}$
            \State $\hat{p}_{11} \gets \frac{\sum_{p \in \mathcal{P}_c} p_{11} + \alpha}{|X_c| + 4 \alpha}$
            \State $\hat{p}_{10} \gets \frac{\sum_{p \in \mathcal{P}_c} p_{10} + \alpha}{|X_c| + 4 \alpha}$
        \Else
            \State $\hat{p}_{00} \gets \bar{p}_{00}$
            \State $\hat{p}_{01} \gets \bar{p}_{01}$
            \State $\hat{p}_{11} \gets \bar{p}_{11}$
            \State $\hat{p}_{10} \gets \bar{p}_{10}$
        \EndIf

        \State Store class probability: $\mathcal{P}_{avg}[c] \gets \{(0,0): \hat{p}_{00}, (0,1): \hat{p}_{01}, (1,1): \hat{p}_{11}, (1,0): \hat{p}_{10}\}$
    \EndFor
    \State \Return $\mathcal{P}_{avg}$
\end{algorithmic}
\vspace{0.5em}
{\small \textbf{$^*$} Compute Pair Probabilities includes the calculation of the joint probability for pairs of class outcomes, such as $(0,0), (0,1), (1,1), (1,0)$ for each sample.}
\end{algorithm}



\begin{algorithm}[!h]
\caption{GLS Coding Compression Classifier - Prediction Phase}
\label{alg:GLS_Coding_Predict}
\begin{algorithmic}[1]
\Require $X_{\text{test\_norm}}$: Normalized test data, $n_{\text{classes}}$: Number of classes, 
$\mathcal{P}_{avg}$: Dictionary of average class probabilities, $\tau$: Threshold for binarization
\Ensure Predicted class labels for each test instance

\State Binarize $X_{\text{test\_norm}}$ using threshold $\tau$: 
\[
X_{\text{test\_bin}} = (X_{\text{test\_norm}} \geq \tau)
\]

\State Initialize matrix $S_{\text{test}} \in \mathbb{R}^{m \times n_{\text{classes}}}$ to store compressed sizes 

\For{each test instance $x_i \in X_{\text{test\_bin}}$} 
    \For{each class label $c \in \{0, \dots, n_{\text{classes}} - 1\}$}
        \State Retrieve average probabilities $\hat{p}_{00}, \hat{p}_{01}, \bar{p}_{11}, \hat{p}_{10}$ from $\mathcal{P}_{avg}[c]$
        \State Perform back iteration on second return skew tent map to compute interval:
        \[
        [L_c, U_c] = \text{back\_iterate}(\hat{p}_{00}, \hat{p}_{01}, \hat{p}_{11}, \hat{p}_{10}, x_i)
        \]
        \State Compute compressed file size:
        \[
        S_{\text{test}}[i, c] = \lceil -\log_2 (U_c - L_c) \rceil
        \]
    \EndFor
\EndFor

\State Assign each test instance the class with the smallest compressed file size:
\[
\hat{y}_i = \arg\min_{c} S_{\text{test}}[i, c]
\]

\State \Return Predicted class labels $\hat{y}$
\end{algorithmic}
\end{algorithm}
\section{Experiments and Results}
We conducted experiments on six datasets, the \emph{Iris} (\cite{fisher1936iris}), \emph{Breast Cancer Wisconsin} (\cite{breast_cancer_wisconsin_(diagnostic)_17}), \emph{Wine} (\cite{wine_109}), and \emph{Bank Note Authentication} (\cite{banknote_authentication_70}), \emph{Ionosphere} \cite{ionosphere_49}, \emph{Seeds} (\cite{seeds_236}) and \emph{Haberman's Survival dataset} (\cite{haberman1973analysis}). Each dataset was split into 80\% for training and 20\% for testing, with a fixed random seed of 42 to ensure reproducibility. The number of training and test instances per class for each dataset is summarized in Table~\ref{tab:train_test_instances}.
\textbf{Iris :}
Using five-fold cross-validation with a fixed random state of 90, we optimized the threshold for macro F1-score by varying it from 0.01 to 1.00 in steps of 0.01. The best threshold (0.59) yielded an average macro F1-score of 0.73. We have applied laplace smoothing to the model for Iris data.
\textbf{Breast Cancer Wisconsin :}
Following the same procedure, the optimal threshold (0.32) achieved an average macro F1-score of 0.92.   
\textbf{Wine : }
With an optimal threshold of 0.25, cross-validation resulted in an average macro F1-score of 0.812. 
\textbf{Bank Note Authentication :}
With an optimal threshold of 0.62, cross-validation resulted in an average macro F1-score of 0.77. 
\textbf{Ionosphere :}
With an optimal threshold of 0.01, cross-validation resulted in an average macro F1-score of 0.82. 
\textbf{Seeds : }
With an optimal threshold of 0.51, cross-validation resulted in an average macro F1-score of 0.85.
\textbf{Haberman's Survival :}
With an optimal threshold of 0.61, cross-validation resulted in an average macro F1-score of 0.52.
The performance results for all datasets are summarized Table~\ref{tab:classification_results}. 

\begin{table*}
    \centering
    \caption{Classification Performance on Different Datasets}
    \label{tab:classification_results}
    \begin{tabular}{lccccc}
        \toprule
        Dataset & Accuracy & Macro Precision & Macro Recall & Macro F1-score & Best Threshold \\
        \midrule
        Iris\tablefootnote{Laplace Smoothing is applied.} & 0.8667 & 0.8953 & 0.8519 & 0.8468 & 0.5900 \\
         Breast Cancer Wisconsin & 0.9298 & 0.9292 & 0.9207 & 0.9246 & 0.3200 \\
       Wine\tablefootnote{Laplace Smoothing is applied.} & 0.7500 & 0.7357 & 0.7500 & 0.7387 & 0.2500 \\
       Bank Note Authentication & 0.7927 & 0.7923 & 0.7940 & 0.7923 & 0.6200 \\
       Ionosphere & 0.8028 & 0.8772 & 0.7500 & 0.7633 & 0.0100 \\
       Seeds\tablefootnote{Laplace Smoothing is applied.} & 0.7857 & 0.7871 & 0.7872 & 0.7789 & 0.5100 \\
       Haberman's Survival & 0.6774 & 0.6129 & 0.6250 & 0.6206 & 0.6100\\
        \bottomrule
    \end{tabular}
\end{table*}

\begin{table}[!h]
    \centering
    \caption{Class Distribution in Train and Test Sets}
    \label{tab:train_test_instances}
    \begin{tabular}{l|c|c}
        \hline
        \textbf{Dataset} & \textbf{Train Instances} & \textbf{Test Instances} \\
        \hline
        Iris & (40,41,39) & (10,9,11) \\
        Breast Cancer & (169,286) & (43,71) \\
        Wine & (45,57,40) & (14,14,8) \\
        Bank Note & (614,483) & (148,127) \\
        Ionosphere & (98,182) & (28,43) \\
        Seeds & (59,56,53) & (11,14,17) \\
        Haberman & (181, 63) & (44, 18)\\
        \hline
    \end{tabular}

\end{table}
\subsection{ Connection with MDL optimality}
% In this section, we make connections between our approach and principles of MDL for optimal encodings. Let \( \mathcal{H} \) be the hypothesis space consisting of all second return maps parameterized by transition probabilities \( (p_{00}, p_{01}, p_{11}, p_{10}) \). The optimal hypothesis \( h^* \in \mathcal{H} \) is one that best explains the data while maintaining simplicity. Using the MDL principle, we aim to minimize the total description length.


In this section, we make connections between our approach and principles of MDL for optimal encodings. Let \( \mathcal{H} \) be the hypothesis space consisting of all second return maps parameterized by transition probabilities \( (p_{00}, p_{01}, p_{11}, p_{10}) \). The optimal hypothesis \( h^* \in \mathcal{H} \) is one that best explains the data while maintaining simplicity. Using the MDL principle, we aim to minimize the total description length:
\[
h^* = \underset{h \in \mathcal{H}}{\arg\min} \, [L(D|h) + L(h)].
\]
where  \( L(D|h) \) is the description length of the data given hypothesis \( h \) and \( L(h) \) is the description length of the hypothesis itself.
This corresponds to Maximum a Posteriori (MAP) estimation:
\[
h^* = \underset{h \in \mathcal{H}}{\arg\max} \, P(h \mid \mathcal{D}).
\]
Using Bayes' theorem:
\[
h^* = \underset{h \in \mathcal{H}}{\arg\max} \, P(\mathcal{D} \mid h) P(h).
\]
Taking the negative log:
\[
h^* = \underset{h \in \mathcal{H}}{\arg\min} \, -\log_2 P(\mathcal{D} \mid h) - \log_2 P(h).
\]
Now, using the second return map formulation:
\[
P(\mathcal{D} \mid h) \propto U - L.
\]
Thus, the total description length is:
\[
L_{\text{total}} = -\log_2 (U - L ) - \log_2 (P(h)).
\]
where \( h \) in this context is the second return skew tent map. We can assume a uniform prior to $P(h)$.

This formulation ensures that the choice of the best hypothesis aligns with both the data likelihood and the structure imposed by the second return map.

\subsection{Limitations and Future Work}
While our preliminary work demonstrates promising results, several limitations and opportunities for improvement exist. A fundamental limitation of our approach lies in its dependence on computational precision when performing back-iteration to determine intervals of initial conditions. For high-dimensional data (represented by long symbolic sequences), the limited precision available in standard computing environments may affect our ability to resolve the correct intervals. However, this limitation could potentially be addressed through renormalization techniques (\cite{sayood2017introduction}), which we plan to explore in future work. Further, the current implementation restricts the hypothesis class to skew tent maps. Future research could explore other classes of chaotic maps for potential performance improvements, as well as alternative coding algorithms. Finally, another key area is for future work is to enhance model performance while maintaining strict bounds on complexity.


\section{Conclusion}
This work emphasizes on the need for alternative frameworks for learning, ones that put more emphasis on compressibility, interpretability and computational efficiency. To this end, this paper is fundamental in proposing  a method that views learning not as a search for the `best fit' model in the limit of data samples problem, but as a search for minimal yet efficient encoding problem. We leverage fundamentals from information theory for compression and Chaos for non-linear dynamics as those observed in real world problems and data and bring them together to into a novel GLS coding compression classifier. Our GLS coding compression classifier demonstrates that sophisticated pattern recognition can emerge from remarkably simple mechanisms. By mapping data to intervals of initial conditions, we achieve classification performance comparable to established machine learning algorithms - achieving 92.98\% accuracy on breast cancer detection compared to 94.74\% with Naive Bayes, while maintaining significantly lower computational complexity and high interpretability.
The framework approaches traditional machine learning performance while prioritizing simplicity and transparency suggests promising directions for sustainable AI development. Rather than pursuing ever-increasing model complexity and computational demands, we show how fundamental principles from information theory and dynamical systems can yield efficient, interpretable solutions to real-world classification problems.

\section*{Code Availability}
The python code used in this research are available in the following GitHub repository: 
 \href{https://github.com/i-to-the-power-i/gls-coding-classifier}{https://github.com/i-to-the-power-i/gls-coding-classifier}

\section*{Acknowledgements}
Harikrishnan N. B. gratefully acknowledges the financial support provided by the New Faculty Seed Grant (NFSG/GOA/2024/G0906) from BITS Pilani, K. K. Birla Goa Campus. Anuja Vats acknowledges the support from Research Council of Norway, under CapsNetwork Project (Project No:322600) for this work.
% References


% (used to reserve space for the reference number labels box)

% \bibliographystyle{IEEEtran}
% \bibliography{uai2025-template}

%\newpage

\ifarxiv
% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{rissanen1978modeling}
J.~Rissanen, ``Modeling by shortest data description,'' \emph{Automatica}, vol.~14, no.~5, pp. 465--471, 1978.

\bibitem{solomonoff1964formal}
R.~J. Solomonoff, ``A formal theory of inductive inference. part i,'' \emph{Information and control}, vol.~7, no.~1, pp. 1--22, 1964.

\bibitem{motiwala2022efficient}
A.~Motiwala, S.~Soares, B.~V. Atallah, J.~J. Paton, and C.~K. Machens, ``Efficient coding of cognitive variables underlies dopamine response and choice behavior,'' \emph{Nature Neuroscience}, vol.~25, no.~6, pp. 738--748, 2022.

\bibitem{chater2003simplicity}
N.~Chater and P.~Vit{\'a}nyi, ``Simplicity: a unifying principle in cognitive science?'' \emph{Trends in cognitive sciences}, vol.~7, no.~1, pp. 19--22, 2003.

\bibitem{hansen2001model}
M.~H. Hansen and B.~Yu, ``Model selection and the principle of minimum description length,'' \emph{Journal of the american statistical association}, vol.~96, no. 454, pp. 746--774, 2001.

\bibitem{hinton1993keeping}
G.~E. Hinton and D.~Van~Camp, ``Keeping the neural networks simple by minimizing the description length of the weights,'' in \emph{Proceedings of the sixth annual conference on Computational learning theory}, 1993, pp. 5--13.

\bibitem{adriaans2007mdl}
P.~Adriaans, ``Learning as data compression,'' in \emph{Computation and Logic in the Real World: Third Conference on Computability in Europe, CiE 2007, Siena, Italy, June 18-23, 2007. Proceedings 3}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2007, pp. 11--24.

\bibitem{galland2003mdl}
F.~Galland, N.~Bertaux, and P.~R{\'e}fr{\'e}gier, ``Minimum description length synthetic aperture radar image segmentation,'' \emph{IEEE Transactions on Image Processing}, vol.~12, no.~9, pp. 995--1006, 2003.

\bibitem{blier2018description}
L.~Blier and Y.~Ollivier, ``The description length of deep learning models,'' \emph{Advances in Neural Information Processing Systems}, vol.~31, 2018.

\bibitem{saito1997mdl}
K.~Saito and R.~Nakano, ``Mdl regularizer: a new regularizer based on the mdl principle,'' in \emph{Proceedings of International Conference on Neural Networks (ICNN'97)}, vol.~3.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 1997, pp. 1833--1838.

\bibitem{lan2022minimum}
N.~Lan, M.~Geyer, E.~Chemla, and R.~Katzir, ``Minimum description length recurrent neural networks,'' \emph{Transactions of the Association for Computational Linguistics}, vol.~10, pp. 785--799, 2022.

\bibitem{soramaki2007topology}
K.~Soram{\"a}ki, M.~L. Bech, J.~Arnold, R.~J. Glass, and W.~E. Beyeler, ``The topology of interbank payment flows,'' \emph{Physica A: Statistical Mechanics and its Applications}, vol. 379, no.~1, pp. 317--333, 2007.

\bibitem{stein2013ecological}
R.~R. Stein, V.~Bucci, N.~C. Toussaint, C.~G. Buffie, G.~R{\"a}tsch, E.~G. Pamer, C.~Sander, and J.~B. Xavier, ``Ecological modeling from time-series inference: insight into dynamics and stability of intestinal microbiota,'' \emph{PLoS computational biology}, vol.~9, no.~12, p. e1003388, 2013.

\bibitem{palmer1993extended}
T.~N. Palmer, ``Extended-range atmospheric prediction and the lorenz model,'' \emph{Bulletin of the American Meteorological Society}, vol.~74, no.~1, pp. 49--66, 1993.

\bibitem{nagaraj2009arithmetic}
N.~Nagaraj, P.~G. Vaidya, and K.~G. Bhat, ``Arithmetic coding as a non-linear dynamical system,'' \emph{Communications in Nonlinear Science and Numerical Simulation}, vol.~14, no.~4, pp. 1013--1020, 2009.

\bibitem{fisher1936iris}
R.~Fisher, ``Iris data set. uci machine learning repository,'' 1936.

\bibitem{breast_cancer_wisconsin_(diagnostic)_17}
W.~Wolberg, O.~Mangasarian, N.~Street, and W.~Street, ``Breast cancer wisconsin (diagnostic),'' UCI Machine Learning Repository, 1993, {DOI}: \url{https://doi.org/10.24432/C5DW2B}.

\bibitem{wine_109}
S.~Aeberhard and M.~Forina, ``Wine,'' UCI Machine Learning Repository, 1991, {DOI}: \url{https://doi.org/10.24432/C56W2R}.

\bibitem{banknote_authentication_70}
V.~Lohweg and W.~Drobisch, ``Banknote authentication,'' UCI Machine Learning Repository, 2012, {DOI}: \url{https://doi.org/10.24432/C5K306}.

\bibitem{ionosphere_49}
V.~Sigillito, ``Ionosphere,'' UCI Machine Learning Repository, 1989, {DOI}: \url{https://doi.org/10.24432/C5KP4J}.

\bibitem{seeds_236}
M.~Charytanowicz, J.~Niewczas, P.~Kulczycki, P.~Kowalski, and S.~Lukasik, ``Seeds,'' UCI Machine Learning Repository, 2010, {DOI}: \url{https://doi.org/10.24432/C5H30K}.

\bibitem{haberman1973analysis}
S.~J. Haberman, ``The analysis of residuals in cross-classified tables,'' \emph{Biometrics}, pp. 205--220, 1973.

\bibitem{sayood2017introduction}
K.~Sayood, \emph{Introduction to data compression}.\hskip 1em plus 0.5em minus 0.4em\relax Morgan Kaufmann, 2017.
\bibitem{marton2005compression}
Y.~Marton, N.~Wu. and L.~Hellerstein, 2005. "On compression-based text classification."
\emph{In Advances in Information Retrieval: 27th European Conference on IR Research, ECIR 2005.} Proceedings 27 (pp. 300-314). Springer Berlin Heidelberg.


\end{thebibliography}


% \else
% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{authors/Anuja.jpg}}]{Anuja Vats}
% (Member, IEEE) received the master’s degree in automotive electronics in 2018 and her Ph.D. degree with the Department of Computer Science, Norwegian University of Science and Technology (NTNU), Norway. Her research focus is towards developing unsupervised and semi-supervised deep learning algorithms for different applied problems such as autonomous driving, Computer Aided Diagnoses (CADx) for medical as well as  in remote sensing. She is currently working as a postdoctoral researcher with the Department of Computer Science, NTNU, Gjøvik.
% \end{IEEEbiography}

% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{authors/david.jpg}}]{David V\"olgyes}
% (Member, IEEE) David V\"olgyes received his MSc in Physics in 2008 from the E\"otv\"os Lor\'and University, Budapest. He received his Ph.D. degree in computer science in 2018 from the program in the Norwegian University of Science and Technology, Norway. His research focuses on image processing with deep learning, primarily in medical imaging and remote sensing.
% \end{IEEEbiography}

% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{authors/martijn.jpg}}]{Martijn Vermeer} received his master's degree in Geomatics from Delft University of Technology in 2017.  Since then he has worked in industry as a machine learning engineer in the remote sensing domain, until recently at Science and Technology AS and currently at Field Group AS. The projects and research he has been involved with focus on applications of deep learning on remotely sensed data, mostly within the forestry domain.
% \end{IEEEbiography}
% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{authors/marius}}]{Marius Pedersen}
% received his BSc in Computer Engineering in 2006, and MiT in Media Technology in 2007, both from Gjøvik University College, Norway. He completed a PhD program in color imaging in 2011 from the University of Oslo, Norway, sponsored by Océ. He is a professor at the Department of Computer Science at NTNU in Gjøvik, Norway. He is also the director of the Norwegian Colour and Visual Computing Laboratory (Colourlab). His work is centered on subjective and objective image quality.
% \end{IEEEbiography}

% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{authors/kiran-raja-profile-pic-2022}}]{Kiran Raja} (Senior Member, IEEE) received the Ph.D. degree in Computer Science from the Norwegian University of Science and Technology, Norway, in 2016, where he is Faculty Member with the Department of Computer Science. He was/is participating in EU projects SOTAMD, iMARS, and other national projects such as CapsNetwork. He is a member of the European Association of Biometrics (EAB) and chairs the Academic Special Interest Group at EAB. He is currently serving as Section Chair, IEEE Norway. He serves as a reviewer for a number of journals and conferences. He is also a member of the editorial board for various journals. His research focuses on statistical pattern recognition, image processing, and machine learning with applications to biometrics, security, privacy protection, and imaging for health applications.
% \end{IEEEbiography}
% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{authors/Daniele.jpeg}}]{Daniele S. M. Fantin} received his master's degree in astronomy from the University of Bologna in 2005 and his PhD in Astronomy from the University of Nottingham in 2011. From 2012 to 2013 he was a Postdoctoral Researcher at the Centre of Research for Astronomy (CIDA), in Merida, Venezuela. Since 2014 he has been working at Science And Technology AS, first as a scientific software developer and now as project manager and manager of the Earth Observation/Machine Learning team.
% \end{IEEEbiography}
% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{authors/jacob2.jpg}}]{Jacob A. Hay} is an upper secondary drop-out who got deeply interested in AI in the fall of 2011, composed his bachelor's degree in physics and mathematics from the University of Oslo in 2018, and has worked with machine learning since. In 2020 he started working part-time on a master's degree in informatics with the University of Bergen. Since 2022 he has been working with Science And Technology AS as a machine learning engineer, focused on deep learning for remote sensing data.
% \end{IEEEbiography}

% \fi


\end{document}
