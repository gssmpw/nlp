
\section{Semantic Satellite NeRF}

\begin{figure*}[t!]
	\centering
	\includegraphics[width=\linewidth]{imgs/architecture/Pipeline}
	\caption{Overview of our proposed model. The satellite-domain-adapted outputs  (\ie elements in the blue area)  are combined using an irradiance lighting model to produce the color rendering as originally proposed by \emph{SatNeRF} \cite{satnerf}. Using an additional semantic head (\ie elements in the red area) our proposed method is able to produce a corresponding semantic pixel-wise labeling. We combine this with the learned lighting scalar to create a three-dimensional semantic visualization. We introduce a transient regularization loss $L_t$ to reduce artifacts in the learned appearance based on the semantic input data. }
	\label{fig:pipeline}
\end{figure*}

An overview of our proposed satellite-domain-adapted semantic \emph{NeRF} can be seen in \cref{fig:pipeline}.
We utilize satellite images with their corresponding semantic pixel-wise annotations to train a dual-modality \emph{NeRF} specifically adapted for satellite-imagery. 
Thus, in contrast to previous approaches, our pipeline renders pixel-wise semantic labels for novel, unseen views of scenes captured by satellite data.


\subsection{Satellite-Domain-Specific Camera Model}
\label{subsec:irradiance_lighting:model}



During the training and testing phase of a \emph{NeRF}, rays are constructed for each pixel of the given images and their corresponding camera poses, traversing a pre-defined, bounded three-dimensional volume. 
As proposed by \emph{SatNeRF} \cite{satnerf} we use the \emph{RPC} \cite{rpc} camera model instead of the commonly used pinhole \cite{pinhole} camera model.
This domain-specific camera model is an approximation of the physical satellite camera. 
We extract it from the satellite image metadata as it is typically used for georegistration of the image. 
To improve accuracy, \emph{SatNeRF} \cite{satnerf} performs \emph{Bundle Adjustment} on the \emph{RPC} models. This process optimizes relative camera poses to increase ray accuracy. 




\subsection{Learning Semantic in 3D for Satellite Data}
\emph{NeRF} \cite{nerf} represent a static three-dimensional scene as a continuous volumetric function $\mathcal{F}_{NeRF}$ encoded with a \emph{MLP} network. 
\begin{equation}
	\mathcal{F}_{NeRF}: (\vec{x}, \vec{d}) \mapsto (\sigma, \vec{c})
\end{equation}
Given a 3D point $\vec{x} = (x, y, z)$ and a viewing direction $\vec{d} = (d_x, d_y, d_z)$, the network predicts a density scalar $\sigma$ and a color $\vec{c}=(r, g, b)$.
The scene structure is represented by the density $\sigma$ and used to weigh the influence of the color $\vec{c}$.
\emph{SatNeRF} \cite{satnerf} proposes an expanded satellite-domain-specific architecture. %
\begin{equation}
	\mathcal{F}_{SatNeRF}: (\vec{x}, \vec{\omega}, \vec{t}_j) \mapsto (\sigma, \vec{c}_a, \mathit{sun}, \vec{a}, \beta)
\end{equation}
The scene color $\vec{c}$ is split up into three separate lighting components: the albedo color $\vec{c}_a$, the sun shading scalar $\mathit{sun}$ and the ambient color $\vec{a}$. To predict the variable lighting components $\mathit{sun}$ and $\vec{a}$, the sun direction $\vec{\omega}$ is used as auxiliary input.
The uncertainty $\beta$ is used to reduce artifacts caused by transient objects such as vehicles. It is learned with the per-image embedding $\vec{t}_j$ as auxilliary input. 
Our proposed semantic satellite-adapted network represents the following function $\mathcal{F}_{SemSatNeRF}$.
\begin{equation}
		\mathcal{F}_{SemSatNeRF}: (\vec{x}, \vec{\omega}, \vec{t}_j) \mapsto (\sigma, \vec{c}_a, \mathit{sun}, \vec{a}, \beta, \vec{s})
\end{equation} 
We extend the satellite-domain-adapted architecture from \emph{SatNeRF} \cite{satnerf} with the additional semantic output $\vec{s}$.
Similiar to \emph{Semantic-NeRF} \cite{semanticnerf} the semantic output $\vec{s}$
is represented as a view-invariant function $\mathcal{F}_s(\vec{x}) \mapsto \vec{s}$ of the world coordinates $\vec{x}$ and describes a distribution over $C$ semantic labels as pre-softmax logits.
In addition, we introduce a $\mathit{sigmoid}$ activation function with the intention to increase the generalization across views, using the 
$\mathit{sigmoid}$ activation function as form of normalization. This reduces the ability of single samples to dominate the semantic prediction. 
\begin{equation}
	\vec{s}(\vec{x}) = \mathit{sigmoid}(\mathcal{F}_s(\vec{x}))
\end{equation}
To produce the final result for a specific pixel position, \emph{NeRF}-based pipelines aggregate the outputs of the \emph{MLP} for individual points $\vec{x}$ along a ray $\vec{r}(t) = \vec{o} + t \cdot \vec{d}$ representing the corresponding line of sight. Here, $\vec{o}$ and $\vec{d}$ denote the origin and direction vectors.
\begin{equation}
	\vec{s}(\vec{r}) = \sum_{i=1}^{N} T_i \alpha_i \vec{s}(\vec{x}_i)	
	\label{eq:semantic_rendering}
\end{equation}
The influence of each sample $\vec{x}_i$ along a ray is based on its opacity $\alpha_i$ and transmittance $T_i$.
\begin{equation}
	\alpha_i = 1 - \exp(-\sigma_i\delta_i) \text{ and } T_i = \prod_{j=1}^{i-1} (1 - \alpha_j)
\end{equation}
The distance $\delta_i = t_i -  t_{i-1}$ describes the length traveled along the ray between the previous and current sampled position $\vec{x}$.
Using softmax normalization the combined logits $\vec{s}(\vec{r})$ are converted into probabilities $\hat{\vec{p}}(\vec{r})$. The class with the highest probability is chosen to represent the semantic class of the ray.

\subsection{Semantic Structural Visualization}
\label{subsec:3d_viz}

To visualize the semantic labels for a given image, usually a simple color mapping operation $f_c: \vec{s} \mapsto \vec{RGB}$ is used. 
This visualization is sub-optimal for large areas with the same class, as the structure of the scene is lost. 
For large structures consisting of multiple buildings with large height differences this is especially noticeable, such as in the scenes JAX\_068 and JAX\_214 in \cref{fig:results_labels}.

To handle variance in lighting caused by different sun positions in training images, we use a satellite-domain-adapted irradiance lighting model as proposed by \emph{Shadow-NeRF} \cite{snerf}.
The sample color $\vec{c}(\vec{x}, \vec{\omega})$ is split up into three components: 
scene color $\vec{c_a}(\vec{x})$, lighting scalar $\mathit{sun}(\vec{x}, \vec{\omega})$ and ambient sky color $\vec{a}(\vec{\omega})$.
\begin{equation}
	\vec{c}(\vec{x}, \vec{\omega}) = \vec{c}_a(\vec{x}) \cdot (\mathit{sun}(\vec{x}, \vec{\omega}) + (1 - \mathit{sun}(\vec{x}, \vec{\omega})) \cdot \vec{a}(\vec{\omega}))
	\label{eq:satnerf_lighting_model}
\end{equation}




By referencing the visualization of the $sun(\vec{x}, \vec{\omega})$ lighting component in \cref{fig:results_shadows} we can see that it resembles a grayscale relief map of the scene structure.
We use this to provide a three-dimensional depth to the semantic output. We aggregate the $\mathit{sun}(\vec{x}, \vec{\omega})$ component along rays $\vec{r}$ analog to \cref{eq:semantic_rendering} and combine it with the mapped semantic color $f_c(\vec{s}(\vec{r}))$.
\begin{equation}
	\vec{c}_{s}(\vec{r}) = f_c(\vec{s}(\vec{r})) * \sum_{i=1}^{N} T_i \alpha_i \mathit{sun}(\vec{x}_i, \vec{\omega})
	\label{eq:semantic_viz_shading}
\end{equation}
The resulting visualization of the structural semantic prediction is shown in \cref{fig:results_semantic_shaded}. The commonly flat color visualization is enriched with a three-dimensional structure. Finer details are made visible and objects such as the distinct main building can now be easily identified. 


\subsection{Handling Transient Objects}
\label{subsec:handling_transients}
A major issue by processing multi-date satellite images are temporal inconsistencies. Besides changes in the scene geometry, transient objects pose a substantial challenge for the reconstruction process. As transient objects change positions across training images, the network receives contradictory information. 
Since we are interested in reconstructing a consistent model reflecting the stationary scene geometry, transient objects represent a type of noise. 
To improve the model consistency, we use available transient category information contained in the semantic labels to reduce the impact of such objects on the semantic as well as the appearance prediction. 
We separately consider the problem for both modalities as semantic and appearance are learned with their own distinct loss function.


\subsubsection{Handling Transient Objects in RGB Data}
\label{subsubsec:transients_rgb}

\emph{NeRF} are optimized by minimizing the $L2$-Loss between the predicted ray color $\vec{c}(\vec{r})$ and the ground truth pixel color $\vec{c}_{GT}(\vec{r})$.
\begin{equation}
	L_{2}(\mathcal{R}) = \sum_{\vec{r} \in \mathcal{R}}|| \vec{c}(\vec{r}) - \vec{c}_{GT}(\vec{r}) ||_2^2
	\label{eq:rgb_loss_normal}
\end{equation}

To reduce the effect of transient objects in the color prediction \textit{SatNeRF} \cite{satnerf} introduces an uncertainty mechanism. Based on a per-image learned embedding $t_j$ the network predicts an uncertainty factor $\beta$ for each location $\vec{x}$.
The color-loss is decreased depending on the uncertainty, giving the network the ability to selectively reduce the loss of specific rays. 
\begin{equation}
	L_{color}(\mathcal{R}) = \sum_{\vec{r} \in \mathcal{R}} \frac{|| \vec{c}(\vec{r}) - \vec{c}_{GT}(\vec{r}) ||^2_2}{2\beta'(\vec{r})^2} + \left( \frac{\log{\beta'(\vec{r}) + \eta}}{2} \right)
	\label{eq:loss_rgb}
\end{equation}
The second term forces the network to use the uncertainty sparingly preventing the naive solution of $\beta$ converging towards infinity. To prevent negative values in the logarithm we set $\beta'(\vec{r}) = \beta(\vec{r}) + 0.05$ and $\eta = 3$ analog to \emph{SatNeRF} \cite{satnerf}.


The uncertainty $\beta$ gives the network an mechanism to reduce the impact of local outliers in the color training images. Although many transient objects such as vehicles move inbetween images, locations such as parking lots contain vehicles in majority of the images. To achieve our goal of learning a static, transient free representation of the scene we introduce a transient regularization loss. Instead of a purely statistical approach we propose guiding the uncertainty towards transient objects using existing information from the semantical ground truth data.
To this end, we introduce a transient regularization loss as seen in \cref{eq:car_reg_loss}. For rays $\vec{r} \in \mathcal{R}^t$ that are associated with transient ground truth labels, the following loss guides the uncertainty $\beta$ towards $1$. 
\begin{equation}
		L_{t}(\mathcal{R}^t) = \sum_{\vec{r} \in \mathcal{R}^t} || 1.0 - \beta(\vec{r})  ||^2_2 
		\label{eq:car_reg_loss}
\end{equation}
We do not make any assumptions about rays outside of the transient classes, allowing the network to still learn its own uncertainty values. This ensures that the network still handles other occurring phenomena such as chromatic aberration. 
Whereas the uncertainty $\beta$ is theoretically unbounded, the second term of the color-loss in \cref{eq:loss_rgb} punishes high values. The uncertainty scalar $\beta$ naturally resides in the interval from $0$ to around $1$. We therefore guide the network towards $1$ for rays $\vec{r} \in \mathcal{R}^{t}$.

\emph{SatNeRF} \cite{satnerf} proposes using the non-modified color-loss as seen in \cref{eq:rgb_loss_normal} for the first two epochs. This gives the network time to learn an initial understanding of the scene geometry and color. We enable our proposed transient regularization loss after three epochs. 
This gives the network one additional epoch to initialize the uncertainty $\beta$.

\subsubsection{Handling Transient Objects in Semantic Data}
\label{subsubsec:transients_semantic}

Common \emph{NeRF} methods only derive a 3d appearance model of the scene. By introducing a semantic representation, the network now has a secondary goal, requiring an additional loss term.
\emph{Semantic-NeRF} \cite{semanticnerf} uses a cross-entropy-loss $\sum_{c=1}^{C} p^c(\vec{r}) \log{\hat{p}^c(\vec{r})}$ comparing the ground truth label $p^c$ with the predicted semantic probability $\hat{p}^c$ for each class $c$ across all rays $\vec{r} \in \mathcal{R}$.
We define the semantic loss to only include rays from static classes $\vec{r} \in \mathcal{R} \setminus \mathcal{R}^t$. $\mathcal{R}^t$ hereby describes all rays $\vec{r}$ with a semantic class defined as transient. 
\begin{equation}
	L_{semantic}(\mathcal{R} \setminus \mathcal{R}^t) = - \sum_{\vec{r} \in (\mathcal{R} \setminus \mathcal{R}^t )} \sum_{c=1}^{C} p^c(\vec{r}) \log{\hat{p}^c(\vec{r})}
	\label{eq:loss_semantic_modified}
\end{equation}
This effectivelly equals in ignoring parts of the semantic training labels, leaving holes in the pixel-wise label map. The network does not receive any learning guidance for these positions and has to rely on information given by different training views to fill in the missing information. This is made possible by the inherent multi-view consistency mechanism provided by \emph{NeRFs}.








