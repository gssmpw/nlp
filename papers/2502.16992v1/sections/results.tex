\section{Evaluation and Experiments}

\begin{table}[t]
	\centering
	\setlength{\tabcolsep}{0.5em} %
	\begin{tabular}{l|cccc|c}
		\toprule
		& \multicolumn{5}{c}{\textbf{Semantic Accuracy $\uparrow$}} \\
		Split &004&068&214&260& Mean\\
		\midrule
		
		Train & 0.936 & 0.950 & 0.947 & 0.934& 0.942 \\
		Test & 0.930 & 0.948 & 0.944 & 0.916& 0.935\\
		
		\bottomrule
	\end{tabular}
	\caption{Quantitative evaluation of our domain-adapted semantic \emph{NeRF}. The high semantic accuracy for test views shows the ability of the network to render a correct semantic segmentation of the scene for novel, during training unseen views. }
	\label{tbl:semantic_results}
\end{table}

We evaluate the ability of our proposed \emph{NeRF} pipeline to learn a generalized semantic understanding of satellite scenes. 
Since the \emph{Vehicle} category consists of transient objects, we treat this class according to \cref{subsec:handling_transients} when computing the semantic and color representation

In \cref{subsec:learned_semantic_rep} we evaluate the general quality of the semantic rendering in comparison to ground truth data. In \cref{subsec:transient_reg} we show the impact of our transient regularization term for color representation and how it reduces artifacts. 
Lastly, we evaluate the capability of our pipeline to improve the semantic segmentation quality of input images by leveraging the inherent multi-view-consistency provided by \emph{NeRF} in \cref{subsec:multi_view_consistency}.



\subsection{Learned Semantic Representation}
\label{subsec:learned_semantic_rep}

To evaluate the quality of the learned semantic representation, we use the semantic accuracy metric, \ie the ratio of correctly to incorrectly identified pixels. 
We create alternative transient-free ground truth annotations to assess whether our proposed network is able to accurately replace the missing semantic information for transient locations with the correct static class (in most cases either \emph{Ground} or \emph{Building}) . 
The evaluation results are reported in \cref{tbl:semantic_results}.
Our domain-adapted semantic \emph{NeRF} achieves an accuracy of over 90\% for all scenes, proving its ability to generalize the learned semantic understanding of the scene for both training and testing viewpoints. 


We visualize one view of each scene in \cref{fig:results} 
with the corresponding ground truth color image (\cref{fig:results_rgb}), the semantic annotations (\cref{fig:results_labels}), and the outputs learned by our domain-adapted semantic \emph{NeRF}.
Comparing the learned semantic prediction in \cref{fig:results_semantic} with the ground truth annotations shows a high degree of accuracy. 
By combining the predicted semantic class with the learned shadow scalar in \cref{fig:results_shadows} we produce a three-dimensional visualization in \cref{fig:results_semantic_shaded}.
Comparing this visualization to the ground truth color image shows how the network is able to transfer  the scene structure on the semantic understanding.
















\begin{table}[t]
	\centering
	\setlength{\tabcolsep}{0.5em} %
	\begin{tabular}{c|cccc|c}
		\toprule
		\textbf{Transient} & \multicolumn{5}{c}{\textbf{Transient Uncertainty $\uparrow$}}\\
		\textbf{Regularization} &004&068&214&260&Mean\\
		\midrule
		
		\crossmark & 0.121 & 0.092 & 0.144 & 0.027 & 0.096\\
		
		\checkmark & 0.976 & 0.900 & 0.877 & 0.915 & 0.917\\
		\bottomrule
	\end{tabular}
	\caption{
		Quantitative evaluation of the proposed transient regularization loss. The results shows a great increase in uncertainty for locations marked as transient in the semantic ground truth data. 
		The model learns a static, transient free appearance of the scene. }
	\label{tbl:semantic_results_car_filtering}
\end{table}




\begin{figure*}[ht!]
	\centering
	
	\begin{minipage}{0.025\linewidth}
		\centering
		\rotatebox[origin=center]{90}{SatNeRF \cite{satnerf}}
	\end{minipage}
	\begin{minipage}{0.97\linewidth}
		\centering
		\includegraphics[width=.24\linewidth]{imgs/car_reg/rgb_JAX_068_011_RGB_-1_cropped.png}
		\hfill
		\includegraphics[width=.24\linewidth]{imgs/car_reg/beta_JAX_068_011_RGB_-1_cropped.png}
		\hfill
		\includegraphics[width=.24\linewidth]{imgs/car_reg/rgb_JAX_214_019_RGB_-1_cropped.png}
		\hfill
		\includegraphics[width=.24\linewidth]{imgs/car_reg/beta_JAX_214_019_RGB_-1_cropped.png}
	\end{minipage}\\
	
	\vspace{0.2 cm}
	\begin{minipage}{0.02\linewidth}
		\centering
		\rotatebox[origin=center]{90}{Ours}
	\end{minipage}
	\begin{minipage}{0.975\linewidth}
		\centering
		\subcaptionbox{JAX\_068}{
			\includegraphics[width=.24\linewidth]{imgs/car_reg/rgb_JAX_068_011_RGB_-1_crl_cropped.png}
			\hfill
			\includegraphics[width=.24\linewidth]{imgs/car_reg/beta_JAX_068_011_RGB_-1_crl_cropped.png}
		}%
		\hfill
		\hspace{0.02cm}
		\subcaptionbox{JAX\_214}{
			\includegraphics[width=.24\linewidth]{imgs/car_reg/rgb_JAX_214_019_RGB_-1_crl_cropped.png}
			\hfill
			\includegraphics[width=.24\linewidth]{imgs/car_reg/beta_JAX_214_019_RGB_-1_crl_cropped.png}
		}%
		
	\end{minipage}\\
	
	
	\caption{
		Qualitative comparison of the proposed transient regularization loss. Using this loss, our model is able to substantially reduce blurry artifacts by guiding the uncertainty $\beta$ on locations of transient objects.
		Shown here are closeups of the rendered color image and visualization of the uncertainty scalar $\beta$ for training views of the scenes JAX\_068 and JAX\_214. The Baseline \emph{SatNeRF} \cite{satnerf} model is unable to filter the cars out, leading to visible remnants. Our method is able to render a static, transient free representation of the scene.}
	\label{fig:car_reg_loss}
\end{figure*}

\subsection{Transient Regularization}
\label{subsec:transient_reg}



We evaluate the impact of our proposed transient regularization loss in increasing uncertainty of known transient locations by measuring the \emph{Transient Uncertainty}.
We define this metric as the mean uncertainty for all transient locations $\mathcal{R}^t$, \ie the \emph{Vehicles} class, in the semantic ground truth data. The uncertainty $\beta(\vec{x}, \vec{t}_j)$ is aggregated along rays $\vec{r}$ analog to \cref{eq:semantic_rendering} with the per-image embedding $\vec{t}_j$ as additional input. 
\begin{equation}
	\beta_t(\mathcal{R}^t) = \frac{1}{|\mathcal{R}^t|}\sum_{\vec{r} \in \mathcal{R}^t} \sum_{i = 0}^{N} \alpha_i T_i \beta(\vec{x}_i, \vec{t}_j)
\end{equation}
Since the uncertainty $\beta$ is only relevant during training, we do not consider test images during this evaluation. 
In \cref{tbl:semantic_results_car_filtering} we can see the impact of our transient regularization loss. 
The baseline \emph{SatNeRF} \cite{satnerf} model is not able to adequately capture the transient objects, resulting in a low \emph{Transient Uncertainty} of $0.096$ across all four scenes. 
This causes the \emph{SatNeRF} model to integrate the corresponding appearance information into the color representation.
Our proposed transient regularization loss greatly increases the \emph{Transient Uncertainty} to $0.917$, showing the benefit from the additional guidance. 

We provide close up visualizations of the uncertainty $\beta$ and the corresponding rendered color image for two scenes in \cref{fig:car_reg_loss}. Both contain parking lots with large numbers of vehicles parked across multiple training views.
For both scenes the uncertainty $\beta$ shows increased focus using our transient regularization loss, resulting in a drastic decrease in visible vehicles in the rendered color image. 
Only the static elements of the scene are rendered by the model.



\subsection{Multi-View Consistency}
\label{subsec:multi_view_consistency}
 


\begin{table*}[t]
	\centering
	\begin{tabular}{l|c|cccc|c}
		\toprule
		& & \multicolumn{4}{c}{\textbf{Semantic Accuracy (Train) $\uparrow$}} &  \\
		Labels & Activation Function & JAX\_004&JAX\_068&JAX\_214&JAX\_260& mean\\
		\midrule
		
		Corrupted & - & 0.800 & 0.800 & 0.800 & 0.800 & 0.800\\
		
		Fused Labels & None & 0.857 &  0.930 & 0.924 & 0.902 & 0.903\\
		
		Fused Labels & Sigmoid & 0.894 &  0.936 & 0.933 & 0.920 & 0.921\\
		
		\bottomrule
	\end{tabular}
	\caption{
		Quantitative evaluation of the multi-view consistency on imperfect labels.
		Enforced multi-view consistency is able to improve initial semantic segmentations used as training input data. By using a sigmoid activation function for semantic logit prediction we can increase the semantic accuracy by a mean of $12.1\%$. }
	\label{tbl:corrupted_labels}
\end{table*}



\begin{figure*}[t]
	\centering
	
	\begin{minipage}{0.035\linewidth}
		\centering
		\rotatebox[origin=center]{90}{JAX\_068}
	\end{minipage}
	\begin{minipage}{0.95\linewidth}
		\centering
		\hspace{-0.13cm}
		\includegraphics[width=.24\linewidth]{imgs/corrupted/ground_truth_068_04.jpg}
		\hfil
		\includegraphics[width=.24\linewidth]{imgs/corrupted/corrupted_068_04.jpg}
		\hfil
		\includegraphics[width=.24\linewidth]{imgs/corrupted/nerf_068_04.png}
		\hfil
		\includegraphics[width=.24\linewidth]{imgs/corrupted/nerf_068_04_wo_sigmoid.png}
	\end{minipage}
	\\
	\vspace{0.2cm}
	\begin{minipage}{0.04\linewidth}
		\centering
		\rotatebox[origin=center]{90}{JAX\_214}
	\end{minipage}
	\begin{minipage}{0.95\linewidth}
		\begin{subfigure}{0.24\textwidth}
			\centering
			\includegraphics[width=\textwidth]{imgs/corrupted/ground_truth_015}
			\caption{Ground Truth.}
			\label{subfig:lbl_corruption_gt}
		\end{subfigure}
		\hfil
		\begin{subfigure}{0.24\textwidth}
			\centering
			\includegraphics[width=\textwidth]{imgs/corrupted/corrupted_015.jpg}
			\caption{Simulated Segmentation.}
			\label{subfig:lbl_corruption_corrupted}
		\end{subfigure}
		\hfil
		\begin{subfigure}{0.24\textwidth}
			\centering
			\includegraphics[width=\textwidth]{imgs/corrupted/nerf_015.png}
			\caption{Fused Labels with Sigmoid.}
			\label{subfig:lbl_corruption_nerf_output}
		\end{subfigure}
		\hfil
		\begin{subfigure}{0.24\textwidth}
			\centering
			\includegraphics[width=\textwidth]{imgs/corrupted/nerf_015_wo_sigmoid}
			\caption{Fused Labels w/o Sigmoid.}
			\label{subfig:lbl_corruption_nerf_output_no_activation}
		\end{subfigure}
	\end{minipage}
	\caption{Effect of the multi-view consistency on local segmentation errors.  
		The semantic information across all training views is merged in order to improve accuracy.
		Using a $\mathit{sigmoid}$ activation function as normalization 	decreases noise in the reconstructed segmentation. }
	\label{fig:s_nerf_shading}
\end{figure*}


\emph{NeRF} learn a generalized scene understanding through maximizing consistency across all training views. 
This multi-view consistency reflects an averaging of information between input images. 
The semantic prediction potentially benefits from this process, improving semantic segmentation quality through enforcing multi-view consistency.

\emph{Semantic NeRF} \cite{semanticnerf} conducts multiple experiments regarding the effect of partial or corrupted labels. 
They show that the network is able to learn a meaningful semantic representation, even if pixel-wise noise is added or entire instances are flipped to a different class. 
With these experiments they demonstrate that the underlying multi-view consistency enforced by \emph{NeRF} enables the generation of coherent semantic renderings even when training with partial and/or faulty labels. 
The \emph{NeRF} allows to denoise and improve the initial semantic segmentations by rendering the same viewpoint. 
We conduct additional experiments researching this use case for the domain of segmenting satellite data.

We degrade the quality of our training data to more closely match the predicted output of automatic segmentation methods. 
We aim to simulate the effect of automatic segmentation methods correctly recognizing and segmenting most of an object, with smaller areas being incorrectly classified. 
We therefore explicitly refrain from pixel-wise noise and instance specific manipulation in contrast to \emph{Semantic-NeRF} \cite{semanticnerf}.
To corrupt our labels we generate a normal noise map for each semantic class. 
We blur and rescale the noise maps to create larger regions.  
Based on a specified accuracy loss, thresholds are determined to identify regions in each noise map that should be corrupted. 
Each noise region is reassigned to a different class.  
We release the corrupted labels alongside the code for the automated label corruption as part of the dataset.
To evaluate the impact of training our pipeline with corrupted labels, we degrade the annotations for each scene by $20\%$, resulting in an overall semantic accuracy of $80\%$. 
The impact on the pixel-wise labels is illustrated in \cref{subfig:lbl_corruption_gt,subfig:lbl_corruption_corrupted}.


Rendering the same training views with our fully trained \emph{NeRF} model leads to a considerable improvement in semantic accuracy. The results are reported in \cref{tbl:corrupted_labels}. 
Across all four scenes the semantic accuracy increases by a mean of $12.1\%$ when using a sigmoid activation function for the semantic logits $\vec{s}$. If using no activation function, as originally proposed by \emph{Semantic-NeRF} \cite{semanticnerf}, the semantic accuracy increases only by $10.3\%$.
Comparing the predicted semantic output of the \emph{NeRF} in \cref{subfig:lbl_corruption_nerf_output} with the corrupted labels in \cref{subfig:lbl_corruption_corrupted} shows the ability of the model to largely replace and fix the holes caused by the automatic label degradation. Using no activation function leads to a higher amount of noise as visible in \cref{subfig:lbl_corruption_nerf_output_no_activation}.















