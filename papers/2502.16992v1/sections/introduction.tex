\section{Introduction}



As the number of satellites in orbit continues to rise, high-resolution satellite imagery is more accessible than ever. Image-based reconstruction for creating large-scale environmental models has gained popularity due to its cost-effectiveness compared to dedicated systems like LiDAR. There is also an increasing emphasis on assigning semantic information to 3D environments, which enhances the analysis of urban areas and natural resources, leading to better decision-making in fields such as urban planning, environmental monitoring, and disaster management. 


Traditional methods have focused on extracting explicit representations like pointclouds or meshes from the satellite images by matching image features \cite{s2p,ames,vissat,ssr}.
Subsequent works \cite{pointnet++,pt3,kpconv} assign the semantic information to the extracted representation as additional step.
\emph{Neural Radiance Fields} (\emph{NeRF}) \cite{nerf} take a different approach to 3D-reconstruction by leveraging \emph{Multi-Layer-Perceptrons} (\emph{MLP}). 
The scene structure is represented with the weights of the network. 
Aggregation of \emph{MLP} outputs along visual rays allows rendering of novel, unseen views.
\emph{NeRF} are capable to learn and reproduce additional modalities beside color using a shared three-dimensional representation \cite{semanticnerf,beyondrgb}.
Handling multi-date satellite image data introduces additional challenges such as domain-specific camera models, variable lighting, moving shadows and transient objects such as vehicles. 
Existing works \cite{snerf,satnerf,eonerf,sundial} present \emph{NeRF} adaptions to tackle these challenges.
As of our best knowledge this is the first work to present a dual modality \emph{NeRF} to the satellite domain.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.451\columnwidth]{imgs/intro/test/JAX_214_001_RGB_-1.jpg}
	\hfil
	\includegraphics[width=0.451\columnwidth]{imgs/intro/test/JAX_260_004_RGB_-1_cropped.jpg}
	\caption{Example output of our proposed satellite-domain-adapted semantic \emph{NeRF} model. Fusion of RGB and semantics allows to render the scene in two distinct modalities for novel, during training unseen views. The semantic visualization is enriched with a three-dimensional structure through combination with a learned lighting component.}
	\label{fig:intro_figure}
\end{figure}

\subsection{Contributions}
Our contributions are as following:~(A) A satellite-domain-adapted semantic \emph{NeRF} capable to fuse semantic and color information onto a unified 3D representation.~(B) Improved color consistency for transient areas such as vehicles in inconsistent images by exploiting semantic information.~(C) Demonstration of the robustness/feasibility of our approach and its capability to improve quality of semantic training data through enforcement of multi-view consistency for the satellite domain.~(D) Publicly released dataset with manually generated pixel-wise annotations for 71 satellite images across four multi-view scenes covering five semantic classes: \emph{Ground}, \emph{Water}, \emph{Vegetation}, \emph{Buildings}, \emph{Vehicles}.~(E) Publicly available code base to reproduce our results and allow further research.








\section{Related Works}



\emph{Semantic-NeRF} \cite{semanticnerf} shows that \emph{NeRF} architectures are capable to simultaneously learn a combined appearance and semantic 3D model for a scene given a set of input color images with corresponding semantic labels.
This allows to render novel, unseen views as color image with a corresponding pixel-wise segmentation image.
The two modalities share the same scene structure, leading to a high cohesion in rendering quality. 
Additionally, they show that the inherent multi-view consistency provided by \emph{NeRF} is able to smooth noisy semantic training labels.





\emph{Shadow-NeRF} \cite{snerf} adapts \emph{NeRF} to the satellite domain by introducing an altitude-based ray sampling and an irradiance lighting model. They separate the static scene color from variable shadows and ambient lighting using the sun direction as input. This allows the network to learn highly variant lighting in its own component. 
\emph{SatNeRF} \cite{satnerf} introduces transient-object filtering and adapts the domain-specific \emph{Rational Polynomial Coefficient} (\emph{RPC}) \cite{rpc} camera model for ray generation, increasing accuracy. 
As satellite datasets are usually multi-date, transient objects are moving inbetween image captures. To reduce artifacts \emph{SatNeRF} \cite{satnerf} introduces an uncertainty term, learned based on an image-specific embedding. 
\emph{EO-NeRF} \cite{eonerf} further iterates on handling the varying lighting conditions. Instead of learning shadows they are dynamically rendered based on the scene geometry and sun direction.
\emph{SUNDIAL} \cite{sundial} models complex illumination to improve dynamic shadow rendering of vegetation and water areas.  
Whereas the results for \emph{EO-NeRF} \cite{eonerf} and \emph{SUNDIAL} \cite{sundial} are promising, no code is publicly available. 
We therefore base our work on \emph{Shadow-NeRF} \cite{snerf} and \emph{SatNeRF} \cite{satnerf}.

