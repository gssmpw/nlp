\section{Semantic Multi-View Satellite Dataset}

\begin{figure*}[ht]
	\centering
	\begin{minipage}{0.015\linewidth}
		\centering
		\rotatebox[origin=center]{90}{JAX\_004}
	\end{minipage}
	\begin{minipage}{0.98\linewidth}
		\centering
		\includegraphics[width=.19\linewidth]{imgs/results/gt_rgb/JAX_004_015_RGB.jpg}
		\hfill
		\includegraphics[width=.19\linewidth]{imgs/results/gt/JAX_004_015_CLS.jpg}
		\hfill
		\includegraphics[width=.19\linewidth]{imgs/results/semantic/JAX_004_015_RGB_-1.png}
		\hfill
		\includegraphics[width=.19\linewidth]{imgs/results/shadows/JAX_004_015_RGB_-1.png}
		\hfill
		\includegraphics[width=.19\linewidth]{imgs/results/semantic_shaded/JAX_004_015_RGB_-1.png}\\
	\end{minipage}\\
	
	\vspace{0.2 cm}
	\begin{minipage}{0.015\linewidth}
		\centering
		\rotatebox[origin=center]{90}{JAX\_068}
	\end{minipage}
	\begin{minipage}{0.98\linewidth}
		\centering
		\includegraphics[width=.19\linewidth]{imgs/results/gt_rgb/JAX_068_003_RGB.jpg}
		\hfill
		\includegraphics[width=.19\linewidth]{imgs/results/gt/JAX_068_003_CLS.jpg}
		\hfill
		\includegraphics[width=.19\linewidth]{imgs/results/semantic/JAX_068_003_RGB_-1.png}
		\hfill
		\includegraphics[width=.19\linewidth]{imgs/results/shadows/JAX_068_003_RGB_-1.png}
		\hfill
		\includegraphics[width=.19\linewidth]{imgs/results/semantic_shaded/JAX_068_003_RGB_-1.png}\\
	\end{minipage}\\
	
	
	
	\vspace{0.2 cm}
	\begin{minipage}{0.015\linewidth}
		\centering
		\rotatebox[origin=center]{90}{JAX\_214}
	\end{minipage}
	\begin{minipage}{0.98\linewidth}
		\centering
		\includegraphics[width=.19\linewidth]{imgs/results/gt_rgb/JAX_214_002_RGB.jpg}
		\hfill
		\includegraphics[width=.19\linewidth]{imgs/results/gt/JAX_214_002_CLS.jpg}
		\hfill
		\includegraphics[width=.19\linewidth]{imgs/results/semantic/JAX_214_002_RGB_-1.png}
		\hfill
		\includegraphics[width=.19\linewidth]{imgs/results/shadows/JAX_214_002_RGB_-1.png}
		\hfill
		\includegraphics[width=.19\linewidth]{imgs/results/semantic_shaded/JAX_214_002_RGB_-1.png}\\
	\end{minipage}\\
	
	\vspace{0.2 cm}
	\begin{minipage}{0.015\linewidth}
		\centering
		\rotatebox[origin=center]{90}{JAX\_260}
	\end{minipage}
	\begin{minipage}{0.98\linewidth}
		\centering
		\subcaptionbox{RGB Ground Truth.\label{fig:results_rgb}}{\includegraphics[width=.19\linewidth]{imgs/results/gt_rgb/JAX_260_007_RGB.jpg}}%
		\hfill
		\subcaptionbox{Our Annotations.\label{fig:results_labels}}{\includegraphics[width=.19\linewidth]{imgs/results/gt/JAX_260_007_CLS.jpg}}%
		\hfill
		\subcaptionbox{Predicted Semantic.\label{fig:results_semantic}}{\includegraphics[width=.19\linewidth]{imgs/results/semantic/JAX_260_007_RGB_-1.png}}%
		\hfill
		\subcaptionbox{Predicted Lighting.\label{fig:results_shadows}}{\includegraphics[width=.19\linewidth]{imgs/results/shadows/JAX_260_007_RGB_-1.png}}%
		\hfill
		\subcaptionbox{Semantic Visualization.\label{fig:results_semantic_shaded}}{\includegraphics[width=.19\linewidth]{imgs/results/semantic_shaded/JAX_260_007_RGB_-1.png}}%
	\end{minipage}\\
	
	
	\caption{
		Qualitative results of our proposed \emph{NeRF} model for the popular scenes of the JAX dataset. The pixel-wise semantic annotations in (b) are part of our dataset.
		The domain-adapted \emph{NeRF} model is able to learn and reproduce 
		a representation of the scene containing color and semantic information. By combining the predicted semantic class with the learned lighting component we enrich the visualization with three-dimensional depth cues. }
	\label{fig:results}
\end{figure*}

The publicly available \textit{Urban Semantic 3D} \cite{us3d} dataset provides automatically generated labels for the \emph{Data-Fusion-Contest 2019 Track-3} (\emph{DFC2019}) \cite{dfc2019} dataset. 
The generation process fuses information from LiDAR source data with limited supervision, leading to a high degree of noise and poor accuracy.
Transient objects such as \emph{Vehicles} are not captured in the provided labels.
We therefore chose to produce our own annotations and ensure high quality annotations through a high degree of manual refinement. 
We make it publicly available to further advance research in the field of semantic 3D reconstruction.

Our semantic multi-view satellite dataset contains manually generated annotations for 71 satellite images across four scenes. 
Analog to previous work \cite{snerf,satnerf,eonerf} we select a popular subset containing four scenes ($004, 068,214,260$) of the Jacksonville (Florida, USA) area. The provided Maxar WorldView-3 satellite images are captured between $2014$ and $2016$ with a $0.3$m/pixel resolution at nadir. 


The annotations are generated as pixel-wise segmentation maps and cover the main $256 \times 256$m area-of-interest of each scene. This area is defined by the LiDAR \emph{Digital Surface Model} (\emph{DSM}) contained in  the \emph{DFC2019} \cite{dfc2019} dataset. 
To create the annotations we manually refine initial rough estimates of a user guided, class agnostic foundation model for semantic segmentation \cite{sam}. We manually verify and potentially correct all annotations to ensure high accuracy and consistency across all images. 
The five following classes were selected based on scene content:
\emph{Ground}, \emph{Water}, \emph{Vegetation}, \emph{Buildings} and \emph{Vehicles}.
We show one satellite image per scene in \cref{fig:results_rgb} and its accompanying pixel-wise segmentation in \cref{fig:results_labels}.

