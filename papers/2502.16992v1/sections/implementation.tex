


\section{Implementation and Training Details}

Analog to \emph{SatNeRF} \cite{satnerf} we use $8$ layers of $512$ features for the main feature backbone of the MLP. The additional semantic head consists of a single hidden layer with $256$ features and uses a $\mathit{sigmoid}$ activation function for semantic logit prediction. The architecture of the other heads such as the lighting scalar $\mathit{sun}$ or the uncertainty $\beta$ are identical to \emph{SatNeRF} \cite{satnerf}.

The semantic loss weight is set to $\lambda_s = 0.04$ as suggested by \emph{Semantic-NeRF} \cite{semanticnerf}.
A transient regularization loss value of $\lambda_t = 0.1$ shows in our experiments a good compromise between guiding the network to filter transient objects while simultaneously applying the uncertainty $\beta$ for other color inconsistencies.
The transient embedding $\vec{t}_j$ has a dimension of $4$.
As proposed by \emph{SatNeRF} \cite{satnerf} we use additional depth supervision rays for the first $25\%$ of training iterations with a weight of $\lambda_{DS} = 1000$ and solar correction rays with a weight of $\lambda_{SC} = 0.05$.
Depth supervision refers to using a sparse pointcloud created during the inital \emph{Bundle Adjustment} on the \emph{RPC} camera models for depth guidance. 
Solar correction rays are used as part of the satellite-domain-adapted lighting model to align shadows with the scene structure based on the density. 


We trained all models in this work for $300k$ iterations with an Adam optimizer starting with a learning rate of $5e^{-4}$, which is decreased by a factor of $0.9$ every epoch. Each ray is discretized into $64$ points and we process $1024$ rays each training iteration. 
All models were trained using a NVIDIA RTX 4090 with $24$ GB of RAM. 



