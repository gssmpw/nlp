@misc{videodb,
    author = {{VideoDB}},
    title = {{VideoDB}: Video Infrastructure for the {AI} first world},
    year = {2024},
    url = {https://videodb.io/},
    note = {A modern video processing and analysis platform}
}

@misc{RapidOCR2021,
    author = {{RapidAI} Team},
    title = {{RapidOCR}: A Lightweight {OCR} Framework},
    year = {2021},
    url = {https://github.com/RapidAI/RapidOCR},
    note = {Open-source {OCR} solution}
}

@misc{EasyOCR2024,
    author = {{JaidedAI}},
    title = {{EasyOCR}: Ready-to-Use {OCR} with 80+ Supported Languages},
    year = {2024},
    url = {https://github.com/JaidedAI/EasyOCR}
}

@misc{paddleocr2023,
    author = {{PaddlePaddle} Team},
    title = {{PaddleOCR}: An {OCR} Toolset Based on {PaddlePaddle}},
    year = {2023},
    url = {https://github.com/PaddlePaddle/PaddleOCR}
}

@misc{anthropic2024claude,
    author = {{Anthropic}},
    title = {{Claude 3.5 Sonnet}: Advancements in Multimodal {AI}},
    year = {2024},
    url = {https://www.anthropic.com/news/claude-3-5-sonnet}
}

@misc{google2024gemini,
    author = {{Google DeepMind}},
    title = {{Gemini 1.5 Pro}: Pushing the Boundaries of Multimodal Learning},
    year = {2024},
    url = {https://deepmind.google/technologies/gemini/pro/}
}

@misc{openai2024gpt4,
    author = {{OpenAI}},
    title = {{GPT-4o}: Omni-Modal Language Model},
    year = {2024},
    url = {https://openai.com/index/hello-gpt-4o/}
}

@inproceedings{baek2019character,
  title={Character region awareness for text detection},
  author={Baek, Youngmin and Lee, Bado and Han, Dongyoon and Yun, Sangdoo and Lee, Hwalsuk},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={9365--9374},
  year={2019}
}

@article{graves2012connectionist,
  title={Connectionist temporal classification},
  author={Graves, Alex and Graves, Alex},
  journal={Supervised sequence labelling with recurrent neural networks},
  pages={61--93},
  year={2012},
  publisher={Springer}
}

@article{lee2024vhelm,
  title={Vhelm: A holistic evaluation of vision language models},
  author={Lee, Tony and Tu, Haoqin and Wong, Chi Heem and Zheng, Wenhao and Zhou, Yiyang and Mai, Yifan and Roberts, Josselin Somerville and Yasunaga, Michihiro and Yao, Huaxiu and Xie, Cihang and others},
  journal={arXiv preprint arXiv:2410.07112},
  year={2024}
}

@inproceedings{xu2016msr,
  title={Msr-vtt: A large video description dataset for bridging video and language},
  author={Xu, Jun and Mei, Tao and Yao, Ting and Rui, Yong},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5288--5296},
  year={2016}
}

@article{de2025video,
  title={Video Summarisation with Incident and Context Information using Generative AI},
  author={De Silva, Ulindu and Fernando, Leon and Bandara, Kalinga and Nawaratne, Rashmika},
  journal={arXiv preprint arXiv:2501.04764},
  year={2025}
}

@article{lei2018tvqa,
  title={Tvqa: Localized, compositional video question answering},
  author={Lei, Jie and Yu, Licheng and Bansal, Mohit and Berg, Tamara L},
  journal={arXiv preprint arXiv:1809.01696},
  year={2018}
}

@article{ataallah2024infinibench,
  title={Infinibench: A comprehensive benchmark for large multimodal models in very long video understanding},
  author={Ataallah, Kirolos and Gou, Chenhui and Abdelrahman, Eslam and Pahwa, Khushbu and Ding, Jian and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:2406.19875},
  year={2024}
}