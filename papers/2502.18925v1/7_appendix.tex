
\section{Metric}
\label{sec:metric}

\textbf{Mean Squared Error (MSE)} Mean Squared Error (MSE) is a common statistical metric used to assess the difference between predicted and actual values. The formula is:
\begin{equation}
    MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\end{equation}
where $ n $ is the number of samples, $ y_i $ is the actual value, and $ \hat{y}_i $ is the predicted value.

\textbf{Relative L2 Error} Relative L2 error measures the relative difference between predicted and actual values, commonly used in time series prediction. The formula is:
\begin{equation}
    \text{Relative L2 Error} = \frac{\| Y_{\text{pred}} - Y_{\text{true}} \|_2}{\| Y_{\text{true}} \|_2}
\end{equation}
where $ Y_{\text{pred}} $ is the predicted value and $ Y_{\text{true}} $ is the actual value.

\textbf{Structural Similarity Index Measure (SSIM)} The Structural Similarity Index (SSIM) measures the similarity between two images in terms of luminance, contrast, and structure. The formula is:
\begin{equation}
    SSIM(x, y) = \frac{(2\mu_x \mu_y + C_1)(2\sigma_{xy} + C_2)}{(\mu_x^2 + \mu_y^2 + C_1)(\sigma_x^2 + \sigma_y^2 + C_2)}
\end{equation}
where $ \mu_x $ and $ \mu_y $ are the mean values, $ \sigma_x $ and $ \sigma_y $ are the standard deviations, $ \sigma_{xy} $ is the covariance.

\input{3_related_work}


\section{Detailed Mathematical Proof}
\label{sec:proof}
\textbf{Proof of Theorem 1}

Now we have N augmented data and we need to select the best from them. We consider both the quality and the diversity of these data and get the sampling strategy from an optimization problem.

We model the sampling strategy as a multinomial distribution supported on all the augmented data $S = \{\mathbf{X}_j\}_{j=1}^N$, which means that the sampling strategy $\pi=(\pi_1,...,\pi_N)^\top$ is the corresponding probabilities of selecting $\mathbf{X}_1,...,\mathbf{X}_N$, then we can model the expectation of the similarity as:
$$\begin{aligned}
 & \mathbb{E}_{Y_x,Y_{x^{\prime}}\in\mathcal{C}}\{g(x,x^{\prime})\mid S\} \\
 & =\quad\int g(\mathbf{x},\mathbf{x}^{\prime})\boldsymbol{\pi}(\mathbf{x})\mathrm{Pr}_{S}(Y_{x}\in\mathcal{C}\mid\boldsymbol{x}=\mathbf{x})\boldsymbol{\pi}(\mathbf{x}^{\prime})\mathrm{Pr}_{S}(Y_{x}\in\mathcal{C}\mid\boldsymbol{x}=\mathbf{x}^{\prime})d\mathbf{x}d\mathbf{x}^{\prime} \\
 & =\quad\sum_{i,j=1}^Ng(\mathbf{X}_i,\mathbf{X}_j)\pi_i\pi_j\mathrm{Pr}_{S}(Y_x\in\mathcal{C}\mid\boldsymbol{x}=\mathbf{X}_i)\mathrm{Pr}_{S}(Y_x\in\mathcal{C}\mid\boldsymbol{x}=\mathbf{X}_j),
\end{aligned}$$
where the set $\mathcal{C}$ denotes the criterion of selection we are using, the function $g$ can be chosen as any similarity metric function and $x$ means a random variable.

The core to solving the above optimization problem is to use predictive inference to approximate the conditional probability of $\{Y_x\in\mathcal{C}\}$ given $x = \mathbf{X}$
Let $\mu ( \mathbf{x} ) : = \mathbb{E} ( Y\mid \mathbf{X} = \mathbf{x} )$ be the oracle associated with $( \mathbf{X} , Y) .$ Denote $\theta_j=\mathbb{I}\{Y_j\in\mathcal{C}\}$. As the augmented data
$\mathbf{X}_1,...,\mathbf{X}_N$ are independently identically distributed, $\theta_1,...,\theta_N$ can be regarded as independent Bernoulli($q)$ variables with $q=\Pr(Y_j\in\mathcal{C}).$ The probability distribution of the predicted result $W_j$ for $j=1,...,N$ is
$$\Pr(W_j\mid\theta_j)=(1-\theta_j)f_0+\theta_jf_1,\quad$$
where $f_0$ and $f_1$ are the conditional distributions of $W_j$ on $Y_j \in \mathcal{C}$ or not.

Denote $T(w) = \frac{(1-q)f_0(W_j)}{f(W_j)}$, we can rewrite the expectation of the similarity as
$$\mathbb{E}_{Y_x,Y_{x^{\prime}}\in\mathcal{C}}\{g(x,x^{\prime})|S\}=\sum_{i,j=1}^Ng(\mathbf{X}_i,\mathbf{X}_j)\pi_i\pi_j(1-T_i)(1-T_j)=\boldsymbol{\pi}^\top A_\mathbb{T}\boldsymbol{\pi},$$

Next, we use the expectation to control the quality of the data.
$$\mathbb{E}\{\mathbb{I}(Y_x\not\in\mathcal{C})\mid S\}=\sum_{i=1}^N\Pr(Y_i\not\in\mathcal{C}\mid\mathbf{X}_i)\pi_i=\sum_{i=1}^N\pi_iT_i\leq\alpha,$$

In all, the optimization problem can be modeled as 
\begin{align}
    & \arg\min_{\boldsymbol{\pi}}\quad h(\boldsymbol{\pi},\mathbb{T}):=\boldsymbol{\pi}^\top A_\mathbb{T}\boldsymbol{\pi}, \\
    & \text{subject to} \quad
        \begin{cases}
            \sum_{i = 1}^N\pi_iT_i\leq\alpha, \\
            \sum_{i = 1}^N\pi_i = 1, \\
            0\leq\pi_i\leq m^{-1}, \quad 1\leq i\leq N.
        \end{cases}
\end{align}

where $m$ is used to control the maximum selection.

The best selection of K is determined by the strategy $\pi$ which serves as the solution to the above optimization problem.

\section{Additional Experiments}
\label{sec:more_experiments}
\subsection{Long-term forecasting experiment expansion}

In the long-term forecasting experiments, we compare the performance of different backbone models on the SWE benchmark, evaluating the relative L2 error for three variables (U, V, and H). Our setup inputs 5 frames and predicts 50 frames. For the SimVP-v2 model, using \method{} reduces the relative L2 error for SWE (u) from 0.0187 to 0.0154, SWE (v) from 0.0387 to 0.0342, and SWE (h) from 0.0443 to 0.0397. We visualize SWE (h) in 3D as shown in Figure~\ref{fig:case} [\textcolor{red}{I}]. For the ConvLSTM model, applying \method{} reduces the relative L2 error for SWE (u) from 0.0487 to 0.0321, SWE (v) from 0.0673 to 0.0351, and SWE (h) from 0.0762 to 0.0432. For the FNO model, using \method{} reduces the relative L2 error for SWE (u) from 0.0571 to 0.0502, SWE (v) from 0.0832 to 0.0653, and SWE (h) from 0.0981 to 0.0911. Overall, \method{} significantly improves the long-term forecasting accuracy of different backbone models.

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{image/casestudy.pdf}
    \caption{
    \textcolor{red}{I.} 3D visualization of the SWE(h), showing Ground-truth, SimVP-V2+BeamVQ predictions, and Error at T=1, 10, 20, 30, 40, 50. The first row shows Ground-truth, the second SimVP-V2+BeamVQ predictions, and the third Error. \textcolor{red}{II.} A case study. Building fire simulation with ventilation settings added to Wu's Prometheus~\cite{wu2024prometheus}. (a) Layout and HRR growth. (b) Comparison of physical metrics for different methods. (c) Ground-truth, ResNet+BeamVQ, and ResNet predictions.
    }
    \label{fig:case} 
\end{figure*}


\subsection{Experiment Statistical Significance}
\label{sec:significance}
To measure the statistical significance of our main experiment results, we choose three backbones to train on two datasets to run 5 times. 
Table~\ref{tab:significance} records the average and standard deviation of the test MSE loss.
The results prove that our method is statistically significant to outperform the baselines
because our confidence interval is always upper than the confidence interval of the baselines. 
Due to limited computation resources, we do not cover all ten backbones and five datasets, 
but we believe these results have shown that our method has consistent advantages.


\begin{table}[h]
\label{tab:significance}
\centering
\begin{scriptsize}
    \begin{sc}
    \caption{ The average and standard deviation of MSE in 5 runs}
    \label{tab:significance}
    \centering
        \renewcommand{\multirowsetup}{\centering}
        \setlength{\tabcolsep}{10pt}
        \begin{tabular}{l|cc|cc}
            \toprule
            
            \multirow{4}{*}{Model} & \multicolumn{4}{c}{Benchmarks}  \\
            \cmidrule(lr){2-5}
            & \multicolumn{2}{c}{NSE} &   \multicolumn{2}{c}{SEVIR}   \\
            \cmidrule(lr){2-5}
           & Ori & + BeamVQ & Ori & + BeamVQ  \\
            \midrule
            ConvLSTM &0.4092$\pm$0.0002 &\textbf{0.1277$\pm$0.0001}  & 0.1762 0.0007  & \textbf{0.1279$\pm$0.0009}  \\
            FNO &  0.2227$\pm$0.0003 &\textbf{0.1007 $\pm$0.0002}& 0.0787$\pm$0.0012 & \textbf{ 0.0437$\pm$0.0013} \\
            CNO & 0.2192 $\pm$0.0008 &\textbf{ 0.1492$\pm$0.0011}& 0.0057$\pm$0.0005 & \textbf{ 0.0053$\pm$0.0006} \\
            \bottomrule
        \end{tabular}
    \end{sc}

\end{scriptsize}
\end{table}
