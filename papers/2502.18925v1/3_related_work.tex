\section{Related Work}
\label{sec:related_work}
\begin{itemize}
    \item \textbf{Numerical Methods and Ensemble Forecasting}: they are the traditional methods to realize physical spatial-temporal forecasting~\cite{jouvet2009numerical, rogallo1984numerical, orszag1974numerical, griebel1998numerical}, which employ discrete approximation techniques to solve sets of equations derived from physical laws. Although these physics-driven methods ensure compliance with fundamental principles such as conservation laws~\cite{karpatne2017theory,karnopp2012system,pukrushpan2004control}, they require highly trained professionals for development~\cite{lam2022graphcast}, incur high computational costs~\cite{pathak2022fourcastnet}, are less effective when the underlying physics is not fully known~\cite{takamoto2022pdebench}, and cannot easily improve as more observational data become available~\cite{lam2022graphcast}.
    Moreover, traditional numerical methods usually perturb initial observation inputs with different random noises, which can alleviate the problem of observation errors.  
    Then Ensemble Forecasting \cite{LEUTBECHER20083515, karlbauer2024advancing} can average the outputs of different noisy inputs to improve the robustness.
    
    \item \textbf{Data-Driven Methods}: Recently, data-driven deep learning starts to revolutionize the space of space-time forecasting for complex physical systems~\cite{gao2022earthformer, wu2024earthfarsser, li2020fourier, tan2022simvp, shi2015convolutional, pathak2022fourcastnet, wu2023pastnet,bi2023accurate,lam2022graphcast,zhang2023skilful}. Rather than relying on differential equations governed by physical laws, the data-driven approach constructs model by optimizing statistical metrics such as Mean Squared Error (MSE), using large-scale datasets. These methods~\cite{wang2022predrnn, shi2015convolutional, wang2018eidetic, tan2022simvp, gao2022earthformer, wu2024earthfarsser} are orders of magnitude faster, and excel in capturing the intricate patterns and distributions present in high-dimensional nonlinear systems~\cite{pathak2022fourcastnet}. Despite their success, purely data-driven methods fall short in generating physically plausible predictions, leading to unreliable outputs that violate critical constraints~\cite{bi2023accurate, pathak2022fourcastnet, wu2024neural}.

Previous works have tried to combine physics-driven methods and data-driven methods to get the best of both worlds. Some methods try to embed physical constraints in the neural network~\cite{long2018pde,greydanus2019hamiltonian,cranmer2020lagrangian,guen2020disentangling}. For example, PhyDNet~\cite{guen2020disentangling} adds a physics-inspired PhyCell in the recurrent network. However, such methods require explicit formulation of the physical rules along with specialized designs for network architectures or training algorithms. As a result, they lack flexibility and cannot easily adapt to different backbone architectures. Another type of methods~\cite{raissi2019physics,li2021physics,hansen2023learning}, best exemplified by the Physics-Informed Neural Network (PINN)~\cite{raissi2019physics}, leverages physical equations as additional regularizers in neural network training~\cite{hansen2023learning}. Physics-Informed Neural Operator (PINO)~\cite{li2021physics} extends the data-driven Fourier Neural Operator (FNO) to be physics-informed by adding soft regularizers in the loss function. However, PDE-based regularizers not only impose multiple-object optimization challenges~\cite{krishnapriyan2021characterizing,wang20222} but also only work in limited scenarios where has simplified physical equations and fixed boundary conditions. More recently, PreDiff~\cite{gao2024prediff} trains a latent diffusion model for probabilistic forecasting, and guides the model's sampling process with a physics-informed energy function. However, PreDiff requires training a separate knowledge alignment network to integrate the physical constraints, which is not needed in our method. 

Most importantly, all the above-mentioned works require large-scale datasets to train for good performance, while collecting scientific data can be expensive and even infeasible sometimes.
Still worse,
they can suffer from the poor prediction of extreme events, since there are sparse and imbalance extreme event data even in the large datsets.

\item \textbf{Data Augmentation:} CV has a long history of employing data augmentation to improve generalization. 
Traditionally, almost all CV works manipulate the semantic invariance in images to conduct various data pre-processing, such as random cropping, resizing, flipping, rotation, color normalization, and so on~\cite{kumar2024image}. 
More recently, CV has developed advanced techniques, such as mixup, to generate new data samples by combining different images and their labels, which can achieve even better generalization.
However, these data augmentation techniques are domain-specific, which is based on the domain knowledge of CV~\cite{kumar2024image}. 
Other fields, such as NLP, audio, and physical spatiotemporal forecasting, cannot directly adopt the same data augmentation techniques.

\item \textbf{Self-training:} it has proved to be an effective semi-supervised learning method that exploits the extra unlabeled data~\cite{du2020self}. 
Typically, self training first gives the pseudo labels on the unable data. Then it estimates the confidence of its own classification, and adds the high-confidence samples into training sets to improve the model training. 
However, our work does not have access to extra unlabeled data, making it improper to employ the existing self-training strategies.

\item \textbf{Self-ensemble:} it is well known that Ensemble methods can enhance the performance~\cite{caruana2004ensemble} and improve the robustness ~\cite{tramer2017ensemble}. 
Since Ensemble of different models needs high training costs of multiple models,
self-ensemble~\cite{wang2022self} typically makes use of different states in the training process to be free of extra training costs. 
However, existing self-ensemble cannot explore rare but critical phenomena beyond
the original dataset.

\end{itemize}