\section{Introduction}

In physical spatiotemporal forecasting (e.g., meteorological forecasting~\cite{bi2023accurate, lam2022graphcast}, fluid simulation~\cite{wu2024prometheus, wupure}, and various multiphysics system models~\cite{li2020fourier, wu2024neural}),
%driven by partial differential equations (PDEs)~\cite{li2020fourier, wu2024neural}), 
researchers typically need to capture physical patterns and predict extreme events,
such as heavy rainfall due to severe convective weather~\cite{ravuri2021skilful,doswell2001severe}, marine heatwave~\cite{frolicher2018marine}, and intense turbulence~\cite{moisy2004geometry}). 
However, they suffer from the fundamental problem of data scarcity to ensure physical consistency and accurately predict extreme events.
Collecting large-scale and high-resolution physical data can be expensive and even infeasible.
Consequently, limited training data can prevent data driven models~\cite{sun2020surrogate, zhu2019physics} like physics-informed neural networks~\cite{raissi2019physics} from generalizing well, even though they have adopted physical laws as prior knowledge.
Furthermore, extreme events occur infrequently in nature, making their labeled data quite sparse and imbalanced throughout the entire data set.
Therefore, data-driven methods usually fail to capture these low-probability phenomena.
%On one hand, for physical processes with clear equations or prior laws, lacking sufficient and high-quality observational data often prevents good generalization when using Physics-Informed Neural Networks (PINNs)~\cite{raissi2019physics} or other physics-constrained models~\cite{sun2020surrogate, zhu2019physics} for large-scale high-dimensional predictions. 
%On the other hand, extreme events occur infrequently in nature, making them difficult and costly to observe, which results in even more limited labeled data. \textit{This limitation reduces the ability of conventional data-driven models or ensemble forecasting methods to capture these low-probability phenomena.}
\begin{figure}[t]
  \centering
  \includegraphics[width=1\linewidth]{image/beam_icml_introduction.png}
  \caption{The visualization of extreme marine heatwave events shows that BeamVQ enhances Backbone models and improves their ability to capture extreme events. Detailed experimental results are provided in the experiments section.}
  \label{fig:intro}
\end{figure}

Existing studies on physical spatiotemporal forecasting belong to two categories, namely \textit{Numerical Methods} and \textit{Data-driven Methods}.
Traditional numerical methods like finite difference and finite element simulate future changes by solving physical equations~\cite{jouvet2009numerical, rogallo1984numerical, orszag1974numerical}. Although these numerical methods can be consistent with fundamental physical principles, they not only suffer incredibly expensive computations but also can be infeasible, if we cannot fully understand the underlying mechanism of complex or rare physical events~\cite{takamoto2022pdebench}.
Moreover, they are sensitive to the input disturbance (aka. the Butterfly Effect~\cite{lorenz1972predictability}), so they usually perturb initial conditions with different random noises to make multiple predictions for ensemble forecasting~\cite{LEUTBECHER20083515, karlbauer2024advancing}, resulting in even higher time costs.
    %Similarly,  physics-constrained models like PINN~\cite{raissi2019physics, li2021physics,hansen2023learning} leverage physical equations as additional loss terms to improve physical consistency and generalization, but they can only work on limited problems that have simplified equations and fixed boundary conditions.

Driven by large-scale data, deep learning has emerged as the revolutionary approach for complex physical systems~\cite{shi2015convolutional, gao2022earthformer, tan2022simvp}.
    Some methods attempt to combine physical knowledge with model training for better physical consistency and generalization~\cite{long2018pde,greydanus2019hamiltonian,cranmer2020lagrangian}. 
    For example, PhyDNet~\cite{guen2020disentangling} and FNO~\cite{li2020fourier} add physics-inspired operators into the deep networks.
    Some works also introduce generative settings into extreme weather simulations, but they require numerical simulations to generate enough artificial data~\cite{zhang2023skilful, ravuri2021skilful}.
    Some works like PINN~\cite{raissi2019physics, li2021physics,hansen2023learning} leverage physical equations as additional loss regularization, which only works on specific problems with simplified equations and fixed boundary conditions. 
    PreDiff~\cite{gao2024prediff} trains a latent diffusion model with guidance from a physics-informed energy function. 
    Since these works manipulate physical prior as soft constraints to optimize the statistical metrics across the existing data,
    they rely on large-scale and high-quality data that is non-trivial to collect.
    %and some methods can even require numerical simulations to generate enough artificial data~\cite{raissi2019physics, ravuri2021skilful, zhang2023skilful}. 
    Worse still, extreme events are always a small proportion of the full set, leading to poor prediction.



%\textit{(1)Numerical simulations or neural networks with physical knowledge} Traditional numerical methods (finite difference, finite element, etc.) and \textbf{PINN} \cite{raissi2019physics, karniadakis2021physics} can improve the fitting accuracy for specific problems (such as simplified equations and fixed boundary conditions) using a small amount of data when the equations are known and the scenarios are concentrated. However, if the system coupling is complex, the theoretical equations under ideal assumptions cannot fully describe it. Moreover, when applying to new scenarios, physics-based methods are often limited by insufficient data and struggle to ensure generalization performance \cite{jagtap2020conservative}. 
%\textit{(2) Data-driven Deep Models and Ensemble Forecasting.}  Leveraging large-scale data, Deep Neural Networks (such as convolutional networks, recurrent networks, and self-attention models) achieve breakthroughs in tasks like weather forecasting and turbulence surrogate modeling \cite{shi2015convolutional, dosovitskiy2020image, bi2023accurate}. Methods like Fourier Neural Operators (FNO) \cite{li2020fourier} also exhibit excellent operator-level learning capabilities for PDE systems. However, extreme events still account for a very small proportion of observations, preventing models from receiving sufficient training samples. Some studies introduce Ensemble Forecasting \cite{LEUTBECHER20083515, karlbauer2024advancing}, which enhances coverage of abnormal situations by randomly perturbing initial conditions multiple times, but it incurs high computational costs and struggles to comprehensively capture extreme states when both observational and computational resources are limited.
% todo: need revise by haowu

%Some other works also explore the data synthesis approach by introducing Generative Adversarial Networks (GANs) or Variational Autoencoders (VAEs) into extreme weather simulations~\cite{zhang2023skilful, ravuri2021skilful}. Although they enhance learning of rare scenarios based on artificially synthesized data. However, directly applying these methods to spatiotemporal predictions with strict physical constraints or extreme events still faces challenges in ensuring physical consistency and capturing abnormal distribution characteristics \cite{wu2024physics}. Therefore, developing a solution to the specific problem of \textbf{data scarcity} that ensures physical consistency and enhances the ability to capture extreme events remains a significant research gap.


% \textcolor{red}{Todo: add more details.}

%Some other fields like Computer Vision(CV) and Natural Language Processing(NLP) have explored various techniques to mitigate data scarcity, but these works are domain-specific.
%To mitigate the problem of data scarcity, fields such as computer vision and natural language processing widely adopt Data Augmentation and Self-Training \cite{takase2021self, amini2025self} strategies to expand sample distributions. 

% 因此，如何针对“数据稀缺”这一特定问题，提出兼顾物理合理性与对极端事件捕捉能力，仍是一个重要的研究空白。

% 为了从数据层面根本性地缓解这两大痛点（即\textbf{物理约束}和\textbf{极端事件}均因数据匮乏而面临建模困境），本文提出了一种创新性的学习方法，专门聚焦在“数据稀缺”背景下的时空预测任务，着力于\textbf{数据增强}与\textbf{模型自训练}的协同设计。具体来说，我们首先将时空预测模型的输出离散化，借助向量量化（VQ）机制在潜在空间中搜索可能的未来预测结果的分布；然后通过自集成（Self-Ensemble）策略将多个候选进行融合，以提高预测稳健性与对极端态的覆盖；最后利用自训练（Self-Training）方法在缺少真实标注的场景下生成伪样本，实现对极端事件分布的近似扩增。

Some works in other files have explored various techniques to alleviate the data shortage, but they focus on the classification tasks and exploit domain characters.
For example, Computer Vision (CV) develops data augmentation like Mixup~\cite{zhang2017mixup}, which mixes different images and their labels to generate new samples. 
Natural Language Processing (NLP) conducts self-training to make use of extra unlabeled data with pseudo labels~\cite{du2020self}.
CV also widely adopts self-ensemble like EMA models~\cite{wang2022self} to improve the robustness.
However, physical spatiotemporal forecasting cannot directly adopt these domain-specific methods designed for classification tasks. More details about all related works are in Appendix~\ref{sec:related_work}.

% todo: need revise! by weiyan
%To fundamentally alleviate the two major challenges of \textit{physical constraints} and \textit{extreme events} (both of which face modeling difficulties due to \textbf{data scarcity}), this paper proposes~\method{},
To mitigate the problem of \underline{\textit{data scarcity}} in physical spatiotemporal forecasting, 
we propose Beam Search with Vector Quantization (\textbf{\method{}}) to improve physic consistency and generalization on extreme events.
At its core, it extends the beam search from discrete states typically in NLP to continuous state spaces of this field, enabling the self-ensemble of top-quality outputs for iterative self-training. 
%It specifically focuses on spatio-temporal prediction tasks under "data scarcity" and emphasizes the collaborative design of data augmentation and model self-training. 
Specifically, BeamVQ as a plugin, we can follow previous works to train a base spatiotemporal predictor to generate deterministic outputs. And we construct a variational quantization framework with a vector code book to realize the vector quantization (VQ) mechanism, which discretizes the continuous output spaces of the base prediction. Therefore, we can conduct beam searches through the time steps in physical spatiotemporal forecasting, in a similar way to NLP sentence generation. Through the beam search, we can filter out top-k good-quality candidates with any metrics (even the non-differentiable ones that cannot be directly optimized), leading to better exploration of possible future evolution paths.  Then \method{} develops a new self-ensemble strategy by combining all the top-k candidates. Besides improving the final predicting quality and robustness, the self-ensemble of top candidates can work as additional pseudo samples to iteratively augment the data set for continuous self-training, 
resulting in better physical consistency and generalization even on extreme events.
For example, Figure~\ref{fig:intro} demonstrates our capability in extreme marine heatwave events, whose frequency ranges from one to three annual events~\cite{oliver2018longer}. 

In summary, \method{} has the following main contributions:

\textit{\textbf{Novel Methodology}}. We introduce the \method{} framework, which discretizes outputs via Vector Quantization. By combining Beam Search and self-ensemble strategies, it efficiently explores possible future evolution paths. This approach can significantly enhances the capture of extreme events and increases prediction diversity.

\textit{\textbf{New Training Strategy}}. During the self-training phase, we incorporate "pseudo-labeled" samples from beam search into the training data and iteratively update the model. This process effectively compensates for the lack of real labels and indirectly optimizes any metrics for better physical consistency.

\textit{\textbf{Consistent Improvement}}. We conduct systematic evaluations on multiple datasets, including meteorological, fluid, and PDE simulations, and on different backbone networks such as CNN, RNN, and Transformer. \method{} reduces the average MSE by $18.97\%\sim39.08\%$, showing consistent and significant improvements in accuracy, extreme event capture, and physical plausibility, demonstrating our effectiveness in mitigating data scarcity.














