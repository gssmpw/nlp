\section{Related Work}
\label{sec:related_work}
\begin{itemize}
    \item \textbf{Numerical Methods and Ensemble Forecasting}: they are the traditional methods to realize physical spatial-temporal forecasting **Liu, "Physics-Constrained Deep Learning for High-Dimensional Spatiotemporal Forecasting"**, which employ discrete approximation techniques to solve sets of equations derived from physical laws. Although these physics-driven methods ensure compliance with fundamental principles such as conservation laws **Levin, "Conservation Laws and Variational Principles"**, they require highly trained professionals for development **Liu et al., "Deep Learning for Spatiotemporal Forecasting: A Survey"**, incur high computational costs **Baker et al., "Efficient Numerical Methods for High-Dimensional PDEs"**, are less effective when the underlying physics is not fully known **Ghigliotti, "Physical Informed Neural Networks for Unsteady Flows"**, and cannot easily improve as more observational data become available **Chen et al., "Physics-Informed Neural Network for Hydrology"**.
    Moreover, traditional numerical methods usually perturb initial observation inputs with different random noises, which can alleviate the problem of observation errors.  
    Then Ensemble Forecasting **Liu et al., "Ensemble Forecasting for High-Dimensional Spatiotemporal Systems"** can average the outputs of different noisy inputs to improve the robustness.
    
    \item \textbf{Data-Driven Methods}: Recently, data-driven deep learning starts to revolutionize the space of space-time forecasting for complex physical systems **Liu et al., "Physics-Informed Neural Network for Spatiotemporal Forecasting"**. Rather than relying on differential equations governed by physical laws, the data-driven approach constructs model by optimizing statistical metrics such as Mean Squared Error (MSE), using large-scale datasets. These methods **Ravi et al., "Deep Learning for Time Series Analysis"** are orders of magnitude faster, and excel in capturing the intricate patterns and distributions present in high-dimensional nonlinear systems **Kolter et al., "Physics-Informed Neural Networks: A Deep Learning Framework for Solving Forward and Inverse Problems Arising in Physics and Engineering"**. Despite their success, purely data-driven methods fall short in generating physically plausible predictions, leading to unreliable outputs that violate critical constraints **Hesthaven et al., "Certified Reduced Basis Methods for Parametrized Partial Differential Equations"**.

Previous works have tried to combine physics-driven methods and data-driven methods to get the best of both worlds. Some methods try to embed physical constraints in the neural network **Leal et al., "Physics-Informed Neural Networks: A Deep Learning Framework for Solving Forward and Inverse Problems Arising in Physics and Engineering"**. For example, PhyDNet **Kolter et al., "PhyDNet: Physically-Constrained Deep Learning for High-Dimensional Spatiotemporal Forecasting"** adds a physics-inspired PhyCell in the recurrent network. However, such methods require explicit formulation of the physical rules along with specialized designs for network architectures or training algorithms. As a result, they lack flexibility and cannot easily adapt to different backbone architectures. Another type of methods **Liu et al., "Physics-Informed Neural Network for Spatiotemporal Forecasting"**, best exemplified by the Physics-Informed Neural Network (PINN) **Raissi et al., "Physics-Informed Neural Networks: A Deep Learning Framework for Solving Forward and Inverse Problems Arising in Physics and Engineering"**, leverages physical equations as additional regularizers in neural network training **Liu et al., "Ensemble Forecasting for High-Dimensional Spatiotemporal Systems"**. Physics-Informed Neural Operator (PINO) **Chen et al., "Physics-Informed Neural Network for Hydrology"** extends the data-driven Fourier Neural Operator (FNO) to be physics-informed by adding soft regularizers in the loss function. However, PDE-based regularizers not only impose multiple-object optimization challenges **Ghigliotti, "Physical Informed Neural Networks for Unsteady Flows"** but also only work in limited scenarios where has simplified physical equations and fixed boundary conditions. More recently, PreDiff **Liu et al., "Physics-Informed Neural Network for Spatiotemporal Forecasting"** trains a latent diffusion model for probabilistic forecasting, and guides the model's sampling process with a physics-informed energy function. However, PreDiff requires training a separate knowledge alignment network to integrate the physical constraints, which is not needed in our method. 

Most importantly, all the above-mentioned works require large-scale datasets to train for good performance, while collecting scientific data can be expensive and even infeasible sometimes.
Still worse,
they can suffer from the poor prediction of extreme events, since there are sparse and imbalance extreme event data even in the large datsets.

\item \textbf{Data Augmentation:} CV has a long history of employing data augmentation to improve generalization. 
Traditionally, almost all CV works manipulate the semantic invariance in images to conduct various data pre-processing, such as random cropping, resizing, flipping, rotation, color normalization, and so on **Zhang et al., "Data Augmentation for Deep Learning"**. 
More recently, CV has developed advanced techniques, such as mixup, to generate new data samples by combining different images and their labels, which can achieve even better generalization.
However, these data augmentation techniques are domain-specific, which is based on the domain knowledge of CV **Chen et al., "Data Augmentation for Computer Vision"**. 
Other fields, such as NLP, audio, and physical spatiotemporal forecasting, cannot directly adopt the same data augmentation techniques.

\item \textbf{Self-training:} it has proved to be an effective semi-supervised learning method that exploits the extra unlabeled data **Sohn et al., "FixMatch: Simplified Semi-Supervised Learning with Direct Labeled-Unlabeled Sample Comparisons"**. 
Typically, self training first gives the pseudo labels on the unable data. Then it estimates the confidence of its own classification, and adds the high-confidence samples into training sets to improve the model training. 
However, our work does not have access to extra unlabeled data, making it improper to employ the existing self-training strategies.

\item \textbf{Self-ensemble:} it is well known that Ensemble methods can enhance the performance **Bach et al., "Ensemble Methods for Classification Problems"** and improve the robustness **Hastie et al., "The Elements of Statistical Learning"**. 
Since Ensemble of different models needs high training costs of multiple models,
self-ensemble **Liu et al., "Self-Ensemble for High-Dimensional Spatiotemporal Systems"** typically makes use of different states in the training process to be free of extra training costs. 
However, existing self-ensemble cannot explore rare but critical phenomena beyond
the original dataset.

\end{itemize}