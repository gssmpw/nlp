\section{Experiment}
\label{sec:experiment}
\begin{table*}[htbp]
  \centering
  \begin{sc}
  \caption{Performance comparison of various models with and without the \method{} method across five benchmark tests (SWE(u), RBC, NSE, Prometheus, SEVIR), using MSE as the evaluation metric. We bold-case the entries with lower MSE. ``Improvement'' represents the average percentage improvement in MSE achieved with \method{}.}
    \label{tab:mainres}
    \resizebox{\linewidth}{!}{%
      \begin{tabular}{l|cc|cc|cc|cc|cc}
        \toprule
        \multirow{4}{*}{Model} & \multicolumn{10}{c}{Benchmarks}  \\
        \cmidrule(lr){2-11}
        & \multicolumn{2}{c}{SWE (u)} & \multicolumn{2}{c}{RBC} & \multicolumn{2}{c}{NSE} &  \multicolumn{2}{c}{Prometheus} &  \multicolumn{2}{c}{SEVIR}   \\
        \cmidrule(lr){2-11}
        & Ori & + \method{} & Ori & + \method{} & Ori & + \method{} & Ori & + \method{}& Ori & + \method{} \\
        \midrule
        ResNet &0.0076 & \textbf{0.0033} & 0.1599 &\textbf{0.1283} & 0.2330 &\textbf{0.1663}  & 0.2356 &\textbf{0.1987} &  0.0671&  \textbf{0.0542} \\
        ConvLSTM &0.0024 &\textbf{0.0016}  &0.2726  & \textbf{0.0868} & 0.4094& \textbf{0.1277} & 0.0732 &\textbf{0.0533}  & 0.1757 & \textbf{0.1283}  \\
        Earthformer & 0.0135&\textbf{0.0093}  & 0.1273 &\textbf{0.1093} &1.8720  &  \textbf{0.1202} &0.2765  &\textbf{0.2001}  &0.0982  & \textbf{0.0521}  \\
        SimVP-v2 & 0.0013&\textbf{0.0010}  & 0.1234 & \textbf{0.1087} & 0.1238 &\textbf{0.1022}  & 0.1238 &\textbf{0.0921}  &0.0063&  \textbf{0.0032}\\
        TAU &0.0046 & \textbf{0.0031} & 0.1221 &\textbf{0.0965}  & 0.1205 &\textbf{0.1017} & 0.1201 &\textbf{0.0899}& 0.0059 &  \textbf{0.0029} \\
        Earthfarseer &0.0075 &\textbf{0.0059}  & 0.1454 &\textbf{0.1023}  & 0.1138 &\textbf{0.0987}  & 0.1176 &\textbf{0.1092}&  0.0065  &  \textbf{0.0021}  \\
        FNO & 0.0031&  \textbf{0.0024}& 0.1235 & \textbf{0.1053} & 0.2237 & \textbf{0.1005} & 0.3472 & \textbf{0.2275} & 0.0783 &  \textbf{0.0436} \\
        NMO &0.0021 &\textbf{0.0004}  &0.1123  & \textbf{0.1092} & 0.1007 & \textbf{0.0886} &0.0982  &\textbf{0.0475}  & 0.0045 & \textbf{0.0029}  \\
        CNO & 0.0146&  \textbf{0.0016}& 0.1327 & \textbf{0.1086} &0.2188 & \textbf{0.1483} &0.1097  &  \textbf{0.0254}&0.0056 &  \textbf{0.0053}  \\
        FourcastNet &0.0065 &\textbf{0.0061}  & 0.0671 & \textbf{0.0219} & 0.1794 &\textbf{0.1424}  &0.0987  &\textbf{0.0542}  & 0.0721 &\textbf{0.0652}   \\
        \midrule
         Improvement(\%)& \multicolumn{2}{c|}{+39.08$\%$}  &  \multicolumn{2}{c|}{+18.97$\%$}   &   \multicolumn{2}{c|}{+35.83$\%$}   &  \multicolumn{2}{c|}{+33.65$\%$}   &  \multicolumn{2}{c}{+35.27$\%$}   \\
        \bottomrule
      \end{tabular}
    }
  \end{sc}
\end{table*}
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{image/mh_icml.png}
    \caption{\textbf{The prediction results of marine extreme heatwave events include}: A visual comparison (from left to right): ground truth labels, SimVP+\method{} prediction results on day 10, and SimVP prediction results on day 10. The cumulative changes of RMSE over prediction time. The cumulative changes of CSI over prediction time.}
    \label{fig:mh_icml}
\end{figure*}
In this section, we verify the effectiveness of our method by evaluating 5 benchmarks and 10 backbone models. The experiments aim to answer the following research questions:
\textit{{\textbf{RQ1.}}} Can~\method{} enhance the performance of the baselines? 
\textit{{\textbf{RQ2.}}} How does~\method{} perform under data-scarce conditions?
\textit{\textbf{RQ3.}} Can~\method{} have better physical alignment? 
\textit{\textbf{RQ4.}} Can~\method{} produce long-term forecasting?  
Appendix~\ref{sec:more_experiments} also has additional results.

\subsection{Experimental Settings}
\textbf{Benchmarks \& Backbones.} Our dataset spans multiple spatiotemporal dynamical systems, summarized as follows: \textbf{$\bullet$ Real-world Datasets}, including SEVIR~\cite{veillette2020sevir}; \textbf{$\bullet$ Equation-driven Datasets}, focusing on PDE~\cite{takamoto2022pdebench} (Navier-Stokes equations, Shallow-Water Equations) and Rayleigh-Bénard convection flow~\cite{wang2020towards}; \textbf{(3) Computational Fluid Dynamics Simulation Datasets}, namely Prometheus~\cite{wu2024prometheus}.  We select core models from three different fields for analysis. Specifically: \textbf{$\bullet$ Spatio-temporal Predictive Learning}, we choose ResNet~\cite{he2016deep}, ConvLSTM~\cite{shi2015convolutional}, Earthformer~\cite{gao2022earthformer}, SimVP-v2~\cite{tan2022simvp}, TAU~\cite{tan2023temporal},  Earthfarseer~\cite{wu2024earthfarsser}, and FourcastNet~\cite{pathak2022fourcastnet}as representative models; \textbf{$\bullet$ Neural Operator}, we compare models like FNO~\cite{li2020fourier}, NMO~\cite{wu2024neural} and CNO~\cite{raonic2024convolutional};

\textbf{Metric.} We use Mean Squared Error (MSE) as the evaluation metric to assess each model's prediction performance. Additionally, to thoroughly evaluate the model's performance on specific tasks, we employ metrics such as Root Mean Squared Error (RMSE), Critical Success Index (CSI), Structural Similarity Index (SSIM), relative L2 error, and Turbulent Kinetic Energy (TKE). More details can be found in Appendix~\ref{sec:metric}.

\textbf{Implementation details.} Our method trains with MSE loss, uses the ADAM optimizer~\cite{kingma2014adam}, and sets the learning rate to $10^{-3}$. We set the batch size to 10. The training process early stops within 500 epochs. 
Additionally, we set our code bank size as $1024\times 64$, beam size $K$ as 5 or 10, and the threshold as the first quartile of all candidate's scores, which we find suitable for all backbones. We implement all experiments in PyTorch~\cite{paszke2019pytorch}. Training and inference for all our experiments run on a single NVIDIA A100-PCIE-40GB GPU.

\begin{table}[htbp]
  \centering
  \small
  \begin{sc}
    \caption{Comparison of backbones on marine heatwaves to evaluate \method{}'s ability to capture extreme events.}
    \label{tab:heatwaves}
      \begin{tabular}{lccc}
        \toprule
        \multirow{2}{*}{Model} & \multicolumn{2}{c}{MSE} & \multirow{2}{*}{Promotion (\%)} \\ 
        \cmidrule(lr){2-3}
        & Ori & +\method{} &  \\ 
        \midrule
        U-Net & 0.0968 & \textbf{0.0848} & 12.40\% \\
        ConvLSTM & 0.1204 & \textbf{0.0802} & \textbf{33.38\%} \\
        SimVP & 0.0924 & \textbf{0.0653} & 29.33\% \\
        \bottomrule
      \end{tabular}%
  \end{sc}
\end{table}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{image/rq2.png}
    \caption{\textbf{The \method{} plugin improves physical consistency and prediction accuracy.} \textcolor{red}{(a)} shows a visual comparison of the actual target, predicted results, and errors at different time steps. \textcolor{red}{(b)} displays the changes in SSIM, RMSE, and relative L2 error over time steps. \textcolor{red}{(c)} compares the turbulent TKE. \textcolor{red}{(d)} presents the energy spectrum at different wavenumbers.}
    \label{fig:phy}
\end{figure*}
\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{image/modelayalise.pdf}
    \caption{The t-SNE visualization in (a), (b), and (c) shows the Ground-truth, ConvLSTM and ConvLSTM+\method{} predictions, respectively. (d) shows the analysis of the Codebank parameters.}
    \label{fig:tsne} 
\end{figure*}




\subsection{\method{} improves all backbone models (RQ1)}

As shown in Table~\ref{tab:mainres}, \method{} significantly improves performance across five benchmark tests and ten backbone models. After introducing \method{}, all models show a decreasing trend in MSE, with average improvements ranging from 18.97\% to 39.08\%. For example, in the RBC fluid convection task, ConvLSTM's MSE decreases from 0.2726 to 0.0868 (a 68.15\% improvement), indicating a strong enhancement in capturing complex physical dynamics. Earthformer’s MSE in the NSE turbulence prediction task drops sharply from 1.8720 to 0.1202 (a 93.58\% improvement), demonstrating \method{}'s advantage in modeling high-dimensional chaotic systems. Even advanced models like SimVP-v2 and TAU see MSE reductions of 49.21\% and 50.85\%, respectively, in the SEVIR extreme weather prediction task, proving \method{}'s compatibility with advanced architectures. In the Prometheus combustion dynamics task, the FNO operator reduces MSE by 34.47\% (from 0.3472 to 0.2275), highlighting its enhanced ability to incorporate physical constraints. FourcastNet's MSE in the RBC task decreases from 0.0671 to 0.0219 (a 67.36\% improvement). The lightweight ResNet model's MSE in the SWE shallow water equations task drops by 56.58\% (from 0.0076 to 0.0033), demonstrating that significant accuracy gains can be achieved without complex architectures.


\subsection{\method{} helps alleviate data scarcity (RQ2)}
In scientific computing, data scarcity is a core challenge. We use extreme marine heatwaves as a scenario closely linked to human activities and economic development. To evaluate model performance, we adopt RMSE (numerical accuracy) and CSI (extreme-event capture). We compare U-Net, ConvLSTM, and SimVP as backbone networks. Table~\ref{tab:heatwaves} and Figure~\ref{fig:mh_icml} present the results, followed by our analysis. First, Table~\ref{tab:heatwaves} shows that \method{} significantly lowers MSE on the extreme marine heatwave task (e.g., ConvLSTM’s error decreases from 0.1204 to 0.0802, SimVP’s from 0.0924 to 0.0653). Even in data-scarce scenarios, these models better capture dynamic changes and improve overall prediction accuracy. Second, Figure~\ref{fig:mh_icml} compares day-10 visualizations and plots RMSE and CSI over time, indicating that \method{} generates distributions closer to the real sea temperature fields. The cumulative RMSE remains lower for models with \method{}, and CSI stays high, suggesting stronger sensitivity to extreme events and more accurate forecasts throughout the prediction period.




\subsection{\method{} Boosts Physical Alignment (RQ3)}
Figure~\ref{fig:phy} shows that \method{} significantly enhances physical consistency and prediction accuracy. In Figure~\ref{fig:phy}(a), comparing actual targets, predicted results, and errors at different time steps reveals better detail and physical consistency with smaller errors when using \method{}. Figure~\ref{fig:phy}(b) shows that \method{} improves SSIM by 23.40\%, and reduces RMSE and relative L2 error by 37.07\% and 45.46\%, respectively, indicating stronger robustness in spatiotemporal prediction. Figure~\ref{fig:phy}(c) compares turbulent kinetic energy (TKE), demonstrating more accurate capture of TKE changes, especially in small-scale turbulence. Figure~\ref{fig:phy}(d) displays the energy spectrum at different wavenumbers, where \method{} maintains better physical consistency in high-wavenumber regions—indicative of more accurate small-scale vortex prediction. Overall, \method{} not only improves numerical accuracy but also better captures the essence of physical phenomena.


\subsection{\method{} Excels In Long-term Dynamic System Forecasting (RQ4)}   
\label{sec:long}
In our long-term forecasting experiments on the SWE benchmark, we compare different backbone models by evaluating the relative L2 error for three variables (U, V, and H). We input 5 frames and predict 50 frames. For the SimVP-v2 model, using \method{} reduces the relative L2 error for SWE (u) from 0.0187 to 0.0154, SWE (v) from 0.0387 to 0.0342, and SWE (h) from 0.0443 to 0.0397, with the 3D visualization of SWE (h) shown in Figure~\ref{fig:case} [\textcolor{red}{I}]. For ConvLSTM, applying \method{} reduces the relative L2 error for SWE (u) from 0.0487 to 0.0321, SWE (v) from 0.0673 to 0.0351, and SWE (h) from 0.0762 to 0.0432. For FNO, using \method{} reduces the relative L2 error for SWE (u) from 0.0571 to 0.0502, SWE (v) from 0.0832 to 0.0653, and SWE (h) from 0.0981 to 0.0911. These results, obtained under a consistent experimental protocol, underscore the efficacy of \method{} in systematically mitigating prediction errors over extended time horizons, thereby enhancing the stability and robustness of each model's forecasts. Overall, \method{} significantly enhances the long-term forecasting accuracy of these backbone models, offering promising implications for its application in complex dynamical systems and real-world fluid dynamics scenarios.
\begin{table}[t]
  \centering
   \small
  \begin{sc}
    \caption{We compare different backbones on the SWE Benchmark for Long-term Forecasting.}
    \label{tab:time}
    \small
      \begin{tabular}{lccccc}
        \toprule
        \multirow{2}{*}{Model} & \multirow{2}{*}{} & \multicolumn{3}{c}{SWE} \\ 
        \cmidrule(lr){3-5}
        & & (u) & (v) & (h) \\ 
        \midrule
        \multirow{2}{*}{Simvp-v2} & Ori & 0.0187 & 0.0387 & 0.0443 \\
        & +\method{} & \textbf{0.0154} & \textbf{0.0342} & \textbf{0.0397} \\ 
        \midrule
        \multirow{2}{*}{ConvLSTM} & Ori & 0.0487 & 0.0673 & 0.0762 \\
        & +\method{} & \textbf{0.0321} & \textbf{0.0351} & \textbf{0.0432} \\ 
        \midrule
        \multirow{2}{*}{FNO} & Ori & 0.0571 & 0.0832 & 0.0981 \\
        & +\method{} & \textbf{0.0502} & \textbf{0.0653} & \textbf{0.0911} \\ 
        \midrule
        \multirow{2}{*}{CNO} & Ori & 0.1283 & 0.1422 & 0.1987 \\
        & +\method{} & \textbf{0.0621} & \textbf{0.0674} & \textbf{0.0965} \\ 
        \bottomrule
      \end{tabular}%
  \end{sc}
\end{table}

\begin{table}[t]
 \small
  \centering
  \begin{sc}
    \caption{Ablation studies on the NSE benchmark.}  
    \label{tab:ablation}
    \resizebox{\linewidth}{!}{%
      \begin{tabular}{l|c|c}
        \toprule
        Variants & MSE & TKE \\ 
        \midrule
        FNO & 0.2237 & 0.3964 \\
        FNO+\method{} & \textbf{0.1005} & \textbf{0.1572} \\ 
        FNO+\method{} (w/o BeamS) & 0.1207 & 0.2003 \\ 
        FNO+\method{} (w/o SelfT) & 0.1118 & 0.1872 \\ 
        FNO+\method{} (w. MSE) & 0.1654 & 0.2847 \\  
        FNO+VQVAE & 0.1872 & 0.3652 \\ 
        FNO+PINO & 0.1249 & 0.2342 \\  
        \midrule
      \end{tabular}%
    }
  \end{sc}
\end{table}


\subsection{Interpretation Analysis \& Ablation Study}

\textbf{Qualitative Analysis Using t-SNE.} Figure~\ref{fig:tsne} shows t-SNE visualizations on the RBC dataset: (a) ground truth, (b) ConvLSTM predictions, and (c) ConvLSTM + \method{} predictions. In (a), the ground truth has clear clusters. In (b), ConvLSTM’s clustering is blurry with overlaps, indicating limited capability in capturing data structure. In (c), ConvLSTM + \method{} yields clearer clusters closer to the ground truth, demonstrating that \method{} significantly enhances the model’s predictive accuracy and physical consistency.

\textbf{Analysis on Code Bank.} We train FNO+\method{} on NSE for 100 epochs with a learning rate of 0.001 and batch size of 100. In the VQVAE codebank dimension experiment, increasing the number of vectors $L$ notably reduces MSE. When $L=1024$ and $D=64$, the MSE reaches a minimum of 0.1271. Although MSE fluctuates more at $L=256$ or $512$, overall, higher $L$ helps improve accuracy. Most training losses quickly stabilize within 20 epochs; $L=512$ and $D=128$ notably shows higher stability, but $L=1024$ and $D=64$ achieves the lowest MSE.

\textbf{Ablation Study.} 
We use NSE with FNO for ablation. Variants: 
(I) \textit{FNO}; 
(II) \textit{FNO+\method{}}; 
(III) \textit{FNO+\method{} (w/o Beamsearch)}; 
(IV) \textit{FNO+\method{} (w/o self-Training)}; 
(V) \textit{FNO+\method{} (w MSE)}; 
(VI) \textit{FNO+VQVAE}; 
(VII) \textit{FNO+PINO}~\cite{10.1145/3648506}. 
Table~\ref{tab:ablation} shows FNO starts with an MSE of 0.2237 and a TKE error of 0.3964. Adding \method{} drops them to 0.1005 and 0.1572. Omitting Beamsearch or self-training increases MSE but still outperforms the base. VQVAE and PINO yield MSEs of 0.1872 and 0.1249, with TKE errors of 0.3652 and 0.2342. Overall, \method{} significantly enhances accuracy.




