\section{Introduction\label{sec:introduction}}

Assessing the writing quality of essays manually is both time-consuming and labor-intensive. This task becomes even more demanding and challenging due to high cognitive load \citep{Cai2015}, when assessors have to assign scores and provide comments based on multi-dimensional analytic criteria, referred to here as \textit{multi-dimensional analytic assessments} (see Fig.~\ref{fig:illustraion} for an illustration). For evaluation of non-native language (L2) learners' writing, such precise and multi-dimensional assessments are highly valuable and desirable, but they are often not provided, due to the significant time, cost, and expertise required to produce them. This is also evidenced by the dearth of publicly available L2 writing corpora annotated with multi-dimensional analytic assessments \citep{banno-etal-2024-gpt}.


\begin{figure}[]
    \centering
    \small
    \includegraphics[width=1\linewidth]{figures/illustration_fig.png}
    \caption{Multi-dimensional analytic assessments, where each assessment contains a score and a comment.}
    \label{fig:illustraion}
\end{figure}


In recent years, large language models (LLMs) have emerged as promising tools for self-regulated writing assessments among L2 learners. A growing number of studies \citep[][\textit{i.a.}]{chiang-lee-2023-large, MIZUMOTO2023100050, han-etal-2024-llm, yancey-etal-2023-rating} have indicated the general usefulness of LLMs for automated writing assessments. Given their increasing use for this task, the following question remains understudied: \textit{can LLMs provide reasonably good multi-dimensional analytic writing assessments?} We use the phrase ``\textbf{reasonably good}'' intentionally, given the open-ended nature of the task, particularly generating essay-level feedback comments.



To address this question, we utilize an English-language corpus of literature reviews written by L2 graduate students and assessed by human experts on 9 analytic assessment criteria. We prompt various popular LLMs to assess the corpus using the same criteria under various conditions, and we examine the quality of their generated assessments compared to human-generated assessments.

\smallskip
\noindent Our study makes three primary contributions: 

\begin{enumerate}
    % \itemsep-0.25em
    \item We provide comprehensive and reproducible evidence that LLMs can generate reasonably good and generally reliable multi-dimensional analytic writing assessments. This is the primary goal of this study; we do not argue in favor of a specific LLM, nor do we advocate replacing humans with LLMs for this task.
    
    \item We release a corpus of L2 English graduate-level literature reviews, annotated with multi-dimensional analytic assessments, which will facilitate future studies.
    
    \item We propose and validate a novel LLM-based framework for evaluating the quality of feedback comments. This framework is time- and cost-efficient, scalable, and reproducible, compared to manual judgments. It is also interpretable, compared to direct quality ratings. 

    
\end{enumerate}

