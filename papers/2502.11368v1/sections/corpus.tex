\section{Corpus\label{sec:corpus}}


\paragraph{Overview} Our corpus consists of 141 literature reviews written in English by 51 L2 graduate students, with an average word count of 1321 (930 excluding references). The reviews cover five broad topics from the humanities and social sciences, chosen to minimize the need for specialized disciplinary knowledge: (1) the social consequences of legalized cannabis, (2) the Canadian linguistic landscape, (3) online learning, (4) lessons from the COVID-19 pandemic, and (5) pacifism. Essays on topics 1, 3, and 5 were written individually, while those on topics 2 and 4 were completed collaboratively by 2-4 authors.


The corpus is a result of a large research project conducted at the University of Saskatchewan in 2021 with an aim to examine the developmental trajectory of literature review writing skills among L2 graduate students. The project involved three rounds of a 5-unit online tutorial series conducted over the course of 2021, with each round lasting 13 weeks (see Appendix~\ref{app:corpus} for details). Participation was voluntary, with 31 participants completing all five writing tasks across all rounds, and 20 further students completing at least one task before withdrawing. 


\paragraph{Our Previous Studies} The corpus was used in our previous studies \citep{li2023assessment, li2023developing, makarova2024can}, but has never been made public. These three studies all use only a subset of the corpus, namely essays written individually or those based on topics 1, 3, and 5.   

More concretely, \citet{li2023assessment, li2023developing} focus on the individual writing skill development, but without examining the feedback comments provided in the corpus. In other words, these two studies belong to the field of English for Academic Purposes, but has less relevance to AWE. 

While \citet{makarova2024can} investigate whether ChatGPT can assess L2 academic English writing, they do not compare human- and ChatGPT-generated scores and comments on the basis of each assessment criterion. Rather, they simply compare ChatGPT-generated scores and comments with average scores and concatenated comments produced by multiple human assessors for all criteria, which is not only less nuanced but also over-simplistic. Moreover, their analysis of feedback comments utilizes surface-level linguistic features such as word counts, type-token ratio, comment length, and the experiments do not consider possible user-LLM interaction modes nor prompt variations, which we do in Sections~\ref{sec:experiments} and ~\ref{sec:furtherAnalyses}, respectively. 

In short, this study not only presents a more comprehensive and thorough evaluation of LLMs using the full corpus, instead of its subset, but also employs a different set of evaluation methodologies. As a result, we identify no substantial overlap between this study and our previous studies.



\paragraph{Essay Authors} The corpus authors comprise a diverse group of L2 learners, representing a wide range of first languages and enrolled in graduate programs across various disciplines at multiple Canadian universities. Their English proficiency ranged from upper-intermediate to advanced, with an average score equivalent to IELTS Band 7 based on conversions from various standardized English language tests. Scores varied from IELTS 6.5 to 8.5, with a standard deviation of 0.55.


\paragraph{Human Assessments} Most essays in the corpus were assessed by three (94.3\%) or two (5.0\%) independent human experts. As illustrated in Fig.~\ref{fig:illustraion}, the assessments consist of scores on a 10-point scale and comments based on 9 analytic assessment criteria. While scores were required, comments were optional for the assessors. A total of six assessors with professional experience in English language teaching participated at different stages of the research project. Table~\ref{tab:feedbackRate} provides basic information about them.

The 9 assessment criteria (see Appendix~\ref{app:criteria} for details) include: (C1) material selection; (C2) material integration and citation; (C3) quality of key components; (C4) logic of structure; (C5) content and clarity of ideas; (C6) coherence (flow of ideas) ; (C7) cohesion (use of connectors); (C8) grammar and sentence structure; and (C9) academic vocabulary. 




\paragraph{Assessment Quality}  The 31 students who completed all writing tasks evaluated the quality of human assessments on a 4-point scale in an anonymous final project survey. Based on the 30 submitted survey responses, all participants agreed that the assessments were at least ``useful'' (rating = 3), with 24 participants (80\%) rating them as ``very useful'' (rating = 4). 

\paragraph{Data Contamination} Since the corpus was created prior to the release of ChatGPT and has never been made public, it contains no LLM-generated contents and is free from the risk of data contamination \citep{jacovi-etal-2023-stop, sainz-etal-2023-nlp}, making it an ideal resource for LLM evaluation.


\input{tables/assessors}
