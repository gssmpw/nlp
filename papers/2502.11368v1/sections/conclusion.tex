\section{Conclusion\label{sec:conclusion}} 

This study provides evidence that LLMs can generate reasonably good and generally reliable multi-dimensional analytic assessments. Our findings highlight the promising role of LLMs in assessing academic English writing, especially for graduate-level literature reviews, which is a highly technical genre. In short, LLMs show strong pedagogical potential, benefiting both L2 learners and instructors for self-regulated learning or teaching assistance. We propose and validate a novel feedback comment quality framework to facilitate our analysis.

% \jordan{There's space. Re-add a paragraph which summarizes our main results in some more detail}
% \jack{I highlight the contribution instead.}


Looking ahead, future studies could further characterize and compare the writing problems identified by human- versus LLM-generated comments, offering deeper qualitative insights. Additionally, it would be valuable to develop a metric grounded in our proposed framework that can directly compare the relative quality of two sets of comments.


% This study demonstrates with comprehensive evidence that LLMs can generate reasonably good multi-dimensional analytic writing assessments for L2 graduate-level literature reviews. We also show that LLM-generated assessments are generally reliable across various prompting conditions tested. 

% \paragraph{Educational Implications.} Our findings showcase the promising role LLMs can play in automated writing assessments, which can benefit both L2 learners and instructors, either for self-regulated learning or teaching assistance. Given the highly technical nature of literature review writing and the extended length of essays in our corpus, our results may imply that LLMs are well-suited for analytically assessing academic English writing in general. The wide range of assessment criteria included in our corpus highlight the general capabilities of LLMs providing writing assessments.


% \paragraph{Methodological Implications.} We propose and validate a novel, LLM-based feedback comment quality evaluation framework. Compared to existing methods that rely on manual judgments, our framework is cost-efficient, scalable, and reproducible. It is also more interpretable than directly rating feedback comment quality using humans or LLMs, as it associates the feedback comment quality with the number and characteristics of essay problems identified. The framework can be applied to various types of L2 writing and adapted to facilitate text quality analysis in other domains.

% \paragraph{Future Research.} There are two research questions worth further investigations. First, it is interesting to examine why and how interaction modes affect the levels of elaboration, specificity, and helpfulness of feedback comments generated under them, while the respective scores remain similar. Second, our study uses descriptive results from our proposed framework to evaluate the general quality of some feedback comments. It would be important, however, to derive a metric that can directly compare the relative quality of two sets of comments, if possible. 

