\section{Experiments\label{sec:experiments}}

This sections describes and presents the main experiments conducted and the results obtained.


\subsection{LLM Prompting\label{sec:prompting}}

\paragraph{List of LLMs} We evaluate variants of three popular LLMS: \textsc{gpt-4o-2024-08-06} (GPT-4o, \citealp{openai2024gpt4ocard}), \textsc{Gemini-1.5-flash} (Gemini-1.5, \citealp{geminiteam2024gemini15}), and \textsc{Llama-3 70B-Instruct} (Llama-3, \citealp{grattafiori2024llama3herdmodels}).% to represent three popular LLMs, i.e., ChatGPT, Gemini, and Meta AI, respectively.

\paragraph{Default Prompt Setting} All prompts contain a system prompt, an input essay, and an assessment instruction. There are four default conditions. \textbf{(1)} The system prompt contains not only essential background information, such as writing topic, but also helpful information regarding the L2 nature of the input essay, year of writing, the same general assessment guidance used by human assessors. \textbf{(2)} The input essay always includes references. \textbf{(3)} LLMs are instructed to produce a score before an optional comment for each assessment question \textbf{(4)} via greedy decoding, i.e., with temperature set to 0. Conditions 1-3 are used to maximize the alignment between human and LLM assessment conditions.


\paragraph{Interaction Modes} We consider three possible user-LLM interaction modes, depending on how the 9 assessment questions are presented. In Interaction Mode 1 (IM 1), all questions are prompted at once in a single-turn conversation, where all LLM assessments are generated in a single response. In Interaction Mode 2 (IM 2), the questions are asked one at a time, with an LLM generating answers to each question in corresponding turns in a multi-turn conversation. In Interaction Mode 3 (IM 3), however, the assessment questions are provided independently of one another in 9 separate prompts to elicit 9 separate outputs from an LLM.




\subsection{Baselines}

Given the open-ended nature of the task, we compare raw assessments produced across individual assessors to understand the assessment patterns and behaviors of humans and LLMs. For a more robust statistical analysis, we only consider raw assessments made by assessors B, C, and F, since the essays they each assessed and co-assessed both cover at least half of the corpus. 



\subsection{Evaluation of Scores\label{sec:evalOfScores}} 




\paragraph{Quadratic Weighted Kappa (QWK)} This is a metric for rating inter-rater agreement. It ranges from 0 (random agreement) to 1 (perfect agreement), though it can be negative when agreement is worse than chance. QWK places higher penalties for larger score mismatches, but can yield misleadingly high or low values due to chance correction when the distribution of scores is highly skewed \citep{yannakoudakis-cummins-2015-evaluating}. 


\paragraph{Adjacent Agreement Rate (AAR)} AAR measures the percentage of scores (from two raters) that lie within a specified threshold $k$ of one another. When $k=0$, it assesses exact matches. For this study, we set $k=1$ (AAR1), meaning raters' scores are treated as matching as long as they differ by no greater than 1.

We use AAR1 in addition to QWK to account for the limitation of QWK's chance correction, as we observe that both human- and LLM-assigned scores are highly biased toward the respective means. AAR1 also helps address observed scoring inconsistency issues (often by 1 point) by humans. See Appendix~\ref{app:scores} for more details and discussions.




\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/overall_agreement_heatmaps_masked.png}
    \caption{Heatmaps of overall QWK (bottom, green) and AAR1 (top, blue) among assessors. Darker shades indicate a higher degree of agreement.}    \label{fig:overall_agreement_heatmap}
\end{figure}


\subsection{Results}

We compare human- and LLM-generated assessments in terms of scores, comments, and the interaction between scores and comments.

\subsubsection{Scores\label{sec:scores}}

Figure~\ref{fig:overall_agreement_heatmap} illustrates the overall scoring agreement between all pairs of assessors. 

\paragraph{Humans score more like humans and LLMs score more like LLMs.} More concretely, human-human QWK and AAR1 are almost always higher than the corresponding human-LLM agreement. Similarly, LLM-LLM agreement exceeds human-LLM agreement in virtually all cases, with a much larger margin, suggesting that LLMs may resemble each other in scoring more closely than humans resemble each other. Criterion-level agreement between human/LLM assessors shows similar patterns, as shown in Fig.~\ref{fig:criterion-level_agreement_heatmap}.


\paragraph{LLMs can score approximately like humans.} The best human-LLM AAR1 for the three LLMs ranges from 0.59 to 0.88, with all LLMs achieving an AAR1 above 0.5 with assessor F (Fig.~\ref{fig:overall_agreement_heatmap}). Moreover, the AAR1 scores between GPT-4o and assessor B and between Llama-3 and assessors B and C are always greater than 0.5. \textit{Overall, it shows that LLMs can generate sensible or reasonably good scores, often differing by no more than 1 point from the corresponding human-generated scores.}


\paragraph{Human-LLM agreement tends to be higher when LLMs respond to each assessment criterion separately under IM 3.} This is particularly true compared to when LLMs respond to all criteria at once under IM 1, since IM 3 exhibits a generally higher agreement level (Fig.~\ref{fig:overall_agreement_heatmap}). This result may imply that, while human assessors score the 9 assessment criteria sequentially, they effectively make independent scoring decisions based on the specifics of each assessment question. 

That said, the effect of interaction modes is overall limited, given the fairly close scores (i.e., high QWK/AAR1) assigned across them for each LLM. Therefore, we average human-LLM agreement for each LLM across the three interaction modes to obtain human-LLM agreement in Fig.~\ref{fig:criterion-level_agreement_heatmap}.


\paragraph{The degree of human-LLM agreement varies across assessment criteria.} For example, Fig.~\ref{fig:criterion-level_agreement_heatmap} shows that LLM-assigned scores are relatively closer to human-assigned scores on assessment criteria C1 (material selection), C2 (material integration and citation), C8 (grammar and sentence structure), and C9 (academic vocabulary) than the other criteria. Among criteria C3-C7, LLMs and humans agree rather poorly on C7 (use of connectors), with LLMs consistently assigning scores more than 1 point away from human-assigned ones.



\begin{figure}
    \centering
    \small
    \includegraphics[width=0.95\linewidth]{figures/criterion-level_ARR1_agreement_heatmap.png}
    \caption{Criterion-level AAR1 between average human scores (``Human Avg'') and human or LLM assessors. See Appendix~\ref{app:scores} for full results for QWK and AAR1.}
    \label{fig:criterion-level_agreement_heatmap}
\end{figure}


\input{tables/mainResults/overallCommentStats}

\begin{figure*}
    \centering
    \small
    \includegraphics[width=1\linewidth]{figures/criterion_level_problemChars.png}
    \caption{Percentage of comments identifying a problem that mentions a specific essay part (left), offers a comment (middle), and offers a concrete correction (right) across assessment criteria by different assessors.}
    \label{fig:criterion_level_problemChars}
\end{figure*}


\subsubsection{Comments}

Table~\ref{tab:overallCommentStats} shows the percentage of time an assessor provided a comment, and when they did, the average length of these comments, the percentage of comments identifying a problem, and the average number of problems identified in each comment. 

\paragraph{LLMs always provide comments and identify problems, but humans do not.} This is an apparent advantage of LLMs since, unlike humans, they do not experience practical constraints like mental fatigue and limited time for writing comments. While humans show different tendencies in comment writing, they tend to write more comments and/or identify more problems (with longer comments) on criteria that are technical and objective, including C2, C8, and C9, also mentioned in the end of Section~\ref{sec:scores}. See Appendix~\ref{app:comments} for details.


\paragraph{Interacting with LLMs one question at a time leads to more elaborate, specific, and helpful comments.} LLM comments are much longer and identify more problems in IM 2 and IM 3 than in IM 1 (see Table~\ref{tab:overallCommentStats}). Additionally, Fig.~\ref{fig:criterion_level_problemChars} shows that comments generated in IM 1 are also less likely to refer to a specific essay part and offer a concrete correction than those generated in IM 2 and IM 3 or human-generated comments. This suggests that IM 2 and IM 3 provide higher levels of elaboration than IM 1. Furthermore, IM 3 produces more corrections than both IM 2 and humans across all assessment criteria, except C1, for which a correction is unlikely since it is about evaluating the relevance of cited references. In other words, \textit{LLMs can be more elaborate, specific, and potentially helpful than humans in their comments.}


\paragraph{LLMs can be  more specific than humans on assessing subjective criteria.} While humans and LLMs (in IM 3) are comparably likely to include a correction in their comments for objective criteria C2, C8, and C9, LLMs' comments (in IM 3) tend to offer more corrections on other subjective criteria (e.g., C3: quality of key components, C4: logic of structure etc.), except for C1 (see above). This aligns with the observation that humans tend to comment more on objective criteria, since commenting on subjective criteria requires more explanations and can thus be more demanding to do. 



\subsubsection{Score-Comment Interaction} 

Since lower scores reflect a perception of more writing problems, an assessor typically needs to provide a more extensive feedback comment to both cover the identified problems and justify their low scores. We highlight this score-comment interaction by measuring the correlations between scores and the token counts of or the numbers of identified problems in the related comments. 

As expected, the last column in Table~\ref{tab:overallCommentStats} shows strongly negative score-comment correlations across both human- and LLM-generated assessments. The fact that these negative correlations are generally much stronger when measured with the number of identified problems suggests that it is a more fine-grained metric than comment length and also indicates the usefulness of our framework proposed in Section~\ref{sec:evalOfComments}. 
 


\subsection{Summary}

We show that LLMs can generate sensible scores, typically within 1 point of human-generated ones on a 10-point scale, and feedback comments that identify more writing problems than human assessors that are specific, and potentially helpful. This is particularly true when LLMs are prompted in IM 3 where each assessment question is asked independently of each other. Moreover, like humans, LLMs also generate assessments that exhibit an expected and negative score-comment correlation, justifying the validity of their assessments. \textit{Overall, these results highlight that LLMs can generate reasonably good multi-dimensional analytic assessments.} 

% , highly useful for pedagogical purposes for L2 writing