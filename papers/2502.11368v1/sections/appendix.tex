\appendix

\input{tables/appendices/basicCorpusStats}


\section{Corpus\label{app:corpus}}

\subsection{Basic Corpus Statistics}

Table~\ref{tab:basicStatistics} provides the basic statistics of the corpus. Note that throughout this study, we use the default word tokenizer of NLTK to compute word counts. See: \url{https://www.nltk.org/api/nltk.tokenize.html}.


\subsection{Details of the 5-Unit Tutorial Series}

Table~\ref{tab:details-of-the-5-unit-tutorial-series} presents details of the 5-unit tutorial series, including the themes, notions, activities, duration, and writing task for each unit. 

To support their writing, the authors were provided with a short, curated bibliography for each task, designed to help them focus on literature review writing while minimizing the effort required for bibliographic searches. Prior to submitting their final writing samples for expert assessments, the authors engaged in peer reviews (for topics 1, 3, and 5) or group collaboration (for topics 2 and 4). 

% Although there was no second round of feedback from human experts for each task, authors were encouraged to apply insights from previous feedback to subsequent writing tasks.

% % These authors were likely to be highly motivated L2 writers, given the fact that over 280 graduate students signed up for participation, but only 31 completed all the writing tasks, possibly due to the COVID-19 pandemic as well as the non-credit-bearing nature of the tutorial series. The participants were only compensated with a 100-Canadian-dollar git card for completing the entire tutorial series in 12 weeks, which was unlikely to be a main factor of motivation, since 20 out of 51 (39.2\%) authors quit the project even after they had submitted at least one writing samples. 


\subsection{Assessment Criteria\label{app:criteria}}

The 9 assessment criteria/questions provided to human assessors are detailed in Table~\ref{tab:assessmentCriteria}.


\input{tables/appendices/details-of-the-tutorial-series}


\input{tables/appendices/assessment-criteria}



\section{Feedback Comment Quality Evaluation Framework \label{app:framework}}

\subsection{Implementation}

The framework is implemented using LLMs. More concretely, we used \textsc{gpt-4o-2024-11-20} for Problem Extraction and Problem Classification, and \textsc{gpt-4-turbo-2024-04-09} for Correction Relevance Check. An example implementation of our framework can be found in Table~\ref{tab:frameworkImpExample}.

Related prompts used for implementing our framework can be found in Appendix~\ref{app:promptFramework}.


\input{tables/appendices/frameworkImpExample}




\subsection{Annotation}

\paragraph{Guidelines} Table~\ref{tab:problemCharacterizations} provides explanations and examples of what is considered as a problem for Problem Extraction, and the three characteristics relevant to Problem Classification: whether an extracted problem (1) refers to a specific part of the essay, (2) provides a suggestion (general or specific), and (3) offers a concrete correction. 


\paragraph{Samples for Problem Extraction} We employed stratified sampling to randomly select 100 human-generated feedback comments and 108 LLM-generated feedback comments. In total, there are 208 comments for manual annotations.

For LLM-generated comments, half of them were generated under Interaction Mode 1 and the other half under Interaction Modes 2 and 3. Comments from Interaction Modes 2 and 3 were sampled together to reduce manual annotation effort, as these comments tend to be lengthy. The sampling covered the 9 assessment criteria, with 2 comments from each of the 3 LLMs used, resulting in 9 * 3 * 2 = 54 comments from Interaction Mode 1 and another 54 comments from the combined Interaction Modes 2 and 3.


\paragraph{Samples for Problem Classification} We randomly sampled 100 problems extracted from both human- and LLM-generated comments, resulting in 200 problems for annotations. 

Since the distribution of extracted problems across the nine assessment criteria are highly skewed, we ensured that there were at least 5 problems for each assessment criterion.


\paragraph{Problem Extraction} For each feedback comment, the two annotators were provided with LLM-extracted problems and asked to identify the number of correctly extracted problems (true positives), the number of incorrectly extracted problems (false positives), and the number of problems not extracted (false negatives). The number of true negatives is always set to 0, as there is no negative prediction in problem extraction.

A problem is considered correctly extracted if the LLM output contains the exact or paraphrased problem stated or implied in the feedback comment. It is acceptable if additional information relevant to the problem, such as elaborations, suggestions, clarifying questions, or quoted text from the assessed essay, is not included in the LLM-identified problems, which appears to be uncommon based on our annotations. However, if the problem and relevant additional information are extracted as separate problems, only the stated or implied problem is counted as a true positive, and the relevant information is treated as a false positive. This over-segmentation is the primary source of errors in LLM-extracted problems.


\paragraph{Problem Classification} For each extracted problem, the two annotators were asked to answer the three classification problems based on Table~\ref{tab:problemCharacterizations}. 


\input{tables/appendices/problemCharacterizations}



\subsection{Correction Relevance Check\label{app:relevanceCheck}}

Table~\ref{tab:overallRelevanceCheckResults} demonstrates that comments generated by both humans and LLMs are overall highly relevant. However, human-generated comments tend to exhibit slightly lower relevance—either broadly or strictly—compared to those generated by LLMs. 

We conducted a small-scale error analysis to investigate the reasons behind the 8\%, 15\%, and 9\% of human-identified problems that GPT-4 incorrectly classified as not present in the essays, not adhering to the assessment criteria, and being incorrect, respectively. 

\paragraph{Problems not Present in Essays} We randomly selected 10 problems identified by GPT-4 as not present in the assessed essays. Upon reviewing each human-identified problem in the original essay, we found that 6 of these problems were indeed present, while 4 were not. Of the 4 problems that did not exist in the essays, 3 appeared to be misassigned comments (2 of these 3 were extracted from the same comment), while the remaining one seemed to be an assessor error. Among the 6 problems that GPT-4 misclassified, 4 were due to GPT-4 misunderstanding the identified problems, 1 was due to GPT-4 failing to locate a quoted word in the essay, and 1 was because GPT-4 mistakenly deemed the identified problem not to be a problem, despite its presence in the essay.

\paragraph{Problems not Adherent to the Assessment Criteria} We randomly selected 10 problems identified by GPT-4 as not adhering to the assessment criteria. Of these, 9 were related to C8 (grammar \& sentence structure), and 1 was related to C9 (academic vocabulary). Our manual validation showed that 7 of the problems were less related to grammar and sentence structure but more related to word choice or clarity of expression. The remaining 3 were misclassified by GPT-4, mostly due to its requirement that problems be explicitly related to both grammar and sentence structure in order to adhere to C8.

\paragraph{Correction being Incorrect} We randomly selected 10 problems containing corrections identified by GPT-4 as incorrect. We found that 5 of these problems involved accurate corrections, all related to grammar. There were 2 corrections proposed to be suggestions and 3 corrections that require subjective judgments to determine their correctness. 

\paragraph{Remarks} Based on this error analysis, we can attributed the discrepancy in relevance to two primary reasons: (1) human comments often include (inconsistent use of) diacritics that complicate problem extraction and characterization, and (2) human assessors may occasionally deviate from instructions, providing corrections unrelated to the assessment question. These issues are less frequent in LLM-generated comments, which benefit from their strong adherence to instructions and the ability to handle extended context windows. That said, both human- and LLM-identified problems are highly relevant.


\input{tables/appendices/overallRelevanceCheckResults}






\section{Results\label{app:results}}


\subsection{Scores\label{app:scores}}

\paragraph{Scoring Ranges} Table~\ref{tab:descStats} summarizes the scoring ranges, in the form of means and standard deviations for each assessment criterion, as produced by three human assessors and the three LLMs under three interaction modes. 


\paragraph{Full QWK/AAR1} Table~\ref{tab:QWK_full} presents the full results for Quadratic Weighted Kappa (QWK) and Table~\ref{tab:AAR1_full} presents the full results for AAR1. 

\paragraph{Inconsistencies in Scoring by Human Assessors} First, there is an instance in the corpus, where assessor B accidentally assessed the same essay twice on separate days.\footnote{Four days apart and assessor B had no access to their earlier assessments.} While assessor B provided identical scores for 5 out of the 9 assessment criteria, discrepancies of 1 point occurred for the remaining 4 criteria, with scores alternating between (8, 7), (8, 7), (4, 5), and (7, 8). 

Second, we observe that human assessors assigned different scores to identical or similar comments, mostly within 1-point differences. For example, assessor F gave the same comment ``Decent number of citations'' three times but assigned three different scores: 6, 7, and 8. Similarly, assessor C assigned scores of 7 and 8 to the comment ``Appropriate use of connectors.'' However, when the same comment is repeated, scores tend to be very close, typically within one point. For instance, assessor A assigned a score of 8 to the comment ``Great use of academic words and formal tone'' five times, with only one instance where the score was 9.



\input{tables/appendices/descriptiveStats}
\input{tables/appendices/QWK_full}
\input{tables/appendices/AAR1_full}


\subsection{Comments\label{app:comments}}
 
\input{tables/appendices/generalStatsFeedbackComments}

Table~\ref{tab:generalStatsFeedbackComments} presents the general statistics of feedback comments generated by human assessors and LLMs under the three interaction modes.



\subsection{Score-Comment Interaction}

Fig.~\ref{fig:score-comment-interaction} provides the full results of the correlations measured between scores and the token counts of or the numbers of identified problems in the related comments.


\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures/score-comment-interaction.png}
    \caption{Heatmaps showing score-comment correlations between scores and the length of the related comments (left) and between scores and the number of problems identified in the related comments (right). Darker blue shades indicate a stronger negative correlation and darker orange shades a stronger positive correlation, with gray-ish colors indicating negligible correlations. To ensure meaningful analysis, correlations are calculated only when at least 10 score-comment pairs are available. C1: Material selection. C2: Material integration and citation; C3: Quality of key components. C4: Logic of structure. C5: Content and clarity of ideas. C6: Coherence (flow of ideas). C7: Cohesion (use of connectors). C8: Grammar and sentence structure. C9: Academic vocabulary.}
    \label{fig:score-comment-interaction}
\end{figure*}


\section{Further Analyses\label{app:furtherAnalyses}}

Table~\ref{tab:commentPairs} provides five random example comment pairs sampled from GPT-4o-Aug and GPT-4o-May prompted under default prompt setting specified in Section~\ref{sec:prompting}. We find that when BERTScore is low (the last row), the comment pair is less similar compared to other pairs. While other two metrics (BLEU and ROUGE-L) are highly correlated with BERTScore (BLUE: 0.78, ROUGE-L: 0.88, Pearson), they consistently yield lower values than BERTScore. This indicates that these two lexical overlap-based metrics may be less effective at measuring comment reliability compared to the semantic similarity captured by BERTScore.


\input{tables/appendices/commentPairs}




\section{Prompts\label{app:prompts}}

Note that, any word followed by a dollar sign ``\$'' is a placeholder for all prompt templates included in this section. For example, ``\$comment'' is a placeholder for a comment. 


\subsection{Prompts for the Feedback Comment Quality Evaluation Framework Pipeline\label{app:promptFramework}}


The full prompt templates for the three steps in the pipeline of the feedback comment quality evaluation framework are given below. Among these three prompts, the prompt for Problem Extraction contains three in-context exemplars, whereas the prompts for the other two steps are zero-shot prompts. 

\subsubsection{Prompt for Problem Extraction}

\begin{quote}

\footnotesize

You will be given a feedback comment written for a student's essay. Your task is to identify and extract all the writing-related problems mentioned or implied in the comment, along with any explanations, suggestions, corrections, questions, quotations, or other relevant information provided in the comment for each extracted problem. \newline

A writing-related problem is any issue that affects the quality of the writing, such as citation errors,
logical flaws, coherence issues, grammatical mistakes, or inappropriate word choices, among others. \newline

\#\#\# Extraction Instructions\newline

- Each extracted problem must be clear and can be understood without the need to refer to the original comment. \newline

- Each extracted problem must faithfully reflect the provided comment by including any relevant information. Relevant information includes a further explanation or an elaboration of the problem, a suggestion for improvement, a concrete correction, a clarifying question, an excerpt (possibly without quotation marks) from the student's essay, or any other relevant information that helps to understand the problem. \newline

- Whenever possible, extract each problem and the relevant information as they are written in the comment. \newline

\#\#\# Output Instructions\newline

- Output each extracted problem along with their relevant information line by line headed by ``-''.
- Output ``None'' if no writing-related problems are mentioned or implied in the comment.\newline

\#\#\# Examples\newline

Example 1 input:\newline

The content is generally informative and relevant, but the clarity of ideas could be improved. Some sentences are overly complex and could be simplified for better understanding. For instance, the sentence ``Gandhi's Satyagraha as an adequate substitute for violent methods of conducting social conflict in an early and thorough philosophical examination of Gandhi's attitude to violence in extreme group conflict'' is difficult to parse and could be rephrased for clarity.\newline

Example 1 output: \newline

- The clarity of ideas could be improved. Some sentences are overly complex and could be simplified for better understanding. For instance, the sentence ``Gandhi's Satyagraha as an adequate substitute for violent methods of conducting social conflict in an early and thorough philosophical examination of Gandhi's attitude to violence in extreme group conflict'' is difficult to parse and could be rephrased for clarity.\newline

Example 2 input:\newline

The content and clarity of ideas are generally good, but there are some areas where the author could provide more depth or analysis. For example, the author could have explored the potential reasons why students in India may be more vulnerable to substance abuse, or discussed the implications of legalization for public health policy. To improve, the author could revisit the body of the literature review and provide more nuanced analysis of the findings.\newline

Example 2 output:\newline

- There are some areas where the author could provide more depth or analysis. For example, the author could have explored the potential reasons why students in India may be more vulnerable to substance abuse, or discussed the implications of legalization for public health policy. To improve, the author could revisit the body of the literature review and provide more nuanced analysis of the findings.\newline

Example 3 input:\newline

The author has generally done a good job of integrating source materials and presenting information clearly. However, there are some instances where the connections between ideas could be more explicitly stated, and the citation practices could be more consistent (e.g., some sources are cited with author names, while others are cited with only the year).\newline

Example 3 output:\newline

- There are some instances where the connections between ideas could be more explicitly stated.\newline
- The citation practices could be more consistent (e.g., some sources are cited with author names, while others are cited with only the year).\newline

\#\#\# Input\newline

\$comment\newline

\#\#\# Output

\end{quote}


\subsubsection{Prompt for Problem Classification}


\begin{quote}
\footnotesize

You will be given an excerpt of a feedback comment written for a student's essay. Your task is to answer the following questions: \newline

1. Does the excerpt refer to a specific part of the essay? A specific part refers to a part of the essay that can be easily located by the student.
For example, it can be a specific word, phrase, sentence, paragraph, reference etc. used in the essay. It can be a concrete location, such as ``sentence 2 in paragraph 2,'' ``in paragraph 6,'' ``the first citation,'' or ``the first sentence of the paper'' and so on. A less concrete location, such as ``the introduction,'' or ``the conclusion,'' is also considered a specific part if it is accompanied by some referenceable details, such as ``The significance of South Australian policy is unclear, as it is the first citation and the only one in the Introduction.'' Note that the excerpt may only contain a quoted text from the essay, in which case, the quoted text is considered a specific part.\newline

2. Does the excerpt offer some form of suggestions, general or specific, for the student to improve the essay? If the excerpt only describes a problem and it is unclear what the student should do to fix it, then there is no suggestion. If the excerpt provides a concrete correction, it is considered a suggestion.\newline

3. Does the excerpt provide a concrete correction for the student to apply? Note that when the excerpt only contains a quoted text from the essay and there are some notes indicating a correction (e.g., adding/removing a punctuation, correcting a spelling), this is considered a correction.\newline

Answer each question with ``Yes'' or ``No'' based on the content of the excerpt and briefly justify your answer. After answering all the questions, 
produce your final answers in a newline separated by commas.

Excerpt: \$excerpt

\end{quote}


\subsubsection{Prompt for Correction Relevancy Check}


\begin{quote}

\footnotesize

You will be given an excerpt of a feedback comment written for a student's essay according to an assessment question. Your task is to answer the following questions: \newline

1. Does the problem pointed out in the excerpt exist in the corresponding essay? If the excerpt uses a quoted text to point out a problem, check if the quoted text is present in the essay. Please note that the quoted text may not be an exact match either due to misspellings, capitalization errors etc., or because the quoted already contains the correction in place. \newline

2. Is the problem pointed out in the excerpt relevant to the corresponding assessment question? Check if the excerpt is broadly related to any aspect of the assessment question. \newline

3. Is the correction of the problem pointed out in the excerpt correct? If the problem does exist in the essay, check if the correction fixes the problem or presents a plausible solution or improvement. \newline

Here is the essay: \newline

\$essay \newline

Here is the assessment question:\newline

\$question \newline

Here is the excerpt:\newline

\$excerpt\newline

Answer each question with ``Yes'' or ``No'' utilizing all the information provided and briefly justify your answer. After answering all the questions, produce your final answers in a newline separated by commas.
    
\end{quote}




\subsection{Prompts for the Main Experiments\label{app:mainPrompts}}


Our prompts consist of three parts: (1) a system prompt part that provides general background information and specifies the writing topic and some general assessment guidance; (2) a writing part that includes an entire literature review (with references); (3) an assessment instruction part, where one or multiple assessment questions (see Table~\ref{tab:assessmentCriteria}) are asked in various manners according to the interaction modes.  

We keep the system prompt fixed across the three interaction modes. For the main experiments, the system prompt is as follows:

\begin{quote}
\footnotesize

You are an expert academic writing instructor specializing in graduate-level work, with particular experience supporting students who speak English as an additional language. You have been asked to evaluate a literature review submitted by a graduate student on the following topic: \$Topic. The review was written in 2021, so references after this year are not expected.

When assessing the student's writing, please strictly follow the instruction provided to you and make sure your score/feedback is carefully considered and constructive. Please provide your comments and/or suggestions with as much detail and specificity as possible. Please provide specific examples of sentences, paragraphs or sections that you think could use improvement. If you write comments, please start them with something positive. Please proceed with things that could be improved, would make things clearer for the reader, would make the text flow better, etc.

\end{quote}


For the writing part, we explicitly mark the beginning and end of the writing for clarity: 

\begin{quote}
\footnotesize

\#\#\#\#\#\#\#\#\#\# Writing starts \#\#\#\#\#\#\#\#\#\#

\$writing

\#\#\#\#\#\#\#\#\#\# Writing ends \#\#\#\#\#\#\#\#\#\#
\end{quote}

The specifics of how the assessment instruction part is constructed are detailed below. 



\subsubsection{Interaction Mode 1}

In Interaction Mode 1, all assessment questions (see Table\ref{tab:assessmentCriteria}) are asked at once:

\begin{quote}
\footnotesize

Q1: \{Assessment question 1\}

Q2: \{Assessment question 2\}

...

Q9: \{Assessment question 9\}

\end{quote}


After these assessment questions is an answer instruction: 

\begin{quote}
\footnotesize

For each of the 9 questions above, provide your comments or suggestions if any, followed by your score out of 10. Please indicate which question you are providing feedback for by starting your response with `A1:', `A2:', etc. Each response should use the following format:

Score: ...

Comments or suggestions: ...

\end{quote}


Note that we use ``if any'' to denote the optionality of the comments and suggestions. We tried putting ``(Optional)'' after ``Comments or suggestions,'' but that does not make a difference. 



\subsubsection{Interaction Mode 2}

In Interaction Mode 2, the assessment questions are presented sequentially and one at a time. Below is the basic structure:

\begin{quote}
\footnotesize

Q$_i$: \{The $i$th assessment question.\}

\{Answer instruction\}

A$_i$:
    
\end{quote}


The answer instruction resembles the one used in the Interaction Mode 1. 

\begin{quote}
\footnotesize

Provide your score out of 10, followed by comments or suggestions if any. Your response should use the following format:

Score: ...

Comments or suggestions: ...
    
\end{quote}


Note that, we append LLM's response to the $i$th assessment question to the original prompt to form a new prompt, to which the next assessment question is added. This way, the writing is only provided once (at the beginning), but the LLM will have access to previous assessment questions as well as its answers to those questions.


\subsubsection{Interaction Mode 3}

In Interaction Mode 3, each assessment question is asked independently, so there are 9 separate prompts for each essay. 

The structure for the assessment part of the prompt is similar to that in Interaction Mode 2, but without indexation and prefix ``Q/A'':

\begin{quote}
\footnotesize

\{An assessment question.\}

\{Answer instruction\}
    
\end{quote}

The answer instruction works exactly the same as in Interaction Mode 2. 


\subsection{Prompts for the Follow-Up Experiments\label{app:followUpPrompts}}

\subsubsection{System Prompt Simplification}

Below is a simplified system prompt removing the helpful information from the default system prompt used in Section~\ref{sec:experiments}.

\begin{quote}
    \footnotesize
You are an expert academic writing instructor for graduate students. You have been asked to evaluate a literature review submitted by a student below. The writing is broadly related to the following topic: \$Topic. 

When assessing the student's writing, please strictly follow the instruction provided to you and make sure your score/feedback is carefully considered and constructive.
\end{quote}


\subsection{Prompts for Assessing Specificity and Helpfulness}


\begin{quote}
    \footnotesize

You will be given a feedback comment written for a student’s essay according to an assessment question. Your task is to rate the feedback comment on (1) specificity and (2) helpfulness, using a scale from 1 to 10, where 1 is the lowest and 10 is the highest. Conclude your response with the final ratings in this format: "Specificity: X, Helpfulness: X" (where X is a score from 1 to 10).

Here is the essay:

\$essay

Here is the assessment question:

\$question

Here is the feedback comment:

\$feedback

Please rate the specificity and helpfulness of the feedback comment.

\end{quote}