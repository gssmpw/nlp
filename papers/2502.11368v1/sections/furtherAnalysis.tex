\section{Further Analyses\label{sec:furtherAnalyses}} 


\input{tables/specificityHelpfulnessCorr}


\subsection{Re-examining Our Assumption about Feedback Comment Quality} 

Our proposed feedback comment quality evaluation framework assumes that the quality of a feedback comment is related to how well it identifies relevant writing problems of an assessed essay. The framework extracts and characterizes problems of assessed essays identified in comments to evaluate the specificity and helpfulness of these comments.


To assess this assumption, we adopt an LLM-as-a-judge approach \cite{zheng2023judgingllmasajudge}, prompting \textsc{openai-o1-mini-2024-09-12} (o1-mini, \citealp{openai2024openaio1card}) to directly assess the specificity and helpfulness of a feedback comment, given the corresponding essay and assessment question on a 10-point scale. We do not define specificity and helpfulness to avoid injecting biases and choose all comments, generated by humans and LLMs, from one subjective criterion (C6: coherence or flow of ideas) and one objective criterion (C9: academic vocabulary) to balance our examination. We then calculate the corrections between these two scores produced by o1-mini and the number of different types of problems identified by our framework. 

The results in Table~\ref{tab:specificityHelpfulnessCorr} shows that the characteristics extractable from applying the framework correlate very well with the o1-mini-assigned specificity and helpfulness scores. In particular, the number of problems that mention specific essay parts and offer corrections appears to be overall stronger signals of specificity and helpfulness than the mere number of problems, which shows negligible correlations for comments from IM 1 or IM 2. This shows the potential of our framework in providing a more fine-grained and interpretable measurement of specificity and helpfulness levels of comments.


\input{tables/mainResults/reliabilityResults}


\subsection{Reliability of LLM-generated Assessments} 

We evaluate the reliability of LLM-generated assessments across different realistic conditions that mirror potential real-world use cases. To prevent experimental confounding, we change only one condition at a time for a given LLM in a specific interaction mode, assuming that users tend to interact with their chosen LLM in a consistent manner.

First, we consider \textsc{gpt-4o-2024-08-06} (GPT-4o-Aug) in IM 1 with the default prompt setting from Section~\ref{sec:prompting} as the baseline. To test the effect of model variant, we run the same experiment but with \textsc{gpt-4o-2024-05-13} (GPT-4o-May). We also prompt GPT-4o-Aug while varying one of the four conditions in the default prompt setting (see Section~\ref{sec:prompting}) by (1) removing the helpful information from the system prompt, (2) excluding references in the input essays, (3) instructing LLMs to produce a comment before a score, or (4) setting temperature to 1 to increase output randomness. 

To ensure the comprehensiveness of our experiments, we prompt GPT-4o-May in IM 2 and IM 3 under default prompt setting to study the effect of model variant under other interaction modes. We also prompt Llama-3 in IM 1 changing the first three conditions in the default prompt setting mentioned in the last paragraph. The baselines here are GPT-4o-Aug and Llama-3 prompted under respective interaction modes from Section~\ref{sec:prompting}.

We use QWK and AAR1 and three widely adopted machine translation metrics, i.e., BLEU \citep{papineni2002bleu}, ROUGE-L \citep{lin-2004-rouge}, and BERTScore \citep{bert-score}, to evaluate the reliability of the generated scores and comments between contrastive condition pairs, respectively. 

The results in Table~\ref{tab:reliabilityResults} show that LLMs are capable of generating highly stable scores, with an AAR1 score at least 0.81 and mostly above 0.9 across all conditions. Their generated comments are also decently similar with BERTScore typically no lower than 0.67. A small-scale manual check and a correlation analysis performed in Appendix~\ref{app:furtherAnalyses} further verify the validity of BERTScore.







