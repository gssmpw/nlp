% \section{Evaluation\label{sec:evaluation}}


% LLM-generated scores and comments are evaluated separately employing the following methods.


% \subsection{Evaluation of Scores\label{sec:evalOfScores}} \jack{To revise.}


% We consider two metrics to evaluate the quality of LLM-assigned scores, compared to human-assigned ones: Quadratic Weighted Kappa (QWK) and adjacent agreement rate (AAR). 

% \paragraph{QWK.} This is a widely adopted metric for scoring performance \citep{Ke2019, Xu2024}. The resulting score $\kappa$ typically ranges from 0 to 1, where 0 indicates no agreement, and 1 indicates perfect agreement. A negative $\kappa$ indicates less agreement than would be expected by chance.

% \paragraph{AAR.} This metric measures the percentage of time two arrays of scores fall within a threshold $k$, point wise \citep{yannakoudakis-cummins-2015-evaluating}. When $k=0$, AAR calculates the exact match rate between the two score arrays. For this study, we set $k=1$ for AAR (AAR1), which is the minimum positive difference for two ordinal scores.  

% % \footnote{We also experimented with other values for $k$, such as 0, 0.5 (averaged versus raw scores), and 2, and found similar patterns of agreement between human and LLM scores.} 



\begin{figure*}[ht]
    \centering
    \small
    \includegraphics[width=1\linewidth]{figures/novelCommentEvalFramework.png}
    \caption{\textit{Left}: Pipeline of the proposed feedback comment quality evaluation framework. The input and output for each step of the pipeline are illustrated using a human-generated comment on the use of academic vocabulary, with related tasks performed by an LLM. Answers to the 6  classification questions from the last two steps are highlighted in bold. \textit{Right}: Validation results for the pipeline, where IAA (inter-annotator agreement) and exact match rate are measured between raw annotations by two annotators. See Appendix~\ref{app:framework} for further details.}
    \label{fig:novelCommentEvalFramework}
\end{figure*}


% \subsection{Evaluation of Feedback Comments\label{sec:evalOfComments}}


\section{A Novel Feedback Comment Quality Evaluation Framework\label{sec:evalOfComments}}

A common approach to evaluating feedback comment quality for an
essay uses manual judgments (e.g., rating on a Likert scale), since generating essay-level feedback is an open-ended task. However, this approach is expensive, time-consuming, not scalable, and may not always be reproducible. 

For L2-related feedback comments, common criteria for assessing comment quality include specificity, relevance, helpfulness \citep{han-etal-2024-llm, stahl-etal-2024-exploring, behzad-etal-2024-assessing, behzad-etal-2024-leaf}, and the ability to identify writing problems \citep{stahl-etal-2024-exploring, behzad-etal-2024-assessing, behzad-etal-2024-leaf}. These criteria reflect a common and practical need of L2 learners to be shown specific problems in their essays and how to correct them to improve their writing quality.

To address the issues of manual judgment, we propose an automatic evaluation framework that \textit{evaluates the quality of a feedback comment in terms of its ability to effectively identify relevant writing problems within the assessed essay.} As illustrated in Fig.~\ref{fig:novelCommentEvalFramework} (left), the framework utilizes LLMs to extract problems identified in assessment comments and to characterize their specificity and potential helpfulness. The framework consists of the following three steps.\footnote{See Appendix~\ref{app:framework} for additional details and explanations.} 


\paragraph{Problem Extraction} We start out by extracting any writing problems stated or implied in assessment comments, along with any relevant contextual information for each problem, such as further explanations, suggestions for improvement, concrete corrections, or clarifying questions. We define a problem as any writing-related issue that affects the quality of the writing, such as citation errors, logical flaws, or grammatical mistakes.


\paragraph{Problem Classification} The extracted problems are further characterized along three dimensions: whether an extracted problem \textbf{(1)} points to a specific part of the essay, \textbf{(2)} includes any form of suggestion (general or specific), and \textbf{(3)} provides a concrete correction that can be directly applied to fix an identified problem. These classifications offer a way to assess the \textit{specificity} and \textit{potential helpfulness} of related comments.


\paragraph{Correction Relevance Check} We perform a sanity check to determine whether the proposed correction (and thus the comment) is in fact relevant to the original essay. The Correction Relevance Check also contains three binary classification questions for a more nuanced relevance analysis: \textbf{(1)} does the problem indicated in the correction exist in the essay? \textbf{(2)} is the indicated problem related to the given assessment question? and \textbf{(3)} is the correction correct? 

The results show that both human- and LLM-provided corrections are highly relevant, with answers to those three questions being ``Yes'' typically above 90\% time (see Appendix~\ref{app:relevanceCheck}). We thus focus on the Problem Classification results when comparing human- and LLM-identified problems in the subsequent sections.  

% We consider a correction \textit{broadly relevant} if answers to the first and last questions are both positive and \textit{strictly relevant} if answers to all questions are positive.

\paragraph{Validating the Proposed Framework} The first author and a paid graduate student in Linguistics (native speaker) first annotated some held-out samples for training and developing the annotation guidelines. Each then independently annotated at least another 200 samples containing human- and LLM-generated comments or problems for Problem Extraction and Problem Classification. Afterward, they met to resolve disagreements before the inter-annotator agreement (IAA) was calculated. 

% \owen{You need to say more.  In gneral, exact match is not a valid IAA measure.  You need to provide the details on teh imbalance.  And you need to start a new sentence when you introduce exact match (maybe: "Our datasets are highly imbalanced (details).  ")}

% \owen{Rewrite, "and/or" not acceptable -- say high kappa, and for teh cases of scewed data, the kappa is lower but  the exact match is high}

We measure IAA using Cohen's Kappa. As is known \cite{Feinstein1990}, Cohen's Kappa can provide misleading values with highly imbalanced class distributions. We therefore also provide exact match rates which have not been corrected for random agreement. Fig.~\ref{fig:novelCommentEvalFramework} (right) shows that the IAA is typically high. When the Cohen's Kappa is low due to class imbalance (i.e., problems being incorrectly or not extracted is uncommon or rare and nearly all extracted problems contain a suggestion), the exact match rates are high. LLM task performance, evaluated based on the resolved annotations, is also notably high.

We automatically evaluate LLM performance on the Correction Relevance Check by assuming that human-identified corrections are generally relevant. Specifically, we assess whether the LLM classifies these corrections as mostly \textit{relevant} when presented with their corresponding essays and assessment questions (positive samples), and as mostly \textit{irrelevant} when paired with random essays and questions (negative samples). As shown in Fig.~\ref{fig:novelCommentEvalFramework} (right), our results confirm this expectation.

