% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{booktabs}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage{multirow}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\usepackage{todonotes}




\title{LLMs can Perform Multi-Dimensional Analytic Writing Assessments: \\A Case Study of L2 Graduate-Level Academic English Writing}


\author{
Zhengxiang Wang\textsuperscript{1}\thanks{Zhengxiang Wang was a research assistant at the University of Saskatchewan for the research project that led to the creation of the corpus released in this study.} \hspace{.1cm} 
\textbf{Veronika Makarova}\textsuperscript{2} \hspace{.1cm} \textbf{Zhi Li}\textsuperscript{2} \hspace{.1cm} Jordan Kodner\textsuperscript{1} \hspace{.1cm} Owen Rambow\textsuperscript{1} \\
\textsuperscript{1}Department of Linguistics \& IACS, Stony Brook University, USA \\ \textsuperscript{2}Department of Linguistics, University of Saskatchewan, Canada \\
\href{mailto:zhengxiang.wang@stonybrook.edu}{\texttt{zhengxiang.wang@stonybrook.edu}}
}

\begin{document}
\maketitle
\begin{abstract}

The paper explores the performance of LLMs in the context of multi-dimensional analytic writing assessments, i.e. their ability to provide both scores and comments based on multiple assessment criteria. Using a corpus of literature reviews written by L2 graduate students and assessed by human experts against 9 analytic criteria, we prompt several popular LLMs to perform the same task under various conditions. To evaluate the quality of feedback comments, we apply a novel feedback comment quality evaluation framework. This framework is interpretable, cost-efficient, scalable, and reproducible, compared to existing methods that rely on manual judgments. We find that LLMs can generate reasonably good and generally reliable multi-dimensional analytic assessments. We release our corpus\footnote{\url{https://github.com/jaaack-wang/multi-dimensional-analytic-writing-assessments}.} for reproducibility.




\end{abstract}


\input{sections/introduction}

\input{sections/relatedWork}

\input{sections/corpus}

\input{sections/evaluation}

\input{sections/experiments}

\input{sections/furtherAnalysis}


% \input{sections/followUpExperiments}

% \input{sections/discussion}

\input{sections/conclusion}



\section*{Acknowledgments}

Zhengxiang Wang, Veronika Makarova, and Zhi Li would like to thank Social Sciences and Humanities Research Council of Canada (SSHRC) for funding the writing project (``Collaborative development of written academic genre awareness by international graduate students") under the Insight Development Grants (430-2020-00179). They also appreciate three graduate students, i.e., Leslee G. Mann, Abdelrahman Alqudah, and Hanh Pham who expertly assessed the participants’ submitted writings, and the participants who participated in the project. 

Zhengxiang Wang, Jordan Kodner, and Owen Rambow are grateful for the supports from the Institute for Advanced Computational Science (IACS) at Stony Brook University, in particular the free GPT access it provides. Zhengxiang Wang is supported by IACS's Junior Researcher Award since Fall 2024.

We thank Hannah Stortz for providing manual annotations for our study.



\section*{Limitations}


\paragraph{Generality of Findings} This study focuses on L2 graduate-level academic writing, specifically literature reviews in the humanities and social sciences. While this domain represents a significant subset of academic writing, the findings may not generalize to other genres (e.g., technical reports, creative writing) or proficiency levels (e.g., undergraduate or professional writers). Additionally, our study is limited to English, a high-resource language, which means our results may not be indicative of LLMs’ capabilities in other languages, particularly low-resource ones. Future research should explore the applicability of our findings across diverse writing contexts and linguistic backgrounds.

\paragraph{Weakness of Our Assumption About Feedback Quality} A key limitation of our approach is that it does not account for other factors that may influence the \textit{perceived} quality of a feedback comment, such as politeness (e.g., rude comments may not be well received) or the logical coherence of the argument (e.g., illogical comments could be misleading). However, this concern is less pronounced for LLM-generated feedback comments, as LLMs are trained to align with human preferences and social norms \citep{ouyang2022traininglanguagemodelsfollow}. Moreover, these factors could potentially be incorporated into our framework by adding additional steps focused on politeness and argumentation etc.

\paragraph{Indirect Evaluation of Feedback Quality} While our approach to measuring the general quality of LLM-generated assessments is intuitive and simple, it is inherently indirect. A large-scale manual evaluation remains necessary to more accurately assess and compare the quality of human- and LLM-generated multi-dimensional analytic assessments. Due to resource constraints, we leave this investigation to future studies. 


\paragraph{Limited Validation and Reliability Testing} Due to time and resource constraints, we were unable to comprehensively validate our proposed feedback comment quality evaluation framework. As a result, we may have overlooked some potential issues with the framework or the LLM outputs. Similarly, the reliability assessments we conducted are limited, with only one factor being changed at a time in each evaluation. More extensive experiments are needed to further validate our claim that LLM-generated assessments are generally reliable and to explore the conditions influencing this reliability.


\section*{Ethical Considerations}


\paragraph{Corpus Creation} The research project that led to the construction of the corpus was ethically reviewed and received approval from the University of Saskatchewan for involving human participants. Participants provided informed consent to allow the use of their materials, with the option to withdraw at any time.


\paragraph{Human Annotations} We compensated the hired annotator at a rate of approximately US\$25 per hour, which exceeds the minimum wage in the region where the annotations took place.

\paragraph{Potential Biases in LLM Assessments} LLMs are trained on large-scale datasets that may contain inherent biases, which can be reflected in their assessments. For example, they might systematically favor certain writing styles, linguistic structures, or cultural conventions, leading to biased evaluations. However, we argue that in contexts where human assessments are not readily accessible, the benefits of LLM-generated feedback\,\---\,particularly for L2 learners\,\---\,may outweigh potential biases. Furthermore, bias mitigation strategies, such as improved prompting techniques or advancements in LLM development, could help reduce these concerns.





\bibliography{references}

\input{sections/appendix}


\end{document}
