\begin{figure*}[t]
\includegraphics[width=0.8\linewidth]{Figures/framework.pdf}
  \caption{\yh{The framework of our context-aware LLM modularization technique. The central LLM module generates and maintains central code. It manages individual LLM modules to generate individual class codes. The contextual information is extracted from individual codes and input to the central LLM module for reference. }
  }
  \label{fig:framework}
\end{figure*}

\section{MoGraphGPT System}



% \begin{figure*}[t]
% \includegraphics[width=0.6\linewidth]{Figures/element.png}
%   \caption{\yh{Example elements in our system. Both single entities (e.g., platform, human) and components (e.g., head, legs) can be considered as elements, which are scripted separately in {individual LLM modules and managed by central LLM module.}
%   % to 
%   % % \hbc{there is some gap here? separate scripting and complete effect} 
%   % form a complete effect.}
%   }
%   }
%   \label{fig:element}
% \end{figure*}

According to the design considerations, we first \yh{integrate an element-level} context-aware \emph{modularization} technique (D1, D2) to help generate code for individual elements and interactions for multiple elements {(Section \ref{sec:LLM_modularization})}. We further design and develop a graphical interface, \sysName, combining modular LLMs with graphical control for users to create 2D interactive scenes {(Section \ref{sec:UI})}. It enables direct integration of graphical information (D3) and offers quick, precise control through automatically generated sliders (D4).



\subsection{\yh{Element-level} Context-aware LLM Modularization}\label{sec:LLM_modularization}
2D interactive scenes contain elements in various forms. We define the \emph{element} as {a} %the
general representation of the content within these scenes, \yh{encompassing both individual visual components and broader concepts}. For example, \yh{a layered character animation includes animations for individual body parts as well as a global transformation, meaning both the parts and the entire body are considered {elements}  in our design}. 

% To enhance the independence of code generation for {individual scene} elements, % in 2D interactive scenes, 
\yh{Our element-level}
%\yh{In our element-level} 
context-aware \emph{modularization} technique \yh{(Figure \ref{fig:framework})} %, it approach 
opens modular LLMs for individual elements and {uses} a central LLM module {to manage} %manages
interactions and relationships {among} %between 
elements. \yh{It employs a hierarchical %\hbc{essentially only two levels in our current implementation? Can an element contain a set of child elements?}\yhc{yes, only central-individuals structure} 
structure where the central module oversees coordination, while the individual modules operate independently. This design ensures a clear and cohesive update logic, allowing modifications to a single element without affecting others. When creating interactions, the relevant function code is updated within the element class, while the central module uniformly calls these functions.} {We employ ChatGPT-4o Mini as our LLM {model} %models 
in our implementation.}

\textbf{Individual LLM Modules for Individual Elements.} Each individual element in a 2D interactive scene is associated with its own LLM module. These individual modules are {used}
% designed \hbc{do you design those sessions? used? employed?} 
to %understand, and 
generate and maintain 
% \hbc{generate and maintain?} 
class {codes}
% \hbc{if "code" is considered countable, use it consistently}
%code 
{for} %to 
their respective elements. For example, when creating a Super Mario platform game, the Mario element has its own LLM module (Figure \ref{fig:framework}), which generates a class named Mario for its own properties (e.g., sizes) and behaviors (e.g., using arrow keys to control its movement) from the text input. {To} %If input the text to 
modify Mario's properties and behaviors {with additional text prompts}, it will search the created Mario's module and continue to update there. {This} %These individual
approach allows each element to operate independently, enabling users to customize and enhance each element without disrupting other elements with rapid iteration and testing.



\begin{figure*}[t]
\includegraphics[width=0.95\linewidth]{Figures/workflow.pdf}
  \caption{
  %\hbc{Only Mario class code and spring class code will provide the context? If yes, the bottom part might be a bit confusing since it also involves the central code here.}\cfc{Figure updated.}
  \yh{\sysName~workflow. When users input text prompts for individual elements, our system integrates graphical information into prompts and sends {them} to individual modules to generate class codes {(Top)}. For interactions {(Bottom)}, prompts {with the integrated graphical information} go to the central LLM, which creates the central code. It then notifies individual LLM modules to update their codes with new variables and functions. Changes are reflected in real-time, and the central and individual codes together form the final result.}
  }
  \label{fig:workflow}
  \vspace{-2mm}
\end{figure*}

\begin{figure*}[t]
\includegraphics[width=0.99\linewidth]{Figures/element_types_revision.pdf}
  \caption{
  % \cfc{Figure updated.}
  % \hbc{Change "User Upload Elements" to "User-uploaded Elements". Update the rest correspondingly. }
  \yh{Four ways to create elements in our system. (a) Upload an image. (b) Draw a sketch. (c) Add {a} group % and element image and 
  and let LLM generate {a group of elements (with a user-uploaded element image)}, % element group, 
  either explicitly mentioning ``group'' in text prompt or not. (d) Ask LLM to generate elements.}}
  \label{fig:elemen_type}
\end{figure*}


\textbf{Central LLM Module.}
In contrast to the individual LLM modules, the central LLM module serves as the orchestrator of interactions and relationships {among} %between 
elements (Figure \ref{fig:framework}). It is responsible for instantiating classes from individual modules, coordinating their communication, {and} managing interactions among elements, {thus} ensuring that they work together cohesively within the interactive scene. For example, in the Super Mario platform game  (Figure \ref{fig:workflow}), the central module generates codes for instantiating all the elements and scripting interactions among elements (e.g., when Mario falls on the spring, Mario bounces up and the spring is stretched). Importantly, when generating interaction code, it may involve variables and behaviors specific to individual elements. To prevent interference between elements, we instruct it to 
define the code of variables and functions for each element %in their respective classes 
within their respective classes {(e.g., Mario bouncing code in Mario class, spring stretching code in spring class)} and to call these functions in the central module. This approach allows the central module to directly invoke functions from individual modules while keeping their definitions separate. As a result, modifications to individual elements do not impact the interaction code, maintaining the integrity and functionality of the overall system.




\textbf{Contextual Communication between Modules.}
Independent code generation will lead to a lack of contextual information. To address this issue, %it, 
we design a contextual communication mechanism  (Figure \ref{fig:framework}) between the central module and individual modules. Each time {the} code for {an} individual element is generated, we guide LLM to also provide a summarized overview of the class, including the class name, variables (name, initial value, and short description), and functions (name, argument, return value, and short description). {Please refer to the supplementary materials for more details.} Such information is then compiled into a context information repository. When generating the code from the central module, this {context} %contextual 
is referenced, enabling the central module to maintain an understanding of the overall state of all elements in the scene. It can directly access the variables 
% \hbc{this doesn't sound good. Typically class variables should be used within the classes. To access them outside, we often use get and set methods.} 
and call the functions defined in the element classes. If a user revises any element {class}, %classes, 
both {its} %the 
code and context information will be updated accordingly. Additionally, if the central module modifies or updates variables and behaviors for elements, this will also be reflected in the contextual information. This dynamic updating ensures that the central module remains aware of all changes, promoting a more responsive and flexible operation.

By integrating the strengths of the individual and central LLM modules with contextual communication, our context-aware modularization technique not only enhances independence in code generation but also fosters a more dynamic and interconnected interaction creation. 
% In traditional conversational interactions with ChatGPT, context can be lost over long exchanges, impacting coherence and relevance. In contrast, our approach enhances the independence of individual elements while maintaining contextual awareness for a comprehensive understanding of the system as a whole.
%\hbc{independence and context-awareness are repeatedly emphasized in this paragraph.}\yhc{updated}






\subsection{Graphical Interface}\label{sec:UI}
In our graphical interface \sysName~  (Figure \ref{fig:UI}), context-aware LLM modularization and graphical control are seamlessly integrated to facilitate the creation of 2D interactive scenes using natural language inputs and graphical specifications.
\yh{Our target users are \yh{those with no or limited programming skills,}
% beginners 
%\hbc{what kind of beginners? you mean users with basic programming skills?} \yhc{updated} 
% and non-programmers, 
and our goal is to help them create interactive scenes rather than learning programming. To simplify the user experience and avoid overwhelming newcomers, we do not reveal {the generated} code %representation 
in the UI, as %is 
common in other tools \cite{scratch,python_playground,flutter}. Instead, we focus entirely on prompts and graphical elements, encouraging users to engage with this specialized tool for scene creation rather than transitioning to full programming.



}



\begin{figure*}[t]
\includegraphics[width=0.99\linewidth]{Figures/spatial_revision.pdf}
  \caption{
  % \hbc{Pay attention to the grammar and consistency of the text prompts in the figure.}
  % \cfc{Figure updated.}
  {We allow users to specify four types of graphical inputs: (a) point, (b) line, (c) curve, and (d) region. {Users can refer to their names in the text prompts}. %They are automatically checked and integrated into the text prompts to generate results.
  }} 
  \vspace{-2mm}
  \label{fig:spatial}
\end{figure*}

\begin{figure*}[t]
\includegraphics[width=0.99\linewidth]{Figures/UI-new.pdf}
  \caption{\yh{\sysName~user interface. Element Pane contains the buttons and preview images for all the created elements in the scene. Canvas Area shows all the elements that can be manipulated by users directly. Once users press the ``Generate'' button, the result is generated or updated in the Result Area. Effect Control Pane displays the automatically generated parameter values and sliders for precise control.}}

  \label{fig:UI}
\end{figure*}

\textbf{Element Creation.}
{Users have the flexibility to upload, draw, and request our system to generate elements for them.
% to upload images from their own repositories, draw 2D sketches on canvas, \yh{upload an image and request our system to generate an element group}, 
% or request our system to generate elements for them (Figure \ref{fig:elemen_type}). 
% \hbc{The following descriptions might be too detailed and do not carry important info.} To import an image to create a new element, users can click the ``Upload'' button to select a file from their local device.
Users can press the ``Upload'' button to upload an image element (Figure \ref{fig:elemen_type}(a)) and draw elements with 2D sketches on the canvas area (Figure \ref{fig:elemen_type}(b)). If users have not prepared any images, they can create an empty asset by pressing the ``Add'' button and then input text descriptions to ask the {associated} individual module to generate an element for them (Figure \ref{fig:elemen_type}(d)), such as texts, graphics, and particle effects. Besides single elements, users can create element groups in two ways: 1) the user can press the ``Add Group'' button and upload an element image, and then add a text description to let our system generate a group of elements {with the uploaded image}; 2) %second, 
the user can upload an image element and let our system generate an element group with a proper text prompt including words like ``group'' (Figure \ref{fig:elemen_type}(c)).
}


% \hbc{You mean "Upload" and "Add" are two ways to create elements? This should be clarified. } By pressing the ``Add'' button, an empty element is added to the element pane. Users can rename the element by double-clicking its name.

% For the \hb{added} empty element, users can either draw directly on the canvas to create a sketch or provide \hb{a text description} %text descriptions 
% to prompt GPT to generate \hb{the content of this element}, %corresponding elements, 
% such as graphics or particle effects. Additionally, users can click the ``Add Group'' button to create a group element, %allowing them to upload images for each group member, 
% which is particularly useful for creating common game items like bullets or hearts. 

Once the element is created, it is displayed in the canvas area (Figure \ref{fig:UI}) and rendered in the result area (Figure \ref{fig:UI}). It opens a dedicated ChatGPT session for that element in the left text pane (Figure \ref{fig:UI}). {Our system automatically switches the GPT session to an element after its selection (by pressing its associated button or clicking on it in the canvas)}. %Users can select an element either by pressing its \hb{associated} button or by clicking on it in the canvas. This action automatically switches the GPT session to the selected element. 
The first element in the element pane serves as a central proxy, representing the central session. By clicking on this proxy, users can access the central session in the left pane.

\textbf{Graphical Control.}
Once {an} %the 
element is created, the user %users 
can move, rotate, and scale it on the canvas. These graphical properties are updated in real-time in the generated code, as displayed in the result area. Since describing graphical properties in a natural language can be challenging, we {introduce} a drawing mode allowing %that allows 
users to specify four types of graphical inputs: point, line, curve, and region (Figure \ref{fig:spatial}). They can switch to a certain mode and draw on the canvas. After completing their drawings, each input is labeled with an index, designated as \( \mathit{P}_i \), \( \mathit{L}_i \), \( \mathit{C}_i \), \( \mathit{R}_i \), respectively. Users can then reference these labels explicitly in their text input, facilitating {explicit} %clearer 
communication of their graphical specifications.



\textbf{Text Input.}
% Inheriting the \hb{what} nature of ChatGPT, 
{We} allow users to input any text to describe the interactive scenes. For properties and behaviors of individual elements, users enter the text in the module of each element by selecting element button and press the ``Generate'' button. Then the code for the element is generated and rendered in the result area. For interactions among multiple elements, users input text in the central module by selecting ``Multiple'' button and press the ``Generate'' button to send their request. Since each element has its own ChatGPT session, users do not need to mention the element names explicitly in the individual sessions. Instead, users can use pronouns such as ``it'', ``each of them'', or ``all of them'' to refer to specific elements.

\textbf{Precise Refinement.} After the code for each element is generated, we let LLM to extract the defined variables and their current values. Then the system automatically generates sliders and number input fields in the effect control pane (Figure \ref{fig:UI}), with the range normalized. This allows users to quickly and precisely adjust the parameter values (e.g., movement speed, shake amplitude) without needing to describe the desired changes in text and ask for refinement again. 

{
\textbf{Result Testing.} Users can watch and test the created results in the result area (Figure \ref{fig:UI}) at any time during the creation process. Any change, including text revision, slider revision, element manipulation, % manipulating elements 
in the canvas area will lead to instant updates in the result area.

}
% This approach provides a more direct and intuitive method for users, minimizing the risk of ambiguous interpretations regarding precise parameter values in text prompts. When the codes for the elements are updated, the sliders are adjusted correspondingly to reflect these changes\hbc{I don't understand why the sliders need to be updated: slide manipulation leads to changes in the code but not the other way around?}.


