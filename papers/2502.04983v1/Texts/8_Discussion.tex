\section{Discussions and Future Work}

\subsection{Modularization with LLMs}
We tackled the challenge of interdependent effects on multiple elements within conversational LLMs through our \yh{element-level} modularization technique. While our primary focus is on creating 2D interactive scenes, this issue is prevalent across {many other} %all 
LLM applications. Despite advancements in LLM models, the problem of interdependence is likely to persist for the foreseeable future and is not confined to specific iterations, such as different versions of GPT. Our %innovative 
technique presents a novel and valuable approach applicable to {other} %all
LLM-based frameworks {(e.g., Google Gemini, Claude, LLaMA)}, extending beyond just 2D visual creation to encompass a broader range of interactive scenarios.

\subsection{Trade-off between Modularization and Original ChatGPT}
Our modularization technique encourages users to provide descriptions for individual elements separately. While this promotes clarity and specificity, it can hinder seamless interaction to some extent. Users are required to manually delineate the behaviors of each element, in contrast to the original ChatGPT interface, where they can combine these elements more fluidly, although it often results in undesired and uncontrolled outcomes.

Looking ahead, we plan to develop an advanced input parsing and segmentation method that will allow users to input their descriptions without the need for strict separation. Our system will intelligently analyze and partition the descriptions of both individual and multiple elements, feeding them into distinct modules as necessary. This enhancement aims to strike a balance between user control and the fluidity of interaction, ultimately improving the overall user experience.

\subsection{Limitation of Context-awareness}
Although we provide context for code generation, when tasks become extremely complex and conversations lengthen, the issue of context loss for individual element creation persists. The inherent token limits of LLMs restrict the amount of information that can be processed within a single session. As context accumulates, it contributes to the text prompt, which can lead to truncated or incomplete responses, ultimately hindering the model's ability to generate coherent and contextually relevant outputs.

To address these challenges, future work should focus on developing effective context management strategies that prioritize and summarize essential information while discarding less relevant details. Implementing dynamic context windowing could allow the model to allocate more tokens to critical portions of the conversation while compressing less relevant exchanges. Additionally, exploring chunking mechanisms to break down complex tasks into smaller, manageable parts could help preserve context during longer interactions. By enhancing context-awareness in these ways, we aim to improve the robustness and coherence of LLM outputs, even in complex scenarios.

\yh{
\subsection{Potential Use Cases}
The open-ended study indicated that \sysName~can be used for creating dynamic and interactive scenarios for diverse purposes, such as interactive gaming experiences, engaging educational tools, visual communication in research, and UI design. Beyond these applications, we envision its use in other domains. For example, users can create their own medication reminders, journaling tools, and interactive mood trackers, by creating 2D representations of medicine, feelings, and mood and then crafting interactivity for them. Multiple family members can co-create interactive shared albums by importing photos to \sysName~and designing visualization patterns for memory recording and photo management. In addition, museums and exhibition centers can leverage \sysName~to enable visitors to express their feelings by creating interactive featured items, which can be analyzed for insights into visitor preferences. Furthermore, \sysName~can facilitates interactive storytelling, enabling children to create their own books and share their narratives in a fun and engaging way. 
% \yhc{out of focus?}

\subsection{Integrating Code Representations and Element Visualizations}
Currently, we do not display the generated code in our interface.
However, we plan to gradually introduce code features as users become more comfortable with the basics. This will help them understand the underlying logic of their creations and provide a pathway to more advanced programming concepts. Besides, we are considering the development of educational modules aimed at teaching programming fundamentals. These components will be designed to prepare users for future coding tasks through engaging scene creation.

Incorporating visual chips into the UI design presents a significant opportunity to enhance user interaction. Recent generative AI tools \cite{wang2024promptcharm,li2024evaluating} effectively utilize chips to visually associate text prompts with corresponding visual elements, allowing users to quickly identify and swap components such as subjects and adjectives. While our current UI lists raw text, exploring a chip-based interface in future iterations could greatly improve user experience and accessibility. By visually representing elements like “Mario,” “Platform,” and “P2,” we can make it easier for users to understand and manipulate their inputs.

\subsection{Scalabiltiy for Complex Scenarios}
As more elements are added to a scene, the behavior and interactions among them can become complex. Users may need to view and manage these elements, their relationships, and interaction logic. To support this, we can enhance our system by integrating a graph representation \cite{yan2023xcreation}, where each node represents an element and each edge represents their relationships, with additional connected lines indicating the next event logic. This allows us to visualize both element behaviors and interactions as attributed objects \cite{xia2016object}, after the system generates corresponding codes. Users can then easily reuse these behaviors and interactions by dragging objects to other elements in the graph, as well as directly reconnecting or deleting elements and edges for quick management.

%\hb{While} %For multiple-scene creation, while 
{In our current implementation,} 
\sysName~supports creating results containing multiple scenes (e.g., Figure \ref{fig:result}(d)) by switching in the central module to manage the visibility of created elements. However, more complex applications may contain more scenes. Our current implementation involves two levels: scene and scene elements. We might add one more level for scene management, which opens modules for scenes.
% \hbc{Our current implementation involves two levels: scene and scene elements. We might add one more level for scene management}
Users can manually create scenes and add elements, or we can extract scenes and elements from users' text input. 
We can visualize scenes as workflow UIs, connected by key condition variables, allowing users to manipulate scene order and relationships directly. This will open new opportunities for exploring conditional layered and nested LLMs in various applications.


}

