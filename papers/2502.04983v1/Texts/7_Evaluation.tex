\yh{
\section{Comparative Study}

% \subsection{Comparison between \sysName~and ChatGPT}
% \subsection{\yh{Comparative Study}}
To evaluate the effectiveness of element-level modularization, graphical control, and precise refinement of our system, we designed a \emph{within-subject} study {to compare} %comparing 
\sysName~with a state-of-the-art tool. We selected Cursor \cite{cursor2023} as the baseline since it is a well-recognized AI code editor that allows for partial code modification and simultaneous updates of files or modules. Participants created interactive effects using both \sysName~and Cursor.}

% % \hbc{maybe use the past tense consistently in this section} 
% a comparative study between \sysName~and \yh{a baseline system Cursor \cite{cursor2023}}
% % the original ChatGPT 
% to evaluate the usefulness of modularization, graphical control, and precise refinement. We chose {ChatGPT} %GPT 
% as the baseline system since it is one of the most popularly used LLM interface \cite{masson2024directgpt}. We implemented the baseline system with a graphical user interface that has a similar element pane and a conversational text field to \sysName, and the back-end connected to the OpenAI API with the GPT-4o Mini model. The baseline system only provides the function of uploading element images to the element field, and the conversational field %will 
% allows users to input text prompt and see the response. The generated code will be rendered in the result area once {the response is} received. % the response.
\yh{
\subsection{Baseline Setup}
The Cursor Composer with the GPT-4o Mini Model served as our baseline tool. It supports both full and partial code generation and refinement from text input, applicable to one or multiple files. However, it lacks graphical control features. To ensure a fair comparison, we prepared a basic code template identical to that of the \sysName~system, featuring a central JavaScript file along with separate JavaScript files for individual elements. Participants {could} %had the option to 
select {those} files as context to enhance modular code refinement. To observe the results in real time, we launched a live server that rendered the outputs immediately. Participants were instructed to focus solely on text input, context selection, and result rendering, without visibility into the underlying code.


\subsection{Participants and Apparatus}
}
\yh{We invited 10 participants (aged 22-34, M: 28, SD: 3.77, 6 females and 4 males, U1-U10) from our personal and university network. They include{d} 4 university students and 6 staff. They {had} %have 
diverse backgrounds, including atmospheric environment (U1, U10), fine arts (U4), interaction design (U8), computer science (U2, U5-6), entrepreneurship (U3), chemistry (U7), and business (U9). On a self-rated 5-point scale (1-no to 5-strong) for coding experience, 2  (U4 and U9) of them rated 1, 3 users (U3, U8, U10) rated 2, 1 user (U7) rated 3, 3 users (U1-2, U5) rated 4, and 1 user (U6) rated 5. In a self-rated 5-point scale (1-no to 5-strong) for ChatGPT using experience, 2 users (U9-10) of them rated 2, 3 users (U1, U7-8) rated 3, 3 users (U3-5) rated 4, and 2 users (U2, U6) rated 5. They used ChatGPT for searching information (U1, U4),  polishing writing (U1-3, U6-7, U9-10), checking codes (U2, U6-7), generating codes (U2, U5). The study was conducted on a laptop running both systems, and the participants could use a keyboard, touchpad, and mouse for inputting.}

% We invited 8 participants (aged 24-33, M: 28, SD: 3.25, 6 females and 2 males, U1-U8) from our personal and university network. They include 5 university students and 3 staff. They have diverse backgrounds, including electronic engineering (U1), media arts (U2-3), fine arts (U2), information system (U3), financial mathematics and statistics (U5), accounting (U6), information and technology management (U6), computer science (U4, U7), and atmospheric environment (U8). On a self-rated 5-point scale (1-no to 5-strong) for coding experience, 2  (U3 and U6) of them rated 1, 4 (U1-2, U5, and U8) of them rated 2, and 2 (U4 and U7) of them rated 4. In a self-rated 5-point scale (1-no to 5-strong) for ChatGPT using experience, 5 (U3-6, U8) of them rated 2, and 3 (U1-2, U7) of them rated 4. They mainly used ChatGPT for searching information (U1, U3, U4-7),  polishing writing (U1-2, U5-6, U8), checking codes (U1, U4), generating codes (U1), and generating prompts (U7). The study was conducted on a laptop running both systems, and the participants can use a keyboard, touchpad, and mouse for inputting and drawing.

\begin{figure*}[t]
\includegraphics[width=0.99\linewidth]{Figures/fix.png}
  \caption{Three tasks in the comparative study.} 
  \label{fig:fix}
\end{figure*}

\subsection{Tasks} 
We designed three tasks (Figure \ref{fig:fix}) for users to reproduce the following effects using \sysName~and \yh{Cursor Composer}: (Task1) a fish moves from a specific point to another point; (Task2) a fish moves along a curved path with a constant speed; (Task3) a three-step iterative animation: place the sun and the earth {at specific positions} %left and right 
on the canvas, {then} let the earth {rotate around its own center}, %self-rotate, 
{and finally} let the earth {orbit} %rotate around 
the sun while keeping self-rotation. These three tasks are common in 2D interactive scenes and %since they
involve typical features such as behaviors of single elements and interactions between two elements, spatial properties like positions, translations, rotations, paths, and speed. Users were required to create their results as similar as the given effect example video. In particular, the following features should be similar to the target effects as much as possible: (Task1) the positions of starting point and ending point in the canvas; (Task2) the moving path and the speed; (Task3) the positions of the sun and earth and the rotation speeds {of the earth}.

\subsection{Procedure} 
We gave participants a \yh{15-minute} introduction of the tasks and two systems and allow them to try the systems freely. Then, we showed them both an image and a video for the target effect of {each task}, %three tasks,
and they can further see the image and video during the whole study process. To avoid the learning effects of our system, we asked each participant to first use \sysName~and then \yh{Cursor Composer} to reproduce the target effect for each task. Once the participants considered that their created target effect has been reproduced successfully, it was double-checked by two of our authors. If both 
% \hbc{if here it means two of our authors, it should use "both"} 
of us reach %achieve 
a consensus, it was considered a complete result. He or she can move to the next step. If the participant tried over  \yh{10 minutes} 
% five times 
for %the
similar text prompts {but} %and 
the system still does not provide a clear result, or the participant thinks the effect is very difficult to achieve and he/she does not have any idea for it, it is considered a incomplete result, and it can move to the next step as well. After completing all the tasks, they were asked to fill in the questionnaire on a 5-Likert scale. The questions are {elaborated} %discussed in elaborate 
in Figure \ref{fig:sub-compare}. We then conducted semi-structured interviews with them to collect their feedback, including the differences between \sysName~and \yh{Cursor Composer} and our observations during the study. We recorded the time spent on each task, text prompts, operations, and results. The whole process was audio-recorded and later transcribed with their agreement by filling out an informed consent form. In compensation for their time, each participant received a 13-USD gifted card for about \yh{one-hour} participation.



\subsection{Data Analysis} 
The {questionnaire} includes personal information background questions, subjective ratings (Closeness to target, Graphical control, Precise refinement, Effect independency, Effect consistency, Effect clearness, Easy to specify action, Mental demand for formulating prompts), and selected questions from NASA-TLX (Figure \ref{fig:sub-compare}). Objective metrics consist of time taken, the number of prompts, and prompt word counts. We conducted Wilcoxon signed-rank tests to analyze significant differences.

\begin{table}[]
    \centering
    \caption{The performance
of MoGraphGPT and Cursor across three metrics averaged over 10 participants.
Note that the time, prompt number, and prompt length are averaged across participants for the total of the three tasks.}
    \label{tab:stat_comparison}
    \begin{tabular}{l|c|c|c}
    \hline
                           & Time (s)   & Prompt \textit{N} &  Prompt \textit{L}  \\ \hline
    Ours                     & 402.40      & 4.80          & 27.70         \\ \hline
    \yh{Cursor}                      & 1339.00  & 17.20         & 269.30        \\ \hline
    Reduction & 69.57\%  & 69.34\%       & 89.21\%       \\ \hline
    \end{tabular}
\end{table}

% \begin{wrapfigure}{r}{0.38\textwidth}
%     \vspace{-3.5mm}
%     \centering
%     \footnotesize
%     \begin{tabular}{l|c|c|c}
%     \hline
%                            & Time (s) \footnotemark[4]  & Prompt \textit{N}\footnotemark[4] &  Prompt \textit{L} \footnotemark[4] \\ \hline
%     Ours                     & 402.40      & 4.80          & 27.70         \\ \hline
%     \yh{Cursor}                      & 1339.00  & 17.20         & 269.30        \\ \hline
%     Reduction & 69.57\%  & 69.34\%       & 89.21\%       \\ \hline
%     \end{tabular}
%     \vspace{-3mm}
% \end{wrapfigure}

% \begin{wrapfigure}{r}{0.35\textwidth}
%     \vspace{-3.5mm}
%     \centering
%     \footnotesize
%     \begin{tabular}{l|c|c|c}
%     \hline
%                              & Time (s) & Prompt \textit{N} & Prompt \textit{L} \\ \hline
%     Ours                     & 310      & 4.12          & 21.75         \\ \hline
%     ChatGPT                      & 1079.88  & 14.88         & 293.25        \\ \hline
%     Reduction & 70.36\%  & 70.73\%       & 91.49\%       \\ \hline
%     \end{tabular}
%     \vspace{-2mm}
% \end{wrapfigure}
% \footnotetext[4]{The time, prompt number, and prompt length are averaged across participants for the total of the three tasks.}

\begin{figure*}[t]
\includegraphics[width=0.99\linewidth]{Figures/comparison_with_cursor.pdf}
  \caption{Subjective ratings on \sysName~and \yh{Cursor Composer.} 
  For the scores, the higher, the better.}
  \label{fig:sub-compare}
\end{figure*}

\subsection{Results}
\textbf{Completion.} 
\yh{All participants successfully completed {each} task in our system within 6 minutes.
% \hbc{402 seconds is more than 6 minutes}\cfc{The table records the total of the three tasks.
% I have added a footnote.} minutes.
However, U7 {spent over 10 minutes on Task1 and U4-6 and U8-10 spent over 10 minutes on Tasks} % and 6 participants \hbc{Why to highlight U7 here?}\yhc{U7 spent over 10min for task1, 6 participants spend over 10min for task2.} (U4-6, U8-10) spent over 10 minutes on Tasks 1 and 2 \hbc{you meant when they failed in Task 1 they failed in Task 2 too?} 
using Cursor but were still unhappy with their results. They found Cursor was hard to handle graphical information, such as specific positions and curved paths. Despite attempts to change descriptions or correct responses, Cursor often retained the original results or produced undesirable effects. 
{Table \ref{tab:stat_comparison} compares the performance of \sysName~with Cursor across three metrics averaged over the 10 participants {for the three tasks}: total completion time {(in seconds)}, prompt number, and prompt length (in words) for all the tasks. We also calculated the reduction rates for these metrics by averaging individual improvements of all the participants with \sysName~compared to Cursor. The results indicate \sysName~achieves desired outcomes in significantly less time and with fewer prompts than Cursor.}}

\textbf{Time.} 
\yh{%Cases over 10 minutes are counted as 10 minutes of using Cursor. 
The average time spent on each task using \sysName~is significantly lower than Cursor{, as confirmed by the Wilcoxon signed-rank test (p<0.01)}: Task1: \yh{M: 46.1s (SD: 12.5s) and M: 268.0s (SD: 134.4s) for \sysName~and Cursor, respectively; 
Task2: M: 152.1s (SD: 73.9s) for \sysName~and M: 448.0s (SD: 110.7s) for Cursor; Task3: M: 204.2s (SD: 79.6s) for \sysName~and M: 623.0s (SD: 161.6s) for Cursor.} {Here, we trim the time for cases over 10 minutes to 10 minutes.} 
%\cfc{The time over 10mins is regarded as 10mins cost.}
Our graphical specification feature enables participants to quickly define positions, moving paths, and both absolute and relative positions quickly. Participants (U1, U4, U6-7, U9-10) were able to adjust motion parameters, such as speed, using sliders and numerical inputs with precision. In contrast, participants using Cursor spent considerable time formulating prompts to integrate graphical information. Some (U1, U4-6, U8) struggled with precise descriptions for several minutes, especially when Cursor continued to produce undesired results. Participants (U1-3, U5, U7-9) often had to try multiple word variations to adjust motion parameters.
In Cursor, adding context does not guarantee independent control, often requiring participants to attempt multiple times and impose additional constraints. In \sysName, individual and interaction behaviors can be created in the earth and central sessions just one trial. 
% \hbc{Maybe swap the order of the previous two sentences to make the connection here smoother?} 
This is because our modular structure provides clear division when updating interactions--defining function code for elements within their individual classes and invoking these functions in the central module. In contrast, Cursor’s context offers only soft constraints, causing interaction code and individual behavior code to become intertwined, leading to inconsistent updates and chaotic modifications.}



\textbf{Prompt numbers and length.}
\yh{Our system results in significantly fewer and shorter prompts, as confirmed by Wilcoxon signed-rank tests (p < 0.01). In Task1 and Task2, the participants used pronouns to refer to each element in element sessions and mentioned the created graphical proxies (e.g., P1, C1) in their prompts. In contrast, accurate mention of element names and graphical details is required in Cursor. 

Participants described the relative positions to the canvas (e.g., left-top, right-bottom, U1-3, U5, U7-9), estimated specific coordinates (U4, U6, U10), and refined results  using reference objects and iterations (e.g.,  set it higher to the corner, make them much closer, U1-2, U4-6, U8-9). They adjusted coordinates through multiple iterations and articulated the curved motion path with terms like "curves with two circles" (U2), "waved curves" (U7), and "tilde" (U1, U3, U9). They further specified the shape with phrases such as "make it more curved" (U2) and "let it curve to the left-bottom and then top-right" (U3).

In Task3, even for the second step, % \hbc{why to say "even" for the second step? This step is easier?} \yhc{yes, it just makes earth self rotate}, 
Cursor often failed to achieve target effects on the first trial. Success typically came only after participants added the earth file as context and retried multiple times. Some participants (U1-2, U5, U8-10) directly inputted text for the third step by instructing the earth to orbit the sun, resulting in the earth orbiting but losing its self-rotation effect. This necessitated rewriting the prompt to include this effect, such as ``let the earth rotate around the sun while also rotating around itself.''  In contrast, with \sysName, participants could input the self-rotation instruction in the earth module and the orbiting effect in the central module independently, requiring less prompt engineering effort.
}  

\yh{
\textbf{Subjective ratings and qualitative feedback.}
% particularly in terms of Q6 - Effect Clearness and Q7 - Easy to Specify Actions. We conducted Wilcoxon signed-rank tests separately for the ratings of each term between ours and the baseline and confirmed the significance of all the aspects (p<0.05) except for Q9 (p=0.07)\yhc{updated later}.
% {It is because some participants (U1, U2) expressed that the graphical control feature of our method required extra effort compared to ChatGPT, which only interacts via texts, though {they acknowledged} the feature provided much more precise control. }
% \textbf{Qualitative feedback.}
% Based on the study
% \hbc{do we have multiple user studies?} 
We analyzed the participants' rating and feedback, distilling our findings into four key aspects.

\emph{User Overall Experience.} 
Overall, subjective ratings and feedback indicate that our system offers a more intuitive, easy-to-use, and effective experience 
% \hbc{I'm not sure if it's appropriate to say the programming experience with our tool} 
compared to Cursor.
% , especially for users with varying programming expertise. 
As shown in Figure \ref{fig:sub-compare}, \sysName~significantly outperformed Cursor across all metrics. {Wilcoxon signed-rank tests confirmed the significance of all aspects (p < 0.05). As shown in Figure \ref{fig:sub-compare} (b),} the participants had a strong preference for \sysName, {particularly in terms of {Q2 - Graphical Control, Q3 - Precise Control, Q7 - Easy to Specify Actions, Q8 - Low Mental Demand, and Q9 - Low Effort Cost (p<0.01).}} 

\emph{Strength of Modularization.} Participants (U2, U4-6, U7, U9) highlighted the modularization of our system as a key advantage over Cursor (Q4 \& Q5). For example, U4 noted, ``the refinement effect for a single element [in our system] is clear (Q6), while in the other tool [Cursor], the modular refinement is ambiguous due to a lack of clear distinction between different elements.''


\emph{Strength of Graphical Control.} Participants appreciated our GUI for its intuitive manipulation of visual elements (Q2). Several participants (U1, U4, U6, U9) were surprised that they could create target spatial effects in just a few seconds using our system (Q7). Even those who redrew curves in Task 2 to better match the target path (U2, U8-9) expressed a willingness to experiment without significant effort (Q8 \& Q9). Most participants noted that relying solely on text to describe shapes made it difficult and cumbersome for Cursor to generate accurate results, leading to frustration (Q10).

\emph{Strength of Precise Control.} The precise control over effect parameters (Q3) in \sysName~was praised by participants. They highlighted its intuitiveness and effectiveness, even for the interaction effects with multiple elements (e.g., orbit radius, orbit speed). In contrast, they found that using text to adjust parameters in Cursor ``does not have a reference'' (U7) and required balancing between excessive and inadequate control (U2, U9).
}



% \begin{figure*}[t]
% \includegraphics[width=0.85\linewidth]{Figures/comparison_mean.pdf}
%   \caption{ \yhc{figure and captions update later}}
%   \label{fig:sub}
% \end{figure*}



