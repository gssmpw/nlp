\section{Formative Steps}

To better understand the challenges of utilizing LLMs to generate codes for interactive scenes, we employed a {mixed-method} %mixed methods 
approach in our formative steps: (1) content analysis on {video tutorials} %videos 
about using ChatGPT to generate codes for building interactive scenes; (2) further analysis {of} %on 
existing AI coding tools; 

\subsection{Content Analysis on {Video Tutorials} %Videos 
about Creating Interactive Scenes Using ChatGPT}
\label{sec:content_analysis}
%Since there is a lack
\yh{Due to the lack} of well-developed workflows and criteria for creating interactive scenes using LLMs, it is not easy to understand the existing challenges according to \yh{experts' experiences.} %the experiences of experts. 
So we resort to online videos -- many users uploaded %the 
video tutorials {documenting their} %and using
experience in using LLMs to create games or make interactive demos for popular video platforms (e.g., {``Can AI code Flappy Bird? Watch ChatGPT try''}%\hbc{This name indicates creating games using ChatGP seems very easy. You'd better refer to this video when discussing challenges.}\yhc{change it to another}
\footnote[3]{\url{https://www.youtube.com/watch?v=8y7GRYaYYQg}}). We want to distill valuable insights from their videos.



\textbf{\yh{Corpus and Methodology.}} We searched {for} the videos on popular video websites (e.g., YouTube, Vimeo, TikTok) using keywords such as ``GPT for interactive scenes'', ``GPT for games'', ``GPT for animations'', and ``GPT for dynamic effects''. Through the first round of searching, we collected 208 video clips. After a thorough filtering process, we retained 56 videos that specifically guided or demonstrated how to use GPT to build a complete interactive or dynamic scene. \yh{All the videos are in English and feature a single speaker. The styles include full-screen screencasts, screen recordings with annotations, tutorial formats, and picture-in-picture {screencasts}. %with a screencast. 
The duration of the videos ranges from 4 minutes 13 seconds to 26 minutes 48 seconds. The programming languages covered in these videos include C\#, JavaScript, Python, GML, Lua, and Scratch pseudocode.} Two of our authors employed the open-coding approach \cite{charmaz2008constructionism} to analyze the content of the selected videos. \yh{We began by watching each video multiple times to {understand its content comprehensively.}  %gain a comprehensive understanding of its content. 
In the initial coding phase, we identified explicit difficulties mentioned in the videos, as well as challenges reflected in the creation process. We focused on common issues faced by {those} users when using GPT for creating interactive scenes, {their} strategies %used by the speakers 
to overcome these difficulties, and the problems that persisted even after applying these methods. We developed a coding framework that categorized the identified difficulties and strategies into several themes.}
\yh{We then compared individual results and summarized the findings into overarching themes.} The coding process was iterative, allowing for refinement as new insights emerged.

% \begin{figure*}[t]
% \includegraphics[width=0.9\linewidth]{Figures/analysis.png}
% \caption{\yhc{later}\cfc{solved.}
%   (a) Use ChatGPT to create a game\project\footnotemark: the blogger lets ChatGPT generate guidance and code for each element and manually composes them together into a whole project. 
%   (b) Create a mobile app using ChatGPT\project\footnotemark: the blogger uses text to describe the UI components and their spatial layout (e.g., positions). 
%   (c) Create games using ChatGPT\project\footnotemark: the blogger tells ChatGPT that the element does not move fast enough for precise refinement.
%   }
%   \label{fig:analysis}
% \end{figure*}

% \addtocounter{footnote}{-2}
% \footnotetext[\thefootnote]{\url{https://youtu.be/3xNNuYikwes?si=PJbAoAKeV9LXW4U9}}
% \addtocounter{footnote}{1}
% \footnotetext[\thefootnote]{\url{https://youtu.be/_g4BiBcYdZQ?si=I0xvBcACmZ4x0a-w}}
% \addtocounter{footnote}{1}
% \footnotetext[\thefootnote]{\url{https://youtu.be/a5PSe0lbdkM?si=DYZuTz_4RUlnELTH}}


\textbf{Findings.} We distilled three main {issues} as follows. %\hbc{If time allows, add an illustration figure}

\emph{Independent generation and refinement \yh{(29/56)}}. 
{Three} types of strategies are mainly used for generating codes: (1) describing all the elements in the scene at one time and then iteratively adjusting the results; (2) describing each element and element interactions step by step and manually adjusting the code snippet of different elements;
% \cfc{analysis figure is removed.}
% (Figure \ref{fig:analysis}(a)); 
\yh{(3) describing the scene and asking ChatGPT to implement a basic version as the start, and then {adding} %added 
more features iteratively.} The first strategy does not require much programming understanding, but it \yh{may produce incorrect results.} %does not always produce correct results. 
% Iterative refinement %will also 
% tends to keep the original results. 
Refining one element would sometimes affect other elements. %\pfc{the logic between the previous two sentences is not clear}\yhc{I have removed the first sentence}. 
For example, in {reproducing the} %creating 
Super Mario game, adding a moving feature to one platform might also make another static platform move. This is due to ambiguous references and a limited understanding of ChatGPT. The second \yh{and third strategies} require coding skills to some degree. Sometimes GPT generates incorrect results, so the users need to use their prior knowledge to fix the errors. The dependent results will also require manual adjustment and differentiation.

\emph{Graphical control \yh{(42/56)}.}
To enable GPT to generate graphical scene interfaces or demos, there are {three} types of strategies to employ: (1) let GPT generate {graphic effects} using simple descriptions 
% \cfc{analysis figure is removed.}
%(Figure \ref{fig:analysis}(b)) 
and then adjusting {them} %\hbc{Is this related to the next challenge "precise refinement" } 
using text prompts or manual coding refinement; (2) preparing a 2D snapshot of a %the 
desired graphical scene {with} %and having 
accurate graphical information of each element; {(3) copy and paste the generated code to game engines like Unity and manipulate elements}. The first strategy {often} produces random results -- even different for every trial. Users need to use wording like ``make the rectangle larger'' and ``move it to the top'' to adjust the accurate properties. If users would like to make a random game for fun, {this strategy could} %it can 
be acceptable. {Otherwise, they have to bear a tedious adjustment process}. %But 
%if they design is seriously, it is a tedious procedure to adjust it precisely. 
The second strategy requires users to make a lot of preparation effort and then translate this graphical information into text. Some information, like user-defined curved paths, is very difficult to describe in text. So, they only use a rough representation of the results they create. {In the third strategy, users need a good understanding of both coding and game engines. For those without coding skills, filling the gap between the generated code and the manipulation in game engines requires GPT to guide users step by step to find the correspondence.}

\emph{Precise refinement \yh{(27/56)}.} Once the scene code is generated, users may refine the specific effects, such as the moving speed and the rotation radius of elements. 
% \cfc{analysis figure is removed.} 
%(Figure \ref{fig:analysis}(c)). 
In ChatGPT, they input the text again using comparative expressions (e.g., make the Mario jumps lower each time the space key is pressed, let the star move slower). However, they usually do not have an intuitive understanding of the magnitude of the parameters. So, they need to refine it back and forth to achieve the desired effect.

{In summary,} the analysis identified three main challenges in creating interactive scenes using ChatGPT: the difficulty of independent generation and refinement of code, the lack of graphical control requiring extensive manual adjustments, and the need for precise parameter adjustments due to users' limited understanding of effect magnitudes. These challenges highlight the complexities users face when leveraging LLMs %large language models 
for interactive scene creation.


\subsection{Further Analysis on Existing AI Coding Tools}
\textbf{Methodology.} We collected and reviewed six common AI coding tools (i.e., GitHub Copilot, Cursor, Tabnine, Codeium, Replit Ghostwriter, and Amazon CodeWhisperer). For the analysis of these tools, we mainly focus on the {three distilled} issues %\hbc{issues? or which issue?} 
in Section \ref{sec:content_analysis}. % 3.1. % above.

\textbf{Findings.}
These tools provide functions for generating code snippets or blocks, {either} %whether
independently or considering the entire file context. However, \yh{implementing individual elements in an interactive scene, such as those in game development, often requires creating entire classes.}%\pfc{concatenate the following paragraph here?}

The challenge lies in constructing these classes and maintaining the interactions 
% \hbc{Is it necessary to use both relationships and interactions?} 
between multiple classes. While these tools can generate basic class structures, they typically lack support for managing complex interactions, such as communication between a player character class and an enemy class. {Therefore,} %This limitation means 
users often need to manually refine and adjust the generated code to ensure that the components work together effectively, {demanding} %highlighting the need for 
a solid understanding of object-oriented programming principles.
\yh{Some tools, like Cursor \cite{cursor2023}, support modular code generation and {refinement} %refinements 
by selecting project files as context to constrain the generation. However, it {performs modular code generation in a soft manner}. % does not necessarily impose hard constraints on those files. 
{In other words}, %Instead, 
it treats them as contexts, which can lead to unintended modifications across other files. \yh{Such confusion can intertwine interaction code and individual behavior code, causing inconsistent updates and chaotic modifications.} For users seeking more controllable hard modularization, a solid understanding of the entire project structure and code framework is essential {with Cursor}, creating a significant barrier to entry.
% code generation only references the provided context. If complete independence and enforcement are required, users need to understand the code framework and make modifications in specific parts of designated files. Besides, the management of generated and updated code may lack uniformity; for instance, code for element interactions could become mixed, leading to unintended interactions when modifications are made. 
}


Since these tools are primarily designed for general programming tasks, none of them supports graphical control of the scripted elements, making it difficult to create interactive scenes directly and intuitively. This limitation forces users to rely solely on text-based refinement, which can be cumbersome when managing complex visual components. Without a graphical interface, users cannot easily manipulate or visualize elements in real-time, leading to a tedious trial-and-error process. The precise control also relies heavily on text-based adjustment. While these tools can provide suggestions for modifying {element} properties, % and positions, 
they lack the ability for direct manipulation of graphical elements. This means users must translate their visual intentions into code, which can be a time-consuming process, especially for intricate designs. They may spend significant time fine-tuning parameters through trial and error rather than simply dragging and dropping elements or adjusting them visually. 
% \cfc{analysis figure is removed.} %(Figure \ref{fig:analysis}(c)).

% \yhc{I have re-written Section 3.2. But it seems Section 3.1 and 3.2 has some overlap... Originally I plan to further discuss the issues and solutions of existing tools in Section 3.2 and their limitations. But it seems that they only provide solution for the first issue.} \hbc{Let's keep it first and revisit this issue if we have time.}



\subsection{Design Considerations}
\label{sec:design_consideration}
Based on the above findings, we envision an ideal LLM-based tool for creating interactive scenes should consider the following points:

D1. Independent {code} generation and control on elements: refining individual elements does not affect others.

D2. Context-aware code generation: independency will not lose context.

D3. Graphical control: 
integrating graphical information directly into text prompts.

D4. Easy and precise parameter control: direct manipulation of effect parameters.


