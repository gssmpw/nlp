\section{Related Work}
\label{sec:related_work}
\subsection{LLM-based Code Generation and Improvement}
LLM-based code generation leverages advanced language models \cite{li2022competition,nijkamp2022codegen,roziere2023code} to translate natural language prompts into executable code. Although these models are powerful at streamlining software development and enhancing productivity, many issues arise from the generated code, including compilation and syntax errors, wrong outputs, maintainability problems, not following standard coding practice, need refactoring, security smells, etc \cite{tian2023chatgpt, liu2024refining,liu2024no,siddiq2024quality}. In particular, ChatGPT struggles to generate code for new and unseen problems \cite{tian2023chatgpt}. Lengthy prompts might have negative impacts on the code generation \cite{tian2023chatgpt, liu2024refining}. {These issues} %It 
could be due to the increased {code complexity for more complex problems}, %complexity and the greater number of potential interactions between code elements as the code size grows, 
making it harder for {LLM models} %the model 
to generate a correct and complete solution \cite{liu2024refining}. 

Researchers have proposed \yh{to use Chain-of-Thought \cite{wu2022ai, wei2022chain}} %approaches 
to make LLMs more controllable for complex tasks,  %using chaining \cite{wu2022ai}, 
{especially} for text-based organization tasks. For coding tasks, many decomposition strategies are employed using interactive decomposition \cite{huang2024anpl}, block-based hierarchical structure \cite{ritschel2022can}, node-based diagrams \cite{wu2022promptchainer}. 
CoLadder \cite{yen2023coladder} further enables programmers to decompose tasks flexibly. These works mainly focus on general programming tasks, which typically involve static code generation and logic implementation. \yh{Modularized LLM generation mechanisms have been researched by Tree-of-Thoughts and its variations \cite{yao2024tree, besta2024graph}. \yh{They focus on general thought exploration, and we aim for a specific scenario of interactive scene creation. We integrate textual and graphical inputs with LLM code generation to enable intuitive control over element behavior and interaction.
Agentic workflows \cite{qian2024chatdev} achieve agent-level modularization and communication for entire software development across roles (Table \ref{tab:comparison}). In contrast, %Different from it, 
we emphasize element-level modular creation. We focus on precise and independent control over code generation for individual elements and their interaction.}
% \hbc{communication and interaction are the same thing?}\yhc{communication means the update among modules. I remove "communication" here.}
% .}
Kim et al. \cite{kim2023cells} explore a design framework for users to control the modularized generation of configuration components for writing tasks. We extend this framework to a specific application of interactive scene creation, where elements act as objects. This shift presents unique challenges in managing element interactions. We address this by encapsulating the code from the interaction generation or refinement for each element within its module while invoking the interaction logic in a central module.
}

% \yh{
% \begin{table}[]
% \caption{\yh{The comparison among the closely related works.
% }}
% \footnotesize
% \begin{tabular}{|c|c|c|c|c|c|c|}
% \hline
% { }                     & { \textbf{Task}}                   & { \textbf{Method}} & { \textbf{\begin{tabular}[c]{@{}c@{}}Multiple Element \\ Interaction\end{tabular}}} & { \textbf{\begin{tabular}[c]{@{}c@{}}Code \\ Modularization\end{tabular}}} & { \textbf{\begin{tabular}[c]{@{}c@{}}Graphical \\ Control\end{tabular}}} & { \textbf{\begin{tabular}[c]{@{}c@{}}Precise \\ Control\end{tabular}}} \\ \hline
% { \textbf{MoGraphGPT}}  & { Interacitve scene creation}   & { LLM-based}       & { Yes}                                                                              & { Element-level}                                                           & { Yes}                                                                   & { Yes}                                                                 \\ \hline
% { \textbf{DrawTalking \cite{rosenberg2024drawtalking}}} & { Interacitve scene creation}   & { Rule-based}      & { Yes}                                                                              & { /}                                                                       & { Yes}                                                                   & { No}                                                                  \\ \hline
% { \textbf{Spellburst \cite{angert2023spellburst}}}  & { Generative art creation}         & { LLM-based}       & { No}                                                                               & { Node-level}                                                              & { No}                                                                    & { Yes}                                                                 \\ \hline
% { \textbf{DirectGPT \cite{masson2024directgpt}}}   & { Text/code/vector images edition} & { LLM-based}       & { No}                                                                               & { No}                                                                      & { Yes}                                                                   & { No}                                                                  \\ \hline
% { \textbf{ChatDev \cite{qian2024chatdev}}}     & { General software development}    & { LLM-based}       & { Yes}                                                                              & { Agent-level}                                                             & { No}                                                                    & { No}                                                                  \\ \hline
% { \textbf{Cursor \cite{cursor2023}}} & { General programming}             & { LLM-based}       & { Yes}                                                                              & { Code line/block/file-level}                                              & { No}                                                                    & { No}                                                                  \\ \hline
% \end{tabular}
% \label{tab:comparison}
% \end{table}
% }

% \yh{
\begin{table*}[]
\caption{\yh{The comparison among the closely related works.
}}
\footnotesize
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
{ }                     & { \textbf{Task}}                   & { \textbf{Method}} & { \textbf{\begin{tabular}[c]{@{}c@{}}Multiple Element \\ Interaction\end{tabular}}} & { \textbf{\begin{tabular}[c]{@{}c@{}}Code \\ Modularization\end{tabular}}} & { \textbf{\begin{tabular}[c]{@{}c@{}}Graphical \\ Control\end{tabular}}} & { \textbf{\begin{tabular}[c]{@{}c@{}}Precise \\ Control\end{tabular}}} \\ \hline
{ \textbf{MoGraphGPT}}  & { Interacitve scene creation}   & { LLM-based}       & { Yes}                                                                              & { Element-level}                                                           & { Yes}                                                                   & { Yes}                                                                 \\ \hline
{ \textbf{DrawTalking \cite{rosenberg2024drawtalking}}} & { Interacitve scene creation}   & { Rule-based}      & { Yes}                                                                              & { /}                                                                       & { Yes}                                                                   & { No}                                                                  \\ \hline
{ \textbf{Spellburst \cite{angert2023spellburst}}}  & { Generative art creation}         & { LLM-based}       & { No}                                                                               & { Node-level}                                                              & { No}                                                                    & { Yes}                                                                 \\ \hline
{ \textbf{DirectGPT \cite{masson2024directgpt}}}   & { Text/code/vector images edition} & { LLM-based}       & { No}                                                                               & { No}                                                                      & { Yes}                                                                   & { No}                                                                  \\ \hline
{ \textbf{ChatDev \cite{qian2024chatdev}}}     & { General software development}    & { LLM-based}       & { Yes}                                                                              & { Agent-level}                                                             & { No}                                                                    & { No}                                                                  \\ \hline
{ \textbf{Cursor \cite{cursor2023}}} & { General programming}             & { LLM-based}       & { Yes}                                                                              & { Code line/block/file-level}                                              & { No}                                                                    & { No}                                                                  \\ \hline
\end{tabular}
\label{tab:comparison}
% \vspace{-10mm}
\end{table*}
% }

\subsection{LLM-powered Tools for Visual Design and Development}
LLMs {have been} %are 
employed to enhance visual design \cite{brade2023promptify} and development processes {and generate} %. 
2D static visual design{s}, % are generated 
such as personalized logos \cite{xiao2024typedance}, interior color designs \cite{hou2024c2ideas}, \yh{storybook \cite{yan2023xcreation}}, and editorial illustrations \cite{liu2022opal}. Designing dynamic 2D visuals {with LLMs}, however, presents greater challenges. Unlike static designs, animations require consideration of timing, motion, and interaction. Keyframer \cite{tseng2024keyframer} leverages LLMs to empower users in creating animations by generating keyframes based on textual descriptions. LogoMotion \cite{liu2024logomotion} develops an LLM-based system that automatically generates content-aware animations for logos by synthesizing code from visual layouts. Spellburst \cite{angert2023spellburst} introduces a node-based interface that enables users to explore creative coding through natural language prompts for interactive visual design \yh{with precise parameter control (Table \ref{tab:comparison})}. \yh{While these works enhance the use of LLMs in animation and dynamic visual design, they do not address interactions between multiple elements, thus overlooking the challenges of independent control. \yh{v0 \cite{v0_vercel} can generate interactive and dynamic effects, but lacks clear control for individual elements and graphical information.}}

% face limitations in ensuring consistent quality and coherence across diverse user inputs and complex visual contexts, which involve interactions among multiple elements. 

Games are one of the most popular applications of interactive scenes. Researchers have explored the potential of LLMs in game content design, including investigating the use of video game description language \cite{hu2024generating}, automated level design and generation from text \cite{sudhakaran2024mariogpt, todd2023level}, and co-creative game design \cite{anjum2024ink}. They aim to enhance the game content {design}
% development 
process, making it more accessible for creators to {design} 
% and implement engaging
game experiences. 
% In contrast, 
{Differently, we provide an LLM-based solution, allowing users to interactively create complete games without coding.}
% our work encompasses the entire production of interactive scenes, including implementation, code generation, and the creation of a complete
%\hbc{You meant the above works facilitate only part of the game creation process? This could be made clearer}\yhc{They focus on game (experience) design, we focus on game creation including automatic implementation without coding.}
% , functional game.

Some studies further explore the potential of LLMs in application development, including using communicative agents to support software development \cite{qian2024chatdev}, assisting end-users in generating robot programs \cite{bimbatti2023can}, enabling zero-code generation of trigger-action IoT programs \cite{li2023chatiot}, and providing an AI-augmented system for autonomous visual programming learning for children \cite{chen2024chatscratch}. They aim to lower the barriers to programming, making it easier for non-experts to create and manage complex applications. {Compared to them, our work focuses on a novel system for creating 2D interactive scenes.}
% Our work shares a similar goal, but it differs in that we propose a coding-free system
%\hbc{but we focus on a novel system for creating 2D interactive scenes? You meant the above works still involve explicit coding to some extent? What do you mean by "zero-code"?}.


\subsection{Supporting Creating 2D Interactive Scenes}
The traditional practice of creating 2D interactive scenes requires artists to create frame-by-frame animations and integrate them with programming to enable user interactions and responses. To simplify the production process, previous works introduce sketch-based interactions to animate virtual elements \cite{liu2020posetween}, manipulate kinetic 
% \hbc{kinetic?} 
textures
% \hbc{what are kinect textures?} 
\cite{kazi2014draco}, and using filters for dynamic illustrations \cite{xing2016energy}. Besides dynamic effects, Kitty \cite{kazi2014kitty} enhances the sketching interface for interactive illustrations, which {involve} %includes 
more user interactions. These works are powerful for creating diverse, fascinating, dynamic effects, but expect users to have drawing skills 
{for specifying animation effects}. Recently, several tools employ visual programming interfaces using blocks \cite{ye2024prointerar}, node-graphs \cite{snap_lensstudio, unreal_blueprints}, and flowcharts \cite{chen2021entanglevr, yigitbas2023end, zhang2020flowmatic} to build interactive scenes. {For example,} as a pioneering platform, Scratch \cite{scratch} enables users to create animations and games through a block-based coding environment. {Our graphical interface is largely inspired by the interface of Scratch. However, instead of explicit coding in Scratch, our system uses LLMs to generate code given natural language text inputs, aiming to lower the barriers to creating interactive scenes.} 

%\yh{It is a good platform to learn programming, but it has a steep learning curve for users to understand basic programming concepts. Compared to it, we provide a coding-free approach that only takes natural languages and simple graphical control as input, which has lower entry-barriers. Users does not need to know the specific implementation details insides.}

%A closely related work 
DrawTalking \cite{rosenberg2024drawtalking} is closely related to our work and enables {the creation of} interactive worlds using sketching and speaking. The main difference between DrawTalking and our work is that we introduce LLMs to generate codes for scenes automatically \yh{(Table \ref{tab:comparison})},  \yh{while their dependency-tree structure is difficult to replace directly %be directly replaced 
with an LLM model, a point confirmed by the authors of DrawTalking}. The rule-based interactions in DrawTalking can involve multiple effects, but {lack} %lacks 
flexibility and generalization for scripting customized effects. 
{For example,
% \hbc{Is the following an example of the claim in the previous sentence?}\yhc{move the newly added sentence to the front}, 
{our system supports the use of} %we support to use 
different {development} frameworks to better create scenes according to users' intention, e.g., Phaser to create games, p5.js to create creative coding effects. {Such a feature is more difficult} %It is not easy 
to achieve in DrawTalking since it is designed with defined rules.  %\hbc{I'm not sure here. Different frameworks are for the underlying implementation? Is it difficult to integrate their rules with different frameworks?} \yhc{Yes}.
In addition, the natural language input in DrawTalking needs to conform to the rules while ours allows users to describe {desired} %the
scenes freely due to the understanding capability of ChatGPT.}
%\hbc{Is this complete? Maybe we need to refer to specific examples that can be done by our method but not theirs.} \hb{If it is a short paragraph, better to merge it with the previous one?}


\vspace{-1.1mm}
% \subsection{Manipulating LLM Input and Output}
\subsection{\yh{Interacting with LLM from Input and Output}}
Recently, researchers have explored \yh{interacting with}
% manipulating 
LLM input and output to enable more controllability. For example, DirectGPT \cite{masson2024directgpt} offers an intuitive interface for users to engage directly with {LLMs}, %large language models, 
making it easier to input prompts with direct manipulation. \yh{It focuses on a {task different from ours}—editing text, code, and vector images—so it does not address dynamic interactions among multiple elements or allow for precise adjustment of editing parameters (Table \ref{tab:comparison}).} Graphologue \cite{jiang2023graphologue} enhances this interaction by allowing users to explore LLM responses through interactive diagrams, which help visualize the relationships and structures within generated content, thus clarifying complex ideas. Meanwhile, Visual ChatGPT \cite{wu2023visual} combines conversational capabilities with visual foundation models, enabling users to talk, draw, and edit visuals simultaneously, creating a richer and more dynamic engagement experience. ChainForge \cite{arawjo2023chainforge} serves as a visual toolkit for prompt engineering and hypothesis testing, allowing users to iteratively refine their inputs and analyze outputs in an intuitive visual format, thereby enhancing the overall interaction with LLMs. Compared to these works, we {use graphical control and direct manipulation to specify both input and output}.
%focus on not only enabling input but also output with graphical control and direct manipulation. 
For input, we integrate graphical information {(}by specifying or drawing visual proxies{)} into text prompts. For output, we allow users to quickly refine the effect parameters via sliders and value inputs.
