\section{RELATED WORK}
\label{sec:relatedwork}
% \qw{
% p1. data quality verification, and traditional methods
% p2. recent popular methods, i.e., statistical, and their problem
% p3. we are different: using VAE and GNN; VAE/GNN has been used in other similar problems, but not exactly ours; our benefits
% }
Research in data quality has traditionally emphasized the calculation of data quality dimensions and statistics to assess data quality. Fundamental components of this field include data profiling, which identifies issues like missing values and syntax violations~\cite{chiang2008discovering, ilyas2015trends, razniewski2011completeness}, and data quality assessment, which measures attributes such as accuracy, completeness, and consistency~\cite{DBLP:journals/jmis/WangS96, sidi2012data} to ensure datasets meet standards applicable to various domains~\cite{jayawardene2013curse, jayawardene2015analysis, YEGANEH201424}. 

% In \sj{the context of} ML pipelines, 
% studies have explored the relationship between data quality and ML performance~\cite{Budach2022TheEO}, emphasizing the necessity for high-quality data preparation~\cite{gupta2021data, Foroni2021EstimatingTE} that addresses completeness~\cite{karlavs2020nearest, schelter2020learning} and data drift~\cite{tahmasbi2020driftsurf,concept-drift-96,dong2024efficiently}. 

% Studies on data quality have traditionally focused on calculating data quality dimensions and statistics to assess quality, including data profiling \cite{chiang2008discovering, ilyas2015trends, razniewski2011completeness} and quality assessment~\cite{DBLP:journals/jmis/WangS96, sidi2012data} to ensure datasets meet standards applicable to various domains~\cite{jayawardene2013curse, jayawardene2015analysis, YEGANEH201424}.

Several studies have investigated data quality within ML pipelines, examining its influence on ML performance~\cite{Budach2022TheEO}, methods for high-quality data preparation~\cite{gupta2021data, Foroni2021EstimatingTE}, ensuring data completeness~\cite{karlavs2020nearest, schelter2020learning}, detecting data drift~\cite{tahmasbi2020driftsurf,concept-drift-96,dong2024efficiently}, and validating data quality~\cite{fadlallah2023context,sinthong2021dqdf,schelter2018automating,caveness2020tensorflow}. Frameworks like Dagger~\cite{rezig2020dagger} and Mlinspect~\cite{grafberger2021mlinspect} incorporate advanced debugging and interactive query mechanisms to streamline systematic data management. Furthermore, tools such as HoloClean~\cite{rekatsinas2017holoclean} and ActiveClean~\cite{krishnan2016activeclean} extend these efforts to data cleaning and repair, leveraging probabilistic models and active learning for iterative dataset refinement.

Existing quality validation systems typically use statistical methods or statistics to train machine learning models for detecting data quality problems. 
These methods often rely on predefined rules and constraints that require expert adjustment~\cite {fadlallah2023context, sinthong2021dqdf}, e.g., automated data quality verification systems, such as Deequ~\cite{schelter2018automating} and TFDV~\cite{caveness2020tensorflow}, which generate constraints automatically. 
However, these constraints are not always accurate and often need expert fine-tuning to effectively detect hidden data relationships and dependencies~\cite{schelter2019differential}. 
Other systems like ADQV~\cite{redyuk2021automating} use k-nearest neighbors to evaluate data quality by computing limited metrics, but fail to detect hidden errors and cannot pinpoint the incorrect samples.


Our approach uses a multi-task learning framework that combines GNN encoder~\cite{li2024graph} and dural decoders. 
While GNNs and other machine learning models are used for data imputation~\cite{telyatnikov2023egg, cappuzzo2024relational, spinelli2020missing}, anomaly detection~\cite{du2022graph}, and fraud detection~\cite{rao2020xfraud, liu2021pick}, their use in an integrated manner for direct data quality validation remains underexplored.
Our work applies GNN to automatically validate data quality, focusing on both explicit data errors and complex interdependencies without relying on expert-defined rules and manual adjustments and optimizes for both detection and repair tasks, setting our approach apart from conventional data management techniques that typically rely on isolated, task-specific methods.


% Research in data quality has traditionally emphasized the calculation of data quality dimensions and statistics to assess data quality. Fundamental components of this field include data profiling, which identifies issues like missing values and syntax violations \cite{chiang2008discovering, ilyas2015trends, razniewski2011completeness}, and data quality assessment, which measures attributes such as accuracy, completeness, and consistency \cite{DBLP:journals/jmis/WangS96, sidi2012data} to ensure datasets meet standards applicable to various domains \cite{jayawardene2013curse, jayawardene2015analysis, YEGANEH201424}.

% In machine learning (ML) pipelines, the importance of data quality is increasingly recognized, given its direct impact on model performance. 
% Studies have explored the relationship between data quality and ML outcomes~\cite{Budach2022TheEO}, emphasizing the necessity for high-quality data preparation~\cite{gupta2021data, Foroni2021EstimatingTE} that addresses completeness~\cite{karlavs2020nearest, schelter2020learning} and data drift~\cite{tahmasbi2020driftsurf,concept-drift-96}.
% \qw{briefly describe the methods that we used in experiments: what problem they address, and how they solve it, in one sentence}

% The verification of data quality ensures datasets are up to predefined standards before their deployment in analysis or training. Our approach leverages a novel combination of a feature graph \cite{li2024graph} and Variational Autoencoder (VAE) \cite{doersch2016tutorial} to unearth and rectify hidden data quality issues, which are often overlooked by existing methods. 
% This technique allows for the detection of latent errors and automates the reporting of problematic data instances without the need for manual intervention.

% Although Graph Neural Networks (GNN) and other machine learning models have been employed for tasks such as data imputation \cite{telyatnikov2023egg, cappuzzo2024relational, spinelli2020missing}, anomaly detection \cite{du2022graph}, and fraud detection \cite{rao2020xfraud, liu2021pick}, their application in direct data quality validation remains underexplored. 
% Most current validation methods deploy statistical techniques or basic machine-learning models that utilize statistical data to identify quality issues. 
% However, these often require extensive expert tuning to adapt to complex data interdependencies \cite{fadlallah2023context, sinthong2021dqdf, schelter2018automating, schelter2019differential, caveness2020tensorflow, shrivastava2019dqa, redyuk2021automating}.

% Our method distinguishes itself by utilizing advanced machine learning techniques to automatically understand and leverage these interdependencies, thereby improving the precision and automation of data quality validation. 
% This capability addresses a significant gap in traditional approaches that rely heavily on expert-defined rules and manual adjustments, which are not only resource-intensive but also prone to overlooking subtle yet critical data anomalies.
%____________________________------------------------
% Data quality research ensures datasets meet specific benchmarks for accurate and reliable analysis. 
% Fundamental components include data profiling and assessment. 
% Data profiling generates metadata about data sources, capturing attributes like data types, missing values, and distinct values, crucial for applications such as data cleansing and integration~\cite{DBLP:journals/sigmod/Naumann13, oliveira2006data, abedjan2017data}. Profiling helps identify data quality issues, such as missing values and syntax violations~\cite{chiang2008discovering, ilyas2015trends, razniewski2011completeness}. 
% Data quality assessment uses profiling outputs to measure dimensions like accuracy, completeness, and consistency~\cite{DBLP:journals/jmis/WangS96, sidi2012data}, ensuring datasets adhere to quality standards for various applications~\cite{jayawardene2013curse, jayawardene2015analysis, YEGANEH201424}.

% Data quality is a critical area of research that focuses on ensuring datasets meet specific quality benchmarks to support accurate and reliable data analysis. Data quality profiling and assessment are fundamental components of this field. Data profiling generates metadata about data sources, capturing attributes such as data types, missing values, and distinct values. This metadata is essential for various applications, including data cleansing, data integration, and query optimization \cite{DBLP:journals/sigmod/Naumann13, oliveira2006data, abedjan2017data}. Profiling helps identify data quality issues, such as missing values, syntax violations, and referential integrity problems \cite{chiang2008discovering, ilyas2015trends, razniewski2011completeness}. The subsequent data quality assessment uses these profiling outputs to measure data quality dimensions like accuracy, completeness, and consistency, categorized into intrinsic, accessibility, contextual, and representational qualities \cite{DBLP:journals/jmis/WangS96, sidi2012data}. This assessment ensures that datasets adhere to quality standards, facilitating their effective use in various applications \cite{jayawardene2013curse, jayawardene2015analysis, YEGANEH201424}.




% In ML pipelines, addressing quality issues has gained interest. Research has studied the relationship between data quality and ML performance, the impact of data quality dimensions on ML algorithms \cite{Budach2022TheEO}, their link to MLOps \cite{renggli2021data,Xin2021ProductionML}, and cleaning for ML \cite{DBLP:journals/corr/abs-1904-09483, krishnan2016activeclean}. The quality of ML models is related to the quality of the underlying data, leading to approaches for data preparation \cite{gupta2021data, Foroni2021EstimatingTE}, focusing on data completeness \cite{karlavs2020nearest, schelter2020learning} and data drift \cite{tahmasbi2020driftsurf,concept-drift-96}.
% Addressing quality issues in ML pipelines is a field with growing interest in recent years. Several research works studied the relationship between data quality and ML-model performance, including the impact of data quality dimensions on ML algorithms \cite{Budach2022TheEO}, their link to MLOps \cite{renggli2021data,Xin2021ProductionML}, cleaning for ML \cite{DBLP:journals/corr/abs-1904-09483, krishnan2016activeclean}. The quality of an ML model is often related to the quality of the underlying data, namely training and serving data. This has led to proposed approaches for data preparation, like assessing data across various quality dimensions \cite{gupta2021data} and task-dependant quality assessment \cite{Foroni2021EstimatingTE}. 
% Several approaches focused on the completeness of data \cite{karlavs2020nearest, schelter2020learning}. 
% Others focus on the data drift between training and serving data~\cite{tahmasbi2020driftsurf,concept-drift-96}.



%Bertino and Li~\cite{bertino2018adaptive,li2022blindly} focus on visual data quality with techniques not easily generalizable to other image datasets.

% Data quality verification is essential for ensuring that datasets meet predefined quality standards before they are used for analysis or model training. 
% Our method employs a feature graph~\cite{} and Variational Autoencoder (VAE)~\cite{} to detect hidden errors deeply and automatically report specific samples with issues. 
% While some studies use GNN and other ML models for data imputation~\cite{} and anomaly detection~\cite{}, these methods are not applied to data quality validation.

% Most data quality validation work uses statistical methods to verify data quality or use statistical information to train ML models for detecting data quality problems.
% \cite{fadlallah2023context,sinthong2021dqdf} 
% emphasize context awareness and data-quality-aware dataframes, yet they often depend on predefined rules and constraints that require expert intervention. 
% \cite{schelter2018automating, schelter2019differential, caveness2020tensorflow} explore automating data quality verification in large-scale scenarios but struggle with detecting hidden data relationships, they also used some statistic or simple ML methods to automatically generate constrains, but they constrains is not accuracy, need expert continue to fine-turn. 
% \cite{shrivastava2019dqa} introduces an interactive advisor for scalable data quality, yet it still relies heavily on expert-defined constraints. 
% \cite{redyuk2021automating} trained a k-nearest neighbors (KNN)~\cite{} model to judge data quality by computing limited quality metrics, which fails to detect many hidden errors, and their method cannot pinpoint which specific sample is incorrect.
% \cite{bertino2018adaptive,li2022blindly} focus on visual data quality, presenting techniques that are not easily generalizable to other image datasets. 
% for examples,

% Various studies have addressed different aspects of this topic. 

%Our method employs a feature graph and VAE to deeply detect some hidden errors, and it is fully automatic, capable of reporting which specific samples have issues.add some work using GNN and other Ml models to do the data imputation and anomaly detection, but there is no use for data quality validation, most of the work use statistic method of calculate statistic information the use statistic information to train a ml model to detect data quality problem, but these method just use not too much metrics to evaluate the quality, may lost some hidden error, and they can not report which sample have problem, because they calculate the metric for each feature, so they can not find which sample have problem, and these work also used ML model to detect the contains in the dataset. but not accuracy. need expert to fine-turn.

% Data quality verification is essential for ensuring that datasets meet predefined quality standards before they are used for analysis or model training. Various studies have addressed different aspects of this topic. 
% \cite{fadlallah2023context,caveness2020tensorflow,sinthong2021dqdf} 
% emphasize context awareness, continuous ML pipelines, and data-quality-aware dataframes, yet they often depend on predefined rules and constraints that require expert intervention. Gu et al. \cite{bertino2018adaptive} and Li et al. \cite{li2022blindly} focus on visual data quality, presenting techniques that are not easily generalizable to other datasets. Schelter and colleagues \cite{schelter2018automating, schelter2019differential} explore automating data quality verification in large-scale scenarios but struggle with detecting hidden data relationships. Shrivastava et al. \cite{shrivastava2019dqa} introduce an interactive advisor for scalable data quality, yet it still relies heavily on expert-defined rules. These methods, while insightful, often fall short of providing fully automated, comprehensive solutions adaptable to various data types and scales without extensive expert input.



%Fadlallah et al. \cite{fadlallah2023context}, Caveness et al. \cite{caveness2020tensorflow}, and Sinthong et al. \cite{sinthong2021dqdf} 