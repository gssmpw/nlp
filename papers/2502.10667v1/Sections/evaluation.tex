%\vspace*{-0.2cm}
\section{EXPERIMENTAL EVALUATION}

\subsection{Experimental Setup}
The experiments were conducted using Python 3.11 and PyTorch 1.12.1. All computations were performed on an NVIDIA A100 GPU. The source code and data have been made available\textsuperscript{\ref{footnote:code}}.

% \begin{flushleft}
% %\hrulefill
% \end{flushleft}
\footnotetext[1]{\label{footnote:code}Source code and data: \url{https://github.com/SiSijie/DQuaG}}


\subsubsection{Datasets.} We evaluate the robustness and generality of our approach using datasets with varied error types and data structures, following methodologies outlined in prior research~\cite{redyuk2021automating}.

\noindent \textit{Datasets with ground-truth errors.}
\textbf{Airbnb Data\cite{airbnb}} which contains information about Airbnb listings in New York City, including attributes such as price, location, and property type; and
\textbf{Chicago Divvy Bicycle Sharing Data\cite{divvy}} includes trip data from the Divvy bike-sharing program in Chicago, with details like the trip duration, start and end locations, and bike ID. 
\textbf{Google Play Store Apps Data\cite{google_play}} includes app data, capturing ratings, downloads, categories, and trends in app performance.
For these datasets, we have uncleaned versions with real-world errors.
We involved data-cleaning techniques to create cleaned version datasets, such as removing duplicates, handling missing data, and filtering out illogical records.


%  \sj{A “cleaned” dataset is one that has undergone data-cleaning steps
% such as removing duplicates, handling missing data, and filtering
% out illogical records.}
%While these procedures make the data cleaner than the original, it does not guarantee absolute error-free data.

\noindent \textit{Datasets without ground-truth errors.} 
\textbf{New York Taxi Trip Data\cite{nyc_taxi}} which comprises taxi trip records in New York City, detailing pickup and dropoff locations, fares, and trip durations;
\textbf{Hotel Booking Data\cite{antonio2019hotel}} contains booking information for a city hotel and a resort hotel; and
%, including booking dates, duration of stay, and number of guests.
\textbf{Credit Card Data\cite{credit_card}} includes information on credit card applications.
For these datasets, we directly utilized clean versions of data from reliable sources~\cite{credit_card,antonio2019hotel,nyc_taxi}. These datasets were publicly released, carefully collected, and cleaned before use. From these clean versions, we generated four types of data errors to create the corresponding dirty version datasets.

Clean datasets are not entirely error-free. Clean data is defined as having higher quality than dirty datasets and meeting the user's standards.




%A “clean” dataset is one that has undergone data-cleaning steps (e.g., removing duplicates/irrelevant columns, handling missing data, and filtering out illogical records). 
%For example, the Airbnb dataset was reduced to 69,305 rows (from 102,599) by dropping duplicates, handling missing values, etc.; the Divvy dataset retained only trips under one hour and removed missing values, resulting in 9,495,235 rows (from 13,774,715).
% \sj{
% A “clean” dataset is one that has undergone data-cleaning steps such as removing duplicates, handling missing data, and filtering out illogical records. 
% The cleaned datasets we used were sourced directly from Kaggle, where they had already been processed with a variety of cleaning methods, typically including the aforementioned steps. 
% While these procedures make the data cleaner than the original, it does not guarantee absolute error-free data.} 

%—only that it meets usage needs more reliably than the “dirty” version.}
%, with attributes like applicant details and credit approval status.

% \textbf{New York Taxi Trip Data\footnote{\href{https://data.cityofnewyork.us/Transportation/2015-Yellow-Taxi-Trip-Data/2yzn-sicd/about\_data}{New York taxi trip data: https://data.cityofnewyork.us/Transportation/2015-Yellow-Taxi-Trip-Data/2yzn-sicd/about\_data}}:} This dataset comprises taxi trip records in New York City, detailing pickup and dropoff locations, fares, and trip durations.
% \textbf{Hotel Booking Data\footnote{\href{https://www.kaggle.com/datasets/jessemostipak/hotel-booking-demand/data}{Hotel Booking Data: https://www.kaggle.com/datasets/jessemostipak/hotel-booking-demand/data}}:} This dataset contains booking information for a city hotel and a resort hotel, including booking dates, duration of stay, and number of guests.
% \textbf{Credit Card Data\footnote{\href{https://www.kaggle.com/datasets/rikdifos/credit-card-approval-prediction?select=application\_record.csv}{Credit Card Data: https://www.kaggle.com/datasets/rikdifos/credit-card-approval-prediction?select=application\_record.csv}}:} This dataset includes information on credit card applications, with attributes like applicant details and credit approval status.


\vspace*{-0.2cm}
\subsubsection{Synthetic Errors in Datasets without ground-truth errors.}

We simulate three ordinary errors and propose two potential hidden errors to evaluate the effectiveness of our approach:

%\soror{regarding the comment of reviewer 3:The errors of missing tabular values and spelling mistakes are trivial (there is 0 scientific value in investigating them), it would be better to not highlight the missing and typos errors, just say ordinary errors such as ...}
% \textit{Ordinary Errors:} We create three type of errors in 20\% of values in three selected attributes.
% \textbf{Missing Values:} Empty cells due to data collection or integration errors.
% \textbf{Numeric Anomalies:} Unexpected values from sensor malfunctions or scaling errors. Simulated by introducing random out-of-range or nonsensical values.
% \textbf{String Typos:} Spelling mistakes from user errors or parsing issues. Simulated by randomly replacing letters with neighboring ones on a "qwerty" keyboard.

\textit{Ordinary Errors:} We introduce three types of errors affecting 20\% of values in three selected attributes: missing Values arise from empty cells due to collection or integration errors; numeric anomalies occur when sensor malfunctions or scaling issues result in out-of-range values; and String typos are caused by spelling errors simulated by randomly replacing letters with neighboring keys on a "qwerty" keyboard.

% \textbf{Explicit Missing Values:} Explicit missing values manifest as empty cells in the dataset, typically resulting from errors in data collection or integration processes. To simulate explicit missing values, a fraction of attribute values is randomly removed and replaced with NULLs.
% \textbf{Numeric Anomalies:} Numeric anomalies involve unexpected numeric values arising from malfunctioning sensors, scaling errors, or type casting issues. These are simulated by randomly introducing out-of-range or nonsensical numeric values in the dataset.
% \textbf{String Typos:} Typos involve unexpected spelling variations in textual attributes due to user mistakes or parsing errors, such as incorrect encoding. To simulate typos, we employ the "butterfinger" strategy, randomly replacing a fraction of letters in textual attributes with neighboring letters on a "qwerty" keyboard layout.


\textit{Hidden Errors:}
\textbf{Logical and Temporal Conflicts between Attributes}, which occur when related attributes contain values that are either conflicting or illogical when considered together, or when time-related data does not follow chronological logic. 
%Such as a birth date indicates a person is a minor while another attribute indicates employment as a senior manager.
% In the Credit Card dataset, temporal conflicts were generated where \textit{DAYS\_EMPLOYED} exceeded \textit{DAYS\_BIRTH}, suggesting employment before birth. 
In the Credit Card dataset, we set two hidden conflicts. 
First, a conflict arises when \textit{DAYS\_EMPLOYED} exceeds \textit{DAYS\_BIRTH}, implying employment before birth. 
Second, a conflict involves \textit{AMT\_INCOME\_TOTAL},
\textit{NAME\_EDUCATION\_TYPE}, and \textit{OCCUPATION\_TYPE}, producing improbable combinations (e.g., high education and advanced occupation but extremely low income) that simple domain rules rarely cover.
%\soror{i would be more careful to add that simple rules rarely... unless we are aware of  existing work: there are certainly newly introduced classes of rules....}
In the Hotel Booking dataset, a hidden error was generated for bookings labeled \textit{customer\_type} as 'Group' with zero \textit{adults} and more than zero \textit{babies}, conflicting with logic.
%The first involves logical inconsistencies in booking details, where the sum of stays on weekend nights and weeknights exceeds the lead time, which is not possible logically. The features involved in this error are \textit{stays\_in\_weekend\_nights}, \textit{stays\_in\_week\_nights}, and \textit{lead\_time}. 
%The second error pertains to contextual inconsistencies, particularly evident in bookings labeled as 'Group' where the number of adults and babies does not logically align with such a designation, involving the features \textit{adults}, \textit{babies}, and \textit{customer\_type}.

%The involved features here are \textit{DAYS\_EMPLOYED} and \textit{DAYS\_BIRTH}. Additionally, conflicting attribute values were generated between \textit{NAME\_INCOME\_TYPE} and \textit{DAYS\_EMPLOYED}, such as marking a client as a 'Pensioner' who still shows significant positive employment days. This error involves the features \textit{NAME\_INCOME\_TYPE} and \textit{DAYS\_EMPLOYED}.



% \soror{ here we need to give more detail about each method, regarding the comments of reviwers 3: it's not known if they're designed to tackle the inconsistencies introduced synthetically in their dataset.}
% \soror{reviewer's comment: A more reasonable baseline is to make use of a knowledge base to check these kinds of errors. }
\subsubsection{Baselines}
We selected four SOTA baselines, each widely recognized for their effectiveness in detecting and validating data quality issues.
\noindent\textbf{Deequ~\cite{schelter2018automating}:} A tool by Amazon for scalable data quality validation, using constraints and metrics. \textit{Deequ auto} generates constraints automatically, while \textit{Deequ expert} incorporates expert-tuned adjustments for higher precision.
\noindent\textbf{TFDV~\cite{caveness2020tensorflow}:} TensorFlow Data Validation for scalable ML pipeline validation, automatically detecting anomalies and schema violations. \textit{TFDV auto} uses auto-generated constraints, while \textit{TFDV expert} supports expert fine-tuning.
\noindent\textbf{ADQV~\cite{redyuk2021automating}:} A tool leveraging adaptive learning to dynamically adjust validation criteria, excelling in evolving datasets.
\noindent\textbf{Gate~\cite{shankar2023automatic}:} A machine learning-based method for automated detection and correction of data quality issues, offering an adaptive alternative to rule-based systems.

Similar to previous work~\cite{redyuk2021automating}, we manually performed the fine-tuning work required by experts for Deequ and TFDV.
% \noindent\textbf{Deequ~\cite{schelter2018automating}:} A tool for data quality validation developed by Amazon, designed to verify data quality in large-scale datasets through a series of constraints and metrics. \textit{Deequ auto} uses automatically generated constraints, while \textit{Deequ expert} involves expert-tuned constraints.
% \noindent\textbf{TFDV~\cite{caveness2020tensorflow}:} TensorFlow Data Validation, a tool designed for scalable data exploration and validation within ML pipelines. It automatically identifies anomalies and schema violations using a predefined set of metrics. \textit{TFDV auto} uses automatically generated constraints, while \textit{TFDV expert} involves expert-tuned constraints.
% \noindent\textbf{ADQV~\cite{redyuk2021automating}:} An advanced data quality validation tool that leverages adaptive learning capabilities to automatically adjust its validation criteria. \noindent\textbf{Gate~\cite{shankar2023automatic}:} A machine learning-based approach for automated data quality assessment and improvement, focusing on detecting and correcting data quality issues without manual intervention. It uses statistical models to learn patterns within data and identifies deviations, offering a more adaptive approach compared to traditional rule-based systems.
% Similar to previous work~\cite{redyuk2021automating}, we manually performed the fine-tuning work required by experts for Deequ and TFDV.

%\noindent\textbf{DQA~\cite{shrivastava2019dqa}:} Data Quality Advisor, an interactive system for scalable data quality verification.



\subsection{Accuracy of Synthetic Error Detection}
\begin{table}[tb]
\centering
\caption{Accuracy and recall across different methods and two datasets with synthetic data errors (Ordinary errors: N = Numeric Anomalies, S = String Typos, M = Missing Values; Hidden Errors: Conflicts = Logical and Temporal Conflicts between Attributes). \small{Note: * Indicates average value.}}
%\vspace{-0.5\baselineskip}
\label{table:consolidated_validation}
\footnotesize % Setting the font size to small
%\small
\begin{tabular}{lllcc}
\hline
\textbf{Dataset} & \textbf{Error Types} & \textbf{Methods} & \textbf{Acc.} & \textbf{Recall} \\
\hline
\multirow{5}{*}{Hotel Booking} 
 & N, S, M & Deequ auto & 0.530* & 1 \\
 & N, S, M & Deequ expert & 1 & 1 \\
 & N, S, M & TFDV auto, expert & 1 & 1 \\
 & N, S, M & ADQV & 0.963* & 1 \\
 & N & Gate & 0.500 & 0 \\
  & S, M & Gate & 0.980* & 0.960* \\
 & N, S, M & \textbf{DQuaG} & 1  & 1 \\
\cline{2-5}
 & Conflicts & Deequ expert & 0.500 & 0 \\
 & Conflicts & TFDV expert & 0.500 & 0 \\
& Conflicts &  ADQV & 0.970 & 1 \\
& Conflicts &  Gate & 0.820 & 0.640 \\
 & Conflicts & \textbf{DQuaG} &  \textbf{1} & \textbf{1} \\
\hline
\multirow{6}{*}{Credit Card} 
 & N, S, M & Deequ auto & 0.550* & 1 \\
 & N, S, M & Deequ expert & 0.970* & 1 \\
 & S, M & TFDV auto & 1 & 1 \\
 & N  & TFDV auto &  0.500 & 0 \\
 & N, S, M & TFDV expert &  1 & 1 \\
 & N, S, M  & ADQV & 0.960* & 1 \\
 & N, S, M  & Gate & 0.510* & 1 \\
 & N, S, M  & \textbf{DQuaG} & 1 & 1 \\
 \cline{2-5}
 & Conflicts-1 & Deequ expert & 0.500 & 0 \\
& Conflicts-1 & TFDV expert & 0.500 & 0 \\
& Conflicts-1 & ADQV & 0.500 & 1 \\
& Conflicts-1 & Gate & 0.510 & 1 \\
 & Conflicts-1 & \textbf{DQuaG} & \textbf{1} & \textbf{1}\\
  \cline{2-5}
 & Conflicts-2 & Deequ expert & 0.500 & 0 \\
 & Conflicts-2 & TFDV expert & 0.500 & 0 \\
 & Conflicts-2 & ADQV & 0.960 & 1 \\
 & Conflicts-2 & Gate & 0.560 & 1 \\
 & Conflicts-2 & \textbf{DQuaG} & \textbf{1} & \textbf{1} \\ 
\hline
\end{tabular}%

\end{table}





We used a clean dataset, randomly sampling 10\% to generate 50 batches of clean data, and did the same with a dirty dataset to generate 50 batches of dirty data. We then used these 100 batches to test
our method and baselines.

According to Table~\ref{table:consolidated_validation}, our method performs well in detecting both ordinary and hidden errors, achieving accuracy and recall of 1. 
%\soror{what about this comment of reviewer 3: A perfect method that yields an accuracy of 1 means that either the task is too simple or there's a data leakage between the generation and the prediction!}. 
For ordinary errors, Deequ-auto and TFDV-auto methods are inaccurate due to overly strict or soft constraints but perform well after expert tuning. However, they cannot detect hidden errors. 
ADQV can automatically detect ordinary errors but fails to identify hidden errors. 
For hidden errors in the Hotel Booking dataset, ADQV shows an accuracy of 0.97 but actually flags change in numeric feature distribution, missing the real issues. 
For the Gate, the results are also unstable. The constraints it sets are too strict and cannot distinguish between dirty and clean datasets. It also cannot distinguish well between hidden errors.
% \soror{what about comparing with TFDV expert?consider the comment of Reviwer 1:The paper mentions a comparison with other data quality verification methods, but more detailed discussion and analysis may be needed to more fully evaluate the pros and cons of various methods. }
The experimental results highlight our method's robustness, particularly in identifying complex data interdependencies.
%, which existing methods often overlook due to their reliance on static rule-based systems. 





\subsection{Accuracy of Real-World Error Detection}
\begin{figure}[tb]
\centering
%\hspace*{-0.2cm}
\includegraphics[width=1.05\linewidth]{Figures/accuracy_plot.pdf}
%\vspace{-1\baselineskip}
\caption{Accuracy across different methods and two datasets with real-world data errors. (All methods have Recall=1)}
\label{fig:accuracy}
\end{figure}

We used the same method to generate 100 test batches from Airbnb and Bicycle datasets as in Section 4.2. In Figure~\ref{fig:accuracy}, our approach showed excellent performance in detecting problematic data in both real-world datasets, achieving an accuracy of 1. 
ADQV and Gate performed poorly, flagging all batches due to overly strict error detection. 
Deequ auto and TFDV auto performed poorly on the Airbnb dataset but well on the bicycle dataset. Deequ expert and TFDV expert performed well on both datasets but required manual tuning of constraints. 
Therefore, our method effectively detects real-world errors without the need for manual intervention.


\subsection{Comparison with Encoder Architectures}
\label{sec:gnn_comparison}
%\soror{it would be relevant to start with the objective of this experiment.}
To validate the effectiveness of the GAT+GIN architecture, 
we tested five encoder architectures: \textit{Graph2Vec}~\cite{narayanan2017graph2vec}, \textit{GCN}, \textit{GCN+GAT}, \textit{GCN+GIN}, and \textit{GAT+GIN} on Airbnb and Bicycle datasets. Table~\ref{tab:architectures} shows that \textit{GAT+GIN} achieves the highest difference in flagged errors, reflecting its stronger ability to distinguish clean from dirty data. 
We attribute this to GAT’s attention mechanism (focusing on important neighbors) and GIN’s injective aggregation (capturing nuanced relationships). 
For hyperparameters, all models used four layers, a hidden dimension of 64, a learning rate of 0.01, and a batch size of 128.



\begin{table}[tb]
\centering
\footnotesize
\caption{Difference (\%) in flagged errors for clean vs.~dirty data. 
(Higher is better.)}
%\vspace{-1\baselineskip}

\begin{tabular}{lccccc}
\toprule
\textbf{Dataset} & \textbf{Graph2Vec} & \textbf{GCN} & \textbf{GCN+GAT} & \textbf{GCN+GIN} & \textbf{GAT+GIN} \\
\midrule
Airbnb  & 2.72 & 1.83 & 2.60 & \textbf{4.55} & \textbf{4.17} \\
Bicycle & 21.49 & 11.06 & 12.36 & 17.51 & \textbf{21.72} \\
\bottomrule
\end{tabular}

\label{tab:architectures}
\end{table}




\subsection{Scalability Analysis}
\begin{figure}[tb]
\centering\includegraphics[width=0.7\columnwidth]{Figures/NY_Taxi_Data_Quality_Validation_Comparison_log.pdf}
%\vspace{-1.2\baselineskip}
\caption{Scalability Analysis: data quality validation time of our approach, when varying the data dimensionality and data size, on the New York Taxi dataset.}
\label{fig:Scalability_log}
\end{figure}

Figure~\ref{fig:Scalability_log} illustrates the scalability analysis of our data quality validation method on the New York Taxi dataset, showing the validation time across different data dimensions (5, 10, and 18) and varying data sizes (up to 1 million samples).
The results demonstrate that as the dataset size and the number of dimensions increases, our method's computation time increases linearly rather than exponentially, which is logical. When processing datasets with millions of data points, our method only takes ten minutes. These findings indicate that our method is scalable.

To further assess scalability and robustness, we conducted an experiment using different sample sizes (10 to 1000). The accuracy results are summarized in Table~\ref{tab:accuracy_summary}. The sample size represents the number of new data instances used for quality validation.
        
\begin{table}[ht]
    \centering
    %\footnotesize
    \small
    \caption{Summary of Overall Accuracy for Different Sample Sizes across 3 Datasets.
    %Airbnb Dataset, Bicycle Dataset and NY Taxi Trip Dataset.
    }
    %\vspace{-1\baselineskip}
    \begin{tabular}{@{}lcccccc@{}}
        \toprule
        \textbf{Sample Size} & \textbf{10} & \textbf{20} & \textbf{50} & \textbf{100} & \textbf{500} & \textbf{1000} \\ \midrule
        \textbf{Airbnb (Accuracy \%)}  & 85.0 & 93.0 & 99.0 & 99.0 & \textbf{100.0} & \textbf{100.0} \\
        \textbf{Bicycle (Accuracy \%)} & 86.0 & 92.0 & 89.0 & 97.0 & \textbf{100.0} & \textbf{100.0}
        \\
        \textbf{NY Taxi (Accuracy \%)} & 83.0 & 89.0 & 98.0 & 97.0 & \textbf{100.0} & \textbf{100.0}
        \\ \bottomrule
    \end{tabular}
    \label{tab:accuracy_summary}
\end{table}

The experimental results show that as the sample size increases, the model's accuracy also improves. When the sample size exceeds 500, the model achieves 100\% accuracy across both datasets, indicating stable performance at larger scales. These findings highlight the effectiveness of our method for large sample sizes but also suggest limitations in scenarios with smaller data availability.

%\subsection{Discussion} 
%\soror{it's better to remove this section, and put this as future work, instead of saying we encourage continued collaborative efforts within the community to advance this field}


%Data quality validation remains a foundational task in data management, and we encourage continued collaborative efforts within the community to advance this field.
%To provide a comprehensive comparison, we have thoroughly evaluated each baseline system and included the Criteria for System Selection in Appendix Table~\ref{tab:system_recommendation}. 
%


% Table~\ref{tab:system_recommendation} summarizes recommended tools to help users select the appropriate tool for their specific requirements.

% \begin{table}[h]
% \centering
% \footnotesize
% \caption{Recommendation of Systems Based on Scenarios}
% \vspace{-1\baselineskip}
% \label{tab:system_recommendation}
% \begin{tabular}{@{}lll@{}}
% \toprule
% \textbf{Scenario} & \textbf{Criteria} & \textbf{Recommended System(s)} \\ \midrule
% \multirow{2}{*}{Data Nature} & Single dataset validation & Deequ, TFDV \\
%  & Streaming data & DQuaG, Deequ, TFDV, ADQV \\ \addlinespace
% \multirow{3}{*}{Data Size} & Data size < 500 & ADQV (faster) \\
%  & 500 < Data size < 10,000 & ADQV (faster), DQuaG\\
%  & Data size $\geq$ 10,000 & Deequ (first choice), TFDV \\ \addlinespace
% \multirow{2}{*}{Data Type} & Structured data & All systems \\
%  & Text & Gate, ADQV \\ \addlinespace
% \makecell[l]{Custom Metric\\ Validation} & \makecell[l]{Requirement for detailed,\\ customized validation of metrics} & Deequ \\ \addlinespace

% \makecell[l]{Data Drift\\ Detection} & Need to detect data drift over time & TFDV \\ \bottomrule
% \end{tabular}
% \end{table}




\subsection{Data Repair Evaluation}
To assess the effectiveness of our data repair process, we conducted experiments on the Airbnb and Bicycle datasets. 
According to our data quality validation results of the Airbnb dataset, The error rate of the original dirty dataset was 10.52\%. 
After applying repair suggestions generated by our repair decoder, the error rate was reduced to 4.97\%, closely matching the error rate of 4.95\% observed in the clean dataset. 
For the results of the Bicycle dataset, the repair decoder reduced the error rate of the dirty data from 21.11\% to 2.75\%. 
Importantly, the repaired dataset was classified as clean under our quality validation framework. 



%This confirms that our approach can handle larger and more complex datasets efficiently, maintaining consistent performance.


% In this study, we e our approach with other baselines on the New York Taxi dataset. The objective was to assess the efficiency of conducting data quality validation tasks. The data sizes used ranged from 1,000 to 1,000,000 records. Detailed results can be found in Figure~\ref{fig:Scalability_log}.

% The experimental results demonstrate that our approach exhibits linear growth in processing time as the data size increases. This linear scalability is comparable to other methods. Specifically, our method shows significant advantages when the data size is less than 50,000 records, where it is faster than Deequ, Gate, TFDV, and ADQ.

%For datasets larger than 50,000 records, the processing time of our approach remains comparable to Deequ, while being substantially lower than TFDV and ADQ. Overall, our approach demonstrates efficient scalability and competitive performance across varying data sizes, particularly excelling in smaller datasets and remaining effective for larger datasets. This confirms the robustness and applicability of our method in diverse Big Data environments.


% \begin{table}[ht]
% \centering
% \begin{tabular}{lccc}
% \hline
% \textbf{Data Size} & \textbf{TFDV (s)} & \textbf{ADQ (s)} & \textbf{Deequ (s)} \\
% \hline
% 1,000 & 1.253 & 0.201 & 1.318 \\
% 10,000 & 1.993 & 1.657 & 1.008 \\
% 100,000 & 7.740 & 16.217 & 3.448 \\
% 1,000,000 & 66.321 & 163.070 & 10.248 \\
% 10,000,000 & 650.859 & 1629.924 & 72.092 \\
% \hline
% \end{tabular}
% \caption{Performance of TFDV, ADQ, and Deequ for NY Taxi Data Checking}
% \label{table:performance_comparison}
% \end{table}



% \subsection{VAE Loss Distribution Analysis from Dirty and Clean Airbnb Datasets}

% In this experiment, we used the Airbnb dataset to compare the VAE loss distributions of 2000 samples from both dirty and clean datasets. We normalized the loss values to facilitate a clear comparison. The results are visualized in Figure \ref{fig:normalized_loss_distribution}.

% The analysis reveals a distinct difference between the loss distributions of the two datasets. Specifically, the loss distribution of the dirty dataset is significantly higher than that of the clean dataset. This indicates that the VAE is more likely to encounter reconstruction difficulties with dirty data, resulting in higher loss values.

% \begin{figure}[tb]
%     \centering
%     \includegraphics[width=0.75\linewidth]{Figures/normalized_loss_distribution.pdf}
%     \vspace{-1.5\baselineskip}
%     \caption{Normalized VAE loss distributions for 2000 dirty and clean Airbnb dataset samples. The dirty dataset exhibits higher loss values, indicating poorer data quality.}
%     \label{fig:normalized_loss_distribution}
% \end{figure}




    

































