\section{OUR APPROACH: DQuaG}

\begin{figure*}[tb]
\centering
\includegraphics[width=0.7\textwidth]{Figures/structure.pdf}
%\vspace{-0.6\baselineskip}
\caption{Data Quality Validation Framework Using GNN. Top: Training on clean data. Bottom: Validating unseen data by reconstruction error comparison.}
%\vspace*{-0.3cm}
%\soror{highlight in the caption the process at the top and the one at the bottom of teh figure }
\label{fig:framework}
\end{figure*}

%In this section, we detail our novel approach to automated data quality validation using graph representation learning and a Variational Autoencoder (VAE) framework. Assuming we start with a clean dataset, our method addresses the limitations of traditional data quality verification techniques through a series of steps designed to capture intrinsic relationships within tabular data and assess data quality with minimal expert intervention.


In this section, we present DQuaG (Data Quality Graph), a novel approach for data quality validation. 
Figure~\ref{fig:framework} illustrates the framework of our approach, which includes two main phases: model training on a clean dataset and data quality validation and repair for new data. 
In Phase 1, we train a model using a clean dataset to learn the normal patterns and relationships between features. 
In Phase 2, we use the trained model to assess the quality of new data and provide repair suggestions for any detected errors. 
%Our method incorporates several key innovations, including the use of an improved Graph Neural Network (GNN) encoder combining Graph Attention Network (GAT) and Graph Isomorphism Network (GIN), and a multi-task learning framework with dual decoders for data quality validation and repair. 


\subsection{Phase 1: Training GNN on Clean Data}
%\soror{it's betetr to remove data preprocessing as a step as we don't see it in the figure! we can only keep its text}
%\subsubsection{\textbf{Data Preprocessing}}
We assume the availability of a high-quality, clean dataset, $\mathcal{D}_{\text{clean}}$, that has undergone rigorous quality control and is free from errors. This dataset serves as the foundation for training our model. 
For feature encoding and normalization, categorical features are converted to numerical form using label encoding, where the encoder is fitted on both clean data and any possible future data to ensure consistency. 
For numerical features, we apply min-max normalization to scale values to the range [0, 1], which helps improve training stability and ensures that all features are on a comparable scale.


\subsubsection{\textbf{Feature Graph Construction}}

We use ChatGPT-4~\cite{openai2024gpt4} to automate the feature graph construction. 
Given a clean dataset, we extract the feature names ($F$) and their descriptions ($D$) from the data source. We then randomly sample 100 data points from the dataset, denoted as ($S$). 
These feature names, descriptions, and sample points are provided to ChatGPT-4 in a structured format to infer potential relationships between features. 
The output from ChatGPT-4 is a JSON file capturing feature relationships, which we denote as  \(\text{Feature\_Relationships} = \{ (f_i, f_j) \mid f_i, f_j \in F \}\), indicating that there is a relationship between features \( f_i \) and \( f_j \).

%The prompt used for ChatGPT-4 is as follows:
\begin{tcolorbox}[
    sharp corners=south,
    colback=white!98!black,
    colframe=white!45!black,
    boxrule=0.5mm,
    width=0.48\textwidth,
    enlarge left by=0mm,
    enlarge right by=0mm,
    arc=5mm,
    outer arc=3mm,
    %drop shadow south east={shadow xshift=0.5ex, shadow yshift=-0.5ex, fill=black!20},
    fonttitle=\bfseries,
    title=Prompt for Feature Relationship Inference,
    before upper=\par\small,
    after upper=\par\small
]
\vspace*{-0.2cm}
\small
Given the following information, please infer the relationships between features. Provide your output in JSON format, capturing the type of relationships.\newline
\textbf{Feature Names:} {List of feature names ($F$)}\newline
\textbf{Feature Descriptions:} {List of descriptions ($D$) for each feature}\newline
\textbf{Sample Data Points:} {100 data samples ($S$) from the dataset}\newline
\textbf{Output:} Please return a JSON object in the format:
\begin{verbatim}
{"relationships": [{"feature1", "feature2"}, 
                   {"feature3", "feature4"}, ...]}
\end{verbatim}
\vspace*{-0.3cm}
\end{tcolorbox}




Using these relationships, we construct the knowledge-based feature graph \( G = (V, E) \), where \( V \) represents features and \( E \) represents edges indicating relationships between features.

% \subsubsection{\textbf{Feature Graph Construction}}

% The initial step in our approach involves constructing a feature graph from clean tabular data to capture intrinsic relationships and dependencies between data features.
% First, we address the challenge of diverse data types: categorical variables are transformed using label encoding, and timestamp data is broken into components (i.e., day, month, year). 
% This uniform input format is critical for graph-based processing.

% We use ChatGPT-4~\cite{openai2024gpt4} to automate the feature graph construction. 
% Given a clean dataset, we extract the feature names \( F \) and their descriptions \( D \) from the data source. We then randomly sample 100 data points from the dataset, denoted as \( S \). These feature names, descriptions, and sample data points are provided to the ChatGPT-4, structured as follows: \(\text{Input} = \{ F, D, S \}\), then ChatGPT-4 generates a JSON file capturing feature relationships.
% The output format is \(\text{Feature\_Relationships} = \{ (f_i, f_j) \mid f_i, f_j \in F \}\), indicating that there is a relationship between features \( f_i \) and \( f_j \).

% Using these relationships, we construct the knowledge-based feature graph \( G = (V, E) \), where \( V \) represents features and \( E \) represents edges indicating relationships between features.

% \subsubsection{\textbf{Feature Graph Construction}}

% The initial step in our methodology involves constructing a feature graph from clean tabular data, which is essential for capturing the intrinsic relationships and dependencies between different data features.

% To facilitate this process, our approach first addresses the challenge of handling diverse data types. 
% In the preprocessing stage, categorical variables are transformed using label encoding, which assigns each unique category a unique integer based on alphabetical ordering. 
% For timestamp data, we extract significant components such as day, month, and year. 
% This uniform input format is critical for the subsequent graph-based processing. 

% Following the preprocessing, we utilize a large language model, ChatGPT-4 \cite{openai2024gpt4}, to automate the construction of the feature graph. This integration allows for a more nuanced capture of feature relationships and dependencies, reducing reliance on expert knowledge and manual effort.

% Given a clean dataset, we extract the feature names \( F = \{f_1, f_2, \ldots, \\f_n\} \) and their descriptions \( D = \{d_1, d_2, \ldots, d_n\} \) from the data source. We then randomly sample 100 data points from the dataset, denoted as \( S = \{s_1, s_2, \ldots, s_{100}\} \). These feature names, descriptions, and sample data points are provided to the LLM, structured as follows: \(\text{Input} = \{ F, D, S \}\).

% The LLM analyzes the provided input and generates a structured JSON file capturing the relationships between features. The output format is \(\text{Feature\_Relationships} = \{ (f_i, f_j) \mid f_i, f_j \in F \}\), indicating that there is a relationship between features \( f_i \) and \( f_j \).

% Using the relationships provided by the LLM, we construct the feature graph \( G = (V, E) \) where \( V = F \) (nodes representing features) and \( E = \{(f_i, f_j) \mid (f_i, f_j) \in \text{Feature\_Relationships} \} \) (edges representing relationships). 

%----------------------------
%This graph-based representation allows us to model complex interdependencies within the data that are often overlooked by traditional methods, enhancing our ability to perform thorough data quality assessments.



% \subsubsection{\textbf{Training the Graph Neural Network (GNN) and Representing the Clean Dataset}}
\subsubsection{\textbf{GNN Model Architecture}}
Our model architecture combines the strengths of different graph neural network variants to effectively capture complex feature relationships. 
The architecture consists of three main components: an improved GNN encoder that fuses Graph Attention Network (GAT)~\cite{velivckovic2017graph} layers and Graph Isomorphism Network (GIN)~\cite{xu2018powerful} layers, and two specialized decoders for quality validation and repair suggestion generation.

\noindent{\textbf{GNN Encoder (GAT + GIN)}.
Our encoder consists of four layers: alternating \textit{Graph Attention Network (GAT)} and \textit{Graph Isomorphism Network (GIN)} layers, in the order of GAT-GIN-GAT-GIN. 
%We employ this combination to leverage the complementary strengths of both GAT and GIN. 
This design is inspired by recent findings in the field of graph representation learning that demonstrate how combining different types of graph layers can yield improved performance in feature extraction and relational representation tasks~\cite{zhang2019heterogeneous}. 
Our experimental results demonstrate the advantages of this structure.

The GAT layers compute attention weights between connected features, enabling the model to adaptively assign importance to significant relationships in the data. This allows the model to focus on critical connections and ignore irrelevant information, which enhances its ability to learn meaningful feature representations. Our approach uses GAT layers, which automatically learn edge weights through attention mechanisms during training. This eliminates the need to manually assign weights in the initial feature graph. 
%\soror{I think here we should well emphasize the concerns of R1W2: we are not using statistical correlations as suggested by the reviewer? how to prove that the rleationships generated with our approach improve the model’s representation of real-world datadependencies }

The GIN layers aggregate feature information from neighboring nodes to capture structural information more effectively. By using GIN, the encoder gains a strong ability to represent the underlying structure of the data, preserving key relationships crucial for data quality validation and repair tasks.

This alternating GAT and GIN structure enhances the model's ability to both prioritize important features and learn intricate structural relationships, thereby making it more effective at representing complex feature dependencies in the data.
Specifically, the GNN encoder processes the feature graph $G = (V, E)$ along with the input data matrix $\mathbf{X} \in \mathbb{R}^{n \times d}$, where $n$ is the number of nodes (features) and $d$ is the dimensionality of each feature vector. The output from the GNN encoder is a feature embedding matrix $\mathbf{Z} \in \mathbb{R}^{n \times h}$, where $h$ represents the size of the learned feature embeddings.


% \noindent{\textbf{Encoder Structure}.}
% Our encoder consists of two layers: a GAT layer followed by a GIN layer. We combine GAT and GIN because of their complementary strengths. 

% The GAT layer computes attention weights between connected features, allowing the model to prioritize significant relationships. 
% For each feature node $i$ and its neighbor $j$, the attention coefficient $alpha_{ij}$ is computed as:
% \begin{equation}
% \alpha_{ij} = \frac{\exp(\text{LeakyReLU}(\mathbf{a}^T[\mathbf{W}\mathbf{h}_i \Vert \mathbf{W}\mathbf{h}j]))}{\sum{k \in \mathcal{N}(i)} \exp(\text{LeakyReLU}(\mathbf{a}^T[\mathbf{W}\mathbf{h}_i \Vert \mathbf{W}\mathbf{h}_k]))}
% \end{equation}

% In this equation, $\mathbf{h}_i$ is the feature representation of node $i$, $\mathbf{W}$ is a learned weight matrix, $\mathbf{a}$ is a learned attention vector, and the symbol $\Vert$ denotes concatenation. The GAT layer enables the model to adaptively assign importance to different features based on the data context. 

% Following the GAT layer, the GIN layer enhances the model's ability to capture structural information by aggregating feature information from neighboring nodes. The GIN layer updates the representation of each node $i$ as follows:
% \begin{equation}
% \mathbf{h}_i^{(l+1)} = \text{MLP}^{(l)}\left((1 + \epsilon^{(l)})\mathbf{h}i^{(l)} + \sum{j \in \mathcal{N}(i)} \mathbf{h}_j^{(l)}\right)
% \end{equation}

% Here, $\epsilon^{(l)}$ is a learnable parameter at layer $l$ that controls the importance of the original node representation, and $\text{MLP}^{(l)}$ represents a multi-layer perceptron, which adds non-linearity to enhance the model's expressive power. 

% The combination of GAT and GIN layers ensures that our model learns both the local importance of features and their broader, structural context, leading to richer and more accurate feature representations.
% Specifically, the GNN encoder processes the feature graph $G = (V, E)$ along with the input data matrix $\mathbf{X} \in \mathbb{R}^{n \times d}$, where $n$ is the number of nodes (features) and $d$ is the dimensionality of each feature vector. The output from the GNN encoder is a feature embedding matrix $\mathbf{Z} \in \mathbb{R}^{n \times h}$, where $h$ represents the size of the learned feature embeddings.

\noindent{\textbf{Dual Decoder Structure}.}  
Our model employs two separate decoders to address the tasks of Data Quality Validation and Repair Suggestion, enabling focused optimization for each objective.

\textit{Data Quality Validation Decoder} is responsible for reconstructing the original feature space from the learned embeddings, denoted as \(\mathbf{Z}\). 
The primary objective of this decoder is to learn the correct patterns from clean data and reconstruct the features in a way that captures the underlying structure of the dataset. This allows us to identify abnormalities by measuring reconstruction errors. We have designed a unique loss function that ensures the model focuses on learning accurate representations of clean data while effectively distinguishing abnormal samples.

For normal data samples, the decoder should ideally have a low reconstruction error, as the learned embeddings should effectively capture the true relationships between the features, resulting in an accurate reconstruction. For abnormal samples, the reconstruction error will be higher, indicating that these samples do not conform to the learned patterns from the clean data.
The reconstruction loss is defined as 
\( L_{\text{validation}} = \frac{1}{N} \sum_{i=1}^{N} w_i \left\| \mathbf{X}_i - \hat{\mathbf{X}}_i \right\|_2^2 \),
% The reconstruction loss is defined as follows:
% \begin{equation}
% L_{\text{validation}} = \frac{1}{N} \sum_{i=1}^{N} w_i \left\| \mathbf{X}_i - \hat{\mathbf{X}}_i \right\|_2^2
% \end{equation}
where \(\mathbf{X}_i\) represents the original input features, and \(\hat{\mathbf{X}}_i\) is the reconstructed feature vector for the \(i\)-th sample. The weights \(w_i\) are assigned to each sample based on its reconstruction error.

We assign larger weights to normal data samples (\(w_i\) s higher for samples with smaller reconstruction errors), giving them a greater influence in minimizing their reconstruction loss. This encourages the model to accurately reconstruct the normal data and effectively learn the correct data distribution. 
For samples with potential quality issues, the weights \(w_i\) are reduced, meaning that their influence on the overall loss is diminished. 
This allows the model to focus on minimizing the reconstruction loss for normal data while maintaining high reconstruction errors for problematic samples during the backpropagation. 
By using this weighting mechanism, we ensure that the validation decoder can distinguish between normal data and data with potential issues based on reconstruction errors.


\textit{Data Repair Decoder}, on the other hand, takes the same learned embeddings \(\mathbf{Z}\) as input, but its goal is different: it aims to suggest repaired values for features identified as erroneous. 
Unlike the Data Quality Validation Decoder, which reconstructs data to highlight discrepancies, the Data Repair Decoder attempts to produce an output that aligns with the clean, underlying data distribution, effectively suggesting corrections for the detected errors. 
The objective of this decoder is defined through the following loss function:
\(
L_{\text{repair}} = \frac{1}{N} \sum_{i=1}^{N} \left\| {\mathbf{X}}_i - \tilde{\mathbf{X}}_i \right\|_2^2
\).
Here, \(\tilde{\mathbf{X}}_i\) represents the feature values repaired by the decoder, while \(\mathbf{X}_i\) stands for the corresponding clean feature values from the input dataset. Since the input is already clean, \(\mathbf{X}_i\) can directly serve as the target for the repair task.

The combination of these two decoders is essential for effectively handling data quality issues. 
The overall loss function is a weighted sum of the validation and repair losses:
\(
L_{\text{total}} = \alpha L_{\text{validation}} + \beta L_{\text{repair}}
\),
where \(\alpha\) and \(\beta\) are hyperparameters used to balance the contributions of reconstruction and repair, both of which are set to 1 in our experiments.

The two decoders serve different purposes: the \textit{Data Quality Validation Decoder} is optimized to detect data issues by maximizing reconstruction errors for problematic instances, while the \textit{Data Repair Decoder} aims to provide realistic corrections for identified issues. By separating these tasks, the model avoids conflicting optimization goals, ensuring it is both effective at identifying problems and providing reliable repairs.


\noindent{\textbf{Multi-Task Learning Framework}.}
%\paragraph{\textbf{Multi-Task Learning Framework}} 
% The encoder is shared between the quality validation and data repair tasks, while each task-specific decoder learns independently. 
% This multi-task framework enables the model to exploit shared information between these two tasks, allowing the model to learn a unified representation that is beneficial for both.
The encoder is shared between the quality validation and repair tasks, while each task-specific decoder learns independently. This multi-task framework enables the model to exploit shared information between these tasks, allowing the model to learn a unified representation beneficial for both.

\subsubsection{\textbf{Training Process.}}
We train the model on the clean dataset using an optimizer Adam to minimize $L_{\text{total}}$. %During training, the GNN encoder alternates between GAT and GIN layers to improve its representation capabilities, enabling it to better capture complex feature relationships. 
%Training is performed iteratively, with each iteration including forward propagation through both the encoder and decoders, loss calculation, and parameter updates.

\subsubsection{\textbf{Collecting the statistics of reconstruction errors.}}
During training, we record the reconstruction error for each instance.
The reconstruction error is essentially the loss for each instance.
Let $e_i$ denote the reconstruction error for instance $i$, and let $\mathcal{E}$ be the set of all reconstruction errors from the clean dataset. 
Given that even cleaned datasets may contain undetected errors, we do not set the maximum reconstruction error as the threshold for identifying problematic instances. 
Instead, we set the threshold at the 95th percentile of \(\mathcal{E}\), denoted as \( e_{threshold} \).
% :
% \begin{equation}
% e_{\text{threshold}} = \text{Quantile}(\mathcal{E}, 0.95)
% \end{equation}

Instances in the next phase with reconstruction errors above \( e_{threshold} \) are flagged as potentially problematic.


\subsection{Phase 2: Data Quality Validate and Repair}

\subsubsection{\textbf{Data Quality Validation Process}}
In this phase, we validate the quality of incoming data by comparing it to the patterns learned during model training.

%\noindent{\textbf{New Data Preprocessing}.}
The new unseen data is preprocessed in the same manner as the clean dataset to ensure consistency in feature encoding, normalization, and feature graph construction. These new unseen datasets must keep the same schema as the original clean dataset.

\noindent{\textbf{Detecting Data Quality Issues by Reconstruction Errors}.}
After preprocessing, the model uses the validation decoder to reconstruct the features of the new data. 
For each data instance, the reconstruction error $e_i$ is calculated. We then obtain a list of reconstruction errors, denoted as \(\mathcal{E}_{\text{new}}\).
Next, we compare each reconstruction error in \(\mathcal{E}_{\text{new}}\) with the threshold \( e_{threshold} \) from the clean dataset. 
We calculate the proportion of instances in the new dataset with reconstruction errors exceeding \( e_{threshold} \), denoted as \( R_{error} \). 
Since the threshold was set at the 95th percentile for the clean dataset, we expect around 5\% of clean data instances to exceed this value. 

To account for data variability, if \( R_{error} \) exceeds \( 5\% \times n \), we classify the new dataset as problematic. This means if more than \( 5n\% \) of instances in the new dataset have errors greater than \( e_{threshold} \), we will report the dataset has data quality issues. The parameter \( n \) can be adjusted based on observed reconstruction errors after deployment.
In our experiments, we set \( n = 1.2 \), which exhibited good performance.
Finally, we report the indices of all instances in the new dataset with reconstruction errors above \( e_{threshold} \), clearly identifying problematic samples.


\noindent{\textbf{Detecting Feature Errors}.}
Each instance's reconstruction error \( e \) is a list corresponding to each feature's loss. To identify specific problematic features, we detect outliers with significantly higher reconstruction errors.
For an instance \( \mathbf{x}_i \), let \( \mathbf{e}_i = [e_{i1}, e_{i2}, \ldots, e_{in}] \) be the reconstruction errors for the \( n \) features. We calculate the mean \( \mu_i \) and standard deviation \( \sigma_i \) of the errors. Features with errors greater than \( \mu_i + 5\sigma_i \) are flagged as problematic.

% By reporting these outlier features, we can pinpoint which specific parts of an instance contribute most to data quality issues. 
% This drill-down process helps identify exact feature-level problems within instances, facilitating targeted data cleaning.

\subsubsection{\textbf{Repair Suggestion Generation}}
In this phase, we provide repair suggestions for detected errors to improve the quality of the data for downstream use.
The repair decoder is used to generate a repaired feature vector, which includes suggested repaired values for all features. 
In the previous step, we flagged which specific instances and features were problematic. 
Then we selectively apply modifications only to the flagged problematic features. For categorical features, the repair decoder predicts the most likely corrected category, while for numerical features, it predicts a value that aligns with the learned data distribution. 
%This approach ensures that the repaired values are both accurate and contextually coherent, reducing the risk of introducing new inconsistencies.


% \noindent{\textbf{Confidence Scoring}.}
% A confidence score $c_{ij} \in [0, 1]$ is assigned to each repaired feature $\tilde{x}{ij}$ to quantify the reliability of the suggested repair. The confidence score is defined based on the distance between $\tilde{x}{ij}$ and the expected distribution of clean data, using the standard deviation $\sigma_j$ of feature $j$ in the clean dataset:

% \begin{equation} c_{ij} = \exp\left(-\frac{(\tilde{x}_{ij} - \mu_j)^2}{2\sigma_j^2}\right) \end{equation}

% where $\mu_j$ and $\sigma_j$ are the mean and standard deviation of feature $j$ from the clean dataset. A higher confidence score indicates that the repaired value $\tilde{x}_{ij}$ is more consistent with the expected normal distribution of clean data.

% By providing confidence scores, we enable data engineers to better assess the trustworthiness of the repair suggestions, facilitating informed decision-making in the data cleaning process.


% \noindent{\textbf{Confidence Scoring}.}  
% To quantify the reliability of each repaired feature \(\tilde{x}_{ij}\), we assign a confidence score \(c_{ij} \in [0, 1]\). This score reflects how closely the repaired value matches the expected distribution of clean data. It is computed using the mean \(\mu_j\) and standard deviation \(\sigma_j\) of feature \(j\) from the clean dataset, where higher confidence scores indicate greater consistency with the expected normal distribution.

% By providing confidence scores, we help data engineers evaluate the trustworthiness of repair suggestions, facilitating better decision-making in the data cleaning process.

%Our approach offers several key advantages over traditional data quality verification methods. By leveraging GNNs and VAEs, it automatically identifies data quality issues without predefined constraints and detects hidden relationships within the data. This reduces the need for continuous expert input, making the process more efficient and scalable. Additionally, it can pinpoint problematic samples and specific features, facilitating targeted data cleaning and correction.




