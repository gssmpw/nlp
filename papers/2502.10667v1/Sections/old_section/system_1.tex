
\section{(Our Solution)}

In this section, we present our innovative approach for automated data quality validation and repair using advanced graph neural network techniques, with a particular focus on multi-task learning. Our solution is designed to efficiently detect both ordinary and hidden data quality issues, while also providing corrective suggestions for repairing identified errors. The key steps involved in our method are detailed below.

\subsection{Data Representation and Feature Graph Construction}

The first step in our approach is to represent the input tabular dataset as a graph. This representation helps in capturing intrinsic relationships between features, which is critical for identifying hidden errors that may not be detectable using traditional methods.

We begin by constructing a feature graph \$G = (V, E)\$, where each node \$v\_i \in V\$ represents a feature in the tabular dataset, and edges \$e\_{ij} \in E\$ represent dependencies or correlations between features \$v\_i\$ and \$v\_j\$. To construct the graph, we leverage a large language model (LLM) such as ChatGPT-4 to automatically infer meaningful relationships between features.

\noindent The process of using ChatGPT-4 for feature graph construction is as follows:

- **Input to ChatGPT-4**: We provide the LLM with feature names \$F = {f\_1, f\_2, \dots, f\_n}\$, their descriptions, and a random sample of 100 data points from the dataset. This input is structured as:

  \begin{itemize}
  \item **Feature Names (\$F\$)**: A list of feature names in the dataset.
  \item **Feature Descriptions (\$D\$)**: Detailed descriptions of each feature, which provide semantic information about the feature's purpose.
  \item **Sample Data Points (\$S\$)**: A subset of data points from the dataset to provide context on how features are related.
  \end{itemize}

  The input prompt provided to ChatGPT-4 is:
  \begin{quote}
  "Given the feature names, descriptions, and sample data points, determine the relationships between features. Provide the relationships in JSON format, where each key-value pair represents a pair of features and the type of relationship between them."
  \end{quote}

- **Output from ChatGPT-4**: The model generates a JSON file capturing feature relationships, which is then parsed to construct the feature graph. The relationships are represented as:

  indicating that there is a relationship between features \$f\_i\$ and \$f\_j\$.

Using these relationships, we construct the knowledge-based feature graph \$G = (V, E)\$, where \$V\$ represents features and \$E\$ represents edges indicating relationships between features.

\subsection{Graph Neural Network Encoder}

Once the feature graph is constructed, we employ a Graph Neural Network (GNN) to learn a latent representation of the features. The goal is to capture the underlying structure of the clean dataset and learn meaningful feature embeddings that can be used for data validation and repair.

We use a hybrid Graph Attention Network (GAT) and Graph Isomorphism Network (GIN) encoder to effectively model both the feature-level attention and the feature aggregation:

\begin{itemize}
\item \textbf{Graph Attention Network (GAT)}: The GAT layer learns attention coefficients for each node's neighbors, allowing the model to focus on the most relevant features during aggregation. The output of the GAT layer for node \$v\_i\$ is given by:
\begin{equation}
h\_i^{(l+1)} = \sigma\left( \sum\_{j \in \mathcal{N}(i)} \alpha\_{ij}^{(l)} W^{(l)} h\_j^{(l)} \right),
\end{equation}
where \$\alpha\_{ij}^{(l)}\$ are the learned attention coefficients, \$W^{(l)}\$ is the weight matrix, \$\mathcal{N}(i)\$ represents the neighbors of node \$i\$, and \$\sigma\$ is a non-linear activation function. The GAT layer helps the model focus on the most important relationships by assigning different attention weights to different neighbors, which is crucial for capturing complex dependencies between features.

```
\item \textbf{Graph Isomorphism Network (GIN)}: The GIN layer aggregates feature information by summing the features from neighboring nodes, followed by a multi-layer perceptron (MLP). This allows for stronger feature aggregation. The output for node $v_i$ is given by:
\begin{equation}
h_i^{(l+1)} = \text{MLP}^{(l)}\left( (1 + \epsilon) h_i^{(l)} + \sum_{j \in \mathcal{N}(i)} h_j^{(l)} \right),
\end{equation}
where $\epsilon$ is a learnable parameter or fixed scalar. The GIN layer is effective in capturing structural information and distinguishing different graph structures, making it useful for learning robust feature embeddings.
```

\end{itemize}

\noindent \textbf{Combining GAT and GIN:} The GAT and GIN layers are combined sequentially to leverage the strengths of both models. First, the GAT layer is applied to focus on the most relevant neighboring features through attention mechanisms. Then, the GIN layer is used to aggregate information from these neighbors in a more expressive manner, capturing both the attention-based relevance and the structural properties of the graph. This combination allows us to learn a comprehensive representation of each feature that incorporates both its importance (through GAT) and its structural context (through GIN).

The final output of the GNN encoder is a latent representation matrix \$Z \in \mathbb{R}^{n \times d}\$, where \$d\$ is the embedding dimension. The model is trained using the clean dataset, and the loss function used is Mean Squared Error (MSE) between the reconstructed feature values and the original values, ensuring that the learned embeddings accurately represent the clean data structure.

\subsection{Multi-Task Decoder for Data Quality Validation and Repair}

Our solution incorporates a multi-task learning framework to simultaneously perform data quality validation and data repair.

The multi-task decoder consists of two main components:

\begin{itemize}
\item \textbf{Data Quality Validation Decoder}: The first task is to validate the data quality by reconstructing the feature embeddings. Given the latent representation \$Z\$, we apply a reconstruction layer to predict the original feature values:
\begin{equation}
\hat{X}*{\text{validation}} = W*{\text{validation}} Z + b\_{\text{validation}},
\end{equation}
where \$W\_{\text{validation}}\$ and \$b\_{\text{validation}}\$ are learnable parameters. The reconstruction error \$L\_{\text{validation}}\$ is used to detect potential data quality issues.

```
\item \textbf{Data Repair Decoder}: The second task is to repair detected errors by generating corrected feature values. We use an additional MLP-based decoder to map the latent representation $Z$ to repaired feature values:
\begin{equation}
\hat{X}_{\text{repair}} = W_{\text{repair}} Z + b_{\text{repair}},
\end{equation}
where $W_{\text{repair}}$ and $b_{\text{repair}}$ are learnable parameters. The repair loss $L_{\text{repair}}$ is used to train the model to generate realistic corrected values for erroneous entries.
```

\end{itemize}

We define a combined loss function to optimize both tasks simultaneously:
\begin{equation}
L = \lambda\_{\text{validation}} L\_{\text{validation}} + \lambda\_{\text{repair}} L\_{\text{repair}},
\end{equation}
where \$\lambda\_{\text{validation}}\$ and \$\lambda\_{\text{repair}}\$ are hyperparameters that balance the contributions of the validation and repair tasks.

The model is trained using the Adam optimizer with a learning rate of \$0.01\$, and training is performed for 50 epochs. The training process involves feeding batches of clean data through the GNN encoder and multi-task decoder, and updating the parameters based on the combined loss function.

\subsection{Threshold-Based Anomaly Detection}

After training, we use the validation set to determine a threshold for detecting data quality issues. The reconstruction error for each instance in the validation set is computed as follows:
\begin{equation}
E\_i = | X\_i - \hat{X}*{\text{validation}, i} |^2,
\end{equation}
where \$X\_i\$ represents the original feature values and \$\hat{X}*{\text{validation}, i}\$ represents the reconstructed values. We set the anomaly threshold \$T\$ to the 95th percentile of the reconstruction errors in the validation set:
\begin{equation}
T = \text{quantile}*{0.95}(E*{\text{validation}}).
\end{equation}
Instances with reconstruction errors exceeding this threshold are flagged as potentially problematic.

\subsection{Data Quality Evaluation and Repair}

Once the model is trained and the quality threshold is established, we proceed to evaluate the quality of data in test sets and perform data repair where necessary. The evaluation and repair process involves the following steps:

The trained model is applied to two test sets: a clean test set (Test Set 1) and a dirty test set (Test Set 2). For each test instance, we compute the reconstruction error using the validation decoder:
\begin{equation}
E\_i = | X\_i - \hat{X}*{\text{validation}, i} |^2,
\end{equation}
where \$X\_i\$ represents the original feature values, and \$\hat{X}*{\text{validation}, i}\$ represents the reconstructed values. If the reconstruction error exceeds the threshold \$T\$, the instance is flagged as potentially problematic.

For instances flagged as problematic, we use the repair decoder to generate corrected feature values. The repaired data \$\hat{X}*{\text{repair}}\$ is computed as:
\begin{equation}
\hat{X}*{\text{repair}} = W\_{\text{repair}} Z + b\_{\text{repair}},
\end{equation}
where \$W\_{\text{repair}}\$ and \$b\_{\text{repair}}\$ are learnable parameters from the repair decoder. These corrected values are used to replace the erroneous entries in the dataset.

For each erroneous feature \$f\_j\$ of instance \$X\_i\$, the repaired value is given by:
\begin{equation}
\hat{x}*{ij} = \hat{X}*{\text{repair}, i, j},
\end{equation}
where \$\hat{x}\_{ij}\$ represents the repaired value for feature \$f\_j\$ in instance \$i\$. These repair suggestions are logged for further inspection and manual verification by domain experts if needed.

We evaluate the effectiveness of our approach using metrics such as accuracy of error detection, precision, recall, and the quality of the repair suggestions. The clean test set (Test Set 1) is used to verify the model's false positive rate, while the dirty test set (Test Set 2) is used to evaluate the true positive rate and repair accuracy.

Through these steps, our approach provides a comprehensive solution for not only detecting data quality issues but also suggesting and applying corrections, thereby improving the overall reliability of the dataset.