\section{OUR APPROACH: DQuaG}

\begin{figure*}[tb]
\centering
\includegraphics[width=0.75\textwidth]{Figures/framework_adqv.png}
\vspace{-0.6\baselineskip}
\caption{Data Quality Validation Framework Using GNN and VAE. Top: Training on clean data for Approach Establishment. Bottom: Validating unseen datasets by reconstruction error comparison.}
\vspace*{-0.3cm}
%\soror{highlight in the caption the process at the top and the one at the bottom of teh figure }
\label{fig:framework}
\end{figure*}

%In this section, we detail our novel approach to automated data quality validation using graph representation learning and a Variational Autoencoder (VAE) framework. Assuming we start with a clean dataset, our method addresses the limitations of traditional data quality verification techniques through a series of steps designed to capture intrinsic relationships within tabular data and assess data quality with minimal expert intervention.


In this section, we present DQuaG (Data Quality Graph), a novel approach for data quality validation. 
Figure~\ref{fig:framework} illustrates the framework of our approach, which includes both the training process using a clean dataset to train the GNN and VAE, and the data quality validation process using these trained models. 
%\qw{For the training phase, }
%\qw{For the validation phase, }

Note that DQuaG requires a clean dataset to train its models, which is a common assumption when embracing VAE in relevant problems.
The clean dataset serves as the foundational benchmark for our model, providing a reference state of high data quality against which data errors are identified. 
%This dataset is not only used to train the GNN and VAE models, but also to define a normative baseline for what constitutes acceptable data quality.

% \subsection{Training GNN and VAE on Clean Data for Data Quality Validation}
\subsection{Training GNN and VAE on Clean Data}
\subsubsection{\textbf{Feature Graph Construction}}

The initial step in our approach involves constructing a feature graph from clean tabular data to capture intrinsic relationships and dependencies between data features.
First, we address the challenge of diverse data types: categorical variables are transformed using label encoding, and timestamp data is broken into components (i.e., day, month, year). 
This uniform input format is critical for graph-based processing.

We then use ChatGPT-4~\cite{openai2024gpt4} to automate the feature graph construction. 
Given a clean dataset, we extract the feature names \( F \) and their descriptions \( D \) from the data source. We then randomly sample 100 data points from the dataset, denoted as \( S \). These feature names, descriptions, and sample data points are provided to the ChatGPT-4, structured as follows: \(\text{Input} = \{ F, D, S \}\), then ChatGPT-4 generates a JSON file capturing feature relationships.
The output format is \(\text{Feature\_Relationships} = \{ (f_i, f_j) \mid f_i, f_j \in F \}\), indicating that there is a relationship between features \( f_i \) and \( f_j \).

Using these relationships, we construct the knowledge-based feature graph \( G = (V, E) \), where \( V \) represents features and \( E \) represents edges indicating relationships between features.

% \subsubsection{\textbf{Feature Graph Construction}}

% The initial step in our methodology involves constructing a feature graph from clean tabular data, which is essential for capturing the intrinsic relationships and dependencies between different data features.

% To facilitate this process, our approach first addresses the challenge of handling diverse data types. 
% In the preprocessing stage, categorical variables are transformed using label encoding, which assigns each unique category a unique integer based on alphabetical ordering. 
% For timestamp data, we extract significant components such as day, month, and year. 
% This uniform input format is critical for the subsequent graph-based processing. 

% Following the preprocessing, we utilize a large language model, ChatGPT-4 \cite{openai2024gpt4}, to automate the construction of the feature graph. This integration allows for a more nuanced capture of feature relationships and dependencies, reducing reliance on expert knowledge and manual effort.

% Given a clean dataset, we extract the feature names \( F = \{f_1, f_2, \ldots, \\f_n\} \) and their descriptions \( D = \{d_1, d_2, \ldots, d_n\} \) from the data source. We then randomly sample 100 data points from the dataset, denoted as \( S = \{s_1, s_2, \ldots, s_{100}\} \). These feature names, descriptions, and sample data points are provided to the LLM, structured as follows: \(\text{Input} = \{ F, D, S \}\).

% The LLM analyzes the provided input and generates a structured JSON file capturing the relationships between features. The output format is \(\text{Feature\_Relationships} = \{ (f_i, f_j) \mid f_i, f_j \in F \}\), indicating that there is a relationship between features \( f_i \) and \( f_j \).

% Using the relationships provided by the LLM, we construct the feature graph \( G = (V, E) \) where \( V = F \) (nodes representing features) and \( E = \{(f_i, f_j) \mid (f_i, f_j) \in \text{Feature\_Relationships} \} \) (edges representing relationships). 

%----------------------------
%This graph-based representation allows us to model complex interdependencies within the data that are often overlooked by traditional methods, enhancing our ability to perform thorough data quality assessments.



% \subsubsection{\textbf{Training the Graph Neural Network (GNN) and Representing the Clean Dataset}}
\subsubsection{\textbf{Training GNN and preparing training data for VAE}}

Once the feature graph is constructed, we train a Graph Convolutional Network (GCN)~\cite{zhang2019graph} to generate feature embeddings. 
The GCN processes both the feature graph \( G = (V, E) \) and the original tabular data. Assume each instance in the original tabular data is an \( n \)-dimensional tuple \( \mathbf{x} \in \mathbb{R}^n \).

The GCN leverages the feature graph to learn the intrinsic relationships between features and produces embeddings that reflect the underlying structure of the clean data. Specifically, for each instance \( \mathbf{x} \), the GCN generates an \( n \)-dimensional embedding \( \mathbf{z} \in \mathbb{R}^n \). 
%This embedding \( \mathbf{z} \) captures the information from each feature's value and incorporates the relationships between features as learned from the feature graph.
% We train the GCN using the Adam optimizer, which is well-suited for handling graph-based data. The loss function used during training is the mean-squared error (MSE) between the predicted and true values for a set of labeled data. 
Formally, let \( \mathbf{Z} \) be the matrix of embeddings, where each row \( \mathbf{z}_i \) corresponds to an instance \( \mathbf{x}_i \). 
The GCN updates the embeddings by aggregating information from neighboring nodes in the feature graph, ensuring that the final embedding \( \mathbf{z}_i \) incorporates both the feature values and the relationships between features.

%These embeddings \( \mathbf{Z} \) serve as a compact and informative representation of the data's quality attributes, providing a robust basis for subsequent data quality assessment.


% \subsubsection{\textbf{Training the VAE for Encoding and Decoding}}
\subsubsection{\textbf{Training the VAE}}
%\qw{the input of VAE is composed by both the original data and the GNN output, i.e., move the above embedding part here}
The feature embeddings \( \mathbf{Z} \) generated by the GNN are then used to train a Variational Autoencoder (VAE). 
The VAE consists of an encoder and a decoder. The encoder maps the embeddings \( \mathbf{z} \) into a latent space, and the decoder reconstructs the embeddings back to their original feature space. This training is performed using the embeddings from the clean data, allowing the VAE to learn a probabilistic model of the normal data distribution.

\subsubsection{\textbf{Collecting the statistics of reconstruction errors}}
During training, we record the reconstruction error for each instance. The reconstruction error is essentially the loss for each instance. This results in a list of reconstruction errors, \(\mathcal{E}\). Given that even cleaned datasets may contain undetected errors, we do not set the maximum reconstruction error as the threshold for identifying problematic instances. Instead, we set the threshold at the 95th percentile of \(\mathcal{E}\), denoted as \( e_{clean} \). Instances with reconstruction errors above \( e_{clean} \) are flagged as potentially problematic.

%\qw{adjsut this paragraph}
%This process ensures that the VAE effectively learns the characteristics of the clean data while providing a robust method for detecting deviations from the norm in new datasets.


\subsection{Data Quality Validation Process}

\noindent{\textbf{Detecting Data Quality Issues by Reconstruction Errors}.}
With the GNN and VAE trained on the embeddings of the clean data, we proceed to assess the quality of new, unseen datasets. These unseen datasets must keep the same schema as the original clean dataset. The process involves several steps.
First, we generate embeddings for the new dataset using the trained GNN. Let \( \mathbf{Z}_{\text{new}} \) be the embeddings of the new dataset instances. These embeddings are then input into the trained VAE to obtain a list of reconstruction errors, denoted as \(\mathcal{E}_{\text{new}}\).
Next, we compare each reconstruction error in \(\mathcal{E}_{\text{new}}\) with the threshold \( e_{clean} \) from the clean dataset. 
We calculate the proportion of instances in the new dataset with reconstruction errors exceeding \( e_{clean} \), denoted as \( R_{error} \). 
Since the threshold was set at the 95th percentile for the clean dataset, we expect around 5\% of clean data instances to exceed this value. 

To account for data variability, if \( R_{error} \) exceeds \( 5\% \times n \), we classify the new dataset as problematic. This means if more than \( 5n\% \) of instances in the new dataset have errors greater than \( e_{clean} \), we will report the dataset has data quality issues. The parameter \( n \) can be adjusted based on observed reconstruction errors after deployment.
In our experiments, we set \( n = 1.2 \), which exhibited good performance.
Finally, we report the indices of all instances in the new dataset with reconstruction errors above \( e_{clean} \), clearly identifying problematic samples.

% Next, we compare each reconstruction error in \(\mathcal{E}_{\text{new}}\) with the previously determined threshold \( e_{cleaned} \). We calculate the proportion of instances in the new dataset that have reconstruction errors exceeding \( e_{cleaned} \), denoted as \( R_{error} \). Since the threshold was set at the 95th percentile for the clean dataset, it is expected that approximately 5\% of the clean dataset instances would have reconstruction errors above this threshold.
% To account for data variability, if \( R_{error} \) exceeds \( 5\% \times n = 5n\% \), we classify the new dataset as problematic. This means that if more than 5n\% of the instances in the new dataset have reconstruction errors greater than \( e_{cleaned} \), the dataset is flagged as having data quality issues.

% Finally, we report the indices of all instances in the new dataset that have reconstruction errors above \( e_{cleaned} \), providing a clear indication of which specific samples are problematic.

%This process allows us to effectively detect data quality issues in new datasets, capturing both explicit errors and subtle inconsistencies that traditional approaches may miss.


\noindent{\textbf{Detecting Feature Errors}.}
Each instance's reconstruction error \( e \) is a list corresponding to each feature's loss. To identify specific problematic features, we detect outliers with significantly higher reconstruction errors.
For an instance \( \mathbf{x}_i \), let \( \mathbf{e}_i = [e_{i1}, e_{i2}, \ldots, e_{in}] \) be the reconstruction errors for the \( n \) features. We calculate the mean \( \mu_i \) and standard deviation \( \sigma_i \) of the errors. Features with errors greater than \( \mu_i + 5\sigma_i \) are flagged as problematic.

%By reporting these outlier features, we can pinpoint which specific parts of an instance contribute most to data quality issues. 
This drill-down process helps identify exact feature-level problems within instances, facilitating targeted data cleaning.


%Our approach offers several key advantages over traditional data quality verification methods. By leveraging GNNs and VAEs, it automatically identifies data quality issues without predefined constraints and detects hidden relationships within the data. This reduces the need for continuous expert input, making the process more efficient and scalable. Additionally, it can pinpoint problematic samples and specific features, facilitating targeted data cleaning and correction.




