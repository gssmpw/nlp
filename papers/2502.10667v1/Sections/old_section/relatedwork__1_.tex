\section{RELATED WORK}
\label{sec:relatedwork}
Previous work in data quality research focuses on calculating data quality dimensions and statistics to evaluate data quality. Fundamental components of this research include data profiling and assessment.
% Data profiling generates metadata about data sources, capturing attributes such as data types, missing values, and distinct values. 
% This metadata is crucial for applications like data cleansing and integration~\cite{DBLP:journals/sigmod/Naumann13, oliveira2006data, abedjan2017data}. 
Profiling helps identify data quality issues, such as missing values and syntax violations~\cite{chiang2008discovering, ilyas2015trends, razniewski2011completeness}. Data quality assessment then uses these profiling outputs to measure dimensions like accuracy, completeness, and consistency~\cite{DBLP:journals/jmis/WangS96, sidi2012data}, ensuring datasets adhere to quality standards for various applications~\cite{jayawardene2013curse, jayawardene2015analysis, YEGANEH201424}.

% Data quality research ensures datasets meet specific benchmarks for accurate and reliable analysis. 
% Fundamental components include data profiling and assessment. 
% Data profiling generates metadata about data sources, capturing attributes like data types, missing values, and distinct values, crucial for applications such as data cleansing and integration~\cite{DBLP:journals/sigmod/Naumann13, oliveira2006data, abedjan2017data}. Profiling helps identify data quality issues, such as missing values and syntax violations~\cite{chiang2008discovering, ilyas2015trends, razniewski2011completeness}. 
% Data quality assessment uses profiling outputs to measure dimensions like accuracy, completeness, and consistency~\cite{DBLP:journals/jmis/WangS96, sidi2012data}, ensuring datasets adhere to quality standards for various applications~\cite{jayawardene2013curse, jayawardene2015analysis, YEGANEH201424}.

% Data quality is a critical area of research that focuses on ensuring datasets meet specific quality benchmarks to support accurate and reliable data analysis. Data quality profiling and assessment are fundamental components of this field. Data profiling generates metadata about data sources, capturing attributes such as data types, missing values, and distinct values. This metadata is essential for various applications, including data cleansing, data integration, and query optimization \cite{DBLP:journals/sigmod/Naumann13, oliveira2006data, abedjan2017data}. Profiling helps identify data quality issues, such as missing values, syntax violations, and referential integrity problems \cite{chiang2008discovering, ilyas2015trends, razniewski2011completeness}. The subsequent data quality assessment uses these profiling outputs to measure data quality dimensions like accuracy, completeness, and consistency, categorized into intrinsic, accessibility, contextual, and representational qualities \cite{DBLP:journals/jmis/WangS96, sidi2012data}. This assessment ensures that datasets adhere to quality standards, facilitating their effective use in various applications \cite{jayawardene2013curse, jayawardene2015analysis, YEGANEH201424}.

In ML pipelines, addressing quality issues has gained interest. Research has studied the relationship between data quality and ML performance, the impact of data quality dimensions on ML algorithms \cite{Budach2022TheEO}, their link to MLOps \cite{renggli2021data,Xin2021ProductionML}, and cleaning for ML \cite{DBLP:journals/corr/abs-1904-09483, krishnan2016activeclean}. The quality of ML models is related to the quality of the underlying data, leading to approaches for data preparation \cite{gupta2021data, Foroni2021EstimatingTE}, focusing on data completeness \cite{karlavs2020nearest, schelter2020learning} and data drift \cite{tahmasbi2020driftsurf,concept-drift-96}.
% Addressing quality issues in ML pipelines is a field with growing interest in recent years. Several research works studied the relationship between data quality and ML-model performance, including the impact of data quality dimensions on ML algorithms \cite{Budach2022TheEO}, their link to MLOps \cite{renggli2021data,Xin2021ProductionML}, cleaning for ML \cite{DBLP:journals/corr/abs-1904-09483, krishnan2016activeclean}. The quality of an ML model is often related to the quality of the underlying data, namely training and serving data. This has led to proposed approaches for data preparation, like assessing data across various quality dimensions \cite{gupta2021data} and task-dependant quality assessment \cite{Foroni2021EstimatingTE}. 
% Several approaches focused on the completeness of data \cite{karlavs2020nearest, schelter2020learning}. 
% Others focus on the data drift between training and serving data~\cite{tahmasbi2020driftsurf,concept-drift-96}.

Data quality verification is essential for ensuring datasets meet predefined quality standards before use in analysis or model training. 
Our method employs a feature graph~\cite{li2024graph} and Variational Autoencoder (VAE)~\cite{doersch2016tutorial} to detect hidden errors and automatically report specific problematic samples. 
While some studies use GNN and other ML models for data imputation~\cite{telyatnikov2023egg,cappuzzo2024relational,spinelli2020missing}, anomaly detection~\cite{du2022graph}, and Fraud Detection~\cite{rao2020xfraud,liu2021pick}, these methods are not being used for data quality validation. 
Most data quality validation work uses statistical methods or trains ML models with statistical information to detect data quality issues.
Fadlallah and Sinthong~\cite{fadlallah2023context,sinthong2021dqdf} emphasize context awareness and data-quality-aware dataframes but rely on predefined rules requiring expert intervention. Schelter et al.~\cite{schelter2018automating, schelter2019differential, caveness2020tensorflow} automate data quality verification in large-scale scenarios but struggle with hidden data relationships, they also used some statistic or simple ML methods to automatically generate constraints, but require expert fine-tuning. Shrivastava~\cite{shrivastava2019dqa} introduces an interactive advisor for scalable data quality but still relies heavily on expert-defined constraints. Redyuk~\cite{redyuk2021automating} trained the k-nearest neighbors (KNN)~\cite{peterson2009k} model to judge data quality using limited metrics, failing to detect many hidden errors and pinpoint specific problematic samples. %Bertino and Li~\cite{bertino2018adaptive,li2022blindly} focus on visual data quality with techniques not easily generalizable to other image datasets.

% Data quality verification is essential for ensuring that datasets meet predefined quality standards before they are used for analysis or model training. 
% Our method employs a feature graph~\cite{} and Variational Autoencoder (VAE)~\cite{} to detect hidden errors deeply and automatically report specific samples with issues. 
% While some studies use GNN and other ML models for data imputation~\cite{} and anomaly detection~\cite{}, these methods are not applied to data quality validation.

% Most data quality validation work uses statistical methods to verify data quality or use statistical information to train ML models for detecting data quality problems.
% \cite{fadlallah2023context,sinthong2021dqdf} 
% emphasize context awareness and data-quality-aware dataframes, yet they often depend on predefined rules and constraints that require expert intervention. 
% \cite{schelter2018automating, schelter2019differential, caveness2020tensorflow} explore automating data quality verification in large-scale scenarios but struggle with detecting hidden data relationships, they also used some statistic or simple ML methods to automatically generate constrains, but they constrains is not accuracy, need expert continue to fine-turn. 
% \cite{shrivastava2019dqa} introduces an interactive advisor for scalable data quality, yet it still relies heavily on expert-defined constraints. 
% \cite{redyuk2021automating} trained a k-nearest neighbors (KNN)~\cite{} model to judge data quality by computing limited quality metrics, which fails to detect many hidden errors, and their method cannot pinpoint which specific sample is incorrect.
% \cite{bertino2018adaptive,li2022blindly} focus on visual data quality, presenting techniques that are not easily generalizable to other image datasets. 
% for examples,

% Various studies have addressed different aspects of this topic. 

%Our method employs a feature graph and VAE to deeply detect some hidden errors, and it is fully automatic, capable of reporting which specific samples have issues.add some work using GNN and other Ml models to do the data imputation and anomaly detection, but there is no use for data quality validation, most of the work use statistic method of calculate statistic information the use statistic information to train a ml model to detect data quality problem, but these method just use not too much metrics to evaluate the quality, may lost some hidden error, and they can not report which sample have problem, because they calculate the metric for each feature, so they can not find which sample have problem, and these work also used ML model to detect the contains in the dataset. but not accuracy. need expert to fine-turn.

% Data quality verification is essential for ensuring that datasets meet predefined quality standards before they are used for analysis or model training. Various studies have addressed different aspects of this topic. 
% \cite{fadlallah2023context,caveness2020tensorflow,sinthong2021dqdf} 
% emphasize context awareness, continuous ML pipelines, and data-quality-aware dataframes, yet they often depend on predefined rules and constraints that require expert intervention. Gu et al. \cite{bertino2018adaptive} and Li et al. \cite{li2022blindly} focus on visual data quality, presenting techniques that are not easily generalizable to other datasets. Schelter and colleagues \cite{schelter2018automating, schelter2019differential} explore automating data quality verification in large-scale scenarios but struggle with detecting hidden data relationships. Shrivastava et al. \cite{shrivastava2019dqa} introduce an interactive advisor for scalable data quality, yet it still relies heavily on expert-defined rules. These methods, while insightful, often fall short of providing fully automated, comprehensive solutions adaptable to various data types and scales without extensive expert input.



%Fadlallah et al. \cite{fadlallah2023context}, Caveness et al. \cite{caveness2020tensorflow}, and Sinthong et al. \cite{sinthong2021dqdf} 