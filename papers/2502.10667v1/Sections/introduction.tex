\section{Introduction}
In the era of artificial intelligence and large models, ensuring data quality is crucial. High-quality data is essential for reliable decision-making, particularly in machine learning, where data quality directly impacts model performance. This paper addresses data quality validation, focusing on innovative approaches that overcome the limitations of existing methods.

\noindent\textbf{Background:}  
Quality validation is important to ensure that data meets specific requirements. Its main steps start
with data profiling~\cite{abedjan2017data}, generating metadata to describe quality like missing values and data dependencies~\cite{maydan1991efficient,naumann2014data}. 
This metadata forms the basis for assessing data quality against established metrics. 
However, this often relies on expert-defined constraints, which are resource-intensive and time-consuming to develop, maintain, and adjust.

Automated constraint generation~\cite{schelter2018automating,caveness2020tensorflow} attempts to learn from dataset characteristics, but these constraints are often incomplete or either too strict or too soft.
Strict constraints can lead to false positives, such as flagging acceptable minor data entry variations as errors. While soft 
constraints may miss critical issues, such as overlooking small discrepancies in numerical data. They need expert intervention for fine-tuning, which limits practical effectiveness.

Furthermore, these automated constraint generation methods often fail to detect hidden 
relationships and dependencies within the data, which we refer to as hidden errors. 
Indeed, data quality involves not only individual data points but also the complex interrelationships among them. 

\begin{figure}[tb]
\centering
\includegraphics[width=0.95\linewidth]{Figures/motivation.pdf}
%\vspace{-0.6\baselineskip}
\caption{Examples of Data Errors in tabular data: anomalies, typos, and conflicts between attributes.}
\label{fig:motivation}
\end{figure}

\noindent \textbf{Motivation:} 
To motivate our work, consider a dirty tabular dataset with four data quality errors that require verification. 
Figure~\ref{fig:motivation} shows common data errors: numeric anomalies, typos, and attribute mismatches. Tuple 1 includes a numeric anomalies value in the Age field, and tuple 2 has a typo in the Occupation field. 
Existing data quality validation systems, such as Amazon Deequ~\cite{schelter2018automating} and Google TFDV~\cite{caveness2020tensorflow}, can automatically generate constraints (e.g., the \textit{age} should be less than 100) to detect errors in tuples 1 and 2. 
However, these constraints are often difficult to set accurately, as valid cases may exist where age exceeds 100. In tuple 1, being employed as a truck driver at that age is highly unlikely and should be flagged as an error. Traditional systems often set such constraints either too strictly or too leniently, resulting in false positives or missed errors.
Tuple 3 contains a simple dependency issue, such as a city listed as \textit{Toronto} while the country is listed as the \textit{USA}. This type of error can often be addressed through expert-defined constraints. 
However, tuple 4 involves deeper dependencies, which is an illogical combination of age and income, referred to as hidden errors. Although data profiling~\cite{maydan1991efficient,naumann2014data,abedjan2017data} methods can discover such dependencies, existing automated data quality validation approaches only rely on expert-based strategies to identify such dependencies. 
As a result, they frequently overlook errors like those in tuple 4.

% Our goal is to not only accurately detect ordinary errors like missing data and typos, but also be able to automatically identify hidden errors (e.g., tuple 4), without using expert input. Additionally, we want to provide suggestions for repairing the identified issues.

Our goal is to detect challenging errors, such as hidden errors (e.g., tuple 4), without relying on expert input, while also detecting ordinary errors like missing data and typos. Additionally, we provide suggestions for repairing the identified errors.


\noindent \textbf{Challenges:} 
Addressing the limitations of existing methods involves several challenges.
First, obtaining expert input for defining constraints is costly and often impractical, making an automated approach essential.
Second, existing methods struggle to detect subtle issues embedded within complex feature relationships, which we refer to as hidden data quality errors.
% Finally, it must accurately detect problematic samples and identify which feature of the sample is causing the issue. 
Third, the approach must be capable of identifying which specific features of a sample are contributing to the data quality issue, ensuring a precise assessment of problematic areas. 
Lastly, the model should provide suggestions to repair problematic features, while maintaining consistency with the overall clean data distribution.

\noindent \textbf{Our Solution:} To address these challenges, we propose DQuag, a multi-task learning framework, which integrates a GNN encoder~\cite{li2024graph} for feature embedding and dual decoders for data quality validation and repair. 
This combination leverages graph-based learning and multi-task optimization to detect and repair data quality issues.

The method includes the following key steps in the training phase: 
First, we construct a feature graph from clean tabular data to capture relationships between features. 
This graph representation enables the model to leverage relational dependencies inherent in the data. 
Next, a GNN encoder generates feature embeddings that encapsulate these relationships. 
The generated feature embeddings are then passed to two separate decoders. 
The Data Quality Validation Decoder reconstructs the original data from the feature embeddings, aiming to maximize the reconstruction accuracy for clean data and identify errors based on reconstruction loss. 
The Data Repair Decoder generates corrected values for problematic features. 
These decoders are optimized with distinct loss functions.

During the verification phase, we apply the trained model to unseen
datasets and use reconstruction errors to measure data quality, with
large errors indicating potential data quality issues. 
The repair decoder then suggests corrections for these features.
%, reducing the need for manual intervention. 
This approach detects both explicit and underlying data quality problems, reducing the need for expert-defined constraints and manual intervention. 
%And we can suggest corrections for errors.

\noindent\textbf{Contributions:}  
This paper makes three key contributions. 
First, we introduce a novel advanced data quality validation and repair framework based on graph neural networks and multi-task learning, eliminating the need for expert-defined constraints. 
Second, we develop a novel dual-decoder architecture to address two critical tasks: data quality validation and data repair. 
The Data Quality Validation Decoder is designed to highlight potential quality issues, while the Data Repair Decoder provides correction suggestions.
Finally, we validate our approach through comprehensive and extensive experiments, thoroughly demonstrating its effectiveness and adaptability across various scenarios.



% \noindent \textbf{Our Solution:} To address these challenges, we propose a novel approach using graph representation learning~\cite{li2024graph} and a VAE~\cite{doersch2016tutorial}. 
% These techniques are effective because they have been successfully used in mining latent correlations and uncovering hidden patterns within complex datasets~\cite{telyatnikov2023egg, cappuzzo2024relational, spinelli2020missing, du2022graph}.


% Our method has four main steps in the training phase: First, we construct a feature graph from clean tabular data to capture relationships between different features. 
% Second, we train a GNN to generate feature embeddings that reflect intrinsic relationships within the clean data.
% Third, these GNN-generated embeddings are then used to train a VAE, which encodes them into a latent space and decodes them back, enabling the VAE to learn how to reconstruct clean data.  
% Finally, we collect the statistics of reconstruction errors to set the threshold for identifying instances with errors.
% During the verification phase: 
% We apply the trained VAE and GNN to unseen datasets and use reconstruction errors to measure data quality, with large errors indicating potential data quality issues. 
% This approach detects both explicit and underlying data quality problems, reducing the need for expert-defined constraints and manual intervention.


% \noindent\textbf{Contributions:} 
% This paper makes three key contributions. 
% First, we develop a VAE-based model that identifies and locates problematic data samples and features using reconstruction errors, eliminating the need for expert-defined constraints. 
% Second, we propose a quality validation approach that employs feature graphs and graph representation learning to detect hidden errors in tabular datasets. 
% Finally, we validate our approach through extensive experiments, demonstrating its effectiveness across various scenarios.




