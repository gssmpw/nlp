\section{Related Works}
%\vspace{-0.1in}
LLM compression\cite{wang2024modelsurvey}\cite{zhu2023survey} devotes to reducing the memory and computational cost of the model in the inference process. Quantization and Pruning are two typical compression methods for models\cite{deepcompression}.
%Common model compression methods for LLM mainly include quantization, pruning, and knowledge distillation.

Quantization reduces model size by converting floating point numbers to low-precision integers or other discrete forms. \cite{gptq} \cite{awq}\cite{llmint8} focus on weight quantization, while\cite{smoothquant} \cite{zeroquant} \cite{os+} focus on weight and activation quantization. These quantization works research how to reduce quantization error, and the compression ratio is fixed. 
%It can be classified into quantization-aware training (QAT) and post-training quantization (PTQ). 

%The QAT method preserves higher performance while leading to higher quant computation difficulty. More approaches focus on PTQ methods, including only weight quantization and weight/activation quantization. LLM.int8() employs 8-bit vector-wise quantization and mixed INT8/FP16 precision to address the activation outliers. GPTQ\cite{gptq} proposes a layer-wise 3/4 bits quantization based on approximate second-order information. AWQ\cite{awq} finds that weights have different importance for LLMsâ€™ performance and proposes to keep 1\% salient weights  to minimize quantization errors. 

%Except for these works, which only quantize weights, some focus on weight-activation quantization. ZeroQuant uses a token-wise quantization for activation based on the dynamic activation range and a group-wise quantization scheme for weight. SmoothQuant\cite{smoothquant} presents that across-channel outliers of activation cause high quantization error. It introduces a per-channel scaling transformation. The scaling method can effectively smooth the activation magnitudes, and render the model more amenable to quantization. Outlier Suppression+\cite{os+} extends this understanding of activation outliers, which exhibit asymmetric distribution on specific channels. 

Pruning reduces model parameter size by removing unnecessary or redundant components. \cite{llmpruner}\cite{gum} proposes structured pruning methods, which remove units based on specific rules. \cite{sparsegpt}\cite{wanda}\cite{prunerzero} aims at unstructured pruning methods. 

In this paper, we propose to compress quantized/pruned LLM further.
%Pruning methods for LLMs can be divided into two types: structured pruning and unstructured pruning. Structured pruning removes connections or hierarchical structures based on specific rules while preserving the overall network structure. On the other hand, unstructured pruning prunes individual parameters, resulting in an irregular sparse structure. SparseGPT, LoRAPrune, Wanda, GUM, LLM-Pruner. 

%However, the increased model parameter sizes remain challenging for the limited-resource devices.