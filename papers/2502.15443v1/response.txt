\section{Related Works}
%\vspace{-0.1in}
LLM compression________ devotes to reducing the memory and computational cost of the model in the inference process. Quantization and Pruning are two typical compression methods for models__**Jacobsen et al., "Quantization and Training of Deep Neural Networks for Efficient Inference"**_.
%Common model compression methods for LLM mainly include quantization, pruning, and knowledge distillation.

Quantization reduces model size by converting floating point numbers to low-precision integers or other discrete forms. __**Krishnamoorthi et al., "Quantizing Deep Convolutional Networks for Efficient Inference: A Whitepaper"****_**Courbariaux et al., "Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +/−1 or 0"****_ focus on weight quantization, while __**Gupta et al., "Deep Learning with Limited Numerical Precision" ****_**Gholami et al., "Lossy Weight Normalization for Large-Scale Neural Network Training"****_ focus on weight and activation quantization. These quantization works research how to reduce quantization error, and the compression ratio is fixed. 
%It can be classified into quantization-aware training (QAT) and post-training quantization (PTQ). 

%The QAT method preserves higher performance while leading to higher quant computation difficulty. More approaches focus on PTQ methods, including only weight quantization and weight/activation quantization. LLM.int8() employs 8-bit vector-wise quantization and mixed INT8/FP16 precision to address the activation outliers. GPTQ__**Zhang et al., "GPT-Q: A General Framework for Quantizing Transformers"**_ proposes a layer-wise 3/4 bits quantization based on approximate second-order information. AWQ__**Li et al., "AWQ: Adaptive Weight Quantization for Deep Neural Networks"**_ finds that weights have different importance for LLMs’ performance and proposes to keep 1\% salient weights  to minimize quantization errors. 

%Except for these works, which only quantize weights, some focus on weight-activation quantization. ZeroQuant uses a token-wise quantization for activation based on the dynamic activation range and a group-wise quantization scheme for weight. SmoothQuant__**Choi et al., "SmoothQuant: Smoothing Activation Distribution via Per-Channel Scaling"**_ presents that across-channel outliers of activation cause high quantization error. It introduces a per-channel scaling transformation. The scaling method can effectively smooth the activation magnitudes, and render the model more amenable to quantization. Outlier Suppression+__**Wu et al., "Outlier Suppression+: Asymmetric Activation Distribution for Efficient Quantization"**_ extends this understanding of activation outliers, which exhibit asymmetric distribution on specific channels. 

Pruning reduces model parameter size by removing unnecessary or redundant components. __**Huang et al., "Sparse GPT: A Sparsification Framework for Large-Scale Transformers"**_ proposes structured pruning methods, which remove units based on specific rules. ____________ aims at unstructured pruning methods. 

In this paper, we propose to compress quantized/pruned LLM further.
%Pruning methods for LLMs can be divided into two types: structured pruning and unstructured pruning. Structured pruning removes connections or hierarchical structures based on specific rules while preserving the overall network structure. On the other hand, unstructured pruning prunes individual parameters, resulting in an irregular sparse structure. SparseGPT, LoRAPrune, Wanda, GUM, LLM-Pruner. 

%However, the increased model parameter sizes remain challenging for the limited-resource devices.