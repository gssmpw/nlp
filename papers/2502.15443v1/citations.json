[
  {
    "index": 0,
    "papers": [
      {
        "key": "wang2024modelsurvey",
        "author": "Wang, Wenxiao and Chen, Wei and Luo, Yicong and Long, Yongliu and Lin, Zhengkai and Zhang, Liye and Lin, Binbin and Cai, Deng and He, Xiaofei",
        "title": "Model Compression and Efficient Inference for Large Language Models: A Survey"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "zhu2023survey",
        "author": "Zhu, Xunyu and Li, Jian and Liu, Yong and Ma, Can and Wang, Weiping",
        "title": "A survey on model compression for large language models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "deepcompression",
        "author": "Han, Song and Mao, Huizi and Dally, William J",
        "title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "gptq",
        "author": "Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan",
        "title": "Gptq: Accurate post-training quantization for generative pre-trained transformers"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "awq",
        "author": "Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and Dang, Xingyu and Gan, Chuang and Han, Song",
        "title": "Awq: Activation-aware weight quantization for llm compression and acceleration"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "llmint8",
        "author": "Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke",
        "title": "Llm. int8 (): 8-bit matrix multiplication for transformers at scale, 2022"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "smoothquant",
        "author": "Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song",
        "title": "Smoothquant: Accurate and efficient post-training quantization for large language models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "zeroquant",
        "author": "Yao, Zhewei and Yazdani Aminabadi, Reza and Zhang, Minjia and Wu, Xiaoxia and Li, Conglong and He, Yuxiong",
        "title": "Zeroquant: Efficient and affordable post-training quantization for large-scale transformers"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "os+",
        "author": "Wei, Xiuying and Zhang, Yunchen and Li, Yuhang and Zhang, Xiangguo and Gong, Ruihao and Guo, Jinyang and Liu, Xianglong",
        "title": "Outlier suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "gptq",
        "author": "Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan",
        "title": "Gptq: Accurate post-training quantization for generative pre-trained transformers"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "awq",
        "author": "Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and Dang, Xingyu and Gan, Chuang and Han, Song",
        "title": "Awq: Activation-aware weight quantization for llm compression and acceleration"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "smoothquant",
        "author": "Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song",
        "title": "Smoothquant: Accurate and efficient post-training quantization for large language models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "os+",
        "author": "Wei, Xiuying and Zhang, Yunchen and Li, Yuhang and Zhang, Xiangguo and Gong, Ruihao and Guo, Jinyang and Liu, Xianglong",
        "title": "Outlier suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "llmpruner",
        "author": "Ma, Xinyin and Fang, Gongfan and Wang, Xinchao",
        "title": "Llm-pruner: On the structural pruning of large language models"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "gum",
        "author": "Santacroce, Michael and Wen, Zixin and Shen, Yelong and Li, Yuanzhi",
        "title": "What matters in the structured pruning of generative language models?"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "sparsegpt",
        "author": "Frantar, Elias and Alistarh, Dan",
        "title": "Sparsegpt: Massive language models can be accurately pruned in one-shot"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "wanda",
        "author": "Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J Zico",
        "title": "A simple and effective pruning approach for large language models"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "prunerzero",
        "author": "Dong, Peijie and Li, Lujun and Tang, Zhenheng and Liu, Xiang and Pan, Xinglin and Wang, Qiang and Chu, Xiaowen",
        "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models"
      }
    ]
  }
]