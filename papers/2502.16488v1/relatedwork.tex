\section{Related Works}
\textbf{Deep learning on point clouds.} Existing deep learning based point cloud processing methods can be roughly divided into four folds: point based~\cite{lai2022stratified,jiang2020pointgroup,yan2020pointasnl,hu2020jsenet}, graph based~\cite{li2021pointvgg,ding2021graph,lei2020seggcn}, multi-view based~\cite{li2020end,xu2023multi,chen2020compositional,le2017multi} and voxelization based methods~\cite{malik2021handvoxnet++,meng2019vv,poux2019voxel}.
\begin{figure*}[t!]
	\vspace{-15pt}
	\centering
	\includegraphics[width=1.0\textwidth]{"images/framework_v5.pdf"}
	\caption{The pipeline of our geometry-aware 3D salient object detection network. Given a point cloud, we first use the 3D CNN backbone to extract point features. Then, we adopt the superpoint partition module to obtain superpoints. After that, we propose the geometry enhancement module to encode structural information into point clouds. In addition, we propose a point cloud class-agnostic loss $\mathcal{L}_{agn}$ to learn discriminative point features for improving superpoint quality.}
	\label{fig:framework}
	\vspace{-10pt}
\end{figure*}
Qi~\emph{et al.}~\cite{qi2017pointnet} introduced PointNet as a pioneering method for learning features on point clouds, which directly handles point clouds with multi-layer perceptron (MLP), max-pooling, and rigid transformations to achieve extracting features of point clouds. Following PointNet, the appearance of PointNet++~\cite{qi2017pointnet++} enhanced the ability to characterize the local geometric structures of point clouds. Subsequently, in order to extends the convolution operation from 2D images to 3D point cloud, Li~\emph{et al.}~\cite{li2018pointcnn} proposed a transformation to simultaneously weight and permute the input features in PointCNN. The graph-based methods~\cite{cheng2021sspc,cheng2020cascaded,shen2018mining} regard point cloud data as a graph structure, where points represent nodes and relationships between points represent edges. These methods utilizing techniques such as graph convolutional networks to analyze and process point clouds in object recognition and segmentation tasks. For instance, DGCNN~\cite{wang2019dynamic} is a commonly used graph-based method, which dynamically aggregate local geometric feature of point clouds. The methods based voxelization~\cite{choy20194d,liu2019point} divides point cloud data into regular voxels in space. Therefore, we can use the voxelized data as input to 3D convolutional networks for further processing and analysis. These networks are specifically designed to handle three-dimensional data, allowing us to extract meaningful features and patterns from the voxelized representation. Applying 3D convolutions can capture spatial relationships and learn hierarchical representations that are useful for various tasks such as object recognition, segmentation, and reconstruction. However, the sparsity of point cloud data often results in empty voxels, leading to the wastage of computational resources. The last multi-view based method~\cite{wu2019squeezesegv2,chen2020compositional} projects point clouds into different view spaces and utilizes these views as input to accomplish analysis and processing of point clouds. Although the above methods have made significant progress in 3D classification, segmentation, reconstruction, and generation tasks, their applicability to the 3D salient object detection task is not particularly strong.

\noindent\textbf{Point cloud salient object detection.} Point cloud salient object detection refers to accurately identifying and locating salient objects from point cloud data. Similar to salient object detection in images, it often serves as a valuable preprocessing step, providing better solutions for applications such as 3D scene understanding, object recognition, and robotic navigation. However, unlike traditional image-based salient object detection, salient object detection in point clouds requires consideration of factors such as spatial distribution, density, and shape of the points. On the other hand, point cloud data consists of a large number of irregularly distributed points, posing numerous challenges for salient object detection. In recent years, with the development of deep learning techniques, significant progress has been made in salient object detection in point clouds using neural network-based methods. In point cloud SOD, Fan~\emph{et al.}~\cite{fan2022salient} proposed PointSal is the pioneering work and it take a hand-labeled dataset PCSOD for advancing this important field. In this work, PointSal is a typical encoder-decoder architecture and design two important modules, $i.e.$, point perceptron block and saliency perception block, which local salient objects though taking full advantage of multi-scale features and global semantics. However, due to PointSal capture feature all by using MLPs, the capability of learning long-range feature representations of PointSal is very limted. Subsequently, Zhang~\emph{et al.}~\cite{zhang2023enhanced} proposed an enhanced point feature network (EPFNet) for point cloud SOD, which take full advantage of color information available in point cloud for point cloud SOD. As the dominant frameworks Transform in natural language processing are applied to point clouds, Wei ~\emph{et al.}~\cite{wei2024point} later proposed PSOD-Net, a model featuring two contextual transformer modules designed to effectively capture multi-scale contextual point information.

\noindent\textbf{Superpoint representation of point cloud.} Superpoints are similar to superpixels in 2D images, which refer to a collection of points within a point cloud that exhibit certain semantic and geometric similarities. Lin~\emph{et al.}~\cite{lin2018toward} proposed a method that segments superpoints by utilizing locally crafted information to minimize an energy function. Guinard~\emph{et al.}~\cite{guinard2017weakly} utilized handcrafted local descriptors to generate geometrically simple superpoints using a greedy graph-cut algorithm~\cite{landrieu2016cut}. Landrieu~\cite{landrieu2019point} proposed employing a deep network for extracting point embeddings instead of using handcrafted features %in ~\cite{landrieu2016cut}%
to segment superpoints. Hui~\emph{et al.}~\cite{hui2021superpoint} introduced an end-to-end superpoint framework, which iteratively learns the correlation mapping between individual points and superpoints for the purpose of clustering. To handle LiDAR point clouds, Hui~\emph{et al.}~\cite{hui2023efficient} propose an efficient point cloud oversegmentation network by applying clustering on the range image.