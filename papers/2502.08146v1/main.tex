\documentclass[12pt]{article}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{changepage}
\usepackage{microtype}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{array}
\usepackage{bm}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{authblk}
\usepackage{xcolor} 
\usepackage{colortbl}
\usepackage{makecell}
\include{macros}

\title{Knowledge-Guided Wasserstein Distributionally Robust Optimization}

\author[1]{Zitao Wang}
\author[2]{Ziyuan Wang}
\author[3]{Molei Liu\thanks{These authors contributed equally to this work.}}
\author[4]{Nian Si$^*$}

\affil[1]{Department of Statistics, Columbia University.}
\affil[2]{Department of Industrial Engineering and Management Sciences, Northwestern University.}
\affil[3]{Department of Biostatistics, Columbia Mailman School of Public Health.}
\affil[4]{Department of Industrial Engineering and Decision Analytics, Hong Kong University of Science and Technology.}

\date{}

\begin{document}

\setstretch{1.1} 
\maketitle

\begin{abstract}
Transfer learning is a popular strategy to leverage external knowledge and improve statistical efficiency, particularly with a limited target sample. We propose a novel knowledge-guided Wasserstein Distributionally Robust Optimization (KG-WDRO) framework that adaptively incorporates multiple sources of external knowledge to overcome the conservativeness of vanilla WDRO, which often results in overly pessimistic shrinkage toward zero. Our method constructs smaller Wasserstein ambiguity sets by controlling the transportation along directions informed by the source knowledge. This strategy can alleviate perturbations on the predictive projection of the covariates and protect against information loss. Theoretically, we establish the equivalence between our WDRO formulation and the knowledge-guided shrinkage estimation based on collinear similarity, ensuring tractability and geometrizing the feasible set. This also reveals a novel and general interpretation for recent shrinkage-based transfer learning approaches from the perspective of distributional robustness. In addition, our framework can adjust for scaling differences in the regression models between the source and target and accommodates general types of regularization such as lasso and ridge. Extensive simulations demonstrate the superior performance and adaptivity of KG-WDRO in enhancing small-sample transfer learning.

\vspace{0.3cm}
\noindent \textbf{Keywords:} Wasserstein distributionally robust optimization; Knowledge-guided learning; Difference-of-convex optimization; Shrinkage-based transfer learning.

\end{abstract}

\section{Introduction}
Traditional machine learning methods or empirical risk minimization often suffer from overfitting and a lack of generalization power, particularly in high-dimensional and small-sample-size settings. In recent years, distributionally robust optimization (DRO) has emerged as a powerful framework for mitigating the effects of model misspecification and enhancing robustness in machine learning generalizations. Among various DRO formulations, Wasserstein-DRO (WDRO) gained more attention due to its  tractability and generalizability. Specifically, in WDRO, one  optimizes over worst-case distributions within an ambiguity set defined by a Wasserstein ball centered at an empirical measure.  

However, one persistent challenge with WDRO is its tendency to be overly conservative, which can lead to suboptimal performance in practice as found in \cite{liu2024rethinkingdistributionshifts}. In many real-world scenarios, prior knowledge can be leveraged to improve model performance and robustness. For example, in electronic healthcare record data, prior knowledge might come from predictive models trained on existing large, population-wide datasets. In such a context, transfer learning has proven to be a versatile approach for  improving performance on a target task. Despite its successes, the integration of prior knowledge into WDRO frameworks has remained an open question.

In this work, we introduce Knowledge-Guided Wasserstein Distributionally Robust Optimization (KG-WDRO), a novel framework that adapts the Wasserstein ambiguity set using external knowledge (parameters). We assume access to prior predictors of pre-trained models, which can guide the predictive model in the target dataset. By constraining the transport cost along directions informed by prior knowledge, our approach addresses the conservativeness of vanilla WDRO while preserving robustness. Intuitively, this strategy allows the model to focus its uncertainty on regions where prior knowledge is less reliable, effectively robustify knowledge-guided generalization.

\subsection{Related Works}
\subsubsection{Wasserstein DRO}
Wasserstein DRO  has recently garnered significant attention due to its tractability \citep{blanchet2019quantifying, mohajerin2018data, gao2023distributionally} and generalizability \citep{blanchet2019rwpi, gao2022}. Notably, \citet{blanchet2019rwpi} and \citet{gao2022} demonstrate that Wasserstein DRO with mean square loss is equivalent to the square root lasso \cite{belloni2011sqrtlasso}. Similarly, \citet{abadeh2015drologistic, abadeh2019regularizationviamass, blanchet2019rwpi, gao2022} establish that Wasserstein DRO with logistic loss and hinge loss corresponds to their regularized counterparts. Moreover, the statistical properties of the WDRO estimator have also been investigated in \cite{blanchet2021statistical, blanchet2022confidence, gao2023finite}. However, leveraging external knowledge in Wasserstein DRO has been an open problem.

\subsubsection{Transfer Learning}\label{sec:review:tl}
Improving prediction accuracy for target populations by integrating diverse source datasets has driven methodological advances in transfer learning. Contemporary approaches aim to address challenges including distributional heterogeneity and limited labeled target data. A common assumption is that the target outcome model aligns partially with source models, enabling knowledge transfer. For example, recent frameworks employ selective parameter reduction to identify transferable sources and sparse or ridge shrinkage to leverage their knowledge \citep{bastani2020predicting,li2021translasso,tian2023transglm}. Subsequent works tackle covariate distribution mismatches and semi-supervised scenarios, enhancing robustness when labeled target data is scarce \citep{cai2022semi,he2024transfusion,zhou2024Model}. Further innovations include geometric or profile-based adaptations, where the target model is represented as a weighted combination of source coefficients \citep{gu2024angle-based,lin2024profiled}. 

\begin{table*}[!ht]
\caption{Overview of recent transfer learning techniques. Each column represents a key capability:  
\textbf{Ridge-type / Lasso-type} - Regularization type used;  
\textbf{Scale Adjustment} - Robustness against feature-wise scaling;  
\textbf{Continuous outcome / Binary outcome} - Supports regression or classification;  
\textbf{Partial Transfer} - Selections of prior knowledge;  
\textbf{Multi-Source ensemble} - Profiles on multiple prior knowledges.}
\label{tab:comparison}
\vskip 0.1in
\begin{center}
\begin{small}
\setlength{\tabcolsep}{3pt} % Reduce column padding
\renewcommand{\arraystretch}{1.1} % Adjust row height
\begin{tabular}{lccccccc}
\toprule
Methods & \makecell{Ridge \\ -type}  & \makecell{Lasso \\ -type} & \makecell{Scale \\ Adjustment} & \makecell{Continuous \\ outcome} & \makecell{Binary \\ outcome} & \makecell{Partial \\ Transfer} & \makecell{Multi-Source \\ ensemble} \\
\midrule
KG-WDRO   & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ &  $\checkmark$ & $\checkmark$& $\checkmark$\\
\citet{bastani2020predicting} & $\checkmark$ &  & & $\checkmark$ & & & \\
\citet{li2021translasso} & & $\checkmark$ &  & $\checkmark$ &   &  &  \\
\citet{tian2023transglm} &  & $\checkmark$ & & $\checkmark$ &  $\checkmark$ &  &  \\
\citet{gu2024angle-based} & $\checkmark$ &  & $\checkmark$ & $\checkmark$ &  &  & $\checkmark$ \\
\citet{lin2024profiled} &  & $\checkmark$ & $\checkmark$ & $\checkmark$ &   &  & $\checkmark$ \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\end{table*}

\subsection{Our Contribution}
Our contributions are fourfold. \textbf{Framework:} We introduce KG-WDRO, a principled and flexible framework that integrates prior knowledge into WDRO for linear regression and binary classification. This framework mitigates the conservativeness of standard WDRO, enables automated covariate scaling adjustments, and prevents negative transfer. \textbf{Theory:} We establish the equivalence between KG-WDRO and shrinkage-based estimation methods, offering a novel perspective that unifies and interprets a broad range of knowledge transfer learning approaches through the lens of distributional robustness. Table \ref{tab:comparison} provides an overview of them, highlighting their key capabilities and advantages and comparing them with our framework. \textbf{Technicalities:} Leveraging Toland's Duality (Theorem \ref{thm:toland}), we reformulate the innermost maximization in WDRO's strong duality (Proposition \ref{prop:duality}) into a univariate optimization problem \eqref{eqn:duality}. This reformulation enhances tractability while accommodating more general cost functions. \textbf{Empirical Validation:} Through extensive experiments, we demonstrate the effectiveness of KG-WDRO in improving small-sample transfer learning.

Below is an overview of our main results for the linear regression case.
\begin{example}
Suppose \(\theta\) is an accessible prior predictor for a linear model parameterized with $\beta$. We show that the shrinkage-based transfer-learning regression problem, which estimates a target predictor \(\beta\) by solving  
\[
\inf_{\beta,\kappa} \Vert \mathbf{y} - \mathbf{X}\beta\Vert_2 + \sqrt{\delta} \Vert \beta - \kappa \theta \Vert_p,
\]
can be interpreted as a Wasserstein distributionally robust optimization (WDRO) problem of the form \eqref{obj:wdro}, where the loss function is least squares,
$\ell(X, Y; \beta) = (Y - \beta\trans X)^2,$ and the ambiguity set \(\mathcal{B}_{\delta}(\mathbb{P}_N; c_{2,\infty})\) is defined as a ball around the empirical measure. The cost function \(c_{2,\infty}\) augments the standard transport cost by the constraint \(x\trans \theta = u\trans \theta\) so that
\begin{align*}
    c_{2,\infty}\big((x, y), (u, v)\big) 
   =  \Vert x - u \Vert_q^2 + \infty \cdot |y - v| + \infty \cdot |(x-u)\trans\theta|.
\end{align*}
This establishes a distributionally robust optimization (DRO) perspective on a broad class of transfer-learning methods as will be discussed in Section \ref{sec:kgdro}.  
\end{example}

\subsection{Notations \& Organizations}
We summarize the mathematical notations used in this work. The positive integers \(N\), \(M\), and \(d\) denote, respectively, the target sample size, the number of sources, and the dimension of the support of the covariate \(X\). The integers \(p\) and \(q \in [1, \infty]\) are reserved for pairs of H\"{o}lder conjugates, satisfying \(p^{-1} + q^{-1} = 1\) for \(p, q \in (1, \infty)\), as well as the pair $1$ and $\infty$. For a distribution \(\mathbb{P}\) supported on the Euclidean space \(\mathbb{R}^d\), we use \(\mathbb{P}_N\) to denote the empirical measure of \(\mathbb{P}\) with sample size \(N\). In modeling the target-covariate relationship, the distribution is often factorized as \(\mathbb{P} = \mathbb{P}^{Y|X} \times \mathbb{P}^X\). For a vector \(v \in \mathbb{R}^d\), \(\|v\|_p\) denotes the \(p\)-norm, where \(p \in [1, \infty]\), and $v\trans$ denote the transpose of $v$. For any two vectors $u,v \in \mathbb{R}^d$, the notation $\cos{(u,v)}$ denote the cosine of the angle between $u$ and $v$, calculated by $\cos{(u,v)}\Vert u\Vert_2\Vert v\Vert_2 = u\trans v$. All vectors are assumed to be column vectors. Other specialized notations are defined in context as needed.

The remainder of the paper is organized as follows. Section \ref{sec:prelim} provides a review of the WDRO framework, including the strong duality result. In Section \ref{sec:kgdro}, we introduce our KG-WDRO framework and demonstrate its equivalence to shrinkage-based estimations in both linear regression and binary classification. Section \ref{sec:numerical} presents comprehensive results from our numerical simulations. All proofs and detailed descriptions of the numerical simulation setups are provided in the appendix.

\section{Preliminaries}
\label{sec:prelim}
We first begin with a short overview of the distributionally robust framework on statistical learning.
\subsection{Optimal Transport Cost}
Let $\mathbb{P}$ and $\mathbb{Q}$ denote two probability distributions supported on $\mathbb{R}^d$, and we use $\mathcal{P}(\mathbb{R}^d\times \mathbb{R}^d)$ to label the set of all probability measures on the product space $\mathbb{R}^d\times \mathbb{R}^d$. We say that an element $\pi\in\mathcal{P}(\mathbb{R}^d\times \mathbb{R}^d)$ has first marginal $\mathbb{P}$ and second marginal $\mathbb{Q}$ if \[
\pi(A\times \mathbb{R}^d) = \mathbb{P}(A), \,\,\,\pi(\mathbb{R}^d\times B) =\mathbb{Q}(B),
\]for all Borel measurable sets $A,B\in \mathbb{R}^d$. The class of all such measures $\pi$ is collected as $\Pi(\mathbb{P},\mathbb{Q})$, and is called the set of \textit{transport plans}, which is always non-empty. Choose a non-negative, lower semi-continuous function $c:\mathbb{R}^d\times \mathbb{R}^d \to [0,\infty]$ such that $c(u,v) = 0$ whenever $u=v$, then the \textit{Kantorovich's formulation} of optimal transport is defined as \[
\mathcal{D}_c(\mathbb{P},\mathbb{Q}) \coloneqq \inf_{\pi\in \Pi(\mathbb{P},\mathbb{Q})}  \mathbb{E}_\pi \left[ c(U,V) \right].
\] It is well-known that \citep[Theorem 4.1]{villani2008ot} there exists an optimal coupling $\pi^\dagger$ that solves the Kantorovich's problem $\inf_{\pi\in \Pi(\mathbb{P},\mathbb{Q})}  \mathbb{E}_\pi \left[ c(U,V) \right]$. Intuitively, we may think of the value $c(u,v)$ as the cost of transferring one unit of mass from $u \in \mathbb{R}^d$ to $v\in \mathbb{R}^d$, then $\mathbb{E}_\pi[c(U,V)]$ gives the average cost of transferring under the plan $\pi$. The \textit{optimal transport cost} $\mathcal{D}_c(\mathbb{P},\mathbb{Q})$ gives a measure of discrepancy between probability distributions on $\mathbb{R}^d$.

If $c(u,v)$ defines a metric on $\mathbb{R}^d$, then for any $p\in [1,\infty)$ the optimal transport cost, \[
\mathcal{D}_c^{1/p}(\mathbb{P},\mathbb{Q}) \coloneqq \left(\inf_{\pi\in \Pi(\mathbb{P},\mathbb{Q})}  \mathbb{E}_\pi \left[ c(U,V)^p \right] \right)^{1/p},
\]defines a metric between probability distributions and metrizes weak convergence under moment assumptions. It is called the $p$-Wasserstein distance. We direct the interested readers to \citep[Chapter 6]{villani2008ot} for more details. It is worth mentioning that none of our judiciously chosen cost functions qualify as metrics on the support of the data. 

\subsection{Distributionally Robust Optimization}
In standard statistical learning framework, one generally assumes that the target-covariate pair $(X,Y)\in \mathbb{R}^d\times \mathbb{R} \cong\mathbb{R}^{d+1}$ follows a data-generating distribution $\mathbb{P} \coloneqq \mathbb{P}_{X,Y}$ on the support $\mathbb{R}^{d+1}$. One then seeks to find a `best' parameter $\beta$ that relates $Y$ to $X$ through a parameterized model by solving the stochastic optimization, \[
\inf_{\beta} \mathbb{E}_{\mathbb{P}}\left[\ell{(X,Y;\beta}) \right]. \tag{SO}\label{obj:so}
\]The \textit{loss function} $\ell{(x,y;\beta)}$ provides a quantification of the goodness-of-fit in the parameter $\beta$ given the realized observation $(x,y)$. Since only samples $\{(x_i,y_i)\}_{i=1,\ldots,N}$ are observed, we can typically only solve the empirical objective, \[
\inf_\beta \mathbb{E}_{\mathbb{P}_N}[\ell(X,Y;\beta)] = \inf_\beta \dfrac{1}{N}\sum_{i=1}^N \ell(x_i,y_i;\beta).\tag{ERM}\label{obj:erm}
\]Therefore the distribution $\mathbb{P}$ that underlies the data-generating mechanism is uncertain to the decision-maker. This motivated the \textit{distributionally robust optimization} (DRO) framework, which entails solving the following minimax stochastic program:\[
\inf_{\beta}\sup_{\mathbb{P}\in \mathcal{P}_{\rm amb}} \mathbb{E}_\mathbb{P} [\ell(X,Y;\beta)], \tag{\rm DRO}\label{obj:dro}
\]where the \textit{ambiguity set} $\mathcal{P}_{\rm amb}$ represents a class of probability measures supported on $\mathbb{R}^{d+1}$ that are candidates to the true data-generating distributions. In \textit{Wasserstein}-DRO, the ambiguity set is constructed by forming a `$\delta$-ball' around the canonical empirical measure $\mathbb{P}_N$ associated to the decision-maker-defined transport cost $c$, i.e. we let the ambiguity set $\mathcal{P}_{\rm amb}$ be chosen as: \begin{align*}
    \mathcal{B}_\delta (\mathbb{P}_N;c)\coloneqq  \{\mathbb{P}\in \mathcal{P}(\mathbb{R}^{d+1})| \mathcal{D}_c(\mathbb{P},\mathbb{P}_N)\leq \delta\}. \tag{WDRO} \label{obj:wdro}
\end{align*}
This ambiguity set captures probability measures that are close to the observed empirical measure in the transport cost $\mathcal{D}_c$, which may be taken as a class of candidates of measures perturbed from $\mathbb{P}_N$. The solution $\beta_{\rm DRO}$ to \eqref{obj:dro} that solves the worst case expected loss should perform well over the entire set of perturbations in the ambiguity set. This is in contrast to $\beta_{\rm ERM}$ that solves \eqref{obj:erm} only performs well on the training samples. This adds a robustness layer to the WDRO problem \eqref{obj:wdro}. For a comprehensive overview of different constructions of ambiguity sets, we direct the interested reader to \citep[Section 2]{kuhn2024dro}.

\subsection{Strong Duality of Wasserstein DRO}
The Wasserstein DRO problem involves an inner maximization over an infinite-dimensional set, which appears computationally intractable. However, the distribution $\mathbb{P}_n$ is discrete, strong duality of the Wasserstein DRO reformulates it as a simple univariate optimization.

\begin{prop}[Strong Duality, \texorpdfstring{\citep[Proposition 1]{blanchet2019rwpi}}{}]
\label{prop:duality}
    Let $c:\mathbb{R}^{d+1}\times \mathbb{R}^{d+1}\to [0,\infty]$ be a lower semi-continuous cost function satisfying $c\big((x,y),(u,v)\big) = 0$ whenever $(x,y) = (u,v)$. Then the distributionally robust regression problem \[
    \inf_{\beta\in\mathbb{R}^d} \sup_{\mathbb{P}:\mathcal{B}_\delta(\mathbb{P}_N)} \mathbb{E}_{\mathbb{P}}\left[\ell(X,Y;\beta) \right],
    \]is equivalent to, \[
    \inf_{\beta\in\mathbb{R}^d}\inf_{\gamma\geq 0}\left\{\gamma \delta + \dfrac{1}{n}\sum_{i=1}^N \phi_\gamma(x_i,y_i;\beta)\right\},
    \]where $\phi_\gamma(x_i,y_i;\beta)$ is given by, \[
        \sup_{(u,v)\in \mathbb{R}^{d+1}} \big\{ \ell(u,v;\beta) - \gamma c\big((u,v),(x_i,y_i) \big)\big\}.\]
\end{prop}For more general results, see  \citep[Theorem 1]{blanchet2019quantifying} and \citep[Section 2]{gao2022}. The exchangeability of $\sup$ and $\inf$ in Wasserstein-DRO is also established by \citep[Lemma 1]{blanchet2019rwpi}.

\section{Knowledge-Guided Wasserstein DRO}
\label{sec:kgdro}
In this section, we propose new cost functions for the Wasserstein DRO framework that leverage prior knowledge for transfer learning. For linear regression and binary classification, these cost functions act as regularizers, encouraging collinearity with prior knowledge.

\subsection{Knowledge-Guided Transport Cost}
It is shown in \citep[Theorem 1]{blanchet2019rwpi} that using the squared $q$-norm on the covariates as the cost function
\begin{equation}
c_2\big((x,y),(u,v)\big) = \|x - u\|_q^2 + \infty \cdot |y - v|,
\label{eqn:c_2}
\end{equation}
equates Wasserstein distributionally robust linear regression with $p$-norm regularization on the root mean squared error (RMSE). The cost function $c_2$ perturbs only the observed covariates $\{x_i\}_{i=1}^N$, while keeping the observed targets $\{y_i\}_{i=1}^N$ fixed. Keeping the observed target $Y$ as fixed often leads to more mathematically tractable reformulation, another intuition is that we trust the mechanism by which the target $Y$ is generated  once $X$ is known.

In the presence of prior knowledge $\theta$ that may aid in inferring $\beta$, we aim to control the extent of perturbation along the direction of $\theta$. 

Specifically, we constrain the size of the prediction discrepancy $\theta\trans x - \theta\trans u = \theta\trans \Delta$, where $\Delta \coloneqq x - u$.  
To achieve this goal, we henceforth augment the cost function $c_2$ with an additional penalty term that accounts for the size of the perturbation in the direction of $\theta$:
\begin{align*}
    c_{2,\lambda}\big((x,y),(u,v)\big) 
    = \|\Delta\|_q^2 + \infty \cdot |y - v| + \lambda h(|\theta\trans \Delta|),
    \label{eqn:c_2lambda} \tag{2}
\end{align*}
where $\lambda > 0$ and $h(x): \mathbb{R} \to \mathbb{R}^+ \cup \{0\}$ is a non-negative, monotone increasing function of $|x|$ such that $h(0) = 0$. Recall that in the cost function $c_2(\cdot)$, the targets $y$ remain fixed. Intuitively, the new cost function (\ref{eqn:c_2lambda})  encourages the Wasserstein ambiguity set to include distributions whose marginals in 
$X$ generate predictions that align with the data based on the prior predictor $\theta$. The parameter $\lambda$ controls the level of confidence in the prior knowledge. We call this kind of cost functions \textit{knowledge-guided}. Since $c_{2,\lambda}$ upper bounds the cost function $c_2$, we have $\mathcal{B}_\delta(\mathbb{P}^X_N;c_{2,\lambda_2}) \subseteq\mathcal{B}_\delta(\mathbb{P}^X_N;c_{2,\lambda_1})  \subseteq\mathcal{B}_\delta(\mathbb{P}^X_N;c_{2})$ whenever $\lambda_2>\lambda_1$.

The corresponding optimal transport problem given by:
\[
\inf_{\pi \in \Pi(\mathbb{Q}^X, \mathbb{P}_N^X)} \mathbb{E}_\pi [c_{2,\lambda}(X, U)],
\]
can also be expressed as:
\[
\inf_{\pi \in \Pi(\mathbb{Q}^X, \mathbb{P}_N^X)} \mathbb{E}_\pi [c_2(X, U)] + \lambda \mathbb{E}_\pi [h(|\theta\trans \Delta|)].
\]
This formulation regularizes the original optimal transport problem by penalizing large values of the expectation $\mathbb{E}_\pi [h(|\theta\trans \Delta|)]$.

For any user-defined function \( h \) that measures the discrepancy in generalization with respect to the prior knowledge \( \theta \), we refer to it as \textit{weak-transferring} of knowledge if \(\lambda < +\infty\), and \textit{strong-transferring} of knowledge if \(\lambda = +\infty\). In the case of strong-transferring, to ensure the finiteness of the optimal transport problem, the minimizing transport plan \(\pi^\dagger\) must satisfy the orthogonality condition \(\theta\trans \Delta = 0\), \(\pi^\dagger\)-almost surely. Consequently, the value of \(\theta\trans X\) remains unchanged after perturbing \(\mathbb{P}_N^X\) within \(\mathcal{B}_\delta(\mathbb{P}_N^X; c_{2,\infty})\). As a result, this should promote $\beta_{\mathrm{DRO}} \to \theta$ as $\delta \to \infty$.

\begin{remark}
The above framework extends to incorporate multi-sites prior knowledge, meaning that instead of a single prior knowledge coefficient \(\theta_1\), we consider a set of coefficients \(\{\theta_1, \theta_2, \ldots, \theta_M\}\). Let \(\Theta \coloneqq \operatorname{span}\{\theta_1, \theta_2, \ldots, \theta_M\}\) represent the linear span of these prior knowledge coefficients. In the case of strong-transferring, we must ensure that \(\operatorname{rank}(\Theta) < d\); otherwise, the set of orthogonality conditions \(\{\theta_m\trans \Delta = 0; m \in [M]\}\) would imply that the perturbation \(\Delta\) is identically zero (\(\Delta = \mathbf{0}\)). This would render the ambiguity set redundant and reduce the WDRO problem \eqref{obj:wdro} to the ERM problem \eqref{obj:erm}. This result is confirmed by the statements of Theorems \ref{thm:linear_l2} and \ref{thm:logistic}.
\end{remark}

\subsection{Linear Regression}
We begin by examining the WDRO problem \eqref{obj:wdro} for linear regression within the strong-transferring domain. Following this, we present a specific case within the weak-transferring domain. Let $\Theta \coloneqq \spn\{\theta_1, \ldots, \theta_M\}$ represent the linear span of the prior knowledge.

\subsubsection{Strong-Transferring}
Define the cost function $c_{2,\infty} \big((x,y),(u,v)\big) \coloneqq \Vert x-u\Vert_q^2 + \infty\cdot|y-v| + \infty\cdot|\theta_1\trans x - \theta_1\trans u| + \ldots + \infty \cdot |\theta_M\trans x - \theta_M\trans u|$, and for a set of observed samples $\{(x_i,y_i)\}_{i\in[N]}$, we use ${\rm MSE}_N(\beta) \coloneqq N^{-1}\sum_{i=1}^N(y_i-\beta\trans x_i)^2$. Without making any additional distributional assumptions on $(X, Y)$, we obtain the following finite-dimensional representation.
\begin{theorem}[Linear Regression with Strong-Transferring]
\label{thm:linear_l2}
    Consider the least-squared loss $\ell(X,Y;\beta) = (Y-\beta\trans X)^2$, then for any $q\in[1,\infty]$ we have \begin{align*}
    &\inf_{\beta\in\mathbb{R}^d} \sup_{\mathbb{P}:\mathcal{B}_\delta(\mathbb{P}_N;c_{2,\infty})} \mathbb{E}_{\mathbb{P}}\left[(Y-\beta\trans X)^2\right]\\
    =&
    \inf_{\beta\in\mathbb{R}^d,\vartheta\in\Theta} \left(\sqrt{{\rm MSE}_N(\beta)} + \sqrt{\delta} \Vert \beta - \vartheta \Vert_p \right)^2,
\end{align*}
where $p$ is such that $p^{-1}+q^{-1}=1$.
\end{theorem}

From the above result, we observe that the knowledge-guided WDRO problem for linear regression is equivalent to regularizing the RMSE with a $p$-norm distance to the linear span $\Theta$. The regularization parameter is entirely determined by the size (or radius) of the Wasserstein ambiguity set. Importantly, the penalty term focuses on the collinearity with the prior knowledge rather than their algebraic difference or angular proximity.

Consider the case when there is only a single prior knowledge $\theta_1$, the penalty term does not constrain the solution $\beta_{\rm DRO}$ to be close to $\theta_1$, but rather to $\kappa \cdot \theta_1$ for some $\kappa \in \mathbb{R}$ to be optimized. Consequently, this knowledge transfer automatically robustify solution against scaling of covariates. Furthermore, it can prevent negative transfer by adapting its sign to be positively correlated with $\beta^*$, which is the solution to population objective \eqref{obj:so}. When $\delta \to \infty$, the penalty term becomes dominant, forcing $\beta$ to lie in $\Theta$ for any $p \geq 1$. This reduces the WDRO problem to a simple constrained regression problem,
\[
\inf_{\beta \in \Theta} {\rm MSE}_N(\beta), 
\]
reflecting the complete reliance on the prior knowledge and prevents excessive shrinkage towards the null estimator.
\begin{remark}
    We now discuss two special cases of the penalty term, $p=2$ (ridge-type regularization) and $p=1$ (lasso-type regularization). For simplicity, we consider the case of a single prior knowledge vector $\theta$.
    
    \textbf{Ridge-type.} The penalty term can be explicitly calculated as \[
    \min_{\kappa \in \mathbb{R}} \Vert \beta - \kappa \theta \Vert_2 = \left\Vert \beta - \dfrac{\beta\trans \theta}{\Vert \theta \Vert_2^2} \theta \right\Vert_2 = \Vert \beta^{\perp \theta} \Vert_2,
    \]where $\beta^{\perp \theta}$ is the component of $\beta$ orthogonal to $\theta$. This penalty term shrinks distance to the line in the direction of $\theta$. Furthermore, note that 
    \[
    \Vert \beta^{\perp \theta} \Vert_2 = \Vert \beta \Vert_2 \sin(\beta, \theta) = \Vert \beta \Vert_2 \sqrt{1 - \cos^2(\beta, \theta)},
    \]
    which represents a trade-off between the magnitude of $\beta$ and its angular proximity to the prior knowledge $\theta$. This trade-off is illustrated in the leftmost figure of Fig.\ref{fig:feasibleset_illustration}, drawing the feasibility set of the regularization as a constraint. This regularization is closely related but different to the one proposed in \cite{gu2024angle-based}, where they penalize large values of a computational relaxation of $\sin{(\beta,\theta)}$.
    
    \textbf{Lasso-type.} When the prior knowledge $\theta$ is sparse, the penalty term $\min_\kappa \Vert \beta - \kappa \theta \Vert_1$ promotes sparse representation learning. Consider a simple example where the dimension is $d = 3$ and $\theta = (1, 0, 0)\trans$. In this case, we have:
    \begin{align*}
        \min_\kappa \Vert \beta - \kappa \theta \Vert_1 &= \min_\kappa \big(|\beta_1 - \kappa| + |\beta_2| + |\beta_3|\big) \\
        &= |\beta_2| + |\beta_3| \eqqcolon \Vert \beta_{\shortminus 1} \Vert_1,
    \end{align*}
    where $\beta_{\shortminus 1} = (\beta_2, \beta_3)\trans$. This formulation enforces sparsity only on the last two components of $\beta$, reflecting the sparsity pattern of $\theta$.
\end{remark}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.32\textwidth]{Figures/illustrations/setplot1.pdf}
    \hfill
    \includegraphics[width=0.32\textwidth]{Figures/illustrations/setplot2.pdf}
    \hfill
    \includegraphics[width=0.32\textwidth]{Figures/illustrations/setplot3.pdf}
    \caption{The two-dimensional contour plots of the regularization term in Theorem \ref{thm:linear_l2} and Theorem \ref{thm:weak_l2} with $\lambda$ ranging from $+\infty$ to $2$ to $0.1$. The prior knowledge parameter is taken as $\theta = (2,1)\trans$. The area between the black contours constitute a feasibility set of the regularization term when written in its equivalent constraint form. The feasibility set shrinks in the direction of $\theta$, to a circle of radius $K$ when $\lambda \to 0$ from above.}
    \label{fig:feasibleset_illustration}
\end{figure*}
\subsubsection{Weak Transferring}
For the special case of $q=p=2$, we define the weak-transferring cost function $c_{2,\lambda}\big((x,y),(u,v) \big) = \Vert x-u\Vert_2^2 + \lambda (\theta\trans x - \theta\trans u)^2 + \infty \cdot |y-v|$ with $0<\lambda<+\infty$. Here, we select $h(x) = x^2$ as the user-defined function on controlling the size of perturbation in $\theta$.  For simplicity, we consider a single prior knowledge vector $\theta$ in this setup. This result can be straightforwardly extended to a multi-source setup with different values of $\lambda$'s.

\begin{theorem}[Linear Regression with Weak Transferring]
\label{thm:weak_l2}
    Consider the least-squared loss $\ell(X,Y;\beta) = (Y-\beta\trans X)^2$, then for $p=q=2$ we have \begin{align*}&\inf_{\beta\in\mathbb{R}^d} \sup_{\mathbb{P}:\mathcal{B}_\delta(\mathbb{P}_N;c_{2,\lambda})} \mathbb{E}_{\mathbb{P}}\left[(Y-\beta\trans X)^2\right]\\
        =&
    \inf_{\beta\in\mathbb{R}^d} \left(\sqrt{{\rm MSE}_N(\beta)} + \sqrt{\delta} \left\Vert \beta \right\Vert_{\Psi_{\lambda^{\shortminus 1}}} \right)^2.
    \end{align*}With $\Psi_\lambda = I_d - \dfrac{1}{ \Vert \theta\Vert_2^2 + \lambda}\theta \theta\trans$ and $\Vert \beta \Vert_{\Psi_\lambda}^2 = \beta\trans \Psi_\lambda \beta$.
\end{theorem}

Write $P_\lambda = {\theta\theta\trans}/({\lVert\theta\Vert_2^2+\lambda})$, we note that as $\lambda\to\infty$, we have $P_{\lambda^{\shortminus 1}} \to P_0 = {\theta\theta\trans}/{\Vert\theta\Vert_2^2}$ recovering the projection matrix onto the prior knowledge $\theta$. Consequently, $\Vert\beta\Vert_{\Psi_{\lambda^{\shortminus 1}}} \to \Vert \beta^{\perp\theta}\Vert_2$.

We observe that the action \[P_{\lambda^{\shortminus 1}} \beta = \dfrac{\beta\trans\theta}{\Vert\theta\Vert_2^2+\lambda^{-1}}\theta\] is exactly the ridge regression of $\beta$ onto $\theta$ with a regularization parameter $\lambda^{-1}$. Thus, the finiteness of $\lambda$, which can reflect a caution in the prior knowledge $\theta$, induces a shrinkage effect on the component of $\beta$ explainable by $\theta$ in the dot product geometry. Since $\Psi_\lambda \succ I_d-P$, we have $\Vert\beta\Vert_{\Psi_{\lambda^{\shortminus 1}}} > \Vert\beta^{\perp\theta}\Vert_2$ for any finite $\lambda > 0$, this implies the inclusion of feasibility set \[\{\beta: \Vert\beta\Vert_{\Psi_{\lambda^{\shortminus 1}}}\leq K\} \subset \{\beta:\Vert\beta^{\perp\theta}\Vert_2\leq K\},\] as plotted in Fig.\ref{fig:feasibleset_illustration} for an illustration on $\mathbb{R}^2$. The contour $\{\beta \in \mathbb{R}^2:\Vert\beta\Vert_{\Psi_{\lambda^{\shortminus 1}}} = K\}$ forms an ellipse centered around the origin $\boldsymbol{0}$. The ellipse has a major axis of half length $K\sqrt{\dfrac{\Vert\theta\Vert_2^2+\lambda^{-1}}{\lambda^{-1}}}$ aligned with the direction of $\theta$, and a minor axis with half-length $K$ aligned with the direction of $\theta^\perp$. As $\lambda \to 0$, representing no-confidence in $\theta$, the half-length of the major axis converges to $K$, resulting in a perfect circle as in ridge regression.

The two-dimensional hyper-parameters $(\delta, \lambda^{- 1})$ enable the use of data-driven methods, such as grid-search cross-validation, for hyper-parameter tuning. Unlike the strong-transferring domain, the inclusion of $\lambda^{- 1}$ allows the data to self-determine the informativeness of the source samples. 


\subsection{Binary Classifications}
In this section, we focus on the context of binary classification, where the goal is to predict the discrete label $Y \in \{-1, 1\}$ based on the covariates $X \in \mathbb{R}^d$. Unlike the previous section, we use the $q$-norm, rather than its square, to account for distributional ambiguity in the covariate distribution. Define the strong-transferring cost function $c_{1,\infty}\big((x,y),(u,v)\big) \coloneqq \Vert x-u\Vert_q + \infty\cdot|y-v| + \infty\cdot|\theta_1\trans x - \theta_1\trans u| + \ldots +\infty \cdot |\theta_M\trans x - \theta_M\trans u|.$ We consider two loss functions here. The \textbf{logistic loss} function is given by \[
\ell(X,Y;\beta) = \log{\left(1+e^{-Y\beta\trans X} \right)},
\]which is the negative log-likelihood of the model that postulates\[
\log{\dfrac{\mathbb{P}(Y=1|X=x)}{\mathbb{P}(Y=-1|X=x)}} = \beta^{*}{\trans}x.
\]The \textbf{hinge loss} is given by \[
\ell{(X,Y;\beta)} = (1-Y\beta\trans X)^+,
\]which is typically used for training classifiers that look for `maximum-margins' in class boundaries, most notably \textit{support vector machines}.

Suppose $Y\in\{-1,1\}$ is binary and without any distributional assumptions on $X$, we have the following result which recovers regularized logistic regressions and support vector machines.
\begin{theorem}[Binary Classification with Strong Transferring]
\label{thm:logistic}
Suppose the loss function $\ell(X,Y;\beta)$ is either the logistic loss $\log{\left(1+e^{-Y\beta\trans X}\right)}$ or the hinge loss $(1-Y\beta\trans X)^+$, then for any $q\in[1,\infty]$  we have
\begin{align*}
    &\inf_{\beta\in\mathbb{R}^d} \sup_{\mathbb{P}:\mathcal{B}_\delta(\mathbb{P}_N;c_{1,\infty})} \mathbb{E}_{\mathbb{P}}\left[\ell(X,Y;\beta)\right]\\
   =& \inf_{\beta\in\mathbb{R}^d,\vartheta\in\Theta}\dfrac{1}{N}\sum_{i=1}^N \ell(x_i,y_i;\beta) + \delta \Vert \beta - \vartheta \Vert_p,
\end{align*}
where $p$ is such that $p^{-1}+q^{-1}=1$.
\end{theorem}


\subsection{Sub-Coefficient-Vector Transferring}
In this subsection, we generalize the statements of Theorems \ref{thm:linear_l2} and \ref{thm:logistic} for $p=2$ to arbitrary norms induced by positive-definite quadratic forms. Let $\Lambda \in \mathbb{R}^{d \times d}$ be a positive-definite symmetric matrix. The norm $\Vert x \Vert_\Lambda = \sqrt{x\trans \Lambda x}$ induces a metric on $\mathbb{R}^d$, defined as $d_{\Lambda}(x, u) = \Vert x - u \Vert_\Lambda$, known as the \textit{Mahalanobis distance}. Since $\Lambda$ is positive definite, it admits a decomposition $\Lambda = \Gamma\trans \Gamma$ with $\Gamma$ invertible, and the norm $\Vert x \Vert_\Lambda = \Vert \Gamma x \Vert_2$ measures length in the geometry distorted by $\Gamma$. By \citep[Lemma 1]{blanchet2019data-drivencost}, the dual norm of $\Vert \cdot \Vert_\Lambda$ is $\Vert \cdot \Vert_{\Lambda^{-1}}$. Using Proposition \ref{prop:generalized_adaptive}, the statements of Theorems \ref{thm:linear_l2} and \ref{thm:logistic} can be easily generalized. Define the space of positive-definite symmetric matrices as $\mathbb{S}_+^{d \times d}$ and the cost function: $c_{2,\infty}^\Lambda \big((x, y), (u, v)\big) \coloneqq \Vert x - u \Vert_\Lambda^2 + \infty \cdot |y - v| + \infty \cdot \sum_{m=1}^M |\theta_m\trans x - \theta_m\trans u|$.

\begin{corollary}[Theorem \ref{thm:linear_l2}]
\label{cor:linear_l2}
For the least-squares loss $\ell(X, Y; \beta) = (Y - \beta\trans X)^2$ and any $\Lambda \in \mathbb{S}_+^{d \times d}$:
\begin{align*}
    &\inf_{\beta \in \mathbb{R}^d} \sup_{\mathbb{P} : \mathcal{B}_\delta(\mathbb{P}_N; c_{2,\infty}^\Lambda)} \mathbb{E}_{\mathbb{P}}\left[(Y - \beta\trans X)^2\right] \\
=&\inf_{\beta \in \mathbb{R}^d, \vartheta \in \Theta} \left(\sqrt{{\rm MSE}_N(\beta)} + \sqrt{\delta} \Vert \beta - \vartheta \Vert_{\Lambda^{-1}}\right)^2.
\end{align*}
\end{corollary}

This formulation enables the use of metric learning methods to determine $\Lambda$ directly from the data, as detailed in \cite{blanchet2019data-drivencost}. For example, if the two-dimensional prior $\theta = [\theta_1, \theta_2]$ is known to primarily influence the first component of the truth $\beta = [\theta_1 + \epsilon, \beta_2]$, we can select $\Lambda = \mathrm{diag}(d_1, d_2)$ with $d_1 \ll d_2$. This imposes a weaker penalty on perturbations in the first direction, resulting in a weighted penalty term: $\min_{\kappa} \big({(\beta_1 - \kappa \theta_1)}/{d_1} + {(\beta_2 - \kappa \theta_2)}/{d_2}\big)$, which prioritizes aligning $\beta_1$ with $\theta_1$, while $\beta_2$ is determined more flexibly based on the data. We call this sub-coefficient-vector transferring, or the ability to partially transfer prior knowledge. A similar corollary applies to Theorem \ref{thm:logistic}, as stated in Corollary \ref{cor:logistic}. 

Finally, we again draw the reader's attention to Table \ref{tab:comparison}, which compares several transfer learning methods discussed in Section \ref{sec:review:tl}. Notably, our proposed KG-WDRO framework brings together a broad range of desirable capabilities within a single, unified approach to transfer learning.

\section{Numerical Results}
\label{sec:numerical}
\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.90\linewidth]{Figures/Simulation_Plots/sim_main_text.pdf}
    \caption{Out-of-sample performance plot of the proposed KG-WDRO method for high-dimensional regression tasks, compared against benchmark methods. The plot shows performance variations as $\rho$, representing the correlation between true and prior coefficient pairs, increases. Results are displayed for four specific settings across three experimental groups.}
    \label{fig:sim_main_text}
\end{figure*}

In this section, we present numerical simulations to validate the effectiveness of the proposed KG-WDRO method. We compare learners across different settings, including high-dimensional sparse models, correlated covariates, and multi-source prior knowledge, for either linear regression or binary classification tasks. Performance is evaluated using out-of-sample classification error for binary classifiers and out-of-sample \(R^2\) for linear regressors.

For the single-source experiments, target-source coefficient pairs \((\beta, \theta)\) are generated from a multivariate normal distribution:
\[
\label{sim:scheme}
(\beta_j, \theta_j) \sim N\left( 
\begin{pmatrix} 
0 \\ 
0 
\end{pmatrix}, 
\begin{pmatrix} 
\sigma^2 & \rho\sigma^2 \\ 
\rho\sigma^2 & \sigma^2 
\end{pmatrix}
\right), \tag{3}
\]
where \(\rho\) is the correlation between \(\beta\) and \(\theta\), and the expected length of \(\theta\) is approximately \(\sigma \sqrt{d - 0.5}\). We scale \(\beta\) as \(\beta \gets s\beta\) with \(s \in (0, 1]\) to study the stabilizing effects of strong prior knowledge in small-sample settings. The dimension-to-sample ratio \(d/N\) is varied by fixing \(d\) and increasing \(N\). Performance is averaged over 100 simulations. Each dataset consists of three parts: \texttt{data = (train, val, test)}. The \texttt{(train, val)} pair shares the same size, and hyperparameters are selected based on validation performance. The source data contains 800 samples, with source truth \(\theta\) estimated accordingly. Out-of-sample performance is measured on the \texttt{test} set of 5000 data points.


\subsection{Simulation 1: Logistic with \texorpdfstring{$\ell_1$}{l1}-Strong Transferring}

\label{subsec:sim1}
In the first experiment, we compare two learners for binary classification tasks with high-dimensional sparse coefficients against our proposed KG-WDRO learner, $\beta_{\rm KG}$, derived using Theorem \ref{thm:logistic} with $p=1$. The competing learners are the target-only vanilla WDRO learner $\beta_{\rm WDRO}$ \citep[Theorem 2]{blanchet2019rwpi} and $\beta_{\rm TransG}$, obtained via the $\mathcal{A}$-Trans-GLM algorithm \citep[Algorithm 1]{tian2023transglm}. The target-source pair $(\beta, \theta)$ is generated using \eqref{sim:scheme} with a dimension of 50 and augmented with 100 zeros for sparsity, resulting in a total dimension of 150. We test six settings, varying the sample size $N \in \{20, 50, 80\}$, signal strength $s \in \{0.5, 1\}$, and truth-prior correlation $\rho \in \{0.3, 0.5, 0.7, 0.8, 0.9, 0.95\}$.

The comparison between $\beta_{\rm KG}$ and $\beta_{\rm TransG}$ is highly competitive, with $\beta_{\rm KG}$ consistently outperforming $\beta_{\rm TransG}$ by up to $2\%$ in accuracy when the sample size is small ($N = 20$) across all values of $\rho$, as shown in the upper-left plot of Figure \ref{fig:sim_main_text}. In larger sample size scenarios, both learners perform similarly (see Table \ref{tab:sim1} for detailed results). Both transfer learning methods, $\beta_{\rm KG}$ and $\beta_{\rm TransG}$, significantly outperform the target-only learner, $\beta_{\rm WDRO}$.

\subsection{Simulation 2: Linear Regression with \texorpdfstring{$\ell_2$}{l2}-Weak Transferring}

\label{subsec:sim2}
In this simulation, we compare two learners on high-dimensional linear regression with correlated covariates against our proposed learners, $\beta_{\rm KGweak}$ (Theorem \ref{thm:weak_l2}) and $\beta_{\rm KGstrong}$ (Theorem \ref{thm:linear_l2}), both using $p=2$. There is no sparsity in the regression coefficients. The competing learners are the target-only vanilla WDRO learner $\beta_{\rm WDRO}$ \citep[Theorem 1]{blanchet2019rwpi} and the Trans-Ridge algorithm adapted from \citep[Algorithm 1]{li2021translasso}, denoted as $\beta_{\rm TransR}$. The covariates are fixed at dimension 100, with a pairwise correlation of 0.3. The experiment is conducted across six settings, varying the sample size $N \in \{50, 70, 90\}$, signal strength $s \in \{0.8, 1\}$, and truth-prior correlation $\rho \in \{0.3, 0.5, 0.7, 0.8, 0.9, 0.95\}$.

As shown in the upper-right plot of Figure \ref{fig:sim_main_text}, the performance of $\beta_{\rm TransR}$ lags significantly behind both $\beta_{\rm KGstrong}$ and $\beta_{\rm KGweak}$ until the correlation $\rho$ becomes sufficiently high. Across all settings, $\beta_{\rm KGstrong}$ and $\beta_{\rm KGweak}$ consistently outperform $\beta_{\rm TransR}$ when $\rho$ is moderate or low, as documented in Table \ref{tab:sim2}. Furthermore, all three transfer learning methods demonstrate superior performance compared to the target-only learner, $\beta_{\rm WDRO}$. 

\subsection{Simulation 3: Transfer Learning with Multiple Sites}
\label{subsec:sim3}
In the final set of experiments, we validate our methods in a multi-source transfer learning setting with high-dimensional sparse linear regression. The significant components of the three source coefficients are generated using \eqref{sim:scheme} with correlation $\varrho$ and dimension 50, denoted as $\{\theta_1, \theta_2, \theta_3\}$. We construct a linear combination, $\theta_S = a\theta_1 + b\theta_2 + c\theta_3$, and generate $\beta = \rho \theta_S + \varepsilon$, where $\varepsilon \sim N(0, (1-\rho^2)\mathrm{Var}(\theta_S))$, ensuring $\mathrm{Corr}(\beta, \theta_S) = \rho$. $\beta$ is then scaled to match the magnitude of $\theta_S$, and all vectors are augmented with 100 zeros, yielding a total dimension of 150. Our proposed method, $\beta_{\rm KG}$ (Theorem \ref{thm:linear_l2}, $p=1$), is compared against the oracle Trans-Lasso algorithm \citep[Algorithm 1]{li2021translasso} ($\beta_{\rm TransL}$) and the vanilla WDRO learner $\beta_{\rm WDRO}$. The experiment spans six settings: $[a, b, c] = [1, -0.5, 0.2]$ and $[1, 1, 1]$, with $\varrho = 0.9$ and $0.6$, respectively. Sample sizes vary in $N \in \{50, 60, 70\}$. The truth-prior correlation ranges in $\rho \in \{0.7, 0.75, 0.8, 0.85, 0.9, 0.95\}$.


When $[a, b, c] = [1, -0.5, 0.2]$, the contributions of the $\theta$'s to the generation of $\theta_S$ are unequal. In this case, it is not surprising that $\beta_{\rm KG}$ outperforms $\beta_{\rm TransL}$, as shown in the bottom-left plot of Figure \ref{fig:sim_main_text}. When $\theta_S$ is an equal-weighted average of the $\theta$'s ($[a, b, c] = [1, 1, 1]$), the performance of $\beta_{\rm KG}$ and $\beta_{\rm TransL}$ becomes similar. However, $\beta_{\rm KG}$ still demonstrates superior performance in larger sample sizes and higher correlations, as documented in Table \ref{tab:sim3}.

\section{Conclusion}
We propose the knowledge-guided Wasserstein
distributionally robust optimization (KG-WDRO) framework, which utilizes prior knowledge of predictors to mitigate the over-conservativeness of  conventional DRO methods. We establish tractable reformulations and demonstrate  their superior performance compared to other methods. For future work, we aim to provide statistical guarantees of our proposed estimators. Furthermore, based on these statistical properties, we plan to develop a principled approach for selecting hyperparameters such as  $\delta$ and $\lambda$.


\bibliographystyle{apalike}
\bibliography{ref.bib}


\setcounter{lemma}{0}
\renewcommand{\thelemma}{A\arabic{lemma}}
\setcounter{equation}{0}
\setcounter{algorithm}{0}
\setcounter{table}{0}
\setcounter{assumption}{0}
\renewcommand{\theequation}{A\arabic{equation}}
\renewcommand{\thealgorithm}{A\arabic{algorithm}}
\renewcommand{\thetable}{A\arabic{table}}
\renewcommand{\thedefinition}{A\arabic{definition}}
\renewcommand{\theprop}{A\arabic{prop}}
\renewcommand{\thetheorem}{A\arabic{theorem}}
\renewcommand{\theexample}{A\arabic{example}}
\renewcommand{\theassumption}{A\arabic{assumption}}

\clearpage
\newpage
\setcounter{page}{1}
\appendix

\section{Additional Details in Numerical Results}
This section provides  details to supplement Section \ref{sec:numerical}. We outline the data-generating distributions for all three sets of experiments, the hyperparameter grids, and the learners used to identify prior knowledge. We present the exact numerical results for all three sets of experiments. Recall that the notation \( s \in (0,1] \) represents the signal strength of the true parameter \( \beta \), which works by rescaling the magnitude of \( \beta \) such that \( \beta \gets s\beta \). The notation $d$ is the dimension of the covariates, and $N$ is the sample size. Finally, the symbol \( \rho \) represents the correlation between the true \( \beta \) and the prior \( \theta \).
\subsection{Simulation Results}
\subsubsection{Simulation 1: Logistic Regression}

\begin{table}[H]
\begin{center}
\begin{small}
\setlength{\tabcolsep}{3pt} % Reduce column padding
\renewcommand{\arraystretch}{1.1} % Adjust row height
\begin{sc}
\begin{tabular}{l|c|cccccc|cr}
\toprule
Setting & & $\rho = 0.3$ & $\rho = 0.5$ & $\rho = 0.7$ & $\rho = 0.8$ & $\rho = 0.9$ & $\rho = 0.95$ & WDRO\\
\midrule
$s = 1$ & KG-WDRO  & \textbf{0.587} & \textbf{0.647} & \textbf{0.714} & \textbf{0.748} & \textbf{0.794} & \textbf{0.817} & 0.565\\
$N=20$ & Trans-GLM & 0.585 & 0.641 & 0.702 & 0.735 & 0.778 & 0.800 & - \\
\midrule
$s = 1$ & KG-WDRO  & {0.586} & \textbf{0.647} & \textbf{0.713} & 0.751 & \textbf{0.797} & {0.823} & 0.619\\
$N=50$ & Trans-GLM & \textbf{0.586} & 0.645 & 0.710 & \textbf{0.752} & 0.792 & \textbf{0.823} & - \\
\midrule
$s = 1$ & KG-WDRO  & 0.583 & \textbf{0.646} & {0.713} & 0.751 & {0.798} & 0.823 & 0.654 \\
$N=80$ & Trans-GLM  & \textbf{0.584} & {0.646} & \textbf{0.714} & \textbf{0.755} & \textbf{0.800} & \textbf{0.826} & - \\
\midrule
$s = 0.5$ & KG-WDRO     & \textbf{0.581} & \textbf{0.634} & \textbf{0.690} & \textbf{0.721} & \textbf{0.762} & \textbf{0.787} & 0.549\\
$N=20$ & Trans-GLM   & 0.579 & 0.626 & 0.674 & 0.708 & 0.748 & 0.760 & -\\
\midrule
$s = 0.5$ & KG-WDRO     & \textbf{0.580} & \textbf{0.635} & 0.689 & \textbf{0.728} & \textbf{0.768} & \textbf{0.794} & 0.588\\
$N=50$ & Trans-GLM   & 0.579 & 0.633 & \textbf{0.693} & 0.723 & {0.769} & 0.789 & -\\
\midrule
$s = 0.5$ & KG-WDRO     & \textbf{0.581} & 0.637 & {0.700} & {0.732} & 0.775 & 0.790 & 0.617\\
$N=80$ & Trans-GLM   & 0.581 & \textbf{0.638} & \textbf{0.702} & \textbf{0.737} & \textbf{0.779} & \textbf{0.799} & -\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}

\caption{Out-of-sample classification accuracies for Simulation \ref{subsec:sim1}, comparing KG-WDRO, Trans-GLM, and WDRO across six settings with varying values of $\rho$.}
\label{tab:sim1}
\end{table}

\subsubsection{Simulation 2: Linear Regression}
\begin{table}[H]
\begin{center}
\setlength{\tabcolsep}{4pt} % Reduce column padding
\renewcommand{\arraystretch}{1.1} % Adjust row height
\begin{small}
\begin{sc}
\begin{tabular}{l|c|cccccc|r}
\toprule
Setting & & $\rho = 0.3$ & $\rho = 0.5$ & $\rho = 0.7$ & $\rho = 0.8$ & $\rho = 0.9$ & $\rho = 0.95$ & WDRO\\
\midrule
$s = 1$ & KG-WDRO (Strong) & \textbf{0.585} & 0.645 & 0.740 & \textbf{0.801} & 0.870 & 0.912 & 0.108\\
$N=50$  & KG-WDRO (Weak) & 0.583 & \textbf{0.646} & \textbf{0.741} & 0.800 & \textbf{0.871} & 0.910 & -\\
        & Trans-Ridge & 0.391 & 0.548 & 0.706 & 0.786 & 0.870 & \textbf{0.915} & -\\
\midrule
$s = 1$ & KG-WDRO (Strong) & \textbf{0.707} & \textbf{0.745} & \textbf{0.803} & \textbf{0.843} & \textbf{0.894} & {0.924} & 0.513\\
$N=70$  & KG-WDRO (Weak) & 0.704 & 0.743 & 0.803 & 0.842 & 0.892 & 0.923 & -\\
        & Trans-Ridge & 0.599 & 0.692 & 0.788 & 0.838 & 0.893 & \textbf{0.925} & -\\
\midrule
$s = 1$ & KG-WDRO (Strong) & \textbf{0.806} & \textbf{0.827} & {0.859} & \textbf{0.881} & \textbf{0.911} & {0.932} & 0.758\\
$N=90$    & KG-WDRO (Weak) & 0.804 & 0.825 & 0.857 & 0.880 & 0.910 & 0.930 & -\\
          & Trans-Ridge & 0.762 & 0.802 & 0.849 & 0.877 & {0.910} & \textbf{0.932} & -\\
\midrule
$s = 0.8$ & KG-WDRO (Strong) & \textbf{0.563} & \textbf{0.621} & \textbf{0.716} & \textbf{0.777} & \textbf{0.850} & \textbf{0.894} & 0.030\\
$N=50$    & KG-WDRO (Weak) & 0.561 & 0.622 & 0.716 & 0.777 & 0.849 & 0.892 & -\\
          & Trans-Ridge & 0.213 & 0.405 & 0.600 & 0.700 & 0.803 & 0.858 & -\\
\midrule
$s = 0.8$ & KG-WDRO (Strong) & \textbf{0.673} & \textbf{0.713} & \textbf{0.774} & \textbf{0.818} & \textbf{0.872} & \textbf{0.905} & 0.361\\
$N=70$    & KG-WDRO (Weak) & 0.670 & 0.710 & 0.774 & 0.816 & 0.869 & 0.903 & -\\
          & Trans-Ridge & 0.470 & 0.585 & 0.704 & 0.768 & 0.837 & 0.875 & -\\
\midrule
$s = 0.8$ & KG-WDRO (Strong) & \textbf{0.768} & \textbf{0.791} & \textbf{0.826} & \textbf{0.851} & \textbf{0.886} & \textbf{0.911} & 0.703\\
$N=90$    & KG-WDRO (Weak) & 0.765 & 0.788 & 0.825 & 0.851 & 0.885 & 0.909 & -\\
          & Trans-Ridge & 0.671 & 0.724 & 0.785 & 0.821 & 0.863 & 0.890 & -\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
%\vskip -0.1in
\caption{Out-of-sample $R^2$ for Simulation \ref{subsec:sim2}, comparing KG-WDRO (Strong), KG-WDRO (Weak), Trans-Ridge, and WDRO across six settings with varying values of $\rho$.}
\label{tab:sim2}
\end{table}
\newpage
\subsubsection{Simulation 3: Multi-Sites}
Here, recall that the notation $\varrho$ denote the correlation of generating the three prior knowledge under the scheme \eqref{sim:scheme}.
\begin{table}[H]
\begin{center}
\setlength{\tabcolsep}{4pt} % Reduce column padding
\renewcommand{\arraystretch}{1.1} % Adjust row height
\begin{small}
\begin{sc}
\begin{tabular}{l|c|cccccc|r}
\toprule
Setting & & $\rho = 0.7$ & $\rho = 0.75$ & $\rho = 0.8$ & $\rho = 0.85$ & $\rho = 0.9$ & $\rho = 0.95$ & WDRO\\
\midrule
$[1,-0.5,0.2]$ & KG-WDRO & 0.560 & \textbf{0.640} & \textbf{0.713} & \textbf{0.783} & \textbf{0.850} & \textbf{0.916} & -0.584\\
$\varrho = 0.9,N=50$        & Trans-Lasso & \textbf{0.578} & 0.625 & 0.673 & 0.723 & 0.767 & 0.815 & - \\
\midrule
$[1,-0.5,0.2]$ & KG-WDRO & \textbf{0.674} & \textbf{0.728} & \textbf{0.776} & \textbf{0.825} & \textbf{0.875} & \textbf{0.926} & 0.027\\
$\varrho = 0.9,N=60$    & Trans-Lasso & 0.666 & 0.697 & 0.732 & 0.770 & 0.808 & 0.850 & -\\
\midrule
$[1,-0.5,0.2]$ & KG-WDRO & \textbf{0.793} & \textbf{0.820} & \textbf{0.848} & \textbf{0.878} & \textbf{0.907} & \textbf{0.939} & 0.375\\
$\varrho = 0.9,N=70$    & Trans-Lasso & 0.756 & 0.779 & 0.805 & 0.832 & 0.857 & 0.882 & -\\
\midrule
$[1,1,1]$ & KG-WDRO & 0.565 & 0.642 & 0.715 & 0.785 & \textbf{0.852} & \textbf{0.916} & -2.837\\
$\varrho = 0.6, N=50$    & Trans-Lasso & \textbf{0.628} & \textbf{0.680} & \textbf{0.735} & \textbf{0.790} & {0.838} & {0.889} & - \\
\midrule
$[1,1,1]$ & KG-WDRO & 0.673 & 0.729 & 0.778 & \textbf{0.829} & \textbf{0.877} & \textbf{0.928} & -0.015\\
$\varrho = 0.6, N=60$    & Trans-Lasso & \textbf{0.708} & \textbf{0.744} & \textbf{0.786} & {0.826} & {0.863} & {0.902} & - \\
\midrule
$[1,1,1]$ & KG-WDRO & \textbf{0.797} & \textbf{0.825} & \textbf{0.852} & \textbf{0.880} & \textbf{0.911} & \textbf{0.942} & 0.354\\
$\varrho = 0.6, N=70$    & Trans-Lasso & {0.794} & {0.820} & {0.844} & {0.868} & {0.894} & {0.919} & - \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
%\vskip -0.1in
\caption{Out-of-sample $R^2$ for Simulation \ref{subsec:sim3}, comparing KG-WDRO, Trans-Lasso, and WDRO across six settings with varying values of $\rho$.}
\label{tab:sim3}
\end{table}

\subsection{Simulation Setup}
Let ${\rm Ber}(p)$ denote a bernoulli distribution with probability parameter $p$, $\mathcal{U}[a,b]$ denote a uniform distribution supported on $[a,b]$, and $\mathcal{N}(\mu,\sigma^2)$ denote a univariate normal distribution with mean $\mu$ and variance $\sigma^2$.

\subsubsection{Simulation 1: Logistic Regression}  
In this simulation, the coefficients are generated in a high-dimensional sparse setting. The dimension of the nonzero components is set to 50, which is then augmented with 100 zero components to introduce sparsity. The nonzero components of the true coefficient-prior pair $(\beta, \theta)$ are generated using the multivariate normal scheme in \eqref{sim:scheme}, with component variance $\sigma^2 = 0.4$ and $\rho \in \{0.3, 0.5, 0.7, 0.8, 0.9, 0.95\}$. The target labels are generated as $Y_{\rm target} \sim {\rm Ber}\left( 1/(1+\exp{(\shortminus\beta\trans X)} \right)$, and the source labels are generated as $Y_{\rm source} \sim  {\rm Ber}\left( 1/(1+\exp{(\shortminus \theta\trans X)} \right)$, where $X \sim \mathcal{U}[-2, 2]^{150}$. The sample size $N$ for $(X_{\rm target}, Y_{\rm target})$ is varied across $\{20, 50, 80\}$, while the sample size for the source data $(X_{\rm source}, Y_{\rm source})$ is fixed at 800. Each dataset is paired with a validation set of the same size for hyperparameter selection.

Let $\rm grid_1$ denote a hyperparameter grid ranging from $0.0001$ to $1$ with $10$ log-spaced values, and let $\rm grid_2$ denote a hyperparameter grid ranging from $0.0001$ to $2$ with $20$ log-spaced values. The $\beta_{\rm WDRO}$ estimator is learned by selecting the best-performing hyperparameter on $\rm grid_1$ using validation data. For the $\mathcal{A}$-Trans-GLM learner \citep[Algorithm 1]{tian2023transglm}, the transferring step is optimized using $\rm grid_1$, and the debiasing step is optimized using $\rm grid_2$. For the KG-WDRO learner $\beta_{\rm KG}$ proposed in Theorem \ref{thm:logistic} with $p=1$, the prior $\theta$ is first learned from the source data using the vanilla WDRO method on $\rm grid_1$, followed by learning $\beta_{\rm KG}$ on $\rm grid_2$ with the learned $\theta_{\rm WDRO}$ as input.

The simulations are conducted on the parameter grid $$N \in \{20, 50, 80\} \times \rho \in \{0.3, 0.5, 0.7, 0.8, 0.9, 0.95\} \times s \in \{0.5, 1\},$$ with each configuration repeated $100$ times. The average results are reported.

\subsubsection{Simulation 2: Linear Regression}
In this simulation, the coefficients are generated in a high-dimensional correlated setting. The dimension of the coefficients is set to 100 and the components of the true coefficient-prior pair $(\beta, \theta)$ are generated using the multivariate normal scheme in \eqref{sim:scheme}, with component variance $\sigma^2 = 0.1$ and $\rho \in \{0.3, 0.5, 0.7, 0.8, 0.9, 0.95\}$. The target labels are generated as $Y_{\rm target} \sim \mathcal{N}(\beta\trans X , \sqrt{0.5})$, and the source labels are generated as $Y_{\rm source} \sim  \mathcal{N}(\theta\trans X , \sqrt{0.5})$, where $X\sim \mathcal{N}(\mathbf{0}, \Sigma)$ with \[
\Sigma_{i,j} = 
\begin{cases}
1 & \text{if } i = j, \\
0.3 & \text{if } i \neq j,
\end{cases}
\quad \text{for all } i, j = 1, 2, \dots, 100.
\] The sample size $N$ for $(X_{\rm target}, Y_{\rm target})$ is varied across $\{50, 70, 90\}$, while the sample size for the source data $(X_{\rm source}, Y_{\rm source})$ is fixed at 800. Each dataset is paired with a validation set of the same size for hyperparameter selection.

Let $\rm grid_1$ denote a hyperparameter grid ranging from $0.0001$ to $1$ with $10$ log-spaced values, and let $\rm grid_2$ denote a hyperparameter grid ranging from $0.0001$ to $1.5$ with $20$ log-spaced values. The $\beta_{\rm WDRO}$ estimator is learned by selecting the best-performing hyperparameter on $\rm grid_1$ using validation data. For the Trans-Ridge learner adapted from \citep[Algorithm 1]{li2021translasso}, the transferring step is optimized using $\rm grid_1$, and the debiasing step is optimized using $\rm grid_2$. For the KG-WDRO learner $\beta_{\rm KGstrong}$ proposed in Theorem \ref{thm:linear_l2} with $p=2$, and the $\beta_{\rm KGweak}$ learner proposed in Theorem \ref{thm:weak_l2}, the prior $\theta$ is first learned from the source data using the vanilla WDRO method on $\rm grid_1$, followed by learning $\beta_{\rm KGstrong}$ on $\rm grid_2$ with the learned $\theta_{\rm WDRO}$ as input. The $\lambda^{-1}$ grid for $\beta_{\rm KGweak}$ is 0.0001 to 8 with 20 log-spaced values.

The simulations are conducted on the parameter grid $$N \in \{50, 70, 90\} \times \rho \in \{0.3, 0.5, 0.7, 0.8, 0.9, 0.95\} \times s \in \{0.8, 1\},$$ with each configuration repeated $100$ times. The average results are reported.

\subsubsection{Simulation 3: Multiple Sites}
In this simulation, the coefficients are generated in a high-dimensional sparse setting. The dimension of the nonzero components is set to 50, which is then augmented with 100 zero components to introduce sparsity. The number of external source is 3, we generate the their coefficients $\theta_1,\theta_2,\theta_3$ using the scheme \eqref{sim:scheme}. We construct a linear combination, $\theta_S = a\theta_1 + b\theta_2 + c\theta_3$, and generate the target coefficient $\beta = \rho \theta_S + \varepsilon$, where $\varepsilon \sim N(0, (1-\rho^2)\mathrm{Var}(\theta_S))$, ensuring $\mathrm{Corr}(\beta, \theta_S) = \rho$. The target coefficient $\beta$ is then scaled to match the magnitude of $\theta_S$.

The target labels are generated as $Y_{\rm target} \sim \mathcal{N}(\beta\trans X , \sqrt{0.5})$, and the source labels are generated as $Y_{{\rm source},m} \sim  \mathcal{N}(\theta_m\trans X , \sqrt{0.5})$ for $m\in [3]$, where $X\sim \mathcal{N}(\mathbf{0}, \Sigma)$ with \[
\Sigma_{i,j} = 
\begin{cases}
1 & \text{if } i = j, \\
0.1 & \text{if } i \neq j,
\end{cases}
\quad \text{for all } i, j = 1, 2, \dots, 150.
\] The sample size for the target data ranges in $\{50,60,70\}$.

Let $\rm grid_1$ denote a hyperparameter grid ranging from $0.0001$ to $1$ with $15$ log-spaced values, and let $\rm grid_2$ denote a hyperparameter grid ranging from $0.0001$ to $3$ with $20$ log-spaced values. The $\beta_{\rm WDRO}$ estimator is learned by selecting the best-performing hyperparameter on $\rm grid_1$ using validation data. For the oracle Trans-Lasso learner \citep[Algorithm 1]{li2021translasso}, the transferring step is optimized using $\rm grid_1$, and the debiasing step is optimized using $\rm grid_2$ using all three source data. For the KG-WDRO learner $\beta_{\rm KG}$ proposed in Theorem \ref{thm:linear_l2} with $p=1$, the priors $\theta_1,\theta_2,\theta_3$ are first learned from the three source data using the vanilla WDRO method on $\rm grid_1$, followed by learning $\beta_{\rm KG}$ on $\rm grid_2$ with the learned $\theta_{1,\rm WDRO},\theta_{2,\rm WDRO},\theta_{3,\rm WDRO}$ as input.

The simulations are conducted on the parameter grid $$N \in \{50, 60, 70\} \times \rho \in \{0.3, 0.5, 0.7, 0.8, 0.9, 0.95\} \times [a,b,c] \in \{[1,-0.5,0.2],[1,1,1]\},$$ with each configuration repeated $100$ times. The average results are reported.


\section{Proof of Results in Regression.}
\begin{lemma}
\label{lem:1}
    Let $f_\beta:\mathbb{R}^d \to \mathbb{R}$ be defined as $\Delta\in\mathbb{R}^d \mapsto (\beta\trans \Delta)^2 - 2r(\beta)\beta\trans \Delta$ depending on some $\beta \in \mathbb{R}^d$ and let $r(\beta)$ be a non-negative real-valued function in $\beta$. Then the convex conjugate $f_\beta^*(\Delta^*):\mathbb{R}^d\to\mathbb{R}$ is given by \[
    f_\beta^*(\Delta^*) = \begin{cases}
        \dfrac{(\beta\trans \Delta^* + 2r(\beta)\lVert \beta \rVert_2^2)^2}{4\lVert \beta\rVert_2^4} & \text{if } \Delta^* \in \spn{\beta},\\
        +\infty & \text{otherwise}.
    \end{cases}
    \]Therefore the biconjugate $f_\beta^{**}(\Delta):\mathbb{R}^d\to \mathbb{R}$ of $f_\beta(\Delta)$ has representation:
    \[
    f_\beta^{**}(\Delta) = \sup_{\alpha\in \mathbb{R}} \left(\alpha (\beta\trans\Delta) - \dfrac{(\alpha+2r(\beta))^2}{4}\right).
    \]
\end{lemma}
\begin{proof}
    The convex conjugate $f_\beta^*(\Delta^*)$ is defined as \[
    f_\beta^*(\Delta^*) \coloneqq \sup_{\Delta\in\mathbb{R}^d} \big( \Delta^*{\trans}\Delta - (\beta\trans\Delta)^2 + 2r(\beta) (\beta\trans\Delta) \big),
    \]where $\Delta^*,\beta \in\mathbb{R}^d$ and $r(\beta)\in\mathbb{R}$ are taken as fixed values. Orthogonalize $\Delta = a\beta + \omega$ in the direction of $\beta$ with $a\in\mathbb{R}$, and $\omega\in\mathbb{R}^d$ such that $\beta\trans \omega = 0$. Then , we have $\Delta^*{\trans}\Delta = a\Delta^*{\trans}\beta + \Delta^*{\trans}\omega$, and the convex conjugate becomes \begin{align*}
         f^*(\Delta^*) &= \sup_{a,\omega}\big(a (\Delta^*{\trans}\beta) + \Delta^*{\trans}\omega - a^2\Vert \beta\Vert_2^4 + 2ar(\beta)\Vert\beta\Vert_2^2 \big)\\
         \rm{s.t}&\quad \beta\trans\omega = 0.
    \end{align*}
    Fixing $\omega$, the objective is a negative quadratic function in $a$, hence the objective in $a$ is bounded from above by a finite value. Now, if $\Delta^*$ is not orthogonal to $\omega$, the term $\sup_\omega \Delta^*{\trans}\omega$ is unbounded, and the convex conjugate $f^*(\Delta^*) = +\infty$. If $\Delta^*$ is orthogonal to $\omega$, then the convex conjugate attains finite value. Note that $\Delta^*{\trans} \omega = 0 \iff \Delta^* \in \spn{\beta}$. Hence condition on $\{\Delta^* = \alpha \beta\,;\alpha\in\mathbb{R}\}$, we have \begin{align*}
        f^*(\Delta^*) &= \sup_a \big(a(\Delta^*{\trans}\beta) - a^2 \Vert \beta\Vert_2^4 + 2r(\beta) a\Vert\beta\Vert_2^2 \big)\\
        &=\dfrac{\big(\Delta^*{\trans}\beta + 2r(\beta) \Vert \beta\Vert_2^2\big)^2}{4\Vert \beta\Vert_2^4},
    \end{align*} where the optimal solution is $a^* = \dfrac{\alpha + 2r(\beta)}{2\Vert \beta\Vert_2^2}$, and the coefficient $\alpha$ is given by the projection scalar $\alpha = \dfrac{\Delta^*{\trans}\beta}{\Vert\beta\Vert_2^2}$.

    The biconjuagte \[f^{**}(\Delta) = \sup_{\Delta^*} \big(\Delta\trans\Delta^* - f^*(\Delta^*)\big),\]
    is therefore bounded from below if and only if $\Delta^* \in \spn{\beta}$. Let $\Delta^* = \alpha \beta$ for some $\alpha \in \mathbb{R}$, then substituting we get the representation,
    \begin{align*}
        f^{**}(\Delta) &= \sup_{\alpha} \left( \Delta\trans (\alpha\beta) - \dfrac{\big(\beta\trans(\alpha \beta) + 2r(\beta) \Vert \beta\Vert_2^2\big)^2}{4\Vert\beta\Vert_2^4}\right)\\
        &= \sup_{\alpha} \left(\alpha (\Delta\trans\beta) - \dfrac{(\alpha + 2r(\beta))^2}{4} \right).
    \end{align*}
    It can be readily verified that $f^{**}(\Delta) = f(\Delta)$ as promised by the \textit{Fenchel-Moreau Theorem} (Theorem \ref{thm:fenchel-moreau}).
\end{proof}

\begin{lemma}
\label{lem:2}
    Let $g_\theta(\Delta):\mathbb{R}^d\to \mathbb{R}$ be defined as $\Delta\in\mathbb{R}^d \mapsto |\theta\trans \Delta|$ for some $\theta\in \mathbb{R}^d$. Then the convex conjugate $g_\theta^*(\Delta^*)$ is given by \[
    g_\theta^*(\Delta^*) = \begin{cases}
     0 &\text{if } \Delta^* = \alpha \theta \text{ and } |\alpha| \leq 1,\\
     +\infty &\text{otherwise.}
    \end{cases}
    \]Therefore the convex conjugate of the function $g(\Delta) \coloneqq \gamma \sum_{m=1}^Mg_{\theta_m}(\Delta)$ for some $\gamma > 0$ is given by \[
    g^*(\Delta^*) = \begin{cases}
        0 &\text{if } \Delta^* = \sum_{m=1}^M \alpha_m \theta_m \text{ and }, |\alpha_m| \leq \gamma \text{ for each }m, \\
        +\infty & \text{otherwise.}
    \end{cases}
    \]
\end{lemma}
\begin{proof}
    The convex conjugate is defined as \[
    g_\theta^*(\Delta^*) = \sup_{\Delta} \big(\Delta^*{\trans}\Delta - |\theta\trans\Delta| \big),
    \]again, orthogonalize $\Delta = a \theta + \omega$, where $a = \dfrac{\theta\trans \Delta}{\Vert \theta\Vert_2^2}$ and $\theta\trans\omega = 0$. Now by the change of variable $u \coloneqq \theta\trans\Delta$, the convex conjugate is now \begin{align*}
        g^*_\theta(\Delta^*) &= \sup_{u,\omega}\left(\dfrac{u}{\Vert \theta\Vert_2^2}(\Delta^*{\trans}\theta) + \Delta^*{\trans}\omega - |u| \right)\\
        \text{s.t }\quad &\theta\trans \omega = 0.
    \end{align*}Thus the convex conjugate $g_\theta^*(\Delta^*) = + \infty$ if $\Delta^* \not\in \spn{\theta}$. If $\Delta^* = \alpha \theta$ for some $\alpha\in\mathbb{R}$, then \begin{align*}
        g^*_\theta(\Delta^*) = g^*_\theta(\alpha\theta) &= \sup_{u} \left(\dfrac{u}{\Vert \theta\Vert_2^2 }\alpha \Vert \theta\Vert_2^2 - |u| \right)\\
        &= \sup_{u} \big( \alpha u - |u|\big)\\
        &= \begin{cases}
            0 &\text{if } |\alpha|\leq 1,\\
            +\infty &\text{otherwise,}
         \end{cases}
    \end{align*}
    where the last equality holds by noting that $\sup_u \alpha u - |u| = |\cdot|^*(\alpha)$ is the convex conjugate of the absolute value function (Proposition \ref{prop:examples_conjugates}). This proofs the convex conjugate of $g_\theta^*(\Delta^*)$. Now $g(\Delta) = \gamma \sum_{m=1}^M g_{\theta_m}(\Delta)=\gamma \bar{g}(\Delta)$, the convex conjugate of $\bar{g}(\Delta)$ is \begin{align*}
        \bar{g}^*(\Delta^*) &= (g_{\theta_1}+\ldots +g_{\theta_M})^*(\Delta^*) \\
        &=\inf_{\Delta^*} \big(g^*_{\theta_1}(\Delta^*_1) + \ldots + g_{\theta_M}^*(\Delta_M^*) \big) \,\,\, \text{ s.t}\,\,\, \Delta_1^* + \ldots +\Delta_M^* = \Delta^*,
    \end{align*}
    where the second line follows from the \textit{infimal convolution property} of sum of convex conjugates (Theorem \ref{thm:inf_conv}). We know that $\bar{g}^*$ is finite if and only if $g^*_{\theta_m}(\Delta_m^*) = 0$ for all $m\in[M]$, that is $\Delta_m^* = \alpha_m \theta_m$ for some $\alpha_m \in [-1,1]$ for all $m\in[M]$. Hence $\bar{g}^*(\Delta^*) = 0$ if and only if $\Delta^* = \sum_{m=1}^M\alpha_m \theta_m$ and $\alpha_m \in [-1,1]$ for all $m\in [M]$. Finally we can calculate the convex conjugate $g^*(\Delta^*) = (\gamma \bar{g})^*(\Delta^*) = 
 \gamma \bar{g}^*\left(\dfrac{\Delta^*}{\gamma} \right)$ by the scaling law of convex conjugates (Proposition \ref{prop:scaling_laws}) given $\gamma > 0$. This concludes the proof.
\end{proof}

We now give the proof to Theorem \ref{thm:linear_l2}.
\begin{proof}[\textbf{Proof of Theorem \ref{thm:linear_l2}}]
    Let $r(\beta) \coloneqq y - \beta\trans x$. Then first consider the cost function\[
    c_2\big((x,y),(u,v)\big) \coloneqq \Vert x-u\Vert_q^2 + \infty\cdot|y-v| +  d(\theta_1\trans x - \theta_1\trans u) + \ldots +  d(\theta_M\trans x - \theta_M\trans u).
    \]where we replaced the transferring strength from $+\infty$ to a finite-valued distance function $d(x):\mathbb{R}\to\mathbb{R}$ that is a monotone function in $|x|$, with $d(0)=0$. We will then let $d(x) \to \infty$ except at $x=0$. Then the supremum function \begin{align*}
        \phi_\gamma(x,y;\beta) = 
        \sup_{(u,v)\in \mathbb{R}^{d+1}} \big\{ \ell(u,v;\beta) - \gamma c\big((u,v),(x,y) \big)\big\},
    \end{align*}is finite if and only if $v = y$. Then, we have\begin{align*}
        &l\left( u,v;\beta \right) -\gamma c\left( \left( u,v\right) ,(x,y\right)
) \\
=&\left(y-\beta\trans u\right) ^{2}-\gamma \left\Vert x-u\right\Vert
_{q}^{2}-\gamma d\left( \theta_1\trans x-\theta_1\trans u\right)  - \ldots - \gamma d(\theta_M\trans x - \theta_M\trans u).
    \end{align*}Denote by $\Delta \coloneqq u-x$, we get
\begin{align*}
    &l\left( u,v;\beta \right) -\gamma c\left( \left( u,v\right) ,(x,y\right)) \\
    =&r(\beta)^2 + \big\{(\beta\trans \Delta)^2 - 2r(\beta)\beta\trans\Delta - \gamma\Vert\Delta\Vert_q^2 - \gamma d(\theta_1\trans\Delta) - \ldots-\gamma d(\theta_M\trans \Delta) \big\}.
\end{align*}
Consider the objective in $\Delta$ \begin{align*}
    &\sup_{\Delta} \big\{(\beta\trans \Delta)^2 - 2r(\beta)\beta\trans\Delta - \gamma\Vert\Delta\Vert_q^2 - \gamma d(\theta_1\trans\Delta) - \ldots-\gamma d(\theta_M\trans \Delta) \big\}\\
    \coloneqq &\sup_{\Delta} \big\{ f_\beta(\Delta) - g(\Delta)\big\},
\end{align*}
where we let $f_\beta(\Delta)\coloneqq (\beta\trans \Delta)^2 - 2r(\beta)\beta\trans\Delta$ and $g(\Delta) \coloneqq\gamma\Vert\Delta\Vert_q^2 + \gamma d(\theta_1\trans\Delta) + \ldots +\gamma d(\theta_M\trans \Delta)$. This is a convex + concave optimization, we express the convex part of $f_\beta(\Delta)$ as a supremum of infinitely many affine functions. Then by Lemma \ref{lem:1}, we have $f_\beta(\Delta) = f_\beta^{**}(\Delta) = \sup_{\alpha\in \mathbb{R}} \left(\alpha (\beta\trans\Delta) - \dfrac{(\alpha+2r(\beta))^2}{4}\right)$, then we may write 
\begin{align*}
\label{eqn:duality}
    &\sup_{\Delta} \big\{f_\beta(\Delta) -g(\Delta) \big\}\\
    =&\sup_{\Delta} \left\{ \sup_{\alpha\in \mathbb{R}} \left(\alpha (\beta\trans\Delta) - \dfrac{(\alpha+2r(\beta))^2}{4}\right) - g(\Delta)\right\}\\
    =&\sup_\alpha \left\{ \sup_\Delta \big(\Delta \trans(\alpha \beta)-g(\Delta)\big) - \dfrac{\big(\alpha + 2r(\beta)\big)^2}{4} \right\}\\
    =&\sup_\alpha \left\{ g^*(\alpha\beta) - \dfrac{\big(\alpha + 2r(\beta)\big)^2}{4}\right\}, \tag{Toland's Duality}
\end{align*}where $g^*$ is the convex conjugate of $g$. Let $g(\Delta) \coloneqq g_1(\Delta) +g_\theta(\Delta)$, with $g_1(\Delta) = \gamma \Vert \Delta\Vert_q^2$ and $g_\theta(\Delta) \coloneqq \gamma\sum_{m=1}^M d(\theta_m\trans \Delta)$. Then we can compute the convex conjugate of $g$ using the \textit{infimal convolution property} (Theorem \ref{thm:inf_conv}). Then \begin{align*}
    g^*(\Delta^*) &=\inf_{\Delta^*_1+\Delta^*_2 = \Delta^*} \big( g_1^*(\Delta_1^*) + g_\theta^*(\Delta_2^*)\big). 
\end{align*}
We know that $g_1^*(\Delta_1^*) = \dfrac{1}{4\gamma}\Vert\Delta_1^*\Vert_p^2$, where $p^{-1}+q^{-1}=1$ (Proposition \ref{prop:examples_conjugates}). Now suppose $d(x) = \lambda|x|$ for some $\lambda>0$, by Lemma \ref{lem:2}, we have,\[
    g_\theta^*(\Delta_2^*) = \begin{cases}
        0 &\text{if } \Delta_2^* = \sum_{m=1}^M \alpha_m \theta_m \text{ and }, |\alpha_m| \leq \gamma\lambda \text{ for each }m, \\
        +\infty & \text{otherwise.}
    \end{cases}
    \]
Then the convex conjugate $g^*(\Delta^*)$ is \begin{align*}
    g^*(\Delta^*) &= \inf_{\Delta_2^*} g_1^*(\Delta^*-\Delta_2^*),\\
    \text{s.t }\,\Delta_2^* = \sum_{m=1}^M \alpha_m &\theta_m \text{ and }, |\alpha_m| \leq \gamma\lambda \text{ for each }m,
\end{align*}which is equivalently,
\begin{align*}
    g^*(\Delta^*) &= \dfrac{1}{4\gamma}\inf_{\boldsymbol{\alpha}}\left\Vert \Delta^* - \sum_{m=1}^M\alpha_m\theta_m\right\Vert_p^2,\\
    &\text{s.t }\,|\alpha_m| \leq \gamma\lambda \text{ for each }m.
\end{align*}Letting $\lambda \to\infty$, we recover the cost function $c_{2,\infty}$, and when $\lambda \to \infty$, each $\alpha_m$ is now free in $\mathbb{R}$. Then we have $g^*(\Delta^*) = \dfrac{1}{4\gamma} \inf_{\vartheta \in \Theta} \Vert\Delta^* -\vartheta\Vert_p^2$, with $\Theta \coloneqq \spn{\{\theta_1,\ldots,\theta_M\}}$, the validity of this tactic follows from \citep[Theorem 1, Section 13.1]{luenberger2008linear}. Then we have $g^*(\alpha\beta) = \dfrac{1}{4\gamma}\inf_{\vartheta\in\Theta} \Vert \alpha \beta - \vartheta\Vert_p^2.$ Suppose $\alpha \neq 0,$ then dividing by $\alpha$, we get \[
g^*(\alpha\beta) = \dfrac{\alpha^2}{4\gamma}\inf_{\vartheta\in\Theta}\Vert \beta - \vartheta\Vert_p^2.
\]If $\alpha = 0$, then $g^*(\alpha\beta) = g^*(\boldsymbol{0}) = \dfrac{1}{4\gamma}\inf_\vartheta \Vert\vartheta\Vert_p^2 = 0$, so the representation $g^*(\alpha\beta) = \dfrac{\alpha^2}{4\gamma}\inf_{\vartheta\in\Theta}\Vert \beta - \vartheta\Vert_p^2,$ is valid for all $\alpha\in\mathbb{R}$. Therefore following the proof of \citep[Theorem 1]{blanchet2019rwpi}, \begin{align*}
    \phi_\gamma(x,y;\beta) &= r(\beta)^2 + \dfrac{1}{4}\sup_{\alpha} \left\{\dfrac{\alpha^2}{\gamma}\inf_{\vartheta\in\Theta} \Vert \beta - \vartheta\Vert_p^2-\big(\alpha +  2r(\beta)\big)^2\right\}\\
    &=\dfrac{1}{4}\sup_{\alpha}\left\{\left(\dfrac{\inf_\vartheta\Vert \beta - \vartheta\Vert_p^2}{\gamma}-1\right)\alpha^2 - 4r(\beta)\alpha\right\}\\
    &= \begin{cases}
        \dfrac{r(\beta)^2\gamma}{\gamma-\inf_\vartheta\Vert\beta-\vartheta\Vert_p^2} &\text{if }\inf_\vartheta\Vert\beta-\vartheta\Vert_p^2 \leq \gamma,\\
        +\infty&\text{otherwise.}
    \end{cases}
\end{align*}
Then the minimization objective can be simplified as \begin{align*}
  &\inf_{\beta \in \mathbb{R}^d} \min_{\gamma \geq 0} 
\left\{
\gamma \delta + \dfrac{1}{n} \sum_{i=1}^N \phi_\gamma(x_i, y_i; \beta)
\right\}\\
= &\inf_{\beta} \inf_{\gamma \geq \inf_\vartheta \lVert \beta - \vartheta \rVert_p^2} 
\left\{
\gamma \delta + \dfrac{1}{n} \sum_{i=1}^N \dfrac{r_i(\beta)^2 \gamma}{\gamma - \inf_\vartheta \lVert \beta - \vartheta \rVert_p^2}
\right\}\\
=&\inf_\beta \inf_{\gamma \geq \inf_\vartheta \lVert \beta - \vartheta \rVert_p^2} 
\left\{
\gamma \delta + \text{MSE}(\beta) \dfrac{ \gamma}{\gamma - \inf_\vartheta \lVert \beta - \vartheta \rVert_p^2}
\right\}\\
=&\inf_\beta \left(\sqrt{\text{MSE}(\beta)} + \sqrt{\delta}\inf_\vartheta \Vert \beta-\vartheta\Vert_p\right)^2,
\end{align*}where the last equality follows because $\gamma \delta + \dfrac{1}{n} \text{MSE}(\beta) \dfrac{ \gamma}{\gamma - \inf_\vartheta \lVert \beta - \vartheta \rVert_p^2}$ is a convex function in $\gamma$ that tends to $+\infty$ approaching the boundaries $\inf_\vartheta \Vert \beta-\vartheta\Vert_p^2$ and $+\infty$, so the optimization over $\gamma$ can be solved using first-order condition. Then by Proposition \ref{prop:duality}, strong duality holds and, \[\inf_{\beta} \sup_{\mathbb{P}:\mathcal{D}_{c_2}(\mathbb{P},\mathbb{P}_n)\leq \delta} \mathbb{E}_{\mathbb{P}}\left[(Y-\beta\trans X)^2\right] = \inf_{\beta,\vartheta}\left(\sqrt{\text{MSE}(\beta)} + \sqrt{\delta} \Vert \beta-\vartheta\Vert_p\right)^2.\]
This reduces the infinite-dimensional optimization to a finite-dimensional problem, where we interchanged $\inf_\vartheta$ and the quadratic function, since the quadratic function is monotone increasing on the positive reals.
\end{proof}

The next proof is to Theorem \ref{thm:weak_l2} with the weak transferring cost function $c_{2,\lambda}\big((x,y),(u,v) \big) = \Vert x-u\Vert_2^2 + \lambda (\theta\trans x - \theta\trans u)^2 + \infty \cdot |y-v|$ with some $\lambda>0$. The statements generalizes to multi-sites by first considering orthogonalizing the prior knowledge $\{\theta_1,\ldots,\theta_M\}$.

\begin{proof}[\textbf{Proof of Theorem \ref{thm:weak_l2}}]
    Following the proof of Theorem \ref{thm:linear_l2}, we solve the optimization problem \[
    \sup_{\Delta\in \mathbb{R}^d} \big((\beta\trans \Delta)^2-2r(\beta)\beta\trans \Delta - \gamma \Vert \Delta\Vert_2^2 - \gamma\lambda (\theta\trans\Delta)^2\big),
    \]where we recall that $\gamma$ is the dual-variable in statement of Proposition \ref{prop:duality}, $\lambda>0$ is the transferring strength, $\theta\in\mathbb{R}^d$ is the prior knowledge, and $r(\beta) = (y-\beta\trans x)^2$ is the residual in $\beta$. 

    Then let $\mathbb{O}$ be an orthogonal matrix, whose first column is $\theta/\Vert \theta\Vert_2$, then use $\widetilde{\Delta} \coloneqq \mathbb{O}^{-1}\Delta.$ The objective function now becomes \[
    (\beta\trans \mathbb{O}\widetilde{\Delta})^2 - 2r(\beta) \beta\trans \mathbb{O}\widetilde{\Delta} -\gamma\Vert \widetilde{\Delta}\Vert_2^2-\gamma \lambda\Vert\theta\Vert_2^2\widetilde{\Delta}_1^2,
    \]where the last term follows because $\theta\trans \mathbb{O} = (\Vert\theta\Vert_2,0,\ldots,0)$, and $\widetilde{\Delta}_1$ denotes the first component of $\widetilde{\Delta}$. Now define \[
    D = \diag{\left\{\sqrt{\lambda\Vert \theta\Vert_2^2+1},1,\ldots,1\right\}},
    \]and consider the change of variable $\bar{\Delta} = D\widetilde{\Delta}$, then the last two terms become
    \begin{align*}
    \Vert \widetilde{\Delta}\Vert_2^2 + \lambda\Vert\theta\Vert_2^2\widetilde{\Delta}_1^2=
    \Vert D^{-1}\bar{\Delta}\Vert_2^2 + \lambda \Vert\theta\Vert_2\dfrac{\bar{\Delta}_1^2}{{\lambda\Vert\theta\Vert_2^2+1}}
    =\sum_{i=1}^d \bar{\Delta}_d^2 = \Vert\bar{\Delta}\Vert_2^2.
    \end{align*}
    Therefore, the objective becomes \begin{align*}
        &\sup_{\bar{\Delta}} \big((\beta\trans \mathbb{O}D^{-1}\bar{\Delta})^2 - 2r(\beta)\beta\trans\mathbb{O}D^{-1}\bar{\Delta} - \gamma \Vert\bar{\Delta}\Vert_2^2\big) \\
        =&\sup_{\bar{\Delta}}\big(\Vert \beta\trans \mathbb{O}D^{-1}\Vert_2^2\Vert\bar{\Delta}\Vert_2^2 -2r(\beta)\Vert \beta\trans \mathbb{O}D^{-1}\Vert_2 \Vert\bar{\Delta}\Vert_2 - \gamma\Vert\bar{\Delta}\Vert_2^2)\\
        =&\sup_{\bar{\Delta}}\big( (\Vert \beta\Vert_{\Psi_\lambda} - \gamma)\Vert\bar{\Delta}\Vert_2^2 - 2r(\beta) \Vert\beta\Vert_{\Psi_\lambda} \Vert\bar{\Delta}\Vert_2\big)
    \end{align*}
    which has finite optimal value $\dfrac{r(\beta)^2\Vert \beta\Vert_{\Psi_\lambda}^2}{\gamma - \Vert \beta \Vert_{\Psi_\lambda}^2}$ whenever $\gamma \geq \Vert \beta \Vert_{\Psi_\lambda}$, with $\Psi_\lambda$ denoting the positive-definite symmetric matrix, \[
    \Psi_\lambda = I_d - \dfrac{1}{ \Vert \theta\Vert_2^2 + \lambda^{-1}}\theta \theta\trans,
    \]that is independent of the choice of $\mathbb{O}$. The first equality follows because we applied Cauchy-Schwarz inequality and since $\bar{\Delta}\in\mathbb{R}^d$ is free, there is some $\bar{\Delta}$ that achieves equality. The rest of the proof follows exactly along the proof of Theorem \ref{thm:linear_l2} by completing the optimization over the dual problem using Proposition \ref{prop:duality}.
\end{proof}

\section{Proof of Results in Classification.}
\begin{lemma}
\label{lem:3}
    Consider the convex function $h_\beta(x):\mathbb{R}^d\to\mathbb{R}$ by $x\in\mathbb{R}^d \mapsto \log{(1+\exp{(\shortminus\beta\trans x)})}$, for some $q>0$ and $x'\in \mathbb{R}$. Then for every $\gamma > 0$, the constraint optimization problem $H_\beta(x')$ defined as,
    \begin{align*}
        \sup_{x\in\mathbb{R}^p} \quad &h_\beta(x) - \gamma \lVert x'-x\rVert_q,\\
        \rm s.t \quad &\theta\trans (x'-x) = 0,
    \end{align*}
    has optimal objective value, \[
    H_\beta(x') = \begin{cases}
        h_\beta(x') & \text{if } \inf_{\kappa\in\mathbb{R}} \lVert \beta - \kappa \theta\rVert_p \leq \gamma,\\
        +\infty & \text{otherwise,}
    \end{cases}
    \]where $p,q\in[1,\infty)$ with $p^{-1} + q^{-1} = 1$.
\end{lemma}
\begin{proof}
    This lemma is a simple extension of \citep[Lemma 1]{abadeh2015drologistic}. Following their proof, it is shown that \[
    h_\beta(x) = h_\beta^{**}(x) = \sup_{0\leq \alpha \leq 1} \big((\alpha \beta)\trans x - \bar{h}^*(\alpha)\big),
    \]where \[\bar{h}^*(\alpha) = \begin{cases}
        \alpha \log{(\alpha)} + (1-\alpha)\log{(1-\alpha)} &\text{if } \alpha\in[0,1],\\
        +\infty & \text{otherwise},
    \end{cases}\]
    is the convex conjugate of the function $\log{\big(1+e^{\shortminus x}\big)}$ (Proposition \ref{prop:examples_conjugates}). Then it is shown that the objective $H_\beta$ must has representation \begin{align*}
        \sup_{0\leq \alpha\leq 1}\inf_{\Vert q\Vert_p \leq \gamma}\sup_x &\big( (\alpha \beta + q)\trans x - \bar{h}^*(\alpha) -q\trans x'\big),\\
        &\text{s.t} \,\,\, \theta\trans(x-x') = 0.
    \end{align*}

    Fixing $\alpha$ and $q$, then the inner maximization in $x$
    \begin{align*}
       \sup_x &\big( (\alpha \beta + q)\trans x  -q\trans x'\big),\\
        &\text{s.t} \,\,\, \theta\trans(x-x') = 0,
    \end{align*}
    has solution $(\alpha \beta)\trans x'$ subject to $\alpha \beta + q = \mu\theta$ for some $\mu\in \mathbb{R}$ derived using the first-order condition of the Lagrangian duality or $+\infty$ otherwise. Then condition on $\{\alpha \beta + q = \mu\theta|\mu\in\mathbb{R}\}$, the objective has representation \begin{align*}
        H_\beta(x') &= \sup_{0\leq \alpha \leq 1} \inf_{\Vert q\Vert_p\leq \gamma} \big((\alpha \beta)\trans x' - \bar{h}^*(\alpha) \big) \,\,\, \text{s.t}\,\,\,q = \mu\theta - \alpha \beta \\
        &=\sup_{0\leq \alpha\leq 1}\inf_{\mu,\Vert \mu\theta - \alpha \beta \Vert_p\leq \gamma} \big((\alpha \beta)\trans x' - \bar{h}^*(\alpha) \big).
    \end{align*}
    Consider the constraint $\Vert \mu\theta - \alpha \beta \Vert_p\leq \gamma$ over $\mu$. Suppose $\alpha > 0$, then dividing by $-\alpha$, we get the equivalent constraint $\left\{|\alpha|\left\Vert \beta - \dfrac{\mu}{\alpha}\theta\right\Vert_p\right\} \leq \gamma$ over $\mu$. Defining the change of variable $\kappa \coloneqq \dfrac{\mu}{\alpha}$, then since the Lagrange multiplier $\mu \in \mathbb{R}$ is free, we have $\kappa$ is free, and the constraint becomes $ \inf_\kappa|\alpha|\Vert \beta - \kappa \theta\Vert_p \leq \gamma$ over $\kappa \in \mathbb{R}$. If $\alpha = 0$, then $\inf_\mu \Vert\mu\theta - 0\Vert = 0 = 0\cdot \inf_{\kappa}\Vert \beta - \kappa \theta\Vert_p$. So the equivalent constraint $\inf_\kappa |\alpha| \Vert \beta -\kappa \theta\Vert_p\leq \gamma$ is valid for all $\alpha \in [0,1]$. Then condition on $\{\alpha \beta + q = \mu\theta|\mu\in\mathbb{R}\}$, the objective becomes, \begin{align*}
        H_\beta(x') &= \sup_{0\leq \alpha\leq 1}\big((\alpha\beta)\trans x' - \bar{h}^*(\alpha)\big) \text{ s.t } \sup_{0\leq \alpha\leq 1}|\alpha| \inf_\kappa \Vert\beta-\kappa \theta\Vert_p \leq \gamma,\\
        & = \sup_{0\leq \alpha\leq 1}\big((\alpha\beta)\trans x' - \bar{h}^*(\alpha)\big) \text{ s.t } \inf_\kappa \Vert\beta-\kappa \theta\Vert_p \leq \gamma,
    \end{align*}
    Recognizing that \[\sup_{0\leq \alpha\leq 1}\big((\alpha \beta)\trans x' - \bar{h}^*(\alpha)\big) = \sup_{0\leq \alpha\leq 1} \alpha(\beta\trans x') - \bar{h}^*(\alpha) = \bar{h}^{**}(\beta\trans x') = h_\beta(x'),\]we get\[
     H_{\beta}(x')=
    \begin{cases}
        h_\beta(x') & \text{if }\inf_\kappa \Vert \beta-\kappa\theta\Vert_p\leq \gamma,\\
        +\infty &\text{otherwise.}
    \end{cases}
    \]
\end{proof}

The above Lemma \ref{lem:3} is easily generalized to incorporate multiple orthogonality constraints $\{\theta_m\trans (x'-x) =0\,;m\in [M]\}$ using the exact same Lagrangian formulation. Again, recall $\Theta = \spn{\{\theta_1,\ldots,\theta_M\}}$. Thus the optimal objective value under multiple constraints becomes \[
H_\beta(x') = \begin{cases}
   h_\beta(x') &\text{if }\inf_{\vartheta\in\Theta} \Vert \beta -\vartheta \Vert_p \leq \gamma,\\
   +\infty & \text{otherwise.}
\end{cases}
\]
We now give the proof to Theorem \ref{thm:logistic}.

\begin{proof}[\textbf{Proof of Theorem \ref{thm:logistic} for Logistic Loss}]
    Using Proposition \ref{prop:duality}, we apply the strong duality, and consider the inner optimization problem \begin{align*}
        &\sup_{\mathbb{P}:\mathcal{D}_{c_1,\infty}(\mathbb{P},\mathbb{P}_n)\leq \delta} \mathbb{E}_{\mathbb{P}}\left[\log{\left(1+e^{-Y\beta\trans X}\right)}\right]  \\
    =&\begin{cases}
        \inf_{\gamma \geq 0}\left\{\gamma\delta + \dfrac{1}{n}\sum_{i=1}^N \sup_{u\in\mathbb{R}^d}\left(\log{\left(1+e^{-y_i\beta\trans u} \right)}-\gamma \Vert x_i-u\Vert_q\right) \right\}, &\\
        \text{s.t }\,\theta_m\trans (x_i-u) = 0, \text{ for all }m\in [M] \text{ and }i\in[N].
    \end{cases}
    \end{align*}
    
    
    For each $i\in[N]$, we apply Lemma \ref{lem:3} to the maximization problem, \[
    H_\beta(x_i)=\begin{cases}
         \sup_{u\in\mathbb{R}^d}\left(\log{\left(1+e^{-y_i\beta\trans u} \right)}-\gamma \Vert x_i-u\Vert_q\right), &\\
        \text{s.t }\,\theta_m\trans (x_i-u) = 0, \text{ for all }m\in [M].
    \end{cases}
    \]
    which has solution \[
    \begin{cases}
   \log{\left(1+e^{-y_i\beta\trans x_i} \right)} &\text{if }\inf_{\vartheta\in\Theta} \Vert \beta -\vartheta \Vert_p \leq \gamma,\\
   +\infty & \text{otherwise.}
\end{cases}
    \]
    Therefore, the maximization problem $ \sup_{\mathbb{P}:\mathcal{D}_{c_1,\infty}(\mathbb{P},\mathbb{P}_n)\leq \delta} \mathbb{E}_{\mathbb{P}}\left[\log{\left(1+e^{-Y\beta\trans X}\right)}\right]$ is bounded from above if and only if $\gamma \geq \inf_{\vartheta} \Vert \beta-\vartheta\Vert_p$. Under this condition, this reduces the inner optimization problem, \begin{align*}
    \sup_{\mathbb{P}:\mathcal{D}_{c_1,\infty}(\mathbb{P},\mathbb{P}_n)\leq \delta} \mathbb{E}_{\mathbb{P}}\left[\log{\left(1+e^{-Y\beta\trans X}\right)}\right] &= \inf_{\gamma\geq \inf_{\vartheta} \Vert \beta-\vartheta\Vert_p}\left\{\gamma\delta + \dfrac{1}{n}\sum_{i=1}^N \log{\left(1+e^{-y_i\beta\trans x_i} \right)} \right\} \\
    &=\dfrac{1}{n}\sum_{i=1}^N \log{\left(1+e^{-y_i\beta\trans x_i} \right)} + \delta \inf_{\vartheta} \Vert \beta-\vartheta\Vert_p.
\end{align*}
This concludes the proof.
\end{proof}

We now give the proof to the maximum margin classifier using the hinge loss.
\begin{proof}[\textbf{Proof of Theorem \ref{thm:logistic} for Hinge Loss}]
    As in the case to the proof of Theorem \ref{thm:linear_l2}, we first consider the relaxed cost function $$c_{1}((x,y),(u,v)) = \Vert x-u\Vert_q +\infty\cdot |y-v| + \lambda\cdot \sum_{m=1}^M |\theta_m\trans x - \theta_m\trans u|,$$ where we relaxed the transferring strength from $+\infty$ to some finite value $\lambda > 0$. We will then let $\lambda \to +\infty$. Again, by strong duality, we can solve the worst case hinge loss by solving the dual problem \[
    \inf_{\gamma\geq 0}\left\{ \gamma\delta+ \dfrac{1}{n}\sum_{i=1}^N \sup_u \left( (1-y_i\beta\trans u)^+ - \gamma \Vert u-x_i \Vert_q - \gamma \lambda \sum_{m=1}^M |\theta_m\trans (x_i-u)|\right)\right\}.
    \]Let $\Delta \coloneqq u-x$, then we have \begin{align*}
        &\sup_u \left( (1-y\beta\trans u)^+ - \gamma \Vert u-x \Vert_q - \gamma \lambda \sum_{m=1}^M |\theta_m\trans (x-u)|\right) \\
        =&\sup_\Delta \left( (1-y\beta\trans (\Delta+x))^+ - \gamma \Vert \Delta \Vert_q - \gamma \lambda \sum_{m=1}^M |\theta_m\trans \Delta|\right)\\
        =& \sup_{\Delta}\sup_{0\leq \alpha\leq 1}\left( \alpha(1-y\beta\trans (\Delta+x)) - \gamma \Vert \Delta \Vert_q - \gamma \lambda \sum_{m=1}^M |\theta_m\trans \Delta|\right)\\
        =&\sup_{0\leq \alpha\leq 1} \sup_{\Delta}\left( -\alpha y \beta\trans\Delta - \gamma\Vert\Delta\Vert_q - \gamma\lambda \sum_{m=1}^M|\theta_m\trans \Delta| +\alpha (1-y\beta\trans x)\right).
    \end{align*}
    Where in the second equality we used $x^+ = \sup_{0\leq \alpha\leq 1}\alpha x$. Fixing $\alpha$, consider the inner minimization in $\Delta$,
    \begin{align*}
        \sup_{\Delta}\left( -\alpha y \beta\trans\Delta - \gamma\Vert\Delta\Vert_q - \gamma\lambda \sum_{m=1}^M|\theta_m\trans \Delta|\right) 
        =g^*(-\alpha y \beta),
    \end{align*}
    where $g^*(\Delta^*)$ is the convex conjugate of $g(\Delta) \coloneqq \gamma\Vert\Delta\Vert_q + \gamma\lambda \sum_{m=1}^M|\theta_m\trans \Delta|$. Set $\gamma\Vert\Delta_1\Vert_q \eqqcolon g_1(\Delta_1)$ and $\gamma\lambda \sum_{m=1}^M|\theta_m\trans \Delta_2|\eqqcolon g_2(\Delta_2)$, then by the \textit{infimal convolution property} of convex conjugates (Theorem \ref{thm:inf_conv}), we know that \[
    g^*(\Delta^*) = \inf_{\Delta^*_1+\Delta^*_2 = \Delta^*}\big(g_1^*(\Delta^*_1)+g^*_2(\Delta^*_2) \big).
    \]From Lemma \ref{lem:2}, we know that if $g^*(\Delta^*)$ is finite, then $g_2^*(\Delta_2^*) = 0$ subject to $\Delta_2^* = \sum_{m=1}^M\alpha_m\theta_m$ and $|\alpha_m|\leq \lambda\gamma$ for all $m\in[M]$. Now it is well known that (Proposition \ref{prop:examples_conjugates}), \[
    g_1^*(\Delta_1^*) = (\gamma\Vert \,\cdot\,\Vert_q)^*(\Delta_1^*) = I_{\{\Vert \Delta_1^*\Vert_p \leq \gamma\}}(\Delta_1^*),
    \]where $I_C(x)$ denotes the convex indicator on the set $C$. Therefore, letting $\lambda\to \infty$, the constraints $\{|\alpha_m|\leq \lambda\gamma |m\in[M]\}$ is redundant, and we have \[
    g^*(\Delta^*) = 
    \begin{cases}
        0 &\text{if }\inf_{\vartheta\in\Theta}\Vert \Delta^* -  \vartheta\Vert_p\leq \gamma,\\
        +\infty &\text{otherwise,}
    \end{cases}
    \]where we let $\Theta \coloneqq \spn{\{\theta_1,\ldots,\theta_M\}}$. Therefore, $g^*(-\alpha y \beta)$ is finite if and only if $\inf_\vartheta \Vert -\alpha y \beta -\vartheta\Vert_p \leq \gamma$. Now $y =\pm 1$, so we can remove $-y$, and this leaves us the condition that $\inf_\vartheta \Vert \alpha \beta - \vartheta \Vert_p \leq \gamma$, which is equivalent to $\alpha \inf_\vartheta \Vert \beta - \vartheta\Vert_p\leq \gamma$ for all $\alpha \in [0,1]$, including $\alpha = 0$. Taking supremum over $\alpha \in [0,1]$, the final condition is $\inf_\vartheta \Vert \beta-\vartheta\Vert_p\leq \gamma$. Therefore, assuming the dual problem is bounded from above, it reduces as\begin{align*}
        &\sup_{0\leq \alpha\leq 1} \sup_{\Delta}\left( -\alpha y \beta\trans\Delta - \gamma\Vert\Delta\Vert_q - \gamma\lambda \sum_{m=1}^M|\theta_m\trans \Delta| +\alpha (1-y\beta\trans x)\right) \\
        =&\sup_{0\leq \alpha\leq 1}\left(I_{\{\inf_\vartheta \Vert \beta-\vartheta\Vert_p\leq \gamma\}} + \alpha(1-y\beta\trans x) \right)\\
        =&(1-y\beta\trans x)^+ \quad \text{given} \quad\inf_\vartheta \Vert \beta-\vartheta\Vert_p\leq \gamma.
    \end{align*}
    Finally, the dual form of the distributionally robust optimization problem is \begin{align*}
        &\inf_\beta\inf_{\gamma\geq 0}\left\{ \gamma\delta+ \dfrac{1}{n}\sum_{i=1}^N \sup_u \left( (1-y_i\beta\trans u)^+ - \gamma \Vert u-x_i \Vert_q - \gamma \lambda \sum_{m=1}^M |\theta_m\trans (x_i-u)|\right)\right\}\\
        =&\inf_\beta \inf_{\gamma\geq \inf_\vartheta \Vert \beta-\vartheta\Vert_p}\left\{\gamma\delta + \dfrac{1}{n}\sum_{i=1}^N (1-y_i\beta\trans x_i)^+ \right\}\\
        =&\inf_{\beta,\vartheta} \left\{\dfrac{1}{n}\sum_{i=1}^N (1-y_i\beta\trans x_i)^+ + \delta\Vert \beta-\vartheta\Vert_p\right\}.
    \end{align*}
    This completes the proof.
\end{proof}

\section{Proof of Results in Mahalanobis Norm Regularization}
\begin{proof}[\textbf{Proof of Corollary \ref{cor:linear_l2}}]
    This is a direct consequence of the convex conjugate of $\Vert x\Vert_{\Lambda}^2$ given in Proposition \ref{prop:generalized_adaptive}.
\end{proof}

Define the cost function $c_{1,\infty}^\Lambda\big((x,y),(u,v)\big) \coloneqq \Vert x-u\Vert_{\Lambda} + \infty\cdot|y-v| + \infty\cdot\sum_{m=1}^M|\theta_m\trans x - \theta_m\trans u|$. 

\begin{corollary}[Theorem \ref{thm:logistic}]
\label{cor:logistic}
    Suppose the loss function $\ell(X,Y;\beta)$ is either the logistic loss $\log{\left(1+e^{-Y\beta\trans X}\right)}$ or the hinge loss $(1-Y\beta\trans X)^+$, then for any $\Lambda\in\mathbb{S}_+^{d\times d}$  we have
\begin{align*}
    &\inf_{\beta\in\mathbb{R}^d} \sup_{\mathbb{P}:\mathcal{B}_\delta(\mathbb{P}_N^X;c^\Lambda_{1,\infty})} \mathbb{E}_{\mathbb{P}}\left[\ell(X,Y;\beta)\right]\\
   =& \inf_{\beta\in\mathbb{R}^d,\vartheta\in\Theta}\dfrac{1}{N}\sum_{i=1}^N \ell(x_i,y_i;\beta) + \delta \Vert \beta - \vartheta \Vert_{\Lambda^{\shortminus 1}}.
\end{align*}
\end{corollary}
\begin{proof}
    For the logistic loss case, this is a direct consequence of the dual norm of $\Vert x\Vert_{\Lambda}$, for the hinge loss case this is a direct consequence of the convex conjugate of $\Vert x\Vert_{\Lambda}$. Both given by Proposition \ref{prop:generalized_adaptive}.
\end{proof}

\section{Useful Results on Convex Conjugation}
In this section we review some results on the concept of convex conjugates that repeatedly come up in the proofs. For more details on convex conjugations, the interested readers can consult \citep[Sections 12 \& 16]{rockafellar1970convex}.

\begin{definition}[Convex Conjugate]
    Let $f:\mathbb{R}^d\to\mathbb{R}$ be a real-valued function on the Euclidean space, then the convex conjugate of $f$ is the function $f^*:\mathbb{R}^d\to \mathbb{R}$ that evaluates $x^*\in\mathbb{R}^n$ by \[
    f^*(x^*) = \sup_{x\in\dom{(f)}}\big( x^*{\trans}x -f(x)\big).
    \]
\end{definition}
This is also called the \textit{Legendre transformation} of $f$, and the \textit{Legendre-Fenchel transformation} for $f$ defined on arbitrary real topological vector spaces. Here we collect some examples of convex conjugates that appeared in the appendix. These are well-known.

\begin{prop}
\label{prop:examples_conjugates}
Let $p,q\geq 1$ be such that $\dfrac{1}{p}+\dfrac{1}{q}=1$.
    \begin{enumerate}
        \item The convex conjugate of the absolute value function $f(x) = |x|$ on $\mathbb{R}$ is given by $|\cdot|^*(x^*) = I_{|x^*|\leq 1}(x^*)$, the convex indicator function on the set $\{|x^*|\leq 1 | x^*\in\mathbb{R}\}$.

        \item The convex conjugate of the $q$-norm $\Vert x\Vert_q$ on $\mathbb{R}^d$ is given by $\Vert \cdot \Vert_q^*(x^*) = I_{\Vert x^*\Vert_p\leq 1}(x^*)$, the convex indicator function on the set $\{\Vert x^*\Vert_p\leq 1 | x^*\in \mathbb{R}^d\}$.

        \item The convex conjugate of $\dfrac{1}{2}\Vert x\Vert_q^2$ on $\mathbb{R}^d$ is given by $\left(\dfrac{1}{2}\Vert \cdot\Vert_q^2 \right)^* (x^*) = \dfrac{1}{2}\Vert x^*\Vert_p^2$.

        \item The convex conjugate of $\log{\big(1+e^{\shortminus x}\big)}$ on $\mathbb{R}$ is given by 
        \[
        \begin{cases}
            x^*\log{(x^*)} + (1-x^*)\log{(1-x^*)} &\text{if }x^*\in (0,1)\\
            0 &\text{if } x^* = 0,1\\
            +\infty &\text{otherwise}.
        \end{cases}
        \]
    \end{enumerate}
\end{prop}

Another easy consequence from the definition of convex conjugation is the below scaling laws.
\begin{prop}[Scaling Laws]
\label{prop:scaling_laws}
    Let $f^*(x^*)$ be the convex conjugate of $f(x)$ on $\mathbb{R}^d$. Then we have,\begin{enumerate}
        \item the convex conjugate of $f(ax)$ whenever $a\neq 0$ is given by $f^*(x^*/a)$.
        \item the convex conjugate of $af(x)$ whenever $a>0$ is given by $af^*(x^*/a)$.
    \end{enumerate}
\end{prop}

Let $\Gamma\left(\mathbb{R}^d\right)$ denote the class of proper convex lower-semi continuous functions on $\mathbb{R}^d$, the next statement says that this conjugation induces an one-to-one symmetric correspondence on the class $\Gamma\left(\mathbb{R}^d\right)$. It is a cornerstone of modern convex analysis and used in the proof of Theorem \ref{thm:linear_l2} and Lemma \ref{lem:3}.

\begin{theorem}[Fenchel-Moreau]
\label{thm:fenchel-moreau}
    Let $f$ be a proper convex, lower semi-continuous function on $\mathbb{R}^d$, then 
    \begin{enumerate}
        \item the convex conjugation $f\mapsto f^*$ is a bijection on $\Gamma\left(\mathbb{R}^d\right)$;
        \item $f\in \Gamma\left(\mathbb{R}^d\right) \iff f^{**} = f$.
    \end{enumerate}
\end{theorem}
\begin{proof}
    For a proof please consult \citep[Section 12]{rockafellar1970convex}.
\end{proof}

The next statement concerns the commutativity of convex conjugation and function summation. Its usefulness is profound, and applied to the proof of Theorem \ref{thm:linear_l2} and Theorem \ref{thm:logistic}.

\begin{theorem}[Infimal Convolution Property of Convex Conjugation]
\label{thm:inf_conv}
    Let $f_1,\ldots,f_M$ be proper convex functions on $\mathbb{R}^d$, then \[
    (f_1\square\ldots\square f_M)^* = f_1^* +\ldots f_M^*,
    \]and \[
    (f_1+\ldots +f_M)^*(x^*) = \inf_{x^*}\{ f_1^*(x_1^*) + \ldots f_M^*(x_M^*)\, |\, x_1^*+\ldots +x_M^* = x^*\}.
    \]
\end{theorem}
\begin{proof}
    For a proof please consult \citep[Theorem 16.4]{rockafellar1970convex}.
\end{proof}

\begin{prop}
\label{prop:generalized_adaptive}
    Let $\Lambda\in \mathbb{S}_+^{d\times d}$, then the dual norm of $\Vert x\Vert_{\Lambda}$ is $\Vert x\Vert_{\Lambda^{-1}}.$ The Cauchy-Schwarz inequality $x\trans u \leq \Vert x\Vert_{\Lambda}\Vert u \Vert_{\Lambda^{\shortminus 1}}$ holds, and equality is attainable. The convex conjugate of $\Vert x\Vert_{\Lambda}$ is given by $I_{\Vert x^*\Vert_{\Lambda^{\shortminus 1}} \leq 1}(x^*)$, and the convex conjugate of $\Vert x\Vert_{\Lambda}^2$ is given by $\Vert x^*\Vert_{\Lambda^{\shortminus 1}}^2/4$.
\end{prop}

\begin{proof}
    The dual norm of $\Vert x\Vert_{\Lambda}$, the Cauchy-Schwarz inequality and attainability of equality follows from \citep[Lemma 1]{blanchet2019data-drivencost}. Now to compute the convex conjugate of $\Vert x\Vert_{\Lambda}^2$, we want to evaluate \[
    \sup_{x\in \mathbb{R}^d}(x^*{\trans}x - \Vert x\Vert_\Lambda^2).
    \]By the Cauchy-Schwarz inequality we have $x^*{\trans}x \leq \Vert x\Vert_{\Lambda} \Vert x^*\Vert_{\Lambda^{\shortminus 1}}$, and so we have \[
    x^*{\trans} x - \Vert x\Vert_{\Lambda}^2 \leq  \Vert x\Vert_{\Lambda} \Vert x^*\Vert_{\Lambda^{\shortminus 1}} - \Vert x\Vert_{\Lambda}^2.
    \]Hence \[
     \sup_{x\in \mathbb{R}^d}(x^*{\trans}x - \Vert x\Vert_\Lambda^2) \leq \sup_{t\geq 0} (t\Vert x^*\Vert_{\Lambda^{\shortminus 1}} - t^2) = \dfrac{1}{4}\Vert x^*\Vert_{\Lambda^{\shortminus 1}}^2.
    \]By attainability of equality in the Cauchy-Schwarz inequality, the supremum are equal, and we have \[
     \sup_{x\in \mathbb{R}^d}(x^*{\trans}x - \Vert x\Vert_\Lambda^2) = \dfrac{1}{4}\Vert x^*\Vert_{\Lambda^{\shortminus 1}}^2.
    \]This proofs the convex conjugate of $\Vert x\Vert_\Lambda^2$. Now consider the convex conjugate of $\Vert x\Vert_{\Lambda}$, then we need to evaluate \[
    \sup_{x\in \mathbb{R}^d} (x^*{\trans}x- \Vert x\Vert_{\Lambda}),
    \]again, by Cauchy-Schwarz and the attainability of equality, we have\begin{align*}
            \sup_{x\in \mathbb{R}^d} (x^*{\trans}x- \Vert x\Vert_{\Lambda}) &= \sup_{x\in\mathbb{R}^d}(\Vert x\Vert_{\Lambda} \Vert x^*\Vert_{\Lambda^{\shortminus 1}} - \Vert x\Vert_{\Lambda}) \\
            &=\sup_{x\in \mathbb{{R}^d}}(\Vert x\Vert_\Lambda (\Vert x^*\Vert_{\Lambda^{\shortminus 1}}-1))\\
            &=\begin{cases}
                0 &\text{if }\Vert x^*\Vert_{\Lambda^{\shortminus 1}} \leq 1,\\
                +\infty &\text{otherwise}.
            \end{cases}
    \end{align*}
    This completes the proof.

\end{proof}

\section{Toland's Duality}
The duality theory of Toland's \citep{toland1978duality_nonconvex,toland1979duality_principle} concerns the minimization of nonconvex functions, in particular, applies to the minimization of the difference of convex functions (DC problems). The duality holds under minimal conditions, and one tries to see if the DC problem can be transformed into something more manageable. 
\begin{theorem}[Toland's Duality]
\label{thm:toland}
    Let $f$ and $g$ be functions on $\mathbb{R}^d$, if $f\in \Gamma\left(\mathbb{R}^d\right)$, then we have \[
    \inf_{x\in \mathbb{R}^d}\left\{f(x)-g(x) \right\} = \inf_{x^*\in \mathbb{R}^d}\left\{g^*(x^*) - f^*(x^*) \right\}.
    \]
\end{theorem}
Toland's duality is implicitly used in the proof to Theorem \ref{thm:linear_l2} and Lemma \ref{lem:3} which also sketches a proof to the above duality theorem.
\newpage

\end{document}
