\section{Appendix}


% Below was the original consideration for high-dimensional learning but under parametric settings.
% \subsection{High-Dimensional Learning}
% Following the approach presented in \cite{persistence} and \cite{graphical-lasso}, instead of assuming a fixed ground truth on the underlying model, we allow the dimension of the covariates, denoted by $p(n)$, and therefore the underlying data generating distribution to depend in general on the number of observed samples. In general, $p(n)$ is an increasing function of $n$, and we allow $p(n) \to \infty$ as $n\to \infty$. The number of sites $M$ stays fixed in the limit of $n$.

% \vspace{0.3cm}
% Recall that $R(\theta,q) = \sum_{m=1}^M q_m \mathbb{E}_m(\ell(Y,X;\theta) - \ell(Y,X;0))$ is the expected value of the sample maximin objective function $\mathbb{P}_n(R(\theta,q)) = \sum_{m=1}^M q_n\left(\dfrac{1}{n}\sum_{i=1}^n \ell(y_i^m,x_i^m;\theta) - \ell(y_i^m,x_i^m;0)\right)$. For a slight abuse of notation let $\ell(\theta)$ denote the $M$-dimensional vector valued function from $\mathbb{R}^p \to \mathbb{R}^M$ by $\theta \mapsto [\ell(X^1,Y^1;\theta) - \ell(X^1,Y^1;0),\ldots,\ell(X^M,Y^M;\theta) - \ell(X^M,Y^M;0)]^T$, and thus $R(\theta,q) = q^T \ell(\theta)$. 
% \vspace{0.3cm}
% We consider the following set of assumptions:
% \begin{assumption}
% \label{A5}
%     There exists a sequence of sets $\widehat{\Theta}_{p(n)} \subset \Theta_{p(n)} \subset \mathbb{R}^{p(n)}$, with $\widehat{\Theta}_{p(n)}$ being a convex compact neighbourhood of $\theta^*_{p(n)}$, and $\mathbb{P}(\widehat{\theta}_{p(n)} \in \widehat{\Theta}_{p(n)}) \geq 1-\zeta(n)$, with $\zeta(n) \to 0$. Furthermore, each $\widehat{\theta}_{p(n)}$ is a stationary point of $\widehat{R}_{p(n)}(\theta,\widehat{q}_{p(n)})$ with respect to the first argument.
% \end{assumption}

% The below persistency assumption should be invoked using sample splitting and cross fitting. In this case, the empirical measure treats $(\theta^*,q^*)$ as (independent) constant parameters.
% \begin{assumption}[Persistency]
% \label{A6}
%     Up to some rate $r_n \to \infty$ and $r_n \lesssim \sqrt{n}$, depending on $n$ and $p(n)$, for every $\theta\in \widehat{\Theta}_{p(n)}$ and $q\in \Delta^M$, we have the uniform rate of convergence \[
%     |\widehat{R}_{p(n)}(\theta,q) - R_{p(n)}(\theta,q)| \lesssim \dfrac{1}{r_n},
%     \]and furthermore, we have the weak convergence,\[
%     r_n\left(\widehat{R}_{p(n)}(\theta^*_{p(n)},q^*_{p(n)}) - R(\theta^*_{p(n)},q^*_{p(n)}) \right) \rightsquigarrow T,
%     \]for some stabilizing law $T$ supported on $\mathbb{R}$.
% \end{assumption}

% Given $q\in \Delta^M$, the saddle extremum solution \begin{align*}
%     \theta^*_{p(n)}(q) &\coloneqq \argmax_{\theta\in \widehat{\Theta}_{p(n)}} \sum_{m=1}^M q_m \mathbb{E}(\ell(Y^m,X^m;\theta_{p(n)})-\ell(Y^m,X^m;0))\\
%     &= \argmax_{\theta\in \widehat{\Theta}_{p(n)}} \sum_{m=1}^M \mathbb{E}(\ell(Y,X;\theta_{p(n)}),
% \end{align*}
% is a weighted ERM estimator, and we consider a minimum rate of convergence that is achievable by vast majority of ML algorithms.

% \begin{assumption}[Minimum rate of convergence]
%     For every $q\in \Delta^M$, we have $\lVert \theta_{p(n)}^* - \widehat{\theta}_{p(n)} \rVert \lesssim o_p( n^{-1/4})$.
% \end{assumption}
% Since $R(\theta,q)$ is a be strongly-concave function over $\Theta_{p(n)}$ for every $q\in \Delta^M$, the set-valued mapping $\theta^*_{p(n)}:\Delta^M \rightrightarrows \mathbb{R}^{p(n)}$ has unique image for each elements of $\Delta^M$, and we may consider it as a vector-valued multivariate function $\theta^*_{p(n)}(q):\Delta^M \to \Theta_{p(n)}$. In fact, with Assumption \ref{A5}, $\theta^*(q)$ satisfied the first order condition,\[
% \partial_1 R(\theta^*(q),q) = \bold{0}
% \]further differentiate with respect to $q$, with the implicit function theorem, we have the derivative \[
% \dfrac{d}{dq}\theta^*(q) = -\left[\mathcal{H}(R)_{(\theta^*(q),q)}\right]^{-1}\left[\mathcal{J}(\ell)^T_{\theta^*(q)}, \right]
% \]controlling the size of the derivative and with the generalized mean-value theorem for vector-valued multivariate functions, we have $\theta^*(q)$ is a Lipschitz function in $q\in \Delta^M$. For the linear case we have, $\dfrac{d(\theta^*(q))}{dq} = B$, and with the Cauchy-Schwarz inequality, the Lipschitz constant $\lVert B\rVert_2 \propto \sqrt{p(n)}$. 
% \begin{assumption}
% \label{A8}
%     The Lipschitz constant $L_n$ of the differentiable function $\theta^*(q)$ satisfies the asymptotic order $L_n^2 / r_n = o_p(1)$.
% \end{assumption}


% Now we look at properties of the saddle extremum function \[
% \lambda^*_{p(n)}(q) = \sup_{\theta\in\widehat{\Theta}_{p(n)}} R(\theta,q), 
% \]and let $\widehat{\lambda}_{p(n)}(q)$ denotes it empirical version. The functions $\lambda^*$ and $\widehat{\lambda}$ are supremum over a concave variable, and therefore, both are convex functions on $\Delta^M$. It is easy to see that, from Assumption \ref{A6}, we have the uniform convergence \[
% |\lambda^*_{p(n)}(q) - \widehat{\lambda}_{p(n)}(q)| \lesssim r_n,
% \]and both $\lambda^*$ and $\widehat{\lambda}$ are Lipschitz function in $q\in\Delta^M$.

% \begin{assumption}(Strict Convexity)
% \label{A9}
%     The convex function $\lambda^*_{p(n)}(q)$ and $\widehat{\lambda}_{p(n)}(q)$ is strictly convex on the domain $\Delta^M$.
% \end{assumption}

% \begin{theorem}
%     With Assumptions \ref{A5}-\ref{A9}. Given the weak convergence \[
%     r_n\left(\widehat{R}_{p(n)}(\theta^*_{p(n)},q^*_{p(n)}) - R(\theta^*_{p(n)},q^*_{p(n)}) \right) \rightsquigarrow T,
%     \]at the population optimal $(\theta^*_{p(n)},q^*_{p(n)})$, the sample variable importance $\widehat{I}_n^X$ satisfies \[
%     r_n\left(\widehat{I}_n^X - R(\theta^*_{p(n)},q^*_{p(n)}) \right) \rightsquigarrow T.
%     \]
% \end{theorem}

% \vspace{0.3cm}
% \textit{Sketch of proof}. \newline Consider the second order expansion $R(\theta^*,q^*)$ around $(\widehat{\theta}_n,\widehat{q}_n)$. Then we have
% \begin{align*}
%     R(\theta^*,q^*) = &R(\widehat{\theta}_n,\widehat{q}_n) + (q^*-\widehat{q}_n)^T \ell(\widehat{\theta}_n) + (\theta^* - \widehat{\theta}_n)^T (\mathcal{J}(\ell)_{\widehat{\theta}_n}^T \widehat{q}_n)\\
%     &+ \dfrac{1}{2}(\theta^* - \widehat{\theta}_n)^T (\widehat{q}_n^T \mathcal{H}(\ell)_{\widehat{\theta}_n})(\theta^*-\widehat{\theta}_n) + o_p(\lVert \theta^* -\widehat{\theta}_n \rVert^2).
% \end{align*}The second order term is independent of the fluctuation $(q^* - \widehat{q}_n)$ since $R(\theta,q)$ is linear in $q$. The term $\mathcal{H}(\ell)$ is a $M\times M \times p$-dimensional tensor. Since $\widehat{\theta}_n = \argmax_{\theta\in \Theta}\widehat{R}_n(\theta,\widehat{q}_n)$, by Assumption \ref{A5}, under the empirical measure, $\mathbb{P}_n R(\theta,q)$ has vanishing gradient with respect to $\widehat{\theta}_n$, which implies that the Jacobian-vector product vanishes under taking the empirical measure$$\mathbb{P}_n\,\mathcal{J}(\ell)_{\widehat{\theta}_n}^T\widehat{q}_n = 0.$$
% \vspace{0.3cm}
% Assume that with high probability that $\widehat{q}_n$ has positive entries for every corresponding entries of $q^*$ for which is positive and likewise for the zero values of $q^*$, then since $(\widehat{\theta}_n,\widehat{q}_n)$ is a (unique) nash equilibrium to the zero-sum game of $\theta$ versus $q$, we have that for every nonzero entries of $\widehat{q}_n$, the corresponding site rewards are the same, therefore under the empirical measure the first-order term $$\mathbb{P}_n(q^* - \widehat{q}_n)^T\ell(\widehat{\theta}_n) = 0.$$ Now assume that the Hessian matrix process $q^T \mathcal{H}(\ell)_\theta$ in $(\theta,q)$ is uniformly bounded in probability, then the remaining second-order bias is,\[
% (\theta^* - \widehat{\theta}_n)^T (\widehat{q}_n^T \mathcal{H}(\ell)_{\widehat{\theta}_n})(\theta^* - \widehat{\theta}_n) \asymp \lVert \theta^* - \widehat{\theta}_n\rVert_2^2
% \]is the second moment of $\widehat{\theta}_n$. We analyze sufficient conditions which allow the second order bias vanishes at the controlled rate $\lVert \theta^* - \widehat{\theta}_n\rVert_2 = o_p(1/r_n)$ where $r_n \to \infty$ is the stabilizing rate in Assumption \ref{A6}. 

% \vspace{0.3cm}
% Then the second order bias has decomposition \begin{align*}
%     \lVert \theta^* - \widehat{\theta}_n\rVert^2 = \lVert \theta^*(q^*) - \widehat{\theta}_n(\widehat{q}_n)\lVert^2 &= \lVert \theta^*(q^*) - \theta^*(\widehat{q}_n) + \theta^*(\widehat{q}_n) -\widehat{\theta}_n(\widehat{q}_n)\lVert^2\\
%     &\leq \lVert \theta^*(q^*) - \theta^*(\widehat{q}_n)  \rVert^2 + \lVert \theta^*(\widehat{q}_n) -\widehat{\theta}_n(\widehat{q}_n)\rVert^2\\
%     & \lesssim L_n^2 \lVert q^* - \widehat{q}_n\rVert^2 + (1/r_n)^2
% \end{align*}
% Now, we consider the rate of vanishing of $\lVert q^* - \widehat{q}_n \rVert^2$. Now \[\lVert q^* - \widehat{q}_n \rVert = \lVert \argmin_q \lambda^*(q) - \argmin_q\widehat{\lambda}_n(q)\rVert,\]
% and $|\min_q \lambda^*(q) - \min_q\widehat{\lambda}_n(q)| \leq \max_q |\lambda^*(q) - \widehat{\lambda}_n(q)|$ which vanishes uniformly over $q\in\Delta^M$, at rate, $1/r_n$ (Assumption \ref{A5}), therefore, it is reasonable to think that $\lVert \argmin_q \lambda^*(q) - \argmin_q\widehat{\lambda}_n(q)\rVert \lesssim r_n$. Indeed, following Theorem 3.2.5 of \cite{wellness-vandervaart}, by consider the unconstrained Lagrangian dual of the convex optimization of $\lambda^*(q)$ and $\widehat{\lambda}_n(q)$ with $q\in\Delta^M$, and the fact that $\lambda^*(q)$ and $\widehat{\lambda}_n(q)$ is Lipschitz, we have by expanding $\lambda^*(\widehat{q}_n)$ around $q^*$,
% \begin{align*}
%     \lVert q^* - \widehat{q}_n\rVert_2^2 \asymp & \lambda^*(\widehat{q}_n) - \lambda^*(q^*) \\
%     &\leq \lambda^*(\widehat{q}_n) - \widehat{\lambda}_n(\widehat{q}_n) - \lambda^*(q^*) + \widehat{\lambda}_n(q^*)\\
%    &\approx(\mathbb{P}_n-\mathbb{P})(\lambda^*(\widehat{q}_n) -\lambda^*{(q)})\\
%    &\leq \sqrt{\var(\lambda^*(\widehat{q}_n) -\lambda^*{(q)})}\\
%    & \lesssim \dfrac{1}{r_n} \lVert q^* - \widehat{q}_n\rVert_2,
% \end{align*}
% therefore, we have $\lVert q^* - \widehat{q}_n\rVert_2^2 \lesssim 1/(r_n)^2$.

% \vspace{0.3cm}
% The second order bias therefore has asymptotic rate $\lVert \theta^* - \widehat{\theta}_n\rVert^2 \lesssim (L_n^2+1) /(r_n)^2 \asymp L_n^2 /(r_n)^2$. Finally taking $\mathbb{P}_n$ on the second order expansion on $R(\theta^*,q^*)$ and subtract the expectation $R(\theta^*,q^*)$ we have \[
% \mathbb{P}_nR(\theta^*,q^*) - R(\theta^*,q^*) = \mathbb{P}_n R(\widehat{\theta}_n,\widehat{q}_n) - R(\theta^*,q^*) + L_n/(r_n)^2,
% \]by Assumption \ref{A8}, we have $L_n^2/r_n = o_p(1)$, then we may claim the weak convergence of $R(\widehat{\theta}_n,\widehat{q}_n) - R(\theta^*,q^*) = \widehat{I}^X_n - I^X$ with Assumption \ref{A6}.
