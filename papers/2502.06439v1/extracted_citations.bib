@article{conitzerTechnicalPerspectiveImpact2022,
  title = {Technical {{Perspective}}: {{The Impact}} of {{Auditing}} for {{Algorithmic Bias}}},
  shorttitle = {Technical {{Perspective}}},
  author = {Conitzer, Vincent and Hadfield, Gillian K. and Vallor, Shannon},
  year = {2022},
  month = dec,
  journal = {Communications of the ACM},
  volume = {66},
  number = {1},
  pages = {100},
  issn = {0001-0782},
  doi = {10.1145/3571152}
}

@misc{counciloftheeuropeanunionCouncilDirective20002000,
  title = {Council {{Directive}} 2000/43/{{EC}} of 29 {{June}} 2000 Implementing the Principle of Equal Treatment between Persons Irrespective of Racial or Ethnic Origin},
  author = {{Council of the European Union}},
  year = {2000},
  month = jun,
  url = {http://data.europa.eu/eli/dir/2000/43/oj/eng},
  langid = {english}
}

@misc{europeancourtofjusticeAssociationBelgeConsommateurs2011,
  title = {Association {{Belge}} Des {{Consommateurs Test-Achats ASBL}} and {{Others}} v {{Conseil}} Des Ministres},
  author = {{European Court of Justice}},
  year = {2011},
  month = mar,
  number = {Case C-236/09},
  url = {https://eur-lex.europa.eu/legal-content/en/TXT/?uri=CELEX:62009CJ0236},
  abstract = {Reference for a preliminary ruling - Fundamental rights - Combating discrimination - Equal treatment for men and women - Access to and supply of goods and services - Insurance premiums and benefits - Actuarial factors - Sex as a factor in the assessment of insurance risks - Private life assurance contracts - Directive 2004/113/EC - Article 5(2) - Derogation not subject to any temporal limitation - Charter of Fundamental Rights of the European Union - Articles 21 and 23 - Invalidity},
  langid = {english}
}

@inproceedings{fabrisAlgorithmicAuditItalian2021a,
  title = {Algorithmic {{Audit}} of {{Italian Car Insurance}}: {{Evidence}} of {{Unfairness}} in {{Access}} and {{Pricing}}},
  shorttitle = {Algorithmic {{Audit}} of {{Italian Car Insurance}}},
  booktitle = {Proceedings of the 2021 {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  author = {Fabris, Alessandro and Mishler, Alan and Gottardi, Stefano and Carletti, Mattia and Daicampi, Matteo and Susto, Gian Antonio and Silvello, Gianmaria},
  year = {2021},
  month = jul,
  series = {{{AIES}} '21},
  pages = {458--468},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3461702.3462569},
  abstract = {We conduct an audit of pricing algorithms employed by companies in the Italian car insurance industry, primarily by gathering quotes through a popular comparison website. While acknowledging the complexity of the industry, we find evidence of several problematic practices. We show that birthplace and gender have a direct and sizeable impact on the prices quoted to drivers, despite national and international regulations against their use. Birthplace, in particular, is used quite frequently to the disadvantage of foreign-born drivers and drivers born in certain Italian cities. In extreme cases, a driver born in Laos may be charged 1,000 more than a driver born in Milan, all else being equal. For a subset of our sample, we collect quotes directly on a company website, where the direct influence of gender and birthplace is confirmed. Finally, we find that drivers with riskier profiles tend to see fewer quotes in the aggregator result pages, substantiating concerns of differential treatment raised in the past by Italian insurance regulators.},
  isbn = {978-1-4503-8473-5},
  keywords = {algorithmic audit,algorithmic fairness,car insurance,fairness through unawareness}
}

@article{goodmanellenp.ALGORITHMICAUDITINGCHASING2023,
  title = {{{ALGORITHMIC AUDITING}}: {{CHASING AI ACCOUNTABILITY}}},
  shorttitle = {{{ALGORITHMIC AUDITING}}},
  author = {Goodman, Ellen P. and Trehu, Julia},
  year = {2023},
  month = may,
  journal = {Santa Clara High Technology Law Journal},
  volume = {39},
  number = {3},
  pages = {289},
  issn = {0882-3383},
  url = {https://digitalcommons.law.scu.edu/chtlj/vol39/iss3/1}
}

@misc{grovesAuditingWorkExploring2024,
  title = {Auditing {{Work}}: {{Exploring}} the {{New York City}} Algorithmic Bias Audit Regime},
  shorttitle = {Auditing {{Work}}},
  author = {Groves, Lara and Metcalf, Jacob and Kennedy, Alayna and Vecchione, Briana and Strait, Andrew},
  year = {2024},
  month = feb,
  number = {arXiv:2402.08101},
  eprint = {2402.08101},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.08101},
  abstract = {In July 2023, New York City (NYC) initiated the first algorithm auditing system for commercial machine-learning systems. Local Law 144 (LL 144) mandates NYC-based employers using automated employment decision-making tools (AEDTs) in hiring to undergo annual bias audits conducted by an independent auditor. This paper examines lessons from LL 144 for other national algorithm auditing attempts. Through qualitative interviews with 16 experts and practitioners within the regime, we find that LL 144 has not effectively established an auditing regime. The law fails to clearly define key aspects, such as AEDTs and independent auditors, leading auditors, AEDT vendors, and companies using AEDTs to define the law's practical implementation in ways that failed to protect job applicants. Contributing factors include the law's flawed transparency-driven theory of change, industry lobbying narrowing the definition of AEDTs, practical and cultural challenges faced by auditors in accessing data, and wide disagreement over what constitutes a legitimate auditor, resulting in four distinct 'auditor roles.' We conclude with four recommendations for policymakers seeking to create similar bias auditing regimes, emphasizing clearer definitions, metrics, and increased accountability. By exploring LL 144 through the lens of auditors, our paper advances the evidence base around audit as an accountability mechanism, providing guidance for policymakers seeking to create similar regimes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society}
}

@article{liuMedicalAlgorithmicAudit2022,
  title = {The Medical Algorithmic Audit},
  author = {Liu, Xiaoxuan and Glocker, Ben and McCradden, Melissa M. and Ghassemi, Marzyeh and Denniston, Alastair K. and {Oakden-Rayner}, Lauren},
  year = {2022},
  month = may,
  journal = {The Lancet Digital Health},
  volume = {4},
  number = {5},
  pages = {e384-e397},
  publisher = {Elsevier},
  issn = {2589-7500},
  doi = {10.1016/S2589-7500(22)00003-6},
  langid = {english},
  pmid = {35396183}
}

@inproceedings{rajiActionableAuditingInvestigating2019,
  title = {Actionable {{Auditing}}: {{Investigating}} the {{Impact}} of {{Publicly Naming Biased Performance Results}} of {{Commercial AI Products}}},
  shorttitle = {Actionable {{Auditing}}},
  booktitle = {Proceedings of the 2019 {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  author = {Raji, Inioluwa Deborah and Buolamwini, Joy},
  year = {2019},
  month = jan,
  series = {{{AIES}} '19},
  pages = {429--435},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3306618.3314244},
  abstract = {Although algorithmic auditing has emerged as a key strategy to expose systematic biases embedded in software platforms, we struggle to understand the real-world impact of these audits, as scholarship on the impact of algorithmic audits on increasing algorithmic fairness and transparency in commercial systems is nascent. To analyze the impact of publicly naming and disclosing performance results of biased AI systems, we investigate the commercial impact of Gender Shades, the first algorithmic audit of gender and skin type performance disparities in commercial facial analysis models. This paper 1) outlines the audit design and structured disclosure procedure used in the Gender Shades study, 2) presents new performance metrics from targeted companies IBM, Microsoft and Megvii (Face++) on the Pilot Parliaments Benchmark (PPB) as of August 2018, 3) provides performance results on PPB by non-target companies Amazon and Kairos and, 4) explores differences in company responses as shared through corporate communications that contextualize differences in performance on PPB. Within 7 months of the original audit, we find that all three targets released new API versions. All targets reduced accuracy disparities between males and females and darker and lighter-skinned subgroups, with the most significant update occurring for the darker-skinned female subgroup, that underwent a 17.7\% - 30.4\% reduction in error between audit periods. Minimizing these disparities led to a 5.72\% to 8.3\% reduction in overall error on the Pilot Parliaments Benchmark (PPB) for target corporation APIs. The overall performance of non-targets Amazon and Kairos lags significantly behind that of the targets, with error rates of 8.66\% and 6.60\% overall, and error rates of 31.37\% and 22.50\% for the darker female subgroup, respectively.},
  isbn = {978-1-4503-6324-2},
  keywords = {artificial intelligence,commercial applications,computer vision,ethics,facial recognition,fairness,machine learning}
}

@article{rajiActionableAuditingRevisited2022,
  title = {Actionable {{Auditing Revisited}}: {{Investigating}} the {{Impact}} of {{Publicly Naming Biased Performance Results}} of {{Commercial AI Products}}},
  shorttitle = {Actionable {{Auditing Revisited}}},
  author = {Raji, Inioluwa Deborah and Buolamwini, Joy},
  year = {2022},
  month = dec,
  journal = {Communications of the ACM},
  volume = {66},
  number = {1},
  pages = {101--108},
  issn = {0001-0782},
  doi = {10.1145/3571151},
  abstract = {Although algorithmic auditing has emerged as a key strategy to expose systematic biases embedded in software platforms, we struggle to understand the real-world impact of these audits and continue to find it difficult to translate such independent assessments into meaningful corporate accountability. To analyze the impact of publicly naming and disclosing performance results of biased AI systems, we investigate the commercial impact of Gender Shades, the first algorithmic audit of gender- and skin-type performance disparities in commercial facial analysis models. This paper (1) outlines the audit design and structured disclosure procedure used in the Gender Shades study, (2) presents new performance metrics from targeted companies such as IBM, Microsoft, and Megvii (Face++) on the Pilot Parliaments Benchmark (PPB) as of August 2018, (3) provides performance results on PPB by non-target companies such as Amazon and Kairos, and (4) explores differences in company responses as shared through corporate communications that contextualize differences in performance on PPB. Within 7 months of the original audit, we find that all three targets released new application program interface (API) versions. All targets reduced accuracy disparities between males and females and darker- and lighter-skinned subgroups, with the most significant update occurring for the darker-skinned female subgroup that underwent a 17.7--30.4\% reduction in error between audit periods. Minimizing these disparities led to a 5.72--8.3\% reduction in overall error on the Pilot Parliaments Benchmark (PPB) for target corporation APIs. The overall performance of non-targets Amazon and Kairos lags significantly behind that of the targets, with error rates of 8.66\% and 6.60\% overall, and error rates of 31.37\% and 22.50\% for the darker female subgroup, respectively. This is an expanded version of an earlier publication of these results, revised for a more general audience, and updated to include commentary on further developments.},
  keywords = {auditing}
}

@article{shenEverydayAlgorithmAuditing2021,
  title = {Everyday {{Algorithm Auditing}}: {{Understanding}} the {{Power}} of {{Everyday Users}} in {{Surfacing Harmful Algorithmic Behaviors}}},
  shorttitle = {Everyday {{Algorithm Auditing}}},
  author = {Shen, Hong and DeVos, Alicia and Eslami, Motahhare and Holstein, Kenneth},
  year = {2021},
  month = oct,
  journal = {Proceedings of the ACM on Human-Computer Interaction},
  volume = {5},
  number = {CSCW2},
  pages = {433:1--433:29},
  doi = {10.1145/3479577},
  abstract = {A growing body of literature has proposed formal approaches to audit algorithmic systems for biased and harmful behaviors. While formal auditing approaches have been greatly impactful, they often suffer major blindspots, with critical issues surfacing only in the context of everyday use once systems are deployed. Recent years have seen many cases in which everyday users of algorithmic systems detect and raise awareness about harmful behaviors that they encounter in the course of their everyday interactions with these systems. However, to date little academic attention has been granted to these bottom-up, user-driven auditing processes. In this paper, we propose and explore the concept of everyday algorithm auditing, a process in which users detect, understand, and interrogate problematic machine behaviors via their day-to-day interactions with algorithmic systems. We argue that everyday users are powerful in surfacing problematic machine behaviors that may elude detection via more centrally-organized forms of auditing, regardless of users' knowledge about the underlying algorithms. We analyze several real-world cases of everyday algorithm auditing, drawing lessons from these cases for the design of future platforms and tools that facilitate such auditing behaviors. Finally, we discuss work that lies ahead, toward bridging the gaps between formal auditing approaches and the organic auditing behaviors that emerge in everyday use of algorithmic systems.},
  keywords = {algorithmic bias,auditing algorithms,everyday algorithm auditing,everyday users,fair machine learning}
}

@misc{thenewyorkcitycouncilLocalLawAmend2021,
  title = {A {{Local Law}} to Amend the Administrative Code of the City of {{New York}}, in Relation to Automated Employment Decision Tools},
  author = {{The New York City Council}},
  year = {2021},
  url = {https://legistar.council.nyc.gov/LegislationDetail.aspx?ID=4344524&GUID=B051915D-A9AC-451E-81F8-6596032FA3F9&Options=ID%7CText%7C&Search=}
}

@inproceedings{vecchioneAlgorithmicAuditingSocial2021,
  title = {Algorithmic {{Auditing}} and {{Social Justice}}: {{Lessons}} from the {{History}} of {{Audit Studies}}},
  shorttitle = {Algorithmic {{Auditing}} and {{Social Justice}}},
  booktitle = {Proceedings of the 1st {{ACM Conference}} on {{Equity}} and {{Access}} in {{Algorithms}}, {{Mechanisms}}, and {{Optimization}}},
  author = {Vecchione, Briana and Levy, Karen and Barocas, Solon},
  year = {2021},
  month = nov,
  series = {{{EAAMO}} '21},
  pages = {1--9},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3465416.3483294},
  abstract = {``Algorithmic audits'' have been embraced as tools to investigate the functioning and consequences of sociotechnical systems. Though the term is used somewhat loosely in the algorithmic context and encompasses a variety of methods, it maintains a close connection to audit studies in the social sciences---which have, for decades, used experimental methods to measure the prevalence of discrimination across domains like housing and employment. In the social sciences, audit studies originated in a strong tradition of social justice and participatory action, often involving collaboration between researchers and communities; but scholars have argued that, over time, social science audits have become somewhat distanced from these original goals and priorities. We draw from this history in order to highlight difficult tensions that have shaped the development of social science audits, and to assess their implications in the context of algorithmic auditing. In doing so, we put forth considerations to assist in the development of robust and engaged assessments of sociotechnical systems that draw from auditing's roots in racial equity and social justice.},
  isbn = {978-1-4503-8553-4}
}

