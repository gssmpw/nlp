@misc{acmcode2018taskforceACMCodeEthics2018,
  title = {{{ACM Code}} of {{Ethics}} and {{Professional Conduct}}},
  author = {{ACM Code 2018 Task Force}},
  year = {2018},
  url = {https://www.acm.org/code-of-ethics},
  abstract = {Ethical and social computing are embodied in the ACM Code of Ethics. The core values expressed in the ACM Code inspire and guide computing professionals. The actions of computing professionals change the world, and the Code is the conscience of the field. Serving as the Hippocratic Oath for the IT Professional, the Software Engineer, the Programmer and all those responsible for shaping and contributing to the future of computing. The Code covers ethics across the computing field, representing the tech code of ethics, computing ethics, software ethics, programming ethics, AI ethics and computing for public good.},
  langid = {english}
}

@article{bakerAlgorithmicBiasEducation2022,
  title = {Algorithmic {{Bias}} in {{Education}}},
  author = {Baker, Ryan S. and Hawn, Aaron},
  year = {2022},
  month = dec,
  journal = {International Journal of Artificial Intelligence in Education},
  volume = {32},
  number = {4},
  pages = {1052--1092},
  issn = {1560-4306},
  doi = {10.1007/s40593-021-00285-9},
  abstract = {In this paper, we review algorithmic bias in education, discussing the causes of that bias and reviewing the empirical literature on the specific ways that algorithmic bias is known to have manifested in education. While other recent work has reviewed mathematical definitions of fairness and expanded algorithmic approaches to reducing bias, our review focuses instead on solidifying the current understanding of the concrete impacts of algorithmic bias in education---which groups are known to be impacted and which stages and agents in the development and deployment of educational algorithms are implicated. We discuss theoretical and formal perspectives on algorithmic bias, connect those perspectives to the machine learning pipeline, and review metrics for assessing bias. Next, we review the evidence around algorithmic bias in education, beginning with the most heavily-studied categories of race/ethnicity, gender, and nationality, and moving to the available evidence of bias for less-studied categories, such as socioeconomic status, disability, and military-connected status. Acknowledging the gaps in what has been studied, we propose a framework for moving from unknown bias to known bias and from fairness to equity. We discuss obstacles to addressing these challenges and propose four areas of effort for mitigating and resolving the problems of algorithmic bias in AIED systems and other educational technology.},
  langid = {english},
  keywords = {Algorithmic bias,Algorithmic fairness,Artificial intelligence and education,discrimination/education,Machine learning,principles/fairness}
}

@inproceedings{brunSoftwareFairness2018,
  title = {Software Fairness},
  booktitle = {Proceedings of the 2018 26th {{ACM Joint Meeting}} on {{European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {Brun, Yuriy and Meliou, Alexandra},
  year = {2018},
  month = oct,
  series = {{{ESEC}}/{{FSE}} 2018},
  pages = {754--759},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3236024.3264838},
  abstract = {A goal of software engineering research is advancing software quality and the success of the software engineering process. However, while recent studies have demonstrated a new kind of defect in software related to its ability to operate in fair and unbiased manner, software engineering has not yet wholeheartedly tackled these new kinds of defects, thus leaving software vulnerable. This paper outlines a vision for how software engineering research can help reduce fairness defects and represents a call to action by the software engineering research community to reify that vision. Modern software is riddled with examples of biased behavior, from automated translation injecting gender stereotypes, to vision systems failing to see faces of certain races, to the US criminal justice sytem relying on biased computational assessments of crime recidivism. While systems may learn bias from biased data, bias can also emerge from ambiguous or incomplete requirement specification, poor design, implementation bugs, and unintended component interactions. We argue that software fairness is analogous to software quality, and that numerous software engineering challenges in the areas of requirements, specification, design, testing, and verification need to be tackled to solve this problem.},
  isbn = {978-1-4503-5573-5},
  keywords = {software bias,Software fairness,software process}
}

@misc{cherianStatisticalInferenceFairness2023,
  title = {Statistical {{Inference}} for {{Fairness Auditing}}},
  author = {Cherian, John J. and Cand{\`e}s, Emmanuel J.},
  year = {2023},
  month = may,
  number = {arXiv:2305.03712},
  eprint = {2305.03712},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.03712},
  abstract = {Before deploying a black-box model in high-stakes problems, it is important to evaluate the model's performance on sensitive subpopulations. For example, in a recidivism prediction task, we may wish to identify demographic groups for which our prediction model has unacceptably high false positive rates or certify that no such groups exist. In this paper, we frame this task, often referred to as "fairness auditing," in terms of multiple hypothesis testing. We show how the bootstrap can be used to simultaneously bound performance disparities over a collection of groups with statistical guarantees. Our methods can be used to flag subpopulations affected by model underperformance, and certify subpopulations for which the model performs adequately. Crucially, our audit is model-agnostic and applicable to nearly any performance metric or group fairness criterion. Our methods also accommodate extremely rich -- even infinite -- collections of subpopulations. Further, we generalize beyond subpopulations by showing how to assess performance over certain distribution shifts. We test the proposed methods on benchmark datasets in predictive inference and algorithmic fairness and find that our audits can provide interpretable and trustworthy guarantees.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning,Statistics - Methodology}
}

@article{conitzerTechnicalPerspectiveImpact2022,
  title = {Technical {{Perspective}}: {{The Impact}} of {{Auditing}} for {{Algorithmic Bias}}},
  shorttitle = {Technical {{Perspective}}},
  author = {Conitzer, Vincent and Hadfield, Gillian K. and Vallor, Shannon},
  year = {2022},
  month = dec,
  journal = {Communications of the ACM},
  volume = {66},
  number = {1},
  pages = {100},
  issn = {0001-0782},
  doi = {10.1145/3571152}
}

@misc{counciloftheeuropeanunionCouncilDirective20002000,
  title = {Council {{Directive}} 2000/43/{{EC}} of 29 {{June}} 2000 Implementing the Principle of Equal Treatment between Persons Irrespective of Racial or Ethnic Origin},
  author = {{Council of the European Union}},
  year = {2000},
  month = jun,
  url = {http://data.europa.eu/eli/dir/2000/43/oj/eng},
  langid = {english}
}

@misc{deckImplicationsAIAct2024,
  title = {Implications of the {{AI Act}} for {{Non-Discrimination Law}} and {{Algorithmic Fairness}}},
  author = {Deck, Luca and M{\"u}ller, Jan-Laurin and Braun, Conradin and Zipperling, Domenique and K{\"u}hl, Niklas},
  year = {2024},
  month = mar,
  number = {arXiv:2403.20089},
  eprint = {2403.20089},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.20089},
  abstract = {The topic of fairness in AI, as debated in the FATE (Fairness, Accountability, Transparency, and Ethics in AI) communities, has sparked meaningful discussions in the past years. However, from a legal perspective, particularly from European Union law, many open questions remain. Whereas algorithmic fairness aims to mitigate structural inequalities at the design level, European non-discrimination law is tailored to individual cases of discrimination after an AI model has been deployed. The AI Act might present a tremendous step towards bridging these two concepts by shifting non-discrimination responsibilities into the design stage of AI models. Based on an integrative reading of the AI Act, we comment on legal as well as technical enforcement problems and propose practical implications on bias detection and bias correction in order to specify and comply with specific technical requirements.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence}
}

@inproceedings{dworkFairnessAwareness2012,
  title = {Fairness through Awareness},
  booktitle = {Proceedings of the 3rd {{Innovations}} in {{Theoretical Computer Science Conference}}},
  author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
  year = {2012},
  month = jan,
  series = {{{ITCS}} '12},
  pages = {214--226},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2090236.2090255},
  abstract = {We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of "fair affirmative action," which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.},
  isbn = {978-1-4503-1115-1},
  keywords = {principles/fairness}
}

@misc{europeancourtofjusticeAssociationBelgeConsommateurs2011,
  title = {Association {{Belge}} Des {{Consommateurs Test-Achats ASBL}} and {{Others}} v {{Conseil}} Des Ministres},
  author = {{European Court of Justice}},
  year = {2011},
  month = mar,
  number = {Case C-236/09},
  url = {https://eur-lex.europa.eu/legal-content/en/TXT/?uri=CELEX:62009CJ0236},
  abstract = {Reference for a preliminary ruling - Fundamental rights - Combating discrimination - Equal treatment for men and women - Access to and supply of goods and services - Insurance premiums and benefits - Actuarial factors - Sex as a factor in the assessment of insurance risks - Private life assurance contracts - Directive 2004/113/EC - Article 5(2) - Derogation not subject to any temporal limitation - Charter of Fundamental Rights of the European Union - Articles 21 and 23 - Invalidity},
  langid = {english}
}

@misc{europeanparliamentDirectiveEU20212021,
  title = {Directive ({{EU}}) 2021/2118 of the {{European Parliament}} and of the {{Council}} of 24 {{November}} 2021 Amending {{Directive}} 2009/103/{{EC}} Relating to Insurance against Civil Liability in Respect of the Use of Motor Vehicles, and the Enforcement of the Obligation to Insure against Such Liability},
  author = {{European Parliament} and {European Council}},
  year = {2021},
  url = {https://eur-lex.europa.eu/eli/dir/2021/2118/oj},
  langid = {english},
  annotation = {Doc ID: 32021L2118\\
Doc Sector: 3\\
Doc Title: Directive (EU) 2021/2118 of the European Parliament and of the Council of 24~November 2021 amending Directive 2009/103/EC relating to insurance against civil liability in respect of the use of motor vehicles, and the enforcement of the obligation to insure against such liability (Text with EEA relevance)\\
Doc Type: L\\
Usr\_lan: en}
}

@misc{europeanunionagencyforfundamentalrightsEUCharterFundamental2015,
  title = {{{EU Charter}} of {{Fundamental Rights}} - {{Title III}}: {{Quality}} - {{Article}} 21 - {{Non-discrimination}}},
  author = {{European Union Agency For Fundamental Rights}},
  year = {2015},
  month = apr,
  url = {http://fra.europa.eu/en/eu-charter/article/21-non-discrimination},
  langid = {english}
}

@inproceedings{fabrisAlgorithmicAuditItalian2021a,
  title = {Algorithmic {{Audit}} of {{Italian Car Insurance}}: {{Evidence}} of {{Unfairness}} in {{Access}} and {{Pricing}}},
  shorttitle = {Algorithmic {{Audit}} of {{Italian Car Insurance}}},
  booktitle = {Proceedings of the 2021 {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  author = {Fabris, Alessandro and Mishler, Alan and Gottardi, Stefano and Carletti, Mattia and Daicampi, Matteo and Susto, Gian Antonio and Silvello, Gianmaria},
  year = {2021},
  month = jul,
  series = {{{AIES}} '21},
  pages = {458--468},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3461702.3462569},
  abstract = {We conduct an audit of pricing algorithms employed by companies in the Italian car insurance industry, primarily by gathering quotes through a popular comparison website. While acknowledging the complexity of the industry, we find evidence of several problematic practices. We show that birthplace and gender have a direct and sizeable impact on the prices quoted to drivers, despite national and international regulations against their use. Birthplace, in particular, is used quite frequently to the disadvantage of foreign-born drivers and drivers born in certain Italian cities. In extreme cases, a driver born in Laos may be charged 1,000 more than a driver born in Milan, all else being equal. For a subset of our sample, we collect quotes directly on a company website, where the direct influence of gender and birthplace is confirmed. Finally, we find that drivers with riskier profiles tend to see fewer quotes in the aggregator result pages, substantiating concerns of differential treatment raised in the past by Italian insurance regulators.},
  isbn = {978-1-4503-8473-5},
  keywords = {algorithmic audit,algorithmic fairness,car insurance,fairness through unawareness}
}

@inproceedings{feldtValidityThreatsEmpirical2010,
  title = {Validity Threats in Empirical Software Engineering Research-an Initial Survey},
  booktitle = {Proceedings of the 22nd {{International Conference}} on {{Software Engineering}} and {{Knowledge Engineering}}},
  author = {Feldt, Robert and Magazinius, Ana},
  year = {2010},
  pages = {374--379},
  publisher = {Knowledge Systems Institute Graduate School},
  address = {Redwood City, San Francisco Bay, CA, USA},
  url = {https://www.cse.chalmers.se/~feldt/publications/feldt_2010_validity_threats_in_ese_initial_survey.pdf}
}

@inproceedings{galhotraFairnessTestingTesting2017,
  title = {Fairness Testing: Testing Software for Discrimination},
  shorttitle = {Fairness Testing},
  booktitle = {Proceedings of the 2017 11th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}}},
  author = {Galhotra, Sainyam and Brun, Yuriy and Meliou, Alexandra},
  year = {2017},
  month = aug,
  series = {{{ESEC}}/{{FSE}} 2017},
  pages = {498--510},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3106237.3106277},
  abstract = {This paper defines software fairness and discrimination and develops a testing-based method for measuring if and how much software discriminates, focusing on causality in discriminatory behavior. Evidence of software discrimination has been found in modern software systems that recommend criminal sentences, grant access to financial products, and determine who is allowed to participate in promotions. Our approach, Themis, generates efficient test suites to measure discrimination. Given a schema describing valid system inputs, Themis generates discrimination tests automatically and does not require an oracle. We evaluate Themis on 20 software systems, 12 of which come from prior work with explicit focus on avoiding discrimination. We find that (1) Themis is effective at discovering software discrimination, (2) state-of-the-art techniques for removing discrimination from algorithms fail in many situations, at times discriminating against as much as 98\% of an input subdomain, (3) Themis optimizations are effective at producing efficient test suites for measuring discrimination, and (4) Themis is more efficient on systems that exhibit more discrimination. We thus demonstrate that fairness testing is a critical aspect of the software development cycle in domains with possible discrimination and provide initial tools for measuring software discrimination.},
  isbn = {978-1-4503-5105-8},
  keywords = {Discrimination testing,fairness testing,principles/fairness,software bias,testing}
}

@article{gautierAIAlgorithmsPrice2020,
  title = {{{AI}} Algorithms, Price Discrimination and Collusion: A Technological, Economic and Legal Perspective},
  shorttitle = {{{AI}} Algorithms, Price Discrimination and Collusion},
  author = {Gautier, Axel and Ittoo, Ashwin and Van Cleynenbreugel, Pieter},
  year = {2020},
  month = dec,
  journal = {European Journal of Law and Economics},
  volume = {50},
  number = {3},
  pages = {405--435},
  issn = {1572-9990},
  doi = {10.1007/s10657-020-09662-6},
  abstract = {In recent years, important concerns have been raised about the increasing capabilities of pricing algorithms to make use of artificial intelligence (AI) technologies. Two issues have gained particular attention: algorithmic price discrimination (PD) and algorithmic tacit collusion (TC). Although the risks and opportunities of both practices have been explored extensively in the literature, neither has yet been observed in the actual practice. As a result, there remains much confusion as to the ability of algorithms to engage in potentially harmful behavior with respect to price discrimination and collusion. In this article, we embed the economic and legal literature on these topics in a technological grounding to provide a more objective account of the capabilities of current AI technologies to engage in price discrimination and collusion. We argue that attention to these current technological capabilities should more directly inform on-going discussions on the urgency to reform legal rules or enforcement practices governing algorithmic PD and TC.},
  langid = {english},
  keywords = {AI,C,Competition,D,Deep learning,Economics,GDPR,K2,K4,L,Machine learning,Markets,Price discrimination,Reinforcement learning,Tacit collusion}
}

@article{gomes-francoExplainingAssociationDriver2020,
  title = {Explaining the {{Association}} between {{Driver}}'s {{Age}} and the {{Risk}} of {{Causing}} a {{Road Crash}} through {{Mediation Analysis}}},
  author = {{Gomes-Franco}, Karoline and {Rivera-Izquierdo}, Mario and {Mart{\'i}n-delosReyes}, Luis Miguel and {Jim{\'e}nez-Mej{\'i}as}, Eladio and {Mart{\'i}nez-Ruiz}, Virginia},
  year = {2020},
  month = jan,
  journal = {International Journal of Environmental Research and Public Health},
  volume = {17},
  number = {23},
  pages = {9041},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1660-4601},
  doi = {10.3390/ijerph17239041},
  abstract = {It has been widely reported that younger and older drivers have an excess risk of causing a road crash. Two casual hypotheses may coexist: the riskier driving behaviors and age-related mechanisms in extreme age groups (direct path) and the different environmental and vehicle circumstances (indirect path). Our aim was to quantify, through a mediation analysis, the percentage contribution of both paths. A case-control study was designed from the Spanish Register of Road Crashes with victims from 2014 to 2017. Assuming a quasi-induced exposure approach, controls were non-responsible drivers involved in clean collisions between two or more vehicles (n = 52,131). Responsible drivers for these collisions plus drivers involved in single crashes constituted the case group (n = 82,071). A logit model in which the outcome was the log (odds) of causing a road crash and the exposure was age groups was adjusted for driver, vehicle and environmental factors. The highest crash risk was observed in extreme age groups, compared to the 35--44 year old age group: the youngest (18--24 years old, odds ratio = 2.14, 95\% confidence interval: 2.06--2.24) and the oldest drivers ({$>$}74 years old, odds ratio = 3.30, 95\% confidence interval: 3.04--2.58). The mediation analysis identified the direct path as the main explanatory mechanism for these increases: 89\% in the youngest and 93\% in the oldest drivers. These data support the hypothesis that the excess crash risk observed for younger and older drivers is mainly related to their higher frequency of risky driving behaviors and age-related loss of capabilities. Preventive strategies in extreme-aged drivers should focus on decreasing these behaviors.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {mediation analysis,older drivers,risk factors,road crash,younger drivers}
}

@article{goodmanellenp.ALGORITHMICAUDITINGCHASING2023,
  title = {{{ALGORITHMIC AUDITING}}: {{CHASING AI ACCOUNTABILITY}}},
  shorttitle = {{{ALGORITHMIC AUDITING}}},
  author = {Goodman, Ellen P. and Trehu, Julia},
  year = {2023},
  month = may,
  journal = {Santa Clara High Technology Law Journal},
  volume = {39},
  number = {3},
  pages = {289},
  issn = {0882-3383},
  url = {https://digitalcommons.law.scu.edu/chtlj/vol39/iss3/1}
}

@misc{grovesAuditingWorkExploring2024,
  title = {Auditing {{Work}}: {{Exploring}} the {{New York City}} Algorithmic Bias Audit Regime},
  shorttitle = {Auditing {{Work}}},
  author = {Groves, Lara and Metcalf, Jacob and Kennedy, Alayna and Vecchione, Briana and Strait, Andrew},
  year = {2024},
  month = feb,
  number = {arXiv:2402.08101},
  eprint = {2402.08101},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.08101},
  abstract = {In July 2023, New York City (NYC) initiated the first algorithm auditing system for commercial machine-learning systems. Local Law 144 (LL 144) mandates NYC-based employers using automated employment decision-making tools (AEDTs) in hiring to undergo annual bias audits conducted by an independent auditor. This paper examines lessons from LL 144 for other national algorithm auditing attempts. Through qualitative interviews with 16 experts and practitioners within the regime, we find that LL 144 has not effectively established an auditing regime. The law fails to clearly define key aspects, such as AEDTs and independent auditors, leading auditors, AEDT vendors, and companies using AEDTs to define the law's practical implementation in ways that failed to protect job applicants. Contributing factors include the law's flawed transparency-driven theory of change, industry lobbying narrowing the definition of AEDTs, practical and cultural challenges faced by auditors in accessing data, and wide disagreement over what constitutes a legitimate auditor, resulting in four distinct 'auditor roles.' We conclude with four recommendations for policymakers seeking to create similar bias auditing regimes, emphasizing clearer definitions, metrics, and increased accountability. By exploring LL 144 through the lens of auditors, our paper advances the evidence base around audit as an accountability mechanism, providing guidance for policymakers seeking to create similar regimes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society}
}

@techreport{isoISOIEC250192023,
  type = {Standard},
  title = {{{ISO}}/{{IEC}} 25019:2023 - {{Systems}} and Software Engineering --- {{Systems}} and Software {{Quality Requirements}} and {{Evaluation}} ({{SQuaRE}}) --- {{Quality-in-use}} Model},
  shorttitle = {{{ISO}}/{{IEC}} 25019:2023},
  author = {{ISO}},
  year = {2023},
  address = {Geneva, CH},
  institution = {International Organization for Standardization},
  url = {https://www.iso.org/standard/78177.html},
  abstract = {Systems and software engineering --- Systems and software Quality Requirements and Evaluation (SQuaRE) --- Quality-in-use model},
  langid = {english},
  keywords = {standard/iso}
}

@techreport{isoISOIEC250592023,
  type = {Standard},
  title = {{{ISO}}/{{IEC}} 25059:2023 - {{Software}} Engineering --- {{Systems}} and Software {{Quality Requirements}} and {{Evaluation}} ({{SQuaRE}}) --- {{Quality}} Model for {{AI}} Systems},
  author = {{ISO}},
  year = {2023},
  address = {Geneva, CH},
  institution = {International Organization for Standardization},
  url = {https://www.iso.org/standard/80655.html},
  keywords = {standard/iso}
}

@techreport{istitutonazionaledistatisticaIncidentiStradaliItalia2022,
  title = {{Incidenti stradali in Italia. Anno 2022.}},
  author = {{Istituto Nazionale di Statistica}},
  year = {2022},
  institution = {ISTAT},
  url = {https://www.istat.it/it/archivio/286933},
  langid = {italian}
}

@misc{kearnsPreventingFairnessGerrymandering2018,
  title = {Preventing {{Fairness Gerrymandering}}: {{Auditing}} and {{Learning}} for {{Subgroup Fairness}}},
  shorttitle = {Preventing {{Fairness Gerrymandering}}},
  author = {Kearns, Michael and Neel, Seth and Roth, Aaron and Wu, Zhiwei Steven},
  year = {2018},
  month = dec,
  number = {arXiv:1711.05144},
  eprint = {1711.05144},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1711.05144},
  abstract = {The most prevalent notions of fairness in machine learning are statistical definitions: they fix a small collection of pre-defined groups, and then ask for parity of some statistic of the classifier across these groups. Constraints of this form are susceptible to intentional or inadvertent "fairness gerrymandering", in which a classifier appears to be fair on each individual group, but badly violates the fairness constraint on one or more structured subgroups defined over the protected attributes. We propose instead to demand statistical notions of fairness across exponentially (or infinitely) many subgroups, defined by a structured class of functions over the protected attributes. This interpolates between statistical definitions of fairness and recently proposed individual notions of fairness, but raises several computational challenges. It is no longer clear how to audit a fixed classifier to see if it satisfies such a strong definition of fairness. We prove that the computational problem of auditing subgroup fairness for both equality of false positive rates and statistical parity is equivalent to the problem of weak agnostic learning, which means it is computationally hard in the worst case, even for simple structured subclasses. We then derive two algorithms that provably converge to the best fair classifier, given access to oracles which can solve the agnostic learning problem. The algorithms are based on a formulation of subgroup fairness as a two-player zero-sum game between a Learner and an Auditor. Our first algorithm provably converges in a polynomial number of steps. Our second algorithm enjoys only provably asymptotic convergence, but has the merit of simplicity and faster per-step computation. We implement the simpler algorithm using linear regression as a heuristic oracle, and show that we can effectively both audit and learn fair classifiers on real datasets.},
  archiveprefix = {arXiv},
  keywords = {bias/mitigation,Computer Science - Computer Science and Game Theory,Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning}
}

@article{kochlingDiscriminatedAlgorithmSystematic2020,
  title = {Discriminated by an Algorithm: A Systematic Review of Discrimination and Fairness by Algorithmic Decision-Making in the Context of {{HR}} Recruitment and {{HR}} Development},
  shorttitle = {Discriminated by an Algorithm},
  author = {K{\"o}chling, Alina and Wehner, Marius Claus},
  year = {2020},
  month = nov,
  journal = {Business Research},
  volume = {13},
  number = {3},
  pages = {795--848},
  issn = {2198-2627},
  doi = {10.1007/s40685-020-00134-w},
  abstract = {Algorithmic decision-making is becoming increasingly common as a new source of advice in HR recruitment and HR development. While firms implement algorithmic decision-making to save costs as well as increase efficiency and objectivity, algorithmic decision-making might also lead to the unfair treatment of certain groups of people, implicit discrimination, and perceived unfairness. Current knowledge about the threats of unfairness and (implicit) discrimination by algorithmic decision-making is mostly unexplored in the human resource management context. Our goal is to clarify the current state of research related to HR recruitment and HR development, identify research gaps, and provide crucial future research directions. Based on a systematic review of 36 journal articles from 2014 to 2020, we present some applications of algorithmic decision-making and evaluate the possible pitfalls in these two essential HR functions. In doing this, we inform researchers and practitioners, offer important theoretical and practical implications, and suggest fruitful avenues for future research.},
  langid = {english},
  keywords = {Algorithmic decision-making in HRM,Discrimination,Ethics,Fairness,Literature review,Perceived fairness,principles/fairness}
}

@article{lemaireUseAnnualMileage2016,
  title = {The Use of Annual Mileage as a Rating Variable},
  author = {Lemaire, Jean and Park, Sojung Carol and Wang, Kili C.},
  year = {2016},
  month = jan,
  journal = {ASTIN Bulletin: The Journal of the IAA},
  volume = {46},
  number = {1},
  pages = {39--69},
  issn = {0515-0361, 1783-1350},
  doi = {10.1017/asb.2015.25},
  abstract = {Auto insurance companies must adapt to ever-evolving regulations and technological progress. Several variables commonly used to predict accidents rates, such as gender and territory, are being questioned by regulators. Insurers are pressured to find new variables that predict accidents more accurately and are socially acceptable. Annual mileage seems an ideal candidate. The recent development in new technologies should induce insurance carriers to explore ways to introduce mileage-based insurance premiums. We use the unique database of a major insurer in Taiwan to investigate whether annual mileage should be introduced as a rating variable in auto third-party liability insurance. We find that annual mileage is an extremely powerful predictor of the number of claims at-fault. The inclusion of mileage as a new variable should, however, not take place at the expense of bonus-malus systems; rather, the information contained in the bonus-malus premium level complements the value of annual mileage. An accurate rating system should therefore include annual mileage and bonus-malus as the two main building blocks, possibly supplemented by the use of other variables like age, territory and engine cubic capacity. While Taiwan has specific characteristics (high traffic density, a mild bonus-malus system and limited compulsory auto coverage), our results are so strong that we can confidently conjecture that they extend to all developed nations.},
  langid = {english},
  keywords = {Annual mileage,Auto liability insurance,Rating variables}
}

@article{liuMedicalAlgorithmicAudit2022,
  title = {The Medical Algorithmic Audit},
  author = {Liu, Xiaoxuan and Glocker, Ben and McCradden, Melissa M. and Ghassemi, Marzyeh and Denniston, Alastair K. and {Oakden-Rayner}, Lauren},
  year = {2022},
  month = may,
  journal = {The Lancet Digital Health},
  volume = {4},
  number = {5},
  pages = {e384-e397},
  publisher = {Elsevier},
  issn = {2589-7500},
  doi = {10.1016/S2589-7500(22)00003-6},
  langid = {english},
  pmid = {35396183}
}

@article{malekCriminalCourtsArtificial2022,
  title = {Criminal Courts' Artificial Intelligence: The Way It Reinforces Bias and Discrimination},
  shorttitle = {Criminal Courts' Artificial Intelligence},
  author = {Malek, Md. Abdul},
  year = {2022},
  month = feb,
  journal = {AI and Ethics},
  volume = {2},
  number = {1},
  pages = {233--245},
  issn = {2730-5961},
  doi = {10.1007/s43681-022-00137-9},
  abstract = {Embracive, pervasive, and unstoppable global algorithmization greatly influences the deployment of artificial intelligence systems in criminal courts to replace obsolete bail and sentencing practices, reduce recidivism risk, and modernize judicial practices. Since artificial intelligence systems have provably appeared to have the duality of golden promises and potential perils, applying such a system in the justice system also entails some associated risks. Hence, allocating this unchecked-novel resource in judicial domains sparks vigorous debate over their legal and ethical implications. With such backgrounds, this paper examines how and why artificial intelligence systems reinforce bias and discrimination in society and suggest what approach could be an alternative to the current predictive justice mechanisms in use.},
  langid = {english},
  keywords = {Artificial intelligence,Machine bias,Predictive justice,principles/fairness,Risk prediction}
}

@article{obermeyerDissectingRacialBias2019,
  title = {Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations},
  author = {Obermeyer, Ziad and Powers, Brian and Vogeli, Christine and Mullainathan, Sendhil},
  year = {2019},
  month = oct,
  journal = {Science},
  volume = {366},
  number = {6464},
  pages = {447--453},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.aax2342},
  abstract = {Health systems rely on commercial prediction algorithms to identify and help patients with complex health needs. We show that a widely used algorithm, typical of this industry-wide approach and affecting millions of patients, exhibits significant racial bias: At a given risk score, Black patients are considerably sicker than White patients, as evidenced by signs of uncontrolled illnesses. Remedying this disparity would increase the percentage of Black patients receiving additional help from 17.7 to 46.5\%. The bias arises because the algorithm predicts health care costs rather than illness, but unequal access to care means that we spend less money caring for Black patients than for White patients. Thus, despite health care cost appearing to be an effective proxy for health by some measures of predictive accuracy, large racial biases arise. We suggest that the choice of convenient, seemingly effective proxies for ground truth can be an important source of algorithmic bias in many contexts.},
  keywords = {discrimination/health,discrimination/race,principles/fairness}
}

@inproceedings{rajiActionableAuditingInvestigating2019,
  title = {Actionable {{Auditing}}: {{Investigating}} the {{Impact}} of {{Publicly Naming Biased Performance Results}} of {{Commercial AI Products}}},
  shorttitle = {Actionable {{Auditing}}},
  booktitle = {Proceedings of the 2019 {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  author = {Raji, Inioluwa Deborah and Buolamwini, Joy},
  year = {2019},
  month = jan,
  series = {{{AIES}} '19},
  pages = {429--435},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3306618.3314244},
  abstract = {Although algorithmic auditing has emerged as a key strategy to expose systematic biases embedded in software platforms, we struggle to understand the real-world impact of these audits, as scholarship on the impact of algorithmic audits on increasing algorithmic fairness and transparency in commercial systems is nascent. To analyze the impact of publicly naming and disclosing performance results of biased AI systems, we investigate the commercial impact of Gender Shades, the first algorithmic audit of gender and skin type performance disparities in commercial facial analysis models. This paper 1) outlines the audit design and structured disclosure procedure used in the Gender Shades study, 2) presents new performance metrics from targeted companies IBM, Microsoft and Megvii (Face++) on the Pilot Parliaments Benchmark (PPB) as of August 2018, 3) provides performance results on PPB by non-target companies Amazon and Kairos and, 4) explores differences in company responses as shared through corporate communications that contextualize differences in performance on PPB. Within 7 months of the original audit, we find that all three targets released new API versions. All targets reduced accuracy disparities between males and females and darker and lighter-skinned subgroups, with the most significant update occurring for the darker-skinned female subgroup, that underwent a 17.7\% - 30.4\% reduction in error between audit periods. Minimizing these disparities led to a 5.72\% to 8.3\% reduction in overall error on the Pilot Parliaments Benchmark (PPB) for target corporation APIs. The overall performance of non-targets Amazon and Kairos lags significantly behind that of the targets, with error rates of 8.66\% and 6.60\% overall, and error rates of 31.37\% and 22.50\% for the darker female subgroup, respectively.},
  isbn = {978-1-4503-6324-2},
  keywords = {artificial intelligence,commercial applications,computer vision,ethics,facial recognition,fairness,machine learning}
}

@article{rajiActionableAuditingRevisited2022,
  title = {Actionable {{Auditing Revisited}}: {{Investigating}} the {{Impact}} of {{Publicly Naming Biased Performance Results}} of {{Commercial AI Products}}},
  shorttitle = {Actionable {{Auditing Revisited}}},
  author = {Raji, Inioluwa Deborah and Buolamwini, Joy},
  year = {2022},
  month = dec,
  journal = {Communications of the ACM},
  volume = {66},
  number = {1},
  pages = {101--108},
  issn = {0001-0782},
  doi = {10.1145/3571151},
  abstract = {Although algorithmic auditing has emerged as a key strategy to expose systematic biases embedded in software platforms, we struggle to understand the real-world impact of these audits and continue to find it difficult to translate such independent assessments into meaningful corporate accountability. To analyze the impact of publicly naming and disclosing performance results of biased AI systems, we investigate the commercial impact of Gender Shades, the first algorithmic audit of gender- and skin-type performance disparities in commercial facial analysis models. This paper (1) outlines the audit design and structured disclosure procedure used in the Gender Shades study, (2) presents new performance metrics from targeted companies such as IBM, Microsoft, and Megvii (Face++) on the Pilot Parliaments Benchmark (PPB) as of August 2018, (3) provides performance results on PPB by non-target companies such as Amazon and Kairos, and (4) explores differences in company responses as shared through corporate communications that contextualize differences in performance on PPB. Within 7 months of the original audit, we find that all three targets released new application program interface (API) versions. All targets reduced accuracy disparities between males and females and darker- and lighter-skinned subgroups, with the most significant update occurring for the darker-skinned female subgroup that underwent a 17.7--30.4\% reduction in error between audit periods. Minimizing these disparities led to a 5.72--8.3\% reduction in overall error on the Pilot Parliaments Benchmark (PPB) for target corporation APIs. The overall performance of non-targets Amazon and Kairos lags significantly behind that of the targets, with error rates of 8.66\% and 6.60\% overall, and error rates of 31.37\% and 22.50\% for the darker female subgroup, respectively. This is an expanded version of an earlier publication of these results, revised for a more general audience, and updated to include commentary on further developments.},
  keywords = {auditing}
}

@incollection{rinivansolingenGoalQuestionMetric2002,
  title = {Goal {{Question Metric}} ({{GQM}}) {{Approach}}},
  booktitle = {Encyclopedia of {{Software Engineering}}},
  author = {{Rini van Solingen} and Basili, Victor and Caldiera, Gianluigi and Rombach, H. Dieter},
  editor = {{J.J. Marciniak}},
  year = {2002},
  publisher = {John Wiley \& Sons, Ltd},
  address = {USA},
  doi = {10.1002/0471028959.sof142},
  abstract = {As with any engineering discipline, software development requires a measurement mechanism for feedback and evaluation. Measurement supports creating a corporate memory and is an aid in answering a variety of questions associated with the enactment of any software process. Measurement also helps, during the course of a project, to assess its progress, to take corrective action based on this assessment, and to evaluate the impact of such action. According to many studies made on the application of metrics and models in industrial environments, measurement in order to be effective must be. Focused on specific goals Applied to all life-cycle products, processes, and resources Interpreted on the basis of characterization and understanding of the organizational context, environment, and goals This means that measurement must be defined in a top-down fashion. It must be focused, based on goals and models. A metric-driven, bottom-up approach, will not work because there are many observable characteristics in software (e.g., time, number of defects, complexity, lines of code, severity of failures, effort, productivity, defect density). A context specific selection of metrics and guidelines on how to use and interpret them should be made, based on the appropriate models and goals of that environment. The most common and popular mechanism for goal-oriented software measurement is the Goal Question Metric approach which is presented in this article in combination with examples from GQM application in industry},
  copyright = {Copyright {\copyright} 2002 by John Wiley \& Sons, Inc. All rights reserved.},
  isbn = {978-0-471-02895-6},
  langid = {english},
  keywords = {industrial application,measurable software,phases,quality improvement}
}

@misc{segugio.itPolizzeAutoMoto,
  title = {{Polizze auto e moto: la comparazione si conferma lo strumento di riferimento degli italiani}},
  shorttitle = {{Polizze auto e moto}},
  author = {{Segugio.it}},
  journal = {Segugio.it},
  url = {https://assicurazioni.segugio.it/news-assicurazioni/polizze-auto-e-moto-la-comparazione-si-conferma-lo-strumento-di-riferimento-degli-italiani-00037593.html},
  abstract = {La comparazione si conferma un riferimento per gli italiani nella stipula della polizza RC Auto/Moto: lo conferma l'indagine condotta da Segugio.it.},
  langid = {italian}
}

@misc{selingerAIAuditsWho2023,
  type = {{{SSRN Scholarly Paper}}},
  title = {{{AI Audits}}: {{Who}}, {{When}}, {{How}}...{{Or Even If}}?},
  shorttitle = {{{AI Audits}}},
  author = {Selinger, Evan and Leong, Brenda and Cahn, Albert Fox},
  year = {2023},
  month = sep,
  number = {4568208},
  address = {Rochester, NY},
  url = {https://papers.ssrn.com/abstract=4568208},
  abstract = {Artificial intelligence (AI) tools are increasingly being integrated into decision-making processes in high-risk settings, including employment, credit, health care, housing, and law enforcement. Given the harms that poorly designed systems can lead to, including matters of life and death, there is a growing sense that crafting policies for using AI responsibly must necessarily include, at a minimum, assurances about the technical accuracy and reliability of the model design.Because AI auditing is still in its early stages, many questions remain about how to best conduct them. While many people are optimistic that valid and effective best practice standards and procedures will emerge, some civil rights advocates are skeptical of both the concept and the practical use of AI audits. These critics are reasonably concerned about audit-washing---bad actors gaming loopholes and ambiguities in audit requirements to demonstrate compliance without actually providing meaningful reviews.This chapter aims to explain why AI audits often are regarded as essential tools within an overall responsible governance system and how they are evolving toward accepted standards and best practices. We will focus most of our analysis on these explanations, including recommendations for conducting high-quality AI audits. Nevertheless, we will also articulate the core ideas of the skeptical civil rights position. This intellectually and politically sound view should be taken seriously by the AI community. To be well-informed about AI audits is to comprehend their positive prospects and be prepared to address their most serious challenges.},
  langid = {english},
  keywords = {AI Audits,AI governance,Artificial Intelligence,Machine Learning,source/jc,to-read}
}

@article{shenEverydayAlgorithmAuditing2021,
  title = {Everyday {{Algorithm Auditing}}: {{Understanding}} the {{Power}} of {{Everyday Users}} in {{Surfacing Harmful Algorithmic Behaviors}}},
  shorttitle = {Everyday {{Algorithm Auditing}}},
  author = {Shen, Hong and DeVos, Alicia and Eslami, Motahhare and Holstein, Kenneth},
  year = {2021},
  month = oct,
  journal = {Proceedings of the ACM on Human-Computer Interaction},
  volume = {5},
  number = {CSCW2},
  pages = {433:1--433:29},
  doi = {10.1145/3479577},
  abstract = {A growing body of literature has proposed formal approaches to audit algorithmic systems for biased and harmful behaviors. While formal auditing approaches have been greatly impactful, they often suffer major blindspots, with critical issues surfacing only in the context of everyday use once systems are deployed. Recent years have seen many cases in which everyday users of algorithmic systems detect and raise awareness about harmful behaviors that they encounter in the course of their everyday interactions with these systems. However, to date little academic attention has been granted to these bottom-up, user-driven auditing processes. In this paper, we propose and explore the concept of everyday algorithm auditing, a process in which users detect, understand, and interrogate problematic machine behaviors via their day-to-day interactions with algorithmic systems. We argue that everyday users are powerful in surfacing problematic machine behaviors that may elude detection via more centrally-organized forms of auditing, regardless of users' knowledge about the underlying algorithms. We analyze several real-world cases of everyday algorithm auditing, drawing lessons from these cases for the design of future platforms and tools that facilitate such auditing behaviors. Finally, we discuss work that lies ahead, toward bridging the gaps between formal auditing approaches and the organic auditing behaviors that emerge in everyday use of algorithmic systems.},
  keywords = {algorithmic bias,auditing algorithms,everyday algorithm auditing,everyday users,fair machine learning}
}

@misc{thenewyorkcitycouncilLocalLawAmend2021,
  title = {A {{Local Law}} to Amend the Administrative Code of the City of {{New York}}, in Relation to Automated Employment Decision Tools},
  author = {{The New York City Council}},
  year = {2021},
  url = {https://legistar.council.nyc.gov/LegislationDetail.aspx?ID=4344524&GUID=B051915D-A9AC-451E-81F8-6596032FA3F9&Options=ID%7CText%7C&Search=}
}

@inproceedings{vecchioneAlgorithmicAuditingSocial2021,
  title = {Algorithmic {{Auditing}} and {{Social Justice}}: {{Lessons}} from the {{History}} of {{Audit Studies}}},
  shorttitle = {Algorithmic {{Auditing}} and {{Social Justice}}},
  booktitle = {Proceedings of the 1st {{ACM Conference}} on {{Equity}} and {{Access}} in {{Algorithms}}, {{Mechanisms}}, and {{Optimization}}},
  author = {Vecchione, Briana and Levy, Karen and Barocas, Solon},
  year = {2021},
  month = nov,
  series = {{{EAAMO}} '21},
  pages = {1--9},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3465416.3483294},
  abstract = {``Algorithmic audits'' have been embraced as tools to investigate the functioning and consequences of sociotechnical systems. Though the term is used somewhat loosely in the algorithmic context and encompasses a variety of methods, it maintains a close connection to audit studies in the social sciences---which have, for decades, used experimental methods to measure the prevalence of discrimination across domains like housing and employment. In the social sciences, audit studies originated in a strong tradition of social justice and participatory action, often involving collaboration between researchers and communities; but scholars have argued that, over time, social science audits have become somewhat distanced from these original goals and priorities. We draw from this history in order to highlight difficult tensions that have shaped the development of social science audits, and to assess their implications in the context of algorithmic auditing. In doing so, we put forth considerations to assist in the development of robust and engaged assessments of sociotechnical systems that draw from auditing's roots in racial equity and social justice.},
  isbn = {978-1-4503-8553-4}
}

@article{vetroDataQualityApproach2021,
  title = {A Data Quality Approach to the Identification of Discrimination Risk in Automated Decision Making Systems},
  author = {Vetr{\`o}, Antonio and Torchiano, Marco and Mecati, Mariachiara},
  year = {2021},
  month = oct,
  journal = {Government Information Quarterly},
  volume = {38},
  number = {4},
  pages = {101619},
  issn = {0740-624X},
  doi = {10.1016/j.giq.2021.101619},
  abstract = {Automated decision-making (ADM) systems may affect multiple aspects of our lives. In particular, they can result in systematic discrimination of specific population groups, in violation of the EU Charter of Fundamental Rights. One of the potential causes of discriminative behavior, i.e., unfairness, lies in the quality of the data used to train such ADM systems. Using a data quality measurement approach combined with risk management, both defined in ISO standards, we focus on balance characteristics and we aim to understand how balance indexes (Gini, Simpson, Shannon, Imbalance Ratio) identify discrimination risk in six large datasets containing the classification output of ADM systems. The best result is achieved using the Imbalance Ratio index. Gini and Shannon indexes tend to assume high values and for this reason they have modest results in both aspects: further experimentation with different thresholds is needed. In terms of policies, the risk-based approach is a core element of the EU approach to regulate algorithmic systems: in this context, balance measures can be easily assumed as risk indicators of propagation -- or even amplification -- of bias in the input data of ADM systems.},
  langid = {english},
  keywords = {Algorithm fairness,Automated decision making,bias,Data ethics,Data quality,Digital governance,Digital policy}
}
