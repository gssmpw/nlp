\section{Background and Motivation}
The programming models for smart contracts, such as Solidity, face significant limitations in implementing the complex mathematical models that are commonplace in traditional quantitative finance. These limitations are primarily due to constraints that prevent the use of high-performance computing required for sophisticated quantitative models within smart contracts, thereby impeding innovation in Decentralized Finance (DeFi) and other emerging blockchain applications~\cite{shah2023systematic}. Moreover, recent significant advancements in artificial intelligence present new opportunities for blockchain and the decentralized web. The traditional finance sector and the DeFi space are beginning to explore the transformative potential of AI in enhancing financial decision-making, dynamic pricing, prediction, and portfolio optimization~\cite{cao:2020}. Among various AI technologies, large language models (LLMs) and generative AI are viewed as particularly promising for their potential applications in the finance industry~\cite{krause:2023}.
Generative AI consists of a wide range of technologies that can synthesize new data instances based on statistical patterns. Generative AI models targeting finance applications have the potential to revolutionize both traditional financial modeling and decentralized finance by their capabilities to evaluate massive amounts of numerical and textual data, generate time series, and predict financial performance. Besides data-driven modeling in quantitative finance, large language models present promising opportunities for general-purpose on-chain decision-making and governance where natural language content in conjunction with code can be applied for specifying rules and contract logic.

In Cosmos-based blockchains, smart contracts can be executed as WebAssembly (WASM) code, providing a robust environment for implementing smart contracts beyond traditional approaches. Although Cosmos can facilitate Ethereum Virtual Machine (EVM) compatibility through Ethermint, allowing for the porting of existing Solidity contracts, this work concentrates on the built-in support for WASM smart contracts within the Cosmos ecosystem. The Cosmos SDK supports the execution of WASM smart contracts through a dedicated module, x/WASM. Thanks to the modular design of the Cosmos SDK, the WASM module can seamlessly interact with other modules, such as the staking and bank modules. Notably, the EVM support within Cosmos is also modular, implemented in a similar fashion to the WASM module. This modular architecture enhances flexibility and integration possibilities within the Cosmos ecosystem.

% Through messages and queries, the WASM module can access other Cosmos modules for operations such as staking, token transfer, swapping. The messages are encoded. Using plugin, WASM contracts can issue requests and queries to the native modules. The requests and queries are dispatched to the corresponding module interface (called keeper). The WASM interface is flexible enough to support custom  extension. In our case, the goal is to equip WASM contracts with the capability to run high performance computing code in GPU and invoke AI inferences.


\subsection{Transformer Models and Its Applications}

The GPT series of language models, which utilize the Transformer architecture~\cite{vaswani2017attention}, operate in an auto-regressive manner~\cite{brown2020language}. The process starts with a sequence of tokens known as a prompt. The model attentively analyzes this initial input, progressing iteratively. In each iteration, it evaluates the likelihood of possible subsequent tokens. Through intensive training, the model learns the complex process of prediction and selection, thereby refining its ability to generate coherent and contextually appropriate text.

The procedure for processing and sampling to generate a single output token in a language model is known as an iteration. After being trained on a substantial dataset, models like GPT are capable of performing language tasks with notable proficiency. For instance, when presented with the prompt "knowledge is," the model is more likely to predict "power" rather than "apple" as the next word. Following this iteration, the generated token is appended to the initial prompt and re-fed into the model for the generation of the next token. This cycle repeats until an End of Sequence token is emitted, indicating the end of the text, or until a predetermined maximum output length is reached. This inference method, characterized by its iterative nature, contrasts sharply with architectures like ResNet, which typically exhibit a fixed and predictable execution time~\cite{gujarati2020serving}. While each individual iteration of the language model may be predictable, the total number of iterations—and thus the total inference time—can vary.

Additionally, it is important to mention that large language models (LLMs) are not solely confined to language tasks; they can also be easily configured to model and predict time series data.

% \subsection{Transformer Applications}

% At its core, GPT is a word prediction machine for language modeling. However, by feeding it specific instructions (prompts), we can bend it to tackle various NLP tasks. ChatGPT \cite{openai2023Introducing} proves this principle. It's built on top of GPT, but with extra training via supervised learning and human feedback reinforcement (RLHF). This lets ChatGPT handle diverse tasks interactively, from translation to analyzing emotions and creative writing. But this interactivity is demanding. Many users can hit ChatGPT at once, each expecting quick responses. So, how fast it finishes each job or the Job Completion Time becomes critical for keeping ChatGPT-like applications smooth and efficient.

\subsection{Inference Engine}

Inference serving systems like TensorFlow Serving \cite{olston2017tensorflow} and Triton \footnote{https://github.com/triton-inference-
server/server} act as middleware for Deep Neural Networks (DNNs), handling resource allocation, task execution, and result delivery. They maximize hardware utilization, especially on GPUs, by batching multiple tasks. This batching improves overall efficiency but increases memory usage compared to single-task processing. This becomes especially critical for Large Language Models (LLMs) with their heavy memory footprint, limiting their batch size during inferences.

The surge in popularity of GPT models has prompted optimizations in these serving systems to accommodate GPT's unique design and iterative generation process. At the heart of GPT is the Transformer architecture, featuring the distinctive Masked Self-Attention module. This module generates three key values (query, key, and value) for each input token, allowing each token to "see" how relevant other tokens are, regardless of their position. Masked Self-Attention ensures causality by hiding future tokens from each token during prediction, enabling GPT to focus on the next token in the sequence. The attention mechanism essentially grants each token a global view of the entire input sentence, considering the importance of every other token in its prediction.

Within the GPT inference loop, the attention mechanism relies heavily on key-value pairs from preceding tokens. Traditionally, these are recalculated for each step, incurring significant computational overhead. To address this inefficiency, Fairseq \cite{ott2019fairseq} proposes a caching scheme, pre-computing and storing these elements for subsequent use. This two-stage process involves an initial analysis phase where caches are built for each GPT layer based on the prompt. During decoding, only the newly generated token's key, value, and query are computed, with the cache progressively updated. This optimized scheme significantly reduces per-iteration computational workload compared to the initial computations. Similar caching optimizations are also employed by libraries such as HuggingFace\cite{wolf2019huggingface} and FasterTransformer\cite{chelba2020faster}.

Orca\cite{yu2022orca}, on the other hand, introduces a distinct scheduling approach known as iteration-level scheduling. This contrasts with the typical batch processing paradigm by handling just one iteration per batch. While this enables dynamic job entry and completion, it presents challenges in terms of GPU memory limitations and the stringent latency requirements of interactive applications. Therefore, while Orca offers flexibility, it effective utilization requires meticulous resource management.

\subsection{Blockchain}

Blockchain has emerged as a pivotal decentralized framework for data management and storage, serving as the foundation for various distributed applications, including digital currencies\cite{nakamoto2008bitcoin}, decentralized finance,  and networks\cite{luo2023escm}. It functions as a distributed ledger system, enabling transaction recording and verification without relying on central authority or third-party validation. This technology fosters a peer-to-peer network\cite{zyskind2015decentralizing} where participants collaboratively maintain a transparent and immutable public ledger\cite{cao2022blockchain}. Blockchain's structure integrates a physical network that supports communication, computing, and data storage. This infrastructure underpins features like the blockchain consensus mechanism, forming a dual-layer system comprising the physical network and the blockchain. Transactions in blockchain encapsulate client information, recorded in blocks that collectively form a chain, delineating the logical sequence of these transactions. The system's security and efficacy hinge on the consensus mechanism and smart contracts, automating transaction execution and maintaining system integrity. These attributes position blockchain as a critical technology for enhancing AI's reliability and trustworthiness.
