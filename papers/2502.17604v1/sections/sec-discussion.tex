\section{Discussion}
We have run the LLM/AI execution and Cosmwasm in a machine equipped with Intel I9 Processor and NVIDIA 4080 Mobile graphics processor with 16GB of RAM. We intentionally chose a moderately configured device to note any potential limitation of the system. In our testing both of the models ran in reasonable amount of time. The llama2 model was able to generate a 35 token per second and llava 30 token per second.  

\subsection{CosmWasm and WASM Contract}
We had modified the name space example contract from the Cosmos example repository in Github to incorporate our Rust code. And we were able to run the compiled WASM smart contract as described in Section \ref{compilecontract}.

\begin{tcolorbox}
\textbf{RQ1} \textit{Can smart contracts utilize LLM and AI inferences with reasonable performance?}

We were able to write a proof of concept code and call a LLM/AI inference engine we to produce a model output in a moderately configured commodity hardware. Within our limited scope smart contracts, or more specifically smart contract executed using CosmWasm is able to generate LLM inferences using our framework. Note that LLMs are more demanding in computing resources than other AI models. 
\end{tcolorbox}

\subsection{Security}

Since the input is executed directly from the contract and the contract is compiled into a WASM file before running. The data should be safe as long as the host machine is safe. Also since the inference is being run in either that machine or in another clustered machine, the models and inferences on device and protected as long as the system integrity is protected.

\begin{tcolorbox}
\textbf{RQ2} \textit{Can we get inference in local in secure way?}

The inference is done in local since the compiled models are also in local. It assumes that all the validator nodes have access to the same models.
\end{tcolorbox}

\subsection{Model Performance}

Since we use WasmEdge for our model evaluation, we are able to utilize WebGPU API completely. That gives us access to GPU in the host system if its available, or switch to a CPU runtime based on the inference engine. 

\begin{tcolorbox}
\textbf{RQ3} \textit{Can we utilize GPU for fast inferences while executing a smart contract?}

WebGPU allows us to access and utilize the GPU runtime to run our models in the GPU in an efficient way.
\end{tcolorbox}

\subsection{Security Consideration}

Even though the model is running locally, as the model is loaded into memory, attacks are possible. We specifically look into one specific family of attack that makes most of the local inference system today vulnerable if using native solution.

The VU\#446598~\footnote{https://kb.cert.org/vuls/id/446598} dubbed as \textit{LeftoverLocals} affects all Apple, AMD, Qualcomm family of GPUs. 

% Originally designed for enhancing graphics processing, GPUs have evolved rapidly, emphasizing performance. This rapid development often overshadowed security concerns, which rarely impacted applications significantly. As a result, both GPU hardware and software experienced swift and substantial changes in architecture and programming models. This fast-paced evolution led to intricate system stacks and ambiguous specifications. For instance, in contrast to the extensive documentation available for CPU ISAs, NVIDIA offers minimal guidance, mostly limited to concise tables. Such lack of detailed specifications has been a factor in worrying issues, historically and presently, as demonstrated by cases like LeftoverLocals.

In the context of GPU computing, it has been identified that one GPU kernel is capable of accessing memory values from another kernel, even when these kernels are segregated across different applications, processes, or user environments. The memory area where this interaction occurs is known as local memory. This local memory functions akin to a software-managed cache, comparable to the L1 cache found in CPUs. The size of this local memory varies between GPUs, ranging from tens of kilobytes to several megabytes. Trail of Bits demonstrated this vulnerability's presence~\footnote{https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-
llm-responses-through-leaked-gpu-local-memory/} through various programming interfaces, including Metal, Vulkan, and OpenCL, across diverse combinations of operating systems and drivers. This finding highlights a significant security concern in GPU architecture.

However their findings report that even in vulnerable machines, the models when accessed through WebGPU did not dump any secrets apart from zeros~\cite{trailofbitsLeftover}. This makes \sln{} resilient to these type of memory dump attacks. However, these kind of attacks show us more work is needed to continually protect the system from future threat attacks, even though the present one did not pose a threat.

In case model confidentiality is a concern, AI models can be executed by enclaves and TEEs (trusted execution environments). TEEs such as Intel SGX are capable of running inferences for machine learning models like DNNs. The new GPU TEEs may soon support running LLM inferences in concealed environment. 

\begin{tcolorbox}
\textbf{RQ4} \textit{What are the security implications?}

Attacks like LeftoverLocals are mitigated in this framework. but future work is necessary to understand the security and privacy needs of on-chain AI inferences and assess the attack surface.
\end{tcolorbox}

\subsection{Potential Applications}
The capability of running AI inference engine as part of contract execution could open new opportunities for application areas such as DeFi, decentralized insurance, DAO governance, voting, prediction market, AI agent based decision making, to name just few.

\subsubsection{Open Research Avenues}

% \textbf{Natural language text or multi-modal data as smart contract inputs and outputs}: Ability to understand natural languages allows smart contracts to accept diverse data as inputs. The input can be interpreted by the AI models before it is further processed. 

\textbf{AI based DeFi models}: With the kind of support described in this work, machine learning based finance models can be incorporated on-chain as part of a DeFi contract. This will certainly expand the scope of DeFi to a new level where DeFi contracts can directly apply machine learning on-chain in decision making such as pricing, liquidity optimization, risk reduction. 

\textbf{On-chain governance based on natural language texts}: With LLM support, the concept "code is law" can be expanded to use both code and nature language to regulate the behavior of smart contracts. Complex business logic could be described in natural language and enforced as smart contracts. 

\textbf{On-chain AI agents}: With integration of AI into smart contracts, smart contracts can act as AI agents. This may redefine to our current understanding of smart contracts and significantly increase the scope of smart contracts in future. 

%\textbf{Decentralized AI Marketplaces}: The ability to execute AI models on-chain could lead to the development of decentralized marketplaces where AI models are bought, sold, and executed in a trustless manner.
%\textbf{AI-Driven Smart Contract Automation}: AI models could be used to automate complex decision-making processes within smart contracts, potentially revolutionizing areas like decentralized finance and supply chain management.
%\textbf{Privacy-Preserving AI on the Blockchain}: The paper touches on the potential for privacy-aware AI inference. This could be a significant research area, exploring how to leverage blockchain's security features to protect sensitive data used in AI models.


\subsection{Remarks}

Depending on the types of blockchain, for instance, permissioned or permissionless, running AI inferences may require gas fee. It is outside the scope of this work to specify a detailed gas model for AI inferences. It is plausible to compute gas fee based on model size and the number of LLM tokens (similar to how fee is calculated by OpenAI for using ChapGPT).  In case of Cosmos module, aiwasm module can charge gas fee using a specific fee model implemented by the module. A simple approach is to apply a fee model similar to EVM pre-compiles. Another remark is that although the current work focuses on enabling AI for WASM based smart contracts, it is plausible to apply the same concept to EVM. This could be achieved using dedicated pre-compiles. Last but not the least, it is not assumed that AI inferences must be executed on the main chain. The framework could be applied to support AI modeling in side-chain, application specific chain (app chain), or layer 2 network.

\section{Future Work}

\subsection{WebGPU Native}

Having received universal support from top browser developers, WebGPU~\footnote{https://developer.chrome.com/blog/webgpu-io2023} is an impressive step forward. %It performs far better on GPU efficiency than WebGL. Among the prominent features of WebGPU are storage textures and compute shaders, which allow direct storage in shaders without requiring texture sampling. This lessens the need for data repackaging and makes structured data easier to use. But for our use case WebGPU also gives significant performance improvement and near native speedup for using GPU~\cite{Unlock1}.
There have been some work which tries to run AI inferences directly using WebGPU like tokenhawk~\footnote{https://github.com/kayvr/token-hawk} which tries to implement models by hand, which is not scalable. The other more mature methods all use WASM as a backend like mlc-llm~\cite{MLCLLMHome-2024-01-18} and WebONNX~\cite{webonnx8}. 

\subsection{Usage of WebAssembly (WASM)}

The large language model (LLM) and AI inference engine in our architecture run on both CPUs and GPUs and is written in Rust. This dual compatibility addresses common limitations in web-based computation by facilitating a balance between memory consumption, data transfer volume, and CPU/GPU utilization. %WebAssembly improves this by lowering JavaScript-related overhead by enabling pre-compiled assembly code to run in web browsers. This also allows us to use the same API to access it using Rust. Additionally, WebAssembly facilitates the compilation of a single WASM file from both Rust and C++ (in case of our Rust code and llama2.cpp). It greatly simplifies the programming logic for us. 
However we still need to load the model every time. That is not suitable for large models and specially for execution on-chain and from a smart contract. %It does not allow for native data storing for the inference results.

In our future work, we want to explore these two venues, of (a) coming up with a WebGPU framework that can run inferences in native speed in a programming language agnostic way, communicated just by message passing. Similar to what WebGPU allows us but not limited to Rust. We also want to explore if (b) we can develop a better way to \textit{stream} model weights instead of loading the model completely. That will allow us to have more control for performance optimization. %what gets stored and executed directly from the smart contracts, allowing us to have usage of contracts in spaces like building AI agents on blockchain, insurance negotiators and voting negotiating agents based on smart contracts.