\section{Implementation Highlights}

For \sln{} we decided to use the fairly large Llama2\cite{touvron2023llama} family of models. Before we can pass the input to the model, the CosmWasm modules has to compile and run the contract.

The compilation process is same for both the smart contract and the Rust inference engine we wrote for loading the model. 

\subsection{Compiling the Contract}\label{compilecontract}
The contract written in Rust has to be compiled to WASM code to be executed. The easiest way for us to do that was to use "cargo". Cargo is Rust's build system and package manager which allows us to easily get packages created in the Rust ecosystem. We use the following to optimize and compile it to a WASM runtime

\begin{lstlisting}[caption={WasmCosm Contract Compile},breaklines, language=bash, label=contractcompile]
RUSTFLAGS= -C link-arg=-s cargo wasm
\end{lstlisting}

Once we have the compiled WASM file, this was deployed in the blockhain.
\begin{lstlisting}[caption={WasmCosm Contract Store},breaklines, language=bash, label=contractcompile]
wasmd tx wasm store nameservice.wasm  --from <your-key> --chain-id <chain-id> --gas auto
\end{lstlisting}

\subsection{Llama 2 Inference}\label{llamainfer}

For our inference engine we chose to use WasmEdge as to use WASM runtime. However to do that we needed a way for us to firs have a program to run LLM inferences (for proof-of-concept and feasibility evaluation). For the inference engine we take inspiration from the excellent llama.cpp \footnote{https://github.com/ggerganov/llama.cpp} project and create a simple Rust program for running the inferences. Here we chose to use Rust as a programming language to remove friction between our smart contract execution on CosmosWasm and here, since our Rust code can directly be integrated into the smart contract and does not need any external message passing scheme to work. 

% \begin{lstlisting}[caption={Wasmedge Code},breaklines, language=bash, label=lst-api-reg]
% wasmedge --dir --nn-preload default:GGML:AUTO:llama-2-7b-chat-q5_k_m.gguf llama-chat.wasm

% [USER]:
% Who is the "father of computer"?
% [ASSISTANT]:
% The term "father of the computer" is often attributed to Charles Babbage, a British mathematician and inventor who conceived the concept of a programmable computer in the 19th century. 
% However, the term has also been used to describe other pioneers in the field of computing, such as Alan Turing, who is known for his work on the Turing machine and the concept of computational universality.
% [USER]:
% Was he the only one?
% [ASSISTANT]:
% No, Charles Babbage was not the only pioneer in the field of computing. He was one of many individuals who contributed to the development of modern computers. 
% .....
% \end{lstlisting}

\subsection{AI Inferences across Nodes}
While our solution works completely fine running in a single node, to be useful in real-world blockchain setting we also explore how it performs if we have multiple Cosmos validator nodes running inferences (use the same AI model). 
One of the challenging use cases of such a setting will be multiple node running an inference with the same machine learning model and an identical input. 

To achieve consensus, it requires AI inference reproducibility.  Since LLMs are not-deterministic, we need a way to support determinism for this use case. Though inference reproducibility is a research topic of its own, existing results in the literature are enough for the purpose of our research. 

Running advanced ML and LLM inference across multiple blockchain nodes could introduce variability in inference results due to several factors. These factors include using deterministic AI code, floating number resolution, node hardware capabilities, and the stochastic nature of some LLMs if they utilize mechanisms like dropout during inference for regularization purposes. It is crucial to understand that blockchain architectures are inherently designed to ensure consensus despite potential discrepancies across nodes. Typically, mechanisms like Proof of Work (PoW) or Proof of Stake (PoS) are employed to achieve agreement among nodes regarding the validity of transactions, which could analogously be applied to agreeing on LLM inference results. 

Based on experiments, the best practice to achieve inference reproducibility across nodes, is to use deterministic implementation of machine learning algorithm, manage random seeds with on-chain support like using on-chain random beacon (a well studied problem in the recent years), ran AI models using 64 bit floating numbers.  It is important to highlight that inference reproducibility has been successfully demonstrated in one application use case where LLMs are used as a covert communication channel between a sender and a receiver. In ~\cite{bauercodaspy}, the authors demonstrate how LLMs can be configured to produce consistent LLM outputs across devices. Our case directly benefits from the findings and best practice identified in the LLM covert communication research for achieving reproducible inferences across validator nodes.  

If the LLM inferences do vary across nodes, the blockchain protocol would need to specify a method for selecting the 'correct' or 'agreed-upon' inference. This is based on a majority rule where the most frequently generated result is chosen, or more sophisticated approaches like weighted scoring based on node reputability or stake can be employed.

%Furthermore, the scalability and performance implications must be considered. As more nodes participate in the inference process, the computational and communication overhead could increase significantly, potentially slowing down the inference process. Techniques such as sharding, where the blockchain is divided into smaller, more manageable segments (shards), could be leveraged to maintain efficiency.

%While running LLM inference in a blockchain framework on a single node does not fully harness the distributed nature of blockchain technology, expanding this to multiple nodes introduces complexities in ensuring consistent and reliable inference across the network. Bauer et al~\cite{bauercodaspy} shows how these can be used to produce consistent LLM outputs.

\section{Portability}
One of the key benefits to our proposed solution is model portability and inference engine (as well as model) agnostic nature. Even though we had chosen WasmEdge to showcase our proposed framework, this can easily be replaced by any other code that can communicate directly with a Web Assembly runtime. The primary reason we chose WasmEdge or any Web Assembly runtime because of their ability to communicate with the WebGPU~\cite{kenwright2022introduction} API. We have also tested our implementation using the well documented WGPU ~\cite{wgpu} API.

\subsection{Inference Engine Portability}

Rust is used to construct the demo inference code, which is compiled to WebAssembly. This core Rust code is only forty lines long and is surprisingly small. It can process inputs from the user, log the flow of the conversation, modify the text to make it compatible with the llama2 input template, and do inference tasks using the WASI NN API. This simplified method demonstrates how well the code manages these operations and how efficient it is.

\begin{lstlisting}[caption={Rust Inference}, style=boxed, breaklines, language=Rust, label=infer rust, basicstyle=\tiny]
fn main() {
    let arguments: Vec<String> = env::args().collect();
    let model_identifier: &str = &arguments[1];

    let neural_network =
        wasi_nn::GraphBuilder::new(wasi_nn::GraphEncoding::Ggml, wasi_nn::ExecutionTarget::AUTO)
            .assemble_from_cache(model_identifier)
            .expect("Successful build");
    let mut execution_context = neural_network.create_execution_context().expect("Context initialization");

.......
.......
.......

        // Input processing.
        let input_tensor_data = combined_prompt.as_bytes().to_vec();
        execution_context
            .assign_input(0, wasi_nn::TensorType::U8, &[1], &input_tensor_data)
            .expect("Input set");

        // Inference execution.
        execution_context.execute().expect("Successful computation");

        // Output retrieval.
        let mut result_buffer = vec![0u8; 1000];
        let result_size = execution_context.obtain_output(0, &mut result_buffer).expect("Output retrieval");
        let response = String::from_utf8_lossy(&result_buffer[..result_size]).to_string();
        println!("Response:\n{}", response.trim());

        historical_prompt.push_str(&format!("{} {}", combined_prompt, response.trim()));
    }
}
\end{lstlisting}

This can be compiled using

\begin{lstlisting}[caption={Rust Compile},breaklines, language=bash, label=rust compile]
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
rustup target add wasm32-wasi
\end{lstlisting}

Our handwritten Rust code is also definitely not a requirement. To showcase portability we had directly taken the llama2.c~\cite{karpathy} engine and compiled it into WASM file.

\begin{lstlisting}[caption={llama2.c wasm},breaklines, language=bash, label=llama2.c wasm]
$ run.c -D_WASI_EMULATED_MMAN -lwasi-emulated-mman -D_WASI_EMULATED_PROCESS_CLOCKS -lwasi-emulated-process-clocks -o run.wasm
\end{lstlisting}

\subsection{Model Portability}

Even though we have primarily targeted the Llama 2 family of models. We are not bounded by them. Since we base our implementation on the llama.cpp, the model formats have to be ggml. For which we utilize the GGML plugin for WasmEdge~\cite{WasmEdge-WASINN}. We also have tested the framework with LLAVA~\cite{liu2023llava} which is a multi-modal model. However our inference code is not equipped to handle non-text inputs, hence we only tested the text input and output of the model. However, a base64 encoder and decoder is certainly possible to pass multi-modal (image, numbers, text) input to the model to get its feedback.

\sln{} works as shown in Fig  \ref{fig-archi}.

\begin{figure*}%[!htbp]
	\centering
	\includegraphics[height=4.2cm, width=7cm]{sections/fig/figure2.pdf}
	\caption{Working of \sln{}.}   
	\label{fig-archi}
\end{figure*}

The validator node running in Cosmwasm generates the input message to be passed to the x/aiwasm runtime (which in this case is WasmEdge) which runs the inferences. 

WasmEdge can utilize both GPU or CPU based on what is available in the host system. It uses WebGPU API to see if there is a compatible GPU available, and runs the inference if it is available. If not then tries to run the inference in CPU. \sln{} is compatible with other Cosmos modules since it does not modify anything within the CosmWasm, but only individual contract. 